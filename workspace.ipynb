{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import dependencies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a75e577969a4ea19"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:26.964697Z",
     "start_time": "2024-04-11T05:59:24.549370Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from argparse import Namespace\n",
    "from collections import (\n",
    "    Counter,\n",
    "    defaultdict,\n",
    ")\n",
    "from shutil import (\n",
    "    copyfile,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchcrf import CRF\n",
    "\n",
    "from utils import (\n",
    "    torch_utils,\n",
    "    scorer,\n",
    "    constant,\n",
    ")\n",
    "from utils.constant import UNK_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Parse arguments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f84e7d6f0b68ce09"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data_dir='dataset/definition/textbook',\n",
    "    vocab_dir='dataset/definition/textbook/vocab',\n",
    "    emb_dim=300,\n",
    "    ner_dim=30,\n",
    "    pos_dim=30,\n",
    "    hidden_dim=200,\n",
    "    num_layers=2,\n",
    "    input_dropout=0.5,\n",
    "    gcn_dropout=0.5,\n",
    "    word_dropout=0.04,\n",
    "    topn=10000000000.0,\n",
    "    lower=False,\n",
    "    ratio=1,\n",
    "    only_label=0,\n",
    "    sent_loss=100.0,\n",
    "    dep_path_loss=100.0,\n",
    "    consistency_loss=1.0,\n",
    "    prune_k=-1,\n",
    "    conv_l2=0,\n",
    "    pooling='max',\n",
    "    pooling_l2=0.003,\n",
    "    mlp_layers=2,\n",
    "    no_adj=False,\n",
    "    rnn=True,\n",
    "    rnn_hidden=200,\n",
    "    rnn_layers=1,\n",
    "    rnn_dropout=0.5,\n",
    "    lr=0.0003,\n",
    "    lr_decay=0.9,\n",
    "    decay_epoch=5,\n",
    "    optim='adamax',\n",
    "    num_epoch=100,\n",
    "    batch_size=50,\n",
    "    max_grad_norm=5.0,\n",
    "    log_step=20,\n",
    "    log='logs.txt',\n",
    "    save_epoch=100,\n",
    "    save_dir='./saved_models',\n",
    "    id='1',\n",
    "    info='',\n",
    "    seed=0,\n",
    "    cuda=torch.cuda.is_available(),\n",
    "    cpu=not torch.cuda.is_available(),\n",
    "    load=False,\n",
    "    model_file=None\n",
    ")\n",
    "opt = vars(args)\n",
    "\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:26.966338Z",
     "start_time": "2024-04-11T05:59:26.962208Z"
    }
   },
   "id": "dd72a73236ec9c70",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Set random seed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4134762beb40c57"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:26.971651Z",
     "start_time": "2024-04-11T05:59:26.966443Z"
    }
   },
   "id": "f33ebc55d0d55924",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Load vocab"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e7f4322efdd3c41"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.id2word = pickle.load(f)\n",
    "            self.word2id = defaultdict(\n",
    "                lambda: UNK_ID,\n",
    "                {self.id2word[idx]: idx for idx in range(len(self.id2word))}\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.id2word)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:26.977128Z",
     "start_time": "2024-04-11T05:59:26.973660Z"
    }
   },
   "id": "8e49cdda540104f0",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab with 26106 words and 300 dims.\n"
     ]
    }
   ],
   "source": [
    "# vocabulary: set of unique words that the dataset contains.\n",
    "vocab = Vocab(os.path.join(opt['vocab_dir'], 'vocab.pkl'))\n",
    "opt['vocab_size'] = vocab.size\n",
    "\n",
    "# word embedding: vector representation of each word in the vocabulary\n",
    "emb_matrix = np.load(os.path.join(opt['vocab_dir'], 'embedding.npy'))\n",
    "\n",
    "print(f\"\"\"Loaded vocab with {vocab.size} words and {emb_matrix.shape[1]} dims.\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:27.038934Z",
     "start_time": "2024-04-11T05:59:26.976305Z"
    }
   },
   "id": "f0f49b2cc7a355bb",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11f9e2f2b1a55161"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Load data from json files, preprocess and prepare batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, opt, vocab, evaluation=False):\n",
    "        self.batch_size = opt['batch_size']\n",
    "        self.opt = opt\n",
    "        self.vocab = vocab\n",
    "        self.eval = evaluation\n",
    "\n",
    "        with open(filename) as f:\n",
    "            data = self.preprocess(json.load(f), vocab, opt)\n",
    "\n",
    "        # shuffle for training\n",
    "        if not evaluation:\n",
    "            indices = list(range(len(data)))\n",
    "            random.shuffle(indices)\n",
    "            data = [data[i] for i in indices]\n",
    "\n",
    "        self.id2label = dict([(v, k) for k, v in constant.LABEL_TO_ID.items()])\n",
    "        self.sent_id2label = dict([(v, k) for k, v in constant.SENT_LABEL_TO_ID.items()])\n",
    "        self.labels = [[self.id2label[l]] for d in data for l in d[-2]]\n",
    "        self.sent_labels = [self.sent_id2label[d[-1]] for d in data]\n",
    "\n",
    "        # chunk into batches\n",
    "        data = [\n",
    "            data[idx:idx + opt['batch_size']]\n",
    "            for idx in range(0, len(data), opt['batch_size'])\n",
    "        ]\n",
    "        self.data = data\n",
    "        print(f\"{len(data)} batches created for {filename}\")\n",
    "\n",
    "    def preprocess(self, dataset, vocab, opt):\n",
    "        \"\"\" Preprocess the data and convert to ids. \"\"\"\n",
    "        processed = []\n",
    "        for sentence in dataset:\n",
    "            tokens = list(sentence['tokens'])\n",
    "            if opt['lower']:\n",
    "                tokens = [t.lower() for t in tokens]\n",
    "\n",
    "            tokens = [vocab.word2id[token] for token in tokens]\n",
    "            pos = [constant.POS_TO_ID[pos] for pos in sentence['pos']]\n",
    "            labels = [constant.LABEL_TO_ID[label] for label in sentence['labels']]\n",
    "\n",
    "            # Parses the dependency head information\n",
    "            head = [int(x) for x in sentence['heads']]\n",
    "            # checks for at least one root in the dependency tree\n",
    "            assert any([x == -1 for x in head])\n",
    "\n",
    "            # Initializes dependency path representation\n",
    "            dep_path = [0] * len(sentence['tokens'])\n",
    "            for i in sentence['dep_path']:\n",
    "                if i != -1:\n",
    "                    dep_path[i] = 1\n",
    "\n",
    "            # Constructs adjacency matrix\n",
    "            # indicating direct connections between tokens.\n",
    "            adj = np.zeros((len(sentence['heads']), len(sentence['heads'])))\n",
    "            for i, h in enumerate(sentence['heads']):\n",
    "                adj[i][h] = 1\n",
    "                adj[h][i] = 1\n",
    "\n",
    "            if self.eval or self.opt['only_label'] != 1 or sentence['label'] != 'none':\n",
    "                counter = Counter(sentence['labels'])\n",
    "                terms = [0] * len(sentence['labels'])\n",
    "                defs = [0] * len(sentence['labels'])\n",
    "                # Identifies 'Term' and 'Definition' entities within the labels, marking their positions.\n",
    "                if counter['B-Term'] == 1 and counter['B-Definition'] == 1:\n",
    "                    for i, label in enumerate(sentence['labels']):\n",
    "                        if 'Term' in label:\n",
    "                            terms[i] = 1\n",
    "                        if 'Definition' in label:\n",
    "                            defs[i] = 1\n",
    "\n",
    "                processed.append(\n",
    "                    (\n",
    "                        tokens, pos, head, terms, defs, dep_path, adj, labels,\n",
    "                        constant.SENT_LABEL_TO_ID[sentence['label']])\n",
    "                )\n",
    "\n",
    "        return processed\n",
    "\n",
    "    def gold(self):\n",
    "        \"\"\" Return gold labels as a list. \"\"\"\n",
    "        return self.labels\n",
    "\n",
    "    def sent_gold(self):\n",
    "        return self.sent_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a batch with index. \"\"\"\n",
    "        batch = self.data[index]\n",
    "        batch_size = len(batch)\n",
    "        batch = list(zip(*batch))\n",
    "        assert len(batch) == 9\n",
    "\n",
    "        # sort all fields by lens for easy RNN operations\n",
    "        lens = [len(x) for x in batch[0]]\n",
    "        batch, orig_idx = self.sort_all(batch, lens)\n",
    "\n",
    "        # word dropout\n",
    "        if not self.eval:\n",
    "            words = [\n",
    "                self.word_dropout(sent, self.opt['word_dropout'])\n",
    "                for sent in batch[0]\n",
    "            ]\n",
    "        else:\n",
    "            words = batch[0]\n",
    "\n",
    "        # convert to tensors\n",
    "        words = self.get_long_tensor(words, batch_size)\n",
    "        masks = torch.eq(words, 0)\n",
    "        pos = self.get_long_tensor(batch[1], batch_size)\n",
    "        head = self.get_long_tensor(batch[2], batch_size)\n",
    "        terms = self.get_long_tensor(batch[3], batch_size)\n",
    "        defs = self.get_long_tensor(batch[4], batch_size)\n",
    "        dep_path = self.get_long_tensor(batch[5], batch_size).float()\n",
    "        adj = self.get_float_tensor2D(batch[6], batch_size)\n",
    "\n",
    "        labels = self.get_long_tensor(batch[7], batch_size)\n",
    "\n",
    "        sent_labels = torch.FloatTensor(batch[8])\n",
    "\n",
    "        return words, masks, pos, head, terms, defs, adj, labels, sent_labels, dep_path, orig_idx\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.__len__()):\n",
    "            yield self.__getitem__(i)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_long_tensor(tokens_list, batch_size):\n",
    "        \"\"\" Convert list of list of tokens to a padded LongTensor. \"\"\"\n",
    "        token_len = max(len(x) for x in tokens_list)\n",
    "        tokens = torch.LongTensor(batch_size, token_len).fill_(constant.PAD_ID)\n",
    "        for i, s in enumerate(tokens_list):\n",
    "            tokens[i, :len(s)] = torch.LongTensor(s)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def get_float_tensor2D(tokens_list, batch_size):\n",
    "        \"\"\" Convert list of list of tokens to a padded LongTensor. \"\"\"\n",
    "        token_len = max(len(x) for x in tokens_list)\n",
    "        tokens = torch.FloatTensor(batch_size, token_len, token_len).fill_(constant.PAD_ID)\n",
    "        for i, s in enumerate(tokens_list):\n",
    "            tokens[i, :len(s), :len(s)] = torch.FloatTensor(s)\n",
    "        return tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_all(batch, lens):\n",
    "        \"\"\" Sort all fields by descending order of lens, and return the original indices. \"\"\"\n",
    "        unsorted_all = [lens] + [range(len(lens))] + list(batch)\n",
    "        sorted_all = [list(t) for t in zip(*sorted(zip(*unsorted_all), reverse=True))]\n",
    "        return sorted_all[2:], sorted_all[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def word_dropout(tokens, dropout):\n",
    "        \"\"\" Randomly dropout tokens (IDs) and replace them with <UNK> tokens. \"\"\"\n",
    "        return [\n",
    "            constant.UNK_ID if x != constant.UNK_ID and np.random.random() < dropout else x\n",
    "            for x in tokens\n",
    "        ]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:27.051495Z",
     "start_time": "2024-04-11T05:59:27.035516Z"
    }
   },
   "id": "2c8f66495223c032",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataset/definition/textbook with batch size 50...\n",
      "354 batches created for dataset/definition/textbook/train.json\n",
      "45 batches created for dataset/definition/textbook/dev.json\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading data from {opt['data_dir']} with batch size {opt['batch_size']}...\")\n",
    "train_batch = DataLoader(os.path.join(opt['data_dir'], 'train.json'), opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(os.path.join(opt['data_dir'], 'dev.json'), opt, vocab, evaluation=True)\n",
    "\n",
    "model_save_dir = os.path.join(opt['save_dir'], opt['id'])\n",
    "opt['model_save_dir'] = os.path.join(opt['save_dir'], opt['id'])\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:28.114142Z",
     "start_time": "2024-04-11T05:59:27.052370Z"
    }
   },
   "id": "75c9801be7cc9a90",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25bcc98828ff4c07"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\" A GCN/Contextualized GCN module operated on dependency graphs. \"\"\"\n",
    "\n",
    "    def __init__(self, opt, embeddings, mem_dim, num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.layers = num_layers\n",
    "        self.use_cuda = opt['cuda']\n",
    "        self.mem_dim = mem_dim\n",
    "        self.in_dim = opt['emb_dim'] + opt['pos_dim']\n",
    "\n",
    "        self.emb, self.pos_emb = embeddings\n",
    "\n",
    "        # rnn layer\n",
    "        if self.opt.get('rnn', False):\n",
    "            input_size = self.in_dim\n",
    "            self.rnn = nn.LSTM(input_size, opt['rnn_hidden'], opt['rnn_layers'], batch_first=True,\n",
    "                               dropout=opt['rnn_dropout'], bidirectional=True)\n",
    "            self.in_dim = opt['rnn_hidden'] * 2\n",
    "            self.rnn_drop = nn.Dropout(opt['rnn_dropout'])  # use on last layer output\n",
    "\n",
    "        self.in_drop = nn.Dropout(opt['input_dropout'])\n",
    "        self.gcn_drop = nn.Dropout(opt['gcn_dropout'])\n",
    "\n",
    "        # gcn layer\n",
    "        self.W = nn.ModuleList()\n",
    "\n",
    "        for layer in range(self.layers):\n",
    "            input_dim = self.in_dim if layer == 0 else self.mem_dim\n",
    "            self.W.append(nn.Linear(input_dim, self.mem_dim))\n",
    "\n",
    "    def conv_l2(self):\n",
    "        conv_weights = []\n",
    "        for w in self.W:\n",
    "            conv_weights += [w.weight, w.bias]\n",
    "        return sum([x.pow(2).sum() for x in conv_weights])\n",
    "\n",
    "    def encode_with_rnn(self, rnn_inputs, masks, batch_size):\n",
    "        seq_lens = list(masks.data.eq(constant.PAD_ID).long().sum(1).squeeze())\n",
    "        h0, c0 = self.rnn_zero_state(batch_size, self.opt['rnn_hidden'], self.opt['rnn_layers'])\n",
    "        rnn_inputs = nn.utils.rnn.pack_padded_sequence(rnn_inputs, seq_lens, batch_first=True)\n",
    "        rnn_outputs, (ht, ct) = self.rnn(rnn_inputs, (h0, c0))\n",
    "        rnn_outputs, _ = nn.utils.rnn.pad_packed_sequence(rnn_outputs, batch_first=True)\n",
    "        return rnn_outputs\n",
    "\n",
    "    def forward(self, adj, inputs):\n",
    "        words, masks, pos, head, terms, defs, adj = inputs  # unpack\n",
    "        word_embs = self.emb(words)\n",
    "        embs = [word_embs]\n",
    "        if self.opt['pos_dim'] > 0:\n",
    "            embs += [self.pos_emb(pos)]\n",
    "        embs = torch.cat(embs, dim=2)\n",
    "        embs = self.in_drop(embs)\n",
    "\n",
    "        # rnn layer\n",
    "        if self.opt.get('rnn', False):\n",
    "            gcn_inputs = self.rnn_drop(self.encode_with_rnn(embs, masks, words.size()[0]))\n",
    "        else:\n",
    "            gcn_inputs = embs\n",
    "\n",
    "        lstm_outs = gcn_inputs.clone()\n",
    "\n",
    "        # gcn layer\n",
    "        denom = adj.sum(2).unsqueeze(2) + 1\n",
    "        mask = (adj.sum(2) + adj.sum(1)).eq(0).unsqueeze(2)\n",
    "\n",
    "        for l in range(self.layers):\n",
    "            Ax = adj.bmm(gcn_inputs)\n",
    "            AxW = self.W[l](Ax)\n",
    "            AxW = AxW + self.W[l](gcn_inputs)  # self loop\n",
    "            AxW = AxW / denom\n",
    "\n",
    "            gAxW = F.relu(AxW)\n",
    "            gcn_inputs = self.gcn_drop(gAxW) if l < self.layers - 1 else gAxW\n",
    "\n",
    "        return lstm_outs, masks.unsqueeze(2), gcn_inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def rnn_zero_state(batch_size, hidden_dim, num_layers, bidirectional=True, use_cuda=True):\n",
    "        total_layers = num_layers * 2 if bidirectional else num_layers\n",
    "        state_shape = (total_layers, batch_size, hidden_dim)\n",
    "        h0 = c0 = Variable(torch.zeros(*state_shape), requires_grad=False)\n",
    "        if torch.cuda.is_available():\n",
    "            return h0.cuda(), c0.cuda()\n",
    "        else:\n",
    "            return h0, c0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:28.119684Z",
     "start_time": "2024-04-11T05:59:28.110142Z"
    }
   },
   "id": "6edbbdf12bdab4fa",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GCNRelationModel(nn.Module):\n",
    "    def __init__(self, opt, emb_matrix=None):\n",
    "        super().__init__()\n",
    "        self.opt = opt\n",
    "        self.emb_matrix = emb_matrix\n",
    "\n",
    "        # create embedding layers\n",
    "        self.emb = nn.Embedding(opt['vocab_size'], opt['emb_dim'], padding_idx=constant.PAD_ID)\n",
    "        self.pos_emb = nn.Embedding(len(constant.POS_TO_ID), opt['pos_dim']) if opt['pos_dim'] > 0 else None\n",
    "        embeddings = (self.emb, self.pos_emb)\n",
    "        self.init_embeddings()\n",
    "\n",
    "        # gcn layer\n",
    "        self.gcn = GCN(opt, embeddings, opt['hidden_dim'], opt['num_layers'])\n",
    "\n",
    "        # output mlp layers\n",
    "        in_dim = opt['hidden_dim'] * 2\n",
    "        layers = [nn.Linear(in_dim, opt['hidden_dim']), nn.ReLU()]\n",
    "        for _ in range(self.opt['mlp_layers'] - 1):\n",
    "            layers += [nn.Linear(opt['hidden_dim'], opt['hidden_dim']), nn.ReLU()]\n",
    "        self.out_mlp = nn.Sequential(*layers)\n",
    "\n",
    "        # gcn output mlp layers\n",
    "        in_dim = opt['hidden_dim']\n",
    "        layers = [nn.Linear(in_dim, opt['hidden_dim']), nn.ReLU()]\n",
    "        for _ in range(self.opt['mlp_layers'] - 1):\n",
    "            layers += [nn.Linear(opt['hidden_dim'], opt['hidden_dim']), nn.ReLU()]\n",
    "        self.gcn_out_mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def init_embeddings(self):\n",
    "        if self.emb_matrix is None:\n",
    "            self.emb.weight.data[1:, :].uniform_(-1.0, 1.0)\n",
    "        else:\n",
    "            self.emb_matrix = torch.from_numpy(self.emb_matrix)\n",
    "            self.emb.weight.data.copy_(self.emb_matrix)\n",
    "        # decide finetuning\n",
    "        if self.opt['topn'] <= 0:\n",
    "            print(\"Do not finetune word embedding layer.\")\n",
    "            self.emb.weight.requires_grad = False\n",
    "        elif self.opt['topn'] < self.opt['vocab_size']:\n",
    "            print(\"Finetune top {} word embeddings.\".format(self.opt['topn']))\n",
    "            self.emb.weight.register_hook(lambda x: torch_utils.keep_partial_grad(x, self.opt['topn']))\n",
    "        else:\n",
    "            print(\"Finetune all embeddings.\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        words, masks, pos, head, terms, defs, adj = inputs  # unpack\n",
    "        l = (masks.data.cpu().numpy() == 0).astype(np.int64).sum(1)\n",
    "        maxlen = max(l)\n",
    "\n",
    "        h, pool_mask, gcn_outputs = self.gcn(adj, inputs)\n",
    "\n",
    "        outputs = self.out_mlp(h)\n",
    "        gcn_outputs = self.gcn_out_mlp(gcn_outputs)\n",
    "        return outputs, gcn_outputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:28.134481Z",
     "start_time": "2024-04-11T05:59:28.123041Z"
    }
   },
   "id": "400742edb7b34193",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GCNClassifier(nn.Module):\n",
    "    \"\"\" A wrapper classifier for GCNRelationModel. \"\"\"\n",
    "\n",
    "    def __init__(self, opt, emb_matrix=None):\n",
    "        super().__init__()\n",
    "        self.opt = opt\n",
    "        self.gcn_model = GCNRelationModel(opt, emb_matrix=emb_matrix)\n",
    "        in_dim = opt['hidden_dim']\n",
    "        self.classifier = nn.Linear(in_dim * 2, opt['num_class'])\n",
    "        self.selector = nn.Sequential(nn.Linear(in_dim, 1), nn.Sigmoid())\n",
    "\n",
    "        in_dim = opt['hidden_dim']\n",
    "        layers = [nn.Linear(in_dim, opt['hidden_dim']), nn.ReLU()]\n",
    "        for _ in range(self.opt['mlp_layers'] - 1):\n",
    "            layers += [nn.Linear(opt['hidden_dim'], opt['hidden_dim']), nn.ReLU()]\n",
    "        self.out_mlp = nn.Sequential(*layers)\n",
    "\n",
    "        self.sent_classifier = nn.Sequential(nn.Linear(in_dim, 1), nn.Sigmoid())\n",
    "        self.term_classifier = nn.Sequential(nn.Linear(in_dim * 2, 1), nn.Sigmoid())\n",
    "        self.opt = opt\n",
    "\n",
    "    def conv_l2(self):\n",
    "        return self.gcn_model.gcn.conv_l2()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        _, masks, _, _, terms, defs, _ = inputs  # unpack\n",
    "\n",
    "        outputs, gcn_outputs = self.gcn_model(inputs)\n",
    "        logits = self.classifier(torch.cat([outputs, gcn_outputs], dim=2))\n",
    "\n",
    "        pool_type = self.opt['pooling']\n",
    "        out = self.pool(outputs, masks.unsqueeze(2), type=pool_type)\n",
    "        out = self.out_mlp(out)\n",
    "        sent_logits = self.sent_classifier(out)\n",
    "\n",
    "        terms_out = self.pool(F.softmax(outputs), terms.unsqueeze(2).byte(), type=pool_type)\n",
    "        defs_out = self.pool(F.softmax(outputs), defs.unsqueeze(2).byte(), type=pool_type)\n",
    "        term_def = (terms_out * defs_out).sum(1).mean()\n",
    "        not_term_def = (terms_out * defs_out[torch.randperm(terms_out.shape[0])]).sum(1).mean()\n",
    "\n",
    "        selections = self.selector(gcn_outputs)\n",
    "\n",
    "        defs_out = self.pool(outputs, defs.unsqueeze(2).byte(), type=pool_type).repeat(1, outputs.shape[1]).view(\n",
    "            *outputs.shape)\n",
    "        term_selections = self.term_classifier(torch.cat([defs_out, outputs], dim=2))\n",
    "\n",
    "        return logits, sent_logits.squeeze(), selections.squeeze(), term_def, not_term_def, term_selections\n",
    "\n",
    "    @staticmethod\n",
    "    def pool(h, mask, type='max'):\n",
    "        if type == 'max':\n",
    "            h = h.masked_fill(mask.bool(), -constant.INFINITY_NUMBER)\n",
    "            return torch.max(h, 1)[0]\n",
    "        elif type == 'avg':\n",
    "            h = h.masked_fill(mask.bool(), 0)\n",
    "            return h.sum(1) / (mask.size(1) - mask.float().sum(1))\n",
    "        else:\n",
    "            h = h.masked_fill(mask.bool(), 0)\n",
    "            return h.sum(1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:28.169096Z",
     "start_time": "2024-04-11T05:59:28.131411Z"
    }
   },
   "id": "9a955ef527711534",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GCNTrainer:\n",
    "    def __init__(self, opt, emb_matrix=None):\n",
    "        self.opt = opt\n",
    "        self.emb_matrix = emb_matrix\n",
    "        self.model = GCNClassifier(opt, emb_matrix=emb_matrix)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        self.parameters = [\n",
    "            parameter\n",
    "            for parameter in self.model.parameters()\n",
    "            if parameter.requires_grad\n",
    "        ]\n",
    "        self.crf = CRF(self.opt['num_class'], batch_first=True)\n",
    "        self.bc = nn.BCELoss()\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "            self.criterion.cuda()\n",
    "            self.crf.cuda()\n",
    "            self.bc.cuda()\n",
    "\n",
    "        self.optimizer = torch_utils.get_optimizer(\n",
    "            opt['optim'],\n",
    "            self.parameters,\n",
    "            opt['lr']\n",
    "        )\n",
    "\n",
    "    def update(self, batch):\n",
    "        inputs, labels, sent_labels, dep_path, tokens, head, lens = self.unpack_batch(batch, self.opt['cuda'])\n",
    "\n",
    "        _, _, _, _, terms, _, _ = inputs\n",
    "\n",
    "        # step forward\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        logits, class_logits, selections, term_def, not_term_def, term_selections = self.model(inputs)\n",
    "\n",
    "        labels = labels - 1\n",
    "        labels[labels < 0] = 0\n",
    "        mask = inputs[1].float()\n",
    "        mask[mask == 0.] = -1.\n",
    "        mask[mask == 1.] = 0.\n",
    "        mask[mask == -1.] = 1.\n",
    "        mask = mask.byte()\n",
    "        loss = -self.crf(logits, labels, mask=mask)\n",
    "\n",
    "        sent_loss = self.bc(class_logits, sent_labels)\n",
    "        loss += self.opt['sent_loss'] * sent_loss\n",
    "\n",
    "        selection_loss = self.bc(selections.view(-1, 1), dep_path.view(-1, 1))\n",
    "        loss += self.opt['dep_path_loss'] * selection_loss\n",
    "\n",
    "        term_def_loss = -self.opt['consistency_loss'] * (term_def - not_term_def)\n",
    "        loss += term_def_loss\n",
    "        # loss += self.opt['consistency_loss'] * not_term_def\n",
    "\n",
    "        term_loss = self.opt['sent_loss'] * self.bc(term_selections.view(-1, 1), terms.float().view(-1, 1))\n",
    "        loss += term_loss\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.opt['max_grad_norm'])\n",
    "        self.optimizer.step()\n",
    "        return loss_val, sent_loss.item(), term_loss.item()\n",
    "\n",
    "    def predict(self, batch, unsort=True):\n",
    "        inputs, labels, sent_labels, dep_path, tokens, head, lens = self.unpack_batch(batch, self.opt['cuda'])\n",
    "\n",
    "        orig_idx = batch[-1]\n",
    "        # forward\n",
    "        self.model.eval()\n",
    "        logits, sent_logits, _, _, _, _ = self.model(inputs)\n",
    "\n",
    "        labels = labels - 1\n",
    "        labels[labels < 0] = 0\n",
    "        mask = inputs[1].float()\n",
    "        mask[mask == 0.] = -1.\n",
    "        mask[mask == 1.] = 0.\n",
    "        mask[mask == -1.] = 1.\n",
    "        mask = mask.byte()\n",
    "        loss = -self.crf(logits, labels, mask=mask)\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        predictions = self.crf.decode(logits, mask=mask)\n",
    "\n",
    "        sent_predictions = sent_logits.round().long().data.cpu().numpy()\n",
    "\n",
    "        if unsort:\n",
    "            _, predictions, probs, sent_predictions = [list(t) for t in zip(*sorted(\n",
    "                zip(orig_idx, predictions, probs, sent_predictions)))]\n",
    "        return predictions, probs, loss.item(), sent_predictions\n",
    "\n",
    "    def update_lr(self, new_lr):\n",
    "        \"\"\"\n",
    "        This function updates the learning rate of the optimizer used by the Trainer.\n",
    "        It sets a new learning rate new_lr for all parameter groups in the optimizer.\n",
    "\n",
    "        The learning rate influences the speed and quality of the learning process\n",
    "        \"\"\"\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"\n",
    "        The load function loads a model from the given filename using torch.load.\n",
    "\n",
    "        It then loads the state dictionary of the model and the configuration from the checkpoint.\n",
    "\n",
    "        If an exception occurs during the loading process,\n",
    "        it prints an error message and exits the program.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(filename)\n",
    "        except BaseException:\n",
    "            checkpoint = torch.load(filename, map_location=torch.device('cpu'))\n",
    "        self.model.load_state_dict(checkpoint['model'])\n",
    "        self.opt = checkpoint['config']\n",
    "\n",
    "    def save(self, filename, epoch):\n",
    "        params = {\n",
    "            'model':  self.model.state_dict(),\n",
    "            'config': self.opt,\n",
    "        }\n",
    "        try:\n",
    "            torch.save(params, filename)\n",
    "            print(\"model saved to {}\".format(filename))\n",
    "        except BaseException:\n",
    "            print(\"[Warning: Saving failed... continuing anyway.]\")\n",
    "\n",
    "    @staticmethod\n",
    "    def unpack_batch(batch, cuda):\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = [Variable(b.cuda()) for b in batch[:7]]\n",
    "            labels = Variable(batch[7].cuda())\n",
    "            sent_labels = Variable(batch[8].cuda())\n",
    "            dep_path = Variable(batch[9].cuda())\n",
    "        else:\n",
    "            inputs = [Variable(b) for b in batch[:7]]\n",
    "            labels = Variable(batch[7])\n",
    "            sent_labels = Variable(batch[8])\n",
    "            dep_path = Variable(batch[9])\n",
    "\n",
    "        tokens = batch[0]\n",
    "        head = batch[3]\n",
    "        lens = batch[1].eq(0).long().sum(1).squeeze()\n",
    "        return inputs, labels, sent_labels, dep_path, tokens, head, lens\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:28.171072Z",
     "start_time": "2024-04-11T05:59:28.133159Z"
    }
   },
   "id": "83bda6851b10034c",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune all embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linguohui/Projects/definition_extraction/venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "trainer = GCNTrainer(opt, emb_matrix=emb_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:28.778115Z",
     "start_time": "2024-04-11T05:59:28.146045Z"
    }
   },
   "id": "cd94812420ce6438",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "id2label = dict([(v, k) for k, v in constant.LABEL_TO_ID.items()])\n",
    "dev_score_history = []\n",
    "current_lr = opt['lr']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:28.782955Z",
     "start_time": "2024-04-11T05:59:28.778393Z"
    }
   },
   "id": "7e3782621f00f367",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T05:59:28.783407Z",
     "start_time": "2024-04-11T05:59:28.781375Z"
    }
   },
   "id": "4343235710302177",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/d0m974hx2f931dg9q0xcjx480000gn/T/ipykernel_15348/2940291242.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  terms_out = self.pool(F.softmax(outputs), terms.unsqueeze(2).byte(), type=pool_type)\n",
      "/var/folders/t9/d0m974hx2f931dg9q0xcjx480000gn/T/ipykernel_15348/2940291242.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  defs_out = self.pool(F.softmax(outputs), defs.unsqueeze(2).byte(), type=pool_type)\n",
      "/Users/linguohui/Projects/definition_extraction/venv/lib/python3.12/site-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorCompare.cpp:519.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:59:36: step 20/35400 (epoch 1/100), loss = 912.394592, sent_loss = 0.605355, dep_path_loss = 7.904532 (0.344 sec/batch), lr: 0.000300\n",
      "01:59:42: step 40/35400 (epoch 1/100), loss = 780.796692, sent_loss = 0.650625, dep_path_loss = 8.824707 (0.257 sec/batch), lr: 0.000300\n",
      "01:59:47: step 60/35400 (epoch 1/100), loss = 835.857971, sent_loss = 0.638335, dep_path_loss = 7.909427 (0.301 sec/batch), lr: 0.000300\n",
      "01:59:54: step 80/35400 (epoch 1/100), loss = 571.141479, sent_loss = 0.607847, dep_path_loss = 5.441769 (0.267 sec/batch), lr: 0.000300\n",
      "01:59:59: step 100/35400 (epoch 1/100), loss = 756.993103, sent_loss = 0.586071, dep_path_loss = 4.268195 (0.371 sec/batch), lr: 0.000300\n",
      "02:00:06: step 120/35400 (epoch 1/100), loss = 810.769470, sent_loss = 0.441815, dep_path_loss = 7.167094 (0.254 sec/batch), lr: 0.000300\n",
      "02:00:12: step 140/35400 (epoch 1/100), loss = 594.306335, sent_loss = 0.481428, dep_path_loss = 4.250006 (0.288 sec/batch), lr: 0.000300\n",
      "02:00:19: step 160/35400 (epoch 1/100), loss = 503.317139, sent_loss = 0.540523, dep_path_loss = 4.767273 (0.301 sec/batch), lr: 0.000300\n",
      "02:00:26: step 180/35400 (epoch 1/100), loss = 640.609131, sent_loss = 0.646000, dep_path_loss = 3.321907 (0.345 sec/batch), lr: 0.000300\n",
      "02:00:32: step 200/35400 (epoch 1/100), loss = 727.300964, sent_loss = 0.646927, dep_path_loss = 5.663400 (0.281 sec/batch), lr: 0.000300\n",
      "02:00:38: step 220/35400 (epoch 1/100), loss = 556.973145, sent_loss = 0.318897, dep_path_loss = 6.601409 (0.255 sec/batch), lr: 0.000300\n",
      "02:00:44: step 240/35400 (epoch 1/100), loss = 471.588257, sent_loss = 0.519859, dep_path_loss = 4.323723 (0.204 sec/batch), lr: 0.000300\n",
      "02:00:52: step 260/35400 (epoch 1/100), loss = 751.890137, sent_loss = 0.552178, dep_path_loss = 2.636035 (0.487 sec/batch), lr: 0.000300\n",
      "02:00:58: step 280/35400 (epoch 1/100), loss = 360.864319, sent_loss = 0.371641, dep_path_loss = 2.357398 (0.332 sec/batch), lr: 0.000300\n",
      "02:01:05: step 300/35400 (epoch 1/100), loss = 748.672363, sent_loss = 0.585859, dep_path_loss = 6.079431 (0.332 sec/batch), lr: 0.000300\n",
      "02:01:11: step 320/35400 (epoch 1/100), loss = 428.958649, sent_loss = 0.477434, dep_path_loss = 2.665231 (0.205 sec/batch), lr: 0.000300\n",
      "02:01:22: step 340/35400 (epoch 1/100), loss = 674.718994, sent_loss = 0.445938, dep_path_loss = 4.719592 (0.283 sec/batch), lr: 0.000300\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 53.006%\n",
      "   Recall (micro): 38.563%\n",
      "       F1 (micro): 44.645%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linguohui/Projects/definition_extraction/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train_loss = 34531.108598, train_sent_loss = 27.384392, train_dep_path_loss = 291.068073, dev_loss = 25822.843348, dev_f1 = 0.2335\n",
      "model saved to ./saved_models/1/checkpoint_epoch_1.pt\n",
      "Training ended with 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/d0m974hx2f931dg9q0xcjx480000gn/T/ipykernel_15348/2940291242.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  terms_out = self.pool(F.softmax(outputs), terms.unsqueeze(2).byte(), type=pool_type)\n",
      "/var/folders/t9/d0m974hx2f931dg9q0xcjx480000gn/T/ipykernel_15348/2940291242.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  defs_out = self.pool(F.softmax(outputs), defs.unsqueeze(2).byte(), type=pool_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:01:35: step 360/35400 (epoch 2/100), loss = 578.206909, sent_loss = 0.531332, dep_path_loss = 2.718844 (0.282 sec/batch), lr: 0.000300\n",
      "02:01:41: step 380/35400 (epoch 2/100), loss = 452.817261, sent_loss = 0.375164, dep_path_loss = 4.162277 (0.283 sec/batch), lr: 0.000300\n",
      "02:01:47: step 400/35400 (epoch 2/100), loss = 632.138123, sent_loss = 0.568697, dep_path_loss = 4.397583 (0.305 sec/batch), lr: 0.000300\n",
      "02:01:56: step 420/35400 (epoch 2/100), loss = 526.174255, sent_loss = 0.433998, dep_path_loss = 3.143573 (0.425 sec/batch), lr: 0.000300\n",
      "02:02:03: step 440/35400 (epoch 2/100), loss = 662.606689, sent_loss = 0.426815, dep_path_loss = 4.662734 (0.289 sec/batch), lr: 0.000300\n",
      "02:02:10: step 460/35400 (epoch 2/100), loss = 847.318970, sent_loss = 0.494855, dep_path_loss = 5.670879 (0.376 sec/batch), lr: 0.000300\n",
      "02:02:16: step 480/35400 (epoch 2/100), loss = 666.997864, sent_loss = 0.525844, dep_path_loss = 3.012007 (0.319 sec/batch), lr: 0.000300\n",
      "02:02:22: step 500/35400 (epoch 2/100), loss = 422.132690, sent_loss = 0.399806, dep_path_loss = 3.413748 (0.245 sec/batch), lr: 0.000300\n",
      "02:02:28: step 520/35400 (epoch 2/100), loss = 496.690002, sent_loss = 0.521390, dep_path_loss = 3.536198 (0.308 sec/batch), lr: 0.000300\n",
      "02:02:35: step 540/35400 (epoch 2/100), loss = 577.576599, sent_loss = 0.424847, dep_path_loss = 2.277050 (0.336 sec/batch), lr: 0.000300\n",
      "02:02:42: step 560/35400 (epoch 2/100), loss = 762.211243, sent_loss = 0.383304, dep_path_loss = 3.459394 (0.325 sec/batch), lr: 0.000300\n",
      "02:02:48: step 580/35400 (epoch 2/100), loss = 603.236267, sent_loss = 0.442086, dep_path_loss = 5.278774 (0.267 sec/batch), lr: 0.000300\n",
      "02:02:57: step 600/35400 (epoch 2/100), loss = 684.172180, sent_loss = 0.336841, dep_path_loss = 4.989686 (0.320 sec/batch), lr: 0.000300\n",
      "02:03:04: step 620/35400 (epoch 2/100), loss = 473.331055, sent_loss = 0.530678, dep_path_loss = 2.207151 (0.291 sec/batch), lr: 0.000300\n",
      "02:03:16: step 640/35400 (epoch 2/100), loss = 547.947327, sent_loss = 0.559506, dep_path_loss = 4.205103 (0.410 sec/batch), lr: 0.000300\n",
      "02:03:34: step 660/35400 (epoch 2/100), loss = 336.978455, sent_loss = 0.390438, dep_path_loss = 3.688559 (0.232 sec/batch), lr: 0.000300\n",
      "02:03:51: step 680/35400 (epoch 2/100), loss = 339.949677, sent_loss = 0.521779, dep_path_loss = 3.178061 (0.343 sec/batch), lr: 0.000300\n",
      "02:03:59: step 700/35400 (epoch 2/100), loss = 582.377136, sent_loss = 0.540494, dep_path_loss = 3.341803 (0.304 sec/batch), lr: 0.000300\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 49.388%\n",
      "   Recall (micro): 49.097%\n",
      "       F1 (micro): 49.242%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linguohui/Projects/definition_extraction/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: train_loss = 29223.149057, train_sent_loss = 23.580261, train_dep_path_loss = 184.560276, dev_loss = 25951.950294, dev_f1 = 0.2754\n",
      "model saved to ./saved_models/1/checkpoint_epoch_2.pt\n",
      "Training ended with 2 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/d0m974hx2f931dg9q0xcjx480000gn/T/ipykernel_15348/2940291242.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  terms_out = self.pool(F.softmax(outputs), terms.unsqueeze(2).byte(), type=pool_type)\n",
      "/var/folders/t9/d0m974hx2f931dg9q0xcjx480000gn/T/ipykernel_15348/2940291242.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  defs_out = self.pool(F.softmax(outputs), defs.unsqueeze(2).byte(), type=pool_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:04:18: step 720/35400 (epoch 3/100), loss = 392.521881, sent_loss = 0.392996, dep_path_loss = 1.604202 (1.104 sec/batch), lr: 0.000300\n",
      "02:04:34: step 740/35400 (epoch 3/100), loss = 490.627441, sent_loss = 0.515500, dep_path_loss = 3.180596 (0.787 sec/batch), lr: 0.000300\n",
      "02:04:51: step 760/35400 (epoch 3/100), loss = 588.346375, sent_loss = 0.363829, dep_path_loss = 3.823520 (0.839 sec/batch), lr: 0.000300\n",
      "02:05:02: step 780/35400 (epoch 3/100), loss = 318.281494, sent_loss = 0.347403, dep_path_loss = 2.923869 (0.309 sec/batch), lr: 0.000300\n",
      "02:05:15: step 800/35400 (epoch 3/100), loss = 631.860596, sent_loss = 0.446777, dep_path_loss = 4.156435 (0.296 sec/batch), lr: 0.000300\n",
      "02:05:24: step 820/35400 (epoch 3/100), loss = 647.165649, sent_loss = 0.528728, dep_path_loss = 2.137358 (0.342 sec/batch), lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch'] + 1):\n",
    "    train_loss = 0\n",
    "    train_sent_loss = 0\n",
    "    train_dep_path_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss, sent_loss, dep_path_loss = trainer.update(batch)\n",
    "        train_loss += loss\n",
    "        train_sent_loss += sent_loss\n",
    "        train_dep_path_loss += dep_path_loss\n",
    "        if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(\n",
    "                f\"{time.strftime('%H:%M:%S', time.localtime())}: step {global_step}/{max_steps} (epoch {epoch}/{opt['num_epoch']}), loss = {loss:.6f}, sent_loss = {sent_loss:.6f}, dep_path_loss = {dep_path_loss:.6f} ({duration:.3f} sec/batch), lr: {current_lr:.6f}\"\n",
    "            )\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss, _ = trainer.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "\n",
    "    predictions = [[id2label[l + 1]] for p in predictions for l in p]\n",
    "    train_loss = train_loss / len(train_batch) * opt['batch_size']  # avg loss per batch\n",
    "    train_sent_loss = train_sent_loss / len(train_batch) * opt['batch_size']  # avg loss per batch\n",
    "    train_dep_path_loss = train_dep_path_loss / len(train_batch) * opt['batch_size']  # avg loss per batch\n",
    "    dev_loss = dev_loss / len(dev_batch) * opt['batch_size']\n",
    "\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions, method='macro')\n",
    "    print(\n",
    "        f\"epoch {epoch}: train_loss = {train_loss:.6f}, \"\n",
    "        f\"train_sent_loss = {train_sent_loss:.6f}, \"\n",
    "        f\"train_dep_path_loss = {train_dep_path_loss:.6f}, \"\n",
    "        f\"dev_loss = {dev_loss:.6f}, dev_f1 = {dev_f1:.4f}\"\n",
    "    )\n",
    "    dev_score = dev_f1\n",
    "\n",
    "    # save\n",
    "    model_file = model_save_dir + f'/checkpoint_epoch_{epoch}.pt'\n",
    "    trainer.save(model_file, epoch)\n",
    "    if (epoch == 1 or dev_score > max(dev_score_history)) and epoch % opt['save_epoch'] == 0:\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "\n",
    "        print(f\"new best model saved at epoch {epoch}: {dev_p * 100:.2f}\\t{dev_r * 100:.2f}\\t{dev_score * 100:.2f}\")\n",
    "\n",
    "    # lr schedule\n",
    "    if (\n",
    "            len(dev_score_history) > opt['decay_epoch'] and\n",
    "            dev_score <= dev_score_history[-1] and\n",
    "            opt['optim'] in ['sgd', 'adagrad', 'adadelta']\n",
    "    ):\n",
    "        current_lr *= opt['lr_decay']\n",
    "\n",
    "    trainer.update_lr(current_lr)\n",
    "    dev_score_history += [dev_score]\n",
    "\n",
    "    print(\"Training ended with {} epochs.\".format(epoch))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-11T05:59:28.790360Z"
    }
   },
   "id": "f412c354f4cc0d2f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2399f1c65794723c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
