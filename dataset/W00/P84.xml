<?xml version="1.0" encoding="UTF-8"?>
	<volume id="P84">

		<paper id="1103">
			<definition id="0">
				<sentence>Evidently , subjects had resolved the ambiguity in ( 1 ) before receiving the last word , and they chose the resolution fitting ( la ) , in which `` examined '' is a main-clause pasttense verb , rather than the resolution fitting ( Ib ) , in which it is a past participle of a reduced relative clause .</sentence>
				<definiendum id="0">resolution fitting</definiendum>
				<definiens id="0">a past participle of a reduced relative clause</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>By providing separate representation schemes for linguistic knowledge , general world knowledge , and application domain knowledge , DATALOG achieves a high degree of portability and extendability .</sentence>
				<definiendum id="0">DATALOG</definiendum>
				<definiens id="0">linguistic knowledge , general world knowledge , and application domain knowledge ,</definiens>
				<definiens id="1">achieves a high degree of portability and extendability</definiens>
			</definition>
			<definition id="1">
				<sentence>If no items satisfy the user 's request , DATALOG gives an informative response explaining what part of the query could not be satisfied .</sentence>
				<definiendum id="0">DATALOG</definiendum>
				<definiens id="0">gives an informative response explaining what part of the query could not be satisfied</definiens>
			</definition>
			<definition id="2">
				<sentence>In DATALOG , ASSIGN is a function of three arguments : the BEAD of the current clause or phrase , the CONSTITUENT which is being added to the interpretation of the phrase , and the SYNTACTIC SLOT which the constituent occupies .</sentence>
				<definiendum id="0">ASSIGN</definiendum>
				<definiendum id="1">SYNTACTIC SLOT</definiendum>
				<definiens id="0">a function of three arguments : the BEAD of the current clause or phrase , the CONSTITUENT which is being added to the interpretation of the phrase</definiens>
			</definition>
			<definition id="3">
				<sentence>This convention ensures the generality of the grammar ; although the linguistic component ( through the assignment mechanism ) controls the information that is passed to the semantic interpreter , the only information that flows back to the grazm~ar is CONSTITUENT SYNTACTIC SLOT employee NPHEAD ( AMOD female ) NPPREMOD ( ADJp SUPER ( ADV most ) ( ADJ tall ) ) ( the ) DET ( PREMODS ( ( ADJP ( ADV most ) ( ADJ tall ) ) ( AMOD female ) ) ( HEAD employee ) ( SEMANTICS ( ENTITY ( Q nil ) ( KIND employee ) ( RESTRICTIONS ( ( ( ATT sex ) ( RELOP ISA ) ( VALUE female ) ) ( ( ATT height ) ( RANKOP MOST ) ( CUTOFF i ) ) ) ) ) ) ) Figure 3 .</sentence>
				<definiendum id="0">DET ( PREMODS ( ( ADJP</definiendum>
				<definiens id="0">Q nil ) ( KIND employee ) ( RESTRICTIONS ( ( ( ATT sex ) ( RELOP ISA ) ( VALUE female ) ) ( ( ATT height ) ( RANKOP MOST )</definiens>
			</definition>
			<definition id="4">
				<sentence>When the grammar builds a constituent structure for a phrase or clause , it includes an extra constituent called `` SEMANTICS '' , which it takes from a semantic register .</sentence>
				<definiendum id="0">SEMANTICS ''</definiendum>
				<definiens id="0">it takes from a semantic register</definiens>
			</definition>
</paper>

		<paper id="1109">
			<definition id="0">
				<sentence>He noted that a ~ood ~ , where X is a role nominal , means : a. if X is an agent : one who performs the associated activity skilfully ( s good driver , a good pianist ) ; b. if X is an instrument : a thing which permits the associated activity to be performed easily ( a good knife , a good broom ) ; c° In other cases , it seems that the resultant meaning is less predictible ( good food has certain properties concerning nutritiousness and taste ; a good house is comfortable , built to last , etc. ) .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">an instrument : a thing which permits the associated activity to be performed easily</definiens>
			</definition>
</paper>

		<paper id="1080">
</paper>

		<paper id="1092">
			<definition id="0">
				<sentence>ABSTRACT Computational neurolinguistics ( CN ) is an approach to computational linguistics which includes neurally-motivated constraints in the design of models of natural language processing .</sentence>
				<definiendum id="0">ABSTRACT Computational neurolinguistics</definiendum>
				<definiendum id="1">CN</definiendum>
				<definiens id="0">an approach to computational linguistics which includes neurally-motivated constraints in the design of models of natural language processing</definiens>
			</definition>
			<definition id="1">
				<sentence>I. INTRODUCTION Computational Neurolinguistics ( CN ) incorporates initial assumptions about language processing that are often indirectly referenced in other computational approaches to language study .</sentence>
				<definiendum id="0">I. INTRODUCTION Computational Neurolinguistics ( CN )</definiendum>
				<definiens id="0">incorporates initial assumptions about language processing that are often indirectly referenced in other computational approaches to language study</definiens>
			</definition>
			<definition id="2">
				<sentence>INTERPRETATION is a result of activation of a pragmatic representation of a disambiguated word meaning .</sentence>
				<definiendum id="0">INTERPRETATION</definiendum>
				<definiens id="0">a result of activation of a pragmatic representation of a disambiguated word meaning</definiens>
			</definition>
			<definition id="3">
				<sentence>The CN paradigm for natural language processing includes claims that new perspectives on linguistically interpretable hierarchical representations that arise in language behavior are introduced by including neurally motivated processing control as the focus of model definition and by including behaviorially defined constraints , both normal and pathological .</sentence>
				<definiendum id="0">CN paradigm</definiendum>
				<definiens id="0">includes claims that new perspectives on linguistically interpretable hierarchical representations that arise in language behavior are introduced by including neurally motivated processing control as the focus of model definition and by including behaviorially defined constraints</definiens>
			</definition>
			<definition id="4">
				<sentence>McClelland , J.L. and Rumelhart , D.E. , An Interactive Activation Model of Context Effects in Letter Perception : Part I. An Account of Basic Findings .</sentence>
				<definiendum id="0">McClelland , J.L.</definiendum>
				<definiens id="0">An Interactive Activation Model of Context Effects in Letter Perception : Part I. An Account of Basic Findings</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>II DISTRIBUTED MEMORY MACHINES Distributed memory machines ( DMM ) can be represented formally by the septuple DMM= ( V , X , Y , Q , qo , p , A ) , where V is a finite set denoting the total vocabulary ; X is a finite set of inputs , and XGV ; Y is a finite set of acceptable outputs and Y~V ; Q is a set of internal states ; q0 is a distinguished initial state ; ~ .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">XGV ; Y</definiendum>
				<definiendum id="2">Q</definiendum>
				<definiendum id="3">q0</definiendum>
				<definiens id="0">DISTRIBUTED MEMORY MACHINES Distributed memory machines ( DMM ) can be represented formally by the septuple DMM= ( V , X , Y , Q , qo , p , A ) , where V is a finite set denoting the total vocabulary ;</definiens>
				<definiens id="1">a finite set of inputs , and</definiens>
				<definiens id="2">a finite set of acceptable outputs and Y~V</definiens>
				<definiens id="3">a set of internal states ;</definiens>
				<definiens id="4">a distinguished initial state</definiens>
			</definition>
			<definition id="1">
				<sentence>Further , where Y '' denotes the set of all finite concatenations of the elements of the set Y , Q~Y ' , and therefore QgV ' .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">the set of all finite concatenations of the elements of the set Y , Q~Y ' , and therefore QgV '</definiens>
			</definition>
			<definition id="2">
				<sentence>Convolving an item wlth an attenuated delta vector ( i.e. , a vector with values of zero on all features except the central one , which has a value between 0 and i ) produces the original item with a strength that is equal to the value of the central feature of the attenuated delta vector .</sentence>
				<definiendum id="0">Convolving an item wlth an attenuated delta vector</definiendum>
				<definiens id="0">has a value between 0 and i ) produces the original item with a strength that is equal to the value of the central feature of the attenuated delta vector</definiens>
			</definition>
			<definition id="3">
				<sentence>III BUILDING NATURAL LANCUACZ PARSERS A. Case-Frame Parsing The computational properties of distributed memory machines ( DMM ) make them natural mechanisms for case-frame parsing .</sentence>
				<definiendum id="0">DMM</definiendum>
				<definiens id="0">computational properties of distributed memory machines</definiens>
			</definition>
			<definition id="4">
				<sentence>Consider a DMM which encodes case-frame structures of the following form : &lt; Pred &gt; * ( &lt; Cl &gt; * &lt; Pl &gt; + &lt; C2 &gt; * &lt; P2 &gt; + ... + &lt; Cn &gt; * &lt; Pn &gt; ) where &lt; Pred &gt; is the vector representing the predicate associated with the verb of an input clause ; &lt; C1 &gt; to &lt; Cn &gt; are the case vectors such as &lt; agent &gt; , &lt; instrument &gt; , etc. , and &lt; PI &gt; to &lt; Pn &gt; are vectors representing prototype concepts which can fill the associated cases .</sentence>
				<definiendum id="0">DMM</definiendum>
				<definiens id="0">encodes case-frame structures of the following form : &lt; Pred &gt; *</definiens>
			</definition>
			<definition id="5">
				<sentence>As an example , the grammatical relations for the sentence John handed Mary a book are encoded in the f-structure below : SUBJ NUM RED 'JO PAST 'HAND\ [ ( SUBJ ) ( OSJ2 ) ( OBJ ) \ ] TENSE PRED OBJ \ [ ~UM MARY 3 SG RED `` OBJ2 \ [ ~C ASG K~ '' \ [ , PRED `` BOO The lists of grammatical functions and features are encoded as single vectors under the + operator , and the embedded structure is preserved by the associative operator , * .</sentence>
				<definiendum id="0">SUBJ )</definiendum>
				<definiens id="0">single vectors under the + operator</definiens>
			</definition>
			<definition id="6">
				<sentence>The computational architecture consists of a large number of appropriately connected computing units communicating through weighted levels of excitation and inhibition .</sentence>
				<definiendum id="0">computational architecture</definiendum>
				<definiens id="0">consists of a large number of appropriately connected computing units communicating through weighted levels of excitation and inhibition</definiens>
			</definition>
</paper>

		<paper id="1020">
</paper>

		<paper id="1098">
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>Reduced Conjunction is the case where the conjoined surface strings are not well-formed constituents as in ( 18 ) John drove his car through and completely demolished a plate glass window .</sentence>
				<definiendum id="0">Reduced Conjunction</definiendum>
				<definiens id="0">the case where the conjoined surface strings</definiens>
			</definition>
			<definition id="1">
				<sentence>The right conjunct is a verb phrase to be treated as a clause with the subject deleted : ( 31 ) The man kicked the child and threw the ball .</sentence>
				<definiendum id="0">right conjunct</definiendum>
				<definiens id="0">a verb phrase to be treated as a clause with the subject deleted</definiens>
			</definition>
			<definition id="2">
				<sentence>co~junctlon ( S , Stn , Sub j , HeadNoun , Verbl , V_Type , Contentverb , Tensel , Obj , ObjHeadNoun ) • rest sentence I ( Type , Sub j , Head_Noun , Verbl , VType , ~ontentver5 , Tense , Prep ObJ , Prep ObJHead Noun , P_Obj , P ObJ Head Noun , Indobj , s ( type ( Type ) , tense ( Tense ) , v ( Verb sense , agent ( Subj ) , object ( Obj ) , pos t -- ve rb_ mods ( prep ( Prep ) , pre~obj ( Prep_Obj ) ) ) Y -- &gt; % Here Prep ObJ is a logical variable which % will be Instantlated later when the % right conjunct has been parsed .</sentence>
				<definiendum id="0">P_Obj</definiendum>
				<definiendum id="1">prep ( Prep ) , pre~obj ( Prep_Obj</definiendum>
				<definiendum id="2">Prep ObJ</definiendum>
				<definiens id="0">• rest sentence I ( Type , Sub j , Head_Noun , Verbl , VType , ~ontentver5 , Tense , Prep ObJ , Prep ObJHead Noun ,</definiens>
			</definition>
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>ABSTRACT Informally , a disposition is a proposition which is preponderantly , but no necessarily always , true .</sentence>
				<definiendum id="0">ABSTRACT Informally</definiendum>
			</definition>
			<definition id="1">
				<sentence>It should be stressed , however , that restoration ( or ezplicitation ) -viewed as the inverse of suppression is an interpretation-dependent process in the sense that , in general , a disposition may be interpreted in different ways depending on the manner in which the fuzzy quantifiers are restored and defined .</sentence>
				<definiendum id="0">suppression</definiendum>
				<definiens id="0">an interpretation-dependent process in the sense that , in general , a disposition may be interpreted in different ways depending on the manner in which the fuzzy quantifiers are restored and defined</definiens>
			</definition>
			<definition id="2">
				<sentence>312 Q1A ' s ore Bt s ( 1.4 ) Q : BI s are CI s &gt; _ ( QI ~ Q2 ) A # s are C 's in which Q1 ~ Q2 represents the product of the fuzzy numbers QI and Q2 ( Figure 1 ) .</sentence>
				<definiendum id="0">Q2</definiendum>
				<definiens id="0">s ( 1.4 ) Q : BI s are CI s &gt; _ ( QI ~ Q2 ) A # s are C 's in which Q1 ~</definiens>
			</definition>
			<definition id="3">
				<sentence>First , the suppressed fuzzy quantifiers in d are restored , resulting in a fuzzily quantified proposition p. Then , the meaning of p is represented -through the use of test-score semantics ( Zadeh , 1978 , 1982 ) as a procedure which acts on a collection of relations in an explanatory database and returns a test score which represents the degree of compatibility of p with the database .</sentence>
				<definiendum id="0">test-score semantics</definiendum>
			</definition>
			<definition id="4">
				<sentence>The relative sigma-count , denoted by ~ Count ( B / A ) , may be interpreted as the proportion of elements of B in A. More explicitly , ~Count ( B/A ) -- ~ ~Count ( A fl B ) ( 2.2 ) ECount ( a ) ' where B D A , the intersection of B and A , is defined by 313 itBnA ( U ) fUS/U ) ^ US ( U ) , U e U , where A denotes the sin operator in infix form .</sentence>
				<definiendum id="0">relative sigma-count</definiendum>
				<definiendum id="1">Count ( B / A )</definiendum>
				<definiens id="0">the proportion of elements of B in A. More explicitly , ~Count ( B/A ) -- ~ ~Count ( A fl B ) ( 2.2 ) ECount ( a ) ' where B D A , the intersection of B and A , is defined by 313 itBnA ( U ) fUS/U ) ^ US ( U ) , U e U , where A denotes the sin operator in infix form</definiens>
			</definition>
			<definition id="5">
				<sentence>The relation POPULA TION is a list of names of individuals , with the variables Overeat and Obese representing , respectively , the degrees to which Name overeats and is obese .</sentence>
				<definiendum id="0">relation POPULA TION</definiendum>
				<definiens id="0">a list of names of individuals , with the variables Overeat and Obese representing , respectively , the degrees to which Name overeats and is obese</definiens>
			</definition>
			<definition id="6">
				<sentence>In MOST , p is the degree to which a numerical value of Proportion fits the intended meaning of MOST .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the degree to which a numerical value of Proportion fits the intended meaning of MOST</definiens>
			</definition>
			<definition id="7">
				<sentence>This test score represents the compatibility of p with the explanatory database .</sentence>
				<definiendum id="0">test score</definiendum>
				<definiens id="0">the compatibility of p with the explanatory database</definiens>
			</definition>
			<definition id="8">
				<sentence>In more explicit terms , PI and P2 may be expressed as PI A P , \ [ Name ; p\ ] ( 3.2 ) P2 ~P2\ [ Name ; p\ ] , in which Name is the name of a male person and # is the degree to which the person in question satisfies the predicate .</sentence>
				<definiendum id="0">PI</definiendum>
				<definiens id="0">PI A P , \ [ Name ; p\ ] ( 3.2 ) P2 ~P2\ [ Name ; p\ ] , in which Name is the name of a male person and # is the degree to which the person in question satisfies the predicate</definiens>
			</definition>
			<definition id="9">
				<sentence>\ [ Equivalently , p is the grade of membership of the person in the fuzzy set which represents the denotation or , equivalently , the extension of the predicate . )</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the grade of membership of the person in the fuzzy set which represents the denotation or , equivalently , the extension of the predicate</definiens>
			</definition>
			<definition id="10">
				<sentence>M.POPULATION , and the population of females , F.POPULA TION : M.POPULA TION A N ... . Ag , POPULA TION\ [ Sez -- -Male\ ] F.POPULA TON A Ne , , , ,age POPULA TION\ [ Sez -- -Female\ ] , where N~mc , AocPOPULATION denotes the projection of POPULATION on the attributes Name and Age .</sentence>
				<definiendum id="0">F.POPULA TION</definiendum>
				<definiendum id="1">N~mc , AocPOPULATION</definiendum>
				<definiens id="0">the projection of POPULATION on the attributes Name and Age</definiens>
			</definition>
			<definition id="11">
				<sentence>If d is interpreted as rd2 , which is a more likely interpretation , then the procedure is unchanged except that r i in ( 3.5 ) should he replaced by r i = ~MOST\ [ Proportion -~6i\ ] where 6 , A ~Count ( YW/WL , ) Concepts The approach described in the preceding sections can be applied not only to the representation of the meaning of dispositions and dispositional predicates , but , more generally , to various types of semantic entities as well as dispositional concepts .</sentence>
				<definiendum id="0">~Count</definiendum>
				<definiens id="0">the representation of the meaning of dispositions and dispositional predicates , but</definiens>
			</definition>
			<definition id="12">
				<sentence>The relation RECORD may be interpreted as a diary -kept during the period of interest -in which Name is the name of a man ; pBald is the degree to which he is bald ; and Action describes whether the man in question was stayed away from ( Action~l ) or not ( Action=0 ) .</sentence>
				<definiendum id="0">pBald</definiendum>
				<definiendum id="1">Action</definiendum>
				<definiens id="0">a diary -kept during the period of interest -in which Name is the name of a man ;</definiens>
				<definiens id="1">the degree to which he is bald ; and</definiens>
			</definition>
			<definition id="13">
				<sentence>The test procedure which defines the meaning of dc may be described as follows : which Namel is bald ; and ( b ) the action taken : # Baldi A , B~IdRECORD\ [ Name -- .</sentence>
				<definiendum id="0">test procedure</definiendum>
				<definiens id="0">defines the meaning of dc may be described as follows : which Namel is bald</definiens>
			</definition>
			<definition id="14">
				<sentence>J ' ( 4.8 ) where tHIGH , PMOSr , PS and PA are the membership functions of HIGH , MOST , S and A , respectively , and the summation Zu extends over the elements of U. It is of interest to observe that if pa ( t ) -- -- 1 and .</sentence>
				<definiendum id="0">tHIGH</definiendum>
				<definiendum id="1">PA</definiendum>
				<definiens id="0">the membership functions of HIGH , MOST , S and A , respectively , and the summation Zu extends over the elements of U. It is of interest to observe that if pa ( t ) -- -- 1 and</definiens>
			</definition>
			<definition id="15">
				<sentence>As a general inference schema , a fuzzy syllogism may be expressed in the form QIA'a are Bin ( 5.1 ) Q2 CI8 are DIs fQs E ' a are F~ a where Ql and Q2 are given fuzzy quantifiers , Q3 is fuzzy quantifier which is to be determined , and A , /3 , C , D , E and F are interrelated fuzzy predicates .</sentence>
				<definiendum id="0">Q3</definiendum>
				<definiens id="0">F~ a where Ql and Q2 are given fuzzy quantifiers</definiens>
				<definiens id="1">fuzzy quantifier which is to be determined , and A , /3 , C , D , E and F are interrelated fuzzy predicates</definiens>
			</definition>
			<definition id="16">
				<sentence>( 6.2 ) Now , using the semantic equivalence established in Zadeh ( 1978 ) , we may write not ( Q A 's are B 's ) E ( not Q ) A 's ore B'o , ( 6.3 ) where not Q is the complement of the fuzzy quantifier Q in the sense that the membership function of not Q is given by P , ,ot Q ( u ) .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiens id="0">A 's are B 's ) E ( not Q ) A 's ore B'o , ( 6.3 ) where not</definiens>
				<definiens id="1">the complement of the fuzzy quantifier Q in the sense that the membership function of not Q is given by P</definiens>
			</definition>
			<definition id="17">
				<sentence>( 6.4 ) Furthermore , the following inference rule can readily be established ( gadeh , 1983a ) : Q A ' s ore B ' s ( 0.5 ) ~__ ( ant Q ) A I s arc not B t o ' where ant Q denotes the antonym of Q , defined by ~ , , , ~ ( u ) = ~q ( 1-n ) , o &lt; u &lt; 1 , ( 6 .</sentence>
				<definiendum id="0">Furthermore</definiendum>
				<definiendum id="1">Q</definiendum>
				<definiens id="0">established ( gadeh , 1983a ) : Q A ' s ore B ' s ( 0.5 ) ~__ ( ant Q ) A I s arc not B t o ' where ant</definiens>
				<definiens id="1">the antonym of Q , defined by ~ , , , ~ ( u ) = ~q ( 1-n</definiens>
			</definition>
</paper>

		<paper id="1001">
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>The IAA characterizes utterances who~e sole p.rpose is to secure referent identification , and the PAA characterizes the use of referring phrases within an illocutionary act .</sentence>
				<definiendum id="0">PAA</definiendum>
				<definiens id="0">characterizes the use of referring phrases within an illocutionary act</definiens>
			</definition>
			<definition id="1">
				<sentence>n , for a. propositional act can only occur as part of s , uuc illocutionary act , never simply by itself ( Ibid , p. 15 .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">part of s , uuc illocutionary act</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>If we require that such systems be k-reversible , as defined by Angluin ( in press ) , then art efficient polynomial time induction algorithm exists .</sentence>
				<definiendum id="0">Angluin (</definiendum>
				<definiens id="0">in press ) , then art efficient polynomial time induction algorithm exists</definiens>
			</definition>
			<definition id="1">
				<sentence>BI ) E is a property related to the `` separability '' of langu : tges and grammars given simple data : if there is a way for the learner to tell that a currently hypnthesized language { and grammar ) is incorrect , then there must be some simple scntc'~ce that reveals this -all languages in the family must be separable b ' , ' simple sentences .</sentence>
				<definiendum id="0">BI ) E</definiendum>
				<definiendum id="1">grammar</definiendum>
				<definiens id="0">a property related to the `` separability '' of langu : tges and grammars given simple data : if there is a way for the learner to tell that a currently hypnthesized language { and</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>The set ( 1 ) : ( I ) { a white male ( whom a white male ) n ( hired ) n hired another white male I n ~ 0 } is the intersection of English with the regular set &amp; whi ; e male ( .</sentence>
				<definiendum id="0">; e male</definiendum>
				<definiens id="0">the intersection of English with the regular set &amp; whi</definiens>
			</definition>
			<definition id="1">
				<sentence>In Moru , we find examples such as this ( slightly simplified from Ha~ege ( 1976 , 200 ) ; Xi is the possession marker for nonhuman nouns , and ro is the equivalent for human nouns ) : ( 2 ) kokyE \ [ toko \ [ odrupi \ [ ma ro\ ] ro\ ] ri\ ] drate 1 2 3 3 2 1 dog wife brother me of of of is-dead `` My brother 's chief wife 's black dog is dead . ''</sentence>
				<definiendum id="0">Xi</definiendum>
				<definiendum id="1">ro</definiendum>
				<definiens id="0">the possession marker for nonhuman nouns , and</definiens>
			</definition>
			<definition id="2">
				<sentence>Manaster-Ramer ( 1983 ) points to the contemptuous reduplication pattern of Yiddish-influenced English , and suggests that it instantiates an infinite string matching language .</sentence>
				<definiendum id="0">Manaster-Ramer ( 1983 )</definiendum>
				<definiens id="0">points to the contemptuous reduplication pattern of Yiddish-influenced English , and suggests that it instantiates an infinite string matching language</definiens>
			</definition>
			<definition id="3">
				<sentence>Consider the possibility of a `` respectively '' -sentence with a meaning like '~he NI , N2 , and N3 are respectively AI , A2 , and A3 , '' where NI , N2 , and N3 have different genders end AI , A2 , and A3 are required to agree with their corresponding nouns in gender .</sentence>
				<definiendum id="0">A3</definiendum>
				<definiens id="0">required to agree with their corresponding nouns in gender</definiens>
			</definition>
			<definition id="4">
				<sentence>L I L J R 1 I I C I C I I I L L R L R L R R RR1 \ [ RR2 I ... .. I ... .. I I I S I R I I I R R 1 2 119 ( 7 ) Syntax Lexicon S -- - &gt; LCI ( M , A ) S -- - &gt; LCI ( N , B ) M -- - &gt; LL2 ( X , O ) N -- - &gt; LL2 ( Y , O ) X -- - &gt; LL2 ( Z , A ) Y -- - &gt; LL2 ( Z , B ) Z -- - &gt; LCI ( X , A ) Z -- - &gt; LCI ( Y , B ) A -- - &gt; a B -- - &gt; b 0 -- - &gt; o Z -- - &gt; e The structure this gr -- - , -r assigns to the string ba-o-ba is shown in figure 2 in the form of a tree with crossing branches , using asterisks to indicate heads ( or strictly , nodes through which the path from a label to the head of its terminal string ) asses ) .</sentence>
				<definiendum id="0">LCI</definiendum>
				<definiens id="0">R L R L R R RR1 \ [ RR2 I ... .. I ... .. I I I S I R I I I R R 1 2 119 ( 7 ) Syntax Lexicon S -- - &gt;</definiens>
				<definiens id="1">nodes through which the path from a label to the head of its terminal string ) asses )</definiens>
			</definition>
</paper>

		<paper id="1075">
			<definition id="0">
				<sentence>PATR-II grammars consist of rules with a context-free phrase structure portion and a set of unifications on the DAGs associated with the constituents that participate in the application of the rule .</sentence>
				<definiendum id="0">PATR-II grammars</definiendum>
				<definiens id="0">consist of rules with a context-free phrase structure portion and a set of unifications on the DAGs associated with the constituents that participate in the application of the rule</definiens>
			</definition>
			<definition id="1">
				<sentence>As s*l , 'h , PATR-II is a tool , not a result .</sentence>
				<definiendum id="0">PATR-II</definiendum>
				<definiens id="0">a tool</definiens>
			</definition>
			<definition id="2">
				<sentence>For instance , given a PATR-II grammar in which the DAGs are used to emulate the f-structures of LFG , we might write a passive lexical rule as follows ( following Bresnan \ [ 83\ ] ) : e Define Passive as &lt; out cat &gt; = &lt; in cat &gt; &lt; out form &gt; = passprt &lt; out subj &gt; = &lt; in obj &gt; &lt; out obj &gt; = &lt; in subj &gt; The rule states in effect that the output DAG ( the one associated with the passive verb form ) marks the lexical item as being a passive verb whose object is the input DAG 's subject and whose subject is the input 's object .</sentence>
				<definiendum id="0">DAG</definiendum>
			</definition>
			<definition id="3">
				<sentence>• A ZETALISP version for the Symbolics 3600 using a left-corner parsing algorithm and the KIMMO morphological analyzer , with an extensive programming environment { due primarily to Mabry Tyson } that includes incremental compilation , multiple window debugging facilities , tracing , and an integrated editor .</sentence>
				<definiendum id="0">KIMMO morphological analyzer</definiendum>
				<definiens id="0">includes incremental compilation</definiens>
			</definition>
</paper>

		<paper id="1077">
			<definition id="0">
				<sentence>Kafka is a rule based English sentence generator used in the XCALIBUR natural language interface .</sentence>
				<definiendum id="0">Kafka</definiendum>
				<definiens id="0">a rule based English sentence generator used in the XCALIBUR natural language interface</definiens>
			</definition>
			<definition id="1">
				<sentence>Kafka includes confirmational information in the generated text , providing sufficient redundancy for the user to ascertain whether his query/command was correctly understood .</sentence>
				<definiendum id="0">Kafka</definiendum>
				<definiens id="0">includes confirmational information in the generated text , providing sufficient redundancy for the user to ascertain whether his query/command was correctly understood</definiens>
			</definition>
			<definition id="2">
				<sentence>XCALIBUR supports mixed-initiative dialogs which allow the user to issue commands , request data , and answer system queries in any desired order .</sentence>
				<definiendum id="0">XCALIBUR</definiendum>
				<definiens id="0">supports mixed-initiative dialogs which allow the user to issue commands , request data , and answer system queries in any desired order</definiens>
			</definition>
			<definition id="3">
				<sentence>der ) ) ) ) ( level ( *main ) ) ( verb ( °conjugation ( root ( be ) ) ( mode ( *interrogative ) ) ( tense ( *present ) ) ( number ( *singular ) ) ) ) ) Figure 3-1 : A Sample Case-frame Kafka is used to build replies to user queries , to paraphrase the user 's input for clarificational dialogs , and to generate the system 's queries for the user .</sentence>
				<definiendum id="0">Sample Case-frame Kafka</definiendum>
				<definiens id="0">used to build replies to user queries , to paraphrase the user 's input for clarificational dialogs , and to generate the system 's queries for the user</definiens>
			</definition>
			<definition id="4">
				<sentence>query I 1 HI H°°i -- n '' English Figure 4-1 : Data flow in the Kafka Generator Kafka is a direct descendant of an earlier natural language generator described in \ [ 2\ ] , which in turn had many components either derived from or inspired by Goldman 's BABEL generator \ [ 7\ ] .</sentence>
				<definiendum id="0">Kafka Generator Kafka</definiendum>
				<definiens id="0">a direct descendant of an earlier natural language generator described in \</definiens>
			</definition>
			<definition id="5">
				<sentence>The earlier XCALIBUR generator was very much ad hoc , and Kafka is an effort to formalize the processes used in that generator .</sentence>
				<definiendum id="0">Kafka</definiendum>
				<definiens id="0">an effort to formalize the processes used in that generator</definiens>
			</definition>
			<definition id="6">
				<sentence>Kafka has minimal capability to generate anaphora .</sentence>
				<definiendum id="0">Kafka</definiendum>
				<definiens id="0">has minimal capability to generate anaphora</definiens>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>INTRODUCTION Natural Language and Databases has been a common panel topic for some years , partly because it has been an active area of work , but more importantly , because it has been widely assumed that database access is a good test environment for language research .</sentence>
				<definiendum id="0">Databases</definiendum>
			</definition>
</paper>

		<paper id="1041">
</paper>

		<paper id="1087">
			<definition id="0">
				<sentence>Meaningless nonwords must have some surface property such as their shape mapped onto their actions .</sentence>
				<definiendum id="0">Meaningless nonwords</definiendum>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>In Figure 2 , PE 's are shown by using the `` segmenting marker ( T ) '' , such as TWith some help ( ADVL ) \ [ , \ [ from overseas ( ADJV ) \ [ j T , ( co~ ) T , Tthe Japanese ( SUBJ ) T and Tare beginning ( GOV ) T , where the terminologies in parentheses are the syntactic roles which will be discussed later .</sentence>
				<definiendum id="0">ADVL</definiendum>
				<definiens id="0">SUBJ ) T and Tare beginning ( GOV ) T , where the terminologies in parentheses are the syntactic roles which will be discussed later</definiens>
			</definition>
			<definition id="1">
				<sentence>A `` clausal element ( CE ) '' is composed of one or more PE ( 's ) which carries a part of sentential meaning in a nexus-like form .</sentence>
				<definiendum id="0">clausal element ( CE</definiendum>
				<definiens id="0">carries a part of sentential meaning in a nexus-like form</definiens>
			</definition>
			<definition id="2">
				<sentence>An example of top-down operation would be the segmentation of an input sentence ( i.e. the sequence of word elements ( WE 's ) ) to get phrasal elements ( PE ) , and an example of bottom-up operation would be the construction of tree-forms or link-forms to get clausal elements ( CE ) or a sentence ( SE ) .</sentence>
				<definiendum id="0">CE</definiendum>
				<definiens id="0">the sequence of word elements ( WE 's ) ) to get phrasal elements</definiens>
			</definition>
			<definition id="3">
				<sentence>( 2 ) Therefore we have decided to adopt the conceptual dependency diagram ( CDD ) as a compact and powerful semantics directed internal representation .</sentence>
				<definiendum id="0">CDD</definiendum>
				<definiens id="0">a compact and powerful semantics directed internal representation</definiens>
			</definition>
			<definition id="4">
				<sentence>( g ) Each NPN is to be labeled with some properly selected semantic features which are under the control of a thesaurus type lexicon .</sentence>
				<definiendum id="0">NPN</definiendum>
				<definiens id="0">to be labeled with some properly selected semantic features which are under the control of a thesaurus type lexicon</definiens>
			</definition>
			<definition id="5">
				<sentence>Semantics plays an assistant role in disambiguating the dependency among phrases .</sentence>
				<definiendum id="0">Semantics</definiendum>
				<definiens id="0">plays an assistant role in disambiguating the dependency among phrases</definiens>
			</definition>
			<definition id="6">
				<sentence>\ [ 9\ ] Robinson , J.J. , Case , Category and Configuration , Journal of Linguistics , vol.6 no.l ( 1970 ) 57-80 \ [ I0\ ] Robinson , J.J. , Dependency Structures and Transformational Rules , Language , voi.46 , no.2 ( 1970 ) 259-285 \ [ ii\ ] Robinson , J.J. , DIAGRAM : A Grammar for Dialogues , Co=~m. ACM voi.25 , no.l ( 1982 ) 27-47 .</sentence>
				<definiendum id="0">DIAGRAM</definiendum>
				<definiens id="0">A Grammar for Dialogues</definiens>
			</definition>
</paper>

		<paper id="1073">
			<definition id="0">
				<sentence>R parser produces all possible parse trees witftoul parsing any part of the input sentenc : e more than once in the same way , despite the fact that the parser does not maintain a chart as in chart par~ing .</sentence>
				<definiendum id="0">R parser</definiendum>
				<definiens id="0">produces all possible parse trees witftoul parsing any part of the input sentenc : e more than once in the same way</definiens>
			</definition>
</paper>

		<paper id="1099">
			<definition id="0">
				<sentence>A translation pair is a pair of texts ( T~ , T~ ) from the source and target language , respectively .</sentence>
				<definiendum id="0">translation pair</definiendum>
				<definiens id="0">a pair of texts ( T~ , T~ ) from the source and target language , respectively</definiens>
			</definition>
			<definition id="1">
				<sentence>Isoduidy is an equivalence relation between representations that belong to the same language .</sentence>
				<definiendum id="0">Isoduidy</definiendum>
				<definiens id="0">an equivalence relation between representations that belong to the same language</definiens>
			</definition>
			<definition id="2">
				<sentence>465 Given isoduidy , one can give a more sophisticated version of the principle of division of labour as follows : ( 7 ) Division of labour ( final version ) : For each language L in the system , R ' , T7 ~ GEN L iff KT , R7 6AN L and R ' is isoduid to R As a consequence , TRF has not to take responsibility for target language specific aspects like word order anymore .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">final version ) : For each language L in the system</definiens>
			</definition>
			<definition id="3">
				<sentence>Clearl~ , TRF is a subset of translates-as .</sentence>
				<definiendum id="0">TRF</definiendum>
				<definiens id="0">a subset of translates-as</definiens>
			</definition>
</paper>

		<paper id="1100">
			<definition id="0">
				<sentence>ABSTRACT Our MT systems integrate many advanced concepts from the fields of computer science , linguistics , and AI : specialized languages for linguistic programming based on production systems , complete linguistic programming environment , multilevel representations , organization of the lexicons around `` lexical units '' , units of translation of the size of several paragraphs , possibility of using text-driven heuristic strategies .</sentence>
				<definiendum id="0">AI</definiendum>
				<definiens id="0">many advanced concepts from the fields of computer science , linguistics , and</definiens>
			</definition>
			<definition id="1">
				<sentence>468 II DIRECTIONS OF CURRENT WORK I Linguistic knowledge processing The experience gained by the development of a Russian-French translation unit of a realistic size over the last three years ( 6 ) has shown that maintaining and upgrading the lingware , even in an admittedly limited second generation CAT system , requires a good deal of expertise .</sentence>
				<definiendum id="0">II DIRECTIONS OF CURRENT WORK I Linguistic knowledge</definiendum>
				<definiens id="0">processing The experience gained by the development of a Russian-French translation unit of a realistic size over the last three years ( 6 ) has shown that maintaining and upgrading the lingware</definiens>
			</definition>
</paper>

		<paper id="1097">
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>A KL-ONE syntaxonomy allows straightforward statement of common properties , as well as individually distinct properties of group members .</sentence>
				<definiendum id="0">KL-ONE syntaxonomy</definiendum>
				<definiens id="0">allows straightforward statement of common properties , as well as individually distinct properties of group members</definiens>
			</definition>
			<definition id="1">
				<sentence>Indirect Object ) then ( Paraphrase-as addressee ) ~slation Rule : ( Paraphrase .</sentence>
				<definiendum id="0">Object</definiendum>
			</definition>
			<definition id="2">
				<sentence>I. Brief Description of KL-ONE KL-ONE offers a rigorous means of specifying terms ( concepts ) and basic relationships among them , such as subset/superset , disjointness , exhaustive cover , and relational structure .</sentence>
				<definiendum id="0">KL-ONE KL-ONE</definiendum>
				<definiens id="0">offers a rigorous means of specifying terms ( concepts ) and basic relationships among them , such as subset/superset , disjointness , exhaustive cover</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>As : ,um ; ; t , ' : l there is apl ) rof~riaLe inde ' &lt; ing at entiLies tl~rough lex~cai ~toms that mir ; iht appt~ar in a surface dt.'.~cription '. } f them. thi : ~ rc.cognitior : c ; ; n be done bottom.up , thus rnuking pos : .ible recognition of elliptical , tru~Fner { ~ary , or p~rtially incornpr~.h~ ; , ,siblo input. The same de~imtions can ~i..- ; ( , be us~cl i~ a m. : .~re eft ; cic : nt top-down f\ [ l ; Jt*ll ! ~ : 'l when t ! le input conlorrns to the system 's exDect.alio~\ ] s. , , Recem work \ [ 5 , 8\ ] h~ls suggested the usefulness of multiple cor~structioq.specific reco.qnition str ; tt ( ; gies f , ar restrict , ~d domah\ ] parsing , pat ticularly for dealing witll extragr ; .'nimaiic.q ! input. 1 he ir~dividual entity cJo ! initlons form an i ( h ; al \ [ rc , rnewur } ~ arcq~ , d which to organize lhr multiple 212 strateg ! es. In particular , each definitio~ can specify which strategies are applicable to recognizing it. Of course , `` this only provides a framework for robust recognition , the robustness achieved still depends on the quality of the actual recognition strategies used. The advantages of entity-oriented parsing for language definition include : • All information relating to an entity is grouped in one place , so that a language definer will be able to see more clearly whether a dehnition is complete and what would be the conseouences of any addition or change to the definition. • Since surface ( syntactic ) nnd structural information about an entity is groupe~t to~\ ] ether , tile s , .trface information cau refer to the structure in a clear al ' ; { \ ] coherent way. In particular , this allows hierarchical surface information to use the natural hierarchy defined by the structural informatiol~ , leading to greater consistency of coverage in the surface language. • Since entity definitions are independent , the information necessary In drive Jecognilion by the mulliple constructionspucific strL , tegi~ : s mentioned above can be represented directly in the form most useful to each strategy , thus removing the need for any kind of `` grammar co~pilation '' step and allowing more rapid £irammar development. In the remainder of the paper , we make these arguments more concrete by looking at some fragments of an entity-oriented lan ( \ ] u~ge definition , by outlining the control : ~truclure of a robust resUicted-domain parser driven by such defiqitions , and by tracing through some worked examples of ! he parser in operation. These examples also shown describe some specifi~ parsing strategies that exploit the control structures. A parser i~=corporating the control structure and the parsing strategies is currently under implementation. Its design embodies our e ; { perience with ~ pilot entily-oriented parser that has already been implemented , but is not described here. r -- v 4 . , . ~ , ,ampie Entity Definitions This section present'~ .~r ) me example eat=t , / and language ( lefi , fitions suitable for use in entity-oriente ( \ ] parsing. The examples are drawn fi om the Oomain of an in ! ~rface to a database of college courses. Here is the ( partial ) de\ [ initio=~ of a course , \ [ Ent ttyNarne : Col legeCourse type : Structured Components : ( \ [ Componen tName : £.otlrseNumber type : Integer Greater1han : g9 LeSSI I~an : |000 \ ] \ [ ComponentName : CourseDepartment lype : Co1 legeDepartment \ ] \ [ C 011ll } 0 n e n L N ~ll\ [ le : CourseC I &amp; ss F3 , po : CollegeC lass \ ] \ [ CemponentName : Cuurse\ [ nstructo¢ lype : Col|egeProressor J ) Silt raceRupresen LaL ion : \ [ SynLaxfype : NounPhr~se IIo , l¢l : ( course I sesninsr $ CoursoDepartmenL SCour'set , umber I • • • ) AdiectivalCo , lponen£s : ( Courseaepartment ... ) Adjectives : ( JAdjecLiva\ ] Phrase : ( new J most. recent ) CotllpOllOn L : CollrseSemos ter Value : CUI'I t ! q LSdm ( } S ter \ ] i '' PostNomina ICases : ( \ [ PreposiLion : ( ? intended For J directed to J . ) Cofi|ponellt : CourseClass J LPrl : posiLion : ( ? L~ughL b v I .. , ) Colnpollel1 t : Co ( ~rse \ [ i1.~ L rllc tot \ ] ) J \ ] For reasons of space , we can not explain all the details of this language. In essence , zz course is definc 'd as 3 structured object with components : number , department , instructor , etc. ( square brackets denote attribute/value lists , and round brackets ordinary lists ) . `` lhis definition is kept separate from the surface representation of a course which is defined to be a noun phrase with adjectives , postnor~irla ! cases , etc.. At a more deiailed level , note the special purpose way of specifying a course by its department juxtaposed with its number ( e.g. Computer Science 101 ) is handled by an alternate patt.'. , rn for the head of the noun phrase ( dollar signs refer back to the components ) . Tiffs allows the user to s , sy ( redur= , ~antly ) phrases like `` CS 101 taught by Smith '' . Nolo. also that the way the dep~¢rtment of a course can appear in the surface representation of a course is specified in terms of the £ : ourseDepartment component ( and hence in terms of its type , Colleg ( ; Depmln\ ] ent ) rather than directly as an explicit surface representation. This ensures consistency througl~out the language in what will be recognized as a description of a department. Coupled wdh the ability to use general syntactic descriptors ( like NounPhrase in the description of a SurfaceRepresentation ) , this can prevent the ki~ , J of patchy coveraqe prevalent with standard semantic grammar language definitions. Subsidiary objects like CollegeDepartment are defined in similar fashion. \ [ r n t i LyNnmn : £o I I egel ) epa v Linen t |ypo : Er.uiiler'~L ion E numeratodVa lues : { Conlptltel SC i , nceDepartment Ma t hema I. i c sl ) el ) a r Linen t II istorylJeparLment `` i '' SurfaceRepresentat ion : J Syntaxlype : PaLternSet Patterns : ( \ [ Patt* : rn : ( CS I Computer Scie , ce J Camp Sol J ... ) Va hte : CompuLerSc ietLcel } ~lpal'tment \ ] ) \ ] 1 213 r ; cllegeCoursu will also be involved in higher-level entities ef our restricted domain such as a cc } mrnan ( I to the data base ay.*t : .~m to + : .rol a student in a course. \ [ I Ill. i~l , lalllO : \ [ l ) l'O|COlll/ll~tl ( I lype : Structured Comllonul~ts : ( I.CompononI.Nam+ ! : Fnrol leo fypo : CO I I~UeSL.det~L .I \ [ CemponenLNamu : I : nee\ ] \ [ n Type : Co I leg , '~Co , lrse \ ] ) Sur f'aceRopr , ; se.ta L =el ; : Sy=lta~ \ [ : tp~ , : \ [ lll ; ~.~r.lt. iveC.tsel'ramo Ilea'J : ( corgi I ¢etliSLe¢ \ ] incl~ ( le \ [ ... ) II i re¢ LObju , : I. : ( $ E.rol lee ) Cases : ( \ [ PreposiLi , ~n : ( in I tote J ... ) CO ; tlpOltOl| L : ~ : It I'01 \ ] I } \ ] ) \ ] \ ] These examples als~ show how all information about an entity , co.cerning both tundamental structure and surface representation , is grouped tooeth ' , ~r al~d integrated. Tiff , . ; supports the claim that entity-c~ri~nted lanuuage definition makes it easier to deter.nine whether a language definition is complete. Oriented Parser lhe potential advanta.qes of an entily-oriented approach from tile point of view of robLmtne.~3 in the face of ungr : ¢mmatical input were outlined in the inlrodu ( .tion. To exploit this potential while maintaining efficiency in parsing grammatical input , special attention must he paid to the control structure of the parser used. Desirable characteri , =.tics for the control Structure uf ; my parser capable of handling ungrammatical as well as grammatical input include : . the control structure allows grammatical input to be parsed straightforwardly without consider.ring any of the possible gralnmatical deviations d ; at could occur ; • the om~trol structure enables progr~ : ,~siw : .ly highP.r degrees of grammatical ( leviatior~ Io be consi ( Ic~ : .~d when the ilt\ [ ~LIt does not satisfy grammatical exp , ~ctations ; • the control structure ; dlows simpler deviatio.s to be considered before more complex deviations. \ ] he first two points are self-evident , but the third lll ; +ty require some explanalion. `` The r , robl~m it addresses arises particularly when there are several alternative parses under consideration. In s.ch cases , it is important to prevent the parser h'om cons ! tiering drastic ( levi.xtions in one branch of the par.~'e before cor~si ( lering si~nple ones in the othur. For in : :'.ance , tile par.~er sh ( ; uld not start hypothesizir=g missing words ir ; one bra.ch when a ~ ; impl , ~ ) sp~flli~l O correction in another blanch would allow tile parse I¢~ go through. We have ( le- ; i ( jned a parser control .~hucture for use in e~ , tityoriented p~. ' : ; in U which i } a~ ; all ( , ~ , the rh ; lracteristics lis~e , t above. Thi.~ control structure operates thrr~u~ ; h an acJenda mechanism. Each item of the agenda represents a dii'ier , . : nt nonU/\ ] uati.on of the paine , i.e. a partial parse plus a specificatit , +~ of what to do next to continue that partial parse , With each cont } nuation is associated an integer flexibility level that represents the degree of grammatical deviation imphed by the continuation. That is , the flexibility level represents the degree of grammatical deviation in the input if the continuation were to produce a complete parse ' without finding any more deviation. Continuations with a lower flexibility are run before continuations with a higher flexibility level. Once a complete parse has been obtained , continuations with a , flexibility level higher than that of the continuation which resulted in the parse are abandoned. This means that the agenda mechanism never activates any continuations with a flexibility level higher than the level representing the lowest level of grammatical deviation necessary to account for the input. Thus effort is not wasted exploring more exotic grammatical deviations when the input can be accounted for by simpler ones. This shows that the parser has the first two of the characteristics listed above. In addition to taking care of alternatives at different flexibility levels , this control structure also handles the more usual kind of alternatives faced by parsers -those representing alternative parses due to local ambiguity in the input. Whenever such an ambiguity arises , the control structure duplicates the relevant continuation as many times as there are ambiguous alternatives , giving each of the duplicated continuations the same flexibility level. From there on , the same agenda mechanism used for the various flexibility levels will keep each of the ambiguous alternatives separate and ensure that all are investigated ( as long as their flexibility level is not too high ) . Integrating the treatment of the normal kind of ambiguities with the treatment of alternative ways of handling grammatical deviations ensures that the level of grammatical deviation under consideration can be kept the same in locally cmbiguous branches of a parse. This fulfills the third characteristic listed above. Flexibility levels are additive , i.e. if some grammatical deviation has already been found in the input , then finding a new one will raise the flexibility level of the continuation concerned to the sum of the flexibility levels involved. This ensures a relatively h ! gh flexibility level and thus a relatively low likelihood of activation for continuations in which combinations of deviations are being postulated to account for the input , Since space is limited , we can not go into the implementation of this control structure. However , it is possible to give a brief description of the control structure primitives used in programming the parser. Recall first that the kind of entityoriented parser we have been discussing consists of a collection of recognition strategies. The more specific strategies exploit the idiosyncratic features of the entities/construction types they are specific to , while the more general strategies apply to wider cl3sses of entities and depend on more universal characteristics. In either case , the strategies are pieces of ( Lisp ) program r~.ther than more abstract rules or networks. Integration of such strategies with the general scheme of flexibility levels described above is made straightforward through a special split function which the control structure supports as a primitive. This split function allows the programmer of a strategy to specify one or more alternative continuations from any point in the strategy and to associate a different flexibility increment with each of them. 214 The implementation of this statement takes care of restarting each of the alternative continuations at the appropriate time and with the appropriate local context. Some examples should make this account of the control structure much clearer. The examples will also present some specific parsing strategies and show how they use the split function described above. These strategies are designed to effect robust recognition of extragrammatical input and efficient recognition of grammatical input by exploiting entity-oriented language definitions like those in the previous section. t.et us examine first how a simple data base command like : Enro ; Susan Smith in CS 101 might be parsed with the control structure and language defin ; tions presented in the two previous sections. We start off with the top-level parsing strategy , RecognizeAnyEntity. This strategy first tries to identify a top-level domain entity ( in this case a data base command ) that might account for the entire input. It does this in a bottom-up manner by indexing from words in the input to those entities that they could appear in. In this case , the best indexer is the first word , 'enro ! ' , which indexes EnrolCommand. In general , however , the best indexer need not be the first word of the input and we need to consider all words , thus raising the potential of indexing more than one entity. In our example , we would also index CollegeStudent , CollegeCourse , and Co ! legeDepartment However , tt'ese are not top.level domain entities and are subsumed by EnrolCommand , and so can be ignored in favour of it. Once EnrolCommand has been identified as an entity that might account for the input , RecognizeAnyEntity initiates an attempt to recognize it. Since EnrolCommand is listed as an imperative case frama , this task is handled by the ImperativeCaseFrame recognizer strategy. In contrast to the bottom-up approach of RecognizeAnyEntity , this strategy tackles its more specific task in a top-down manner using the case frame recognition algorithm developed for the CASPAR parser \ [ 8\ ] . In particular , the strategy will match the case frame header and the preposition 'in ' , and initiate recognitions of fillers of its direct object case and its case marked by 'in'. These subgoals are to recognize a CollegeStudent to fill the Enrollee case on the input segment `` Susan Smith ' '' and a CollegeCourse to fill the Enrolln case on the segment `` CS 101 `` . Both of the~e recognitions will be successful , hence causing the ImperativeCaseFrame recognizer to succeed and hence the entire recognition. The resulting parse would be : \ [ InstanceOf : Enro ICo~nand £nrol\ ] ee : \ [ InstanceOt ' : Co\ ] \ ] egeStudent FirstNaaes : ( Susan ) Surname : Smith \ ] \ [ nrotZn : \ [ \ ] nstance0£ : CollegeCourse EourseDepar tment : Compute rSc I enceDepar tment. CourseNumber : t01 \ ] \ ] Note how this parse result is expressed in terms of the underlying structural representation used in the entity definitions without the need for a separate semantic interpretation step. The last example was completely grammatical and so did not require any flexibility. After an initial bottom-up step to find a dominant entity , that entity was recognized in a highly efficient top-down manner. For an example involving input that is ungrammaUcal ( as far as the parser is concerned ) , consider : Place Susan Smith in computer science for freshmen There are two problems here : we assume that the user intended 'place ' as a synonym for 'enror , but that it happens not to be in the system 's vocabulary ; the user has a ! so shortened the grammatically acceptable phrase , 'the computer science course for freshmen ' , to an equivalent phrasenot covered by the surface representation for CollegeCourse as defined earlier. Since 'place ' is not a synonym for 'enrol ' in the language as presently defined , the RecognizeAnyEntity strategy can not index EnrolCommand from it and hence can not ( as it did in tl~e previous example ) initiate a top-down recognition of the entire input. To deal with such eventualities , RecognizeAnyEntity executes a split statement specifying two continuations immediately after it has found all the entities indexed by the input. The first continuation has a zero flexibility level increment. It looks at the indexed entities to see if one subsumes all the others. If it finds one , it attempts a top-down recognition as described in the previous example. If it can not find one , or if it does and the topdown recognition fails , then the continuation itself fails. The second continuation has a positive flexibility increment and follows a more robust bottom-up approach described below. This second continuation was established in the previous example too , but was never activated since a complete parse was found at the zero flexibility level. So we did not mention it. In the present example , the first continuation fails since there is no subsuming entity , and so the second continuation gets a chance to run. Instead of insisting on identifyir , g a single top-level entity , this second continuation attempts to recognize all of the entities that are indexed in the hope of later being able to piece together the various fragmentary recognitions that result. The entities directly indexed are CollegeStudent by `` Susan '' and `` Smith '' , 2 CollegeDepartment by `` computer '' and `` science '' , and CollegeClass by `` freshmen '' . So a top-down attempt is made to recognize each of these entities. We can assume these goals are fulfilled by simple top-down strategies , appropriate to the SurfaceRepresentation of the corresponding entities , and operating with no flexibility level increment. Having recognized the low-level fragments , the second continuation of RecognizeAnyEntity now attempts to unify them into larger fragments , with the ultimate goal of unifying them into a description of a single entity that spans the whole input. To do this , it takes adjacent fragments pairwise and looks for entities of which they are both components , and then tries to recognize the subsuming entity in the spanning segment. The two pairs here are CollegeStudent and CollegeDepartment ( subsumed by CollegeStudent ) and CollegeDepartment and CollegeClass ( subsumed by CollegeCourse ) . To investigate the second of these pairings , RecognizeAnyEntity would try to recognize a CollegeCourse in the spanning segment 'computer science for freshmen ' using an elevated level of flexibility. This gGal would be handled , just like all recognitions of 215 CollegeCourse , by the NominalCaseFrame recognizer. With no flexibility increment , tiffs strategy fails because the head noun is missing. However. with another flexibility increment , the recognition can go through with the CcllegeDepartment being treated as an adjective and the CollegeClass being treated as a postnominal case -it has the right case marker , `` for '' , and the adjective and post-nominal are in the right order. This successful fragment unification leaves two fragments to unify -the old CollegeStudent and the newly derived CollegeCourse. There are several ways of unifying a CollegeStudent and a CollegeCourse -either could subsume the other , or they could form the parameters to one of three database modification commands : EnrolCommand , WithdrawCommand , and TransferCommand ( with the obvious interpretations ) . Since the commands are higher level entities than CollegeStudent and CollegeCourse , they would be preferred as top.level fragment unifiers. We can also rule out TransferCommand in favour of the first two because it requires two courses and we only have one. In addition , a recognition of EnrolCommand would succeed at a lower Ile×ibility increment than WithdrawCommand , 3 since the preposition 'in ' tilat marks the CollegeCourse in the input is the correct marker of the Enrolln case of EnrolCommand , but is not the appropriate marker for WithdrawFrom , the course-containing case of WithdrawCommand. Thus a fragment unification based on EnrolCommand would be preferred. Also , the alternate path of fragment amalgamation -combining CollegeStudent and CollegeDepartment into CollegeStudent and then combining CoilegeStudent and CollegeCourse -that we left pending above can not lead to a complete instantiation of a top-level database command. So RecognizeAnyEntity will be in a position to assume that the user really intended the EnrolCommand. Since th~s recognition involved several significant assumptions , we would need to use focused interaction techniques\ [ 7\ ] to present the interpretation to the user for approval before acting on it. Note that if the user does approve it , it should be possible ( with further approval ) to add 'place ' to the vocabulary as a synonym for 'enrol ' since 'place ' was an unrecognized word in the surface position where 'enrol ' should have been. For a final example , let us examine an extragrammatical input that involves continuations at several different flexibility levels : Transfel Smith from Coi , ~pter Science 101 Economics 203 The problems here are that 'Computer ' has been misspelt and the preposition 'to ' is missing from before 'Economics'. The example is similar to the first one in that RecognizeAnyEntity is able to identify a top-level entity to be recognized top-down , in this case , TransferCommand. Like EnrolCommand , TransferCommand is an imperative case frame , and so the task of recognizing it is handled by the ImperativeCaseFrame strategy. This strategy can find the preposition 'from ' , and so can ! nitiate the appropriate recognitions for fillers of the O.tOfCour~e and Student cases. The recognition for the student case succeeds without trouble , but the recognition for the OutOfCourse case requires a spelling correction. 2We assume we have a complete listing of students and SO can index from their names. Whenever a top-down parsing strategy fails to verify that an input word is in a specific lexical class , there is the possibility that the word that failed is a misspelling of a word that would have succeeded. In such cases , the lexical lookup mechanism executes a split statement. 4 A zero increment branch fails immediately , but a second branch with a small positive increment tries spelling correction against the words in the predicted lexical class. If the correction fails , this second branch fails , but if the correction succeeds , the branch succeeds also. In our example , the continuation involving the second branch of the lexical lookup is highest on the agenda after the primary branch has failed. In particular , it is higher than the second branch of RecognizeAnyEntity described in the previous example , since the flexibility level increment for spelling correction is small. This means that the lexical lookup is continued with a spelling correction , thus resolving the problem. Note also that since the spelling correction is only attempted within the context of recognizing a CollegeCourse -the filler of OutOfCourse -the target words are limited to course names. This means spelling correction is much more accurate and efficient than if correction were attempted against the whole dictionary. After the OutOfCourse and Student cases have been successfully filled , the ImperativeCaseFrame strategy can do no more without a flexibility level increment. But it has not filled all the required cases of TransferCommand , and it has not used up all the input it was given , so it splits and fails at the zero-level flexibility increment. However , in a continuation with a positive flexibility level increment , it is able to attempt recognition of cases without their marking prepositions. Assuming the sum of this increment and the 3pelling correction increment are still less than the increment associated with the second branch of RecognizeAnyEntity , this continuation would be the next one run. In this continuation , the ImperativeCaseFrameRecognizer attempts to match unparsed segments of the input against unfilled cases. There is only one of each , and the resulting attempt to recognize 'Economics 203 ' as the filler of IntoCourse succeeds straightforwardly. Now all required cases are filled and all input is accounted for , so the ImperativeCaseFrame strategy and hence the whole parse succeeds with the correct result. For the example just presented , obtaining the ideal behaviour depends on careful choice of the flexibility level increments. There is a danger here that the performance of the parser as a whole will be dependent on iterative tuning of these increments , and may become unstable with even small changes in the increments. It is too early yet to say how easy it will be to manage this problem , but we plan to pay close attention to it as the parser comes into operatio n . 3This relatively fine distinction between Enro\ ] Command and Withd~awCemmand. based on the appropriateness of the preposition 'in ' , is problem~ ' , tical in that it assumes that the flexibility level would be incremented in very fine grained steps. If that was impractical , the final outcome of the parse would be ambiguous between an EnrolCommand and a WithdrawCommand and the user would have to be asked to make the discrimination. 4If this causes too many splits , an alternative is only to do the split when the input word in question is not in the system 's lexicon at all. 216 Entity-oriented parsing has several ~dvantages as a basisfor language rueognilion in restricted domain natural language int.£\ [ faces. Like techniques based on semantic grammar , it ext~loits limited domain semantics through a series of domainspecific entity types. However , because of its suitability for fragmentary recogniticn and its ability to accornmodate multiple construction.specific parsing strategies , it has the i &gt; otential for greater robustness in the face of extragrammaLical input than the usu\ [ ; I semantic grammar techniques .</sentence>
				<definiendum id="0">RecognizeAnyEntity</definiendum>
				<definiendum id="1">TransferCommand. Like EnrolCommand , TransferCommand</definiendum>
				<definiendum id="2">RecognizeAnyEntity</definiendum>
				<definiendum id="3">ImperativeCaseFrameRecognizer</definiendum>
				<definiens id="0">rof~riaLe inde ' &lt; ing at entiLies tl~rough lex~cai ~toms that mir ; iht appt~ar in a surface dt.'.~cription '. } f them. thi : ~ rc.cognitior : c ; ; n be done bottom.up , thus rnuking pos : .ible recognition of elliptical , tru~Fner { ~ary , or p~rtially incornpr~.h~ ; , ,siblo input. The same de~imtions can ~i..- ; ( , be us~cl i~ a m. : .~re eft</definiens>
				<definiens id="1">the system 's exDect.alio~\ ] s. , , Recem work \ [ 5 , 8\ ] h~ls suggested the usefulness of multiple cor~structioq.specific reco.qnition str ; tt ( ; gies f , ar restrict , ~d domah\ ] parsing , pat ticularly for dealing witll extragr ; .'nimaiic.q ! input. 1 he ir~dividual entity cJo ! initlons form an i ( h ; al \ [ rc , rnewur } ~ arcq~ , d which to organize lhr multiple 212 strateg ! es. In particular , each definitio~ can specify which strategies are applicable to recognizing it. Of course , `` this only provides a framework for robust recognition , the robustness achieved still depends on the quality of the actual recognition strategies used. The advantages of entity-oriented parsing for language definition include : • All information relating to an entity is grouped in one place , so that a language definer will be able to see more clearly whether a dehnition is complete and what would be the conseouences of any addition or change to the definition. • Since surface ( syntactic ) nnd structural information about an entity is groupe~t to~\ ] ether , tile s , .trface information cau refer to the structure in a clear al ' ; { \ ] coherent way. In particular , this allows hierarchical surface information to use the natural hierarchy defined by the structural informatiol~ , leading to greater consistency of coverage in the surface language. • Since entity definitions are independent , the information necessary In drive Jecognilion by the mulliple constructionspucific strL , tegi~ : s mentioned above can be represented directly in the form most useful to each strategy , thus removing the need for any kind of `` grammar co~pilation '' step and allowing more rapid £irammar development. In the remainder of the paper , we make these arguments more concrete by looking at some fragments of an entity-oriented lan ( \ ] u~ge definition , by outlining the control : ~truclure of a robust resUicted-domain parser driven by such defiqitions , and by tracing through some worked examples of ! he parser in operation. These examples also shown describe some specifi~ parsing strategies that exploit the control structures. A parser i~=corporating the control structure and the parsing strategies is currently under implementation. Its design embodies our e ; { perience with ~ pilot entily-oriented parser that has already been implemented , but is not described here. r -- v 4 . , . ~ , ,ampie Entity Definitions This section present'~ .~r ) me example eat=t , / and language ( lefi , fitions suitable for use in entity-oriente ( \ ] parsing. The examples are drawn fi om the Oomain of an in ! ~rface to a database of college courses. Here is the ( partial ) de\ [ initio=~ of a course , \ [ Ent ttyNarne : Col legeCourse type : Structured Components : ( \ [ Componen tName : £.otlrseNumber type : Integer Greater1han : g9 LeSSI I~an : |000 \ ] \ [ ComponentName : CourseDepartment lype : Co1 legeDepartment \ ] \ [ C 011ll } 0 n e n L N ~ll\ [ le : CourseC I &amp; ss F3 , po : CollegeC lass \ ] \ [ CemponentName : Cuurse\ [ nstructo¢ lype : Col|egeProressor J ) Silt raceRupresen LaL ion : \ [ SynLaxfype : NounPhr~se IIo , l¢l : ( course I sesninsr $ CoursoDepartmenL SCour'set , umber I • • • ) AdiectivalCo , lponen£s : ( Courseaepartment ... ) Adjectives : ( JAdjecLiva\ ] Phrase : ( new J most. recent ) CotllpOllOn L : CollrseSemos ter Value : CUI'I t ! q LSdm ( } S ter \ ] i '' PostNomina ICases : ( \ [ PreposiLion : ( ? intended For J directed to J . ) Cofi|ponellt : CourseClass J LPrl : posiLion : ( ? L~ughL b v I .. , ) Colnpollel1 t : Co ( ~rse \ [ i1.~ L rllc tot \ ] ) J \ ] For reasons of space , we can not explain all the details of this language. In essence , zz course is definc 'd as 3 structured object with components : number , department , instructor , etc. ( square brackets denote attribute/value lists , and round brackets ordinary lists ) . `` lhis definition is kept separate from the surface representation of a course which is defined to be a noun phrase with adjectives , postnor~irla ! cases , etc.. At a more deiailed level , note the special purpose way of specifying a course by its department juxtaposed with its number ( e.g. Computer Science 101 ) is handled by an alternate patt.'. , rn for the head of the noun phrase ( dollar signs refer back to the components ) . Tiffs allows the user to s , sy ( redur= , ~antly ) phrases like `` CS 101 taught by Smith '' . Nolo. also that the way the dep~¢rtment of a course can appear in the surface representation of a course is specified in terms of the £ : ourseDepartment component ( and hence in terms of its type , Colleg ( ; Depmln\ ] ent ) rather than directly as an explicit surface representation. This ensures consistency througl~out the language in what will be recognized as a description of a department. Coupled wdh the ability to use general syntactic descriptors ( like NounPhrase in the description of a SurfaceRepresentation ) , this can prevent the ki~ , J of patchy coveraqe prevalent with standard semantic grammar language definitions. Subsidiary objects like CollegeDepartment are defined in similar fashion. \ [ r n t i LyNnmn : £o I I egel ) epa v Linen t |ypo : Er.uiiler'~L ion E numeratodVa lues : { Conlptltel SC i , nceDepartment Ma t hema I. i c sl ) el ) a r Linen t II istorylJeparLment `` i '' SurfaceRepresentat ion : J Syntaxlype : PaLternSet Patterns : ( \ [ Patt* : rn : ( CS I Computer Scie , ce J Camp Sol J ... ) Va hte : CompuLerSc ietLcel } ~lpal'tment \ ] ) \ ] 1 213 r ; cllegeCoursu will also be involved in higher-level entities ef our restricted domain such as a cc } mrnan ( I to the data base ay.*t : .~m to + : .rol a student in a course. \ [ I Ill. i~l , lalllO : \ [ l ) l'O|COlll/ll~tl ( I lype : Structured Comllonul~ts : ( I.CompononI.Nam+ ! : Fnrol leo fypo : CO I I~UeSL.det~L .I \ [ CemponenLNamu : I : nee\ ] \ [ n Type : Co I leg , '~Co , lrse \ ] ) Sur f'aceRopr , ; se.ta L =el ; : Sy=lta~ \ [ : tp~ , : \ [ lll ; ~.~r.lt. iveC.tsel'ramo Ilea'J : ( corgi I ¢etliSLe¢ \ ] incl~ ( le \ [ ... ) II i re¢ LObju , : I. : ( $ E.rol lee ) Cases : ( \ [ PreposiLi , ~n : ( in I tote J ... ) CO ; tlpOltOl| L : ~ : It I'01 \ ] I } \ ] ) \ ] \ ] These examples als~ show how all information about an entity , co.cerning both tundamental structure and surface representation , is grouped tooeth ' , ~r al~d integrated. Tiff , . ; supports the claim that entity-c~ri~nted lanuuage definition makes it easier to deter.nine whether a language definition is complete. Oriented Parser lhe potential advanta.qes of an entily-oriented approach from tile point of view of robLmtne.~3 in the face of ungr : ¢mmatical input were outlined in the inlrodu ( .tion. To exploit this potential while maintaining efficiency in parsing grammatical input , special attention must he paid to the control structure of the parser used. Desirable characteri , =.tics for the control Structure uf ; my parser capable of handling ungrammatical as well as grammatical input include : . the control structure allows grammatical input to be parsed straightforwardly without consider.ring any of the possible gralnmatical deviations d ; at could occur ; • the om~trol structure enables progr~ : ,~siw : .ly highP.r degrees of grammatical ( leviatior~ Io be consi ( Ic~ : .~d when the ilt\ [ ~LIt does not satisfy grammatical exp , ~ctations ; • the control structure ; dlows simpler deviatio.s to be considered before more complex deviations. \ ] he first two points are self-evident , but the third lll ; +ty require some explanalion. `` The r , robl~m it addresses arises particularly when there are several alternative parses under consideration. In s.ch cases , it is important to prevent the parser h'om cons ! tiering drastic ( levi.xtions in one branch of the par.~'e before cor~si ( lering si~nple ones in the othur. For in : :'.ance , tile par.~er sh ( ; uld not start hypothesizir=g missing words ir ; one bra.ch when a ~ ; impl , ~ ) sp~flli~l O correction in another blanch would allow tile parse I¢~ go through. We have ( le- ; i ( jned a parser control .~hucture for use in e~ , tityoriented p~. ' : ; in U which i } a~ ; all ( , ~ , the rh ; lracteristics lis~e , t above. Thi.~ control structure operates thrr~u~ ; h an acJenda mechanism. Each item of the agenda represents a dii'ier , . : nt nonU/\ ] uati.on of the paine , i.e. a partial parse plus a specificatit , +~ of what to do next to continue that partial parse , With each cont } nuation is associated an integer flexibility level that represents the degree of grammatical deviation imphed by the continuation. That is , the flexibility level represents the degree of grammatical deviation in the input if the continuation were to produce a complete parse ' without finding any more deviation. Continuations with a lower flexibility are run before continuations with a higher flexibility level. Once a complete parse has been obtained , continuations with a , flexibility level higher than that of the continuation which resulted in the parse are abandoned. This means that the agenda mechanism never activates any continuations with a flexibility level higher than the level representing the lowest level of grammatical deviation necessary to account for the input. Thus effort is not wasted exploring more exotic grammatical deviations when the input can be accounted for by simpler ones. This shows that the parser has the first two of the characteristics listed above. In addition to taking care of alternatives at different flexibility levels , this control structure also handles the more usual kind of alternatives faced by parsers -those representing alternative parses due to local ambiguity in the input. Whenever such an ambiguity arises , the control structure duplicates the relevant continuation as many times as there are ambiguous alternatives , giving each of the duplicated continuations the same flexibility level. From there on , the same agenda mechanism used for the various flexibility levels will keep each of the ambiguous alternatives separate and ensure that all are investigated ( as long as their flexibility level is not too high ) . Integrating the treatment of the normal kind of ambiguities with the treatment of alternative ways of handling grammatical deviations ensures that the level of grammatical deviation under consideration can be kept the same in locally cmbiguous branches of a parse. This fulfills the third characteristic listed above. Flexibility levels are additive , i.e. if some grammatical deviation has already been found in the input , then finding a new one will raise the flexibility level of the continuation concerned to the sum of the flexibility levels involved. This ensures a relatively h ! gh flexibility level and thus a relatively low likelihood of activation for continuations in which combinations of deviations are being postulated to account for the input</definiens>
				<definiens id="2">possible to give a brief description of the control structure primitives used in programming the parser. Recall first that the kind of entityoriented parser we have been discussing consists of a collection of recognition strategies. The more specific strategies exploit the idiosyncratic features of the entities/construction types they are specific to , while the more general strategies apply to wider cl3sses of entities and depend on more universal characteristics. In either case , the strategies are pieces of ( Lisp ) program r~.ther than more abstract rules or networks. Integration of such strategies with the general scheme of flexibility levels described above is made straightforward through a special split function which the control structure supports as a primitive. This split function allows the programmer of a strategy to specify one or more alternative continuations from any point in the strategy and to associate a different flexibility increment with each of them. 214 The implementation of this statement takes care of restarting each of the alternative continuations at the appropriate time and with the appropriate local context. Some examples should make this account of the control structure much clearer. The examples will also present some specific parsing strategies and show how they use the split function described above. These strategies are designed to effect robust recognition of extragrammatical input and efficient recognition of grammatical input by exploiting entity-oriented language definitions like those in the previous section. t.et us examine first how a simple data base command like : Enro ; Susan Smith in CS 101 might be parsed with the control structure and language defin ; tions presented in the two previous sections. We start off with the top-level parsing strategy , RecognizeAnyEntity. This strategy first tries to identify a top-level domain entity ( in this case a data base command ) that might account for the entire input. It does this in a bottom-up manner by indexing from words in the input to those entities that they could appear in. In this case , the best indexer is the first word , 'enro ! ' , which indexes EnrolCommand. In general , however , the best indexer need not be the first word of the input and we need to consider all words , thus raising the potential of indexing more than one entity. In our example , we would also index CollegeStudent , CollegeCourse , and Co ! legeDepartment However , tt'ese are not top.level domain entities and are subsumed by EnrolCommand , and so can be ignored in favour of it. Once EnrolCommand has been identified as an entity that might account for the input</definiens>
				<definiens id="3">initiates an attempt to recognize it. Since EnrolCommand is listed as an imperative case frama , this task is handled by the ImperativeCaseFrame recognizer strategy. In contrast to the bottom-up approach of RecognizeAnyEntity , this strategy tackles its more specific task in a top-down manner using the case frame recognition algorithm developed for the CASPAR parser \ [ 8\ ] . In particular , the strategy will match the case frame header and the preposition 'in ' , and initiate recognitions of fillers of its direct object case and its case marked by 'in'. These subgoals are to recognize a CollegeStudent to fill the Enrollee case on the input segment `` Susan Smith ' '' and a CollegeCourse to fill the Enrolln case on the segment `` CS 101 `` . Both of the~e recognitions will be successful , hence causing the ImperativeCaseFrame recognizer to succeed and hence the entire recognition. The resulting parse would be : \ [ InstanceOf : Enro ICo~nand £nrol\ ] ee : \ [ InstanceOt ' : Co\ ] \ ] egeStudent FirstNaaes : ( Susan ) Surname : Smith \ ] \ [ nrotZn : \ [ \ ] nstance0£ : CollegeCourse EourseDepar tment : Compute rSc I enceDepar tment. CourseNumber : t01 \ ] \ ] Note how this parse result is expressed in terms of the underlying structural representation used in the entity definitions without the need for a separate semantic interpretation step. The last example was completely grammatical and so did not require any flexibility. After an initial bottom-up step to find a dominant entity , that entity was recognized in a highly efficient top-down manner. For an example involving input that is ungrammaUcal ( as far as the parser is concerned ) , consider : Place Susan Smith in computer science for freshmen There are two problems here : we assume that the user intended 'place ' as a synonym for 'enror , but that it happens not to be in the system 's vocabulary ; the user has a ! so shortened the grammatically acceptable phrase , 'the computer science course for freshmen ' , to an equivalent phrasenot covered by the surface representation for CollegeCourse as defined earlier. Since 'place ' is not a synonym for 'enrol ' in the language as presently defined , the RecognizeAnyEntity strategy can not index EnrolCommand from it and hence can not ( as it did in tl~e previous example ) initiate a top-down recognition of the entire input. To deal with such eventualities , RecognizeAnyEntity executes a split statement specifying two continuations immediately after it has found all the entities indexed by the input. The first continuation has a zero flexibility level increment. It looks at the indexed entities to see if one subsumes all the others. If it finds one , it attempts a top-down recognition as described in the previous example. If it can not find one , or if it does and the topdown recognition fails , then the continuation itself fails. The second continuation has a positive flexibility increment and follows a more robust bottom-up approach described below. This second continuation was established in the previous example too , but was never activated since a complete parse was found at the zero flexibility level. So we did not mention it. In the present example , the first continuation fails since there is no subsuming entity , and so the second continuation gets a chance to run. Instead of insisting on identifyir , g a single top-level entity , this second continuation attempts to recognize all of the entities that are indexed in the hope of later being able to piece together the various fragmentary recognitions that result. The entities directly indexed are CollegeStudent by `` Susan '' and `` Smith '' , 2 CollegeDepartment by `` computer '' and `` science '' , and CollegeClass by `` freshmen '' . So a top-down attempt is made to recognize each of these entities. We can assume these goals are fulfilled by simple top-down strategies , appropriate to the SurfaceRepresentation of the corresponding entities , and operating with no flexibility level increment. Having recognized the low-level fragments , the second continuation of RecognizeAnyEntity now attempts to unify them into larger fragments , with the ultimate goal of unifying them into a description of a single entity that spans the whole input. To do this , it takes adjacent fragments pairwise and looks for entities of which they are both components , and then tries to recognize the subsuming entity in the spanning segment. The two pairs here are CollegeStudent and CollegeDepartment ( subsumed by CollegeStudent ) and CollegeDepartment and CollegeClass ( subsumed by CollegeCourse ) . To investigate the second of these pairings , RecognizeAnyEntity would try to recognize a CollegeCourse in the spanning segment 'computer science for freshmen ' using an elevated level of flexibility. This gGal would be handled , just like all recognitions of 215 CollegeCourse , by the NominalCaseFrame recognizer. With no flexibility increment , tiffs strategy fails because the head noun is missing. However. with another flexibility increment , the recognition can go through with the CcllegeDepartment being treated as an adjective and the CollegeClass being treated as a postnominal case -it has the right case marker , `` for '' , and the adjective and post-nominal are in the right order. This successful fragment unification leaves two fragments to unify -the old CollegeStudent and the newly derived CollegeCourse. There are several ways of unifying a CollegeStudent and a CollegeCourse -either could subsume the other , or they could form the parameters to one of three database modification commands : EnrolCommand , WithdrawCommand , and TransferCommand ( with the obvious interpretations ) . Since the commands are higher level entities than CollegeStudent and CollegeCourse , they would be preferred as top.level fragment unifiers. We can also rule out TransferCommand in favour of the first two because it requires two courses</definiens>
				<definiens id="4">the correct marker of the Enrolln case of EnrolCommand , but is not the appropriate marker for WithdrawFrom , the course-containing case of WithdrawCommand. Thus a fragment unification based on EnrolCommand would be preferred. Also , the alternate path of fragment amalgamation -combining CollegeStudent and CollegeDepartment into CollegeStudent and then combining CoilegeStudent and CollegeCourse -that we left pending above can not lead to a complete instantiation of a top-level database command. So RecognizeAnyEntity will be in a position to assume that the user really intended the EnrolCommand. Since th~s recognition involved several significant assumptions , we would need to use focused interaction techniques\ [ 7\ ] to present the interpretation to the user for approval before acting on it. Note that if the user does approve it , it should be possible ( with further approval ) to add 'place ' to the vocabulary as a synonym for 'enrol ' since 'place ' was an unrecognized word in the surface position where 'enrol ' should have been. For a final example , let us examine an extragrammatical input that involves continuations at several different flexibility levels : Transfel Smith from Coi , ~pter Science 101 Economics 203 The problems here are that 'Computer ' has been misspelt and the preposition 'to ' is missing from before 'Economics'. The example is similar to the first one in that RecognizeAnyEntity is able to identify a top-level entity to be recognized top-down , in this case</definiens>
				<definiens id="5">an imperative case frame , and so the task of recognizing it is handled by the ImperativeCaseFrame strategy. This strategy can find the preposition 'from ' , and so can ! nitiate the appropriate recognitions for fillers of the O.tOfCour~e and Student cases. The recognition for the student case succeeds without trouble , but the recognition for the OutOfCourse case requires a spelling correction. 2We assume we have a complete listing of students and SO can index from their names. Whenever a top-down parsing strategy fails to verify that an input word is in a specific lexical class , there is the possibility that the word that failed is a misspelling of a word that would have succeeded. In such cases , the lexical lookup mechanism executes a split statement. 4 A zero increment branch fails immediately , but a second branch with a small positive increment tries spelling correction against the words in the predicted lexical class. If the correction fails , this second branch fails , but if the correction succeeds , the branch succeeds also. In our example , the continuation involving the second branch of the lexical lookup is highest on the agenda after the primary branch has failed. In particular , it is higher than the second branch of RecognizeAnyEntity described in the previous example , since the flexibility level increment for spelling correction is small. This means that the lexical lookup is continued with a spelling correction , thus resolving the problem. Note also that since the spelling correction is only attempted within the context of recognizing a CollegeCourse -the filler of OutOfCourse -the target words are limited to course names. This means spelling correction is much more accurate and efficient than if correction were attempted against the whole dictionary. After the OutOfCourse and Student cases have been successfully filled , the ImperativeCaseFrame strategy can do no more without a flexibility level increment. But it has not filled all the required cases of TransferCommand , and it has not used up all the input it was given , so it splits and fails at the zero-level flexibility increment. However , in a continuation with a positive flexibility level increment , it is able to attempt recognition of cases without their marking prepositions. Assuming the sum of this increment and the 3pelling correction increment are still less than the increment associated with the second branch of</definiens>
				<definiens id="6">attempts to match unparsed segments of the input against unfilled cases. There is only one of each , and the resulting attempt to recognize 'Economics 203 ' as the filler of IntoCourse succeeds straightforwardly. Now all required cases are filled and all input is accounted for , so the ImperativeCaseFrame strategy and hence the whole parse succeeds with the correct result. For the example just presented , obtaining the ideal behaviour depends on careful choice of the flexibility level increments. There is a danger here that the performance of the parser as a whole will be dependent on iterative tuning of these increments , and may become unstable with even small changes in the increments. It is too early yet to say how easy it will be to manage this problem , but we plan to pay close attention to it as the parser comes into operatio n . 3This relatively fine distinction between Enro\ ] Command and Withd~awCemmand. based on the appropriateness of the preposition 'in ' , is problem~ ' , tical in that it assumes that the flexibility level would be incremented in very fine grained steps. If that was impractical , the final outcome of the parse would be ambiguous between an EnrolCommand and a WithdrawCommand and the user would have to be asked to make the discrimination. 4If this causes too many splits , an alternative is only to do the split when the input word in question is not in the system 's lexicon at all. 216 Entity-oriented parsing has several ~dvantages as a basisfor language rueognilion in restricted domain natural language int.£\ [ faces. Like techniques based on semantic grammar , it ext~loits limited domain semantics through a series of domainspecific entity types. However , because of its suitability for fragmentary recogniticn and its ability to accornmodate multiple construction.specific parsing strategies , it has the i &gt; otential for greater robustness in the face of extragrammaLical input than the usu\ [ ; I semantic grammar techniques</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>ABSTRACT This paper pinpoints some of the problems faced when a computer text production model ( COMMENTATOR ) is to produce spontaneous speech , in particular the problem of chunking the utterances in order to get natural prosodic units .</sentence>
				<definiendum id="0">COMMENTATOR</definiendum>
				<definiens id="0">to produce spontaneous speech</definiens>
			</definition>
			<definition id="1">
				<sentence>The Commentator produces comments consisting of grammatical sentences making up coherent and wellformed text ( although often soon boring ) .</sentence>
				<definiendum id="0">Commentator</definiendum>
				<definiens id="0">produces comments consisting of grammatical sentences making up coherent and wellformed text ( although often soon boring )</definiens>
			</definition>
			<definition id="2">
				<sentence>The Votrax is an inexpensive and unsophisticated synthesis device and it is not our hope to achieve perfect pronunciation using this circuit , of course .</sentence>
				<definiendum id="0">Votrax</definiendum>
				<definiens id="0">an inexpensive and unsophisticated synthesis device</definiens>
			</definition>
			<definition id="3">
				<sentence>Votrax offers no way of changing the amplitude or the duration .</sentence>
				<definiendum id="0">Votrax</definiendum>
				<definiens id="0">offers no way of changing the amplitude or the duration</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>3 An Exan~le of The Analysing Result Obtained by The Preprocessing Subsystem 224 ( process ) ( have/finish ) ( -ed ) Translation : had processed I : fragment given the higher priority r -- ~ : fragment given : ~ the lower priority Fi~.4 An Example of Fragment Priority ( Fig.4 ) .</sentence>
				<definiendum id="0">Fragment Priority</definiendum>
				<definiens id="0">process ) ( have/finish ) ( -ed ) Translation : had processed I : fragment given the higher priority</definiens>
			</definition>
			<definition id="1">
				<sentence>c ) Fragment length Length is a useful parameter , but its effect on priority depends on the kind of fragment .</sentence>
				<definiendum id="0">c ) Fragment length Length</definiendum>
				<definiens id="0">a useful parameter , but its effect on priority depends on the kind of fragment</definiens>
			</definition>
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>This paper describes DPARSER ( Deterministic PARSER ) which is an implementation of an alternate deterministic parsing model intended to reduce the complexity of deterministic grammars .</sentence>
				<definiendum id="0">DPARSER ( Deterministic PARSER )</definiendum>
				<definiens id="0">an implementation of an alternate deterministic parsing model intended to reduce the complexity of deterministic grammars</definiens>
			</definition>
			<definition id="1">
				<sentence>The PUSIt operation inserts a new node into the buffer immediately preceding the constituent phrase and begins executing the specified Isupported in part by an I.W. Killaw Predoctoral Fellowship 2supported in part by the Blum-Kovler Foundation , Chicago , Ill. subgrammar .</sentence>
				<definiendum id="0">PUSIt operation</definiendum>
				<definiens id="0">inserts a new node into the buffer immediately preceding the constituent phrase and begins executing the specified Isupported in part by an I.W. Killaw Predoctoral Fellowship 2supported in part by the Blum-Kovler Foundation , Chicago , Ill. subgrammar</definiens>
			</definition>
			<definition id="2">
				<sentence>The interface consists of the ATTACH action which immediately performs the specified attachment and the W-ATTACH test which only succeeds if the attachment can be performed in light of the other constituents which may want to attach it .</sentence>
				<definiendum id="0">interface</definiendum>
				<definiens id="0">consists of the ATTACH action which immediately performs the specified attachment and the W-ATTACH test which only succeeds if the attachment can be performed in light of the other constituents which may want to attach it</definiens>
			</definition>
			<definition id="3">
				<sentence>Control now returns to the predicate node which attaches the suspended NP as the object of the verb .</sentence>
				<definiendum id="0">Control</definiendum>
				<definiens id="0">the object of the verb</definiens>
			</definition>
			<definition id="4">
				<sentence>Again since the PP is the first constituent in the buffer the IF-ATTACH test is performed and the predicate node is suspended returning control to the sentence active node ( Figure 8 ) .</sentence>
				<definiendum id="0">PP</definiendum>
			</definition>
</paper>

		<paper id="1114">
			<definition id="0">
				<sentence>For this reason , in the following sentences the second conjuncts have quite different roles : John loves Mary and Susy ( i ) John loves Mary and Susy Fred ( 2 ) John loves Mary and hates Violet ( 3 ) Thus , as in the case of ellipsis , a syntactic ana lyser designed to handle conjunctions must be able to operate on ill-formed fragments , but with the additional difficulty of modifying the parse tree on the basis of the type of ill-formedness .</sentence>
				<definiendum id="0">ellipsis</definiendum>
				<definiens id="0">a syntactic ana lyser designed to handle conjunctions must be able to operate on ill-formed fragments , but with the additional difficulty of modifying the parse tree on the basis of the type of ill-formedness</definiens>
			</definition>
</paper>

		<paper id="1004">
</paper>

		<paper id="1038">
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>Unlike other query programs , REFER does not search a word form ( or a combinantion of graphemes ) in the corpus itself , but in registers containing all the word forms .</sentence>
				<definiendum id="0">REFER</definiendum>
				<definiens id="0">does not search a word form ( or a combinantion of graphemes ) in the corpus itself , but in registers containing all the word forms</definiens>
			</definition>
</paper>

		<paper id="1067">
			<definition id="0">
				<sentence>ABSTRACT In this paper we will present three systems , ATLAS , THAM and VISULEX , which have been designed and implemented at GETA ( Study Group for Machine Translation ) in collaboration with IFCI ( Institut de Formation et de Conseil en Informatique ) as tools operating around the ARIANE-78 system .</sentence>
				<definiendum id="0">VISULEX</definiendum>
				<definiens id="0">tools operating around the ARIANE-78 system</definiens>
			</definition>
			<definition id="1">
				<sentence>An ATLAS session begins with an optional compilation phase .</sentence>
				<definiendum id="0">ATLAS session</definiendum>
			</definition>
			<definition id="2">
				<sentence>THAM consists of a set of functions programmed in the macro language associated with a powerful text editor .</sentence>
				<definiendum id="0">THAM</definiendum>
				<definiens id="0">consists of a set of functions programmed in the macro language associated with a powerful text editor</definiens>
			</definition>
			<definition id="3">
				<sentence>Hence , the THAM system works with four types of objects : source text ( S ) , translated text ( T ) , revised text ( R ) and uncoded dictionary ( D ) .</sentence>
				<definiendum id="0">THAM system</definiendum>
				<definiens id="0">works with four types of objects : source text ( S ) , translated text ( T ) , revised text ( R</definiens>
			</definition>
			<definition id="4">
				<sentence>Finally , the file R is the final translation of the original text realized by the user from the three previous files .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">the final translation of the original text realized by the user from the three previous files</definiens>
			</definition>
			<definition id="5">
				<sentence>IV VISULEX VISULEX is a handy and easy-to-use visualisation tool designed to reassemble and clearly distinguish certain information contained in a linguistic application data base .</sentence>
				<definiendum id="0">IV VISULEX VISULEX</definiendum>
				<definiens id="0">a handy and easy-to-use visualisation tool designed to reassemble and clearly distinguish certain information contained in a linguistic application data base</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>For Hintikka , a YNQ is a ~peciai case of alterna ( Jve question in which the negative alternative 'or not P ' has been suppressed .</sentence>
				<definiendum id="0">YNQ</definiendum>
				<definiens id="0">a ~peciai case of alterna ( Jve question in</definiens>
			</definition>
</paper>

		<paper id="1040">
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>LEXICAT is a species of shift-reduce parser which ernp~oys the same stack for the storage and analysis of input and inspects the top three cells of the stack before each parsing operation .</sentence>
				<definiendum id="0">LEXICAT</definiendum>
				<definiens id="0">a species of shift-reduce parser which ernp~oys the same stack for the storage</definiens>
			</definition>
			<definition id="1">
				<sentence>LEXICAT is mrnilar to recent proposals by Church ( 1980 ) , i : 'ercira ( in press ) and Shieber ( 1983 ) , in that it employs general strategies , stated in terms of the parser 's basic operations , in order to parse determinislieally with an ambiguous grammar .</sentence>
				<definiendum id="0">LEXICAT</definiendum>
				<definiens id="0">employs general strategies , stated in terms of the parser 's basic operations</definiens>
			</definition>
</paper>

		<paper id="1089">
			<definition id="0">
				<sentence>Case frames provide a high-level set of syntactic and semantic expectations that can be applied to the input in a variety of ways .</sentence>
				<definiendum id="0">Case frames</definiendum>
				<definiens id="0">provide a high-level set of syntactic and semantic expectations that can be applied to the input in a variety of ways</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>Context-free grammars possess an obvious declarative semantics in which nonterminals represent sets of strings and rules represent n-ary relations over strings .</sentence>
				<definiendum id="0">Context-free grammars</definiendum>
				<definiens id="0">an obvious declarative semantics in which nonterminals represent sets of strings and rules represent n-ary relations over strings</definiens>
			</definition>
			<definition id="1">
				<sentence>The information system D describing the elements of F is defined , following Scott , as the tuple D = ( /9 , A , Con , ~- ) , where 19 is a set of propositions , Con is a set of finite subsets of P , the consistent subsets , Iis an entailment relation between elements of Con and elements of D and A is a special least informative element that gives no information at all .</sentence>
				<definiendum id="0">Con</definiendum>
				<definiendum id="1">19</definiendum>
				<definiendum id="2">Con</definiendum>
				<definiens id="0">a set of propositions</definiens>
				<definiens id="1">a set of finite subsets of P , the consistent subsets , Iis an entailment relation between elements of Con</definiens>
			</definition>
			<definition id="2">
				<sentence>The greatest-lower-bound operation n gives the most instantiated description containing all the equations common to two descriptions , their generalization .</sentence>
				<definiendum id="0">greatest-lower-bound operation n</definiendum>
				<definiens id="0">gives the most instantiated description containing all the equations common to two descriptions , their generalization</definiens>
			</definition>
			<definition id="3">
				<sentence>The denotation of a grammar is the fimetion that maps each lexicon ~ into the smallest fixed point of To containing e. The fixed point is defined by i=O as Tc is contimmus .</sentence>
				<definiendum id="0">denotation of a grammar</definiendum>
				<definiens id="0">the fimetion that maps each lexicon ~ into the smallest fixed point of To containing e. The fixed point</definiens>
			</definition>
</paper>

		<paper id="1070">
</paper>

		<paper id="1037">
</paper>

		<paper id="1083">
			<definition id="0">
				<sentence>Simple conditions to be tested in that knowledge base are : ACTIVE ( f ) : &lt; -- -= &gt; f is an active frame EISA ( f , f '' ) : &lt; -- - &gt; frame f is subordinate or instance of frame f '' HAS SLOT ( f , s ) : &lt; === &gt; frame f has slot s associated to it HAS SVAL ( f , s , v ) : &lt; -== &gt; slot s of frame f has been assigned the slot value v SVAL RANGE ( sir , s , f ) : &lt; ffi== &gt; -- string sir is a permitted slot value with respect to slot s of frame f Co-text is a data repository which keeps record of the sequential course of the text analysis actually going on this linear type of information is completely lost in the context , although it is badly needed for various sorts of textual cohesion and coherence phenomena .</sentence>
				<definiendum id="0">Simple conditions</definiendum>
				<definiens id="0">f , s ) : &lt; === &gt; frame f has slot s associated to it HAS SVAL ( f , s , v ) : &lt; -== &gt; slot s of frame f has been assigned the slot value v SVAL RANGE ( sir , s , f ) : &lt; ffi== &gt; -- string sir is a permitted slot value with respect to slot s of frame f Co-text is a data repository which keeps record of the sequential course of the text analysis actually going on this linear type of information is completely lost in the context</definiens>
			</definition>
			<definition id="1">
				<sentence>Items stored in the co-text are in the format TOKEN TYPE ANNOT actual form of text word normalized form of text word after morphological reduction or decomposition procedures have operated on it annotation indicating whether TYPE is identified as FRAME a frame name WEXP a word expert name STOP a stop word or NUM a numerical string NIL an unknown text word or TYPE consists of parameters frame .</sentence>
				<definiendum id="0">Items</definiendum>
				<definiens id="0">stored in the co-text are in the format TOKEN TYPE ANNOT actual form of text word normalized form of text word after morphological reduction or decomposition procedures have operated on it annotation indicating whether TYPE is identified as FRAME a frame name WEXP a word expert name STOP a stop word or NUM a numerical string NIL an unknown text word or TYPE consists of parameters frame</definiens>
			</definition>
			<definition id="2">
				<sentence>When more than a slngle frame within the same transaction may be referred to by word experts the following reference convention is applied : \ [ 2i\ ] \ [ 2ii\ ] if ANNOT FRAME and an annotation of type FACT exists examine the frame corresponding to FACT if ANNOT FRAME or ANNOT WEXP and annotations of type SACT or SVAL exist examine f as frame , s as slot , and v as slot value , resp .</sentence>
				<definiendum id="0">v</definiendum>
				<definiens id="0">slot , and</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus , one gets the reading SHIFT ( System , M/krocomputer ) which says that the activation weight of frame ( System ) has to be decremented ( thus neutralizing the default activation ) , while the activation weight of frame '' ( Mikrocomputer ) gets incremented instead .</sentence>
				<definiendum id="0">reading SHIFT</definiendum>
				<definiens id="0">says that the activation weight of frame ( System ) has to be decremented ( thus neutralizing the default activation ) , while the activation weight of frame ''</definiens>
			</definition>
			<definition id="4">
				<sentence>The SEPARATOR reading is a basic indicator of interruptions of toplc/comment sequencing .</sentence>
				<definiendum id="0">SEPARATOR reading</definiendum>
				<definiens id="0">a basic indicator of interruptions of toplc/comment sequencing</definiens>
			</definition>
</paper>

		<paper id="1081">
			<definition id="0">
				<sentence>The stack item is the quantum value that can be pushed and popped , that is no part of the item is accessed separately from the rest of the values in it .</sentence>
				<definiendum id="0">stack item</definiendum>
				<definiens id="0">no part of the item is accessed separately from the rest of the values in it</definiens>
			</definition>
</paper>

		<paper id="1102">
			<definition id="0">
				<sentence>Explanation List Comgarison ( ELC ) is the technique that implements this process .</sentence>
				<definiendum id="0">Explanation List Comgarison ( ELC )</definiendum>
				<definiens id="0">the technique that implements this process</definiens>
			</definition>
			<definition id="1">
				<sentence>Rule Head &lt; a &gt; \ [ z z\ ] &lt; b &gt; \ [ 1Z\ ] &lt; c &gt; \ [ 1\ ] &lt; d &gt; \ [ 1 2\ ] &lt; e &gt; It\ ] &lt; f &gt; It Z\ ] &lt; g &gt; \ [ 1 2\ ] For instance , the head definition of the rule &lt; b ) says that the head of the construction `` NP + VP + PP '' is a concatenation of the head of 1.st constituent ( NP ) and the head of 2-nd constituent ( VP ) . The i~ead of `` A GIRL with A RED BAG saw A GREEN TREE WITH a telescope '' is , therefore , `` A GIRL saw A TREE '' , because the head of `` A GIRL with A RED BAG '' ( NP ) is `` A GIRL '' and the head of `` saw A GREEN `` IREE '' ( VP ) is `` saw A TREE '' . in our example , the explanation ( Mary ) is a subject of the action ( saw a man with a telescope ) becomes ( Mary ) is a subject of the action ( saw a man ) , and the explanation ( a man with a telescope ) is an object of the verb ( saw ) becomes ( a man ) is an object of the verb ( saw ) , because the head of `` saw a man with a telescope '' is `` saw a man '' , and the head of `` a man with a telescope '' is `` a man '' . The difference of the two alternatives are now : t ) The action ( Mary saw a man ) take place ( with a telescope ) ; 2 ) ( Mary ) is a subject of the action ( saw a man ) , ( a man ) is ( with a telescope ) ; In the example system we have discussed above , each rule generates exactly one explanation.. In general , multiple explanations ( including zero ) can be generated by each rule. For example , rule &lt; b ) S -- &gt; NP + VP + PP should have two explanation templates : ( 1 ) ts a subject of Lhe acLton ( 2 ) The actton ( 1 2 ) takes place ( 3 ) , whereas rule &lt; a &gt; S -- &gt; NP + VP should have only one explanation template : ( 1 ) `` Is a subject of the actton ( 2 ) .</sentence>
				<definiendum id="0">GREEN TREE WITH</definiendum>
				<definiens id="0">A GIRL '' and the head of `` saw A GREEN `` IREE ''</definiens>
				<definiens id="1">a subject of the action ( saw a man with a telescope</definiens>
				<definiens id="2">a subject of the action ( saw a man ) , and the explanation ( a man with a telescope ) is an object of the verb ( saw ) becomes ( a man ) is an object of the verb</definiens>
				<definiens id="3">a subject of the action ( saw a man</definiens>
			</definition>
			<definition id="2">
				<sentence>477 Rule Ilead Explanation Iemplate &lt; a &gt; \ [ 1 2\ ] ( t ) is a subject of the action ( 2 ) &lt; b &gt; \ [ t 2\ ] ( 1 ) is a subject of the action ( 2 ) The action ( 1 2 ) takes place ( 3 ) &lt; c &gt; \ [ t\ ] &lt; &lt; none &gt; &gt; &lt; d &gt; \ [ t 2\ ] ( 1 ) is a determiner of ( 2 ) &lt; e &gt; \ [ 1\ ] ( 1 ) is ( 2 ) &lt; f &gt; It 2\ ] ( 1 ) is a preposition of ( 2 ) &lt; g &gt; \ [ t 2\ ] ( 2 ) is an object of the verb ( 1 ) With the ideas of head and multiple explanation , the system builds the following two explanation lists from the sentence `` Mary saw a man with a telescope '' .</sentence>
				<definiendum id="0">2 )</definiendum>
				<definiens id="0">an object of the verb ( 1 ) With the ideas of head and multiple explanation , the system builds the following two explanation lists from the sentence `` Mary saw a man with a telescope ''</definiens>
			</definition>
			<definition id="3">
				<sentence>The Qlist is a list of explanations Qlist = { e I , e 2 ... .. en } which is shown to the user to ask a question as follows : t ) e I 2 ) e 2 n ) e n Number ?</sentence>
				<definiendum id="0">Qlist</definiendum>
			</definition>
			<definition id="4">
				<sentence>C is a set of covered explanation lists in A , and U is a set of uncovered explanation lists in A. • 1-3 : initialization , let Olisl be empty .</sentence>
				<definiendum id="0">C</definiendum>
				<definiendum id="1">U</definiendum>
				<definiens id="0">a set of covered explanation lists in A , and</definiens>
			</definition>
</paper>

		<paper id="1116">
			<definition id="0">
				<sentence>Where translation is a fact of life rather than an oddity , it is realized that any translator 's competence is sharply restricted to a few domains ( this is especially true of technical areas ) , and that native fluency in a foreign language does not bestow on one the ability to serve as a translator .</sentence>
				<definiendum id="0">translation</definiendum>
				<definiens id="0">a foreign language does not bestow on one the ability to serve as a translator</definiens>
			</definition>
			<definition id="1">
				<sentence>Ih~HT refers to a system wherein the computer is responsible for producing the translation per se , but may interact with a human monitor at many stages along the way -for example , asking the human to disambiguate a word 's part of speech or meaning , or to indicate where to attach a phrase , or to choose a translation for a word or phrase from among several candidates discovered in the system 's dictionary .</sentence>
				<definiendum id="0">Ih~HT</definiendum>
			</definition>
			<definition id="2">
				<sentence>M ( A ) T systems lack style , but excel at terminology : they are best suited for technical translation .</sentence>
				<definiendum id="0">M</definiendum>
			</definition>
			<definition id="3">
				<sentence>`` Indirect translation , '' on the other hand , is characteristic of a system ( e.g. , EUROTRA ) wherein the analysis of the source language and the synthesis of the target language are totally independent processes ; for example , disambiguntion is performed to the extent necessary to determine the `` meaning '' ( however represented ) of the source language input , irrespective of which target language ( s ) that input might be translated into .</sentence>
				<definiendum id="0">Indirect translation</definiendum>
				<definiens id="0">characteristic of a system ( e.g. , EUROTRA ) wherein the analysis of the source language and the synthesis of the target language are totally independent processes ; for example</definiens>
			</definition>
			<definition id="4">
				<sentence>`` Global scope '' characterizes a system ( e.g. , METAL ) in which the meaning of a word is determined by its context within a unified analysis of the sentence ( or , rarely , paragraph ) .</sentence>
				<definiendum id="0">Global scope</definiendum>
				<definiendum id="1">METAL )</definiendum>
				<definiens id="0">in which the meaning of a word is determined by its context within a unified analysis of the sentence</definiens>
			</definition>
			<definition id="5">
				<sentence>LATSEC soon developed the SYSTRAN system ( based on GAT technology ) , which in 1970 replaced the IBM Mark II system at the USAF Foreign Technology Division ( FTD ) at Wright Patterson AYB , and in 1976 replaced GAT at EURATOM .</sentence>
				<definiendum id="0">SYSTRAN system</definiendum>
				<definiens id="0">GAT technology ) , which in 1970 replaced the IBM Mark II system at the USAF Foreign Technology Division ( FTD ) at Wright Patterson AYB , and in 1976 replaced GAT at EURATOM</definiens>
			</definition>
			<definition id="6">
				<sentence>Though U.S. government interest once again languished , the Sprachendienst ( Language Services ) department of Siemens b~ in Munich had begun supporting the project , and in 1980 Siemens AG became the sole sponsor .</sentence>
				<definiendum id="0">Sprachendienst</definiendum>
				<definiens id="0">Language Services ) department of Siemens b~ in Munich had begun supporting the project , and in 1980 Siemens AG became the sole sponsor</definiens>
			</definition>
			<definition id="7">
				<sentence>To MT system users , acceptability is a function of the amount of preand/or post-editing that must be done ( which is also the greatest determinant of cost ) .</sentence>
				<definiendum id="0">acceptability</definiendum>
				<definiens id="0">a function of the amount of preand/or post-editing that must be done ( which is also the greatest determinant of cost )</definiens>
			</definition>
			<definition id="8">
				<sentence>SYSTRAN SYSTRAN was one of the first MT systems to be marketed ; the first installation replaced the IBM Mark II Russian-English system at the USAF FTD in 1970 , and is still operational , Eased on the CAT technology ( SYSTRAN uses the same linguistic strategies , to the extent they can be argued to exist ) , SYSTRAN 's software basis has been much improved by the introduction of modularity ( separating the analysis and synthesis stages ) , by a recent shift away from simple `` direct '' translation ( from the Source Language straight into the Target Language ) toward the inclusion of something resembling an intermediate `` transfer '' stage , and by the allowance of manually-selected topical glossaries ( essentially , dictionaries specific to \ [ the subject area of\ ] the text ) .</sentence>
				<definiendum id="0">SYSTRAN SYSTRAN</definiendum>
				<definiendum id="1">SYSTRAN</definiendum>
				<definiens id="0">one of the first MT systems to be marketed ; the first installation replaced the IBM Mark II Russian-English system at the USAF FTD in 1970</definiens>
				<definiens id="1">much improved by the introduction of modularity ( separating the analysis and synthesis stages ) , by a recent shift away from simple `` direct '' translation ( from the Source Language straight into the Target Language ) toward the inclusion of something resembling an intermediate `` transfer '' stage</definiens>
			</definition>
			<definition id="9">
				<sentence>The USAF FTD dictionaries number over a million entries ; Eostad \ [ 82\ ] reports that dictionary updating must be severely constrained , lest a change to one entry disrupt the activities of many others .</sentence>
				<definiendum id="0">USAF FTD</definiendum>
				<definiens id="0">dictionaries number over a million entries</definiens>
			</definition>
			<definition id="10">
				<sentence>NASA selected SYSTRAN in 1974 to translate materials relating to the Apollo-Soyuz collaboration , and EURATOM replaced GAT with SYSTRAN in 1976 .</sentence>
				<definiendum id="0">NASA</definiendum>
				<definiens id="0">selected SYSTRAN in 1974 to translate materials relating to the Apollo-Soyuz collaboration , and EURATOM replaced GAT with SYSTRAN in 1976</definiens>
			</definition>
			<definition id="11">
				<sentence>This approach is not necessarily feasible for all organizations , but Xerox is willing to employ it and claims it also enhances source-text clarity .</sentence>
				<definiendum id="0">Xerox</definiendum>
				<definiens id="0">willing to employ it and claims it also enhances source-text clarity</definiens>
			</definition>
			<definition id="12">
				<sentence>METEO TAUM-METEO is the world 's only example of a truly fully-automatic MT system .</sentence>
				<definiendum id="0">METEO TAUM-METEO</definiendum>
			</definition>
			<definition id="13">
				<sentence>METEO scans the network traffic for English weather reports , translates them `` directly '' into French , and sends the translations back out over the communications network automatically .</sentence>
				<definiendum id="0">METEO</definiendum>
				<definiens id="0">scans the network traffic for English weather reports , translates them `` directly '' into French , and sends the translations back out over the communications network automatically</definiens>
			</definition>
			<definition id="14">
				<sentence>EUROTRA is a true multi-national development project .</sentence>
				<definiendum id="0">EUROTRA</definiendum>
				<definiens id="0">a true multi-national development project</definiens>
			</definition>
			<definition id="15">
				<sentence>Translation here is seen as just one application of such a system : the system `` understands '' natural language input , then `` generates '' natural language output ; if the languages happen to be different , then translation has been performed via paraphrase .</sentence>
				<definiendum id="0">Translation</definiendum>
				<definiens id="0">the system `` understands '' natural language input , then `` generates '' natural language output</definiens>
			</definition>
</paper>

		<paper id="1086">
			<definition id="0">
				<sentence>J-CAT=Verb J-LEX ffi It~ `` f ~ ( increase ) J-DEEP-CASE = MAIN J-GAPffi ' ( SOUrce GOAl ) ' J-SEI~WENCE -CONNECTOR = DECLARATIVE J-SENTENCE-RELATION = NIL J.SEh~I'ENCE-END © NIL J-DEEP .</sentence>
				<definiendum id="0">J-CAT=Verb J-LEX ffi It~</definiendum>
			</definition>
			<definition id="1">
				<sentence>The number i attached to verbs like form-l , produce2 is the i-th usage of the verb .</sentence>
				<definiendum id="0">produce2</definiendum>
				<definiens id="0">the i-th usage of the verb</definiens>
			</definition>
			<definition id="2">
				<sentence>/\ A B A B ( SUB ) ( COM ) ( OBJ ) ( COM ) NARU \ [ J-VP=V2 \ ] /\ A B ( suB ) ( GOAL ) provide \ [ B. J-SEM=CE\ ] ) '' // X CE : means , equipment A B ( AGT ) ( OBJ ) reach MU : unit A B ( OBJ ) ( STO ) `` B. J-CAT=ADJ ~ bzec°~e J-LEX= % '~ ( easy ) |~ / k I= &lt; ~ ( diffiI A B cult ) J ( OBJ ) ( GOAL ) turn \ [ B. J-SEM=IT , IC\ ] ~ / IT : theory , method A B IC : conceptual ( OBJ ) ( GOAL ) object get B : complement marke I ( OBJ ) ( GOAL ) become , default\ ] .</sentence>
				<definiendum id="0">B A B ( SUB</definiendum>
				<definiendum id="1">B ( OBJ )</definiendum>
				<definiendum id="2">OBJ )</definiendum>
				<definiens id="0">B cult ) J ( OBJ ) ( GOAL ) turn \ [ B. J-SEM=IT</definiens>
			</definition>
</paper>

		<paper id="1111">
</paper>

		<paper id="1105">
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>~lJ ( Recognition-Thought ) ( Part ) ( Attrl~te ) ~ m @ ( Part ) • t `` ~ ( ELlee.t-Contemt ) ~ ~1 ( Property-Character t st Ic ) ) Bt~ -- -- -~ AF i\ ] BS ( For= .</sentence>
				<definiendum id="0">~lJ ( Recognition-Thought )</definiendum>
			</definition>
			<definition id="1">
				<sentence>( Space-Topography ) ( Tile-SPace ) I ~'~1~-~1 I TP 'iB~J~ ( Tile Point ) ( Tile ) / TO ~l~mm u ( Tile Ouration ) I ' J -TA ,1~ ( Tile Attrtbute~ Sem~nt~g__M~r ke~a_fo r _Np_u ns 44 This conceptual facet contains attributes : that is , properties , qualities or features representative of things .</sentence>
				<definiendum id="0">Space-Topography )</definiendum>
				<definiens id="0">contains attributes : that is , properties , qualities or features representative of things</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>In general , we can represent R 's reasoning about Q 's reaction to a simple direct response •Yes , B ( a ) ' , given Q believes `` Most Bs F= , in terms of the following default schema , using the notation introduced in \ [ 15 I. 135 told { ILQ , l~ ( c ) ) k ( Most x ) \ [ B ( x ) = F ( x ) \ ] &amp; -~h : ,ld ( R , Q , -~Flc ) ) : M ( F\ [ c } ) .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">R 's reasoning about Q 's reaction to a simple direct response •Yes</definiens>
			</definition>
			<definition id="1">
				<sentence>i~lat Q is in situation SI and truthfully responds to Q 's request by simply telling him P* , Q may falsely conclude that P* is a general procedure for achieving $ 2 .</sentence>
				<definiendum id="0">i~lat Q</definiendum>
				<definiens id="0">in situation SI and truthfully responds to Q 's request by simply telling him P*</definiens>
			</definition>
			<definition id="2">
				<sentence>ion needed for a direct response is contained in RBc , a.~ is the information needed for many types of helpful responses .</sentence>
				<definiendum id="0">a.~</definiendum>
				<definiens id="0">the information needed for many types of helpful responses</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>Next , consider E3b and its associated affixed string E3*b. According to rule Rc , the G is a postfix that covers the affixes NP and G. Two occurrences of the prefix NP are available to be covered ; again , we may suppose that rule Rc picks out the nearer one .</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">a postfix that covers the affixes NP and G. Two occurrences of the prefix NP are available to be covered</definiens>
			</definition>
</paper>

		<paper id="1078">
			<definition id="0">
				<sentence>Superordinate substitution is the replacement of an etement with a noun or phrase that ps a .</sentence>
				<definiendum id="0">Superordinate substitution</definiendum>
				<definiens id="0">the replacement of an etement with a noun or phrase that ps a</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>Logic Programming as a mental aid , and Prolog ( Coelho , 1983 ; Clocksin &amp; Melish , 1981 ) and Extraposition Grammars ( Pereira , 1983 ) as practical tools , were adopted to implement a natural language interface for Portuguese .</sentence>
				<definiendum id="0">Logic Programming</definiendum>
				<definiens id="0">adopted to implement a natural language interface for Portuguese</definiens>
			</definition>
			<definition id="1">
				<sentence>SSIPA is a complex knowledge information processing system with natural language comprehension and synthesis capabilitites that interacts with users in Portuguese due to the linguistic knowledge that is logically organized and codified in the above mentioned SSIPA 's interface ca\ ] led LUSO .</sentence>
				<definiendum id="0">SSIPA</definiendum>
			</definition>
			<definition id="2">
				<sentence>As a matter of fact , SSIPA can create and delete files , fill them , change their names , list and change their , contents ; SSIPA receives , keeps and send messaqes answers questions not only about geography but also about the knowledge SSIPA represents ; it a grees or disagrees with the opinions stated byusers about the Knowledg~ context behind dialogues , reacts when users try to cheat it but , as a rule , SSIPA behaves as a helpful , deligent and cooperat~ ve interlocutor willing to serve human users , chan ging from one to another topic of conversation and developing intelligent clarification dialogues ( Lo pes , 1984 ) .</sentence>
				<definiendum id="0">SSIPA</definiendum>
				<definiens id="0">delete files , fill them , change their names , list and change their , contents ; SSIPA receives , keeps and send messaqes answers questions not only about geography but also about the knowledge SSIPA represents ; it a grees or disagrees with the opinions stated byusers about the Knowledg~ context behind dialogues , reacts when users try to cheat it but , as a rule</definiens>
			</definition>
			<definition id="3">
				<sentence>A Logical Form is here understood as a sequence of predicates , envelopes for knowledge transportation from users to SSIPA central processing unit ( the EVENT DRIVER ) and from this unit to users .</sentence>
				<definiendum id="0">Logical Form</definiendum>
			</definition>
			<definition id="4">
				<sentence>LUSO is a natural language interface that concentrates linguistic expert knowledge about Pot tuguese language .</sentence>
				<definiendum id="0">LUSO</definiendum>
				<definiens id="0">a natural language interface that concentrates linguistic expert knowledge about Pot tuguese language</definiens>
			</definition>
</paper>

		<paper id="1054">
</paper>

		<paper id="1101">
			<definition id="0">
				<sentence>ROBUST PROCESSING IN MACHINE TRANSLATION This paper is an attempt to provide part of the basis for a general theory of robust processing in Machine Translation ( MT ) with relevance to other areas of Natural Language Processing ( NLP ) .</sentence>
				<definiendum id="0">ROBUST PROCESSING IN MACHINE TRANSLATION This paper</definiendum>
			</definition>
			<definition id="1">
				<sentence>Clearly , the notion of error or correctness of P depends on the independent standard provided by T and R. If , for the sake of simplicity we ignore the possibility of ambiguous inputs here , we can define correctness thus : ( 1 ) Given P ( x ) =y , and a set W such that ~or all w in W , R ( w ) =y , then y is correct with respect to R and w iff x is a member of W. Intuitively , W is the set of items for which y is the correct representation according to R. One possible source of errors in P would be if P correctly implemented T , but T did not embody R. Clearly , in this case , the only sensible solution is to modify T. Since we can imagine no automatic way of finding such errors and doing this , we will 472 ignore this possibility , end assume that T is a we11-defined , correct and complete embodiment of R. We can thus replace R by T in ( I ) , and treat T as the standard of correctness below .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">the set of items for which y is the correct representation according to R. One possible source of errors in P would be if P correctly implemented T , but T did not embody R. Clearly</definiens>
				<definiens id="1">a we11-defined , correct and complete embodiment of R. We can thus replace R by T in ( I )</definiens>
			</definition>
			<definition id="2">
				<sentence>Problem ( ii ) : where P is a correct implementation so far as it goes , but is incomplete , so that the domain of P is a proper-subset of the domain of T. This will also be very common : in reality processes are often faced with inputs that violate the expectations implicit in an implementation .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a correct implementation so far as it goes , but is incomplete , so that the domain of P is a proper-subset of the domain of T. This will also be very common : in reality processes are often faced with inputs that violate the expectations implicit in an implementation</definiens>
			</definition>
			<definition id="3">
				<sentence>The most likely result will he robust P will now produce errors of the third type : case ( c ) : P ( x ) =y , where y is a legal output for P according to T , but is not the intended output according to T. i.e. y is in the range of T , but yqT ( x ) .</sentence>
				<definiendum id="0">y</definiendum>
				<definiendum id="1">yqT</definiendum>
				<definiens id="0">a legal output for P according to T , but is not the intended output according to T. i.e. y is in the range of T , but</definiens>
			</definition>
			<definition id="4">
				<sentence>Suppose both input x and output y of some process are legal objects , it nevertheless does not follow that they have been correctly paired by the process : e.g.in the case of a parsing process , x may be some sentence and y some representatiom Obviously , the fact that x and y are legal objects for the parsing process and that y is the output of the parser for input x does not guarantee that y is a correct representation of x. Of course , robust processing should be resistant to this kind of malfunctloning also .</sentence>
				<definiendum id="0">y</definiendum>
				<definiens id="0">e.g.in the case of a parsing process , x may be some sentence and y some representatiom Obviously , the fact that x and y are legal objects for the parsing process</definiens>
			</definition>
			<definition id="5">
				<sentence>( ii ) That construction of p-1 is somehow more straightforward than construction of P , so that p-i is likely to be more reliable ( correct and complete ) than P. In fact this is not implausible for some applications ( e.g. consider the case where P is a parser : it is a widely held idea that generators are easier to build than parsers ) .</sentence>
				<definiendum id="0">P</definiendum>
			</definition>
</paper>

		<paper id="1096">
</paper>

		<paper id="1115">
			<definition id="0">
				<sentence>The term `` information linguistics '' refers to a scientific discipline combining the fields of linguistic data processing , applied computer science , linguistics , artificial intelligence , and information science .</sentence>
				<definiendum id="0">information linguistics</definiendum>
				<definiens id="0">a scientific discipline combining the fields of linguistic data processing , applied computer science , linguistics , artificial intelligence , and information science</definiens>
			</definition>
			<definition id="1">
				<sentence>the following relation is in I/R ( -values ) &gt; I/A &gt; F/R &gt; F/A There are some exceptions to this general rule , such as Re-I/A &gt; I/R ( both in Set A and Set C ) ; Ha-F/R &gt; I/R ( in Set C ) ; ( Re-F/R ant F/A ) &gt; I/R -- ( in Set_C ) ; and I1-F/R &gt; ~/R ( both in Set_A and SetC ) .</sentence>
				<definiendum id="0">Ha-F/R &gt; I/R</definiendum>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>, the \ [ ) re~ { ent version ( see : 'l. &lt; ite\ ] - , Sgall an/ qgall , in } ~ress ) w~rks : :ith the notion of basic .\ ] e } ) endency structure ( 5DR ) , ; hich is defined a~ \ ] structure over the alohabet A ( corres\~onding to tne labels of no~les ) and the set of sy~ , ~ools C ( corres~onding.to the labels of e'lqes ) . 'i'he set of 5Drs is the sec of the tectogra : unatical representations of sentences containing no coordinated structures. 'fi % e ~-\ ] Dq s are generated by the gra : ,~.~ar G = ( V. , V ,5 , q ) , where V = A ka C , A = { ( a ~ , , ~ ) \ ] , a is inT terpreted as a lexical unit , g is a variaole standing for t and f ( contextually bound and non-bound , res~ectively\ ] an. , ~ is internreted as a set of &lt; Ira , ~ , ~aten~es belonging to a ; C is a '~et of com~ ) lementations ( c ~ C , where c is an inter ; or denoting a certain type of comi~ler.'entation , called a functor ) , C '' lenotes the set \ [ &lt; , &gt; , % , &gt; c~ for uvery C ~ C. % 'o reuresent coordination , the formal a~paratus for sentence generation is to be complemented by another aluhabet Q , .</sentence>
				<definiendum id="0">C</definiendum>
				<definiendum id="1">c</definiendum>
				<definiens id="0">:ith the notion of basic .\ ] e } ) endency structure ( 5DR ) , ; hich is defined a~ \ ] structure over the alohabet A ( corres\~onding to tne labels of no~les ) and the set of sy~ , ~ools C ( corres~onding.to the labels of e'lqes</definiens>
				<definiens id="1">unatical representations of sentences containing no coordinated structures. 'fi % e ~-\ ] Dq s are generated by the gra : ,~.~ar G = ( V. , V ,5 , q ) , where V = A ka C , A = { ( a ~ , , ~ ) \ ] , a is inT terpreted as a lexical unit , g is a variaole standing for t and f ( contextually bound and non-bound , res~ectively\ ] an.</definiens>
				<definiens id="2">an inter ; or denoting a certain type of comi~ler.'entation , called a functor ) , C '' lenotes the set \ [ &lt; , &gt; , %</definiens>
			</definition>
			<definition id="1">
				<sentence>Grammatemes representing morphological rleanin C in the narrow sense are specific for different ( semantic ) word classes : for nouns , w~ distinguish grammatemes of number an~ of delimitation ( indefinite , definite , specifying ) : for adjectives and adverbs , grammate~es of degree , for verbs , we work with grammatemes of aspect ( processual , complex , resultative ) , iterativehess ( iterative , non-iterative ) , tense ( simultaneous , anterior , posterior ) , im : nediateness ( immediate , non -- immediate ) , predicate modality ( indicative , Dossibilitive , necessitive , voluntative ) , assertive modality ( affirmative , negative ) , and sentential modality ( ieclarative , interrogative , imperative ) .</sentence>
				<definiendum id="0">sentential modality</definiendum>
				<definiens id="0">immediate , non -- immediate ) , predicate modality ( indicative , Dossibilitive , necessitive , voluntative ) , assertive modality ( affirmative , negative )</definiens>
			</definition>
			<definition id="2">
				<sentence>p~ex symbol of some node of the CDS ( i.e. label ( s ) of some node ( s ) in the Q-tree of the TR ) : ( a ) change of a grammateme : V exform-POssib ( Ndevice-ACt ) ( X-Pat ) ... == Vperform-lndic INdevice -Act ) ~X-Pat ) ... A0te : , In our highly simplified and schematic shapes of the rules we quote only thos~ labels of the nodes that are relevlnt for the rule in question ; the sign == stands for `` rewrite as '' ; Ndevice stands for any no~n , ,i % h the sem~ , ~t£u f=ature of `` device '' , Vperfor m for a verb with the semantic feature of action ve£b= , ~ossib and II~dic denote the |raimnatemes of predicate modality .</sentence>
				<definiendum id="0">CDS</definiendum>
				<definiens id="0">i.e. label ( s ) of some node ( s ) in the Q-tree of the TR ) : ( a ) change of a grammateme : V exform-POssib ( Ndevice-ACt ) ( X-Pat ) ... == Vperform-lndic INdevice -Act</definiens>
			</definition>
			<definition id="3">
				<sentence>Note : Act , Pat , Instr , Accomp , Regard stand for the functors of Actor , Patient , Instrument , Accompaniment and Regard , respectively ; D denotes a general participant , gen ~g~ change of the lexical part of the complex symbol accompanied by a change of some gramnlateme or functor : V.-Possibl ( ( few ) Ni ) ( V-use ( Nk-ACc°mpneg ) ... ) ... ==Vi-Necess ( ( most ) Ni ) ( V-use ( Nk-ACcompposit ) ... ) ... Ex. : With few hlgh-performance opera-~ional amplifiers it is possible to maintain a linear relationship between input and output without employing negative feedback .</sentence>
				<definiendum id="0">; D</definiendum>
				<definiendum id="1">Ex.</definiendum>
				<definiens id="0">Act , Pat , Instr , Accomp , Regard stand for the functors of Actor , Patient , Instrument , Accompaniment and Regard , respectively</definiens>
				<definiens id="1">a general participant , gen ~g~ change of the lexical part of the complex symbol accompanied by a change of some gramnlateme or functor : V.-Possibl ( ( few ) Ni ) ( V-use ( Nk-ACc°mpneg ) ... ) ... ==Vi-Necess ( ( most ) Ni ) ( V-use ( Nk-ACcompposit ) ... ) ...</definiens>
			</definition>
			<definition id="4">
				<sentence>EX. : Since an operational amplifier i-~ designed to perform mathematical operations , such basic operations as ... are performed readily .</sentence>
				<definiendum id="0">EX.</definiendum>
				<definiens id="0">Since an operational amplifier i-~ designed to perform mathematical operations , such basic operations as ... are performed readily</definiens>
			</definition>
</paper>

		<paper id="1091">
			<definition id="0">
				<sentence>We summarize the evidence for these possible correspondences in an TxS matrix , where T is the number of different characters found in the Titla wordllst , S that in the Sese wordllst .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">the number of different characters found in the Titla wordllst</definiens>
			</definition>
</paper>

		<paper id="1076">
			<definition id="0">
				<sentence>The Request schema is one of about 25 schemas in the current version of RST .</sentence>
				<definiendum id="0">Request schema</definiendum>
				<definiens id="0">one of about 25 schemas in the current version of RST</definiens>
			</definition>
			<definition id="1">
				<sentence>The nuclear part is the one whose function most nearly represents the function of the text span analyzed in the structure by using the schema .</sentence>
				<definiendum id="0">nuclear part</definiendum>
				<definiens id="0">the function of the text span analyzed in the structure by using the schema</definiens>
			</definition>
			<definition id="2">
				<sentence>The concession relation relates the conceded conceptual span to the conceptual span which the writer is emphasizing .</sentence>
				<definiendum id="0">concession relation</definiendum>
				<definiens id="0">relates the conceded conceptual span to the conceptual span which the writer is emphasizing</definiens>
			</definition>
</paper>

		<paper id="1095">
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>ABSTRACT We discuss how a deductive question-answering system can represent the beliefs or other cognitive states of users , of other ( interacting ) systems , and of itself .</sentence>
				<definiendum id="0">ABSTRACT We</definiendum>
				<definiens id="0">discuss how a deductive question-answering system can represent the beliefs or other cognitive states of users , of other ( interacting ) systems , and of itself</definiens>
			</definition>
			<definition id="1">
				<sentence>Lucy is young ( m3 ) , John believes that someone named 'Lucy ' is rich ( mlS ) , and John 's Lucy is the system 's Lucy ( m16 ) .</sentence>
				<definiendum id="0">Lucy</definiendum>
				<definiendum id="1">Lucy</definiendum>
			</definition>
</paper>

		<paper id="1094">
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>For example , English has a rich vocabulary of values for a relation called Ma~n ( from Latin magnus ) that denotes the superlative degree of its argument : Magn ( sit ) = ti6ht , Magn ( black ) =Jet , pitch , coal , Magn ( left ) = hard , Magn -- -~ay ) = for all you 're worth , and on and on .</sentence>
				<definiendum id="0">Magn</definiendum>
				<definiens id="0">a rich vocabulary of values for a relation called Ma~n ( from Latin magnus ) that denotes the superlative degree of its argument : Magn ( sit ) = ti6ht , Magn ( black ) =Jet , pitch , coal , Magn ( left ) = hard ,</definiens>
			</definition>
			<definition id="1">
				<sentence>Qu~i 1.1 'house ' in the sense of 'habitation of humans ' -- ~ersus 'stable ' or 'lair ' or 'hangar ' 1.2 and 'ranch ' 1.3 ) is pretty well defined by the function S_ , substantive of the second actant , plus the head v~rb ca/tel 1.2 'live in a house ' ( versus 'be sitting somewhere ' , 1,1 and 'live in a locality ' of its own , includin @ the antonym set given earlier , and only one of those functions matches one of the nine that are associated with ca/tel 1.2 : S. ( ca/tei 1.2 ) = S 2 ( ~u~i 1.1 ) = ~u~ 'inhabitant , householder ' .</sentence>
				<definiendum id="0">Qu~i 1.1 'house</definiendum>
				<definiens id="0">' in the sense of 'habitation of humans ' -- ~ersus 'stable ' or 'lair ' or 'hangar ' 1.2 and 'ranch ' 1.3 ) is pretty well defined by the function S_ , substantive of the second actant , plus the head v~rb ca/tel 1.2 'live in a house ' ( versus 'be sitting somewhere ' , 1,1 and 'live in a locality ' of its own , includin @ the antonym set given earlier , and only one of those functions matches one of the nine that are associated with ca/tel 1.2 : S. ( ca/tei 1.2 ) = S 2 ( ~u~i 1.1 ) = ~u~ 'inhabitant , householder '</definiens>
			</definition>
</paper>

		<paper id="1042">
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>Each rule consists of a rule number , a phrase structure rule , and a semantic ( logical translation ) rule .</sentence>
				<definiendum id="0">semantic</definiendum>
				<definiens id="0">a rule number , a phrase structure rule</definiens>
			</definition>
			<definition id="1">
				<sentence>A sentence is a noun phrase and a verb phrase .</sentence>
				<definiendum id="0">sentence</definiendum>
				<definiens id="0">a noun phrase and a verb phrase</definiens>
			</definition>
			<definition id="2">
				<sentence>~n -the latter said after an application of the universal grinder ) 2 or an a followed by a noun ( as in John is a man or Claret is aq wine ) or is an entire noun phrase ( as in John is the man most likely to succeed or Claret is ~ favourite red wine ) .</sentence>
				<definiendum id="0">Claret</definiendum>
				<definiens id="0">a man or</definiens>
			</definition>
			<definition id="3">
				<sentence>A noun phrase is either a bare noun ( as in Claret is a dry red wine or Dogs are barking outside ) or else is a quantified term ( as in All men are mortal or Sm red wine is tasty -- we include as determiners this , all , some , sin , much , little , each , every , and the numeral quantifiers ) .</sentence>
				<definiendum id="0">noun phrase</definiendum>
				<definiens id="0">a dry red wine or Dogs are barking outside ) or else is a quantified term</definiens>
			</definition>
			<definition id="4">
				<sentence>In addition to the sorts of examples stated above , both these theories can generate and give the correct logical form to such sentences as Wine is wine ( two readings , both analytic ) Wine is a wine ( false ) All wine is wine ( analytic ) Claret is a wine ( true ) Cheap wine is a wine ( false ) *All wine is a wine ( semantically anomalous ) Water is dripping from the faucet ( entails : sm water is dripping from the faucet ) Water is a liquid ( entails : water is liquid ) Both theories make the following six inferences valid i. Claret is a wine , wine is a liquid , so claret is a liquid And they both make these two inferences invalid liquid We know of no other theories which can do all these things .</sentence>
				<definiendum id="0">wine</definiendum>
				<definiendum id="1">*All wine</definiendum>
				<definiendum id="2">sm water</definiendum>
				<definiendum id="3">Water</definiendum>
				<definiens id="0">a wine ( semantically anomalous</definiens>
				<definiens id="1">a wine</definiens>
			</definition>
			<definition id="5">
				<sentence>Given the above rules ( augmented with additional features such as number and person agreement features in rule i ) we are able to extend the capabilities of our parsers ( Schubert &amp; PeIletier 1982 ) so that they deliver logical form translations of sentences involving mass expressions .</sentence>
				<definiendum id="0">above rules</definiendum>
				<definiens id="0">able to extend the capabilities of our parsers ( Schubert &amp; PeIletier 1982 ) so that they deliver logical form translations of sentences involving mass expressions</definiens>
			</definition>
</paper>

		<paper id="1110">
			<definition id="0">
				<sentence>Graph grammars are a multidimensional generalization of linear string grammars .</sentence>
				<definiendum id="0">Graph grammars</definiendum>
			</definition>
			<definition id="1">
				<sentence>A relational graph ( r-graph ) RG is pair RG = ( EDGES , NP ) consisting of a set of edges EDGES , ARCSxNODESxNODES and a function liP that associates each node in EDGES to a set of labeled property values : tJP : NODESxPROPS - &gt; PVALUES PVALUES is the set of possible node property values .</sentence>
				<definiendum id="0">relational graph</definiendum>
				<definiendum id="1">PVALUES PVALUES</definiendum>
				<definiens id="0">pair RG = ( EDGES , NP ) consisting of a set of edges EDGES , ARCSxNODESxNODES and a function liP that associates each node in EDGES to a set of labeled property values : tJP : NODESxPROPS - &gt;</definiens>
				<definiens id="1">the set of possible node property values</definiens>
			</definition>
			<definition id="2">
				<sentence>Definition 1.2 ( r-production ) An r-production RP is a pair : RP = ( LS , RS ) LS ( left side ) and RS ( right side ) are r-graphs .</sentence>
				<definiendum id="0">Definition 1.2 ( r-production</definiendum>
				<definiendum id="1">r-production RP</definiendum>
				<definiens id="0">a pair : RP = ( LS , RS ) LS ( left side</definiens>
			</definition>
			<definition id="3">
				<sentence>Definition 1.3 ( direct r-derivation ) The direct r-derivation of r-graph H from r-graph G via an r-production RP = ( LS , RS ) is defined by the following algorithm : Algorithm 1.1 ( Direct r-derivation ) Input : An r-graph G and an r-production RP = ( LS , RS ) Output : An r-graph H derived via RP from G 517 PROCEDURE Di rect-r-deri vation : BEGIN IF RP is applicable to G ( see text ) THEN EDGES G : = EDGES G EDGESLs H : =GURS RETURN H ELSE RETURN `` Not applicable '' END Here U is an operation defined for two r-graphs RGI and RG2 as follows : H = RGI I~ RG2 i ff EDGES H = EDGESRG 1 U EDGESRG 2 and NPw ( ni , propj ) = NPDr .</sentence>
				<definiendum id="0">Definition 1.3 ( direct r-derivation</definiendum>
				<definiendum id="1">Direct r-derivation ) Input</definiendum>
				<definiens id="0">An r-graph G and an r-production RP = ( LS , RS ) Output : An r-graph H derived via RP from G 517 PROCEDURE Di rect-r-deri vation : BEGIN IF RP is applicable to G ( see text ) THEN EDGES G : = EDGES G EDGESLs H : =GURS RETURN H ELSE RETURN `` Not applicable '' END Here U is an operation defined for two r-graphs RGI and RG2 as follows : H</definiens>
			</definition>
			<definition id="4">
				<sentence>to Definition 1.4 ( r-graph gralnmar and r-graph language ) An r-graph grammar ( RGG ) is a pair : RGG = ( PROD , START ) PROD is a set of r-productions and START is a set of r-graphs .</sentence>
				<definiendum id="0">r-graph language</definiendum>
				<definiendum id="1">r-graph grammar</definiendum>
				<definiendum id="2">RGG</definiendum>
				<definiendum id="3">START ) PROD</definiendum>
				<definiendum id="4">START</definiendum>
				<definiens id="0">a pair : RGG = ( PROD ,</definiens>
				<definiens id="1">a set of r-productions</definiens>
				<definiens id="2">a set of r-graphs</definiens>
			</definition>
			<definition id="5">
				<sentence>Definition 1.5 ( controlled r-graph grammar ) A controlled r-graph grammar ( CRG ) is a pair : CRG = ( CG , RGG ) CG is an r-graph called control graph ( c-graph ) .</sentence>
				<definiendum id="0">CRG</definiendum>
				<definiens id="0">a pair : CRG = ( CG , RGG ) CG is an r-graph called control graph ( c-graph )</definiens>
			</definition>
			<definition id="6">
				<sentence>Definition 1.6 ( Controlled graph language ) A controlled graph language ( CGL ) corresponding to a controlled r-graph grammar CRG = ( CG , RGG ) is the set of r-graphs derived by the CG using the start graphs START and the productions of the grammar RGG .</sentence>
				<definiendum id="0">RGG )</definiendum>
				<definiens id="0">the set of r-graphs derived by the CG using the start graphs START and the productions of the grammar RGG</definiens>
			</definition>
			<definition id="7">
				<sentence>The morphological representation of a sentence consists of star-like morphological representations of the words ( fig .</sentence>
				<definiendum id="0">morphological representation of a sentence</definiendum>
			</definition>
			<definition id="8">
				<sentence>The semantic representatien of a sentence consists of a semantic deop case structure corresponding tc Lhe main verb .</sentence>
				<definiendum id="0">semantic representatien of a sentence</definiendum>
				<definiens id="0">consists of a semantic deop case structure corresponding tc Lhe main verb</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>A complex value is a collection of features , for example : Isgreement : r per $ °n : 3rdll Lnumber : sgJJ Lexical Functional Grammar ( LFG ) \ [ Kaplan and Bresnan , 83\ ] , Unification Grammar ( UG ) \ [ Kay , 79\ ] , Generalized Phrase Structure Grammar ( GPSG ) \ [ Gazdar and Pullum , 82l , among others , use complex features .</sentence>
				<definiendum id="0">Unification Grammar</definiendum>
				<definiendum id="1">Generalized Phrase Structure Grammar ( GPSG</definiendum>
				<definiens id="0">3rdll Lnumber : sgJJ Lexical Functional Grammar ( LFG ) \ [ Kaplan and Bresnan , 83\ ] ,</definiens>
			</definition>
			<definition id="1">
				<sentence>If some attribute appears both in A and B , then the value of that attribute in ( Unify A B ) is the unification of the two values .</sentence>
				<definiendum id="0">B )</definiendum>
				<definiens id="0">the unification of the two values</definiens>
			</definition>
			<definition id="2">
				<sentence>~\ ] Generalization seems to be a very useful notion for expressing how number and gender agreement works in coordinate noun phrases .</sentence>
				<definiendum id="0">Generalization</definiendum>
				<definiens id="0">seems to be a very useful notion for expressing how number and gender agreement works in coordinate noun phrases</definiens>
			</definition>
			<definition id="3">
				<sentence>The result is C. Now suppose that A , B , and C are all complex .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">C. Now suppose that A , B , and</definiens>
			</definition>
			<definition id="4">
				<sentence>c ' = { ( A C ) ( B C ) ) The new value of C , C ' , is a disjunction of tuples which can be , but have not yet been unified .</sentence>
				<definiendum id="0">C )</definiendum>
				<definiens id="0">a disjunction of tuples which can be</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>The quantification ambi~uit\ [ explosion problem Quantification is a complex phenomenon that occurs whenever a nominal and a verbal constituent are combined in such a way that the denotation of the verbal constituent is predicated of arguments supplied by the ( denotation of the ) nominal constituent .</sentence>
				<definiendum id="0">quantification ambi~uit\ [ explosion problem Quantification</definiendum>
				<definiens id="0">a complex phenomenon that occurs whenever a nominal and a verbal constituent are combined in such a way that the denotation of the verbal constituent is predicated of arguments supplied by the ( denotation of the ) nominal constituent</definiens>
			</definition>
			<definition id="1">
				<sentence>Partee 's analysis is in fact still rather crude ; a somewhat more refined analysis , which distinguishes group readings and readings with equally wide scope of the quantifiers , leads to 30 interpretations ( Bunt , in press ) .</sentence>
				<definiendum id="0">Partee 's analysis</definiendum>
				<definiens id="0">distinguishes group readings and readings with equally wide scope of the quantifiers , leads to 30 interpretations</definiens>
			</definition>
			<definition id="2">
				<sentence>Ambiguity resolution In a semantic analysis system which translates natural language expressions into formal representations , all disambiguation takes place during this translation .</sentence>
				<definiendum id="0">Ambiguity resolution</definiendum>
				<definiens id="0">translates natural language expressions into formal representations , all disambiguation takes place during this translation</definiens>
			</definition>
			<definition id="3">
				<sentence>DOMAIN : P ( x ) } ) For example , consider the representation of the readings of sentence ( I ) 'Five boats were lifted ' , with individual , collective , and weak and strong group distribution : ( 12a ) ( Az : ~z ) =5 ) ( { x ~ BOATS : LIFTED ( x ) } ) ( 12b ) ( ~z : ~ ( z ) &gt; l ) ( { x 6 ~ ( BOATS ) : LIFTED ( x ) } ) ( 12c ) ( Az : ~z ) =l ) ( { x q~ ( BOATS ) : LIFTED ( x ) } ) ( 12d ) ( Az : ~z ) =5 ) ( UBoATSD ( { X e BOATS U ~+ ( BOATS ) : LIFTED ( x ) } ) ) where~+ ( S ) denotes the set of plural subsets of S. The notation U ( D ) is used to represent the set of S ,1 those members of S occuring in D '' ; the precise definition is : ( 13 ) Us ( D ) = { xES : xED v ( B yED : x6y ) } Note that in all cases the quantification domain is closely related to the source in a way determined by the distribution .</sentence>
				<definiendum id="0">Az</definiendum>
				<definiendum id="1">Az</definiendum>
				<definiendum id="2">Az</definiendum>
				<definiendum id="3">where~+ ( S )</definiendum>
				<definiens id="0">For example , consider the representation of the readings of sentence ( I ) 'Five boats were lifted ' , with individual , collective</definiens>
				<definiens id="1">the set of plural subsets of S. The notation U ( D ) is used to represent the set of S ,1 those members of S occuring in D '' ; the precise definition is : ( 13 ) Us ( D ) = { xES : xED v ( B yED : x6y ) }</definiens>
			</definition>
</paper>

		<paper id="1112">
			<definition id="0">
				<sentence>They take these topoi to be of the form : ( To ) The more X is P , the more Y is Q. where 'X is P ' is the idea expressed by the original utterance , and 'Y is Q ' is the argumentative orientation ( the conclusion argued for by producing the original utterance in the particular situation in which it is uttered ) .</sentence>
				<definiendum id="0">'X</definiendum>
				<definiens id="0">the conclusion argued for by producing the original utterance in the particular situation in which it is uttered )</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , the analysis of ( 1 ) , ( i ' ) , and ( 2 ) suggests the following description of the argumentative aspects of but : in any utterance of P but Q , the presence of but Ii am talking here of normal situations , where expensiveness is a reason not to buy , while beauty is a reason to buy 2The idea is that it is not the degree of P-uess of X ( when this means something ) that makes Y ( more or less ) Q , but the degree to which the speaker believes X is P that entitles him ( her ) to believe ( more or less ) that Y is Q. 525 requires that the utterances of P and Q be interpreted as oriented towards opposite conclusions , indicates that the complex utterance is oriented towards the conclusion towards which Q is oriented .</sentence>
				<definiendum id="0">expensiveness</definiendum>
				<definiens id="0">the degree of P-uess of X ( when this means something ) that makes Y ( more or less ) Q , but the degree to which the speaker believes X is P that entitles him ( her ) to believe ( more or less</definiens>
			</definition>
			<definition id="2">
				<sentence>sL/Rl~\ [ represenL/ -- ~ ... . ~ `` ~P2\ [ | tati°n JIM2/ Ix -- / I Where| : S is a sentenc~l expresses what is presupposed P2 expresses what is asserted R1 expresses conditions on argumentation R2 expresses pre-argumentation M1 is a model representing P1 M2 is a model representing P2 .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">a model representing P1 M2 is a model representing P2</definiens>
			</definition>
			<definition id="3">
				<sentence>The form of these formulae ( which can certainly be improved ) is ~cl~ where c is a logical expression ( stan6i ~n for the pre-orientation ) and ~ is an index standing for the pre-argumentative value .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">a logical expression ( stan6i ~n for the pre-orientation</definiens>
			</definition>
			<definition id="4">
				<sentence>The formal description of but is following P1 ( X but Y ) : Pl ( x ) A PI ( Y ) P2 ( X but Y ) : P2 ( x ) ~ P2 ( Y ) R1 ( X but Y ) : Topos/R2 ( Y ) = ~Topos/R2 ( x ) R2 ( X but Y ) : R2 ( Y ) the ~ere the first expression says that what is presupposed by X but Y is the conjunction of what is presupposed by X and what is presupposed by Y ; the second expression says that what is asserted by X but Y is the conjunction of what is asserted by X and what is asserted by Y ; the third expression says that the topoi that can be selected are those which are such that their application to the respective pre-orientations of X and Y leads to opposite formulae ( i.e. such that the argumentative orientations of the corresponding utterances of X and Y are opposite ) ; the last expression says that the pre-orientation of X bu~Y is that of Y. Applying this description of but to ( 4 ) and ( 5 ) leads to the following description of ( 3 ) : pl ( 3 ) : H~ ( ~ ) + -- &gt; ~ ~ ) which corresponds to the actual interpretations of ( 3 ) .</sentence>
				<definiendum id="0">PI</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiendum id="2">Y</definiendum>
				<definiens id="0">the third expression says that the topoi that can be selected are those which are such that their application to the respective pre-orientations of X and Y leads to opposite formulae ( i.e. such that the argumentative orientations of the corresponding utterances of X and Y are opposite )</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>Counectloulst models form a class of spreading activation or active semantic network model .</sentence>
				<definiendum id="0">Counectloulst models</definiendum>
				<definiens id="0">form a class of spreading activation or active semantic network model</definiens>
			</definition>
			<definition id="1">
				<sentence>Each component consists of sets of neuron-llke units which can either excite or inhibit neighbouring nodes , and nodes in connected components .</sentence>
				<definiendum id="0">component</definiendum>
				<definiens id="0">consists of sets of neuron-llke units which can either excite or inhibit neighbouring nodes , and nodes in connected components</definiens>
			</definition>
			<definition id="2">
				<sentence>The actor focus represents the animate object in the agent case in the most recent sentence .</sentence>
				<definiendum id="0">actor focus</definiendum>
				<definiens id="0">the animate object in the agent case in the most recent sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>The schema level consists of a hierarchy of ever more abstract schemas .</sentence>
				<definiendum id="0">schema level</definiendum>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>Functional Unification Grammar provides an opportunity to encompass within one formalism and computational system the parts of machine translation systems that have usually been treated separately , natably analysis , transfer , and synthesis .</sentence>
				<definiendum id="0">Functional Unification Grammar</definiendum>
				<definiens id="0">provides an opportunity to encompass within one formalism and computational system the parts of machine translation systems that have usually been treated separately , natably analysis , transfer , and synthesis</definiens>
			</definition>
			<definition id="1">
				<sentence>A description is an expression over an essentially arbitrary basic vocabulary .</sentence>
				<definiendum id="0">description</definiendum>
			</definition>
			<definition id="2">
				<sentence>So Cat ~S is a feature of the most elementary type .</sentence>
				<definiendum id="0">Cat ~S</definiendum>
				<definiens id="0">a feature of the most elementary type</definiens>
			</definition>
			<definition id="3">
				<sentence>A FD is a Boolean expression over features .</sentence>
				<definiendum id="0">FD</definiendum>
				<definiens id="0">a Boolean expression over features</definiens>
			</definition>
			<definition id="4">
				<sentence>Grammars , which are the descriptions of the infinite sets of sentences that make up a language constitute a type of description that is structurally identical an ordinary FD but is distinguished on the grounds that it behaves slightly differently under unification .</sentence>
				<definiendum id="0">Grammars</definiendum>
				<definiens id="0">the descriptions of the infinite sets of sentences that make up a language constitute a type of description that is structurally identical an ordinary FD but is distinguished on the grounds that it behaves slightly differently under unification</definiens>
			</definition>
			<definition id="5">
				<sentence>X is built into the very fabric of the version of FUG illutrated here where , for example , a setence is by definition a phrase whose bead 's head is a verb .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">definition a phrase whose bead 's head is a verb</definiens>
			</definition>
</paper>

		<paper id="1085">
			<definition id="0">
				<sentence>The Dynamic Discourse Model , outlined in this paper , is a discourse grammar under development which analyses the structure of a discourse in order to be able to deal adequately with its semantic aspects .</sentence>
				<definiendum id="0">Dynamic Discourse Model</definiendum>
				<definiens id="0">a discourse grammar under development which analyses the structure of a discourse in order to be able to deal adequately with its semantic aspects</definiens>
			</definition>
			<definition id="1">
				<sentence>The Topic provides a frame which determines the interpretation of many lexical items and descriptions .</sentence>
				<definiendum id="0">Topic</definiendum>
				<definiens id="0">provides a frame which determines the interpretation of many lexical items and descriptions</definiens>
			</definition>
			<definition id="2">
				<sentence>The Speech Event provides a script which describes the conventional development of the discourse , and justifies assumptions about the purposes of discourse participants .</sentence>
				<definiendum id="0">Speech Event</definiendum>
			</definition>
			<definition id="3">
				<sentence>The Dynamic Discourse Model , therefore , must construct the semantic interpretation of a discourse on a clause by clause basis , from left to right , yielding intermediate semantic representations of unfinished constituents , as well as setting the semantic parameters whose values influence the interpretation of subsequent constituents .</sentence>
				<definiendum id="0">Dynamic Discourse Model</definiendum>
				<definiens id="0">setting the semantic parameters whose values influence the interpretation of subsequent constituents</definiens>
			</definition>
			<definition id="4">
				<sentence>A syntactic/semantic system of this sort may very well be fromulated as an Augmented Transition Network grammar ( Woods , 1970 ) , a non-deterministic parsing system specified by a set of transition networks which may call each other recursively .</sentence>
				<definiendum id="0">syntactic/semantic system</definiendum>
				<definiendum id="1">Augmented Transition Network grammar</definiendum>
				<definiens id="0">a non-deterministic parsing system specified by a set of transition networks which may call each other recursively</definiens>
			</definition>
			<definition id="5">
				<sentence>Narratives are subdivided into different genres , marked by different tense and/or person orientation of their main line clauses : specific past time narratives ( marked by clauses in the simple past , though clauses in the `` historical present '' may also occur ) , generic past time narratives ( marked by the use of `` would '' and `` used to '' ) , procedural narratives ( present tense ) , simultaneous reporting ( present tense ) , plans ( use of `` will '' and `` shall '' ; present tense also occurs ) .</sentence>
				<definiendum id="0">procedural narratives</definiendum>
				<definiens id="0">present tense ) , simultaneous reporting ( present tense ) , plans ( use of `` will '' and `` shall ''</definiens>
			</definition>
			<definition id="6">
				<sentence>Arcs have labels of the form `` A : B '' ( or sometimes just `` A '' ) , where A indicates the category of the constituent which must be parsed to traverse the arc , and B is a label identifying additional conditions and/or actions .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">a label identifying additional conditions and/or actions</definiens>
			</definition>
			<definition id="7">
				<sentence>A topic chain consists of a series of clauses C. , ... , C k , with a semantic structure of the form~ .</sentence>
				<definiendum id="0">topic chain</definiendum>
				<definiens id="0">consists of a series of clauses C. , ...</definiens>
			</definition>
			<definition id="8">
				<sentence>In the first clause of the chain , the topic is expressed by a phrase ( either a full NP or a pronoun ) which occurs in subject position or as a preposed constituent .</sentence>
				<definiendum id="0">phrase</definiendum>
				<definiens id="0">either a full NP or a pronoun ) which occurs in subject position or as a preposed constituent</definiens>
			</definition>
			<definition id="9">
				<sentence>Because of the pragmatic relation between the Discourse Unit and the surrounding talk ( specifically , the need to appear `` locally occasioned '' ( Jefferson , 1979 ) and to make a `` point '' ( Polanyi , 1978b ) , the central part of the Discourse Unit usually is not a piece of talk standing completely on its o~ feet , but is supported by one or more stages of preparatory and introductory talk on one end , and by an explicit closure and/or conclusion at the other .</sentence>
				<definiendum id="0">surrounding talk</definiendum>
			</definition>
			<definition id="10">
				<sentence>( Cf. Goffman , 1979 ) The set of participants of an Interaction determines the possible speakers and addressees of the talk occurring in it .</sentence>
				<definiendum id="0">Interaction</definiendum>
				<definiens id="0">determines the possible speakers and addressees of the talk occurring in it</definiens>
			</definition>
</paper>

		<paper id="1106">
			<definition id="0">
				<sentence>The understanding process in NALIG flows through several steps ( distinguishable only from a logic point of view ) , which perform object instantiation , relation inheritance , translation of the surface expression into unambiguous primitives , ( * ) NALIG has been developed for the Italian language ; the prepositions it can presently analyze are : su , sopra , sotto , a destra , a sinistra , vicino , davanti , dietro , in .</sentence>
				<definiendum id="0">dietro</definiendum>
				<definiens id="0">perform object instantiation , relation inheritance , translation of the surface expression into unambiguous primitives</definiens>
			</definition>
			<definition id="1">
				<sentence>Geographic objects ( GEO ( X ) ) impose a special care : `` the mountains on the lake '' can not be interpreted as the lake supporting the mountains and even if only B is a geographic object , but A can fly , physical contact seems not to be the most common inference ( `` the birds on the garden '' ) .</sentence>
				<definiendum id="0">Geographic objects</definiendum>
				<definiendum id="1">GEO</definiendum>
				<definiens id="0">a geographic object , but A can fly</definiens>
			</definition>
			<definition id="2">
				<sentence>Hence , a first tentative rule is the following ( the actual rule is much more complex ) : not GEO ( A ) and not ( FLYING ( A ) and not REPOSITORY ( A , B ) ) and ( ( FREETOP ( B ) and not GEO ( B ) ) or ( GEO ( B ) and not CANFLY ( A ) ) ) ===~ , H SUPPORT ( A , B ) A complete discussion of NALIG 's taxonomy of objects is in ( Bo83a ) .</sentence>
				<definiendum id="0">GEO</definiendum>
				<definiendum id="1">FLYING</definiendum>
				<definiendum id="2">REPOSITORY</definiendum>
				<definiendum id="3">B )</definiendum>
				<definiens id="0">A , B ) ) and ( ( FREETOP ( B ) and not GEO ( B ) ) or ( GEO ( B ) and not CANFLY ( A ) ) ) ===~</definiens>
			</definition>
			<definition id="3">
				<sentence>If a face f is a base face for A ( BASE ( f , A ) ) , it is possible to find the point e , which is the projection of the barlcenter of A on the plane containing f along its normal .</sentence>
				<definiendum id="0">BASE</definiendum>
				<definiens id="0">the projection of the barlcenter of A on the plane containing f along its normal</definiens>
			</definition>
			<definition id="4">
				<sentence>A support is a convex region of the upper surface of B ; it may coincide with the whole upper surface of B , as it happens with a table top , or with a limited subset of it , as a piece of the upper edge of the back of a chair .</sentence>
				<definiendum id="0">support</definiendum>
				<definiens id="0">a convex region of the upper surface of B ; it may coincide with the whole upper surface of B , as it happens with a table top , or with a limited subset of it , as a piece of the upper edge of the back of a chair</definiens>
			</definition>
			<definition id="5">
				<sentence>If the base f of a has the same kind of approximately radial simmetry ( a regular polygon could be a good approximation ) and if the projection c of the baricenter of A coincides with the center of f , then the supporting a is the circle with radius Ra under the condition r R , where r is the radius of the `` central hole '' in the pattern of supports and R is the ( minimal ) radius of f. This simply means that the most obvious positioning strategy is to center A with respect to the pattern of supports ; their actual shape is not important provided that they can be touched by A. In case of failure of equilibrium rules a lower number of supports must be considered and the radial simmetry is lost ( for instance , the case of a single support may be analyzed ) .</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">the radius of the `` central hole '' in the pattern of supports</definiens>
			</definition>
			<definition id="6">
				<sentence>An example is the phrase `` the book on the table '' , which is accepted by the logic module as H_SUPPORT ( book , table ) but can be rejected at this level if there is no enough free space on the table top , and therefore modified into a new relation H_SUPPORT ( book , B ) , where B is a suitable object which is known to be supported by the table and is transparent to respect the On relationship ( another book , for instance ) .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">a suitable object which is known to be supported by the table and is transparent to respect the On relationship</definiens>
			</definition>
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>For example , the first plan in Figure 1 summarizes a simple plan schema with a header `` BOARD ( agent , train ) , '' with parameters `` agent '' and `` train , '' and with the constraint `` depart-station ( train ) = Toronto . ''</sentence>
				<definiendum id="0">BOARD</definiendum>
			</definition>
			<definition id="1">
				<sentence>The plan consists of the HEADER : BOARD ( agent , train ) STEPS : do BUY-TICKET ( agent , train ) do GOTO ( agent , depart-location ( train ) , depart-time ( train ) ) do GETON ( agent , train ) CONSTRAINTS : depart-station ( train ) = Toronto ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... .. HEADER : GOTO ( agent , location , time ) EFFECT : AT ( agent , location , time ) ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... .. HEADER : MEET ( agent , train ) STEPS : do GOTO ( agent , arrive-location ( train ) , arrive-time ( train ) ) CONSTRAINTS : arrive-station ( train ) = Toronto Figure I : Domain Plans 303 shown .</sentence>
				<definiendum id="0">plan</definiendum>
			</definition>
			<definition id="2">
				<sentence>Thus , in the above formula , agent is restricted to entities capable of agency , term is a description of some object , and plan is restricted to objects that are plans .</sentence>
				<definiendum id="0">term</definiendum>
			</definition>
			<definition id="3">
				<sentence>For HEADER : SEEK-ID-PARAMETER ( agent , parameter , plan ) STEPS : achieve KNOWREF ( agent , parameter , plan ) CONSTRAINTS : IS-PARAMETER-OF ( parameter , plan ) ~KNOWREF ( agent , parameter , plan ) ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... .. HEADER : ASK ( agent , term , plan ) STEPS : do REQUEST ( agent , agent2 , INFORMREF ( agent2 , agent , term , plan ) , plan ) do INFORMREF ( agent2. , agent , term , plan ) EFFECTS : KNOWREF ( agent , term , plan ) CONSTRAINTS : ~KNOWREF ( agent , term , plan ) ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... .. Figure 2 : Metaplans example , the action INFORMREF ( agent , hearer , term , plan ) consists of the agent informing the hearer of a description of the term with the effect that KNOWREF ( hearer , term , plan ) .</sentence>
				<definiendum id="0">plan ) CONSTRAINTS</definiendum>
				<definiens id="0">SEEK-ID-PARAMETER ( agent , parameter , plan ) STEPS : achieve KNOWREF ( agent , parameter , plan ) CONSTRAINTS : IS-PARAMETER-OF ( parameter , plan ) ~KNOWREF ( agent , parameter</definiens>
				<definiens id="1">do REQUEST ( agent , agent2 , INFORMREF ( agent2 , agent , term , plan ) , plan ) do INFORMREF ( agent2. , agent , term , plan ) EFFECTS : KNOWREF ( agent , term ,</definiens>
				<definiens id="2">Metaplans example , the action INFORMREF ( agent , hearer , term , plan ) consists of the agent informing the hearer of a description of the term with the effect</definiens>
			</definition>
			<definition id="4">
				<sentence>As in Grosz \ [ 1977\ ] , the task structure guides the focus mechanism , which marks the currently executing subtask as focused .</sentence>
				<definiendum id="0">focus mechanism</definiendum>
				<definiens id="0">marks the currently executing subtask as focused</definiens>
			</definition>
			<definition id="5">
				<sentence>Similarly , our focus mechanism updates the current focus by knowing what kind of plan structure traversals correspond to coherent topic continuation .</sentence>
				<definiendum id="0">focus mechanism</definiendum>
				<definiens id="0">updates the current focus by knowing what kind of plan structure traversals correspond to coherent topic continuation</definiens>
			</definition>
			<definition id="6">
				<sentence>PLAN3 S-REQUEST ( Person1 , clerk1 , INFORMREF ( clerk1 , Person1 , loc ( Gate7 ) , PLAN2 ) SEEK-ID-PARAMETER ( Person1 , loc ( Gate7 ) , PLAN2 ) l ASK ( ~rsonl , loc ( Gate7~ ) , PLAN2 ) INFO-~MREF ( clerkl , Person1 , loc ( Gate7 ) , PLAN2 ) PLAN2 REQUEST ( Person1 , Clerk1 , INFORMREF ( Clerk1 , Person1 , depart-loc ( train1 ) , PLAN1 ) ) SEEK-ID-PARAMETER ( Person1 , depart-loc ( uainl ) , PLAN1 ) / A~ , ~nl , depart-loc~LAN1 ) INFORMREF ( Clerk1 , Person1 , depart-loc ( train1 ) , PLAN1 ) I S-INFORM ( Clerk1 , Person1 , equal ( depart-loc ( trainl ) , loc ( Gate7 ) ) ) PLAN1 ~ .</sentence>
				<definiendum id="0">PLAN3 S-REQUEST</definiendum>
				<definiens id="0">I S-INFORM ( Clerk1 , Person1 , equal ( depart-loc ( trainl ) , loc ( Gate7 ) )</definiens>
			</definition>
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>While we have found the explanation facility for the advisor system to be a valuable testbed for the surface generator , the generator is an independent module that can be transported to other domains by changing only the vocabulary .</sentence>
				<definiendum id="0">generator</definiendum>
				<definiens id="0">an independent module that can be transported to other domains by changing only the vocabulary</definiens>
			</definition>
			<definition id="1">
				<sentence>Immediate focus refers to how a speaker 's center of attention shifts or remains constant over two consecutive sentences , while global focus describes the effect of a speaker 's center of attention throughout a sequence of discourse utterances on succeeding utterances .</sentence>
				<definiendum id="0">Immediate focus</definiendum>
				<definiens id="0">how a speaker 's center of attention shifts or remains constant over two consecutive sentences , while global focus describes the effect of a speaker 's center of attention throughout a sequence of discourse utterances on succeeding utterances</definiens>
			</definition>
			<definition id="2">
				<sentence>Nigel ( Mann , 1983 ) is a fourth system that makes use of functional information and is based on systemic grammar ( Hudson , 1974 ) .</sentence>
				<definiendum id="0">Nigel</definiendum>
			</definition>
</paper>

		<paper id="1079">
			<definition id="0">
				<sentence>ABSTRACT KIPS is an automatic programming system which generates standardized business application programs through interactive natural language dialogue .</sentence>
				<definiendum id="0">ABSTRACT KIPS</definiendum>
				<definiens id="0">an automatic programming system which generates standardized business application programs through interactive natural language dialogue</definiens>
			</definition>
			<definition id="1">
				<sentence>Model To get a better understanding of the way users describe programs , we asked programmers to specify programs in a short paragraph , and sampled illustrative descriptions of simple programs from a Hyper COBOL user 's manual ( Fujitsu , 1981 ) ( Hyper COBOL is the target programming language of KIPS ) .</sentence>
				<definiendum id="0">Hyper COBOL</definiendum>
				<definiens id="0">the target programming language of KIPS</definiens>
			</definition>
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>Expressions of PAL are related to expressions of natural language as follows : Generally , when a phrase consists of its immediate descendants , say z and y , a PAL expression for the phrase is one of the following forms : &lt; z &gt; ( &lt; V &gt; ) or &lt; p &gt; ( &lt; z &gt; ) where ~a &gt; stands for a PAL expression for a phrase ~* .</sentence>
				<definiendum id="0">PAL expression for the phrase</definiendum>
				<definiendum id="1">~a &gt;</definiendum>
				<definiens id="0">related to expressions of natural language as follows : Generally , when a phrase consists of its immediate descendants</definiens>
				<definiens id="1">one of the following forms : &lt; z &gt; ( &lt; V &gt;</definiens>
			</definition>
			<definition id="1">
				<sentence>Consider the following sentence : z wants It to do z. This sentence implies that the subject of the infinitive is the grammatical object of the main verb `` wants ~ .</sentence>
				<definiendum id="0">infinitive</definiendum>
				<definiens id="0">the subject of the</definiens>
			</definition>
</paper>

		<paper id="1107">
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>• • Unix is a trademark of Bell Laboratories .</sentence>
				<definiendum id="0">Unix</definiendum>
			</definition>
</paper>

		<paper id="1104">
			<definition id="0">
				<sentence>ABSTRACT By integrating syntactic and semantic processing , our parser ( LAZY ) is able to deterministically parse sentences which syntactically appear to be garden path sentences although native speakers do not need conscious reanalysis to understand them .</sentence>
				<definiendum id="0">LAZY</definiendum>
			</definition>
			<definition id="1">
				<sentence>LAZY comprises an extension to conceptual analysis which yields an explicit representation of syntactic information and a flexible interaction between semantic and syntactic knowledge .</sentence>
				<definiendum id="0">LAZY</definiendum>
				<definiens id="0">comprises an extension to conceptual analysis which yields an explicit representation of syntactic information and a flexible interaction between semantic and syntactic knowledge</definiens>
			</definition>
			<definition id="2">
				<sentence>LAZY uses a language recognition scheme capable of waiting long enough to select the correct parse of both ( 1 ) and { 2 ) without guessing and backing up \ [ MARCUS 76\ ] .</sentence>
				<definiendum id="0">LAZY</definiendum>
				<definiens id="0">uses a language</definiens>
			</definition>
			<definition id="3">
				<sentence>`` Mary '' is the subject of the sentence and • Bill '' is the direct object .</sentence>
				<definiendum id="0">Mary</definiendum>
				<definiendum id="1">Bill</definiendum>
				<definiens id="0">the subject of the sentence and •</definiens>
			</definition>
			<definition id="4">
				<sentence>Although there are other types of GPs ( e.g. , imperative and yes/no questions with an initial `` have ' ) , we will only demonstrate how LAZY understands or misunderstands passive participle and main verb conflicts .</sentence>
				<definiendum id="0">LAZY</definiendum>
				<definiens id="0">understands or misunderstands passive participle and main verb conflicts</definiens>
			</definition>
			<definition id="5">
				<sentence>LAZY processes a sentence one word at a time from left to right .</sentence>
				<definiendum id="0">LAZY</definiendum>
				<definiens id="0">processes a sentence one word at a time from left to right</definiens>
			</definition>
			<definition id="6">
				<sentence>Dyer , M.G. , In-Depth Understanding : A Computer Model of Integrated Processing for Narrative Comprehension , Cambridge , MA : The MIT Press , 1083 .</sentence>
				<definiendum id="0">MA</definiendum>
				<definiens id="0">A Computer Model of Integrated Processing for Narrative Comprehension , Cambridge ,</definiens>
			</definition>
</paper>

		<paper id="1084">
			<definition id="0">
				<sentence>The system consists of a set of verb classes and Jnformatlon on mapping them together with noun phrases , tense and aspect , and \ ] s supposed to he utilized for inference in automatic text understanding .</sentence>
				<definiendum id="0">system</definiendum>
				<definiens id="0">consists of a set of verb classes and Jnformatlon on mapping them together with noun phrases , tense and aspect , and \ ] s supposed to he utilized for inference in automatic text understanding</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>Since it is already possible to trace these chains of hyponyms going upwards or downwards for more than one level , one can immediately ask whether , for example , MASSERIA belongs to the set of collectives even if it is defined as HANDRIA , because MANDRIA is defined as BRANCO , which is in turn defined as INSIENE , which finally is one of the nouns belonging to the class `` SET-OF '' .</sentence>
				<definiendum id="0">MASSERIA</definiendum>
			</definition>
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>Thts 'TO '' tS optional , but tn real texts one often places tt to make the scope unambiguous , especially when the second conjunct IS a long noun phrase and the scope is highly ambiguous without tt .</sentence>
				<definiendum id="0">Thts 'TO</definiendum>
				<definiens id="0">'' tS optional , but tn real texts one often places tt to make the scope unambiguous</definiens>
			</definition>
			<definition id="1">
				<sentence>JISSOKU°CHI ( actual value ) and YOSOKU-CH\ [ ( predtcted value ) , have the same morpheme `` CH\ [ `` ( whtch meams `` value '' ) .</sentence>
				<definiendum id="0">JISSOKU°CHI</definiendum>
				<definiens id="0">actual value ) and YOSOKU-CH\ [ ( predtcted value )</definiens>
			</definition>
</paper>

		<paper id="1113">
			<definition id="0">
				<sentence>The basic synthesis scheme consists of two steps : Generation of an excitation signal from pitch and galn contours and excitation of the linear system model described by linear prediction coefficients , We show that a number of basic studies such as time expansion/ compression , pitch modifications and spectral expansion/compression can be made to study the effect of these parameters on the quality of synthetic speech .</sentence>
				<definiendum id="0">basic synthesis scheme</definiendum>
			</definition>
			<definition id="1">
				<sentence>C. Synthesis Synthesis consists of two steps : Generation of the excitation signal and synthesis of speech .</sentence>
				<definiendum id="0">C. Synthesis Synthesis</definiendum>
				<definiens id="0">consists of two steps : Generation of the excitation signal and synthesis of speech</definiens>
			</definition>
</paper>

		<paper id="1066">
</paper>

		<paper id="1055">
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>ABSTRACT The Layered Domain Class system ( LDC ) is an experimental natural language processor being developed at Duke University which reached the prototype stage in May of 1983 .</sentence>
				<definiendum id="0">Layered Domain Class system</definiendum>
				<definiendum id="1">LDC )</definiendum>
				<definiens id="0">an experimental natural language processor being developed at Duke University which reached the prototype stage in May of 1983</definiens>
			</definition>
			<definition id="1">
				<sentence>I INTRODUCTION The Layered Domain Class system ( LDC ) is an experimental natural language processor being developed at Duke .</sentence>
				<definiendum id="0">Layered Domain Class system</definiendum>
				<definiendum id="1">LDC )</definiendum>
				<definiens id="0">an experimental natural language processor being developed at Duke</definiens>
			</definition>
			<definition id="2">
				<sentence>Second , the User-Phase Processor , which enables a user to obtain statistical reductions on his or her data by typed English inputs .</sentence>
				<definiendum id="0">User-Phase Processor</definiendum>
			</definition>
			<definition id="3">
				<sentence>Table 1 Modifier Types Available in LDC Modifier Type Example Usage Syntax Implemented Semantics Implemented Ordinal the second floor yes yes 3uperlative the largest office yes yes Anaphoric better students Comparative more desirable instructors yes no Adjective the large rooms classes that were small yes yes Anaphoric Argument-Taking Adjective adjacent offices yes no Anaphoric Implied-Parameter Verb failing students yes no Noun Modifier conference rooms yes yes Subtype offices yes yes Argument-Taking Noun classmates of Jim Jim 's classmates yes yes Anaphoric Argument-Taking Noun the best classmate yes no Prepositional Phrase students in CPS215 yes ( yes ) Comparative Phrase students better than Jim a higher grade than a C yes yes Trivial instructors who teach AI Verb Phrase students who took AI from Smith yes yes Implied-Parameter Verb Phrase students who failed AI yes yes Operational Verb Phrase students who outscored Jim yes yes Argument-Taking Adjective offices adjacent to X-238 yes yes Negations the non graduate students ( of many sorts ) offices not adjacent to X-23B instructors that did not teach M yes yes etc. 53 III KNOWLEDGE ACQUISITION FOR MODIFIERS The job of the knowledge acquisition module of LDC , called `` Prep '' in Figure 1 , is to ' find out about ( a ) the vocabulary of the new domain and ( b ) the composition of the physical data file .</sentence>
				<definiendum id="0">Prep</definiendum>
				<definiens id="0">yes Argument-Taking Noun classmates of Jim Jim 's classmates yes yes Anaphoric Argument-Taking Noun the best classmate yes no Prepositional Phrase students in CPS215 yes ( yes ) Comparative Phrase students better than Jim a higher grade than a C yes yes Trivial instructors who teach AI Verb Phrase students who took AI from Smith yes yes Implied-Parameter Verb Phrase students who failed AI yes yes Operational Verb Phrase students who outscored Jim yes yes Argument-Taking Adjective offices adjacent to X-238 yes yes Negations the non graduate students ( of many sorts ) offices not adjacent to X-23B instructors that did not teach M yes yes etc. 53 III KNOWLEDGE ACQUISITION FOR MODIFIERS The job of the knowledge acquisition module of LDC , called ``</definiens>
			</definition>
			<definition id="4">
				<sentence>For this reason , LDC restricts itself to a class of domains \ [ 1\ ] in which the important relationships among domain entities involve hierarchical decompositions .</sentence>
				<definiendum id="0">LDC</definiendum>
				<definiens id="0">restricts itself to a class of domains \ [ 1\ ] in which the important relationships among domain entities involve hierarchical decompositions</definiens>
			</definition>
			<definition id="5">
				<sentence>For this reason , Prep acquires the meanings of all user-defined modifiers in the same manner by providing such primitives as id , the identity function ; va2 , which retrieves a specified field of a record ; vzzern , which returns the size of its argument , which is assumed to be a set ; sum , which returns the sum of ' . '</sentence>
				<definiendum id="0">sum</definiendum>
				<definiens id="0">retrieves a specified field of a record</definiens>
			</definition>
			<definition id="6">
				<sentence>-s list of inputs ; aug , which returns the average of its list of inputs ; and pct , which returns the percentage of its list of boolean arguments which are true .</sentence>
				<definiendum id="0">aug</definiendum>
				<definiendum id="1">pct</definiendum>
				<definiens id="0">returns the average of its list of inputs</definiens>
				<definiens id="1">returns the percentage of its list of boolean arguments which are true</definiens>
			</definition>
			<definition id="7">
				<sentence>Thus , a `` desirable instructor '' might be defined as an instructor who gave a good grade to more than half his students , where a `` good grade '' is defined as a grade of B or above .</sentence>
				<definiendum id="0">desirable instructor</definiendum>
				<definiens id="0">an instructor who gave a good grade to more than half his students</definiens>
				<definiens id="1">a grade of B or above</definiens>
			</definition>
			<definition id="8">
				<sentence>Finally , the macro file contains the meanings of modifiers , roughly in the form in which they were acquired using the specification language discussed in the previous section .</sentence>
				<definiendum id="0">macro file</definiendum>
				<definiens id="0">contains the meanings of modifiers , roughly in the form in which they were acquired using the specification language discussed in the previous section</definiens>
			</definition>
</paper>

		<paper id="1062">
</paper>

		<paper id="1051">
</paper>

		<paper id="1072">
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>The NP structure is described as follows , ( R4 ) &lt; NP &gt; : : = ( &lt; NHD &gt; ) { &lt; NP &gt; /NOUN } ( &lt; NMP &gt; ) / &lt; Gerund-PH &gt; / &lt; To-infmitive~PH &gt; /That &lt; CLAUSE &gt; 154 where NHD ( Noun HeaDer ) is ~premodification '' and NMP ( Noun Modifier Phrase ) is `` postmodification ' .</sentence>
				<definiendum id="0">NHD</definiendum>
				<definiens id="0">follows , ( R4 ) &lt; NP &gt; : : = ( &lt; NHD &gt;</definiens>
			</definition>
			<definition id="1">
				<sentence>A case slot consists of three elements : one semantic filler condition slot and two syntactic and semantic marker slots .</sentence>
				<definiendum id="0">case slot</definiendum>
				<definiens id="0">consists of three elements : one semantic filler condition slot and two syntactic and semantic marker slots</definiens>
			</definition>
			<definition id="2">
				<sentence>Then , LUTE-EJ Parser extracts the semantic structure implied in a sentence ( S or CLAUSE ) as an event or state instance created from a case frame , which is a class or a prototype .</sentence>
				<definiendum id="0">LUTE-EJ Parser</definiendum>
				<definiens id="0">extracts the semantic structure implied in a sentence ( S or CLAUSE</definiens>
			</definition>
			<definition id="3">
				<sentence>However , a VTYPE-NMP has one ( or more ) structural missing element ( a hole ) compared with a CLAUSE .</sentence>
				<definiendum id="0">VTYPE-NMP</definiendum>
				<definiens id="0">has one ( or more ) structural missing element ( a hole ) compared with a CLAUSE</definiens>
			</definition>
			<definition id="4">
				<sentence>( a ) The phrase which is an adjective phrase and modifies `` each '' , appositive to the preceding `` statements '' , ( b ) The phrase which is a past participle phrase and modifies `` names '' .</sentence>
				<definiendum id="0">phrase</definiendum>
				<definiens id="0">an adjective phrase</definiens>
				<definiens id="1">a past participle phrase and modifies</definiens>
			</definition>
</paper>

		<paper id="1088">
			<definition id="0">
				<sentence>Similarly , if the value for a student under the attribute NATIONALITY is a member of the set ( U.K.U.S.A. Australia ... ) , he may be designated as coming from an English-speaking country .</sentence>
				<definiendum id="0">NATIONALITY</definiendum>
				<definiens id="0">a member of the set ( U.K.U.S.A. Australia ... ) , he may be designated as coming from an English-speaking country</definiens>
			</definition>
</paper>

		<paper id="1035">
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>Computation proceeds by progressive modifications to the data base as the interpreter searches the data base and attempts to match patterns in rules and apply the corresponding actions in the event of a successful match .</sentence>
				<definiendum id="0">Computation proceeds</definiendum>
				<definiens id="0">searches the data base and attempts to match patterns in rules and apply the corresponding actions in the event of a successful match</definiens>
			</definition>
			<definition id="1">
				<sentence>This has two consequences : a. if there are different interpretations of rules according to the task which they are supposed to perform , then we need different interpreters to interpret them , which is contrary to assumption ( 2 ) ; an obvious case is the same set of phrase structure rules used to drive a builder of phrase structure trees given a string as input , and to drive an integrity checker given a set of possibly well-formed trees ; b. alternatively , in some cases , information which is implicit for one type of interpreter may need to be made explicit for another , causing violation of assumption ( 3 ) ; an obvious case here is the fact that a phrase structure analyser can be written in terms of transductions on trees for a general rewrite interpreter , but at considerable cost in clarity and security .</sentence>
				<definiendum id="0">obvious case</definiendum>
				<definiens id="0">interpretations of rules according to the task which they are supposed to perform , then we need different interpreters to interpret them , which is contrary to assumption ( 2 ) ; an</definiens>
				<definiens id="1">the same set of phrase structure rules used to drive a builder of phrase structure trees given a string as input , and to drive an integrity checker given a set of possibly well-formed trees ; b. alternatively , in some cases , information which is implicit for one type of interpreter may need to be made explicit for another , causing violation of assumption ( 3 ) ; an obvious case here is the fact that a phrase structure analyser can be written in terms of transductions on trees for a general rewrite interpreter , but at considerable cost in clarity and security</definiens>
			</definition>
			<definition id="2">
				<sentence>A grammar specifies a primitive task to be performed .</sentence>
				<definiendum id="0">grammar</definiendum>
				<definiens id="0">specifies a primitive task to be performed</definiens>
			</definition>
			<definition id="3">
				<sentence>f. a CPU , which is essentially an FP system , to be complemented with the definitions of point b. The CPU is responsible for interpreting the scheduling ( control ) parts of 231 the user program .</sentence>
				<definiendum id="0">CPU</definiendum>
				<definiens id="0">responsible for interpreting the scheduling ( control ) parts of 231 the user program</definiens>
			</definition>
			<definition id="4">
				<sentence>The main difference between FP systems and FFP systems is that in the latter objects ( e.g. sequences ) are used to represent functions , which has as a consequence that in FFP one can create new functionals .</sentence>
				<definiendum id="0">FFP systems</definiendum>
				<definiens id="0">a consequence that in FFP one can create new functionals</definiens>
			</definition>
			<definition id="5">
				<sentence>\ [ ~ ( 3-1 ) , 'CD3 , apply'\ [ ~ ( 4 '' l ) , 'DDT\ ] \ ] where : M is the name of the system monitor uld is the user language definition in BNF cd is the control definition ( controldef in Fig 4 . )</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">the name of the system monitor uld is the user language definition in BNF cd is the control definition</definiens>
			</definition>
			<definition id="6">
				<sentence>usd is the user solution definition The meaning of the definition is as follows : M is defined to be the application of capply to the internal programe ip apply : &lt; capply , ip. &gt; capply is the semantic definition of the machine 's CPU ( see below ) .</sentence>
				<definiendum id="0">usd</definiendum>
				<definiens id="0">the semantic definition of the machine 's CPU ( see below )</definiens>
			</definition>
			<definition id="7">
				<sentence>It is defined in the following way : x is an object - &gt; ~x = x e = &lt; el ... .. en &gt; is an expression - &gt; ~ef~el ... .. pen &gt; if x , y are objects - &gt; ~ ( x : y ) = ~ ( ~ : y ) where OX is the function represented by the object x. is the FFP functional 'fetch ' DD is the definition store of the MMU CD is the definition store of the CPU # is the result of an unsuccesful search mapply is the apply mechanism of the MMU The execution of a primitive ( i.e. a granuuar ) represents a recursive call to the monitor M , modulo the different function of the control interpreter ( the CPU ) .</sentence>
				<definiendum id="0">OX</definiendum>
				<definiendum id="1">x.</definiendum>
				<definiendum id="2">DD</definiendum>
				<definiendum id="3">CPU #</definiendum>
				<definiens id="0">the function represented by the object</definiens>
				<definiens id="1">the definition store of the MMU CD is the definition store of the</definiens>
			</definition>
			<definition id="8">
				<sentence>The recursive call of M is caused by capply whose definition has to be augmented by inserting after line 6 Of the definition given above ~he following eondt % ion| y = applyprlm 9 &lt; M , uld , cd , dd ) : x where x is the specification of the primitive ( e.g. the rule set ) . IV. EXPERIMENTAL IMPLEMENTATION An experimental implementation of the architecture described above has to accomodate two distinct aims. First , it must reflect the proposed functionality , which is to say , roughly , that the parts out of which it is made correspond in content , in function and interrelationship to those laid down in the design. Second , it must , when supplied with a set of definitions , generate a system instance that is both correct , and sufficiently robust to be released into the user community to serve as an experlmental tool. The entire implementation runs under , and is partly defined in terms of the Unix* operating system. The main reason for this choice is that from the start , Unix has been conceived as a functional architecture. What the user sees is externally defined , being the result of applying the Unix kernel to a shell program. Furthermore , the standard shell , or csh , itself provides us with a language which can both describe and construct a complex system , essentially by having the vocabulary and the constructs to express the decomposition of the whole into more primitive parts. We shall see some examples of this below. Another reason for the choice of Unix is the availability of suitable , ready-made software that has turned out to be sufficient , in large measure , to construct a respectable first approximation to the system. Finall~ , the decentralised nature of our project demands that experimental implementations should be maximally distributable over a potentially large number of different hardware configurations. At present , Unix is the only practical choice. A. System Components The system consists of 4 main parts , these being : a. A user language compiler generator. b. A control definition generator. c. a kernel CPU. d. A data definition generator. These modules , together with a user language description , a control description , and a data description , are sufficient to specify an instance of the system. YACC After reviewing a number of compiler-compilers , it was decided to use YACC * UNIX is a trademark of the Bell Laboratories 233 ( Johnson 1915 ) . Quite apart from its availability under Unix , YACC accepts an LALR ( 1 ) grammar , a development of LR ( k ) grammars ( Knuth cit ; Aho &amp; Johnson ( 1974 ) . LALR parsers ( Look Ahead LR ) give considerably smaller parsing tables than canonical LR tables. The reader is referred to Aho &amp; Ullman ( 1977 ) which gives details of how to derive LALR parsing tables from LR ones. LEg LEX ( Lesk 1975 ) generates lexlcal anslysers , end is designed to be used in conjunction with YACC. LEg accepts a specification of lexical rules in the form of reBular expressions. Arbitrary actions may be performed when certain strings are recognised , although in our case , the value of the token recognised is passed , and an entry in the symbol table created. A user programe presupposes , and an inner program contains a number of control constructs for organlslng the scheduling of processes , end the performance of complex database manipulations. The meaning that these constructs shall have is determined by the definitions present in the control store of the kernel. The language in which we have chosen to define such constructs is FP ( Backus cit ) . It follows that the generator must provide compilations of these defintions in the language of the kernel machine. The implementation of the control generator is an adaptation of Baden 's ( 1982 ) FP interpreter. This is a stand-alone program that essentially translates FP definitions into kernel language ones. We are currently using the Unix Lisp interpreter ( Foderaro &amp; Sklower 1982 ) to stand in for FFP , although an efficient interpreter for the latter is under development. Notice that an FFP ( or Lisp ) system is necessary to implement the appllcative schema described in section Ill , since these systems have the power to describe their own evaluation mechanisms ; FP itself does not. Unfortunately , we know of no language as suitable for the description of data as FP for the description of control. The reason is that at this moment , we are insufficiently confident of the basic character of data in this domain to make any definitive claims about the nature of an ideal data description \ ] anguage. We have therefore chosen to express data definitions in the precise , but over general terms of first order logic , which are then embedded with very little syntactic transformation into the database of a standard Prolog implementation ( Pereira &amp; Byrd 1982 ) . The augmented interpreter then constitutes the MMU referred to above. The data definition for the current experiment presents the user with a database consisting of an ordered collection of trees , over which he may define arbitrary transductions. The CPU and MMU run in parallel , and communicate with each other through a pair of Unix pipelines usin 8 a defined protocol that minlmises the quantity of information passed. A small bootstrap program initlelises the MMU and sets up the pipelines. B. ConstructinK the System The decomposition of a system instance into parts can be largely described within the shell language. Figure 5. below summarises the organisation using the convention that a module preceded by a colon is constructed by executing the shell commands on the next line. The runnable version of figure 4. ( that contains rather more odd symbols ) conforms to the input requirements of the Unix 'make ' program. targettext : ( ( cpu &lt; bootstratp ) &lt; eurotra ) &lt; sourcetext &gt; targettext /*capply*/ eurotra : compiler &lt; usd &gt; eurotra /*apply I*/ COMPILER : yacc &lt; uld \ [ cc~compiler /*apply 2*/ controldef : fpcomp &lt; cd &gt; controldef MMU : echo 'save ( mmu ) ' I prolog dd CPU : echop ' ( damplisp cpu ) ' I lisp &lt; controldef Fig .</sentence>
				<definiendum id="0">Unix</definiendum>
				<definiens id="0">The recursive call of M is caused by capply whose definition has to be augmented by inserting after line 6 Of the definition given above ~he following eondt % ion| y = applyprlm 9 &lt; M , uld , cd , dd ) : x where x is the specification of the primitive ( e.g. the rule set ) . IV. EXPERIMENTAL IMPLEMENTATION An experimental implementation of the architecture described above has to accomodate two distinct aims. First , it must reflect the proposed functionality , which is to say , roughly , that the parts out of which it is made correspond in content , in function and interrelationship to those laid down in the design. Second , it must , when supplied with a set of definitions , generate a system instance that is both correct , and sufficiently robust to be released into the user community to serve as an experlmental tool. The entire implementation runs under , and is partly defined in terms of the Unix* operating system. The main reason for this choice is that from the start , Unix has been conceived as a functional architecture. What the user sees is externally defined , being the result of applying the Unix kernel to a shell program. Furthermore , the standard shell , or csh , itself provides us with a language which can both describe and construct a complex system , essentially by having the vocabulary and the constructs to express the decomposition of the whole into more primitive parts. We shall see some examples of this below. Another reason for the choice of Unix is the availability of suitable , ready-made software that has turned out to be sufficient , in large measure , to construct a respectable first approximation to the system. Finall~ , the decentralised nature of our project demands that experimental implementations should be maximally distributable over a potentially large number of different hardware configurations. At present ,</definiens>
			</definition>
</paper>

		<paper id="1005">
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>Pr is the set of propositional variables , .</sentence>
				<definiendum id="0">Pr</definiendum>
				<definiens id="0">the set of propositional variables ,</definiens>
			</definition>
			<definition id="1">
				<sentence>Sp is the set of persons , .</sentence>
				<definiendum id="0">Sp</definiendum>
				<definiens id="0">the set of persons ,</definiens>
			</definition>
			<definition id="2">
				<sentence>( A3 ) says that what FOOL knows at time t , FOOL knows at time t that anyone knows it at time t. FOOL 's knowledge represents universal knowledge , that is to say all agents knowledge .</sentence>
				<definiendum id="0">A3</definiendum>
				<definiendum id="1">FOOL</definiendum>
				<definiens id="0">knows at time t that anyone knows it at time t. FOOL 's knowledge represents universal knowledge , that is to say all agents knowledge</definiens>
			</definition>
			<definition id="3">
				<sentence>E. Axiom An axiom is any formula of the form F\ [ P+ , P-\ ] where P is a propositional variable .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a propositional variable</definiens>
			</definition>
			<definition id="4">
				<sentence>F\ [ ( A V B ) j V ~ A , FI ( A V B ) \ ] v ~ B ~ FL ( A V B ) J - ( R2 ) F\ [ ( St ) A 3 V~A ~ FT ( st ) A~ ( PO ) ~ ( Su ) A 1V ... V ~ ( Su ) Am V ~ ( Ou ) B. V ... V ~ ( Ou ) Bn V C where ( Su ) A I ... .. ( Su ) Am , ( Ou ) B I , ... , ( Ou ) B6 must appear as neg6tire parts in the conclusion , and uK t 51c 9 , F2\ [ C-\ ] F , v F2\ [ J ( cut ) G. Cut-elimlnation theorem ( Hauptsatz ) Any KT4 ' proof-figure can be transformed into a KT4 ' proof-figure with the same conclusion and without any cut as a rule of inference ( hence , the rule ( R4 ) is superfluous .</sentence>
				<definiendum id="0">St</definiendum>
				<definiendum id="1">Su</definiendum>
				<definiens id="0">A V B ) j V ~ A , FI ( A V B ) \ ] v ~ B ~ FL ( A V B ) J - ( R2 ) F\ [</definiens>
				<definiens id="1">1V ... V ~ ( Su ) Am V ~ ( Ou ) B. V ... V ~ ( Ou ) Bn V C where (</definiens>
				<definiens id="2">Cut-elimlnation theorem ( Hauptsatz ) Any KT4 ' proof-figure can be transformed into a KT4 ' proof-figure with the same conclusion</definiens>
			</definition>
			<definition id="5">
				<sentence>A proof is an single-rooted tree of formulas all of whose leaves are logical axioms .</sentence>
				<definiendum id="0">proof</definiendum>
				<definiens id="0">an single-rooted tree of formulas all of whose leaves are logical axioms</definiens>
			</definition>
			<definition id="6">
				<sentence>In the case where ( R3 ) does not lead to a loss of formulas , it is more efficient to choose it at first• The following example is given to illustrate this strategy : Example Take ( A4 ) as an example and let Fo denotes its equivalent version in our language ( Fo is at the start node ) : Fo = ~ ( St ) ( ~a V b ) V ~ ( Su ) a V ( Su ) b where t &lt; u P~ denotes positive parts and P ?</sentence>
				<definiendum id="0">Fo</definiendum>
			</definition>
			<definition id="7">
				<sentence>denotes I negative parts l P+ = { ~ ( St ) ( ~ a V b ) , % ( Su ) a , ( Su ) b } ; 2 P = { ( St ) ( ~ a V b ) , ( Su ) a } ; O By ( R3 ) we have ( no losses of formulas ) : F l = ~ ( St ) ( % a V b ) V % ( Su ) a V b ÷ PI = { % ( St ) ( ~ a V b ) , ~ ( Su ) a , b } F= { ( St ) ( % a V b ) , ( Su ) a } By ( ~2 ) we have : F~ = F~ V ~ , ( ~a V b ) P2 PI U { % ( ~a V b ) } P2 = P7 U { ~a V b } By ( RI ) we have : F~ = F~ V ~ a P3 P2 U { ~ a , a } andP~ = P2 O { ~ a } F 4 = F 2 V % b + + P4 = P2 ~ { ~ b } P~= P2 U { b } + ' P~ { b } F 4 is a logical axiom because P4 ~ = Finally , we have to apply ( R2 ) to the last but one node : F 5 F~ F~V~a P5 \ [ P3 U { ~ a } P5 = P3 iJ { a } is a logical axiom because P51~ F 5 =\ [ a } The generated derivation tree is then : I ÷ -Fo , Po , Po I F , ,P , FT , F2'P~'P2 j 1 / + -F3 , P3 , P 3 R 2 + + ; \ ] P5 = { a } F5'Pb'P5 P5 I ÷ -I F4'P4'P4 1 rPV~4- { b } Derivation tree 198 V ACKNOWLEDGMENTS We would like to express our sincerest thanks to Professor AndrOs Raggio who has guided and adviced us to achieve this work .</sentence>
				<definiendum id="0">Su</definiendum>
				<definiendum id="1">Su</definiendum>
				<definiendum id="2">~a V b ) P2 PI U { %</definiendum>
				<definiens id="0">a V b ÷ PI = { % ( St ) ( ~ a V b</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>An entity-relationship model states the possible primitive relationships among entity sets .</sentence>
				<definiendum id="0">entity-relationship model</definiendum>
			</definition>
			<definition id="1">
				<sentence>The suggestion mechanic , contains five simple substitution rules for handling such erroneous queries .</sentence>
				<definiendum id="0">suggestion mechanic</definiendum>
				<definiens id="0">contains five simple substitution rules for handling such erroneous queries</definiens>
			</definition>
			<definition id="2">
				<sentence>The substitution mechanism searches the user 's inferred plan and its possible expansions for propositions whose arguments unify with the arguments in the erroneous proposition causing the pragmatic overshoot .</sentence>
				<definiendum id="0">substitution mechanism</definiendum>
				<definiens id="0">searches the user 's inferred plan and its possible expansions for propositions whose arguments unify with the arguments in the erroneous proposition causing the pragmatic overshoot</definiens>
			</definition>
			<definition id="3">
				<sentence>SS if PRIM-CLASS ( ATTRFENT , CLASSal ) and SUPER ( CLA $ Sal , CLASSa2 ) and SUPER ( CLASSa2 , CLASSaS ) and ... and SUPER ( CLkSSal , CLASS ) If a revised query proposes substituting ATTRFENTnew for ATTRFENTold , then semantl c # difference ( ATTRFEN Tnew , ATTRFEN Told ) =NIL if there does not exist j , k such that f ( ATTRFEN Tnew , j ) =f ( ATTRFENTold , k ) =mln k such that there exists j such that f ( ATTRFEN Tnew , j ) =f ( ATTRFEN Tol d , k ) otherwise An initial set is constructed conslstil~g of those suggested revised queries that interrogate an aspect of the current focused plan in the context model .</sentence>
				<definiendum id="0">k ) otherwise</definiendum>
				<definiens id="0">An initial set is constructed conslstil~g of those suggested revised queries that interrogate an aspect of the current focused plan in the context model</definiens>
			</definition>
			<definition id="4">
				<sentence>Thus the speaker 's utterance contains the erroneous proposition For-Sale ( apar tment ) where apartment is a member of entity set APARTMENT .</sentence>
				<definiendum id="0">apartment</definiendum>
				<definiens id="0">a member of entity set APARTMENT</definiens>
			</definition>
			<definition id="5">
				<sentence>Thus one expansion of this Purchase plan includes the precondition For-Sale ( apartment-bullding ) The substitution rules propose substituting entity set APABT~NT-BUILDING from thls plan for the entity set APABT~NT in the speaker 's utterance .</sentence>
				<definiendum id="0">precondition For-Sale ( apartment-bullding</definiendum>
				<definiens id="0">) The substitution rules propose substituting entity set APABT~NT-BUILDING from thls plan for the entity set APABT~NT in the speaker 's utterance</definiens>
			</definition>
			<definition id="6">
				<sentence>The speaker 's utterance contalns the erroneous proposition MeetTlme ( Dr. Mt tchel , time ) If the preceding dialogue indicates that the speaker is considering taking CSI05 , then an expansion of the context model representing the speaker 's inferred plan will contain the action Earn-Credi tIn-Sectl on ( SPEAKER , section ) such that Is-Sectlon-Of ( section , CS105 ) Expansion of the plan for Earn-Credlt-lnSection contains the action Learn-FromTeacherIn-C1 ass ( SPE AKEB , section , faculty ) such that Teach ( faculty , section ) and the plan for thls action contains the action At tend-Cl ass ( SPEAKER , place , time ) such that Meet-Plave ( sectlon , place ) and MeetTime ( section , time ) The two relations Teach ( Dr.~fltchel , sectton ) and Meet-Time ( section , time ) appear on the • same path in the context model .</sentence>
				<definiendum id="0">Earn-Credlt-lnSection</definiendum>
				<definiendum id="1">SPEAKER</definiendum>
				<definiendum id="2">Meet-Plave</definiendum>
				<definiens id="0">contains the action Learn-FromTeacherIn-C1 ass ( SPE AKEB , section</definiens>
				<definiens id="1">Teach ( Dr.~fltchel , sectton ) and Meet-Time ( section , time ) appear on the • same path in the context model</definiens>
			</definition>
			<definition id="7">
				<sentence>Therefore the path expansion heuristics suggest the expanded relational path Teach ( Dr. Mi tchel , section ) `` Meet-Time ( ae ctlon , time ) as a substitution for the relation MeetTime ( Dr. Mi tchel , time ) in the user 's utterance .</sentence>
				<definiendum id="0">path expansion heuristics</definiendum>
				<definiens id="0">a substitution for the relation MeetTime ( Dr. Mi tchel , time ) in the user 's utterance</definiens>
			</definition>
			<definition id="8">
				<sentence>Kaplan proposes using the shortest relational path connecting the entity sets ; Chang proposes an algorithm based on minimal spanning trees , using an a priori weighting of the arcs ; $ owa uses a conceptual graph ( semantic net ) for constructing the expanded relation .</sentence>
				<definiendum id="0">Kaplan</definiendum>
				<definiens id="0">proposes using the shortest relational path connecting the entity sets ; Chang proposes an algorithm based on minimal spanning trees , using an a priori weighting of the arcs ; $ owa uses a conceptual graph ( semantic net</definiens>
			</definition>
			<definition id="9">
				<sentence>on Artificial Int~ , Stanford , 1980 Quine , W. V. , `` Ontologlcal Relativity '' in Ontological ~ @ lativltv and Qther ~ , Columbia University Press , New York 1969 Ramshaw , L. A. end N. ~ Weischedel , `` Problem Localization Strategies for Pragmatic Processing in Natural Language Front Ends '' , Proe .</sentence>
				<definiendum id="0">Ontologlcal Relativity</definiendum>
				<definiens id="0">Problem Localization Strategies for Pragmatic Processing in Natural Language Front Ends ''</definiens>
			</definition>
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>ABSTRACT A lexicon-grammar is constituted ot the elementary sentences of a language .</sentence>
				<definiendum id="0">lexicon-grammar</definiendum>
				<definiens id="0">constituted ot the elementary sentences of a language</definiens>
			</definition>
			<definition id="1">
				<sentence>We call support verbs ( Vsup ) the verbs in such sentences that have no selectional function , Some support verbs are semantically neutral , others introduce modal or aspectual meanings , as for example in Bob loves Jo = Bob Is in love with Jo = Bob fell in love with Jo = Bob has a deep love for Jo to tall , as other motion verbs do , introduces an inchoative meaning .</sentence>
				<definiendum id="0">Bob</definiendum>
				<definiens id="0">the verbs in such sentences that have no selectional function , Some support verbs are semantically neutral , others introduce modal or aspectual meanings , as for example in Bob loves Jo = Bob Is in love with Jo = Bob fell in love with Jo =</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>++ C ( CLOSABCB ) + + C ( CLOSCF R C B ) + R1 + ( SYNONYM R R1 ) B R1 A ( CO~ R R1 ) C + ÷ ( CLOSAB C A ) where CLOSAB stands for Abstractive Closure and is defined in procedural logic ( where the symbol &lt; is shorthand for the reversed implication sign &lt; -- , i.e. P &lt; Q S is equivalent to Q `` S -- &gt; P ) : ( CLOSAB NI N2 ) &lt; ( OR CINST NI N2 ) ( SUP N1 N2 ) ) ( INST N1 N2 ) &lt; ( OR ( NI INST N2 ) ( N1 ~* N2 ) ) ( INST N1N2 ) &lt; ( INST N1X ) ( INSTX N2 ) ( SUP Ni N2 ) &lt; ( OR ( Ni E~U£V N2 ) ( Ni SUP N2 ) ) ( SUP NI N2 ) &lt; ( SUP NI X ) ( SUPX N2 ) CLOSCP stands for Complex Product Closure and is defined as ( CLOSCP R N1N2 ) &lt; ( TRANSITIVE R ) ( NI R N2 ) =N1R N2 is the new A R B '' ( CLOSCP R N1N2 ) &lt; ( NI ~OF N2 ) *~ ( CLOSCF R N1N2 ) &lt; ( NI LOC N2 ) ** ( CLOSCF R NI N2 ) &lt; ( NI *AND N2 ) ( CLOSCP R N1N2 ) &lt; ( NI *OR N2 ) ** These two relations turn out not to be universally true complex products ; they only give answers that are possibly true , so they have been dropped for most question answering applications .</sentence>
				<definiendum id="0">++ C</definiendum>
				<definiendum id="1">OR ( Ni E~U£V N2 ) ( Ni SUP N2 ) ) ( SUP NI N2</definiendum>
				<definiendum id="2">CLOSCP R N1N2 ) &lt; ( NI ~OF N2</definiendum>
				<definiendum id="3">NI LOC N2 ) ** ( CLOSCF R NI N2</definiendum>
				<definiens id="0">+ + C ( CLOSCF R C B ) + R1 + ( SYNONYM R R1 ) B R1 A ( CO~ R R1 ) C + ÷ ( CLOSAB C A ) where CLOSAB stands for Abstractive Closure and is defined in procedural logic ( where the symbol &lt; is shorthand for the reversed implication sign &lt; -- , i.e. P &lt; Q S is equivalent to Q `` S -- &gt; P ) : ( CLOSAB NI N2 ) &lt; ( OR CINST NI N2 ) ( SUP N1 N2 ) ) ( INST N1 N2 ) &lt; ( OR ( NI INST N2 ) ( N1 ~* N2 ) ) ( INST N1N2 ) &lt; ( INST N1X ) ( INSTX N2 ) ( SUP Ni N2</definiens>
				<definiens id="1">CLOSCP R N1N2 ) &lt; ( TRANSITIVE R ) ( NI R N2 ) =N1R N2 is the new A R B ''</definiens>
				<definiens id="2">NI *OR N2 ) ** These two relations turn out not to be universally true complex products ; they only give answers that are possibly true , so they have been dropped for most question answering applications</definiens>
			</definition>
			<definition id="1">
				<sentence>SRq ( Head Arcl Vail , Arc2 Val2 , ... , Arcn Vain ) SRc ( Head Arc2 Val2 , Arcl Vail , ... , Arch Vain ) The SR may be represented ( actually or virtually ) as a list of triples as follows : SRq ( ( Head Arcl Vail ) ( Head Arc2 Val2 ) ... , ( Head Arcn Vain } ) Two triples match in Relaxed Unification according ( at least ) to the conditions shown in Figure 2 .</sentence>
				<definiendum id="0">SRq</definiendum>
				<definiendum id="1">Arc2 Val2 , ... , Arcn Vain</definiendum>
				<definiendum id="2">Arcl Vail , ... , Arch Vain</definiendum>
				<definiens id="0">a list of triples as follows : SRq ( ( Head Arcl Vail ) ( Head Arc2 Val2 ) ... , ( Head Arcn Vain } ) Two triples match in Relaxed Unification according</definiens>
			</definition>
</paper>

		<paper id="1082">
			<definition id="0">
				<sentence>It is our goal , therefore , to extend the syntactic coverage for DRT by linking it to the grammars described for the User Specialty Languages ( USL ) system ( Lehmann ( 1978 ) , Ott and Zoeppritz ( 1979 ) , Lehmann ( 1980 ) , Sopefia ( 1982 ) , Zoeppritz ( 1984 ) ) which are comprehensive enough to deal with realistic discourses .</sentence>
				<definiendum id="0">User Specialty Languages</definiendum>
				<definiens id="0">comprehensive enough to deal with realistic discourses</definiens>
			</definition>
			<definition id="1">
				<sentence>The parser used in the Natural Language Analyzer was originally described by Kay ( 1967 ) and subsequently implemented in the REL system ( Thompson et .</sentence>
				<definiendum id="0">parser</definiendum>
				<definiens id="0">used in the Natural Language Analyzer was originally described by Kay ( 1967 ) and subsequently implemented in the REL system ( Thompson et</definiens>
			</definition>
			<definition id="2">
				<sentence>The Natural Language Analyzer uses a modified version of this parser which is due to Bertrand &amp; al ( 1976 , IBM ( 1981 ) ) .</sentence>
				<definiendum id="0">Natural Language Analyzer</definiendum>
				<definiens id="0">uses a modified version of this parser</definiens>
			</definition>
			<definition id="3">
				<sentence>The syntactic coverage of the Natural Language Analyzer presently includes Nouns Verbs Adjectives and adjectival phrases : gradation , modification by modal adverbial , modification by ordinal number Units of measure Noun phrases : definiteness , quantification , interrogative pronouns , personal pronouns , possessive pronouns , relative pronouns Verb complements : subjects and nominative complements , direct objects , indirect objects , prepositional objects Noun complements : relative clauses , participial attribute phrases , genitive attributes , appositions , prepositional attributes Complements of noun and verb : negation , locative adverbials , temporal adverbials Coordination for nouns , noun phrases , adjectives , verb complexes and sentences Comparative constructions Subordinate clauses : conditionals Sentences : declarative sentences , questions , commands 398 In this section we give the categories and rules used to process the Kamp fragment .</sentence>
				<definiendum id="0">locative adverbials</definiendum>
				<definiens id="0">The syntactic coverage of the Natural Language Analyzer presently includes Nouns Verbs Adjectives and adjectival phrases : gradation , modification by modal adverbial , modification by ordinal number Units of measure Noun phrases : definiteness , quantification , interrogative pronouns</definiens>
			</definition>
			<definition id="4">
				<sentence>from parse trees To cover the Ramp fragment the following interpretation routines are needed : PRNAME and NOMEN which map strings of characters to elements of AIS ; NPDEF , NPINDEF and blPQUAN which map pairs consisting of strings of characters and elements of AIS to elements of AIS ; VERB which maps strings of characters to elements of SIS ; NOM and ACC which operate according to Intermediate Structure formation rule 3 ; RELCL which applies Intermediate Structure formation rule 5 and then 4 ; COND which combines a pair of elements of SIS by applying Intermediate Structure formation rule 5 and then rule 3 ; STMT which maps elements of SIS to DRSs .</sentence>
				<definiendum id="0">COND</definiendum>
				<definiens id="0">combines a pair of elements of SIS by applying Intermediate Structure formation rule 5 and then rule 3 ; STMT which maps elements of SIS to DRSs</definiens>
			</definition>
</paper>

		<paper id="1074">
			<definition id="0">
				<sentence>Prolog provides a Eood tools for the implementation of LFG .</sentence>
				<definiendum id="0">Prolog</definiendum>
				<definiens id="0">provides a Eood tools for the implementation of LFG</definiens>
			</definition>
			<definition id="1">
				<sentence>LFG is an e : :tention of context free grammar ( C~'G ) and has two-levels of representation , i.e. c-structures ( constituent structures ) and f-~tructures ( functional structures ) .</sentence>
				<definiendum id="0">LFG</definiendum>
				<definiens id="0">an e : :tention of context free grammar ( C~'G ) and has two-levels of representation</definiens>
			</definition>
			<definition id="2">
				<sentence>A c-structure is generated by CFG and represents the surface uord and phrase configurations in a ~entence , and the f-structure is generated by the functional equations a=sociated with the o~rammar rules and represents the conflo~uratlon of the surface ~ra=matical functions .</sentence>
				<definiendum id="0">c-structure</definiendum>
				<definiens id="0">generated by CFG and represents the surface uord and phrase configurations in a ~entence</definiens>
			</definition>
			<definition id="3">
				<sentence>Each syntaotle node h~s i~s own f-structure and the partial value of the f-structure is defined by the Equational ~ch~m. For exauple , the functional equation `` ( ~ sub~ ) = $ '' associated with the dau~hter `` np '' node of ~r~-u~r rule I. of Fi~ .</sentence>
				<definiendum id="0">syntaotle node</definiendum>
			</definition>
			<definition id="4">
				<sentence>Time used in analysis is 972 ms. ( parsing ) 19 ms. ( checkin~ constraints ) ~I ms. ( for checFin~ completeness ) subJ spec the nun sg per 3 pred sem ( glrl ) pred sam ( persuade ( \ [ subj , A\ ] , \ [ obJ , A\ ] , \ [ vcomp , A\ ] ) ) obj spec the num sg per 3 pred sam ( baby ) tense past vcomp subj spee the hUm sg per 3 pred sam ( baby ) Inf ÷ pred sam ( so ( \ [ subJ , B\ ] ) ) to ÷ Fig .</sentence>
				<definiendum id="0">Time</definiendum>
				<definiendum id="1">~I ms.</definiendum>
				<definiens id="0">the hUm sg per 3 pred sam ( baby ) Inf ÷ pred sam ( so ( \ [ subJ , B\ ] ) ) to ÷ Fig</definiens>
			</definition>
</paper>

		<paper id="1108">
			<definition id="0">
				<sentence>II TEXT COMPRESSION IN NOUN PHRASES We can recognize the sources of text compression by two means : ( 1 ) comparing a full grammar of the standard language to that of the domain in which we are working , 505 and { 2 ) comparing the distribution of constructions in two different sublanguages .</sentence>
				<definiendum id="0">II TEXT COMPRESSION IN NOUN PHRASES</definiendum>
				<definiens id="0">recognize the sources of text compression by two means : ( 1 ) comparing a full grammar of the standard language to that of the domain in which we are working , 505 and { 2 ) comparing the distribution of constructions in two different sublanguages</definiens>
			</definition>
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>Thus , for example , if Commentator notes that the object called Adam is approaching the object called the gate ( where approach is defined as something like `` moving in the direction of the goal , with diminishing distance '' this is not obvious , but perhaps a problem of pattern recognition rather than semantics ) , the system will say something like ( I ) `` Adam is approaching the gate '' .</sentence>
				<definiendum id="0">Adam</definiendum>
				<definiens id="0">notes that the object called Adam is approaching the object called the gate</definiens>
				<definiens id="1">obvious , but perhaps a problem of pattern recognition rather than semantics</definiens>
			</definition>
			<definition id="1">
				<sentence>This involves not only the linguistic context ( what has been said before ) and the extra-linguistic context ( the speech situation ) , but also the listener 's knowledge about the speaker and the world in general .</sentence>
				<definiendum id="0">extra-linguistic context</definiendum>
				<definiens id="0">the speech situation ) , but also the listener 's knowledge about the speaker and the world in general</definiens>
			</definition>
			<definition id="2">
				<sentence>\ [ 7\ ] Unification is an operation a bit like putting together two pieces of a jigsaw puzzle .</sentence>
				<definiendum id="0">Unification</definiendum>
				<definiens id="0">an operation a bit like putting together two pieces of a jigsaw puzzle</definiens>
			</definition>
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>Indexing is seen as a legitimate form of shallow text processing , but one requiring serious semantically based language processing , particularly to obtain well-founded complex terms , which is the main objective of the project described .</sentence>
				<definiendum id="0">Indexing</definiendum>
				<definiens id="0">a legitimate form of shallow text processing , but one requiring serious semantically based language processing</definiens>
			</definition>
			<definition id="1">
				<sentence>The major operation is the identification of indexing concepts , or term sources , in text meaning representations .</sentence>
				<definiendum id="0">major operation</definiendum>
				<definiens id="0">the identification of indexing concepts , or term sources , in text meaning representations</definiens>
			</definition>
			<definition id="2">
				<sentence>The particular requirements to be met 288 Request : GIVE HE INFORMATION ABOUT THE PRACTICAL CIRCUIT DETAILS OF HIGH FREQUENCY OSCILLATORS USING TRANSISTORS a ) 18 analyses including ( simplifled illustration ) : ( clause ... ( V.o. I @ @ sKent ... ) ( @ @ reclpient ... ) @ @ oSject ... ( @ @ mental object ... &lt; n ( detaill szgn ( @ @ atttribu~e ( trace ( clause v agent ) ( clause ( v ( use1 use ( @ @ agent ( n ( osclllatorl thing ( # # nmod ( trace ( clause v agent ) ( clause ( v ( be2 be l @ @ ag ent ( n ( frequencyl sign ) ) ) @ @ state ... high3kind~ ) ) ~ ) ) ) ) ( @ @ object ( n ( transistorl thing ) ) ) ) ) ) ) ) ( # # r~nod ( trace ( clause v agent ) ( clause ( v ( be2 be ( @ @ agent ( n ( circuitl thing ) ) ) ( @ @ state ... practical2 kind ) ) ) ) ) ) ) &gt; ) ) ) b ) 10 term sources of scale 2 for this analysis including : ( n ( detaill sign ( # # nmod ( ( ( n ( circuitl thing ) ) ) ) ) ) ) ( ( trace ( clause v agent ) ) ( clause tel ) • ( type ( v ( usel use ( @ @ agent ( n ( oscillator1 thing ) ) ) ) ) ) ) ( ( trace ( clause v object ) ) ( clause ( type rel ) ( v ( use1 use ( @ @ object ( n ( transistor1 thing ) ) ) ) ) ) ) c ) semantic variants using inference for compound nouns , selecting prepositional cases from 17 possible : for 'circuit detail ' in this analysis 3 new variants : ( n ( detaill sign ( @ @ abstract location ( n ( circuitl thing ) ) ) ) ) ( n ( detaill sign ( @ @ mental oSJect ( n ( circuitl thing ) ) ) ) ) ( n ( detaill sign ( @ @ attribute ( n ( circuitl thing ) ) ) ) ) d ) 15 term search specification for the request using terms of scale 2 , with compound noun inference : variant set of 5 for 'frequency oscillator ' including : `` a frequency oscillator '' `` frequency oscillators '' variant set of 25 for 'circuit detail ' interpreted as 'detail about circuit ' including : `` the details about the circuits '' `` detail about circuits '' `` details about a circuit '' Figure 1 .</sentence>
				<definiendum id="0">osclllatorl thing</definiendum>
				<definiendum id="1">ag ent</definiendum>
				<definiens id="0">@ @ object ( n ( transistor1 thing )</definiens>
			</definition>
</paper>

		<paper id="1043">
</paper>

		<paper id="1090">
			<definition id="0">
				<sentence>s other properties which define it to be a mamm : d. Thus , the strategy the expert uses when s/he perceives the misc , ,J , ct , ption tu be of TYPE ONE may be characterized as : ( I ) l ) e , y the posited superordinate and iudk : ate the correct one , ( 2 ) State at tributes ( prol &gt; , 'rties ) that the obj+ct has in common with the posited super &lt; ~rdin : tte , ( at State defining attributes of the real super-r &lt; thmte , thus giviug evidence/justification for the correct ch , ~+ : ifi , '~ti.n. The sy , lem may hdlow this strategy when the user mod~l indicates that the itser thinks the p++sited suFerordinate and the .hi\ ] el are simih\ ] r bee : ruse they share man ) ' common properties { n , ,t held by the real SUl~.rordinate ) . TYPE TWO Objt , ct Shares Properties with Auother Object which is a Member of Pos : ited Superordinate In this c : rse the lAhho , Jgh the analysis given hero wa~ d~ : rived through , t~ , lying xr~uLI human interactions , the exarapDs given ire simply illustrative and have not been extrs , ,-t~d frorn a real interaetiJn. 445 misclassified object and the `` other object '' are similar because they have some other common superordinate. The properties that they share arc no_..~t those inherited from the posited superordinate ; but those inherited from this other common superordhlate. Figure 3-1 shows a representation of this situation. OBJECT and OTIIEIi-LIBJEC'E have many common properties because they slt : .t.re a CtHltllton superordinate ( COMMON-St ! I'E|2OI2DINATE ) . Hence. if the user knows that OTIIEI1-OBJECT is a tnember of the POSrFED SUPEROllDINATE , ~/J|e inay wr~mgly conclude that OBJECT is also a member of POSITED : SUI &gt; ERORD1NATE .</sentence>
				<definiendum id="0">OTIIEI1-OBJECT</definiendum>
				<definiens id="0">the expert uses when s/he perceives the misc , ,J , ct , ption tu be of TYPE ONE may be characterized as : ( I ) l ) e , y the posited superordinate</definiens>
				<definiens id="1">] el are simih\ ] r bee : ruse they share man ) ' common properties { n , ,t held by the real SUl~.rordinate )</definiens>
				<definiens id="2">properties that they share arc no_..~t those inherited from the posited superordinate</definiens>
			</definition>
			<definition id="1">
				<sentence>Heuristics are currently being worked out for determining the most likely misconception type based on what kinds of things { e.g. , sets of attributes or objects ) have been focused on in the recent past .</sentence>
				<definiendum id="0">Heuristics</definiendum>
				<definiens id="0">sets of attributes or objects</definiens>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>GRADE uses an annotated tree structure for expressing a sentence .</sentence>
				<definiendum id="0">GRADE</definiendum>
				<definiens id="0">uses an annotated tree structure for expressing a sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>V ( PRON ) ADVPART -- - &gt; matching_condition : • ( V PRON ADVPART ) : PRON : optional : Atyptcal Japanese sententtal structure tn whtch three adverbial phrases ( ADVP ) .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">Atyptcal Japanese sententtal structure tn whtch three adverbial phrases</definiens>
			</definition>
			<definition id="2">
				<sentence>When • subgrammar network tn the analysts phase consists of a subgrammar for a noun-phrase ( SG1 ) and a subgrammar for a verb-phrase ( SG2 ) tn this sequence , the executor of GRADE first appltes SG1 to an input sentence , then appltes SG2 to the result of an application of SG1 .</sentence>
				<definiendum id="0">subgrammar network tn the analysts phase consists of a subgrammar</definiendum>
				<definiens id="0">the result of an application of SG1</definiens>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>Next is a theory of granularity , in which the key concept is `` x is indistinguishable from y with respect to grain g ' .</sentence>
				<definiendum id="0">Next</definiendum>
				<definiens id="0">a theory of granularity , in which the key concept is `` x is indistinguishable from y with respect to grain g '</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>The planner decides what information should be imparted and most of its rhetorical features ; Mumble filters those decis/ons in accordance with grammatical constraints , handles syntax and morphology , and performs the `` smoothing '' operations that are required by the discourse context in which the information aIvears .</sentence>
				<definiendum id="0">Mumble</definiendum>
				<definiens id="0">filters those decis/ons in accordance with grammatical constraints , handles syntax and morphology</definiens>
			</definition>
			<definition id="1">
				<sentence>PUGG ( Plot Unit Graph Generator ) operates on an affect-state representation of a story , and produces a graph or network of plot units that act as pointers to the o~e of the conceptual representation of the input story and organizes how it will be '~n~sented '' to the program that plans the text of the summary , Precis .</sentence>
				<definiendum id="0">PUGG</definiendum>
				<definiens id="0">Plot Unit Graph Generator ) operates on an affect-state representation of a story , and produces a graph or network of plot units that act as pointers to the o~e of the conceptual representation of the input story and organizes how it will be '~n~sented '' to the program that plans the text of the summary</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>The flipflops ( FF ) are simple register units and the shift register is a simple PLA network of well known structure .</sentence>
				<definiendum id="0">shift register</definiendum>
				<definiens id="0">a simple PLA network of well known structure</definiens>
			</definition>
</paper>

		<paper id="1068">
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>The GTT-system ( Geneva Teaching Transducer ) 1 is a ger~ral tree-to-tree transducer developed as a tool for training linguists in machine translation and computational linguistics .</sentence>
				<definiendum id="0">GTT-system</definiendum>
				<definiens id="0">a ger~ral tree-to-tree transducer developed as a tool for training linguists in machine translation and computational linguistics</definiens>
			</definition>
			<definition id="1">
				<sentence>The system takes the dictionaries and the grammars as data , cc~piles these data and the interpreter then uses them to process the input text .</sentence>
				<definiendum id="0">system</definiendum>
				<definiens id="0">takes the dictionaries and the grammars as data , cc~piles these data and the interpreter then uses them to process the input text</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>The parse file for the patient documents had correct parses for 236 sentences ( and sentence fragments ) ; the file for the CASREPS had correct parses tor 123 sentences .</sentence>
				<definiendum id="0">parse file</definiendum>
				<definiens id="0">236 sentences ( and sentence fragments ) ; the file for the CASREPS had correct parses tor 123 sentences</definiens>
			</definition>
			<definition id="1">
				<sentence>X = the number of sentences ( and sentence fragments ) in the text samplc ; ~ '' = the number of non-terminal symbols m the context-free component of thc ~'ammar .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the number of sentences ( and sentence fragments ) in the text samplc</definiens>
			</definition>
			<definition id="2">
				<sentence>X = the number of sentences ( and sentence fragments ) in the text sample ; Y = the number of productions in the context-free component of the grammar .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the number of sentences ( and sentence fragments ) in the text sample ; Y = the number of productions in the context-free component of the grammar</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>A c-graph G is a cycle free , labelled graph \ [ 1,9\ ] without isolated nodes and with exactly one entry node and one exit node .</sentence>
				<definiendum id="0">c-graph G</definiendum>
				<definiens id="0">a cycle free , labelled graph \ [ 1,9\ ] without isolated nodes and with exactly one entry node and one exit node</definiens>
			</definition>
			<definition id="1">
				<sentence>E.g. : G5=i is a segif of G. b ) Normal isolatability .</sentence>
				<definiendum id="0">E.g.</definiendum>
				<definiens id="0">a segif of G. b ) Normal isolatability</definiens>
			</definition>
			<definition id="2">
				<sentence>A seg G ' of G is strongly isolatable ( segfi ) if and only if the only node that has entering arcs not in G ' is I ' and the only node that has leaving arcs not in G ' is 0 ' .</sentence>
				<definiendum id="0">seg G</definiendum>
				<definiens id="0">has entering arcs not in G ' is I ' and the only node that has leaving arcs not in G ' is 0 '</definiens>
			</definition>
			<definition id="3">
				<sentence>E.g. G4 is a segfi of G. Order and roads .</sentence>
				<definiendum id="0">E.g. G4</definiendum>
				<definiens id="0">a segfi of G. Order and roads</definiens>
			</definition>
			<definition id="4">
				<sentence>A TS is a cycle free oriented graph , with only one input and such that , CI ) Each node is labelled with a RSC or &amp; nul .</sentence>
				<definiendum id="0">TS</definiendum>
				<definiens id="0">a cycle free oriented graph</definiens>
			</definition>
			<definition id="5">
				<sentence>A special case constitutes a segfi G ' such that I and 0 do not belong to G ' .</sentence>
				<definiendum id="0">special case</definiendum>
				<definiens id="0">constitutes a segfi G ' such that I and 0 do not belong to G '</definiens>
			</definition>
</paper>

		<paper id="1093">
			<definition id="0">
				<sentence>Introduction Dictionaries constitute a unique resource for a broad range of research involving natural language , information , knowledge , and the analysis of contemporary culture .</sentence>
				<definiendum id="0">Introduction Dictionaries</definiendum>
				<definiens id="0">involving natural language , information , knowledge</definiens>
			</definition>
			<definition id="1">
				<sentence>Dictionary data figure in psychological experiments on language and perception .</sentence>
				<definiendum id="0">Dictionary data</definiendum>
				<definiens id="0">figure in psychological experiments on language and perception</definiens>
			</definition>
</paper>

	</volume>
