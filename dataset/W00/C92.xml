<?xml version="1.0" encoding="UTF-8"?>
	<volume id="C92">

		<paper id="4190">
			<definition id="0">
				<sentence>It is very difficult for natural language processing ( NLP ) systems to handle adverbs because of the large number of syntactic roles that adverbs can assume in sentences .</sentence>
				<definiendum id="0">NLP</definiendum>
				<definiens id="0">large number of syntactic roles that adverbs can assume in sentences</definiens>
			</definition>
			<definition id="1">
				<sentence>Jackendoff , for example , classifies adverbs as subject-oriented , speaker-oriented ( sentence ) , and manner adverbs , and represents the meaning structures of these adverbs in essentially prolog form as follows : Adverb class : Sentence Prolog-like formula : ADJ ( f ( Np1 , ... , NPa ) ) Example : EVIDENT ( WALKED ( JOHN , IN ) ) It is evident that John walked in .</sentence>
				<definiendum id="0">ADJ</definiendum>
				<definiens id="0">subject-oriented , speaker-oriented ( sentence ) , and manner adverbs , and represents the meaning structures of these adverbs in essentially prolog form as follows : Adverb class : Sentence Prolog-like formula :</definiens>
			</definition>
			<definition id="2">
				<sentence>Adverb class : Subject-oriented Prolog-like formula : ADJ ( Npi , f ( NP 1 ... .. NPa ) ) Example : CLUMSY ( FRED , DROPPED ( FRED , THE BOOK ) ) It was clumsy of Fred to drop the book .</sentence>
				<definiendum id="0">Adverb class</definiendum>
				<definiens id="0">Subject-oriented Prolog-like formula : ADJ ( Npi , f ( NP 1 ... .. NPa )</definiens>
			</definition>
			<definition id="3">
				<sentence>Huang uses similar prolog-like formulas to represent the semantic structure of adverbs .</sentence>
				<definiendum id="0">Huang</definiendum>
				<definiens id="0">uses similar prolog-like formulas to represent the semantic structure of adverbs</definiens>
			</definition>
</paper>

		<paper id="2123">
			<definition id="0">
				<sentence>A specification language for describing linguistic knowledge could be based on a feature logic and has an object-oriented inheritance style that makes it possible to distinguish formally between generic knowledge and specific ( e.g. , sublanguage ) knowledge , thus enabling the reuse of specifications in the development of the specifications tllemselves .</sentence>
				<definiendum id="0">specification language</definiendum>
				<definiens id="0">makes it possible to distinguish formally between generic knowledge and specific ( e.g. , sublanguage ) knowledge , thus enabling the reuse of specifications in the development of the specifications tllemselves</definiens>
			</definition>
</paper>

		<paper id="4172">
			<definition id="0">
				<sentence>gr ( grammatical relation ) takes either subj , obj , or iobj as the value .</sentence>
				<definiendum id="0">gr ( grammatical relation )</definiendum>
				<definiens id="0">takes either subj , obj , or iobj as the value</definiens>
			</definition>
			<definition id="1">
				<sentence>subcat ( subcategorization ) designates the set of categories ( complements ) that a particular category ( head ) requires .</sentence>
				<definiendum id="0">subcategorization )</definiendum>
				<definiens id="0">designates the set of categories ( complements ) that a particular category</definiens>
			</definition>
			<definition id="2">
				<sentence>dep ( dependent ) designates the category that a particular category ( adjunct ) modifies .</sentence>
				<definiendum id="0">dep ( dependent )</definiendum>
			</definition>
			<definition id="3">
				<sentence>ta X ( where X is a noun ) , there are at least two possibilities in the interpretation of the role of X , that is , either subject or object .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a noun</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>The same is true of the subjacency principle and the ECP , which constrain the relation between traces and their antecedents .</sentence>
				<definiendum id="0">ECP</definiendum>
			</definition>
			<definition id="1">
				<sentence>We shall see that with an indexing scheme on trees the check can be done in log ( n ) time , where n is the size of the tree .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the size of the tree</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>A bottom-up parser builds the tree by working upward from the terminal symbols in the input string , constructing each parent node after all its children have been recognized .</sentence>
				<definiendum id="0">bottom-up parser</definiendum>
				<definiens id="0">builds the tree by working upward from the terminal symbols in the input string</definiens>
			</definition>
			<definition id="1">
				<sentence>Johnson-Laird examines the psychological plausibility of parsers , not parsing strategies , but otherwise his argument is very much the same as the discussion in the previous section .</sentence>
				<definiendum id="0">Johnson-Laird</definiendum>
				<definiens id="0">examines the psychological plausibility of parsers , not parsing strategies</definiens>
			</definition>
			<definition id="2">
				<sentence>Johnson-Laird , ibrmalizing parsers as push-down automata , provides a characterization that clearly defines the terms of the comparison , but his leftcorner automaton lacks the properties needed to make the argument succeed .</sentence>
				<definiendum id="0">leftcorner automaton</definiendum>
				<definiens id="0">lacks the properties needed to make the argument succeed</definiens>
			</definition>
			<definition id="3">
				<sentence>Composition is an integral part of CCG , as is the notion of type-raising , which resembles left-corner prediction .</sentence>
				<definiendum id="0">Composition</definiendum>
				<definiens id="0">an integral part of CCG</definiens>
				<definiens id="1">the notion of type-raising , which resembles left-corner prediction</definiens>
			</definition>
</paper>

		<paper id="4173">
			<definition id="0">
				<sentence>Casual collocation includes items which have no bearing on the node , and as Sinclair explains `` may be accidental , reflecting the place , perhaps , where someone breaks into a committee meeting with the coffee ; or they may include the magnificent images of some of our greatest poetry '' ( 1966:418 ) .</sentence>
				<definiendum id="0">Casual collocation</definiendum>
				<definiens id="0">includes items which have no bearing on the node</definiens>
			</definition>
			<definition id="1">
				<sentence>Sinclair fixes no limit on the size of a polymorphemic item .</sentence>
				<definiendum id="0">Sinclair</definiendum>
				<definiens id="0">fixes no limit on the size of a polymorphemic item</definiens>
			</definition>
			<definition id="2">
				<sentence>cx are each a word , then S is an overlapping ambiguous segment , or in other words the segment S displays disjunctive ambiguity .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">an overlapping ambiguous segment , or in other words the segment S displays disjunctive ambiguity</definiens>
			</definition>
			<definition id="3">
				<sentence>The segment bl ... bt is known as an overlap , which is usually one character long .</sentence>
				<definiendum id="0">segment bl ... bt</definiendum>
				<definiens id="0">an overlap , which is usually one character long</definiens>
			</definition>
			<definition id="4">
				<sentence>Token , then is a terminal node ill processing .</sentence>
				<definiendum id="0">Token</definiendum>
				<definiens id="0">a terminal node ill processing</definiens>
			</definition>
</paper>

		<paper id="2069">
			<definition id="0">
				<sentence>The solution W , if it exists , should satisfy all the given text/term pairs , i.e. the equation WE~ = b~ holds for i = 1 , ... , k , where k is the number of text/term pairs , Ei ( n x 1 ) is a text vector , a column of A ; bi ( rn x 1 ) is a term vector , the corresponding column in B ; n is the number of distinct source words and m is the number of distinct target words .</sentence>
				<definiendum id="0">k</definiendum>
				<definiendum id="1">rn x 1 )</definiendum>
				<definiendum id="2">n</definiendum>
				<definiens id="0">satisfy all the given text/term pairs</definiens>
				<definiens id="1">a term vector , the corresponding column in B</definiens>
			</definition>
			<definition id="1">
				<sentence>The LLSF problem is to find W which minimizes the sum k k i=l i=1 where ~ d=~ Wgl b'i is the mapping error of the ith text/term pair ; the notation 11 ... 112 is vector 2-norm , defined as 11712 x\ ] r~ ' 2 = =iv~ and ~'is m x 1 ; II ... lit is the Frobenius matrix norm , defined as IIMIIF = m 2 q i=1 j=l and M is m x k. The meaning of the LLSF problem is to find the mapping function W that minimizes the total mapping errors for a given text/term pair collection ( the `` training AcrEs DE COLING-92 , NANTES , 23-28 AOt~r 1992 4 4 8 PROC .</sentence>
				<definiendum id="0">LLSF problem</definiendum>
				<definiendum id="1">II ... lit</definiendum>
				<definiendum id="2">M</definiendum>
			</definition>
			<definition id="2">
				<sentence>3 , Mapping arbitrary queries to canonical terms The LLSF mapping consists of the following steps : ( 1 ) Given an arbitrary text ( a `` query '' ) , first form a query vector , ~ , in the source vector space .</sentence>
				<definiendum id="0">LLSF mapping</definiendum>
				<definiens id="0">consists of the following steps : ( 1 ) Given an arbitrary text ( a `` query '' ) , first form a query vector , ~ , in the source vector space</definiens>
			</definition>
			<definition id="3">
				<sentence>The patient records include diagnoses ( DXs ) written by physicians , operative reports written by surgeons , etc .</sentence>
				<definiendum id="0">patient records</definiendum>
			</definition>
			<definition id="4">
				<sentence>The target language consists of 376 canonical names of cardiovascular diseases as defined in the classification system ICD-9-CM \ [ 9\ ] .</sentence>
				<definiendum id="0">target language</definiendum>
			</definition>
			<definition id="5">
				<sentence>The LLSF method gives an effective solution for capturing semantic implications between surface expressions .</sentence>
				<definiendum id="0">LLSF method</definiendum>
				<definiens id="0">gives an effective solution for capturing semantic implications between surface expressions</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>Compositionality is defined as the property that the meaning of a whole is a function of the meaning of its parts ( cf. e.g. Keenan and Faltz ( 1985 ) , pp.24-25 ) ?</sentence>
				<definiendum id="0">Compositionality</definiendum>
				<definiens id="0">the property that the meaning of a whole is a function of the meaning of its parts ( cf. e.g. Keenan and Faltz</definiens>
			</definition>
			<definition id="1">
				<sentence>Clearly , the meaning of any string is a composition of the meanings of digits ( notice that the values of the digits are the same in both bases ) .</sentence>
				<definiendum id="0">meaning of any string</definiendum>
				<definiens id="0">a composition of the meanings of digits</definiens>
			</definition>
			<definition id="2">
				<sentence>Let M be a set s.t. for any swhich is a member of S , there ism=m ( s ) which isa member of Ms.t.m is the meaning of s. We want to show that there is a compositional semantics for S which agrees with the function associating m with m ( s ) , which will be denoted by rn ( x ) .</sentence>
				<definiendum id="0">s )</definiendum>
				<definiens id="0">a member of S , there ism=m ( s ) which isa member of Ms.t.m is the meaning of s. We want to show that there is a compositional semantics for S which agrees with the function associating m with m (</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>Zeta-Machines '' sales manager , Brian Wilson , says that they also plan to market the Gamma-Z , a CAD/CAM workstation based on a Connection Machine architecture .</sentence>
				<definiendum id="0">Zeta-Machines</definiendum>
			</definition>
			<definition id="1">
				<sentence>Global text structure can also be introduced by a variety of topics which share conceptual commonalities ( facets ) at the knowledge repreSelltation level ( not necessarily need this be paralleled with properties actually mentioned in the text ! )</sentence>
				<definiendum id="0">repreSelltation level</definiendum>
				<definiens id="0">introduced by a variety of topics which share conceptual commonalities ( facets ) at the knowledge</definiens>
			</definition>
			<definition id="2">
				<sentence>Basically ( see Figure 1 ) , these are constituted by the PARSE BULLETIN , a blackboard-type memory which records the single events of the parsing process , the DOMAIN KNOWLEDGE BASE , which contains file domain-specific background knowledge needed for the parse , and various EXPER~Ps for actually driving the parse through the text grammar specifications they incorporate ( cf. tlahn \ [ 1990\ ] for a more comprehensive presentation ) .</sentence>
				<definiendum id="0">PARSE BULLETIN</definiendum>
				<definiendum id="1">DOMAIN KNOWLEDGE BASE</definiendum>
				<definiens id="0">records the single events of the parsing process , the</definiens>
			</definition>
			<definition id="3">
				<sentence>ame , slot , bullpos ) where frame is file name of a frame , and slot is the name of a slot of that frame , both co-occurring as lexical parameters of some parse tuple in the PARSE BULLETIN with a LC*-typc pal~e descriptor ; bullpos gives file parse point in file PARSE BULLETIN where frwne mid slot occur iustzmtancously .</sentence>
				<definiendum id="0">frame</definiendum>
				<definiendum id="1">slot</definiendum>
				<definiens id="0">lexical parameters of some parse tuple in the PARSE BULLETIN with a LC*-typc pal~e descriptor</definiens>
			</definition>
			<definition id="4">
				<sentence>theme ( textptw~ tes~pos ) = ( theme , IffI~X~IES , newpos ) if\ [ `` ( a ) testpos &lt; textpos &amp; ( b ) ( textpos , O , EOP ) is in the PARSE BUIA , ETIN ~ &amp; ( el ( prepos , O , COP ) is also in the PMLSE BULLI ' ; TIN such that prepos &lt; textpos and such that no other triple with '¢ ' as text item interwmes between prepos and textpos in the l'Al~qE BUIAA : ,~ TIN &amp; ( d ) newpos • Imax ( prepos , testpos ) +1 , textposI \ [ &amp; ( el theme is a frame in the DOMAIN KNOWLEI ) GE BASE &amp; ( f ) V ki c \ [ max ( prepos , testpos ) ~1 , ~tewpos1\ ] : ( theme , slot , k i ) { TIIEME8 .</sentence>
				<definiendum id="0">theme</definiendum>
				<definiendum id="1">Imax</definiendum>
				<definiens id="0">text item interwmes between prepos and textpos in the l'Al~qE BUIAA : ,~ TIN &amp; ( d ) newpos •</definiens>
				<definiens id="1">a frame in the DOMAIN KNOWLEI ) GE BASE &amp; ( f ) V ki c \ [ max ( prepos , testpos ) ~1 , ~tewpos1\ ] : ( theme , slot , k i</definiens>
			</definition>
			<definition id="5">
				<sentence>( { 1 ) After lixing the search intelwll in the bulletin for which a col\ ] stanl IhenK : is going to bc coiuputed , tle~ .</sentence>
				<definiendum id="0">stanl IhenK</definiendum>
				<definiens id="0">going to bc coiuputed</definiens>
			</definition>
			<definition id="6">
				<sentence>The PARSE BULLETIN contains a sequence of local theme-theme pairs withframeTi being tile current local theme and slotftllerTi being its associated local rheme .</sentence>
				<definiendum id="0">PARSE BULLETIN</definiendum>
				<definiens id="0">contains a sequence of local theme-theme pairs withframeTi being tile current local theme</definiens>
			</definition>
			<definition id="7">
				<sentence>Tiros , various coherence phenomemt can be distinguished by particular instantiation pattents : f3 constant theme is defined by multiple instantiafions of aggregatiou ( or conceptual association ) relations for one particular f'r ' , une item in KB ; ffl continuous thematization of rhemes is deiino ed by multiple instantiations of aggregation relations for continuously changing , though locally overlapping frame items in KB ; \ [ -J derived theme is defined by multiple instantiations of generalization/classilication relations holding between subparts of a frame hierarchy in KB .</sentence>
				<definiendum id="0">Tiros</definiendum>
				<definiens id="0">conceptual association ) relations for one particular f'r ' , une item in KB ; ffl continuous thematization of rhemes is deiino ed by multiple instantiations of aggregation relations for continuously changing , though locally overlapping frame items in KB ; \ [ -J derived theme is defined by multiple instantiations of generalization/classilication relations holding between subparts of a frame hierarchy in KB</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>Grammar based robustness tools have been explored in a variety of formalisms , e.g. the metarule device within the ATN formalism ( Weischedel and Sondheimer 1898 ) , entity data structures in a case frame approach ( Hayes 1984 ) or the weak description approach in unification based grammars ( Kudo et al. 1988 , Goeser 1990 ) .</sentence>
				<definiendum id="0">Grammar based robustness tools</definiendum>
				<definiens id="0">explored in a variety of formalisms</definiens>
			</definition>
			<definition id="1">
				<sentence>The SUB index licenses arbitrary terminal strings to the right or left of the indexed symbol 's lexied projection .</sentence>
				<definiendum id="0">SUB index</definiendum>
				<definiens id="0">licenses arbitrary terminal strings to the right or left of the indexed symbol 's lexied projection</definiens>
			</definition>
			<definition id="2">
				<sentence>For a cfg G = &lt; Cat , Lex , P , , qset &gt; , where Cat is a set of non-terminals , Lez a set of terminals , P a set of rules and , qset a set of start symbols , it is charact , ; ri~ed by the fonowing predictor concept : * the predictor is a relation D ( i , A ) C n + x C , al between a vertex i &lt; n and a rtort-termirtal .</sentence>
				<definiendum id="0">Cat</definiendum>
				<definiens id="0">a set of non-terminals</definiens>
			</definition>
			<definition id="3">
				<sentence>ULMER TEXTBANK : A machlne-readable corpus of spoken language from psychotherapeutic discourse , University of Uhn Weischedel , R.M. and Sondhelmer , N.K. : Metarules as n Basis for Processing HIFormed Input , in : AJCL 9 , 3-4 , 1983</sentence>
				<definiendum id="0">ULMER TEXTBANK</definiendum>
				<definiens id="0">A machlne-readable corpus of spoken language from psychotherapeutic discourse</definiens>
			</definition>
</paper>

		<paper id="4205">
			<definition id="0">
				<sentence>The output of the mapping is a set of tuples &lt; ei , R , ej &gt; where ei and ej represent events and R is the temporal relation which exists between them ( overlap , precedence or temporal inclusion ) .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">a set of tuples &lt; ei , R , ej &gt; where ei and ej represent events</definiens>
				<definiens id="1">the temporal relation which exists between them ( overlap , precedence or temporal inclusion )</definiens>
			</definition>
			<definition id="1">
				<sentence>Inclusion is a special case of overlap ; thus , when an event is temporally included in another these events also overlap , and the algorithm makes use of this fact .</sentence>
				<definiendum id="0">Inclusion</definiendum>
				<definiens id="0">a special case of overlap</definiens>
			</definition>
			<definition id="2">
				<sentence>The relations of overlap and precedence which hold between pairs of events are governed by a set of axioms ; event structures are defined as a collection of events and constraints .</sentence>
				<definiendum id="0">event structures</definiendum>
				<definiens id="0">The relations of overlap and precedence which hold between pairs of events are governed by a set of axioms</definiens>
				<definiens id="1">a collection of events and constraints</definiens>
			</definition>
			<definition id="3">
				<sentence>( 3 ) T.empor~l input from the phonetic leve_l &lt; voiceless , 0 , 91A9 , C &gt; &lt; voiced , 91.2 , 517.5 , C &gt; &lt; glide , 452.6 , 498.2 , C &gt; &lt; occlusive , 0 , 35.4 , C &gt; &lt; transient , 34.5 , 641.6 , C &gt; &lt; noise , ( : ~ ) .61 , 91.16 , C &gt; &lt; vowellike , 94.29 , 392.6 , C &gt; &lt; nasal , 402.9 , 518.6 , C &gt; &lt; bilabial , 20.45 , 93.2 , C &gt; &lt; tongue-retracted , 93.21 , 392.6 , C &gt; &lt; bilabial , 392.62 , 518.2 , C &gt; ( 4 ) Ev¢nt invent~ ) r3~ et : VOI ( voiceless , &lt; 0,91.19 &gt; ) e2 : VOl ( voiced , &lt; 91.2,517.5 &gt; ) e~ : GLt ( glide , &lt; 452.6,498.2 &gt; ) e~ : OCC ( oeclusive , &lt; 0,35.4 &gt; ) es : TRA ( transient , &lt; 34.5,60.6 &gt; ) e6 : NOl ( noise , &lt; 60.61,91.16 &gt; ) eT : VOW ( vowellike , &lt; 94.29,392.6 &gt; ) es : NAS ( nasal , &lt; 402.9,518.6 &gt; ) e~ : LAB ( bilabial , &lt; 20.45,93.2 &gt; ) et0 : TON ( retracted , &lt; 93.21,392.6 &gt; ) eL~ : LAB ( bilabial , &lt; 392.62,518.2 &gt; ) Of particular interest to the phonological parser are the precedence relations between those event properties of the same type and the overlap and temlx~ral inclusion relations between event properties of differing types .</sentence>
				<definiendum id="0">NAS</definiendum>
				<definiens id="0">the precedence relations between those event properties of the same type and the overlap and temlx~ral inclusion relations between event properties of differing types</definiens>
			</definition>
			<definition id="4">
				<sentence>~t~r ( fra~mCn 0 c~ &lt; c4 ( explicitly required by phonotactics ) e2 &lt; e4 ( explicitly required by phonotactic~s ) e 3 &lt; e 4 ( explicitly required by phonotaetics ) el ° e2 ( not specified by phonotactics ) e~ ° e~ ( not specified by phonotactics ) e 2 ° c~ ( explicitly required by phonotactics ) AC'I~ES DE COLING-92 .</sentence>
				<definiendum id="0">~t~r</definiendum>
				<definiens id="0">not specified by phonotactics ) e~ ° e~ ( not specified by phonotactics</definiens>
			</definition>
</paper>

		<paper id="4179">
			<definition id="0">
				<sentence>For example , to describe a participant who is active and possibly affected , but not responsible nor directly affected , the Passive and Causative Construe 'lhe t'~asive Coils ( ruction example : `` Ma~y wa~ , given a fork '' ( 'r ) l ; dd .</sentence>
				<definiendum id="0">Passive</definiendum>
				<definiens id="0">active and possibly affected , but not responsible nor directly affected , the</definiens>
			</definition>
			<definition id="1">
				<sentence>FIG , a 'Flexible Incremental Generator ' , produces English and Japanese sentences starting from a meaning representation , using spreading activation in a knowledge net , work .</sentence>
				<definiendum id="0">FIG</definiendum>
			</definition>
			<definition id="2">
				<sentence>FI ( '~ originally expected deep ( : as ( : relations in its int ) uts , and its grmnrnar and lexicon referred to those cases .</sentence>
				<definiendum id="0">FI</definiendum>
				<definiens id="0">relations in its int ) uts</definiens>
			</definition>
</paper>

		<paper id="2092">
			<definition id="0">
				<sentence>In the MARRIAGE PROBLEM the task is a constrained pairwise matching of elements from two disjoint sets , while in the MENAGE A TROIS PROBLEM , the task is the construction of triples based on elements from three disjoint sets .</sentence>
				<definiendum id="0">TROIS PROBLEM</definiendum>
				<definiens id="0">the construction of triples based on elements from three disjoint sets</definiens>
			</definition>
</paper>

		<paper id="2111">
			<definition id="0">
				<sentence>The example in Figure II shows the mapping rule for the absurdity rule , where x denotes a derivation path .</sentence>
				<definiendum id="0">x</definiendum>
				<definiens id="0">a derivation path</definiens>
			</definition>
			<definition id="1">
				<sentence>\ [ 1\ ] Haeusler , E.H. Automatic Theorem Proving : An Attempt to Improve Readability of proofs Generated by Resolution .</sentence>
				<definiendum id="0">E.H. Automatic Theorem Proving</definiendum>
				<definiens id="0">An Attempt to Improve Readability of proofs Generated by Resolution</definiens>
			</definition>
</paper>

		<paper id="3161">
			<definition id="0">
				<sentence>ABSTRACT Shall2 is a knowledge-based machine translation system with a symmetric architecture .</sentence>
				<definiendum id="0">ABSTRACT Shall2</definiendum>
				<definiens id="0">a knowledge-based machine translation system with a symmetric architecture</definiens>
			</definition>
			<definition id="1">
				<sentence>Shalt2 is a research prototype of a knowledge-based , multi-domain/multi-lingual MT system .</sentence>
				<definiendum id="0">Shalt2</definiendum>
			</definition>
			<definition id="2">
				<sentence>Second , conceptual structures are a desirable interface for representing the meaning of sentences , machine-generated output ( such as diagnosis by an expert system ) , and expressions in graphical languages .</sentence>
				<definiendum id="0">conceptual structures</definiendum>
				<definiens id="0">a desirable interface for representing the meaning of sentences , machine-generated output ( such as diagnosis by an expert system ) , and expressions in graphical languages</definiens>
			</definition>
			<definition id="3">
				<sentence>F-structure in the PUG English grammar : ( ( ROOT insert ) ( CAT v ) ( SUBCAT trans ) ( FORM inf ) ( MOOD imp ) ( TENSE pros ) ( 0BJ ( ( ROOT diskette ) ( CAT n ) ( DET indef ) ( NUM sg ) ) ) ( PPADJUNCT ( ( ( ROOT drive ) ( CAT n ) ( DET def ) ( NUM sg ) ) ) ) ) DS : insert ( CAT v , SUBCAT traus , MOOD imp , TENSE pros ) DOBJECT diskette ( CAT n , DET indof , NUM sg ) PPADJUNCT drive ( CAT n , PREP into , DET def , NUM ag ) The reader *nay no*lee that the above sentence should really have ambiguities in prepositional phrase attachment , which result in two conflicting dependencies `` insert -PPADJUNCTdrive '' and `` diskette PPADJUNCTdrkve '' in a single DS .</sentence>
				<definiendum id="0">NUM ag</definiendum>
				<definiendum id="1">diskette PPADJUNCTdrkve</definiendum>
				<definiens id="0">ROOT insert ) ( CAT v ) ( SUBCAT trans ) ( FORM inf ) ( MOOD imp ) ( TENSE pros ) ( 0BJ ( ( ROOT diskette ) ( CAT n ) ( DET indef ) ( NUM sg ) ) ) ( PPADJUNCT ( ( ( ROOT drive ) ( CAT n ) ( DET def ) ( NUM sg ) ) ) ) ) DS : insert ( CAT v , SUBCAT traus , MOOD imp , TENSE pros ) DOBJECT diskette ( CAT n , DET indof , NUM sg ) PPADJUNCT drive ( CAT n</definiens>
			</definition>
			<definition id="4">
				<sentence>The NL class system consists of a set of constant classes , three meta-classes , and virtual classeo with an exclusive inheritance mechanism discussed below .</sentence>
				<definiendum id="0">NL class system</definiendum>
				<definiens id="0">consists of a set of constant classes , three meta-classes , and virtual classeo with an exclusive inheritance mechanism discussed below</definiens>
			</definition>
			<definition id="5">
				<sentence>The *malething is a class that includes instances of any class with the : sex slot filled with *male .</sentence>
				<definiendum id="0">*malething</definiendum>
				<definiens id="0">a class that includes instances of any class with the : sex slot filled with *male</definiens>
			</definition>
			<definition id="6">
				<sentence>An example of a paraphrasing rule is ( oquiv ( *equal ( : agent ( *X ( : hUm ( *V ) ) ) ) ( : theme ( *Y/*porson ( : def *indef ) ( : sum ( *W ) ) ) ) ) ( *Z/*ac % ion ( : agent ( *X ) ( : num ( *V ) ) ) ) ( such-that ( humanization *Z *Y ) ( sibling *V *W ) ) ) where *Y/*person specifies *Y to be an instance of any subclass of *person , *equal is roughly the verb `` be , '' humanization is a relation that holds for pairs such ms ( *singer , *sing ) and ( *swimmer , *swim ) , and sibling holds for two instances of the same class .</sentence>
				<definiendum id="0">*X</definiendum>
				<definiendum id="1">*X )</definiendum>
				<definiendum id="2">humanization</definiendum>
				<definiens id="0">a relation that holds for pairs such ms ( *singer , *sing ) and ( *swimmer , *swim ) , and sibling holds for two instances of the same class</definiens>
			</definition>
			<definition id="7">
				<sentence>JAUNT applies grammatical constraints ( for instance , nlodifier-modifiee links between nodes do not cross one another ) and semantic constraints ( such as selectional restrictions , t~ functional control , and other NL object identity constraints detected by the context analyzer ) uniformly to a DS that has ambiguities , and calculates pairwise consistency efficiently for each combination of nodes .</sentence>
				<definiendum id="0">semantic constraints</definiendum>
			</definition>
			<definition id="8">
				<sentence>The context analyzer uses a similar method to determine lexical ambiguities that were not resolved by the sentence analyzer wlmn the case base failed to provide enough information .</sentence>
				<definiendum id="0">context analyzer</definiendum>
				<definiens id="0">uses a similar method to determine lexical ambiguities that were not resolved by the sentence analyzer wlmn the case base failed to provide enough information</definiens>
			</definition>
</paper>

		<paper id="2065">
			<definition id="0">
				<sentence>\ [ Jelinek et al. , 1990\ ] characterize a probabilistic context-free grammar ( PCFG ) as a context-free grammar in which each production has been assigned a probability of use .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">characterize a probabilistic context-free grammar (</definiens>
			</definition>
			<definition id="1">
				<sentence>Tile nmtual information between two events is defined as • Pr ( x , y ) l ( x ; Y ) = tOgp~ ) where , for tile purposes of corpus analysis , Pr ( x ) and Pr ( y ) are the respective probabilities of words x and y appearing within the corpus , and Pr ( z , y ) is the probability of word x being followed by word y within a window of w words .</sentence>
				<definiendum id="0">Pr ( y )</definiendum>
				<definiens id="0">x , y ) l ( x ; Y ) = tOgp~ ) where , for tile purposes of corpus analysis</definiens>
				<definiens id="1">the respective probabilities of words x and y appearing within the corpus</definiens>
				<definiens id="2">the probability of word x being followed by word y within a window of w words</definiens>
			</definition>
			<definition id="2">
				<sentence>Hindle uses co-occurrence statistics , collected over parse trees , in order to classify nouns on the basis of the verb contexts in which they appear .</sentence>
				<definiendum id="0">Hindle</definiendum>
				<definiens id="0">uses co-occurrence statistics , collected over parse trees</definiens>
			</definition>
			<definition id="3">
				<sentence>I verb \ ] subject I object I e~t we food breathe we air drink we water sust'ain land us Next , Hindle calculates a similarity measure based on tile mutual information of verbs and their arguments : nouns that tend to appear as subjects and objects of the sanae verbs are judged more similar .</sentence>
				<definiendum id="0">Hindle</definiendum>
				<definiens id="0">calculates a similarity measure based on tile mutual information of verbs and their arguments : nouns that tend to appear as subjects</definiens>
			</definition>
			<definition id="4">
				<sentence>a~ is proposed as an entry in ttle chart spanning input symbols al through ai , it is assigned a `` score '' based on hal symbol A , and For example , given the input My first love was named Pearl , a proposed chart entry VP -- -+ V NP starting its span at love ( i.e. , a theory trying to interpret love as a verb ) would be scored on the basis of the rule that generated the VP ( in this ease , probably S -- -* NP VP ) together with the part-of-speech trigram `` adjective verb verb . ''</sentence>
				<definiendum id="0">VP</definiendum>
				<definiens id="0">a theory trying to interpret love as a verb</definiens>
			</definition>
			<definition id="5">
				<sentence>So , although tire context-free probability favors the interpretation of love as the beginning of a verb phrase , ACRES DI '' COLUqG-92 , NANTES , 23-28 AOl3 '' r 1992 4 2 0 PROC .</sentence>
				<definiendum id="0">ACRES DI</definiendum>
				<definiens id="0">the beginning of a verb phrase</definiens>
			</definition>
			<definition id="6">
				<sentence>The adjunction operation is a generalization of substitution that permits internal as well as frontier nodes to be expanded .</sentence>
				<definiendum id="0">adjunction operation</definiendum>
			</definition>
			<definition id="7">
				<sentence>3Schabes ( personal communication ) capture~ the generallzation quite nicely in obscxving that for both CFG derivation trees and TAG derivation histories , the path setA ( set of possible paths from root to frontier ) are regular .</sentence>
				<definiendum id="0">3Schabes</definiendum>
				<definiendum id="1">path setA</definiendum>
				<definiens id="0">personal communication ) capture~ the generallzation quite nicely in obscxving that for both CFG derivation trees and TAG derivation histories</definiens>
			</definition>
			<definition id="8">
				<sentence>A probabilis~ic tree-adjoining grammar is a 5-tuple , ( I , A , P1 , Ps , Pa ) , where I and A are defined as above , PI is a function from I to the real interval \ [ 0,1\ ] , and Ps and PA are functions from fl to \ [ 0,1\ ] , such that : aEl c~EI flEAU ( .</sentence>
				<definiendum id="0">probabilis~ic tree-adjoining grammar</definiendum>
				<definiendum id="1">PI</definiendum>
				<definiendum id="2">Ps</definiendum>
				<definiens id="0">a function from I to the real interval \ [ 0,1\ ] , and</definiens>
			</definition>
			<definition id="9">
				<sentence>o.¢ ) Pl ( c~ ) is interpreted as the probability that a derivation begins with initial tree a. Ps ( S ( ce , cd , r/ ) ) denotes the probability of substituting cd at node ~t of tree a , and Pa ( A ( a , ~ , r/ ) ) denotes the probability of adjoining fl at node 7/of tree a ( where PA ( A ( a , none , t\ ] ) ) denotes the probability that no adjunetion takes place at node o of a ) .</sentence>
				<definiendum id="0">Pl ( c~</definiendum>
				<definiendum id="1">) )</definiendum>
				<definiendum id="2">Pa ( A</definiendum>
				<definiendum id="3">r/ ) )</definiendum>
				<definiens id="0">the probability that a derivation begins with initial tree a. Ps ( S ( ce , cd , r/</definiens>
				<definiens id="1">the probability of substituting cd at node ~t of tree a</definiens>
				<definiens id="2">the probability of adjoining fl at node 7/of tree a ( where PA ( A ( a , none</definiens>
				<definiens id="3">the probability that no adjunetion takes place at node o of a )</definiens>
			</definition>
			<definition id="10">
				<sentence>Denoting each operation as op ( cq , a2 , ~l ) , op E { S , A } , and denoting the initial tree with whieb tim derivation starts as ~0 , the probability of a TAG derivation ~-= ( , to , opt ( ... ) ... .. op , ( ... ) ) i~ Pr ( ~ ) = P~ ( , ~o ) ~I Po~ , ( op , ( ... ) ) l &lt; i &lt; _n This definition is directly analogous to the probability of a context-free derivation r = ( rh ... , r , ) , Pr ( r ) = PI ( S ) H P ( ri ) , l &lt; i &lt; _n though in a CFG every derivation starts with S and so PI ( S ) -= 1 .</sentence>
				<definiendum id="0">Denoting each operation as op</definiendum>
				<definiens id="0">the probability of a context-free derivation r = ( rh ... , r , ) , Pr ( r ) = PI ( S ) H P ( ri ) , l &lt; i &lt; _n though in a CFG every derivation starts with S and so PI ( S ) -= 1</definiens>
			</definition>
			<definition id="11">
				<sentence>Pearl : A probabilistic chart parser .</sentence>
				<definiendum id="0">Pearl</definiendum>
				<definiens id="0">A probabilistic chart parser</definiens>
			</definition>
</paper>

		<paper id="2109">
			<definition id="0">
				<sentence>Section 3 describes the approach , which provides a straightforward treatment of cases where filler-gap dependencies are subject to different constraints in source and target languages ( e.g. where one language allows , and the other forbids , Preposition Stranding ) .</sentence>
				<definiendum id="0">Preposition Stranding</definiendum>
				<definiens id="0">provides a straightforward treatment of cases where filler-gap dependencies are subject to different constraints in source and target languages ( e.g. where one language allows</definiens>
			</definition>
			<definition id="1">
				<sentence>tource tente~e , the minimal ~mcture aatigned to the Utrget teatznce by the argot grammar mntt be •ubturned by the mlnim•l solution of the • and Z ' coettndnL ( 4 ) NP -- ~ NP S ~ tRELMOD-~ `` ~ ( t RELMOD ) ' ( '~ t RELMOD ) x ( \ [ RELTOPIC ) - ( x~ , RELTOPIC ) ( 5 ) S ' -- * XP ( tRELTOPIC ) ~ , ( t { XCOMP , COMP } * GV3 ( 6 ) see : V PRED-'see &lt; SUBJ , OBJ &gt; ' x ( t SU~ ' ) - ( T t strm3 z ( tom ) = ( Ttom ) ( 7 ) who : N PRED-'who ' HUMAN-+ ( '~ tPRED FN ) ='OUE '6 In the functional uncertaimy equation in ( 5 ) , { XCOMP , COMP } * allows the 'gap ' associated with the RELTOPIC to be inside zero or more COMPs or XCOMPs , and GF is an abbreviation for a set of paths including length one paths such as SUBJ , OBJ , etc. , and paths of length two , such OBLto OBJ , which allows preposition stranding , as in man who i I replied to Hi ( 8 ) IRELTOPIC e2 \ [ PRED 'who'\ ] \ ] \ ] ( 9 ) \ [ RELTOPIC f2 \ [ PRED 'QUE'\ ] \ ] \ ] The equations on rule ( 4 ) , which are specifically for dealing with relative slrnctmr , s , are quite simple in themselves , and combine vmanta qua , qui , /aq~//e , ©tc .</sentence>
				<definiendum id="0">XCOMP , COMP } *</definiendum>
				<definiendum id="1">GF</definiendum>
				<definiens id="0">allows the 'gap ' associated with the RELTOPIC to be inside zero or more COMPs</definiens>
				<definiens id="1">an abbreviation for a set of paths including length one paths such as SUBJ , OBJ , etc. , and paths of length two , such OBLto OBJ , which allows preposition stranding</definiens>
				<definiens id="2">s , are quite simple in themselves , and combine vmanta qua , qui , /aq~//e , ©tc</definiens>
			</definition>
			<definition id="2">
				<sentence>I , and 'thematic ' OBLiques ( which includes OBLgo ) , excluding the possibility of preposition stranding .</sentence>
				<definiendum id="0">OBLiques</definiendum>
				<definiens id="0">includes OBLgo ) , excluding the possibility of preposition stranding</definiens>
			</definition>
			<definition id="3">
				<sentence>The `` closest '' solution is defined as the pemfissibie solution whicfi contains the minimal solution of the equation ( 19 ) ( which requires RELTOPIC to contain a +WH item ) .</sentence>
				<definiendum id="0">closest '' solution</definiendum>
				<definiens id="0">the pemfissibie solution whicfi contains the minimal solution of the equation ( 19 ) ( which requires RELTOPIC to contain a +WH item )</definiens>
			</definition>
</paper>

		<paper id="3133">
			<definition id="0">
				<sentence>When the predicate is a noun or a noun phrase ( DNOM ) , as in the remaining two `` block '' sentences , we have to ask if that predicate nominative is the same term as the subject ( or is an equivalent empty anaphorie team , like `` one '' ) .</sentence>
				<definiendum id="0">DNOM</definiendum>
				<definiens id="0">the same term as the subject ( or is an equivalent empty anaphorie team , like `` one '' )</definiens>
			</definition>
			<definition id="1">
				<sentence>Giving a name to the relation ( and , for that purpose , knowing that love is a concept , garden is a place , and table is an object ) is the task of the sense disambiguation component , which consults dictionary def'mitions to find the necessary semantic information .</sentence>
				<definiendum id="0">garden</definiendum>
				<definiendum id="1">table</definiendum>
				<definiens id="0">the task of the sense disambiguation component , which consults dictionary def'mitions to find the necessary semantic information</definiens>
			</definition>
			<definition id="2">
				<sentence>Among these , we ackatowledge here especially the following : George Heidom , who provided us with tools and advice ; Joel Fagan , who initialized rite concept grammar work ( see Fagan 1990 ) ; and Wlodek Zadrozny , with whom we have had lively and interesting conversations about senmntics .</sentence>
				<definiendum id="0">Joel Fagan</definiendum>
				<definiens id="0">the following : George Heidom , who provided us with tools and advice ;</definiens>
			</definition>
</paper>

		<paper id="3144">
			<definition id="0">
				<sentence>The Budapest Unification Grammar ( BUG ) system described in this paper is a system for generating natural language parsers from feature-structure based grammatical descriptions ( graamnars ) .</sentence>
				<definiendum id="0">Budapest Unification Grammar</definiendum>
				<definiens id="0">a system for generating natural language parsers from feature-structure based grammatical descriptions ( graamnars )</definiens>
			</definition>
			<definition id="1">
				<sentence>The string completion limit , which is a small integer parameter of BUG 's compiler , expresses a performance limitation that BUG incorporates into the automaton it produces .</sentence>
				<definiendum id="0">string completion limit</definiendum>
				<definiens id="0">a small integer parameter of BUG 's compiler</definiens>
			</definition>
			<definition id="2">
				<sentence>This is Kornai 's ( 1984 ) hypothesis , in terms of which any string that can he the beginning of a grammatical string can be completed with k or less terminal symbols , where k ( i.e. , the SCL ) is a small integer .</sentence>
				<definiendum id="0">k</definiendum>
				<definiendum id="1">SCL )</definiendum>
				<definiens id="0">a small integer</definiens>
			</definition>
			<definition id="3">
				<sentence>In those cases when the above procedure would not terminate ( i.e. , when s2 is an accepting state in A and B is the same RTN as some other RTN C the acceptance of which takes the machine to s~ , we eliminate the transition corresponding to B , and collapse sl with the initial state of C ( with the standard technique ) .</sentence>
				<definiendum id="0">B</definiendum>
			</definition>
</paper>

		<paper id="2102">
			<definition id="0">
				<sentence>A logical form consists of an unordered net of Ierms ; each term is either a property predicated of an it~dex , or ~t relation between two indices .</sentence>
				<definiendum id="0">logical form</definiendum>
				<definiens id="0">consists of an unordered net of Ierms ; each term is either a property predicated of an it~dex , or ~t relation between two indices</definiens>
			</definition>
			<definition id="1">
				<sentence>it ) IIead-switching as hi the translatlolt of Gernlan tlaas sehwimmt gem -John likes swinmfing John swinm ~ladly like ( L ) &amp; obj ( L , S ) geru ( L ) &amp; subj ( L , S ) iyshift ( L , S ) The Germau sentence is a description of a situation to do with swimming ; the English is a description of a situation to do with liking .</sentence>
				<definiendum id="0">sentence</definiendum>
			</definition>
			<definition id="2">
				<sentence>Shifl ( H , E ) means that in any instance of subj ( H , X ) or l ' ( X , tt ) ( where V stands tbr auy rela~ tion ) in the logical form representing the English text , the H corresponds to an E in the logical form representing the other language .</sentence>
				<definiendum id="0">Shifl</definiendum>
				<definiendum id="1">E )</definiendum>
				<definiendum id="2">V</definiendum>
				<definiens id="0">stands tbr auy rela~ tion ) in the logical form representing the English text , the H corresponds to an E in the logical form representing the other language</definiens>
			</definition>
			<definition id="3">
				<sentence>wear ( w ) &amp; obj ( w , o ) &amp; coord ( o , ol ) &amp; hat ( ol ) &amp; coord ( o , o2 ) &amp; shoe ( o2 ) Input Logical Form U Expansi ... .. \ [ Expandruleu \ ] , 'runsler +\ [ Transfer rules \ ] g Reduction Output Logical Form t , 'igure 2 : Transfer with Coordinate Expansion In this form , o is a linguistic object , and a predicate coord represents a relation between the linguistic object and its coustituents .</sentence>
				<definiendum id="0">obj</definiendum>
				<definiendum id="1">predicate coord</definiendum>
			</definition>
</paper>

		<paper id="4207">
			<definition id="0">
				<sentence>nstrainl : s • The Spatial Eutitivs in the World ganmshita Park ( a park ) , il i~ , ttntaill .</sentence>
				<definiendum id="0">Spatial Eutitivs</definiendum>
				<definiens id="0">a park ) , il i~ , ttntaill</definiens>
			</definition>
</paper>

		<paper id="4192">
			<definition id="0">
				<sentence>Lexical covering takes an important part in such system assessment .</sentence>
				<definiendum id="0">Lexical covering</definiendum>
				<definiens id="0">takes an important part in such system assessment</definiens>
			</definition>
</paper>

		<paper id="3126">
</paper>

		<paper id="4194">
</paper>

		<paper id="2078">
			<definition id="0">
				<sentence>The lexical database I , OLA has been developed in connection with the Logicprogramming-based Machine Translation ( LMT ) system and shall be presented here .</sentence>
				<definiendum id="0">OLA</definiendum>
				<definiens id="0">developed in connection with the Logicprogramming-based Machine Translation ( LMT ) system and shall be presented here</definiens>
			</definition>
			<definition id="1">
				<sentence>Transl , exis aims at the theoretically and empirically well motivated lexical description and the management of the lexical information of LMT in a database .</sentence>
				<definiendum id="0">Transl</definiendum>
				<definiens id="0">exis aims at the theoretically and empirically well motivated lexical description and the management of the lexical information of LMT in a database</definiens>
			</definition>
			<definition id="2">
				<sentence>LMT , developed by Michael MeCord , is in basic design a source-based transfer system in which the source analysis is done with Slot Grammar ( cf. McCord 1989 , 1990 , forthcoming ) .</sentence>
				<definiendum id="0">LMT</definiendum>
				<definiens id="0">in basic design a source-based transfer system in</definiens>
			</definition>
			<definition id="3">
				<sentence>I , MT currently requires the lollowing types of information to bc specified k ) r lexical units ( I , lJ ) : pat1 of speech ; u word SellSCS i u morphological properties ; rl agreelnent features ; o the valency , i.e. the li'anlc of optiomd/ohligatory complement slots ; o the specification of the fillers ( Nl ) s , suhordinate clauses ) for each slot ; t~ semantic compatibility constraints and collocations ; n characterization of mulliword lexmncs ; u subject area ; translation relations ; tq lexieal transtbmmtions .</sentence>
				<definiendum id="0">rl agreelnent</definiendum>
				<definiendum id="1">t~ semantic compatibility constraints</definiendum>
				<definiens id="0">information to bc specified k ) r lexical units ( I</definiens>
				<definiens id="1">the specification of the fillers ( Nl ) s , suhordinate clauses</definiens>
			</definition>
			<definition id="4">
				<sentence>I , MT is file technical basis of an international project at IBM wifll cooperation between IBM Researdl , tile IBM Science Centers in lleidelberg , Madrid , Paris , llaifa , and Cairo , and IBM European l.anguage Services in Copenhagen ( cf. Rimon et al. 1991 ) .</sentence>
				<definiendum id="0">MT</definiendum>
			</definition>
			<definition id="5">
				<sentence>The ER-diagram is a simplified version of the actual conceptual model .</sentence>
				<definiendum id="0">ER-diagram</definiendum>
				<definiens id="0">a simplified version of the actual conceptual model</definiens>
			</definition>
			<definition id="6">
				<sentence>1 ) 11 fl'O _I , MT consists of two components : u a datalmsc extractor and O a conversion program The database extractor selects the source entries and the corresponding target entries and stores them in database format .</sentence>
				<definiendum id="0">MT</definiendum>
				<definiens id="0">consists of two components : u a datalmsc extractor and O a conversion program The database extractor selects the source entries and the corresponding target entries and stores them in database format</definiens>
			</definition>
			<definition id="7">
				<sentence>The lexicon compiler is the component of the I , M'I '' system which converts the EI , F into tire internal I , MT format s .</sentence>
				<definiendum id="0">lexicon compiler</definiendum>
				<definiens id="0">the component of the I , M'I '' system which converts the EI</definiens>
			</definition>
			<definition id="8">
				<sentence>The conversion ennlponent converts the restructured dictionary entry to database format .</sentence>
				<definiendum id="0">conversion ennlponent</definiendum>
				<definiens id="0">converts the restructured dictionary entry to database format</definiens>
			</definition>
			<definition id="9">
				<sentence>( 1990 ) : `` Slot Grammar : A System for Simpler Construction of Practical Natural l~tHguage Grammars '' , In R. Studer ( ed . )</sentence>
				<definiendum id="0">Slot Grammar</definiendum>
				<definiens id="0">A System for Simpler Construction of Practical Natural l~tHguage Grammars ''</definiens>
			</definition>
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>The NARRATIVE-CMOP is a simplified version of Rumelhart 's \ [ 13\ ] story grammar and specifies how to build C-MOPs directly from MOPs in episodic memory .</sentence>
				<definiendum id="0">NARRATIVE-CMOP</definiendum>
				<definiens id="0">a simplified version of Rumelhart 's \ [ 13\ ] story grammar and specifies how to build C-MOPs directly from MOPs in episodic memory</definiens>
			</definition>
			<definition id="1">
				<sentence>OF COLING-92 , NAr , 'rES , AUG. 23-28 , 1992 To a large extent , the narrative-CMOP is a great deal like the episode in memory which it will relate .</sentence>
				<definiendum id="0">narrative-CMOP</definiendum>
				<definiens id="0">a great deal like the episode in memory which it will relate</definiens>
			</definition>
			<definition id="2">
				<sentence>Most importantly , JUDIS is an interface to an advisory system .</sentence>
				<definiendum id="0">JUDIS</definiendum>
			</definition>
</paper>

		<paper id="2100">
			<definition id="0">
				<sentence>n Rules~ -~ Revision Process Controller \ [ i. '' I~ /iDa a Co s ste Ic~l i I R° '' d'°r I-~ '~'Ir- ' -- # t~anager 1 The Morphological Analyzer divides the sentence string into word sequences .</sentence>
				<definiendum id="0">Morphological Analyzer</definiendum>
				<definiens id="0">divides the sentence string into word sequences</definiens>
			</definition>
			<definition id="1">
				<sentence>Next , the Syntactic Analyzer finds all possible binary relations between modifier Bunsetsu and modified Bunsetsu .</sentence>
				<definiendum id="0">Syntactic Analyzer</definiendum>
				<definiens id="0">finds all possible binary relations between modifier Bunsetsu and modified Bunsetsu</definiens>
			</definition>
			<definition id="2">
				<sentence>The Diagnoser , which utilizes the detection counterpart of the revision rule , finds all possible badly-styled expressions .</sentence>
				<definiendum id="0">Diagnoser</definiendum>
				<definiens id="0">utilizes the detection counterpart of the revision rule , finds all possible badly-styled expressions</definiens>
			</definition>
			<definition id="3">
				<sentence>The Revision Process Controller sequences the successive execution of partial rewriting operations , and the Data Consistency Manager maintains consistency between the current sentence string and the internal data during the dynamic rewriting process .</sentence>
				<definiendum id="0">Revision Process Controller</definiendum>
				<definiens id="0">sequences the successive execution of partial rewriting operations , and the Data Consistency Manager maintains consistency between the current sentence string and the internal data during the dynamic rewriting process</definiens>
			</definition>
			<definition id="4">
				<sentence>Detection of Badl_v-stvled Ext~ressions The Diagnoser detects badly-styled expressions liom the Kakari-Uke network which contains all detectable syntactic structures .</sentence>
				<definiendum id="0">Diagnoser detects badly-styled expressions</definiendum>
				<definiens id="0">the Kakari-Uke network which contains all detectable syntactic structures</definiens>
			</definition>
			<definition id="5">
				<sentence>The algorithm employs the following heuristics based on the preferences in word ordering\ [ 10J : ( 1 ) Constituents which include the thematic marker ( post position 'ha ' ) are put at the head of the sentence , and punctuation marks are put after them .</sentence>
				<definiendum id="0">thematic marker</definiendum>
				<definiens id="0">the following heuristics based on the preferences in word ordering\ [ 10J : ( 1 ) Constituents which include the</definiens>
				<definiens id="1">the head of the sentence , and punctuation marks are put after them</definiens>
			</definition>
			<definition id="6">
				<sentence>In this example , B2 is the Bunsetsu that contains the thematic marker and B5 indicates the top-level clause boundary .</sentence>
				<definiendum id="0">B2</definiendum>
				<definiens id="0">contains the thematic marker and B5 indicates the top-level clause boundary</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>uk ABSTRACT Attention on constraint-based grammar formalisms such as Head-driven Phrase Structure Grammar ( HPS6 ) has focussed on syntax and semantics to the exclusion of phonology .</sentence>
				<definiendum id="0">uk ABSTRACT Attention</definiendum>
				<definiens id="0">Head-driven Phrase Structure Grammar ( HPS6</definiens>
			</definition>
			<definition id="1">
				<sentence>The second principle is Partee 's ( 1979 , 276 ) WELL-FORMEDNESS CONSTRAINT , which states that well-formed expressions are buih up out of well-formed parts .</sentence>
				<definiendum id="0">WELL-FORMEDNESS CONSTRAINT</definiendum>
				<definiens id="0">states that well-formed expressions are buih up out of well-formed parts</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , in a given language , \ [ +voice\ ] denotes a class of speech sounds , \ [ +nasal\ ] denotes another class , and \ [ +voice , +nasal\ ] denotes the intersection of these two 2Historical note : There is a close parallel between this theoretical position and that adopted by Th~o Venneman , Joan Hooper and Grover Hudson in the 70 's in the theoretical framework known as Natural Generative Phonology ( Hooper , 1976 ) .</sentence>
				<definiendum id="0">Generative Phonology</definiendum>
				<definiens id="0">in a given language , \ [ +voice\ ] denotes a class of speech sounds , \ [ +nasal\ ] denotes another class , and \ [ +voice , +nasal\ ] denotes the intersection of these two 2Historical note : There is a close parallel between this theoretical position and that adopted by Th~o Venneman , Joan Hooper and Grover Hudson in the 70 's in the theoretical framework known as Natural</definiens>
			</definition>
			<definition id="3">
				<sentence>A more recent example of the interaction between prosody and morphology is the field of prosodic morphology ( McCarthy &amp; Prince , 1990 ) .</sentence>
				<definiendum id="0">morphology</definiendum>
				<definiens id="0">the field of prosodic morphology</definiens>
			</definition>
			<definition id="4">
				<sentence>\ [ I } II ( ) N ( NfI ( ) ) * ( ( NNO ) N ' ) \ ] , \ [ IIEAI ) IMAJ NIIM\ ] SYN'I : K : \ [ SUBCAT 0 J s I. ; M IC ( } NTI INI ) I VA~ : I PI¢I~ , / .</sentence>
				<definiendum id="0">M IC</definiendum>
				<definiens id="0">NfI ( ) ) * ( ( NNO ) N ' ) \ ] , \ [ IIEAI ) IMAJ NIIM\ ] SYN'I : K : \ [ SUBCAT 0 J s I.</definiens>
			</definition>
			<definition id="5">
				<sentence>Suppose that an intonational phrase consists of a sequence of stress feet 9 , feet consist of syllables and syllables consist of segments .</sentence>
				<definiendum id="0">intonational phrase</definiendum>
				<definiens id="0">consists of a sequence of stress feet 9 , feet consist of syllables and syllables consist of segments</definiens>
			</definition>
			<definition id="6">
				<sentence>The frmnework starts where HPSG left off , in the sense that \ ] IPSG 's phonology attribute-a list -- can be viewed as a subclass of automata .</sentence>
				<definiendum id="0">frmnework</definiendum>
				<definiens id="0">starts where HPSG left off , in the sense that \ ] IPSG 's phonology attribute-a list</definiens>
			</definition>
			<definition id="7">
				<sentence>PC-KIMMO : A Two-Level Processor forMorphologicalAnalysis .</sentence>
				<definiendum id="0">PC-KIMMO</definiendum>
			</definition>
			<definition id="8">
				<sentence>Two-Level Morphology : A General Computational Model for Word-Form Recognition and Production .</sentence>
				<definiendum id="0">Two-Level Morphology</definiendum>
			</definition>
</paper>

		<paper id="4212">
			<definition id="0">
				<sentence>Corpus-based prefereltce : Corpus analysis ( WSJ , 80-million words ) provides wordassociation preference \ [ Beckwith el at. , 1991\ ] collocation total vb-nn aj-nn preferred stock 2314 100 O expressed concern 318 1 99 The construct expressed concern , which appears 318 times in the corpus , is 99 % a verbnoun construct ; on tile other hand , preferred stock , which appears in the corpus 2314 times , is 99 % an adjective-norm construct .</sentence>
				<definiendum id="0">Corpus-based prefereltce</definiendum>
				<definiendum id="1">Corpus analysis</definiendum>
				<definiens id="0">appears 318 times in the corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>Next Observation : Co-occurrence Entails Thematic Relations While column 1 ( preferred ) yields good syntactic associations , column 2 ( ezpressed ) and column 3 ( closed ) yield different conclusions .</sentence>
				<definiendum id="0">Next Observation</definiendum>
				<definiens id="0">Co-occurrence Entails Thematic Relations While column 1 ( preferred ) yields good syntactic associations , column 2 ( ezpressed ) and column 3 ( closed ) yield different conclusions</definiens>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>Proof-Nets ( Roorda 1990 ) are a good device for processing with eategorial grammars , mainly because they avoid spurious ambiguities .</sentence>
				<definiendum id="0">Proof-Nets</definiendum>
				<definiens id="0">a good device for processing with eategorial grammars , mainly because they avoid spurious ambiguities</definiens>
			</definition>
			<definition id="1">
				<sentence>A proof-net is a device which contains all the equivalent proofs of the same result .</sentence>
				<definiendum id="0">proof-net</definiendum>
				<definiens id="0">a device which contains all the equivalent proofs of the same result</definiens>
			</definition>
			<definition id="2">
				<sentence>As for the proof-nets , we want to demonstrate theorems that have a sequent form like : F -- -~ X , where F is a non empty sequence of categories and X is a category .</sentence>
				<definiendum id="0">F</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a non empty sequence of categories</definiens>
			</definition>
			<definition id="3">
				<sentence>Definition : A type-2 edge in a connection tree is : an odd level edge in a left-tree , or an even level edge in a right-tree A type-1 edge in a connection tree is : an even level edge in a left-tree , or an odd level edge in a right-tree A type-i ( i =1.2 ) node is the target of a type-i edge .</sentence>
				<definiendum id="0">Definition</definiendum>
			</definition>
			<definition id="4">
				<sentence>Theorem : ( Conservativity of Connection Operations ) The Non-Overlap condition is a conservative extension of the conditions on connection ( restriction C rec ) stipulated in ~4 .</sentence>
				<definiendum id="0">Theorem</definiendum>
				<definiens id="0">a conservative extension of the conditions on connection ( restriction C rec ) stipulated in ~4</definiens>
			</definition>
			<definition id="5">
				<sentence>A 3 ) Let F -- ~ A be a sequent expressed in the Product-Free Lambek Calculus , where F is a non empty sequence of categories and A is a category , let L be the sequence of left connection trees associated to the elements of F and R be the right tree associated to A , the sequent is a theorem if and only if L and R yield a correct neL In other terms : the Connection Net System is sound and complete w.r.t , the Product-Free Lambek Calculus , Examples : figure ( 6 ) shows that : s/ ( s/np ) s/ ( ( s/np ) ks ) I-s is a theorem of the Lambek Calculus .</sentence>
				<definiendum id="0">F</definiendum>
				<definiendum id="1">I-s</definiendum>
				<definiens id="0">a non empty sequence of categories</definiens>
				<definiens id="1">the sequence of left connection trees associated to the elements of F and R be the right tree associated to A , the sequent is a theorem if and only if L and R yield a correct neL In other terms : the Connection Net System is sound and complete w.r.t</definiens>
			</definition>
			<definition id="6">
				<sentence>Lecomte , A. : 1990 , 'Connection Grammars : a GraphOriented lntertn'etation of Categorial Grammars ' in Lecomte , A. ( ed ) , 1992 .</sentence>
				<definiendum id="0">'Connection Grammars</definiendum>
				<definiens id="0">a GraphOriented lntertn'etation of Categorial Grammars ' in Lecomte</definiens>
			</definition>
</paper>

		<paper id="3165">
			<definition id="0">
				<sentence>( 1 ) S -- &gt; NP VP ( 2 ) S -- &gt; S PP ( 3 ) NP -- &gt; n ( 4 ) NP -- &gt; det n ( 5 ) NP -- &gt; NP PP ( 6 ) PP -- &gt; prep NP ( 7 ) VP -- &gt; v NP Figure 1 : Example CFG Rules tences The original GLR parsing method was not designed to handle ungrammatical sentences .</sentence>
				<definiendum id="0">prep NP</definiendum>
				<definiens id="0">2 ) S -- &gt; S PP ( 3 ) NP -- &gt; n ( 4 ) NP -- &gt; det n ( 5 ) NP -- &gt; NP PP ( 6 ) PP -- &gt;</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>In addition , all lexical types are of the form Ox , i.e. have a single o as their outermost operator , which allows them to appear embedded within modal domains ( c.f. the D !</sentence>
				<definiendum id="0">outermost operator</definiendum>
				<definiens id="0">allows them to appear embedded within modal domains</definiens>
			</definition>
</paper>

		<paper id="1047">
</paper>

		<paper id="2098">
			<definition id="0">
				<sentence>This kind of rule may be exemplified by the following one , which may be used for finding habituality markers like indefinite frequency adverbials , adverbials expressing durativity or the verb brukade ( 'used to ' ) in the Swedish input : uniqueness ind ( past , sem_rep ( Slist ) , not_uni ) : in list ( Functor ( Repr , Feature ) ) , Slist ) , uniqueness_relevant ( Fu nctor ) , not unifl'ense , Functor , Feature ) .</sentence>
				<definiendum id="0">Functor</definiendum>
			</definition>
			<definition id="1">
				<sentence>Grammatika russkogo jazyka ( Soviet Academic Grammar ) , vol .</sentence>
				<definiendum id="0">Grammatika russkogo jazyka</definiendum>
				<definiens id="0">Soviet Academic Grammar ) , vol</definiens>
			</definition>
</paper>

		<paper id="3164">
			<definition id="0">
				<sentence>A relationship consists of a relationship name and its case roles .</sentence>
				<definiendum id="0">relationship</definiendum>
			</definition>
			<definition id="1">
				<sentence>A generation rule is defined as a phrase definition \ [ Kikui-92\ ] .</sentence>
				<definiendum id="0">generation rule</definiendum>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>o~s = { , ( g , VH me , ,a~r ( H , El , mine ( H , job , t ) A Igl : 0 } \ ] This says that there is a singleton set A of `` eating events '' ; that for each member of A the agent is E ( remember , A is a singleton set so there is in fact only one such member ) ; that there is a singleton set B of peaches which is the object of every member of A ; that every member of A is extended and took place at some past instant ; and that the existence of a unique singleton set E of items called John is presupposed .</sentence>
				<definiendum id="0">Igl</definiendum>
				<definiens id="0">the object of every member of A ; that every member of A is extended and took place at some past instant ; and that the existence of a unique singleton set E of items called John is presupposed</definiens>
			</definition>
			<definition id="1">
				<sentence>The matrix of the NP consists of a generalised 2 quantifier derived from the matrix of the NN , and the presuppositions of the NN are inherited by the NP .</sentence>
				<definiendum id="0">matrix of the NP</definiendum>
				<definiens id="0">consists of a generalised 2 quantifier derived from the matrix of the NN</definiens>
			</definition>
			<definition id="2">
				<sentence>Deciding , for instance , that using A ( A , -~B VC ' member ( C ' , B ) -4 D.C A IBI = 1 A A.A ( E , E.B ) ) for the matrix of a led to more satisfactory analyses of indefinite NPs than X ( A , A.X ( E , 3B VC memher ( C , B ) -~ D.C A IBI= dious than it was if I had not had a system which would show me the consequences of the two choices in n variety of settings .</sentence>
				<definiendum id="0">A.A</definiendum>
				<definiens id="0">a system which would show me the consequences of the two choices in n variety of settings</definiens>
			</definition>
</paper>

		<paper id="4197">
			<definition id="0">
				<sentence>BIAS is a multi-level analyzer which is developed not only to extract the syntactic and semantic structure of sentences but also to provide a unifying method for knowledge reasoning .</sentence>
				<definiendum id="0">BIAS</definiendum>
			</definition>
			<definition id="1">
				<sentence>Indonesia , Malaysia and Thailand ) , we developed BIAS : an analysis prograni for Indonesian language which output an interlingual representation .</sentence>
				<definiendum id="0">BIAS</definiendum>
				<definiens id="0">an analysis prograni for Indonesian language which output an interlingual representation</definiens>
			</definition>
			<definition id="2">
				<sentence>In particular BIAS is a program that takes natural language text ms input and produces its tmderlying interlingual representation at a certain level of details that serve as a language-independent representation for the machine translation environment .</sentence>
				<definiendum id="0">BIAS</definiendum>
				<definiens id="0">a program that takes natural language text ms input and produces its tmderlying interlingual representation at a certain level of details that serve as a language-independent representation for the machine translation environment</definiens>
			</definition>
			<definition id="3">
				<sentence>BIAS is a multi-level analyzer , similar to the first type describe above , with the ability to perfom~ reasoning in each level of analysis .</sentence>
				<definiendum id="0">BIAS</definiendum>
				<definiens id="0">a multi-level analyzer , similar to the first type describe above</definiens>
			</definition>
			<definition id="4">
				<sentence>'Ihis phasewill consistof themapping of the structural ( syntactic ) description of the sentence into an interlingual representation language .</sentence>
				<definiendum id="0">'Ihis phasewill</definiendum>
				<definiens id="0">consistof themapping of the structural ( syntactic ) description of the sentence into an interlingual representation language</definiens>
			</definition>
			<definition id="5">
				<sentence>Hence , the Case grlu'nmar is tile appropriate selectioa tor the semantic analysis part .</sentence>
				<definiendum id="0">Case grlu'nmar</definiendum>
				<definiens id="0">tile appropriate selectioa tor the semantic analysis part</definiens>
			</definition>
</paper>

		<paper id="2096">
			<definition id="0">
				<sentence>Schemas ( McKeown , 1985 ) encode conventional patterns of text StlUCture .</sentence>
				<definiendum id="0">Schemas</definiendum>
				<definiens id="0">encode conventional patterns of text StlUCture</definiens>
			</definition>
			<definition id="1">
				<sentence>While schemas label each proposition as the instantiation of a predicate , RST attempts to label the relation between propositions .</sentence>
				<definiendum id="0">RST</definiendum>
				<definiens id="0">attempts to label the relation between propositions</definiens>
			</definition>
			<definition id="2">
				<sentence>RST ( Mann &amp; Thompson , 1987 ) was t-u'st introduced as a descriptive theory aiming at enmnerating possible rhetodcal relations between discourse segments .</sentence>
				<definiendum id="0">RST</definiendum>
			</definition>
			<definition id="3">
				<sentence>This input represents the proposition that AI covers ( among others ) a set of topics in the area of theory ( namely , logic ) , and the AO feature indicates that this proposition is used as an argument for the conclusion that AI is a difficult course , by virtue of the topos theoretical + / difficult + ( the conclusinn AcrEs DE COLING-92 , NANn .</sentence>
				<definiendum id="0">AI</definiendum>
				<definiens id="0">the conclusinn AcrEs DE COLING-92 , NANn</definiens>
			</definition>
			<definition id="4">
				<sentence>Because of this AO specification , the grammar will choose appropriately realization ( 5 ) instead of ( 6 ) : ( 5 ) AI requires a lot of programming ( 6 ) AI involves some programming .</sentence>
				<definiendum id="0">AI</definiendum>
				<definiens id="0">involves some programming</definiens>
			</definition>
			<definition id="5">
				<sentence>In this Figme , the notation alt indicates a disjunction between alternatives ; ralt indicates a random alternation , and is used to indicate that the grammar does not account for the difference between the alternatives ; the curly braces notation in pairs of the form ( ( go ) value ) indicates that the go feature is not embedded in the lexical verb constituent unified with the grammar but rather is a top level feature within the clause .</sentence>
				<definiendum id="0">ralt</definiendum>
				<definiens id="0">a top level feature within the clause</definiens>
			</definition>
			<definition id="6">
				<sentence>A Contrastive Evaluation of Functional Unification Glammar for Surface Language Generators : A Case Study in Choice of Connectives .</sentence>
				<definiendum id="0">Contrastive Evaluation</definiendum>
				<definiens id="0">A Case Study in Choice of Connectives</definiens>
			</definition>
</paper>

		<paper id="2083">
			<definition id="0">
				<sentence>complex stnzctural iuformation , information which can not be expressed adequately in string patterns , The following addition makes the pattern for extracting the PURPOSE relation , paratthrased in the previous section , more complete : if tile PP with `` for '' is not a post-modifier of a verb `` used '' , then a PURPOSE relation between the definiendum and the head ( s ) of the PP c , 'm be hypothesized if the nearest noun that the PP post-modifies is the genus term.4 Consider the syntactic analysis of the relevant portion of text in the definition of `` laboratory '' ( W7 n , l ) shown below in Figure 5 .</sentence>
				<definiendum id="0">post-modifies</definiendum>
				<definiens id="0">a PURPOSE relation between the definiendum and the head ( s ) of the PP c , 'm be hypothesized if the nearest noun that the PP</definiens>
			</definition>
			<definition id="1">
				<sentence>In this case , knowledge ol dicti ( nlary peculiarities resolves the initial partial parse and converts it into a complete and succc .</sentence>
				<definiendum id="0">knowledge ol dicti</definiendum>
				<definiens id="0">the initial partial parse and converts it into a complete and succc</definiens>
			</definition>
</paper>

		<paper id="2113">
</paper>

		<paper id="2066">
			<definition id="0">
				<sentence>LTAG is a tree-rewriting system that combines trees of large domain with adjoining and substitution .</sentence>
				<definiendum id="0">LTAG</definiendum>
				<definiens id="0">a tree-rewriting system that combines trees of large domain with adjoining and substitution</definiens>
			</definition>
			<definition id="1">
				<sentence>A stochastic linear indexed grammar , G , is denoted by ( VN , VT , VI , S , Prod ) , where VN is a finite set of nonterminal symbols ; VT is a finite set of terminal symbols ; VI is a finite set of stack symbols ; S E VN is the start symbol ; Prod is a finite set of productions of the form : Xo\ [ $ po\ ] -- * a Xo\ [ .</sentence>
				<definiendum id="0">VN</definiendum>
				<definiendum id="1">VT</definiendum>
				<definiendum id="2">; VI</definiendum>
				<definiendum id="3">S E VN</definiendum>
				<definiendum id="4">Prod</definiendum>
				<definiens id="0">a finite set of nonterminal symbols</definiens>
				<definiens id="1">a finite set of terminal symbols</definiens>
				<definiens id="2">a finite set of stack symbols</definiens>
				<definiens id="3">the start symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>The probability of a derivation is defined as the product of tile probabilities of all individual rules involved ( counting repetition ) in the derivation , the derivation being validated by a correct configuration of the stack at each level .</sentence>
				<definiendum id="0">probability of a derivation</definiendum>
				<definiens id="0">the product of tile probabilities of all individual rules involved ( counting repetition ) in the derivation , the derivation being validated by a correct configuration of the stack at each level</definiens>
			</definition>
			<definition id="3">
				<sentence>P is the probability that a derivation starts from the initial tree associated with a lexical item and rooted by % .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the probability that a derivation starts from the initial tree associated with a lexical item</definiens>
			</definition>
			<definition id="4">
				<sentence>• Similarly , if thT/~ are the 2 children of a node r/such that r h is on the spine ( i.e. subsumes the foot node ) , include : b\ [ .</sentence>
				<definiendum id="0">i.e.</definiendum>
				<definiens id="0">subsumes the foot node</definiens>
			</definition>
			<definition id="5">
				<sentence>rt\ ] P=-*~ t\ [ `` rl~\ ] t\ [ $ ~\ ] ( 3 ) Since ( 3 ) encodes a~t immediate domination link defined by the tree-adjoining grammar , its associated probability is one .</sentence>
				<definiendum id="0">rt\ ] P=-*~ t\</definiendum>
			</definition>
			<definition id="6">
				<sentence>• Finally , if r h is the root node of an initial tree that can be substituted on a node marked for substitution r ) , include : t\ [ $ ~\ ] L t\ [ S~t\ ] ( g ) Here , p is the probability that the initial tree rooted by ~/~ is substituted at node q. It corresponds to the probability of substituting the lexicalized initial tree whose root node is 71 , say 6 , at the node q of a lexicalized elementary tree , say a. 5 The SLIG constructed as above is well defined if the following equalities hold for all nodes ~l : P ( t\ [ .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">the root node of an initial tree that can be substituted on a node marked for substitution r</definiens>
				<definiens id="1">the probability that the initial tree rooted by ~/~ is substituted at node q. It corresponds to the probability of substituting the lexicalized initial tree whose root node is 71 , say 6 , at the node q of a lexicalized elementary tree</definiens>
			</definition>
			<definition id="7">
				<sentence>At the first iteration , a SLTAG which generates all possible structures over a given set of nodes and terminal symbols is used .</sentence>
				<definiendum id="0">SLTAG</definiendum>
				<definiens id="0">generates all possible structures over a given set of nodes and terminal symbols is used</definiens>
			</definition>
			<definition id="8">
				<sentence>The initial grammar for the reestimation algoritiim consists of all SLIG rules for the tress ill Lexicalized Normal I~brm ( ill short LNF ) over a given set = { aill .</sentence>
				<definiendum id="0">initial grammar for the reestimation algoritiim</definiendum>
				<definiens id="0">consists of all SLIG rules for the tress ill Lexicalized Normal I~brm ( ill short LNF ) over a given set = { aill</definiens>
			</definition>
			<definition id="9">
				<sentence>The initial grammar consists of the trees ~ ' , fl~ , c~ a and ab with random probability of adjoining and null adjoining .</sentence>
				<definiendum id="0">initial grammar</definiendum>
				<definiens id="0">consists of the trees ~ ' , fl~ , c~ a and ab with random probability of adjoining and null adjoining</definiens>
			</definition>
			<definition id="10">
				<sentence>The initial grammar consists of all trees ( 96 ) of the form fl~ , a ~ for all 48 terminal symbols for part-ofspeech .</sentence>
				<definiendum id="0">initial grammar</definiendum>
				<definiens id="0">consists of all trees ( 96 ) of the form fl~ , a ~ for all 48 terminal symbols for part-ofspeech</definiens>
			</definition>
			<definition id="11">
				<sentence>Preliminary experiments with a context-free subset of SLTAG confirms that SLTAG enables faster convergence than stochastic context-free grammars ( SCFG ) .</sentence>
				<definiendum id="0">SCFG</definiendum>
				<definiens id="0">a context-free subset of SLTAG confirms that SLTAG enables faster convergence than stochastic context-free grammars</definiens>
			</definition>
			<definition id="12">
				<sentence>And for all ( i , j ) ~ ( 0 , N ) , O ( t , ~ , i , - , - , j ) = o ( t , ,10 , i , - , - , j ) × P ( @ % \ ] ~ @ ) ~\ ] ) of the elementary tree it belongs to , O ( t , ~ , i , j , k , l ) = ~ O ( b , % , i , j , k , q ) ) × l ( t , 7~ , 1 , - , - , q ) q=t+ , × P ( b\ [ `` 70\ ] ~ t\ [ `` Tlt\ [ $ 7~\ ] ) i-1 O ( b , qo , p , j , k , l ) ) +Z × l ( t'71'P'-'-'i ) ~=0 x P ( b\ [ .</sentence>
				<definiendum id="0">] ) i-1 O</definiendum>
				<definiens id="0">( b , qo , p , j , k , l ) ) +Z × l ( t'71'P'-'-'i ) ~=0 x P ( b\ [</definiens>
			</definition>
</paper>

		<paper id="4198">
			<definition id="0">
				<sentence>Pour r6soudre une ambigu~t6 de g6om6trie en respectant le second et le troisi~me principe , nous distinguons hi6rarchiquement les probl~mes suivants : I. coordination verbale : probl~me de coordination pour lequel un m~me groupe peut ~tre une PHrase VerBale coordonn~e ou non .</sentence>
				<definiendum id="0">coordination verbale</definiendum>
				<definiens id="0">probl~me de coordination pour lequel un m~me groupe peut ~tre une PHrase VerBale coordonn~e ou non</definiens>
			</definition>
			<definition id="1">
				<sentence>SIS : An Experimental Sentence Translation System .</sentence>
				<definiendum id="0">SIS</definiendum>
				<definiens id="0">An Experimental Sentence Translation System</definiens>
			</definition>
</paper>

		<paper id="4211">
			<definition id="0">
				<sentence>governor list ( govlLst ) : It indicates which word can be the parent node of thc given word , and what is the dependency relation between the word and its parent node .</sentence>
				<definiendum id="0">governor list ( govlLst ) : It</definiendum>
				<definiens id="0">indicates which word can be the parent node of thc given word , and what is the dependency relation between the word and its parent node</definiens>
			</definition>
			<definition id="1">
				<sentence>What Ls the position ( to the right or left of the word ) of the children node in a sentence in our corpus ?</sentence>
				<definiendum id="0">position</definiendum>
			</definition>
			<definition id="2">
				<sentence>Sccond , remove impossible links in the SMS , and form a network .</sentence>
				<definiendum id="0">Sccond</definiendum>
				<definiens id="0">remove impossible links in the SMS , and form a network</definiens>
			</definition>
</paper>

		<paper id="2082">
			<definition id="0">
				<sentence>2 WordNet ( Miller et al. 1990 ) is a hand-built online thesaurus whose organization is modeled after the results of psycbolinguistic research .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="1">
				<sentence>To use tile authors ' words , Wordnet `` ... is an attempt to organize lexical information in terms of word meanings , rather than word forms .</sentence>
				<definiendum id="0">Wordnet `` ...</definiendum>
				<definiens id="0">an attempt to organize lexical information in terms of word meanings</definiens>
			</definition>
</paper>

		<paper id="2075">
			<definition id="0">
				<sentence>The architecture with sequential levels , in which each module corresponds to a linguistic level ( pretreatments , morphology , syntax , semantics ) has shown its limits .</sentence>
				<definiendum id="0">sequential levels</definiendum>
				<definiens id="0">pretreatments , morphology , syntax , semantics</definiens>
			</definition>
			<definition id="1">
				<sentence>The Talisman system is an environment which iutegrates linguistic tools where different , agents c , 'm bring into use diflerent methods such as symbolic and/or statistic ones .</sentence>
				<definiendum id="0">Talisman system</definiendum>
				<definiens id="0">an environment which iutegrates linguistic tools where different , agents c , 'm bring into use diflerent methods such as symbolic and/or statistic ones</definiens>
			</definition>
			<definition id="2">
				<sentence>Talisman is a linguistic environment based on the most recent techniques used in software engineering environments .</sentence>
				<definiendum id="0">Talisman</definiendum>
				<definiens id="0">a linguistic environment based on the most recent techniques used in software engineering environments</definiens>
			</definition>
			<definition id="3">
				<sentence>Dints Talisman , la soci6t6 comprend actuellement huit agents lingaistiques experts en : pr6traitements ( PRET ) : d61ivre des informations morphographiques et morpho-s3mVaxiques utiles aux autres agents , morphologie ( MORPH ) : met en ~euvre une expertise pour lever les ambiguit~s contextuelles , san but 6umt de donner la bonne cat6gorie h chaque mot , segmentation ( SEGM ) : renseigne sur le hombre de verhes attendus &amp; ms la phrase et d6compese celleci en propositions relatives , compl6tives , etc. syntaxe ( SYNT ) : comprend des gr~unmaires locales en interrogatives , infinitives , par exemple un agent local nonun6 NR tralUmt les restrictions n6gatives etc ; son but 6tant de donner la meilleure repr6sentation syntaxique de la phr &amp; se , n6gation ( NEGA ) : repute les nnit6s dans lesquelles s'effectue la n6gation , ellipses ( ELLIP ) : analyse les propositions elliptiques et les complete , coordination ( COORD ) : traite les c , '~s simples de coordination , statistique ( STAT ) : utilis6 sur des s6quences qu'il est impossible de d6sambigui'ser A l'aide d'heuristiques linguistiques .</sentence>
				<definiendum id="0">PRET )</definiendum>
				<definiendum id="1">segmentation</definiendum>
			</definition>
</paper>

		<paper id="2080">
			<definition id="0">
				<sentence>A.Elithorn and R.Banerji , North-Holland , 1984 \ [ Nomiyama 91\ ] H.Nomiyama `` Lexieal Selection Mechanism Using Target Language Knowledge and Its Learning Ability '' , IPSJ-WG , NL86 8 , 1991 ( in Japanese ) \ [ Okumura 91\ ] A.Okumura , K.Muraki and S.Akamine `` Multi-lingual Sentence Generation from the PIVOT interlingua , '' Prec .</sentence>
				<definiendum id="0">A.Elithorn</definiendum>
				<definiens id="0">H.Nomiyama `` Lexieal Selection Mechanism Using Target Language Knowledge and Its Learning Ability ''</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>A bounded push-down automaton M = ( Q , Y~ , I',6 , qs , Zo , F ) is a PDA that has an upper limit k E ~ on the number of items on its stack , i.e. H ~ &lt; k for every instantaneous description ( ID ) ( q , w , a ) of M. The set of stack states of this PDA is delined to be : QST = { a I ( qo , w , Zs ) P*~t ( q , e , ¢~ ) } .</sentence>
				<definiendum id="0">ID )</definiendum>
				<definiens id="0">a PDA that has an upper limit k E ~ on the number of items on its stack</definiens>
			</definition>
			<definition id="1">
				<sentence>An NDA state is rap resented by all activity patterns that represent a pair containing that state , and input patterns are represented by a component-wise OR over all activity patterns containing that input symbol .</sentence>
				<definiendum id="0">NDA state</definiendum>
				<definiens id="0">all activity patterns that represent a pair containing that state , and input patterns are represented by a component-wise OR over all activity patterns containing that input symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>The representations of the states of this deterministic finite-state automaton ( FSA ) are dynamically constructed along the way ; they are mixed temporal images .</sentence>
				<definiendum id="0">FSA</definiendum>
				<definiens id="0">dynamically constructed along the way ; they are mixed temporal images</definiens>
			</definition>
			<definition id="3">
				<sentence>'i ( t+l ) : 0 if hi ( t ) &lt; U , h~ ( t ) : h~ ( t ) +hl ( t ) , N j=l the temporal transition term , t P u all a ) N jz~=l '' '' a ) ( ~¢~ ' a ) , with J. = 0 , ~ ( t ) ~ r O0 L ~ Sj ( tt ' ) w ( t ' ) dt ' , where { 7~ifO &lt; t &lt; r w ( t ) = 0 otherwise , h~ ( t ) = x~ ( s , ' ( t ) a ) , the external input term , The Si are neuronal variables ( 5 '' , '. is a neuron in another network ) , hi is the total input on Si , U is a threshold value which is equal for all Si , Jij is the synaptic efficacy of the synapse connecting S i to Si , and A is the relative magnitude of the synapses. The average at time ~ is expressed by ~/ ( ~ ) , where r = ( 2t. + ta + 3tr ) /N and ~ -~ ( ta + t~ ) /N. The function w ( t ) determines over which period activity is averaged. The input synapses are nonzero only in case i = j. These synapses carry a negative ground signal -A'a , which is equivalent to an extra threshold generated by the input synapses. The activity patterns { ~ ' } ( { ~ '' } ~ ( ~\ ] , ~ ... .. ~N ) ) are statist , tally independent , and satisfy the same probability distribution as the patterns in the model of Buhmann et al. \ [ 2\ ] : ... ... ( 1a ) b ( ~ ) , where l if x : ~ 0 ~ ( x ) ~ 0 otherwise. If a ¢ ½ the pattern is biased. For N -~ co , I/N ~N=I ~ ' -- * a. The updating p ... . is a Monte Carlo random walk. D II~ I Figure 1 : 7'he model for N = 3. Usually HopJield models consist of very many neurons. The arced arrows denote temporal synapses. The straiyht alT'ows denote input synapses. A number of system parameters need to be related in order to make the model work correctly. Timing ; is fairly important ill this network. Tile time the network is active ( to ) should not exceed tile delay time t~. If it does then ta+lr &gt; ta+tr , and since no average is computed over a period ta + tr back in time , not the fldl time average of the previous activity need to be computed , consequently we choose ta &lt; ta .</sentence>
				<definiendum id="0">'i ( t+l )</definiendum>
				<definiendum id="1">N j=l</definiendum>
				<definiendum id="2">The Si</definiendum>
				<definiendum id="3">U</definiendum>
				<definiendum id="4">Jij</definiendum>
				<definiens id="0">0 if hi ( t ) &lt; U , h~ ( t ) : h~ ( t ) +hl ( t ) ,</definiens>
				<definiens id="1">the temporal transition term , t P u all a ) N jz~=l '' '' a ) ( ~¢~ ' a ) , with J. = 0 , ~ ( t ) ~ r O0 L ~ Sj ( tt ' ) w ( t ' ) dt ' , where { 7~ifO &lt; t &lt; r w ( t ) = 0 otherwise , h~ ( t ) = x~ ( s , ' ( t ) a ) , the external input term</definiens>
				<definiens id="2">neuronal variables ( 5 '' , '. is a neuron in another network ) , hi is the total input on Si</definiens>
				<definiens id="3">a threshold value which is equal for all Si ,</definiens>
			</definition>
			<definition id="4">
				<sentence>for an NDA M is a list of the form ( a , A ~ , A t , N , p , prnaz , ta , td , tr , U ) , where : I { Y 6 ~ , 16 ( q ' , y ) # 0 } I ; Ac-t~ DE COLING-92 , NANTES , 23-28 AO6 '' r 1992 1 1 6 PROC .</sentence>
				<definiendum id="0">NDA M</definiendum>
				<definiens id="0">a list of the form ( a , A ~ , A t , N , p , prnaz , ta , td , tr</definiens>
			</definition>
			<definition id="5">
				<sentence>Definition 5.2 Let M = ( Q , E , ti , Qo , F ) he an NDA , let t '' : ( a , A e , M , N , p , pma , ,ta , ta , tr , U ) be a parameter list defined according to definition 5.1 .</sentence>
				<definiendum id="0">NDA</definiendum>
				<definiens id="0">p , pma , ,ta , ta , tr</definiens>
			</definition>
			<definition id="6">
				<sentence>The coding \ [ unction c is given by : c : QxE~ { 0,1 } N , such that for q E Q , and x E : E : c ( q , x ) = { ~ } , where ~i is chosen at random from { 0 , 1 } with probability distribution r ' ( ~ ) = a6 ( ~ I ) + ( t a ) ~ ( ~ , ) , a.d The set of codes is then partitioned : into sets of activity patterns corresponding to NDA states , and into sets of patterns corresponding to input symbols .</sentence>
				<definiendum id="0">~i</definiendum>
				<definiens id="0">into sets of activity patterns corresponding to NDA states , and into sets of patterns corresponding to input symbols</definiens>
			</definition>
			<definition id="7">
				<sentence>Then a network transition is defined ms a matrix operator specified by the network 's storage prescription , and related to NDA transitions using the previously defined partition of the set of codes .</sentence>
				<definiendum id="0">network transition</definiendum>
				<definiens id="0">a matrix operator specified by the network 's storage prescription</definiens>
			</definition>
			<definition id="8">
				<sentence>A neural-network acceptor ( NNA ) defined for an NDA M that takes its parameters from P is a quadruple H = ( T , f , U , S ) , where : ( N ) ; 1 if E~J/j .</sentence>
				<definiendum id="0">neural-network acceptor</definiendum>
				<definiendum id="1">NNA</definiendum>
				<definiendum id="2">NDA M</definiendum>
				<definiens id="0">takes its parameters from P is a quadruple H = ( T , f</definiens>
			</definition>
			<definition id="9">
				<sentence>( K ' } , ( ) lt ( { ~q ... .. { ~ '' } ) ) ; ~ud a OR ( P~ ) = OR ( { ~ ~ } ... .. { C ' } ) if • e~ = { W } ... .. { ~ '' } } , At last a formal definition can be given of a temporal image a.s the set of all activity patterns for which there is a network input that makes such an activity pattern the network 's next qua.st-stable state .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">P~ ) = OR ( { ~ ~ } ... .. { C ' } ) if • e~ = { W } ... .. { ~ '' } } , At last a formal definition can be given of a temporal image a.s the set of all activity patterns for which there is a network input that makes such an activity pattern the network 's next qua.st-stable state</definiens>
			</definition>
			<definition id="10">
				<sentence>, , , t , , ta , it , U ) be a parameter list defined according to definition 5.1 , and let H = ( T , f , U , S ) be an NNA defined according to definition 5.5 that takes its parameters from P. A temporal image is a set : { c ( q , x ) \ [ input OIt ( P~ ) for H implies m ¢ ( q'~ ) = 1 -a } , a set Pq , is a temporal image of a quasi-stable stale { S } = c ( q , x ) of H if and only if J~q , ,q , , : ) is a transition of H. Now that we have a neural-uetwork acceptor , we triay also Wahl to use it to judge the legality of strings against a given grammar with it .</sentence>
				<definiendum id="0">OIt</definiendum>
				<definiens id="0">a temporal image of a quasi-stable stale { S } = c</definiens>
			</definition>
			<definition id="11">
				<sentence>Lemma 5.10 intrinsical correctness Let M = ( Q , ~2,8 , Qo , F ) be an NDA , let P = ( a , A ' , ~t , N , p , pm , : : , ta , ta , tr , U ) be a parameter list defined according to definitions 5.1 and 5.9 , and let H = ( T , f , U , S ) be an NNA defined according to definition 5.5 that takes its parameters from P , then H is such that : during network evolution ; and if l~ 7 £ `` , then P ( ~ , ~ = ~ = 1 ) ~ O , where i=l , ... , N. Then the correctness of an NNA follows .</sentence>
				<definiendum id="0">S )</definiendum>
				<definiens id="0">such that : during network evolution</definiens>
			</definition>
			<definition id="12">
				<sentence>1 1 ' ( , '¢ , = 0 ) , where the latter is : ( -l 2np ' 1.-2 , ,i ) ~ In this expressi ... . P -- - ( ... .. , ) /v wh ... . -_ ( ~ ) , , : is the nmnber of activity patterns stored in the network , and m is the number of patterns that were supposed to be present in the mixture state .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">the number of patterns that were supposed to be present in the mixture state</definiens>
			</definition>
</paper>

		<paper id="4193">
			<definition id="0">
				<sentence>E.g. a proposition BEC ( A ) instantiates a monotonic path such that initially NOT ( A ) holds true , mad finally ( A ) holds true .</sentence>
				<definiendum id="0">A )</definiendum>
				<definiens id="0">instantiates a monotonic path such that initially NOT ( A ) holds true , mad finally</definiens>
			</definition>
			<definition id="1">
				<sentence>Thematic information includes information such as which participants can be realized emphatically ( not emphatically ) in which surface form .</sentence>
				<definiendum id="0">Thematic information</definiendum>
				<definiens id="0">includes information such as which participants can be realized emphatically ( not emphatically ) in which surface form</definiens>
			</definition>
			<definition id="2">
				<sentence>\ [ Fillmore 68\ ] Fillmore , Charles : The Case for Case .</sentence>
				<definiendum id="0">Fillmore , Charles</definiendum>
				<definiens id="0">The Case for Case</definiens>
			</definition>
</paper>

		<paper id="2067">
			<definition id="0">
				<sentence>D is the total distance for one specific editing sequence ; that is to say , D is the number of key strokes required to lXlSt-edit the raw translation sentence into file final version sentence .</sentence>
				<definiendum id="0">D</definiendum>
				<definiens id="0">the number of key strokes required to lXlSt-edit the raw translation sentence into file final version sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>The Chinese sentences are the revised version of the corresponding English sentences , which are to be published us a Chinese technical manual .</sentence>
				<definiendum id="0">Chinese sentences</definiendum>
				<definiens id="0">the revised version of the corresponding English sentences , which are to be published us a Chinese technical manual</definiens>
			</definition>
			<definition id="2">
				<sentence>Furthermore , the best transfer and generation is to be selected to maximize file following transfer score Stx/ and generation score Sa~ , , : s , ~/= P ( T~ 17 ; ) ~ I ' ( 7 ~ , 17/ ' ; ) where 7 ' , , 7 ; are the target and source of intemtediate representations in the form of an annotated syntax tree ( AST ) ; qL `` / ; are the normalized version of the AST 's , called the normal forms ( NF ) of the AST 's , which are Aca~s DE COLING-92 , Nnl , rrl 's .</sentence>
				<definiendum id="0">generation</definiendum>
				<definiendum id="1">AST</definiendum>
				<definiens id="0">the normalized version of the AST 's , called the normal forms ( NF ) of the AST 's , which are Aca~s DE COLING-92 , Nnl , rrl 's</definiens>
			</definition>
			<definition id="3">
				<sentence>Figure 4 shows such a conceptual model for the parameterized transfer and generation phases \ [ Chan 92b\ ] , where AST \ [ Chan 92a\ ] is a syntax tree whose nodes are annotated with syntactic and semantic features and NF is a normalized version of AST , which consists of only atomic transfer units .</sentence>
				<definiendum id="0">AST</definiendum>
				<definiendum id="1">NF</definiendum>
				<definiens id="0">a syntax tree whose nodes are annotated with syntactic and semantic features</definiens>
				<definiens id="1">a normalized version of AST , which consists of only atomic transfer units</definiens>
			</definition>
			<definition id="4">
				<sentence>`` GPSM : A Generalized Probabilistic Semantic Model for Ambiguity Resolution , '' to appear in Proceedings ofACL-92 , 30th Annual Meeting of the Association for Computational Linguistics , University of Delaware , Newark , DE , USA , 28 June-2 July , 1992 .</sentence>
				<definiendum id="0">GPSM</definiendum>
				<definiens id="0">A Generalized Probabilistic Semantic Model for Ambiguity Resolution</definiens>
			</definition>
			<definition id="5">
				<sentence>`` ArchTran : A Corpus-based Statisticsoriented English-Chinese Machine Translation System , '' Proceedings of Machine Translation Summit 111 , pp .</sentence>
				<definiendum id="0">ArchTran</definiendum>
			</definition>
			<definition id="6">
				<sentence>Su , `` Syntactic Ambiguity Resolution Using A Discrimination and Robustness Oriented Adaptive Learning Algorithm , '' to appear in Proceedings of COLING92 , 14th International Conference on Computatioual Linguistics , Nantes , France , 20-28 July , 1992 .</sentence>
				<definiendum id="0">Su</definiendum>
				<definiens id="0">Syntactic Ambiguity Resolution Using A Discrimination and Robustness Oriented Adaptive Learning Algorithm , '' to appear in Proceedings of COLING92</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>For exanlple , the semantic rel ) resentation of ( 1 ) is : Surface WH-Question ( fil , $ 2 , _fac , Teaches ( _fac , csa @ ) ) From this surface question , t ) lan inference rules sug gest that ( 1 ) is executing a Rcquest action and that this Request action is part of an Ask-Re\ ] action which in turn is part of an Obtain-lnfl~-Re\ ] action since each of these actions is part of the body of a recipe that performs the higher level action As the system infers these actions , tile system also tentatively ascribes certain beliet~ that must hold in order fl~r the agent to be pursuing these discourse actions l '' or example , m order for ( 1 ) to lw part of ; m Obtaln-ln\ ] o-Rcf action , , ql must not know the answer to thv questh ) n ; if SI knew who was teaching CS360 , this utterance might be part of a 7'est-L+slcncr action instead .</sentence>
				<definiendum id="0">Surface WH-Question</definiendum>
				<definiens id="0">part of an Ask-Re\ ] action which in turn is part of an Obtain-lnfl~-Re\ ] action since each of these actions is part of the body of a recipe that performs the higher level action As the system infers these actions , tile system also tentatively ascribes certain beliet~ that must hold in order fl~r the agent</definiens>
			</definition>
			<definition id="1">
				<sentence>Figure 1 gives the DM that our system builds from utterances ( l ) and ( 2 ) with the current focus of 2We differentiate between telling a listener some string of words said informing a listener of a proposition , hi order to inform a listener of some proposition , the listener must first understaald the content of the proposition ; tiffs is the goal of the Tell action .</sentence>
				<definiendum id="0">tiffs</definiendum>
				<definiens id="0">the current focus of 2We differentiate between telling a listener some string of words said informing a listener of a proposition , hi order to inform a listener of some proposition , the listener must first understaald the content of the proposition</definiens>
			</definition>
			<definition id="2">
				<sentence>Ill both ( 14b ) and ( 14c ) , the system believes l ) that S1 believes that Dr. Smitll won the teaching award last year ( though S1 is not sure of this ) , 2 ) that S1 believes tlmt $ 2 thinks that Dr. Smith is teaching CS360 , and 3 ) that the proposition that Dr. Smith teaches CS360 is in focus .</sentence>
				<definiendum id="0">CS360</definiendum>
				<definiens id="0">in focus</definiens>
			</definition>
			<definition id="3">
				<sentence>One of the possible infereuees from the ConveyUncertain-Belief action is an Ezpress-Doubt action .</sentence>
				<definiendum id="0">ConveyUncertain-Belief action</definiendum>
				<definiens id="0">an Ezpress-Doubt action</definiens>
			</definition>
			<definition id="4">
				<sentence>The Linguistics Discourse Model : Towards a Formal Theory of Discourse Structure .</sentence>
				<definiendum id="0">Linguistics Discourse Model</definiendum>
			</definition>
</paper>

		<paper id="3159">
			<definition id="0">
				<sentence>The manner of presentation , i.e. the repeating pattern of expression produced by a given subject or ill a certain community , is known as style .</sentence>
				<definiendum id="0">manner of presentation</definiendum>
				<definiens id="0">the repeating pattern of expression produced by a given subject or ill a certain community</definiens>
			</definition>
			<definition id="1">
				<sentence>Informative texts are a type of objective narrative texts well-classified by their subjeet domains ( e.g. weather , ecology ) and inheriting from those domains properly devised models .</sentence>
				<definiendum id="0">Informative texts</definiendum>
				<definiens id="0">a type of objective narrative texts well-classified by their subjeet domains ( e.g. weather , ecology ) and inheriting from those domains properly devised models</definiens>
			</definition>
			<definition id="2">
				<sentence>The extraction of normal information is limited to no more than ( 1 + d ) /2 assertions giving the highest precision rate , where d is the number of assertions that would be extracted if the style were detailed .</sentence>
				<definiendum id="0">d</definiendum>
				<definiens id="0">the number of assertions that would be extracted if the style were detailed</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>Ill this pal ) er , we pi'eSell ( r the design and implementation of a morphological root-driven parser tor Turkish word structures which has been mcorporatoed into a spelling checking kerllel for on-line Tiirkish texl , The agghltmative Ilatllre of the language and the resulting ( 'Olll\ [ ) l &lt; ? x Wol 'd \ [ 'ornlatiollS , V ; ll'iOllS pholleLic llall/lOlly l'tlleS alld sill ) tie eKcepLiOllS \ [ ) reselll , cel'taill difficulties llOl usually on ( 'ountered in the spelling checking of laagua , ges like English and make this a very challenging probhnH. Morphological cbussilicat , ion of natttral languages according to their word Stl'tI ( 'ttlrt+s idaces languages like Turkish , Finnish , and lhmgarian Io a class called `` ag ghfl+inalive langua.ges '' . \ [ n sllch hmguages , words are COlllbillaLiOll of several Iilorphel\ ] les. There is a root and several suffixes are conlbined lo this root in order to modil } , ' or extend its meaning. Whal characterizes agglut , inative languages is thai stem fornlation hy at '' fixation 1o previously derived st.oms is extremely productive. A given stellL ew'n Ihough itself qlnt0 corn ph+x , call generally serve as basis for evell lllol'l ' ( 'o111 l ) lex words. Consequ.ntly , agglutinative languages contain words of considerable COnll~lexity , and parsing such languages necessitates a thorough morphological analysis. Morphological parsing has allracted relatively \ ] itl , le attention ill con'tputational linguistics. Tile reason is that nearly all parsing research has been concerned wMl English , or wit.h languages morl ) hologicaII ) similar to English. , qillce in such languages words contain only a fi~w nalldJer of affixes , or none at all. alhnost all of the parsing mod &lt; +ls \ [ br Ill ( ! lll consider recognizing those affix &lt; +s +Is being trivial and thus do nol require a mOt'l+hological nnalysis , hi agghni native langaages , words C ( /lll , ail111o direct indication Of t/lOrl ; llel/le bOtlltdarios whMi at , , i. gellela\ [ ( IOpOll dent on tit ( ? inorpho\ ] ogieal and pllon ( Jh ) gical conlex\ [ + A morphological parser requires a nlorpholdaOloglest\ [ COlllpollellt which l/lediat , es I ) olwl ? ell I\ [ he Sill\ [ kl ( 't • 1\ [ 'o1'111 of a \ [ llorp\ ] lellll ! as ellco/llllel'l ? d ill Ihe il/ptll text aud the lexical form in which the t\ ] torl ) h &lt; ~me is stor &lt; .d ill tile lllOl'phellle illVelltory , ie , a i\ [ WallS of i'e ( 'oglliZing variallt forms of \ [ ll'phelllOS as tll~ ! SaltlO. alld a nlorl ) hotactic component which specilies which corn hi.rot , ions of Inorl ) henws at , '' Iwrn : itt , 'd \ [ 7\ ] \lorphotogical parsing algorithms ma+x he divided it/to Ix ' , ( ) classes as ollir .slrtpl~la 9 ; llt ( I rool-df'iv~ It ; nlal+ ysis met.hods. FIolh approaches hawr beell Ilse/l frOlll very early on in l.he history of morphologicM parsing , For instance , I ) ackal'd 's parser flw ancien| Greek \ [ 15 ) . aud Brodda and Karlsson 's for Finnish \ [ 3\ ] used affix slripping. Sagval\ [ , on tile other hand , devised a rootdriwnl morpllological analyzer for Russian \ [ 17\ ] . In addition , other tool ; driwm morphological parsers for tile agglutinative langmtges Quechna \ [ 9 , 10\ ] , Finnish \ [ l 1\ ] , and Turkish \ [ 6\ ] were developed independently ill the early 1980's+ All of these Ihree pars ( ~rs proceed from left to righl , . Iltlot , s ~tre SOllgh| ill the lexicon that , mat.oh imtial suhstl'ings of the word , and t , he gram Iltatica\ [ category o\ [ the root del , ermines what ( : lass of sutlixes may follow. When a suttix in the permilted class is found to match a furttler substring of t , he word , grammatical mfornlation in 1he lexical entry fl ) r that sulflx del , ernlines once again what class of suffixes may follow. If the end of tile word can be reached hy il.eration of this process , and if the last sullix analyzed is one which illay elld ; i word. t , \ ] le parse is successful \ [ 7\ ] . Another Icft-t+o-right parsing algol'itllni for autolnttlic analysis of Turkish words was proposed and ap plied by I ( iiksal ill his Ph.l ) , thesis II2 } Ills algo rithm called 'qdentified Maxillllllll Mat , ch ( IMM ) AI golithnl '' , tries to find the Ill ; IXinllllll h'ngth subslring , which is present , in a reel dict.ionary , h'OI\ ] l the left of tim word. If a soltltriOll is ollLailled , ie. , the rool IllOl+ \ ] ) ht ? lllU iS identilledL the retnainhlg I ) art of the word is considered as th ( search ( ? \ [ elllellL. This part is looked tbr in the suffix ItlOrl ) henle forms dictionary aml the nlorphemes are idl ! ntified one by one. '\ [ 'he process StOpS whell there is no relllaillillg part. \ [ \ ] owevet ill SOllle casi.s , ; llt\ [ iotlgll it nolat+ioll is ohtained furl , her consistency analysis proves that this solution is tLot the corrccl one. In such cases Ill. previotts pseudo solution is reduced by one character alld all t , he search procedure is initiated once \ [ ll ( ) l'C. 'l'heso approaches to tnorphologicaL parsing of Turk Ac+~+s DE COLING-92 , NANTES , 23-28 Ao~r 1992 3 9 l'roc. OF COLING-92 , NANTES , AUG. 23-28 , 1992 ish words have tim following short.coming : They do not consider the fact that in Turkish , words contain l , rPlllelldOllS alllOlln\ [ , of selnantic illfOrlllat , iOll that has to } ) e taken into account. Ill these parsers , it is only the granlniatical category of the stein that detrrmine *lie suffixes that may follow , l|owever , niost of the sultixes in Turkish , especially the derivational oaes , call be at.taclled only to a linlited number of reels or sleltlS Inostl } duo to Sel/lallliC reasollS. Another shortcolnhig of the previous parsers for Turkish is ihat they allow ille iterativr ilsage of derNaiional su\ [ fixes. Although , bi6ksal \ [ 12\ ] , prevelltS the COIISeC/liiVe |lsagl , of the Sallle ltlOl'i ) hellle lwicc , lie slill l ) arsos the word G ( 3ZI , ( II,2 ( ' { iI , ( 'YI , ~ ( '.i ) L { 31 , ; correctly , so do llankalner \ [ 7\ ] . It is tl'lli '' l\ ] lat. SOltle Turkish sutlixes can form aa iteraiive loop. but usually th , ' number of iteratioli is not too high. rl'he above word ran I ) e parsed correctly Ul ; to lhe point G ( 3Zl , { 'l ( ( i : l~ ! L { 'tl , ; ( the occultation of oculists ) , but the words GOZI , UI,2 ( , 'UI , UI , ; ( , ' { : and ( IOZLUI ( ( , :trLI l ( ( ' ( ! l , { iK are meaningh'ss , and tllerefore sonle conlro\ ] llle ( ' } lalliSlllS IlSilig semantic iii\ [ oriliat , iOll SilOllid be illeluded wilhin the parser Io avoid parsing StlCli inealtinglrss words as if lhey werr corrl &gt; ci .</sentence>
				<definiendum id="0">word Stl'tI</definiendum>
				<definiendum id="1">IOZLUI</definiendum>
				<definiens id="0">l'tlleS alld sill ) tie eKcepLiOllS \ [ ) reselll , cel'taill difficulties llOl usually on ( 'ountered in the spelling checking of laagua , ges like English</definiens>
				<definiens id="1">~rs proceed from left to righl , . Iltlot , s ~tre SOllgh| ill the lexicon that , mat.oh imtial suhstl'ings of the word , and t</definiens>
			</definition>
			<definition id="1">
				<sentence>Lea : is used Io separate tile suffixes of a word from left to right , ; 111 ( I I/ace is tlsed to p ; q'se tilose su { \ [ ixes tlsil\ ] g Illorpilological rules of Turkish granllrlar .</sentence>
				<definiendum id="0">Lea</definiendum>
				<definiens id="0">used Io separate tile suffixes of a word from left to right</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>Prince states : Their function , or at least one of their functions , isTO MARK A PIECE OF INFORMATION AS FACT~ known to some people although not yet known to the intended hearer .</sentence>
				<definiendum id="0">Prince states</definiendum>
				<definiens id="0">Their function , or at least one of their functions</definiens>
			</definition>
			<definition id="1">
				<sentence>The Aspectual Effect of the Cleft What we want at this point is an account which can recruit the syntactic and semantic features of the cleft , to explain the background and regress data that has been observed , feed into the discourse parse process , and explain the Known Fact Effect .</sentence>
				<definiendum id="0">Aspectual Effect</definiendum>
				<definiens id="0">an account which can recruit the syntactic and semantic features of the cleft , to explain the background and regress data that has been observed , feed into the discourse parse process , and explain the Known Fact Effect</definiens>
			</definition>
			<definition id="2">
				<sentence>The DRT notion of temporal overlap is a permissive relation ; in a case like ( 10 ) , we can follow a pair of event-expressions with various state-expressions , all of which DRT would say denote states which overlap the event already introduced .</sentence>
				<definiendum id="0">DRT notion of temporal overlap</definiendum>
			</definition>
			<definition id="3">
				<sentence>J : If yon look in the basket there 's that purple one S : I thought you were drying some out on the SBall ( p.c. ) has pointed out that it is not acceptable as complement of a copular sentence whose subject is it in any case : e.g. *it 's it vs. that 's it .</sentence>
				<definiendum id="0">J</definiendum>
			</definition>
			<definition id="4">
				<sentence>J : That 's it S : Oh , so it is A second plausible explanation may be that it , unique among the pronouns , has no cuntrastive reading ( el .</sentence>
				<definiendum id="0">J</definiendum>
				<definiens id="0">That 's it S : Oh , so it</definiens>
			</definition>
			<definition id="5">
				<sentence>Polanyi , L. \ [ 1986\ ] The linguistic discourse model : towards a formal theory of discourse structure , nBN Report No. 6409 , Cambridge , MA .</sentence>
				<definiendum id="0">discourse model</definiendum>
				<definiens id="0">towards a formal theory of discourse structure</definiens>
			</definition>
</paper>

		<paper id="2115">
			<definition id="0">
				<sentence>2 At , ldg G consists of a set of nodes N , and a set of arcs A. Further , each node and art : has a label , ht particular , node labels are unique .</sentence>
				<definiendum id="0">ldg G</definiendum>
				<definiens id="0">consists of a set of nodes N , and a set of arcs A. Further , each node and art : has a label , ht particular , node labels are unique</definiens>
			</definition>
			<definition id="1">
				<sentence>A translation rnle 4 r consists of the folk , wing three corrtpo , leots : r = ( G , , , ,M , G~ ) where Gm is a matching gr~rph , G~ is a construction graph , e.nd M is a set of mappings between Gm and A matching graph G ' , , , and a construction graph G~ must be at lea .</sentence>
				<definiendum id="0">G~</definiendum>
				<definiens id="0">r = ( G , , , ,M , G~ ) where Gm is a matching gr~rph ,</definiens>
				<definiens id="1">a construction graph , e.nd M is a set of mappings between Gm and A matching graph G ' , , , and a construction graph G~ must be at lea</definiens>
			</definition>
			<definition id="2">
				<sentence>We call the former an upward mapping and the latter a downward mapping , and denote these twn kinds of mapping as follows : where M T is upward mapping , and M ~ is downward mapping .</sentence>
				<definiendum id="0">M T</definiendum>
				<definiens id="0">downward mapping</definiens>
			</definition>
			<definition id="3">
				<sentence>, R. , ) + E , , , min ( D'a ( VS ( Ri ... .. ) , GS ( t~ , , ... . ) ) ) where R/ , and /~ are roots of Gi~ and Gm , respectlvely~ D , , is a node distance , a= is an arc in G , n such that its source node is R.m , and GS ( n~ a ) denotes a subgraph that is related to an arc a from n. Briefly , a simple distance is the sum of the node distance between two roots and the sum of the minimal simple distances between Gin subgraphs and Gm subgraphs that , far each arc a outgoing from the GmmOt node , are related to the all arcs a from the root nodes .</sentence>
				<definiendum id="0">min</definiendum>
				<definiendum id="1">/~</definiendum>
				<definiendum id="2">a=</definiendum>
				<definiendum id="3">GS</definiendum>
				<definiens id="0">D'a ( VS ( Ri ... .. ) , GS ( t~ , , ... . ) ) ) where R/ , and</definiens>
				<definiens id="1">a node distance</definiens>
				<definiens id="2">an arc in G , n such that its source node is R.m</definiens>
			</definition>
			<definition id="4">
				<sentence>The node distance between a Gin node n i and a G , , , node nm is detined ms follows : Dn ( hi , nm ) D/+ D , * 6 , N S +a. where DI is a feature node distance , D , is a semantic no ( h. '' distance , N I is the number of features in nm for DI , and 6 , is the weight of a semantic distance .</sentence>
				<definiendum id="0">DI</definiendum>
				<definiendum id="1">N I</definiendum>
				<definiens id="0">a feature node distance</definiens>
				<definiens id="1">a semantic no ( h. '' distance</definiens>
				<definiens id="2">the weight of a semantic distance</definiens>
			</definition>
			<definition id="5">
				<sentence>A pivot is a node of ( ; ~ , ~ that has more than one origin graph , attd a matching pivot is the origin node of a pivot .</sentence>
				<definiendum id="0">pivot</definiendum>
				<definiendum id="1">matching pivot</definiendum>
				<definiens id="0">a node of ( ; ~ , ~ that has more than one origin graph , attd a</definiens>
			</definition>
			<definition id="6">
				<sentence>Although there were several early experimental projects on CBMT \ [ 4\ ] \ [ 9\ ] \ [ 11\ ] , MWF-H \ [ 10\ ] is the first working prototype of a case-based transfer systern~ and demonstrates the promise of the CBMT alrproadL It uses Japanese-to-English translation exanlples as translation rules : chooses the source trees of examples that are most similar to the iuput tree from the root node down to the leaves , and assembles those target trees to produce an output tree , With respect to the transducing mechanism , MBT-II is a tree-totree transducer adopting one -- to-one correspondeuce .</sentence>
				<definiendum id="0">MBT-II</definiendum>
				<definiens id="0">the first working prototype of a case-based transfer systern~ and demonstrates the promise of the CBMT alrproadL It uses Japanese-to-English translation exanlples as translation rules</definiens>
				<definiens id="1">a tree-totree transducer adopting one -- to-one correspondeuce</definiens>
			</definition>
			<definition id="7">
				<sentence>As a transducing mechanism , RCT is a parallel nondestructive rldag-to-rklag transducing system .</sentence>
				<definiendum id="0">RCT</definiendum>
			</definition>
</paper>

		<paper id="4210">
			<definition id="0">
				<sentence>The system consists o\ [ two I ) asi ( : colnpollellts : \ ] , cxi ( : on ( containing ~'~ome 13.000 most corn\ [ non words ) ; l|ibliograt ) hical ( 1 ; md ) ase .</sentence>
				<definiendum id="0">system</definiendum>
				<definiens id="0">consists o\ [ two I ) asi ( : colnpollellts : \ ] , cxi ( : on ( containing ~'~ome 13.000 most corn\ [ non words</definiens>
			</definition>
			<definition id="1">
				<sentence>the forill of a ( lataln~se , which provides the fitcility of compiling all sorts of \ [ exical lists .</sentence>
				<definiendum id="0">forill of a ( lataln~se</definiendum>
				<definiens id="0">provides the fitcility of compiling all sorts of \ [ exical lists</definiens>
			</definition>
</paper>

		<paper id="2073">
			<definition id="0">
				<sentence>The student asks the questions and the system answers them .</sentence>
				<definiendum id="0">student</definiendum>
			</definition>
			<definition id="1">
				<sentence>student ) asks questions and the teacher ( native speaker , parent/teacher ) answers them .</sentence>
				<definiendum id="0">student</definiendum>
			</definition>
			<definition id="2">
				<sentence>A notable exception is the work of V. Ehrich \ [ 6\ ] who describes the properties of a situation ( a convenient neutral term for making indiscriminately reference to eveals , actious , processes , states ) in terms of category , aspoct , relation , and position .</sentence>
				<definiendum id="0">notable exception</definiendum>
				<definiens id="0">describes the properties of a situation ( a convenient neutral term for making indiscriminately reference to eveals , actious , processes , states ) in terms of category , aspoct , relation , and position</definiens>
			</definition>
			<definition id="3">
				<sentence>Nakhimovsky , in a more recent classification \ [ 17\ ] , introduces additional features ( generic vs. specific resources for atclic processes ) to Vendler 's punctuality , stativity , and telieity .</sentence>
				<definiendum id="0">Nakhimovsky</definiendum>
				<definiens id="0">generic vs. specific resources for atclic processes ) to Vendler 's punctuality , stativity , and telieity</definiens>
			</definition>
			<definition id="4">
				<sentence>\ [ 7\ ] Fum D. , P. Giagrandi &amp; C. Tasso , ET : an Intelligent Tutor for Foreign Language Teaching .</sentence>
				<definiendum id="0">ET</definiendum>
			</definition>
			<definition id="5">
				<sentence>\ [ 12\ ] Hinrichs E. , A Compositional Semantics of Temporal Expressions in English .</sentence>
				<definiendum id="0">Hinrichs E.</definiendum>
				<definiens id="0">A Compositional Semantics of Temporal Expressions in English</definiens>
			</definition>
</paper>

		<paper id="3128">
			<definition id="0">
				<sentence>4The notation has been borrowed from Gazdar &amp; Mellish 1989 ; '= -- = ' is the unification operator , and Node : Attr indicates a path in a graph ( or a field iu a record ) .</sentence>
				<definiendum id="0">Attr</definiendum>
				<definiens id="0">indicates a path in a graph ( or a field iu a record )</definiens>
			</definition>
			<definition id="1">
				<sentence>To this end , Baart ( 1987 ) assumes the following rule : DEFAULT ACCENT a I\ = &gt; I\ W S S W A B A B b /\ = &gt; /\ S W W S B A B A Condition : B is deaccented In PROS-3 , this rule is implemented as a filter , called STP , wtfich takes as input a syntactic structure assigned by the parser , and produces as output a metrical tree .</sentence>
				<definiendum id="0">DEFAULT ACCENT</definiendum>
				<definiens id="0">a I\ = &gt; I\ W S S W A B A B b /\ = &gt; /\ S W W S B A B A Condition : B is deaccented In PROS-3</definiens>
			</definition>
</paper>

		<paper id="2093">
			<definition id="0">
				<sentence>Our conceptual model , which constitutes the input to the text generation process , is based on Qualitative Process Theory IForbus 84\ ] .</sentence>
				<definiendum id="0">conceptual model</definiendum>
				<definiens id="0">constitutes the input to the text generation process</definiens>
			</definition>
			<definition id="1">
				<sentence>Esr , ccially , communicative acts on causal opposition relations are introduced in two kind of situation : ( a ) if a complete description of causal behaviour is required or ( b ) within a concessive strategy , which seem s appropriate when the user focuses on an expected but yet unoeccurred event .</sentence>
				<definiendum id="0">Esr</definiendum>
				<definiens id="0">seem s appropriate when the user focuses on an expected but yet unoeccurred event</definiens>
			</definition>
			<definition id="2">
				<sentence>Cerbah , E &amp; Fournier , C. &amp; Raccah , P.Y. `` Qualitative reasoning and Argumentation : A study of some affinities when Generating Causal Explanations '' , 1st Workshop on Qualitative Reasoning and Decision Support Systems , 1991 .</sentence>
				<definiendum id="0">Argumentation</definiendum>
				<definiens id="0">A study of some affinities when Generating Causal Explanations ''</definiens>
			</definition>
</paper>

		<paper id="4176">
			<definition id="0">
				<sentence>The Concept is the primary representational entity .</sentence>
				<definiendum id="0">Concept</definiendum>
				<definiens id="0">the primary representational entity</definiens>
			</definition>
			<definition id="1">
				<sentence>Concepts are connected to one another by superC links , represented in tile figures by double arrows , A superC link indicates that tile subordinate Concept ( subConeept ) stands in an inheritance and subsumption relationship with the superordinate Concept ( superConeept ) .</sentence>
				<definiendum id="0">subConeept )</definiendum>
			</definition>
			<definition id="2">
				<sentence>The algorithm requires three functions : 1 , I '' indI-loms ( HC1 , HC2 , C1 , C2 ) searches the knowledge base for two homophonous Coneepts , HC1 and HC2 where HCl and HC2 are the value restrictions of two Coneepts ' RoleSets .</sentence>
				<definiendum id="0">I '' indI-loms</definiendum>
				<definiendum id="1">C2 )</definiendum>
				<definiens id="0">searches the knowledge base for two homophonous Coneepts , HC1 and HC2 where HCl and HC2 are the value restrictions of two Coneepts ' RoleSets</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , Lex ( RIVER_-MOUTH , H ) returns/mawO'/in 13 , :3 , MisMateh ( CI , C2 , HC1 , HC2 , Type , RSVR ) examines the knowledge base ( KB ) for a mismateh of the following type : HCI has a RoleSet value restrietion ( RSVR ) that He2 does have .</sentence>
				<definiendum id="0">RSVR )</definiendum>
				<definiendum id="1">KB</definiendum>
				<definiendum id="2">HCI</definiendum>
				<definiens id="0">examines the knowledge base (</definiens>
			</definition>
</paper>

		<paper id="2085">
			<definition id="0">
				<sentence>In order to overcmne the difficulties of these methods , we propose a Linguistic Knowledge Generator ( LKG ) which working on the principle of `` Gradual Approximation '' involving both human introspection and discovery programs .</sentence>
				<definiendum id="0">Linguistic Knowledge Generator ( LKG</definiendum>
				<definiens id="0">working on the principle of `` Gradual Approximation '' involving both human introspection and discovery programs</definiens>
			</definition>
</paper>

		<paper id="4195">
			<definition id="0">
				<sentence>KNOWLEDGE The main linguistic knowledge sourccs of thc system are the morph dictionary , which contains information about tile morph class/es each morph belongs to , and tile word syntax .</sentence>
				<definiendum id="0">morph dictionary</definiendum>
				<definiens id="0">contains information about tile morph class/es each morph belongs to , and tile word syntax</definiens>
			</definition>
			<definition id="1">
				<sentence>Adl'ectives The adjective net consists of three subnets , each representing a possible way of adjectival derivation in German .</sentence>
				<definiendum id="0">adjective net</definiendum>
				<definiens id="0">consists of three subnets , each representing a possible way of adjectival derivation in German</definiens>
			</definition>
</paper>

		<paper id="2112">
			<definition id="0">
				<sentence>Beside grammatical representations , Weathra uses representations of the meteorological raw facts and secondary facts , e.g. the fact that it will probably rain at a place where there is a low pressure area .</sentence>
				<definiendum id="0">Weathra</definiendum>
				<definiens id="0">uses representations of the meteorological raw facts and secondary facts</definiens>
			</definition>
			<definition id="1">
				<sentence>The following is one of the DCG rules generating and analyzing full sentences : ewsent ( N , I event ( V ) , actor ( N ) , time ( T ) , advl ( A 1 ) , co ( C ) \ ] ) -- &gt; enp ( Agr , N ) , evi ( Agr , m ( V , T ) ) , eadv ( A 1 ) , esco ( C ) .</sentence>
				<definiendum id="0">1</definiendum>
				<definiendum id="1">V , T ) ) , eadv</definiendum>
				<definiens id="0">one of the DCG rules generating and analyzing full sentences : ewsent ( N , I event ( V ) , actor ( N ) , time ( T ) , advl ( A</definiens>
			</definition>
</paper>

		<paper id="1002">
</paper>

		<paper id="4175">
			<definition id="0">
				<sentence>Other systems ( e.g. \ [ 5\ ] ) allow the representation of situations etc. within some other formalism ( e.g. feature structures ) but do not use situation theory itself as the basis for the language .</sentence>
				<definiendum id="0">Other systems</definiendum>
				<definiens id="0">the representation of situations etc. within some other formalism ( e.g. feature structures ) but do not use situation theory itself as the basis for the language</definiens>
			</definition>
			<definition id="1">
				<sentence>ASTL ASTL is a language based on situation theory .</sentence>
				<definiendum id="0">ASTL ASTL</definiendum>
			</definition>
			<definition id="2">
				<sentence>ASTL includes stone builtill support for natural language parsing based on tile ideas of Situation Theoretic Grammar \ [ 4\ ] while PRoslrr is designed more for knowledge representation than direct language processing .</sentence>
				<definiendum id="0">ASTL</definiendum>
				<definiens id="0">includes stone builtill support for natural language parsing based on tile ideas of Situation Theoretic Grammar \ [ 4\ ] while PRoslrr is designed more for knowledge representation than direct language processing</definiens>
			</definition>
			<definition id="3">
				<sentence>Based on EKN \ [ l\ ] ASTL objects can be displayed a.s boxes , making comple× objects nmch easier to view .</sentence>
				<definiendum id="0">ASTL</definiendum>
				<definiens id="0">objects can be displayed a.s boxes , making comple× objects nmch easier to view</definiens>
			</definition>
			<definition id="4">
				<sentence>Situation Theoretic Grammar takes the view that utterances can be represented by situations .</sentence>
				<definiendum id="0">Situation Theoretic Grammar</definiendum>
				<definiens id="0">takes the view that utterances can be represented by situations</definiens>
			</definition>
			<definition id="5">
				<sentence>A I ) RS consists of two parts ; a set of domain markrr .</sentence>
				<definiendum id="0">I ) RS</definiendum>
				<definiens id="0">consists of two parts ; a set of domain markrr</definiens>
			</definition>
			<definition id="6">
				<sentence>A DRS itself is represented as a parametric situation -- a situation whose type contains parameters .</sentence>
				<definiendum id="0">DRS itself</definiendum>
				<definiens id="0">a parametric situation -- a situation whose type contains parameters</definiens>
			</definition>
			<definition id="7">
				<sentence>hi this description , a discourse consists of a set of utterance situations which call In ' viewed tim ) ugh a number of different structural relations , The tirsl is through tile relation daughter which defines tile syntactic structure of lhe discourse as defined by the grammar rules ( immediate doininance and linear precedence ) .</sentence>
				<definiendum id="0">discourse</definiendum>
				<definiens id="0">consists of a set of utterance situations which call In ' viewed tim ) ugh a number of different structural relations , The tirsl is through tile relation daughter which defines tile syntactic structure of lhe discourse as defined by the grammar rules ( immediate doininance and linear precedence )</definiens>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>However , since Alice is subject , their theory predicts that Alice is the prinrary candidate of antecedent of/l ) , ubj .</sentence>
				<definiendum id="0">Alice</definiendum>
			</definition>
			<definition id="1">
				<sentence>Principle 1 ( Preference ( tentative ) ) Consider a discourse of two sentences or a complex sentence in which one of the sentence or clause describes the action taken by an agent , and the other sentence or clause provides the reason of the action respectively .</sentence>
				<definiendum id="0">Preference</definiendum>
				<definiens id="0">( tentative ) ) Consider a discourse of two sentences or a complex sentence in which one of the sentence or clause describes the action taken by an agent , and the other sentence or clause provides the reason of the action respectively</definiens>
			</definition>
			<definition id="2">
				<sentence>1992 Definition 2 ( Observer ) Observer is defined as a person who , from his/her point of view , recognizes some other experieneer 's psychological state by observing that experieneer .</sentence>
				<definiendum id="0">Observer ) Observer</definiendum>
				<definiens id="0">a person who , from his/her point of view , recognizes some other experieneer 's psychological state by observing that experieneer</definiens>
			</definition>
			<definition id="3">
				<sentence>Strong POV Bound Verb `` kininaru '' ( feel anxiety ) used in ( 7 ) , so called subjective adjectives , that express a psychological state~ like `` ureshi-i '' ( be happy ) , `` kanashi-i '' ( be sad } , `` X-wo-kanziru '' ( feel X ) where X is a noun which represents a certain psychological state , and so on are oJ this type .</sentence>
				<definiendum id="0">subjective adjectives</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a noun which represents a certain psychological state</definiens>
			</definition>
			<definition id="4">
				<sentence>W.Poser ) CSLI , 1988 \ [ 5\ ] Kameyama , M. , A Property-Sharing Constraint in Centering , 24th Annual Meeting of ACL , pp200-206 , 1986 \ [ 6\ ] Kinsui , S. , Houkoku ni tuite no oboegaki ( 'Memo about Reporting ' ) in Nihongo no Modality ( 'Modality in Japanese ' ) , Tokyo , Kuroshio-Shuppan , 1989 \ [ 7\ ] Kuno , S. , Danwa no Bunpoo ( 'Grarmner of Discourse ' ) , Tokyo , Taishuukan , 1978 \ [ 8\ ] Walker , M. , M. lida and S. Cote , Centering in Japanese Discourse , COLING'90 , 1990</sentence>
				<definiendum id="0">Property-Sharing Constraint</definiendum>
				<definiens id="0">in Centering , 24th Annual Meeting of ACL</definiens>
				<definiens id="1">Kuno , S. , Danwa no Bunpoo ( 'Grarmner of Discourse ' ) , Tokyo , Taishuukan , 1978 \ [ 8\ ] Walker , M. , M. lida</definiens>
			</definition>
</paper>

		<paper id="3168">
			<definition id="0">
				<sentence>We believe that KANT is the first system to bring these ideas together in a system that provides fast , accurate , high-quality knowledge-based translation .</sentence>
				<definiendum id="0">KANT</definiendum>
				<definiens id="0">the first system to bring these ideas together in a system that provides fast , accurate , high-quality knowledge-based translation</definiens>
			</definition>
			<definition id="1">
				<sentence>To resolve the ambiguity introduced by multiple possible phrase attachments , KANT uses an explicit domain model to narrow the set of potential interpretations ( cf. Figure 1 ) .</sentence>
				<definiendum id="0">KANT</definiendum>
			</definition>
			<definition id="2">
				<sentence>For every phrase ( such as verb phrase or noun phrase ) that accepts a potentially ambiguous phrase attachment ( such as a prepositional phrase ) , KANT constrains the set of allowable attached phrases to just those that meet the narrow semantic restrictions of the particular domain .</sentence>
				<definiendum id="0">KANT</definiendum>
				<definiens id="0">such as verb phrase or noun phrase</definiens>
			</definition>
			<definition id="3">
				<sentence>Each mapping rule is intended to apply to a single Interlingua concept , which may contain other Interlingan concepts as slot fillers ; the Mapper uses a recursive-descent f-structure composition algorithm , which is discussed in ( Nyberg et al. , 1991 ) .</sentence>
				<definiendum id="0">Mapper</definiendum>
			</definition>
			<definition id="4">
				<sentence>In a narrow technical domain , KANT achieves near-perfect semantic accuracy .</sentence>
				<definiendum id="0">KANT</definiendum>
				<definiens id="0">achieves near-perfect semantic accuracy</definiens>
			</definition>
			<definition id="5">
				<sentence>The target language lexicons contain these technical terms and a smaller subset of the general terms , and are currently being extended .</sentence>
				<definiendum id="0">target language lexicons</definiendum>
				<definiens id="0">contain these technical terms and a smaller subset of the general terms , and are currently being extended</definiens>
			</definition>
</paper>

		<paper id="4206">
			<definition id="0">
				<sentence>Yes/no and `` how many '' questions elicit a simple textual response , whereas the response to other queries is a textual pointer ( e.g. See table 13 ) to a report displayed elsewhere on the screen .</sentence>
				<definiendum id="0">Yes/no</definiendum>
				<definiens id="0">a textual pointer ( e.g. See table 13 ) to a report displayed elsewhere on the screen</definiens>
			</definition>
			<definition id="1">
				<sentence>None of our gestural query methods have the expressive power of a relational query language such as , say , QBE ( Zloof , 1975 ) ; rather , we have created a set of graphical access methods tailored to our target users ' needs , which strike a balance between expressive power and ease of use .</sentence>
				<definiendum id="0">gestural query methods</definiendum>
				<definiens id="0">strike a balance between expressive power and ease of use</definiens>
			</definition>
</paper>

		<paper id="2114">
			<definition id="0">
				<sentence>b ) Textual conlext consists of the linguistic content of previous and subsequent clauses .</sentence>
				<definiendum id="0">Textual conlext</definiendum>
				<definiens id="0">consists of the linguistic content of previous and subsequent clauses</definiens>
			</definition>
			<definition id="1">
				<sentence>Formally defined , a Lexical Function f is a correspondence between a lexical item L , called the key word of f , and a set of lexical items f ( L ) the values of f ( Mel'~uk &amp; Zholkovskij 1970 , Mel'~uk 1988b ) .</sentence>
				<definiendum id="0">Lexical Function f</definiendum>
				<definiens id="0">a correspondence between a lexical item L , called the key word of f , and a set of lexical items f ( L ) the values of f ( Mel'~uk &amp; Zholkovskij 1970</definiens>
			</definition>
			<definition id="2">
				<sentence>Some paradigmatic LFs can be used to analyse or generate lexical coreference relations : Syn : synonym ConvUkl : conversive aener : generic word St : typical noun for the i-th actant Slratr : noun for typical instrument Sm~ : noun for typical means Site : noun for typical place Sr~ : noun for typical result Stood : noun for typical mod~ S a : name of action Syn ( callhlg ) = vocation Conv32 t 4 ( sell ) = buy Gener ( apple ) = fruit SI ( lecture ) = lecturer Sinsn ( palnt ) = brush Smed ( ltolsalt ) = salt Slo~ ( box ) ~ ring S~ ( mix ) = mixture Stood ( write ) = writing S0 ( buy ) = purchase Relations encoded by these LFs can appear in direct eoreferential relations in texts when the value of the function and the key word maintain a semantic relationship directly formalizable through a LF such as Sres , Gener , Syn and Convijkl , as in : ( 8 ) Gener ( lamb ) = meat Buy lamb .</sentence>
				<definiendum id="0">paradigmatic LFs</definiendum>
				<definiendum id="1">synonym ConvUkl</definiendum>
				<definiendum id="2">Gener</definiendum>
				<definiens id="0">( lamb ) = meat Buy lamb</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , the LF Syn plays a transparent role in composition .</sentence>
				<definiendum id="0">LF Syn</definiendum>
				<definiens id="0">plays a transparent role in composition</definiens>
			</definition>
			<definition id="4">
				<sentence>They define the basic level as follows : `` basic objects a~e the most inclusive categories whose members : ( a ) possess significant numbers of attributes in cmmnon , ( b ) have motor programs which are similar to one another , ( c ) have similar shapes , and ( d ) can be identified from averaged shapes of members of the class '' ( Rosch et al. 1976 : 382 ) It has been shown that lexemes correspomling to basic level objects seem to be the most natural terms to introduce referents already idcntified .</sentence>
				<definiendum id="0">basic level</definiendum>
			</definition>
			<definition id="5">
				<sentence>`` llm distance from the antecedent can be quite far bat focalization coustraints , in particular global focus defined as the subset of the most salient items play a determining role for the production of this anaphor .</sentence>
				<definiendum id="0">llm distance</definiendum>
				<definiens id="0">the subset of the most salient items play a determining role for the production of this anaphor</definiens>
			</definition>
			<definition id="6">
				<sentence>Concepicual Properties Linguistic Non ambiguity `` Lexleal anaphor Properties Constraints Examples Unique object'or set of Antecedent is a 'single No instance previously lapin -- &gt; lapin \ [ rabi~it\ ] Strict Repetition Initial Strict Repetition identical objects Unique object or set of noun ( or fixed compound ) and is a basic noun introduced has the same repetition No constraints Tile other devices are A small rabbit ... the identical objects ambiguous rabbit -- &gt; the small rabbit Partial Repetition Unique object or set of Antecedent is a not fixed No previously introduced petit lapin \ [ small rabbit\ ] identical objects compound ( except `` partNP has the same partial -- &gt; lapin of '' types ) and the NP repetition head is a basic nt~un Superordlnatlon Set ' of objects having a\ [ Nominal heads of anteNo previously introduced { carottes , poireaux , close common genericl cedents have the same NP has the same concept common superordinate , supemrdinate term LF : Gener Basic Denomination Umque Object or set of Nominal head of NP is No previously introduced identical objects not a basic noun NP has the same basic denomination N°mlnallzatl°n Action ' '' No constraints Object ( s ) having been , affected by a strong transformation Set of different objects which have no common genetic concept Typical Result Mention Antecedent verb can be nominalized or superordinate of antecedent verb can be nominalized .</sentence>
				<definiendum id="0">Antecedent</definiendum>
				<definiens id="0">a 'single No instance previously lapin -- &gt; lapin \ [ rabi~it\ ] Strict Repetition Initial Strict Repetition identical objects Unique object or set of noun ( or fixed compound ) and is a basic noun introduced has the same repetition No constraints Tile other devices</definiens>
				<definiens id="1">a not fixed No previously introduced petit lapin \ [ small rabbit\ ] identical objects compound ( except `` partNP has the same partial -- &gt; lapin of '' types</definiens>
				<definiens id="2">a basic nt~un Superordlnatlon Set ' of objects having a\ [ Nominal heads of anteNo previously introduced { carottes , poireaux</definiens>
				<definiens id="3">the same concept common superordinate , supemrdinate term LF : Gener Basic Denomination Umque Object or set of Nominal head of NP is No previously introduced identical objects not a basic noun NP has the same basic denomination N°mlnallzatl°n Action ' '' No constraints Object ( s ) having been , affected by a strong transformation Set of different objects</definiens>
			</definition>
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>In the travelnig salesman example , the configurations are the different paths through the cities , and E is the total length of his trip .</sentence>
				<definiendum id="0">E</definiendum>
			</definition>
			<definition id="1">
				<sentence>Given a sentence with N words , we may represent the senses of the ith word as sil , si2 , ' sik , , where k~ is the number of senses of the ith word which appear in LDOCE .</sentence>
				<definiendum id="0">k~</definiendum>
				<definiens id="0">the number of senses of the ith word which appear in LDOCE</definiens>
			</definition>
			<definition id="2">
				<sentence>0 , we change to C ' with probability ~E P = e r. In this expression , T is a constant whose initial value is 1 , and thedecision of whether or not to adopt C ' is made by calling a random number generator .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">a constant whose initial value is 1 , and thedecision of whether or not to adopt C ' is made by calling a random number generator</definiens>
			</definition>
			<definition id="3">
				<sentence>This process of generating new configurations and checking to see whether or not to choose them is repeated on the order of 1000 times , T is replaced by 0.9T , and the loop entered again .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">checking to see whether or not to choose them is repeated on the order of 1000 times</definiens>
			</definition>
			<definition id="4">
				<sentence>Preference Semantics : a family history , To appear m Computing and Mathematics with Applications ( in press ) .</sentence>
				<definiendum id="0">Preference Semantics</definiendum>
				<definiens id="0">a family history , To appear m Computing and Mathematics with Applications ( in press )</definiens>
			</definition>
</paper>

		<paper id="3149">
			<definition id="0">
				<sentence>The Workstation is being constructed around two main components : a bilingual lexical database system and a system that creates and manages bilingual text archives .</sentence>
				<definiendum id="0">Workstation</definiendum>
				<definiens id="0">a bilingual lexical database system and a system that creates and manages bilingual text archives</definiens>
			</definition>
			<definition id="1">
				<sentence>The lexical components of the MLDB include the Italian Machine Dictionary ~ mainly based on the Zingarelli Italian Dictionary - , and LDBs derived from the Garzanti 'Nuovo Dizionario Italiano ' , and the Collins Concise ItalianEnglish , English-Italian Dictionary ; we hope to add an English LDB shortly .</sentence>
				<definiendum id="0">LDBs</definiendum>
				<definiens id="0">the Italian Machine Dictionary ~ mainly based on the Zingarelli Italian Dictionary - , and</definiens>
			</definition>
</paper>

		<paper id="4201">
			<definition id="0">
				<sentence>13 Words with zero jnm : ture can be at any position ill a Conlponnrt word : Import-beschrSnkung ( 'import restrletion ' ) Fisch-import ( 'fish import ' ) Fi $ ch-lmport-belchr~nk ung E1 Words of which the connecting element is in the inflectional paradigm ( : an also be al , any position in a compound word : Parlament+s-debatte ( 'parliamentary debate ' ) ( der Sitz des ) Btmdes-parlament+s ( ' ( the seat of the ) federal parliament ' ) \ [ \ ] Words of which the ending is deleled can only hc in front or middle position : Schul- .</sentence>
				<definiendum id="0">Parlament+s-debatte</definiendum>
				<definiens id="0">the seat of the ) federal parliament '</definiens>
			</definition>
			<definition id="1">
				<sentence>*MuBik-schul , but -~ehule ( 'music school ' ) \ [ :3 Words of which the connecting element is not in the inflectional para &lt; ligm ( : an only be in front or middle position : Information+t-materlal ( 'inform. material ' ) `` Studenten-information+s , but-information ( 'information lot students ' ) COMPGE in LMT-GE \ ] 'he general frarnework for our research work and implementation is the machine translation system LMT developed by Michael McCord. '~ LMT is a lexicalistlc , source based transfer system , in this section , we concentrate on the performance of the PI { DLOG algorithm 'Compound Interpretation COMPGE ' as a hook up component to LMT GE ( German F , nglish ) . The segmentation and translation algorithm COMPGE is only called upon if an input word ( with more than five letters ) has not heen found in the system 's lexicon or in the on llnc accessible MR1 ) Collins German English ~ , i.e. when lookup and the regular '~LMT and related pr¢~jects are described in detail in ( \ [ McC.rd 1989\ ] ; \ [ Rimon et el. 1991\ ] ; \ [ Schwall t991D. AcrEs DE COLING-92. N/ , tClT~. 23-28 AOtJT 1992 1 2 5 0 PROC. OF COLlNG-92. NANTES , AUG. 23-28 , 1992 remrphological analysis fail. The segmentation is then carried ont front left to right , begianlng after the third letter. The decomposition process eontinues until the first word is fonnd in the lexicon ; the dictionary el/try contains , among other data , information ahont tile connecting element ( Fugcn code ) . The algorithm then takes the complete dictionary entry with sonrce and target word and all information contained in it+ , stores the word and continues by looking up the rest as a whole. If an entry is fraud , it is stored as well , together with the relevant ntorphological , syntactic , and seinantic information. If there is , on the other hand , no entry for the remainder as a whole , the segmentation is carried on letter for letter , the same way as for tile first constituent until an analysis Sir an ex isting entry is derived. When all eonstltuents are found , the words are stored , and segmentation is started again in order to allow , in a , nbiguous cases , for /rtorc than one possihie segmentation. Let us look at the word Messeralienist , rl'he result of the first de ( : omposition wouht be Messe.-rallen-lat ( 'mass-ral-aclion ' ) , in accordance with the bitgcn codes of till+ segments ; the second result wouht be Messer-allental ( 'kniJe-aflack ' } , also in accordance with the l'hgen codes. The system which then has to choose between tile two possibilities wouhl take the second result following the general strategy that cmnpounds with two nominal constituents are rnuch more frequent than those with three elements , those with three more frequent than those with four , etc. ( el. \ [ Jczlorski 1982\ ] , \ [ Mfiller 1q77\ ] ) . Wt ... ... gmentation is finished , the algorithm begins with the semantic interpretation of the coup ( rand be\ [ ore starting transfer. Since , in non lexicalized conlpounds~ tile compourld is generally a member of the syntactic and semantil : t : lass to which its head word belongs , this informa tlon can be passed on to the whole conepoand+ As mentioned carrier , the entry for each constituent or the componnd is extracted from the lexicon. Then the relevant nmrphologit : al , syntactic and semantle information of the last constltnent , the head nmm , is attributed to the compound word as a whole. The following exatnplc Umwellbewe.qung illustrates the procedure : Whereas Umwell has the semantic type physical : ' , tlcwegnng gets the type abstract. Conseqnently , tile eompoand word is attrlhnted the semantic type abstract , too. This passing on of se.. mantle informatlon s can be nsed , for instance , for target lcxeme selection using semantic constraints or for anaphora rest &gt; lotion .</sentence>
				<definiendum id="0">-~ehule</definiendum>
				<definiendum id="1">segmentation</definiendum>
				<definiendum id="2">segmentation</definiendum>
				<definiens id="0">the machine translation system LMT developed by Michael McCord. '~ LMT is a lexicalistlc , source based transfer system</definiens>
			</definition>
</paper>

		<paper id="3160">
			<definition id="0">
				<sentence>With an eye to semi-Thue o1 extended axiomatic systems one could say that a linearly ordered sequence of strings W , C1 , C2 , ... , Cm is a derivatkm of Cm iff ( 1 ) W is a ( faulty ) string ( in the text to be corrected ) and ( 2 ) each Ci follows from the immediately preceding string by one of the productions listed in the lexicon ( Partee et al. , 1990 ) .</sentence>
				<definiendum id="0">C2 , ... , Cm</definiendum>
				<definiendum id="1">W</definiendum>
			</definition>
</paper>

		<paper id="2077">
			<definition id="0">
				<sentence>E.g. , , / ) t ' 'day ' , , _O'~ days ( this is one of the '' few Hebrew nouns where the internal flection is changed with word-formation ) .</sentence>
				<definiendum id="0">_O'~ days</definiendum>
				<definiens id="0">one of the '' few Hebrew nouns where the internal flection is changed with word-formation )</definiens>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>The parsing tree is more formally defined as follows : Definition ( Parsing Tree ) : Given an input string a , the parsing tree on a is recursively defined as following : I. ( a ) is a parsing tree on a , if a is in on a , if a is in a and T k ( l~ .</sentence>
				<definiendum id="0">parsing tree</definiendum>
				<definiendum id="1">T k</definiendum>
				<definiens id="0">follows : Definition ( Parsing Tree ) : Given an input string a , the parsing tree on a is recursively defined as following : I. ( a ) is a parsing tree on a , if a is in on a , if a is in a and</definiens>
			</definition>
			<definition id="1">
				<sentence>If the mput is of a vocabulary E , the P-rules on it can be defined as follows : Definition ( P-rule ) : A P-rule on E is a member of set E ' xE ' xE ' xA , where E ' isE to \ [ nil } and A is the set of actions defined below .</sentence>
				<definiendum id="0">A</definiendum>
				<definiens id="0">a vocabulary E , the P-rules on it can be defined as follows : Definition ( P-rule ) : A P-rule on E is a member of set E ' xE ' xE ' xA , where E ' isE to \ [ nil } and</definiens>
			</definition>
			<definition id="2">
				<sentence>Definition ( Action ) : Actions are defined as functions of type E ~ E , where E is the set of configurations .</sentence>
				<definiendum id="0">Definition ( Action )</definiendum>
				<definiendum id="1">E</definiendum>
				<definiens id="0">Actions are defined as functions of type E ~ E , where</definiens>
				<definiens id="1">the set of configurations</definiens>
			</definition>
			<definition id="3">
				<sentence>Pollack , `` Massive Parallel Prosing : A Strongly h~teractive Model of Natural Language hlterpretation , '' Cognitive Science , vol .</sentence>
				<definiendum id="0">Pollack</definiendum>
				<definiens id="0">A Strongly h~teractive Model of Natural Language hlterpretation , '' Cognitive Science</definiens>
			</definition>
</paper>

		<paper id="3137">
			<definition id="0">
				<sentence>A-models are rceursive structures of infi ) rmation with attitude contexts , each layer el Milch consists of an agent and an attitude he holds toward the deeper hwel information .</sentence>
				<definiendum id="0">A-models</definiendum>
				<definiens id="0">rceursive structures of infi ) rmation with attitude contexts , each layer el Milch consists of an agent and an attitude he holds toward the deeper hwel information</definiens>
			</definition>
			<definition id="1">
				<sentence>OF COLING.92 , NANTES , AUG. 23-28 , 1992 ( Ru=alng ~ ) ( R~lng = } ( 84 ) ~ p4arllti~d In AIB lud~= apm~e Figure 3 : The two attitude ruodels for the readings of $ I. If the speaker provides more detailed information about a clMm ( Y ) in a statement ( X ) Then Y is `` supported '' by `` evidence '' X and becomes more believable As demonstrated by ( A ) ( C ) , semantic association emerges from embedded attitude contexts to calibrate the attitude in higher contexts i.e. , how attitude emergence happens .</sentence>
				<definiendum id="0">clMm</definiendum>
				<definiens id="0">A ) ( C ) , semantic association emerges from embedded attitude contexts to calibrate the attitude in higher contexts i.e. , how attitude emergence happens</definiens>
			</definition>
			<definition id="2">
				<sentence>The basic framework of attitude emergence , which consists of attitude model construction , assimilation , and effects propagation , was first proposed in ( Wu and Lytinen 1991 ) with limited operations .</sentence>
				<definiendum id="0">attitude emergence</definiendum>
				<definiens id="0">consists of attitude model construction , assimilation , and effects propagation</definiens>
			</definition>
			<definition id="3">
				<sentence>BUYER is the computer implementation of attitude emergence and is implentented as a rule-based system which currently has 348 rules organized in 1O problem-solving modules .</sentence>
				<definiendum id="0">BUYER</definiendum>
				<definiens id="0">the computer implementation of attitude emergence and is implentented as a rule-based system which currently has 348 rules organized in 1O problem-solving modules</definiens>
			</definition>
</paper>

		<paper id="2122">
			<definition id="0">
				<sentence>DOMAIN-INDEPENDENT knowledge consists of grammatical rules and lexical definitions .</sentence>
				<definiendum id="0">DOMAIN-INDEPENDENT knowledge</definiendum>
			</definition>
			<definition id="1">
				<sentence>The process of customising an NLI consists in adding the domaindependent knowledge abont a particular application to the domain-independent knowledge that comes with the NLI\ [ 4\ ] .</sentence>
				<definiendum id="0">NLI</definiendum>
			</definition>
			<definition id="2">
				<sentence>Aruoug the knowledge sources that came with the NLI was a Dictionary of 10000 initial English words , and a set of Concepts that provided internal notions of predicates , and set and membership hierarchies .</sentence>
				<definiendum id="0">Aruoug</definiendum>
				<definiens id="0">the knowledge sources that came with the NLI was a Dictionary of 10000 initial English words , and a set of Concepts that provided internal notions of predicates , and set and membership hierarchies</definiens>
			</definition>
			<definition id="3">
				<sentence>There are three types of inference that will conccrn us here : • Coercion • Generalisation and Specification • Ambiguity rezolution COERCIONS depend on tile type information a.¢sociated with the arguments to verbs .</sentence>
				<definiendum id="0">Ambiguity rezolution COERCIONS</definiendum>
			</definition>
			<definition id="4">
				<sentence>A similar inference is supported by tile type organiuation ; if X works for organisation Y , and Y is a suborganisation of organisation Z , then the NLI is supposed to be able to infer that X works for Z. AMBIGUITY resolution consists of iiliing ill underspecified relations .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">a suborganisation of organisation Z , then the NLI is supposed to be able to infer that X works for Z. AMBIGUITY resolution consists of iiliing ill underspecified relations</definiens>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>Such coordinates arc stored in a structured list. , called T-list , which rettects the discourse structure of the preceding text .</sentence>
				<definiendum id="0">T-list</definiendum>
				<definiens id="0">rettects the discourse structure of the preceding text</definiens>
			</definition>
			<definition id="1">
				<sentence>Very often there , tile preceding text consists of only one sentence or tile problem is restricted to thc intra-sentential one presented by temporal conjunctions .</sentence>
				<definiendum id="0">tile preceding text</definiendum>
				<definiens id="0">consists of only one sentence or tile problem is restricted to thc intra-sentential one presented by temporal conjunctions</definiens>
			</definition>
			<definition id="2">
				<sentence>Witidn tile framework of Discourse Representation Theory ( DRT ) ( Kamp ( 1981 ) ) a Discourse Representation Structure ( D1LS ) is a pair &lt; U , K &gt; consisting of a set U of discourse referents ( DRFs ) and a met K of conditions .</sentence>
				<definiendum id="0">Witidn tile framework of Discourse Representation Theory</definiendum>
				<definiendum id="1">Discourse Representation Structure ( D1LS</definiendum>
				<definiens id="0">a pair &lt; U , K &gt; consisting of a set U of discourse referents ( DRFs ) and a met K of conditions</definiens>
			</definition>
			<definition id="3">
				<sentence>So , for instance , the LIL ( ) G-DILS for cominy o\ ] x can be illustrated i~s follows : a ( le~tt ( e ) = x For the \ ] ) ITS to be valid there must exist an embedding function which nmps e onto an event of the model structure such that e satisfies tile conditions as described ill the I ) RS .</sentence>
				<definiendum id="0">LIL ( ) G-DILS</definiendum>
				<definiens id="0">embedding function which nmps e onto an event of the model structure such that e satisfies tile conditions as described ill the I</definiens>
			</definition>
			<definition id="4">
				<sentence>We call the relation between a new event e2 and its reference event el an elaboration , if e2 describes et on a nmre fine-grained level ( which gives rise to the temporal condition of inclusion ( C_ ) between the new event and tile reference event within the representation of the text ) .</sentence>
				<definiendum id="0">fine-grained level</definiendum>
				<definiens id="0">gives rise to the temporal condition of inclusion ( C_ ) between the new event and tile reference event within the representation of the text )</definiens>
			</definition>
			<definition id="5">
				<sentence>Skiplfing such technical details , the simplified syntax of the T-list is the following : T-list : = \ [ \ ] ; \ [ E ( D-list ) JT-List\ ] where E ( D-List ) : list item with E : the discourse referent for the event , and D-List : tile list of the items depending on the event , where D-List : = \ [ \ ] ; \ [ bg ( \ [ E ( \ [ \ ] ) lT'-List\ ] ) lD-List\ ] ; \ [ fl ) ( \ [ E ( D-List ) lW-List\ ] ) \ [ D-List\ ] ; \ [ el ( \ [ E ( D-List ) lT-ListD ID-List\ ] ; Here , of course , embeddings which are given by the dimensions by , fb , elin turn stand for background , flashback and elaboration .</sentence>
				<definiendum id="0">E ( D-List</definiendum>
				<definiens id="0">the discourse referent for the event , and D-List : tile list of the items depending on the event</definiens>
				<definiens id="1">of course , embeddings which are given by the dimensions by , fb , elin turn stand for background , flashback and elaboration</definiens>
			</definition>
			<definition id="6">
				<sentence>T'-List is a T-list where each D-list is the empty list .</sentence>
				<definiendum id="0">T'-List</definiendum>
				<definiens id="0">a T-list where each D-list is the empty list</definiens>
			</definition>
			<definition id="7">
				<sentence>M. ( 1991 ) : Common Sense Entailment : A Modal Theory of Non-monotonic Reasoning .</sentence>
				<definiendum id="0">Entailment</definiendum>
				<definiens id="0">Common Sense</definiens>
			</definition>
</paper>

		<paper id="2121">
			<definition id="0">
				<sentence>SNAP is an experimental massively parallel machine which is dedicated to , but not limited to , the natural language processing ushag semantic networks .</sentence>
				<definiendum id="0">SNAP</definiendum>
				<definiens id="0">an experimental massively parallel machine which is dedicated to , but not limited to , the natural language processing ushag semantic networks</definiens>
			</definition>
			<definition id="1">
				<sentence>SNAP consists of a processor array and an array controller ( Figure 1 ) .</sentence>
				<definiendum id="0">SNAP</definiendum>
			</definition>
			<definition id="2">
				<sentence>The SNAP array consists of 160 processing elements each of which consists of a TMS320C30 DSP chip , local SRAM , etc .</sentence>
				<definiendum id="0">SNAP array</definiendum>
			</definition>
			<definition id="3">
				<sentence>* Comb ( rl , r~ ) : The Comb ( combine ) propagation rule allows the marker to propagate to all rl and r~ links without limitation .</sentence>
				<definiendum id="0">Comb</definiendum>
				<definiens id="0">the marker to propagate to all rl and r~ links without limitation</definiens>
			</definition>
			<definition id="4">
				<sentence>Language Processing Memory-baaed NLP is an idea of viewing NLP as a memory activity .</sentence>
				<definiendum id="0">Language Processing Memory-baaed NLP</definiendum>
				<definiens id="0">an idea of viewing NLP as a memory activity</definiens>
			</definition>
			<definition id="5">
				<sentence>DMSNAP is a SNAP implementation of the ( I ) DMDIALOG speech-to-speech dialogue translation system which is based on , in part , the memory-based approach .</sentence>
				<definiendum id="0">DMSNAP</definiendum>
				<definiens id="0">a SNAP implementation of the ( I ) DMDIALOG speech-to-speech dialogue translation system which is based on , in part , the memory-based approach</definiens>
			</definition>
			<definition id="6">
				<sentence>DMSNAP consists of the nlemory network , syntactic constraint network , and markers to carry out inference .</sentence>
				<definiendum id="0">DMSNAP</definiendum>
				<definiens id="0">consists of the nlemory network , syntactic constraint network , and markers to carry out inference</definiens>
			</definition>
			<definition id="7">
				<sentence>Nodes are connected by a number of different links such as concept ahstraction links ( ISA ) , expression links for both source language and target language ( ENG and JPN ) , Role links ( ROLE ) , constraint links ( CONSTRAINT ) , contextual llnk~ ( CONTEXT ) and others .</sentence>
				<definiendum id="0">ISA</definiendum>
				<definiens id="0">expression links for both source language and target language ( ENG and JPN ) , Role links ( ROLE ) , constraint links ( CONSTRAINT ) , contextual llnk~ ( CONTEXT ) and others</definiens>
			</definition>
			<definition id="8">
				<sentence>A C-MARKER moves from the designated contextual root node to other contextually relevant nodes through contextual links , and ( 6 ) SC-Markers indicate active syntax conattaints , and primed and/or inhibited nodes by currently active syntactic constraints .</sentence>
				<definiendum id="0">C-MARKER</definiendum>
				<definiens id="0">moves from the designated contextual root node to other contextually relevant nodes through contextual links , and ( 6 ) SC-Markers indicate active syntax conattaints , and primed and/or inhibited nodes by currently active syntactic constraints</definiens>
			</definition>
			<definition id="9">
				<sentence>Currently , DMSNAP handles a substantial portion of the ATR conference registration domain ( vocabulary 450 words , 329 sentences ) and sentences from other corpora .</sentence>
				<definiendum id="0">DMSNAP</definiendum>
				<definiens id="0">handles a substantial portion of the ATR conference registration domain ( vocabulary 450 words , 329 sentences</definiens>
			</definition>
			<definition id="10">
				<sentence>DMSNAP completes the parsing in the order of milliseconds .</sentence>
				<definiendum id="0">DMSNAP</definiendum>
				<definiens id="0">completes the parsing in the order of milliseconds</definiens>
			</definition>
			<definition id="11">
				<sentence>Actually , the time-complexity of the sequential classification algorithm is O ( Mn2 ) , and that of the retrieval algorithm is O ( R , ,~JogM ) , where M is a number of concepts , n is an average number of property links per concept , R , .</sentence>
				<definiendum id="0">M</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">a number of concepts</definiens>
			</definition>
			<definition id="12">
				<sentence>c is an average number of roleset relations for one concept .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">an average number of roleset relations for one concept</definiens>
			</definition>
			<definition id="13">
				<sentence>Theoretically , time-complexity of the classification on SNAP is O ( loggo~ , M ) , and that of the parallel retrieval is O ( FinDa. , + 17~ ) , where Fo~t is an average fan-out ( average number of suhconcepts for one concept ) , J~ is an average fan-in ( average number of superconcept for one concept ) , and D.~ .</sentence>
				<definiendum id="0">Fo~t</definiendum>
				<definiendum id="1">J~</definiendum>
				<definiens id="0">an average fan-out ( average number of suhconcepts for one concept</definiens>
			</definition>
			<definition id="14">
				<sentence>One of the central knowledge sources of the KBMT is the ontological hierarchy which encodes abstraction hierarchies of concepts in the given domain , prop~ erty information of each concept , etc .</sentence>
				<definiendum id="0">KBMT</definiendum>
				<definiens id="0">the ontological hierarchy which encodes abstraction hierarchies of concepts in the given domain</definiens>
			</definition>
			<definition id="15">
				<sentence>al. , 1990\ ] reports that access to large frame systems on serial computers have a time-complexity ofO ( M x B 't ) where M is the number of conjuncts in the query , B is the average branching factor in the network , and d is the deptb of the network .</sentence>
				<definiendum id="0">M</definiendum>
				<definiendum id="1">B</definiendum>
				<definiens id="0">the number of conjuncts in the query</definiens>
				<definiens id="1">the average branching factor in the network</definiens>
			</definition>
			<definition id="16">
				<sentence>Next , we have applied the SNAP architecture for a new classification-based parsing model ltere , SNAP iB used to search tile MSS to test tile unifiability of the two feature graphs .</sentence>
				<definiendum id="0">SNAP architecture</definiendum>
				<definiendum id="1">SNAP iB</definiendum>
				<definiens id="0">used to search tile MSS to test tile unifiability of the two feature graphs</definiens>
			</definition>
</paper>

		<paper id="3134">
			<definition id="0">
				<sentence>High risk agents make this assumption , while low risk agents do not , making them precede new concepts in the dialogam with subdialogues which establish certain knowledge of the partner 's knowledge such as direct questions about the status of the concepts .</sentence>
				<definiendum id="0">High risk agents</definiendum>
			</definition>
			<definition id="1">
				<sentence>Utterance Reallsation Parameters Thrum parameters affect the way in which each ut terance in the given discourse structure is realised or understood .</sentence>
				<definiendum id="0">Utterance Reallsation Parameters Thrum parameters</definiendum>
				<definiens id="0">affect the way in which each ut terance in the given discourse structure is realised or understood</definiens>
			</definition>
</paper>

		<paper id="3145">
			<definition id="0">
				<sentence>This package can be easily embedded into a natural language parser ; hooks for accessing the morphological database from a parser are provided for both Lucid Common Lisp and C. This morphological database is currently being used in a graphical workbench ( XTAG ) for the development of tree-adjoining grammars and their parsers ( Paroubek et al. , 1992 ) .</sentence>
				<definiendum id="0">XTAG</definiendum>
			</definition>
			<definition id="1">
				<sentence>Each continuation class specifies the inflectional rules which can apply to the given lexical item .</sentence>
				<definiendum id="0">continuation class</definiendum>
				<definiens id="0">specifies the inflectional rules which can apply to the given lexical item</definiens>
			</definition>
			<definition id="2">
				<sentence>Rootl `` V ( taach ) PPART STR '' teach V-Root7 `` V ( teach ) '' Examples of runs follow : recognizer &gt; &gt; admires admireTs V ( admire ) 3SG PRES recognizer &gt; &gt; admired admire+ed V ( admire ) PAST WK admire-Fed V ( admire ) PPART WK recognizeC ; ~admiring adrnire+ing V ( admire ) PROG recognizer &gt; admire admire V ( admire ) INF recognizer &gt; &gt; dyed dyeTed V ( dye ) PAST WK dye+ed V ( dye ) PPART WK recognizer &gt; &gt; dyes dye+s N ( dye ) PL dyeTs V ( dye ) 3SG PRES recognlzer &gt; &gt; teaches teach+s V ( teach ) 3SG PRES recognizer &gt; &gt; teached *** NONE *** recoguizer &gt; &gt; taught taught V ( teach ) PAST STR taught V ( teach ) PPART STR recognizer : ; ~tangoed tango+ed V ( tango ) PAST WK tangoTed V ( tango ) PPART WK recognizer~tangoing tango+ing V ( tango ) PROG recognizer~tangoes tangoes V ( tango ) 3SG PRES The attributes WE ( for `` weak '' ) and STR ( for `` strong '' ) mark whether the verb forms its past tense regularly or irregularly , respectively .</sentence>
				<definiendum id="0">PPART STR</definiendum>
				<definiendum id="1">admire ) PAST WK admire-Fed V ( admire ) PPART WK recognizeC</definiendum>
				<definiens id="0">dye ) PAST WK dye+ed V ( dye ) PPART WK recognizer &gt; &gt; dyes dye+s N ( dye ) PL dyeTs V ( dye ) 3SG PRES recognlzer &gt; &gt; teaches teach+s V ( teach ) 3SG PRES recognizer &gt; &gt; teached *** NONE *** recoguizer &gt; &gt; taught taught V ( teach ) PAST STR taught V ( teach ) PPART STR recognizer : ; ~tangoed tango+ed V ( tango ) PAST WK tangoTed V ( tango ) PPART WK recognizer~tangoing tango+ing V ( tango ) PROG recognizer~tangoes tangoes V ( tango</definiens>
				<definiens id="1">strong '' ) mark whether the verb forms its past tense regularly or irregularly , respectively</definiens>
			</definition>
			<definition id="3">
				<sentence>KIMMO : A two-level morphological analyzer .</sentence>
				<definiendum id="0">KIMMO</definiendum>
			</definition>
			<definition id="4">
				<sentence>Two-level morphology : a general computational model for word-form recognition and production .</sentence>
				<definiendum id="0">Two-level morphology</definiendum>
				<definiens id="0">a general computational model for word-form recognition and production</definiens>
			</definition>
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>E.g. 61~vant ( 6levant ) , ~ssai ( essai ) , 6tages ( otages ) • Accent deletion .</sentence>
				<definiendum id="0">E.g. 61~vant</definiendum>
			</definition>
			<definition id="1">
				<sentence>E.g. compauys ( company 's ) Knowing the configuration of a standard keyboard and the way people type suggests several plausible reasons for the inserfiou of superfluous characters .</sentence>
				<definiendum id="0">E.g. compauys</definiendum>
				<definiens id="0">the configuration of a standard keyboard and the way people type suggests several plausible reasons for the inserfiou of superfluous characters</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>A ( simplified ) representation for the morphologically processed but syntactically unanalyzed sentence as a regular expression could be roughly as follows : 0 @ the DEF ART \ [ 8 I @ / I 8 &lt; I 8 &gt; \ ] \ [ \ [ program N NOM SG \ [ eSUBJ \ [ 8OBJ I 8PREDC } \ ] 1 \ [ Drogram V PRES NON-SG3 8FINV 8MAINV\ ] i \ [ program V INF\ ] \ ] \ [ 8 I 8/ I 8 &lt; I @ &gt; \ ] \ [ \ [ run V PRES SG3 @ FINV 8MAINV\ ] I \ [ run N NOM PL \ [ eSUBJ I 8OBJ I 8PREDC\ ] \ ] \ ] 88 Here 8S represents a sentence boundary , @ a word boundary , 8/ an ordinary clause houndamy , @ &lt; a begi , Lrflng of a center embedded clause , and @ &gt; the end of such an embedding .</sentence>
				<definiendum id="0">DEF ART</definiendum>
				<definiens id="0">a sentence boundary , @ a word boundary , 8/ an ordinary clause houndamy</definiens>
			</definition>
			<definition id="1">
				<sentence>The whole finite-state grammar consists of a set of rules which constrain the possible choices of word Interpretations , tags and boundaries to ACRES DE COLING-92 , NAN 'D !</sentence>
				<definiendum id="0">finite-state grammar</definiendum>
				<definiens id="0">consists of a set of rules which constrain the possible choices of word Interpretations , tags and boundaries to ACRES DE COLING-92</definiens>
			</definition>
			<definition id="2">
				<sentence>P CENTRAL ART SG @ DN &gt; @ arrow N NOM SG @ OBJ 8 @ fly N NOM PL 80BJ @ like PREP 8 &lt; NOM 8 an &lt; Indef &gt; DET CENTRAL ART SG 8DN &gt; 8 arrow N NOM SG 8 &lt; P 88 fly N NOM PL @ OBJ 8 like PREP @ ADVL 8 an &lt; Indef &gt; DET CENTRAL ART SG 8DN &gt; @ arrow N NOM SG 8 &lt; P @ 8 The finite-state grammar for English consists of some 200 rules dedicated for several areas of the grammar : • Internal structure of nominal and non-finite verbal phrases. The structure is described as head-modifier relations , including determiners , premodiflers and postmodiflers. • CoordinaUon at various levels of the grammar. • Surface-syntactlc functions of nominal phrases. The structure of noun phrases is described using two approaches together. A coarse structure is fixed with the mechanism of deflnIUons. It would not be feasible to use that mechanism alone ( because it would lead to a context-free descripUon ) . The deflniUons are supplemented with ordinary finite-state rules which enforce further restrictions. 1 5 9 PROC. OF COLING-92 , NANTES , AUO. 23-28 , 1992 Between the level of the nominal phrase and the finite clause , there is an Intermediary level , that of non-finite t~nsmtct/ons ( see Quirk &amp; el. 1985 ) . These constructions resemble noun phrases when seen as parts of the surrounding clause because they act eg. as subjects , objects , preposition complements , etc. , postmodifiers , or adverbials , eg. : ( Wa~ng home } was wearisome. • She wants ( to come } 1. She was fond of ( singing in the dark } . The dog ( barking in the corridor } was irritable. ( 'fired by her journey } , she fell asleep. Internally , non-finite constructions are like finite clauses because the main verb of a nonfinite construction can have subjects , objects , adverbials etc. of Its own. Both finite and non-finite constructions have a verbal skeleton , which in a finite construction starts with aJO~e verb and ends with the first main verb. The finite verbal skeletons In the following examples are underlined : Shs sinas. Will she ~ ? She would no t have been singinq unless .. A non-finite verbal skeleton starts with certain kinds of non-finite verb ( to+infinitive. present participle , past participle , non-finite auxiliary ) and ends with the first main verb to the right : It is easy lode it. ~red by her journey , she went into her room. They knew it all , ~ there before. Non-finite verb chains do not contain centerembedded verbs , whereas a non-finite construction can be center-embedded within a finite verb chain only ff it is ( a part off a nominal phrase : Can \ [ shooting hunters } be dangerous ? Can men ( shooting hunters } be dangerous ? The use of syntactic tags instead of a hierarchical tree-structure forces us to a very fiat description of sentences. This might result in problems when describing clauses with nonfinite constructions with a small set of tags , eg. : The boy \ [ kicking @ MAINV\ ] the \ [ ball @ OBJ\ ] \ [ saw @ MAINV\ ] the \ [ cow @ OBJ\ ] . A useful concept in clause-level syntax is the uniqueness principle. We wlsh to say , for Instance , that In a clause , there is at most one l. The~ is another way to interpret this sentence without any non-finite constructions by including 'to come ' in the finite verb chain. We have adopted the current interprctation in order to achieve certaing linguistle generallzaUona. ( possibly co-ordinated ) subject , object , or predicate complement. Uniqueness holds for the finite clause , and each non-finite construction separately , and this will be very difficult to formulate , ff we use same tags for both domains ( as in the above example ) . The syntactic tags as given In the finite-state version of ENGTWOL capitalize heavily on nonfinite constructions in order to overcome this problem : The boy \ [ kicking @ MAINV/-F\ ] the ( ball @ OBJ/-F\ ] \ [ saw @ MAINV\ ] the \ [ cow @ OBJ\ ] . Here , the object in the non-finite construction is furnished with a label different from the corresponding label used in the finite construction , so there is no risk of confusion between the two levels. The duplication of certain labels for certain categories Increases the amount of ambiguity , but , on the other hand , the new ambiguity seems to be of a fairly controllable type. The description of non-finite constructions boils down to two subtasks. One is to express constraints on the Internal structure of non-finite constructions ; the other , the control on their distribution. In terms of verb chain and constituent structure , non-finite constructions resemble finite constructions. Their main difference is that word order in non-finite constructions is much more rigid. We proceed with some examples of rules describing non-finite constructions. An infinitive acting as main verb in a non-finite construction is preceded by to acting as an Infinitive marker or by a subject of a non-finite phrase or by a co-ordinated infinitive. So we wish. for instance , the following utterances to be accepted : He wants \ [ to @ INFMARK &gt; \ ] \ [ go INF @ -FMAINV/-F\ ] .</sentence>
				<definiendum id="0">dog</definiendum>
				<definiendum id="1">eg.</definiendum>
				<definiens id="0">an &lt; Indef &gt; DET CENTRAL ART SG 8DN &gt; 8 arrow N NOM SG 8 &lt; P 88 fly N NOM PL @ OBJ 8 like PREP @ ADVL 8 an &lt; Indef &gt; DET CENTRAL ART SG 8DN &gt; @ arrow N NOM SG 8 &lt; P @ 8 The finite-state grammar for English consists of some 200 rules dedicated for several areas of the grammar : • Internal structure of nominal and non-finite verbal phrases. The structure is described as head-modifier relations , including determiners</definiens>
				<definiens id="1">a context-free descripUon ) . The deflniUons are supplemented with ordinary finite-state rules which enforce further restrictions. 1 5 9 PROC. OF COLING-92 , NANTES , AUO. 23-28 , 1992 Between the level of the nominal phrase and the finite clause</definiens>
				<definiens id="2">non-finite t~nsmtct/ons ( see Quirk &amp; el. 1985 ) . These constructions resemble noun phrases when seen as parts of the surrounding clause because they act eg. as subjects , objects , preposition complements , etc. , postmodifiers , or adverbials</definiens>
				<definiens id="3">A non-finite verbal skeleton starts with certain kinds of non-finite verb ( to+infinitive. present participle , past participle , non-finite auxiliary ) and ends with the first main verb to the right : It is easy lode it. ~red by her journey , she went into her room. They knew it all , ~ there before. Non-finite verb chains do not contain centerembedded verbs</definiens>
				<definiens id="4">a part off a nominal phrase : Can \ [ shooting hunters</definiens>
				<definiens id="5">The boy \ [ kicking @ MAINV\ ] the \ [ ball @ OBJ\ ] \ [ saw @ MAINV\ ] the \ [ cow @ OBJ\ ] . A useful concept in clause-level syntax is the uniqueness principle. We wlsh to say</definiens>
				<definiens id="6">another way to interpret this sentence without any non-finite constructions by including 'to come ' in the finite verb chain. We have adopted the current interprctation in order to achieve certaing linguistle generallzaUona. ( possibly co-ordinated ) subject , object , or predicate complement. Uniqueness holds for the finite clause</definiens>
			</definition>
			<definition id="3">
				<sentence>Voutilainen is a member of the SIMPR project at RUCL , sponsored by the Finnish Technology Development Center ( TEKES ) .</sentence>
				<definiendum id="0">Voutilainen</definiendum>
			</definition>
			<definition id="4">
				<sentence>l , Constraint Grammar : A Language-Independent System for Parsing Runnlng TexL L. Karttunen , K.Koskermiemi , IL Kaplan 1987 .</sentence>
				<definiendum id="0">Constraint Grammar</definiendum>
				<definiens id="0">A Language-Independent System</definiens>
			</definition>
			<definition id="5">
				<sentence>Two-level Morphology : A General Computational Model for Word-Form Recognition and Production .</sentence>
				<definiendum id="0">Two-level Morphology</definiendum>
			</definition>
</paper>

		<paper id="2084">
			<definition id="0">
				<sentence>1992 The target lexicon conta : imq the fol\ ] owitN information about 'the valency of a verb ( or its complementation ) , grouped in an entry as a comp\ ] ementation t~radigm : SUBCATFC~ ) RIZATION LIST ( SC ) gives syntactic and merphologica\ ] categories for every del~endent , i .</sentence>
				<definiendum id="0">SC</definiendum>
				<definiens id="0">imq the fol\ ] owitN information about 'the valency of a verb</definiens>
			</definition>
</paper>

		<paper id="3166">
			<definition id="0">
				<sentence>This indexing process works on a single bibliographical record and combines both linguistic methods and , artificial intelligence ( keywords generation ) .</sentence>
				<definiendum id="0">indexing process</definiendum>
				<definiens id="0">works on a single bibliographical record and combines both linguistic methods and , artificial intelligence ( keywords generation )</definiens>
			</definition>
			<definition id="1">
				<sentence>2 L'indexation dans le dispositif d'analyse infom~trique Nous avons ddvelopp6 un ensemble d'outils permettant de passer d'un ensemble de textes non index6s ~ une structure hypertexte via un processus d'indexation automatique des textes ct des mEcanismes de clusterisation opdrant un regroupement des mots-clds en classes ( mEthode des roots associ6s utili~nt I'algorithme du simple lien \ [ 16\ ] ) .</sentence>
				<definiendum id="0">mEthode</definiendum>
				<definiens id="0">des roots associ6s utili~nt I'algorithme du simple lien \ [ 16\ ] )</definiens>
			</definition>
</paper>

		<paper id="3157">
</paper>

		<paper id="4191">
			<definition id="0">
				<sentence>To meet these problems , ANA ( Auomatic Natural Acquisition ) had been developed .</sentence>
				<definiendum id="0">ANA</definiendum>
				<definiens id="0">Auomatic Natural Acquisition ) had been developed</definiens>
			</definition>
</paper>

		<paper id="2119">
			<definition id="0">
				<sentence>NArCrEs , AUG. 23-28 , 1992 dialogue act label : rcq-fnr-spellmg dialogue act owner : system structural preconditions : S = \ [ ... \ [ E , i ( s ) , r ( u ) , \ [ E Ev , i ( s ) , contest ( u ) Jl ... J &amp; E1Ev is a currently open exchange structural effects : S = l..</sentence>
				<definiendum id="0">E1Ev</definiendum>
				<definiens id="0">rcq-fnr-spellmg dialogue act owner : system structural preconditions : S = \ [ ... \ [ E , i ( s ) , r ( u )</definiens>
			</definition>
			<definition id="1">
				<sentence>E denotes an exchange , made up here of one initiative ( i ( s ) ) , one reaction ( r ( s ) ) and one evaluative exchange Ev E t .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">an exchange</definiens>
			</definition>
			<definition id="2">
				<sentence>In the case of the user 's contest ( continuation 1I ) , the system 's evaluation becomes the initiative of an evaluative exchange ( E E ' ) and E2 is postponed .</sentence>
				<definiendum id="0">continuation 1I</definiendum>
				<definiendum id="1">E2</definiendum>
				<definiens id="0">the initiative of an evaluative exchange ( E E ' )</definiens>
			</definition>
			<definition id="3">
				<sentence>U2 is a complete topic shift ( not related to the problem formulation of the oneway -the current transaction- ) .</sentence>
				<definiendum id="0">U2</definiendum>
				<definiens id="0">a complete topic shift</definiens>
			</definition>
</paper>

		<paper id="4189">
			<definition id="0">
				<sentence>Z ( no semantic rlnltriction ) /2 T , W , X , Y~2,4,6,7 C ( abstract ) ( concrete ) LW Q , Y , S ( inanimate ) ( animate ) S , E,1,2,5 L , E , ~,7 G,7 P , V A , O , V It , O , X , l ( Iolid ) ( liquid ) ( gas ) ( plant ) ( animal ) ( human ) J N F , ,R D , K M , K F , R ( movable ( m~ movable ( msmull ( aalmal ( human ( human salkl ) solid ) femnle ) male ) auk ) female ) Figure 2 : Revised Hierarchy of LDOCE Semantic Codes Based on this study of the semantic codes used in LDOCE , three inlplelnentations of a partial genus sense selection algorittun ( partial becanse at this time we are only considehng the contribution made by the semantic code comparison to sense selection ) were found to be possible .</sentence>
				<definiendum id="0">Z</definiendum>
				<definiens id="0">aalmal ( human ( human salkl ) solid ) femnle ) male ) auk</definiens>
				<definiens id="1">Revised Hierarchy of LDOCE Semantic Codes Based on this study of the semantic codes used in LDOCE</definiens>
			</definition>
</paper>

		<paper id="3158">
			<definition id="0">
				<sentence>Since 1987 we have been implementing Meaning-Text language models ( MTMs ) \ [ 6 , 7\ ] for the task of realizing sentences from semantic specifications that are output by a text planner .</sentence>
				<definiendum id="0">Meaning-Text language models</definiendum>
				<definiens id="0">semantic specifications that are output by a text planner</definiens>
			</definition>
			<definition id="1">
				<sentence>We have therefore chosen to use a conceptual interlingua ( the output of the text planning process ) in order to derive separate semantic net representations of the sentences in each language .</sentence>
				<definiendum id="0">conceptual interlingua</definiendum>
				<definiens id="0">the output of the text planning process ) in order to derive separate semantic net representations of the sentences in each language</definiens>
			</definition>
			<definition id="2">
				<sentence>Grammatical realization in the LFS system is the process by which the semantic nets produced by the planner for the incipient sentences are converted into surface sentences of each language .</sentence>
				<definiendum id="0">Grammatical realization</definiendum>
				<definiens id="0">the process by which the semantic nets produced by the planner for the incipient sentences are converted into surface sentences of each language</definiens>
			</definition>
			<definition id="3">
				<sentence>The second operation consists of `` replacing '' single or complex ( configurations of ) meauing-bearing nodes in the semantic network by actual lexemes of the language , and replacing semantic features on those nodes by grammatical features which will be attached to the nodes of the future deep-syntactic tree .</sentence>
				<definiendum id="0">second operation</definiendum>
				<definiens id="0">consists of `` replacing '' single or complex ( configurations of ) meauing-bearing nodes in the semantic network by actual lexemes of the language , and replacing semantic features on those nodes by grammatical features which will be attached to the nodes of the future deep-syntactic tree</definiens>
			</definition>
			<definition id="4">
				<sentence>A final operation produces actual text by computing the final ( graphical ) wordforms based on the morphological features attached to lexemes in MorphR .</sentence>
				<definiendum id="0">final operation</definiendum>
				<definiens id="0">produces actual text by computing the final ( graphical ) wordforms based on the morphological features attached to lexemes in MorphR</definiens>
			</definition>
			<definition id="5">
				<sentence>A general paraphrase rule states that a verbal lexeme ( here , decrease ) , can be paraphrased by a syntactic construction where the new verb ( i.e. , show ) is the value of Oper I operating on the nominalizatiou ( i.e. , S0 ) of the old verb .</sentence>
				<definiendum id="0">general paraphrase rule</definiendum>
				<definiens id="0">a verbal lexeme ( here , decrease ) , can be paraphrased by a syntactic construction where the new verb</definiens>
				<definiens id="1">the value of Oper I operating on the nominalizatiou</definiens>
			</definition>
			<definition id="6">
				<sentence>In addition to the above `` well-known '' lexical functions , our domain also makes use of Syn ( synonym ) , AntiMagn ( diminutive modifier ) , Locin ( locative preposition ) , Adv 1 ( locative adverb ) and several more `` exotic '' ones .</sentence>
				<definiendum id="0">AntiMagn</definiendum>
				<definiens id="0">diminutive modifier ) , Locin ( locative preposition</definiens>
			</definition>
</paper>

		<paper id="4203">
			<definition id="0">
				<sentence>Translation aM is another kind of machine translation : the user is the agent of translation , while the computer provides him or her with the helpfifl tools , e.g. , quick-retrieval electronic dictionaries .</sentence>
				<definiendum id="0">Translation aM</definiendum>
				<definiens id="0">the agent of translation , while the computer provides him or her with the helpfifl tools , e.g. , quick-retrieval electronic dictionaries</definiens>
			</definition>
			<definition id="1">
				<sentence>We define it as follows : s ( i , j ) = 0 ifi=ovj=o s ( il , j l ) +m ( i , j ) , ) max s ( i l , j ) , s ( i , j 1 ) if ( l _ &lt; i &lt; x ) A ( 1 &lt; _j &lt; y ) 1 if a~ = b 3 m { i , j ) = 0 ifa , ~bj This measure often produces the undesirable resuits , because we ignore continuation of matching characters. For example , consider the following strings : A = I '' I~R4~'¢70 ( solve the problem ) f~ = t~a~ m 5 , ~j~1ce~ Ltco ( He solved the problem yesterday. ) ( determine tile method for solving the problem ) We want to be S ( A,13 ) &gt; ,9 ( A , F¢ ' ) , but the above measure produces ,5 ' ( A , B ) &lt; , S ' ( A , B~ ) .</sentence>
				<definiendum id="0">S '</definiendum>
				<definiens id="0">the problem ) f~ = t~a~ m 5 , ~j~1ce~ Ltco ( He solved the problem yesterday. ) ( determine tile method for solving the problem ) We want to be S ( A,13 ) &gt; ,9 ( A , F¢ ' ) , but the above measure produces ,5 ' ( A , B ) &lt; ,</definiens>
			</definition>
			<definition id="2">
				<sentence>s ( i -1 , j 1 ) + min ( cm ( i , j ) , W ) max s ( il , j ) , s ( i , j 1 ) if ( l &lt; i &lt; ~ ) A ( 1 _ &lt; j _ &lt; y ) ~ , ,~ ( / , j ) = 0 ifi=OVj=O em ( i l , j 1 ) + m ( i , j ) if ( 1 _ &lt; i _ &lt; x ) ^ ( ~ _ &lt; j &lt; : / ) 1 if ai = bj m ( i , j ) = 0 ifai~bj This is the similarity score that we use , where W is a parameter that determines the maximum value of the bonus for tile continuons matching characters. When 14 '' = 1 , this definition is the same with tile previous definition. Table l shows ,5 ' ( A , B ) and S ( A , B ' ) with varying vahws of W. l_lsually we use W = 4. 4 4'l'his value was detemni , ted empirically. II may be ex plained ~-s follows , '\ [ 'he average character length of a Japanese word is abottt two , and we frel that the COlllillll ( lllS lll~.tChillg of two w~ ) rds is Ihe Mrollg match. AC1T.S DE COLING-92 , NANTES , 23-28 ^O£rC 1992 1 2 6 O PROC. OF COL1NG-92 , NANTES , AUG. 23-28 , 1992 Table h Scores vs. W W \ [ 'l '2 3 4 5 S ( A , I\ ] ) \ [ 5 : 9 : 12 14 15 5 ' ( A , B ' ) 7 9 9 9 9 Table 2 : Translation I ) atah , '~se l ID .lnp~umse English l ~ &lt; '95 '' 69 several 2 ~ '' 3 '' C ~ every tlmc 3 ~aO~ some ( lity 4 ~ O ' ) } yeuterd~ty Tahle 3 : Character Index Ch. lI ) 's Ch. II ) 's ~ 1 , 2 , 3 O 1 , 2 , 3 fl 4 -¢ 2 7~ ~ 1 , 3 a ) 1 , 4 4 ~ 2 &lt; 1 At the be'st n/arch retrieval , we use the acceleration method using the character index. '~ The character index is tile tahle of every character with ll ) 's of examples in which the character is appeared. Table 2 shows all exatnple of translation database and Table 3 shows the character index of it. In the first stage of the retrieval , the character index is used for the pre-seleetion of tile examples. Figore 2 illustrates the pre-selection process : it is appeared in the input string. score , I'SS , wtfich can he ohtained by counting tile nurnher of the example ll ) 's in the records. It is the number of matching characters between the input string and tilt ! example ignoring the character order constraint. pre-selection score , where N is the parameter and we usually use N = 200. s In the second stage of the retrieval , the similarity scores of life-selected examples arc eomptlte ( I , and the examples are ordered by the score. Above mentioned retrieval mechanism hP-~ been implemented in CTM , a Japanese-English translation 5We C &amp; llllOt COllll ) llte tile similarity re : ore of every exltnlt ) le ill tile tlatabm~e , because the C ( ) llll ) lll~iltioll Ileeds almut 5 llliltisecond between the ' ItVel'age illl } ll ( siring ( lO &lt; 'll &amp; racter~ ) &amp; lid the average extmtDle ( 5 ( \ ] cha~'actet~ ) ( m SparcSlalion 2 , eThis value wa.~ determined empirically. , \ [ L~k~l~ 4~ \ [ CI ... ... ter Index\ ] Ch. II ) s ID PSS '' Jap. I , 2 , 3 `` &amp; '2 3 ~ ~ :9 `` 72 ~o Figure 2 : Prc-seleetiml using Character Index 'l'ranslati ... . _ ~ _ , ~ I ) it t al ) i~.~e I I I\ ] M'I'C ( Clienl ... .. NE ... .. ) ~\ ] l '' igure 3 : The CTM system aid system CTM is written by C and runs on Sun Workstations. Pigure 3 shows tile contlguration of CTM : it consists of three programs. mkdb The program to create the character index t¥om tile translation database. CTM server The main program , which retrieves the hest matched examples with the given input. 7 MTC ~ The client program on NF.macs ( Nihongo ( Japanese ) GNU Emacs ) , which interacts the C'I 'M server via Ethernet. The translation datah , -Lse of ( YI 'M is text tiles , in which a Japanese text string and an English text string appear one .after the other. These files call be made from J al ) anese text files and the correspondent English text tiles hy nsing the alignment progratn \ [ 1\ ] semi-automatically. We have made the translation datahase from several sources : Tahle 4 shows ollr translation databases. We show here C , TM retrieval cxaml ) les with the following features : phra.qal expression , long-distance ( lependency , idiom , synonynl , and semantic ambiguity. Figure 4 shows a retriewd exanlple of phrasal expression `` ~ &lt; `` Dh~C ) ~J~ : ) : :~¢ , ~'~.~J'Yo ( consider from several points of view ) '' . Although there is no exact matched expression in the datahase , CTM can retrieve helpful examples for us to translate it. rThe CTM ~erver ha~ ~tller facilities : tile charactelq ) aaed exact lllatdl retrieval fiw Jap~tllese texts , and tire word-bmsed hem or exert nl &amp; tch retrieval f~n ' English texts. s M'I ' ( Y is named t'r~\ ] n tile lanal~pne phra-~ ' , `` Molt. `` 1 ~ukatte C , hondai ' , whi , 'h III~RII~ `` 11~ il lllOr~ and Hlore '' . ACRES DE COLING-92 , NANTES , 23-28 AO~r 1992 l 2 6 1 PREC. OF COLING-92 , NANTES , AUG. 23-28 , 1992 Table 4 : The ~TM Translation Databases Name Direction ll.eco rds K Byte Sonrce ( s ) ScienceYYMM 1 ' ; ~J 11,115 3,175 Scientilic American &amp; its Japanese translation ( Nikkei Science ) MLI E~ , I 2,655 458 Chap. I 4 in Machine Learning \ [ 3\ ] , tz its Japanese translation .11 , ~ .I~E 4,230 139 ~ ; ntry words on \ [ 4\ ] MTE J~E 3,938 379 Test examph ! s on \ [ 2\ ] EX J~E 6,624 595 Translation examples co\ ] leered by Oikawa TJ , I~E 1 , d67 259 The column , Tensei-Jingn , on Asahi Newspaper K\ ] ) .\ ] ~l '' , 38,190 2,729 F , xamples on \ [ 7\ ] Total 67,619 7,733 C'/ 'M ( AI , ) &gt; ~ ' { -9 h'o ) ~ , t.~ ( 7 , ~ ' ¢9 ~-~g , -J7a Score = 28 , 1311 = Science8710 , lI ) = 598 , I '' ile = 03 .</sentence>
				<definiendum id="0">W</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">YI 'M</definiendum>
				<definiens id="0">a parameter that determines the maximum value of the bonus for tile continuons matching characters. When 14 '' = 1</definiens>
				<definiens id="1">be ex plained ~-s follows , '\ [ 'he average character length of a Japanese word is abottt two</definiens>
				<definiens id="2">the acceleration method using the character index. '~ The character index is tile tahle of every character with ll ) 's of examples in which the character is appeared. Table 2 shows all exatnple of translation database</definiens>
				<definiens id="3">the number of matching characters between the input string and tilt ! example ignoring the character order constraint. pre-selection score , where</definiens>
				<definiens id="4">the parameter</definiens>
				<definiens id="5">The CTM system aid system CTM is written by C and runs on Sun Workstations. Pigure 3 shows tile contlguration of CTM : it consists of three programs. mkdb The program to create the character index t¥om tile translation database. CTM server The main program</definiens>
				<definiens id="6">text tiles , in which a Japanese text string and an English text string appear one .after the other. These files call be made from J al ) anese text files and the correspondent English text tiles hy nsing the alignment progratn \ [ 1\ ] semi-automatically. We have made the translation datahase from several sources</definiens>
				<definiens id="7">tile charactelq ) aaed exact lllatdl retrieval fiw Jap~tllese texts , and tire word-bmsed hem or exert nl &amp; tch retrieval f~n ' English texts. s M'I '</definiens>
				<definiens id="8">The ~TM Translation Databases Name Direction ll.eco rds K Byte Sonrce ( s ) ScienceYYMM 1 '</definiens>
			</definition>
			<definition id="3">
				<sentence>time ( I , k , N ) = I x ( D0 x k+2/3 x N ) where I is tile length of tile input string , k ( mega byte ) is the database size , and N is tile we-selection parameter .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">I x ( D0 x k+2/3 x N ) where I is tile length of tile input string</definiens>
				<definiens id="1">tile we-selection parameter</definiens>
			</definition>
</paper>

		<paper id="3130">
			<definition id="0">
				<sentence>ABEX ( ABstraction EXtraction system ) utilizes the LIS outpttt of the semantic engine .</sentence>
				<definiendum id="0">ABEX ( ABstraction EXtraction system )</definiendum>
			</definition>
			<definition id="1">
				<sentence>The LIS form expresses the inlormation structure that pernlits commnnication between individuals .</sentence>
				<definiendum id="0">LIS form</definiendum>
				<definiens id="0">expresses the inlormation structure that pernlits commnnication between individuals</definiens>
			</definition>
			<definition id="2">
				<sentence>The semantic engine infers the semantic meaning of words or phrases from the system default and user registrations held in the dictionary .</sentence>
				<definiendum id="0">semantic engine</definiendum>
				<definiens id="0">infers the semantic meaning of words or phrases from the system default and user registrations held in the dictionary</definiens>
			</definition>
			<definition id="3">
				<sentence>Finally , ABEX provides a abstraction .</sentence>
				<definiendum id="0">ABEX</definiendum>
				<definiens id="0">provides a abstraction</definiens>
			</definition>
			<definition id="4">
				<sentence>First , ABEX utilizes the selectivity of the semantic engine according to the domain and the event .</sentence>
				<definiendum id="0">ABEX</definiendum>
				<definiens id="0">utilizes the selectivity of the semantic engine according to the domain and the event</definiens>
			</definition>
</paper>

		<paper id="3135">
			<definition id="0">
				<sentence>Ot ; C ( ) I.ING-92 , NAN'I'ES , AUG. 23-28 , 1992 mais toulours au moins cor ( ~f~rent a. sujet En reconnaissant d'emblee ces phrases et teur verbe comma Vsup , on pourra r/esoudre automatiquement I'anaphore Pour cela nous proposons de reperer les autres phrases , pu~s de considerer les phrases non reconnues comme ~tant de carte categorie 33 Phrases construites autour de verbes ordinalres Elles s'articulent autour d'un verbe ~ un ou deux arguments , rNement predicatif de la phrase puisqu'il definit la structure des arguments II est determin # , par : le nombre d'arguments I'articulation syntaxique de ces arguments les traits semantiques de ces a { guments Dans la st\ [ ucture \ [ NO V Nl-poss\ ] que ron ne peut poursuivre avec \ [ &amp; /de N2\ ] , la coreference est obligatoire aun autre nero que le sujet NO ( du discours anterieur ) Luc approuve son choix Luc apptouve le choix de L # a Par centre , si une phrase a deux arguments et qu'elle peut etre completee par un troJsiCme , la presence ou non de ce dernier fait varier la coref~rence , ou tout au moins la preference entre lee antecedents possibles : Luc i avoue son i depit .</sentence>
				<definiendum id="0">rNement</definiendum>
				<definiens id="0">V Nl-poss\ ] que ron ne peut poursuivre avec \ [ &amp; /de N2\ ] , la coreference est obligatoire aun autre nero que le sujet NO ( du discours anterieur</definiens>
			</definition>
</paper>

		<paper id="3152">
			<definition id="0">
				<sentence>A word such as the geological term magma typically has a conceptual meaning only , another one such as bloody ( as in 'you bloody fool ' ) typically combines collocational meaning ( intensification ) with stylistic meaning aspects ( very informal ) , whereas the same word , bloody , e.g. in a sentence like 'I got my bloody foot caught in the bloody chair ' ( example taken from LDOCE ) mainly gets a discursive , a contextual , meaning ( functioning as an ( emotional ) stopword ) .</sentence>
				<definiendum id="0">meaning</definiendum>
				<definiens id="0">collocational meaning ( intensification ) with stylistic meaning aspects ( very informal ) , whereas the same word , bloody</definiens>
			</definition>
</paper>

		<paper id="2106">
			<definition id="0">
				<sentence>'lds and lingl , islic des ( 'riplions for thetil as tWO input donlains , it can be seen as a language arquisilion system without language specific constraints .</sentence>
				<definiendum id="0">islic des</definiendum>
				<definiens id="0">a language arquisilion system without language specific constraints</definiens>
			</definition>
			<definition id="1">
				<sentence>In this paper , we describe lhe nm &lt; 'hine h , arnin~ syslenl Rhea an ( l its applicali &lt; n : to Ihe donlain of language artluisition. We show \ [ hal with ( m ! a pri ( ll'i inl~.Jrulatio ( I almut how outer worlds an , or , ~anized , Rhea can learn lhe `` 'set ting for new word.~ '~ , which children con\ [ 'rc , ldillg flew w ( lrds s ( ! elll t ( I possess. The imim is how the model aeqlfir~ , ~ and tormalizes the `` meaning '' of an expression. To achiew , this aut ( : mOrlmusly , Rhea has its own rep resenlati ( m language for outer-worlds. If one lin guistir expression is repeatedly given along wilh ( tifli , renl otHer-worlds , it builds up one ( 'OmnlOtl r ( , im~senlati ( m tbr ; tl\ ] lhe ( mt ( , r-worlds. This in |el'llal I'e\ [ H'eselll ; llioll that has a Olle Io ( ) tie correspondence to a linguistic expressi ( m is regar ( led as the `` meaning '' of the expression iu our model. Ill order to ehicidate the rhildren 's rapid a ( 'quisit\ [ on of wmahulal'y , constraints ou~ the possible hypotheses about I\ [ 1 ( . meanings of linguistir e× pressions hay : ' heen postulaled. ( 'lark\ [ 2\ ] pro poses lh~ prhzciplf of eo'rdrasl whereby every two forlns ec.lltrast ill Illeallill~ , anIl Marklnan\ [ : l\ ] suggests a Sll'Oll , l~el ' ass~Hnplio~t of tft3 '' 011Otllie tll ' ( \ ] ( l+ nizatiom The assunlp ! ion of lax ( re , male organizalioll cold\ [ lieS children Io assumil : g lhal a wc.r ( l , givell with all unknown objert refers Io a taxonomic class of the ob.iert. As ostensive d &lt; , finitioll is the only way to acquire early wwabulary , the assumption reduces the possihle search space of area.sing. With this assulnlHion , if yoa see son.eone point Io an un familiar objec ! and say a word , VOII Call i ) resll ( lle that I It ( , word is eil her ! he label of lhe object or the label of one of Ihe categories it belongs Io and can li~rget about the possibility of the word 's I ' ( ! l ' ( 'l'l'illg I ( I O11 ( ' ( If its attribtlt ( 's or AcrEs DE COLING-92 , NANTES , 23-28 ^ofrr 1992 7 0 7 I'ROC. OF COLING-92 , NA~rrEs , AUG. 23-28 , 1992 ( Represent -Classify __ Generalize ) ins~ Figure I : ll , \ [ ormation flow its relation to other objects. Chil ( lren seenl to ( : onsJder the assunlpliOll of taxonolni ( `` organization. Markntan 's experiment shows thai even though they are liahle to consider thematic relations in dmnains other than language acquisition , children hearing a new word attend Io taxonomic relations. This tendency is called the `` setting for ' new words '' . It is /tot cleat ' , however , if such constraints are innate or not , or more essentially if they call he derived froln restri &lt; `tioas thai ally intelligent system should observe. One way to &lt; `\ ] arify this point is to examine whether tit ( , model that does not contain the constraint can acquire it during the learning process. tem Fig.1 illustrates Rhea 's learning process in two dif\ [ erent domains , A and B. The system 's task is to find general rules that predict which instance from l ) olltain A can at ( pear with a certain instal'tee front B , and vice versa. Rhea accepts as input a \ [ ) air of instances i = ( a , b ) . One instance is fronl l ) omain A an ( 1 the other front l ) omain B. One pair is given at a time. Rhea is equipped with an internal representation language for each domain , D.4 and De , and has predelined methods to extend the reprosentation languages in case of nee ( l. Sindlarities , generalization operations and specialization operations are defined upon ea &lt; , h language. Rhea represents all input pair using these languages and their extensions , anti makes all internal l'et tresentation D ( i ) ( 1 ) A ( a ) , De ( b ) } , which is a pab '' of a representation of '' I ) olnain A instan ( : e att ( I that of a l ) omaill H instance. More tha.ll one possible internal representation may exist for one input , but the one found first is stored. When represelltations are acctHlflulated , R hea is able to find out rules , h tirst sorts intert~al representations into classes based on similarities. Classes nlay or may not overlap. Then Rhea generalizes representations of ' each ( 'lass. This process of &lt; `lassili ( 'ation and generalizatiol~ is , done oil demand. When a partial input , ( an instan &lt; `e frotn l ) omain A ) is given and its &lt; `otlnterpart b ( front l ) o nmizl B ) is to he predlcte ( I , tilt , ntodel first ( : las stiles the partial input isle a class N using tlt ( ' infol'nlalion about a , makes the gelmraLiz ; ltion of l ) omain |l part ttf all the other reltresentalions ill &lt; '\ ] ass : ' , : and expects one of its spe ( 'ializalions to he b 's rt , presentalion l ) B ( b ) . The nmdel \ [ ortns classes so that reF.resentalions in ea ( : h ( : lass share sortie characteristics. Two internal representations , ( l ) , t ( a\ ] ) , l ) h , ( bj ) ) an ( l ( l ) A ( a ; \ ] ) ~ I ) B ( b2 ) ) , belong to ihe same class if DA ( al ) and DA ( a2 ) are sinlilar in the trite+ rion defitmd in the representation language I ) A. and l ) B ( bl ) an ( l I ) B ( b2 ) are also sindlar it\ ] the erileri ( m defined in D~j. In the extreme t'ase , if l ) A ( al ) equals 1 ) 4 ( a2 ) , then I ) H ( bl ) nlusl equal DMb2 ) and viee versa , which n , eans that when two instarlces frt : ,nt Olle domain are represented as the same , instances from the other ( lonlain that altpear with them l`tttlst also } lave the Sallte lilt el'l` ( a.l ropresett t at ion. a.2 Rhea as a language acquisition model l'thea , when applied to the domain c , f outerworMs ,5 ' and the domain of linguisl ic expressions L tha , t descril ) e t he outer worlds , can I ) e regarded as a language acquisition model. In these domains , Rhea learns the followings : of linguistic expressions I ) 1 , pressions Dz ( ll ) ... .. DI. ( I , ~ ) of outer-worhls D , s '' l ) s ( +t ) ... .. l ) s ( , % ) which resl ) e &lt; , tively can be seen as I. Syntacti ( ' rides 4+ Meanings of linguistic exl ) ressions derived from Oilier-worlds ACRES DE COLING-92 , NANTES , 23-28 AOt~ '' r 1992 7 0 8 PRec. OF COLING-92 , NANTES , AUO. 23-28 , 1992 Pigure 2 : Rhea as a language aC ( luisit , ion niii , h~t - ( with imcchan ( time-slices \ [ ( T~ T2 `` m T4 ... ... . Tnl ) I S ... , . : ( ) Llie~ \ [ `` igure 3 : Scene : a parl ( if the inpul Fig.2 shows tire conliguration of lh ( ! language al : quisition nlodel , Rtiea. 11 rec ( , iv ( , s a pa_h '' of Oil ( , scelt~ , and a linguistic expression thai describes ill ( ) SC ( Hi ( L All OXpl ' ( ! S. '' iiOll iS a , SPIILI ( HI ( : ( ! ( if words and contahls IIO S~i'/iCtllr~t , I iilt ( li'lliatioll. A si : ( ! Iio is tim equlva , lenl of SeilSOl'y hit ) ul fronl Olilor worlds. Pig.3 shows all e×aliilile o\ [ it ~ ( : ( , ii ( ! , A , M : ellO is a Se ( lil ( ~il ( : ( ! of , &lt; ; ilflp , ~ ; hol ' , ~ wliicl~ ar ( , lists ( if assertions thai , be ( : onie t , rllo ( ) 1 ' false at the tilliO when the sii~lmhots have been taken. Each assertiOll expresses a r ( ! la , tion between two iorlliS. The tornis llla ) ' lie olije ( 'ts , attributes or wthies , which can not tie distingulshed t ) y Rllea. The parser makes tile iliterna\ ] rl ! l ) resollla Lions of \ ] lnguisli ( : e×l ) ressions , all ( \ ] the Jilter finder makes those of scenem The elas , siJTer divides rel ) resentalions into classes and nlakes rules. Sin\ ] c ( ' two inputs reprosexnh , ( I as the sam ( , lit ( ill ( ! dOlllaill liltlSI Jlavo tllO Sill\ [ l ( ' F ( ~t ) l ' ( ~S ( ! ll| ; ttiC ) l~ ill I IIO olh01 ' ( IOlllaili , I\ [ \ [ 101'o lll ; , ly \ [ ) U li ( i S.Vllfiliyllls or l ) olysemaIHs , which means tha ! the model has `` the i ) rin ( : il ) k , of ( ' ( ) ill l'asl ~ ' implanted from the beginning ; . 4 hlternal l'epresentatlons of inputs Th ( ' inlernal r ( , pr ( , setltatiim of an inllut is a pair of internal n ' ( , i ) ros ( mtati ( ms of th ( , inpul 's conMilu ( ! nls , which i~ a pair of one .~h'uclure and one jiltrr. tie expressions The ilH ( q'nM repr ( , s ( mtatio , ofa lingHisllc eXl ) Ves sion is the synla ( 'ti ( ' structure ol Iho , &gt; xl ) ri !</sentence>
				<definiendum id="0">expression. To achiew</definiendum>
				<definiendum id="1">finitioll</definiendum>
				<definiendum id="2">DA</definiendum>
				<definiendum id="3">Rhea</definiendum>
				<definiendum id="4">ellO</definiendum>
				<definiendum id="5">ttiC</definiendum>
				<definiendum id="6">ilH</definiendum>
				<definiens id="0">a pri ( ll'i inl~.Jrulatio ( I almut how outer worlds an , or , ~anized</definiens>
				<definiens id="1">a taxonomic class of the ob.iert. As ostensive d &lt; ,</definiens>
				<definiens id="2">innate or not , or more essentially if they call he derived froln restri &lt; `tioas thai ally intelligent system should observe. One way to &lt; `\ ] arify this point is to examine whether tit ( , model that does not contain the constraint can acquire it during the learning process. tem Fig.1 illustrates Rhea 's learning process in two dif\ [ erent domains , A and B. The system 's task is to find general rules that predict which instance from l ) olltain A can at ( pear with a certain instal'tee front B , and vice versa. Rhea accepts as input a \ [</definiens>
				<definiens id="3">the other front l ) omain B. One pair is given at a time. Rhea is equipped with an internal representation language for each domain , D.4 and De , and has predelined methods to extend the reprosentation languages in case of nee ( l. Sindlarities , generalization operations and specialization operations</definiens>
				<definiens id="4">a pab '' of a representation of '' I ) olnain A instan ( : e att ( I that of a l ) omaill H instance. More tha.ll one possible internal representation may exist for one input</definiens>
				<definiens id="5">learns the followings : of linguistic expressions I ) 1 , pressions Dz ( ll ) ... .. DI. ( I , ~ ) of outer-worhls D , s '' l ) s ( +t ) ... .. l ) s ( , % ) which resl ) e &lt;</definiens>
			</definition>
			<definition id="2">
				<sentence>llhea sels Ih ( ' class ( ll all illl\ [ &lt; llfiWii v , ,ol ' ( l C ( ll/ si ( lerhlg the ~ ( : ( 'n ( ' ~iVlql with Ihe wor ( l , ltxl : liei\ ] SOllle rule predh'is the ( 'lass flf Ill ( , w ( ird and tim ~ ( : ( , Ii ( ! i ) r ( , sonie ( l with the word Call be giv ( 'n all ilit ( , rnal r ( ! t ) r ( ~s ( ! llLali ( in sitnilar to Ihos ( , ( if other words in the class , Ill ( , w ( ir ( I is ad ( le ( l to the tire dicled class. If nol , a iil , ,w caleg ( iry is a~signe ( \ ] Io the word. An internal r ( , pi'esontallon of a scene I ) rovides Ihe s ( , ananlic~ , of Ill ( ' linguimic expression that comes AcrEs DE COLING-92 , NANTES , 23-28 ^oL~r 1992 7 0 9 PROC. OY COLING-92 , NANTES , AUG. 23-28. 1992 ~e.e s2 ~ ... .. -- . -- ~ Focus of atleniion * , @ ~i** i ) f ( s2 ) f '' igure 4 : Relalionship among Filter , Scenes and l '' tlcllS Ill Attentions witii the scene. Lhlguistic expressions cilange or cotll tel tile listeners ' illlerprelaliolls of tile outer world , and make speakers and listeners share lille focll , s of at ( chiton ( hereinafler , F ( ) A ) . hi order to niodel this process , a scene is internally top resented a.s a prlwellure thai COllVel'IS the SCelle inlo ail FOA. We call this procedure a filhr. As stated before , a SCelle is il seq/lellce of lists of assertions , and so is all FOA. PeAs must conlain . : it lea.st olie non-variable assertion lleeatlse there rnusl exist non-variable FOAs to tie shared arulnig speakers a , ll ( I listeners. If a filter applied to s ( tene s yiehls a non-variable sequence el lists of a~ssertions , the filter is valid for .s. Any valid filter for sceile .s can lie a reln'eselitation of the sceile. \ ] \ [ ? o1 '' t , ×ainple , a scelie ( liar COil ( sins solneolle eating pancakes ilia ) ' lit. ' internally represented ill several ways. A llroee dilre ( hal focuses lhe listeners ' aJtentioii Oil pal ( cakes and yiehls pallcakes as ; Ill VOA is valid for tim scene , and one that stresses lhe eating aclion call also lie all internal representation of the scelle , tlowevel ' , scelleS whicii appeared v.rith lilt ' saliie expression milS1 have lhe sanie fiher t ) ecallse iiiere lllay tie iio polysenlalltS. Fig. 4 shows the relationsl ( ip auiolig fillers , sc0n0s alld f '' ( ) As. Sill ( : ( ' the FOAs derived by fillet '' f frOlll SCelle , sl and scelle , s~ both contain sonic objects , the filler is valid for both scenes. Thus two sceiies thai appear with linguistic ex presslon / are represented by tile filter. l '' ilters art , mappings from scenes 1o I '' OAs. ilhea has 32 parameterized sinlple nlallpillgs as its representalioll language Ds al the start. Ii COlllliilleS lllappings and seaFches a given s£elle for values to instant ( ate paralneters , % % : ( ' call these paralneterized lnapplngs Jill+r-primiliv6.s and ilistantialed niapphlgs fillfr-~l~ln~ll. , &gt; .</sentence>
				<definiendum id="0">Ii</definiendum>
				<definiendum id="1">liar COil</definiendum>
				<definiendum id="2">Ill VOA</definiendum>
				<definiens id="0">a~signe ( \ ] Io the word. An internal r ( , pi'esontallon of a scene I ) rovides Ihe s ( , ananlic~ , of Ill ( ' linguimic expression that comes AcrEs DE COLING-92</definiens>
			</definition>
			<definition id="3">
				<sentence>\ ] f there is 11o r0presentaiion of lhe fornl ( IlL ( I ) , f ) , I is regarded a.s a new expression and Rhea builds a candidate lbr filler f. The candidate consists of one filter elenielil made by st ' letting one fiher-pl+iniitive randotnly and subsliluting terms in the giveli scelle , , ; fol '' parauielers of the fiiler-ilrinlilive .</sentence>
				<definiendum id="0">Rhea</definiendum>
				<definiens id="0">11o r0presentaiion of lhe fornl ( IlL ( I ) , f ) , I is regarded a.s a new expression</definiens>
			</definition>
			<definition id="4">
				<sentence>This paper ha+'+ described Rhea , the model of language aequisillou , which uses wwy general aC &lt; luisition procedure .</sentence>
				<definiendum id="0">Rhea</definiendum>
				<definiens id="0">the model of language aequisillou , which uses wwy general aC &lt; luisition procedure</definiens>
			</definition>
</paper>

		<paper id="2068">
			<definition id="0">
				<sentence>Copying is an expensive operation .</sentence>
				<definiendum id="0">Copying</definiendum>
				<definiens id="0">an expensive operation</definiens>
			</definition>
			<definition id="1">
				<sentence>Two wdues are returned , the first value being the Col ) y ( or original ) nolle and the second value being the flag representing whethe , r any of the node below that node ( including t , hat node ) h ; us been ehanged .</sentence>
				<definiendum id="0">Col ) y</definiendum>
				<definiens id="0">value being the flag representing whethe , r any of the node below that node ( including t , hat node</definiens>
			</definition>
			<definition id="2">
				<sentence>If the eomI ) lex node is a target of forwarding , if no node behiw that node is changed then the ( ) rigin ; d eonlplex node is shared ; however , the ' ( : hanged ' inforniatit ) n 20I .</sentence>
				<definiendum id="0">eomI ) lex node</definiendum>
				<definiens id="0">a target of forwarding , if no node behiw that node is changed then the ( ) rigin</definiens>
			</definition>
			<definition id="3">
				<sentence>nnifyl wi~ analysis ( \ [ Ponard and Sag , 1987\ ] ) eovering llhenomena such as coordination , case adjmlction , adjunets , control , shLsh categories , zero-pronouns , in terrogativcs , WH constructs , and sonic pragmatics ( speaker , hearer relations , politeness , etc. ) ( \ [ Yoshimoto and Kogm'e , 1989\ ] } .</sentence>
				<definiendum id="0">sonic pragmatics</definiendum>
				<definiens id="0">coordination , case adjmlction , adjunets , control , shLsh categories , zero-pronouns , in terrogativcs</definiens>
			</definition>
			<definition id="4">
				<sentence>no &lt; tern \ [ Karttunen slid Kay , 1985\ ] suggests the use of lazy evaluation to delay dcstructive chalLges during unificatiou. \ [ Goddcn , 1990 } presents one method to delay copying until a destructive change is al ) out to take phtcc. Godden uses delayed closures to directly imphm|cnt lazy evaluatitm during unification. While it may be concel ) tually straightforward to takc iulvantagc of delayc ( I cVahlation functionalities ill progranuning laagtlages , actllal efficiency gain fl'on ! such a schelnc may not bc significant. This is l ) et : aase such a schenle siml ) ly shifts t4e time and space consmned for Col ) ying to creating and evaluating closures ( which couhl be very costly compared to 'dcfstruct '' operations to create COl ) ics 31The discussion of Karltunen 's lnethod is ba.e , l on the D PATR imphnuent ; ttion on Xerox 1109 machines ( \ [ Karttunmt , 1986\ ] ) . 32I+e. , arc stru ( : turen : 'label '' and 'vld\ [ l ( ~ ' \ ] ) ilii'~ ill oUr w~cabulary. Num of Arcs W qD OS 113 123 36 2441 1917 760 182 183 62 2408 2191 879 9373 7142 2272 874 797 204 113 123 36 2572 2334 710 17358 12427 3394 20323 15375 5116 3089 2712 992 14321 10218 3059 19014 13055 4471 893 983 199 2436 2185 793 2436 2185 793 97946 73950 23776 100~ 76 % 24~ which arc often effectively ( ) l ) timized lit many tom ... ... • cial ... ... . lfilcrs ) . \ [ Kog , |l'C , 1990\ ] anti \ [ Eulelc. 19611 also use the lazy evaluation i ( h ' , a to delay destructive changes. Both Kogurc all ( l Eme , h~ avoid direct usage of delayed evMuation by using pointer Oln : rations. As b3mchr suggests , KogurCs method also requires a special dcl ) endcncy inf ( , rmation to bc mifintained which adds all overhead ahmg with the cost fin ' traversing tile dCl ) cudency arcs. Also , a secon ( I travcrsa\ ] of the set of dellelldellt liodes i8 required for actually pcrfl ) rming the copying. Emele proposes a method of dcrcfercncing by adding enviromnent inforlltatioli tllld , carries it scqtl ( Hl ( : t ! of gCllCl'atioll COllll* tcrs so that a specific generation node ( : all I ) ( ~ fOlllld by traversing tim forwarding links until a node with that generation is found. While this allows undoing destructive changes cheaply by backtracking the envirolllOCllt , every tinlc a spccilic graph is to bc at : ( : cssed the whoh ! gt'aph ilcctls tO bc rccollstrll ( : ted by following the fol'wardillg pointers , ~cqucntildly as speciticd in the environment list ( cxt : et ) ~ for the root node ) to find the node that shal'CS the smne generation number as the root llOde. Therefore , similar to Pcrcira 's mcthotl , there will be Nlog ( d ) ovcrhcad iLssociated with constructing t ! ach graph every tinle &amp; gral ) h is accessed , where ( 1 is the llUillbCr of nodes ill the graph and N is the average depth of the tmvironlllClttal dcfcrcncc chain , This would cause a probh ! nl if the algtlrithm is adolltcd fin ' a large-sclde systcm ill which result graphs arc unified agidnst other graphs many times. Like Wroblewski 's method , `` all three lazy methods ( i.c , Godden 's , Kogure 's and Emele 's ) suffer fi'mn the t ) roblenl of Early Copying ms defined in \ [ Tonlabcchi , 1991\ ] . This is because the copies that arc incrcnmntally created u 1 , to tile point of failure during the same topAcvel unification arc wasted. The problent is inherent ill increlnental copying scheme and this probhml is elinfil|ated completely in \ [ Karttuncn , ACRES DE COL1NG-92 , NANTES , 23-28 Aotrr 1992 4 4 5 PROC. oF COLING-92 , NANTES , AUO. 23-28 , 1992 19861 and ill tile Q-D nmtltod. 3a There is one l/otential problent with the structure , sharing idea whMt is shared by each of the schemes inchlding tile proposed tnethod. This ltallpens when operations other tllan ttnification modify the graphs. ( This is typical whco a parser cuts off '' a part of a graph for sltbsequellt analysisa4. ) When such ol ) erations are perfornmd , structure-sharing of t ) ottolll ( vlu'iablc ) nodes stay cause probhmts when a subgraph cotttaitdog a 1 ) ottmn is shared by two different graphs and these graphs are used as argtllllelltS of a utfification function ( either ~Ls the part of the same input graph or as ehmumts of dgl and dg2 ) . When a gt'aplL that shares st bottl ) lO ( lode iN llOt Ilsed ill its entirety , then the , represented i : lmstraint postulated by the path leading to the bottotn no ( h ; is no longer the same , Therefl &gt; re , when such a graph appears in the same unification aloog with soIoe other graph with which it DS shares the same bottotn node .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">al ) out to take phtcc. Godden uses delayed closures to directly imphm|cnt lazy evaluatitm during unification. While it may be concel ) tually straightforward to takc iulvantagc of delayc ( I cVahlation functionalities ill progranuning laagtlages</definiens>
				<definiens id="1">typical whco a parser cuts off '' a part of a graph for sltbsequellt analysisa4. ) When such ol ) erations are perfornmd , structure-sharing of t ) ottolll ( vlu'iablc ) nodes stay cause probhmts when a subgraph cotttaitdog a 1 ) ottmn is shared by two different graphs</definiens>
			</definition>
</paper>

		<paper id="4178">
			<definition id="0">
				<sentence>OR ( V ) links different properties of an object .</sentence>
				<definiendum id="0">V )</definiendum>
			</definition>
			<definition id="1">
				<sentence>In the figure next page , T represents an unspecified temporal object , which can denote either an interval , I , or a time point , t. We define three 'basic ' classes , corresponding to three irreducible temporal patterns , and then five others in terms of the former , with some additional `` low-level '' conditions , linking the temporal variables among the elementary intervening aspectual class definitions .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">an unspecified temporal object , which can denote either an interval</definiens>
				<definiens id="1">the temporal variables among the elementary intervening aspectual class definitions</definiens>
			</definition>
			<definition id="2">
				<sentence>OF COLING-92 , NANTES , AUG. 23-28 , 1992 state ( P ) ~ P ( T ) act ( P ) ~ P ( I ) change ( P ) ~ P ( t ) A notQ ( i ) A Q ( t ) A initial ( i , I ) A final ( t , 1 ) ace ( P ) ~-~ act ( P ) A change ( R ) A final ( t , I ) =P ( I1 ) A R ( t ) A notQ ( i ) A Q ( t ) A initial ( i , I2 ) A final ( t , I~ ) A final ( t , I1 ) ach ( e ) ~-~ act ( P ) A change ( P ) A inside ( t , 1 ) -- -- -P ( I1 ) A P ( t ) A notQ ( i ) A Q ( t ) A initial ( i , 1~ ) A final ( t , I~ ) A inside ( Iz , It ) act state ( P ) ~ act ( P ) V state ( P ) A inside ( T , \ [ ) =-P ( I ) V P ( T ) Ainside ( T , I ) acq ( P ) ~ change ( P ) V state ( P ) A initial ( t , T ) -P ( t ) A notQ ( i ) A Q ( t ) A initial ( , ' , I ) A fi .</sentence>
				<definiendum id="0">notQ</definiendum>
				<definiendum id="1">Q ( t</definiendum>
				<definiendum id="2">P</definiendum>
				<definiendum id="3">notQ</definiendum>
				<definiendum id="4">P</definiendum>
				<definiendum id="5">notQ</definiendum>
				<definiendum id="6">notQ</definiendum>
				<definiens id="0">A inside ( T , \ [ ) =-P ( I ) V P ( T ) Ainside ( T , I ) acq ( P ) ~ change ( P ) V state ( P ) A initial ( t</definiens>
			</definition>
			<definition id="3">
				<sentence>al ( t , I ) v P ( T ) A initial ( t , T ) series ( P ) ~ change ( P ) V act ( P ) A initial ( t , I ) =e ( t ) A notQ ( i ) A Q ( t ) A initial ( i , I ) A final ( t , I ) V P ( I ) A initial ( t , I ) press both the change that takes place and the resulting state , such as remember , understand , know .</sentence>
				<definiendum id="0">notQ</definiendum>
				<definiens id="0">initial ( t , T ) series ( P ) ~ change ( P ) V act ( P ) A initial ( t , I ) =e ( t</definiens>
			</definition>
			<definition id="4">
				<sentence>die : act ( die ) A change ( die ) A inside ( t , ll ) yesterday : P ~ before ( T , ,tow ) A day ( T ) A 0 &lt; T now &lt; 24h A inside ( Tp , 7 '' ) simple-past : P ~ before ( T , now ) plural-NP : P = &gt; Act ( P ) 5 die-yesterday : act ( die ) A change ( die ) A inside ( t , I ) A before ( T , now ) A day ( T ) A inside ( t , T ) A inside ( I , T ) died-yesterday : act ( die ) A change ( die ) A inside ( t , I ) A befo~e ( T , .</sentence>
				<definiendum id="0">T</definiendum>
				<definiendum id="1">T</definiendum>
				<definiendum id="2">change ( die ) A inside</definiendum>
				<definiens id="0">A inside ( t , T ) A inside ( I , T ) died-yesterday : act</definiens>
			</definition>
			<definition id="5">
				<sentence>Redundancy is a pervasive property of natural language , but it is hard to model when one device is required to apply before another , each bringing different information .</sentence>
				<definiendum id="0">Redundancy</definiendum>
				<definiens id="0">a pervasive property of natural language</definiens>
			</definition>
</paper>

		<paper id="1062">
			<definition id="0">
				<sentence>OF COI. , ING-92 , NANTES , AUG. 23-28 , 1992 s -- *ID a , b , c , d ( 1 ) s -~x. a , b , e , f ( 2 ) a , b , c &lt; d ( 3 ) b &lt; c ( 4 ) a , e &lt; f ( 5 ) Figure 1 : An example ID/LP grammar : Gl Parser Identifier Bit Vector 111111ollo1 Iln2lolxol dl If 10111~011101 \ [ 011211011101 Strategy Chart parsing is one of the most well-known and efficient techniques for parsing general context-free grammars .</sentence>
				<definiendum id="0">Strategy Chart parsing</definiendum>
				<definiens id="0">An example ID/LP grammar : Gl Parser Identifier Bit Vector 111111ollo1 Iln2lolxol dl If 10111~011101 \ [ 011211011101</definiens>
			</definition>
			<definition id="1">
				<sentence>GDN is a generalization of a discrimination tree that can be traversed according to the order in which constralnts are obtained incrementally during the analytical process , independently of the order of the network 's arcs .</sentence>
				<definiendum id="0">GDN</definiendum>
				<definiens id="0">a generalization of a discrimination tree that can be traversed according to the order in which constralnts are obtained incrementally during the analytical process</definiens>
			</definition>
			<definition id="2">
				<sentence>This digit is followed by the sequence S ( v ) , which is the concatenation of the sequence S ( u ) and the integer k , where u is the immediate predecessor of v and k is the numerical number of the arcs issuing from u. 1 Note that the identifier of the root node r has only the first leftmost digit ( S ( r ) is null ) .</sentence>
				<definiendum id="0">u</definiendum>
				<definiendum id="1">k</definiendum>
				<definiens id="0">the numerical number of the arcs issuing from u. 1 Note that the identifier of the root</definiens>
			</definition>
			<definition id="3">
				<sentence>First , the definition of a state is changed from a 2-tuple &lt; Id , BitV &gt; to a 4-tuple &lt; Cat , Id , Pre , BitV &gt; where each element is defined as the following : Cat : the mother category of the state ; Id : the identifier of the state ; Pre : the precedence vector of the state ; BitV : the bit vector of the state .</sentence>
				<definiendum id="0">Id</definiendum>
				<definiendum id="1">Pre</definiendum>
				<definiendum id="2">BitV</definiendum>
				<definiens id="0">the identifier of the state</definiens>
				<definiens id="1">the precedence vector of the state ;</definiens>
			</definition>
			<definition id="4">
				<sentence>• w , , where wi is a terminal in G , we construct the chart as follows : k +-0 ; while k &lt; n do begin the inactive ( complete ) edges corresponding to every possible category of w~+ : between vertices k and k + 1 .</sentence>
				<definiendum id="0">wi</definiendum>
				<definiens id="0">the chart as follows : k +-0 ; while k &lt; n do begin the inactive ( complete ) edges corresponding to every possible category of w~+</definiens>
			</definition>
</paper>

		<paper id="2095">
			<definition id="0">
				<sentence>The action table consists of a totM of 984 individual ( nonerror ) entries .</sentence>
				<definiendum id="0">action table</definiendum>
			</definition>
			<definition id="1">
				<sentence>\ ] 2\ ] \ ] \ [ c r~\ ] \ ] \ ] LF : Cc2\ [ sP t~an|\ ] -a ( ~ \ [ CI\ [ 12\ [ NP taro\ ] -t~ \ [ II\ [ VP\ [ VP\ [ NPEC2\ [ 12 pro \ [ II\ [ VP\ [ \ ] \ [ VP\ [ NP te\ ] -daL \ [ VI\ [ NPt-~-P\ ] \ [ V I ( P~R ) 1 2 0 t 4 1 Cv Ireta\ ] 5\ ] 5\ ] \ ] \ ] \ [ It3\ ] \ ] Ccl\ ] Cs koto\ ] \ ] -ace6 \ [ vCAov ~ot~i\ ] Cv okotte\ ] 7 \ ] 7 \ ] Creel Cl , ( mR ) 2 Cv Iru\ ] o\ ] 2\ ] \ ] Cc ~\ ] \ ] \ ] LF : \ [ c2 \ [ NP aanl\ ] -ac¢i \ [ el It2 ~ro 2 ell \ [ vP \ [ vP \ [ NP \ [ c2 \ [ 12 the taro\ ] ~8 \ [ 11 \ [ vP \ [ \ ] I \ [ VP CsP t~\ ] -dat 4 \ [ v1 \ [ NPt-~-P\ ] I Cv l ( fl~R ) 3 \ [ v Ireta\ ] \ ] \ ] \ ] \ ] tit \ ] \ ] \ [ C\ ] \ ] \ [ N koto\ ] \ ] -a~c \ [ vtAov ~i\ ] \ [ v ~o~te\ ] \ ] \ ] \ [ vt \ ] \ [ I I ( ~R ) \ [ v it'u\ ] \ ] \ ] \ ] re n~\ ] \ ] \ ] 55 3 6 77 8 2 82 Ro ( .</sentence>
				<definiendum id="0">LF</definiendum>
				<definiens id="0">Cc ~\ ] \ ] \ ] LF : \ [ c2 \ [ NP aanl\ ] -ac¢i \ [ el It2 ~ro 2 ell \ [ vP \ [ vP \ [ NP \ [</definiens>
			</definition>
			<definition id="2">
				<sentence>where the empty operator ( Op ) is base-generated in an A-position and subsequently fronted by Move-c~ ( Chomsky , 1986:86 ) .</sentence>
				<definiendum id="0">empty operator</definiendum>
			</definition>
			<definition id="3">
				<sentence>UNITRAN : A Principle-Based Appro~tch to Machine Translation .</sentence>
				<definiendum id="0">UNITRAN</definiendum>
			</definition>
</paper>

		<paper id="2086">
			<definition id="0">
				<sentence>DS ~ C give all answer ) answer ( verb ) 1 say , write or do something in response to somebody 2 be suitable for I Figure 1 Word sense selection Since a deverbal noun in a DS possesses some nominal sense of the matching verb , we can assume that a DS chooses a single sense of the deverbal noun and verbalizes it as shown in figure 2 .</sentence>
				<definiendum id="0">DS</definiendum>
			</definition>
			<definition id="1">
				<sentence>A voice control rule was developed based on the `` flow of action '' of delexical verbs and the `` transitivity '' of deverbal nouns ( the original verb ) .</sentence>
				<definiendum id="0">transitivity</definiendum>
				<definiendum id="1">deverbal nouns</definiendum>
				<definiens id="0">the original verb )</definiens>
			</definition>
</paper>

		<paper id="2072">
			<definition id="0">
				<sentence>( 7 ) Confusion of that , , their , and they 're : a. *Their is a dog here .</sentence>
				<definiendum id="0">*Their</definiendum>
				<definiens id="0">a dog here</definiens>
			</definition>
			<definition id="1">
				<sentence>Here , N is the set of nccessary constraints and O is the set of optional constraints , both for a given relaxation level L ; R is the set of constraints which have to be relaxed in order for the rule to he used .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">O</definiendum>
				<definiendum id="2">R</definiendum>
				<definiens id="0">the set of nccessary constraints and</definiens>
				<definiens id="1">the set of constraints which have to be relaxed in order for the rule to he used</definiens>
			</definition>
			<definition id="2">
				<sentence>Here , R is the set of relaxation packages required in order to complete the parse .</sentence>
				<definiendum id="0">R</definiendum>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>The Commission of the European Communities began to promote large scale activities in natural language processing more or less around the time of the Bergen COLING Conference in 1978 .</sentence>
				<definiendum id="0">Commission of</definiendum>
				<definiens id="0">the European Communities began to promote large scale activities in natural language processing more or less around the time of the Bergen COLING Conference in 1978</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>This paper presents Trace &amp; Unification Grammar ( TUG ) , a declarative and reversible grammar formalism that brings together Unification Grammar ( UG ) and ideas of Government &amp; Binding Theory ( on ) ~ The main part of the paper consists in a description of many free word order phenomena of German syntax .</sentence>
				<definiendum id="0">Unification Grammar ( TUG</definiendum>
				<definiens id="0">a declarative and reversible grammar formalism that brings together Unification Grammar ( UG ) and ideas of Government &amp; Binding Theory ( on ) ~ The main part of the paper consists in a description of many free word order phenomena of German syntax</definiens>
			</definition>
			<definition id="1">
				<sentence>23-28 , 1992 Further to these more standard UG-features , TUG provides special rule formats for the description of discontinuous dependencies , so called `` movement rules '' .</sentence>
				<definiendum id="0">TUG</definiendum>
				<definiens id="0">provides special rule formats for the description of discontinuous dependencies , so called `` movement rules ''</definiens>
			</definition>
			<definition id="2">
				<sentence>vp is_head of s. Second , the landing site is defined by a rule like s ' -- - &gt; v+s I ... where landing site and root node are linked by a + .</sentence>
				<definiendum id="0">landing site</definiendum>
				<definiens id="0">a rule like s ' -- - &gt; v+s I ... where landing site and root node are linked by a +</definiens>
			</definition>
			<definition id="3">
				<sentence>~t , b ' ~ete ; re , , re , die ~ila~ \ [ v. ab t , \ ] \ ] \ ] \ ] \ ] \ ] Today copies Pete~ : the pictures c. \ [ s~ Es \ [ s ' , , , alt~ \ [ s , °ete , '\ [ v , ~ \ [ v ' die raider Iv. , , '' , ,b td\ ] \ ] \ ] \ ] \ ] It copies Peter the picture~ `` Peter copies tire pictures '' This facts can be described by tim following rules : ( 10 ) s2 -- - &gt; es , sl s2 -- - &gt; pp , sl s2 -- - &gt; advp , st s2 -- - &gt; rip &lt; trace ( vat , rip ) , sl trace ( vat , rip ) . Free word order in £1xe Miltclfchl is described by `` moving '' an argument to a chomsky-adjoined position on the V-projection. llere it obeys the same conditions a.s an adverbial and leaves a trace in the original argument position. Acr\ [ ~s DF , COLING 92 , NANrJiS , 23-28 ao ( rr 1992 8 9 PRec. OF CO\ [ , ING-92 , NAN'H~S , AUd. 23-28 , 1992 ( 11 ) a. \ [ sa Is , daft Is der M ... . \ [ vP \ [ N~ '' da , g Buch\ ] i \ [ vn. der bYan \ [ v , tl \ [ v~ gegebe , aatllllll\ ] that the tan the book the soaan given has b. is , \ [ s , d~ \ [ s ( uP d~r -~ '' b \ [ s \ [ uP da , ~atl\ ] \ ] \ ] \ ] \ ] \ ] \ ] that the uoaan\ [ dat\ ] the book\ [ anal the aan\ [ noa\ ] given has ~that the man has given the woman the book '' So , for scrambling , we basically need the following rules : a -- - &gt; np &lt; trace ( a~a , np ) , s vp -- - &gt; up &lt; trace ( ann , up ) 0 vp Whereas meet concurrent theories adopt the view that an argument phrase in the Vorfeld is linked to the argument position by a trace be it by movement or by the slash-feature the relative free word order in the Miffelfeld is often accounted for by the distinction of phrase structure rules into immediate dominance ( ID ) rules and linear precedence ( LP ) rules. ID rules define the hierarchical structure of constituents , LP rules the linear ordering of daughters constituents. In this paradigm the german Miftelfeld inelu'ding the finite verb typically is supposed to form a fiat structure , generated by an ID rule like a -- - &gt; np\ [ no~ , np\ [ akk\ ] , np\ [ dat\ ] , v\ [ ~ln\ ] , vk\ [ infin\ ] r. The elements on the right hand side can then be ( partially ) ordered by LP statements of the form vf : fi~ &lt; up0 np &lt; vk ( a finite verb precedes an NP , a VK follows an NP ) .</sentence>
				<definiendum id="0">LP</definiendum>
				<definiendum id="1">VK</definiendum>
			</definition>
</paper>

		<paper id="3142">
			<definition id="0">
				<sentence>Nodes contain informative contents , while arcs represent the possible associations between different informative contents , in accordance with the logic of the hypertext itself .</sentence>
				<definiendum id="0">Nodes</definiendum>
				<definiens id="0">contain informative contents , while arcs represent the possible associations between different informative contents , in accordance with the logic of the hypertext itself</definiens>
			</definition>
			<definition id="1">
				<sentence>The knowledge representation language is a member of the family of hybrid systems , and is made up of a terminological component and an assertional one , although certain characteristics make it more similar to classical KL-One ( Cappelli et al. , 1983 ; Bracbman &amp; Scbmoltze , 1985 ; Nebel , 1989 ) .</sentence>
				<definiendum id="0">knowledge representation language</definiendum>
				<definiens id="0">a member of the family of hybrid systems , and is made up of a terminological component and an assertional one , although certain characteristics make it more similar to classical KL-One ( Cappelli et al. , 1983</definiens>
			</definition>
			<definition id="2">
				<sentence>l-tecun = max max nU PR OD ( t I ( \ [ T 1\ ] rain 5 , ... , t n ( \ [ T r , \ ] nun ~ ) '/Zt~t/0_f_c ( \ [ P\ ] 11 ) ) where : PROD denotes the Cartesian product , max \ [ An\ ] min~ denotes the lists of elements belonging to A , whose length is between rain and max ( if max=nil then there is no upper bound to the length of the lists ) , m m_ .</sentence>
				<definiendum id="0">PROD</definiendum>
				<definiendum id="1">] min~</definiendum>
				<definiens id="0">the lists of elements belonging to A , whose length is between rain</definiens>
			</definition>
			<definition id="3">
				<sentence>~ , which is the name of the role member , acts as a type constructor , tl , ... , tn are the names of the properties inherited by team , TI , ... , Tn are the value-restrictions of the properties inherited by team , minl , maxl , ... , minn , max n are the number-restrictions of the properties inherited by team , P is the denotation of football-player .</sentence>
				<definiendum id="0">Tn</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">the name of the role member , acts as a type constructor , tl , ... , tn are the names of the properties inherited by team , TI , ... ,</definiens>
				<definiens id="1">the value-restrictions of the properties inherited by team , minl , maxl , ... , minn , max n are the number-restrictions of the properties inherited by team ,</definiens>
			</definition>
			<definition id="4">
				<sentence>On the basis of these types , a set of constraints can be specified , such as , for instance : the property 'Stuff ' follows the part-whole taxonomic model as shown in Winston et al. ( 1987 ) and Frederking &amp; Gerhke , ( 1988 ) ; the property 'Content ' is organized on the basis of the `` place/area '' model , where the following transitivity principle is valid : if in ( x , y ) ~ in ( z , x ) then in ( z , y ) ; 'Shape ' in certain cases refers to the shape of one of the components of a container , which may coincide with the shape of the whole ; 'Component ' also follows the part-whole model ; 'Contextual use ' is to be intended as a social and not a functional use , the latter being the specific use of containing something .</sentence>
				<definiendum id="0">'Shape</definiendum>
				<definiens id="0">to be intended as a social and not a functional use , the latter being the specific use of containing something</definiens>
			</definition>
</paper>

		<paper id="4208">
			<definition id="0">
				<sentence>I~tch job card indicates how to go about a specific servicing operation .</sentence>
				<definiendum id="0">I~tch job card</definiendum>
				<definiens id="0">indicates how to go about a specific servicing operation</definiens>
			</definition>
			<definition id="1">
				<sentence>Imperative clauses No imperatives ( imperative mood ) were encountered in the pertinent corpus , they are replaced by infinitive clauses ( such is the case for most service or maintenance notes in French ) : proc~der au campement de I'avlon ne pas tendre exag~rement les cordes Amongst the non-pertinent texts , messages for users of a data processing system , user guides or training guides may contain imperatives or ( sometimes ) interrogatives .</sentence>
				<definiendum id="0">Imperative clauses No imperatives</definiendum>
				<definiens id="0">imperative mood ) were encountered in the pertinent corpus</definiens>
				<definiens id="1">the case for most service or maintenance notes in French ) : proc~der au campement de I'avlon ne pas tendre exag~rement les cordes Amongst the non-pertinent texts , messages for users of a data processing system</definiens>
			</definition>
			<definition id="2">
				<sentence>VAUQUOIS , B. , &amp; CHAPPUY , S. , `` Static grammars : a formalism for the description of linguistic models '' , International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Language , Colgate University , 1985 .</sentence>
				<definiendum id="0">Static grammars</definiendum>
				<definiens id="0">a formalism for the description of linguistic models '' , International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Language</definiens>
			</definition>
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>tal Interpretation System A semantic representatiou in the Incremental Interpretation ( henceforth II ) System is called a `` Conditional Interpretation '' , which is defined as an assumptionsense pair , A : s , where A is a set of assumptions , and s is the sense .</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">tal Interpretation System A semantic representatiou in the Incremental Interpretation ( henceforth II ) System is called a `` Conditional Interpretation '' , which is defined as an assumptionsense pair , A : s , where A is a set of assumptions</definiens>
			</definition>
			<definition id="1">
				<sentence>Structural rules build the conditional interpretation of a phrase compositionally , from the conditional interpretation of its parts .</sentence>
				<definiendum id="0">Structural rules</definiendum>
				<definiens id="0">the conditional interpretation of a phrase compositionally , from the conditional interpretation of its parts</definiens>
			</definition>
			<definition id="2">
				<sentence>P denotes a syntactic node , where its immediate constituents are denoted by variables P1 through Pk .</sentence>
				<definiendum id="0">P</definiendum>
			</definition>
			<definition id="3">
				<sentence>The form of a discharge rule is P ~ A ' : s ' if P ~ A : s Here , A I ~A { P~ } , where R is the discharged assumption .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">the discharged assumption</definiens>
			</definition>
			<definition id="4">
				<sentence>For example , `` every jet '' is represented bind ( x , every , jet ) : x Simplifying slightly , the discharge of quantifier assumptions cml be represented as follows : bind ( x , q , s ) : Pt =* '' ( q s x ) p As an example , bind ( x , everyjet ) : fly ( x ) =~ ( every jet x ) fly ( x ) As mentioned above , when a pronoun assumption is discharged , its parameter is replaced either by an entity in the discourse model , or by some , yet undischarged parameter .</sentence>
				<definiendum id="0">bind</definiendum>
				<definiens id="0">bind ( x , q , s ) : Pt =* '' ( q s x ) p As an example</definiens>
				<definiens id="1">an entity in the discourse model , or by some , yet undischarged parameter</definiens>
			</definition>
			<definition id="5">
				<sentence>23-28 , 1992 P ( SUB~I , at ... .. an ) where SUBJ represents an unfilled subject argument position , with the remaining arguments al through a. filled .</sentence>
				<definiendum id="0">SUBJ</definiendum>
				<definiens id="0">an unfilled subject argument position</definiens>
			</definition>
			<definition id="6">
				<sentence>The derivation tree is defined as follows : each node contains a conditional interpretation , a current discourse model , and a derivation rule R , such that the node is consistent with the b~ ( Hklji~bo~ : z application of 1~ to the node 's daughters .</sentence>
				<definiendum id="0">derivation tree</definiendum>
				<definiendum id="1">b~ ( Hklji~bo~</definiendum>
				<definiens id="0">each node contains a conditional interpretation , a current discourse model , and a derivation rule R , such that the node is consistent with the</definiens>
			</definition>
			<definition id="7">
				<sentence>The Centering model ( \ [ 6\ ] , \ [ 7\ ] , \ [ 1\ ] ) of pronominal anaphora is a leading example , applying a variety of constraints dealing with such factors as recency , salience , and attention .</sentence>
				<definiendum id="0">Centering model</definiendum>
				<definiens id="0">recency , salience , and attention</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>HPSG is one of the best known uni fication-based grammar formalisms .</sentence>
				<definiendum id="0">HPSG</definiendum>
				<definiens id="0">one of the best known uni fication-based grammar formalisms</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , the set of modifiers for NPs and Ns ( i.e. , NPs lacking determiners ) includes adjectives , nominals , PPs and even VPs ( relative clauses ) .</sentence>
				<definiendum id="0">VPs</definiendum>
				<definiens id="0">NPs lacking determiners ) includes adjectives , nominals , PPs and even</definiens>
			</definition>
			<definition id="2">
				<sentence>The Rogers Technical Opera~ tions Database is a statistical database ; that is , each table in the database contains one or motx : category attributes ( columns ) whose values define sets of entities of a single type , and one or more statistic attributes ( columns ) whose values smnmarizc these sets .</sentence>
				<definiendum id="0">Rogers Technical Opera~ tions Database</definiendum>
				<definiens id="0">a statistical database ; that is , each table in the database contains one or motx : category attributes ( columns ) whose values define sets of entities of a single type</definiens>
			</definition>
			<definition id="3">
				<sentence>Chart parsing is a type of parsing in which all syntactic structures which are built are placed on a single graph structure called a chart .</sentence>
				<definiendum id="0">Chart parsing</definiendum>
				<definiens id="0">a type of parsing in which all syntactic structures</definiens>
			</definition>
			<definition id="4">
				<sentence>Classification-based Phrase Structure Grammar : An Extended Revised Version of HPSG .</sentence>
				<definiendum id="0">Classification-based Phrase Structure Grammar</definiendum>
				<definiens id="0">An Extended Revised Version of HPSG</definiens>
			</definition>
</paper>

		<paper id="2079">
</paper>

		<paper id="4215">
</paper>

		<paper id="4196">
</paper>

		<paper id="3141">
			<definition id="0">
				<sentence>A homonym is a list that includes a lexeme identifier , a part-of-speech marker , and morphological features of the wordform .</sentence>
				<definiendum id="0">homonym</definiendum>
				<definiens id="0">a list that includes a lexeme identifier , a part-of-speech marker , and morphological features of the wordform</definiens>
			</definition>
			<definition id="1">
				<sentence>The key role in parsing proper is played by a combinatorial ( syntactic ) dictionary that contains versatile information on syntactic properties of lexemcs , i.e. on their ability to participate in various syntactic constructions ( for details see Mel'~uk 19 '' /4 , 1988 ; Apresjan et al. 1989 , 1992 ) .</sentence>
				<definiendum id="0">combinatorial</definiendum>
				<definiens id="0">contains versatile information on syntactic properties of lexemcs</definiens>
			</definition>
			<definition id="2">
				<sentence>1992 consider nodes for which distance from X or Y exceeds 3 ( where distance is the number of links in the path connecting two nodes in the dependency tree of a fragment ) .</sentence>
				<definiendum id="0">distance</definiendum>
			</definition>
</paper>

		<paper id="2120">
			<definition id="0">
				<sentence>The speech recognition unit is a Japanese bunselsu , which roughly corresponds to a phrase and is the next largest unit after the word .</sentence>
				<definiendum id="0">speech recognition unit</definiendum>
				<definiens id="0">a Japanese bunselsu , which roughly corresponds to a phrase and is the next largest unit after the word</definiens>
			</definition>
			<definition id="1">
				<sentence>A Japanese phrase consists of one independent word ( e.g. noun , adverb , verb ) and zero , one or more than one dependent words ( e.g. postposition , auxiliary verb ) .</sentence>
				<definiendum id="0">Japanese phrase</definiendum>
				<definiens id="0">consists of one independent word ( e.g. noun , adverb , verb ) and zero , one or more than one dependent words ( e.g. postposition , auxiliary verb )</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>COLING 92 , NAN'II~S , 23 28 ao ( rr 1992 8 1 Pit &lt; It. OF COLING 92 , NANrES , AU &lt; ; . 23-28 , 1992 The basic mechanism of constraint logic programming is the restriction of the search space , or the reduction of the domain-variables. Tiffs goal can be reached differently depending on the active or passive constraint type ( ef \ [ Vanllentenryck89\ ] ) . In the classical logic programming framework , the basic technique is that of generate-and-test. Iu this ease , the program generates values for the variables before verifying some of their properties : the search space is reduced a posteriori. On the other hand , in the CLP paradigm , the use of constraints allows the reduction of this space a priori. Moreover , the set of constraints forms a system which incorporates new constraints ( luring the process , while the use of simple predicatcs verifying a property only has a local scope. This active/passive distinction can be useful for parsing , especially according to the type of knowledge that is constrained. Active constraints can easily be defined for syntactic structures and their formation. On the other hand , expressing relations between these structures with this kind of constraint is not always possible. We will describe the principles governing the formarion of the structures. A syntactic structure can be of two types : * simplestructures : lexical categories ( e.g. Del , N , V ... ) • complex structures : phrases or propositions ( e.g. NP , VP ... ) The formation of complex structures is governed by two types of knowledge : • internal : specific information within a structure • external : relations between structures Internal knowledge concerns the structure composition , independently of its context. For a phrase , it is the set of its constituents. External knowledge describes interactions between structures. They concern on the one hand the order and on the other hand tile government ( in the sense of phrase-structure grammars : selection , agreement ... ) . ID/LP formalism uses such a distinction : it separates information about immediate dominance ( i.e. the set of possible constituents of a phrase ) from that on linear precedence ( i.e. the partial order relation between these constituents ) . It is possible to consider these two types of knowledge as constraints ( cf \ [ Saint-Dizier91\ ] ) . But it is important to distinguish their respective funetionings. We will illustrate this point by presenting principles for each type. o Internal knowledge Each complex structure must contain at least one particular element called the head. This category gives the phrase its type and its presence is compulsory. The other constituents are usually optional. We must specify that local constraints could require the presence of a particular category , but it is a sub-categorization aspect : it concerns relations between the sub-structures of the complex structure and is not specific to the structure itself. We will see that this distinction between optional and compulsory constituents can be represented directly as an active constraint. o External knowledge In the case of ID/LP formalism , the order constraints ( i.e. linear precedence ) can not be easily used with an a priori reduction of the search space. Indeed , LP-rules define a partial order upon the set of categories. The LP-aeceptability relation uses this order and can be regarded as a constraint upon the domain-variables. It is a symbolic userdefined constraint. The use of this kind of constraint is possible in Chip ( ef \ [ Dincbas88\ ] ) , but not in Prolog III ( cf \ [ ColmerauergO\ ] ) . tlowever , using this order relation as an actual constraint allowing the reduction of domainvariables is difficult. In so far as it is a partial order , the LP notion can not be used to predict the categories that can follow a constituent. It is used during the parse to verify the possibility for each new category to appear at a given place in the syntactic structure. Generally speaking , internal properties allow an easier use of active constraints than external ones. As we have seen , ID-rules of ID/LP formalism only contain tile set of possible constituents ( without any notion of order ) . Therefore , an ID-rule is strictly equivalent to a clause. Example : N P `` -'*id Del , N , AP ~ N P V ~De~ V ~N V ~AP This equivalence is the basis of the conciseness and generality properties of GPSG. But it is difficult to represent. As we have seen , logic programming can not directly represent the non-ordered aspect of a clause. Ilowever , it is possible to represent this kind of information as active constraints. These must allow the expression of tile simple fact that a phrase is well-formed if it is at least composed of the constituents Ct , ... , C , . Other relations between the structures ( like order or selection ) will only be verified if this constraint is satisfied. ACT~.S DE COLING-92 , NANTES , 23-28 AOt~rr 1992 8 2 PROC. OF COLING-92 , NANTES , AUG. 23-28 , 1992 Practically , each rule descrihing a phrase cor : responds to a clause whose literals represent categories. An ID-rule is thus translated into a boolean formula where each category corresponds to a boolean. The semantics of this representatiou is the following : A literal is true if it corresponds to a well-formed structure. A structure is wellformed if it corresponds to a le~cical category ( simple structure ) or to a wellformed phrase ( compler structure ) . Thus , the boolean value of a complex structure is the interpretation of this formula , and so depends on the value of its constituents. Ezample : Given the following set of ID-rules describing a NP : N P -- q~ DeC N NP ~i , * N NP -old DeC AP , PP , N NP ~ia Det , AP , N NP -- qa Det , PP , N This set of rules corresponds to the following fornmla : ( 1 ) el A N ) V ( N ) V ( Det A AP A PP A N ) V ( Dot A AP A N ) V ( Det A PPA N ) D NP It is interesting to note that the ID/LP formalism strongly reduces the problem of PS-rules multiplication inherent in phrase-structure grammars , tlowever , as we have seen in tile previous example , there is still a redundancy in the information. Indeed , a set of rules describing a phrase allows us to distinguish between two types of constituents according to their ot ) tional or eomtmlsory aspect. Hence , for each phrase we can define a minimal set of compulsory constituents ( generally limited to the head of the phrase ) , which we call the minimal set of a phrase. Ezample : In the previous example , the minimal set of the NP is { N } . We introduce an additional restriction preventing the repetition of an identical category within a phr , ~se. This restriction is very strong and has to be relaxed for some categories ( such as PP ) . But it remains a general principle : most of the categories should not be repeated. We then construct a principle defining tile wellformedness of complex structures. 't'his principle only concerns internal knowledge : A phrase is well-formed iff it respects the following properties : m it contains at least one head • no constituent is repeated ~ , all its embedded phrases are well-formed In the logical paradigm ( equivalence between a role and a clause ) , we say that a literal is true ~ it corresponds to a lexieal category of the parsed sentence or if it correslmnds to a well-formed phrase. This formation rule allows its to simplify the veritication of the grammatieality of a sentence. We simply need to verify the presence of the minimal set of compulsory constituents to indicate the wellformedness of a phrase. The boolean value of the complete structure is then evaluated recursively. If all the intermediate structures are true , the complete structure is also true and corresponds to a gralomatical sentence. We will call realization the actual presence of a category in tile syntactic structure corresponding to a sentence. The verification process of the wellfornmdness of a phrase follows these steps set constituents within the minimal set stituents in a pllr , '~se 4~ verification of the well4ormedness of embedded phrases In an active constraint , we replace the set of clauses describing all the possible constructions with a system &lt; ) f constraints S defining the set of l ) ossihle constituents and the condition of realization for the minitelal set. We can represent it as follow : Let G ' he the set of possible constituents of a phrase XP , let X t &gt; e the head of XI ' , let M be the minimal set such xs M = { X } UC ' ( where C ' C C ) , and let zX be the disjtmction of the literals of M. The well-formedness constraint is : s = { A 7 ) xl , } Example : The well-formedness constraint for a Nt ' is : { NDNI ' } The well : formedness constraint for a PP is : { f'rel &gt; A N P D I ' P } ACIES DE COLING-92 , NAIqrES , 23-28 ^o~r 1992 8 3 PROC .</sentence>
				<definiendum id="0">order constraints</definiendum>
				<definiendum id="1">literal</definiendum>
				<definiendum id="2">phrase</definiendum>
				<definiens id="0">the restriction of the search space , or the reduction of the domain-variables. Tiffs goal can be reached differently depending on the active or passive constraint type</definiens>
				<definiens id="1">a system which incorporates new constraints ( luring the process</definiens>
				<definiens id="2">it separates information about immediate dominance ( i.e. the set of possible constituents of a phrase ) from that on linear precedence ( i.e. the partial order relation between these constituents</definiens>
				<definiens id="3">used to predict the categories that can follow a constituent. It is used during the parse to verify the possibility for each new category to appear at a given place in the syntactic structure. Generally speaking</definiens>
				<definiens id="4">well-formed if it is at least composed of the constituents Ct , ... , C , . Other relations between the structures</definiens>
				<definiens id="5">( 1 ) el A N ) V ( N ) V ( Det A AP A PP A N ) V ( Dot A AP A N ) V ( Det A PPA N ) D NP It is interesting to note that the ID/LP formalism strongly reduces the problem of PS-rules multiplication inherent in phrase-structure grammars</definiens>
				<definiens id="6">a set of rules describing a phrase allows us to distinguish between two types of constituents according to their ot ) tional or eomtmlsory aspect. Hence , for each phrase we can define a minimal set of compulsory constituents ( generally limited to the head of the phrase )</definiens>
				<definiens id="7">the set of clauses describing all the possible constructions with a system &lt; ) f constraints S defining the set of l ) ossihle constituents and the condition of realization for the minitelal set. We can represent it as follow : Let G ' he the set of possible constituents of a phrase XP , let X t &gt; e the head of XI ' , let M be the minimal set such xs M = { X } UC ' ( where C ' C C )</definiens>
			</definition>
			<definition id="1">
				<sentence>It is de tined , as follows ( LP relation between sets is noted with ~ : ) : Let P be a phrase , ga such that f ' -~ c~ then FLD , the set of first legal daughters , is defined , '~s R ) llows : m , D ( P ) = { e E ~ such that e - &lt; , , { e } } &lt; , Immediate precedence ( noted ll ' , , ( c ) ) : this fimetlon defines for each FLI ) c of a phrase P the set of categories that can precede e in P. It is defined as follows : Let P be a phrase , V ( * such that P -- ÷ o , let x be a non-terminal , let c E FLD ( P ) , then IPv ( e ) , the set of immediate precedence of c for P , is defined as follows : IPp ( c ) = { ... .. h that ( x -4 c ) or ( , c E ... ...</sentence>
				<definiendum id="0">Immediate precedence</definiendum>
				<definiens id="0">LP relation between sets is noted with ~ : ) : Let P be a phrase , ga such that f ' -~ c~ then FLD , the set of first legal daughters</definiens>
				<definiens id="1">{ e E ~ such that e - &lt; , , { e } } &lt;</definiens>
			</definition>
</paper>

		<paper id="3129">
			<definition id="0">
				<sentence>The IPS system is a large-scale interactive GB-based parsing system ( English , French ) under development at the University of Geneva .</sentence>
				<definiendum id="0">IPS system</definiendum>
				<definiens id="0">a large-scale interactive GB-based parsing system ( English , French ) under development at the University of Geneva</definiens>
			</definition>
			<definition id="1">
				<sentence>The IPS ( Interactive Parsing System ) research project , at the Linguistics Departement of the University of Geneva , aims at developing a large , interactive , parsing model based on Chomsky 's Government and Binding ( henceforth GB ) linguistic theory t. This project , which focuses both on English and French , has theoretical as well as practical goals , including the following : • to show the feasibility and the soundness of a GB-based approach to natural language parsing ; *The research described in this paper has been supported in part by n grant from the Swiss national science foundation ( grant no 11-25362.88 ) .</sentence>
				<definiendum id="0">IPS</definiendum>
				<definiens id="0">the soundness of a GB-based approach to natural language parsing</definiens>
			</definition>
			<definition id="2">
				<sentence>We assume the X schema in ( 1 ) : ( 1 ) XP -- ~ Spec X -- + X Compl where X is a lexical or a functional category , Spee and Compl are lists ( possibly empty ) of maximal projections ( YP ) .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the X schema in ( 1 ) : ( 1 ) XP -- ~ Spec X -- + X Compl where</definiens>
				<definiens id="1">a lexical or a functional category , Spee and Compl are lists ( possibly empty ) of maximal projections ( YP )</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>say ( it , B , u , p ) -- A- &gt; aThis is an utterance action version of the STRIPS assumption .</sentence>
				<definiendum id="0">aThis</definiendum>
				<definiens id="0">an utterance action version of the STRIPS assumption</definiens>
			</definition>
			<definition id="1">
				<sentence>SUPPORT is a general relation that holds between beliefs and intentions in this model .</sentence>
				<definiendum id="0">SUPPORT</definiendum>
				<definiens id="0">a general relation that holds between beliefs and intentions in this model</definiens>
			</definition>
			<definition id="2">
				<sentence>STiffs is a simplification of the COLLABOnATIVE PLANNING PRINC~PLE~ described in \ [ 15\ ] .</sentence>
				<definiendum id="0">STiffs</definiendum>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>Japanese predicate phrases consist of a main verb followed by a sequence of auxiliaries azld sentence final particles .</sentence>
				<definiendum id="0">Japanese predicate phrases</definiendum>
				<definiens id="0">consist of a main verb followed by a sequence of auxiliaries azld sentence final particles</definiens>
			</definition>
			<definition id="1">
				<sentence>Voice auxiliaries precede all other auxiliaries , and within this category , the causative auxiliary ( sa ) se , 'u precedes the passive auxiliary ( ra ) re~t. Aspect auxiliaries , such a.s the progressive auxiliary ( Ie ) ivu precede modal auxiliaries ; rod follow voice auxiliaries .</sentence>
				<definiendum id="0">Voice auxiliaries</definiendum>
				<definiendum id="1">causative auxiliary</definiendum>
				<definiendum id="2">'u</definiendum>
				<definiens id="0">precedes the passive auxiliary ( ra ) re~t. Aspect auxiliaries , such a.s the progressive auxiliary</definiens>
			</definition>
			<definition id="2">
				<sentence>Mood1 iuehldes the optative arlxiliaries , such as tai ( want ) , beki ( should/must ) , etc .</sentence>
				<definiendum id="0">Mood1</definiendum>
			</definition>
			<definition id="3">
				<sentence>OF COLING-92 , NANTI~S , AUG. 23-28 , 1992 kernel &lt; voice &lt; aspect &lt; moodl &lt; negate &lt; tense &lt; mood2 &lt; tense ( sa ) seru ( te ) iru tai nai ta rasii ta ( ra ) reru ( te ) morau tagaru n da desu u masu darou Figure l : The predicate hierarchy of Japanese AUXV-caus AUXV-deac AUXV-aspc AUXV-dont causative auxiliary : ( sa ) sei'u passive auxiliary : ( ra ) reru aspect auxiliary : ( ~e ) iru , Oe ) a ( u benefactive auxiliary : ( le ) tnorau AUXV-optt optative auxiliary : tai , beki AUX % negt negative auxiliary : nai , n AUXV-tense tense auxiliary : la , da AUXV-evid evidential auxiliary : rashii , darou AUXV-copl copulative auxiliary : da , desu Table 2 : Subcategories of auxiliaries in the mediumgrained grammar and Unification Descriptions Unification is an expensive operation , so the point of evaluating feature descriptions during CFG parsing has serious affects on the overall performance .</sentence>
				<definiendum id="0">Unification Descriptions Unification</definiendum>
				<definiens id="0">an expensive operation , so the point of evaluating feature descriptions during CFG parsing has serious affects on the overall performance</definiens>
			</definition>
			<definition id="4">
				<sentence>AcpOneStep carries out a cycle of rule invocation which consists of getting a new pending edge ( GetPendingEdge ) , adding it to the chart ( AddEdge ) , combining active and inactive edges ( TryToContinueActiveEdge/TryToContinuelnactiveEdge ) , and proposing new edges ( ProposeProductions ) .</sentence>
				<definiendum id="0">AcpOneStep</definiendum>
				<definiens id="0">carries out a cycle of rule invocation which consists of getting a new pending edge ( GetPendingEdge ) , adding it to the chart ( AddEdge ) , combining active and inactive edges ( TryToContinueActiveEdge/TryToContinuelnactiveEdge ) , and proposing new edges ( ProposeProductions )</definiens>
			</definition>
			<definition id="5">
				<sentence>Unification is an associative and comnmtative operation , so the same results from the featuresensitive lazy unification are assured .</sentence>
				<definiendum id="0">Unification</definiendum>
				<definiens id="0">an associative and comnmtative operation , so the same results from the featuresensitive lazy unification are assured</definiens>
			</definition>
</paper>

		<paper id="2110">
			<definition id="0">
				<sentence>A standard case-based parser consists basically of a pattern marcher and a case base which stores a large number of linguistic pa~tern-concept pairs .</sentence>
				<definiendum id="0">standard case-based parser</definiendum>
			</definition>
			<definition id="1">
				<sentence>CAPIT combines a simple keyword analyzer , KBP ( Keyword-Based Parsing module ) , with a case-based parser , CBP ( Case-Based Parsing module ) .</sentence>
				<definiendum id="0">CAPIT</definiendum>
				<definiens id="0">combines a simple keyword analyzer</definiens>
			</definition>
			<definition id="2">
				<sentence>The ease-based parser ( CBP ) is a supplemental module to KBP .</sentence>
				<definiendum id="0">ease-based parser</definiendum>
				<definiendum id="1">CBP</definiendum>
			</definition>
			<definition id="3">
				<sentence>CBP is a conventional case-based parser .</sentence>
				<definiendum id="0">CBP</definiendum>
				<definiens id="0">a conventional case-based parser</definiens>
			</definition>
			<definition id="4">
				<sentence>If CBP finds a pattern which matches with a part of tile query in its case base , CBP replaces the matched part of the NL query with ttle corresponding concept , then passes the modified NL query to KBP ( Step-3 ) .</sentence>
				<definiendum id="0">CBP</definiendum>
				<definiens id="0">a pattern which matches with a part of tile query in its case base</definiens>
				<definiens id="1">replaces the matched part of the NL query with ttle corresponding concept</definiens>
			</definition>
			<definition id="5">
				<sentence>CBP processes only those queries which KBP fails to interpret .</sentence>
				<definiendum id="0">CBP</definiendum>
				<definiens id="0">processes only those queries which KBP fails to interpret</definiens>
			</definition>
			<definition id="6">
				<sentence>The aPl ) lieation designer judges that the part of the query mnst be defined as a pattern-concept pair .</sentence>
				<definiendum id="0">aPl ) lieation designer judges</definiendum>
				<definiens id="0">a pattern-concept pair</definiens>
			</definition>
			<definition id="7">
				<sentence>PDI and an Application Designer PDI ( Pattern Definition interviewer ) is CAPIT 's interface to all application designer .</sentence>
				<definiendum id="0">PDI</definiendum>
				<definiendum id="1">Application Designer PDI</definiendum>
				<definiens id="0">CAPIT 's interface to all application designer</definiens>
			</definition>
			<definition id="8">
				<sentence>The guidance system has an internal database containing data about the functions and the elements of tile specific VCR .</sentence>
				<definiendum id="0">guidance system</definiendum>
				<definiens id="0">has an internal database containing data about the functions and the elements of tile specific VCR</definiens>
			</definition>
			<definition id="9">
				<sentence>CAPIT is a eombiuation of a keyword analyzer and a case-based parser .</sentence>
				<definiendum id="0">CAPIT</definiendum>
				<definiens id="0">a eombiuation of a keyword analyzer and a case-based parser</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>ING 92 , NAN-rEs , AU ( I. 23-28 , 1992 The relation between word forms aml canonical forms is many-to-many : ortimgrapllic variants are mapped onto a single canonical form , and a single word form call be related to ~veral lexical entries via inflectional rule* s. The monolingual dictionary is a net of lexical entries , which are pairings of canonical word forms of a language and their designations , and in addition describe their grarnrnatical properties .</sentence>
				<definiendum id="0">AU</definiendum>
				<definiens id="0">canonical word forms of a language and their designations , and in addition describe their grarnrnatical properties</definiens>
			</definition>
			<definition id="1">
				<sentence>Bilingual dictionaries can be viewed as a relation between words in two languages .</sentence>
				<definiendum id="0">Bilingual dictionaries</definiendum>
			</definition>
			<definition id="2">
				<sentence>E.g. according to tile Van Dale Dutch-Spanish dictionary , reading one synonymous reading in Dutch and three synonyms in Spanish .</sentence>
				<definiendum id="0">E.g.</definiendum>
				<definiens id="0">according to tile Van Dale Dutch-Spanish dictionary , reading one synonymous reading in Dutch and three synonyms in Spanish</definiens>
			</definition>
			<definition id="3">
				<sentence>Wordnet : A lexieal database organized on psycholinguistic principles .</sentence>
				<definiendum id="0">Wordnet</definiendum>
			</definition>
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>Abduction of temporal relations between segments is a feasible and precise method for discourse segmentation .</sentence>
				<definiendum id="0">segments</definiendum>
				<definiens id="0">a feasible and precise method for discourse segmentation</definiens>
			</definition>
			<definition id="1">
				<sentence>A discourse segment is a discourse object .</sentence>
				<definiendum id="0">discourse segment</definiendum>
			</definition>
			<definition id="2">
				<sentence>Next segments will be defined as well as their construction and use in the semantic interpretation .</sentence>
				<definiendum id="0">Next segments</definiendum>
				<definiens id="0">well as their construction and use in the semantic interpretation</definiens>
			</definition>
			<definition id="3">
				<sentence>tim~e2~t2~ tl &lt; t2 ~m~sv. Av~cd. ~m~e~p. ApffiVcd. { a } fh ©ven~© I , buy~j,11mb I ) ) I \ [ time ( el , tl ) Itease=~ , An=perf.. ¢vent ( e 1 , tmY ( J , umlbl ) ) I ~v~/l¢ ( c2 , k~o~ ( J , ~a~b2 ) ) I tln~el , tl ) ~ime ( e2 , t2 ) , ; 2 &lt; 11 tense~p. Ap=ved. I~nse-pf , AP=verf. tb ) Figure 1 : Segmentation for ex. I and lI feature time contains the set of all subsegments ' times. The tense feature refers to the tense of the last subsegment. • sequence the feature eventuality is the composition of all the segments ' eventualities obtained by the seq operator. The time feature interval is the time interval \ [ t , ,~ , t , ,s \ ] , where t , h is the initial point of the first segment time interval and t , ,s is the final point of the last segment time interval. The tense feature is the tense of the last segment ( fig. 1.a ) . This segment can be augmented by adding a new segment to the list of segments. In this case the features of the sequence segment have to be evaluated again s. • overthe feature eventuality is unknown , the feature time is the time interval of the intersection of all time segments. The tense feature is the tense of its last subsegment. The segments that have two subsegments are fb , bk , and elab. These segments can only he augmented by the replacement of its second subsegment by a new one obeying the same set of constraints. The replaced segment is the first subsegment of the new one. These segments have the same features of the first subsegment ( fig. 1.b for a segment of sort fb ) . Discourse segmentation will be done incrementally. A sentence will be represented by a segment. The processed discourse will be represented by a segment. 3A notion similar to the sequence segntent is used in other theorl¢~ for discourse segmentation named in RST \ [ 15\ ] u `` narration '' , in Hobba \ [ 9\ ] also As `` narration '' and in planning \ [ 10\ ] as U~quence'. The steps for the discourse segmentation are : segment will be the discourse segment. discourse sentence. segment. This step will have the following substeps : ( a ) To compute the set of visible segments ( i.e. the right subsegments of the discourse segment ) from the discourse segment. This set of segments can be ordered by some discourse preference rule , e.g. we may prefer to continue the current segment or prefer to continue the first opened segment and close the other ones 4 . ( b ) to choose one segment sl from the set of visible segments. ( c ) to add the new sentence segment s2 to segment s I by : • continuing the subsegment list of Sl if s2 can satisfy the constraints of sl. • substituting sj by a new one as. sz contains s I as first subsegment and s2 as second subsegment in its subsegnlent list. The sort of segment s3 is one of the 6 nonbasic ones. ( d ) if it is not possible to add the new segment then choose another segment from the set of the visible ones , call it sl and try again going back to step 3c. to process in the discourse. The main process in discourse segmentation is to check for temporal relations between segments because this is the only criterion used for segmentation. For deciding how to link segment s2 given segment sl , do : • if sl is going to be continued by s2 , then the referent will be the last subsegment in the subsegment list of segment Sl. • if sl is going to be substituted by a new segment sa then the referent will be Sl. 4The way segments are ordered will have consequenc~ on the discourse segmentation. In caAes where there are more then one possible segmentation preference will be given to the first one. ACTF.S DE COLING-92 , NANTES , 23-28 ^OfYr 1992 3 3 4 PROC. OF COLING-92 , NANTES. AUG. 23.28 , 1992 poral relation between s2 and the referent. the referent , do : * if sl is to be continued by s2 then cheek if s2 satisfies the set of constraints of st. If so , update the st features if needed and repeat the procedure ( update features in the parent node and check constraints ) until the root segment or a node segment whose features do n't need to be updated is reached. If this process terminates successfully then s2 can continue st. • if s2 is to be replaced by a new segment sa then compute the features of segment sa and check if the set of temporal constraints of the old parent of segment sl is satisfied. If so , update the old parent st features if needed and repeat the procedure ( update features in the parent node and check constraints ) until the root or a node segment whose features do n't need to be updated is reached. If this process ends sueceasfully then sa can replace st. Whenever a temporal entity in the form of a discourse referent is added to the discourse structure , the structure containing all temporal discourse referents as well as their temporal constraints should be updated ( fig. 4 for ex. of see. 4 ) . This way we can distinguish relations that are implied by the temporal system from those inferred using other knowledge sources , e.g. the constraints tl &lt; t2 , ta &lt; t2 do not imply tl &lt; ta , but satisfy it. In order to check for temporal constraints we do not only deal with constraints over temporal intervals but use also world knowledge for abducting relations between eventualities that imply some temporal relation between them. In order to abduct a temporal relation it is not enough to block inconsistencies in the temporal system. There should also be some kind of justification , like : * temporal if the temporal system implies relation t0 , @ t , ~ there is a justification to abduct tot @ t , ~ , with @ being a temporal relation. • causal if et can cause e2 then there is a justification to abduct t~ &lt; t~. • contingent Assuming an event ontology like that of Moens and Steedman \ [ 17\ ] where eventualities have a tripartite structure with a preparatory phase , a culmination and a consequence state there are clauses stating what are the eventualities of the preparatory phase and those of the consequence state of an eventuality. if el can be in the preparatory phase of e2 there is a justification to abduct t~ C re2. if el can be in the consequence state of e2 there is a justification to abduct tet &gt; re2 .</sentence>
				<definiendum id="0">,s</definiendum>
				<definiendum id="1">segment</definiendum>
				<definiens id="0">the composition of all the segments ' eventualities obtained by the seq operator. The time feature interval is the time interval \ [ t , ,~ , t</definiens>
				<definiens id="1">a ) To compute the set of visible segments ( i.e. the right subsegments of the discourse segment ) from the discourse segment. This set of segments can be ordered by some discourse preference rule</definiens>
				<definiens id="2">the set of constraints of st. If so , update the st features if needed and repeat the procedure ( update features in the parent node and check constraints ) until the root segment or a node segment whose features do n't need to be updated</definiens>
				<definiens id="3">satisfied. If so , update the old parent st features if needed and repeat the procedure ( update features in the parent node and check constraints ) until the root or a node segment whose features do n't need to be updated is reached. If this process ends sueceasfully then sa can replace st. Whenever a temporal entity in the form of a discourse referent is added to the discourse structure , the structure containing all temporal discourse referents as well as their temporal constraints should be updated ( fig. 4 for ex. of see. 4 ) . This way we can distinguish relations that are implied by the temporal system from those inferred using other knowledge sources , e.g. the constraints tl &lt; t2 , ta &lt; t2 do not imply tl &lt; ta</definiens>
			</definition>
			<definition id="4">
				<sentence>Sl is the reference to anchor s2 , the eventuality of s~ is a state , so the relation t~t C t , ~ is abducted and the new segment is of sort bk ( fig .</sentence>
				<definiendum id="0">Sl</definiendum>
				<definiens id="0">a state</definiens>
			</definition>
			<definition id="5">
				<sentence>Segment bk is the reference for sa because the eventuality of segment s2 is a state with an imperfective aspectnal perspective and there is no general knowledge about the eventualities of s2 and sa allowing us to abduct a temporal relation between them .</sentence>
				<definiendum id="0">Segment bk</definiendum>
			</definition>
</paper>

		<paper id="3163">
			<definition id="0">
				<sentence>... ... ... . ( moving pict ure~ % '~talist ic sveciflcationJ Figure 1 : interaction between lexicon and image Understanding a text relative to spatial motion consists of : i ) a semantic representation of this text ; ii ) movement with pictures ; this animation aims to reproduce what is understood during the reading of the text .</sentence>
				<definiendum id="0">animation</definiendum>
			</definition>
			<definition id="1">
				<sentence>x surgit means a sudden transition , a movewent entering the perception zone of an entity able to perceive .</sentence>
				<definiendum id="0">x surgit</definiendum>
				<definiens id="0">means a sudden transition , a movewent entering the perception zone of an entity able to perceive</definiens>
			</definition>
			<definition id="2">
				<sentence>definition of perception : The localisation PERCEP ( x ) is defined as a result of the intersection 7 of an external area to be perceived and of the properties of perception of the person perceiving ( its senses ) .</sentence>
				<definiendum id="0">definition of perception</definiendum>
				<definiendum id="1">localisation PERCEP ( x )</definiendum>
			</definition>
</paper>

		<paper id="2091">
			<definition id="0">
				<sentence>The main features used in the signs are ORTHOGRAPHY , CAT ( the categorial grammar syntax ) , OItDER ( the directionality of the `` slash '' , which specifies linear ordering ) , FEATS ( a set of syntactic features ) , CASES ( a case-assignment mechanism built on top of standard UCG ) , and SEM , a unification-based semantics with a neoDavidsonian treatment of roles ( \ [ Parsons 80 , Dowty 89\ ] ) .</sentence>
				<definiendum id="0">OItDER</definiendum>
				<definiendum id="1">FEATS</definiendum>
				<definiendum id="2">SEM</definiendum>
				<definiens id="0">the categorial grammar syntax )</definiens>
				<definiens id="1">the directionality of the `` slash '' , which specifies linear ordering</definiens>
				<definiens id="2">a set of syntactic features</definiens>
				<definiens id="3">a unification-based semantics with a neoDavidsonian treatment of roles</definiens>
			</definition>
			<definition id="1">
				<sentence>The operation U stands for set union , and `` all the propositions in the semantics are interpreted here as being conjoined .</sentence>
				<definiendum id="0">operation U</definiendum>
			</definition>
			<definition id="2">
				<sentence>The case-assignment mechanism , which identifies the indices of the NPs , can be used to interact with the ORDER feature if this is desired .</sentence>
				<definiendum id="0">case-assignment mechanism</definiendum>
				<definiens id="0">identifies the indices of the NPs</definiens>
			</definition>
			<definition id="3">
				<sentence>For instance , if a word has more than one translation depending on how various semantic featnres become instantiated , the bilinguM lexical entries may express these restrictions .</sentence>
				<definiendum id="0">bilinguM lexical</definiendum>
				<definiens id="0">if a word has more than one translation depending on how various semantic featnres become instantiated , the</definiens>
			</definition>
			<definition id="4">
				<sentence>Such an entry consists of pointers to monolingual signs ( for instance , ( 9 ) pnts signs ( 1 ) and ( 6 ) into correspondence ) , together with constraints about the semantic indices contalned in these signs .</sentence>
				<definiendum id="0">Such an entry</definiendum>
			</definition>
			<definition id="5">
				<sentence>Generation then takes place starting from the bag of TL lexical entries , which have their semantic indices instantiated as a result of the parsing and look-up process .</sentence>
				<definiendum id="0">Generation</definiendum>
				<definiens id="0">takes place starting from the bag of TL lexical entries , which have their semantic indices instantiated as a result of the parsing and look-up process</definiens>
			</definition>
			<definition id="6">
				<sentence>The CKY parsing algorithm may be eharacterised a~ follows : it uses a chart or table where all well-formed substrings ( WFSs ) that are found are recorded , together with their position ( i.e. the words that they span in the string ) .</sentence>
				<definiendum id="0">CKY parsing algorithm</definiendum>
				<definiens id="0">it uses a chart or table where all well-formed substrings ( WFSs ) that are found are recorded , together with their position ( i.e. the words that they span in the string )</definiens>
			</definition>
</paper>

		<paper id="2094">
			<definition id="0">
				<sentence>Government Module Government Theory is a central notion to the Case and Trace modules .</sentence>
				<definiendum id="0">Government Module Government Theory</definiendum>
			</definition>
			<definition id="1">
				<sentence>The Case Filter rules out any sentence that contains a non-casemarked noun phrase .</sentence>
				<definiendum id="0">Case Filter</definiendum>
				<definiens id="0">rules out any sentence that contains a non-casemarked noun phrase</definiens>
			</definition>
			<definition id="2">
				<sentence>OF COLING-92 , NANTES , AUG. 23-28 , 1992 Module After case has been assigned , the Trace module applies the empty category principle ( ECP ) which checks for proper government of empty elements .</sentence>
				<definiendum id="0">Trace module</definiendum>
				<definiens id="0">applies the empty category principle ( ECP ) which checks for proper government of empty elements</definiens>
			</definition>
			<definition id="3">
				<sentence>The LCS Linking rule and the CS~ function are the two fundmnental principles of the lexical= semantic component .</sentence>
				<definiendum id="0">LCS Linking rule</definiendum>
				<definiens id="0">the two fundmnental principles of the lexical= semantic component</definiens>
			</definition>
			<definition id="4">
				<sentence>Choranky , Noam A. ( 1982 ) ~Some Concepts and Consequences of the Theory of Government and Binding , '' MIT PresL Dorr , Bonnie J. ( 1987 ) `` UNITRAN : A Principle-Ba~d App~ach to Machine Translation , '' AI Technical Report 1000 , MMter of Science thesis , Department of Electrical Engineering and Computer Science , Mauachusetts Institute of Technology .</sentence>
				<definiendum id="0">UNITRAN</definiendum>
				<definiens id="0">A Principle-Ba~d App~ach to Machine Translation , '' AI Technical Report 1000</definiens>
			</definition>
</paper>

		<paper id="4186">
			<definition id="0">
				<sentence>FOULUP extended a lexicon by referring restrictions placed on unknown words by instantiating scripts that matched the sentences containing the nnknown words .</sentence>
				<definiendum id="0">FOULUP</definiendum>
				<definiens id="0">extended a lexicon by referring restrictions placed on unknown words by instantiating scripts that matched the sentences containing the nnknown words</definiens>
			</definition>
			<definition id="1">
				<sentence>h strategies used in an existing interactive tool for lexical acquisition ( VEX \ [ Carter 89 } ) .</sentence>
				<definiendum id="0">strategies</definiendum>
			</definition>
			<definition id="2">
				<sentence>The Core Language Engine is a general purpose natural language processing system for English developed by SRI Cambridge .</sentence>
				<definiendum id="0">Core Language Engine</definiendum>
				<definiens id="0">a general purpose natural language processing system for English developed by SRI Cambridge</definiens>
			</definition>
			<definition id="3">
				<sentence>The object of the CLE is to map certain natural language expressions into appropriate predicates in logical form ( or Quasi-Logical Form \ [ Alshawi , ( .</sentence>
				<definiendum id="0">CLE</definiendum>
				<definiens id="0">to map certain natural language expressions into appropriate predicates in logical form</definiens>
			</definition>
			<definition id="4">
				<sentence>VEX allows for the creation of lexical entries by users with knowledge both of a natural language and of a Sl ) ecilic application domain , but not of linguistic theory or of tile way lexical entries are represented in the CLE .</sentence>
				<definiendum id="0">VEX</definiendum>
				<definiens id="0">allows for the creation of lexical entries by users with knowledge both of a natural language and of a Sl ) ecilic application domain , but not of linguistic theory or of tile way lexical entries are represented in the CLE</definiens>
			</definition>
			<definition id="5">
				<sentence>S1CS ( L ~ Lexicon Learning ) used a set of words. '</sentence>
				<definiendum id="0">S1CS</definiendum>
			</definition>
			<definition id="6">
				<sentence>,1 template is a tree spanning the parse with a mother category as root and a collection of its ancestor nodes 2t~xplanation-lmsed learning is n machine learning techIllqlle closely related to tllaCro-operator learllil|g , chtlllkillg , and parliM evaluation and is described in e.g..</sentence>
				<definiendum id="0">,1 template</definiendum>
				<definiens id="0">n machine learning techIllqlle closely related to tllaCro-operator learllil|g , chtlllkillg</definiens>
			</definition>
</paper>

		<paper id="2071">
			<definition id="0">
				<sentence>• Commen~ons par d6crire le type universel T : Statut 8 ingr ( T ) Universe ingr ( T ) Cat6gorie E ingffl- ) Autres-noms e ingr ( T ) Structurels e ingr ( T ) Fonctifs e ing ( T ) Les woes de valeurs : Type-de-valeurs ( Statut ) 8 Valeur-statut Type-de-valears ( Univers ) 8 Valeur-univers Type-de-valears ( Sttuctarels ) E Type-structurels Type-de-valeurs ( Fonctifs ) e Type-sch6ma-action Valeur-statut _ &lt; `` lType-ind0NT ) e V , 'deur-statut Type-ind ( EXT ) c Valeur-statut Valeur-univers _ &lt; T Type-structurels _ &lt; T Type-sch6ma-action _ &lt; T • Soit le type Personne - &lt; T Persoune : Statut E INT Universe Uo Nom e ingr ( Structurels ) Type-de-valeurs ( Nom ) e Liste-de-noms Pr6nom e ing ( Structurels ) Type-de-valeurs ( Pr6nom ) e Liste-de-noms Age e ingr ( Structurels ) Type-de-valeurs ( Age ) e lnt-O-150 Date-naiss e ingr ( Structurels ) Type-de-valears ( Date-naiss ) e Date Jour e ingr ( Date-naiss ) Type-de-valeurs ( Joar ) e Int1-31 PROC .</sentence>
				<definiendum id="0">Structurels ) Type-de-valeurs</definiendum>
				<definiens id="0">Universe ingr ( T ) Cat6gorie E ingffl- ) Autres-noms e ingr ( T ) Structurels e ingr ( T ) Fonctifs e ing ( T ) Les woes de valeurs : Type-de-valeurs ( Statut ) 8 Valeur-statut Type-de-valears ( Univers ) 8 Valeur-univers Type-de-valears ( Sttuctarels ) E Type-structurels Type-de-valeurs ( Fonctifs ) e Type-sch6ma-action Valeur-statut _ &lt; `` lType-ind0NT ) e V , 'deur-statut Type-ind ( EXT ) c Valeur-statut Valeur-univers _ &lt; T Type-structurels _ &lt; T Type-sch6ma-action _ &lt; T • Soit le type Personne - &lt; T Persoune : Statut E INT Universe Uo Nom e ingr</definiens>
			</definition>
</paper>

		<paper id="2076">
			<definition id="0">
				<sentence>Disjunction is a representation tool in the representation language , intended to describe sets of feature structures .</sentence>
				<definiendum id="0">Disjunction</definiendum>
				<definiens id="0">a representation tool in the representation language , intended to describe sets of feature structures</definiens>
			</definition>
			<definition id="1">
				<sentence>We will say that ( hid , { nit ... .. nit } ) is a k-arc from hid to ni t ... .. nit , that hid is an immediate predecessor of i'ti I ... .. nit , and that nit ... .. nik are immediate successors of hid .</sentence>
				<definiendum id="0">hid</definiendum>
				<definiens id="0">a k-arc from hid to ni t ... .. nit</definiens>
				<definiens id="1">an immediate predecessor of i'ti I ... .. nit , and that nit ... .. nik are immediate successors of hid</definiens>
			</definition>
			<definition id="2">
				<sentence>Informally , the factor operator extracts a factor common to all the top-level disjanets , and raises it to the root level .</sentence>
				<definiendum id="0">factor operator</definiendum>
				<definiens id="0">extracts a factor common to all the top-level disjanets , and raises it to the root level</definiens>
			</definition>
			<definition id="3">
				<sentence>u_L : o~J | II B : b4 I I lLh : eqJ~L 1111 B : bl C : cl E : 01 B : b2 C : c2 E : e : d2 C : c3 E : e C : C4 E : e Fig .</sentence>
				<definiendum id="0">u_L</definiendum>
			</definition>
			<definition id="4">
				<sentence>of disjunctive feature structures Definition 5.4 We will say that a FNF feature structure x subsumes a FNF feature structure y , and note x &lt; y , if ( 1 ) x &lt; -a.fY ( 2 ) If x ) - &lt; : , , , Ify ) Definition 5,5 Let x and y be two FNF feature structures. The unification of x mid y , noted x LI y , is the greatest lower bound of x and y according to the subsumption relation. The generalization of S1 and $ 2 , noted x INy , is the least upper bound of x and y according to the format subsumption relation. The following proposition states thatx LI y is dnfequivalent to the dnf-unification of the DNFs of x and y , and the format ofx IJ y is the unification of the formats of x and y : Proposition 5.1 ( 1 ) DNF ( x L\ ] y ) = DNF ( x ) Lid , ,/-DNF ( y ) ( 2 ) f ( x U y ) = f ( x ) Lip. , y ( x ) As a result , the unification of x and y can be computed by completely unformatting both x and y , unifying them , and formatting the result according to the unification of their formats : Proposition 5.2 x U y = v f ( x ) U\ [ , .tf ( y ) ( Vf ( x ) ( x ) LIdn f V f ( y ) ( y ) ) ( Dual proposition holds for generalization. ) Proposition 5.3 The class of FNF feature structures is closed under factoring , unfactoring , unification , and generalization. This follows directly from the definitions. Proposition 5.4 ( 1 ) 7s ( XMy ) = Z~ ( x ) LI~ ( y ) ( 2 ) ~ ( x Lly ) = ~ ( x ) Ll~ ( y ) ( 3 ) eAx LI y ) = es ( x ) u O , , ( Y ) ( 4 ) ~s ( X lly ) : es ( x ) LlOs ( y ) ( Dual propositions hold for generalization. ) Proposition 5.2 does not imply that complete unfactoring and re-factoring is the most efficient computation of unification and generalization. Because of the properties given in proposition 5.4 , unification can be carried out layer by layer , and only partial unfactoring is needed ( algorithm 5.1 ) . In the extreme case , when the formats of x and y are compatible , no unfactoring is needed , and the procedure match-formats does nothing. ACyF~ DE COLING-92 , Nnl , rn~s , 23-28 AOt7 1992 5 0 3 Pe.oc , OI ; COLING-92 , NAN'I~.S. AUG. 23-28 , 1992 Algorithm 5.1 Unification of FNF feature structures function unify ( x , y : feature-structure ) : feature-structure match-formats ( x , y ) //Unify AND-parts z.AND ~ dag-unify ( x.AND , y.AND ) if z.AND =failure then return failure //Unify OR-parts z.OR 4-unify-disjuncts ( x.OR , y.OR ) If z.OR =failure then return failure else return z function unify-disjuncts ( x , y : feamre-structare ) : feature-structure //assume x.AND and y.AND are empty match-formats ( x , y ) k~0 for each x , DISJi for each y.DISJj t 4-dag-unify ( x.DISJi.AND , y.DISJj.AND ) If t ~ : failure then u ~-unify-disjuncts ( x.DISJi.OR , y.DISIj.OR ) if u ~ : failure then k~.-k+l z.DISJk.AND 4-t I / \ [ I z'DISJk'OR 4-u if k = 0 then return failure elsereturu z We will consider the complexity of this algorithm in terms of the number of dag-unifications , which is the only costly operation ( O ( n log ( n ) ) , where n is the total number of symbols in the two dag feature structures-see AIt-Kaci , 1984 ) . We will first consider the case where the formats are compatible. One dag-unification is performed in the unify function , but the bulk of the dagunifications are performed in the unify-disjuncts function. There are two nested loops , and the function is applied recursively through all the layers. Therefore , in the worst case , the algorithm requires O ( d 2 ) ragunifications , whre d is the total number of disjancts. When the formats are not compatible , some unfactoring and ungrouping has to be performed by the match-formats function in order to force the formats to match. The number of operations can be limited if the two formats are partially compatible , due to the properties of FNF. Complete unformatting will be necessary only in cases where the two formats are completely incompatible. For example , if f ( x ) = &lt; { A } , { B , C } , { D , E } , { F } , { G } , { H } &gt; , and f ( y ) = &lt; { I } , { B , J } , { D , F } , { E , K } , { G } , { L } &gt; , the resulting format is &lt; { A , I } , { B , C , J } , { D } , { E , F , K } , { G } , { H , L } &gt; .</sentence>
				<definiendum id="0">DNF</definiendum>
				<definiendum id="1">feature-structure )</definiendum>
				<definiendum id="2">n</definiendum>
				<definiens id="0">the greatest lower bound of x and y according to the subsumption relation. The generalization of S1</definiens>
				<definiens id="1">the unification of x and y can be computed by completely unformatting both x and y , unifying them , and formatting the result according to the unification of their formats : Proposition 5.2 x U y = v f ( x ) U\ [ , .tf ( y ) ( Vf ( x ) ( x ) LIdn f V f ( y ) ( y ) ) ( Dual proposition holds for generalization.</definiens>
				<definiens id="2">LI y ) = es ( x ) u O , , ( Y ) ( 4 ) ~s ( X lly</definiens>
			</definition>
</paper>

		<paper id="2107">
			<definition id="0">
				<sentence>Case-Based/Example-Based Machine Translation ( CBMT/EBMT ) has been proposed as a way of overcoming the knowledge acquisition bottleneck in machine translation .</sentence>
				<definiendum id="0">Case-Based/Example-Based Machine Translation</definiendum>
			</definition>
			<definition id="1">
				<sentence>rL ) The importance of a link ( IL ) is the probability of occurrence of eases that occurred in the subtree of PTH , ( j ) .</sentence>
				<definiendum id="0">IL )</definiendum>
				<definiens id="0">the probability of occurrence of eases that occurred in the subtree of PTH</definiens>
			</definition>
			<definition id="2">
				<sentence>S IL=-c , where S is the total number of cases in the subtree connected with the link , and C~ is the total number of LPTCIs extracted from tile case-base according to TP .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">C~</definiendum>
				<definiens id="0">the total number of cases in the subtree connected with the link</definiens>
				<definiens id="1">the total number of LPTCIs extracted from tile case-base according to TP</definiens>
			</definition>
			<definition id="3">
				<sentence>where Pk is the probability of each value in the subtree 3 .</sentence>
				<definiendum id="0">Pk</definiendum>
			</definition>
			<definition id="4">
				<sentence>If node k is a word node , then \ [ Vkt = frequency of value L in node k aWe adopt the s~me expre~ion as that used by Stanfill \ [ 6\ ] and Sumita \ [ 4\ ] .</sentence>
				<definiendum id="0">s~me expre~ion</definiendum>
				<definiens id="0">a word node , then \ [ Vkt = frequency of value L in node k aWe adopt the</definiens>
			</definition>
			<definition id="5">
				<sentence>23-28 , 1992 else \ [ V~ , t = INj , t~__ , ( IL , , x IV , ,i ) where m is a node linked to node k , and/14 , a is the importaatce of value L in node m. Importance of a Generalized Case ( IC ) The importance of a GLTPCi ( IC ) is defined a.s follows .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">a node linked to node k , and/14 , a is the importaatce of value L in node m. Importance of a Generalized Case ( IC ) The importance of a GLTPCi</definiens>
			</definition>
			<definition id="6">
				<sentence>Mi j=l where IVjt is tile importance of value L , which is the same as the value of the GLTPCi .</sentence>
				<definiendum id="0">IVjt</definiendum>
				<definiens id="0">tile importance of value L , which is the same as the value of the GLTPCi</definiens>
			</definition>
			<definition id="7">
				<sentence>se is one of the values with the higtlest IV .</sentence>
				<definiendum id="0">se</definiendum>
			</definition>
			<definition id="8">
				<sentence>FinMly , translation rules ( TRis ) are added to the set of GLTPC , s. TRis are descriptions in which concepts are specified as the values of variables of L. of LTPC , s. If the same case Mready exists in the set of GLTPC , , then it is not added .</sentence>
				<definiendum id="0">translation rules</definiendum>
				<definiens id="0">descriptions in which concepts are specified as the values of variables of L. of LTPC</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>CLG : Constraint logic grammars .</sentence>
				<definiendum id="0">CLG</definiendum>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>However the set of DMs is a regular language which can be expressed by regular expressions and reCOgnized by finite automata \ [ 19\ ] .</sentence>
				<definiendum id="0">DMs</definiendum>
				<definiens id="0">a regular language which can be expressed by regular expressions</definiens>
			</definition>
			<definition id="1">
				<sentence>Usually each Chinese character is a meaningful unit .</sentence>
				<definiendum id="0">Chinese character</definiendum>
			</definition>
			<definition id="2">
				<sentence>This is equivalent to find the chunk with the minimal value on ( L ( W1 ) -Mean ) **2 + ( L ( W2 ) -Mean ) **2 + ( 14'W3 ) Mean ) **2 , where Wl , W2 , and W3 are three words in a chunk ; Mean is the average length of Wl , W2. , and W3 ; L ( W ) denotes the length of the word W. Heuristic rule 2 simply says that the word length are usually evenly distributed .</sentence>
				<definiendum id="0">Mean</definiendum>
				<definiendum id="1">W3</definiendum>
				<definiens id="0">Mean ) **2 , where Wl , W2 , and W3 are three words in a chunk</definiens>
				<definiens id="1">the average length of Wl , W2. , and</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>1992 For this reason , our generation knowledge consists of trees represented as feature structures .</sentence>
				<definiendum id="0">generation knowledge</definiendum>
				<definiens id="0">consists of trees represented as feature structures</definiens>
			</definition>
			<definition id="1">
				<sentence>ACTION\ ] J ) Figure 1 : an example of a PD A PD consists of two parts : a structure definition and feature structure annotation ( Structure a.nd Annotation in Figure 1 ) .</sentence>
				<definiendum id="0">PD A PD</definiendum>
			</definition>
			<definition id="2">
				<sentence>The semantic feature on the root node of a PD represents the semaattics of the PD ; thus we call it the semantic structure of the PD .</sentence>
				<definiendum id="0">PD</definiendum>
				<definiens id="0">the semaattics of the PD</definiens>
			</definition>
			<definition id="3">
				<sentence>The main part of the generation process is expansion process , which iterates through expanding node selection , activation , prccombination , and application , using an e~Tmnding node agenda .</sentence>
				<definiendum id="0">expansion process</definiendum>
				<definiens id="0">iterates through expanding node selection , activation , prccombination</definiens>
			</definition>
			<definition id="4">
				<sentence>A propagation node of a PD is a leaf node whose semantic structure is identical with the semantic structure of the root node of the PD 4 .</sentence>
				<definiendum id="0">propagation node of a PD</definiendum>
				<definiens id="0">a leaf node whose semantic structure is identical with the semantic structure of the root node of the PD 4</definiens>
			</definition>
			<definition id="5">
				<sentence>( a ) the root node of PDI is unifiable with tile expanding node ( b ) t &gt; Di and t ) Di+l are connectable where PDI is connectable to PD 5 if PDI is a propagation P\ ] ) , and tim propagation node of PDi is unitiable with the root node of PDj .</sentence>
				<definiendum id="0">PDI</definiendum>
				<definiendum id="1">PDI</definiendum>
				<definiens id="0">a propagation P\ ] ) , and tim propagation node of PDi is unitiable with the root node of PDj</definiens>
			</definition>
			<definition id="6">
				<sentence>ILesolution Expansion failure occurs when : combination , or ity in the application .</sentence>
				<definiendum id="0">ILesolution Expansion failure</definiendum>
				<definiens id="0">occurs when : combination , or ity in the application</definiens>
			</definition>
			<definition id="7">
				<sentence>The module searches tbr the ne ; ~rest ( i.e. , lowest ) hypothesis node ( Nh ) dominating the failed expanding node and deletes Nh fi'om the hypotheses slot containing it .</sentence>
				<definiendum id="0">module</definiendum>
				<definiens id="0">searches tbr the ne ; ~rest ( i.e. , lowest ) hypothesis node ( Nh ) dominating the failed expanding node and deletes Nh fi'om the hypotheses slot containing it</definiens>
			</definition>
			<definition id="8">
				<sentence>McDonald et al. , editors , NaturM Language Generation Systems , Chapter 7 , Springer-Verlag , 1988 \ [ 3\ ] Shieber , S. M , `` An introduction to Unification-Based Approaches to Graznmar ' , CSLI , 1986 \ [ 4\ ] Knight , K. , `` Unification : A Multidisciplinaxy Survey '' , ACM Computing Surveys , VoL21 , No.i , 1989 \ [ 5\ ] Pollard , C. et ah , `` Information-ba .</sentence>
				<definiendum id="0">Unification</definiendum>
				<definiens id="0">A Multidisciplinaxy Survey ''</definiens>
			</definition>
</paper>

		<paper id="4184">
			<definition id="0">
				<sentence>In the figur % \ [ ... \ ] is a featnre structure , { ... } is a llst , and { ~ { ... Y , } is a disjunction .</sentence>
				<definiendum id="0">}</definiendum>
				<definiens id="0">a disjunction</definiens>
			</definition>
			<definition id="1">
				<sentence>`` Phus~ the variable S represents the ( packed ) structure of sentence ( 1 ) as a list of five eom\ [ } mmnts 1 each of whldl corresl ) onds to a V : an NP , or a PP .</sentence>
				<definiendum id="0">Phus~ the variable S</definiendum>
				<definiens id="0">an NP</definiens>
			</definition>
			<definition id="2">
				<sentence>The grammatical relation ( gr= ) and the modiflee ( meal= ) of the three PPs are disjunctions , meaning that one of the wdues shouhl be selected , but that the correct candidate has not yet beat determined .</sentence>
				<definiendum id="0">grammatical relation</definiendum>
				<definiens id="0">disjunctions , meaning that one of the wdues shouhl be selected , but that the correct candidate has not yet beat determined</definiens>
			</definition>
			<definition id="3">
				<sentence>JAUNT first locates an n dimensional con straint matrix connected to X1 , X2 , ... , X= , and set its element corresponding to the value combination &lt; xi , x2 , ... , x , ~ &gt; to 0 .</sentence>
				<definiendum id="0">JAUNT</definiendum>
				<definiens id="0">n dimensional con straint matrix connected to X1 , X2 , ... , X= , and set its element corresponding to the value combination &lt; xi , x2 , ... , x</definiens>
			</definition>
			<definition id="4">
				<sentence>The cmnputathmal complexity of our constralut propagation algorithm is hounded by O ( eIMD , where IMI is the siz , ~ of the constraint matrices and e is the number of the cunstraint matrices , becattse at lemst oue element in st ) me matrix is changed to 0 from I for every iteration of constraint propagatiom If the con str~ints are Iocal~ that is , if the arity of each ennstraint is bounded by a small integer , this time bound is a polynomial of the number of disjunctions .</sentence>
				<definiendum id="0">IMI</definiendum>
				<definiens id="0">a polynomial of the number of disjunctions</definiens>
			</definition>
			<definition id="5">
				<sentence>SHALT2 , an experimental English-to-Japanese machine translation system currently being developed at IBM 's Tokyo Research Laboratory , has a similar system structure ( Nagao 1990 ) .</sentence>
				<definiendum id="0">SHALT2</definiendum>
				<definiens id="0">an experimental English-to-Japanese machine translation system currently being developed at IBM 's Tokyo Research Laboratory , has a similar system structure</definiens>
			</definition>
</paper>

		<paper id="4214">
</paper>

		<paper id="4180">
			<definition id="0">
				<sentence>Chinese Narrative Discourse Unlike the languages Du Bois has studied ( 1987 ) , Mandarin Chinese is a typologically different language with no inflection and relatively free word order .</sentence>
				<definiendum id="0">Chinese Narrative Discourse Unlike</definiendum>
				<definiendum id="1">Chinese</definiendum>
				<definiens id="0">a typologically different language with no inflection and relatively free word order</definiens>
			</definition>
</paper>

		<paper id="2103">
			<definition id="0">
				<sentence>The algorithm given in \ [ 15\ ] and \ [ 16\ ] then Mlows to generate frolu all uuderspceified structure , if there is a fully specified ( semantic ) predicatc-argontentstructure which is nnt ~dlowed to be extended during generation , e.g. tile l ) redicate-argunlent structure must be conqllete and coherent with respect to the target grammar , One of the disadvantages of this algorithm is , that it must be marked for tile generator , which substructure is not allowed to be changed during generation .</sentence>
				<definiendum id="0">tile generator</definiendum>
				<definiens id="0">substructure is not allowed to be changed during generation</definiens>
			</definition>
			<definition id="1">
				<sentence>The equations are distinguished into • defining equations indicated by tile operator = • inequatimts indicated by the operator # • constraining equations indicated by the operator =e All equation consists of a reference to a structure , tile el ) era ) or , and , 'L , ~ second argulueut of the operatiou oue of • all atomic v~due like raas • a semantic form , indicated by double quotes , with an atou ) ic uaule aud all optional arguuleut list , , i.e. `` man '' , `` give ( SuuJ , ot~J } '' • a referellee to a structure A reference to a structure is either a mete-variable or a path applied to a mete-variable .</sentence>
				<definiendum id="0">give</definiendum>
				<definiens id="0">• defining equations indicated by tile operator = • inequatimts indicated by the operator # • constraining equations indicated by the operator =e All equation consists of a reference to a structure , tile el ) era ) or</definiens>
				<definiens id="1">all atomic v~due like raas • a semantic form , indicated by double quotes , with an atou ) ic uaule aud all optional arguuleut list , , i.e. `` man ''</definiens>
				<definiens id="2">either a mete-variable or a path applied to a mete-variable</definiens>
			</definition>
			<definition id="2">
				<sentence>• ttw meta-variMilc 1 , which stands fur tile structure a.ssociate ( 1 with a ( laughter uode of a rule , e.g. the nolle on the right hand side of a rule where tile feature description is an annotation of .</sentence>
				<definiendum id="0">tile feature description</definiendum>
				<definiens id="0">an annotation of</definiens>
			</definition>
			<definition id="3">
				<sentence>23-28 , 1992 trivial equation : N is a non-terminal node .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a non-terminal node</definiens>
			</definition>
			<definition id="4">
				<sentence>( 1 X ) = l : N is a non-terminal node .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a non-terminal node</definiens>
			</definition>
			<definition id="5">
				<sentence>lUalUU der : rennt : rannte : S ~ NP VP ( T SUBJ ) = I T = l NP ~ D N T=lT=l NP ~ N T=I VP ~ V l=l N , ( T PRED ) = `` mmm '' ( 1 NUM ) = sg ( T GENDER ) = mas ( T CASE ) # gen ( \ ] '' SEM REL ) = `` man '' ( T SEM NUM ) = sg D , ( T SPEC ) = def ( j '' GENDER ) = mas ( 1 CASE ) = nom ( T NUM ) = sg ( T SEM SPEC ) =def V , ( T PRED ) : `` rennen ( SUBJ ) '' ( 1 '' TENSE ) = present ( T SUBJ CASE ) : nora ( I SUBJ NUM ) : sg ( T SEM REL ) = `` run '' ( \ ] '' SEM TIME START ) = now ( 1 '' SEM ARG1 ) = ( T SUBJ SEM ) V , ( T PRED ) = `` rennen ( SUBJ ) '' ( T TENSE ) = past ( \ ] '' SUBJ CASE ) = nora ( T SUBJ NUM ) = sg ( \ [ SEM REL ) - '' run '' ( T SEM TIME START ) = \ ] ) ast ( I SEM TIME END ) = past ( T SEM ARG1 ) = ( 1 '' SUBJ SEM ) Figure 1 : Example grammar would allow to activate botb verbs of the example lexicon , but the equation ( T SEM TIME END ) = past excludes the eutry for rannte .</sentence>
				<definiendum id="0">SUBJ</definiendum>
				<definiendum id="1">SUBJ NUM ) : sg ( T SEM REL</definiendum>
				<definiendum id="2">SEM TIME START</definiendum>
				<definiens id="0">T SPEC ) = def ( j '' GENDER ) = mas ( 1 CASE ) = nom ( T NUM ) = sg ( T SEM SPEC</definiens>
				<definiens id="1">T SUBJ SEM ) V , ( T PRED ) = `` rennen ( SUBJ ) '' ( T TENSE ) = past ( \ ] '' SUBJ CASE ) = nora ( T SUBJ NUM ) = sg ( \ [ SEM REL ) - '' run '' ( T SEM TIME START ) = \ ] ) ast ( I SEM TIME END</definiens>
			</definition>
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>A component network for the nonterminal labeled n has a parameter set ( A , B , I , N , F , Top , n ) .</sentence>
				<definiendum id="0">component network</definiendum>
			</definition>
			<definition id="1">
				<sentence>( ~ , y , p , n ) i p 0 &lt; y &lt; Y i ~_ Term ( n ) &amp; : i E Final ( n ) 0 &lt; v &lt; y p E gontcrm ( n ) &amp; p e Final ( n ) ( 6 ) crnte ( x , y , p , n ) represents the probability of a con ( l ) stituent for n that spans x..</sentence>
				<definiendum id="0">n )</definiendum>
			</definition>
			<definition id="2">
				<sentence>Nonterm ( n ) 0 &lt; x &lt; y i e Term ( n ) ( 3 ) ... . ( ~ , ~ , p , n ) = 1 ( p ) 0 &lt; x &lt; Y p e Nonterm ( n ) ( 4 ) ant , ( x , y , p , n ) represents an incomplete constituent for nonterminal n whose left subtrees span z..</sentence>
				<definiendum id="0">Nonterm</definiendum>
				<definiendum id="1">n )</definiendum>
			</definition>
			<definition id="3">
				<sentence>Nonterm ( n ) p • Final ( n ) &amp; Top ( n ) ( 14 ) /3 , t~ ( x , y , p , n ) has the same form as the previous formula for fit , but is used with nonterminal states .</sentence>
				<definiendum id="0">Nonterm</definiendum>
				<definiendum id="1">n )</definiendum>
				<definiens id="0">has the same form as the previous formula for fit , but is used with nonterminal states</definiens>
			</definition>
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>The negation relation takes one argument , its associate , and relates it to the rest of the sentence which forms the second argument , its frame .</sentence>
				<definiendum id="0">negation relation</definiendum>
				<definiens id="0">takes one argument , its associate , and relates it to the rest of the sentence which forms the second argument , its frame</definiens>
			</definition>
			<definition id="1">
				<sentence>lforn argues that tile Aristotelian system of predicate term logic , which analyzes negation as a mode of predication , is a more accurate formulation of linguistic negation .</sentence>
				<definiendum id="0">lforn</definiendum>
				<definiens id="0">argues that tile Aristotelian system of predicate term logic , which analyzes negation as a mode of predication , is a more accurate formulation of linguistic negation</definiens>
			</definition>
			<definition id="2">
				<sentence>Tile mode analysis of negation involves a linking of two elements , subject and predicate , to form a distinct kind of element , a sentence or proposition .</sentence>
				<definiendum id="0">negation</definiendum>
				<definiens id="0">involves a linking of two elements , subject and predicate , to form a distinct kind of element , a sentence or proposition</definiens>
			</definition>
			<definition id="3">
				<sentence>Instead , information strncture is all additional interpretation of a relation structure , made simtdtaneously with the semantic one .</sentence>
				<definiendum id="0">information strncture</definiendum>
				<definiens id="0">all additional interpretation of a relation structure , made simtdtaneously with the semantic one</definiens>
			</definition>
			<definition id="4">
				<sentence>A model theoretic semantics for a file card representation of discourse is defined by Helm ( 1982 ) in terms of a function for embedding the set of file cards in a model .</sentence>
				<definiendum id="0">model theoretic semantics for a file card representation of discourse</definiendum>
				<definiens id="0">1982 ) in terms of a function for embedding the set of file cards in a model</definiens>
			</definition>
			<definition id="5">
				<sentence>Content which is presented as informative \ [ nay give rise to inferences which make it informative .</sentence>
				<definiendum id="0">Content</definiendum>
				<definiens id="0">informative \ [ nay give rise to inferences which make it informative</definiens>
			</definition>
			<definition id="6">
				<sentence>Focus : An idea in motion .</sentence>
				<definiendum id="0">Focus</definiendum>
				<definiens id="0">An idea in motion</definiens>
			</definition>
</paper>

		<paper id="4216">
			<definition id="0">
				<sentence>Information found within intransitive motion verb definitions , therefore , was used also for dividing verbs according to the distinctions individuated by Levin &amp; Rappaport .</sentence>
				<definiendum id="0">Information</definiendum>
				<definiens id="0">found within intransitive motion verb definitions</definiens>
			</definition>
</paper>

		<paper id="3162">
			<definition id="0">
				<sentence>The Inference Engine draws on the knowledge contained in the domain-specific knowledge base to provide the basis for extracting the flow of argumentation of the editorial during the dialog session with the informant .</sentence>
				<definiendum id="0">Inference Engine</definiendum>
				<definiens id="0">draws on the knowledge contained in the domain-specific knowledge base</definiens>
			</definition>
			<definition id="1">
				<sentence>Finally , the Text Generator transforms the internal representation of the abstract into output text .</sentence>
				<definiendum id="0">Text Generator</definiendum>
				<definiens id="0">transforms the internal representation of the abstract into output text</definiens>
			</definition>
			<definition id="2">
				<sentence>The Inference Engine performs inferencing on the Knowledge Base to support the Discourse Manager in identifying possible conclusion ( s ) in an editorial , and in assisting the informant to relate the flow of argumentation leading to each conclusion .</sentence>
				<definiendum id="0">Inference Engine</definiendum>
				<definiens id="0">performs inferencing on the Knowledge Base to support the Discourse Manager in identifying possible conclusion ( s ) in an editorial , and in assisting the informant to relate the flow of argumentation leading to each conclusion</definiens>
			</definition>
			<definition id="3">
				<sentence>The discourse model provides the base framework for organising the internal representation of the abstract which is subsequently used by the Text Generator for producing the output text .</sentence>
				<definiendum id="0">discourse model</definiendum>
				<definiens id="0">provides the base framework for organising the internal representation of the abstract which is subsequently used by the Text Generator for producing the output text</definiens>
			</definition>
			<definition id="4">
				<sentence>The Discourse Manager performs a four-phase dialog session with the informant , each phase being designed to capture relevant information AcrES DE COLING-92 .</sentence>
				<definiendum id="0">Discourse Manager</definiendum>
				<definiens id="0">performs a four-phase dialog session with the informant , each phase being designed to capture relevant information AcrES DE COLING-92</definiens>
			</definition>
			<definition id="5">
				<sentence>\ [ 6\ ] Tsou , B.K. , Ho , H.C. , Lin , H.L. , Liu , G.K.F. , Lun , C.S. and Heung , A.Y.L. `` Automated Chinese Text Abstraction : A Human-Machine Co-operative Approach . ''</sentence>
				<definiendum id="0">Text Abstraction</definiendum>
			</definition>
</paper>

		<paper id="2074">
</paper>

		<paper id="4199">
			<definition id="0">
				<sentence>Word Identification ( WI , also known as Segmentation ) has been an important and active issue ill Chinese Natural Language Processing .</sentence>
				<definiendum id="0">Word Identification ( WI</definiendum>
				<definiendum id="1">Segmentation )</definiendum>
			</definition>
			<definition id="1">
				<sentence>We consider the WI process as a parsing process with word composition grammar , instead of a CSP problem \ [ 2\ ] , a unification problem \ [ 12\ ] ... . scanning process .</sentence>
				<definiendum id="0">WI process</definiendum>
				<definiens id="0">a parsing process with word composition grammar</definiens>
			</definition>
			<definition id="2">
				<sentence>The WI system consists of a lexicon , the word composition grammar , the preference scoring module , the test functions , and the parser .</sentence>
				<definiendum id="0">WI system</definiendum>
				<definiens id="0">consists of a lexicon , the word composition grammar , the preference scoring module , the test functions</definiens>
			</definition>
			<definition id="3">
				<sentence>Recall rates ( RR ) and precision rates ( PK ) are computed automatically by comparing the segmentation output with the correct answers segmented by human .</sentence>
				<definiendum id="0">Recall rates</definiendum>
				<definiens id="0">computed automatically by comparing the segmentation output with the correct answers segmented by human</definiens>
			</definition>
			<definition id="4">
				<sentence>Comparison with NTHU 's System In Chang , et al. \ [ 4\ ] , which we will call NTHU 's system , they reported a 95 percent precision rate and a recall rate greater than 95 percent , and listed 5 samplea ( A-samples ) the name in which their system can identify correctly , 34 examples ( B-samples ) for which the names are missed , and 22 examples ( C-samples ) for which Chinese names are over-generated .</sentence>
				<definiendum id="0">A-samples</definiendum>
				<definiens id="0">the name in which their system can identify correctly , 34 examples ( B-samples ) for which the names are missed</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>The topic-focus articulation OVA ) of a semence can be specified according to the sentence structure as follows ( eL Sgall 1979 ) : ( i ) F contains the main verb iff the verb is NB ; ( ii ) F contains all daughter nodes of the verb which are N'B , together with all nodes subordinated to them ( which in tam are either NB or CB ) ; ( iii ) if the verb together with all daughter nodes is CB ( and , therefore , none of ( i ) , ( ii ) applies ) , F is defined with respect to a deeper embedded node .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">contains all daughter nodes of the verb which are N'B , together with all nodes subordinated to them ( which in tam are either NB or CB</definiens>
			</definition>
			<definition id="1">
				<sentence>The starting conditions for the evaluating procedure are then as follows : We assume that our procedure is a part of a larger complex system which is able to provide our procedure with the result of syntactico-semantic parsing of any sentence in the form of a dependency tree as a representation of the meaning of the sentence in the sense of Sgall , Haji~ov~l , Panevov~ , ( 1986 ) .</sentence>
				<definiendum id="0">Haji~ov~l</definiendum>
				<definiens id="0">the result of syntactico-semantic parsing of any sentence in the form of a dependency tree as a representation of the meaning of the sentence in the sense of Sgall ,</definiens>
			</definition>
			<definition id="2">
				<sentence>The SSK as a basic data structure can be viewed in our modified account as a set of items , which represent all mental objects rendered by nouns or pronouns from the respective text .</sentence>
				<definiendum id="0">SSK</definiendum>
				<definiens id="0">represent all mental objects rendered by nouns or pronouns from the respective text</definiens>
			</definition>
			<definition id="3">
				<sentence>Each data entry has the form of an ordered quantuple : &lt; LEX , MORPH , LAST , SYNT , OCCUR &gt; , where LEX represents the lexical value of the item ; MORPH is a set of morphological characteristics of the word ( e.g. gender , number , etc. ) .</sentence>
				<definiendum id="0">LEX</definiendum>
				<definiendum id="1">MORPH</definiendum>
				<definiens id="0">the form of an ordered quantuple : &lt; LEX , MORPH , LAST , SYNT</definiens>
				<definiens id="1">the lexical value of the item ;</definiens>
				<definiens id="2">a set of morphological characteristics of the word ( e.g. gender , number , etc. )</definiens>
			</definition>
			<definition id="4">
				<sentence>1992 SYNT contains the data about the syntactic structure of the sentence where the respective LEX was mentioned for the last time .</sentence>
				<definiendum id="0">SYNT</definiendum>
				<definiens id="0">contains the data about the syntactic structure of the sentence where the respective LEX was mentioned for the last time</definiens>
			</definition>
			<definition id="5">
				<sentence>OCCUR is a pair of integers which represent the number of occurrences of the given item both from the beginning of the text and from the beginning of the paragraph .</sentence>
				<definiendum id="0">OCCUR</definiendum>
				<definiens id="0">a pair of integers which represent the number of occurrences of the given item both from the beginning of the text and from the beginning of the paragraph</definiens>
			</definition>
			<definition id="6">
				<sentence>The function of salience has the form : p S ( w ) = O/ ( N* ( N-L + 1 ) ) , where w is the item of SSK under consideration , O is the number of occurrences of the item in the given paragraph N is the serial number of the current utterance ( in the given paragraph ) L is the serial number of the utterance in the paragraph where this item was mentioned for the last time This function is essential for the whole process of anaphora resolution .</sentence>
				<definiendum id="0">p S</definiendum>
				<definiendum id="1">w</definiendum>
				<definiendum id="2">O</definiendum>
				<definiendum id="3">N</definiendum>
				<definiendum id="4">L</definiendum>
				<definiens id="0">the item of SSK under consideration</definiens>
				<definiens id="1">the number of occurrences of the item in the given paragraph</definiens>
				<definiens id="2">the serial number of the current utterance ( in the given paragraph )</definiens>
			</definition>
			<definition id="7">
				<sentence>The basic form of the function is : E ( W ) = ~ ( ci* f~ } , i-I where f~ is a function describing the value of the factoq q is a constant expressing file weight of the factoq ACRES DE COLING-92 .</sentence>
				<definiendum id="0">f~</definiendum>
				<definiens id="0">a constant expressing file weight of the factoq ACRES DE COLING-92</definiens>
			</definition>
</paper>

		<paper id="1036">
</paper>

		<paper id="2104">
			<definition id="0">
				<sentence>Scope means the phrase sequence in which only the syntactic head has dependency relation with other phrases outside of it .</sentence>
				<definiendum id="0">Scope</definiendum>
				<definiens id="0">means the phrase sequence in which only the syntactic head has dependency relation with other phrases outside of it</definiens>
			</definition>
			<definition id="1">
				<sentence>( 1 ) PIVOT analyzes a source sentence .</sentence>
				<definiendum id="0">PIVOT</definiendum>
				<definiens id="0">analyzes a source sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>( 4 ) After the user finishes asking corrections , PIVOT translates the sentence again .</sentence>
				<definiendum id="0">PIVOT</definiendum>
				<definiens id="0">translates the sentence again</definiens>
			</definition>
			<definition id="3">
				<sentence>( I ) PIVOT analyzes a source sentence , ( 2 ) If there is ambiguity at s certain stage of analysis , PIVOT retrieves data in the association database .</sentence>
				<definiendum id="0">PIVOT</definiendum>
				<definiens id="0">retrieves data in the association database</definiens>
			</definition>
			<definition id="4">
				<sentence>( 3 ) PIVOT compares the possible analysis structures of the given sentence with the analysis results accumulated in the association database .</sentence>
				<definiendum id="0">PIVOT</definiendum>
				<definiens id="0">compares the possible analysis structures of the given sentence with the analysis results accumulated in the association database</definiens>
			</definition>
			<definition id="5">
				<sentence>( 4 ) PIVOT selects the analysis structure that matches with the analysis results accumulated in the association database .</sentence>
				<definiendum id="0">PIVOT</definiendum>
				<definiens id="0">selects the analysis structure that matches with the analysis results accumulated in the association database</definiens>
			</definition>
			<definition id="6">
				<sentence>SADB is a subset of ADH .</sentence>
				<definiendum id="0">SADB</definiendum>
				<definiens id="0">a subset of ADH</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>Tile present paper represents the first attempt to integrate Mark Steedman 's theory of Combinatorial Categorial Grammar ( CCG ) \ [ Ste90 , Ste91\ ] with Ray Jackendoff 's theory of Conceptual Semantics \ [ Jac90 , ~lac91\ ] .</sentence>
				<definiendum id="0">Tile present paper</definiendum>
				<definiens id="0">the first attempt to integrate Mark Steedman 's theory of Combinatorial Categorial Grammar ( CCG ) \ [ Ste90 , Ste91\ ] with Ray Jackendoff 's theory of Conceptual Semantics \</definiens>
			</definition>
			<definition id="1">
				<sentence>The immediate motivation to attempt such an integration , and the focus of the present paper , is CCG 's incomplete treatment of sameness of role ( i.e. thematic ) information .</sentence>
				<definiendum id="0">CCG</definiendum>
				<definiens id="0">'s incomplete treatment of sameness of role ( i.e. thematic ) information</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus the syntactic category ( S\NP ) /NP defines a fimction that takes all NP to tile right and returns a function from an NP on the left to an S. Categories may combine via forward or backward functional application , indicated as ACTF~S DE COLING-92 , Nhrcre .</sentence>
				<definiendum id="0">S\NP ) /NP</definiendum>
				<definiendum id="1">fimction</definiendum>
				<definiens id="0">takes all NP to tile right and returns a function from an NP on the left to an S. Categories may combine via forward or backward functional application</definiens>
			</definition>
			<definition id="3">
				<sentence>Jackendoff represents an urn as the conceptual structure shown in ( 6 ) : ( ~ ) \ [ Thing URN\ ] This represents an entity of ontological type Thing that meets the featural description URN .</sentence>
				<definiendum id="0">urn</definiendum>
				<definiens id="0">an entity of ontological type Thing that meets the featural description URN</definiens>
			</definition>
			<definition id="4">
				<sentence>Arguments to the verb are integrated into the above conceptual structure using the Argument Fusion Rule , which links the coindexed constituents in the obvious way , as long as they are semantically compatible .</sentence>
				<definiendum id="0">Argument Fusion Rule</definiendum>
				<definiens id="0">links the coindexed constituents in the obvious way</definiens>
			</definition>
			<definition id="5">
				<sentence>withTheme y ( f x ) Here with is defined as a function from an NP to a VPnit shtmld be noted that Jackendoif does not adequately address the issue of why the class of apread~ appropriate for the verb butter ia larger than the class apppopriate for the , mue noun .</sentence>
				<definiendum id="0">withTheme y ( f x</definiendum>
				<definiens id="0">a function from an NP to a VPnit shtmld be noted that Jackendoif does not adequately address the issue of why the class of apread~ appropriate for the verb butter ia larger than the class apppopriate for the , mue noun</definiens>
			</definition>
			<definition id="6">
				<sentence>Italian Syntax : A Government Binding Approach .</sentence>
				<definiendum id="0">Italian Syntax</definiendum>
			</definition>
			<definition id="7">
				<sentence>Learnability and Cognition : The Acquisition o\ ] Argument Structure .</sentence>
				<definiendum id="0">Learnability</definiendum>
				<definiendum id="1">Cognition</definiendum>
			</definition>
</paper>

		<paper id="2087">
			<definition id="0">
				<sentence>Definition I v L is a subordinate verb of v v , if for some N w~ \ [ v~ ( ~ , ~ ) ~ N ( ~ ) ^ V~ ( ~ , ~ ) \ ] , where bohlface N stands for a tuple of predicate letters and N ( z ) means Nl ( Zl ) A..</sentence>
				<definiendum id="0">N ( z</definiendum>
				<definiens id="0">a subordinate verb of v v</definiens>
			</definition>
			<definition id="1">
				<sentence>A small letter , such as n , v , and v L , stands for a linguistic expression and a capital letter , such as N , V , and V L , stands for the predicate symbol corresponding to the linguistic expression represented by its small letter .</sentence>
				<definiendum id="0">v L</definiendum>
				<definiendum id="1">V L</definiendum>
				<definiens id="0">such as n , v , and</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , the main clause of the sentence `` ~¢~ ~ ~ J : 5 ~zt~~3 '' ( something adheres to X as it covers X ) is `` ~ ~ , ~ ~ ; 5 `` ( something adheres to ) , and it corresponds to the following formula , ~ , ( ~ ) A ~'~~ ( ~1 , ~ ) , and the following formula holds , V~l~\ [ s ( ~ , ~ ) where S ( ~\ ] l , r/~ ) is the formula corresponding to `` ~ ; O~ ~ .</sentence>
				<definiendum id="0">r/~ )</definiendum>
				<definiens id="0">the formula corresponding to</definiens>
			</definition>
			<definition id="3">
				<sentence>The restriction that if a verb vk c~m take a noun phrase with a e~e e the semantic category of the noun phrase is D is expressed logically as follows , w \ [ vk ( ~ ) ~ D ( ~ , ) \ ] , ( ~ ) where xi is the argument corresponding to the case c , and k is the meaning number of v. We call D in ( 5 ) the domain for c of vk .</sentence>
				<definiendum id="0">xi</definiendum>
				<definiendum id="1">k</definiendum>
				<definiens id="0">the argument corresponding to the case c</definiens>
			</definition>
			<definition id="4">
				<sentence>is correct , then Bx \ [ V~ ( x ) A D '' ( x ) A D'l ( x ) \ ] , where D ~ is the domain for i-th case of v ~ and D ~ is one forj-th case ofv~ and the noun ofj-th case of v~ in the definition sentence is n and the domain for n is D '~ .</sentence>
				<definiendum id="0">D'l</definiendum>
				<definiendum id="1">D ~</definiendum>
				<definiens id="0">the domain for i-th case of v ~ and</definiens>
			</definition>
			<definition id="5">
				<sentence>The meaning of an entry verb v ~ is defined by using the definition verb v d. Then , the less the number of the variables appearing either only in v ~ or only in v d ( i.e. ( size of tuple y ) + ( size of tuple z ) in the formula ( 4 ) ) , the more v ~ restricts the meaning of vL An editor of a dictionary would select such a definition verb .</sentence>
				<definiendum id="0">meaning of an entry</definiendum>
			</definition>
			<definition id="6">
				<sentence>Since we can infer 3x\ [ human ( x ) A all_entities ( x ) l from Assumption I and the relation between domain predicate , the correspondence from the first case of `` '~-~ 6 1 '' to the first case of `` ~ 3 '' satisfies the necessary condition described in paragraph 3.2 .</sentence>
				<definiendum id="0">all_entities</definiendum>
				<definiens id="0">( x ) l from Assumption I and the relation between domain predicate</definiens>
			</definition>
			<definition id="7">
				<sentence>That is , '~o5 ' is used with the form 'NP1 ~ NP2 ¢z NP3 ~ ~ o ' , and the semantic category of NP~ is 'human ' or 'organization ' , and one of NP2 is 'concrete object ' or 'abstract object ' , and one of NP3 is 'mental object ' .</sentence>
				<definiendum id="0">NP2</definiendum>
				<definiens id="0">'concrete object ' or 'abstract object '</definiens>
				<definiens id="1">'mental object '</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>Woman SWIMMER is an appositional compound ( similar to helicopter GUNSHIP ) .</sentence>
				<definiendum id="0">Woman SWIMMER</definiendum>
			</definition>
</paper>

		<paper id="4171">
			<definition id="0">
				<sentence>For example , an analysis of the sentence John thought Mary showed Ben ~o Sue might be represented as follows : John thought Mary showed Ben to Sue The word thought is the head of the whole sentence , and it has two dependents , John and showed , showed is the head of the embedded sentence , with three dependents , Mary , Ben and to .</sentence>
				<definiendum id="0">Mary</definiendum>
				<definiens id="0">the head of the embedded sentence , with three dependents ,</definiens>
			</definition>
			<definition id="1">
				<sentence>Adjacency is a reasonably standard restriction , and has been proposed as a universal principle e.g. by Hudson ( 1988 ) .</sentence>
				<definiendum id="0">Adjacency</definiendum>
			</definition>
			<definition id="2">
				<sentence>6Axiomatic Grmnm~r is a particular dynamic grammar designed for English , which take~ relationslfips between states tm a primary phenomenon , to be justified solely by linguistic data ( rather thmt by an existing formalism such as dependency granmlar ) .</sentence>
				<definiendum id="0">6Axiomatic Grmnm~r</definiendum>
				<definiens id="0">a particular dynamic grammar designed for English , which take~ relationslfips between states tm a primary phenomenon , to be justified solely by linguistic data</definiens>
			</definition>
			<definition id="3">
				<sentence>XL1 where X is a base type , and L and It are rR\ ] lists of base types .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a base type</definiens>
			</definition>
			<definition id="4">
				<sentence>For example , tile following deduction rule ( again restricted to non-empty strings ) , Co String , C , , C~C , Co String , , `` and'~C~ provides all account of the syntax of non-constituent coordination ( Milward , 1991 ) .</sentence>
				<definiendum id="0">and'~C~</definiendum>
			</definition>
</paper>

		<paper id="3143">
</paper>

		<paper id="4181">
</paper>

		<paper id="2090">
			<definition id="0">
				<sentence>Ericsson , AECMA , and IBM control more or less identical grammatical milts , notwithslanding each company has its own way of simplifying syntax .</sentence>
				<definiendum id="0">IBM</definiendum>
				<definiens id="0">its own way of simplifying syntax</definiens>
			</definition>
			<definition id="1">
				<sentence>The AECMA grammar , for example , instrncts the writer how to change a passive sentence into an active one and states that no verbs should be left out to reduce rite sentence length .</sentence>
				<definiendum id="0">AECMA grammar</definiendum>
			</definition>
			<definition id="2">
				<sentence>Receudy , this list has been extended to a vocabulary package of approximately 50110 words .</sentence>
				<definiendum id="0">Receudy</definiendum>
				<definiens id="0">extended to a vocabulary package of approximately 50110 words</definiens>
			</definition>
			<definition id="3">
				<sentence>Control Algorithm ( LCA ) At the third stage , the Lexical Control Algorithm ( LCA ) operates on all major and minor classes : noun control , verb control , adjective control , adverb control , auxiliary control , pronoun control , conjunction control , proposition control , and interjection control .</sentence>
				<definiendum id="0">Control Algorithm</definiendum>
				<definiendum id="1">LCA ) At</definiendum>
				<definiens id="0">the third stage , the Lexical Control Algorithm ( LCA ) operates on all major and minor classes : noun control , verb control , adjective control , adverb control , auxiliary control , pronoun control</definiens>
			</definition>
			<definition id="4">
				<sentence>The output of the LCA is a controlled lexico-syntactic unit .</sentence>
				<definiendum id="0">LCA</definiendum>
			</definition>
			<definition id="5">
				<sentence>+ For information , contact our local salesmanaeer , Stage four aims at controlling particular microfeatures of the lexico-syntactic unit , The Micro Control Algorithm ( MCA ) includes a.o. numeric control , reference control , series control , omission control , crucial term control , expression control .</sentence>
				<definiendum id="0">Micro Control Algorithm</definiendum>
				<definiens id="0">includes a.o. numeric control , reference control , series control , omission control , crucial term control</definiens>
			</definition>
			<definition id="6">
				<sentence>/x , I Use unless Fig , 4 Algorithmic grammar flow-chart present and future ( CALL ) When the controlled grammar ( COGRAM ) has been structured according to strict algorithmic principles ( ALCOGRAM ) , the notion of applying a computer in the process of technical writing ( CAI ) is obviously close .</sentence>
				<definiendum id="0">CAI</definiendum>
				<definiens id="0">the controlled grammar ( COGRAM ) has been structured according to strict algorithmic principles ( ALCOGRAM ) , the notion of applying a computer in the process of technical writing (</definiens>
			</definition>
			<definition id="7">
				<sentence>Schreurs D. ( 1989a ) COGRAM , Controlled Grammar Sehreurs D. ( 1989b ) Grammatical Analysis of a DATACOM 2 sample through the Controlled English Grammar COGRAM .</sentence>
				<definiendum id="0">COGRAM</definiendum>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>A data type consists of one or more domains of data items , of which certaiu elements are designated as basic , together with a set of opera tious on the domains which suffice to generate al\ ] data items in the domains fl'om the I ) asic items .</sentence>
				<definiendum id="0">data type</definiendum>
			</definition>
</paper>

		<paper id="3125">
			<definition id="0">
				<sentence>We use the more abstract term `` relational morphology '' in place of tile usual `` two-level morphology '' in order to emphasize an aspect of Koskenniemi 's work which has been overlooked in favor of implementation issues using the finite state paradigm , namely , that a mathematical relation can be specified between the lexical and surface levels of a language .</sentence>
				<definiendum id="0">relational morphology</definiendum>
				<definiens id="0">the lexical and surface levels of a language</definiens>
			</definition>
			<definition id="1">
				<sentence>It is verified that L1 is the lexeme specified by Lcx , and S1 the surface character specified by Surface .</sentence>
				<definiendum id="0">L1</definiendum>
				<definiens id="0">the lexeme specified by Lcx , and S1 the surface character specified by Surface</definiens>
			</definition>
			<definition id="2">
				<sentence>KIMMO : a general morphological processor .</sentence>
				<definiendum id="0">KIMMO</definiendum>
				<definiens id="0">a general morphological processor</definiens>
			</definition>
			<definition id="3">
				<sentence>Two-level morphology : a general computational model for wordqbun recognition and production .</sentence>
				<definiendum id="0">Two-level morphology</definiendum>
				<definiens id="0">a general computational model for wordqbun recognition and production</definiens>
			</definition>
			<definition id="4">
				<sentence>set ( con , \ [ b , c , d , f , g , h , j , k , l , m , u , p , q , r , s , t , v , w , x , y , zl ) , In addition to specifying characters such as s , x , etc. , we can also deliue ~qnences of characters noted aS lists Is , h\ ] , not ( characte0 , uot ( sequeuce ol characters ) , in ( con ) means any member of the ~t con , whereas iu ( X , char c ) is a member of the set char e ~ssigncd to the variable X for unification in another part of the talc. '</sentence>
				<definiendum id="0">x</definiendum>
				<definiendum id="1">characte0 , uot ( sequeuce ol characters</definiendum>
				<definiendum id="2">char c )</definiendum>
				<definiens id="0">c , d , f , g , h , j , k , l , m , u , p , q , r , s , t , v , w , x , y , zl</definiens>
			</definition>
</paper>

		<paper id="3139">
			<definition id="0">
				<sentence>Syllable bigrams Prob ( R ) = SylProb ( S ) , for is a single-syllable root R = S = Bigram ( S i $ 2 ) , for a bi-syllable root R = SIS 2 = Min ( Bigram ( StS2 ) , Bigram ( S2 , S3 ) ) for a tri-syllable root R = S18283 , The root dissection is done using Algorithm 2 which is similar to the hyphenation algorithm .</sentence>
				<definiendum id="0">Bigram</definiendum>
				<definiens id="0">a single-syllable root R = S = Bigram ( S i $ 2</definiens>
			</definition>
			<definition id="1">
				<sentence>The terminology bank consists of more than 4,000 lines of chemical terms compiled by a leading chemical company hi Germany for internal use .</sentence>
				<definiendum id="0">terminology bank</definiendum>
				<definiens id="0">consists of more than 4,000 lines of chemical terms</definiens>
			</definition>
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>A recognition algorithm collects a set of items that denote the existence of trees , rather than trees themselves .</sentence>
				<definiendum id="0">recognition algorithm</definiendum>
				<definiens id="0">collects a set of items that denote the existence of trees</definiens>
			</definition>
			<definition id="1">
				<sentence>Formally , for terminal strings s ( ~ E* we define extends ( s , t ) d~r 3U , v C ~ : * ( t = USV ) , i.e. s is a substring of t. For strings in V* containing at least one nonterminal , we define extends reeursively a.~ extend4 ( ~Z , t ) '~°~ 3s C ~ : ( extends ( ~sZ , t ) ) .</sentence>
				<definiendum id="0">i.e. s</definiendum>
				<definiens id="0">is a substring of t. For strings in V* containing at least one nonterminal</definiens>
			</definition>
			<definition id="2">
				<sentence>A strategy is a characterization of trees that are to be added to the primordial soup S under some additional constraints .</sentence>
				<definiendum id="0">strategy</definiendum>
				<definiens id="0">a characterization of trees that are to be added to the primordial soup S under some additional constraints</definiens>
			</definition>
			<definition id="3">
				<sentence>We will first define unification , which is a special case of superposition in which the roots of two trees are mapped onto each other , for the definition of unification , we use the derivation operator =~ for trees .</sentence>
				<definiendum id="0">unification</definiendum>
			</definition>
			<definition id="4">
				<sentence>Palm trees consist of a roof ( corresponding to a single production ) and a trunk ( consisting of a number of adjacent complete trees ) .</sentence>
				<definiendum id="0">Palm trees</definiendum>
				<definiens id="0">consist of a roof ( corresponding to a single production ) and a trunk ( consisting of a number of adjacent complete trees )</definiens>
			</definition>
			<definition id="5">
				<sentence>X , ; each X~ is the root of a complete tree XC , ~ P-r Degenerate cases , with only a trunk ( a~ = e ) or only a roof ( 3 = ¢ = v ) are excluded explicitly .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the root of a complete tree XC</definiens>
			</definition>
</paper>

		<paper id="2108">
			<definition id="0">
				<sentence>Joshi et al illustrate with tile following example ; given the KB in ( 5 ) , and the question in ( 6 ) , they want the process to show why the answer in ( 7b ) is preferred to that in ( 7a ) : ( 5 ) Sam is an associate professor ; most associate professors are tenured ; Sam is not tenured .</sentence>
				<definiendum id="0">Sam</definiendum>
				<definiendum id="1">Sam</definiendum>
				<definiens id="0">an associate professor ; most associate professors are tenured ;</definiens>
			</definition>
			<definition id="1">
				<sentence>Thc Causal Law is a mixture defea-sible linguistic knowledge and worhl knowledge : given that tim clauses are diseourse-rclated somehow , the events they describe must he commetcd in a causal , part/wholc or overlap relation ; here , given the events in question , they must staud illa causal relation~ if things are norreal .</sentence>
				<definiendum id="0">Thc Causal Law</definiendum>
				<definiens id="0">a mixture defea-sible linguistic knowledge and worhl knowledge : given that tim clauses are diseourse-rclated somehow , the events they describe must he commetcd in a causal , part/wholc or overlap relation ; here , given the events in question</definiens>
			</definition>
			<definition id="2">
				<sentence>Two i ) atterns of inference are particularly relevant : Defensible Modus Ponens ( birds normally fly , Twecty is a bird ; sn Tweety flies ) ; and the Penguin Principle ( all penguins are birds , birds normally fly , penguius normally do n't fly , q'weety is a penguin ; so Tweety does n't fly ) .</sentence>
				<definiendum id="0">Twecty</definiendum>
				<definiendum id="1">q'weety</definiendum>
				<definiens id="0">a penguin ; so Tweety does n't fly )</definiens>
			</definition>
			<definition id="3">
				<sentence>Note that although double applications of the Penguin Principle , as in ( 9 ) , are not valid in genera\ ] , they show that for the particular case considered here , o~ validates the double application .</sentence>
				<definiendum id="0">o~</definiendum>
				<definiens id="0">the double application</definiens>
			</definition>
			<definition id="4">
				<sentence>There are two possibilities : either p represents defeasible knowledge of tile lmlguage or the world , or p is some fact in the Km We investigate these in turn .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">either p represents defeasible knowledge of tile lmlguage or the world</definiens>
			</definition>
			<definition id="5">
				<sentence>qc where S intends to convey the proposition that John 's pushing Max caused the latter to fall .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">intends to convey the proposition that John 's pushing Max caused the latter to fall</definiens>
			</definition>
			<definition id="6">
				<sentence>p is a fact in the Krl We now turn to the case where p is a fact about tim Kn which S knows and which S knows H lacks .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">p</definiendum>
				<definiens id="0">a fact about tim Kn which S knows and which S knows H lacks</definiens>
			</definition>
			<definition id="7">
				<sentence>p is a fact about the KB : Ambiguity Suppose that 5 ' wants to convey the information that Max 's fall immediately preceded John pushing \ ] tim , and suppose S knows that H knows the causal law , but S does n't know for sure if H knows already that Max fell before John pushed him .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">a fact about the KB : Ambiguity Suppose that 5 ' wants to convey the information that Max 's fall immediately preceded John pushing \ ] tim , and suppose S knows that H knows the causal law , but S does n't know for sure if H knows already that Max fell before John pushed him</definiens>
			</definition>
			<definition id="8">
				<sentence>But this time suppose that S wants to describe the situation where John 's greeting Max caused him to stand up .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">wants to describe the situation where John 's greeting Max caused him to stand up</definiens>
			</definition>
			<definition id="9">
				<sentence>So this time , S wants to describe an instance of the causal law .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">wants to describe an instance of the causal law</definiens>
			</definition>
			<definition id="10">
				<sentence>ID applies to the candidate utterances ( or tile space of utterances ) , and criticises the utterances ( or the space ) , producing better utterances , or a smaller space .</sentence>
				<definiendum id="0">ID</definiendum>
				<definiens id="0">applies to the candidate utterances ( or tile space of utterances ) , and criticises the utterances ( or the space</definiens>
			</definition>
			<definition id="11">
				<sentence>Asher , N. &amp; Morreau , M. \ [ 1991\ ] Comnton Sense Entailment : A Modal Theory of Nomnouotonic Reasoning .</sentence>
				<definiendum id="0">Comnton Sense Entailment</definiendum>
			</definition>
</paper>

		<paper id="2101">
			<definition id="0">
				<sentence>A translation template is a bilingual pair of sentences in which corresponding units ( words and phrases ) are coupled and replaced with variables .</sentence>
				<definiendum id="0">translation template</definiendum>
				<definiens id="0">a bilingual pair of sentences in which corresponding units ( words and phrases ) are coupled and replaced with variables</definiens>
			</definition>
			<definition id="1">
				<sentence>Each translation template is a bilingual pair of pseudo sentences .</sentence>
				<definiendum id="0">translation template</definiendum>
				<definiens id="0">a bilingual pair of pseudo sentences</definiens>
			</definition>
			<definition id="2">
				<sentence>And each pseudo sentence is a sentence which includes variables .</sentence>
				<definiendum id="0">pseudo sentence</definiendum>
			</definition>
			<definition id="3">
				<sentence>x ~¢ ) ) and NP ( path name ) are couplexl togethel : ltere , X ( w I w 2 `` '' Wn ) stands for a phrase whose syntactic category is X and which is constituted by words w 1 ' w2 ' `` '' , and W n , ( ii ) Resolution of syntactic ambiguity A phrase X in one hmguage sentence S is uot coupled to any phrase in the other langnage ~ntencc T , if T d ( ~s not include a phrase which includes counterparts fi ) r all the words inside X , but none for words outside of X. This means that syntactic ambiguity is resolved implicitly in file process of coupling phrases .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">syntactic ambiguity A phrase X in one hmguage sentence</definiens>
				<definiens id="1">~s not include a phrase which includes counterparts fi ) r all the words inside X , but none for words outside of X. This means that syntactic ambiguity is resolved implicitly in file process of coupling phrases</definiens>
			</definition>
			<definition id="4">
				<sentence>While the English sentence analysis table contains NP ( a car with four dollars ) , tim Japanese sentence analysis table does not contain a phrase which includes ' 4 ' , ' \ ] e it/ , and ' tic and none of the other content words .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">a car with four dollars ) , tim Japanese sentence analysis table does not contain a phrase which includes ' 4 ' , ' \ ] e it/ , and ' tic and none of the other content words</definiens>
			</definition>
			<definition id="5">
				<sentence>While the Japanese sentence analysis table contains NP ( A ¢ ) B ) , the English sentence analysis table does not contain a phrase which includes A and B and does not include C. Accordingly NP ( A ¢ ) B ) is rejectexl .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">does not contain a phrase which includes A and B and does not include C. Accordingly NP ( A ¢ ) B ) is rejectexl</definiens>
			</definition>
			<definition id="6">
				<sentence>A translation template is a bilingual pair of sentences in which corresponding units are coupled and replaced with variables .</sentence>
				<definiendum id="0">translation template</definiendum>
				<definiens id="0">a bilingual pair of sentences in which corresponding units are coupled and replaced with variables</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>tially by character level , ad &lt; l the number of matchin~ characters x 2 \ ] mints. by the Itcnyoh fc*rtns of predicates ( | { enyoh c|nmshi-ho ) which do ltOt accompany COllllll*t , } ) e¢llll~ : almost all of these prc , llc , ties iilOdify thc llCXL llt~al¢~st \ [ ) l'edicltte lilld there is 11~ ) need t , ~ chc &lt; : k the possibility of conjunct|oil. Acn : .s DE COLING-92 , NAutilus , 23-28 Aom ' 1992 1 7 1 PROC. O1~ COLING-92 , NANTES , AU6.23-28 , 1992 ^ ' : ~.pmlal maulr. P ~ ... ... ... ... ... ... ... r ... ... ... ... ... ... . . ~ I ~ ( p n+l ) \ . '' , .. I `` Ne ~ ' '' `` similarity value : '' ... I~ -- -- A = ( . ( ij ) ) Figure 1 : A path. the thesaurus 'Buurui Goi Ityou ' ( BGH ) \ [ 3\ ] . BGH has the six layer abstraction hierarchy and more than 60,000 words are assigned to the leaves of it. If the most specific common layer between two IWs is the k-th layer and if k is greater than 2 , add ( k 2 ) × 2 points. If either or both IWs are not contained in BGH , no addition is made. Matching of the generic two layers are ignored to prevent too vague matching in broader sense. add the number of matchin $ AWs x 3 points. Maximum sum of the similarity values which can be added by the steps 2 and 3 above is limited to 10 points. • Although the parts of speech oflWs are not equal , give 2_.points if both bunsetsus can be predicate ( see footnote 1 ) . For example , the similarity point between `` ~ ~Pi~ ( low level language ) + , '' and `` ~lt¢ , ~'~ ( high level language ) + ~ ( and ) '' is calculated as 2 ( match of parts of speech ) + 8 ( match of four characters : Y~l/t~ ~ ) = 10 points. The point between `` ~\ ] 'aq~ ( revision ) + L ( do ) + , '' and `` l~U3 ( deteetion ) +'J-~ ( do ) '' is 2 ( match of parts of speech ) + 2 ( match by BGII ) + 3 ( match of one AWs ) 7 points. Bunsetsus Our method detects the scope ofa CS by two series of bunaetsus which have the greatest similarity. These two aeries of bunsetsus are searched for on a triangular matrix A = ( a ( i , j ) ) ( Figure 1 ) , whose diagonal element a ( i , i ) is the i-th bunsetsu in a sentence and whose element a ( i , j ) ( i &lt; j ) is the similarity value between bunsetsu a ( i , i ) and bunsetsu a ( j , j ) . We call the rectangular matrix A ' a partial matrix , where A ' = ( a ( i , j ) ) ( O &lt; i &lt; n ; n+ l &lt; j &lt; 1 ) t , ( i , ~3~ ... ... ... . i ' -- -~i , j-I ) -~i , 9 `` J ... .. I i i\ , ( .. ni i i % Figure 2 : An ignored element. ... .. ~ ... ... ... ~ , ~ : ... ... ... . ! ... ... . i c5~. ( `` , , i ... .. i ... ... .. -2 'v-\ ... ... .. ~ ... ... . ... .. i ... ... ... .. : ; 'i. : ' : ~'~ : ~ ... ..~ ... ... . Figure 3 : Penalty points. is the upper right part of a KB ( Figure 1 ) . In tile following , 1 indicates the number of bunsetsus and a ( n , n ) is a KB. We define a path as a series of elements from a non-zero element in the lowest row to an element in the leftmost column of a partial matrix ( Figure 1 ) . path : := ( a ( pl , m ) , a ( p2 ... . 1 ) ... .. a ( p ... .. + 1 ) ) , where n + l &lt; m &lt; 1 , a ( pl , m ) ¢ O , Pi = n , PI &gt; &gt; .</sentence>
				<definiendum id="0">Penalty points.</definiendum>
				<definiendum id="1">n )</definiendum>
				<definiens id="0">A path. the thesaurus 'Buurui Goi Ityou ' ( BGH ) \ [ 3\ ] . BGH has the six layer abstraction hierarchy and more than 60,000 words</definiens>
				<definiens id="1">made. Matching of the generic two layers are ignored to prevent too vague matching in broader sense. add the number of matchin $ AWs x 3 points. Maximum sum of the similarity values</definiens>
				<definiens id="2">low level language ) + , '' and `` ~lt¢ , ~'~ ( high level language</definiens>
				<definiens id="3">the i-th bunsetsu in a sentence and whose element a</definiens>
				<definiens id="4">the upper right part of a KB ( Figure 1 ) . In tile following</definiens>
				<definiens id="5">a KB. We define a path as a series of elements from a non-zero element in the lowest row to an element in the leftmost column of a partial matrix ( Figure 1 ) . path</definiens>
			</definition>
			<definition id="1">
				<sentence>-k 4.-b ~f~'~ ( Actlve Chart Parsing ) '' and `` llPSG ( Head-drtve , Phrase Structure Grami lly the additional usage of relatively simple syntactic conditions , some sentences which are analyzed wrongly by this method will be analyzed rightly .</sentence>
				<definiendum id="0">llPSG</definiendum>
				<definiens id="0">Head-drtve , Phrase Structure Grami lly the additional usage of relatively simple syntactic conditions</definiens>
			</definition>
</paper>

		<paper id="4188">
			<definition id="0">
				<sentence>Valency patterns are stored as a feature-value pair on verbs in the monolingual dictionaries , in such a way that all patterns are coded only once with the verb ; reading distinctions can give rise to different valency patterns , but even then they are all stored together with the verb .</sentence>
				<definiendum id="0">Valency patterns</definiendum>
				<definiens id="0">a feature-value pair on verbs in the monolingual dictionaries , in such a way that all patterns are coded only once with the verb ; reading distinctions can give rise to different valency patterns , but even then they are all stored together with the verb</definiens>
			</definition>
			<definition id="1">
				<sentence>Additional software takes care of putting the Metal frames in their canonical order ( i.e. a subject is coded before an object , etc. ) , and provides tools fi ) r lexicographers to manipulate the conversion outpnt .</sentence>
				<definiendum id="0">Additional software</definiendum>
				<definiens id="0">takes care of putting the Metal frames in their canonical order</definiens>
			</definition>
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>Probabilistic approaches attack these problems by providing a more objective measure on the preference to a given interpretation .</sentence>
				<definiendum id="0">Probabilistic approaches</definiendum>
				<definiens id="0">attack these problems by providing a more objective measure on the preference to a given interpretation</definiens>
			</definition>
			<definition id="1">
				<sentence>Score Function A Score Function for a given syntactic tree , say Synj , is defined as follows : S~or~ ( s~ , ,~ ) _-v ( % , , .</sentence>
				<definiendum id="0">Score Function A Score Function</definiendum>
				<definiens id="0">for a given syntactic tree , say Synj , is defined as follows : S~or~ ( s~ , ,~ ) _-v</definiens>
			</definition>
			<definition id="2">
				<sentence>Misclassification distance is defined as the difference between the score of the top candidate and that of the correct one .</sentence>
				<definiendum id="0">Misclassification distance</definiendum>
			</definition>
			<definition id="3">
				<sentence>syntactic score = \ [ 41.6 -0.7 -0.4 -0.3\ ] = 2.0 , log integrated score = -4.21 ; ( where * denotes the top candidate , and A denotes the desired candidate ) It is clear that after the second iteration , paranaeters have been adjusted so that the desired candidate ( i.e. , candidate 1 ) would be selected .</sentence>
				<definiendum id="0">*</definiendum>
				<definiendum id="1">A</definiendum>
				<definiens id="0">the desired candidate ) It is clear that after the second iteration , paranaeters have been adjusted so that the desired candidate ( i.e. , candidate 1 ) would be selected</definiens>
			</definition>
			<definition id="4">
				<sentence>I=\ ] `` leX tlJ + iOst/n `` ( 15 ) where do is a constant which stands for a window size , and e is the learning constant for controlling the speed of convergence .</sentence>
				<definiendum id="0">do</definiendum>
				<definiendum id="1">e</definiendum>
				<definiens id="0">a constant which stands for a window size</definiens>
				<definiens id="1">the learning constant for controlling the speed of convergence</definiens>
			</definition>
			<definition id="5">
				<sentence>Su , `` ArchTran : A Corpusbased Statistics-oriented English-Chinese Machine Translation System , '' Proc .</sentence>
				<definiendum id="0">ArchTran</definiendum>
				<definiens id="0">A Corpusbased Statistics-oriented English-Chinese Machine Translation System</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>A French grannnar may divide verbs into the first , secoud , and third conjugations ; German grammars speak of `` weak '' and `` strong '' verbs ; Spanish grammars classify verbs by their infiuitival endings , etc .</sentence>
				<definiendum id="0">German grammars</definiendum>
				<definiendum id="1">Spanish grammars</definiendum>
			</definition>
			<definition id="1">
				<sentence>Oar paradigm description language ( PDL ) is composcd of three major components form rules , an inheritance hierarchy of paradigms , and orthographic rules .</sentence>
				<definiendum id="0">Oar paradigm description language</definiendum>
				<definiendum id="1">PDL</definiendum>
				<definiens id="0">composcd of three major components form rules , an inheritance hierarchy of paradigms , and orthographic rules</definiens>
			</definition>
			<definition id="2">
				<sentence>Stems are any forms which include the primary \ [ exical base of tile word , whereas affixes comprise tile prefixes and suffixes which can be concatenated with a stein in the process of word formation .</sentence>
				<definiendum id="0">Stems</definiendum>
				<definiens id="0">any forms which include the primary \ [ exical base of tile word , whereas affixes comprise tile prefixes and suffixes which can be concatenated with a stein in the process of word formation</definiens>
			</definition>
			<definition id="3">
				<sentence>Form construction rides , are restrictexl to the five cases below : • &lt; form &gt; : &lt; stem &gt; + &lt; affix &gt; • &lt; form &gt; : &lt; stem &gt; &lt; affix &gt; • &lt; form &gt; : + &lt; affix &gt; &lt; stem &gt; • &lt; form &gt; : &lt; affix &gt; &lt; stem &gt; • &lt; forul &gt; : &lt; stein &gt; The &lt; lotto &gt; is a name for the string form created by the rule .</sentence>
				<definiendum id="0">Form construction rides</definiendum>
			</definition>
</paper>

		<paper id="3140">
			<definition id="0">
				<sentence>Finally , The core Prolog program ensuring that a sentence is well-formed is the following : well formed ( S ) : conceptuallywell_formed ( R ) , list of leaves ( D , S ) , sentence ( D , R ) .</sentence>
				<definiendum id="0">well-formed</definiendum>
				<definiens id="0">well formed ( S ) : conceptuallywell_formed ( R ) , list of leaves</definiens>
			</definition>
</paper>

		<paper id="4182">
			<definition id="0">
				<sentence>NP denotes a noun phrase , and SEF denotes a surface expression form .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiendum id="1">SEF</definiendum>
				<definiens id="0">a noun phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>Configuration : q-'he domain-dependent knowledge base consists of a network of nodes and links .</sentence>
				<definiendum id="0">Configuration</definiendum>
				<definiens id="0">q-'he domain-dependent knowledge base consists of a network of nodes and links</definiens>
			</definition>
			<definition id="2">
				<sentence>where agent Q is the questioner and S is tfie secretariat .</sentence>
				<definiendum id="0">agent Q</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">the questioner and</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>Partial parsing techniques have been used with a considerable success in processing large volumes of text , for example AT &amp; T 's Fidditch ( Hindle and Rooth , 1991 ) parsed 13 million words of Associated Press news messages , while MIT 's parser ( de Marcken , 1990 ) was used to process the 1 million word Lancasterslo/Bergen ( LOB ) corpus .</sentence>
				<definiendum id="0">Partial parsing techniques</definiendum>
				<definiendum id="1">Lancaster/Oslo/Bergen ( LOB</definiendum>
				<definiens id="0">used with a considerable success in processing large volumes of text , for example AT &amp; T 's Fidditch ( Hindle and Rooth , 1991 ) parsed 13 million words of Associated Press news messages</definiens>
			</definition>
			<definition id="1">
				<sentence>TTP is based on the Linguistic String Grammar developed by Sager ( 1981 ) .</sentence>
				<definiendum id="0">TTP</definiendum>
			</definition>
			<definition id="2">
				<sentence>Therefore , if X is timed out , and Y is a low contents adjunct phrase , we can make the parser to jump fight to the next nonterminal Z. In the clause production discussed before , subtail is skipped over if verbphrase is timed ouL 14 Finally , it is not an entirely trivial task to select non-terminals at which the input skipping can occur .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">a low contents adjunct phrase</definiens>
			</definition>
			<definition id="3">
				<sentence>t4 : mbta//it the remainder of a discontinued subject phrase .</sentence>
				<definiendum id="0">t4</definiendum>
				<definiens id="0">mbta//it the remainder of a discontinued subject phrase</definiens>
			</definition>
			<definition id="4">
				<sentence>In the rn predicate , SR is the list of starter tags and P is the parse tree fragment .</sentence>
				<definiendum id="0">SR</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">the parse tree fragment</definiens>
			</definition>
			<definition id="5">
				<sentence>OF COLING-92 , NANTES , AUG. 23-28 , 1992 TIP is a robust parser and it will process nearly every sentence or phrase , provided the latter is reasonably correctly tagged .</sentence>
				<definiendum id="0">TIP</definiendum>
				<definiens id="0">a robust parser and it will process nearly every sentence or phrase , provided the latter is reasonably correctly tagged</definiens>
			</definition>
			<definition id="6">
				<sentence>TIP has been used as front-end of a natural language processing component to a traditional document-based information retrieval system ( Strzalkowski and Vauthey , 1992 ) .</sentence>
				<definiendum id="0">TIP</definiendum>
				<definiens id="0">front-end of a natural language processing component to a traditional document-based information retrieval system</definiens>
			</definition>
			<definition id="7">
				<sentence>In Figure 1 , `` ITP has failed to find the main verb and it had to jump over much of the last phrase such as the LR ( k ) grammars , partly due to an improper tokenization of LR ( k ) ( note skipped nodes indicating the material ignored in the first pass ) .</sentence>
				<definiendum id="0">ITP</definiendum>
				<definiens id="0">has failed to find the main verb and it had to jump over much of the last phrase such as the LR ( k ) grammars</definiens>
			</definition>
</paper>

		<paper id="4213">
</paper>

		<paper id="3150">
			<definition id="0">
				<sentence>ABSTRACT LEXTER is a software package for extracting terminology .</sentence>
				<definiendum id="0">ABSTRACT LEXTER</definiendum>
				<definiens id="0">a software package for extracting terminology</definiens>
			</definition>
			<definition id="1">
				<sentence>A corpus of French language texts on any subject field is fed in , and LEXTER produces a list of likely terminological units to be submitted to an expert to be validated .</sentence>
				<definiendum id="0">LEXTER</definiendum>
				<definiens id="0">produces a list of likely terminological units to be submitted to an expert to be validated</definiens>
			</definition>
			<definition id="2">
				<sentence>In the first stage , LEXTER uses a base of rules designed to indentify frontier markers in view to analysing the texts and extracting maximallength noun phrases .</sentence>
				<definiendum id="0">LEXTER</definiendum>
				<definiens id="0">uses a base of rules designed to</definiens>
			</definition>
			<definition id="3">
				<sentence>OF COLING-92 , NANTES , AUG. 23-28 , 1992 3 ) How LEXTER works : analysis and parsing To detect terminological units , LEXTER takes the form of these units into consideration , and works in two phases : analysis and parsing .</sentence>
				<definiendum id="0">LEXTER</definiendum>
				<definiens id="0">takes the form of these units into consideration , and works in two phases : analysis and parsing</definiens>
			</definition>
			<definition id="4">
				<sentence>This principle , called `` of relative strictness '' , is justified in that it will be easier for the te~ninologist to eliminate certain likely units than to find real terminological units that escaped detection by LEXTER `` Introduction ~t une science du langage '' , Scull , Paris \ [ Monteil 1990\ ] Monteil Marie Gaelle , P~not Nadine ( 1990 ) , `` Indexation Automatique , fonctionnement Principes gtntraux '' , Note interne HN46464 , EDF , Direction des Etudes et Recherches , Service IPN , Clanlart AC'~E.S DE COLING-92 , N^tcr s , 23-28 , ~o~ 1992 9 8 l Pgoc .</sentence>
				<definiendum id="0">EDF</definiendum>
				<definiens id="0">likely units than to find real terminological units that escaped detection by LEXTER `` Introduction ~t une science du langage ''</definiens>
			</definition>
</paper>

		<paper id="2070">
			<definition id="0">
				<sentence>`` Sense '' is not a well defined concept ; it has been based on subjective and often subtle distinctions in topic , register , dialect , collocation , part of speech and valency .</sentence>
				<definiendum id="0">Sense</definiendum>
				<definiens id="0">a well defined concept ; it has been based on subjective and often subtle distinctions in topic , register , dialect , collocation , part of speech and valency</definiens>
			</definition>
			<definition id="1">
				<sentence>Y includes a broad set of relations , such as meronomy ( blade , engine , gear , wheel , shaft , tooth , piston and cylinder ) , typical functions of machines ( cut , rotate , move , turn , pull ) , typical objecls of those actions ( wood , metal ) , as well as typical modifiers for machines ( electric , mechanical , pneumatic ) .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">includes a broad set of relations , such as meronomy ( blade , engine , gear , wheel , shaft , tooth , piston and cylinder ) , typical functions of machines ( cut , rotate , move , turn , pull )</definiens>
			</definition>
			<definition id="2">
				<sentence>Pre-Nominal Modifiers : The disambiguation of prenominal modifiers ( adjectives and compound nominals ) is heavily dependent on the noun modified , and much less so on distant context .</sentence>
				<definiendum id="0">Pre-Nominal Modifiers</definiendum>
				<definiens id="0">The disambiguation of prenominal modifiers ( adjectives and compound nominals</definiens>
			</definition>
			<definition id="3">
				<sentence>Each of these approaches have faced a fundamental obstacle : word sense is an abstract concept that is not identified in natural texL Hence any system which hopes to acquire discriminators for specific senses of a word will need to isolate samples of those senses .</sentence>
				<definiendum id="0">word sense</definiendum>
			</definition>
			<definition id="4">
				<sentence>Miller , George ( 1990 ) , `` Woednea : An On-line Leaical Database , '' InterncUionalJournal of Lexicography , 4 ( 3 ) , 1990 .</sentence>
				<definiendum id="0">Woednea</definiendum>
				<definiens id="0">An On-line Leaical Database , '' InterncUionalJournal of Lexicography</definiens>
			</definition>
</paper>

		<paper id="2116">
			<definition id="0">
				<sentence>The FAS is a semantic representation for sentences which has been developed in the preceding phase of our project .</sentence>
				<definiendum id="0">FAS</definiendum>
			</definition>
			<definition id="1">
				<sentence>The representations Ri in the CAT2 system are derivational trees generated by context-free grammars G i which are made up of a pair ( Ci , Ai ) , where C is a set of Constructors ( structural rules ) and A a set of Atoms ( lexical rules ) .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">derivational trees generated by context-free grammars G i which are made up of a pair ( Ci , Ai ) , where</definiens>
				<definiens id="1">a set of Constructors ( structural rules</definiens>
			</definition>
			<definition id="2">
				<sentence>The nodes are pairs ( C , F ) , where C is a distinguished feature and F is a set of feature-value pairs .</sentence>
				<definiendum id="0">C</definiendum>
				<definiendum id="1">F</definiendum>
				<definiens id="0">a distinguished feature</definiens>
				<definiens id="1">a set of feature-value pairs</definiens>
			</definition>
			<definition id="3">
				<sentence>A TFS is an atomic or complex type .</sentence>
				<definiendum id="0">TFS</definiendum>
				<definiens id="0">an atomic or complex type</definiens>
			</definition>
			<definition id="4">
				<sentence>An atomic type consists of a type symbol and a complex type of a type symbol with a set of pairs of features and TFSes ( values ) .</sentence>
				<definiendum id="0">atomic type</definiendum>
				<definiendum id="1">TFSes</definiendum>
				<definiens id="0">consists of a type symbol and a complex type of a type symbol with a set of pairs of features</definiens>
			</definition>
			<definition id="5">
				<sentence>Definitions have the form T = TFSt v ... v TFS m : C , where T is a type symbol , the TFS~ are TVSes of type F i ( T _ &gt; Fi ) and C is a conditional constraint , which may be omitted and is expressed by a logical conjunction of TFSes .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">a type symbol</definiens>
			</definition>
			<definition id="6">
				<sentence>xluence h -~ t2 -~ , ... -~ t~ l -- &gt; t , ~ if and only if tl &gt; tz &gt; ... &gt; t,4 &gt; t~ , where tl is the input term , t , ~ the `` target term and t i - &gt; ti+t a derivation step , in which the resulting term t~+ t is derived from the original term ti by the application of one TR rule .</sentence>
				<definiendum id="0">tl</definiendum>
				<definiens id="0">the original term ti by the application of one TR rule</definiens>
			</definition>
			<definition id="7">
				<sentence>LANG is a special sort which represents the language to which a category belongs .</sentence>
				<definiendum id="0">LANG</definiendum>
			</definition>
			<definition id="8">
				<sentence>\ [ u order to compute the cycles of &gt; *w ' the transitive closure &gt; , +p~ is computed , which may contain an equivalence relation &gt; ~y~ ( &gt; ¢y~ is reflexive , symmetric and transitive , &gt; ¢y~ ~ &gt; ~+pp and &gt; ~y~ ~ Rc × Pc , where Rc ~ RN is the set of numbers of cyclic TR rules ) .</sentence>
				<definiendum id="0">Rc ~ RN</definiendum>
				<definiens id="0">may contain an equivalence relation &gt; ~y~ ( &gt; ¢y~ is reflexive , symmetric and transitive</definiens>
				<definiens id="1">the set of numbers of cyclic TR rules )</definiens>
			</definition>
			<definition id="9">
				<sentence>The rhs of n i is the rhs of hi , in which the variables are replaced according to the substitution o. The rbs of n~ is the lhs of n l , in which the subterm ~ , jhl is replaced by the rhs of n 2 , the variables of which are replaced according to AcrEs DE COLING-92 , NANTES , 23-28 Attar 1992 7 8 1 PROC .</sentence>
				<definiendum id="0">n~</definiendum>
				<definiens id="0">the lhs of n l , in which the subterm ~</definiens>
			</definition>
			<definition id="10">
				<sentence>If the applicable TR rale n is an alternative rule ( n E R^ ) , then all the TR rules m c In\ ] ~ R^ are applied alternatively .</sentence>
				<definiendum id="0">TR rale n</definiendum>
				<definiens id="0">an alternative rule ( n E R^ ) , then all the TR rules m c In\ ] ~ R^ are applied alternatively</definiens>
			</definition>
			<definition id="11">
				<sentence>The interpreter , which uses the basic unification algorithm of \ [ Eisele/DOrre 86\ ] to superpose the input term with the the lhs of the TR rules , is intended to be expanded with disjunction according to \ [ l ) Srre/Eisele 90\ ] .</sentence>
				<definiendum id="0">interpreter</definiendum>
				<definiens id="0">uses the basic unification algorithm of \ [ Eisele/DOrre 86\ ] to superpose the input term with the the lhs of the TR rules , is intended to be expanded with disjunction according to</definiens>
			</definition>
			<definition id="12">
				<sentence>With these additional features , term-rewriting is a powerful , elegant , complete and coherent device to describe the relations between all levels of representation in machine translation systems .</sentence>
				<definiendum id="0">term-rewriting</definiendum>
				<definiens id="0">a powerful , elegant , complete and coherent device to describe the relations between all levels of representation in machine translation systems</definiens>
			</definition>
			<definition id="13">
				<sentence>551-553 \ [ EmelefZajac 89\ ] : M. Emele , R. Zajac , `` RETIF : A Rewriting System lot Typed Feature Structures '' , ATR Technical Report TR-I-0071 1989 \ [ Gazdar et al. 851 : G. G~dar , E. Klein , G. Pullum and I. Sag : `` Generalized Phrase Structure Grammar '' , Oxford , Blackwell 1985 lHauenschild/Bnsemann 88\ ] : Ch .</sentence>
				<definiendum id="0">RETIF</definiendum>
				<definiens id="0">A Rewriting System lot Typed Feature Structures ''</definiens>
				<definiens id="1">Generalized Phrase Structure Grammar ''</definiens>
			</definition>
</paper>

		<paper id="3155">
			<definition id="0">
				<sentence>Italian is a language presenting a lot of syntactical problems , sucb as a rather unrestricted word order , unbounded agreement controls , long distance structure checkings and so on .</sentence>
				<definiendum id="0">Italian</definiendum>
				<definiens id="0">a language presenting a lot of syntactical problems , sucb as a rather unrestricted word order</definiens>
			</definition>
			<definition id="1">
				<sentence>the different interpretations of a sentence like 1 if it were found in the answer of a fool to a psychological test , in a novel about ancient chivalry , or in some essay titled 'Which is the best place for writing a letter ? ' )</sentence>
				<definiendum id="0">'Which</definiendum>
				<definiens id="0">the different interpretations of a sentence like 1 if it were found in the answer of a fool to a psychological test</definiens>
			</definition>
			<definition id="2">
				<sentence>DIMACheck is a general-purpose unification-based natural language parser that , while retaining computational effectiveness and linguistic expression power , stresses the concepts of monotonicy , declarativity and robustness .</sentence>
				<definiendum id="0">DIMACheck</definiendum>
				<definiens id="0">a general-purpose unification-based natural language parser that , while retaining computational effectiveness and linguistic expression power , stresses the concepts of monotonicy , declarativity and robustness</definiens>
			</definition>
			<definition id="3">
				<sentence>Formally a User Defined Operator ( UDO ) is function of the form Boolean &lt; DataTypcl '~ DalaTypc2 i.e. a UDO is a function mapping pairs of values belonging respectively to DataTypel and DataType2 onto boolean values. The composition rule ( the rule that associates the relevant boolean value to each pair ) is given explicitly , by listing all the value pairs that map onto true ( all the other ones are mapped automatically onto false ) . User Defined Functions ( UDFs ) have been introduced to stress the locality of computation. The basic idea is that each value inside a constraint ( be it an equality constraint or not ) may , in principle , be replaced by a function that computes it on the basis of some given parameters. So , whenever one must compute the value of an attribute which is known to depend on and only on a finite set of other features , AcrEs DE COLING-92 , NANTES. 23-28 AOl~rr 1992 1 0 0 4 PROC. OF COLING-92. NANTES , AUt\ ] . 23-28. 1992 instead of writing lots of rules which embed ( and hide ) this piece of knowledge into a larger description , it is possible to declare a UDF that manages the computation , thus reducing the number of rules from many to one. Formally , a UDF is a function that maps values from N data types into values of a given data type. in symbols : TargetDataType OataTypel* *DataTypeN UDFs are declared explicitely , more or less like UDOs : for each n-tuple of relevant values the result value is stated. UDFs need not to be deterministic : a given pair of input parameters may map into more than one target value. As stated above parsing is , in our view , applying cation a finite set of constraints over an input list of words. We may therefore distinguish between structural constraints ( the ones that deal with the order , the occurrences , etc. of parse trees ) and feature constraints , that put restrictions on the value of a given variable ( the value of an attribute inside the parse tree ) , The former kind is described in paragraph 34 , while the latter , is described here. A feature constraint , or simply a constraint , is a triple of the form : &lt; Operator , At|ributeName , ValueExpression - , Opcralor is the name of either a system defined operator ( ' - , ' and ... . ' ) or the name of a user-definod one ( e.g. 'is a ' ) . AttribulcNamc is a legal name for an attribute , the type of which matches the type foreseen by the operator for its lefthand side. ValucExprcssion may be an atomic value a single variable a disjunction of atomic values a user-defined function In our formalism a constraint is stated in an infix form ( e.g. 'tense = , pres ' or 'tense agreem tense ' T or 'tense compute tense ( M , T ) ' ) . When a constraint is applied 1o an object it may evaluate either to true or to false : we can therefore say that a constraint is a boolean function. The way in which the result of the application of the constraint is handled by the system leads to the distinction between strong and weak constraints. A strong constraint is a constraint that , if it fails , causes a strong failure , i.e. the object to which it applied is rejected , When a strong constraint succeeds nothing happens , apart from some possible variable binding. Strong constraints are used mainly to prevent useless overgeneration over irrelevant paths. ( Usually , but not necessarily , constraints that involve the major syntactic categories are strong ) . They are also used to propagate values from lower nodes to upper ones. A weak constraint is a constraint that behaves like a strong one if it evaluates to true , but which otherwise produces a soft failure. A soft failure simply consists in recording in the object the information that a weak constraint has failed , without rejecting it. In order to mantain trace of the failed constraints , they are annotated by the user with a number which is used at the end of parsing to generate a proper error message irrdicating which constraint failed and where. Apart from annotation , the syntax of weak constraints is the same of the strong ones , and the same restriction applies. A constraint bundle ( CB ) is a list of conjuncted constraints ( both weak and strong ) . Notationally , a CB is delimited by braces , single constraints are separated by commas ; a slash ( '/ ' ) splits the list in two parts : the strong one and the weak one. If the weak part is empty ' , the slash is omitted. Here are some examples of legal CB 's : { catnp / ( 1 ) nb ~ N , ( 2 ) gd G } { cat v : aux vlype .V/ ( 81 } cat v } { cat = pp t During parsing , the parse trees which are built are labelled with a list of pending constraints i.e. of constraints that have not yet proved to be true or falseand a score .. i.e. an indication of how many weak constraints associated to the tree have already proved to be false , Intuitively , the lower is the score , the better is the object. The constraint solver applies to the list of pending constraints of each final tree , trying to minimize the number of failed weak constraints. The constraint solver selects , as final result the tree with the smallest associated error It 's worth noting that this is a global strategy , not a local one. All parse trees , independently of their score are carried or\ ] up to the end of parsing , and only then the selection is made. There are two reasons for this choice. The first one is theoretical : it is not possible to assume that a locally well formed subtree will lead to a better global tree than that produced by a locally ill-formed one. The second reason is pragmatical : since constraints are solved only when the variables they involve get instantiated , partial trees tend to contain few or no failed weak constraints but long lists of constraints still to be evaluated. Applying the constraint solver in the middle of parsing would be a waste of time , and making the choice disregarding the ; ) ending constraints is definitely wrong. The system operates or\ ] the input data driven by rules. Rules mix together structure and feature constraints in order to produce a quasi-well-formed ( sub ) tree ( the 'quasi ' is there because the subtree may contain failed weak constraints : it would not be proper At'firs I ) E COL1NG-92 , NANTI~.S. 23-28 Ao~ , r 1992 l 0 0 5 PRec. ov COLING-92 , N^N'I'I~S , Auo. 23-280 1992 to call it a well.formed one ) . Rules are handled by a parser in order to produce all possible results. Currently the system uses a bottom-up , left to right algorithm. However the result is totally independent of the parsing strategy , Structure building rules ( sb-rules ) are augmented rewrite rules used to describe the structure of quasi well-formed subtrees.A sb-rule has the following form : &lt; RuleName &gt; = &lt; TopConstrainteundle &gt; = &gt; &lt; SubTreeExpre~sion 1 &gt; . ``</sentence>
				<definiendum id="0">User Defined Operator ( UDO</definiendum>
				<definiendum id="1">UDO</definiendum>
				<definiendum id="2">UDF</definiendum>
				<definiendum id="3">Opcralor</definiendum>
				<definiendum id="4">constraint bundle</definiendum>
				<definiendum id="5">CB )</definiendum>
				<definiendum id="6">Intuitively</definiendum>
				<definiens id="0">a function mapping pairs of values belonging respectively to DataTypel and DataType2 onto boolean values. The composition rule ( the rule that associates the relevant boolean value to each pair ) is given explicitly , by listing all the value pairs that map onto true ( all the other ones are mapped automatically onto false ) . User Defined Functions ( UDFs ) have been introduced to stress the locality of computation. The basic idea is that each value inside a constraint ( be it an equality constraint or not ) may , in principle , be replaced by a function that computes it on the basis of some given parameters. So , whenever one must compute the value of an attribute which is known to depend on and only on a finite set of other features</definiens>
				<definiens id="1">possible to declare a UDF that manages the computation</definiens>
				<definiens id="2">a function that maps values from N data types into values of a given data type. in symbols : TargetDataType OataTypel* *DataTypeN UDFs are declared explicitely , more or less like UDOs : for each n-tuple of relevant values the result value is stated. UDFs need not to be deterministic : a given pair of input parameters may map into more than one target value. As stated above parsing is , in our view , applying cation a finite set of constraints over an input list of words. We may therefore distinguish between structural constraints ( the ones that deal with the order , the occurrences , etc. of parse trees ) and feature constraints , that put restrictions on the value of a given variable ( the value of an attribute inside the parse tree ) , The former kind is described in paragraph 34 , while the latter , is described here. A feature constraint , or simply a constraint , is a triple of the form : &lt; Operator , At|ributeName , ValueExpression - ,</definiens>
				<definiens id="3">the name of either a system defined operator ( ' - , ' and ... . ' ) or the name of a user-definod one ( e.g. 'is a ' ) . AttribulcNamc is a legal name for an attribute , the type of which matches the type foreseen by the operator for its lefthand side. ValucExprcssion may be an atomic value a single variable a disjunction of atomic values a user-defined function In our formalism a constraint is stated in an infix form ( e.g. 'tense = , pres ' or 'tense agreem tense ' T or 'tense compute tense ( M , T ) ' ) . When a constraint is applied 1o an object it may evaluate either to true or to false : we can therefore say that a constraint is a boolean function. The way in which the result of the application of the constraint is handled by the system leads to the distinction between strong and weak constraints. A strong constraint is a constraint that , if it fails , causes a strong failure , i.e. the object to which it applied is rejected , When a strong constraint succeeds nothing happens , apart from some possible variable binding. Strong constraints are used mainly to prevent useless overgeneration over irrelevant paths. ( Usually , but not necessarily , constraints that involve the major syntactic categories are strong ) . They are also used to propagate values from lower nodes to upper ones. A weak constraint is a constraint that behaves like a strong one if it evaluates to true , but which otherwise produces a soft failure. A soft failure simply consists in recording in the object the information that a weak constraint has failed , without rejecting it. In order to mantain trace of the failed constraints , they are annotated by the user with a number which is used at the end of parsing to generate a proper error message irrdicating which constraint failed and where. Apart from annotation , the syntax of weak constraints is the same of the strong ones</definiens>
				<definiens id="4">a list of conjuncted constraints ( both weak and strong ) . Notationally , a CB is delimited by braces , single constraints are separated by commas ; a slash ( '/ ' ) splits the list in two parts : the strong one and the weak one. If the weak part is empty ' , the slash is omitted. Here are some examples of legal CB 's : { catnp / ( 1 ) nb ~ N , ( 2 ) gd G } { cat v : aux vlype .V/ ( 81 } cat v } { cat = pp t During parsing , the parse trees which are built are labelled with a list of pending constraints i.e. of constraints that have not yet proved to be true or falseand a score .. i.e. an indication of how many weak constraints associated to the tree have already proved to be false ,</definiens>
				<definiens id="5">the list of pending constraints of each final tree , trying to minimize the number of failed weak constraints. The constraint solver selects , as final result the tree with the smallest associated error It 's worth noting that this is a global strategy , not a local one. All parse trees</definiens>
				<definiens id="6">theoretical : it is not possible to assume that a locally well formed subtree will lead to a better global tree than that produced by a locally ill-formed one. The second reason is pragmatical : since constraints are solved only when the variables they involve get instantiated , partial trees tend to contain few or no failed weak constraints but long lists of constraints still to be evaluated. Applying the constraint solver in the middle of parsing would be a waste of time , and making the choice disregarding the ; ) ending constraints is definitely wrong. The system operates or\ ] the input data driven by rules. Rules mix together structure and feature constraints in order to produce a quasi-well-formed ( sub ) tree ( the 'quasi ' is there because the subtree may contain failed weak constraints : it would not be proper At'firs I ) E COL1NG-92 , NANTI~.S. 23-28 Ao~ , r 1992 l 0 0 5 PRec. ov COLING-92 , N^N'I'I~S , Auo. 23-280 1992 to call it a well.formed one ) . Rules are handled by a parser in order to produce all possible results. Currently the system uses a bottom-up , left to right algorithm. However the result is totally independent of the parsing strategy , Structure building rules ( sb-rules ) are augmented rewrite rules used to describe the structure of quasi well-formed subtrees.A sb-rule has the following form : &lt; RuleName &gt; = &lt; TopConstrainteundle &gt; = &gt; &lt; SubTreeExpre~sion 1 &gt;</definiens>
			</definition>
			<definition id="4">
				<sentence>The resulting CB is the union of the CB associated to the ending of the word and the CB defined in the lexical rule associated to the root .</sentence>
				<definiendum id="0">CB</definiendum>
				<definiens id="0">the union of the CB associated to the ending of the word and the CB defined in the</definiens>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>A feature structure is either atomic or complex : an atomic FS is denoted by an atomic symbol ; a complex FS consists of a set of feature-value pairs each of which describes an aspect of an object .</sentence>
				<definiendum id="0">feature structure</definiendum>
				<definiens id="0">an atomic FS is denoted by an atomic symbol ; a complex FS consists of a set of feature-value pairs each of which describes an aspect of an object</definiens>
			</definition>
			<definition id="1">
				<sentence>A TFS consists of a type symbol from a lattice and a set of rearm : e-value pairs .</sentence>
				<definiendum id="0">TFS</definiendum>
			</definition>
			<definition id="2">
				<sentence>structure , and an e-type is a maximal set of ¢ types representing the disjunction of them .</sentence>
				<definiendum id="0">e-type</definiendum>
				<definiens id="0">a maximal set of ¢ types representing the disjunction of them</definiens>
			</definition>
			<definition id="3">
				<sentence>The partial order &lt; ~denotes the subsumption relation between these sets ; for any type symbols a , b , and c , A feature symbol denotes a function from a subset of U to U. A feature path is a finite string of feature symbols and denotes the function obtained by tile composition of the functions that tile feature symbols denote .</sentence>
				<definiendum id="0">feature symbol</definiendum>
				<definiens id="0">a function from a subset of U to U. A feature path is a finite string of feature symbols and denotes the function obtained by tile composition of the functions that tile feature symbols denote</definiens>
			</definition>
			<definition id="4">
				<sentence>T , r is a type symbol function fi'om 2-* to T such that r ( f* A ) = { T } , and v is a tag symbol 5ruction front A to Y. Given a tag symbol fimction v , Addr .</sentence>
				<definiendum id="0">r</definiendum>
				<definiendum id="1">v</definiendum>
				<definiens id="0">a type symbol function fi'om 2-* to T such that r ( f* A ) = { T } , and</definiens>
				<definiens id="1">a tag symbol 5ruction front A to Y. Given a tag symbol fimction v</definiens>
			</definition>
			<definition id="5">
				<sentence>Definition 5 An augmented term is a quintuple ( A , r , o , ¢ , X ) where A is a term domain on 5 v , r is a type symbol timer ( on from ~'* to T such that r ( 2-* A ) = { T } , v is a tag symbol function front A to V , ¢ is an inhibited feature filnction front 5 r* to 2 ~ such that ¢ ( p ) is finite for any p ( 5 A and ~ ( ~'* A ) = { 0 } , and X is a disagreement tag symbol function from J'* to 2 v such that X ( P ) is finite for any p ( 5 A and X ( f'* A ) _ { 0 } , 2 The inhibited feature fimction ¢ specifies which features can not exist at a given address .</sentence>
				<definiendum id="0">r</definiendum>
				<definiendum id="1">v</definiendum>
				<definiendum id="2">¢</definiendum>
				<definiendum id="3">X</definiendum>
				<definiens id="0">a tag symbol function front A to V</definiens>
			</definition>
			<definition id="6">
				<sentence>Definition 8 A well-formed term ( wft ) is a referentially-consistent regnlar term .</sentence>
				<definiendum id="0">well-formed term</definiendum>
			</definition>
			<definition id="7">
				<sentence>First , the join of t~ and t2 , ta = tl V t2 = ( Aa , ra , Va , ~ba , Xa ) , is defined as follows : Aa = Alna= ( 10a ) va : Aa -- -* ~1 such that Ker ( va ) = ~x Ntis , ( lOb ) and Vp E .</sentence>
				<definiendum id="0">lOb</definiendum>
				<definiens id="0">ta = tl V t2 = ( Aa , ra</definiens>
			</definition>
			<definition id="8">
				<sentence>A node structure consists of live fields : lsymbol for a type symbol , arcs for a set of feature-vafile pairs , ifeatures for a set of inhibited features , dnodes for a set of disagreement nodes i.e. , disagreement K-classes , and forward .</sentence>
				<definiendum id="0">node structure</definiendum>
				<definiens id="0">consists of live fields : lsymbol for a type symbol , arcs for a set of feature-vafile pairs</definiens>
			</definition>
			<definition id="9">
				<sentence>Latin : a logic programming language with built-in inheritance .</sentence>
				<definiendum id="0">Latin</definiendum>
				<definiens id="0">a logic programming language with built-in inheritance</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>* argument node ; arft specifies the node for the argnment being introduced by the entry .</sentence>
				<definiendum id="0">arft</definiendum>
				<definiens id="0">specifies the node for the argnment being introduced by the entry</definiens>
			</definition>
			<definition id="1">
				<sentence>1992 TRANSITIVE superelasses : VERB nodes : vp , v , lip via description : v np constraints equations : ... art : np s : completion ( v ) v = head-daughier ( vp ) ailehor ~ The following entry is associated with the class of verbs taking an NP as indirect objects ( IOBJ ) which may be possibly found within a prepositional phrase or not : IOBJ superclasses : VERB nodes : vp , v , np constraints equations : ... arg : np vp description : /\\ v i~p anchor = v PP-IOBJ superclasaes : IOBJ nodes : vp , v , pp , p , np constraints equations : ... art : pp vp A description : v A p np np : arg ( IOBJ ) anchor = v The following entry is associated with the class of ditransitive verbs taking a noun phrase as direct object and a prepositional phrase as an indirect object .</sentence>
				<definiendum id="0">VERB</definiendum>
				<definiens id="0">nodes : vp , v , lip via description : v np constraints equations : ... art : np s : completion ( v ) v = head-daughier ( vp</definiens>
				<definiens id="1">indirect objects ( IOBJ ) which may be possibly found within a prepositional phrase or not : IOBJ superclasses : VERB nodes : vp , v , np constraints equations : ... arg : np vp description : /\\ v i~p anchor = v PP-IOBJ superclasaes : IOBJ nodes : vp , v , pp , p , np constraints equations : ... art : pp vp A description : v A p np np : arg ( IOBJ ) anchor = v The following entry is associated with the class of ditransitive verbs taking a noun phrase as direct object</definiens>
			</definition>
			<definition id="2">
				<sentence>Copy ( x ) indicates that a copy of the description of entire sub-tree rooted at node x needs to recorded in output description .</sentence>
				<definiendum id="0">Copy</definiendum>
				<definiens id="0">a copy of the description of entire sub-tree rooted at node x needs to recorded in output description</definiens>
			</definition>
			<definition id="3">
				<sentence>We say that C~ = CItANGE-ARITY ( C1 ) if CI is the immediate superclass of C1 distinct from TRANSITIVE .</sentence>
				<definiendum id="0">CI</definiendum>
				<definiens id="0">the immediate superclass of C1 distinct from TRANSITIVE</definiens>
			</definition>
</paper>

		<paper id="1003">
</paper>

		<paper id="2105">
			<definition id="0">
				<sentence>Note that we assume for simplicity ttlat linguistic signs are represented with terms sign ( LF , Str , Syn , Der ) where LF represents tile semantic information , Str represents the string and Syn represents syntactic information .</sentence>
				<definiendum id="0">LF</definiendum>
				<definiendum id="1">Syn</definiendum>
				<definiens id="0">tile semantic information</definiens>
				<definiens id="1">the string</definiens>
				<definiens id="2">syntactic information</definiens>
			</definition>
			<definition id="1">
				<sentence>monitor ( sign ( LF , Str , Syn , Der ) ) : geuerate ( sign ( LF , Str , Syn , Der ) ) , unambiguous ( Sir ) .</sentence>
				<definiendum id="0">monitor ( sign ( LF , Str , Syn , Der ) )</definiendum>
			</definition>
			<definition id="2">
				<sentence>In our examples each sign sign ( LF , Str , Syn , D ) is specified for its corresponding derivation tree D. In Prolog such a tree is represented with terms of the form t ; ( Label , Ds , M ) where Label is the node name ( the unique name of a rule ) and Ds is a list of Daughter trees .</sentence>
				<definiendum id="0">Label</definiendum>
				<definiendum id="1">Ds</definiendum>
				<definiens id="0">the node name ( the unique name of a rule</definiens>
			</definition>
			<definition id="3">
				<sentence>The following definition ~sumes that grammar rules are represented simply as rule ( Name , No'thor , Ds ) where Name is the rule name , Mother is the mother sign and Ds is a list of daughter signs .</sentence>
				<definiendum id="0">Mother</definiendum>
				<definiendum id="1">Ds</definiendum>
				<definiens id="0">rule ( Name , No'thor , Ds ) where Name is the rule name ,</definiens>
				<definiens id="1">the mother sign</definiens>
				<definiens id="2">a list of daughter signs</definiens>
			</definition>
			<definition id="4">
				<sentence>~ monitored_genorat ion ( LF , Sign ) : generate ( sign ( LF , Str , Syn , D ) ) , !</sentence>
				<definiendum id="0">LF , Sign )</definiendum>
				<definiens id="0">generate ( sign ( LF , Str , Syn</definiens>
			</definition>
</paper>

		<paper id="4183">
			<definition id="0">
				<sentence>UNIFORM RECOGNITION INSTANCE : A grammar G and a string w. QUESTION : Is w in the language generated by G ?</sentence>
				<definiendum id="0">UNIFORM RECOGNITION INSTANCE</definiendum>
			</definition>
			<definition id="1">
				<sentence>Definitions A grammar is a 4-tuple , G = ( V , E , R , S ) , where V is a set of symbols , : E C V is the set of terminal symbols .</sentence>
				<definiendum id="0">grammar</definiendum>
				<definiendum id="1">S</definiendum>
				<definiendum id="2">V</definiendum>
				<definiens id="0">a 4-tuple , G = ( V , E , R ,</definiens>
				<definiens id="1">a set of symbols , : E C V is the set of terminal symbols</definiens>
			</definition>
			<definition id="2">
				<sentence>Derivability ( - % ) between strings is defined as follows : uc~v ~ uflv ( u , v , c* , fl E V* ) iff ( ~ , fl ) E R. The transitive closure of - % is denoted by =L~ .</sentence>
				<definiendum id="0">Derivability</definiendum>
			</definition>
			<definition id="3">
				<sentence>The notation we use for context-sensitive rules is ms follows : the rule aZfl -- -* ceTfl is written as Z -~ \ [ all\ [ a~\ ] ... iakl 3 ' \ [ flllL621 ... \ [ fill with ~ : C~la2 ... ¢~k andfl = fllfl2 ... fll , ai , flj E V ( l &lt; i &lt; k , l &lt; j_ &lt; l ) . An example of a context-sensitive grammar with the corresponding context-flee rules is : context-sensitive rules context-free part 1 -~ \ [ 0\ ] 2 1 ~ 2 0-~ i \ [ 21 0 -~ 1 2 - , \ [ 1\ ] 0 2-~ 0 This contextMsensitive grammaris cyclic. Iris able to permute ( } 's and its , Recognition is NP-complete UNIFORM RECOGNITION FOR ACYCLIC CONTEXT-SENSITIVE GRAMMAR INSTANCE : An acyehc context-sensitive grammar G = ( V , Z , R , S ) and a string w G E*. QUESTION : Is w in the language generated by G ? The proof can be found in Aarts \ [ 1991b\ ] . To prove that it is in NP wc have to prove that derivations ill ACSG 's aa'e short ( have polynomial length ) . Tiffs follows from the fact that derivations in context-free grammars have polynomial length. Derivations in an acyclie CSG are identical with derivations in the associated contextfree grammar. The proof of NP-hardness is more complicated. The known NP-hard problem 3-SAT can be reduced to UNIFORM RECOGNITION for ACSG. Any 3-SAT formula can be translated in a grammar and an input for ACSG-recogultion. AcrEs DE COLING-92 , NANTES , 23-28 Aour 1992 1 1 5 9 PROc , O1 : COLING-92. NAbrl'ES , AUG. 23-28 , 1992 Recognizing Power Any context-free grammar can be transformed into ant acyclic context-free grammar without loss of recognizing power. A cycle can be removed by introduction of a new symbol. This symbol rewrites to any member of the cycle. Any context-free grammar with empty productions can bc changed into a context-free grammar without empty productions that recognizes the same language. There 's one exception here : languages containing the empty string can not be generated. Any acyclic context-free grammar withont empty productions is an acyclic context-sensitive granlmar. Therefore , ACSG 's recognize all context-free 10alguages that do not contain the empty word. Furthermore , acyclic context-sensitive granamars recognize languages that are not contextfree. One example is the language { anb2~c** In &gt; 1 } This language is recognized by the grammar ( `` X '' is a nouternfinM ) : X~ \ [ A\ ] ABB \ [ B\ ] B-~\ [ A\ ] X\ [ X\ ] A-4a x -~ IX\ ] B B\ [ B\ ] B - .</sentence>
				<definiendum id="0">Recognition</definiendum>
				<definiens id="0">NP-complete UNIFORM RECOGNITION FOR ACYCLIC CONTEXT-SENSITIVE GRAMMAR INSTANCE : An acyehc context-sensitive grammar G = ( V , Z , R , S ) and a string w G E*. QUESTION : Is w in the language generated by G ? The proof can be found in Aarts \ [ 1991b\ ] . To prove that it is in NP wc have to prove that derivations ill ACSG 's aa'e short ( have polynomial length ) . Tiffs follows from the fact that derivations in context-free grammars have polynomial length. Derivations in an acyclie CSG are identical with derivations in the associated</definiens>
				<definiens id="1">any member of the cycle. Any context-free grammar with empty productions can bc changed into a context-free grammar without empty productions that recognizes the same language. There 's one exception here : languages containing the empty string can not be generated. Any acyclic context-free grammar withont empty productions is an acyclic context-sensitive granlmar. Therefore , ACSG 's recognize all context-free 10alguages that do not contain the empty word. Furthermore</definiens>
			</definition>
</paper>

		<paper id="3136">
</paper>

		<paper id="3154">
			<definition id="0">
				<sentence>A Language for the Statement of Binary Relation over Feature Structures .</sentence>
				<definiendum id="0">Language for</definiendum>
			</definition>
</paper>

		<paper id="3167">
			<definition id="0">
				<sentence>Topic structures consist of topic segments , topics in the segment and relations between the segments : nests or conjunctions .</sentence>
				<definiendum id="0">Topic structures</definiendum>
				<definiens id="0">consist of topic segments , topics in the segment and relations between the segments : nests or conjunctions</definiens>
			</definition>
			<definition id="1">
				<sentence>Figure 1 : Post Of lice Dialogue TOPIC &gt; EMPATHY &gt; subject &gt; object &gt; others TOPIC is a noun phrase marked by the postpositional particle `` ~ ( wa ) '' .</sentence>
				<definiendum id="0">TOPIC</definiendum>
				<definiens id="0">a noun phrase marked by the postpositional particle</definiens>
			</definition>
			<definition id="2">
				<sentence>EMPATHY includes the subject of mental verbs such as tg .</sentence>
				<definiendum id="0">EMPATHY</definiendum>
				<definiens id="0">includes the subject of mental verbs such as tg</definiens>
			</definition>
			<definition id="3">
				<sentence>If TOPIC does not exist but EMPATHY does , EMPATHY is a topic .</sentence>
				<definiendum id="0">EMPATHY</definiendum>
				<definiens id="0">a topic</definiens>
			</definition>
			<definition id="4">
				<sentence>Utterance A~4 generates a new topic unit and the candidate list is reset to an empty set .</sentence>
				<definiendum id="0">Utterance A~4</definiendum>
				<definiens id="0">generates a new topic unit and the candidate list is reset to an empty set</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>`` Free Order '' TAGs is a variant analogous to the ID/LP format for GPSG which was first introduced by Joshi 1987b and developed by Becket et al. 1991 .</sentence>
				<definiendum id="0">TAGs</definiendum>
				<definiens id="0">a variant analogous to the ID/LP format for GPSG which was first introduced by Joshi 1987b and developed by Becket et al. 1991</definiens>
			</definition>
			<definition id="1">
				<sentence>They can he used for relating two TAGs for two different hutguages for the purpose of machine translation ( Abeilld et al. 1990 ) , or lk~r relating a NI \ e ( /can gives it to him ) where first clilic=N1 , second clltic =N2 , and for Jean me le donne ( Jean gives it m me ) where first clitic=N2 , secemd clitic -N1 .</sentence>
				<definiendum id="0">Jean</definiendum>
			</definition>
			<definition id="2">
				<sentence>`` A Lexicalized Tree Adjoining Grammar for English '' , Tech .</sentence>
				<definiendum id="0">A Lexicalized</definiendum>
				<definiens id="0">Tree Adjoining Grammar for English ''</definiens>
			</definition>
			<definition id="3">
				<sentence>`` XTAG : a graphical Workbench for developing TAGs '' , 3rd Conf .</sentence>
				<definiendum id="0">XTAG</definiendum>
			</definition>
			<definition id="4">
				<sentence>`` Parsing strategies with lexicalized grammars : Tree adjoining grammars '' , 12th COLING , Budapest .</sentence>
				<definiendum id="0">lexicalized grammars</definiendum>
				<definiens id="0">Tree adjoining grammars ''</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>Intransitive and transitive verbs , are detined as follows ( where +STRUCT denotes a positive vMue for STRUCT ) : ( 20 ) IV : ( ( NI'Nom ) \S ) .</sentence>
				<definiendum id="0">+STRUCT</definiendum>
				<definiens id="0">a positive vMue for STRUCT ) : ( 20 ) IV : ( ( NI'Nom ) \S )</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , hjalp~i denotes the feature struture ( where IVis left unexpanded ) : dir right \ [ cat ~ : I ' L arg \ ] l~'~ ; uct aatjj Lnom Note also that vantar assigns \ ] exicM ( i.e. nonstructural ) accusative case to its subject .</sentence>
				<definiendum id="0">hjalp~i</definiendum>
				<definiendum id="1">exicM</definiendum>
				<definiens id="0">the feature struture ( where IVis left unexpanded ) : dir right \ [ cat ~ : I ' L arg \ ] l~'~ ; uct aatjj Lnom Note also that vantar assigns \ ]</definiens>
			</definition>
</paper>

		<paper id="4209">
			<definition id="0">
				<sentence>23-28 , 1992 ( ROBRA is the tree transtlacor designed by GETA in the ARIANE MT system , cL \ [ Boitet 82\ ] , \ [ Boitet 86\ ] ) .</sentence>
				<definiendum id="0">ROBRA</definiendum>
				<definiens id="0">the tree transtlacor designed by GETA in the ARIANE MT system</definiens>
			</definition>
			<definition id="1">
				<sentence>\ [ Hutchins 86\ ] HUTCHINS , W.J. , `` MACHINE TRANSLATION : past , present , future '' , Chichester , Ellis Horwood series in Computer and their applications , 1986 .</sentence>
				<definiendum id="0">MACHINE TRANSLATION</definiendum>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>One of the problems of both these variations of the standard PSapproach is the description of adjuncts ( free modifiers ) .</sentence>
				<definiendum id="0">PSapproach</definiendum>
				<definiens id="0">the description of adjuncts ( free modifiers )</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus one , ,dj ( ... . 1,1/ \ ~f ~o NP ~x~P N ( boy ) V ( a &lt; ~ A~t ( an ) / ~ Np ~\ ] Vp N ( app\ ] e ) ACRES DE COLING-92 , NAN'\ [ 'IkS , 23-28 AOt\ ] T 1992 1 8 5 l'~tOC .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">Vp N ( app\ ] e ) ACRES DE COLING-92</definiendum>
				<definiens id="0">a &lt; ~ A~t ( an ) / ~ Np ~\ ]</definiens>
			</definition>
</paper>

		<paper id="2081">
			<definition id="0">
				<sentence>ULTRA ( Universal Language TRAnslator ) is a multilingual , interlingual machine translation system which currently translates between five languages ( Chinese , English , German , Japanese , Spanish ) with vocabularies in each language based on about 10,000 word senses .</sentence>
				<definiendum id="0">ULTRA ( Universal Language TRAnslator )</definiendum>
				<definiens id="0">a multilingual , interlingual machine translation system which currently translates between five languages ( Chinese , English , German , Japanese , Spanish ) with vocabularies in each language based on about 10,000 word senses</definiens>
			</definition>
			<definition id="1">
				<sentence>Relations : @ the sense token indexes a corresponding LDOCE word sense definition , • whether it is stative or dynamic , • the semantic class , @ the number of case roles , • the case roles , • the semantic preferences for the fillers of the case roles , @ the LDOCE semantic preferences for the fillers of the case roles , @ the LDOCE subject domain ; In the case of relations , LDOCE does not provide case roles or semantic classes ( for verbs ) , or a direct marking as to whether a verb is stative or dynamic .</sentence>
				<definiendum id="0">LDOCE</definiendum>
				<definiens id="0">the semantic class , @ the number of case roles , • the case roles , • the semantic preferences for the fillers of the case roles , @ the LDOCE semantic preferences for the fillers of the case roles</definiens>
				<definiens id="1">does not provide case roles or semantic classes ( for verbs</definiens>
			</definition>
			<definition id="2">
				<sentence>Extraction is performed by applying a sequence of flex programs ( a new generation version of the UNIX lexical analyzer utility , lex ) which transform information from the LDOCE Lisp format into a Lisp association list , the data structure used by the interactive lcxical entry interface for the ULTRA system ( sample screens appear in the previous secton ) .</sentence>
				<definiendum id="0">Extraction</definiendum>
				<definiendum id="1">flex programs</definiendum>
				<definiendum id="2">lex</definiendum>
				<definiens id="0">a new generation version of the UNIX lexical analyzer utility</definiens>
			</definition>
</paper>

		<paper id="4174">
			<definition id="0">
				<sentence>GPSG is a grmnmar fomvalism that states most ol ils generalizations on the level of local trees .</sentence>
				<definiendum id="0">GPSG</definiendum>
			</definition>
			<definition id="1">
				<sentence>cifications of the input ID rnle are tanled over to file output ID rule if not specilied otherwise by the melarule , For example the metamle VP\ [ -PAS\ ] -- ~ W , NP\ [ acc\ ] ~ VP\ [ +PAS\ ] -- ) W , ( PPIby\ ] ) states the connection between active and passive , where W is a vltriablc ranging over a ( possibly empty ) nmltiset of categories .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">a vltriablc ranging over a ( possibly empty ) nmltiset of categories</definiens>
			</definition>
			<definition id="2">
				<sentence>We think that the restriction on the application of metarules imposed by the Finite Closure ( FC ) is too strong .</sentence>
				<definiendum id="0">Closure</definiendum>
				<definiens id="0">the restriction on the application of metarules imposed by the Finite</definiens>
			</definition>
			<definition id="3">
				<sentence>Extraposition Metarule : V 3 -- - ) V ° , W , X\ [ -COH\ ] V3\ [ SLASH1 XI-COH\ ] \ ] ~ V ° , W The metarule for passive is an example in which the termination conditions ( ii ) and ( iii ) are necessary , because no category is deleted and an optional prepositio~ml phrase is introduced that replaces the accusative determiner phrase : Passive Metarule : V3\ [ -PAS\ ] -~ V o , W , DP\ [ acc\ ] V3\ [ +PASI -~ V ° , W , ( PP\ [ von\ ] ) .</sentence>
				<definiendum id="0">passive</definiendum>
				<definiendum id="1">Metarule</definiendum>
				<definiens id="0">deleted and an optional prepositio~ml phrase is introduced that replaces the accusative determiner phrase : Passive</definiens>
			</definition>
			<definition id="4">
				<sentence>detailed description of file parser without direct application of metandes ) , the metarules are defined according to the following scheme : Co ~ Co , W , Cd ~ C6 ~ C~ , W , ( C~ ) Co , Cc and Cd arc categories and W is a variable for a ( possibly empty ) multiset of categories .</sentence>
				<definiendum id="0">W</definiendum>
			</definition>
			<definition id="5">
				<sentence>Cd is the category which is to be deleted or modified .</sentence>
				<definiendum id="0">Cd</definiendum>
				<definiens id="0">the category which is to be deleted or modified</definiens>
			</definition>
			<definition id="6">
				<sentence>Suppose the Completer tries to complete with the inactive edge ( C~ , i , j , T ' { } , ~ ) , which is spanning li'om node i to node j of the chart , where ~ ' is a multiset of daughter categories which have already been analysed and the remainder , i.e. the tnultiset of daughter categories still to be analysed , is empty and C , ~ is the mother category of the ID rule , which is licensing this edge .</sentence>
				<definiendum id="0">inactive edge</definiendum>
				<definiens id="0">spanning li'om node i to node j of the chart</definiens>
			</definition>
			<definition id="7">
				<sentence>M is the ~t of metarules .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">the ~t of metarules</definiens>
			</definition>
			<definition id="8">
				<sentence>If ( Co , h , i , ¢z-13 ) is an inactive edge and ( C0~C~ , W , Cd : :~ C6 ~C'o , W , C , ~ ) ~ Mand C~ c ~ L ) m ~ and Cd E D and C~ is consistent to the categories in ~/ { C , l } m with respect to linear precedence then the Completer introduces a new edge ( C6 , h , j. 0¢ u , , , ( C~ } ~-~/ { C , j } m ) and the category C~ in O~ t.J~ 13 is replaced by C~ .</sentence>
				<definiendum id="0">Completer</definiendum>
				<definiens id="0">introduces a new edge ( C6 , h</definiens>
			</definition>
</paper>

		<paper id="2088">
			<definition id="0">
				<sentence>Function UNIFY-CONJ ( f , g ) retnrns one possihle 34uple of feature descriptions &lt; &lt; h , fr , gr ~- ' : where f and g are feature descriptions , and h is a unified feature description , and fr , gr are r~t parts of f , g that are not used to generate h. ( .</sentence>
				<definiendum id="0">Function UNIFY-CONJ</definiendum>
				<definiendum id="1">h</definiendum>
				<definiens id="0">a unified feature description , and fr , gr are r~t parts of f</definiens>
			</definition>
			<definition id="1">
				<sentence>Finally , CA is defined ms CA : ) U CAND .</sentence>
				<definiendum id="0">CA</definiendum>
			</definition>
			<definition id="2">
				<sentence>E ( h ) calculates the validity of a unified feature description h. This function returns a 2-tnple of real numbers s , ( xl , x2 ) ( xl , x2 E R ( set of real numbers ) ) , where xl is the number of word pairs extracted from bilingual dictionaries and contained ill the unified feature description , on the other hand x~ is tile number of word pairs aLso contained in the unified feature description but not extracted from bilingual dictionaries .</sentence>
				<definiendum id="0">E ( h )</definiendum>
				<definiendum id="1">xl</definiendum>
				<definiens id="0">calculates the validity of a unified feature description h. This function returns a 2-tnple of real numbers s , ( xl , x2 ) ( xl , x2 E R ( set of real numbers ) )</definiens>
			</definition>
			<definition id="3">
				<sentence>Function SCORE ( h ) returns ( xl , x~ ) ( xl , x2 ( 5 R ( set of real numbers ) ) : where h is a unified feature description .</sentence>
				<definiendum id="0">Function SCORE</definiendum>
				<definiendum id="1">h</definiendum>
				<definiens id="0">a unified feature description</definiens>
			</definition>
			<definition id="4">
				<sentence>score = ( 4 , 0 ) pred : ( write , ~ ( ) tense : past ( o~j , \ [ spec : a \ ] score = ( 3 , 0 ) pred : ( write , ~ &lt; ) tense : past ( ~ , ,bj , ~ ) : \ [ pr , a : ( x , ~ , ) \ ] | spec : a J &lt; o~j , ~ ) : \ [ , ..h \ [ , ,~.1 : pe , ,.l \ ] L `` L spec : a j -¢ : \ [ prea : ~ , i \ ] Tt~e prepositional phrase `` with a pencil '' modifies the verb `` wrote '' m the upper feature description. The score of tile upper feature description is greater than that of tile lower one. So in this ease , the upper one is regarded as tile correct ease frame example for tile pair ( write , ~ '' &lt; ) . Experiment and Evaluation in order to evaluate how well syntactic ambiguities of translation examples are resolved , we made all experiment of syntactic disambiguation using 189 translation examples extracted from a J apanese-English dictionary. Firstly , each sentence of a translation exampie is syntactically analyzed and translated into feature descriptions. For 44 translation examples , syntactic analysis of tile Japanese or English sentence is faile.d. For those which are successfully analyzed , the average number of feature descriptions generated from one scntcncc is 4.4 for Japanese and 17.1 for English. Secondly , these feature descriptions are unified. After this process of syntactic disamhiguation , from 86 translation examples , a uniquc ee~sc framc of the unified verb pair of Japanese and English is acquired. Calculating from this result , the success ratio of acquiring unified case frames of verbs , ( the number of translation examples such that a unique unified case frame of verbs is acquired from each translation exampie ) / ( tile uumher of translation examples such that each sentence is successfully analyzed ) , is 86/145 = 59.3 % . And the success ratio of syntactic disambiguation , ( tile number of sentences such that a unique ease frame of the verb is acquircd from more than one feature descriptions ) / ( tile number of sentences such that more than one feature descriptions are originally generated ) , is 70/103 = 68.0 % for Japanese , and 84/133 = 63.2 % for English. tion of Verbs As described ill 2.2 , a feature description unified between English aud Japanese is as below. pred : ( write , ~ &lt; ) tense : past obj ¢~ pred : ( letter ~5.\ ] ~ ) F pred &lt; p , , , ea , ) I ( h -c ) / .wtt , `` : L spec : a J This feature description tells that tile verbal concept represented hy tile pair of the English verb `` tv~te '' and the Japane~qe verb `` ~ &lt; `` have at least three eases that are marked by some syntactic information mid some surface functional words such as ( subj , *2 ) , ( obj , ~ ) , ( with , T ' ) . it also tells that each case takes a certain nominal coueept represented by tile pair of English and Japanese words , such as U , *h &gt; , &lt; fetter , ~ ; : ~ ) , ( pe , leit , ~ ) . Once a large amount of this kind of data is collected , statistical data ahout case frames of verbs eaal he extracted , making use of a thesaurus of nominM concepts 7. In the remainder of this section , we will illustrate a general procedure for acquiring case frames of verbs. Lct us start with a collection of a large amount of unified feature descriptions like above for a specific Japanesc verb V~. Suppose that we want to get possible case frames of this verb. By a case frame , we mean something tikc a feature description for this verb , consisting of surface cases each of which is marked hy a postpositional particlc p~ and some specific semantic categories taken from a thesaurus like BGI\ [ . Usually , a verh has several distinct case frames. However , it is not easy to extract those case frames automatically only from the collected unified feature descriptions. So the system finds critical points to distinguish possible case frames for a verh using some heuristics , then it asks tile human instructor whether the distinctions of ease frames arc correct. These heuristics and human interactions arc smmnarized as follows. 7At present , ~m oiL-line thesaurus called 'Bunrui Goi Hyou ' ( BGH ) \ [ 8\ ] is available for Japanese. BGII has a sixlayered abstraction hierarchy mrd more t|mat 60 , OOO words are assigned at the leaves. At the presettt stage , it is ntot certain whether this the~sautim is reliable enouglt for our initial research target of acquiring case frames of verbs. It is , however~ the most precise and broad coveri|kg 3apsmeae thesaurus obtahtable for us , currently. ACRES DE COLING-92 , NANTES , 23-28 ho~r 1992 5 8 5 PRO ( . OF COLING-92 , NAN'I'ES , AUG. 23-28 , 1992 Heuristics First , collect the nouns marked by pj in a feature description of the verb Vj from the set of unified feature descriptions. Then mark each collected noun in the thesaurus. If the most specific common layer of the marked nouns is low enough , then we assume that the case marked by pj takes a noun of the semantic category that corresponds to that layer. But if the most specific common layer is higher than a predetermined layer s , the information provided by that layer is too general for tile semantic categories of the case marked by pj. For instance , it is quite rare that both an animate concept and an abstract concept can be the subject of a certain verb. Such a case strongly suggests that the verb has at least two distinct conceptual meanings or two distinct case frames. It then becomes necessary to classify the marked nouns in the thesaurus. Some of the heuristics come from the advantages of bilingual intersection of concepts , which we have already shows in Chapter 1 as semantic disambiguation. For a Japanese verb Vj and its case marked by a postpositional particle p j , suppose that unified feature descriptions such as \ [ pred : ( VEl , Vj ) , ( IEI , pj ) : { NE1 , NJI ) \ ] and \ [ pred : ( VE2 , Vj ) , ( IE2 , pJ ) : ( NE2 , NJ2 ) \ ] are ohtained. Both of these two feature descriptions have a feature label pj for Vj. llowever , if VE1 and V~2 are different verbs or IEl and IE2 are different feature labels , these two feature descriptions may be classified into different case frames of the verb Vj. Another heuristics are related to sentence patterns of verbs. Sometimes the ease marked by pj has a correlation with other eases in sentence patterns. If the correlations between cases are detected , then it helps tile classification , and some sentence patterns ( or c0.se frames ) of the verb Vj will be aeqnired. Human Interactions As described above , the system can find critical points to distinguish possible case frames for a verb by those heuristics. The system , however , can not determine the distinction only with positive data collected from examples. The main purpose of human interaction is to obtain negative examples. The system asks the human instructor whether a case marked by p J1 and another case marked by P J2 call co-occur or not. If STile \ ] ) redetermined layers depend on tile thesaurus we are dealing with. Table 1 : Semantic Marker of IPAL CON concrete ABS abstract ANI animal ACT action HUM human MEN mental ORG organization LIN linguistic products PLA plant CHA character PAR parts REL relation NAT natural LOC location PRO products TIM time QUA quantity I PHE I pt ... ... ... . II DIV I dl ... .. Table 2 : Acquired Case Slots for '~ &lt; ( write ) '' Case Slots Sere. Mark. Freq. Examples ( subj , ~ ; ; t ~ , ¢ ) HUM 95 $ /~ ( l ) ( obj , t : t • ¢ ) REL , ~ ( letter ) , ( \ [ subj , passive\ ] , QUA , 153 ~fi~/ ( name ) I $ • h~ ) L1N ( with , `` ~ ) PRO 10 `` ~ 5 '' ( per , ) ( in , ~ '' ) L1N , ~.~ ( kanji ) ItEL 28 Jt~ ( form ) ( on , ~ : ) PRO 16 i~ ( paper ) ! to , iS. ) 11UM 13. ~ ( father ) . they can not co=occur , then the system learns that Vj } lets at least two sentence patterns ( or case frames ) and that one of them has the case marked by P J1 and tile other has the case marked by P J2An example of human interactions of this type is shown in next section. It is often said that hand-made semantic dictionary contains quite unstable data , which means that it strongly depends on the human composer. In order to acquire stable lexicat knowledge base , we decided to limit hmnan interactions to yes-no type of questions and answers , such that the system asks the human instructor whether something is true or false so that he can answer only yes or no. Wc have collected about 50,000 translation exampies from a machine readable dal ) anese-English dictionary and an English learners ' textbook. In this bilingual corpus , about 70 distinct Japanese verbs appear in more than 100 examples. We have obtained unified feature descriptions for several verbs which appeared more than 200 times. From them we have gotten some case frames. In this experiment we used the set of semantic markers defined in IPAL \ [ 4\ ] , listed in Table 1. Table 2 shows the case slots of `` ~ &lt; ( write ) '' extracted from 207 translation examples. In the process of extraction , bilingual feature label pairs are quite uscfut to find different case slots that are marked by the same postpositional particle in Japanese. In order to acquire ease fralne.s of tile verb `` ~ &lt; ( write ) '' from Ac'u~s DE COLING-92 , NAtCrF.S , 23-28 Ao ( rr 1992 5 8 6 PROC. OF COLING-92 , NANTES , AUG. 23-28 , 1992 'Fable 3 : Acquired Case Frames for `` ~-\ [ &lt; ( wr ; le ) '' ( 7~Lse Frame I Ca.~e Frlmm 2 15 ( on ) PRO V ( to ) HUM l~t . :6¢ ( sub3 ) IIUM ~ ; t • fie ( subj ) HUM ~ ) REL , l ; ~ `` ~ ( obj ) J3.. ~ ( QUA , t : \ ] : • fit ( LIN \ [ subj , passive\ ] ) LIN \ [ subj , p , ,ssive\ ] ) `` ~ '' ( with ) ~ ( with ) -e ( i , , ) -e ( i , , ) the extracted cm~e slots , ttle systenr ~sks the human instructor about the pcx~sibi\ [ ities of tile co-occurrence of the case slots that do not cc.occur in the trans lation examples by composing saml ) le phr , '~ses. The questions and answers are as follows. QUESTION 1 : C'a , I say , ,..'t Y ( pen ) -r. ( with ) l~.~ , l~ ( English ) `` C ( in ) ~ &lt; { write ) '' ? ... ... . YES. QUESTION 2 : Cat , I say `` 2 -I- '' U.a , 'd ) ~5 ( o , , ) 5t \ [ I , ,the , ' ) ~5 ( t , , ) ~ &lt; ( , mr , O '' ... .. NO. The postpositional particle `` ~ : '' is used to mark two different cases of the verb `` ~ &lt; ( write ) '' in Japm~ese sentences. One of them represents things on which smnething is written like in `` wrile something on , sh~ : et of paper '' , and the other reI ) resents someone to whom a correspondence is written , like ill `` wtalc a letter to a lover '' . The difference of these two usages is clear by tit ( : bilingual feature label pairs ( on , ~= ) and ( to , { ~ ) . 'File human instructor answers that only these two ease slots can not co-occur. Then two case frames are obtained as in Table 3. This simple experiment suggests that it is quite possible to acquire case frames of verbs from bilingual corpora if enough translation examples are available. Actually , on tim assumption that 200 translation examples arc necessary for acquiring case framcs of onc verb , 100,000 translation examples are necessary for 70 verbs. If a bilingual corpus of 1,000,000 translation examples is obtained , it is possible to compile a semantic dictionary with the same scale as IPAI , through a little interaction with a human instructor for each verb. Wc think it possible to construct a bilingual corl ) us of that scale or more in the near fitlure , We haw~ proposed a method for resolving the syntactic ambiguities of translation examples of bilingual corpora and a method for acquiring case frames of verbs. At present , we are extending our prototype system for acquiring case frames of verbs , attd the detail of the extended system will be reported in the future. We believe that the I ) roposed method is applicable to sew : ral otller problenrs as well. One of them is to acquire features of nominal concepts. We are at the moment looking at some specitie nominal expression `` A q ) B '' in Japanese , corresponding literally to `` I1 of A '' in English. That expression specifies a variety of relationships of noun phrases , which are often stated in different expressions in English. They will help to acquire typical attributes of nominal concepts fl'om bilingual corpora. Our ntethod is also useful to collect parsed traamlation examples tbr example-based translation \ [ 9\ ] attd to acquire translation patterns between two languages. \ [ l\ ] \ [ h'cnt , M. : `` Automatic Acquisition of Subcategorization Frames from Untagged Text '' , f &gt; roc , of the 29th Anuual Meeti.9 of the AUL , 1991 .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">write , ~ '' &lt; ) . Experiment and Evaluation in order to evaluate how well syntactic ambiguities of translation examples</definiens>
				<definiens id="1">tile number of sentences such that a unique ease frame of the verb is acquircd from more than one feature descriptions ) / ( tile number of sentences such that more than one feature descriptions are originally generated )</definiens>
				<definiens id="2">asks tile human instructor whether the distinctions of ease frames arc correct. These heuristics and human interactions arc smmnarized as follows. 7At present , ~m oiL-line thesaurus called 'Bunrui Goi Hyou '</definiens>
				<definiens id="3">necessary to classify the marked nouns in the thesaurus. Some of the heuristics come from the advantages of bilingual intersection of concepts</definiens>
				<definiens id="4">Semantic Marker of IPAL CON concrete ABS abstract ANI animal ACT action HUM human MEN mental ORG organization LIN linguistic products PLA plant CHA character PAR parts REL relation NAT natural LOC location PRO products TIM time QUA quantity I</definiens>
				<definiens id="5">ral otller problenrs as well. One of them is to acquire features of nominal concepts. We are at the moment looking at some specitie nominal expression `` A q ) B '' in Japanese , corresponding literally to `` I1 of A '' in English. That expression specifies a variety of relationships of noun phrases , which are often stated in different expressions in English. They will help to acquire typical attributes of nominal concepts fl'om bilingual corpora. Our ntethod is also useful to collect parsed traamlation examples tbr example-based translation \ [ 9\ ] attd to acquire translation patterns between two languages. \</definiens>
			</definition>
</paper>

		<paper id="4170">
			<definition id="0">
				<sentence>One of the main goals of an interpreter is to map the syntactic descriptions found in the sentence into the correct roles that the elements ( described by the nominals ) play in the situation at hand ( described by the verb ) .</sentence>
				<definiendum id="0">an interpreter</definiendum>
				<definiens id="0">to map the syntactic descriptions found in the sentence into the correct roles that the elements ( described by the nominals ) play in the situation at hand ( described by the verb )</definiens>
			</definition>
			<definition id="1">
				<sentence>The passivization rule induces only changes of function : the SUBJ becomes the BYcomplement and the OBJ becomes the SUBJ .</sentence>
				<definiendum id="0">SUBJ</definiendum>
				<definiens id="0">becomes the BYcomplement and the OBJ becomes the SUBJ</definiens>
			</definition>
			<definition id="2">
				<sentence>attachment , DEP represents a deep dependency between two words that are structurally independent .</sentence>
				<definiendum id="0">DEP</definiendum>
				<definiens id="0">a deep dependency between two words that are structurally independent</definiens>
			</definition>
			<definition id="3">
				<sentence>Consider 5a ) Mary gave the book to John , where Mary is the 1-element , the book is the 2-element and John is the 3-element .</sentence>
				<definiendum id="0">Mary</definiendum>
				<definiendum id="1">John</definiendum>
				<definiens id="0">the 3-element</definiens>
			</definition>
			<definition id="4">
				<sentence>The semantic interpretation process takes advantage of the functional analysis , i.e. the analysis in terms of grammatical functions : relational structures are easily mapped onto logical representations , because of the resemblance between a Relational Network and a Predicate-Argument structure : the initial stratum states which are the grammatical relations ( actually the elements at the sentence level ) that act as arguments of the predicate identified by P. From a computational point of view , syntactic and semantic clues must be taken into account , in order to map the grammatical relations onto the surface descriptions .</sentence>
				<definiendum id="0">semantic interpretation process</definiendum>
				<definiens id="0">takes advantage of the functional analysis</definiens>
			</definition>
			<definition id="5">
				<sentence>The semantic check , which is activated on tbe basic relation ( i.e. OBJ ) validates such an assignment , because a girl that works at the wardrobe may happen to be persuaded .</sentence>
				<definiendum id="0">semantic check</definiendum>
				<definiens id="0">activated on tbe basic relation ( i.e. OBJ ) validates such an assignment , because a girl that works at the wardrobe may happen to be persuaded</definiens>
			</definition>
			<definition id="6">
				<sentence>When we find the verb comprare ( buy ) , the PR group assigns to such a description the SUB-Goal relation and consequently the lexical rule associated with persuadere assigns the SUBJ relation of the initial stratum of comprare to la ragazza the ... . The initial stratum of comprare features also an OBJ relation , that will be assigned to eneiclopedia , when it is found .</sentence>
				<definiendum id="0">OBJ relation</definiendum>
				<definiens id="0">the SUB-Goal relation and consequently the lexical rule associated with persuadere assigns the SUBJ relation of the initial stratum of comprare to la ragazza the ...</definiens>
			</definition>
</paper>

		<paper id="3127">
			<definition id="0">
				<sentence>For example , the sentence Some target was hit by every arrow has one reading on which the quantified noun phrase ( NP ) some target has wider scope than the quantified NP every arrow ( some particular target got hit by all the arrows ) , and another reading on which every arrow has wide scope ( each arrow hit some target or other ) .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">some particular target got hit by all the arrows</definiens>
			</definition>
			<definition id="1">
				<sentence>Organization of Scoping Module Given a parse tree , PSM returns a list of the preferred scope orders of the quantified phrases .</sentence>
				<definiendum id="0">PSM</definiendum>
				<definiens id="0">returns a list of the preferred scope orders of the quantified phrases</definiens>
			</definition>
			<definition id="2">
				<sentence>The top-level scoping procedure calls the horizontal scoping procedure ( H-SCOPE ) for the top-level clause of the parsed input .</sentence>
				<definiendum id="0">top-level scoping procedure</definiendum>
				<definiens id="0">calls the horizontal scoping procedure ( H-SCOPE ) for the top-level clause of the parsed input</definiens>
			</definition>
			<definition id="3">
				<sentence>RIO is initialized with strong natural orders , i.e. naturally ordered pairs which must stay that way .</sentence>
				<definiendum id="0">RIO</definiendum>
				<definiens id="0">initialized with strong natural orders , i.e. naturally ordered pairs which must stay that way</definiens>
			</definition>
</paper>

		<paper id="3147">
			<definition id="0">
				<sentence>The B-SURE systcm is iml ) lemented as a series of extensions to a classical ATMS .</sentence>
				<definiendum id="0">B-SURE systcm</definiendum>
				<definiens id="0">a series of extensions to a classical ATMS</definiens>
			</definition>
			<definition id="1">
				<sentence>Real sitnations denote situations as they actually are in the real world .</sentence>
				<definiendum id="0">Real sitnations</definiendum>
				<definiens id="0">situations as they actually are in the real world</definiens>
			</definition>
			<definition id="2">
				<sentence>A slate consists of a proposition about the world .</sentence>
				<definiendum id="0">slate</definiendum>
				<definiens id="0">consists of a proposition about the world</definiens>
			</definition>
			<definition id="3">
				<sentence>A situation is a set of positive and negative ( withdrawn ) states .</sentence>
				<definiendum id="0">situation</definiendum>
				<definiens id="0">a set of positive and negative ( withdrawn ) states</definiens>
			</definition>
			<definition id="4">
				<sentence>1992 A transition instance is defined as a happens mssumption .</sentence>
				<definiendum id="0">transition instance</definiendum>
				<definiens id="0">a happens mssumption</definiens>
			</definition>
			<definition id="5">
				<sentence>The probability of an outcome situation i occurring following performance of an uncertain action is estimated using the new estimator ~ instead of ~ , where m is the total number of previously observed trials of that action type , k~ is the previously observed number of ith situation-type outcomes , and n is the number of known possible outcome situations from that action .</sentence>
				<definiendum id="0">m</definiendum>
				<definiendum id="1">k~</definiendum>
				<definiendum id="2">n</definiendum>
				<definiens id="0">the total number of previously observed trials of that action type</definiens>
				<definiens id="1">the previously observed number of ith situation-type outcomes , and</definiens>
				<definiens id="2">the number of known possible outcome situations from that action</definiens>
			</definition>
			<definition id="6">
				<sentence>The BEIIOLDER scheduler uses the B-SURE system to keep track of which processes are running and which have been executed .</sentence>
				<definiendum id="0">BEIIOLDER scheduler</definiendum>
				<definiens id="0">uses the B-SURE system to keep track of which processes are running and which have been executed</definiens>
			</definition>
</paper>

		<paper id="2089">
			<definition id="0">
				<sentence>Neff , Byrd , and Rizk describe a lexical database ( the IBM LDB ) based on an unnormalized ( also Non First Normal Form or NF 2 ) relational data model , in which attribute values may be nested relations with their own internal structure ( see Abiteboul and Bidoit , 1984 ; Roth et al. , 1988 ) .</sentence>
				<definiendum id="0">lexical database</definiendum>
				<definiens id="0">the IBM LDB ) based on an unnormalized ( also Non First Normal Form or NF 2 ) relational data model , in which attribute values may be nested relations with their own internal structure</definiens>
			</definition>
			<definition id="1">
				<sentence>The outermost table consists of a rclation between a headword and some number of homographs .</sentence>
				<definiendum id="0">outermost table</definiendum>
				<definiens id="0">consists of a rclation between a headword and some number of homographs</definiens>
			</definition>
			<definition id="2">
				<sentence>In turn , a homograph consists of a part of speech , a grammar code , and some number of senses , etc .</sentence>
				<definiendum id="0">homograph</definiendum>
			</definition>
			<definition id="3">
				<sentence>The domain of values for form is feature structures of type FORM , which consists of feature structures whose legal features include orth , hyph , and pron .</sentence>
				<definiendum id="0">FORM</definiendum>
				<definiens id="0">consists of feature structures whose legal features include orth , hyph , and pron</definiens>
			</definition>
</paper>

		<paper id="3156">
			<definition id="0">
				<sentence>DIPETT ( Domain-Independent Parser for English Technical Texts ) is a linguistic-theoryneutral parser with a broad surface-syntactic coverage of English .</sentence>
				<definiendum id="0">DIPETT</definiendum>
				<definiens id="0">a linguistic-theoryneutral parser with a broad surface-syntactic coverage of English</definiens>
			</definition>
			<definition id="1">
				<sentence>The lexical analyzer builds a list of annotated words with the root form and the syntactic parameters .</sentence>
				<definiendum id="0">lexical analyzer</definiendum>
				<definiens id="0">builds a list of annotated words with the root form and the syntactic parameters</definiens>
			</definition>
			<definition id="2">
				<sentence>At present , we discontinue a parse operation that exceeds the time allowed for a single parse ( specified by the user at the beginning of a session ) .</sentence>
				<definiendum id="0">parse operation</definiendum>
				<definiens id="0">specified by the user at the beginning of a session )</definiens>
			</definition>
</paper>

		<paper id="3131">
			<definition id="0">
				<sentence>The U-node represenls the speech situation whose dreumetances form the dependency relations .</sentence>
				<definiendum id="0">U-node</definiendum>
				<definiens id="0">represenls the speech situation whose dreumetances form the dependency relations</definiens>
			</definition>
</paper>

		<paper id="3151">
			<definition id="0">
				<sentence>This function permits , therefore , to move upward ( towards the root node ) in the structure of the sets .</sentence>
				<definiendum id="0">to move upward</definiendum>
				<definiens id="0">towards the root node</definiens>
			</definition>
</paper>

		<paper id="4177">
			<definition id="0">
				<sentence>Aspect can be informally defined as a property which reflects the temporal organization of an event , such as duration ( whether an event involves change over time ) , telicity ( whether it has a definite endpoint or is ongoing ) , iteratlvlty ( whether or not it is repetitive ) and so on ( see Comrie 1976 . )</sentence>
				<definiendum id="0">telicity</definiendum>
				<definiens id="0">a property which reflects the temporal organization of an event , such as duration ( whether an event involves change over time )</definiens>
				<definiens id="1">whether or not it is repetitive ) and so on ( see Comrie 1976</definiens>
			</definition>
			<definition id="1">
				<sentence>Coercion can be defined as the process by which verbs in context appear to demonstrate some regular ambiguities , i.e. they appear to change categories .</sentence>
				<definiendum id="0">Coercion</definiendum>
			</definition>
			<definition id="2">
				<sentence>For example , Passoneau demonstrates how , without an ~ccurate specification of the ~pectual tendencies of the verb coupled with the effect of temporal and aspectual adjuncts , messages , which tend to be in the present tense , ttre not correctly understood nor generated in the PUNDIT system .</sentence>
				<definiendum id="0">Passoneau</definiendum>
			</definition>
			<definition id="3">
				<sentence>O000 ) ( n = 258 ) e-type ( be ) = V , ( .0124 ) ( n = 12482 ) e-type ( seem ) = V , ( .0000 ) ( n = 260 ) Figure Four Additional Syntactic Tests The progressive test is only one of several tests , and in and of itself is certainly inadequate .</sentence>
				<definiendum id="0">O000 )</definiendum>
				<definiens id="0">n = 260 ) Figure Four Additional Syntactic Tests The progressive test is only one of several tests , and in and of itself is certainly inadequate</definiens>
			</definition>
</paper>

		<paper id="2117">
			<definition id="0">
				<sentence>The representation of a sentence is a bag of ( extensions of ) lexical items , called its base .</sentence>
				<definiendum id="0">representation of a sentence</definiendum>
				<definiens id="0">a bag of ( extensions of ) lexical items , called its base</definiens>
			</definition>
</paper>

		<paper id="3132">
			<definition id="0">
				<sentence>Examples : on ( Act ( Nora ) ) zemfel \ [ he died\ ] Jan ( Act ( Nora ) ) vidfi_ rii ( ob3 ( Ace ) ) \ [ John sees Mary\ ] ze semene ( Obj ( Prep ( z ) 4Gen ) ) rostl strom ( Act ( Nora ) ) \ [ from a seed grew a tree\ ] to ( obj ( Nora ) ) se mi ( Act ( Dat ) ) libl \ [ it to me appeals\ ] ( I like it ) Ma~ .</sentence>
				<definiendum id="0">Obj</definiendum>
				<definiens id="0">Act ( Nora ) ) \ [ from a seed grew a tree\ ] to ( obj ( Nora ) ) se mi ( Act ( Dat ) ) libl \ [ it to me appeals\ ] ( I like it ) Ma~</definiens>
			</definition>
</paper>

		<paper id="4187">
			<definition id="0">
				<sentence>A principle is a constraint that must apply everywhere mid which defines the admissible data .</sentence>
				<definiendum id="0">principle</definiendum>
				<definiens id="0">a constraint that must apply everywhere mid which defines the admissible data</definiens>
			</definition>
</paper>

		<paper id="2099">
</paper>

		<paper id="4200">
			<definition id="0">
				<sentence>SINTESI extracts the diagnostic information by performing a full text analysis ; it is based on a semantics driven approach integrated by a general syntactic module and it is able to cope with the complexity of the ( sub ) language , maintaining both accuracy and robustness .</sentence>
				<definiendum id="0">SINTESI</definiendum>
				<definiens id="0">extracts the diagnostic information by performing a full text analysis ; it is based on a semantics driven approach integrated by a general syntactic module</definiens>
			</definition>
			<definition id="1">
				<sentence>S1NTESI integrates five knowledge sources : lexical , syntactic , pragmatic , general semantic and world knowledge .</sentence>
				<definiendum id="0">S1NTESI</definiendum>
			</definition>
			<definition id="2">
				<sentence>The TDS is an expectation-driven analysis of the connections among objects , driven by the main roles of some kinds of constituents ( verbs , conjunctions , etc. ) .</sentence>
				<definiendum id="0">TDS</definiendum>
				<definiens id="0">an expectation-driven analysis of the connections among objects</definiens>
			</definition>
			<definition id="3">
				<sentence>Robustness is achieved during the object recognition because the syntactic analyser is able to cope with some kinds of ill-formed input ( the loss of prepositions and determiners ) , without introducing extra-rules .</sentence>
				<definiendum id="0">Robustness</definiendum>
				<definiens id="0">the loss of prepositions and determiners</definiens>
			</definition>
</paper>

		<paper id="2118">
			<definition id="0">
				<sentence>A MAss noun denotes an unbounded region in physical or mental space ( butter , water ) .</sentence>
				<definiendum id="0">MAss noun</definiendum>
				<definiens id="0">an unbounded region in physical or mental space ( butter , water )</definiens>
			</definition>
			<definition id="1">
				<sentence>This generalization is achieved by the predication of an EVaLUaTION : indispensable is an EWtLUX'rIVE adjective , and if the speaker utters an evaluation this results in a habitual meaning which implies TEMPORAL UNBOUNDEDNESS in the absence of conflicting conditions ( cf. rule ( 14 ) in the annex ) .</sentence>
				<definiendum id="0">predication of an EVaLUaTION</definiendum>
				<definiens id="0">an EWtLUX'rIVE adjective , and if the speaker utters an evaluation this results in a habitual meaning which implies TEMPORAL UNBOUNDEDNESS in the absence of conflicting conditions</definiens>
			</definition>
			<definition id="2">
				<sentence>In ( 8 ) the relative clause by virtue of its predicate liefcrn , which denotes a~ ACHIEVEMENT -narrows down the unboundedness which Information expresses in its basic meaning to that amount which holds for a specific period of time ( cf. rule ( 2 ) in the annex ) : ( 8 ) Die Information , die 9eliefert wird ... The information which is given ... ( 9 ) Die 1ndasfrie , die entwickelt wird , braucht finauzielle Untcrst~tzun9 .</sentence>
				<definiendum id="0">predicate liefcrn</definiendum>
				<definiens id="0">denotes a~ ACHIEVEMENT -narrows down the unboundedness which Information expresses in its basic meaning to that amount which holds for a specific period of time ( cf. rule ( 2 ) in the annex ) : ( 8 ) Die Information , die 9eliefert wird ... The information which is given</definiens>
			</definition>
			<definition id="3">
				<sentence>COMPARISON adjectives such as similar behave in the same way by identifying the specific part of the unbounded MAss which is compased , ms we can only compare what we can identify ( cf. rule ( 8 ) in the annex ) : ( 17 ) Die veryleichbare lnformatiou \ [ chit .</sentence>
				<definiendum id="0">COMPARISON</definiendum>
				<definiendum id="1">MAss</definiendum>
				<definiens id="0">adjectives such as similar behave in the same way by identifying the specific part of the unbounded</definiens>
			</definition>
			<definition id="4">
				<sentence>CAT2 consists in stepwise translation between two linguistically motivated leveis , both in source language analysis and in target language synthesis .</sentence>
				<definiendum id="0">CAT2</definiendum>
				<definiens id="0">consists in stepwise translation between two linguistically motivated leveis , both in source language analysis and in target language synthesis</definiens>
			</definition>
</paper>

		<paper id="4202">
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>In this paper , we define a referring expression in intentional terms : a noun phrase is considered to be a referring expression if and only if its only communicative purpose is to identify an object to the hearer , in Kronfeld 's terminology \ [ Kro86\ ] , we only use the modal aspect of Donefian 's distinction between attributive and referential descriptions \ [ Don66\ ] ; we consider a noun phrase to be referential if it is intended to identify the object it describes to the hearer , and attributive if it is intended to communicate information about that object to the hearer .</sentence>
				<definiendum id="0">attributive</definiendum>
				<definiens id="0">if it is intended to communicate information about that object to the hearer</definiens>
			</definition>
			<definition id="1">
				<sentence>user-knows ( object , attribute-value-pair ) returns true if the user knows or can easily determine ( e.g. , by direct visual perception ) that the attribute-valuc pair applies to the object ; false if the user knows or can easily determine that the attribute-value pair does not apply to the object ; and unknown otherwise .</sentence>
				<definiendum id="0">user-knows</definiendum>
				<definiens id="0">the object ; false if the user knows or can easily determine that the attribute-value pair does not apply to the object</definiens>
			</definition>
			<definition id="2">
				<sentence>Here , r is the intended referent , C is the contrast set , P is the list of preferred attributes , D is the set of distractom ( contrast set members ) that have not yet been ruled out , and L is the list of attribute-value pairs returned , a make-referring-expression is the top level function .</sentence>
				<definiendum id="0">C</definiendum>
				<definiendum id="1">P</definiendum>
				<definiendum id="2">D</definiendum>
				<definiendum id="3">L</definiendum>
				<definiendum id="4">make-referring-expression</definiendum>
				<definiens id="0">the intended referent ,</definiens>
				<definiens id="1">the list of preferred attributes ,</definiens>
				<definiens id="2">the set of distractom ( contrast set members ) that have not yet been ruled out , and</definiens>
				<definiens id="3">the list of attribute-value pairs returned , a</definiens>
			</definition>
			<definition id="3">
				<sentence>ll ) hS is a natural lm~guage generation system that generates on-line documentation and help texts from a domain arid linguistic knowledge base , lining user expertise models , user task models , and discourse models .</sentence>
				<definiendum id="0">ll ) hS</definiendum>
				<definiens id="0">a natural lm~guage generation system that generates on-line documentation and help texts from a domain arid linguistic knowledge base</definiens>
			</definition>
			<definition id="4">
				<sentence>IDAS uses a KL-ONE type knowledge repr~entation system , with roles corresponding to attributes and lillem to values .</sentence>
				<definiendum id="0">IDAS</definiendum>
				<definiens id="0">uses a KL-ONE type knowledge repr~entation system , with roles corresponding to attributes and lillem to values</definiens>
			</definition>
			<definition id="5">
				<sentence>A *preferred-attributes* list has been crcated for IOAS 's domain ( complex electronic machinery ) by visual inspection of the equipment being documented ; its first members are type , colour , and label .</sentence>
				<definiendum id="0">*preferred-attributes* list</definiendum>
				<definiens id="0">complex electronic machinery ) by visual inspection of the equipment being documented</definiens>
			</definition>
			<definition id="6">
				<sentence>In this case , in the equipment rack is navigation information that is intended to bring the equipment rack and its components into the hearer 's focus of attention , while black power supply is discrimination information that is intended Ix ) distinguish the intended referent from other members of the context ~t ( e.g. , the white power supply that is also present in the equipment rack ) .</sentence>
				<definiendum id="0">navigation information</definiendum>
				<definiens id="0">discrimination information that is intended Ix ) distinguish the intended referent from other members of the context ~t</definiens>
			</definition>
</paper>

		<paper id="3138">
			<definition id="0">
				<sentence>A linguistic description consists of a set of feature type definitions which contain information about the placement of the feature term in the type hierarchy and about the set of well-formedness constraints that hold for this particular type .</sentence>
				<definiendum id="0">linguistic description</definiendum>
				<definiens id="0">consists of a set of feature type definitions which contain information about the placement of the feature term in the type hierarchy and about the set of well-formedness constraints that hold for this particular type</definiens>
			</definition>
			<definition id="1">
				<sentence>In this expression , DEVOUR , C00KIE , and KIM are concepts ill the domain model that are subordinated to types defined in the upper nmdel in the standard way defined for interfacing with PENMAN-style systems , favor , set-totality-individuality are semantic correlates of textual inquiries concerning the comnmnicatire functions of referring expressions , speechact is the semantic correlate of an interpersonal inquiry concerned with illocutionary force .</sentence>
				<definiendum id="0">KIM</definiendum>
				<definiendum id="1">speechact</definiendum>
				<definiens id="0">the semantic correlate of an interpersonal inquiry concerned with illocutionary force</definiens>
			</definition>
			<definition id="2">
				<sentence>Figure 2 : Translationofchooser nodes An important point to note here is the strict separation of 'syntactic ' and sclnantic information that is enforced .</sentence>
				<definiendum id="0">Translationofchooser</definiendum>
				<definiens id="0">nodes An important point to note here is the strict separation of 'syntactic ' and sclnantic information that is enforced</definiens>
			</definition>
			<definition id="3">
				<sentence>A fnrther area is a closer study of the similarities and differeuces between , e.g. , the information of the SFG and the HPSG modules -it is to be expected that there is currently duplication which could be more effectively distributed , perhaps providing a more effective TFS translation .</sentence>
				<definiendum id="0">fnrther area</definiendum>
			</definition>
			<definition id="4">
				<sentence>Further , performing the TFS translation for the entire PENMAN grammar would provide an effective test of the TFS formalism ( and its implementation ) overall since there are no comparable grammars ( i.e. , paradigmatic feature based without a phrase struc-</sentence>
				<definiendum id="0">comparable grammars</definiendum>
			</definition>
</paper>

		<paper id="3153">
			<definition id="0">
				<sentence>1992 Given a text , its FSA representation ( e.g. figure 1 ) AI is defined by the 5-tuple ( Alph , QL , il , Fl , dl ) which respectively denotes its alphabet , its state set , its starting state , its final state set and its transition function 4 which maps ( Ql*Alph ) into Q1 .</sentence>
				<definiendum id="0">AI</definiendum>
				<definiens id="0">the 5-tuple ( Alph , QL , il , Fl , dl ) which respectively denotes its alphabet</definiens>
			</definition>
			<definition id="1">
				<sentence>These automata define respectively the regular languages LI=L ( AI ) ( i.e. the language accepted by A1 ) and L2=L ( A2 ) ( i.e. the language accepted by A2 ) • Since L2 describes the set of sequences ( or factors ) forbidden in any word of L1 , if A describes the text after the filtering , this means that L=L ( A ) follows the condition L = L1 \Alph* L2 Alph* This operation on languages will be called factor subtraction and will be noted L=L1 fL2 .</sentence>
				<definiendum id="0">L=L</definiendum>
				<definiens id="0">the language accepted by A1 ) and L2=L ( A2 ) ( i.e. the language accepted by A2 ) • Since L2 describes the set of sequences ( or factors ) forbidden in any word of L1</definiens>
			</definition>
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>Form for Definite Clause Grammars As usually defined ( see \ [ 6\ ] ) , a definite clause grammar consists in two separate sets of clauses : a ( Tl ... .. 7h ) -- , a , where n is a nonterminal , T~ ... .. T. am terms ( variable .</sentence>
				<definiendum id="0">] )</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">consists in two separate sets of clauses : a ( Tl ... .. 7h ) -- , a</definiens>
			</definition>
			<definition id="1">
				<sentence>a side-effect of the GGNF is to provide a decision procedure for the problem of knowing whether a DCG is offline-parsable or not .</sentence>
				<definiendum id="0">GGNF</definiendum>
			</definition>
			<definition id="2">
				<sentence>These clauses am written : p ( 7~ ... .. 7 ) ) : f~ , where fl is some sequence of predicate goals .</sentence>
				<definiendum id="0">fl</definiendum>
				<definiens id="0">some sequence of predicate goals</definiens>
			</definition>
			<definition id="3">
				<sentence>of rules , called the factorization group ( defining the elements of A ) , the unit group ( defining the elements of U ) , and the co-unit group ( defining the elements of N ) , graphically presented in the following mannet : factorization rules : definition of the elements of A unit rules : definition of the elements of U co-unit rules : definition of the elements of N ing rules : al ( Xl ... .. X. ) -- , nl ( X1 ... .. X , ~ ) , and/or : al ( Xl , ... , Xn ) `` ~ ul ( Xl ... .. Xn ) , where nl E N and ul E U , and where n E N is the arity of the initial nonterminal al. u ( Xl ... .. X , , ) -- * tl , where u E U is a unit nonterminal of arity m , m E N , and where H is a finite sequence of nonterminal unit goals of U , of auxiliary predicates of Q , or is the empty string \ [ \ ] .</sentence>
				<definiendum id="0">n E N</definiendum>
				<definiendum id="1">H</definiendum>
				<definiens id="0">called the factorization group ( defining the elements of A ) , the unit group ( defining the elements of U ) , and the co-unit group ( defining the elements of N ) , graphically presented in the following mannet : factorization rules : definition of the elements of A unit rules : definition of the elements of U co-unit rules : definition of the elements of N ing rules : al ( Xl ... .. X. ) -- , nl ( X1 ... .. X</definiens>
				<definiens id="1">the arity of the initial nonterminal al. u ( Xl ... .. X , , ) -- * tl , where u E U is a unit nonterminal of arity m , m E N , and where</definiens>
				<definiens id="2">a finite sequence of nonterminal unit goals of U , of auxiliary predicates of Q , or is the empty string \ [ \ ]</definiens>
			</definition>
			<definition id="4">
				<sentence>n ( Xt , . . . , X~ ) -- + \ [ term\ ] Af , where n E N is a co-unit nonterminal of arity k , k E N. where \ [ term\ ] E V and where .</sentence>
				<definiendum id="0">n E N</definiendum>
				<definiens id="0">a co-unit nonterminal of arity k , k E N. where \ [ term\ ] E V and where</definiens>
			</definition>
			<definition id="5">
				<sentence>Example 3 Consider the definite grammar scheme DGS1 given in Example 1 , repeated below : al ( X ) -~ a3 ( E ) , al ( A ) , a2 ( B ) , { q ( E , A , B , X ) } ul ( X ) ~ \ [ oh\ ] , { pl ( X ) } a2 ( X ) -- , \ [ dull , { p2 ( X ) } ~3 ( x ) - , \ [ j , { p3 ( x ) ) The following definite grammar scheme DGS ; u is in GGNF and is equivalent to DGSI : al ( X ) -- ~ nl ( X ) ¢ nl ( X ) -- * \ [ oh\ ] , { pl ( X ) } nl ( X ) -- * \ [ dill , { pl ( Y ) } , h ( Y , X ) h ( Y , X ) ~ \ [ dull , { p2 ( B ) } , { p3 ( E ) } , { q ( E , Y , I3 , X ) } h ( Y , X ) -- , \ [ oui\ ] , { p2 ( B ) } , { p3 ( E ) } , { q ( E , 1I , B , Z ) } , h ( Z , X ) Suppose P is any auxiliary definite program which defines the auxiliary predicates p l , p2 , p3 , q. Then the definite clause grammars DCGt and DCG2 , obtained by adjunction of this program to DGSt and DGS2 , respectively , are equivalent .</sentence>
				<definiendum id="0">u</definiendum>
				<definiens id="0">al ( X ) -- ~ nl ( X ) ¢ nl ( X ) -- * \ [ oh\ ] , { pl ( X ) } nl ( X ) -- * \ [ dill , { pl ( Y ) }</definiens>
				<definiens id="1">B ) } , { p3 ( E ) }</definiens>
				<definiens id="2">B ) } , { p3 ( E ) } , { q ( E , 1I , B , Z ) } , h ( Z , X ) Suppose P is any auxiliary definite program which defines the auxiliary predicates p l</definiens>
			</definition>
			<definition id="6">
				<sentence>16 • If the DCG ( or equivalently its GGNF ) is ofllineparsable then top-down parsing with the GGNF finds all solutions to the parsing problem and terminates .</sentence>
				<definiendum id="0">DCG</definiendum>
				<definiens id="0">ofllineparsable then top-down parsing with the GGNF finds all solutions to the parsing problem and terminates</definiens>
			</definition>
			<definition id="7">
				<sentence>Using such concepts , a context4ree grammar such as : al ~ ala2 I a2 I \ [ \ ] a~ -- , \ [ v\ ] is refommlated into the algebraic system : al = ala2+a2+ l a s = \ [ v \ ] which represents a fixpoint equation in the variables ( or `` nontermitmls '' ) al , a2 on a certain algebraic structure ( a non-commutative semiring ) of formal power series Noo &lt; &lt; V'~ , where Net is the set of non-negative integers , extended to infinity , hfformally , an element of N , ,~ &lt; &lt; V ' &gt; &gt; represents a language on the vocabulary V ( such that \ [ v\ ] E V ) , where each string in the language is associated with a number , finite or infinite , which can be interpreted as the degree of ambiguity of this string relative to the system ( or , equivalently the corresponding CFG ) .</sentence>
				<definiendum id="0">Net</definiendum>
				<definiens id="0">] is refommlated into the algebraic system : al = ala2+a2+ l a s = \ [ v \ ] which represents a fixpoint equation in the variables ( or `` nontermitmls ''</definiens>
				<definiens id="1">a non-commutative semiring ) of formal power series Noo &lt; &lt; V'~ , where</definiens>
			</definition>
</paper>

		<paper id="4204">
			<definition id="0">
				<sentence>This Medical Language Processor ( MLP ) is an extension of the Linguistic String Project ( LSP ) ( Sager 1981 ) , which aimed at analyzing technical and specialized language by means of String Grammar ( Sager et al 1987 ) .</sentence>
				<definiendum id="0">Medical Language Processor ( MLP )</definiendum>
			</definition>
			<definition id="1">
				<sentence>Restriction Grammar ( RG ) is the Prolog-version of String Grammar , which emanated during the 60s as the grammar formalism of the distributionalism school promoted by Harris ( 1962 ) .</sentence>
				<definiendum id="0">Restriction Grammar ( RG )</definiendum>
				<definiens id="0">the Prolog-version of String Grammar , which emanated during the</definiens>
			</definition>
			<definition id="2">
				<sentence>A grammar consists in both formalisms of a set of context-free production rules interspersed with Prolog predicates that function as restrictions .</sentence>
				<definiendum id="0">grammar</definiendum>
				<definiens id="0">consists in both formalisms of a set of context-free production rules interspersed with Prolog predicates that function as restrictions</definiens>
			</definition>
			<definition id="3">
				<sentence>The rule states that a sentence adjunct can consist of prepositional strings ( indicating a date or moment in time ) , an expression of time ( nstgt ) , a subordinate sentence-structure ( csstg ) , or an adverbial suing ( dstg ) .</sentence>
				<definiendum id="0">subordinate sentence-structure</definiendum>
				<definiendum id="1">adverbial suing</definiendum>
				<definiens id="0">indicating a date or moment in time ) , an expression of time ( nstgt ) , a</definiens>
			</definition>
			<definition id="4">
				<sentence>The abstraction function can be defined as follows : F : Node - &gt; \ [ TreeTerm , Path\ ] : TreeTerm = tt ( Label , Child , Sibling , Word ) , Word = \ [ Item , lnfo\ ] .</sentence>
				<definiendum id="0">abstraction function</definiendum>
				<definiens id="0">F : Node - &gt; \ [ TreeTerm , Path\ ] : TreeTerm = tt ( Label , Child , Sibling</definiens>
			</definition>
			<definition id="5">
				<sentence>The representation invariant is : VkE { Nodes } : Item is a Dutch word or punctuation sign , Info is one of the grammatical categories , Item = \ [ \ ] &lt; = &gt; Label = *nulln , Word is instantiated &lt; = &gt; Label = ( literal v terminal ) , Label is always ground .</sentence>
				<definiendum id="0">representation invariant</definiendum>
				<definiendum id="1">Info</definiendum>
				<definiens id="0">a Dutch word or punctuation sign ,</definiens>
				<definiens id="1">one of the grammatical categories , Item = \ [ \ ] &lt; = &gt; Label = *nulln , Word is instantiated &lt; = &gt; Label = ( literal v terminal )</definiens>
			</definition>
			<definition id="6">
				<sentence>The backbone consists of a kind of sparse matrix , of which each element is a stack of grammatical labels .</sentence>
				<definiendum id="0">backbone</definiendum>
			</definition>
</paper>

		<paper id="4185">
			<definition id="0">
				<sentence>The model M = ( G , V , L ) where G is a graph , V a valuation and L a subset of the switches occurring in the formula .</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">a graph</definiens>
				<definiens id="1">a valuation and L a subset of the switches occurring in the formula</definiens>
			</definition>
			<definition id="1">
				<sentence>• M sat ( i ) { T , v ) for all v • M sat ( i ) ( t , v ) for no v • M sat ( i ) ( a , v ) iff node i in G is the leaf a E A • M sat ( i ) ( x , v ) iff V ( x ) =i and M sat ( i ) ( v ( x ) , v ) • M sat ( i ) ( \ [ fl : Sl ... .. fn : Sn\ ] , v ) iff for all k = 1 ... n ~ ( if~z ) =jk and M sat ( jr : ) ( s k , v ) • M sat ( i ) ( { ol : s I ... .. On : Sn } , v ) iffprecisely one of o t ... o n is in L and M sat ( i ) ( s k , v ) for k such that OkE L These rules correspond to the usual sansfaction definitions for feature structures .</sentence>
				<definiendum id="0">iff V</definiendum>
				<definiendum id="1">M sat</definiendum>
				<definiens id="0">the leaf a E A • M sat ( i ) ( x , v )</definiens>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>If we only used search-based approaches ( those that merely used heuristic and algorithmic methods without much linguistic knowledge ) , the performance was limited .</sentence>
				<definiendum id="0">search-based approaches</definiendum>
				<definiens id="0">those that merely used heuristic and algorithmic methods without much linguistic knowledge</definiens>
			</definition>
			<definition id="1">
				<sentence>G-UNIMEM , a modified version of UNIMEM , is an incremental learning system that uses GBM ( Geueralized-based Memory ) to generalize concepts from a large set of training instances .</sentence>
				<definiendum id="0">G-UNIMEM</definiendum>
				<definiens id="0">an incremental learning system that uses GBM ( Geueralized-based Memory ) to generalize concepts from a large set of training instances</definiens>
			</definition>
			<definition id="2">
				<sentence>In this case , G-UNIMEM retains the possible causal accounts without committing to erroneous conclusion .</sentence>
				<definiendum id="0">G-UNIMEM</definiendum>
				<definiens id="0">retains the possible causal accounts without committing to erroneous conclusion</definiens>
			</definition>
			<definition id="3">
				<sentence>The classifier is the first module that processes all training instances for G-UNIMEM .</sentence>
				<definiendum id="0">classifier</definiendum>
				<definiens id="0">the first module that processes all training instances for G-UNIMEM</definiens>
			</definition>
			<definition id="4">
				<sentence>4 A new GBM after inserting a new instance \ [ ( g , type ( pronoun ) ) , ( g , ante ( theme ) ) \ ] 1 : \ [ ( g , type ( nil ) ) , ( g , ante ( theme ) ) \ ] : \ [ ( c , :fl ( gheme ) ) , ( c , anaphor ( theme ) ) \ ] 2 : \ [ ( g , type ( nil ) ) , ( g , aute ( agent ) ) \ ] : \ [ ( c , :fl ( agent ) ) , ( c , anaphor ( theme ) ) \ ] 3 : \ [ ( g , type ( nil ) ) , ( g , ante ( agen¢ ) ) J : \ [ ( c , fl ( agent ) ) , ( c , :f2 ( theme ) ) , ( ¢ , a.aaphor ( agent ) ) '1 4 : \ [ ( g , type ( nil ) ) , ( g , ante ( agent ) ) \ ] : \ [ ( c , f l ( agent ) ) \ ] 5 : \ [ ( g , type ( nil ) ) \ ] : \ [ \ ] ( c , fl ( theme ) ) :9 \ ] I ( c'fl ( agent ) ) :4 16 : \ [ ( g , type ( pronotm ) ) , ( g , a.ate ( gheme ) ) \ ] : c , amaphor ( theme ) ) :9\ [ / \ [ ( c , :f 1 ( theme ) ) ( c , aaaphor ( th~e ) ) \ ] ( g , ante ( theme ) ) :9 \ [ Fig .</sentence>
				<definiendum id="0">c , anaphor</definiendum>
				<definiens id="0">agen¢ ) ) J : \ [ ( c , fl ( agent ) )</definiens>
			</definition>
			<definition id="5">
				<sentence>G , H , and K are sets of features .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">sets of features</definiens>
			</definition>
			<definition id="6">
				<sentence>J is an instance stored on a node .</sentence>
				<definiendum id="0">J</definiendum>
				<definiens id="0">an instance stored on a node</definiens>
			</definition>
			<definition id="7">
				<sentence>P~ is a variable of set .</sentence>
				<definiendum id="0">P~</definiendum>
				<definiens id="0">a variable of set</definiens>
			</definition>
			<definition id="8">
				<sentence>Let G be the set of features stores in N. Let H be the features in F that match features in G. Let K1 be the features in F that do not match features in G. Let K2 be the features in G that do not match features in H. Let H ' , KI ' and K2 ' be the sets of features after Adjust ( H , K1 , Ki , H ' , KI ' , Ki ' ) /* adjust goal and cause features for g-c-hierarchy or c-g-hierarchy */ if N is not the root node , then if H is empty set/* no features match */ then return False else if both H ' and KI ' are not empty sets then ~/* split node N */ spht N into N ' and NC where NC is a child of N ' ; N ' contains features in H ' with confidence scores and I as a instance with features KI ' ; each confidence score in tl ' is increased by 1 ; the remaining features and instances belong to NC ; return Split. )</sentence>
				<definiendum id="0">NC</definiendum>
				<definiens id="0">a child of N '</definiens>
			</definition>
			<definition id="9">
				<sentence>Machine Learning : An Artificial Intelligence Approach Vol .</sentence>
				<definiendum id="0">Machine Learning</definiendum>
				<definiens id="0">An Artificial Intelligence Approach Vol</definiens>
			</definition>
			<definition id="10">
				<sentence>Machine Learning : An Artificial \ [ nlelligence Approach Vol .</sentence>
				<definiendum id="0">Machine Learning</definiendum>
				<definiens id="0">An Artificial \ [ nlelligence Approach Vol</definiens>
			</definition>
</paper>

		<paper id="1005">
</paper>

		<paper id="3148">
			<definition id="0">
				<sentence>VAq~I'ON ( 1986 ) Grif : An Interactive System for Structured DocumentManipulation .</sentence>
				<definiendum id="0">Grif</definiendum>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>Translation rules map from f-structures to semantic structures , and these structures are then interpreted ( or translated into a formula of intensional logic ) .</sentence>
				<definiendum id="0">Translation rules</definiendum>
				<definiens id="0">map from f-structures to semantic structures</definiens>
			</definition>
			<definition id="1">
				<sentence>erash ( x ) ( j ) \ ] = \ [ 0 } crash ( j ) \ ] Sentence 9 contains a quantified noun phrase and has the meaning represented in ( 10 ) : ( 9 ) Every car crashed .</sentence>
				<definiendum id="0">erash ( x )</definiendum>
				<definiens id="0">contains a quantified noun phrase and has the meaning represented in ( 10 ) : ( 9 ) Every car crashed</definiens>
			</definition>
			<definition id="2">
				<sentence>The quant assmnption acts like an element in a Cooper store , keeping together the information associated with the quantified noun phrase , ms in the quant asstmlption is the meaning of the specifier ( here , every ) ; z is the variable introduced by the quantifier as the meaning of the quantified norm phrase ; and mp is the meaning of the PRED , which will form the first argument of the generalized quantifier every when quantifier discharge takes place .</sentence>
				<definiendum id="0">quant assmnption</definiendum>
				<definiendum id="1">quant asstmlption</definiendum>
				<definiendum id="2">z</definiendum>
				<definiendum id="3">mp</definiendum>
				<definiens id="0">the meaning of the specifier ( here , every )</definiens>
				<definiens id="1">the variable introduced by the quantifier as the meaning of the quantified norm phrase</definiens>
				<definiens id="2">the meaning of the PRED , which will form the first argument of the generalized quantifier every when quantifier discharge takes place</definiens>
			</definition>
			<definition id="3">
				<sentence>This is indicated in the second condition by the regular expression TOPIC OF* ; this expression involves functional uncertainty ( Kaplan and Maxwell , 1988 ) and requires that the relative pronoun relp must appear at the end of a path that is a member of the language described by the regular exprestmn .</sentence>
				<definiendum id="0">functional uncertainty</definiendum>
				<definiens id="0">a member of the language described by the regular exprestmn</definiens>
			</definition>
			<definition id="4">
				<sentence>The result is a set of sequents : ( 24 ) apply ( f , Set , Arg ) dff { s \ [ Fun E Set A s = apply ( f , Fun , Arg ) } The function conj is defined as the conjunction of a set of sequents ; the matrices of the sequents are conjoined and the union of the assumptions is taken : ( 25 ) conj ( S ) de=/\ [ U a FA M\ ] IaP-M\ ] eS \ [ a~-M\ ] ES nEST , then , is the conjunction of the result of applying the PRED meaning and the meaning of each of the modifiers in MOPS to the variable z. Finally , a rule for the interpretation of a clause containing a transitive verb is also needed : ( 26 ) Clause with transitive verb : ?</sentence>
				<definiendum id="0">function conj</definiendum>
				<definiens id="0">a set of sequents : ( 24 ) apply ( f , Set , Arg ) dff { s \ [ Fun E Set A s = apply</definiens>
				<definiens id="1">the conjunction of a set of sequents ; the matrices of the sequents are conjoined and the union of the assumptions is taken : ( 25 ) conj ( S ) de=/\ [ U a FA M\ ] IaP-M\ ] eS \ [ a~-M\ ] ES nEST , then , is the conjunction of the result of applying the PRED meaning and the meaning of each of the modifiers in MOPS to the variable z. Finally , a rule for the interpretation of a clause containing a transitive verb is also needed</definiens>
			</definition>
			<definition id="5">
				<sentence>Lexical-Functional Grammar : A formal system for grammatical representation .</sentence>
				<definiendum id="0">Lexical-Functional Grammar</definiendum>
				<definiens id="0">A formal system for grammatical representation</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>For example , in Finnish the lexical counterpart of otin 'I took ' might be rendered as otTallln , where T , al , and I1 are an arbitrary encoding of morphological alternations that determine the allomorphs of the stem and the past tense morpheme .</sentence>
				<definiendum id="0">I1</definiendum>
				<definiens id="0">an arbitrary encoding of morphological alternations that determine the allomorphs of the stem and the past tense morpheme</definiens>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>\ [ 1\ ] Multi-action utterances like these present two particular challenges for natural language interpretation systems : determining an appropriate representation of their meaning , and defining the process by which this representation can be derived from natural language utterances .</sentence>
				<definiendum id="0">Multi-action utterances</definiendum>
				<definiens id="0">determining an appropriate representation of their meaning , and defining the process by which this representation can be derived from natural language utterances</definiens>
			</definition>
			<definition id="1">
				<sentence>They do so by defining the meanings of the by and in_order_to LF predicates in terms of independently motivated action relations .</sentence>
				<definiendum id="0">in_order_to LF</definiendum>
				<definiens id="0">predicates in terms of independently motivated action relations</definiens>
			</definition>
</paper>

		<paper id="2097">
			<definition id="0">
				<sentence>The TDMT prototype system , which translates Japanese spoken dialogs into English , has shown great promise .</sentence>
				<definiendum id="0">TDMT prototype system</definiendum>
				<definiens id="0">translates Japanese spoken dialogs into English , has shown great promise</definiens>
			</definition>
			<definition id="1">
				<sentence>A transfer module utilizes analysis knowledge ( syntactic/szmantic infonnation ) which helps to apply transfer L , ~owledge to some part of the input .</sentence>
				<definiendum id="0">transfer module</definiendum>
				<definiens id="0">utilizes analysis knowledge ( syntactic/szmantic infonnation ) which helps to apply transfer L , ~owledge to some part of the input</definiens>
			</definition>
			<definition id="2">
				<sentence>In other words , TDMT prodaces translation results by utilizing these dift'cK , ~ut kinds of knowledge cooperatively and by centering on transfer , and achieves efficient translation according to the nature of the input .</sentence>
				<definiendum id="0">TDMT</definiendum>
				<definiens id="0">prodaces translation results by utilizing these dift'cK , ~ut kinds of knowledge cooperatively and by centering on transfer , and achieves efficient translation according to the nature of the input</definiens>
			</definition>
			<definition id="3">
				<sentence>TDMT provides multi-level transfer knowledge , which correspoods to each translation strategy .</sentence>
				<definiendum id="0">TDMT</definiendum>
				<definiens id="0">provides multi-level transfer knowledge , which correspoods to each translation strategy</definiens>
			</definition>
			<definition id="4">
				<sentence>TDMT achieves efficient translation by utilizing multilevel knowledge effectively according to the nature of input .</sentence>
				<definiendum id="0">TDMT</definiendum>
				<definiens id="0">achieves efficient translation by utilizing multilevel knowledge effectively according to the nature of input</definiens>
			</definition>
			<definition id="5">
				<sentence>TDMT utilizes distance calculation to determine the most plausible target expression and structure in transfer .</sentence>
				<definiendum id="0">TDMT</definiendum>
				<definiens id="0">utilizes distance calculation to determine the most plausible target expression and structure in transfer</definiens>
			</definition>
			<definition id="6">
				<sentence>TDMT achieves efficient translation by utilizing multi-level knowledge effectively .</sentence>
				<definiendum id="0">TDMT</definiendum>
				<definiens id="0">achieves efficient translation by utilizing multi-level knowledge effectively</definiens>
			</definition>
			<definition id="7">
				<sentence>Normalization is putting together minor colloquial expressions into standard expressions It leads to robust translation and efficient knowledge storage .</sentence>
				<definiendum id="0">Normalization</definiendum>
				<definiens id="0">leads to robust translation and efficient knowledge storage</definiens>
			</definition>
			<definition id="8">
				<sentence>Such knowledge helps the application of transfer knowledge to the input sentence .</sentence>
				<definiendum id="0">Such knowledge</definiendum>
				<definiens id="0">helps the application of transfer knowledge to the input sentence</definiens>
			</definition>
			<definition id="9">
				<sentence>It is normalized to `` Wataknshi wa Suzuki desu '' , which has the omitted particle `` wa '' recovered , hy applying the following analysis knowledge : Pronoun Proper-Noun = &gt; Pronoun wa Proper-Noun ( a set of examples ) The analysis module sends the information about tile application of the analysis knowledge to the transfer module .</sentence>
				<definiendum id="0">analysis module sends</definiendum>
				<definiens id="0">the information about tile application of the analysis knowledge to the transfer module</definiens>
			</definition>
			<definition id="10">
				<sentence>The transfer module receives the information and applies the transfer knowledge to produce the English sentence `` I am Suzuki '' By examples , tbis kind of analysis knowledge cau also classify the particles to be recovered as shown below : CN Verb = &gt; CN o Verb ( ( hotem { hotel } , yoyaku-snra { reserve\ ] ) , , .</sentence>
				<definiendum id="0">transfer module</definiendum>
				<definiens id="0">receives the information and applies the transfer knowledge to produce the English sentence `` I am Suzuki '' By examples , tbis kind of analysis knowledge cau also classify the particles to be recovered as shown below : CN Verb = &gt; CN o Verb ( ( hotem { hotel }</definiens>
			</definition>
			<definition id="11">
				<sentence>EBMT uses phrase examples and will be integrated with conventional rulebased machine translation .</sentence>
				<definiendum id="0">EBMT</definiendum>
			</definition>
</paper>

		<paper id="3146">
			<definition id="0">
				<sentence>COGNITERM is a bilingual ( French/English ) TKB constructed using a generic knowledge engineering tool ( CODE ) that has been used in terminology , software engineering and database design applications .</sentence>
				<definiendum id="0">COGNITERM</definiendum>
				<definiens id="0">a bilingual ( French/English ) TKB constructed using a generic knowledge engineering tool ( CODE ) that has been used in terminology , software engineering and database design applications</definiens>
			</definition>
			<definition id="1">
				<sentence>The COGNITERM Project ( 1991-94 ) is focussing on the domain of optical storage technologies ( e.g. optical discs , drives , processes , etc. ) .</sentence>
				<definiendum id="0">COGNITERM Project</definiendum>
			</definition>
			<definition id="2">
				<sentence>The Conceptual Information category is the knowledge base component , listing conceptual characteristics and their values .</sentence>
				<definiendum id="0">Conceptual Information category</definiendum>
			</definition>
			<definition id="3">
				<sentence>Finally , navigation through COGNITERM is facilitated by CODE 's Browser , which allows the knowledge to be accessed either by names of concepts or names of their characteristics , both of which can be presented in a conceptual ( i.e. hierardocumented elsewhere : a general technical description of CODE can be found in Skuce ( in press b ) ; an analysis of the relationship between terminology and knowledge engineering can be found in Meyer 1991 and ( in press ) ; the three terminology-intensive applications are described in Skuce and Meyer 1990a/b ( term bank construction ) , Skuce ( in preparation ) ( software engineering ) , and Downs et al. 1991 ( database design ) .</sentence>
				<definiendum id="0">knowledge engineering</definiendum>
				<definiens id="0">facilitated by CODE 's Browser , which allows the knowledge to be accessed either by names of concepts or names of their characteristics</definiens>
			</definition>
			<definition id="4">
				<sentence>ACKNOWLEDGEMENTS The COGNITERM Project is supported by the Social Sciences and Humanities Research Council of Canada ( SSHRC ) and Research Services of the University of Ottawa .</sentence>
				<definiendum id="0">COGNITERM Project</definiendum>
				<definiens id="0">supported by the Social Sciences and Humanities Research Council of Canada ( SSHRC ) and Research Services of the University of Ottawa</definiens>
			</definition>
</paper>

	</volume>
