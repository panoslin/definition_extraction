<?xml version="1.0" encoding="UTF-8"?>
	<volume id="W91">

		<paper id="0219">
			<definition id="0">
				<sentence>Finally , ( c ) is the semantic analysis section , where the semantic network representing the meaning of the sentence is decomposed into a set of triplets .</sentence>
				<definiendum id="0">c )</definiendum>
				<definiens id="0">the semantic analysis section , where the semantic network representing the meaning of the sentence is decomposed into a set of triplets</definiens>
			</definition>
			<definition id="1">
				<sentence>4 ) Selecting a category from the category candidates : classifying concepts into the most appropriate category by considering from the following three points of view : a ) the names of the categories and their upper clusters , b ) the distinctive patterns of the categories , and c ) the basic concepts assigned to the categories .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">the distinctive patterns of the categories , and</definiens>
			</definition>
</paper>

		<paper id="0205">
			<definition id="0">
				<sentence>39 In their feature-based system , Katz and Fodor ( 1963 ) factored the meaning of a word into a string of features and an undigested lump called a distinguisher .</sentence>
				<definiendum id="0">feature-based system</definiendum>
				<definiens id="0">the meaning of a word into a string of features and an undigested lump called a distinguisher</definiens>
			</definition>
			<definition id="1">
				<sentence>In this definition , noun is the syntactic marker ; the features ( Animal ) , ( Male ) , and ( Young } are the semantic markers that contain the theoretically significant information ; and the phrase in brackets is the unanalyzed distinguisher .</sentence>
				<definiendum id="0">noun</definiendum>
				<definiens id="0">the semantic markers that contain the theoretically significant information ; and the phrase in brackets is the unanalyzed distinguisher</definiens>
			</definition>
			<definition id="2">
				<sentence>This 2-expression defines the intension of be as a function with two arguments : ~ ' is the intension of the phrase that follows the word be , and x is the intension of the subject of be .</sentence>
				<definiendum id="0">x</definiendum>
				<definiens id="0">the intension of the phrase that follows the word be , and</definiens>
				<definiens id="1">the intension of the subject of be</definiens>
			</definition>
			<definition id="3">
				<sentence>The template is a script of type 42 $ DEMONSTRATE with an unknown actor , object , and demands .</sentence>
				<definiendum id="0">template</definiendum>
				<definiens id="0">a script of type 42 $ DEMONSTRATE with an unknown actor , object , and demands</definiens>
			</definition>
			<definition id="4">
				<sentence>Montague is the epitome of the kind of logician that Schank has always denounced as misguided or at least irrelevant .</sentence>
				<definiendum id="0">Montague</definiendum>
			</definition>
			<definition id="5">
				<sentence>For Montague , the meaning of a sentence is a function from possible worlds to truth values ; for Schank , it is a diagram that represents human conceptualizations .</sentence>
				<definiendum id="0">meaning of a sentence</definiendum>
				<definiens id="0">a function from possible worlds to truth values ; for Schank , it is a diagram that represents human conceptualizations</definiens>
			</definition>
			<definition id="6">
				<sentence>Conceptual graphs are a complete system of logic with their own model-theoretic semantics , but there is also a formula operator ck that maps conceptual graphs into predicate calculus .</sentence>
				<definiendum id="0">Conceptual graphs</definiendum>
				<definiens id="0">a complete system of logic with their own model-theoretic semantics , but there is also a formula operator ck that maps conceptual graphs into predicate calculus</definiens>
			</definition>
			<definition id="7">
				<sentence>The predicate description ( s , p ) means that a situation s is described by a proposition p. This is one of the predicates that result from the context-creating boxes in conceptual graphs , which are necessary for representing a variety of structures in language .</sentence>
				<definiendum id="0">predicate description ( s , p )</definiendum>
				<definiens id="0">one of the predicates that result from the context-creating boxes in conceptual graphs , which are necessary for representing a variety of structures in language</definiens>
			</definition>
			<definition id="8">
				<sentence>This definition says that PENNILESS is a state x of a person y , where it is false that y has possession ( POSS ) of a penny .</sentence>
				<definiendum id="0">PENNILESS</definiendum>
				<definiendum id="1">POSS</definiendum>
				<definiens id="0">a state x of a person y , where it is false that y has possession (</definiens>
			</definition>
			<definition id="9">
				<sentence>This defines ( PAST ) as a monadic relation with a formal parameter x. It applies to a situation whose point in time ( PTIM ) is a successor to some contextually defined time .</sentence>
				<definiendum id="0">PAST )</definiendum>
				<definiendum id="1">PTIM )</definiendum>
				<definiens id="0">a monadic relation with a formal parameter x. It applies to a situation whose point in time (</definiens>
			</definition>
			<definition id="10">
				<sentence>Syntactically , White House is the subject of announce , but semantic constraints rule out the building as a possible agent of the concept ANNOUNCE .</sentence>
				<definiendum id="0">White House</definiendum>
				<definiens id="0">the subject of announce , but semantic constraints rule out the building as a possible agent of the concept ANNOUNCE</definiens>
			</definition>
</paper>

		<paper id="0111">
			<definition id="0">
				<sentence>A clause is a set of literals .</sentence>
				<definiendum id="0">clause</definiendum>
				<definiens id="0">a set of literals</definiens>
			</definition>
			<definition id="1">
				<sentence>A feature specification may be regarded as an atomic formula with a special binary predicate called a feature .</sentence>
				<definiendum id="0">feature specification</definiendum>
			</definition>
			<definition id="2">
				<sentence>A splitting domain is a part S of the network in which every link is of the form R \ [ 3 R -1 where R is the union of ( o n f ) x ( ,8 n £ ) over all the layers f of S and o and ,8 are the two endnodes of that link .</sentence>
				<definiendum id="0">splitting domain</definiendum>
			</definition>
			<definition id="3">
				<sentence>A joint is a part of a node which connects the node with a link or more .</sentence>
				<definiendum id="0">joint</definiendum>
				<definiens id="0">a part of a node which connects the node with a link or more</definiens>
			</definition>
			<definition id="4">
				<sentence>( H3 ) amounts to a generalization ( or relaxation ) of the condition on which an inference link absorbs one of its endnodes .</sentence>
				<definiendum id="0">H3 )</definiendum>
			</definition>
			<definition id="5">
				<sentence>where n is the sentence length .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
</paper>

		<paper id="0101">
</paper>

		<paper id="0109">
			<definition id="0">
				<sentence>Machine translation is an obvious application for reversible natural language systems , since both understanding and generation are important parts of the process .</sentence>
				<definiendum id="0">Machine translation</definiendum>
				<definiens id="0">an obvious application for reversible natural language systems</definiens>
			</definition>
			<definition id="1">
				<sentence>Reversible MT systems , just like the broader class of MT systems as a whole , fall into two roughly defined families : transfer systems and interlingua ( or pivot systems ) .</sentence>
				<definiendum id="0">Reversible MT systems</definiendum>
				<definiens id="0">a whole , fall into two roughly defined families : transfer systems and interlingua ( or pivot systems</definiens>
			</definition>
</paper>

		<paper id="0223">
			<definition id="0">
				<sentence>The lexical semantic theory of naive semantics , which identifies word meanings with naive theories , and its use in computational text interpretation , demonstrate that a shallow , constrained layer of knowledge which is linguistic can be identified .</sentence>
				<definiendum id="0">naive semantics</definiendum>
				<definiens id="0">identifies word meanings with naive theories</definiens>
			</definition>
			<definition id="1">
				<sentence>Interpretext contains algorithms for structural and word sense disambiguation which use naive semantics .</sentence>
				<definiendum id="0">Interpretext</definiendum>
				<definiens id="0">contains algorithms for structural and word sense disambiguation which use naive semantics</definiens>
			</definition>
</paper>

		<paper id="0110">
			<definition id="0">
				<sentence>The right-hand side is an expression of conjunctions and disjunctions of typed feature terms ( Figure 1 ) .</sentence>
				<definiendum id="0">right-hand side</definiendum>
			</definition>
			<definition id="1">
				<sentence>A rewrite step for a term t is defined as follows : if u is a subterm of t of type A and there exists a rewrite rule A\ [ a\ ] ~ B\ [ b\ ] such that A\ [ a\ ] N u ~ _l_ , the right-hand side B\ [ b\ ] is unified with the subterm u , giving a new term t ' which is more specific than t. This rewrite step is applied nondeterministically everywhere in the term until no further rule is applicable 5 .</sentence>
				<definiendum id="0">rewrite step</definiendum>
				<definiendum id="1">~ _l_</definiendum>
				<definiens id="0">if u is a subterm of t of type A and there exists a rewrite rule A\ [ a\ ] ~ B\ [ b\ ] such that A\ [ a\ ] N u</definiens>
			</definition>
</paper>

		<paper id="0115">
			<definition id="0">
				<sentence>A plan consists of precon ditions , constraints , plan expansion ( usually termed the body ) , and effects .</sentence>
				<definiendum id="0">plan</definiendum>
			</definition>
			<definition id="1">
				<sentence>The word dependency indicates the states where variables are shared by constraints 3 .</sentence>
				<definiendum id="0">word dependency</definiendum>
			</definition>
			<definition id="2">
				<sentence>Syn stands for syntactic information like the part of speech and subcategorization information using HPSG .</sentence>
				<definiendum id="0">Syn</definiendum>
				<definiens id="0">syntactic information like the part of speech and subcategorization information using HPSG</definiens>
			</definition>
			<definition id="3">
				<sentence>HPSG inherits the fundamental properties of Generalized : Phrase Structure Grammar ( GPSG ) .</sentence>
				<definiendum id="0">HPSG</definiendum>
			</definition>
			<definition id="4">
				<sentence>Sem shows that : 1 ) the quantifier at the top level is indefinite ; 2 ) the property of the variable X is a boy ; 3 ) the property of the variable Y bounded by the quantifier definite is a girl ; 4 ) the boy loved the girl .</sentence>
				<definiendum id="0">Sem</definiendum>
				<definiendum id="1">X</definiendum>
				<definiendum id="2">definite</definiendum>
				<definiens id="0">a boy ; 3 ) the property of the variable Y bounded by the quantifier</definiens>
			</definition>
			<definition id="5">
				<sentence>The system has a language containing the following operators : t ( P ) which indicates the truth of the proposition P ; beI ( A , PF ) which mentions that agent A believes plan fragment PF ; int ( A , PF ) which shows that agent A intends plan fragment PF ; exp ( A , P ) which means that agent A expects proposition P to be true ; and by ( Actexpl , Actexp2 , Pexp ) which signifies the complex plan fragment which consists of the action expression Actexp2 , by doing action expression Actexpl while propositional expression Pexp is true 5 .</sentence>
				<definiendum id="0">int ( A , PF )</definiendum>
				<definiendum id="1">P )</definiendum>
				<definiens id="0">a language containing the following operators : t ( P ) which indicates the truth of the proposition P ; beI ( A , PF ) which mentions that agent A believes plan fragment PF</definiens>
				<definiens id="1">signifies the complex plan fragment which consists of the action expression Actexp2 , by doing action expression</definiens>
			</definition>
			<definition id="6">
				<sentence>Thus , the new operator by ( Actexp 1 , Actexp2 , Pexp 1 , Pexp2 ) is defined as the complex plan fragment , consisting of doing Actexp2 ( effect ( s ) ) , by doing Actexpl ( plan expansion ) while Pexpl ( constraints ) is true , and Pexp2 ( precondition ( s ) ) is true or held as assumptions .</sentence>
				<definiendum id="0">Pexp2 ( precondition ( s ) )</definiendum>
				<definiens id="0">true or held as assumptions</definiens>
			</definition>
			<definition id="7">
				<sentence>Argument ( b ) : beI ( S , by ( PE , E , C , PR ) ) , int ( S , PE ) , exp ( S , C ) , exp 1 ( S , PR ) by2 ' ... .. &gt; int ( S , by ( PE , E , C , PR ) ) , int ( S , E ) .</sentence>
				<definiendum id="0">Argument</definiendum>
			</definition>
			<definition id="8">
				<sentence>123 Since syntactic , semantic and pragmatic information can be represented with the new by relation , arguments ( a ) and ( b ) enable us to simulate parsing : ( a ) says that true propositions can be ascribed to a speaker 's belief ; ( b ) states that , if a speaker is assumed to believe that E is an effect of performing plan expansion PE , while constraint C is true and precondition PR is assumed to be true H , then it is plausible that his reason for doing PE is his intention to do E. Parsing is executed as follows : first , axioms whose constraints match an input word are collected ; second , the axiom which satisfies the constraint is selected ( preconditions are asserted ) ; third , an effect , or semantic information is derived using an instance of argument ( b ) ; fourth , another instance of the argument is applied to the effect and the effect which was already derived to obtain a new effect .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">an effect of performing plan expansion PE , while constraint C is true and precondition PR is assumed to be true H , then it is plausible that his reason for doing PE is his intention to do E. Parsing is executed as follows : first , axioms whose constraints match an input word are collected ; second , the axiom which satisfies the constraint is selected ( preconditions are asserted</definiens>
			</definition>
			<definition id="9">
				<sentence>l ( 13 ) I Sign : phn : Expl2 syn : Synl sem : rest : arg0 : Y prag : \ [ \ ] Sign : phn : '' girl '' syn : pos : noun sem : rest : arg0 : Y prag : \ [ \ ] ( 5 ) Sign : phn : Exp211 syn : Syn211 sem : qnt : indefinite prag : \ [ \ ] ( 3 ) Sign : phn : Exp2 syn : pos : verb subcat : subc ( \ ] ) slash : sl ( \ [ Obj\ ] ) sem : qnt : indefinite var : X rest : arg0 : X pred : BOY body : arg0 : X argl : Y pred : LOVED i ( 10~ Sign : phn : '' a '' il syn : pos : det | sem : qnt : indefinite :1 var : X ii prag : \ [ \ ] prag : \ [ \ ] pred : GIRL pred : GIRL Sign : phn : Exp21 ( 4 ) syn : Syn21 sem : qnt : indefinite var : X rest : arg0 : X pred : BOY prag : \ [ \ ] ( 6 ) , ... - .</sentence>
				<definiendum id="0">Synl sem</definiendum>
				<definiens id="0">indefinite var : X rest : arg0 : X pred : BOY body : arg0</definiens>
			</definition>
			<definition id="10">
				<sentence>`` Dependency Propagation : A Unified Theory of Sentence Comprehension and Generation '' .</sentence>
				<definiendum id="0">Dependency Propagation</definiendum>
				<definiens id="0">A Unified Theory of Sentence Comprehension and Generation ''</definiens>
			</definition>
</paper>

		<paper id="0214">
			<definition id="0">
				<sentence>The one-level theory , which is the predominant one in cognitive and computational linguistics , considers semantic representations to be part of the conceptual system .</sentence>
				<definiendum id="0">one-level theory</definiendum>
				<definiens id="0">the predominant one in cognitive and computational linguistics , considers semantic representations to be part of the conceptual system</definiens>
			</definition>
			<definition id="1">
				<sentence>Bierwisch ( in \ [ 3\ ] ) proposes a general format for semantic representations of gradable adjectives within the two-level theory which meets these demands .</sentence>
				<definiendum id="0">Bierwisch</definiendum>
				<definiendum id="1">] )</definiendum>
				<definiens id="0">proposes a general format for semantic representations of gradable adjectives within the two-level theory which meets these demands</definiens>
			</definition>
			<definition id="2">
				<sentence>b ) at all , it is a habitual interpretation ( which of course is a special kind of a state interpretation ) : Peter took the newspaper after he quit his habit of regularly sitting down in a particular chair of his whatever this is supposed to tell us .</sentence>
				<definiendum id="0">habitual interpretation</definiendum>
			</definition>
</paper>

		<paper id="0102">
			<definition id="0">
				<sentence>The next constraaint imposes the presence of an attribute in a type : has ( Attribute , Type ) where Attribute is either an attribute label or a full pair attribute-value and Type is a reference to a given type .</sentence>
				<definiendum id="0">Attribute</definiendum>
				<definiendum id="1">Type</definiendum>
				<definiens id="0">a reference to a given type</definiens>
			</definition>
			<definition id="1">
				<sentence>Very briefly , the co-occurence of two or more subtypes in a larger type is expressed by the constraint : ponding ( A , B ) where A is a type specification and B is a list of type specifications .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">the co-occurence of two or more subtypes in a larger type is expressed by the constraint : ponding ( A , B ) where A is a type specification</definiens>
				<definiens id="1">a list of type specifications</definiens>
			</definition>
			<definition id="2">
				<sentence>Here is the description of the lexical entry corresponding to the verb to give : xO ( cat = &gt; v , string = &gt; \ [ give\ ] ) : pending ( xO ( cat = &gt; v ) , \ [ xp ( cat = &gt; n , string = &gt; $ 1 , role = &gt; patient ) , xp ( cat = &gt; p , string = &gt; $ 2 , role = &gt; recipient ) \ ] ) , precede ( \ [ give\ ] , S1 ) , precede ( S1 , $ 2 ) .</sentence>
				<definiendum id="0">xp</definiendum>
				<definiens id="0">cat = &gt; v , string = &gt; \ [ give\ ] ) : pending ( xO ( cat = &gt; v</definiens>
			</definition>
			<definition id="3">
				<sentence>The three above le~ , els become : ( 1 ) current state °i represented by the couple : &lt; c0 ( a l= &gt; t 1 , a 2= &gt; t 2 ... .. a n= &gt; tn ) , S &gt; where S is the set of current constraints , ( 2 ) selection in the current programme P of a type construction specification : cl ( b 1 = &gt; t'l , ... ; b m = &gt; t ' m ) : R. where R is the set of constraints associated to cl , and t 1 subsumes Or unifies with t ' 1 modulo the mgu 0 i. ( 3 ) New state o i+ 1 characterized by the following couple : &lt; c0 ( a 1 = &gt; cl ( b 1 = &gt; t ' 1 ... .. b m = &gt; t ' m ) , a 2 = &gt; t 2 , ... . a n = &gt; t n ) 0 i , S u R u subsume ( tl , Cl ( b 1 = &gt; t ' 1 ... .. bm = &gt; i t 'm ) ) &gt; with the condition that the new set of constraints must be satisfiable with respect to the constraint resolution axioms defined for each type of constraint and , if not , :a backtracking occurs .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">the set of constraints associated to cl , and t 1 subsumes Or unifies with t ' 1 modulo the mgu</definiens>
				<definiens id="1">characterized by the following couple : &lt; c0 ( a 1 = &gt; cl ( b 1 = &gt; t ' 1 ... .. b m = &gt; t ' m ) , a 2 = &gt; t 2 , ... . a n = &gt; t n ) 0 i , S u R u subsume</definiens>
			</definition>
			<definition id="4">
				<sentence>( a ) a set of subtypes from which a more general type can be constructed : o i = ( a ) ( C 1 , C 2 ... .. C n ) is an unordered sequence of subtypes ; or ( b ) it is a single type : o i = D1 ( a ) let DC be such that : DC has exactly k attributes constj , k &lt; _ n , and DC is of the form : DC : = xp ( ... . const I = &gt; C ' 1 ... .. const k = &gt; C ' k ) and : for all j E \ [ l , k\ ] , subsume ( C'i , Ci ) ( notice that the Cj are not n~cess~rily the jtn element of the list given in 1 above , notice also that the type constructor DC contains the subtypes constq together with other information like category and morphology . )</sentence>
				<definiendum id="0">DC</definiendum>
				<definiens id="0">of the form : DC : = xp ( ...</definiens>
			</definition>
			<definition id="5">
				<sentence>We then have : ( a ) &lt; ( C 1 , C 2 ... .. Cn ) , S &gt; sequence of subtypes C i and of active constraaints S ( b ) &lt; D1 , S &gt; L ( a ) &lt; DC , R &gt; where R is the set of constraints associated to DC and such that : i ) same restrictions as above on DC and ii ) R is consistent with S ( b ) &lt; D ' , R &gt; ( single type ) with R consistent with S. ( a ) &lt; ( DC , Ck+ 1 ... .. Cn ) , ( S uRu ( subsume ( C'j = &gt; Cj ) ) !</sentence>
				<definiendum id="0">S uRu ( subsume</definiendum>
				<definiens id="0">&gt; sequence of subtypes C i and of active constraaints S ( b ) &lt; D1 , S &gt; L ( a ) &lt; DC , R &gt; where R is the set of constraints associated to DC and such that : i ) same restrictions as above on DC and ii ) R is consistent with S ( b ) &lt; D ' , R &gt; ( single type ) with R consistent with S. ( a ) &lt; ( DC , Ck+ 1 ... .. Cn</definiens>
			</definition>
			<definition id="6">
				<sentence>Secondary Predication is a term used in the literature to denote a very productive structural relationship in many languages : the relationship between a subject and a predicate , the subject being assigned a thematic role by that predicate and by an obligatory thematic roie assigner in the sentence , namely the verb .</sentence>
				<definiendum id="0">Secondary Predication</definiendum>
				<definiens id="0">a term used in the literature to denote a very productive structural relationship in many languages : the relationship between a subject and a predicate , the subject being assigned a thematic role by that predicate and by an obligatory thematic roie assigner in the sentence , namely the verb</definiens>
			</definition>
			<definition id="7">
				<sentence>We have the following construction : xp ( cat = &gt; v , string = &gt; SV , constl = &gt; xl ( cat = &gt; v , string = &gt; $ 3 ) , b const2 = &gt; xp ( cat : = &gt; a , string = &gt; $ 4 , index = &gt; I ) : pending ( xp ( cat = &gt; v ) , \ [ xl ( cat = &gt; v , string = &gt; T , constl = &gt; xO ( cat = &gt; v , string = &gt; $ 1 ) , const2 = &gt; xp ( cat = &gt; n , string = &gt; $ 2 , index = &gt; I ) ) \ ] ) , precede ( S1 , $ 2 ) , precede ( S3 , S4 ) .</sentence>
				<definiendum id="0">pending</definiendum>
				<definiendum id="1">xO</definiendum>
				<definiens id="0">cat = &gt; v ) , \ [ xl ( cat = &gt; v , string = &gt; T , constl = &gt;</definiens>
			</definition>
			<definition id="8">
				<sentence>The type construction is the following : xl ( cat = &gt; v , string = &gt; SV , constl = &gt; xO ( cat = &gt; v , string = &gt; $ 1 ) , const2 = &gt; xp ( cat = &gt; sc , string = &gt; $ 2 ) ) : pending ( x1 ( cat = &gt; v ) , \ [ xl ( cat = &gt; sc , string = &gt; SV , constl = &gt; xp ( cat = &gt; n , index = &gt; I , string = &gt; $ 3 ) , const2 = &gt; xp ( cat = &gt; a , string = &gt; S4 , index = &gt; I ) ) \ ] ) , precede ( S1 , S2 ) , precede ( S3 , S4 ) .</sentence>
				<definiendum id="0">type construction</definiendum>
				<definiens id="0">pending ( x1 ( cat = &gt; v ) , \ [ xl ( cat = &gt; sc , string = &gt; SV , constl = &gt; xp ( cat = &gt; n , index = &gt; I , string = &gt; $ 3</definiens>
			</definition>
			<definition id="9">
				<sentence>Construction ( 10c ) introduces a double indexation but no long-distance dependency for the compound predicate 'rend-blanc ' .</sentence>
				<definiendum id="0">Construction ( 10c )</definiendum>
				<definiens id="0">introduces a double indexation but no long-distance dependency for the compound predicate 'rend-blanc '</definiens>
			</definition>
</paper>

		<paper id="0222">
			<definition id="0">
				<sentence>This paper proposes a two-level model that integrates tense and aspect information , based on theories by both Hornstein ( in the spirit of Reichenbach ) and Allen , with lexicai-semantic information based on an extended version of $ ackendoff 's theory that includes a verb classification system proposed by Dowty and Vendler .</sentence>
				<definiendum id="0">Hornstein</definiendum>
				<definiens id="0">lexicai-semantic information based on an extended version of $ ackendoff 's theory that includes a verb classification system proposed by Dowty and Vendler</definiens>
			</definition>
			<definition id="1">
				<sentence>2It is assumed that the reader is familiar with the Reichenbachian framework , which postulates three theoretical entities : S ( the moment of speech ) , R ( a reference point ) , and E ( the moment of the event ) .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">postulates three theoretical entities : S ( the moment of speech )</definiens>
				<definiens id="1">the moment of the event</definiens>
			</definition>
			<definition id="2">
				<sentence>In particular , \ [ Dowty , 1979\ ] and \ [ Vendler , 1967\ ] have proposed an aspectually oriented lexical-semantic structure that provides a four-way classification system for verbs : states , activities , achievements , and accomplishments , each of which has a different degree of telicity ( i. e. , culminated vs. nonculminated ) , and/or atomicity ( i.e. , point vs. extended ) .</sentence>
				<definiendum id="0">, and/or atomicity</definiendum>
				<definiens id="0">provides a four-way classification system for verbs : states , activities , achievements , and accomplishments , each of which has a different degree of telicity ( i. e. , culminated vs. nonculminated )</definiens>
			</definition>
			<definition id="3">
				<sentence>255 Dototy ; Vendler ; Bennett ; Mourelatos ; Jackendoff Examples Passonneau Moens ~ Comrie ; Bach ; \ ] Steedman Pustejovsky State I-d\ ] State State ( BE ) be , like , know Activity ( point ) \ [ +d , -t , +a\ ] Process Event ( GO , STAY ) tap , wink Activity ( extended ) Achievement Accomplishment \ [ +d , -t , -a\ ] \ [ +d , +t , +a\ ] t+d , +t , -a\ ] Process Event Event Event ( GO , STAY ) Event ( GO , STAY ) Event ( GO , STAY ) swim , run obliterate , kill destroy , give Figure 2 : Proposals for Lexical-Semantic Frameworks that Accommodate Tense/Aspect \ [ Bach , 1986\ ] and \ [ Pustejovksy , 1989\ ] ( following \ [ Mourelatos , 1981\ ] and \ [ Comrie , 1976\ ] ) in which actions are classified into states , processes , and events .</sentence>
				<definiendum id="0">STAY ) Event</definiendum>
				<definiens id="0">1976\ ] ) in which actions are classified into states , processes , and events</definiens>
			</definition>
			<definition id="4">
				<sentence>For example , if we determine from a knowledge source that event E1 John went to the store and event E2 Mary arrived have both occurred in the past , then the time of the sit is assumed that the reader is familiar with Allen 's 13 notational relations : &gt; ( after ) , &lt; ( before ) , = ( equal ) , m ( meets ) , mi ( is met by ) , o ( overlaps ) , oi ( is overlapped by ) , d ( during ) , di ( contains ) , s ( starts ) , si ( is started by ) , f ( finishes ) , and fi ( is finished by ) .</sentence>
				<definiendum id="0">o ( overlaps</definiendum>
				<definiendum id="1">s ( starts</definiendum>
				<definiendum id="2">fi</definiendum>
				<definiens id="0">determine from a knowledge source that event E1 John went to the store</definiens>
			</definition>
</paper>

		<paper id="0218">
			<definition id="0">
				<sentence>For example , semantic class constraints consist of properties such as animate or machine-part which are associated with generic entities .</sentence>
				<definiendum id="0">semantic class constraints</definiendum>
				<definiens id="0">consist of properties such as animate or machine-part which are associated with generic entities</definiens>
			</definition>
			<definition id="1">
				<sentence>So , in Two particles are suspended from a pulley , the following instantiations occur , where strlng57 is an hypothetical string whose existence is assumed hang - : effect ( string57 , support ( \ [ particlel , particle2\ ] , pulleyl ) ) This is expanded similarly to the expansion above , resulting in : effect ( string57 , support ( \ [ particlel , particle2\ ] , pulleyl ) ) - : support ( \ [ particlel , particle2\ ] , string57 ) ) , support ( string57 , pulleyl ) ) This is intended to represent the information that the two particles are supported by the string , and the string is in turn supported by the pulley .</sentence>
				<definiendum id="0">strlng57</definiendum>
				<definiens id="0">an hypothetical string whose existence is assumed hang - : effect</definiens>
			</definition>
</paper>

		<paper id="0203">
			<definition id="0">
				<sentence>Term banks can be seen as learning tools for the terminologists themselves , for example , when they are assigned a new field in which they have little knowledge , or when they are working in a field that is highly influenced by neighboufing fields with which they are not very familiar .</sentence>
				<definiendum id="0">Term banks</definiendum>
				<definiens id="0">assigned a new field in which they have little knowledge , or when they are working in a field that is highly influenced by neighboufing fields with which they are not very familiar</definiens>
			</definition>
			<definition id="1">
				<sentence>Machine uses ( e.g. machine translation , expert systems , NL interfaces to databases ) will require very large quantities of explicitly represented conceptual information , since they do not possess much of the world knowledge that humans know implicitly .</sentence>
				<definiendum id="0">NL</definiendum>
				<definiens id="0">interfaces to databases</definiens>
			</definition>
			<definition id="2">
				<sentence>If they are attempting the classic intensional ( i.e. genus-differentia ) definition , terminologists will need to compare the characteristics of a given concept with those of concepts at the same hierarchical level ( i.e. with the characteristics of the coordinate concepts ( 12 ) ) in order to determine the distinguishing characteristics ( i.e. the differentia in an intensional definition ) .</sentence>
				<definiendum id="0">hierarchical level</definiendum>
				<definiens id="0">the characteristics of the coordinate concepts ( 12 ) ) in order to determine the distinguishing characteristics ( i.e. the differentia in an intensional definition )</definiens>
			</definition>
			<definition id="3">
				<sentence>The AI research group at the University of Ottawa , Canada , has over the past few years developed a generic knowledge engineering tool called CODE ( Conceptually Oriented Design Environment , Skuce et al. 1989 ) , which is written in Smalltalk and runs on a Macintosh , 386 or UNIX platform .</sentence>
				<definiendum id="0">CODE ( Conceptually Oriented Design Environment</definiendum>
				<definiens id="0">has over the past few years developed a generic knowledge engineering tool called</definiens>
			</definition>
			<definition id="4">
				<sentence>CODE allows the user to construct a knowledge base which describes concepts in frame-like units called CDs ( concept descriptors ) that are normally , though not necessarily , arranged in inheritance hierarchies .</sentence>
				<definiendum id="0">CODE</definiendum>
				<definiens id="0">allows the user to construct a knowledge base which describes concepts in frame-like units called CDs ( concept descriptors ) that are normally , though not necessarily , arranged in inheritance hierarchies</definiens>
			</definition>
			<definition id="5">
				<sentence>CODE has been tested in two terminology applications : a bilingual vocabulary project at the Department of the Secretary of State of Canada ( Meyer and Paradis 1991 , Skuce and Meyer 1990a/b ) and a software documentation project at Bell Northern Research , the Canadian counterpart of Bell Labs ( Skuce 1991 ) .</sentence>
				<definiendum id="0">CODE</definiendum>
			</definition>
			<definition id="6">
				<sentence>CODE permits rapid entry of hunches , guesses , trials , etc. , followed by experimentation with the consequences of entering new knowledge .</sentence>
				<definiendum id="0">CODE</definiendum>
				<definiens id="0">permits rapid entry of hunches , guesses , trials , etc. , followed by experimentation with the consequences of entering new knowledge</definiens>
			</definition>
			<definition id="7">
				<sentence>CODE offers a `` masking '' facility that allows one to restrict what is visible in the knowledge base by Boolean conditions on concepts and characteristics .</sentence>
				<definiendum id="0">CODE</definiendum>
				<definiens id="0">visible in the knowledge base by Boolean conditions on concepts and characteristics</definiens>
			</definition>
			<definition id="8">
				<sentence>In order to ensure terminological consistency , CODE offers a number of features for assisting users in naming a conceptual characteristic ( a common terminological problem in knowledge base building ) .</sentence>
				<definiendum id="0">CODE</definiendum>
				<definiens id="0">a common terminological problem in knowledge base building )</definiens>
			</definition>
			<definition id="9">
				<sentence>The COGNITERM project ( 1991-1994 ) is supported by the Social Sciences and Humanities Research Council of Canada ( SSHRC ) and Research Services of the University of Ottawa .</sentence>
				<definiendum id="0">COGNITERM project</definiendum>
				<definiens id="0">supported by the Social Sciences and Humanities Research Council of Canada ( SSHRC ) and Research Services of the University of Ottawa</definiens>
			</definition>
			<definition id="10">
				<sentence>A neologism is a term that is used in a new sense .</sentence>
				<definiendum id="0">neologism</definiendum>
				<definiens id="0">a term that is used in a new sense</definiens>
			</definition>
			<definition id="11">
				<sentence>`` Concept Analysis and Terminology : A Knowledge-Based Approach to Documentation '' .</sentence>
				<definiendum id="0">Terminology</definiendum>
				<definiens id="0">A Knowledge-Based Approach to Documentation ''</definiens>
			</definition>
</paper>

		<paper id="0211">
			<definition id="0">
				<sentence>The insights gained from our work with machine readable dictionaries ( MRDs ) the Longman Dictionary of Contemporary English , henceforth LDOCE , and the Van Dale dictionary of contemporary Dutch `` Groot Woordenboek van het Hedendaags Nederlands '' , henceforth Van Dale , lead to the conclusion that lexical knowledge in the end is perceptual ( in the form of primitive perceptual images ) and grammatical ( in the form of grammatical primitives ) .</sentence>
				<definiendum id="0">MRDs</definiendum>
			</definition>
			<definition id="1">
				<sentence>The complex property consists of an involvement of `` x 1 '' ( expressed by the eo-referentiality ) in an event designated by the main predicate `` make '' .</sentence>
				<definiendum id="0">complex property</definiendum>
				<definiens id="0">consists of an involvement of `` x 1 '' ( expressed by the eo-referentiality ) in an event designated by the main predicate `` make ''</definiens>
			</definition>
			<definition id="2">
				<sentence>For any utterance it will hold that its meaning can be explained in terms of the underlying linguistic-conceptual structure and the meaning of the content words that it contains : Meaning ( expression ) Meaning ( underlying linguistic-conceptual structure ) + Meaning ( words ) This means that expressions of expert knowledge in some natural language are subject to the same rules and restrictions imposed by the grammar on expressions of the language as a whole .</sentence>
				<definiendum id="0">Meaning</definiendum>
				<definiens id="0">the meaning of the content words that it contains : Meaning ( expression )</definiens>
			</definition>
			<definition id="3">
				<sentence>Such chains , which have been generated for LDOCE nouns and verbs and Van Dale nouns , tied together form a semantic classification of the vocabulary of a language in terms of a small set of circularily defined words .</sentence>
				<definiendum id="0">Such chains</definiendum>
			</definition>
</paper>

		<paper id="0215">
			<definition id="0">
				<sentence>W. Labov calls the semantic potential of words enabling them to constitute various links between linguistic expressions and elements of the domain ( semantic ) variability .</sentence>
				<definiendum id="0">W. Labov</definiendum>
				<definiens id="0">calls the semantic potential of words enabling them to constitute various links between linguistic expressions and elements of the domain ( semantic ) variability</definiens>
			</definition>
			<definition id="1">
				<sentence>Conceptual knowledge combines the structural information conveyed by the ontology with the additional information expressed by rules of inference .</sentence>
				<definiendum id="0">Conceptual knowledge</definiendum>
				<definiens id="0">combines the structural information conveyed by the ontology with the additional information expressed by rules of inference</definiens>
			</definition>
			<definition id="2">
				<sentence>Formally speaking , a context is a set of factors marked by a temporal index specific to a certain stage of processing at which the system is observed .</sentence>
				<definiendum id="0">context</definiendum>
				<definiens id="0">a set of factors marked by a temporal index specific to a certain stage of processing at which the system is observed</definiens>
			</definition>
			<definition id="3">
				<sentence>A segmentation of the interpretation process according to our classification of knowledge types leads to three components which by application of functional composition constitute word meaning .</sentence>
				<definiendum id="0">segmentation</definiendum>
				<definiens id="0">by application of functional composition constitute word meaning</definiens>
			</definition>
			<definition id="4">
				<sentence>Additional parameters of the discourse situation ( the time and location of the utterance as well as a proposition rl ) allow to establish an individuation which maps conc34 onto a referent r2 with the following properties : actor ( r2 ) = rspeaker ^ proposition ( r2 ) : rl ^ location ( r2 ) = r a ^ r3 C Sdiscourse ^ time ( r2 ) = r4 A r4 &lt; tdi , ¢our , e The three components of word meaning can be considered intermediate steps of the interpretation process .</sentence>
				<definiendum id="0">Additional parameters of the discourse situation</definiendum>
				<definiens id="0">the time and location of the utterance as well as a proposition rl ) allow to establish an individuation which maps conc34 onto a referent r2 with the following properties : actor ( r2 ) = rspeaker ^ proposition ( r2 ) : rl ^ location ( r2 ) = r a ^ r3 C Sdiscourse ^ time ( r2 ) = r4 A r4 &lt; tdi , ¢our</definiens>
			</definition>
</paper>

		<paper id="0112">
			<definition id="0">
				<sentence>REVERSIBLE GRAMMARS A reversible grammar is usually understood as a computational or linguistic system that can be used both for analysis ~d generation of the language it defines .</sentence>
				<definiendum id="0">REVERSIBLE GRAMMARS</definiendum>
				<definiens id="0">a computational or linguistic system</definiens>
			</definition>
			<definition id="1">
				<sentence>In the literal subject ( A1 , A2 , NUM , P ) , taken from an English grammar , AI and A2 are input and output strings of words , NUM is the number of the subject phrase , and P is the final translation .</sentence>
				<definiendum id="0">NUM</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">the number of the subject phrase , and</definiens>
				<definiens id="1">the final translation</definiens>
			</definition>
			<definition id="2">
				<sentence>Consider , for example , the problem of assigning the set of MSEA 's to mem ( Elem , List ) , where mem ( list membership ) is defined as follows : mem ( Elem , \ [ First IList \ ] ) : mere ( Elem , List ) .</sentence>
				<definiendum id="0">mem ( list membership )</definiendum>
				<definiens id="0">the problem of assigning the set of MSEA 's to mem</definiens>
			</definition>
			<definition id="3">
				<sentence>This relation can be formally characterized as follows : given two terms X and Y we say that Y is always unifiable with X ( and write X_ &lt; Y ) iff the unification of X and Y yields Y , where the variables occurring in X and Y have been standardized apart. 1° Since _ &lt; describes a partial order among terms , we can talk of its transitive closure _ &lt; *. Now we can augment the MSEAS procedure with the following two steps ( to be placed between steps ( 2 ) and ,0 So defined , the relation always uni~ble becomes an inverse of another relation : less instantiat~d , hence the particular direction of S sign. 93 ( 3 ) ) that would exclude certain MSEAs from re.cursive clauses. ( 2A ) If r i = p then for every mi , u E MRUi if for every argument Yt ~ mi. , , , where Yt is the l-th argument in ri , and Xi is the l-th argument in p , we have that Xt_ &lt; * Yi then remove mi , u from MRU i. ( 2B ) For every set mi , uj = mi. u u { Zi. j } , where Zi , j is the j-th argument in r~ such that it is not already in mi.u and it is not the case that YiS'Zid , where Yj is a j-th argument in p , if mi.ui ts not a superset of any other mi , t remaining in MRUi , then add mi , ui to MRU1. In order for the MSEAS procedure to retain its practical significance we need to restrict the closure of &lt; _ to be defined only on certain special sets of terms that we call ordered series. H It turns out that this restricted relation is entirely sufficient in the task of grammar inversion , if we assume that the original grammar is itself well-defined. DEFINITION 1 ( argument series ) Let p ( . • • Yo `` • • ) : rl , • • • , rn be a clause , and ril , `` `` • , rid be an ordered subset of the literals on the right-hand side of this clause. Let ri~ , t be either a literal to the right of rlk or the head literal p. The ordered set of terms &lt; Yo , Xi , Yl , `` ' '' , Xk , Yk , Xk+l &gt; is an argument series iff the following conditions are met : ( 1 ) Xk+~ is an argument in ri~+~ ; ( 2 ) for every i=1 `` .</sentence>
				<definiendum id="0">Yt</definiendum>
				<definiendum id="1">Xi</definiendum>
				<definiendum id="2">Yj</definiendum>
				<definiendum id="3">Xk+~</definiendum>
				<definiens id="0">the l-th argument in ri</definiens>
			</definition>
			<definition id="4">
				<sentence>-k , Xi is different from any Xj for j &lt; i ; ( 3 ) for every j=l `` .k , X i and Yi are arguments to % , that is , rlj ( ... Xi , Yj ... ) , such that if Xj is `` in '' then Yj is `` out '' 12 ; and ( 4 ) for every j=0..-k , either Xj+i=Y j or X j+ 1 =f ( Yj ) or Yj=f ( X j+l ) , where f ( X ) denotes a term containing a subterm X. Note that this definition already ensures that the argument series obtained between X0 and Xk+t is the shortest one. As an example , consider the following clauses : u A similar concept of guide-structure is introduced in ( Dymetman et al. , 1990 ) , however the ordered series is less restrictive and covers a larger class of recursive programs. 12 yj may be partially `` out '' ; see ( Strzalkowski , 1990c ) for the definition of delayed `` out '' status. vp ( X ) : np ( X , Y ) , vp ( Y ) . np ff ( x ) , x ) . Assuming that the argument X in the literal vp ( X ) on the left-hand side ( lhs ) of the first clause is `` in '' , we can easily check that &lt; X , X , Y , Y &gt; constitutes an argument series between arguments of vp in the first clause .</sentence>
				<definiendum id="0">Xk+t</definiendum>
				<definiens id="0">a term containing a subterm X. Note that this definition already ensures that the argument series obtained between X0 and</definiens>
				<definiens id="1">Dymetman et al. , 1990 ) , however the ordered series is less restrictive and covers a larger class of recursive</definiens>
				<definiens id="2">vp ( Y ) . np ff ( x ) , x ) . Assuming that the argument X in the literal vp ( X ) on the left-hand side ( lhs ) of the first clause is `` in ''</definiens>
			</definition>
			<definition id="5">
				<sentence>DEFINITION 2 ( weakly ordered series ) 13 An argument series &lt; Yo , X1 , Y1 , ... , Xk , YkX~+i &gt; in the clause P : -rl ... r , is weakly ordered iff Yo_ &lt; *Xk+l \ [ or Xk+l_ &lt; 'Y0\ ] , where _ &lt; * is a closure of &lt; _ defined as follows : ( 1 ) for every i=1 .</sentence>
				<definiendum id="0">DEFINITION 2 ( weakly</definiendum>
			</definition>
			<definition id="6">
				<sentence>We also note that in clause \ [ 3\ ] , the collection of MSEAs for vp include { V1 } and { Vsem } , where V1 represents the surface suing , and Vsem its `` semantics '' .</sentence>
				<definiendum id="0">V1</definiendum>
				<definiendum id="1">Vsem</definiendum>
				<definiens id="0">the surface suing</definiens>
			</definition>
			<definition id="7">
				<sentence>PROCEDURE INTERCLAUSAL ( DLC ) \ [ Inter-clausal inversion\ ] ( 1 ) Convert the deadlocked clause into a special canonical form in which the clause consists exclusively of two types of literals : the unification goals in the form X=Y where X is a variable and Y is a term , and the remaining literals whose arguments are only variables ( i.e. , no constants or functional terms are allowed ) .</sentence>
				<definiendum id="0">PROCEDURE INTERCLAUSAL</definiendum>
				<definiendum id="1">DLC</definiendum>
				<definiendum id="2">Y</definiendum>
				<definiens id="0">the unification goals in the form X=Y where X is a variable and</definiens>
				<definiens id="1">a term , and the remaining literals whose arguments are only variables ( i.e. , no constants or functional terms are allowed )</definiens>
			</definition>
</paper>

		<paper id="0212">
			<definition id="0">
				<sentence>&lt; ent h=introduce &gt; &lt; def &gt; bring in &lt; /def &gt; &lt; def &gt; bring forward &lt; /def &gt; &lt; ip &gt; introduce into / to &lt; /ip &gt; &lt; ex &gt; introduce a Bill before Parliament &lt; /ex &gt; &lt; def &gt; bring ( sth ) into use or into operation for the first time &lt; /def &gt; &lt; def &gt; cause ( sb ) to be acquainted with ( sth ) &lt; /def &gt; &lt; ex &gt; introduce nee ideas into a business &lt; /ex &gt; &lt; ex &gt; Tobacco was introduced into Europe from America &lt; /ex &gt; &lt; ex &gt; The teacher introduced his young pupils to the intricacies of geometry &lt; /ex &gt; &lt; ip &gt; introduce sb ( to sb ) &lt; /ip &gt; &lt; def &gt; make ( persons ) known by name ( to one another ) , esp in the usual formal way &lt; /def &gt; &lt; ex &gt; introduce two friends &lt; /ex &gt; &lt; ex &gt; He introduced me to his parents &lt; /ex &gt; &lt; ex &gt; The chairman introduced the lecturer to the audience &lt; /ex &gt; &lt; ip &gt; introduce ( into ) &lt; /ip &gt; &lt; def &gt; insert &lt; /def &gt; &lt; ex &gt; introduce a tube into a wound &lt; /ex &gt; &lt; ex &gt; introduce a subject into a conversation &lt; /ex &gt; °° .</sentence>
				<definiendum id="0">introduce sb</definiendum>
				<definiens id="0">esp in the usual formal way &lt; /def &gt; &lt; ex &gt; introduce two friends &lt; /ex &gt; &lt; ex &gt; He introduced me to his parents &lt; /ex &gt;</definiens>
			</definition>
</paper>

		<paper id="0107">
			<definition id="0">
				<sentence>A bottom-up generation algorithm allows the production of sentences given an initial semantic form .</sentence>
				<definiendum id="0">bottom-up generation algorithm</definiendum>
				<definiens id="0">allows the production of sentences given an initial semantic form</definiens>
			</definition>
			<definition id="1">
				<sentence>loves : V C3 represents the entries in the lexicon which are compatible with the initial logical form .</sentence>
				<definiendum id="0">V C3</definiendum>
				<definiens id="0">represents the entries in the lexicon which are compatible with the initial logical form</definiens>
			</definition>
</paper>

		<paper id="0106">
			<definition id="0">
				<sentence>This requirement to scan forces the adoption of algorithms based on the management of multiple hypotheses and predictions that feed a representation that must be expanded dynamically .</sentence>
				<definiendum id="0">scan</definiendum>
				<definiens id="0">forces the adoption of algorithms based on the management of multiple hypotheses and predictions that feed a representation that must be expanded dynamically</definiens>
			</definition>
			<definition id="1">
				<sentence>A semantic grammar brings the categories of analysis used by the parser into the same realm as those used by the generator , namely the categories of the application domain ( in the present case personnel changes ) , for example people , companies , dates , ages , job titles , relations such as former , new , or has-title , and event types such as appoint , succeed , retire , etc .</sentence>
				<definiendum id="0">semantic grammar</definiendum>
				<definiens id="0">those used by the generator , namely the categories of the application domain</definiens>
			</definition>
			<definition id="2">
				<sentence>The object promotion rule is a syntactic ( `` form '' ) rule that makes reference to the form label on an edge rather than theft semantic label .</sentence>
				<definiendum id="0">object promotion rule</definiendum>
				<definiens id="0">a syntactic ( `` form '' ) rule that makes reference to the form label on an edge rather than theft semantic label</definiens>
			</definition>
</paper>

		<paper id="0208">
			<definition id="0">
				<sentence>Created ( by Atkins ) in an effort to systematize and accelerate the process of dictionary compiling , the database draws on the relationships discussed by Fillmore ( e.g. 1969 , 1978 and elsewhere ) ; Apresjan 's cases of regularpolysemy ( 1973 ) ; Leech 's semantic transfer rules ( 1981 ) ; the verb alternations noted in Levin ( forthcoming ) ; the cases of systematic polysemy discussed in Miller ( 1978 ) , Clark &amp; Clark ( 1979 ) , Aronoff ( 1980 ) and Lehrer ( 1990 ) ; items participating in the crosslanguage lexicalizafion patterns which Talmy discusses ( 1985 ) ; the nom/nal compounds analysed by Levi ( 1982 ) ; and Pustejovsky 's generative lexicon ( 1991 ) .</sentence>
				<definiendum id="0">Created</definiendum>
			</definition>
			<definition id="1">
				<sentence>But to posit this is already to accept that the LIR is a feature of the grammar of English .</sentence>
				<definiendum id="0">LIR</definiendum>
			</definition>
</paper>

		<paper id="0105">
			<definition id="0">
				<sentence>In general , the goal of parsing is the derivation of all possible grammatical structures defined by a grammar of a given string a ( i.e. especially the determination of all possible logical forms of o~ ) and the goal of the corresponding generation task is the computation , of all possible strings defined by a grammar of a !</sentence>
				<definiendum id="0">generation task</definiendum>
				<definiens id="0">the derivation of all possible grammatical structures defined by a grammar of a given string a ( i.e. especially the determination of all possible logical forms of o~ ) and the goal of the corresponding</definiens>
			</definition>
			<definition id="1">
				<sentence>Systems The Problem It is widely accepted to cut down the problem of natural language generation ( NLG ) into two subtasks : • determination of the content of an utterance • determination of its linguistic realization This 'divide and conquer ' view of generation is the base of current architectures of systems .</sentence>
				<definiendum id="0">Problem It</definiendum>
				<definiendum id="1">conquer</definiendum>
				<definiens id="0">widely accepted to cut down the problem of natural language generation ( NLG ) into two subtasks : • determination of the content of an utterance • determination of its linguistic realization This 'divide and</definiens>
			</definition>
			<definition id="2">
				<sentence>Popelhow : A distributed parallel model for incremental natural language production with feedback .</sentence>
				<definiendum id="0">Popelhow</definiendum>
				<definiens id="0">A distributed parallel model for incremental natural language production with feedback</definiens>
			</definition>
			<definition id="3">
				<sentence>Dependency propagation : A unified theory of sentence comprehension and generation .</sentence>
				<definiendum id="0">Dependency propagation</definiendum>
				<definiens id="0">A unified theory of sentence comprehension and generation</definiens>
			</definition>
			<definition id="4">
				<sentence>Popel : A parallel and incremental natural language generation system .</sentence>
				<definiendum id="0">Popel</definiendum>
				<definiens id="0">A parallel and incremental natural language generation system</definiens>
			</definition>
</paper>

		<paper id="0206">
			<definition id="0">
				<sentence>More specifically , a CCM is a structured mapping that has the following characteristics .</sentence>
				<definiendum id="0">CCM</definiendum>
				<definiens id="0">a structured mapping that has the following characteristics</definiens>
			</definition>
			<definition id="1">
				<sentence>Representation : The explicit representation of the conventional metaphors in a language in the form of explicit associations between concepts .</sentence>
				<definiendum id="0">Representation</definiendum>
				<definiens id="0">The explicit representation of the conventional metaphors in a language in the form of explicit associations between concepts</definiens>
			</definition>
			<definition id="2">
				<sentence>61 This approach has been embodied in MIDAS ( Metaphor Interpretation , Denotation , and Acquisition System ) \ [ Martin , 1990\ ] MIDAS is a set of computer programs that can be used to perform the following tasks : explicitly represent knowledge about conventional metaphors , apply this knowledge to interpret metaphoric language , and learn new metaphors as they are encountered .</sentence>
				<definiendum id="0">MIDAS</definiendum>
				<definiens id="0">MIDAS ( Metaphor Interpretation , Denotation , and Acquisition System ) \ [</definiens>
				<definiens id="1">a set of computer programs that can be used to perform the following tasks : explicitly represent knowledge about conventional metaphors , apply this knowledge to interpret metaphoric language , and learn new metaphors as they are encountered</definiens>
			</definition>
			<definition id="3">
				<sentence>In this case , the metaphor consists of component associations that specify that the state of being enclosed represents the idea of currently using the editor , where the user plays the role of the enclosed thing , and the Emacs process plays the role of the enclosure .</sentence>
				<definiendum id="0">metaphor</definiendum>
				<definiens id="0">consists of component associations that specify that the state of being enclosed represents the idea of currently using the editor , where the user plays the role of the enclosed thing</definiens>
			</definition>
			<definition id="4">
				<sentence>KODIAK is an extended semantic network language in the tradition of KL-ONE \ [ Brachman and Schmolze , 1985\ ] and its variants .</sentence>
				<definiendum id="0">KODIAK</definiendum>
				<definiens id="0">an extended semantic network language in the tradition of KL-ONE \ [</definiens>
			</definition>
			<definition id="5">
				<sentence>MIDAS checks the input constraints against all the possible interpretations that can be conventionally associated with the input .</sentence>
				<definiendum id="0">MIDAS</definiendum>
				<definiens id="0">checks the input constraints against all the possible interpretations</definiens>
			</definition>
			<definition id="6">
				<sentence>In this case , MIDAS finds that the direct interpretation and one of the other possible entering metaphors can be rejected before the appropriate Enter-Lisp metaphor is found .</sentence>
				<definiendum id="0">MIDAS</definiendum>
				<definiens id="0">finds that the direct interpretation</definiens>
			</definition>
			<definition id="7">
				<sentence>( A Enter-Lisp ( T Container-Metaphor Metaphor-Schema ) ( enter-lisp-res enter-res -~ lisp-invoke-result ) ( lisp-enterer enterer -~ lisp-invoker ) ( entered-lisp entered -- + lisp-invoked ) ( enter-lisp-map Entering -~ Invoke-Lisp ) ) Mapping input concept EnteringSO to concept Invoke-Lisp30 Mapping input role entererSO with filler I203 to target role lisp-invoker30 Mapping input role enteredSO with filler LispS8 to target role lisp-invoked30 Yielding interpretation : ( A Invoke-Lisp30 ( ~ Invoke-Lisp ) ( lisp-invoked30 ( ~ lisp-invoked ) ( A Lisp58 ( ~ Lisp ) ) ) ( lisp-invoker30 ( T lisp-invoker ) ( A I203 ( T I ) ) ) ) MIDAS then begins the process of mapping from the given source concepts to the appropriate target concepts based on the constraints imposed by the metaphor .</sentence>
				<definiendum id="0">Enter-Lisp ( T Container-Metaphor Metaphor-Schema )</definiendum>
				<definiendum id="1">Lisp58</definiendum>
				<definiens id="0">enter-lisp-map Entering -~ Invoke-Lisp ) ) Mapping input</definiens>
				<definiens id="1">begins the process of mapping from the given source concepts to the appropriate target concepts based on the constraints imposed by the metaphor</definiens>
			</definition>
			<definition id="8">
				<sentence>The mapping process , called metaphoric unviewing , creates a new instance of the metaphor itself along with the attendant source and target concepts .</sentence>
				<definiendum id="0">mapping process</definiendum>
				<definiendum id="1">metaphoric unviewing</definiendum>
				<definiens id="0">creates a new instance of the metaphor itself along with the attendant source and target concepts</definiens>
			</definition>
			<definition id="9">
				<sentence>Final Representation : ( A How-q207 ( T How-q ) ( topic206 ( T topic ) ( A Invoke-Lisp30 ( T Invoke-Lisp ) ( lisp-invoked30 ( T lisp-invoked ) ( A Lisp58 ( T Lisp ) ) ) ( lisp-invoker30 ( ~ lisp-invoker ) ( A I203 ( T I ) ) ) ) ) ) The competence required for the frequent and conventional use of idiom , metaphor , metonymy , irony , and sarcasm can not be accounted for by either a narrow theory of lexicM semantics or any theory relying on world knowledge alone .</sentence>
				<definiendum id="0">Final Representation</definiendum>
				<definiendum id="1">T How-q ) ( topic206 ( T topic )</definiendum>
				<definiens id="0">A Invoke-Lisp30 ( T Invoke-Lisp ) ( lisp-invoked30 ( T lisp-invoked ) ( A Lisp58 ( T Lisp ) ) ) ( lisp-invoker30 ( ~ lisp-invoker ) ( A I203 ( T I ) ) ) ) ) ) The competence required for the frequent and conventional use of idiom , metaphor , metonymy , irony , and sarcasm can not be accounted for by either a narrow theory of lexicM semantics or any theory relying on world knowledge alone</definiens>
			</definition>
</paper>

		<paper id="0209">
			<definition id="0">
				<sentence>A well-known example of metaphoric sense extension is that involving use of a word denoting an animal to refer to humans ( `` John is a pig '' , `` John is a wombat '' etc ) .</sentence>
				<definiendum id="0">John</definiendum>
				<definiens id="0">a pig ''</definiens>
			</definition>
			<definition id="1">
				<sentence>There are other similarities between sense extension and derivational morphology ; clearly , productivity is an issue in both , and in particular , sense extension processes may apparently be blocked ( preempted by synonymy ) , in a way comparable to the situation in derivational morphology ( see e.g. Bauer 1983:87f ) .</sentence>
				<definiendum id="0">productivity</definiendum>
			</definition>
			<definition id="2">
				<sentence>( We can formally specify this relationship between the ground sense G and the base sense B as Vx , t\ [ G ( x , t ) ~ 3y , t'\ [ *B ( y , t ' ) A t ' &lt; t A x Eo y\ ] \ ] using the formalisation developed in Copestake ( 1990a ) following Krifka ( 1987 ) where nominal predicates are taken as being true of quantities of matter at some time index , where *B denotes a potentially plural entity , and where Eo represents a relationship of material constituency .</sentence>
				<definiendum id="0">*B</definiendum>
				<definiendum id="1">Eo</definiendum>
				<definiens id="0">nominal predicates are taken as being true of quantities of matter at some time index</definiens>
				<definiens id="1">a relationship of material constituency</definiens>
			</definition>
			<definition id="3">
				<sentence>Lexicalisation is defined as irregular and unpredictable modification of some or all of the semantic , syntactic or phonological properties of a derived form .</sentence>
				<definiendum id="0">Lexicalisation</definiendum>
				<definiens id="0">irregular and unpredictable modification of some or all of the semantic , syntactic or phonological properties of a derived form</definiens>
			</definition>
</paper>

		<paper id="0221">
			<definition id="0">
				<sentence>A slot is a named set of facets .</sentence>
				<definiendum id="0">slot</definiendum>
				<definiens id="0">a named set of facets</definiens>
			</definition>
			<definition id="1">
				<sentence>A facet is a named set of views and a view is a named set of fillers .</sentence>
				<definiendum id="0">facet</definiendum>
				<definiens id="0">a named set of views and a view is a named set of fillers</definiens>
			</definition>
			<definition id="2">
				<sentence>FRAMEKIT slots are interpreted as a subset of concepts called properties .</sentence>
				<definiendum id="0">FRAMEKIT slots</definiendum>
			</definition>
			<definition id="3">
				<sentence>ME MACIKTOSH ( IS-A ( VALUE ( COMMOB PERSORAL-COMPUTER ) ) ) ( PRODUCED-BY ( SEN ( COMMOB APPLE ) ) ) ( SUBCLASSES ( SEM ( COMMOK PLUS SEII IIFX ) ) ) ( HAS-AS-PART ( SEH ( COMMOK DISK-DRIVE SYSTEM-U|IT MORITOR CPU MEMORY EXTERKAL-D I SKETTE-DRI VE IRTERMAL-DI SKETTE-DRIVE ) ) ) ) Other slot values inherit from PERSONAL-COMPUTER ( e.g. , MAXIMUM-NUMBEROF-USERS ) , or from COMPUTER , INDEPENDENT-DEVICE , DEVICE , ARTIFACT , etc .</sentence>
				<definiendum id="0">ME MACIKTOSH ( IS-A</definiendum>
				<definiendum id="1">SEM</definiendum>
			</definition>
			<definition id="4">
				<sentence>In the above example , MACINTOSH is a frame name , IS-A is a slot name , VALUE and SEM are examples of facet names , COMMON is the view name , and APPLE and MEMORY are fillers , for example .</sentence>
				<definiendum id="0">MACINTOSH</definiendum>
				<definiendum id="1">IS-A</definiendum>
				<definiendum id="2">COMMON</definiendum>
				<definiens id="0">a frame name</definiens>
				<definiens id="1">a slot name</definiens>
			</definition>
			<definition id="5">
				<sentence>The zones are CAT ( syntactic category ) , ORTH ( orthography abbreviations and variants ) , PHON ( phonology ) , MORPH ( morphological irregular forms or class information ) , SYN ( syntactic features such as attributive ) , SYN-STRUC ( indication of sentenceor phrase-level syntactic inter-dependencies such as subcategorization ) , SEM ( lexical semantics / meaning representation ) , LEXICAL-RELATIONS ( collocations , etc. ) , PRAGM ( pragmatics hooks for deictics , for example , and stylistic factors ) , and STUFF ( user information such as modification audit trail , example sentences , etc. ) In this paper we concentrate on the SEM zone of the lexicon , since it is the locus of lexical-semantic analysis .</sentence>
				<definiendum id="0">CAT</definiendum>
				<definiendum id="1">SYN</definiendum>
				<definiendum id="2">SEM</definiendum>
				<definiendum id="3">STUFF</definiendum>
				<definiens id="0">orthography abbreviations and variants ) , PHON ( phonology ) , MORPH ( morphological irregular forms or class information</definiens>
				<definiens id="1">pragmatics hooks for deictics , for example , and stylistic factors</definiens>
				<definiens id="2">user information such as modification audit trail , example sentences</definiens>
			</definition>
			<definition id="6">
				<sentence>The LEX-MAP field contains mappings indicating the links between lexical units and ontological concepts .</sentence>
				<definiendum id="0">LEX-MAP field</definiendum>
				<definiens id="0">contains mappings indicating the links between lexical units and ontological concepts</definiens>
			</definition>
			<definition id="7">
				<sentence>In DIONYSUS , the semantic analyzer instantiates an instance of the dog concept ( e.g. , ~dog3~ ) when processing a sentence containing the word `` dog '' or a reference to it .</sentence>
				<definiendum id="0">semantic analyzer</definiendum>
			</definition>
			<definition id="8">
				<sentence>( VALUE `` $ varl ) ) ... ) The caret is an operator ( akin to an intension operator ) which retrieves the meaning of the syntactic constituent bound to the variable .</sentence>
				<definiendum id="0">caret</definiendum>
				<definiens id="0">an operator ( akin to an intension operator ) which retrieves the meaning of the syntactic constituent bound to the variable</definiens>
			</definition>
</paper>

		<paper id="0108">
</paper>

		<paper id="0216">
			<definition id="0">
				<sentence>Thus , the section lexical universe for the word skis consists of a list of the types of skis ( racing , mountain , jumping , hunting ) , their main parts ( skis proper and bindings ) , the main objects and actions necessary for the correct use ( exploitation ) of skis ( poles , grease , to wax ) , the main types of activities connected with skis ( a ski-trip , a ski-race ... ) ... the sections contain only such words as are necessary for talking on the topic , and nothing else .</sentence>
				<definiendum id="0">section lexical universe for the word skis</definiendum>
				<definiens id="0">consists of a list of the types of skis ( racing , mountain , jumping , hunting ) , their main parts ( skis proper and bindings ) , the main objects and actions necessary for the correct use ( exploitation ) of skis ( poles , grease , to wax ) , the main types of activities connected with skis ( a ski-trip , a ski-race ... ) ... the sections contain only such words as are necessary for talking on the topic , and nothing else</definiens>
			</definition>
</paper>

		<paper id="0207">
			<definition id="0">
				<sentence>3 The first part of the definition of park , namely `` public garden '' , can be represented by a concept with superconcept garden and a relation called PROPERTY to the concept public : a park is a garden with PROPERTY public A visualization of this definition ( in a KL-ONE-Iike graphical notation ) is given in Fig .</sentence>
				<definiendum id="0">park</definiendum>
				<definiens id="0">a garden with PROPERTY public A visualization of this definition</definiens>
			</definition>
			<definition id="1">
				<sentence>The part-whole relation is a relation between nominal concepts and can be represented in a TSL-based KR formalism by means of roles .</sentence>
				<definiendum id="0">part-whole relation</definiendum>
				<definiens id="0">a relation between nominal concepts</definiens>
			</definition>
			<definition id="2">
				<sentence>In the example above the two components brake and steering-wheel have to be related to bobsled by two COMPONENT roles : a bobsled is a sleigh with COMPONENTI brake with COMPONENT2 steering-wheel The roles are organized in a role hierarchy : COMPONENT is a PART COMPONENTlis a COMPONENT COMPONENT2 is a COMPONENT The PROPERTY relation is a relation between nominal concepts and `` property concepts '' , e.g. , public in the first example .</sentence>
				<definiendum id="0">COMPONENT</definiendum>
				<definiendum id="1">COMPONENT2</definiendum>
				<definiendum id="2">relation</definiendum>
				<definiens id="0">a PART COMPONENTlis a COMPONENT</definiens>
				<definiens id="1">a relation between nominal concepts and `` property concepts ''</definiens>
			</definition>
			<definition id="3">
				<sentence>The FUNCTION relation relates nominal concepts to concepts representing verb meanings , e.g. , 71 a bobsled is a sleigh with ... with FUNCTION racing The representation of verb meanings involves a number of further problems , e.g. , the representation of space and time , that can not be investigated in this paper .</sentence>
				<definiendum id="0">FUNCTION relation</definiendum>
				<definiens id="0">relates nominal concepts to concepts representing verb meanings</definiens>
			</definition>
			<definition id="4">
				<sentence>As a first step we have implemented a KR system that consists of a terminological and an assertional component .</sentence>
				<definiendum id="0">KR system</definiendum>
				<definiens id="0">consists of a terminological and an assertional component</definiens>
			</definition>
			<definition id="5">
				<sentence>Introduction to WordNet : An On-line Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="0201">
			<definition id="0">
				<sentence>And , though the idea of normalcy that Dowty found to be needed in accounting for progressive aspect seems intuitively to sanction defeasible inferences , Intensionai Logic provides no good way of accounting for the validity of examples that have exceptions , like ( 2 ) Harry is crossing the street .</sentence>
				<definiendum id="0">Intensionai Logic</definiendum>
				<definiens id="0">provides no good way of accounting for the validity of examples</definiens>
			</definition>
</paper>

		<paper id="0213">
			<definition id="0">
				<sentence>Every term , then , may be seen as the intersection of values in three dimensions : the hierarchical ( its ISA link to its immediate superordinate ) , the collocational ( the list of words with which it occurred in definition texts ) , and the transfer ( the corresponding entry term in the target language , itself an intersection of three dimensions ) .</sentence>
				<definiendum id="0">collocational</definiendum>
				<definiens id="0">the list of words with which it occurred in definition texts</definiens>
			</definition>
			<definition id="1">
				<sentence>Nor do they handle the subtleties of vagueness/homophony issues ( *John plays the flute and Harry football ) which turn up more than we expect in the operational use of machine translation .</sentence>
				<definiendum id="0">*John</definiendum>
			</definition>
</paper>

		<paper id="0104">
			<definition id="0">
				<sentence>When this is the ; case , it is by definition possible to find a program Pp for parsing and a ( not necessarily identical ) program Pa for generation such that , for any string z , Pp enumerates all associated semantics y and terminates , and , for any semantics y , Pg enumerates all associated strings z and terminates .</sentence>
				<definiendum id="0">Pp enumerates</definiendum>
				<definiens id="0">all associated semantics y and terminates</definiens>
			</definition>
			<definition id="1">
				<sentence>Inherent finite reversibility is the concept which , in my opinion , permit us to capture formally the intuitive notion that a certain grammar is , or is not , `` reversible '' .</sentence>
				<definiendum id="0">Inherent finite reversibility</definiendum>
				<definiens id="0">the concept which , in my opinion , permit us to capture formally the intuitive notion that a</definiens>
			</definition>
			<definition id="2">
				<sentence>Informally , the denotations of the predicate symbols p are defined as n-ary relations p ( zx , ... , Xn ) over H , built as the limit of a bottom-up process which starts from the unit clauses and uses the non-unit clauses to add new instances to each relation .</sentence>
				<definiendum id="0">the denotations of the predicate symbols</definiendum>
				<definiens id="0">n-ary relations p ( zx , ... , Xn ) over H , built as the limit of a bottom-up process which starts from the unit clauses and uses the non-unit clauses to add new instances to each relation</definiens>
			</definition>
			<definition id="3">
				<sentence>In particular , this process defines the unary relation r ( x ) on H , which we shall call the denotational semantics of the main predicate r relative to program P. Let T be a term over H ; We define the specialization of r ( X ) on T as the relation rT ( x ) on H defined by : def rT ( ~ ) =-r ( ~ ) ^ x E Z where ff is the relation of subsumption .</sentence>
				<definiendum id="0">ff</definiendum>
				<definiens id="0">the unary relation r ( x ) on H , which we shall call the denotational semantics of the main predicate r relative to program P. Let T be a term over H ; We define the specialization of r ( X ) on T as the relation rT ( x</definiens>
				<definiens id="1">the relation of subsumption</definiens>
			</definition>
			<definition id="4">
				<sentence>In case the term T is a variable X , we say that X is the trivial specialization , and we note that the relation rx ( z ) is identical to the relation r ( z ) .</sentence>
				<definiendum id="0">term T</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a variable X</definiens>
			</definition>
			<definition id="5">
				<sentence>We say that r~r is the operational semantics of the main predicate r of P , for specialization T , relative to interpreter intpr .</sentence>
				<definiendum id="0">r~r</definiendum>
				<definiens id="0">the operational semantics of the main predicate r of P , for specialization T , relative to interpreter intpr</definiens>
			</definition>
			<definition id="6">
				<sentence>Soundness is a minimal requirement for an interpreter , and we will always assume it , but completeness is a requirement which is not always met in practice .</sentence>
				<definiendum id="0">Soundness</definiendum>
				<definiens id="0">a minimal requirement for an interpreter</definiens>
				<definiens id="1">a requirement which is not always met in practice</definiens>
			</definition>
			<definition id="7">
				<sentence>is any ground term , and Y is a certain variable ( whose name is indifferent ) .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">a certain variable ( whose name is indifferent )</definiens>
			</definition>
			<definition id="8">
				<sentence>The specialization class is associated with both these problems is the class G~ = { X # Y } ueH consisting in all the terms X # y where y is any ground term , and X is a certain variable ( whose name is indifferent ) .</sentence>
				<definiendum id="0">specialization class</definiendum>
				<definiendum id="1">y</definiendum>
				<definiendum id="2">X</definiendum>
				<definiens id="0">any ground term , and</definiens>
				<definiens id="1">a certain variable ( whose name is indifferent )</definiens>
			</definition>
			<definition id="9">
				<sentence>mars We say that a grammar G is ( inherently ) finitely reversible iff , in the terminology of §3.1.1 and §3.1.2 , G is such that : In other words , G is finitely reversible iff there exists a program Pp for parsing and a ( not necessarily identical ) program P9 for generation such that , relative to some sound and complete interpreter : is any ground term , Pp returns a finite list of answers x # T1 , x # T2 , ... , z # Tk and stops .</sentence>
				<definiendum id="0">G</definiendum>
				<definiendum id="1">Pp</definiendum>
				<definiens id="0">any ground term ,</definiens>
			</definition>
			<definition id="10">
				<sentence>any ground term , Pg returns a finite list of answers TI # y , T~ # y , . . . , T/ # y and stops .</sentence>
				<definiendum id="0">Pg</definiendum>
				<definiens id="0">returns a finite list of answers TI # y</definiens>
			</definition>
			<definition id="11">
				<sentence>, a diophantine equation being a multivariable polynomial in integer coefficients and whose variables range over N. 2° Let K be a recursively enumerable , but nonrecursive , subset of N. One corollary of Matiyasevich 's theorem is the following proPerty \ [ 2 , p. 127-28\ ] : There exists a polynomial q ( zx , ... , zn ) in integer coefficients such that K is the set of values taken by q , for zl , ... , z , ~ ranging over all integers .</sentence>
				<definiendum id="0">theorem</definiendum>
			</definition>
			<definition id="12">
				<sentence>Indeed , the problem of g-acceptation is the problem of deciding , for any given integer y , whether y is in the image of polynomial q , that is , whether y belongs to K. But K is a non-recursive set , hence the conclusion .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">a non-recursive set , hence the conclusion</definiens>
			</definition>
			<definition id="13">
				<sentence>Consider ordered pairs ( x , y ) of ( ground ) terms where x is a string encoding a certain first-order logic tautology , and y ( the `` semantics '' ) is a derivation of x using a certain fixed set of axiom schemata and rules of inference for a complete system of firstorder logic .</sentence>
				<definiendum id="0">Consider ordered pairs</definiendum>
				<definiendum id="1">x</definiendum>
				<definiendum id="2">y</definiendum>
				<definiens id="0">a string encoding a certain first-order logic tautology , and</definiens>
				<definiens id="1">a derivation of x using a certain fixed set of axiom schemata and rules of inference for a complete system of firstorder logic</definiens>
			</definition>
			<definition id="14">
				<sentence>Each clause corresponding to an axiom schema of name as defines `` terminal constituents '' ( x , as ( x ) ) , where string z is any instance of schema as ; each clause corresponding to an inference rule of name ir takes two `` constituents '' ( xx , yl ) and ( x2 , y2 ) , and , if applicable ( which is checked on the basis of strings Xl and x2 ) , builds a new constituent ( x , y ) , where x is the string obtained from xl and x2 according to it , and where y is a new derivation tree ir ( x , yx , y2 ) .</sentence>
				<definiendum id="0">string z</definiendum>
				<definiens id="0">any instance of schema as ; each clause corresponding to an inference rule of name ir takes two `` constituents '' ( xx , yl</definiens>
				<definiens id="1">the string obtained from xl and x2 according to it , and where y is a new derivation tree ir</definiens>
			</definition>
</paper>

		<paper id="0210">
			<definition id="0">
				<sentence>Generative Lexicon theory postulates a number of representational subsystems for the description of lexical items and the rules of syntactic and semantic composition .</sentence>
				<definiendum id="0">Generative Lexicon theory</definiendum>
				<definiens id="0">postulates a number of representational subsystems for the description of lexical items and the rules of syntactic and semantic composition</definiens>
			</definition>
			<definition id="1">
				<sentence>These include , in addition to argument and event structures , ( 1 ) a qualia structure , which partitions the aspects of a noun 's meaning into formal , constitutive , agentive , and telic roles , or qualia , ( 2 ) a lexical inheritance network reflecting the same partition , ( 3 ) Lexical Conceptual Paradigms , which describe sets of syntactic behaviors which correspond to lexicai semantic categories , and ( 4 ) generative devices for extending the logical senses of lexical items dynamically , as for instance coercion rules , mentioned later in this paper .</sentence>
				<definiendum id="0">Conceptual Paradigms</definiendum>
				<definiens id="0">partitions the aspects of a noun 's meaning into formal , constitutive , agentive , and telic roles , or qualia</definiens>
			</definition>
			<definition id="2">
				<sentence>A meal directly denotes both the event pertaining to thc activity of eating the food , as well as tile food itself .</sentence>
				<definiendum id="0">meal</definiendum>
				<definiens id="0">the event pertaining to thc activity of eating the food</definiens>
			</definition>
			<definition id="3">
				<sentence>We therefore introduce a semantic type REPORTING-VERB , which allows us to define a small part of the lexical hierarchy of reporting verbs as follows : utter ( A , B ) \ [ Form : produce ( P1 , A , B ) &amp; is ( B , sound ) &amp; is ( B , words ) \ ] \ [ Constitutive : intentional ( P 1 ) \ ] \ [ Agentive : human ( A ) &amp; individual ( A ) &amp; use ( A , voice ) \ ] REPORTING-VERB ( A , B ) \ [ Form : utter ( T1 , C , utter ( P1 , A , B ) ) &amp; proposition ( B ) \ ] \ [ Telic : has-expertise ( A , B ) or has-insight ( A , B ) \ [ Constitutive : interpretation ( T1 , P1 ) \ ] \ [ Agentive : use ( C , voice or writing ) \ ] assert ( A , B ) \ [ Form : utter ( A , B ) &amp; true ( B ) \ ] 107 say ( A , B ) \ [ Form : \ [ Literal : utter ( A , B ) \ ] \ [ REPORTING-VERB : assert ( A , B ) \ ] \ ] announce ( A , B ) \ [ Form : say ( T1 , A , B ) \ ] \ [ Constitutive : new ( B ) \ ] \ [ Agentive : legitimation ( A , T1 ) claim ( A , B ) \ [ Form : \ [ Literal : assert ( A , P1 ) &amp; own ( P1 , A , B ) ; demand ( T1 , A , B ) \ ] \ [ REPORTING-VERB : assert ( A , B ) &amp; opposition ( C , B ) \ ] insist ( A , B ) \ [ Form : claim ( A , B ) \ [ Constitutive : MANNER : vehement\ ] This partial hierarchy is a very crude classification of the four reporting verbs say , announce , claim , and insist .</sentence>
				<definiendum id="0">semantic type REPORTING-VERB</definiendum>
				<definiendum id="1">P1</definiendum>
				<definiendum id="2">B</definiendum>
				<definiens id="0">allows us to define a small part of the lexical hierarchy of reporting verbs as follows : utter ( A , B ) \ [ Form : produce ( P1 , A , B ) &amp; is ( B , sound ) &amp; is ( B , words</definiens>
				<definiens id="1">human ( A ) &amp; individual ( A ) &amp; use ( A , voice ) \ ] REPORTING-VERB ( A , B ) \ [ Form : utter ( T1 , C , utter</definiens>
				<definiens id="2">has-expertise ( A , B ) or has-insight ( A , B ) \ [ Constitutive : interpretation ( T1 , P1 ) \ ] \ [ Agentive : use ( C , voice or writing ) \ ] assert ( A , B ) \ [ Form : utter ( A , B ) &amp; true ( B ) \ ] 107 say ( A , B ) \ [ Form : \ [ Literal : utter ( A , B ) \ ] \ [ REPORTING-VERB : assert ( A , B ) \ ] \ ] announce ( A , B ) \ [ Form : say ( T1 , A , B ) \ ] \ [ Constitutive : new ( B ) \ ] \ [ Agentive : legitimation ( A , T1 ) claim ( A , B ) \ [ Form : \ [ Literal : assert ( A , P1 ) &amp; own ( P1 , A , B ) ; demand ( T1 , A , B ) \ ] \ [ REPORTING-VERB : assert ( A , B ) &amp; opposition ( C , B ) \ ] insist ( A , B ) \ [ Form : claim ( A ,</definiens>
			</definition>
			<definition id="4">
				<sentence>Coercion rules are a generative device , illustrating one of the dynamic aspects of the Generative Lexicon formalism , which aims to represent world knowledge that is reflected in the syntactic and semantic behavior of words .</sentence>
				<definiendum id="0">Coercion rules</definiendum>
				<definiens id="0">a generative device , illustrating one of the dynamic aspects of the Generative Lexicon formalism</definiens>
				<definiens id="1">aims to represent world knowledge that is reflected in the syntactic and semantic behavior of words</definiens>
			</definition>
</paper>

		<paper id="0220">
			<definition id="0">
				<sentence>Default Logic is one formal method for performing default reasoning in the area of Artificial Intelligence called Knowledge Representation .</sentence>
				<definiendum id="0">Default Logic</definiendum>
				<definiens id="0">one formal method for performing default reasoning in the area of Artificial Intelligence called Knowledge Representation</definiens>
			</definition>
			<definition id="1">
				<sentence>Being implied by a simple natural language sentence and the natural ( or preferred ) interpretation of its simple negation is the primary quality that qualifies an inference as a presupposition .</sentence>
				<definiendum id="0">negation</definiendum>
				<definiens id="0">a simple natural language sentence and the natural ( or preferred ) interpretation of its simple</definiens>
			</definition>
			<definition id="2">
				<sentence>Examples of this type of inference include : deducing that a particular individual has a property from the knowledge that all individuals have that property ; deducing the truth of two conjuncts given the truth of the conjunction ; and deducing the consequent of a material implication given the truth of the antecedent .</sentence>
				<definiendum id="0">deducing</definiendum>
				<definiens id="0">the consequent of a material implication given the truth of the antecedent</definiens>
			</definition>
			<definition id="3">
				<sentence>Hence MALE ( c1 ) is a lexical presupposition of the utterance , but ADULT ( c1 ) is not .</sentence>
				<definiendum id="0">Hence MALE</definiendum>
				<definiendum id="1">ADULT</definiendum>
				<definiens id="0">a lexical presupposition of the utterance , but</definiens>
			</definition>
			<definition id="4">
				<sentence>MALE ( z ) D ~FEMALE ( x ) The first case , represented by ( 27 ) , together with the lexical information represented by ( 29 ) - ( 37 ) form a default theory which allows MALE ( cl ) and ADULT ( cl ) to be derived as entailments .</sentence>
				<definiendum id="0">~FEMALE ( x</definiendum>
				<definiendum id="1">ADULT</definiendum>
				<definiens id="0">( cl ) to be derived as entailments</definiens>
			</definition>
			<definition id="5">
				<sentence>( 39 ) -~MALE ( x ) V -~ADULT ( x ) V MARRIED ( x ) The contextual nature of the presupposition inference as captured by default reasoning indicates that one difference between semantics and pragmatics revolves around the monotonic/non-monotonic character of semantic and pragmatic inferences .</sentence>
				<definiendum id="0">-~MALE ( x ) V -~ADULT ( x ) V MARRIED ( x</definiendum>
				<definiens id="0">The contextual nature of the presupposition inference as captured by default reasoning indicates that one difference between semantics and pragmatics revolves around the monotonic/non-monotonic character of semantic and pragmatic inferences</definiens>
			</definition>
			<definition id="6">
				<sentence>In order to prevent these unwanted inferences , we have used guards on the default rules ( the LF predicate ) to limit them to appropriate contexts .</sentence>
				<definiendum id="0">rules</definiendum>
			</definition>
			<definition id="7">
				<sentence>How this method compares with the standard view of default rules in a Default Logic remains to be investigated .</sentence>
				<definiendum id="0">Default Logic</definiendum>
				<definiens id="0">remains to be investigated</definiens>
			</definition>
</paper>

		<paper id="0116">
			<definition id="0">
				<sentence>GENESYS is a very large SG generator for English , written in Prolog ( Fawcett and Tucker \ [ 10\ ] ) and it is version PG1.5 that has been used for the development and testing of REVELATIONI .</sentence>
				<definiendum id="0">GENESYS</definiendum>
			</definition>
			<definition id="1">
				<sentence>Systemic Grammar Fawcett 's SG is a meaning-oriented model of language in which there is a large 'and/or ' network of semantic features , defining the choices in meaning that are available .</sentence>
				<definiendum id="0">SG</definiendum>
				<definiens id="0">a meaning-oriented model of language in which there</definiens>
			</definition>
			<definition id="2">
				<sentence>Systemic syntax trees consist of a number of levels of structure called units ; the tree in Figure 1 has four : one clause ( CI ) , two nominal groups ( ngp ) and a quantityquality group ( qqgp ) .</sentence>
				<definiendum id="0">Systemic syntax trees</definiendum>
				<definiens id="0">consist of a number of levels of structure called units ; the tree in Figure 1 has four : one clause ( CI ) , two nominal groups ( ngp ) and a quantityquality group ( qqgp )</definiens>
			</definition>
			<definition id="3">
				<sentence>A realization rule specifies the structural consequences of choosing a feature ; they map the semantic choices onto syntactic structures .</sentence>
				<definiendum id="0">realization rule</definiendum>
				<definiens id="0">specifies the structural consequences of choosing a feature ; they map the semantic choices onto syntactic structures</definiens>
			</definition>
			<definition id="4">
				<sentence>REVELATION2 ( currently under development ) will attempt to implement this process of elimination by moving forward through the system network ( after a partial semantic representation has been obtained ) systematically verifying any realization rules that it meets , eliminating features that can not be chosen and so possibly inferring something about unrealized features which need to be chosen .</sentence>
				<definiendum id="0">REVELATION2</definiendum>
				<definiens id="0">a partial semantic representation has been obtained ) systematically verifying any realization rules that it meets , eliminating features that can not be chosen and so possibly inferring something about unrealized features which need to be chosen</definiens>
			</definition>
			<definition id="5">
				<sentence>`` NIGEL : A Systemic Grammar for Text Generation '' .</sentence>
				<definiendum id="0">NIGEL</definiendum>
				<definiens id="0">A Systemic Grammar for Text Generation ''</definiens>
			</definition>
</paper>

		<paper id="0114">
			<definition id="0">
				<sentence>, ( s ) =d , l ( ix ... . , in ) ( 2 ) where U ( s ) = { il , ... , in } and \ [ j &lt; k\ ] ~ \ [ ( s , it ) &lt; p , . , ( s , ik ) \ ] GPo , . ( i ) =d , ! ( s , ... so 0 ( 3 ) where G ( 0 = { sx , ... , sin } and IJ &lt; -- . &lt; p. , . ( i , sk ) ) \ ] Alternatively , we note that any Preference P induces an equivalence relation ='p which groups together the objects that are equal under p.4 We can therefore view the task of Generation and Understanding as being the enumeration of P 's equivalence classes in order , without worrying about order within classes ( note that Formulae 2 and 3 specify the order only of pairs where one member is less than the other under P. ) The question now arises of what the relation between understanding Preferences and generation Preferences should be. Understanding heuristics are intended to find the meaning that the speaker is most likely to have intended for an utterance , and generation heuristics should select the string that is most likely to communicate a given meaning to the hearer. We would expect these Preferences to be inverses of each other : if s is the best way to express meaning i , then i should be the most likely interpretation of s. If we do n't accept this condition , we will generate sentences that we expect the listener to misinterpret. Therefore we define class ( Preference , pair ) to be the equivalence class that pair is assigned to under Preference 's ordering , 5 and link the the first 3Note that this definition allows Preferences to work 'across derivations. ' For example , it allows Pint to rank pairs ( s , , } , ( s ' , i9 where 8 # s'. It permits a Preference to say that i is a better interpretation for s than i ' is for s : . It is not clear if this sort of power is necessary , and the algorithtns below require only that Preferences be able to rank different interpretations ( strings ) for a given string ( interpretation ) . 4Any order P on a set of objects D partitions D into a set of equivalence classes by assigning each x E D to the set { ulv _ &lt; P x : z x _ &lt; p u } . Selass ( Preference , pair ) is defined as the number of classes containing items that rank more highly than pair under Preference. ( most highly ranked ) classes under P/. , and P. , r as follows : elass ( eo , r , ( / , 8 ) ) = 0 ( 4 ) -. crass ( P , . , , ( s , O ) = 0 It is also reasonable to require that opposing sets of preferences in understanding be reflected in generation. If string s , has two interpretations it and i2 , with it being preferred to is , and string ss has the same two interpretations with the preferences reversed , then s , should be a better way of expressing i , than i2 , and vice-versa for ss : \ [ ( sl , il ) &lt; p , . , @ 1 , is ) &amp; ( as , i2 ) &lt; e , . , ( as , il ) \ ] \ [ ( il , sd &lt; p. , . ( i ,82 ) &amp; ( is , as ) &lt; p. , . 62 , .x ) \ ] ( 5 ) Formula 4 provides a tight coupling of heuristics for understanding and generating the most preferred structures , but it does n't provide any way to share Preferences for secondary readings. Formula 5 offers a way to share heuristics for secondary interpretations , but it is quite weak and would be highly inefficient to use. To employ it during generation to choose between sl and ss as ways of expressing il , we would have to run the understanding system on both sl and ss to see if we could find another interpretation i2 that both strings share but with opposite rankings relative to il. If we want to share Preferences for secondary readings , we will need to make stronger assumptions. The question of ranking secondary interpretations brings us onto treacherous ground since most common heuristics ( e.g. , preferring low attachment ) specify only the best reading and do n't help choose between secondary and tertiary readings. Furthermore , native speakers do n't seem to have clear intuitions about the relative ranking of lesser readings. Finally , there is some question about why we should care about non-primary readings , since the best interpretation or string is normally what we want. However , it is important to deal with secondary preferences , in part for systematic completeness , but mostly because secondary readings are vital in any at , tempt to deal with figurative languagehumor , irony , and metaphor which depends on the interplay between primary and secondary readings. ii0 i To begin to develop a theory of secondary Preferences , we will simply stipulate that the heuristics in question are shared 'across the board ' beI tween understanding and generatmn.~ The simplest way to do this is to extend Formula 4 into a biconditional , a~d require it to hold of all classes ( we will reconsider this stipulation in Section 5 ) . For all s6Strin~l and i6Int , we have : et.ss ( P , . , , ( , , , ) ) = el.ss ( P. , . , ( i , 8 ) ) ( 6 ) Since Preferences now work in either direction , we can simplify our notation and represent them as total orderings of a set T of trees , where each node of each tre4 is annotated with syntactic and semantic information , and , for any t 6 T , str ( t ) • k returns the string in String that t dominates ( i.e. , spans ) , and sere ( t ) returns the interpretation in Int for the root node of t. For apreferenee P on T and trees tl , th , we stipulate : t , &lt; p t2 Up ( str ( tl ) ) tl &lt; p t2 = &amp; Gp ( sem ( tl ) ) = str ( tl ) =str ( t2 ) ( 7 ) ~ . . sem ( tl ) . . . sem ( t2 ) . . . ) sere ( q ) = sere ( t2 ) ( 8 ) We close this Section by noting a property of Preferences thatwill be important in Section 4 : an ordered list Of Preferences can be combined into a new Preference by using each item in the list to refine the bordering specified by the previous ones. That is , the second Preference orders pairs that are equal under the first Preference , and the third Preference applies to those that are still equal under the second Preference , etc. If P1 -- . P , are Preferences , we define a new Complex Preference P &lt; , ... , &gt; as follows : tl &lt; Pc- , ... , . ) t2 ( 9 ) ~- ; B l &lt; j &lt; n \ [ Q &lt; pj t2\ ] &amp; -,3i &lt; j \ [ t2 &lt; p , tl\ ] Preferences If we consider ways of sharing Preferences between understanding and generation , the simplest one is to simply produce all possible interpretations ( strings ) , and then sort them using the Preference. This is , of course , inefficient in cases where we are interested in only the more highly ranked possibilities. We can do better if we are willing to make few assumptions about the structure of Preferences and the understanding and generation routines. The crucial requirement on Preferences is that they be 'upwardly monotonic ' in the following sense : if t , is preferred to t2 , then it is also preferred to any tree containing tz as a subtree. Using subtree ( t , ,t2 ) to mean that tx is a subtree of t2 , we stipulate \ [ tl &lt; p t2 ~ subtree ( t2 , t3 ) \ ] ( 10 ) -- ~ tl &lt; P gS Without such a requirement , there is no way to cut off unpromising paths , since we ca n't predict the ranking of a complete structure from that of its constituents• FinaLly , we assume that both understanding and generation are agenda-driven procedures that work by creating , combining , and elaborating trees. 6 Under these assumptions , the following high-level algorithm can be wrapped around the underlying parsing and generation routines to cause the output to be enumerated in the order given by a Preference P. In the pseudo-code below , mode specifies the direction of processing and input is a string ( if mode is understanding ) or a semantic representation ( if mode is generation ) . execute_item removes an item from the agenda and executes it , returning 0 or more new trees. generate_items takes a newly formed tree , a set of previously existing trees , and the mode , and adds a set of new actions to the agenda. ( The underlying understanding or generation algorithm is hidden inside generate_items. ) The variable active holds the set of trees that are currently being used to generate new items , while frozen holds those that wo n't be used until later , complete_tree is a termination test that returns True if a tree is complete for the mode in question ( i.e. , if it has a full semantic interpretation for understanding , or dominates a complete string for generation ) . The global variable classes holds a list of equivalence classes used by equiv_class ( defined below ) , while level holds the number of the equivalence class currently being enumerated. Thaw~restart is called each time level is incremented to generate new agenda items for trees that may belong to that class. ALGORITHM 1 e A wide variety of NLP algorithms can be implemented in this manner , particularly such recent reversible generation algorithms as \ [ Shieber , van Noord , Moore , and Pereira , 1989\ ] and \ [ Calder , Reape , and Zeevat , 1989\ ] . iii classes : = Nil ; solutions : = Nil ; new-trees : = Nil ; agenda : = Nil ; level : = 1 ; frozen : = initialize.agenda ( input , mode ) ; { end of global declarations } while frozen do begin solutions : = get_complete_trees ( frozen , level , mode ) ; agenda : = thaw &amp; restart ( frozen , level , agenda , mode ) ; while agenda do begin new_trees : = execute_item ( agenda ) ; while new_trees do begin new_tree : = pop ( new_trees ) ; if equiv_class ( P , new_tree ) , &gt; level then push ( new_tree , frozen ) ; else if complete_tree ( new_tree , mode ) then push ( newAree , solutions ) ; else generate , items ( new_tree , active , agenda , mode ) ; end ; end ; { agenda exhausted for this level } { solutions may need partitioning } while solutions do begin complete_tree : = pop ( solutions ) ; if equiv_class ( P , complete_tree ) &gt; level then push ( complete_tree , frozen ) ; else output ( complete_tree , level ) ; end { increment level to output next class } level : = level + 1 ; end ; The function equiv_class keeps track of the equivalence classes induced by the Preferences .</sentence>
				<definiendum id="0">s</definiendum>
				<definiendum id="1">input</definiendum>
				<definiendum id="2">complete_tree</definiendum>
				<definiens id="0">=d , l ( ix ... . , in ) ( 2 ) where U ( s ) = { il , ... , in } and \ [ j &lt; k\ ] ~ \ [ ( s , it ) &lt; p , . , ( s , ik ) \ ] GPo , . ( i ) =d , ! ( s , ... so 0 ( 3 ) where G ( 0 = { sx , ... , sin } and IJ &lt; -- . &lt; p. , . ( i , sk ) ) \ ] Alternatively , we note that any Preference P induces an equivalence relation ='p which groups together the objects that are equal under p.4 We can therefore view the task of Generation and Understanding as being the enumeration of P 's equivalence classes in order , without worrying about order within classes ( note that Formulae 2 and 3 specify the order only of pairs where one member is less than the other under P. ) The question now arises of what the relation between understanding Preferences and generation Preferences should be. Understanding heuristics are intended to find the meaning that the speaker is most likely to have intended for an utterance , and generation heuristics should select the string that is most likely to communicate a given meaning to the hearer. We would expect these Preferences to be inverses of each other : if s is the best way to express meaning i , then i should be the most likely interpretation of s. If we do n't accept this condition , we will generate sentences that we expect the listener to misinterpret. Therefore we define class ( Preference , pair ) to be the equivalence class that pair is assigned to under Preference 's ordering , 5 and link the the first 3Note that this definition allows Preferences to work 'across derivations. ' For example , it allows Pint to rank pairs ( s , , } , ( s ' , i9 where 8 # s'. It permits a Preference to say that i is a better interpretation for s than i ' is for s</definiens>
				<definiens id="1">able to rank different interpretations ( strings ) for a given string ( interpretation ) . 4Any order P on a set of objects D partitions D into a set of equivalence classes by assigning each x E D to the set { ulv _ &lt; P x : z x _ &lt; p u }</definiens>
				<definiens id="2">the number of classes containing items that rank more highly than pair under Preference. ( most highly ranked ) classes under P/. , and P. , r as follows : elass ( eo , r , ( / , 8 ) ) = 0 ( 4 ) -. crass ( P , . , , ( s , O ) = 0 It is also reasonable to require that opposing sets of preferences in understanding be reflected in generation. If string s , has two interpretations it and i2 , with it being preferred to is , and string ss has the same two interpretations with the preferences reversed , then s , should be a better way of expressing i , than i2 , and vice-versa for ss : \ [ ( sl , il ) &lt; p , . , @ 1 , is ) &amp; ( as , i2 ) &lt; e , . , ( as , il ) \ ] \ [ ( il , sd &lt; p. , . ( i ,82 ) &amp; ( is , as ) &lt; p. , . 62 , .x ) \ ] ( 5 ) Formula 4 provides a tight coupling of heuristics for understanding and generating the most preferred structures , but it does n't provide any way to share Preferences for secondary readings. Formula 5 offers a way to share heuristics for secondary interpretations , but it is quite weak and would be highly inefficient to use. To employ it during generation to choose between sl and ss as ways of expressing il , we would have to run the understanding system on both sl and ss to see if we could find another interpretation i2 that both strings share but with opposite rankings relative to il. If we want to share Preferences for secondary readings , we will need to make stronger assumptions. The question of ranking secondary interpretations brings us onto treacherous ground since most common heuristics ( e.g. , preferring low attachment ) specify only the best reading and do n't help choose between secondary and tertiary readings. Furthermore , native speakers do n't seem to have clear intuitions about the relative ranking of lesser readings. Finally , there is some question about why we should care about non-primary readings , since the best interpretation or string is normally what we want. However , it is important to deal with secondary preferences , in part for systematic completeness , but mostly because secondary readings are vital in any at , tempt to deal with figurative languagehumor , irony , and metaphor which depends on the interplay between primary and secondary readings. ii0 i To begin to develop a theory of secondary Preferences , we will simply stipulate that the heuristics in question are shared 'across the board ' beI tween understanding and generatmn.~ The simplest way to do this is to extend Formula 4 into a biconditional , a~d require it to hold of all classes ( we will reconsider this stipulation in Section 5 ) . For all s6Strin~l and i6Int</definiens>
				<definiens id="3">et.ss ( P , . , , ( , , , ) ) = el.ss ( P. , . , ( i , 8 ) ) ( 6 ) Since Preferences now work in either direction , we can simplify our notation and represent them as total orderings of a set T of trees , where each node of each tre4 is annotated with syntactic and semantic information , and , for any t 6 T , str ( t ) • k returns the string in String that t dominates ( i.e. , spans ) , and sere ( t ) returns the interpretation in Int for the root node of t. For apreferenee P on T and trees tl , th , we stipulate : t , &lt; p t2 Up ( str ( tl ) ) tl &lt; p t2 = &amp; Gp ( sem ( tl ) ) = str ( tl ) =str ( t2 ) ( 7 ) ~ . . sem ( tl ) . . . sem ( t2 ) . . . ) sere ( q ) = sere ( t2 ) ( 8 ) We close this Section by noting a property of Preferences thatwill be important in Section 4 : an ordered list Of Preferences can be combined into a new Preference by using each item in the list to refine the bordering specified by the previous ones. That is , the second Preference orders pairs that are equal under the first Preference , and the third Preference applies to those that are still equal under the second Preference , etc. If P1 --</definiens>
				<definiens id="4">a new Complex Preference P &lt; , ... , &gt; as follows : tl &lt; Pc- , ... , . ) t2 ( 9 ) ~- ; B l &lt; j &lt; n \ [ Q &lt; pj t2\ ] &amp; -,3i &lt; j \ [ t2 &lt; p , tl\ ] Preferences If we consider ways of sharing Preferences between understanding and generation , the simplest one is to simply produce all possible interpretations ( strings ) , and then sort them using the Preference. This is , of course , inefficient in cases where we are interested in only the more highly ranked possibilities. We can do better if we are willing to make few assumptions about the structure of Preferences and the understanding and generation routines. The crucial requirement on Preferences is that they be 'upwardly monotonic ' in the following sense : if t , is preferred to t2 , then it is also preferred to any tree containing tz as a subtree. Using subtree ( t , ,t2 ) to mean that tx is a subtree of t2 , we stipulate \ [ tl &lt; p t2 ~ subtree ( t2 , t3 ) \ ] ( 10 ) -- ~ tl &lt; P gS Without such a requirement , there is no way to cut off unpromising paths , since we ca n't predict the ranking of a complete structure from that of its constituents• FinaLly , we assume that both understanding and generation are agenda-driven procedures that work by creating , combining , and elaborating trees. 6 Under these assumptions , the following high-level algorithm can be wrapped around the underlying parsing and generation routines to cause the output to be enumerated in the order given by a Preference P. In the pseudo-code below , mode specifies the direction of processing and</definiens>
				<definiens id="5">a string ( if mode is understanding ) or a semantic representation ( if mode is generation ) . execute_item removes an item from the agenda and executes it , returning 0 or more new trees. generate_items takes a newly formed tree , a set of previously existing trees , and the mode , and adds a set of new actions to the agenda. ( The underlying understanding or generation algorithm is hidden inside generate_items. ) The variable active holds the set of trees that are currently being used to generate new items , while frozen holds those that wo n't be used until later ,</definiens>
			</definition>
			<definition id="1">
				<sentence>Create_class takes a tree and creates a new class whose only member is that tree , while insert adds a class to classes in the indicated position ( shifting other classes down , if necessary ) , and select_member returns an arbitrary member of a class .</sentence>
				<definiendum id="0">Create_class</definiendum>
				<definiendum id="1">select_member</definiendum>
				<definiens id="0">takes a tree and creates a new class whose only member is that tree</definiens>
				<definiens id="1">returns an arbitrary member of a class</definiens>
			</definition>
			<definition id="2">
				<sentence>function equiv_class ( P : Preference , T : Tree ) begin class_num : = 1 ; for class in classes do begin if less_than ( P , T , select_member ( class ) ) then begin insert ( new_class ( T ) , classes , class_num ) ; return ( classmum ) ; end ; else if equal ( P , T , select_member ( class ) ) then begin add_member ( T , class ) ; return ( class_hum ) ; end ; else class_num : = class_num + 1 ; end ; { T &lt; all classes } insert ( new_class ( T ) , classes , class_num ) ; return ( class_num ) ; end { equiv_elass } To see that the algorithm enumerates trees in the order given by &lt; p , note that the first iteration outputs trees which are minimal under &lt; p. Now consider any tree t , which is output on a subseqent itertion N. For all other t , , output on that iteration , t , =p t , ,. Furthermore , t , contains a subtree t , ub which was frozen for all levels up to N. Using T ( J ) to denote the set of trees output on iteration J , we have : VI_ &lt; I &lt; N IV ti 6 T ( I ) ti &lt; p t , ub\ ] \ ] , whence , by stipulation 10 , t , &lt; p ti. Thus t , is greater than or equal to 112 all trees which were enumerated before it. To calculate the time complexity of the algorithm , note that it calls equiv_class once for each tree created by the underlying understanding or generation algorithm ( and once for each complete interpretation ) . Equiv_class , in turn , must potentially compare its argument with each existing equivalence class. Assuming that the comparison takes constant time , the '.complexity of the algorithm depends on the number k of equivalence classes &lt; p induces : if the Underlying algorithm is O ( f ( n ) ) , the overall comp~lexity is O ( f ( n ) ) x k. Depending on the Preference , k could be a small constant , or itself proportional to f ( n ) , in which case the complexity woul~ be O ( f ( n ) ~ ) . ences As we make more restrictive assumptions about Preferences , more efficient algorithms become possible. Initialily , we assumed only that Preferences specified ! total orders on trees , i.e. , that would take two I trees as input and determine if one was less than , greater than , or equal to the other ~. Given such an unrestricted view of Preferences , ~ve can do no better than producing all interp~-etations ( strings ) and then sorting them. This simple approach is fine if we want all possibilities , especially if we assume that there wo n't , be a large number of them , so that standard n ,2 or n logn sorting algorithms ( see \ [ Aho , Hopcroft , and Ullman , 1983\ ] ) wo n't be much of an addit~ional burden. However , this approach is inefficient if we are interested in only some of the possibilities. Adding the monotonicity restriction 10 permits Algorithm 1 , which is more efficient in. that it postpones the creation of ( successors of ) lower ranked trees. However , we are still opera'ting with a very general view of what Preferencesl are , and further improvements are possible when we look at individual Preferences in detail , in this section , we will consider heuristics for lexical selection , scope , and anaphor resolution. We do not make any claims for the usefullness of these heuristics as such , but take them as concrete 'examples that show the importance of considering the computational properties of Preferences. Note that Algorithm 1 is stated in terms of a single Preference. It is possible to combine multiple Preferences into a single one using Formula 9 , rWe also assume \ [ hat this test takes constant time. and we are currently investigating other methods of combination. Since the algorithms below are highly specialized , they can not be combined with other Preferences using Formula 9. The ultimate goal of this research , however , is to integrate such specialized algorithms with a more sophisticated version of Algorithm 1. One simple preferencing scheme involves assigning integer weights to lexical items and syntactic rules. Items or rules with higher weights are less common and are considered only if lower ranked items fail. When combined with restriction 10 , this weighting scheme yields a Preference &lt; wt that ranks trees according to their lexical and rule weights. Using maz_wt ( T ) to denote the most heavily weighted lexical item or rule used in the construction of T , we have : tl &lt; tot 7t2 ( '~del maz-wt ( tl ) &lt; maz_wt ( t2 ) ( 11 ) The significant property here is that the equivalence classes under &lt; wt can be computed without directly comparing trees. Given a lexical item with weight n , we know that any tree containing it must be in class n or lower. Noting that our algorithm works by generate-and-test ( trees are created and then ranked by equiv_class ) , we can achieve a modest improvement in efficiency by not creating trees with level n lexical items or rules until it is time to enumerate that equivalence class. We can implement this change for both generation and understanding by adding level as a parameter to both initialize_agenda and generate_items , and changing the functions they call to consider only rules and lexical items at or below level. How much of an improvement this yields will depend on how many classes we want to enumerate and how many lexical items and rules there are below the last class enumerated. Scope is another place where we can improve on the basic algorithm. We start by considering scoping during Understanding. Given a sentence s with operators ( quantifiers ) ol ... o , , assigning a scope amounts to determining a total order on ol ... o , s. If a scope Preference can do SNote that this ordering is not a Preference. A Preference will be a total ordering of trees , each of which contains such a scope ordering , i.e. , a scope Preference will be an ordering of orderings of operators. 113 no more than compare and rank pairs of scopings , then the simple generate-and-test algorithm will require O ( n ! ) steps to find the best scoping since it will potentially have to examine every possible ordering. However , the standard heuristics for assigning scope ( e.g. , give `` strong '' quantifiers wide scope , respect left-to-right order in the sentence ) can be used to directly assign the preferred ordering of ox ... ON. If we assume that secondary readings are ranked by how closely they match the preferred scoping , we have a Preference &lt; , c can be defined. In the following ( ol , oj ) 6 Sc ( s ) means that oi preceeds oj in scoping Sc of sentence s , and Scb , ,t ( s ) is the preferred ordering of the operators in s given by the heuristics : Sc , ( s ) &lt; , ~ Se2 ( s ) ~d , ! ( 12 ) Vo , oi \ [ ( o , o9 e Sc ( s ) -- . ( o , ,o9 sc , ( . ) \ ] \ ] Given such a Preference , we can generate the scopings of a sentence more efficiently by first producing the preferred reading ( the first equivalence class ) , then all scopes that have one pair of operators switched ( the second class ) , then all those with two pairs out of order , etc. In the following algorithm , ops is the set of operators in the sentence , and sort is any sorting routine , switched ? is a predicate returning True if its two arguments have already been switched ( i.e. , if its first arg was to the right of its second in Scbe , t ( s ) ) , while switch ( o , , o2 , ord ) is a function that returns new ordering which is the same as ord except that o~ precedes o , in it. { the best scoping } root_set : = sort ( operators , SCbe°~ ( s ) ) ; level : = 1 ; output ( root_set , level ) ; new_set : = Nil ; old_set : = add_item ( root_set , Nil ) ; { loop will execute n ! 1 times } while old_set do begin for ordering in old_set do begin for op in ordering do begin { consider adjacent pairs of operators } next : = right_neighbor ( op , ordering ) ; { switch any pair that has n't already been } if next and not ( switched ? ( op , next ) ) then do begin new_scope : = switch ( op , next , ordering ) ; add_item ( new.scope , new_set ) ; output ( new_scope , level ) ; end end end old_set : = new_set ; new_set : = Nil ; end While the Algorithm 1 would require O ( n ! ) steps to generate the first scoping , this algorithm will output the best scoping in the n 2 or n log n steps that it takes to do the sort ( cf \ [ Aho , Hopcroft , and Ullman , 1983\ ] ) , while each additional scoping is produced in constant time. 9 The algorithm is profligate in that it generates all possible orderings of quantifiers , many of which do not correspond to legal scopings ( see \ [ Hobbs and Shieber , 1987\ ] ) . It can be tightened up by adding a legality test before scope is output. When we move from Understanding to Generation , following Formula 6 , we see that the task is to take an input semantics with scoping Sc and enumerate first all strings that have Sc as their best scoping , then all those with Sc as the second best scoping , etc. Equivalently , we enumerate first strings whose scopings exactly match Sc , then those that match Sc except for one pair of operators , then those matching except for two pairs , etc. We can use the Algorithm 1 to implement this efficiently if we replace each of the two conditional calls to equiv_class. Instead of first computing the equivalence class and then testing whether it is less than level , we call the following function class_less_than : { True iff candidate ranked at level or below } { Target is the desired scoping } function classAess_than ( candidate , target , level ) begin switchAimit : = level ; { global variable } switches : = O ; { global variable } return test_order ( candidate , target , target ) ; end { class_less_than } function test_order ( eand , targ_rest , targ ) begin if null ( cand ) return True ; else 9switched.¢ can be implemented in constant time if we record the position of each operator in the original scoping SCbest. Then switched.¢ ( Ol , 02 ) returns True iff posiaon ( o2 ) &lt; p0siao , ( ol ) . 114 ! begin targ_tail : = member ( first ( cand ) , targ_rest ) ; if targ_tail return test_order ( rest ( cand ) , targ_tail , targ ) ; else begin switches : = switches + 1 ; if &gt; ( switches , switch .</sentence>
				<definiendum id="0">f</definiendum>
				<definiendum id="1">Preference. It</definiendum>
				<definiendum id="2">preferred reading</definiendum>
				<definiendum id="3">sort</definiendum>
				<definiendum id="4">ord )</definiendum>
				<definiens id="0">new_class ( T ) , classes , class_num ) ; return ( class_num ) ; end { equiv_elass } To see that the algorithm enumerates trees in the order given by &lt; p , note that the first iteration outputs trees which are minimal under &lt; p. Now consider any tree t , which is output on a subseqent itertion N. For all other t , , output on that iteration , t , =p t</definiens>
				<definiens id="1">contains a subtree t , ub which was frozen for all levels up to N. Using T ( J ) to denote the set of trees output on iteration J</definiens>
				<definiens id="2">IV ti 6 T ( I ) ti &lt; p t</definiens>
				<definiens id="3">greater than or equal to 112 all trees which were enumerated before it. To calculate the time complexity of the algorithm , note that it calls equiv_class once for each tree created by the underlying understanding or generation algorithm</definiens>
				<definiens id="4">the comparison takes constant time , the '.complexity of the algorithm depends on the number k of equivalence classes &lt; p induces</definiens>
				<definiens id="5">f ( n ) ) x k. Depending on the Preference , k could be a small constant , or itself proportional to f ( n ) , in which case the complexity</definiens>
				<definiens id="6">lexical selection , scope , and anaphor resolution. We do not make any claims for the usefullness of these heuristics as such , but take them as concrete 'examples that show the importance of considering the computational properties of Preferences. Note that Algorithm 1 is stated in terms of a single</definiens>
				<definiens id="7">to integrate such specialized algorithms with a more sophisticated version of Algorithm 1. One simple preferencing scheme involves assigning integer weights to lexical items and syntactic rules. Items or rules with higher weights are less common and are considered only if lower ranked items fail. When combined with restriction 10 , this weighting scheme yields a Preference &lt; wt that ranks trees according to their lexical and rule weights. Using maz_wt ( T ) to denote the most heavily weighted lexical item or rule used in the construction of T</definiens>
				<definiens id="8">'~del maz-wt ( tl ) &lt; maz_wt ( t2 ) ( 11 ) The significant property here is that the equivalence classes under &lt; wt can be computed without directly comparing trees. Given a lexical item with weight n , we know that any tree containing it must be in class n or lower. Noting that our algorithm works by generate-and-test ( trees are created and then ranked by equiv_class ) , we can achieve a modest improvement in efficiency by not creating trees with level n lexical items or rules until it is time to enumerate that equivalence class. We can implement this change for both generation and understanding by adding level as a parameter to both initialize_agenda and generate_items , and changing the functions they call to consider only rules and lexical items at or below level. How much of an improvement this yields will depend on how many classes we want to enumerate and how many lexical items and rules there are below the last class enumerated. Scope is another place where we can improve on the basic algorithm. We start by considering scoping during Understanding. Given a sentence s with operators ( quantifiers ) ol ... o , , assigning a scope amounts to determining a total order on ol ... o</definiens>
				<definiens id="9">a scope Preference will be an ordering of orderings of operators. 113 no more than compare and rank pairs of scopings</definiens>
				<definiens id="10">strong '' quantifiers wide scope , respect left-to-right order in the sentence ) can be used to directly assign the preferred ordering of ox ... ON. If we assume that secondary readings are ranked by how closely they match the preferred scoping</definiens>
				<definiens id="11">s ) means that oi preceeds oj in scoping Sc of sentence s , and Scb , ,t ( s ) is the preferred ordering of the operators in s given by the heuristics : Sc , ( s ) &lt;</definiens>
				<definiens id="12">the first equivalence class ) , then all scopes that have one pair of operators switched ( the second class ) , then all those with two pairs out of order</definiens>
				<definiens id="13">the set of operators in the sentence , and</definiens>
				<definiens id="14">a function that returns new ordering which is the same as ord except that o~ precedes o , in it. { the best scoping } root_set : = sort ( operators , SCbe°~ ( s ) ) ; level : = 1 ; output ( root_set , level ) ; new_set : = Nil ; old_set : = add_item ( root_set , Nil ) ; { loop will execute n ! 1 times } while old_set do begin for ordering in old_set do begin for op in ordering do begin { consider adjacent pairs of operators } next : = right_neighbor ( op , ordering ) ; { switch any pair that has n't already been } if next and not ( switched ? ( op , next ) ) then do begin new_scope : = switch ( op , next , ordering</definiens>
				<definiens id="15">output the best scoping in the n 2 or n log n steps that it takes to do the sort</definiens>
				<definiens id="16">strings whose scopings exactly match Sc , then those that match Sc except for one pair of operators , then those matching except for two pairs</definiens>
				<definiens id="17">of the two conditional calls to equiv_class. Instead of first computing the equivalence class and then testing whether it is less than level</definiens>
			</definition>
			<definition id="3">
				<sentence>limit ) return FalSe ; end else i if ( simple_test ( rest ( cand ) , targ_rest ) return tesLorder ( cand , targ , targ ) ; else return False ; end end { test_order } i function simple~test ( cand_rest , targ_rest ) begin for cand in cand_rest do begin if not ( member ( cand , targ_rest ) ) begin switches : = switches + l ; if &gt; ( switches , switch_limit ) return falsei end end return true ; end { simple_test } To estimate the complexity of class_less_than , note that if no switches are encountered , test_orderwill make one pass through targ_rest ( = targ ) in O ( n ) steps , where n is the length of targ .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">= switches + l ; if &gt; ( switches , switch_limit ) return falsei end end return true</definiens>
				<definiens id="1">the length of targ</definiens>
			</definition>
			<definition id="4">
				<sentence>Under this algorithm , anaphora resolution is entrusted to Experts of three kinds : a Proposer finds likely candidate antecendents , Filters provide a quick way of rejecting many candidates , and Rankers perform more expensive tests to choose among the rest .</sentence>
				<definiendum id="0">Filters</definiendum>
				<definiens id="0">a Proposer finds likely candidate antecendents</definiens>
			</definition>
			<definition id="5">
				<sentence>Recency is a good example of a Proposer ; antecedents are often found in the last couple of sentences , so we should start with the most recent sentences and work back .</sentence>
				<definiendum id="0">Recency</definiendum>
				<definiens id="0">a good example of a Proposer ; antecedents are often found in the last couple of sentences</definiens>
			</definition>
</paper>

		<paper id="0204">
			<definition id="0">
				<sentence>Certain referring functions operate on the lexical items in the model and return sets of things that particular speakers ( in particular contexts ) often refer to by using that particular lexical item .</sentence>
				<definiendum id="0">Certain referring functions</definiendum>
				<definiens id="0">operate on the lexical items in the model and return sets of things that particular speakers ( in particular contexts</definiens>
			</definition>
</paper>

		<paper id="0217">
</paper>

		<paper id="0103">
			<definition id="0">
				<sentence>In van Noord ( 1991 ) I define a head-driven parser ( based on Kay ( 1989 ) ) for a class of constraint-based grammars in which the construction of strings may use more complex operations that simple context-free concatenation .</sentence>
				<definiendum id="0">head-driven parser</definiendum>
				<definiens id="0">a class of constraint-based grammars in which the construction of strings may use more complex operations that simple context-free concatenation</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , this condition is satisfied if the semantics of each element in the subcat list is a proper subpart of the semantics of the entry , and each element of the subcat list is a proper subpart of the word-order domain of the entry .</sentence>
				<definiendum id="0">subcat list</definiendum>
				<definiens id="0">a proper subpart of the semantics of the entry , and each element of the</definiens>
			</definition>
			<definition id="2">
				<sentence>I use the sequence-union predicate ( abbreviated su ) defined by Reape as a possible constraint as well .</sentence>
				<definiendum id="0">sequence-union predicate</definiendum>
				<definiendum id="1">Reape</definiendum>
				<definiens id="0">a possible constraint as well</definiens>
			</definition>
			<definition id="3">
				<sentence>The predicate su ( A , B , C ) is true in case the elements of the list C is the multi set union of the elements of the lists A and B ; moreover , a &lt; b in either A or B iff a &lt; b in C. I also use the notation X U 0 Y to denote the value Seq , where su ( X , Y , $ eq ) .</sentence>
				<definiendum id="0">predicate su</definiendum>
				<definiens id="0">true in case the elements of the list C is the multi set union of the elements of the lists A and B</definiens>
			</definition>
</paper>

		<paper id="0202">
			<definition id="0">
				<sentence>This lexical knowledge consists of Lexical Conceptual Structures ( LCS ) and Linking Rules .</sentence>
				<definiendum id="0">lexical knowledge</definiendum>
			</definition>
			<definition id="1">
				<sentence>Linking Rules describe how LCSs are syntactically encoded in a language and they partition LCSs into classes of predicates that have similar syntactic encodings .</sentence>
				<definiendum id="0">Linking Rules</definiendum>
				<definiens id="0">describe how LCSs are syntactically encoded in a language and they partition LCSs into classes of predicates that have similar syntactic encodings</definiens>
			</definition>
			<definition id="2">
				<sentence>of the Theory Ontology-driven lexical semantics defines the meaning of a lexical unit through a mapping between a word sense and an ontological concept ( or a part thereof ) .</sentence>
				<definiendum id="0">Theory Ontology-driven lexical semantics</definiendum>
				<definiens id="0">defines the meaning of a lexical unit through a mapping between a word sense and an ontological concept ( or a part thereof )</definiens>
			</definition>
			<definition id="3">
				<sentence>2It should be noted that in our application of lexlcal semantics , the I~ltmvsus project , such mappings are allowed to be constrained , not just univocal ( for instance , the English verb taxi maps , in one of its senses , to the concept move-on-surface but only if the theme of this instance is constrained to the concept of airplane or one of its ontological descendents ) -see Onyshkevich and Nirenburg ( 1991 ) for additional detail .</sentence>
				<definiendum id="0">I~ltmvsus</definiendum>
				<definiens id="0">project , such mappings are allowed to be constrained</definiens>
			</definition>
</paper>

		<paper id="0113">
			<definition id="0">
				<sentence>ABSTRACT This paper presents Trace &amp; Unification Grammar ( TUG ) , a declarative and reversible grammar formalism that brings together Unification Grammar ( uG ) and ideas of Government &amp; Binding Theory ( GB ) in an undogmatic way .</sentence>
				<definiendum id="0">TUG</definiendum>
				<definiens id="0">brings together Unification Grammar ( uG ) and ideas of Government &amp; Binding Theory ( GB ) in an undogmatic way</definiens>
			</definition>
			<definition id="1">
				<sentence>Further to these more standard uG-features , TUG provides special rule formats for the description of discontinuous dependencies , so called `` movement rules '' .</sentence>
				<definiendum id="0">TUG</definiendum>
				<definiens id="0">provides special rule formats for the description of discontinuous dependencies , so called `` movement rules ''</definiens>
			</definition>
			<definition id="2">
				<sentence>newly introduced slash rules sl ( S1 ) -- - &gt; \ [ v ( V ) , push ( v ( V ) ) , s_v ( S ) \ ] .</sentence>
				<definiendum id="0">V</definiendum>
			</definition>
			<definition id="3">
				<sentence>Given a rule a ( A ) -- - &gt; b ( B ) &lt; trace ( var , b ( BT ) ) , c ( C ) } we can distinguish two cases : 1 ) The rule is a nonchain rule in the sense of Shieber e.a. ( 1990 ) or it is a chain rule and the antecedent of the trace is the semantic head. In this case the antecedent has to be generated prior to the trace. A typical example is a predicate logic analysis as in : node ( sl ( Sem ) , S , S0 ) -- - &gt; node ( np ( Sem , SemIn ) &lt; trace ( vax0np ( NPSem , NPSem ) ) , S , Sl ) , nods ( s ( SemIn ) , Sl , SO ) .</sentence>
				<definiendum id="0">Sem , SemIn</definiendum>
				<definiens id="0">A ) -- - &gt; b ( B ) &lt; trace ( var , b ( BT ) ) , c ( C ) } we can distinguish two cases : 1</definiens>
			</definition>
</paper>

	</volume>
