<?xml version="1.0" encoding="UTF-8"?>
	<volume id="J01">

		<paper id="1005">
			<definition id="0">
				<sentence>124 Cucchiarelli and Velardi Unsupervised Named Entity Recognition where x = w\ ] or x = Wk and U-PN=wk or wj ( the unknown PN can be either the head or the modifier ) , type i is the syntactic type of esl ( e.g. N-of-N , NAN , V-for-N , etc. ) , and furthermore let : pl ( esli ( x , U_PN ) ) be the plausibility of a detected esl .</sentence>
				<definiendum id="0">pl ( esli</definiendum>
			</definition>
			<definition id="1">
				<sentence>Plausibility is a measure of the statistical evidence of a detected syntactic relation ( Basili , Marziali , and Pazienza 1994 ; Grishman and Sterling 1994 ) that depends upon local ( i.e. , sentence-level ) syntactic ambiguity and global corpus evidence .</sentence>
				<definiendum id="0">Plausibility</definiendum>
				<definiens id="0">a measure of the statistical evidence of a detected syntactic relation</definiens>
			</definition>
			<definition id="2">
				<sentence>Errors and misses occur both during the off-line learning phase ( as we said , NE instances and syntactic contexts economic journals regardless of the language .</sentence>
				<definiendum id="0">NE</definiendum>
				<definiens id="0">instances and syntactic contexts economic journals regardless of the language</definiens>
			</definition>
			<definition id="3">
				<sentence>N means no generalization , only the evidence provided by ESLA is computed ; 0 means that ESLB collects the evidence provided by contexts in which w is a strict synonym of x according to WordNet ; 1 , 2 , and 3 refer to incremental levels of generalization in the ( pruned ) WordNet hierarchy .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">means no generalization</definiens>
			</definition>
</paper>

		<paper id="3004">
			<definition id="0">
				<sentence>Here , a data-classification algorithm flexibly links relational data with elements of a graphical language .</sentence>
				<definiendum id="0">data-classification algorithm flexibly</definiendum>
				<definiens id="0">links relational data with elements of a graphical language</definiens>
			</definition>
			<definition id="1">
				<sentence>FCA is an applied mathematical discipline based on a formal notion of concepts and concept hierarchies and allowing the exploitation of mathematical reasoning for conceptual data analysis and processing .</sentence>
				<definiendum id="0">FCA</definiendum>
				<definiens id="0">an applied mathematical discipline based on a formal notion of concepts and concept hierarchies and allowing the exploitation of mathematical reasoning for conceptual data analysis and processing</definiens>
			</definition>
			<definition id="2">
				<sentence>FCA starts from the notion of a formal context ( G , M , I ) representing a dataset in which G is a set of objects , M is a set of attributes , and I establishes a binary relation between the two sets .</sentence>
				<definiendum id="0">FCA</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">starts from the notion of a formal context ( G , M , I ) representing a dataset in which G is a set of objects</definiens>
				<definiens id="1">a set of attributes</definiens>
			</definition>
			<definition id="3">
				<sentence>The extension is a subset A of the set of objects G and the intension is a subset B of the set of attributes M. We call the pair ( A , B ) a formal concept if each object of the extension has all the properties of the intension .</sentence>
				<definiendum id="0">extension</definiendum>
			</definition>
			<definition id="4">
				<sentence>Formally , a multivalued context is a generalisation of a one-valued context and may be represented as a quadruple ( G , M , W , I ) where G , M , and I are as before , and W represents the set of values of the attributes-412 Bateman , Kamps , Kleinz and Reichenberger Constructive Page Generation Architect Hilberseimer __ Designer Breuer ~ Figure 2 Concept lattice example , more succinctly labeled .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">a generalisation of a one-valued context and may be represented as a quadruple ( G , M , W , I ) where G , M</definiens>
			</definition>
			<definition id="5">
				<sentence>Here , the extension label for each node consists of just those elements which are added at that node moving up the lattice ; conversely the members of the intensions are shown moving down the lattice , again adding just those elements that are new for that node .</sentence>
				<definiendum id="0">extension label for each node</definiendum>
				<definiens id="0">consists of just those elements which are added at that node moving up the lattice ; conversely the members of the intensions are shown moving down the lattice , again adding just those elements that are new for that node</definiens>
			</definition>
			<definition id="6">
				<sentence>To identify the value w c W of attribute m E M for an object g E G , we adopt the notation rn ( g ) = w and read this as attribute m of object g has value w. Kamps ( 1997 ) renders multivalued contexts amenable to the techniques for dependency-lattice construction by deriving a one-valued context that captures the functional dependencies of the original multivalued context .</sentence>
				<definiendum id="0">notation rn</definiendum>
			</definition>
			<definition id="7">
				<sentence>Macrotypography is a central component of professional document design ; indeed , Every designer knows that how elements are put together on a page communicates a powerful message ( Adobe Inc. , InDesign product information sheet ) .</sentence>
				<definiendum id="0">Macrotypography</definiendum>
				<definiens id="0">a central component of professional document design ; indeed , Every designer knows that how elements are put together on a page communicates a powerful message ( Adobe Inc. , InDesign product information sheet )</definiens>
			</definition>
			<definition id="8">
				<sentence>Originally , RST sought to describe the recursive structure of any text in terms of rhetorical relations which hold between the segments ( called spans ) of the text .</sentence>
				<definiendum id="0">RST</definiendum>
				<definiens id="0">sought to describe the recursive structure of any text in terms of rhetorical relations which hold between the segments ( called spans ) of the text</definiens>
			</definition>
			<definition id="9">
				<sentence>\ [ 12\ ] Unihoc allows many alternatives in how it is played .</sentence>
				<definiendum id="0">Unihoc</definiendum>
				<definiens id="0">allows many alternatives in how it is played</definiens>
			</definition>
			<definition id="10">
				<sentence>\ [ 18\ ] Each receives a clear function , which determines their effective playing area .</sentence>
				<definiendum id="0">clear function</definiendum>
				<definiens id="0">determines their effective playing area</definiens>
			</definition>
			<definition id="11">
				<sentence>Presentation plans therefore begin their life as partially specified generic structures , defined as incomplete rhetorical structures .</sentence>
				<definiendum id="0">Presentation plans</definiendum>
				<definiens id="0">therefore begin their life as partially specified generic structures</definiens>
				<definiens id="1">incomplete rhetorical structures</definiens>
			</definition>
			<definition id="12">
				<sentence>KOMET ( Knowledge-oriented production of multimodal documents ) and PAVE ( Publication and advanced visualization environments ) were two departments of the German National Research Center for Information Technology 's ( GMD ) institute for Integrated Publication and Information Systems ( IPSI ) in Darmstadt that cooperated closely on this work .</sentence>
				<definiendum id="0">KOMET</definiendum>
				<definiens id="0">Knowledge-oriented production of multimodal documents ) and PAVE ( Publication and advanced visualization environments ) were two departments of the German National Research Center for Information Technology 's ( GMD ) institute for Integrated Publication and Information Systems ( IPSI ) in Darmstadt that cooperated closely on this work</definiens>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>We then provide an example from the MUC-6 development corpus that illustrates properties of the alignment process that interfere with the stated goals of the evaluation .</sentence>
				<definiendum id="0">MUC-6 development corpus</definiendum>
				<definiens id="0">illustrates properties of the alignment process that interfere with the stated goals of the evaluation</definiens>
			</definition>
			<definition id="1">
				<sentence>IE systems process streams of natural language input and produce representations of the information relevant to a particular task , typically in the form of database templates .</sentence>
				<definiendum id="0">IE</definiendum>
				<definiens id="0">systems process streams of natural language input and produce representations of the information relevant to a particular task</definiens>
			</definition>
			<definition id="2">
				<sentence>Precision is the number of correct fills divided by the total number generated by the system , and recall is the number of correct fills divided by the the total number in the key : COR PRE ACT COR REC POS In the example , there are 8 correct fills , 13 generated fills , and 15 possible fills in the key , resulting in a precision of 0.615 and a recall of 0.533 .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the number of correct fills divided by the total number generated by the system</definiens>
			</definition>
</paper>

		<paper id="3002">
			<definition id="0">
				<sentence>Here , Ni is the number of distinct /-grams and Si is the sum of their frequencies .</sentence>
				<definiendum id="0">Ni</definiendum>
				<definiendum id="1">Si</definiendum>
			</definition>
			<definition id="1">
				<sentence>The model can be summarized as follows : ( s~ C ( w~_2 , wi_l , wi ) if C ( wi_2 , Wi_l , wi ) ~ &gt; 0 p ( wilwi_2 , wi_l ) = INB+S3 C ( w , _ , ,w , ) ( 4 ) \ [ N3~3P ( wi \ [ Wi-1 ) otherwise ( s2 C ( wi_l , wi ) P ( Wi \ ] Wi -- 1 ) ~~ N2+S2 ~ if C ( wi-1 , wi ) &gt; 0 N2 P ( wi ) ( 5 ) / ~ otherwise ( c ( ~ ) P ( wi ) = ~Nkts ~ if C ( wi ) &gt; 0 ( 6 ) \ [ ~P~ ( wi ) otherwise ki r ( # ) I-I r ( wi~'\ ] ) PE ( wi ) = j=l 1 r ( # ) ( 7 ) where C 0 denotes the count or frequency function , ki denotes the length of word wi , excluding the sentinel character # , wi\ [ j\ ] denotes its jth phoneme , and r 0 denotes the relative frequency function .</sentence>
				<definiendum id="0">C 0</definiendum>
				<definiens id="0">s~ C ( w~_2 , wi_l , wi ) if C ( wi_2 , Wi_l , wi ) ~ &gt; 0 p</definiens>
			</definition>
			<definition id="2">
				<sentence>The learning algorithm considers each utterance in turn and computes the most probable segmentation of the utterance using a Viterbi search ( Viterbi 1967 ) implemented as a dynamic programming algorithm , as described in Section 4.2 .</sentence>
				<definiendum id="0">learning algorithm</definiendum>
				<definiens id="0">considers each utterance in turn and computes the most probable segmentation of the utterance using a Viterbi search</definiens>
			</definition>
			<definition id="3">
				<sentence>If seg ( x ) represents the best segmentation of the utterance x and 356 Venkataraman Word Discovery in Transcribed Speech word ( x ) denotes that x is treated as a word , then ~ word ( abcde ) seg ( a ) + word ( bcde ) seg ( abcde ) = best of seg ( ab ) + word ( cde ) seg ( abc ) + word ( de ) seg ( abcd ) + word ( e ) The evalUtterance algorithm in Figure 1 does precisely this .</sentence>
				<definiendum id="0">seg ( x )</definiendum>
				<definiens id="0">represents the best segmentation of the utterance x and 356 Venkataraman Word Discovery in Transcribed Speech word ( x ) denotes that x is treated as a word</definiens>
			</definition>
			<definition id="4">
				<sentence>Precision is defined as the proportion of predicted words that are actually correct .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the proportion of predicted words that are actually correct</definiens>
			</definition>
			<definition id="5">
				<sentence>Recall is defined as the proportion of correct words that were predicted .</sentence>
				<definiendum id="0">Recall</definiendum>
			</definition>
			<definition id="6">
				<sentence>Lexicon precision is defined as the proportion of words in the predicted lexicon that are correct .</sentence>
				<definiendum id="0">Lexicon precision</definiendum>
				<definiens id="0">the proportion of words in the predicted lexicon that are correct</definiens>
			</definition>
			<definition id="7">
				<sentence>Precision is defined as the percentage of identified words that are correct , as measured against the target data .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the percentage of identified words that are correct , as measured against the target data</definiens>
			</definition>
			<definition id="8">
				<sentence>More often than not , however , when the 3-gram model does not have enough evidence to infer words , it simply outputs the default segmentation , which is a single word ( the entire utterance ) instead of more than one incorrectly inferred one .</sentence>
				<definiendum id="0">segmentation</definiendum>
				<definiens id="0">a single word ( the entire utterance ) instead of more than one incorrectly inferred one</definiens>
			</definition>
			<definition id="9">
				<sentence>P ( a J X ) C ( a IX ) ~ , C ( ai I X ) E~x~x ( N ) IxqJx ( N ) where , as before , C ( a I X ) is the count function that gives the frequency of phoneme a in X. If ~x is deterministic , we can then write P ( a I X ) = Eax ( 10 ) Ix Our experiments suggest that EaL ~ Eac and that 1L ~ lC .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the count function that gives the frequency of phoneme a in X. If ~x is deterministic</definiens>
			</definition>
			<definition id="10">
				<sentence>The relative probability of a familiar word is given in Equation 22 of Brent ( 1999 ) as 2 368 Venkataraman Word Discovery in Transcribed Speech where k is the total number of words and fk ( k ) is the frequency at that point in segmentation of the kth word .</sentence>
				<definiendum id="0">k</definiendum>
				<definiendum id="1">k )</definiendum>
			</definition>
			<definition id="11">
				<sentence>Having no evidence to infer otherwise , both programs assume that damnbritish is a single word and update their lexicons accordingly .</sentence>
				<definiendum id="0">damnbritish</definiendum>
				<definiens id="0">a single word and update their lexicons accordingly</definiens>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>First , the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string , which allows it to calculate a generative probability for * Department of Cognitive and Linguistic Sciences , Box 1978 , Brown University , Providence , RI 02912 ( ~ ) 2001 Association for Computational Linguistics Computational Linguistics Volume 27 , Number 2 each prefix string from the probabilistic grammar , and hence a conditional probability for each word given the previous words and the probabilistic grammar .</sentence>
				<definiendum id="0">top-down parsing algorithm</definiendum>
				<definiens id="0">builds a set of rooted candidate parse trees from left to right over the string , which allows it to calculate a generative probability for * Department of Cognitive and Linguistic Sciences , Box 1978</definiens>
				<definiens id="1">prefix string from the probabilistic grammar , and hence a conditional probability for each word given the previous words and the probabilistic grammar</definiens>
			</definition>
			<definition id="1">
				<sentence>PCFGs model the syntactic combinatorics of a language by extending conventional context-free grammars ( CFGs ) .</sentence>
				<definiendum id="0">PCFGs</definiendum>
				<definiens id="0">model the syntactic combinatorics of a language by extending conventional context-free grammars ( CFGs )</definiens>
			</definition>
			<definition id="2">
				<sentence>A CFG G = ( V , T , P , St ) , consists of a set of nonterminal symbols V , a set of terminal symbols T , a start symbol St E V , and a set of rule productions P of the form : A ~ a , where a c ( V U T ) * .</sentence>
				<definiendum id="0">CFG G</definiendum>
				<definiens id="0">consists of a set of nonterminal symbols V , a set of terminal symbols T , a start symbol St E V , and a set of rule productions P of the form : A ~ a , where a c ( V U T ) *</definiens>
			</definition>
			<definition id="3">
				<sentence>Consider , for example , the parse tree shown in ( a ) in Figure 1 : the start symbol is St , which expands into an S. The S node expands into an NP followed by a VP .</sentence>
				<definiendum id="0">St</definiendum>
				<definiens id="0">expands into an S. The S node expands into an NP followed by a VP</definiens>
			</definition>
			<definition id="4">
				<sentence>A CFG G defines a language Lc , which is a subset of the set of strings of terminal symbols , including only those that are leaves of complete trees rooted at St , built with rules from the grammar G. We will denote strings either as w or as WoW1 ... wn , where wn is understood to be the last terminal symbol in the string .</sentence>
				<definiendum id="0">CFG G</definiendum>
				<definiendum id="1">wn</definiendum>
				<definiens id="0">defines a language Lc , which is a subset of the set of strings of terminal symbols , including only those that are leaves of complete trees rooted at St</definiens>
			</definition>
			<definition id="5">
				<sentence>A PCFG is a CFG with a probability assigned to each rule ; specifically , each righthand side has a probability given the left-hand side of the rule .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">a CFG with a probability assigned to each rule</definiens>
			</definition>
			<definition id="6">
				<sentence>Provided a PCFG is consistent ( or tight ) , which it always will be in the approach we will be advocating , this defines a proper probability distribution over completed trees .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">consistent ( or tight ) , which it always will be in the</definiens>
			</definition>
			<definition id="7">
				<sentence>3 A PCFG also defines a probability distribution over strings of words ( terminals ) in the following way : P ( w~ ) = ~ P ( t ) ( 1 ) tETw~ The intuition behind Equation 1 is that , if a string is generated by the PCFG , then it will be produced if and only if one of the trees in the set Twg generated it .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">a probability distribution over strings of words ( terminals ) in the following way : P ( w~ ) = ~</definiens>
			</definition>
			<definition id="8">
				<sentence>We will adopt the convention that an explicit beginning of string symbol , ( s/ , and an explicit end symbol , &lt; /s ) , are part of the vocabulary , and a string wg is a complete string if and only if w 0 is ( s ) and wn is ( /s ) . Since the beginning of string symbol is not predicted by language models , but rather is axiomatic in the same way that S ~f is for a parser , we can safely omit it from the current discussion , and simply assume that it is there. See Figure l ( b ) for the explicit representation. While a complete string of words must contain the end symbol as its final word , a string prefix does not have this restriction. For example , `` Spot chased the ball ( /s ) '' is a complete string , and the following is the set of prefix strings of this complete string : `` Spot '' ; `` Spot chased '' ; `` Spot chased the '' ; `` Spot chased the ball '' ; and `` Spot chased the ball ( /s &gt; ' .</sentence>
				<definiendum id="0">string wg</definiendum>
				<definiens id="0">adopt the convention that an explicit beginning of string symbol , ( s/ , and an explicit end symbol</definiens>
				<definiens id="1">a complete string if and only if w 0 is ( s ) and wn is ( /s ) . Since the beginning of string symbol is not predicted by language models , but rather is axiomatic in the same way that S ~f is for a parser , we can safely omit it from the current discussion , and simply assume that it is there. See Figure l ( b ) for the explicit representation. While a complete string of words must contain the end symbol as its final word , a string prefix does not have this restriction. For example , `` Spot chased the ball ( /s ) '' is a complete string</definiens>
			</definition>
			<definition id="9">
				<sentence>A partial derivation ( or parse ) d is defined with respect to a prefix string w~ as follows : it is the leftmost derivation of the string , with wj on the right-hand side of the last expansion in the derivation ) Let Dw0J be the set of all partial derivations for a prefix string w0 J. Then P ( wg ) = ~ P ( d ) ( 2 ) dCDwJ ° We left-factor the PCFG , so that all productions are binary , except those with a single terminal on the right-hand side and epsilon productions , s We do this because it delays predictions about what nonterminals we expect later in the string until we have seen more of the string .</sentence>
				<definiendum id="0">partial derivation</definiendum>
				<definiens id="0">a prefix string w~ as follows : it is the leftmost derivation of the string , with wj on the right-hand side of the last expansion in the derivation</definiens>
			</definition>
			<definition id="10">
				<sentence>For a grammar G , we define a factored grammar Gf as follows : i. ( A -- * B A-B ) E Gf iff ( A ~ Bfl ) E G , s.t. B E V and fl E V* ii .</sentence>
				<definiendum id="0">factored grammar Gf</definiendum>
				<definiens id="0">A -- * B A-B ) E Gf iff ( A ~ Bfl ) E G , s.t. B E V and fl E V* ii</definiens>
			</definition>
			<definition id="11">
				<sentence>( A-a ~ B A-aB ) E Gf iff ( A ~ aBfl ) E G , s.t. B E V , a E V + , andflEV* iii .</sentence>
				<definiendum id="0">B A-aB ) E Gf iff</definiendum>
				<definiens id="0">A ~ aBfl ) E G , s.t. B E V , a E V + , andflEV* iii</definiens>
			</definition>
			<definition id="12">
				<sentence>For an interpolated ( n + 1 ) -gram : P ( wi\ ] i-1 i-1 A i-1 i-1 wi_ ) + n ( wi_ ) ) P ( wi I /~n ( Wi_n ) P ( wi I ( 1 i-1 Wi_n ) = ( 6 ) Wi-n+l ) Here 13 is the empirically observed relative frequency , and /~n is a function from V n to \ [ 0,1\ ] .</sentence>
				<definiendum id="0">/~n</definiendum>
			</definition>
			<definition id="13">
				<sentence>As mentioned in Section 2.1 , a PCFG defines a probability distribution over strings of words .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">a probability distribution over strings of words</definiens>
			</definition>
			<definition id="14">
				<sentence>If the left-hand side has no parent ( i.e. , it is at the root of the tree ) , the function returns the null value ( NULL ) .</sentence>
				<definiendum id="0">NULL</definiendum>
				<definiens id="0">at the root of the tree ) , the function returns the null value</definiens>
			</definition>
			<definition id="15">
				<sentence>Then a subsequent function could be defined as follows : return the parent of the parent ( the grandparent ) of constituent ( A ) only if constituent ( A ) has no sibling to the left -- in other words , if the previous function returns NULL ; otherwise return the second closest sibling to the left of constLtuent ( A ) , or , as always , NULL if no such node exists .</sentence>
				<definiendum id="0">NULL</definiendum>
				<definiens id="0">if no such node exists</definiens>
			</definition>
			<definition id="16">
				<sentence>For example , ( 4,3,2 ) would represent a conditional probability model that ( i ) returns NULL for all functions below level 4 in all contexts ; ( ii ) returns NULL for all functions below level 3 if the left-hand side is a POS ; and ( iii ) returns NULL for all functions below level 2 for nonleftmost POS expansions .</sentence>
				<definiendum id="0">side</definiendum>
				<definiens id="0">represent a conditional probability model that ( i ) returns NULL for all functions below level 4 in all contexts ; ( ii ) returns NULL for all functions below level 3 if the left-hand</definiens>
			</definition>
			<definition id="17">
				<sentence>The interpolation coefficients are a function of the frequency of the set of conditioning events , and are estimated by iteratively adjusting the coefficients so as to maximize the likelihood of a held-out corpus .</sentence>
				<definiendum id="0">interpolation coefficients</definiendum>
				<definiens id="0">a function of the frequency of the set of conditioning events , and are estimated by iteratively adjusting the coefficients so as to maximize the likelihood of a held-out corpus</definiens>
			</definition>
			<definition id="18">
				<sentence>The LAP is the probability of a particular terminal being the next left-corner of a particular analysis .</sentence>
				<definiendum id="0">LAP</definiendum>
				<definiens id="0">the probability of a particular terminal being the next left-corner of a particular analysis</definiens>
			</definition>
			<definition id="19">
				<sentence>Perplexity is a standard measure within the speech recognition community for comparing language models .</sentence>
				<definiendum id="0">Perplexity</definiendum>
				<definiens id="0">a standard measure within the speech recognition community for comparing language models</definiens>
			</definition>
			<definition id="20">
				<sentence>Given a random variable X with distribution p and a probability model q , the cross entropy , H ( p , q ) is defined as follows : H ( p , q ) = ~ p ( x ) log q ( x ) ( 12 ) xcX Let p be the true distribution of the language .</sentence>
				<definiendum id="0">probability model</definiendum>
				<definiendum id="1">cross entropy</definiendum>
				<definiens id="0">follows : H ( p , q ) = ~ p ( x ) log q ( x ) ( 12 ) xcX Let p be the true distribution of the language</definiens>
			</definition>
			<definition id="21">
				<sentence>14 That is H ( p , q ) = lira 1_ logq ( w~ ) ( 13 ) r/ -- + Oo n where w~ is a string of the language L. In practice , one takes a large sample of the language , and calculates the negative log probability of the sample , normalized by its size .</sentence>
				<definiendum id="0">w~</definiendum>
			</definition>
			<definition id="22">
				<sentence>Word error rate is the number of deletion , insertion , or substitution errors per 100 words .</sentence>
				<definiendum id="0">Word error rate</definiendum>
			</definition>
			<definition id="23">
				<sentence>Precision is the number of common constituents in GOLD and TEST divided by the number of constituents in TEST .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the number of common constituents in GOLD and TEST divided by the number of constituents in TEST</definiens>
			</definition>
			<definition id="24">
				<sentence>Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD</definiens>
			</definition>
			<definition id="25">
				<sentence>Since , /4init~i+l is produced by expanding only analyses on priority queue HI '~it , the set of complete trees consistent with the partial derivations on priority queue ~4i , ,~t is a subset of the `` i+1 set of complete trees consistent with the partial derivations on priority queue H~ nit , that is , the total probability mass represented by the priority queues is monotonically decreasing .</sentence>
				<definiendum id="0">,~t</definiendum>
				<definiens id="0">a subset of the `` i+1 set of complete trees consistent with the partial derivations on priority queue H~ nit</definiens>
			</definition>
			<definition id="26">
				<sentence>The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal , a total of 3,446 words .</sentence>
				<definiendum id="0">DARPA '93 HUB1 test setup</definiendum>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>Term frequency ( tf ) is the standard notion of frequency in corpus-based natural language processing ( NLP ) ; it counts the number of times that a type ( term/word/ngram ) appears in a corpus .</sentence>
				<definiendum id="0">Term frequency</definiendum>
				<definiendum id="1">tf )</definiendum>
				<definiens id="0">the standard notion of frequency in corpus-based natural language processing ( NLP ) ; it counts the number of times that a type</definiens>
			</definition>
			<definition id="1">
				<sentence>Term frequency is an integer between 0 and N ; document frequency is an integer between 0 and D , the number of documents in the corpus .</sentence>
				<definiendum id="0">Term frequency</definiendum>
				<definiendum id="1">document frequency</definiendum>
				<definiens id="0">an integer between 0</definiens>
			</definition>
			<definition id="2">
				<sentence>The statistics , tf and df , and functions of these statistics such as mutual information ( MI ) and inverse document frequency ( IDF ) , are usually computed over short n-grams such as unigrams , bigrams , and trigrams ( substrings of 1-3 tokens ) ( Charniak 1993 ; Jelinek 1997 ) .</sentence>
				<definiendum id="0">inverse document frequency</definiendum>
				<definiendum id="1">IDF</definiendum>
			</definition>
			<definition id="3">
				<sentence>In information retrieval , document frequencies are converted into inverse document frequency ( IDF ) , which plays an important role in term weighting ( Sparck Jones 1972 ) .</sentence>
				<definiendum id="0">IDF</definiendum>
			</definition>
			<definition id="4">
				<sentence>MI tends to pick out phrases with noncompositional semantics ( which often violate the independence assumption ) whereas RIDF tends to highlight technical terminology , names , and good keywords for information retrieval ( which tend to exhibit nonrandom distributions over documents ) .</sentence>
				<definiendum id="0">MI</definiendum>
				<definiens id="0">tends to pick out phrases with noncompositional semantics ( which often violate the independence assumption ) whereas RIDF tends to highlight technical terminology , names , and good keywords for information retrieval ( which tend to exhibit nonrandom distributions over documents</definiens>
			</definition>
			<definition id="5">
				<sentence>where 0 = np , n is the average length of a document , and p is the occurrence probability of the term .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
			<definition id="6">
				<sentence>Class ( ( i , j ) ) is the set of substrings that start every suffix within the interval and no suffix outside the interval .</sentence>
				<definiendum id="0">Class ( ( i</definiendum>
				<definiens id="0">the set of substrings that start every suffix within the interval and no suffix outside the interval</definiens>
			</definition>
			<definition id="7">
				<sentence>Figure 3 Lcp vector icp\ [ O O '' icp \ [ i 3 icp \ [ 2 I icp \ [ 3 I Icp \ [ 4 I icp\ [ 5 0 icp\ [ 6\ ] 2 icp\ [ 7\ ] 0 icp\ [ 8\ ] I Icp\ [ 9\ ] 0 icp\ [ lO\ ] 0 lcp \ [ 12\ ] 1 Icp\ [ 13\ ] 1 icp \ [ 14\ ] 0 icp\ [ 15\ ] 0 icp\ [ 16\ ] I icp\ [ 17\ ] 5 icp\ [ 18\ ] O-always 0 always 0 The longest common prefix is a vector of N + 1 integers , lcp\ [ i\ ] denotes the length of the common prefix between the suffix s\ [ i 1\ ] and the suffix s\ [ i\ ] .</sentence>
				<definiendum id="0">] O-always</definiendum>
			</definition>
			<definition id="8">
				<sentence>above can be rewritten as class ( ( i , jl ) = { s\ [ i\ ] mlLBL ( ( i , jl ) &lt; m &lt; _ SIL ( ( i , jl ) } , where LBL ( longest bounding lcp ) is LBL ( ( i , j ) ) = max ( lcp\ [ i\ ] , lcp~ + 1\ ] ) , and SIL ( shortest interior lcp ) is sIn ( ( i , j ) ) = min ( lcp\ [ i + 1\ ] , Icp\ [ i + 2\ ] ... .. lcp~\ ] ) .</sentence>
				<definiendum id="0">SIL ( shortest interior lcp</definiendum>
				<definiens id="0">jl ) = { s\ [ i\ ] mlLBL ( ( i , jl ) &lt; m &lt; _ SIL ( ( i , jl ) } , where LBL ( longest bounding lcp ) is LBL ( ( i , j ) ) = max ( lcp\ [ i\ ] , lcp~ + 1\ ] )</definiens>
			</definition>
			<definition id="9">
				<sentence>The class array is a stored list of five-tuples : ( SIL , LBL , tf , df , longest suffix I. The fifth element of the five-tuple is a canonical member of the class ( the longest suffix ) .</sentence>
				<definiendum id="0">class array</definiendum>
				<definiens id="0">a stored list of five-tuples : ( SIL , LBL , tf , df , longest suffix I. The fifth element of the five-tuple is a canonical member of the class ( the longest suffix )</definiens>
			</definition>
			<definition id="10">
				<sentence>The English collection consists of 50 million words ( 113 thousand articles ) of the Wall Street Journal ( distributed by the ACL/DCI ) and the Japanese collection consists of 216 million characters ( 436 thousand articles ) of the CD-Mainichi Shimbun from 19911995 ( which are distributed in CD-ROM format ) .</sentence>
				<definiendum id="0">English collection</definiendum>
				<definiens id="0">consists of 50 million words ( 113 thousand articles ) of the Wall Street Journal ( distributed by the ACL/DCI ) and the Japanese collection consists of 216 million characters ( 436 thousand articles ) of the CD-Mainichi Shimbun from 19911995 ( which are distributed in CD-ROM format )</definiens>
			</definition>
			<definition id="11">
				<sentence>When Y is the empty string , tf ( Y ) -N. Figure 9 plots RIDF and MI values of 5,000 substrings randomly selected as a function of string length .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">the empty string</definiens>
			</definition>
			<definition id="12">
				<sentence>Panel ( a ) compares RIDF and MI for a sample of English word sequences from the WSJ corpus ( excluding unigrams ) ; panel ( b ) makes the same comparison but for Japanese phrases identified as keywords on the CD-ROM .</sentence>
				<definiendum id="0">Panel</definiendum>
				<definiens id="0">a ) compares RIDF and MI for a sample of English word sequences from the WSJ corpus ( excluding unigrams ) ; panel ( b ) makes the same comparison but for Japanese phrases identified as keywords on the CD-ROM</definiens>
			</definition>
			<definition id="13">
				<sentence>( A ) : Low RIDF ( poor keywords ) tf df RIDF MI Phase 11 11 -0.0 11.1 the up side 73 66 0.1 9.3 the will of 16 16 -0.0 8.6 the sell side 17 16 0.1 8.5 the Stock Exchange of 16 15 0.1 8.5 the buy side 20 20 -0.0 8.4 the down side 55 54 0.0 8.3 the will to 14 14 -0.0 8.1 the saying goes 15 15 -0.0 7.6 the going gets ( B ) : High RIDF ( better keywords ) tf df RIDF MI Phase 37 3 3.6 2.3 the joint commission 66 8 3.0 3.6 the SSC 55 7 3.0 2.0 The Delaware &amp; 37 5 2.9 3.6 the NHS 22 3 2.9 3.4 the kibbutz 22 3 2.9 4.1 the NSA 's 29 4 2.9 4.2 the DeBartolos 36 5 2.8 2.3 the Basic Law 21 3 2.8 2.3 the national output ( words or phrases ) than substrings that are high in just one or the other .</sentence>
				<definiendum id="0">Low RIDF</definiendum>
				<definiendum id="1">RIDF</definiendum>
			</definition>
			<definition id="14">
				<sentence>We have observed previously that MI is high for general vocabulary ( words found in dictionary ) and RIDF is high for names , technical terminology , and good keywords for information retrieval .</sentence>
				<definiendum id="0">RIDF</definiendum>
				<definiens id="0">high for general vocabulary ( words found in dictionary )</definiens>
			</definition>
			<definition id="15">
				<sentence>We presented algorithms ( and C code ) for computing term frequency ( tf ) and document frequency ( dr ) for all n-grams ( substrings ) in a corpus ( sequence ) .</sentence>
				<definiendum id="0">document frequency</definiendum>
				<definiens id="0">substrings ) in a corpus</definiens>
			</definition>
			<definition id="16">
				<sentence>RIDF looks for n-grams whose distributions pick out a relatively small number documents , unlike a random ( Poisson ) distribution .</sentence>
				<definiendum id="0">RIDF</definiendum>
				<definiens id="0">looks for n-grams whose distributions pick out a relatively small number documents , unlike a random ( Poisson ) distribution</definiens>
			</definition>
			<definition id="17">
				<sentence>MI tends to pick out general vocabulary -- words and phrases that appear in dictionaries .</sentence>
				<definiendum id="0">MI</definiendum>
				<definiens id="0">tends to pick out general vocabulary -- words and phrases that appear in dictionaries</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>Boas ( Nirenburg 1998 ; Nirenburg and Raskin 1998 ) is a semiautomatic knowledge elicitation system that guides a team of two people ( a language informant and a programmer ) through the process of developing the static knowledge sources required to produce a moderate-quality , broad-coverage MT system from any `` low-density '' language into English .</sentence>
				<definiendum id="0">Boas</definiendum>
				<definiens id="0">a semiautomatic knowledge elicitation system that guides a team of two people ( a language informant and a programmer ) through the process of developing the static knowledge sources required to produce a moderate-quality , broad-coverage MT system from any `` low-density '' language into English</definiens>
			</definition>
			<definition id="1">
				<sentence>The box in Figure 1 labeled Morphological Analyzer Generation is the main component , which takes in the elicited information and generates a series of regular expressions for describing the morphological lexicon and morphographemic rules .</sentence>
				<definiendum id="0">Morphological Analyzer Generation</definiendum>
				<definiens id="0">takes in the elicited information and generates a series of regular expressions for describing the morphological lexicon and morphographemic rules</definiens>
			</definition>
			<definition id="2">
				<sentence>The analyzer consists of the union of transducers , each of which implements the morphological analysis process for one paradigm .</sentence>
				<definiendum id="0">analyzer</definiendum>
				<definiens id="0">consists of the union of transducers , each of which implements the morphological analysis process for one paradigm</definiens>
			</definition>
			<definition id="3">
				<sentence>The bottom component is an ordered sequence of morphographemic rules that are learned via transformation-based learning from the sample inflectional paradigms provided by the human informant .</sentence>
				<definiendum id="0">bottom component</definiendum>
				<definiens id="0">an ordered sequence of morphographemic rules that are learned via transformation-based learning from the sample inflectional paradigms provided by the human informant</definiens>
			</definition>
			<definition id="4">
				<sentence>The morphological analyzer is a finite-state transducer that is actually the union of the transducers for each paradigm definition in the description provided .</sentence>
				<definiendum id="0">morphological analyzer</definiendum>
				<definiens id="0">a finite-state transducer that is actually the union of the transducers for each paradigm definition in the description provided</definiens>
			</definition>
			<definition id="5">
				<sentence>Let Sk = ( cl , c2 ... .. Ckl , 1 &lt; k &lt; _ c be a ( string ) prefix of C length k. We assume that the stem onto which morphological affixes are attached is Sk for some k. 11 The set of inflectional forms given in the primary J J , fill ( f//are alphabet example are { F1 , F2 , ... , El } , with each Fj = ~f~ , f~ ... . symbols in the of the language and lj is the length of the jth form ) .</sentence>
				<definiendum id="0">lj</definiendum>
				<definiens id="0">assume that the stem onto which morphological affixes are attached is Sk for some k. 11 The set of inflectional forms given in the primary J J</definiens>
			</definition>
			<definition id="6">
				<sentence>The Sk with the minimum d ( Sk ) is then chosen as the stem S. Creating segmentations based on stem S proceeds as follows : To determine the affixes in each inflected form Fj = ~f~ , f~ ... .. f/i/ , we compute the projection of the stem Pj = ~f~ ... . , f/el in Fj , as that 10 Note that other finite state tools could also be used ( e.g. , Mohri , Pereira and Riley 1998 ; van Noord 1999 ) .</sentence>
				<definiendum id="0">Sk</definiendum>
				<definiens id="0">the stem S. Creating segmentations based on stem S proceeds as follows : To determine the affixes in each inflected form Fj = ~f~</definiens>
			</definition>
			<definition id="7">
				<sentence>70 Oflazer , Nirenburg , and McShane Bootstrapping Morphological Analyzers Segmented Forms I Figure 4 l ( incrementally ) transformed segmented forms Learner Surface forms ( Truth ) Transformation-based learning of morphographemic rules .</sentence>
				<definiendum id="0">McShane Bootstrapping Morphological Analyzers</definiendum>
				<definiens id="0">segmented forms Learner Surface forms ( Truth ) Transformation-based learning of morphographemic rules</definiens>
			</definition>
			<definition id="8">
				<sentence>From each segmented pair we generate rewrite rules of the sort is u- &gt; i \ [ \ ] LeftContext , RightContext ; where u ( pper ) is a symbol in the segmented form , l ( ower ) is a symbol in the surface form .</sentence>
				<definiendum id="0">u</definiendum>
				<definiens id="0">a symbol in the segmented form</definiens>
				<definiens id="1">a symbol in the surface form</definiens>
			</definition>
			<definition id="9">
				<sentence>All segmentation rules go to the bottom of the list , though within this group , rules are still ranked based on decreasing promise and context generality .</sentence>
				<definiendum id="0">segmentation rules</definiendum>
				<definiens id="0">still ranked based on decreasing promise and context generality</definiens>
			</definition>
			<definition id="10">
				<sentence>The complete procedure for rule learning can now be given as follows : Align surface and segmented forms in the example base ; Compute total Error ; while ( Error &gt; O ) { -Generate all possible rewrite rules subject to context size limits ; -Rank Rules ; -while ( there are more rules and a rule has not yet been selected ) { Tentatively apply the next rule to all the segmented forms ; Re-align the resulting segmented forms with the corresponding surface forms to see how many ''errors '' have been fixed ; If the number of errors fixed is equal to what the rule promised to fix AND the result does not have generation ambiguity , select this rule ; } -Commit the changes performed by the rule on the segmented forms to the example base ; -Reduce Error by the promise score of the selected rule ; This procedure eventually generates an ordered sequence of two ordered groups of rewrite rules .</sentence>
				<definiendum id="0">-Rank Rules</definiendum>
				<definiens id="0">given as follows : Align surface and segmented forms in the example base ; Compute total Error</definiens>
				<definiens id="1">yet been selected ) { Tentatively apply the next rule to all the segmented forms</definiens>
			</definition>
</paper>

		<paper id="4003">
			<definition id="0">
				<sentence>The Cf set for an utterance /do is the set of discourse entities evoked by that utterance .</sentence>
				<definiendum id="0">Cf set for an utterance /do</definiendum>
				<definiens id="0">the set of discourse entities evoked by that utterance</definiens>
			</definition>
			<definition id="1">
				<sentence>The Cb represents the most highly ranked element of the previous utterance that is found in the current utterance .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiens id="0">the most highly ranked element of the previous utterance that is found in the current utterance</definiens>
			</definition>
			<definition id="2">
				<sentence>The three sets are further subdivided : OLD consists of evoked and unused entities ; MED consists of inferrables , containing inferrables , and anchored brand-new discourse intrasentential entities ; NEW consists solely of brand-new entities .</sentence>
				<definiendum id="0">MED</definiendum>
				<definiens id="0">consists of evoked and unused entities</definiens>
				<definiens id="1">consists of inferrables , containing inferrables , and anchored brand-new discourse intrasentential entities ; NEW consists solely of brand-new entities</definiens>
			</definition>
			<definition id="3">
				<sentence>Pronoun resolution is a simple lookup in the S-list .</sentence>
				<definiendum id="0">Pronoun resolution</definiendum>
			</definition>
			<definition id="4">
				<sentence>The first domain consists of 3,900 utterances ( 1,694 unquoted pronouns ) in New York Times articles provided by Ge , Hale , and Charniak ( 1998 ) , who annotated the corpus with coreference information .</sentence>
				<definiendum id="0">first domain</definiendum>
				<definiens id="0">consists of 3,900 utterances ( 1,694 unquoted pronouns ) in New York Times articles provided by Ge , Hale , and Charniak ( 1998 ) , who annotated the corpus with coreference information</definiens>
			</definition>
			<definition id="5">
				<sentence>Walker defines a discourse segment as a paragraph ( unless the first sentence of the paragraph has a pronoun in subject position or unless it has a pronoun with no antecedent among the preceding NPs that match syntactic features ) .</sentence>
				<definiendum id="0">discourse segment</definiendum>
				<definiens id="0">a pronoun in subject position or unless it has a pronoun with no antecedent among the preceding NPs that match syntactic features</definiens>
			</definition>
			<definition id="6">
				<sentence>This puts the two algorithms on equal footing with the Hobbs and LRC algorithms , which allow one to look back as far as possible within the discourse .</sentence>
				<definiendum id="0">LRC algorithms</definiendum>
				<definiens id="0">allow one to look back as far as possible within the discourse</definiens>
			</definition>
			<definition id="7">
				<sentence>`` Success rate '' as defined by Mitkov ( 2000 ) is the number of successfully resolved pronouns divided by the total number of pronouns .</sentence>
				<definiendum id="0">Success rate</definiendum>
				<definiens id="0">the number of successfully resolved pronouns divided by the total number of pronouns</definiens>
			</definition>
			<definition id="8">
				<sentence>LRC ranks the Cf-list by grammatical function .</sentence>
				<definiendum id="0">LRC</definiendum>
			</definition>
			<definition id="9">
				<sentence>LRC-F is the best instantiation of LRC and involves moving entities in a prepended phrase to the back of the Cf-list ( which is still ranked by grammatical function ) .</sentence>
				<definiendum id="0">LRC-F</definiendum>
				<definiens id="0">the best instantiation of LRC and involves moving entities in a prepended phrase to the back of the Cf-list ( which is still ranked by grammatical function )</definiens>
			</definition>
			<definition id="10">
				<sentence>LRC favors entities near the head of the sentence under the assumption that they are more salient .</sentence>
				<definiendum id="0">LRC</definiendum>
				<definiens id="0">favors entities near the head of the sentence under the assumption that they are more salient</definiens>
			</definition>
</paper>

		<paper id="4006">
			<definition id="0">
				<sentence>Clefts provide contrastive stress with a dummy subject it and the focal NP placed after the verb .</sentence>
				<definiendum id="0">Clefts</definiendum>
				<definiens id="0">provide contrastive stress with a dummy subject it and the focal NP placed after the verb</definiens>
			</definition>
			<definition id="1">
				<sentence>Prop-it is the ascription of properties to an entity with no existential force ( Quirk and Greenbaum 1973 ) .</sentence>
				<definiendum id="0">Prop-it</definiendum>
			</definition>
			<definition id="2">
				<sentence>Exophors refer outside the discourse to entities in the discourse setting .</sentence>
				<definiendum id="0">Exophors</definiendum>
			</definition>
			<definition id="3">
				<sentence>Quoted speech : Either the pronoun or its sponsor occurs in reported speech .</sentence>
				<definiendum id="0">Quoted speech</definiendum>
				<definiens id="0">Either the pronoun or its sponsor occurs in reported speech</definiens>
			</definition>
			<definition id="4">
				<sentence>R uses the above definition of T as its denominator , and P remains unchanged .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">uses the above definition of T as its denominator , and P remains unchanged</definiens>
			</definition>
			<definition id="5">
				<sentence>Her She Herself He Him His Himself It Its Itself Scope b Total A : Raw Word Count 22 25 3 89 44 7 14 94 12 1 186 497 Nonreferential Exclusions c Pleonastic 0 0 0 0 0 0 0 6 0 0 2 8 Abandoned Utterance 0 0 0 1 0 1 0 0 0 0 2 4 B : Sum Nonreferential 0 0 0 1 0 1 0 6 0 0 4 12 C : Total Referential ( A -- B ) 22 25 3 88 44 6 14 88 12 1 182 485 Referential Exclusions d Plural 0 0 0 0 0 0 0 0 0 0 120 120 Demonstrative 0 0 0 0 0 0 0 0 0 0 36 36 lst/2nd Person 0 0 0 0 0 0 0 0 0 0 24 24 Reported Speech 0 0 0 1 0 0 0 0 0 0 2 3 Event Anaphora 0 0 0 0 0 0 0 15 0 0 0 15 D : Sum Ref Exclusions 0 0 0 1 0 0 0 15 0 0 182 198 E : Evaluation Set ( C-D ) 22 25 3 87 44 6 14 73 12 1 0 e 287 Results Technique Alpha F : # Correct : Ante ( Inter ) 7/7 16/17 0/3 35/45 20/21 2/3 0/14 30/41 2/3 0/1 0 112 ( 82 % ) F : # Correct : Ante ( Intra ) 15/15 7/8 0/0 35/42 20/23 3/3 0/0 24/32 9/9 0/0 0 113 ( 86 % ) Errors : Cataphora 0 0 0 7/7 0 0 0 3/3 0 0 0 10 Errors : Long Distance 0 2/2 0 4/4 0 0 0 4/4 0 0 0 10 G : # Correct : Refs 21 22 0 67 38 5 0 52 11 0 0 216 ( 75 % ) Errors : Chaining 1 0 0 0 1 0 0 0 0 0 0 2 Resolution Rate ( G/C ) 100 % 88 % 0 % 76 % 86 % 83 % 0 % 59 % 92 % 0 % 0 % 45 % New Technique Beta H : # Correct : Ante ( Inter ) 5/7 17/17 3/3 45/45 15/21 2/3 13/14 34/41 3/3 1/1 0 138 ( 90 % ) H : # Correct : Ante ( Intra ) 15/15 7/8 0/0 31/42 24/31 3/3 0/0 27/32 6/9 0/0 0 113 ( 85 % ) Errors : Cataphora 0 0 0 7/7 0 0 0 1/3 0 0 0 8 Errors : Long Distance 0 2 0 4 0 0 0 4 0 0 0 10 I : # Correct : Refs 20 23 3 76 38 5 13 61 8 1 0 248 ( 86 % ) Errors : Chaining 0 0 0 1 2 0 0 0 0 0 0 3 Resolution Rate d ( I/C ) 90 % 92 % 100 % 86 % 86 % 83 % 93 % 69 % 67 % 100 % 0 % 51 % Notes on the format : ~Pronouns shown as column headings are those included in this ( fictional ) study .</sentence>
				<definiendum id="0">Referential</definiendum>
				<definiens id="0"># Correct : Refs 21 22 0 67 38 5 0 52 11 0 0 216 ( 75 % ) Errors</definiens>
			</definition>
			<definition id="6">
				<sentence>By tabulating the number of referential pronouns that are excluded , the format clarifies the composition of the test data set and enables the calculation of the resolution rate ( RR ) , which is a more accurate general measure of performance .</sentence>
				<definiendum id="0">format</definiendum>
				<definiens id="0">clarifies the composition of the test data set and enables the calculation of the resolution rate ( RR )</definiens>
			</definition>
</paper>

		<paper id="4005">
			<definition id="0">
				<sentence>Anai opens the gate and Oi it closes after herself 'Ana opens the gate and she closes it after herself . '</sentence>
				<definiendum id="0">Anai</definiendum>
				<definiens id="0">opens the gate and Oi it closes after herself 'Ana opens the gate and she closes it after herself</definiens>
			</definition>
			<definition id="1">
				<sentence>A preference is a characteristic that is not always satisfied by the solution of an anaphor .</sentence>
				<definiendum id="0">preference</definiendum>
				<definiens id="0">a characteristic that is not always satisfied by the solution of an anaphor</definiens>
			</definition>
			<definition id="2">
				<sentence>the NP is in a different clause or sentence ( 11 ) Anaj trajo un cuchillo y Eva/ sei cort6 .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">in a different clause or sentence ( 11 ) Anaj trajo un cuchillo y Eva/ sei cort6</definiens>
			</definition>
			<definition id="3">
				<sentence>Anaphora Resolution in Spanish Texts a clause or appositive ) preposition is a 'to ' or de 'of ' Preferences of personal and demonstrative pronouns that are included in a PP ( PPRinPP and DPRinPP ) : in the case of personal pronouns , the NP can not be a company type a relative clause or appositive ) verb as the anaphor Preferences of personal and demonstrative pronouns that are not included in a PP and of reflexive pronouns ( PPRnotPP , DPRnotPP , and RPR ) : in the case of personal pronouns , the NP can not be a company type a relative clause or appositive ) preposition is a 'to ' or de 'of ' included in a PP with the preposition en 'in ' which the NP appears ) The resolution procedure consists of the following steps : demonstrative ( DPRinPP or DPRnotPP ) , reflexive ( RPR ) , or omitted ( oPR ) .</sentence>
				<definiendum id="0">RPR )</definiendum>
				<definiendum id="1">resolution procedure</definiendum>
				<definiens id="0">a 'to ' or de 'of ' Preferences of personal and demonstrative pronouns that are included in a PP ( PPRinPP and DPRinPP ) : in the case of personal pronouns , the NP can not be a company type a relative clause or appositive ) verb as the anaphor Preferences of personal and demonstrative pronouns that are not included in a PP and of reflexive pronouns ( PPRnotPP , DPRnotPP</definiens>
			</definition>
			<definition id="4">
				<sentence>Spanish is a free-word-order language and has different syntactic conditions , which increases the difficulty of resolving Spanish pronouns ( hence , the greater accuracy rate for English texts ) .</sentence>
				<definiendum id="0">Spanish</definiendum>
				<definiens id="0">a free-word-order language and has different syntactic conditions</definiens>
			</definition>
			<definition id="5">
				<sentence>The list of forward-looking centers Cf ( Ui ) ranks discourse entities within the utterance Ui .</sentence>
				<definiendum id="0">list of forward-looking centers Cf ( Ui )</definiendum>
			</definition>
			<definition id="6">
				<sentence>According to Strube 's ranking criteria , two different sets of expressions , hearerold discourse entities ( OLD ) and hearer-new discourse entities ( NEW ) , can be distinguished .</sentence>
				<definiendum id="0">NEW )</definiendum>
				<definiens id="0">two different sets of expressions , hearerold discourse entities ( OLD ) and hearer-new discourse entities</definiens>
			</definition>
			<definition id="7">
				<sentence>OLD discourse entities consist of evoked entities -- -coreferring resolved expressions ( pronominal and nominal anaphora , previously mentioned proper names , relative pronouns , appositives ) -- and unused entities ( proper names and titles ) .</sentence>
				<definiendum id="0">OLD discourse entities</definiendum>
				<definiens id="0">consist of evoked entities -- -coreferring resolved expressions ( pronominal and nominal anaphora , previously mentioned proper names , relative pronouns , appositives ) -- and unused entities ( proper names and titles</definiens>
			</definition>
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>The algorithm uses the same set of operations as Earley 's ( 1970 ) algorithm for context-free grammars , but modified for unification grammars .</sentence>
				<definiendum id="0">algorithm</definiendum>
				<definiens id="0">uses the same set of operations as Earley 's ( 1970 ) algorithm for context-free grammars , but modified for unification grammars</definiens>
			</definition>
			<definition id="1">
				<sentence>Shieber ( 1992 ) defines a unification grammar as a 3-tuple ( G , P , p0 ) , where ~ is the vocabulary of the grammar , P is the set of productions , and P0 E P is the start production .</sentence>
				<definiendum id="0">unification grammar</definiendum>
				<definiendum id="1">~</definiendum>
				<definiendum id="2">P</definiendum>
				<definiens id="0">the set of productions , and P0 E P is the start production</definiens>
			</definition>
			<definition id="2">
				<sentence>A phrasal production is a 2-tuple ( a , ~ ) , where a is the arity of the rule ( the number of right-hand-side \ [ RHS\ ] constituents ) , and ~ is a logical formula .</sentence>
				<definiendum id="0">phrasal production</definiendum>
				<definiendum id="1">~</definiendum>
				<definiens id="0">a logical formula</definiens>
			</definition>
			<definition id="3">
				<sentence>Typically , q~ is a conjunction of equations of the form pl p2 or pl - '' c , where pl , p2 E L* are paths , and c E C. In an equation , any path which begins with an integer i ( 1 &lt; i &lt; a ) represents the ith RHS constituent of the rule .</sentence>
				<definiendum id="0">q~</definiendum>
				<definiens id="0">a conjunction of equations of the form pl p2 or pl - '' c</definiens>
			</definition>
			<definition id="4">
				<sentence>To define a valid parse tree , he first defines the set of possible parse trees I1 = Ui &gt; _0 Hi for a given grammar G , where each Eli is defined as follows : Definition A parse tree r is a model that is a member of the infinite union of sets of boundeddepth parse trees FI = Ui_ &gt; 0 I1i , where each IIi is defined as : this paper , we omit the 0 arcs and place the features of the LHS constituent directly at the root .</sentence>
				<definiendum id="0">IIi</definiendum>
				<definiens id="0">the set of possible parse trees I1 = Ui &gt; _0 Hi for a given grammar G , where each Eli is defined as follows : Definition A parse tree r is a model that is a member of the infinite union of sets of boundeddepth parse trees FI = Ui_ &gt; 0 I1i , where each</definiens>
			</definition>
			<definition id="5">
				<sentence>rio is the set of models 7for which there is a lexical production p = &lt; w , q ) ) E G such that 7~ 4 &lt; IIi ( i &gt; 0 ) is the set of models 7for which there is a phrasal production p = ( a , q~ ) C G such that 7~ ~ and , for all 1 &lt; i &lt; a , 7-/ { i ) is defined and 7-/ &lt; i } C Uj &lt; iIIy. In the second condition , the extraction operator , denoted by/ , retrieves the feature structure found at the end of a particular path ; so for instance 7-/ &lt; 1 ) retrieves the first subconstituent on the RHS of the production that licenses 7-. In the definition above , II0 contains all models that satisfy any lexical production in the grammar , while Hi contains all models that satisfy a phrasal production , and whose subconstituents are all in UjGi I\ ] j. To specify what constitutes a valid parse for a particular sentence , the next step is to define the yield of a parse tree. It is defined recursively as follows : if 7is licensed by some lexical production p = { w , q~/ , then the yield of 7is w ; or if 7is licensed by some phrasal production { a , q~ } and O~ 1 ... .. ( X a are the yields of 7-/ ( 1 ) ... .. 7-/ &lt; a ) respectively , then the yield of 7is ~1 ... % . Finally , Shieber defines a valid parse tree 7c II for sentence Wl ... wn as follows : o The yield of 7is Wl ... Wn 7is licensed by the start production po Notice that this definition allows extra features in a parse tree , because a parse tree 7is defined by the satisfaction relation ( 7~ ~ ) , which allows the existence of features in the model that are not in the licensing production 's formula. Given this definition , for any valid parse tree 7- , we can construct another parse tree 7- ' by simply adding an arbitrary ( nonnumeric ) feature to any node in 7-. Such a parse tree T ' is nonminimal because extra features are nonminimal with respect to the minimal features in the licensing productions. We will return to the issue of minimal and nonminimal parse trees in Section 4. Based on the logic described above , Shieber defines an abstract parsing algorithm as a set of four logical deduction rules. Each rule derives a new item , from previous items and/or productions in the grammar. An item is a 5-tuple { i , j , p , M , d ) , where i and j are indices into the sentence and specify which words in the sentence have been used to construct the item ; p is the production used to construct the item ; M is a model ; and d is the position of the `` dot '' ; i.e. , how many subconstituents in p have been completed so far. The logical rules of the abstract algorithm are shown in Figure 2. The Initial Item rule produces the first item , and is constructed from the start production P0. It spans none of the input ( i and j are both 0 ) , and its model is the minimal model ( ram ) of P0. The Prediction rule is essentially the top-down rewriting of the expectation ( a subconstituent just after the dot ) in a prior item. In this rule , the extraction of M/ ( d + 1 / retrieves the d + 1st submodel in M ( i.e. , expectation ) . The function p , which is left underspecified as a parameter in the abstract algorithm , filters out some features predefined in the various instantiations of the algorithm. Here , it is applied to the expectation , by which it effectively controls the top-down predictive power of the 279 Computational Linguistics Volume 27 , Number 2 INITIAL ITEM : { O , O , po , mm ( ~o ) , O ) PREDICTION : SCANNING : li , j , p = la , ~l , M , d ) ( j , j , p ' , p ( M/ ( d+l ) ) t3 mm ( ~ ' ) , 0 ) ' where d K a and p ' = ( a ' , O ' ) • P ( i , j , p = ( a , ~ } , M , d } { i , j+lip , M t_l ( mm ( ~2 ' ) \ { d+l ) ) , d+l } ' where d &lt; a and ( wj+l , O ' } • P COMPLETION : li'j'P = la ' ~l'M 'd ) ( j , k , p ' = ( a ' , /I~ ' ) , M ' , a ' / where d &lt; a I { i , kip , M El ( M ' \ { d+l ) ) , d+l ) Figure 2 Shieber 's parsing operations. I0 = ( O , O , po , mm ( 420 ) , O ) 11 = ( O , 1 , po , Ml,1 ) 12 = ( 1,1 , p2 , M2,0 I I3 = ( 1,2 , p2 , M3,1 ) I4 = ( 0 , 2 , p0 , M4 , 2 ) ag 5 ? yP° pers~ -n~ p mtrans 3rd sing Figure 3 Items produced in the parse of John sleeps , and the final parse. algorithm and provides flexibility to the instantiated algorithms. Then the expectation is unified with a production ( ~ ' ) , which can consistently rewrite it. By this operation , some features in the expectation may be propagated down in the production. The remaining two rules advance the dot in a prior item , by unifying the subconstituent to the right of the dot with either a lexical item from the input string ( the Scanning rule ) or some other completed higher-level item ( the Completion rule ) . Both rules perform the correct unification by utilizing the embedding operator ( signified by \ ) , which places a model M under a path p ( M\p ) . We illustrate these operators with a simple step-by-step example parse. Consider the grammar that consists of the rules presented in Figure 1. Using this grammar , Figure 3 shows the parse of the sentence John sleeps. First , the Initial Item operator is applied , producing item I0 , whose model is mm ( ~o ) . Next , the Scanning operator scans the word John , producing 11. The Prediction operator then produces 12. Next , the word sleeps is scanned ( since the first subconstituent of the model in 12 is a V ) , producing 13. Finally , since the item in 13 is complete ( d = 1 , the arity of production p2 ) , Completion is applied to items 11 and/3 , producing 14. Model M4 is the final parse of the sentence. In Section 2 , we noted that Shieber 's definition of parse trees allows them to be nonminimal. We consider these to be invalid based on a principle that , since the unification operation as set union preserves minimality ( as proved in Shieber , \ [ 1992\ ] ) , repeated applications of unification using licensing productions should result in parses that contain features only from those productions and nothing more. In this section , we 280 Tomuro and Lytinen Nonminimal Derivations ( ( cat ) VP I ( 1 cat ) -VP p4 = ( 2 , ~4 : { ( 2 cat } -ADV } | ( head ) ( 1 head ) / ( ( head modified ) true ) Figure 4 A phrasal production that results in a nonminimal derivation. I~ = ( 1,1 , p4 , M~ , 0 } /~ ' = ( 1,1 , p2 , M~r , 0 ) I~ = ( 1,2 , p2 , M~ , 1 ) I~ = { 0 , 2 , p0 , M~ , 2 } M4 ~ . '' cat~-43 c Salt NP nea~/l~Vub~V t ag~.. 7se.t~pe~modified per~ n~ lntrans t } ue 3rd sing Figure 5 Nonminimal derivation of John sleeps. formally define minimal and nonminimal parse trees , and show an example in which nonminimal parse trees are produced by Shieber 's algorithm. Our definition of minimal parse tree is to a large extent similar to Shieber 's definition , but to ensure minimality , our definition uses the equality relation instead of D , and inductively specifies a minimal parse tree bottom-up. Definition Given a grammar G , a minimal parse tree r admitted by G is a model that is a member of the infinite union of sets of bounded-depth parse trees 11 ' = Oi &gt; 0 IIl , where each 171 is defined as : .</sentence>
				<definiendum id="0">rio</definiendum>
				<definiendum id="1">p</definiendum>
				<definiendum id="2">M</definiendum>
				<definiendum id="3">j</definiendum>
				<definiendum id="4">Lytinen Nonminimal Derivations ( ( cat ) VP I</definiendum>
				<definiendum id="5">171</definiendum>
				<definiens id="0">a lexical production p = &lt; w , q ) ) E G such that 7~</definiens>
				<definiens id="1">the set of models 7for which there is a phrasal production p = ( a , q~ ) C G such that 7~ ~ and , for all 1 &lt; i &lt; a , 7-/ { i )</definiens>
				<definiens id="2">retrieves the feature structure found at the end of a particular path ; so for instance 7-/ &lt; 1 ) retrieves the first subconstituent on the RHS of the production that licenses 7-. In the definition above , II0 contains all models that satisfy any lexical production in the grammar , while Hi contains all models that satisfy a phrasal production , and whose subconstituents are all in UjGi I\ ] j. To specify what constitutes a valid parse for a particular sentence , the next step is to define the yield of a parse tree. It is defined recursively as follows : if 7is licensed by some lexical production p = { w , q~/ , then the yield of 7is w ; or if 7is licensed by some phrasal production { a , q~ } and O~ 1 ... .. ( X a are the yields of 7-/ ( 1 ) ... .. 7-/ &lt; a ) respectively , then the yield of 7is ~1 ... % . Finally , Shieber defines a valid parse tree 7c II for sentence Wl ... wn as follows : o The yield of 7is Wl ... Wn 7is licensed by the start production po Notice that this definition allows extra features in a parse tree , because a parse tree 7is defined by the satisfaction relation ( 7~ ~ ) , which allows the existence of features in the model that are not in the licensing production 's formula. Given this definition , for any valid parse tree 7- , we can construct another parse tree 7- ' by simply adding an arbitrary ( nonnumeric ) feature to any node in 7-. Such a parse tree T ' is nonminimal because extra features are nonminimal with respect to the minimal features in the licensing productions. We will return to the issue of minimal and nonminimal parse trees in Section 4. Based on the logic described above , Shieber defines an abstract parsing algorithm as a set of four logical deduction rules. Each rule derives a new item , from previous items and/or productions in the grammar. An item is a 5-tuple { i , j , p , M , d ) , where i and j are indices into the sentence and specify which words in the sentence have been used to construct the item ;</definiens>
				<definiens id="3">the production used to construct the item ;</definiens>
				<definiens id="4">a model ; and d is the position of the `` dot '' ; i.e. , how many subconstituents in p have been completed so far. The logical rules of the abstract algorithm are shown in Figure 2. The Initial Item rule produces the first item , and is constructed from the start production P0. It spans none of the input ( i and</definiens>
				<definiens id="5">the minimal model ( ram ) of P0. The Prediction rule is essentially the top-down rewriting of the expectation ( a subconstituent just after the dot ) in a prior item. In this rule , the extraction of M/ ( d + 1 / retrieves the d + 1st submodel in M ( i.e. , expectation ) . The function p , which is left underspecified as a parameter in the abstract algorithm , filters out some features predefined in the various instantiations of the algorithm. Here , it is applied to the expectation , by which it effectively controls the top-down predictive power of the 279 Computational Linguistics Volume 27 , Number 2 INITIAL ITEM : { O , O , po , mm ( ~o ) , O ) PREDICTION : SCANNING : li , j , p = la , ~l , M , d ) ( j , j , p ' , p ( M/ ( d+l ) ) t3 mm ( ~ ' ) , 0 ) ' where d K a and p ' = ( a ' , O ' ) • P ( i , j , p = ( a</definiens>
				<definiens id="6">mm ( ~2 ' ) \ { d+l ) ) , d+l } ' where d &lt; a and ( wj+l , O ' } • P COMPLETION : li'j'P = la ' ~l'M 'd ) ( j , k , p ' = ( a ' , /I~ ' ) , M ' , a ' / where d &lt; a I { i , kip , M El ( M ' \ { d+l ) ) , d+l ) Figure 2 Shieber 's parsing operations. I0 = ( O , O , po , mm ( 420 ) , O ) 11 = ( O , 1 , po , Ml,1 ) 12 = ( 1,1 , p2 , M2,0 I I3 = ( 1,2 , p2 , M3,1 ) I4 = ( 0 , 2 , p0 , M4 , 2 ) ag 5 ? yP° pers~ -n~ p mtrans 3rd sing Figure 3 Items produced in the parse of John sleeps , and the final parse. algorithm and provides flexibility to the instantiated algorithms. Then the expectation is unified with a production ( ~ ' ) , which can consistently rewrite it. By this operation , some features in the expectation may be propagated down in the production. The remaining two rules advance the dot in a prior item , by unifying the subconstituent to the right of the dot with either a lexical item from the input string ( the Scanning rule ) or some other completed higher-level item ( the Completion rule ) . Both rules perform the correct unification by utilizing the embedding operator ( signified by \ ) , which places a model M under a path p ( M\p ) . We illustrate these operators with a simple step-by-step example parse. Consider the grammar that consists of the rules presented in Figure 1. Using this grammar , Figure 3 shows the parse of the sentence John sleeps. First , the Initial Item operator is applied , producing item I0 , whose model is mm ( ~o ) . Next , the Scanning operator scans the word John , producing 11. The Prediction operator then produces 12. Next , the word sleeps is scanned ( since the first subconstituent of the model in 12 is a V ) , producing 13. Finally , since the item in 13 is complete ( d = 1 , the arity of production p2 ) , Completion is applied to items 11 and/3 , producing 14. Model M4 is the final parse of the sentence. In Section 2 , we noted that Shieber 's definition of parse trees allows them to be nonminimal. We consider these to be invalid based on a principle that , since the unification operation as set union preserves minimality ( as proved in Shieber</definiens>
				<definiens id="7">head modified ) true ) Figure 4 A phrasal production that results in a nonminimal derivation. I~ = ( 1,1 , p4 , M~ , 0 } /~ ' = ( 1,1 , p2 , M~r , 0 ) I~ = ( 1,2 , p2 , M~ , 1 ) I~ = { 0 , 2 , p0 , M~ , 2 } M4 ~ . '' cat~-43 c Salt NP nea~/l~Vub~V t ag~.. 7se.t~pe~modified per~ n~ lntrans t } ue 3rd sing Figure 5 Nonminimal derivation of John sleeps. formally define minimal and nonminimal parse trees , and show an example in which nonminimal parse trees are produced by Shieber 's algorithm. Our definition of minimal parse tree is to a large extent similar to Shieber 's definition , but to ensure minimality , our definition uses the equality relation instead of D , and inductively specifies a minimal parse tree bottom-up. Definition Given a grammar G , a minimal parse tree r admitted by G is a model that is a member of the infinite union of sets of bounded-depth parse trees 11 ' = Oi &gt; 0 IIl , where each</definiens>
			</definition>
			<definition id="6">
				<sentence>For each lexical production p = ( w , ~b ) E G , mm ( ~ ) E 11'o. For each phrasal production p = ( a , ~ } E G , let rl ... .. ra E Uj &lt; i I1 ; . If r = mm ( ~ ) lit1\ ( 1 ) t3 ... Ilrl\ ( a } , then r E 1I ; . It is obvious that 1I ' is a subset of 17 in Shieber 's definition. Then , a nonminimal parse tree is defined as a model that is a member of the difference of the two sets ( II 1I ' ) . 3 Here is a simple example in which a nonminimal parse is produced in Shieber 's algorithm. Say that we add the production in Figure 4 to the grammar in the previous section. The intent of this production is to mark the verb with the feature modified if an adverb follows. Using this grammar , Shieber 's algorithm will produce a nonminimal parse for the sentence John sleeps , in addition to the minimal parse shown in the previous section. 4 The nonminimal parse , shown in Figure 5 , arises as follows : after scanning John , Prediction can produce items I~ and I~ ' , first using production p4 ( thus inserting /head modified } true into the model ) , and then P2. Scanning the word saying `` a model r '' is a nonminimal parse tree if r '' E 17 and there exists r ' E II such that r ' _ &lt; r '' '' , because some r '' 's are minimal. See the example in Section 5. 281 Computational Linguistics Volume 27 , Number 2 sleeps then produces I~ from I~ I. Completion then can be applied directly to 11 and 11 by skipping a completion using I~ and I~ , thereby producing item I~. The feature modified remains in I~ , even though an adverb was never encountered in the sentence. The final parse M~ , shown in Figure 5 , is clearly nonminimal according to our definition because of this feature. Note that the example grammar can be changed to prevent the nonminimal parse , by moving the feature modified off of the head path in ff~4 ( i.e. , ( modified / true instead of ( head modified / true ) , s However , the point of the example is not to argue whether or not well-designed grammars will produce erroneous parses. A formally defined parser ( see the discussion below ) should in principle produce correct parses regardless of the grammar used ; otherwise , the grammar formalism ( i.e. , Shieber 's logic for unification grammars ) must be revised and properly constrained to allow only the kinds of productions with which the parser produces correct results. In general , nonminimal derivations may arise whenever two or more predictions that are not mutually exclusive can be produced at the same point in the sentence ; i.e. , two prediction items ( i , i , p , M , 0 / and ( i , i , p ' , M ~ , 0 / are produced such that M M / and M and M ~ are unifiable. In the example , items 12 = ( 1,1 , p2 , M2 , 0/ and I~ - ( 1,1 , P4 , M~ , 0 ) ( as well as I2 and I~ ~ = ( 1,1 , p2 , M~ ~ , 0/ ) are two such items. Since the two predictions did not have any conflicting features from the beginning , a situation may occur where a completion generated from one prediction can fill the other prediction without causing conflict. When this happens , features that were in the other prediction but not the original one become nonminimal in the resulting model. As to what causes nonminimal situations , we speculate that there are a number of possibilRies. First , nonminimal derivations occur when a prediction is filled by a complete item that was not generated from the prediction. This mismatch will not happen if parsing is done in one direction only ( e.g. purely top-down or bottom-up parsing ) . Thus , the mixed-direction parsing strategy is a contributing factor. Second , wrong complete items are retrieved because Shieber 's item-based algorithm makes all partial results available during parsing , as if they are kept in a global structure ( such as a chart in chart parsing ) . But if the accessibility of items were somehow restricted , prediction-completion mismatch would not happen. In this respect , other chart-based algorithms for unification grammars which adopt mixed-direction parsing strategy , including head-corner parsing ( van Noord 1997 ) and left-corner parsing ( Alshawi 1992 ) , are subject to the same problem. Third , extra features can only appear when the grammar contains rules which interact in a certain way ( such as rules P2 and P4 above ) . If the grammar contained no such rules , or if p ( the filtering function applied in Prediction ) filtered out those features , even the prediction-completion mismatch would not produce nonminimal derivations. As we stated in the beginning of this section , we consider nonminimal parses to be invalid on the basis of minimality. It then immediately follows that any parsing algorithm that produces nonminimal parses is considered to be unsound ; in particular , Shieber 's algorithm is unsound. However , since nonminimal parse trees have the same yield as their minimal counterparts , his algorithm does indeed recognize exactly the language of a given grammar. So , Shieber 's algorithm is sound as a recognizer , 6 but not as a transducer or parser ( as in van Noord , \ [ 1997\ ] ) where the correctness of output models ( i.e. , parse trees ) is critical. In other words , Shieber 's algorithm is correct up to can not specify the modified feature at their level , 6 In fact , Shieber hints at this : `` The process of parsing ( more properly , recognition ) ... '' ( Shieber 1992 , 78 ) . 282 Tomuro and Lytinen Nonminimal Derivations licensing , but incorrect on the basis of a stronger criteria of minimality. Thus , to guarantee correctness based on minimality , we need another algorithm ; such an algorithm is exactly the solution to the nonminimal derivation problem. Before presenting our solution to the nonminimal derivation problem , we discuss several possible practical techniques to get around the problem in implemented systems. These are known techniques , which have been applied to solve other problems in unification-based systems. However , most of them only offer partial solutions to the nonminimal derivation problem. First , whenever Shieber 's algorithm produces a nonminimal derivation , it also produces a corresponding minimal derivation ( Tomuro 1999 ) . Thus , one possible solution is to use subsumption to discard items that are more specific than any other items that are produced. Subsumption has often been used in unification-based systems to pack items or models ( e.g. , Alshawi 1992 ) . However , simple subsumption may filter out valid parses for some grammars , thus sacrificing completeness. 7 Another possibility is to filter out problematic features in the Prediction step by using the function p. However , automatic detection of such features ( i.e. , automatic derivation of p ) is undecidable for the same reason as the prediction nontermination problem ( caused by left recursion ) for unification grammars ( Shieber 1985 ) . Manual detection is also problematic : when a grammar is large , particularly if semantic features are included , complete detection is nearly impossible. As for the techniques developed so far which ( partially ) solve prediction nontermination ( e.g. , Shieber 1985 ; Haas 1989 ; Samuelsson 1993 ) , they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general s One way is to define p to filter out all features except the context-free backbone of predictions. However , this severely restricts the range of possible instantiations of Shieber 's algorithm. 9 A third possibility is to manually fix the grammar so that nonminimal derivations do not occur , as we noted in Section 4. However , this approach is problematic for the same reason as the manual derivation of p mentioned above. Finally , we propose an algorithm that does not produce nonminimal derivations. It is a modification of Shieber 's algorithm that incorporates parent pointers. Figure 6 shows M1 : { &lt; cat &gt; -C , &lt; x &gt; a } , M2 : { &lt; cat &gt; C , &lt; y &gt; b } , and p = &lt; 1 , { &lt; cat &gt; C , &lt; 1 cat &gt; `` D , &lt; x &gt; a } &gt; respectively , the resulting model M2 ~ = { &lt; cat &gt; C , &lt; 1 cat &gt; D , &lt; x &gt; -a , &lt; y &gt; -b } will have strictly more information than the other resulting model MI ' = { &lt; cat &gt; ~ C , &lt; 1 cat &gt; D , &lt; x &gt; a } , although both models are minimal .</sentence>
				<definiendum id="0">mixed-direction parsing strategy</definiendum>
				<definiendum id="1">Shieber 's algorithm</definiendum>
				<definiens id="0">production p = ( w , ~b ) E G , mm ( ~ ) E 11'o. For each phrasal production p = ( a , ~ } E G , let rl ... .. ra E Uj &lt; i I1 ;</definiens>
				<definiens id="1">a subset of 17 in Shieber 's definition. Then</definiens>
				<definiens id="2">a model that is a member of the difference of the two sets ( II 1I ' ) . 3 Here is a simple example in which a nonminimal parse is produced in Shieber 's algorithm. Say that we add the production in Figure 4 to the grammar in the previous section. The intent of this production is to mark the verb with the feature modified if an adverb follows. Using this grammar , Shieber 's algorithm will produce a nonminimal parse for the sentence John sleeps , in addition to the minimal parse shown in the previous section. 4 The nonminimal parse , shown in Figure 5 , arises as follows : after scanning John , Prediction can produce items I~ and I~ ' , first using production p4 ( thus inserting /head modified } true into the model ) , and then P2. Scanning the word saying `` a model r '' is a nonminimal parse tree if r '' E 17 and there exists r ' E II such that r ' _ &lt; r '' '' , because some r '' 's are minimal. See the example in Section 5. 281 Computational Linguistics Volume 27 , Number 2 sleeps then produces I~ from I~ I. Completion then can be applied directly to 11 and 11 by skipping a completion using I~ and I~ , thereby producing item I~. The feature modified remains in I~ , even though an adverb was never encountered in the sentence. The final parse M~ , shown in Figure 5 , is clearly nonminimal according to our definition because of this feature. Note that the example grammar can be changed to prevent the nonminimal parse , by moving the feature modified off of the head path in ff~4 ( i.e. , ( modified / true instead of ( head modified / true ) , s However , the point of the example is not to argue whether or not well-designed grammars will produce erroneous parses. A formally defined parser ( see the discussion below ) should in principle produce correct parses regardless of the grammar used ; otherwise , the grammar formalism ( i.e. , Shieber 's logic for unification grammars ) must be revised and properly constrained to allow only the kinds of productions with which the parser produces correct results. In general , nonminimal derivations may arise whenever two or more predictions that are not mutually exclusive can be produced at the same point in the sentence ; i.e. , two prediction items ( i</definiens>
				<definiens id="3">the example , items 12 = ( 1,1 , p2 , M2 , 0/ and I~ - ( 1,1 , P4 , M~ , 0 ) ( as well as I2 and I~ ~ = ( 1,1 , p2 , M~ ~ , 0/ ) are two such items. Since the two predictions did not have any conflicting features from the beginning , a situation may occur where a completion generated from one prediction can fill the other prediction without causing conflict. When this happens , features that were in the other prediction but not the original one become nonminimal in the resulting model. As to what causes nonminimal situations</definiens>
				<definiens id="4">a number of possibilRies. First , nonminimal derivations occur when a prediction is filled by a complete item that was not generated from the prediction. This mismatch will not happen if parsing is done in one direction only ( e.g. purely top-down or bottom-up parsing )</definiens>
				<definiens id="5">a contributing factor. Second , wrong complete items are retrieved because Shieber 's item-based algorithm makes all partial results available during parsing , as if they are kept in a global structure ( such as a chart in chart parsing ) . But if the accessibility of items were somehow restricted</definiens>
				<definiens id="6">other chart-based algorithms for unification grammars which adopt mixed-direction parsing strategy , including head-corner parsing ( van Noord 1997 ) and left-corner parsing ( Alshawi 1992 ) , are subject to the same problem. Third , extra features can only appear when the grammar contains rules which interact in a certain way ( such as rules P2 and P4 above ) . If the grammar contained no such rules , or if p ( the filtering function applied in Prediction ) filtered out those features , even the prediction-completion mismatch would not produce nonminimal derivations. As we stated in the beginning of this section , we consider nonminimal parses to be invalid on the basis of minimality. It then immediately follows that any parsing algorithm that produces nonminimal parses is considered to be unsound ; in particular , Shieber 's algorithm is unsound. However , since nonminimal parse trees have the same yield as their minimal counterparts , his algorithm does indeed recognize exactly the language of a given grammar. So , Shieber 's algorithm is sound as a recognizer , 6 but not as a transducer or parser ( as in van Noord , \ [ 1997\ ] ) where the correctness of output models ( i.e. , parse trees ) is critical. In other words</definiens>
				<definiens id="7">The process of parsing ( more properly , recognition ) ... '' ( Shieber 1992 , 78 ) . 282 Tomuro and Lytinen Nonminimal Derivations licensing , but incorrect on the basis of a stronger criteria of minimality. Thus , to guarantee correctness based on minimality , we need another algorithm ; such an algorithm is exactly the solution to the nonminimal derivation problem. Before presenting our solution to the nonminimal derivation problem , we discuss several possible practical techniques to get around the problem in implemented systems. These are known techniques , which have been applied to solve other problems in unification-based systems. However , most of them only offer partial solutions to the nonminimal derivation problem. First , whenever Shieber 's algorithm produces a nonminimal derivation , it also produces a corresponding minimal derivation ( Tomuro 1999 ) . Thus , one possible solution is to use subsumption to discard items that are more specific than any other items that are produced. Subsumption has often been used in unification-based systems to pack items or models ( e.g. , Alshawi 1992 ) . However , simple subsumption may filter out valid parses for some grammars , thus sacrificing completeness. 7 Another possibility is to filter out problematic features in the Prediction step by using the function p. However , automatic detection of such features ( i.e. , automatic derivation of p ) is undecidable for the same reason as the prediction nontermination problem ( caused by left recursion ) for unification grammars ( Shieber 1985 ) . Manual detection is also problematic : when a grammar is large , particularly if semantic features are included , complete detection is nearly impossible. As for the techniques developed so far which ( partially ) solve prediction nontermination ( e.g. , Shieber 1985 ; Haas 1989 ; Samuelsson 1993 ) , they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general s One way is to define p to filter out all features except the context-free backbone of predictions. However , this severely restricts the range of possible instantiations of Shieber 's algorithm. 9 A third possibility is to manually fix the grammar so that nonminimal derivations do not occur</definiens>
				<definiens id="8">problematic for the same reason as the manual derivation of p mentioned above. Finally , we propose an algorithm that does not produce nonminimal derivations. It is a modification of Shieber 's algorithm that incorporates parent pointers. Figure 6 shows M1 : { &lt; cat &gt; -C , &lt; x &gt; a } , M2 : { &lt; cat &gt; C , &lt; y &gt; b } , and p = &lt; 1 , { &lt; cat &gt; C , &lt; 1 cat &gt; `` D , &lt; x &gt; a } &gt; respectively , the resulting model M2 ~ = { &lt; cat &gt; C , &lt; 1 cat &gt; D , &lt; x &gt; -a , &lt; y &gt; -b } will have strictly more information than the other resulting model MI ' = { &lt; cat &gt; ~ C , &lt; 1 cat &gt; D , &lt; x &gt; a }</definiens>
			</definition>
			<definition id="7">
				<sentence>283 Computational Linguistics Volume 27 , Number 2 INITIAL ITEM : PREDICTION : ( id , nil , ( O , O , po , mm ( ~o ) , O ) ) ' where id is a new symbol ( id , pid , ( i , j , p = ( a , ~ ) , M , d ) ) ( id ' , id , ( j , j , p ' , p ( M/ ( d+l ) ) U mm ( ~I , ' ) , 0 ) ) ' where id I is a new symbol , and d ( a and pl = ( ar , ~t ) C P SCANNING : COMPLETION : ( id , pid , ( i , j , p = ( a , ~ ) , M , d ) ) ( id , pid , ( i , j+l , p , M U mm ( ~ ' ) \ ( d+l ) , d+l ) ) ' where d &lt; a and ( wj+D ~ ' ) E P ( id , pid , ( i , j , p , M , d ) ) ( id '' , id , ( j , k , p ' , M ' , a ' ) ) where d &lt; a ( ia , pie , ( i , k , p , UU ( U ' \ ( d+l ) ) , d+l ) ) ' Figure 6 Shieber 's parsing operations modified .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">j , k , p ' , M ' , a ' ) ) where d &lt; a ( ia , pie , ( i , k , p , UU ( U ' \ ( d+l ) ) , d+l ) ) ' Figure 6 Shieber 's parsing operations modified</definiens>
			</definition>
</paper>

		<paper id="4001">
			<definition id="0">
				<sentence>Corpora ( especially when annotated ) are an invaluable source not only for empirical research but also for automated learning ( e.g. , machine learning ) methods aiming to develop new rules and approaches ; they also provide an important resource for evaluation of the implemented approaches .</sentence>
				<definiendum id="0">Corpora</definiendum>
				<definiens id="0">machine learning ) methods aiming to develop new rules and approaches ; they also provide an important resource for evaluation of the implemented approaches</definiens>
			</definition>
			<definition id="1">
				<sentence>The coreference resolution module is part of a larger coreference resolution system that also includes sentence segmentation , tokenization , morphological analysis , part-of-speech tagging , noun phrase identification , named entity recognition , and semantic class determination ( via WordNet ) .</sentence>
				<definiendum id="0">coreference resolution module</definiendum>
				<definiens id="0">part of a larger coreference resolution system that also includes sentence segmentation , tokenization , morphological analysis , part-of-speech tagging , noun phrase identification , named entity recognition</definiens>
			</definition>
			<definition id="2">
				<sentence>The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally .</sentence>
				<definiendum id="0">LRC</definiendum>
				<definiens id="0">an alternative to the original BFP algorithm in that it processes utterances incrementally</definiens>
			</definition>
</paper>

		<paper id="4007">
			<definition id="0">
				<sentence>Centering theory ( henceforth CT ) is a theory of local discourse structure that models the interaction of referential continuity and salience of discourse entities in the internal organization of a text .</sentence>
				<definiendum id="0">henceforth CT )</definiendum>
				<definiens id="0">a theory of local discourse structure that models the interaction of referential continuity and salience of discourse entities in the internal organization of a text</definiens>
			</definition>
			<definition id="1">
				<sentence>Cb ( U , ) is the highest-ranked element of Cf ( Un-1 ) that is realized in Un .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiens id="0">the highest-ranked element of Cf ( Un-1 ) that is realized in Un</definiens>
			</definition>
			<definition id="2">
				<sentence>Cb ( Un ) is defined as the highest-ranked member of Cf ( Un-1 ) that is realized in U , .</sentence>
				<definiendum id="0">Cb ( Un</definiendum>
			</definition>
			<definition id="3">
				<sentence>Cp ( U , ) is the highest-ranked member of Cf ( Un ) , and is predicted to be Cb ( Un+l ) .</sentence>
				<definiendum id="0">Cp</definiendum>
			</definition>
			<definition id="4">
				<sentence>The principle of `` cheapness '' is intended to capture the intuition that a Retain is naturally followed by a Smooth Shift and is defined as follows : A transition pair is cheap if the backward-looking center of the current utterance is correctly predicted by the preferred center of the immediately preceding utterance , i.e. , Cb ( Ui ) = Cp ( Ui_l ) ... ( Strube and Hahn 1999 , page 332 ) Cheapness is claimed to minimize the inferential costs of processing sequences of utterances , and is proposed as a constraint on pairs of successive transitions as a replacement for the canonical orderings in Rule 2 , which is restated as follows : Rule 2 ~ Cheap transition pairs are preferred over expensive ones .</sentence>
				<definiendum id="0">principle of `` cheapness ''</definiendum>
				<definiendum id="1">transition pair</definiendum>
				<definiendum id="2">page 332 ) Cheapness</definiendum>
				<definiens id="0">cheap if the backward-looking center of the current utterance is correctly predicted by the preferred center of the immediately preceding utterance</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>LTAG provides two operations for combining trees : substitution and adjunction .</sentence>
				<definiendum id="0">LTAG</definiendum>
				<definiens id="0">provides two operations for combining trees : substitution and adjunction</definiens>
			</definition>
			<definition id="1">
				<sentence>Elementary objects are descriptions of possible syntactic contexts for the anchor , formalized in a logic for describing nodes and the relationships ( dominance , immediate dominance , linear precedence ) that hold between them .</sentence>
				<definiendum id="0">Elementary objects</definiendum>
				<definiens id="0">dominance , immediate dominance , linear precedence ) that hold between them</definiens>
			</definition>
			<definition id="2">
				<sentence>The adjunction operation embeds all of the adjoined tree within that part of the tree at which adjunction occurs .</sentence>
				<definiendum id="0">adjunction operation</definiendum>
				<definiens id="0">embeds all of the adjoined tree within that part of the tree at which adjunction occurs</definiens>
			</definition>
			<definition id="3">
				<sentence>DSG is intended to be a simple framework with which it is possible to provide analyses for those cases described with LTAG as well as for various cases in which extensions of LTAG have been needed , such as different versions of multicomponent grammar ) .</sentence>
				<definiendum id="0">DSG</definiendum>
				<definiens id="0">different versions of multicomponent grammar )</definiens>
			</definition>
			<definition id="4">
				<sentence>D-trees are the primitive elements of a DSG .</sentence>
				<definiendum id="0">D-trees</definiendum>
				<definiens id="0">the primitive elements of a DSG</definiens>
			</definition>
			<definition id="5">
				<sentence>For example , in Figure 6 , x3 is a frontier of a component but not a frontier of the tree description .</sentence>
				<definiendum id="0">x3</definiendum>
				<definiens id="0">a frontier of a component but not a frontier of the tree description</definiens>
			</definition>
			<definition id="6">
				<sentence>Let x E vars ( dl ) be a root of a component of dl and y E vars ( d2 ) be a substitution node in the frontier of d2 .</sentence>
				<definiendum id="0">dl )</definiendum>
				<definiens id="0">a root of a component of dl and y E vars ( d2 ) be a substitution node in the frontier of d2</definiens>
			</definition>
			<definition id="7">
				<sentence>Recall that not every set of positive literals involving A , /~ , and -~ is a legal d-tree .</sentence>
				<definiendum id="0">-~</definiendum>
				<definiens id="0">a legal d-tree</definiens>
			</definition>
			<definition id="8">
				<sentence>Definition A d-tree substitution grammar ( DSG ) G is a 4-tuple ( VT , VN , T , ds ) , where VT and VN are pairwise distinct terminal and nonterminal alphabets , respectively , T is a finite set of elementary d-trees such that the functor label assigns each node in each d-tree in T a label in VT U VN and such that only d-tree frontier nodes take labels in VT , and ds is a characterization of the labels that can appear at the root of a derived tree .</sentence>
				<definiendum id="0">d-tree substitution grammar</definiendum>
				<definiendum id="1">DSG</definiendum>
				<definiendum id="2">T</definiendum>
				<definiens id="0">a 4-tuple ( VT , VN , T , ds ) , where VT and VN are pairwise distinct terminal and nonterminal alphabets , respectively</definiens>
				<definiens id="1">a finite set of elementary d-trees such that the functor label assigns each node in each d-tree in T a label in VT U VN and such that only d-tree frontier nodes take labels in VT , and ds is a characterization of the labels that can appear at the root of a derived tree</definiens>
			</definition>
			<definition id="9">
				<sentence>A d-edge represents a domination relation of length zero or more .</sentence>
				<definiendum id="0">d-edge</definiendum>
				<definiens id="0">a domination relation of length zero or more</definiens>
			</definition>
			<definition id="10">
				<sentence>The tree language T ( G ) generated by G is the set of trees that can be read off from sentential d-trees in T ( G ) .</sentence>
				<definiendum id="0">tree language T</definiendum>
			</definition>
			<definition id="11">
				<sentence>However , when y is not the root of a component , and z is the root of the component containing y , then the tree we obtain from the reading off process is one where x dominates z and not where z properly dominates x. That is , in this case , we replace the d-edge between x and y with a d-edge between x and z , which we then eliminate in the reading off process by equating x and z. But in order to replace the d-edge between x and y with a d-edge between x and z , we need to make sure that the path between z and y does not violate the path constraint associated with the d-edge between x and y. 10 In Rambow , Vijay-Shanker , and Weir ( 1995 ) , path constraints are called `` subsertion-insertion constraints . ''</sentence>
				<definiendum id="0">z</definiendum>
				<definiendum id="1">process</definiendum>
				<definiens id="0">the root of the component containing y</definiens>
			</definition>
			<definition id="12">
				<sentence>100 Rainbow , Vijay-Shanker , and Weir D-Tree Substitution Grammars A A a A a A t I I I o n B B b B b B !</sentence>
				<definiendum id="0">Weir D-Tree Substitution Grammars A A</definiendum>
			</definition>
			<definition id="13">
				<sentence>DSG is closely related ( and weakly equivalent ) to two equivalent string rewriting systems , UVG-DL and { } -LIG ( Rainbow 1994a , 1994b ) .</sentence>
				<definiendum id="0">DSG</definiendum>
				<definiens id="0">closely related ( and weakly equivalent ) to two equivalent string rewriting systems</definiens>
			</definition>
			<definition id="14">
				<sentence>{ } -LIG is a multisebvalued variant of Linear Index Grammar ( Gazdar 1988 ) .</sentence>
				<definiendum id="0">{ } -LIG</definiendum>
			</definition>
			<definition id="15">
				<sentence>To obtain an auxiliary tree in order to give a usual TAG-style account of long-distance dependencies , the complement of the equi-verb ( control verb ) hopes must be given an S label , which in turn imposes a linguistic analysis using an empty ( PRO ) subject as shown in Figure 14 ( or , at any rate , an analysis in which the infinitival to meet projects to S ) .</sentence>
				<definiendum id="0">S label</definiendum>
			</definition>
			<definition id="16">
				<sentence>However , TDG allows for more than one node to be equated in a derivation step : nodes are `` marked '' and all marked nodes are required to be equated with other nodes in a derivation step .</sentence>
				<definiendum id="0">TDG</definiendum>
				<definiens id="0">allows for more than one node to be equated in a derivation step</definiens>
			</definition>
			<definition id="17">
				<sentence>The principal motivation for using DSG is that DSG is a lexicalized formalism which can provide derivations that correspond to the traditional notion of ( deep ) syntactic dependency ( see Section 5 ) , which is often considered to be the input to the syntactic component of a generation system .</sentence>
				<definiendum id="0">DSG</definiendum>
				<definiens id="0">a lexicalized formalism which can provide derivations that correspond to the traditional notion of ( deep ) syntactic dependency</definiens>
				<definiens id="1">often considered to be the input to the syntactic component of a generation system</definiens>
			</definition>
</paper>

		<paper id="2002">
			<definition id="0">
				<sentence>bP ( CK I Esub ) Fsub C Fcase with the weight WG , b for an Fsub containing n elements equal to n -- -L-r ' where Wnorm is a normalizing Wnor m i constant so that ~cx P ( Cx ) = 1 .</sentence>
				<definiendum id="0">bP</definiendum>
				<definiendum id="1">Wnorm</definiendum>
				<definiens id="0">the weight WG , b for an Fsub containing n elements equal to n -- -L-r ' where</definiens>
				<definiens id="1">a normalizing Wnor m i constant so that ~cx P ( Cx ) = 1</definiens>
			</definition>
			<definition id="1">
				<sentence>In the basic version ( Tags ) , each training case for the second-level learner consists of the tags suggested by the component taggers and the correct tag ( Figure 4 ) .</sentence>
				<definiendum id="0">Tags )</definiendum>
			</definition>
			<definition id="2">
				<sentence>For the Tags version , the similarity metric used is Overlap ( a count of the number of matching feature values between a test and a training item ) and k is kept at 1 .</sentence>
				<definiendum id="0">Overlap</definiendum>
				<definiens id="0">a count of the number of matching feature values between a test and a training item</definiens>
			</definition>
			<definition id="3">
				<sentence>The model has the form of an exponential model : 1 e Y~i ~i~ ( ca~ , ~g ) pA ( tag l Case ) -Za ( Case ) where i indexes all the binary features , fi is a binary indicator function for feature i , ZA is a normalizing constant , and ) ~i is a weight for feature i. The model is trained by iteratively adding binary features with the largest gain in the probability of the training data , and estimating the weights using a numerical optimization method called improved iterafive scaling .</sentence>
				<definiendum id="0">ZA</definiendum>
				<definiens id="0">a normalizing constant , and ) ~i is a weight for feature i. The model is trained by iteratively adding binary features with the largest gain in the probability of the training data , and estimating the weights using a numerical optimization method called improved iterafive scaling</definiens>
			</definition>
			<definition id="4">
				<sentence>In variety , it lies between LOB and WSJ , containing 150K words each of samples from Dutch newspapers ( subcorpus CDB ) , weeklies ( OBL ) , magazines ( GBL ) , popular scientific writings ( PWE ) , and novels ( RNO ) .</sentence>
				<definiendum id="0">PWE</definiendum>
				<definiens id="0">weeklies ( OBL ) , magazines ( GBL ) , popular scientific writings</definiens>
			</definition>
			<definition id="5">
				<sentence>The system then searches through a space of transformation rules ( defined by rule templates ) in order to reduce the discrepancy between its current annotation and the provided correct one .</sentence>
				<definiendum id="0">transformation rules</definiendum>
			</definition>
			<definition id="6">
				<sentence>tropy modeling ( see Section 2.4 ) : a maximum entropy tagger , called MXPOST , was developed by Ratnaparkhi ( 1996 ) ( we will refer to this tagger as MXP below ) .</sentence>
				<definiendum id="0">tropy modeling</definiendum>
				<definiens id="0">a maximum entropy tagger , called MXPOST</definiens>
			</definition>
			<definition id="7">
				<sentence>2° TnT is a trigram tagger ( Brants 2000 ) , which means that it considers the previous two tags as features for deciding on the current tag .</sentence>
				<definiendum id="0">TnT</definiendum>
				<definiens id="0">a trigram tagger</definiens>
			</definition>
			<definition id="8">
				<sentence>21 We see that TBL achieves the lowest accuracy on all data sets .</sentence>
				<definiendum id="0">TBL</definiendum>
			</definition>
			<definition id="9">
				<sentence>Tagger Pair MXP MXP MXP HMM HMM MBT Data Set HMM MBT TBL MBT TBL TBL LOB 97.56 96.70 96.27 97.27 96.96 96.78 WSJ 97.41 96.85 96.90 97.18 97.39 97.21 Wotan 93.02 90.81 92.06 WotanLite 95.74 95.12 95.00 95.48 95.36 95.52 ( WSJ and WotanLite ) MXPOST is the better system .</sentence>
				<definiendum id="0">WotanLite ) MXPOST</definiendum>
				<definiens id="0">the better system</definiens>
			</definition>
			<definition id="10">
				<sentence>WPDV shows a relatively constant significant improvement over all data sets .</sentence>
				<definiendum id="0">WPDV</definiendum>
			</definition>
			<definition id="11">
				<sentence>For each word , we list the total number of instances in the test set ( n ) , the number of tags associated with the word ( tags ) , and then , for each base tagger and WPDV ( Tags+Context ) , the rank in the error list ( rank ) , the absolute number of errors ( err ) , and the percentage of instances that is mistagged ( % ) .</sentence>
				<definiendum id="0">WPDV</definiendum>
				<definiens id="0">the total number of instances in the test set ( n ) , the number of tags associated with the word ( tags )</definiens>
			</definition>
			<definition id="12">
				<sentence>MXP HMM MBT TBL WPDV ( T+C ) Tagger Correct rank err rank err rank err rank err rank err VBN VBD 6 92 1 154 1 205 1 236 3 102 VBD VBN 3 118 3 117 3 152 3 149 4 100 pair 210 271 357 385 202 JJ NN 2 132 2 150 2 168 2 205 2 109 NN JJ 1 153 6 75 4 148 4 148 1 110 pair 285 225 316 353 219 IN CS 4 105 4 93 5 122 s 97 5 79 CS IN 10 55 7 70 10 64 6 122 8 48 pair 160 163 186 219 127 NN VB 5 98 5 78 6 116 5 132 6 59 VB NN 25 28 14 45 12 60 7 100 15 35 pair 126 123 176 232 94 IN RP 7 59 10 61 7 99 12 83 7 50 RP 1N 24 30 18 38 27 34 21 42 18 30 pair 89 99 133 125 80 of course especially the case where HMM has particular difficulties with a word , e.g. , about with a 46.3 % reduction in error rate , but in other cases as well , e.g. , to with a 32.2 % reduction , which is still well above the overall error rate reduction of 24.3 % .</sentence>
				<definiendum id="0">MXP HMM MBT TBL WPDV</definiendum>
				<definiendum id="1">HMM</definiendum>
				<definiens id="0">particular difficulties with a word</definiens>
			</definition>
			<definition id="13">
				<sentence>MXP , although having a lower accuracy by itself than HMM , yet leads to better combination results , again witnessed by the Gain columns .</sentence>
				<definiendum id="0">MXP</definiendum>
				<definiens id="0">although having a lower accuracy by itself than HMM</definiens>
			</definition>
			<definition id="14">
				<sentence>The columns contain , from left to right , the accuracies for the base taggers , the combination accuracies when using only tags ( WPDV ( Tags ) ) at three different levels of combination granularity ( Full , Lite , and Main ) and the combination accuracies when adding context ( WPDV ( Tags+Context ) ) , at the same three levels of combination granularity .</sentence>
				<definiendum id="0">WPDV</definiendum>
				<definiendum id="1">WPDV</definiendum>
				<definiens id="0">Tags ) ) at three different levels of combination granularity</definiens>
			</definition>
			<definition id="15">
				<sentence>WPDV and Maccent make the best use of the extra information , with WPDV having an edge for less consistent data ( WSJ ) and Maccent for material with a high error rate ( Wotan ) .</sentence>
				<definiendum id="0">WPDV</definiendum>
				<definiens id="0">the best use of the extra information , with WPDV having an edge for less consistent data ( WSJ</definiens>
			</definition>
			<definition id="16">
				<sentence>BoosTexter : A system for multiclass multi-label text categorization .</sentence>
				<definiendum id="0">BoosTexter</definiendum>
				<definiens id="0">A system for multiclass multi-label text categorization</definiens>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>MDL focuses on the analysis of a corpus of data that is optimal by virtue of providing both the most compact representation of the data and the most compact means of extracting that compression from the original data .</sentence>
				<definiendum id="0">MDL</definiendum>
				<definiens id="0">focuses on the analysis of a corpus of data that is optimal by virtue of providing both the most compact representation of the data and the most compact means of extracting that compression from the original data</definiens>
			</definition>
			<definition id="1">
				<sentence>6 While not all of the approaches discussed here use no prior language-particular knowledge ( which is the goal of the present system ) , I exclude from discussions those systems that are based essentially on a prior human-designed analysis of the grammatical morphemes of a language , aiming at identifying the stem ( s ) and the correct parsing ; such is the with the present approach .</sentence>
				<definiendum id="0">language-particular knowledge</definiendum>
				<definiens id="0">the goal of the present system ) , I exclude from discussions those systems that are based essentially on a prior human-designed analysis of the grammatical morphemes of a language , aiming at identifying the stem ( s ) and the correct parsing</definiens>
				<definiens id="1">the with the present approach</definiens>
			</definition>
			<definition id="2">
				<sentence>( Precision here indicates proportion of predicted morpheme breaks that are correct , and recall denotes the proportion of correct breaks that are predicted . )</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the proportion of correct breaks that are predicted</definiens>
			</definition>
			<definition id="3">
				<sentence>The quality function is defined as that average divided by the length of the largest suffix in the signature ; reject any signature class for which that ratio is less than 1.0 .</sentence>
				<definiendum id="0">quality function</definiendum>
				<definiens id="0">that average divided by the length of the largest suffix in the signature ; reject any signature class for which that ratio is less than 1.0</definiens>
			</definition>
			<definition id="4">
				<sentence>cat C. Signatures : 4 Signature 1 : / treat SimpleStem : ptr ( dog ) SimpleStem : ptr ( hat ) L ptr ( s ) J ComplexStem : ptr ( Sig2 ) : ptr ( sav ) + ptr ( ing ) Signature 2 : f ptr ( e ) ~ { SimpleStem : ptr ( sav ) } ~ptr ( es ) ~ ~ , ptr ( ing ) ) Signature 3 : ptr ( NULL ) `` f SimpleStem : ptr ( jump ) ~ ~ ptr ( ed ) ~ SimpleStem : ptr ( laugh ) ~ | ptr ( ing ) I , SimpleStem : ptr ( walk ) ) I , ptr ( s ) Signature 4 : SimpleStem : ptr ( John ) SimpleStem : ptr ( the ) J Figure 2 A sample morphology .</sentence>
				<definiendum id="0">f SimpleStem</definiendum>
				<definiendum id="1">SimpleStem</definiendum>
				<definiens id="0">ptr ( laugh ) ~ | ptr ( ing ) I ,</definiens>
			</definition>
			<definition id="5">
				<sentence>Hence the length of a signature is the sum of the ( inverse ) log probabilities of its stems , plus that of its suffixes , plus the number of bits it takes to specify the number of its stems and suffixes , using the A function .</sentence>
				<definiendum id="0">Hence the length of a signature</definiendum>
				<definiens id="0">the sum of the ( inverse ) log probabilities of its stems , plus that of its suffixes , plus the number of bits it takes to specify the number of its stems and suffixes</definiens>
			</definition>
			<definition id="6">
				<sentence>The length in bits of the stem list is &amp; ( ( T ) ) + ~ Ltypo ( t ) tCStems and the length of the suffix list is A ( ( r ) ) + L , po ( f ) , f ff Suffixes where LtvpoO is the measurement of the length of a string of letters in bits , which we take to be log 2 26 times the number of letters ( but recall note 18 ) .</sentence>
				<definiendum id="0">suffix list</definiendum>
				<definiendum id="1">LtvpoO</definiendum>
				<definiens id="0">The length in bits of the stem list is &amp; ( ( T ) ) + ~ Ltypo ( t ) tCStems and the length of the</definiens>
				<definiens id="1">the measurement of the length of a string of letters in bits</definiens>
			</definition>
			<definition id="7">
				<sentence>The length of the signature list is A ( ( ~ , ) ) + Z L ( ¢ ) , c~ ff Sign atures where L ( ~ ) is the length of signature or .</sentence>
				<definiendum id="0">length of the signature list</definiendum>
				<definiens id="0">A ( ( ~ , ) ) + Z L ( ¢ ) , c~ ff Sign atures where L ( ~ ) is the length of signature or</definiens>
			</definition>
			<definition id="8">
				<sentence>A signature ¢r has both a stem count ( the number of stems associated with it ) and an affix count ( the number of affixes it contains ) , and we use log ( stem count ) ~ log ( affix count ) as a rough guide to the centrality of a signature in the corpus .</sentence>
				<definiendum id="0">signature ¢r</definiendum>
				<definiendum id="1">affix count</definiendum>
				<definiens id="0">a rough guide to the centrality of a signature in the corpus</definiens>
			</definition>
			<definition id="9">
				<sentence>When suffix frequencies ( which are used to compute the compressed length of any analyzed word ) are based on the frequency of the suffixes in the entire lexicon , rather than conditionally within the signature in question , the loss of a signature entails a hit on the compression of all other words in the lexicon that employed that suffix ; hence triage is less dramatic under that modeling assumption .</sentence>
				<definiendum id="0">suffix frequencies</definiendum>
				<definiens id="0">used to compute the compressed length of any analyzed word ) are based on the frequency of the suffixes in the entire lexicon</definiens>
			</definition>
			<definition id="10">
				<sentence>Chomsky 's early conception of generative grammar ( Chomsky 1975 \ [ 1955\ ] ; henceforth LSLT ) was developed along these lines as well ; his notion of an evaluation metric for grammars was equivalent in its essential purpose to the description length of the morphology utilized in the present paper .</sentence>
				<definiendum id="0">LSLT</definiendum>
				<definiens id="0">the description length of the morphology utilized in the present paper</definiens>
			</definition>
			<definition id="11">
				<sentence>But they disagree with regard to two points , and on these points , MDL makes clearer , more explicit claims , and both claims appear to be strongly supported by the present study .</sentence>
				<definiendum id="0">MDL</definiendum>
				<definiens id="0">makes clearer , more explicit claims</definiens>
			</definition>
			<definition id="12">
				<sentence>191 Computational Linguistics Volume 27 , Number 2 UG is the shortest one of all the candidate English grammars , but the winning UG is all-round the supplier of the shortest grammars around the worldJ s MDL could be formulated in those terms , undoubtedly , but it also can be formulated in a language-particular fashion , which is how it has been used in this paper .</sentence>
				<definiendum id="0">UG</definiendum>
				<definiens id="0">the shortest one of all the candidate English grammars</definiens>
			</definition>
			<definition id="13">
				<sentence>192 Goldsmith Unsupervised Learning of the Morphology of a Natural Language ( 7 ) - ( 11 ) as offering us an exact and explicit statement of how a morphology can be improved .</sentence>
				<definiendum id="0">Goldsmith Unsupervised Learning of the Morphology of a Natural Language</definiendum>
				<definiens id="0">offering us an exact and explicit statement of how a morphology can be improved</definiens>
			</definition>
			<definition id="14">
				<sentence>Morphology : The Descriptive Analysis of Words .</sentence>
				<definiendum id="0">Morphology</definiendum>
			</definition>
</paper>

		<paper id="3003">
			<definition id="0">
				<sentence>Furthermore , it appears that the classes capture typological distinctions that are useful for machine translation ( for example , causative unergatives are ungrammatical in many languages ) , as well as processing distinctions that are useful for generating naturally occurring language ( for example , reduced relatives with unergative verbs are awkward , but they are acceptable , and in fact often preferred to full relatives for unaccusative and object-drop verbs ) ( Stevenson and Merlo 1997b ; Merlo and Stevenson 1998 ) .</sentence>
				<definiendum id="0">Merlo</definiendum>
				<definiens id="0">the classes capture typological distinctions that are useful for machine translation ( for example , causative unergatives are ungrammatical in many languages ) , as well as processing distinctions that are useful for generating naturally occurring language ( for example , reduced relatives with unergative verbs are awkward , but they are acceptable , and in fact often preferred to full relatives for unaccusative and object-drop verbs</definiens>
			</definition>
			<definition id="1">
				<sentence>We assume here that a thematic role is a label taken from a fixed inventory of grammaticalized semantic relations ; for example , an Agent is the doer of an action , and a Theme is the entity undergoing an event ( Gruber 1965 ) .</sentence>
				<definiendum id="0">Agent</definiendum>
				<definiens id="0">a label taken from a fixed inventory of grammaticalized semantic relations</definiens>
				<definiens id="1">the doer of an action</definiens>
				<definiens id="2">the entity undergoing an event</definiens>
			</definition>
			<definition id="2">
				<sentence>Unergatives are intransitive action verbs whose transitive form , as in ( lb ) , can be the causative counterpart of the intransitive form ( la ) .</sentence>
				<definiendum id="0">Unergatives</definiendum>
				<definiens id="0">intransitive action verbs whose transitive form , as in ( lb ) , can be the causative counterpart of the intransitive form ( la )</definiens>
			</definition>
			<definition id="3">
				<sentence>Transitive Intransitive Classes Subject Object Subject Unergative Agent ( of Causation ) Agent Agent Unaccusative Agent ( of Causation ) Theme Theme Object-Drop Agent Theme Agent introduced with the causing event .</sentence>
				<definiendum id="0">Transitive Intransitive</definiendum>
				<definiens id="0">Classes Subject Object Subject Unergative Agent ( of Causation ) Agent Agent Unaccusative Agent ( of Causation ) Theme Theme Object-Drop Agent Theme Agent introduced with the causing event</definiens>
			</definition>
			<definition id="4">
				<sentence>Unlike unergatives , though , the alternating argument of an unaccusative ( the subject of the intransitive form that becomes the object of the transitive ) is an entity undergoing a change of state , without active participation , and is therefore a Theme .</sentence>
				<definiendum id="0">unaccusative</definiendum>
				<definiens id="0">the subject of the intransitive form that becomes the object of the transitive ) is an entity undergoing a change of state</definiens>
			</definition>
			<definition id="5">
				<sentence>The thematic assignment for these verbs is simply Agent for the subject ( in both transitive and intransitive forms ) , and Theme for the optional object ; see the last row of Table 3 .</sentence>
				<definiendum id="0">thematic assignment</definiendum>
				<definiens id="0">the subject ( in both transitive and intransitive forms ) , and Theme for the optional object</definiens>
			</definition>
			<definition id="6">
				<sentence>382 Merlo and Stevenson Statistical Verb Classification The first three counts ( TRANS , PASS , VBN ) were performed on the tagged ACL/DCI corpus available from the Linguistic Data Consortium , which includes the Brown Corpus ( of one million words ) and years 1987-1989 of the Wall Street Journal , a combined corpus in excess of 65 million words .</sentence>
				<definiendum id="0">Linguistic Data Consortium</definiendum>
				<definiens id="0">includes the Brown Corpus ( of one million words</definiens>
			</definition>
			<definition id="7">
				<sentence>* PASS : A main verb ( i.e. , tagged VBD ) was counted as active .</sentence>
				<definiendum id="0">VBD</definiendum>
				<definiens id="0">A main verb</definiens>
			</definition>
			<definition id="8">
				<sentence>Here , the system is run N times , where N is the size of the data set ( i.e. , the 59 verbs in our case ) , each time holding out a single data vector as the test case and using the remaining N-1 vectors as the training set .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the size of the data set ( i.e. , the 59 verbs in our case ) , each time holding out a single data vector as the test case and using the remaining N-1 vectors as the training set</definiens>
			</definition>
			<definition id="9">
				<sentence>We can observe , for instance , that the feature CAUS , which performs very well alone , is average in feature combinations of size 3 or 4 .</sentence>
				<definiendum id="0">CAUS</definiendum>
				<definiens id="0">performs very well alone , is average in feature combinations of size 3 or 4</definiens>
			</definition>
			<definition id="10">
				<sentence>The verb is unusual for a manner-of-motion verb in that the action is inherently `` uncontrolled '' , and thus the subject of the intransitive/object of the transitive is a more passive entity than with the other unergatives ( perhaps indicating that the inventory of thematic roles should be refined to distinguish activity verbs with less agentive subjects ) .</sentence>
				<definiendum id="0">transitive</definiendum>
				<definiens id="0">perhaps indicating that the inventory of thematic roles should be refined to distinguish activity verbs with less agentive subjects</definiens>
			</definition>
			<definition id="11">
				<sentence>Unergative : A verb that assigns an agent theta role to the subject in the intransitive .</sentence>
				<definiendum id="0">Unergative</definiendum>
			</definition>
			<definition id="12">
				<sentence>Object-Drop : A verb that assigns an agent role to the subject and patient/theme role to the object , which is optional .</sentence>
				<definiendum id="0">Object-Drop</definiendum>
				<definiens id="0">A verb that assigns an agent role to the subject and patient/theme role to the object</definiens>
			</definition>
			<definition id="13">
				<sentence>Learnability and Cognition : the Acquisition of Argument Structure .</sentence>
				<definiendum id="0">Learnability</definiendum>
				<definiendum id="1">Cognition</definiendum>
			</definition>
			<definition id="14">
				<sentence>Selectional constraints : an information-theoretic model and its computational realization .</sentence>
				<definiendum id="0">Selectional constraints</definiendum>
				<definiens id="0">an information-theoretic model and its computational realization</definiens>
			</definition>
</paper>

		<paper id="4002">
			<definition id="0">
				<sentence>Based on this algorithm , the ROSANA system , which works on the partial syntactic descriptions generated by the robust FDG ( Functional Dependency Grammar of English ) parser of J~irvinen and Tapanainen ( 1997 ) , is implemented .</sentence>
				<definiendum id="0">ROSANA system</definiendum>
				<definiens id="0">works on the partial syntactic descriptions generated by the robust FDG ( Functional Dependency Grammar of English ) parser of J~irvinen</definiens>
			</definition>
			<definition id="1">
				<sentence>By referring to Chomsky 's Government and Binding ( GB ) Theory , the core of the syntactic coindexing restrictions may be stated as follows ( Chomsky 1981 ) : 4 Definition Binding principles A , B , and C : ( A ) A reflexive or reciprocal is bound in its binding category .</sentence>
				<definiendum id="0">Definition Binding</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">principles A , B , and</definiens>
			</definition>
			<definition id="2">
				<sentence>where binding category denotes the next surface-structural dominator containing some kind of subject , and binding is defined as coindexed and c-commanding : reference conditions are descriptive principles of grammar , the choice of the theoretical model is , in this sense , arbitrary .</sentence>
				<definiendum id="0">binding category</definiendum>
			</definition>
			<definition id="3">
				<sentence>As will 10 It is evident that there are cases in which the latter condition does not hold and the coindexing would violate binding principle C. 11 The following notational conventions are used : round brackets delimit constituents ; square brackets emphasize fragment boundaries ; bc ( X ) denotes the binding category of surface structure node X ; bn ( X ) denotes the branching node dominating X according to the c-command definition ; the subscript of Xtype y denotes that the binding-theoretic class of the occurrence contributed by X is Y C { A , B , C } ; for example , PtypeB is a pronoun .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">PtypeB</definiendum>
				<definiens id="0">a pronoun</definiens>
			</definition>
			<definition id="4">
				<sentence>Select 3 '' j ( c~ ) as candidate if there is no interdependency , that is , if ( a ) the morphosyntactic features of c~ and `` ~j ( c~ ) are still compatible , ( b ) for all occurrences ~j ( ~ ) and 6~ the coindexing of which with 3 '' j ( c~ ) and ( respectively ) c~ has been determined in the current invocation of the algorithm : the coindexing of ~ , j ( ~ and ~ , which results transitively when choosing Vj ( c~ ) as antecedent for c~ , violates neither the binding principles nor the i-within-i condition ; that is , • if ~ , j/~ and 6~ belong to the same syntactic fragment , then , for both occurrences , verify the respective binding conditions and the i-within-i condition according to steps l ( b ) ii and l ( b ) iii , • else if 6~j/~ ) and 6~ belong to different syntactic fragments , then proceed according to steps l ( b ) iv , l ( b ) v , l ( b ) vi , and l ( b ) vii ( with the exception of the rule patterns \ [ F2\ ] , \ [ E2\ ] , and \ [ E4I , by means of which binding principle A is constructively verified ) .</sentence>
				<definiendum id="0">i-within-i condition</definiendum>
				<definiens id="0">the coindexing of which with 3 '' j ( c~ ) and ( respectively ) c~ has been determined in the current invocation of the algorithm : the coindexing of ~</definiens>
			</definition>
			<definition id="5">
				<sentence>The FDG parser for English developed by Jarvinen and Tapanainen ( 1997 ) has been chosen as the syntactic preprocessor , is In giving robustness and processing speed priority over normativity and syntactic coverage of the underlying grammar , the parser meets the requirements on a preprocessor for robust anaphor resolution on unrestricted texts29 Regardless of the typical parsing problems like structurally ambiguous or grammatically incorrect input , the parser always yields a result , comprising one or more syntactic fragments that cover the analyzed sentence .</sentence>
				<definiendum id="0">FDG parser</definiendum>
				<definiens id="0">comprising one or more syntactic fragments that cover the analyzed sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>18 Since the parser generates dependency descriptions rather than constituent structure ( to which the formal definitions of the above GB-theoretic statement of syntactic disjoint reference refer ) , ROSANA applies a preprocessor that reconstructs the structural ( e.g. , subject-object ) asymmetries of constituency that are vital to the verification of the disjoint reference conditions .</sentence>
				<definiendum id="0">ROSANA</definiendum>
			</definition>
			<definition id="7">
				<sentence>In the ROSANA system , the following factors are employed : SYR ( contributed by occurrences with identical syntactic function ) , EEP ( occUrrences realized in the syntactic position of existential emphasis ) , SUP ( syntactic subject ) , PGP ( possessive pronouns , saxonian genitives , and genitive attributes ) , DOP/IOP/APP ( salience of direct/indirect objects and adverbial PPs , respectively ) , KAM ( negative preference of cataphoric resumptions ) , SDM ( sentence recency ; i.e. , a factor of negative salience to be multiplied with the sentence distance between anaphor and antecedent ) , WDM ( word recency ) .</sentence>
				<definiendum id="0">EEP ( occUrrences</definiendum>
				<definiendum id="1">KAM</definiendum>
				<definiens id="0">realized in the syntactic position of existential emphasis ) , SUP ( syntactic subject )</definiens>
			</definition>
			<definition id="8">
				<sentence>Formally , let R s and R k be the coreference relations computed by the anaphor resolution system and specified by the key , respectively ; moreover , let \ [ R s\ ] and \ [ Rk\ ] be the respective sets of equivalence classes .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">the coreference relations computed by the anaphor resolution system and specified by the key , respectively</definiens>
			</definition>
			<definition id="9">
				<sentence>For each class C , there is a maximum of ICN O\ ] 1 correct contributions ; the actual number of errors , which equals the number of equivalence classes \ ] \ [ q~ ( C , R ) \ ] \ [ of the restricted relation minus 1 , has to be deducted .</sentence>
				<definiendum id="0">errors</definiendum>
			</definition>
			<definition id="10">
				<sentence>icism is the lack of expressiveness regarding the different typical subproblems to be solved by coreference resolution systems .</sentence>
				<definiendum id="0">icism</definiendum>
				<definiens id="0">the lack of expressiveness regarding the different typical subproblems to be solved by coreference resolution systems</definiens>
			</definition>
</paper>

		<paper id="4004">
			<definition id="0">
				<sentence>Coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world .</sentence>
				<definiendum id="0">Coreference resolution</definiendum>
				<definiens id="0">the process of determining whether two expressions in natural language refer to the same entity in the world</definiens>
			</definition>
			<definition id="1">
				<sentence>Specifically , a coreference relation denotes an identity of reference and holds between two textual elements known as markables , which can be definite noun phrases , demonstrative noun phrases , proper names , appositives , sub-noun phrases that act as modifiers , pronouns , and so on .</sentence>
				<definiendum id="0">coreference relation</definiendum>
				<definiens id="0">an identity of reference and holds between two textual elements known as markables , which can be definite noun phrases , demonstrative noun phrases , proper names , appositives , sub-noun phrases that act as modifiers , pronouns , and so on</definiens>
			</definition>
			<definition id="2">
				<sentence>Our feature vector consists of a total of 12 features described below , and is derived based on two extracted markables , i and j , where i is the potential antecedent and j is the anaphor .</sentence>
				<definiendum id="0">feature vector</definiendum>
				<definiendum id="1">j</definiendum>
				<definiens id="0">consists of a total of 12 features described below , and is derived based on two extracted markables</definiens>
			</definition>
			<definition id="3">
				<sentence>A demonstrative noun phrase is one that starts with the word this , that , these , or those .</sentence>
				<definiendum id="0">demonstrative noun phrase</definiendum>
				<definiens id="0">one that starts with the word this</definiens>
			</definition>
			<definition id="4">
				<sentence>If the selected semantic class of a markable is a subclass of one of our defined semantic classes C , then the semantic class of the markable is C ; else its semantic class is `` unknown . ''</sentence>
				<definiendum id="0">semantic class of a markable</definiendum>
			</definition>
			<definition id="5">
				<sentence>The learning algorithm used in our coreference engine is C5 , which is an updated version of C4.5 ( Quinlan 1993 ) .</sentence>
				<definiendum id="0">C5</definiendum>
			</definition>
			<definition id="6">
				<sentence>Metaphor , a software subsidiary that IBM purchased in 1991 , also named ( Chris Grejtak ) l , ( 43 years old ) 2 , currently a senior vice president , president and chief executive officer .</sentence>
				<definiendum id="0">Metaphor</definiendum>
				<definiens id="0">a software subsidiary that IBM purchased in 1991</definiens>
			</definition>
			<definition id="7">
				<sentence>540 Soon , Ng , and Lira Coreference Resolution ( 18 ) ( 19 ) ( Metaphorh , a software subsidiary that IBM purchased in 1991 , also named Chris Grejtak , ... Mr. Grejtak said in an interview that the staff reductions will affect most areas of ( the company ) 2 related to its early proprietary software products .</sentence>
				<definiendum id="0">Metaphorh</definiendum>
				<definiendum id="1">Mr. Grejtak</definiendum>
				<definiens id="0">a software subsidiary that IBM purchased in 1991 , also named Chris Grejtak , ...</definiens>
			</definition>
			<definition id="8">
				<sentence>To our knowledge , the research efforts of Aone and Bennett ( 1995 ) , Ge , Hale , and Charniak ( 1998 ) , Kehler ( 1997 ) , McCarthy and Lehnert ( 1995 ) , Fisher et al. ( 1995 ) , and McCarthy ( 1996 ) are the only ones that are based on learning from an annotated corpus .</sentence>
				<definiendum id="0">McCarthy</definiendum>
				<definiens id="0">the only ones that are based on learning from an annotated corpus</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>In the TDT framework , the UMass HMM approach described in Allan et al. ( 1998 ) uses an HMM that models the initial , middle , and final sentences of a topic segment , capitalizing on discourse cue words that indicate beginnings and ends of segments .</sentence>
				<definiendum id="0">HMM</definiendum>
				<definiens id="0">models the initial , middle , and final sentences of a topic segment , capitalizing on discourse cue words that indicate beginnings and ends of segments</definiens>
			</definition>
			<definition id="1">
				<sentence>Prosodic cues form a subset of discourse cues in speech , reflecting systematic duration , pitch , and energy patterns at topic changes and related locations of interest .</sentence>
				<definiendum id="0">Prosodic cues</definiendum>
				<definiens id="0">form a subset of discourse cues in speech , reflecting systematic duration , pitch , and energy patterns at topic changes and related locations of interest</definiens>
			</definition>
			<definition id="2">
				<sentence>They used automatically extracted pitch , energy , and `` other '' features ( such as the cross-correlation value used by the pitch tracker in determining the estimate of F0 ) as inputs to CART-style trees , and aimed to predict major discourse-level boundaries .</sentence>
				<definiendum id="0">F0 )</definiendum>
				<definiens id="0">such as the cross-correlation value used by the pitch tracker in determining the estimate of</definiens>
			</definition>
			<definition id="3">
				<sentence>Furthermore , our two knowledge sources are the ( chopped ) word sequence W and the stream of prosodic features F. Our approach aims to find the segmentation B with highest probability given the information in W and F argmax P ( BI W , F ) ( 1 ) B using statistical modeling techniques .</sentence>
				<definiendum id="0">knowledge sources</definiendum>
				<definiens id="0">aims to find the segmentation B with highest probability given the information in W and F argmax P ( BI W</definiens>
			</definition>
			<definition id="4">
				<sentence>Entropy reduction is the difference in entropy between the prior class distribution and the posterior distribution estimated by the tree , as measured on a held-out set ; it is a more fine-grained metric than classification accuracy , and is also more relevant to the model combination approach described later .</sentence>
				<definiendum id="0">Entropy reduction</definiendum>
				<definiens id="0">the difference in entropy between the prior class distribution and the posterior distribution estimated by the tree</definiens>
			</definition>
			<definition id="5">
				<sentence>TSP represents the topic switch penalty .</sentence>
				<definiendum id="0">TSP</definiendum>
				<definiens id="0">the topic switch penalty</definiens>
			</definition>
			<definition id="6">
				<sentence>TSP represents the topic switch penalty .</sentence>
				<definiendum id="0">TSP</definiendum>
				<definiens id="0">the topic switch penalty</definiens>
			</definition>
			<definition id="7">
				<sentence>• , Bc denote the topic boundary states and , similarly , let N1 , ... , Nc denote the nontopic boundary states , where C is the number of topic clusters .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">let N1 , ... , Nc denote the nontopic boundary states</definiens>
				<definiens id="1">the number of topic clusters</definiens>
			</definition>
			<definition id="8">
				<sentence>TSP is the topic switch penalty .</sentence>
				<definiendum id="0">TSP</definiendum>
				<definiens id="0">the topic switch penalty</definiens>
			</definition>
			<definition id="9">
				<sentence>Recall that , like Wi , Fi refers to complete sentence units ; specifically , Fi denotes the prosodic features of the ith boundary between such units .</sentence>
				<definiendum id="0">Fi</definiendum>
				<definiendum id="1">Fi</definiendum>
				<definiens id="0">the prosodic features of the ith boundary between such units</definiens>
			</definition>
			<definition id="10">
				<sentence>We used two test conditions : forced alignments using the true words , and recognized words as obtained by a simplified version of the SRI Broadcast News recognizer ( Sankar et al. 1998 ) , with a word error rate of 30.5 % .</sentence>
				<definiendum id="0">Broadcast News recognizer</definiendum>
				<definiens id="0">obtained by a simplified version of the SRI</definiens>
			</definition>
			<definition id="11">
				<sentence>Segmentation accuracy was measured using TDT evaluation software from NIST , which implements a variant of an evaluation metric suggested by Beeferman , Berger , and Lafferty ( 1999 ) .</sentence>
				<definiendum id="0">Segmentation accuracy</definiendum>
				<definiens id="0">implements a variant of an evaluation metric suggested by Beeferman</definiens>
			</definition>
			<definition id="12">
				<sentence>M=I \ -dShyp ( l '' i + k ) ) x d~ef ( Z , ~ + k ) ( 9 ) v- , N~-k as ( i i + k ) Es Z.~i=l ~ref \ `` where the summation is over all broadcast shows s and word positions i in the test corpus and where d sli , ,= { i if words i and j in show s are deemed by sys to be within the same story otherwise Here sys can be ref to denote the reference ( correct ) segmentation , or hyp to denote the segmenter 's decision .</sentence>
				<definiendum id="0">M=I \ -dShyp</definiendum>
			</definition>
			<definition id="13">
				<sentence>F0 of the word preceding the boundary ( measured from voiced regions within that word ) to either the speaker 's estimated baseline F0 ( FOK_LR_MEAN_KBASELN ) or to the mean F0 of the word following the boundary ( FOK_WRD_DIFF_ .</sentence>
				<definiendum id="0">boundary</definiendum>
			</definition>
			<definition id="14">
				<sentence>Notably , the segmentation accuracy of the prosodic 54 Tier , Hakkani-T ( ir , Stolcke , and Shriberg Integrating Prosodic and Lexical Cues model alone is competitive with a word-based segmenter , and a combined prosodic/ lexical HMM achieves a substantial error reduction over the individual knowledge sources .</sentence>
				<definiendum id="0">segmentation accuracy</definiendum>
				<definiendum id="1">HMM</definiendum>
			</definition>
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>The SENSEVAL evaluation framework ( Kilgarriff 1998 ) was a DARPA-style competition designed to bring some conformity to the field of WSD , although it has yet to achieve that aim completely .</sentence>
				<definiendum id="0">SENSEVAL evaluation framework</definiendum>
				<definiens id="0">a DARPA-style competition designed to bring some conformity to the field of WSD</definiens>
			</definition>
			<definition id="1">
				<sentence>A third difference concerns the granularity of WSD attempted , which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries : homograph and sense ( see Section 3.1 ) .</sentence>
				<definiendum id="0">third difference</definiendum>
				<definiens id="0">concerns the granularity of WSD attempted , which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries : homograph and sense</definiens>
			</definition>
			<definition id="2">
				<sentence>Disambiguation makes use of several knowledge sources : frequency information , syntactic tags , morphological information , semantic context ( clusters ) , collocations and word associations , role-related expectations , and selectional restrictions .</sentence>
				<definiendum id="0">Disambiguation</definiendum>
				<definiens id="0">makes use of several knowledge sources : frequency information , syntactic tags , morphological information , semantic context ( clusters ) , collocations and word associations , role-related expectations , and selectional restrictions</definiens>
			</definition>
			<definition id="3">
				<sentence>Yarowsky ( 1995 ) dealt with this problem largely by producing an unsupervised learning algorithm that generates probabilistic decision list models of word senses from seed collocates .</sentence>
				<definiendum id="0">unsupervised learning algorithm</definiendum>
				<definiens id="0">generates probabilistic decision list models of word senses from seed collocates</definiens>
			</definition>
			<definition id="4">
				<sentence>Their system is known as LEXAS ( LEXical Ambiguity-resolving System ) , a supervised learning approach which requires disambiguated training text .</sentence>
				<definiendum id="0">LEXAS ( LEXical Ambiguity-resolving System</definiendum>
				<definiens id="0">a supervised learning approach which requires disambiguated training text</definiens>
			</definition>
			<definition id="5">
				<sentence>For example , we have not discussed connectionist approaches , as used by Waltz and Pollack ( 1985 ) , V6ronis and Ide ( 1990 ) , Hirst ( 1987 ) , and Cottrell ( 1984 ) , However , we have attempted to discuss some of the approaches to combining diverse types of linguistic knowledge for WSD and have concentrated on those which are related to the techniques used in our own disambiguation system .</sentence>
				<definiendum id="0">connectionist approaches</definiendum>
				<definiens id="0">attempted to discuss some of the approaches to combining diverse types of linguistic knowledge for WSD and have concentrated on those which are related to the techniques used in our own disambiguation system</definiens>
			</definition>
			<definition id="6">
				<sentence>LDOCE is a learners ' dictionary , designed for students of English , containing roughly 36,000 word types .</sentence>
				<definiendum id="0">LDOCE</definiendum>
				<definiens id="0">a learners ' dictionary , designed for students of English , containing roughly 36,000 word types</definiens>
			</definition>
			<definition id="7">
				<sentence>For example , one of the homographs of bank means 326 Stevenson and Wilks Interaction of Knowledge Sources in WSD bank 1 n I land along the side of a river , lake , etc. 2 earth which is heaped up in a field or a garden , often making a border or division 3 a mass of snow , mud , clouds , etc. : The banks of dark cloud promised a heavy storln 4 a slope made at bends in a road or race-track , so that they are safer for cars to go round 5 SANDBANK : The Dogger Bank in the North Sea can be dangerous for ships bank 2 v \ [ If~\ ] ( of a car or aircraft ) to move with one side higher than the other , esp .</sentence>
				<definiendum id="0">etc.</definiendum>
				<definiendum id="1">Dogger Bank</definiendum>
				<definiens id="0">a mass of snow , mud , clouds ,</definiens>
			</definition>
			<definition id="8">
				<sentence>This system currently incorporates a single filter ( part-of-speech filter ) , three partial taggers ( simulated annealing , subject codes , selectional restrictions ) and a single feature extractor ( collocation extractor ) .</sentence>
				<definiendum id="0">collocation extractor</definiendum>
				<definiens id="0">part-of-speech filter ) , three partial taggers ( simulated annealing , subject codes , selectional restrictions</definiens>
			</definition>
			<definition id="9">
				<sentence>LDOCE senses are marked with selectional restrictions expressed by 36 semantic codes not ordered in a hierarchy .</sentence>
				<definiendum id="0">LDOCE senses</definiendum>
				<definiens id="0">selectional restrictions expressed by 36 semantic codes not ordered in a hierarchy</definiens>
			</definition>
			<definition id="10">
				<sentence>Sense Definition and Example Restriction John ran ( 1 ) ran ( 2 ) hilly ( 1 ) course ( 1 ) course ( 2 ) proper name to control an organisation run IBM to move quickly by foot run a marathon undulating terrain hilly road route race course programme of study physics course type : human subject : human object : abstract subject : human object : inanimate modifies : nonmovable solid type : noumovable solid type : abstract run ( l ) restriction : human restriction : abstract John course ( 2 ) { run ( 1 ) , run ( 2 ) } ~bject-~b I John { course ( 1 ) , course ( 2 ) } f I adjective-noun~ I { hilly ( l ) } run ( 2 ) restriction : human restriction : inanimate John course ( I ) type : nonmovable solid hilly ( l ) Figure 4 Restriction resolution in toy example .</sentence>
				<definiendum id="0">Sense Definition</definiendum>
				<definiens id="0">human subject : human object : abstract subject : human object : inanimate modifies : nonmovable solid type : noumovable solid type : abstract run ( l ) restriction : human restriction</definiens>
			</definition>
			<definition id="11">
				<sentence>As this was not the case , we consider the direct-object slot , which places the restriction abstract on the noun which fills it .</sentence>
				<definiendum id="0">direct-object slot</definiendum>
			</definition>
			<definition id="12">
				<sentence>In Yarowsky 's implementation , the correct subject category is estimated by applying ( 6 ) , which maximizes the sum of a Bayesian term ( the fraction on the right ) over all possible subject categories ( SCat ) for the ambiguous word over the words in its context ( w ) .</sentence>
				<definiendum id="0">SCat</definiendum>
				<definiens id="0">maximizes the sum of a Bayesian term ( the fraction on the right</definiens>
			</definition>
			<definition id="13">
				<sentence>ARGMAX Pr ( w\ [ S Cat ) Pr ( SCat ) scat ~ log Pr ( w ) ( 6 ) w e context Yarowsky assumed the prior probability of each subject category to be constant , so the value Pr ( SCat ) has no effect on the maximization in ( 6 ) , and ( 7 ) was in effect being maximized .</sentence>
				<definiendum id="0">ARGMAX Pr</definiendum>
				<definiens id="0">w e context Yarowsky assumed the prior probability of each subject category to be constant , so the value Pr ( SCat ) has no effect on the maximization in ( 6 ) , and ( 7 ) was in effect being maximized</definiens>
			</definition>
			<definition id="14">
				<sentence>Memory-based learning is another name for exemplar-based learning , as employed by Ng and Lee ( Section 2.3 ) .</sentence>
				<definiendum id="0">Memory-based learning</definiendum>
			</definition>
			<definition id="15">
				<sentence>337 Computational Linguistics Volume 27 , Number 3 Like PEBLS , which formed the core of Ng and Lee 's LEXAS system , TiMBL classifies new examples by comparing them against previously seen cases .</sentence>
				<definiendum id="0">TiMBL</definiendum>
				<definiens id="0">formed the core of Ng and Lee 's LEXAS system</definiens>
			</definition>
			<definition id="16">
				<sentence>H ( C ) ~- , v Pr ( v ) x H ( CIv ) ( 10 ) wi = H ( v ) Where C is the set of classifications , v ranges over all values of the feature i and H ( C ) is the entropy of the class labels .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">the set of classifications , v ranges over all values of the feature i and H ( C ) is the entropy of the class labels</definiens>
			</definition>
			<definition id="17">
				<sentence>SENSUS is a large-scale ontology designed for machine-translation and was itself produced by merging the ontological hierarchies of WordNet , LDOCE ( as derived by Bruce and Guthrie , see Section 4.4 ) , and the Penman Upper Model ( Bateman et al. , 1990 ) from ISI .</sentence>
				<definiendum id="0">SENSUS</definiendum>
				<definiens id="0">a large-scale ontology designed for machine-translation and was itself produced by merging the ontological hierarchies of WordNet</definiens>
			</definition>
			<definition id="18">
				<sentence>This is represented by ( 11 ) , where w ranges over all ambiguous tokens in the corpus , S ( w ) is the number of possible senses for word w , and N is the number of ambiguous tokens .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of possible senses for word w</definiens>
				<definiens id="1">the number of ambiguous tokens</definiens>
			</definition>
			<definition id="19">
				<sentence>The most commonly used metric is the ratio of words for which the system has assigned the correct sense compared to those which it attempted to disambiguate .</sentence>
				<definiendum id="0">metric</definiendum>
				<definiens id="0">the ratio of words for which the system has assigned the correct sense compared to those which it attempted to disambiguate</definiens>
			</definition>
			<definition id="20">
				<sentence>The formula in ( 13 ) shows the method for computing this metric , where the WSD system has processed N words and Pr ( csi ) is the probability assigned to the correct sense of word i. N 1 N ~ l°g2 Pr ( csi ) ( 13 ) i=1 This evaluation metric may be useful for disambiguation systems that assign probabilities to each sense , such as those developed by Resnik and Yarowsky , since it provides more information than the exact match metric .</sentence>
				<definiendum id="0">evaluation metric</definiendum>
				<definiens id="0">those developed by Resnik and Yarowsky , since it provides more information than the exact match metric</definiens>
			</definition>
			<definition id="21">
				<sentence>SENSEVAL : An Exercise in Evaluating Word Sense Disambiguation Programs .</sentence>
				<definiendum id="0">SENSEVAL</definiendum>
			</definition>
			<definition id="22">
				<sentence>In C. Fellbaum , editor , WordNet : An Electronic Lexical Database and Some Applications .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

	</volume>
