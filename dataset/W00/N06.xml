<?xml version="1.0" encoding="UTF-8"?>
	<volume id="N06">

		<paper id="2041">
			<definition id="0">
				<sentence>Acronyms are short forms of multiword expressions ( we call them definitions ) that are very convenient and commonly used , and are constantly invented independently everywhere .</sentence>
				<definiendum id="0">Acronyms</definiendum>
				<definiens id="0">short forms of multiword expressions ( we call them definitions ) that are very convenient and commonly used , and are constantly invented independently everywhere</definiens>
			</definition>
			<definition id="1">
				<sentence>However , our proposal , Web-Based Language Modeling ( Sarikaya , 2005 ) , and Bootstrapping Large Sense-Tagged corpora ( Mihalcea , 2002 ) use the content within the hit pages .</sentence>
				<definiendum id="0">Web-Based Language Modeling</definiendum>
				<definiendum id="1">Bootstrapping Large Sense-Tagged corpora</definiendum>
				<definiens id="0">use the content within the hit pages</definiens>
			</definition>
</paper>

		<paper id="2016">
			<definition id="0">
				<sentence>The document collection for the CL-SR task is a part of the oral testimonies collected by the USC Shoah Foundation Institute for Visual History and Education ( VHI ) for which some Automatic Speech Recognition ( ASR ) transcriptions are available ( Oard et al. , 2004 ) .</sentence>
				<definiendum id="0">document collection</definiendum>
			</definition>
			<definition id="1">
				<sentence>MAP , indexing all fields ( MK , summaries , ASR transcripts , AK1 and AK2 ) , 25 test topics .</sentence>
				<definiendum id="0">AK1</definiendum>
				<definiens id="0">summaries , ASR transcripts</definiens>
			</definition>
</paper>

		<paper id="4004">
			<definition id="0">
				<sentence>The MTTK alignment toolkit for statistical machine translation can be used for word , phrase , and sentence alignment of parallel documents .</sentence>
				<definiendum id="0">MTTK alignment toolkit</definiendum>
				<definiens id="0">word , phrase , and sentence alignment of parallel documents</definiens>
			</definition>
			<definition id="1">
				<sentence>Alignment quality can be further improved when the chunking procedure is based on translation lexicons from IBM Model-1 alignment model ( Brown et al. , 1993 ) .</sentence>
				<definiendum id="0">Alignment quality</definiendum>
			</definition>
			<definition id="2">
				<sentence>1http : //www.opensource.org/licenses/ecl1.php 267 { t ( f|e ) , a ( i|j ; l , m ) } WtW HMM Training { t ( f|e ) , P ( i|i’ ; l ) } AlignSHmm { t ( f|e ) , P ( i|i’ ; l ) , n ( phi ; e ) , t2 ( f|f’ , e ) } Model−1 Training { t ( f|e ) } Model−2 Training WtP HMM Training w/ Bigram t−table { t ( f|e ) , P ( i|i’ ; l ) , n ( phi ; e ) } WtP HMM Training N=2,3 ... Length Statistics AlignHmm AlignM2 AlignM1 AlignSHmm PPEM PPEM PPEHmm PPEHmm PPEHmm High Quality Pairs Model−1 Training { t ( f|e ) } FilterChunk Alignments Word AlignmentsPhrase Alignments Document Alignments Bitext Chunking Figure 2 : A Typical Unsupervised Translation Alignment Procedure with MTTK .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">A Typical Unsupervised Translation Alignment Procedure with MTTK</definiens>
			</definition>
</paper>

		<paper id="2047">
			<definition id="0">
				<sentence>Grammatical relations ( GR ) refer to linkages such as subject , object , modi er , etc .</sentence>
				<definiendum id="0">Grammatical relations</definiendum>
			</definition>
			<definition id="1">
				<sentence>The Remedia corpus contains 55 training stories and 60 testing stories ( about 20K words ) .</sentence>
				<definiendum id="0">Remedia corpus</definiendum>
				<definiens id="0">contains 55 training stories and 60 testing stories ( about 20K words )</definiens>
			</definition>
			<definition id="2">
				<sentence>The ChungHwa corpus contains 50 training stories and 50 test stories ( about 18K words ) .</sentence>
				<definiendum id="0">ChungHwa corpus</definiendum>
				<definiens id="0">contains 50 training stories and 50 test stories ( about 18K words )</definiens>
			</definition>
			<definition id="3">
				<sentence>Equation 1 can be computed by the ME method ( Zhou et al. , 2003 ) : p ( y|x ) = 1Z ( x ) exp summationtext j λjfj ( x , y ) , ( 2 ) where Z ( x ) = summationtexty exp summationtext j λjfj ( x , y ) is a normalization factor , fj ( x , y ) is the indicator function for feature fj ; fj occurs in the context x , λj is the weight of fj .</sentence>
				<definiendum id="0">y )</definiendum>
				<definiendum id="1">fj</definiendum>
				<definiendum id="2">λj</definiendum>
				<definiens id="0">Zhou et al. , 2003 ) : p ( y|x ) = 1Z ( x ) exp summationtext j λjfj ( x , y ) , ( 2 ) where Z ( x ) = summationtexty exp summationtext j λjfj</definiens>
				<definiens id="1">a normalization factor</definiens>
				<definiens id="2">the indicator function for feature fj ; fj occurs in the context x</definiens>
				<definiens id="3">the weight of fj</definiens>
			</definition>
			<definition id="4">
				<sentence>In a lexicalized syntactic parse tree , a dependency can be de ned as : &lt; hc → hp &gt; or &lt; hr → TOP &gt; , where hc is the headword of the child node , hp is the headword of the parent node ( hc negationslash= hp ) , hr is the headword of the root node .</sentence>
				<definiendum id="0">hc</definiendum>
				<definiendum id="1">hp</definiendum>
				<definiens id="0">the headword of the child node</definiens>
				<definiens id="1">the headword of the parent node ( hc negationslash= hp ) , hr is the headword of the root node</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>Blaheta and Charniak proposed a statistical method ∗Currently , National Institute of Information and Communications Technology , JAPAN , dk @ nict.go.jp †Currently , Graduate School of Informatics , Kyoto University , kuro @ i.kyoto-u.ac.jp for analyzing function tags in Penn Treebank , and achieved a really high accuracy of 95.7 % for syntactic roles , such as SBJ ( subject ) and DTV ( dative ) ( Blaheta and Charniak , 2000 ) .</sentence>
				<definiendum id="0">DTV</definiendum>
			</definition>
			<definition id="1">
				<sentence>For example , let us describe how to apply case structure analysis to the following sentence : bentou-wa taberu lunchbox-TM eat ( eat lunchbox ) In this sentence , taberu ( eat ) is a verb , and bentouwa ( lunchbox-TM ) is a case component ( i.e. argument ) of taberu .</sentence>
				<definiendum id="0">eat )</definiendum>
				<definiendum id="1">bentouwa</definiendum>
				<definiens id="0">a verb</definiens>
				<definiens id="1">a case component ( i.e. argument ) of taberu</definiens>
			</definition>
			<definition id="2">
				<sentence>P ( T , L , S ) is defined as the product of a probability for generating a clause Ci as follows : P ( T , L , S ) = productdisplay i=1 .</sentence>
				<definiendum id="0">P ( T , L , S )</definiendum>
				<definiens id="0">the product of a probability for generating a clause Ci as follows : P ( T , L , S ) = productdisplay i=1</definiens>
			</definition>
			<definition id="3">
				<sentence>A case structure CSi consists of a predicate vi , a case frame CFl and a case assignment CAk .</sentence>
				<definiendum id="0">case structure CSi</definiendum>
				<definiens id="0">consists of a predicate vi , a case frame CFl and a case assignment CAk</definiens>
			</definition>
			<definition id="4">
				<sentence>Taking into account these assumptions , the generativeprobabilityofacasecomponentisapproximated as follows : P ( nj , fj|CFl , fi , A ( sj ) = 1 , sj ) ≈ P ( nj|CFl , A ( sj ) = 1 , sj ) P ( fj|sj , fi ) ( 8 ) P ( nj|CFl , A ( sj ) = 1 , sj ) is the probability of generating a content part nj from a case slot sj in a case frame CFl .</sentence>
				<definiendum id="0">sj )</definiendum>
				<definiens id="0">the probability of generating a content part nj from a case slot sj in a case frame CFl</definiens>
			</definition>
			<definition id="5">
				<sentence>When bhi is a predicate bunsetsu , Ci is a subordinate clause embedded in the clause of bhi .</sentence>
				<definiendum id="0">bhi</definiendum>
				<definiendum id="1">Ci</definiendum>
				<definiens id="0">a predicate bunsetsu</definiens>
			</definition>
			<definition id="6">
				<sentence>In the table , “baseline” means the rule-based syntactic parser , KNP ( Kurohashi and Nagao , 1994 ) , and “proposed” represents the proposed method .</sentence>
				<definiendum id="0">“proposed”</definiendum>
				<definiens id="0">the rule-based syntactic parser</definiens>
			</definition>
</paper>

		<paper id="2043">
			<definition id="0">
				<sentence>Trouble tickets exhibit a special discourse structure , combining systemgenerated , structured data and free-text sections ; a special lexicon , full of acronyms , abbreviations and symbols ; and consistent “bending” of grammar rules in favor of speed writing ( Johnson , 1992 ; Marlow , 2004 ) .</sentence>
				<definiendum id="0">Trouble tickets</definiendum>
				<definiens id="0">exhibit a special discourse structure , combining systemgenerated , structured data and free-text sections ; a special lexicon , full of acronyms , abbreviations and symbols</definiens>
			</definition>
			<definition id="1">
				<sentence>Two Biomedical Sublanguages : a Description Based on the Theories of Zellig Harris .</sentence>
				<definiendum id="0">Biomedical Sublanguages</definiendum>
				<definiens id="0">a Description Based on the Theories of Zellig Harris</definiens>
			</definition>
			<definition id="2">
				<sentence>Investigating Technical Trouble Tickets : An Analysis of a Homely CMC Genre .</sentence>
				<definiendum id="0">Investigating Technical Trouble Tickets</definiendum>
			</definition>
			<definition id="3">
				<sentence>, Computers and Translation : A translator 's guide .</sentence>
				<definiendum id="0">Translation</definiendum>
				<definiens id="0">A translator 's guide</definiens>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>A standard test collection consists of lots of documents , a set of information needs , called topics and human judgment about the relevance status of each document for a topic .</sentence>
				<definiendum id="0">standard test collection</definiendum>
				<definiens id="0">consists of lots of documents , a set of information needs , called topics and human judgment about the relevance status of each document for a topic</definiens>
			</definition>
			<definition id="1">
				<sentence>The training process consists of two main steps .</sentence>
				<definiendum id="0">training process</definiendum>
				<definiens id="0">consists of two main steps</definiens>
			</definition>
			<definition id="2">
				<sentence>Ranking SVM ( Joachims , 2002b ) , rSVM for short , is a straightforward adaptation of the maxmargin principle ( Vapnik , 2000 ) to pairwise object ranking .</sentence>
				<definiendum id="0">rSVM</definiendum>
			</definition>
			<definition id="3">
				<sentence>The score function is often assumed to be linear in some feature space , that is H ( d ) = wTΨ ( d ) where w is the vector of weights to be estimated and Ψ is a feature mapping .</sentence>
				<definiendum id="0">w</definiendum>
				<definiendum id="1">Ψ</definiendum>
				<definiens id="0">the vector of weights to be estimated and</definiens>
				<definiens id="1">a feature mapping</definiens>
			</definition>
			<definition id="4">
				<sentence>Each document is described by an N-dimensional feature vector where N is the number of participating systems .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of participating systems</definiens>
			</definition>
			<definition id="5">
				<sentence>The non-interpolated average precision ( MAP ) has been chosen to measure system performance5 .</sentence>
				<definiendum id="0">non-interpolated average precision</definiendum>
			</definition>
			<definition id="6">
				<sentence>In all figures in the next section , the abscissa denotes the pool size m and values of n will be present along the Depth-n curve .</sentence>
				<definiendum id="0">abscissa</definiendum>
				<definiens id="0">the pool size m and values of n will be present along the Depth-n curve</definiens>
			</definition>
</paper>

		<paper id="2006">
			<definition id="0">
				<sentence>Important sentences are first extracted according to the following score for each sentence W = w1 , w2 , ... , wn , obtained from the automatic speech recognition output : S ( W ) = 1N Nsummationdisplay i=1 { αCC ( wi ) +αII ( wi ) +αLL ( wi ) } , ( 1 ) where N is the number of words in the sentence W , and C ( wi ) , I ( wi ) and L ( wi ) are the confidence score , the significance score and the linguistic score of word wi , respectively .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">L</definiendum>
				<definiens id="0">w1 , w2 , ... , wn , obtained from the automatic speech recognition output : S ( W ) = 1N Nsummationdisplay i=1 { αCC ( wi ) +αII ( wi ) +αLL ( wi ) }</definiens>
				<definiens id="1">the number of words in the sentence W , and C ( wi )</definiens>
			</definition>
			<definition id="1">
				<sentence>C ( wi−1 ) ) , ( 3 ) where Pk ( wi|C ( wi ) ) is the probability of the word wi belonging to a given class C , and Pk ( C ( wi ) |C ( wi−n+1 ) .</sentence>
				<definiendum id="0">Pk ( wi|C</definiendum>
				<definiens id="0">the probability of the word wi belonging to a given class C</definiens>
			</definition>
			<definition id="2">
				<sentence>The word accuracy of automatic summarisation is calculated as the summarisation accuracy ( SumACCY ) using the word network ( Hori et al. , 2003 ) : Accuracy = ( Len−Sub−Ins−Del ) /Len∗100 [ % ] , ( 5 ) where Sub is the number of substitution errors , Ins is the number of insertion errors , Del is the number of deletion errors , and Len is the number of words in the most similar word string in the network .</sentence>
				<definiendum id="0">summarisation accuracy</definiendum>
				<definiendum id="1">Sub</definiendum>
				<definiendum id="2">Ins</definiendum>
				<definiendum id="3">Del</definiendum>
				<definiendum id="4">Len</definiendum>
				<definiens id="0">the number of substitution errors</definiens>
				<definiens id="1">the number of insertion errors</definiens>
				<definiens id="2">the number of deletion errors</definiens>
				<definiens id="3">the number of words in the most similar word string in the network</definiens>
			</definition>
			<definition id="3">
				<sentence>For the third experiment ( Mixed ) , the LiMB is an interpolation of class and word models , while the component LiMs are class models .</sentence>
				<definiendum id="0">Mixed</definiendum>
				<definiendum id="1">LiMB</definiendum>
				<definiens id="0">an interpolation of class and word models</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>The parser’s grammar is a smoothed third-order Markov grammar , enhanced with lexical heads , their parts of speech , and parent and grandparent information .</sentence>
				<definiendum id="0">parser’s grammar</definiendum>
				<definiens id="0">a smoothed third-order Markov grammar , enhanced with lexical heads , their parts of speech , and parent and grandparent information</definiens>
			</definition>
			<definition id="1">
				<sentence>NANC contains no syntactic information .</sentence>
				<definiendum id="0">NANC</definiendum>
				<definiens id="0">contains no syntactic information</definiens>
			</definition>
			<definition id="2">
				<sentence>2The harmonic mean of labeled precision ( P ) and labeled recall ( R ) , i.e. f = 2×P×RP+R Sentences added Parser-best Reranker-best 0 ( baseline ) 90.3 50k 90.1 90.7 250k 90.1 90.7 500k 90.0 90.9 750k 89.9 91.0 1,000k 90.0 90.8 1,500k 90.0 90.8 2,000k – 91.0 Table 1 : f-scores after adding either parser-best or reranker-best sentences from NANC to WSJ training data .</sentence>
				<definiendum id="0">recall</definiendum>
				<definiens id="0">f-scores after adding either parser-best or reranker-best sentences from NANC to WSJ training data</definiens>
			</definition>
			<definition id="3">
				<sentence>Conjunctions are about the hardest things in parsing , and we have no grip on exactly what it takes to help parse them .</sentence>
				<definiendum id="0">Conjunctions</definiendum>
				<definiens id="0">takes to help parse them</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>Named Entity recognition ( NER ) is an important part of many natural language processing tasks .</sentence>
				<definiendum id="0">NER</definiendum>
			</definition>
			<definition id="1">
				<sentence>For a given NE in one language , the transliteration model chooses a top ranked list of candidates in another language .</sentence>
				<definiendum id="0">transliteration model</definiendum>
				<definiens id="0">chooses a top ranked list of candidates in another language</definiens>
			</definition>
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>Sausages are a parsimonious approximation of lattices , but due to the presence of null links , they do not lend themselves naturally for matching phrases .</sentence>
				<definiendum id="0">Sausages</definiendum>
				<definiens id="0">a parsimonious approximation of lattices , but due to the presence of null links , they do not lend themselves naturally for matching phrases</definiens>
			</definition>
			<definition id="1">
				<sentence>A multi-word phrase is matched if all of its words match in order .</sentence>
				<definiendum id="0">multi-word phrase</definiendum>
				<definiens id="0">matched if all of its words match in order</definiens>
			</definition>
			<definition id="2">
				<sentence>The whole set consists of 169 documents , with an average of 391 segments in each document .</sentence>
				<definiendum id="0">whole set</definiendum>
				<definiens id="0">consists of 169 documents , with an average of 391 segments in each document</definiens>
			</definition>
			<definition id="3">
				<sentence>The evaluation metric is the mean average precision ( mAP ) as computed by the standard trec_eval package used by the TREC evaluations ( NIST , 2005 ) .</sentence>
				<definiendum id="0">evaluation metric</definiendum>
			</definition>
			<definition id="4">
				<sentence>PSPL is an attractive alternative to the work presented in 420 024 681 0 1 21 41 6 1 82 0 4 8 5 3 5 8 6 3 6 8P h r a s e S p o t t i n g A c c u r a c y ( F i g u r e O f M e r i t [ % ] ) ( a ) p o d c a s t si n d e x e n t r i e s / s p o k e n w o r d b e s t p a t hb a s e l i n eP S P L T M I b a s eT M I a r cT M I t q T M I n o d e 024 681 0 1 21 41 6 1 82 0 4 2 4 7 5 2 5 7 6 2P h r a s e S p o t t i n g A c c u r a c y ( F i g u r e O f M e r i t [ % ] ) ( b ) v i d e o si n d e x e n t r i e s / s p o k e n w o r d b e s t p a t hb a s e l i n eP S P L T M I b a s eT M I a r cT M I t q T M I n o d e 024 681 0 1 21 41 6 1 82 0 4 0 4 5 5 0 5 5 6 0P h r a s e S p o t t i n g A c c u r a c y ( F i g u r e O f M e r i t [ % ] ) ( c ) l e c t u r e si n d e x e n t r i e s / s p o k e n w o r d b e s t p a t hb a s e l i n eP S P L T M I b a s eT M I a r cT M I t q T M I n o d e 024 681 0 1 21 41 6 1 82 0 5 2 5 4 5 6 5 8 6 0 6 2 6 4R e l e v a n c e R a n k i n g A c c u r a c y ( m A P [ % ] ) ( d ) l e c t u r e si n d e x e n t r i e s / s p o k e n w o r d b e s t p a t hb a s e l i n eP S P L T M I b a s eT M I a r cT M I t q T M I n o d e Figure 2 : Index size vs. accuracy for different pruning thresholds for word-spotting on ( a ) podcasts , ( b ) videos , ( c ) lectures , and ( d ) relevance ranking for lectures .</sentence>
				<definiendum id="0">PSPL</definiendum>
				<definiens id="0">a ) p o d c a s t si n d e x e n t r i e s / s p o k e n w o r d b e s t p a t hb a s e l i n eP S P L T M I b a s eT</definiens>
				<definiens id="1">e l e v a n c e R a n k i n g A c c u r a c y</definiens>
			</definition>
</paper>

		<paper id="1059">
</paper>

		<paper id="1050">
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>Thai , an alphabetical language , has 44 letters for 21 consonant sounds , 19 letters for 24 vowel sounds ( 9 short vowels , 9 long vowels and 6 diphthongs ) , 4 letters for tone markers ( 5 tones ) , few special letters , and numerals .</sentence>
				<definiendum id="0">Thai</definiendum>
				<definiens id="0">an alphabetical language , has 44 letters for 21 consonant sounds , 19 letters for 24 vowel sounds ( 9 short vowels , 9 long vowels and 6 diphthongs ) , 4 letters for tone markers ( 5 tones ) , few special letters</definiens>
			</definition>
			<definition id="1">
				<sentence>The second system ( PB-LTS ) uses an automatically generated dictionary using letter-tosound rules .</sentence>
				<definiendum id="0">second system ( PB-LTS )</definiendum>
				<definiens id="0">uses an automatically generated dictionary using letter-tosound rules</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>McCallum and Jensen ( 2003 ) argue the theoretical benefits of an integrated probabilistic model for extraction and mining , but do not construct such a system .</sentence>
				<definiendum id="0">McCallum</definiendum>
				<definiens id="0">the theoretical benefits of an integrated probabilistic model for extraction and mining , but do not construct such a system</definiens>
			</definition>
			<definition id="1">
				<sentence>Labeling Relation extraction is the task of discovering semantic connections between entities .</sentence>
				<definiendum id="0">Labeling Relation extraction</definiendum>
				<definiens id="0">the task of discovering semantic connections between entities</definiens>
			</definition>
			<definition id="2">
				<sentence>CRFs are undirected graphical models ( i.e. Markov networks ) thatarediscriminatively-trainedtomaximize the conditional probability of a set of output variables y given a set of input variables x. This conditional distribution has the form pΛ ( y|x ) = 1Z x productdisplay c∈C φc ( yc , xc ; Λ ) ( 1 ) where φ are potential functions parameterized by Λ and Zx = summationtextyproducttextc∈C φ ( yc , xc ) is a normalization factor .</sentence>
				<definiendum id="0">undirected graphical models</definiendum>
				<definiendum id="1">xc )</definiendum>
				<definiens id="0">a normalization factor</definiens>
			</definition>
			<definition id="3">
				<sentence>Assuming φc factorizes as a log-linear combination of arbitrary features computed over clique c , then φc ( yc , xc ; Λ ) = exp ( summationtextk λkfk ( yc , xc ) ) , where f is a set of arbitrary feature functions over the input , each of which has an associate model parameter λk .</sentence>
				<definiendum id="0">f</definiendum>
				<definiens id="0">a set of arbitrary feature functions over the input , each of which has an associate model parameter λk</definiens>
			</definition>
			<definition id="4">
				<sentence>This is similar to work by Dehaspe ( 1997 ) , where inductive logic programming is embedded as a feature induction technique for a maximum entropy classifier .</sentence>
				<definiendum id="0">inductive</definiendum>
				<definiens id="0">a feature induction technique for a maximum entropy classifier</definiens>
			</definition>
			<definition id="5">
				<sentence>In this work , the top-down knowledge consists of relational patterns describing the database path between entities in text .</sentence>
				<definiendum id="0">top-down knowledge</definiendum>
				<definiens id="0">consists of relational patterns describing the database path between entities in text</definiens>
			</definition>
			<definition id="6">
				<sentence>We evaluate performance by calculating the precision ( P ) and recall ( R ) of extracted relations , as well as the F1 measure , which is the harmonic mean of precision and recall .</sentence>
				<definiendum id="0">recall</definiendum>
				<definiens id="0">the harmonic mean of precision and recall</definiens>
			</definition>
			<definition id="7">
				<sentence>CRF0 is the conditional random field constructed without relational features .</sentence>
				<definiendum id="0">CRF0</definiendum>
				<definiens id="0">the conditional random field constructed without relational features</definiens>
			</definition>
			<definition id="8">
				<sentence>A synthetic article for the entity “Tom Jones” contains the sentences “He was awarded the Pulitzer Prize in 1998” and “Tom got the Pulitzer Prize in 1998.”</sentence>
				<definiendum id="0">synthetic article for the entity “Tom Jones”</definiendum>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>Capitalization is the process of recovering case information for texts in lowercase .</sentence>
				<definiendum id="0">Capitalization</definiendum>
				<definiens id="0">the process of recovering case information for texts in lowercase</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , Lita et al. ( 2003 ) use a trigram language model estimated from a corpus with case information ; Chelba and Acero ( 2004 ) use a maximum entropy Markov model ( MEMM ) combining features involving words and their cases .</sentence>
				<definiendum id="0">MEMM</definiendum>
				<definiens id="0">use a trigram language model estimated from a corpus with case information</definiens>
			</definition>
			<definition id="2">
				<sentence>The bilingual capitalization algorithm recovers the capitalized sentence E from e , according to the input sentence F , and the alignment A. Formally , we look for the best capitalized sentence E∗ such that E∗ = arg maxE∈GEN ( e ) p ( EjF , A ) ( 1 ) where GEN ( e ) is a function returning the set of possible capitalized sentences consistent with e. Notice that e does not appear in p ( EjF , A ) because we can uniquely obtain e from E. p ( EjF , A ) is the capitalization model of concern in this paper.2 To further decompose the capitalization model p ( EjF , A ) , we make some assumptions .</sentence>
				<definiendum id="0">GEN ( e )</definiendum>
				<definiendum id="1">A )</definiendum>
				<definiendum id="2">capitalization model p ( EjF</definiendum>
				<definiens id="0">a function returning the set of possible capitalized sentences consistent with e. Notice that e does not appear in p ( EjF , A</definiens>
			</definition>
			<definition id="3">
				<sentence>In Figure 3 , ˜Fj is the j-th phrase of F , ˜Ei is the i-th phrase of E , and they align to each other .</sentence>
				<definiendum id="0">˜Fj</definiendum>
				<definiendum id="1">˜Ei</definiendum>
				<definiens id="0">the j-th phrase of F ,</definiens>
			</definition>
			<definition id="4">
				<sentence>σ is the variance of the Gaussian prior dictating the cost of feature weights moving away from the mean — a smaller value of σ keeps feature weights closer to the mean .</sentence>
				<definiendum id="0">σ</definiendum>
				<definiens id="0">the variance of the Gaussian prior dictating the cost of feature weights moving away from the mean — a smaller value of σ keeps feature weights closer to the mean</definiens>
			</definition>
			<definition id="5">
				<sentence>Formally , for word Ei , and an aligned phrase pair ˜El and ˜Fm , where Ei 2 ˜El , the capitalized translation model feature of Ei is fcap·t1 ( Ei , F , A ) = log | ˜Fm|summationdisplay k=1 p ( Eij ˜Fm , k ) ( 7 ) p ( Eij ˜Fm , k ) is the capitalized translation table .</sentence>
				<definiendum id="0">k )</definiendum>
				<definiens id="0">the capitalized translation table</definiens>
			</definition>
			<definition id="6">
				<sentence>The lowercased word translation probability , i.e. , p ( clickjok ) , is used to decide how much of the tag translation probability , i.e. , p ( IUjAU ) , will contribute to the final decision .</sentence>
				<definiendum id="0">IUjAU</definiendum>
				<definiens id="0">used to decide how much of the tag translation probability</definiens>
			</definition>
			<definition id="7">
				<sentence>Formally , this feature is defined as fcap·tag·t1 ( Ei , F , A ) = log | ˜fm|summationdisplay k=1 p ( eij ˜fm , k ) p ( τ ( Ei ) jτ ( ˜Fm , k ) ) ( 8 ) p ( eij ˜fm , k ) is the t-table over lowercased word pairs , which is the usual “t-table” in a SMT system .</sentence>
				<definiendum id="0">, k )</definiendum>
				<definiens id="0">fcap·tag·t1 ( Ei , F , A ) = log | ˜fm|summationdisplay k=1 p ( eij ˜fm , k ) p ( τ ( Ei ) jτ ( ˜Fm , k ) ) ( 8 ) p ( eij ˜fm</definiens>
				<definiens id="1">the t-table over lowercased word pairs</definiens>
				<definiens id="2">the usual “t-table” in a SMT system</definiens>
			</definition>
			<definition id="8">
				<sentence>p ( τ ( Ei ) jτ ( ˜Fm , k ) ) is the probability of a target capitalization tag given a source capitalization tag and can be easily estimated from a word-aligned bilingual corpus .</sentence>
				<definiendum id="0">τ</definiendum>
				<definiens id="0">the probability of a target capitalization tag given a source capitalization tag and can be easily estimated from a word-aligned bilingual corpus</definiens>
			</definition>
			<definition id="9">
				<sentence>Function GEN generates the set of case-sensitive candidates from a lowercased token .</sentence>
				<definiendum id="0">Function GEN</definiendum>
			</definition>
			<definition id="10">
				<sentence>The NPA chunks the source sentence into phrases according to a probabilistic distribution over source phrase lengths .</sentence>
				<definiendum id="0">NPA</definiendum>
				<definiens id="0">chunks the source sentence into phrases according to a probabilistic distribution over source phrase lengths</definiens>
			</definition>
			<definition id="11">
				<sentence>Obviously , the NPA is a special case of the phrase extractor in ( Och and Ney , 2004 ) in that it considers only one phrase alignment rather than all possible ones .</sentence>
				<definiendum id="0">NPA</definiendum>
				<definiens id="0">a special case of the phrase extractor in ( Och and Ney , 2004 ) in that it considers only one phrase alignment rather than all possible ones</definiens>
			</definition>
			<definition id="12">
				<sentence>TestPrecision is used to test the capitalization precision of the capitalizer on well-formed sentences drawn from genres similar to those used for training .</sentence>
				<definiendum id="0">TestPrecision</definiendum>
				<definiens id="0">used to test the capitalization precision of the capitalizer on well-formed sentences drawn from genres similar to those used for training</definiens>
			</definition>
			<definition id="13">
				<sentence>To this end , we used the precision metric that counted the number of cor6 rectly capitalized words produced by our capitalizer on well-formed , lowercased input precision = # correctly capitalized words # total words ( 9 ) To obtain the capitalization precision , we implemented the capitalizer as a standalone program .</sentence>
				<definiendum id="0">precision metric</definiendum>
				<definiens id="0">counted the number of cor6 rectly capitalized words produced by our capitalizer on well-formed , lowercased input precision = # correctly capitalized words # total words</definiens>
			</definition>
			<definition id="14">
				<sentence>100 99 98 97 96 95 94 93 92 64.032.016.08.04.02.01.00.50.20.1 Precision ( x % ) Training Corpus Size ( MWs ) CRF-based capitalizer LM-based capitalizer Figure 4 : Capitalization precision with respect to size of training corpus .</sentence>
				<definiendum id="0">Precision ( x % ) Training Corpus Size ( MWs ) CRF-based</definiendum>
				<definiens id="0">Capitalization precision with respect to size of training corpus</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>These “segment choice” models ( SCMs ) can be trained on “segment-aligned” sentence pairs ; they can be applied during decoding or rescoring .</sentence>
				<definiendum id="0">“segment choice” models</definiendum>
				<definiendum id="1">SCMs</definiendum>
				<definiens id="0">“segment-aligned” sentence pairs ; they can be applied during decoding or rescoring</definiens>
			</definition>
			<definition id="1">
				<sentence>By analogy , the higher the overall probability a given SCM assigns to a test corpus of representative distorted sentence hypotheses ( DSHs ) , the better the quality of the SCM .</sentence>
				<definiendum id="0">DSHs</definiendum>
				<definiens id="0">a test corpus of representative distorted sentence hypotheses</definiens>
			</definition>
			<definition id="2">
				<sentence>We implemented one based on decision trees ( DTs ) , not because DTs necessarily yield the best results but for software engineering reasons : DTs are a quick way to explore a variety of features , and are easily interpreted when grown ( so that examining them can suggest further features ) .</sentence>
				<definiendum id="0">DTs</definiendum>
				<definiens id="0">a quick way to explore a variety of features</definiens>
			</definition>
			<definition id="3">
				<sentence>Suppose a left-to-right decoder with an N=4 SCM is translating a sentence with seven phrases .</sentence>
				<definiendum id="0">SCM</definiendum>
			</definition>
			<definition id="4">
				<sentence>The fseg ( ) and bseg ( ) functions count segments in the RS from left to right and right to left respectively , allowing , e.g. , the question whether a given segment is the second last segment in the RS .</sentence>
				<definiendum id="0">bseg ( )</definiendum>
				<definiens id="0">functions count segments in the RS from left to right and right to left respectively , allowing</definiens>
			</definition>
			<definition id="5">
				<sentence>Each DSH generates several data items .</sentence>
				<definiendum id="0">DSH</definiendum>
			</definition>
			<definition id="6">
				<sentence>“PP” denotes presence of the phrase penalty component .</sentence>
				<definiendum id="0">“PP” denotes</definiendum>
				<definiens id="0">presence of the phrase penalty component</definiens>
			</definition>
			<definition id="7">
				<sentence>The entries show how often the A system ( columns ) had a better score than the B system ( rows ) , in 1000 observations .</sentence>
				<definiendum id="0">entries</definiendum>
				<definiens id="0">show how often the A system ( columns ) had a better score than the B system ( rows ) , in 1000 observations</definiens>
			</definition>
			<definition id="8">
				<sentence>Unlike some recent distortion models ( Kumar and Byrne , 2005 ; Tillmann and Zhang , 2005 ; Tillmann , 2004 ) these Segment Choice Models ( SCMs ) allow phrases to be moved globally , between any positions in the sentence .</sentence>
				<definiendum id="0">Segment Choice Models</definiendum>
				<definiens id="0">phrases to be moved globally , between any positions in the sentence</definiens>
			</definition>
</paper>

		<paper id="2014">
			<definition id="0">
				<sentence>Taking a Bayesian approach , a contrast classi er for the j-th class is de ned as : ccj ( x ) = rjg ( x ) ( 1 − r j ) hj ( x ) + rjg ( x ) ( 1 ) where hj ( x ) is the likelihood of x generated by class j in the labeled data , g ( x ) is the distribution of unlabeled data , and rj is the relative proportion of unlabeled data compared to the labeled data for class j. This discriminates the class j in the labeled data from the unlabeled data .</sentence>
				<definiendum id="0">hj ( x )</definiendum>
				<definiendum id="1">g ( x )</definiendum>
				<definiendum id="2">rj</definiendum>
				<definiens id="0">the likelihood of x generated by class j in the labeled data</definiens>
				<definiens id="1">the distribution of unlabeled data , and</definiens>
				<definiens id="2">the relative proportion of unlabeled data compared to the labeled data for class j. This discriminates the class j in the labeled data from the unlabeled data</definiens>
			</definition>
			<definition id="1">
				<sentence>( 2 ) Then , the posterior probability of an input x for class j , p ( j|x ) , can be approximated as : p ( j|x ) = hj ( x ) qjsummationtext i hi ( x ) qi ( 3 ) where qj is the prior class probability which can be approximated by the fraction of instances in the class j among the labeled data .</sentence>
				<definiendum id="0">qj</definiendum>
				<definiens id="0">the prior class probability which can be approximated by the fraction of instances in the class j among the labeled data</definiens>
			</definition>
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>LSA is a dimensionality reduction technique that projects a word co-occurrence matrix to lower dimensions .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">a dimensionality reduction technique that projects a word co-occurrence matrix to lower dimensions</definiens>
			</definition>
			<definition id="1">
				<sentence>Accuracy measures the correctness of the proposed substitutions in the context of a reference sentence .</sentence>
				<definiendum id="0">Accuracy</definiendum>
				<definiens id="0">measures the correctness of the proposed substitutions in the context of a reference sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>A paraphrase example consists of a reference sentence , a reference word to be paraphrased and a proposed paraphrase of that reference ( that actually occurred in a corresponding system translation ) .</sentence>
				<definiendum id="0">paraphrase example</definiendum>
				<definiens id="0">consists of a reference sentence , a reference word to be paraphrased and a proposed paraphrase of that reference ( that actually occurred in a corresponding system translation )</definiens>
			</definition>
			<definition id="3">
				<sentence>Paraphrases produced by the four methods were judged by two native English speakers .</sentence>
				<definiendum id="0">Paraphrases</definiendum>
				<definiens id="0">produced by the four methods were judged by two native English speakers</definiens>
			</definition>
</paper>

		<paper id="2007">
			<definition id="0">
				<sentence>Let X = { xi } ni=1 be a set of contexts of occurrences of all entity pairs , where xi represents the contexts of the i-th occurrence , and n is the total number of occurrences of all entity pairs .</sentence>
				<definiendum id="0">xi</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the total number of occurrences of all entity pairs</definiens>
			</definition>
			<definition id="1">
				<sentence>The first l examples are labeled as yg ( yg ∈ { rj } Rj=1 , rj denotes relation type and R is the total number of relation types ) .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">yg ( yg ∈ { rj } Rj=1 , rj denotes relation type and</definiens>
			</definition>
			<definition id="2">
				<sentence>Define a n × n probabilistic transition matrix T Tij = P ( j → i ) = wijsummationtextn k=1 wkj , where Tij is the probability to jump from vertex xj to vertex xi .</sentence>
				<definiendum id="0">Tij</definiendum>
				<definiens id="0">the probability to jump from vertex xj to vertex xi</definiens>
			</definition>
</paper>

		<paper id="3009">
			<definition id="0">
				<sentence>NER identifies named entities from natural language texts and classifies them into specific classes according to a defined ontology or classification .</sentence>
				<definiendum id="0">NER</definiendum>
				<definiens id="0">identifies named entities from natural language texts and classifies them into specific classes according to a defined ontology or classification</definiens>
			</definition>
			<definition id="1">
				<sentence>Since biomedical language and vo243 cabulary are highly complex and evolving rapidly , Bio-NER is a very challenging problem , which raises a number of difficulties .</sentence>
				<definiendum id="0">Bio-NER</definiendum>
				<definiens id="0">raises a number of difficulties</definiens>
			</definition>
			<definition id="2">
				<sentence>Second , we construct SEROW , which uses BioProp as its training corpus .</sentence>
				<definiendum id="0">SEROW</definiendum>
				<definiens id="0">uses BioProp as its training corpus</definiens>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>conveys the semantic relation “President” between the entities “George Bush” ( PER ) and “the United States” ( GPE : a Geo-Political Entity -- an entity with land and a government ( ACE , 2004 ) ) .</sentence>
				<definiendum id="0">semantic relation “President”</definiendum>
				<definiendum id="1">“George Bush”</definiendum>
				<definiens id="0">a Geo-Political Entity -- an entity with land and a government</definiens>
			</definition>
			<definition id="1">
				<sentence>Therefore , although this kernel shows non-trivial performance improvement than that of Culotta and Sorensen ( 2004 ) , the constraint makes the two dependency kernels share the similar behavior : good precision but much lower recall on the ACE corpus .</sentence>
				<definiendum id="0">constraint</definiendum>
			</definition>
			<definition id="2">
				<sentence>In order to study which relation feature spaces ( i.e. , which portion of parse trees ) are optimal for relation extraction , we define seven different relation feature spaces as follows ( as shown in Figure 1 ) : ( 1 ) Minimum Complete Tree ( MCT ) : It is the complete sub-tree rooted by the node of the nearest common ancestor of the two entities under consideration .</sentence>
				<definiendum id="0">Minimum Complete Tree</definiendum>
				<definiens id="0">relation feature spaces ( i.e. , which portion of parse trees ) are optimal for relation extraction</definiens>
				<definiens id="1">the complete sub-tree rooted by the node of the nearest common ancestor of the two entities under consideration</definiens>
			</definition>
			<definition id="3">
				<sentence>( 6 ) Flattened PT ( FPT ) : We define two criteria to flatten the PT in order to generate the Flattened Parse tree : if the in and out arcs of a non-terminal node ( except POS node ) are both single , the node is to be removed ; if a node has the same phrase type with its father node , the node is also to be removed .</sentence>
				<definiendum id="0">Flattened PT</definiendum>
				<definiens id="0">if the in and out arcs of a non-terminal node ( except POS node</definiens>
				<definiens id="1">the same phrase type with its father node , the node is also to be removed</definiens>
			</definition>
			<definition id="4">
				<sentence>The kernel function is defined as follows : 11 2 2 12 1 2 1 2 12 ( , ) ( ) , ( ) ( ) [ ] , ( ) [ ] ( ) ( ) i ii nN n N i K TT T T T i T i In In φφ φ φ ∈∈ = &lt; &gt; = =∗ ∑ ∑∑∑ where N 1 and N 2 are the sets of all nodes in trees T 1 and T 2 , respectively , and I i ( n ) is the indicator function that is 1 iff a sub-tree of type i occurs with root at node n and zero otherwise .</sentence>
				<definiendum id="0">kernel function</definiendum>
				<definiendum id="1">I i</definiendum>
				<definiens id="0">follows : 11 2 2 12 1 2 1 2 12 ( , ) ( ) , ( ) ( ) [ ] , ( ) [ ] ( ) ( ) i ii nN n N i K TT T T T i T i In In φφ φ φ ∈∈ = &lt; &gt; = =∗ ∑ ∑∑∑ where N 1 and N 2 are the sets of all nodes in trees T 1 and T 2 , respectively , and</definiens>
			</definition>
			<definition id="5">
				<sentence>Collins and Duffy ( 2002 ) show that 12 ( , ) KT T is an instance of convolution kernels over tree structures , and which can be computed in 12 ( | | | | ) ON N× by the following recursive definitions ( Let 12 ( , ) nn∆ = 12 ( ) ( ) ii i I nIn∗ ∑ ) : ( 1 ) if 1 n and 2 n do not have the same syntactic tag or their children are different then 12 ( , ) 0nn∆= ; ( 2 ) else if their children are leaves ( POS tags ) , then 12 ( , ) 1nn λ∆=× ; ( 3 ) else 1 ( ) 12 1 2 1 ( , ) ( 1 ( ( , ) , ( , ) ) nc n j nn chn jchn jλ = ∆= +∆ ∏ , where 1 ( ) nc n is the number of the children of 1 n , ( , ) ch n j is the j th child of node n and λ ( 01λ &lt; &lt; ) is the decay factor in order to make the kernel value less variable with respect to the tree sizes .</sentence>
				<definiendum id="0">KT T</definiendum>
				<definiens id="0">an instance of convolution kernels over tree structures , and which can be computed in 12 ( | | | | ) ON N× by the following recursive definitions ( Let 12 ( , ) nn∆ = 12 ( ) ( ) ii i I nIn∗ ∑ ) : ( 1 ) if 1 n and 2 n do not have the same syntactic tag</definiens>
				<definiens id="1">if their children are leaves ( POS tags ) , then 12 ( , ) 1nn λ∆=× ; ( 3 ) else 1 ( ) 12 1 2 1 ( , ) ( 1 ( ( , ) , ( , ) ) nc n j nn chn jchn jλ = ∆= +∆ ∏ , where 1 ( ) nc n is the number of the children of 1 n , ( , ) ch n j is the j th child of node n</definiens>
			</definition>
			<definition id="6">
				<sentence>The training set consists of 674 annotated text documents and 9683 relation instances .</sentence>
				<definiendum id="0">training set</definiendum>
			</definition>
			<definition id="7">
				<sentence>The test set consists of 97 documents and 1386 relation instances .</sentence>
				<definiendum id="0">test set</definiendum>
			</definition>
			<definition id="8">
				<sentence>, “merge” is the context word ) , the context information is helpful .</sentence>
				<definiendum id="0">“merge”</definiendum>
				<definiens id="0">the context word</definiens>
			</definition>
</paper>

		<paper id="2021">
			<definition id="0">
				<sentence>( 2 ) From the labeled training set , we train a language model ( LM ) , which provides the transition probabilities in the HMM , i.e. , the P ( R ) term in Equation ( 1 ) .</sentence>
				<definiendum id="0">LM</definiendum>
			</definition>
			<definition id="1">
				<sentence>A Maxent model estimates the conditional probability : P ( Ri|O ) = 1Z λ ( O ) exp ( summationdisplay k λkgk ( Ri , O ) ) , ( 3 ) where Zλ ( O ) is the normalization term , functions gk ( Ri , O ) are indicator functions weighted by λ , and k is used to indicate different ‘features’ .</sentence>
				<definiendum id="0">Maxent model</definiendum>
				<definiendum id="1">Zλ</definiendum>
				<definiens id="0">the normalization term</definiens>
			</definition>
			<definition id="2">
				<sentence>We used punctuation ( period , question mark , and exclamation ) available from the transcriptions ( though not very accurate ) to generate sentences , and a left-to-right longest word match approach to segment sentences into words .</sentence>
				<definiendum id="0">punctuation</definiendum>
				<definiens id="0">available from the transcriptions ( though not very accurate ) to generate sentences , and a left-to-right longest word match approach to segment sentences into words</definiens>
			</definition>
			<definition id="3">
				<sentence>Parameters ( e.g. , weighting factor ) are tuned based on the average performance over the ten development sets , and the same weights are applied to all the splits during testing .</sentence>
				<definiendum id="0">Parameters</definiendum>
				<definiens id="0">e.g. , weighting factor ) are tuned based on the average performance over the ten development sets , and the same weights are applied to all the splits during testing</definiens>
			</definition>
			<definition id="4">
				<sentence>The Maxent model provides a convenient way to incorporate various knowledge sources .</sentence>
				<definiendum id="0">Maxent model</definiendum>
			</definition>
</paper>

		<paper id="4002">
			<definition id="0">
				<sentence>Mechanical speaking machines in the 1700 's eventually gave way to electrical devices in the early 1920 's , which in turn gave way to computer generation of speech by the 1960 's .</sentence>
				<definiendum id="0">Mechanical speaking</definiendum>
				<definiens id="0">machines in the 1700 's eventually gave way to electrical devices in the early 1920 's , which in turn gave way to computer generation of speech by the 1960 's</definiens>
			</definition>
</paper>

		<paper id="3003">
			<definition id="0">
				<sentence>Next , the Rule Refinement Module attacks the problem at its core and uses the local fixes to detect incorrect rules that need to be refined .</sentence>
				<definiendum id="0">Rule Refinement Module</definiendum>
				<definiens id="0">attacks the problem at its core and uses the local fixes to detect incorrect rules that need to be refined</definiens>
			</definition>
			<definition id="1">
				<sentence>The Translation Correction Tool ( TCTool ) is a userfriendly online tool that allows users to add , delete and modify words and alignments , as well as to drag words around to change word order .</sentence>
				<definiendum id="0">Translation Correction Tool ( TCTool )</definiendum>
				<definiens id="0">a userfriendly online tool that allows users to add , delete and modify words and alignments , as well as to drag words around to change word order</definiens>
			</definition>
			<definition id="2">
				<sentence>The Automatic Rule Refinement ( RR ) module extracts all the relevant information from the Learning Module Learned Tr .</sentence>
				<definiendum id="0">Automatic Rule Refinement ( RR ) module</definiendum>
			</definition>
			<definition id="3">
				<sentence>A Correction Instance stores the source language sentence ( SL ) , the target language sentence ( TL ) and the initial alignments ( AL ) , as well as all the correction actions done by the user .</sentence>
				<definiendum id="0">Correction Instance</definiendum>
				<definiendum id="1">AL</definiendum>
				<definiens id="0">stores the source language sentence ( SL ) , the target language sentence ( TL ) and the initial alignments</definiens>
			</definition>
</paper>

		<paper id="3010">
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>Other learning methods involve the use of fullyannotated augmented parse trees ( Ge and Mooney , 2005 ) or prior knowledge of the NL syntax ( Zettlemoyer and Collins , 2005 ) in training , and hence require extensive human efforts when porting to a new domain or language .</sentence>
				<definiendum id="0">Other learning methods</definiendum>
				<definiens id="0">the use of fullyannotated augmented parse trees ( Ge and Mooney , 2005 ) or prior knowledge of the NL syntax ( Zettlemoyer and Collins , 2005 ) in training , and hence require extensive human efforts when porting to a new domain or language</definiens>
			</definition>
			<definition id="1">
				<sentence>The algorithm learns a semantic parser given a set of NL sentences annotated with their correct MRs. It requires no prior knowledge of the NL syntax , although it assumes that an unambiguous , context-free grammar ( CFG ) of the target MRL is available .</sentence>
				<definiendum id="0">CFG</definiendum>
				<definiens id="0">a semantic parser given a set of NL sentences annotated with their correct MRs. It requires no prior knowledge of the NL syntax , although it assumes that an unambiguous , context-free grammar (</definiens>
			</definition>
			<definition id="2">
				<sentence>Analogous to an ordinary CFG , each SCFG rule consists of a single non-terminal on the left-hand side ( LHS ) .</sentence>
				<definiendum id="0">SCFG rule</definiendum>
				<definiens id="0">consists of a single non-terminal on the left-hand side ( LHS )</definiens>
			</definition>
			<definition id="3">
				<sentence>The right-hand side ( RHS ) of an SCFG rule is a pair of strings , 〈α , β〉 , where the non-terminals in β are a permutation of the non-terminals in α .</sentence>
				<definiendum id="0">right-hand side</definiendum>
				<definiendum id="1">RHS</definiendum>
				<definiendum id="2">SCFG rule</definiendum>
				<definiens id="0">a pair of strings , 〈α , β〉 , where the non-terminals in β are a permutation of the non-terminals in α</definiens>
			</definition>
			<definition id="4">
				<sentence>The semantic parsing model of WASP thus consists of an SCFG , G , and a probabilistic model , parameterized by λ , that takes a possible derivation , d , and returns its likelihood of being correct given an input sentence , e. The output translation , f⋆ , for a sentence , e , is defined as : f⋆ = m parenleftBigg argmax d∈D ( G|e ) Prλ ( d|e ) parenrightBigg ( 1 ) where m ( d ) is the MR string that a derivation d yields , and D ( G|e ) is the set of all possible derivations of G that yield e. In other words , the output MR is the yield of the most probable derivation that yields e in the NL stream .</sentence>
				<definiendum id="0">d∈D ( G|e ) Prλ ( d|e ) parenrightBigg</definiendum>
				<definiendum id="1">D ( G|e )</definiendum>
				<definiendum id="2">output MR</definiendum>
				<definiendum id="3">most probable derivation</definiendum>
				<definiens id="0">consists of an SCFG , G , and a probabilistic model , parameterized by λ , that takes a possible derivation , d , and returns its likelihood of being correct given an input sentence</definiens>
				<definiens id="1">the MR string that a derivation d yields</definiens>
				<definiens id="2">the set of all possible derivations of G that yield e. In other words , the</definiens>
			</definition>
			<definition id="5">
				<sentence>Then we consider productions whose RHS contains non-terminals , i.e. predicates with arguments .</sentence>
				<definiendum id="0">RHS</definiendum>
				<definiens id="0">contains non-terminals , i.e. predicates with arguments</definiens>
			</definition>
			<definition id="6">
				<sentence>In this case , an extracted pattern consists of the words to which the production is linked , as well as non-terminals showing where the arguments are realized .</sentence>
				<definiendum id="0">extracted pattern</definiendum>
				<definiens id="0">consists of the words to which the production is linked , as well as non-terminals showing where the arguments are realized</definiens>
			</definition>
			<definition id="7">
				<sentence>We propose a maximum-entropy model that defines a conditional probability distribution over derivations ( d ) given the observed NL string ( e ) : Prλ ( d|e ) = 1Z λ ( e ) exp summationdisplay i λifi ( d ) ( 2 ) where fi is a feature function , and Zλ ( e ) is a normalizing factor .</sentence>
				<definiendum id="0">maximum-entropy model</definiendum>
				<definiendum id="1">fi</definiendum>
				<definiens id="0">defines a conditional probability distribution over derivations ( d ) given the observed NL string</definiens>
			</definition>
			<definition id="8">
				<sentence>This heuristic , commonly known as Viterbi approximation , is used to improve accuracy , assuming that rules used in the best parses are the most accurate .</sentence>
				<definiendum id="0">parses</definiendum>
				<definiens id="0">Viterbi approximation , is used to improve accuracy , assuming that rules used in the best</definiens>
			</definition>
</paper>

		<paper id="5001">
</paper>

		<paper id="4003">
			<definition id="0">
				<sentence>The SmartNotes system consists of a laptop based note taking application , and a web based note retrieval system .</sentence>
				<definiendum id="0">SmartNotes system</definiendum>
				<definiens id="0">consists of a laptop based note taking application , and a web based note retrieval system</definiens>
			</definition>
			<definition id="1">
				<sentence>Action item detection : An obvious application of meeting understanding is the automatic discovery and recording of action items as they are discussed during a meeting .</sentence>
				<definiendum id="0">Action item detection</definiendum>
				<definiens id="0">the automatic discovery and recording of action items as they are discussed during a meeting</definiens>
			</definition>
			<definition id="2">
				<sentence>SmartNotes consists of two major components : The note taking application which meeting participants use to take notes during the meeting , and the note retrieval application which users use to retrieve notes at a later point .</sentence>
				<definiendum id="0">SmartNotes</definiendum>
				<definiens id="0">consists of two major components : The note taking application which meeting participants use to take notes during the meeting , and the note retrieval application which users use to retrieve notes at a later point</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>We present here a multilevel coarse-to-fine ( mlctf ) PCFG parsing algorithm that reduces the complexity of the search involved in finding the best parse .</sentence>
				<definiendum id="0">PCFG parsing algorithm</definiendum>
			</definition>
</paper>

		<paper id="2039">
			<definition id="0">
				<sentence>The Levin Hypothesis ( LH ) contends that verbs that exhibit similar syntactic behavior share element ( s ) of meaning .</sentence>
				<definiendum id="0">Levin Hypothesis ( LH )</definiendum>
				<definiens id="0">contends that verbs that exhibit similar syntactic behavior share element ( s ) of meaning</definiens>
			</definition>
			<definition id="1">
				<sentence>We create four different data sets based on the syntactic frame information reflecting four levels of frame information : FRAME1 includes all frames with all head information for PPs and SBARs , FRAME2 includes only head information for PPs but no head information for SBARs , FRAME3 includes no head information for neither PPs nor SBARs , and FRAME4 is constructed with all head information , but no constituent ordering information .</sentence>
				<definiendum id="0">FRAME3</definiendum>
				<definiendum id="1">FRAME4</definiendum>
				<definiens id="0">constructed with all head information , but no constituent ordering information</definiens>
			</definition>
			<definition id="2">
				<sentence>The ATB is a collection of 1800 stories of newswire text from three different press agencies , comprising a total of 800 , 000 Arabic tokens after clitic segmentation .</sentence>
				<definiendum id="0">ATB</definiendum>
				<definiens id="0">a collection of 1800 stories of newswire text from three different press agencies</definiens>
			</definition>
			<definition id="3">
				<sentence>The result is an F β measure , where β is the coefficient of the relative strengths of precision and recall .</sentence>
				<definiendum id="0">β</definiendum>
				<definiens id="0">the coefficient of the relative strengths of precision and recall</definiens>
			</definition>
			<definition id="4">
				<sentence>The score measures the maximum overlap between a hypothesized cluster ( HYP ) and a corresponding gold standard cluster ( GOLD ) , and computes a weighted average across all the HYP clusters : F β = summationdisplay A∈A bardblAbardbl V tot max C∈C ( β 2 + 1 ) bardblA ∩ Cbardbl β 2 bardblCbardbl + bardblAbardbl Here A is the set of HYP clusters , C is the set of GOLD clusters , and V tot = summationdisplay A∈A bardblAbardbl is the total number of verbs that were clustered into the HYP set .</sentence>
				<definiendum id="0">score</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">measures the maximum overlap between a hypothesized cluster ( HYP ) and a corresponding gold standard cluster ( GOLD ) , and computes a weighted average across all the HYP clusters : F β = summationdisplay A∈A bardblAbardbl V tot max C∈C</definiens>
				<definiens id="1">the set of HYP clusters</definiens>
				<definiens id="2">the set of GOLD clusters , and V tot = summationdisplay A∈A bardblAbardbl is the total number of verbs that were clustered into the HYP set</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>BasicS ( n , vcon , vpre , arg , argprime ) = Pcoord ( vcon , vpre ) Pargprime ( n|vpre ) Parg ( n|vcon ) /P ( n ) 2 Here , Pcoord ( vcon , vpre ) is the probability that vcon and vpre are observed in coordinated sentences in a way that the event described by vcon temporally precedes or occurs at the same time as the event described by vpre .</sentence>
				<definiendum id="0">BasicS</definiendum>
				<definiens id="0">the probability that vcon and vpre are observed in coordinated sentences in a way that the event described by vcon temporally precedes or occurs at the same time as the event described by vpre</definiens>
			</definition>
			<definition id="1">
				<sentence>According to the following discussion , Parg ( vcon ) can be seen as a metric indicating how easily we can establish an interpretation of the rule , which is formalized as a mapping between events .</sentence>
				<definiendum id="0">Parg ( vcon</definiendum>
				<definiens id="0">a metric indicating how easily we can establish an interpretation of the rule</definiens>
			</definition>
			<definition id="2">
				<sentence>Here , the mapping f represents a temporal relation betweenevents , andtheformulae2 = f ( e1 ) expresses that e2 occurs at the same time as or beforee1 .</sentence>
				<definiendum id="0">mapping f</definiendum>
			</definition>
			<definition id="3">
				<sentence>The probability is denoted as P { e2 : exp2 ∧ e2 = frandom ( e1 ) |e1 : exp1 } E1 where E1 denotes the number of events describable by exp1 .</sentence>
				<definiendum id="0">E1</definiendum>
				<definiens id="0">the number of events describable by exp1</definiens>
			</definition>
			<definition id="4">
				<sentence>Scorecooc ( n , vcon , vpre , arg , argprime ) = Parg ( n , vcon ) BasicS ( n , vcon , vpre , arg , argprime ) In our experiments , though , this score did not work well .</sentence>
				<definiendum id="0">Scorecooc</definiendum>
				<definiens id="0">n , vcon , vpre , arg , argprime ) = Parg ( n , vcon ) BasicS ( n , vcon , vpre , arg , argprime</definiens>
			</definition>
			<definition id="5">
				<sentence>Then , we used Parg ( vcon ) , which denotes the probability of observing sentences that contain vcon and its argument position arg , no matter which noun occupies arg , instead of Parg ( n , vcon ) .</sentence>
				<definiendum id="0">Parg ( vcon )</definiendum>
				<definiens id="0">the probability of observing sentences that contain vcon and its argument position arg , no matter which noun occupies arg</definiens>
			</definition>
			<definition id="6">
				<sentence>Score ( n , vcon , vpre , arg , argprime ) = Parg ( vcon ) BasicS ( n , vcon , vpre , arg , argprime ) We parsed 35 years of newspaper articles ( Yomiuri 87-01 , Mainichi 91-99 , Nikkei 90-00 , 3.24GB in total ) and 92.6GB of HTML documents downloaded from the WWW using an existing parser ( Kanayama et al. , 2000 ) to obtain the word ( co-occurrence ) frequencies .</sentence>
				<definiendum id="0">Score</definiendum>
				<definiens id="0">n , vcon , vpre , arg , argprime ) = Parg ( vcon ) BasicS ( n , vcon , vpre , arg , argprime ) We parsed 35 years of newspaper articles</definiens>
			</definition>
			<definition id="7">
				<sentence>S-VV ( n , vcon , vpre , arg , argprime ) = Parg ( n , vcon ) Pargprime ( n , vpre ) /P ( n ) 2 S-NV ( n , vcon , vpre ) = Pcoord ( vcon , vpre ) MI ( n , vcon , vpre ) = Pcoord ( vcon , vpre ) / ( P ( vcon ) P ( vpre ) ) Cond ( n , vcon , vpre , arg , argprime ) = Pcoord ( vcon , vpre , arg , argprime ) Parg ( n|vcon ) Pargprime ( n|vpre ) / ( Pargprime ( n , vpre ) P ( n ) ) Rand ( n , vcon , vpre , arg , argprime ) = randomnumber S-VV was obtained by approximating the probabilities of coordinated sentences , as in the case of BasicS .</sentence>
				<definiendum id="0">S-VV</definiendum>
				<definiens id="0">n , vcon , vpre , arg</definiens>
			</definition>
			<definition id="8">
				<sentence>MI is a score based onmutual information androughly corresponds to the score used in a previous attempt to acquire temporal relations between events ( Chklovski and Pantel , 2004 ) .</sentence>
				<definiendum id="0">MI</definiendum>
				<definiens id="0">a score based onmutual information androughly corresponds to the score used in a previous attempt to acquire temporal relations between events</definiens>
			</definition>
			<definition id="9">
				<sentence>Cond is an approximation of the probability P ( n , vcon|n , vpre ) ; i.e. , the conditional proba0 20 40 60 80 100 0 20 40 60 80 100 120 Pre cisi on ( % ) Number of inference rules BasicS S-VV S-NV MI Cond Figure3 : Comparison with the alternatives ( 3 judges ) bility that the coordinated sentences consisting of n , vcon andvpre are observed given the precondition part consisting of vpre and n. Rand is a random number and generates rules by combining verbs that co-occur with the given n randomly .</sentence>
				<definiendum id="0">Cond</definiendum>
				<definiendum id="1">n. Rand</definiendum>
				<definiens id="0">an approximation of the probability P ( n , vcon|n , vpre )</definiens>
				<definiens id="1">of inference rules BasicS S-VV S-NV MI Cond Figure3 : Comparison with the alternatives ( 3 judges ) bility that the coordinated sentences consisting of n , vcon andvpre are observed given the precondition part consisting of vpre and</definiens>
				<definiens id="2">a random number and generates rules by combining verbs that co-occur with the given n randomly</definiens>
			</definition>
</paper>

		<paper id="2031">
			<definition id="0">
				<sentence>Switchboard contains 80,000 utterances of spontaneous spoken conversations over the telephone among randomly paired , North American speakers , syntactically annotated with phrase-structure grammar ( Marcus et al. , 1994 ) .</sentence>
				<definiendum id="0">Switchboard</definiendum>
				<definiens id="0">contains 80,000 utterances of spontaneous spoken conversations over the telephone among randomly paired</definiens>
			</definition>
			<definition id="1">
				<sentence>The HCRC Map Task corpus comprises more than 110 dialogues with a total of 20,400 utterances ( Anderson et al. , 1991 ) .</sentence>
				<definiendum id="0">HCRC Map Task corpus</definiendum>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>As input TEA takes English text with temporal expressions already identified , and transduces the expressions into their representations using Time Calculus for Natural Language ( TCNL ) ( Han and Kohlhase , 2003 ) .</sentence>
				<definiendum id="0">Language</definiendum>
				<definiens id="0">English text with temporal expressions already identified , and transduces the expressions into their representations using Time Calculus for Natural</definiens>
			</definition>
			<definition id="1">
				<sentence>The TCNL language is designed in such a way that syntactically different formulae can be evaluated to denote the same date ; e.g. , { tue , now+|1 week | } ( “Tuesday next week” ) and { now+|1 tue | } ( “next Tuesday” ) can denote the same date .</sentence>
				<definiendum id="0">TCNL language</definiendum>
				<definiens id="0">designed in such a way that syntactically different formulae</definiens>
			</definition>
			<definition id="2">
				<sentence>139 Table 2 : Summary of operators in TCNL ; LHS/RHS is the left/right operand , g ( e ) returns the granularity of e and min ( s ) returns the set of minimal units among s. operator Type requirement Granularity requirement Semantics Example + and − C × Q → C g ( LHS ) ← g ( RHS ) fuzzy forward/backward shifting { now+|1 day | } ( “tomorrow” ) ++ and −− C × Q → C g ( LHS ) ← min ( g ( LHS ) ∪g ( RHS ) ) exact forward/backward shifting { now++|2 hour | } ( “2 hours from now” ) @ Q × E → C g ( RHS ) ← g ( LHS ) ordinal { |2 { sun } | @ { may } } ( “the 2nd Sunday in May” ) &amp; C × C → C C × E → E E × C → E E × E → E g ( LHS ) ← min ( g ( LHS ) ∪g ( RHS ) ) distribution { now &amp; { now+|1 year | } } ( “this time next year” ) [ { 15 hour } &amp; [ { wed } : { fri } ] ] ( “3pm from Wednesday to Friday” ) into an enumeration so that the ordinal operator can select a requested element out of it .</sentence>
				<definiendum id="0">LHS/RHS</definiendum>
				<definiendum id="1">LHS ) ∪g ( RHS</definiendum>
				<definiens id="0">the left/right operand , g ( e ) returns the granularity of e and min ( s ) returns the set of minimal units among s. operator Type requirement Granularity requirement Semantics Example + and − C × Q → C g ( LHS ) ← g ( RHS ) fuzzy forward/backward shifting { now+|1 day | } ( “tomorrow” ) ++ and −− C × Q → C g ( LHS ) ← min ( g ( LHS ) ∪g ( RHS</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>The official F-score metric represents the harmonic mean between recall and precision at the nugget level .</sentence>
				<definiendum id="0">official F-score metric</definiendum>
				<definiens id="0">represents the harmonic mean between recall and precision at the nugget level</definiens>
			</definition>
			<definition id="1">
				<sentence>System responses to complex questions consist of an unordered set of passages .</sentence>
				<definiendum id="0">System responses to complex questions</definiendum>
			</definition>
			<definition id="2">
				<sentence>To evaluate answers , NIST pools answer strings from all participants , removes their association with the runs that produced them , and presents them to a human assessor .</sentence>
				<definiendum id="0">NIST</definiendum>
				<definiens id="0">pools answer strings from all participants , removes their association with the runs that produced them , and presents them to a human assessor</definiens>
			</definition>
			<definition id="3">
				<sentence>The per-question F-score is a harmonic mean between nugget precision and nugget recall , where recall is heavily favored ( controlled by the β parameter , set to five in 2003 and three in 2004 and 2005 ) .</sentence>
				<definiendum id="0">per-question F-score</definiendum>
			</definition>
			<definition id="4">
				<sentence>We propose that nugget recall be modified to take into account nugget weight : R = summationtext m∈Awmsummationtext n∈V wn Where A is the set of reference nuggets that are matched within a system’s response and V is the set of all reference nuggets ; wm and wn are the weights of nuggetsmandn , respectively .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">wn</definiendum>
				<definiens id="0">nugget recall be modified to take into account nugget weight : R = summationtext m∈Awmsummationtext n∈V wn Where A is the set of reference nuggets that are matched within a system’s response</definiens>
				<definiens id="1">the weights of nuggetsmandn , respectively</definiens>
			</definition>
</paper>

		<paper id="4010">
			<definition id="0">
				<sentence>In particular , we hypothesize that the answer a0 depends on two sets of features extracted from a1 : a2a4a3a6a5a8a7 a1a10a9 and a11 a3a13a12a14a7 a1a10a9 as follows : a15a16a7 a0a18a17 a1a10a9 a3a19a15a20a7 a0a21a17 a2a23a22 a11 a9 a22 ( 1 ) where a2 can be thought of as a set of a24a26a25 features describing the “question-type” part of a1 such as who , when , where , which , etc. and a11 is a set of features comprising the “information-bearing” part of a1 i.e. what the question is actually about and what it refers to .</sentence>
				<definiendum id="0">a11</definiendum>
			</definition>
</paper>

		<paper id="5003">
			<definition id="0">
				<sentence>Rada Mihalcea is an Assistant Professor of Computer Science at the University of North Texas .</sentence>
				<definiendum id="0">Rada Mihalcea</definiendum>
				<definiens id="0">an Assistant Professor of Computer Science at the University of North Texas</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>Word alignment is an important component of a complete statistical machine translation pipeline ( Koehn et al. , 2003 ) .</sentence>
				<definiendum id="0">Word alignment</definiendum>
			</definition>
			<definition id="1">
				<sentence>All three models are generative models of the form p ( f | e ) = summationtexta p ( a , f | e ) , where e = ( e1 , ... , eI ) is the English sentence , f = ( f1 , ... , fJ ) is the French sentence , and a = ( a1 , ... , aJ ) is the ( asymmetric ) alignment which specifies the position of an English word aligned to each French word .</sentence>
				<definiendum id="0">eI )</definiendum>
				<definiendum id="1">fJ )</definiendum>
				<definiendum id="2">aJ )</definiendum>
				<definiens id="0">generative models of the form p ( f | e ) = summationtexta p ( a , f | e ) , where e = ( e1 , ... ,</definiens>
				<definiens id="1">the English sentence , f = ( f1 , ... ,</definiens>
				<definiens id="2">the French sentence , and a = ( a1 , ... ,</definiens>
				<definiens id="3">the ( asymmetric ) alignment which specifies the position of an English word aligned to each French word</definiens>
			</definition>
			<definition id="2">
				<sentence>All three models factor in the following way : p ( a , f | e ) = Jproductdisplay j=1 pd ( aj | aj− , j ) pt ( fj | eaj ) , ( 1 ) where j− is the position of the last non-null-aligned French word before position j.2 The translation parameters pt ( fj | eaj ) are parameterized by an ( unsmoothed ) lookup table that stores the appropriate local conditional probability distributions .</sentence>
				<definiendum id="0">j−</definiendum>
				<definiens id="0">the position of the last non-null-aligned French word before position j.2 The translation parameters pt ( fj | eaj ) are parameterized by an ( unsmoothed ) lookup table that stores the appropriate local conditional probability distributions</definiens>
			</definition>
			<definition id="3">
				<sentence>is null-aligned ) : pd ( aj =0 | aj−=i ) = p0 pd ( aj =iprime negationslash= 0 | aj−=i ) ∝ ( 1−p0 ) ·    1 ( IBM 1 ) c ( iprime−floorleftjIJ floorright ) ( IBM 2 ) c ( iprime−i ) ( HMM ) , where p0 is the null-word probability and c ( · ) contains the distortion parameters for each offset argument .</sentence>
				<definiendum id="0">p0</definiendum>
				<definiens id="0">IBM 1 ) c ( iprime−floorleftjIJ floorright ) ( IBM 2 ) c ( iprime−i ) ( HMM ) , where</definiens>
			</definition>
			<definition id="4">
				<sentence>this example , COJO is a rare word that becomes a garbage collector ( Moore , 2004 ) for the models in both directions .</sentence>
				<definiendum id="0">COJO</definiendum>
				<definiens id="0">a rare word that becomes a garbage collector ( Moore , 2004 ) for the models in both directions</definiens>
			</definition>
			<definition id="5">
				<sentence>Intersection eliminates the spurious alignments , but at the expense of recall .</sentence>
				<definiendum id="0">Intersection</definiendum>
				<definiens id="0">eliminates the spurious alignments , but at the expense of recall</definiens>
			</definition>
			<definition id="6">
				<sentence>E : q ( z ; x ) : = 1Zxp1 ( z | x ; θ1 ) p2 ( z | x ; θ2 ) , M : θprime = argmax θ summationtext x , z q ( z ; x ) logp1 ( x , z ; θ1 ) + summationtext x , z q ( z ; x ) log p2 ( x , z ; θ2 ) , where Zx is a normalization constant .</sentence>
				<definiendum id="0">Zx</definiendum>
				<definiens id="0">a normalization constant</definiens>
			</definition>
			<definition id="7">
				<sentence>Using these alignments , alignment error rate ( AER ) is calculated as : parenleftbigg 1− |A∩S|+|A∩P||A|+|S| parenrightbigg ×100 % , where A is a set of proposed edges , S is the sure gold edges , and P is the possible gold edges .</sentence>
				<definiendum id="0">AER</definiendum>
				<definiendum id="1">S</definiendum>
				<definiendum id="2">P</definiendum>
				<definiens id="0">the sure gold edges</definiens>
				<definiens id="1">the possible gold edges</definiens>
			</definition>
</paper>

		<paper id="1062">
			<definition id="0">
				<sentence>For example , broadcast news ( BN ) in English can now be recognized with about ten percent word error rate ( WER ) ( NIST , 2000 ) which results in mostly quite understandable text .</sentence>
				<definiendum id="0">BN</definiendum>
				<definiens id="0">recognized with about ten percent word error rate ( WER ) ( NIST , 2000 ) which results in mostly quite understandable text</definiens>
			</definition>
			<definition id="1">
				<sentence>Finnish is a highly inflected language , in which words are formed mainly by agglutination and compounding .</sentence>
				<definiendum id="0">Finnish</definiendum>
				<definiens id="0">a highly inflected language , in which words are formed mainly by agglutination and compounding</definiens>
			</definition>
			<definition id="2">
				<sentence>Turkish is another a highly-inflected and agglutinative language with relatively free word order .</sentence>
				<definiendum id="0">Turkish</definiendum>
				<definiens id="0">a highly-inflected and agglutinative language with relatively free word order</definiens>
			</definition>
			<definition id="3">
				<sentence>The training corpus consists of approximately 27M words taken from literature , law , politics , social sciences , popular science , information technology , medicine , newspapers , magazines and sports news .</sentence>
				<definiendum id="0">training corpus</definiendum>
				<definiens id="0">consists of approximately 27M words taken from literature , law , politics , social sciences , popular science , information technology , medicine , newspapers , magazines and sports news</definiens>
			</definition>
</paper>

		<paper id="2042">
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>Deidentification refers to the removal of identifying information from records .</sentence>
				<definiendum id="0">Deidentification</definiendum>
			</definition>
			<definition id="1">
				<sentence>SNoW is a statistical classifier that includes a NER component for recognizing entities and their relations .</sentence>
				<definiendum id="0">SNoW</definiendum>
			</definition>
			<definition id="2">
				<sentence>We added to this some surface characteristics for the TW itself and obtained the following features : the TW itself , the word before , and the word after ( all lemmatized ) ; the bigram before and the bigram after TW ( lemmatized ) ; the part of speech of TW , of the word before , and of the word after ; capitalization of TW ; length of TW ; MeSH ID of the noun phrase containing TW ( MeSH is a dictionary of Medical Subject Headings and is a subset of the Unified Medical Language System ( UMLS ) of the National Library of Medicine ) ; presence of TW , of the word before , and of the word after TW in the name , location , hospital , and month dictionaries ; the heading of the section in which TW appears , e.g. , “History of Present Illness” ; and , whether TW contains “-” or “/” characters .</sentence>
				<definiendum id="0">TW ( MeSH</definiendum>
				<definiens id="0">the TW itself , the word before , and the word after ( all lemmatized ) ; the bigram before and the bigram after TW ( lemmatized ) ; the part of speech of TW , of the word before , and of the word after</definiens>
				<definiens id="1">a dictionary of Medical Subject Headings and is a subset of the Unified Medical Language System ( UMLS ) of the National Library of Medicine ) ; presence of TW , of the word before , and of the word after TW in the name , location , hospital , and month dictionaries ; the heading of the section in which TW appears , e.g. , “History of Present Illness” ; and , whether TW contains “-” or “/” characters</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>“This is a society that has been focused most of all on stability , [ ... ] ” .</sentence>
				<definiendum id="0">“This</definiendum>
				<definiens id="0">a society that has been focused most of all on stability , [ ... ] ”</definiens>
			</definition>
			<definition id="1">
				<sentence>SRL provides the semantic relationships that constituents have with predicates , thus allowing us to include such documentlevel event descriptive information into the relations holding between referring expressions ( REs ) .</sentence>
				<definiendum id="0">SRL</definiendum>
				<definiens id="0">provides the semantic relationships that constituents have with predicates , thus allowing us to include such documentlevel event descriptive information into the relations holding between referring expressions ( REs )</definiens>
			</definition>
			<definition id="2">
				<sentence>Both the Newswire ( NWIRE ) and Broadcast News ( BNEWS ) sections where split into 60-20-20 % document-based partitions for training , development , and testing , and later per-partition merged ( MERGED ) for system evaluation .</sentence>
				<definiendum id="0">Newswire</definiendum>
				<definiens id="0">BNEWS ) sections where split into 60-20-20 % document-based partitions for training , development , and testing</definiens>
			</definition>
			<definition id="3">
				<sentence>The MaxEnt model produces a probability for each category y ( coreferent or not ) of a candidate pair , conditioned on the context x in which the candidate occurs .</sentence>
				<definiendum id="0">MaxEnt model</definiendum>
			</definition>
			<definition id="4">
				<sentence>The conditional probability is calculated by : p ( y|x ) = 1Z x bracketleftBiggsummationdisplay i λifi ( x , y ) bracketrightBigg 1We used the training data corpus only , as the availability of the test data is restricted to ACE participants .</sentence>
				<definiendum id="0">conditional probability</definiendum>
			</definition>
			<definition id="5">
				<sentence>587 876 572 980 904 1,037 1,210 2,023 DEVEL 201 315 163 465 399 358 485 923 TEST 228 291 238 420 354 329 484 712 TOTAL 1,016 1,482 973 1,865 1,657 1,724 2,179 3,658 TOTAL ( % ) 34.3 % 22.5 % 43.2 % 22.8 % 28.8 % 48.4 % Table 1 : Partitions of the ACE 2003 training data corpus where fi ( x , y ) is the value of feature i on outcome y in context x , and λi is the weight associated with i in the model .</sentence>
				<definiendum id="0">λi</definiendum>
				<definiens id="0">Partitions of the ACE 2003 training data corpus where fi</definiens>
				<definiens id="1">the weight associated with i in the model</definiens>
			</definition>
			<definition id="6">
				<sentence>Zx is a normalization constant .</sentence>
				<definiendum id="0">Zx</definiendum>
				<definiens id="0">a normalization constant</definiens>
			</definition>
			<definition id="7">
				<sentence>Wikipedia is a multilingual Web-based free-content encyclopedia5 .</sentence>
				<definiendum id="0">Wikipedia</definiendum>
			</definition>
			<definition id="8">
				<sentence>In our experiments we use the ASSERT parser ( Pradhan et al. , 2004 ) , an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments , which are output as PropBank arguments ( Palmer et al. , 2005 ) .</sentence>
				<definiendum id="0">ASSERT parser</definiendum>
				<definiens id="0">an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments</definiens>
			</definition>
			<definition id="9">
				<sentence>In addition , we report the accuracy score for all three types of ACE mentions , namely pronouns , common nouns and proper names .</sentence>
				<definiendum id="0">accuracy score</definiendum>
				<definiens id="0">for all three types of ACE mentions , namely pronouns , common nouns and proper names</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>Paraphrases are alternative ways of expressing the same information within one language .</sentence>
				<definiendum id="0">Paraphrases</definiendum>
				<definiens id="0">alternative ways of expressing the same information within one language</definiens>
			</definition>
			<definition id="1">
				<sentence>The paraphrase probability p ( e2|e1 ) is defined in terms of two translation model probabilities : p ( f|e1 ) , the probability that the original English phrase e1 translates as a particular phrase f in the other language , and p ( e2|f ) , the probability that the candidate paraphrase e2 translates as the foreign language phrase .</sentence>
				<definiendum id="0">paraphrase probability p</definiendum>
				<definiens id="0">the probability that the original English phrase e1 translates as a particular phrase f in the other language , and p ( e2|f ) , the probability that the candidate paraphrase e2 translates as the foreign language phrase</definiens>
			</definition>
			<definition id="2">
				<sentence>Corpus size is measured in sentences .</sentence>
				<definiendum id="0">Corpus size</definiendum>
				<definiens id="0">measured in sentences</definiens>
			</definition>
</paper>

		<paper id="2008">
			<definition id="0">
				<sentence>SARIMA models are a class of seasonal , non-stationary temporal models based on the ARIMA process ( defined as a non-stationary extension of the stationary ARMA model ) .</sentence>
				<definiendum id="0">SARIMA models</definiendum>
			</definition>
</paper>

		<paper id="2046">
			<definition id="0">
				<sentence>MEAD is an implementation of the centroid-based method ( Radev et al. , 2004 ) that scores sentences based on sentence-level and inter-sentence features , including cluster centroids , position , TF*IDF , etc .</sentence>
				<definiendum id="0">MEAD</definiendum>
			</definition>
			<definition id="1">
				<sentence>Websumm ( Mani and Bloedorn , 2000 ) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information .</sentence>
				<definiendum id="0">Websumm</definiendum>
				<definiens id="0">uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information</definiens>
			</definition>
			<definition id="2">
				<sentence>LexPageRank ( Erkan and Radev , 2004 ) is an approach for computing sentence importance based on the concept of eigenvector centrality .</sentence>
				<definiendum id="0">LexPageRank</definiendum>
				<definiens id="0">an approach for computing sentence importance based on the concept of eigenvector centrality</definiens>
			</definition>
			<definition id="3">
				<sentence>( 6 ) where intra~M is the affinity matrix containing only the intra-document links ( the entries of interdocument links are set to 0 ) and inter~M is the affinity matrix containing only the inter-document links ( the entries of intra-document links are set to 0 ) .</sentence>
				<definiendum id="0">intra~M</definiendum>
				<definiendum id="1">inter~M</definiendum>
				<definiens id="0">the affinity matrix containing only the intra-document links ( the entries of interdocument links are set to 0</definiens>
				<definiens id="1">the affinity matrix containing only the inter-document links ( the entries of intra-document links are set to 0 )</definiens>
			</definition>
			<definition id="4">
				<sentence>w/ diffusion� is the system with the diffusion process ( our system ) and �w/o diffusion� is the system without the diffusion proccating longer summaries , and �-m� option for word stemming .</sentence>
				<definiendum id="0">�w/o diffusion�</definiendum>
				<definiens id="0">the system without the diffusion proccating longer summaries , and �-m� option for word stemming</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>Since these phrases are not bound by or even related to syntactic constituents , linguistic generalizations ( such as SVO becoming SOV , or prepositions becoming postpositions ) are not easily incorporated into the movement models .</sentence>
				<definiendum id="0">linguistic generalizations</definiendum>
				<definiens id="0">SVO becoming SOV , or prepositions becoming postpositions</definiens>
			</definition>
			<definition id="1">
				<sentence>Picture a generative process that produces a sentence pair in left to right , emitting a pair of words in lock step .</sentence>
				<definiendum id="0">generative process</definiendum>
				<definiens id="0">produces a sentence pair in left to right , emitting a pair of words in lock step</definiens>
			</definition>
			<definition id="2">
				<sentence>Candidates consist of a target dependency tree along with a treelet and word alignment .</sentence>
				<definiendum id="0">Candidates</definiendum>
				<definiens id="0">consist of a target dependency tree along with a treelet and word alignment</definiens>
			</definition>
</paper>

		<paper id="2029">
			<definition id="0">
				<sentence>The Basic Travel Expressions Corpus ( BTEC ) is a collection of sentences that bilingual travel experts consider useful for people going to or coming from another country and cover utterances in travel situations ( Kikui et al. , 2003 ) .</sentence>
				<definiendum id="0">Basic Travel Expressions Corpus ( BTEC )</definiendum>
			</definition>
			<definition id="1">
				<sentence>For evaluation , we used the BLEU metrics , which calculates the geometric mean of n-gram precision for the MT outputs found in reference translations ( Papineni et al. , 2002 ) .</sentence>
				<definiendum id="0">BLEU metrics</definiendum>
			</definition>
			<definition id="2">
				<sentence>In addition , we trained the statistical models of the hypothesis selection method on the corpus obtained 2The word error rate ( WER ) is an objective evaluation measures that , in contrast to BLEU , can be applied on sentencelevel .</sentence>
				<definiendum id="0">WER )</definiendum>
				<definiens id="0">an objective evaluation measures that , in contrast to BLEU</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>In many NLP tasks , X is the set of words , and Y the tags .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the set of words</definiens>
			</definition>
			<definition id="1">
				<sentence>Constraints are formally the same— any function C : Y∗ mapsto→ R is a constraint , including weighted features from a classifier or probabilistic model .</sentence>
				<definiendum id="0">Constraints</definiendum>
				<definiens id="0">a constraint , including weighted features from a classifier or probabilistic model</definiens>
			</definition>
			<definition id="2">
				<sentence>One possible model is a finite-state transducer , where M ( x ) is an FSA found by composing the transducer with x. Another is a CRF , where M ( x ) is a lattice with sums of logpotentials for arc weights.1 To find the best constrained labeling in a lattice , y∗ , according to ( 1 ) , we could simply intersect the lattice with all the constraints , then extract the best path .</sentence>
				<definiendum id="0">M ( x )</definiendum>
				<definiendum id="1">M ( x )</definiendum>
				<definiens id="0">an FSA found by composing the transducer with x. Another is a CRF , where</definiens>
				<definiens id="1">a lattice with sums of logpotentials for arc weights.1 To find the best constrained labeling in a lattice</definiens>
				<definiens id="2">the lattice with all the constraints , then extract the best path</definiens>
			</definition>
			<definition id="3">
				<sentence>Weighted FSA intersection is a generalization of ordinary unweighted FSA intersection ( Mohri et al. , 1996 ) .</sentence>
				<definiendum id="0">Weighted FSA intersection</definiendum>
			</definition>
			<definition id="4">
				<sentence>The Proposition Bank : An annotated corpus of semantic roles .</sentence>
				<definiendum id="0">Proposition Bank</definiendum>
				<definiens id="0">An annotated corpus of semantic roles</definiens>
			</definition>
</paper>

		<paper id="3006">
			<definition id="0">
				<sentence>ITSpoke is built upon the Why2-Atlas tutoring back-end ( VanLehn et al. , 2002 ) , a text-based Intelligent Tutoring System designed to tutor students in the domain of qualitative physics using natural language interaction .</sentence>
				<definiendum id="0">ITSpoke</definiendum>
				<definiens id="0">a text-based Intelligent Tutoring System designed to tutor students in the domain of qualitative physics using natural language interaction</definiens>
			</definition>
			<definition id="1">
				<sentence>Active listening is a technique that has been shown to diffuse negative emotion in general ( Klein et al. , 2002 ) .</sentence>
				<definiendum id="0">Active listening</definiendum>
				<definiens id="0">a technique that has been shown to diffuse negative emotion in general</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>Student learning is a primary metric for evaluating the performance of these systems ; it can be measured , e.g. , by comparing student pretests taken prior to system use with posttests taken after system use .</sentence>
				<definiendum id="0">Student learning</definiendum>
				<definiens id="0">a primary metric for evaluating the performance of these systems ; it can be measured , e.g. , by comparing student pretests taken prior to system use with posttests taken after system use</definiens>
			</definition>
			<definition id="1">
				<sentence>ITSPOKE ( Intelligent Tutoring SPOKEn dialogue system ) ( Litman et al. , 2006 ) is a speech-enabled tutor built on top of the text-based Why2-Atlas conceptual physics tutor ( VanLehn et al. , 2002 ) .</sentence>
				<definiendum id="0">ITSPOKE ( Intelligent Tutoring SPOKEn dialogue system )</definiendum>
			</definition>
			<definition id="2">
				<sentence>ALMOST ALWAYS ( 5 ) , OFTEN ( 4 ) , SOMETIMES ( 3 ) , RARELY ( 2 ) , ALMOST NEVER ( 1 ) Figure 2 : ITSPOKE Survey Questionnaire Prior PARADISE applications predicted user satisfaction using a wide range of system-generic parameters , which include measures of speech recognition quality ( e.g. word error rate ) , measures of dialogue communication and ef ciency ( e.g. total turns and elapsed time ) , and measures of task completion ( e.g. a binary representation of whether the task was completed ) ( Mcurrency1oller , 2005a ; Mcurrency1oller , 2005b ; Walker et al. , 2002 ; Bonneau-Maynard et al. , 2000 ; Walker et al. , 2000 ; Walker et al. , 1997 ) .</sentence>
				<definiendum id="0">system-generic parameters</definiendum>
				<definiens id="0">include measures of speech recognition quality ( e.g. word error rate ) , measures of dialogue communication and ef ciency ( e.g. total turns and elapsed time ) , and measures of task completion</definiens>
			</definition>
			<definition id="3">
				<sentence>As in other tutoring research , e.g. ( Chi et al. , 2001 ; Litman et al. , 2006 ) , we use posttest score ( POST ) controlled for pretest score ( PRE ) as our target student learning prediction metric , such that POST is our target variable and PRE is always a parameter in the nal model , although it is not necessarily the strongest predictor.11 In this way , we measure student learning gains , not just nal test score .</sentence>
				<definiendum id="0">PRE</definiendum>
				<definiens id="0">always a parameter in the nal model</definiens>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>Preemptive Information Extraction is an attempt to automatically create all feasible IE systems in advance without human intervention .</sentence>
				<definiendum id="0">Preemptive Information Extraction</definiendum>
				<definiens id="0">an attempt to automatically create all feasible IE systems in advance without human intervention</definiens>
			</definition>
			<definition id="1">
				<sentence>An IE task can be defined as finding a relation among several entities involved in a certain type of event .</sentence>
				<definiendum id="0">IE task</definiendum>
				<definiens id="0">finding a relation among several entities involved in a certain type of event</definiens>
			</definition>
			<definition id="2">
				<sentence>Unrestricted Relation Discovery is a technique to automatically discover such relations that repeatedly appear in a corpus and present them as a table , with absolutely no human intervention .</sentence>
				<definiendum id="0">Unrestricted Relation Discovery</definiendum>
				<definiens id="0">a technique to automatically discover such relations that repeatedly appear in a corpus and present them as a table , with absolutely no human intervention</definiens>
			</definition>
			<definition id="3">
				<sentence>A basic pattern is a part of the text that is syntactically connected to an entity .</sentence>
				<definiendum id="0">basic pattern</definiendum>
				<definiens id="0">a part of the text that is syntactically connected to an entity</definiens>
			</definition>
			<definition id="4">
				<sentence>A basic cluster consists of a set of articles that report the same event which happens at a certain time , and a metacluster consists of a set of events that contain the same relation over a certain period .</sentence>
				<definiendum id="0">basic cluster</definiendum>
				<definiens id="0">consists of a set of articles that report the same event which happens at a certain time , and a metacluster consists of a set of events that contain the same relation over a certain period</definiens>
			</definition>
			<definition id="5">
				<sentence>First we computed a word vector from each article : V w ( A ) = IDF ( w ) summationdisplay i∈POS ( w , A ) exp ( − i avgwords ) where V w ( A ) is a vector element of word w in article A , IDF ( w ) is the inverse document frequency of word w , and POS ( w , A ) is a list of w’s positions in the article .</sentence>
				<definiendum id="0">V w</definiendum>
				<definiendum id="1">V w</definiendum>
				<definiendum id="2">, IDF ( w )</definiendum>
				<definiendum id="3">POS</definiendum>
				<definiens id="0">a vector element of word w in article A</definiens>
				<definiens id="1">the inverse document frequency of word w</definiens>
			</definition>
			<definition id="6">
				<sentence>Then we calculated the cosine value of each pair of vectors : Sim ( A 1 , A 2 ) = cos ( V ( A 1 ) · V ( A 2 ) ) We computed the similarity of all possible pairs of articles from the same day , and selected the pairs 307 whose similarity exceeded a certain threshold ( 0.65 in this experiment ) to form a basic cluster .</sentence>
				<definiendum id="0">Sim</definiendum>
				<definiendum id="1">A 1</definiendum>
				<definiendum id="2">V</definiendum>
				<definiens id="0">A 1 ) · V ( A 2 ) ) We computed the similarity of all possible pairs of articles from the same day</definiens>
			</definition>
			<definition id="7">
				<sentence>First , we compute the weight for each cross-document entity E in a certain basic cluster as follows : W E = summationdisplay e∈E mentions ( e ) · exp ( −C · firstsent ( e ) ) where e ∈ E is an entity within one article and mentions ( e ) and firstsent ( e ) are the number of mentions of entity e in a document and the position 1 The ACE task description can be found at http : //www.itl.nist.gov/iad/894.01/tests/ace/ and the ACE guidelines at http : //www.ldc.upenn.edu/Projects/ACE/ 2 The hurricane names used in the examples were recognized as PERSON .</sentence>
				<definiendum id="0">e ∈ E</definiendum>
				<definiens id="0">the weight for each cross-document entity E in a certain basic cluster as follows : W E = summationdisplay e∈E mentions</definiens>
				<definiens id="1">an entity within one article and mentions</definiens>
			</definition>
			<definition id="8">
				<sentence>C is a constant value which was 0.5 in this experiment .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a constant value which was 0.5 in this experiment</definiens>
			</definition>
			<definition id="9">
				<sentence>Now , we replace both entities “Katrina” and “Louisiana” with variables 308 based on their NE tags and obtain parameterized patterns : “GPE+T-POS : coast ( Louisiana’s coast ) ” , “PER+SBJ : hit ( Katrina hit something ) ” , and “PER+SBJ : hit-OBJ : coast ( Katrina hit some coast ) ” .</sentence>
				<definiendum id="0">“PER+SBJ</definiendum>
				<definiendum id="1">“PER+SBJ</definiendum>
				<definiens id="0">variables 308 based on their NE tags and obtain parameterized patterns : “GPE+T-POS : coast ( Louisiana’s coast ) ” ,</definiens>
				<definiens id="1">coast ( Katrina hit some coast ) ”</definiens>
			</definition>
</paper>

		<paper id="2012">
			<definition id="0">
				<sentence>Georgian is a less commonly studied language with complex , non-concatenative verbal morphology .</sentence>
				<definiendum id="0">Georgian</definiendum>
			</definition>
			<definition id="1">
				<sentence>A preverb appears on forms from the Future subgroup of series I , and on all forms of series II and III in transitive verbs .</sentence>
				<definiendum id="0">preverb</definiendum>
			</definition>
			<definition id="2">
				<sentence>The core of the model consists of several levels of finitestate transducer ( FST ) networks such that the result of compiling a lower-level network serves as input to a higher-level network .</sentence>
				<definiendum id="0">core of the model</definiendum>
				<definiens id="0">consists of several levels of finitestate transducer ( FST ) networks such that the result of compiling a lower-level network serves as input to a higher-level network</definiens>
			</definition>
</paper>

		<paper id="4009">
			<definition id="0">
				<sentence>Alongside the Text View , SconeEdit provides a navigable KB View of the knowledge base , centered on concepts that appear in the text .</sentence>
				<definiendum id="0">SconeEdit</definiendum>
				<definiens id="0">provides a navigable KB View of the knowledge base , centered on concepts that appear in the text</definiens>
			</definition>
			<definition id="1">
				<sentence>SconeEdit expands on the function of traditional ontology editors by showing the user an interactive text window ( Text View ) where the user can view and edit concepts from the knowledge base as highlighted terms in their original context .</sentence>
				<definiendum id="0">SconeEdit</definiendum>
				<definiens id="0">expands on the function of traditional ontology editors by showing the user an interactive text window ( Text View ) where the user can view and edit concepts from the knowledge base as highlighted terms in their original context</definiens>
			</definition>
			<definition id="2">
				<sentence>SconeEdit searches for instances of KB concepts in the text and highlights them in the Text View .</sentence>
				<definiendum id="0">SconeEdit</definiendum>
				<definiens id="0">searches for instances of KB concepts in the text and highlights them in the Text View</definiens>
			</definition>
			<definition id="3">
				<sentence>SconeEdit is a software client to the Scone Knowledge Base System , or simply “Scone” ( Fahlman , 2005 ) .</sentence>
				<definiendum id="0">SconeEdit</definiendum>
				<definiens id="0">a software client to the Scone Knowledge Base System</definiens>
			</definition>
			<definition id="4">
				<sentence>Scone is an efficient , open-source knowledge base ( KB ) system being developed in the Language Technologies Institute of Carnegie Mellon University .</sentence>
				<definiendum id="0">Scone</definiendum>
				<definiens id="0">an efficient , open-source knowledge base ( KB ) system being developed in the Language Technologies Institute of Carnegie Mellon University</definiens>
			</definition>
			<definition id="5">
				<sentence>The Text View helps a user find relevant knowledge quickly , even in a large general-domain KB .</sentence>
				<definiendum id="0">Text View</definiendum>
				<definiens id="0">helps a user find relevant knowledge quickly</definiens>
			</definition>
			<definition id="6">
				<sentence>The KB View contains two tabs : a Graph Tab and a List Tab .</sentence>
				<definiendum id="0">KB View</definiendum>
				<definiens id="0">contains two tabs : a Graph Tab and a List Tab</definiens>
			</definition>
			<definition id="7">
				<sentence>The Graph Tab displays an excerpt from the knowledge base as a network of linked concepts with one focus concept in the center .</sentence>
				<definiendum id="0">Graph Tab</definiendum>
				<definiens id="0">displays an excerpt from the knowledge base as a network of linked concepts with one focus concept in the center</definiens>
			</definition>
			<definition id="8">
				<sentence>SconeEdit allows the user to simply click on a word in the text to create a new concept in the KB ( see Figure 5 ) .</sentence>
				<definiendum id="0">SconeEdit</definiendum>
				<definiens id="0">allows the user to simply click on a word in the text to create a new</definiens>
			</definition>
			<definition id="9">
				<sentence>The CNet Big Picture ( CNet News Online , 2000 ) is one example of a system that does link ontology knowledge to text , but the concepts in the ontology are limited to a small fixed set .</sentence>
				<definiendum id="0">CNet Big Picture</definiendum>
				<definiens id="0">one example of a system that does link ontology knowledge to text , but the concepts in the ontology are limited to a small fixed set</definiens>
			</definition>
</paper>

		<paper id="3002">
			<definition id="0">
				<sentence>Hand gesture relates to spoken language in several ways : • Hand gesture communicates meaning .</sentence>
				<definiendum id="0">Hand gesture</definiendum>
				<definiens id="0">relates to spoken language in several ways : • Hand gesture communicates meaning</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>Word alignment is a key component of most endto-end statistical machine translation systems .</sentence>
				<definiendum id="0">Word alignment</definiendum>
				<definiens id="0">a key component of most endto-end statistical machine translation systems</definiens>
			</definition>
			<definition id="1">
				<sentence>We do this by formulating the alignment problem as a quadratic assignment problem ( QAP ) , where in addition to scoring individual edges , we also define scores of pairs of edges that connect consecutive words in an alignment .</sentence>
				<definiendum id="0">QAP</definiendum>
				<definiens id="0">scores of pairs of edges that connect consecutive words in an alignment</definiens>
			</definition>
			<definition id="2">
				<sentence>QAP is an NP-hard problem , but in the range of problem sizes that we need to tackle the problem can be solved efficiently .</sentence>
				<definiendum id="0">QAP</definiendum>
				<definiens id="0">an NP-hard problem , but in the range of problem sizes</definiens>
			</definition>
			<definition id="3">
				<sentence>The score of an assignment is the sum of edge scores : s ( y ) = summationtextjk sjkyjk .</sentence>
				<definiendum id="0">score of an assignment</definiendum>
				<definiens id="0">the sum of edge scores : s ( y ) = summationtextjk sjkyjk</definiens>
			</definition>
			<definition id="4">
				<sentence>We would like to find parameters w that predict correct alignments on the training data : yi = arg max ¯yi∈Yi wlatticetopf ( xi , ¯yi ) for each i , where Yi is the space of matchings for the sentence pair xi .</sentence>
				<definiendum id="0">Yi</definiendum>
				<definiens id="0">the space of matchings for the sentence pair xi</definiens>
			</definition>
			<definition id="5">
				<sentence>While the F-measure is a natural loss function for this task , we instead chose a sensible surrogate that fits better in our framework : weighted Hamming distance , which counts the number of variables in which a candidate solution ¯y differs from the target output y , with different penalty for false positives ( c+ ) and false negatives ( c− ) : lscript ( y , ¯y ) = summationdisplay jk bracketleftBig c+ ( 1 − yjk ) ¯yjk + c− ( 1 − ¯yjk ) yjk bracketrightBig .</sentence>
				<definiendum id="0">F-measure</definiendum>
				<definiens id="0">counts the number of variables in which a candidate solution ¯y differs from the target output y , with different penalty for false positives ( c+ ) and false negatives ( c− ) : lscript ( y , ¯y</definiens>
			</definition>
			<definition id="6">
				<sentence>Minimizing this upper bound encourages the true alignment yi to be optimal with respect to w for each instance i : min ||w||≤γ summationdisplay i max¯ yi∈Yi [ wlatticetopfi ( ¯yi ) + lscripti ( ¯yi ) ] − wlatticetopfi ( yi ) , where γ is a regularization parameter .</sentence>
				<definiendum id="0">γ</definiendum>
				<definiens id="0">min ||w||≤γ summationdisplay i max¯ yi∈Yi [ wlatticetopfi ( ¯yi ) + lscripti ( ¯yi ) ] − wlatticetopfi ( yi )</definiens>
				<definiens id="1">a regularization parameter</definiens>
			</definition>
			<definition id="7">
				<sentence>In fact the LP is a relaxation of the integer LP and provides an upper bound on the value of the highest scoring assignment .</sentence>
				<definiendum id="0">LP</definiendum>
				<definiens id="0">a relaxation of the integer LP and provides an upper bound on the value of the highest scoring assignment</definiens>
			</definition>
</paper>

		<paper id="5006">
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>In this formulation , sentences are represented as normalized syntactic dependency graphs ( like the one shown in figure 1 ) and entailment is approximated with an alignment between the graph representing the hypothesis and a portion of the corresponding graph ( s ) representing the text .</sentence>
				<definiendum id="0">entailment</definiendum>
				<definiens id="0">an alignment between the graph representing the hypothesis and a portion of the corresponding graph ( s ) representing the text</definiens>
			</definition>
			<definition id="1">
				<sentence>However , the likely result of that is that the final part of the hypothesis will align with were civilians at the end of the text , assuming that we allow an alignment with “loose” arc correspondence.2 Under this candidate alignment , the lexical alignments are perfect , and the only imperfect alignment is the subject arc of were is mismatched in the two .</sentence>
				<definiendum id="0">imperfect alignment</definiendum>
				<definiens id="0">the subject arc of were is mismatched in the two</definiens>
			</definition>
			<definition id="2">
				<sentence>Their classification phase features an output space of five semantic relations , and performs well at distinguishing entailing sentence pairs .</sentence>
				<definiendum id="0">classification phase</definiendum>
			</definition>
			<definition id="3">
				<sentence>The nodes in the final graph are then annotated with their associated word , part-of-speech ( given by the parser ) , lemma ( given by a finite-state transducer described by Minnen et al. ( 2001 ) ) and named-entity tag .</sentence>
				<definiendum id="0">lemma</definiendum>
				<definiens id="0">annotated with their associated word , part-of-speech ( given by the parser</definiens>
			</definition>
			<definition id="4">
				<sentence>An alignment consists of a mapping from each node ( word ) in the hypothesis graph to a single node in the text graph , or to null.3 Figure 1 gives the alignment for ID 971 .</sentence>
				<definiendum id="0">alignment</definiendum>
				<definiens id="0">consists of a mapping from each node ( word ) in the hypothesis graph to a single node in the text graph , or to null.3 Figure 1 gives the alignment for ID 971</definiens>
			</definition>
			<definition id="5">
				<sentence>Preserved edges receive the highest score , and longer paths receive lower scores .</sentence>
				<definiendum id="0">Preserved edges</definiendum>
				<definiens id="0">receive the highest score , and longer paths receive lower scores</definiens>
			</definition>
			<definition id="6">
				<sentence>These features are designed to capture entailment relations among simple sentences involving quantification , such as Every company must report |= A company must report ( or The company , or IBM ) .</sentence>
				<definiendum id="0">IBM</definiendum>
				<definiens id="0">designed to capture entailment relations among simple sentences involving quantification</definiens>
			</definition>
			<definition id="7">
				<sentence>“Hand-tuning” describes experiments in which all features are on , but no training occurs ; rather , weights are set by hand , according to human intuition .</sentence>
				<definiendum id="0">“Hand-tuning”</definiendum>
				<definiens id="0">describes experiments in which all features are on , but no training occurs ; rather , weights are set by hand , according to human intuition</definiens>
			</definition>
</paper>

		<paper id="2002">
			<definition id="0">
				<sentence>The corpus consists of 12,875 English sentences , totaling 76,202 word tokens .</sentence>
				<definiendum id="0">corpus</definiendum>
				<definiens id="0">consists of 12,875 English sentences , totaling 76,202 word tokens</definiens>
			</definition>
			<definition id="1">
				<sentence>context : Translate this as though it were spoken to a peer co-worker ; ( ( actor ( ( np-function fn-actor ) ( np-animacy anim-human ) ( np-biological-gender bio-gender-female ) ( np-general-type proper-noun-type ) ( np-identifiability identifiable ) ( np-specificity specific ) … ) ) ( pred ( ( np-function fn-predicate-nominal ) ( np-animacy anim-human ) ( np-biological-gender biogender-female ) ( np-general-type common-noun-type ) ( np-specificity specificity-neutral ) … ) ) ( c-v-lexical-aspect state ) ( c-copula-type copula-role ) ( c-secondary-type secondary-copula ) ( csolidarity solidarity-neutral ) ( c-power-relationship power-peer ) ( c-v-grammatical-aspect gramaspect-neutral ) ( c-v-absolute-tense past ) ( c-v-phase-aspect phase-aspect-neutral ) ( c-generaltype declarative-clause ) ( c-polarity polarity-negative ) ( c-my-causer-intentionality intentionalityn/a ) ( c-comparison-type comparison-n/a ) ( c-relative-tense relative-n/a ) ( c-our-boundary boundaryn/a ) … ) Figure 2 : An abridged feature structure , sentence and context field The MILE ( Minor Language Elicitation ) corpus is a highly structured set of English sentences .</sentence>
				<definiendum id="0">np-function fn-predicate-nominal )</definiendum>
				<definiendum id="1">c-polarity polarity-negative )</definiendum>
			</definition>
			<definition id="2">
				<sentence>Third , what we want to know about each LCTL is not how it translates the structural elements of English such as determiners and auxiliary verbs , but how it renders certain meanings such as 6 List of semantic features and values The Corpus Feature Maps : which combinations of features and values are of interest ClauseLevel NounPhrase Tense &amp; Aspect Modality Feature Structure Sets Feature Specification Reverse Annotated Feature Structure Sets : add English sentences Smaller Corpus Sampling … Figure 3 : An overview of the elicitation corpus production process definiteness , tense , and modality , which are not in one-to-one correspondence with English words .</sentence>
				<definiendum id="0">modality</definiendum>
				<definiens id="0">An overview of the elicitation corpus production process definiteness , tense</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>For example , in the template pair “buy ( obj : X ) ⇒ belong ( subj : X ) ” the operator ⇒ specifies that the premise buy entails the consequence belong , and X indicates a mapping between the object of buy and the subject of belong , as in The company bought shares .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">X ) ⇒ belong ( subj : X ) ” the operator ⇒ specifies that the premise buy entails the consequence belong , and</definiens>
				<definiens id="1">a mapping between the object of buy and the subject of belong</definiens>
			</definition>
			<definition id="1">
				<sentence>The output of this step is V ∈ P ×Q , a set of pairs of templates { p , q } , where p ∈ P is the premise , consisting of the verb vp and rp – the syntactic relation between vp and the anchor , and q ∈ Q is the consequence , consisting of the verb vq and rq – its syntactic relation to the anchor .</sentence>
				<definiendum id="0">p ∈ P</definiendum>
				<definiendum id="1">Q</definiendum>
				<definiens id="0">the consequence , consisting of the verb vq and rq – its syntactic relation to the anchor</definiens>
			</definition>
</paper>

		<paper id="2026">
			<definition id="0">
				<sentence>PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank ( Marcus et al. , 1993 ) .</sentence>
				<definiendum id="0">PropBank</definiendum>
			</definition>
			<definition id="1">
				<sentence>Verbal predicates in the Penn Treebank ( PTB ) receive a label REL and their arguments are annotated with abstract semantic role labels A0A5 or AA for those complements of the predicative verb that are considered arguments while those complements of the verb labelled with a semantic functional label in the original PTB receive the composite semantic role label AM-X , where X stands for labels such as LOC , TMP or ADV , for locative , temporal and adverbial modifiers respectively .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">for locative , temporal and adverbial modifiers respectively</definiens>
			</definition>
			<definition id="2">
				<sentence>The Proposition Bank : An annotated corpus of semantic roles .</sentence>
				<definiendum id="0">Proposition Bank</definiendum>
				<definiens id="0">An annotated corpus of semantic roles</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>The valence , which applies specifically to judgment opinions and not beliefs , is the value of the judgment .</sentence>
				<definiendum id="0">valence</definiendum>
				<definiens id="0">applies specifically to judgment opinions and not beliefs , is the value of the judgment</definiens>
			</definition>
			<definition id="1">
				<sentence>The following formula describes our model for determining the category of a word : ( 1 ) ) ... .. , | ( maxarg ) | ( maxarg 21 n cc synsynsyncPwcP ≅ where c is a category ( Positive , Negative , or Neutral ) and w is a given word ; syn n is a WordNet synonym of the word w. We calculate this closeness as follows ; ( 2 ) ) | ( ) ( maxarg ) | ( ) ( maxarg ) | ( ) ( maxarg ) | ( maxarg 1 ) ) ( , ( ... 3 2 1 ∏ = = = = m k wsynsetfcount k c n c cc k cfPcP csynsynsynsynPcP cwPcPwcP where k f is the k th feature of class c which is also a member of the synonym set of the given word w. count ( f k , synset ( w ) ) is the total number of occurrences of the word feature f k in the synonym set of word w. In section 4.1 , we describe our manually annotated dataset which we used for seed words and for our evaluation .</sentence>
				<definiendum id="0">c</definiendum>
				<definiendum id="1">Neutral</definiendum>
				<definiendum id="2">w</definiendum>
				<definiendum id="3">syn n</definiendum>
				<definiendum id="4">k f</definiendum>
				<definiens id="0">determining the category of a word : ( 1 ) ) ... ..</definiens>
				<definiens id="1">a category ( Positive , Negative , or</definiens>
				<definiens id="2">a WordNet synonym of the word w. We calculate this closeness as follows ; ( 2 ) ) | ( ) ( maxarg ) | ( ) ( maxarg ) | ( ) ( maxarg ) | ( maxarg 1</definiens>
				<definiens id="3">the k th feature of class c which is also a member of the synonym set of the given word w. count ( f k , synset ( w ) ) is the total number of occurrences of the word feature f k in the synonym set of word w. In section 4.1</definiens>
			</definition>
			<definition id="2">
				<sentence>We define an opinion holder as an entity ( person , organization , country , or special group of people ) who expresses explicitly or implicitly the opinion contained in the sentence .</sentence>
				<definiendum id="0">opinion holder</definiendum>
				<definiens id="0">an entity ( person , organization , country , or special group of people ) who expresses explicitly or implicitly the opinion contained in the sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>Expressivesubjectivity marks words and phrases that indirectly express a private state that is defined as a term for opinions , evaluations , emotions , and speculations .</sentence>
				<definiendum id="0">Expressivesubjectivity</definiendum>
			</definition>
			<definition id="4">
				<sentence>In order to see how Hhead and Ehead are related to each other in the parse tree , we define another node , HEhead , which covers both Hhead and Ehead .</sentence>
				<definiendum id="0">HEhead</definiendum>
				<definiens id="0">covers both Hhead and Ehead</definiens>
			</definition>
			<definition id="5">
				<sentence>HEpath is defined as a path from HEhead to its left and right child nodes that are also parents of Hhead and Ehead .</sentence>
				<definiendum id="0">HEpath</definiendum>
				<definiens id="0">a path from HEhead to its left and right child nodes that are also parents of Hhead and Ehead</definiens>
			</definition>
			<definition id="6">
				<sentence>Hpath is a path from Hhead and one of its ancestor nodes that is a child of HEhead .</sentence>
				<definiendum id="0">Hpath</definiendum>
				<definiens id="0">a path from Hhead and one of its ancestor nodes that is a child of HEhead</definiens>
			</definition>
			<definition id="7">
				<sentence>Similarly , Epath is defined as a path from Ehead to one of its ancestors that is also a child of HEhead .</sentence>
				<definiendum id="0">Epath</definiendum>
				<definiens id="0">a path from Ehead to one of its ancestors that is also a child of HEhead</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>While recent work has focused on using Reinforcement Learning ( RL ) to address the first issue ( such as ( Walker , 2000 ) , ( Henderson et al. , 2005 ) , ( Williams et al. , 2005a ) ) , there has been very little empirical work on the issue of feature selection in prior RL approaches to dialogue systems .</sentence>
				<definiendum id="0">Learning ( RL</definiendum>
				<definiens id="0">feature selection in prior RL approaches to dialogue systems</definiens>
			</definition>
			<definition id="1">
				<sentence>For our study , we used an annotated corpus of 20 human-computer spoken dialogue tutoring sessions ( for our work we use the ITSPOKE system ( Litman and Silliman , 2004 ) which uses the textbased Why2-ATLAS dialogue tutoring system as its “back-end” ( VanLehn et al. , 2002 ) ) .</sentence>
				<definiendum id="0">ITSPOKE system</definiendum>
				<definiens id="0">uses the textbased Why2-ATLAS dialogue tutoring system</definiens>
			</definition>
			<definition id="2">
				<sentence>Each session consists of an interaction with one student over 5 different college-level physics problems , for a total of 100 dialogues .</sentence>
				<definiendum id="0">session</definiendum>
				<definiens id="0">consists of an interaction with one student over 5 different college-level physics problems</definiens>
			</definition>
			<definition id="3">
				<sentence>We use the following four step process : ( 1 ) establish an action set and reward function to be used as constants throughout the test since the state space is the one MDP parameter that will be changed during the tests ; ( 2 ) establish a baseline state and policy , and ( 3 ) add a new feature to that state and test if adding the feature results in policy changes .</sentence>
				<definiendum id="0">state space</definiendum>
				<definiens id="0">establish an action set and reward function to be used as constants throughout the test since the</definiens>
				<definiens id="1">the one MDP parameter that will be changed during the tests</definiens>
				<definiens id="2">a new feature to that state and test if adding the feature results in policy changes</definiens>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>Instead of the traditional termbased vectors , a document is represented as an a0-dimensional vector , where a0 is the number of documents in the cluster .</sentence>
				<definiendum id="0">a0</definiendum>
				<definiens id="0">the number of documents in the cluster</definiens>
			</definition>
			<definition id="1">
				<sentence>Document clustering is one of the oldest and most studied problems of information retrieval ( van Rijsbergen , 1979 ) .</sentence>
				<definiendum id="0">Document clustering</definiendum>
			</definition>
			<definition id="2">
				<sentence>In other words , our document vectors are a0-dimensional , where a0 is the number of documents in the corpus to be clustered .</sentence>
				<definiendum id="0">a0</definiendum>
			</definition>
			<definition id="3">
				<sentence>a33 is the general smoothing parameter that takes different forms in various smoothing methods .</sentence>
				<definiendum id="0">a33</definiendum>
				<definiens id="0">the general smoothing parameter that takes different forms in various smoothing methods</definiens>
			</definition>
			<definition id="4">
				<sentence>A common smoothing technique is to use Bayesian smoothing with the Dirichlet prior ( Zhai and Lafferty , 2004 ; Liu and Croft , 2004 ) : a33 a24 a27 a28 a29 a30a31 tf a15a18 a32 a16 a19 a17 a27 a28 a29 a30a31 tf a15a18 a32 a16 a19 a17 a34 a42 Here , a42 is the smoothing parameter .</sentence>
				<definiendum id="0">a42</definiendum>
				<definiens id="0">the smoothing parameter</definiens>
			</definition>
			<definition id="5">
				<sentence>Assuming the terms in a text are independent from each other , the generation probability of a text sequence a43 given the document a19 is the product of the generation probabilities of the terms of a43 : a20 a15 a43 a23a19 a17 a24 a44 a28 a30a45 a20 a15a18 a23a19 a17 ( 1 ) In the context of information retrieval , a43 is a query usually composed of few terms .</sentence>
				<definiendum id="0">a43</definiendum>
				<definiens id="0">a query usually composed of few terms</definiens>
			</definition>
			<definition id="6">
				<sentence>Cosine , also the standard metric used in a1a2 a3 a4a5a2 based document clustering , is one of these metrics .</sentence>
				<definiendum id="0">Cosine</definiendum>
				<definiens id="0">the standard metric used in a1a2 a3 a4a5a2 based document clustering , is one of these metrics</definiens>
			</definition>
			<definition id="7">
				<sentence>We denote a ( directed ) graph as a59 a15a60 a16 a18 a17 where a60 is the set of nodes and a18 a61 a60 a62 a60 a63 a64 is the link weight function .</sentence>
				<definiendum id="0">a60</definiendum>
			</definition>
			<definition id="8">
				<sentence>We computed the a4a5a2 of each term using the following formula : idfa15a18 a17 a24 a95a38a50 a52 a96 a0 dfa15a18 a17 a97 where a0 is the total number of documents and dfa15a18 a17 is the number of documents that the term a18 appears in .</sentence>
				<definiendum id="0">a0</definiendum>
				<definiens id="0">the total number of documents</definiens>
			</definition>
			<definition id="9">
				<sentence>k-means is a clustering algorithm popular for its simplicity and efficiency .</sentence>
				<definiendum id="0">k-means</definiendum>
				<definiens id="0">a clustering algorithm popular for its simplicity and efficiency</definiens>
			</definition>
			<definition id="10">
				<sentence>Mutual information is a metric that does not require a mapping function .</sentence>
				<definiendum id="0">Mutual information</definiendum>
			</definition>
			<definition id="11">
				<sentence>Mutual information ( MI ) of these two labelings is defined as : MIa15a106 a16 a84 a17 a24 a108 a109 a8 a30 a22 a110a111 a88 a30a112 a113 a15 a95a9 a16 a102 a13 a17 a3 log a52 a113 a15 a95a9 a16 a102 a13 a17 a113 a15 a95a9 a17 a3 a113 a15a102 a13 a17 where a113 a15 a95a9 a17 and a113 a15a102 a13 a17 are the probabilities that a document is labeled as a95a9 and a102a13 by the algorithm and in the actual corpus , respectively ; a113 a15 a95a9 a16 a102 a13 a17 is the probability that these two events occur at the same time .</sentence>
				<definiendum id="0">Mutual information</definiendum>
				<definiendum id="1">MI</definiendum>
				<definiens id="0">MIa15a106 a16 a84 a17 a24 a108 a109 a8 a30 a22 a110a111 a88 a30a112 a113 a15 a95a9 a16 a102 a13 a17 a3 log a52 a113 a15 a95a9 a16 a102 a13 a17 a113 a15 a95a9 a17 a3 a113 a15a102 a13 a17 where a113 a15 a95a9 a17 and a113 a15a102 a13 a17 are the probabilities that a document is labeled as a95a9 and a102a13 by the algorithm and in the actual corpus , respectively ; a113 a15 a95a9 a16 a102 a13 a17 is the probability that these two events occur at the same time</definiens>
			</definition>
			<definition id="12">
				<sentence>TDT26 is a similar corpus of news articles collected from six news agencies in 1998 .</sentence>
				<definiendum id="0">TDT26</definiendum>
				<definiens id="0">a similar corpus of news articles collected from six news agencies in 1998</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>Human conversation focus is a hard NLP ( Natural Language Processing ) problem in general because people may frequently switch topics in a real conversation .</sentence>
				<definiendum id="0">Human conversation focus</definiendum>
				<definiens id="0">a hard NLP ( Natural Language Processing ) problem in general because people may frequently switch topics in a real conversation</definiens>
			</definition>
			<definition id="1">
				<sentence>Threaded discussion is a special case of human conversation , where people may express their ideas , elaborate arguments , and answer others’ questions ; many of these aspects are unexplored by traditional IR techniques .</sentence>
				<definiendum id="0">Threaded discussion</definiendum>
				<definiens id="0">a special case of human conversation</definiens>
			</definition>
			<definition id="2">
				<sentence>Suppose that each message is represented by m i , i =1,2 , … , n. Then the entire thread is a directed graph that can be represented by G= ( V , E ) , where V is the set of nodes ( messages ) , V= { m i , i=1 , ... , n } , and E is the set of directed edges .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">E</definiendum>
				<definiens id="0">the set of nodes ( messages ) , V= { m i , i=1 , ... , n } , and</definiens>
			</definition>
			<definition id="3">
				<sentence>E is a subset of VxV .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">a subset of VxV</definiens>
			</definition>
			<definition id="4">
				<sentence>The hub score represents the quality of the message as a pointer to valuable or useful messages ( or resources , in general ) .</sentence>
				<definiendum id="0">hub score</definiendum>
				<definiens id="0">the quality of the message as a pointer to valuable or useful messages</definiens>
			</definition>
			<definition id="5">
				<sentence>The authority score measures the quality of the message as a resource itself .</sentence>
				<definiendum id="0">authority score</definiendum>
				<definiens id="0">measures the quality of the message as a resource itself</definiens>
			</definition>
			<definition id="6">
				<sentence>For conversation analysis , we adopted the theory of Speech Acts proposed by ( Austin , 1962 ; Searle , 1969 ) and defined a set of speech acts ( SAs ) that relate every pair of messages in the corpus .</sentence>
				<definiendum id="0">SAs</definiendum>
				<definiens id="0">relate every pair of messages in the corpus</definiens>
			</definition>
			<definition id="7">
				<sentence>Lexical similarity is an important measure for distinguishing relationships between message pairs .</sentence>
				<definiendum id="0">Lexical similarity</definiendum>
				<definiens id="0">an important measure for distinguishing relationships between message pairs</definiens>
			</definition>
			<definition id="8">
				<sentence>, ( cos_ ji l mmsimW = ( 3 ) For a given a speech act , SA ij ( m i →m j ) , connecting message m i and m j , the link generation function g 1 is defined as follows : ) ( ) ( 1 l ijij WarcSAg = ( 4 ) The new generated link is added to the thread graph connecting message node m i and m j with a weight of W l .</sentence>
				<definiendum id="0">SA ij</definiendum>
				<definiens id="0">follows : ) ( ) ( 1 l ijij WarcSAg = ( 4 ) The new generated link is added to the thread graph connecting message node m i and m j with a weight of W l</definiens>
			</definition>
			<definition id="9">
				<sentence>Suppose the poster is represented by k person , the poster score , p W , is a weight calculated by ) ) ( ( ) ) ( _ ( ) ( k k k p personfeedbackcount personfeedbackpositivecount personW = ( 5 ) For a given single speech act , SA ij ( m i →m j ) , the poster score indicates the importance of message m i by itself and the generation function is given by ) ( ) ( 2 p iiij WarcSAg = ( 6 ) The generated link is self-pointing , and contains the strength of the poster information .</sentence>
				<definiendum id="0">SA ij</definiendum>
				<definiendum id="1">generation function</definiendum>
				<definiens id="0">a weight calculated by ) ) ( ( ) ) ( _ ( ) ( k k k p personfeedbackcount personfeedbackpositivecount personW = ( 5 ) For a given single speech act</definiens>
			</definition>
</paper>

		<paper id="3005">
			<definition id="0">
				<sentence>Denote a training corpus as pairs of documents Wn and their perspectives labels Dn , n = 1 , ... , N , N is the total number of documents in the corpus .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the total number of documents in the corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>Denote the intensity of the m-th sentence of the n-th document as a binary random variable Sm , n , m = 1 , ... , Mn , Mn is the total number of sentences of the n-th document .</sentence>
				<definiendum id="0">Mn</definiendum>
				<definiens id="0">the total number of sentences of the n-th document</definiens>
			</definition>
			<definition id="2">
				<sentence>S is naturally modeled as a binary variable , where τ is the parameter of S and represents how likely a perspective is strongly expressed at the sentence given on the overall document perspective .</sentence>
				<definiendum id="0">τ</definiendum>
			</definition>
</paper>

		<paper id="2011">
			<definition id="0">
				<sentence>Spectral clustering is a general term used to describe a group of algorithms that cluster points using the eigenvalues of ‘distance matrices’ obtained from data .</sentence>
				<definiendum id="0">Spectral clustering</definiendum>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>A context-free grammar ( CFG ) is a 4-tupleG = ( N , Σ , S , R ) where N and Σ are finite disjoint sets of nonterminal and terminal symbols , respectively , S ∈ N is the start symbol and R is a finite set of rules .</sentence>
				<definiendum id="0">context-free grammar ( CFG )</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">a 4-tupleG = ( N , Σ , S , R ) where N and Σ are finite disjoint sets of nonterminal and terminal symbols , respectively , S ∈ N is the start symbol</definiens>
				<definiens id="1">a finite set of rules</definiens>
			</definition>
			<definition id="1">
				<sentence>A leftmost derivation for G is a string d = pi1···pim , m ≥ 0 , such that γ0 pi1⇒ γ1 pi2⇒ ··· pim⇒ γm , for some γ0 , ... , γm ∈ V∗ ; d = ε ( where ε denotes the empty string ) is also a left-most derivation .</sentence>
				<definiendum id="0">leftmost derivation for G</definiendum>
				<definiens id="0">a string d = pi1···pim , m ≥ 0 , such that γ0 pi1⇒ γ1 pi2⇒ ··· pim⇒ γm , for some γ0 , ... , γm ∈ V∗</definiens>
				<definiens id="1">the empty string</definiens>
			</definition>
			<definition id="2">
				<sentence>For ( A → α ) ∈ R and a derivation d , f ( A → α , d ) denotes the number of occurrences of A → α in d. We let f ( A , d ) =summationtextαf ( A → α , d ) .</sentence>
				<definiendum id="0">f</definiendum>
				<definiens id="0">the number of occurrences of A → α in d. We let f ( A , d ) =summationtextαf ( A → α , d )</definiens>
			</definition>
			<definition id="3">
				<sentence>A probabilistic CFG ( PCFG ) is a pair G = ( G , pG ) , where G is a CFG and pG is a function from R to real numbers in the interval [ 0,1 ] .</sentence>
				<definiendum id="0">probabilistic CFG ( PCFG</definiendum>
				<definiendum id="1">pG</definiendum>
				<definiens id="0">a pair G = ( G , pG ) , where G is a CFG</definiens>
			</definition>
			<definition id="4">
				<sentence>The probability of a string w ∈ Σ∗ is defined as pG ( w ) = summationdisplay y ( d ) =w pG ( d ) .</sentence>
				<definiendum id="0">probability of a string w ∈ Σ∗</definiendum>
				<definiendum id="1">pG</definiendum>
			</definition>
			<definition id="5">
				<sentence>Consider a CFG G = ( N , Σ , R , S ) defined by all and only the nonterminals , terminals and rules observed in D. The criterion of maximum likelihood estimation ( MLE ) prescribes the construction of a PCFGG = ( G , pG ) such that pG maximizes the likelihood of D , defined as pG ( D ) = productdisplay d∈D pG ( d ) f ( d , D ) , ( 6 ) subject to the properness conditions summationtextαpG ( A → α ) = 1 for eachA ∈ N. The maximization problem above has a unique solution , provided by the estimator ( see for instance ( Chi and Geman , 1998 ) ) pG ( A → α ) = f ( A → α , D ) f ( A , D ) .</sentence>
				<definiendum id="0">productdisplay d∈D pG ( d ) f</definiendum>
				<definiendum id="1">summationtextαpG (</definiendum>
				<definiendum id="2">Chi</definiendum>
				<definiendum id="3">D ) f</definiendum>
				<definiens id="0">Consider a CFG G = ( N , Σ , R , S ) defined by all and only the nonterminals , terminals and rules observed in D. The criterion of maximum likelihood estimation ( MLE ) prescribes the construction of a PCFGG = ( G , pG ) such that pG maximizes the likelihood of D</definiens>
				<definiens id="1">A → α ) = 1 for eachA ∈ N. The maximization problem above has a unique solution , provided by the estimator ( see for instance (</definiens>
				<definiens id="2">A → α ) = f ( A → α ,</definiens>
			</definition>
			<definition id="6">
				<sentence>The MLE criterion prescribes the construction of a PCFG G = ( G , pG ) such that pG maximizes the likelihood of C , defined as pG ( C ) = productdisplay w∈C pG ( w ) f ( w , C ) , ( 8 ) subject to the properness conditions as in the supervised case above .</sentence>
				<definiendum id="0">MLE criterion</definiendum>
				<definiendum id="1">productdisplay w∈C pG ( w ) f</definiendum>
				<definiens id="0">prescribes the construction of a PCFG G = ( G , pG ) such that pG maximizes the likelihood of C</definiens>
			</definition>
			<definition id="7">
				<sentence>The above maximization problem provides a system of |R| nonlinear equations ( see ( Chi and Geman , 1998 ) ) pG ( A → α ) =summationtext w∈C f ( w , C ) ·EpG ( d|w ) f ( A → α , d ) summationtext w∈C f ( w , C ) ·EpG ( d|w ) f ( A , d ) , ( 9 ) where Ep denotes an expectation computed under distribution p , and pG ( d|w ) is the probability of derivation d conditioned by sentence w ( so that pG ( d|w ) &gt; 0 only if y ( d ) = w ) .</sentence>
				<definiendum id="0">|R| nonlinear equations</definiendum>
				<definiendum id="1">A → α ) =summationtext w∈C f</definiendum>
				<definiendum id="2">f</definiendum>
				<definiendum id="3">Ep</definiendum>
				<definiendum id="4">pG</definiendum>
				<definiendum id="5">d|w )</definiendum>
				<definiens id="0">an expectation computed under distribution p</definiens>
				<definiens id="1">the probability of derivation d conditioned by sentence w</definiens>
			</definition>
			<definition id="8">
				<sentence>Starting with an initial function pG that probabilistically extends G , a so-called growth transformation is computed , defined as pG ( A → α ) = summationtext w∈C f ( w , C ) · summationtext y ( d ) =w pG ( d ) pG ( w ) ·f ( A → α , d ) summationtext w∈C f ( w , C ) · summationtext y ( d ) =w pG ( d ) pG ( w ) ·f ( A , d ) .</sentence>
				<definiendum id="0">pG ( w ) ·f ( A , d</definiendum>
				<definiens id="0">Starting with an initial function pG that probabilistically extends G , a so-called growth transformation is computed , defined as pG ( A → α ) = summationtext w∈C f ( w , C ) · summationtext y ( d ) =w pG ( d ) pG ( w ) ·f ( A → α , d ) summationtext w∈C f ( w , C ) · summationtext y ( d ) =w pG ( d )</definiens>
			</definition>
			<definition id="9">
				<sentence>One possible criterion is minimization of 345 the cross-entropy between pD and pG , defined as the expectation , under distribution pD , of the information of the derivations in D computed under distribution pG , that is H ( pD ||pG ) = EpD log 1p G ( d ) = −summationdisplay d∈D pD ( d ) ·logpG ( d ) .</sentence>
				<definiendum id="0">pG</definiendum>
			</definition>
			<definition id="10">
				<sentence>The solution to the above minimization problem provides the estimator pG ( A → α ) = EpD f ( A → α , d ) E pD f ( A , d ) .</sentence>
				<definiendum id="0">estimator pG</definiendum>
				<definiendum id="1">EpD f</definiendum>
				<definiendum id="2">) E pD f</definiendum>
				<definiens id="0">A → α , d</definiens>
			</definition>
			<definition id="11">
				<sentence>We define the renormalization of G as the PCFG R ( G ) = ( G , pR ) with pR specified by pR ( A → α ) = pG ( A → α ) · summationtext d , w pG ( α d⇒ w ) summationtext d , w pG ( A d⇒ w ) .</sentence>
				<definiendum id="0">w pG</definiendum>
				<definiendum id="1">w pG</definiendum>
				<definiens id="0">the renormalization of G as the PCFG R ( G ) = ( G , pR ) with pR specified by pR ( A → α</definiens>
			</definition>
			<definition id="12">
				<sentence>We can then write α as u0A1u1A2···uq−1Aquq , for q ≥ 1 , Ai ∈ N , 1 ≤ i ≤ q , and uj ∈ Σ∗ , 0 ≤ j ≤ q. In words , A1 , ... , Aq are all of the occurrences of nonterminals in α , as they appear from left to right .</sentence>
				<definiendum id="0">A1 , ... , Aq</definiendum>
				<definiens id="0">all of the occurrences of nonterminals in α</definiens>
			</definition>
			<definition id="13">
				<sentence>Below we use the fact that pR ( uj ε⇒ uj ) = pG ( uj ε⇒ uj ) = 1 for each j with 0 ≤ j ≤ q , and further using the definition of pR and the inductive hypothesis , we can write pR ( A d⇒ w ) = = pR ( A → α ) · qproductdisplay i=1 pR ( Ai di⇒ wi ) = pG ( A → α ) · summationtext dprime , wprime pG ( α dprime⇒ wprime ) summationtext dprime , wprime pG ( A dprime⇒ wprime ) · · qproductdisplay i=1 pR ( Ai di⇒ wi ) = pG ( A → α ) · summationtext dprime , wprime pG ( α dprime⇒ wprime ) summationtext dprime , wprime pG ( A dprime⇒ wprime ) · · qproductdisplay i=1 pG ( Ai di⇒ wi ) summationtext dprime , wprime pG ( Ai dprime⇒ wprime ) = pG ( A → α ) · summationtext dprime , wprime pG ( α dprime⇒ wprime ) summationtext dprime , wprime pG ( A dprime⇒ wprime ) · · producttextq i=1 pG ( Ai di⇒ w i ) producttextq i=1 summationtext dprime , wprime pG ( Ai dprime⇒ wprime ) = pG ( A → α ) · summationtext dprime , wprime pG ( α dprime⇒ wprime ) summationtext dprime , wprime pG ( A dprime⇒ wprime ) · · producttextq i=1 pG ( Ai di⇒ w i ) summationtext dprime , wprime pG ( α dprime⇒ wprime ) = pG ( A → α ) · producttextq i=1 pG ( Ai di⇒ w i ) summationtext dprime , wprime pG ( A dprime⇒ wprime ) · = pG ( A d⇒ w ) summationtext dprime , wprime pG ( A dprime⇒ wprime ) .</sentence>
				<definiendum id="0">wprime pG</definiendum>
				<definiendum id="1">pG</definiendum>
				<definiendum id="2">wprime pG</definiendum>
				<definiendum id="3">wprime pG</definiendum>
				<definiendum id="4">wprime pG</definiendum>
				<definiendum id="5">wprime pG</definiendum>
				<definiens id="0">A → α ) · summationtext dprime</definiens>
				<definiens id="1">dprime⇒ wprime ) · · qproductdisplay i=1 pG ( Ai di⇒ wi ) summationtext dprime , wprime pG ( Ai dprime⇒ wprime ) = pG ( A → α</definiens>
				<definiens id="2">A → α ) · summationtext dprime , wprime pG ( α dprime⇒ wprime ) summationtext dprime ,</definiens>
			</definition>
			<definition id="14">
				<sentence>( 17 ) As an easy corollary of Lemma 1 , we have that R ( G ) is a consistent PCFG , as we can write summationdisplay d , w pR ( S d⇒ w ) = = summationdisplay d , w pG ( S d⇒ w ) summationtext dprime , wprime pG ( S dprime⇒ wprime ) = summationtext d , w pG ( S d⇒ w ) summationtext dprime , wprime pG ( S dprime⇒ wprime ) = 1 .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiendum id="1">w pG</definiendum>
				<definiendum id="2">w pG ( S d⇒ w</definiendum>
				<definiendum id="3">wprime pG</definiendum>
				<definiens id="0">a consistent</definiens>
			</definition>
			<definition id="15">
				<sentence>Therefore , G is consistent and pG is a probability distribution over set D ( G ) .</sentence>
				<definiendum id="0">pG</definiendum>
			</definition>
			<definition id="16">
				<sentence>Let D ( C ) be the set of all complete derivations for G that generate sentences in C , that is , D ( C ) = { d|d ∈ D ( G ) , y ( d ) ∈ C } .</sentence>
				<definiendum id="0">D ( C )</definiendum>
				<definiens id="0">the set of all complete derivations for G that generate sentences in C</definiens>
			</definition>
			<definition id="17">
				<sentence>According to Lemma 2 , we have that ˆG is a consistent PCFG .</sentence>
				<definiendum id="0">ˆG</definiendum>
				<definiens id="0">a consistent PCFG</definiens>
			</definition>
			<definition id="18">
				<sentence>From ∂∇∂pG ( A→α ) = 0 we obtain λA ·ln ( 2 ) ·pG ( A → α ) = EpDf ( A → α , d ) .</sentence>
				<definiendum id="0">·pG</definiendum>
				<definiens id="0">A → α ) = EpDf ( A → α , d )</definiens>
			</definition>
			<definition id="19">
				<sentence>( 27 ) We sum over all strings α such that ( A → α ) ∈ R , deriving λA ·ln ( 2 ) ·summationdisplay α pG ( A → α ) = = summationdisplay α EpD f ( A → α , d ) = summationdisplay α summationdisplay d∈D pD ( d ) ·f ( A → α , d ) = summationdisplay d∈D pD ( d ) ·summationdisplay α f ( A → α , d ) = summationdisplay d∈D pD ( d ) ·f ( A , d ) = EpD f ( A , d ) .</sentence>
				<definiendum id="0">EpD f</definiendum>
				<definiendum id="1">summationdisplay d∈D pD</definiendum>
				<definiendum id="2">summationdisplay d∈D pD</definiendum>
				<definiens id="0">sum over all strings α such that ( A → α ) ∈ R</definiens>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>An “active learner” is a class of machine learning algorithms that choose the order in which it is exposed to training examples ( Auer , 2000 ) .</sentence>
				<definiendum id="0">“active learner”</definiendum>
				<definiens id="0">a class of machine learning algorithms that choose the order in which it is exposed to training examples</definiens>
			</definition>
			<definition id="1">
				<sentence>By convention , the underscore character denotes the predicted position , while the hash represents word termination .</sentence>
				<definiendum id="0">underscore character</definiendum>
				<definiens id="0">the predicted position , while the hash represents word termination</definiens>
			</definition>
			<definition id="2">
				<sentence>Our test suite consists of pronunciation dictionaries from seven languages , with English considered under two manifestations .</sentence>
				<definiendum id="0">test suite</definiendum>
				<definiens id="0">consists of pronunciation dictionaries from seven languages</definiens>
			</definition>
			<definition id="3">
				<sentence>Afrikaans is a language of South Africa and is a recent derivative of Dutch .</sentence>
				<definiendum id="0">Afrikaans</definiendum>
				<definiens id="0">a language of South Africa and is a recent derivative of Dutch</definiens>
			</definition>
			<definition id="4">
				<sentence>CART tree nodes ( i.e. questions ) are the element comparable to LTS rules used in letter context chains .</sentence>
				<definiendum id="0">CART tree nodes</definiendum>
				<definiens id="0">i.e. questions ) are the element comparable to LTS rules used in letter context chains</definiens>
			</definition>
			<definition id="5">
				<sentence>Legend English ( w/stress ) English ( no stress ) Dutch German Afrikaans Italian Telugu Spanish Num Words in Lexicon 100 1000 10000 100000 LT S Ru le Pe rp lex ity 10.0 15.0 20.0 25.0 30.0 LTS Rule Perplexity vs Lexicon Size Legend English ( no stress ) Dutch German Afrikaans Italian Telugu ( itrans ) Spanish 37k4k 170k1k Ave Productions per Letter 0 2 4 6 8 10 12 Av e Pr od uct ion Pe rp lex ity Letter to Phoneme Perplexity Spanish Iraqi Phonetic alphabet 1k 40k 80k Telugu Italian Num Letters Examined 0 1000 2000 3000 4000 5000 6000 W or ds Corr ect ( % ) 10 20 30 40 50 60 70 80 Word Accuracy , Random Selection Italian , 10k dict , maxwin=5 237 Figure 7 compares average random performance to four deterministic strategies .</sentence>
				<definiendum id="0">Telugu</definiendum>
				<definiens id="0">Telugu Spanish Num Words in Lexicon 100 1000 10000 100000 LT S Ru le Pe rp lex ity 10.0 15.0 20.0 25.0 30.0 LTS Rule Perplexity vs Lexicon Size Legend English ( no stress</definiens>
			</definition>
</paper>

		<paper id="2025">
			<definition id="0">
				<sentence>We experimented such kernels with Support Vector Machines ( SVMs ) on the classification of semantic roles of PropBank ( Kingsbury and Palmer , 2002 ) and FrameNet ( Fillmore , 1982 ) data sets .</sentence>
				<definiendum id="0">FrameNet</definiendum>
				<definiens id="0">kernels with Support Vector Machines ( SVMs ) on the classification of semantic roles of PropBank</definiens>
			</definition>
			<definition id="1">
				<sentence>3 can be distributed with respect to different types of sequences , e.g. those composed by p children , it follows that ∆ ( n1 , n2 ) = µ parenleftbig λ2 +summationtextlmp=1 ∆p ( n1 , n2 ) parenrightbig , ( 4 ) where ∆p evaluates the number of common subtrees rooted in subsequences of exactly p children ( of n1 and n2 ) and lm = min { l ( cn1 ) , l ( cn2 ) } .</sentence>
				<definiendum id="0">∆p</definiendum>
				<definiens id="0">evaluates the number of common subtrees rooted in subsequences of exactly p children ( of n1 and n2 ) and lm = min { l ( cn1 ) , l ( cn2 ) }</definiens>
			</definition>
			<definition id="2">
				<sentence>This means that the worst case complexity of the PT kernel is O ( pρ2|NT1||NT2| ) , where ρ is the maximum branching factor of the two trees .</sentence>
				<definiendum id="0">ρ</definiendum>
			</definition>
</paper>

		<paper id="2009">
</paper>

		<paper id="2048">
			<definition id="0">
				<sentence>We tested three different methods to obtain the IDF value for each word feature : 1 ) IDF ( abstract+caption ) : the IDF values were calculated from the pool of abstract sentences and image captions ; 2 ) IDF ( full-text ) : the IDF values were calculated from all sentences in the full-text article ; and 3 ) IDF ( abstract ) : :IDF ( caption ) : two sets of IDF values were obtained .</sentence>
				<definiendum id="0">IDF</definiendum>
				<definiendum id="1">IDF</definiendum>
				<definiens id="0">the IDF values were calculated from all sentences in the full-text article</definiens>
				<definiens id="1">:IDF ( caption ) : two sets of IDF values were obtained</definiens>
			</definition>
			<definition id="1">
				<sentence>The TF*IDF weighted cosine similarity for a pair of documents i and j is Sim ( i , j ) , and the final similarity metric W ( i , j ) is : ( ) ) ) // ( 1 ( * ) , ( , jjii TPTPabsjiSimjiW −−= Ti=Tj=total number of abstract sentences ; and Pi and Pj represents the positions of sentences i and j in the abstract .</sentence>
				<definiendum id="0">Pj</definiendum>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>For example , we might maximize , summationdisplay x∈D logp ( x|θ ) − summationdisplay w summationdisplay z∈Sw KL ( t|z||t|w ) where t|w is the model’s distribution of tags for word w. The disadvantage of a penalty-basedapproach is that it is difficult to construct the penalty term in a way which produces exactly the desired behavior .</sentence>
				<definiendum id="0">x∈D logp</definiendum>
				<definiendum id="1">t|w</definiendum>
				<definiens id="0">the model’s distribution of tags for word w. The disadvantage of a penalty-basedapproach is that it is difficult to construct the penalty term in a way which produces exactly the desired behavior</definiens>
			</definition>
			<definition id="1">
				<sentence>In this work , we learned this structure automaticallythough prototype similarity features withoutmanuallyconstrainingthe model ( see 325 INPUNC PRT TO VBN LPUNC W DET ADV V POS ENDPUNC VBG PREP ADJ RPUNC N CONJ INPUNC PR T TO VBN LPUNC W DET AD V V POS ENDPUNC VBG PREP ADJ RPUNC N CONJ INPUNC PRT TO VBN LPUNC W DET ADV V POS ENDPUNC VBG PREP ADJ RPUNC N CONJ INPUNC PR T TO VBN LPUNC W DET AD V V POS ENDPUNC VBG PREP ADJ RPUNC N CONJ ( a ) ( b ) Figure 6 : English coarse POS tag structure : a ) correspondsto “correct” transition structure from labeled data , b ) correspondsto PROTO+SIM on 24Ktokens ROOMATES UTILITIES RESTRICTIONS AVAILABLE SIZE PHOTOS RENT FEATURES CONTACT NEIGHBORHOOD ADDRESS ROOMATES UTILITIES RESTRICTIONS AVAILABLE SIZE PHOTOS RENT FEATURES CONTACT NEIGHBORHOOD ADDRESS ROOMATES UTILITIES RESTRICTIONS AVAILABLE SIZE PHOTOS RENT FEATURES CONTACT NEIGHBORHOOD ADDRESS ( a ) ( b ) ( c ) Figure 8 : Field segmentation observed transition structure : ( a ) labeled data , ( b ) BASE ( c ) BASE+PROTO+SIM+BOUND ( afterpost-processing ) figure8 ) , thoughwe did changethe similarity function ( seebelow ) .</sentence>
				<definiendum id="0">DET ADV V POS ENDPUNC VBG PREP ADJ RPUNC N CONJ INPUNC PR</definiendum>
				<definiendum id="1">N CONJ</definiendum>
				<definiens id="0">T TO VBN LPUNC W DET AD V V POS ENDPUNC VBG PREP ADJ RPUNC N CONJ INPUNC PRT TO VBN LPUNC W DET ADV V POS ENDPUNC VBG PREP ADJ RPUNC N CONJ INPUNC PR T TO VBN LPUNC W DET AD V V POS ENDPUNC VBG PREP ADJ RPUNC</definiens>
			</definition>
</paper>

		<paper id="5002">
</paper>

		<paper id="2010">
			<definition id="0">
				<sentence>ˆy is the partitioning that maximizes equation 1 given the set of weights λt−1 .</sentence>
				<definiendum id="0">ˆy</definiendum>
				<definiens id="0">the partitioning that maximizes equation 1 given the set of weights λt−1</definiens>
			</definition>
			<definition id="1">
				<sentence>Using a planned , one-tailed pairwise t-test , the gesture features improved performance significantly 38 MARKABLE DIST The number of markables between the candidate NPs EXACT MATCH True if the candidate NPs have identical surface forms STR MATCH True if the candidate NPs match after removing articles NONPRO MATCH True if the candidate NPs are not pronouns and have identical surface forms NUMBER MATCH True if the candidate NPs agree in number PRONOUN True if the NP is a pronoun DEF NP True if the NP begins with a definite article , e.g. “the box” DEM NP True if the NP is not a pronoun and begins with the word “this” INDEF NP True if the NP begins an indefinite article , e.g. “a box” pronouns Individual features for each of the four most common pronouns : “this” , “it” , “that” , and “they” FOCUS DIST Distance between the position of the in-focus hand during j and i ( see text ) WHICH HAND Whether the hand in focus during j is the same as in i ( see text ) Table 1 : The feature set System Feature set F1 AdaBoost Gesture + Speech 54.9 AdaBoost Speech only 52.8 Voted Perceptron Gesture + Speech 53.7 Voted Perceptron Speech only 52.9 Baseline EXACT MATCH only 50.2 Baseline None corefer 41.5 Baseline All corefer 18.8 Table 2 : Results for the boosted decision trees ( t ( 15 ) = 2.48 , p &lt; .02 ) , though not for the voted perceptron ( t ( 15 ) = 1.07 , p = .15 ) .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">if the candidate NPs have identical surface forms STR MATCH True if the candidate NPs match after removing articles NONPRO MATCH True if the candidate NPs are not pronouns and have identical surface forms NUMBER MATCH True if the candidate NPs agree in number PRONOUN True if the</definiens>
				<definiens id="1">not a pronoun and begins with the word “this” INDEF NP True if the NP begins an indefinite article</definiens>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>Furthermore , Budanitsky and Hirst ( 2006 ) noted that various applications tend to pick the same measures of relatedness , which suggests a certain commonality in what is required from such a measure by the different applications .</sentence>
				<definiendum id="0">relatedness</definiendum>
				<definiens id="0">suggests a certain commonality in what is required from such a measure by the different applications</definiens>
			</definition>
			<definition id="1">
				<sentence>The standard log-frequency calculation transforms these counts into taxonomy-and-gloss based information content ( ICGT ) values .</sentence>
				<definiendum id="0">standard log-frequency calculation</definiendum>
				<definiens id="0">transforms these counts into taxonomy-and-gloss based information content ( ICGT ) values</definiens>
			</definition>
			<definition id="2">
				<sentence>B JC measure of similarity In the formula , IC is taxonomy-only based information content , as in ( Resnik , 1995 ) , LS is the lowest common subsumer of the two concepts in the WordNet hierarchy , and Max is the maximum distance11 between any two concepts .</sentence>
				<definiendum id="0">IC</definiendum>
				<definiendum id="1">LS</definiendum>
				<definiendum id="2">Max</definiendum>
				<definiens id="0">taxonomy-only based information content</definiens>
			</definition>
</paper>

		<paper id="2035">
			<definition id="0">
				<sentence>The weblog classifier uses a segmenter which splits the URL in tokens and then the token sequence is used for supervised learning and classification .</sentence>
				<definiendum id="0">weblog classifier</definiendum>
				<definiens id="0">uses a segmenter which splits the URL in tokens</definiens>
			</definition>
			<definition id="1">
				<sentence>Certain characters ( e.g. , digits ) are generalized both during training and segmentation .</sentence>
				<definiendum id="0">Certain characters</definiendum>
				<definiens id="0">generalized both during training and segmentation</definiens>
			</definition>
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>The Levenshtein algorithm measures the edit distance where edit distance is defined as the number of insertions , deletions or substitutions required to make the two strings match .</sentence>
				<definiendum id="0">Levenshtein algorithm</definiendum>
				<definiendum id="1">edit distance</definiendum>
				<definiens id="0">the number of insertions , deletions or substitutions required to make the two strings match</definiens>
			</definition>
			<definition id="1">
				<sentence>The Levenshtein “edit-distance” algorithm returns a simple integer indicating the number of edits required to make the two strings match .</sentence>
				<definiendum id="0">Levenshtein “edit-distance” algorithm</definiendum>
				<definiens id="0">returns a simple integer indicating the number of edits required to make the two strings match</definiens>
			</definition>
			<definition id="2">
				<sentence>Recall R is defined as the number of correctly matched English names divided by the number of available correct English matches in the test set .</sentence>
				<definiendum id="0">Recall R</definiendum>
				<definiens id="0">the number of correctly matched English names divided by the number of available correct English matches in the test set</definiens>
			</definition>
			<definition id="3">
				<sentence>Precision P is defined as the total number of correct names returned by the algorithm divided by the total number of names returned .</sentence>
				<definiendum id="0">Precision P</definiendum>
				<definiens id="0">the total number of correct names returned by the algorithm divided by the total number of names returned</definiens>
			</definition>
			<definition id="4">
				<sentence>SLIM is an iterative statistical learning algorithm based on a variety of estimation-maximization in which a Levenshtein edit-distance matrix is iteratively processed to find the statistical probabilities of the overlap between two strings .</sentence>
				<definiendum id="0">SLIM</definiendum>
				<definiens id="0">an iterative statistical learning algorithm based on a variety of estimation-maximization in which a Levenshtein edit-distance matrix is iteratively processed to find the statistical probabilities of the overlap between two strings</definiens>
			</definition>
			<definition id="5">
				<sentence>Jaro is a type n-gram algorithm which measures the number and the order of the common characters between two strings .</sentence>
				<definiendum id="0">Jaro</definiendum>
				<definiens id="0">a type n-gram algorithm which measures the number and the order of the common characters between two strings</definiens>
			</definition>
			<definition id="6">
				<sentence>Normalizing the Arabic Normalization enhancements were aimed at making the English string more closely match the transliterated form of the Arabic string .</sentence>
				<definiendum id="0">Normalizing the Arabic Normalization</definiendum>
				<definiens id="0">enhancements were aimed at making the English string more closely match the transliterated form of the Arabic string</definiens>
			</definition>
</paper>

		<paper id="2020">
			<definition id="0">
				<sentence>Latent semantic analysis ( LSA ) is a mathematical technique used in natural language processing for finding complex and hidden relations of meaning among words and the various contexts in which they are found ( Landauer and Dumais , 1997 ; Landauer et al , 1998 ) .</sentence>
				<definiendum id="0">Latent semantic analysis</definiendum>
				<definiendum id="1">LSA</definiendum>
				<definiens id="0">a mathematical technique used in natural language processing for finding complex and hidden relations of meaning among words and the various contexts in which they are found</definiens>
			</definition>
			<definition id="1">
				<sentence>LSA is based on the idea of association of elements ( words ) with contexts and similarity in word meaning is defined by similarity in shared contexts .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">based on the idea of association of elements ( words ) with contexts</definiens>
			</definition>
</paper>

		<paper id="2017">
			<definition id="0">
				<sentence>Entity coherence , which arises from the way NP referents relate subsequent sentences in the text , is an important aspect of textual felicity .</sentence>
				<definiendum id="0">Entity coherence</definiendum>
				<definiens id="0">arises from the way NP referents relate subsequent sentences in the text</definiens>
			</definition>
			<definition id="1">
				<sentence>Example ( 1 ) presents the rst two sentences of a text in NEWS ( Barzilay and Lapata , Table 2 ) : ( 1 ) ( a ) [ The Justice Department ] S is conducting [ an antitrust trial ] O against [ Microsoft Corp. ] X with [ evidence ] X that [ the company ] S is increasingly attempting to crush [ competitors ] O. ( b ) [ Microsoft ] O is accused of trying to forcefully buy into [ markets ] X where [ its own products ] S are not competitive enough to unseat [ established brands ] O. ( ... ) Barzilay and Lapata automatically annotated their corpora for the grammatical role of the NPs in each sentence ( denoted in the example by the subscripts S , O and X for subject , object and other respectively ) 1 as well as their coreferential relations .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">subject , object and other respectively ) 1 as well as their coreferential relations</definiens>
			</definition>
			<definition id="2">
				<sentence>Karamanis et al. ( 2004 ) introduce a measure called the classi cation rate which estimates this penalty as the weighted sum of the percentage of alternative orderings that score equally to or better than the GSO.4 When comparing several metrics with each other , the one with the lowest classi cation rate is the most appropriate for sentence ordering .</sentence>
				<definiendum id="0">classi cation rate</definiendum>
				<definiens id="0">introduce a measure called the classi cation rate which estimates this penalty as the weighted sum of the percentage of alternative orderings that score equally to or better than the GSO.4 When comparing several metrics with each other , the one with the lowest</definiens>
			</definition>
			<definition id="3">
				<sentence>Better ( M , GSO ) stands for the percentage of orderings that score better than the GSO according to a metric M , whilst Equal ( M , GSO ) is the percentage of orderings that score equal to the GSO .</sentence>
				<definiendum id="0">Better</definiendum>
				<definiendum id="1">GSO )</definiendum>
				<definiendum id="2">Equal</definiendum>
				<definiendum id="3">GSO )</definiendum>
				<definiens id="0">the percentage of orderings that score better than the GSO according to a metric M , whilst</definiens>
				<definiens id="1">the percentage of orderings that score equal to the GSO</definiens>
			</definition>
			<definition id="4">
				<sentence>The average classi cation rate of M.NOCB is an estimate of exactly this variable .</sentence>
				<definiendum id="0">average classi cation rate of M.NOCB</definiendum>
				<definiens id="0">an estimate of exactly this variable</definiens>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>Aggregation is an essential component of many natural language generation systems ( Reiter and Dale , 2000 ) .</sentence>
				<definiendum id="0">Aggregation</definiendum>
				<definiens id="0">an essential component of many natural language generation systems</definiens>
			</definition>
			<definition id="1">
				<sentence>First , our ultimate goal is a generation system which can be entirely induced from a parallel corpus of sentences and their corresponding database entries .</sentence>
				<definiendum id="0">ultimate goal</definiendum>
				<definiens id="0">a generation system which can be entirely induced from a parallel corpus of sentences and their corresponding database entries</definiens>
			</definition>
			<definition id="2">
				<sentence>In this framework , aggregation is the task of grouping semantic content without making any decisions about sentence structure or its surface realization .</sentence>
				<definiendum id="0">aggregation</definiendum>
				<definiens id="0">the task of grouping semantic content without making any decisions about sentence structure or its surface realization</definiens>
			</definition>
			<definition id="3">
				<sentence>A pair of entries is represented by a binary feature vector { xi } in which coordinate xi indicates whether two entries have the same value for attribute i. The feature vector is further expanded by conjuctive features that explicitly represent overlap in values of multiple attributes up to size k. The parameter k controls the cardinality of the maximal conjunctive set and is optimized on the development set .</sentence>
				<definiendum id="0">parameter k</definiendum>
				<definiens id="0">controls the cardinality of the maximal conjunctive set and is optimized on the development set</definiens>
			</definition>
			<definition id="4">
				<sentence>The set of shared attributes A consists of five features that capture overlap in players , time ( measured by game quarters ) , action type , outcome type , and number of yards .</sentence>
				<definiendum id="0">time</definiendum>
				<definiens id="0">measured by game quarters ) , action type , outcome type , and number of yards</definiens>
			</definition>
			<definition id="5">
				<sentence>Using an ILP formulation may be an advantage here since we could use feedback ( in the form of constraints ) from other components and knowlegde sources ( e.g. , discourse relations ) to improve aggregation or indeed the generation pipeline as a whole ( Marciniak and Strube , 2005 ) .</sentence>
				<definiendum id="0">ILP formulation</definiendum>
				<definiendum id="1">pipeline</definiendum>
				<definiens id="0">in the form of constraints ) from other components and knowlegde sources ( e.g. , discourse relations ) to improve aggregation or indeed the generation</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>Regular tree grammars ( Brainerd , 1969 ) , which subsume the tree substitution grammars developed in the NLP community ( Schabes , 1990 ) , are of particular interest to those wishing to work with additional levels of structure that string grammars can not provide .</sentence>
				<definiendum id="0">Regular tree grammars</definiendum>
				<definiens id="0">subsume the tree substitution grammars developed in the NLP community</definiens>
			</definition>
			<definition id="1">
				<sentence>For a given v where v is a vector of states , , , and , let v and v .</sentence>
				<definiendum id="0">v</definiendum>
				<definiens id="0">a vector of states , , , and , let v and v</definiens>
			</definition>
			<definition id="2">
				<sentence>COMBINATIONS returns all possible vectors of length containing members of and at least one member of .</sentence>
				<definiendum id="0">COMBINATIONS</definiendum>
				<definiens id="0">returns all possible vectors of length containing members of and at least one member of</definiens>
			</definition>
			<definition id="3">
				<sentence>Data-Oriented Parsing ( DOP ) ’s methodology is to calculate weighted derivations , but asnoted in ( Bod,2003 ) , itis thehighest ranking parse , notderivation , thatisdesired .</sentence>
				<definiendum id="0">Data-Oriented Parsing</definiendum>
				<definiens id="0">to calculate weighted derivations , but asnoted in ( Bod,2003 ) , itis thehighest ranking parse , notderivation , thatisdesired</definiens>
			</definition>
</paper>

		<paper id="2030">
			<definition id="0">
				<sentence>This very modern nuisance reflects a dilemma as ancient as writing itself : the association between a language as it is spoken and the language as it is written has a sort of internal logic to it that we can comprehend , but the conventions are different in every individual case — even among languages that use the same script , or between scripts used by the same language .</sentence>
				<definiendum id="0">language</definiendum>
				<definiens id="0">the association between a language as it is spoken and the</definiens>
				<definiens id="1">the same script , or between scripts used by the same language</definiens>
			</definition>
			<definition id="1">
				<sentence>Roughly , logography is the capacity of a writing system to associate the symbols of a script directly 118 with the meanings of specific words rather than indirectly through their pronunciations .</sentence>
				<definiendum id="0">logography</definiendum>
			</definition>
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>Human communication involves a variety of multimodal behaviors that signal both propositional content and structure , e.g. , gesture , gaze , and body posture .</sentence>
				<definiendum id="0">Human communication</definiendum>
				<definiens id="0">involves a variety of multimodal behaviors that signal both propositional content and structure , e.g. , gesture , gaze , and body posture</definiens>
			</definition>
			<definition id="1">
				<sentence>Gestures play an important role in human communication but use quite different expressive mechanisms than spoken language .</sentence>
				<definiendum id="0">Gestures</definiendum>
				<definiens id="0">an important role in human communication but use quite different expressive mechanisms than spoken language</definiens>
			</definition>
			<definition id="2">
				<sentence>Our previous research efforts related to multimodal analysis of human communication can be roughly grouped to three fields : ( 1 ) multimodal corpus col211 Figure 1 : VACE meeting corpus production lection , annotation , and data processing , ( 2 ) measurement studies to enrich knowledge of non-verbal cues to structural events , and ( 3 ) model construction using a data-driven approach .</sentence>
				<definiendum id="0">VACE meeting</definiendum>
				<definiens id="0">corpus production lection , annotation , and data processing , ( 2 ) measurement studies to enrich knowledge of non-verbal cues to structural events</definiens>
			</definition>
			<definition id="3">
				<sentence>We observed that modification gestures ( MGs ) , which exhibit a change in gesture state during speech repair , have a high correlation with content modification ( CM ) speech repairs , but rarely occur with content repetitions .</sentence>
				<definiendum id="0">MGs )</definiendum>
			</definition>
			<definition id="4">
				<sentence>A sentence unit ( SU ) is defined as the complete expression of a speaker’s thought or idea .</sentence>
				<definiendum id="0">sentence unit ( SU</definiendum>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>The Berkeley FrameNet project ( Baker et al. , 1998 ) is building a semantic lexicon for English describing the frames and linking them to the words and expressions that can evoke them .</sentence>
				<definiendum id="0">Berkeley FrameNet project</definiendum>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>The symbol DB represents a derivational boundary and splits the parse into chunks called in ectional groups ( IGs ) .1 We will use the term feature to refer to individual morphological features like +Acc and +With ; the term IG to refer to groups of features split by derivational boundaries ( DB ) , and the term tag to refer to the sequence of IGs following the root .</sentence>
				<definiendum id="0">symbol DB</definiendum>
				<definiens id="0">a derivational boundary and splits the parse into chunks called in ectional groups</definiens>
			</definition>
			<definition id="1">
				<sentence>Morphological disambiguation is a useful rst step for higher level analysis of any language but it is especially critical for agglutinative languages like Turkish , Czech , Hungarian , and Finnish .</sentence>
				<definiendum id="0">Morphological disambiguation</definiendum>
			</definition>
			<definition id="2">
				<sentence>A decision list is an ordered list of rules where each rule consists of a pattern and a classi cation ( Rivest , 1987 ) .</sentence>
				<definiendum id="0">decision list</definiendum>
			</definition>
			<definition id="3">
				<sentence>The original PREPEND uses an admissible search algorithm , OPUS , which is guaranteed to nd the best possible candidate ( Webb , 1995 ) , but we found OPUS to be too slow to be practical for a problem of this scale .</sentence>
				<definiendum id="0">original PREPEND</definiendum>
				<definiens id="0">uses an admissible search algorithm , OPUS , which is guaranteed to nd the best possible candidate</definiens>
			</definition>
</paper>

		<paper id="2037">
			<definition id="0">
				<sentence>To make the selection more re ned we can impose a condition T1 + thr ( j ) &lt; T2 where thr ( j ) is a function of j. A good choice for thr ( j ) based on empirical study is a function that declines at the same rate as the ratio ln ( N+nj ) N ≈ nj/N ≈ 1/kj where k is the average number of words for every sentence .</sentence>
				<definiendum id="0">thr ( j )</definiendum>
				<definiendum id="1">ln</definiendum>
				<definiendum id="2">k</definiendum>
				<definiens id="0">a function that declines at the same rate as the ratio</definiens>
				<definiens id="1">the average number of words for every sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>The test set for perplexity evaluations consists of 5000 sentences ( 35K words ) and the heldout set had 2000 sentences ( 12K words ) .</sentence>
				<definiendum id="0">test set for perplexity</definiendum>
				<definiens id="0">evaluations consists of 5000 sentences ( 35K words ) and the heldout set had 2000 sentences ( 12K words )</definiens>
			</definition>
			<definition id="2">
				<sentence>AllWeb refers to the case where the entire web-data was used .</sentence>
				<definiendum id="0">AllWeb</definiendum>
				<definiens id="0">the case where the entire web-data was used</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>Maximum entropy ( ME ) models have been used in bilingual sense disambiguation , word reordering , and sentence segmentation ( Berger et al. , 1996 ) , parsing , POS tagging and PP attachment ( Ratnaparkhi , 1998 ) , machine translation ( Och and Ney , 2002 ) , and FrameNet classification ( Fleischman et al. , 2003 ) .</sentence>
				<definiendum id="0">Maximum entropy</definiendum>
				<definiendum id="1">PP attachment</definiendum>
				<definiens id="0">used in bilingual sense disambiguation , word reordering , and sentence segmentation</definiens>
			</definition>
			<definition id="1">
				<sentence>We show that ACME yields a significant relative error reduction over the input alignment systems and heuristic-based combinations on three different language pairs .</sentence>
				<definiendum id="0">ACME</definiendum>
			</definition>
			<definition id="2">
				<sentence>In a statistical classification problem , the goal is to estimate the probability of a class y in a given context x , i.e. , p ( y|x ) .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">to estimate the probability of a class y in a given context x</definiens>
			</definition>
			<definition id="3">
				<sentence>Formally , the evidence is represented as feature functions , i.e. , binary valued functions that map a class y and a context x to either 0 or 1 , i.e. , h m : Y ×X → { 0 , 1 } , where Y is the set of all classes and X is the set of all facts .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">the set of all facts</definiens>
			</definition>
			<definition id="4">
				<sentence>In our combination framework , first , n different word-alignment systems , A 1 , . . . , A n , generate word alignments between a given English sentence and a foreign-language ( FL ) sentence .</sentence>
				<definiendum id="0">n different word-alignment systems</definiendum>
				<definiens id="0">generate word alignments between a given English sentence and a foreign-language ( FL ) sentence</definiens>
			</definition>
			<definition id="5">
				<sentence>Next , an Alignment Combiner decides whether to include or exclude an alignment link based on the extracted feature functions and the model parameters associated with them .</sentence>
				<definiendum id="0">Alignment Combiner</definiendum>
			</definition>
			<definition id="6">
				<sentence>Assuming Y = { yes , no } represents the set of classes , where each class denotes the existence or absence of a link in the combined alignment , and X is the set of features above , we generate various feature functions h ( y , x ) , where y ∈ Y and x are instantiations of one or more features in X. Table 1 lists the feature sets with an example feature func97 Features Example Feature Function posE h ( prime yes prime , i , j ) = 1 if ( i , j ) ∈ A C and pos ( e i ) = Noun posF h ( prime no prime , i , j ) = 1 if ( i , j ) /∈ A C and pos ( f j ) = V erb out h ( prime yes prime , i , j , k ) = 1 if ( i , j ) ∈ A C and ( i , j ) ∈ A k out , neigh h ( prime yes prime , i , j , k ) = 1 if ( i , j ) ∈ A C and ( i−1 , j + 1 ) ∈ A k h ( prime yes prime , i , j , k ) = 1 if ( i , j ) ∈ A C and |NC| = 2 where NC = { n|n ∈ N ( i , j ) , n ∈ A k } out , fertE h ( prime no prime , i , j , k ) = 1 if ( i , j ) /∈ A C and |FT| = 0 where FT = { t| ( i , t ) ∈ A k } out , fertF h ( prime no prime , i , j , k ) = 1 if ( i , j ) /∈ A C and |FT| = 1 where FT = { t| ( t , j ) ∈ A k } mon h ( prime yes prime , i , j ) = 1 if ( i , j ) ∈ A C and |i−j| = 2 Table 1 : Feature Functions .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">x</definiendum>
				<definiendum id="2">C and pos ( e</definiendum>
				<definiens id="0">the set of classes , where each class denotes the existence or absence of a link in the combined alignment , and</definiens>
				<definiens id="1">the set of features above , we generate various feature functions h ( y , x ) , where y ∈ Y and</definiens>
				<definiens id="2">prime no prime , i , j ) = 1 if ( i , j ) /∈ A C and pos ( f j ) = V erb out h ( prime yes prime , i , j , k ) = 1 if ( i , j ) ∈ A C and ( i , j ) ∈ A k out , neigh h ( prime yes prime , i , j , k ) = 1 if ( i , j ) ∈ A C and ( i−1 , j + 1 ) ∈ A k h ( prime yes prime , i , j , k ) = 1 if ( i , j ) ∈ A C and |NC| = 2 where NC = { n|n ∈ N ( i , j ) , n ∈ A k } out , fertE h ( prime no prime , i , j , k ) = 1 if ( i , j ) /∈ A C and |FT| = 0 where FT = { t| ( i , t ) ∈ A k } out , fertF h ( prime no prime , i , j , k ) = 1 if ( i , j ) /∈ A C and |FT| = 1 where FT = { t| ( t , j ) ∈ A k } mon h</definiens>
			</definition>
			<definition id="7">
				<sentence>Precision ( Pr ) , recall ( Rc ) and alignment error rate ( AER ) are defined as follows : 6 Pr = |A∩P| |A| Rc = |A∩S| |S| AER = 1− |A∩S| + |A∩P| |A| + |S| 5 Note that both GIZA++ and SAHMM are unsupervised learning systems .</sentence>
				<definiendum id="0">Precision ( Pr )</definiendum>
				<definiens id="0">recall ( Rc ) and alignment error rate ( AER ) are defined as follows : 6 Pr = |A∩P| |A| Rc = |A∩S| |S| AER = 1− |A∩S| + |A∩P| |A| + |S| 5 Note that both GIZA++ and SAHMM are unsupervised learning systems</definiens>
			</definition>
			<definition id="8">
				<sentence>Moreover , ACME provides similar relative improvements for different sizes of training data for the input alignment systems .</sentence>
				<definiendum id="0">ACME</definiendum>
				<definiens id="0">provides similar relative improvements for different sizes of training data for the input alignment systems</definiens>
			</definition>
</paper>

		<paper id="2040">
			<definition id="0">
				<sentence>A route plan is a linked list of arcs between nodes representing locations and decision-points in the world .</sentence>
				<definiendum id="0">route plan</definiendum>
				<definiens id="0">a linked list of arcs between nodes representing locations and decision-points in the world</definiens>
			</definition>
			<definition id="1">
				<sentence>A log le created by the VR engine recorded the DF’s coordinates , gaze angle , and the position of objects in the world .</sentence>
				<definiendum id="0">DF’s</definiendum>
				<definiens id="0">coordinates , gaze angle , and the position of objects in the world</definiens>
			</definition>
			<definition id="2">
				<sentence>As each subsequent target on the list is selected , content planning considers the tuple of variables a0 ID , LOCa1 where ID is an identi er for the target and LOC is the DF’s location ( his Cartesian coordinates and orientation angle ) .</sentence>
				<definiendum id="0">LOC</definiendum>
				<definiens id="0">content planning considers the tuple of variables a0 ID , LOCa1 where ID is an identi er for the target</definiens>
			</definition>
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>where V is the set of all the words in our vocabulary .</sentence>
				<definiendum id="0">V</definiendum>
			</definition>
			<definition id="1">
				<sentence>Here P ( w|ΘC ) is the probability of word w given by the collection language model ΘC , which is usually estimated using the whole collection of documents C , e.g. , P ( w|ΘC ) = P d∈C c ( d , w ) P d∈C |d| .</sentence>
				<definiendum id="0">P ( w|ΘC )</definiendum>
				<definiendum id="1">ΘC</definiendum>
				<definiens id="0">the probability of word w given by the collection language model</definiens>
			</definition>
			<definition id="2">
				<sentence>Both cluster information and collection information are used to improve the estimate of the document model : P ( w|ˆΘd ) = λc ( w , d ) |d| + ( 1 − λ ) × [ βP ( w|ΘLd ) + ( 1 − β ) P ( w|ΘC ) ] , where ΘLd stands for document d’s cluster model and λ and β are smoothing parameters .</sentence>
				<definiendum id="0">ΘLd</definiendum>
				<definiens id="0">document d’s cluster model and λ and β are smoothing parameters</definiens>
			</definition>
			<definition id="3">
				<sentence>Function φ serves to balance the con dence between d and its neighborhood N ( d ) in the model estimation step .</sentence>
				<definiendum id="0">Function φ</definiendum>
				<definiens id="0">serves to balance the con dence between d and its neighborhood N ( d ) in the model estimation step</definiens>
			</definition>
</paper>

		<paper id="2033">
			<definition id="0">
				<sentence>We start by decomposing each tree into its constituents , with each constituent being a 4-tuple [ label , begin , end , weight ] , where label is the phrase structure type , such as NP or VP , begin is the index of the word where the constituent starts , end is the index of the word where the constituent ends plus one , and weight is the weight of the constituent .</sentence>
				<definiendum id="0">label</definiendum>
				<definiendum id="1">begin</definiendum>
				<definiens id="0">the index of the word where the constituent starts , end is the index of the word where the constituent ends plus one , and weight is the weight of the constituent</definiens>
			</definition>
</paper>

		<paper id="2038">
			<definition id="0">
				<sentence>The purpose of information extraction ( IE ) is to find desired pieces of information in natural language texts and store them in a form that is suitable for automatic querying and processing .</sentence>
				<definiendum id="0">information extraction</definiendum>
				<definiens id="0">to find desired pieces of information in natural language texts and store them in a form that is suitable for automatic querying and processing</definiens>
			</definition>
</paper>

		<paper id="2023">
			<definition id="0">
				<sentence>A significant exception is the work of Conroy and O’Leary ( 2001 ) , which employs an HMM model with pivoted QR decomposition for text summarization .</sentence>
				<definiendum id="0">significant exception</definiendum>
				<definiens id="0">employs an HMM model with pivoted QR decomposition for text summarization</definiens>
			</definition>
			<definition id="1">
				<sentence>N : The state space , representing a set of states where N is the total number of states in the model ; O = o1k , o2k , o3k , ... oMk : The set of observation vectors , where each vector is of size k ; A = { aij } : The transition probability matrix , where aij is the probability of transition from state i to state j ; bj ( ojk ) : The observation probability density function , estimated by ΣMk=1cjkN ( ojk , µjk , Σjk ) , where ojk denotes the feature vector ; N ( ojk , µjk , Σjk ) denotes a single Gaussian density function with mean of µjk and covariance matrix Σjk for the state j , with M the number of mixture components and cjk the weight of the kth mixture component ; Π = pii : The initial state probability distribution .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">aij</definiendum>
				<definiendum id="3">ojk</definiendum>
				<definiendum id="4">Σjk )</definiendum>
				<definiens id="0">The state space , representing a set of states where</definiens>
				<definiens id="1">the total number of states in the model ; O = o1k , o2k , o3k , ... oMk : The set of observation vectors , where each vector is of size k ; A = { aij } : The transition probability matrix</definiens>
				<definiens id="2">a single Gaussian density function with mean of µjk and covariance matrix Σjk for the state j</definiens>
				<definiens id="3">cjk the weight of the kth mixture component</definiens>
			</definition>
			<definition id="2">
				<sentence>It includes speaking rate ( the ratio of voiced/total frames ) ; F0 minimum , maximum , and mean ; F0 range and slope ; minimum , maximum , and mean RMS energy ( minDB , maxDB , meanDB ) ; RMS slope ( slopeDB ) ; sentence duration ( timeLen = endtime starttime ) .</sentence>
				<definiendum id="0">slopeDB</definiendum>
				<definiens id="0">the ratio of voiced/total frames</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>For a given parse y , its score is the sum of the scores of all its dependency links ( i , j ) ∈ y : s ( x , y ) = summationdisplay ( i , j ) ∈y d ( i , j ) = summationdisplay ( i , j ) ∈y w · f ( i , j ) ( 2 ) where the link ( i , j ) indicates a head-child dependency between the token at position i and the token at position j. The score d ( i , j ) of each dependency link ( i , j ) is further decomposed as the weighted sum of its features f ( i , j ) .</sentence>
				<definiendum id="0">score d</definiendum>
				<definiendum id="1">dependency link</definiendum>
				<definiens id="0">the sum of the scores of all its dependency links ( i , j ) ∈ y : s ( x , y ) = summationdisplay ( i</definiens>
				<definiens id="1">further decomposed as the weighted sum of its features f ( i , j )</definiens>
			</definition>
			<definition id="1">
				<sentence>We reimplemented Eisner’s decoder ( Eisner , 1996 ) , which searches among all projective parse trees , and the Chu-Liu-Edmonds’ decoder ( Chu and Liu , 1965 ; Edmonds , 1967 ) , which searches in the space of both projective and non-projective parses .</sentence>
				<definiendum id="0">Chu-Liu-Edmonds’ decoder</definiendum>
				<definiens id="0">searches among all projective parse trees</definiens>
				<definiens id="1">searches in the space of both projective and non-projective parses</definiens>
			</definition>
			<definition id="2">
				<sentence>Both algorithms are O ( N 3 ) , where N is the number of words in a sentence .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of words in a sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>The Bayes Point Machine ( BPM ) achieves good generalization similar to that of large margin methods , but is motivated by a very different philosophy of Bayesian learning or model averaging .</sentence>
				<definiendum id="0">Bayes Point Machine ( BPM )</definiendum>
				<definiens id="0">achieves good generalization similar to that of large margin methods</definiens>
			</definition>
			<definition id="4">
				<sentence>The term |V ( D ) | is the size of the version space V ( D ) , which is the set of weights w i that is consistent with the training data ( i.e. the set of w i that classifies the training data with zero error ) .</sentence>
				<definiendum id="0">term |V</definiendum>
			</definition>
			<definition id="5">
				<sentence>However , most content words consist of more than one morpheme , typically two .</sentence>
				<definiendum id="0">most content words</definiendum>
			</definition>
			<definition id="6">
				<sentence>Complete match indicates how many sentences were a complete match with the oracle dependency parse .</sentence>
				<definiendum id="0">Complete match</definiendum>
			</definition>
			<definition id="7">
				<sentence>, MIRA , a large margin classifier , and the current Bayes Point Machine results .</sentence>
				<definiendum id="0">MIRA</definiendum>
			</definition>
			<definition id="8">
				<sentence>MIRA is significantly better than BPM when measuring dependency accuracy and root accuracy , but BPM is significantly better when measuring sentences that match completely .</sentence>
				<definiendum id="0">BPM</definiendum>
				<definiens id="0">significantly better than BPM when measuring dependency accuracy and root accuracy , but</definiens>
			</definition>
</paper>

		<paper id="3004">
			<definition id="0">
				<sentence>Many researchers have explored syntax-based methods , for instance , Wu ( 1996 ) and Chiang ( 2005 ) both uses binary-branching synchronous context-free grammars ( SCFGs ) .</sentence>
				<definiendum id="0">syntax-based methods</definiendum>
				<definiens id="0">synchronous context-free grammars ( SCFGs )</definiens>
			</definition>
			<definition id="1">
				<sentence>These translations have been formalized as a synchronous context-free grammar ( SCFG ) that generates two languages simultaneously ( Aho and Ullman , 1972 ) , and equivalently , as a top-down tree-to-string transducer ( G´ecseg and Steinby , 1984 ) .</sentence>
				<definiendum id="0">SCFG</definiendum>
				<definiens id="0">a synchronous context-free grammar (</definiens>
			</definition>
			<definition id="2">
				<sentence>These rules can be learned from a parallel corpus using English parsetrees , Chinese strings , and word alignment ( Galley et al. , 2004 ) .</sentence>
				<definiendum id="0">Chinese</definiendum>
			</definition>
			<definition id="3">
				<sentence>With memoizationm , we get a dynamic programming algorithm that is guaranteed to run in O ( n ) time where n is the length of the input string , since the size of the parsetree is proportional to n. Similar algorithms have also been proposed for dependency-based translation ( Lin , 2004 ; Ding and Palmer , 2005 ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the input string</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>We then use classification and regression trees ( CART ) as a means to evaluate the relative importance and salience of our features .</sentence>
				<definiendum id="0">CART</definiendum>
			</definition>
			<definition id="1">
				<sentence>The data we are using for the experiments in this paper comes from a 2002 trial administration of TOEFLiBT® ( Test Of English as a Foreign Language—internet-Based Test ) for nonnative speakers ( LanguEdge ™ ) .</sentence>
				<definiendum id="0">TOEFLiBT® ( Test</definiendum>
				<definiens id="0">Of English as a Foreign Language—internet-Based Test ) for nonnative speakers ( LanguEdge ™ )</definiens>
			</definition>
			<definition id="2">
				<sentence>A wide variety of disfluencies are accounted for , such as , e.g. , false starts , repetitions , fillers , or incomplete words .</sentence>
				<definiendum id="0">false</definiendum>
				<definiens id="0">starts , repetitions , fillers , or incomplete words</definiens>
			</definition>
			<definition id="3">
				<sentence>The disagreement between annotators , measured as word error rate ( WER = ( substitutions + deletions + insertions ) / ( substitutions + deletions + correct ) ) was slightly above 20 % ( only lexical entries were measured here ) .</sentence>
				<definiendum id="0">disagreement between annotators</definiendum>
				<definiens id="0">word error rate ( WER = ( substitutions + deletions + insertions ) / ( substitutions + deletions + correct</definiens>
			</definition>
			<definition id="4">
				<sentence>Our speech recognizer is a gender-independent Hidden Markov Model system that was trained on 200 hours of dictation data by native speakers of English .</sentence>
				<definiendum id="0">speech recognizer</definiendum>
				<definiens id="0">a gender-independent Hidden Markov Model system that was trained on 200 hours of dictation data by native speakers of English</definiens>
			</definition>
			<definition id="5">
				<sentence>In the alignhypo case , where SVM models were built based on features derived from ASR forced alignment and where these models were tested using ASR output in recognition mode , we see a general drop in performance – again except for P-C – which is to be expected as the training and test data were derived in different ways .</sentence>
				<definiendum id="0">SVM</definiendum>
				<definiens id="0">models were built based on features derived from ASR forced alignment</definiens>
				<definiens id="1">to be expected as the training and test data were derived in different ways</definiens>
			</definition>
			<definition id="6">
				<sentence>Automated essay scoring : A cross-disciplinary perspective .</sentence>
				<definiendum id="0">Automated essay scoring</definiendum>
				<definiens id="0">A cross-disciplinary perspective</definiens>
			</definition>
</paper>

		<paper id="2024">
			<definition id="0">
				<sentence>Named entity recognition ( NER ) is the task of identifying named entities in free text—typically personal names , organizations , gene-protein entities , and so on .</sentence>
				<definiendum id="0">NER</definiendum>
				<definiens id="0">the task of identifying named entities in free text—typically personal names , organizations , gene-protein entities</definiens>
			</definition>
			<definition id="1">
				<sentence>Both learners use dynamic programming to find the label sequence y = ( y1 , ... , yi , ... , yN ) for a word sequence x = ( x1 , ... , xi , ... , xN ) that maximizes the function W · summationtextif ( x , i , yi−1 , yi ) , where W is the learned weight vector and f is a vector of features computed from x , i , the label yi for xi , and the previous label yi−1 .</sentence>
				<definiendum id="0">xN</definiendum>
				<definiendum id="1">W</definiendum>
				<definiendum id="2">f</definiendum>
				<definiens id="0">dynamic programming to find the label sequence y = ( y1 , ... , yi , ... , yN ) for a word sequence x = ( x1 , ... , xi , ... ,</definiens>
				<definiens id="1">the learned weight vector and</definiens>
				<definiens id="2">a vector of features computed from x</definiens>
			</definition>
			<definition id="2">
				<sentence>The widely-used MUC-6 dataset includes news articles drawn from the Wall Street Journal .</sentence>
				<definiendum id="0">widely-used MUC-6 dataset</definiendum>
				<definiens id="0">includes news articles drawn from the Wall Street Journal</definiens>
			</definition>
			<definition id="3">
				<sentence>94 collection , extracted from the CSpace email corpus , which contains email messages sent by MBA students taking a management course conducted at Carnegie Mellon University in 1997 .</sentence>
				<definiendum id="0">CSpace email corpus</definiendum>
				<definiens id="0">contains email messages sent by MBA students taking a management course conducted at Carnegie Mellon University in 1997</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>That algorithm has complexity O ( n3|P| ) , where n is the length in words of the sentence parsed , and |P| is the number of grammar productions .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">|P|</definiendum>
				<definiens id="0">the number of grammar productions</definiens>
			</definition>
			<definition id="1">
				<sentence>In the case of bilexical grammars , where categories in binary grammars are annotated with their lexical heads , the grammar factor contributes an additional O ( n2|VD|3 ) complexity , leading to an overall O ( n5|VD|3 ) parsing complexity , where |VD| is the number of delexicalized non-terminals ( Eisner , 1997 ) .</sentence>
				<definiendum id="0">|VD|</definiendum>
			</definition>
			<definition id="2">
				<sentence>A PCFG is a CFG with a probability assigned to each production .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">a CFG with a probability assigned to each production</definiens>
			</definition>
			<definition id="3">
				<sentence>Binarized PCFGs are induced from a treebank whose trees have been factored so that n-ary productions with n &gt; 2 become sequences of n−1 binary productions .</sentence>
				<definiendum id="0">Binarized PCFGs</definiendum>
				<definiens id="0">a treebank whose trees have been factored so that n-ary productions with n &gt; 2 become sequences of n−1 binary productions</definiens>
			</definition>
			<definition id="4">
				<sentence>The number of tagger candidates k for all trials reported in this paper was 0.2n , where n is the length of the string .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the string</definiens>
			</definition>
			<definition id="5">
				<sentence>Evaluation includes the standard PARSEVAL measures labeled precision ( LP ) and labeled recall ( LR ) , plus the harmonic mean ( F-measure ) of these two scores .</sentence>
				<definiendum id="0">Evaluation</definiendum>
				<definiens id="0">includes the standard PARSEVAL measures labeled precision ( LP ) and labeled recall ( LR ) , plus the harmonic mean ( F-measure ) of these two scores</definiens>
			</definition>
			<definition id="6">
				<sentence>Parent annotation occurs prior to treebank factorization .</sentence>
				<definiendum id="0">Parent annotation</definiendum>
				<definiens id="0">occurs prior to treebank factorization</definiens>
			</definition>
			<definition id="7">
				<sentence>If we assume , however , that all unobserved sequences are given some epsilon1 count , then when cab = 0 , I ( ab ) = K −logca −logcb , ( 2 ) where K is a constant .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">a constant</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>We find that in clean read speech , unsupervised techniques can identify the underlying Mandarin tone categories with high accuracy , while even on noisier broadcast news speech , Mandarin tones can be recognized well above chance levels , with English pitch accent recognition at near the levels achieved with fully supervised Support Vector Machine ( SVM ) classifiers .</sentence>
				<definiendum id="0">Mandarin tones</definiendum>
			</definition>
</paper>

		<paper id="2013">
			<definition id="0">
				<sentence>Certain letters in Arabic script are often spelled inconsistently which leads to an increase in both sparsity ( multiple forms of the same word ) and ambiguity ( same form corresponding to multiple words ) .</sentence>
				<definiendum id="0">ambiguity</definiendum>
				<definiens id="0">same form corresponding to multiple words</definiens>
			</definition>
			<definition id="1">
				<sentence>MT04 is a mix of news , editorials and speeches , whereas MT05 , like the training data , is purely news .</sentence>
				<definiendum id="0">MT04</definiendum>
				<definiens id="0">a mix of news , editorials and speeches , whereas MT05 , like the training data , is purely news</definiens>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>The rst is the LDC CallHome Egyptian Colloquial Arabic ( ECA ) Corpus , consisting of transcriptions of phone conversations .</sentence>
				<definiendum id="0">rst</definiendum>
				<definiens id="0">the LDC CallHome Egyptian Colloquial Arabic ( ECA ) Corpus , consisting of transcriptions of phone conversations</definiens>
			</definition>
			<definition id="1">
				<sentence>ECA is a morphologically rich language that is almost exclusively used in informal spoken communication .</sentence>
				<definiendum id="0">ECA</definiendum>
				<definiens id="0">a morphologically rich language that is almost exclusively used in informal spoken communication</definiens>
			</definition>
			<definition id="2">
				<sentence>Model ECA ( ·102 ) Turkish ( ·102 ) dev eval dev eval baseline 3gram 4.108 4.128 6.385 6.438 hand-optimized FLM 4.440 4.327 4.269 4.479 GA-optimized FLM 4.325 4.179 6.414 6.637 NLM 3-gram 4.857 4.581 4.712 4.801 FNLM-NULL 5.672 5.381 9.480 9.529 FNLM-ALL 5.691 5.396 9.518 9.555 FNLM-TAIL 10 % 5.721 5.420 9.495 9.540 FNLM-LEAST 5.819 5.479 10.492 10.373 Table 1 : Average probability ( scaled by 102 ) of known words with unknown words in order-2 context The second corpus consists of Turkish newspaper text that has been morphologically annotated and disambiguated ( Hakkani-Tcurrency1ur et al. , 2002 ) , thus providing information about the word root , POS tag , number and case .</sentence>
				<definiendum id="0">Average probability</definiendum>
				<definiens id="0">scaled by 102 ) of known words with unknown words in order-2 context The second corpus consists of Turkish newspaper text that has been morphologically annotated and disambiguated ( Hakkani-Tcurrency1ur et al. , 2002 ) , thus providing information about the word root , POS tag , number and case</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>The predicate-argument dependencies are represented as 5-tuples : 〈hf , f , s , ha , l〉 , where hf is the lexical item of the lexical category expressing the dependency relation ; f is the lexical category ; s is the argument slot ; ha is the head word of the argument ; and l encodes whether the dependency is non-local .</sentence>
				<definiendum id="0">f</definiendum>
				<definiendum id="1">ha</definiendum>
				<definiens id="0">〈hf , f , s , ha</definiens>
				<definiens id="1">the lexical item of the lexical category expressing the dependency relation ;</definiens>
			</definition>
			<definition id="1">
				<sentence>The function fi is the integer-valued frequency function of the ith feature ; λi is the weight of the ith feature ; and ZS is a normalising constant .</sentence>
				<definiendum id="0">function fi</definiendum>
				<definiendum id="1">λi</definiendum>
				<definiendum id="2">ZS</definiendum>
				<definiens id="0">the weight of the ith feature</definiens>
				<definiens id="1">a normalising constant</definiens>
			</definition>
			<definition id="2">
				<sentence>The objective function is defined below , where L ( Λ ) is the likelihood and G ( Λ ) is a Gaussian prior term for smoothing .</sentence>
				<definiendum id="0">objective function</definiendum>
				<definiendum id="1">L</definiendum>
				<definiens id="0">the likelihood</definiens>
				<definiens id="1">a Gaussian prior term for smoothing</definiens>
			</definition>
			<definition id="3">
				<sentence>f ( ω ) − nsummationdisplay i=1 λ2i 2σ2 S1 , ... , Sm are the sentences in the training data ; pi1 , ... , pim are the corresponding gold-standard dependency structures ; ρ ( S ) is the set of possible 〈derivation , dependency-structure〉 pairs for S ; σ is a smoothing parameter ; and n is the number of features .</sentence>
				<definiendum id="0">pi1 , ...</definiendum>
				<definiendum id="1">ρ ( S )</definiendum>
				<definiendum id="2">σ</definiendum>
				<definiendum id="3">n</definiendum>
				<definiens id="0">the set of possible 〈derivation , dependency-structure〉 pairs for S ;</definiens>
				<definiens id="1">a smoothing parameter</definiens>
				<definiens id="2">the number of features</definiens>
			</definition>
			<definition id="4">
				<sentence>The estimation process attempts to make the expectations in ( 5 ) equal ( ignoring the Gaussian prior term ) .</sentence>
				<definiendum id="0">estimation process</definiendum>
				<definiens id="0">attempts to make the expectations in ( 5 ) equal ( ignoring the Gaussian prior term )</definiens>
			</definition>
			<definition id="5">
				<sentence>C , D , R , γ , δ〉 is a packed chart / feature forest G is a set of dependencies returned by the extraction procedure Let c be a conjunctive node Let d be a disjunctive node deps ( c ) is the set of dependencies on node c cdeps ( c ) = |deps ( c ) ∩ G| dmax ( c ) =summationtextd∈δ ( c ) dmax ( d ) + cdeps ( c ) dmax ( d ) = max { dmax ( c ) | c ∈ γ ( d ) } mark ( d ) : mark d as a correct node foreach c ∈ γ ( d ) if dmax ( c ) == dmax ( d ) mark c as a correct node foreach dprime ∈ δ ( c ) mark ( dprime ) foreach dr ∈ R such that dmax .</sentence>
				<definiendum id="0">δ〉</definiendum>
				<definiendum id="1">c )</definiendum>
				<definiens id="0">a packed chart / feature forest G is a set of dependencies returned by the extraction</definiens>
				<definiens id="1">the set of dependencies on node c cdeps ( c ) = |deps ( c ) ∩ G| dmax ( c ) =summationtextd∈δ ( c ) dmax ( d ) + cdeps ( c ) dmax ( d ) = max { dmax ( c ) | c ∈ γ ( d ) } mark ( d ) : mark d as a correct node foreach c ∈ γ ( d ) if dmax ( c ) == dmax ( d ) mark c as a correct node foreach dprime ∈ δ ( c ) mark ( dprime ) foreach dr ∈ R such that dmax</definiens>
			</definition>
			<definition id="6">
				<sentence>( dr ) = |G| mark ( dr ) Figure 2 : Finding nodes in derivations consistent with a partial dependency structure • C is a set of conjunctive nodes ; • D is a set of disjunctive nodes ; • R ⊆ D is a set of root disjunctive nodes ; • γ : D → 2C isaconjunctivedaughterfunction ; • δ : C → 2D is a disjunctive daughter function .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">Finding nodes in derivations consistent with a partial dependency structure •</definiens>
				<definiens id="1">a set of conjunctive nodes ; • D is a set of disjunctive nodes ; • R ⊆ D is a set of root disjunctive nodes</definiens>
			</definition>
			<definition id="7">
				<sentence>In Figure 2 , cdeps ( c ) is the number of dependencies on conjunctive node c which appear in partial structure G ; dmax ( c ) is the maximum number of dependencies in G produced by any sub-derivation headed by c ; dmax ( d ) is the same value for disjunctive node d. Recursive definitions for calculating these values are given ; the base case occurs when conjunctive nodes have no disjunctive daughters .</sentence>
				<definiendum id="0">c )</definiendum>
				<definiendum id="1">dmax ( c )</definiendum>
				<definiens id="0">the maximum number of dependencies in G produced by any sub-derivation headed by c</definiens>
				<definiens id="1">the same value for disjunctive node d. Recursive definitions for calculating these values are given ; the base case occurs when conjunctive nodes have no disjunctive daughters</definiens>
			</definition>
			<definition id="8">
				<sentence>The SentAcc column gives the percentage of training sentences for which the partial dependency structures are completely correct .</sentence>
				<definiendum id="0">SentAcc column</definiendum>
				<definiens id="0">gives the percentage of training sentences for which the partial dependency structures</definiens>
			</definition>
			<definition id="9">
				<sentence>The dependency model uses the same set of features described in Clark and Curran ( 2004b ) : dependency features representing predicate-argument dependencies ( with and without distance measures ) ; rule instantiation features encoding the combining categories together with the result category ( with and without a lexical head ) ; lexical category features , consisting of word–category pairs at the leaf nodes ; and root category features , consisting of headword–category pairs at the root nodes .</sentence>
				<definiendum id="0">dependency model</definiendum>
				<definiens id="0">uses the same set of features described in Clark and Curran ( 2004b ) : dependency features representing predicate-argument dependencies ( with and without distance measures</definiens>
			</definition>
			<definition id="10">
				<sentence>The CCG parsing consists of two phases : first the supertagger assigns the most probable categories to each word , and then the small number of combinatory rules , plus the type-changing and punctuation rules , are used with the CKY algorithm to build a packedchart.5 WeusethemethoddescribedinClark and Curran ( 2004b ) for integrating the supertagger with the parser : initially a small number of categories is assigned to each word , and more categories are requested if the parser can not find a spanning analysis .</sentence>
				<definiendum id="0">CCG parsing</definiendum>
				<definiens id="0">consists of two phases : first the supertagger assigns the most probable categories to each word , and then the small number of combinatory rules , plus the type-changing and punctuation rules</definiens>
				<definiens id="1">integrating the supertagger with the parser : initially a small number of categories is assigned to each word , and more categories are requested if the parser can not find a spanning analysis</definiens>
			</definition>
			<definition id="11">
				<sentence>The training data consists of over 1,000 annotated questions which took less than a week to create .</sentence>
				<definiendum id="0">data</definiendum>
				<definiens id="0">consists of over 1,000 annotated questions which took less than a week to create</definiens>
			</definition>
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>Pourpre calculates an idfor count-based , stemmed , unigram similarity between each nugget description and each tinction between ‘vital’ and ‘okay’ .</sentence>
				<definiendum id="0">Pourpre</definiendum>
				<definiens id="0">calculates an idfor count-based , stemmed , unigram similarity between each nugget description and each tinction between ‘vital’ and ‘okay’</definiens>
			</definition>
			<definition id="1">
				<sentence>Nuggeteer makes scores interpretable by making binary decisions about each nugget and each system response , just as assessors do , and then calculating the final score in the usual way .</sentence>
				<definiendum id="0">Nuggeteer</definiendum>
				<definiens id="0">makes scores interpretable by making binary decisions about each nugget and each system response</definiens>
			</definition>
			<definition id="2">
				<sentence>w n ) &gt; 0 0 otherwise ( 1 ) where count ( g , w 1 ... w n ) is the number of occurrences of the n-gram in responses containing the nugget g. Then informativeness is : I ( g , w 1 ... w n ) = 1 − summationtext g prime ∈G i ( g prime , w 1 ... w n ) |G| ( 2 ) This captures the Bayesian intuition that the more outcomes a piece of evidence is associated with , the less confidence we can have in predicting the outcome based on that evidence .</sentence>
				<definiendum id="0">Bayesian intuition</definiendum>
				<definiens id="0">the number of occurrences of the n-gram in responses containing the nugget g. Then informativeness</definiens>
			</definition>
			<definition id="3">
				<sentence>POURPRE ROUGE NUGGETEER Run micro , cnt macro , cnt micro , idf macro , idf default stop nostem , bigram , micro , idf D 2003 ( β = 3 ) 0.846 0.886 0.848 0.876 0.780 0.816 0.879 D 2003 ( β = 5 ) 0.890 0.878 0.859 0.875 0.807 0.843 0.849 O 2004 ( β = 3 ) 0.785 0.833 0.806 0.812 0.780 0.786 0.898 O 2005 ( β = 3 ) 0.598 0.709 0.679 0.698 0.662 0.670 0.858 R 2005 ( β = 3 ) 0.697 1 Table 2 : Kendall’s τ correlation between rankings generated by POURPRE/ROUGE/NUGGETEER and official scores , for each data set ( D=“definition” , O=“other” , R=“relationship” ) .</sentence>
				<definiendum id="0">D=“definition”</definiendum>
				<definiens id="0">Kendall’s τ correlation between rankings generated by POURPRE/ROUGE/NUGGETEER and official scores</definiens>
			</definition>
			<definition id="4">
				<sentence>POURPRE NUGGETEER Run R 2 R 2 √ mse D 2003 ( β = 3 ) 0.963 0.966 0.067 D 2003 ( β = 5 ) 0.965 0.971 0.077 O 2004 ( β = 3 ) 0.929 0.982 0.026 O 2005 ( β = 3 ) 0.916 0.952 0.026 R 2005 ( β = 3 ) 0.764 0.993 0.009 Table 3 : Correlation ( R 2 ) and Root Mean Squared Error ( √ mse ) between scores generated by Pourpre/Nuggeteer and official scores , for the same settings as the τ comparison above .</sentence>
				<definiendum id="0">POURPRE NUGGETEER</definiendum>
				<definiens id="0">√ mse ) between scores generated by Pourpre/Nuggeteer and official scores</definiens>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>The first corpus , which we refer to as the Olney &amp; Cai corpus , is a set of dialogues selected randomly from the same corpus Olney and Cai selected their corpus from ( Olney and Cai , 2005 ) .</sentence>
				<definiendum id="0">Cai corpus</definiendum>
				<definiens id="0">a set of dialogues selected randomly from the same corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>TextTiling measures the term-overlap between adjacent regions in the discourse .</sentence>
				<definiendum id="0">TextTiling</definiendum>
				<definiens id="0">measures the term-overlap between adjacent regions in the discourse</definiens>
			</definition>
</paper>

		<paper id="3008">
</paper>

		<paper id="4001">
			<definition id="0">
				<sentence>As input , InfoMagnets accepts a corpus of textual documents .</sentence>
				<definiendum id="0">InfoMagnets</definiendum>
				<definiens id="0">accepts a corpus of textual documents</definiens>
			</definition>
			<definition id="1">
				<sentence>As noted above , each InfoMagnet represents a topic concept through a collection of words ( from the corpus ) that convey that concept .</sentence>
				<definiendum id="0">InfoMagnet</definiendum>
				<definiens id="0">a topic concept through a collection of words ( from the corpus ) that convey that concept</definiens>
			</definition>
			<definition id="2">
				<sentence>“Snapping” is a way of overriding the attraction between the document and other InfoMagnets .</sentence>
				<definiendum id="0">“Snapping”</definiendum>
				<definiens id="0">a way of overriding the attraction between the document and other InfoMagnets</definiens>
			</definition>
			<definition id="3">
				<sentence>LSA is a dimensionality reduction technique that can be used to compute the semantic similarity between text spans of arbitrary size .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">a dimensionality reduction technique that can be used to compute the semantic similarity between text spans of arbitrary size</definiens>
			</definition>
			<definition id="4">
				<sentence>Cluster centroids ( InfoMagnets ) and documents ( or topic segments ) are all treated as bag-of-words .</sentence>
				<definiendum id="0">Cluster centroids</definiendum>
			</definition>
			<definition id="5">
				<sentence>Their vector-space representation is the sum of the LSA vectors of their constituent terms .</sentence>
				<definiendum id="0">vector-space representation</definiendum>
				<definiens id="0">the sum of the LSA vectors of their constituent terms</definiens>
			</definition>
			<definition id="6">
				<sentence>gCLUTO : An Interactive Clustering , Visualization , and Analysis System .</sentence>
				<definiendum id="0">gCLUTO</definiendum>
				<definiens id="0">An Interactive Clustering , Visualization , and Analysis System</definiens>
			</definition>
			<definition id="7">
				<sentence>A First Evaluation of the Instructional Value of Negotiable Problem Solving Goals on the Exploratory Learning Continuum , Proceedings of AI in Education ‘05 Michael Steinbach , George Karypis , and Vipin Kuma ( 2000 ) .</sentence>
				<definiendum id="0">First Evaluation</definiendum>
				<definiens id="0">of the Instructional Value of Negotiable Problem Solving Goals on the Exploratory Learning Continuum</definiens>
			</definition>
</paper>

		<paper id="5005">
			<definition id="0">
				<sentence>Anoop Sarkar is an Assistant Professor in the School of Computing Science at Simon Fraser University .</sentence>
				<definiendum id="0">Anoop Sarkar</definiendum>
				<definiens id="0">an Assistant Professor in the School of Computing Science at Simon Fraser University</definiens>
			</definition>
</paper>

		<paper id="2045">
			<definition id="0">
				<sentence>Each subtopic is a link to a page ( or pages ) with paragraphs about the topic ( Lemieux ) with respect to such subtopics as Games , Seasons , Pittsburgh Penguins , Wayne Gretzky , and others , including the unpromising subtopic ice .</sentence>
				<definiendum id="0">subtopic</definiendum>
				<definiens id="0">a link to a page ( or pages</definiens>
			</definition>
			<definition id="1">
				<sentence>Discourse Topic is an under-theorized notion in linguistic theory : not all linguists agree that the notion of Discourse Topic is required in discourse analysis at all ( cf. Asher , 2004 ) .</sentence>
				<definiendum id="0">Discourse Topic</definiendum>
			</definition>
</paper>

		<paper id="3007">
			<definition id="0">
				<sentence>Latent Semantic Analysis ( LSA ) ( Deerwester et al. , 1990 ) is one of the best known dimensionality reduction algorithms in information retrieval .</sentence>
				<definiendum id="0">Latent Semantic Analysis ( LSA )</definiendum>
				<definiens id="0">one of the best known dimensionality reduction algorithms in information retrieval</definiens>
			</definition>
			<definition id="1">
				<sentence>Different dimensionality reduction techniques impose different conditions on how the similarities are preserved .</sentence>
				<definiendum id="0">Different dimensionality reduction techniques</definiendum>
				<definiens id="0">impose different conditions on how the similarities are preserved</definiens>
			</definition>
			<definition id="2">
				<sentence>235 Locality Preserving Projection Algorithm The Locality Preserving Projection algorithm ( LPP ) ( He and Niyogi , 2003 ) is a graph-based dimensionality reduction algorithm that computes low dimensional document vectors by preserving local similarities between the documents .</sentence>
				<definiendum id="0">Locality Preserving Projection algorithm</definiendum>
				<definiendum id="1">LPP</definiendum>
				<definiens id="0">a graph-based dimensionality reduction algorithm that computes low dimensional document vectors by preserving local similarities between the documents</definiens>
			</definition>
			<definition id="3">
				<sentence>At higher levels of recall , LSA achieves a precision that is about 0.1 better than LPP .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">achieves a precision that is about 0.1 better than LPP</definiens>
			</definition>
			<definition id="4">
				<sentence>The columns of UT are k-dimensional term vectors combinations of term vectors ˆD = UTD In step 2 of the GLSA algorithm we used pointwise mutual information ( PMI ) as the co-occurrence based measure of semantic associations between pairs of the vocabulary terms .</sentence>
				<definiendum id="0">PMI</definiendum>
				<definiens id="0">the co-occurrence based measure of semantic associations between pairs of the vocabulary terms</definiens>
			</definition>
			<definition id="5">
				<sentence>The GLSA document vectors improved the classification accuracy over the baseline and outperformed LSA document vectors .</sentence>
				<definiendum id="0">GLSA document vectors</definiendum>
			</definition>
</paper>

		<paper id="2015">
			<definition id="0">
				<sentence>The OntoNotes project focuses on a domain independent representation of literal meaning that includes predicate structure , word sense , ontology linking , and coreference .</sentence>
				<definiendum id="0">OntoNotes project</definiendum>
				<definiens id="0">focuses on a domain independent representation of literal meaning that includes predicate structure , word sense , ontology linking , and coreference</definiens>
			</definition>
			<definition id="1">
				<sentence>A firststage parser matches the Collins ( 2003 ) parser on which it is based on the Parseval metric , while simultaneously achieving near state-of-the-art performance on recovering function tags ( F-measure 89.0 ) .</sentence>
				<definiendum id="0">firststage parser</definiendum>
				<definiens id="0">matches the Collins ( 2003 ) parser on which it is based on the Parseval metric</definiens>
			</definition>
			<definition id="2">
				<sentence>, WN2 : “drive to school , ” , WN3 : “drive her to school , ” , WN12 : “this truck drives well , ” WN13 : “he drives a taxi , ” , WN14 : “The car drove around the corner , ” , WN:16 : “drive the turnpike to work , ” G2 : force to a position or stance NP drive NP/PP/infinitival WN4 : “He drives me mad. , ” WN6 : “drive back the invaders , ” WN7 : “She finally drove him to change jobs , ” WN8 : “drive a nail , ” WN15 : “drive the herd , ” WN22 : “drive the game.”</sentence>
				<definiendum id="0">WN2</definiendum>
				<definiendum id="1">WN14</definiendum>
				<definiens id="0">“The car drove around the corner</definiens>
			</definition>
			<definition id="3">
				<sentence>Omega , which has been used for MT , summarization , and database alignment , has been assembled semi-automatically by merging a variety of sources , including Princeton’s WordNet , New Mexico State University’s Mikrokosmos , and a variety of Upper Models , including DOLCE ( Gangemi et al. , 2002 ) , SUMO ( Niles and Pease , 2001 ) , and ISI’s Upper Model , which are in the process of being reconciled .</sentence>
				<definiendum id="0">Omega</definiendum>
				<definiendum id="1">SUMO</definiendum>
				<definiendum id="2">ISI’s Upper Model</definiendum>
				<definiens id="0">used for MT , summarization , and database alignment , has been assembled semi-automatically by merging a variety of sources</definiens>
			</definition>
			<definition id="4">
				<sentence>PropBank II annota59 tion ( eventuality ID’s , coarse-grained sense tags , nominal coreference and selected discourse connectives ) is being applied to a small ( 100K ) parallel Chinese/English corpus ( Babko-Malaya et al. , 2004 ) .</sentence>
				<definiendum id="0">Chinese/English corpus</definiendum>
				<definiens id="0">tion ( eventuality ID’s , coarse-grained sense tags , nominal coreference and selected discourse connectives</definiens>
			</definition>
			<definition id="5">
				<sentence>The OntoNotes representation extends these annotations , and allows eventual inclusion of additional shallow semantic representations for other phenomena , including temporal and spatial relations , numerical expressions , deixis , etc .</sentence>
				<definiendum id="0">OntoNotes representation</definiendum>
			</definition>
			<definition id="6">
				<sentence>In intent and in many details , OntoNotes is compatible with all these efforts , which may one day all participate in a larger multilingual corpus integration effort .</sentence>
				<definiendum id="0">OntoNotes</definiendum>
				<definiens id="0">may one day all participate in a larger multilingual corpus integration effort</definiens>
			</definition>
</paper>

		<paper id="2032">
			<definition id="0">
				<sentence>The NIGHTINGALE system searches a diverse news corpus to return answers to user queries .</sentence>
				<definiendum id="0">NIGHTINGALE system</definiendum>
				<definiens id="0">searches a diverse news corpus to return answers to user queries</definiens>
			</definition>
			<definition id="1">
				<sentence>For English , we stem the input before calculating these features , using an implementation of the Porter stemmer ( Porter , 1980 ) ; we have not yet attempted to identify root forms for Mandarin or Arabic .</sentence>
				<definiendum id="0">Porter stemmer</definiendum>
				<definiens id="0">yet attempted to identify root forms for Mandarin or Arabic</definiens>
			</definition>
</paper>

		<paper id="2034">
			<definition id="0">
				<sentence>A phrase ( bunsetsu ) in Japanese is a basic component of sentences , and consists of one or more content words and zero or more function words .</sentence>
				<definiendum id="0">phrase</definiendum>
				<definiendum id="1">bunsetsu</definiendum>
				<definiens id="0">a basic component of sentences , and consists of one or more content words and zero or more function words</definiens>
			</definition>
			<definition id="1">
				<sentence>Here , noun-x means all types of nouns except common nouns , i.e. verbal nouns , proper nouns , pronouns , etc. “ ( noun-x | verb | adjective ) ?</sentence>
				<definiendum id="0">noun-x</definiendum>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>Let G be an undirected graphical model over random vectors y and x. As a typical special case , y = { y t } and x = { x t } for t = 1 , . . . , T , so that y is a labeling of an observed sequence x. For a given collection C = { { y c , x c } } of cliques in G , a CRF models the conditional probability of an assignment to labels y given the observed variables x as : p Λ ( y|x ) = 1 Z ( x ) productdisplay c∈C Φ ( y c , x c ) , ( 1 ) where Φ is a potential function and the partition function Z ( x ) = summationtext y producttext c∈C Φ ( y c , x c ) is a normalization factor over all possible label assignments .</sentence>
				<definiendum id="0">y</definiendum>
				<definiendum id="1">partition function Z</definiendum>
				<definiendum id="2">c )</definiendum>
				<definiens id="0">{ y c , x c } } of cliques in G , a CRF models the conditional probability of an assignment to labels y given the observed variables x as : p Λ ( y|x ) = 1 Z ( x ) productdisplay c∈C Φ ( y c , x c ) , ( 1 ) where Φ is a potential function and the</definiens>
			</definition>
			<definition id="1">
				<sentence>We assume the potentials factorize according to a set of features { f k } , which are given and fixed , so that Φ ( y c , x c ) = exp parenleftBigg summationdisplay k λ k f k ( y c , x c ) parenrightBigg ( 2 ) The model parameters are a set of real weightsΛ = { λ k } , one weight for each feature .</sentence>
				<definiendum id="0">model parameters</definiendum>
				<definiens id="0">the potentials factorize according to a set of features { f k } , which are given and fixed , so that Φ ( y c , x c ) = exp parenleftBigg summationdisplay k λ k f k ( y c</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>lemma : freepos : Verb features : Past , Pass , T1 , Proposition lemma : _Xpos : PronTsub lemma : hostagepos : Noun features : Plur , Humn , Count , Anim , Conc , Humn_sr Tobj lemma : sixpos : Adj features : Quant , Plur , Num , Value 6Lops lemma : Iraqpos : Noun features : Sing , PrprN , Pers3 , Cntry Locn_in Figure 1 : Logical form produced by NLPWIN for the sentence “Six hostages in Iraq were freed.”</sentence>
				<definiendum id="0">Quant</definiendum>
			</definition>
			<definition id="1">
				<sentence>The NLPWIN parser assigns a normalized numeric value feature to each piece of text inferred to correspond to a numeric value ; this allows us to align “6th” to “sixth” in Test Ex .</sentence>
				<definiendum id="0">NLPWIN parser</definiendum>
				<definiens id="0">assigns a normalized numeric value feature to each piece of text inferred to correspond to a numeric value ; this allows us to align “6th” to “sixth” in Test Ex</definiens>
			</definition>
			<definition id="2">
				<sentence>Negation Mismatch : ALIGN ( h , t ) ∧ NEG ( t ) negationslash= NEG ( h ) → False .</sentence>
				<definiendum id="0">Negation Mismatch</definiendum>
			</definition>
			<definition id="3">
				<sentence>For any two aligned verb nodes ( h1 , t1 ) , we consider each noun child h2 of h1 possessing any of 36 Hypothesis Textkill Verb Prime MinisterRobert Malval Noun Tobj AristideNoun Tsub killVerb Prime MinisterRobert Malval Noun AristideNoun Tsub conferenceNoun Tobj callVerbAttrib conferenceNoun TobjTsub Port-au-PrinceNounLocn_in Figure 3 : Example of object movement signaling false entailment the subject , object , or indirect object relations to h1 , i.e. , there exists REL ( h1 , h2 ) such that REL ∈ { SUBJ , OBJ , IND } .</sentence>
				<definiendum id="0">OBJ</definiendum>
				<definiens id="0">object movement signaling false entailment the subject</definiens>
			</definition>
			<definition id="4">
				<sentence># 908 ) : T : Time Warner is the world’s largest media and Internet company .</sentence>
				<definiendum id="0">Time Warner</definiendum>
				<definiens id="0">the world’s largest media and Internet company</definiens>
			</definition>
			<definition id="5">
				<sentence>We would expect a higher CWS to result from learning a more appropriate confidence function ; nonetheless our overall 6As in ( Dagan et al. , 2005 ) we compute the confidenceweighted score ( or “average precision” ) over n examples { c1 , c2 , ... , cn } ranked in order of decreasing confidence as cws = 1n summationtextni=1 ( # correct-up-to-rank-i ) i Dev Set Test Set Task acc cws acc cws CD 0.8061 0.8357 0.7867 0.8261 RC 0.5534 0.5885 0.6429 0.6476 IR 0.6857 0.6954 0.6000 0.6571 MT 0.7037 0.7145 0.6000 0.6350 IE 0.5857 0.6008 0.5917 0.6275 QA 0.7111 0.7121 0.5308 0.5463 PP 0.7683 0.7470 0.5200 0.5333 All 0.6878 0.6888 0.6250 0.6534 Table 2 : Summary of accuracies and confidenceweighted scores , by task Alignment Feature Dev Test Synonym Match 0.0106 0.0038 Derivational Form 0.0053 0.0025 Paraphrase 0.0053 0.0000 Lexical Similarity 0.0053 0.0000 Value Match 0.0017 0.0013 Acronym Match 0.0017 0.0013 Adjectival Form7 0.0000 0.0063 False Entailment Feature Dev Test Negation Mismatch 0.0106 0.0025 Argument Movement 0.0070 0.0250 Conditional Mismatch 0.0053 0.0037 Modal Mismatch 0.0035 0.0013 Superlative Mismatch 0.0035 -0.0025 Entity Mismatch 0.0018 0.0063 Table 3 : Feature ablation study ; quantity is the accuracy loss obtained by removal of single feature test set CWS of 0.6534 is higher than previouslyreported task-independent systems ( however , the task-dependent system reported in ( Raina et al. , 2005 ) achieves a CWS of 0.686 ) .</sentence>
				<definiendum id="0">quantity</definiendum>
				<definiens id="0">the accuracy loss obtained by removal of single feature test set CWS of 0.6534 is higher than previouslyreported task-independent systems</definiens>
			</definition>
</paper>

		<paper id="2022">
			<definition id="0">
				<sentence>One such type of information consists of cues to the speaker’s personality traits , typically assessed along five dimensions known as the Big Five ( Norman , 1963 ) : • Extraversion ( sociability , assertiveness ) • Emotional stability ( vs. neuroticism ) • Agreeableness to other people ( friendliness ) • Conscientiousness ( discipline ) • Intellect ( openness to experience ) Findings include that extraverts talk more , louder , and faster , with fewer pauses and hesitations , and more informal language ( Scherer , 1979 ; Furnham , 1990 ; Heylighen and Dewaele , 2002 ; Gill and Oberlander , 2002 ) .</sentence>
				<definiendum id="0">information</definiendum>
			</definition>
			<definition id="1">
				<sentence>The data consists of daily-life conversation extracts of 96 participants wearing an Electronically Activated Recorder ( EAR ) for two days , collected by Mehl et al. ( in press ) .</sentence>
				<definiendum id="0">data</definiendum>
				<definiens id="0">consists of daily-life conversation extracts of 96 participants wearing an Electronically Activated Recorder ( EAR ) for two days , collected by Mehl et al. ( in press )</definiens>
			</definition>
			<definition id="2">
				<sentence>As personality influences speech , we also use Praat LIWC FEATURES ( Pennebaker et al. , 2001 ) : · STANDARD COUNTS : Word count ( WC ) , words per sentence ( WPS ) , type/token ratio ( Unique ) , words captured ( Dic ) , words longer than 6 letters ( Sixltr ) , negations ( Negate ) , assents ( Assent ) , articles ( Article ) , prepositions ( Preps ) , numbers ( Number ) Pronouns ( Pronoun ) : 1 st person singular ( I ) , 1 st person plural ( We ) , total 1 st person ( Self ) , total 2 nd person ( You ) , total 3rd person ( Other ) · PSYCHOLOGICAL PROCESSES : Affective or emotional processes ( Affect ) : positive emotions ( Posemo ) , positive feelings ( Posfeel ) , optimism and energy ( Optim ) , negative emotions ( Negemo ) , anxiety or fear ( Anx ) , anger ( Anger ) , sadness ( Sad ) Cognitive Processes ( Cogmech ) : causation ( Cause ) , insight ( Insight ) , discrepancy ( Discrep ) , inhibition ( Inhib ) , tentative ( Tentat ) , certainty ( Certain ) Sensory and perceptual processes ( Senses ) : seeing ( See ) , hearing ( Hear ) , feeling ( Feel ) Social processes ( Social ) : communication ( Comm ) , other references to people ( Othref ) , friends ( Friends ) , family ( Family ) , humans ( Humans ) · RELATIVITY : Time ( Time ) , past tense verb ( Past ) , present tense verb ( Present ) , future tense verb ( Future ) Space ( Space ) : up ( Up ) , down ( Down ) , inclusive ( Incl ) , exclusive ( Excl ) Motion ( Motion ) · PERSONAL CONCERNS : Occupation ( Occup ) : school ( School ) , work and job ( Job ) , achievement ( Achieve ) Leisure activity ( Leisure ) : home ( Home ) , sports ( Sports ) , television and movies ( TV ) , music ( Music ) Money and financial issues ( Money ) Metaphysical issues ( Metaph ) : religion ( Relig ) , death ( Death ) , physical states and functions ( Physcal ) , body states and symptoms ( Body ) , sexuality ( Sexual ) , eating and drinking ( Eating ) , sleeping ( Sleep ) , grooming ( Groom ) · OTHER DIMENSIONS : Punctuation ( Allpct ) : period ( Period ) , comma ( Comma ) , colon ( Colon ) , semi-colon ( Semic ) , question ( Qmark ) , exclamation ( Exclam ) , dash ( Dash ) , quote ( Quote ) , apostrophe ( Apostro ) , parenthesis ( Parenth ) , other ( Otherp ) Swear words ( Swear ) , nonfluencies ( Nonfl ) , fillers ( Fillers ) MRC FEATURES ( Coltheart , 1981 ) : Number of letters ( Nlet ) , phonemes ( Nphon ) , syllables ( Nsyl ) , KuceraFrancis written frequency ( K-F-freq ) , Kucera-Francis number of categories ( K-F-ncats ) , Kucera-Francis number of samples ( K-F-nsamp ) , ThorndikeLorge written frequency ( T-L-freql ) , Brown verbal frequency ( Brownfreq ) , familiarity rating ( Fam ) , concreteness rating ( Conc ) , imageability rating ( Imag ) , meaningfulness Colorado Norms ( Meanc ) , meaningfulness Paivio Norms ( Meanp ) , age of acquisition ( AOA ) UTTERANCE TYPE FEATURES : Ratio of commands ( Command ) , prompts or back-channels ( Prompt ) , questions ( Question ) , assertions ( Assertion ) PROSODIC FEATURES : Average , minimum , maximum and standard deviation of the voice’s pitch in Hz ( Pitch-mean , Pitch-min , Pitch-max , Pitch-stddev ) and intensity in dB ( Int-mean , Int-min , Int-max , Int-stddev ) , voiced time ( Voiced ) and speech rate ( Word-per-sec ) Table 2 : Description of all features , with feature labels in brackets .</sentence>
				<definiendum id="0">Senses )</definiendum>
				<definiendum id="1">Social )</definiendum>
				<definiendum id="2">Comma</definiendum>
				<definiens id="0">Pennebaker et al. , 2001 ) : · STANDARD COUNTS : Word count ( WC ) , words per sentence ( WPS ) , type/token ratio ( Unique ) , words captured ( Dic ) , words longer than 6 letters ( Sixltr ) , negations ( Negate ) , assents ( Assent ) , articles ( Article ) , prepositions ( Preps ) , numbers</definiens>
				<definiens id="1">total 3rd person ( Other ) · PSYCHOLOGICAL PROCESSES : Affective or emotional processes ( Affect ) : positive emotions ( Posemo ) , positive feelings ( Posfeel ) , optimism and energy ( Optim ) , negative emotions ( Negemo ) , anxiety or fear ( Anx ) , anger ( Anger ) , sadness</definiens>
			</definition>
			<definition id="3">
				<sentence>RankBoost expresses the learned models as rules , which support the analysis of differences in the personality models ( see section 3 ) .</sentence>
				<definiendum id="0">RankBoost</definiendum>
			</definition>
			<definition id="4">
				<sentence>The RankBoost rules indicate the impact of each feature on the recognition of a personality trait by the magnitude of the parameter α associated with that feature .</sentence>
				<definiendum id="0">RankBoost rules</definiendum>
				<definiens id="0">indicate the impact of each feature on the recognition of a personality trait by the magnitude of the parameter α associated with that feature</definiens>
			</definition>
</paper>

		<paper id="2051">
			<definition id="0">
				<sentence>BAMA uses simple word-based heuristics to rank the splitting alternatives .</sentence>
				<definiendum id="0">BAMA</definiendum>
				<definiens id="0">uses simple word-based heuristics to rank the splitting alternatives</definiens>
			</definition>
</paper>

		<paper id="2036">
			<definition id="0">
				<sentence>WordNet Domains is an extension of WordNet ( http : //wordnet.princeton.edu/ ) where synonym sets have been annotated with one or more subject domain labels , as shown in Figure 1 .</sentence>
				<definiendum id="0">WordNet Domains</definiendum>
				<definiens id="0">an extension of WordNet ( http : //wordnet.princeton.edu/ ) where synonym sets have been annotated with one or more subject domain labels</definiens>
			</definition>
</paper>

		<paper id="2028">
			<definition id="0">
				<sentence>The confidence value of a term T , C ( T ) , is defined as C ( T ) = α∗TD ( T ) +β ∗TC ( T ) ( 1 ) where TD and TC denote the term domain-specificity and term cohesion , respectively .</sentence>
				<definiendum id="0">confidence value of a term T , C ( T )</definiendum>
			</definition>
			<definition id="1">
				<sentence>The domain specificity is further defined as TD = summationtext wi∈T Pd ( wi ) Pg ( wi ) | T | ( 2 ) where , | T | is the number of words in term T , pd ( wi ) is the probability of word wi in a domain document collection , and pg ( wi ) is the probability of word wi in a general document collection .</sentence>
				<definiendum id="0">domain specificity</definiendum>
				<definiens id="0">TD = summationtext wi∈T Pd ( wi ) Pg ( wi ) | T | ( 2 ) where , | T | is the number of words in term T , pd ( wi ) is the probability of word wi in a domain document collection , and pg ( wi ) is the probability of word wi in a general document collection</definiens>
			</definition>
			<definition id="2">
				<sentence>And the term cohesion is defined as TC = | T |×f ( T ) ×log10f ( T ) summationtext wi∈T f ( wi ) ( 3 ) where , f ( T ) is the frequency of term T , and f ( wi ) is the frequency of a component word wi .</sentence>
				<definiendum id="0">term cohesion</definiendum>
				<definiendum id="1">f</definiendum>
				<definiens id="0">TC = | T |×f ( T ) ×log10f ( T ) summationtext wi∈T f ( wi ) ( 3 ) where , f ( T ) is the frequency of term T , and</definiens>
				<definiens id="1">the frequency of a component word wi</definiens>
			</definition>
			<definition id="3">
				<sentence>Here , N1 is a threshold which will be automatically adjusted for each segment during the process .</sentence>
				<definiendum id="0">N1</definiendum>
				<definiens id="0">a threshold which will be automatically adjusted for each segment during the process</definiens>
			</definition>
			<definition id="4">
				<sentence>Specifically , we set N1 to min ( SS,3 ) where SS is the number of sentences that are overlapped with each segment .</sentence>
				<definiendum id="0">SS</definiendum>
				<definiens id="0">the number of sentences that are overlapped with each segment</definiens>
			</definition>
			<definition id="5">
				<sentence>Now , given each keyword ( K ) obtained from GlossEx , we recalculate its salience by considering the following three factors : 1 ) its original confidence value assigned by GlossEx ( CGlossEx ( K ) ) ; 2 ) the frequency of the keyword occurring in the aforementioned cue context ( Fcue ( K ) ) ; and 3 ) the number of component words in the keyword ( |K| ) .</sentence>
				<definiendum id="0">CGlossEx</definiendum>
				<definiens id="0">the frequency of the keyword occurring in the aforementioned cue context</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>In this work , we employ a syntax-based model that applies a series of tree/string ( xRS ) rules ( Galley et al. , 2004 ; Graehl and Knight , 2004 ) to a source language string to produce a target language phrase structure tree .</sentence>
				<definiendum id="0">syntax-based model</definiendum>
				<definiens id="0">applies a series of tree/string ( xRS ) rules ( Galley et al. , 2004 ; Graehl and Knight , 2004 ) to a source language string to produce a target language phrase structure tree</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>A synchronous CFG ( SCFG ) is a context-free rewriting system for generating string pairs .</sentence>
				<definiendum id="0">synchronous CFG ( SCFG )</definiendum>
			</definition>
			<definition id="1">
				<sentence>Each co-indexed child nonterminal pair will be further rewritten as a unit.2 We de ne the language L ( G ) produced by an SCFG G as the pairs of terminal strings produced by rewriting exhaustively from the start symbol .</sentence>
				<definiendum id="0">co-indexed child nonterminal pair</definiendum>
				<definiens id="0">a unit.2 We de ne the language L ( G ) produced by an SCFG G as the pairs of terminal strings produced by rewriting exhaustively from the start symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>So we now write rules in the following notation : X →X ( 1 ) 1 ... X ( n ) n , X ( pi ( 1 ) ) pi ( 1 ) ... X ( pi ( n ) ) pi ( n ) where each Xi is a variable which ranges over nonterminals in the grammar and pi is the permutation of the rule .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">pi</definiendum>
				<definiens id="0">pi ( 1 ) ) pi ( 1 ) ... X ( pi ( n ) ) pi ( n ) where each Xi is a variable which ranges over nonterminals in the grammar and</definiens>
			</definition>
			<definition id="3">
				<sentence>This set represents an important subclass of SCFG that is easy to handle ( parsable in O ( |w|6 ) ) and covers many interesting longer-than-two rules.3 3Although we factor the SCFG rules individually and dene bSCFG accordingly , there are some grammars ( the dashed SCFG bSCFG SCFG-2 O ( |w|6 ) parsable Figure 4 : Subclasses of SCFG .</sentence>
				<definiendum id="0">grammars</definiendum>
				<definiens id="0">Subclasses of SCFG</definiens>
			</definition>
			<definition id="4">
				<sentence>The goal of the ITG parsing is to nd a synchronous tree that agrees with the alignment indicated by the permutation .</sentence>
				<definiendum id="0">ITG parsing</definiendum>
				<definiens id="0">to nd a synchronous tree that agrees with the alignment indicated by the permutation</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>This produces O ( n2 ) trans250 fer rules in the worst case , where n is the number of f-structures in the source .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of f-structures in the source</definiens>
			</definition>
			<definition id="1">
				<sentence>Pharaoh integrates the following 8 statistical models : relative frequency of phrase translations in source-to-target and targetto-source direction , lexical weighting in source-totarget and target-to-source direction , phrase count , language model probability , word count , and distortion probability .</sentence>
				<definiendum id="0">Pharaoh</definiendum>
				<definiens id="0">relative frequency of phrase translations in source-to-target and targetto-source direction , lexical weighting in source-totarget and target-to-source direction , phrase count , language model probability , word count</definiens>
			</definition>
			<definition id="2">
				<sentence>For automatic evaluation , we use the NIST metric ( Doddington , 2002 ) combined with the approximate randomization test ( Noreen , 1989 ) , providing the desired combination of a sensitive evaluation metric and an accurate significance test ( see Riezler and 252 Table 1 : NIST scores on test set for IBM model 4 ( M4 ) , phrase-based SMT ( P ) , and the LFG-based SMT ( LFG ) on the full test set and on in-coverage examples for LFG .</sentence>
				<definiendum id="0">NIST metric</definiendum>
				<definiens id="0">NIST scores on test set for IBM model 4 ( M4 ) , phrase-based SMT ( P ) , and the LFG-based SMT ( LFG ) on the full test set and on in-coverage examples for LFG</definiens>
			</definition>
</paper>

		<paper id="4011">
			<definition id="0">
				<sentence>The test set consists of 50 callswith 113 speakers totalingabout 3 hoursof speech .</sentence>
				<definiendum id="0">test set</definiendum>
			</definition>
</paper>

		<paper id="2044">
			<definition id="0">
				<sentence>In this model , a rule base consists of a population of N state-action rules known as classifiers .</sentence>
				<definiendum id="0">rule base</definiendum>
			</definition>
			<definition id="1">
				<sentence>This evolutionary learning process searches the space of possible rule sets to find an optimal policy as defined by the reward function .</sentence>
				<definiendum id="0">evolutionary learning process</definiendum>
				<definiens id="0">searches the space of possible rule sets to find an optimal policy as defined by the reward function</definiens>
			</definition>
			<definition id="2">
				<sentence>From the logs of the test dialogues we extracted the state-action rules ( classifiers ) that were executed .</sentence>
				<definiendum id="0">state-action rules</definiendum>
				<definiens id="0">classifiers ) that were executed</definiens>
			</definition>
</paper>

		<paper id="2018">
			<definition id="0">
				<sentence>Named-entity recognition is one of the most elementary and core problems in biomedical text mining .</sentence>
				<definiendum id="0">Named-entity recognition</definiendum>
				<definiens id="0">one of the most elementary and core problems in biomedical text mining</definiens>
			</definition>
			<definition id="1">
				<sentence>The obstacle of supervised machine-learning methods is the lack of the annotated training data which is essential for achieving good performance .</sentence>
				<definiendum id="0">obstacle of supervised machine-learning methods</definiendum>
				<definiens id="0">the lack of the annotated training data which is essential for achieving good performance</definiens>
			</definition>
			<definition id="2">
				<sentence>The entropy H ( N ) is defined as the entropy of the distribution of the N-best state sequences : ∑ ∑∑ = = ∧ ∧ = ∧ ∧ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎣ ⎡ −= N i N i i i N i i i P P P P NH 1 1 2 1 ) ( ) ( log ) ( ) ( ) ( o|s o|s o|s o|s .</sentence>
				<definiendum id="0">entropy H ( N )</definiendum>
				<definiens id="0">the entropy of the distribution of the N-best state sequences</definiens>
			</definition>
			<definition id="3">
				<sentence>The following MMR ( Maximal Marginal Relevance ) ( Carbonell and Goldstein 1998 ) formula is used to calculate the active learning score : ) , ( Similaritymax ) 1 ( ) , ( yUncertaint ) ( jiTs i def i ss Mssscore Mj ∈ ∗ −−∗= λλ ( 3 ) where si is the sentence to be selected , Uncertainty is the entropy of si given current NER module M , and Similarity indicates the divergence degree between the si and the sentence sj in the training corpus TM of M. The combination rule could be interpreted as assigning a higher score to a sentence of which the NER module is uncertain and whose configuration differs from the sentences in the existing training corpus .</sentence>
				<definiendum id="0">si</definiendum>
				<definiendum id="1">Uncertainty</definiendum>
				<definiendum id="2">Similarity</definiendum>
				<definiens id="0">the entropy of si given current NER module M , and</definiens>
				<definiens id="1">the divergence degree between the si and the sentence sj in the</definiens>
				<definiens id="2">assigning a higher score to a sentence of which the NER module is uncertain and whose configuration differs from the sentences in the existing training corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>POSBIOTM-NER : a trainable biomedical namedentity recognition system .</sentence>
				<definiendum id="0">POSBIOTM-NER</definiendum>
				<definiens id="0">a trainable biomedical namedentity recognition system</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>We differ from Levy and Manning in using a perceptron–based approach for these , rather than a 10The non–PSLB system of Jijkoun and de Rijke uses function tags , and Levy and Manning mention that the lack of this information was sometimes an obstacle for them .</sentence>
				<definiendum id="0">Levy</definiendum>
				<definiens id="0">uses function tags , and</definiens>
			</definition>
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>Paraphrases are alternative verbalizations for conveying the same information and are required by many Natural Language Processing ( NLP ) applications .</sentence>
				<definiendum id="0">Paraphrases</definiendum>
			</definition>
			<definition id="1">
				<sentence>We can express the calculations in the following formulas : where f y ( x b ) denotes the optimal recall coverage ( number of words in the reference summary matched by the phrases from the peer summary ) at state x b in stage y. r ( x b ) is the recall coverage given state x b .</sentence>
				<definiendum id="0">f y</definiendum>
				<definiens id="0">the optimal recall coverage ( number of words in the reference summary matched by the phrases from the peer summary</definiens>
			</definition>
			<definition id="2">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>Named Entity Recognition ( NER ) is a fundamental task in text mining and natural language understanding .</sentence>
				<definiendum id="0">Named Entity Recognition</definiendum>
				<definiendum id="1">NER )</definiendum>
			</definition>
			<definition id="1">
				<sentence>Named Entity Recognition ( NER ) is the task of identifying and classifying phrases that denote certain types of named entities ( NEs ) , such as persons , organizations and locations in news articles , and genes , proteins and chemicals in biomedical literature .</sentence>
				<definiendum id="0">Named Entity Recognition ( NER )</definiendum>
				<definiens id="0">the task of identifying and classifying phrases that denote certain types of named entities ( NEs ) , such as persons , organizations and locations in news articles , and genes , proteins and chemicals in biomedical literature</definiens>
			</definition>
			<definition id="2">
				<sentence>NER is a fundamental task in many natural language processing applications , such as question answering , machine translation , text mining , and information retrieval ( Srihari and Li , 1999 ; Huang and Vogel , 2002 ) .</sentence>
				<definiendum id="0">NER</definiendum>
				<definiens id="0">a fundamental task in many natural language processing applications , such as question answering , machine translation , text mining , and information retrieval</definiens>
			</definition>
			<definition id="3">
				<sentence>Without loss of generality , let us use r T : F → { 1 , 2 , . . . , |F| } to denote a ranking function that maps a feature f ∈ F to a rank r T ( f ) based on a set of training examples T , where F is the set of all features , and the rank denotes the position of the feature in the final ranked list .</sentence>
				<definiendum id="0">F</definiendum>
				<definiendum id="1">rank</definiendum>
				<definiens id="0">maps a feature f ∈ F to a rank r T ( f ) based on a set of training examples T , where</definiens>
				<definiens id="1">the position of the feature in the final ranked list</definiens>
			</definition>
			<definition id="4">
				<sentence>Indeed , when the features in a maximum entropy model are defined as conjunctions of a feature on observations only and a Kronecker delta of a class label , which is a common practice in NER , the maximum entropy model is equivalent to a logistic regression model ( Finkel et al. , 2005 ) .</sentence>
				<definiendum id="0">Kronecker delta of a class label</definiendum>
				<definiendum id="1">maximum entropy model</definiendum>
			</definition>
			<definition id="5">
				<sentence>To avoid overfitting , a zero mean Gaussian prior on the weights is usually used ( Chen and Rosenfeld , 1999 ; Bender et al. , 2003 ) , and a maximum a posterior ( MAP ) estimator is used to maximize the posterior probability : ˆ β = arg max β p ( β ) N productdisplay j=1 p ( y j |x j , β ) , ( 4 ) where y j is the true class label for x j , N is the number of training examples , and p ( β ) = |F | productdisplay i=1 1 radicalBig 2piσ 2 i exp ( − β 2 i 2σ 2 i ) .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a zero mean Gaussian prior on the weights is usually used</definiens>
				<definiens id="1">the true class label for x j</definiens>
				<definiens id="2">the number of training examples</definiens>
			</definition>
			<definition id="6">
				<sentence>Named entity recognition is an important problem that can help many text mining and natural language processing tasks such as information extraction and question answering .</sentence>
				<definiendum id="0">Named entity recognition</definiendum>
				<definiens id="0">an important problem that can help many text mining and natural language processing tasks such as information extraction</definiens>
			</definition>
</paper>

		<paper id="4005">
			<definition id="0">
				<sentence>AquaLog is a portable question-answering system which takes queries expressed in natural language ( NL ) and an ontology as input , and returns answers drawn from one or more knowledge bases ( KB ) .</sentence>
				<definiendum id="0">AquaLog</definiendum>
				<definiens id="0">a portable question-answering system which takes queries expressed in natural language ( NL ) and an ontology as input , and returns answers drawn from one or more knowledge bases ( KB )</definiens>
			</definition>
			<definition id="1">
				<sentence>AquaLog ( Lopez , 2005 ) is a fully implemented ontology-driven Question Answering ( QA ) system 1 , which takes an ontology and a NL query as an input and returns answers drawn from semantic markup ( viewed as a KB ) compliant with the input ontology .</sentence>
				<definiendum id="0">AquaLog</definiendum>
				<definiens id="0">a fully implemented ontology-driven Question Answering ( QA ) system 1 , which takes an ontology and a NL query as an input and returns answers drawn from semantic markup ( viewed as a KB ) compliant with the input ontology</definiens>
			</definition>
			<definition id="2">
				<sentence>In contrast with much existing work on ontology-driven QA , which tends to focus on the use of ontologies to support query expansion in information retrieval ( Mc Guinness , 2004 ) , AquaLog exploits the availability of semantic statements to provide precise answers to complex queries expressed in NL .</sentence>
				<definiendum id="0">AquaLog</definiendum>
				<definiens id="0">the availability of semantic statements to provide precise answers to complex queries expressed in NL</definiens>
			</definition>
			<definition id="3">
				<sentence>AquaLog uses a sequential process model ( see Fig .</sentence>
				<definiendum id="0">AquaLog</definiendum>
			</definition>
			<definition id="4">
				<sentence>The Linguistic Component uses the GATE infrastructure and resources ( Cunningham , 2002 ) to obtain a set of syntactic annotations associated with the input query and to classify the query .</sentence>
				<definiendum id="0">Linguistic Component</definiendum>
				<definiens id="0">uses the GATE infrastructure and resources</definiens>
			</definition>
			<definition id="5">
				<sentence>Currently , the linguistic component , through the Jape grammars , dynamically identifies around 14 different linguistic categories or intermediate representations , including : basic queries requiring an affirmation/negation or a description as an answer ; or the big set of queries constituted by a whquestion ( such as the ones starting with : what , who , when , where , are there any , does anybody/anyone or how many , and imperative commands like list , give , tell , name , etc. ) , like “are there any PhD students in dotkom ? ”</sentence>
				<definiendum id="0">Jape grammars</definiendum>
				<definiens id="0">basic queries requiring an affirmation/negation or a description as an answer ; or the big set of queries constituted by a whquestion ( such as the ones starting with : what , who</definiens>
			</definition>
			<definition id="6">
				<sentence>For demonstration purposes AquaLog application is used with the AKT ontology in the context of the academic domain in our department ( Lei , 2006 ) , e.g. , AquaLog translates the query “what is the homepage of Peter who has an interest on the semantic web ? ''</sentence>
				<definiendum id="0">AquaLog</definiendum>
			</definition>
</paper>

		<paper id="4007">
			<definition id="0">
				<sentence>There is a gradual curve in this graph and there is no obvious knee point ( i.e. , sharp increase ) that indicates the appropriate value of k. 0 2 4 6 8 10 12 14 16 H2 vs k a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114a114 -1.5000 -1.0000 -0.5000 0 2 4 6 8 10 12 14 16 PK1 vs k a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114a114 2 4 6 8 10 12 14 16 PK2 vs ka114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 2 4 6 8 10 12 14 PK3 vs k a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 Figure 1 : H2 ( top ) and PK1 , PK2 , and PK3 for the name conflate pair Sonia Gandhi and Leonid Kuchma .</sentence>
				<definiendum id="0">PK3</definiendum>
				<definiens id="0">vs k a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 Figure 1 : H2 ( top ) and PK1 , PK2 , and PK3 for the name conflate pair Sonia Gandhi and Leonid Kuchma</definiens>
			</definition>
</paper>

		<paper id="4006">
			<definition id="0">
				<sentence>Knowtator facilitates the manual creation of annotated corpora that can be used for evaluating or training a variety of natural language processing systems .</sentence>
				<definiendum id="0">Knowtator</definiendum>
				<definiens id="0">facilitates the manual creation of annotated corpora that can be used for evaluating or training a variety of natural language processing systems</definiens>
			</definition>
			<definition id="1">
				<sentence>Building on the strengths of the widely used Protégé knowledge representation system , Knowtator has been developed as a Protégé plug-in that leverages Protégé’s knowledge representation capabilities to specify annotation schemas .</sentence>
				<definiendum id="0">Knowtator</definiendum>
				<definiens id="0">a Protégé plug-in that leverages Protégé’s knowledge representation capabilities to specify annotation schemas</definiens>
			</definition>
			<definition id="2">
				<sentence>Knowtator is a general-purpose text annotation tool for creating annotated corpora suitable for evaluating Natural Language Processing ( NLP ) systems .</sentence>
				<definiendum id="0">Knowtator</definiendum>
			</definition>
			<definition id="3">
				<sentence>An annotation schema is a specification of the kinds of annotations that can be created .</sentence>
				<definiendum id="0">annotation schema</definiendum>
				<definiens id="0">a specification of the kinds of annotations that can be created</definiens>
			</definition>
			<definition id="4">
				<sentence>Protégé is a widely used knowledge representation system that facilitates construction and visualization of knowledge-bases ( Noy , 2003 ) 1 .</sentence>
				<definiendum id="0">Protégé</definiendum>
				<definiens id="0">a widely used knowledge representation system that facilitates construction and visualization of knowledge-bases</definiens>
			</definition>
			<definition id="5">
				<sentence>Knowtator has been implemented as a Protégé plug-in and runs in the Protégé environment .</sentence>
				<definiendum id="0">Knowtator</definiendum>
				<definiens id="0">a Protégé plug-in and runs in the Protégé environment</definiens>
			</definition>
			<definition id="6">
				<sentence>In addition to configuration files , WordFreak provides a plug-in architecture for creating task specific code modules that can be integrated into the user interface .</sentence>
				<definiendum id="0">WordFreak</definiendum>
				<definiens id="0">provides a plug-in architecture for creating task specific code modules that can be integrated into the user interface</definiens>
			</definition>
			<definition id="7">
				<sentence>GATE is a software architecture for NLP that has , as one of its many components , text annotation functionality .</sentence>
				<definiendum id="0">GATE</definiendum>
				<definiens id="0">a software architecture for NLP that has , as one of its many components , text annotation functionality</definiens>
			</definition>
			<definition id="8">
				<sentence>Knowtator approaches the definition of an annotation schema as a knowledge engineering task by leveraging Protégé’s strengths as a knowledgebase editor .</sentence>
				<definiendum id="0">Knowtator</definiendum>
				<definiens id="0">approaches the definition of an annotation schema as a knowledge engineering task by leveraging Protégé’s strengths as a knowledgebase editor</definiens>
			</definition>
			<definition id="9">
				<sentence>Knowtator provides a pluggable infrastructure for handling different kinds of text source types .</sentence>
				<definiendum id="0">Knowtator</definiendum>
				<definiens id="0">provides a pluggable infrastructure for handling different kinds of text source types</definiens>
			</definition>
			<definition id="10">
				<sentence>Knowtator provides stand-off annotation such that the original text that is being annotated is not modified .</sentence>
				<definiendum id="0">Knowtator</definiendum>
			</definition>
			<definition id="11">
				<sentence>Annotation data can be exported to a simple XML format .</sentence>
				<definiendum id="0">Annotation data</definiendum>
			</definition>
</paper>

		<paper id="2050">
			<definition id="0">
				<sentence>Logistic regression ( LR ) is indeed a softmax linear regression , which models the posterior probabilities of the class label with the softmax of linear functions of feature vectors .</sentence>
				<definiendum id="0">Logistic regression ( LR</definiendum>
				<definiens id="0">models the posterior probabilities of the class label with the softmax of linear functions of feature vectors</definiens>
			</definition>
			<definition id="1">
				<sentence>According to the ROC curves presented in Christensen et al. ( 2004 ) , the structural feature ( utterance position ) is one of the best features for summarizing read news stories , and is less effective when news stories contain spontaneous speech .</sentence>
				<definiendum id="0">structural feature ( utterance position )</definiendum>
				<definiens id="0">one of the best features for summarizing read news stories , and is less effective when news stories contain spontaneous speech</definiens>
			</definition>
			<definition id="2">
				<sentence>Language and Speech , 44 ( 2 ) : 123-147 Christensen , H. , Kolluru , B. , Gotoh , Y. , Renals , S. , 2004 .</sentence>
				<definiendum id="0">Language</definiendum>
			</definition>
</paper>

		<paper id="5004">
			<definition id="0">
				<sentence>Ciprian Chelba is a Research Scientist with Google .</sentence>
				<definiendum id="0">Ciprian Chelba</definiendum>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>A context-free grammar ( CFG ) is a tuple G = ( N , Σ , R , S ) , where N is a finite set of nonterminal symbols , Σ is a finite set of terminal symbols disjoint from N , S ∈ N is the start symbol and R is a finite set of rules .</sentence>
				<definiendum id="0">context-free grammar ( CFG )</definiendum>
				<definiendum id="1">Σ</definiendum>
				<definiendum id="2">R</definiendum>
				<definiens id="0">a tuple G = ( N , Σ , R , S ) , where N is a finite set of nonterminal symbols</definiens>
				<definiens id="1">a finite set of terminal symbols disjoint from N , S ∈ N is the start symbol</definiens>
				<definiens id="2">a finite set of rules</definiens>
			</definition>
			<definition id="1">
				<sentence>A probabilistic context-free grammar ( PCFG ) is a pair G = ( G , pG ) , with G a CFG and pG a function from R to the real numbers in the interval [ 0,1 ] .</sentence>
				<definiendum id="0">probabilistic context-free grammar ( PCFG</definiendum>
			</definition>
			<definition id="2">
				<sentence>( 2 ) A PCFG is consistent ifsummationtextt∈T ( G ) pG ( t ) = 1 .</sentence>
				<definiendum id="0">PCFG</definiendum>
			</definition>
			<definition id="3">
				<sentence>The skeleton CFG underlying T is defined as G = ( N , Σ , R , S ) .</sentence>
				<definiendum id="0">skeleton CFG underlying T</definiendum>
			</definition>
			<definition id="4">
				<sentence>The cross-entropy between pT and pG is defined as the expectation under distributionpT of the information , computed under distribution pG , of the trees in T ( G ) H ( pT ||pG ) = EpT log 1p G ( t ) = −summationdisplay t∈T pT ( t ) ·logpG ( t ) .</sentence>
				<definiendum id="0">pG</definiendum>
				<definiens id="0">the expectation under distributionpT of the information , computed under distribution pG , of the trees in T ( G ) H ( pT ||pG ) = EpT log 1p G ( t ) = −summationdisplay t∈T pT ( t ) ·logpG ( t )</definiens>
			</definition>
			<definition id="5">
				<sentence>( 5 ) Since G should be proper , the minimization of ( 5 ) is subject to the constraintssummationtextα pG ( A → α ) = 1 , for each A ∈ N. To solve the minimization problem above , we use Lagrange multipliers λA for each A ∈ N and define the form ∇ = summationdisplay A∈N λA · ( summationdisplay α pG ( A → α ) −1 ) + −summationdisplay t∈T pT ( t ) ·logpG ( t ) .</sentence>
				<definiendum id="0">pG</definiendum>
				<definiendum id="1">pG</definiendum>
			</definition>
			<definition id="6">
				<sentence>From each equation ∂∇∂pG ( A→α ) = 0 we obtain λA ·ln ( 2 ) ·pG ( A → α ) = = EpT f ( A → α , t ) .</sentence>
				<definiendum id="0">·pG</definiendum>
				<definiens id="0">A → α ) = = EpT f ( A → α , t )</definiens>
			</definition>
			<definition id="7">
				<sentence>( 15 ) 338 We can now prove the equality Hd ( pG ) = H ( pT ||pG ) , ( 16 ) where G is the PCFG estimated by minimizing the cross-entropy in ( 5 ) , as described in Section 3 .</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">the PCFG estimated by minimizing the cross-entropy in ( 5 )</definiens>
			</definition>
			<definition id="8">
				<sentence>( 17 ) From our estimator in ( 10 ) we can write EpT f ( A → α , t ) = = pG ( A → α ) ·EpT f ( A , t ) .</sentence>
				<definiendum id="0">·EpT f</definiendum>
				<definiens id="0">A → α , t ) = = pG ( A → α )</definiens>
			</definition>
			<definition id="9">
				<sentence>( 22 ) For each A ∈ N with A negationslash= S we can then write EpT f ( A , t ) = = summationdisplay t∈T pT ( t ) ·f ( A , t ) = summationdisplay t∈T pT ( t ) · summationdisplay B→β f ( B → β , t ) ·f ( A , β ) = summationdisplay B→β summationdisplay t∈T pT ( t ) ·f ( B → β , t ) ·f ( A , β ) = summationdisplay B→β EpT f ( B → β , t ) ·f ( A , β ) .</sentence>
				<definiendum id="0">summationdisplay t∈T pT</definiendum>
			</definition>
			<definition id="10">
				<sentence>Based on the results in this section , we can instead compute the exact value of H ( pT ||pG ) by computing the derivational entropy Hd ( pG ) , using relation ( 13 ) and solving the linear system in ( 14 ) and ( 15 ) , which takes cubic time in the number of nonterminals of the grammar .</sentence>
				<definiendum id="0">derivational entropy Hd</definiendum>
				<definiens id="0">takes cubic time in the number of nonterminals of the grammar</definiens>
			</definition>
			<definition id="11">
				<sentence>We define f ( A → α , T ) = = summationdisplay t∈T f ( t , T ) ·f ( A → α , t ) , ( 25 ) and let f ( A , T ) = summationtextα f ( A → α , T ) .</sentence>
				<definiendum id="0">f (</definiendum>
			</definition>
			<definition id="12">
				<sentence>In the MLE method we probabilistically extend the skeleton CFG G by means of a function pG that maximizes the likelihood of T , defined as pG ( T ) = productdisplay t∈T pG ( t ) f ( t , T ) , ( 27 ) subject to the usual properness conditions on pG .</sentence>
				<definiendum id="0">T )</definiendum>
				<definiens id="0">the skeleton CFG G by means of a function pG that maximizes the likelihood of T , defined as pG ( T ) = productdisplay t∈T pG ( t ) f ( t ,</definiens>
			</definition>
			<definition id="13">
				<sentence>Ifwereplace distribution pT with our empirical distribution pT , we derive pG ( A → α ) = = EpT f ( A → α , t ) E pT f ( A , t ) = summationtext t∈T f ( t , T ) |T| ·f ( A → α , t ) summationtext t∈T f ( t , T ) |T| ·f ( A , t ) = summationtext t∈T f ( t , T ) ·f ( A → α , t ) summationtext t∈T f ( t , T ) ·f ( A , t ) = f ( A → α , T ) f ( A , T ) .</sentence>
				<definiendum id="0">T ) ·f</definiendum>
				<definiens id="0">A → α , t ) E pT f ( A , t ) = summationtext t∈T f ( t , T ) |T| ·f ( A → α , t ) summationtext t∈T f ( t , T ) |T| ·f ( A , t ) = summationtext t∈T f ( t , T ) ·f ( A → α , t ) summationtext t∈T f ( t ,</definiens>
			</definition>
			<definition id="14">
				<sentence>In fact , cross-entropy indicates how much the estimated model fits the observed data , and is commonly exploited in comparison of different models on the same data set .</sentence>
				<definiendum id="0">cross-entropy</definiendum>
				<definiens id="0">indicates how much the estimated model fits the observed data</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>Dialog act ( DA ) tags are useful for many applications in natural language processing and automatic speech recognition .</sentence>
				<definiendum id="0">Dialog act</definiendum>
				<definiendum id="1">DA</definiendum>
				<definiens id="0">useful for many applications in natural language processing and automatic speech recognition</definiens>
			</definition>
			<definition id="1">
				<sentence>The joint probability under this baseline model is written as follows : P ( W , D ) = productdisplay k P ( dkjdk−1 ) productdisplay i P ( wk , ijwk , i−1 , dk ) , ( 1 ) where W = fwk , ig is the word sequence , D = fdkg is the DA sequence , dk is the DA of the k-th sentence , and wk , i is the i-th word of the k-th sentence in the meeting .</sentence>
				<definiendum id="0">ig</definiendum>
				<definiens id="0">the word sequence</definiens>
				<definiens id="1">the i-th word of the k-th sentence in the meeting</definiens>
			</definition>
			<definition id="2">
				<sentence>MRDA is a rich data set that contains 75 natural meetings on different topics with each meeting involving about 6 participants .</sentence>
				<definiendum id="0">MRDA</definiendum>
			</definition>
			<definition id="3">
				<sentence>Each DA contains a main tag , several optional special tags and an optional disruption form .</sentence>
				<definiendum id="0">DA</definiendum>
				<definiens id="0">contains a main tag , several optional special tags and an optional disruption form</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>The 368 matrix of features is decomposed as follows : A = USV T where U is an m×n matrix of left-singular vectors , S is an n × n diagonal matrix of singular values , and V is the n×n matrix of right-singular vectors .</sentence>
				<definiendum id="0">U</definiendum>
				<definiendum id="1">S</definiendum>
				<definiendum id="2">V</definiendum>
				<definiens id="0">A = USV T where</definiens>
				<definiens id="1">an m×n matrix of left-singular vectors ,</definiens>
				<definiens id="2">an n × n diagonal matrix of singular values , and</definiens>
				<definiens id="3">the n×n matrix of right-singular vectors</definiens>
			</definition>
			<definition id="1">
				<sentence>Using sub-matrices S and V T , the LSA sentence scores are obtained using : ScLSAi = radicaltpradicalvertex radicalvertexradicalbt nsummationdisplay k=1 v ( i , k ) 2 ∗ σ ( k ) 2 , where v ( i , k ) is the kth element of the ith sentence vector and σ ( k ) is the corresponding singular value .</sentence>
				<definiendum id="0">LSA sentence scores</definiendum>
				<definiendum id="1">k )</definiendum>
				<definiens id="0">the kth element of the ith sentence vector</definiens>
			</definition>
			<definition id="2">
				<sentence>The centroid is a pseudo-document representing the important aspects of the document as a whole ; in the work of [ 3 ] , this pseudodocument consists of keywords and their modified tf .</sentence>
				<definiendum id="0">centroid</definiendum>
				<definiens id="0">a pseudo-document representing the important aspects of the document as a whole</definiens>
				<definiens id="1">consists of keywords and their modified tf</definiens>
			</definition>
			<definition id="3">
				<sentence>Our test set consists of six meetings , each with multiple human annotations .</sentence>
				<definiendum id="0">test set</definiendum>
			</definition>
			<definition id="4">
				<sentence>ROUGE-SU4 , which evaluates bigrams with intervening material between the two elements of the bigram , has recently been shown in the context of the Document Understanding Conference ( DUC ) 2 to bring no significant additional information as compared with ROUGE-2 .</sentence>
				<definiendum id="0">ROUGE-SU4</definiendum>
				<definiens id="0">evaluates bigrams with intervening material between the two elements of the bigram</definiens>
			</definition>
			<definition id="5">
				<sentence>The Pearson correlation is man correlation is 0.282 with a significance of p &lt; meeting is worse yet , with a Pearson correlation of 0.185 ( p &lt; 0.208 ) and a Spearman correlation of The following is the text of a summary of meeting Bed004 using the speech features approach : -so its possible that we could do something like a summary node of some sort that -and then the question would be if if those are the things that you care about uh can you make a relatively compact way of getting from the various inputs to the things you care about -this is sort of th the second version and i i i look at this maybe just as a you know a a whatever uml diagram or you know as just a uh screen shot not really as a bayes net as john johno said -and um this is about as much as we can do if we dont w if we want to avoid uh uh a huge combinatorial explosion where we specify ok if its this and this but that is not the case and so forth it just gets really really messy -also it strikes me that we we m may want to approach the point where we can sort of try to find a uh a specification for some interface here that um takes the normal m three l looks at it -so what youre trying to get out of this deep co cognitive linguistics is the fact that w if you know about source source paths and goals and nnn all this sort of stuff that a lot of this is the same for different tasks -what youd really like of course is the same thing youd always like which is that you have um a kind of intermediate representation which looks the same o over a bunch of inputs and a bunch of outputs -and pushing it one step further when you get to construction grammar and stuff what youd like to be able to do is say you have this parser which is much fancier than the parser that comes with uh smartkom -in independent of whether it about what is this or where is it or something that you could tell from the construction you could pull out deep semantic information which youre gon na use in a general way Though the speech features approach was considered the best system , it is unclear why the combined approach did not yield improvement .</sentence>
				<definiendum id="0">Pearson correlation</definiendum>
				<definiens id="0">looks the same o over a bunch of inputs</definiens>
			</definition>
</paper>

		<paper id="2049">
			<definition id="0">
				<sentence>Its calculation is defined as : CM ( tiob|w ) = αCMiob ( tiob|w ) + ( 1 − α ) δ ( tw , tiob ) ng ( 2 ) where tiob is the word w’s IOB tag assigned by the IOB tagging ; tw , a prior IOB tag determined by the results of the dictionary-based segmentation .</sentence>
				<definiendum id="0">tiob</definiendum>
				<definiens id="0">the word w’s IOB tag assigned by the IOB tagging ; tw , a prior IOB tag determined by the results of the dictionary-based segmentation</definiens>
			</definition>
			<definition id="1">
				<sentence>CMiob ( t|w ) , a confidence probability derived in the process of IOB tagging , is defined as CMiob ( t|wi ) = summationtext T=t0t1···tM , ti=t P ( T|W , wi ) summationtext T=t0t1···tM P ( T|W ) where the numerator is a sum of all the observation sequences with word wi labeled as t. 194 δ ( tw , tiob ) ng denotes the contribution of the dictionarybased segmentation .</sentence>
				<definiendum id="0">CMiob ( t|w )</definiendum>
				<definiendum id="1">numerator</definiendum>
				<definiendum id="2">tiob ) ng</definiendum>
				<definiens id="0">a confidence probability derived in the process of IOB tagging , is defined as CMiob ( t|wi ) = summationtext T=t0t1···tM , ti=t P ( T|W , wi ) summationtext T=t0t1···tM P ( T|W ) where the</definiens>
				<definiens id="1">a sum of all the observation sequences with word wi labeled as t. 194 δ ( tw ,</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>Usually , the ranking rule incorporates the model trained on the currently labeled data .</sentence>
				<definiendum id="0">ranking rule</definiendum>
			</definition>
			<definition id="1">
				<sentence>Uncertainty sampling is a term invented by Lewis and Gale ( Lewis and Gale , 1994 ) to describe a heuristic where a probabilistic classifier picks examples for which the model’s current predictions are least certain .</sentence>
				<definiendum id="0">Uncertainty sampling</definiendum>
				<definiens id="0">a probabilistic classifier picks examples for which the model’s current predictions are least certain</definiens>
			</definition>
			<definition id="2">
				<sentence>The features used for disambiguating the verb senses included topical , collocation , syntactic ( e.g. , the subject , object , and preposition phrases taken by a target verb ) , and semantic ( e.g. , the WordNet synsets and hypernyms of the head nouns of a verb’s NP arguments ) features ( Chen and Palmer , 2005 ) .</sentence>
				<definiendum id="0">semantic</definiendum>
				<definiens id="0">disambiguating the verb senses included topical , collocation , syntactic ( e.g. , the subject , object , and preposition phrases taken by a target verb</definiens>
			</definition>
</paper>

		<paper id="4008">
			<definition id="0">
				<sentence>Ndaona includes embedding and graphics parameter estimation algorithms , and generates files in the format of Partiview ( Levy , 2001 ) , an existing free open-source fast multidimensional data displayer that has traditionally been used in the planetarium community .</sentence>
				<definiendum id="0">Ndaona</definiendum>
				<definiens id="0">includes embedding and graphics parameter estimation algorithms , and generates files in the format of Partiview ( Levy , 2001 ) , an existing free open-source fast multidimensional data displayer that has traditionally been used in the planetarium community</definiens>
			</definition>
			<definition id="1">
				<sentence>Ndaona uses the Parametric Embedding algorithm ( Iwata et al. , 2004 ) to find a low-dimensional embedding of the N points so that pairs of points that were given similar predictions by the classification algorithm ( i.e. have low Kullback-Leibler distance between their prediction probability distributions ) are closer together .</sentence>
				<definiendum id="0">Ndaona</definiendum>
				<definiens id="0">uses the Parametric Embedding algorithm ( Iwata et al. , 2004 ) to find a low-dimensional embedding of the N points so that pairs of points that were given similar predictions by the classification algorithm</definiens>
			</definition>
			<definition id="2">
				<sentence>Ndaona is an interface package that helps reearchers produce compelling visual representations of their data .</sentence>
				<definiendum id="0">Ndaona</definiendum>
			</definition>
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>The Chinese Nombank extends the general annotation framework of the English Proposition Bank ( Palmer et al. , 2005 ) and the English Nombank ( Meyers et al. , 2004 ) to the annotation of nominalized predicates in Chinese .</sentence>
				<definiendum id="0">Chinese Nombank</definiendum>
				<definiens id="0">extends the general annotation framework of the English Proposition Bank ( Palmer et al. , 2005 ) and the English Nombank ( Meyers et al. , 2004 ) to the annotation of nominalized predicates in Chinese</definiens>
			</definition>
			<definition id="1">
				<sentence>Like the English Nombank project , the Chinese Nombank adds a layer of semantic annotation to the Chinese TreeBank ( CTB ) , a syntactically annotated corpus of 500 thousand words .</sentence>
				<definiendum id="0">Chinese Nombank</definiendum>
				<definiendum id="1">TreeBank</definiendum>
				<definiens id="0">a syntactically annotated corpus of 500 thousand words</definiens>
			</definition>
			<definition id="2">
				<sentence>Nominalized predicates also tend to take fewer types of adjuncts ( ARGMs ) than their verbal counterpart and they also tend to be less polysemous , having only a subset of the senses of their verb counterpart .</sentence>
				<definiendum id="0">ARGMs</definiendum>
				<definiens id="0">than their verbal counterpart and they also tend to be less polysemous , having only a subset of the senses of their verb counterpart</definiens>
			</definition>
			<definition id="3">
				<sentence>This version of the Chinese Nombank consists of standoff annotation on the first 760 articles ( chtb_001 .</sentence>
				<definiendum id="0">Chinese Nombank</definiendum>
			</definition>
			<definition id="4">
				<sentence>The NomBank Project : An Interim Report .</sentence>
				<definiendum id="0">NomBank Project</definiendum>
				<definiens id="0">An Interim Report</definiens>
			</definition>
			<definition id="5">
				<sentence>The Proposition Bank : An Annotated Corpus of Semantic Roles .</sentence>
				<definiendum id="0">Proposition Bank</definiendum>
			</definition>
</paper>

		<paper id="2027">
			<definition id="0">
				<sentence>Natural language generation ( NLG ) refers to the process of producing text in a spoken language , starting from an internal knowledge representation structure .</sentence>
				<definiendum id="0">Natural language generation</definiendum>
				<definiens id="0">the process of producing text in a spoken language , starting from an internal knowledge representation structure</definiens>
			</definition>
			<definition id="1">
				<sentence>Bliss is a graphic meaning-referenced language , created by Charles Bliss to be used as a written universal language ( Bliss , 1965 ) ; since 1971 , Blissymbols are used for communication with severely languageimpaired children .</sentence>
				<definiendum id="0">Bliss</definiendum>
				<definiens id="0">a graphic meaning-referenced language , created by Charles Bliss to be used as a written universal language</definiens>
			</definition>
</paper>

		<paper id="2019">
			<definition id="0">
				<sentence>Edit word detection varies between parser and oracle , and filler word detection varies between none , system ( Johnson et al. , 2004 ) , and oracle .</sentence>
				<definiendum id="0">Edit word detection</definiendum>
				<definiens id="0">varies between parser and oracle , and filler word detection varies between none , system ( Johnson et al. , 2004 ) , and oracle</definiens>
			</definition>
</paper>

	</volume>
