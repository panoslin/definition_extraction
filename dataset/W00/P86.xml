<?xml version="1.0" encoding="UTF-8"?>
	<volume id="P86">

		<paper id="1024">
			<definition id="0">
				<sentence>Japanese sentences consist of thousands of Kanji characters , more than one hundred different kana characters ( two kana character sets ~ Hiragana and Katakana are used in Japanese sentences ) and alphanumeric characters .</sentence>
				<definiendum id="0">Japanese sentences</definiendum>
				<definiens id="0">consist of thousands of Kanji characters , more than one hundred different kana characters ( two kana character sets ~ Hiragana and Katakana are used in Japanese sentences</definiens>
			</definition>
			<definition id="1">
				<sentence>However , the fifth character proves an exception with the second ranking character candidate as the desired character .</sentence>
				<definiendum id="0">fifth character</definiendum>
				<definiens id="0">proves an exception with the second ranking character candidate as the desired character</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>Kimmo is an implementation of the `` two-level '' model of morphology that Kimmo Koskenniemi proposed and developed in his Ph.D. thesis .</sentence>
				<definiendum id="0">Kimmo</definiendum>
				<definiens id="0">an implementation of the `` two-level '' model of morphology that Kimmo Koskenniemi proposed and developed in his Ph.D. thesis</definiens>
			</definition>
			<definition id="1">
				<sentence>Finally , use the following satisfaction automaton , which does not vary from formula to formula : `` satisfaction '' 3 4 = = , ( lexical characters } T F , ( surface characters } 2 : 2 2 2 1 ( true seen in this group } The satisfaction automaton determines whether the truthvalues assigned to the variables cause the formula to come out true .</sentence>
				<definiendum id="0">satisfaction automaton</definiendum>
				<definiens id="0">determines whether the truthvalues assigned to the variables cause the formula to come out true</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>Selection is a directed action that , for Warlpiri , may take the category preceding it as its object .</sentence>
				<definiendum id="0">Selection</definiendum>
			</definition>
			<definition id="1">
				<sentence>This follows from the setting of the head parameter of GB : Warlpiri is a head-final language• Selection involves a co-projection of the selector and its object , where both categories are projected one level• For example , the tensed element , rni , selects verbs , and then co-projects to form the combined `` inflected verb '' category• An example is presented below• The other three events occur under the undirected structural relation of siblinghood .</sentence>
				<definiendum id="0">Warlpiri</definiendum>
				<definiens id="0">a head-final language• Selection involves a co-projection of the selector and its object</definiens>
			</definition>
			<definition id="2">
				<sentence>THE PARSING ENGINE The parsing engine is the core of both the lexical and the syntactic parsers .</sentence>
				<definiendum id="0">parsing engine</definiendum>
				<definiens id="0">the core of both the lexical and the syntactic parsers</definiens>
			</definition>
</paper>

		<paper id="1027">
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>Cb is the entity an utterance most centrally concerns .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiens id="0">the entity an utterance most centrally concerns</definiens>
			</definition>
			<definition id="1">
				<sentence>Rather , its role in the centering framework is to give an ordered fist of referents ( centers ) so that commonsense inferences can be controlled .</sentence>
				<definiendum id="0">centering framework</definiendum>
				<definiens id="0">to give an ordered fist of referents ( centers ) so that commonsense inferences can be controlled</definiens>
			</definition>
			<definition id="2">
				<sentence>9The following symbols are used for grammatical markers in the gloss : SB ( subject ) , OB ( direct object ) , 02 ( indirect/second object ) , TP ( topic ) , ASN ( assertion ) , CMP ( complementizer ) , Q ( question ) .</sentence>
				<definiendum id="0">SB</definiendum>
				<definiendum id="1">TP ( topic</definiendum>
				<definiens id="0">direct object ) , 02 ( indirect/second object )</definiens>
			</definition>
			<definition id="3">
				<sentence>Examples like these are the basis for the first version of the Centering Constraint : ( 6 ) Centering Constraint \ [ Japanese\ ] ( 1st approximation ) Two zero pronominals that retain the same Cb in adjacent utterances should share one of the following properties : Io= : indicates the association between a linguistic item ( leR-hand side ) and a non-linguistic entity ( right-hand side ) .</sentence>
				<definiendum id="0">non-linguistic entity</definiendum>
				<definiendum id="1">right-hand side</definiendum>
				<definiens id="0">the first version of the Centering Constraint : ( 6 ) Centering Constraint \ [ Japanese\ ] ( 1st approximation ) Two zero pronominals that retain the same Cb in adjacent utterances should share one of the following properties : Io= : indicates the association between a linguistic item ( leR-hand side</definiens>
			</definition>
			<definition id="4">
				<sentence>This has to do with what Kuno calls empathy , a grammatical feature especially prominent in Japanese , defined as follows : ( 22 ) Empathy ( Kuno &amp; Kaburagi 1977:628 ) Empathy is the speaker 's identification , with varying degrees , with a person who participates in the event that he describes in a sentence .</sentence>
				<definiendum id="0">Empathy</definiendum>
				<definiens id="0">the speaker 's identification , with varying degrees , with a person who participates in the event that he describes in a sentence</definiens>
			</definition>
			<definition id="5">
				<sentence>17 When the main predicate of an utterance selects one of its arguments for the identifu : ation locus ( henceforth Ident ) , the speaker automatically identifies ( with varying degrees ) with the viewpoint of its referent ( usually human ) .</sentence>
				<definiendum id="0">ation locus</definiendum>
				<definiens id="0">with varying degrees ) with the viewpoint of its referent ( usually human )</definiens>
			</definition>
			<definition id="6">
				<sentence>lSOBJECT2 is the indirect or second object .</sentence>
				<definiendum id="0">lSOBJECT2</definiendum>
				<definiens id="0">the indirect or second object</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>~ = ( Q , L , 5 , qo , F ) where L is the set of labels above , 6 is a partial function from Q × L to Q , and where certain elements of F may be atoms from the set A. We require that ~ be connected , acyclic , and have no transitions from any final states .</sentence>
				<definiendum id="0">L</definiendum>
				<definiens id="0">the set of labels above , 6 is a partial function from Q × L to Q , and where certain elements of F may be atoms from the set A. We require that ~ be connected</definiens>
			</definition>
			<definition id="1">
				<sentence>We can now state inductively the exact conditions under which an automaton Jl satisfies a formula : with no transitions ; class of N ( ~ ) ; and A/I ~ ~ ; where ~/I is defined by a subgraph of the automaton A with start state 5 ( qo , l ) , that is ira = ( Q , L , 6 , qo , F ) , then .</sentence>
				<definiendum id="0">~/I</definiendum>
				<definiens id="0">the exact conditions under which an automaton Jl satisfies a formula : with no transitions</definiens>
			</definition>
			<definition id="2">
				<sentence>260 Failure : l : TOP = TOP Conjunction ( unification } : ¢ A TOP = TOP CANIL = ~b aAb = TOP , Va , b6Aanda # b aAl : ¢ = TOP / : ¢AZ : , # , = t : ( ¢A¢ ) Disjunction : ¢ v NIL = NIL ¢vTOP = z : ¢v~ : ¢ = t : ( ¢v¢ ) Commutative : ¢A¢ = ¢^¢ ¢v¢ = ¢v¢ Associative : ( ¢^¢ ) ^x = ¢^ ( ¢^x ) ( ¢v¢ ) vx = ¢ , v ( ¢vx ) Idempotent : ¢A~ = ~b 4v4 = @ Distributive : ( ~v¢ ) ^x = ( ~^x ) v ( ¢^x ) ( ~ , A¢ ) Vx = ( ~VX ) ^ ( ¢VX ) Absorption : ( ¢A¢ ) V~ = ~ , ( ¢v¢ ) A¢ = 4 , Path Equivalence : E1 AE2 E , ^ E2 EAz : c E l : E { , ) E -- -- E2 whenever E1 _C E2 = E1 ^ ( E2 u { zy I ~ e El } ) for any y such that 3z : z ~ El and zy E E2 -EA ( A y : c ) wherexeE glEE = E A { z } if '' z is a prefix of a string in E = NIL = TOP for any E such that there are strings z , zy E E and y # e ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( s } ( 6 } ( 7 ) is ) ( 9 ) ( 1o ) ( 11 ) ( n ) ( 13 ) ( 14 ) ( 15 ) ( 16 ) ( 17 ) ( 18 ) ( 19 ) ( 20 ) ( 21 ) ( 22 ) ( 23 ) ( 24 ) ( 2s ) ( 26 ) Figure 4 : Laws of Equivalence for Formulas .</sentence>
				<definiendum id="0">TOP</definiendum>
				<definiendum id="1">Idempotent</definiendum>
				<definiens id="0">¢A¢ = ¢^¢ ¢v¢ = ¢v¢ Associative : ( ¢^¢ ) ^x = ¢^ ( ¢^x ) ( ¢v¢ ) vx = ¢ , v ( ¢vx )</definiens>
				<definiens id="1">E1 AE2 E , ^ E2 EAz : c E l : E { , ) E -- -- E2 whenever E1 _C E2 = E1 ^</definiens>
				<definiens id="2">a prefix of a string in E = NIL = TOP for any E such that there</definiens>
			</definition>
			<definition id="3">
				<sentence>A formula is in disjt , neti , ~s normal form ( DNF ) if and only if it has the form ~1 V ... v ~bn , where each ~i is either ( a ) lx : ... : lk : a , where a E A , and no path occurs more than once ( b ) \ [ &lt; pl &gt; , ... , &lt; p~ &gt; \ ] , where each p~ E L* , and each set denotes an equivalence class of paths , and all such sets disjoint .</sentence>
				<definiendum id="0">formula</definiendum>
				<definiendum id="1">DNF</definiendum>
				<definiens id="0">either ( a ) lx : ... : lk : a , where a E A , and no path occurs more than once ( b ) \ [ &lt; pl &gt; , ... , &lt; p~ &gt; \ ] , where each p~ E L* , and each set denotes an equivalence class of paths , and all such sets disjoint</definiens>
			</definition>
			<definition id="4">
				<sentence>When Y is unified with the term containing \ [ &lt; adjunct obj &gt; , &lt; actor &gt; \ ] , the equivalence ( 22 ) specifies that we can add the term adjunct : obj : case : nominative .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">unified with the term containing \ [ &lt; adjunct obj &gt;</definiens>
			</definition>
			<definition id="5">
				<sentence>The size of the formula with paths expanded is at most n x p , where n is the length of the original formula , and p is the length of the longest path .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the original formula , and p is the length of the longest path</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>ABSTRACT Existing models of plan inference ( PI ) in conversation have assumed that the agent whose plan is being inferred ( the actor ) and the agent drawing the inference ( the observer ) have identical beliefs about actions in the domain .</sentence>
				<definiendum id="0">ABSTRACT Existing</definiendum>
				<definiendum id="1">PI</definiendum>
				<definiens id="0">the actor ) and the agent drawing the inference ( the observer</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , Allen 's model , which was one of the earliest accounts of PI in conversation 1 and impired a great deal of the work done subsequently , includes , as a typical PI rule , the following : `` SBAW ( P ) ~i SBAW ( ACT ) if P is a precondition of ACT '' \ [ 2 , page 120\ ] .</sentence>
				<definiendum id="0">Allen 's model</definiendum>
				<definiendum id="1">SBAW</definiendum>
				<definiens id="0">a precondition of ACT '' \</definiens>
			</definition>
			<definition id="2">
				<sentence>Generation is a relation over actions , not over act-types .</sentence>
				<definiendum id="0">Generation</definiendum>
				<definiens id="0">a relation over actions , not over act-types</definiens>
			</definition>
			<definition id="3">
				<sentence>PLAN INFERENCE IN QUESTION-ANSWERING Models of the question-answering process often include a claim that the respondent ( R ) must infer the plans of the questioner ( Q ) .</sentence>
				<definiendum id="0">PLAN INFERENCE IN QUESTION-ANSWERING Models</definiendum>
				<definiens id="0">a claim that the respondent ( R ) must infer the plans of the questioner</definiens>
			</definition>
			<definition id="4">
				<sentence>209 The discrepancy resulting in ( 3 ) is represented in ( 5 ) ; the discrepancy in ( 4 ) is represented in ( 5 ) plus ( 6 ) : ( 5 ) BEL ( R , B EL ( Q , EXEC ( set-permissions ( mmfile , read , facult y ) , Q , tz ) , tl ) , t~ ) A BEL ( R , - , EXEC ( set-permissions ( mmfile , read , facult y ) , Q , t2 ) , t~ ) ( 6 ) BEL ( R , BEL ( Q , EXEC ( prevent ( mmfile , read , tom ) , Q , t2 ) , tl ) , ti ) A BEL ( R , -- EXgC ( prevent ( ram file , read , tom ) , Q , t2 ) , h ) The second potential discrepancy is that R may believe false some belief corresponding to Clause ( ii ) of ( P1 ) that , by virtue of ( 2 ) , she ascribes to Q. I will then say that she believes the plan to be ill-formed .</sentence>
				<definiendum id="0">, EXEC ( set-permissions</definiendum>
				<definiendum id="1">BEL</definiendum>
				<definiens id="0">read , facult y ) , Q , tz ) , tl ) , t~ ) A BEL ( R , -</definiens>
				<definiens id="1">mmfile , read , facult y )</definiens>
			</definition>
			<definition id="5">
				<sentence>Specifically , all of the following may be tr~e : ( 13 ) BEL ( R , BgL ( Q , gXEC ( set-permissions ( mmfile , read , facult y ) , q , tz ) , tl ) , t~ ) ( 14 ) BEL ( R , BEL ( Q , gXEC ( prevent ( mmfile , read , tom ) , Q , t2 ) , tl ) , t~ ) ( 15 ) BEL ( R , BEL ( Q , G EN ( set-permissions ( mmfile , read , facult y ) , prevent ( mmfile , read , tom ) , Q , tz ) , tl ) , t~ ) ( 16 ) BEL ( R , I NT ( Q , set-permissions ( mm file , read , facult y ) , t2 , ~l ) , tt ) ( 17 ) BEL ( R , INT ( Q , prevent ( mmfile , read , tom ) , t2 , tl ) , t~ ) ( 18 ) BEL ( R , I iT ( Q , by ( set-permissions ( mmfile , read , facult y ) , prevent ( mmfile , read , tom ) ) , t2 , tl ) , tl ) Together , ( 13 ) - ( 18 ) are sumcient for R 's believing that Q has the simple plan as expressed in ( 2 ) .</sentence>
				<definiendum id="0">BEL</definiendum>
				<definiendum id="1">BEL</definiendum>
				<definiendum id="2">BEL</definiendum>
				<definiens id="0">mmfile , read , facult y ) , q , tz ) , tl</definiens>
				<definiens id="1">read , tom ) ) , t2 , tl ) , tl</definiens>
			</definition>
			<definition id="6">
				<sentence>And if R further believes that the system manager can override file permissions and that Tom is the system manager , but also that Q does not know the former fact , R will judge that Q 's plan is ill-formed , and may provide a response such as that in ( 7 ) .</sentence>
				<definiendum id="0">Tom</definiendum>
				<definiens id="0">the system manager</definiens>
			</definition>
			<definition id="7">
				<sentence>Thus , the sentence CGEN ( a , B , C ) can be seen as one possible interpretation of a hieran=hical planning operator with header B , preconditions C , and body a. Conditional generation is a relation between two act-types and a set of conditions ; generation , which is a relation between two actions , can be defined in terms of conditional generation .</sentence>
				<definiendum id="0">sentence CGEN</definiendum>
				<definiendum id="1">body a. Conditional generation</definiendum>
				<definiendum id="2">generation</definiendum>
				<definiens id="0">one possible interpretation of a hieran=hical planning operator with header B , preconditions C , and</definiens>
				<definiens id="1">a relation between two act-types and a set of conditions</definiens>
			</definition>
			<definition id="8">
				<sentence>Similarly , in reasoning about query ( 1 ) in the case in which R does not believe that Q knows that Tom is a faculty member , R can ascribe to Q the beliefs that , by setting the permissions on a file to restrict access to a partieulac group , one denies access to everyone who is neither s member of that group nor the system manager , as expressed in ( 20 ) : ( 20 ) BEL ( R , BEL ( Q , CGEN ( set-permissions ( X , P , Y ) , prevent ( X , P , Z ) , - , member ( g , Y ) ) , h ) , tt ) She can also ascribe to Q the belief that Tom is not a member of the faculty , ( or more precisely , that Tom will not be a member of the faculty at the intended performance time tz ) , i.e. , 211 • I ( 21 ) BEL ( R , BEL ( Q , HOLDS ( -~member ( tom , facuity ) , t2 ) , tl ) , tl } The conjunction of these two beliefs explains Q 's further belief , expressed in ( 15 ) , that , by setting the permissions to facultyread only at t2 , he can prevent Tom from reading the file .</sentence>
				<definiendum id="0">BEL ( Q , CGEN ( set-permissions</definiendum>
				<definiendum id="1">BEL</definiendum>
				<definiens id="0">a faculty member</definiens>
			</definition>
			<definition id="9">
				<sentence>Saying that an agent R believes that another agent Q has some eplan is shorthand for describing a set of beliefs possessed by R , specifically : ( P2 ) ( R , EPLAN ( Q , ~n , \ [ al ... .. an-l\ ] , \ [ pl ... .. Pn-l\ ] , t2 , tl ) , tl ) ( i ) BEL ( R , BEL ( Q , EXEC ( cq , Q , t2 ) , tl ) , tl ) , for i = 1 , ... , n A ( ii ) BEL ( R , BEL ( Q , G EN ( ~ , ai+t , Q , t2 ) , tt ) , tl ) , for i = 1 , ... , n-I A ( iii ) BEL ( R , INT ( Q , ~I , tz , tl ) , tl ) , for i = 1 , ... , n A ( iv ) BEL ( R , INT ( Q , by~al , ai+l ) , t2 , tl ) , tl ) , for i = 1 , ... , n-1 A ( v ) BEL ( R , BEL ( Q , pi , tl ) , tl ) , where each Pi is CGEN ( ai , cq+l , Ci ) A HOLDS ( Ci , t2 ) I claim that the PI process underlying cooperative questionanswering can be modeled as an attempt to infer an eplan , i.e. , to form a set of beliefs about the questioner 's beliefs and intentions that satisfies ( P2 ) .</sentence>
				<definiendum id="0">BEL</definiendum>
				<definiendum id="1">EXEC</definiendum>
				<definiendum id="2">... , n A ( ii ) BEL</definiendum>
				<definiendum id="3">... , n-I</definiendum>
				<definiens id="0">describing a set of beliefs possessed by R , specifically : ( P2 ) ( R , EPLAN ( Q , ~n , \ [ al ... .. an-l\ ] , \ [ pl ... .. Pn-l\ ] , t2 , tl ) , tl ) ( i ) BEL ( R ,</definiens>
				<definiens id="1">A ( iii ) BEL ( R , INT ( Q , ~I , tz , tl ) , tl</definiens>
				<definiens id="2">by~al , ai+l ) , t2 , tl ) , tl</definiens>
			</definition>
			<definition id="10">
				<sentence>In more detail , if part of R 's belief that Q has the antecedent eplan is a belief that Q intends to do some act a , and R has reason to believe that Q believes that act-type a conditionally generates act-type 3 ' under condition C , then R can infer that Q intends to do a in order to do % believing as well that C will hold at performance time .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">if part of R 's belief that Q has the antecedent eplan is a belief that Q intends to do some act a</definiens>
			</definition>
			<definition id="11">
				<sentence>This reasoning can be encoded in the representation language in rule ( PI1 ) : ( PII ) BEL ( R , EPLAN ( Q , an , \ [ al ... .. an-a\ ] , \ [ pl ... .. On-t\ ] , t2 , h ) , h ) A BEL ( R , CGEN ( an , % C ) , q ) BEL ( R , EPLAN ( Q , % \ [ al ... .. a , \ ] , \ [ pl ... .. p , \ ] , t2 , tl ) , tl ) where p , ~ .</sentence>
				<definiendum id="0">BEL</definiendum>
				<definiens id="0">.. p , \ ] , t2 , tl ) , tl ) where p</definiens>
			</definition>
			<definition id="12">
				<sentence>CGEN ( ar , , '' I , C ) ^ HOLDS ( C , t2 ) This rule says that , if R 's belief that Q has some eplan includes a belief that Q intends to do an act an , and R also believes that act-type a~ conditionally generates some `` ~ under condition C , then R can ( nonmonotonically ) infer that Q has the additional intention of doing a , in order to do ~ -- i.e. , that he intends to do by ( an , `` ~ ) .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">some eplan includes a belief that Q intends to do an act an , and</definiens>
			</definition>
			<definition id="13">
				<sentence>212 ( 23 ) B E L ( R , E P b A N ( Q , set-p ermissions ( mmfile , read , facult y ) , \ [ \ ] , t2 , t , ) , tl ) ( 24 ) BEL ( R , EPLAN ( Q , prevent ( mmfile , read , tom ) , \ [ \ ] , t2 , tl ) , tl ) The belief in ( 23 ) is justified by the fact that ( 13 ) satisfies Clause ( i ) of ( P2 ) , ( 16 ) satisfies Clause ( iv ) of ( P2 ) , and Clauses ( ii ) , ( iii ) , and ( v ) are vacuously satisfied .</sentence>
				<definiendum id="0">set-p ermissions</definiendum>
				<definiendum id="1">Clauses ( ii</definiendum>
				<definiens id="0">read , facult y ) , \ [ \ ] , t2 , t , )</definiens>
			</definition>
			<definition id="14">
				<sentence>SPIRIT is a demonstration system , implemented to demonstrate the PI model developed in this work ; consequently only a few key examples , which are sufficient to demonstrate SPIRIT 's capabilities , have been implemented .</sentence>
				<definiendum id="0">SPIRIT</definiendum>
				<definiens id="0">a demonstration system , implemented to demonstrate the PI model developed in this work</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>An individuating set for an agent S is a maximal set of terms , all believed by S to be denoting the same object .</sentence>
				<definiendum id="0">individuating set for an agent S</definiendum>
				<definiens id="0">a maximal set of terms</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>ABSTRACT While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody , the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody .</sentence>
				<definiendum id="0">ABSTRACT</definiendum>
				<definiens id="0">a robust working system to test the possible relations between syntax and prosody</definiens>
			</definition>
			<definition id="1">
				<sentence>Current text-to-speech systems have no access to the syntactic and semantic properties of a sentence that influence phrase-level prosody .</sentence>
				<definiendum id="0">Current text-to-speech systems</definiendum>
				<definiens id="0">the syntactic and semantic properties of a sentence that influence phrase-level prosody</definiens>
			</definition>
			<definition id="2">
				<sentence>To build syntactic structure , Fidditch uses a grammar that requires the representations produced by lexical and syntactic rules to be consistent with the ( semantic ) predicateargument structure .</sentence>
				<definiendum id="0">Fidditch</definiendum>
				<definiens id="0">uses a grammar that requires the representations produced by lexical and syntactic rules to be consistent with the ( semantic ) predicateargument structure</definiens>
			</definition>
			<definition id="3">
				<sentence>The surface syntactic structures generated by the parser represent the argument structure of a phrase or sentence , i.e. the `` core '' constituents of a sentence ( its subject ( NP ) , modality ( AUX ) , and predicate ( VP ) ) and the complements of phrasal heads .</sentence>
				<definiendum id="0">AUX</definiendum>
				<definiens id="0">surface syntactic structures generated by the parser represent the argument structure of a phrase or sentence</definiens>
			</definition>
			<definition id="4">
				<sentence>THE ENGLISH THAT IS SPOKEN IN AMERICA AT THE PRESENT DAY HAS RETAINED A GOOD MANY CHARACTERISTICS OF EARLIER BRITISH ENGLISH THAT DO NOT SURVIVE IN BRITISH ENGLISH TODAY .</sentence>
				<definiendum id="0">ENGLISH THAT</definiendum>
				<definiens id="0">IS SPOKEN IN AMERICA AT THE PRESENT DAY HAS RETAINED A GOOD MANY CHARACTERISTICS OF EARLIER BRITISH ENGLISH THAT DO NOT SURVIVE IN BRITISH ENGLISH TODAY</definiens>
			</definition>
</paper>

		<paper id="1040">
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>~ For example , return to the artificial Kimmo system that decides Boolean satisfiability for formulas in variables x , y , and z ( Figure 2 ) .</sentence>
				<definiendum id="0">z</definiendum>
				<definiens id="0">the artificial Kimmo system that decides Boolean satisfiability for formulas in variables x , y , and</definiens>
			</definition>
			<definition id="1">
				<sentence>50 y-consistency ... . 2,3 '' '' z-consistency ... . 1 `` '' z/T z/F ... . 2,3 ... ... .. 2,3 '' '' • `` 2,3 ... ... .. 1 `` '' ( 2 , z/T,2 ) &lt; 3 , z/T,3 ) &lt; 2 , z/F , 2 ) ( 3 , z/F,3 ) ( 1 , z/T,2 ) &lt; 1 , z/F , 3 &gt; ... . 2,3 ... . ... . 2,3 ... . Figure 4 : When the active transitions of each automaton are represented by triples , it is easy to enforce the constraints that relate the left and right state-sets and the pair set .</sentence>
				<definiendum id="0">z/T z/F</definiendum>
				<definiens id="0">relate the left and right state-sets and the pair set</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>Fundamental frequency ( fO ) is the paramount physical correlate of the sensation of pitch , and , in many languages , the time course of f0 is one of the primary phonetic manifestations of intonation .</sentence>
				<definiendum id="0">Fundamental frequency</definiendum>
				<definiendum id="1">fO )</definiendum>
				<definiens id="0">one of the primary phonetic manifestations of intonation</definiens>
			</definition>
			<definition id="1">
				<sentence>Fundamental frequency contours for two intonation patterns for the utterance An orange ballgown .</sentence>
				<definiendum id="0">Fundamental frequency</definiendum>
				<definiens id="0">contours for two intonation patterns for the utterance An orange ballgown</definiens>
			</definition>
			<definition id="2">
				<sentence>We account for this rise by positing a L % tone ( the boundary L~ s marking the phrase boundary , and a H tone ( the phrasal I~ associated with a designated syllable near the beginning of the phrase .</sentence>
				<definiendum id="0">H tone</definiendum>
				<definiens id="0">the phrasal I~ associated with a designated syllable near the beginning of the phrase</definiens>
			</definition>
			<definition id="3">
				<sentence>An intermediate phrase consists of one or more accentual phrases ( only rarely more than three ) .</sentence>
				<definiendum id="0">intermediate phrase</definiendum>
			</definition>
			<definition id="4">
				<sentence>The difference in timing between a weak L~ and a strong L~v ( see Section 2.4 ) is accomplished by giving a weak L~o only a point duration and a strong L~o the `` standard tone duration '' ( a speakerand ratespecific value roughly the length of a short syllable ) .</sentence>
				<definiendum id="0">strong L~v</definiendum>
				<definiens id="0">a speakerand ratespecific value roughly the length of a short syllable )</definiens>
			</definition>
			<definition id="5">
				<sentence>Labeled arrows illustrate the application of representative tone scaling rules .</sentence>
				<definiendum id="0">Labeled arrows</definiendum>
				<definiens id="0">illustrate the application of representative tone scaling rules</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>Although the context does not contain the specific trial script , the social authority which relates the judge and the defendant exists also between the dean and John .</sentence>
				<definiendum id="0">social authority</definiendum>
				<definiens id="0">relates the judge and the defendant exists also between the dean and John</definiens>
			</definition>
			<definition id="1">
				<sentence>As a result , RINA generates the literal interpretation and awaits confirmation from the user .</sentence>
				<definiendum id="0">RINA</definiendum>
				<definiens id="0">generates the literal interpretation and awaits confirmation from the user</definiens>
			</definition>
			<definition id="2">
				<sentence>If the user repeats the utterance or generates a negation , then RINA generates a number of utterances , based on the current context , in hypothesizing a novel phrase interpretation .</sentence>
				<definiendum id="0">RINA</definiendum>
				<definiens id="0">generates a number of utterances , based on the current context , in hypothesizing a novel phrase interpretation</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>Topics to be considered include the components of a natural language processing system ; syntax analysis ( including context-free grammars , augmented context-free grammars , grammatical constraints , and sources of syntactic ambiguity ) ; semantic analysis ( including meaning representation , semantic constraints , quantifier analysis ) ; and discourse analysis ( identifying implicit information , establishing text coherence , frames , and scripts ) .</sentence>
				<definiendum id="0">discourse analysis</definiendum>
				<definiens id="0">a natural language processing system ; syntax analysis ( including context-free grammars , augmented context-free grammars , grammatical constraints , and sources of syntactic ambiguity ) ; semantic analysis ( including meaning representation , semantic constraints</definiens>
			</definition>
</paper>

		<paper id="1002">
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>Three central goals of work in the generalized phrase structure grammar ( GPSG ) linguistic framework , as stated in the leading book `` Generalized Phrase Structure Grammar '' Gazdar et al ( 1985 ) ( hereafter GKPS ) , are : ( 1 ) to characterize all and only the natural language grammars , ( 2 ) to algorithmically determine membership and generative power consequences of GPSGs , and ( 3 ) to embody the universalism of natural language entirely in the formal system , rather than by statements made in it .</sentence>
				<definiendum id="0">GPSG</definiendum>
				<definiens id="0">to characterize all and only the natural language grammars</definiens>
			</definition>
			<definition id="1">
				<sentence>On the basis of this result , I argue that GPSG fails to define the natural language grammars , and that the generative power consequences of the GPSG framework can not be algorithmically determined , contrary to goals one and two .</sentence>
				<definiendum id="0">GPSG</definiendum>
				<definiens id="0">fails to define the natural language grammars</definiens>
			</definition>
			<definition id="2">
				<sentence>restrictions ( FCRs ) , and feature specification defaults ( FSDs ) ) and four universal components : a theory of syntactic features , principles of universal feature instantiation , principles of semantic interpretation , and formal relationships among various components of the grammar .</sentence>
				<definiendum id="0">restrictions</definiendum>
				<definiens id="0">a theory of syntactic features , principles of universal feature instantiation , principles of semantic interpretation , and formal relationships among various components of the grammar</definiens>
			</definition>
			<definition id="3">
				<sentence>The essence of GPSG is the constrained mapping of ID rules into local trees .</sentence>
				<definiendum id="0">essence of GPSG</definiendum>
			</definition>
			<definition id="4">
				<sentence>The only constraint strongly inherent in GPSG theory ( when compared to context-free grammars ( CFGs ) ) is finite feature closure , which limits the number of GPSG nonterminal symbols to be finite and bounded .</sentence>
				<definiendum id="0">context-free grammars ( CFGs ) )</definiendum>
				<definiens id="0">finite feature closure , which limits the number of GPSG nonterminal symbols to be finite and bounded</definiens>
			</definition>
			<definition id="5">
				<sentence>For example , exhaustive constant partial ordering ( ECPO ) -which is a constraint on strong generative capacity -can be done away with for all intents and purposes by nonterminal renaming , and constraints arising from principles of universal feature instantiation do n't apply to fully instantiated ID rules .</sentence>
				<definiendum id="0">ECPO</definiendum>
			</definition>
			<definition id="6">
				<sentence>The productions N4 -- * OL41L4 I OOL4 I 01L~ I llL4 I ... are added to the converted CFG GtVTM , which generates a language of the form L4 -- * oooo I OOOl \ ] OOlO I ... I E I L4L4 Where L4 generates all symbols of length 4 , and N4 generates all strings not of length 0 rood k , where k = 4 ( i.e. all strings of length 1,2,3 mod 4 ) .</sentence>
				<definiendum id="0">CFG GtVTM</definiendum>
				<definiendum id="1">N4</definiendum>
				<definiens id="0">generates a language of the form L4 -- * oooo I OOOl \ ] OOlO I ... I E I L4L4 Where L4 generates all symbols of length 4</definiens>
				<definiens id="1">generates all strings not of length 0 rood k</definiens>
			</definition>
			<definition id="7">
				<sentence>7 In short , the context-free framework is the wrong idea completely , and this is to be expected : why should the arbitrary generative power classifications of mathematics ( formal language theory ) be at all relevant to biology ( human language ) ?</sentence>
				<definiendum id="0">context-free framework</definiendum>
				<definiens id="0">to be expected : why should the arbitrary generative power classifications of mathematics ( formal language theory</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>SPECIALTY ) where `` PHYS '' is the ID of the treating physician , and `` DIAMOTHER '' is a boolean field indicating whether or not the patient 's mother is diabetic .</sentence>
				<definiendum id="0">SPECIALTY ) where `` PHYS</definiendum>
			</definition>
			<definition id="1">
				<sentence>The problem is that the database chooses to highlight the complex property DIAMOTHER while avoiding the cost of storing its constituent predicates MOTHER and DIABETIC the conceptual units corresponding to the words of the utterance .</sentence>
				<definiendum id="0">DIABETIC</definiendum>
				<definiens id="0">the conceptual units corresponding to the words of the utterance</definiens>
			</definition>
			<definition id="2">
				<sentence>NIKL is a taxonomic frame-like system with two basic data structures : concepts and roles .</sentence>
				<definiendum id="0">NIKL</definiendum>
				<definiens id="0">a taxonomic frame-like system with two basic data structures : concepts and roles</definiens>
			</definition>
			<definition id="3">
				<sentence>The expression `` ( FORMULA X ) '' denotes a formula of the meaning representation language in which the variable X ( and perhaps others ) appears freely .</sentence>
				<definiendum id="0">FORMULA X )</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a formula of the meaning representation language in which the variable</definiens>
			</definition>
			<definition id="4">
				<sentence>( 5 ) ( foroll X2 : S ( - &gt; ( R XI X2 ) ( P X2 ) ) ) i &gt; ( P ' Xl ) where P ' : = ( VALUERESTRICT ( VRDIFF R S ) P ) ( S ) ( exists X2 : S ( R X1 X2 ) ) = &gt; ( P ' X1 ) where P ' : = ( VALUERESTRICT R S ) and R must be a functional role ( 7 ) ( exists X2 : S ( R Xl X2 ) ) = &gt; ( P ' X1 ) where P ' : = ( NUMBERRESTRICT ( VRDIFF R S ) 1 NIL ) ( S ) ( and ( P X ) ) = &gt; ( P X ) ( 9 ) ( R X C ) = &gt; ( P X ) where P : = ( VALUERESTRICT R C ) and R is functional , C an individual concept Now , let us suppose that the exercise at the end of the last section has been carried out , and that the concept PATIENT-WITH-DIABETIC-MOTHER has been created and given the appropriate translation rule .</sentence>
				<definiendum id="0">P X )</definiendum>
				<definiens id="0">P ' : = ( VALUERESTRICT ( VRDIFF R S ) P ) ( S ) ( exists X2 : S ( R X1 X2 ) ) = &gt; ( P ' X1 ) where P ' : = ( VALUERESTRICT R S ) and R must be a functional role ( 7 ) ( exists X2 : S ( R Xl X2 ) ) = &gt; ( P ' X1 ) where P ' : = ( NUMBERRESTRICT ( VRDIFF R S ) 1 NIL ) ( S )</definiens>
				<definiens id="1">P : = ( VALUERESTRICT R C ) and R is functional</definiens>
			</definition>
</paper>

		<paper id="1028">
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>Second , since GPSG has weak context-free generative power and context-free languages can be parsed in O ( n ~ ) by a wide range of algorithms , GPSG parsers would appear to run in polynomial time .</sentence>
				<definiendum id="0">GPSG</definiendum>
				<definiens id="0">weak context-free generative power and context-free languages</definiens>
			</definition>
			<definition id="1">
				<sentence>This result puts GPSG-Recognition in a complexity class occupied by few natural problems : GPSG-Recognition is harder than the traveling salesman problem , context-sensitive language recognition , or winning the game of Chess on an n x n board .</sentence>
				<definiendum id="0">GPSG-Recognition</definiendum>
			</definition>
			<definition id="2">
				<sentence>A generalized phrase structure grammar contains five languageparticular components -immediate dominance ( ID ) rules , metarules , linear precedence ( LP ) statements , feature co-occurrence restrictions ( FCRs ) , and feature specification defaults ( FSDs ) -and four universal components -a theory of syntactic features , principles of universal feature instantiation , principles of semantic interpretation , and formal relationships among various components of the grammar , s Syntactic categories are partial functions from features to atomic feature values and syntactic categories .</sentence>
				<definiendum id="0">feature co-occurrence restrictions ( FCRs</definiendum>
				<definiendum id="1">feature specification defaults</definiendum>
				<definiens id="0">principles of universal feature instantiation , principles of semantic interpretation , and formal relationships among various components of the grammar , s Syntactic categories are partial functions from features to atomic feature values and syntactic categories</definiens>
			</definition>
			<definition id="3">
				<sentence>The QBF problem is { QIF1Qzyz ... Qmy , nF ( yh YZ , ... , y , n ) I Qi 6 { V , 3 } , where the yi are boolean variables , F is a boolean formula of length n in conjunctive normal form with exactly ~More precisely , the metarule finite closure operation can increase the size of a GPSG G worse than exponentially : from I Gi to O ( \ ] G \ [ 2~ ) .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">yh YZ , ... , y , n ) I Qi 6 { V , 3 } , where the yi are boolean variables</definiens>
				<definiens id="1">a boolean formula of length n in conjunctive normal form with exactly ~More precisely , the metarule finite closure operation can increase the size of a GPSG G worse than exponentially</definiens>
			</definition>
			<definition id="4">
				<sentence>The feature wi represents the formula literal wi in w , yj represents the variable yj in f2 , and ci represents the truth value of the i th clause in F. Atom = { LEVEL , LABEL } u { w , : 1 &lt; i &lt; lwl } u { y : : 1 &lt; j &lt; m } u { c~:1 &lt; ; &lt; ~ } F-Atom = { qk , q~ : l &lt; k &lt; m } p° ( LEVEL ) = { k : l &lt; k &lt; mA-1 } po ( f ) = { 0,1 } Vf E Atom { LEVEL } FCR 's are included to constrain both the form and content of the guesses : Vk , l &lt; k &lt; m , \ ] LEVEL k\ ] = \ [ qk \ [ \ [ Yk 1\ ] \ [ LEVEL k + 1\ ] \ ] \ ] &amp; \ [ ql \ [ \ [ Vk 0\ ] \ [ LEVEL k + 1\ ] \ ] \ ] Vi , 1 &lt; i &lt; m \ [ c , \ ] = \ [ w3 , -~\ ] &amp; \ [ ~3~-l\ ] &amp; \ [ ~3 , \ ] \ ] LABEL \ ] -= \ [ cl\ ] Vk , 1 &lt; k &lt; m , \ [ LABEL\ ] -- = \ [ yk\ ] mined by quantifier meaning : Vk , l &lt; k &lt; rn , if Qk = `` V '' , then include : \ [ LEVEL k\ ] &amp; \ [ LABEL 1\ ] -- -- -\ [ qk \ [ \ [ LABEL ll\ ] \ ] &amp; \ [ q~ \ [ \ [ LABEL 1\ ] \ ] 1 \ [ LEVEL k\ ] &amp; \ [ LABEL O\ ] -- -- \ [ qk \ [ \ [ LABEL 0\ ] \ ] \ ] V \ [ q~ \ [ \ [ LABEL 0\ ] \ ] 1 otherwise Qk = `` 3 '' , and include : \ [ LEVEL k\ ] &amp; \ [ LABEL 1\ ] -\ [ qk \ [ \ [ LABEL 11\ ] \ ] Y \ [ q~ \ [ \ [ LABEL I\ ] \ ] \ ] \ [ LEVEL k\ ] &amp; \ [ LABEL O\ ] -\ [ qk \ [ \ [ LABEL 0\ ] \ ] \ ] &amp; Iq ~ \ [ \ [ LABEL 0\ ] \ ] \ ] The category-valued features qk and q~ represent the quantifier Qk. In the category value of qk , the formula variable yk = 1 everywhere , while in the category value of q~ , Yk = 0 everywhere. are permitted : \ [ LEVEL 1\ ] ~ ILABEL 1\ ] `` down the tree '' : Vi , k l &lt; _i &lt; k &lt; m , \ [ Yi 1\ ] D \ [ qk \ [ \ [ Yi 1\ ] \ ] \ ] &amp; \ [ q~ \ [ \ [ Yi 1\ ] \ ] \ ] \ [ ~ , O\ ] ~ \ [ q~ \ [ \ [ y~ o\ ] \ ] \ ] &amp; \ [ q i \ [ \ [ y~ 0\ ] \ ] \ ] 33 F : Vi , kl &lt; i &lt; lw\ [ and 1 &lt; k &lt; m , if wi = Yk , then include : \ [ Yk 11 D \ [ w , 11 \ [ ~ko\ ] D \ [ ~o\ ] else if wi = Y-~ , then include : \ [ y , ~ : \ ] D \ [ ~ , o\ ] \ [ ~ , ~ , o\ ] D N , 1\ ] nodes : Vi l &lt; i &lt; ~ , It , o\ ] _= \ [ ~s , -2 o\ ] ~\ [ ~ , _ , o\ ] ~\ [ ~ , o\ ] \ [ ci 1\ ] -\ [ ws , -~ 1\ ] V\ [ ws , _I 1\ ] V\ [ ws , 1\ ] \ [ LEVEL rn + l\ ] &amp; \ [ c , 0\ ] D \ [ LABEL 0\ ] \ [ LEVEL m+ 1\ ] d~\ [ Cx 1\ ] &amp; : \ [ c2 l\ ] &amp; ... &amp; \ [ c~ol/31 \ ] D \ [ LABEL 11 The reduction constructs O ( 1~1 ) features and O ( m ~ ) FCRs of size O ( log m ) in a simple manner , and consequently may be seen to be polynomial time. 0.~.P The primary source of intractability in the theory of syntactic features is the large number of possible syntactic categories ( arising from finite feature closure ) in combination with the computational power of feature co-occurrence restrictions , s FCRs of the `` disjunctive consequence '' form \ [ f v\ ] D \ [ fl vl\ ] V ... V \ [ fn vn\ ] compute the direct analogue of Satisfiability : when used in conjunction with other FCRs , the GPSG effectively must try all n feature-value combinations. Two isolated membership problems for GPSG 's component formal devices were considered above in an attempt to isolate sources of complexity in GPSG theory. In this section the recognition problem ( RP ) for GPSG theory as a whole is considered. I begin by arguing that the linguistically and computationally relevant recognition problem is the universal recognition problem , as opposed to the fixed language recognition problem. I then show that the former problem is exponential-polynomial ( Exp-Poly ) time-hard. SFinite feature closure admits a surprisingly large number of possible categories. Given a specification ( F , Atom , A , R , p ) of K , let a =lAteral and b =IF Atom I. Assume that all atomic features are binary : a feature may be + , - , or undefined and there are 3 a 0-1evel categories. The b categoryvalued features may each assume O ( 3 ~ ) possible values in a 1-1evel category , so I/f ' I= O ( 3= ( 3 '' ) b ) . More generally , IK = K'IO ( 3~'~C ~ orr~ , = ) = O ( 3 ~° '' ~C : oo , ~ ) = O ( ~* '' . ) = O ( 3 o. '' ) where E~=o ~ converges toe ~ 2.7 very rapidly and a , b = O ( IGI ) ; a = 25 , b = 4 in GKPS. The smallest category in K will be 1 symbol ( null set ) , and the largest , maximally-specified , category wilt be of symbol-slze log I K I = oca. b ! ) . The universal recognition problem is : given a grammar G and input string x , is z C L ( G ) ? . Alternately , the recognition problem for a class of grammars may be defined as the family of questions in one unkown. This fized language recognition problem is : given an input string x , is z E L for some fixed language L ? . For the fixed language RP , it does not matter which grammar is chosen to generate L -typically , the fastest grammar is picked. It seems reasonable clear that the universal RP is of greater linguistic and engineering interest than the fixed language RP. The grammars licensed by linguistic theory assign structural descriptions to utterances , which are used to query and update databases , be interpreted semantically , translated into other human languages , and so on. The universal recognition problem -unlike the fixed language problem -determines membership with respect to a grammar , and therefore more accurately models the parsing problem , which must use a grammar to assign structural descriptions. The universal RP also bears most directly on issues of natural language acquisition. The language learner evidently possesses a mechanism for selecting grammmars from the class of learnable natural language grammars/~a on the basis of linguistic inputs. The more fundamental question for linguistic theory , then , is `` what is the recognition complexity of the class /~c ? '' . If this problem should prove computationally intractable , then the ( potential ) tractability of the problem for each language generated by a G in the class is only a partial answer to the linguistic questions raised. Finally , complexity considerations favor the universal RP. The goal of a complexity analysis is to characterize the amount of computational resources ( e.g. time , space ) needed to solve the problem in terms of all computationally relevent inputs on some standard machine model ( typically , a multi-tape deterministic Turing machine ) . We know that both input string length and grammar size and structure affect the complexity of the recognition problem. Hence , excluding either input from complexity consideration would not advance our understanding. 9 Linguistics and computer science are primarily interested in the universal recognition problem because both disciplines are concerned with the formal power of a family of grammars. Linguistic competence and performance must be considered in the larger context of efficient language acquisition , while computational considerations demand that the recognition problem be characterized in terms of both input string and grammar size. Excluding grammar size from complexity consideration in order SThis ~consider all relevant inputs ~ methodology is universally assumed in the formal language and computational complexity literature. For example , Hopcraft and Ullman ( 1979:139 ) define the context-free grammar recognition problem as : `` Given a CFG G = ( V , T , P , $ ) and a string z in Y ' , is x in L ( G ) ? . '' . Garey and Johnson ( 1979 ) is a standard reference work in the field of computational complexity. All 10 automata and language recognition problems covered in the book ( pp. 265-271 ) are universal , i.e. of the form `` Given an instance of a machine/grammar and an input , does the machine/grammar accept the input7 ~ The complexity of these recognition problems is alt # ays calculated in terms of grammar and input size. 34 to argue that the recognition problem for a family of grammars is tractable is akin to fixing the size of the chess board in order to argue that winning the game of chess is tractable : neither claim advances our scientific understanding of chess or natural language. Theorem 3 : GPSG-Recognition is Exp-Poly time-hard Proof 3 : By direct simulation of a polynomial space bounded alternating Turing Machine M on input w. Let S ( n ) be a polynomial in n. Then , on input M , a S ( n ) space-bounded one tape alternating Turing Machine ( ATM ) , and string w , we construct a GPSG G in polynomial time such that w E L ( M ) iff $ 0wllw22 ... w , ~n $ n÷l E L ( G ) . By Chandra and Stockmeyer ( 1976 ) , ASPACE ( S ( n ) ) = U DTIM~ cs ( `` ) ) c : &gt; 0 where ASPACE ( S ( n ) ) is the class of problems solvable in space Sin ) on an ATM , and DTIME ( F ( n ) ) is the class of problems solvable in time F ( n ) on a deterministic Turing Machine .</sentence>
				<definiendum id="0">feature wi</definiendum>
				<definiendum id="1">O</definiendum>
				<definiendum id="2">E~=o ~ converges</definiendum>
				<definiendum id="3">universal recognition problem</definiendum>
				<definiendum id="4">1979 )</definiendum>
				<definiendum id="5">ASPACE ( S ( n ) )</definiendum>
				<definiendum id="6">DTIME ( F ( n ) )</definiendum>
				<definiens id="0">represents the formula literal wi in w , yj represents the variable yj in f2 , and ci represents the truth value of the i th clause in F. Atom = { LEVEL</definiens>
				<definiens id="1">p° ( LEVEL ) = { k : l &lt; k &lt; mA-1 } po ( f ) = { 0,1 } Vf E Atom { LEVEL } FCR 's are included to constrain both the form and content of the guesses : Vk , l &lt; k &lt; m , \ ] LEVEL k\ ] = \ [ qk \ [ \ [ Yk 1\ ] \ [ LEVEL k + 1\ ] \ ] \ ] &amp; \ [ ql \ [ \ [ Vk 0\ ] \ [ LEVEL k + 1\ ] \ ] \ ] Vi , 1 &lt; i &lt; m \ [ c , \ ] = \ [ w3 , -~\ ] &amp; \ [ ~3~-l\ ] &amp; \ [ ~3 , \ ] \ ] LABEL \ ] -= \ [ cl\ ] Vk , 1 &lt; k &lt; m , \ [ LABEL\ ] -- = \ [ yk\ ] mined by quantifier meaning : Vk , l &lt; k &lt; rn , if Qk = `` V '' , then include : \ [ LEVEL k\ ] &amp; \ [ LABEL 1\ ] -- -- -\ [ qk \ [ \ [ LABEL ll\ ] \ ] &amp; \ [ q~ \ [ \ [ LABEL 1\ ] \ ] 1 \ [ LEVEL k\ ] &amp; \ [ LABEL O\ ] -- -- \ [ qk \ [ \ [ LABEL 0\ ] \ ] \ ] V \ [ q~ \ [ \ [ LABEL 0\ ] \ ] 1 otherwise Qk = `` 3 '' , and include : \ [ LEVEL k\ ] &amp; \ [ LABEL 1\ ] -\ [ qk \ [ \ [ LABEL 11\ ] \ ] Y \ [ q~ \ [ \ [ LABEL I\ ] \ ] \ ] \ [ LEVEL k\ ] &amp; \ [ LABEL O\ ] -\ [ qk \ [ \ [ LABEL 0\ ] \ ] \ ] &amp; Iq ~ \ [ \ [ LABEL 0\ ] \ ] \ ] The category-valued features qk and q~ represent the quantifier Qk. In the category value of qk , the formula variable yk = 1 everywhere , while in the category value of q~ , Yk = 0 everywhere. are permitted : \ [ LEVEL 1\ ] ~ ILABEL 1\ ] `` down the tree '' : Vi , k l &lt; _i &lt; k &lt; m , \ [ Yi 1\ ] D \ [ qk \ [ \ [ Yi 1\ ] \ ] \ ] &amp; \ [ q~ \ [ \ [ Yi 1\ ] \ ] \ ] \ [ ~ , O\ ] ~ \ [ q~ \ [ \ [ y~ o\ ] \ ] \ ] &amp; \ [ q i \ [ \ [ y~ 0\ ] \ ] \ ] 33 F : Vi , kl &lt; i &lt; lw\ [ and 1 &lt; k &lt; m , if wi = Yk , then include : \ [ Yk 11 D \ [ w , 11 \ [ ~ko\ ] D \ [ ~o\ ] else if wi = Y-~ , then include : \ [ y , ~ : \ ] D \ [ ~ , o\ ] \ [ ~ , ~ , o\ ] D N , 1\ ] nodes : Vi l &lt; i &lt; ~ , It , o\ ] _= \ [ ~s , -2 o\ ] ~\ [ ~ , _ , o\ ] ~\ [ ~ , o\ ] \ [ ci 1\ ] -\ [ ws , -~ 1\ ] V\ [ ws , _I 1\ ] V\ [ ws , 1\ ] \ [ LEVEL rn + l\ ] &amp; \ [ c , 0\ ] D \ [ LABEL 0\ ] \ [ LEVEL m+ 1\ ] d~\ [ Cx 1\ ] &amp; : \ [ c2 l\ ] &amp; ... &amp; \ [ c~ol/31 \ ] D \ [ LABEL 11 The reduction constructs O ( 1~1 ) features and</definiens>
				<definiens id="2">log m ) in a simple manner , and consequently may be seen to be polynomial time. 0.~.P The primary source of intractability in the theory of syntactic features is the large number of possible syntactic categories ( arising from finite feature closure ) in combination with the computational power of feature co-occurrence restrictions , s FCRs of the `` disjunctive consequence '' form \ [ f v\ ] D \ [ fl vl\ ] V ... V \ [ fn vn\ ] compute the direct analogue of Satisfiability : when used in conjunction with other FCRs , the GPSG effectively must try all n feature-value combinations. Two isolated membership problems for GPSG 's component formal devices were considered above in an attempt to isolate sources of complexity in GPSG theory. In this section the recognition problem ( RP ) for GPSG theory as a whole is considered. I begin by arguing that the linguistically and computationally relevant recognition problem is the universal recognition problem , as opposed to the fixed language recognition problem. I then show that the former problem is exponential-polynomial ( Exp-Poly ) time-hard. SFinite feature closure admits a surprisingly large number of possible categories. Given a specification ( F , Atom , A , R , p ) of K , let a =lAteral and b =IF Atom I. Assume that all atomic features are binary : a feature may be + , - , or undefined</definiens>
				<definiens id="3">3 '' ) b ) . More generally , IK = K'IO ( 3~'~C ~ orr~ , = ) = O ( 3 ~° '' ~C : oo , ~ ) = O ( ~* ''</definiens>
				<definiens id="4">the family of questions in one unkown. This fized language recognition problem is : given an input string x , is z E L for some fixed language L ? . For the fixed language RP , it does not matter which grammar is chosen to generate L -typically , the fastest grammar is picked. It seems reasonable clear that the universal RP is of greater linguistic and engineering interest than the fixed language RP. The grammars licensed by linguistic theory assign structural descriptions to utterances , which are used to query and update databases , be interpreted semantically , translated into other human languages , and so on. The universal recognition problem -unlike the fixed language problem -determines membership with respect to a grammar , and therefore more accurately models the parsing problem , which must use a grammar to assign structural descriptions. The universal RP also bears most directly on issues of natural language acquisition. The language learner evidently possesses a mechanism for selecting grammmars from the class of learnable natural language grammars/~a on the basis of linguistic inputs. The more fundamental question for linguistic theory , then , is `` what is the recognition complexity of the class /~c ? '' . If this problem should prove computationally intractable , then the ( potential ) tractability of the problem for each language generated by a G in the class is only a partial answer to the linguistic questions raised. Finally , complexity considerations favor the universal RP. The goal of a complexity analysis is to characterize the amount of computational resources ( e.g. time , space ) needed to solve the problem in terms of all computationally relevent inputs on some standard machine model ( typically , a multi-tape deterministic Turing machine ) . We know that both input string length and grammar size and structure affect the complexity of the recognition problem. Hence , excluding either input from complexity consideration would not advance our understanding. 9 Linguistics and computer science are primarily interested in the universal recognition problem because both disciplines are concerned with the formal power of a family of grammars. Linguistic competence and performance must be considered in the larger context of efficient language acquisition , while computational considerations demand that the recognition problem be characterized in terms of both input string and grammar size. Excluding grammar size from complexity consideration in order SThis ~consider all relevant inputs ~ methodology is universally assumed in the formal language and computational complexity literature. For example , Hopcraft and Ullman ( 1979:139 ) define the context-free grammar recognition problem as : `` Given a CFG G = ( V , T , P , $ ) and a string z in Y ' , is x in L ( G ) ? . ''</definiens>
				<definiens id="5">a standard reference work in the field of computational complexity. All 10 automata and language recognition problems covered in the book ( pp. 265-271 ) are universal , i.e. of the form `` Given an instance of a machine/grammar and an input , does the machine/grammar accept the input7 ~ The complexity of these recognition problems is alt # ays calculated in terms of grammar and input size. 34 to argue that the recognition problem for a family of grammars is tractable is akin to fixing the size of the chess board in order to argue that winning the game of chess is tractable : neither claim advances our scientific understanding of chess or natural language. Theorem 3 : GPSG-Recognition is Exp-Poly time-hard Proof 3 : By direct simulation of a polynomial space bounded alternating Turing Machine M on input w. Let S ( n ) be a polynomial in n. Then , on input M , a S ( n ) space-bounded one tape alternating Turing Machine ( ATM ) , and string w , we construct a GPSG G in polynomial time such that w E L ( M ) iff $ 0wllw22 ... w , ~n $ n÷l E L ( G ) . By Chandra and Stockmeyer ( 1976 ) , ASPACE ( S ( n ) ) = U DTIM~ cs ( `` ) ) c : &gt; 0 where</definiens>
				<definiens id="6">the class of problems solvable in space Sin ) on an ATM , and</definiens>
			</definition>
			<definition id="5">
				<sentence>If C is a universal configuration , then we construct an ID rule of the form c ~ Co , Cl , ... , ck ( 6 ) Otherwise , C is an existential coi~figuration and we construct the k + 1 ID rules c -- , c~ vi , 0 &lt; i &lt; k ( 7 ) A universal ATM configuration is labeled accepting if and only if it has halted and accepted , or if all of its daughters are labeled accepting. We reproduce this with the ID rules in 6 ( or 8 ) , which will be admissible only if all subtrees rooted by the RHS nodes are also admissible. An existential ATM configuration is labeled accepting if and only if it has halted and accepted , or if one of its daughters is labeled accepting. We reproduce this with the ID rules in 7 ( or 9 ) , which will be admissible only if one subtree rooted by a RHS node is admissible. All features that represent tape squares are declared to be in the HEAD feature set , and all daughter categories in the constructed ID rules are head daughters , thus ensuring that the Head Feature Convention ( HFC ) will transfer the tape contents of the mother to the daughter ( s ) , modulo the tape writing activity specified by the next move relation. Details. Le__tt Result0M ( i , a , d ) = \ [ \ [ HEAD0 i+ll , \ [ i a\ ] , \ [ A 1\ ] \ ] ifd=R \ [ \ [ HEAD0 i 1\ ] , \ [ i a\ ] , \ [ A 1\ ] \ ] if d = L ResultlM ( j , c , p , d ) = \ [ \ [ HEAD1 j+l\ ] , \ [ rf c\ ] \ [ STATE p\ ] \ ] if d= R \ [ \ [ HEAD1 jl\ ] , \ [ ri c\ ] \ [ STATE pl\ ] if d= L TransM ( q , a , b ) = ( ( p , c , dl , d2 ) : ( ( q , a , b ) , ( p ; c , dl , d2 &gt; ) e B } where a is the read-only ( R ) tape symbol currently being scanned b is the read-write ( R/W ) tape symbol currently being scanned dl is the R tape direction d2 is the R/W tape direction The GPSG G contains : 35 A category in K ° represents a node of an ATM computation tree , where the features in Atom encode the ATM configuration .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a universal configuration</definiens>
				<definiens id="1">an existential coi~figuration</definiens>
				<definiens id="2">head daughters , thus ensuring that the Head Feature Convention ( HFC ) will transfer the tape contents of the mother to the daughter ( s ) , modulo the tape writing activity specified by the next move relation. Details. Le__tt Result0M ( i , a</definiens>
				<definiens id="3">the R/W tape direction The GPSG G contains : 35 A category in K ° represents a node of an ATM computation tree</definiens>
			</definition>
			<definition id="6">
				<sentence>n3 ) where I G'I is the size of the CFG G ' and n the input string length , so a GPSG G of size m will be recognized in time O ( 3= .</sentence>
				<definiendum id="0">I G'I</definiendum>
				<definiens id="0">the size of the CFG G ' and n the input string length , so a GPSG</definiens>
			</definition>
			<definition id="7">
				<sentence>Any efficient compilation procedure must perform more than an exponential polynomial amount of work ( GPSG-Recognition takes at least Exp-Poly time ) on at least an exponential number of inputs ( all inputs that fit in the t w t space of the ATM 's read-only tape ) .</sentence>
				<definiendum id="0">GPSG-Recognition</definiendum>
			</definition>
			<definition id="8">
				<sentence>The immediately preceding section demonstrates exactly how a particular algorithm for GPSG-Recognition ( the EP argument ) comes to grief : weak context-free generative power does not ensure efficient parsability because a GPSG G is weakly equivalent to a very large CFG G ~ , and CFG size affects recognition time .</sentence>
				<definiendum id="0">EP argument )</definiendum>
				<definiens id="0">comes to grief : weak context-free generative power does not ensure efficient parsability because a GPSG G is weakly equivalent to a very large CFG G ~ , and CFG size affects recognition time</definiens>
			</definition>
			<definition id="9">
				<sentence>For example , the iterating coordination schema ( ICS ) of GPSG is an unbeatably succinct encoding of an infinite set of context-free rules ; from a computational complexity viewpoint , the ICS is utterly trivial using a slightly modified Earley algorithm .</sentence>
				<definiendum id="0">ICS</definiendum>
				<definiens id="0">an unbeatably succinct encoding of an infinite set of context-free rules ; from a computational complexity viewpoint , the ICS is utterly trivial using a slightly modified Earley algorithm</definiens>
			</definition>
			<definition id="10">
				<sentence>`` Complexity of Linguistic Models : A Computational Analysis and Reconstruction of Generalized Phrase Structure Grammar , '' S.M. Thesis , MIT Department of Electrical Engineering and Computer Science .</sentence>
				<definiendum id="0">Complexity of Linguistic Models</definiendum>
				<definiens id="0">A Computational Analysis and Reconstruction of Generalized Phrase Structure Grammar , '' S.M. Thesis , MIT Department of Electrical Engineering and Computer Science</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>Systems which do no morphology and even those which handle primarily inflectional affixation ( such as Winograd ( 1971 ) and Koskenniemi ( 1983 ) ) are limited by the fixed size of their lists of stored words .</sentence>
				<definiendum id="0">inflectional affixation</definiendum>
				<definiens id="0">fixed size of their lists of stored words</definiens>
			</definition>
			<definition id="1">
				<sentence>The retrieval mechanism contains a grammar of derivational ( and inflectional ) affixation which is used to analyse input strings in terms of the stored words .</sentence>
				<definiendum id="0">retrieval mechanism</definiendum>
				<definiens id="0">contains a grammar of derivational ( and inflectional ) affixation which is used to analyse input strings in terms of the stored words</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>They are defined as follows : CI ( toITW2 , UlTU2 ) = t01TW2UlU 2 C2 ( WlTW2 , u1Tu2 ) : t/ ) lt/ ) 2UlTU2 Since the split point is not a symbol ( which can be split either to its left or right ) but a position between strings , separate left and right wrapping operations are not needed .</sentence>
				<definiendum id="0">CI</definiendum>
				<definiendum id="1">u1Tu2 )</definiendum>
			</definition>
			<definition id="1">
				<sentence>: X. derives the strings appearing on the frontier of trees derived from the subtree rooted at r/ ; Y , derives the concatenation of the strings derived under each daughter of 7 .</sentence>
				<definiendum id="0">X.</definiendum>
				<definiens id="0">derives the strings appearing on the frontier of trees derived from the subtree rooted at r/</definiens>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>To see that beliefs are a complicating factor , consider the following sentence , usually considered to be two ways ambiguous -transparent or opaque : ( 8 ) Nadia wants a dog like Ross 's .</sentence>
				<definiendum id="0">Nadia</definiendum>
				<definiens id="0">wants a dog like Ross 's</definiens>
			</definition>
			<definition id="1">
				<sentence>a completed sentential clause is a proposition of the form ( term-list ) &lt; predication &gt; .</sentence>
				<definiendum id="0">sentential clause</definiendum>
				<definiens id="0">a proposition of the form ( term-list ) &lt; predication &gt;</definiens>
			</definition>
			<definition id="2">
				<sentence>They have the general form ( Det X. '' R ( X ) ) where Det is a quantifier corresponding to the explicit or implicit determiner of the noun phrase , X is the variable introduced , and R ( X ) indicates restrictions on X. In the examples below , we restrict ourselves to only three quantifiers -indcf , def , and label , introduced by indefinite descriptions , definite descriptions , and proper nouns respectively .</sentence>
				<definiendum id="0">Det</definiendum>
				<definiendum id="1">X</definiendum>
				<definiendum id="2">R ( X</definiendum>
				<definiens id="0">a quantifier corresponding to the explicit or implicit determiner of the noun phrase</definiens>
				<definiens id="1">the variable introduced , and</definiens>
				<definiens id="2">-indcf , def , and label , introduced by indefinite descriptions , definite descriptions , and proper nouns respectively</definiens>
			</definition>
			<definition id="3">
				<sentence>( 28 ) Nadia buys a dog like Ross 's .</sentence>
				<definiendum id="0">Nadia</definiendum>
				<definiens id="0">buys a dog like Ross 's</definiens>
			</definition>
			<definition id="4">
				<sentence>For Montague , ^x denotes an object that is intensional .</sentence>
				<definiendum id="0">^x</definiendum>
				<definiens id="0">an object that is intensional</definiens>
			</definition>
			<definition id="5">
				<sentence>where x is the variable introduced , R is the restriction on the variable , and P is the new predication on the variable .</sentence>
				<definiendum id="0">x</definiendum>
				<definiendum id="1">R</definiendum>
				<definiendum id="2">P</definiendum>
				<definiens id="0">the variable introduced</definiens>
				<definiens id="1">the restriction on the variable</definiens>
				<definiens id="2">the new predication on the variable</definiens>
			</definition>
			<definition id="6">
				<sentence>Montague combines the two approaches .</sentence>
				<definiendum id="0">Montague</definiendum>
				<definiens id="0">combines the two approaches</definiens>
			</definition>
			<definition id="7">
				<sentence>Quantified terms consist of a variable and restriction , but do not incorporate the main predication .</sentence>
				<definiendum id="0">Quantified terms</definiendum>
				<definiens id="0">consist of a variable and restriction , but do not incorporate the main predication</definiens>
			</definition>
			<definition id="8">
				<sentence>A typical substitution replaces the target descriptor , dl , with an equivalent descriptor , d2 , from the background assumptions , but otherwise preserves the form of the target sentence , i.e. , RESULT ~ TARGET \ [ dl/d2\ ] .</sentence>
				<definiendum id="0">typical substitution</definiendum>
				<definiens id="0">with an equivalent descriptor , d2 , from the background assumptions , but otherwise preserves the form of the target sentence</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>PUNDIT , written in Prolog , is a highly modular system consisting of distinct syntactic , semantic and pragmatics components .</sentence>
				<definiendum id="0">PUNDIT</definiendum>
				<definiens id="0">a highly modular system consisting of distinct syntactic , semantic and pragmatics components</definiens>
			</definition>
			<definition id="1">
				<sentence>The grammar consists of context-free BNF definitions ( currently nulnbering approximately 80 ) and associated restrictions ( approximately 35 ) .</sentence>
				<definiendum id="0">grammar</definiendum>
			</definition>
			<definition id="2">
				<sentence>For example , the tvo fragment consists of a tensed verb + object as in Replaced spindle motor .</sentence>
				<definiendum id="0">tvo fragment</definiendum>
				<definiens id="0">consists of a tensed verb + object as in Replaced spindle motor</definiens>
			</definition>
			<definition id="3">
				<sentence>Restrict : on Grammar is a descendent of Sager 's string grammar \ [ Sager1981\ ] .</sentence>
				<definiendum id="0">Restrict</definiendum>
			</definition>
			<definition id="4">
				<sentence>replace ( tinm ( Per ) ) &lt; cause ( agent ( A ) , use ( instrument ( Z ) , exchange ( object 1 ( O 1 ) , obj ect2 ( O2 ) , time ( Per ) ~ The following mapping rule specifies that the agent can be indicated by the subject of the clause .</sentence>
				<definiendum id="0">replace ( tinm</definiendum>
				<definiens id="0">time ( Per ) ~ The following mapping rule specifies that the agent can be indicated by the subject of the clause</definiens>
			</definition>
			<definition id="5">
				<sentence>haspart ( \ [ drivel\ ] , \ [ motor 1\ ] ) haspart ( \ [ systeml\ ] , \ [ drivel\ ] ) Awp is an abbreviation for an idiom specific to this domain , awaiting part .</sentence>
				<definiendum id="0">Awp</definiendum>
			</definition>
			<definition id="6">
				<sentence>\ [ eventS\ ] be ( tlme ( \ [ 11/17-12361 ) ) attribute ( theme ( \ [ drivel\ ] ) , mod ( up ) , tlme ( \ [ 11/17-123 @ \ ] ) ) The sixth sentence is another fragment consisting of a verb phrase with no subject .</sentence>
				<definiendum id="0">sixth sentence</definiendum>
				<definiens id="0">another fragment consisting of a verb phrase with no subject</definiens>
			</definition>
			<definition id="7">
				<sentence>As before , reference resolution tries to find a referent in the current FocusList which is a semantically acceptable subject given the thematic structure of the verb and the domain-specific selectional restrictions associated with them .</sentence>
				<definiendum id="0">FocusList</definiendum>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>A speech synthesizer is a machine that inputs a stream of text and outputs a speech signal .</sentence>
				<definiendum id="0">speech synthesizer</definiendum>
				<definiens id="0">a machine that inputs a stream of text and outputs a speech signal</definiens>
			</definition>
			<definition id="1">
				<sentence>The results of this statistical estimation are shown in the figure below ( where -0 denotes the null suffix ) : -ability -able -aceous -acity -acy -age -al -ality -ament -an -ance -ancy -able ( 43 % ) , -ate ( 29 % ) -0 ( 24 % ) , -ation ( 18 % ) , -ate ( 17 % ) , -e ( 14 % ) , -al ( 6 % ) , -y ( 3 % ) , -ion ( 2 % ) , -ity ( 2 % ) , -ous ( 2 % ) , -ent ( 1 % ) , -ive ( 1 % ) -0 ( 19 % ) , -e ( 7 % ) , -ate ( 7 % ) , -ation ( 4 % ) , -y ( 4 % ) , -ous ( 4 % ) , -al ( 3 % ) , -ary ( 3 % ) , -ic ( 3 % ) -acious ( 38 % ) -ate ( 42 % ) , -ation ( 18 % ) , -al ( 13 % ) , -e ( 8 % ) -0 ( 51 % ) , -y ( 13 % ) , -e ( 12 % ) , -al ( 5 % ) , -ate ( 4 % ) , -ation ( 4 % ) , -able ( 4 % ) , -on ( 4 % ) , -ion ( 3 % ) , -le ( 3 % ) , -ic ( 3 % ) , -ar ( 2 % ) , -or ( 2 % ) , -ial ( 2 % ) -0 ( 17 % ) , -e ( 7 % ) , -ic ( 2 % ) , -y ( 2 % ) , -on ( 1 % ) , -le ( 1 % ) -al ( 76 % ) , -0 ( 19 % ) , -ate ( 13 % ) , -e ( 9 % ) , -ation ( 7 % ) , -ary ( 5 % ) , -ous ( 5 % ) , -able ( 4 % ) , -ative ( 4 % ) -0 ( 38 % ) , -ate ( 29 % ) -0 ( 6 % ) , -e ( 2 % ) , -al ( 2 % ) , -ous ( 1 % ) , -y ( 1 % ) , -on ( 1 % ) , -ate ( 1 % ) , -ation ( 1 % ) -ant ( 30 % ) , -0 ( 26 % ) , -e ( 15 % ) , -ate ( 10 % ) , -able ( 9 % ) , -ation ( 9 % ) , -or ( 7 % ) , -al ( 4 % ) , -ous ( 4 % ) , -ion ( 4 % ) , -ative ( 3 % ) , -ive ( 3 % ) , -y ( 3 % ) -ant ( 40 % ) , -0 ( 19 % ) , -ation ( 12 % ) 159 -ant -ar -arity -ary -ate -ation -ational -ative '-ator -atorial -atory -ature -bility -ble -bly -e -ee -ence -ency -ent -ential -eous -ia -iac -ial -ian -iant -iary -iate -iative -ibility -ible -ic -ical -icate -ate ( 27 % ) , -ation ( 21 % ) , -0 ( 21 % ) , -e ( 11 % ) , -able -ication ( 9 % ) , -y ( 5 % ) , -al ( 5 % ) , -ous ( 5 % ) , -ion ( 4 % ) , -ent -icative ( 3 % ) , -ity ( 3 % ) , -or ( 3 % ) , -ive ( 2 % ) , -an ( 1 % ) , -ar -icatory ( 1 % ) , -ic ( 1 % ) , -ize ( 1 % ) , -on ( 1 % ) -ician -ate ( 13 % ) , -e ( 9 % ) , -ation ( 7 % ) , -0 ( 6 % ) , -ous ( 2 % ) , -y -icity ( 2 % ) , -able ( 1 % ) , -al ( 1 % ) , -ite ( 1 % ) -ar ( 63 % ) , -ate ( 26 % ) , -ation ( 22 % ) , -0 ( 13 % ) -icize -0 ( 25 % ) , -al ( 13 % ) , -ate ( 10 % ) , -e ( 8 % ) , -ation ( 8 % ) , -ar ( 6 % ) , -ous ( 4 % ) , -y ( 4 % ) , -able ( 3 % ) , -ion ( 3 % ) , -ic -ide ( 2 % ) , -ity ( 2 % ) , -ize ( 2 % ) , -ant ( 2 % ) , -or ( 2 % ) -0 ( 13 % ) , -e ( 9 % ) , -al ( 8 % ) , -ic ( 4 % ) , -y ( 3 % ) , -on ( 1 % ) , -le ( 1 % ) , -ion ( 0 % ) -ience -ate ( 42 % ) , -e ( 21 % ) , -0 ( 18 % ) , -al ( 9 % ) , -y ( 3 % ) , -ous -iency ( 3 % ) , -ion ( 1 % ) , -ic ( 1 % ) , -on ( 1 % ) -ient -ation ( 40 % ) , -e ( 25 % ) -ification -ation ( 56 % ) , -ate ( 42 % ) , -e ( 19 % ) , -0 ( 17 % ) , -able ( 17 % ) , -ant ( 12 % ) , -al ( 9 % ) , -y ( 5 % ) , -ity ( 4 % ) , -ous -ify ( 3 % ) , -ance ( 3 % ) -ate ( 61 % ) , -ation ( 48 % ) , -ant ( 18 % ) , -ative ( 18 % ) , -able ( 18 % ) , -e ( 15 % ) , -al ( 9 % ) , -0 ( 7 % ) , -ar ( 6 % ) , -ity ( 5 % ) , -ous ( 4 % ) , -ary ( 4 % ) , -on ( 4 % ) -ation ( 37 % ) , -ator ( 26 % ) , -atory ( 26 % ) -ation ( 63 % ) , -ate ( 46 % ) , -e ( 21 % ) , -ative ( 20 % ) , -ator ( 16 % ) , -able ( 15 % ) , -0 ( 13 % ) , -ant ( 11 % ) , -al ( 7 % ) , -ar ( 4 % ) -ate ( 26 % ) , -0 ( 21 % ) , -ation ( 18 % ) -ion -ional -ionary -ious -isation -ish -ist -ble ( 62 % ) , -on ( 14 % ) -on ( 5 % ) , -0 ( 3 % ) , -le ( 1 % ) -ble ( 73 % ) -0 ( 4 % ) -istic -0 ( 28 % ) , -e ( 13 % ) , -or ( 11 % ) , -y ( 6 % ) , -ation ( 6 % ) , -ment ( 5 % ) , -ate ( 5 % ) , -ant ( 3 % ) , -al ( 3 % ) , -ion ( 3 % ) , -itarian -able ( 3 % ) -ite -ent ( 54 % ) , -e ( 18 % ) , -0 ( 15 % ) , -ment ( 3 % ) -ent ( 73 % ) , -ence ( 24 % ) , -e ( 14 % ) , -0 ( 12 % ) -0 ( 6 % ) , -e ( 6 % ) , -y ( 1 % ) , -ate ( 1 % ) , -al ( 1 % ) , -ation -ity ( 1 % ) -ence ( 59 % ) , -ent ( 59 % ) , -0 ( 26 % ) , -e ( 20 % ) -ium -e ( 5 % ) , -y ( 4 % ) , -0 ( 3 % ) , -ic ( 3 % ) , -ous ( 3 % ) , -ate ( 3 % ) , -on ( 2 % ) -ic ( 14 % ) , -0 ( 7 % ) , -y ( 7 % ) , -e ( 4 % ) , -ous ( 2 % ) , -al -ival ( 1 % ) , -ate ( 1 % ) -ire -ia ( 44 % ) , -ic ( 19 % ) -0 ( 26 % ) , -y ( 15 % ) , -e ( 5 % ) , -ate ( 3 % ) , -al ( 2 % ) , -ic -ivity ( 2 % ) , -ize ( 2 % ) -0 ( 23 % ) , -y ( 14 % ) , -ic ( 7 % ) , -al ( 6 % ) , -e ( 4 % ) , -ize -ization ( 3 % ) , -ia ( 3 % ) , -ity ( 3 % ) , -ium ( 3 % ) -ize -iate ( 27 % ) -ial ( 25 % ) , -0 ( 22 % ) , -e ( 22 % ) -ial ( 13 % ) , -e ( 9 % ) , -0 ( 7 % ) , -ate ( 6 % ) , -ium ( 6 % ) , -ia ( 5 % ) , -ious ( 5 % ) -le -iate ( 70 % ) -ment -ible ( 73 % ) , -ive ( 45 % ) -ion ( 25 % ) , -ive ( 22 % ) , -0 ( 20 % ) , -e ( 12 % ) , -or ( 10 % ) , -mental -ent ( 7 % ) , -able ( 5 % ) , -ory ( 5 % ) , -enee ( 4 % ) , -al ( 4 % ) , -y ( 4 % ) -e ( 18 % ) , -y ( 14 % ) , -0 ( 12 % ) -y ( 55 % ) , -ic ( 11 % ) , -0 ( 8 % ) , -ize ( 8 % ) , -e ( 6 % ) , -ist ( 6 % ) , -al ( 2 % ) , -ate ( 2 % ) -ication ( 26 % ) , -ic ( 17 % ) , -icity ( 15 % ) , -e ( 14 % ) , -y ( 11 % ) , -0 ( 7 % ) , -ical ( 7 % ) -y ( 66 % ) , -ic ( 14 % ) , -e ( 9 % ) -ieation ( 50 % ) , -icate ( 38 % ) , -y ( 38 % ) -ication ( 50 % ) , -y ( 43 % ) , -icate ( 36 % ) -ic ( 61 % ) , -ical ( 32 % ) , -0 ( 16 % ) , -e ( 13 % ) , -y ( 13 % ) -ie ( 63 % ) , -e ( 18 % ) , -0 ( 16 % ) , -y ( 12 % ) , -ieal ( 10 % ) , -ize ( 8 % ) , -al ( 7 % ) , -ieation ( 7 % ) -ie ( 71 % ) -ate ( 8 % ) , -ic ( 8 % ) , -0 ( 7 % ) , -ite ( 6 % ) , -e ( 4 % ) , -on ( 3 % ) , -ous ( 3 % ) , -al ( 3 % ) , -ize ( 3 % ) , -age ( 2 % ) , -ium ( 2 % ) -ient ( 40 % ) -ient ( 100 % ) -e ( 11 % ) , -0 ( 10 % ) -ify ( 71 % ) , -0 ( 22 % ) , -e ( 18 % ) , -ity ( 16 % ) , -y ( 16 % ) , -ic ( 11 % ) -0 ( 25 % ) , -e ( 15 % ) , -ic ( 15 % ) , -y ( 15 % ) , -ity ( 13 % ) , -al ( 11 % ) , -ate ( 9 % ) , -ion ( 7 % ) , -ite ( 6 % ) , -ize ( 5 % ) , -or ( 5 % ) , -ar ( 4 % ) , -ary ( 4 % ) , -ical ( 4 % ) -e ( 31 % ) , -0 ( 15 % ) , -ic ( 1 % ) , -y ( 1 % ) , -al ( 1 % ) -ion ( 57 % ) , -ire ( 21 % ) , -0 ( 18 % ) , -e ( 18 % ) , -or ( 11 % ) -ion ( 87 % ) , -e ( 30 % ) , -0 ( 26 % ) , -ive ( 26 % ) -y ( 15 % ) , -ity ( 13 % ) , -ion ( 10 % ) , -0 ( 9 % ) , -e ( 9 % ) , -ial ( 6 % ) , -ium ( 5 % ) , -ie ( 4 % ) , -ate ( 3 % ) , -ive ( 3 % ) , -ist ( 2 % ) -ization ( 93 % ) , -ize ( 70 % ) , -0 ( 53 % ) , -ity ( 33 % ) , -ist ( 27 % ) , -ic ( 20 % ) , -e ( 17 % ) -0 ( 27 % ) , -e ( 11 % ) , -y ( 7 % ) , -le ( 2 % ) , -ic ( 2 % ) -0 ( 40 % ) , -ie ( 19 % ) , -ize ( 18 % ) , -y ( 18 % ) , -e ( 14 % ) , -al ( 6 % ) , -ity ( 5 % ) , -ation ( 3 % ) , -ate ( 2 % ) , -able ( 1 % ) , -ion ( 1 % ) -ist ( 46 % ) , -ize ( 29 % ) , -0 ( 27 % ) , -e ( 17 % ) , -ic ( 15 % ) , -ity ( 13 % ) , -y ( 13 % ) , -al ( 10 % ) -ity ( 57 % ) , -ize ( 43 % ) , -0 ( 36 % ) , -e ( 36 % ) -0 ( 13 % ) , -ic ( 11 % ) , -e ( 6 % ) , -ate ( 6 % ) , -ous ( 6 % ) , -y ( 2 % ) , -ia ( 2 % ) , -on ( 2 % ) , -al ( 1 % ) , -able ( 1 % ) , -ity ( 1 % ) , -ation ( 1 % ) , -ion ( 1 % ) , -or ( 1 % ) -0 ( 37 % ) , -e ( 24 % ) , -ous ( 6 % ) , -ate ( 5 % ) , -al ( 4 % ) , -ation ( 3 % ) , -y ( 2 % ) , -ion ( 1 % ) , -ic ( 1 % ) -ic ( 11 % ) , -0 ( 8 % ) , -ial ( 6 % ) , -y ( 6 % ) , -ia ( 6 % ) , -e ( 6 % ) , -ite ( 5 % ) , -ate ( 4 % ) , -ous ( 4 % ) , -al ( 2 % ) , -on ( 2 % ) , -ion ( 2 % ) , -ize ( 2 % ) , -ist ( 2 % ) -ire ( 47 % ) -ion ( 59 % ) , -e ( 26 % ) , -0 ( 22 % ) , -al ( 1 % ) , -y ( 1 % ) , -ation ( 1 % ) -ive ( 66 % ) , -ion ( 61 % ) , -0 ( 39 % ) , -or ( 32 % ) , -anee ( 14 % ) , -e ( 14 % ) , -ible ( 11 % ) -ize ( 75 % ) , -0 ( 59 % ) , -ity ( 31 % ) , -ist ( 25 % ) , -ic ( 22 % ) -0 ( 47 % ) , -ie ( 17 % ) , -ity ( 17 % ) , -y ( 14 % ) , -e ( 12 % ) , -ous ( 6 % ) , -ate ( 4 % ) , -al ( 4 % ) , -ite ( 2 % ) , -ation ( 1 % ) , -ia ( 1 % ) -0 ( 11 % ) , -y ( 3 % ) , -e ( 3 % ) , -on ( 2 % ) , -ic ( 1 % ) -0 ( 63 % ) , -able ( 6 % ) , -e ( 4 % ) , -ation ( 4 % ) , -or ( 3 % ) , -ant ( 2 % ) , -ate ( 2 % ) , -ble ( 2 % ) -ment ( 77 % ) , -0 ( 20 % ) 160 -mentary -on -or -ory -osity -OUS -ular -ularity -ure -ute -utive -y -ment ( 56 % ) -0 ( 4 % ) , -e ( 2 % ) , -ic ( 2 % ) , -y ( 1 % ) -ion ( 30 % ) , -e ( 27 % ) , -0 ( 22 % ) , -ive ( 16 % ) , -ation ( 3 % ) , -able ( 3 % ) , -y ( 2 % ) , -al ( 2 % ) , -ate ( 2 % ) , -ent ( 1 % ) , -le ( 1 % ) -ion ( 56 % ) , -e ( 34 % ) , -ive ( 21 % ) , -or ( 20 % ) , -0 ( I 1 % ) -ous ( 65 % ) , -0 ( 15 % ) , -al ( 12 % ) , -ate ( 11 % ) , -e ( 11 % ) -0 ( 13 % ) , -ic ( 7 % ) , -ate ( 6 % ) , -e ( 6 % ) , -y ( 4 % ) , -al ( 4 % ) , -on ( 2 % ) -le ( 31 % ) , -0 ( 4 % ) , -e ( 4 % ) , -ate ( 4 % ) -ular ( 67 % ) , -le ( 28 % ) -0 ( 21 % ) , -e ( 15 % ) , -ion ( 11 % ) , -or ( 8 % ) , -ire ( 4 % ) , -al ( 2 % ) -e ( 8 % ) -ute ( 67 % ) -0 ( 19 % ) , -e ( 6 % ) The decomposition program uses the table above to decide which suffixes can be truncated and when .</sentence>
				<definiendum id="0">decomposition program</definiendum>
				<definiens id="0">51 % ) , -y ( 13 % ) , -e ( 12 % ) , -al ( 5 % ) , -ate ( 4 % ) , -ation ( 4 % ) , -able</definiens>
			</definition>
			<definition id="2">
				<sentence>The resulting decomposition program has been used to construct a forest of related words as illustrated below : ( 38 port ( aport ) ( comport ( cosportmtnt ) ) ( deport ( depoEtatlon ) ( doporCee ) ( doper tment ) ) ( disport ) ( export ( exportation ) ( reexport ) ) ( import ( important ( importance ) ) ( importation ) ( relmport ) ) ( portable ) ( portage ) ( portal ) ( portative ) ( portent ( portentous ) ) ( portion ( apportion ( apportionment ) ( reapportlon ( reapportionment ) ) ) ( proportlon ( disproportlon ( disproportionate ( dlspzoportionation ) ) ( pzoportional ) ( proportionate ) ) ) ( report ( reportage ) ) ( transport ( transportation ) ) ) ( 36 infect ( affect ( affectation ) ( a£fection ( affectionate ) ) ( effective ( affeotiviCy ) ) ( disaffect ) ) ( confeet ( confection ) ( confec~ienary ) ) ( defect ( defection ) ( defeotlve ) ( effect ( effecClve ( ineffectlve ) ) ) ) ( disinfect ( disinfectant ) ) ( infection ) ( infectious ) ( infective ) ( refect ( perfect ( imperfect ( imperfection ) ( imper fective ) ) ( perfection ( perfectionist ) ) ( perfective ( perfectible ) ) ) ( prefect ( prefecture ) ) ( refection ) ( refectory ( prefectorial ) ) ) ) The forest was constructed by applying the decomposition procedure to every word in the dictionary and then indexing the results to show which forms were derived from which stems .</sentence>
				<definiendum id="0">portal )</definiendum>
				<definiendum id="1">portative )</definiendum>
				<definiendum id="2">effecClve</definiendum>
				<definiendum id="3">infective )</definiendum>
				<definiens id="0">applying the decomposition procedure to every word in the dictionary and then indexing the results to show which forms were derived from which stems</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>In the formalism there is a nominalization operator `` ' `` for reifying events and conditions , as expressed in the following axiom schema : ( ¥x ) p ( x ) ( 3e ) p ' ( e , x ) A Exist ( e ) That is , p is true of x if and only if there is a condition e of p being true of z and e exists in the real world .</sentence>
				<definiendum id="0">nominalization operator</definiendum>
				<definiendum id="1">Exist ( e</definiendum>
				<definiens id="0">a condition e of p being true of z and e exists in the real world</definiens>
			</definition>
			<definition id="1">
				<sentence>order ( &lt; , e , ) A grain ( ~ , e , ) ( Vs~ ) \ [ subscate ( ee , e , ) = subset ( sz , el ) A order ( &lt; , ez ) A grain ( ~ , sz ) \ ] An interval can be defined as a connected subseale : ( V i ) interval ( i ) ( 3 s ) ecale ( s ) A subseale ( i , e ) ^ econnected ( i ) The relations between time intervals that Allen and Kautz ( 1985 ) have defined can be defined in a straightforward manner in the approach presented here , applied to intervals in general .</sentence>
				<definiendum id="0">s ) A subseale</definiendum>
				<definiens id="0">e , ) A grain ( ~ , e , ) ( Vs~ ) \ [ subscate ( ee , e , ) = subset ( sz , el ) A order ( &lt; , ez ) A grain ( ~ , sz ) \ ] An interval can be defined as a connected subseale : ( V i ) interval ( i ) ( 3 s ) ecale (</definiens>
				<definiens id="1">a straightforward manner in the approach presented here , applied to intervals in general</definiens>
			</definition>
			<definition id="2">
				<sentence>Verticality is a concept that would be most properly analyzed in the section on space , but it is a property that many other scales have acquired metaphorically , for whatever reason .</sentence>
				<definiendum id="0">Verticality</definiendum>
				<definiens id="0">a concept that would be most properly analyzed in the section on space , but it is a property that many other scales have acquired metaphorically , for whatever reason</definiens>
			</definition>
			<definition id="3">
				<sentence>We take before to be the ordering on the time line : ( V ti , t2 ) be f ore ( t~ , tz ) ( 3 T , &lt; ) Time-line ( T ) ^ order ( &lt; , T ) Atl ET A t2ET A tl &lt; t2 We allow both instants and intervals of time. Most events occur at some instant or during some interval. In this approach , nearly every predicate takes a time argument. In the second ontology , the one that seems to be more deeply rooted in language , the world consists of a large number of more or less independent processes , or histories , or sequences of events. There is a primitive relation change between conditions. Thus , change ( el , ez ) ^ p ' ( el , x ) A q ' ( ez , x ) says that there is a change from the condition el of p being true of z to the condition e2 of q being true of x. The time line in this ontology is then an artificial construct , a regular sequence of imagined abstract events-think of them as ticks of a clock in the National Bureau of Standards -- to which other events can be related. The change ontology seems to correspond to the way we experience the world. We recognize relations of causality , change of state , and copresence among events and conditions. When events are not related in these ways , judgments of relative time must be mediated by copresence relations between the events and events on a clock and change of state relations on the clock. The predicate change possesses a limited transitivity. There has been a change from Reagan being an actor to Reagan being President , even though he was governor in between. But we probably do not want to say there has been a change from Reagan being an actor to Margaret Thatcher being Prime Minister , even though the second comes after the first. We can say that times , viewed in this ontology as events , always have a change relation between them. ( Vtl , tz ) before ( tl , tz ) D change ( tl , t2 ) The predicate change is related to before by the axiom ( Vel , ez ) change ( el , e2 ) D ( 3 tl , tz ) at ( el , t~ ) A at ( e2 , t2 ) A before ( q , t2 ) This does not allow us to derive change of state from temporal succession. For this , we need axioms of the form ( Vet , e : , t , , t2 , z ) p ' ( el , z ) ^ at ( e , , t , ) ^q ' ( e2 , x ) A at ( ez , tz ) ^ before ( q , tz ) D change ( el , ez ) That is , if z is p at time tl and q at a later time t2 , then there has been a change of state from one to the other. Time arguments in predications can be viewed as abbreviations : ( Vx , t ) p ( z , t ) = ( qe ) p ' ( e , x ) ^ at ( e , t ) 234 The word `` move '' , or the predicate move , ( as in `` x moves from y to z ' ) can then be defined equivalently in terms of change ( Vx , y , z ) move ( x , y , z ) ( 3 el , e2 ) change ( el , e2 ) A at ' ( e , , z , y ) A at ' ( e2 , x , z ) or in terms of the time line ( V x , y , z ) move ( x , y , z ) =- ( 3 tl , t2 ) at ( x , y , tl ) A at ( x , z , 12 ) A before ( ti , t2 ) In English and apparently all other natural languages , both ontologies are represented in the lexicon. The time line ontology is found in clock and calendar terms , tense systems of verbs , and in the deictic temporal locatives such as `` yesterday '' , `` today '' , `` tomorrow '' , `` last night '' , and so on. The change ontology is exhibited in most verbs , and in temporal clausal connectives. The universal presence of both classes of lexical items and grammatical markers in natural languages requires a theory which can accommodate both ontologies , illustrating the importance of methodological principle 4. Among temporal connectives , the word `` while '' presents interesting problems. In `` el while e~ ' , e2 must be an event occurring over a time interval ; el must be an event and may occur either at a point or over an interval. One 's first guess is that the point or interval for el must be included in the interval for e2. However , there are cases , such as or It rained while I was in Philadelphia. The electricity should be off while the switch is being repaired. which suggest the reading `` ez is included in el '' . We came to the conclusion that one can infer no more than that el and ez overlap , and any tighter constraints result from implicatures from background knowledge. The word `` immediately '' also presents a number of problems. It requires its argument e to be an ordering relation between two entities x and y on some scale s. immediate ( e ) : ( 3 x , y , s ) less-than ' ( e , x , y , s ) It is not clear what the constraints on the scale are. Temporal and spatial scales are okay , as in `` immediately after the alarm '' and `` immediately to the left '' , but the size scale is n't : * John is immediately larger than Bill. Etymologically , it means that there are no intermediate entities between x and y on s. Thus , ( V e , x , y , s ) immediate ( e ) A less-than ' ( e , x , y , s ) D -. ( 3 z ) less-than ( x , z , s ) A less-than ( z , y , s ) \ [ 5 A/.Figure 1 : The simplest space. However , this will only work if we restrict z to be a relevant entity. For example , in the sentence We disengaged the compressor immediately after the alarm. the implication is that no event that could damage the compressor occurred between the alarm and the disengagement , since the text is about equipment failure. Structure The notion of dimension has been made precise in linear algebra. Since the concept of a region is used metaphorically as well as in the spatial sense , however , we were concerned to determine the minimal structure that a system requires for it to make sense to call it a space of more than one dimension. For a two-dimensional space , l~re must be a scale , or partial ordering , for each dimension. Moreover , the two scales must be independent , in that the order of elements on one scale can not be determined from their order on the other. Formally , ( Vsp ) spaee ( sp ) =- ( 3 sl , s2 , &lt; 1 , &lt; 2 ) scalel ( sl , sp ) A scalez ( s2 , sp ) ^ order ( &lt; 1 , sl ) h order ( &lt; 2 , sz ) A ( 3z ) ( 3y , ) ( z &lt; , y , A z &lt; 2 Y , ) A ( 3 ~ ) ( z &lt; , y~ A y~ &lt; 2 z ) Note that this does not allow &lt; 2 to be simply the reverse of &lt; 1. An unsurprising consequence of this definition is that the minimal example of a two-dimensional space consists of three points { three points determine a plane ) , e.g. , the points A , B , and C , where A &lt; IB , A &lt; IC , C &lt; 2A , A &lt; 2B. This is illustrated in Figure 1. The dimensional scales are apparently found in all natural languages in relevant domains. The familiar threedimensional space of common sense is defined by the three scale pairs `` up-down '' , `` front-back '' , and `` left-right '' ; the two-dimensional plane of the commonsense conception of the earth 's surface is represented by the two scale pairs `` north-south '' and `` east-west '' . 235 The simplest , although not the only , way to define adjacency in the space is as adjacency on both scales : ( Vz , y , sp ) adi ( z , y , sp ) = ( 3 s~ , s2 ) scalel ( sl , sp ) A scale2 ( s~ , sp ) Aadj ( x , y , sl ) A adj ( x , y , s2 ) A region is a subset of a space. The surface and interior of a region can be defined in terms of adjacency , in a manner paralleling the definition of a boundary in point-set topology. In the following , s is the boundary or surface of a twoor three-dimensional region r embedded in a space sp. ( Vs , r ) surf ace ( s , r , sp ) =__ ( Vz ) z~r~\ [ zes = ( Ey ) ( y e sp A -~ ( y e r ) ^ adi ( z , y , sp ) ) \ ] Finally , we can define the notion of `` contact '' in terms of points in different regions being adjacent. ( Vrl , r~ , sp ) contact ( rl , r2 , sp ) disjoint ( rl , r2 ) A ( Ez , y ) ( z e r , Aye r2 A adj ( z , y , sp ) ) By picking the scales and defining adjacency right , we can talk about points of contact between communicational networks , systems of knowledge , and other metaphorical domains. By picking the scales to be the real line and defining adjacency in terms of e-neighborhoods , we get Euclidean space and can talk about contact between physical objects. Physical objects and materials must be distinguished , just as they are apparently distinguished in every natural language , by means of the count noun mass noun distinction. A physical object is not a bit of material , but rather is comprised of a bit of material at any given time. Thus , rivers and human bodies are physical objects , even though their material constitution changes over time. This distinction also allows us to talk about an object losing material through wear and still being the same object. We will say that an entity b is a bit of material by means of the expression material ( b ) . Bits of material are characterized by both extension and cohesion. The primitive predication occupies ( b , r , t } encodes extension , saying that a bit of material b occupies a region r at time t. The topology of a bit of material is then parasitic on the topology of the region it occupies. A part bl of a bit of material b is a bit of material whose occupied region is always a subregion of the region occupied by b. Point-like particles ( particle } are defined in terms of points in the occupied region , disjoint bits { disjointbit ) in terms of disjointness of regions , and contact between bits in terms of contact between their regions. We can then state as follows the Principle of NonJoint-Occupancy that two bits of material can not occupy the same place at the same time : ( Vb~ , b2 ) ( disjointbit ( b~ , bz ) D ( Vx , y , bs , b4 ) interior ( bs , b~ ) A interior ( b4 , bz ) ^ particle ( z , bs ) A particle ( y , b4 ) D ~ ( Ez ) ( at ( z , z ) ^ at ( y , z ) ) At some future point in our work , this may emerge as a consequence of a richer theory of cohesion and force. The cohesion of materials is also a primitive property , for we must distinguish between a bump on the surface of an object and a chip merely lying on the surface. Cohesion depends on a primitive relation bond between particles of material , paralleling the role of adj in regions. The relation attached is defined as the transitive closure of bond. A topology of cohesion is built up in a manner analogous to the topology of regions. In addition , we have encoded the relation that bond bears to motion , i.e. that bonded bits remain adjacent and that one moves when the other does , and the relation of bond to force , i.e. that there is a characteristic force that breaks a bond in a given material. Different materials react in different ways to forces of various strengths. Materials subjected to force exhibit or fail to exhibit several invariance properties , proposed by linger ( 1985 ) . If the material is shape-invariant with respect to a particular force , its shape remains the same. If it is topologically invariant , particles that are adjacent remain adjacent. Shape invariance implies topological invariance. Subject to forces of a certain strength or degree dl , a material ceases being shape-invariant. At a force of strength dz _ &gt; dl , it ceases being topologically invariant , and at a force of strength ds &gt; _ dz , it simply breaks .</sentence>
				<definiendum id="0">s2 ) A region</definiendum>
				<definiens id="0">the ordering on the time line : ( V ti , t2 ) be f ore ( t~ , tz ) ( 3 T , &lt; ) Time-line ( T ) ^ order ( &lt; , T ) Atl ET A t2ET A tl &lt; t2 We allow both instants and intervals of time. Most events occur at some instant or during some interval. In this approach , nearly every predicate takes a time argument. In the second ontology , the one that seems to be more deeply rooted in language , the world consists of a large number of more or less independent processes , or histories , or sequences of events. There is a primitive relation change between conditions. Thus , change ( el , ez ) ^ p ' ( el , x ) A q ' ( ez , x ) says that there is a change from the condition el of p being true of z to the condition e2 of q being true of x. The time line in this ontology is then an artificial construct , a regular sequence of imagined abstract events-think of them as ticks of a clock in the National Bureau of Standards -- to which other events can be related. The change ontology seems to correspond to the way we experience the world. We recognize relations of causality , change of state , and copresence among events and conditions. When events are not related in these ways , judgments of relative time must be mediated by copresence relations between the events and events on a clock and change of state relations on the clock. The predicate change possesses a limited transitivity. There has been a change from Reagan being an actor to Reagan being President , even though he was governor in between. But we probably do not want to say there has been a change from Reagan being an actor to Margaret Thatcher being Prime Minister , even though the second comes after the first. We can say that times , viewed in this ontology as events , always have a change relation between them. ( Vtl , tz ) before ( tl , tz ) D change ( tl , t2 ) The predicate change is related to before by the axiom ( Vel , ez ) change ( el , e2 ) D ( 3 tl , tz ) at ( el , t~ ) A at ( e2 , t2 ) A before ( q , t2 ) This does not allow us to derive change of state from temporal succession. For this , we need axioms of the form ( Vet , e : , t , , t2 , z ) p ' ( el , z ) ^ at ( e , , t , ) ^q ' ( e2 , x ) A at ( ez , tz ) ^ before ( q , tz ) D change ( el , ez ) That is , if z is p at time tl and q at a later time t2 , then there has been a change of state from one to the other. Time arguments in predications can be viewed as abbreviations : ( Vx , t ) p ( z , t ) = ( qe ) p ' ( e , x ) ^ at ( e , t ) 234 The word `` move '' , or the predicate move , ( as in `` x moves from y to z ' ) can then be defined equivalently in terms of change ( Vx , y , z ) move ( x , y , z ) ( 3 el , e2 ) change ( el , e2 ) A at ' ( e , , z , y ) A at ' ( e2 , x , z ) or in terms of the time line ( V x , y , z ) move ( x , y , z ) =- ( 3 tl , t2 ) at ( x , y , tl ) A at ( x , z , 12 ) A before ( ti , t2 ) In English and apparently all other natural languages , both ontologies are represented in the lexicon. The time line ontology is found in clock and calendar terms , tense systems of verbs , and in the deictic temporal locatives such as `` yesterday '' , `` today '' , `` tomorrow '' , `` last night '' , and so on. The change ontology is exhibited in most verbs , and in temporal clausal connectives. The universal presence of both classes of lexical items and grammatical markers in natural languages requires a theory which can accommodate both ontologies , illustrating the importance of methodological principle 4. Among temporal connectives , the word `` while '' presents interesting problems. In `` el while e~ ' , e2 must be an event occurring over a time interval ; el must be an event and may occur either at a point or over an interval. One 's first guess is that the point or interval for el must be included in the interval for e2. However , there are cases , such as or It rained while I was in Philadelphia. The electricity should be off while the switch is being repaired. which suggest the reading `` ez is included in el '' . We came to the conclusion that one can infer no more than that el and ez overlap , and any tighter constraints result from implicatures from background knowledge. The word `` immediately '' also presents a number of problems. It requires its argument e to be an ordering relation between two entities x and y on some scale s. immediate ( e ) : ( 3 x , y , s ) less-than ' ( e , x , y , s ) It is not clear what the constraints on the scale are. Temporal and spatial scales are okay , as in `` immediately after the alarm '' and `` immediately to the left '' , but the size scale is n't : * John is immediately larger than Bill. Etymologically , it means that there are no intermediate entities between x and y on s. Thus , ( V e , x , y , s ) immediate ( e ) A less-than ' ( e , x , y , s ) D -. ( 3 z ) less-than ( x , z , s ) A less-than ( z , y , s ) \ [ 5 A/.Figure 1 : The simplest space. However , this will only work if we restrict z to be a relevant entity. For example , in the sentence We disengaged the compressor immediately after the alarm. the implication is that no event that could damage the compressor occurred between the alarm and the disengagement , since the text is about equipment failure. Structure The notion of dimension has been made precise in linear algebra. Since the concept of a region is used metaphorically as well as in the spatial sense , however , we were concerned to determine the minimal structure that a system requires for it to make sense to call it a space of more than one dimension. For a two-dimensional space , l~re must be a scale , or partial ordering , for each dimension. Moreover , the two scales must be independent , in that the order of elements on one scale can not be determined from their order on the other. Formally , ( Vsp ) spaee ( sp ) =- ( 3 sl , s2 , &lt; 1 , &lt; 2 ) scalel ( sl , sp ) A scalez ( s2 , sp ) ^ order ( &lt; 1 , sl ) h order ( &lt; 2 , sz ) A ( 3z ) ( 3y , ) ( z &lt; , y , A z &lt; 2 Y , ) A ( 3 ~ ) ( z &lt; , y~ A y~ &lt; 2 z ) Note that this does not allow &lt; 2 to be simply the reverse of &lt; 1. An unsurprising consequence of this definition is that the minimal example of a two-dimensional space consists of three points { three points determine a plane ) , e.g. , the points A , B , and C , where A &lt; IB , A &lt; IC , C &lt; 2A , A &lt; 2B. This is illustrated in Figure 1. The dimensional scales are apparently found in all natural languages in relevant domains. The familiar threedimensional space of common sense is defined by the three scale pairs `` up-down '' , `` front-back '' , and `` left-right '' ; the two-dimensional plane of the commonsense conception of the earth 's surface is represented by the two scale pairs `` north-south '' and `` east-west '' . 235 The simplest , although not the only , way to define adjacency in the space is as adjacency on both scales : ( Vz , y , sp ) adi ( z , y , sp ) = ( 3 s~ , s2 ) scalel ( sl , sp ) A scale2 ( s~ , sp ) Aadj ( x , y , sl ) A adj ( x , y ,</definiens>
			</definition>
			<definition id="4">
				<sentence>invariant ( o ) ( re , m , o , bo ) abr-event ' ( e , m , o , bo ) = -- ( 3 t , b , s , bo , el , e , , es ) at ( e , t ) ^ consists-of ( o , b , t ) ^ surface ( s , b ) ^ particle ( bo , s ) ^ change ' ( e , el , e~ ) ^ attached ' ( el , bo , b ) ^ not ' ( e2 , el ) A cause ( es , e ) ^ hit ' ( es , m , bo ) After the abrasive event , the pointlike bit b0 is no longer a part of the object o : 237 ( re , m , o , bo , el , e2 , t2 ) abr-event ' ( e , m , o , b0 ) A change ' ( e , el , ez ) ^ attaehed ' ( el , bo , b ) ^ not ' ( e2 , el ) A at ( ez , tz ) A consists-of ( o , bz , tz ) D -~part ( bo , bz ) It is necessary to state this explicitly since objects and bits of material can be discontinuous .</sentence>
				<definiendum id="0">invariant ( o )</definiendum>
				<definiens id="0">s , bo , el , e , , es ) at ( e , t ) ^ consists-of ( o , b , t ) ^ surface ( s , b ) ^ particle ( bo , s ) ^ change ' ( e , el , e~ ) ^ attached '</definiens>
				<definiens id="1">no longer a part of the object o : 237 ( re , m , o , bo , el , e2 , t2 ) abr-event ' ( e , m , o</definiens>
			</definition>
			<definition id="5">
				<sentence>An abrasion is a large number of abrasive events widely distributed through some nonpointlike region on the surface of an object : ( Ve , m , o } abrade ' ( e , m , o ) ( : lbs ) \ [ ( ¥e , ) \ [ e , e e : : ) ( 3 bo ) bo e bs ^ abr-evenr ( el , m , o , bo ) \ ] ^ ( Vb , s , t ) \ [ at ( e , t ) ^ consists-of ( o , b , t ) A surface ( s , b ) D ( B r ) subregion ( r , s ) A widely-distributed ( bs , r ) \ ] \ ] Wear can occur by means of a large collection of abrasive events distributed over time as well as space ( so that there may be no time at which enough abrasive events occur to count as an abrasion ) .</sentence>
				<definiendum id="0">b ) D ( B r ) subregion</definiendum>
				<definiendum id="1">widely-distributed</definiendum>
				<definiens id="0">a large number of abrasive events widely distributed through some nonpointlike region on the surface of an object : ( Ve , m , o } abrade '</definiens>
				<definiens id="1">a large collection of abrasive events distributed over time as well as space ( so that there may be no time at which enough abrasive events occur to count as an abrasion )</definiens>
			</definition>
			<definition id="6">
				<sentence>If z is distributed in y , then y is a system and z is a set of entities which are located at components of y. For the distribution to be wide , most of the elements of a partition of y determined independently of the distribution must contain components which have elements of x at them .</sentence>
				<definiendum id="0">y</definiendum>
				<definiendum id="1">z</definiendum>
				<definiens id="0">elements of x at them</definiens>
			</definition>
			<definition id="7">
				<sentence>A norm is a pattern which is established either by conventional stipulation or by statistical regularity .</sentence>
				<definiendum id="0">norm</definiendum>
				<definiens id="0">a pattern which is established either by conventional stipulation or by statistical regularity</definiens>
			</definition>
</paper>

		<paper id="1039">
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>Wore is a verb and so the Verb rules are tried .</sentence>
				<definiendum id="0">Wore</definiendum>
				<definiens id="0">a verb and so the Verb rules are tried</definiens>
			</definition>
			<definition id="1">
				<sentence>Also like theirs , NEXUS solves the third problem by means of special rules which detect gaps in conjuncts and which fill those gaps by copying constituents from the other conjunct .</sentence>
				<definiendum id="0">NEXUS</definiendum>
				<definiens id="0">solves the third problem by means of special rules which detect gaps in conjuncts and which fill those gaps by copying constituents from the other conjunct</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>Using the Text to Speech system ( TTS ) i17\ ] , we have been able , by systematic variation of pitch llngnlstlc structure , which is the text/speech itself ; an attentlonal structure , including information about the relative salience of objects , properties , relations , and intentions at a given point in the discourse ; and an Intentional structure , which relates dlscourse segment purposes ( those purposes whose recognition is essential to a segment achieving its intended effect ) to one another .</sentence>
				<definiendum id="0">Speech system ( TTS</definiendum>
				<definiens id="0">the relative salience of objects , properties , relations , and intentions at a given point in the discourse</definiens>
			</definition>
			<definition id="1">
				<sentence>TNT was designed to teach computer-naive subjects vi , a simple UNIX screen-oriented text editor .</sentence>
				<definiendum id="0">TNT</definiendum>
				<definiens id="0">a simple UNIX screen-oriented text editor</definiens>
			</definition>
			<definition id="2">
				<sentence>Pitch accents , which fall on the stressed syllable of lexical items , mark those items as intonationally prominent .</sentence>
				<definiendum id="0">Pitch accents</definiendum>
				<definiens id="0">fall on the stressed syllable of lexical items , mark those items as intonationally prominent</definiens>
			</definition>
			<definition id="3">
				<sentence>Remind , tells you again what to do If you forget .</sentence>
				<definiendum id="0">Remind</definiendum>
				<definiens id="0">tells you again what to do If you forget</definiens>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>The symbols II and Y : , are used in conjunction with the abstraction operation to represent universal and existential quantification : Vx P is an abbreviation for H ( Ax P ) and 3x P is an abbreviation for G ( Ax P ) .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">used in conjunction with the abstraction operation to represent universal and existential quantification : Vx P is an abbreviation for H</definiens>
			</definition>
			<definition id="1">
				<sentence>The predicate name_age whose definition is obtained by dropping the quantifier from the predicate term defines a different property ; ( same_age L K ) is true only when the objects in K have the same age .</sentence>
				<definiendum id="0">same_age L K</definiendum>
				<definiens id="0">true only when the objects in K have the same age</definiens>
			</definition>
			<definition id="2">
				<sentence>We have been able to show ( Theorem 1 \ [ 9\ ] ) that any proof in T of a goal formula from a set of definite clauses which uses a predicate term containing the logical connectives ~ , D , or V , can be converted into another proof in which only predicate terms from ~/+ are used .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">uses a predicate term containing the logical connectives ~ , D , or</definiens>
			</definition>
			<definition id="3">
				<sentence>subsume ( X\ ( all Y\ ( recipient X Y = &gt; crew Y ) ) ) message .</sentence>
				<definiendum id="0">subsume</definiendum>
				<definiendum id="1">X\</definiendum>
				<definiens id="0">( all Y\ ( recipient X Y = &gt; crew Y ) ) ) message</definiens>
			</definition>
			<definition id="4">
				<sentence>If a and B are two terms that represent concepts , then rather elementary proof-theoretic arguments may be employed to show that ( subsumes A B ) is provable from the above clauses if and only if the first-order term ( all X\ ( B X = &gt; A X ) ) is logically entailed by the primitive subsumption relations .</sentence>
				<definiendum id="0">B )</definiendum>
				<definiens id="0">all X\ ( B X = &gt; A X ) ) is logically entailed by the primitive subsumption relations</definiens>
			</definition>
			<definition id="5">
				<sentence>A term of the form ( iota Q ) represents a first-order individual ( i.e. some object ) , but it does so by carrying with it a description of that object ( the concept Q ) .</sentence>
				<definiendum id="0">term of the form</definiendum>
				<definiendum id="1">iota Q )</definiendum>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>Temporal adverbs ( such as tomorrow or now ) add additional information about the events in a sentence .</sentence>
				<definiendum id="0">Temporal adverbs</definiendum>
				<definiens id="0">such as tomorrow or now ) add additional information about the events in a sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>A parser uses the semantic rules of tense as follows .</sentence>
				<definiendum id="0">parser</definiendum>
			</definition>
			<definition id="2">
				<sentence>( CHECK ( and ( &lt; ( begin bel ) ( end now5 ) ) ( &gt; ( end bel ) ( begin now5 ) ) ) ) Step 3 : If the overlap check of the anchor returns true , then do overlap checks on the remaining events .</sentence>
				<definiendum id="0">CHECK</definiendum>
				<definiens id="0">If the overlap check of the anchor returns true , then do overlap checks on the remaining events</definiens>
			</definition>
			<definition id="3">
				<sentence>In this case assert : ( or ( and ( &lt; ( begin bel ) ( end nowS ) ) ( end bel ) ( begin nowS ) ) ) ( and ( &lt; ( begin run4 ) ( end now5 ) ) ( &gt; ( end run4 ) ( begin now5 ) ) ) ) An example of a sentence in which the anchor event and the adverb can not overlap is * '' He ran tomorrow . ''</sentence>
				<definiendum id="0">begin bel ) ( end nowS</definiendum>
				<definiens id="0">begin now5 ) ) ) ) An example of a sentence in which the anchor event</definiens>
			</definition>
</paper>

		<paper id="1041">
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>Dictionary lookup is a computational activity that can be greatly accelerated when performed on large amounts of text by a parallel computer such as the Connection Machine TM Computer ( CM ) .</sentence>
				<definiendum id="0">Dictionary lookup</definiendum>
			</definition>
			<definition id="1">
				<sentence>The CM consists of a large number number of processor/memory cells .</sentence>
				<definiendum id="0">CM</definiendum>
			</definition>
			<definition id="2">
				<sentence>In effect , communication is the parallel analog of the pointer-following executed by a serial computer as it traverses the links of a data structure or graph .</sentence>
				<definiendum id="0">communication</definiendum>
				<definiens id="0">the parallel analog of the pointer-following executed by a serial computer as it traverses the links of a data structure or graph</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>As an example of the `` internal representation '' ( IR ) of an input , which results from a recursive traversal of a completed parse tree , and which illustrates preparations for compositional analysis , the ( artificially complex ) input `` Which Mexican restaurants in the largest city other than New Providence that are not expensive are open for lunch ' ? ''</sentence>
				<definiendum id="0">IR</definiendum>
				<definiens id="0">results from a recursive traversal of a completed parse tree</definiens>
			</definition>
			<definition id="1">
				<sentence>SIUDEMT ( BILL , DOUG , ... ) = \ [ \ ] SSM ( iii-22-3333 , 12:3-4J-6789 , ... ) \ [ \ ] CLASS ( i , 2 ... . ) \ [ \ ] RDVIS ( BACHEMK0 , BALLARD , ... ) \ [ \ ] Abort \ [ \ ] Return \ [ \ ] l Uords associated with the CLASS value 1 : fre~hmar , I Uords associated with the CLASS value G : 9raduatel Modif'iers in CLASS Ad , iectlve Mounmod Houn FRESHMRH ( i ) \ [ \ ] \ [ \ ] \ [ \ ] SOPHOMORE ( 2 ) \ [ \ ] \ [ \ ] \ [ \ ] JUMIOR ( 3 ) \ [ \ ] \ [ \ ] \ [ \ ] SEMIOR ( 4 ) 0 \ [ \ ] \ [ \ ] GRADUATE ( g ) \ [ \ ] \ [ \ ] \ [ \ ] Return \ [ \ ] Figure 3 : \ [ l ) termediate Acquisitions Semantic Specification Adjective : FILE is LARGE \ [ Sample Usa .</sentence>
				<definiendum id="0">SIUDEMT</definiendum>
				<definiendum id="1">SEMIOR</definiendum>
			</definition>
			<definition id="2">
				<sentence>The current internal representation of a menu specification is a triple of the form suggested by 24 Which relation gives tile meanin ( j of HEIGHT of MOUNTAIN HOUNT , qlNS : N , ql , iE , ELEL , , ' , qTION , PIAP- '' ~-~ C , qI,1PSITES : SITE , C , qP , qCITY , TYPE ... ... ... ... ... ... ... ... ... ... ... ...</sentence>
				<definiendum id="0">qP</definiendum>
				<definiens id="0">a triple of the form suggested by 24 Which relation gives tile meanin ( j of HEIGHT of MOUNTAIN HOUNT , qlNS : N , ql , iE , ELEL , , ' , qTION , PIAP- '' ~-~ C , qI,1PSITES : SITE , C ,</definiens>
			</definition>
			<definition id="3">
				<sentence>In contrast , TELI provides several customization modes , as described in Section 3 , and discourages low-level database specifications .</sentence>
				<definiendum id="0">TELI</definiendum>
				<definiens id="0">provides several customization modes , as described in Section 3 , and discourages low-level database specifications</definiens>
			</definition>
			<definition id="4">
				<sentence>but IRACQ allows proper nouns as well as common nouns to be used .</sentence>
				<definiendum id="0">IRACQ</definiendum>
				<definiens id="0">allows proper nouns as well as common nouns to be used</definiens>
			</definition>
			<definition id="5">
				<sentence>Since IRUS provides for quite general taxonomic relationships among defined concepts ( e.g. nouns ) , IRACQ proceeds to ascertain which of the possibly several classes that `` Jones '' belongs to is the most general one that can act as the subject of `` write '' .</sentence>
				<definiendum id="0">IRUS</definiendum>
				<definiens id="0">provides for quite general taxonomic relationships among defined concepts ( e.g. nouns ) , IRACQ proceeds to ascertain which of the possibly several classes that `` Jones '' belongs to is the most general one that can act as the subject of `` write ''</definiens>
			</definition>
			<definition id="6">
				<sentence>The primary responsibility of the customization module is to acquire information that relates language concepts , e.g. subject of a given verb , to the columns of the database at hand .</sentence>
				<definiendum id="0">customization module</definiendum>
				<definiens id="0">to acquire information that relates language concepts , e.g. subject of a given verb</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>The ROMPER system functions as a part of a naturallanguage interface to a database or expert system .</sentence>
				<definiendum id="0">ROMPER system functions</definiendum>
				<definiens id="0">a part of a naturallanguage interface to a database or expert system</definiens>
			</definition>
			<definition id="1">
				<sentence>The system knowledge base includes an object taxonomy and knowledge about object attributes and their possible values .</sentence>
				<definiendum id="0">system knowledge base</definiendum>
				<definiens id="0">includes an object taxonomy and knowledge about object attributes and their possible values</definiens>
			</definition>
			<definition id="2">
				<sentence>ROMPER instantiates the schema corresponding to R3 when neither of the two above mentioned knowledge base configurations can be found in the user model .</sentence>
				<definiendum id="0">ROMPER</definiendum>
				<definiens id="0">instantiates the schema corresponding to R3 when neither of the two above mentioned knowledge base configurations can be found in the user model</definiens>
			</definition>
			<definition id="3">
				<sentence>The similarity metric used by ROMPER is one that is based on the objects ' common and disjoint attributes which takes attribute salience into account \ [ Tve77\ ] .</sentence>
				<definiendum id="0">ROMPER</definiendum>
			</definition>
			<definition id="4">
				<sentence>Second , ROMPER uses the highlighting from object perspective to instantiate the selected response schema .</sentence>
				<definiendum id="0">ROMPER</definiendum>
				<definiens id="0">uses the highlighting from object perspective to instantiate the selected response schema</definiens>
			</definition>
			<definition id="5">
				<sentence>Suppose we have two objects a and b where A is the set of properties associated with object a and B is the set of properties associated with object b. Tversky 's measure can be expressed as : s ( a , b ) = Of ( AN B ) ~f ( A B ) fir ( BA ) for some 0 , o~ , and fl &gt; 0 .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">the set of properties associated with object a and</definiens>
				<definiens id="1">the set of properties associated with object b. Tversky 's measure can be expressed as : s ( a , b ) = Of ( AN B ) ~f ( A B ) fir ( BA ) for some 0 , o~ , and fl &gt; 0</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>More specifically , CORRECT-PLAN takes a pre-existing plan having subparts that do not interact as expected during execution , and debugs the plan by adding a new goal to restore the expected interactions .</sentence>
				<definiendum id="0">CORRECT-PLAN</definiendum>
				<definiens id="0">takes a pre-existing plan having subparts that do not interact as expected during execution</definiens>
			</definition>
			<definition id="1">
				<sentence>MODIFIES ( action2 , actionl ) means that action2 is a variant of action1 , for example , the same action with different parameters or a new action achieving the still required effects .</sentence>
				<definiendum id="0">MODIFIES</definiendum>
				<definiens id="0">a variant of action1 , for example , the same action with different parameters or a new action achieving the still required effects</definiens>
			</definition>
			<definition id="2">
				<sentence>ENABLES ( action1 , action2 ) means that false prerequisites of action2 are in the effects of action1 .</sentence>
				<definiendum id="0">ENABLES</definiendum>
				<definiendum id="1">action2 )</definiendum>
				<definiens id="0">means that false prerequisites of action2 are in the effects of action1</definiens>
			</definition>
			<definition id="3">
				<sentence>CORRECT-PLAN is an example of a topic interruption that relates to a previous topic , To illustrate how these discourse plans represent the relationships between utterances , consider a naturally-occurring protocol ( Sidner \ [ 22\ ] ) in which a user interacts with a person simulating an editing system to manipulate network structures in a knowledge representation language : 1 ) User : Hi .</sentence>
				<definiendum id="0">CORRECT-PLAN</definiendum>
			</definition>
			<definition id="4">
				<sentence>Imperatives indicate REQUESTS and the propositional content ( e.g. DISPLAY ) is determined via the standard syntactic and semantic analysis of most parsers .</sentence>
				<definiendum id="0">Imperatives</definiendum>
				<definiens id="0">indicate REQUESTS and the propositional content ( e.g. DISPLAY ) is determined via the standard syntactic and semantic analysis of most parsers</definiens>
			</definition>
			<definition id="5">
				<sentence>Using the knowledge that `` no '' typically does not signal a topic continuation , the plan recognizer first modifies its default mode of processing , i.e. the assumption that the REQUEST is a CONTINUE-PLAN ( preference 1 ) is overruled .</sentence>
				<definiendum id="0">REQUEST</definiendum>
				<definiens id="0">a CONTINUE-PLAN ( preference 1 ) is overruled</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>The classificatory capacity is the set of languages generated by the various subgrammars of a grammar , and if we are willing to assume that linguists can tell which sentences in a language exemplify the same or different syntactic patterns , then we can usually simply demonstrate that , e.g. , no CFG can have a subgrammar generating all and only the sentences of some particular construction if that construction involves reduplication .</sentence>
				<definiendum id="0">classificatory capacity</definiendum>
				<definiens id="0">sentences in a language exemplify the same or different syntactic patterns , then we can usually simply demonstrate that , e.g. , no CFG can have a subgrammar generating all and only the sentences of some particular construction if that construction involves reduplication</definiens>
			</definition>
			<definition id="1">
				<sentence>Rounds , Manaster-Ramer , and Friedman ( to appear ) argue that facts like this mean that a natural language should not be modeled as a formal language but rather as a family of languages , each of which may be taken as an approximation to an ideal language .</sentence>
				<definiendum id="0">Friedman</definiendum>
				<definiens id="0">a formal language but rather as a family of languages</definiens>
			</definition>
			<definition id="2">
				<sentence>Formal language theory deals with sets of strings over well-defined finite vocabularies ( also often called alphabets ) such as the hackneyed { a , b } .</sentence>
				<definiendum id="0">Formal language theory</definiendum>
				<definiens id="0">deals with sets of strings over well-defined finite vocabularies ( also often called alphabets ) such as the hackneyed { a , b }</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>Support comes from the frequency with which that pattern alternates adjectives , which are normally stative , with the past participle .</sentence>
				<definiendum id="0">Support</definiendum>
				<definiens id="0">comes from the frequency with which that pattern alternates adjectives , which are normally stative , with the past participle</definiens>
			</definition>
			<definition id="1">
				<sentence>For instance , we can say either that the phrase `` big dog '' denotes a particular kind of ( the more general term ) `` dog '' ; or that it denotes a dog with the additional feature of `` bigness '' .</sentence>
				<definiendum id="0">big dog ''</definiendum>
				<definiens id="0">the more general term ) `` dog '' ; or that it denotes a dog with the additional feature of `` bigness ''</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>Grosz defines a focus space as that subset of the participant 's total knowledge that is in the focus of attention and that is relevant to process a discourse segment .</sentence>
				<definiendum id="0">Grosz</definiendum>
			</definition>
			<definition id="1">
				<sentence>The task of which the completed subtask is a part then returns in focus .</sentence>
				<definiendum id="0">subtask</definiendum>
				<definiens id="0">a part then returns in focus</definiens>
			</definition>
			<definition id="2">
				<sentence>A task analysis is a detailed description of the determinants of the user 's behaviors arising from the task context .</sentence>
				<definiendum id="0">task analysis</definiendum>
				<definiens id="0">a detailed description of the determinants of the user 's behaviors arising from the task context</definiens>
			</definition>
			<definition id="3">
				<sentence>Another consequence of the meta-plan ASK-ADVISERHELP is the presence of acknowledgement subdlalogues whereby participants ensure that the communication is successful by acknowledging that they have understood the message .</sentence>
				<definiendum id="0">meta-plan ASK-ADVISERHELP</definiendum>
				<definiens id="0">the presence of acknowledgement subdlalogues whereby participants ensure that the communication is successful by acknowledging that they have understood the message</definiens>
			</definition>
			<definition id="4">
				<sentence>Finally , a few pronominal anaphors ( i.e. it ) have their antecedent ( i.e. , the statistical package ) found in the root subdialogue which contains important antecedents .</sentence>
				<definiendum id="0">few pronominal anaphors</definiendum>
				<definiens id="0">the statistical package ) found in the root subdialogue which contains important antecedents</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>ABSTRACT Computer Science Department Boston University 111 Cummington Street Boston , Massachusetts 02215 USA PREL1MIN A R IES We study the formal and linguistic properties of a class of parenthesis-free categorial grammars derived from those of Ades and Steedman by varying the set of reduction rules .</sentence>
				<definiendum id="0">R IES</definiendum>
				<definiens id="0">We study the formal and linguistic properties of a class of parenthesis-free categorial grammars derived from those of Ades and Steedman by varying the set of reduction rules</definiens>
			</definition>
			<definition id="1">
				<sentence>A categorial grammar under a set R of reduction rules is a quadruple CGR ( VT , VA , S , F ) , whose elements are defined as follows : VT is a finite set of morphemes .</sentence>
				<definiendum id="0">categorial grammar</definiendum>
			</definition>
			<definition id="2">
				<sentence>VA is a finite set of atomic category symbols .</sentence>
				<definiendum id="0">VA</definiendum>
			</definition>
			<definition id="3">
				<sentence>S EVA is a distinguished element of VA .</sentence>
				<definiendum id="0">S EVA</definiendum>
				<definiens id="0">a distinguished element of VA</definiens>
			</definition>
			<definition id="4">
				<sentence>Notation : Morphemes are denoted by a , b ; morpheme strings by u , v , w. The symbols S , A , B , C denote atomic category symbols , and U. V , X , Y denote arbitrary ( complex ) category symbols .</sentence>
				<definiendum id="0">U. V</definiendum>
				<definiens id="0">a , b ; morpheme strings by u , v , w. The symbols S , A , B , C denote atomic category symbols , and</definiens>
			</definition>
			<definition id="5">
				<sentence>A morpheme string w=wlu , ~ '' `` 'w , is accepted by CGR ( VT , VA , S , F ) if there is a category string z = X1X2 `` '' • X , such that XiEF ( w , ) for each i=l,2 , ' -- n , and x = &gt; * S. The language L ( CGR ) accepted by CGR ( VT , VA , S , F ) is the set of all morpheme strings that are accepted .</sentence>
				<definiendum id="0">CGR ( VT , VA , S , F )</definiendum>
				<definiens id="0">each i=l,2 , ' -- n , and x = &gt; * S. The language L ( CGR ) accepted by</definiens>
			</definition>
			<definition id="6">
				<sentence>u is a permutation of v iff ~ ( u ) =~ ( v ) .</sentence>
				<definiendum id="0">u</definiendum>
				<definiens id="0">a permutation of v iff ~ ( u ) =~ ( v )</definiens>
			</definition>
			<definition id="7">
				<sentence>Then there exists a LCF such that ¢ ( L ( CGR ) ) = ¢ ( LcF ) , where LcF is context free .</sentence>
				<definiendum id="0">LcF</definiendum>
				<definiens id="0">context free</definiens>
			</definition>
			<definition id="8">
				<sentence>\ [ \ ] 76 Corollary For any R ~ { F , FP , B , BP } , L ( CGR ) is semilinear , Parikh bounded and has the linear growth property .</sentence>
				<definiendum id="0">CGR</definiendum>
				<definiens id="0">semilinear , Parikh bounded and has the linear growth property</definiens>
			</definition>
			<definition id="9">
				<sentence>Semilinearity follows from Parikh 's Lemma and linear growth from the pumping lemma for context-free languages .</sentence>
				<definiendum id="0">Semilinearity</definiendum>
				<definiens id="0">follows from Parikh 's Lemma and linear growth from the pumping lemma for context-free languages</definiens>
			</definition>
			<definition id="10">
				<sentence>Thus , though TAG Languages and CG languages share some properties like linear growth , semilinearity , generation of all context-free languages , limited context sensitive power , and Parikh boundedness , they are different in their generative capacities .</sentence>
				<definiendum id="0">CG languages</definiendum>
				<definiens id="0">share some properties like linear growth</definiens>
			</definition>
</paper>

		<paper id="1026">
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>/ Revision is a large part of the writing process for people .</sentence>
				<definiendum id="0">/ Revision</definiendum>
				<definiens id="0">a large part of the writing process for people</definiens>
			</definition>
			<definition id="1">
				<sentence>Thematic progression is the organization of given and new information into theme-rheme patterns in successive sentences .</sentence>
				<definiendum id="0">Thematic progression</definiendum>
				<definiens id="0">the organization of given and new information into theme-rheme patterns in successive sentences</definiens>
			</definition>
			<definition id="2">
				<sentence>`` Paragraph '' in this sense ( Longacre , 1979 ) refers to a structural unit which does not necessarily correspond to the orthographic unit indicated by an indentation of the text .</sentence>
				<definiendum id="0">Paragraph</definiendum>
				<definiens id="0">a structural unit which does not necessarily correspond to the orthographic unit indicated by an indentation of the text</definiens>
			</definition>
			<definition id="3">
				<sentence>The example is from the UMass COUNSELOR Project , which is developing a natural language discourse system based on the HYPO legal reasoning system ( Rissland , Valcarce , &amp; Ashley , 1984 ) .</sentence>
				<definiendum id="0">UMass COUNSELOR Project</definiendum>
				<definiens id="0">developing a natural language discourse system based on the HYPO legal reasoning system</definiens>
			</definition>
			<definition id="4">
				<sentence>KDS and ¥h use a top down approach where intermediate representations are evaluated and improved before any text is actually generated ; Penman uses a cyclic approach similar to that described here .</sentence>
				<definiendum id="0">Penman</definiendum>
				<definiens id="0">uses a cyclic approach similar to that described here</definiens>
			</definition>
			<definition id="5">
				<sentence>Literacy , Language and Learning : The nature and consequences of reading and writing , Cambridge University Press , pp .</sentence>
				<definiendum id="0">Learning</definiendum>
				<definiens id="0">The nature and consequences of reading and writing</definiens>
			</definition>
</paper>

	</volume>
