<?xml version="1.0" encoding="UTF-8"?>
	<volume id="J99">

		<paper id="4001">
			<definition id="0">
				<sentence>Set theoretically , rules can be regarded as being ordered pairs where the first element is a nonterminal symbol and the second is a tuple of symbols , i.e. , of the form ( A0 , ( A1 ... .. Ak ) ) where k &gt; 0 , but for ease of exposition they will be written as A0 -- + A1 ... Ak We will make the following simplifying assumptions ( which do not lose generality ) : .</sentence>
				<definiendum id="0">Ak</definiendum>
				<definiens id="0">a tuple of symbols , i.e. , of the form ( A0 , ( A1 ... .. Ak ) ) where k &gt; 0 , but for ease of exposition they will be written as A0 -- + A1 ...</definiens>
			</definition>
			<definition id="1">
				<sentence>Where a tree T spans a terminal string al ... an , and M is a node within T that spans ai .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">a node within T that spans ai</definiens>
			</definition>
			<definition id="2">
				<sentence>A derivation is a sequence of symbol strings w1 ... .. wn such that wi ~ wi+l for all 1 &lt; i &lt; n. A rightmost derivation is one in which each step from wi to ~i+1 is made by replacing the furthest right nonterminal symbol in W i USing some rule ( i.e. , -y in the above definition of directly derives is entirely made up of terminal symbols ) ( cf. Aho and Ullman 1972 ) .</sentence>
				<definiendum id="0">derivation</definiendum>
				<definiendum id="1">rightmost derivation</definiendum>
			</definition>
			<definition id="3">
				<sentence>Definition 4 A bidirectionally strategy-marked context free-grammar ( BSCFG ) is a pair ( G , tr ) where G is a CFG and tr is a bidirectional strategy marking of G. Definition 5 Let ( ( VN , VT , P , S ) , tr ) be a bidirectionally strategy-marked context-free grammar .</sentence>
				<definiendum id="0">G</definiendum>
				<definiendum id="1">tr</definiendum>
				<definiendum id="2">tr</definiendum>
				<definiens id="0">a pair ( G , tr ) where</definiens>
			</definition>
			<definition id="4">
				<sentence>Definition 6 Given a CFG G of the form ( VN , VT , P , S ) a double-dotted rule based on G is a triple ( p , l , r ) where p is a rule in P of the form Ao -- * A1 ... Ak and l , r are integers such that O &lt; l &lt; r &lt; k. Such a rule will be written as : Ao -- * A1 ... Al • Ai+l . . . Ar • Ar+l ... Ak for ease of exposition and similarity to previous literature. Where either l = 0 or r = k , the empty portions will be omitted from the expression. Definition 7 Given a CFG G = ( VN , VT , P , S ) , an edge based on G is a triple ( i , j , d ) where i and j are nonnegative integers with i _ &lt; j , and d is a double-dotted rule based on G. An edge is said to be lexical or nonlexical according to whether or not the rule is lexical. An edge of the form ( i , j , Ao ~ A1 ... Aq-1 • Aq. .. Ap • Ap+l ... Ak ) where either q &gt; 1 or p &lt; k ( i.e. , with a nonempty component at either end ) is referred to as an active edge , and an edge of the form ( i , j , Ao -- + •A1 ... Ak o ) is an inactive edge .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">Given a CFG G of the form ( VN , VT , P ,</definiens>
				<definiens id="1">a triple ( p , l</definiens>
				<definiens id="2">a triple ( i , j , d ) where i</definiens>
			</definition>
			<definition id="5">
				<sentence>Definition 8 Given a CFG G = ( VN , VT , P , S ) and a string al ... .. an from V~ , a chart based on al , ... , an and using G is a set C of edges based on G that meets the following conditions : .</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">a CFG G = ( VN , VT , P , S ) and a string al ... .. an from V~ , a chart based on al , ... , an and using</definiens>
				<definiens id="1">a set C of edges based on G that meets the following conditions</definiens>
			</definition>
			<definition id="6">
				<sentence>A1 ... Am ' ) there is an edge in C : ( i , j , Bo ~ B1 ... Bq_l • Bq • Bq+l ... Bv ) for every rule r in G of the form Bo -- * B1 ... Bv such that q C tr ( r ) and Bq = A0 , where S is the distinguished symbol of G and 0 c tr ( r ) , there is an edge in C of the form : ( 0,0 , S ~ • ° B1 ... Bk ) ( i , j , Bo -- * B1 ... Bq • Bq+l . . . Bp ° Bp+l . . . Bv ) where 0 _ &lt; p &lt; v , and every rule r in G of the form Ao ~ A1 ... Ak for which Bp+l = Ao and 0 C tr ( r ) , there is also an edge in C of the form : ( j , j , A0 -- * `` • A1. . .Ak ) ( i , j , Bo ~ B1. . . Bq • Bq+ l . . . Bp • Bp+ l . . . By ) where 0 &lt; q _ &lt; v , and every rule r in G of the form Ao -- * A1 ... Ak for which Bq = Ao and 0 c tr ( r ) , there is also an edge in C of the form : ( i , i , Ao -- * A1. . . Ak • • ) For brevity , the term fully bidirectional will be used for a chart that is both bidirectionally resolved and bidirectionally mixed strategy explored .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">the distinguished symbol of G and 0 c tr</definiens>
				<definiens id="1">a chart that is both bidirectionally resolved and bidirectionally mixed strategy explored</definiens>
			</definition>
			<definition id="7">
				<sentence>Mj is a preterminal node of T ; Mj is reachable from below .</sentence>
				<definiendum id="0">Mj</definiendum>
				<definiendum id="1">Mj</definiendum>
				<definiens id="0">a preterminal node of T ;</definiens>
			</definition>
			<definition id="8">
				<sentence>Then for any set gl ... .. gn of elements of A~P ( G , tr ) , the function g ' ( in A3V ( G , tr ) ) given by g ' ( A ) = true iff either gl ( A ) = true or ... gn ( A ) = true is at least upper bound ( Maclane and Birkhoff 1967 ; Stoy 1981 ) for gl ... .. gn with respect to G. Since A3V ( G , tr ) is finite , the presence of a 1 .</sentence>
				<definiendum id="0">... gn</definiendum>
				<definiens id="0">true iff either gl ( A ) = true or</definiens>
			</definition>
			<definition id="9">
				<sentence>If M is licensed by a purely bottom-up rule , then C contains a representation of the subtree rooted at M. If M is licensed by a top-down rule , and there is in C an active edge ( t , g , B0 -- ~ B1 ... Bp_l • Bp ... Bq_l • Bq ... Bv ) where either Bp-1 = A and t is the end of M , or Bq = A and g is the start of M , then C contains a representation of the subtree rooted at M. Proof By induction on the height of nodes .</sentence>
				<definiendum id="0">g</definiendum>
				<definiens id="0">a representation of the subtree rooted at M. Proof By induction on the height of nodes</definiens>
			</definition>
			<definition id="10">
				<sentence>Since T is a tree , there is a path of nonterminal nodes ( N1 ... . , N t ) where Ni is labeled Bi , for 1 &lt; i &lt; t , N1 = M , Nt is the root of T , and Ni is the mother of Ni-1 for 2 &lt; i &lt; t .</sentence>
				<definiendum id="0">Ni</definiendum>
				<definiendum id="1">Nt</definiendum>
				<definiendum id="2">Ni</definiendum>
				<definiens id="0">the root of T , and</definiens>
			</definition>
			<definition id="11">
				<sentence>\ [ \ ] 473 Computational Linguistics Volume 25 , Number 4 Definition 20 In a BSCFG ( G , tr ) , suppose A is a nonterminal symbol , and cr is a string of terminal symbols .</sentence>
				<definiendum id="0">BSCFG</definiendum>
				<definiendum id="1">cr</definiendum>
				<definiens id="0">a string of terminal symbols</definiens>
			</definition>
			<definition id="12">
				<sentence>Since Ao -- * A1 ... Ak is bottom-up , this means there is an active edge in C ( 1 , j ' , A0 -* •A1 •A2 ... ak ) where j is the start of the inactive edge for the root of this tree ( i.e. , the node labeled A1 ) , and j ' is its end .</sentence>
				<definiendum id="0">Ak</definiendum>
				<definiendum id="1">j</definiendum>
				<definiens id="0">the start of the inactive edge for the root of this tree</definiens>
			</definition>
			<definition id="13">
				<sentence>If M is licensed by a purely bottom-up rule , then C contains a representation of the subtree rooted at M. If M is licensed by a top-down rule , and there is in C an active edge ( t , g , Bo -- * B1 ... Bp-1 • Bp ... Bq_l • Bq ... An ) where either Bp-1 = A and t is the end of M , or Bq = A and g is the start of M , then C contains a representation of the subtree rooted at M. Proof By induction on the height of nodes , in a manner very similar to Lemma 3 , except that part ( a ) of the Inductive Step is as follows : Inductive Step ( a ) : Suppose M0 , with daughter nodes M1 ... .. Mk , is licensed by a purely bottom-up rule A0 -- * A1 ... Ak .</sentence>
				<definiendum id="0">g</definiendum>
			</definition>
			<definition id="14">
				<sentence>Suppose T C trees ( G ) , and C is a fully bidirectional chart based on the string spanned by T and using ( G , tr ) .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a fully bidirectional chart based on the string spanned by T and using ( G , tr )</definiens>
			</definition>
			<definition id="15">
				<sentence>Base Case : Suppose M is of depth 1 ( i.e. , a daughter of the root node ) .</sentence>
				<definiendum id="0">Base Case</definiendum>
				<definiens id="0">a daughter of the root node )</definiens>
			</definition>
			<definition id="16">
				<sentence>Suppose the root is licensed by a rule S -~ A1 ... Ak , where Mi is the ith daughter of the root ( 1 &lt; i &lt; k ) and M = My .</sentence>
				<definiendum id="0">Mi</definiendum>
			</definition>
			<definition id="17">
				<sentence>Since r ' is bottom-up and A1 is the leftmost ( trigger ) symbol of its RHS , this leads to an active edge of the form ( i , i , Ao -- ~ • •al..</sentence>
				<definiendum id="0">A1</definiendum>
			</definition>
			<definition id="18">
				<sentence>The purely bottom-up rules are all the rules of G2 , together with B1 ~ B2B3 B6 -- ~ $ 2B2 where each Si is the distinguished symbol of Gi .</sentence>
				<definiendum id="0">Si</definiendum>
				<definiens id="0">the distinguished symbol of Gi</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>In addition , contextual knowledge determines the salience ( or degree of prominence ) of propositions at the current point in the dialogue , and salience is a factor that constrains the interpretation of coherent discourse actions .</sentence>
				<definiendum id="0">salience</definiendum>
				<definiens id="0">contextual knowledge determines the salience ( or degree of prominence ) of propositions at the current point in the dialogue , and</definiens>
				<definiens id="1">a factor that constrains the interpretation of coherent discourse actions</definiens>
			</definition>
			<definition id="1">
				<sentence>Linguistic knowledge consists of clue words and the surface form of the utterance ; world knowledge includes a set of stereotypical beliefs that users generally hold and recipes for performing discourse acts ; and contextual knowledge consists of a model of the user 's beliefs 8 Carberry and Lambert Modeling Negotiation Subdialogues acquired from the preceding dialogue , the current structure of the discourse , the existing focus of attention ( that aspect of the task on which the participants ' attention is currently centered ) , and the relative salience ( degree of prominence ) of propositions in the discourse .</sentence>
				<definiendum id="0">Linguistic knowledge</definiendum>
				<definiendum id="1">contextual knowledge</definiendum>
				<definiens id="0">consists of clue words and the surface form of the utterance ; world knowledge includes a set of stereotypical beliefs that users generally hold and recipes for performing discourse acts</definiens>
				<definiens id="1">consists of a model of the user 's beliefs 8 Carberry and Lambert Modeling Negotiation Subdialogues acquired from the preceding dialogue , the current structure of the discourse , the existing focus of attention ( that aspect of the task on which the participants ' attention is currently centered ) , and the relative salience ( degree of prominence ) of propositions in the discourse</definiens>
			</definition>
			<definition id="2">
				<sentence>Our representation of a recipe includes a header giving the action defined by the recipe , the recipe type , preconditions , applicability conditions , constraints , a body , effects , and a goal .</sentence>
				<definiendum id="0">representation of a recipe</definiendum>
				<definiens id="0">includes a header giving the action defined by the recipe , the recipe type , preconditions , applicability conditions , constraints , a body , effects , and a goal</definiens>
			</definition>
			<definition id="3">
				<sentence>12 Carberry and Lambert Modeling Negotiation Subdialogues Discourse Recipe Action : Inform ( _agentl , _agent2 , _proposition ) { _agent1 informs _agent2 of_proposition } Recipe-Type : Decomposition Appl Cond : believe ( _agentl , _proposition , \ [ C : C\ ] ) believe ( _agentl , believe ( _agent2 , _proposition , \ [ CN : S\ ] ) , \ [ 0 : C\ ] ) Body : Tell ( _agentl , _agent2 , _proposition ) # Address-Behevability ( _agentl , _agent2 , _proposition ) Effects : believe ( _agent2 , want ( _agentl , believe ( _agent2 , _proposition , \ [ C : C\ ] ) ) , \ [ c : c\ ] ) Goal : believe ( _agent2 , _proposition , \ [ C : C\ ] ) Discourse Recipe Action : Tell ( _agentl , _agent2 , _proposition ) { _agent1 tells _agent 2 of_proposition } Recipe-Type : Decomposition Appl Cond : believe ( _agentl , _proposition , \ [ C : C\ ] ) Body : Surface-Say-Prop ( _agentl , _agent2 , _proposition ) # Address-Understanding ( _agentl , _agent2 , _proposition ) Effects : told-about ( _agentl , _agent2 , _proposition ) Goal : beheve ( _agent2 , believe ( _agentl , _proposition , \ [ C : C\ ] ) , \ [ C : C\ ] ) Figure S Recipes for Inform and Tell discourse acts .</sentence>
				<definiendum id="0">_proposition ) Effects</definiendum>
				<definiens id="0">C\ ] ) ) , \ [ c : c\ ] ) Goal : believe ( _agent2 , _proposition</definiens>
			</definition>
			<definition id="4">
				<sentence>The body of the AskRef action consists of making the request itself and making the request acceptable ; this is because in our own interactions we have encountered situations in which an agent will make a request and then justify it to the listener .</sentence>
				<definiendum id="0">AskRef action</definiendum>
				<definiens id="0">consists of making the request itself and making the request acceptable</definiens>
			</definition>
			<definition id="5">
				<sentence>14 Carberry and Lambert Modeling Negotiation Subdialogues an Evaluate-Answer discourse act is an expected follow-up to an Answer-Ref when they are part of a higher-level Test-Knowledge discourse act but not when the Answer-Ref is part of an Obtain-Info-Ref discourse act .</sentence>
				<definiendum id="0">Evaluate-Answer discourse act</definiendum>
			</definition>
			<definition id="6">
				<sentence>The active path consists of the sequence of actions along the path from the action that is the focus of attention to the root node .</sentence>
				<definiendum id="0">active path</definiendum>
				<definiens id="0">consists of the sequence of actions along the path from the action that is the focus of attention to the root node</definiens>
			</definition>
			<definition id="7">
				<sentence>In such a chain , action Ai contributes to the performance of its successor action Ai+l. For example , the semantic representation of an utterance such as `` Dr. Smith is teaching Architecture '' is Surface-Say-Prop ( _agentl , _agent2 , Teaches ( Dr.Smith , Architecture ) ) A Surface-Say-Prop is a subaction in the recipe for a Tell discourse act , which in turn is a subaction in the recipe for an Inform discourse act .</sentence>
				<definiendum id="0">A Surface-Say-Prop</definiendum>
				<definiens id="0">a subaction in the recipe for a Tell discourse act</definiens>
			</definition>
			<definition id="8">
				<sentence>In utterance ( 8 ) , CA initiates a Tell discourse act as part of an Inform discourse act ; thus immediately after ( 8 ) , both the Inform and the Tell are part of the existing dialogue context .</sentence>
				<definiendum id="0">CA</definiendum>
				<definiens id="0">initiates a Tell discourse act as part of an Inform discourse act ; thus immediately after ( 8 ) , both the Inform and the Tell are part of the existing dialogue context</definiens>
			</definition>
			<definition id="9">
				<sentence>For example , the actions in the body of the Inform recipe ( see Figure 3 ) are : 1 ) the speaker ( _agent1 ) tells the listener ( _agent2 ) the proposition that the speaker wants the listener to believe ; and 2 ) the speaker and listener address believability by discussing whatever is necessary in order for the listener and speaker to come to an agreement about this proposition .</sentence>
				<definiendum id="0">Inform recipe</definiendum>
				<definiens id="0">tells the listener ( _agent2 ) the proposition that the speaker wants the listener to believe</definiens>
			</definition>
			<definition id="10">
				<sentence>This is reflected in the discourse tree that results from a statement , such as the one in Figure 5 , where the Tell action ( whose recipe contains an Address-Understanding action ) is a descendant of the Inform action ( whose recipe contains an Address-Believability action ) ; in addition , since the statement in Figure 5 is intended to answer a question , the Inform act is a descen16 Questions must also be accepted and assimilated into a dialogue .</sentence>
				<definiendum id="0">Tell action</definiendum>
				<definiendum id="1">Address-Understanding action )</definiendum>
				<definiens id="0">a descen16 Questions must also be accepted and assimilated into a dialogue</definiens>
			</definition>
			<definition id="11">
				<sentence>Next , use plan inference rules to hypothesize sequences of actions A1 , Ai2 ... .. Ai~ i ( inference paths ) such that A1 is the surface action directly associated with the speaker 's utterance and Aidi is an action on the active path in the existing dialogue context .</sentence>
				<definiendum id="0">Aidi</definiendum>
				<definiens id="0">the surface action directly associated with the speaker 's utterance and</definiens>
			</definition>
			<definition id="12">
				<sentence>24 Carberry and Lambert Modeling Negotiation Subdialogues A1 = surface action associated with the speaker 's utterance LE = clue words extracted from semantic representation of utterance D = dialogue model B -listener 's beliefs A d = action at current focus of attention in D ; ; Construct inference paths that link up to active path of dialogue model S* -- { Pi = A1 , Ai2 ... .. Aiei \ ] on-active-path ( Ai~i , D ) APi is an inference path constructed from A1 } ; ; Eliminate inference paths with unsatisfied constraints or implausible applicability conditions For each Pi C S Do Begin Bi~ -- -B If A~ , ¢ A i Then Bi ~ BiU { beliefs that A d and all actions on the active path between Ai and Aid i have completed successfully } If ( 3Aj ) ( 3Ck ) Aj C Pi A is-constraint ( Ck , Aj ) A -~Ck Then S ~ SPi Else If ( 3Aj ) ( 3ACk ) Aj E Pi A is-app-cond ( ACk , Aj ) A-~plausible ( ACk , Bi ) Then S ~ S Pi End ; ; Determine how much evidence is available for each e-action So ~ O , $ 1 ~ O , $ 2 ~ 0 For each Pi E S Do If ( 3Aj ) Aj C Pi A e-action ( Aj ) Then Begin If ling-evid ( Aj , LE ) A app-cond-evid ( Aj , Bi ) Then $ 2 ~ S2U { Pi } Else If ling-evid ( Aj , LE ) V app-cond-evid ( Aj , Bi ) Then $ 1 *-S1U { Pi } End Else So ~-SoU { Pi } ; ; So contains inference paths with no e-actions ; ; Select inference paths containing actions with the most evidence If $ 2 ~ 0 Then S ~ $ 2 ; ; S contains inference paths with multiple evidence Else If $ 1 # 0 Then S ~ $ 1 ; ; S contains inference paths with evidence Else S ~ So ; ; S contains inference paths with no e-actions ; ; Select inference path containing the action closest to current focus of attention If S # 0 Then P , -Pi \ ] Pi E S A Pi = A1 , A 6 ... . , Aidi A-~ ( 3Pj ) Pj E S APj = A1 , Aj2 ... .. Aid j A closer-to-curr-discourse-focus ( Aja j , Aiei , D ) Else Begin B ~BU { beliefs that all actions on active path have completed successfully } S ~ { Pi = A1 , A6 ... .. Ain \ ] Pi is an inference path constructed from A1 A no-elts-on-active-path ( Pi , D ) A ( VAj ) ( VCk ) ( Aj ff Pi A is-constraint ( Ck , Aj ) ~ Ck ) A ( VAj ) ( VACk ) ( Aj E Pi A is-app-cond ( ACk , Aj ) -- ~ plausible ( ACk , B ) ) P ~-Pi \ [ Pi E S A -~ ( 3Pj ) Pj E S A links-closer-to-ps-dom-focus ( Pj , Pi , D ) End ; ; Assimilate utterance into dialogue model Add P = A1 , Ap2 ... . , Apk to D Mark Ap2 as new focus of attention in D Figure 9 Pseudocode outlining our recognition algorithm .</sentence>
				<definiendum id="0">Pi</definiendum>
				<definiens id="0">an inference path constructed from A1 } ; ; Eliminate inference paths with unsatisfied constraints or implausible applicability conditions For each Pi C S Do Begin Bi~ -- -B If A~ , ¢ A i Then Bi ~ BiU { beliefs that A d and all actions on the active path between Ai and Aid i have completed successfully } If ( 3Aj ) ( 3Ck ) Aj C Pi A is-constraint ( Ck , Aj ) A -~Ck Then S ~ SPi Else If ( 3Aj ) ( 3ACk ) Aj E Pi A is-app-cond ( ACk , Aj ) A-~plausible ( ACk , Bi ) Then S ~ S Pi End ;</definiens>
			</definition>
			<definition id="13">
				<sentence>Utterances ( 20 ) , ( 23 ) , and ( 27 ) illustrate the use of world knowledge in resolving expressions of doubt .</sentence>
				<definiendum id="0">Utterances</definiendum>
				<definiens id="0">illustrate the use of world knowledge in resolving expressions of doubt</definiens>
			</definition>
			<definition id="14">
				<sentence>The semantic representation of ( 18 ) is : Surface-WH-question ( EA , CA , _course , Teaches ( Dr. Smith , _course ) ) The Surface-WH-Question is a subaction in the body of a recipe for a Ref-Request discourse act ; the Ref-Request is a subaction in the recipe for an Ask-Ref discourse act ; and 29 Computational Linguistics Volume 25 , Number 1 the Ask-Ref is a subaction in the recipe for an Obtain-Info-Ref discourse act .</sentence>
				<definiendum id="0">Surface-WH-Question</definiendum>
				<definiendum id="1">Ref-Request</definiendum>
				<definiendum id="2">Ask-Ref</definiendum>
				<definiens id="0">a subaction in the body of a recipe for a Ref-Request discourse act</definiens>
				<definiens id="1">a subaction in the recipe for an Ask-Ref discourse act</definiens>
			</definition>
			<definition id="15">
				<sentence>It indicates that CA must believe that _propanswer ( where _propanswer is instantiated from the Inform act as Teaches ( Dr. Smith , Arch ) ) is an instance of the queried proposition , _proposition , with the queried term _term instantiated .</sentence>
				<definiendum id="0">_propanswer</definiendum>
				<definiens id="0">an instance of the queried proposition , _proposition , with the queried term _term instantiated</definiens>
			</definition>
			<definition id="16">
				<sentence>For example , CS180 is a legal instantiation of the _course term in the proposition Teaches ( Jones _course ) although Teaches ( Jones , CS180 ) may be false .</sentence>
				<definiendum id="0">CS180</definiendum>
				<definiens id="0">a legal instantiation of the _course term in the proposition Teaches ( Jones _course ) although Teaches</definiens>
			</definition>
			<definition id="17">
				<sentence>As we have seen previously , Express-Doubt is an e-action since it is the action on the inference path at which the parameter _propo s it ion 1 is first introduced .</sentence>
				<definiendum id="0">Express-Doubt</definiendum>
				<definiens id="0">the action on the inference path at which the parameter _propo s it ion 1 is first introduced</definiens>
			</definition>
			<definition id="18">
				<sentence>The semantic representation of ( 22 ) is : Surface-Say-Prop ( CA , EA , on-sabbatical ( Dr.Brown ) ) The Surface-Say-Prop is part of Tell ( CA , EA , on-sabbatical ( Dr.Brown ) ) , which is part of Inform ( CA , EA , on-sabbatical ( Dr.Brown ) ) .</sentence>
				<definiendum id="0">on-sabbatical ( Dr.Brown</definiendum>
				<definiens id="0">Surface-Say-Prop ( CA , EA , on-sabbatical ( Dr.Brown ) ) The Surface-Say-Prop is part of Tell ( CA , EA , on-sabbatical ( Dr.Brown )</definiens>
			</definition>
			<definition id="19">
				<sentence>Address-Acceptance ( CA , EA , _propositionl ) is part of a recipe for Address-Believability ( CA , EA , _propositionl ) , which in turn is part of a recipe for Inform ( CA , EA , _propositionl ) .</sentence>
				<definiendum id="0">Address-Acceptance</definiendum>
				<definiens id="0">part of a recipe for Address-Believability ( CA , EA , _propositionl ) , which in turn is part of a recipe for Inform</definiens>
			</definition>
			<definition id="20">
				<sentence>Since Address-Acceptance is a subaction in a recipe for Address-Believability , and Address-Believability is a subaction in a recipe for Inform , CA might be trying to offer support for the Inform act of ( 24 ) , Inform ( CA , EA , on-campus ( Dr.Brown , Yesterday ) ) .</sentence>
				<definiendum id="0">Address-Believability</definiendum>
				<definiens id="0">a subaction in a recipe for Inform</definiens>
			</definition>
			<definition id="21">
				<sentence>Yes , CS510 is a graduate course .</sentence>
				<definiendum id="0">CS510</definiendum>
				<definiens id="0">a graduate course</definiens>
			</definition>
			<definition id="22">
				<sentence>When utterance ( 33 ) occurs , there are three propositions that have not yet been accepted by EA , and the system considers the possibility that EA is performing one of three express doubt actions , namely Express-Doubt ( EA , Express-Doubt ( EA , Expre s s-Doubt ( EA , CA , Meets ( CS510 , MonTPM ) , Graduate-Course ( CS510 ) ) CA , ~Teaches ( Dr. Jones , CS510 ) , Graduate-Course ( CS510 ) ) CA , Teaehes ( Dr.Hart , CS510 ) , Graduate-Course ( CS510 ) ) 41 Computational Linguistics Volume 25 , Number 1 In all three cases , the system lacks evidence that the third applicability condition in the Express-Doubt recipe is satisfied .</sentence>
				<definiendum id="0">~Teaches</definiendum>
				<definiens id="0">have not yet been accepted by EA</definiens>
			</definition>
			<definition id="23">
				<sentence>Walker ( 1992 ) has found many occasions of redundancy in collaborative dialogues , and explains these by claiming that people repeat themselves in order to ensure that each utterance has been understood .</sentence>
				<definiendum id="0">Walker ( 1992 )</definiendum>
				<definiens id="0">has found many occasions of redundancy in collaborative dialogues , and explains these by claiming that people repeat themselves in order to ensure that each utterance has been understood</definiens>
			</definition>
			<definition id="24">
				<sentence>Cond : believe ( _agentl , believe ( _agent2 , _proposition2 ~ ~_propositionl , \ [ s : c\ ] ) , \ [ W : Cl ) believe ( _agentl , _proposition3 ~ -~in-conflict ( _propositionl , _proposition2 ) , \ [ S : C\ ] ) believe ( _agentl , believe ( _agent2 , _proposition2 , \ [ W : C\ ] ) , \ [ S : C\ ] ) Constraints : salient ( _propositionl ) salient ( _proposition2 ) Body : Inform ( _agentl , _agent2 , _proposition3 ) Effects : claim-explained ( _agentl , _agent2 , _proposition1 ) Goal : believe ( _agent2 , _proposition2 ~ ~_propositionl , \ [ CN : CN\ ] ) Discourse Recipe Action : Express-Doubt ( _agentl , _agent2 , _proposition1 , _proposition2 ) { _agent1 expresses doubt to _agent2 about _proposition1 by contending that _proposition2 is true } Recipe-Type : Appl Cond : Constraints : Body : Effects : Goal : Decomposition believe ( _agentl , believe ( _agent2 , _proposition1 , \ [ S : C\ ] ) , IS : C\ ] ) believe ( _agentl , _proposition2 , \ [ W : S\ ] ) believe ( _agentl , _proposition2 ~ ~_propositionl , \ [ S : C\ ] ) salient ( _proposition1 ) Convey-Uncertain-Belief ( _agentl , _agent2 , _proposition2 ) believe ( _agent2 , believe ( _agentl , _proposition1 , \ [ SN : WN\ ] ) , \ [ S : C\ ] ) believe ( _agent2 , believe ( _agentl , _proposition2 ~ ~_propositionl , \ [ S : Cl ) , \ [ s : c\ ] ) believe ( _agent2 , want ( _agentl , Resolve-Conflict ( _agent2 , _agent1 , _proposition1 , _proposition2 ) ) , \ [ S : C\ ] ) want ( _agent2 , Resolve-Conflict ( _agent2 , _agent1 , _proposition1 , _proposition2 ) ) Discourse Recipe Action : Inform ( _agentl , _agent2 , _proposition ) { _agent1 informs _agent2 of_proposition } Recipe-Type : Decomposition Appl Cond : believe ( _agentl , _proposition , \ [ C : C\ ] ) believe ( regent1 , believe ( _agent2 , _proposition , \ [ CN : S\ ] ) , \ [ 0 : C\ ] ) Body : Tell ( _agentl , _agent2 , _proposition ) # Address-Believability ( _agentl , _agent2 , _proposition ) Effects : believe ( _agent2 , want ( _agentl , believe ( _agent2 , _proposition , \ [ c : c\ ] ) ) , \ [ c : c\ ] ) Goal : believe ( _agent2 , _proposition , \ [ C : C\ ] ) Discourse Recipe Action : Obtain-Info-Ref ( _agentl , _agent2 , _term , _proposition ) { _agent1 learns from _agent2 the referent of_term in _proposition ) } Recipe-type : Decomposition Appl Cond : believe ( _agentl , knowref ( _agent2 , _term , _proposition ) , \ [ W : C\ ] ) -~knowref ( _agentl , _term , _proposition ) 48 Carberry and Lambert Modeling Negotiation Subdialogues Constraints : Body : Effects : Goal : want ( _agentl , knowref ( _agentl , _term , _proposition ) ) term-in ( _term , _proposition ) Ask-Ref ( _agentl , _agent2 , _term , _proposition ) Answer-Ref ( _agent2 , _agent1 , _term , _proposition ) information-sought ( _agentl , _agent2 , _proposition ) knowref ( _agentl , _term , _proposition ) Discourse Recipe Action : Ref-Request ( _agentl , _agent2 , _term , _proposition ) { _agent1 requests the referent of_term in _proposition } Recipe-Type : Constraint : Body : Effects : Goal : Specialization term-in ( _term , proposition ) Surface-WH-Question ( _agentl , _agent2 , _term , _proposition ) believe ( _agent2 , requested ( _agentl , _term , _proposition ) , \ [ C : C\ ] ) believe ( _agent2 , want ( _agentl , know-ref ( _agentl , _term , believe ( _agent2 , _proposition , \ [ C : C\ ] ) ) ) , \ [ C : C\ ] ) Discourse Recipe Action : Resolve-Conflict ( _agentl , _agent2 , _proposition1 , _proposition2 ) { _agent1 resolves the conflict of_proposition1 and _proposition2 } Recipe-Type : Decomposition Appl Cond : believe ( _agentl , believe ( _agent2 , _proposition2 ~ -~_propositionl , \ [ S : Cl ) , \ [ w : c\ ] ) believe ( _agentl , believe ( _agent2 , _proposition2 , \ [ W : S\ ] ) , \ [ W : C\ ] ) believe ( _agentl , _proposition1 , \ [ C : C\ ] ) Constraints : equalorneg ( _proposition2 , _proposition3 ) salient ( _propositionl ) salient ( _proposition2 ) Body : Inform ( _agentl , _agent2 , _proposition3 ) # Explain-Claim ( _agentl , _agent2 , _proposition1 , _proposition2 ) Effects : conflict-addressed ( _agentl , _agent2 , _proposition1 , _proposition2 ) Goal : believe ( _agent2 , in-conflict ( _propositionl , _proposition2 ) , \ [ CN : WN\ ] ) Discourse Recipe Action : Surface-Neg-YN-Question ( _agentl , _agent2 , _proposition ) { _agent1 makes a surface negative request about _proposition } Recipe-Type : Primitive Appl Cond : believe ( _agentl , _proposition , \ [ S : S\ ] ) Effects : asked-about ( _agentl , _agent2 , _proposition ) Goal : asked-about ( _agentl , _agent2 , _proposition ) Discourse Recipe Action : Surface-Say-Prop ( _agentl , _agent2 , _proposition ) { _agent1 makes a surface utterance of_proposition to _agent2 } Recipe-Type : Primitive Appl Cond : believe ( _agentl , _proposition , \ [ C : C\ ] ) Effects : said ( _agentl , _agent2 , _proposition ) Goal : said ( _agentl , _agent2 , _proposition ) Discourse Recipe Action : Surface-WH-Question ( _agentl , _agent2 , _term , _proposition ) 49 Computational Linguistics Volume 25 , Number 1 { _agent1 makes a surface request for the _term in _proposition } Recipe-Type : Primitive Constraints : term-in ( _term , _proposition ) Effects : asked-for ( _agentl , _agent2 , _term , _proposition ) Goal : asked-for ( _agentl , _agent2 , _term , _proposition ) Discourse Recipe Action : Tell ( _agentl , _agent2 , _proposition ) { _agent l tells _agent2 of_proposition } Recipe-Type : Appl Cond : Body : Effects : Goal : Decomposition believe ( _agentl , _proposition , \ [ C : C\ ] ) Surface-Say-Prop ( _agentl , _agent2 , _proposition ) # Address-Understanding ( _agentl , _agent2 , _proposition ) told-about ( _agentl , _agent2 , _proposition ) believe ( _agent2 , believe ( _agentl , _proposition , \ [ C : C\ ] ) , \ [ C : C\ ] )</sentence>
				<definiendum id="0">Cond</definiendum>
				<definiens id="0">C\ ] ) , \ [ S : C\ ] ) Constraints : salient ( _propositionl ) salient ( _proposition2</definiens>
				<definiens id="1">CN\ ] ) Discourse Recipe Action : Express-Doubt ( _agentl , _agent2 , _proposition1 , _proposition2 ) { _agent1 expresses doubt to _agent2 about _proposition1 by contending that _proposition2 is true } Recipe-Type : Appl Cond : Constraints : Body : Effects : Goal : Decomposition believe</definiens>
				<definiens id="2">C\ ] ) , IS : C\ ] ) believe</definiens>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>Lexicalization provides a clean interface for combining the syntactic and semantic information in the lexicon .</sentence>
				<definiendum id="0">Lexicalization</definiendum>
				<definiens id="0">provides a clean interface for combining the syntactic and semantic information in the lexicon</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , the most probable supertag sequence for an n-word sentence is given by : = argmaxTPr ( T1 , T2 ... .. TN ) * Pr ( W1 , W2 , ... , WN I T1 , T2 ... .. TN ) ( 3 ) where Ti is the supertag for word Wi .</sentence>
				<definiendum id="0">Ti</definiendum>
				<definiens id="0">the supertag for word Wi</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus , the Good-Turing discounting technique estimates the frequency of unseen events based on the distribution of the frequency of the counts of observed events in the corpus .</sentence>
				<definiendum id="0">Good-Turing discounting technique</definiendum>
				<definiens id="0">estimates the frequency of unseen events based on the distribution of the frequency of the counts of observed events in the corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>If r is the observed frequency of an event , and Nr is the number of events with the observed frequency r , and N is the total number of events , then the probability of an unseen event is given by N1/N .</sentence>
				<definiendum id="0">Nr</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">the total number of events</definiens>
			</definition>
			<definition id="4">
				<sentence>N ( Tj ) is the frequency of the supertag Tj and NUNK ( Tj ) is the estimated count of UNK in Tj .</sentence>
				<definiendum id="0">N ( Tj )</definiendum>
				<definiens id="0">the frequency of the supertag Tj and NUNK ( Tj ) is the estimated count of UNK in Tj</definiens>
			</definition>
			<definition id="5">
				<sentence>The output of the supertagger , an almost parse , has been used in a variety of applications including information retrieval ( Chandrasekar and Srinivas 1997b , 1997c , 1997d ) and information extraction ( Doran et al. 1997 ) , text simplification ( Chandrasekar , Doran , and Srinivas 1996 , Chandrasekar and Srinivas 1997a ) , and language modeling ( Srinivas 1996 ) to illustrate that supertags provide an appropriate level of lexical description needed for most applications .</sentence>
				<definiendum id="0">retrieval</definiendum>
				<definiendum id="1">information extraction</definiendum>
				<definiendum id="2">language modeling</definiendum>
				<definiens id="0">used in a variety of applications including information</definiens>
			</definition>
			<definition id="6">
				<sentence>The grammar consists of a set of rules that eliminate functional tags for words based on the context of a sentence .</sentence>
				<definiendum id="0">grammar</definiendum>
				<definiens id="0">consists of a set of rules that eliminate functional tags for words based on the context of a sentence</definiens>
			</definition>
			<definition id="7">
				<sentence>A trigram supertag disambiguation model , trained on 1,000,000 ( word , supertag ) pairs of the Wall Street Journal corpus , performs at an accuracy level of 92.2 % .</sentence>
				<definiendum id="0">trigram supertag disambiguation model</definiendum>
				<definiens id="0">trained on 1,000,000 ( word , supertag ) pairs of the Wall Street Journal corpus</definiens>
			</definition>
			<definition id="8">
				<sentence>The XTAG system consists of a morphological analyzer , a part-of-speech tagger , a wide-coverage LTAG English grammar , a predictive left-to-right Earley-style parser for LTAG ( Schabes 1990 ) , and an X-windows interface for grammar development ( Doran et al. 1994 ) .</sentence>
				<definiendum id="0">XTAG system</definiendum>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>As we mentioned in Section 1.2 , the versions of default unification based on Kaplan 's priority union ( e.g. , Carpenter 1993 ) are binary operations on TFSs , where the LHS TFS represents nondefault information and the RHS TFS is default information .</sentence>
				<definiendum id="0">RHS TFS</definiendum>
				<definiens id="0">the versions of default unification based on Kaplan 's priority union ( e.g. , Carpenter 1993 ) are binary operations on TFSs , where the LHS TFS represents nondefault information and the</definiens>
			</definition>
			<definition id="1">
				<sentence>As we suggested in Section 2 , from an informal perspective , a TDFS contains a typed feature structure ( TFS ) , which specifies what is indefeasible , and tails , which specify defeasible information .</sentence>
				<definiendum id="0">TDFS</definiendum>
				<definiens id="0">contains a typed feature structure ( TFS ) , which specifies what is indefeasible , and tails</definiens>
			</definition>
			<definition id="2">
				<sentence>65 Computational Linguistics Volume 25 , Number 1 they are , but also that , indefeasibly , F 's value is c and G 's value is c ( where c is a supertype of a and b ) as given in the TDFS below : ( 2 ) The point is that tails do n't record all the information that forms part of the unification history ; they only record the strictly default information that is compatible with the LHS TFS in the TDFS .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">tails do n't record all the information that forms part of the unification history</definiens>
				<definiens id="1">compatible with the LHS TFS in the TDFS</definiens>
			</definition>
			<definition id="3">
				<sentence>A TDFS is a TFS ( which intuitively represents the indefeasible information ) , plus a tail ( which intuitively is a record of default information that played a part in building the TDFS , and which is compatible with the indefeasible TFS ) .</sentence>
				<definiendum id="0">TDFS</definiendum>
			</definition>
			<definition id="4">
				<sentence>Definition 2 : Typed Feature Structures A typed feature structure defined on a set of features Feat , a type hierarchy ( Type , El and a set of indices N is a tuple/Q , r , 6 , 0 ) , where : • Q is a finite set of nodes , • r c Q ( this is the root node ; see conditions 1 and 2 below ) • 0 : Q ~ Type is a partial typing function ( this labels nodes with types ) .</sentence>
				<definiendum id="0">Typed Feature</definiendum>
				<definiendum id="1">Q</definiendum>
				<definiens id="0">a tuple/Q , r</definiens>
				<definiens id="1">a partial typing function</definiens>
			</definition>
			<definition id="5">
				<sentence>Then 7r -- -- -- F ~ '' means that F contains path equivalence or reentrancy between the paths ~r and ~r ' ( i.e. , 6 ( n , ~r ) = 6 ( n , ~r ' ) where n is the root node of F ) ; and ~F0r ) = omeans that the type on the path ~r in F is cr ( i.e. , PF0r ) = cr if and only if O ( 6 ( n , ~r ) ) = or , where n is the root node of F ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">n , ~r ) = 6 ( n , ~r '</definiens>
				<definiens id="1">the root node of F )</definiens>
				<definiens id="2">n , ~r ) ) = or</definiens>
				<definiens id="3">the root node of F )</definiens>
			</definition>
			<definition id="6">
				<sentence>Subsumption is then defined as follows : 66 Lascarides and Copestake Default Representation Definition 3 : Subsumption of Typed Feature Structures F subsumes F ' , written F ~ G_ F , if and only if : • Tr ~F 7r ' implies ~ ~F ' WI • ~F0r ) = t implies ~VF , 0r ) = t ' and t ' G_ t The subsumption hierarchy is a bounded complete partial order .</sentence>
				<definiendum id="0">Subsumption</definiendum>
				<definiendum id="1">subsumption hierarchy</definiendum>
				<definiens id="0">66 Lascarides and Copestake Default Representation Definition 3 : Subsumption of Typed Feature Structures F subsumes F ' , written F ~ G_ F , if and only if : • Tr ~F 7r ' implies ~ ~F ' WI • ~F0r ) = t implies ~VF , 0r ) = t '</definiens>
				<definiens id="1">a bounded complete partial order</definiens>
			</definition>
			<definition id="7">
				<sentence>Then : pfs ( T ) = { 4 : ( 4 , t ) ET } and pt ( T ) = { t : ( 4 , t ) ET } Informally , ~ &gt; takes two TDFSs , and produces a new TDFS I/T , where I is the ( monotonic ) unification of the TFSs in the argument TDFSs , and T is the union &lt; o &gt; f the tails , with all information that 's incompatible with I removed .</sentence>
				<definiendum id="0">I</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">the union &lt; o &gt; f the tails , with all information that 's incompatible with I removed</definiens>
			</definition>
			<definition id="8">
				<sentence>In other words , M serves to accumulate the indefeasible information in I , and accumulate the defaults that are compatible with I in T. The formal definition is as follows : &lt; &gt; Definition 7 : M Let F1 =aq I1/T 1 and F2 =a~ I2/T 2 be two TDFSs , and let F12 =a # F1M &gt; F2 .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">serves to accumulate the indefeasible information in I , and accumulate the defaults that are compatible with I in T. The formal definition is as follows : &lt; &gt;</definiens>
			</definition>
			<definition id="9">
				<sentence>The Indefeasible Part : I12 =/1 N I2 That is , the indefeasible TFS is the unification of the indefeasible parts of the arguments .</sentence>
				<definiendum id="0">indefeasible TFS</definiendum>
			</definition>
			<definition id="10">
				<sentence>The extended definition , which works on a set of atomic FSs , is given below : &lt; F1 Mca { G1 ... .. Gn } = { F1 M F2 : F2 is the unification of a maximal subset of { G1 ... .. Gn } such that Fa M F2 is defined } We will want to iterate credulous default unification in what happens below , and for &lt; this purpose , we define Mcs as follows : &lt; Definition 10 : Credulous Default Unification Mcs on Sets Let Y : I be a set of TFSs { F1 ... .. Fn } , and ~2 a set of atomic FSs .</sentence>
				<definiendum id="0">extended definition</definiendum>
				<definiens id="0">works on a set of atomic FSs , is given below : &lt; F1 Mca { G1 ... .. Gn } = { F1 M F2 : F2 is the unification of a maximal subset of { G1 ... ..</definiens>
			</definition>
			<definition id="11">
				<sentence>Then SubTDFS ( F , 7r ) = SubTFS ( I , 7r ) / SubTail ( T , 7r ) where SubTail ( T , Tr ) = { /F ' , t/c T : There is an element IF , t ) E T such that r is the root node of F and 6 ( r , ~r ) is defined ; and F 1 = SubTFS ( F , ~r ) } Similarly , we can define the prefixation of a TDFS by a path as follows : Definition 16 : SuperTFS Let I =/Q , r , 6 , 0 / be a TFS and ~r = F1 ... Fn be a path .</sentence>
				<definiendum id="0">SubTDFS</definiendum>
				<definiens id="0">7r ) = SubTFS ( I , 7r ) / SubTail ( T , 7r ) where SubTail ( T , Tr ) = { /F ' , t/c T : There is an element IF , t ) E T such that r is the root node of F and 6 ( r , ~r</definiens>
			</definition>
			<definition id="12">
				<sentence>Then the basic TDFS BasicTDFS ( FSi , FSD ) of FSi and FSD is the TDFS FSi/T , such that : T = { ( F , t } : t is the root type on FSD n FSi , and F is an atomic TFS such that : ( a ) FS~ ~ F ; ( b ) FSD N FSi G F ; and ( c ) there 's no other atomic FS F ' such that F ' E F and F ' satisfies conditions ( a ) and ( b ) } Note that the basic TDFS derived from FS~ and FSD is indeed a TDFS , since FSi and T satisfy all the conditions given in Definition 5 .</sentence>
				<definiendum id="0">TDFS BasicTDFS</definiendum>
				<definiendum id="1">FSD</definiendum>
				<definiendum id="2">F</definiendum>
				<definiens id="0">the TDFS FSi/T , such that : T = {</definiens>
				<definiens id="1">an atomic TFS such</definiens>
			</definition>
			<definition id="13">
				<sentence>N preserves all nondefault information , because the nondefault result of N &gt; is defined in terms of N on the nondefault parts of the arguments , and N preserves information .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">preserves all nondefault information</definiens>
				<definiens id="1">preserves information</definiens>
			</definition>
			<definition id="14">
				<sentence>Lemma 3 &lt; &gt; N is a function .</sentence>
				<definiendum id="0">N</definiendum>
			</definition>
			<definition id="15">
				<sentence>Lemma 4 DefFS is a function .</sentence>
				<definiendum id="0">DefFS</definiendum>
			</definition>
			<definition id="16">
				<sentence>However , although much simplified , the treatment in the grammar fragments here is substantially based on that assumed in the English Resource Grammar ( ERG ) under development at CSLI ( Flickinger , Sag , and Copestake , in preparation ) .</sentence>
				<definiendum id="0">Copestake</definiendum>
				<definiens id="0">substantially based on that assumed in the English Resource Grammar ( ERG ) under development at CSLI ( Flickinger , Sag , and</definiens>
			</definition>
			<definition id="17">
				<sentence>Subcategorization properties of signs are specified via valence ( VAL ) features , which describe a list of specifications with which the SYNSEM values of other signs may be unified .</sentence>
				<definiendum id="0">VAL</definiendum>
			</definition>
			<definition id="18">
				<sentence>Here head-comp-phrase is a type , corresponding to head-complement phrases , as we will discuss in more detail in Section 4.2 .</sentence>
				<definiendum id="0">head-comp-phrase</definiendum>
				<definiens id="0">a type , corresponding to head-complement phrases</definiens>
			</definition>
			<definition id="19">
				<sentence>VALP is a reformulation of the Valence Principle in Pollard and Sag ( 1994 , 348 ) .</sentence>
				<definiendum id="0">VALP</definiendum>
			</definition>
			<definition id="20">
				<sentence>ECC is a simpler paths when specifying constraints , but for the sake of clarity we show full paths here .</sentence>
				<definiendum id="0">ECC</definiendum>
				<definiens id="0">a simpler paths when specifying constraints , but for the sake of clarity we show full paths here</definiens>
			</definition>
			<definition id="21">
				<sentence>To explain the account , we must first briefly introduce the style of semantic en88 Lascarides and Copestake Default Representation ' % AD \ [ \ ] \ ] SUBJ : list \ ] SS : IVAL : SPR : list | \ ] BasicTDFS ( CPS : list J \ [ SS VAL : \ [ \ ] ' SSVAL \ [ \ ] \ [ CPS o\ ] ) HD-DTR : : SUBJ : list HD-DTR SS : VAL : SPR : list CPS : list \ [ hd-phr \ [ SS VAL : \ ] HD-DTR \ [ \ ] \ ] / FHEAD \ [ \ ] { ( ~ VAL : , hd-phr ) , SUBJ : list \ ] \ [ SS VAL SUBJ : \ [ \ ] , hd-phr ) , | SS : VAL : SPR : list | ( \ [ HD-DTR SS VAL SUBJ : \ [ \ ] = CPS list \ ] / ( \ [ SS VAL SPR : \ [ i3 \ ] , hd-phr ) , \ [ HD-DTR SS VANL SPR : \ [ \ ] HD-DTR SS : i S~UBJ : list \ ] ( \ [ SS VAL CPS : \ [ \ ] ~ \ ] , hd-phr ) , \ [ HD-DTR SS VAL CPS \ [ \ ] /SPR : list | tCPS : list J ( \ [ HD-DTRSSVALCPS : /o\ ] ) } Figure 15 Encoding VALP using the Basic TDFS notation ( SYNSEM is abbreviated SS , COMPS is CPS ) .</sentence>
				<definiendum id="0">Basic TDFS notation ( SYNSEM</definiendum>
				<definiendum id="1">COMPS</definiendum>
				<definiens id="0">SPR : list CPS : list \ [ hd-phr \ [ SS VAL : \ ] HD-DTR \</definiens>
				<definiens id="1">list \ ] \ [ SS VAL SUBJ : \ [ \ ] , hd-phr ) , | SS : VAL : SPR : list | ( \ [ HD-DTR SS VAL SUBJ : \ [ \ ] = CPS list \ ] / ( \ [ SS VAL SPR : \ [ i3 \ ] , hd-phr ) , \ [ HD-DTR SS VANL SPR : \ [ \ ] HD-DTR SS : i S~UBJ : list \ ] ( \ [ SS VAL CPS : \</definiens>
			</definition>
			<definition id="22">
				<sentence>The LISZT contains a list of relations , which is a singleton here as for most lexical signs ( composition of the semantics proceeds by appending LISZTs ) .</sentence>
				<definiendum id="0">LISZT</definiendum>
				<definiens id="0">contains a list of relations , which is a singleton here as for most lexical signs ( composition of the semantics proceeds by appending LISZTs )</definiens>
			</definition>
			<definition id="23">
				<sentence>The initial underscore in the type name is a notational convention to indicate a lexical predicate , so that we can for instance distinguish the type noun_rel , which is a general type for noun relations in the type hierarchy , and _noun_rel , which is the relation corresponding to the noun noun .</sentence>
				<definiendum id="0">_noun_rel</definiendum>
				<definiens id="0">the relation corresponding to the noun noun</definiens>
			</definition>
			<definition id="24">
				<sentence>The type noun-sign is a subtype of sign , and mass-sign is a subtype of noun-sign ( along with types for count nouns , pair nouns , and so on , which are not shown here ) .</sentence>
				<definiendum id="0">type noun-sign</definiendum>
				<definiens id="0">count nouns , pair nouns , and so on , which are not shown here )</definiens>
			</definition>
			<definition id="25">
				<sentence>90 Lascarides and Copestake Default Representation number mass agrnum sg pl `` noun-sign SS CONT LISZT : noun_rel PLMOD : number/\ [ ~ INST AGR NUM : agrnum/\ [ \ ] mass-sign noun_rel SS CONT LISZT : &lt; PLMOD : mass INST AGR NUM : /sg mass-sign ORTH : clothes F SS CONT LISZT | _clothes_tel : &lt; | INST AGR NUM L ) ORTH : clothing : pl I ~ SS CONT LISZT : &lt; _clothing_rel Figure 16 Relating agreement and semantics for nouns ( SYNSEM is abbreviated SS ) .</sentence>
				<definiendum id="0">SYNSEM</definiendum>
				<definiens id="0">number/\ [ ~ INST AGR NUM : agrnum/\ [ \ ] mass-sign noun_rel SS CONT LISZT : &lt; PLMOD : mass INST AGR NUM : /sg mass-sign ORTH : clothes F SS CONT LISZT | _clothes_tel : &lt; | INST AGR NUM L ) ORTH : clothing : pl I ~ SS CONT LISZT : &lt; _clothing_rel Figure 16 Relating agreement and semantics for nouns</definiens>
			</definition>
			<definition id="26">
				<sentence>We assume that flour-based_rel is a type that is incompatible with all lexical relational types , and thus any explicit object will override this specification .</sentence>
				<definiendum id="0">flour-based_rel</definiendum>
				<definiens id="0">a type that is incompatible with all lexical relational types</definiens>
			</definition>
			<definition id="27">
				<sentence>93 Computational Linguistics Volume 25 , Number 1 Whe &lt; n &gt; discussing the complexity properties of YADU , we have to distinguish between the N operation , which involves the combination of two TDFSs , and DefFS , the calculation of the default structure from a TDFS .</sentence>
				<definiendum id="0">operation</definiendum>
				<definiens id="0">discussing the complexity properties of YADU</definiens>
				<definiens id="1">involves the combination of two TDFSs</definiens>
			</definition>
			<definition id="28">
				<sentence>Checking path-equivalence elements is roughly equivalent to unifying the relevant parts of the indefeasible structure : in the worst case this could amount to unifying two TFSs of ( n 1 ) /2 nodes each per tail-element , where n is the number of nodes in the indefeasible structure .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of nodes in the indefeasible structure</definiens>
			</definition>
			<definition id="29">
				<sentence>The relation ~/+C Q x Q is added to the tuple that currently defines TFSs ( Definition 2 ) , and a fifth condition is added to the four that are already in that definition , which ensures that ~ is a relation of the right sort ( see Carpenter \ [ 1992\ ] ) : • ~/-~C Q x Q is an anti-reflexive and symmetric relation .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiens id="0">ensures that ~ is a relation of the right sort ( see Carpenter \ [ 1992\ ] ) : • ~/-~C Q x</definiens>
				<definiens id="1">an anti-reflexive and symmetric relation</definiens>
			</definition>
			<definition id="30">
				<sentence>Second , the definition of subsumption changes as in Carpenter ( 1992 ) • First , some notation : re ~F re ' means 6 ( r , re ) ~ 6 ( r , re ' ) , Where r is the root node of the TFS .</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">the root node of the TFS</definiens>
			</definition>
			<definition id="31">
				<sentence>However , one can always convert a TFS into a unique most general fully inequated TFS , as defined in Carpenter ( 1992 ) ( where a fully inequated TFS is one where any two incompatibly typed nodes in the TFS stand in the inequality relation defined by ~ , z+ ) .</sentence>
				<definiendum id="0">TFS</definiendum>
				<definiens id="0">always convert a TFS into a unique most general fully inequated TFS</definiens>
				<definiens id="1">one where any two incompatibly typed nodes in the TFS stand in the inequality relation defined by ~</definiens>
			</definition>
			<definition id="32">
				<sentence>Furthermore , every TDFS has a unique most general fully inequated TDFS : it amounts to the unique most general fully inequated TFS , plus the tail .</sentence>
				<definiendum id="0">TDFS</definiendum>
				<definiens id="0">a unique most general fully inequated TDFS : it amounts to the unique most general fully inequated TFS , plus the tail</definiens>
			</definition>
			<definition id="33">
				<sentence>Specifically , the second member of the pairs in the tails , which we have defined as types , should be replaced with specificity information of the relevant sort , and the specificity partition of a tail defined accordingly .</sentence>
				<definiendum id="0">second member of the pairs in the tails</definiendum>
				<definiens id="0">types , should be replaced with specificity information of the relevant sort , and the specificity partition of a tail defined accordingly</definiens>
			</definition>
			<definition id="34">
				<sentence>Then the fine-grained basic TDFS BasicTDFS ( I , ID ) of I and ID is the TDFS I/T , such that : T = { ( F , t ) : t is the root type on ID N I , and F is an atomic TFS such that : ( a ) I ~ F ; ~o ) SDnSEF } The existing definitions of ~ and DefFS will then provide the finer-grained notion 98 Lascarides and Copestake Default Representation of maximal incorporation of default information , from these fine-grained basic TDFSs .</sentence>
				<definiendum id="0">TDFS BasicTDFS</definiendum>
				<definiendum id="1">ID</definiendum>
				<definiendum id="2">F</definiendum>
				<definiens id="0">the TDFS I/T , such that : T = {</definiens>
				<definiens id="1">an atomic TFS such that : ( a</definiens>
			</definition>
			<definition id="35">
				<sentence>Another way in which the definition could be varied would be to omit the generalization step from DeJFS , which ensures that the default result of a TDFS is a single TFS , and to have a credulous variant of DefFS instead , which would be analogous to Carpenter 's ( 1993 ) credulous asymmetric default unification : Definition 21 : Credulous DefFS Let F be a TDFS I/T .</sentence>
				<definiendum id="0">TDFS</definiendum>
				<definiens id="0">a single TFS</definiens>
			</definition>
			<definition id="36">
				<sentence>Then DefFS ( F ) = ( I Flcs &lt; Pfs ( # 1 ) ) r-lcs ... &lt; II-lcs &lt; ~Ofs ( # n ) where { # 1 ... .. # n/ is a specificity partition on T. We argued in Section 1.1 that a unique result is preferable in order to avoid multiplication of disjunctive structures , but disjtmctive results might be useful in cases where there are multiple alternative structures ( e.g. , in modeling dreamed~dreamt , see Russell et al. \ [ 1993\ ] ) .</sentence>
				<definiendum id="0">DefFS</definiendum>
				<definiens id="0">a specificity partition on T. We argued in Section 1.1 that a unique result is preferable in order to avoid multiplication of disjunctive structures</definiens>
			</definition>
			<definition id="37">
				<sentence>So by the definitions of u and Mes : Def~S ( S/T ) = u ( S ncs &lt; ~ , ( W ) ) = S n ~ ( W ) Thus we need to prove that InSD = Sn ~s~ ( T ) But this follows immediately by the definition of T for BasicTDFS ( I , ID ) ( T is the set of atomic TFSs that are subsumed by I rG ID but not subsumed by I ) .</sentence>
				<definiendum id="0">ID ) ( T</definiendum>
				<definiens id="0">the set of atomic TFSs that are subsumed by I rG ID but not subsumed by I )</definiens>
			</definition>
			<definition id="38">
				<sentence>DISCO -- An HPSG-based NLP system and its application for appointment scheduling .</sentence>
				<definiendum id="0">DISCO</definiendum>
				<definiens id="0">An HPSG-based NLP system and its application for appointment scheduling</definiens>
			</definition>
</paper>

		<paper id="4003">
			<definition id="0">
				<sentence>These repairs consist of an editing term , but with no reparandum , as the following example illustrates .</sentence>
				<definiendum id="0">repairs</definiendum>
				<definiens id="0">consist of an editing term , but with no reparandum</definiens>
			</definition>
			<definition id="1">
				<sentence>The corpus consists of six and a half hours of speech produced by 34 different speakers solving 20 different problems .</sentence>
				<definiendum id="0">corpus</definiendum>
				<definiens id="0">consists of six and a half hours of speech produced by 34 different speakers solving 20 different problems</definiens>
			</definition>
			<definition id="2">
				<sentence>We can rewrite W explicitly as the sequence of words WIW2W3 ... WN , where N is the number of words in the sequence .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the sequence of words WIW2W3 ... WN , where</definiens>
				<definiens id="1">the number of words in the sequence</definiens>
			</definition>
			<definition id="3">
				<sentence>The goal of the speech recognition process is to now solve the following : 61b = argmaxPr ( WDIA ) = argmaxPr ( AIWD ) Pr ( WD ) ( 6 ) The first term Pr ( AIWD ) is the acoustic model , which can be approximated by Pr ( AIW ) .</sentence>
				<definiendum id="0">goal of the speech recognition process</definiendum>
				<definiendum id="1">AIWD</definiendum>
				<definiens id="0">the acoustic model</definiens>
			</definition>
			<definition id="4">
				<sentence>algorithm is the form of the questions that it is allowed to ask .</sentence>
				<definiendum id="0">algorithm</definiendum>
				<definiens id="0">the form of the questions that it is allowed to ask</definiens>
			</definition>
			<definition id="5">
				<sentence>For a numeric variable N , the decision tree searches for questions of the form `` is N &gt; = n ' , where n is a numeric constant .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">a numeric constant</definiens>
			</definition>
			<definition id="6">
				<sentence>Perplexity is an estimate of how well the language model is able to predict the next word of a test corpus in terms of the number of alternatives that need to be considered at each point .</sentence>
				<definiendum id="0">Perplexity</definiendum>
				<definiens id="0">an estimate of how well the language</definiens>
			</definition>
			<definition id="7">
				<sentence>For word-based language models , with estimated probability distribution of Pr ( wilwl , i_l ) , the perplexity of a test set Wl , N is calculated as 2 H , where H is the i ~ -- -- 1 log2 Pr ( wilwl , i-1 ) • entropy , which is defined as H = -~ 542 Heeman and Allen Modeling Speakers ' Utterances Table 6 Using richer histories to estimate probabilities .</sentence>
				<definiendum id="0">word-based language models</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">H</definiendum>
				<definiendum id="3">entropy</definiendum>
				<definiens id="0">the perplexity of a test set Wl ,</definiens>
			</definition>
			<definition id="8">
				<sentence>The recall rate is the number of times that the algorithm correctly identifies an event over the total number of times that it actually occurred .</sentence>
				<definiendum id="0">recall rate</definiendum>
				<definiens id="0">the number of times that the algorithm correctly identifies an event over the total number of times that it actually occurred</definiens>
			</definition>
			<definition id="9">
				<sentence>The precision rate is the number of times the algorithm correctly identifies it over the total number of times it identifies it .</sentence>
				<definiendum id="0">precision rate</definiendum>
			</definition>
			<definition id="10">
				<sentence>The error rate is the number of errors in identifying an event over the number of times that the event occurred .</sentence>
				<definiendum id="0">error rate</definiendum>
				<definiens id="0">the number of errors in identifying an event over the number of times that the event occurred</definiens>
			</definition>
			<definition id="11">
				<sentence>In the previous section , we presented a POS-based language model that uses special tags to denote discourse markers .</sentence>
				<definiendum id="0">POS-based language model</definiendum>
			</definition>
			<definition id="12">
				<sentence>For repairs that do not have an editing term , the interruption point is where the local context is disrupted , and hence is the logical place to tag such repairs .</sentence>
				<definiendum id="0">hence</definiendum>
				<definiens id="0">the logical place to tag such repairs</definiens>
			</definition>
			<definition id="13">
				<sentence>Push ET Ei : Pop null if Wi-1 is not part of an editing term but Wi is if Wi-1 and Wi are both part of an editing term if Wi-1 is part of an editing term but Wi is not if neither Wi-1 nor Wi are part of an editing term Below , we give an example and show all non-null editing term and repair tags .</sentence>
				<definiendum id="0">Wi</definiendum>
				<definiens id="0">Pop null if Wi-1 is not part of an editing term but</definiens>
				<definiens id="1">if Wi-1 and Wi are both part of an editing term if Wi-1 is part of an editing term but Wi is not if neither Wi-1 nor Wi are part of an editing term Below</definiens>
			</definition>
			<definition id="14">
				<sentence>We also include two variables that indicate whether we are processing an editing term without forcing it to look for an editing term Push in the utterance context : ETo state indicates whether we are processing an editing term and whether a cue phrase was seen ; and ET-prev indicates the number of editing term words seen so far .</sentence>
				<definiendum id="0">ET-prev</definiendum>
				<definiens id="0">processing an editing term without forcing it to look for an editing term Push in the utterance context</definiens>
				<definiens id="1">the number of editing term words seen so far</definiens>
			</definition>
			<definition id="15">
				<sentence>For Ri E { Mod , Can } and j &lt; i , we define Oq as follows : Onset Wj is the reparandum onset of repair R i Oq = null otherwise We normalize the probabilities to ensure that ~-~j ( Oij = Onset ) = 1 .</sentence>
				<definiendum id="0">Onset Wj</definiendum>
				<definiens id="0">the reparandum onset of repair R i Oq = null otherwise We normalize the probabilities to ensure that ~-~j ( Oij = Onset ) = 1</definiens>
			</definition>
			<definition id="16">
				<sentence>Silence , as well as other acoustic information , can also give evidence as to whether an intonational phrase , speech repair , or editing term occurred , as was shown in Table 8 .</sentence>
				<definiendum id="0">Silence</definiendum>
				<definiens id="0">acoustic information , can also give evidence as to whether an intonational phrase , speech repair</definiens>
			</definition>
			<definition id="17">
				<sentence>The ATIS corpus ( MADCOW 1992 ) , on the other hand , is a collection of human-computer dialogues .</sentence>
				<definiendum id="0">ATIS corpus</definiendum>
				<definiens id="0">a collection of human-computer dialogues</definiens>
			</definition>
			<definition id="18">
				<sentence>The Switchboard corpus ( Godfrey , Holliman , and McDaniel 1992 ) is a collection of human-human dialogues , which are much less constrained and about a much wider domain .</sentence>
				<definiendum id="0">Switchboard corpus</definiendum>
				<definiens id="0">a collection of human-human dialogues</definiens>
			</definition>
			<definition id="19">
				<sentence>We also note that this work is the first proposal that combines the detection and correction of speech repairs , the identification of intonational phrases and discourse markers , and POS tagging , in a framework that is amenable to speech recognition .</sentence>
				<definiendum id="0">POS tagging</definiendum>
				<definiens id="0">combines the detection and correction of speech repairs , the identification of intonational phrases and discourse markers , and</definiens>
			</definition>
			<definition id="20">
				<sentence>They combined normalized syllable duration , length of pauses , pitch contour , and energy using a multilayered perceptron that estimates the probability Pr ( vilci ) , where vi indicates if there is a boundary after the current word and ci is the acoustic features of the neighboring six syllables .</sentence>
				<definiendum id="0">ci</definiendum>
				<definiens id="0">syllable duration , length of pauses , pitch contour , and energy using a multilayered perceptron that estimates the probability Pr ( vilci )</definiens>
			</definition>
			<definition id="21">
				<sentence>In any event , the work of Litman and Hirschberg indicates the usefulness of modeling intermediate phrase boundaries and word accents .</sentence>
				<definiendum id="0">Hirschberg</definiendum>
				<definiens id="0">the usefulness of modeling intermediate phrase boundaries and word accents</definiens>
			</definition>
			<definition id="22">
				<sentence>Conclusion and Future Work In this paper , we redefined the speech recognition language model so that it also identifies POS tags , intonational phrases , and discourse markers , and resolves speech repairs .</sentence>
				<definiendum id="0">Conclusion</definiendum>
				<definiens id="0">tags , intonational phrases , and discourse markers , and resolves speech repairs</definiens>
			</definition>
			<definition id="23">
				<sentence>We have already used our POS-based model to rescore word-graphs , which results in a one percent absolute reduction in word error rate in comparison to a word-based model ( Heeman 1999 ) .</sentence>
				<definiendum id="0">POS-based model to rescore word-graphs</definiendum>
				<definiens id="0">results in a one percent absolute reduction in word error rate in comparison to a word-based model</definiens>
			</definition>
			<definition id="24">
				<sentence>Our full model , which accounts for intonational phrases and speech repairs , should lead to a further reduction , as well as return a richer understanding of the speech .</sentence>
				<definiendum id="0">full model</definiendum>
				<definiens id="0">accounts for intonational phrases and speech repairs</definiens>
			</definition>
</paper>

		<paper id="4004">
			<definition id="0">
				<sentence>A CFG G is a 4-tuple ( N , ~ , R , S ) where N is the set of nonterminals including the start symbol S , ~ is the set of terminal symbols , and R is the set of rules , each of the form A -- * a for A c N and a E ( N U ~ ) * .</sentence>
				<definiendum id="0">CFG G</definiendum>
				<definiendum id="1">~</definiendum>
				<definiendum id="2">R</definiendum>
				<definiendum id="3">N</definiendum>
				<definiendum id="4">E</definiendum>
				<definiens id="0">a 4-tuple ( N , ~ , R , S ) where N is the set of nonterminals including the start symbol S</definiens>
				<definiens id="1">the set of terminal symbols , and</definiens>
				<definiens id="2">the set of rules , each of the form A -- * a for A c</definiens>
			</definition>
			<definition id="1">
				<sentence>n+1\ ] : = FALSE ; for s : = 1 to n/* start position */ for each rule A -+ ws c R chart\ [ s , A , s+l\ ] : = TRUE ; for l : = 2 to n/* length , shortest to longest */ for s : = 1 to n-l+1/*startposition */ for t : = 1 to / 1/* split length */ for each rule A -+ BC ¢ R /* extra TRUE for expository purposes */ chart\ [ s , A , s.l.l\ ] : = chart\ [ s , A , s+l\ ] V ( chart\ [ s , B , s + t\ ] A chart\ [ s ÷ t , C , s + l\ ] A TRUE ) ; return chart\ [ l , S , n+ 1\ ] ; Figure 1 CKY recognition algorithm .</sentence>
				<definiendum id="0">A , s+l\ ] V</definiendum>
				<definiens id="0">s , B , s + t\ ] A chart\ [ s ÷ t , C , s + l\ ] A TRUE</definiens>
			</definition>
			<definition id="2">
				<sentence>A complete semiring is a set of values over which a multiplicative operator and a commutative additive operator have been defined , and for which infinite summations are defined .</sentence>
				<definiendum id="0">complete semiring</definiendum>
				<definiens id="0">a set of values over which a multiplicative operator</definiens>
			</definition>
			<definition id="3">
				<sentence>For this example , we will use the inside 575 Computational Linguistics Volume 25 , Number 4 Item form : \ [ i , A , j\ ] Goal : \ [ 1 , S , n + 1\ ] Rules : R ( A -+ wi ) { i , A , i+l\ ] R ( A -- + BC ) \ [ i , B , k\ ] \ [ k , C , j\ ] \ [ i , A , j\ ] Figure 3 Item-based description of a CKY parser .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">] Goal : \ [ 1 , S , n + 1\ ] Rules : R ( A -+ wi ) { i , A , i+l\ ] R ( A -- + BC ) \ [ i , B , k\ ] \ [ k ,</definiens>
			</definition>
			<definition id="4">
				<sentence>The other grammar deriva~S -- *AA-~A -- *a ~A -- +AA __ ~A -- *a __A -- *a tion is ~ ~ .4.4 ~ aA = &gt; aAA ~ aaA = &gt; aaa , which has value R ( S -- + AA ) ® R ( A -- + a ) ® R ( A -+ AA ) ® R ( A -- + a ) ® R ( A -- -* a ) .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">has value R ( S -- + AA ) ® R ( A -- + a ) ® R ( A -+ AA ) ® R ( A -- + a ) ®</definiens>
			</definition>
			<definition id="5">
				<sentence>We say that ~ Cl ... cj is an instantiation of deduction rule A1 .</sentence>
				<definiendum id="0">cj</definiendum>
				<definiens id="0">an instantiation of deduction rule A1</definiens>
			</definition>
			<definition id="6">
				<sentence>a -- -- A.-+a -- A. -- ~a = : ~ AA : =~ AAA : :~ aAA =~ aaA = : ~ aaa Grammar Derivation R ( S -- + AA ) R ( A ~~-~ a ) a ) Grammar Derivation Tree \ [ 1 , S , 4\ ] R ( S ,4\ ] -- +~A~ R ( A -- ~ a ) R ( A ,3\ ] I I _ R ( A -- +a ) _R ( A~a ) Item Derivation \ ] tee R ( S -~ AA ) ® R ( A ~ AA ) ® R ( A -- + a ) ® R ( A ~ a ) ® R ( A -+ a ) Derivation Value Figure 6 Grammar derivation , grammar derivation tree , item derivation tree , and derivation value .</sentence>
				<definiendum id="0">) R</definiendum>
				<definiens id="0">~a = : ~ AA : =~ AAA : :~ aAA =~ aaA = : ~ aaa Grammar Derivation R ( S -- + AA ) R ( A ~~-~ a ) a ) Grammar Derivation Tree \ [ 1 , S , 4\ ] R ( S ,4\ ] -- +~A~ R ( A -- ~ a</definiens>
				<definiens id="1">S -~ AA ) ® R ( A ~ AA ) ® R ( A -- + a ) ® R ( A ~ a ) ® R ( A -+ a ) Derivation Value Figure 6 Grammar derivation , grammar derivation tree , item derivation tree , and derivation value</definiens>
			</definition>
			<definition id="7">
				<sentence>The base case is rules of the grammar : ( r / is an item derivation tree , where r is a rule of the grammar .</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">an item derivation tree</definiens>
			</definition>
			<definition id="8">
				<sentence>ak , Cl ... Cj respectively , and if ~cl ... cj is the instantiation of a deduction rule , then ( b : D~ 1 ... .. D~k/ is also a derivation tree .</sentence>
				<definiendum id="0">cj</definiendum>
				<definiens id="0">the instantiation of a deduction rule</definiens>
			</definition>
			<definition id="9">
				<sentence>The additive operator kJ produces a union of derivations , and the multiplicative operatorproduces the concatenation , one derivation concatenated with the next .</sentence>
				<definiendum id="0">additive operator kJ</definiendum>
				<definiens id="0">produces a union of derivations</definiens>
			</definition>
			<definition id="10">
				<sentence>Elements of this semiring are a pair , a real number v and a derivation forest E , i.e. , the set of derivations with score v. We define max , the additive operator , as Vit ( v , E ) if v &gt; w max ( ( v , E ) , ( w , D ) ) = ( w , D ) ifv &lt; w Vit ( V , E kJ D ) if v = w In typical practical Viterbi parsers , when two derivations have the same value , one of the derivations is arbitrarily chosen. In practice , this is usually a fine solution , and one that could be used in a real-world implementation of the ideas in this paper , but from a theoretical viewpoint , the arbitrary choice destroys the associative property of the additive operator , max. To preserve associativity , we keep derivation forests of all elements that tie for beret. The definition for max is only defined for two elements. Since the operator is Vit associative , it is clear how to define max for any finite number of elements , but we also Vit need infinite summations to be defined. We use the supremum , sup : the supremum of a set is the smallest value at least as large as all elements of the set ; that is , it is a noncommuting variables. 586 Goodman Semiring Parsing maximum that is defined in the infinite case. We can now define max for the case of vit infinite sums. Let W ~sup V ( v , E &gt; 6X D = { EI &lt; w , E &gt; E X } Then max X = ( w , D/ .</sentence>
				<definiendum id="0">Vit</definiendum>
				<definiens id="0">a real number v and a derivation forest E , i.e. , the set of derivations with score</definiens>
			</definition>
			<definition id="11">
				<sentence>We define x as vit ( v , E I vXit ( w , D &gt; = ( v x w , E. D &gt; where E • D represents the concatenation of the two derivation forests .</sentence>
				<definiendum id="0">E • D</definiendum>
				<definiens id="0">the concatenation of the two derivation forests</definiens>
			</definition>
			<definition id="12">
				<sentence>In this case , inner ( x ) is trivially { ( x/ } , the set containing the single derivation tree x. Thus , V ( x ) = ( ~Dcinner ( x ) V ( D ) = ( ~DC { &lt; x ) } V ( D ) = V ( ( x &gt; ) = R ( x ) The second and third cases occur when x is an item .</sentence>
				<definiendum id="0">inner</definiendum>
				<definiens id="0">~DC { &lt; x ) } V ( D ) = V ( ( x &gt; )</definiens>
			</definition>
			<definition id="13">
				<sentence>al¢ s.t. al'~c , ak DEinner ( aI '' x '' ak ) Consider item derivation trees Dal ... Dak headed by items al ... ak such that ~g~ .</sentence>
				<definiendum id="0">ak DEinner</definiendum>
				<definiens id="0">Consider item derivation trees Dal ... Dak headed by items al ... ak such that ~g~</definiens>
			</definition>
			<definition id="14">
				<sentence>Recall that ( x : Da , ... . , Dakl is the item derivation tree formed by combining each of these trees into a full tree , and notice that U ( x : Dal , ... , Dakl = inner ( ~ ) .</sentence>
				<definiendum id="0">Dakl</definiendum>
				<definiens id="0">the item derivation tree formed by combining each of these trees into a full tree , and notice that U ( x : Dal , ...</definiens>
			</definition>
			<definition id="15">
				<sentence>We can define the Kg generation value of an item x in bucket B , V &lt; _~ ( x , B ) : V &lt; _g ( x , B ) = ( ~ V ( D ) D 6 inner &lt; g ( x , B ) Intuitively , as g increases , for x E B , inner &lt; ~ ( x , B ) becomes closer and closer to inner ( x ) . That is , the finite sum of values in the former approaches the infinite sum of values in the latter. For w-continuous semirings ( which includes all of the semirings considered in this paper ) , an infinite sum is equal to the supremum of the partial sums ( Kuich 1997 , 613 ) . Thus , V ( x ) = ( ~ V ( D ) = sup V &lt; g ( x , B ) OC inner ( x , B ) g It will be easier to compute the supremum if we find a simple formula for V &lt; _g ( x , B ) . Notice that for items x E B , there will be no generation 0 derivations , so V_ &lt; 0 ( x , B ) = the general case : Theorem 3 For x an item in a looping bucket B , and for g ~ 1 , V &lt; g ( x , B ) i=1 \ [ V &lt; _g-l ( ai , B ) al ... ak s.t. al'x '' ak if ai ~ B if ai E B ( 7 ) The proof parallels that of Theorem 2 ( Goodman 1998 ) . A formula for V &lt; _g ( x , B ) is useful , but what we really need is specific techniques for computing the supremum , V ( x ) = supg V &lt; &lt; _g ( x , B ) . For all w-continuous semirings , the supremum of iteratively approximating the value of a set of polynomial equations , as we are essentially doing in Equation 7 , is equal to the smallest solution to the equations ( Kuich 1997 , 622 ) . In particular , consider the equations : k ~V ( ai ) if ai ~ B V &lt; _oo ( x , B ) = 0 ~ \ [ V &lt; _oo ( ai , B ) if a i C B ( 8 ) i:1 al ... ak s.t. al '' x '' at ; 589 Computational Linguistics Volume 25 , Number 4 where V &lt; ~ ( x , B ) can be thought of as indicating \ [ B\ [ different variables , one for each item x in the looping bucket B. Equation 7 represents the iterative approximation of Equation 8 , and therefore the smallest solution to Equation 8 represents the supremum of Equation 7. One fact will be useful for several semirings : whenever the values of all items x E B at generation g + 1 are the same as the values of all items in the preceding generation , g , they will be the same at all succeeding generations , as well. Thus , the value at generation g will be the value of the supremum. Elsewhere ( Goodman 1998 ) , we give a trivial proof of this fact. Now , we can consider various semiring-specific algorithms for computing the supremum. Most of these algorithms are well known , and we have simply extended them from specific parsers ( described in Section 7 ) to the general case , or from one semiring to another. Notice in this section the wide variety of different algorithms , one for each semiring , and some of them fairly complicated. In a conventional system , these algorithms are interweaved with the parsing algorithm , conflating computation of infinite sums with parsing. The result is algorithms that are both harder to understand , and less portable to other semirings. We first examine the simplest case , the Boolean semiring. Notice that whenever a particular item has value TRUE at generation g , it must also have value TRUE at generation g+ 1 , since if the item can be derived in at most g generations then it can certainly be derived in at most g + 1 generations. Thus , since the number of TRUE valued items is nondecreasing , and is at most IB\ [ , eventually the values of all items must not change from one generation to the next. Therefore , for the Boolean semiring , a simple algorithm suffices : keep computing successive generations , until no change is detected in some generation ; the result is the supremum. We can perform this computation efficiently if we keep track of items that change value in generation g and only examine items that depend on them in generation g+l. This algorithm is then similar to the algorithm of Shieber , Schabes , and Pereira ( 1993 ) . For the counting semiring , the Viterbi semiring , and the derivation forest semiring , we need the concept of a derivation subgraph. In Section 2.2 we considered the strongly connected components of the dependency graph , consisting of items that for some sentence could possibly depend on each other , and we put these possibly interdependent items together in looping buckets. For a given sentence and grammar , not all items will have derivations. We will find the subgraph of the dependency graph of items with derivations , and compute the strongly connected components of this subgraph. The strongly connected components of this subgraph correspond to loops that actually occur given the sentence and the grammar , as opposed to loops that might occur for some sentence and grammar , given the parser alone. We call this subgraph the derivation subgraph , and we will say that items in a strongly connected component of the derivation subgraph are part of a loop. Now , we can discuss the counting semiring ( integers under + and x ) . In the counting semiring , for each item , there are three cases : the item can be in a loop ; the item can depend ( directly or indirectly ) on an item in a loop ; or the item does not depend on loops. If the item is in a loop or depends on a loop , its value is infinite. If the item does not depend on a loop in the current bucket , then its value becomes fixed after some generation. We can now give the algorithm : first , compute successive generations until the set of items in B does not change from one generation to the next. Next , compute the derivation subgraph , and its strongly connected components. Items in a strongly connected component ( a loop ) have an infi590 Goodman Semiring Parsing nite number of derivations , and thus an infinite value. Compute items that depend directly or indirectly on items in loops : these items also have infinite value. Any other items can only be derived in finitely many ways using items in the current bucket , so compute successive generations until the values of these items do not change. The method for solving the infinite summation for the derivation forest semiring depends on the implementation of derivation forests. Essentially , that representation will use pointers to efficiently represent derivation forests. Pointers , in various forms , allow one to efficiently represent infinite circular references , either directly ( Goodman 1999 ) , or indirectly ( Goodman 1998 ) . Roughly , the algorithm we will use is to compute the derivation subgraph , and then create pointers analogous to the directed edges in the derivation subgraph , including pointers in loops whenever there is a loop in the derivation subgraph ( corresponding to an infinite number of derivations ) . Details are given elsewhere ( Goodman 1998 ) . As in the finite case , this representation is equivalent to that of Billot and Lang ( 1989 ) . For the Viterbi semiring , the algorithm is analogous to the Boolean case. Derivations using loops in these semirings will always have values no greater than derivations not using loops , since the value with the loop will be the same as some value without the loop , multiplied by some set of rule probabilities that are at most 1. Since the additive operation is max , these lower ( or at most equal ) looping derivations do not change the value of an item. Therefore , we can simply compute successive generations until values fail to change from one iteration to the next. Now , consider implementations of the Viterbi-derivation semiring in practice , in which we keep only a representative derivation , rather than the whole derivation forest. In this case , loops do not change values , and we use the same algorithm as for the Viterbi semiring. In an implementation of the Viterbi-n-best semiring , in practice , loops can change values , but at most n times , so the same algorithm used for the Viterbi semiring still works. Elsewhere ( Goodman 1998 ) , we describe theoretically correct implementations for both the Viterbi-derivation and Viterbin-best semirings that keep all values in the event of ties , preserving addition 's associativity. The last semiring we consider is the inside semiring. This semiring is the most difficult. There are two cases of interest , one of which we can solve exactly , and the other of which requires approximations. In many cases involving looping buckets , all alx deduction rules will be of the form ~- , where al and b are items in the looping bucket , and x is either a rule , or an item in a previously computed bucket. This case corresponds to the items used for deducing singleton productions , such as those Earley 's algorithm uses for rules of the form A -- * B and B -- + A. In this case , Equation 8 forms a set of linear equations that can be solved by matrix inversion. In the more general case , as is likely to happen with epsilon rules , we get a set of nonlinear equations , and must solve them by approximation techniques , such as simply computing successive generations for many iterations. 5 Stolcke ( 1993 ) provides an excellent discussion of these cases , including a discussion of sparse matrix inversion , useful for speeding up some computations. the current generation ; thus , the number of deduction trees over which we are summing grows exponentially with the number of generations : a linear amount of computation yields the sum of the values of exponentially many trees. 591 Computational Linguistics Volume 25 , Number 4 goal Derivation of \ [ goal\ ] Figure 7 Outside algorithm. goal Outer tree of \ [ b\ ] The previous section showed how to compute several of the most commonly used values for parsers , including Boolean , inside , Viterbi , counting , and derivation forest values , among others. Noticeably absent from the list are the outside probabilities , which we define below. In general , computing outside probabilities is significantly more complicated than computing inside probabilities. In this section , we show how to compute outside probabilities from the same item-based descriptions used for computing inside values. Outside probabilities have many uses , including for reestimating grammar probabilities ( Baker 1979 ) , for improving parser performance on some criteria ( Goodman 1996b ) , for speeding parsing in some formalisms , such as data-oriented parsing ( Goodman 1996a ) , and for good thresholding algorithms ( Goodman 1997 ) . We will show that by substituting other semirings , we can get values analogous to the outside probabilities for any commutative semiring ; elsewhere ( Goodman 1998 ) we have shown that we can get similar values for many noncommutative semirings as well. We will refer to these analogous quantities as reverse values. For instance , the quantity analogous to the outside value for the Viterbi semiring will be called the reverse Viterbi value. Notice that the inside semiring values of a hidden Markov model ( HMM ) correspond to the forward values of HMMs , and the reverse inside values of an HMM correspond to the backwards values. Compare the outside algorithm ( Baker 1979 ; Lari and Young 1990 ) , given in Figure 7 , to the inside algorithm of Figure 2. Notice that while the inside and recognition algorithms are very similar , the outside algorithm is quite a bit different. In particular , while the inside and recognition algorithms looped over items from shortest to longest , the outside algorithm loops over items in the reverse order , from longest to shortest. Also , compare the inside algorithm 's main loop formula to the outside algorithm 's main loop formula. While there is clearly a relationship between the two equations , the exact pattern of the relationship is not obvious. Notice that the outside formula is about twice as complicated as the inside formula. This doubled complexity is typical of outside formulas , and partially explains why the item-based description format is so useful : descriptions for the simpler inside values can be developed with relative ease , and then automatically used to compute the twice-as-complicated outside values. 6 the number of terms above the line. Notice that the reverse values equation sums over k times as many terms as the forward values equation. Parsers where all rules have k = 1 terms above the line can only 592 Goodman Semiring Parsing goal Derivation of \ [ goal\ ] Figure 8 goal Outer tree of \ [ b\ ] Item derivation tree of \ [ goal\ ] and outer tree of \ [ b\ ] . For a context-free grammar , using the CKY parser of Figure 3 , recall that the inside probability for an item \ [ i , A , j\ ] is P ( A -~ wi ... wj-1 ) . The outside probability for the same item is P ( S G wl ... Wi_lAWj. , . Wn ) . Thus , the outside probability has the property that when multiplied by the inside probability , it gives the probability that the start symbol generates the sentence using the given item , P ( S G Wl .. , wi_dAwj ... Wn G Wl ... Wn ) . This probability equals the sum of the probabilities of all derivations using the given item. Formally , letting P ( D ) represent the probability of a particular derivation , and C ( D , \ [ i , X , j\ ] ) represent the number of occurrences of item \ [ i , X , j\ ] in derivation D ( which for some parsers could be more than one if X were part of a loop ) , inside ( i , X , j ) x outside ( i , X , j ) = Z P ( D ) C ( D , \ [ i , X , j\ ] ) D a derivation The reverse values in general have an analogous meaning. Let C ( D , x ) represent the number of occurrences ( the count ) of item x in item derivation tree D. Then , for an item x , the reverse value Z ( x ) should have the property V ( x ) ® Z ( x ) = V ( D ) C ( D , x ) ( 9 ) D a derivation Notice that we have multiplied an element of the semiring , V ( D ) , by an integer , C ( D , x ) . This multiplication is meant to indicate repeated addition , using the additive operator of the semiring. Thus , for instance , in the Viterbi semiring , multiplying by a count other than 0 has no effect , since x ® x = max ( x , x ) = x , while in the inside semiring , it corresponds to actual multiplication. This value represents the sum of the values of all derivation trees that the item x occurs in ; if an item x occurs more than once in a derivation tree D , then the value of D is counted more than once. To formally define the reverse value of an item x , we must first define the outer trees outer ( x ) . Consider an item derivation tree of the goal item , containing one or more instances of item x. Remove one of these instances of x , and its children too , leaving a gap in its place. This tree is an outer tree of x. Figure 8 shows an item derivation tree of the goal item , including a subderivation of an item b , derived from terms al ... . , ak. It also shows an outer tree of b , with b and its children removed ; the spot b was removed from is labeled ( b ) . parse regular grammars , and tend to be less useful. Thus , in most parsers of interest , k &gt; 1 , and the complexity of ( at least some ) outside equations , when the sum is written out , is at least doubled .</sentence>
				<definiendum id="0">B )</definiendum>
				<definiendum id="1">B )</definiendum>
				<definiendum id="2">V</definiendum>
				<definiendum id="3">B )</definiendum>
				<definiendum id="4">k ~V</definiendum>
				<definiendum id="5">B</definiendum>
				<definiendum id="6">x</definiendum>
				<definiendum id="7">outside probability</definiendum>
				<definiens id="0">becomes closer and closer to inner ( x ) . That is , the finite sum of values in the former approaches the infinite sum of values in the latter. For w-continuous semirings ( which includes all of the semirings considered in this paper</definiens>
				<definiens id="1">( x ) = ( ~ V ( D ) = sup V &lt; g ( x , B ) OC inner ( x , B ) g It will be easier to compute the supremum if we find a simple formula for V &lt; _g ( x , B ) . Notice that for items x E B , there will be no generation 0 derivations , so V_ &lt; 0 ( x , B ) = the general case : Theorem 3 For x an item in a looping bucket B</definiens>
				<definiens id="2">whenever the values of all items x E B at generation g + 1 are the same as the values of all items in the preceding generation</definiens>
				<definiens id="3">computing the supremum. Most of these algorithms are well known , and we have simply extended them from specific parsers ( described in Section 7 ) to the general case , or from one semiring to another. Notice in this section the wide variety of different algorithms</definiens>
				<definiens id="4">find the subgraph of the dependency graph of items with derivations , and compute the strongly connected components of this subgraph. The strongly connected components of this subgraph correspond to loops that actually occur given the sentence and the grammar , as opposed to loops that might occur for some sentence and grammar , given the parser alone. We call this subgraph the derivation subgraph , and we will say that items in a strongly connected component of the derivation subgraph are part of a loop. Now , we can discuss the counting semiring ( integers under + and x</definiens>
				<definiens id="5">first , compute successive generations until the set of items in B does not change from one generation to the next. Next , compute the derivation subgraph , and its strongly connected components. Items in a strongly connected component ( a loop ) have an infi590 Goodman Semiring Parsing nite number of derivations , and thus an infinite value. Compute items that depend directly or indirectly on items in loops</definiens>
				<definiens id="6">to compute the derivation subgraph , and then create pointers analogous to the directed edges in the derivation subgraph</definiens>
				<definiens id="7">a loop in the derivation subgraph ( corresponding to an infinite number of derivations ) . Details are given elsewhere</definiens>
				<definiens id="8">those Earley 's algorithm uses for rules of the form A -- * B and B -- +</definiens>
				<definiens id="9">previous section showed how to compute several of the most commonly used values for parsers , including Boolean , inside , Viterbi , counting , and derivation forest values</definiens>
				<definiens id="10">the reverse Viterbi value. Notice that the inside semiring values of a hidden Markov model ( HMM ) correspond to the forward values of HMMs</definiens>
				<definiens id="11">the inside and recognition algorithms looped over items from shortest to longest , the outside algorithm loops over items in the reverse order</definiens>
				<definiens id="12">typical of outside formulas , and partially explains why the item-based description format is so useful : descriptions for the simpler inside values can be developed with relative ease , and then automatically used to compute the twice-as-complicated outside values. 6 the number of terms above the line. Notice that the reverse values equation sums over k times as many terms as the forward values equation. Parsers where all</definiens>
				<definiens id="13">for the same item is P ( S G wl ... Wi_lAWj. , . Wn ) . Thus , the outside probability has the property that when multiplied by the inside probability , it gives the probability that the start symbol generates the sentence using the given item , P ( S G Wl .. , wi_dAwj ... Wn G Wl ... Wn ) . This probability equals the sum of the probabilities of all derivations using the given item. Formally , letting P ( D ) represent the probability of a particular derivation</definiens>
				<definiens id="14">the number of occurrences of item \ [ i , X , j\ ] in derivation D ( which for some parsers could be more than one if X were part of a loop )</definiens>
				<definiens id="15">the count ) of item x in item derivation tree D. Then , for an item x , the reverse value Z ( x ) should have the property V ( x ) ® Z ( x ) = V</definiens>
				<definiens id="16">the sum of the values of all derivation trees that the item x occurs in ; if an item x occurs more than once in a derivation tree D</definiens>
			</definition>
			<definition id="16">
				<sentence>Then , the reverse value of an item can be formally defined as Z ( x ) = 0 Z ( D ) ( 10 ) DEouter ( x ) That is , the reverse value of x is the sum of the values of each outer tree of x. Now , we show that this definition of reverse values has the property described by Equation 9 .</sentence>
				<definiendum id="0">reverse value of an item</definiendum>
				<definiendum id="1">Z</definiendum>
				<definiens id="0">( x ) = 0 Z ( D ) ( 10 ) DEouter ( x ) That is , the reverse value of x is the sum of the values of each outer tree of x. Now</definiens>
			</definition>
			<definition id="17">
				<sentence>For certain grammars , Earley 's algorithm examines only a linear number of items and a linear number of dependencies , even though there are O ( n 2 ) possible items , and O ( n 3 ) possible dependencies .</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">a linear number of items and a linear number of dependencies</definiens>
			</definition>
			<definition id="18">
				<sentence>Unlike our version of Earley 's algorithm , Tendeau 's version requires time O ( n L+I ) where L is the length of the longest right-hand side , as opposed to O ( n 3 ) for the classic version , and for our description .</sentence>
				<definiendum id="0">L</definiendum>
				<definiens id="0">the length of the longest right-hand side</definiens>
			</definition>
</paper>

		<paper id="2007">
			<definition id="0">
				<sentence>When Xi assumes a word or a special symbol `` 0 '' as its value , we refer to the corresponding model as a word-based model .</sentence>
				<definiendum id="0">Xi</definiendum>
				<definiens id="0">assumes a word or a special symbol</definiens>
			</definition>
			<definition id="1">
				<sentence>A dependency forest model can be represented by a dependency forest ( i.e. , a set of dependency trees ) , whose nodes represent random variables ( each labeled with a number of parameters ) , and whose directed links represent the dependencies that exist between these random variables .</sentence>
				<definiendum id="0">dependency forest model</definiendum>
				<definiens id="0">a set of dependency trees ) , whose nodes represent random variables ( each labeled with a number of parameters ) , and whose directed links represent the dependencies that exist between these random variables</definiens>
			</definition>
			<definition id="2">
				<sentence>is defined as 2 H ( PT'PM ) , H ( PT , PM ) = -~-~x PT ( X ) • logPM ( x ) , where PM ( X ) denotes the estimated model , PT ( X ) the empirical distribution of the test data .</sentence>
				<definiendum id="0">H ( PT'PM ) , H ( PT , PM ) = -~-~x PT ( X ) • logPM ( x )</definiendum>
				<definiendum id="1">PM ( X )</definiendum>
			</definition>
</paper>

		<paper id="2002">
			<definition id="0">
				<sentence>The marginal distribution of FV1 and CV is the full joint distribution `` collapsed '' over FV2 .</sentence>
				<definiendum id="0">CV</definiendum>
				<definiens id="0">the full joint distribution `` collapsed '' over FV2</definiens>
			</definition>
			<definition id="1">
				<sentence>A probability model ( more specifical134 its parametric form ) expresses the relationships among the variables of the model , and specifies a family of distributions -- all distributions in which those relationships hold .</sentence>
				<definiendum id="0">probability model</definiendum>
				<definiens id="0">expresses the relationships among the variables of the model , and specifies a family of distributions -- all distributions in which those relationships hold</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , the model in which FV1 is conditionally independent of FV2 given the value of CV is the family of all distributions for vector X in which this constraint holds .</sentence>
				<definiendum id="0">CV</definiendum>
				<definiens id="0">the family of all distributions for vector X in which this constraint holds</definiens>
			</definition>
			<definition id="3">
				<sentence>A probabilistic model ( a parametric form complete with parameter estimates ) forms the basis of a probabilistic classifier .</sentence>
				<definiendum id="0">probabilistic model</definiendum>
				<definiens id="0">a parametric form complete with parameter estimates ) forms the basis of a probabilistic classifier</definiens>
			</definition>
			<definition id="4">
				<sentence>Graphical models are the subset of log-linear models in which the only kind of noninteraction is conditional independence ( Whittaker 1990 ) .</sentence>
				<definiendum id="0">Graphical models</definiendum>
			</definition>
			<definition id="5">
				<sentence>Therefore , the maximal sets of interdependent variables correspond exactly to the cliques of the graph ( where a clique is a maximal fully connected component ) .</sentence>
				<definiendum id="0">clique</definiendum>
				<definiens id="0">a maximal fully connected component )</definiens>
			</definition>
			<definition id="6">
				<sentence>A chord is an edge between nonadjacent nodes in the cycle .</sentence>
				<definiendum id="0">chord</definiendum>
				<definiens id="0">an edge between nonadjacent nodes in the cycle</definiens>
			</definition>
			<definition id="7">
				<sentence>The fit of the model is how closely the counts observed in a training sample correspond to those that would be expected if the model being tested were the true population model .</sentence>
				<definiendum id="0">fit of the model</definiendum>
				<definiens id="0">how closely the counts observed in a training sample correspond to those that would be expected if the model being tested were the true population model</definiens>
			</definition>
			<definition id="8">
				<sentence>These include Pearson 's X 2 , the Kullback-Leibler information divergence D , which is also known as cross entropy ; and the log-likelihood ratio statistic , G 2 .</sentence>
				<definiendum id="0">Kullback-Leibler</definiendum>
				<definiens id="0">information divergence D , which is also known as cross entropy ; and the log-likelihood ratio statistic , G 2</definiens>
			</definition>
			<definition id="9">
				<sentence>In the general case , D is used to evaluate the difference between any two density functions gy and fy for the same random vector Y. When D is used to evaluate model fit , gy is the distribution of Y in the data sample , fy is the distribution of Y predicted by the model , and G 2 is 2N x D ( gy ; fy ) .</sentence>
				<definiendum id="0">gy</definiendum>
				<definiendum id="1">fy</definiendum>
				<definiens id="0">used to evaluate the difference between any two density functions gy and fy for the same random vector Y. When D is used to evaluate model fit ,</definiens>
				<definiens id="1">the distribution of Y predicted by the model</definiens>
			</definition>
			<definition id="10">
				<sentence>Model selection can be based directly on the value of a goodness-of-fit statistic , or it can be based on a cost function that combines a goodness-of-fit statistic with a penalty for model complexity , such as the Akaike information criterion ( AIC ) ( Akaike 1974 ) or the Bayesian information criterion ( BIC ) ( Schwarz 1978 ) .</sentence>
				<definiendum id="0">Model selection</definiendum>
				<definiens id="0">Akaike information criterion ( AIC ) ( Akaike 1974 ) or the Bayesian information criterion ( BIC ) ( Schwarz 1978 )</definiens>
			</definition>
			<definition id="11">
				<sentence>A naive Bayes model includes edges between the classification variable and each feature variable ( and contains no other edges ) .</sentence>
				<definiendum id="0">naive Bayes model</definiendum>
				<definiens id="0">includes edges between the classification variable and each feature variable ( and contains no other edges )</definiens>
			</definition>
</paper>

		<paper id="3002">
			<definition id="0">
				<sentence>The position of the syntax error can be defined as the rightmost symbol of the shortest prefix of the input that can not be extended to be a correct sentence in the language L. In formal notation , this prefix for a given erroneous input w ~ L is defined as the string va , where w = vax , for some x , such that vy E L , for some y , but vaz ~ L , for any z. ( The symbols v , w ... . denote strings , and a denotes an input symbol . )</sentence>
				<definiendum id="0">position of the syntax error</definiendum>
				<definiens id="0">the rightmost symbol of the shortest prefix of the input that can not be extended to be a correct sentence in the language L. In formal notation</definiens>
				<definiens id="1">the string va , where w = vax , for some x , such that vy E L , for some y , but vaz ~ L , for any z.</definiens>
			</definition>
			<definition id="1">
				<sentence>A tree-adjoining grammar is a 4-tuple ( ~ , NT , L A ) , where ~ is the set of terminals , I is the set of initial trees , and A is the set of auxiliary trees .</sentence>
				<definiendum id="0">tree-adjoining grammar</definiendum>
				<definiendum id="1">~</definiendum>
				<definiendum id="2">A</definiendum>
				<definiens id="0">the set of terminals</definiens>
			</definition>
			<definition id="2">
				<sentence>• an , where n is the length of the input .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the input</definiens>
			</definition>
			<definition id="3">
				<sentence>Here , N is a node from some elementary tree t , and o~fl is the list of the daughter nodes of N. The daughters in o~ together generate the input between positions i and j. The whole elementary tree generates input from position h onwards .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a node from some elementary tree t</definiens>
			</definition>
			<definition id="4">
				<sentence>R is the root of some initial tree .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">the root of some initial tree</definiens>
			</definition>
			<definition id="5">
				<sentence>The second and third completer steps complete recognition of a list of daughter nodes '7 , and initiate recognition of the list of nodes fl to the right of the mother node of % \ [ h , M -- *'7. , k , l , f~ , fd\ ] , t E Adj ( M ) , ~ , Ft -- ~ , _L , k , k , - , -\ ] , \ [ h , N-+a.Mfl , i , j , f~ , f2\ ] ~ , F t -- -+ l. , k , l , k , I\ ] ( Comp 1 ) 351 Computational Linguistics Volume 25 , Number 3 L h i j k l Comp 1 Figure 8 h ij Two of the completer steps .</sentence>
				<definiendum id="0">k</definiendum>
				<definiendum id="1">k , I\ ]</definiendum>
				<definiens id="0">nodes '7 , and initiate recognition of the list of nodes fl to the right of the mother node of % \ [ h , M -- *'7. , k , l</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>Finite parse trees , or parses , generated by a context-free grammar ( CFG ) can be equipped with a variety of probability distributions .</sentence>
				<definiendum id="0">Finite parse</definiendum>
				<definiens id="0">trees , or parses , generated by a context-free grammar ( CFG ) can be equipped with a variety of probability distributions</definiens>
			</definition>
			<definition id="1">
				<sentence>More specifically , denote a finite parse tree by T. For any production rule A ~ a of the CFG , letf ( A ~ a ; ~- ) be the number of times it occurs in T. Let R be the set of all production rules .</sentence>
				<definiendum id="0">letf</definiendum>
				<definiens id="0">the number of times it occurs in T. Let R be the set of all production rules</definiens>
			</definition>
			<definition id="2">
				<sentence>Entropy is a measure of the uncertainty of a probability distribution .</sentence>
				<definiendum id="0">Entropy</definiendum>
				<definiens id="0">a measure of the uncertainty of a probability distribution</definiens>
			</definition>
			<definition id="3">
				<sentence>We may ask how much one can learn from the sentence `` T is a random sample from S. '' At one extreme , let the distribution on S be p ( ~-l ) = 1 , and P ( ; i ) = O , for i ~ 1 .</sentence>
				<definiendum id="0">T</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">a random sample from S. '' At one extreme</definiens>
			</definition>
			<definition id="4">
				<sentence>Entropy plays a central role in the theory of information .</sentence>
				<definiendum id="0">Entropy</definiendum>
				<definiens id="0">plays a central role in the theory of information</definiens>
			</definition>
			<definition id="5">
				<sentence>Conceptually , having finite entropy is a basic requirement for a `` good '' probabilistic model because a probability distribution with infinite entropy has too much uncertainty to be informative .</sentence>
				<definiendum id="0">finite entropy</definiendum>
			</definition>
			<definition id="6">
				<sentence>The difference between the second moment and the mean squared is the variance of sizes , which tells us how `` scattered '' sizes of parses are distributed around the mean .</sentence>
				<definiendum id="0">mean squared</definiendum>
			</definition>
			<definition id="7">
				<sentence>We apply the Kullback-Leibler divergence , which is the information distance between two probability distributions , to prove the convergence of the approximation .</sentence>
				<definiendum id="0">Kullback-Leibler divergence</definiendum>
				<definiens id="0">the information distance between two probability distributions , to prove the convergence of the approximation</definiens>
			</definition>
			<definition id="8">
				<sentence>Definition 1 A context-free grammar ( CFG ) G is a quadruple ( N , T , R , S ) , where N is the set of variables , T the set of terminals , R the set of production rules , and S E N is the start symbol .</sentence>
				<definiendum id="0">context-free grammar ( CFG ) G</definiendum>
				<definiendum id="1">S )</definiendum>
				<definiendum id="2">N</definiendum>
				<definiendum id="3">S E N</definiendum>
				<definiens id="0">a quadruple ( N , T , R ,</definiens>
				<definiens id="1">the set of variables , T the set of terminals , R the set of production rules</definiens>
				<definiens id="2">the start symbol</definiens>
			</definition>
			<definition id="9">
				<sentence>Let f~ denote the set of finite parse trees of G , an element of which is always denoted as r. For each 7 c f~ and each production rule ( A -- + o~ ) E R , definef ( A -- + o4 7 ) to be the number of occurrences , or frequency , of the rule in r , and f ( A ; r ) to be the number of occurrences of A in r. f ( A ; r ) and f ( A -- + c~ ; r ) are related by f ( A ; v ) : E f ( A -- -* o4 7 ) .</sentence>
				<definiendum id="0">definef</definiendum>
				<definiendum id="1">frequency</definiendum>
				<definiendum id="2">f</definiendum>
			</definition>
			<definition id="10">
				<sentence>For any A E N and any sentential form `` 7 C ( N tO T ) * , define n ( A ; ~ ) as the number of instances of A in % Define I'7\ ] as the length of the sentential form .</sentence>
				<definiendum id="0">E N</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the length of the sentential form</definiens>
			</definition>
			<definition id="11">
				<sentence>If A E T , let T A be the left-most maximum subtree of T rooted in A , which is the subtree of 7 rooted in A satisfying the condition that if ; ' ~ 7A is also a subtree of v rooted in A , then 7 ~ is either a subtree of `` rA , or a right sibling of 7A , or a subtree of a right sibling of 7A .</sentence>
				<definiendum id="0">E T</definiendum>
				<definiens id="0">the left-most maximum subtree of T rooted in A</definiens>
			</definition>
			<definition id="12">
				<sentence>Let AT be the root of rA , which is the left-most `` shallowest '' instance of A in r. Definition 3 For any two symbols A E N and B c N tO T , not necessarily different , B is said to be reachable from A in G , if there is a sequence of symbols A0 , A1 ... .. An with A0 = A and An = B , and a sequence of sentential forms oe0 ... .. o~n-1 , such that each Ai -+ oq is a production in R and each Oq contains the next symbol Ai+l. G is called connected if all elements in N U T can be reached from all nonterminal symbols .</sentence>
				<definiendum id="0">E N</definiendum>
				<definiens id="0">symbols A0 , A1 ... .. An with A0 = A and An = B , and a sequence of sentential forms oe0 ... .. o~n-1</definiens>
				<definiens id="1">a production in R and each Oq contains the next symbol Ai+l. G is called connected if all elements in N U T can be reached from all nonterminal symbols</definiens>
			</definition>
			<definition id="13">
				<sentence>Suppose p is a distribution on fL For any two symbols A C N and B c NU T , not necessarily different , B is said to be reachable from A in G under p , if there is a T E f~ with p ( T ) &gt; 0 and there is a subtree r ' of 7 , such that 7 ' is rooted in A and B E 7 ' .</sentence>
				<definiendum id="0">Suppose p</definiendum>
				<definiens id="0">a distribution on fL For any two symbols A C N and B c NU T , not necessarily different</definiens>
			</definition>
			<definition id="14">
				<sentence>It can be proved that the ML estimate fi is given by n x~'~Ef ) \ [ f ( A ~ o~ ; r ) lr E f~ri\ ] ~O ( A -- * oz ) = i=1 , ( 6 ) ~ Ep\ [ f ( A ; r ) lw C f~wi\ ] i=1 where fy is the set of all parses with yield Y , i.e. , f~w = { r E f~ : Y ( r ) = Y } .</sentence>
				<definiendum id="0">fy</definiendum>
				<definiens id="0">the set of all parses with yield Y</definiens>
			</definition>
			<definition id="15">
				<sentence>For each A E V , letf ( A ; T ) be the number of nonroot instances of A in T. Given a E ( V U T ) * , let ai be the ith symbol of the sentential form a. For any A E V qA = P ( ~J ( a ~ ) caU { derivati°nbeginsA -- ~°qandc~ifailst°terminate } ) i = E p ( A -- ~a ) p ( LJ { oqfailstoterminate } ) ( A. -- ~a ) ER - &lt; E p ( A ~ a ) E p ( { ai fails to terminate } ) ( A -- -~a ) ER i = Z p ( A ~ a ) En ( B ; a ) qe ( A -- ~a ) ~a BEV 139 Computational Linguistics = E q B { ~ ( A._ , ~ ) eRn ( B ; a ) ~_EAf ( A -- + a ; ~- ) W ( T ) } BEV ~EAf ( A ; T ) W ( T ) Volume 25 , Number 1 qA Ef ( A ; T ) W ( T ) _ &lt; Eqs E E n ( B ; a ) f ( A -- 4 a ; v ) W ( q- ) TEA BEV tEA ( A -- .</sentence>
				<definiendum id="0">E V , letf</definiendum>
				<definiendum id="1">T</definiendum>
				<definiendum id="2">A._ , ~ ) eRn</definiendum>
				<definiens id="0">A -- + a ; ~- ) W ( T ) } BEV ~EAf ( A ; T ) W ( T ) Volume 25 , Number 1 qA Ef ( A ; T ) W ( T ) _ &lt; Eqs E E n ( B ; a ) f ( A -- 4 a ; v ) W ( q- ) TEA BEV tEA ( A --</definiens>
			</definition>
			<definition id="16">
				<sentence>-~oOER Sum over A E V : E qA Ef ( A ; T ) WO ' ) _ &lt; E qB E E E n ( B ; a ) f ( A ~ a ; T ) W ( T ) AEV TEA BEV ~-EA AEV ( A -- -~o~ ) ER BEV ~-EA i , e. !</sentence>
				<definiendum id="0">-~oOER Sum</definiendum>
				<definiendum id="1">) f</definiendum>
				<definiens id="0">over A E V : E qA Ef ( A ; T ) WO ' ) _ &lt; E qB E E E n ( B ; a</definiens>
			</definition>
			<definition id="17">
				<sentence>Proof For any A e N , there is a ~E A such that A E T. Since p ( ~- ) &gt; 0 , this implies A is reachable from S under p. Using the notation given in Definition 2 , we have qs &gt; _ p ( { A E 7 and TA fails to terminate } ) = p ( { TA fails to terminate } lA E T ) p ( A ¢ q- ) p ( { TA fails to terminate } lA ¢ ~- ) = 0 , since qs = 0 and p ( A ¢ ~- ) &gt; 0 .</sentence>
				<definiendum id="0">TA</definiendum>
				<definiens id="0">fails to terminate } ) = p ( { TA fails to terminate } lA E T ) p ( A ¢ q- ) p ( { TA fails to terminate } lA ¢ ~- ) = 0 , since qs = 0 and p ( A ¢ ~-</definiens>
			</definition>
			<definition id="18">
				<sentence>Proof For the first equation , EF ( A ) = E Ef ( A ; T ) W ( T ) = E Ef ( A ; T ) W ( T ) AEN AENTEA rEAAEN For the second equation , = Z TEA Z Z BEN 3 ' s.t. ( B -- -- .7 ) ER F ( B~7 ) n ( A ; 7 ) = ~ E ~f ( B-~v ; T ) WO- ) n ( A ; 'Y ) BEN '3 ' s.t. `` rcA ( B~ '' y ) cR = EW ( T ) E E f ( B~3 , ; rln ( A ; 7 ) tEA BEN 3 ' s.t. ( B -- ~7 ) ER ( 12 ) 141 Computational Linguistics Volume 25 , Number 1 For each A , E E f ( B~9 ; ~ ' ) n ( A ; 9 ) BEN 7 s.t. ( B -- -+7 ) ER is the number of nonroot instances of A in ~- .</sentence>
				<definiendum id="0">rln</definiendum>
				<definiendum id="1">A , E E f</definiendum>
				<definiendum id="2">B -- -+7 ) ER</definiendum>
				<definiens id="0">A ) = E Ef ( A ; T ) W ( T ) = E Ef ( A ; T ) W ( T ) AEN AENTEA rEAAEN For the second equation , = Z TEA Z Z BEN 3 ' s.t. ( B -- -- .7 ) ER F ( B~7 ) n ( A ; 7 ) = ~ E ~f ( B-~v ; T ) WO- ) n ( A ; 'Y ) BEN '3 ' s.t. `` rcA ( B~ '' y ) cR = EW ( T ) E E f ( B~3 , ;</definiens>
				<definiens id="1">the number of nonroot instances of A in ~-</definiens>
			</definition>
			<definition id="19">
				<sentence>For each A E N and k E N , define Mk , A E P A ( r ) irjm '' TErrA hO- ) &lt; k It is easy to check Mk+l , A z L E E ( 1 + E \ [ TiI ) mp ( A ~ °~ ) P~I ( T1 ) '' ' `` PaL ( TL ) '' aE ( NuT ) * T1 , -.. , TL i=1 ( A -- ~a ) ER TiE~c~i h ( Ti ) &lt; k ( 13 ) where for ease of typing , we write L for la\ [ . For fixed o~ , write L L ( 1 + E ITii ) m ~P ( \ ] 7 '' l\ ] ... . , \ ] TLI ) + E \ ] Tilm '' i=1 i=1 P is a polynomial in ITll ... .. 17LI , each term of which is of the form ITllS ' IT2I s : ... iTLi sa , 0 ~ 5i &lt; m , s1 +82+ ... SL ~ m. ( 14 ) By induction hypothesis , there is a constant C &gt; 1 , such that for all 0 &lt; s &lt; m and AENUT , pA ( ~- ) \ [ T\ ] s = EpA\ ] T\ ] s &lt; C. `` rE~A Then for each term with the form given in ( 14 ) , E TIi ... pTL TiEf~a i L = II - &lt; i=1 TiE'co i 142 Chi Probabilistic Context-Free Grammars There are less than L m = la\ [ m terms in P ( Inl ... .. D-LI ) .</sentence>
				<definiendum id="0">E N</definiendum>
				<definiendum id="1">Mk</definiendum>
				<definiendum id="2">TL i=1</definiendum>
				<definiens id="0">] 7 '' l\ ] ... . , \ ] TLI ) + E \ ] Tilm '' i=1 i=1 P is a polynomial in ITll ... .. 17LI , each term of which is of the form ITllS ' IT2I s : ... iTLi sa</definiens>
			</definition>
			<definition id="20">
				<sentence>aE ( NUT ) * ~E ( NuT ) * i=1 s.t. ( A-+a ) ER s.t. ( A -- ~a ) ER Because the set of production rules is finite , the length of a sentenfial form that occurs on the right-hand side of a production rule is upper bounded , i.e. , sup { iaI : for some AEN , ( A~a ) ER } &lt; ~ .</sentence>
				<definiendum id="0">aE</definiendum>
				<definiens id="0">finite , the length of a sentenfial form that occurs on the right-hand side of a production rule is upper bounded</definiens>
			</definition>
			<definition id="21">
				<sentence>By ( 10 ) and ( 11 ) , we then get ~Mk+i , AF ( A ) _ &lt; K~\ ] TIW ( T ) + ~ ~ ~Mk , c~iF ( A-+c 0 ( by ( 10 ) ) AEN ~-EA AEN aE ( NUT ) * i=1 s.t. ( A -- ~a ) ER = K ~ I~-\ ] W ( -r ) + ~ ~ ~ n ( B ; a ) Mk , BF ( A -- ~ a ) -tEA AEN o~E ( NuT ) * BEN s.t. ( A -- ~oOER TEA = K~ Hwff ) + tEA Mk , B ~ ~ n ( B ; a ) F ( A -~ a ) BEN AcN aE ( NuT ) * s.t. ( A -- -~oOER ~Mk , BF ( B ) + Mk , s ( F ( S ) 1 ) .</sentence>
				<definiendum id="0">AF</definiendum>
				<definiens id="0">A -- ~ a ) -tEA AEN o~E ( NuT ) * BEN s.t. ( A -- ~oOER TEA = K~ Hwff ) + tEA Mk , B ~ ~ n ( B ; a ) F ( A -~ a ) BEN AcN aE ( NuT ) * s.t. ( A -- -~oOER ~Mk</definiens>
			</definition>
			<definition id="22">
				<sentence>Proposition 3 Under the same conditions of Proposition 2 , the mean frequency of the production rule ( A ~ o~ ) c R is the weighted sum of the numbers of its occurrences in parses of A , with weights W ( T ) , i.e. , Epf ( A ~ a ; ; ) = Ef ( A ~ c~ ; T ) W ( T ) TcA ( 17 ) Proof Fix ( A ~ oL ) E R. For each C c N , write E ( C ) for Epcf ( A ~ c~ ; T ) .</sentence>
				<definiendum id="0">~ o~ ) c R</definiendum>
				<definiens id="0">the weighted sum of the numbers of its occurrences in parses of A , with weights W ( T )</definiens>
				<definiens id="1">A ~ c~ ; T ) W ( T ) TcA ( 17 ) Proof Fix ( A ~ oL ) E R. For each C c N , write E ( C ) for Epcf ( A ~ c~ ; T )</definiens>
			</definition>
			<definition id="23">
				<sentence>Then 1 , % A PA ( 'r ) Z ) , ( 1-A ) e ~'f ( T ) -Z~A ) e -~ fie `` ~'fO'k ) , k=l where Tk is the daughter subtree of T rooted in ilkEach ~-k has height &lt; h. Hence , by induction assumption , 1 e~ .</sentence>
				<definiendum id="0">PA</definiendum>
				<definiendum id="1">Tk</definiendum>
				<definiens id="0">the daughter subtree of T rooted in ilkEach ~-k has height &lt; h. Hence , by induction assumption , 1 e~</definiens>
			</definition>
			<definition id="24">
				<sentence>Adopting the set-up given by Miller and O'Sullivan ( 1992 ) , we define the mean matrix M of p as a \ ] N I x I N\ ] square matrix , with its ( A , B ) th entry being the expected number of variables B resulting from rewriting A : M ( A , B ) = ~ p ( A ~ o~ ) n ( B ; oL ) .</sentence>
				<definiendum id="0">~ o~</definiendum>
				<definiens id="0">the mean matrix M of p as a \ ] N I x I N\ ] square matrix , with its ( A , B ) th entry being the expected number of variables B resulting from rewriting A : M ( A , B ) = ~ p</definiens>
			</definition>
			<definition id="25">
				<sentence>Then a ( M ) = lin~ IiMnII l/n , 152 Chi Probabilistic Context-Free Grammars where I\ ] MII is the norm of M defined by \ [ IMII = sup IM~I. IOr=l Now we can prove the following result .</sentence>
				<definiendum id="0">Chi Probabilistic Context-Free Grammars</definiendum>
				<definiendum id="1">MII</definiendum>
				<definiens id="0">the norm of M defined by \</definiens>
			</definition>
			<definition id="26">
				<sentence>Then , instead of smnming over A , we observe that ( 15 ) can be written as Mk+l , A ~ K + ~ M ( A , B ) Mk , B. BcN Write { Mk , A } A~N as 1V\ [ k , which is a vector indexed by A E N. We then have fitk+l &lt; K1 + M/qt~ , where 1 is defined as { 1 ... .. 1 } , and for two-column vectors//and 17 , //_ &lt; 17 means each component of # is _ &lt; the corresponding component of t , .</sentence>
				<definiendum id="0">Mk+l</definiendum>
				<definiendum id="1">~ K + ~ M ( A , B ) Mk , B. BcN Write { Mk</definiendum>
				<definiendum id="2">} A~N</definiendum>
				<definiendum id="3">k</definiendum>
				<definiens id="0">a vector indexed by A E N. We then have fitk+l &lt; K1 + M/qt~ , where 1 is defined as { 1 ... .. 1 } , and for two-column vectors//and 17</definiens>
			</definition>
			<definition id="27">
				<sentence>There we got the relation qs _ &gt; qaps ( A C T ) , where qa is the probability that trees rooted in A fail to terminate .</sentence>
				<definiendum id="0">C T</definiendum>
				<definiens id="0">the probability that trees rooted in A fail to terminate</definiens>
			</definition>
			<definition id="28">
				<sentence>Similarly , we can prove p ( f~A ) _ &gt; p ( f~S ) pA ( S E ~ ' ) &gt; O. For each A , define generating functions { gA } as in Harris ( 1963 , Section 2.2 ) , gA ( S ) = Z p ( A ~ c~ ) H s~ ( B ; ~ ) ~ ( 32 ) ac ( NuT ) * BEN s.t. ( A~c~ ) ER where s = { SA } A~N. Write g = { ga } acx and define g ( n ) =_ { g~ ) } recursively as g Is/ `` \ [ S ( 33 ) It is easy to see that ga ( O ) is the total probability of parses with root A and height Therefore , g ( A n ) ( 0 ) T p ( f~a ) &lt; 1 .</sentence>
				<definiendum id="0">g</definiendum>
				<definiens id="0">the total probability of parses with root A and height Therefore</definiens>
			</definition>
			<definition id="29">
				<sentence>Write r = { P ( f~A ) } A~N. Then g ( r ) = g ( nlL % g ( n ) ( o ) ) = nl~mcx g ( g ( n ) ( o ) ) = nlL % g ( nq-1 ) ( 0 ) = r. Therefore , r is a nonnegative solution of g ( s ) = s. It is also the smallest among such solutions .</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">a nonnegative solution of g ( s ) = s. It is also the smallest among such solutions</definiens>
			</definition>
			<definition id="30">
				<sentence>Then f ( sl = II s ; tiE ( NUT ) * BEN s.t. ( A-+a ) ER 154 Chi Probabilistic Context-Free Grammars Z erE ( NuT ) * s.t. ( A -- -~a ) cR 1 ~AP ( A~ ol ) H r~ ( B ; '~ ) H S~ ( B ; ~ ) ' BEN BEN ( 34 ) For two vectors r = { rA } and { SA } , write rs for { rASA } , and r/s for { rA/SA } .</sentence>
				<definiendum id="0">tiE</definiendum>
				<definiens id="0">A~ ol ) H r~ ( B ; '~ ) H S~ ( B ; ~</definiens>
			</definition>
			<definition id="31">
				<sentence>Then h ' ( O ) = Mu-u , where M is the mean matrix corresponding to ~ .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">the mean matrix corresponding to ~</definiens>
			</definition>
			<definition id="32">
				<sentence>One way to do this is to pick a large finite subset f~ ' of f~ and replace the fraction by Ep ( f ( A -- + ~ ; ~- ) 1~e ~ ' ) Ep ( f ( A ; ~- ) ir C ~ ' ) where Ep ( f ( A ~ a ; T ) IT C f~ ' ) is the conditional expectation of f ( A -~ c~ ; r ) given f~ ' , which is defined as E f ( A -~ c~ ; r ) p ( T ) Ep ( f ( A -~ ~ ; ~- ) 1~ ~ ~ ' ) = ~*n ' TEf~ ' Because f~l is finite , the top of the fraction on the right-hand side is finite .</sentence>
				<definiendum id="0">E f</definiendum>
				<definiendum id="1">side</definiendum>
				<definiens id="0">to pick a large finite subset f~ ' of f~ and replace the fraction by Ep ( f ( A -- + ~ ; ~- ) 1~e ~ ' ) Ep ( f ( A ; ~- ) ir C ~ ' ) where Ep ( f ( A ~ a ; T ) IT C f~ ' ) is the conditional expectation of f ( A -~ c~ ; r ) given f~ '</definiens>
				<definiens id="1">A -~ c~ ; r ) p ( T ) Ep</definiens>
				<definiens id="2">finite , the top of the fraction on the right-hand</definiens>
			</definition>
			<definition id="33">
				<sentence>Lemma 2 If f~ ' is an arbitrary subset of f~ , then D ( plIq ) &gt; p ( fY ) log P ( fY ) 1 p ( fY ) _ q~ + ( 1 p ( f~ ' ) ) log1 q ( f~ ' ) ' Proof Consider the Kullback-Leibler divergence between the conditional distributions p ( TlfY ) and q ( rlfY ) , which equals E p ( T\ ] fY ) log P ( TIfY ) -1 p ( T ) `` P ( fY ) &gt; 0 ; ca ' q ( T\ [ f~ ' -- -- -- -- ) p ( f2 ' ) TEn ' ~ p ( r ) log q~ -'og q~ __ p ( r ) &gt; p ( fy ) , p ( fY ) E p ( 7 ) log q~ _ , og q~ .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">an arbitrary subset of f~ , then D ( plIq ) &gt; p ( fY ) log P ( fY ) 1 p ( fY ) _ q~ + ( 1 p ( f~ '</definiens>
			</definition>
			<definition id="34">
				<sentence>rein Sum both sides over all a E ( N U T ) * such that ( A -+ a ) E R. By the constraints ( 37 ) , AA = Z f ( A ; r ) P ( rlf~n ) '' TEfl~ Therefore we prove that/f there is a minimizer , it has to be pn , where Zf ( A~a ; r ) p ( r ) pn ( A -- -+~ ) = rE~ .</sentence>
				<definiendum id="0">Sum</definiendum>
				<definiens id="0">both sides over all a E ( N U T ) * such that ( A -+ a ) E R. By the constraints ( 37 )</definiens>
			</definition>
			<definition id="35">
				<sentence>ZU ( A ; T ) p ( T ) rein To see that there is a minimizer of Kn ( q ) subject to ( 37 ) , consider the boundary points of the region { q_ &gt; 0 , Z q ( A -- ~ o~ ) = 1 } c¢ s.t. ( A -- ~a ) ER Any boundary point of the region has a component equal to zero , hence for some r E fin , q ( T ) = 0 , implying Kn ( q ) = ~ .</sentence>
				<definiendum id="0">ZU</definiendum>
				<definiendum id="1">implying Kn</definiendum>
				<definiens id="0">a component equal to zero , hence for some r E fin , q ( T ) = 0 ,</definiens>
			</definition>
</paper>

		<paper id="2006">
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>The appropriate movement and marking of local focus , and the appropriate choice of the form of a noun phrase ( NP ) based on local focus information , are considered to contribute to the local coherence exhibited by discourse ( Sidner \ [ 1979\ ] , Grosz , Joshi , and Weinstein \ [ 1983 , 1995\ ] , Carter \ [ 1987\ ] , and others ) .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiendum id="1">Weinstein</definiendum>
				<definiens id="0">discourse ( Sidner \ [ 1979\ ] , Grosz , Joshi , and</definiens>
			</definition>
			<definition id="1">
				<sentence>Strube , in very recent work ( Strube 1998 ) , handles arbitrary sentence complexity .</sentence>
				<definiendum id="0">Strube</definiendum>
				<definiens id="0">handles arbitrary sentence complexity</definiens>
			</definition>
			<definition id="2">
				<sentence>To better illustrate the 181 Computational Linguistics Volume 25 , Number 2 types of discourses our methodology calls for constructing , let us consider what would be needed to extend a particular focusing framework , RAFT/RAPR ( described in Suri \ [ 1993\ ] ) , to handle resolving subject pronouns in sentences of the form `` SX because SY '' where SX and SY are simple sentences , and in a sentence following that type of sentence .</sentence>
				<definiendum id="0">RAFT/RAPR</definiendum>
				<definiens id="0">SX because SY '' where SX and SY are simple sentences , and in a sentence following that type of sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>If the Subject ( SY ) is a pronoun , how should Subject ( SY ) be resolved ?</sentence>
				<definiendum id="0">SY )</definiendum>
				<definiens id="0">a pronoun</definiens>
			</definition>
			<definition id="4">
				<sentence>We illustrated the SSD Methodology by 1 ) explaining how the first part of the methodology led to the Prefer-SX hypothesis ( a hypothesis about how readers prefer to resolve subjects in a sentence following an `` SX because SY '' sentence ) ; and 2 ) discussing a ( scaled-down ) preliminary corpus analysis we performed to test the validity of the Prefer-SX hypothesis .</sentence>
				<definiendum id="0">scaled-down</definiendum>
				<definiens id="0">illustrated the SSD Methodology by 1 ) explaining how the first part of the methodology led to the Prefer-SX hypothesis ( a hypothesis about how readers prefer to resolve subjects in a sentence following an `` SX because SY '' sentence</definiens>
			</definition>
</paper>

		<paper id="4002">
			<definition id="0">
				<sentence>In most previous accounts ( see , for example , Bouma \ [ 1992\ ] , Carpenter \ [ 1993\ ] , Copestake \ [ 1993\ ] , and Russell et al. \ [ 1993\ ] ) , default unification is an asymmetric operation that combines two ordinary ( T ) FSs , one of which is treated 492 Briscoe and Copestake Lexical Rules as default and one nondefault , to produce a normal TFS .</sentence>
				<definiendum id="0">unification</definiendum>
				<definiens id="0">an asymmetric operation that combines two ordinary ( T ) FSs , one of which is treated 492 Briscoe and Copestake Lexical Rules as default</definiens>
			</definition>
			<definition id="1">
				<sentence>We can consider the subsumption ordering on well-formed typed feature structures , where well-formed TFSs are a subset of TFSs in general .</sentence>
				<definiendum id="0">well-formed TFSs</definiendum>
				<definiens id="0">a subset of TFSs in general</definiens>
			</definition>
			<definition id="2">
				<sentence>The Tail T12 : T 12 =eel Filter ( I12 , ( T 1 U T2 ) ) where Filter ( I12 , T ) includes only the elements of T where the atomic FS p/s ( T ) is compatible with/12 .</sentence>
				<definiendum id="0">Filter</definiendum>
				<definiens id="0">includes only the elements of T where the atomic FS p/s ( T ) is compatible with/12</definiens>
			</definition>
			<definition id="3">
				<sentence>All lexical rules are of the following form , where Ia/Ta and Ib/Tb are TDFSs : Z , /T , IdTb 498 Briscoe and Copestake Lexical Rules Applied to : r 3rdsg \ ] 3RDSNG\ [ A~GS : &lt; T.\ [ \ ] &gt; \ ] FUNCTION • f3rdsng \ [ base \ ] ~+ PHON : base \ ] PHON r FUNCTION : \ ] 3RDSN : G\ [ ARGS : &lt; walk &gt; \ ] SYN LOC : SUBCAT : &lt; NP &gt; is equivalent to : r 3rdsg \ ] F base 1 FUNCTION : FUNCTION f3rdsng /PHON : \ [ ARGS : &lt; walk &gt; \ ] PHON : \ ] \ [ ARGS : &lt; T.\ [ \ ] &gt; \ ] ~t |3RDSNG : L 3RDSNG : \ [ \ ] L SYN LOC SUBCAT : &lt; NP &gt; 3rdsg \ [ FUNCTION : f3rdsng \ ] = PHON : ARGS : &lt; walk , \ [ \ ] &gt; \ ] 3RDSNG , \ [ \ ] SYN LOC SUBCAT : &lt; NP &gt; Figure 7 Reinterpreted Third Singular Verb Formation lexical rule .</sentence>
				<definiendum id="0">Ia/Ta</definiendum>
				<definiens id="0">F base 1 FUNCTION : FUNCTION f3rdsng /PHON : \ [ ARGS : &lt; walk &gt; \ ] PHON : \ ] \ [</definiens>
			</definition>
			<definition id="4">
				<sentence>In this case , for word forms i in sentence : Prob ( sent-interp ) = II ( lex-entry { word-form/ ) i In frameworks that incorporate alternative competing syntactic rule schemata or operations , it might be necessary to associate probabilities with such rules and treat the probability of a derivation as the combined product of the probability of the syntactic operations applied and the lexical entries utilized ( e.g. , Schabes 1992 ) .</sentence>
				<definiendum id="0">Prob</definiendum>
			</definition>
			<definition id="5">
				<sentence>Nevertheless we can represent the relative productivity of each lexical rule by calculating the ratio of possible to attested outputs for each rule ( see Aronoff \ [ 1976\ ] ) : M Prod ( lexical-rule ) = ( where N is the number of attested lexical entries that match the lexical rule input and M is the number of attested output entries ) .</sentence>
				<definiendum id="0">M Prod</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">M</definiendum>
				<definiens id="0">the number of attested lexical entries that match the lexical rule input</definiens>
			</definition>
			<definition id="6">
				<sentence>Verb alternations are a class of morphological conversion rules that exhibit similar semiproductive behavior to other processes of derivation and conversion .</sentence>
				<definiendum id="0">Verb alternations</definiendum>
				<definiens id="0">a class of morphological conversion rules that exhibit similar semiproductive behavior to other processes of derivation and conversion</definiens>
			</definition>
			<definition id="7">
				<sentence>We assume the following types : unlinked-active : diffiist\ ] \ ] SYN E R+ AL I unlinked-passive \ ] \ ] SYN : ARGREAL : \ [ LIST : \ [ HD : \ [ ~ppbysign\ ] \ [ LAST : \ [ THL D : e~st\ ] \ ] linked-active \ ] SYN I ARGREAL : \ [ LIST : \ [ \ ] \ ] \ ] : LSUBCAT : \ [ \ ] linked-~assive SYN : \ [ ARGREAL : \ [ LIST : \ [ TL : \ [ \ ] \ ] \ ] \ ] LSUBCAT : \ [ \ ] The Passive lexical rule simply states : unlinked-active ~-~ unlinked-passive The feature ARGREAL is encoded as a difference list on unlinked-active : that is , there are two features , LIST and LAST , such that the value of the LIST feature is a list in the usual HD/TL encoding and LAST is maintained as a pointer to the end of the list .</sentence>
				<definiendum id="0">LAST</definiendum>
				<definiens id="0">linked-~assive SYN : \ [ ARGREAL : \ [ LIST : \ [ TL : \</definiens>
				<definiens id="1">a list in the usual HD/TL encoding</definiens>
			</definition>
			<definition id="8">
				<sentence>DATR : A language for lexical knowledge representation .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">A language for lexical knowledge representation</definiens>
			</definition>
			<definition id="9">
				<sentence>Learnability and Cognition : The Acquisition of Argument Structure .</sentence>
				<definiendum id="0">Learnability</definiendum>
				<definiendum id="1">Cognition</definiendum>
			</definition>
</paper>

		<paper id="4005">
			<definition id="0">
				<sentence>More concretely , we have : • v total tags • A bigram source model with v 2 parameters of the form b ( t\ ] t ) , where P ( tl ... tin ) `` '' b ( tllboundary ) • b ( t2\ ] tl ) ... .. b ( tn\ ] tm-1 ) `` b ( boundary\ ] tm ) • Information Sciences Institute , Marina del Rey , CA 90292 @ 1999 Association for Computational Linguistics Computational Linguistics Volume 25 , Number 4 • A substitution channel model with parameters of the form s ( w\ ] t ) , where P ( wl ... Wmlh ... tm ) ~ S ( Wllh ) '' S ( W21t2 ) '' ... '' S ( Wraltm ) • an m-word text annotated with correct tags • an m-word unannotated text We can assign parts-of-speech to a previously unseen word sequence wl ... Wm by finding the sequence tl ... tm that maximizes P ( h..</sentence>
				<definiendum id="0">'' S</definiendum>
				<definiens id="0">A substitution channel model with parameters of the form s ( w\ ] t ) , where P ( wl ... Wmlh ... tm ) ~ S ( Wllh ) '' S ( W21t2 ) '' ...</definiens>
			</definition>
			<definition id="1">
				<sentence>An alignment is a set of connections between English and French words in a sentence pair .</sentence>
				<definiendum id="0">alignment</definiendum>
				<definiens id="0">a set of connections between English and French words in a sentence pair</definiens>
			</definition>
</paper>

		<paper id="3003">
			<definition id="0">
				<sentence>We roughly classified the calls into the following three broad classes : Destination Name , in which the caller explicitly specifies the name of the department to which he wishes to be transferred .</sentence>
				<definiendum id="0">Destination Name</definiendum>
			</definition>
			<definition id="1">
				<sentence>On the other hand , the similar request I want to talk to someone about car loans is ambiguous between Consumer Lending , which handles new car loans , and Loan Services , which handles existing car loans .</sentence>
				<definiendum id="0">Loan Services</definiendum>
				<definiens id="0">handles new car loans</definiens>
			</definition>
			<definition id="2">
				<sentence>Once the bag of salient terms for each destination is constructed , it is very straightforward to construct an m x n term-document frequency matrix A , where m is the number of salient terms , n is the number of destinations , and an element at , d represents the number of times the term t occurred in calls to destination d. This number indicates the degree of association between term t and destination d , and our underlying assumption is that if a term occurred frequently in calls to a destination in our training corpus , then occurrence of that term in a caller 's request indicates that the call should be routed to that destination .</sentence>
				<definiendum id="0">m</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the number of times the term t occurred in calls to destination d. This number indicates the degree of association between term t and destination d</definiens>
			</definition>
			<definition id="3">
				<sentence>This score is given by : n IDF ( t ) = log 2 d ( t ) where t is a term , n is the number of documents in the corpus , and d ( t ) is the number of documents containing the term t. If t only occurred in one document , IDF ( t ) = log 2 n ; if t occurred in every document , IDF ( t ) = log 2 1 -0 .</sentence>
				<definiendum id="0">IDF</definiendum>
				<definiendum id="1">t</definiendum>
				<definiendum id="2">n</definiendum>
				<definiendum id="3">IDF</definiendum>
				<definiens id="0">the number of documents containing the term t. If t only occurred in one document , IDF ( t ) = log 2 n ; if t occurred in every document</definiens>
			</definition>
			<definition id="4">
				<sentence>U is an m x m orthonormal matrix ; V is an n x n orthonormal matrix ; and S is an m x n positive matrix whose nonzero values are Sl,1 , ... , Sr , r , where r is the rank of C , and they are arranged in descending order S1,1 ~ S2,2 ~ ' '' ~ Sr , r ~ O. Figure 6 illustrates the results of singular value decomposition according to the above equation .</sentence>
				<definiendum id="0">U</definiendum>
				<definiendum id="1">V</definiendum>
				<definiendum id="2">S</definiendum>
				<definiendum id="3">r</definiendum>
				<definiens id="0">an m x m orthonormal matrix ;</definiens>
				<definiens id="1">an n x n orthonormal matrix ; and</definiens>
				<definiens id="2">an m x n positive matrix whose nonzero values are Sl,1 , ... , Sr , r</definiens>
			</definition>
			<definition id="5">
				<sentence>The shaded portions of the matrices are what we use as the basis for our term and document vector representations , as follows : . . . Ur is an m x r matrix , in which each row forms the basis of our term vector representation ; Vr is an n x r matrix , in which each row forms the basis of our document vector representation ; and Sr is an r x r positive diagonal matrix whose values are used for appropriate scaling in the term and document vector representations .</sentence>
				<definiendum id="0">document vector representations</definiendum>
				<definiendum id="1">Vr</definiendum>
				<definiendum id="2">Sr</definiendum>
				<definiens id="0">an n x r matrix</definiens>
				<definiens id="1">an r x r positive diagonal matrix whose values are used for appropriate scaling in the term and document vector representations</definiens>
			</definition>
			<definition id="6">
				<sentence>simply be recovered by element ( C T • C ) /j. Since U is orthonormal , S is a diagonal matrix , we have : C T • C = ( U-S '' vT ) T '' ( U 'S '' V T ) = V.S T.U T-U.S.V T = V.S.S.V T : ( V 'S ) '' ( V 'S ) w Because only the first r diagonal elements of S are nonzero , we have : ( W. S ) .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">C T • C = ( U-S '' vT ) T '' ( U 'S '' V T ) = V.S T.U T-U.S.V T = V.S.S.V T : ( V 'S ) ''</definiens>
			</definition>
			<definition id="7">
				<sentence>17 Assuming da and db are the coefficients of the fitted sigmoid function for destination d , we have the following confidence function for a destination d and cosine value x : Conf ( da , db , x ) = 1/ ( 1 + e - ( dax+db ) ) Thus the score given a request and a destination , where d is the vector corresponding to destination d , and r is the vector corresponding to the request is Conf ( da , db , cos ( r , d ) ) .</sentence>
				<definiendum id="0">d</definiendum>
				<definiendum id="1">r</definiendum>
				<definiens id="0">the vector corresponding to destination d</definiens>
			</definition>
			<definition id="8">
				<sentence>18 Recall that kappa is defined by ( a -e ) / ( 1 -e ) where a is the system 's accuracy and e is the expected agreement by chance .</sentence>
				<definiendum id="0">e</definiendum>
				<definiens id="0">the expected agreement by chance</definiens>
			</definition>
			<definition id="9">
				<sentence>Precision is the percentage of words/terms in the recognizer output that are actually in the transcription , i.e. , percentage of found words/terms 23 Recall that our current system uses simple template filling for response generation by utilizing manually constructed mappings from n-gram terms to their inflected forms , such as from exist+car+loan to an existing car loan .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the percentage of words/terms in the recognizer output that are actually in the transcription , i.e. , percentage of found words/terms 23 Recall that our current system uses simple template filling for response generation by utilizing manually constructed mappings from n-gram terms to their inflected forms</definiens>
			</definition>
			<definition id="10">
				<sentence>Word Accuracy Term Accuracy Raw Rooted Unigram Bigram Trigram Precision 78.6 % 79.8 % 93.7 % 96.5 % 98.5 % Recall 76.0 % 77.2 % 88.4 % 85.5 % 83.6 % that are correct , while recall is the percentage of words/terms in the transcription that are correctly returned by the recognizer , i.e. , percentage of actual words/terms that are found .</sentence>
				<definiendum id="0">recall</definiendum>
				<definiens id="0">the percentage of words/terms in the transcription that are correctly returned by the recognizer , i.e. , percentage of actual words/terms that are found</definiens>
			</definition>
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>Whether screening whole populations of children , or assessing individual referrals , the articulation test is an important tool for the speech clinician .</sentence>
				<definiendum id="0">articulation test</definiendum>
				<definiens id="0">an important tool for the speech clinician</definiens>
			</definition>
			<definition id="1">
				<sentence>Whereas Covington seeks to align the segments in possible historical cognates , CAT aligns the segments of a child 's articulation with those of the adult model , and on the basis of this looks for evidence of the phonological processes listed in Table 1 .</sentence>
				<definiendum id="0">CAT</definiendum>
				<definiens id="0">aligns the segments of a child 's articulation with those of the adult model</definiens>
			</definition>
			<definition id="2">
				<sentence>CAT has problems in three cases where the French has lost syllables that are stressed in Spanish , as in cabdza : cap ( 1 ) ; 3 in the case of drboharbre ( 2 ) , CAT gets the correct alignment as identified by Covington ( p. 488 ) if we omit the schwa in the French transcription ( as would be normal for Parisian French ( Armstrong 1967 , 117 ) .</sentence>
				<definiendum id="0">CAT</definiendum>
				<definiens id="0">has problems in three cases where the French has lost syllables that are stressed in Spanish</definiens>
			</definition>
			<definition id="3">
				<sentence>With just one exception ( knee : genF~ ) , CAT does as well as or better than Covington .</sentence>
				<definiendum id="0">CAT</definiendum>
				<definiens id="0">does as well as or better than Covington</definiens>
			</definition>
			<definition id="4">
				<sentence>Finally , Covington presents a variety of language-pair examples ( p. 495 ) .</sentence>
				<definiendum id="0">Covington</definiendum>
			</definition>
			<definition id="5">
				<sentence>In the second part of the article , Connolly introduces a distance measure for comparing sequences of phones , based on the Levenshtein distance wellknown in the speech processing and corpus alignment literature ( inter alia ) .</sentence>
				<definiendum id="0">Connolly</definiendum>
				<definiens id="0">introduces a distance measure for comparing sequences of phones , based on the Levenshtein distance wellknown in the speech processing and corpus alignment literature ( inter alia )</definiens>
			</definition>
			<definition id="6">
				<sentence>CAT uses a very crude phonetic transcription based only on a minimal character set , not even including lower-case letters .</sentence>
				<definiendum id="0">CAT</definiendum>
				<definiens id="0">uses a very crude phonetic transcription based only on a minimal character set</definiens>
			</definition>
			<definition id="7">
				<sentence>The software system PDAC ( Phonological Deviation Analysis by Computer ) uses a software package called LIPP ( Logical International phonetic Programs ) for input of transcriptions ( Perry 1995 ) .</sentence>
				<definiendum id="0">software system PDAC</definiendum>
				<definiens id="0">Phonological Deviation Analysis by Computer ) uses a software package called LIPP ( Logical International phonetic Programs</definiens>
			</definition>
			<definition id="8">
				<sentence>Acknowledgments I would like to thank the following people for their help in gathering the information presented in this paper : Catherine Adams ( University of Manchester ) , Lawrence Shriberg ( University of Wisconsin-Madison ) , Julie Masterson ( Southwest Missouri State University ) , Carol Stoel-Gammon ( University of Washington ) and John Connolly ( Loughborough University ) ; Michael Covington , for collaborating on the `` bake-off '' ; Joe Somers , for providing some of the example data ; and the three anonymous reviewers for their suggestions , which have been extremely valuable .</sentence>
				<definiendum id="0">Catherine Adams</definiendum>
				<definiendum id="1">Carol Stoel-Gammon</definiendum>
				<definiens id="0">providing some of the example data ; and the three anonymous reviewers for their suggestions</definiens>
			</definition>
			<definition id="9">
				<sentence>The CHILDES Project : Tools for Analyzing Talk .</sentence>
				<definiendum id="0">CHILDES Project</definiendum>
			</definition>
</paper>

		<paper id="4006">
			<definition id="0">
				<sentence>That is , rather than identifying information in main memory that needs to be made salient again , the IRU is a repetition that helps to establish to which part of the dialogue ( i.e. , which focus space on the stack ) attention is now returning .</sentence>
				<definiendum id="0">IRU</definiendum>
				<definiens id="0">needs to be made salient again , the</definiens>
			</definition>
			<definition id="1">
				<sentence>621 Computational Linguistics Volume 25 , Number 4 questions : ( 1 ) How is the cache searched for related entities and how is relatedness determined ?</sentence>
				<definiendum id="0">How</definiendum>
			</definition>
			<definition id="2">
				<sentence>Implicit focus ( Grosz 1977b ) and the treatment of functionally related entities ( Grosz , Joshi , and Weinstein 1995 ) respond to a related issue in discourse processing , namely , what other than an entity itself becomes focused when the entity is focused ?</sentence>
				<definiendum id="0">Implicit focus</definiendum>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>TAG is a tree-rewriting system : the derivation process consists in applying operations to trees in order to obtain a ( derived ) tree whose sequence of leaves is a sentence .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">a tree-rewriting system</definiens>
			</definition>
			<definition id="1">
				<sentence>Adjunction increases the expressive power of the formalism in such a way that noncontext-free languages can be represented although the parse process is done in polynomial time .</sentence>
				<definiendum id="0">Adjunction</definiendum>
				<definiens id="0">increases the expressive power of the formalism in such a way that noncontext-free languages can be</definiens>
			</definition>
			<definition id="2">
				<sentence>The right-hand side is the variable labeling the mother node of the tree .</sentence>
				<definiendum id="0">right-hand side</definiendum>
				<definiens id="0">the variable labeling the mother node of the tree</definiens>
			</definition>
			<definition id="3">
				<sentence>The left-hand side is a sequence of formulas of the following kinds : A for some leaf A of the tree , A o-B1 ® ... ® Bn where A is the label of some internal node and Bi are the labels of its daughters , A oA whenever A is a node where an adjunction can take place .</sentence>
				<definiendum id="0">left-hand side</definiendum>
				<definiendum id="1">Bi</definiendum>
				<definiendum id="2">oA whenever A</definiendum>
				<definiens id="0">a sequence of formulas of the following kinds : A for some leaf A of the tree , A o-B1 ® ... ® Bn where A is the label of some internal node</definiens>
			</definition>
			<definition id="4">
				<sentence>A TAG is defined by two finite sets of trees composed by means of the substitution and adjunction operations .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">defined by two finite sets of trees composed by means of the substitution and adjunction operations</definiens>
			</definition>
			<definition id="5">
				<sentence>210 Abrusci , Fouquer6 , and Vauzeilles Tree Adjoining Grammars Definition A TAG G is a 5-tuple ( VN , Vv , S , I , A ) where • VN is a finite set of nonterminal symbols , • VT is a finite set of terminal symbols , • S is a distinguished nonterminal symbol the start symbol , • I is a set of initial trees , • A is a set of auxiliary trees .</sentence>
				<definiendum id="0">TAG G</definiendum>
				<definiendum id="1">VN</definiendum>
				<definiendum id="2">VT</definiendum>
				<definiendum id="3">S</definiendum>
				<definiens id="0">a 5-tuple ( VN , Vv , S , I , A ) where •</definiens>
				<definiens id="1">a finite set of nonterminal symbols</definiens>
				<definiens id="2">a finite set of terminal symbols</definiens>
				<definiens id="3">a distinguished nonterminal symbol the start symbol</definiens>
			</definition>
			<definition id="6">
				<sentence>We write 7a ~ 72 when 72 is the result of an adjunction or a substitution of an elementary tree of a TAG G on the derived tree 71 ; ~h is the reflexive , transitive closure of ~c. The set { 7/3a E G and c~ ~ 7 } is represented by T ( G ) .</sentence>
				<definiendum id="0">~h</definiendum>
				<definiens id="0">the result of an adjunction or a substitution of an elementary tree of a TAG G on the derived tree 71</definiens>
			</definition>
			<definition id="7">
				<sentence>The adjunction results from two atomic cut rules between the sequent corresponding to the adjunction tree and two suitable sequents corresponding to two subparts of the 213 Computational Linguistics Volume 25 , Number 2 A e A ( axiom ) PeA F1 , A , F2 e B ( cut ) F1 , F , F2 e B PeA AeB ( r-® ) F , AeA®B F1 , A , B , F2 e C rl , A , r2ec AeB ( l-o- ) ~ : Ao-B , A , F2eC P , Be A ( r-o -- ) FeAo-B F1 , A , F2eC Ae B ( l -- o ) P1 , A , B -- o A , F2 e c B , FeA ( r-o ) Fe B-oA Figure 3 Language and sequent calculus for the Lambek calculus .</sentence>
				<definiendum id="0">F2 e B PeA AeB</definiendum>
				<definiendum id="1">A , r2ec AeB</definiendum>
				<definiendum id="2">A , F2eC Ae B</definiendum>
			</definition>
			<definition id="8">
				<sentence>Mary , MarykN15~x ) Oo ( Npo -- N ) o -- a , a , NkNP No-book , bookFN NP , ( ( ( NP -- o S ) oNP ) o-NP ) ogives , gives , NP , NP ~S ( cut ) NP , ( ( ( NP -- o S ) oNP ) oNP ) o-gives , gives , NP o-Mary , Mary , NP F S ( NP o-N ) o-a , a , N o-book , book ~ '' NP ( cut ) NPoJohn , JohntNP `` lex ' ( ~ NP , ( ( ( NP -oS ) oNp ) o-NP ) ogives , gives , NPoMary , Mary , ( NPo-N ) o-a , a , Nobook , bookF S ( cut ) NP o-John , John , { ( ( NP -- a S ) eNP ) oNP ) a-gives , gives , NP o-Mary , Mary , ( NP o-N ) oa , a , N o-book , book ~S Figure 5 Proof of John gives Mary a book : ( Lambek-style ) two implications .</sentence>
				<definiendum id="0">, NP ~S</definiendum>
				<definiendum id="1">bookF S ( cut ) NP o-John</definiendum>
				<definiendum id="2">NP -- a S ) eNP ) oNP</definiendum>
				<definiens id="0">NP -- o S ) oNP ) o-NP ) ogives , gives , NP</definiens>
			</definition>
			<definition id="9">
				<sentence>We logically represent the set of trees T ( ~2 ) as ( the set of provable theorems of ) a calculus A ( ~2 ) : the formulas are built with the alphabet { c , a , b , c , d , S } and the set of connectives { ® , o- } , the sequent calculus consists of the axioms s Fs and the rules ( in both axioms and rules , s is a propositional letter ) : P } -¢ F1 , S , F2~-B Pt-a®S®d I~I , S , P2 } -B YI-b®S®c I~I , S , I~2 } -B I~1 , S o-¢ , I~ , F2 FB P1 , So-a®S®d , P , P2 } -B P1 , So-b®S®c , P , P2t-B Ft-A AI-B ( ® ) sl-s I~I , S , P2~-B F , A FA ® B \ [ ~1 , S ( 3-S , S , P 2 ~B The introduction of a left implication ( o -- ) corresponds to the building of a partial tree .</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">the formulas are built with the alphabet { c , a , b , c , d , S } and the set of connectives { ® , o- } , the sequent calculus consists of the axioms s Fs and the rules ( in both axioms and rules</definiens>
				<definiens id="1">a propositional letter ) : P } -¢ F1 , S , F2~-B Pt-a®S®d I~I , S , P2 } -B YI-b®S®c I~I , S , I~2 } -B I~1 , S o-¢ , I~ , F2 FB P1 , So-a®S®d , P , P2 } -B P1 , So-b®S®c , P , P2t-B Ft-A AI-B ( ® ) sl-s I~I , S , P2~-B F , A FA ® B \ [ ~1</definiens>
			</definition>
			<definition id="10">
				<sentence>Such introductions are then restricted either to the formalization of the trees of the grammar ( the first three rules correspond exactly to the trees of ~2 ) , or to the formalization of adjunction nodes ( the formula s o-s `` marks '' s as being an adjunction node , i.e. , the adjunction rule may be applied only on this kind of node as it will be clear below ) .</sentence>
				<definiendum id="0">adjunction rule</definiendum>
				<definiens id="0">the formula s o-s `` marks '' s as being an adjunction node</definiens>
			</definition>
			<definition id="11">
				<sentence>216 Abrusci , Fouquer6 , and Vauzeilles Tree Adjoining Grammars Sequents : F ~A , where F is a finite sequence of formulas and A is a formula .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">a finite sequence of formulas</definiens>
			</definition>
			<definition id="12">
				<sentence>A ( G ) is the calculus obtained from .4 as follows : propositional letters are exactly all the labels of the trees in ~ , the rule ( o- ) is restricted as follows : Pt-A FI , a , F2F-B PI , aO-A , P , F2 } -B ( o.- , ~ ) where A , B are simple Q-formulas of the language of A ( ~ ) , a is a propositional letter of the language of A ( G ) and one of the following cases occurs : -Aisa -A is a propositional letter b different from a , and the tree IE G b A is bl ® • .</sentence>
				<definiendum id="0">-Aisa -A</definiendum>
				<definiens id="0">the calculus obtained from .4 as follows : propositional letters are exactly all the labels of the trees in ~ , the rule</definiens>
				<definiens id="1">simple Q-formulas of the language of A ( ~ ) , a is a propositional letter of the language of A ( G ) and one of the following cases occurs :</definiens>
			</definition>
			<definition id="13">
				<sentence>SeqO ( respectively , Tree ( ) ) associates a sequent ( respectively , a tree ) to each tree ( respectivel3¢ each sequent ) , and we prove the two are converse .</sentence>
				<definiendum id="0">SeqO</definiendum>
				<definiendum id="1">Tree ( ) )</definiendum>
				<definiens id="0">associates a sequent ( respectively , a tree ) to each tree ( respectivel3¢ each sequent</definiens>
			</definition>
			<definition id="14">
				<sentence>Girard ( 1987 ) has defined , in a purely geometric way , a class of graphs of formulas , called 219 Computational Linguistics Volume 25 , Number 2 proofnets : for each proof of a sequent tF in the one-sided sequent calculus for multiplicative linear logic , there is a corresponding proofnet whose conclusions are exactly the formulas in F , and for each proofnet , there is at least one corresponding proof of the sequent tF in the one-sided sequent calculus for multiplicative linear logic ( where P is a sequence of all the conclusions of the proofnet ) .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a sequence of all the conclusions of the proofnet )</definiens>
			</definition>
			<definition id="15">
				<sentence>A shorttrip is a trip that does not contain each node twice .</sentence>
				<definiendum id="0">shorttrip</definiendum>
				<definiens id="0">a trip that does not contain each node twice</definiens>
			</definition>
			<definition id="16">
				<sentence>Note that VP is an adjunction node so the sequent associated to the item saw includes the formula VP o-VP .</sentence>
				<definiendum id="0">VP</definiendum>
				<definiens id="0">an adjunction node so the sequent associated to the item saw includes the formula VP o-VP</definiens>
			</definition>
			<definition id="17">
				<sentence>John Mary saw NP oJohn , John tNP NP o-Mary , Mary lNP S o-NP ® VP , NP , VP o-VP , VP oV ® NP , V o-saw , saw , NP tS 221 Computational Linguistics Volume 25 , Number 2 So -- NP®VP , NP , VPoVP , VPo-V®NP , Vo -- saw , saw , NPI-S NPo -- Mary , MaryI-NP ( cut ) NP ¢ , -John , John ~ '' NP S o-NP ® VP , NP , VP o.VP , VP e-V ® NP , V o-saw , saw , NP oMary , Mary IS S o-NP ® VP , NP o-John , John , VP o-VP , VP o-V ® NP , V o-saw , saw , NP o-Mary , Mary IS ( cut ) So -- NP®VP , NP , VPo-VP , VPo -- V®NP , Vo -- saw , saw , NPI-S NPo -- John , JohnI-NP ( cut ) NP oMary , Mary INP S oNP ® VP , NP o-John , John , VP o-VP , VP o-V ® NP , V o-saw , saw , NP tS S o-NP ® VP , NP o-John , John , VP o-VP , VP o-V ® NP , V c-saw , saw , NP e-Mary , Mary IS ( cut ) Figure 9 A ( G ) proofs of John saw Mary .</sentence>
				<definiendum id="0">MaryI-NP</definiendum>
				<definiens id="0">proofs of John saw Mary</definiens>
			</definition>
			<definition id="18">
				<sentence>( M ( G~ ) denotes this new set ) : John meets who meets _ who _ meets NP o-John , John k NP S o-NP ® VP , NP , VP o-V @ NP , V omeets , meets , NP k S No -- N ®who® ( SoNP ) , N , who , ( SoNP ) o-VP , VPoV ® NP , Vo-meets , meets , NPk N No -- N @ who® ( So-NP ) , N , who , SoNP® VP , NP , VPoV @ NP , Vo -- rneets , meetsI-N It should be noted that the two sequents given below are provable in the calculus M ( ~ ) ( cut and adjunction rules only ) .</sentence>
				<definiendum id="0">M ( G~ )</definiendum>
				<definiens id="0">this new set ) : John meets who meets _ who _ meets NP o-John , John k NP S o-NP ® VP , NP , VP o-V @ NP , V omeets , meets , NP k S No</definiens>
				<definiens id="1">the two sequents given below are provable in the calculus M ( ~ ) ( cut and adjunction rules only )</definiens>
			</definition>
			<definition id="19">
				<sentence>-NP ) , N , who , ( S o-NP ) o-VP , VP o-V @ NP , V o-meets , meets , NP o-John , John k N N o-N ® who @ ( S o-NP ) , N , who , S o-NP ® VP , NP o-John , John , VP o-VP , VP o-V @ NP , V omeets , meets tN But they are not provable with the cut and adjunction rules from M ( ~ ) .</sentence>
				<definiendum id="0">V o-meets</definiendum>
			</definition>
			<definition id="20">
				<sentence>r k A , B , A , A ' ( r exchange ) FPA , A , B , A ' be defined in the following way : B o-A = B ~ ±A and A -- o B =_ A±~B. In Figure 19 , we give the one-sided sequent calculus for the multiplicative fragment of noncommutative linear logic ( N-LL ) , and in Figure 3 in Section 3 , the two-sided sequent calculus for the multiplicative fragment of intuitionistic noncommutative linear logic ( N-ILL ) : sequent calculus for N-LL and sequent calculus for N-ILL satisfy the cut elimination theorem , i.e. , for each proof there exists a cut-free proof with the same conclusion ; however , we make use of cut rules in Section 4 .</sentence>
				<definiendum id="0">r k A , B</definiendum>
				<definiendum id="1">N-ILL )</definiendum>
				<definiendum id="2">sequent</definiendum>
				<definiens id="0">calculus for N-LL and sequent calculus for N-ILL satisfy the cut elimination theorem</definiens>
			</definition>
			<definition id="21">
				<sentence>• Sequents : F tA , where F is a finite sequence of formulas and A is a formula .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">a finite sequence of formulas</definiens>
			</definition>
			<definition id="22">
				<sentence>L ... ip • connectives : ® , Formulas : usual definition Sequents : tr where F is a finite sequence of formulas Metalinguistic definition of A ± and IA s.t. `` ( A± ) = ( 'A ) ± = A , for every formula A : times .</sentence>
				<definiendum id="0">Formulas</definiendum>
				<definiendum id="1">F</definiendum>
				<definiens id="0">a finite sequence of formulas Metalinguistic definition of A ± and IA s.t. `` ( A± ) = ( 'A ) ± = A , for every formula A : times</definiens>
			</definition>
			<definition id="23">
				<sentence>-± ± ( `` ... ±p ) = , ... , p ( B®C ) ±=C ±~B ± ( B : ~C ) x=C ±®B ± `` ( B ® C ) =~C : ~'B Z ( B ~ C ) =Q:7 ® ±B Rules of sequent calculus : tA ± , A ( axiom ) tF1 , A , F2 tA ± , A ( cut 1 ) tF , A ~A1 , A ± , A2 ( cut 2 ) 1PI , A , F2 IAl , F , A2 I-F1 , A , F2 PB , A PF , A PAl B , A2 IA1 , A , B , A2 ( rFigure 19 Language and sequent calculus for multiplicative noncommutative linear logic .</sentence>
				<definiendum id="0">... , p</definiendum>
				<definiendum id="1">tF</definiendum>
				<definiendum id="2">~A1</definiendum>
				<definiendum id="3">A2</definiendum>
				<definiendum id="4">A2 I-F1</definiendum>
				<definiendum id="5">PF</definiendum>
				<definiendum id="6">PAl B</definiendum>
				<definiendum id="7">B , A2</definiendum>
				<definiens id="0">~'B Z ( B ~ C ) =Q:7 ® ±B Rules of sequent calculus : tA ± , A ( axiom</definiens>
			</definition>
			<definition id="24">
				<sentence>Definition Let A be a simple ®-formula or a o -- -formula ( calculus A ) and a a propositional variable , the number of positive occurrences p ( a , A ) ( and negative occurrences n ( a , A ) ) of a in A is defined by : • if A = a then p ( a , A ) = 1 , n ( a , A ) = 0 • if A = b and b is a propositional variable distinct from a , then p ( a , A ) = 0 , n ( a , A ) = 0 • if A ~ B ® C , then p ( a , A ) = p ( a , B ) + p ( a , C ) , n ( a , A ) = n ( a , B ) + n ( a , C ) • if A =_ B o-AI® ... ®A , , then p ( a , A ) = p ( a , B ) and n ( a , A ) = p ( a , A1 ® ... ® An ) as A1 , ... , A , are ®-simple formulas , cf. the calculus A. Let S be the sequent C1 ... .. Cn tA defined as for calculus A ( ® and o- ) : • p ( a , $ ) =p ( a , A ) +n ( a , C1 ) + ... +n ( a , Cn ) • n ( a , S ) = p ( a , C1 ) +- .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">n</definiendum>
				<definiendum id="2">n</definiendum>
				<definiendum id="3">) +n</definiendum>
				<definiens id="0">a simple ®-formula or a o -- -formula ( calculus A ) and a a propositional variable , the number of positive occurrences p ( a , A ) ( and negative occurrences n ( a , A )</definiens>
				<definiens id="1">a , B ) + p ( a</definiens>
				<definiens id="2">a , B ) + n ( a</definiens>
				<definiens id="3">a , A1 ® ... ® An ) as A1 , ... , A , are ®-simple formulas</definiens>
			</definition>
			<definition id="25">
				<sentence>the calculus obtained from ,4 as follows : • propositional letters are exactly all the labels of the trees in ~ , • the rule ( o- ) is restricted as follows : Ft-A FI , a , F2 tB FI , aO-A , F , F2F-B ( o -- , G ) 232 Abrusci , Fouquer6 , and Vauzeilles Tree Adjoining Grammars where A , B are simple ®-formulas of A ( ~ ) , a is a propositional letter of A ( O ) , and one of the following cases occurs : -Aisa -A is a propositional letter b different from a , and the tree TE G b -Aisbl® ... ®bn , andthetree //~ c bl ... bn Proposition Calculus A ( ~ ) Properties 1-4 of A are also properties of A ( ~ ) .</sentence>
				<definiendum id="0">-Aisa -A</definiendum>
				<definiendum id="1">bn Proposition Calculus A</definiendum>
				<definiens id="0">o -- , G ) 232 Abrusci , Fouquer6 , and Vauzeilles Tree Adjoining Grammars where A , B are simple ®-formulas of A ( ~</definiens>
			</definition>
			<definition id="26">
				<sentence>there is a formula c o-c iff c is a possibly internal point of T on which the adjoining operation may be performed ; Seq ( T ) is provable in .</sentence>
				<definiendum id="0">formula c o-c iff c</definiendum>
				<definiens id="0">provable in</definiens>
			</definition>
			<definition id="27">
				<sentence>Seq ( T ) is obtained from Seq ( T1 ) and Seq ( T2 ) by using the atomic cut rule , so that by property 3 it is provable in ¢4 ( G ) since Seq ( T1 ) and Seq ( T2 ) are provable in A ( ~ ) by induction hypothesis .</sentence>
				<definiendum id="0">Seq</definiendum>
				<definiens id="0">provable in A ( ~ ) by induction hypothesis</definiens>
			</definition>
			<definition id="28">
				<sentence>• To every provable sequent F tA in ~4 ( G ) , we associate Tree ( F IA ) s.t. if A is a propositional letter , then Tree ( F I-A ) c T ( G ) where the root is A , the terminal points ( from left to right ) are exactly all the propositional letters occurring in F and in the same order in which they occur in F , and the possibly internal points on which the adjoining operation may be performed are exactly all the propositional letters c s.t. c 0c occur in F ; if A is bl ® ... ® bn , and so F = F1 ... Fn with the sequents Fi tbi provable in A ( G ) for every 1 &lt; i &lt; n , then Tree ( F IA ) is a sequence T1 ... .. T , of trees E T ( G ) , s.t. Ti = Tree ( Fi } bi ) .</sentence>
				<definiendum id="0">IA )</definiendum>
				<definiens id="0">F I-A ) c T ( G ) where the root is A , the terminal points ( from left to right ) are exactly all the propositional letters occurring in F and in the same order in which they occur in F</definiens>
				<definiens id="1">a sequence T1 ... .. T , of trees E T ( G ) , s.t. Ti = Tree ( Fi } bi )</definiens>
			</definition>
			<definition id="29">
				<sentence>Define CL ( M ) as follows : MC_CL ( M ) ( closure under atomic cut rule ) if F Fa E CL ( M ) and A1 , a , A2 tB ECL ( M ) , then A1 , F , A2 FB cCL ( M ) ( closure under adjoining operation ) if I~1 , a , F2 } a ECL ( M ) and A , a o-a , A tb cCL ( M ) , then A , F1 , A1 , I~2 , A2 Fb ECL ( M ) , where ( A1 , A2 ) is the splitting pair of A in A , a , A Fb ; nothing else belongs to CL ( M ) .</sentence>
				<definiendum id="0">ECL</definiendum>
				<definiendum id="1">F1</definiendum>
				<definiendum id="2">Fb ; nothing</definiendum>
				<definiens id="0">follows : MC_CL ( M ) ( closure under atomic cut rule</definiens>
				<definiens id="1">A1 , F , A2 FB cCL ( M ) ( closure under adjoining operation ) if I~1 , a</definiens>
			</definition>
</paper>

		<paper id="3004">
			<definition id="0">
				<sentence>An indirect answer is the result of the speaker ( R ) expressing only part of the planned response , i.e. , omitting the direct answer ( and possibly more ) , but intending for his discourse plan to be recognized by the hearer ( Q ) .</sentence>
				<definiendum id="0">indirect answer</definiendum>
				<definiens id="0">the result of the speaker ( R ) expressing only part of the planned response , i.e. , omitting the direct answer ( and possibly more ) , but intending for his discourse plan to be recognized by the hearer ( Q )</definiens>
			</definition>
			<definition id="1">
				<sentence>A full answer consists of a direct answer , which we refer to as the nucleus , and possibly extra information of various types , which we refer to as satellites } ° Then , an indirect answer can be modeled as the result of R giving one or more satellites of the full answer , without giving the nucleus explicitly , but intending for the full answer to be recognized .</sentence>
				<definiendum id="0">full answer</definiendum>
				<definiendum id="1">direct answer</definiendum>
				<definiens id="0">the nucleus , and possibly extra information of various types , which we refer to as satellites } ° Then , an indirect answer can be modeled as the result of R giving one or more satellites of the full answer , without giving the nucleus explicitly , but intending for the full answer to be recognized</definiens>
			</definition>
			<definition id="2">
				<sentence>In the next section , we informally describe how different types of satellites of full answers ( i.e. , types of indirect answers ) can be characterized .</sentence>
				<definiendum id="0">full answers</definiendum>
				<definiens id="0">types of indirect answers ) can be characterized</definiens>
			</definition>
			<definition id="3">
				<sentence>An RST relation is defined as a relation between two text spans , called the nucleus and satellite .</sentence>
				<definiendum id="0">RST relation</definiendum>
				<definiens id="0">a relation between two text spans , called the nucleus and satellite</definiens>
			</definition>
			<definition id="4">
				<sentence>395 Computational Linguistics Volume 25 , Number 3 It is mutually plausible to the agent that ( cr-obstacle q p ) holds , where q is the proposition that a state Sq does not hold during time period tq , and p is the proposition that an event e v does not occur during time period t v , if the agent believes it to be mutually believed that Sq is a precondition of a typical plan for doing ev , and that tq is before or includes tv , unless it is mutually believed that sq does hold during tq , or that ep does occur during tp .</sentence>
				<definiendum id="0">q</definiendum>
				<definiendum id="1">p</definiendum>
				<definiens id="0">the proposition that an event e v does not occur during time period t v , if the agent believes it to be mutually believed that Sq is a precondition of a typical plan for doing ev , and that tq is before or includes tv , unless it is mutually believed that sq does hold during tq , or that ep does occur during tp</definiens>
			</definition>
			<definition id="5">
				<sentence>It is mutually plausible to the agent that ( cr-obstacle q p ) holds , where q is the proposition that a state sq holds during time period tq , and p is the proposition that a state sv does not hold during time period tv , if the agent believes it to be nmtually believed that 8q typically prevents sp , and that tq is before or includes tv , unless it is mutually believed that Sq does not hold during lq , or that s v does hold during t v. Figure 1 Glosses of two coherence rules for cr-obstacle .</sentence>
				<definiendum id="0">q</definiendum>
				<definiendum id="1">p</definiendum>
				<definiens id="0">the proposition that a state sv does not hold during time period tv , if the agent believes it to be nmtually believed that 8q typically prevents sp , and that tq is before or includes tv , unless it is mutually believed that Sq does not hold during lq , or that s v does hold during t v. Figure 1 Glosses of two coherence rules for cr-obstacle</definiens>
			</definition>
			<definition id="6">
				<sentence>Interpretation is modeled as inference of R 's discourse plan from R 's response using the same set of discourse plan operators and coherence rules .</sentence>
				<definiendum id="0">Interpretation</definiendum>
				<definiens id="0">modeled as inference of R 's discourse plan from R 's response using the same set of discourse plan operators and coherence rules</definiens>
			</definition>
			<definition id="7">
				<sentence>Coherence rules specify sufficient conditions for the plausibility to an agent with respect to the agent 's shared beliefs ( which we hereafter refer to as the mutual plausibility ) of a relational proposition ( CR q p ) , where CR is a coherence relation and q and p are propositions .</sentence>
				<definiendum id="0">Coherence rules</definiendum>
				<definiendum id="1">CR</definiendum>
				<definiens id="0">the mutual plausibility ) of a relational proposition ( CR q p )</definiens>
			</definition>
			<definition id="8">
				<sentence>( Each satellite operator has a name of the form , Use-CR , where CR is the name of a coherence relation . )</sentence>
				<definiendum id="0">CR</definiendum>
				<definiens id="0">the name of a coherence relation</definiens>
			</definition>
			<definition id="9">
				<sentence>Of course , dialogue consists of more than questions and answers .</sentence>
				<definiendum id="0">dialogue</definiendum>
			</definition>
			<definition id="10">
				<sentence>The output of the first phase of answer recognition is a set of candidate discourse plans since there may be alternate interpretations of R 's response .</sentence>
				<definiendum id="0">answer recognition</definiendum>
				<definiens id="0">a set of candidate discourse plans since there may be alternate interpretations of R 's response</definiens>
			</definition>
			<definition id="11">
				<sentence>The inputs consist of : • sat-op , a discourse plan operator for a possible satellite , • the proposition p conveyed by the nucleus of the higher-level plan ( i.e. , the plan whose satellites are currently being recognized ) , • act-list , a list of acts in R 's turn that have not yet been assimilated into the candidate plan , • cur-act , the current act ( inform s h q ) in act-list , where s is the speaker , h is the hearer , and q is the propositional content of the act .</sentence>
				<definiendum id="0">s</definiendum>
				<definiendum id="1">q</definiendum>
				<definiens id="0">a discourse plan operator for a possible satellite , • the proposition p conveyed by the nucleus of the higher-level plan ( i.e. , the plan whose satellites are currently being recognized ) , • act-list , a list of acts in R 's turn that have not yet been assimilated into the candidate plan , • cur-act , the current act ( inform s h q ) in act-list</definiens>
				<definiens id="1">the speaker , h is the hearer , and</definiens>
			</definition>
			<definition id="12">
				<sentence>Thus , when top-down recognition has reached an impasse , hypothesis generation ( a type of bottom-up data-driven reasoning ) provides a hypothesis that enables top-down recognition to continue another level of growth .</sentence>
				<definiendum id="0">hypothesis generation</definiendum>
				<definiens id="0">a type of bottom-up data-driven reasoning ) provides a hypothesis that enables top-down recognition to continue another level of growth</definiens>
			</definition>
			<definition id="13">
				<sentence>Note that a chain may have a length greater than three , e.g. , the chain may consist of propositions ( p0 , pl , P2 , P3 ) , where P0 is the proposition to be related to the candidate plan , p3 is the goal , and P2 would be returned as a hypothesized proposition .</sentence>
				<definiendum id="0">P0</definiendum>
				<definiens id="0">the proposition to be related to the candidate plan</definiens>
			</definition>
			<definition id="14">
				<sentence>The answer recognizer returns a partially ordered set ( possibly empty ) of answer discourse plans that it is plausible to ascribe to R as underlying ( part or all of ) the turn .</sentence>
				<definiendum id="0">answer recognizer</definiendum>
				<definiens id="0">returns a partially ordered set ( possibly empty ) of answer discourse plans that it is plausible to ascribe to R as underlying ( part or all of ) the turn</definiens>
			</definition>
			<definition id="15">
				<sentence>Hirschberg 's model ( Hirschberg 1985 ) addresses a class of conversational implicatures , scalar implicatures , which overlaps with the class of implicated answers addressed in our model .</sentence>
				<definiendum id="0">scalar implicatures</definiendum>
				<definiens id="0">overlaps with the class of implicated answers addressed in our model</definiens>
			</definition>
			<definition id="16">
				<sentence>Her model provides licensing rules that specify , given such a set , which scalar implicatures are 407 Computational Linguistics Volume 25 , Number 3 It is mutually plausible to the agent that ( cr-contrast q p* ) holds , where q is a proposition and p* is the proposition that p is partly true , if the agent believes it to be mutually believed that q is less than p in a salient partial order , unless it is mutually believed that p is true or that q is not true .</sentence>
				<definiendum id="0">q</definiendum>
				<definiens id="0">the proposition that p is partly true , if the agent believes it to be mutually believed that q is less than p in a salient partial order</definiens>
			</definition>
			<definition id="17">
				<sentence>44 According to Levinson ( 1983 ) , the presence of an explanation is a distinguishing feature of dispreferred responses to questions and other second parts of adjacency pairs ( Schegloff 1972 ) .</sentence>
				<definiendum id="0">presence of an explanation</definiendum>
			</definition>
			<definition id="18">
				<sentence>For example in ( 1 ) , repeated below as ( 25 ) , R gives an explanation of why R wo n't get a car .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">gives an explanation of why R wo n't get a car</definiens>
			</definition>
			<definition id="19">
				<sentence>R : No no no. iv .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">No no no. iv</definiens>
			</definition>
			<definition id="20">
				<sentence>Q : Is the blue block on the table surface ?</sentence>
				<definiendum id="0">Q</definiendum>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>If necessary , SIMR can be used with the Geometric Segment Alignment ( GSA ) algorithm , which uses segment boundary information to reduce general bitext maps to segment alignments .</sentence>
				<definiendum id="0">SIMR</definiendum>
				<definiendum id="1">Geometric Segment Alignment</definiendum>
				<definiens id="0">uses segment boundary information to reduce general bitext maps to segment alignments</definiens>
			</definition>
			<definition id="1">
				<sentence>Given parallel texts U and V , an alignment is a segmentation of U and V into n segments each , so that for each i , 1 &lt; i &lt; n , ui and vi are mutual translations .</sentence>
				<definiendum id="0">alignment</definiendum>
				<definiens id="0">a segmentation of U and V into n segments each , so that for each i , 1 &lt; i &lt; n , ui and vi are mutual translations</definiens>
			</definition>
			<definition id="2">
				<sentence>A translation lexicon T can be represented as a sequence of t entries , where each entry is a pair of words : T ~ / ( xl , yl ) ... .. ( xt , yt ) l. Roughly speaking , Wu ( 1994 ) extended Gale and Church 's ( 1991a ) method with a matching function m ( u , v , j ) , which was equal to one whenever xj E u and yj E v for lexicon entry ( xj , yj ) , and zero otherwise• The information in the matching function was then used along with the information in d ( ui , Vi ) to condition the probability of alignments in Equation 3 : IAI Amax = arg max H Pr ( aild ( ui '' vi ) ; m ( ui , vi , 1 ) ... .. m ( ui , vi , t ) ) .</sentence>
				<definiendum id="0">translation lexicon</definiendum>
				<definiens id="0">IAI Amax = arg max H Pr ( aild ( ui '' vi ) ; m ( ui , vi , 1 ) ... .. m ( ui , vi , t ) )</definiens>
			</definition>
			<definition id="3">
				<sentence>Like the algorithms of Gale and Church ( 1991a ) and Brown , Lai , and Mercer ( 1991 ) , SIMR exploits the cor111 Computational Linguistics Volume 25 , Number 1 relation between the lengths of mutual translations .</sentence>
				<definiendum id="0">SIMR</definiendum>
				<definiens id="0">exploits the cor111 Computational Linguistics Volume 25 , Number 1 relation between the lengths of mutual translations</definiens>
			</definition>
			<definition id="4">
				<sentence>The most likely chain of TPCs is the set of points whose geometric arrangement most resembles the typical arrangement of TPCs .</sentence>
				<definiendum id="0">most likely chain of TPCs</definiendum>
				<definiens id="0">the set of points whose geometric arrangement most resembles the typical arrangement of TPCs</definiens>
			</definition>
			<definition id="5">
				<sentence>SIMR employs a simple heuristic to select regions of the bitext space to search .</sentence>
				<definiendum id="0">SIMR</definiendum>
				<definiens id="0">employs a simple heuristic to select regions of the bitext space to search</definiens>
			</definition>
			<definition id="6">
				<sentence>A matching predicate is a heuristic for deciding whether two given tokens might be mutual translations .</sentence>
				<definiendum id="0">matching predicate</definiendum>
				<definiens id="0">a heuristic for deciding whether two given tokens might be mutual translations</definiens>
			</definition>
			<definition id="7">
				<sentence>The matching predicates in SIMR 's current implementation threshold the Longest Common Subsequence Ratio ( LCSR ) .</sentence>
				<definiendum id="0">matching</definiendum>
			</definition>
			<definition id="8">
				<sentence>The weight of the most likely path is an estimate of the probability that the former is a transliteration of the latter .</sentence>
				<definiendum id="0">weight of the most likely path</definiendum>
				<definiens id="0">an estimate of the probability that the former is a transliteration of the latter</definiens>
			</definition>
			<definition id="9">
				<sentence>space containing n points , there will be only n k + 1 such subsequences of length k. The most computationally expensive step in the chain recognition process is the insertion of candidate points into the sorted point sequence .</sentence>
				<definiendum id="0">recognition process</definiendum>
				<definiens id="0">subsequences of length k. The most computationally expensive step in the chain</definiens>
			</definition>
			<definition id="10">
				<sentence>SIMR has no problem with small nonmonotonic segments inside chains .</sentence>
				<definiendum id="0">SIMR</definiendum>
			</definition>
			<definition id="11">
				<sentence>The TBM consists of a set of TPCs .</sentence>
				<definiendum id="0">TBM</definiendum>
			</definition>
			<definition id="12">
				<sentence>Formally , an alignment is a correspondence relation that does not permit crossing correspondences .</sentence>
				<definiendum id="0">alignment</definiendum>
				<definiens id="0">a correspondence relation that does not permit crossing correspondences</definiens>
			</definition>
			<definition id="13">
				<sentence>The rest of this article presents the Geometric Segment Alignment ( GSA ) algorithm , which uses segment boundary information to reduce the correspondence relation in SIMR 's output to a segment alignment .</sentence>
				<definiendum id="0">Geometric Segment Alignment</definiendum>
				<definiens id="0">uses segment boundary information to reduce the correspondence relation in SIMR 's output to a segment alignment</definiens>
			</definition>
			<definition id="14">
				<sentence>GSA employs several backing-off heuristics to reduce the number of errors .</sentence>
				<definiendum id="0">GSA</definiendum>
				<definiens id="0">employs several backing-off heuristics to reduce the number of errors</definiens>
			</definition>
			<definition id="15">
				<sentence>To reduce such errors , GSA asks Gale &amp; Church 's length-based alignment algorithm ( Gale and Church 1991a ; Michel Simard , personal communication ) for a second opinion on any aligned block that is not lx 1 .</sentence>
				<definiendum id="0">GSA</definiendum>
				<definiens id="0">asks Gale &amp; Church 's length-based alignment algorithm ( Gale and Church 1991a ; Michel Simard , personal communication</definiens>
			</definition>
			<definition id="16">
				<sentence>The Smooth Injective Map Recognizer ( SIMR ) is based on innovative approaches to each of the three main components of a bitext mapping algorithm : signal generation , noise tering , and search .</sentence>
				<definiendum id="0">Smooth Injective Map Recognizer ( SIMR</definiendum>
				<definiens id="0">signal generation , noise tering , and search</definiens>
			</definition>
			<definition id="17">
				<sentence>SIMR encapsulates its language-specific heuristics , so that it can be ported to any language pair with a minimal effort ( Melamed 1997 ) .</sentence>
				<definiendum id="0">SIMR</definiendum>
			</definition>
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>It relates differences in coherence ( in part ) to varying demands on inferences as required by different types of referring expressions , given a particular attentional state of the hearer in a discourse setting ( Grosz , Joshi , and Weinstein 1995 , 204-205 ) .</sentence>
				<definiendum id="0">Weinstein</definiendum>
				<definiens id="0">relates differences in coherence ( in part ) to varying demands on inferences as required by different types of referring expressions , given a particular attentional state of the hearer in a discourse setting ( Grosz , Joshi , and</definiens>
			</definition>
			<definition id="1">
				<sentence>The centering model , in addition , defines transition relations across pairs of adjacent utterances ( Table 2 ) .</sentence>
				<definiendum id="0">centering model</definiendum>
				<definiens id="0">in addition , defines transition relations across pairs of adjacent utterances</definiens>
			</definition>
			<definition id="2">
				<sentence>Cb : Cf : \ [ SENTRY : sentry\ ] ( 2b ) He was , in fact , showing signs of reviving ... Cb : SENTRY : he CONTINUE Cf : \ [ SENTRY : he , SIGNS : signs\ ] ( 2c ) He was partially uniformed in a cavalry tunic .</sentence>
				<definiendum id="0">SIGNS</definiendum>
				<definiens id="0">in fact , showing signs of reviving ... Cb : SENTRY : he CONTINUE Cf : \ [ SENTRY : he ,</definiens>
			</definition>
			<definition id="3">
				<sentence>Cb : SENTRY : him RETAIN Cf : \ [ MIKE : Mike , TUNIC : this , it , SENTRY : him\ ] ( 2e ) He tied and gagged the man ... . Cb : MIKE : he SMOOTH-SHIFT Cf : \ [ MIKE : he , SENTRY : the man\ ] Cb : ... ...</sentence>
				<definiendum id="0">TUNIC</definiendum>
				<definiendum id="1">SENTRY</definiendum>
				<definiendum id="2">SENTRY</definiendum>
				<definiens id="0">he SMOOTH-SHIFT Cf : \ [ MIKE : he ,</definiens>
			</definition>
			<definition id="4">
				<sentence>The set of hearer-old discourse entities ( OLD ) consists of evoked ( E ) and unused ( U ) discourse entities , while the set of hearer-new discourse entities ( NEW ) consists of brand-new ( BN ) discourse entities .</sentence>
				<definiendum id="0">hearer-old discourse entities</definiendum>
				<definiendum id="1">OLD )</definiendum>
				<definiendum id="2">hearer-new discourse entities</definiendum>
				<definiens id="0">consists of evoked ( E ) and unused ( U ) discourse entities</definiens>
			</definition>
			<definition id="5">
				<sentence>On Prince 's ( 1981 ) familiarity scale , the set of hearer-old discourse entities ( OLD ) remains the same as before , i.e. , it consists of evoked ( E ) and unused ( U ) discourse entities , while the set of hearer-new discourse entities ( NEW ) now consists only of brand-new ( BN ) discourse entities .</sentence>
				<definiendum id="0">OLD )</definiendum>
			</definition>
			<definition id="6">
				<sentence>We assume Mike in ( 2d ) to be evoked , too ( MIKE is the main character of that story ) .</sentence>
				<definiendum id="0">MIKE</definiendum>
				<definiens id="0">the main character of that story )</definiens>
			</definition>
			<definition id="7">
				<sentence>Following Walker ( 1989 ) , a discourse segment is defined as a paragraph unless its first sentence has a pronoun in subject position or a pronoun whose syntactic features do not match the syntactic features of any of the preceding sentence-internal noun phrases .</sentence>
				<definiendum id="0">discourse segment</definiendum>
				<definiens id="0">a paragraph unless its first sentence has a pronoun in subject position or a pronoun whose syntactic features do not match the syntactic features of any of the preceding sentence-internal noun phrases</definiens>
			</definition>
			<definition id="8">
				<sentence>( 9a ) Cb : Cf : \ [ SENTENCE : Satz , dem , der , der , RUTH : Ruth Messinger , ihr , DEBATES : Fernsehdebatten , RACE : Biirgermeisterwahlkampf , NEW YORK : New York , RECOLLECTION : Erinnerung\ ] ( 9b ) Cb : SENTENCE : er CONTINUE Cf : \ [ SENTENCE : er , VICTORY : Wahlsieg , GIULIANI : Rudolph Giuliani\ ] ( 9c ) Cb : SENTENCE : ihn RETAIN Cf : \ [ NEWSPAPERS : Zeitungen , SENTENCE : ihn , NEW YORK : Stadt\ ] Cb ... ... ... ... ... : ~ ut~ .</sentence>
				<definiendum id="0">DEBATES</definiendum>
				<definiendum id="1">VICTORY</definiendum>
				<definiens id="0">Cf : \ [ SENTENCE : Satz , dem , der , der , RUTH : Ruth Messinger , ihr ,</definiens>
			</definition>
			<definition id="9">
				<sentence>( 10a ) Cb : Cf : \ [ BRENNAN : Brennan , ALFA ROMEO : Alfa Romeo\ ] ( 10b ) Cb : \ [ BRENNAN : she\ ] CONTINUE Cf : \ [ BRENNAN : she\ ] ( 10c ) Cb : \ [ BRENNAN : her\ ] RETAIN Cf : \ [ FRIEDMAN : Friedman , BRENNAN : her\ ] ( 10d ) Cb : \ [ BRENNAN : she\ ] CONTINUE Cf : \ [ BRENNAN : she\ ] Cb : \ [ FRIEDMAN : slie\ ] SMOOTH-SHIFT Cf : \ [ FaiE~l '' ~ia~ .</sentence>
				<definiendum id="0">BRENNAN</definiendum>
				<definiens id="0">her\ ] ( 10d ) Cb : \ [ BRENNAN : she\ ] CONTINUE Cf : \ [ BRENNAN : she\ ] Cb : \ [ FRIEDMAN : slie\ ] SMOOTH-SHIFT Cf : \ [ FaiE~l '' ~ia~</definiens>
			</definition>
			<definition id="10">
				<sentence>( 10a ) Cb : Cf : \ [ BRENNAN : Brennan , ALFA ROMEO : Alfa Romeo\ ] ( 10b ) Cb : \ [ BRENNAN : she\ ] Cf : \ [ BRENNAN : she\ ] CONTINUE ( 10c ' ) CD : \ [ BRENNAN : her\ ] Cf : \ [ DRIVER : driver , BRENNAN : her\ ] RETAIN ( 10d ) Cb : \ [ BRENNAN : she\ ] Cf : \ [ BRENNAN : she\ ] CONTINUE Cb : \ [ D F~7¢F , R. d~e\ ] Cf : \ [ D~ivF~a. she\ ] ... ... .. ... .. ( lOd ' ) Cb : \ [ DRIVER : she\ ] SMOOTH-SHIFT Cf : \ [ DRIVER : she , BRENNAN : her\ ] Cb : \ [ DKIVEF~ .</sentence>
				<definiendum id="0">BRENNAN</definiendum>
				<definiendum id="1">BRENNAN</definiendum>
				<definiens id="0">she\ ] CONTINUE Cb : \ [ D F~7¢F , R. d~e\ ] Cf : \ [ D~ivF~a. she\ ] ... ... .. ... .. ( lOd '</definiens>
				<definiens id="1">she\ ] SMOOTH-SHIFT Cf : \ [ DRIVER : she ,</definiens>
			</definition>
			<definition id="11">
				<sentence>Furthermore , ML procedures operate on incomplete parses ( hence , they accept noisy data ) , which dis337 Computational Linguistics Volume 25 , Number 3 tinguishes them from the requirements of perfect information and high data fidelity imposed by almost any other anaphora resolution scheme .</sentence>
				<definiendum id="0">ML procedures</definiendum>
				<definiens id="0">operate on incomplete parses ( hence , they accept noisy data ) , which dis337 Computational Linguistics Volume 25 , Number 3 tinguishes them from the requirements of perfect information and high data fidelity imposed by almost any other anaphora resolution scheme</definiens>
			</definition>
			<definition id="12">
				<sentence>339 Computational Linguistics Volume 25 , Number 3 . . . The centering model covers the standard cases of anaphora , i.e. , pronominal and nominal anaphora and even functional anaphora based on the proposal we have developed in this article .</sentence>
				<definiendum id="0">centering model</definiendum>
				<definiens id="0">covers the standard cases of anaphora , i.e. , pronominal and nominal anaphora</definiens>
			</definition>
</paper>

	</volume>
