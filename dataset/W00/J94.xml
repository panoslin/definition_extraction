<?xml version="1.0" encoding="UTF-8"?>
	<volume id="J94">

		<paper id="2004">
			<definition id="0">
				<sentence>234 Janyce M. Wiebe Tracking Point of View in Narrative Following Ann Banfield ( 1982 ) , a literary theorist who analyzes point of view linguistically , we shall call sentences that take a character 's psychological point of view ( hereafter , simply point of view or POV ) subjective , in contrast to sentences that objectively narrate events or describe the fictional world .</sentence>
				<definiendum id="0">POV</definiendum>
				<definiens id="0">in contrast to sentences that objectively narrate events or describe the fictional world</definiens>
			</definition>
			<definition id="1">
				<sentence>The SC is the subject of a narrative parenthetical , when one is present ; the implicit experiencer , when the sentence is a represented thought or represented perception without a narrative parenthetical ; and the explicit experiencer , when the sentence is a private-state report .</sentence>
				<definiendum id="0">SC</definiendum>
				<definiens id="0">the subject of a narrative parenthetical , when one is present ; the implicit experiencer , when the sentence is a represented thought or represented perception without a narrative parenthetical ; and the explicit experiencer</definiens>
			</definition>
			<definition id="2">
				<sentence>Sentence s resumes y 's POV : s is a subjective sentence of y and is preceded by an objective context , which is in turn preceded by a subjective context of the same character y. Sentence s initiates y 's POV : s is a subjective sentence of y and either s is the first subjective sentence of a scene , or the SC of the previous subjective sentence is a different character z. The approach taken in this work is to seek , by extensive examination of naturally occurring narratives , regularities in the ways that authors initiate , resume , and continue a character 's point of view , and to develop an algorithm that tracks point of view on the basis of the regularities found .</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">a subjective sentence of y and is preceded by an objective context , which is in turn preceded by a subjective context of the same character y. Sentence s initiates y 's</definiens>
				<definiens id="1">a subjective sentence of y and either</definiens>
				<definiens id="2">the first subjective sentence of a scene</definiens>
			</definition>
			<definition id="3">
				<sentence>• Function ITEM maps the text and the current position in the text into the input item at that position , i.e. , the paragraph break , scene break ( see Section 8.2.1 ) , or sentence ( see Section 6.3.2 ) that is at the current position : ITEM : Text x Position -- + Inputltem .</sentence>
				<definiendum id="0">Function ITEM</definiendum>
				<definiendum id="1">scene break</definiendum>
				<definiens id="0">maps the text and the current position in the text into the input item at that position</definiens>
				<definiens id="1">at the current position : ITEM : Text x Position -- + Inputltem</definiens>
			</definition>
			<definition id="4">
				<sentence>A FeatureSet consists of the following ( some of the features will be expanded upon in later sections , as indicated ; for extensive detail , see Wiebe \ [ 1990\ ] ) : . . . The potential subjective elements in the sentence , if any .</sentence>
				<definiendum id="0">FeatureSet</definiendum>
				<definiendum id="1">potential subjective</definiendum>
				<definiens id="0">consists of the following ( some of the features will be expanded upon in later sections</definiens>
			</definition>
			<definition id="5">
				<sentence>If an experiencer or actor is not mentioned in the sentence ( as is the case for the subject nouns in ( 10 ) and ( 11 ) ) , then that experiencer or actor is the empty set .</sentence>
				<definiendum id="0">actor</definiendum>
				<definiens id="0">the empty set</definiens>
			</definition>
			<definition id="6">
				<sentence>A Context consists of ( 1 ) the identity of the SC of the last subjective sentence that appeared in the text , if there was one , ( 2 ) the identity of the last active character ( defined in Section 8.2.2 ) , if there was one , ( 3 ) the identities of any characters whose points of view were taken earlier in the text , and ( 4 ) the current text situation ( defined just below in Section 6.3.4 ) : Context = { LastSC , LastActiveCharacter , PreviousSCs , TextS ituation ) An Interpretation is either that the sentence is the subjective sentence of a particular character , or that the sentence is objective and has a particular active character ( ActiveCharacter is the empty set for objective sentences without active characters ) : Interpretation E { { objective , ActiveCharacter } \ ] ActiveCharacter C Characters } U { { subjective , SC } i SC c Characters } .</sentence>
				<definiendum id="0">Context</definiendum>
				<definiendum id="1">ActiveCharacter</definiendum>
				<definiens id="0">consists of ( 1 ) the identity of the SC of the last subjective sentence that appeared in the text</definiens>
				<definiens id="1">the identities of any characters whose points of view were taken earlier in the text , and ( 4 ) the current text situation ( defined just below in Section 6.3.4 ) : Context = { LastSC , LastActiveCharacter , PreviousSCs , TextS ituation ) An Interpretation is either that the sentence is the subjective sentence of a particular character , or that the sentence is objective and has a particular active character (</definiens>
				<definiens id="2">the empty set for objective sentences without active characters ) : Interpretation E { { objective , ActiveCharacter } \ ] ActiveCharacter C Characters } U { { subjective , SC } i SC c Characters }</definiens>
			</definition>
			<definition id="7">
				<sentence>242 Janyce M. Wiebe Tracking Point of View in Narrative The Context of the i th input item in text t , ci , is ( { } , { } , { } , presubjective-nonactive ) NEW-CONTEXT ' ( ITEM ( t , i -1 ) ~ ci-1 ) Ci NEW-CONTEXT ( POV ( FEATURES ( ITEM ( t , i -1 ) ) , ci-1 ) ~ Ci-1 ) ifi=l ifi &gt; l &amp; ~SENTENCE ( ITEM ( t , i-1 ) ) ifi &gt; l &amp; SENTENCE ( ITEM ( t , i -1 ) ) where `` presubjective-nonactive '' is the text situation of the first item in a text , and SENTENCE ( X ) is true iff x is a sentence .</sentence>
				<definiendum id="0">POV</definiendum>
				<definiendum id="1">FEATURES ( ITEM</definiendum>
				<definiendum id="2">SENTENCE</definiendum>
				<definiens id="0">true iff x is a sentence</definiens>
			</definition>
			<definition id="8">
				<sentence>In the above definitions , LastSC , LastActiveCharacter , SC , and ActiveCharacter are sets , and PreviousSCs is a set of sets .</sentence>
				<definiendum id="0">PreviousSCs</definiendum>
				<definiens id="0">a set of sets</definiens>
			</definition>
			<definition id="9">
				<sentence>\ [ McMurtry , Lonesome Dove , p. 181\ ] Following is the specification of CHOSEN-STATE-OF-AFFAIRS .</sentence>
				<definiendum id="0">Following</definiendum>
				<definiens id="0">the specification of CHOSEN-STATE-OF-AFFAIRS</definiens>
			</definition>
			<definition id="10">
				<sentence>CHOOSE-AN-EXPECTED-SC ( featureSet , context ) if EXPERIENCER-OR-ACTOR-OF ( CHOSEN-STATE-OF-AFFAIRS ~eatureSet ) , featureSet ) = LAST-ACTIVE-CHARACTER-OF ( context ) then return LAST-SC-OF ( context ) else return LAST-ACTIVE-CHARACTER-OF ( context ) end if The criterion for choosing the last subjective character is correct for the situation in which the last subjective character 's attention is directed toward the last active character , and the sentence represents the last subjective character 's reflection about or observation of the other .</sentence>
				<definiendum id="0">CHOOSE-AN-EXPECTED-SC</definiendum>
				<definiendum id="1">EXPERIENCER-OR-ACTOR-OF</definiendum>
			</definition>
			<definition id="11">
				<sentence>Competition is ( correctly ) resolved in favor of the last subjective character ( Lorena ) , because the sentence is about the last active character ( Lippy ) .</sentence>
				<definiendum id="0">Competition</definiendum>
				<definiendum id="1">Lorena</definiendum>
				<definiens id="0">the last active character ( Lippy )</definiens>
			</definition>
			<definition id="12">
				<sentence>Consider the following schema , in which a subjective sentence S , whose SC is X , is followed , without a paragraph break , by a private-state sentence P whose experiencer is a different character Y : ¶ sentence* subjective-sentence-S private-state-sentence-P SC = X experiencer = Y Character Y is the experiencer of the private state that P is about .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">a different character Y : ¶ sentence* subjective-sentence-S private-state-sentence-P SC = X experiencer = Y Character</definiens>
			</definition>
			<definition id="13">
				<sentence>So , if some oracle were to inform you that ( 21 ) is a private-state report , you would then know that the SC is the experiencer of the private state ( John ) .</sentence>
				<definiendum id="0">SC</definiendum>
			</definition>
			<definition id="14">
				<sentence>27 '' 3 '' Deunys -- '' \ [ UEngle , Many Waters , p. 91 Sentence ( 27.2 ) is a private-state sentence ; specifically , CHOSEN-STATE-OF-AFFAIRS ( FEATURES ( 27.2 ) ) = ps , where ps is the private state that the private-state noun `` anxiety '' is about ( see Section 6.3.3 ) .</sentence>
				<definiendum id="0">CHOSEN-STATE-OF-AFFAIRS ( FEATURES</definiendum>
				<definiens id="0">a private-state sentence</definiens>
			</definition>
			<definition id="15">
				<sentence>Thus , for a private-state sentence with an unspecified experiencer , IDENTIFY-SC-FROM-THE-SENTENCE returns the empty set , and the algorithm goes on to consider expected subjective characters .</sentence>
				<definiendum id="0">IDENTIFY-SC-FROM-THE-SENTENCE</definiendum>
				<definiens id="0">returns the empty set , and the algorithm goes on to consider expected subjective characters</definiens>
			</definition>
			<definition id="16">
				<sentence>One of the arguments is a feature set , featureSet ; we will use s to refer to the sentence thatfeatureSet is a feature set of ( i.e. , FEATURES ( S ) = featureSet ) .</sentence>
				<definiendum id="0">thatfeatureSet</definiendum>
				<definiens id="0">a feature set</definiens>
				<definiens id="1">a feature set of ( i.e. , FEATURES ( S ) = featureSet )</definiens>
			</definition>
			<definition id="17">
				<sentence>IDENTIFY-SC-FROM-THE-SENTENCE ( featureSet , context ) soa ~ CHOSEN-STATE-OF-AFFAIRS ( featureSet ) if s contains a narrative parenthetical then return the subject of the narrative parenthetical else if 1 ( soa is a private state ) and 2 % not an unspecified experiencer ( EXPERIENCER-OR-ACTOR-OF ( soa , featureSet ) # { } ) and 3 ( There are no nonsubordinated subjective elements in the sentence ) and 4 : ( a ) ( ( TEXT-SITUATION-OF ( context ) # continuing-subjective ) or ( b ) : ( i ) ( ( TEXT-SITUATION-OF ( context ) = continuing-subjective ) and ( ii ) : % broadening or narrowing of POV ( 1 ) ( ( EXPERIENCER-OR-ACTOR-OF ( soa ) C LAST-SC-OF ( context ) or ( 2 ) ( EXPERIENCER-OR-ACTOR-OF ( soa ) D LAST-SC-OF ( context ) ) ) ) then return EXPERIENCER-OR-ACTOR-OF ( soa ) else return { } end if end if 256 Janyce M. Wiebe Tracking Point of View in Narrative We now turn to deciding whether or not a sentence is subjective in the first place .</sentence>
				<definiendum id="0">IDENTIFY-SC-FROM-THE-SENTENCE</definiendum>
				<definiendum id="1">soa</definiendum>
				<definiens id="0">a narrative parenthetical then return the subject of the narrative parenthetical else if 1</definiens>
				<definiens id="1">There are no nonsubordinated subjective elements in the sentence ) and 4 : ( a ) ( ( TEXT-SITUATION-OF ( context ) # continuing-subjective ) or ( b ) : ( i ) ( ( TEXT-SITUATION-OF ( context ) = continuing-subjective )</definiens>
			</definition>
			<definition id="18">
				<sentence>Now consider a text situation in which there has been a subjective sentence in the scene , but objective sentences and paragraph breaks have appeared since then : ( ii ) start-of-scene ... subjective-sentence ( objective-sentence + ¶ ) + sentence-S SC = X In S 's context in schema f/i ) , X is an expected subjective character .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">an expected subjective character</definiens>
			</definition>
			<definition id="19">
				<sentence>`` Subjective element '' is the term I use for an instance of a potential subjective element that actually is subjective in the context of use .</sentence>
				<definiendum id="0">Subjective element</definiendum>
				<definiens id="0">the term I use for an instance of a potential subjective element that actually is subjective in the context of use</definiens>
			</definition>
			<definition id="20">
				<sentence>Caenepeel suggests that states appearing in what I call the continuing-subjective situation continue the current POV .</sentence>
				<definiendum id="0">Caenepeel</definiendum>
				<definiens id="0">suggests that states appearing in what I call the continuing-subjective situation continue the current POV</definiens>
			</definition>
			<definition id="21">
				<sentence>The situation is continuing-subjective at the beginning , and Sandy is the last subjective character .</sentence>
				<definiendum id="0">Sandy</definiendum>
				<definiens id="0">the last subjective character</definiens>
			</definition>
			<definition id="22">
				<sentence>Quoted speech is a major way of communicating the beliefs , intentions , etc. , of characters who are not to become the SC ; merely the fact that what a character says expresses her point of view should not lead the reader to anticipate later subjective sentences of that character , as the reader does after a subjective sentence .</sentence>
				<definiendum id="0">Quoted speech</definiendum>
				<definiens id="0">a major way of communicating the beliefs , intentions , etc. , of characters who are not to become the SC ; merely the fact that what a character says expresses her point of view should not lead the reader to anticipate later subjective sentences of that character</definiens>
			</definition>
			<definition id="23">
				<sentence>Like quoted speech , a private-state-action sentence is a way to communicate something about the consciousness of a character who is not to become the SC .</sentence>
				<definiendum id="0">private-state-action sentence</definiendum>
				<definiens id="0">a way to communicate something about the consciousness of a character who is not to become the SC</definiens>
			</definition>
			<definition id="24">
				<sentence>ITEM ( t , i ) : input item Returns : The input item at position i in text t. SENTENCE ( item ) : Boolean Returns : true iff input item item is a sentence .</sentence>
				<definiendum id="0">ITEM</definiendum>
				<definiens id="0">a sentence</definiens>
			</definition>
			<definition id="25">
				<sentence>POTENTIAL-SUBJECTIVE-ELEMENTS ( featureSet ) : set of potential subjective elements Returns : the set of potential subjective elements in featureSet .</sentence>
				<definiendum id="0">POTENTIAL-SUBJECTIVE-ELEMENTS</definiendum>
				<definiens id="0">set of potential subjective elements Returns : the set of potential subjective elements in featureSet</definiens>
			</definition>
			<definition id="26">
				<sentence>TYPE-OF-PSE ( pse , featureSet ) : potential subjective element type ( e.g. , habitual , comparative-'like ' ) Given : pse E POTENTIAL-SUBJECTWE-ELEMENTS ( featureSet ) Returns : The type of potential subjective element that pse is .</sentence>
				<definiendum id="0">TYPE-OF-PSE</definiendum>
				<definiendum id="1">featureSet</definiendum>
				<definiens id="0">potential subjective element type</definiens>
			</definition>
			<definition id="27">
				<sentence>NARRATIVE-PARENTHETICAL ( featureSet ) : Boolean Returns : true iff the sentence contains a narrative parenthetical .</sentence>
				<definiendum id="0">NARRATIVE-PARENTHETICAL</definiendum>
				<definiens id="0">Boolean Returns : true iff the sentence contains a narrative parenthetical</definiens>
			</definition>
			<definition id="28">
				<sentence>IS-IN-THE-SIMPLE-P AST ( clause , featureSet ) : Boolean This is a pattern for other functions used by ACTIVE-CHARACTER-OF in A.4 .</sentence>
				<definiendum id="0">IS-IN-THE-SIMPLE-P AST</definiendum>
				<definiens id="0">a pattern for other functions used by ACTIVE-CHARACTER-OF in A.4</definiens>
			</definition>
			<definition id="29">
				<sentence>IS-PSE-SUBORDINATED-TO-SOA ( pse , soa , featureSet ) : Boolean Given : pse E POTENTIAL-SUBJECTIVE-ELEMENTS ~featureSet ) and soa E STATES-OF-AFFAIRSIfeatureSet ) Returns : False , if soa = SOAhn ( featureSet ) .</sentence>
				<definiendum id="0">IS-PSE-SUBORDINATED-TO-SOA</definiendum>
				<definiens id="0">Boolean Given : pse E POTENTIAL-SUBJECTIVE-ELEMENTS ~featureSet ) and soa E STATES-OF-AFFAIRSIfeatureSet ) Returns : False , if soa = SOAhn ( featureSet )</definiens>
			</definition>
			<definition id="30">
				<sentence>i~1 context ~- ( { } , { } , { } , presubjective-nonactive/ loop if ~SENTENCE ( ITEM ( text , i ) ) then context ~ NEW-CONTEXT ' ( ITEM ( text , i ) , context ) else interpretation ~-PO V ( FEATURES ( ITEM ( text , i ) ) , context ) 274 Janyce M. Wiebe Tracking Point of View in Narrative context ~NEW-CONTEXT ( interpretation , context ) end if i~i+l end loop NEW-CONTEXT ( interpretation = ( value , character ) , context = ( lastSC , lastActiveCharacter , previousSCs , textSituationl ) : context if value = subjective then ISCnew ~-character pSCne w ~ character U previousSCs laCnew ~-lastActiveCharacter tSnew ~ continuing-subjective else if character # { } then lacnew ~-character else laCnew ~ lastActiveCharacter end if \ ] SCnew ~ lastSC pSC ne w ~ previousSCs if character # { } and textSituation = presubjective-nonactive then tSnew ~presubjective-active else if character ~ ( } and textSituation C { postsubjective-nonactive , brokensubjective } then tSnew ~ postsubjective-active else if character = { } and textSituation = broken-subjective then tSnew ~-postsubjective-nonactive else if textSituation = continuing-subjective then tSn~w ~ interrupted-subjective else tSnew ~-textSituation end if end if return ( ISCnew , lacnew , pSCne w , tSnew ) NEW-CONTEXT / ( break , context = ( lastSC , lastActiveCharacter , previousSCs if break = scene-break then tSnew ~-presubjective-nonactive else if textSituation -presubjective-active then tSnew ~ presubjective-nonactive else if textSituation = continuing-subjective then tSnew ~broken-subjective else if textSituation = interrupted-subjective then tSne w ~ postsubjective-nonactive else if textSituation = postsubjective-active then tSnew ~-postsubjective-nonactive else tSnew +-textSituation end if return ( lastSC , lastActiveCharacter , previousSCs , ts , ~ew ) textSituationl ) : context 275 Computational Linguistics Volume 20 , Number 2 POV ( featureSet , context ) : Interpretation if SENTENCE-IS-SUBJECTIVE ~featureSet , context ) then return / subjective , IDENTIFY-SC ( featureSet , context ) I else return / objective , ACTIVE-CHARACTER-OF ( featureSet , context ) I end if SENTENCE-IS-SUBJECTIVEIfeatureSet , context ) : Boolean soa ~-CHOSEN-STATE-OF-AFFAIRS ( featureSet , context ) return 1 ( NARRATIVE-PARENTHETICAL ( featureSet ) ) or 2 ( SUBJECTIVE-ELEMENTS ( featureSet , context ) # { } ) ) or 3 ( TYpE-OF-SOA ( soa , featureSet ) = private-state ) or 4 : ( a ) ( ( TYPE-OF-SOA ( soa , ffeatureSet ) = private-state-action ) and ( b ) ( TO-BE-TREATED-AS-A-PRIVATE-STATE ( soa , featureSet , context ) ) or 5 : ( a ) ( ( TYPE-OF-SOA ( soa , featureSet ) = nonprivate-state ) and ( b ) ( TEXT-SITUATION-OF ( context ) = continuing-subjective ) ) SUBJECTIVE-ELEMENTS~eatureSet , context ) : set of potential subjective elements % As specified in Section A.1 , the potential subjective element categories and the % text situations with which they are associated are not listed in this paper .</sentence>
				<definiendum id="0">context</definiendum>
				<definiendum id="1">Boolean soa ~-CHOSEN-STATE-OF-AFFAIRS</definiendum>
				<definiens id="0">presubjective-nonactive then tSnew ~presubjective-active else if character ~ ( } and textSituation C { postsubjective-nonactive , brokensubjective } then tSnew ~ postsubjective-active else if character = { }</definiens>
			</definition>
			<definition id="31">
				<sentence>return { pse \ [ pse E POTENTIAL-SUBJECTIVE-ELEMENTS ( featureSet ) and pse is associated with TEXT-SITUATION-OF ( context ) } else else else end IDENTIFY-SC ~featureSet , context ) : set of characters if IDENTIFY-SC-FROM-THE-SENTENCE ( featureSet~ context ) ~ { } then return IDENTIFY-SC-FROM-THE-SENTENCE ( featureSet , context ) else if LAST-SC-IS-AN-EXPECTED-SC ( context ) and LAST-ACTIVE-CHARACTER-IS-AN-EXPECTED-SC ( context ) then return CHOOSE-AN-EXPECTED-SC ( featureSet~ context ) if LAST-SC-IS-AN-EXPECTED-SC ( context ) then return LAST-SC-OF ( context ) if LAST-ACTIVE-CHARACTER-IS-AN-EXPECTED-SC ( context ) then return LAST-ACTIVE-CHARACTER-OF ( context ) return { } if IDENTIFY-SC-FROM-THE-SENTENCE ( featureSet , context ) : set of characters soa ~ CHOSEN-STATE-OF-AFFAIRS ( featureSet , context ) if NARRATIVE-PARENTHETICAL~featureSet ) then return SUBJECT-OF-NARRATIVE-PARENTHETICAL ( featureSet ) else if 1 ( EXPERIENCER-OR-ACTOR-OF ( soa , featureSet ) # { } ) and 2 ( SUBJECTIVE-ELEMENTS-TO-CONSIDER ( soa , featureSet , context ) = { } ) and 3 : ( a ) ( ( TYPE-OF-SOA ( soa , featureSet ) = private-state ) or 276 Janyce M. Wiebe Tracking Point of View in Narrative 4 : ( b ) : ( i ) ( ( TYPE-OF-SOA ( soa , featureSet ) = private-state-action ) and ( ii ) ( TO-BE-TREATED-AS-A-PRIVATE-STATE ( soa , featureSet , context ) ) ) ) and ( a ) ( ( TEXT-SITUATION-OF ( context ) ~ continuing-subjective ) or ( b ) : ( i ) ( ( TEXT-SITUATION-OF ( context ) = continuing-subjective ) and ( ii ) : ( 1 ) ( ( EXPERIENCER-OR-ACTOR-OF ( soa ) C LAST-SC-OF ( context ) or ( 2 ) ( EXPERIENCER-OR-ACTOR-OF ( soa ) D LAST-SC-OF ( context ) ) ) ) then return EXPER1ENCER-OR-ACTOR-OF ( soa ) else return { } end if LAST-SC-IS-AN-EXPECTED-SC ( context ) : Boolean If TEXT-SITUATION-OF ( context ) ~ { presubjective-nonactive , presubjective-active } then return true else return false LAST-ACTIVE-CHARACTER-IS-AN-EXPECTED-SC ( context ) : Boolean If TEXT-SITUATION-OF ( context ) E { presubjective-active , postsubjective-active } then return true else return false end if CHOOSE-AN-EXPECTED-SC ( featureSet , context ) : set of characters if EXPERIENCER-OR-A CTOR-OF ( CHOSEN-STA TE-OF-AFFAIRS ( featureSet ) , featureSet ) = LAST-ACTIVE-CHARACTER-OF ( context ) then return LaST-SC-OF ( context ) else return LAST-ACTIVE-CHARACTER-OF ( context ) end if SUBJECTWE-ELEMENTS-TO-CONSIDER ( soa , featureSet , context ) : set of potential subjective elements return { pse I pse E SUBJECTIVE-ELEMENTS ( featureSet , context ) and IS-PSE-SUBORDINATED-TO-SOA ( pse , soa , featureSet ) and TYPE-OF-PSE ( pse , featureSet ) { habitual , comparative-'like ' , 'as'-followed-by-modifier , some kinds of intensifier adverbs. } }</sentence>
				<definiendum id="0">pse</definiendum>
				<definiendum id="1">LAST-SC-IS-AN-EXPECTED-SC</definiendum>
				<definiens id="0">TO-BE-TREATED-AS-A-PRIVATE-STATE ( soa , featureSet , context ) ) )</definiens>
			</definition>
			<definition id="32">
				<sentence>IS-IN-THE-SIMPLE-PAST is a model for the ones implied by the English statements .</sentence>
				<definiendum id="0">IS-IN-THE-SIMPLE-PAST</definiendum>
			</definition>
			<definition id="33">
				<sentence>\ [ McMurtry , Lonesome Dove , p. 200\ ] The system will be demonstrated on a slightly modified version of the critical part of this passage ( Newt is the last subjective character at the beginning ) : Newt was riding around the herd when Jake Spoon went by on his way to Lonesome Dove .</sentence>
				<definiendum id="0">Newt</definiendum>
				<definiens id="0">the last subjective character at the beginning ) : Newt was riding around the herd when Jake Spoon went by on his way to Lonesome Dove</definiens>
			</definition>
			<definition id="34">
				<sentence>Newt is the last_subj_char : Newt was riding around the herd when Jake went by on his way to Lonesome Dove .</sentence>
				<definiendum id="0">Newt</definiendum>
				<definiens id="0">the last_subj_char : Newt was riding around the herd when Jake went by on his way to Lonesome Dove</definiens>
			</definition>
			<definition id="35">
				<sentence>Before the paragraph break : The situation is postsubj-active Expected subjective characters : Newt , the last subj char Newt , the last_active_char After the paragraph break : The situation is postsubj-nonactive The last_active_char is no longer an expected subjective character The last_subj_char is still an expected subjective character : Ouoted_speech Newt asked .</sentence>
				<definiendum id="0">postsubj-active Expected subjective characters</definiendum>
				<definiens id="0">no longer an expected subjective character The last_subj_char is still an expected subjective character : Ouoted_speech Newt asked</definiens>
			</definition>
			<definition id="36">
				<sentence>Before the paragraph break : The situation is postsubj-active Expected subjective characters : Newt , the last_subj_char Newt , the last_active_char After the paragraph break : The situation is postsubj-nonactive The last active_char is no longer an expected subjective character The last_subj_char is still an expected subjective character : Quoted_speech Jake said .</sentence>
				<definiendum id="0">postsubj-active Expected subjective characters</definiendum>
				<definiens id="0">no longer an expected subjective character The last_subj_char is still an expected subjective character : Quoted_speech Jake said</definiens>
			</definition>
			<definition id="37">
				<sentence>At the beginning of this sentence : The situation is postsubj-nonactive Expected subjective character : Newt , the last_subj char Jake is the active_char of this sentence The sentence is not subjective Sentence with an active_char in postsubj-nonactive situation : situation is now postsubj-active : He did not stop to pass the time .</sentence>
				<definiendum id="0">last_subj char Jake</definiendum>
				<definiens id="0">postsubj-nonactive Expected subjective character : Newt , the</definiens>
			</definition>
			<definition id="38">
				<sentence>Lonesome Dove is an adult novel that has many subjective characters , and The Magic of the Glits is a childrens ' novel that has one main subjective character .</sentence>
				<definiendum id="0">Lonesome Dove</definiendum>
				<definiens id="0">an adult novel that has many subjective characters , and The Magic of the Glits is a childrens ' novel that has one main subjective character</definiens>
			</definition>
</paper>

		<paper id="3012">
</paper>

		<paper id="4005">
			<definition id="0">
				<sentence>Each preference function is defined as a numerical ( possibly real-valued ) function on representations corresponding to the sentence analyses .</sentence>
				<definiendum id="0">preference function</definiendum>
				<definiens id="0">a numerical ( possibly real-valued ) function on representations corresponding to the sentence analyses</definiens>
			</definition>
			<definition id="1">
				<sentence>QLFs express semantic content , but are derived compositionally from complete syntactic analyses of a sentence and therefore mirror much syntactic structure as well .</sentence>
				<definiendum id="0">QLFs</definiendum>
				<definiens id="0">express semantic content , but are derived compositionally from complete syntactic analyses of a sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>A third , SynRules , returns an estimate of the log probability of the syntactic rules used to construct the analysis .</sentence>
				<definiendum id="0">SynRules ,</definiendum>
				<definiens id="0">returns an estimate of the log probability of the syntactic rules used to construct the analysis</definiens>
			</definition>
			<definition id="3">
				<sentence>The training score functions we considered for a QLF q with respect to a treebank tree t were functions of the form score ( q , t ) = allQ N T I a21Q \ T\ ] a31T \ QI , where Q is the set of string segments induced by the term and form expressions of q ; T is the set of constituents in t ; al , a2 , and a3 are positive constants ; and the `` \ '' operator denotes set difference .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">the set of string segments induced by the term and form expressions of q ;</definiens>
			</definition>
			<definition id="4">
				<sentence>As mentioned earlier , the preference score is a weighted sum of a set of preference functions : Each preference function f/ takes a complete QLF representation qi as input , returning a numerical score sq , the overall preference score being computed by summing over the 639 Computational Linguistics Volume 20 , Number 4 product of function scores with their associated scaling factors cj : ClSil q. .</sentence>
				<definiendum id="0">preference score</definiendum>
			</definition>
			<definition id="5">
				<sentence>Suppose also that the training scores and the scores assigned by preference functions , G fl , and fz , are as follows : Training ~ fl f2 ql 10 16 8 4 q2 10 16 6 10 q3 4 10 2 12 After relativizing ( subtracting the average of the ql and q2 values ) , we get Training ~ fl f2 ql 0 0 1 -3 q2 0 0 -- 1 3 q3 -- 6 -- 6 -- 5 5 An initial set of scaling factors is calculated in a straightforward analytic way by approximating gi , the relativized training score of qi , by ~j cjzij , where cj is the scaling factor for preference function fj and zq is the relativized score assigned to qi by ~ .</sentence>
				<definiendum id="0">cj</definiendum>
				<definiens id="0">the relativized score assigned to qi by ~</definiens>
			</definition>
			<definition id="6">
				<sentence>• Mutual information : this relates the probability Pl ( a ) P2 ( b ) P3 ( c ) of the triple ( a , b~ c ) assuming independence between its three fields , where P~ ( x ) is the probability of observing x in position p , with the probability A estimated from actual observations of triples derived from analyses ranked highest ( or joint highest ) in training score .</sentence>
				<definiendum id="0">P~</definiendum>
				<definiens id="0">the probability of observing x in position p , with the probability A estimated from actual observations of triples derived from analyses ranked highest ( or joint highest ) in training score</definiens>
			</definition>
</paper>

		<paper id="3008">
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>Repeated composition reduces the machines corresponding to the rules of a complete phonological grammar to a single transducer that works with only two tapes , one containing the abstract phonological string and the other containing its phonetic realization .</sentence>
				<definiendum id="0">Repeated composition</definiendum>
				<definiens id="0">reduces the machines corresponding to the rules of a complete phonological grammar to a single transducer that works with only two tapes , one containing the abstract phonological string</definiens>
			</definition>
			<definition id="1">
				<sentence>If X = ( xl , x2 , . . . x , ) and Y = ( Yl , Y2 , . . . yn ) are n-tuples of strings , then the concatenation of X and Y , written X. Y or simply XY , is defined by X. Y= # IXlYl , X2Y2 , ... Xnyn ) That is , the n-way concatenation of two string-tuples is the tuple of strings formed by string concatenation of corresponding elements .</sentence>
				<definiendum id="0">yn )</definiendum>
				<definiendum id="1">strings</definiendum>
				<definiens id="0">the concatenation of X and Y , written X. Y or simply XY , is defined by X. Y= # IXlYl , X2Y2 , ... Xnyn ) That is , the n-way concatenation of two string-tuples is the tuple of strings formed by string concatenation of corresponding elements</definiens>
			</definition>
			<definition id="2">
				<sentence>Just as the empty string c is the identity for simple string concatenation , the n-tuple all of whose elements are e is the identity for n-way concatenation , and the length of such a tuple is zero .</sentence>
				<definiendum id="0">empty string c</definiendum>
			</definition>
			<definition id="3">
				<sentence>Recall , for example , the usual recursive definition of a regular language over an alphabet G ( superscript i denotes concatenation repeated i times , according to the usual convention , and we let G~ denote G t3 { e } ) : .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">for example , the usual recursive definition of a regular language over an alphabet G ( superscript i denotes concatenation repeated i times , according to the usual convention</definiens>
			</definition>
			<definition id="4">
				<sentence>If R1 , R2 , and R are regular n-relations , then so are R1 • R2 = { xy \ ] x E Rl , y E R2 } ( n-way concatenation ) R1 U R2 ( union ) OO .</sentence>
				<definiendum id="0">R</definiendum>
			</definition>
			<definition id="5">
				<sentence>The regular-expression notation provides for concatenation , union , and Kleene-closure of these terms .</sentence>
				<definiendum id="0">regular-expression notation</definiendum>
				<definiens id="0">provides for concatenation , union</definiens>
			</definition>
			<definition id="6">
				<sentence>A nondeterministic ( one-tape ) finite-state machine is a quintuple ( G , Q , q , F , 5 ) , where G is a finite alphabet , Q is a finite set of states , q c Q is the initial state , and F c Q is the set of final states .</sentence>
				<definiendum id="0">nondeterministic ( one-tape</definiendum>
				<definiendum id="1">G</definiendum>
				<definiendum id="2">Q</definiendum>
				<definiendum id="3">F c Q</definiendum>
				<definiens id="0">a quintuple ( G , Q , q</definiens>
				<definiens id="1">a finite set of states , q c Q is the initial state</definiens>
				<definiens id="2">the set of final states</definiens>
			</definition>
			<definition id="7">
				<sentence>A nondeterministic n-way finite-state transducer ( fst ) is defined by a quintuple similar to that of an fsm except for the transition function 6 , a total function that maps Q x E ~ x ... x E ~ to 2 Q. Partly to simplify the mathematical presentation and partly because only the binary relations are needed in the analysis of rewriting rules and Koskenniemi 's two-level systems , from here on we frame the discussion in terms of binary relations and two-tape transducers .</sentence>
				<definiendum id="0">nondeterministic n-way finite-state transducer</definiendum>
			</definition>
			<definition id="8">
				<sentence>We write xRy if the pair ( x , y ) belongs to the relation R. The image of a string x under a relation R , which we write x/R , is the set of strings y such that ( x , y ) is in R. Similarly , R/y is the set of strings that R carries onto y. We extend this notation to sets of strings in the obvious way : X/R = Uxcx x/R .</sentence>
				<definiendum id="0">R/y</definiendum>
				<definiens id="0">the relation R. The image of a string x under a relation R , which we write x/R , is the set of strings y such that ( x , y</definiens>
			</definition>
			<definition id="9">
				<sentence>For example , if R is the regular relation recognized by the transducer in Figure 4 , then R/intractable is the set of strings that R maps to intractable , namely { intractable , iNtractable } , as illustrated in Figure 7 .</sentence>
				<definiendum id="0">R/intractable</definiendum>
				<definiens id="0">the regular relation recognized by the transducer in</definiens>
				<definiens id="1">the set of strings that R maps to intractable</definiens>
			</definition>
			<definition id="10">
				<sentence>Similarly , iNtractable/R is the set of strings { intractable } that R maps from iNtractable ( Figure 5 ) .</sentence>
				<definiendum id="0">iNtractable/R</definiendum>
				<definiens id="0">the set of strings { intractable } that R maps from iNtractable ( Figure 5 )</definiens>
			</definition>
			<definition id="11">
				<sentence>We also rely on several of the closure properties of regular languages ( Hopcroft and Ullman 1979 ) : for regular languages L1 and L2 , L1L2 is the regular language containing all strings XlX 2 such that x1 E L1 and x2 C L2 .</sentence>
				<definiendum id="0">L1L2</definiendum>
				<definiens id="0">the regular language containing all strings XlX 2 such that x1 E L1 and x2 C L2</definiens>
			</definition>
			<definition id="12">
				<sentence>We write E for the complement of L , the regular language containing all strings not in L , namely , ~* L. Finally , Rev ( L ) denotes the regular language consisting of the reversal of all the strings in L. There are a number of basic connections between regular relations and regular languages .</sentence>
				<definiendum id="0">L )</definiendum>
				<definiens id="0">the complement of L , the regular language containing all strings not in L</definiens>
				<definiens id="1">the regular language consisting of the reversal of all the strings in L. There are a number of basic connections between regular relations and regular languages</definiens>
			</definition>
			<definition id="13">
				<sentence>That is , if L is a regular language and R is an arbitrary regular relation , then the languages L/R and R/L are both regular .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">a regular language and</definiens>
				<definiens id="1">an arbitrary regular relation</definiens>
			</definition>
			<definition id="14">
				<sentence>Suppose that R1 is the relation { ( a ' , bnc * ) \ [ n &gt; 0 } and R2 is the relation { ( ant b*c n ) I n &gt; _ 0 } .</sentence>
				<definiendum id="0">R2</definiendum>
				<definiens id="0">the relation { ( a ' , bnc *</definiens>
				<definiens id="1">the relation { ( ant b*c n ) I n &gt; _ 0 }</definiens>
			</definition>
			<definition id="15">
				<sentence>A path-string for any finite-state transducer T is a ( possibly empty ) sequence of symbol-pairs ul : Vl u2 : v2 .</sentence>
				<definiendum id="0">path-string</definiendum>
			</definition>
			<definition id="16">
				<sentence>Also , if P is a finite-state machine that accepts a pair-symbol language , we define the path-relation ReI ( P ) to be the relation accepted by the fst constructed from P by reinterpreting every one of its pair-symbol labels as the corresponding symbol pair of a transducer label .</sentence>
				<definiendum id="0">path-relation ReI</definiendum>
				<definiens id="0">a finite-state machine that accepts a pair-symbol language</definiens>
				<definiens id="1">the corresponding symbol pair of a transducer label</definiens>
			</definition>
			<definition id="17">
				<sentence>If L1 , L2 , and L are regular languages and R1 , R2 , and R are regular relations , then we know that the following relations are regular : R1 U R2 R1 • R2 R* R -1 RIo R2 Id ( L ) L1 × L2 Rev ( R ) We know also that the following languages are regular ( x is a string ) : Dom ( R ) Range ( R ) L/R R/L x/R R/x Furthermore , if R1 , R2 , and R are in the same-length subclass , then the following also belong to that restricted subclass : R1 t3 R2 R1 • R2 R* R -1 R1 o R2 Rev ( R ) R1 N R2 R1 R2 344 Ronald M. Kaplan and Martin Kay Regular Models of Phonological Rule Systems Id ( L ) is also same-length for all L. Intersections and relative differences of arbitrary regular relations are not necessarily regular , however .</sentence>
				<definiendum id="0">R</definiendum>
				<definiendum id="1">regular ( x</definiendum>
				<definiendum id="2">R</definiendum>
				<definiens id="0">also same-length for all L. Intersections and relative differences of arbitrary regular relations are not necessarily regular , however</definiens>
			</definition>
			<definition id="18">
				<sentence>The first operator produces a relation that freely introduces symbols from a designated set S. This relation , Intro ( S ) , is defined by the expression \ [ Id ( ~ ) U \ [ { e } x S\ ] \ ] * .</sentence>
				<definiendum id="0">first operator</definiendum>
				<definiendum id="1">Intro ( S )</definiendum>
				<definiens id="0">produces a relation that freely introduces symbols from a designated set S. This relation</definiens>
			</definition>
			<definition id="19">
				<sentence>The rule might be formulated as follows : Vi -- -+ Bi/BjC * where Bi is the back counterpart of the vowel Vi , and Bj is another ( possibly different ) back vowel .</sentence>
				<definiendum id="0">Bi</definiendum>
				<definiendum id="1">Bj</definiendum>
				<definiens id="0">formulated as follows : Vi -- -+ Bi/BjC * where</definiens>
				<definiens id="1">the back counterpart of the vowel Vi , and</definiens>
			</definition>
			<definition id="20">
				<sentence>Then our next approximation to the replacement relation is defined as follows : Replace = \ [ Id ( Gm ) Opt ( Id ( &lt; ) ~m x ~m Id ( &gt; ) ) \ ] * This allows arbitrary strings of matching symbols drawn from ~ , U { &lt; , &gt; } between rule applications and requires &lt; : &lt; and &gt; : &gt; to key off a ~-~ replacement .</sentence>
				<definiendum id="0">replacement relation</definiendum>
				<definiens id="0">follows : Replace = \ [ Id ( Gm ) Opt ( Id ( &lt; ) ~m x ~m Id ( &gt; ) ) \ ] * This allows arbitrary strings of matching symbols drawn from ~ , U { &lt; , &gt; } between rule applications and requires &lt; : &lt; and &gt; : &gt; to key off a ~-~ replacement</definiens>
			</definition>
			<definition id="21">
				<sentence>Thus , the requisite set of strings is the regular language Leftcontext ( &amp; , &lt; , &gt; ) , where the Leflcontext operator is defined as follows : Leflcontext ( &amp; , l , r ) = P-iff-S ( C ?</sentence>
				<definiendum id="0">requisite set of strings</definiendum>
			</definition>
			<definition id="22">
				<sentence>Of course , if the features are incompatible , the feature matrix will be replaced by the empty set of segments .</sentence>
				<definiendum id="0">incompatible</definiendum>
				<definiens id="0">the empty set of segments</definiens>
			</definition>
			<definition id="23">
				<sentence>The following result is then established by induction on the number of rules in the grammar : Theorem If G = ( R1 , ... , Rn ) is a grammar defined as a finite ordered sequence of rewriting rules each of which denotes a regular relation , then the set of input-output string-pairs for the grammar as a whole is the regular relation given by R1 o..</sentence>
				<definiendum id="0">Rn )</definiendum>
				<definiens id="0">established by induction on the number of rules in the grammar : Theorem If G = ( R1 , ... ,</definiens>
			</definition>
			<definition id="24">
				<sentence>A simple context restriction rule is an expression of the form ~-~A__p where ~- , ~ , and p denote subsets of 7r* .</sentence>
				<definiendum id="0">simple context restriction rule</definiendum>
				<definiens id="0">an expression of the form ~-~A__p where ~- , ~ , and p denote subsets of 7r*</definiens>
			</definition>
			<definition id="25">
				<sentence>If E is a relation for a finite list of exceptional input-output pairs and P is the general phonological relation , then the combination is given by E m \ [ Id ( Dom ( E ) ) o P\ ] This relation is regular because E is regular ( as is any finite list of pairs ) ; it suppresses the general mapping provided by P for the exceptional items , allowing outputs for them to come from E only .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">relation</definiendum>
				<definiens id="0">a relation for a finite list of exceptional input-output pairs and</definiens>
				<definiens id="1">the general phonological relation</definiens>
				<definiens id="2">regular ( as is any finite list of pairs ) ; it suppresses the general mapping provided by P for the exceptional items , allowing outputs for them to come from E only</definiens>
			</definition>
			<definition id="26">
				<sentence>`` Twoqevel morphology : A general computational model for word-form recognition and production . ''</sentence>
				<definiendum id="0">Twoqevel morphology</definiendum>
				<definiens id="0">A general computational model for word-form recognition and production</definiens>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>158 Bernard Merialdo Tagging English Text with a Probabilistic Model We consider two different types of training : • Relative Frequency ( RF ) training • Maximum Likelihood ( ML ) training which is done via the Forward-Backward ( FB ) algorithm .</sentence>
				<definiendum id="0">Maximum Likelihood</definiendum>
				<definiens id="0">a Probabilistic Model We consider two different types of training : • Relative Frequency ( RF ) training •</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>The feature structure for an utterance is the result ( if one exists ) of unifying all of the feature structures for the lexical entries and syntactic rules in appropriate ways .</sentence>
				<definiendum id="0">feature structure for an utterance</definiendum>
				<definiens id="0">if one exists ) of unifying all of the feature structures for the lexical entries and syntactic rules in appropriate ways</definiens>
			</definition>
			<definition id="1">
				<sentence>Specialized languages can be constructed specifically for the task of expressing feature structure constraints ( such as Kasper and Rounds 's FDL \ [ Kasper and Rounds 1990\ ] and Johnson 's attribute-value languages \ [ Johnson 1988\ ] ) .</sentence>
				<definiendum id="0">Specialized languages</definiendum>
			</definition>
			<definition id="2">
				<sentence>We can conceptualize of attribute-value arcs as instances of Computational Linguistics Volume 20 , Number 1 a three-place relation arc , where arc ( x , a~ y ) means that there is an arc leaving node x labeled a pointing to node y.6 Of course , not all interpretations qualify as attribute-value structures ; e.g. , those which satisfy both arc ( x~ a~ y ) and arc ( x~ a~ z ) for some y ~ z violate the requirement that there is at most one arc with any given label leaving any node .</sentence>
				<definiendum id="0">arc ( x , a~ y</definiendum>
				<definiens id="0">arc leaving node x labeled a pointing to node y.6 Of course , not all interpretations qualify as attribute-value structures</definiens>
				<definiens id="1">x~ a~ y ) and arc ( x~ a~ z</definiens>
			</definition>
			<definition id="3">
				<sentence>We can express this requirement as an SB formula that is true in the intended interpretations ( namely attribute-value feature structures ) .</sentence>
				<definiendum id="0">SB formula</definiendum>
				<definiens id="0">true in the intended interpretations ( namely attribute-value feature structures )</definiens>
			</definition>
			<definition id="4">
				<sentence>Vx Va Vy Vz arc ( x~a~y ) Aarc ( x , a , z ) ~ y = z. ( 1 ) Similarly , we can express the properties of the `` attribute-value constants '' with SB formulae .</sentence>
				<definiendum id="0">Vx Va Vy Vz arc</definiendum>
				<definiens id="0">1 ) Similarly , we can express the properties of the `` attribute-value constants '' with SB formulae</definiens>
			</definition>
			<definition id="5">
				<sentence>More precisely , being an 'attribute-value constant ' is a property of an individual in an interpretation ( i.e. , an element of a feature structure ) , whereas being a constant is a property of a symbol in a formula .</sentence>
				<definiendum id="0">constant</definiendum>
				<definiens id="0">a property of an individual in an interpretation ( i.e. , an element of a feature structure</definiens>
				<definiens id="1">a property of a symbol in a formula</definiens>
			</definition>
			<definition id="6">
				<sentence>For example , a constraint that the value of n 's al arc is bl would be represented by the atom arc ( n , al , bl ) , a constraint that the value of n 's a2 arc is b2 is represented by arc ( n , a2 , b2 ) , and a constraint that the value of n 's al arc is the same as the value of its a2 arc is represented by the conjunction arc ( n , all n ' ) A arc ( n , a2~ n ' ) ( n ~ is the single value of both arcs ) .</sentence>
				<definiendum id="0">atom arc</definiendum>
				<definiens id="0">n , al , bl ) , a constraint that the value of n 's a2 arc is b2 is represented by arc ( n , a2</definiens>
			</definition>
			<definition id="7">
				<sentence>These constraints are intended to appear as annotations on phrase structure rules ( in the same way that attribute-value constraints do ) and could be used to enforce a variety of `` long-distance '' relationships , such as the coand contra-indexing constraints of binding theory ( i.e. , equality and inequality constraints on the values of index attributes ) .</sentence>
				<definiendum id="0">rules</definiendum>
				<definiens id="0">intended to appear as annotations on phrase structure</definiens>
			</definition>
			<definition id="8">
				<sentence>VX `` ~X K X Vx Vy x &lt; y ~ ~y &lt; x VxVyVzx &lt; yAy &lt; z-+x &lt; z Vx D ( x , x ) +-+ N ( x ) Vx Vy D ( x , y ) A D ( y , x ) ~ x = y Vx Vy Vz D ( x , y ) A D ( y , z ) ~ D ( x , z ) ( irreflexivity ) ( 7a ) ( asymmetry ) ( 7b ) ( transitive closure ) ( 7c ) ( reflexivity ) ( 8a ) ( antisymmetry ) ( 8b ) ( transitive closure ) ( 8c ) Axiom ( 9 ) requires that there is a node that dominates all other nodes , and axiom ( 10 ) requires that for each pair of nodes either one precedes the other or one dominates the other. Axiom ( 11 ) enforces the `` no tangling '' constraint. 3 x X ( x ) A Vy X ( y ) + D ( x , y ) Vx Vy X ( x ) A X ( y ) + ( ( x &lt; y v y &lt; x ) -~ ( D ( x , y ) v D ( y , x ) ) ) Vw Vx Vy Vz w &lt; xA D ( w , y ) AD ( x , z ) ~ y &lt; z ( single root condition ) ( 9 ) ( exclusivity ) ( 10 ) ( nontangling condition ) ( 11 ) Finally , the following axioms ( implicit in the standard treatments cited above ) require the precedence and dominance relations to range over tree nodes. vx Vy x &lt; y + X ( x ) A X ( y ) ( 12a ) Vx Vy D ( x , y ) -~ N ( x ) A N ( y ) ( 12b ) This concludes the specification of linear precedence and dominance relations over nodes. We now turn to the specification of other relations in terms of these. The proper dominance relation P can be defined in terms of dominance as follows. Vx Vy p ( x , y ) ~ x # y A D ( x , y ) . ( 13 ) However , many interesting linguistic relations can not be defined by Sch6nfinkel-Bernays ' axioms. For example , the c-commands relation C is defined by the following formula ( which says that x c-commands y iff x does not dominate y , and every node z that properly dominates x also properly dominates y ) . Vx vy C ( x , y ) ~ ~D ( x , y ) A Vz e ( z , x ) ~ P ( z , y ) . ( 14 ) It is easy to see that this definition is not equivalent to a Sch6nfinkel-Bernays ' formula by expanding the equivalence into two implications and moving the embedded quantifier out. Vx Vy Vz C ( x , y ) + ( -~D ( x , y ) A ( P ( z , x ) ~ P ( z , y ) ) ) . ( 14a ) Vx Vy 3z ( ~D ( x , y ) A P ( z , x ) ~ P ( z , y ) ) ~ C ( x , y ) . ( 14b ) Formula ( 14b ) is not in SB because it contains an existential quantifier inside the scope of a universal quantifier. There are a number of ways to respond to this problem. First , we can abandon the attempt to work within the Sch6nfinkel-Bernays ' class , and work with some other language. Rounds ( 1988 ) describes such a language called LFP , whose decidability follows from the fact that the domain of quantification is Computational Linguistics Volume 20 , Number 1 restricted ( just as in SB ) . However , it seems to be difficult to devise a decidable system capable of simultaneously expressing both tree structure and the variety of feature structure constraints that the SB approach described here can. Blackburn , Gardent , and Meyer-viol ( 1993 ) introduce a modal language L T for describing trees decorated with feature structures , whose satisfiability problem is undecidable. In the long run , such specialized `` designer logics '' may provide the most satisfying integration of tree structure and feature structure constraints. Second , the 'one-sided ' approximation ( 14a ) can be used in place of the correct axiom ( 14 ) . The effect of using such one-sided approximations was investigated in Johnson ( 1991a ) . It was shown there that if ~ is a formula such as the one in ( 14 ) and ~/is the one-sided approximation ( 14a ) , then for any formula ~+ ( C ) in which C only appears positively , ~ A ~+ ( C ) is satisfiable iff ~ ' A ~+ ( C ) is satisfiable. That is , if we are concerned only with positively occurring constraints , we can simplify ( 14 ) to ( 14a ) , i.e. , ignore ( 14b ) , without affecting constraint satisfiability. Third , we can regard formulae such as ( 14 ) as the `` macro '' ( 15 ) , used to expand constraints at the interface between the syntactic rules and the constraint solver. This `` macro expansion '' rewrites c-commands constraints into boolean combinations of constraints that the constraint solver can handle. C ( x , y ) ~ - @ ( x~y ) A vz p ( z~x ) ~ P ( z~y ) . ( 15a ) -~C ( x~y ) ~ D ( x~y ) v 3z ( P ( z~x ) A -~P ( z~y ) ) . ( 15b ) The second and the third approaches differ in important ways. In the second approach , c-commands is a relation that is `` understood '' by the constraint solver ( albeit only in its one-sided form ) , so it can be used to define other relations. In the third approach , c-commands constraints are not primitive constraints , so relations defined in terms of c-commands must also be expressible in terms of `` macro expansion. '' In the second approach , constraints are quantifier-free formulae ( quantifiers appear only in the axioms ) , so the satisfiability problem is in NP. But in the third approach , macro expansion produces formulae that contain additional quantifiers , so the satisfiability problem may be PSPACE-complete. Of course , SB is not as expressive as full first-order logic. It is incapable of expressing functional relationships , since these require an existential quantifier inside the scope of a universal quantifier. This means , among other things , that it is impossible to state a constraint in SB requiring that a certain node must exist ( as was noted in the discussion of c-command in the previous section ) or that all nodes possess certain attributes. Thus for example , the following constraint , which requires that every tensed entity possess number and person attributes , is a first-order formula that is not in SB , since it requires a functional relationship between entities with tense attributes and the values of their number and person attributes. Vx Vy arc ( x~ tensG y ) ~ ( 3z arc ( x~ number~ z ) ) A ( 3z arc ( x~ person~ z ) ) ( 16 ) Similarly , a number of other extensions to the basic attribute-value framework discussed in the literature can not be formalized in SB. Subsumption constraints , used in the treatment of ( natural language ) conjunction , are not expressible as SB formulae because the satisfiability problem for conjunctions of subsumption and attribute-value constraints is undecidable ( D6rre and Rounds 1992 ) . Positively occurring functional 10 Mark Johnson Computing with Features as Formulae ( El ) Vxx=x. ( E2 ) VxVyx=y~y=x. ( E3 ) Vx0 ... VXn xj = x 0 A P ( Xl~ ... ~ xj~ ... ~ xn ) ~ P ( Xl~ ... ~ Xo~ ... ~ Xn ) for j = 1~ ... ~ n , for every predicate symbol P appearing in ~. ( E4 ) VXo ... VXn xj =x0 -- ~ f ( xl~ ... ~Xj~ ... ~Xn ) =f ( Xl~ ... ~Xo~ ... ~Xn ) for j = 1~ ... ~ n , for every function symbol f appearing in ~. Figure 2 Equality axiom schemata for a first-order formula ~. uncertainty constraints , used in the LFG treatment of long-distance dependencies ( Kaplan and Zaenen 1989 ) appear to have a decidable satisfiability problem ( Kaplan and Maxwell 1988a ) , but the satisfiability problem for arbitrary boolean combinations of functional uncertainty constraints is undecidable ( Keller 1991 ) , so these can not be expressed using SB formulae either ( since the quantifier-free subclass of SB is closed under boolean operations ) . In this paper the intended interpretation of the equality relation is identity ; i.e. , a = b if and only if a and b denote the same individual. However , for some purposes ( e.g. , in the least-fixed-point characterization of minimal models given below ) this `` special '' interpretation of the equality complicates matters , and it is more convenient to treat the equality relation as a `` normal '' relation that is defined by a set of axioms E. The idea is that E has the property that a formula ~ is satisfiable under the identity interpretation of equality if and only if { 9~ } U E is satisfiable in an interpretation in which equality is not given any special treatment. In effect , the axioms E require that the equality relation denotes an equivalence relation , and permit the substitution of equals for equals. Together these imply that no predicate can distinguish equal individuals. This means that in terms of satisfiability and the consequence relation , exactly the same results are obtained irrespective of whether equality is treated as identity or defined by the axioms E. Such treatments of equality in first-order logic are well known and described in standard texts. For example , Chang and Lee ( 1973 ) give the axiom schemata in Figure 2 , which generates syntactic equality axioms E for a first-order formula 9~ , and prove that E has the properties just described , s What is important here is that for an SB formula ~ the instances of the axiom schemata are all SB formulae , and there are only finitely many instances of these schemata. This means that for an arbitrary SB formula ~ there is another SB formula ~ such that ~ is satisfiable with respect to an identity interpretation of equality if and only if A ~ is satisfiable with respect to an interpretation in which equality is treated like any other relation. Thus a method for determining the satisfiability of SB formulae without equality can be used to determine satisfiability of SB formulae in which equality is interpreted as identity. symbols. 11 Computational Linguistics Volume 20 , Number 1 ( 17 ) Vx x = x. ( 18 ) Vx Vy x = y ~ y = x. ( 19 ) Vx Va Vy VXl x -~ Xl /X arc ( x , a , y ) ~ arc ( x1 , a , y ) . ( 20 ) Vx Va Vy Val a = al A arc ( x , a , y ) ~ arc ( x , al , y ) . ( 21 ) Vx Va Vy Vyl y = yl Aarc ( x , a , y ) ~ arc ( x , a , yl ) . ( 22 ) VCl Vc2 c = Cl A con ( c ) ~ con ( c1 ) . ( 23 ) Vx Vy x = y A 3rd-sg ( x ) ~ 3rd-sg ( y ) . Figure 3 The equality axioms for arc , con , and 3rd-sg predicates. For example , consider the SB formulae in ( 1-4 ) and ( 5-16 ) . These contain the three-place relation symbol arc and the one-place relation symbols con and 3rd-sg. The equality axioms obtained from schemata ( El-E4 ) for any system of constraints that mention just these relations are given in Figure 3. It is technically easier to work with a syntactically restricted class of SB formulae where the body of each formula has a particular syntactic form known as clausal form or Skolem standard form. Definition A clause is a formula of the form , -~ o~1 V • • • V ~- , c~m V fll V • • • V ft , , where each oq and fly is an atomic formula ( i.e. , is of the form p ( h , ... , tn ) ) , and m , n &gt; 0 .</sentence>
				<definiendum id="0">VX `` ~X K X Vx Vy x &lt; y ~ ~y &lt; x VxVyVzx &lt; yAy &lt; z-+x &lt; z Vx D ( x , x ) +-+ N ( x ) Vx Vy D ( x , y ) A D ( y , x ) ~ x = y Vx Vy Vz D ( x , y ) A D</definiendum>
				<definiendum id="1">z ) ( irreflexivity ) ( 7a ) ( asymmetry ) ( 7b ) ( transitive closure ) ( 7c ) ( reflexivity ) ( 8a ) ( antisymmetry ) ( 8b ) ( transitive closure ) ( 8c ) Axiom ( 9 ) requires</definiendum>
				<definiendum id="2">Axiom ( 11 ) enforces</definiendum>
				<definiendum id="3">X ( x ) A Vy X</definiendum>
				<definiendum id="4">Vx Vy X ( x ) A X ( y ) + ( ( x &lt; y v y &lt; x ) -~</definiendum>
				<definiendum id="5">y ) v D ( y , x ) ) ) Vw Vx Vy Vz w &lt; xA D</definiendum>
				<definiendum id="6">N ( x ) A N ( y ) ( 12b ) This</definiendum>
				<definiendum id="7">. ( 15a ) -~C ( x~y ) ~ D</definiendum>
				<definiens id="0">a node that dominates all other nodes , and axiom ( 10 ) requires that for each pair of nodes either one precedes the other or one dominates the other.</definiens>
				<definiens id="1">( y ) + D ( x , y )</definiens>
				<definiens id="2">( w , y ) AD ( x , z ) ~ y &lt; z ( single root condition ) ( 9 ) ( exclusivity ) ( 10 ) ( nontangling condition ) ( 11 ) Finally , the following axioms ( implicit in the standard treatments cited above ) require the precedence and dominance relations to range over tree nodes. vx Vy x &lt; y + X ( x ) A X ( y ) ( 12a ) Vx Vy D ( x , y ) -~</definiens>
				<definiens id="3">concludes the specification of linear precedence and dominance relations over nodes. We now turn to the specification of other relations in terms of these. The proper dominance relation P can be defined in terms of dominance as follows. Vx Vy p ( x , y ) ~ x # y A D ( x , y ) . ( 13 ) However , many interesting linguistic relations can not be defined by Sch6nfinkel-Bernays ' axioms. For example , the c-commands relation C is defined by the following formula ( which says that x c-commands y iff x does not dominate y , and every node z that properly dominates x also properly dominates y ) . Vx vy C ( x , y ) ~ ~D ( x , y ) A Vz e ( z , x ) ~ P ( z , y ) . ( 14 ) It is easy to see that this definition is not equivalent to a Sch6nfinkel-Bernays ' formula by expanding the equivalence into two implications and moving the embedded quantifier out. Vx Vy Vz C ( x , y ) + ( -~D ( x , y ) A ( P ( z , x ) ~ P ( z , y ) ) ) . ( 14a ) Vx Vy 3z ( ~D ( x , y ) A P ( z , x ) ~ P ( z , y ) ) ~ C ( x , y ) . ( 14b ) Formula ( 14b ) is not in SB because it contains an existential quantifier inside the scope of a universal quantifier. There are a number of ways to respond to this problem. First , we can abandon the attempt to work within the Sch6nfinkel-Bernays ' class , and work with some other language. Rounds ( 1988 ) describes such a language called LFP , whose decidability follows from the fact that the domain of quantification is Computational Linguistics Volume 20 , Number 1 restricted ( just as in SB ) . However , it seems to be difficult to devise a decidable system capable of simultaneously expressing both tree structure and the variety of feature structure constraints that the SB approach described here can. Blackburn , Gardent , and Meyer-viol ( 1993 ) introduce a modal language L T for describing trees decorated with feature structures , whose satisfiability problem is undecidable. In the long run , such specialized `` designer logics '' may provide the most satisfying integration of tree structure and feature structure constraints. Second , the 'one-sided ' approximation ( 14a ) can be used in place of the correct axiom ( 14 ) . The effect of using such one-sided approximations was investigated in Johnson ( 1991a ) . It was shown there that if ~ is a formula such as the one in ( 14 ) and ~/is the one-sided approximation ( 14a ) , then for any formula ~+ ( C ) in which C only appears positively , ~ A ~+ ( C ) is satisfiable iff ~ ' A ~+ ( C ) is satisfiable. That is , if we are concerned only with positively occurring constraints , we can simplify ( 14 ) to ( 14a ) , i.e. , ignore ( 14b ) , without affecting constraint satisfiability. Third , we can regard formulae such as ( 14 ) as the `` macro '' ( 15 ) , used to expand constraints at the interface between the syntactic rules and the constraint solver. This `` macro expansion '' rewrites c-commands constraints into boolean combinations of constraints that the constraint solver can handle. C ( x , y ) ~ - @ ( x~y ) A vz p ( z~x ) ~ P ( z~y )</definiens>
				<definiens id="4">x~y ) v 3z ( P ( z~x ) A -~P ( z~y ) ) . ( 15b ) The second and the third approaches differ in important ways. In the second approach , c-commands is a relation that is `` understood '' by the constraint solver ( albeit only in its one-sided form ) , so it can be used to define other relations. In the third approach , c-commands constraints are not primitive constraints , so relations defined in terms of c-commands must also be expressible in terms of `` macro expansion. '' In the second approach , constraints are quantifier-free formulae ( quantifiers appear only in the axioms ) , so the satisfiability problem is in NP. But in the third approach , macro expansion produces formulae that contain additional quantifiers , so the satisfiability problem may be PSPACE-complete. Of course , SB is not as expressive as full first-order logic. It is incapable of expressing functional relationships , since these require an existential quantifier inside the scope of a universal quantifier. This means , among other things , that it is impossible to state a constraint in SB requiring that a certain node must exist ( as was noted in the discussion of c-command in the previous section ) or that all nodes possess certain attributes. Thus for example , the following constraint , which requires that every tensed entity possess number and person attributes , is a first-order formula that is not in SB , since it requires a functional relationship between entities with tense attributes and the values of their number and person attributes. Vx Vy arc ( x~ tensG y ) ~ ( 3z arc ( x~ number~ z ) ) A ( 3z arc ( x~ person~ z ) ) ( 16 ) Similarly , a number of other extensions to the basic attribute-value framework discussed in the literature can not be formalized in SB. Subsumption constraints , used in the treatment of ( natural language ) conjunction , are not expressible as SB formulae because the satisfiability problem for conjunctions of subsumption and attribute-value constraints is undecidable ( D6rre and Rounds 1992 ) . Positively occurring functional 10 Mark Johnson Computing with Features as Formulae ( El ) Vxx=x. ( E2 ) VxVyx=y~y=x. ( E3 ) Vx0 ... VXn xj = x 0 A P ( Xl~ ... ~ xj~ ... ~ xn ) ~ P ( Xl~ ... ~ Xo~ ... ~ Xn ) for j = 1~ ... ~ n , for every predicate symbol P appearing in ~. ( E4 ) VXo ... VXn xj =x0 -- ~ f ( xl~ ... ~Xj~ ... ~Xn ) =f ( Xl~ ... ~Xo~ ... ~Xn ) for j = 1~ ... ~ n , for every function symbol f appearing in ~. Figure 2 Equality axiom schemata for a first-order formula ~. uncertainty constraints , used in the LFG treatment of long-distance dependencies ( Kaplan and Zaenen 1989 ) appear to have a decidable satisfiability problem ( Kaplan and Maxwell 1988a ) , but the satisfiability problem for arbitrary boolean combinations of functional uncertainty constraints is undecidable ( Keller 1991 ) , so these can not be expressed using SB formulae either ( since the quantifier-free subclass of SB is closed under boolean operations ) . In this paper the intended interpretation of the equality relation is identity ; i.e. , a = b if and only if a and b denote the same individual. However , for some purposes ( e.g. , in the least-fixed-point characterization of minimal models given below ) this `` special '' interpretation of the equality complicates matters , and it is more convenient to treat the equality relation as a `` normal '' relation that is defined by a set of axioms E. The idea is that E has the property that a formula ~ is satisfiable under the identity interpretation of equality if and only if { 9~ } U E is satisfiable in an interpretation in which equality is not given any special treatment. In effect , the axioms E require that the equality relation denotes an equivalence relation , and permit the substitution of equals for equals. Together these imply that no predicate can distinguish equal individuals. This means that in terms of satisfiability and the consequence relation , exactly the same results are obtained irrespective of whether equality is treated as identity or defined by the axioms E. Such treatments of equality in first-order logic are well known and described in standard texts. For example , Chang and Lee ( 1973 ) give the axiom schemata in Figure 2 , which generates syntactic equality axioms E for a first-order formula 9~ , and prove that E has the properties just described , s What is important here is that for an SB formula ~ the instances of the axiom schemata are all SB formulae , and there are only finitely many instances of these schemata. This means that for an arbitrary SB formula ~ there is another SB formula ~ such that ~ is satisfiable with respect to an identity interpretation of equality if and only if A ~ is satisfiable with respect to an interpretation in which equality is treated like any other relation. Thus a method for determining the satisfiability of SB formulae without equality can be used to determine satisfiability of SB formulae in which equality is interpreted as identity. symbols. 11 Computational Linguistics Volume 20 , Number 1 ( 17 ) Vx x = x. ( 18 ) Vx Vy x = y ~ y = x. ( 19 ) Vx Va Vy VXl x -~ Xl /X arc ( x , a , y ) ~ arc ( x1 , a , y ) . ( 20 ) Vx Va Vy Val a = al A arc ( x , a , y ) ~ arc ( x , al , y ) . ( 21 ) Vx Va Vy Vyl y = yl Aarc ( x , a , y ) ~ arc ( x , a , yl ) . ( 22 ) VCl Vc2 c = Cl A con ( c ) ~ con ( c1 ) . ( 23 ) Vx Vy x = y A 3rd-sg ( x ) ~ 3rd-sg ( y ) . Figure 3 The equality axioms for arc , con , and 3rd-sg predicates. For example , consider the SB formulae in ( 1-4 ) and ( 5-16 ) . These contain the three-place relation symbol arc and the one-place relation symbols con and 3rd-sg. The equality axioms obtained from schemata ( El-E4 ) for any system of constraints that mention just these relations are given in Figure 3. It is technically easier to work with a syntactically restricted class of SB formulae where the body of each formula has a particular syntactic form known as clausal form or Skolem standard form. Definition A clause is a formula of the form , -~ o~1 V • • • V ~- , c~m V fll V • • • V ft , , where each oq and fly is an atomic formula ( i.e. , is of the form p ( h , ... , tn ) ) , and m , n &gt; 0</definiens>
			</definition>
			<definition id="9">
				<sentence>As noted above , the equality axioms ensure that the relation that the equality symbol denotes is an equivalence relation and the substitutivity of equals for equals .</sentence>
				<definiendum id="0">equality axioms</definiendum>
				<definiens id="0">an equivalence relation and the substitutivity of equals for equals</definiens>
			</definition>
			<definition id="10">
				<sentence>V fl~ is a ground instance of a positive clause in S } , until &amp; A : = 0 do A : =AU &amp; A , ' V .</sentence>
				<definiendum id="0">V fl~</definiendum>
				<definiens id="0">a ground instance of a positive clause in S } , until &amp; A : = 0 do A : =AU &amp; A , ' V</definiens>
			</definition>
			<definition id="11">
				<sentence>V fl~ is a ground instance , ... , ~ is in &amp; A of an implication in S such that { oL~ Oq'n } C A , at least one of the o~ i and no flj~ is in A } , return A. Figure 4 The semi-naive algorithm for computing A. The find operation dereferences its argument , i.e. , it follows these pointers until it reaches a constant with a null pointer , which is the equivalence classes ' representative .</sentence>
				<definiendum id="0">V fl~</definiendum>
				<definiens id="0">a ground instance , ...</definiens>
			</definition>
			<definition id="12">
				<sentence>15 Computational Linguistics Volume 20 , Number 1 s I 3rd-sg ( u ) , arc ( u , number , v ) , v # sg , Vx Va Vy Vz arc ( x , a , y ) A arc ( x , a , z ) ~ y = z , Vx Va Vy , -~ con ( x ) V ~ arc ( x , a , y ) , con ( sg ) , con ( pl ) , con ( 3rd ) , sg # pl , sg # 3rd , pl # 3rd , Vx 3rd-sg ( x ) ~ arc ( x , person , 3rd ) , Vx 3rd-sg ( x ) ~ arc ( x , number , sg ) constraints AV axioms sort defn .</sentence>
				<definiendum id="0">arc</definiendum>
				<definiendum id="1">-~ con</definiendum>
				<definiendum id="2">con</definiendum>
				<definiendum id="3">con</definiendum>
				<definiens id="0">u ) , arc ( u , number , v ) , v # sg</definiens>
			</definition>
			<definition id="13">
				<sentence>Definition A choice function for a clause c = ( al A ... A am ~ fa V ... V ft , ) is any function in ( Vc -+ U ) ~ \ [ 1 , ... , n\ ] , where Vc is the set of variables in the clause c. That is , a choice function for a clause is a function from variable assignments to an integer representing one of the clause 's consequents .</sentence>
				<definiendum id="0">Vc</definiendum>
				<definiens id="0">al A ... A am ~ fa V ... V ft , ) is any function in ( Vc -+ U</definiens>
			</definition>
			<definition id="14">
				<sentence>Feature structures : A logical theory with application to language analysis .</sentence>
				<definiendum id="0">Feature structures</definiendum>
				<definiens id="0">A logical theory with application to language analysis</definiens>
			</definition>
			<definition id="15">
				<sentence>II : The New Technologies .</sentence>
				<definiendum id="0">II</definiendum>
				<definiens id="0">The New Technologies</definiens>
			</definition>
</paper>

		<paper id="3000">
			<definition id="0">
				<sentence>Introduction to Computational Phonology Articles Regular Models of Phonological Rule Systems Commentary on Kaplan and Kay Commentary on Kaplan and Kay The Reconstruction Engine : A Computer Implementation of the Comparative Method Commentary on Lowe and Mazaudon Commentary on Lowe and Mazaudon The Acquisition of Stress : A Data-Oriented Approach Commentary on Daelemans , GiUis , and Durieux Commentary on Daelemans , Gillis , and Durieux Phonological Analysis in Typed Feature Systems Commentary on Bird and Klein Commentary on Bird and Klein Book Reviews English Verb Classes and Alternations : A Preliminary~ Investigation Beth Levin Statistically-Driven Computer Grammars of English : The IBM/Lancaster Approach Ezra Black , Roger Garside , and Geoffrey Leech ( editors ) Intelligent Multimedia Interfaces Mark T. Maybury ( editor ) ~utational Phonology Steven Bird iii Ronald M. Kaplan and Martin Kay 331 Mark Liberman 379 Graeme Ritchie 380 John B. Lowe and Martine Mazaudon 381 Steven Lee Hartman 418 John Hewson 419 Walter Daelemans , Steven Gillis , and 421 Gert Durieux Prahlad Gupta 452 Jonathan Kaye 453 Steven Bird and Ewan Klein 455 John Coleman 492 Richard Sproat 493 Reviewed by Ha~ : old Somers Reviewed by Dekai Wu Reviewed by Kent Wittenburg 495 498 501 Published Quarterly by the MIT Press for the Association for Computational Linguistics .</sentence>
				<definiendum id="0">Preliminary~ Investigation Beth Levin Statistically-Driven</definiendum>
				<definiens id="0">A Computer Implementation of the Comparative Method Commentary on Lowe and Mazaudon Commentary on Lowe and Mazaudon The Acquisition of Stress : A Data-Oriented Approach Commentary on Daelemans , GiUis , and Durieux Commentary on Daelemans , Gillis</definiens>
			</definition>
			<definition id="1">
				<sentence>Phonology is the study of the systems of sounds that are manifested by natural languages , the significant contrasts between sounds that are relevant to meaning .</sentence>
				<definiendum id="0">Phonology</definiendum>
				<definiens id="0">the study of the systems of sounds that are manifested by natural languages , the significant contrasts between sounds that are relevant to meaning</definiens>
			</definition>
			<definition id="2">
				<sentence>Daelemans , Gillis , and Durieux : The Acquisition of Stress : A Data-Oriented Approach .</sentence>
				<definiendum id="0">Durieux</definiendum>
			</definition>
			<definition id="3">
				<sentence>`` Lexical phonology : A computational system . ''</sentence>
				<definiendum id="0">Lexical phonology</definiendum>
			</definition>
</paper>

		<paper id="4004">
			<definition id="0">
				<sentence>The distinction between these two 598 Bonnie J. Dorr Machine Translation Divergences divergence type is structural : in ( 4 ) , the verbal object is realized as a noun phrase ( the house ) in English and as a prepositional phrase ( en la casa ) in Spanish .</sentence>
				<definiendum id="0">noun phrase</definiendum>
				<definiens id="0">the house ) in English and as a prepositional phrase ( en la casa ) in Spanish</definiens>
			</definition>
			<definition id="1">
				<sentence>Conflation is the incorporation of necessary participants ( or arguments ) of a given action .</sentence>
				<definiendum id="0">Conflation</definiendum>
				<definiens id="0">the incorporation of necessary participants ( or arguments ) of a given action</definiens>
			</definition>
			<definition id="2">
				<sentence>In particular , Barnett et al. ( 1991a ) divide distinctions between the source language and the target language into two categories : translation divergences , in which the same information is conveyed in the source and target texts , but the structures of the sentences are different ( as in previous work by Dorr \ [ 1990a , 1990b\ ] ) ; and translation mismatches , in which the information that is conveyed is different in the source and target languages ( as described by Kameyama et al. \ [ 1991\ ] ) .</sentence>
				<definiendum id="0">translation mismatches</definiendum>
				<definiens id="0">divide distinctions between the source language and the target language into two categories : translation divergences , in which the same information is conveyed in the source and target texts , but the structures of the sentences are different</definiens>
			</definition>
			<definition id="3">
				<sentence>Definition 1 A lexical conceptual structure ( LCS ) is a modified version of the representation proposed by Jackendoff ( 1983 , 1990 ) that conforms to the following structural form : \ [ T ( X ' ) X ' ( \ [ T ( W ' ) Wt\ ] , \ [ T ( Zq ) Ztl\ ] `` ' '' \ [ T ( Z ' , , ) Ztn\ ] \ [ T ( Q ' , ) Q'I\ ] - '' \ [ T ( Q ' , , , ) Q'm\ ] ) \ ] This corresponds to the tree-like representation shown in Figure 2 , in which ( 1 ) X ' is the logical head ; ( 2 ) W ' is the logical subject ; ( 3 ) Z~ ... Z~ are the logical arguments ; and ( 4 ) Q~ ... Q~m are the logical modifiers .</sentence>
				<definiendum id="0">LCS</definiendum>
				<definiens id="0">a modified version of the representation proposed</definiens>
			</definition>
			<definition id="4">
				<sentence>The former is identified as a root LCS ( RLCS ) and the latter is identified as a composed LCS ( CLCS ) : Definition 2 A RLCS ( i.e. , a root LCS ) is an uninstantiated LCS that is associated with a word definition in the lexicon ( i.e. , a LCS with unfilled variable positions ) .</sentence>
				<definiendum id="0">RLCS</definiendum>
				<definiendum id="1">RLCS</definiendum>
				<definiens id="0">a root LCS ) is an uninstantiated LCS that is associated with a word definition in the lexicon ( i.e. , a LCS with unfilled variable positions</definiens>
			</definition>
			<definition id="5">
				<sentence>Definition 3 A CLCS ( i.e. , a composed LCS ) is an instantiated LCS that is the result of combining two or more RLCSs by means of unification ( roughly ) .</sentence>
				<definiendum id="0">CLCS</definiendum>
				<definiendum id="1">LCS</definiendum>
				<definiendum id="2">unification</definiendum>
				<definiens id="0">a composed LCS ) is an instantiated</definiens>
			</definition>
			<definition id="6">
				<sentence>The CLCS is a structure that results from combining the lexical items of a source-language sentence into a single underlying pivot form by means of LCS composition .</sentence>
				<definiendum id="0">CLCS</definiendum>
				<definiens id="0">a structure that results from combining the lexical items of a source-language sentence into a single underlying pivot form by means of LCS composition</definiens>
			</definition>
			<definition id="7">
				<sentence>Definition 4 A syntactic phrase is a maximal projection that conforms to the following structural form : 602 Bonnie J. Dorr Machine Translation Divergences Y-MAX Q-MAXj+I ... Q-MAXk Y-MAX Q-MAXk+ I ... Q.MAX m W-MAX X-MAX X Z-MAX1 ... Z-MAX a Q-MAX 1 ... Q-MAXi X Q-MAXi+ 1 ... Q-MAXj Figure 5 Formal definition of syntactic phrase .</sentence>
				<definiendum id="0">syntactic phrase</definiendum>
				<definiens id="0">a maximal projection that conforms to the following structural form : 602 Bonnie J. Dorr Machine Translation Divergences Y-MAX Q-MAXj+I</definiens>
			</definition>
			<definition id="8">
				<sentence>\ [ Y-MAX Q-MAXj+~ ... Q-MAXk \ [ Y-MAX W-MAX \ [ X-MAX \ [ X Q-MAX1 ... Q-MAXi X Q-MAXi+I ... Q-MAX i\ ] Z-MAX1 ... Z-MAXn\ ] \ ] Q-MAXk+1 ... Q-MAXm\ ] 6 This corresponds to the tree-like representation shown in Figure 5 , in which ( 1 ) X is the syntactic head ( of category V , N , A , P , I , or C ) ; ( 2 ) W-MAX is the external argument ; ( 3 ) Z-MAX1 , ... , Z-MAXn are the internal arguments ; and ( 4 ) Q-MAX1 ... .. Q-MAXm are the syntactic adjuncts .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">W-MAX</definiendum>
				<definiens id="0">the syntactic head ( of category V , N , A , P , I , or C</definiens>
				<definiens id="1">the external argument ; ( 3 ) Z-MAX1 , ...</definiens>
			</definition>
			<definition id="9">
				<sentence>This argument reversal is resolved by means of the : INT and : EXT parameters , which force the ~£T4 mapping to be overridden with respect to the positioning of the logical subject and logical argument in Spanish .</sentence>
				<definiendum id="0">EXT parameters</definiendum>
				<definiens id="0">force the ~£T4 mapping to be overridden with respect to the positioning of the logical subject and logical argument in Spanish</definiens>
			</definition>
			<definition id="10">
				<sentence>In particular , these approaches address the problem of thematic divergence by means of transfer rules of the following form , respectively : ( 39 ) ( 40 ) ( 41 ) like ( SUBJ ( ARG2 : GN ) , OBJI ( ARGI : GN ) ) plaire ( SUBJ ( ARGI : GN ) , OBJI ( ARG2 : PREP , GN ) ) 4~ gverb ( like ( dat : , , nom : X ) , ge+f all , • : X ) like V ~ gustar V NP ( \ [ ROLE SUBJD ~ NP ( \ [ ROLE IOBJ\ ] ) NP ( \ [ ROLE DOBJ\ ] ) ~ NP ( \ [ ROLE SUBJ\ ] ) One problem with these approaches is that surface syntactic decisions are , in a sense , performed off-line by means of lexical entries and transfer rules that specifically encode language-specific syntactic information .</sentence>
				<definiendum id="0">SUBJ</definiendum>
				<definiendum id="1">nom</definiendum>
				<definiens id="0">lexical entries and transfer rules that specifically encode language-specific syntactic information</definiens>
			</definition>
			<definition id="11">
				<sentence>The RLCSs for the Spanish case conform to the following formal specifications : ( 56 ) gustar : \ [ T ( X ' ) X ' ( \ [ T ( W ' ) : INT W'\ ] , \ [ T ( Z ' ) : EXT Z'\ ] ) \ ] soler : IT ( Q , ) : PROMOTE Q'\ ] When the ~£T4 is applied to the CLCS of ( 51 ) , the promotional override ( 52 ) is immediately triggered by the : PROMOTE marker in the RLCS for soler ; this invocation crucially precedes the invocation of the thematic override ( 53 ) .</sentence>
				<definiendum id="0">)</definiendum>
				<definiens id="0">EXT Z'\ ] ) \ ] soler : IT ( Q ,</definiens>
			</definition>
			<definition id="12">
				<sentence>The details of the matching process that achieves lexical selection of the target-language RLCSs ( e.g. , the selection of the RLCSs for like \ [ or gustar\ ] from the underlying CLCS shown in Definition 1 ) have not been presented here , but see Dorr ( 1993a ) for a discussion with examples .</sentence>
				<definiendum id="0">RLCSs</definiendum>
				<definiens id="0">achieves lexical selection of the target-language</definiens>
			</definition>
</paper>

		<paper id="3011">
</paper>

		<paper id="3003">
</paper>

		<paper id="3004">
			<definition id="0">
				<sentence>The essential step in historical reconstruction is the arrangement of related words in different languages into sets of cognates and the specification of the regular phonological correspondences that support that arrangement ; the well-known means for carrying out this arrangement and specification is the comparative method ( see , for example , Meillet 1966 ; Hoenigswald 1950 , 1960 ; Watkins 1989 ; Baldi 1990 ) .</sentence>
				<definiendum id="0">specification</definiendum>
				<definiens id="0">the arrangement of related words in different languages into sets of cognates and the specification of the regular phonological correspondences that support that arrangement</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , English father , German Vater , Greek pater , and Sanskrit pitrare all reflexes of a Proto-Indo-European ( PIE ) etymon reconstructed as something like *poter ( the asterisk indicates that this word is a reconstruction and not an attested form ) .</sentence>
				<definiendum id="0">*poter</definiendum>
				<definiens id="0">a reconstruction and not an attested form )</definiens>
			</definition>
			<definition id="2">
				<sentence>RE implements ( i ) a set of algorithms that generate possible reconstructions given word forms in modern languages ( and vice versa as well ) and ( ii ) a set of algorithms that arrange input modern forms into possible cognate sets based on those reconstructions .</sentence>
				<definiendum id="0">RE implements</definiendum>
				<definiens id="0">a set of algorithms that generate possible reconstructions given word forms in modern languages</definiens>
			</definition>
			<definition id="3">
				<sentence>The core functions of RE compute all possible ancestor forms ( using a Table of Correspondences and a phonotactic description , a Syllable Canon , both described in Section 3.1 ) and makes sets of those modern forms that share the same reconstructions .</sentence>
				<definiendum id="0">Syllable Canon</definiendum>
				<definiens id="0">makes sets of those modern forms that share the same reconstructions</definiens>
			</definition>
			<definition id="4">
				<sentence>To the second class belong programs that model sound change as sets of rules applied to derive later forms from earlier forms , and RE is a member of this class .</sentence>
				<definiendum id="0">RE</definiendum>
				<definiens id="0">a member of this class</definiens>
			</definition>
			<definition id="5">
				<sentence>DOC is one of the earliest projects to attempt a comprehensive treatment of the lexicons of a group of related languages .</sentence>
				<definiendum id="0">DOC</definiendum>
				<definiens id="0">one of the earliest projects to attempt a comprehensive treatment of the lexicons of a group of related languages</definiens>
			</definition>
			<definition id="6">
				<sentence>PHONO ( Hartman 1981 , 1993 ) is a DOS program that applies ordered sets of phonological rules to input forms .</sentence>
				<definiendum id="0">PHONO</definiendum>
			</definition>
			<definition id="7">
				<sentence>Unlike RE , which handles only one step ( at a time ) in the development of multiple languages , PHONO traces the history of the words of a single language through multiple stages .</sentence>
				<definiendum id="0">PHONO</definiendum>
				<definiens id="0">traces the history of the words of a single language through multiple stages</definiens>
			</definition>
			<definition id="8">
				<sentence>( 3 ) ( 4 ) Segmentations whose elements are ALL constituents of the table : ( a ) Segmentation : Tokenized form : ( b ) Segmentation : Tokenized form : A k r a ( \ ] ,3 ) ( 93,94,181 ) ( 142,169 ) ( 31,186 ) A kr a ( 1,3 ) ( 102,103,104,105,106 ) ( 31,186 ) Segmentations that contain elements NOT found in the table : Segmentation Tokenized form ( c ) Akr-a ( ? )</sentence>
				<definiendum id="0">Segmentation Tokenized form</definiendum>
				<definiens id="0">Segmentations whose elements are ALL constituents of the table : ( a ) Segmentation : Tokenized form : ( b ) Segmentation : Tokenized form : A k r a ( \ ] ,3</definiens>
			</definition>
			<definition id="9">
				<sentence>In other cases , forms collected : in the check files may indicate a mistake in the Table of Correspondences , which needs to be corrected to allow the word to reconstruct successfully .</sentence>
				<definiendum id="0">Correspondences</definiendum>
				<definiens id="0">needs to be corrected to allow the word to reconstruct successfully</definiens>
			</definition>
			<definition id="10">
				<sentence>From this sorted list , RE extracts matching reconstructions , with their supporting forms , and proposes them as potential cognate sets ( Figure 18 ) .</sentence>
				<definiendum id="0">RE</definiendum>
			</definition>
			<definition id="11">
				<sentence>Rewriting each side of the rule in terms of features , we get : ( 14 ) *kl = MANNER ( Cluster ) &gt; *kj = MODE ( Unvoiced ) PLACE ( Velar ) PLACE ( Lateral ) MANNER ( Cluster ) MODE ( Unvoiced ) PLACE ( Velar ) PLACE ( Palatal ) 414 Lowe and Mazaudon The Reconstruction Engine ris Cluster ris Stop ris Aspirated ris Unaspirated ris Velar = hl , hr , k j , kr , khr , kl , kw , p j , phj , pr , pi , ~ j , mj , ml , mr = k , kh , ts , tsh , ~ , ~h , t , thp , ph = kh , ~h t sh , th , ph , khr , phj = k , ~ , t s , t , p , k j , k r , k l , kw , p j , pl = k , kh , r3 , k j , kr , khr , kl , kw Figure 32a `` Feature set '' for Risiangku ( excerpt ) .</sentence>
				<definiendum id="0">mj</definiendum>
				<definiens id="0">Cluster ris Stop ris Aspirated ris Unaspirated ris Velar = hl , hr , k j , kr , khr , kl , kw , p j , phj , pr</definiens>
				<definiens id="1">~h t sh , th , ph , khr , phj = k , ~ , t s , t , p , k j , k r , k l , kw , p j</definiens>
			</definition>
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>view that inflection sponsors all of the empty functional nodes of the clause in which it appears .</sentence>
				<definiendum id="0">inflection</definiendum>
				<definiens id="0">sponsors all of the empty functional nodes of the clause in which it appears</definiens>
			</definition>
</paper>

		<paper id="3009">
</paper>

		<paper id="2002">
			<definition id="0">
				<sentence>A tree-adjoining grammar is a tree rewriting system denoted by a tuple G = ( VN , VT , S , I , A ) , where VN and VT are finite , disjoint sets of nonterminal and terminal symbols respectively , S E VN is a distinguished symbol , and I and A are finite sets of elementary trees .</sentence>
				<definiendum id="0">tree-adjoining grammar</definiendum>
				<definiendum id="1">S E VN</definiendum>
				<definiens id="0">finite , disjoint sets of nonterminal and terminal symbols respectively ,</definiens>
			</definition>
			<definition id="1">
				<sentence>Let w =dld2 '' '' dn , n &gt; 0 , be a string over some alphabet ; symbol pWq denotes the substring dpdp+l ... dq for 1 _ &lt; p &lt; q &lt; n and is undefined otherwise .</sentence>
				<definiendum id="0">symbol pWq</definiendum>
				<definiens id="0">a string over some alphabet ;</definiens>
			</definition>
			<definition id="2">
				<sentence>In practice , most commonly used algorithms solve the parsing problem for TAGs by computing a superset of a parse relation ( defined as above ) and by representing it in such a way that its instances can be tested in constant time ; such a condition is satisfied by the methods reported in Vijay-Shanker and Joshi ( 1985 ) , Schabes and Joshi ( 1988 ) , Palis , Schende , and Wei ( 1990 ) , Schabes ( 1991 ) , Lavelli and Satta ( 1991 ) , Lang ( 1992 ) , and Vijay-Shanker and Weir ( 1993 ) .</sentence>
				<definiendum id="0">Wei</definiendum>
				<definiens id="0">a condition is satisfied by the methods reported in Vijay-Shanker and Joshi ( 1985 ) , Schabes and Joshi ( 1988 ) , Palis , Schende , and</definiens>
			</definition>
			<definition id="3">
				<sentence>In the following we will refer to sets of terminal symbols VT ( `` ) = { dp I ' 1Gp_ &lt; 6 ( n+1 ) } , and to sets of nonterminal symbols V ( N n ) = { ( A , u , v ) , ( B , u , v ) , ( C , u , v ) \ ] l &lt; u &lt; v &lt; n4 } U { C , D , S } . Based on these sets , Figure 5 defines families of auxiliary trees F~ n ) , 1 ( h _ 6. For example , an auxiliary tree 7 ( P , q , r , s , u , v ) C 1~ `` ) will be specified by providing actual values for the integer parameters p , q , r , s , u , and v , consistently with the definitions of sets V ( T n ) and V ( N n ) . In the following we will also use the initial tree % depicted in Figure 6. The next definition introduces map ~ , which is the core component of the proposed reduction. The definition is rather technical : it will be followed by a more intuitive example. Definition 3 Let ( A , B ) be an instance of BMM , m the order of matrices A and B. Let also n = Lm~/+1 and ~r -n + 1. A map ~c is specified in such a way that YZ ( ( A , B ) ) = ( G , w ) , where G = ( w ( n ) ~ V ( Tn ) ~ S~ I1 a ( n ) ) and w = dld2 ... d6~. Set I contains the only initial tree % ; set 181 Computational Linguistics Volume 20 , Number 2 Ys : S I D I C I D I E Figure 6 Definition of initial tree % . A ( n ) contains all and only the following elementary trees ( f ( h n ) =fh , 1 &lt; h &lt; 3 ) : ( i ) for every aq = 1 in A , the auxiliary tree 7 ( p , q , r , s , u , v ) E P~ n ) belongs to A ( n ) , where ; -- -f3 ( / ) , q -- -cr +f3 ( J ) , r = 4or +f2 ( j ) + 1 , s = 5cr +f2 ( i ) , U= A ( i ) , V= A ( J ) ; ( ii ) for every bq -1 in B , the auxiliary tree 7 ( p , q , y , s , u , v ) E F~ n ) belongs to A ( n ) , where p= a+f3 ( i ) +l , q= 2cr+f3 ( j ) , r = 3a+f2 ( j ) , s = 4or +f2 ( i ) , u= A ( i ) , v= A ( J ) ; ( iii ) for every pair ( A , u , t ) , ( B , t , v ) E V ( N n ) , the auxiliary trees 7 ( u , t , v ) E C~ `` ) , 7 ( u , v ) E r~ `` ) belong to A ( nJ ; ( iv ) for every 1 &lt; p &lt; 6a , the auxiliary trees n ) , 7 ( p ) E r ( n ) 6 belong to A ( `` ) . In order to have a better understanding of map 9 r and of the idea underlying grammar G and string w , we discuss in the following a simple example , adding more details to the informal discussion presented in Section 3.1. Let us define a Boolean matrix by specifying only its non-null elements. Assume then that an input instance ( A , B / of the BMM problem consists of two matrices of 182 Giorgio Satta Tree-Adjoining Grammar Parsing a2,15 1 B : C : i b 153 I multiplication c23 mapping YI : Y3 : &lt; C , l , l &gt; &lt; A,1,2 &gt; \ [ d 2 ~d20+1 &lt; ,4,1,2 &gt; 1 d d 4+3 &lt; A,1,2 &gt; 16+2+1 &lt; B,2,1 &gt; I &lt; C,1,1 &gt; Y2 : &lt; 8,2,1 &gt; d4+3+1 '' ~/~ d16+ 2 ds+t &lt; B2 , I &gt; d12+3 Y4 '' C I &lt; C , I , I &gt; I c C I &lt; C , I , I &gt; d 2 d8+ 1 C d12+3 d20+ 1 ( a ) ( b ) Figure 7 Part ( a ) shows how non-null elements a2,15 and b15,7 in matrices A and B combine together , forcing element c2,7 in matrix C to value 1 ; each array element is represented as an arc in a directed graph .</sentence>
				<definiendum id="0">auxiliary tree 7 ( P</definiendum>
				<definiendum id="1">V</definiendum>
				<definiendum id="2">B</definiendum>
				<definiens id="0">sets of terminal symbols VT ( `` ) = { dp I ' 1Gp_ &lt; 6 ( n+1 ) } , and to sets of nonterminal symbols V ( N n ) = { ( A , u , v ) , ( B , u , v ) , ( C , u , v</definiens>
				<definiens id="1">s , u , v ) C 1~ `` ) will be specified by providing actual values for the integer parameters p , q , r , s , u , and v , consistently with the definitions of sets V ( T n ) and</definiens>
				<definiens id="2">an instance of BMM , m the order of matrices A and B. Let also n = Lm~/+1 and ~r -n + 1. A map ~c is specified in such a way that YZ ( ( A , B ) ) = ( G , w ) , where G = ( w ( n ) ~ V</definiens>
				<definiens id="3">p , q , r , s , u , v ) E P~ n ) belongs to A ( n ) , where</definiens>
				<definiens id="4">p , q , y , s , u , v ) E F~ n ) belongs to A ( n ) , where p= a+f3 ( i ) +l , q= 2cr+f3 ( j ) , r = 3a+f2 ( j ) , s = 4or +f2</definiens>
				<definiens id="5">A , u , t ) , ( B , t , v ) E V ( N n ) , the auxiliary trees 7 ( u , t , v ) E C~</definiens>
				<definiens id="6">a ) shows how non-null elements a2,15 and b15,7 in matrices A and B combine together , forcing element c2,7 in matrix C to value 1 ; each array element is represented as an arc in a directed graph</definiens>
			</definition>
			<definition id="4">
				<sentence>A map G is specified in such a way that G ( Rp ) = C , C a Boolean matrix of order m , and element cij is non-null if and only if Rp ( 'y , p , q , r , s ) holds for an auxiliary tree ~/ ( u , v ) E E~ n ) , where ( f ( h n ) =fh , 1 &lt; h &lt; 3 ) p = f3 ( i ) , q = 2¢+f3 ( j ) , r = 3 r+f2 ( j ) , s = 5cr+f2 ( i ) , u = fl ( i ) , v = A ( J ) .</sentence>
				<definiendum id="0">element cij</definiendum>
				<definiens id="0">specified in such a way that G ( Rp ) = C</definiens>
			</definition>
			<definition id="5">
				<sentence>In the above definition , functionf ( n ) is used to retrieve the indices of non-null elements 184 Giorgio Satta Tree-Adjoining Grammar Parsing il , kl kl , Yl ) 2 i i i i .</sentence>
				<definiendum id="0">functionf</definiendum>
				<definiens id="0">used to retrieve the indices of non-null elements 184 Giorgio Satta Tree-Adjoining Grammar Parsing il , kl kl</definiens>
			</definition>
			<definition id="6">
				<sentence>Equivalently , there exists at least one derivation from '/4 of strings ( x , y ) = &lt; f3 ( i ) w2o-+f3 ( j ) ~3o.+f2~ ) w5o.+f2 ( i ) ) ( 2 ) that participates in a sentential derivation of w. Fix such a derivation. We first observe that , in order to derive any terminal symbol from '/4 , auxiliary trees in F~ n ) , F~ n ) and F~ n ) must be used. Any tree in F~ n ) can only derive symbols in slices w ( h ) , h E { 1,2 , 5 , 6 } , and any tree in F~ n ) can only derive symbols in slices w ( h ) , h E { 2 , 3 , 4 , 5 } . Therefore at least one tree in F~ n ) and at least one tree in F~ n ) must be used in the derivation of ( x , y ) , since ( x , y ) includes terminal symbols from every slice of w. Furthermore , if more than one tree in F~ `` ) is used in a derivation in G , the resulting string can not match w. The same argument applies to trees in F~ n ) . We must then conclude that exactly one tree in F~ `` ) , one tree in F~ `` ) , and one tree in F~ n ) have been used in the derivation of ( x , y ) from 74. Call the above trees 71 = q ' ( p , k3 , k2+1 , s , u , kl ) E F~ n ) , `` /2 ~ -- '' 7 ( k~+1 , q , r , k~ , k~ , v ) E F~ n ) , and `` /3 = `` / ( u/ , t , V ' ) E F~ n ) . As a second step , we observe that 73 can be adjoined into 74 only if u ' =fl ( i ) and v ' = A q ) and 3'3 can host 71 and `` /2 just in case u ' = u , v ' = v , and k I = k~ = t. We also observe that , after these adjunctions take place , the leftmost terminal symbol in the yield of 3'4 will be the leftmost terminal symbol in the yield of 71 , that is dp. From relation ( 2 ) we then conclude that p =f3 ( i ) . Similarly , we can argue that q = 20 +f3 ( j ) , r = 3o +f2 ( j ) and s = 50 +f2 ( i ) . Finally , adjunction of '/1 and `` /2 into `` /3 can match w just in case k 3 = k~ and k2 = k~. From the relations inferred above , we conclude that we can rewrite 71 as 70c3 ( i ) , k3 , k2 + 1,5o +f2 ( i ) , fl ( i ) , kl ) C F~ n ) and '/2 as '/ ( k3 + 1 , 20 +f3 ( / ' ) , 3o +f2 ( j ) , k2 , kl , fl ( j ) ) E F~ n ) . Sincef is one-to-one and k2 , k3 c { 1..n } , there exists k such thatf ( k ) = ( kl , k2 , k3 ) . From steps ( i ) and ( ii ) in Definition 3 , we then have that aik and bkj are non-null and then cq = l. \ [ \ ] The results presented in the previous section are developed here under a computational perspective. Some interesting computational consequences will then be drawn for the tree-adjoining grammar parsing problem. The following analysis assumes the random-access machine as the model of computation. We show in the following how time upper bounds for the TGP problem can be transferred to time upper bounds for the BMM problem using the commutative diagram studied in the previous section. Let ( A , B ) be an instance of BMM and let ( G , w } = .T ( ( A , B ) ) ; m and n are specified as in Definition 3. Observe that , since n 6 &gt; m , functionf ( n ) maps set { 1 .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">in order to derive any terminal symbol from '/4 , auxiliary trees in F~ n ) , F~ n</definiens>
			</definition>
</paper>

		<paper id="4006">
			<definition id="0">
				<sentence>( DOWN T ) ) 22 ( ( NP14 NP ) ( DOWN ( 7 8 19 ) ) ) 21 ( ( $ 13 S ) ( DOWN ( 3 20 ) ) ) 20 ( ( VP12 VP ) ( DOWN ( 6 9 19 ) ( 6 22 ) ) ) 19 ( ( PP+11 PP+ NIL ) ( DOWN ( 18 ) ) ) 18 ( ( PP10 PP NIL ) ( DOWN ( 12 17 ) ) ) 17 ( ( NP9 NP ) ( DOWN ( 16 15 ) ) ) 16 ( ( POSS8 POSS ) ( DOWN ( 13 ) ) ) 15 ( ( POSS-NOM7 POSS-NOM ) ( DOWN ( 14 ) ) ) 14 ( ( BINOCULARS7 NOUN BINOCULARS ) ( DOWN T ) ) 13 ( ( HIS6 PRONOUN HIS ) ( DOWN T ) ) 12 ( ( WITH5 PREP WITH ) ( DOWN T ) ) 11 ( ( $ 6 S ) ( DOWN ( 3 10 ) ) ) 10 ( ( VP5 VP ) ( DOWN ( 6 9 ) ) ) 9 ( ( NP4 NP ) ( DOWN ( 7 8 ) ) ) 8 ( ( BOY4 NOUN BOY ) ( DOWN T ) ) 7 ( ( THE3 DET THE ) ( DOWN T ) ) 6 ( ( VERBS3 VERBS ) ( DOWN ( 5 ) ) ) 5 ( ( TENSED-MAIN2 TENSED-MAIN ) ( DOWN ( 4 ) ) ) 4 ( ( SAW2 VERB SEE ) ( DOWN T ) ) 3 ( ( NP1 NP ) ( DOWN ( 1 2 ) ) ) 2 ( ( MAN1 NOUN MAN ) ( DOWN T ) ) 1 ( ( EVERY0 DET EVERY ) ( DOWN T ) ) Figure 1 The shared-packed parse forest for Every man saw the boy with his binoculars .</sentence>
				<definiendum id="0">POSS8 POSS )</definiendum>
				<definiendum id="1">BOY4 NOUN BOY ) ( DOWN T )</definiendum>
				<definiens id="0">THE3 DET THE ) ( DOWN T ) ) 6 ( ( VERBS3 VERBS ) ( DOWN ( 5 ) ) ) 5 ( ( TENSED-MAIN2 TENSED-MAIN ) ( DOWN ( 4 ) ) ) 4 ( ( SAW2 VERB SEE ) ( DOWN T</definiens>
			</definition>
			<definition id="1">
				<sentence>A shared-packed parse forest is a data structure that stores all parses of a sentence in a compact form .</sentence>
				<definiendum id="0">shared-packed parse forest</definiendum>
				<definiens id="0">a data structure that stores all parses of a sentence in a compact form</definiens>
			</definition>
			<definition id="2">
				<sentence>( DOWN T ) ( UP 22 ) ) ( ( NP14 NP ) ( DOWN ( 7 8 17 ) ) ( UP 18 ) ) ( ( $ 13 S ) ( DOWN ( 3 18 ) ) ( UP 22 ) ) ( ( VP12 VP ) ( DOWN ( 6 9 17 ) ( 6 20 ) ) ( UP 19 ) ) ( ( PP+11 PP+ NIL ) ( DOWN ( 16 ) ) ( UP 18 20 ) ) ; attatch to an NP or VP ( ( PP10 PP NIL ) ( DOWN ( 10 15 ) ) ( UP 17 ) ) ( ( NP9 NP ) ( DOWN ( 14 13 ) ) ( UP 16 ) ) ( ( POSS8 POSS ) ( DOWN ( 11 ) ) ( UP 15 ) ) ( ( POSS-NOM7 POSS-NOM ) ( DOWN ( 12 ) ) ( UP 15 ) ) ( ( BINOCULARS7 NOUN BINOCULARS ) ( DOWN T ) ( UP 13 ) ) ( ( HIS6 PRONOUN HIS ) ( DOWN T ) ( UP 14 ) ) ( ( WITH5 PREP WITH ) ( DOWN T ) ( UP 16 ) ) ( ( NP4 NP ) ( DOWN ( 7 8 ) ) ( UP 18 ) ) ( ( BOY4 NOUN BOY ) ( DOWN T ) ( UP 9 20 ) ) ; in two different NPs ( ( THE3 DET THE ) ( DOWN T ) ( UP 9 20 ) ) ; in two different NPs ( ( VERBS3 VERBS ) ( DOWN ( 5 ) ) ( UP 18 ) ) ( ( TENSED-MAIN2 TENSED-MAIN ) ( DOWN ( 4 ) ) ( UP 6 ) ) ( ( SAW2 VERB SEE ) ( DOWN T ) ( UP 5 ) ) ( ( NP1 NP ) ( DOWN ( 1 2 ) ) ( UP 19 ) ) ( ( MAN1 NOUN MAN ) ( DOWN T ) ( UP 3 ) ) ( ( EVERY0 DET EVERY ) ( DOWN T ) ( UP 3 ) ) Figure 2 The pruned shared-packed parse forest for Every man saw the boy with his binoculars with pointers to parent nodes .</sentence>
				<definiendum id="0">DOWN T )</definiendum>
				<definiens id="0">BOY4 NOUN BOY ) ( DOWN T ) ( UP 9 20 ) ) ; in two different NPs ( ( THE3 DET THE ) ( DOWN T ) ( UP 9 20 ) ) ; in two different NPs ( ( VERBS3 VERBS ) ( DOWN ( 5 ) ) ( UP 18 ) ) ( ( TENSED-MAIN2 TENSED-MAIN )</definiens>
			</definition>
			<definition id="3">
				<sentence>( DOWN T ) ( UP 22 ) ) ( ( NP14 NP ) ( DOWN ( 7 8 17 ) ) ( UP 18 ) ) ( ( S13 S ) ( DOWN ( 3 18 ) ) ( UP 22 ) ) ( ( VP12 VP ) ( DOWN ( 6 9 17 ) ) ( UP 19 ) ) ( ( PP+11 PP+ NIL ) ( DOWN ( 16 ) ) ( UP 18 ) ) ( ( PP10 PP NIL ) ( DOWN ( 10 15 ) ) ( UP 17 ) ) ( ( NP9 NP ) ( DOWN ( 14 13 ) ) ( UP 16 ) ) ( ( POSS8 POSS ) ( DOWN ( 11 ) ) ( UP 15 ) ) ( ( POSS-NOM7 POSS-NOM ) ( DOWN ( 12 ) ) ( UP 15 ) ) ( ( BINOCULARS7 NOUN BINOCULARS ) ( DOWN T ) ( UP 13 ) ) ( ( HIS6 PRONOUN HIS ) ( DOWN T ) ( UP 14 ) ) ( ( WITH5 PREP WITH ) ( DOWN T ) ( UP 16 ) ) ( ( NP4 NP ) ( DOWN ( 7 8 ) ) ( UP 18 ) ) ( ( BOY4 NOUN BOY ) ( DOWN T ) ( UP 9 ) ) ( ( THE3 DET THE ) ( DOWN T ) ( UP 9 ) ) ( ( VERBS3 VERBS ) ( DOWN ( 5 ) ) ( UP 18 ) ) ( ( TENSED-MAIN2 TENSED-MAIN ) ( DOWN ( 4 ) ) ( UP 6 ) ) ( ( SAW2 VERB SEE ) ( DOWN T ) ( UP 5 ) ) ( ( NP1 NP ) ( DOWN ( 1 2 ) ) ( UP 19 ) ) ( ( MAN1 NOUN MAN ) ( DOWN T ) ( UP 3 ) ) ( ( EVERY0 DET EVERY ) ( DOWN T ) ( UP 3 ) ) ; can delete node ; attatch to a VP for Every man saw the boy with his binoculars given a certain world model. . . thus reducing the overhead when it is necessary to keep all parses until it is possible to make an informed choice among the alternative meanings .</sentence>
				<definiendum id="0">DOWN T )</definiendum>
				<definiens id="0">BOY4 NOUN BOY ) ( DOWN T ) ( UP 9 ) ) ( ( THE3 DET THE ) ( DOWN T ) ( UP 9 ) ) ( ( VERBS3 VERBS ) ( DOWN ( 5 ) ) ( UP 18 ) ) ( ( TENSED-MAIN2 TENSED-MAIN )</definiens>
			</definition>
			<definition id="4">
				<sentence># $ PHRASE 'statement ) ( logical-form 'sentence : np # $ NP : vp # $ VP ) ) ) ; Subject-verb agreement test ; Set the MOOD of the sentence ; Create the logical form The left-hand side of this rule is S , and the right-hand side is a list consisting of an NP and a VP .</sentence>
				<definiendum id="0">logical-form 'sentence</definiendum>
				<definiendum id="1">right-hand side</definiendum>
				<definiens id="0">Subject-verb agreement test ; Set the MOOD of the sentence</definiens>
			</definition>
			<definition id="5">
				<sentence>654 Mary P. Harper Storing Logical Form in a Shared-Packed Forest Unlike the ATN parser used by Harper ( 1990 , 1992 ) , the Tomita parser is a bottomup , all-path parsing algorithm that creates a parse forest by packing parse nodes together to save space and time .</sentence>
				<definiendum id="0">Tomita parser</definiendum>
				<definiens id="0">a bottomup , all-path parsing algorithm that creates a parse forest by packing parse nodes together to save space and time</definiens>
			</definition>
			<definition id="6">
				<sentence>A third approach is to store a procedure call for creating the logical form of a constituent in the forest and to delay the creation of the logical form , as shown in 657 Computational Linguistics Volume 20 , Number 4 26 ( ( S-MAJ196 S-MAJ ) ( DOWN ( 20 25 ) ) ( UP T ) ( CREATE-S-MAJ-LF : S 20 : PUNC 25 ) ) 25 ( ( .7 FINALPUNC . )</sentence>
				<definiendum id="0">UP T )</definiendum>
				<definiens id="0">to store a procedure call for creating the logical form of a constituent in the forest and to delay the creation of the logical form</definiens>
			</definition>
			<definition id="7">
				<sentence>( DOWN T ) ( UP 26 ) ) 24 ( ( PP174 PP ) ( DOWN ( 8 23 ) ) ( UP 21 ) ( CREATE-PP-LF : PREP 8 : OBJ 23 ) ) 23 ( ( NP165 NP ) ( DOWN ( 9 18 ) ) ( UP 24 ) ( CREATE-NP-LF : NOUN 9 : POSTNOUN-MODS 18 ) ) 22 ( ( NP130 NP ) ( DOWN ( 6 21 ) ) ( UP 19 ) ( CREATE-NP-LF : NOUN 6 : POSTNOUN-MODS 21 ) ) 21 ( ( PP+113 PP+ ) ( DOWN ( 11 18 ) ( 24 ) ) ( UP 22 19 ) ( CREATE-PP+-LF : PP 11 : PP+ 18 ) ( CREATE-PP+-LF : PP 24 ) ) 20 ( ( $ 106 S ) ( DOWN ( 2 19 ) ) ( UP 26 ) ( CREATE-S-LF : NP 2 : VP 19 ) ) 19 ( ( VP95 VP ) ( DOWN ( 5 13 18 ) ( 5 22 ) ( 5 7 21 ) ) ( UP 20 ) ( CREATE-VP-LF : VERB 5 : OBJ1 13 : PP+ 18 : SUBCAT 'TRANS ) ( CREATE-VP-LF : VERB 5 : OBJ1 22 : SUBCAT 'TRANS ) ( CREATE-VP-LF : VERB 5 : OBJ1 7 : PP+ 21 : SUBCAT 'TRANS ) ) 18 ( ( PP+84 PP+ ) ( DOWN ( 17 ) ) ( UP 19 21 23 ) ( CREATE-PP+-LF : PP 17 ) ) 17 ( ( PP79 PP ) ( DOWN ( 14 16 ) ) ( UP 18 ) ( CREATE-PP-LF : PREP 14 : OBJ 16 ) ) 16 ( ( NP76 NP ) ( DOWN ( 15 ) ) ( UP 17 ) ( CREATE-PROPERNOUN-LF : PROPERNOUN 15 ) ) 15 ( ( BILL6 PROPERNOUN BILL ) ( DOWN T ) ( UP 16 ) ) 14 ( ( WITH5 PREP WITH ) ( DOWN T ) ( UP 17 ) ) 13 ( ( NP56 NP ) ( DOWN ( 6 12 ) ) ( UP 19 ) ( CREATE-NP-LF : NOUN 6 : POSTNOUN-MODS 12 ) ) 12 ( ( PP+31 PP+ ) ( DOWN ( 11 ) ) ( UP 13 ) ( CREATE-PP+-LF : PP 11 ) ) 11 ( ( PP26 PP ) ( DOWN ( 8 10 ) ) ( UP 12 21 ) ( CREATE-PP-LF : PREP 8 : OBJ 10 ) ) 10 ( ( NP23 NP ) ( DOWN ( 9 ) ) ( UP 11 ) ( CREATE-NP-LF : NOUN 9 ) ) 9 ( ( CARS4 NOUN CAR ) ( DOWN T ) ( UP 10 23 ) ) 8 ( ( IN3 PREP IN ) ( DOWN T ) ( UP 11 24 ) ) 7 ( ( NP10 NP ) ( DOWN ( 6 ) ) ( UP 19 ) ( CREATE-NP-LF : NOUN 6 ) ) 6 ( ( FROGS2 NOUN ) ( DOWN T ) ( UP 7 13 22 ) ) 5 ( ( VERBS7 VERBS ) ( DOWN ( 4 ) ) ( UP 19 ) ( CREATE-VERBS-LF : VERBS 4 ) ) 4 ( ( TENSED-MAIN4 TENSED-MAIN ) ( DOWN ( 3 ) ) ( UP 5 ) ( CREATE-VERB-ONLY-LF : VERB 3 ) ) 3 ( ( SAW1 VERB SEE ) ( DOWN T ) ( UP 4 ) ) 2 ( ( NP1 NP ) ( DOWN ( 1 ) ) ( UP 20 ) ( CREATE-PROPERNOUN-LF : PROPERNOUN 1 ) ) 1 ( ( FRED0 PROPERNOUN ) ( DOWN T ) ( UP 2 ) ) Figure 6 Method 3 : Store procedure calls to create logical forms in the packed forest .</sentence>
				<definiendum id="0">DOWN T )</definiendum>
				<definiendum id="1">19 ) ) ( UP 26 )</definiendum>
				<definiens id="0">PP+ 21 : SUBCAT 'TRANS ) ) 18 ( ( PP+84 PP+ )</definiens>
				<definiens id="1">Store procedure calls to create logical forms in the packed forest</definiens>
			</definition>
</paper>

		<paper id="4003">
			<definition id="0">
				<sentence>Information sources used by the disambiguation method .</sentence>
				<definiendum id="0">Information sources</definiendum>
				<definiens id="0">used by the disambiguation method</definiens>
			</definition>
			<definition id="1">
				<sentence>The fourth is a methodological issue , which is relevant for developing , testing , and comparing disambiguation methods .</sentence>
				<definiendum id="0">fourth</definiendum>
				<definiens id="0">a methodological issue , which is relevant for developing , testing , and comparing disambiguation methods</definiens>
			</definition>
			<definition id="2">
				<sentence>To reduce the noise further , Yarowsky uses a system of weights that assigns lower weights to frequent words , since such words may introduce more noise .</sentence>
				<definiendum id="0">Yarowsky</definiendum>
				<definiens id="0">uses a system of weights that assigns lower weights to frequent words</definiens>
			</definition>
</paper>

		<paper id="3010">
			<definition id="0">
				<sentence>Rather , we believe a more constrained and linguistically appealing approach is to employ finite-state automata ( FSAs ) in preference to FSTS , since it has been shown how FSAS can encode autosegmental representations and a variety of constraints on those representations ( Bird and Ellison 1994 ) .</sentence>
				<definiendum id="0">FSAs</definiendum>
				<definiens id="0">shown how FSAS can encode autosegmental representations and a variety of constraints on those representations</definiens>
			</definition>
			<definition id="1">
				<sentence>A PROSODIC TYPE HIERARCHY is a subsumption network akin to the lexical hierarchy of HPSG ( Pollard and Sag 1987 ) .</sentence>
				<definiendum id="0">PROSODIC TYPE HIERARCHY</definiendum>
				<definiens id="0">a subsumption network akin to the lexical hierarchy of HPSG ( Pollard and Sag 1987 )</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus , we shall arrive at a scheme like the following , where the Ci indicate the extra constraints : 8 Example 12 a. phrase =_ foot + A C1 A ... A Ck b. foot = syl + ACt A ... A Cn This concludes our discussion of string-based phonology .</sentence>
				<definiendum id="0">Cn</definiendum>
				<definiens id="0">a. phrase =_ foot + A C1 A ... A Ck b. foot</definiens>
			</definition>
			<definition id="3">
				<sentence>In and of itself , HPSG imposes no restrictions on the kind of operations that can be performed in the course of composing morphemes into words , or words into phrases .</sentence>
				<definiendum id="0">HPSG</definiendum>
				<definiens id="0">imposes no restrictions on the kind of operations that can be performed in the course of composing morphemes into words , or words into phrases</definiens>
			</definition>
			<definition id="4">
				<sentence>Similarly , an automaton denotes a set of objects , namely strings ( o1 '' automaton tapes ) .</sentence>
				<definiendum id="0">automaton</definiendum>
				<definiens id="0">a set of objects , namely strings ( o1 '' automaton tapes )</definiens>
			</definition>
			<definition id="5">
				<sentence>Two-level morphology : A general computational model for word-form recognition and production .</sentence>
				<definiendum id="0">Two-level morphology</definiendum>
				<definiens id="0">A general computational model for word-form recognition and production</definiens>
			</definition>
</paper>

		<paper id="3006">
</paper>

		<paper id="3002">
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>Then we define state-labeled automata ( Section 3.1 ) , show their equivalence to finite state automata ( Section 3.2 ) , define the operations of concatenation , union , intersection , and complement ( Section 3.3 ) , and further define state-labeled transducers ( Section 3.4 ) .</sentence>
				<definiendum id="0">complement</definiendum>
				<definiens id="0">define the operations of concatenation , union , intersection</definiens>
			</definition>
			<definition id="1">
				<sentence>Definition 1 A STATE-LABELED NONDETERMINISTIC FINITE AUTOMATON ( SFA ) is a septuple ( V~ ~ , ~ , 6~ S , F , e ) where V is a finite set , the set of STATES , ~ .</sentence>
				<definiendum id="0">STATE-LABELED NONDETERMINISTIC FINITE AUTOMATON</definiendum>
				<definiens id="0">a septuple ( V~ ~ , ~ , 6~ S , F , e ) where V is a finite set</definiens>
			</definition>
			<definition id="2">
				<sentence>is a finite set , the ALPHABET , C V x ~ , is the LABELING RELATION ( states are labeled with subsets of the alphabet ) , 5 6 c V x V is the TRANSITION RELATION , S C V is the set of START STATES , and F c V is the set of FINAL STATES .</sentence>
				<definiendum id="0">S C V</definiendum>
				<definiendum id="1">F c V</definiendum>
				<definiens id="0">the LABELING RELATION ( states are labeled with subsets of the alphabet ) , 5 6 c V x V is the TRANSITION RELATION ,</definiens>
			</definition>
			<definition id="3">
				<sentence>e is a Boolean flag that is true iff the null string A is accepted , and false otherwise .</sentence>
				<definiendum id="0">e</definiendum>
				<definiens id="0">a Boolean flag that is true iff the null string A is accepted , and false otherwise</definiens>
			</definition>
			<definition id="4">
				<sentence>Now F-A is defined as a successor relation on situations for an automaton A. Definition 4 Let Ix , T , y I and Ix ' , T ' , y ' I be two situations .</sentence>
				<definiendum id="0">F-A</definiendum>
				<definiens id="0">a successor relation on situations for an automaton A. Definition 4 Let Ix , T , y I and Ix ' , T ' , y ' I be two situations</definiens>
			</definition>
			<definition id="5">
				<sentence>If we use SFAs instead , we find the reverse : Kleene star requires two that ignores its input , while an SFA is a Moore machine that ignores its input .</sentence>
				<definiendum id="0">SFA</definiendum>
				<definiens id="0">a Moore machine that ignores its input</definiens>
			</definition>
			<definition id="6">
				<sentence>Similarly , if A is the set of SFAs , then ( A ; U , , 4 , 3_ , T ) is also a Boolean algebra , where 3_ is the empty automaton ( i.e. , L ( 3_ ) = 0 ) and T is the automaton that accepts G* ( i.e. , L ( T ) = y~ $ k ) .7 The concatenation of two SFAs A and B , written AB , has an arrow linking each final state of the first SFA to each initial state of the second .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">the empty automaton</definiens>
			</definition>
			<definition id="7">
				<sentence>The states that are initial or final in AB depend on whether A or B accepts the empty string A , as specified in the following table .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">accepts the empty string A , as specified in the following table</definiens>
			</definition>
			<definition id="8">
				<sentence>AB : 1 4 \l \l 3 '' ® Since neither A nor B accepts A , the initial state of AB is the initial state of A , and the final state of AB is the final state of B. The union ( or disjunction ) on SFAs is similar , in many ways , to the concatenation operation .</sentence>
				<definiendum id="0">AB</definiendum>
				<definiens id="0">similar , in many ways , to the concatenation operation</definiens>
			</definition>
			<definition id="9">
				<sentence>The union of two automata A and B , written A U B , accepts the string s iff either A or B , or both , accept s. The union of A and B is expressed dia7 of course , these algebras are different , for there are many languages that can not be defined by SFAs .</sentence>
				<definiendum id="0">union of two</definiendum>
				<definiens id="0">written A U B , accepts the string s iff either A or B , or both , accept s. The union of A and B is expressed dia7 of course</definiens>
			</definition>
			<definition id="10">
				<sentence>So if S is the set of states and F is the set of final states , then S\F is the set of final states in the complement .</sentence>
				<definiendum id="0">F</definiendum>
				<definiendum id="1">S\F</definiendum>
				<definiens id="0">the set of states</definiens>
				<definiens id="1">the set of final states</definiens>
				<definiens id="2">the set of final states in the complement</definiens>
			</definition>
			<definition id="11">
				<sentence>A B \/ C A singly associated autosegment , such as A , is written down as the following formula : A : I = s ( A ) Ma ( 1 ) - ( A , . )</sentence>
				<definiendum id="0">B \/ C A singly associated autosegment</definiendum>
				<definiendum id="1">) Ma</definiendum>
				<definiens id="0">the following formula : A : I = s ( A</definiens>
			</definition>
			<definition id="12">
				<sentence>-~ ( o* ( NS F1LL U AA ) o* ) This states that it is not possible to find anywhere a nasal-stop cluster ( NS ) that is not made up of two labials ( LL ) or two alveolars ( AA ) .</sentence>
				<definiendum id="0">nasal-stop cluster ( NS</definiendum>
				<definiens id="0">not made up of two labials ( LL ) or two alveolars ( AA )</definiens>
			</definition>
			<definition id="13">
				<sentence>Vs c s , SD ( s ) ~ SC ( s ) Vs C_ S , ~SD ( s ) V SC ( s ) ~s c s , SD ( s ) A -~SC ( s ) -4 , * ( SD n SC ) , * ) This is how we arrived at ( 22 ) from ( 21b ) .</sentence>
				<definiendum id="0">SD</definiendum>
				<definiendum id="1">-~SC</definiendum>
				<definiens id="0">s ) ~ SC ( s ) Vs C_ S , ~SD ( s ) V SC ( s ) ~s c s</definiens>
			</definition>
			<definition id="14">
				<sentence>Turkish is an agglutinating language , and the vowels in many affixes depend on the final vowel in the root of the word .</sentence>
				<definiendum id="0">Turkish</definiendum>
				<definiens id="0">an agglutinating language , and the vowels in many affixes depend on the final vowel in the root of the word</definiens>
			</definition>
			<definition id="15">
				<sentence>Kay specifies a transducer by providing a set of FRAMES .</sentence>
				<definiendum id="0">Kay</definiendum>
				<definiens id="0">specifies a transducer by providing a set of FRAMES</definiens>
			</definition>
			<definition id="16">
				<sentence>A simple frame is the following : k : C : c : k This frame specifies that when a k is being read from the consonantal tape and a C is being read from the CV tape , then there is an empty transition on the vocalism tape and the surface tape must have a k. There is a similar frame for each consonant .</sentence>
				<definiendum id="0">simple frame</definiendum>
			</definition>
			<definition id="17">
				<sentence>Wiebe shows how the multi-linear code satisfies the criteria for computability , invertibility , iconicity , and compositionality .</sentence>
				<definiendum id="0">Wiebe</definiendum>
				<definiens id="0">shows how the multi-linear code satisfies the criteria for computability , invertibility , iconicity , and compositionality</definiens>
			</definition>
			<definition id="18">
				<sentence>tier 1 a:2:0:0 tier2 C:0:1:0 V : I:0:0 C:0:1:0 C:0:1:0 V : I:0:0 C:0:1:0 tier3 k:0 : l:0 t:0:2:0 b:0 : l:0 Suppose that a : p : q : r is an arbitrary 4-tuple of the kind in ( 43 ) .</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">an arbitrary 4-tuple of the kind in ( 43 )</definiens>
			</definition>
			<definition id="19">
				<sentence>The root SFA generalizes over all stems constructed from the same root .</sentence>
				<definiendum id="0">SFA</definiendum>
				<definiens id="0">generalizes over all stems constructed from the same root</definiens>
			</definition>
			<definition id="20">
				<sentence>`` KIMMO : A general morphological processor . ''</sentence>
				<definiendum id="0">KIMMO</definiendum>
			</definition>
			<definition id="21">
				<sentence>Phonology : A Cognitive View .</sentence>
				<definiendum id="0">Phonology</definiendum>
			</definition>
			<definition id="22">
				<sentence>Two-level morphology : A general computational model for word-form recognition and production .</sentence>
				<definiendum id="0">Two-level morphology</definiendum>
				<definiens id="0">A general computational model for word-form recognition and production</definiens>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>The grammatical topic is the wa-marked entity , which is by default predicted to be the most salient discourse entity in the following discourse .</sentence>
				<definiendum id="0">grammatical topic</definiendum>
				<definiens id="0">the wa-marked entity , which is by default predicted to be the most salient discourse entity in the following discourse</definiens>
			</definition>
			<definition id="1">
				<sentence>Within a theory of discourse , CENTERING is a computational model of the process by which conversants coordinate attention in discourse ( Grosz , Joshi , and Weinstein unpublished ) .</sentence>
				<definiendum id="0">CENTERING</definiendum>
			</definition>
			<definition id="2">
				<sentence>Centering is intended to reflect aspects of ATYENTIONAL STATE in a tripartite view of discourse structure that also includes INTENTIONAL STRUCTURE and LINGUISTIC STRUCTURE ( Grosz and Sidner 1986 ) .</sentence>
				<definiendum id="0">Centering</definiendum>
			</definition>
			<definition id="3">
				<sentence>The Cb is the discourse entity that the utterance most centrally concerns , what has been elsewhere called the 'theme ' ( Reinhart 1981 ; Horn 1986 ) .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiens id="0">the discourse entity that the utterance most centrally concerns , what has been elsewhere called the 'theme '</definiens>
			</definition>
			<definition id="4">
				<sentence>4 The PREFERRED CENTER represents a prediction about the Cb of the following utterance .</sentence>
				<definiendum id="0">PREFERRED CENTER</definiendum>
				<definiens id="0">a prediction about the Cb of the following utterance</definiens>
			</definition>
			<definition id="5">
				<sentence>The center , Cb ( Ui ) , is the highest-ranked element of Cf ( Ui-D that is realized in Ui .</sentence>
				<definiendum id="0">Cb ( Ui )</definiendum>
				<definiens id="0">the highest-ranked element of Cf ( Ui-D that is realized in Ui</definiens>
			</definition>
			<definition id="6">
				<sentence>An utterance U realizes a center c if c is an element of the situation described by U , or c is the semantic interpretation of some subpart of U ( Grosz , Joshi , and Weinstein unpublished ) .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">an element of the situation described by U , or</definiens>
			</definition>
			<definition id="7">
				<sentence>A specialization of the relation REALIZE is the relation DIRECTLY REALIZE .</sentence>
				<definiendum id="0">specialization of the relation REALIZE</definiendum>
			</definition>
			<definition id="8">
				<sentence>Cb : ZIROO Cfl : \ [ ZIROO , TAROO\ ] SMOOTH-SHIFT 32 subj obj Cf2 : \ [ TAROO , ZIROO\ ] ROUGH-SHIFT 2 SUBJ OBJ In example 4 , the use of TOPIC marking in the phrase Ziroo wa of utterance ( c ) means that ( c ) is interpreted as a RETAINJ 2 Ziroo becomes the most highly ranked discourse entity for c , although Taroo is the Cb since Taroo was most highly ranked for utterance ( b ) ( by Constraint 3 ) .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiendum id="1">Taroo</definiendum>
				<definiens id="0">the most highly ranked discourse entity for c , although</definiens>
			</definition>
			<definition id="9">
				<sentence>The theory of centering is a formal specification that is intended to model attentional state and is defined by the rules and constraints given in Section 2.1 .</sentence>
				<definiendum id="0">theory of centering</definiendum>
			</definition>
			<definition id="10">
				<sentence>Empathy is the speaker 's identification with a discourse entity , but the speaker does not have to take the perspective of the person who he empathizes with .</sentence>
				<definiendum id="0">Empathy</definiendum>
			</definition>
			<definition id="11">
				<sentence>For instance , Kuno 's EMPATHY HIERARCHY consists of different scales for EMPATHY that include notions such as TOPIC and SPEAKER ( Kuno 1987 ) .</sentence>
				<definiendum id="0">EMPATHY HIERARCHY</definiendum>
			</definition>
			<definition id="12">
				<sentence>INITIAL CENTER INSTANTIATION is a process by which a discourse entity introduced in a segment-initial utterance becomes the Cb .</sentence>
				<definiendum id="0">INITIAL CENTER INSTANTIATION</definiendum>
				<definiens id="0">a process by which a discourse entity introduced in a segment-initial utterance becomes the Cb</definiens>
			</definition>
			<definition id="13">
				<sentence>Japanese Discourse no clear indication of topic is given , the Cf ordering alone is not a strong constraint ; ( 2 ) the ordering of the Cf should be partly determined by lexical semantics or other knowledge about the situation being described .</sentence>
				<definiendum id="0">Cf ordering</definiendum>
				<definiens id="0">partly determined by lexical semantics or other knowledge about the situation being described</definiens>
			</definition>
			<definition id="14">
				<sentence>ZTA explains why the discourse entity Hanako , which is realized as OBJECT2 in example 32c is interpreted as the SUBJECT of example 32d .</sentence>
				<definiendum id="0">ZTA</definiendum>
				<definiens id="0">explains why the discourse entity Hanako</definiens>
			</definition>
			<definition id="15">
				<sentence>The only difference between 32 and 33 is that in 32c , MITIKO is a gamarked subject , whereas in 33c , MITIKO is a wa-marked subject/grammatical topic .</sentence>
				<definiendum id="0">MITIKO</definiendum>
				<definiendum id="1">MITIKO</definiendum>
				<definiens id="0">a gamarked subject , whereas in 33c</definiens>
				<definiens id="1">a wa-marked subject/grammatical topic</definiens>
			</definition>
			<definition id="16">
				<sentence>In other words , none of the required properties of SUBJ , IDENT , NONSUBJ , NONIDENT , which 'should ' be shared according to the PS constraint , are shared .</sentence>
				<definiendum id="0">NONIDENT</definiendum>
				<definiens id="0">none of the required properties of SUBJ , IDENT , NONSUBJ ,</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>A prosodic parse S is therefore given by S = ( bl , b2 , ... , bL ) , where bi is the break index after the ith word and L is the number of words in the sentence .</sentence>
				<definiendum id="0">bi</definiendum>
				<definiendum id="1">L</definiendum>
				<definiens id="0">the number of words in the sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>A break is a random variable that can take on one of a finite number of values from `` no break '' ( orthographic word boundary , but not a prosodic constituent boundary ) to `` sentence boundary , '' where the values form an ordered set that correspond to the different levels of the hierarchy .</sentence>
				<definiendum id="0">break</definiendum>
				<definiens id="0">orthographic word boundary , but not a prosodic constituent boundary ) to `` sentence boundary , '' where the values form an ordered set that correspond to the different levels of the hierarchy</definiens>
			</definition>
			<definition id="2">
				<sentence>_l ) , Mi-1 ) = p ( rnijIWi , mi ( j-1 ) ) p ( mil\ ] Wi , Mi-1 ) -p ( mil\ ] Wi~m ( i_l ) ni_l ) ~ where Wi is the sequence of feature vectors spanning the ith major phrase .</sentence>
				<definiendum id="0">Wi</definiendum>
				<definiens id="0">the sequence of feature vectors spanning the ith major phrase</definiens>
			</definition>
			<definition id="3">
				<sentence>Eight categories of syntactic constituent were used : sentence ( S ) , noun phrase ( NP ) , verb phrase ( VP ) , prepositional phrase ( PP ) , wh-noun phrase ( WHNP ) , adjective or adverbial phrase ( AP ) , any other constituent ( O ) , and both words in the same lowest level constituent ( same ) .</sentence>
				<definiendum id="0">AP</definiendum>
				<definiens id="0">sentence ( S ) , noun phrase ( NP ) , verb phrase ( VP ) , prepositional phrase ( PP ) , wh-noun phrase ( WHNP ) , adjective or adverbial phrase</definiens>
			</definition>
			<definition id="4">
				<sentence>This probability distribution p ( blt ) is used in the hierarchical model for computing the probability of a minor phrase , Equation ( 6 ) , by running test data through the tree and using the probability distribution associated with the final leaf node = f ( Wi , mprev ) .</sentence>
				<definiendum id="0">probability distribution p</definiendum>
				<definiendum id="1">Equation</definiendum>
				<definiens id="0">used in the hierarchical model for computing the probability of a minor phrase</definiens>
			</definition>
			<definition id="5">
				<sentence>The most likely prosodic parse can be found efficiently using a dynamic programming ( DP ) algorithm that is similar to algorithms used in speech recognition , in particular that for the Stochastic Segment Model ( Ostendorf and Roukos 1989 ) , except that the dynamic programming routine is called recursively for successive levels in the hierarchy .</sentence>
				<definiendum id="0">Stochastic Segment Model</definiendum>
				<definiens id="0">similar to algorithms used in speech recognition</definiens>
			</definition>
			<definition id="6">
				<sentence>Defining pt ( uil ... Uinl~/Vi , Ui ) as the probability of the most likely sequence of n subunits in but not necessarily spanning Ui and ending at location t , and Uij ( S ~ t ) as a subunit that spans boundaries { bs , ... , bt } , the 37 Computational Linguistics Volume 20 , Number 1 dynamic programming algorithm can be expressed generally in the subroutine that follows .</sentence>
				<definiendum id="0">Uij</definiendum>
				<definiens id="0">the probability of the most likely sequence of n subunits in but not necessarily spanning Ui and ending at location t</definiens>
			</definition>
			<definition id="7">
				<sentence>For each n-length sequence of subunits spanning \ [ 1 , t\ ] ( n = 2 , ... ~ t ) : log Pt ( Uil ' '' Uin \ [ \ ] /Vi~ Ui-1 ) = maxs &lt; t log ps ( Uil , . . . , Ui , ( n-1 ) \ ] ~/Vi~ Ui-1 ) + logp ( uin ( S + 1 , t ) \ [ Wi , Ui ( n-1 ) ) ( Computing logp ( uin ( s + 1~ t ) \ ] Wi , Ui ( n-1 ) ) with a recursive call to this routine. ) Save pointers to best previous break location s. To find the most likely sequence , p ( Ui\ [ ~/Vi , Ui-1 ) -~ maxn logPl , ( Uil~ . . . , Uin\ [ ~/Vi , Ui-1 ) qlogq ( n\ [ li ) The final step is to decode the sequence of breaks once the value n* that maximizes the above equation is determined. Using the n* associated with any level unit , we can trace back to find the optimal segmentation of subunits that comprise that unit. The complete parse is found by tracing back at the highest level units and successively tracing back in each lower level. For the specific case of a three-level hierarchy , the most likely major phrase sequence in a sentence p ( S\ [ W ) and the most likely minor phrase sequence in a major phrase p ( Mil W ) are found by a dynamic programming algorithm , called recursively. The lowest level unit considered here is the minor phrase , and the probability of the minor phrase is computed as given in Equation ( 6 ) using the decision tree. For our investigation of prosodic phrase structure , an FM radio news story corpus was used. The training data included ten stories from one announcer and another ten stories from a second announcer , both female , for a total of 312 sentences ( 6,157 words , or potential boundary locations ) . The stories were studio recordings of actual radio broadcasts , which were transcribed by a listener who did not have access to the original scripts. It is likely that transcription of punctuation did not exactly match the original written text and may have been biased by the prosody of the utterance. However , the radio announcers tended to annotate the transcribed text before reading the test stories , so we conjecture that commas were more often omitted than inserted in our transcriptions. All of the training stories were used to estimate the probabilities of the 38 M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction number of subconstituents ( Equations 1-3 ) . In the first pass of tree design , two-thirds of the training data was used to grow the tree and one-third was used to determine the performance complexity trade-off , but the final tree used was redesigned on the entire training set. For testing , we used five versions of a different story spoken by two female and two male announcers ( one radio broadcast version and four radio-news-style lab recordings ) . One of the female announcers ( two spoken versions ) was the same as the speaker who provided roughly three-quarters of the training data. Multiple test versions are used in order to allow for some acceptable differences in phrasing in the context of the FM radio news style , and to investigate the possibility of speakerdependent effects. On average , there were 3.3 different prosodic parses among the five versions. The test story contained 23 sentences ( 385 words ) ranging in length from 3 to 36 words. For reference , the test sentences are included in an appendix with the phrase predictions of our best system. Prosodic phrase breaks were hand-labeled in the entire corpus ; the training set labels were used for estimating the parameters of the model and test set labels were used for evaluating the performance of the model. The prosodic phrase labeling system used break indices marked between each pair of words , based on auditory perceptual judgments ( that is , the labelers did not have access to spectrogram or pitch displays ) . The break indices ranged on a scale of 0 to 6 , chosen to map to a superset of the prosodic hierarchies proposed in the literature. The labeling scheme is described in more detail in Price , Ostendorf , Shattuck-Hufnagel , and Fong ( 1991 ) . Six of the stories were labeled by polling two listeners who discussed any discrepancies. The remaining stories were labeled by a third listener working independently. Comparing the labels of one story using both schemes showed that there was a high degree of consistency across labelers. For the full seven-level labeling system , the correlation between the two sets of labels was 0.93 , where correlation is computed as the maximum likelihood estimate of the correlation coefficient based on the two sets of labels. Only 1 % of the labels differed by 2 , and these were at locations where the disagreement was actually over the location of the boundary rather than the relative strength of the boundary. In this work we considered only a three-level hierarchy and therefore mapped breaks 0-2 to `` no break , '' 3 to a `` minor break '' ( I ) , 4 and 5 to a `` major break '' ( ll ) and 6 to a `` sentence break. '' The goal of this algorithm is to predict placement of phrase breaks that sound natural to listeners and that communicate the intended meaning of the sentence. As mentioned above , many renditions of a sentence can fulfill this criterion. Therefore , we have attempted to estimate system performance by comparing the predicted breaks to parses observed in five spoken versions of the sentence. Although the ultimate test of the algorithm is in a speech synthesis system , a quantitative measure of system performance is useful in algorithm development and comparison. We have considered four performance measures in this work. Since one incorrectly assigned break could make a whole sentence or clause unacceptable , one measure of system performance is the number of sentences with a predicted parse that matches entirely a parse observed in any of the five spoken versions. When such a match occurs , we call the predicted parse `` correct. '' The five spoken versions do not represent an exhaustive set of acceptable parses , however. Therefore in a separate evaluation , the sentence is also judged subjectively to determine whether it is an `` acceptable '' parse. The number of sentences that fall into these two categories 39 Computational Linguistics Volume 20 , Number 1 are reported separately , and for the best case system are marked separately in the results in the appendix. In order to better understand the system performance , we have chosen to compute additional error measures based on the prediction accuracy at individual break locations. A predicted sentence is compared to each of the five spoken versions , and the closest spoken version is used as the reference for that sentence. ( The closeness of parses is measured using a Euclidean distance with 0 for no break , 1 for minor break and 2 for major break. ) Then the correspondence between predicted and observed breaks is tabulated in a confusion matrix. Sentence breaks are deterministically assigned at periods , but these are included in the performance results reported here ( as major breaks ) to be consistent with results reported elsewhere. Also , note that confusion tables for different systems sometimes reflect different numbers of observed minor and major breaks because the predicted sentences may best match different versions of the test sentence. It is also useful to have a simple measure for comparing systems. One possible performance figure is the overall percent correct , but we have found this measure to be difficult to interpret because the overall figure is dominated by the performance on the much more frequent `` no break '' locations. Instead , we compute the correct prediction and false prediction rate for breaks as a combined class ( merging minor and major breaks ) . Using terminology from detection theory , these are also referred to as correct detection ( CD ) and false detection ( FD ) in the following sections. CD/FD results must be interpreted with some caution , because there is a trade-off between the two error rates : higher break detection rates are associated with a higher rate of false break insertion. If the insertion rate is too high , there will be few good parses at the sentence level. We have therefore tried to control the insertion rate as much as possible for the different systems evaluated. Two types of CD/FD results are reported. One figure is computed based on comparison to the nearest sentence of the five versions. In addition , since other research results have been reported based on comparison to only one spoken version , we include correct prediction and false prediction rates that correspond to the average rates over the five separate test versions. In general , the correct prediction rates using the single version comparison are roughly 10 % lower than using the comparison to five versions , so comparison to one version significantly underestimates performance of the algorithms. The variation in error rate over the five versions is relatively small , as shown later in the discussion of speaker-dependent effects. Several experiments using different sets of questions to train the embedded decision trees were performed in order to compare the relative merits of different information in the hierarchical model , as well as trade-offs associated with computational complexity. The entire set of questions is listed below. All experiments included questions 1-8 , which were based on features that were relatively straightforward to extract from text , using a table look-up to assign part-of-speech labels. Experiments that made use of syntactic features also allowed questions 9-13. The syntax experiments were based on trees that were trained using only 14 of the 20 stories , since skeletal parses were only available for these stories. Another set of experiments included question 14 , which tested the ratio of the current minor phrase length to the previous minor phrase length. Finally , experiments that made use of the more detailed POS classifications included question 15 , and used the additional particle category in question 3. All questions were based on features derived from text information only. 40 M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction Below we enumerate the questions used in the different tree design experiments , together with the motivation for each question. . . . . . . . . 10. 11. 12. Is this a sentence or major phrase boundary ? Assuming major breaks occur at qualitatively different locations than minor breaks , we effectively remove the major breaks and sentences from our training corpus with this question. Is the left word a content word and the right word a function word ? In the training data , 65 % of the minor and major breaks combined occur at content word/function word ( CW/FW ) boundaries , and about half of the CW/FW boundaries are marked with breaks. The CW/FW boundaries also correspond to the prosodic group boundaries used deterministically in Sorin , Larreur , and Llorca ( 1987 ) and in Veilleux et al. ( 1990 ) . What is the function word type of the word to the right ? Previous work in prosodic parsing with a small dictionary ( Sorin , Larreur , and Llorca 1987 ) suggested that different types of function words may be more or less likely to signal a prosodic phrase break. Is either adjacent word a proper name ( capitalized ) ? Preliminary examination of our data suggested there was some relationship between proper nouns and phrase boundaries , probably related to the phrasing of complex nominals. How many content words have occurred since the previous function word ? Speakers seemed to insert phrase breaks when a string of content words became long , e.g. , exceeded four or five words. Is there a comma at this location ? Usually , but not always , a major phrase break occurs at locations orthographically transcribed with commas. What is the relative location in the sentence ( in eighths ) ? Previous work ( Gee and Grosjean 1983 ) has suggested that prosodic phrase boundaries tend to bisect a longer unit. Therefore , one of the questions used to partition the training data is the ratio of the word number over the sentence length , quantized to the nearest eighth. What is the relative location in the proposed major phrase ( in eighths ) ? This question is included following the same reasoning as the previous question. What is the largest syntactic unit that dominates the word preceding the potential boundary location and not dominating the succeeding word ? Phrase breaks are known to co-occur with certain syntactic configurations. For example , phrase breaks often occur before subordinate clauses. What is the largest syntactic unit that dominates the word succeeding the potential boundary location and not dominating the preceding word ? The rationale behind this question is similar to that of the previous question. What is the smallest syntactic unit that dominates both ? Some syntactic units may be less likely to be broken up by a phrase break. How many syntactic units end between the two words ? This question provides information on the relative level of syntactic attachment between the two words , capturing the effect of constituent endings. 41 Computational Linguistics Volume 20 , Number 1 13. 14. 15. How many syntactic units begin between the two words ? This question is similar to the previous one , except that it captures effects associated with the start of new constituents. How large is the ratio of the current minor phrase length over the previous minor phrase length ? This question incorporates the concept of balancing minor phrase lengths noted by other researchers ( Gee and Grosjean 1983 ; Bachenko and Fitzpatrick 1990 ) , and was found to be useful in phrase prediction trees investigated by Wang and Hirschberg ( 1992 ) . In the beginning of a sentence where there is no previous minor phrase , the ratio is treated as missing data and handled using a surrogate variable ( Breiman , Friedman , Olshen , and Stone 1984 ) . What is the label of the content word to the right ? to the left ? Wang and Hirschberg found that part-of-speech information is useful in phrase break prediction ( Wang and Hirschberg 1992 ) . Questions 5 , 12 , 13 , and 14 are based on numerical features , so the binary question asks whether the feature is greater than some threshold , where the threshold is determined automatically in tree design. All other questions are based on categorical variables , and the best binary groupings of the possible values are determined automatically ( Breiman , Friedman , Olshen , and Stone 1984 ) . Two of the questions ( 8 , 14 ) require knowledge of major or minor phrase boundaries. This information is available in the training data or from a spoken utterance , but hypothesized locations of minor and major phrase breaks must be used in phrase prediction from text. Therefore , these features are calculated dynamically in the prediction algorithm for each hypothesized prosodic parse. The first tree designed used only the very simple information represented by questions 1-8. The resulting tree is shown in Figure 2 , with the relative frequency of a break in the training data included at each node. The first split trivially locates the sentence and major break boundaries. The second split utilized the content word/function word boundary question that we had used deterministically in previous work ( Veilleux et al. 1990 ) . The content/function word boundaries seem to be important in other algorithms as well : they correspond closely to the phi-phrase boundaries that would be predicted by the Bachenko-Fitzpatrick algorithm , and they seem to be captured in the Wang-Hirschberg text-only tree by a succession of questions about the part-of-speech labels of the words adjacent to the break. Of the boundaries that were preceded by a content word and followed by a function word , 30 % were hand-labeled as minor breaks , whereas only 4 % of other locations were labeled as minor breaks and these were identified by the next question as coinciding with a comma. The complete tree was relatively small ( 9 nodes ) , and used almost all questions provided. On the training data , the resulting tree classified 89 % of the nonbreaks correctly and 59 % of the minor breaks correctly. All sentence and major breaks were given in the tree design. The next stage was to incorporate syntactic information ( questions 9-13 ) into the tree design algorithm to determine minor phrase probabilities. Syntactic parses were available for only 14 of the 20 training stories ( 217 sentences , 4,230 words ) , and the tree was designed using this subset. A very simple five-node tree was designed , as shown in Figure 3. Again the first nontrivial question was concerning the content/function word boundaries , and the presence of a comma was again used to predict minor breaks at other locations. The two other questions in the tree were based on which syntactic unit dominated one or the other words at the boundary site. The tree design algorithm chose syntactic units that were less likely to contain a boundary as : words 42 M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction sentence / major word boundary ? riO content / function word ~boundary ? comma ? yes other place in major phrase ? other first or last , Quarter ? -la^e in ~ function word 180/571 1-1 ~ 9 ~71/323 J t , ,n e 9 sentence .. ~ ~v • ' \eighth __Z_ , ~ -- -- -~- , xfunction word~ -- -\ ] ~~ capital ? other X _\ ] g yes / \ no Figure 2 Tree designed using only simple part-of-speech information , questions 1-8. Relative frequency of a `` break '' ( in the training data ) is indicated in each node for the subset of data associated with that node , and the left branch in a split is more likely to have a break. in the same constituent , words separated by a wh-noun phrase boundary , and words separated by a verb phrase initial boundary. ( In their work on spontaneous speech , Wang and Hirschberg found that noun phrases in general tended to be less likely to contain boundaries. ) The tree with syntactic information seemed to classify minor breaks with slightly higher accuracy than the previous tree : 90 % correct classification of nonbreaks and 62 % correct classification of minor breaks in the training data. A third tree , illustrated in Figure 4 , was grown using the first 8 baseline questions and question 14 , which examines the ratio of current minor phrase length to previous minor phrase length. The motivation behind this question is constituent balancing , as mentioned earlier. The main difference between this tree and the first one is that the minor phrase length ratio test is chosen instead of the question about the position in a major phrase. These two questions served similar roles , as evidenced by the fact that the surrogate variable for the ratio test was the location of the current word within 43 Computational Linguistics Volume 20 , Number 1 sentence / major phrase dary ? \ ] t'~'~ , ~ content/function word 752/752 \ ] ~b~mdary ? ... . left word comma ? k , ~_/° ~ , Y dominated by ... \. right word I 1091212 ~ . ~ ~oghmti~a~er~ by ... I 8/8 I I 10412760 I \s o , I Jl P Figure 3 Tree designed using syntactic information but not the minor phrase length ratio test , questions 1-13. Relative frequency of a `` break '' ( in the training data ) is indicated in each node for the subset of data associated with that node , and the left branch in a split is more likely to have a break. the major phrase , in terms of the ratio of the number of words up to the current position over the total length of the major phrase. Classification rates on the training data for this tree were 87 % for nonbreaks and 66 % for minor breaks. A fourth tree was designed using the first fourteen questions , and performance was similar to that for the third tree. The decision tree design algorithm 's performance was not significantly changed by the introduction of additional features. New features can supplant previously used ones , as also found by Wang and Hirschberg ( 1992 ) , because of the redundancy in information between features. For example , in the syntax trees , many of the baseline questions were no longer chosen , but the overall classification performance was similar. The trees were used in the hierarchical model , and the phrase break prediction algorithm was evaluated on the independent test set described in Section 4.1. A summary of the results is given in Table 2 , and the corresponding confusion matrices are in Tables 3 and 4. The baseline system ( questions 1-8 ) gave the best performance , with a correct prediction rate of 81 % and a false prediction rate of 4 % . The results indicate that syntactic information did not improve the performance of the algorithm , and in fact gave poorer phrase predictions by every measure of performance on the test data. The difference in performance can not be attributed to the smaller amount of training data used in the experiments with syntax , because designing the model without syntax on this subset actually yielded slightly better performance on the test set than that 44 M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction entence/major phrase boundary ? -- . % __. \ [ 992/992 \ [ ( 430/4599 ) content/function word I `` 'J ~ary ? ~ comma ? ~comma ? 111118/ &gt; !</sentence>
				<definiendum id="0">Ui ( n-1 ) ) ( Computing logp</definiendum>
				<definiendum id="1">false detection</definiendum>
				<definiendum id="2">FD</definiendum>
				<definiendum id="3">content/function word boundaries</definiendum>
				<definiendum id="4">corresponding confusion matrices</definiendum>
				<definiens id="0">the most likely major phrase sequence in a sentence p ( S\ [ W ) and the most likely minor phrase sequence in a major phrase p ( Mil W ) are found by a dynamic programming algorithm , called recursively. The lowest level unit considered here is the minor phrase , and the probability of the minor phrase is computed</definiens>
				<definiens id="1">likely that transcription of punctuation did not exactly match the original written text and may have been biased by the prosody of the utterance. However , the radio announcers tended to annotate the transcribed text before reading the test stories , so we conjecture that commas were more often omitted than inserted in our transcriptions. All of the training stories were used to estimate the probabilities of the 38 M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction number of subconstituents</definiens>
				<definiens id="2">the test sentences are included in an appendix with the phrase predictions of our best system. Prosodic phrase breaks were hand-labeled in the entire corpus ; the training set labels were used for estimating the parameters of the model and test set labels were used for evaluating the performance of the model. The prosodic phrase labeling system used break indices marked between each pair of words , based on auditory perceptual judgments</definiens>
				<definiens id="3">to predict placement of phrase breaks that sound natural to listeners and that communicate the intended meaning of the sentence. As mentioned above , many renditions of a sentence can fulfill this criterion. Therefore , we have attempted to estimate system performance by comparing the predicted breaks to parses observed in five spoken versions of the sentence. Although the ultimate test of the algorithm is in a speech synthesis system , a quantitative measure of system performance is useful in algorithm development</definiens>
				<definiens id="4">tested the ratio of the current minor phrase length to the previous minor phrase length. Finally</definiens>
				<definiens id="5">the relative location in the sentence ( in eighths</definiens>
				<definiens id="6">the ratio of the word number over the sentence length , quantized to the nearest eighth. What is the relative location in the proposed major phrase ( in eighths</definiens>
				<definiens id="7">the ratio of the current minor phrase length over the previous minor phrase length ? This question incorporates the concept of balancing minor phrase lengths noted by other researchers</definiens>
				<definiens id="8">well : they correspond closely to the phi-phrase boundaries that would be predicted by the Bachenko-Fitzpatrick algorithm , and they seem to be captured in the Wang-Hirschberg text-only tree by a succession of questions about the part-of-speech labels of the words adjacent to the break. Of the boundaries that were preceded by a content word and followed by a function word</definiens>
				<definiens id="9">All sentence and major breaks were given in the tree design. The next stage was to incorporate syntactic information ( questions 9-13 ) into the tree design algorithm to determine minor phrase probabilities. Syntactic parses were available for only 14 of the 20 training stories ( 217 sentences , 4,230 words )</definiens>
				<definiens id="10">part-of-speech information , questions 1-8. Relative frequency of a `` break '' ( in the training data ) is indicated in each node for the subset of data associated with that node , and the left branch in a split is more likely to have a break. in the same constituent , words separated by a wh-noun phrase boundary , and words separated by a verb phrase initial boundary. ( In their work on spontaneous speech , Wang and Hirschberg found that noun phrases in general tended to be less likely to contain boundaries. ) The tree with syntactic information seemed to classify minor breaks with slightly higher</definiens>
				<definiens id="11">examines the ratio of current minor phrase length to previous minor phrase</definiens>
				<definiens id="12">designed using syntactic information but not the minor phrase length ratio test , questions 1-13. Relative frequency of a `` break '' ( in the training data ) is indicated in each node for the subset of data associated with that node , and the left branch in a split is more likely to have a break. the major phrase , in terms of the ratio of the number of words up to the current position over the total length of the major phrase. Classification rates on the training data</definiens>
				<definiens id="13">trees were used in the hierarchical model , and the phrase break prediction algorithm was evaluated on the independent test set described in Section</definiens>
			</definition>
</paper>

		<paper id="3007">
			<definition id="0">
				<sentence>More specifically , the a priori knowledge consists of a finite set of parameters , the values of which have to be fixed by the learner .</sentence>
				<definiendum id="0">priori knowledge</definiendum>
			</definition>
			<definition id="1">
				<sentence>The learning material consists of a lexicon that contains a substantial amount of the attested monomorphemic multisyllabic words of Dutch ( see Section 2.1 ) .</sentence>
				<definiendum id="0">learning material</definiendum>
			</definition>
			<definition id="2">
				<sentence>Antepenultimate stress , which is the dominant pattern in -VV-VC final words , can not be achieved if stress consistently falls within the final foot .</sentence>
				<definiendum id="0">Antepenultimate stress</definiendum>
				<definiens id="0">the dominant pattern in -VV-VC final words</definiens>
			</definition>
			<definition id="3">
				<sentence>IBL is a framework and methodology for incremental , supervised , similarity-based learning .</sentence>
				<definiendum id="0">IBL</definiendum>
			</definition>
			<definition id="4">
				<sentence>The distinguishing feature of IBL is the fact that no explicit abstractions are constructed on the basis of the training examples during the training phase .</sentence>
				<definiendum id="0">distinguishing feature of IBL</definiendum>
			</definition>
			<definition id="5">
				<sentence>tures ( geometrical distance between two patterns in pattern space ) , or an overlap metric for symbolic features ( number of features with equal values in both patterns ) , all features are interpreted as being equally important .</sentence>
				<definiendum id="0">tures</definiendum>
				<definiens id="0">geometrical distance between two patterns in pattern space ) , or an overlap metric for symbolic features</definiens>
			</definition>
			<definition id="6">
				<sentence>It is computed by the formula in ( 4 ) , where Pi ( probability of category i ) is estimated by its relative frequency in the training set .</sentence>
				<definiendum id="0">Pi</definiendum>
				<definiens id="0">( probability of category i ) is estimated by its relative frequency in the training set</definiens>
			</definition>
			<definition id="7">
				<sentence>The expression D\ [ f=v\ ] refers to those patterns in the database that have value v for feature f ; V is the set of possible values for feature f. H ( D~\ ] ) = ~ H ( D\ [ f=v , \ ] ) ID~ vdI ( 5 ) viEV Information gain is then obtained by equation ( 6 ) and scaled to be used as a weight for the feature during similarity matching .</sentence>
				<definiendum id="0">expression D\</definiendum>
				<definiendum id="1">V</definiendum>
			</definition>
			<definition id="8">
				<sentence>Output of the system consists of a prediction of the category ( FIN , ANT , or PEN ) of the input word .</sentence>
				<definiendum id="0">Output of the system</definiendum>
				<definiendum id="1">PEN</definiendum>
				<definiens id="0">consists of a prediction of the category ( FIN , ANT , or</definiens>
			</definition>
			<definition id="9">
				<sentence>In the following tables we select those types from Table 1 in which the regular pattern alternates with cases that need \ [ -ex\ ] , LF , or a combination of both .</sentence>
				<definiendum id="0">LF</definiendum>
				<definiens id="0">the regular pattern alternates with cases that need \ [ -ex\ ] ,</definiens>
			</definition>
			<definition id="10">
				<sentence>The memory of an Instance-Based Learning ( IBL ) system is a kind of table in which representations of words are associated with 13 Church ( 1992 ) , in a reaction to Dresher ( 1992 ) proposes lookup in a table of syllable weight strings ( associated with their stress string ) as an alternative approach .</sentence>
				<definiendum id="0">memory of an Instance-Based Learning</definiendum>
				<definiendum id="1">stress string )</definiendum>
				<definiens id="0">a kind of table in which representations of words</definiens>
			</definition>
</paper>

		<paper id="3005">
</paper>

		<paper id="4001">
			<definition id="0">
				<sentence>A component depends on a component to its right ( not necessarily the adjacent component ) , and the suffix ( postposition ) of a component indicates what kind of element it can depend on .</sentence>
				<definiendum id="0">component</definiendum>
				<definiens id="0">depends on a component to its right ( not necessarily the adjacent component ) , and the suffix ( postposition ) of a component indicates what kind of element it can depend on</definiens>
			</definition>
			<definition id="1">
				<sentence>In Japanese , bunsetsu is the smallest meaningful sequence consisting of an independent word ( IW ; nouns , verbs , adjectives , etc. ) and accompanying words ( AW ; copulas , postpositions , auxiliary verbs , and so on ) ~ A bunsetsu whose IW is a verb or an adjective , or whose AW is a copula , functions as a predicate and thus is called a predicative bunsetsu ( PB ) .</sentence>
				<definiendum id="0">bunsetsu</definiendum>
				<definiens id="0">the smallest meaningful sequence consisting of an independent word ( IW ; nouns , verbs , adjectives , etc. ) and accompanying words ( AW ; copulas , postpositions , auxiliary verbs , and so on ) ~ A bunsetsu whose IW is a verb or an adjective , or whose AW is a copula , functions as a predicate and thus is called a predicative bunsetsu ( PB )</definiens>
			</definition>
			<definition id="2">
				<sentence>Example : ( i ) ... KAISEKI ( analysis ) TO ( and ) SEISEI ( generation ) WO ... ( ii ) ... GEN-GENGO ( source language text ) NO ( of ) KAISEKI ( analysis ) TO ( and ) AITEGENGO ( target language text ) NO ( of ) SEISEI ( generation ) WO ... ( iii ) ... GEN-GENGO ( source language text ) WO KAISEKI-SURU ( analyzing ) SHORI ( processing ) TO ( and ) AITE-GENGO ( target language text ) WO SEISEI-SURU ( generating ) SHORI ( processing ) WO ... Conjunctive predicative clauses Words indicating conjunctive predicative clauses : ( b ) TOKA SHI OYOBI NARABINI ( and ) KA ARUIWA MATAWA MOSHIKUWA ( or ) GA NONI-TAISHI/TE / KEREDOMO ( but ) DAKEDEIWAINAKU ( not only .</sentence>
				<definiendum id="0">TOKA SHI OYOBI NARABINI</definiendum>
				<definiendum id="1">KA ARUIWA MATAWA MOSHIKUWA</definiendum>
				<definiens id="0">generation ) WO ... ( iii ) ... GEN-GENGO ( source language text ) WO KAISEKI-SURU ( analyzing ) SHORI ( processing ) TO ( and ) AITE-GENGO ( target language text ) WO SEISEI-SURU ( generating ) SHORI ( processing ) WO ... Conjunctive predicative clauses Words indicating conjunctive predicative clauses</definiens>
			</definition>
			<definition id="3">
				<sentence>That is , it is not easy to determine which bunsetsu to the left of a KB is the leftmost bunsetsu of the pre-conjunct ( we call this starting bunsetsu SB ) and which bunsetsu to the right of a KB is the rightmost bunsetsu of the post-conjunct ( this ending bunsetsu is called EB and is a post-head ) .</sentence>
				<definiendum id="0">KB</definiendum>
				<definiens id="0">the rightmost bunsetsu of the post-conjunct ( this ending bunsetsu is called EB and is a post-head )</definiens>
			</definition>
			<definition id="4">
				<sentence>In detecting a CS whose KB is the nth bunsetsu ( Bn ) , we consider only a partial matrix ( denoted An ) that is the upper right part of Bn ( Figure 2 ) : An = ( a ( i , j ) ) ( O &lt; i &lt; n ; n + l ~_ j &lt; _ l ) .</sentence>
				<definiendum id="0">CS whose KB</definiendum>
				<definiens id="0">An = ( a ( i , j ) ) ( O &lt; i &lt; n ; n + l ~_ j &lt; _ l )</definiens>
			</definition>
			<definition id="5">
				<sentence>~ ... ... ... ... ... ... ... ... ... ... ... &lt; ZZ~ . : = t ~ ! a ( pra_n , n+l ) C~ - , ,..~ ... ~ ( `` '' ) .k '' - '' i x~ `` ~ '' '' A path ' similarity value between sbl~_~ Bn ! .a ( n , m ) 4 ... ... .. `` r ... ... BnandBm. K &lt; ' 1 1 Figure 2 A method using a triangular matrix. `` '' '' Bi ... ... ... ... i -- -- ~ ( ~ `` '' '' , , , , . , . B j-I , i `` '' , , . ! lj'- , , `` ' , , ( . ) Bi-l.~ ... ... ... .. `` -. , , , , ,. = ~j-2 B j-1 ( b ) Bj. , `` B'/-I , . : ... ... ... ... ... ... ... i. -- -~ `` `` '' Bi : ... ... ... .. i ... .. B j-1. ( c ) Bj. Figure 3 The way of calculating path scores. correspondence of a KB to a CB ( note the first process giving the similarity between two bunsetsus ) . We calculate a path score , which shows the similarity between two candidate conjuncts specified by the path , using the following five criteria : . . Basically , the score of a path is the sum of each element 's points on the path. For example , the similarity score between phrase Bi-1 Bi and phrase By-1 By in Figure 3a can be expressed by a ( i 1 , j 1 ) ÷ a ( i , j ) . When candidate conjuncts have one-to-one bunsetsu correspondences , the path extends by jumping over rows one by one , and its score can be obtained simply by the previously described method. However , when an extra bunsetsu is inserted in one conjunct , like `` ... BOKU-NO ( my ) AKAI ( red ) PEN ( pen ) TO ( and ) KARE-NO ( his ) ENPITSU ( pencil ) ... . `` the path extends toward the left adjacent element ( i.e. , horizontally ) or extends by jumping over two or more rows. To handle a bunsetsu 513 Computational Linguistics Volume 20 , Number 4 ... ... ... ... ... ... ... ... ... ... ... . , ° , °°°°°°°. , ... . ... ... ... ... .. : it ... ... ... ... ... ... . ... .. ~ ... ... ... : ,_..~ ... ... ... ... ~ ... .. ... ... ... ... ... 3v+. ' , ... ~ , ... ... .. ~ ... .. `` i \\ i ( ~. i ' , \\ i ... .. } ... ... ... ... . o,1.~.. ; V.~ , °~ , .. , t ... .. ... .. r ... ... ... ... .. \ [ ... ... ... ... . i ... .. ~ ... ... ... . ° ... ... ... ... ... . °° ... ... . °°.o° , °°.°.~ Figure 4 Penalty points. . . insertion in pre-conjunct and that in post-conjunct symmetrically , when a part of the path is horizontal ( a ( i , j ) , a ( i , j1 ) ) , the element 's points a ( i , j 1 ) are not added to the path score ( Figures 3b and 3c ) . 1 Since a pair of conjunctive phrases/clauses often exhibit structural similarity , we hypothesize that analyses of CSs which maximize corresponding bunsetsus tend to lead to a correct resolution of the conjunctive scope. By this hypothesis , we impose penalty points on the pairs of elements in the path that cause one to multiple bunsetsu correspondence , giving priority to CSs that are constructed of components of the same size. Penalty points for ( a ( pi~j ) ~ a ( pi+l , j 1 ) ) calculated by the following formula are subtracted from the path score ( Figure 4 ) : JPi Pi+l 11 x 2. Note that these penalty points are also symmetrical , as shown in Figures 3b and 3c. Since each phrase in the CS has a certain coherence of meaning , special words that separate different meanings in a sentence often limit the scope of a CS. If candidate conjuncts specified by a path include such words , we impose penalty points on the path so that the possibility of selecting such a path is reduced. We define five separating levels ( SLs ) for bunsetsus , which express the strength of separating meanings in a sentence ( Table 2 ; see Table 1 ) , by observing sentences containing CSs. If candidate conjuncts contain a bunsetsu whose SL is equal to the KB 's SL or higher , we reduce the path score by ( SL of the bunsetsu KB 's SL + 1 ) x 7. definition fits the dynamic programming method described later , which calculates scores for partial paths column by column. 514 Sadao Kurohashi and Makoto Nagao Syntactic Analysis Method Table 2 Separating levels ( SLs ) Level Conditions for bunsetsu Being the KB of a conjunctive predicative clause , or accompanying the topic-marking postposition `` WA '' and a comma. Accompanying a case-marking postposition ( e.g. , `` GA , '' `` WO '' ) and a comma , or being an adverb accompanying a comma. Being the renyoh form of a predicate not accompanying a comma , or accompanying the topic-marking postposition `` WA. '' Being the KB of a conjunctive noun phrase accompanying a comma. Accompanying a comma , or being the KB of a conjunctive noun phrase not accompanying a comma. Table 3 Words for bonuses Conjunctive noun phrases Last AW NADO ( and so on ) Next IW KAKU ( each ) SHURUI ( sort ) ... TSU ( numeral ) KUMI ( set ) TSUI ( pair ) RYOUHOU ( both ) Conjunctive predicative clauses Last AW TAME-NI ( in order to ) TAME-NO ( in order to ) TO-IU ( as ) TO-ITTA ( as ) YOUDA ( like ) NADO ( and so on ) Next IW KOTO ( that ) MONO ( that ) TOKI ( when ) HOU-HOU ( way ) HOU-SHIKI ( way ) SHU-HOU ( way ) . However , two high SL bunsetsus corresponding to each other often exist in a CS , and these do not limit the scope of the CS , like `` X TO-SHITE WA ( As to X ) ... . DE-ARI ( be ) , Y TO-SHITE WA ( as to Y ) ... . DE-ARU ( be ) . '' To take this into consideration , penalty points for corresponding high SL bunsetsus are not given to paths. For high SL bunsetsus , Bi and By , to be corresponding , they have to be of the same type , and the path contains the element a ( i~j ) . We define two bunsetsus to be of the same type if : their IWs are of the same part of speech ; when they are conjugated , they have the identical form ; when they contain AWs , their AWs are identical. For example , `` KARE ( he ) -WA '' and `` KANOJO ( she ) -WA '' are of the same type ( noun + postposition `` WA '' ) . So are `` HASHIREBA ( ifrun ) '' and `` ARUKEBA ( ifwalk ) '' ( conditional form of verb ) . These penalty points can be imposed on pairs of elements in a path , namely , extension steps of a path separately because each extension step of a path takes some bunsetsus in candidate conjuncts. Some words frequently are the AW of the last bunsetsu in a CS or the IW following it. These words are shown in Table 3. Bonus points ( 6 points ) are given to paths that have the CS ending with one of the words in Table 3. 515 Computational Linguistics Volume 20 , Number 4 Now calculating - ' '' in this column. ~i '' ~ ' : q : : '' ~~ '' 1 i ' '' ~~ beit P~th'\ [ ~ ... .3. ... . , ... +.~.+ ... +.. , ~ ~ ... ... + -- , '-+ -- -+ -- -+-.-4 ... . 2-~-'4.-~-.-4 , ~ ... +_.+ ... ' ... L. ~ -- -- '' , -. -- 4 Choose i ' , '' , , , , the greatest.\ [ ... L~ , ~'. : '.'6..~ : ~b.. 4 : ~-.~ -- 4 -- -4 score path. i \ ] , @ -- ' , '- '' - '' -~'.d `` _. i ~4 i 0 i ... ... . ' ... ..t._K ... .~ ... .~ t. ne starting -- -- -- -. , -. -- - , -- . -- -- -. -- . -- -- -- - , element. Figure 5 The best path from an element. As described in the preceding subsection , a path score is composed of points for its elements , penalty points for every path extension , and bonus points for its starting position. The key aspect is that these points can be calculated for every extension step of a path independently. For this reason , the greatest score path can be searched for by using dynamic programming method. Calculation is performed column by column going left from a non-zero element in the lowest row in A , to the leftmost column in An. For each element in a column , the best partial path reaching it is found by extending the partial paths from the previous column and choosing the greatest score path ( the left part of Figure 5 ) . In extending partial paths , elements ' points and penalty points are given to paths step by step. Then , among the paths to the leftmost column , the path that has the greatest score becomes the best path from the starting non-zero element ( the right part of Figure 5 ) . Of all the best paths from all the non-zero lowest row elements , the path that has the greatest path score ( the maximum path ) is chosen as defining the scope of the CS ; i.e. , the series of bunsetsus on the left side of the maximum path ( pre-conjunct ) and the series of bunsetsus under it ( post-conjunct ) are conjunctive ( Figure 6 ) . An EB ( the last bunsetsu in the post-conjunct ) corresponds to a KB ( the last bunsetsu in the pre-conjunct ) , and it follows from the definition of a path that the EB has a certain similarity to the KB. On the other hand , when there are modifiers in both conjuncts , an SB shows where the leftmost modifier starts in its pre-conjunct. Since the modifiers in the pre-conjunct and those in the post-conjunct usually do not correspond exactly , an SB is determined mainly on the basis of the balance between preand post-conjuncts and is not always detected precisely. This problem is managed in the next stages when the relations between CSs in a sentence are adjusted and when a dependency structure is constructed ( described in Section 4.1 and Section 5.2 ) . Two examples of detecting CSs are shown in Figures 7 and 8. A chain of matrix elements with the same letters shows the maximum path for the KB marked with this letter and ' &gt; . '</sentence>
				<definiendum id="0">DE-ARI</definiendum>
				<definiendum id="1">DE-ARU</definiendum>
				<definiendum id="2">KARE</definiendum>
				<definiendum id="3">EB</definiendum>
				<definiens id="0">note the first process giving the similarity between two bunsetsus ) . We calculate a path score , which shows the similarity between two candidate conjuncts specified by the path</definiens>
				<definiens id="1">a certain coherence of meaning , special words that separate different meanings in a sentence often limit the scope of a CS. If candidate conjuncts specified by a path include such words</definiens>
				<definiens id="2">If candidate conjuncts contain a bunsetsu whose SL is equal to the KB 's SL or higher , we reduce the path score by ( SL of the bunsetsu KB 's SL + 1 ) x 7. definition fits the dynamic programming method described later , which calculates scores for partial paths column by column. 514 Sadao Kurohashi and Makoto Nagao Syntactic Analysis Method Table 2 Separating levels ( SLs ) Level Conditions for bunsetsu Being the KB of a conjunctive predicative clause</definiens>
			</definition>
			<definition id="6">
				<sentence>Parent-child relation ( cases A , B , C , D , E , G , H , M , and N in Figure 9 ) : Another actual relation between two CSs is a parent-child relation , in which a preor post-conjunct of a CS includes another CS .</sentence>
				<definiendum id="0">Parent-child relation</definiendum>
				<definiens id="0">cases A , B , C , D , E , G , H , M</definiens>
			</definition>
			<definition id="7">
				<sentence>At first , the CS \ [ HYOUDAI ( a title ) , \ ] -\ [ CHOSHA ( an author ) , \ ] \ [ SHUDAI-NADO-NO ( such as a theme ) \ ] is analyzed ; because each conjunct consists of only one bunsetsu , the analysis results only in creating into a dependency tree , as it contains no predicate that should become the root node of a dependencY tree .</sentence>
				<definiendum id="0">CS \ [ HYOUDAI</definiendum>
				<definiens id="0">a title ) , \ ] -\ [ CHOSHA ( an author ) , \ ] \ [ SHUDAI-NADO-NO ( such as a theme ) \ ] is analyzed ; because each conjunct consists of only one bunsetsu , the analysis results only in creating into a dependency tree , as it contains no predicate that should become the root node of a dependencY tree</definiens>
			</definition>
			<definition id="8">
				<sentence>NI ( an obstacle ) KOTO-MO ( that ) \ [ ARU .</sentence>
				<definiendum id="0">NI</definiendum>
			</definition>
			<definition id="9">
				<sentence>~P.WRA &lt; F-~ PARA ( sometimes be ) Of course , a n~ior part of the problem is to ascertain accurately what algorithm is necessary to check a certain phenomenctl , but the architecture of a computer is sometimes 4 help and sometimes au obstacle to its developmelaL Figure 15 An example of analyzing a long sentence into a dependency structure. remaining modifier also depends on the EB ( Figure 14c ) . The problem is to recover the omitted modifiers that depend on a bunsetsu in the post-conjunct except the EB. The key point is that Y and Y~ in Figure 14b have a great similarity because they '525 Computational Linguistics Volume 20 , Number 4 contain not only similar bunsetsus , KB and EB , but also very similar bunsetsus that originally governed the same modifier X. Therefore , we can detect the possibility of modifier ellipsis by checking the similarity score of the CS obtained when detecting its scope. When the extension operation is performed on the pre-conjunct of a CS that is a strong CS , we recover the omitted modifiers by interpreting a bunsetsu that depends on a bunsetsu ( Bi ) in its pre-conjunct as also depending on the bunsetsu ( By ) in its post-conjunct corresponding to Bi ( Figure 14d ) ( we think Bi corresponds to By when the path specifying these conjuncts contains an element a ( i~j ) ) . A CS that satisfies the following two conditions is called a strong CS : • The number of bunsetsus in its pre-conjunct ( nl ) and the number of bunsetsus in its post-conjunct ( n2 ) are about the same , satisfying the equation ( nl/1.3 ) &lt; n2 &lt; ( nl x 1.3 ) . • The score of the path specifying the CS is greater than ( nl + n2 ) x 4. For example , in the sentence in Figure 15 , the detected CS \ [ TASUKE-NI ( a help ) ... ARE-BA ( sometimes be ) , \ ] \ [ SAMATAGE-NI ( an obstacle ) ... ARU ( sometimes be ) .\ ] satisfies the above two conditions. Thus , by checking the relation between the CS and the outside modifier phrase `` SONO KAIHATSU-NO ( to its development ) '' the phrase is considered to depend on both of the bunsetsus `` TASUKE-NI ( a help ) '' and `` SAMATAGE-NI ( an obstacle ) . '' In the same way , `` COMPUTER-NO ARCHITECTURE-GA ( the architecture of a computer ) '' is again thought to depend on both the bunsetsu `` NARU ( be ) '' in the preconjunct and the bunsetsu `` NARU ( be ) '' in the post-conjunct. The dependency tree of this sentence that is supplemented correctly with the omitted modifiers is shown in Figure 15. Another type of ellipsis in CSs that is a serious problem is the omission of predicates in incomplete conjunctive structures. This type of ellipsis can be found by examining the failures of dependency analysis. The failure of dependency analysis here means that a head bunsetsu can not be found for a certain bunsetsu in a certain range of analysis. When two predicates in a conjunctive predicative clause are the same , the first predicate is sometimes omitted and the remaining part constitutes the incomplete conjunctive structure ( Figures 16a and 16b ) . In these structures , neither conjunct can be parsed into a dependency tree , because there is no predicate in it that should become the root node of a dependency tree. For this reason , by checking dependency analysis failures , we find incomplete conjunctive structures and start the process of supplementing the CSs with omitted predicates. The conditions for incomplete conjunctive structures are the following ( Figure 16c ) : • Dependency analysis failure occurs both in the preand post-conjuncts. • The bunsetsus whose heads can not be found ( called FB ) contain identical AWs. The key point is that it is important for successful analysis of CSs containing predicate ellipses to detect the correct scope of the incomplete conjunctive structures. In most cases their scopes can be detected correctly from a significant similarity between the 526 Sadao Kurohashi and Makoto Nagao Syntactic Analysis Method • • : :~ : : : : : ~ : : : , : : ::4 : : : : : :~ : : : : : : ( a ) A conjunctive predicative clause. Co ) An incomplete conjunctive s~ac~tre. Head can not be found. • `` ... . Identical AWs are contained. ... . `` , • -. } iiiiiiiii } i i ! ililiiiiii , ! i i l liiiiiiiiiii } ! iii ! iiil } iii ! l ( c ) Conditions for an incomplete conjunctive structure. ... •..1o. .°° ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ~ `` ''-. t L ! iii ! iiiii ! i ! i ! l Iiii ! iiiii ! i ! i ! J c -- n • ( d ) An incorrect conjunctive noun phrase. ( e ) A dependency tree of an incomplete conjunctive structure. I , , 11 , t ( f ) Recove~'ing the omitted predicate. Figure 16 A predicate ellipsis. preand post-conjuncts that contain the case components of the same predicate• That is , the detection of a CS based on the similarity measure smoothly leads to the omitted predicate being recovered• A method that merely searches for the EB as the most similar bunsetsu for the KB might detect an incorrect scope , and in this case the predicate ellipsis can not be detected , as shown in Figure 16d. When a CS is regarded as an incomplete conjunctive structure , each series of bunsetsus to the left of an FB is analyzed into a dependency tree , and its root node ( FB ) is connected to a CS node in addition to the KB and the EB ( Figure 16e ) • When the head of the CS node is found in the next level analysis , the head is considered to be the omitted predicate and the dependency tree is transformed by supplementing it with this predicate in the pre-conjunct , as shown in Figure 16f. When the postposition of 527 Computational Linguistics Volume 20 , Number 4 ZU-NI 0 5 2 5 2 0 2 2 2 2 2 2 0 ( 'm the figure ) SHIMESU-YOU-NI , 0 0 0 0 5 0 0 0 0 0 0 2 ( as is shown ) J / ( o~ , n~vo-G~-~ 2 5a 2 o 2 2 2 2 2 2 0 ( as current source ) ~ a &gt; PNP-TRANSISTOR , 2 12a0 1266 12 2 2 4 0 ( the pnp transistor ) ... .~ , ~ , __ , swrrca~G-~ 2 o 2 2b 2 2 2 2 0 ( as swltching ) .</sentence>
				<definiendum id="0">~P.WRA &lt; F-~ PARA</definiendum>
				<definiendum id="1">problem</definiendum>
				<definiendum id="2">CS</definiendum>
				<definiendum id="3">SONO KAIHATSU-NO</definiendum>
				<definiendum id="4">TASUKE-NI</definiendum>
				<definiendum id="5">COMPUTER-NO ARCHITECTURE-GA</definiendum>
				<definiendum id="6">NARU</definiendum>
				<definiendum id="7">NARU</definiendum>
				<definiendum id="8">EB</definiendum>
				<definiens id="0">a great similarity because they '525 Computational Linguistics Volume 20 , Number 4 contain not only similar bunsetsus , KB and EB , but also very similar bunsetsus that originally governed the same modifier X. Therefore , we can detect the possibility of modifier ellipsis by checking the similarity score of the CS obtained when detecting its scope. When the extension operation is performed on the pre-conjunct of a CS that is a strong CS , we recover the omitted modifiers by interpreting a bunsetsu that depends on a bunsetsu ( Bi ) in its pre-conjunct as also depending on the bunsetsu</definiens>
				<definiens id="1">score of the path specifying the CS is greater than ( nl + n2 ) x 4. For example , in the sentence in Figure 15 , the detected CS \ [ TASUKE-NI ( a help</definiens>
				<definiens id="2">a help ) '' and `` SAMATAGE-NI ( an obstacle )</definiens>
				<definiens id="3">the architecture of a computer</definiens>
				<definiens id="4">the omitted predicate. Figure 16 A predicate ellipsis. preand post-conjuncts that contain the case components of the same predicate• That is , the detection of a CS based on the similarity measure smoothly leads to the omitted predicate being recovered• A method that merely searches for the EB as the most similar bunsetsu for the KB might detect an incorrect scope</definiens>
			</definition>
			<definition id="10">
				<sentence>As a result , FB `` DENRYUGEN-NI ( as current source ) '' and FB `` SWITCHING-NI ( as switching ) '' are connected to the CS node in addition to the KB and EB .</sentence>
				<definiendum id="0">FB `` DENRYUGEN-NI</definiendum>
				<definiens id="0">current source ) '' and FB `` SWITCHING-NI ( as switching ) '' are connected to the CS node in addition to the KB and EB</definiens>
			</definition>
			<definition id="11">
				<sentence>Test sentences are longer and more complex than sentences in common 529 Computational Linguistics Volume 20 , Number 4 usage and consist of 50 sentences composed of 30 to 50 characters , 50 sentences of 50 to 80 characters , and 50 sentences of over 80 characters .</sentence>
				<definiendum id="0">Test sentences</definiendum>
				<definiens id="0">sentences composed of 30 to 50 characters , 50 sentences of 50 to 80 characters , and 50 sentences of over 80 characters</definiens>
			</definition>
			<definition id="12">
				<sentence>Considering these conditions and comparing results using our method with those using the conventional method , the total success ratio for determining correct dependency structures 531 Computational Linguistics Volume 20 , Number 4 Table 6 Examples of failure of analysis ( i ) JISSAI ( in fact ) , CHOSHA-TACHI-WA ( authors ) { \ [ KORE-WO ( it ) TSUKATTE ( using ) , JUURYOKU-SOUGO-SAYOU-GA ( gravitationally interacting ) SHIHAI-SURU ( governing ) \ ] TENTAI-NO ( astronomical ) UNDOU-NI-TSUITE ( about the motion ) , KOUSEIDO-DE ( highprecision ) KOUSOKU-NO ( high-speed ) SUUCHI-KEISAN-GA ( numerical computation ) DEKIRU ( can ) DIGITAL-ORRERY-TO-IU ( called Digital Orrery ) SENYOU-COMPUTER-WO ( special-purpose computer ) SEISAKU-SHITE-IRU ( create ) . }</sentence>
				<definiendum id="0">] TENTAI-NO</definiendum>
				<definiens id="0">numerical computation ) DEKIRU ( can ) DIGITAL-ORRERY-TO-IU ( called Digital Orrery ) SENYOU-COMPUTER-WO ( special-purpose computer ) SEISAKU-SHITE-IRU ( create )</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>Vijay-Shanker and Weir ( 1990 ) present a way of specifying any TAG as a linear indexed grammar .</sentence>
				<definiendum id="0">Vijay-Shanker</definiendum>
				<definiens id="0">present a way of specifying any TAG as a linear indexed grammar</definiens>
			</definition>
			<definition id="1">
				<sentence>The LIG version makes explicit the standard notion of derivation being presumed .</sentence>
				<definiendum id="0">LIG version</definiendum>
				<definiens id="0">makes explicit the standard notion of derivation being presumed</definiens>
			</definition>
			<definition id="2">
				<sentence>wj , which we take to be the empty string when i is greater than or equal to j. We will use p , A , and { 9 for sequences containing terminals and LIG nonterminals with their stack specifications .</sentence>
				<definiendum id="0">wj</definiendum>
				<definiens id="0">sequences containing terminals and LIG nonterminals with their stack specifications</definiens>
			</definition>
			<definition id="3">
				<sentence>A , i , j , k , l+l ) ( P\ [ r/\ ] -- , P • P ' \ [ r/lA , i , j , k , I ) ( P'\ [ r/'\ ] -- ~ • O , / , - , - , l ) Type 1 and 2 Completor : ( b\ [ rh\ ] -+ r • t\ [ r/\ ] A , m , j ' , k ' , i ) ( t\ [ rl\ ] -+ 0 • , i , j , k , l ) ( b\ [ rh\ ] -+ Pt\ [ r/\ ] • A , m , jUj ' , kUk ' , l ) • Type 3 Completor : • Type 4a Completor : a ~ Wl+l P ' \ [ ~/'\ ] ~ 0 ( t\ [ ~\ ] -- -+ •b\ [ ~\ ] , i , - , - , i ) ( b\ [ , \ ] -- + O , , i , j , k , l ) ( t\ [ ~\ ] -- -+b\ [ , \ ] • , i , j , k , l ) ( t\ [ r/\ ] -- + •t\ [ , r\ ] , i , - , - , i ) ( tier\ ] -- + 0 • , i , j , k , l ) ( bM -- + A• , j , p , q , k ) ( t\ [ ~\ ] -+ t\ [ ~r\ ] • , i , p , q , l ) • Type 4b Completor : ( b\ [ ~\ ] -+ •t\ [ , r\ ] , i , -- , -- , i ) ( t\ [ rlr \ ] -- + 0 • , i , j , k , l ) ( b\ [ , \ ] -- + A• , j , p , q , k ) ( b\ [ w\ ] -+ t\ [ Wr\ ] • , i , p , q , l ) • Type 5 Completor : ( b\ [ r/f\ ] -- + • b\ [ ~/\ ] , i , - , - , i ) ( b\ [ ~/\ ] -- + 0 • , i , j , k , I i ( bit/f\ ] -- + b\ [ r/\ ] • , i , i , l , l ) • Type 6 Completor : ( t\ [ r/\ ] -- + .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">j , k , l ) ( bM -- + A• , j , p , q</definiens>
			</definition>
			<definition id="4">
				<sentence>An operation is built with op ( t , D ) where t is a tree address and D is a derivation tree to be operated at that address .</sentence>
				<definiendum id="0">t</definiendum>
				<definiendum id="1">D</definiendum>
				<definiens id="0">a derivation tree to be operated at that address</definiens>
			</definition>
			<definition id="5">
				<sentence>, 0 , - , - , n , D ) , where D is the derivation developed during the parse .</sentence>
				<definiendum id="0">D</definiendum>
			</definition>
</paper>

		<paper id="4002">
			<definition id="0">
				<sentence>RAP applies to the syntactic structures of McCord 's ( 1990 , 1993 , in press ) Slot Grammar parser , and like the parser , it is implemented in Prolog .</sentence>
				<definiendum id="0">RAP</definiendum>
				<definiens id="0">applies to the syntactic structures of McCord 's ( 1990 , 1993 , in press ) Slot Grammar parser</definiens>
			</definition>
			<definition id="1">
				<sentence>RAP operates primarily on a clausal representation of the Slot Grammar analysis of the current sentence in a text ( McCord et al. 1992 ) .</sentence>
				<definiendum id="0">RAP</definiendum>
			</definition>
			<definition id="2">
				<sentence>The clausal representation consists of a set of Prolog unit clauses that provide information on the head-argument and head-adjunct relations of the phrase structure that the Slot Grammar assigns to a sentence ( phrase ) .</sentence>
				<definiendum id="0">clausal representation</definiendum>
			</definition>
			<definition id="3">
				<sentence>We will say that P is in the adjunct domain of N iff N is an argument of a head H , P is the object of a preposition PREP , and PREP is an adjunct of H. P is in the NP domain of N iff N is the determiner of a noun Q and ( i ) P is an argument of Q , or ( ii ) P is the object of a preposition PREP and PREP is an adjunct of Q. A phrase P is contained in a phrase Q iff ( i ) P is either an argument or an adjunct of Q , i.e. , P is immediately contained in Q , or ( ii ) P is immediately contained in some phrase R , and R is contained in Q. A pronoun P is non-coreferential with a ( non-reflexive or non-reciprocal ) noun phrase N if any of the following conditions hold : value of the algorithm .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">PREP</definiendum>
				<definiendum id="2">P</definiendum>
				<definiendum id="3">PREP</definiendum>
				<definiendum id="4">R</definiendum>
				<definiens id="0">the object of a preposition PREP</definiens>
				<definiens id="1">an argument of Q</definiens>
				<definiens id="2">an adjunct of Q. A phrase P is contained in a phrase Q iff ( i ) P is either an argument or an adjunct of Q , i.e. , P is immediately contained in Q , or ( ii ) P is immediately contained in some phrase R , and</definiens>
			</definition>
			<definition id="4">
				<sentence>P is an argument of a head H , N is not a pronoun , and N is contained in H. P is in the NP domain of N. P is a determiner of a noun Q , and N is contained in Q. Examples of coindexings that would be rejected by these conditions are given in Figure 1 .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">an argument of a head H</definiens>
			</definition>
			<definition id="5">
				<sentence>It is Modaladj that S It is Modaladj ( for NP ) to VP It is Cogv-ed that S It seems/appears/means/follows ( that ) S NP makes/finds it Modaladj ( for NP ) to VP It is time to VP It is thanks to NP that S analysis grammars .</sentence>
				<definiendum id="0">Modaladj</definiendum>
			</definition>
			<definition id="6">
				<sentence>3 following formulation of the binding algorithm is defined by the following hierarchy of argument slots : subj &gt; agent &gt; obj &gt; ( iobjlpobj ) Here subj is the surface subject slot , agent is the deep subject slot of a verb heading a passive VP , obj is the direct object slot , iobj is the indirect object slot , and pobj is the object of a PP complement of a verb , as in put NP on NP .</sentence>
				<definiendum id="0">obj</definiendum>
				<definiendum id="1">pobj</definiendum>
				<definiens id="0">the surface subject slot , agent is the deep subject slot of a verb heading a passive VP</definiens>
				<definiens id="1">the direct object slot , iobj is the indirect object slot , and</definiens>
			</definition>
			<definition id="7">
				<sentence>A is in the argument domain of N , and N fills a higher argument slot than A. A is in the adjunct domain of N. A is in the NP domain of N. N is an argument of a verb V , there is an NP Q in the argument domain or the adjunct domain of N such that Q has no noun determiner , and ( i ) A is an argument of Q , or ( ii ) A is an argument of a preposition PREP and PREP is an adjunct of Q. A is a determiner of a noun Q , and ( i ) Q is in the argument domain of N and N fills a higher argument slot than Q , or ( ii ) Q is in the adjunct domain of N. Examples of bindings licensed by these conditions are given in Figure 3 .</sentence>
				<definiendum id="0">PREP</definiendum>
				<definiens id="0">an argument of a verb V</definiens>
			</definition>
			<definition id="8">
				<sentence>Mary knows the people/who John introduced to each other/ .</sentence>
				<definiendum id="0">Mary</definiendum>
				<definiens id="0">knows the people/who John introduced to each other/</definiens>
			</definition>
			<definition id="9">
				<sentence>5 way as the `` equality '' condition of Discourse Representation Theory ( DRT ) ( Kamp 1981 ) , as in u -- -y. This indicates that the discourse referent u , evoked by an anaphoric NP , is anaphorically linked to a previously introduced discourse referent y. To avoid confusion with comparative relations among the factors as defined by the weights .</sentence>
				<definiendum id="0">Discourse Representation Theory ( DRT )</definiendum>
				<definiens id="0">in u -- -y. This indicates that the discourse referent u , evoked by an anaphoric NP , is anaphorically linked to a previously introduced discourse referent y. To avoid confusion with comparative relations among the factors as defined by the weights</definiens>
			</definition>
			<definition id="10">
				<sentence>Equivalence classes , along with the sentence recency factor and the salience degradation mechanism , constitute a dynamic system for computing the relative attentional prominence of denotational NPs in text .</sentence>
				<definiendum id="0">Equivalence classes</definiendum>
				<definiens id="0">constitute a dynamic system for computing the relative attentional prominence of denotational NPs in text</definiens>
			</definition>
			<definition id="11">
				<sentence>See Section 5.1 for a discussion of these 549 Computational Linguistics Volume 20 , Number 4 Table 3 Results of blind test Total Intersentential cases Intrasentential cases Number of pronoun occurrences 360 70 290 Number of cases that the algorithm 310 ( 86 % ) 52 ( 74 % ) 258 ( 89 % ) resolves correctly ( 1992 ) system , RAPSTAT , which employs both RAP 's salience weighting mechanism and statistically measured lexical preferences , as well as for a detailed analysis of the relative contributions of the various elements of RAP 's salience weighting mechanism to its overall success rate .</sentence>
				<definiendum id="0">RAPSTAT</definiendum>
				<definiens id="0">employs both RAP 's salience weighting mechanism and statistically measured lexical preferences</definiens>
			</definition>
			<definition id="12">
				<sentence>Dagan ( 1992 ) constructs a procedure , which he refers to as RAPSTAT , for using statistically measured lexical preference patterns to reevaluate RAP 's salience rankings of antecedent candidates .</sentence>
				<definiendum id="0">procedure</definiendum>
				<definiendum id="1">RAPSTAT</definiendum>
				<definiens id="0">for using statistically measured lexical preference patterns to reevaluate RAP 's salience rankings of antecedent candidates</definiens>
			</definition>
			<definition id="13">
				<sentence>RAPSTAT assigns a statistical score to each element of a candidate list that RAP generates ; this score is intended to provide a measure ( relative to a corpus ) of the preference that lexical semantic/pragmatic factors impose upon the candidate as a possible antecedent for a given pronoun , is 14 Such a distance measure is reminiscent of Hobbs ' ( 1978 ) tree search procedure .</sentence>
				<definiendum id="0">RAPSTAT</definiendum>
				<definiens id="0">assigns a statistical score to each element of a candidate list that RAP generates ; this score is intended to provide a measure ( relative to a corpus</definiens>
			</definition>
			<definition id="14">
				<sentence>The statistical score that RAPSTAT assigns to Ci is intended to model the probability of the event where Ci stands in the relevant grammatical relation to H , given the occurrence of Ci ( but taken independently of the other elements of L ) .</sentence>
				<definiendum id="0">statistical score</definiendum>
				<definiendum id="1">Ci</definiendum>
				<definiens id="0">intended to model the probability of the event where</definiens>
			</definition>
			<definition id="15">
				<sentence>In general , RAPSTAT is a conservative statistical extension of RAP .</sentence>
				<definiendum id="0">RAPSTAT</definiendum>
				<definiens id="0">a conservative statistical extension of RAP</definiens>
			</definition>
			<definition id="16">
				<sentence>By contrast , RAP uses a multi-dimensional measure of salience that invokes a variety of syntactic properties specified in terms of the head-argument structures of Slot Grammar , as well as a model of attentional state .</sentence>
				<definiendum id="0">RAP</definiendum>
				<definiens id="0">uses a multi-dimensional measure of salience that invokes a variety of syntactic properties specified in terms of the head-argument structures of Slot Grammar</definiens>
			</definition>
			<definition id="17">
				<sentence>The algorithm chooses as the antecedent of a pronoun P the first NPi in the tree obtained by left-to-right breadth-first traversal of the branches to the left of the path T such that ( i ) T is the path from the NP dominating P to the first NP or S dominating this NP , ( ii ) T contains an NP or S node N that contains the NP dominating P , and ( iii ) N does not contain NPi .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">contains an NP or S</definiens>
			</definition>
			<definition id="18">
				<sentence>Find a node N1 such that ( i ) N1 contains the pronoun P ; ( ii ) N1 is an S or NP ; and ( iii ) it is not the case that there is a node N1 , such that N1 contains N1 , and N1 , satisfies ( i ) and ( ii ) .</sentence>
				<definiendum id="0">N1</definiendum>
				<definiendum id="1">N1</definiendum>
				<definiens id="0">an S or NP</definiens>
			</definition>
			<definition id="19">
				<sentence>If an element E in the list of possible 22 `` A discourse segment consists of a sequence of utterances U1 , ... , Urn .</sentence>
				<definiendum id="0">discourse segment</definiendum>
				<definiens id="0">consists of a sequence of utterances U1 , ... , Urn</definiens>
			</definition>
			<definition id="20">
				<sentence>The backward center is a confirmation of an entity that has already been introduced into the discourse ; more specifically , it must be realized in the immediately preceding utterance , Un -- l '' ( Brennan , Friedman , and Pollard 1987 , p. 155 ) .</sentence>
				<definiendum id="0">backward center</definiendum>
				<definiens id="0">a confirmation of an entity that has already been introduced into the discourse ; more specifically , it must be realized in the immediately preceding utterance</definiens>
			</definition>
			<definition id="21">
				<sentence>The global ranking of an antecedent candidate is a function of the scores that it receives from each of the constraint source modules .</sentence>
				<definiendum id="0">global ranking of an antecedent candidate</definiendum>
				<definiens id="0">a function of the scores that it receives from each of the constraint source modules</definiens>
			</definition>
</paper>

		<paper id="2006">
			<definition id="0">
				<sentence>RAFT/RAPR maintains a set of data structures , and uses rules ( which take grammatical roles into account ) for pronoun resolution and computing the foci .</sentence>
				<definiendum id="0">RAFT/RAPR</definiendum>
				<definiens id="0">maintains a set of data structures , and uses rules ( which take grammatical roles into account ) for pronoun resolution and computing the foci</definiens>
			</definition>
			<definition id="1">
				<sentence>• Subject Focus ( SF ) : the surface subject of the clause , except in certain cases as mentioned later .</sentence>
				<definiendum id="0">SF )</definiendum>
				<definiens id="0">the surface subject of the clause</definiens>
			</definition>
			<definition id="2">
				<sentence>For a there-insertion sentence , the SF is the deep subject of the sentence , but for most simple sentence types the SF is the surface subject of the simple sentence .</sentence>
				<definiendum id="0">SF</definiendum>
				<definiendum id="1">SF</definiendum>
				<definiens id="0">the deep subject of the sentence</definiens>
				<definiens id="1">the surface subject of the simple sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>The type of realization of each element : Prefer NPs realized as pronouns over those realized with full NPs .</sentence>
				<definiendum id="0">type of realization</definiendum>
				<definiens id="0">of each element : Prefer NPs realized as pronouns over those realized with full NPs</definiens>
			</definition>
			<definition id="4">
				<sentence>A pronoun carries less semantic information than a full NP .</sentence>
				<definiendum id="0">pronoun</definiendum>
			</definition>
			<definition id="5">
				<sentence>Cb ( Un ) =Cb ( Un-1 ) Cb ( Un ) # Cb ( Un-1 ) Cb ( Un ) =Cp ( Un ) Continue Smooth-Shift Cb ( Un ) ¢Cp ( Un ) Retain ( Rough- ) Shift Volume 20 , Number 2 No-Cb : No element in Un realizes an element of Cf ( Un-l ) .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiens id="0">No element in Un realizes an element of Cf ( Un-l )</definiens>
			</definition>
			<definition id="6">
				<sentence>The centering literature has identified four kinds of transitions between sentences ( Walker , Iida , and Cote 1992 ) , where Un is the current utterance , and Un-1 is the previous utterance .</sentence>
				<definiendum id="0">Un</definiendum>
				<definiens id="0">the current utterance</definiens>
			</definition>
			<definition id="7">
				<sentence>SF=\ [ Lyn\ ] , CF=\ [ Susan\ ] ; 4 ) She wins a lot of trophies .</sentence>
				<definiendum id="0">She</definiendum>
				<definiens id="0">wins a lot of trophies</definiens>
			</definition>
			<definition id="8">
				<sentence>Walker ( 1989 ) indicated that centering needs to handle multiple subjects , but did not specify how to do that .</sentence>
				<definiendum id="0">centering</definiendum>
				<definiens id="0">needs to handle multiple subjects , but did not specify how to do that</definiens>
			</definition>
</paper>

	</volume>
