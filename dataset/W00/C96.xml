<?xml version="1.0" encoding="UTF-8"?>
	<volume id="C96">

		<paper id="2180">
			<definition id="0">
				<sentence>Here we ( : an see tile mapping of inforlnation between the user-defined tlS/t tag ( the attributes of which are listed in tim last line of this rule ) and the linguistic description ( 'ld ' -- 'linguistic description ' , a structured type within tile Typed Feature System ) , using the rule-internal variable TYPEvalue : the value of the attribute TYPE is assigned to the lexical unit ( 'lu ' ) value of the linguistic description .</sentence>
				<definiendum id="0">tlS/t tag</definiendum>
				<definiens id="0">a structured type within tile Typed Feature System</definiens>
			</definition>
</paper>

		<paper id="1097">
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>Desig NNP &gt; &lt; NNI ' L dcsig &gt; &lt; MM Num , Num &gt; &lt; NNP NNP &gt; ... `` VW '' &lt; Volkswagen DUP 2+ , I ) UP 5+ llow inane. , NA NA 100E , IIOS , 60J 70 E , 70 S , 43 J 520 E , 900 S , 570 J 56E , 19S , ! 9J |E , 1S , 0J 4E , 4S , 2J 0E , 100S,7KJ 21KE , 2IKS , 185KJ 20 E , 20 S , 2K J 220 E , 0 S , 0 J 44 E , 49 S , 54 J 210E , 210 S , 210 J 90 E , 95 S , 90 J 190E , 190S , 190J 17E , 18S , 70J 140E , 140S , 140J IE , |S , IJ 5E , 5S , 2J 425 morphological feature of importance. Determining keyword and key phrase features amounts to selecting prudent subject categories. These categories are associated with lists of lexical items or already existing features. For example , many of the statistically derived lexical features may fall under common subject categories. The words `` build '' , `` make '' , `` manufacture '' , and `` produce '' can be associated with the subject category `` make-type verbs '' . Analysis of the immediate context surrounding company names may lead to the discovery of key phrases like `` said it '' , `` entered a venture '' , and `` is located in '' . Table 1 shows a summary of various types of features used in system development. The longest common substring ( LCS ) feature ( Jacobs et al. , 1993 ) is useful for finding proper name aliases. The ID3 algorithm ( Quinlan , 1986 ) selects and organizes features into a discrimination tree , one tree for each type of name ( person , company , etc. ) . The tree , once built , typically contains 100+ nodes , each one inquiring about one feature in the text , within the locality of the current proper name of interest. An example of a tree which was generated for companies is shown in Figure 1. The context level for this example is 3 , meaning that the feature in question must occur within the region starting 3 words to the left of and ending 3 words to the right of the proper name 's left boundary. A `` ( L ) '' or `` ( R ) '' following the feature name indicates that the feature must occur to the left of or to the right of the proper name 's left boundary respectively. The numbers directly beneath a node of the tree represent the number of negative and positive examples present from the training set. These numbers are useful for associating a confidence level with each classification. Definitions for the features in Figure 1 ( and other abbreviations ) can be found in the appendix. The training set used for this example contains 1084 negative and 669 positive examples. To obtain the best initial split of the training set , the feature `` CN_alias '' is chosen. Recursively visiting and optimally splitting each concurrent subset results in the generation of 97 nodes ( not including leaf nodes ) . N N ... ... N 'N - ' P Figure 1. Company tree example ( context is +/3 ) . Figure 2 shows the working development system. The starting point is training text which has been pretagged with the locations of all proper names. The tokenizer separates punctuation from words. For non-token languages ( no spaces between words ) , it also separates contiguous characters into constituent words. The part of speech ( POS ) tagger ( Brill , 1992 ; Farwell et. al. , 1994 ; Matsumoto et al. , 1992 ) attaches parts of speech. Thc set of derived features is attached. During the delimitation phase , proper names are delimited using a set of POS-based handcoded tcmplates. Using ID3 , a dccision tree is generated based on the existing feature set and thc specified level of context to be considered. The generated tree is applied to test data and scored. Manual analysis of the tree and scored result leads to the discovery of new features. The new features are added to the tokenized training text , and the process repeats. In order to work with another language , the following resources are needed : ( 1 ) pre-tagged training text in the new language using same tags as belore , ( 2 ) a tokenizer for non-token languages , ( 3 ) a POS tagger ( plus translation of the tags to a standard POS convention ) , and ( 4 ) translation of designators and lexical ( list-based ) features. These language-specific modules are highlighted in Figure 2 with bold bordcrs. Feature translation occurs through the utilization of : on-line resources , dictionaries , atlases , bilingual speakers , etc. The remainder is constant across languages : a language independent core development system , and an optimally derived feature set for English. Also worth noting are the parts of development system that are executed by hand. These are shown shaded. Everything else is automatic. The system was first built for English and then ported to Spanish and Japanese. For English , the training text consisted of 50 messages obtained from the English Joint Ventures ( EJV ) domain MUC-5 corpus of the US Advanced Research Projects Agency ( ARPA ) . This data was hand-tagged with the locations of company names , person names , locations names , and dates. The test set consisted of 10 new messages. Experimental results were obtained by applying the generated trees to test texts. The initial raw text is tokenized and tagged with parts of speech. All features necessary to apply rules and trees are attached. Phrasal template rules are applied in order to delimit proper names. Then trees for each proper name type are applied individually to the proper names in the featurized text. Proper names which are 426 Figure 2. Multilingual development system. voted into more than one class are handled by choosing the highest priority class. Priorities are determined based on the independent perlormance of each tree. For example , if person trces perform better independently than location trees , then a person classification will be chosen over a location classification. Also , designators have a large impact on resolving conflicts. Various parameterizations were used for system development , including : ( 1 ) context depth , ( 2 ) feature set size , ( 3 ) training set size , and ( 4 ) incorporation of hand-coded phrasal templates. Figure 3 shows ttle performance results for English. The metrics used were recall ( R ) , precision ( P ) , and an averaging measure , P &amp; R , defined as : P &amp; R = 2*P*R/ ( P+R ) ( 2 ) Obtained results for English compare to the English results of Rau ( 1992 ) and McDonald ( 1993 ) . The weighted average of the P &amp; R for companies , persons , locations , and dates is 94.0 % . Ii RecaH • Precision companlos persons locations dates Figure 3. English performance results. The date grammar is rather small in comparison to other name classes , hence the performance for dates was perfect. Locations , by contrast , exhibited the lowest performance. This can be attributed mainly to : ( 1 ) locations are commonly associated with commas , which can create ambiguities with delimitation , and ( 2 ) locations made up a small percentage of all names in the training set , which could have resulted in overfitting of the built tree to the training data. Features strengths were measured for companies , persons , and locations. This experiment involved removing one feature at a time from the text used for testing and then reapplying the stone tree. Figure 4 and Table 2 show performance results ( P &amp; R ) when the three most powerful features are removed , one at a time , for companies , persons , and locations respectively. This experiment demonstrates tim power of designator features across all proper name types , and the importance of the alias feature for companies. 1 F1 F2 F3 F4 None Feature removed NCompanies I IPersons Locations j Figure 4. Feature strengths for English. `` Fable 2. Strongest features for English. Feature Companies FI CAP F2 CN desig F3 CN alias F4 Hyphen Persons Locations P_desig CAP CAP L desi [ j ATH_reg In F I L Re~ion Three experiments have been conducted for Spanish. In the first experiment , the English trees , generated 427 from the feature set optimized for English , are applied to the Spanish text ( E-E-S ) . In the second experiment , new Spanish-specific trees are generated from the feature set optimized for English and applied to the Spanish text ( S-E-S ) . The third experiment proceeds like the second , except that minor adjustments and additions are made to the t'eature set with the goal of improving performance ( S-S-S ) . The additional resources required for the first Spanish experiment ( E-E-S ) are a Spanish POS-tagger ( Farwell et al. , 1994 ) and also the translated feature set ( including POS ) optimally derived for English. The second and third Spanish experiments ( S-E-S , S-S-S ) require in addition pre-tagged Spanish training text using the same tags as for English. The obtained Spanish scores as compared to the scores from the initial English experiment ( E-E-E ) are shown in figure 5. 1 O,7 O,6 0,5 comparllas persons Iocatloos dato5 E-E-E ill E-E-S ! SES ~ : ' % Figure 5. P &amp; R scores lbr Spanish versus English. The additional Spanish specific features derived for S-S-S are shown in Table 3. Only a few new features added to the core feature set allows for significant pcrfommnce improvement. Table 3. Spanish specific features for S-S-S. Type Feature List Companies Ke~cword ( s ) Template Person Person Date Date Instances How man~ ' `` IBM '' , `` AT &amp; T ' , ... 100 `` del '' ( OF THE ) l &lt; FN DE LN &gt; 1 &lt; FN DE NNP &gt; 1 &lt; Num OF MM &gt; 1 &lt; Num OF MM OF Num &gt; l The same three experiments conducted lor Spanish are being conducted for Japanese .</sentence>
				<definiendum id="0">CN_alias</definiendum>
				<definiens id="0">features used in system development. The longest common substring ( LCS ) feature ( Jacobs et al. , 1993</definiens>
				<definiens id="1">3 , meaning that the feature in question must occur within the region starting 3 words to the left of and ending 3 words to the right of the proper name 's left boundary. A `` ( L ) '' or `` ( R ) '' following the feature name indicates that the feature must occur to the left of or to the right of the proper name 's left boundary respectively. The numbers directly beneath a node of the tree represent the number of negative and positive examples present from the training set. These numbers are useful for associating a confidence level with each classification. Definitions for the features in Figure 1 ( and other abbreviations</definiens>
				<definiens id="2">pretagged with the locations of all proper names. The tokenizer separates punctuation from words. For non-token languages ( no spaces between words ) , it also separates contiguous characters into constituent words. The part of speech ( POS ) tagger ( Brill , 1992 ; Farwell et. al. , 1994 ; Matsumoto et al. , 1992 ) attaches parts of speech. Thc set of derived features is attached. During the delimitation phase , proper names are delimited using a set of POS-based handcoded tcmplates. Using ID3 , a dccision tree is generated based on the existing feature set and thc specified level of context to be considered. The generated tree is applied to test data and scored. Manual analysis of the tree and scored result leads to the discovery of new features. The new features are added to the tokenized training text , and the process repeats. In order to work with another language , the following resources are needed : ( 1 ) pre-tagged training text in the new language using same tags as belore , ( 2 ) a tokenizer for non-token languages , ( 3 ) a POS tagger ( plus translation of the tags to a standard POS convention ) , and ( 4 ) translation of designators and lexical ( list-based ) features. These language-specific modules are highlighted in Figure 2 with bold bordcrs. Feature translation occurs through the utilization of : on-line resources , dictionaries , atlases , bilingual speakers</definiens>
				<definiens id="3">a language independent core development system</definiens>
				<definiens id="4">the training text consisted of 50 messages obtained from the English Joint Ventures ( EJV ) domain MUC-5 corpus of the US Advanced Research Projects Agency ( ARPA ) . This data was hand-tagged with the locations of company names , person names , locations names , and dates. The test set consisted of 10 new messages. Experimental results were obtained by applying the generated trees to test texts. The initial raw text is tokenized and tagged with parts of speech. All features necessary to apply rules and trees are attached. Phrasal template rules are applied in order to delimit proper names. Then trees for each proper name type are applied individually to the proper names in the featurized text. Proper names which are 426 Figure 2. Multilingual development system. voted into more than one class are handled by choosing the highest priority class. Priorities are determined based on the independent perlormance of each tree. For example</definiens>
				<definiens id="5">metrics used were recall ( R ) , precision ( P ) , and an averaging measure</definiens>
				<definiens id="6">features for S-S-S. Type Feature List Companies Ke~cword ( s ) Template Person Person Date Date Instances How man~ ' `` IBM ''</definiens>
			</definition>
</paper>

		<paper id="2195">
			<definition id="0">
				<sentence>The EDR Electronic Dictionary\ [ 1,2,31 is the result of a nine-year project ( from fiscal 1986 to fiscal 1994 ) , funded by the Japan Key Technology Center and eight computer manufacturers* aimed at establishing an infrastructure for advanced processing of natural language by computers and knowledge information processing .</sentence>
				<definiendum id="0">EDR Electronic</definiendum>
				<definiens id="0">funded by the Japan Key Technology Center and eight computer manufacturers* aimed at establishing an infrastructure for advanced processing of natural language by computers and knowledge information processing</definiens>
			</definition>
			<definition id="1">
				<sentence>The features of the EDR Electronic Dictionary can be summarized as follows : ( 1 ) A large scale that covers all the vocabulary used in ordinary writing ( 2 ) Aimed at general purpose applications without bias towards a particular application system or algorithm ( 3 ) Provided with the knowledge base required for true semantic analysis ( 4 ) A high degree of objectivity based on large volumes of text ( 5 ) Fundamental content that is highly generalized across different languages and fields The EDR Electronic Dictionary , which is composed of eleven sub-dictionaries , catalogues the lexical knowledge * Fujitsu , Ltd. , NEC Corporation , Hitachi , Ltd. , Sharp Corporation , Toshiba Corporation , Oki Electric Industry Co. , Ltd. , Mitsubishi Electric Corporation , and Matsushita Electric Industrial Co. , Ltd. of Japanese and English ( the Word Dictionary , the Bilingual Dictionary , and the Co-occurrence Dictionary ) , and has unified thesaurus-like concept classifications ( the Concept Dictionary ) with corpus databases ( the EDR Corpus ) .</sentence>
				<definiendum id="0">EDR Electronic Dictionary</definiendum>
				<definiendum id="1">EDR Electronic Dictionary</definiendum>
			</definition>
			<definition id="2">
				<sentence>The Concept Classification Dictionary , a subdictionary of the Concept Dictionary , describes the similarity relation among concepts listed in the Word Dictionary .</sentence>
				<definiendum id="0">Concept Classification Dictionary</definiendum>
				<definiens id="0">describes the similarity relation among concepts listed in the Word Dictionary</definiens>
			</definition>
			<definition id="3">
				<sentence>The EDR Corpus is the source for the information described in each of the sub-dictionaries .</sentence>
				<definiendum id="0">EDR Corpus</definiendum>
				<definiens id="0">the source for the information described in each of the sub-dictionaries</definiens>
			</definition>
			<definition id="4">
				<sentence>Technical Terminology Dictionary ( Information Processing ) Others ( Concept Classification , Bilingual Dictionary , Co-occurrence Data , etc. ) EDR Corpus t Japanese Corpus English Corpus The Japanese Word Dictionary contains 250,000 words , and the English Word Dictionary contains 190,000 words .</sentence>
				<definiendum id="0">Terminology Dictionary ( Information Processing ) Others</definiendum>
				<definiens id="0">contains 250,000 words , and the English Word Dictionary contains 190,000 words</definiens>
			</definition>
			<definition id="5">
				<sentence>The Bilingual Dictionary lists the correspondences between headwords in the different languages .</sentence>
				<definiendum id="0">Bilingual Dictionary</definiendum>
				<definiens id="0">lists the correspondences between headwords in the different languages</definiens>
			</definition>
			<definition id="6">
				<sentence>The JapaneseEnglish Bilingual Dictionary contains 230,000 words , and the English-Japanese Bilingual Dictionary contains 190,000 words .</sentence>
				<definiendum id="0">JapaneseEnglish Bilingual Dictionary</definiendum>
				<definiens id="0">contains 230,000 words , and the English-Japanese Bilingual Dictionary contains 190,000 words</definiens>
			</definition>
			<definition id="7">
				<sentence>The Concept Dictionary contains information on the 400,000 concepts listed in the Word Dictionary and is divided according to information type into the Headconcept Dictionary , the Concept Classification Dictionary , and the Concept Description Dictionary .</sentence>
				<definiendum id="0">Concept Dictionary</definiendum>
				<definiens id="0">contains information on the 400,000 concepts listed in the Word Dictionary and is divided according to information type into the Headconcept Dictionary , the Concept Classification Dictionary , and the Concept Description Dictionary</definiens>
			</definition>
			<definition id="8">
				<sentence>The Concept Description Dictionary describes the semantic ( binary ) relations , such as 'agent , ' 'implement , ' and 'place , ' between concepts that co-occur in a sentence .</sentence>
				<definiendum id="0">Concept Description Dictionary</definiendum>
				<definiens id="0">describes the semantic ( binary ) relations , such as 'agent , ' 'implement , ' and 'place , ' between concepts that co-occur in a sentence</definiens>
			</definition>
			<definition id="9">
				<sentence>The Technical Terminology Dictionary covers the field of infbrmation processing , attd is split into four types of dictionaries of Word , Bilingual , Concept ( Classification ) , and Co-occurrence .</sentence>
				<definiendum id="0">Technical Terminology Dictionary</definiendum>
				<definiens id="0">covers the field of infbrmation processing , attd is split into four types of dictionaries of Word , Bilingual , Concept ( Classification ) , and Co-occurrence</definiens>
			</definition>
			<definition id="10">
				<sentence>The Japanese Corpus contains 220,000 sentences , and the English Corpus contains 160,000 sentences .</sentence>
				<definiendum id="0">Corpus</definiendum>
				<definiens id="0">contains 220,000 sentences , and the English Corpus contains 160,000 sentences</definiens>
			</definition>
			<definition id="11">
				<sentence>The role of the Word Dictionary is to provide part of the information on the morphological , syntactic , and semantic revels that is requited for natulal language processing .</sentence>
				<definiendum id="0">Word Dictionary</definiendum>
				<definiens id="0">to provide part of the information on the morphological , syntactic</definiens>
			</definition>
			<definition id="12">
				<sentence>The headword information of the Bilingual Dictionary is a subset of the Word Dictionary , that is , headword notations , parts of speech , concept identifiers , headconcepts , and concept 1091 explications .</sentence>
				<definiendum id="0">Bilingual Dictionary</definiendum>
				<definiens id="0">a subset of the Word Dictionary , that is , headword notations , parts of speech , concept identifiers</definiens>
			</definition>
			<definition id="13">
				<sentence>The concept explication is an explanation which expresses the meaning of the concept .</sentence>
				<definiendum id="0">concept explication</definiendum>
				<definiens id="0">an explanation which expresses the meaning of the concept</definiens>
			</definition>
			<definition id="14">
				<sentence>The Concept Classification Dictionary contains the set of pairs of concepts that have super-sub ( is_a ) relation .</sentence>
				<definiendum id="0">Concept Classification Dictionary</definiendum>
				<definiens id="0">contains the set of pairs of concepts that have super-sub ( is_a ) relation</definiens>
			</definition>
</paper>

		<paper id="1001">
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>They satisfy X'-theory , which uses well-known syntactic concepts independent of any theoretical Damework .</sentence>
				<definiendum id="0">X'-theory</definiendum>
				<definiens id="0">uses well-known syntactic concepts independent of any theoretical Damework</definiens>
			</definition>
			<definition id="1">
				<sentence>A determiner is associated with a partial DRS , which is , so to speak , what remains of a DRS when one takes away a predicative DRS. As a consequence , there will be variables over predicative DRSs ( PDRS-variables ) in partial DRSs .</sentence>
				<definiendum id="0">determiner</definiendum>
				<definiens id="0">a partial DRS , which is , so to speak , what remains of a DRS when one takes away a predicative DRS. As a consequence</definiens>
			</definition>
			<definition id="2">
				<sentence>Synta ( : ti ( : l/rol ) erties of s ( , ntential negation ; intera ( : tions wit } l c ; Ise , agre ( mi ( mt , and ( in ) ( l ( ~liniteness in -Welsh and / { alssimi .</sentence>
				<definiendum id="0">Synta</definiendum>
				<definiens id="0">ti ( : l/rol ) erties of s ( , ntential negation</definiens>
			</definition>
</paper>

		<paper id="2176">
</paper>

		<paper id="2100">
			<definition id="0">
				<sentence>Bigrams are items ( i.e. word forms ) that occur frequently together in a specific order .</sentence>
				<definiendum id="0">Bigrams</definiendum>
				<definiens id="0">items ( i.e. word forms ) that occur frequently together in a specific order</definiens>
			</definition>
			<definition id="1">
				<sentence>In the following p ( x ) will denote the observed probability as defined by p ( x ) =F ( x ) /N where F ( x ) is the frequency of occurrence of x , and N is the number of observed cases .</sentence>
				<definiendum id="0">following p ( x ) will denote the observed probability</definiendum>
				<definiendum id="1">p ( x ) =F ( x</definiendum>
				<definiendum id="2">F ( x )</definiendum>
				<definiendum id="3">N</definiendum>
				<definiens id="0">the frequency of occurrence of x , and</definiens>
				<definiens id="1">the number of observed cases</definiens>
			</definition>
			<definition id="2">
				<sentence>SUSANNE : A Domesday Book of English Grannnar .</sentence>
				<definiendum id="0">SUSANNE</definiendum>
			</definition>
</paper>

		<paper id="2160">
			<definition id="0">
				<sentence>H is the head and Bo , '' ' , B , ~ are literals in the body of a clause .</sentence>
				<definiendum id="0">H</definiendum>
				<definiens id="0">the head</definiens>
			</definition>
			<definition id="1">
				<sentence>\ [ ) e list \ ] ) \ [ ( 2 31\ [ 8\ ] \ ] ( \ [ q\ [ , , } ) ) a , 'g2 arg , ( f : ~t\ [ \ [ 4\ ] ) \ [ \ [ 7\ ] gl'ga ( \ [ 3Jl\ [ ~ } ) goals arg2 4 i\ [ r\ ] re'g3 I\ [ r ) \ [ e-list \ ] ) next-steps / , u~J 4 \ [ , &lt; i.~t \ ] goals / arg2 5 b ... . xt-steps goals &lt; ) \ ] I\ [ r\ ] \ [ , ! -li , ~t \ ] ) \ ] Figure 2 : An examl ) le of DCP 's execution daughter , such as signs tagged \ [ 1\ ] and \ [ 2\ ] in Figure 3. Kasper c.t al. 1 ) resented an idea similar to this @ line raising in their work on HPSG-TAG compiler ( Kasper et al. , 1995 ) . The difference , is that our algorithm is based ou substitution , not adjoining , Furthermore , it is not clear in their work how offline raising is used to improve ef\ [ icicncy of parsing. Before giving the definition of LAs , we detine the notion of a quasi-sign , which is part of a sign and constitutes l~As. Definition 3 ( quasi-sign ( n ) ) For a given integer n , a fcatu , e structure S is a q'aasi-sign ( n ) if it has some of tile following four attributes : syn , sem , head-dtr , non-head-dtr and does not /Lave values for the paths ( head-dtr + non-head-dtr ) '' '' . A qua , .si-sign ( 'n ) can not rel ) resent a parse tree whose height is inore than n , while a sign can express a parse tree with any height. Tlm ) ugh the rest of this 1 ) aper , we often extract a quasi-sig '' n. ( n ) S from a sign or a quasi-sig ' , , ( , n/ ) S ' where '. , &lt; n'. This operation is denote ( l by S ' = c'x ( S ' , ,n ) . This means that 5 ' is equivMent to S ' except ff ) r the attributes head-dtr mM non-head-dtr whose root is the ( head-dtr + non-head-dtr ) '~ value in S'. Note that S and S ' are completely different entities. In other words , S and S ' pose different scopes on structure sharing tags , in addition , we also extract a feature structure F reached by a path or an attribute 1 ) in a feature structure IP'. We denote this by F = val ( F ' , p ) and regard F and F ' as different entities. Definition 4 ( Lexical Entry Auton , aton ( LA ) ) A Lezical Entry Automaton is a tuplc ( Q , A , qo } whel'e~ Q : a set of states , where a .state is a quasi-sign ( O ) . A : a , set of transition arcs between states , where a transition arc is a tuple ( qd , q ... . N , D , R ) where qd , q , . 6 Q , N is a quasi-sign ( O ) , D is a quasi-sign ( I ) and R is a rule schema. qo : tile initial state , which corresponds to a lezical erLtry. In a transition : -tt ' ( ; &lt; qd , q ... .. N , D , 1~ } , q , ~ denotes the destination of the transition arc , and qd is the root of the arc. The N is a non-head daughter of a l ) hrasal-sign , i.e. , the destination state of the transition , and expresses the input condition for the transition. The D is used to represe , nt : the dependency 1 ) etween the nn ) ther sign and the daughters through structure sharings. This is called a Dependency Feature Strueture ( DFS ) of the transition arc , the role of which will be discussed in Section 4. 1~ , is the rule schema used to create this arc. An LA is generated fl'om a lexieal entry l by the following recursive pro ( : edure : ea ( : h resolution sequence ( rl , ... , 'r , ~ } obtain , 1 ) \ [ head-dtr , Sd \ ] uf.s ( l~ ) u r , u .-. o r , ~ and if l ) is a feature structure , obtain s , , = ex ( D , O ) and N = ex ( w~l ( D , non-head-dtr ) , 0 ) . a. If D is a t~ature structure , • If the , re is a state s~ , ~ 6 S such that s ' , ~ , s ... . 4 let s , ~ be s~ , ~. Otherwise , add s , ,~ to 5 '' . * If there is no T'r = \'/~ '' d , '~ , ,~ '' ' , N '' , D '' , 1~ ) A such that . % ~ ~ s { ' , ~ , s , z ~ sSl , N 4For ~my feature structures f ~md f ' , f ~ f ' iff f E f'~md f ' E f 951 Phase2-proc-dcp ( e : edge ) ; assume e = ( 1 , r , S , Dep ) return S U sub-structure ( e ) sub-structure ( e : edge ) ; assmne e = ( l , r , S , Dep ) If Dep = 4 ) then return sub ( S ) , else for each ( D , eh , e~ , R ) C Dep , assume that el~ = ( lh , rh , Sh , Deph ) and e~ : ( In , r~ , Sn , Dep , ~ ) Sh : = sub-structure ( eh ) , S , ~ : = Sn U sub-strueture ( e~ ) If neither of Sh and Sn is nil , s % bo : ~ fv ( dep ( D ) u sub ( fs ( R ) ) , \ [ head-dtr Sh \ ] non-head-dtr S n ' rs ) ... ... ... ... ... ... ... ... ... ... . ( A ) for each resolution sequence , rd , sub : = .sub0 LIT 1 I~ • • • U Ti ... ... ... ... ... ... ... ... ... ... . ( B ) If sub is not a feature structure or either of Sh or S~ is nil , then return nil else return sub Figure 4 : A recursive procedure for tile Phase 2 N '' and D ~ D '' , then , add the tuple ( s , t , s , ,~ , N , D , R ) to A. S in the previous step , let sd be s , ~ and go to Step 2. When this terminates , ( S , A , l ) is the LA for 1. The major difference of Step 2 and the normal application of a rule schema is that non-head-dtr values are not specified in Step 2. In spite of this underspecification , certain parts of the non-head-dtr are instantiated because they are token-identicM with certain values of the head-d % r domain. By unifying non-head-dtr values with actual signs to be constructed fl'om input sentences , a parser can obtain parsing results. For more intuitive explanation , see ( Torisawa and Tsujii , 1996 ) . However , this simple LA generation algorithm has a termination problem. There are two potential causes of non-termination. The first is the generative capacity of a feature structure of a rule schema , i.e. , a rule schema can generate infinite variety of signs. The second is non-termination of the execution of DCP in Step 2 because of lack of concrete non-head daughters. For the first case , consider a rule schema with the following feature structure. head-dtr syn \ [ counter \ [ 1\ ] \ ] \ ] Then , this can generate an infinite sequence of signs , each of which contains a part , \ [ counter &lt; bar , ba , r , ... , bar ) l and is not equivalent to any previously generated sign. In order to resolve this difficulty , we apply tim restriction ( Shieber , 1985 ) to a rule schemata and a lexical entry , and split the feature structure F = fs ( R ) of a rule schema R or a lexical entry F = l , into two , namely , core ( F ) and sub ( F ) such that F = core ( F ) U sub ( F ) . The definition of the restriction here is given as follows. Definition 5 ( paths ) For arty node n in a feature structure F , paths ( n , F ) is a set of all the paths that reaches n from the root of F. Definition 6 ( Restriction Schema ) A restriction schema rs is a set of paths. Definition 7 ( Res ) F ' = Res ( F , rs ) is a ma.~ ; ireal feature structure such that each node n in F ~ satisfies the following conditions. • The~ is a node no in f : such that paths ( no , F ) = path.s ( n , F ' ) and type ( 'n ) = t ? tpe ( no ) . • For any p C paths ( 'n , F ' ) , there is no path p , , 6 rs which prefixes p. Res eliminates the feature structure nodes which is specified by a restriction schema. For a certMn given restriction schema rs , eore ( fs ( l~ , ) ) -= Res ( fs ( R ) , rs ) and sub ( fs ( R ) ) is a minimM feature structure such that core ( fs ( R ) ) U sub ( fs ( R ) ) = fs ( R ) . Tile nodes eliminated by Res must appear in sub ( fs ( R ) ) . In tile example , if we add ( syn , counter } to a restriction schema and replace fs ( R ) with eorc ( fs ( .R ) ) in the Mgorithm for generating LAs , the termination problenl does not occur because LAs can contain a loop and equivMent signs are reduced to one state in LAs. The sub ( fs ( R ) ) contains the synlcounter , and the value is treated at Phase 2. The other problem , i.e. , termination of DCPs , often occurs because of underspecification of the nork-head-dtr wines. Consider the rule schema in Figure 1. The append does not terminate at Phase 2 because the indices value of non-head ( laughters is \ [ ± \ ] . ( Consider the case of executing append ( X , ( b ) , Y ) in Prolog. ) We introduce the .freeze Nnctor in Prolog which delays the evaluation of the second argument of the functors if the first arguruent is not instantiated. For instance , freeze ( X , append ( X , \ [ b\ ] , Z ) ) means to delay the ewfluation of append until X is instantinted. We introduce the functor in the following forln. goals arg2 ( fl arg3 \ [ ~ freeze \ ] This means the resolution of this query is not performed if \ [ 1\ ] is \ [ ±\ ] . The delayed evaluation is considered later when tile non-head-dtr values are instantiated by an actual sign. Note that this change does not affect the discussion on the correctness of our parsing method , because the difference can be seen as only changes of order of unification. Now , tile two phases of our parsing algorithm can be described in more detail. Phase 1 : Enumerate possible parses or edges in a chart only with unifiability checking in a bottom-up chart-parsing like manner. 952 Phase 2 : For comt ) leted parse trees , compute sub-structures by DFSs , , sub ( fs ( R ) ) for each schema R and frozen 1 ) C1 ) programs. Note that , in \ [ 'has ( ; 1 , unification is replaced with nnifiability checking , which is more efficient than unification in terlns of space an ( l time. The intended side effect by unification , such as building up logical forms in sere values , is COmlntted at Phase 2 only for the parse trees covering the whole input. a.1 Phase 1 Parsing The Phase~ 1 parsing algorithm is quite similar to a bottom-up chart parsing for CFG. The Mgorithm has a chart and edges. Definition 8 ( edge ) An edge is a tupla ( 1 , r , S , l ) ep ) where , • 1 and r arc. vertexes in the chart. • S is a slate of an LA. • .l ) ep i.s a .set of tuples in the form of ( D , eh , c , , , ll } wh , e , rc. eh a7 % d Cn aTY ; ( : dges , \ ] ) i.s a quasi-.sign ( I ) and R is a rule .schema. The intuition behind this definition is , • £ ' l ) lays the role of a non-/termimd in CFG , though it is actually a quasi-sign ( O ) . • ch and e , ~ denote a head daughter edge and a non-head daughter edge , respectively. • Dep represents the dependency of an edge and its daughter edges. Where ( D , eh , c , ~ , l~ } E Dcp , D is a DIeS of a transition arc. Basi ( : ally , Phase 1 parsing creates these tuples , and \ ] ) hase 2 parsing uses them. The Phase 1 parsing ( : onsists of the folh ) wing steps. Assume that a word in input \ ] n~s a lexical entry L~ and that an LA ( Q , ; , A , ,q~ ) generated fi'om Li is attached to the word : chart for each Li , for at ) propriate .ji. pick u t ) an edge e2 which is adjacent to el and whose state is q~. N is unifiable with q2. edge ( l = ( 'm , d , 'n , d , q , Depd ) strictly covering el and e2. ( m , , , 'na , q , Dep , z U { ( D , c , ,eu , B ) } ) it ) the. chart. a new edge ( Tn , n , q , { ( D , el , e2 , R ) } ) strictly covering el and e2. The algorithnl of Phase 2 parsing is given in Figure 4. The procedure sub-.structure is a recursive 1 ) rocedure which takes an edge as input and builds Ul ) sub-structures , which is differ'ential feature structures representing modifications to core-structures , in a bottoln-U 1 ) nlanner. The obtained sub-structures are unified with corestructures when 1 ) the input edge covers a whole input or 2 ) the edge is a non-head daughter edge of sonm other edge. Note that the .~ub-struet'are treats sub ( fs ( R ) ) , a feature structure eliminated l ) y the restriction in the generation of LAs , ( the ( A ) 1 ) art in Figure 4 ) and frozen goals of DCPs , by additional ewduation of DCPs. ( the ( B ) part ) Here , we use two techniques : ( ) tie is dependency analysis which is eml ) odied by the function dep in Figure 4. The other is a partiM unification routine expressed by p_nnify in the figure. The del ) endency analysis is represented with the function , dep ( F , 'rs ) , where F is a DFS and rs is a restriction schema used in generation of LAs : Definition 9 ( dep ) For a feature structure \ [ `` ' and the. restriction schema r.s , F = dep ( l c~ , r , s ) is a maximal fc.atu're~ structure such O~ , at any 'node 'n in F sati , ~fies the conjunction of th , e. following two conditions : t. There is a node n ' in f i '' , such , that v ( tm.+ , . , P ) ~ ) , ,m. , + , , ' , F ' ) a , .Z t : ,mc. ( 7,0 : = typc ( n ' ) . of n , pa , ths ( n , z , F ) contains a path. prefixed by one of ( head-dtr ) , ( non-head-dtr ) and &lt; goa : ts &gt; .</sentence>
				<definiendum id="0">F</definiendum>
				<definiendum id="1">transition arc</definiendum>
				<definiendum id="2">N</definiendum>
				<definiendum id="3">D</definiendum>
				<definiendum id="4">R</definiendum>
				<definiendum id="5">re</definiendum>
				<definiendum id="6">definition</definiendum>
				<definiendum id="7">F )</definiendum>
				<definiendum id="8">R</definiendum>
				<definiendum id="9">F</definiendum>
				<definiens id="0">An examl ) le of DCP 's execution daughter</definiens>
				<definiens id="1">clear in their work how offline raising is used to improve ef\ [ icicncy of parsing. Before giving the definition of LAs , we detine the notion of a quasi-sign , which is part of a sign and constitutes l~As. Definition 3 ( quasi-sign ( n ) ) For a given integer n , a fcatu , e structure S is a q'aasi-sign ( n ) if it has some of tile following four attributes : syn , sem , head-dtr , non-head-dtr and does not /Lave values for the paths ( head-dtr + non-head-dtr ) '' ''</definiens>
				<definiens id="2">the ( head-dtr + non-head-dtr ) '~ value in S'. Note that S and S ' are completely different entities. In other words</definiens>
				<definiens id="3">Lexical Entry Auton , aton ( LA ) ) A Lezical Entry Automaton is a tuplc ( Q , A , qo } whel'e~ Q : a set of states , where a .state is a quasi-sign ( O ) . A : a , set of transition arcs between states , where a</definiens>
				<definiens id="4">a rule schema. qo : tile initial state</definiens>
				<definiens id="5">the destination of the transition arc , and qd is the root of the arc. The N is a non-head daughter of a l ) hrasal-sign , i.e. , the destination state of the transition , and expresses the input condition for the transition. The D is used to represe , nt : the dependency 1 ) etween the nn ) ther sign and the daughters through structure sharings. This is called a Dependency Feature Strueture ( DFS ) of the transition arc , the role of which will be discussed in Section 4. 1~ , is the rule schema used to create this arc. An LA is generated fl'om a lexieal entry l by the following recursive pro</definiens>
				<definiens id="6">r , S , Dep ) return S U sub-structure ( e ) sub-structure ( e : edge ) ; assmne e = ( l , r , S , Dep ) If Dep = 4 ) then return sub ( S ) , else for each ( D , eh , e~ , R ) C Dep , assume that el~ = ( lh , rh , Sh , Deph ) and e~ : ( In , r~ , Sn , Dep , ~ ) Sh : = sub-structure ( eh ) , S , ~ : = Sn U sub-strueture</definiens>
				<definiens id="7">add the tuple ( s , t , s , ,~ , N , D , R ) to A. S in the previous step , let sd be s</definiens>
				<definiens id="8">the generative capacity of a feature structure of a rule schema</definiens>
				<definiens id="9">contains a part</definiens>
				<definiens id="10">a lexical entry , and split the feature structure F = fs ( R ) of a rule schema R or a lexical entry F = l , into two</definiens>
				<definiens id="11">of the restriction here is given as follows. Definition 5 ( paths ) For arty node n in a feature structure F , paths ( n ,</definiens>
				<definiens id="12">a set of all the paths that reaches n from the root of F. Definition 6 ( Restriction Schema</definiens>
				<definiens id="13">a node no in f : such that paths ( no , F ) = path.s ( n , F ' ) and type ( 'n ) = t ? tpe ( no ) . • For any p C paths ( 'n , F ' ) , there is no path p , , 6 rs which prefixes p. Res eliminates the feature structure nodes which is specified by a restriction schema. For a certMn given restriction schema rs , eore ( fs ( l~ , ) ) -= Res ( fs ( R ) , rs ) and sub ( fs ( R ) ) is a minimM feature structure such that core ( fs ( R ) ) U sub ( fs ( R ) ) = fs ( R ) . Tile nodes eliminated by Res must appear in sub ( fs ( R ) )</definiens>
				<definiens id="14">a restriction schema and replace fs ( R ) with eorc ( fs ( .R ) ) in the Mgorithm for generating LAs , the termination problenl does not occur because LAs can contain a loop and equivMent signs are reduced to one state in LAs. The sub ( fs ( R ) ) contains the synlcounter</definiens>
				<definiens id="15">Enumerate possible parses or edges in a chart only with unifiability checking in a bottom-up chart-parsing like manner. 952 Phase 2 : For comt ) leted parse trees , compute sub-structures by DFSs , , sub ( fs ( R ) ) for each schema R and frozen 1 ) C1 ) programs. Note that , in \</definiens>
				<definiens id="16">more efficient than unification in terlns of space an ( l time. The intended side effect by unification , such as building up logical forms in sere values</definiens>
				<definiens id="17">a chart and edges. Definition 8 ( edge ) An edge is a tupla ( 1 , r , S , l ) ep ) where , • 1 and r arc. vertexes in the chart. • S is a slate of an LA. • .l ) ep i.s a .set of tuples in the form of ( D , eh , c , , , ll } wh , e</definiens>
				<definiens id="18">a rule .schema. The intuition behind this definition is , • £ ' l ) lays the role of a non-/termimd in CFG , though it is actually a quasi-sign ( O ) . • ch and e , ~ denote a head daughter edge and a non-head daughter edge , respectively. • Dep represents the dependency of an edge and its daughter edges. Where ( D , eh , c , ~ , l~ } E Dcp , D is a DIeS of a transition arc. Basi ( : ally</definiens>
				<definiens id="19">onsists of the folh ) wing steps. Assume that a word in input \ ] n~s a lexical entry L~ and that an LA ( Q , ; , A , ,q~ ) generated fi'om Li is attached to the word : chart for each Li , for at ) propriate .ji. pick u t ) an edge e2 which is adjacent to el and whose state is q~. N is unifiable with q2. edge ( l = ( 'm , d , 'n , d , q , Depd ) strictly covering el and e2. ( m , , , 'na , q , Dep , z U { ( D , c , ,eu , B ) } ) it ) the. chart. a new edge ( Tn , n , q , { ( D , el , e2 , R ) } ) strictly covering el</definiens>
				<definiens id="20">a recursive 1 ) rocedure which takes an edge as input and builds Ul ) sub-structures , which is differ'ential feature structures representing modifications to core-structures</definiens>
				<definiens id="21">a non-head daughter edge of sonm other edge. Note that the .~ub-struet'are treats sub ( fs ( R ) ) , a feature structure eliminated l ) y the restriction in the generation of LAs</definiens>
				<definiens id="22">the ( B ) part</definiens>
				<definiens id="23">= typc ( n ' ) . of n , pa , ths ( n , z , F ) contains a path. prefixed by one of ( head-dtr ) , ( non-head-dtr ) and &lt; goa : ts &gt;</definiens>
			</definition>
			<definition id="2">
				<sentence>s ( n , ~ , F ) which prefixes p. Roughly , dep eliminates 1 ) the descendant nodes of the node which apl ) ears both in syn/sem domains and head-dtr/non-head-dtr domains and 2 ) the nodes at ) peering only in syn/sem domains , excet ) t for the node which el ) pears in s'ab ( fs ( \ ] ¢ ) ) or goals domains .</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">prefixes p. Roughly , dep eliminates 1 ) the descendant nodes of the node which apl ) ears both in syn/sem domains and head-dtr/non-head-dtr domains and 2 ) the nodes at ) peering only in syn/sem domains , excet ) t for the node which el</definiens>
			</definition>
			<definition id="3">
				<sentence>953 phon `` wrote '' syn , orv , \ ] ... . ... . subcat &lt; NI'\ [ 1\ ] , NP\ [ 2\ ] ) rein wrote sere content agent object indices 0 Figure 5 : A lexical entry for `` wrote '' $ 2 ? A State Pb T : N T2 : N S1 A Transition Arc TI : NP ( N denotes L a non-head-dtr. ) Figure 6 : The LA derived from `` wrote '' n has a counter part in F~. More precisely , it produces the unification results for a nod ( ; n in Fj such that • there is a path p ~ paths ( n , I~ ) such that the node reached by 1 ) is also defined in F2 , or • there is a path p ~ paths ( n , F1 ) prefixed by some p , , C rs or ( goals ) . Note that a node is unified if its structureshared part has a counter-I ) art in F2. Intuitively , the routing produces unified results for the part of Fi instantiated by /7'2. The other part , that is not produced by p_unify , is not required at Phase 2 because it is already computed in a state or DFSs in LAs when the LAs are generated. Then , a sign can be obtained by unifying a sub-structure and the corresponding core-structure. This section describes the parsing process of the sentence `` My colleague wrote a good paper. '' The LA generated fronl the lexical entry for `` wrote '' in Figure 5 is given in Figure 6. The transition arc T1 between the states L and S1 is generated by the rule schema in Figure 1. Note thai ; the query to DCP , freeze ( \ [ 1\ ] , append ( Ill , \ [ 2\ ] , \ [ 3\ ] ) ) , is used to obtain union of indices values of daughters and the result is written to the indices values of the mother sign. During the generation of the transition arc , since the first argument of the query is \ [ ± \ ] , it is frozen. The core-structures arid the dependencyanalyzed DFSs that augment the LA are shown in Figure 7. We assume that we do not use any restriction , i.e. , for any lexical entry l and rule schenaata 2~ , s , bb ( 1 ) ~-\ [ ±1 and sub ( fs ( I { ) ) = \ [ ±1. Note that , in the DFSs , the already raised feature structures are eliminated and , that the DFS of the transition arc T contains the frozen query as the goals. Assmne that the noun phrases `` My colleague '' and ' % good paper '' are already recognized by a parser. At phase 1 , they are checked if they are unifiable to the condition of transition arcs T1 and T2 , i.e. , the NPs which are non-head daughters $ 2 syn senl 5'1 syn head \ [ ... .. ior V \ ] \ ] subcat 0 \ ] rehl wrote content agent ± object ± indices ± head \ [ major V \ ] \ ] subcat ( NP\ [ 2\ ] ) rein wrote senl content agent \ ] object indices A_ The dependency-anMyzed DFS of T2 syn \ [ ~ ... .. l \ [ s\ ] \ ] seln indices \ [ a\ ] synSign 1 \ ] ... . \ ] h~ad-dtr \ [ a\ ] s~beat .\ [ qN ; '\ [ r\ ] ) Z \ [ } t flllajor ... .. syn subcttt ) non-head-dtr \ [ 5\ ] sere indices goals argl arg2 arg2 The dependency-analyzed DFS of T1 head 8\ ] syn subcat { /10\ ] ±\ [ 9\ ] } \ ] content \ [ 6\ ] agent sere object indices \ [ 3\ ] • sign sy 1 ... .. I-dtr \ [ 3\ ] 9\ ] ) ... .. \ [ ic ' : :~itceet : t \ [ 2 t ( , \ ] syn \ [ t subcat ) non-head-dtr \ [ 5\ ] sere indices 1 ... .. \ ] &gt; freeze 1 goals argl a , -~2 ( ) arg2 Nll\ ] N , I\ ] Figure 7 : States and DFSs in tim LA in Figure 6 The sub-structure for $ 2 content | agen~ 1\ ] my_colleaque | ... .. \ [ obje , :t \ [ 2\ ] good_p .</sentence>
				<definiendum id="0">State Pb T</definiendum>
				<definiendum id="1">sub ( fs</definiendum>
				<definiens id="0">the already raised feature structures are eliminated and , that the DFS of the transition arc T contains the frozen query as the goals. Assmne that the noun phrases</definiens>
				<definiens id="1">rehl wrote content agent ± object ± indices ± head \ [ major V \ ] \ ] subcat ( NP\ [ 2\ ] ) rein wrote senl content agent \ ] object indices A_ The dependency-anMyzed DFS of T2 syn \ [ ~ ... .. l \ [ s\ ] \ ] seln indices \ [ a\ ] synSign 1 \ ] ... . \ ] h~ad-dtr \ [ a\ ] s~beat .\ [ qN ; '\ [ r\ ] ) Z \ [ } t flllajor ... .. syn subcttt ) non-head-dtr \ [ 5\ ] sere indices goals argl arg2 arg2 The dependency-analyzed DFS of T1 head 8\ ] syn subcat { /10\ ] ±\ [ 9\ ] } \ ] content \ [ 6\ ] agent sere object indices</definiens>
			</definition>
</paper>

		<paper id="1077">
			<definition id="0">
				<sentence>Etficient interpreters exist for the notation , but until now it has not been clear how to compile to equivalent automata in a transparent way .</sentence>
				<definiendum id="0">Etficient interpreters</definiendum>
				<definiens id="0">exist for the notation , but until now it has not been clear how to compile to equivalent automata in a transparent way</definiens>
			</definition>
			<definition id="1">
				<sentence>IntrosL denotes the set of strings in L into which elements of S may be arbitrarily inserted , and SUBA , ,~L denotes the set of strings in L ill which substrings that are in /3 may be replaced by strings from A. Both operators map regular languages into regular languages , because they can be , t : haract ( !</sentence>
				<definiendum id="0">IntrosL</definiendum>
				<definiendum id="1">SUBA , ,~L</definiendum>
				<definiens id="0">the set of strings in L into which elements of S may be arbitrarily inserted</definiens>
				<definiens id="1">the set of strings in L ill which substrings that are in /3 may be replaced by strings from A. Both operators map regular languages into regular languages</definiens>
			</definition>
			<definition id="2">
				<sentence>SC where I , LC ( lel't l ( , xi , :al corlt , ,~t ) = ( ~ , ... , 1N &gt; LEX ( lexical form ) = &lt; q , ... , oN ) RLC ( right lexical context ) = ( r , , ... , rN ) LSC ( left surface context ) : : &lt; IN+r , ... ,1N+M ) SUll\ ] '' ( surfac , c forlll ) == &lt; c NI.I , ... , CN+M &gt; II , S ( ?</sentence>
				<definiendum id="0">oN ) RLC</definiendum>
				<definiendum id="1">... , rN ) LSC</definiendum>
				<definiens id="0">lel't l ( , xi , :al corlt , ,~t ) = ( ~ , ... , 1N &gt; LEX ( lexical form ) = &lt; q , ... ,</definiens>
			</definition>
			<definition id="3">
				<sentence>Pk ) : this is ( 2 ) with the restriction j = i + 1 ; since SC rules can only apply to the partitions P/ , epenthetic rules such as ( ~* @ , ~i ) , e X E.~ , Z~ X ( t , @ , k ) S* ) ( 'insert an a between lexical and surface ks ' ) can not be enforced : the rule would disallow adjacent ( k , k ) s only if they were separated by an empty partition : ... ( k , k ) , e , ( k , k ) ... would be disallowed , but ... ( k , k ) , ( k , k } ... would be accepted .</sentence>
				<definiendum id="0">Pk )</definiendum>
				<definiendum id="1">Z~ X</definiendum>
				<definiens id="0">an empty partition : ... ( k , k ) , e , ( k , k ) ... would be disallowed , but ... ( k , k ) , ( k , k } ... would be accepted</definiens>
			</definition>
			<definition id="4">
				<sentence>The tbllowing rules illustrate the formalism : V B * = &gt; RI : V b * B B * = &gt; R2 : b b * c d ¢ &gt; R3 : c b d R1 and R2 illustrate the iterative application of rules on strings : they sanction the lexical-surface strings ( VBBB , Vbbb ) , where the second ( B , b ) pair serves as the centre of the first application of R2 and as the left context of the second application of the same rule .</sentence>
				<definiendum id="0">tbllowing rules</definiendum>
				<definiens id="0">V B * = &gt; RI : V b * B B * = &gt; R2 : b b * c d ¢ &gt; R3 : c b d R1 and R2 illustrate the iterative application of rules on strings : they sanction the lexical-surface strings</definiens>
			</definition>
			<definition id="5">
				<sentence>Uninstantiated contexts become Intro { o } ( E~ ) x Intro { o } ( E~ ) ( 13 ) The right context of R3 , for instance , becomes Intro { o } ( dS~ ) x Intro { o } ( dE~ ) ( \ ] 4 ) The algorithm consists of three phases : ( 1 ) constructing a FSA which accepts the centres , ( 2 ) applying CR rules , and ( 3 ) \ [ brcing SC constraints .</sentence>
				<definiendum id="0">Uninstantiated contexts</definiendum>
			</definition>
</paper>

		<paper id="2173">
</paper>

		<paper id="2187">
			<definition id="0">
				<sentence>This paper describes GATE , a General Architecture for Text Engineering , which is a freely-available system designed to help alleviate the problem .</sentence>
				<definiendum id="0">GATE</definiendum>
				<definiens id="0">a General Architecture for Text Engineering , which is a freely-available system designed to help alleviate the problem</definiens>
			</definition>
			<definition id="1">
				<sentence>1057 GATE is an architecture in the sense that it provides a common infrastructure for building language engineering ( LE ) systems .</sentence>
				<definiendum id="0">GATE</definiendum>
				<definiens id="0">an architecture in the sense that it provides a common infrastructure for building language engineering ( LE ) systems</definiens>
			</definition>
			<definition id="2">
				<sentence>GATE comprises three principal elements : • a database for storing information about texts and a database schema based on an object-oriented model of information about texts ( the GATE Document Manager GDM ) ; • a graphical interface for launching processing tools on data and viewing and evaluating the results ( the GATE Graphical Interface GGI ) ; • a collection of wrappers for algorithmic and data resources that interoperate with the database and interface and constitute a Collection of REusable Objects for Language EngineeringCREOLE .</sentence>
				<definiendum id="0">GATE</definiendum>
				<definiens id="0">comprises three principal elements : • a database for storing information about texts and a database schema based on an object-oriented model of information about texts ( the GATE Document Manager GDM ) ; • a graphical interface for launching processing tools on data and viewing and evaluating the results ( the GATE Graphical Interface GGI ) ; • a collection of wrappers for algorithmic and data resources that interoperate with the database and interface and constitute a Collection of REusable Objects for Language EngineeringCREOLE</definiens>
			</definition>
</paper>

		<paper id="2161">
			<definition id="0">
				<sentence>Viewpoints are features that distinguish a. node from other nodes in the thesaurus , and are good ( : lues for estimating the area to which an unknown wor &lt; l should be assign ( d. The area can be efficiently estimate ( / by extracting viewpoints. Several systems have used Wor &lt; lNet a.nd statls : tieal infi ) rmation from large corpora \ [ 31 5 , 6\ ] . Howeve.r , there are two common problems : noisy cooccurrence of words a , nd data sparseness. In WordNet , since each node it , the thesaurus is a set of words that haw~ synonym relationships ( SynSet ) ~ wtrious methods for similarity cah'ulatlon using the SynSet classes have been proposed. In this t ) aper~ ISAMAP \ [ 8\ ] , a hand-crafted Japanese thesaurus , is used as a ( : ore. To overcome the problems of noise 1Tha.t is , unknown words do not lne~\ [ \ [ very |ow-frequency words. ~A Jttpa.nese word in ISAMAP is represented by a pair of capital Rom~tn letters and the word 's English tr~mslatlon. 956 J*.-~/J ( Physical Object ) J3 , \ [ ~ ( Phenomenon ) 4'f , ( , ? , ; tu &lt; ~ ( Creature ) I'N. { ~ ( Relation ) : hllN~ ( Abstract Object ) II , ~/ : ( Time ) ) J ; ? ) ? ( Method ) Jg-/i ) i ( Location ) ~rigOJ ( Action ) ~'2111J ( Space ) ) , '~'~'\ [ : . ( Attribute ) q'i.l'~ ; ( Unit ) ~fL'~ ( State ) @ 'f '' l~ ( Operation ) ) 3 ( Force ) Fig. 1 : '1 ' ( ) 1 ) Categ ( , 'ies , ) \ [ `` ISAMAI ' and ( la , t ; a si ) arse.ess , relati ( mshil ) s ofc ( m imcl , ed , odes in the 1.hesa.urus are used. t { , 's , ik \ [ ) r ( qmsed a. classtmse ( I a , pproach , in which sets o\ [ `` words are .sed insl : ead of words \ [ 5\ ] . ht his apl &gt; r , ) a &lt; Jt , each bynset is used as a , class. In our apl ) roach ~ ( , n tim other ha , n ( \ [ , an a , r , 'm , l , ha.l ; coul ; a.ins ( 'otlrm ( : t , , ( l no ( los iu 1.ho thesa , tlrus is use ( l as a class. The .odes are on. ne ( 'ted by IS=A relatio.ships as well as syn ( mym rela , i , ionshil ) s ~ and theref'ore large areas rel ) resenl , strong similarities to unknow , words. This sectlo. ( lescrihes the thesaurus and stal islical da.l ; a used in l.his pa.lmr. A Jalm.U ( , se uoult t ; h ( 'salJrus ca.lh~. &lt; l 1SAMAI ' is a. set &lt; ) f ' IS=A r , qa.lio , shil , s. 1l contains at ) out 4,000 nou.s wil , h a.lu ) ut tel , h , vels. Each node of \ [ SAMAI ) is a. woM or a woM a.l , l it , s ( one or two ) synonyms. Figure 1 shows the t , o 1 ) categories of ISAMAP. Some words are 1 ) la , :ed al. mult , iph~ l &gt; osil , i ( ms iu l , he thesa , rus , tg ) , '' , &gt; xantlde , SENSUIKAN ( sulmla .</sentence>
				<definiendum id="0">Viewpoints</definiendum>
				<definiendum id="1">SynSet</definiendum>
				<definiendum id="2">coul ; a.ins</definiendum>
				<definiens id="0">a set of words that haw~ synonym relationships ( SynSet ) ~ wtrious methods for similarity cah'ulatlon using the</definiens>
			</definition>
			<definition id="1">
				<sentence>The procedure consists ( ,1 '' 1he following three steps : Step l : I ' ; xtraclh ) n of viewl ) ohgs I'or each node in ISAMAP .</sentence>
				<definiendum id="0">procedure</definiendum>
			</definition>
			<definition id="2">
				<sentence>Step 2 : I &lt; xtra , ction or ( 'andidat , ,~ areas f.r the input , WOF ( \ [ . Step 3 : F , wLhm.tion ol '' the ca , ,ldldates and selection &lt; ) f ' the rooM. preferabh+ area+. The I ) asic idea. is very simple. For a , unk.ow. wor ( l~ I , \ [ le word l , o-.word rela.l.ions\ [ lips l.ha.L ( ~oltl ; a , in it , are exl.racte ( l. 'l'h , , similaril ; y between the word a.nd each , , , ) de in ISAMAP is calculated. The nodes for w hich tim similarity exc ( mds a predefi ned l , h reshold ar ( ' Illal'k ( ~ ( t a , u ( 1 cOIIllCCl ; ( 'd ill the l ; } H~Sallrtl.S. '\ [ '\ [ m left tree in Figure 3 shows nodes in 1.he t.hesa.urus. 'l'h ( , ma.rke , I nodes are represented I ) y hlack circh's. For st raightfi ) rwa.rd statistical similarity caJcula.l.i ( ms , there are ma W similar words , inclu ( tiltg , ( ) isy words. \ [ n this l ) aimr ) the followi , g three hytmtheses a , re used to resolw~ l , he probhmt. First , the marked words \ [ } ) rm cerl ; aiu areas ( connect ( ~d nodes ) &lt; ) fwords in tlmt.hesaurus. Tim areas tha.l , occupy a large sl~ace are preferred. The right tree in Figure 3 ~ ; \ [ IOWS a , i'c~aS of words. Se ( : o , t &lt; l , specific words , that is to sa , y~ words a , t h ) wer hwels of trees a , re preferred. In Figure 3 , areal is pro.ferre ( l to area2. Third , ea.ch node in the thesaurus has viewpoi'nts that distinguish it : from ol.her nodes. The viewlmi.ts fi ) r ca , el , node are ext , ract , ed 1 ) y using case aml modilication relati ( mships t ; i~a , t contain sta , tis1 , teal data extracted \ [ 'rom the corpus , lfa , n unknowll word has the sa , me viewpoints as a certain , ode , t , \ [ le simila , ril , y tbr 1 , he .ode is weighted. 'rh ( ~ next sub secti ( m ( h~s ( : ril ) es how viewlmints axe exl , ra , cted. A viewpoi.t is a set of disl , i.guishing M , ,tures \ [ '.r each node i. a thesaurus. The viewpoint of a no ( le 957 Ma , rkcr GA we HE NI DE TO NA Total l ) istinct lillilll ) er 394.887 483,400 18,564 451,986 225,247 176,738 78,079 ,51,001 1,879,902 Total illltnber 817,030 1,2101581 53,876 1,114,877 61,4619 570,475 569,837 881,25,5 5,832,550 Relatioiiship Subject ( e.g. liia.n go ) Object ( e.g. drink coffee ) Goal ( e.g. go to offme ) Goal , etc. ( e.g. go to church ) histruinent , etc. ( e.g. hit with } laniiner ) Accompanier ( e.g. ili~ll and wonia.n ) a ( InonllnalizMion ( e.g. basic word ) Adnomlna , liza.tion ( e.g. large t ) uilding ) Fig. 2 : Numl ) er of Sta.tistical l ) atlt node is defined ~s a. list , O~od &lt; , marker , word ) . '\ [ 'hough Stl ( : h features are implicitly used in the creation of most existing thesauruses according to hilRiaJi in-tuition , they axe lost wheii the constructe ( l t\ ] ~le-sara'uses are used. An exception is the Wor ( 1Net~ in which the distinguishing \ [ ea.tures a.re nlP~nually listed. In tliis l ) aper~ the distinguishing fea.tllres aa'e extracted automaticMly , reflecting the characteristics of the corpus to be used. For example , Figure ,5 shows a f ) a.rt of ISAMAP. The viewpoint of a. node in the the.sa.urus is estimated by using a certa.in l ) rocedure. Suppose we want to extract the viewpoint o\ [ ' the noun `` HF , I { IKOPUTAA '' ( helicopter ) . Tile wor ( t occurs 131 1.iines in our corpus. Figure 3.2 shows exa.mples of the rela_ tlonshil ) s. For each rela.tionship~ a sea.tell is nlade h ) r nodes that have the sa.me relationship. In tile c~se of the pattern `` TUKAU '' ( use ) , 385 nodes with the s~mm relationship ~tre extra.cted Ijrom a.reas , scattered throughout ISAMAP. On the other ha.nd~ the pattern `` TOBU '' ( fly ) shares only two nodes , h.elicopter aaid aiwlaue. The nodes have direct ISA relationships ; in other words , the nodes are c.an be connected in the hierareDy of nodes. Since the viewpoints of a node are inherited by its children in many cases , the existence of the connected nodes that include ISA relationships is strong evidence for the viewpoints. In this case. , ( fly , SUB ) is a viewpoint for the node `` airpla'n % `` which is the topmost of the connected nodes. Viewpoints are extracted by calculating th.c typicMness of word-to-wo , ¥t relationsh @ s. Given a node nd a'nd its candidate viewpoi.n.t ( a pair of a relation marker rel and a wo , d w ) , the typicalness of the viewpoint is calculated as typicalness ( nd , tel , w ) _ ( E &lt; ~ o.. ( ... . ~ , '~ ) E. , , o &lt; , ( ... ... . ~ , &lt; : ) `` ° : ' t o. ( . , .. , ,- , ' ' where N is a set of no &amp; s i'n \ ] b'AMAP , and C is a set of conuected no &amp; s that contain the : word w. Examples o i the vie'tvpoi , nts ( whose typicalness exceeds 0.5. ) are as follows : I flying vehicle land vehicle water vehicle / / iic ( ~ket balloon / % a ear train coach s I air plane helicopter cargo ship patrol boat Fig. 5 : Exanlple of Viewpoints the Thesaurus Node ( word ) airplane Viewpoints ( fly , SUBJ ) , ( land , SUBJ ) , ( take off , SUBJ ) rocket ( la/in ( 'h , OB3 ) ship ( come alongside the pi~ , r , SUB J ) , ( sink , SUB J ) land vehicle ( transportation , by ) in ISAMAP Let us consider ~n exainph~ to see how Mgorithm works. Suppose the word `` SEN'I'OUKI '' ( fighter a ) is to be placed in the thesaurus. First ; , for each node in the 1SAMAP , the slmil~rity between the word and the node is cMculated. The similarity is ca.lculated according to the rollowing formula. : st 're ( w1 , w~ ) = rnax ( simi , sire2 ) ( o , . ( w , , , . , v ) o , , ( w~ , , _ ' , _s , ) '~ st. , , , , -_ E , , + , ,~p o , , ( _ , `` , v ) ) ( o , ~ ( v_~ , y , 2 ' : ) + o , ,0 , , , , , y~ ) \ ] W si71~ 2 ~ ' , . o~0 , , , . , _ ) o4v , ,. , - ) i pC= P 1 ) is set of words that co-occurs with wiorw2 , and the argument `` _ '' can be any words. If the simib~rity va.lue exceeds a pre-delined threshold , the node is marked. Figure 6 shows marked nodes that h~ve high similarity. aIn English , a fighter meltns 1 ) oth a t ) lane and a , person ; however , the original Jetpturese word SENTOUKI means only a pla , ne. 958 Word M , t , rker ~ W , , , ,l 1 TUKAU ( to ! , ,~e ) ~:0 -I to i : ! yAATA ( J , ( t ... . \ ] w°-.-lSIMA ( , ,nTciVislaT { , l 7 l ) 1 &lt; ! ~ 5 \ [ HUI { UMU ( to contain ) _J W ( ) \ [ 2 \ [ K~VtTI ; JY- ( ) ~-.o~-NI \ ] . ' } J I ( ANPAN ( oll dock ) ~_ 1 ) 1 &lt; __L~_j2 Fig. 4 : l , xa , inl ) le ( if ' llel~tlonshil , s for `` h ( qicopt ( , r. '' 5 6 7 9 io 13 Word \ [ IITO ( hu ma.n ) KABU ( stock ) SE IH IN ( ) } m , n ufa , ( 't t , ) ' ( : ) MONO ( object ) K IN ( gold ) a Y UUTAK U ( house ) ____ GIJYU'I'tlSYA ( engineer ) KIG YOIJ ( compa.IG ) BUH IN ipa , rts ) SET UB I ( fa , cilides ) HeN ( 1 ) ( iok ) TAI ( pa.rty ) \ [ KO UI &lt; U U ) &lt; I 0d , . pla. , le ) HEIKI ( weapon ) N ( ; , h ; : :kl t ( , ~i ; ~tidt~li \ [ 1 ; .~ } nL ( l , protect purch~Ls ( ' , ~ h~Lve : buy purcha , s % ha~v % buy purcha.se , h ; Ge , buy \ ] ) llrch~so , \ [ ia , ve , re } ) l ) UeCh~se , have , buy I.~.0.0.14 ha , v % tit ! y , prot ( ~ct l ) urchasc , buy , ( 'Xl ) ort l ) urcha , se , buy , ha , ve l ) tlr ( 'll~tSo : ha.v ( ~ \ ] ) ~ly sc ) ! d , da.i } g ( ; r , collision l ) } ! rcha , s ( ' , , fly , buy Fig. 6 : Marked ! redes will ! ma.tctmd ! ' ( da.tiouships ( ~h ) uma / engineer ( &lt; : ) r~omployee flying vehicle ~-helicopter `` , air plane Hayer ( d ) ~man vehicle ~bicycle -- woman `` bus ( b ) ( d ) weapon ~nuclear weapon food -confoctionoD/ k missi ) o Fig. 7 : Cat , didati , cii , ,oc ( , io , s for `` light ( , r '' Areas tha , t co ) d ; a , i ) ! ma , rked ! moles ~r ( : ( m , hmla , ti~d. The results ~Lre given in Figure 7. 'l'h ( ~ ) Host suite.hie a.rea , for tim word `` fighter '' must be s ( de ( : ted from mull. ! pie c~mdida.te sets of con imcth ) l ! s. The I\ ] uM liha~se is tim evaJua.thm of tim ca.udi ( hm , s. E~L ( : h ( : aii ! ( lida.te is ( wa.hlail ; ed aic ( ' ( irding ( , o t.l ) e fl ) lh ) wing t'o ( ! r criteria ! . Criterion 1 : The size of the ca. , dida.t ( ~. Giv ( , ~ ~ri inl2UL word w ( in this ca , se , `` fight ( . ' '' ) , a , ud a , nod ( : ( , b~t is conta , ined i , the ca , ndida , te C , CI \ ] C , &lt; , &lt; ~ , ; c c ' `` ~i '' '' ( `` '' , `` , od , : ) . Criterion 2 : q'he h ( : ight ( ) f '' l.hc ( : a. ) ! dida.t ( '. C2 is th ( ; number of levels in the ca , ,dida , l , e. For c×a.mI ) lc , in the c , ~ndidaA ; e ( a. ) in Figure 7 , C2 = 2. Criterion 3 : The a.ver~ge det ) l.h of the nodes. For exa.mph , , the depth of the node `` a.irpla.ne ' , whose. node-id is 0.0.O.0.0.0. l.0.2.0 , is 10. Criterion 4 : The nund ) er of viewpoints. For exa.inple , c~L ) , lida , lc ( a ) ( whose top node is `` human '' ) ha , s the l~rgest imml- ) er ( if no ( h , s. However , ~s show. in Figure 6 , the ma.tclmd rehd , ionshilis ( `` ba.d hume.u/fight ~r ~nd `` l ) rotect hulua.u/fighter '' ) are not typica.l ( ~×pressions h ) r ( he word `` fighteF ' ; th~ ( is , the r ( : h~.tii ) nsltips a.rc not vii : wpoints. On ! hi '' other ha. , d , `` a.iriih~rm '' in cn.ndi ( hLte ( c ) sha.res t\ ] ! ( , `` fighter ( a.irpla ) , ' ) fly '' , which is the viewpi ) int of th ( '. n &lt; ) &lt; le `` ~Lirphm ( C ' C,4 is the numl ) i~r ( if ma.tclmd rela , tionships tha.t a.re ( 'l ) ) tsider ( ~d as viewpoints of the node ill ( , h ( ~ ca. ) ldid~Lte. ) : ' ) Tim totaJ i ) refi~r ( mcc P ( word ) is/'I ( 71 + it 2c , + p3C3+p4C4 , wh ( : re p ! , p2 : / &gt; 3 , amd P4 axe weights for ea .</sentence>
				<definiendum id="0">unk.ow. wor</definiendum>
				<definiendum id="1">cted. A viewpoi.t</definiendum>
				<definiendum id="2">InonllnalizMion</definiendum>
				<definiendum id="3">Tile wor</definiendum>
				<definiendum id="4">SUB )</definiendum>
				<definiendum id="5">N</definiendum>
				<definiendum id="6">C</definiendum>
				<definiendum id="7">Thesaurus Node</definiendum>
				<definiendum id="8">SUBJ ) rocket</definiendum>
				<definiendum id="9">c c</definiendum>
				<definiendum id="10">fighter</definiendum>
				<definiens id="0">h~s ( : ril ) es how viewlmints axe exl , ra</definiens>
				<definiens id="1">e.g. basic word ) Adnomlna , liza.tion ( e.g. large t ) uilding</definiens>
				<definiens id="2">a set of no &amp; s i'n \ ] b'AMAP , and</definiens>
				<definiens id="3">a set of conuected no &amp; s that contain the : word w. Examples o i the vie'tvpoi , nts ( whose typicalness exceeds 0.5. ) are as follows : I flying</definiens>
				<definiens id="4">stock ) SE IH IN ( ) } m , n ufa , ( 't t , ) ' ( : ) MONO ( object ) K IN ( gold ) a Y UUTAK U ( house ) ____ GIJYU'I'tlSYA ( engineer ) KIG YOIJ ( compa.IG ) BUH IN ipa , rts</definiens>
				<definiens id="5">th ( ; number of levels in the ca , ,dida , l</definiens>
				<definiens id="6">~d as viewpoints of the node ill ( , h ( ~ ca. ) ldid~Lte. ) : '</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>GRAM is the development of broad coverage grammars which are also modular and easy to nm .</sentence>
				<definiendum id="0">GRAM</definiendum>
				<definiens id="0">the development of broad coverage grammars which are also modular and easy to nm</definiens>
			</definition>
			<definition id="1">
				<sentence>For Vl ) -l ; ot ) i ( : ~flization ( ) r ( '~xt ; r~ &gt; l , qition an unl/oun/|ed long-dist ; an ( ' ( t de , l ) ( mdlm ( : y must sl : ill 1 ) e a , ssmned , llow ( w ( ~r , a , s die fun ( : d/ ) na , l unl : lwl ; Mnt ; y 1 ) al ; h f ( n ' mlxilia .</sentence>
				<definiendum id="0">h f</definiendum>
				<definiens id="0">s die fun ( : d/ ) na</definiens>
			</definition>
			<definition id="2">
				<sentence>In case of a derived nominal , however , a genitive is interpreted according to tile thematic roles assigned to tile arguments of the verbal base .</sentence>
				<definiendum id="0">genitive</definiendum>
				<definiens id="0">interpreted according to tile thematic roles assigned to tile arguments of the verbal base</definiens>
			</definition>
			<definition id="3">
				<sentence>9 Nominalization is therefore implemented as a morphologically driven process ( lexical rule ) which eliminates suI~a and Ol &lt; J fl'om the verb 's subcategorization frame and enters tile verb 's argument structure into tile 8Abstracting away fl'om bar-level considerations and further ot ) tional constituents , this ruh ; captures the restrictions that determine the dislocation of a genitive in the matrix Nit 9If a semantic or argulnelll ; projection is assumed ( e.g. , Halvorsen and Kaplan , 1988 ) , this information may be represented at another independent projection. lexical entry of the noun. This yields the optionality of genitives while preserving tile underlying semantics , as shown in ( 12 ) . The association of CEN1 and GEN2 then is determined according to a hierarchical order of arguments ( Bresnan , 1995 ) . This approach also provides a means of handling certain eases of categorial stfift. For instance , in German temporal and conditional adjuncts mw be realized as PPs dominating an NP headed by a deverbal noun. English does not have this option , but employs an adjunct-clause instead. Ih ; re , the GEN1 and GFN2 functions of the Germml fstructure have to be related correctly to tile SUlIJ and OBJ functions of tile English f-structure. bei Naris at Karl-Oen nnlssten must-Past Darstellung des Vorfalls report tile accident-Gen alle lachen all laugh 'when Karl reported the accident everyone had to laugh ' Here the linking of the GEN1 and ( ; EN2 functions to the appropriate thematic rote in the German f-structure driw ; s tile transfer of these functions to the SllllJ and oBJ time , lions of tile English f-structure. PREI ) 'Darstellung ' J ARG\ ] AGF , NT \ ] ARG-STI { AI { G2 TIIEME GFN1 \ [ I 'll , H ) 'Karl'\ ] ( ; EN2 \ [ PI { E1 ) 'Vorfall ' \ ] Pt { ED 'ret ) ort &lt; S/Ill , J , OIIJ &gt; ' ~\ ] sulfa \ [ pm , ; ~ ) 'Nm'l ' \ ] l ( mJ \ [ PaEI ) % ccident ' \ ] Under this approach , languages now only differ with respct to the categorial realisation of the flmction by ease-marked NP or PP .</sentence>
				<definiendum id="0">projection</definiendum>
				<definiens id="0">a morphologically driven process ( lexical rule ) which eliminates suI~a and Ol &lt; J fl'om the verb 's subcategorization frame and enters tile verb 's argument structure into tile 8Abstracting away fl'om bar-level considerations and further ot</definiens>
				<definiens id="1">captures the restrictions that determine the dislocation of a genitive in the matrix Nit 9If a semantic or argulnelll</definiens>
				<definiens id="2">PPs dominating an NP headed by a deverbal noun. English does not have this option , but employs an adjunct-clause instead. Ih</definiens>
			</definition>
</paper>

		<paper id="2151">
			<definition id="0">
				<sentence>Quite in general , if ~N is the sample mean of N independent observations of any numerical random variable ( with variance a0 2 , i.e. , - } N = ~- , i=1 ( i , then ~2 = Var\ [ ~N\ ] = 1N 1 N ~ -- -Var\ [ '~-~ ( i\ ] = ~ ~Var\ [ ( i\ ] -- -i=1 i=1 In our case , the number of observations N is simply the size of the context Ck , by which we mean the number of times Ck occurred in the training data , i.e. , the frequency count of Ck , which we will denote \ ] Ck\ [ .</sentence>
				<definiendum id="0">~N</definiendum>
				<definiens id="0">the sample mean of N independent observations of any numerical random variable ( with variance a0 2 , i.e. , - } N = ~-</definiens>
				<definiens id="1">the number of observations N is simply the size of the context Ck , by which we mean the number of times Ck occurred in the training data , i.e. , the frequency count of Ck</definiens>
			</definition>
			<definition id="1">
				<sentence>Part-of-speech ( PoS ) tagging consists in assigning to each word of an input text a ( set of ) tag ( s ) from a finite set of possible tags , a tag palette or a tag set .</sentence>
				<definiendum id="0">Part-of-speech ( PoS ) tagging</definiendum>
				<definiens id="0">consists in assigning to each word of an input text a ( set of ) tag ( s ) from a finite set of possible tags , a tag palette or a tag set</definiens>
			</definition>
			<definition id="2">
				<sentence>( 1 ) accordingly : D ( T I Tt , T~ ) = cr ( Tt , T~ ) -~ f ( T I Tz , T~ ) o ' ( r/ , Tr ) -1 q1 -~ I P ( TIT0 + P ( TIT~ ) +2 ~ ( T , ,T~ ) -~ + i and P ( T I T~ ) P ( TIT , ) a ( Ti ) -1 f ( T I T~ ) + P ( T ) a ( Tz ) -1 + 1 ~r ( Tr ) -1 f ( T ITs ) + P ( T ) ~ ( T~ ) -I + 1 1Or really , P ( T i I 1o , 11 , ... , ln ) where lo is a special symbol indicating the beginning o1 the word .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">lo</definiendum>
			</definition>
			<definition id="3">
				<sentence>Test corpus Tagger Error rate ( % ) tag omissions -unknown words Unknown words Error rate ( % ) B bigram trigram HMM 22.1 19.4 24,5 Test corpus C bigram HMM trigram Tagger Error rate ( % ) tag omissions unknown words Unknown words Error rate ( % ) 18.3 16.8 17.3 Test corpus D bigram HMM Tagger Error rate ( % ) tag omissions unknown words Unknown words Error rate ( % ) trigram 22.3 20.2 25.0 Figure 1 : Results on the Susanne Corpus Tile performance of the tagger was compared with that of an tlMM-based trigram tagger that uses linear interpolation for N-gram smoothing , but where the back-off weights do not depend on the eonditionings .</sentence>
				<definiendum id="0">Test corpus Tagger Error rate</definiendum>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>The Pangloss Example-Based Machine Translation engine ( I'anEI3MT ) l is a translation system reql , iring essentially no knowledge of the structure of a language , merely a large parallel corpus of example sentences atn\ [ a bilingual dictionary .</sentence>
				<definiendum id="0">Pangloss Example-Based Machine Translation engine</definiendum>
				<definiens id="0">a translation system reql , iring essentially no knowledge of the structure of a language</definiens>
			</definition>
			<definition id="1">
				<sentence>Pangloss ( Nirenburg el ; al. , 1995 ) is a multiengine machine translation system , in which several translation engines are .</sentence>
				<definiendum id="0">Pangloss</definiendum>
				<definiens id="0">a multiengine machine translation system</definiens>
			</definition>
			<definition id="2">
				<sentence>Panl'3BMT is one of the translation engines used by Pangloss .</sentence>
				<definiendum id="0">Panl'3BMT</definiendum>
				<definiens id="0">one of the translation engines used by Pangloss</definiens>
			</definition>
			<definition id="3">
				<sentence>The fourth ( minor and optional ) knowledge source is the hmguage-specific information provided in the conliguration tile , which consists of n list of tokenizations equating words within classes such as w0ekdays , a list of words which ntay be elided during alignment ( such as artMes ) , and a list of words which may be inserted The corpus used by PanEBMT consists of a set of source/target sentence , pairs , and is flflly indexed on t , he source-language sentences .</sentence>
				<definiendum id="0">optional ) knowledge source</definiendum>
				<definiens id="0">the hmguage-specific information provided in the conliguration tile , which consists of n list of tokenizations equating words within classes such as w0ekdays , a list of words which ntay be elided during alignment ( such as artMes ) , and a list of words which may be inserted The corpus used by PanEBMT consists of a set of source/target sentence , pairs , and is flflly indexed on t , he source-language sentences</definiens>
			</definition>
			<definition id="4">
				<sentence>The corpus index lists all occurrences of every word and punctuation mark in the sourcelanguage sentences contained in the corpus .</sentence>
				<definiendum id="0">corpus index</definiendum>
				<definiens id="0">lists all occurrences of every word and punctuation mark in the sourcelanguage sentences contained in the corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>PanEBMT consists of approximately 13,300 lines of code , including the code for a glossary mode which will not be described here .</sentence>
				<definiendum id="0">PanEBMT</definiendum>
				<definiens id="0">consists of approximately 13,300 lines of code , including the code for a glossary mode which will not be described here</definiens>
			</definition>
			<definition id="6">
				<sentence>PanEBMT uses a re-processed version of the bilingual dictionary used by Pangloss 's dictionary translation engine ( Figure 2 ) .</sentence>
				<definiendum id="0">PanEBMT</definiendum>
				<definiens id="0">uses a re-processed version of the bilingual dictionary used by Pangloss 's dictionary translation engine ( Figure 2 )</definiens>
			</definition>
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>Japanese function words , such as conjunctive particles ( postpositions ) located at the end of each clause , have modality and suggest global structures of Japanese long sentences in cooperation with modality within predicates or auxiliary verbs .</sentence>
				<definiendum id="0">conjunctive particles ( postpositions</definiendum>
				<definiens id="0">cooperation with modality within predicates or auxiliary verbs</definiens>
			</definition>
			<definition id="1">
				<sentence>LDG presumes tile inter-clausal dependency within Japanese sentences prior to syntactic and semantic analyses , by utilizing the differences of the encapsulating powers each Japanese function word has , and by utilizing modification preference between function words and predicates that reflects consistency of modality in them .</sentence>
				<definiendum id="0">LDG</definiendum>
				<definiens id="0">presumes tile inter-clausal dependency within Japanese sentences prior to syntactic and semantic analyses</definiens>
			</definition>
			<definition id="2">
				<sentence>LDG assumes that Japanese function words , such as conjunctive partides ( postpositions ) located at the end of each clause , convey modMity , or propositional attitude , and suggest global structures of Japanese long sentences in cooperation with modality in predicates , especially within 310 auxiliary verbs .</sentence>
				<definiendum id="0">LDG</definiendum>
				<definiendum id="1">conjunctive partides</definiendum>
			</definition>
			<definition id="3">
				<sentence>LDG is effective in reducing the syntactic ambiguities , and it him alre~My been applied to a machine translation system .</sentence>
				<definiendum id="0">LDG</definiendum>
				<definiens id="0">effective in reducing the syntactic ambiguities , and it him alre~My been applied to a machine translation system</definiens>
			</definition>
			<definition id="4">
				<sentence>A subordinate clause with modality modifies a consistent modality predicate type .</sentence>
				<definiendum id="0">modality</definiendum>
				<definiens id="0">modifies a consistent modality predicate type</definiens>
			</definition>
			<definition id="5">
				<sentence>Based on this assumption , LDG presumes the sentence structure before syntactic and semantic analyses on the ba~sis of previously collected lexical information that characterizes the lexical discourse .</sentence>
				<definiendum id="0">LDG</definiendum>
				<definiens id="0">presumes the sentence structure before syntactic and semantic analyses on the ba~sis of previously collected lexical information that characterizes the lexical discourse</definiens>
			</definition>
			<definition id="6">
				<sentence>The level of conjunctive particles , which indicates the structure of the Japanese long sentences , is the most important feature of LDG .</sentence>
				<definiendum id="0">level of conjunctive particles</definiendum>
				<definiens id="0">the most important feature of LDG</definiens>
			</definition>
</paper>

		<paper id="2110">
			<definition id="0">
				<sentence>A coding system consists of a character set and an encoding S~\ ] te~ne .</sentence>
				<definiendum id="0">coding system</definiendum>
			</definition>
			<definition id="1">
				<sentence>Let TEXT be the set of words in a text , then the likelihood of TEXT with regard to language I is given as tile following P ( 7'EXT '' , l ) = II P ( c ; , 0 wETEXT where P ( Cw , l ) is the unigram probability of C~ in language l , and ( hw is the class name of the word w. P ( C~ , l ) is estimated from text corpora in language I. languages As compared with Western-European languages , East-Asian languages have the following properties : East-Asian languages use over 3,000 ideographic or combined characters .</sentence>
				<definiendum id="0">hw</definiendum>
				<definiendum id="1">East-Asian languages</definiendum>
				<definiens id="0">the unigram probability of C~ in language l</definiens>
			</definition>
			<definition id="2">
				<sentence>F xz , 0 1-\ [ charE'I'EXT where P ( ehar , l ) is the unigram probability of char in language l .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the unigram probability of char in language l</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>The solution adopted by Manning and Sag ( 1995 ) consists of a three step move : i ) to keep the Binding Theory unchanged ; ii ) to create a new list of subcategorized elements , which is named ARG-S ( from argument structure ) ; iii ) to define o-command relations on the basis of the obliqueness hierarchy established on this new list , which may be different from the obliqueness hierarchy established in the SUBCAT list .</sentence>
				<definiendum id="0">subcategorized elements</definiendum>
				<definiens id="0">consists of a three step move : i ) to keep the Binding Theory unchanged</definiens>
			</definition>
			<definition id="1">
				<sentence>Consider the definition of o-command tbr linear obliqueness ( simplified version , ( Xue et al. 1994 , ( 35 ) ) : ( 21 ) ( Linear ) O-command X O-COMMANDS Y iffX is a less oblique coargmnent of Z that dominates Y. \ [ n case Z=Y , X is said to LOCAI , LY o-command Y. where X is less oblique than Y iff X precedes Y in an ARG-S lisL This definition was shown to be adequate for the data considered so thr .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a less oblique coargmnent of Z that dominates Y. \ [ n case Z=Y ,</definiens>
			</definition>
</paper>

		<paper id="1002">
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>Dialogues occur in real-time and so are susceptible to time constraints .</sentence>
				<definiendum id="0">Dialogues</definiendum>
				<definiens id="0">occur in real-time and so are susceptible to time constraints</definiens>
			</definition>
			<definition id="1">
				<sentence>POPEL is a parallel and incremental natural language generation system ( Finkler and Schauder 1992 ; Reithinger 1992 ) .</sentence>
				<definiendum id="0">POPEL</definiendum>
				<definiens id="0">a parallel and incremental natural language generation system</definiens>
			</definition>
			<definition id="2">
				<sentence>Although our model ca , n 1 ) roduce filler terms and repair prior utterances , our chief concern is the tine structure of spoken discourse , which is closely related to incremental utterance pro ( tnction .</sentence>
				<definiendum id="0">chief concern</definiendum>
				<definiens id="0">the tine structure of spoken discourse , which is closely related to incremental utterance pro</definiens>
			</definition>
			<definition id="3">
				<sentence>Elaboration enables speakers to distribute the content to be conveyed among different lUs. '</sentence>
				<definiendum id="0">Elaboration</definiendum>
			</definition>
			<definition id="4">
				<sentence>An utterance plan is a sequence of colnmnnieative actions that achieves a communicative goal .</sentence>
				<definiendum id="0">utterance plan</definiendum>
				<definiens id="0">a sequence of colnmnnieative actions that achieves a communicative goal</definiens>
			</definition>
			<definition id="5">
				<sentence>An action schema consists of an action description , applicability constraints and an effe ( : t. 2 It defines a communicative action .</sentence>
				<definiendum id="0">action schema</definiendum>
				<definiens id="0">consists of an action description , applicability constraints and an effe ( : t. 2 It defines a communicative action</definiens>
			</definition>
			<definition id="6">
				<sentence>A decomposition inetho ( l consists of an action description , applieal ) ility constraints and a plan .</sentence>
				<definiendum id="0">decomposition inetho ( l</definiendum>
				<definiens id="0">consists of an action description , applieal ) ility constraints and a plan</definiens>
			</definition>
			<definition id="7">
				<sentence>P ) , Effect : persuaded-plan ( * P ) ) ( r7 ) Decomp ( propose-acts-in-seq ( \ [ *Act l *Rest\ ] ) , Constr : *Rest ¢ \ [ \ ] , Plan : \ [ aehieve ( persuaded-aet ( * Act ) ) , propose-acts-in-scq ( * Rest ) \ ] ) In these representations , achieve ( P ) designates an action that achieves goal P. Notation \ [ H I L\ ] specifies a list , where H is the head of the list and L is the rest .</sentence>
				<definiendum id="0">H</definiendum>
				<definiendum id="1">L</definiendum>
				<definiens id="0">the head of the list</definiens>
			</definition>
			<definition id="8">
				<sentence>\ [ *R , *E , *0\ ] A *R ¢ type A cont ( *O , , ObjC ) A *Rest = *C { *Thema } plan : \ [ chi e ( descr cd-obj ( , O , *ObjC ) ) , *R , *0 , *E ) ) , ach , ie e ( d sc i ed-e e , t ( , E , , Re t , , At ) ) \ ] ) ( r13 ) Act ( describe-obj-with-thcma ( *O , *C , *R , *E ) , Effect : dcscribcd-obj ( *O , *C ) A described-thema-rcl ( , R , *0 , *E ) ) ( r14 ) Decoinp ( dcscribe-obj-with-thcrna ( , O , *C , *R , *E ) , Plan : surface-desc-obj ( , O , *C , *R ) ) ( r15 ) Act ( dcscribc-cvcnt-type ( , E , *C , *At ) , Constr : *C = { type ( *E , *T ) } , Effect : describcd-cvcnt ( , E , *C , *At ) ) ( r16 ) De~comp ( describc-event-type ( , E , *C , *At ) , Plan : surface-desc-event ( * E , *C , *At ) ) Figure 5 : Action schemata and decomposition methods for proposing domain action ( r8 ) is decomposed by applying the action schemata and decomposition methods shown in Figure 5 .</sentence>
				<definiendum id="0">described-thema-rcl</definiendum>
				<definiendum id="1">Constr</definiendum>
				<definiens id="0">dcscribc-cvcnt-type ( , E , *C , *At ) ,</definiens>
			</definition>
			<definition id="9">
				<sentence>The utterance plan308 ( el ) Musashino sentaa kara-wa desune / PN front-Topic , COPULA ( from the Musashino Center ) ( e2 ) ~to Kichijoji made / dete-kudasai / FILI , F , I~ PN to go-t ) le~se ( crm to Kichijoji station ) ( please go ) @ 3 ) ~to desune sorekara inokashira-sen de FILLER then PN by ( erm then by the Inokastfira Line ) ( e4 ) odakyu-sen hi/ norikaete / PN for change ( ( : hange train for the Odakyu Line ) basu de / moyori-no-eki made / bus by nearest station to ( by Ires ) ( to the nearest station ) desune / shimokitazawa made / COI'UI , A PN to ( to Shimokitazawa station ) aiko-ishida made / ikimasu / ... .. PN to go ( to Aiko-ishida station ) ( go ) Figure 6 : Discourse generated by implemented system ner includes 16 action schemata and 16 decomposition methods .</sentence>
				<definiendum id="0">COPULA</definiendum>
				<definiendum id="1">PN to ( to Shimokitazawa station</definiendum>
				<definiens id="0">Aiko-ishida station ) ( go ) Figure 6 : Discourse generated by implemented system ner includes 16 action schemata and 16 decomposition methods</definiens>
			</definition>
			<definition id="10">
				<sentence>Artificial Intelligence : A Modern Approach .</sentence>
				<definiendum id="0">Artificial Intelligence</definiendum>
				<definiens id="0">A Modern Approach</definiens>
			</definition>
</paper>

		<paper id="2154">
			<definition id="0">
				<sentence>Ilere Weigh , ; ( 'w ) is the weight of word w , F ' ( 'w ) is the , frequency of w ( ird 'W in the 20 Nbest senten ( : es , M is the total Itumb ( _ !</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">the weight of word w</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>The C , hinesc-t ' ; nglish alignmeat consists of segmentation of an inl ) U/ , ( ' , hinese sentence , and aligning the segmented seltteiic ( ?</sentence>
				<definiendum id="0">nglish alignmeat</definiendum>
				<definiens id="0">consists of segmentation of an inl ) U/ , ( ' , hinese sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>A phrase is any arbitrary sequence of ad , iaeent words in a sentence .</sentence>
				<definiendum id="0">phrase</definiendum>
				<definiens id="0">any arbitrary sequence of ad , iaeent words in a sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>p ( e.lk ) = ~ r , ( g Iki ) ( 3 ) j=l i=0 In equation 3 , n and m are the nmnl ) er of words in the English sentence e and its correspoudil G Korean sentence k respectively , cj and kl are tit { &gt; aligtdng unit between l'2nglish sentence e and Korean sentence k. cj rq+resenl , s j-th word in I '' nglish sentence and k/ represents i-th word in Korean sentence .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the nmnl ) er of words in the English sentence e and its correspoudil G Korean sentence k respectively , cj and kl are tit { &gt; aligtdng unit between l'2nglish sentence e and Korean sentence k. cj rq+resenl , s j-th word in I '' nglish sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>\ [ n equation 6 , k~ ~ is j-th phrase of \ ] d '~ , and l ( kP~l , j e denotes the tag sequence of words composing phrase kj'Pk .</sentence>
				<definiendum id="0">l</definiendum>
				<definiens id="0">j-th phrase of \ ] d '~ , and</definiens>
			</definition>
			<definition id="4">
				<sentence>; c. , k ) = v ( elk ) ( It ) I '' or tim exl ; e , l ( led tnethod of phrase alignment , the Itase model is an intcrntediatc stage for the estima l , iou of word-to-w &lt; ) rd f &gt; rol ) abilith~s. Who phraset , ( ) -\ [ ) h\ [ ) O.rse i ) rol &gt; abililics are reesl , \ [ tna .</sentence>
				<definiendum id="0">Itase model</definiendum>
				<definiens id="0">k ) = v ( elk ) ( It ) I '' or tim exl ; e , l ( led tnethod of phrase alignment , the</definiens>
			</definition>
			<definition id="5">
				<sentence>= maxt \ [ pi-a ( t ' ) + logp ( k~ , , c~b ) \ ] l &lt; t/ &lt; r , l &lt; j &lt; N l &lt; a &lt; L , l &lt; b &lt; L = ( j , &lt; ~ ) = arg maxt \ [ ~i_~ ( t ' ) + logp ( ki¢ , , ej , b ) \ ] I &lt; tt &lt; T , I &lt; j &lt; N l &lt; a~L , I &lt; b &lt; L As a result , the running complexity of the proposed algorithm becomes O ( TL2MN ) . Taking T and L as constants , the order of complexity becomes O ( MN ) . As another method to relax the problem of decision dependency on the previous matches , preemptive scheme to find max matching of phrase ki , ~ is adopted. In the preemptive aligmnent , the previous selection can be rematched with the better selection found by later decision. In following algorithm , ~ ( ki , a , n ) denote ej , v which has n-th highest matching wdue with Korean phrase ki , ~ among all possible matching Korean phrase and u ( ki , a , n ) carry the weight tbr tile matching. ~i , b indicate matched Korean phrase with ej , b in current status and v~j , ~ denote their matching weight. 'l'he established matching in previous stage can be changed when another matching , which has higer matching weight , is identified in this algorithm. Initialization O ( ki , ~ , ,n ) = ( j , b ) Oj , b = O , ( I &lt; _.j &lt; _N , I &lt; b &lt; L ) p ( l % a , ~j , b ) . ( / &lt; o , n ) = Preemptive selection n=0 ( j , b ) = ~ ( ki , a , n ) repeat if ( u ( ki/ , , n ) &gt; '~j , b ) lgj , b ~l/ ( \ [ ~i , a , Zl ) \ ] ¢~ , a = ej , b , ~j , b ~\ ] gi , a , Igi , a ~ lg~ , a else n = n + 1 , ( j , b ) = CO ( l~i , a , , Z ) until 0j , b is 0 Table 2 : 'Pile content of training corpus ( English : words , Korean : word-phrases ) Source English Korean middle-school textbook 46,400 34,800 high-school textbook 153,300 106,400 other books 54,400 37,100 total 254,100 178,300 Although the proposed algorithm can not cover all possible alignment cases , the proposed algorithm produces resonably accurate alignment results efliciently as is demonstrated in the following section .</sentence>
				<definiendum id="0">algorithm becomes O</definiendum>
				<definiendum id="1">Korean</definiendum>
				<definiens id="0">another method to relax the problem of decision dependency on the previous matches , preemptive scheme to find max matching of phrase ki , ~ is adopted. In the preemptive aligmnent , the previous selection can be rematched with the better selection found by later decision. In following algorithm , ~ ( ki , a , n ) denote ej , v which has n-th highest matching wdue with Korean phrase ki , ~ among all possible matching Korean phrase and u ( ki , a , n ) carry the weight tbr tile matching. ~i , b indicate matched Korean phrase with ej , b in current status</definiens>
				<definiens id="1">a = ej , b , ~j , b ~\ ] gi , a</definiens>
			</definition>
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>a ) ~7t77~ ~\ [ 'P ( t ) t -4 now e : zeig~~ ) e starts e llere , TP stands for the actual ( past ) temporal perspective that holds for the given utteranee/te×t sil ; uation .</sentence>
				<definiendum id="0">TP</definiendum>
				<definiens id="0">the actual ( past ) temporal perspective that holds for the given utteranee/te×t sil ; uation</definiens>
			</definition>
			<definition id="1">
				<sentence>supposes a reference event c ' such that the senteuce ( ; vent c is the tirst event of m possible elabormtiou sequence of c ' ( cf. ( Ebe92 ) \ [ 'or the computation mad storing of discourse relmI As regards delhtite descriptions , the distinguished t ) I { .</sentence>
				<definiendum id="0">vent c</definiendum>
				<definiens id="0">the tirst event of m possible elabormtiou sequence of c ' ( cf. ( Ebe92 ) \ [ 'or the computation mad storing of discourse relmI As regards delhtite descriptions , the distinguished t ) I {</definiens>
			</definition>
			<definition id="2">
				<sentence>O~def ( X , nte-~liicksze~hl ( x ) \ ] ) ' where n is a number from the set of alternatives of 4 ) .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
			<definition id="3">
				<sentence>Tile order of the oppertunities is inherited from the order of the Ps , - &lt; o , which conforms to the intrinsic order of the set of Mternatives of the focused element ( i.e. O=order ( ALT ( FOC~.I ( DI~S~ ) ) ) ) . In ( 1 ) , this is the canonical order of the numbers. The negation test , which is comlnonly used to detect presuppositions , supports these strueturM assumptions. Now , we think that the EPA-reading interpretes tile asserted event , which is backed by the described scenario , as the lirst one that ; is indeed realized within the range of possible instantiations that the sequence of opportunities provides , i.e. the asserted event presents the first positive outcome to tile test about the instantiation of the Ae.BAC ( DRS~ ) -type that is connected to the eisequence , where each test situation el is characterized by its own specific additional test criterion kx. F O ( ; ( 1 ) I£Se ) \ [ 41n\ ] '2 Further linguistic tests , that we must omit here , support the assumption that the information about the negative tests is an entailment. Therefore , in the representation , the negated DRSs for the pointing-opportunities el e3 are part of the main DRS. In the literature the representation of the fo ( 'using use of ersl ( and corresponding uses of noch and schon ) often comprises the information that tile reported reMization of the event is earlier or later ( depending on the reading and the adverb ) than the speaker/writer and/or the recipient ( or even a third person ) would have expected. We think that such an expectation , in the case of the EPA-reading of erst , is only optional. Following ( K6n91 ) , we think that , if present , it is not a presupposition but a conventional implicature. In ( 1.b ) , we use the specific c~i , ,~ , l-format and the representation convention of ( Ram95 ) for attitudinal states in order to express the EPA-expectation of a previous test to be successful. As for the EPA-reading , we consider the case where tile numeral is focussed only : 2Depending on the tocus structure of tile phrase in the scope of erst in ( 1 ) and depending on the contextuM restrictions of the admissible alternatives , other sets of Ps might result. It is clear that depending on this choice , the focus conditions may characterize a thematic role. , as in the described example , or the event wriable. It is ~dso clear , as we will argue in the next section , that not all of these sets of alternatives can accept the El~A-reading. ( 1.c ) peter now e t TP ( t ) t -4 now x e : zeigen_auf ( peter , x ) e @ t OZpresp ( S , s : ATT ( x , , &lt; MODE , eE Ai=k -- 1 i=1 ( el - &lt; ei+l ) AI : P , ( ° , ) i=1 MODE=PLAN X=peterlA i realization_of ( e , e4 ) I 1 , o I ' '' \ ] / F~ ~j3 e ~J = i=1 i A : - ? reMiz~ti°n-°f ( ei ' , eO S ineets e'l el ' ~ e2 ' ~ e3 ' -~ e ... ... optional or probably obfigatory ... .. Ottmpl ( . ~ : A'PT ( ~ &lt; lt \ ] , :L , I~*l &gt; 4 ) In contrast to the EPA-reading , we assume that , in the R-reading , the predicates Pi that we obtain from the information structure of the erstargument are not related to a sequence of opportunities for doing something , but describe events ei of an expectation about the ongoing of the world or a plan e. The context ( 1 ) does n't provide fin : tiler information about the identity of the person or persons X to whom the introduced attitudinal state has to be ascribed , to the speaker/writer , to the recipient , to Peter , to someone else or to some group of salient people .</sentence>
				<definiendum id="0">ALT ( FOC~.I</definiendum>
				<definiendum id="1">negation test</definiendum>
				<definiendum id="2">tile numeral</definiendum>
				<definiens id="0">the EPA-reading interpretes tile asserted event , which is backed by the described scenario , as the lirst one that ; is indeed realized within the range of possible instantiations that the sequence of opportunities provides , i.e. the asserted event presents the first positive outcome to tile test about the instantiation of the Ae.BAC ( DRS~ ) -type that is connected to the eisequence , where each test situation el is characterized by its own specific additional test criterion kx. F O</definiens>
			</definition>
			<definition id="4">
				<sentence>New is the information about the progress of the instantiation of the presupposed event concepts .</sentence>
				<definiendum id="0">New</definiendum>
				<definiens id="0">the information about the progress of the instantiation of the presupposed event concepts</definiens>
			</definition>
			<definition id="5">
				<sentence>The alternative temporal localization that occurs in the scope of erst is the construction 'COl &gt; = ula + predicative temporal expression ' , which accepts the R-reading .</sentence>
				<definiendum id="0">erst</definiendum>
				<definiens id="0">accepts the R-reading</definiens>
			</definition>
</paper>

		<paper id="2121">
			<definition id="0">
				<sentence>Addition defines a commutative group , and multiplication makes it a field .</sentence>
				<definiendum id="0">Addition</definiendum>
				<definiens id="0">defines a commutative group , and multiplication makes it a field</definiens>
			</definition>
			<definition id="1">
				<sentence>A &lt; listance can be defined by assigning weights to these three ( ) per : ations , 1 for each of them , for simplification. The edit &lt; tistance is then a simple extension fi'om edit operations to strings. Definition 2 ( Edition distanee ) Let V be a vocab'ulary , dist is defined on V* as a commutative operation , in the following way : v ( a , c v v ( , , , ,O ( v* ) dist ( e , e ) = dist ( a , a ) =0 dist ( e , a ) = dist ( a , b ) = 1 ff a : /b dist ( a.u , c ) = dist ( a , e ) + dist ( u , e ) dist ( a , c ) + dist ( u , b.v ) , dist ( a.u , b.'v ) = mi~4 dist ( a , b ) + dist ( u , v ) , ) dist ( &lt; b ) + dist ( a.'u , v ) With this delinition and a weight of 1 for each of the three edit operations , tile distance between mathematical and physics becomes 9. m a t h c m a I i c a / yszcs = 9 As a mathematical result , with more general weighl ; s , it can be proved that , if the edit Ol ) erations define a metric on P U { c } , then the ( '. ( lit distance on V* is also a metric. We recall tile tbrmal definition of a metric. Definition 3 ( Metric ) Let S bc a set , dist a function from , _q x S to IR + , the , so/of non-negative real n'umbcrs , dist is a metric on S if and only if ® ( cqualily ) V ( a , b ) &lt; S 2 , dist ( a , b ) =0C &gt; a=b • ( commulalivity ) v ( ( , , s ( list ( &lt; b ) = di t ( &lt; a ) • ( h'iangle inequality ) V ( a , b , c ) C S a , dist ( a , c ) _ &lt; dist ( a , b ) + dist ( b , c ) tlaving defined what we un &lt; lerstand by analogy in a formal way , we inspect , some o\ [ ' its properties. We first ; make a very strong but necessary assumption about the nature of the solution of an analogy. Following the linguistic feeling , we impose that tile solution of an analogy be built only with the elements of the vocabulary present in the three given terms. In other words , no material from outside should be used. This constraint does not prevent analogies from having multiple solutions. It suffices that the distances become too large relative to the lengths of the words , a : thc = of : x is such a case. The constraint eliminates , for instance , all words of the form txy , with x and y two letters outside of the set { a , e , f , h , o , t } , but does not bar Ill , hhh , eee , which are solutions of this analogy. But , as a matter of fact , this kind of example does not make much linguistic sense. A degenerated c~se of analogy is when two of tile three terms are equal. The only possihle solution is then the third term. IlL other words , nothing new &lt; : an really be said. This meets common sellse. v ) c ( v'f , = = This property is always true. It is proved thanks to the equality property of a metric : u : `` u = v : x dist ( u , ,4=O=dist ( v , x ) ~ x= , :. Some. imt &gt; ortmtt linguistic phenomena are covered hy onr proposal for linguistic examples .</sentence>
				<definiendum id="0">&lt; listance</definiendum>
				<definiendum id="1">/b dist</definiendum>
				<definiendum id="2">u , e ) dist</definiendum>
				<definiendum id="3">Metric</definiendum>
				<definiendum id="4">V</definiendum>
				<definiendum id="5">eee</definiendum>
				<definiens id="0">a commutative operation , in the following way : v ( a , c v v</definiens>
				<definiens id="1">a.u , b.'v ) = mi~4 dist ( a , b ) + dist ( u , v )</definiens>
				<definiens id="2">such a case. The constraint eliminates , for instance , all words of the form txy , with x and y two letters outside of the set { a , e , f , h , o , t } , but does not bar Ill , hhh</definiens>
			</definition>
			<definition id="2">
				<sentence>In fact , it is possible to give a definition of an edit distance on forests which generalises the definitions on strings ( Wagner &amp; Fischer 74 ) and on 7arsala ( he sent ) and aslama ( he became converted ) verbs 3rd person singular past ; mursilun ( a sender ) and muslirnun ( a convert ) agent nouns .</sentence>
				<definiendum id="0">muslirnun</definiendum>
				<definiens id="0">a definition of an edit distance on forests which generalises the definitions on strings</definiens>
			</definition>
			<definition id="3">
				<sentence>Precision Again , in document retriewfl , precision is defined as the ratio of the nmnt ) er of rele721 rant documents retrieved over the total number of documents retrieved .</sentence>
				<definiendum id="0">Precision Again</definiendum>
				<definiendum id="1">precision</definiendum>
			</definition>
</paper>

		<paper id="1068">
			<definition id="0">
				<sentence>Thus with the strategy agents , the user is made aware of the strategy which is specific to the task an &lt; l this mechanism prcvcnts the user using the task specific strategy for other tasks. In the current system , there are two strategy agents for the travel dmnain : 2Travel agent is able to retrive and find `` the hot spring which is the scene of Izu no odoriko '' . business trip strategy agent : indispensable c &lt; mdition for the inlmt is the destination , and the optional con &lt; liti &lt; ) ns are the room charge and the circumstances. When the optional con &lt; litions arc not defined by the user , the strategy agent will rex : olmncnd some choices to the user. The default responses arc the name of the hotel and its telephone number in this task. recreation strategy agent : indisl ) ensablc condition for the input is the recreation equipment and the number of participants and the other conditions are optional. When the optional conditions are not defined by the user , the strategy agent will recommend some choices to the user. The default responses are also the name of the hotel and its telephone lmnJ ) er in this task. These strategy agents not Olfly allow the user to use the system easily 1 ) ut ~dso hell &gt; the user to 1 ) e aware of the &lt; : haraeteristies of the diah ) gue strategy specific to the task .</sentence>
				<definiendum id="0">hot spring</definiendum>
				<definiens id="0">able to retrive and find `` the</definiens>
				<definiens id="1">the destination , and the optional con &lt; liti &lt; ) ns are the room charge and the circumstances. When the optional con &lt; litions arc not defined by the user , the strategy agent will rex : olmncnd some choices to the user. The default responses arc the name of the hotel</definiens>
				<definiens id="2">the recreation equipment and the number of participants and the other conditions are optional. When the optional conditions are not defined by the user , the strategy agent will recommend some choices to the user. The default responses are also the name of the hotel and its telephone lmnJ ) er in this task. These strategy agents not Olfly allow the user to use the system easily 1 ) ut ~dso hell &gt; the user to 1 ) e aware of the &lt; : haraeteristies of the diah</definiens>
			</definition>
			<definition id="1">
				<sentence>We evaluated the system by counting the nun &gt; ber of the interactions between the user and the system ( Tnrns ) , the number of inl ) ut characters of the users ( Characters ) , and session time ( Seconds ) that subjects took to reach the same goal with new system and the old one .</sentence>
				<definiendum id="0">session time ( Seconds )</definiendum>
				<definiens id="0">the system by counting the nun &gt; ber of the interactions between the user and the system ( Tnrns ) , the number of inl ) ut characters of the users ( Characters ) , and</definiens>
			</definition>
</paper>

		<paper id="2140">
			<definition id="0">
				<sentence>ry 2 &lt; I , t'\ ] M M A &gt; Imiser &lt; ( i I~ , A M &gt; 1 ransil ; iv ( ' , verl &gt; &lt; 'I'I { , A NS &gt; kuss &lt; m \ [ 1 , o kiss\ ] 831 Appelfie Ilumani generi~ unitat ( L'Unit.~ du genre humain ) , cette encyclique d6nonce diverses formes de n ation alisme et la mont~e en lalissance d'Etals fauteurs de d~sordre , tout octopus h des prdparatifs de gaerre. La personne l~umaine , voutue par Diet , et plac6e au cmttre du dispositif de la soci6t~ , est bafc~e Iortque le r~.gne de rargent ~e conjugue avec l'agrc~ivit~ d'un r~gime ot'lla pr6dminence de la race ou de la classe remplace le souei d'une politique au ~crvice de tous. Certes , le eonlntultiSme r~t tc~ljours dsigt~d comme l'adversaire principal , le dangt.n '' ~apr~me. Mats le~ diverges figurcs d'un nafionalisme e~x acer b6 ~ont , elics aussi , ( l~tlOIIC~'~ COIllll|e nlelllionger e~ ~'~ eontrairt~ all plan de l ) ieu. On sent cependant entre les lights une moindre ~'wSrit~ ~ lear endroit qu'~ r , ~gard du eOlllllluni~lne. De pesants filences CE qtte Fencyclique dit mr le raeisme fimpire de ee que John I a Farge a d~Sj~ 6crit pour ~tigmatiser le r acinne anti~Noir qui ~6vit aux Etals Uni~. Ellereprendenoutrel'essentieldescondaml~ations tr.N ferrets que Pie XI viem de domwr dam son encyelique ~ur le nazim~e ( Mit Bre~me~der Serge , mart 1937 ) . ! lider. mt effet , f6tait r~joui trop vile d'entendre le pape parler d'un communimte e intrim~quemott perve¢~ ( 7 ) ~. I1 ne soulNannait pas que , qudque~ jc~lrl plus lard , une condamnation en r~..le du laazi~me i~ou~ forme d'encyc|ique t~ait introduite clandcstinement en Allemagne el , h la barbe de~ autoritt~s , terait he mlennellement en chaire dens toutcs les ~glises le dimalache de la f~te de~ Rameaux de 2937. Sent mix en accuxation : . la pr~tendue conception des ancient Get'mains. , h base de panth~im~e , d'idoutifieation entre Dieu et le ( ( de~tin impersonnel. , outre Di~a et la race , le Feu ple , l'Et at , le~ hommcs au pouvoir _ bref ridolfitrie d'an Dicu et d'une religion l~rement nalionau× ( 8 ) . C '' e~t mr la question des juifs pers &amp; : ut~s que le projet d'encyclique de 1938 ~e r~vt~le le plus faible. 11 ¢st largemettt tributaire de ce que le P~re Gundlaeh a 6crit dans un article intituld . , Amis~mitisme. el. pare en 1930 dens une encyclopddie thdologique. ! ( au ! eur dtablit eq.effet ( 1~ distitwtions entre , plusiears ~or.\ [ ~ . ill ~4oordenboek I III Herr. Anal. en Dt.samb. Corpora M0rfol~sct~e aria y~ ~i ( tel '~'ooM ' &amp; tit ... ... . : iai0ffol~ ( ~ : ~it~ly @ ... . van h~ { w0ord : Le LuSh ; Pqbma ~roi'-corafque ~ Botlaatt http ; llwww , mttmp~ttC~ # ¢tt~llt~tlcy/ltOlLI3AU , l , g_L~lN tithe ; 221 `` ; .. Le SOrt , dit le prglat , vaus servtra de lot Que l'on tire au Le ColOn el ¢2habeo. ~ tL d~ 11alza¢ , 1832 Figure 1 : USER INTER.FACI~ '' GLOSSH { -RuG. On the left ; is at text , on the right , from the top are windows for morphological ~nalysis , dictionary , and further examples. Cases like these suggest a po ( ; elltially crippling problem for the GLOSSI~R-I { uG concept : if words are in general ambignous , then providing morphological analyses for them may be too tiresome to be of genuine use to language learners. A long list of potential analyses is potentially of very little use. Since indeed most words are multiply ambiguons , a problem looms. The solution to this problem is disambiguation : to find the right entry in the dictionary , a partof-speech ( POSt disambiguator is applied be\ [ brc morphological analysis in order to obtain the eontextually most plausible morphological analysis. For example in the sentence IJon , donnc-moi un baiscr 'Good , give me a kiss ' , the disambiguator should return a tag for the word baiser indicating \ [ masculine\ ] uoun and in the sentence IIne pent pas baiser 'tie can ' ( ; kiss ' the word baiser should be assigned with a tag indicating verb \ [ infinitive\ ] . The combination of POS disambiguator and morphological analysis suffice to provide the contextually most likely analysis nearly all the time. Stochastic POS disaml ) iguation is implemented in the Rank Xerox Loeolex package. The results of disambiguation and morpltologieel analysis serve not only as input to dictionary lookup hut also to corpus search. The current implementation of this sear ( -h uses only string matching to find farther tokens. Our design calls for L1 , ; XEMb ; -Dased search however , and a preliminary version of this has also been implemented. In order to determine I ; he size of corpus needed , wc experimented with a frequency list of the 10,000 most frequent words. A corpus of 2 MI3 contained 85 % of these , and a corpus of 6 MII 100 % . Our goal is 100 % coverage of the words found in Hedcndaagsc \ [ , 'tans , aud 100 % coverage of the most fi'equent 20,000 words , and we arc close to it. The current corpus size is 8 MB. As the corpus grows , the time for incrcinental search likwise grows linearly. When the average search time grew to severed seconds ( on a 70 MIltS UNIX server ) , it became apparent that some sort of indexing wa.s needed. This was implemented and is described in ( win Slooten 1995 ) . The in dexed lookup is most satisfactorynot only has the absolnte time dropped ~m order of magnitude , but the time appears to be constant when eorpns size is varied between 1 and 10 MB. l~exeme-.based search looks not only lbr further occurrences of the same string , but Mso for thrice832 tional varbml ; s of the word. If the selected word is liw ' ( , .+Hasc+SG+~oun , ( ; he sem ' ( -h shol ) ld find other tokens of this and Mso tokens of the l ) lural l'oriH llvres. This is made possible I ) y lemmatizing the entire corpus in ~t t &gt; ret ) rocessing st , el/ , a.n ( l ret ; a.in-i .</sentence>
				<definiendum id="0">NS &gt; kuss</definiendum>
				<definiendum id="1">USER INTER.FACI~</definiendum>
				<definiens id="0">Ellereprendenoutrel'essentieldescondaml~ations tr.N ferrets que Pie XI viem de domwr dam son encyelique ~ur le nazim~e ( Mit Bre~me~der Serge , mart 1937 ) . ! lider. mt effet</definiens>
				<definiens id="1">made possible I ) y lemmatizing the entire corpus in ~t t &gt; ret ) rocessing st</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>A label is a pointer to a semantic predicate nmking it easy t , o refer to .</sentence>
				<definiendum id="0">label</definiendum>
				<definiens id="0">a pointer to a semantic predicate nmking it easy t</definiens>
			</definition>
			<definition id="1">
				<sentence>Transfer equivalences are stated as relations between sets of source language ( SL ) and sets of target language ( TL ) senlantie entities .</sentence>
				<definiendum id="0">Transfer equivalences</definiendum>
				<definiens id="0">stated as relations between sets of source language ( SL ) and sets of target language ( TL ) senlantie entities</definiens>
			</definition>
			<definition id="2">
				<sentence>Tau0p is an operator indicating the intended application direction ( one of &lt; - &gt; , - &gt; , &lt; - ) .</sentence>
				<definiendum id="0">Tau0p</definiendum>
				<definiens id="0">an operator indicating the intended application direction ( one of &lt;</definiens>
			</definition>
			<definition id="3">
				<sentence>Similarly , German composita like Terminvorschlag axe decomposed into its compounds , e.g. termin ( i2 ) , n n ( il , ±2 ) , vorschlag ( il ) where n_n denotes a generic noun-noun relation .</sentence>
				<definiendum id="0">German composita</definiendum>
				<definiendum id="1">e.g. termin</definiendum>
				<definiendum id="2">n n</definiendum>
				<definiendum id="3">n_n</definiendum>
				<definiens id="0">a generic noun-noun relation</definiens>
			</definition>
			<definition id="4">
				<sentence>If we would use a hierarchical semantics instead , as in the original Shake-and-Bake aproach , where the negation operator embeds the verb semantics we would have to translate schlecht ( e ) , passen ( e ) into not ( suit ( e ) , well ( e ) ) in one rule because there is no coindexation possible to express the correct embedding without the unique labeling of predicates .</sentence>
				<definiendum id="0">negation operator</definiendum>
				<definiens id="0">the correct embedding without the unique labeling of predicates</definiens>
			</definition>
			<definition id="5">
				<sentence>The current transfer implementation consists of a transfer rule compiler which takes a set of rules likc the one presented in section 3 and compiles them into two executable Prolog programs one for each translation direction .</sentence>
				<definiendum id="0">current transfer implementation</definiendum>
				<definiens id="0">consists of a transfer rule compiler which takes a set of rules likc the one presented in section 3 and compiles them into two executable Prolog programs one for each translation direction</definiens>
			</definition>
			<definition id="6">
				<sentence>Machine `` lYanslation Divergences : a Formal Description and Proposed Solution .</sentence>
				<definiendum id="0">Machine `` lYanslation Divergences</definiendum>
				<definiens id="0">a Formal Description</definiens>
			</definition>
			<definition id="7">
				<sentence>Verbmobil : a Translation System for Face-to-Face Dialogs .</sentence>
				<definiendum id="0">Verbmobil</definiendum>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>A treebank is a body of natural language text which has been grammatically annotated by hand , in terms of some previously-established scheme of grammatical analysis .</sentence>
				<definiendum id="0">treebank</definiendum>
				<definiens id="0">a body of natural language text which has been grammatically annotated by hand</definiens>
			</definition>
			<definition id="1">
				<sentence>Treebank Preprocessing The ATR/Lancaster Treebank consists of approximately 730,000 words of grammatically-analyzed text divided into roughly 950 documents ranging in length ffmn about 30 to about 3600 words .</sentence>
				<definiendum id="0">ATR/Lancaster Treebank</definiendum>
			</definition>
			<definition id="2">
				<sentence>Treebanking : The Compilation of'a Corpus of Skeleton Parsed Sentences .</sentence>
				<definiendum id="0">Treebanking</definiendum>
			</definition>
			<definition id="3">
				<sentence>Pearl : A Probabilistic Chart Parser .</sentence>
				<definiendum id="0">Pearl</definiendum>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>ur ( ' in lxwu~s ol ' `` re { : ipro ( : a.l ( 'oll &lt; lil.i ( ) HiHg '' , `` h &lt; '~l : ePo ( : lMs '' a.ml ol , herwise , ra.i~es ill &lt; ' issue o1 ' wha { regular i ) ro { 'e.~ , ~ governs sl { qll ~el ( 'cl , iou. 'l'radil , ioualIy , re { qpro { : al ( ' ( ) udit , iouiug \ [ IH~ I ) { ~Oll ; / ( : ( ~ ( ) lllll , O ( I \ [ 'or ( ~il , \ [ I ( ~l '' ; Is :1 I ) h ( qlOlllOIIOll ( ) I ' ( : o sele { Mou I ) et , we ( ~ll t.wo iNd ( ~pelldenl.ly i ( leni.ilied morl } hological mfil ; ~ , or as a phonologically. gOVel'lleC\ [ a.ii , o.l'a , l , i ( ) li of l , he M , elii wiien iii ( 'OlilI ) a , lly o\ [ ' I ) arl , icular eliciings. Wil , h ; l goo ( I i/i~lli } , \'el'l ) s , illllOilg which 'l'l ' ) Nl'\ ] l/ , b ) is a case iu i ) oin { , , \ [ I ( ) W~'W ( 71 ' , l , he ( lisl , l'il ) ul , ionai ( lill &gt; ereu { : e 1 ) ei , weeu l , wo &lt; d , el/l , ~ ( 'ai/ I ) e hll ' ( 'rre ( 1 neil , hei ' \ [ 'roln ali.V ~.'V , '~l , C'lllat , i ( ' &lt; lill'er ( 'ii ( : ( ' hi IliOi ; l ) \ ] l ( ) ~yiii , a ( % i ( ! f ( ; al.ure C ( ) lileul , ( wllich a , ~iilgle iii ( ) i'l ) hologi { ! a\ ] Uliil. is l ) url ) orl , e { \ ] 1 , o { : ( ) llVey ) , lP l'l ' ( . } lil ordina.i'y i ) houologi ( 'a.l conM ( l ( w.. al.ioii , ~. I ) araclig~ni~ o\ [ l'er all e ( : oliouli { ' aud elega.lll ; wa ) , ' to ca.l ) l , ilr ( ~ a.II ihe.~e i'acl , .~. 2 ~t ; oiu AILerna { ion and Choice In Morl ) h ( ) iogy , a \ ; erl ) i ) a.ra ( ii &lt; ~in deiine~ a sel , o\ [ ' { 'ells wiiere f'ullv hill ( : { q , ed word \ [ 'oriil~ ape i } ah'ed \vii , h a ( : Ollil ) lel ; e iiiorl ) ho.~ynl ; ax % i ( : \ [ 'c'al , ul'e Sl ) e ( gfi ( 'ai , ic , n relle ( % ing { , he way r ( ~ievanl , { : oiljugal , ioila\ [ ( lillleiisiolls Sli ( : h a~ { , ( ~l\ ] . $ e , I/lOOd , I ) ( 'T~Oll , IllllilI ) el ' , a.nd verl ) cla.ss ( : oiiil ) iiie. I ) ara.di~lii~ ( le \ [ inea dittlenMon of ulorl ) llologi ( : ai aiialy , qs \vflich Clll , ~ a ( : i'o , ~s l , lle ~l ) e { : i , i : lllii oF i ) ol , h Ima ( lil , ioiull illOrl ) helli ( `` ail ( I I ) r ( ) ce~-I ) a~ed ; /S , ~lllill ) l.lOll~ : , ~1,1 ' ( '~ &lt; ~ i , ~ i ; li { I t ) l/ l , he way uioPl ) hol { igi { : a.l l/lOallill &lt; ~ is s , y , ~lelll ; ll.i ( ! aliy , '4 ( , Pil ( q , lll ' ( `` ( I ill lailgtlage , ill ) illi/t.l. ( ! i ' wliat , forula.I illeali~ ape l ) ul , 1 , o iiso I , o Nil iiia ~lVell i ) al ' ; /diglm/l , i ( ' ( : ell ( Mai , l , hews 1974 , ( lai'~l , air , ~ 1 ! ) ~8 , 19. ( ) 7 , ( ! ai 'M , air~ and ~ , ( 'lll\ ] ) ( W~el ' \ [ { ) ~ , ( ial ( ler 1 { ) 8 { ) , ~LUlii I ) 1991 ) . I~or ( : ( ) nveuieu { ! ( ! , \v ( ~ will hePeafl , er liSSilll~O l.ha.i , II~aHa.n l ) ara ( liglnai , i ( ' { : ells al'0 tilled ill LhPough 8 I ) asic : ally ( ! Oli ( : a.leil ; i.l ; ive ol ) ePat.ion , l , railiiig ali hi\ [ le ( % iona.\ [ eUdili~ al'l , er a verl ) 77 stem. I , 'or a giwm verb class , ~m.eh inHeet , ional ending presupposes and is presupposed by any other pa.radigmatically-related inllectional ending ( Wurzel 1989 ) . This is illustrat¢~d in tile exampies of the present indicatiw~ paradigm of irregular Italian verbs given below , where a bhmk separates the stem from the inllectiona.I encling a . i ) l'¢~s¢~nI ; illdicat iv &lt; ~ `` I'I ' ) NI , ) I~I ? V ~N { I { H 2s tte~z ~ 2s u~ 2p tc~z ctc 2p uc~t it , 3p tl : ~lg oTzr , ? .p ~c~l ! \ ] ~mo DO L~ I~ I , I ( ' ( ) ( 71 , I I , ; Hl , ; Is dolt , . , 15 colt , , 2s d~tol * 2 , cogl ? , s daol ~ 3s cogl e Ip d , . , I iaTl~o lp , :o91 iamo 3p &lt; tolg o~.o ? p colt o~o Besides the intralexelnie eharacte| ' of Wurzel 's presupl ) osition relation l : m.radigms also exhibit systematic interlexemic redundancies in stem selection ( Pirrelli and Federici tg.q/I ) . '\ [ 'li ' , NE\ ] Zl.E and VENH { .I ! 3 ( 'come ' ) , in spite of I : heir 1 ) elonging to difDrent eon~iugationa.\ ] classes ( resl ) ~ , ctively second and third ) , exhibit an identical I ) attern of stem alternation , as illustrated in the Imradigmarie sclw.ma of grid ? 1 below , where sanlencss of index expresses sa.meness of stem form a.tion : The grid is also a.pplicable to the present indicative of I ) OLEIU ' ; ( 'hurt ' ) given above , independently of | ; Ire substantial variatio|l in phonological content between the stems of 1 ) OLEI- { E and those of VF , NIIU ) ; and TEN\ ] ) ; I { I ) '. When things are o1 &gt; served el .</sentence>
				<definiendum id="0">illllOilg</definiendum>
				<definiens id="0">When things are o1 &gt; served el</definiens>
			</definition>
			<definition id="1">
				<sentence>ount for \ [ , he distribution of an entire class of verb alternants , when the following simplitied General Indexing Convention ( ( HC ) is resorted to : a ) assign thc d @ ult index rl to the ste 'm , occurring in the ( : ell of the second-person plural of present indicatiw ' , ; b ) assign r2 to the stem of the first-person singular of present indlcatiw~ ; c ) assign r3 to the stem of the second-person singular of present indicative .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">the stem of the first-person singular of present indlcatiw~ ;</definiens>
			</definition>
			<definition id="2">
				<sentence>h ( ) ws Ul ) .</sentence>
				<definiendum id="0">h ( )</definiendum>
			</definition>
			<definition id="3">
				<sentence>del'e ' , PIION is assign ( &lt; \ [ ( , wo sl , &lt; ' .</sentence>
				<definiendum id="0">PIION</definiendum>
				<definiens id="0">assign ( &lt; \ [</definiens>
			</definition>
</paper>

		<paper id="2118">
			<definition id="0">
				<sentence>ViewGen is a nested attitude model which constructs intensional environments to model the attitudes of other agents .</sentence>
				<definiendum id="0">ViewGen</definiendum>
			</definition>
			<definition id="1">
				<sentence>A stereotype is a collection of attitudes which are generally applicable to a particular class of agent .</sentence>
				<definiendum id="0">stereotype</definiendum>
				<definiens id="0">a collection of attitudes which are generally applicable to a particular class of agent</definiens>
			</definition>
</paper>

		<paper id="2141">
			<definition id="0">
				<sentence>Total IBM1 10 2.610 6.233 16.267 IBM2 5 2.443 4.003 9.781 HMM 5 2.461 3.934 9.686 IBM1 10 4.373 10.674 46.672 IBM2 5 4.696 6.538 30.706 ItMM 5 4.859 5.452 26.495 The Verbmobil Corpus consists of spontaneously spoken dialogs in the domain of appointment scheduling .</sentence>
				<definiendum id="0">Verbmobil Corpus</definiendum>
				<definiens id="0">consists of spontaneously spoken dialogs in the domain of appointment scheduling</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>A particularly convenient implementation of discourse referents is to represent them as objects in the Common Lisp Object System , with slots which encode the following information parameters ( where ADJUNCT and EMBED indicate whether a discourse referent was observed in either of the two syntactic contexts discussed above ) : TEXT : text form TYPE : referential type ( e.g. , REF , PRO , RFLX ) AGR : person , number , gender GFUN : grammatical function ADJUNCT : T or NIL EMBED : T or NIL POS : text position Note that each discourse referent contains information about itself and the context in which it appears , but the only information about its relation to other discourse referents is in the form of precedence relations ( as determined by text position ) .</sentence>
				<definiendum id="0">ADJUNCT</definiendum>
				<definiens id="0">contains information about itself and the context in which it appears</definiens>
				<definiens id="1">as determined by text position )</definiens>
			</definition>
			<definition id="1">
				<sentence>The interpretation procedure involves moving through the text sentence by sentence and interpreting the discourse referents in each sentence from left to right .</sentence>
				<definiendum id="0">interpretation procedure</definiendum>
				<definiens id="0">involves moving through the text sentence by sentence and interpreting the discourse referents in each sentence from left to right</definiens>
			</definition>
			<definition id="2">
				<sentence>Individual salience factors are associated with numerical values ; the overall salience , or `` salience weight '' of a COREF is the sum of the values of the salience factors that are satisfied by some member of the COREF class ( note that values may be satisfied at most once by each member of the class ) .</sentence>
				<definiendum id="0">Individual salience factors</definiendum>
				<definiendum id="1">COREF</definiendum>
				<definiens id="0">the sum of the values of the salience factors that are satisfied by some member of the COREF class</definiens>
			</definition>
			<definition id="3">
				<sentence>Our salience factors mirror those used by ( Lappin and Leass , 1994 ) , with the exception of Poss-s , discussed below , and CNTX-S , which is sensitive to the context in which a discourse referent appears , where a context is a topically coherent segment of text , as determined by a text-segmentation algorithm which follows ( Hearst , 1994 ) .</sentence>
				<definiendum id="0">CNTX-S</definiendum>
			</definition>
			<definition id="4">
				<sentence>This parallelism heuristic differs from a similar one used by the Lappin/Leass algorithm , which rewards candidates whose grammatical function is identical to that of an anaphor .</sentence>
				<definiendum id="0">Lappin/Leass algorithm</definiendum>
				<definiens id="0">rewards candidates whose grammatical function is identical to that of an anaphor</definiens>
			</definition>
</paper>

		<paper id="2143">
			<definition id="0">
				<sentence>For the generation of pronouns an extension to the Centering Model ( Grosz et M. , 1995 ) has been defined that captures how the rhetorical evolution of the discourse influences the flow of attention of the reader .</sentence>
				<definiendum id="0">Centering Model</definiendum>
				<definiens id="0">captures how the rhetorical evolution of the discourse influences the flow of attention of the reader</definiens>
			</definition>
			<definition id="1">
				<sentence>Therefore , the rhetorical relation that links e. and d. signals that among the elements in Cf ( c ) the envelope is the best candidate to be the primary focus of the following sentence d. This means that the rhetorical information ( 'an `` project '' the default ordering of the elements in / ; he potential focus list Cf ( c ) onto a new order tha~ reflects more closely the content progression .</sentence>
				<definiendum id="0">envelope</definiendum>
				<definiendum id="1">rhetorical information</definiendum>
				<definiens id="0">links e. and d. signals that among the elements in Cf ( c ) the</definiens>
			</definition>
</paper>

		<paper id="2177">
			<definition id="0">
				<sentence>Depending on the viewpoint , DB-MAT can be compared to various approaches and/or systems : ( i ) encoding of term 's meaning : to lexicons and tennbanks ( see 2.1 ) and knowledge based termbanks ( see 2.2 ) ; ( ii ) generation of explanations allowing follow-up questions and cl , 'u'ifications e.g. IDAS ( see 2.3 ) ; ( iii ) NL generation from CGs ( Bon96 , AnBo96 ) .</sentence>
				<definiendum id="0">DB-MAT</definiendum>
				<definiens id="0">i ) encoding of term 's meaning : to lexicons</definiens>
			</definition>
			<definition id="1">
				<sentence>Briefly we mention : ( 1 ) COGNITERM ( MSBE92 ) : the term meaning is represented in a frmne-like structure which is accessible by names of concepts or their characteristics .</sentence>
				<definiendum id="0">COGNITERM</definiendum>
			</definition>
			<definition id="2">
				<sentence>There are similarities between DB-MAT and other NL generation systems , e.g. IDAS which produces technical documentation l~om a domain KB and linguistic and contextual models ( RMLe95 ) .</sentence>
				<definiendum id="0">e.g. IDAS</definiendum>
				<definiens id="0">which produces technical documentation l~om a domain KB and linguistic and contextual models ( RMLe95 )</definiens>
			</definition>
			<definition id="3">
				<sentence>The system generates hypertext nodes ( both text and links ) with relevant follow-up questions .</sentence>
				<definiendum id="0">hypertext nodes</definiendum>
			</definition>
			<definition id="4">
				<sentence>The following particularities however display tile differences between the systems : ( 1 ) IDAS is a fullscale application , its KI , -ONE like KB contains more domain information and the proper system evaluation can be performed ; ( 2 ) the hypertext links act as visuM hints for the available relevant information , while the user should `` guess '' that in DB-MAT ; ( 3 ) the DB-MAT KB pretends to be arbitrary , i.e. we investigated the integration of arbitrary domain KB into applications in the humanities ; as a contrast the IDAS KB contains fixed number of ( rusk-adequate ) `` conceptmd relations '' mtd supports fixed query types ; ( 4 ) the systems cert~dnly have diflcreut interface design oriented towards dilfbrent go~ds and user types .</sentence>
				<definiendum id="0">IDAS</definiendum>
				<definiens id="0">a fullscale application</definiens>
			</definition>
			<definition id="5">
				<sentence>DB-MAT is a knowledge-bm ; ed TWB providing linguistic as well as domain knowledge support .</sentence>
				<definiendum id="0">DB-MAT</definiendum>
				<definiens id="0">a knowledge-bm ; ed TWB providing linguistic as well as domain knowledge support</definiens>
			</definition>
			<definition id="6">
				<sentence>The KB consists of concepts , a type hierarchy and conceptual graphs .</sentence>
				<definiendum id="0">KB</definiendum>
				<definiens id="0">consists of concepts , a type hierarchy and conceptual graphs</definiens>
			</definition>
			<definition id="7">
				<sentence>Given a highlighted term ( i.e. its KB concept ) , and the user request for domain knowledge , the QM searches the KB on the fly and extracts all relevant facts according to the conceptual relations .</sentence>
				<definiendum id="0">QM</definiendum>
				<definiens id="0">searches the KB on the fly and extracts all relevant facts according to the conceptual relations</definiens>
			</definition>
			<definition id="8">
				<sentence>In order to produce a coherent explanation , EGEN orders the CGs by applying a suitable text organisation schema ( AnBo96 ) definition , similarity or difference ( similar to those in McKe85 ) .</sentence>
				<definiendum id="0">EGEN</definiendum>
			</definition>
			<definition id="9">
				<sentence>Additionally , EGEN keeps all uttered concepts in a stack and later refers to them using a definite article or a pronoun .</sentence>
				<definiendum id="0">EGEN</definiendum>
				<definiens id="0">keeps all uttered concepts in a stack</definiens>
			</definition>
			<definition id="10">
				<sentence>The QM extracts the supertype and the conceptual relations ATTR , CHAR and RESULT ( AnBo96 ) .</sentence>
				<definiendum id="0">QM</definiendum>
			</definition>
</paper>

		<paper id="1075">
			<definition id="0">
				<sentence>JANUS is a multi-lingual speech-tospeech translation system designed to facilitate communication between two parties engaged in a spontaneous conversation in a limited domain .</sentence>
				<definiendum id="0">JANUS</definiendum>
				<definiens id="0">a multi-lingual speech-tospeech translation system designed to facilitate communication between two parties engaged in a spontaneous conversation in a limited domain</definiens>
			</definition>
			<definition id="1">
				<sentence>JANUS is a multi-lingual speech-to-speech translation system designed to facilitate communication between two parties engaged in a spontaneous conversation in a limited domain .</sentence>
				<definiendum id="0">JANUS</definiendum>
				<definiens id="0">a multi-lingual speech-to-speech translation system designed to facilitate communication between two parties engaged in a spontaneous conversation in a limited domain</definiens>
			</definition>
			<definition id="2">
				<sentence>The JANUS System is a large scale multi-lingual speech-to-speech translation system designed to facilitate communication between two parties engaged in a spontaneous conversation in a limited domain .</sentence>
				<definiendum id="0">JANUS System</definiendum>
				<definiens id="0">a large scale multi-lingual speech-to-speech translation system designed to facilitate communication between two parties engaged in a spontaneous conversation in a limited domain</definiens>
			</definition>
			<definition id="3">
				<sentence>The parser uses a disambiguation algorithm that attempts to cover the largest number of words using the smallest number of concepts .</sentence>
				<definiendum id="0">parser</definiendum>
				<definiens id="0">uses a disambiguation algorithm that attempts to cover the largest number of words using the smallest number of concepts</definiens>
			</definition>
			<definition id="4">
				<sentence>Acceptable is the sum of `` Perfect '' and `` OK '' sentences .</sentence>
				<definiendum id="0">Acceptable</definiendum>
				<definiens id="0">the sum of `` Perfect '' and `` OK '' sentences</definiens>
			</definition>
</paper>

		<paper id="2152">
			<definition id="0">
				<sentence>Definition 1 Let A ( P ) be a formula , and ( D ( P ) be a tuple of formulas wh , ich , is brokcn into 4il ( P ) ,4,2 ( P ) ... .. ( 1 ) ~ : ( P ) where P is a , tv , ple of predicates used in these formulas .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a , tv , ple of predicates used in these formulas</definiens>
			</definition>
			<definition id="1">
				<sentence>f ( eq , is , time , act , actor , object , recipient , device , sub j , in_the_sentence ) .</sentence>
				<definiendum id="0">f ( eq</definiendum>
				<definiens id="0">act , actor , object , recipient , device , sub j</definiens>
			</definition>
			<definition id="2">
				<sentence>el ( act ( el , Have ) A actor ( el , a ) Aobject ( e~ , o ) A time ( ca , i + 2 ) ) We assmne that ~ is a formula which should ; be satisfied in the first place , O~ in the second place , ( pa ~ in the third place , q54 in the fourth place and • ) in the fifth place .</sentence>
				<definiendum id="0">el ( act</definiendum>
				<definiendum id="1">actor</definiendum>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>S-forms , which we introduce in this paper , are a scoped vet sion of U-fnrms , and we define a compositional semantics mechanism for them .</sentence>
				<definiendum id="0">S-forms</definiendum>
				<definiens id="0">a compositional semantics mechanism for them</definiens>
			</definition>
			<definition id="1">
				<sentence>The S-form representation ( Scoped dependency form ) , which we introduce here , is an extension of U-form notation which makes scope explicit , by allowing dependents to be ordered relative to one another .</sentence>
				<definiendum id="0">S-form representation</definiendum>
				<definiens id="0">an extension of U-form notation which makes scope explicit</definiens>
			</definition>
			<definition id="2">
				<sentence>One then considers tile predication tree T i made by forming the collection of edges ( A , LX ) where I , is positive and either ( A.L , X ) or ( X.inverse ( L ) , A ) is a predication edge of A. Each predication tree denotes a predicate-argtnnent rehition among IJ , +fornl nodes .</sentence>
				<definiendum id="0">predication tree</definiendum>
				<definiens id="0">considers tile predication tree T i made by forming the collection of edges ( A , LX ) where I</definiens>
				<definiens id="1">a predication edge of A. Each</definiens>
			</definition>
			<definition id="3">
				<sentence>\ ] Idt { ' 1// `` ~ 2 / \ , / ) ( 'tg'r WOllldll - &lt; Q d/ \~ '' every like , /\ &lt; .jolm m~t Figure 3 : A dilf0renl t J-form ( ) he can take advantage of such paral ) hrasing potential in cerlain cases of synlaclJc divergence belween languages , l : or instance , French does not have a syn tactic equivalent to the dative-lnoven/etlt + passive configuration o1 : Rachel was given a book by Claude so that a direct syntactic translation is not possible. However , at tile level of U-form , this sentence is equivalent to the French sentence : Claude a donne un livre ~t Rachel and this equivalence can he exploited it ) provide a translation of the first sentence. One serious problenl with 1.\ ] . \ [ ornis , however , is tilat they do not have unainbiguous readhigs in cases where the rehliive scopes of constituents can result in clifl'erent semantic ii~terpretations. So , in the case of sen fence ( S l ) , tile two readings : `` it is not the case thai John likes every woman hated by Peter '' , and 'Lh ) tl.n dislikes every woman thai Peter hates '' are not distinguished by tile t l-l : oi+nl Of l '' ig+ 1 , INTI , I ( ) I ) UCIN ( ; SC ( ) I'E Lefs consider the trec represented in Iqg. 4. The only differeiice hetween this tree aml the l , Jform of l : ig. l is thai the nodes of our new tree are considered ordered whereas they were considered tinordered in the I ! -lorm+ The convention is now that ( tepetlttent sister nodes are interpreted : is having ttil'l\ ] 2retlt scopes , with llarrower scope correspondillg to a position iilore It ) tile right. The tree of l : 'ig. 4 can he glossed in the following way : like //.-J ! 2 t .jolm ~u~t W ( ) lll ( lfl ( q'dl'V \ ] lg.ll ( `` lT , # H't '' I : igure 4 : Inhoducing scope hy ordering the nodes. John , it is not the case that he likes every woman that Pe+ ter hates I f we consider tile six l ) mmulalions of lhe nodes under like. we can produce six differenl scopings. Be cause John teleis to an individual , not a quantified NP. these six pernmtations really corrcsl ) ond to only the two interl ) relaiiens given ahove. The tree of Fig. 4 corresponds to the lirst of Ihese interpretations , which is the preferred interpretation Ik ) l sentence ( S I ) . Our discussion of scope being represented by node order has been infornml so far. In order to nlake it \ [ 'Oi'lllal , we need to encode our representation into a binary-tree fornmt ell which a compositioiml senlan tics can he delined. To tie that. in a lirst step we rephtce the at'gunlent nunibers of l : ig. 4 hy exl ) lici ! argument haines ; ill a seColld slep we encode the resulting or dered mary free inh ) a himuy forumt which makes explicit the order in which dependents are incorlmrated inlo their head. S-I '' ORMS Consider tile mary tree of Fig. 4. For any node A in this tree , take the set of predication edges associated with A , that is the set of edges ( a , +i , B , ) and ( Bi , i , A ) . By renaming each such node A into A ( XI ... . X , + ) . where X I ... .. X , are hesh identiliers , and by renaming each such htbel +i ( resp. +i ) into +X , : ( resp. -X , : ) , one obtains a flew tree where argunmnl numbers have been replaced by argument haines. \ [ : or instance the previous representation now becomes tile tree of l '' ig. 5. This representation is called a scopeU depemh'm3 ' , lotto , or Sqbrm. BINARY TREE FNCODING OF S-FORMS : BFORMS In order to encode tile ordered n-ary tree into a binary tree , we need to apply recursively the transfotnmtiou ilhlstrated in Fig. 6 , which consists in forming a `` head-line '' , projecting in a north-west direction from tile head 11 , and in `` attaching '' to this line `` dependent-lines '' D~ , D2 ... .. 1 ) , , , with l- ) l tile right257 like ( ll,12 ) john not ( nl ) woman /\+ evetlv hate ( h l , h2 ) I +hl peter Figure 5 : An S-form. most dependent ( narrowest scope ) and D~ , the leftmost dependent ( widest scope ) in the original tree. ; \ [ l , n I+ ~\z \ , I , I , / \ `` \ , l ) n D2 151 \ ] 511 //l\ , N\ \ ] +I\ ] /i `` 1 `` \'N\ // , , //~'\\ 1,2// / ) \ , // I l , / ... . , , D2 1 `` \ D1 Figure 6 : The translbrmation between S-forms and Bforms. Applying this encoding to our example , we obtain the binary tree of Fig. 7 , which is called a B-form. The B-form makes explicit the order of incorporation of dependents into the head-line. By permuting several dependent-lines along their head-line , this incorporation order is changed and gives rise to different scopings. S-forms and B-forms are completely equivalent representations. Cle~ly , the encoding , called the Sform/B : fi~rm encoding , which has just been defined is reversible. The S-form is more compact , and makes the dependency relations more conspicuous , whereas the B-form makes the compositionality more explicit. WELL-FORMEDNESS CONDITIONS ON BFORMS AND S-FORMS Stm'ting fromthe U-form and enriching it , we have informally introduced the notions of S-form and B-form. We now define them formally. We start by giving a rect , rsive definition of IBFs ( incomplete B-forms ) , that is , B-forms which may contain unresolved flee variables. We use the notation ( ( D , Label ) , H ) the labelled binary tree obtained by taking H as the right subtree , D as the left subtree , and by labelling the left edge with Label. We , also use the notation fv ( IBF ) for the set of the free variables in IBF. DEFINITION OF INCOMPLETE B-FORMS not ( hi ) +lf.~ ~N , . d~ t ev man like ( ll , 12 ) peter hate ( h l , h2 ) Figure 7 : A B-form. with the set of free variables fv ( N ) = { x I , .. , xn } ; and x ~ fv ( H ) then H'= ( ( D , +x ) , H ) is an IBF with fv ( ll ' ) = fv ( D ) U fv ( H ) \ { x } ; and x C fv ( D ) then H'= ( ( D , -x ) , H ) is an IBF with fv ( H ' ) = fv ( D ) U fv ( H ) \ { x } ; joint , then H'= ( ( D , det ) , H ) is an 1BF with fv ( H ' ) = fv ( D ) U fv ( H ) . DEFINITION OF B-FORMS A B-form is an IBF with an empty set of free variables. The notion of S-form cart now be delined through the nse of the S-form/B-form encoding. DEFINITION OF S-FORMS A S-form is an of demd labelled n-ary tree which can be obtained from a B-form through the inverse application of the S-form/Bform encoding. It can be easily verified that the representation of Fig. 7 is indeed a B-form , and , consequently , the representation of Fig. 5 is a valid S-form. More generally , it can be easily verified that enriching a U-form by ordering its nodes , and then replacing argument variables by argument names always results in a valid Sform ) tThe converse is not true : not all S-forms can be obtained in this way from a U-form. For instance , there exists a S-fonn corresponding to the prefelTed reading for `` Fido visited most trashcans on every street '' , which has `` every street '' 258 We now describe the interpretation process on B-fl ) rms. lnlerpretation proceeds by propagating semantic translations and their types bottom-up. The first step consists in typing the leaves of the tree , while keeping track of the types of fl'ee variables , as in Fig. 8. +/ ( / \ , , / 5 '' , / / ~ \\ john. '' e '+// '' ~ `` '' `` , , , • \ .. till : t } // \\ ttot ( *t 1 ) : t / / '' / &lt; , det/ '' \ , / \ / h2 / \\ , el'crY. ' ? i \ ] +l ) -+ ( u-~t ) ~t / `` \ `` \ ./ \\ , +h ; / '' ' , ,. , \\ , / `` '' `` , , , { hhe , h2 : c } peter : e haleChl , h2 ) . ' t `` \\ , `` -. , `` \ , \ } voDt ( in : U ~1 `` \ , , , { tlm , 12 : e } like ( ll,121 : ! Figure 8 : Typing the leaves. The flee wu'iables and their types are indicated in brackets. The types given to the leaves of the tree are the usual functional types formed starting with e ( entities ) and t ( truth values ) . In the case where the leaf entity cot &gt; tains flee variable arguments , the types of these free variables are indicated , and the type of the leaf takes into account the fact that these free variables have already been included in the functioned form of the leaf .</sentence>
				<definiendum id="0">'Lh</definiendum>
				<definiendum id="1">B-form</definiendum>
				<definiendum id="2">S-form</definiendum>
				<definiens id="0">in the case of sen fence ( S l ) , tile two readings</definiens>
				<definiens id="1">A B-form. with the set of free variables fv ( N ) = { x I , .. , xn }</definiens>
			</definition>
			<definition id="4">
				<sentence>Montague gl+anlmar hits the general requirement that only closed lambda-tetms ( lanibda terms containing only bound variables ) are composed together .</sentence>
				<definiendum id="0">Montague gl+anlmar</definiendum>
				<definiens id="0">hits the general requirement that only closed lambda-tetms ( lanibda terms containing only bound variables</definiens>
			</definition>
			<definition id="5">
				<sentence>The lflst argtuncnt indicates tile type of composition ( ++ ' fl ) r complement incorporation. ``</sentence>
				<definiendum id="0">lflst argtuncnt</definiendum>
				<definiens id="0">indicates tile type of composition ( ++ ' fl</definiens>
			</definition>
			<definition id="6">
				<sentence>++let '' for deter+miner incorporation ) : the second argument is of the Iklrln Lelt : l+eftTypc , where Left is the left translation entering the composition , and LeftTypc is its type : similarly , the second argument Right : RightType corresponds to the right subtree entclin ~r~ .</sentence>
				<definiendum id="0">Left</definiendum>
				<definiendum id="1">LeftTypc</definiendum>
				<definiens id="0">the left translation entering the composition , and</definiens>
			</definition>
</paper>

		<paper id="2193">
			<definition id="0">
				<sentence>We have constructed the IPA Lexicon of Basic Japanese Nouns ( IPAL-BN ) , which has a hierarchical structure based on the syntactic and semantic properties of no , ms. In our lexicon , each lexical entry consists of subentries , and subentries have semantic property information .</sentence>
				<definiendum id="0">lexical entry</definiendum>
				<definiens id="0">constructed the IPA Lexicon of Basic Japanese Nouns ( IPAL-BN ) , which has a hierarchical structure based on the syntactic and semantic properties of no</definiens>
				<definiens id="1">consists of subentries , and subentries have semantic property information</definiens>
			</definition>
			<definition id="1">
				<sentence>Tile Information-technology Promotion Agency ( IPA ) i has compiled the IPA Lexicon of the Japanese Language for Computers , Basic Japanese Verbs ( IPAL-BV ) ( 1987 ) and Basic Japanese Adjectives ( IPAL-BA ) ( 1990 ) .</sentence>
				<definiendum id="0">Tile Information-technology Promotion Agency</definiendum>
				<definiens id="0">compiled the IPA Lexicon of the Japanese Language for Computers</definiens>
			</definition>
			<definition id="2">
				<sentence>The H'AL-BN consists of 1,081 lexical entries .</sentence>
				<definiendum id="0">H'AL-BN</definiendum>
			</definition>
			<definition id="3">
				<sentence>A subentry consists of subentry information and several pieces of semantic property information .</sentence>
				<definiendum id="0">subentry</definiendum>
				<definiens id="0">consists of subentry information and several pieces of semantic property information</definiens>
			</definition>
			<definition id="4">
				<sentence>The subentry information contains syntaeti ( : , semantic , and morphoh ) gical information co , ninon to all parts of the subentry ( each selnantie property ilfforlnation section ) .</sentence>
				<definiendum id="0">subentry information</definiendum>
			</definition>
			<definition id="5">
				<sentence>Tiros a noun is consi &lt; lere &lt; l here to have various aspects depending on the predicates used in the sentence containing the noun. These aspects are called semantic properties ( Aoyama , 1995 ) . Instead of subdividing the lexical entry into multiple subentries , we categorized the regular collocations in each subentry in terms of semantic properties. For exan2ple , let , us take the word ha 'tooth /teeth ' which has three semantic t ) rot ) erties. In our notation , the semantic properties are labeled by three letters in square brackets. The phrases ha-o migaku 'l ) rush one 's teeth ' and ha-o nuku 'pull one 's tooth ' refer to tooth as a concrete object \ [ CON\ ] . And the phrases ha-ga haern 'cut teetlf and ha-ga nukeru 'h ) se teeth ' imply ( natural ) phenomena \ [ PHE\ ] , while the t ) hrases ha-ga jobu-da 'tlave sound teetlF and ha-ga guragurasuru 'a tooth feels h ) ose ' single out a condition of teeth from their potential conditions \ [ POT\ ] . Most , existing Japanese dictionaries merely enumerate various usages. But clarifying the semantic relations between those usages is important. For exan2ple , the noun tamago 'egg/spawn ' has three senses : tanuulo 'egg/spawn ' 01. aIl ol ) jec~ covered wit , It a hard shell or a membrane , produced by a female animal. Kingffo-ga tarnago-o urtda. ( The gohlfish spawm : d. ) 02. a heat 's egg ( i.e. , some kind of food ) . Hana~ : o-wa tamago-o ichi pa~ : ku katta. ( tlanako bought 1 dozen eggs. ) 03. at person ~tt the Imginning of his/her carter. Hanal~o-wa isha-no tam , a.qo-da. ( Hanako is a dot : for lit the inaking. ) A hen 's egg ( 02 ) ix one type of object that ix covered wittl a hard shell ( 01 ) , and ( 03 ) is a metaphor with respect to the relation between hen 's egg ( 02 ) and hen. Our problem was to explicidy ( lescribe these kinds of relations between meanings. Ill the literat , ure , several attempts \ ] lave already been made to analyze such senlantie relationships 2. Yanlanasld ( 1995 ) , among others , 1 ) oints out that apt ) re ( taring such special cognitive devices as metaphor and metonymy is the 2Ulhnann , 1969 ; Lakoff and Johrlson , 1980 ; Kunihire , 1982 ; Yamanashi , 1995. 1083 key to understanding polysemy. We emt ) loy this method for sI ) ecifying the kind of relationship l ) etween subentries. Currently we note three types of relations : • Metaphor , based on similarities • Metonymy , based on various ( : ontiguitics • Synecdoche , l ) ase.d oil the relation between m ( mtl ) er mM ( : ~0 ; ( : gory Metaphor is the similm-ity-based instrument for exte , Ming the meaning of words. At the outset met , al ) horical expressions are temporarily used figures of speech. However , some metaphors conm to be fixed and pass into everyday use. Let us take some examples : h.on-no mushi 'a worm of books ' ( a person who is crazy about reading ) , arasof no tane 'a seed of argument ' ( a cause for argument ) , and kotoba-no kabe 'a wall of words ' ( a language barrier ) . We regard these ext ) ressions as flfll-fledged usages of dm nouns , and accordingly descrihe them in the IPAL-BN , while telnporal usages are not considered fl ) r description in our lexicon. Metonynly is an instrument for employing a word to refer to something that distinct front , hut , is associated in some way with , the original referent of the. word. Tyi ) ieal examples are Nabe-ga oishi , `` The dish is nice ; '' and Ano kyateh &amp; wa ( kata-o shiteiru , `` The catcher has nice shoulders ( The catcher has a powerflfl throw ) . '' In the former , the reference has shifted from the container to the content , and in the latter , the refl~.rence has shifted Dora the part of the body to its flmetion. Synecdoche is the instrument that take.s the name of a category to stand for one of its members or taking the. name of one ineml ) er to stand for the whole category , as shown in the above hen 's egg example : a hen 's egg is one kind of object that is covered with a hard shell. It is lint ) or ( ant not to confitse the colmoted relatio , l of synecdoche with the contiguity relation \ [ part\ ] -\ [ whole\ ] of lltetollyttly. Let 11s take all ( 'xample of the relation \ [ part\ ] -\ [ whole\ ] to show the difference : te carln/hmtd ' 0l. upl ) cr limbs of a hmnan ( th ( : ( mtire.ty , consisting of ~rIn ~tiI ( l hmtd ) . `` Ha.na~ : o-wa ryShS-no te-o hirogcta. '' ( Hmmko st ) r ( .'~t ( 1 her arms. ) 02. the ( : ml of a person 's arm ( tit ( : part consisting only of the hand ) . ' : Hanako-wa tc-o tataita. '' ( Ilanako ( : lal ) I ) ( 'd her ttands. ) We do not consider the. \ [ part\ ] -\ [ whole\ ] relation hetween the arm ( 01 ) and t ; he hand ( 02 ) to be all instance of syne ( : doehe. Let us compare this example of te 'arm/hand ' with the examph ; of tamago 'egg. ' Besides a hen 's egg , there are tmmy other sorts of eggs , such as a turtle 's egg , a pigeon 's egg , mid a swallow 's egg. By contrast of the arln as a whole , only the hand receives the de. ' , , } igiiation te. For e.xamph'. , we do not also express the elbow as te 'arm/hand. ' Hence. we consider the contiguity relation \ [ part\ ] -\ [ whole\ ] to be one type of metonymy. We distinguish it fi'om the member-category relation of synecdoche a. a.a Method In this section , we will show sonm examples of description. Some metaphorical expressions need some nlodifier and others do ttt ) t. kata 'shoulder ' 01. ftana~ : o-wa kata-o sukumcta. ( Hanako shrugged her shouhh : rs. ) 02. Sono uwagi-'no kata-wa 5l : \ [ . ( Thai ; jacket has wide shoulders. ) 03. Sono yam , a-no t : ata-ni yamagoya-ga aru. ( There is a lodge on that mountain shonhtcr. ) ( ( } 1 ) is a non-metaphorical use. ( ( } 2 ) and ( 03 ) are metaphorical uses. One may notice that , ( 02 ) can he used without y @ zku-no 'of clothes ' , but ( ( 13 ) requires yama-no 'mountain-. ' We have the impression that ( 02 ) is conventionalized. We. classify metaphorical usages into two types : conventionalized and novel. To the first group we assign expressions that can be used without being modified. Such an expression is indicated by `` A = &gt; B. '' The second group is indicated by `` A • `` . &gt;</sentence>
				<definiendum id="0">metonymy</definiendum>
				<definiendum id="1">Metaphor</definiendum>
				<definiens id="0">aspects depending on the predicates used in the sentence containing the noun. These aspects are called semantic properties</definiens>
				<definiens id="1">the instrument that take.s the name of a category to stand for one of its members or taking the. name of one ineml</definiens>
			</definition>
			<definition id="6">
				<sentence>Kate-no ashi-wa 6kil ( Ih : has big fi .</sentence>
				<definiendum id="0">Ih</definiendum>
				<definiens id="0">has big fi</definiens>
			</definition>
</paper>

		<paper id="2181">
			<definition id="0">
				<sentence>NKRL is a conceptual language which intends to provide a normalised , pragmatic description of the semantic contents ( in short , the `` meaning '' ) of NL narrative documents .</sentence>
				<definiendum id="0">NKRL</definiendum>
				<definiens id="0">a conceptual language which intends to provide a normalised , pragmatic description of the semantic contents ( in short , the `` meaning '' ) of NL narrative documents</definiens>
			</definition>
			<definition id="1">
				<sentence>NKRL ( Narrative Knowledge Representation Language ) aims to propose some possible , pragmatic solutions for the set up of a standardised description of the semantic contents ( in short , the `` meaning '' ) of natural language ( NL ) n , 'uTative documents .</sentence>
				<definiendum id="0">NKRL ( Narrative Knowledge Representation Language )</definiendum>
				<definiens id="0">aims to propose some possible , pragmatic solutions for the set up of a standardised description of the semantic contents ( in short , the `` meaning '' ) of natural language ( NL ) n , 'uTative documents</definiens>
			</definition>
			<definition id="2">
				<sentence>NKRL is a fully implemented language ; the most recent versions have been rcalised in the frmnework of two European projects : NOMOS , Esprit P5330 , and COBALT , LRE P61011 .</sentence>
				<definiendum id="0">NKRL</definiendum>
				<definiens id="0">a fully implemented language</definiens>
			</definition>
			<definition id="3">
				<sentence>The descriptive component concerns the tools used to produce the formal representations ( called predicative templates ) of general classes of narrative events , like `` moving a generic object '' , `` formulate a need '' , `` be present somewhere '' .</sentence>
				<definiendum id="0">descriptive component</definiendum>
				<definiens id="0">concerns the tools used to produce the formal representations ( called predicative templates ) of general classes of narrative events</definiens>
			</definition>
			<definition id="4">
				<sentence>With respect now to the arguments , sole 24 ore , milan_ , mediobanca_ ( an Italian merchant bank ) , summoning l , etc. , 'u'e individuals ; financialdaily , special_ , cardirtality_ and several_ ( this last belonging , like some , all_ etc. , to the logical_quantifier intensional sub-tree of II_CLASS ) are concepts , q\ ] ~e attributive operator , SPECIF ( icatiou ) , with syulax ( SPECIF el Pl - .</sentence>
				<definiendum id="0">SPECIF</definiendum>
				<definiens id="0">an Italian merchant bank ) , summoning l , etc. , 'u'e individuals</definiens>
			</definition>
			<definition id="5">
				<sentence>2 , c6 is a binding occun'ence .</sentence>
				<definiendum id="0">c6</definiendum>
				<definiens id="0">a binding occun'ence</definiens>
			</definition>
			<definition id="6">
				<sentence>trigger : `` call '' syntactic condition : ( s ( subj ( rip ( noun xl ) ) ) ( vcl ( voice active ) ( t = x2 = call ) ) ( dir-obj ( np ( modifiers ( adjs x31 ) ) ( noun x3 ) ( modifiers ( pp ( prep about I concerning I ... ) ( np ( noun x4 ) ( modifiers ( pp ( prep of I for ... ) ( np ( noun x5 ) ) ) ) ) ) ) ) ) ) parameters for the template : ( PRODUCE4.12 ( roles subj xl obj ( SPECIF x2 ( SPECI F x3 x31 ) ) +topic ( specif x4 x5 ) ) ( constr x3 assembly_ x31 quality_ x5 modification_procedures ) ) Figure 5 .</sentence>
				<definiendum id="0">SPECIF x2</definiendum>
				<definiens id="0">modifiers ( pp ( prep of I for ... ) ( np ( noun x5 ) ) ) ) ) ) ) ) ) ) parameters for the template</definiens>
			</definition>
</paper>

		<paper id="2135">
			<definition id="0">
				<sentence>Therefore , the HPSG principles admit any kind of combination of totally unrelated signs .</sentence>
				<definiendum id="0">HPSG principles</definiendum>
				<definiens id="0">admit any kind of combination of totally unrelated signs</definiens>
			</definition>
			<definition id="1">
				<sentence>The fronted phrase is a maximal projection with the missing constituents moved to SLASH .</sentence>
				<definiendum id="0">fronted phrase</definiendum>
				<definiens id="0">a maximal projection with the missing constituents moved to SLASH</definiens>
			</definition>
			<definition id="2">
				<sentence>The fronted partial phrase is the filler for a nonlocal dependency which is introduced by their PVPTopicalization Lexical Rule .</sentence>
				<definiendum id="0">partial phrase</definiendum>
			</definition>
</paper>

		<paper id="2203">
			<definition id="0">
				<sentence>It is quite clear that in the above string , DET , N , and V are simple symbols that can easily be encoded as single letters like d , n , v , x and e. Transforming the sequence of morph-codes we get the word dnvxe .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">simple symbols that can easily be encoded as single letters like d , n , v</definiens>
			</definition>
			<definition id="1">
				<sentence>According to Kornai 's hypothesis ( Kornai 1985 ) , any string that can be the beginning of a grammatical string can be completed with k or less terminal symbols , where k is a small integer .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">a small integer</definiens>
			</definition>
			<definition id="2">
				<sentence>SCL limits center-embedding but allows arbitrary deep rightbranching structures ( easily defined by right regular grammars ) , l , eft branching is also limited , but this limitatibn is less pronounced than that of center-embedding .</sentence>
				<definiendum id="0">SCL</definiendum>
				<definiens id="0">limits center-embedding but allows arbitrary deep rightbranching structures ( easily defined by right regular grammars ) , l</definiens>
			</definition>
			<definition id="3">
				<sentence>The GRAM2I J.iX tool takes all thc three parameters ( the S ( 'I , , the maximum string length and the maximum iteration length ) as user-defined ones .</sentence>
				<definiendum id="0">GRAM2I J.iX tool</definiendum>
				<definiens id="0">takes all thc three parameters ( the S ( 'I , , the maximum string length and the maximum iteration length ) as user-defined ones</definiens>
			</definition>
			<definition id="4">
				<sentence>When the last level ( usually the highest ) finished updating the graph , it sends a 'ready for next ' signal to level 0 which starts the next session .</sentence>
				<definiendum id="0">last level</definiendum>
				<definiens id="0">starts the next session</definiens>
			</definition>
			<definition id="5">
				<sentence>We have developed a parser called { -lum0rl : Sl ( that is quite powerful ( even in its present format , without feature structures ) and has several important features : phrase levels , sentence level , etc. ) hanced , even in run-time finE-grained parsing l '' eature I seems important if thcrc is not enough space to add a new engine to an existing lnorphology-ascd application ( e.g. a spell-checker ) , but you must handle sentence-level information , as well ( e.g. a grammar checker ) .</sentence>
				<definiendum id="0">Sl</definiendum>
				<definiens id="0">several important features : phrase levels , sentence level</definiens>
				<definiens id="1">a spell-checker ) , but you must handle sentence-level information</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>The FeasPar architecture consists of neural networks and a search .</sentence>
				<definiendum id="0">FeasPar architecture</definiendum>
				<definiens id="0">consists of neural networks and a search</definiens>
			</definition>
			<definition id="1">
				<sentence>FUNCTION convert ( ) VAR S : set ; C : chunk ; BEGIN S : = empty set ; assign ( S , top_level_chunk ) ; return ( S ) ; END ; PROCEDURE assign ( VAR S : set ; C : chunk ) ; BEGIN P : = chunk_relation ( C ) ; F0R each relation element PE in P BEGIN S ' : = empty set ; include ( PE , S ' ) in S ; S : = S ' ; END ; FOR each feature pair FP in C include FP in S ; F0R each chunk C ' in C assign ( S , C ) ; END ; Figure 3 : Algorithm for converting a parse to a feature structure The chunk'n'label principle has a few theoretical limitations compared with the feature structure formalisms commonly used in unification-based parsing , e.g. ( Gazdar et al. , 1985 ) .</sentence>
				<definiendum id="0">FOR each feature pair</definiendum>
				<definiens id="0">chunk ; BEGIN S : = empty set ; assign ( S , top_level_chunk ) ; return ( S ) ; END ; PROCEDURE assign ( VAR S : set ; C : chunk ) ; BEGIN P : = chunk_relation ( C ) ; F0R each relation element PE in P BEGIN S ' : = empty set</definiens>
				<definiens id="1">a few theoretical limitations compared with the feature structure formalisms commonly used in unification-based parsing</definiens>
			</definition>
			<definition id="2">
				<sentence>In the following , tile three main modules required to produce a chunk parse are described : The Chunker splits an input sentence into chunks .</sentence>
				<definiendum id="0">Chunker</definiendum>
				<definiens id="0">splits an input sentence into chunks</definiens>
			</definition>
			<definition id="3">
				<sentence>The Chunk Relation Finder determines how a chunk relates to its parent chunk .</sentence>
				<definiendum id="0">Chunk Relation Finder</definiendum>
				<definiens id="0">determines how a chunk relates to its parent chunk</definiens>
			</definition>
			<definition id="4">
				<sentence>The parser gets the English sentence : `` i have a meeting till twelve '' The Chunker segments the sentence before passing it to the Linguistic Feature Labeler , which adds semantic labels ( see Figure 4 ) .</sentence>
				<definiendum id="0">Chunker</definiendum>
				<definiens id="0">segments the sentence before passing it to the Linguistic Feature Labeler , which adds semantic labels</definiens>
			</definition>
			<definition id="5">
				<sentence>FeasPar uses a full word form lexicon .</sentence>
				<definiendum id="0">FeasPar</definiendum>
				<definiens id="0">uses a full word form lexicon</definiens>
			</definition>
			<definition id="6">
				<sentence>They all share the same interlingua , ILT , which is a special case of LFG or feature structures .</sentence>
				<definiendum id="0">ILT</definiendum>
				<definiens id="0">a special case of LFG or feature structures</definiens>
			</definition>
			<definition id="7">
				<sentence>The FeasPar architecture consists of two n'tajor parts : A neural network collection and a search .</sentence>
				<definiendum id="0">FeasPar architecture</definiendum>
				<definiens id="0">consists of two n'tajor parts : A neural network collection and a search</definiens>
			</definition>
</paper>

		<paper id="2207">
			<definition id="0">
				<sentence>A world is a structured set of objects which is coherent : the exceptions , change of meanings are taken into account by a change of world .</sentence>
				<definiendum id="0">world</definiendum>
				<definiens id="0">a structured set of objects</definiens>
			</definition>
			<definition id="1">
				<sentence>The intension contains those objects whose representation is supposed valid for speakers and situations related to discourse enunciation aitd to the application dolnain : there exists a consensus between the speakers of the discom : se about these objects , which reflects `` general '' background knowledge ( 77~e dog is a stupid and spiteJ ) U animal ) , The intensional objects are then kinds of `` logical '' concepts in their world .</sentence>
				<definiendum id="0">dog</definiendum>
				<definiens id="0">a stupid and spiteJ ) U animal ) , The intensional objects are then kinds of `` logical '' concepts in their world</definiens>
			</definition>
			<definition id="2">
				<sentence>A functive has the following form : f ( s ; , J ) Where Iis the object described and J is the object with which i is connected by the functivc f The functives of an individual object act as relations between objects .</sentence>
				<definiendum id="0">functive</definiendum>
				<definiendum id="1">J</definiendum>
				<definiens id="0">the object with which i is connected by the functivc f The functives of an individual object act as relations between objects</definiens>
			</definition>
			<definition id="3">
				<sentence>For the linguists , the negation is not a simple problem For tit ( : mathematical logiciau , a negation is a simple problem Starting from a worhl where the negation is a , simple problem ( which , \ [ or example , is matheTnatieal logician ) , thc previous assertion entails tim opening of a new world , in which the new fact is asserted ( 7'he ncgation is uot a simple problem ) .</sentence>
				<definiendum id="0">negation</definiendum>
				<definiens id="0">matheTnatieal logician ) , thc previous assertion entails tim opening of a new world , in which the new fact is asserted ( 7'he ncgation is uot a simple problem )</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>In this palter , we introdu &lt; : c the degree of ( : &lt; mtriblltion of cast ; to verb sells ( ' , disambignation intt ) this existing method , in this , greater diversity of semanti ( : range of case filler examples will lead to that ease contributing to verb sense disambiguation more. We also report th ( ; result of a coml ) arative ext ) eriment , in which the t ) erfornlance of disaml ) igui~tion is iml ) rt ) ved t ) y considering this notion of semantic contribution. Word sense disambiguation is a crucial task in many kinds of natural language I ) rot : essing at ) l ) lications , such as word selection in iIla ( ; hine translation ( Sato , 1991 ) , pruning of syntactic structures in parsing ( l , ytinen , 1986 ; Nagao , 11994 ) an ( l text retrieval ( Krovets and Croft , 1992 ; Voorht'.es , 1993 ) . Various researches on word sense disamil ) ignation have recently been utilized in ( : orlms-based apt ) roache.s , reflecting the growth in the numlmr of machine readable texts. Unlike rule-basel1 ~l ) l ) roa ( '.hes , eortms-l ) asext al ) proa ( : hes free us fl'om the task of generalizing observed 1 ) hent ) Illena to l ) roduce rnles for word sense , disaln\ ] ) igmttion , e.g. subt : ittegorization rules. Cortmsbased al ) proaches are exet : ut ( ; ( 1 based on the intuitively t'easibh ' , assmnption that the higher the degree of similarity betwee , n the context of an illput word and tim context ill which tit ( ; word apl ) cars in a sens ( ~ ' in a tort ) us , the more plausible it becomes that the word is used in the same s ( .~nse. Corpus-/ ) ased m ( ; thotls are. classified into two ap1 ) rt ) aches : examI ) le-I ) ased approaches ( Kurohashi and Nagao , 1994 ; Urmnoto , 1994 ) and statisticbased apl ) roa ( : hes ( l~rown et al. , 1991 ; 1 ) tLglm and Itai , 1 ! ) 94 ; Niwa and Nitta , 11994 ; Schiitze , 1992 ; Ym'owsky , 1995 ) . We follow the examt ) h &gt; based apl ) roach ill exl ) laining its effe .</sentence>
				<definiendum id="0">semantic contribution. Word sense disambiguation</definiendum>
				<definiens id="0">ease contributing to verb sense disambiguation more. We also report th</definiens>
				<definiens id="1">hes free us fl'om the task of generalizing observed 1 ) hent ) Illena to l ) roduce rnles for word sense</definiens>
			</definition>
			<definition id="1">
				<sentence>notes the case filler in the case ci , a.nd 'ntc~ denotes the case maker of &lt; : i. The candidates of ilH ; ( ~rl ) ret ; ttion for v , which ~re , sl , , s2 ~md s3 , are deriv ( ; d froln the datal ) ase .</sentence>
				<definiendum id="0">a.nd 'ntc~</definiendum>
				<definiens id="0">the case maker of &lt; : i. The candidates of ilH ; ( ~rl ) ret ; ttion for v , which ~re , sl ,</definiens>
			</definition>
			<definition id="2">
				<sentence>( 52 sibility of interpreting the input verb as sense 3 , and SIM ( nc , $ ~ , c ) is the degree of the similarity between the input complement nc and example complements $ s , c. ws is the weight on an interpretation 3 such that more obligatory cases imposed by s being found in tile input , will lead to a greater value of the weight a. P ( 3 ) = w3 E SIM ( nc , Ss , c ) ( 1 ) c SIM ( nc , £3 , c ) is the maximum degree of similarity between nc and each of £3 , e as in equation ( 2 ) .</sentence>
				<definiendum id="0">SIM</definiendum>
				<definiendum id="1">c. ws</definiendum>
				<definiens id="0">the weight on an interpretation 3 such that more obligatory cases imposed by s being found in tile input</definiens>
				<definiens id="1">w3 E SIM ( nc , Ss , c ) ( 1 ) c SIM ( nc , £3</definiens>
			</definition>
			<definition id="3">
				<sentence>P ( 3 ) = w3 Ec g3 , e ) '' CCD ( c ) Ec CUD ( c ) ( 3 ) Here , CCD ( c ) is a newly introduced weight , such that CCD ( c ) is greater when the degree of case e 's contribution is higher .</sentence>
				<definiendum id="0">CCD ( c )</definiendum>
				<definiendum id="1">CCD ( c )</definiendum>
				<definiens id="0">a newly introduced weight , such that</definiens>
			</definition>
			<definition id="4">
				<sentence>PSS ( nc , £s , c ) = SIM ( nc , Ss , c ) RSSR ( 3 , c ) Here , RSSR ( 3 , c ) denotes the relative strength of tile selectional restriction on a case c associated with a sense 3 .</sentence>
				<definiendum id="0">PSS ( nc</definiendum>
			</definition>
			<definition id="5">
				<sentence>Let a certain verb have n senses ( sl , 32 , ... , s~ ) and the set of example case fillers of a case c associated with 3~ be $ 3~ , c. Then , the degree of c 's contribution to disambiguation , CCD ( c ) , is expected to be higher if the example case filler sets { £si , c I i = 1 , ... , n } share less elements .</sentence>
				<definiendum id="0">CCD ( c )</definiendum>
				<definiens id="0">the degree of c 's contribution to disambiguation</definiens>
			</definition>
			<definition id="6">
				<sentence>CCD ( ) = i=1 j=i+t ( 6 ) a is the constant for parameterizing to what extent CCD influences verb sense disambiguation .</sentence>
				<definiendum id="0">CCD</definiendum>
				<definiens id="0">the constant for parameterizing to what extent CCD influences verb sense disambiguation</definiens>
			</definition>
			<definition id="7">
				<sentence>Thus , given a set Es , c of example case fillers in a case c associated with a verb sense s , tile SSR of c associated with s Call be estimated by equation ( 8 ) , where £~ , c is an i4h element of £3 , c , and m is the number of elements in £s , c , i.e. m = \ [ $ 3 , c\ [ .</sentence>
				<definiendum id="0">c</definiendum>
				<definiendum id="1">m</definiendum>
				<definiens id="0">an i4h element of £3 , c</definiens>
				<definiens id="1">the number of elements in £s , c</definiens>
			</definition>
			<definition id="8">
				<sentence>4Note that. , in equation ( 5 ) , while SIM is an integer , PlSSI/ .</sentence>
				<definiendum id="0">SIM</definiendum>
				<definiens id="0">an integer</definiens>
			</definition>
</paper>

		<paper id="2204">
			<definition id="0">
				<sentence>A context is a set of 'extended ' distribution frames : l. a set : a cluster of syntactic forms which must all be valid for a given verb-sense .</sentence>
				<definiendum id="0">context</definiendum>
				<definiens id="0">a set of 'extended ' distribution frames : l. a set : a cluster of syntactic forms which must all be valid for a given verb-sense</definiens>
			</definition>
			<definition id="1">
				<sentence>A distribution is a list of syntactic constructions ( NPs , PPs and sentences ) ; this list is ordered and corresponds to the way these constructions are linearly realized in the surface form as arguments or modifiers .</sentence>
				<definiendum id="0">distribution</definiendum>
				<definiens id="0">a list of syntactic constructions ( NPs , PPs and sentences</definiens>
			</definition>
			<definition id="2">
				<sentence>Non-basic contexts include the description of : middle reflexives , passives , inchoatives , place-subject inversion , introduction of the senti-auxiliary faire , support verbs with nominalization of the predicate ( e.g. crier pousser un cri ) , various forms of argument deletion , preposition change , reciproquals , body-part reformulations , means-instrument raising , reflexives , argument 'des-incorporation ' , perspective change , there insertion , etc .</sentence>
				<definiendum id="0">Non-basic contexts</definiendum>
				<definiens id="0">middle reflexives , passives , inchoatives , place-subject inversion , introduction of the senti-auxiliary faire , support verbs with nominalization of the predicate ( e.g. crier pousser un cri ) , various forms of argument deletion , preposition change , reciproquals , body-part reformulations</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , we have the famous English spray/load alternation , which also exists m French , which is described as follows : context ( \ [ dist ( lll , 7 , context ID is 111 verb ( I\ ] ) , 7 , no constraint on verb phrases ( \ [ xp ( syntax= &gt; syn ( cat= &gt; n ) ) , 7 , distribution xp ( syntax= &gt; syn ( cat= &gt; p ) ) , xp ( synt ax= &gt; syn ( cat= &gt; n , type-prep= &gt; \ [ sur , dana\ ] ) , semant ics= &gt; sem ( themat ic= &gt; \ [ \ [ loc\ ] \ ] ) ) \ ] ) , constraints ( \ [ \ ] ) , ex ( \ [ j e , pulverise , l a , peinture , sur , le , mur\ ] ) ) , 7 , I spray paint on the wall dist ( lll , verb ( I\ ] ) , phrases ( \ [ xp ( syntax= &gt; syn ( cat= &gt; n ) ) , xp ( syntax= &gt; syn ( cat= &gt; n ) , semant ics= &gt; sem ( themat ic= &gt; \ [ \ [ loc\ ] \ ] ) ) , xp ( syntax= &gt; syn ( cat= &gt; p , type-prep= &gt; \ [ de\ ] ) , semantics= &gt; sem ( thematic = &gt; \ [ \ [ tg\ ] \ ] , sem-type= &gt; t sem ( semp= &gt; substance ) ) ) \ ] ) , constraints ( \ [ \ ] ) , ex ( \ [ je , pulverise , le , mur , de , peinture\ ] ) ) \ ] ) .</sentence>
				<definiendum id="0">lll , verb</definiendum>
			</definition>
</paper>

		<paper id="2174">
</paper>

		<paper id="1089">
</paper>

		<paper id="1084">
			<definition id="0">
				<sentence>`` Akku '' ( accumulator ) in ( lb ) is a nominal anaphor referring to `` Nickel-Metallllydride-Akku '' ( nickel-metal-hydride accumulator ) in ( 1 a ) , which , when resolved , provides the proper referent for relating `` Ladezeit '' ( charge time ) in ( lc ) to it .</sentence>
				<definiendum id="0">Akku</definiendum>
				<definiens id="0">a nominal anaphor referring to `` Nickel-Metallllydride-Akku '' ( nickel-metal-hydride accumulator</definiens>
			</definition>
			<definition id="1">
				<sentence>complete connectivity ( compatibility of domains and ranges of the included relations ) , non-cyclicity ( exclusion of inverses of relations ) and non-redundancy ( exclusion of including paths ) .</sentence>
				<definiendum id="0">complete connectivity ( compatibility</definiendum>
				<definiendum id="1">non-redundancy</definiendum>
				<definiens id="0">of domains and ranges of the included relations ) , non-cyclicity ( exclusion of inverses of relations</definiens>
			</definition>
			<definition id="2">
				<sentence>The Cb ( Un ) , the most highly ranked element of Cf ( U , ~ _ 1 ) realized in U~ , corresponds to the element which represents the given information .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiens id="0">the element which represents the given information</definiens>
			</definition>
			<definition id="3">
				<sentence>PreferredConceptualBridge ( y , x , n ) : ¢~ isPotentialEllipticAntecedent ( y , x , n ) A -~3 z : isPotentialEllipticAntecedent ( z , x , n ) A ( StrongerThan ( CP ... ... .. CP ... . y.~ ) V ( asStrongAs ( CP ... ... .. CP ... . , j.~ ) A z &gt; , s Y ) ) Table 6 : Preferred Conceptual Bridge The actor computation model ( Agha &amp; Hewitt , 1987 ) provides the background for the procedural interpretation of lexicalized grammar specifications , as those 1We assume the following conventions to hold : C = { Word , Nominal , Noun , PronPersonal , ... } denotes the set of word classes , and isac = { ( Nominal , Word ) , ( Noun , Nominal ) , ( PronPersonal , Nominal ) , ... } C C × g denotes the subclass relation which yields a hierarchical ordering among these classes .</sentence>
				<definiendum id="0">PreferredConceptualBridge ( y</definiendum>
				<definiendum id="1">n</definiendum>
				<definiendum id="2">-~3 z</definiendum>
				<definiendum id="3">n ) A ( StrongerThan</definiendum>
				<definiendum id="4">Nominal , Word</definiendum>
				<definiens id="0">Preferred Conceptual Bridge The actor computation model ( Agha &amp; Hewitt , 1987 ) provides the background for the procedural interpretation of lexicalized grammar specifications</definiens>
			</definition>
			<definition id="4">
				<sentence>Word actors communicate via asynchronous message passing ; an actor can only send messages to other actors it knows about , its so-called acquaintances .</sentence>
				<definiendum id="0">Word actors</definiendum>
				<definiens id="0">communicate via asynchronous message passing</definiens>
			</definition>
			<definition id="5">
				<sentence>Message passing consists of two phases : tiator `` Ladezeit '' to the forward-looking centers of the previous sentence ( an acquaintance of that sentence 's punctation mark ) , where its state is set to phase 2 .</sentence>
				<definiendum id="0">Message passing</definiendum>
				<definiens id="0">consists of two phases : tiator `` Ladezeit '' to the forward-looking centers of the previous sentence ( an acquaintance of that sentence 's punctation mark )</definiens>
			</definition>
			<definition id="6">
				<sentence>Besides the intormation technology domain , expcriments with ottr parser have also been successfully run on medical domain texts , thus indicating that the heuristics we have been developing arc not bound to a particular domain , The current lexicon contains a hierarchy of approximately 100 word class specilications with nearly 3.000 lexical entries and col responding concept descriptions available from the LOOM knowledge representation system ( MacGregor &amp; Bates , 1987 ) -- 800 and 500 concept/role spccifications for the inlormation technology and medicine domain , respectively .</sentence>
				<definiendum id="0">LOOM knowledge representation system</definiendum>
				<definiens id="0">spccifications for the inlormation technology and medicine domain , respectively</definiens>
			</definition>
</paper>

		<paper id="2208">
			<definition id="0">
				<sentence>Define that , \ ] a I is the number of clusters ~ in the string 'a ' , n ( a ) is the number of occurrences of the string ~ &amp; ' , &amp; ud n ( a+l ) is the number of occurrences of the string 'a ' with one additional cluster added .</sentence>
				<definiendum id="0">ud n</definiendum>
				<definiens id="0">the number of occurrences of the string</definiens>
			</definition>
			<definition id="1">
				<sentence>We selected 'Thai Revenue Code ( 1995 ) ' , as large as 705,513 bytes , and 'Convention for Avoidance of Double Taxation between Thailand and Japan ' , which has a smaller size of 40,401 bytes .</sentence>
				<definiendum id="0">'Thai Revenue Code</definiendum>
				<definiens id="0">large as 705,513 bytes , and 'Convention for Avoidance of Double Taxation between Thailand and Japan '</definiens>
			</definition>
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>Each pair consists of tim generalized case information and the eorresl ) onding the- .</sentence>
				<definiendum id="0">pair</definiendum>
				<definiens id="0">consists of tim generalized case information and the eorresl ) onding the-</definiens>
			</definition>
			<definition id="1">
				<sentence>A BSF is a propositional description .</sentence>
				<definiendum id="0">BSF</definiendum>
				<definiens id="0">a propositional description</definiens>
			</definition>
			<definition id="2">
				<sentence>CAUSE r : ( agens , act ) ( ACT ( r ) p : &lt; source , have ) ET q : ( goal , have ) ( BEC ( NOT ( HAVE ( p , u ) ) ) u : ( from-obj , have ) BEC ( HAVE ( q , u ) ) ) ) u : ( to-obj , have ) Figure 3. BSF and deel ) cases. From both the syntactic and the semantic point of view , the BSF delivers the maximum case frame of the lexemes that constitute the lexical field. Some of the roles of the maximum case frame can be put into the foreground ; these are said to have cmphasis. Some roles nmst not be verbalized explicitly ; these are said to be blocked. In the subset of roles that are not blocked there are , on the one hand , roles referring to obligatory actants and , on the other hand , roles referring to optional actants. Which roles have emphasis and which do not have emphasis , which are the ones that must be verbalized , and which are the ones that need not be verbalized is determined according to general rules. Exploiting the field specific possibilities to make some variables denote the same reference object ( by renaming of variables ) results in more specific BSFs. These then describe partial lexical fields like , e.g. to give or to take. By adding infbrmation about , emphasis and blocking of roles , a BSF is transformed into a number '' of prototypieal meaning descriptions. We can then derive systematically which are the suitable grammatical realizations of each role. However , there are two important points concerning the determination of which grammatical realizations are possible : Firstly , the predicate that takes the corresponding elementary argument directly and , secondly , the choice of that subset of roles of tim maximum case frame that are not blocked. One of the three prototypical ineaning descriptiolm that constitute the partial field of to 9ire and tile gramrnatical case assiglmmnt of verschenke , n 1 is given in Figure 4. ( Those parts of l ; he description t ; hat have emphasis are written in bold face. Tile occurence of a variable preceded by `` T '' is blocked. The grammatical realization of the optional actant ( an+accusative ) is put , in brackets , z ) CAUSE ( ACT ( p ) ( agens , act &gt; : nora ET ( BEC ( NOT ( HAVE ( Tp , u ) ) ) ( from-obj , have } : ac ( : BEC ( HAVE ( q , Tu ) ) ) ) ( goal , have ) : ( an+ace ) e.g. : Calvin vcrschenkt ein Buch an Itobbes .</sentence>
				<definiendum id="0">ACT</definiendum>
				<definiendum id="1">BSF</definiendum>
				<definiendum id="2">BEC</definiendum>
				<definiens id="0">the maximum case frame of the lexemes that constitute the lexical field. Some of the roles of the maximum case frame can be put into the foreground ; these are said to have cmphasis. Some roles nmst not be verbalized explicitly ; these are said to be blocked. In the subset of roles that are not blocked there are , on the one hand , roles referring to obligatory actants and , on the other hand , roles referring to optional actants. Which roles have emphasis and which do not have emphasis , which are the ones that must be verbalized , and which are the ones that need not be verbalized is determined according to general rules. Exploiting the field specific possibilities to make some variables denote the same reference object ( by renaming of variables ) results in more specific BSFs. These then describe partial lexical fields like , e.g. to give or to take. By adding infbrmation about , emphasis and blocking of roles , a</definiens>
			</definition>
			<definition id="3">
				<sentence>However , due to SITI '' s gener ; , l eq ) pr ( ) ach mty fltrther spe ( : ification of its ( h~s ( : rif ) l ; ions lea ( Is to an enlargement of the ret ) resental ; ion r~ther t ; h ; m to tt change of the common denominator .</sentence>
				<definiendum id="0">ions lea</definiendum>
				<definiens id="0">s gener ; , l eq</definiens>
			</definition>
			<definition id="4">
				<sentence>: to-ob &gt; l.~v , +~ ) I RES ( C '' ^ '' GE-S~G '' -rEu~ ec D ( s2 \ [ &lt; ASS Figure 9b. Semantic structures of lcihcn 's entry. This possibility can be used to state the axiom which represents the specific semantic contribution of C\ [ \ [ ANGE-SI ( IN-TEMI ' : it , s poststate ehara ( &gt; terized by the state s2 of the person r0 being in an psychological attitude state one of whose coinponents ( c ) is a certain belief .</sentence>
				<definiendum id="0">c )</definiendum>
				<definiens id="0">represents the specific semantic contribution of C\ [ \ [ ANGE-SI ( IN-TEMI ' : it , s poststate ehara</definiens>
			</definition>
</paper>

		<paper id="2098">
			<definition id="0">
				<sentence>Thus , T gives the pattern matching of two structures formed by co-occurrence relations ( Section 4.2 ) .</sentence>
				<definiendum id="0">T</definiendum>
			</definition>
			<definition id="1">
				<sentence>Word to be translated ( a~ ) and its relating word ( av ) concerning phrasal structure ( for example objective for verb ) were translated into Lu ( bi and by , respectively ) , using an electronic dictionary .</sentence>
				<definiendum id="0">Word</definiendum>
				<definiens id="0">example objective for verb ) were translated into Lu ( bi and by , respectively ) , using an electronic dictionary</definiens>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>The GLR* parser ( Lavie , 1995 ) produces a set of interlingua texts , or ILFs , for a given sentence .</sentence>
				<definiendum id="0">GLR* parser</definiendum>
				<definiens id="0">produces a set of interlingua texts , or ILFs , for a given sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>Because t ; he spontalleous sche ( \ [ lding dialogues 3 , re unrestricted , ambiguity is a major problenl in Enthusiast .</sentence>
				<definiendum id="0">ambiguity</definiendum>
				<definiens id="0">a major problenl in Enthusiast</definiens>
			</definition>
			<definition id="2">
				<sentence>The Statistical Parse Disambiguation method makes use of the three non context-based scores described in Section 3 .</sentence>
				<definiendum id="0">Statistical Parse Disambiguation method</definiendum>
			</definition>
			<definition id="3">
				<sentence>symbolic Natural Language P~vcessing : An Intcgrated Model of , % ripls , Lexicon , and Memory .</sentence>
				<definiendum id="0">symbolic Natural Language P~vcessing</definiendum>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>LKB-I~RL is an IIPSG-liko formalism based on typed feature structures ( \ [ CAR92\ ] ) developed within the Acquilex Project which makes use of unification and mechanisms of defaull inheritance .</sentence>
				<definiendum id="0">LKB-I~RL</definiendum>
				<definiens id="0">an IIPSG-liko formalism based on typed feature structures ( \ [ CAR92\ ] ) developed within the Acquilex Project which makes use of unification and mechanisms of defaull inheritance</definiens>
			</definition>
			<definition id="1">
				<sentence>This feature system comes out as : +B -I : individu~ds ( a pig ) /closcd events ( , lohn ran to the store ) +B+I : groups ( a committee ) /boundcd ilerativc events ( the lightJlashed until down ) -B-I : substances ( water ) / unbounded homogeneous processes ( , lohn shept ) -B+I : aggregates ( buses , cattle ) /unbounded iterativc processes ( the light flashed continually ) Moreover , he posils that boundaries of entities are conceptualised as minimal elaborations ( notated e ) of their ideal topological boundm'ies ( i.e.the tip of the tongue is tile portion which conesponds to a minimal elaboration of tile 0-dimensional point which would ideally be the tongue 's tip ) .</sentence>
				<definiendum id="0">+B -I</definiendum>
				<definiendum id="1">individu~ds</definiendum>
				<definiendum id="2">groups</definiendum>
				<definiendum id="3">aggregates</definiendum>
				<definiens id="0">a pig ) /closcd events ( , lohn ran to the store</definiens>
			</definition>
			<definition id="2">
				<sentence>The Agenlivc is a process of inodelling of a \ [ -B \ ] substance .</sentence>
				<definiendum id="0">Agenlivc</definiendum>
				<definiens id="0">a process of inodelling of a \ [ -B \ ] substance</definiens>
			</definition>
			<definition id="3">
				<sentence>a tongue , box , cake , LIMON ) CONT : I-B\ ] things ( =substances , plurals/aggregates ) ( a 'container ' of wheat , sugar , water , paper , cakes , tongues , boxes ) DTCHD : \ [ +B\ ] entities ( =individuated ) ( a 'detached portion ' of a lemon , cake ) MDLD : \ [ -B\ ] entities ( =substances ) ( a 'shaped mass ' of sugar , wheat , paper ) With respect to shape , it has to be noticed that while that of ELT and MDLD is inherent to the portion itself ( in ELT because the porlion pre-existed as an individual ; in MDLD because the whole was an mnorphous mass and it is tile process of portioning what has bounded the new thing ) , in BOUND and DTCIID shape is somehow relative to the whole .</sentence>
				<definiendum id="0">=substances )</definiendum>
				<definiens id="0">a 'container ' of wheat , sugar , water , paper , cakes , tongues , boxes</definiens>
				<definiens id="1">an mnorphous mass and it is tile process of portioning what has bounded the new thing ) , in BOUND and DTCIID shape is somehow relative to the whole</definiens>
			</definition>
			<definition id="4">
				<sentence>The Logical Semantics of PNs ( SEM ) will account for their both pm'titive and relational nature by adopting as predicate- , 'ugulnent structure that of their CONST Role .</sentence>
				<definiendum id="0">Logical Semantics of PNs ( SEM</definiendum>
				<definiens id="0">account for their both pm'titive and relational nature by adopting as predicate- , 'ugulnent structure that of their CONST Role</definiens>
			</definition>
			<definition id="5">
				<sentence>_VINO , \ [ IN `` I'ROZO PAN : I 1N 'I'I { OZO PAN ) \ [ 'a glass witie ' = 'a glass o1__~ wine ' , 'a portioli bread ' = 'a porlion of bread'\ ] in which the preposition is elidible -soinelhiug which is not possible in the case of couleutful prepositions ( VENGO 1 ) I { BARCELONA- &gt; *VF , NGO BARCEI , ONA ) \ [ 'l-come from Barcelolm'- &gt; *'I-colne Barcelona'l. Moreover , CAT bears the feature COUNT slaildiug , as well for ease of exposition , for the range of ( surface ) gramlnalical belmviour of lexical signs usually refened to as couulability/uncounlability ( see discussiou alxwe ) .</sentence>
				<definiendum id="0">CAT</definiendum>
				<definiens id="0">gramlnalical belmviour of lexical signs usually refened to as couulability/uncounlability ( see discussiou alxwe )</definiens>
			</definition>
			<definition id="6">
				<sentence>Boundary Portions Provided all which has boon discussed up to here , the general lex-PORTION-sign is defined as in fig.4 ; that is , as selecting NPs and resultiug in FORMAL : \ [ +B\ ] entity-denoting signs ( lherefore individuated and syntactically couutable ) where the only QUALIA feature which percolale from the whole is the TELIC Role -the rest of Quales may be oveiTiddeu by that of the PN .</sentence>
				<definiendum id="0">Boundary Portions Provided</definiendum>
				<definiendum id="1">general lex-PORTION-sign</definiendum>
				<definiens id="0">selecting NPs and resultiug in FORMAL : \ [ +B\ ] entity-denoting signs ( lherefore individuated and syntactically couutable ) where the only QUALIA feature which percolale from the whole is the TELIC Role -the rest of Quales may be oveiTiddeu by that of the PN</definiens>
			</definition>
</paper>

		<paper id="1063">
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>Pronunciation-by-analogy ( PbA ) is an emerging technique for text-phoneme conversion based on a psychological model of reading aloud .</sentence>
				<definiendum id="0">Pronunciation-by-analogy ( PbA</definiendum>
				<definiens id="0">an emerging technique for text-phoneme conversion based on a psychological model of reading aloud</definiens>
			</definition>
			<definition id="1">
				<sentence>Pronunciation-by-analogy ( PbA ) is an influential psychological model of the process of reading aloud .</sentence>
				<definiendum id="0">Pronunciation-by-analogy ( PbA )</definiendum>
				<definiens id="0">an influential psychological model of the process of reading aloud</definiens>
			</definition>
			<definition id="2">
				<sentence>The Icxical datalmsc consists o1 ' `` approximately 20,000 words based on Webster 's tbcket Dictionary '' in which text and phonemes have been auto° matically aligned .</sentence>
				<definiendum id="0">Icxical datalmsc</definiendum>
				<definiens id="0">consists o1 ' `` approximately 20,000 words based on Webster 's tbcket Dictionary '' in which text and phonemes have been auto° matically aligned</definiens>
			</definition>
			<definition id="3">
				<sentence>The output has been scored on words correct and also on symbol score ( i.e. phonemes correct ) using the Levenshtein ( 1966 ) string-edit distance as shown in Table 1 .</sentence>
				<definiendum id="0">Levenshtein</definiendum>
				<definiens id="0">scored on words correct and also on symbol score ( i.e. phonemes correct ) using the</definiens>
			</definition>
</paper>

		<paper id="1067">
			<definition id="0">
				<sentence>The probability of generating hypothesis t at position i is just : Isl p ( tls , i ) = EP ( tlsi ) a ( j li ) j=0 where sj is the jth source text token ( so is a null token ) , p ( tlsj ) is a word-for-word translation probability , and a ( jli ) is a position alignment probability ( equal to 1/ ( M + 1 ) for inodel 1 ) .</sentence>
				<definiendum id="0">sj</definiendum>
				<definiens id="0">the jth source text token ( so is a null token</definiens>
				<definiens id="1">a word-for-word translation probability</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>The semantic priming is a predictive process where some already uttered words ( priming words ) are calling some other ones ( primed words ) through various meaning associations .</sentence>
				<definiendum id="0">semantic priming</definiendum>
				<definiens id="0">a predictive process where some already uttered words ( priming words ) are calling some other ones ( primed words</definiens>
			</definition>
			<definition id="1">
				<sentence>Back priming Generally speaking , the priming process provides a set of words that should follow the already uttered lexemes .</sentence>
				<definiendum id="0">priming process</definiendum>
				<definiens id="0">provides a set of words that should follow the already uttered lexemes</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>Poor depth suggests that knowledge and processing techniques are either applicationor languagespecific and limits the ultimate potential of the system in solving semantic problems .</sentence>
				<definiendum id="0">Poor depth</definiendum>
				<definiens id="0">suggests that knowledge and processing techniques are either applicationor languagespecific and limits the ultimate potential of the system in solving semantic problems</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>Part-of-speech tagging , which assigns the most likely tag to each word in a given sentence , is one .</sentence>
				<definiendum id="0">Part-of-speech tagging</definiendum>
				<definiens id="0">assigns the most likely tag to each word in a given sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>Each inR ) rmation source gives rise to a set of constraints , to be imposed on the con &gt; bined estimate .</sentence>
				<definiendum id="0">inR ) rmation source</definiendum>
				<definiens id="0">gives rise to a set of constraints , to be imposed on the con &gt; bined estimate</definiens>
			</definition>
			<definition id="2">
				<sentence>MRF ( Marker random field ) model is based on ME method and it ; has the facility to combine many inlbrmation sources through feature flmctions .</sentence>
				<definiendum id="0">MRF</definiendum>
				<definiens id="0">the facility to combine many inlbrmation sources through feature flmctions</definiens>
			</definition>
			<definition id="3">
				<sentence>Formulation ( IIMM ) I : % us assunm t , hal , we want i.o I , :ttov , , the n~ , : : ) st ; likc'ly tag seqtt &lt; mcc ~b ( PV ) , given a parlicttlar wot : d Se ( lUtmcc I'V. Tim l.agging prol ) h-nt is dolim 'd as liuding t.lw tttosl , likely t. ? tg s ( `` tltlClt ( ' ( ! `` 1 ' q~ ( w ) : . , ' : z , t~ , tx/ ' ( '/'llI , : ) ( t ) z ' ( Wl ' , '' ) / ' ( '/ ' ) = .. , 'q max ( 2 ) `` , ' P ( w ) = + , rff nF3xI ' ( VVI'l ' ) l ' ( 73 ( : It / wh.cre P ( 7 ' ) is 1 , he a priori iwohal ) ilit.y of a tag scquom : o~ 'r , t , ( WI'I ' ) is t , h.c cot~dil.ioual I , 'olmhil il , y of word sc , ,tu ( m ( -c. H/ , given I.It , :~ .'- ; cqucuco o1 '' tags 7 ' , at , d / ' ( W ) is l , ho tmcotMitiotled i ) roba.I ) ilit , y ot : word scqmmcc W. 'l'hc t &gt; rol &gt; abilit .</sentence>
				<definiendum id="0">Formulation</definiendum>
				<definiendum id="1">rff nF3xI</definiendum>
				<definiens id="0">o~ 'r , t , ( WI'I ' ) is t , h.c cot~dil.ioual I , 'olmhil il , y of word sc</definiens>
			</definition>
			<definition id="4">
				<sentence>co { ~~ eee &lt; L.V &lt; ? L~/ ' , ,IL/ &lt; dJ &lt; L.v Figure 1 : MRF T is defined for the neighborhood system with distance 2 iterative algorithm , `` Generalized Iterative Scaling '' ( GIS ) , exists , which is guaranteed to converge to the solution ( l ) arroch72+ ) . ( 12 ) is similar to Gibbs distribution , which is the primary probability distribution of M\ [ { F model. MRF model uses ME principle in combining information sources and parameter estimation. We will describe MRFF model and its parameter estimation method later. Neighborhood of given random variable is defined by the set of random variables that directly atfect the given random variable. Let N ( i ) denote a set of random variables which are neighbors of ith random variable. Let 's define the neighborhood system with distance L in tagging fbr words W = wl , ... , w , ~ , and tags T = h , ... , t , ~. N ( i ) = { iL , ... , i1 , i+ l , ... , i+ L } ( 13 ) This neighborhood system has on ( ; dimensional relation and describes the one dimenstional structure of sentence. Fig. 1 showes MP~F T which is defined for the neighborhood system with distance ti is affected by the neighbors ti2 , ti1 , ti+ t , ~j+'~. It also showes that ti , ti-t and ti , ti+l have the neighborhood relation connected by bigram , and that ti , ti-l , ti-2 and ti , ti+l , ti+2 have ttm neighborhood relation connected by trigram. A clique is defined as the set of random variables that all of the pairs of random variables are neighborhood in it. Let 's define the clique as the tag sequence with size L in tagging problem. G = { ti-L , ti- ( t , -1 ) , ... , ti } ( 14 ) A clique concept is used to define clique fimction that evaluates current state of random variables in clique. The definition of MRF is presented as following. Definil~ion of MI { F : Random 'variable T is Markov random field if T ' salisfies the following two properties. 238 Positivity : t ) ( 'F ) &gt; O , W ' ( 15 ) Locality : S ' ( t~I % , Vj , j ¢ i ) = P ( t~I % , Vj , j ~ iV ( j ) ) We assume tha .</sentence>
				<definiendum id="0">MRF T</definiendum>
				<definiendum id="1">trigram. A clique</definiendum>
				<definiens id="0">the neighborhood system with distance 2 iterative algorithm , `` Generalized Iterative Scaling '' ( GIS ) , exists , which is guaranteed to converge to the solution ( l ) arroch72+ ) . ( 12 ) is similar to Gibbs distribution</definiens>
				<definiens id="1">the primary probability distribution of M\ [ { F model. MRF model uses ME principle in combining information sources and parameter estimation. We will describe MRFF model and its parameter estimation method later. Neighborhood of given random variable is defined by the set of random variables that directly atfect the given random variable. Let N ( i ) denote a set of random variables which are neighbors of ith random variable. Let 's define the neighborhood system with distance L in tagging fbr words W = wl , ... , w , ~ , and tags T = h , ... , t , ~. N ( i ) = { iL , ... , i1 , i+ l , ... , i+ L } ( 13 ) This neighborhood system has on ( ; dimensional relation and describes the one dimenstional structure of sentence. Fig. 1 showes MP~F T which is defined for the neighborhood system with distance ti is affected by the neighbors ti2 , ti1 , ti+ t , ~j+'~. It also showes that ti , ti-t and ti , ti+l have the neighborhood relation connected by bigram , and that ti , ti-l , ti-2 and ti , ti+l , ti+2 have ttm neighborhood relation connected by</definiens>
				<definiens id="2">the set of random variables that all of the pairs of random variables are neighborhood in it. Let 's define the clique as the tag sequence with size L in tagging problem. G = { ti-L , ti- ( t , -1 ) , ... , ti } ( 14 ) A clique concept is used to define clique fimction that evaluates current state of random variables in clique. The definition of MRF is presented as following. Definil~ion of MI { F : Random 'variable T is Markov random field if T ' salisfies the following two properties. 238 Positivity : t ) ( 'F ) &gt; O , W ' ( 15 ) Locality : S ' ( t~I % , Vj , j ¢ i ) = P ( t~I % , Vj , j ~ iV ( j ) ) We assume tha</definiens>
			</definition>
</paper>

		<paper id="2150">
			<definition id="0">
				<sentence>`` Simple '' combinations are usually taken to be things like function application and set union or intersection .</sentence>
				<definiendum id="0">Simple</definiendum>
				<definiens id="0">usually taken to be things like function application and set union or intersection</definiens>
			</definition>
</paper>

		<paper id="1093">
			<definition id="0">
				<sentence>~rera-rol ' 8fl ll Sll head : i~iE head : ) &lt; hli f ) .~ t\ ] ead : ~fi med-rel : nil mocl~ret : nil modrol : dl Alter the searches , the input is re-analyzed using newly found words. The final result of JMA is then passed to a CFG-parser which calculates the cost of possihlc structures and the attribute-values attached to each node in a solution. In the case that there is ambiguity in the final morphological analysis of a given compound noun , the morphological analyzer picks up the solution with the least number of segmentations. The procedure of the cost calculation era dcpendcncy structure is basically the same in Kobayashi et al. ( 1994 ) . The cost of the dependency between two nodes is given by nsing mulual information between the lexical heads of ihe taxies ( fig. 2 ) . 551 Here two kind of attributes are used ; head , which records the head of a node as a value , and nu , d-rel , which records the kind of relationship found between two heads of children. In Japanese , if the two children are both content words , the value of the head attribute of the parent node is usually identical to the value of the hend attribute of the right daughter. Figure 2 Depe nde ncy R epres ta~t ati on U sing Attribute-Valu e Pai Is NP head : 7 -r et : , ... ~ r~ma } NP NP head : a head : fl mod-rel : { r ... ... . r , m , } m3d-rel : { r ... ... . r~ } The category which the morphological analyzer assigns to a word is one of the following : sn ( stem of a sino-verb ) , n ( noun ) , pn ( proper noun ) , num ( number ) , adj ( stem of an adjective or an adjectival verb ) , prfx ( nominal prefix ) , sfix ( nominal suffix ) , num-prfx ( numerical prefix ) , and numsfix ( numerical suffix ) . CFG rules for compound noun construction use these categories as non-terminals. The following two rules are the most basic : \ [ np -~ np np\ ] and \ [ np -- ~ n\ ] . These rules construct the basic framework of the dependency-structure of a compound noun. We assume that the structure of a compound noun can be represented in the framework of binary-tree grammar by using attribute-wdue pairs. Text Scanning This subsection describes the most important part of our method : the pattern matchers and heuristics on unregistered word treatment. `` Fable 1 shows the main part of the pattern matchers. We will describe the procedure for collecting evidence by using the example mentioned previously , `` ~\ ] E.~ ) ~t~tJ~ ) ~ The initial segmentation of the compound noun is `` ~k~ sn/~ adj/Y~ n/i-~ n/~'~T sn '' . Thus the WL initially contains these five words. The words are used as keys lot the search. As mentioned in the previous section , this solution contains an over-segmentation error , which is the most likely error in the situation when unregistered words appear. Therefore this example captures the typical problem laced in our task. In Table I , 'A ' stands for a given key , 'B ' stands for a sequence of kanji characters ( we only treat kanjicompound nouns in this paper ) , and 'D ' stands for an `` extended '' delimiter : D is identical to a space , a symbol , a katakana or a hiragana except `` © '' ( no ; o3 ' ) . After preliminary experiments , we decided to eliminate `` © '' from the delimiters because if it is used , a pattern such ~ts `` A©B©C '' ( roughly C orB of A ) could be picked up , and it may ~ erroneous evidence because of its ambiguity in dependency structure. Table Part of Pattern D'BA 'D D D D-Be ) A • D D , A~'~cB D D • BV~A • D D '' B~cAD D AT ) ~B-~7~ • D \ D AIZB~7~ • D D B ; O : A-~ `` 7o • D D B~'A~7~ D O BIZA~7~ . D A-~7~B • D AL , Y. : B • D A~ ~'~B • D 1.5 D B'~Z~A • D D BL ? cA • D D B~'LT~A • D D B~-/'cA • D D • A.~3 ~ i~'B • D ~ D • AL g D 1.6 D • B , t~ J : LFA • D D'B~A 'D D • A~Zo~'~cCo ' ) B • D ' 1.7 D • A~ , ~-1~-9~ ~ B D / D • B~ : _o~ , ~Z'09A • D D. B~Y-I~-~A • D Patterns ill 1.I collect evidence of inner-word collocation of A and B. If the length of A is more than or equal to 2 , The length of B is limited to less than or equ~d to 3. If the length of A is ! , the length of B is limited to less than or equal to 2. Additional explanation will be given later in this subsection. Patterns in 1.2 collect the evidence of particlecombined collocation of A and B. A and B are combined by a particle `` © '' which is similar to `` of ' in English. Note that no part of a phrase such as `` A©B©C '' is picked up so that erroneous evidence can be to avoided. The length of B is limited to less than or equal to 3 ( in 1.3 , ... . 1.7 , the same condition on B is used ) . Patterns in 1.3 collect the evidence of an adjectival modifier-modifiee relationship between an adjective ( or an adjectival noun ) and a noun. Patterns in 1.4 collect the evidence of a predicateargument relation between a sino-verb and a noun. Particles `` ¢j~ '' Q~a ) , `` ~ `` ( wo ) and `` l~- '' ( ni ) roughly indicate AGENT , OBJECT and GOAL , respectively. Patterns in 1.5 collect the evidence of a modifiermodifiee relationship between a sino-verb and a noun , the sino-verb which appears at the tail of a noun modifier phrase and the noun which is modified by the phrase. Patterns in 1.6 collect the evidence of a coordination relationship between two words. Patterns in 1.7 collect phrases such as `` A about B '' ~md `` B about A '' . Here we omit the others. One can , add any pattern as long as it supplies reliable evklence. In the following part of this subsection , we will illustrate the search procedure using the initial value of WE { ( ? ~k.d ( sn ) , ( ~ adj ) , ( /~ n ) , ( '~ } ~ n ) , ( ) j~-~ sn ) } . From the first item `` ~kll : Z ' , evidence shown in 3.1 of figure 3 is collected , and the result is stored in the form 552 shown in 3.1'. Note that the number of occurrences ~uxt the observed relationships are recorded. At this stage , the unregistered word `` Jql~'~J~ '' is already captured by using a pattern marcher in 1.5. As for the second word , however , one has to be careful because a word with length 1 is very likely to appear through an over-segmentation error. The pattern matchers gather evidence such as `` AS~ ~ : ~ { U ' ( ~ ~o¢ : big ; ~ ( ~ : change ) , `` J &lt; ~ '' ( university ) , `` ) 2~ ! ! '' ( large ) , `` J &lt; ldi ' { ) : , '' ( large retail-shop law ) etc. as given in 3.2. This evidence contains not only correct examples ( such as `` AS L~ ~oc &gt; ~ . '</sentence>
				<definiendum id="0">head</definiendum>
				<definiens id="0">records the head of a node as a value , and nu , d-rel , which records the kind of relationship found</definiens>
				<definiens id="1">num ( number ) , adj ( stem of an adjective or an adjectival verb ) , prfx ( nominal prefix ) , sfix ( nominal suffix ) , num-prfx ( numerical prefix ) , and numsfix ( numerical suffix )</definiens>
				<definiens id="2">used ) . Patterns in 1.3 collect the evidence of an adjectival modifier-modifiee relationship between an adjective ( or an adjectival noun ) and a noun. Patterns in 1.4 collect the evidence of a predicateargument relation between a sino-verb and a noun. Particles `` ¢j~ '' Q~a</definiens>
			</definition>
			<definition id="1">
				<sentence>To classify the evidence , we developed the following rules : R- ( a ) If ( l ) the length of A is 1 , and the length of B is l , ~md ( 2 ) there is no entry for the concatenated string AB ( BA ) in the dictionary used by JMA , then recognize the concatenated string as an unregistered word , and apply R- ( c ) .</sentence>
				<definiendum id="0">R-</definiendum>
				<definiens id="0">( a ) If ( l ) the length of A is 1 , and the length of B is l , ~md ( 2 ) there is no entry for the concatenated string AB ( BA ) in the dictionary used by JMA , then recognize the concatenated string as an unregistered word , and apply R- ( c )</definiens>
			</definition>
			<definition id="2">
				<sentence>R- ( c ) If ( 1 ) the character string consisting of B is identical to the concatenated string of the first or the first two words following A in the initial solution ( the condition for AB ) , or ( 2 ) the character string consisting of B is identical to the concatenated string of the first previous or the first two previous words preceding A in the initial solution ( the condition for BA ) , then record AB in WL as an unregistered word , which will invoke pattern matching using AB as a key .</sentence>
				<definiendum id="0">R- (</definiendum>
				<definiens id="0">c ) If ( 1 ) the character string consisting of B is identical to the concatenated string of the first</definiens>
			</definition>
			<definition id="3">
				<sentence>According to R- ( b ) and R- ( c ) - ( l ) , ~ ) t~ ) 2 is recorded as an unregistered word and stored in WD , which invokes a search of the patterns around it .</sentence>
				<definiendum id="0">WD</definiendum>
				<definiens id="0">invokes a search of the patterns around it</definiens>
			</definition>
			<definition id="4">
				<sentence>A baseline , which takes leftmost derivation strategy , was also investigated for comparison with our method .</sentence>
				<definiendum id="0">baseline</definiendum>
				<definiens id="0">takes leftmost derivation strategy</definiens>
			</definition>
</paper>

		<paper id="2129">
			<definition id="0">
				<sentence>ADOMIT is an algorithln for Automatic Detection of OMissions in Translations .</sentence>
				<definiendum id="0">ADOMIT</definiendum>
				<definiens id="0">an algorithln for Automatic Detection of OMissions in Translations</definiens>
			</definition>
			<definition id="1">
				<sentence>Quantitative evaluation on simulated omissions showed that , even with today 's poor bitext mapping technology , ADOMIT is a valuable quality control tool for translators and translation bureaus .</sentence>
				<definiendum id="0">ADOMIT</definiendum>
				<definiens id="0">today 's poor bitext mapping technology ,</definiens>
				<definiens id="1">a valuable quality control tool for translators and translation bureaus</definiens>
			</definition>
			<definition id="2">
				<sentence>ADOMIT is an algorithm for Automatic Detection of OMissions in Translations .</sentence>
				<definiendum id="0">ADOMIT</definiendum>
				<definiens id="0">an algorithm for Automatic Detection of OMissions in Translations</definiens>
			</definition>
			<definition id="3">
				<sentence>ADOMIT rests on principles of geometry , and uses no linguistic information .</sentence>
				<definiendum id="0">ADOMIT</definiendum>
				<definiens id="0">rests on principles of geometry , and uses no linguistic information</definiens>
			</definition>
			<definition id="4">
				<sentence>The bitext map is the real-valued fnnclion obtained by interpolating successive points in the bitext space .</sentence>
				<definiendum id="0">bitext map</definiendum>
				<definiens id="0">the real-valued fnnclion obtained by interpolating successive points in the bitext space</definiens>
			</definition>
			<definition id="5">
				<sentence>AI ) OMIT is a valuable qu , dity control tool for tra .</sentence>
				<definiendum id="0">AI ) OMIT</definiendum>
				<definiens id="0">a valuable qu , dity control tool for tra</definiens>
			</definition>
</paper>

		<paper id="2128">
			<definition id="0">
				<sentence>building the chart by asserting edges into the Prolog database , SAX implements the chart by creating concurrent processes .</sentence>
				<definiendum id="0">SAX</definiendum>
				<definiens id="0">implements the chart by creating concurrent processes</definiens>
			</definition>
			<definition id="1">
				<sentence>Like the CLE generator , SGX follows the semantic head-driven ( SHD ) generation algoril ; hm ( Shieber et al. , 1990 ) to ensure efficient ordering of the geImration process .</sentence>
				<definiendum id="0">SGX</definiendum>
				<definiens id="0">follows the semantic head-driven ( SHD ) generation algoril ; hm ( Shieber et al. , 1990 ) to ensure efficient ordering of the geImration process</definiens>
			</definition>
			<definition id="2">
				<sentence>~ specific morl ) hological form , and Sign is a typed li , ~atut'e structure .</sentence>
				<definiendum id="0">Sign</definiendum>
				<definiens id="0">a typed li , ~atut'e structure</definiens>
			</definition>
			<definition id="3">
				<sentence>In ProFIT , sorts axe writl ; en as &lt; sort and features as feature ! value. morph_lex ( Vbse , word , \ [ if ( Cont ) , synsem ! loc ! ( cat ! ( head ! ( vform ! &lt; bse &amp; aux ! &lt; n &amp; inv ! &lt; n ) &amp; subcat ! @ listl ( loc ! ( cat ! ( head ! &lt; noun subcat ! &lt; elist cont ! ( Subj g index ! &lt; ref ) ) ) ) cont ! ( Cont ~ &lt; psoa &amp; quants ! &lt; elist &amp; nucleus ! ( reln ! Reln Role ! Subj ) ) ) \ ] ) verb ( Vbse , Rein , \ [ np/Role\ ] ) . Figure 2 : A morphJex rule for a verb base form 759 We use lexical inference rules to derive flfll HPSG lexical signs from a database of simple Prolog clauses. Such rules can partially re-use available non-HPSG lexical information. The example assumes a lexical entry such as verb ( walk , walk1 , \ [ np/agent\ ] ) specifying a verb with base form walJg and sense , walk1 , which subcategorizes for a noun phrase subject assigned to a thematic role agent. We also use rules like normal HPSG lexical rules , to derive new signs fl'om other lexical signs for morphological derivations , complement extraction and so on. We have no automatic defaults , so these rules must be written carefully. The simplified example in Figure 3 instantiates nominative and 3rd singular in the first subcat item , and copies the rest of subcat by unification. morph_lex ( V3sg , word , \ [ if ( Cont ) , synsem ! loc ! ( cat ! ( head ! ( vform ! &lt; fin aux ! &lt; n &amp; inv ! &lt; n ) subcat ! ( first ! loc ! ( cat ! ( head ! case ! &lt; nom subcat ! &lt; elist ) cont ! ( Subject index ! agr ! ( 3 &amp; sg ) ) ) rest ! Rest ) ) cont ! ( Cont &amp; nucleus ! reln ! Reln ) ) \ ] ) morph_lex ( Vbse , word , \ [ if ( Cont ) , synsem ! loc ! ( cat ! ( head ! ( vform ! &lt; bse aux ! &lt; n &amp; inv ! &lt; n ) subcat ! ( first ! loc ! ( cat ! ( head ! &lt; noun subcat ! &lt; elist ) cont ! Subject ) g rest ! Rest ) ) cont ! Cont ) \ ] ) , morph_infl ( verb_3sg , Vbse , Reln , V3sg ) . Figure 3 : Lexical rule for 3rd singular verb form The typed feature structures in the lexical rules are compiled by ProFIT into Prolog terms. The resulting rules arc then compiled by SICStus Prolog , together with the database of simple lexical entries. Delayed lexical choice is an established technique in natural language generation. When a backtracking algorithm is combined with a lexicon of morphological forms , there is considerable nondeterminism during syntactic generation , because features required for a deterministic choice of morphological form are not yet instantiated. With delayed lexical choice , a lexicon of stems is used during syntactic generation , and the choice of morphological form is delayed to a postprocess. Instead of producing a string of word forms , syntactic generation produces a string of lexical items. The morphological postprocess converts the lexical items to final lexical forms , when all required syntactic features have become instantiated. Describing the implementation of delayed lexical choice in the MiMo2 system , Shieber et al. ( 1990 ) pointed out that only monotonic rules ( which tiltther instantiate the feature structure of a lcxical item but do not change it ) can be delayed. For example , the choice of singular or plural verb i'orm can be delayed until after the subject has been generated , by perforIning syntactic generation with a lexical item based on the verb stem , which does not specify singular or plural. By contrast , a lexical rule for passivization which changes the order of items on the subcat list is nonmonotonic. Both the active and the passive variants must be made available as distinct lexical items during syntactic generation , a In an inheritance-based typed feature formalism , monotonicity is tmilt into the subsumption relation in the sort hierarchy. A sort subsumes its subsorts , which may further instantiate its features , but can not change them. We exploit tile monotonicity of subsumption in the sort hierarchy in our implementation of delayed lexical choice. In place of the MiMo2 lexicon of stems , we specify a syntactic-semantic lezicon interface synsemJex ( Lex , Cat , \ [ LF , Sign\ ] ) where Lex has no significance for generation. Entries in tile syntactic-seInantic lexicon are derived by a small number of lexical rules from entries in the morphological lexicon. Like the morphJex rules , the synsem_lex rules are compiled first , by ProFIT and then by SICStus Prolog. To implement delayed lexical choice , we use the synsem_lex interface during syntactic generation , and then use the morph_lex interface in the morphological postprocess. We must therefore ensure that the delayed morph_lex rules will be monotonic. We do that by ensuring that tile synsem_tex entries subsume the morphJex entries from which they are derived. Figure 4 shows a simplified form of a synsem_lex rule for verbs. The rule derives the synsemJex entry from tile morphJex base form entry , in which vform has a vahm of sort &lt; bse. The subcat of the synsem_lex entry is unified with the subcat of the awe currently make such variants available via alternative forms of the lexicon access rule ( Section 2.3 ) . This could be improved by using constraints to enable subcat-changing rules to be delayed ( van Noord and Bouma , 1994 ; Meurers and Minnen , 1995 ) . 760 morph_lex entry , so that the synsem_lex entry subcategorizes for the appropriate synt ; actic compleinents. The morptl_lex base form entry is used so that the agreement Datures of tile subject , will not t ) c restricted. The content vahte , s are also unified , so that the synseni_lex entry inchldes the approI ) riate semantic roles. However , the head Datures are not unified. Tile synsemJex vform has a value. of sort &lt; vform , wtfich is tim immediate supersort of the mort ) hJex vform sort , &lt; bse. Instead of flfll unification , the synselnJex hea ( l feat , ures sut ) sumc those of the morI ) hJex entry. synsem_lex ( Lex , word , \ [ If ( Cont g &lt; psoa ) , synsem ! loc ! ( cat ! ( head ! ( vform ! &lt; vform aux ! &lt; n &amp; inv ! &lt; n ) subcat ! Subcat ) cont ! Cont ) \ ] ) morph_lex ( Lex , word , \ [ if ( Cont ) , synsem ! loc ! ( cat ! ( head ! ( vform ! &lt; bse aux ! &lt; n &amp; inv ! &lt; n ) subcat ! Subcat ) cont ! Cont ) \ ] ) . Figure. 4 : A synsem_le.x rule for verbs In I ) CG-based systems , the interface between the grammar and the lexicon can often } m speciiied by a DCG rule which accesses the lexicon by means of an extra condition. In our fl'amework , such a rule might , be : word ( Sign ) /LF -- &gt; \ [ Word\ ] , { morph_lex ( Word , word , \ [ LF , Sign\ ] ) } .</sentence>
				<definiendum id="0">morphJex rule</definiendum>
				<definiendum id="1">Lex</definiendum>
				<definiens id="0">lexical inference rules to derive flfll HPSG lexical signs from a database of simple Prolog clauses. Such rules can partially</definiens>
			</definition>
			<definition id="4">
				<sentence>Tile synsemAex entry in Figure 4 also does not , restrict vform , which remains uninstantiated until the verb phrase is combined into a larger phrase .</sentence>
				<definiendum id="0">vform</definiendum>
				<definiens id="0">remains uninstantiated until the verb phrase is combined into a larger phrase</definiens>
			</definition>
			<definition id="5">
				<sentence>The ProFIT system gives an efticient encoding of typed feature structures .</sentence>
				<definiendum id="0">ProFIT system</definiendum>
				<definiens id="0">gives an efticient encoding of typed feature structures</definiens>
			</definition>
</paper>

		<paper id="2131">
			<definition id="0">
				<sentence>Actually , particular subsets of words to be changed are not considered , but instead the bottom-up parsing is performed which constructs syntactic subtrees that contain no more than R modified words ; here R is a parameter which is succesively assigned values l , 2 ... . At present , the linguistic information used by the corrector is not complete .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">contain no more than R modified words</definiens>
				<definiens id="1">a parameter which is succesively assigned values l , 2 ...</definiens>
			</definition>
			<definition id="1">
				<sentence>A homonym consists of a lcxcme name , a part-of-speech marker , and a list of values of morphological features , such as number , case , gender , tense , voice , and so on .</sentence>
				<definiendum id="0">homonym</definiendum>
				<definiens id="0">consists of a lcxcme name , a part-of-speech marker , and a list of values of morphological features , such as number , case , gender , tense , voice , and so on</definiens>
			</definition>
			<definition id="2">
				<sentence>The extended MorphS is the union of the sets of variants built for all homonyms of the initial MorphS .</sentence>
				<definiendum id="0">MorphS</definiendum>
			</definition>
			<definition id="3">
				<sentence>On average , the extended MorphS is much larger than the initial : for 100 sentences from the Computer Science Abstracts the mean number of homonyms in the initial MorphS was 2.4n , while in the extended one it was 12.2n , where n is the number of words in the sentence .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of words in the sentence</definiens>
			</definition>
</paper>

		<paper id="2130">
			<definition id="0">
				<sentence>In English , as in many other languages , morphological word formation is realised by affixation : prefixation and suffixation , so there are two kinds of morphological rules : suffix rules ( A '~ ) rules which are applied to the tail of a word , and prefix rules ( AP ) -rnles which are applied to the beginning of a word .</sentence>
				<definiendum id="0">suffix rules</definiendum>
			</definition>
			<definition id="1">
				<sentence>In the initial learning technique ( Mikheev , 1996 ) which ac ( : ounted only tbr sitnl ) le concatenative regularities a guessing rule was seen as a triph ' , : A = ( S , I , H , ) where S is the affix itself ; I is the l'os-elass of words which should be looked llI ) in the lexicon as main forms ; R is the pos-elass which is assigned to unknown words if the rule is satisfied .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">A = ( S , I , H , ) where S is the affix itself</definiens>
				<definiens id="1">the l'os-elass of words which should be looked llI ) in the lexicon as main forms</definiens>
				<definiens id="2">the pos-elass which is assigned to unknown words if the rule is satisfied</definiens>
			</definition>
			<definition id="2">
				<sentence>If then we find this word in the lexicon as ( NN VB ) ( noun/verb ) , we conclude that the guessed word is of category ( JZ VBD VBN ) ( adjective , past verb or participle ) .</sentence>
				<definiendum id="0">JZ VBD VBN )</definiendum>
				<definiens id="0">adjective , past verb or participle )</definiens>
			</definition>
			<definition id="3">
				<sentence>~/t 1 scorei =/3i 1.65 * V '~ , t~ + log ( ISd ) ) where /3 is the proportion of all positive outcomes ( x ) of the rule application to the total number of compatible to the rule words ( n ) , and ISl is the length of the affix .</sentence>
				<definiendum id="0">ISl</definiendum>
				<definiens id="0">the length of the affix</definiens>
			</definition>
</paper>

		<paper id="2201">
			<definition id="0">
				<sentence>Pa'IYans consists of a core grammar and translation module and a host of peripheral utilities : terin databases , general databases , editors for preand postediting , document handling facilities , facilities for creating and updating term databases .</sentence>
				<definiendum id="0">Pa'IYans</definiendum>
				<definiens id="0">consists of a core grammar and translation module and a host of peripheral utilities : terin databases , general databases , editors for preand postediting , document handling facilities , facilities for creating and updating term databases</definiens>
			</definition>
			<definition id="1">
				<sentence>a l'a~iYans distinguishes two kinds of voealmlm'ies : the general vocabulary and Lhe Lerminologi ( : al vocabulm'ies .</sentence>
				<definiendum id="0">Lhe Lerminologi</definiendum>
				<definiens id="0">the general vocabulary and</definiens>
			</definition>
			<definition id="2">
				<sentence>The most sLriking example of this is the fact ; that PaTrans aims at a very deep analysis of the source , text , and aL the same Lime t ; he formalism alh ) ws for non-lnonotoni ( ; mappings l ) eLweell levels of represenLadon .</sentence>
				<definiendum id="0">aL</definiendum>
				<definiens id="0">the same Lime t ; he formalism alh ) ws for non-lnonotoni ( ; mappings l ) eLweell levels of represenLadon</definiens>
			</definition>
			<definition id="3">
				<sentence>PaTrans is making the translation process faster and more efficient , and it has proven to be a good business for Lingteeh , saving around 50 % of the raw translator cost .</sentence>
				<definiendum id="0">PaTrans</definiendum>
				<definiens id="0">making the translation process faster and more efficient</definiens>
			</definition>
</paper>

		<paper id="2164">
			<definition id="0">
				<sentence>For each sentence in a given Japanese newspaper article , the following features 1 are analyzed : • Important Keywords : An important keyword is defined as a keyword that appears in another sentence or in a title .</sentence>
				<definiendum id="0">Important Keywords</definiendum>
				<definiens id="0">a keyword that appears in another sentence or in a title</definiens>
			</definition>
			<definition id="1">
				<sentence>The importance S of a sentence is calculated as follows : r~ i=t where a is a constant , P/ is the number of points assigned to the i-th feature , which is normalized to be between 0 and 1 , and Wi is the weight assigned to the i-th feature .</sentence>
				<definiendum id="0">P/</definiendum>
				<definiendum id="1">Wi</definiendum>
				<definiens id="0">the weight assigned to the i-th feature</definiens>
			</definition>
			<definition id="2">
				<sentence>Let this importance value be S ; we then get the following equation for each sentence : S=a+LWI*Pi iml where , a is a constant , Pi is the number of points assigned to the i-th featnre which is normalized to be between 0 to l , and Wi is the weight assigned to the i-th feature .</sentence>
				<definiendum id="0">Pi</definiendum>
				<definiendum id="1">Wi</definiendum>
				<definiens id="0">the weight assigned to the i-th feature</definiens>
			</definition>
			<definition id="3">
				<sentence>&lt; / ( the number of Sul ) porters of a sentence s j ) where s { is a sentence that is included in an abstract by group B but not in an abstract created by the system , and s./ is a sentence that is not included in an abstract by group B but is included in an abstract created by the system .</sentence>
				<definiendum id="0">s./</definiendum>
				<definiens id="0">the number of Sul ) porters of a sentence s j ) where s { is a sentence that is included in an abstract by group B but not in an abstract created by the system , and</definiens>
			</definition>
</paper>

		<paper id="2188">
			<definition id="0">
				<sentence>The textual test set consists in a SGML file including the source text sequences aligned with the reference translation sequences and also including the pragmatic , formal and translational characteristics in the form of annotations ( labels and formal descriptions ) .</sentence>
				<definiendum id="0">textual test set</definiendum>
				<definiens id="0">consists in a SGML file including the source text sequences aligned with the reference translation sequences and also including the pragmatic , formal and translational characteristics in the form of annotations ( labels and formal descriptions )</definiens>
			</definition>
			<definition id="1">
				<sentence>In our corpus , we observed that the Meta-Textual Indicators usually present a phrasal structure ( they are not complete sentences ) and include a large number of brachygraphical signs ( acronyms , codes , alpha-numerical references , etc. ) .</sentence>
				<definiendum id="0">brachygraphical signs</definiendum>
				<definiens id="0">acronyms , codes , alpha-numerical references</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , most of the Topical Meta-Utterances ( MET ) correspond to the morpho-syntactic scheme SNDEV which is the following : N + ( AJ ) + ( SN1 ) + ( SPIISN2 ) 3 + ( SAVISP2 ) 4 with the following features : • N : deverb = + , indicating that the noun ( N ) is the result of the nominalisation of a verb ( deverb ) • A J : fonction = # pith # te , indicating that the adjective ( AJ ) has the function of modifier , • SNI : fonction = COMPADV and type = temps , indicating that the nominal phrase has a function of adverbial complement ( COMPADV ) with a `` time '' semantic ( type = temps ) • SPI : fonction = dev-OBJ and prep = de , indicating that the prepositional phrase has a function of object of a nominalisation ( dev-OBJ ) introduced by the preposition de ( prep = de ) .</sentence>
				<definiendum id="0">SNDEV</definiendum>
				<definiens id="0">the following : N + ( AJ ) + ( SN1 ) +</definiens>
				<definiens id="1">the result of the nominalisation of a verb</definiens>
			</definition>
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>ul~/ `` ~ ~ _ ~ t~vAelnents~J Figure 1 : LaSIE Systeln Architecture LaSIE has been designed as a general tmrpose IE research system , initially geared towards , but not solely restricted to , carrying out the tasks specified by the sixth Message Understanding Confe , rence : named entity recognition , coreference resolution , template element tilling , and scenario template filling tasks ( see ( MUC6 , 1995 ) for fllrther details of the task descriptions ) .</sentence>
				<definiendum id="0">LaSIE Systeln Architecture LaSIE</definiendum>
				<definiens id="0">named entity recognition , coreference resolution , template element tilling</definiens>
			</definition>
			<definition id="1">
				<sentence>ature-based formalism for hand-over to the parser ; • parsing does two pass parsing , pass one with a special proper name grairlmal'~ pass two with a general grammar and , after selecting a 'best parse ' , passes on a semantic representation of the current senteliC ( ~ which includes nanle clans infi ) rmation ; • discourse interpretation adds the information ill its input semantic representation to a hierarchically structured selnantic ne , t which encodes the system 's world model , adds additional ilffOl ' , natioi1 presupposed by the input to the world model , perforlns coreference resolution be , tween new instances added and others already ill the world model , and adds information consequent upon the addition of the input to the worhl Inodel .</sentence>
				<definiendum id="0">discourse interpretation</definiendum>
				<definiens id="0">adds the information ill its input semantic representation to a hierarchically structured selnantic ne , t which encodes the system 's world model</definiens>
			</definition>
			<definition id="2">
				<sentence>The LaSIE parser is a simple bottom-up chart parser iinplemented in Prolog .</sentence>
				<definiendum id="0">LaSIE parser</definiendum>
				<definiens id="0">a simple bottom-up chart parser iinplemented in Prolog</definiens>
			</definition>
			<definition id="3">
				<sentence>Here are some examt ) les of the i ) roper nmne grammar rules : NP -- &gt; 0KGAN NP 0RGAN NP -- &gt; LIST_LOC NP NAMES_NP CDG NP 0RGAN_NP -- &gt; LIST_0RGAN_NP NAMES NP CDG_NP ORGAN_NP -- &gt; NAMES NP ' &amp; ' NAMES_NP NAMES_NP -- &gt; NNP NAMES_NP NAMES_NP -- &gt; NNP PUNC ( _ ) NNP NAMES NP -- &gt; NNP The non-terminals LIST_LOCJNP~ LIST_0RGAN_NP and CDG~IP are tags assigned to one or lnor ( ~ input tokens in the name phrase tagging stage of lexical preproeessing .</sentence>
				<definiendum id="0">NNP PUNC</definiendum>
				<definiens id="0">NP CDG_NP ORGAN_NP -- &gt; NAMES NP ' &amp; ' NAMES_NP NAMES_NP -- &gt; NNP NAMES_NP NAMES_NP -- &gt;</definiens>
			</definition>
			<definition id="4">
				<sentence>The non-terminal NNP is the tag for proper name assigned to a single token by the Brill tagger .</sentence>
				<definiendum id="0">non-terminal NNP</definiendum>
				<definiens id="0">the tag for proper name assigned to a single token by the Brill tagger</definiens>
			</definition>
			<definition id="5">
				<sentence>The F-measure ( also called P &amp; R ) allows the differential weighting of precision and recall .</sentence>
				<definiendum id="0">F-measure</definiendum>
				<definiens id="0">also called P &amp; R ) allows the differential weighting of precision and recall</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>The Verbmobil system consists of a large number of components , each of them designed to cope with specific aspects of the interpretation process .</sentence>
				<definiendum id="0">Verbmobil system</definiendum>
				<definiens id="0">consists of a large number of components , each of them designed to cope with specific aspects of the interpretation process</definiens>
			</definition>
			<definition id="1">
				<sentence>The low level basis of ICE is realized by PVM ( Geist et al. , 1994 ) , which is a message passing system for multiple hardware architectures .</sentence>
				<definiendum id="0">PVM</definiendum>
				<definiens id="0">a message passing system for multiple hardware architectures</definiens>
			</definition>
			<definition id="2">
				<sentence>In that case , the ILS notifies the source component of the event and communication c~n take place norreally .</sentence>
				<definiendum id="0">ILS</definiendum>
				<definiens id="0">notifies the source component of the event and communication c~n take place norreally</definiens>
			</definition>
			<definition id="3">
				<sentence>The notification consists of the necessary data to create the intended channel within the component .</sentence>
				<definiendum id="0">notification</definiendum>
				<definiens id="0">consists of the necessary data to create the intended channel within the component</definiens>
			</definition>
</paper>

		<paper id="2202">
			<definition id="0">
				<sentence>Wc propose a method to extract words from a corl ) us and estimate the probability that each word belongs to given parts of speech ( POSs ) , using a distributional analysis .</sentence>
				<definiendum id="0">Wc</definiendum>
				<definiendum id="1">POSs</definiendum>
				<definiens id="0">propose a method to extract words from a corl ) us and estimate the probability that each word belongs to given parts of speech (</definiens>
			</definition>
			<definition id="1">
				<sentence>~ Zp ( posk\ [ a ) D ( posk ) ( 1 ) k where p ( poskla ) is the probability that the string a belongs to posk , and D ( posk ) is tire environment of posk .</sentence>
				<definiendum id="0">p ( poskla )</definiendum>
				<definiens id="0">the probability that the string a belongs to posk</definiens>
			</definition>
			<definition id="2">
				<sentence>The decision variables are the elements of tile probability distribution vector p which expresses tile likelihood that the string is used as each POS : F ( p ) = \ ] D ( et ) ZpkD ( posk ) l 2 ( 2 ) k where p = ( Pl , P2 , ... , P , ~ ) , Pk = p ( poskl o~ ) and n is tile number of POSs in consideration .</sentence>
				<definiendum id="0">decision variables</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the elements of tile probability distribution vector p which expresses tile likelihood that the string is used as each POS : F ( p ) = \ ] D</definiens>
				<definiens id="1">tile number of POSs in consideration</definiens>
			</definition>
			<definition id="3">
				<sentence>f ( tx ) where f ( , x ) is the frequency of the string ~t in the corpus , and p ( poslot ) is the estimated probability that ct belongs to the pos .</sentence>
				<definiendum id="0">f ( tx</definiendum>
				<definiendum id="1">f ( , x )</definiendum>
				<definiens id="0">the frequency of the string ~t in the corpus</definiens>
			</definition>
</paper>

		<paper id="2153">
			<definition id="0">
				<sentence>Let a context-free grammar G be a quadruple &lt; N , T , R , S &gt; where N and T are finite disjoint sets of nonterminal symbols and terminal symbols , respectively , R is a set of rules of the form A -+ a ( A is a nonterminal and a a possibly empty string of nonterminal or terminal symbols ) , S is a speciM nontermin~l , called start symbol .</sentence>
				<definiendum id="0">R</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">a quadruple &lt; N , T , R , S &gt; where N and T are finite disjoint sets of nonterminal symbols and terminal symbols , respectively</definiens>
				<definiens id="1">a set of rules of the form A -+ a ( A is a nonterminal and a a possibly empty string of nonterminal or terminal symbols ) ,</definiens>
			</definition>
			<definition id="1">
				<sentence>An ordered directed graph marked according to grammar ~ is a triple &lt; V , E , m &gt; so that V is a finite set of vertices or nodes , E a finite set of edges e of the form ( vl , ( v2 , ... , vn ) ) ( vi C V , n &gt; 2 , e starts at vl , vl is the predecessor of v2 , ... , vn ) , m is the marking function which associates with each vertex a terrainai or nonterminai symbol or the special symbol e. m is restricted so that the vertices on each edge are marked with the , symbols of a rule in 6 , the empty string being represented by the additional symbol ~ .</sentence>
				<definiendum id="0">vl</definiendum>
				<definiens id="0">a triple &lt; V , E , m &gt; so that V is a finite set of vertices or nodes , E a finite set of edges e of the form ( vl , ( v2 , ... , vn ) ) ( vi C V</definiens>
				<definiens id="1">the predecessor of v2 , ... , vn</definiens>
				<definiens id="2">the marking function which associates with each vertex a terrainai or nonterminai symbol or the special symbol e. m is restricted so that the vertices on each edge are marked with the , symbols of a rule in 6 , the empty string being represented by the additional symbol ~</definiens>
			</definition>
			<definition id="2">
				<sentence>A parse tree is an ordered directed acyclic graph ( DAG ) satisfying the following constraints .</sentence>
				<definiendum id="0">parse tree</definiendum>
				<definiendum id="1">DAG</definiendum>
				<definiens id="0">an ordered directed acyclic graph</definiens>
			</definition>
			<definition id="3">
				<sentence>V is the set of vertices and D is the set of tree readings .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">D</definiendum>
				<definiens id="0">the set of vertices</definiens>
			</definition>
			<definition id="4">
				<sentence>908 Z , , ' np v d n p d n p d n I saw a man in the apt with a tel Figure 1 : a parse forest with a tree reading all : edges used in dl are shown as broken lines The operation x introduces m* n new atomic sets s~j and splits the former atomic sets such that Vi : 1 &lt; i &lt; m : Sli = s~l 0 ... 0 s~ , , and Vj : 1 &lt; &lt; _ j &lt; n : s2j = s~j 0 `` ' ... U Smj. The sets Sl and s2 are now equal modulo associativity and commutativity. Consider the following example : ( 81 0 82 0 83 ) X ( 8a 0 8b ) -- } ( .~ , 0 s~ 0 ~ ) = ( 4o 0 d~ ) 0 ( A , , 0 , ~ , , ) ' 0 ( s~o ' 0 .~ , , ) ' ( , . 0 *b ) = ( . % 0 4. 0 4. ) 0 ( 4b 0,5~ 0 , '~ ) We begin by associating a particular set pointer sl with the root vertex. Sl refers to the , total set of tree readings of the forest ; since the root vertex figures in all trees derivable from the forest. We then traverse the graph in top-down fashion applying to each new vertex v the following procedure : Let ei be the set of tree readings at edge i ending in v , and b # the set of tree readings at edge j starting in v. Then the following actions must be performed. • Apply the procedure to all successors of v. This step yields for each edge j starting in v and for each vertex u at the end of j a set of tree readings b~ , ,. • bj = b ; 1 X ... X b } , , for each edge j starting in v • ( bl 0 ... 0 bn ) x ( el 0 ... 0 era ) If a vertex v h~s already been encountered the only action required is to connect the edge information on v 's predecessor w with the edge information already present on vertex v. In particular , the successors of v need not be checked again. Let k be the edge ( ) vet ' which the vertex v was reached from another vertex w in the top-down traversal. Let C'k , t , be the set of tree readings determined for edge k at vertex w and ek , , the set of tree readings determined for the edge at vertex v. • ( '~kv X Ckw Representation Structures In this section an extension to UDRSs ( Reyle , 1993 ) to express referentially underspecified semantic representations is presented. First a detinition of UDRSs is given. A UDRS/J is a quadruple &lt; L , R , C , &lt; _ &gt; where L and R are disjoint finite sets of labels and discourse referents , respectively .</sentence>
				<definiendum id="0">UDRS/J</definiendum>
				<definiens id="0">a parse forest with a tree reading all : edges used in dl are shown as broken lines The operation x introduces m* n new atomic sets s~j and splits the former atomic sets such that Vi</definiens>
				<definiens id="1">a quadruple &lt; L , R , C , &lt; _ &gt; where L and R are disjoint finite sets of labels and discourse referents , respectively</definiens>
			</definition>
			<definition id="5">
				<sentence>A packed UDRS is a quintuple &lt; L , R , D , C',5 &gt; where L , R , &lt; are the same as in UDRSs , D is a finite set of contexts which is disjoint from L and R. C ' is defined as in UDRSs except that ( 1 ) any condition may 909 also be prefixed by a context set , and ( 2 ) label arguments may~ not , only be labels but also functions from contexts to labels ( £ = L U ( D -- ~ L ) ) , and the same holds for discourse referents ( 7£ = RtA ( D ~ R ) ) .</sentence>
				<definiendum id="0">packed UDRS</definiendum>
				<definiendum id="1">D</definiendum>
				<definiens id="0">a quintuple &lt; L , R , D , C',5 &gt; where L , R , &lt; are the same as in UDRSs</definiens>
				<definiens id="1">a finite set of contexts which is disjoint from L and R. C ' is defined as in UDRSs except that ( 1 ) any condition may 909 also be prefixed by a context set</definiens>
			</definition>
			<definition id="6">
				<sentence>In addition labels and discourse referents are matched as specified in the semantic part ; of the grammar rules ( the `` semantic grammm ' '' ) .</sentence>
				<definiendum id="0">rules</definiendum>
				<definiens id="0">discourse referents are matched as specified in the semantic part</definiens>
			</definition>
			<definition id="7">
				<sentence>Furthermore , the semantic grammar specifies the UDRS conditions introduced by lexical items and rules and determines the arguments to be matched in rules and lexical items .</sentence>
				<definiendum id="0">semantic grammar</definiendum>
			</definition>
			<definition id="8">
				<sentence>Let D2 be the context set { dl , ... , d , ~ } at e , let UpperArg be an argument as provided by the semantic rule corresponding to edge e , let LowerArg be an argument as attached to the vertex w. Then the predicate match2 unifies UpperArg with the restriction of the flmction LowerArg to the context ; s in D2 { dl -~ vl , ... , d , ~ v , } ( a subset of LowerArg ) .</sentence>
				<definiendum id="0">}</definiendum>
				<definiens id="0">an argument as provided by the semantic rule corresponding to edge e</definiens>
				<definiens id="1">the predicate match2 unifies UpperArg with the restriction of the flmction LowerArg to the context ; s in D2 { dl -~ vl , ... , d , ~ v ,</definiens>
			</definition>
</paper>

		<paper id="2125">
</paper>

		<paper id="2211">
			<definition id="0">
				<sentence>ma. , 1993 ; Graham et al. , 1980 ) where K is the number of distinct nonternfinal symbols in the gramma .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">the number of distinct nonternfinal symbols in the gramma</definiens>
			</definition>
			<definition id="1">
				<sentence>I\ [ ( ; i ' ( ' , , /i is the nlllnl ) el ' ( ) t distinct ll`otll ; o , rnliIl`'t.lm symbols in T , a.nd 'n. is | : he size o\ [ the input string .</sentence>
				<definiendum id="0">/i</definiendum>
				<definiens id="0">the nlllnl ) el '</definiens>
				<definiens id="1">rnliIl`'t.lm symbols in T , a.nd 'n. is | : he size o\ [ the input string</definiens>
			</definition>
			<definition id="2">
				<sentence>d due too the spaco luu\ ] la ( ; io ; n. &gt; John should hear from Mary about the news if he returns home , \ [ ( 0 13 ) S : I : /eFIN , ePRES , eSUBJ , eAUX/ - &gt; SI : I : +eFIN PUNCT:2 \ [ ( 0 i2 ) SI:2 : /eFIN , ePRES , eSUBJ , e~UX/ - &gt; NP : I : *ehGRS+eNSMI VP:9 : *ehGRV+eFIN-eSUB~ \ [ ( 0 i ) NP : I : /e3SG , eChP , eNOMI , eCAUS/ - &gt; N~UN : I : -ePRO \ [ ( 0 i ) NOUN : I : /e3SG , eChP/ - &gt; NOUN '' Jo~m '' \ ] \ ] \ [ ( 1 12 ) VP : I : /eFIN , ePRES , eAUX/ - &gt; VP : i SADJ:2 \ [ ( 1 8 ) VP : I : /eFIN , ePRES , eAUX/ - &gt; VP : I PP:2 \ [ ( i 5 ) VP : I : /eFIN , ePRES , eAUX/ - &gt; VP : I : PP:2 \ [ ( i 3 ) VP : I : /eFIN , ePRES , ehUX/ - &gt; AUX '' should '' : -eNEG VP : I : +elNF-eSUBJ-ehUX \ [ ( i 2 ) hUX : I : /eFIN/ - &gt; hUX '' should '' : l\ ] \ [ ( 2 3 ) VP : I : /eFIN , elNF/ - &gt; VERB : I : -ePS \ [ ( 2 3 ) VERB : I : /eFIN , elNF/ - &gt; VERB '' hear '' : l\ ] \ ] \ ] \ [ ( 3 5 ) PP : I : /e3SG , eChP , eNflMI , eChUS/ - &gt; `` from '' NP : I \ [ ( 4 5 ) NP : I : /e3SG , eChP , eNOMI , eChUS/ - &gt; NOUN : I : -ePRO \ [ ( 4 5 ) NOUN : I : /eSG , eCAPi - &gt; NOUN '' Mary ' : I\ ] \ ] \ ] \ ] \ [ ( 5 8 ) PP : I : /eDEF , eNOMI , eCAUS , e3SG/ - &gt; `` about '' NP : I \ [ ( 6 8 ) NP : I/eDEF , eN~MI , eChUS , e3S6/ - &gt; `` the '' NP : I : -eDEF-eINDEF \ [ ( 7 8 ) NP : I : /eNOMI , eCAUS , e3SG/ - &gt; NOUN : I : -ePRO \ [ ( 7 8 ) N~UN : I : /e3SG/ - &gt; NOUN '' news '' \ ] \ ] \ ] \ ] \ ] \ [ ( 8 i2 ) SADJ:2 : /eFIN , e3SG , ePRES , eSUBJ , eOBJ , eSAT/ - &gt; `` if '' NP : i : *eAGRS+eN~MI VP:2 : *eAGRV+eFIN-eSUBJ \ [ ( 9 i0 ) NP : i : /ePRO , eN~MI , e3SG , eI { I\ ] M/ - &gt; PRON : i : -ePOSS \ [ ( 9 iO ) PRON : I : /ePR~ , ~NSMI , e3SG , aHUM/ - &gt; PRON '' he '' : I\ ] \ ] \ [ ( iO 12 ) VP : i : /eFIN , e3SG , ePRES , eOBJ , eSAT/ - &gt; VP '' return '' : i : -eOBJ NP '' home '' :2 : +eCAUS \ [ ( I0 Ii ) VP : i : /eFIN , e3SG , ePRES/ - &gt; VERB : i : -ePS \ [ ( I0 ii ) VERB : I : /eFIN , e3SG , ePRES/ - &gt; VERB '' return '' : i\ ] \ ] \ [ ( Ii 12 ) NP : I : /e3SG , eNOMI , eChUS/ - &gt; NOUN : I : -ePRO \ [ ( II 12 ) NOUN : I : /e3SG/ - &gt; NOUN '' heme '' : l\ ] \ ] \ ] \ ] \ ] \ ] \ [ ( 12 13 ) PUNCT : I - &gt; PUNCT '' . ''</sentence>
				<definiendum id="0">ePRES</definiendum>
				<definiens id="0">/eFIN ,</definiens>
			</definition>
</paper>

		<paper id="2133">
			<definition id="0">
				<sentence>LUD describes a number of DRSs and allows for underspecification of scopal ambiguities .</sentence>
				<definiendum id="0">LUD</definiendum>
				<definiens id="0">describes a number of DRSs and allows for underspecification of scopal ambiguities</definiens>
			</definition>
			<definition id="1">
				<sentence>A discourse relation is represented in LUD as a predicate with three arguments ; the first one is a term for the type of the concerning discourse relation , the second one is an underspecified scope domain of the antecedent part , and the last one is another underspecified scope domain for the conclusion part .</sentence>
				<definiendum id="0">discourse relation</definiendum>
				<definiens id="0">a term for the type of the concerning discourse relation , the second one is an underspecified scope domain of the antecedent part</definiens>
			</definition>
			<definition id="2">
				<sentence>As further described in ( Bos et al. , 1996 ) , LUD is a declarative description language for underspeeifled DRSs .</sentence>
				<definiendum id="0">LUD</definiendum>
				<definiens id="0">a declarative description language for underspeeifled DRSs</definiens>
			</definition>
			<definition id="3">
				<sentence>A LUD-representation U is a triple U = &lt; Hu , Lu , Cv &gt; , where Hu is a set of holes ( variables over labels ) , Lu is a set of labeled conditions , and C~r a set of constraints .</sentence>
				<definiendum id="0">LUD-representation U</definiendum>
				<definiendum id="1">Hu</definiendum>
				<definiendum id="2">Lu</definiendum>
				<definiens id="0">a set of holes ( variables over labels</definiens>
				<definiens id="1">a set of labeled conditions</definiens>
			</definition>
			<definition id="4">
				<sentence>The interpretation of a possible plugging at the top hole is the interpretation of the matrix DRS. In this way , a LUDrepresentation describes a set of possible pluggings at once .</sentence>
				<definiendum id="0">LUDrepresentation</definiendum>
				<definiens id="0">describes a set of possible pluggings at once</definiens>
			</definition>
			<definition id="5">
				<sentence>This enables us to keep the decision , on the one hand , that discourse relation elements are in a next-to-top position in a possible plugging and to keep DRSs for other parts of the sentence underneath the mode predicate , on the other .</sentence>
				<definiendum id="0">discourse relation elements</definiendum>
				<definiens id="0">other parts of the sentence underneath the mode predicate</definiens>
			</definition>
			<definition id="6">
				<sentence>( 8 ) index : ( i8,118 , h0 ) lud_preds : ll-mood ( deal , hO ) 12-discrel ( topic , hl , h2 ) 13-discrel ( node , h3 , h4 ) 14-discrel ( noda , h5 , h6 ) 16-dm ( iD 17-predicate ( get suyoub i , i I ) 19-dm ( i2 ) 1 lO-predicate ( halite , i2 ) 11 O-role ( i2 , art3 , i3 ) lll-role ( i2 , tloc , i4 ) 112-dm ( i5 ) 114-dm ( i6 ) 115-predicat e ( seminaa , i6 ) 116-dm ( i~ ) 117-mo de ( h7 ) 119-dm ( iS ) 120-predicat e ( zikan , i8 ) 122-dm ( i9 ) ll3-neg ( i9 , h8 ) lud_grouping : 15-inc ( \ [ 16,17\ ] ) 18-inc ( \ [ 19,110\ ] ) 113-inc ( \ [ 114,115\ ] ) ll8-inc ( \ [ 119,120\ ] ) 121-inc ( \ [ 122,123\ ] ) lud_meta : modifies ( 18,111 ) lud_scoping : alfa ( i6 , udef , 18,113 ) alfa ( iS , pron , lll,112 ) leq ( 12 , hO ) leq ( 13 , hO ) leq ( 14 , hO ) leq ( 15 , hl ) leq ( 18 , h3 ) leq ( ll6 , h6 ) leq ( llZ , h2 ) leq ( 117 , h4 ) leq ( llT , h5 ) leq ( ll8 , hT ) leq ( 118 , h8 ) leq ( 121 , h7 ) The mode predicate can be seen as a secondary sentence mood predicate .</sentence>
				<definiendum id="0">] ) 121-inc</definiendum>
				<definiens id="0">alfa ( i6 , udef , 18,113 ) alfa ( iS , pron</definiens>
				<definiens id="1">a secondary sentence mood predicate</definiens>
			</definition>
			<definition id="7">
				<sentence>course to Logic : An Introduction to Modeltheoretic Semantics of Natural Language , Formal Logic and DRT .</sentence>
				<definiendum id="0">Logic</definiendum>
				<definiens id="0">An Introduction to Modeltheoretic Semantics of Natural Language , Formal Logic and DRT</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>Cereal is a kind of food .</sentence>
				<definiendum id="0">Cereal</definiendum>
				<definiens id="0">a kind of food</definiens>
			</definition>
			<definition id="1">
				<sentence>\ [ make\ ] &gt; ( sub ) &gt; \ [ John\ ] &gt; ( obj ) &gt; \ [ drawing\ ] &gt; ( nit ) &gt; \ [ nice\ ] &gt; ( on ) &gt; \ [ piece\ ] &gt; ( or ) &gt; \ [ paper\ ] - &gt; ( with ) &gt; \ [ pen\ ] ( 2 ) John uses the big crayon to draw rapidly on the paper .</sentence>
				<definiendum id="0">John</definiendum>
			</definition>
			<definition id="2">
				<sentence>'I'ING POIN'F : TW : letter Def : A letter is a message you write on paper , TG : same as CCKG CO : { letter } CCKG : \ [ write\ ] &gt; ( obj ) &gt; \ [ message ( let ter ) \ ] &gt; ( sub j ) &gt; \ [ per ... . : you\ ] &gt; ( on ) &gt; \ [ &gt; q , e , 'l Tll .</sentence>
				<definiendum id="0">'I'ING POIN'F</definiendum>
				<definiendum id="1">letter Def</definiendum>
				<definiendum id="2">letter</definiendum>
				<definiendum id="3">TG</definiendum>
				<definiens id="0">a message you write on paper ,</definiens>
			</definition>
			<definition id="3">
				<sentence>ssage is a group of words that is sent l'l'Olll ( ) lie person to ; ~ .</sentence>
				<definiendum id="0">ssage</definiendum>
				<definiens id="0">a group of words that is sent l'l'Olll</definiens>
			</definition>
			<definition id="4">
				<sentence>Those knowledge structm : es arc built within the Lexical Knowledge Base ( LKB ) , integrating lnultiple parts of the I , Kt~ around a particular concept to form a clus .</sentence>
				<definiendum id="0">LKB</definiendum>
				<definiens id="0">es arc built within the Lexical Knowledge Base (</definiens>
			</definition>
</paper>

		<paper id="2213">
			<definition id="0">
				<sentence>A category space is a multi-dimensional space in which the syntactic category of words is represented by a vector of co-occurrence counts ( Schiitze 1993 ) .</sentence>
				<definiendum id="0">category space</definiendum>
			</definition>
</paper>

		<paper id="1074">
			<definition id="0">
				<sentence>LOMTA is a large scale Natural Language Engineering ( NLE ) system .</sentence>
				<definiendum id="0">LOMTA</definiendum>
			</definition>
			<definition id="1">
				<sentence>A core problenr for NLE is the design of the internal representation .</sentence>
				<definiendum id="0">core problenr for NLE</definiendum>
				<definiens id="0">the design of the internal representation</definiens>
			</definition>
			<definition id="2">
				<sentence>This paper introduces two new criteria for semantic networks distribut , edness and non-linearity and tl , argc_scalc , Objectd ) ascd , Linguistic lnl , cracl , or , ~l % anslat , or , and Analyser discusses their relevance to NLE .</sentence>
				<definiendum id="0">Analyser</definiendum>
				<definiens id="0">discusses their relevance to NLE</definiens>
			</definition>
			<definition id="3">
				<sentence>'Phe nlorning star is the last p ( ) int of light in the sky to disal ) l ) ear at dawn , the evening star is Lhe first l ) oi , ,t of lighL in the sky to appear at dusk , and Venus is a particMar planet of the solar system .</sentence>
				<definiendum id="0">'Phe nlorning star</definiendum>
				<definiendum id="1">Venus</definiendum>
				<definiens id="0">a particMar planet of the solar system</definiens>
			</definition>
			<definition id="4">
				<sentence>Reading from IAI { MLR to I ) ONI ( I~Y1 , gives : r `` Entity I , 'ARMI'JR is a % uperset ' of I , 'ARMEI { 1 , which is a universal subject of Eu , which has action B , ArlS , and existential object \ ] ) ( ) NKEY1 .</sentence>
				<definiendum id="0">'ARMI'JR</definiendum>
			</definition>
			<definition id="5">
				<sentence>'Fhe 'lUIlox of KL-ONE based systems ( Woods , 1992 ) , ( Beierle , 1992 ) is Semantic Net based , the A-Box usually consists of a subset of FOL .</sentence>
				<definiendum id="0">'Fhe 'lUIlox of KL-ONE based systems</definiendum>
				<definiens id="0">Semantic Net based , the A-Box usually consists of a subset of FOL</definiens>
			</definition>
</paper>

		<paper id="2178">
			<definition id="0">
				<sentence>A translation rule includes a Japanese sentence or words corresponding to an English sentence or words .</sentence>
				<definiendum id="0">translation rule</definiendum>
			</definition>
</paper>

		<paper id="2146">
			<definition id="0">
				<sentence>ALT-J/E c~m trm~slate several special Japm~ese constntclions such as predicate idioms and funclion verb expressions ( Oku , 1992 ) .</sentence>
				<definiendum id="0">ALT-J/E c~m</definiendum>
				<definiens id="0">trm~slate several special Japm~ese constntclions such as predicate idioms and funclion verb expressions</definiens>
			</definition>
			<definition id="1">
				<sentence>• valency pattern `` Valency patterns '' are stnlcture patterns that formulate possible valency structures for predicates .</sentence>
				<definiendum id="0">Valency</definiendum>
				<definiens id="0">patterns '' are stnlcture patterns that formulate possible valency structures for predicates</definiens>
			</definition>
			<definition id="2">
				<sentence>L~L'~L : / ) ) ) ~ ; ~ { 7'~ b-l- &lt; o ( he introduced his sister to me ) ¢ morphological analysis and dependency analysis dependency structure .~ refer to valency pattern sentence analysis as ordinary construction ~r valelley strilt'|lll'e ( -J ~ ( ~ Ji '' `` I % ( past tense ) -- '~ ' ) . , , '' -~_ ( introduce ) ... . ~C ( st , bjectivecase ) } &lt; ctivecase l ) ~ ( iobjectivecas ; ~ Figure 3. An example of the Japanese sentence analysis. Japanese wdcncy mttern tbr `` ; i : ~- ' ) 7~ , ' N1 Sit : \ [ ( agent ) l ( subjective ) JR : /J~ , /~ ' , N2 SR : \ [ ( agent ) \ ] ( oljectivel ) JR : ~ N3 SR : \ [ ( agent ) l ( objective2 ) JR : L &lt; Remarks : Ni : I , abel fur a wdency element SR : Semantic restriction on a noun JR : Restriction on case-tam'king particles marks subjective case , to wflency elements in tile wflcncy pattern lbr the prcdicate. Thc analysis thcn tries to bind modifiers that have ml adverbial particle to the non-bolmd valency elements. Figure 3 shows an example of this type of Japanese sentence mmlysis for the sentcncc `` kare wa watashi ni kate no imouto we shoukai shita ( he theeduced his sister to me ) '' . First , the mmlysis tries to bind modifiers with case-tam'king parlicles `` we '' and `` ni '' to tile wdency elements in the wdency pattern for the predicate `` shoukai sent ( introduce ) '' which is oblaincd IYom lhc valency pattem dictionary. As modifiers satisfy bolh restrictions on the noun memfing and case-marking parlicles , they , are bound to N2 ( objective case 1 ) mid N3 ( objective case 2 ) , respectively. The ~malysis then lfies to bind lhe modifier with adverbial particle `` wa '' to a non-bound valency element. So far , N2 and N3 in the wflcncy paltem have already been bound. Therefore , as shown in figure 3 , the only non-bound valency element is N1 which is a subjective case. The adverbial particle `` wa '' c~m stand-in for case-marking p~wticlc `` ga '' , which is the non-bound wfiency clcment , mid the noun `` km-e ( he ) '' satisfies the scmantic restriction on the subjcctive case N1 i.e. \ [ ( agent ) \ ] . Therefore , file modifier with `` wa '' is bonnd to tim subjective case N1. Finally , lhc valency structure as shown in figure 3 is obtained. construction Many Japmmse adjective predicates domhmtc two subjective cases and so form the double-subject construction. The double-subject constmctiou having an Miective predicate actually has several wlrimlts , so no one approach c , 'm be uscd to mlalyze it. Accordingly , this section classifies the four types mid the characteristics of eadl type ~'e described. construction Tile Japanese doublc-subjcct consmlction ( also calicO the `` wa '' - '' ga '' conslruction ) can be classified into lhe following four types based on a previous study ( lshigami , 1977 ) : • type-I In this vafi~mt , adverbial particle `` wa '' is a proxy for a case-marking particle such as `` ni '' other than subjective case-ram-king `` ga '' . Example-1 shows `` wa '' as a proxy for case-marking particle `` hi '' in tile sentence `` wamshi no ie wa gakkou ga chikai ( file school is near my house ) '' . Example1 ( myhouse ) ( adverbial ( school ) ( case-marking ( nea 0 pro'title `` wa '' ) particle `` ga '' ) ~1¢ `` wa '' is \ [ br `` ni '' . proxy ( myhousc ) ( destination ( school ) ( subjective ( hem ' ) case-marking case-marking particle `` ni '' ) pmticle `` ga '' ) • type-2 in this wu'iant , adverbial parlicle `` wa '' is a proxy for case-umrking panicle `` no '' representing a noun modifier ( pre-nominal ) . Example-2 shows `` wa '' as a proxy for pre-nominal case-marking p~ulicle `` no '' in the sentence `` zou wa ham ga nagai ( elcplumls have long trunks ) '' . In this wu-itmI , the case of '' wa '' modifying a predicate must be analyzed as `` no '' modifying the 867 Japanese input sentence : 7~7~h~~ : t , ~o ( elephants have long trunks ) Japanese valency pattern for `` ~ 1~ v , dependency structure ~ ( subjective ) JR : 7~ /F @ @ ~ Ni : Label ~br a valency element refer to valency pattern SR : Semantic restriction on a noun on case-marking pazticles . ' men '' ana y~ i : sentence analysis as ordinary construction as double-subject construction r. , , , v encyst ct..e va , enc st. , c , ,.'o ' ( presen -- -- , tense ) -- '' / Figure 4. An example of analyzing double-suhject construction ( type-2 ) . case with subjective case-marking particle `` ga '' . Therefore , this analysis involves re-formation of the valency structure. Examlfle-2 ( elephant ) ( adverbial 0rose ) ( case-marking ( long ) particle `` wa '' ) particle `` ga '' ) ~¢ `` wa '' is proxy for `` no '' . ( elephant ) ( pre-nominal ( nose ) ( subjective ( long ) case-marking case-marking particle `` no '' ) particle `` ga '' ) • type-3 In this variant , the case with case-marking particle `` ga '' sometimes represents ~m objective case. Allhough an objective case is usually marked by case-marking particle `` wo '' , some adjective predicates have an oNective case marked by case-marking particle `` ga '' . Example-3 shows that `` kanojo ( she ) '' with `` ga '' is an objective case and `` kare ( he ) '' with `` wa '' is a subjective case in the sentence `` kare wa kanojo ga snkida ( he likes her ) '' . As case-marking particle `` ga '' normally indicates the subjective case , binding `` ga '' to the subjective case leads to incorrect mmlysis if only surface spelling is considered. Examlfle-3 ( he ) ( adveNial ( her ) ( case-marking ( like ) particle `` wa '' ) particle `` ga '' ) • , , , , `` , a '' re rcsenLs • `` r~roxytor ~a.| g , P .. Pwa '' lS ~ ~ ~V all oDjecuve case ~ ( subjective ( her ) ( objective ( like ) ( he ) case-marking case-marking particle `` ga '' ) particle `` wo '' ) • type-4 In this variant , the case with adverbial particle `` wa '' acts as all adverbial phrase representing time and is actually a special form of type-1. Representing time is optional for most predicates. Moreover , Japm~ese time expressions are often translated into English adverbial phi'ases. Therefore , type-4 is separated from type-1 in this classification from viewpoint of enghmering. Example-4 shows that time expression `` 6-gatsu wa ( in Jmle ) '' acts as an adverbial phrase in the sentence `` 6-gatsu wa ame ga tot ( it has much rain in June ) '' . Example-4 ( June ) ( adverbial ( rain ) ( case-marking ( ninth ) ~ ( t particle `` wa '' ) particlc `` ga '' ) ime expression with `` wa '' acts as an adverbial phrase ( June ) ( rain ) ( subjective ( much ) ( adverbial case-marking phrase ) particle `` ga '' ) subject construction Type-I and type-4 cases can be analyzed using the processing flow described in section 3 because adverbial particle `` wa '' simply acts as a case-marking particle. However , the following problems , arise when processing type-2 and type-3 cases in the normal way. • Problem with type-2 cases Fignre 4 shows the m~alysis of the type-2 example , `` zou wa hana ga nagai ( elephanls have long thinks ) '' . The predicate `` nagai ( long ) '' has only one valency element N1 with `` ga '' . According to ordinary sentence analysis , the modifier `` hana ga '' is bound to the valency element NI , which means that the other modifier `` zou wa '' is left unbound. That is , sentence analysis camlot be completed as shown in the left bottom of figure 4. This complicates the accurate Wanslation of this modifier into English. 868 Japanese input sentence : ~:1 : ~/ &amp; '~O~ ~'57o ( he likes her ) 1¢ morphological Japanese wdency pattern lbr `` ~ ( . ( `` ~ 7'~ ( ' '' analysis ~ln ( t dependency analysis dcpcndcllcy structure T N1 ISR : \ [ ( human ) \ ] , \ [ ( animal ) \ ] //~ -- ; ~-~'~ ~Q ~-\ ] ~ '' ~ f : / ) f @ \ ] 5 ' ( present tense ) ~ .fly , } ( suhjective ) \ ] I/JR : 75 { k ... Ol~w~ ) .. } ~h-e ) ( @ ../~ ... ~ ( like ) ./ k ... _ -- -~-~.-~ , ~ ~ `` l ( obiective , ! ~ R '' t ; -~ ... ... ... ... ... -~ refer to valency pattern L__ '' _____ `` '~ ' Remarks : Sentence mmlysis Ni : l.ahel for it valency element as ordimu T constrttclion SR : Semantic restriction on a noun wrong valency str.cture ¢ ( : ~ '' ~L ~ /5 '' ( present tensei -- -'L. ''~ `` ~ , ______ ( like ) _ / I -- -- -m ' -- I '' &gt; &lt; `` \ [ -~ -- N~ -- -I Sentence analysis JR : Restriction on case-mm-king patlicles as double-subject construction ~ correct v~n-e Q'-~ ~ : f~ . '</sentence>
				<definiendum id="0">agent ) l</definiendum>
				<definiens id="0">he introduced his sister to me ) ¢ morphological analysis and dependency analysis</definiens>
				<definiens id="1">/J~ , /~ ' , N2 SR : \ [ ( agent ) \ ] ( oljectivel ) JR : ~ N3 SR : \ [ ( agent</definiens>
				<definiens id="2">all adverbial phrase representing time and is actually a special form</definiens>
				<definiens id="3">an adverbial phrase ( June )</definiens>
				<definiens id="4">adverbial case-marking phrase ) particle `` ga ''</definiens>
			</definition>
</paper>

		<paper id="2198">
</paper>

		<paper id="2212">
			<definition id="0">
				<sentence>( ) lie of bile fulida , rlrient~J issues concernhlg corpus-l ) ased NI , P is t ; he ( tmLa 8I ) a , rsetless prot ) len'l. In view of the eft'e ( ' , tiveliess of class-ha , seal ll-gl'a , lll \ ] &lt; % ll-gllage nlodels i~gMnst Lhe ( \ ] ~ta s\ ] 7 ) i~l'Seliess i ) rol ) lenl ( Kneser iLli ( l Ney 1993 ) , it ; is expected t ; l-li~t classes of words are Mso usefiil for NI , P tasks ill such a wi~y that statistics oil ( : \ ] ~sses ; tre used whenever stal ; istics oil individua , l words il , i'e una , vaihdlle or unreli &amp; i ) le .</sentence>
				<definiendum id="0">P tasks</definiendum>
			</definition>
			<definition id="1">
				<sentence>All ide , al type of clusi , ers for N I , P is the ( ) lie which gu ; tra , rltees in ut iia\ [ substitu I ; M ) ilit , y , ill tern'is ( ) f t ) oth synl ; a , ctic a , ud seltilultic SOUll ( llleSs , &amp; lnOllg words in the sa , rtle class .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">lnOllg</definiendum>
				<definiens id="0">the ( ) lie which gu ; tra , rltees in ut iia\ [ substitu I</definiens>
			</definition>
			<definition id="2">
				<sentence>, ) ( : ~ ) pl~ ( l ) pr~ ( m ) where p/~ ( l , m ) is the probM ) ility that a woM iN ( , '~ ( 1 ) is followed l ) y i~ word ill C~ ( m ) , mid pl~ ( l ) : ~_f~l ) ~ ( l , ,n ) , pr , : ( m ) -= ~_~l , ~ : ( l , ,n ) .</sentence>
				<definiendum id="0">p/~</definiendum>
				<definiens id="0">followed l ) y i~ word ill C~</definiens>
			</definition>
</paper>

		<paper id="2163">
			<definition id="0">
				<sentence>BGIt has a six-layered abstraction hierarchy and more than 60,000 .</sentence>
				<definiendum id="0">BGIt</definiendum>
				<definiens id="0">a six-layered abstraction hierarchy and more than 60,000</definiens>
			</definition>
			<definition id="1">
				<sentence>Af The association score A ( v , c ) of a verb v and a noun class c is defined as Pr ( v , c ) Pr ( c \ [ v ) l ( v ; e ) A ( v , c ) -Pr ( c I v ) log Pr ( v ) Pr ( c ) The association score takes the mutual information between the verb and a noun class , and scales 1This process corresponds to sense disamblguation by maximizing the association score .</sentence>
				<definiendum id="0">Af The association score A ( v</definiendum>
				<definiendum id="1">c ) -Pr ( c I</definiendum>
				<definiendum id="2">association score</definiendum>
				<definiens id="0">takes the mutual information between the verb and a noun class</definiens>
			</definition>
			<definition id="2">
				<sentence>Given the collection of bilingual surface case structures for v j , we introduce the bilingual class/class association score for measuring the association of a class cE of English predicates and a class cj of Japanese case element nouns for a case marker p. Let Eg ( vg , p ) be the set of bilingual surface case structures collected fronl the JapaneseEnglish parallel corpora , each element of which has a Japanese verb vj and a Japanese case marker p. Among the elements of Eg ( vj , p ) , let Eg ( vj , p , c~ ) be the set of those whose semantic label SEME of the English predicate satisfies the class c~ , i.e. , SEME ~ cE , and Eg ( vj , p/cj ) be the set of those whose semantic label SEMj of the Japanese case element noun for the case marker p satisfies the class c j , i.e. , SEMj cj .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the set of those whose semantic label SEME of the English predicate satisfies the class c~</definiens>
			</definition>
			<definition id="3">
				<sentence>Then , conditional probabilities Pr ( cE Ira , p ) , Pr ( cj I va , p ) , and Pr ( cE , cj I vj , p ) are defined as the ratios of the numbers of the elements of those sets : \ [ Eg ( vJ , p , cE ) \ [ pr ( c Pr ( , : , , I vJ , p ) IE ( v , p ) I Pr ( cE , cJ I vj , p ) = \ ] Eg ( v , p ) l Then , given vj and p , the association score A ( c~ , .</sentence>
				<definiendum id="0">Pr ( cj I</definiendum>
				<definiendum id="1">p</definiendum>
				<definiens id="0">va , p ) , and Pr ( cE , cj I vj , p ) are defined as the ratios of the numbers of the elements of those sets : \ [ Eg ( vJ , p , cE ) \ [ pr ( c Pr ( , : , , I vJ , p ) IE ( v ,</definiens>
				<definiens id="1">p ) = \ ] Eg ( v , p ) l Then , given vj and p , the association score A ( c~ ,</definiens>
			</definition>
			<definition id="4">
				<sentence>A Japanese case-class frame can be represented as a feature structure : Pm : CJm 4pr ( cd \ [ vj , p , cE ) and Pr ( eE I vj , p/cd ) are too large in lower parts of the thesaurus , since we focus oi1 examples which have a Japanese verb v.l and a Japanese case marker p. When we used the average of Pr ( ej I vJ , p , cE ) and Pr ( e~ \ [ vj , p/cj ) instead of Pr ( eE , cj \ ] vj , p ) in the experiment d section 6 , most discovered clusters consisted of only one example .</sentence>
				<definiendum id="0">Japanese case-class frame</definiendum>
				<definiens id="0">cd \ [ vj , p</definiens>
			</definition>
			<definition id="5">
				<sentence>Then , conditional probabilities 1 ' , ' ( c~ : Iv j ) , Pr ( fa ira ) , and I'r ( cm fjiva ) are defined as the ratios of the numbers of the elements of those sets : \ [ ) I ' ( CE I V J ) -\ ] ~ ( Vj ) l t ' , .</sentence>
				<definiendum id="0">Pr ( fa ira</definiendum>
				<definiendum id="1">I'r</definiendum>
				<definiens id="0">the ratios of the numbers of the elements of those sets : \ [ ) I ' ( CE I V J</definiens>
			</definition>
			<definition id="6">
				<sentence>( S 's Iv , ) iEv ( ~ 's ) I Pr ( c~ : , f 's I~a ) = Then , given va , the association score A ( cE , fal V 'S ) of cg and fj is defined as A ( cF , , f , , Iv , s ) = PI '' ( CE , f 'S I~ 's ) Pr ( es , : , f , , Iva ) log Pr ( c~ I v 's ) Pr ( f 's I v 's ) As well as the case of the bilingual class/class association score , this definition only needs the set Eg ( 'va ) for a Japanese verb va , not the whole Japanese-English parallel corpora .</sentence>
				<definiendum id="0">fj</definiendum>
				<definiens id="0">~ 's ) I Pr ( c~ : , f 's I~a ) = Then , given va , the association score A ( cE , fal V 'S ) of cg and</definiens>
				<definiens id="1">A ( cF , , f , , Iv , s ) = PI '' ( CE , f 'S I~ 's ) Pr ( es , : , f , , Iva ) log Pr ( c~ I v 's ) Pr ( f 's I v 's ) As well as the case of the bilingual class/class association score</definiens>
			</definition>
			<definition id="7">
				<sentence>Then , among those pairs of c~ and f j , search for a pair eel and fJi which gives maximum association score max A ( cE , fa Iv j ) , a and collect the elecE , fj ments of Eg which satisfy the restrictions of CEi and fJi into the set Eg ( va , eel , f.'i ) .</sentence>
				<definiendum id="0">fJi</definiendum>
				<definiens id="0">gives maximum association score max A ( cE , fa Iv j ) , a and collect the elecE , fj ments of Eg which satisfy the restrictions of CEi</definiens>
			</definition>
			<definition id="8">
				<sentence>26 Association Score 3 0.299 3 0.237 7 0.459 zku Intellect ( Levell , open ) /ga ( NOM ) :14 ( Products ) ( Level2 , to ( door ) ) hold ( Leaf ) / wo ( A CC ) :13510l-CLeve16 , kaigou ( meeting ) ) ~au Completion ( Leaf-I , ~Level4 , negai ~ Quantity ( Leaf-3 , equal ) /ni ( DAT ) :12000-3-10 ( Leaf , kate ( he ) ) 12 3 I 0.460 iment .</sentence>
				<definiendum id="0">Completion</definiendum>
				<definiens id="0">to ( door ) ) hold ( Leaf ) / wo ( A CC</definiens>
			</definition>
</paper>

		<paper id="2106">
			<definition id="0">
				<sentence>A bottom-up hypothesis describes a passive edge ( complete subtree ) constructed by tile syntax parser and consists of the identifier of the rule instantiation that represents the edge and the completion history of the constructed passive edge .</sentence>
				<definiendum id="0">bottom-up hypothesis</definiendum>
				<definiens id="0">describes a passive edge ( complete subtree ) constructed by tile syntax parser and consists of the identifier of the rule instantiation that represents the edge and the completion history of the constructed passive edge</definiens>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>The variation of a character is measured by the inverse of the frequency of the character to be the other words .</sentence>
				<definiendum id="0">variation of a character</definiendum>
				<definiens id="0">measured by the inverse of the frequency of the character to be the other words</definiens>
			</definition>
			<definition id="1">
				<sentence>( 2 ) 1 ' ( C1 ) x P ( C2 ) x P ( C3 ) 1 l 1 ( 3 ) P ( CI ) x -x t ' ( C2 ) x -x P ( C3 ) x -~ - &amp; Cl &amp; C2 &amp; C3 # CI # C2 # C3 ( 4 ) -x -× - &amp; CI &amp; ( 72 &amp; C3 where Ci is a character in the input sentence , P ( Ci ) is the probability of Ci to be a surname or a name , # Ci is the frequency of Ci to be a surname or a name , 223 &amp; Ci is the frequency of Ci to contain in tile other words .</sentence>
				<definiendum id="0">Ci</definiendum>
				<definiendum id="1">Ci</definiendum>
				<definiens id="0">a character in the input sentence</definiens>
				<definiens id="1">the probability of Ci</definiens>
				<definiens id="2">the frequency of Ci to be a surname or a name</definiens>
				<definiens id="3">the frequency of Ci to contain in tile other words</definiens>
			</definition>
			<definition id="2">
				<sentence>Text provides many useful clues from three different levels say , character , sentence and paragraph levels .</sentence>
				<definiendum id="0">Text</definiendum>
				<definiens id="0">provides many useful clues from three different levels say , character , sentence and paragraph levels</definiens>
			</definition>
			<definition id="3">
				<sentence>Mutual information ( Church &amp; Hanks , 1990 ) provides a measure of word association .</sentence>
				<definiendum id="0">Mutual information</definiendum>
				<definiens id="0">provides a measure of word association</definiens>
			</definition>
			<definition id="4">
				<sentence>Here Ci denotes a Chinese character .</sentence>
				<definiendum id="0">Ci</definiendum>
			</definition>
			<definition id="5">
				<sentence>Keyword is a good indicator for an identification system .</sentence>
				<definiendum id="0">Keyword</definiendum>
			</definition>
			<definition id="6">
				<sentence>Prefix is a good marker for possible left boundary .</sentence>
				<definiendum id="0">Prefix</definiendum>
				<definiens id="0">a good marker for possible left boundary</definiens>
			</definition>
			<definition id="7">
				<sentence>Sentence alignment ( Chen &amp; Chen , 1994 ) is important in 228 setup of a bilingual corpus .</sentence>
				<definiendum id="0">Sentence alignment</definiendum>
				<definiens id="0">important in 228 setup of a bilingual corpus</definiens>
			</definition>
</paper>

		<paper id="2111">
			<definition id="0">
				<sentence>Let Age , be the generalization A rq A ' of A and A ' ; then we define X and X ' as distinct copies of Ag~ , such that for every path n where A @ n = A ' @ ~ , it also holds that X @ n = X ' @ n , where F @ n is `` the value of the feature structure F at some path n at which it is defined '' ( Carpenter 1992:38 ) and '= ' denotes token identity .</sentence>
				<definiendum id="0">Age</definiendum>
				<definiendum id="1">X</definiendum>
				<definiendum id="2">F @ n</definiendum>
				<definiens id="0">the generalization A rq A ' of A and A '</definiens>
				<definiens id="1">distinct copies of Ag~ , such that for every path n where A @ n = A ' @</definiens>
			</definition>
			<definition id="1">
				<sentence>IfB is a goal and B ' is a parsed left corner such that &lt; X , X ' &gt; e L and B u X and B ' u X ' exist , then there is a link between B and B ' ; we can stop here with a mere test of unification if we only want to use linking as a filter to reduce the search space .</sentence>
				<definiendum id="0">IfB</definiendum>
				<definiens id="0">a goal and B ' is a parsed left corner such that &lt; X</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>The qualla structure ( QUALIA ) encodes the basic semantic type of a word ( its Lexical Coilceptual Paradigm , or LCP ) and specifics how it is linked to other events and arguments of the event and argument structures ( see Pustejovsky , 1995 , chapter 6 ) .</sentence>
				<definiendum id="0">QUALIA )</definiendum>
				<definiendum id="1">LCP</definiendum>
				<definiens id="0">encodes the basic semantic type of a word ( its Lexical Coilceptual Paradigm , or</definiens>
			</definition>
			<definition id="1">
				<sentence>Each of these types can be projected independently , if no other constraints apply ( see event encoded in the AGENTIVE role ( e2 ) denotes the cause or origin of the state , i.e. the experiencing event ; encoded in the TELIC role ( e3 ) , it denotes then the manifestation of the state , i.e. the intellectual act .</sentence>
				<definiendum id="0">e2 )</definiendum>
				<definiens id="0">the manifestation of the state</definiens>
			</definition>
			<definition id="2">
				<sentence>( 18 ) ( 19 ) FORMAL ¥ triste ( el~je ) ) TBmC = P ( ea , je ) AGENTIVZ = partir ( e2de ) FOCAL = ing6nieux ( elje ) \ ] TELIC = jouer ( eaje,4checs ) ( 20 ) *Je suis habile &amp; ttre malade `` I 'm skilful at being ill '' Saturation of the object of the experiencing or intellectual act event .</sentence>
				<definiendum id="0">FORMAL ¥ triste</definiendum>
				<definiens id="0">I 'm skilful at being ill '' Saturation of the object of the experiencing or intellectual act event</definiens>
			</definition>
</paper>

		<paper id="2104">
			<definition id="0">
				<sentence>sequence extraotion rules : ~ 20 rules \ [ R\ ] R momheme-POS allooat on rules : 14 rules f~\ ] mor~heme-POS disa~bi~uation rules : ~ 50 rules word-constituent rules ~ 60 rules f~\ ] bunaeLsu head exceptional rules ~ 20 rules auxiliary functional verb rules ~ 15 rules Syntaotical ~nalyser \ [ F\ ] bunsetsu features \ [ R\ ] bunaetsu features saltine rules \ [ R\ ] kakari-uke par extracting rules \ [ R\ ] kakari-uke pair exoeptional rules \ [ R\ ] kakari-uke failure reoovery rules 68 features 80 rules 20 rules 40 rules -~ 4 rules • ~\ [ O\ ] : diotionsry \ [ T\ ] : table \ [ R\ ] : rule \ [ P\ ] : POS \ [ F\ ] : foature \ [ 50\ ] ( I1 ) : tll~lll~l~-~ \ [ 49\ ] ( 1O ) : •~ , ~ • : Sol•uteri Pair \ [ 48\ ] ( 9 ) : xO4~ltt &amp; O : Possible Pair \ [ 47\ ] ( 9 ) : • r_ &amp; , ,~ • : Struoturally Prohibited Pair \ [ 46\ ] ( 8 ) 7:0 x•~7~ x : Poaaible l~t \ [ 45\ ] ( 7 ) : • x • • •~C~ StruotursllY Prohibited Pair \ [ 44\ ] ( 6 ) &gt; : x x x • 00~I~ \ [ 43\ ] ( 7 ) ?</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">Sol•uteri Pair \ [ 48\ ] ( 9 ) : xO4~ltt &amp;</definiens>
			</definition>
</paper>

		<paper id="1083">
			<definition id="0">
				<sentence>r &lt; Ollbexl,8. 1 @ 0111 the clique { sl , +e~ , o , sc , b. &lt; Uos b obsl , r~ml , ion , altcinb : } ( of. fig. 3 ) , one ca , t\ ] build l lie cla.ss of all'eel ; ions which arc Io ( : al ; ed in l , he I ) odv as { Idam. : , occ1. &lt; ~7o. , s/..~.Js &lt; ~ , Ic , ~7o. , &lt; : .l &lt; : { li~ : ali &lt; . , , ob , ~'l , rl , :l , ion , aZl , c'inl , c } . Siinila.rly , from l , he gt'al ) h o\ [ ' I ; he ( ~ 'M ( 7 corpus , Oile ( : a.li i ( leni , i\ [ 'y l.ll~ classes of body '' ~ii , cs { artcrc , I.'anchc , rcs+sa~l , `` v &lt; 'ntri , ~dc , intc ' , rve , nlriculairc , cft'roli ( l~ , } , o ( `` diseases { 'malmli+ , arth , crvsclcrose } and oF chirul'gica/ m : ts { l ) o*ll , ( ~gc , rcvasc.ularisatio'n , angioplastic } . Olic ( ; l.\ ] ieS ( ; ( ; Oli ( : ( 'pts axe identilied , t , hei r i ) rolJei ; { , i ( '~S Ca , fl I ) C lisl , e ( l , t ) y i llt ; erl ) l : el , ing ~ I ; he l ld ) cls of I lie links , 'l'he al ; t ; ri hu I~ ( , of the localizaJJon of l , h ( ' aJl'ecl ; ions is descril ) ed l ; \ ] irough three , kinds oF u : lodiliers ( lig. ' , / ) : , io , ,n , ~ ( , - , ( t &lt; '. { a , 'Z , .~ , ' &lt; ~ ' , t , ' , ' &lt; , ,u : ) ) , , , ; , i , os &lt; ' ~ , , : lyrics ( ~ d ( ; { ca'rotTd &lt; ' , # tl , crventrTculaTre } aud a ( l , iectivcs rela , l ; cd to ; ~ q ) ( ' ( : ific aa'l ; ery ( ~ { coro # utiru , co'ronaricn , diaqonal , ci'lvonfl &lt; ~ : ( ; } ) . 'l'he a l , i ; i : iblll ; e ( legr ( : e of ( ; lie a , fl'ecl , ion is a , lso reveaJed IJlrougjh { ~ si , qm { /ical , if , n &lt; m- , siqnificati.l ; severe , 7m , l ) Orl.a.l. , s &lt; ; veriZc } . `` ( 41roupe '\ [ 'erniinologie el ; lnl ; elligence Ari ; iticielle , I ~ I { C -- ( _I I ) 171. | ul ; elligcnce A r @ icicltc , ( 7 NI { S 493 etude~ ~evaluatlon a t~.alyse calcul etudeS5 b N ~ essai analyse Figure 4 : Polysemy of etude Last , relationships between concepts can be extracted , such as the '' part-of '' relation between tronc and artere , and segment and artere ( fig. 3 ) . Polysemy and quasi-synonymy often makes the ontological reading of linguistic data difficult. However , through cliques and edge labels , the SYCLADE structured and documented map of the words helps to capture the word meaning level. Among a set of connected words where w is similar to wi and wj , cliques bring out coherent subsets where wi and wj are also similar to each other. We argue that the various cliques in which a word appears represent different axes of similarity and help to identify the different senses of that word. For instance , in the whole set of words connected to etude ( study ) in a strongly connected component of the NTC graph ( analyse , evaluation , resultat , presentation , principe , calcul , travail ... ) , some subsets form cliques with etude. Two of those cliques ( resp. a and b in fig. 4 threshold of 7 ) bring out a concrete and a more theoretical use of etude. The network also enables to distinguish the uses of quasi-synonyms such as eoronaire and coronarien in the CMC corpus. Even if they are among the most similar adjectives ( 7 shared contexts ) and if they belong to the same clique { coronaire , eoronarien , diagonal , circonflexe } , the fact that eoronarien alone is connected to evaluation adjectives ( severe , signifieatif and important ) shows that they can not always substitute to each other. esfimatation for the building of ontologies The comparison with the similarity score of ( Hindle , 1990 ) shows that SYCLADE similarity indicator is specifically relevant for ontology bootstrap and tuning. Hindle uses the observed frequencies within a specific syntactic pattern ( subject/verb , and verb/object ) to derive a cooccu , &gt; rence score which is an estimate of mutual information ( Church and Hanks , 1990 ) .</sentence>
				<definiendum id="0">ion</definiendum>
				<definiens id="0">.l &lt; : { li~ : ali &lt; . , , ob , ~'l , rl , :l , ion</definiens>
				<definiens id="1">the ontological reading of linguistic data difficult. However , through cliques and edge labels , the SYCLADE structured and documented map of the words helps to capture the word meaning level. Among a set of connected words where w is similar to wi and wj , cliques bring out coherent subsets where wi and wj are also similar to each other. We argue that the various cliques in which a word appears represent different axes of similarity and help to identify the different senses of that word. For instance</definiens>
				<definiens id="2">specifically relevant for ontology bootstrap and tuning. Hindle uses the observed frequencies within a specific syntactic pattern ( subject/verb , and verb/object ) to derive a cooccu</definiens>
				<definiens id="3">an estimate of mutual information</definiens>
			</definition>
			<definition id="1">
				<sentence>9For instance , for Na PN2 CoocNi , N~ : log 2 ~~ where f ( NIPN2 ) is the fi'equency of noun N1 occurring with N2 in a noun preposition pattern , f ( N1 ) is the frequency of NI as head of any N1PN , ~ sequence and f ( N2 ) the frequency of N2 in modifier/argument position of auy N~PN2 sequence and k is the count of NxPN v elementary trees in the corpus .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">the count of NxPN v elementary trees in the corpus</definiens>
			</definition>
</paper>

		<paper id="2205">
			<definition id="0">
				<sentence>`` Z~mY2 M ( , C ) =logs N ( eq.1 ) N N In the above equation , N is the total number of cooccurrence data bits , and f ( v ) and f ( C ) are the frequency of v and C in the whole cooccurrence data set respectively , and f ( v , C ) is the frequency of the cooccurrence data \ [ C , wo , v\ ] .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">f</definiendum>
				<definiens id="0">the total number of cooccurrence data bits , and f ( v ) and f ( C ) are the frequency of v and C in the whole cooccurrence data set respectively , and</definiens>
				<definiens id="1">the frequency of the cooccurrence data \ [</definiens>
			</definition>
</paper>

		<paper id="1094">
			<definition id="0">
				<sentence>However , the Old-to-New ordering prim : iple is a generalization to which exceptions can be found .</sentence>
				<definiendum id="0">iple</definiendum>
				<definiens id="0">a generalization to which exceptions can be found</definiens>
			</definition>
			<definition id="1">
				<sentence>The backward looking center ( Cb ) is the most salient member of t , he Cf list that links the era'rent utterance to the iwevious utterance .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiens id="0">the most salient member of t , he Cf list that links the era'rent utterance to the iwevious utterance</definiens>
			</definition>
			<definition id="2">
				<sentence>The Cb of an utterance is delined as the highest ranke ( l element of the previous u tterance 's Cf list that also occurs iu the curren ( , utterance .</sentence>
				<definiendum id="0">Cb of an utterance</definiendum>
				<definiens id="0">u tterance 's Cf list that also occurs iu the curren ( , utterance</definiens>
			</definition>
			<definition id="3">
				<sentence>The Sentence Planner uses the algorithms in the following subsections to determine the topic , focus , and ground from the given semantic representation ~md the discourse model .</sentence>
				<definiendum id="0">Sentence Planner</definiendum>
				<definiens id="0">uses the algorithms in the following subsections to determine the topic , focus , and ground from the given semantic representation ~md the discourse model</definiens>
			</definition>
			<definition id="4">
				<sentence>Mnltiset-CCG captures the contextdependent meaning of word order in 'Fnrkish by compositionally deriving the predicate-argument structure and the information strnctm'e of a sentence in parallel .</sentence>
				<definiendum id="0">Mnltiset-CCG</definiendum>
				<definiens id="0">captures the contextdependent meaning of word order in 'Fnrkish by compositionally deriving the predicate-argument structure and the information strnctm'e of a sentence in parallel</definiens>
			</definition>
</paper>

		<paper id="2136">
			<definition id="0">
				<sentence>Without loss of generality , we can write , P ( ~ I &lt; ~z &gt; ) = p ( c~ ... , :~ I &lt; ~z &gt; ) = r ( k ) P ( , : , ... , ; ~lk ) ( 7 ) where &lt; '.1 • • • &lt; '+ is the character sequence of length k that constitutes word wi .</sentence>
				<definiendum id="0">~lk )</definiendum>
				<definiens id="0">p ( c~ ... , :~ I &lt; ~z &gt; ) = r ( k ) P ( , : , ... , ;</definiens>
				<definiens id="1">the character sequence of length k that constitutes word wi</definiens>
			</definition>
			<definition id="1">
				<sentence>Edit distance is defined as the ntiniulum number of editing operations ( in sertions , deletions , and substitutions ) required to transform one string into another .</sentence>
				<definiendum id="0">Edit distance</definiendum>
				<definiens id="0">the ntiniulum number of editing operations ( in sertions , deletions , and substitutions ) required to transform one string into another</definiens>
			</definition>
			<definition id="2">
				<sentence>The average numt &gt; er of candidates ibr a character w~s 8.9 in these character matrices 4 First , we compared the proposed word-based spelling corrector using the POS trigram model ( POSe ) with tile conventional character I ) msed spelling eorreetor using tile character trigram model ( Char3 ) .</sentence>
				<definiendum id="0">average numt &gt; er of candidates</definiendum>
				<definiendum id="1">POSe</definiendum>
				<definiens id="0">tile conventional character I ) msed spelling eorreetor using tile character trigram model ( Char3 )</definiens>
			</definition>
			<definition id="3">
				<sentence>For comparison , we count tile number of tuples in tile standard ( Std ) , the number of tuples in the system output ( Sys ) , and tile number of matching tuples ( M ) .</sentence>
				<definiendum id="0">Sys</definiendum>
				<definiendum id="1">matching tuples</definiendum>
				<definiens id="0">the number of tuples in the system output</definiens>
			</definition>
			<definition id="4">
				<sentence>I in ( 11 , o ~tnd Ma , ruya , tn~t , 1992 ) , they itchicvcd 94.61 % , &lt; ha , la , &lt; . : tcr I't ? ( : O , ~ , ll\ [ LiOll a+c ( : tu'+t &lt; : y when | , } it | ) a , selinc ~tc ( : ur+~ &lt; : y is 87.46 % \ [ m '' pal , elLS h , uhx : tri &lt; : c.gilme.rlng , dora ; tin. We ~t ( : hit : vcd 9fi.0 % c\ ] l.+trltci , cr J'e ( : og.il , ion +~ ( : c.ra ( : y , when the I ) +~ell.c a+ ( : c.r+~cy i~ 90 % in thu cunf ( '.rcn &lt; : c roy &lt; istr~tLion doma.i.. It is very ( liflic , lt to c , . ) nlt ) a+v. ! our rusu\ ] ts with thu previous rcsUlll\ [ , ~ I &gt; ( : ca , ust !</sentence>
				<definiendum id="0">y</definiendum>
				<definiens id="0">tu'+t &lt; : y when |</definiens>
			</definition>
</paper>

		<paper id="2168">
</paper>

		<paper id="2185">
			<definition id="0">
				<sentence>Korean language engineering is one for Korean language .</sentence>
				<definiendum id="0">Korean language engineering</definiendum>
				<definiens id="0">one for Korean language</definiens>
			</definition>
			<definition id="1">
				<sentence>sign characteristics , which lead to the ease of system design and tlexibility of the system config~ rations ( Berners-Lee ~5 Connolly , 1993 ) .</sentence>
				<definiendum id="0">sign characteristics</definiendum>
			</definition>
			<definition id="2">
				<sentence>IP consists of three parts .</sentence>
				<definiendum id="0">IP</definiendum>
			</definition>
			<definition id="3">
				<sentence>Keyword-in-context ( KWIC ) manager deals with word usage of text corpus .</sentence>
				<definiendum id="0">Keyword-in-context ( KWIC</definiendum>
				<definiens id="0">) manager deals with word usage of text corpus</definiens>
			</definition>
</paper>

		<paper id="2169">
</paper>

		<paper id="1091">
			<definition id="0">
				<sentence>\ [ B\ ] A Ao-13 A : D B B A The normal form proofs of this system have a straightforward structural characterisation , that their main branch ( the unique path fi'om an assumption to the proof 's end-type that includes no 3It follows that tile parsing method to be developed applies only to categorial systems having only implicational connectives .</sentence>
				<definiendum id="0">main branch</definiendum>
				<definiens id="0">the unique path fi'om an assumption to the proof 's end-type that includes no 3It follows that tile</definiens>
			</definition>
			<definition id="1">
				<sentence>Polarity applies also to subformulae , i.e. in a formula Xo-Y with a given polarity p , the subformula X has the same polarity p , and Y has the opposite polarity .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">the opposite polarity</definiens>
			</definition>
			<definition id="2">
				<sentence>index sets , and t0 denotes union of sets that are required to be disjoint ) : 4 ) : Ao-B : a '~/~ : B : b • ( bao ' , /~ 7r : A : ( all ) In proving I ' = &gt; A , a snccessflfl ow ' , rall analysis is recognised by the prescmee of a database formula 4See Llord &amp; Morrill ( 1995 ) fbr a related use of indexing in ensuring linear use of resources .</sentence>
				<definiendum id="0">t0</definiendum>
			</definition>
			<definition id="3">
				<sentence>For example , The ( indexed ) formula i : Xo- ( yo-Zo-W ) will compile to give three indexed formulae : i : Xo- ( Y : { j , k } ) j : Z k : W We , require a inodified elimination rule that will enforce appropriate usage : 5 ¢ : Ao- ( B : rt ) : a '~/~ : B : b ~r = eU~/~ 7r : A : ( ab ) Note that the compilation process must also gencrate additional assumptions corresponding to the positive subformulae of the right hand side of a query , e.g. compilal ; ion of Xo-Y , Yo-Z ~ , Xo-Z simplifies the right hand side formula to atomic X , giving and additional assumption Z. The second challenge we noted for such an approach is ensuring that a proof term ( loosely , the SNore the requirement that ( t is a proper subset of , /~ , which will have the consequence that other assumptions must also contrihute to deriving the argunwnt B. This will block a derivation of the linear logically valid Xo- ( yo-Y ) = &gt; X. However , this move accords with general categorial practice , where it is standm 'd to require that each deduction rests m , at least one assumption .</sentence>
				<definiendum id="0">Xo-Z</definiendum>
				<definiens id="0">simplifies the right hand side formula to atomic X</definiens>
			</definition>
			<definition id="4">
				<sentence>The , sequent ix proven iJ ) '' ( ( ) : G : u ) ( ~ A* fi ) r some assigmnent of a vahle 1 ; o 'a. Under t ; hat assignment , the original right hand side va.riable x will return a ( : omplete proof term for the imi ) lMt I ) roof of the original s ( ; qll ( : IIIL Not , c { ; hal l , he t ) i'ot ) f Lerllhq so t ) ro ( hlce ( l have a form whi ( : h ( : orrest ) onds , ( m ( h ; r l ; he Currylloward isomorphism , to itormal form dedu ( : l ; ions ( as defin ( xl earlier ) .</sentence>
				<definiendum id="0">ions</definiendum>
				<definiens id="0">omplete proof term for the imi</definiens>
			</definition>
			<definition id="5">
				<sentence>This method is based on a. relational algebraic model for L ( van l { enthem , 1991 ) , which inl ; erprets types as relent , ions on some set V ( intuitively , pointal 541 string positions ) , i.e. sets of ordered pairs from V x V ( intuitively , strings identified by delimiting points ) : D ( A\B ) = { ( vz , v3 ) \ [ V { vl , v2 ) C D ( A ) , ( vl , v3 ) e D ( B ) } D ( B/A ) = { ( vl , v2 ) I V ( v2 , v3 ) C D ( A ) , ( Vl , V3 ) e D ( B ) } Morrill specifies polar translation functions , which convert Lambek types that are marked for position ( 'span ' ) to labelled linear formulae .</sentence>
				<definiendum id="0">B ) } Morrill specifies polar translation functions</definiendum>
				<definiens id="0">relent , ions on some set V ( intuitively , pointal 541 string positions ) , i.e. sets of ordered pairs from V x V ( intuitively , strings identified by delimiting points</definiens>
			</definition>
</paper>

		<paper id="2183">
			<definition id="0">
				<sentence>Wc discuss a simplification process which identifies components of a sentence that may be separated out , and transforms each of these into frec-sta , ding simpler sentences .</sentence>
				<definiendum id="0">Wc</definiendum>
				<definiens id="0">discuss a simplification process which identifies components of a sentence that may be separated out</definiens>
			</definition>
			<definition id="1">
				<sentence>Text simplification uses the f~ct that complex texts typically contains complex syntax , some of which may be particular to specific domain of discourse , such as newswire texts .</sentence>
				<definiendum id="0">Text simplification</definiendum>
				<definiens id="0">of which may be particular to specific domain of discourse , such as newswire texts</definiens>
			</definition>
</paper>

		<paper id="2175">
</paper>

		<paper id="2165">
			<definition id="0">
				<sentence>n ( : ing in an emt ) irically adequate way which ensures decidahility of tile problem all ; hough a flflly expressive , language in used to represent discourse .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">ing in an emt ) irically adequate way which ensures decidahility of tile problem all</definiens>
			</definition>
			<definition id="1">
				<sentence>Lexical disambiguation is a procedure which works according to the communicative convention to interpret the discourse as consistent as possible , if there is a choice .</sentence>
				<definiendum id="0">Lexical disambiguation</definiendum>
				<definiens id="0">a procedure which works according to the communicative convention to interpret the discourse as consistent as possible</definiens>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>Guthrie and Yuasa used word frequencies for weighting ( Guthrie , 1994 ) , ( Yuasa , 1995 ) , and Tokunaga used weighted inverse document frequency which is a word frequency within the document divided by its fl'equency throughout the entire document collection ( Tokunaga , 1994 ) .</sentence>
				<definiendum id="0">Tokunaga used weighted inverse document frequency</definiendum>
			</definition>
			<definition id="1">
				<sentence>One of major problems using thesaurus ( 'ategories a.s sense represe : :tation is a statistical sparseness for thesaurus words , since they are nmstly rather uncommon words ( Niwa , 1995 ) .</sentence>
				<definiendum id="0">:tation</definiendum>
				<definiens id="0">a statistical sparseness for thesaurus words</definiens>
			</definition>
			<definition id="2">
				<sentence>Let x be a t ) olysemous noun and a sentence X be X '' • • • ~ 3 : -n~ • • • ~ a'-i ~ ~1 : ~ ~1:1 , • `` `` ~ ilYn~ `` `` ' The vector representation of X is V ( X ) = ~ V ( xi ) where V ( xi ) is V ( xi ) = ( Mu ( xi , o~ ) , ... , Mu ( xi , om ) ) Here , Mu ( x , y ) is the v ' , due of mutual information proposed by ( Church , 1991 ) .</sentence>
				<definiendum id="0">Mu</definiendum>
				<definiens id="0">V ( X ) = ~ V ( xi ) where V ( xi ) is V ( xi ) = ( Mu ( xi</definiens>
			</definition>
			<definition id="3">
				<sentence>A dissimilarity measure is the degree of deviation of the grout ) in an n-dimensionM Euclidean space , where 'n is the number of nouns which co-occur with t~ 1 and 'U 2 .</sentence>
				<definiendum id="0">dissimilarity measure</definiendum>
				<definiendum id="1">'n</definiendum>
				<definiens id="0">the degree of deviation of the grout</definiens>
				<definiens id="1">the number of nouns which co-occur with t~ 1 and 'U 2</definiens>
			</definition>
			<definition id="4">
				<sentence>'Freq ' , 'Link ' , 'Dis ' , and &lt; Method ' show the nulnlmr of sets which are clustered ( : orrectly in ea ( : h experiment. The samph ' results of 'Article = 20 ' fl ) r each ( , xperiment is shown in Figure 1 , 2 , 3 , and 4. In Figure 1 , 2 , 3 , mid 4 , the X-axis is the similarity wdue. A1 ) l ) reviation words in each Figure and categories are shown in Talile 8. of words : e and y in this order in st window size of 100 words. 409 I I 0.638 I *X \ [ '-BBK ~_~420 |TNM market~ STK 2'~~1 news L~ 260 16 REC 0 .206 metal -~CS retailingRET ~ \ [ ood rRFD O .141 restauranUFOD ARO e em ca t_MT C Figure 1 : Tile results of 'Freq ' experiment news t._ DIV ~ \ [ 0.890 RET CMD .-~981~843 a762 ARC ) _ : = : _==_.a TNM 0.956 ~C S ~382 BVG _____1 ~_1~871 CEO PRO ~_ ~.841 0,651 FOD ~ ~'814 ~204 ENV ' I \ [ -'-HEA I I I MTC STK BON Figure 2 : The results of 'Link ' experiment ... . 0,679 V ~~.53o marketlT~N'~l~ -- -I ~413 news / DI~_____ ~ ~_ 0.263 metalPCS ~ I HI 0 55 -0 263 \ [ restaurant\ ] REC~ ~ ~\ ] I FOP - , \ [ \ [ I t.. RFD ' I I BON0,1'~ I environmentENV 0~7N k scienceARO ~ \ [ _J \ [ farm~CMD 0~35 ~172 \ [ chemical\ [ HEA , ~ , oo - , -- .~j t-MTCJ 0.073 Figure 3 : The results of 'Dis ' experiment According to Table 7 , there , are 24 sets which could be ( : lustered correctly in 'Dis ' , while 21 sets in 'Freq'. Examining the results shown in Figure 3 , 'BVG ' and 'HRD ' are correctly classified into 'food • restaurant ' and 'market news ' , respectively. However , the results of 'Freq ' ( Figure 1 ) shows that they are classified incorre &lt; 'tly. Table BB 1\ [ DIV J ~.923 market I TN M 0~72~L~913 news I ~\ [ L863 I ~~ tAs2s t-HRD~ \ ] 10.819 scienceARO -~ metal-PCS 0.893 ' L , \ [ B VG ~58~-756 I FOD ~ I tl food I PRO ~\ ] I I.- ' '' restaurant\ [ hE~~7~J~ `` a '' t._ RF~I ) retailing-RRF ! l ) , ' ~ ~0:845l I 0 '' 5~73 , environment-ENV ehemical\ [ -MT C 0 farm-~M~Figure 4 : The results of 'Method ' experiment Table 8 : Topic : and category name Category market news science metal food restaurant Topic BBK : Buybacks BON : Bond Market News CEO : Dow Jones interview DIV : dividends ERN : em'nings HI/D : Hem 'd on the street STK : stock mm'ket TNM : tender offers ARO : aerospace PCS : precious metals , stones , gold BVG : beverages FOD : food products PRO : corporate profile REC : recreation , entertainment RFD : restaurant , supermarket retailing RET : retailing environment chemical farm ENV : environment HEA : health care providers , medicine MTC : medicM and biotechnology CMD : commodity news , farm products 9 shows different senses of word ill 'BVG ' , and 'HRD ' which could be discriminated in 'Dis'. In Table 9 , for example , 'security ' is high freqtlenties and used ill 'being secure ' sense ill 'BVG ' artMe , while 'security ' is 'certificate of creditorshiI ) ' sense in 'HRD'. One possible cause that the results of 'Freq ' is worse than 'Dis ' is that these polyselnous words which are high-frequencies are not recognised polysemy in 'Freq'. As shown in Table 7 , there are 23 sets which could be clustered correctly in 'Link ' , while 21 sets ill 'Freq'. For example , 'ERN ' and 'HRD ' are both concerned with 'market news'. In Figure 2 , they are clustered with high similarity wflue ( 0.943 ) , while in Figure 1 , they are not ( 0.260 ) . Exalnilfing the results , there are 811 nouns in 'ERN ' article , and 714 nouns in 'HRD ' , and 410 Table 9 : Different word-senses in BVG and HRD security rate sale stock BVG the state of being secure a quantity in relation the exchange of goods total goods HRD certificate of creditorship l ) rice of charge the alllOllltt of sold stock market of these , 'shares ' , 'stock ' , and 'share ' which are semantically similar ~re included. In linking method , there are 251 nmmn in 'ERN ' and 492 nouns in 'HRD ' whi ( 'h ~tre repl~tccd for representative words. However , in 'Freq ' , each noun corresponds different coordinate , and regards to different meaning. As a result , these tol ) ics are clustered with low similarity wdue. Tit ( '. results of 'Method ' show tha , t 31 out of 40 sets are cbLssified correctly , att ( I the per ( 'entage attained was 77.5 % , while 'Freq ' , 'Link ' , and 'Din ' ext ) eriment att , ~tined 52.5 % , 57.5 % , 6 ( ) .0 % , renl ) e ( : tively. This shows the effe ( -tivelmss of our method. In Figure 4 , the ~u'ticles , tre judged to ( 'l , tssify into eight categories. Examining 'ERN ' , 'CEO ' and 'CMD ' in Figure 1 , 'CE ( ) ' and 'CM1 ) ' are grouped together , while they have ( lifferent c~ , tegories with each other. On the other hand , in Figure 3 , 'ERN ' and 'CE ( ) ' ar ( , groul ) ed together corre ( 'tly. Examining the nouns which arc 1 ) elonging to 'ERN ' mid 'CE ( ) ' , 'plant ' ( factory and food senses ) , 'oil ' ( petrohmnl and food ) , 'order ' ( colmn~nd ; md dema.nd ) , and 'interent ' ( del ) t and curiosity ) whi ( : h are high frequencies ~re correctly dismnbiguated. Furthermore , in Figure 4 , 'ERN ' mM 'CEO ' are classified into 'market news ' , and 'CMD ' are cb~ssilied into 'fm : m ' , correctly. For example , 'plant ' which is used in 'factory ' sense in linked with semanti ( '~lly silnib~r words , 'ntanuf ; wturing ' , 'factory ' , 'production ' , or 'job ' et ( '.. In a simibtr way , 'i ) bmt ' which in uned in flood ' sense is linked with 'environmeltt ' , 'forest'. As a result , the articles are classified correctly. As shown in Table 7 , there arc 9 nets which could not 1 ) e clustered correctly in our method. A possible improwmmnt is that we use all definitions of words in the dictionary. We s ( qeeted the first top 5 definitions in the dictionary for each noun and used theln in the cxperilnent. However , there are some words of which the memfings are not included these selected definitions. Thin ( : ~mses the fact theft it is hard to get a higher percentage of correct clustering. Another interesting t ) ossibil ity in to use ml altermttive weighting policy , such a , s the widf ( weigh.te , d invcr.sc docwmcnt fre , qucncy ) ( Tokunaga , 1994 ) . The widf is reported to have a marked ~ulwmtage over the idf ( invers~ ~. document frequency ) for the text categoris~Ltion tank. \Ve have rei ) orted an exl ) erimentad study for clustering of ~rticles by using on-line ( lictiom~ry deftnitions mid showed how dictionary-definitiolt cam use effectively to classify articles , ea ( 'h of which belongs to the restricted sul ) ject domain. In order to Col ) e with the relnainiug i ) rol ) lems inentioned in section 5 and apply thin work to practical use , we will conduct further e×perilnents. G. Salton and M. a. M ( 'Gill , 1983. Introduction to Modern hfformation Retrieval. McGraw-Hill , 1983. T. Tokunaga and M. Iwayalna , 1994. Text Categoris~fl ; ioll based on Weighted Inverse Doenment FI'equency IPS.\ ] SIG l~.cl ) orts , 94-NL-1f ) 0 , 1994. I ) . Yarowsky , `` Word sense ( lismnbiguation using statistical models of I/oget 's categories trained on large corl ) ora '' , In Proc. of the 14th International Confc &gt; e'ncc on Computational Linguistics , Nantes , France , 1992 , l ) P. 454-46 { ) N. Yuasa ( 't al. , 1995 .</sentence>
				<definiendum id="0">X-axis</definiendum>
				<definiens id="0">the similarity wdue. A1 ) l ) reviation words in each Figure and categories</definiens>
				<definiens id="1">lictiom~ry deftnitions mid showed how dictionary-definitiolt cam use effectively to classify articles</definiens>
			</definition>
</paper>

		<paper id="2127">
			<definition id="0">
				<sentence>A main obstacle for the successfifl application of N I , P is the necessary effort in terms of deve|opinent and adaptation time .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the necessary effort in terms of deve|opinent and adaptation time</definiens>
			</definition>
			<definition id="1">
				<sentence>junction ( given t &gt; y Che alt keywor &lt; l ) ranging over the pllrasal and lexical cal ; egories of l ; he gr ; umnar. The feature , cat is used Co indica/ , c t , hese cat , eAeries. 'Fhe fealmre lex associat.es strings wil ; h lexi &lt; 'at ( ; m ; egories. '\ [ 'he trivial grammar of Fig. I exemplifies the layout of a FU F grammar. ( alt ( ; -- S ( with subject/verb agreement ) ( ( cat s ) ( subj ( ( cat rip ) ) ) ( pred ( ( cat vp ) ( agr ( ^ subj agr } ) ) ) ) -- NP ( only proper nounu ) ( ( : at np ) ( n ( ( cat noun ) ( proper y ) ) ) ) -- VP ( only intransitive verbs ) ( cat vp ) ( v ( ( cat verb ) ( agr { ^ `` agr } ) ) ) ) ; -- Lexicon ( ( cat verb ) ( lex `` laughs '' ) ( agr ( ( peru 3rd ) ( num ug ) ) ) ) ( ( ( : at noun ) ( lex `` Mary '' ) ( agr ( ( pets 3rd ) ( num sg ) ) ) ( proper y ) ) ) ) l , 'igure l : A trivial FUF grammar } 'oinCers are used Co enforce sCl'llcl ; tlre sharing and provide a ~neans { , &lt; &gt; percolaCe informaCion within a \ [ eaCm'e sl ; rlleCllre .</sentence>
				<definiendum id="0">junction</definiendum>
				<definiens id="0">at np ) ( n ( ( cat noun ) ( proper y ) ) ) ) -- VP ( only intransitive verbs ) ( cat vp ) ( v ( ( cat verb ) ( agr { ^ `` agr }</definiens>
			</definition>
			<definition id="2">
				<sentence>The basil ; iii ( ich &amp; \ ] : liSll\ [ o\ [ ' ell ( ( cat phrase ) ( head-dtr ( ( cat lex-cat ) ... ) ) ; ; percolate arguments ( args { ^ head-dtr args } ) ; ; : t'ecursion only on head daughter ( cset ( head-dtr ) ) ; ; realize head and arguments ( pattern ( args head-dtr ) ) ) Figure d : llead driven genera .</sentence>
				<definiendum id="0">percolate arguments</definiendum>
				<definiens id="0">args head-dtr ) ) ) Figure d : llead driven genera</definiens>
			</definition>
			<definition id="3">
				<sentence>The constituent to be topicalized or , if not specified in the input , the subj is extracte &lt; t via the slash mechanism ( cf. Fig. 7 ) . The interaction between top down category driven and `` bottom up '' lexicon driven processing is illustrated in Fig. 9 , showing also the effects of the two slash extraction mechanisms. Dcr Be : unto focus TOP DOWN hat den Brief erhalten syntax driven ... ... ... ... . v2 i ! iiii~ii : vk : : : : : : : : : : : : : : : : : : : : : : : : : : • : : : : : : : : : : : : : : : : : : : : : : : : : : : : : . % head-dtr : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : • ... ... ... , ... ... . , ... , .. , ... . ... ... ... ... ... ... ... ... . : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : e , .. , ... , .v.. , , , , , ,. , .. : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : +.. , ... , ... . , ... ... , ... : i i : i : i : i : i : i : i : i : i : : : i : i : i : i : i : i : : : i : i : : : : : i : i : i ( ... . ~_ __ ' i. ~ : :~ : :~ : :h : ~:5 : : : ~ : ~ : :~ : :5~ : ~ : ,~ ... ... . i'h~ad- , lash , iiiiiiiiiiiiiiiiii : i : i : i : iiiii : i : i : iiiiii : ! ili : i : iii : i : iiii ! i : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : ... ... ... . , ,.. , ... ... . , ... ... ... .. , ,. , ... , . , . , , , . , ... , , ====================================================================== .- , ~.~.~ , ~.~.~.~.~.. , ... , ... ... ... .. , .. , : , ... .. , ... ============================================================================== ~ : ? ' '' ' : ' : : : ' : '' '' ' '' '' : ' : '' : '' : : : '' : : : '' : : ' '' '~a : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : • ... ... ... ... ... ... ... .. | ... ... .. I `` ' '' '' '' '' ' '' ' '' '' ' '' ' '' '' '' ' '' ' '' ' '' '' '' : '' ' '' ' Figure 9 : Generating a l ) edarative Main Clause 756 For the integration of X2MorF into I : UF the unification engine used in X2MorF was replaced by I '' UF itself , and the existing word grammar and ~norph h ; xieon were reformulated in the FUF forlaalism , and tim word form g ( 'neration l , ask is now 1 ) erfontwd l ) y FUI '' itself. The= two-level rules could be taken over in l , heir original form , only l.he morl ) hologieal tillers had to l ) e l , ranstaLed. A simph '' functor/argumenL scheme is suflicient for the word grammar. The possible eombinal , ions are given I/y the phrase structure rules of l.he morl ) h grammar. '\ [ 'he affixes ( fun ( 't.ors ) may fllrthe.r restrict the arguments they may lie applied t , o. Fig. 10 shows an examph ; of morphoh/gical e ; Ltegorics reslIonsible \ [ br nominal inlh ; ction. A u ( mn stem has to be \ [ bllowe. &lt; l by a case suffix which determines ease and number ( ) \ [ ' the resulting noun form. 'l'he , head features of the argumen\ [ , are made availal ) le t , o the functor via the arghead feature , thus &lt; 'aiM ) ling the funcl , or \ [ , o subcateg &lt; ) rize for its argumenl ; ( e.g. , by re ( luiriug a certain inflection \ [ &gt; ara .</sentence>
				<definiendum id="0">.. , ... , .v..</definiendum>
				<definiendum id="1">'' '' '' '' ' '' ' '' '' ' '' ' '' '' '' ' '' ' '' ' ''</definiendum>
				<definiens id="0">... ... .. , ,. , ... , . , . , , , . , ... , , ====================================================================== .- , ~.~.~ , ~.~.~.~.~.. , ... , ... ... ... .. , .. , : , ... ..</definiens>
			</definition>
</paper>

		<paper id="2112">
			<definition id="0">
				<sentence>Prosodic Morphology ( McCarthy and Prince , 1986 , et seq. ) provides adequate means for describing non-linear phenomena such as infixation , reduplication and templatic morphology .</sentence>
				<definiendum id="0">Prosodic Morphology</definiendum>
				<definiens id="0">provides adequate means for describing non-linear phenomena such as infixation , reduplication and templatic morphology</definiens>
			</definition>
			<definition id="1">
				<sentence>In the Prosodic Morphology Hypothesis , morn is the unit of syllabic weight ; a monomoraic syllable , oh , , is light ( L ) , and a bimoraic syllable , a~ , ~ , is heavy ( H ) .</sentence>
				<definiendum id="0">Prosodic Morphology Hypothesis</definiendum>
				<definiendum id="1">morn</definiendum>
				<definiens id="0">the unit of syllabic weight ; a monomoraic syllable , oh , , is light</definiens>
			</definition>
			<definition id="2">
				<sentence>The function ~I~ returns the constituent C that sits on the edge E E { right , left ) of the base B. The result is a factoring of B into : kernel , designated by B : ~ , which is the string returned by the parsing function , and residue , designated by B , which is the remainder of B. The relation between B : ~ and B/~ is given in ( 4 ) , where ~ is the concatenation operator .</sentence>
				<definiendum id="0">function ~I~</definiendum>
				<definiens id="0">returns the constituent C that sits on the edge E E { right</definiens>
				<definiens id="1">the concatenation operator</definiens>
			</definition>
			<definition id="3">
				<sentence>Multi-tape two-level morphology is an extension to standard two-level morphology , where more than one lexical tape is allowed .</sentence>
				<definiendum id="0">Multi-tape two-level morphology</definiendum>
				<definiens id="0">an extension to standard two-level morphology , where more than one lexical tape is allowed</definiens>
			</definition>
			<definition id="4">
				<sentence>LLC LEX RLC { : :~ , ~ } ( 6 ) LSC SURI , ' RSC where LLC is the left , lexical context , LEX is the lexical form , RLC is the right lexical context , LSC is the left surface context , SURF is the surface form , and RSC is the right surface context .</sentence>
				<definiendum id="0">LLC</definiendum>
				<definiendum id="1">LEX</definiendum>
				<definiendum id="2">RLC</definiendum>
				<definiendum id="3">LSC</definiendum>
				<definiendum id="4">SURF</definiendum>
				<definiendum id="5">RSC</definiendum>
				<definiens id="0">the left , lexical context</definiens>
				<definiens id="1">the lexical form</definiens>
				<definiens id="2">the right lexical context</definiens>
				<definiens id="3">the left surface context</definiens>
				<definiens id="4">the surface form</definiens>
			</definition>
			<definition id="5">
				<sentence>( 7 ) ARABIC VERBAL MEASURES ( 1-8 , 10 ) 1 kutib 2 kuttib 3 kuutib 4 'euktib 5 tukuttib 6 tukuutib 7 nkutib 8 ktutib 10 stuktib ( McCarthy , 1993 ) points out that Arabic verbal forms are derived from the base template in ( 8 ) , which represents Measure 1 .</sentence>
				<definiendum id="0">ARABIC VERBAL MEASURES</definiendum>
				<definiens id="0">represents Measure 1</definiens>
			</definition>
			<definition id="6">
				<sentence>a~ represents an extrametrical consonant ; thai ; is , the last consonant in a stein .</sentence>
				<definiendum id="0">a~</definiendum>
				<definiendum id="1">thai ;</definiendum>
				<definiens id="0">an extrametrical consonant ;</definiens>
			</definition>
			<definition id="7">
				<sentence>The operation is O = 'prefix { t } ' ; the rule is O/'I , ( C , \ [ , eft ; ) , where C is a consonant .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a consonant</definiens>
			</definition>
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>Itowever , they claim that the semantic classification of verbs based on standard machine-readable dictionaries ( e.g. , the LDOCE ) is % hopeless pursuit \ [ since\ ] standard dictionaries are simply not equipped to offer this kind of information with consistency and exhaustiveness . ''</sentence>
				<definiendum id="0">LDOCE</definiendum>
				<definiens id="0">the semantic classification of verbs based on standard machine-readable dictionaries</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>Spanish is an inflectiolml language , which increases the possibilities of such exrors .</sentence>
				<definiendum id="0">Spanish</definiendum>
				<definiens id="0">an inflectiolml language , which increases the possibilities of such exrors</definiens>
			</definition>
</paper>

		<paper id="1081">
			<definition id="0">
				<sentence>The Sign Expansion Approach forms a basis for creating non-redundant lexicon systems that are structured along semantic lines .</sentence>
				<definiendum id="0">Sign Expansion Approach</definiendum>
				<definiens id="0">forms a basis for creating non-redundant lexicon systems that are structured along semantic lines</definiens>
			</definition>
			<definition id="1">
				<sentence>The Sign Model ( SM ) by Hcllan and DimitrovaVulchanova ( Hellan and Dimitrova-Vnlchanova , 1994 ) is a semantically based sign expansion theory and is used as the lexical basis of our lexicon .</sentence>
				<definiendum id="0">Sign Model</definiendum>
				<definiens id="0">a semantically based sign expansion theory</definiens>
			</definition>
			<definition id="2">
				<sentence>The realizational part is the string `` t ) aint '' , whereas the semantic part denotes a situation with two arguments , indexed as 1 and 2 .</sentence>
				<definiendum id="0">realizational part</definiendum>
				<definiens id="0">the string `` t ) aint '' , whereas the semantic part denotes a situation with two arguments , indexed as 1 and 2</definiens>
			</definition>
			<definition id="3">
				<sentence>The rule in Figure 4 ( c ) shows how a resultative construction like Jon painted the wall red is supported by a minimal sign like PAINT .</sentence>
				<definiendum id="0">c )</definiendum>
				<definiens id="0">shows how a resultative construction like Jon painted the wall red is supported by a minimal sign like PAINT</definiens>
			</definition>
			<definition id="4">
				<sentence>Syntactic mapping rules are rules that derive syntactic properties from conceptual structures .</sentence>
				<definiendum id="0">Syntactic mapping rules</definiendum>
				<definiens id="0">rules that derive syntactic properties from conceptual structures</definiens>
			</definition>
			<definition id="5">
				<sentence>Category and Infection stem from ingN , Realization is a combination of the values in PAINT and ingN , and Semantics is the minimal sign 's conceptual structure expanded into a complex argument indexed as 3 .</sentence>
				<definiendum id="0">Realization</definiendum>
				<definiendum id="1">Semantics</definiendum>
				<definiens id="0">a combination of the values in PAINT and ingN , and</definiens>
				<definiens id="1">the minimal sign 's conceptual structure expanded into a complex argument indexed as 3</definiens>
			</definition>
			<definition id="6">
				<sentence>The Sign Model ( SM ) gives a theoretical foundation for structuring lexical information along semantic lines• It prescribes a strong semantic basis and suggests various kinds of expansion rules for generating complete word entries .</sentence>
				<definiendum id="0">Sign Model ( SM )</definiendum>
				<definiens id="0">gives a theoretical foundation for structuring lexical information along semantic lines• It prescribes a strong semantic basis and suggests various kinds of expansion rules for generating complete word entries</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>The output of these processes consists in the tagging of the recognized elements : 'P ' for paragraphs , 'S ' for sentences , 'W ' for words ( in case of morphological analysis , the tag 'M ' is provided for morphemes ) and 'PT ' for punctuation signs as exelnplified in ( 1 ) .</sentence>
				<definiendum id="0">'W</definiendum>
				<definiens id="0">the tagging of the recognized elements : 'P ' for paragraphs , 'S ' for sentences</definiens>
			</definition>
			<definition id="1">
				<sentence>ALEP provides a facility ( tsls-rules ) which allows the grammar writer to identify information which is to flow from the TtI to the linguistic processes .</sentence>
				<definiendum id="0">ALEP</definiendum>
				<definiens id="0">provides a facility ( tsls-rules ) which allows the grammar writer to identify information which is to flow from the TtI to the linguistic processes</definiens>
			</definition>
			<definition id="2">
				<sentence>The TH component represents a preprocessing component which covers a substantial part of what occurs in real texts .</sentence>
				<definiendum id="0">TH component</definiendum>
				<definiens id="0">represents a preprocessing component which covers a substantial part of what occurs in real texts</definiens>
			</definition>
			<definition id="3">
				<sentence>( Efficiency is the major reason , as underspecified syntax rules are very inefficient ) .</sentence>
				<definiendum id="0">Efficiency</definiendum>
				<definiens id="0">the major reason</definiens>
			</definition>
			<definition id="4">
				<sentence>Mother : synsem \ [ I c t he'°L Jl\ ] \ ] syn , subcat \ [ compls \ [ 5\ ] Lsubj\ [ ~\ ] Lsem \ [ 3\ ] Head-dtr : ksem \ [ 3\ ] Comp-dtr : \ [ synsem \ [ J\ ] \ ] subcat \ [ compls &lt; \ [ 41115\ ] &gt; \ [ subj \ [ 2\ ] Head information is propagated from head-dtr to mother , so is semantic information .</sentence>
				<definiendum id="0">syn</definiendum>
				<definiens id="0">semantic information</definiens>
			</definition>
			<definition id="5">
				<sentence>These figures are not meant to be an exact measurement as exact measurenrents are not available , in order to give an indication it may be said that ALL the phenomena which increase indeterminism in a grammar of German are covered : All forms of the articles ( 'die ' , 'der ' ) and homomorphous relative pronouns , all readings of verbs ( all frames , all syntactic realizations of complements ) , semantic readings , prepositions and honu ) lnorphous prefixes , PPs as nominal adjuncts , as preadjectival complements , as adjuncts to adverbs , as VP adjuncts , valent nouns ( with optional complementation ) , all readings of Gerlnan 'sein ' , coordination , N -~ N combinations , relatives , Nachfeld .</sentence>
				<definiendum id="0">PPs</definiendum>
				<definiens id="0">realizations of complements ) , semantic readings , prepositions and honu ) lnorphous prefixes</definiens>
			</definition>
			<definition id="6">
				<sentence>ALEP is a promising platform for development of large-scale application-oriented grammars .</sentence>
				<definiendum id="0">ALEP</definiendum>
				<definiens id="0">a promising platform for development of large-scale application-oriented grammars</definiens>
			</definition>
</paper>

		<paper id="2159">
			<definition id="0">
				<sentence>The Decision Tree Learning Algorithms ( DTLAs ) are getting keen attention from the natural language processing research comlnunity , and there have been a series of attempts to apply them to verbal case frame acquisition .</sentence>
				<definiendum id="0">Decision Tree Learning Algorithms ( DTLAs</definiendum>
				<definiens id="0">a series of attempts to apply them to verbal case frame acquisition</definiens>
			</definition>
			<definition id="1">
				<sentence>, ~ ( p ) + r ' : ( p ) , ( 1 ) where a is a coefficient , Gw~ ( p ) is the penalty for generality ~ , and E ( p ) is a I ) enalty for the induced errors by using p. The node that ha , s small S ( p ) is pro , ferabh ; .</sentence>
				<definiendum id="0">p )</definiendum>
				<definiens id="0">the penalty for generality ~</definiens>
			</definition>
			<definition id="2">
				<sentence>And Gv~ , n ( p ) and E ( p ) are generally mutually conflicting : high generality node p ( with low Gv , ,n ( p ) ) will induce many errors resulting in high E ( p ) and vice versa .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">E</definiendum>
				<definiens id="0">( p ) are generally mutually conflicting : high generality node p</definiens>
			</definition>
			<definition id="3">
				<sentence>Proposition 2 : For any P C N , P is a UCC node set if and only if P is a traversal node set .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a UCC node set if and only if P is a traversal node set</definiens>
			</definition>
			<definition id="4">
				<sentence>The T* links all such possible nodes with arcs , and the traversal node sets can exhaust T ' .</sentence>
				<definiendum id="0">T*</definiendum>
				<definiens id="0">links all such possible nodes with arcs , and the traversal node sets can exhaust T '</definiens>
			</definition>
			<definition id="5">
				<sentence>If an attribute has m different values whicil divide A into m subsets as A = { BI , ... , J~m } , the DTLA evahmtes the `` purity after division '' by the `` weighed sum of entropy , '' WSH ( attribute , A ) .</sentence>
				<definiendum id="0">DTLA</definiendum>
				<definiens id="0">evahmtes the `` purity after division '' by the `` weighed sum of entropy</definiens>
			</definition>
			<definition id="6">
				<sentence>The function make~lh'ee0 executes the recursive `` search and division '' and the Wsh ( ) calculates the weighted sum of entropy .</sentence>
				<definiendum id="0">Wsh ( )</definiendum>
				<definiens id="0">calculates the weighted sum of entropy</definiens>
			</definition>
</paper>

		<paper id="2102">
			<definition id="0">
				<sentence>Therefore their grammar keeps the punctuation and part-of-speech rules separate , but still allows them to be applied in an interleaved manner , in effect finding the happy mediuin between the two extreme approaches .</sentence>
				<definiendum id="0">Therefore their grammar</definiendum>
				<definiens id="0">keeps the punctuation and part-of-speech rules separate , but still allows them to be applied in an interleaved manner</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus ( 29 ) represents l g represents a variable 2 +pco represents a comma 608 legal use of punctuation adjoining a I ) hrasal item since it occurs adjacent to the AD .</sentence>
				<definiendum id="0">l g</definiendum>
				<definiens id="0">a comma 608 legal use of punctuation adjoining a I ) hrasal item since it occurs adjacent to the AD</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>Ditferent languages express grammatical flmctions ( such as subject or object ) in a variety of ways , e.g. by position or by inflection .</sentence>
				<definiendum id="0">Ditferent languages</definiendum>
				<definiens id="0">express grammatical flmctions ( such as subject or object ) in a variety of ways</definiens>
			</definition>
			<definition id="1">
				<sentence>F-structures are a mixture of mostly syntactic information ( grammatical flmctions ) with some semantic , predicateargument information encoded via the values of PRED features : `` PRED ~CANDIDATI , ~ ~ NUM SG L OBJ LsPEc .</sentence>
				<definiendum id="0">F-structures</definiendum>
				<definiens id="0">a mixture of mostly syntactic information ( grammatical flmctions ) with some semantic , predicateargument information encoded via the values of PRED features : `` PRED ~CANDIDATI , ~ ~ NUM SG L OBJ LsPEc</definiens>
			</definition>
			<definition id="2">
				<sentence>A Unresolved QLF gives the basic predicateargument structure of a sentence , mixed with some syntactic information encoded in the categories of QLF terms and forms : 1 ?</sentence>
				<definiendum id="0">Unresolved QLF</definiendum>
				<definiens id="0">gives the basic predicateargument structure of a sentence , mixed with some syntactic information encoded in the categories of QLF terms and forms</definiens>
			</definition>
			<definition id="3">
				<sentence>The basic vocabulary consists of five disjoint set ; s : GFs = { SUIU , OBJ , OlU2 , ore,0 , ... } ( subcategorizable grmnnml ; ical flmctions ) , Gl '' ~ , : = { AmS , I , MODS , AMOI ) S , ... } ( noIlsubcategorizahle gralnmatical ftlnctioIlS ) , SI , ' : { candidate0 , marY0 , support ( j '' suns , j '' oB , , } , ... } ( semantic forms ) , A'/ &lt; : { SI'I4C , NUM , 1H , ; II , ... } ( a/ , ; tributes ) and AV= { ~ , ; vl , mY , MOST , el , , FEM , ... } ( atomic values ) .</sentence>
				<definiendum id="0">basic vocabulary</definiendum>
			</definition>
			<definition id="4">
				<sentence>Scope is a new QLF mete-variable , , P ~ new w~riable and ~i 6_ AT ~Prool ' : induction on the formation rules for wffs using the definitions of completeness , coherence atttl consistency ( KalJan &amp; lbesmm , 1982 ) , The not ; ions of a'u , bst'r'u , ct'wre occwrrin .</sentence>
				<definiendum id="0">Scope</definiendum>
				<definiens id="0">a new QLF mete-variable ,</definiens>
			</definition>
			<definition id="5">
				<sentence>l % mctions The treatment of modification in both f-structure and QLF is open to some flexibility and variation .</sentence>
				<definiendum id="0">QLF</definiendum>
				<definiens id="0">open to some flexibility and variation</definiens>
			</definition>
			<definition id="6">
				<sentence>For QLF terms with simple restrictions ( i.e. no modifiers ) , 7 -- 1 is defined as follows : • 7 -- l ( term ( I , &lt; gf=F , ( ~l = vi , ... , ( ~n : Vn &gt; , lI , _ , _ ) ) : = \ [ O~1 Vl r PaED II \ [ \ ] LO~n Vn • T -l ( _ : form ( I , &lt; gf=F , pred=l\ [ ( Fi , .. . , Fm ) , c~l = vi , ... , ( ~j =Vj &gt; , P^P ( Loi , ... tim ) , _ ) ) : = \ [ '~1 Vl ( ~ vj P /PriED II ( tF1 , ... , tFM ) \ [ \ ] r_~ ( ol ) L T1 As an example the reader may verify that r-~ retranslates the QLF associated with Most representatives persuaded a candidate to support every subsidiary back into the f-structure associated with the sentence as required .</sentence>
				<definiendum id="0">P^P ( Loi , ... tim</definiendum>
				<definiendum id="1">Vl ( ~ vj P /PriED II ( tF1 , ... , tFM</definiendum>
				<definiens id="0">~l = vi , ... , ( ~n : Vn &gt; , lI , _ , _ ) ) : = \ [ O~1 Vl r PaED II \ [ \ ] LO~n Vn • T -l ( _ : form ( I</definiens>
			</definition>
			<definition id="7">
				<sentence>R ) and Q is a quantifier such that 8 ( C , Q ) then V , a ( ¢ , v ) if v , ~ ( ¢\ [ O/ ?</sentence>
				<definiendum id="0">Q</definiendum>
				<definiens id="0">a quantifier such that 8 ( C , Q ) then V , a ( ¢ , v ) if v</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>ALT-J/E follows option ( b ) , such nouns are entered into the lexicon twice , once as a common noun and once as a jos~ishi .</sentence>
				<definiendum id="0">ALT-J/E</definiendum>
				<definiens id="0">follows option ( b ) , such nouns are entered into the lexicon twice</definiens>
			</definition>
			<definition id="1">
				<sentence>However , there are three possible translations of a Japanese noun phrase of the form ~XCno-N ' , where C is a unit classifier : Individuate : Translate as 'X N ' , where the classifier C is not translated and the numeral directly modifies the countable English noun phrase : 1-hiki-no-inu 'l-piece of dog ' -- + 1 dog .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a unit classifier</definiens>
			</definition>
			<definition id="2">
				<sentence>Part : Translate as 'X C of N ' , where the classifier is translated by its translation equivalent ( from the transfer dictionary ) and N is uncountable ( headed by a bare singular noun ) : 1-tsubu-no-kome 'l-grain of rice ' -+ 1 'grain of rice .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">uncountable ( headed by a bare singular noun</definiens>
			</definition>
			<definition id="3">
				<sentence>Noun phrases of the form 'N-no-C ' , where C is a GROUP classifier ( but not a jos~shi ) will also be translated as 'C of N ' where N will be plural if it is headed by a fully or strongly countable noun or a pluralia tanta .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a GROUP classifier</definiens>
			</definition>
</paper>

		<paper id="2101">
			<definition id="0">
				<sentence>The dialogue manager operates on the Context Model which is a dynamic knowledge base containing facts about the agents ' goals , expressive and evocative attitudes , central concepts ( topic ) , and new information .</sentence>
				<definiendum id="0">dialogue manager</definiendum>
				<definiens id="0">operates on the Context Model which is a dynamic knowledge base containing facts about the agents ' goals , expressive and evocative attitudes , central concepts ( topic )</definiens>
			</definition>
			<definition id="1">
				<sentence>The World Model uses neo-Davidsonian event representation , and the Application Model provides mappings from World Model concepts to task and role related facts .</sentence>
				<definiendum id="0">World Model</definiendum>
				<definiens id="0">uses neo-Davidsonian event representation , and the Application Model provides mappings from World Model concepts to task and role related facts</definiens>
			</definition>
			<definition id="2">
				<sentence>Specification of the joint purpose via the Application Model captures the cognitive consider= ation of Ideal Cooperation : the agent plans her response to be operationally appropriate in the current situation .</sentence>
				<definiendum id="0">Application Model</definiendum>
				<definiendum id="1">Cooperation</definiendum>
				<definiens id="0">captures the cognitive consider= ation of Ideal</definiens>
			</definition>
			<definition id="3">
				<sentence>601 NEW INPO : needE ( n , u , c ) , user ( u ) , car ( c ) USER C-GOAL : want ( u , want ( s , know ( s , \ [ want~e E ( h , u , e ) \ ] ) ) ) CENTRAL CONCEPT : needE ( n , u , c ) EXPRESSIVE ATTITUDES : intention : user intend that system know P : want ( u , know ( s , \ [ needE ( n , u , c ) , user ( u ) , car ( c ) \ ] ) ) assumptions : user know that system not know P : know ( u , not know ( s , \ [ needE ( n , u , d , user ( u ) , car ( e ) \ ] ) ) EVOCATIVE ATTITUDES : intention : user intend that system intend that system know P : want ( u , want # , know ( s , \ [ .</sentence>
				<definiendum id="0">intention</definiendum>
				<definiens id="0">n , u , c ) , user ( u ) , car ( c ) \ ] )</definiens>
			</definition>
			<definition id="4">
				<sentence>Current interests concern the extension of the communicative principles into different activities and agent roles .</sentence>
				<definiendum id="0">Current interests</definiendum>
				<definiens id="0">concern the extension of the communicative principles into different activities and agent roles</definiens>
			</definition>
</paper>

		<paper id="2166">
			<definition id="0">
				<sentence>1°The tf*idf nmtho &lt; t proved itself better than all the other methods of weight computation which we tested ( see ( ? ) ) ; in particular , those using a combination of w~rious other heuristics , as proposed , e.g. , in ( ? ) . It can be argued that so far we have only dealt with short texts about a single topic. It is not clear how well the system would be able to handh ; texts where multiple threads of contents occur ; possibly , one couhl employ the method of texttiling here ( see e.g. , ( ? ) ) , which helps determining coherent sections within a text and thus could `` guide '' the abstracting system ill that it would be able to track a sequence of multit ) le topics in a text , . While our system currently produces abstracts offline , it is feasible to extend it in a way where it uses the user 's query in an IR environment to determine tile relevant sentences of the retrieved documents , tIere , instead of producing a `` general abstract '' , the resulting on-line abstract would reflect more of the `` user 's perspective '' on the respective text. However , it would have to be investigated , how nmch weight-increase the words from the user 's query should get in order not to bias tile resulting output in too strong a way. Further issues concerning the human-inaehine interface are : • highlighting passages containing the query words • listing of top ranked keywords in tile retrieved text ( s ) • indicating the relative position of the extracted sentences in the text • allowing for scrolling in the main text , starting at an arbitrary position within the abstract Ill this paper , we have shown that it is possible to implement a system for generating text abstracts which purely operates with word frequency statistics , without using either domain specific knowledge or text , sort specific heuristics. It was demonstrated that the resulting abstracts have the same quality in terms of precision/recall as the abstracts created by human subjects ill an experiment. While a simple lead-method is more likely to produce higher readability judgments , the advantage of the tf*idf-method for abstracting is its , superiority in terms of capturing content relevance. Acknowledgments Tile major part of this work has been drawn froln the author 's dissertation at the Centre for Cognitive Science , University of Edinburgh , UK. I wish to thank lily supervisors Steve Finch and Richard 988 Tal ) lc ' l : lh'ecision/r ( ! call wdues tbr default ( lead ) and tf*id\ [ ' methods. 3 ... .. 6.agfi ) . s o.ss/o.sl | 0.45/0.a8 10 0.37/0.62 | 0.41/0.74 12 0.a4/0. ( 9 | 0.ag/0.sa 14 0 '' 33/0 '' 79 l 0.37/0.91 Table 2 : Significance of sentence score correlation between human sul ) jeet , s : All 6 articles HS2 HS4 HS3 HS8 IIS9 HS1 HS5 HS12 ITS11 HS13 ITS10 HS14 height tlS4 IIS3 IIS8 ItS9 IIS1 HS5 ITS12 I1S11 \ [ IS13 HS10 HS14 IIS15 gg* # ~ # g # g # # g *gg # # # # # # gg* *g* *g # g*g g*g * # * # # *g # # g* ggg • g # # ** g # ggg *** # * @ • # # gg # # # $ **g # * # * # # g*g # # # # # # # gg **g # *~ ~** g*g g* # % *g # gg *g* # ** * # $ # # * g # * # * # # gg # g* ggg # ** g # ~ g* $ gg # g** # # # # % * g* # \ ] L , S. IL.S. *** *gg Shillcock for vahlal ) le discussions , suggestions and advice.. Also , I am grateful to Chris Manning for his comments on an ( ~arlier draft ; , as well as t , o the t ; WO allOllyi\ [ lOllS r ( wieweI'8 whose , reillarks gre.al ; ly helped in improving t , his l ) aper. The author has l ) eeal supi ) orted in part by grants from the Austrian Chamber of Commerce and ~Dade ( lhlndeswirtsehafl ; skammer ) and the Austrian Ministry of Science and Research ( BMWF ) . Abstract ; Generation Funct ; ion. In : Proceedings of the 17th ACM-SIGIR Um~fl : ~v.'nce. t ) t ) .152 161 Morris , A.H. , Kasper , G. , Adams , D. 1992. The Effects and Limitations of Automated T ( '~xt ; Condensing on Reading Comi ) rehension Performaime. In : Information Systems l } , esea'rch , 3 ( 1 ) . 1 ) t ) .17 35 Paice , C.I ) . 1990. Constructing l , il ; eralure Abstracts t ) y Comlmter : \ [ \ [ &gt; ( : hniques and Prost ) ecl ; s. In : Information Processing 84 Manageme .</sentence>
				<definiendum id="0">WO allOllyi\</definiendum>
				<definiendum id="1">eralure Abstracts</definiendum>
				<definiens id="0">feasible to extend it in a way where it uses the user 's query in an IR environment to determine tile relevant sentences of the retrieved documents</definiens>
			</definition>
</paper>

		<paper id="2109">
			<definition id="0">
				<sentence>The inference rules provide a clear picture of the way in which DATR works , and should lead to a better understanding of the mathematical and computational properties of the language .</sentence>
				<definiendum id="0">inference rules</definiendum>
				<definiens id="0">a clear picture of the way in which DATR works , and should lead to a better understanding of the mathematical and computational properties of the language</definiens>
			</definition>
			<definition id="1">
				<sentence>DATR was originally introduced by Evans and Gazdar ( 1989a ; 1989b ) as a simple , nonmonotonic language for representing lexical inheritance hierarchies .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">a simple , nonmonotonic language for representing lexical inheritance hierarchies</definiens>
			</definition>
			<definition id="2">
				<sentence>Currently , DATR is the most widely-used lexical knowledge representation language in the natural language processing community .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">the most widely-used lexical knowledge representation language in the natural language processing community</definiens>
			</definition>
			<definition id="3">
				<sentence>In more recent work , DATR has been used to provide a concise , inheritance-based encoding of Lexiealized Tree Adjoining Grammar ( Evans et al. , 1995 ) .</sentence>
				<definiendum id="0">DATR</definiendum>
			</definition>
			<definition id="4">
				<sentence>Thus Dog : &lt; cat ) also has the value noun. The value of Dog : ( plur ) is specified indirectly by a sequence of descriptors Dog : ( root ) Noun : ( suiT ) . Intuitively , the required value is obtained by concatenating the values of the descriptors Dog : ( root ) and Noun : ( surf } , yielding dog s. We wish to provide an inductive definition of an evaluation relation ( denoted ~ ) between sequences of DATR descriptors in DESC* and sequences of atoms ( i.e. values ) in ATOM*. We write to mean that the sequences of descriptors ¢ evaluates to the sequence of atoms a. With respect to the DATR/ theory above we should expect that Dog : ( cat ) ~ noun and that Dog : ( root ) Noun : ( surf ) ~ dog s , amongst other things. The formal definition of ==v for DATRL is provided by just four rules of inference , as shown in figure 1. The rule for Values states simply that a sequence of atoms evaluates to itself. Another way of thinking about this is that atom sequences are basic , and thus can not be evaluated further. The rule for Definitions was briefly discussed in the previous section. It permits inferences to be made about the values associated with node/path pairs , provided that the theory T contains the appropriate definitional sentences. The third rule deals with the evaluation of sequences of descriptors , by breaking them up into shorter sequences. Given that the values of the sequences ¢ and ¢ are known , then the value of ¢¢ can be obtained simply by concatenation. Note that this rule introduces some non-determinism , since in general there is more than one way to break up a sequence of value descriptors. However , whichever way the sequence is broken up , the result ( i.e. value obtained ) should be the same. The following proof serves to illustrate the use of the rules Val , Def and Seq. It establishes formally that the node/path pair Dog : ( plur ) does indeed evaluate to dog s given the DATRL theory above. dog==~dog Val s ~ s Val Dcf Dog : &lt; root ) : =~ dog Noun : &lt; suiT ) ~ s Def Seq Dog : &lt; root ) Noun : ( surf ) ==~ dog s Dcf Dog : ( plur ) ~ dog s The final rule of figure 1 deals with DATR 's evaluable path construct. Consider a value descriptor of the form A : ( B : 0 ) . To determine the value of the descriptor it is first necessary to establish what path is specified by the path descriptor ( B : 0 ) This involves evaluating the descriptor B : 0 and then 'plugging in ' the resultant value a to obtain the path ( a ) . The required value is then obtained by evaluating A : ( a ) . The rule for Evaluable Paths provides a general statement of this process : if a sequence of value descriptors ¢ evaluates to a and N : ( a ) evaluates to/3 , then N : ( ¢ ) also evaluates to ~. DATR 's local inheritance mechanism provides for a simple kind of data abstraction. Thus , in the DATRL theory above , information about the plural suffix is stated once and for all at the abstract Noun node. It is then available to any instance of Noun such as Dog via local inheritance. On the other hand , information about the formation of singular and plural forms of dog must still be located at the Dog node , even though the processes involved are entirely regular. To overcome this problem , DATR provides a second form of inheritance : global inheritance. This section provides an evaluation semantics for a default-free variant of DATR with both local and global inheritance ( DATRG ) . A simple DATRG theory is shown below. Noun : &lt; cat ) : : noun &lt; sum == &lt; sing ) = '' &lt; root ) '' &lt; pint ) == `` ( root ) '' &lt; surf ) Dog : ( cat ) == Noun &lt; root ) == dog ( sing ) == Noun ( plur ) == Noun The new theory is equivalent to that given previously in the sense that it associates exactly the same values with node/path pairs. However , in the DATRa theory global inheritance is used to capture the relevant generalizations about the singular and plural forms of nouns in English. Thus , the sent~ence Noun : &lt; sing ) == `` &lt; root ) '' states that the singular form of any noun is identical to its root ( whatever that may be ) . The sentence Noun : ( plur ) == `` ( root ) '' ( surf ) states that the plural is obtained by attaching the ( plural ) suffix to the root. To understand the way in which global inheritance works , it is necessary to introduce DATR 's 648 notion of global contea't. Suppose that we wish to determine the value of Dog : ( sing ) in the exalnt ) le DATRc ; theory. Initially , the global context will be the pair ( Dog , sing ) , bS '' om tile theory , the value of Dog : ( sing } is to be inherited ( locally ) fl'om Noun : ( sing ) , which ill turn inherits its value ( globally ) from the quoted path `` ( root ) '' . rio evaluate the quoted path , the global context is examined to find the current global node ( this is Dog ) and the vahle of `` ( root ) '' is then obtained by evaluating Dog : ( root ) , which yields dog as required. More generally , the global context is used to fill in the missing node ( t/ath ) when a quoted path ( node ) is encountered. In addition , as a side effect of evahlating a global inheritance descriptor the global context is updated. Thus , after encountering the quoted path `` ( root } '' in the preceding example , tile global context is changed from ( Dog , sing ) to ( Dog , root ) . That is , the path component of the context is set to tile new global path root. Let T be a DATRa theory defined with respect to the set of nodes NODE and the set of atoms ATOM. The set ( : ON'X ' of ( .qlobal ) contexts of 7is defined as the set of all pairs of the form ( N , ( t ) , for N G NODE and ( .~ G ATOM*. Contexts are denoted t ) y C. The evaluation relation ~ is now taken to be a mapping from elements of CeNT X \ ] ) ESC* to ATOM*. We write cF4 ) ~ to mean that ¢ evaluates to fl in the global context C. To axiomatise the IleW evaluation relation , the , DATRc rules m'e modified to incorporate the global context parameter. For example , the rule for Evaluable Paths now becolnes : 0 t4 ) ~ , a C tN : ( , ~ ) =- &gt; fl Sub~ C P N : ( ¢ ) == &gt; fl Two sinfilar rules are required for sentences contMning quoted descriptors of the forms `` N : { ¢ ) '' and `` ( qS ) '' .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">&lt; cat ) also has the value noun. The value of Dog : ( plur ) is specified indirectly by a sequence of descriptors Dog : ( root ) Noun : ( suiT ) . Intuitively , the required value is obtained by concatenating the values of the descriptors Dog : ( root ) and Noun : ( surf } , yielding dog s. We wish to provide an inductive definition of an evaluation relation ( denoted ~ ) between sequences of DATR descriptors in DESC* and sequences of atoms ( i.e. values ) in ATOM*. We write to mean that the sequences of descriptors ¢ evaluates to the sequence of atoms a. With respect to the DATR/ theory above we should expect that Dog : ( cat ) ~ noun and that Dog : ( root ) Noun : ( surf ) ~ dog s , amongst other things. The formal definition of ==v for DATRL is provided by just four rules of inference</definiens>
				<definiens id="1">the previous section. It permits inferences to be made about the values associated with node/path pairs , provided that the theory T contains the appropriate definitional sentences. The third rule deals with the evaluation of sequences of descriptors , by breaking them up into shorter sequences. Given that the values of the sequences ¢ and ¢ are known</definiens>
				<definiens id="2">more than one way to break up a sequence of value descriptors. However , whichever way the sequence is broken up , the result ( i.e. value obtained ) should be the same. The following proof serves to illustrate the use of the rules Val , Def and Seq. It establishes formally that the node/path pair Dog : ( plur ) does indeed evaluate to dog s given the DATRL theory above. dog==~dog Val s ~ s Val Dcf Dog : &lt; root ) : =~ dog Noun : &lt; suiT ) ~ s Def Seq Dog : &lt; root ) Noun : ( surf ) ==~ dog s Dcf Dog : ( plur ) ~ dog s The final rule of figure 1 deals with DATR 's evaluable path construct. Consider a value descriptor of the form A : ( B : 0 ) . To determine the value of the descriptor it is first necessary to establish what path is specified by the path descriptor ( B : 0 ) This involves evaluating the descriptor B : 0 and then 'plugging in ' the resultant value a to obtain the path</definiens>
				<definiens id="3">if a sequence of value descriptors ¢ evaluates to a and N : ( a ) evaluates to/3 , then N : ( ¢ ) also evaluates to ~. DATR 's local inheritance mechanism provides for a simple kind of data abstraction. Thus , in the DATRL theory above , information about the plural suffix is stated once and for all at the abstract Noun node. It is then available to any instance of Noun such as Dog via local inheritance. On the other hand , information about the formation of singular and plural forms of dog must still be located at the Dog node</definiens>
				<definiens id="4">a default-free variant of DATR with both local and global inheritance ( DATRG ) . A simple DATRG theory is shown below. Noun : &lt; cat ) : : noun &lt; sum == &lt; sing ) = '' &lt; root ) '' &lt; pint ) == `` ( root ) '' &lt; surf ) Dog : ( cat ) == Noun &lt; root ) == dog ( sing ) == Noun ( plur ) == Noun The new theory is equivalent to that given previously in the sense that it associates exactly the same values with node/path pairs. However , in the DATRa theory global inheritance is used to capture the relevant generalizations about the singular and plural forms of nouns in English. Thus , the sent~ence Noun : &lt; sing ) == `` &lt; root ) '' states that the singular form of any noun is identical to its root ( whatever that may be ) . The sentence Noun : ( plur ) == `` ( root ) '' ( surf ) states that the plural is obtained by attaching the ( plural ) suffix to the root. To understand the way in which global inheritance works , it is necessary to introduce DATR 's 648 notion of global contea't. Suppose that we wish to determine the value of Dog : ( sing ) in the exalnt ) le DATRc</definiens>
				<definiens id="5">to be inherited ( locally ) fl'om Noun : ( sing ) , which ill turn inherits its value ( globally ) from the quoted path `` ( root ) '' . rio evaluate the quoted path , the global context is examined to find the current global node ( this is Dog ) and the vahle of `` ( root ) '' is then obtained by evaluating Dog : ( root ) , which yields dog as required. More generally , the global context is used to fill in the missing node ( t/ath ) when a quoted path ( node ) is encountered. In addition , as a side effect of evahlating a global inheritance descriptor the global context is updated. Thus , after encountering the quoted path `` ( root } '' in the preceding example , tile global context is changed from ( Dog , sing ) to ( Dog , root ) . That is , the path component of the context is set to tile new global path root. Let T be a DATRa theory defined with respect to the set of nodes NODE and the set of atoms ATOM. The set ( : ON'X ' of ( .qlobal ) contexts of 7is defined as the set of all pairs of the form ( N , ( t ) , for N G NODE and ( .~ G ATOM*. Contexts are denoted t ) y C. The evaluation relation ~ is now taken to be a mapping from elements of CeNT X \ ] ) ESC* to ATOM*. We write cF4 ) ~ to mean that ¢ evaluates to fl in the global context C. To axiomatise the IleW evaluation relation , the , DATRc rules m'e modified to incorporate the global context parameter. For example , the rule for Evaluable Paths now becolnes : 0 t4 ) ~ , a C tN : ( , ~ ) =- &gt; fl Sub~ C P N : ( ¢ ) == &gt; fl Two sinfilar rules are required for sentences contMning quoted descriptors of the forms `` N : { ¢ ) '' and `` ( qS ) ''</definiens>
			</definition>
			<definition id="5">
				<sentence>Val ( Dog , root ) tdog =-=~ dog Def ( Dog , root ) \ [ Dog : ( root ) ==~ , log Q'ao2 ( Dog , sing ) t '' ( root ) '' ==~ dog Def ( Dog , sing ) FNot , , , : ( sing ) : =~ dog Def ( Dog , sing ) FDog : ( sing ) ~ dog In DATR , wflues may be associated with particular node/path pairs either explicitly , in terms of local or global inheritance , or implicitly 'by default ' .</sentence>
				<definiendum id="0">Val</definiendum>
				<definiens id="0">( sing ) : =~ dog Def ( Dog , sing ) FDog : ( sing ) ~ dog In DATR , wflues may be associated with particular node/path pairs either explicitly , in terms of local or global inheritance , or implicitly 'by default '</definiens>
			</definition>
			<definition id="6">
				<sentence>y /3 Q~tO 3 ( N ' , a ) ~ '' N '' = : : : ~ , fl Path Extensions : C F N : ( c~7 ) ~ fl Ezt C V N : &lt; o~ ) == : ~.~ fl Figure 3 : The Evaluation Semantics for DATR ( root } does not follow 'by default ' from the definition of Dog , even though it can be obtained by extending left and right paths in the required manner. The reason is that the theory already contains an explicit statement about the value of Dog : { root } . The evaluation relation is now defined as a mapping from elements of CONT × DESC* X ATOM* ( i.e. context/descriptor sequence/path extension triples ) to ATOM*. We write : to nman that ¢ evaluates to a in context C given path extension 7. When 7 = e is the emi ) ty path extension , we will continue to write C ~¢ ~ c , . A complete set of iifference rules for DATR is shown in figure 3. The rules for Values , Sequences and Evaluable Paths require only slight modification as the path extension is simply passed through from premises to consequent. The rules for Quoted Descriptors are also much as hefore. Here however , the path extension 7 appears as part of the global context in the premise of each rule. This means/ ; hat when a global descriptor is encountered , any path extension present is treated 'globally ' rather than 'locally'. The main change in the Definitions rule lies in the conditions under which it is applicable. The amended rule just captures the 'most specific sentence wins ' default mechanism. Finally , the new rule for Path Eztensions serves as a way of making any path extension explicit. For example , if Dog : ( eat } evaluates to noun , then Dog : ( } also evaluates to noun given the ( explicit ) path extension cat. An example proof showing thai ; Dog : &lt; plur &gt; evaluates to dog s given the DATR theory presented above is shown in figure 4 .</sentence>
				<definiendum id="0">evaluation relation</definiendum>
				<definiens id="0">~ , fl Path Extensions : C F N : ( c~7 ) ~ fl Ezt C V N : &lt; o~ ) == : ~.~ fl Figure 3 : The Evaluation Semantics for DATR ( root } does not follow 'by default ' from the definition of Dog , even though it can be obtained by extending left and right paths in the required manner. The reason is that the theory already contains an explicit statement about the value of Dog : { root }</definiens>
				<definiens id="1">a mapping from elements of CONT × DESC* X ATOM* ( i.e. context/descriptor sequence/path extension triples ) to ATOM*. We write : to nman that ¢ evaluates to a in context C given path extension 7. When 7 = e is the emi ) ty path extension</definiens>
				<definiens id="2">main change in the Definitions rule lies in the conditions under which it is applicable. The amended rule just captures the 'most specific sentence wins ' default mechanism. Finally , the new rule for Path Eztensions serves as a way of making any path extension explicit. For example , if Dog : ( eat } evaluates to noun , then Dog : ( } also evaluates to noun given the ( explicit ) path extension cat. An example proof showing thai ; Dog : &lt; plur &gt; evaluates to dog s given the DATR theory presented above is shown in figure 4</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>We use a 4-tuple &lt; V , N1 , P , N2 } to denote the relationship of a possible PP attachment , where V denotes semantic tag of verbs , N\ ] denotes the semantic tag of accusative noun , P denotes the preposition and N2 denotes the semantic tag of obliqne noun. For example , the following sentence has the 4-tuple { non_speech_act , human , with , in.strurncnt } . Kevin watched the girl with a telescope. Having the 4-tuple in advance , we could apply 67 rule-templates listed in Appendix to determine what the PP type is by aforementioned steps. That is , apply SPP rule-template tirst , and then VPP rule-template. If none succeeds , the PP should be an NPP. We summarize tile algorithm as follows. Algorithln 1 : Resolution to PP-Attachlnent ( 1 ) Check if it is a PPP according to the predicate-argument structure. ( 2 ) Check if it is an SPP according to 21 ruletemplates tbr SPP. ( 3 ) Check if it is a VPI ' according to 46 ruletemplates tbr VPP. ( 4 ) Otherwise , it is an NPI'. The Penn Treebank ( Marcus et al. , 1993 ) is used as the testing corpus. The following is a real example extracted from this treebank. ( ( S ( ADVP ( NP Next week ) ) ( s ( NP ( NP some inmates ) ( VP released ( ADVP early ) ( PP from ( NP the Hampton County jail ( PP in ( NP Springfield ) ) ) ) ) will be ( VP wearing ( RP ( NP a wristband ) ( SBARQ ( WHNP that ) ( S ( NP T ) ( VP hooks up ( pp with ( NP a special jack ( PP on ( NP their home phones ) ) ) ) ) ) ) ) ) ) ) . ) The PPs contained in Penn 'IYeebank are collected and associated with one label of PPP , S1 ) P , VPP , or NPP. For example , the Pl ) s contained in the atbrementioned sentence are extracted as tblh &gt; ws .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">N2</definiendum>
				<definiendum id="2">RP</definiendum>
				<definiens id="0">a 4-tuple &lt; V , N1 , P , N2 } to denote the relationship of a possible PP attachment , where V denotes semantic tag of verbs , N\ ] denotes the semantic tag of accusative noun</definiens>
				<definiens id="1">the preposition</definiens>
				<definiens id="2">the semantic tag of obliqne noun. For example , the following sentence has the 4-tuple { non_speech_act , human , with , in.strurncnt } . Kevin watched the girl with a telescope. Having the 4-tuple in advance , we could apply 67 rule-templates listed in Appendix to determine what the PP type is by aforementioned steps. That is , apply SPP rule-template tirst , and then VPP rule-template. If none succeeds , the PP should be an NPP. We summarize tile algorithm</definiens>
				<definiens id="3">a special jack ( PP on ( NP their home phones ) ) ) ) ) ) ) ) ) ) ) . ) The PPs contained in Penn 'IYeebank are collected and associated with one label of PPP , S1 ) P , VPP , or NPP. For example , the Pl ) s contained in the atbrementioned sentence are extracted as tblh &gt; ws</definiens>
			</definition>
			<definition id="1">
				<sentence>: l.bookslm'c ) -t adv sp &lt; tcc dircct'ion ( e. : l.Souttt ) dimension. ( e. : l.width. ) timc ( c..q. April ) nrder ( e.g.'rc ! l~tht.rily ) ab.st r ctcl ( c .~/ . j ' , ct ) c'l~ct~t ( c.q.ca'rl/~.quahc ) m , o'. , tio'iQ 4/. : ra '' , .s f er ) .adu 't*umbeT ' ( c.g.dozen ) product ( c 41.'wr'it i'n ! / ) 'rel i : /io'n , ( c .g .he.ave'n ) sc. , s. , t , ion , ( e. 41 .P '' i '' ) vol il ion ( c , g.'. : ill ) l , 'igurc 2 : Sommltics 'FaDs for Nouns. 229 not trtisdet , ermine ( l and this corresponds to the behavior l , o model refitmnletg , . Ilowever , tmmy VI ) Ps a.re no/ correctly resolved due l , o t.he rigidity of rulo-t.enll ) les. ThereFore , relaxing these rules will resuli , hi nlore correct , \/lq &gt; ` I ) ul less &lt; : orre &lt; : t NPP .</sentence>
				<definiendum id="0">Ilowever , tmmy VI</definiendum>
			</definition>
</paper>

		<paper id="2103">
			<definition id="0">
				<sentence>An LTAG is a set of trees ( elementary lrees ) which have at least one terminal symbol on its frontier called the anchor .</sentence>
				<definiendum id="0">LTAG</definiendum>
				<definiens id="0">a set of trees ( elementary lrees ) which have at least one terminal symbol on its frontier called the anchor</definiens>
			</definition>
			<definition id="1">
				<sentence>FindRoot returns the lowest node that dominates all nodes in the substitution set of the elementary tree 7 , e.g. FindRoot ( a ( cooked ) { 2.2 } ) will return the root node , i.e. corresponding to the S 4In this paper , we do not consider coordination of unlike categories , e.g. Pat is a Republican and proud of it .</sentence>
				<definiendum id="0">FindRoot</definiendum>
				<definiendum id="1">e.g. Pat</definiendum>
				<definiens id="0">returns the lowest node that dominates all nodes in the substitution set of the elementary tree 7</definiens>
				<definiens id="1">return the root node</definiens>
				<definiens id="2">a Republican and proud of it</definiens>
			</definition>
</paper>

		<paper id="2149">
			<definition id="0">
				<sentence>Given a particular hnmediate Dominance grammar and hierarchies of feature values potentially rel evant for linearization ( =the systelu 's bias ) , the leanler generates appropriate naturM language expressions to be ewduated as positive or negative by a teacher , and produces as output IAnear Precedence rules which can be directly used 1 ) y the gralnmar .</sentence>
				<definiendum id="0">linearization</definiendum>
				<definiens id="0">=the systelu 's bias ) , the leanler generates appropriate naturM language expressions to be ewduated as positive or negative by a teacher , and produces as output IAnear Precedence rules which can be directly used 1 ) y the gralnmar</definiens>
			</definition>
			<definition id="1">
				<sentence>Also , if some right-hand side is a set which ( properly ) includes another right-hand side ( as in rule ( 2 ) and rule ( 1 ) abowe ) , the latter is not added to the sibling list , since we do not want to learn twice the linearization of some two nodes ( `` name '' and `` vp '' in our case ) .</sentence>
				<definiendum id="0">right-hand side</definiendum>
				<definiendum id="1">right-hand side</definiendum>
				<definiens id="0">a set which ( properly</definiens>
			</definition>
			<definition id="2">
				<sentence>i , &lt; , , , ,_ , # , det , _ , &lt; , ,l , _\ ] \ ] S : \ [ \ [ det , sg , &lt; , adj , # , adj , &lt; , n , _ , # , det , _ , &lt; , n , _\ ] \ ] -A c~ , ,-777 adj &lt; n ( sg ) det ( sg ) &lt; n ( sg ) det ( sg ) &lt; adj adj &lt; n ( sg ) det ( sg ) &gt; n ( sg ) ~et ( sg ) &lt; adj adj &gt; n ( sg ) , let ( sg ) &lt; , , ( sg ) -- ~let ( sg ) &gt; ~dj adj &lt; n ( sg ) det ( sg ) &lt; .</sentence>
				<definiendum id="0">n ( sg ) det</definiendum>
				<definiens id="0"># , det , _ , &lt; , ,l , _\ ] \ ] S : \ [ \ [ det , sg , &lt; , adj , # , adj , &lt; , n , _ , # , det , _ , &lt; , n</definiens>
			</definition>
</paper>

		<paper id="1062">
			<definition id="0">
				<sentence>We have designed a model 1 that accounts for structural rules ( in Finin 's terminology ) of interpretation of N N compounds 2 , i.e. domainindependent rnles that are deduced from the rnorpho-syntactic and semantic characteristics of the nominal constituents .</sentence>
				<definiendum id="0">structural rules</definiendum>
				<definiens id="0">in Finin 's terminology ) of interpretation of N N compounds 2 , i.e. domainindependent rnles that are deduced from the rnorpho-syntactic and semantic characteristics of the nominal constituents</definiens>
			</definition>
			<definition id="1">
				<sentence>The key idea tllnt underlies the qualia sl , 'uctu , v is that nouns are implicitly related to predicative information , and that a noun selects tbr the tyl ) e of predicate , that can govern it .</sentence>
				<definiendum id="0">predicate</definiendum>
				<definiens id="0">that nouns are implicitly related to predicative information , and that a noun selects tbr the tyl</definiens>
			</definition>
			<definition id="2">
				<sentence>The nouns that denote such information are mostly elements of the ATTRIBUTE class , which is defined in WordNet as `` an abstraction belonging to or characteristic of an entity '' .</sentence>
				<definiendum id="0">ATTRIBUTE class</definiendum>
				<definiens id="0">an abstraction belonging to or characteristic of an entity ''</definiens>
			</definition>
			<definition id="3">
				<sentence>These nouns are uni-relationM nouns that can appear as the head of `` N1 of N2 '' groups , where N2 is a syntactic argument of N1 ( e.g. height of the desk ) ( Isabelle , 1984 ) .</sentence>
				<definiendum id="0">N2</definiendum>
			</definition>
			<definition id="4">
				<sentence>On the contrary , the predicate CONTAIN is a characteristic feature of the class CONTAINER .</sentence>
				<definiendum id="0">CONTAIN</definiendum>
				<definiens id="0">a characteristic feature of the class CONTAINER</definiens>
			</definition>
</paper>

		<paper id="2117">
			<definition id="0">
				<sentence>( 32 ) VCOMPOSED-OF list of sentence-signsq \ ] BACKGR set of background l \ [ 2S-STA'rUS set of social s'tatus _\ ] The value of the feature COMPOSEI ) -OF is a list .</sentence>
				<definiendum id="0">COMPOSEI ) -OF</definiendum>
				<definiens id="0">a list</definiens>
			</definition>
			<definition id="1">
				<sentence>Second , since the Korean honorification system consists of subject honorification , object honorification and addressee honorification , these types of honorification should be considered simultaneously when we look at a sentence .</sentence>
				<definiendum id="0">honorification system</definiendum>
				<definiens id="0">consists of subject honorification , object honorification and addressee honorification</definiens>
			</definition>
</paper>

		<paper id="2190">
			<definition id="0">
				<sentence>For examt ) le , if v is a movement , p is to and n2 is a t ) lace or direction , the PP teuds to be attached to the verb .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">n2</definiendum>
				<definiens id="0">a t ) lace or direction , the PP teuds to be attached to the verb</definiens>
			</definition>
			<definition id="1">
				<sentence>RA ( v , nl , p , n2 ) , a score fi'om 0 to 1 , ix defined as a value of counts of VP attachments divided by the total of occurrences of ( v , nl,1 ) , n2 ) in the training data .</sentence>
				<definiendum id="0">RA</definiendum>
				<definiendum id="1">p</definiendum>
				<definiens id="0">a value of counts of VP attachments divided by the total of occurrences of ( v , nl,1 ) , n2 ) in the training data</definiens>
			</definition>
			<definition id="2">
				<sentence>4 RA ( v , nl , p , n2 ) = f ( , ,vl , , , , , I , v , ,ce ) f ( v , nl , p , n2 ) , ~ f ( vPlv , ,~ L , p , u2 ) / ( , ,vl ... .. I , v , -2 ) +y ( , wl , , , ,,1 , v , , , u ) ( 1 ) In ( 1 ) , the symbol f denotes frequency of a parti ( 'ular tuple in the training data .</sentence>
				<definiendum id="0">symbol f</definiendum>
				<definiens id="0">denotes frequency of a parti ( 'ular tuple in the training data</definiens>
			</definition>
			<definition id="3">
				<sentence>Tl , is information comes from a machine-readable dictionary EDR dictionary , s SEDII electronic dietionm'y consists of a set of machine-readable dictionaries which includes Japanese and English word dictionary , Japanese and English co-occurrence dictionary , concept dictionary , and Jal ) anese &lt; &gt; English l ) ilingual dletionary ( EI ) R 1993 ) .</sentence>
				<definiendum id="0">Tl</definiendum>
			</definition>
</paper>

		<paper id="2105">
			<definition id="0">
				<sentence>A replacement relation consists of pairs of strings that are related to one another in the manner sketched below : x u.~ y , u~ z upper string \ [ 1\ ] x 1~ y 1~ z lower string We use u i and u~ to represent instances of Ui ( with i C \ [ 1 , n\ ] ) and 1~ and 1~ to represent instances of Li .</sentence>
				<definiendum id="0">replacement relation</definiendum>
			</definition>
			<definition id="1">
				<sentence>( h ) option , \ [ h I 0 \ ] \ [ 2\ ] h* Kleene star h+ Kleene plus h/B ignore ( A possibly interspersed with strings from B ) `` h colnplement ( negation ) $ h contains ( at least one ) A h B concatenation h I B union h g~ t3 intersection h B relative complement ( minus ) h .</sentence>
				<definiendum id="0">h/B ignore</definiendum>
				<definiens id="0">at least one ) A h B concatenation h I B union h g~ t3 intersection h B relative complement</definiens>
			</definition>
			<definition id="2">
				<sentence>A regular relation is a mapping from one regular language to another one .</sentence>
				<definiendum id="0">regular relation</definiendum>
				<definiens id="0">a mapping from one regular language to another one</definiens>
			</definition>
			<definition id="3">
				<sentence>Conditional parallel replacement denotes a relation which maps a set of n expressions Ui ( i E \ [ 1 , n\ ] ) in the upper language into a set of corr~ ; sponding n expressions Li in the lower language if , and only if , they occur between a Left and a right context ( ll , ri ) .</sentence>
				<definiendum id="0">Conditional parallel replacement</definiendum>
				<definiens id="0">a relation which maps a set of n expressions Ui ( i E \ [ 1 , n\ ] ) in the upper language into a set of corr~ ; sponding n expressions Li in the lower language if , and only if , they occur between a Left and a right context</definiens>
			</definition>
			<definition id="4">
				<sentence>o append the appropriate suffix to the subjunctive stem , we use the following transducer which maps the symbol SUFF to a suffix and deletes all tags : \ [ 50\ ] define Suffix : \ [ { SUFF - &gt; e II _ TAG* SG \ [ PIIP3\ ] } , { SUFF - &gt; e s 11 _ TAG* SG P2 } , { SUFF - &gt; i o n s II _ TAG* PL P1 } , { SUFF - &gt; i e z I I _ TAG* PL P2 } , { SUFF - &gt; e n t It _ TAG* PL P3 } \ ] .</sentence>
				<definiendum id="0">_ TAG* PL P3</definiendum>
				<definiens id="0">maps the symbol SUFF to a suffix and deletes all tags : \ [ 50\ ] define Suffix : \ [ { SUFF - &gt; e II _ TAG* SG \ [ PIIP3\ ] } , { SUFF - &gt; e s 11 _ TAG* SG P2 } , { SUFF - &gt; i o n s II _ TAG* PL P1 }</definiens>
			</definition>
			<definition id="5">
				<sentence>\ ] &lt; - &gt; IndP PL P3 Verb I I LETTER _ TAG \ [ 52\ ] would have to be expressed in the two-level formalism by four rules : O : IndP &lt; = &gt; LETTER _ ( : PL ) ( : P3 ) ( : Verb ) TAG ; \ [ 53\ ] O : PL &lt; = &gt; LETTER ( : IndP ) _ ( : P3 ) ( : Verb ) TAG ; O : P3 &lt; = &gt; LETTER ( : IndP ) ( : PL ) _ ( : Verb ) TAG ; 0 : Verb &lt; = &gt; LETTER ( : IndP ) ( : PL ) ( : P3 ) TAG ; IIere , the difficulty comes not only from the large nmnber of rules we would have to write in the above example , but also from the fact that writing one of lihese rules requires to have in mind all the others , to avoid inconsistencies between them .</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">IndP ) ( : PL ) _ ( : Verb ) TAG ; 0 : Verb &lt; = &gt; LETTER ( : IndP ) ( : PL ) ( : P3 ) TAG</definiens>
			</definition>
</paper>

		<paper id="2172">
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>d ( i ) , two , 'dO ) ) ( 'e ) l &lt; i &lt; n l &lt; _i , j &lt; n l'v ( tword ( i ) \ ] tword ( i + 1 ) , tword ( i + 2 ) ) ~ l ' , ' ( tag ( i ) I tag ( i + 1 ) , tag ( i + 2 ) ) . P , ' ( word ( i ) I tag ( / ) ) ( a ) Pr ( words , tags , links ) c~ Pr ( words , tags , preferences ) =/'r ( words , tags ) . Pr ( preferences \ ] words , t~gs ) ( 4 ) \ ] -I l ' , . ( twom ( i ) I two d ( i + 1 ) , t o , 'd ( i + 2 ) ) . H I two , .d ( i ) ) 1 &lt; i &lt; n t &lt; i &lt; n / 1 + # right-kids ( i ) '~ Pv ( words , t+gs , links ) = II { 1-\ [ P , . ( two , .d ( kid+ ( i ) ) I t , gj +dd+_ , ( i ) ) , t+o , 'd ( i ) ) l &lt; i &lt; n \c=- ( \ ] -k # left+kids ( i ) ) , eT~0 kid~q_ 1 if c &lt; 0 Figure 2 : tligh-level views of model A ( formuhrs I 3 ) ; model l : l ( forinul ; t 4 ) ; and model C ( lbrmula , 5 ) . If i and j are tokens , then tword ( i ) represents the pair ( tag ( i ) , word ( i ) ) , and L , j C { 0 , 1 } i~ ~ ill '' i is the p~m : nt of j. exactly one parent. Rather than having the model select a subset of the ~2 possible links , as in model A , and then discard the result unless each word has exactly one parent , we might restrict the model to picking out one parent per word to begin with. Model B generates a sequence of tagged words , then specifies a parent or more precisely , a type of parent for each word j. Of course model A also ends up selecting a parent tbr each word , but its calculation plays careful politics with the set of other words that happen to appear : in the senterl ( ; C : word j considers both the benefit of selecting i as a parent , and the costs of spurning all the other possible parents/'.Model B takes an appro ; ~ch at the opposite extreme , and simply has each word blindly describe its ideal parent. For example , price in Figure 3 might insist ( with some probability ) that it `` depend on a verb to my right. '' To capture arity , words probabilistically specify their ideal children as well : fell is highly likely to want only one noun to its left. The form and coarseness of such specifications is a parameter of the model. When a word stochastically chooses one set of requirements on its parents and children , it is choosing what a link grammarian would call a disjuuct ( set of selectional preferences ) for the word. We may thus imagine generating a Markov sequence of tagged words as before , and then independently `` sense tagging '' each word with a disjunct , a Choosing all the disjuncts does not quite specify a parse , llowever , if the disjuncts are sufficiently specific , it specifies at most one parse. Some sentences generated in this way are illegal because their disjuncts can not be simultaneously satisfied ; as in model A , these sentences are said to be removed fi'om the population , and the probabilities renormalized. A likely parse is therefore one that allows a likely and consistent aln our implementation , the distribution over possible disjuncts is given by a pair of Markov processes , as in model C. set of sells ( ' , tags ; its probability in the population is given in ( 4 ) . The final model we prol ) ose is a generation model , as opposed l ; o the comprehension mo ( lels A and B ( and to other comprehension modc , ls such as ( l , afferty et al. , 1992 ; Magerman , 1995 ; Collins , 1996 ) ) . r\ ] 'he contrast recalls an ohl debate over spoken language , as to whether its properties are driven by hearers ' acoustic needs ( coml ) rehension ) or speakers ' articulatory needs ( generation ) . Models A and B suggest that spe~kers produce text in such a way that the grammatical relations can be easily decoded by a listener , given words ' preferences to associate with each other and tags ' preferences to follow each other. But model C says that speakers ' primary goal is to flesh out the syn tactic and conceptual structure \ [ 'or each word they utter , surrounding it with arguments , modifiers , and flmction words as appropriate. According to model C , speakers should not hesitate to add extra prepositionM phrases to a noun , even if this lengthens some links that are ordinarily short , or leads to tagging or attachment mzJ ) iguities. The generation process is straightforward. Each time a word i is added , it generates a Markov sequence of ( tag , word ) pairs to serve , as its left children , and an separate sequence of ( tag , word ) pairs as its right children. Each Markov process , whose probabilities depend on the word i and its tag , begins in a speciM STAI { T state ; the symbols it generates are added as i 's children , from closest to farthest , until it re~ches the STOP state , q'he process recurses for each child so generated. This is a sort of lexicalized context-free model. Suppose that the Markov process , when gem crating a child , remembers just the tag of the child 's most recently generated sister , if any. Then the probability of drawing a given parse fi'om the population is ( 5 ) , where kid ( i , c ) denotes the cthclosest right child of word i , and where kid ( i , O ) = START and kid ( i , 1 + # , 'ight-kids ( i ) ) = STOP. 342 ( a ) ( b ) dachshund ovcr there can really phty dachshund ow : r there can really play I , 'igure 4 : Spans \ ] ) ~u'ticipa , ting , in the ( : orru ( : l. i ) a , rsc of 7'h , at dachs/*und o'+wr there c ( +u vcalhl ph+g golf~. ( st ) has one pa , rcnt , lcss cndwor ( I ; its sul ) sl ) +tn ( b ) lists two. ( c &lt; 0 in ( h'xes l ( 'ft children , ) 'Fhis may bc thought o\ [ `` as a , non-linca.r l ; rigrrmt model , where each t ; agg ( 'd woM is genera , l , ed 1 ) ascd on the l ) a.r ( 'nl , 1 , ~gg ( : d wor ( l and ; t sistx'r tag. 'l'he links in the parse serve Lo pick o , tt ; t , he r ( 'Jev ; mt t , rit : ; t+a , n~s , and a.rc ' chosen 1 ; o g ( 't ; l , rigrams t , lml , ot ) l , imiz ( ~ t , hc glohM t , a , gging. 'l'tt ; tl ; the liuks also ha.t ) l ) en t ; o ; ulnot , ; : d , ( '. useful setnant ; ic rela , tions is , from this t &gt; crsl ) ective , quil .</sentence>
				<definiendum id="0">n l'v</definiendum>
				<definiendum id="1">kid</definiendum>
				<definiendum id="2">c )</definiendum>
				<definiendum id="3">kid</definiendum>
				<definiendum id="4">o g</definiendum>
				<definiens id="0">two , 'dO ) ) ( 'e ) l &lt; i &lt; n l &lt; _i , j &lt;</definiens>
				<definiens id="1">words , tags , links ) c~ Pr ( words , tags , preferences ) =/'r ( words , tags ) . Pr ( preferences \ ] words</definiens>
				<definiens id="2">n t &lt; i &lt; n / 1 + # right-kids ( i ) '~ Pv ( words , t+gs , links ) =</definiens>
				<definiens id="3">nt of j. exactly one parent. Rather than having the model select a subset of the ~2 possible links</definiens>
				<definiens id="4">the model to picking out one parent per word to begin with. Model B generates a sequence of tagged words</definiens>
				<definiens id="5">a parent , and the costs of spurning all the other possible parents/'.Model B takes an appro ; ~ch at the opposite extreme</definiens>
				<definiens id="6">some probability ) that it `` depend on a verb to my right. '' To capture arity , words probabilistically specify their ideal children as well : fell is highly likely to want only one noun to its left. The form and coarseness of such specifications is a parameter of the model. When a word stochastically chooses one set of requirements on its parents and children , it is choosing what a link grammarian would call a disjuuct ( set of selectional preferences ) for the word. We may thus imagine generating a Markov sequence of tagged words as before , and then independently `` sense tagging '' each word with a disjunct , a Choosing all the disjuncts does not quite specify a parse , llowever , if the disjuncts are sufficiently specific , it specifies at most one parse. Some sentences generated in this way are illegal because their disjuncts can not be simultaneously satisfied ; as in model A , these sentences are said to be removed fi'om the population</definiens>
				<definiens id="7">a generation model , as opposed l ; o the comprehension mo ( lels A and B ( and to other comprehension modc</definiens>
				<definiens id="8">ohl debate over spoken language , as to whether its properties are driven by hearers ' acoustic needs ( coml ) rehension ) or speakers ' articulatory needs ( generation ) . Models A and B suggest that spe~kers produce text in such a way that the grammatical relations can be easily decoded by a listener , given words ' preferences to associate with each other and tags ' preferences to follow each other. But model C says that speakers ' primary goal is to flesh out the syn tactic and conceptual structure \ [ 'or each word they utter , surrounding it with arguments , modifiers , and flmction words as appropriate. According to model C , speakers should not hesitate to add extra prepositionM phrases to a noun</definiens>
				<definiens id="9">tag , word ) pairs as its right children. Each Markov process , whose probabilities depend on the word i and its tag , begins in a speciM STAI { T state ; the symbols it generates are added as i 's children , from closest to farthest , until it re~ches the STOP state , q'he process recurses for each child so generated. This is a sort of lexicalized context-free model. Suppose that the Markov process , when gem crating a child , remembers just the tag of the child 's most recently generated sister , if any. Then the probability of drawing a given parse fi'om the population is ( 5 )</definiens>
				<definiens id="10">genera , l , ed 1 ) ascd on the l ) a.r ( 'nl , 1 , ~gg ( : d wor ( l and ; t sistx'r tag. 'l'he links in the parse serve Lo pick o , tt</definiens>
			</definition>
			<definition id="1">
				<sentence>e a ( -cidcn { , a , l. Note that the revised v ( ' , rsiol~ of ulo ( h : t A uses prol ) a , bilit , ics / '' @ ink to chihl I child , I ) arenl , , closer- ( 'hihh : en ) , where n. ) ( le\ ] ( ; uses l'v ( link 1 , o child \ ] parent , , eloscr-chil ( h'en ) .</sentence>
				<definiendum id="0">n. )</definiendum>
				<definiens id="0">a , l. Note that the revised v ( ' , rsiol~ of ulo ( h : t A uses prol ) a , bilit , ics / '' @ ink to chihl I child</definiens>
			</definition>
			<definition id="2">
				<sentence>hod of cont.exl , -fr ( 'e l ) ~rsing , which combines aJIMys ( : s of shorl , er substrings into analys &lt; : s of progressively longer ones .</sentence>
				<definiendum id="0">-fr</definiendum>
			</definition>
			<definition id="3">
				<sentence>H Pr ( i has peels that j satisfies I tword ( i ) , tword ( j ) ) ( 6 ) k &lt; _i &lt; g k &lt; i , j &lt; g with i , j linked H Pr ( Lij ItW°rd ( i ) ' tword ( j ) , tag ' ( next-closest-kid ( i ) ) ) . H Pr ( LiJ ItW°rd ( i ) ' tword ( j ) , ... ) ( 7 ) k &lt; _i , j &lt; g with i , j linked k &lt; i &lt; ( , ( j &lt; k or ~. &lt; j ) • c must not be given a covering link if either the leftmost word of a or the rightmost word of b has a parent. ( Violating this condition leads to either multiple parents or link cycles. ) Any sufficiently wide span whose left endword has a parent is a legal parse , rooted at the EOS mark ( Figure 1 ) . Note that a span 's signature must specify whether its endwords have parents. Is this one parser really compatible with all three probability models ? Yes , but for each model , we must provide a way to keep tr~tck of probabilities as we parse. Bear in mind that models A , B , and C do not themselves specify probabilities for all spans ; intrinsically they give only probabilities for sentences. Model C. Define each span 's score to be the product of all probabilities of links within the span. ( The link to i from its eth child is associated with the probability Pr ( ... ) in ( 5 ) . ) When spans a and b are combined and one more link is added , it is easy to compute the resulting span 's score : score ( a ) , score ( b ) ./°r ( covering link ) ) When a span constitutes a parse of the whole input sentence , its score as just computed proves to be the parse probability , conditional on the tree root EOS , under model C. The highest-probability parse can therefore be built by dynamic programming , where we build and retain the highestscoring span of each signature. Model B. Taking the Markov process to generate ( tag , word ) pairs from right to left , we let ( 6 ) define the score of a span from word k to word ( ? . The first product encodes the Markovian probability that the ( tag , word ) pairs k through g1 are as claimed by the span , conditional on the appearance of specific ( tag , word ) pairs at g , ~+1. ~ Again , scores can be easily updated when spans combine , and the probability of a complete parse P , divided by the total probability of all parses that succeed in satisfying lexical preferences , is just P 's score. Model A. Finally , model A is scored the same as model B , except for the second factor in ( 6 ) , SThe third factor depends on , e.g. , kid ( i , c1 ) , which we recover fl'om the span signature. Also , matters are complicated slightly by the probabilities associated with the generation of STOP. 6Different k-g spans have scores conditioned on different hypotheses about tag ( g ) and tag ( g + 1 ) ; their signatures are correspondingly different. Under model B , a k-.g span may not combine with an 6-~n span whose tags violate its assumptions about g and g + 1. 11 A I ~1 c I c ' Tx I~ , ~o1~ 1.o I Non-punt 88.9 89.8 89.6 89.'1 89.8 77.J Nouns 90.1 89.8 90.2 90.4 90.0 S ( ; .2 I , ex verbs 74.6 75.9 7. '' /.3 75.8 73.3 67.5 'Fable t : Results of preliminary experiments : Percentage of tokens correctly tagged by each model. which is replaced by the less obvious expression in ( 7 ) . As usual , scores can be constructed from the bottom up ( though tword ( j ) in the second factor of ( 7 ) is not available to the algorithm , j being outside the span , so we back off to word ( j ) ) . We have undertaken a careful study to compare these models ' success at generalizing from training data to test data. Full results on a moderate corpus of 25,000+ tagged , dependency-annotated Wall Street Journal sentences , discussed in ( Eisner , 1996 ) , were not complete hi ; press time. However , Tables 1 2 show pilot results for a small set of data drawn from that corpus. ( The full resnlts show substantially better performance , e.g. , 93 % correct tags and 87 % correct parents fbr model C , but appear qualitatively similar. ) The pilot experiment was conducted on a subset of 4772 of the sentences comprising 93 , a~0 words and punctuation marks. The corpus was derived by semi-automatic means from the Penn Treebank ; only sentences without conjunction were available ( mean length=20 , max=68 ) . A randomly selected set of 400 sentences was set aside for testing all models ; the rest were used to estimate the model parameters. In the pilot ( unlike the full experiment ) , the parser was instructed to `` back oil '' ' from all probabilities with denominators &lt; 10. For this reason , the models were insensitive to most lexical distinctions. In addition to models A , B , and C , described above , the pilot experiment evaluated two other models for comparison. Model C ' was a version of model C that ignored lexical dependencies between parents and children , considering only dependencies between a parent 's tag and a child 's tag. This model is similar to the model nsed by stochastic CFG. Model X did the same n-gram tagging as models A and B ( ~. = 2 for the preliminary experiment , rather than n = 3 ) , but did not assign any links. Tables 1 -2 show the percentage of raw tokens that were correctly tagged by each model , as well as the proportion that were correctly attached to 344 All tokons Ntlll-llllnc NOLIn8 17~1 verbs \ [ A t~ ( ' C r\ [ L~5.. , ~ r 8.1S~ , ,a.~ 47.3 ~l r~ sA rr.~ I '~ 1 ~~-L40 : , &lt; ~A_ ~ ~_ 'l'~d ) le 2 : \ ] { .csults of preliininary ( , Xl ) crimcnts : Per. contage of tokens corrc0Lly attached Lo their paronl ; s by each model. their parents. Per tagging , baseline per\ [ ol : lnance Wa , S I/leaSlli'ed by assigniug each word ill the test set its most frequent tag ( i\ [ ' any ) \ [ 'roiii the trainlug set. Thc iinusually low I ) aseliue t ) crJ'orillance I : esults \ [ 'l'Olll kL conil ) iuation of ; t sHiaJl l &gt; ilot Lr ; ~illing set and ; t Inil ( lly ( ~xten ( |e ( I t~g set .</sentence>
				<definiendum id="0">tword</definiendum>
				<definiendum id="1">Markovian probability</definiendum>
				<definiens id="0">a legal parse , rooted at the EOS mark ( Figure 1 ) . Note that a span 's signature must specify whether its endwords</definiens>
				<definiens id="1">the probability of a complete parse P , divided by the total probability of all parses that succeed in satisfying lexical preferences , is just P 's score. Model A. Finally</definiens>
				<definiens id="2">similar to the model nsed by stochastic CFG. Model X did the same n-gram tagging as models A and B ( ~. = 2 for the preliminary experiment , rather than n = 3 ) , but did not assign any links. Tables 1 -2 show the percentage of raw tokens that were correctly tagged by each model</definiens>
			</definition>
			<definition id="4">
				<sentence>This shows that a t~Lg can 1 ) e predicted ~d ) out as well \ [ 'rolri Lhe tags of its putative p ; Lrel , t ; rod sil ) \ ] in &lt; g as it ( 'an fiX ) ill the \ [ ~ags O\ [ `` string-a ( lja ( : cnt words , eVell when there is ( 'onsideral ) le e/ ; l : OF ill dcterinin-ing the parent and s\ [ bling. I~arc-bories dependency grammar which requires 1lO Ihik labels &gt; no ~ralfliiiai ' , and ItO fll~S tO lirlderstand iS a clean tcstbcd for studying the lexical a\ [ liniLies of words .</sentence>
				<definiendum id="0">Lrel</definiendum>
				<definiendum id="1">ItO fll~S tO lirlderstand</definiendum>
				<definiens id="0">a t~Lg can 1 ) e predicted ~d ) out as well \ [ 'rolri Lhe tags of its putative p ;</definiens>
				<definiens id="1">cnt words</definiens>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>The direct generator could be used without the other submodules to generate texts in an automatic but non-linguistic way ( manipulation of character strings ) .</sentence>
				<definiendum id="0">direct generator</definiendum>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>These examples consist of an inImt outtmt association ( in our case , e.g. , a representation of a llotln as input , and the corresponding dimilmtive sul\ [ ix as output ) .</sentence>
				<definiendum id="0">inImt outtmt association</definiendum>
				<definiens id="0">a representation of a llotln as input , and the corresponding dimilmtive sul\ [ ix as output )</definiens>
			</definition>
			<definition id="1">
				<sentence>Linguistic Generalizations When looking only at the rhyme of the last syllable ( the NC corpus ) , the decision tree generated by C4.5 looks as follows : Decision Tree : coda in { rk , nt , lt , rt , p , k , t , st , s , ts , rs , rp , f , x , ik , Nk , mp , xt , rst , ns , nst , rx , kt , ft , if , mr , Ip , ks , is , kst , ix } : J coda in { n , = , l , j , r , m , N , rn , rm , w , lm } : nucleus in { I , A , } , O , E } : coda in { n , l , r , m } : E coda in { = , j , rn } : T coda in { rm , lm } : P coda = N : 1 nucleus = I : K I nucleus in { A , O , E } : E nucleus in { K , a , e , u , M , @ , y , o , i , L , ) , I , &lt; } : \ [ coda in { n , = , l , j , r , rn , w } : T \ [ coda = m : P Notice that the phoneme representation used by CELEX ( called DISC ) is shown here instead of the more standard IPA font , and that the value grouping mechanism of C4.5 has created a mnnber of phonological categories by collapsing different phonemes into sets indicated by curly brackets .</sentence>
				<definiendum id="0">Linguistic Generalizations</definiendum>
				<definiens id="0">the NC corpus ) , the decision tree generated by C4.5 looks as follows : Decision Tree : coda in { rk , nt , lt , rt , p , k , t , st , s , ts , rs , rp , f , x , ik , Nk , mp , xt , rst , ns , nst , rx , kt , ft , if , mr , Ip , ks , is , kst , ix } : J coda in { n , = , l , j , r , m , N , rn , rm , w , lm } : nucleus in { I , A , } , O , E } : coda in { n , l , r , m } : E coda in { = , j , rn } : T coda in { rm</definiens>
			</definition>
			<definition id="2">
				<sentence>\ [ 'hese ekLIl be interpreted as sets of st ) ee ( : h sounds ( categories ) : e.g. the category ( or feature ) labial groups those speech sounds that involve the lips as an a ( : tive art\ [ culator .</sentence>
				<definiendum id="0">categories )</definiendum>
				<definiens id="0">interpreted as sets of st ) ee ( : h sounds</definiens>
			</definition>
			<definition id="3">
				<sentence>Speech sounds behmg to different categories , i.e. , are defined by ditferent \ [ 'e~tures .</sentence>
				<definiendum id="0">Speech</definiendum>
				<definiens id="0">sounds behmg to different categories</definiens>
			</definition>
</paper>

		<paper id="2192">
			<definition id="0">
				<sentence>• Maxinnun Likelihood ( ML ) training : Given an untagged training corpus , the probabilities can be estimated using the Bauin-Welch algorithm ( also known as tile Forward-Backward algorithin ) ( Baron 1972 ) .</sentence>
				<definiendum id="0">• Maxinnun Likelihood ( ML ) training</definiendum>
			</definition>
			<definition id="1">
				<sentence>More precisely , when a word can not be found in the lexicon , we replace the product in ( 2 ) ( cf. equation 1 above ) with the product in ( 3 ) , where TTR ( ti ) is the typetoken ratio of ti ( in the training corpus ) .</sentence>
				<definiendum id="0">TTR ( ti )</definiendum>
				<definiens id="0">the typetoken ratio of ti ( in the training corpus )</definiens>
			</definition>
			<definition id="2">
				<sentence>• Condition 2 : Pause symbols are added to the lexicon , where short pauses are categorized as minor delimiters ( mid ) ( commas , etc. ) , while long pauses are categorized as mad ( fllll stops , etc. ) , which means that the contextual probabilities of words occurring before and after pauses in spoken language will be modelled on the probabilities of words occurring before and after certain punctuation marks in written language .</sentence>
				<definiendum id="0">mid )</definiendum>
				<definiens id="0">means that the contextual probabilities of words occurring before and after pauses in spoken language will be modelled on the probabilities of words occurring before and after certain punctuation marks in written language</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>The parsing component processes the lattice and assigns each well-formed path through it one or several syntactic and ( compositional ) semantic representations .</sentence>
				<definiendum id="0">parsing component</definiendum>
				<definiens id="0">processes the lattice and assigns each well-formed path through it one or several syntactic and ( compositional ) semantic representations</definiens>
			</definition>
			<definition id="1">
				<sentence>A LUD-representation U is a triple &lt; Hu , Lu , Cu &gt; where Hu is a set of holes ( variables over labels ) , Lu is a set of labeled ( LUD ) conditions , and Cu is a set of constraints .</sentence>
				<definiendum id="0">LUD-representation U</definiendum>
				<definiendum id="1">Lu</definiendum>
				<definiendum id="2">Cu</definiendum>
				<definiens id="0">a triple &lt; Hu , Lu , Cu &gt; where Hu is a set of holes ( variables over labels</definiens>
				<definiens id="1">a set of labeled ( LUD ) conditions , and</definiens>
				<definiens id="2">a set of constraints</definiens>
			</definition>
			<definition id="2">
				<sentence>For each plugging there is a corresponding DRS. The syntax of LUDconditions is formally defined as follows : tity or event ) , then din ( x ) is a LUDcondition ; tion , xl , ... , xn are discourse markers , then pred ( R , xl , ... , x , ~ ) is a LUD-condition ; LUD-condition , then -~l is a LUDcondition ; LUD-conditions , then 11 -- + 12 , 11AI2 and 11 V 12 are LUD-conditions ; 132 There are three types of constraints in LUDrepresentations .</sentence>
				<definiendum id="0">~ )</definiendum>
				<definiendum id="1">LUD-condition</definiendum>
				<definiendum id="2">LUD-conditions</definiendum>
				<definiens id="0">a LUDcondition ; tion , xl , ... , xn are discourse markers , then pred ( R , xl , ... , x ,</definiens>
				<definiens id="1">a LUD-condition</definiens>
				<definiens id="2">a LUDcondition ;</definiens>
			</definition>
			<definition id="3">
				<sentence>The interpretation of a LUD-representation is the interpretation of top , the label or hole of a LUi ) -representation tbr which tt , ere exists no label thai ; subordinates it .</sentence>
				<definiendum id="0">interpretation of a LUD-representation</definiendum>
				<definiens id="0">the interpretation of top , the label or hole of a LUi ) -representation tbr which tt</definiens>
			</definition>
			<definition id="4">
				<sentence>~ The interpretation fnnction I is a function from a labeled condition to a DRS. This hmction is defined with respect to a plugging P. We represent a I ) RS ~ a box ~DI~ , where D is the set of discourse markers and C is the set of conditions .</sentence>
				<definiendum id="0">D</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">the set of discourse markers and</definiens>
				<definiens id="1">the set of conditions</definiens>
			</definition>
			<definition id="5">
				<sentence>The mappings between LUD-conditions and I ) RSs are then detiued in ( 2 ) - ( 9 ) where l is a label or hole and ~b is a labeled condition .</sentence>
				<definiendum id="0">l</definiendum>
				<definiendum id="1">~b</definiendum>
				<definiens id="0">a label or hole and</definiens>
			</definition>
			<definition id="6">
				<sentence>TUG is a formalism that combines ideas from Government and Binding theory , namely the use of traces , with unification in order to account for , for example , the free word order phenomena found in German .</sentence>
				<definiendum id="0">TUG</definiendum>
				<definiens id="0">a formalism that combines ideas from Government and Binding theory</definiens>
			</definition>
			<definition id="7">
				<sentence>The context of a LUD-representation is a three-place structure consisting of the LUDrepresentation 's main label and top hole ( as described in Section 3.3 ) and its main instance , which is a discourse marker or a lambda-bound variable .</sentence>
				<definiendum id="0">context of a LUD-representation</definiendum>
				<definiens id="0">a three-place structure consisting of the LUDrepresentation 's main label</definiens>
			</definition>
			<definition id="8">
				<sentence>Differently from Reyle 's UDRSs , however , LUD assigns labels to the minimal semantic element and may also be interpreted in other object languages than DRT .</sentence>
				<definiendum id="0">LUD assigns</definiendum>
				<definiens id="0">labels to the minimal semantic element</definiens>
			</definition>
			<definition id="9">
				<sentence>course to Logic : An Introduction to Modeltheoretic Semantics of Natural Language , Formal Logic and DRT .</sentence>
				<definiendum id="0">Logic</definiendum>
				<definiens id="0">An Introduction to Modeltheoretic Semantics of Natural Language , Formal Logic and DRT</definiens>
			</definition>
</paper>

		<paper id="2167">
			<definition id="0">
				<sentence>A major step in natural language-generation ( NLG ) consists in choosing content words for expressing the planned message .</sentence>
				<definiendum id="0">NLG )</definiendum>
				<definiens id="0">consists in choosing content words for expressing the planned message</definiens>
			</definition>
			<definition id="1">
				<sentence>While this kind of decomposition has proven useful for practical purposes , -- -dividing the process into separate components increased the control , -it has also encouraged researchers to build into their systems wrong assumptions : content is generally determined in one go ( one-shot process ) , and information flow is one-directional , going downwards from the conceptual level to the linguistic level .</sentence>
				<definiendum id="0">information flow</definiendum>
				<definiens id="0">build into their systems wrong assumptions : content is generally determined in one go ( one-shot process ) , and</definiens>
			</definition>
</paper>

		<paper id="2116">
			<definition id="0">
				<sentence>Else , remains empty in the frame of A ' .</sentence>
				<definiendum id="0">Else</definiendum>
				<definiens id="0">remains empty in the frame of A '</definiens>
			</definition>
			<definition id="1">
				<sentence>\ [ o subj ( n ) top ndet objCn ) iobj ( to ) objprep vsubconj , subjCn ) t sccomp I ndet t objCn ) t iobj ( to ) objprep John ( l ) noun ( prop ) send ( 2 , l,4,6 ) verb ( fin ) the ( 3 ) det ( det ) flower ( 4 ) noun ( cn ) to ( 5,6 ) prep ( to ) Mary ( 6 ) noun ( prop ) before ( 7,9 ) subconj he ( 8 ) noun ( pron ) send ( 9,8,11,6 ) verb ( fin ) the ( l O ) det ( det ) chocolate ( 11 ) noun ( cn ) to ( 5,6 ) prep ( to ) Mary ( 6 ) noun ( prop ) The algorithm is currently being reimplemented in Prolog to apply to the output of a modified ItPSG ( Pollard and Sag ( 1994 ) ) grammar designed to handle ellipsis .</sentence>
				<definiendum id="0">o subj</definiendum>
				<definiens id="0">1994 ) ) grammar designed to handle ellipsis</definiens>
			</definition>
			<definition id="2">
				<sentence>&lt; conj &amp; mtbcat\ [ \ [ QI , O2 I &amp; eomp dtrs\ [ l_Ql &amp; phon ! ljohn , gives , mary , flowersl &amp; syn\ [ loc\ [ heaul ! .I &amp; subeat\ [ \ [ \ ] &amp; dtrs\ [ head_dtr ! phon ! \ [ gives , mary , flowers\ ] &amp; synHoelhead ! _.l &amp; subeat\ [ \ [ _Sl &amp; dtrslhead_dtr\ [ phon\ [ \ [ gives\ ] &amp; synHoc ! head ! _J &amp; vfm 'm ! &lt; fin &amp; subcat ! l_S , _Cl , I | I &amp; comp_dtrs\ [ \ [ CI &amp; phon\ [ \ [ maryl &amp; t t 9 l &lt; syn.loc.head.case , ace &amp; subeattll , II &amp; phon ! \ [ flowers\ ] &amp; synHoe ! head\ [ ease\ [ &lt; acc &amp; subeat ! \ [ I l &amp; comp_dtrs ! \ [ S &amp; phon ! ljohnl &amp; syn ! loe ! lnead\ [ case\ [ &lt; nom &amp; subcatlH I , 02 &amp; phon ! \ [ choeolates , too\ ] &amp; syn ! loe ! head\ [ _Tl &amp; subeat ! \ [ l &amp; dt rsHtead dtr ! phon ! \ [ ehocolates\ ] &amp; syn\ [ Ioc ! head\ [ Tl &amp; soheat ! ll &amp; dt rs\ [ head dtr ! phon\ [ \ [ chocolatesl &amp; synHoc ! head ! _TI &amp; easel &lt; ace &amp; subcattll &amp; eomp_dtrs ! \ [ \ [ &amp; eomp_dtrs\ [ \ [ \ ] &amp; adj dtrs ! phon ! ltoo\ ] &amp; syn\ [ Ioc ! head ! atype ! &lt; too &amp; slnbcattll l phon ! \ [ john , sings , aml , beautifully , too I &amp; syaHoc ! subcat\ [ \ [ l &amp; dt rs\ [ head dt r ! l ) hon ! \ [ and\ [ &amp; syn Hoc\ [ head ! cooj ! &lt; conj &amp; subcat ! \ [ _E 1 , W2\ ] &amp; comp dtrs\ [ \ [ _El &amp; phon ! \ [ john , singsl &amp; syn ! loc ! head ! , l &amp; subcat ! \ [ l &amp; dtrslheaddtr ! phon\ [ \ [ sings\ ] &amp; syn ! locHlead ! _J &amp; sobcat ! \ [ S\ ] &amp; dtrs ! head dtr\ [ phon ! bingsl &amp; syn\ [ IocHlead\ [ J &amp; vfo rm\ [ &lt; fin &amp; subeat ! \ [ _Sl &amp; eomp_dt , 's\ [ ll &amp; comp_dtrs ! \ [ S &amp; phoo ! ljohn\ ] &amp; synHoc ! head ! case ! &lt; nom &amp; subcat ! ll I , W2 &amp; phml ! \ [ beautifully , too I &amp; sya ! loc ! hcad ! Jl &amp; subcat ! ll &amp; dtrs ! head_d trIphon\ [ lheautifullyl &amp; syn ! loc ! hcad ! _Jl &amp; sal ) cat ! \ [ l &amp; ! ! ! dtrs.head dtr.phon.\ [ I ) eautifully I &amp; syn.loc.head. Jt &amp; subcat ! \ [ \ ] &amp; dtrsHtead_dtr ! phon ! H &amp; synHoc ! hcad ! Jl &amp; subeat ! ll &amp; dtrs ! head dtr ! phon\ [ \ [ \ [ &amp; syn.loe.head , ll &amp; vform\ [ &lt; elided &amp; suhcat ! \ [ l &amp; eomp_dtrs ! ll &amp; comp_dtrs\ [ n &amp; adj dtrs ! phon ! \ [ beautifullyl &amp; I ! ! y &lt; syo.loc.head.atype , others &amp; subcat ! \ [ \ ] &amp; comp_dtrs ! H &amp; eomp dtrs\ [ \ [ l &amp; adj_dtrs\ [ phon ! ltoo\ ] &amp; synHocHlead\ [ atype ! &lt; too &amp; subeat\ [ \ [ I \ ] The bare NP chocolates is the head of the 689 elided clause in the second conjunct of 7. The generalized ellipsis reconstruction algorithm will identify gives as the head V of the antecedent clause in the first conjunct , and then will fill one of the positions in its SUBCAT list with the local features of chocolates. If it fills the direct object ( third complement ) position of this list with the bare NP , then it will fill the subject and indirect object positions with the local features of John and Mary , generating the reconstructed feature structure corresponding to 9. \ [ NP flowers\ ] \ ] \ ] and \ [ , , \ [ Ne John\ ] \ [ vP \ [ vp \ [ v gives\ ] \ [ NP Mary\ ] \ [ NP chocolates\ ] \ ] \ [ AdvP tOO\ ] \ ] \ ] By contrast , the bare adverb beautifully is an adjunct daughter of a VP headed by an empty verb in 8. This is due to the fact that in our grammar , an adverb is an adjunct which modifies a VP. The algorithm will identify sings as the head V of the antecedent clause and substitute it for the empty V in 8. This will yield a feature structure corresponding to 10. 10. \ [ m \ [ ~ John\ ] \ [ ve \ [ v plays\ ] \ ] \ ] and \ [ n , \ [ NP John\ ] \ [ vv \ [ vp \ [ vP \ [ v plays\ ] \ ] \ [ AdvP beautifully\ ] \ ] too\ ] \ ] We employ a rule which permits an unbounded number of adverbs to be generated in successively higher VP 's through left recursion on the daughter VP node. The relevant PS rule is of the form VP ~ VP , ADV. We require this rule in order to allow for the fact that there is no apparent upper bound on the number of adverbs in a VP. 11 indicates that it is possible to obtain an unbounded number of bare adverbial adjuncts in an ellipsis site. 11 a. John sang , but not in New York. b. John sang , but not in New York at the concert. c. John sang , but not in New York at the concert for three hours. d. John sang , but not in New York at the concert for three hours on Tuesday. e. John sang , but not in New York at the concert for three hours on Tuesday to impress his music teacher. Ellipsis Reinhart ( 1991 ) suggests a syntactic reconstruction account of bare ellipsis which adjoins an NP in the antecedent clause to an NP fragment by LF movement. The result is a conjoined NP which , taken as a generalized quantifier , applies to the antecedent clause , interpreted as a predicate formed by lambda abstraction. So , for example , adjunction of , flowers in the antecedent clause of 7 to the NP fragment chocolates in the ellipsis site produces the LF structure 12a , which is interpreted as 12b. 12a. \ [ IP'\ [ IP John gives Mary tl \ ] \ [ NP\ [ NP flowers\ ] , \ [ NP and \ [ NP chocolates\ ] 2\ ] 2\ ] \ ] b. ( flowers and chocolates ) ( ~x\ [ john gives mary x\ ] ) Given that Reinhart 's analysis relies on LF adjunction of an NP in the antecedent to an NP in the ellipsis site in order to create a generalized quantifier corresponding to a coordinate NP , it is not clear how it can apply to bare ellipsis cases like 3 , in which a sequence of arguments and adjuncts appear in the ellipsis site. Moreover , the analysis can not deal with bare ellipsis cases like 8 , where a bare adjunct fragment does not correspond to any constituent in the antecedent clause. Therefore , this account does not cover the full range of bare ellipsis cases. As we have seen , the proposed generalized reconstruction algorithm does handle bare ellipsis structures like 8. In cases like 3 the algorithm will substitute the head V of the antecedent for the empty verb of the elided clause , and the bare PP adverb will modify the VP headed by this verb. The algorithm will fill some of the complement positions in the SUBCAT list of the reconstructed V with the NP arguments in the ellipsis site , and it will fill the remaining positions with arguments inherited from the antecedent head V. This procedure will yield 690 at least one appropriate reconstruction for the elided clause. Dalrymple et al. ( 1991 ) and Shieber et al. ( 1995 ) present a generalized semantic account which employs higher order-unification of property and relation variables to resolve ellipsis. Their general strategy is to specify the interpretation of the antecedent clause as an equation between a propositional variable S and a predicateargument structure. The arguments of the predicate correspond to the fragments in the ellipsis site , and ellipsis resolution consists in finding an appropriate value for the predicate variable which can apply to both the sequence of arguments in the interpretation of the antecedent clause , and the sequence of arguments in the ellipsis site. Given the equations in 13a-c , higher-order unification correctly generates 13d as the interpretation of 3. 13a. &lt; a~ , a2 &gt; = &lt; book reviews , last year &gt; &amp; &lt; bj , b2 &gt; = &lt; articles , this year &gt; b. S l ( wrote book reviews for the journal ( during ) last year ) ( bill ) c. R = ) ~x~ , y\ [ bill wrote x for the journal ( during ) Yl d. ( book reviews ) ( ) ~x\ [ ( last year ) ( ) ~y\ [ bill wrote x for the journal ( during ) y\ ] ) \ ] ) and ( articles ) ( ~ , x\ [ ( this year ) 0~y\ [ bill wrote x for the journal ( during ) y\ ] ) \ ] ) While the higher-order unification analysis can deal with bare ellipsis cases like 3 ( as well as VP ellipsis and pseudo-gapping ) , it is not clear how to apply it to bare ellipsis examples like 8 , where the adjunct in the ellipsis site lacks a corresponding element in the antecedent clause .</sentence>
				<definiendum id="0">adverb</definiendum>
				<definiens id="0">the head V of the antecedent clause in the first conjunct , and then will fill one of the positions in its SUBCAT list with the local features of chocolates. If it fills the direct object ( third complement ) position of this list with the bare NP , then it will fill the subject and indirect object positions with the local features of John and Mary , generating the reconstructed feature structure corresponding to 9. \ [ NP flowers\ ] \ ] \ ] and \ [ , , \ [ Ne John\ ] \ [ vP \ [ vp \</definiens>
				<definiens id="1">an adjunct daughter of a VP headed by an empty verb</definiens>
				<definiens id="2">an adjunct which modifies a VP. The algorithm will identify sings as the head V of the antecedent clause and substitute it for the empty V in 8. This will yield a feature structure corresponding to 10. 10. \ [ m \ [ ~ John\ ] \ [ ve \ [ v plays\ ] \ ] \ ] and \ [ n , \ [ NP John\ ] \ [ vv \ [ vp \ [ vP \ [ v plays\ ] \ ] \ [ AdvP beautifully\ ] \ ] too\ ] \ ] We employ a rule which permits an unbounded number of adverbs to be generated in successively higher VP 's through left recursion on the daughter VP</definiens>
				<definiens id="3">a conjoined NP which , taken as a generalized quantifier , applies to the antecedent clause , interpreted as a predicate formed by lambda abstraction. So , for example , adjunction of</definiens>
				<definiens id="4">flowers and chocolates ) ( ~x\ [ john gives mary x\ ] ) Given that Reinhart 's analysis relies on LF adjunction of an NP in the antecedent to an NP in the ellipsis site in order to create a generalized quantifier corresponding to a coordinate NP , it is not clear how it can apply to bare ellipsis cases like 3 , in which a sequence of arguments and adjuncts appear in the ellipsis site. Moreover</definiens>
				<definiens id="5">substitute the head V of the antecedent for the empty verb of the elided clause , and the bare PP adverb will modify the VP headed by this verb. The algorithm will fill some of the complement positions in the SUBCAT list of the reconstructed V with the NP arguments in the ellipsis site</definiens>
				<definiens id="6">present a generalized semantic account which employs higher order-unification of property and relation variables to resolve ellipsis. Their general strategy is to specify the interpretation of the antecedent clause as an equation between a propositional variable S and a predicateargument structure. The arguments of the predicate correspond to the fragments in the ellipsis site , and ellipsis resolution consists in finding an appropriate value for the predicate variable which can apply to both the sequence of arguments in the interpretation of the antecedent clause</definiens>
				<definiens id="7">clear how to apply it to bare ellipsis examples like 8 , where the adjunct in the ellipsis site lacks a corresponding element in the antecedent clause</definiens>
			</definition>
</paper>

		<paper id="2145">
			<definition id="0">
				<sentence>nate &lt; l in order gives the vertex list sequence for a tree , e.g. , ( ( a , b , a , x ) , ( a , b , c ) , ( a , b , k ) , ( a , e ) ) repr &lt; ; sents the tree ( a ) il ) Figure ~. t Note that i~ is possible to obtain more spa &lt; : c reduction by aJso sharing any common postflxes of Lhe vertex labe\ ] sequences using a directed acy &lt; : lic graph representation and not a. trie , but this does not iraprow : ' the execution time. 861 l a , I ) , &amp; , x ) b 2 4.b , '1'1 eea I Tree c I 'l'ree b Figure 3 : 'l'rie representation of the 3 trees in Figure. 2 Our concern in this work is not the exact match of trees but rather approximate match. Given the vertex list sequence for a query tree , exact match over the trie can be performed using the standard t ; ech niques by fbllowing the edge labeled with next vertex list until a loft in the trie is reached , ~-md the query vertex label sequence is exhausted. For approximate tree matching , we use the errortolerant approximate tinite-state recognition algorithm ( Oflazer , 1996 ) , which tinds all strings within a giwm error threshold of some string in the regular set accepted by the underlying finitestate acceptor. An adaptation of this algorithm will be briefly summarized here. hh : ror-tolerant matching of vertex list sequences requires an errol : inetric for measuring how rnuch two such sequences deviate from each other. The distance between two sequences measures the minimum number of insertions , deletions and leaf label changes that are necessary to convert one tree into another. It should be noted that this is different fl : om the error metric defined by ( Wang el M. , 1994 ) . Let Z = Z1 , Z.~ , ... , Zp , denote a generic vertex list sequence of p vertex lists. Z\ [ j\ ] denotes the initim subsequence of Z up to and including the ju~ leaf label. We will use X ( of length rn ) to denote the query vertex list sequence , and Y ( of length n ) to denote the sequence that is a ( possibly pattie.I ) candidate vertex list sequence ( from ( ; he database of trees ) . Given two vertex list sequences X and Y , the distance , disffX\ [ m\ ] , Y\ [ n\ ] ) , computed according to the recurrence below , gives the minimum number of leaf insertions , deletions or lea\ [ ' label ( : hai~ges necessary to change one tree to the other. dist ( X\ [ m\ ] , Y\ [ n\ ] ) = dist ( X\ [ m1\ ] , Y\ [ n1\ ] ) if x , ~ , = y , ,. ( last vertex lists a.re sa.me ) : , ti. &lt; x\ [ .~ l\ ] , z\ [ , ~ , \ ] ) + c ' if x. , a , nd y , ~ differ only ~tt the lea.f l~tbel = dist ( X\ [ rn 11 , Y\ [ n\ ] ) + , '- , ' if y , , &lt; x , , ( lexicographica.lly ) X is missing leaf # , ,. = , ti , ~t ( X\ [ , ,4 , Z b I\ ] ) + , S ' if xm &lt; y~ ( lexicogra.phica.lly ) X has ~n extra lc~ff a : ... . Boundary Conditions dist ( X\ [ O\ ] , Z\ [ n\ ] ) = , ~. S dist ( X\ [ m\ ] , Y\ [ O\ ] ) : m.,5 ' For a tree database D and at distance threshold t &gt; O , we consider a query tree represented by a wertex list sequence X\ [ m\ ] ( not in the database ) to match the database with an error of t , if the set C : { r\ [ `` , \ ] l Y\ [ `` , \ ] &lt; 10 and distX\ [ , ,~\ ] , Yb\ ] ) - &lt; t } is not empty .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiendum id="1">S dist</definiendum>
				<definiens id="0">the exact match of trees</definiens>
				<definiens id="1">tinds all strings within a giwm error threshold of some string in the regular set accepted by the underlying</definiens>
				<definiens id="2">inetric for measuring how rnuch two such sequences deviate from each other. The distance between two sequences measures the minimum number of insertions , deletions and leaf label changes that are necessary to convert one tree into another. It should be noted that this is different fl : om the error metric defined by ( Wang el M. , 1994 ) . Let Z = Z1 , Z.~ , ... , Zp , denote a generic vertex list sequence of p vertex lists. Z\ [ j\ ] denotes the initim subsequence of Z up to and including the ju~ leaf label. We will use X ( of length rn ) to denote the query vertex list sequence</definiens>
			</definition>
			<definition id="1">
				<sentence>I , c't l= lnin ( l , n , LZ/M\ ] ) a , , ( i , , = , ~ , ~× ( , , , , , + + \ [ Z/iV/i ) wl , e , :o a4 is ti , ( , ( ( , so of ittsol : tions nnd deloi ; ions .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">i , , = , ~ , ~× ( , , , , , + + \ [ Z/iV/i ) wl , e</definiens>
			</definition>
			<definition id="2">
				<sentence>noLhig tile nodes of the trio } &gt; y subs ( 'rilfl ; e ( l ( l 'S ( qo being the inil ; ial uode ( e.g. , top node in Figure 3 ) ) a , n ( | the la.bols of tl : lo edges l ) y V , a , nd denoting by 8 ( qi , 17 ) the taodo in IJto t , rie that oiie Ca , ll reach \ [ \ [ rOlll llo ( l ( ' qi with edgo la , bol V ( ( |ellotillg : ,t vortex list , ) , wo l ) rcsettt , in I , 'igurc /I , the a , lgorithut \ [ Lr gonera , thlg a.ll Y 's by a ( slightly tnodifiod ) dopllhfirst probing or l , he trio. '</sentence>
				<definiendum id="0">noLhig tile nodes</definiendum>
				<definiendum id="1">V</definiendum>
				<definiendum id="2">bol V</definiendum>
				<definiens id="0">qo being the inil ; ial uode ( e.g. , top node in Figure 3 ) ) a , n ( | the la.bols of tl : lo edges l ) y</definiens>
			</definition>
			<definition id="3">
				<sentence>t &lt; i ( : i , at.g , 1992 ) . We ( : ~ttt t-tote that the ( ' , OlUt ) Ul ; ~tion or l , he olettic ; nt II ( i + 1 , j q1 ) recursJvely de.ponds on only // ( i , j ) , II ( i , .7-i-i ) , 11 ( i+ 1 , j ) f , .o , u the earlier dof initiou of tlic edit disl , anco ( see l , 'iguro 5. ) \ ] ) uring the dopl , h first , so~u'c , } i ( ) t ' the t ( 'i % ont ; rios in cohnnn 'n , o1 ' the lna , trix 11 \ ] ia , vo I ; o I ) o ( ro ) contl : ) ut ; ed , ottiy when the ( 'an ( li ( hd ; e sl ; rilig is o\ [ Ioligth n. \ ] ) urhlg Imx'kt , rax : king , I , ho entries for the last coititlill are 2Nol , e tl , ; Lt wc ha , vc to do this chock since we may coinc to other irreleva.nt , tcrminat nodes during I.he , q ( } aYch. /*push empty candidate , and start node to start search */ P , t.~h. ( ( ' , q0 ) ) while stack not empgy begin l , op ( ( Y ' , qi ) ) /* pop partial sequence Y ' and the node */ for all qj and V such that 6 ( qi , 'l/ ) : qa begin /* extend the candidate sequence */ Y = conc : ~tt ( Y ' , V ) /* u is the current length o:17 Y */ /* check if g has deviated too much , if not push */ i* . ' , a. , Zi.~l ( X\ [ '. , .\ ] , Yb\ ] ) - &lt; t then p ' , t.~h ( ( &lt; q , ) ) /* also see if we are at a filial state */ i* , Z/s~. ( X\ [ , , , ,\ ] , Y\ [ , ,.\ ] ) &lt; : and q , i is a terminal node then output V end end Figure 4 : Algoridmi for error-tolerant recognition o\ [ ' vertex lis~ sequences : ii `` i } , :i ) , ( '/ , .i + l ) .. . ( /+J , j ) , : ( /+~ , /+ : l ) Figure 5 : ( k ) uqmt ; M ; iou of the elo.nionts of t , ho II maJ , rix. disca.rdod , but the entries in prior c ohumls m : o still valid. Thus all enl ; ries required by It ( i + 1 , ,7 '' -I1 ) , except I\ [ ( i , j + \ ] ) , axe nlre~dy awLihtble in the matrix in cohlmns i \ ] a.nd i. The conaputation of c'uldisl , ( X\ [ , t , \ ] , Y\ [ n\ ] ) invo|vcs ~ loop in whioh the minhntun is colul ) uted. 'l'his loop ( hldexing along coh , m , , .7 ' + 1 ) co , np , ,tos ll ( i , j + 1 ) bcfo , 'e it is needed for the computaiAon ol7 ll ( i + l , j + 1 ) . W ( ; hamo cxperinl ( ; nLed with 3 synthcticly goner a.tod sots of trees with the propeJ'tios given in 'l'+ &gt; I ) lc 1 .</sentence>
				<definiendum id="0">rilig</definiendum>
				<definiens id="0">tcrminat nodes during I.he , q ( } aYch. /*push empty candidate , and start node to start search */ P</definiens>
			</definition>
</paper>

		<paper id="1095">
			<definition id="0">
				<sentence>CE : Crossing Error , the number of bracketpairs produced by the parsing system that constitute a crossing error against the treebank .</sentence>
				<definiendum id="0">CE</definiendum>
				<definiens id="0">the number of bracketpairs produced by the parsing system that constitute a crossing error against the treebank</definiens>
			</definition>
			<definition id="1">
				<sentence>PNINII : Parse-error Non-Inherited , the number of bracket-pairs produced by the parsing system that constitute a crossing error , but were not counted for PINH .</sentence>
				<definiendum id="0">PNINII</definiendum>
				<definiens id="0">Parse-error Non-Inherited , the number of bracket-pairs produced by the parsing system that constitute a crossing error</definiens>
			</definition>
			<definition id="2">
				<sentence>The treebank we used was the EDR corpus ( EDR , 1995 ) , a Japanese treebank with mainly newspaper sentences .</sentence>
				<definiendum id="0">EDR corpus</definiendum>
				<definiens id="0">a Japanese treebank with mainly newspaper sentences</definiens>
			</definition>
			<definition id="3">
				<sentence>Let observed YY in real test = E ( YY ) in real test = real test size x Pa in real test x Pb in real test then we get YY-M1 - ( YY + YN M1 ) ( YY + NY M1 ) TTB M1 We do not give the derivation , but when doing the same for NN and combining the equations the following relation between M 1 and M2 holds : M1 = NY×TTB+M2×YY- ( NY++NN ) ( VY+NY~ M2-NN There are usually rnany values for M1 and M2 that satisfy this condition .</sentence>
				<definiendum id="0">YY</definiendum>
				<definiendum id="1">M2</definiendum>
				<definiendum id="2">NY++NN )</definiendum>
				<definiens id="0">in real test = E ( YY ) in real test = real test size x Pa in real test x Pb in real test then we get YY-M1 - ( YY + YN M1 ) ( YY + NY M1 ) TTB M1 We do not give the derivation , but when doing the same for NN and combining the equations the following relation between M 1 and</definiens>
			</definition>
			<definition id="4">
				<sentence>We have also not mentioned the Parse Base ( calculated as the geometric mean over all sentences of ~ , where n is the number of words in the sentence and p the number of parses tbr the sentence ) , because that relates to a grammar rather than a parsing system .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">also not mentioned the Parse Base ( calculated as the geometric mean over all sentences of ~</definiens>
				<definiens id="1">the number of words in the sentence and p the number of parses tbr the sentence</definiens>
			</definition>
</paper>

		<paper id="1092">
			<definition id="0">
				<sentence>n is a list with exactly one element that has the same category and case spc'cification as the NP in the PIq , R. Therefore , { ; he LE will both unify with and be subsumed by the input Datnre structure of the IJPLI { .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">a list with exactly one element that has the same category</definiens>
			</definition>
			<definition id="1">
				<sentence>7 Ungrammatical sentences such as ( 6 ) can be successfiflly ruled out if the PPLR , is applied to an I , E only if the input specification of the LR subsumes the LE ( Hypothesis B ) .</sentence>
				<definiendum id="0">LR</definiendum>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>if P ( A ) is the prot ) ortion of times the coders agree , and P ( E ) is the t ) rot ) ortion of times that coders are expected to agree by chance , K is computed as follows : K = P ( A ) P ( E ) 1 P ( E ) Thus , if there is total agreement among the coders , K will be 1 ; if there is no agreement other than chance agreement , K will be 0 .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">the prot ) ortion of times the coders agree , and P ( E ) is the t ) rot ) ortion of times that coders are expected to agree by chance</definiens>
			</definition>
			<definition id="1">
				<sentence>DRAFTER is a technical author= ing support tool which generates instructions for graphical interfaces .</sentence>
				<definiendum id="0">DRAFTER</definiendum>
				<definiens id="0">a technical author= ing support tool which generates instructions for graphical interfaces</definiens>
			</definition>
</paper>

		<paper id="1096">
			<definition id="0">
				<sentence>Disambiguation is carried out with the latest version ( April 1996 ) of the Constraint Grammar Parser ( GGP ) .</sentence>
				<definiendum id="0">Disambiguation</definiendum>
			</definition>
			<definition id="1">
				<sentence>CONTEXT CONDITIONS is an optional part , but in most cases necessary .</sentence>
				<definiendum id="0">CONTEXT CONDITIONS</definiendum>
				<definiens id="0">an optional part</definiens>
			</definition>
			<definition id="2">
				<sentence>N ( t ) = number of word-form tokens , N ( w ) = number of unique word-forms , amb- ( t ) = ambiguity in tokens , amb- ( w ) = ambiguity in unique word-forms .</sentence>
				<definiendum id="0">N (</definiendum>
				<definiens id="0">w ) = number of unique word-forms , amb- ( t ) = ambiguity in tokens</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>Figure 1 shows the detail of this Mgorithm , where ki denotes the nun &gt; her of possible values assumed by node ( random variable ) Xi , N the input data size , and qog ' denotes the logarithm to the base 2 .</sentence>
				<definiendum id="0">ki</definiendum>
				<definiendum id="1">qog</definiendum>
				<definiens id="0">the nun &gt; her of possible values assumed by node ( random variable ) Xi</definiens>
			</definition>
			<definition id="1">
				<sentence>Suzuki 's algorithm is derived from the Minimum Description Length ( MDL ) principle ( liissanen , 1989 ) which is a principle for statistical estimation in information theory .</sentence>
				<definiendum id="0">Minimum Description Length</definiendum>
				<definiens id="0">a principle for statistical estimation in information theory</definiens>
			</definition>
			<definition id="2">
				<sentence>VV ( &gt; rel &gt; Catc 'd this procoss t.cn lim ( ~s a.nd calculated tlm ; tvcragc l ) Crl ) lexity. '</sentence>
				<definiendum id="0">VV</definiendum>
				<definiens id="0">~s a.nd calculated tlm</definiens>
			</definition>
</paper>

		<paper id="2189">
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>l l ) rOperties ( as a.ttest &lt; ; ( I in cxanq ) le sentences ) are exl ) loited as a clue l ; o the most likely '1 b.lecI .</sentence>
				<definiendum id="0">l l ) rOperties</definiendum>
				<definiens id="0">a.ttest &lt; ; ( I in cxanq ) le sentences ) are exl ) loited as a clue l</definiens>
			</definition>
			<definition id="1">
				<sentence>la I ; ioil~ whMi ( Io nol ; ( 'xh~i.liSl , i , h~ ' lilaiiy sali~iii , disi , iiid , ioi~ all+ ( , ( q , ing v , ,or &lt; l ( ' ( ) o ( 'cii/'l'l : li ( ! o. 'l'hi &lt; ~ i ) oiui , lias I ) e. &lt; `` ti raised in t , he ( ~xa.iill ) l ( ! -I ) n~e ( I lil.eral , ul+ ( ~ I ) y Nire : nl ) ur~ el ; al. ( 199 : I ) au~l t } rani &lt; ) t.o ( 199:1 ) : iii or ( ler I , o ( ~oiiil ) l &lt; 'nieitl , l , li ( ~stt.ui ; i/\ [ r01al , iOlishJ\ ] ) s , Nit~onl ) llr~ el ; ; I.\ ] . i ; c.sorl , I , o iliOl'l ) \ [ io\ [ ( ) Tj ( ~nl ili\ [ 'Ol'lit : +l. I , ioii a.ncl a.ni , on.yiiis , wli ( ~re.a.s ( \ ] rn.liiOl , O iis ( ~ : -+ '+ ( x ) n .innd , iv ( ~ r~ ; ~hti , i &lt; ) lishit ) s '' a ( : qnired \ [ 'l+ ( llil ( 'Ol'p ( ) l'Ft. ( i. ( `` . w ( ) r ( Is , stich as se ( { ileli ( : ( ~s ( ) 1 ' ; /. ( : l , if ) ii , ,4 , wlliNi al ) l ) enr i , o~'l , lier hi ( : oot ' ( linat , +~ ( l sl ; rli ( : l , tlr ( 's ) . ~ ( : t ; s o\ [ ' S ( ~lii ; i.lil , ici+..lly siniila.r v , ; ( ) rds ( 'n.li al~ ( ) Im ili\ [ ' ( , i ; r &lt; , d I'roiii l.h ( &gt; ir ( I\ ] sl , vi\ ] ) tit ; iou ( llarris , I ! )</sentence>
				<definiendum id="0">whMi</definiendum>
				<definiendum id="1">-I ) n~e</definiendum>
				<definiens id="0">~oiiil ) l &lt; 'nieitl , l , li ( ~stt.ui ; i/\ [ r01al , iOlishJ\ ] ) s , Nit~onl ) llr~ el</definiens>
			</definition>
			<definition id="2">
				<sentence>ry so tl t ; c , ; + ++l.l ' ( ~ t'orli ~a lisexl a.ud st ; ( &gt; r+ ' , d , lit l ; lie ( : urreul , I ( I~ all l ) at , i , crns cousi+t , ( ff 377 two elements : a Verb and a Subject ; ( VS pattern ) , or a. Verb and a.n Object ( VO l ) a.ttern ) , h ' , ach pa.ttern is assigned a two-level representation consisting of a set of `` inllerent features '' describing its elements ( context-indel ) endent representat , ion ) , and a set of `` relational feai , ures '' specifying the role of each element within the lexieo-semantic context described 1 ) y the pa.ttern ( ( : ontext del ) endent representa .</sentence>
				<definiendum id="0">pa.ttern ( ( : ontext del</definiendum>
				<definiens id="0">I~ all l ) at , i , crns cousi+t , ( ff 377 two elements : a Verb and a Subject ; ( VS pattern ) , or a. Verb and a.n Object ( VO l ) a.ttern )</definiens>
			</definition>
</paper>

		<paper id="2157">
			<definition id="0">
				<sentence>s ( MUC ) ; a detailed overview of current research can be found in the procecdil~gs ot7 MUC-5 ( nmcS , 1993 ) and the recently concluded MUC-6 , as well as Tipster Project meetings , or ARPA 's Human Language q &gt; chnology workshops ( tipsterl , 1993 ) , ( hltw , 1994 ) .</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">well as Tipster Project meetings , or ARPA 's Human Language q &gt; chnology workshops</definiens>
			</definition>
			<definition id="1">
				<sentence>I W~ ... W , ,W , , W+.2 ... W , , ... . be m string of subsequellt , tokens ( e.g. , words ) in text , such Lhat W~ W~ ... .I/Km is a unit of interesl , ( e.g. , a noun phrase ) rand n is the maximum size of the context window on either side of the unit .</sentence>
				<definiendum id="0">.I/Km</definiendum>
				<definiendum id="1">rand n</definiendum>
			</definition>
			<definition id="2">
				<sentence>For each evidence item t , its significance weight is computed as : f ( t , A ) -f ( t , R ) f ( t , A ) + f ( t , R ) &gt; s f ( t , A ) +y ( t , R ) SW ( t ) = 0 otherwise ( ~ ) where f ( t , X ) is the fl'equency of t in group X , and s is a constant used to filter the noise of very low frequency items .</sentence>
				<definiendum id="0">f ( t , X )</definiendum>
				<definiendum id="1">s</definiendum>
				<definiens id="0">the fl'equency of t in group X</definiens>
				<definiens id="1">a constant used to filter the noise of very low frequency items</definiens>
			</definition>
			<definition id="3">
				<sentence>l~ ) Here f ( A ) is ( an estimate of ) the probability that any given candidate phrase will be accepted by the spotter , and f ( R ) is the probability that this phrase is rejected , i.e. , f ( R ) = l-f ( A ) .</sentence>
				<definiendum id="0">Here f</definiendum>
				<definiendum id="1">f</definiendum>
				<definiens id="0">an estimate of ) the probability that any given candidate phrase will be accepted by the spotter , and</definiens>
				<definiens id="1">the probability that this phrase is rejected , i.e. , f ( R ) = l-f ( A )</definiens>
			</definition>
			<definition id="4">
				<sentence>Th ( ; same seems to at ) ply to ( : oncel ) t sele ( : l ; ion , thai , is , Inultil } le o ( : ( : m'ren ( : es of a ( : an { lidate 1 } hrase within ~t disc { } urse should all 1 } e eithe\ ] ' a ( : eel ) te { l or reje , ( : t ; ( ; { t \ [ ) y the Sl } Ol , te\ ] ' .</sentence>
				<definiendum id="0">Th</definiendum>
				<definiens id="0">an { lidate 1 } hrase within ~t disc { }</definiens>
			</definition>
			<definition id="5">
				<sentence>11 Conclusions In this paper we presented the Universal Spotter , a system that learns to spot in-text references to instances of a given semantic class : people , organizations , products , equipment , tools , to nmne just a few .</sentence>
				<definiendum id="0">Universal Spotter</definiendum>
				<definiens id="0">a system that learns to spot in-text references to instances of a given semantic class : people , organizations , products , equipment , tools , to nmne just a few</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>For example , the month names arc deuoted by the class Ndabc , where lg denotes Nouu , Nd denotes 'lbmporal Nouns , Igda \ [ 'or 'l'im ( ~ lmmes and Ndab for reusabh ' tilne names. '</sentence>
				<definiendum id="0">lg</definiendum>
				<definiendum id="1">Nd</definiendum>
				<definiens id="0">'lbmporal Nouns</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus Qi = ( ( % ... ci~_ , ) where tie iS the current word ( : lass , ci , is the previ ( ) us word class , etc. '\ [ 'here is a CeCal of L N states , which may nleall too many l ) aranl ( 'ters ( l/v+l possible state transitions , each state can transit to L other states ) for the model if N is anything greater th an ont. '</sentence>
				<definiendum id="0">tie iS</definiendum>
				<definiens id="0">the current word ( : lass , ci , is the previ ( ) us word class</definiens>
			</definition>
			<definition id="2">
				<sentence>205 classification relations { C ( ° ) , C ( 1 ) , ... C ( N-l ) } can be used , where C ( ° ) = C represents the original , most detailed classification relation for the current word , and C ( n ) is the less detailed classification scheme for the nth previous word at each state .</sentence>
				<definiendum id="0">C ( n )</definiendum>
				<definiens id="0">the original , most detailed classification relation for the current word , and</definiens>
			</definition>
			<definition id="3">
				<sentence>v ( s lo ) = v ( sLno As in conventional HMM , the Ergodic Multigram HMM consists of parameters E ) N ~- { A , B } , in which A = { aij\ ] , 0 &lt; i , j &lt; _ LQ ( Total number of states ) , denotes the set of state transition probabilities from Qi to Qi , i.e. P ( Q31Qi ) .</sentence>
				<definiendum id="0">Ergodic Multigram HMM</definiendum>
				<definiens id="0">s lo ) = v ( sLno As in conventional HMM , the</definiens>
				<definiens id="1">consists of parameters E ) N ~- { A , B } , in which A = { aij\ ] , 0 &lt; i , j &lt; _ LQ ( Total number of states ) , denotes the set of state transition probabilities from Qi to Qi</definiens>
			</definition>
			<definition id="4">
				<sentence>St , QI ( t ) '' Qi\ [ ~ ) N ) where Q~ ( t ) is the state of the \ [ IMM when the word containing the character st as the last character is produced .</sentence>
				<definiendum id="0">Q~</definiendum>
			</definition>
</paper>

		<paper id="1085">
			<definition id="0">
				<sentence>Tile PARSETALK parser ( for a survey , cf. Neuhaus &amp; Hahn ( 1996 ) ) generates dependency structures for sentences and coherence relations at the text level of analysis .</sentence>
				<definiendum id="0">PARSETALK parser</definiendum>
				<definiens id="0">generates dependency structures for sentences and coherence relations at the text level of analysis</definiens>
			</definition>
			<definition id="1">
				<sentence>Upon reception of a message , the receiving actor processes the associated method , a program composed of several grammatical predicates , e.g. , SYNTAXCtIECK , which accounts for morphosyntactic or word order constraints , or CONCEPTCItECK , which refers to the terminological knowledge representatiou layer and accounts for type and further conceptual admissibility constraints ( number restrictions , etc. ) .</sentence>
				<definiendum id="0">receiving actor</definiendum>
				<definiens id="0">processes the associated method , a program composed of several grammatical predicates , e.g. , SYNTAXCtIECK , which accounts for morphosyntactic or word order constraints</definiens>
			</definition>
			<definition id="2">
				<sentence>2 ) , X Hence , the incompleteness property of our parser stems firom the selective storage of analyses ( i.e. , an `` incomplete chart '' in chart terms ) , partially compensated by reanalysis .</sentence>
				<definiendum id="0">X Hence</definiendum>
				<definiens id="0">an `` incomplete chart '' in chart terms ) , partially compensated by reanalysis</definiens>
			</definition>
</paper>

		<paper id="2156">
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>In our domain ( coronary diseases ) , one often finds expressions such as ( 2 ) une angioplastie du segment II ( an angioplasty of segment II ) ( 3 ) une angioplastie d'une artbre coronaire ( an angioplasty of a coronary artery ) ( 4 ) l'angioplastie de Monsieur X ( the angioplasty of Mr X ) ( 5 ) une angioplastie de la st6nose ( an angioplasty of the stenosis ) where 'angioplasty ' is an action performed on a segment of an artery to enlarge its diameter , while 'stenosis ' is the state of an artery which has a reduced diameter .</sentence>
				<definiendum id="0">'angioplasty '</definiendum>
				<definiens id="0">an angioplasty of segment II ) ( 3 ) une angioplastie d'une artbre coronaire ( an angioplasty of a coronary artery</definiens>
			</definition>
			<definition id="1">
				<sentence>The main domain knowledge elements consist of the domain ontology ( Fig .</sentence>
				<definiendum id="0">main domain knowledge elements</definiendum>
			</definition>
</paper>

		<paper id="2137">
			<definition id="0">
				<sentence>( 2 ) Constraints based on conjunctions , inodal expressions and verbal semantic attributes Sometimes co-occurrence of conjunctions , verbal semantic attributes and moda .</sentence>
				<definiendum id="0">Constraints</definiendum>
				<definiens id="0">based on conjunctions , inodal expressions and verbal semantic attributes Sometimes co-occurrence of conjunctions , verbal semantic attributes and moda</definiens>
			</definition>
</paper>

		<paper id="2197">
			<definition id="0">
				<sentence>granllnar simple PSG , PSG with features , Categorial Grammar with features .</sentence>
				<definiendum id="0">PSG</definiendum>
				<definiens id="0">features , Categorial Grammar with features</definiens>
			</definition>
			<definition id="1">
				<sentence>A major part of our work on the educational tool was the development of a general graphical browser or grapher for the graphical notations used in computational linguistics , especiMly those in computational semantics such as trees , Attribute-Value-Matrices , EKN ( Barwise and Cooper , 1993 ) and 1 ) RSs .</sentence>
				<definiendum id="0">EKN</definiendum>
				<definiens id="0">the development of a general graphical browser or grapher for the graphical notations used in computational linguistics , especiMly those in computational semantics such as trees , Attribute-Value-Matrices</definiens>
			</definition>
</paper>

		<paper id="2155">
</paper>

		<paper id="1090">
			<definition id="0">
				<sentence>Communication is a game ( interaction among autonomous agents ) by definition .</sentence>
				<definiendum id="0">Communication</definiendum>
				<definiens id="0">a game ( interaction among autonomous agents ) by definition</definiens>
			</definition>
			<definition id="1">
				<sentence>A signaling game consists of sender S 's sending a message ( or a signal ) to receiver R and R 's doing some action in response to that message .</sentence>
				<definiendum id="0">signaling game</definiendum>
				<definiens id="0">consists of sender S 's sending a message ( or a signal ) to receiver R and R 's doing some action in response to that message</definiens>
			</definition>
			<definition id="2">
				<sentence>The top branch is the nature 's initial choice of S 's type according to P , the middle layer is S 's decision on which message to send , and finally the bottom layer is R 's choice of her action .</sentence>
				<definiendum id="0">top branch</definiendum>
				<definiens id="0">the nature 's initial choice of S 's type according to P , the middle layer is S 's decision on which message to send</definiens>
			</definition>
			<definition id="3">
				<sentence>That is , P ( c ) is the probability that S intends to communicate semantic content c to R. As before , M is the set of the messages .</sentence>
				<definiendum id="0">P ( c )</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">the probability that S intends to communicate semantic content c to R. As before ,</definiens>
			</definition>
			<definition id="4">
				<sentence>It considers list Cf ( ui ) of forward-looking centers , which are the semantic entities realize~ in ui , where ul is the i-th utte &gt; ance .</sentence>
				<definiendum id="0">ul</definiendum>
				<definiens id="0">the semantic entities realize~ in ui</definiens>
			</definition>
			<definition id="5">
				<sentence>Cb ( u ) is the entity which the discourse is most centrally concerned with at u. Centering theory stipulates the following rule .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiens id="0">the entity which the discourse is most centrally concerned with at u. Centering theory stipulates the following rule</definiens>
			</definition>
			<definition id="6">
				<sentence>In general , to figure out her own best strategy , S ( R ) attempts to infer R 's ( S 's ) strategy by simulating R 's ( S 's ) inference .</sentence>
				<definiendum id="0">S ( R )</definiendum>
				<definiens id="0">attempts to infer R 's ( S 's ) strategy by simulating R 's ( S 's ) inference</definiens>
			</definition>
			<definition id="7">
				<sentence>Each player attempts to maximize the expected utility over the entire compound game , rather than for each constituent game .</sentence>
				<definiendum id="0">player</definiendum>
				<definiens id="0">attempts to maximize the expected utility over the entire compound game</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>Here , C ( jw ) = { jw , /t~ I i= 1 , ' '' , m } and C ( ew ) = { e % /gj I j= 1 , ' '' , n } are the co-occurrence sets of jw and ew , respectively .</sentence>
				<definiendum id="0">C ( ew</definiendum>
			</definition>
			<definition id="1">
				<sentence>C ( j'w ) f ) C ( ew ) = { ( \ ] w i , ewj ) /lkjl i= 1 , ' '' , m ; j= 1 , ' '' , n } is the intersection of C ( jw ) and C ( ew ) , whose elements ~u'e pairs of a Japanese word and an English word with their frequency .</sentence>
				<definiendum id="0">C</definiendum>
				<definiendum id="1">n }</definiendum>
				<definiens id="0">the intersection of C ( jw ) and C ( ew ) , whose elements ~u'e pairs of a Japanese word and an English word with their frequency</definiens>
			</definition>
			<definition id="2">
				<sentence>C ( /w ) ) and eu~ ( G C ( ew ) ) , as occurs in Fig .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">/w ) ) and eu~ ( G C ( ew ) ) , as occurs in Fig</definiens>
			</definition>
			<definition id="3">
				<sentence>The precision is the proportion of extracted word correspondences that arc actually correct .</sentence>
				<definiendum id="0">precision</definiendum>
				<definiens id="0">the proportion of extracted word correspondences that arc actually correct</definiens>
			</definition>
			<definition id="4">
				<sentence>Tables l ( b ) and ( c ) show the pseudo-recall and the precision in Cases A and B , respectively , lu Case A , the pseudo-recall and precision before feedback were 27.8 % Table 2 Examples of extracted word correspondences .</sentence>
				<definiendum id="0">lu Case A</definiendum>
				<definiens id="0">the pseudo-recall and the precision in Cases A and B , respectively</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>Output is a file of rules in the following form : context = C : P~\ ] ... \ ] 1~ I ... I P , ~ -- + t } , where context is one of PREVWORI ) , NEXTWORD , PREVTA ( ~ or NEXTTAG , 2 Cis a word or tag , P1 , ... , t~ , ... , Pn are the ambiguous parts-of-speech to be reduced , Pi is the part-of-speech that replaces P1 , ... , Pi , ... , Pn .</sentence>
				<definiendum id="0">Output</definiendum>
				<definiendum id="1">context</definiendum>
				<definiendum id="2">Pi</definiendum>
				<definiens id="0">a file of rules in the following form : context = C : P~\ ] ... \ ] 1~ I ... I P , ~ -- + t }</definiens>
			</definition>
			<definition id="1">
				<sentence>The Rule Tagger notes this and applies the rule early , thus incorrectly changing many P/SUI~C ( ) NJ pairs to SUBC ( ) NJ and reducing the accuracy of t , he tagging .</sentence>
				<definiendum id="0">Rule Tagger</definiendum>
				<definiens id="0">notes this and applies the rule early , thus incorrectly changing many P/SUI~C ( ) NJ pairs to SUBC ( ) NJ and reducing the accuracy of t</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>We adopt as a working definition the one by ( Sinclair , 1991 ) Collocation is the occurrence of two or more words within a short space of each other in a text .</sentence>
				<definiendum id="0">Collocation</definiendum>
				<definiens id="0">the occurrence of two or</definiens>
			</definition>
			<definition id="1">
				<sentence>The following example illustrates the problem and their N ) proach : consider the strings a= '' in spite '' and b= '' in spite of '' , with n ( a ) and n ( b ) their numbers of oceurrencies in the corpus respectively .</sentence>
				<definiendum id="0">N ) proach</definiendum>
				<definiens id="0">consider the strings a= '' in spite '' and b= '' in spite of '' , with n ( a ) and n ( b ) their numbers of oceurrencies in the corpus respectively</definiens>
			</definition>
			<definition id="2">
				<sentence>So , they modify the measure of frequency of occurrence to become K ( a ) = ( lal 1 ) ( n ( a ) n ( b ) ) ( 1 ) where a is a word sequence la\ [ is the length of a n ( a ) is the number of occurrencies of a in the curpus .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of a n</definiens>
			</definition>
			<definition id="3">
				<sentence>The issue is how to distinguish when a substring of a ( : andidate ( : ollo ( : ation is a candidate collocation , and when it is not .</sentence>
				<definiendum id="0">ation</definiendum>
				<definiens id="0">a candidate collocation</definiens>
			</definition>
			<definition id="4">
				<sentence>For each of them its exclusive frequency ( number of occurrences the n-gram appeared by itself ) , its inclusive frequency ( number of times it appeared in total ) and its relative frequency ( the ratio of its ac .</sentence>
				<definiendum id="0">exclusive frequency</definiendum>
				<definiendum id="1">inclusive frequency</definiendum>
				<definiens id="0">the ratio of its ac</definiens>
			</definition>
			<definition id="5">
				<sentence>As collocation identification ( either in general language or in sublanguages ) finds many applications , the need to automate , as much as possible , that process increases .</sentence>
				<definiendum id="0">collocation identification</definiendum>
				<definiens id="0">either in general language or in sublanguages</definiens>
			</definition>
</paper>

		<paper id="2119">
			<definition id="0">
				<sentence>Ore ' development effort was initiated with a speech-to-speech translation system , called CCLINC ( Tummala et al. , 19951 , which consists of a modular , multilingual structure incmding sI ) eech recognition , language understanding , language generation , and speech synthesis in each language .</sentence>
				<definiendum id="0">CCLINC</definiendum>
				<definiens id="0">consists of a modular , multilingual structure incmding sI ) eech recognition , language understanding , language generation , and speech synthesis in each language</definiens>
			</definition>
			<definition id="1">
				<sentence>Template ( c ) says that a verb phrase consists of an object followed by a verb .</sentence>
				<definiendum id="0">Template</definiendum>
				<definiendum id="1">c )</definiendum>
				<definiens id="0">says that a verb phrase consists of an object followed by a verb</definiens>
			</definition>
			<definition id="2">
				<sentence>We have also discussed ideas on how to make the system robust , and proposed two specific solutions : integration of a part-of-speech tagger and a wordfor-word translator ( Section 5 ) .</sentence>
				<definiendum id="0">wordfor-word translator</definiendum>
				<definiens id="0">also discussed ideas on how to make the system robust</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>Bag generation is a form of natural language gel &gt; er ; ttion in which the input is ; ~ bag ( Mso known as a inultiset : a set in which rcpe~ted elements are significant ) of lexicM elements and the output is a grammatical sentence or a statistically most probable permutation with respect to some .</sentence>
				<definiendum id="0">Bag generation</definiendum>
				<definiens id="0">a form of natural language gel &gt; er</definiens>
			</definition>
			<definition id="1">
				<sentence>4 { thel , manl , shaves &lt; l , \ ] , himselfl } even though 'himself ' has the same index as 'the IIlan'. The approach introduced here compiles the relevant information of\ [ line fi'om the grammar and uses it to check for connectivity during bag generation. The compilation process results in a set of ( Sign , Lex , Bindings ) triples called outer domains. 'l'his set is based on a unification-based phrase structure grammar defined as follows : Definition 2 d grammar is a tuple ( N , 7 ; P , S ) , where P is a sct of productions ce ~ /3 , a is a sign , /3 is a list of signs , N is the set of all ee , T is the set of all signs appearing as elements of \ [ 3 which unify with lexical entries , and S is the start sign. Outer domains are defined as follow : Definition 3 { ( Sign , Lcx , Binds ) I Sign C N tO T , Lcx ~ T and there exists a derivation Oe ~ /31Signt /32 LeJ /33 or a ~ f11Lez\ ] /32 , S'iqnl /33 , and Sign ' a unifier for Sign , Lez j a unifier for Lcx , and Binds the set of all path pairs &lt; SignPath , LexPalh &gt; such thai Sign ' : SignPath is Ioken identical with LezS : LexPath } Intuitively , the outer domains indicate that preterminal category Lex ( 'an appear in a complete sentence with subconstituent Sign , such that l , cx is not a leaf of Sign .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">T</definiendum>
				<definiendum id="2">S</definiendum>
				<definiendum id="3">Sign</definiendum>
				<definiens id="0">shaves &lt; l , \ ] , himselfl } even though 'himself ' has the same index as 'the IIlan'. The approach introduced here compiles the relevant information of\ [ line fi'om the grammar and uses it to check for connectivity during bag generation. The compilation process results in a set of ( Sign , Lex , Bindings ) triples called outer domains. 'l'his set is based on a unification-based phrase structure grammar defined as follows : Definition 2 d grammar is a tuple ( N , 7 ; P , S ) , where P is a sct of productions ce ~ /3 , a is a sign , /3 is a list of signs</definiens>
				<definiens id="1">the set of all ee</definiens>
				<definiens id="2">the set of all signs appearing as elements of \ [ 3 which unify with lexical entries , and</definiens>
				<definiens id="3">' a unifier for Sign , Lez j a unifier for Lcx , and Binds the set of all path pairs &lt; SignPath , LexPalh &gt; such thai Sign ' : SignPath is Ioken identical with LezS : LexPath } Intuitively , the outer domains indicate that preterminal category Lex ( 'an appear in a complete sentence with subconstituent Sign , such that l</definiens>
			</definition>
			<definition id="2">
				<sentence>The other type of generator applies a greedy algorithm to an initial solution in order Co find a grammatical sentence ( 1 ) oznafiski et al. , 1.995 ) .</sentence>
				<definiendum id="0">generator</definiendum>
			</definition>
			<definition id="3">
				<sentence>Inner domains are defined as follows : Definition 5 { ( Sign , Lex , Binds ) I Sign C N U T , Lex 6 7 ' and there exists a derivation ( ~ : ~ /31LezS f12 , with Sign I a unifier for Sign , Le~ s a unifier for Lex , and Binds the set of all path pairs &lt; SignPath , LexPath &gt; such that Sign ' : SignPath is token identical with LezS : LexPath } The inner domains thus express all the possible terminal categories which may be derived from each nonterminal in the grammar .</sentence>
				<definiendum id="0">Le~</definiendum>
				<definiens id="0">s a unifier for Lex , and Binds the set of all path pairs &lt; SignPath</definiens>
			</definition>
			<definition id="4">
				<sentence>In the general case , however , tile size of the outer domains is O ( n2 ) , where n is the number of distinct signs ; this number can be controlled by employing equivalence classes of different levels of specificity for pre-terminal and non-terminal signs .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of distinct signs ; this number can be controlled by employing equivalence classes of different levels of specificity for pre-terminal and non-terminal signs</definiens>
			</definition>
</paper>

		<paper id="2196">
			<definition id="0">
				<sentence>A time map consists of a set of ( potential ) fact tokens ) A fact token is a triple ( tl , t2 , ~o ) , where tl and t2 are time points and ~ is a timeless fact description ( a term ) , that represents the proposition that ~ holds at tl and continues to persist through t2 or until a contradictory fact holds .</sentence>
				<definiendum id="0">time map</definiendum>
				<definiens id="0">consists of a set of ( potential ) fact tokens ) A fact token is a triple ( tl , t2 , ~o ) , where tl and t2 are time points</definiens>
				<definiens id="1">a timeless fact description ( a term ) , that represents the proposition that ~ holds at tl and continues to persist through t2 or until a contradictory fact holds</definiens>
			</definition>
			<definition id="1">
				<sentence>, without E-P or P-I ' contlict , where v is a new variable , create an edge from start ( ci ) to cnd ( ci ) such that its action , rsubactions , constraints , effects , prceonditions , and aend values respectively are &lt; Y , .</sentence>
				<definiendum id="0">v</definiendum>
				<definiens id="0">a new variable , create an edge from start ( ci ) to cnd ( ci ) such that its action , rsubactions , constraints , effects , prceonditions , and aend values respectively</definiens>
			</definition>
</paper>

		<paper id="2184">
			<definition id="0">
				<sentence>Our proposed segmentation standard consists of two major components to meet the goals discussed above .</sentence>
				<definiendum id="0">segmentation standard</definiendum>
				<definiens id="0">consists of two major components to meet the goals discussed above</definiens>
			</definition>
			<definition id="1">
				<sentence>The tripartite segmentation criteria consist of a definition of the segmentation unit , two segmentation principles , and a set of heuristic guidelines .</sentence>
				<definiendum id="0">tripartite segmentation criteria</definiendum>
				<definiens id="0">consist of a definition of the segmentation unit , two segmentation principles , and a set of heuristic guidelines</definiens>
			</definition>
			<definition id="2">
				<sentence>1 ) Segmentation Unitde f is the smallest string of character ( s ) that has both an independent meaning and a fixed grammatical category .</sentence>
				<definiendum id="0">Segmentation Unitde f</definiendum>
				<definiens id="0">the smallest string of character ( s ) that has both an independent meaning and a fixed grammatical category</definiens>
			</definition>
</paper>

		<paper id="2108">
			<definition id="0">
				<sentence>When the morphological analyzer detects an auxiliary verb or an equivalent while checking the information contained in the predicate phrase , the analyzer develops the verb subcategorization frame from the code in the verb 's lexicon and read from the 5 `` The Unique Case Principle in Case Grammar and empirical studies is formulated and explained by the Lexicalist Hypothesis about thematic roles and the X-bar Theory in the school of Universal Grammar ( Chomsky88 ) .</sentence>
				<definiendum id="0">morphological analyzer</definiendum>
				<definiens id="0">detects an auxiliary verb or an equivalent while checking the information contained in the predicate phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>It is a set of independent semantic heuristic rules that drops the 'Autonomous ' reading of `` rareru '' and ahnost drops tile 'Honorific ' reading of `` rareru '' .</sentence>
				<definiendum id="0">'Honorific</definiendum>
				<definiens id="0">a set of independent semantic heuristic rules that drops the 'Autonomous ' reading of `` rareru ''</definiens>
			</definition>
			<definition id="2">
				<sentence>M-Block contains the very surface information and can in general be linked to multiple S-Blocks .</sentence>
				<definiendum id="0">M-Block</definiendum>
				<definiens id="0">contains the very surface information and can in general be linked to multiple S-Blocks</definiens>
			</definition>
			<definition id="3">
				<sentence>A C-Block linked from an S-Block or more represents an independent word sense , and , ideally , is linked to by other S-blocks that are linked to by other M-Blocks , in effect , other words of the same or the different language .</sentence>
				<definiendum id="0">C-Block</definiendum>
				<definiens id="0">linked from an S-Block or more represents an independent word sense</definiens>
			</definition>
</paper>

		<paper id="1060">
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>\ [ Iere , we summarise the two relations in the form of the following planning statements : , , ( ~ generates fl iff c~ is the body of a plan e whose goal is ft. , , oe enables fl if ce is a precondition of a plan e and/3 is the goal of plan e , or iffl is the body of e and t~ is a preconditkm of/3 .</sentence>
				<definiendum id="0">iffl</definiendum>
				<definiendum id="1">t~</definiendum>
				<definiens id="0">the body of e and</definiens>
			</definition>
</paper>

		<paper id="2215">
			<definition id="0">
				<sentence>An STSG is a five-tuple ( VN , VT , S , d , PT ) , where VN and VT denote respectively the finite set of nonterminal and terminal symbols , S denotes the start non-terminal , C is a finite set of elementarytrees ( of arbitrary depth &gt; 1 ) and PT is a function which assigns a value 0 &lt; PT ( t ) &lt; 1 ( probability ) to each elementary-tree t such that for all N EVN : Y\ ] .</sentence>
				<definiendum id="0">STSG</definiendum>
				<definiendum id="1">S</definiendum>
				<definiendum id="2">C</definiendum>
				<definiendum id="3">PT</definiendum>
				<definiens id="0">a five-tuple ( VN , VT , S , d , PT ) , where VN and VT denote respectively the finite set of nonterminal and terminal symbols</definiens>
				<definiens id="1">the start non-terminal ,</definiens>
				<definiens id="2">a finite set of elementarytrees ( of arbitrary depth &gt; 1</definiens>
			</definition>
			<definition id="1">
				<sentence>tee , root ( tl=N PT ( t ) = 1 ( where root ( t ) denotes the root of tree t ) .</sentence>
				<definiendum id="0">tl=N PT</definiendum>
				<definiens id="0">the root of tree t )</definiens>
			</definition>
			<definition id="2">
				<sentence>The probability of a parse is defined as the sum of the probabilities of the derivations that generate it .</sentence>
				<definiendum id="0">probability of a parse</definiendum>
				<definiens id="0">the sum of the probabilities of the derivations that generate it</definiens>
			</definition>
			<definition id="3">
				<sentence>The probability of a sentence is the sum of the probabilities of all derivations that generate that sentence .</sentence>
				<definiendum id="0">probability of a sentence</definiendum>
				<definiens id="0">the sum of the probabilities of all derivations that generate that sentence</definiens>
			</definition>
			<definition id="4">
				<sentence>The main source of NP-completeness is the following common structure of these problems : they all search for an entity that maximizes the sum of the probabilities of processes which depend on that entity .</sentence>
				<definiendum id="0">NP-completeness</definiendum>
				<definiens id="0">the following common structure of these problems : they all search for an entity that maximizes the sum of the probabilities of processes which depend on that entity</definiens>
			</definition>
			<definition id="5">
				<sentence>Probabilistic Tree-Adjoining Grammar as a Framework for Statistical Natural Language Processing .</sentence>
				<definiendum id="0">Probabilistic Tree-Adjoining Grammar</definiendum>
			</definition>
</paper>

		<paper id="2107">
			<definition id="0">
				<sentence>Global cohesion is a top-down structured context and is based on a hierarchy of topics led by domain ( e.g. , hotel reservation or flight cancellation ) .</sentence>
				<definiendum id="0">Global cohesion</definiendum>
				<definiens id="0">a top-down structured context and is based on a hierarchy of topics led by domain ( e.g. , hotel reservation or flight cancellation )</definiens>
			</definition>
			<definition id="1">
				<sentence>hesion In this paper , we approximate an utterance in a dialogue to a three-tuple ; U = ( SPEECH __ ACT , VERB , NOUNS ) ( 1 ) where SPEECH ACT is the speech act type , VERB is the ma\ ] -n verb in the utterance , and NOUNS is a set of nouns in the utterance ' ( e.g. , a subject noun and an object noun for the main verb ) .</sentence>
				<definiendum id="0">SPEECH ACT</definiendum>
				<definiendum id="1">VERB</definiendum>
				<definiendum id="2">NOUNS</definiendum>
				<definiens id="0">the speech act type</definiens>
				<definiens id="1">the ma\ ] -n verb in the utterance</definiens>
				<definiens id="2">a set of nouns in the utterance '</definiens>
			</definition>
</paper>

		<paper id="2210">
			<definition id="0">
				<sentence>Syntax : A general grammar has rules which interfere with other rules .</sentence>
				<definiendum id="0">Syntax</definiendum>
				<definiens id="0">A general grammar has rules which interfere with other rules</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , during the morphological and syntactical analysis of the sentence `` I ( Y ) want ( V ) the ( D ) c-mail ( F ) address ( F or '' V ) '' , interactions between MORPH and SYNT are useful .</sentence>
				<definiendum id="0">V</definiendum>
			</definition>
			<definition id="2">
				<sentence>Ri : is the information request i at which the agents have to answer .</sentence>
				<definiendum id="0">Ri</definiendum>
				<definiens id="0">the information request i at which the agents have to answer</definiens>
			</definition>
</paper>

		<paper id="2132">
			<definition id="0">
				<sentence>Roughly speaking , in tile active voice , the SUBJECT is the nominative. , on the other hand , in the passive voice , the SUBJECT is the nominative of the corresponding sentence in the active voice .</sentence>
				<definiendum id="0">SUBJECT</definiendum>
				<definiendum id="1">SUBJECT</definiendum>
				<definiens id="0">the nominative of the corresponding sentence in the active voice</definiens>
			</definition>
			<definition id="1">
				<sentence>The RU form is the basic form of verbs and it denotes the non-past tense .</sentence>
				<definiendum id="0">RU form</definiendum>
				<definiens id="0">the basic form of verbs</definiens>
			</definition>
			<definition id="2">
				<sentence>the conditionals REBA , TARA and NARA Because of the characteristics of each conditionals descril ) e ( t in Table 1 , we expect that a ) the conjunctive REBA , which shows a causal relation , has the same constraint as TO has , which also express causality , b ) since both of TARA and NARA express an assumption , they have the same type of constraint , which is difl'erent fi'om the constraint of TO and REBA .</sentence>
				<definiendum id="0">REBA</definiendum>
				<definiendum id="1">constraint</definiendum>
				<definiens id="0">difl'erent fi'om the constraint of TO and REBA</definiens>
			</definition>
</paper>

		<paper id="1066">
			<definition id="0">
				<sentence>The main operation on A DRSs is the , fltnctional 4This is the so called co-descriptive apprvach .</sentence>
				<definiendum id="0">DRSs</definiendum>
				<definiens id="0">the so called co-descriptive apprvach</definiens>
			</definition>
			<definition id="1">
				<sentence>A grammar rule consists of three parts : Context free rule , s over category symbols ( ' , onstitutc the l ) aekt ) onc of every grammar .</sentence>
				<definiendum id="0">grammar rule</definiendum>
				<definiens id="0">consists of three parts : Context free rule , s over category symbols ( ' , onstitutc the l ) aekt ) onc of every grammar</definiens>
			</definition>
			<definition id="2">
				<sentence>Phraseo-Lex is a computational lexicon which was specially develot ) ed for idiomatic knowledge .</sentence>
				<definiendum id="0">Phraseo-Lex</definiendum>
				<definiens id="0">a computational lexicon which was specially develot ) ed for idiomatic knowledge</definiens>
			</definition>
			<definition id="3">
				<sentence>Of all dive , rsed syntactic , semantic , and pragmatic iifformal , ion provided by 1 ) IIRASI. ; O-LEX , we only ne , c ( l for our i ) url ) ose h'mmata , base lexemes , ( idiom t ) artMt ) ating lcxi ( : al words : Bock , , schicfl ( , n ) , tim inte~rnal syntacti ( : structure encoded as a syntactic tre , e , the internal sclnantic structure ellcoded as predicate argument structure and the logical tbrm .</sentence>
				<definiendum id="0">e</definiendum>
				<definiens id="0">al words : Bock , , schicfl ( , n ) , tim inte~rnal syntacti ( : structure encoded as a syntactic tre ,</definiens>
			</definition>
			<definition id="4">
				<sentence>This will reduce the munber of lexical lookups to PItRASI , ; O LEX as well as the number of edges in the t ) arser .</sentence>
				<definiendum id="0">O LEX</definiendum>
				<definiens id="0">the number of edges in the t</definiens>
			</definition>
</paper>

		<paper id="2200">
			<definition id="0">
				<sentence>A mapping technique reduces the size the alphabet to at most IPI where P is the pattern string .</sentence>
				<definiendum id="0">mapping technique</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">the pattern string</definiens>
			</definition>
			<definition id="1">
				<sentence>For instance , Web browsers ( e.g. N etscape ) can not interpret the annotations represented by their equivalent 2-byte characters .</sentence>
				<definiendum id="0">Web browsers</definiendum>
				<definiens id="0">interpret the annotations represented by their equivalent 2-byte characters</definiens>
			</definition>
			<definition id="2">
				<sentence>To achieve O ( 1 tPI D , the function , found ( ) , uses an array , f\ [ \ ] , of size 1El ( where I2 is the alphabet ) to store the equivalent single-byte characters .</sentence>
				<definiendum id="0">I2</definiendum>
				<definiens id="0">the alphabet ) to store the equivalent single-byte characters</definiens>
			</definition>
			<definition id="3">
				<sentence>Since the space-time complexity in constructing the automaton depends on the size of the alphabet ( i.e. o ( \ ] ElxlQcD where Qc is the set of states of Me ) which is large , this approach is not attractive .</sentence>
				<definiendum id="0">Qc</definiendum>
				<definiens id="0">the set of states of Me</definiens>
			</definition>
			<definition id="4">
				<sentence>The NULl , character , Z , represents all the characters in ) .</sentence>
				<definiendum id="0">NULl , character , Z ,</definiendum>
				<definiens id="0">represents all the characters in )</definiens>
			</definition>
</paper>

		<paper id="2144">
			<definition id="0">
				<sentence>m~tic usage resolution h~ts to be ( lea , It with in a. principled way and not by pnttcrn nmtching ( e.g. , ~s in 'l'schichold ( 1995 ) ) , when the l~mg , u~tge has a free word order , where l ) ~l , tern matching al ) pronchcs could 5dl .</sentence>
				<definiendum id="0">u~tge</definiendum>
				<definiens id="0">a free word order</definiens>
			</definition>
</paper>

		<paper id="2126">
			<definition id="0">
				<sentence>OVI &lt; RAI , L ARCIlITECTURE MUI/ITI'ALE SYSTEM OF TIlE The MUI , TITAI , E system consists of 5 modules : language. and PPs. 'l'he syntactical module functions as a preprocessor. As a consequence syntax is kept to a minimum and partial : only clause segmentation , verbdetection and NI'/PI ' demarcation is considered to be necessary. minimal NPs and PPs. Given syntactical `` chuncks '' ( verbs , NPs , PPs ) within a certain space ( ehmse ) , MUI/1TFALE tries to assign eolmepts to them. The system is preferential in that it has defined based upon corpus observation and sublanguage modelling a priority ranking/hierarchy of concepts when occurring in combination , to lbrm one complex concept. This priority ranking/hierarchy looks as follows : CC Surgical l ) eed CC_ l'athology ( 2C_ .lnterventional Equipment CC Anatonay CC Colnbi It governs the concept type calculus so that e.g. in an expression with a Surgical Deed , the latter overrules all other concepts as in rechler \ ] CC-Modifier-lk } dyside\ ] retromasloidale ICC-Anatomyl incisie \ [ CC-Surgicat deed\ ] . So , too in tire expression oste @ taire \ [ CCl'ath { } logy\ ] randen \ [ CC-Combi\ ] van de dekplaat \ ] CCAnatt } my\ ] tile Pathology concept will overrule the other COlrccpts. with other medical concepts witllin the clause. tmkn ( } wn w ( } rds '\ [ 'he last two modules will bc discussed in scction IV. and V. I } elow. 2 ... ... surgical deed clause limit s = : synlactic tag I ( = semantic link q : = linked wilh CC = concept type tag CS : : surgical deed stlbtypc lag I ) uring this process MUI/I'ITAI , E uses the following knowledge bases : ( A ) the st , rgical deed lexicon : a lexicon of surgical deed concepts , containing about 250 tokens with their COllcepl type ( CC st , rgical._deed ) , surgical deed subtype and part-of-speech ( = &lt; CAT &gt; ) .</sentence>
				<definiendum id="0">NI'/PI</definiendum>
				<definiens id="0">preferential in that it has defined based upon corpus observation and sublanguage modelling a priority ranking/hierarchy of concepts when occurring in combination , to lbrm one complex concept. This priority ranking/hierarchy looks as follows : CC Surgical l</definiens>
				<definiens id="1">the concept type calculus so that e.g. in an expression with a Surgical Deed , the latter overrules all other concepts as in rechler \ ] CC-Modifier-lk } dyside\ ] retromasloidale ICC-Anatomyl incisie \ [ CC-Surgicat deed\ ] . So</definiens>
				<definiens id="2">semantic link q : = linked wilh CC = concept type tag CS : : surgical deed stlbtypc lag I ) uring this process MUI/I'ITAI , E uses the following knowledge bases : ( A ) the st , rgical deed lexicon : a lexicon of surgical deed concepts , containing about 250 tokens with their COllcepl type ( CC st , rgical._deed ) , surgical deed subtype and part-of-speech ( = &lt; CAT &gt; )</definiens>
			</definition>
			<definition id="1">
				<sentence>Case grammar gives an analysis of a sentence , centered around lhe verb .</sentence>
				<definiendum id="0">Case grammar</definiendum>
				<definiens id="0">gives an analysis of a sentence , centered around lhe verb</definiens>
			</definition>
			<definition id="2">
				<sentence>CEN gives an a , mlysis of a surgical procedure , centered arotmd the Surgical deed concept ( which is often a verb ) .</sentence>
				<definiendum id="0">CEN</definiendum>
				<definiendum id="1">Surgical deed concept</definiendum>
				<definiens id="0">gives an a , mlysis of a surgical procedure , centered arotmd the</definiens>
			</definition>
			<definition id="3">
				<sentence>The set consists of a slot called ROLEfor the type of Semantic Link , a slot -called ARGlbr the pointer to the clement in the sentence which is linked to the surgical deed concept , a slot -called CC tbr the concept type ' of the element which is linked and finally a slot -called INDfor the indication ( I-value ) of the function of the linked element .</sentence>
				<definiendum id="0">set</definiendum>
			</definition>
			<definition id="4">
				<sentence>The guessing module is an important help tbr the attgmentation of the concept lexicon , and consequently an important part of the Multitale system when tagging unknown texts .</sentence>
				<definiendum id="0">guessing module</definiendum>
				<definiens id="0">an important help tbr the attgmentation of the concept lexicon , and consequently an important part of the Multitale system when tagging unknown texts</definiens>
			</definition>
</paper>

		<paper id="1080">
			<definition id="0">
				<sentence>A large corpus ( about 100 MB of text ) was selected and examples of 750 fl'equently occurring verbs were tagged with their compleinent ( : lass as defined by a large computational syntactic dictionary , COMLEX Syntax .</sentence>
				<definiendum id="0">large corpus</definiendum>
				<definiens id="0">about 100 MB of text ) was selected and examples of 750 fl'equently occurring verbs were tagged with their compleinent ( : lass as defined by a large computational syntactic dictionary , COMLEX Syntax</definiens>
			</definition>
			<definition id="1">
				<sentence>The Brandeis notation is compositional , consisting of lists of elements joined by hyphens e.g. pl-ing-sc ( preposition `` about '' followed by a gerund where tile matrix subject is the subject of the gerund e.g. `` he lied about going '' ) .</sentence>
				<definiendum id="0">tile matrix subject</definiendum>
				<definiens id="0">the subject of the gerund e.g. `` he lied about going '' )</definiens>
			</definition>
			<definition id="2">
				<sentence>473 ( VERB : ORTH `` adJust '' : SUBC ( ( ( P-POSSING : PVAL ( `` to '' ) ) ( PP : PVAL ( `` for ... . to '' ) ) ( NP-PP : PVAL ( `` for ... . to '' ) ) ( NP ) ( INTRANS ) ) : TAGS ( ( TAG : BYTE-NUMBER 6002672 : SOURCE `` brown '' : LABEL ( NP ) ) ( TAG : BYTE-NUMBER 6203043 : SOURCE `` brown '' : LABEL ( PP : PVAL ( `` to '' ) ) ) ( TAG : BYTE-NUMBER 5717471 : SOU RCE `` brown '' : LABEL ( NP ) ) ( TAG : BYTE-NUMBER 5537823 : SOU RCE `` brown '' : LABEL ( NP-PP : PVAL ( `` to '' ) ) ) Figure 1 : Partial Comlex Syntax dictionary entry for ADJUST .</sentence>
				<definiendum id="0">INTRANS ) ) : TAGS (</definiendum>
				<definiens id="0">Partial Comlex Syntax dictionary entry for ADJUST</definiens>
			</definition>
</paper>

		<paper id="2134">
			<definition id="0">
				<sentence>it ( s ) i=1 d : l where k is the number of varieties of the kanji characters and 1 is tile number of the domains .</sentence>
				<definiendum id="0">k</definiendum>
			</definition>
</paper>

		<paper id="2115">
			<definition id="0">
				<sentence>A cross-category projection creates a new constituent with the same start and end vertex in the chart as the subconstituent from which it is projected .</sentence>
				<definiendum id="0">cross-category projection</definiendum>
				<definiens id="0">creates a new constituent with the same start and end vertex in the chart as the subconstituent from which it is projected</definiens>
			</definition>
			<definition id="1">
				<sentence>Scrambhng is a process that modifies the order of clause-internal arguments and adjuncts under some constraints ( cf. for instance , Uszkoreit 1987 ) .</sentence>
				<definiendum id="0">Scrambhng</definiendum>
				<definiens id="0">a process that modifies the order of clause-internal arguments and adjuncts under some constraints ( cf. for instance</definiens>
			</definition>
			<definition id="2">
				<sentence>Extraposition is the occurrence of prepositional or sentential complements or adjuncts after the verb in its base position V ° .</sentence>
				<definiendum id="0">Extraposition</definiendum>
			</definition>
			<definition id="3">
				<sentence>In the example ( 8 ) , the pronoun sic is the direct object of the infinitival clause , although it is attached to the main clause .</sentence>
				<definiendum id="0">pronoun sic</definiendum>
				<definiens id="0">the direct object of the infinitival clause</definiens>
			</definition>
			<definition id="4">
				<sentence>DIPS is a practical system under development , which uses a large-sized lexicon ( over 150,000 entries ) and which , at present , covers a large range of grammatical constructions such as simple and complex sentences , finite and non-finite clauses , active and passive voice , wh-constructions , topicalization , extraposition , scrambling , long-distance dependencies , and verb raising .</sentence>
				<definiendum id="0">DIPS</definiendum>
				<definiens id="0">a practical system under development , which uses a large-sized lexicon ( over 150,000 entries ) and which , at present , covers a large range of grammatical constructions such as simple and complex sentences , finite and non-finite clauses , active and passive voice , wh-constructions , topicalization , extraposition , scrambling , long-distance dependencies , and verb raising</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>The MDL Principle is a well-motivated and theoretically sound principle for data compression and estimation in information theory and statistics .</sentence>
				<definiendum id="0">MDL Principle</definiendum>
				<definiens id="0">a well-motivated and theoretically sound principle for data compression and estimation in information theory and statistics</definiens>
			</definition>
			<definition id="1">
				<sentence>We then define a probabilistic model ( a joint distribution ) , written I ' ( C , , ( : v ) , where random variable C , , assumes a value fl'om a fizcd nouu partition ~PX , and C~ .</sentence>
				<definiendum id="0">probabilistic model</definiendum>
				<definiens id="0">a joint distribution ) , written I ' ( C , , ( : v ) , where random variable C , , assumes a value fl'om a fizcd nouu partition ~PX</definiens>
			</definition>
			<definition id="2">
				<sentence>log I , ~'t ( 4 ) where ISl denotes the input data size , and/¢ , .</sentence>
				<definiendum id="0">ISl</definiendum>
			</definition>
			<definition id="3">
				<sentence>logP ( n , v ) ( 5 ) ( n , v ) ES where f ( n , ,v ) denotes the observed frequency of the noun verb pair ( n , v ) , and P ( n , v ) the estimated probability of ( n , v ) , which is calculated as follows .</sentence>
				<definiendum id="0">logP</definiendum>
				<definiendum id="1">f ( n , ,v )</definiendum>
				<definiens id="0">the observed frequency of the noun verb pair ( n , v ) , and P ( n , v ) the estimated probability of ( n , v</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>£1 is the class of languages generated by context sensitive granlmars -- the sole restriction on production rules in this type of grammar is that the right hand side ( RHS ) of each rule is at least as long as the left hand side ( LHS ) .</sentence>
				<definiendum id="0">RHS</definiendum>
				<definiens id="0">the class of languages generated by context sensitive granlmars -- the sole</definiens>
			</definition>
			<definition id="1">
				<sentence>£2 is the class of context free languages generated by grammars whose productions are restricted such that the LHS of each is a single nonterminal symbol , and each RHS is a sequence of terminals and nonterminals .</sentence>
				<definiendum id="0">RHS</definiendum>
				<definiens id="0">the class of context free languages generated by grammars whose productions are restricted such that the LHS of each is a single nonterminal symbol , and each</definiens>
				<definiens id="1">a sequence of terminals and nonterminals</definiens>
			</definition>
			<definition id="2">
				<sentence>I -- -- ÷ 2The bracketed material indicates a stack of indices ; W denotes a sequence of elements of terminals and nonterminals ; A , B denote nonterminals .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">a sequence of elements of terminals and nonterminals ; A , B denote nonterminals</definiens>
			</definition>
			<definition id="3">
				<sentence>An important property of the each of the language classes is that it is closed under bottl intersection with regular languages ( e.g. , the intersection of a context free language and a regular language is no more expressive than a context free language ) and homomorphism ( e.g. , an order preserving map of each symbol in a language to a single element ( possibly a string ) of a context free language implies that the first language is also context free ) .</sentence>
				<definiendum id="0">regular language</definiendum>
			</definition>
			<definition id="4">
				<sentence>Shieber ( 1985 ) provides a stringset argument about a dialect of Swiss-German , which has a class of verb phrases with cross-serial dependencies ( through case marking ) between NPs and their Vs , which establishes even the weak-noncontext-freeness of natural language because of homomorphism to ww .</sentence>
				<definiendum id="0">Swiss-German</definiendum>
				<definiens id="0">has a class of verb phrases with cross-serial dependencies ( through case marking ) between NPs and their Vs , which establishes even the weak-noncontext-freeness of natural language because of homomorphism to ww</definiens>
			</definition>
			<definition id="5">
				<sentence>It , might be claimed that just as we argue ww not to require the worst case complexity for its language class ( £1.5 ) , neither need ww n for £2 ; but , the reversal language is a canonical example of a language that makes maximal use of the stack in the PDA .</sentence>
				<definiendum id="0">reversal language</definiendum>
				<definiens id="0">a canonical example of a language that makes maximal use of the stack in the PDA</definiens>
			</definition>
			<definition id="6">
				<sentence>Josh| and Rainbow ( Josh| , 1990 ; Rainbow and Josh| , 1994 ) have also considered the perforntan ( 'e data associated with processing crossed vs. nested dependencies and present an alternative computal , ion model , | ; tie bottom-up embedded PDA ( BEPDA ) , designed for a wit|an| ; of tree-adjoining gralnmar ( it uses a stack of slacks and a more complex operation for eml ) tying the stack ) .</sentence>
				<definiendum id="0">Josh|</definiendum>
				<definiens id="0">'e data associated with processing crossed vs. nested dependencies and present an alternative computal , ion model , |</definiens>
			</definition>
			<definition id="7">
				<sentence>Essentially , their analysis ( : oncludes ( ; tie satne : when judging string isomorphisnls , it ; is easier to make the judgment of identic~flly ordered pairs than it is to reversely ordered pairs .</sentence>
				<definiendum id="0">tie satne</definiendum>
				<definiens id="0">easier to make the judgment of identic~flly ordered pairs than it is to reversely ordered pairs</definiens>
			</definition>
</paper>

		<paper id="2142">
			<definition id="0">
				<sentence>TMR is a fr , 'une-based language , where frame names typically refer to instances of ontological concepts , slot names are derived from a set of ontological properties and slot fillers , are either elements of property value sets or pointers to concept instances .</sentence>
				<definiendum id="0">TMR</definiendum>
				<definiens id="0">a fr , 'une-based language , where frame names typically refer to instances of ontological concepts , slot names are derived from a set of ontological properties and slot fillers</definiens>
			</definition>
			<definition id="1">
				<sentence>In ( 3 ) the linking attribute ( SIZE ) is selected rather high in the hierarchy of attributes , because in the ontology SIZE-ATTRIBUTE is the parent of such properties as LENGTH-A TT R I BUT E , WlDTtI-AqTRIBUTE , A R E AATTRIBUTE , WEIGHT-ATTRIBUTE , etc .</sentence>
				<definiendum id="0">SIZE</definiendum>
				<definiendum id="1">SIZE-ATTRIBUTE</definiendum>
			</definition>
			<definition id="2">
				<sentence>Salient properties are , however , hard to identify formally , as is well known , for instance , in the scholarslfip on metaphor , where salience is the determining factor for the similarity dimension on whicll metaphors ( , and similes ) are based , It is , therefore , wise to avoid having to search for the salient property , and the principle of practical effability offers a justification for tiffs .</sentence>
				<definiendum id="0">salience</definiendum>
				<definiens id="0">the determining factor for the similarity dimension on whicll metaphors ( , and similes ) are based , It is , therefore , wise to avoid having to search for the salient property</definiens>
			</definition>
			<definition id="3">
				<sentence>Set2 is the set of , all properties of the members of setl ; set3 is tile set of all properties of ^ $ var 1 ; set4 is , essentially , the intersection of set2 and set3 .</sentence>
				<definiendum id="0">Set2</definiendum>
				<definiendum id="1">set3</definiendum>
				<definiendum id="2">set4</definiendum>
				<definiens id="0">the set of , all properties of the members of setl ;</definiens>
				<definiens id="1">tile set of all properties of ^ $ var 1</definiens>
			</definition>
			<definition id="4">
				<sentence>( 9 ) ( abuse ( abuse-V 1 ( CAT V ) ( SYNSTRUC ( ( root Svar0 ) ( cat v ) ( subj ( ( root $ varl ) ( eat n ) ) ( obj ( ( root Svar2 ) ( cat n ) ) ) ) ) ) ( SEM-STRUC ( LEX-MAP ( communicativeevent ( agent ( value ^ $ varl ) ( sere human ) ) ( benef ( value ^ $ var2 ) ( sem human ) ) ( theme ( value refseml ) ) ( attitudel ( type evaluative ) ( attitude-value ( value ( &lt; 0.25 ) ) ) ( scope refsem 1 ) ( attributed-to ( OR ( ^ $ var'2 speaker ) ) ) ) ( attitude2 ( type evaluative ) ( attitude-value ( value ( &lt; 0.25 ) ) ) ( scope ^ $ var2 ) ( attributed-to ^ $ varl ) ) ) ) ) ) Relalive adjectives , are denourinal , object-related , in their meaning .</sentence>
				<definiendum id="0">abuse-V 1</definiendum>
				<definiendum id="1">CAT V ) ( SYNSTRUC (</definiendum>
				<definiendum id="2">SEM-STRUC ( LEX-MAP</definiendum>
				<definiens id="0">( root Svar0 ) ( cat v ) ( subj ( ( root $ varl ) ( eat n ) ) ( obj ( ( root Svar2 ) ( cat n ) )</definiens>
			</definition>
			<definition id="5">
				<sentence>The MikroKosmos analyzer treats modification by attempting to merge the meanings of the modifiers into the meanings of the modified .</sentence>
				<definiendum id="0">MikroKosmos analyzer</definiendum>
				<definiens id="0">treats modification by attempting to merge the meanings of the modifiers into the meanings of the modified</definiens>
			</definition>
			<definition id="6">
				<sentence>If TEXTURE is defined as a property of PIIYSICAL-OBJECF or MATERIAL , and SILK is a descendent of either of them , then the v ' , duc carried in the lexicon entry for smooth will be inserted by the analyzer as the TEXTURE property value for file instance of silk in tim TMR .</sentence>
				<definiendum id="0">SILK</definiendum>
				<definiens id="0">a property of PIIYSICAL-OBJECF or MATERIAL , and</definiens>
				<definiens id="1">a descendent of either of them , then the v ' , duc carried in the lexicon entry for smooth will be inserted by the analyzer as the TEXTURE property value for file instance of silk in tim TMR</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>A lexical operation removes the argument from Sur~cA'r and puts it onto SI , AStt .</sentence>
				<definiendum id="0">lexical operation</definiendum>
			</definition>
			<definition id="1">
				<sentence>For each syllable to be classified the following prosodic features were computed fully automatically from the speech signal for the syllable under consideration and for the six syllables in the left and the right context : • the normalized duration of the syllable nucleus • the minimum , maximum , onset , and offset of fundamental frequency ( FO ) and the maximum energy and their positions on the time axis relative to the position of the actual syllable • the mean energy , and the mean FO • flags indicating whether the syllable carries the lexical word accent or whether it is in a word final position The following features were computed only for the syllable under consideration : • the length of the pause ( if any ) preceding or succeeding the word containing the syllable • the linear regression coefficients of the F0contour and the energy contour computed over 15 different windows to the left and to the right of the syllable This amounts to a set of 242 features , which so far achieved best results on a large database of read speech ; for a more detailed account of the feature evaluation , ( cf. ( Kief~ling , 1996 ) ) .</sentence>
				<definiendum id="0">FO</definiendum>
				<definiens id="0">syllable • the mean energy , and the mean FO • flags indicating whether the syllable carries the lexical word accent or whether it is in a word final position The following features were computed only for the syllable under consideration : • the length of the pause ( if any ) preceding or succeeding the word containing the syllable • the linear regression coefficients of the F0contour and the energy contour computed over 15 different windows to the left and to the right of the syllable This amounts to a set of 242 features , which so far achieved best results on a large database of read speech</definiens>
			</definition>
</paper>

		<paper id="2199">
			<definition id="0">
				<sentence>Alex is a simple dictionary tool with two main features ( 1 ) a highlevel of scriptability ( declined on MacOS with AppleScript ) and ( 2 ) built-in extension facilities allowing to make it the core of Web and e-mail servers .</sentence>
				<definiendum id="0">Alex</definiendum>
				<definiens id="0">a simple dictionary tool with two main features</definiens>
			</definition>
			<definition id="1">
				<sentence>=46iltering is a feature that is naturally derived flom the conversion of the structure .</sentence>
				<definiendum id="0">=46iltering</definiendum>
				<definiens id="0">a feature that is naturally derived flom the conversion of the structure</definiens>
			</definition>
</paper>

		<paper id="1087">
			<definition id="0">
				<sentence>In general , automatic indexing consists of the identification of index terms and the assignment of weights to the terms ( Salton 1983 ) .</sentence>
				<definiendum id="0">automatic indexing</definiendum>
			</definition>
			<definition id="1">
				<sentence>Simpler methods segment componnd nouns mechanically into unigram or bigram words that are all regarded as index terms ( Lee 1996 ) .</sentence>
				<definiendum id="0">Simpler</definiendum>
				<definiens id="0">methods segment componnd nouns mechanically into unigram or bigram words that are all regarded as index terms</definiens>
			</definition>
			<definition id="2">
				<sentence>'Fhe function words are ( : lassified into 32 groups according to their roles and position in sentences .</sentence>
				<definiendum id="0">'Fhe function words</definiendum>
				<definiens id="0">lassified into 32 groups according to their roles and position in sentences</definiens>
			</definition>
			<definition id="3">
				<sentence>, :q ( '/ ; , : ) j ) : ) 5 ) -= ) _Zk Dk ) For the case o\ [ ' multii ) le terms : E - : i 'l'he similarity t ) etween two Lel'lTis ( or sets o\ [ `` terms ) can be defined as any of vector similarity liieaslires .</sentence>
				<definiendum id="0">:q</definiendum>
				<definiens id="0">( '/ ; , : ) j ) : ) 5 ) -= ) _Zk Dk ) For the case o\ [ ' multii ) le terms : E - : i 'l'he similarity t ) etween two Lel'lTis ( or sets o\ [ `` terms</definiens>
			</definition>
			<definition id="4">
				<sentence>i'l.h doellllTeill , , ~*.7 is the \ ] llTiiii ) er o\ [ `` oc ( 'llrreTl ( x ; s ( if ' l , he i'l , h l , erln hi l , he j'th ( toeuliTenl , , and dfi is the liliTiibcr of dOCTITIielTI , S hi which the i'i , li l , ernT OCCTlrD 4 l , \ ] xperiments 'l'hc goal o\ [ experilu &lt; its is to vali &amp; ~t , e the pro posed algoril , hnl for a.na.lyzing compo.ud nou.s by co|np ; u'ing il , with the mmmal a.nalysis and l , he bigranl lnel , hod. The l , esl , dal , a set consists of 1000 science a l ) stra ( : ~ , s writl.en in Kore~ul ( Kitu 1.99d ) . All nomi Nals nix &gt; manually \ [ aleut\ [ fled and eoinpoulid ltoillis were deconq ) oscd into ~q ) proprinte simple nouns by &amp; t\ [ expert in ( lexcr .</sentence>
				<definiendum id="0">dfi</definiendum>
				<definiens id="0">the \ ] llTiiii ) er o\ [ `` oc ( 'llrreTl ( x ; s ( if ' l , he i'l , h l , erln hi l</definiens>
				<definiens id="1">the liliTiibcr of dOCTITIielTI , S hi which the i'i , li l , ernT OCCTlrD 4 l , \ ] xperiments 'l'hc goal o\ [ experilu &lt; its is to vali &amp; ~t , e the pro posed algoril , hnl for a.na.lyzing compo.ud nou.s by co|np ; u'ing il , with the mmmal a.nalysis</definiens>
			</definition>
</paper>

		<paper id="2209">
			<definition id="0">
				<sentence>Medical patient reports consist mainly of free text , combined with results of various laboratories .</sentence>
				<definiendum id="0">Medical patient reports</definiendum>
				<definiens id="0">consist mainly of free text , combined with results of various laboratories</definiens>
			</definition>
			<definition id="1">
				<sentence>The extensive tagset ( tagsetl ) provides all the morphosyntaetic information as required by the DMLP parser for sentence analysis , while the reduced tagset ( tagset2 ) consists of 15 ( 'ategories and 25 speciliers ( which gives 43 meaningfifl combinations ) .</sentence>
				<definiendum id="0">extensive tagset ( tagsetl )</definiendum>
				<definiens id="0">provides all the morphosyntaetic information as required by the DMLP parser for sentence analysis , while the reduced tagset ( tagset2 ) consists of 15 ( 'ategories and 25 speciliers ( which gives 43 meaningfifl combinations )</definiens>
			</definition>
</paper>

		<paper id="2148">
			<definition id="0">
				<sentence>Relaxation is a well-known technique used to solve consistent labelling problems .</sentence>
				<definiendum id="0">Relaxation</definiendum>
				<definiens id="0">a well-known technique used to solve consistent labelling problems</definiens>
			</definition>
			<definition id="1">
				<sentence>Relaxation labelling is a generic name for a family of iterative algorittuns which perform function optimization , based ( m local infi~rmation .</sentence>
				<definiendum id="0">Relaxation labelling</definiendum>
				<definiens id="0">a generic name for a family of iterative algorittuns which perform function optimization , based ( m local infi~rmation</definiens>
			</definition>
			<definition id="2">
				<sentence>elaxation does n't need -as HMMs ( h ) the prior prot ) at ) ility of a certain tag for a word , since it is not a constraint , but il ; Call \ ] ) e llSCd to SOt ; the initial st ; at ( ; to a 11ot templet ; ely rall ( lol\ [ I OllC .</sentence>
				<definiendum id="0">-as HMMs</definiendum>
				<definiens id="0">a constraint , but il ; Call \ ] ) e llSCd to SOt ; the initial st</definiens>
			</definition>
			<definition id="3">
				<sentence>Since that fllnt : tions are general , we may try to lind ; ~ suI ) I ) ort flmctkm more speciiic tbr our t ) rol ) h : m. Sin ( : e IIMMs lind the maxinmm sequ ( : n ( : e probat ) ility and relaxation is a maximizing algorii ; hm , we ( : an make relaxation maximize th ( , ' se ( lllenc ( ?</sentence>
				<definiendum id="0">e IIMMs</definiendum>
				<definiendum id="1">relaxation</definiendum>
				<definiendum id="2">se</definiendum>
				<definiens id="0">an make relaxation maximize th ( , '</definiens>
			</definition>
</paper>

		<paper id="2206">
			<definition id="0">
				<sentence>Tile se ( : ond uses the flu ' cooccurrence data sets , which are construtted dynamically fl'om the near cooccurrence data sets in the course of processing input sentences , to select the most feasible word among homonyms ill the s ( : ope of a sequence of sentences .</sentence>
				<definiendum id="0">Tile se</definiendum>
				<definiens id="0">ond uses the flu ' cooccurrence data sets , which are construtted dynamically fl'om the near cooccurrence data sets in the course of processing input sentences , to select the most feasible word among homonyms ill the s ( : ope of a sequence of sentences</definiens>
			</definition>
			<definition id="1">
				<sentence>The first near co-occurrence data set is the set EN ... ... .. each element of which ( n ) is a triplet consisting of a noun , a case marking partMe , and a set of w~rl ) s which co-occur with that noun and l ) artMe pair in a sentence , as follows : n = ( noun , par ticle , { ( Vl , kl ) , ( v2 , ~ ; 2 ) , '' '' } ) Ill this description , particle is a Japanese case marking particle , such as 7 ) '- ' ; ( nominative case ) , ( ac ( : usative case ) , or tC ( dative case ) , vi ( i = 1,2 , .</sentence>
				<definiendum id="0">particle</definiendum>
				<definiendum id="1">tC</definiendum>
				<definiens id="0">a triplet consisting of a noun , a case marking partMe , and a set of w~rl ) s which co-occur with that noun and l ) artMe pair in a sentence</definiens>
				<definiens id="1">a Japanese case marking particle , such as 7 ) '- ' ; ( nominative case ) , ( ac ( : usative case ) , or</definiens>
			</definition>
			<definition id="2">
				<sentence>- ) is a verb , and ki ( i -- -1,2 , ... ) is the frequency of occurren ( : e of the combination noun , particle and vl , which is del ; ermined in the course of constructing EN ... ...</sentence>
				<definiendum id="0">vl</definiendum>
				<definiens id="0">a verb</definiens>
			</definition>
			<definition id="3">
				<sentence>each element of which ( v ) is a triplet consisting of a verb , a case marking partMe , and a set of nouns which co-occur with that verb and particle pair in a sentence , as follows : v = ( verb , particle , { ( nt , ll ) , ( n2,12 ) , '' ' `` } ) In this description , particle is a Japanese case marking particle , ni ( i = 1,2 , ... ) is a noun , and li ( i : 1,2 , '' `` ) is the frequency of occurrence of the combination verb , particle and hi .</sentence>
				<definiendum id="0">particle</definiendum>
				<definiendum id="1">`` )</definiendum>
				<definiens id="0">a triplet consisting of a verb , a case marking partMe , and a set of nouns which co-occur with that verb and particle pair in a sentence , as follows : v = ( verb , particle , { ( nt , ll</definiens>
				<definiens id="1">a noun , and li ( i : 1,2 , ''</definiens>
				<definiens id="2">the frequency of occurrence of the combination verb , particle and hi</definiens>
			</definition>
			<definition id="4">
				<sentence>p , , , , v where Ni ( i = 1,2 , ... , m ) is a noun , l'i ( i = 1,2 , ... , m ) is a particle and V is a verb .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">a noun</definiens>
				<definiens id="1">a particle</definiens>
				<definiens id="2">a verb</definiens>
			</definition>
			<definition id="5">
				<sentence>k ) 6n ( v , , v.2 ... ..vm ) The sequence of words WI , W2 , ... , W , ~ is the most feasible conibination of homonymic ka'aji-w , riants for tile ini ) ut kana-string .</sentence>
				<definiendum id="0">~</definiendum>
				<definiens id="0">the most feasible conibination of homonymic ka'aji-w , riants for tile ini</definiens>
			</definition>
			<definition id="6">
				<sentence>, ' , , , ) , ( '~ , ' , ,~ ) , ... , ( &lt; , 'a , ~ ) } where ni ( i = 1,2 , ... , 1 , , ) is a noun , t i is the 1 ) riority wdue of hi , vi ( i = 1 , 2 , ... , Iv ) is ~t verb and ui is the priority wdue of vi .</sentence>
				<definiendum id="0">ni</definiendum>
				<definiendum id="1">ui</definiendum>
				<definiens id="0">a noun</definiens>
				<definiens id="1">~t verb</definiens>
			</definition>
			<definition id="7">
				<sentence>Find all ( nj , lj ) ( j = 1 , 2 , ... , q ) which co-occur with the verb v , and any particle in the near co-occurrence data set P~v..</sentence>
				<definiendum id="0">Find all</definiendum>
			</definition>
			<definition id="8">
				<sentence>Stepl Find set Sn : S , = { ( N , ,Tt ) , ( N2 , T2 ) , ... } where Ni ( i : 1 , 2 , ... ) is a homonynfic l~a'nfivariant for the input word written in kanacharacters and T , is the priority vMue for homonym N , , which can be retrieved from the Jar co-occurrence data set ENid , .</sentence>
				<definiendum id="0">Ni</definiendum>
				<definiens id="0">a homonynfic l~a'nfivariant for the input word written in kanacharacters</definiens>
				<definiens id="1">the Jar co-occurrence data set ENid</definiens>
			</definition>
			<definition id="9">
				<sentence>Step1 Find set S , : sv = Here , Vj ( j = 1,2 , ... ) is a homo , tymic kanjivariant for the input word written in kanacharacters and Uj is the priority value for homonym Vj , which can be retrieved from the far co-occurrence data set Evj ... . Step2 The verb Iv ) which has the greatest Uj priority wflue in S , is the most feasible verb for the input word written in kana-characters .</sentence>
				<definiendum id="0">Uj</definiendum>
				<definiens id="0">the priority value for homonym Vj , which can be retrieved from the far co-occurrence data set Evj ...</definiens>
			</definition>
</paper>

		<paper id="1070">
			<definition id="0">
				<sentence>article , noun , verb , preposition , proper-noun , preposition , numeral , postnominal A constituent boundary pattern is defined as a sequence that ; consists of variables and symbols representing constituent boundaries .</sentence>
				<definiendum id="0">constituent boundary pattern</definiendum>
				<definiens id="0">a sequence that ; consists of variables and symbols representing constituent boundaries</definiens>
			</definition>
			<definition id="1">
				<sentence>A solid line denotes a passive arc that covers a substring of the input below , while a dotted line denotes an active arc .</sentence>
				<definiendum id="0">solid line</definiendum>
				<definiens id="0">a passive arc that covers a substring of the input below , while a dotted line denotes an active arc</definiens>
			</definition>
</paper>

		<paper id="2170">
</paper>

		<paper id="2122">
			<definition id="0">
				<sentence>Dependency grammars describe the structure of a sentence in terms of binary head-modifier ( also called dependency ) relations on the words of the sentence .</sentence>
				<definiendum id="0">Dependency grammars</definiendum>
				<definiens id="0">describe the structure of a sentence in terms of binary head-modifier ( also called dependency ) relations on the words of the sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>A dependency relation is an asymmetric relation between a word callexl head ( governor , parent ) , and a word called modifier ( dependent , daughter ) .</sentence>
				<definiendum id="0">dependency relation</definiendum>
				<definiens id="0">an asymmetric relation between a word callexl head ( governor , parent ) , and a word called modifier ( dependent , daughter )</definiens>
			</definition>
			<definition id="2">
				<sentence>The mainstream of formalisms consists ahnost exclusively of constituency approaches , but some of the original insights of the dependency tradition have found a role in the constituency formalisms : in particular , the concept of head of a phrase and the use of grammatical relations .</sentence>
				<definiendum id="0">mainstream of formalisms</definiendum>
			</definition>
			<definition id="3">
				<sentence>The identification of the head within a phrase has been a major point of all the recent frameworks in linguistics : the X-bar theory ( Jackendoff 1977 ) , defines phrases as projections of ( pre ) terminal symbols , i.e. word categories ; in GPSG ( Gazdar et al , 1985 ) and HPSG ( Pollard , Sag 1987 ) , each phrase structure rule identifies a head and a related subcategorization within its right-hand side ; in HG ( Pollard 1984 ) the head is involved in the so-called head-wrapping operations , which allow the formalism to go beyond the context-free power ( Joshi et al. 1991 ) .</sentence>
				<definiendum id="0">X-bar theory</definiendum>
				<definiendum id="1">phrase structure rule</definiendum>
			</definition>
			<definition id="4">
				<sentence>A dependency grammar is a quintuple &lt; S , C , W , L , T &gt; , where W is a finite set of symbols ( vocabulary of words of a natural language ) , C is a set of syntactic categories ( preterminals , in constituency terms ) , S is a non-empty set of root categories ( C _ S ) , L is a set of category assignment rules of the form X : x , where XCC , x @ W , and X YI Y2 ... Yi-1 Yi+l ... Ym Figure 2 A dependency rule : X is the governor , and Y1 ... .. Ym are the dependent of X in the given order ( X is in # position ) .</sentence>
				<definiendum id="0">dependency grammar</definiendum>
				<definiendum id="1">W</definiendum>
				<definiendum id="2">C</definiendum>
				<definiendum id="3">S</definiendum>
				<definiendum id="4">L</definiendum>
				<definiendum id="5">dependency rule : X</definiendum>
				<definiendum id="6">X</definiendum>
				<definiens id="0">a finite set of symbols ( vocabulary of words of a natural language ) ,</definiens>
				<definiens id="1">a set of syntactic categories ( preterminals , in constituency terms</definiens>
				<definiens id="2">a non-empty set of root categories ( C _ S</definiens>
			</definition>
			<definition id="5">
				<sentence>A dependency tree of x is a tree such that : 1 ) the nodes are the symbols ai~W ( l &lt; i &lt; p ) ; 2 ) a node ak , j has left daughters ak,1 ... .. ak , j-1 occurring in this order and right daughters ak , j+l , ... . ak , q in this order if and only if there exist the roles Ak , l : ak,1 ... .. Akj : akj ... .. Ak , q : ak , q in L and the rule Ak , j ( Ak,1 ... Akj-I # Akj+l ... Ak , q ) in T. We say that ak,1 ... .. akj-1 , ak , j+l ... .. ak , q directly depend on ak , j , or equivalently that ak , j directly governs ak , 1 ... .. ak , j.1 , akj+l ... ... ak , q. akj and ak , h ( h = 1 ... .. j-l , j+l ... .. q ) are said to be in a dependency relation , where ak , j is the head and ak , h is the modifier , if there exists a sequence of nodes ai , ai+l ... .. aj-l , aj such that ak directly depends on ak-1 for each k such that i+l~k-~j , then we say that ai depends ' on aj ; 3 ) it satisfies the condition ofprojectivity with respect to the order in x , that is , if ai depends directly on aj and ak intervenes between them ( i &lt; k &lt; j or j &lt; k &lt; i ) , then either ak depends on a i or ak depends on aj ( see fig. 3 ) ; 4 ) the root is a unique symbol as such that As : as E L and As~S. The condition of projectivity limits the expressive power of the formalism to be equivalent to the context-free power. Intuitively , this principle states aj ai Aik i ' ak i | i | | i i i i aj ai Figure 3. The condition of projectivity. respect to Gaifman : however , it is not uncommon to allow the symbols on the right hand side of a rule to be regular expressions in order to augment the perspicuity of the syntactic representation , but not the expressive power of the grammar ( a similar extension appears in the context-free part of the LFG formalism ( Kaplan , Bresnan 1982 ) ) . 724 that a dependent is never separated from its governor by anything other than another dependent , together with its subtree , or by a dependent of its own. As an example , consider the grammar GI= &lt; iV } , iV , N , P , A , D } , { I , saw , a , tall , old , man , in , the , park , with , telescope } , { N : I , V : saw , D : a , A : tall , A : old , N : man , P : in , D : the , N : park , P : with , N : telescope } TI &gt; , where T1 is the following set of dependency rules : t. V ( N # P* ) ; 2 .</sentence>
				<definiendum id="0">D</definiendum>
				<definiendum id="1">T1</definiendum>
				<definiens id="0">j+l ... .. q ) are said to be in a dependency relation</definiens>
				<definiens id="1">uncommon to allow the symbols on the right hand side of a rule to be regular expressions in order to augment the perspicuity of the syntactic representation , but not the expressive power of the grammar ( a similar extension appears in the context-free part of the LFG formalism</definiens>
			</definition>
			<definition id="6">
				<sentence>3,1 Transition graphs and parse tables A transition graph is a pair ( V , E ) , where V is a set of vertices called states , and E is a set of directed edges labelled with a syntactic category or the symbol # .</sentence>
				<definiendum id="0">transition graph</definiendum>
				<definiendum id="1">V</definiendum>
				<definiendum id="2">E</definiendum>
				<definiens id="0">a pair ( V , E ) , where</definiens>
				<definiens id="1">a set of vertices called states</definiens>
				<definiens id="2">a set of directed edges labelled with a syntactic category or the symbol #</definiens>
			</definition>
			<definition id="7">
				<sentence>13 '' , where Cat ( c~13 ) C T # * and et , \ [ ~ E ( C U { } ) ; an edge is a triple &lt; si , sj , Y &gt; , where si , sj C V and Y G C U { # } .</sentence>
				<definiendum id="0">Cat</definiendum>
				<definiendum id="1">edge</definiendum>
				<definiens id="0">a triple &lt; si , sj</definiens>
			</definition>
			<definition id="8">
				<sentence>star ( dotted-string ) : set-of strings : = { dotted-string } ; rsW~a~t take a non-marked dotted string ds from set-of-strings ; mark ds ; if ds has the form `` .</sentence>
				<definiendum id="0">star ( dotted-string )</definiendum>
				<definiens id="0">set-of strings : = { dotted-string } ; rsW~a~t take a non-marked dotted string ds from set-of-strings</definiens>
			</definition>
			<definition id="9">
				<sentence>The total number of states of all the transition graphs for a grammar G is at most O ( IGI ) , where IGI is the sum of the lengths of the dependency rules .</sentence>
				<definiendum id="0">IGI</definiendum>
				<definiens id="0">the sum of the lengths of the dependency rules</definiens>
			</definition>
			<definition id="10">
				<sentence>The length of a dependency rule Cat ( c0 is the length of ( ~ .</sentence>
				<definiendum id="0">length of a dependency rule Cat</definiendum>
			</definition>
			<definition id="11">
				<sentence>The first of the category Cat is the set of categories that appear as leftmost node of a subtree headed by Cat .</sentence>
				<definiendum id="0">Cat</definiendum>
				<definiens id="0">the set of categories that appear as leftmost node of a subtree headed by Cat</definiens>
			</definition>
			<definition id="12">
				<sentence>E ( t-graphcat ) returns the set of the edges of the graph t-graphca t. The contents of the entries in the parse tables are sets ( possibly empty ) of predict and scan .</sentence>
				<definiendum id="0">E ( t-graphcat )</definiendum>
				<definiens id="0">returns the set of the edges of the graph t-graphca t. The contents of the entries in the parse tables are sets ( possibly empty ) of predict and scan</definiens>
			</definition>
			<definition id="13">
				<sentence>The initialization step consists in setting all entries of the table to the empty set .</sentence>
				<definiendum id="0">initialization step</definiendum>
				<definiens id="0">consists in setting all entries of the table to the empty set</definiens>
			</definition>
</paper>

		<paper id="2120">
			<definition id="0">
				<sentence>ting N\ [ , P systems is 1 ) crfornmd ti ) r a. variety of \ ] mrl ) oscs , t , h ( ' , TSNI , I &gt; a.l ) l ) roach is also gui &lt; l ( ; d l ) y I ; h ( ' , n ( ; ed to l ) rovide test mat ( ! rial which is easily .l ' ( '/tts ( l , Dl ( ' , . rib achiove th ( ; se two goMs of Sl ) ( ~cili ( : ity and reusalfility , the tra.ditional notion of a. l , est suite as a monoliflfi ( : set : of test it ; olns has l ) ( ~ ( ! n M ) andoned in fnvour of the notion of a ( tal ; al ) as ( ~ in which test iLelliS ; tl ' ( ~ sl ; ored l : ogether with a , rich invenl ; ory of asso ( : iated liuguisti ( : mM n ( m-linguist ; i ( : ~lllllO { , &amp; i ; iOllS. Thus , I ; h ( ; l ; ( : sl , Sllig ( ~ da.l. ; I.l ) ~tsc sorves a.s ; t virtunl ( or met ; ; @ tx ; st ; suil ; e thai ; t ) rovitlt~s the metals i ; ( ) ex1 ; 1 '' ~-1 , ( % ( ; 11 ( } l ' ( ' , l ( ; va , ltt sill ) sial ; Of th0 tcst ; ( / ; 1 , ( ; } ~. suital ) le for s ( ) nw Sl ) ( ~ ( : iti ( : tn.sk. Using tim e , xl ) liciL s { ; 171tl : .. 7 ll ture of the data and the TSNLP annotations , the database engine allows searching and retrieving data from the virtual test suite , thereby creating a concrete test suite instance according to arbitrary linguistic and extra-linguistic constraints. Since , additionally , there are tools provided for the maintenance and extension of the test suite database , the TSNLP virtual test suite approach is an essentim innovation leading the way to a new generation of highly-structured reusable test suites. Based on a survey of existing test suites and an analysis of the diagnostic and evaluation requirements of both NL technology developers and users , TSNLP has developed the methodology for the construction of core test data , that is , test items reflecting central language phenomena and that are applicable to a wide range of applications , including parsers , grammar checkers , and controlled language checkers ( Balkan et al. ( 1996 ) ) . The TSNLP methodology is designed to optimize ( i ) control over ' test data , ( ii ) progressivity , and ( iii ) systematicity. These are necessary qualities for an adequate , reusable test suite , which are difficult to find in test corpora. The methodology also addresses the specific goals of TSNLP to produce multi-purpose , multi-user , and multilingual test suites. Control over test data What makes test suites valuable in comparison to corpora is that they can focus on specific linguistic phenomena and that each phenomenon can be presented both in isolation and controlled combinations in which as many linguistic parameters as possible are being kept under control. This is particularly the case when a phenomenon is illustrated by systematic variation over the parameters used to describe this phenomenon , while all other parts of the test items remain constant. Vocabulary is an aspect of the test data that needs to be controlled. TSNI , P achieves this by restricting the vocabulary in size as well as in domain. Categorially and semantically ambiguous words are avoided where possible and only included when ambiguity is explicitly tested for. Additionally , TSNLP attempts to control the interaction of phenomena by keeping the test items as small as possible. Therefore , a number of guidelines for this purpose ( such as use declarative sentences and avoid modifiers and adjuncts ) is provided. Progressivity Progressivity is the principle of starting h'om simple test items and increasing their complexity. In TSNLP , this aspect is addressed by requiring that each test item focuses only on a single t ) henomenon ( or rather subphenomenon or even feature ) which distinguishes it from all other test items. This principle not only ensures systernaticity during the test data ( : onstruction but also allows test data users to apply the test data in a progressive order obtained from the special attribute presupposition in the phenomena classification. Thus , the precise identification of the coverage of a system and of its defteieneies is rendered easier. Systematicity Systematicity refers to the depth of coverage of a test suite , with respect to both well-formed and ill-formed items. Systematicity in TSNLP is achieved for well-formed items by the explicit classification of test items according to phenomena and sub-phenomena. Negative test data permits testing for overgeneration as well as for coverage , ill-formed items are derived from well-formed ones by systematic variation of the parameters through the application of one ( or more ) of four operations , namely : • REPLACEMENT ( e.g. change of person ) ( l ) h'ench ) L ' ingdnieur vient. ( lq'ench ) *L ' ingdnieur viens. • am ) rrION ( e.g. of an object NP ) ( German ) Dcr Managcr arbeitet. ( German ) *Dcr Manager arbeitet den Vortrag. • DELETION ( e.g. of an obligatory complement ) ( German ) Der Mana.qer hiilt den Vortrag. ( German ) *Der Manager hiilt. • PERMUTATION ( e.g. inverting word order ) ( English ) He saw the boy. ( English ) *He the boy saw. In general , tile systematicity of test data was greatly enhanced through the use of specialpurpose tools in the data construction and validation process ( see section 5 below ) . Multillnguallty Multilinguality is achieved in the TSNLP test suites by covering the same range of phenomena in English , French and German , and adopting the same classification for these phenomena in the three languages. Furthermore , the choice of related terminology for the categolial and structural description contributes to I ; he comparability and consistency of the test items ( see section 4 for details ) . Documentation To enhance the usability and extensibility of TSNI , P results , a three-vohnne user guide is under preparation providing clear instructions for the assessment of the methodology , test data , and tools developed. A detailed annotation schema was designed tbr the test data which does not , presuppose a specific linguistic theory , a particular evaluation situation or application type. Test data and am~otations in TSNI , P test suites are organized at four distinct representational levels : 712 • ( ? ore Data The ( : ( ) re of the test data c &lt; &gt; nsists of the individual test items together with all ge , neral , categorial and structural inforlnation that is indepen { lent of a token phenomenon or application .</sentence>
				<definiendum id="0">P systems</definiendum>
				<definiens id="0">ture of the data and the TSNLP annotations , the database engine allows searching and retrieving data from the virtual test suite , thereby creating a concrete test suite instance according to arbitrary linguistic and extra-linguistic constraints. Since , additionally , there are tools provided for the maintenance and extension of the test suite database , the TSNLP virtual test suite approach is an essentim innovation leading the way to a new generation of highly-structured reusable test suites. Based on a survey of existing test suites and an analysis of the diagnostic and evaluation requirements of both NL technology developers and users , TSNLP has developed the methodology for the construction of core test data , that is , test items reflecting central language phenomena and that are applicable to a wide range of applications , including parsers , grammar checkers , and controlled language checkers</definiens>
				<definiens id="1">an aspect of the test data that needs to be controlled. TSNI , P achieves this by restricting the vocabulary in size as well as in domain. Categorially and semantically ambiguous words are avoided where possible</definiens>
				<definiens id="2">the principle of starting h'om simple test items and increasing</definiens>
				<definiens id="3">or rather subphenomenon or even feature ) which distinguishes it from all other test items. This principle not only ensures systernaticity during the test data ( : onstruction but also allows test data users to apply the test data in a progressive order obtained from the special attribute presupposition in the phenomena classification. Thus , the precise identification of the coverage of a system and of its defteieneies is rendered easier. Systematicity Systematicity refers to the depth of coverage of a test suite , with respect to both well-formed and ill-formed items. Systematicity in TSNLP is achieved for well-formed items by the explicit classification of test items according to phenomena and sub-phenomena. Negative test data permits testing for overgeneration as well as for coverage , ill-formed items are derived from well-formed ones by systematic variation of the parameters through the application of one ( or more ) of four operations</definiens>
				<definiens id="4">preparation providing clear instructions for the assessment of the methodology , test data , and tools developed. A detailed annotation schema was designed tbr the test data which does not , presuppose a specific linguistic theory , a particular evaluation situation or application type. Test data and am~otations in TSNI , P test suites are organized at four distinct representational levels : 712 • ( ? ore Data The ( : ( ) re of the test data c &lt; &gt; nsists of the individual test items together with all ge , neral , categorial and structural inforlnation that is indepen { lent of a token phenomenon or application</definiens>
			</definition>
			<definition id="1">
				<sentence>• Phenomenon-Related Data Based on a hi= erarchical classification of linguistic ( an &lt; l extra= linguistic ) phenome+ , a ( e.g. verb wdency as a subtype of general complementation ) , each phenoiuenon ix identitied by a phenomenon id and by its supertype , ( s ) . \ ] interaction with other phenomena as well as the l ) henom &lt; ma which must be presuplmse &lt; l are also given , in addition , the ( syntactic ) parameters which are relevant for the phenomenon ( e.g. the munber an &lt; l tyt ) e of con &gt; plements in the case of verb valency ) are describe &lt; t. Individual test items can be assigned to one or several phenoluena and annotated ge &lt; : ording to the eorresl ) ondii~g parameters. • Test Sets 'lPest items emt optkmally be groul &gt; ed into test sets .</sentence>
				<definiendum id="0">Individual test items</definiendum>
				<definiens id="0">ondii~g parameters. • Test Sets 'lPest items emt optkmally be groul &gt; ed into test sets</definiens>
			</definition>
			<definition id="2">
				<sentence>In ad &lt; lition to l ; he parts of the annotation s ( -henta that follow a formal speeifi &lt; ; ation , there is room for textual conmmnts at the wn'ious levels to accommodate informatioi~ that ( : annot or need not be forlnalized. \ [ Test Item -\ ] item id : 2/~0~20101 author : issco date : jan-95 \ ] register : formal format : n.onc ' origin : inve~ , tcd\ [ difficulty : l wellformedness : / category : 5 ' \ [ input : L ' i'n , g &amp; ~ , icur vie'at . length : 3 I comment : / ~ ion instance category fimction domain 2-I , '' in~h~ , ie.ur ~ &lt; ~ : -.~ , nb : j 2.'3 : ,'_ yic ' , t\ [ ... .. V : . : '~-. ; , j _ J~ , ~ , ! _ __0.. : ¢ \ [ i-_ Phelaomelton phenomenon id : +2/t02 author : issco elate : jan-.95 name : ( /__Co'n~phmw , 'nt~*tion_s'ubj ( Nl ' ) _ V I st , pertypes : ( /_ ( ; o'mplc.'mcv.tal , ion J presupposition : ( 7 : Agreement , NILAqrcemc'n , t j restrictions : 7~e'ulral interaction : none purpose : to.st\ [ comment : l ' , .transitive `` oc.d~ ( va.le'nc~j. '' l ) I Figure \ [ : Sample inst ; ance of the TSNI , P mmotation schema \ [ br one test item : the ; umotations are giwm in I ; abular form for the test itc 'm , analysis , and phe71 , omeno'n levels. l ; &lt; ) llowing the TSNI , P test suite guidelines ( Estiwfl et al. ( 1994 ) ) and using the annotation schema sketched above , the eonstru ( : tion of test data was based on a classitication of the ( synl ; ac= tic ) phenomena to bc &lt; : overed. \ [ , i'om judgements on the linguistic relevance and frequency for dm individual languages , the following list ; &lt; ) f ( : ore pheo , n , omcna for T , qNIA ' was compiled : • coinl ) lententation ; • agreelllenl ; ; • modification ; • &lt; liathesis ; • modality , teltse , and asl ) ect ; • Selltence and clause tyt ) es ; • word order ; ® coordination ; • negation ; and • extragrammatical ( e.g. parenthetieals and ternporal expressions ) . A fin'ther sul ) -elassifieation of phenomena is made according to the relevanl ~ynl , actie domains in which a I &gt; henonmnon occurs ( e.g. sentences ( S ) , clauses ( C ) , n &lt; mn 1 ) hrases ( NP ) et al. ) . Fignre 2 giw ; s an overview of the test material awfilable. For ea &lt; : h of the three languages some 5000 l , esl ; items are l ) rovided. Theret.'ore , TSNI , I ' has already achieved a substantially broader and deeper &lt; : overage than previous general-purpose test suites ( the still very popular Hewlett-Paekard tes~ suite , for instance , has a ( ; overage of 3000 test items for English only ) . In order to enforce consistency of annotations across the three languages , canonical lists of the categories and fimctions used in the &lt; leserit ) tion of categorial and de4 &gt; endency structure were estal &gt; lished ( see Ix'\ ] mlann et al. ( 1996 ) ) .</sentence>
				<definiendum id="0">fin'ther sul</definiendum>
				<definiens id="0">the relevanl ~ynl , actie domains in which a I &gt; henonmnon occurs ( e.g. sentences ( S ) , clauses ( C ) , n &lt; mn 1 ) hrases ( NP ) et al. ) . Fignre 2 giw ; s an overview of the test material awfilable. For ea &lt; : h of the three languages some 5000 l , esl ; items are l ) rovided. Theret.'ore , TSNI , I ' has already achieved a substantially broader and deeper &lt; : overage than previous general-purpose test suites ( the still very popular Hewlett-Paekard tes~ suite , for instance , has a ( ; overage of 3000 test items for English only ) . In order to enforce consistency of annotations across the three languages , canonical lists of the categories and fimctions used in the &lt; leserit ) tion of categorial</definiens>
			</definition>
			<definition id="3">
				<sentence>Because { ; he test data construction proper as well as the custornization and application of a generalpurpose test suite to a specific NLP system or domain are laborious , cost-intensive and error-prone tasks , TSNLP put strong emphasis on supplying suitable special-purpose tools to fitcilitate both the development as well as usage of the TSNIA ' test data ( Oepen et al. ( 1996a ) give an overview ) .</sentence>
				<definiendum id="0">construction proper</definiendum>
				<definiens id="0">the custornization and application of a generalpurpose test suite to a specific NLP system or domain are laborious , cost-intensive and error-prone tasks , TSNLP put strong emphasis on supplying suitable special-purpose tools to fitcilitate both the development as well as usage of the TSNIA ' test data</definiens>
			</definition>
			<definition id="4">
				<sentence>The tool instant , iates the annotation schema ( see section 3 ) as a feral-based input mask and provides for ( limited ) consistency checking of the field values .</sentence>
				<definiendum id="0">tool instant</definiendum>
				<definiens id="0">iates the annotation schema ( see section 3 ) as a feral-based input mask and provides for ( limited ) consistency checking of the field values</definiens>
			</definition>
			<definition id="5">
				<sentence>: The tsdb 1 inll ) leanelfl ; ation is a small and etlicient relational database engine in ANSI C. 11 ; was designed with an open and dot : unrented interface layer ( see figure 3 ) that enalfles test suite users to 1 ) idirectiona .</sentence>
				<definiendum id="0">ation</definiendum>
				<definiens id="0">a small and etlicient relational database engine in ANSI C. 11 ; was designed with an open and dot</definiens>
			</definition>
			<definition id="6">
				<sentence>nd shell interprets a simplitied SQL-stylc query language and provides editing , completion , and command and query result history .</sentence>
				<definiendum id="0">nd shell</definiendum>
				<definiens id="0">interprets a simplitied SQL-stylc query language and provides editing , completion , and command and query result history</definiens>
			</definition>
			<definition id="7">
				<sentence>Tit ( ; database provides ( ; oitv ( +Jliotl| ; graphical browsit|g and editing of tit ( ; data ( using lmll~ down menus fbr tinit ; e ( hmtain fields ; s ( + , o \ [ igure 4 ) as well as standard import and export fa .</sentence>
				<definiendum id="0">Tit</definiendum>
				<definiens id="0">graphical browsit|g and editing of tit ( ; data ( using lmll~ down menus fbr tinit ; e ( hmtain fields ; s ( + , o \ [ igure 4 ) as well as standard import and export fa</definiens>
			</definition>
			<definition id="8">
				<sentence>a Query and Retrieval : An Example '15 ilh|strat ; c the capacity and flcxil ) ility of i ; hc TSNLP annotation schema in ctntjunction with a relational database retri ( wal ( ; ngin ( : , a query examplc in the ' simplified SQL-likc query language interpreted by tsdb I together wit ; h mt informal English paral ) hrasc , in giwm : a • lind all grammatical test items that are associat-ed wil : h Lh ( ; l ) henonmnon of ( : lat|sal ( i.e. sul ) jcct verb ) ag ; roo , lllt+Jl| ; and have l ) ronominal st|l~jecgs : se/ , ect i-id i-input where i-wf : -I g p-name - '' C AgreemenC '' a-lunch±on - : `` subj `` &amp; a-category , -o `` ^PRON '' To validaLe t , hc 'I'SNIA ' &amp; llllOl ; a\ [ ; i ( ) ll mol ; hodology , t ; esl ; data , and tools , the lnOj ( ~t ' .</sentence>
				<definiendum id="0">lnOj</definiendum>
				<definiens id="0">An Example '15 ilh|strat ; c the capacity and flcxil ) ility of i ; hc TSNLP annotation schema in ctntjunction with a relational database retri</definiens>
				<definiens id="1">a • lind all grammatical test items that are associat-ed wil : h Lh</definiens>
			</definition>
</paper>

		<paper id="2124">
			<definition id="0">
				<sentence>In this paper , we show that software documentation is an attractive application for multilingual text generation because it is an area in which pre-built knowledge bases are beconfing available .</sentence>
				<definiendum id="0">software documentation</definiendum>
			</definition>
			<definition id="1">
				<sentence>For examl ) le , the main us ( ; r goat of saving a do ( ; u.meat , represented in the figure by the action node `` Save ; L Document '' , is implemented in the knowledge base as a comple , x of instances repres ( mting the act ; ion being tmrformed ( in this case saving ) , tim agent who performs action ( the reader ) , the t ) atient on whom the aetioll is performed ( the current doeunmnt ) , etc .</sentence>
				<definiendum id="0">ion being tmrformed</definiendum>
			</definition>
			<definition id="2">
				<sentence>t , ion nodt : s at ) pearing in l ; hc Sl ; l'tlt ; l ; llrc all ( \ [ Ho\ [ ; yel ; &lt; t &lt; : rivcd fl'OlIl 1 ; 11 ( : inl ; erfact : designed tool ; and ( 2 ) linking existing nodes ( , ogt : l ; ht : r wit , h ( ; he al ) propria. ( ; ( ; plan insi ; anc ( ; s and relal , ions. The. tirs ( ; of ( ; hese ( ; asks is lmrfornmd using a. &lt; : ontrolle &lt; t nalalra.1 bm guage inl ; erfa &lt; &gt; wlfile the s &lt; ' .</sentence>
				<definiendum id="0">hc Sl</definiendum>
				<definiendum id="1">ht</definiendum>
				<definiens id="0">s at ) pearing in l ;</definiens>
			</definition>
			<definition id="3">
				<sentence>The Knowledge Grapher prevents tile author from losing orientation by maintaining the current state of the procedural structure in graphical form .</sentence>
				<definiendum id="0">Knowledge Grapher</definiendum>
				<definiens id="0">prevents tile author from losing orientation by maintaining the current</definiens>
			</definition>
			<definition id="4">
				<sentence>AF'rl~t calls the Text Planner with the discourse goal : make the user colnpetent to perform tile action specified by the author .</sentence>
				<definiendum id="0">AF'rl~t</definiendum>
				<definiens id="0">calls the Text Planner with the discourse goal : make the user colnpetent to perform tile action specified by the author</definiens>
			</definition>
			<definition id="5">
				<sentence>The Text Planner selects the content appropriate for the instructions and builds a deep representation of the text to be generated .</sentence>
				<definiendum id="0">Text Planner</definiendum>
			</definition>
</paper>

		<paper id="1082">
			<definition id="0">
				<sentence>The German joint research project Verbmobil ( VM ) aims at the deveh ) pment of a speech to speech translation system .</sentence>
				<definiendum id="0">VM )</definiendum>
				<definiens id="0">aims at the deveh ) pment of a speech to speech translation system</definiens>
			</definition>
			<definition id="1">
				<sentence>The LRI parsing algorithm is a modified active chart parser with an agenda driven control mechanism .</sentence>
				<definiendum id="0">LRI parsing algorithm</definiendum>
				<definiens id="0">a modified active chart parser with an agenda driven control mechanism</definiens>
			</definition>
			<definition id="2">
				<sentence>A word hypothesis W is a quadruple ( from , to , key , score ) with J } vm and to being the start and end frames of W. W.Key is the name of the lexical entry of W and W.score is the acoustic score of W for the frames spanned , given by a corresponding HMM acoustic word model .</sentence>
				<definiendum id="0">word hypothesis W</definiendum>
				<definiens id="0">a quadruple ( from , to , key , score ) with J } vm and to being the start</definiens>
				<definiens id="1">the acoustic score of W for the frames spanned , given by a corresponding HMM acoustic word model</definiens>
			</definition>
			<definition id="3">
				<sentence>SCORE is a record with entries for inside and outside probabilities given to an edge by acoustic , bigram , prosody and grammar model : Inside-X Model scores for the spanned portion of an edge .</sentence>
				<definiendum id="0">SCORE</definiendum>
				<definiens id="0">a record with entries for inside and outside probabilities given to an edge by acoustic , bigram , prosody and grammar model : Inside-X Model scores for the spanned portion of an edge</definiens>
			</definition>
			<definition id="4">
				<sentence>Outside-X Optimistic estimates for the portion fi'om vertex 0 to the beginning of an edge .</sentence>
				<definiendum id="0">Outside-X Optimistic</definiendum>
				<definiens id="0">estimates for the portion fi'om vertex 0 to the beginning of an edge</definiens>
			</definition>
			<definition id="5">
				<sentence>hi order to implement our t ) eam search method appropriately but sinq ) ly , we define an operation Agenda-Pu~q~ , which selects pairs of active and passive edges to be prmn ; d or to be processed in the future .</sentence>
				<definiendum id="0">operation Agenda-Pu~q~</definiendum>
				<definiens id="0">selects pairs of active and passive edges to be prmn ; d or to be processed in the future</definiens>
			</definition>
			<definition id="6">
				<sentence>Combined-Score is a linear combination of the outside components of an edge C which would be created by A and E in a Combine operation .</sentence>
				<definiendum id="0">Combined-Score</definiendum>
			</definition>
			<definition id="7">
				<sentence>In INTARC we use three classes of boundaries , B0 ( no boundary ) , B2 ( phrase boundary ) , B3 ( sentence boundary ) and B9 ( real break ) .</sentence>
				<definiendum id="0">B0 (</definiendum>
				<definiens id="0">no boundary ) , B2 ( phrase boundary ) , B3 ( sentence boundary ) and B9 ( real break )</definiens>
			</definition>
			<definition id="8">
				<sentence>A prosody hypothesis consists of a beginning and ending time and model probabilities for the boundary types which sum up to one .</sentence>
				<definiendum id="0">prosody hypothesis</definiendum>
			</definition>
			<definition id="9">
				<sentence>The unpacker , which has exponential complexity , selects only the n best scored packed edges , where n is a parameter .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">has exponential complexity , selects only the n best scored packed edges</definiens>
			</definition>
</paper>

		<paper id="2138">
			<definition id="0">
				<sentence>818 Shape Token A character shape code is a machine-readable code which represents a set of graphically similar characters .</sentence>
				<definiendum id="0">character shape code</definiendum>
				<definiens id="0">a machine-readable code which represents a set of graphically similar characters</definiens>
			</definition>
			<definition id="1">
				<sentence>A word shape token is a sequence of one o1 '' more character shape codes which represents a word .</sentence>
				<definiendum id="0">word shape token</definiendum>
				<definiens id="0">a sequence of one o1 '' more character shape codes which represents a word</definiens>
			</definition>
			<definition id="2">
				<sentence>The document profile D i is represented as a vector of numeric weights , De = ( Wil , Wi2 ... .. Wik ... .. wit ) , where Wik is the weight given the kth word shape token in the ith document , and t is the number of distinct word shape tokens of the ith document .</sentence>
				<definiendum id="0">Wik</definiendum>
				<definiens id="0">the weight given the kth word shape token in the ith document</definiens>
			</definition>
			<definition id="3">
				<sentence>821 Table 3 : categorization accuracy for the word shape token-based and the OCR-based approach ( number of correctly assigned documents / number of test documents ) WST OCR image quality ( nth generation photo-copies ) n=l n=3 n=5 193/200 192/200 154/200 ( 97 % ) ( 96 % ) ( 77 % ) 196/200 196/200 189/200 ( 98 % ) ( 98 % ) ( 95 % ) Table 4 : accuracy of the word shape token-based categorization as a function of the size of test docnmeuts size of test documents ( number of words ) 0 400 400 800 800 n = 1 50/51 ( 98 % ) 84/86 ( 98 % ) 59/63 ( 94 % ) n = 3 51/51 ( 100 % ) 81/86 ( 94 % ) 60/63 ( 95 % ) n = 5 39/51 ( 76 % ) 62/86 ( 72 % ) 53/63 ( 84 % ) Table 5 : accuracy of the OCR-based categorization as a function of the size of test docnments 0 400 400 800 800 n = 1 50/51 ( 98 % ) 85/86 ( 99 % ) 61/63 ( 97 % ) n = 3 50/51 ( 98 % ) 85/86 ( 99 % ) 61/63 ( 97 % ) n = 5 44/51 ( 86 % ) 84/86 ( 98 % ) 61/63 ( 97 % ) From the experimental results in the previous section , our hypothesis that word shape token-based approach is quite adequate for content-oriented categorization was strongly supported at least for the document images from first and third generation photo-copies .</sentence>
				<definiendum id="0">categorization accuracy</definiendum>
				<definiendum id="1">WST OCR image quality</definiendum>
				<definiens id="0">accuracy of the word shape token-based categorization as a function of the size of test docnmeuts size of test documents ( number of words</definiens>
			</definition>
</paper>

		<paper id="2147">
			<definition id="0">
				<sentence>urring ( lis ( : oms ( ~ , q. Fm'therm ( n ' ( ~ , tlm previous CCII| ; CI'ilI~ | ; } l ( ~ory has bm ( lh : d only th ( ' liht : nom ( ma in SIl ( ; CC , qsiv { : silt ( I ) \ ] ( 2 S ( ; Ill ; CltCC , q ; l , lld h ; 4s I\ ] ( ) | ; adequately ad ( h'cssed the way to handle comph ; x s ( , ntences that are t ) rew &gt; lent in naturally occurring discourses .</sentence>
				<definiendum id="0">urring ( lis</definiendum>
				<definiendum id="1">q. Fm'therm</definiendum>
				<definiendum id="2">CltCC</definiendum>
				<definiens id="0">h'cssed the way to handle comph ; x s ( , ntences that are t ) rew &gt; lent in naturally occurring discourses</definiens>
			</definition>
			<definition id="1">
				<sentence>n ( : s ( : discourses .</sentence>
				<definiendum id="0">n ( : s</definiendum>
			</definition>
			<definition id="2">
				<sentence>Roughly both versions use the following same forward center ranking for Japanese : Topic &gt; Empathy &gt; Subject &gt; Object2 &gt; Object &gt; Others , where Empathy is a grammatical property that indicates the speaker 's position in describing a situation .</sentence>
				<definiendum id="0">Empathy</definiendum>
				<definiens id="0">a grammatical property that indicates the speaker 's position in describing a situation</definiens>
			</definition>
			<definition id="3">
				<sentence>For instance , consider processing a complex sentence of the form 'SX Conj SY , ' where SX and SY each consists of a simple sentence and Conj is a conjunctive element ( Suri and McCoy , 1994 ) 1 .</sentence>
				<definiendum id="0">Conj</definiendum>
				<definiens id="0">processing a complex sentence of the form 'SX Conj SY , ' where SX and SY each consists of a simple sentence and</definiens>
				<definiens id="1">a conjunctive element</definiens>
			</definition>
			<definition id="4">
				<sentence>t A TM _1 ... . d -- ~2~_21-2-Z 72~ -30 __1 -- ( ~7.5 ( /0-J these ( liscours ( , s are automati ( : ally l ) ; ~rtitione ( l im ; o simple sent ( mc ( ; s and re ( : ( ; ive ( l stru ( : tural rarely , sis , and the positions of zero pron ( mns ~tr ( ' : a , ul ; oinati ( : ally i ( tentili ( ' , d as missing o| ) liga .</sentence>
				<definiendum id="0">o simple sent ( mc</definiendum>
				<definiens id="0">tural rarely , sis</definiens>
			</definition>
</paper>

		<paper id="1088">
			<definition id="0">
				<sentence>Centering ( Grosz et al. , 1995 ) and Dynamic Semantics* both concern the sequential processing of discourses , with particular emphasis on the resolution of pronouns .</sentence>
				<definiendum id="0">Centering</definiendum>
				<definiendum id="1">Dynamic Semantics*</definiendum>
				<definiens id="0">both concern the sequential processing of discourses , with particular emphasis on the resolution of pronouns</definiens>
			</definition>
			<definition id="1">
				<sentence>( 1 as Lhc dis ( 'ore : s ( ; ( 'eJlt ( ; r o ( : ( : ut ) ics posil , ion 0 as well as il ; s ot ; he .</sentence>
				<definiendum id="0">'eJlt</definiendum>
			</definition>
			<definition id="2">
				<sentence>This is schematicMly represented as follows : Cl ... b , ... . \ [ , , , \ ] ... \ ] ... c2 ... b , '' \ ] ( C1 , C2 : `` controllers '' of sloppy variable YP ) Ilere , XP is the anl ; ecedent for some preform XP ' , and YP is the sloppy variable~ , i.e. , a proform embedded within XP .</sentence>
				<definiendum id="0">XP</definiendum>
				<definiendum id="1">YP</definiendum>
				<definiens id="0">the sloppy variable~</definiens>
			</definition>
			<definition id="3">
				<sentence>: - &gt; \ [ Ul , l ) 0 , Pe , ua , P4 I ul = 1,1 ) 0 -P : ~ , Ua You , P= = av ( \ [ I help ( v , u : d\ ] ) , : av ( \ [ I want ( v , po ( u , ) ) \ ] ) , help ( u , , u : ~ ) , want ( ul , helt , ( Ul , Ua ) ) \ ] ; \ [ 1'o,1 ' , ~ , \ ] P0 = Ps , 1 ' , ~ - , \v ( \ [ I kiss ( v , ua ) \ ] ) , NOT ( P4 ( ua ) ) \ ] The variable P4 represents the t ) roI ) erty of `` wanting ul to Po '' .</sentence>
				<definiendum id="0">help</definiendum>
				<definiendum id="1">P4</definiendum>
				<definiens id="0">represents the t</definiens>
			</definition>
</paper>

		<paper id="2162">
			<definition id="0">
				<sentence>Allusional pretense theory ( Kumon-Nakamura et al. , 1995 ) is the most powerflll one in that it can explain ironic utterances of five speech act classes using the two crucial notions of allusion ( including echoic interpretation and reminder ) and pragmatic insincerity .</sentence>
				<definiendum id="0">Allusional pretense theory</definiendum>
				<definiens id="0">the most powerflll one in that it can explain ironic utterances of five speech act classes using the two crucial notions of allusion ( including echoic interpretation and reminder</definiens>
			</definition>
			<definition id="1">
				<sentence>A situation sj is a part of a situation s2 ( i.e. , Sl &lt; 1 s2 ) if and only if every infon supt ) orted t ) y st is also sut ) ported by su .</sentence>
				<definiendum id="0">situation sj</definiendum>
				<definiens id="0">a part of a situation s2</definiens>
			</definition>
			<definition id="2">
				<sentence>l ) arameters ( : an be restricted I ) y infons : for example , T &lt; &lt; v'~ : ' : ~'l~'t ' , ''r ) ) ix a parameter for tenlporal situations whMl temporally succeed to .</sentence>
				<definiendum id="0">l ) arameters</definiendum>
				<definiens id="0">an be restricted I ) y infons : for example</definiens>
			</definition>
			<definition id="3">
				<sentence>Table 2 : Pragmatic insincerity of ironic utterances in Figure 1 Violated pragmatic principles Utterances violating the principle Sincerity condition for Inform ( S believes P ) ( la ) ( le ) ( 1 ( 1 ) ( 4a ) ( 4b ) ( 5a ) for Question ( S does not know P ) ( lb ) for Advise ( S believes P will I ) enefit H ) ( 5t ) ) for Offer ( S wants to do an action P for H ) ( le ) for Thank ( S feels grateful for an action P ) ( 3b ) Propositional content condition for Thank ( P is a past action clone by H ) ( 3a ) Preparatory condition for Offer ( S is able to do an action P ) ( le ) Maxim of relevance ( P is relevant in Sperber and Wilson 's ( 1986 ) sense ) ( 2a ) Politeness principle ( U should be made at an appropriate level of politeness ) ( 2b ) Maxim of quantity ( P is as informative as required ) ( 2c ) Notes : S , H and P denote the speaker , the hearer and the propositional content , respectively .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">Pragmatic insincerity of ironic utterances in Figure 1 Violated pragmatic principles Utterances violating the principle Sincerity condition for Inform</definiens>
				<definiens id="1">wants to do an action P for H ) ( le ) for Thank</definiens>
				<definiens id="2">able to do an action P ) ( le ) Maxim of relevance ( P is relevant in Sperber</definiens>
			</definition>
			<definition id="4">
				<sentence>966 many cases , sin ( : e the hearer already knows the fact that the three components hold in the situs\ ] ion , interpretation of irony results in confirmation of the mosl ; uncertain information , that is , the speaker 's emotional attitude .</sentence>
				<definiendum id="0">uncertain information</definiendum>
				<definiens id="0">hold in the situs\ ] ion , interpretation of irony results in confirmation of the mosl</definiens>
			</definition>
</paper>

		<paper id="1076">
			<definition id="0">
				<sentence>S ( ; c : i ; ion 3 shows how normal disjunctions c ; m t ) ( ; r ( ; l ) lac ( ; d t ) y ( : ont , cxtx ; d constrainl : s. S ( , ( : tion 4 t ; hcn , &lt; d~ows how t , hcs ( '~ cont ( ; xl ; ( ' , d ( ' , onstraints can encod ( , del ) ( ' , nd ( ; ni , disjunctions. S ( ! ( : l ; ion 5 1 ) r ( ! s ( ' , nts the mo ( hllm'ization a , lgorii ; hm for conlx~xi ; ed ( ' , on-. si ; ra.ini ; s. Ih ) wever , e , ven though this algor { l ; hm is t~ ( ; omt ) ih &gt; t , im ( ' , ot ) ( ; ralJ ( m , it itself has ( ; xt ) on ( nitial comt ) lexity , so lilil , l ( ing it IllOl ( ' , ( ~tli ( ' , i ( mi ; should ~Jso 1 ) ( ; a ( : onc : ( , rn .</sentence>
				<definiendum id="0">ralJ</definiendum>
				<definiens id="0">t~ ( ; omt ) ih &gt; t , im ( ' , ot ) ( ;</definiens>
			</definition>
			<definition id="1">
				<sentence>Compaction is another operation designed to optimize feature structures for unification .</sentence>
				<definiendum id="0">Compaction</definiendum>
				<definiens id="0">another operation designed to optimize feature structures for unification</definiens>
			</definition>
			<definition id="2">
				<sentence>Definition 1 ( Dependency Group ) A dependency group is a conjunction of dependent disjunctions with the same name , d , where each V* A* , 6ieN and ieN are disjunctions and conjunctions of formulae ¢i , respectively , where each i is a member of the set of indices , N. 450 disjunction is an alternative-case form such that there is one alternative for every disjunct of every disjunction in the group , and there is one case for each disjunct in the group which is a co*onetion of the alternative variables for that disjunct &lt; .</sentence>
				<definiendum id="0">Dependency Group ) A dependency group</definiendum>
				<definiendum id="1">ieN</definiendum>
				<definiens id="0">disjunctions and conjunctions of formulae ¢i , respectively , where each i is a member of the set of indices</definiens>
				<definiens id="1">a co*onetion of the alternative variables for that disjunct &lt;</definiens>
			</definition>
			<definition id="3">
				<sentence>The modularization algorithm consists of two main steps .</sentence>
				<definiendum id="0">modularization algorithm</definiendum>
				<definiens id="0">consists of two main steps</definiens>
			</definition>
			<definition id="4">
				<sentence>Definitjop , A 3 ( ConfineInent ) V /\ a~ COII , f ( jc N iE M , J~/It ) 451 V Aa ; is the confinement of J CN iGM with respect to a iff co~tf ( J CN i &lt; M , M ' ) =dnf ( J CN i~M ' ) , where. M ' C M. Constructing the eontinement of a ( : as ( ; form is essentially just throwing out all of the alternative variables that are not in M'. However , only doing this might leave us with duplicate disjuncts , so converting the result to DNF removes any such duplicates. To make the definition of confinement clearer , consider tile following conjunction of dependent disjunctions : ( d ¢ , 0 , ¢ , ( / ) , ¢ ' , ~// ) A ( d ~/ % ~// , t/ , , , // , ~/o , t// ) A ( ( ~ x , x , x ' , x ' , x ' , x ' } . This is equivalent to tile compact alternative forIn : 9 ( a~ - ' , ¢ ' ) A ( a~ -- ~ x ) A ( d + x ' ) , and tile following case fornl : ease. = ( ( el A a~/ , , *9 v ( al A ~4 A a'0 , V ( 4 A .,2 A d ) v ( 4 A a~ A d ) v ( 4 A ( q A ai~ ) V ( ( 4 A a~ A d ) ) ' Now we can compute the confinements. For instance 1 eonf ( case , { 1 , 2 } ) = dnf ( ( a I A a~ ) V ( a I A a , ~ ) V ( al A ( q ) v ( o ' A a,9 v ( ( 4 A d ) v ( 4 A d ) ) , \ ] After removing duplicates we get : . , ,¢ ( ( , ase , { ~ , 2 } ) ( ( 4 A ab V ( o * A `` 9 V ( 4 A a~ ) V ( 4 A a,9 ) '1 Likewise , for the c ( ) mtflement of M ' with respect to M , we get : conf ( case , { 3 } ) : ( ( a a ) V ( ai~ ) ) . Now we just need to test whether two confined case ibrms are independent with respect to the original. This is done with the free combination operation , shown in definition 4. Definition 4 ( \ ] ~¥ee Combination ® ) The free combination of two ease forms is the disjunctive 'normal form of their conjunction : case ' ® case '' -dnf ( case ' A case '' ) The two ease forms , case ' and case '' , are I ) NF formulct. ~ib compute the free combination , we conjoin them and convert the re.suit back into DNF. They are independence if their free combination is equal to the original ease tbrm , case. For example , the flee combination of the two confinements from above , ( ( a I A a , 2 ) V ( a* , a a , ~ ) V ( a~ A a~ ) V ( a~ A a~ ) ) and ( ( d ) v 0,9 ) is ( ra ' A a~ A 4 ) V ( el A 4 A 4 ) V ( a I/ , a~ A d ) V \ \ ] ( 4 A a.~ A 4* ) v ( 4 A , q A d ) v ( 4 A a~/ , d ) V ( a~ A d A d ) v ( a~ A a~ A d ) ) 9in this examl ) le , equivalent alternative variables have again been replaced by representat , ives of their equivalence class. So tbr instance. , a~ , c*~ and a~ are all represented by al. which is not equM t ; o the original ( : as ( . ' form : ( ( el A a~ A a~ ) V ( a ' , A , ~ , ~ A 4 ' ) V ( 4 A d A , ,i\ ] ) v ( al A a~ A all ) v ( d A a~ A d ) v 04 A d A a q ) ) , so tim first two disjunctions are not indet ) endent from the third. However , the second disjmmtion is independe.nt front the first and the third since , conf ( case , { 2 } ) ( ( a~ ) V ( a.~ ) ) , anti co , ¢ ( ca.~e , { 1 , 3 } ) ( q , ' '~ ' ~ ' `` : t , Aa ' , ) V ( % Aai , ) V ( asAa ! i ) ) , and their free combination is equal to the oi'iginal case form. Therefore , the original formula is equivalent to ( d ' ~/a , *// ) A ( d , , ¢ , ¢ , ¢ ' ) A ( d , , X , X ' , Z ' ) . The last section showed all efl'ective algoritlnn for modularizing groups of dependent disjunet ; iolls. However , even dlough this is a compile time algorithm we should be con ( : erned about its eflio ciency since it has ext ) onential comph ; xity. The main source of complexity is that we inight have to check ( ; very pair of sul ) sets of disjun ( : tions fl'oin the group. Ill the worst case this is tnmvoidable ( el o though we do not expect natural language grainmars to exhibit such behavior ) . Other sources of comi ) lexity are computing the fl'ee coinbinadon and testing the result against the original ( : as ( ; form. l , uckily it is possible to avoid both of these operations. This Ceil t ) e done by noting that both the original ( : ase form aim each of the ( : onfine { t ( : as ( ; forms are in DNF. Therefore it ; is a nee-. essary ( : ondition t } tat if l ; he fl'ee combination of the confinements is the same as the original case form then the I ) roduet of tile number of disjun ( 'ts i , , ea ( : h conflneme.t , lease'l x lease '' l , re , st eq , lal the number of disjun ( : ts in the original case form , lease I. Moreover , since both confinements at ( ; derived fl'om the original ease form , it is also a suf ficient , condition. This is shown more forlnally in theorem 1. Theorem 1 ( l~Yee combination elimination ) ~. ' , ~se = ~ : as , ease '' ¢= &gt; \ ] case\ ] = ba~e'l × Icase '' l Proof = : &gt; We assulne that case'®case '' =case .</sentence>
				<definiendum id="0">Definitjop</definiendum>
				<definiens id="0">tnmvoidable ( el o though we do not expect natural language grainmars to exhibit such behavior ) . Other sources of comi ) lexity are computing the fl'ee coinbinadon</definiens>
			</definition>
</paper>

		<paper id="2182">
			<definition id="0">
				<sentence>IDAREX allows to define various types of variables , and to mix canonical and inflected word forms in the regular expressions .</sentence>
				<definiendum id="0">IDAREX</definiendum>
				<definiens id="0">allows to define various types of variables , and to mix canonical and inflected word forms in the regular expressions</definiens>
			</definition>
			<definition id="1">
				<sentence>When the user clicks on an unknown word in a foreign language , LOCOLEX evaluates the context of the queried word .</sentence>
				<definiendum id="0">LOCOLEX</definiendum>
				<definiens id="0">evaluates the context of the queried word</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>The SGML markup delimits phrases whose boundaries were identified by the initial phrase-finding pass .</sentence>
				<definiendum id="0">SGML markup</definiendum>
				<definiens id="0">delimits phrases whose boundaries were identified by the initial phrase-finding pass</definiens>
			</definition>
			<definition id="1">
				<sentence>; to the left of the ; phrase is the word `` og ' ; tothe left of that is an ; unlabelled phrase ; merge the entire left ; contextinto the OIZG , ; phrase and all The first two clauses of the rule are antecedents that look for phrases such as `` America inc. '' The next two clauses are further antecedents that look to the left of the phrase for contextual patterns of form `` &lt; non~ &gt; , .</sentence>
				<definiendum id="0">phrase</definiendum>
				<definiens id="0">the word `` og ' ; tothe left of that is an ; unlabelled phrase ; merge the entire left ; contextinto the OIZG</definiens>
			</definition>
			<definition id="2">
				<sentence>Extending a concept from information retrieval , this amounts to maximizing what we might call initial recall , i.e. , lit= I ( 1 ) I I / I ( i ) I , where ( IJ is the set of actual phrases in a test set , K is the set of candidate phrases generated by the initial phrasing passs , and cI ) I is tile set of those ( D &lt; q~ that arc partially aligned with some 1 ( c K. The general strategy we have adopted for finding initial phrase seeds is to look for either runs of lcxcmes in a fixed word list or runs of lexemcs that have been tagged a certain way by our part-of-speech tagger .</sentence>
				<definiendum id="0">IJ</definiendum>
				<definiendum id="1">K</definiendum>
				<definiens id="0">the set of actual phrases in a test set ,</definiens>
			</definition>
			<definition id="3">
				<sentence>The corc of the rule consists of clauses that test thc lexical context around a candidatc phrase 1 &lt; or that test lcxcmcs spanned by 1 ( .</sentence>
				<definiendum id="0">corc of the rule</definiendum>
			</definition>
			<definition id="4">
				<sentence>p i. For example , say Pi tests for the presence of a lexeme to the left of a phrase and e~tends the phrase 's lxaundaries to include ) v. We extend the machine bt to 276 encode this rule by replacing ~ 's current start state S with a new one S ' , and adding a ~ , transition from S ' to the former start state S. Thus becomes Pv , U &gt; l @ &gt; O- &gt; 0 For a rule I~ that tests whether a phrase contains a certain lcxcme ~'i , wc construct an `` acccptor '' machinc that accepts any string with ) ~i in its midst .</sentence>
				<definiendum id="0">wc construct an `` acccptor</definiendum>
				<definiens id="0">tests for the presence of a lexeme to the left of a phrase and e~tends the phrase 's lxaundaries to include</definiens>
			</definition>
</paper>

		<paper id="1079">
			<definition id="0">
				<sentence>The PERSON and ORGANIZATION templates are the `` temt ) late element '' templates , which are invariant across scenarios .</sentence>
				<definiendum id="0">ORGANIZATION templates</definiendum>
				<definiens id="0">the `` temt ) late element '' templates , which are invariant across scenarios</definiens>
			</definition>
</paper>

		<paper id="2191">
			<definition id="0">
				<sentence>On the contrary , EBMT generates all the possible candidates combining suitable phrases .</sentence>
				<definiendum id="0">EBMT</definiendum>
				<definiens id="0">generates all the possible candidates combining suitable phrases</definiens>
			</definition>
			<definition id="1">
				<sentence>The success rate is 71 % for a test data set consisting of 1,050 unseen sentences in the same domain .</sentence>
				<definiendum id="0">success rate</definiendum>
				<definiens id="0">71 % for a test data set consisting of 1,050 unseen sentences in the same domain</definiens>
			</definition>
			<definition id="2">
				<sentence>VERBMOBIL is a typical translation system for face-to : face dialogue \ [ Wahlster93\ ] .</sentence>
				<definiendum id="0">VERBMOBIL</definiendum>
				<definiens id="0">a typical translation system for face-to</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>Analyses display the root , pattern and all other affixes together with feature tags indicating part of speech , person , number , mood , voice , aspect , etc .</sentence>
				<definiendum id="0">Analyses</definiendum>
			</definition>
			<definition id="1">
				<sentence>~ ~ ) , which is interdigitated with a pattern CaCaC , where C represents a slot for a root consonant , sometimes termed a radical ; various prefixes and suffixes can then concatenate to the stem in the familiar way .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a slot for a root consonant</definiens>
			</definition>
			<definition id="2">
				<sentence>represents any letter , and C represents any radical ( consonant ) , the root drs ( tY ' - ) ~ ) can be interpreted as ?</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">represents any radical ( consonant ) , the root drs</definiens>
			</definition>
			<definition id="3">
				<sentence>Two-Level Morphology : A General Computational Model for Word-Form Recognition and Production .</sentence>
				<definiendum id="0">Two-Level Morphology</definiendum>
			</definition>
</paper>

		<paper id="2179">
			<definition id="0">
				<sentence>The Grammar Development Environment is a computer workbench for the development and evaluation of computational grammars of natural 1This resem'ch is flmded by SSHRCC ( 40-93-0607 ) and by FCAR ( 95ERl198 ) language which are described in a style close to that of GPSG .</sentence>
				<definiendum id="0">Grammar Development Environment</definiendum>
				<definiens id="0">a computer workbench for the development and evaluation of computational grammars of natural 1This resem'ch is flmded by SSHRCC ( 40-93-0607 ) and by FCAR ( 95ERl198 ) language which are described in a style close to that of GPSG</definiens>
			</definition>
			<definition id="1">
				<sentence>There is no specifier in the verb phrase ( VP ) , thus the V2 immediately dominates the V ; complex specifiers in noun phrases ( NP ) and adjectival and adverbial phrases ( AdjP and AdvP ) are given special treatment : there are X2 level specifiers of X2 constituents , as tbr example in : ( I ) N2 -4 R2\ [ POSS +\ ] , H2 the man 's black hat N2 -- + N2\ [ TYPR coll\ ] , H2 une foule de ces 6tudiants ( a crowd of those students ) Furthermore , some constituents have a specifier at level X1 even if there is a specifier at level X2 : ( 2 ) N2 -4 Spec , H2 tous les enfants , all the children N2 -4 Det , HI les enfants , the children Adjectival , adverbial and prepositional phrases are treated in a similar fashion in both grammars .</sentence>
				<definiendum id="0">VP</definiendum>
				<definiens id="0">all the children N2 -4 Det , HI les enfants , the children Adjectival , adverbial and prepositional phrases</definiens>
			</definition>
			<definition id="2">
				<sentence>This is a reflection of phenomena which are characteristic of French : cliticization , present in IS '' ench but not in English , and agreement , which is more limited in English titan in l~¥eneh .</sentence>
				<definiendum id="0">agreement</definiendum>
				<definiens id="0">a reflection of phenomena which are characteristic of French : cliticization , present in IS '' ench but not in English , and</definiens>
			</definition>
			<definition id="3">
				<sentence>To give an idea of the size of the resulting grammar , consider the lexical ID rule for a verb requiring an NP and a PP complements : ( I0 ) V2 -- ~ H\ [ 3\ ] , N2 , P2\ [ ~\ ] Paul donne un livre ~ Marie ( Paul gives a book to Mary ) The following metarules operate on this ID rule : passiveS ; direct object extraction ( SLASH N2 ) ; direct object cliticization ( accusative ) ; direct object cliticization ( oblique ) ; indirect object extraction ( SLASH P2\ [ h\ ] ) ; indirect object cliticization ( dative ) ; indirect object cliticization ( locative ) ; direct object extraction ( SLASH N2\ [ de\ ] ) ; adverb insertion ; auxiliary insertion ; supercompound aux insertion ; negative adverb ( pas ) insertion ; subject clitic insertion .</sentence>
				<definiendum id="0">direct object cliticization</definiendum>
				<definiens id="0">dative ) ; indirect object cliticization ( locative ) ; direct object extraction ( SLASH N2\ [ de\ ] ) ; adverb insertion ; auxiliary insertion ; supercompound aux insertion ; negative adverb ( pas ) insertion ; subject clitic insertion</definiens>
			</definition>
</paper>

		<paper id="1078">
			<definition id="0">
				<sentence>*ghis paper &lt; lescribes an alignment algorithm for F , I~MT whose running time is quadratic in tile size of the input parse trees. 'Phe algorithm uses dynamic programming to score all possible matching nodes between structure-sharing trees or forests. We describe the algorithm , various optimizations , and onr implementation. The development of a glachine translation ( MT ) system re &lt; luires the lengthy manual preparation of bilingual lexicons an ( t transfer rules. I { esearch over the past few years using parallel senten &lt; : ealigned bilingual corpora sugg { '.sts ways in which this manual effort &lt; 'an be partly replaced by corpus-based training. Some o1 ' l ; his research has treated the sentenees as unstructured word sequences to be aligned ; this work has primarily involved the acquisition of bilingual lexical correspondences ( Chen , 1993 ) , although there has also been a , n attempt to create a full MT system based on such trcat , ment ( Brown et al. , 1993 ) . I { ecently , several groul ) S have been exploring the possibility of aligning t ) arMlel syntacticalhj analyzed sentences fr &lt; ) m the source and target languages ( el. ( Sate and Nagao , 1990 ) , ( Klawms and Tzoukermann , 1990 ) , ( Grishman and Kosaka , 1992 ) , ( Kaji et al. , 1992 ) , ( Matsumoto et al. , 11993 ) and ( Grishman , 11994 ) ) . Tiffs offers the potential for acquiring not j ust lcxical but also structural correspondences between the two languages , q'he specific goal in aligning syntax trees is to identify tile ( : orresponding tree fragments in the source and target trees. By processing a. substantial corpus , a large set of such corresponding fragments can be collected. These ( : an then serve as the example base for a form of examph ; -based MT ( of. ( Nagao , 198d ) , ( Sate and Nagao , 1990 ) , ( IG\ii et al. , 1992 ) , ( Matsumoto &lt; % al. , 19 ! ) 3 ) and ( leuruse and lida , 1994 ) ) . This approach requires a fast tree alignment teehuiqu &lt; ~ ; research has I ) een ham/ ) ered by the lack of efli &lt; : icnt algorithms. This pa , per des &lt; : ril/cs an efficient algorithm for bilingual tree alignment. For each input sent ; ellee our parser t ) rodlmes a set of trees , corresponding to each possible syntactic analysis. O.r parse trees are transformed illl , O a `` regularized '' format , to represent the l ) redicate Argument structure. For each senten &lt; : e , the out put of the parser is a stru ( : tm : e-sharing tbrest. An example of strltetllre sharing I : ) etween two ila , rse trees &lt; ff l , } le same input senten ( '.e is shown in Fig are 1. We apply the parser to the source and target sentences , using a Spanish and an F , nglish grain mar , respeetbely. The resulting sets of structureshin : lug parse trees form the input to the alignment procedure. Our alignment program employs dynamic programming I algorithms , which are described in detail in later sect ; ions. The program begins a.t the roots of the source and target trees , and 1 ) roeeeds top down reeursively , filling a matrix of scorcs. ( liven N nodes in the som : ee tree 7 ' , + - '' / ' ( V~ , I5 ' , ) 2 and M nodes in the target tree `` / } . -7 ' ( Vt , El ) , the score matrix is an N × M matrix. For each pair of nodes xi , i = I ... . N 6 V~ and Yi , J 1 , ... M C ~ , the corresponding entry in the score matrix is a. measure of how well z/ illatc : hes &gt; { .</sentence>
				<definiendum id="0">-based MT</definiendum>
				<definiendum id="1">score matrix</definiendum>
				<definiens id="0">uses dynamic programming to score all possible matching nodes between structure-sharing trees or forests. We describe the algorithm , various optimizations , and onr implementation. The development of a glachine translation ( MT ) system re &lt; luires the lengthy manual preparation of bilingual lexicons an ( t transfer</definiens>
				<definiens id="1">n attempt to create a full MT system based on such trcat , ment ( Brown et al. , 1993 ) . I { ecently , several groul ) S have been exploring the possibility of aligning t ) arMlel syntacticalhj analyzed sentences fr &lt; ) m the source and target languages</definiens>
				<definiens id="2">a. measure of how well z/ illatc : hes &gt; {</definiens>
			</definition>
			<definition id="1">
				<sentence>~ ) &lt; lcnotes a tree as a pair of sets : V~ is the , set of vertices ( nodes ) in ( , he tree , and l ' , '.+ is the set ; of edges ( arcs ) . 460 S~I + S~I '' ( llliDItl°2 el / pos ( Icl ) ( te deporte \ [ qm , ~'///f -- -- ~ t'\ ] ) OS/\ ] `` ~+ ~ UlI P ' } / : '' - '' ' '' 2 , ' : ,. , r , , , , / nn favorilo / ~ `` / IlU~l ( ) in2| ~iiiil , , 11\ ] 1~ cl rico I , 'ip ; uro \ ] : Two l } a , t'scs I'or : El I , +~+i.~ ( ~ , ~ +~ dcp &lt; ) 'rLu .fi~vorilo ( h ! l 'm.'+u : /~a &lt; : ho 'ri ( : &lt; ) . on l , h { 2 s ( : or &lt; '.8 of { , h &lt; + I ) esl , n\ ] ; tA+ ( : hiNg t ) a , ir8 of { , hch ' ( l &lt; '~+ ( : &lt; 'mlatH , ,~. I ) yna , tiii ( : l ) ro~ra.tiHui.~ a , '+ , '+Hr &lt; ~H thal , ea ( : h ( ; Ill , ry ill i , h &lt; + tna , t , rix , ( i. &lt; ; . i , he + ( : or ( ~ for l , lie ( : orr &lt; ; , '- ; I ) ( &gt; lMin &lt; ~ I ) air o1+ uo ( Io8 ) is ( : ( mil } Ul , &lt; ~ ( I { &gt; .</sentence>
				<definiendum id="0">V~</definiendum>
				<definiendum id="1">IlU~l (</definiendum>
				<definiens id="0">the , set of vertices ( nodes ) in ( , he tree , and l '</definiens>
				<definiens id="1">the set ; of edges ( arcs )</definiens>
			</definition>
			<definition id="2">
				<sentence>such pairings , where d is the smaller of the degrees of v and v ' .</sentence>
				<definiendum id="0">d</definiendum>
				<definiens id="0">the smaller of the degrees of v and v '</definiens>
			</definition>
			<definition id="3">
				<sentence>n2 ) , where d is the lesser of the degrees of the source and target trees .</sentence>
				<definiendum id="0">d</definiendum>
			</definition>
			<definition id="4">
				<sentence>The following is an alternative , greedy procedure for computing ,5 ' ( v , v ' ) : Procedure GflEEDYLcA : pute the corresponding entry in a d ( v ) × d ( v ' ) matrix M : M &lt; j = Lexa , .~ ( ~ ; i , ~. ) + H ( vi , vj ) The entry Mij of M = M ( v , v ' ) is the score of matching the ith child of v with jth child of v'.6 pairs. such that neither its row nor its cohmm is already occupied by some pair in TOI ' : 7'OP ~-7'OP tO { ( i0 , j0 ) ) where the coordinates ( i0 , jo ) are such that Miojo z Ina.x { Mij I V ( i ' , f ) C 7'OP , i 7 k i ' , j # j ' } ( 3 ) rnin ( d ( v ) , d ( v ' ) ) . M AT'C Ht~. ( v , v ' ) = = l ; ex , ~o~l~ ( v , v ' ) + ( 4 ) ( &lt; ~ ) e~rc ) v With sorting , this can be done in O ( d log d+ d 2 ) time. The validity of this heuristic can be tested by comparing the performance of the procedures using the computation it , ( 2 ) and in ( 4 ) . 6Note : if we disregard the arc labels for simplicity , and set Lex , ,.~ ( . , . ) = O , then we do not need to build M , and may simply use Mij : S ( vi , v5 ) . ( Grishman , 11.994 ) employed an optimization heuristic which favored lexical matches. For each source node v with label L ( v ) , the procedure using this heuristic would first attempt to find a target node v ' with \ ] a , bel L ( v ' ) such that L ( v ) translated as L ( v ' ) in the bilingual dictionary ( a perfcct lexical match ) . If such a lexical match was found , the procedure did not attempt to match v with any other target node. A similar heuristic ( Lex-Match ) was incorporated into our program as the following preprocessing steps : matches are identified in the target tree\ ] If. v has at least one possible lexical match , all of those positions in the score matrix S which do not correspond to a lexical match of v are set m zero. lexical match , all of those positions in the score matrix which do nol , correspond to a lexicM match o\ [ v ' are set to zero. By setting to zero those positions in the score matrix which represent unlikely matches , this heuristic prevents these scores from ever being cah : ulated , substantially reducing the rmming time. Lex-Match , unlike the ( Grishman , 1994 ) heuristic , allows one source node to match lexically with more than one node in the target tree. We have implemented the greedy LCA-preserviug algorithm with the following features : Penalties : The penalties for collapsing edges were set to 0. s Scores : A LeX , ~od , ; score of 100 and a Lexa , . , , score of 21 was awarded for each match using our bilingual dictionary. These fimctions have the value 0 if there is no lexica\ ] match. rA match M ( v , v ' ) is also a lexical match if either M ( v , w ' ) o , ' M ( w , v ' ) is a lexical match , where w and w ' are children of v and v j , respectively. aWhen penalties are set , to zero and an empty t ) ilingum dictionary is used , the alignment algorithm \ [ ills tim scoring matrix with zeros. When we introduce no\ [ l-zero penalties , the alignment procedure prefers matches between nodes dominating similar structures , since nodes dominating dissimilar structures receive negative scores. Wc expect that non-zero penalties will improve precision with a nonempty bilingual dictionary , because they will favor similar structm'es. In preliminary testing , penalty values of 20 and 30 yielded iinprovenmnts in precision. 463 Text Baseline Struc-Share Lcx-Match Struc-ShaTv and Lex-Mateh El Camino Real \ ] \ ] .5 sec 11.3 see 8.3 sec 7.7 sec Curious George 98.0 sec 48.8 sec 87.4 see 44.7 set : Total 109.5 sec 60.1 set : 95.7 sec 52.4 sec Table 1 : Time Improvements Due to Optimizations Text F , I Camino Real Lex-Match Off Lex-Match On 47 out of 57 ( 82 % ) 47.5 out of 57 ( 83 % ) Curious George 44.6 out of 55 ( 81 % ) /14. ( 5 out of 55 ( 81 % ) q'otal 9:1.6 out of 1112 ( 82 % ) 92.1 out of 112 ( 82 % ) Table 2 : Changes in Accuracy due to l , ex-Match Ileuristie Optimization Variables : We experimented with variants of thc. procedures which included Structure Sharing ( Struc-Share ) and the Lexieal Match Optimization ( lmx-Match ) , as well as with those that did not. Table I shows the time consumed by our program to align sentences under different conditions. The baseline refers to our program without any optimiza.tions ( which is at least 6 times faster than before using this algorithm. ) The optimization w~riables have different effec.ts on the different texts. We believe that structure sharing has a much stronger el\ [ bet on Uurious Geo~ve than on El Camino Real because the tbrmer has longer sentences which produced more parses. The l , exM~tch optimization has a greater effect on l'SI ( 7amino Real than on Curious Ceor.q &lt; : because all of the words contained in El Camino Real are ineluded in our bilingual dictionary , but only a small portion of the words in Curious Geovje are included. We expect that as the size of our dictionary increases , the Lex-Match optimization will have a greater effect. The precision for each aligned pair of sentences is computed according to the formula : \ [ ltc.s'ultSct N Answcr Kc ? /\ [ { t~esuliSet I where Re.suit , fret is the set o1 ' source parses to which the alignment procedure assigned the highcst score , and AnswcrKey is the sc't of best source parses as judged by one of the exDerJtnenteFs. 9 This precision measure was previously used in ( Matsumoto et al. , 1993 ) a.nd ( ( h : ishman , 71994 ) . Table 2 compares the precision of the alignment procedure with and without the Lex-Match heuristic ( structure sharing had no eider on the scores. ) The slight increase in precision observed with the 91f there was no correct parse , the parscs wil , h the fewest errors were used for purposes of nligmnent. l , ex-Match optimization , may be an itldication that we should raise the score for lexical matches of node labels. The. era : rent implement~tion aligns trees 63 times faster than our previous program ( Grishman , 199d ) , with a 2.3 % inq ) rovement in pre ( : ision. 1° We expect line-tuning el ' the pa.rameters in our procedures to improve our performance. We expect to gain greater efficiency if all COi\ [ HIHOH IlOdeS between forests are s\ ] lare ( l , rather than .just the NPs. Another efficiency inlprovcment will be aehieved by factoring all ambiguity into the parse tree , as in ( Matsumoto eta\ ] . , 1993 ) . In our cur rent approach , disjunctions are r ( : prcse.ntetl only at ; the root level. In order to inq ) rovc the precision of alig , tment , we plan to ext ) erimenl , with w~rying the values of the Lex fmletions mid penali , ies in our scor.ing algorithm and e×l ) anding our bilh , gua\ ] dictio nnry. We will also experiment with the non -- gr ( '.edy algorithm discussed al ) ove and a th ) nthmn ( : ( .'. preserving algorithm ( ~ less consl.rained version of the algorithni which we have omitted due to .space limitations ) . In the dominance-preserving algorithm we relax the requirement of h : a-preserw~.tion , and require the l ) rcserw~ , tion of the ( \ ] omi\ ] ~ancc rc lationship between lm ( les : If ' , for two nodes a C7 ' a~d b C 7 ' , a dominates b ( ( telloted ~ts a ~ b ) , thell for f ( a ) ~ 7 '' and f ( b ) C '1 '' , f ( a ) j @ ) . The kh : a which makes it possible to align s ( m~ tenets quickly is that we place restrictions on the ways in which we align the parse trees. W ( '. t°'I'ihe dynamic l ) rogreumniug algorithm accounts for ml approximately 600 % iucrc~tsc ill speed of align\ ] nent a rough estimate sin ( : e much of Chc I ) rogrmn has been r ( &gt; implemcntcd .</sentence>
				<definiendum id="0">~-7'OP tO {</definiendum>
				<definiendum id="1">heuristic</definiendum>
				<definiendum id="2">fret</definiendum>
				<definiendum id="3">AnswcrKey</definiendum>
				<definiens id="0">the coordinates ( i0 , jo ) are such that Miojo z Ina.x { Mij I V ( i ' , f</definiens>
				<definiens id="1">the following preprocessing steps : matches are identified in the target tree\ ] If. v has at least one possible lexical match , all of those positions in the score matrix S which do not correspond to a lexical match of v are set m zero. lexical match , all of those positions in the score matrix which do nol</definiens>
				<definiens id="2">By setting to zero those positions in the score matrix which represent unlikely matches , this heuristic prevents these scores from ever being cah : ulated , substantially reducing the rmming time. Lex-Match , unlike the ( Grishman , 1994 ) heuristic , allows one source node to match lexically with more than one node in the target tree. We have implemented the greedy LCA-preserviug algorithm with the following features : Penalties : The penalties for collapsing edges were set to 0. s Scores : A LeX , ~od , ; score of 100 and a Lexa , . ,</definiens>
				<definiens id="3">Changes in Accuracy due to l , ex-Match Ileuristie Optimization Variables : We experimented with variants of thc. procedures which included Structure Sharing ( Struc-Share ) and the Lexieal Match Optimization</definiens>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>( McArthur 1992 ; Mei et al. 1993 ) Classification allows a word to align with a target word using the collective translation tendency of words in the same class .</sentence>
				<definiendum id="0">Classification</definiendum>
				<definiens id="0">allows a word to align with a target word using the collective translation tendency of words in the same class</definiens>
			</definition>
			<definition id="1">
				<sentence>The inside test consists of fitty sentence pairs from LecDOCE as input .</sentence>
				<definiendum id="0">inside test</definiendum>
				<definiens id="0">consists of fitty sentence pairs from LecDOCE as input</definiens>
			</definition>
			<definition id="2">
				<sentence>calculate a composite probability tbr each connection candidate according to fan-out , applicability , specificity of alignment rules , relative distortion , and dictionary evidence .</sentence>
				<definiendum id="0">composite probability tbr</definiendum>
				<definiens id="0">each connection candidate according to fan-out , applicability , specificity of alignment rules , relative distortion</definiens>
			</definition>
</paper>

		<paper id="2194">
			<definition id="0">
				<sentence>I i llltll I ~lfl-I 1 ~l-lll 1 IJl-nl I * * ( much-that ) \ ] more-than ( more-than ) ( electricity ) ( twist ) ( twist ) IW-~11 IW~11 * * ( fire-sky ) ( electricity ) ( raft-down\ ] ( raft-down ) ( push ) II~i-\ [ l~ II~l\ [ l~ { ill,14 El } * ( raft-down\ ] ( twist ) ( leg , hand ) Note : * means any word , L1 means a word in the left. , RI means a word in the right .</sentence>
				<definiendum id="0">L1</definiendum>
				<definiendum id="1">, RI</definiendum>
				<definiens id="0">means a word in the left.</definiens>
				<definiens id="1">means a word in the right</definiens>
			</definition>
			<definition id="1">
				<sentence>0 0 0 ... .. 1 0 1 1 1 0 l 0 0 Note : start means the beginning of a sentence , stop means the end of a sentence .</sentence>
				<definiendum id="0">start</definiendum>
				<definiens id="0">means the beginning of a sentence</definiens>
			</definition>
</paper>

		<paper id="2186">
			<definition id="0">
				<sentence>The code is a reduced version of one defined by GETA in ( Boitet , 1982 ) an accented letter is coded with the letter without 1054 accent , a vertical bar , and a number corresponding to the accent ( see examples on Figure 2 ) I. -- - &gt; a12 a -- - &gt; a13 6 -- &gt; ell -- ~ el2 -- - &gt; el3 Figure 2 • Examples of the code for accented characters All textual data are then completely portable provided that source and target machines use ASCII .</sentence>
				<definiendum id="0">code</definiendum>
			</definition>
			<definition id="1">
				<sentence>where verb is the command and where arg i and param i are respectively the names and the values of its parameters .</sentence>
				<definiendum id="0">verb</definiendum>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>Symbol = represents a voiced closure while symbol _ represents an unvoiced closure .</sentence>
				<definiendum id="0">Symbol =</definiendum>
				<definiens id="0">represents a voiced closure while symbol _ represents an unvoiced closure</definiens>
			</definition>
			<definition id="1">
				<sentence>The ls'adora system builds a large network of nodes that correspond to different speech events like phones , phonemes , words or sentences .</sentence>
				<definiendum id="0">ls'adora system</definiendum>
				<definiens id="0">builds a large network of nodes that correspond to different speech events like phones , phonemes , words or sentences</definiens>
			</definition>
</paper>

		<paper id="2139">
			<definition id="0">
				<sentence>Full-text processing consists of thr ( 'e steps : trees of each seltt ( 'n ( `e ill a sour ( 'e t ( `xt fled parse tree to ( `a &lt; 'h senten &lt; 'e in the text text ntodel an &lt; l generating a. final analysis for ea &lt; 'h sentence in tit ( . text The resl ) ective procedures fl ) r these steps are ( It'scribed in the tolh ) wing thre ( , subs ( `ctions. In order to refer to ( 'ontext information that consists of dat ; t on multiple senten ( 'es in at text , it is esseutim to constru ( 't some eollt ( ` : ~t model ; the tirst st ( ' 1 ) of the full-text 1 ) ro ( 'essing nwthod ix therefore to ( 'onstru ( 't a context lnodel by amalyzing ( `a ( 'h senten ( ' ( ` in an inlmt text. To avoid any ( , rrors that may o ( 'cur during transforlmLtion into any other rel ) r ( `s ( 'ntations , su ( : h as a h ) gicM rel ) resentation , we stayed with surface structures , and to i ) reserve the robustn ( 'ss of this framework , we used only a. set of l ) arsed tr ( 'es as ; t ( : ontext model. Thus , ea ( : h sent.enc ( ` of an inl ) Ut text ix pro ( ' ( `ssed t ) y a syntactic lmrs ( 'r in the first st ( 'I ) , and the positi ( m of eac|t instance of every h'mma. , its morphological information , and its lno ( lifiee-modifier relationships with other content words are extracted from the parser output , and stored to construct a context model , ; ~s shown in Figure 1. In addition , if any on-line knowledge r ( `sourc ( 's are ~tvMbd ) l ( ` , infl ) rmation extracted froln tit ( , resour &lt; : es is also stored in the context model. For examl ) le , infl ) rmation on sym onyms extra. ( 'te ( t from an on-lilw thesaurus dictionary and information ( m wor ( l sense all ( \ [ structural disambiguation extracted D ( ) m an examl ) le l ) ~ts ( ` , such as &lt; me describe &lt; l in ( Urmnoto , 1991 ) and ( Nagao , 1990 ) , may l ) e ad &lt; led to the cont ( 'xt model. In the first step , a syntactic l ) ~trser may not always generate a Mngl ( ` unified parse It ( ` ( ` for e~wh sentence in tiw source text. A syntacti ( ' parser with general grammar ruh 's is often mml ) le to analyze not only se.ntences with grammatical errors and ellipses , but also h ) ng s ( `nten ( : es , owing to their comi ) lexity , l Thus , it : ix indispensable to ( `stablish a ( 'orrect analysis for l In texts front a restricted ( lomain , suelt as compltter manu~tls , most sentences are g1 : mmm~tic~tl\ [ y correct , ttow ( wer , even a well-established syntaetie parser usually fails to generate a ratified parsed structure for a\ ] ) out 10 to 20 1 ) ( ~rc ( : nt of all the sentences in such texts , and the failnre in syntactic analysis leads to a failure in the filt~tl outl ) l/t of a , , NLP system. Context = { Sentencel , Sentence2 , ... , Sentence n\ ] Stenence i = \ [ Word i-I , Word i-2 , ... , Word i-j } John likes apples. Sentence 1 Word1-1 \ [ John\ ] POX : N BASE : John ... ... ... ... ... .. ub ./~o hn~ Word1-2 \ [ llktm\ ] ... ... ... ... ... . ~ POX : V BASE : like ... POX : N BASE : apple .~. Tom ah'o likes apples. Sentence 2 Word2-1 \ [ Tom\ ] POS : N BASE : Tom ... Word2-2 \ [ also\ [ POS : ADV BASF. : also. , Word2-3 \ [ likes\ ] POS : V BASE : like , .. Word2.4 \ [ apples\ ] POS : N BASE : apple .. He also likes oranges. Sentence 3 Word3-1 \ [ He\ ] POS : PN BASE : he ... Word3-2 \ [ also\ ] POX : ADV BASE : also. , Word3-3 \ [ likes\ ] POS : V BASE : like , .. Word3-4 \ [ oranges\ ] POS : N BASE : oranse , . , Figm'e 1 : Example of ~t context mod ( 'l su ( 'h a s ( 'ntenee , hfformation extracted front COlnpl ( `te 1 ) arses of w ( `ll-formed sentences 2 in a context model ( 'all b ( ` us ( ' ( l to cOlnlflete incolnl ) lete parses , in the f ( ) rm of partially parsed chunks that a bottomup 1 ) ars ( , r outlmts fl ) r ill-formed sentences by using a previously des ( 'ribed method ( N~Lsukawa , 1995 ) . On the other hand , fl ) r some sentences in a text , such as Time \ ] lies like an arrow , a syntactic t ) arser lltay gent , rate nlore thatl olle parse tree , owillg to the 1 ) r ( `sen ( -e of words that Call \ ] ) e ; Lssigned to more than one part of st ) eech , or to the l ) resen ( 'e of complicated coordinate structures , or for wtrious other re~Lsons. In attempting to select the correct 1 ) arse of such a sent ( `nee , on ( ' ( ; an use the tyt ) es of the l ) revious and subse ( \ [ lleltt sentences or 1 ) hras ( `s ( Sll ( 'h as sentence , llOllll phrase , verb 1 ) hrasc , anti so ( ) It ) an ( l the modifiermodifiee 1 ) atterns in the context model. Therefore , in the second step , tit ( : context model g ( `nerat ( `d in the firs { ; st ( ' 1 ) is refined by referring to information in the context model. First , the most l ) referable candidate parses are selected for sentences with multit ) le parses by referring to information on ea ( 'h sentence in the context model for which a parser lent'rated a single unified parse. Then , partiM parses of ill-forlned sentences are ( 'ompleted by referring to information on well-h ) rmed senten ( : es in the context model. The algorithm for multiple parse selection based on `` 'Ill this paper , a `` well-fornwd senten ( -e '' life , IllS ( ) It ( ' that is 1 ) arsed as one or lllOl'e than Ol1 ( ` lllli~i ( 'd strll ( 'tllre~ and an `` ill-formed sent ( me ( ` '' means one that c ; mnot be pm'sed as a unified strncture. 825 the context model is as fi ) llows : candidate i ) arses , assign a score for each lnodifiermodifiee relationship that is fl ) und in the context model , and add u I ) the scores to assign a 1 ) reference value to the ( : andidate l ) arse. value. If more than one l ) ; ~rse has the highest t ) ref erenee wdue , go to the next ste 1 ) with those lmrses ; otherwise , leave this i ) ro ( 'edure. date parse that has the same tyl ) e of root node ( su ( 'h as noun phrase , verb l ) hrase , or sentence ) as the parse of the 1 ) receding sentence or the next senten ( 'e. wdue. If more than on ( ' parse has the highest 1 ) ref erence value , go to tit ( , next ste I ) with dtose 1 ) arses ; otherwise , leave this procedure. date parse based on heuristic ruh 's that assign scores to structures according to their grammatical preferability. ence value. If more than one t ) arse has the highest 1 ) reference wfiue , select the first parse in the list of the remmning candidate parses. Tile procedure of conq ) leting l ) artia\ ] \ ] ) kLl'ses of a.n ill-formed sentence consists of two steps : The part of st ) ee ( 'h mid the modifiee-modifier relationshil ) s with other words are inspe ( 'ted for each word in a 1 ) artial l ) arse. If the part of speech and tit ( `` modifiee-modifier relationships with other words are different from those in the eont ( 'x : t model , the 1 ) aerial parse is restructured a ( 'eor ( ling to the information in the context model. If the 1 ) artial l ) arses were not ratified into a singh '' structure in the previous step , they arc , joined together on tit ( `` l ) asis of modifier-modifiee relationshil ) 1 ) atterns in the ( 'ontext model so that a unified i ) arse is obtained. the context model Finally , in the third stel ) , ea , 'h senten ( 'e in the ( 'Olltext lnodel is mmlyzed individually , and its mnl ) iguities and context-dependent prol ) h'ms are resolved by referring to information on other sentences in the context model. The next section des ( 'ribes the 1 ) rocedures for problenl resolution , and explains lheir effectivene , ss in lint ) roving nmehine transla.don output. The a ( : cura ( 'y of syntactic analysis m~\y l ) e improved by refinement of the ( 'ontext nn ) del in tlt ( ' second step of the procedure. For ex~mlple , in an exl ) eriment on 244 sentences from a. chapter of a COml ) uter manual , in which we attempted to select the correct parse of a sentence from multiple candidate l ) arses , ( 'orre ( 't parses were sele ( 'ted for 89.1 % of 110 multiple pa.rsed sentences by using infbrmation in the ( 'ontext model , where~us the success rate obtained when the ( 'ontext model ¢'ontmned no ilfformation was 74.5 % . In our experiment on ill-f ( mned sentences ill technical do ( 'ulnents , in more than h~flf of the incoml ) letely 1 ) ~trsed sentences , the lmrt.iM parses were joined into a single stru ( 'ture by using ilfformation in the context model. However , after the second step , ambiguities in each sentence are kept unresolved in the context model. Thus , we need to resolve problems in each sentence in the context model ill ( lividuMly. In this section , we describe how the accuracy of senten ( 'e mtalysis in other probh'nls is improved by referring to the siml ) le context model , and how the results are refiecte ( l in improved machine translation outlmts. subjuncts Ih , solving the focus of fi ) cusing sul ) juncts such as also ; rod only is a tyl ) ieal context-del ) endent probl ( 'm tha.t requires ilffornmtion on the 1 ) revious context. Fo ( 'using sul ) jnncts ( lr~tw m.tention to a part of ; t senten ( -e th~tt often represents new information. Consider the se ( : ond senten ( 'e , Tom also likes apples , in Figures 1 mM 2. Ill this sentence , the scope of also can 1 ) e To 'm , likes , the entire predicate ( the whole sent.enee except the subject Tom ) , or apple. % acc ( trding to the itrevious context. In this ( 'as ( ' , the preceding senten ( 'e , Joh , n likes apples , has the structure , A likes B , whereas sentence ( 2 ) has the structure , X also likes B , where B and the predi ( : ate fib , s are identical. The eoml ) arison of these two structures indicates that the new intbrmation X ( Tom ) is the scope of also in sentence ( 2 ) . The fl ) ( 'us of focusing sul ) jun ( 'ts ix resolved by means of the following algorithln : model one that contains expressions morphologically identical with those in the sentence containing the focusing suhjunet. the sentence containing the tl ) ( 'using subjunct with words or phrases in tit ( `` senten ( 'e extracted in ste l ) 1. as candidates for the focus , and select the remainder as the focus of the fo ( - , tsing su| ) junct. If more than one candidate remains , take the defaul } , interpretation that wouhl be used if there were no context iuformatiolt. Figure 2 shows the translation outputs of our syste , n with and without information 1 ) rovi ( h~d by context pr ( t ( : essing. As shown in this figure , with ( tar the context information , also modifies the 1 ) redicate like l ) y default in l ) oth senten ( 'es ( 2 ) and ( 3 ) . In contrast , when context pro ( 'essing is apt ) lied , the focus of also ix determined to I ) e Tom in senten ( : e ( 2 ) and orange in sentence ( 3 ) . In our amtlysis of ( 'omlmter manuals , most nouns were repeated with the same expressions unless they were repla. ( 'ed by 1 ) ronouns or definite expressions su ( h as th , is , that , and tit ( '.. ( ) n the other hint ( I , predi ( -ates were sometimes repeated with different expressions. For exanlple : A has B. ~ A also includes C. A contains B. -- ~ C is also included in A. 826 ( 1 ) John likes al ) l ) l'.'s. \ [ With and 'vViihou ( . ( : ottt &lt; , xi\ ] I ) ep ( qidency SI rtl ( 'l iil ' ( q 'l'ranslaiioll : `` ~ ! i ~'+ &amp; , ~J `` / : - : ~ : ~.t'-g*~-51 '' &lt; , , lOhgL \ ] t¢+ 'l'i~tylO 'lllo kOrtLOllti ilZ¢LR'IL , ( 2 ) Tom Mso likes a.l ) l ) l , ,s. \ [ wiu , ,..~ &lt; ' , . , , , .'x , l ( -~ -- -- '' L ~-_ 'l'ranslalh ) n : I '' ACJ. , i ) `` t : : { : , I.J4 &lt; &gt; ~ : :~ : &amp; ~ : -g , : I'0711 , \ ] DfL /'Zll , ¢ llJo f\ [ OTL ' !</sentence>
				<definiendum id="0">Full-text processing</definiendum>
				<definiendum id="1">well-fornwd senten</definiendum>
				<definiendum id="2">context pro</definiendum>
				<definiens id="0">consists of thr ( 'e steps : trees of each seltt ( 'n ( `e ill a sour ( 'e t ( `xt fled parse tree to ( `a &lt; 'h senten &lt; 'e in the text text ntodel an &lt; l generating a. final analysis for ea &lt; 'h sentence in tit</definiens>
				<definiens id="1">It'scribed in the tolh ) wing thre ( , subs ( `ctions. In order to refer to ( 'ontext information that consists of dat</definiens>
				<definiens id="2">' 1 ) of the full-text 1 ) ro ( 'essing nwthod ix therefore to ( 'onstru ( 't a context lnodel by amalyzing ( `a ( 'h senten ( ' ( ` in an inlmt text. To avoid any ( , rrors that may o ( 'cur during transforlmLtion into any other rel ) r ( `s ( 'ntations , su ( : h as a h ) gicM rel</definiens>
				<definiens id="3">ontext model. Thus , ea ( : h sent.enc ( ` of an inl ) Ut text ix pro ( ' ( `ssed t ) y a syntactic lmrs ( 'r in the first st ( 'I ) , and the positi ( m of eac|t instance of every h'mma. , its morphological information , and its lno ( lifiee-modifier relationships with other content words are extracted from the parser output , and stored to construct a context model</definiens>
				<definiens id="4">es is also stored in the context model. For examl ) le , infl ) rmation on sym onyms extra. ( 'te ( t from an on-lilw thesaurus dictionary and information ( m wor ( l sense all ( \ [ structural disambiguation extracted D ( ) m an examl</definiens>
				<definiens id="5">a syntactic l ) ~trser may not always generate a Mngl ( ` unified parse It ( ` ( ` for e~wh sentence in tiw source text. A syntacti ( ' parser with general grammar ruh 's is often mml ) le to analyze not only se.ntences with grammatical errors and ellipses , but also h ) ng s ( `nten ( : es , owing to their comi</definiens>
				<definiens id="6">mmm~tic~tl\ [ y correct , ttow ( wer , even a well-established syntaetie parser usually fails to generate a ratified parsed structure for a\ ] ) out 10 to 20 1 ) ( ~rc ( : nt of all the sentences in such texts , and the failnre in syntactic analysis leads to a failure in the filt~tl outl ) l/t of a , , NLP system. Context = { Sentencel , Sentence2 , ... , Sentence n\ ] Stenence i = \ [ Word i-I , Word i-2 , ... , Word i-j } John likes apples. Sentence 1 Word1-1 \ [ John\ ] POX : N BASE : John ... ... ... ... ... .. ub ./~o hn~ Word1-2 \ [ llktm\ ] ... ... ... ... ... . ~ POX : V BASE : like ... POX : N BASE : apple .~. Tom ah'o likes apples. Sentence 2 Word2-1 \ [ Tom\ ] POS : N BASE : Tom ... Word2-2 \ [ also\ [ POS : ADV BASF. : also. , Word2-3 \ [ likes\ ] POS : V BASE : like , .. Word2.4 \ [ apples\ ] POS : N BASE : apple .. He also likes oranges. Sentence 3 Word3-1 \ [ He\ ] POS : PN BASE : he ... Word3-2 \ [ also\ ] POX : ADV BASE : also. , Word3-3 \ [ likes\ ] POS : V BASE : like , .. Word3-4 \ [ oranges\ ] POS : N BASE : oranse , . , Figm'e 1 : Example of ~t context mod ( 'l su ( 'h a s ( 'ntenee , hfformation extracted front COlnpl ( `te 1 ) arses of w ( `ll-formed sentences 2 in a context model ( 'all b ( ` us ( ' ( l to cOlnlflete incolnl ) lete parses , in the f ( ) rm of partially parsed chunks that a bottomup 1 ) ars ( , r outlmts fl ) r ill-formed sentences by using a previously des ( 'ribed method ( N~Lsukawa , 1995 ) . On the other hand , fl ) r some sentences in a text , such as Time \ ] lies like an arrow , a syntactic t ) arser lltay gent , rate nlore thatl olle parse tree , owillg to the 1 ) r ( `sen ( -e of words that Call \ ] ) e ; Lssigned to more than one part of st ) eech , or to the l ) resen ( 'e of complicated coordinate structures</definiens>
				<definiens id="7">an use the tyt ) es of the l ) revious and subse ( \ [ lleltt sentences or 1 ) hras ( `s ( Sll ( 'h as sentence , llOllll phrase , verb 1 ) hrasc , anti so ( ) It ) an ( l the modifiermodifiee 1 ) atterns in the context model. Therefore , in the second step , tit ( : context model g ( `nerat ( `d in the firs { ; st ( ' 1 ) is refined by referring to information in the context model. First , the most l ) referable candidate parses are selected for sentences with multit ) le parses by referring to information on ea ( 'h sentence in the context model for which a parser lent'rated a single unified parse. Then , partiM parses of ill-forlned sentences</definiens>
				<definiens id="8">'d strll ( 'tllre~ and an `` ill-formed sent ( me ( ` '' means one that c ; mnot be pm'sed as a unified strncture. 825 the context model is as fi ) llows : candidate i ) arses , assign a score for each lnodifiermodifiee relationship that is fl ) und in the context model , and add u I ) the scores to assign a 1 ) reference value to the ( : andidate l ) arse. value. If more than one l ) ; ~rse has the highest t ) ref erenee wdue , go to the next ste 1 ) with those lmrses ; otherwise , leave this i ) ro ( 'edure. date parse that has the same tyl ) e of root node ( su ( 'h as noun phrase , verb l ) hrase , or sentence ) as the parse of the 1 ) receding sentence or the next senten ( 'e. wdue. If more than on ( ' parse has the highest 1 ) ref erence value , go to tit ( , next ste I ) with dtose 1 ) arses ; otherwise , leave this procedure. date parse based on heuristic ruh 's that assign scores to structures according to their grammatical preferability. ence value. If more than one t ) arse has the highest 1 ) reference wfiue , select the first parse in the list of the remmning candidate parses. Tile procedure of conq ) leting l ) artia\ ] \ ] ) kLl'ses of a.n ill-formed sentence consists of two steps : The part of st ) ee ( 'h mid the modifiee-modifier relationshil ) s with other words are inspe ( 'ted for each word in a 1 ) artial l ) arse. If the part of speech</definiens>
				<definiens id="9">the information in the context model. If the 1 ) artial l ) arses were not ratified into a singh '' structure in the previous step , they arc , joined together on tit ( `` l ) asis of modifier-modifiee relationshil ) 1 ) atterns in the ( 'ontext model so that a unified i ) arse is obtained. the context model Finally , in the third stel ) , ea , 'h senten ( 'e in the ( 'Olltext lnodel is mmlyzed individually , and its mnl ) iguities and context-dependent prol ) h'ms are resolved by referring to information on other sentences in the context model. The next section des ( 'ribes the 1 ) rocedures for problenl resolution , and explains lheir effectivene , ss in lint ) roving nmehine transla.don output. The a ( : cura ( 'y of syntactic analysis m~\y l ) e improved by refinement of the ( 'ontext nn ) del in tlt ( ' second step of the procedure. For ex~mlple , in an exl ) eriment on 244 sentences from a. chapter of a COml ) uter manual , in which we attempted to select the correct parse of a sentence from multiple candidate l ) arses</definiens>
				<definiens id="10">technical do ( 'ulnents , in more than h~flf of the incoml ) letely 1 ) ~trsed sentences , the lmrt.iM parses were joined into a single stru ( 'ture by using ilfformation in the context model. However , after the second step , ambiguities in each sentence are kept unresolved in the context model. Thus , we need to resolve problems in each sentence in the context model ill</definiens>
				<definiens id="11">the siml ) le context model , and how the results are refiecte ( l in improved machine translation outlmts. subjuncts Ih , solving the focus of fi ) cusing sul ) juncts such as also</definiens>
				<definiens id="12">'m tha.t requires ilffornmtion on the 1 ) revious context. Fo ( 'using sul ) jnncts ( lr~tw m.tention to a part of</definiens>
				<definiens id="13">the scope of also can 1 ) e To 'm , likes , the entire predicate ( the whole sent.enee except the subject Tom ) , or apple. % acc ( trding to the itrevious context. In this ( 'as ( ' , the preceding senten ( 'e , Joh , n likes apples , has the structure , A likes B , whereas sentence ( 2 ) has the structure , X also likes B , where B and the predi ( : ate fib , s are identical. The eoml ) arison of these two structures indicates that the new intbrmation X ( Tom ) is the scope of also in sentence ( 2 ) . The fl ) ( 'us of focusing sul ) jun ( 'ts ix resolved by means of the following algorithln : model one that contains expressions morphologically identical with those in the sentence containing the focusing suhjunet. the sentence containing the tl ) ( 'using subjunct with words or phrases in tit ( `` senten ( 'e extracted in ste l ) 1. as candidates for the focus , and select the remainder as the focus of the fo ( - , tsing su| ) junct. If more than one candidate remains , take the defaul }</definiens>
				<definiens id="14">n with and without information 1 ) rovi ( h~d by context pr ( t ( : essing. As shown in this figure , with ( tar the context information</definiens>
				<definiens id="15">'essing is apt ) lied , the focus of also ix determined to I ) e Tom in senten ( : e ( 2 ) and orange in sentence ( 3 ) . In our amtlysis of ( 'omlmter manuals , most nouns were repeated with the same expressions unless they were repla. ( 'ed by 1 ) ronouns or definite expressions su ( h as th , is , that , and tit ( '.. ( ) n the other hint ( I , predi ( -ates were sometimes repeated with different expressions. For exanlple : A has B. ~ A also includes C. A contains B. -- ~ C is also included in A. 826 ( 1 ) John likes al ) l ) l'.'s. \ [ With and 'vViihou ( . ( : ottt &lt; , xi\ ] I ) ep ( qidency SI rtl ( 'l iil '</definiens>
				<definiens id="16">2 ) Tom Mso likes a.l ) l ) l , ,s. \ [ wiu , ,..~ &lt; ' , . , , , .'x , l ( -~ -- -- '' L ~-_ 'l'ranslalh ) n : I '' ACJ.</definiens>
			</definition>
			<definition id="1">
				<sentence>By applying h ( mrisii ( ' rules according to which a , candi ( lat ( , that has h ( 'im frequ ( 'ntly r ( , pe~m~ ( l in th ( , 1 ) re ( 'eding sent ( m ( 'es and it candidate th~tt modifi ( , s the morl ) hoh ) gi ( 'a.lly id ( 'nti- ( : al predicat ( 's as tho 1 ) rollollll in i ; he same context are t ) referred , w ( , obt .</sentence>
				<definiendum id="0">'im frequ</definiendum>
				<definiens id="0">'es and it candidate th~tt modifi ( , s the morl</definiens>
				<definiens id="1">al predicat ( 's as tho 1 ) rollollll in i</definiens>
			</definition>
			<definition id="2">
				<sentence>( ' reversed ill the ll : ~ulsllt/ : ion of a. ( : Oml ) h , x senten ( 'e where an initim main ( 'lause ill a , sour ( ' ( , -lmtgmtge s ( , nt ( , n ( ' ( , ( 'om ( , s afl ( , r th ( ' sul ) ordin+tte ( 'l+ms ( ' in th ( ' target language , the r ( 't ' ( 'r ( mt , noun phr~ts ( ' shouhl be repbt ( 'ed with th ( ' I ) ronoull , to avoid ( 'ata .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">-lmtgmtge s ( , nt</definiens>
			</definition>
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>category = &gt; head_cat : { head = &gt; HEAD = &gt; v head : { } } } } } &lt; \ [ id : { syn = &gt; syn : { constype = &gt; phrasal : { constr = &gt; punct_att } , category = &gt; head_cat : { head = &gt; HEAD = &gt; v_head : { } } } } } , Id : { syn = &gt; syn : { constype = &gt; phrasal : { constr = &gt; ( punct_att ; paragraph ) } , category = &gt; head cat : { head = &gt; v_head } } } } \ ] .</sentence>
				<definiendum id="0">Id</definiendum>
				<definiens id="0">{ constr = &gt; punct_att } , category = &gt; head_cat : { head = &gt; HEAD = &gt; v_head : { } } } } } ,</definiens>
			</definition>
			<definition id="1">
				<sentence>p. 44 281 Predicate Logic is based on the syntax of the standard predicate logic , but proposes a new ( dynamic ) interpretation of the quantifiers and connectives which allows the binding of variables within and outside their scope , depending on the interpretation of the corresponding expressions of the natural language .</sentence>
				<definiendum id="0">Predicate Logic</definiendum>
				<definiens id="0">based on the syntax of the standard predicate logic , but proposes a new ( dynamic ) interpretation of the quantifiers and connectives which allows the binding of variables within and outside their scope , depending on the interpretation of the corresponding expressions of the natural language</definiens>
			</definition>
			<definition id="2">
				<sentence>The negation is a static semantic phenomenon and the value of its attribute 'quantifieational_forcc ' is 'static ' .</sentence>
				<definiendum id="0">negation</definiendum>
				<definiens id="0">a static semantic phenomenon</definiens>
			</definition>
</paper>

		<paper id="2171">
			<definition id="0">
				<sentence>Foreign Language Learning : a Psycholinguistic Analysis of the Issues .</sentence>
				<definiendum id="0">Foreign Language Learning</definiendum>
				<definiens id="0">a Psycholinguistic Analysis of the Issues</definiens>
			</definition>
			<definition id="1">
				<sentence>Machine Learning : an Artificial Intelligence Approach .</sentence>
				<definiendum id="0">Machine Learning</definiendum>
			</definition>
</paper>

		<paper id="2113">
			<definition id="0">
				<sentence>Maria welcomes her visit~visitors Maria begriiflt ihren Besuch v , , begriiflt ' , ihren Besuch ( 20 ) `` , begr/iflt ' , /ihren Maria ' , , ' '' Bes~// .</sentence>
				<definiendum id="0">Maria</definiendum>
				<definiens id="0">welcomes her visit~visitors Maria begriiflt ihren Besuch v</definiens>
			</definition>
			<definition id="1">
				<sentence>\ ] F l~'anz reads a book about ships ( 24 ) bYanz liest \ [ ein Buch filler \ [ ScHnq '' m\ ] F\ ] F The solution lies in the observation that the latter F-skeleton entails the former .</sentence>
				<definiendum id="0">F l~'anz</definiendum>
				<definiens id="0">reads a book about ships ( 24 ) bYanz liest \ [ ein Buch filler \ [ ScHnq '' m\ ] F\ ] F The solution lies in the observation that the latter F-skeleton entails the former</definiens>
			</definition>
			<definition id="2">
				<sentence>( 31 ) F-Skeleton Instantiation The F-SKEL value of a word with \ [ O-SEM ( ~\ ] is ( i ) the function variable ( a ) , if ~ is h_linked to foc ; ( ii ) a , if the word is marked \ [ MAX-F __\ ] ; 15 ( iii ) eomposition ( a , fl ) ( where fl is the F-SKEL value of the word 's complement ) , if the word is marked \ [ MAX-F J-J , but a is not h_linked to foe .</sentence>
				<definiendum id="0">fl</definiendum>
				<definiens id="0">the F-SKEL value of the word 's complement</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>Lexicalized Tree Adjoining Grammar ( LTAG ) is a formalism integrating lexicon and grammar ( Joshi , 87 ; Schabes et al. , 88 ) .</sentence>
				<definiendum id="0">Lexicalized Tree Adjoining Grammar ( LTAG )</definiendum>
			</definition>
			<definition id="1">
				<sentence>LTAGs consist of a morphological lexicon , a syntactic qexicon of lemmas and a set of tree schemata , i.e. trees in which the lexical anchor is missing .</sentence>
				<definiendum id="0">LTAGs</definiendum>
				<definiens id="0">consist of a morphological lexicon , a syntactic qexicon of lemmas and a set of tree schemata , i.e. trees in which the lexical anchor is missing</definiens>
			</definition>
			<definition id="2">
				<sentence>A tree family contains the different possible trees for a given canonical subcategorization ( or predicate-argument structure ) .</sentence>
				<definiendum id="0">tree family</definiendum>
				<definiens id="0">contains the different possible trees for a given canonical subcategorization ( or predicate-argument structure )</definiens>
			</definition>
			<definition id="3">
				<sentence>The writing of an I , TAG is a rather fastidious task ; its extension and/or maintenance is very difficult , since maintaining the grammar means for instance adding an equation to hundreds of trees .</sentence>
				<definiendum id="0">TAG</definiendum>
			</definition>
			<definition id="4">
				<sentence>descriptions of trees The hierarchy is a strict multiple inheritance network whose terminal classes represent the elementary trees of the LTAG .</sentence>
				<definiendum id="0">hierarchy</definiendum>
				<definiens id="0">a strict multiple inheritance network whose terminal classes represent the elementary trees of the LTAG</definiens>
			</definition>
			<definition id="5">
				<sentence>A partial description is a set of constraints that characterizes a set of trees .</sentence>
				<definiendum id="0">partial description</definiendum>
			</definition>
			<definition id="6">
				<sentence>A dominance link can be further specified as a path of length superior or equal to zero .</sentence>
				<definiendum id="0">dominance link</definiendum>
			</definition>
			<definition id="7">
				<sentence>The common factor to various expressions of linguistic principles made for \ [ , TAGs is the argument-predicate co-occurrence principle ( Kroch and Joshi , 85 ; Abeill6 , 91 ) : the trees for a predicative item contain positions for all its arguments .</sentence>
				<definiendum id="0">TAGs</definiendum>
				<definiens id="0">the argument-predicate co-occurrence principle</definiens>
			</definition>
			<definition id="8">
				<sentence>IIyTAG : a new type of Tree Adjoining Grammars for Hybrid Syntactic Representation of Free Order Languages , PhD thesis , University of Saarbr6cken .</sentence>
				<definiendum id="0">IIyTAG</definiendum>
				<definiens id="0">a new type of Tree Adjoining Grammars for Hybrid Syntactic Representation of Free Order Languages</definiens>
			</definition>
</paper>

		<paper id="1073">
			<definition id="0">
				<sentence>For instance , the FSV of ( la ) is defined as ( lb ) , the set of properties of the form like-lug y where y is an individual ( in what follows , focus is indica~ted using upper-case ; we also follow Montague 's convention that for any type % D~ is the set of objects of type r and wff~ is the set of wits of type r ) .</sentence>
				<definiendum id="0">y</definiendum>
				<definiendum id="1">wff~</definiendum>
				<definiens id="0">the set of wits of type r</definiens>
			</definition>
			<definition id="1">
				<sentence>In what follows , we assume a restriction similar to the DSP 's Primary Oeeurren ( : e Restriction ( l ) ah'ymple et al. , 1991 ) 's : the occurrence directly associated with the focus is a primary occurrence and any solution containing a primary occurrence is discarded as linguistically invalid .</sentence>
				<definiendum id="0">e Restriction</definiendum>
				<definiens id="0">a primary occurrence and any solution containing a primary occurrence is discarded as linguistically invalid</definiens>
			</definition>
			<definition id="2">
				<sentence>Igu'thermore in ( 310 , t , hc SCOl ) e o\ [ `` &lt; mlgl is the whole Vl ) read Uw letters / , hal , 51/15'~ .sesd , I , o lb'l ULI. \ [ l &lt; ; li ( : ( 1 , if no qua nl , ilier raising ; o ( : ( : urs , oltlyl associates with bot , h 577t£ , an ( I I ) AIJLI. '\ [ '\ [ it/S ill or ( let to g ( ; nerat ; &lt; ; l ; h &lt; ; desired reading , oq'U/ ( 2 Hltlsl ; 13 ( ; lllovod out , of { lie scoI ) o , o\ [ `` ol/ly i . l\ [ owevcr , since the NI &gt; ihc letters l , hat ,5'UI ' ; : , sold , to PAULt is a scope island , quanl ; ilier raising in in'/I ) ossible , l lence , the desir ( 'd l : eading ( : a , illiOt 1 ) &lt; ; ge .</sentence>
				<definiendum id="0">mlgl</definiendum>
				<definiens id="0">a scope island , quanl ; ilier raising in in'/I ) ossible , l lence , the desir ( 'd l : eading</definiens>
			</definition>
			<definition id="3">
				<sentence>l ; cria , I is d ( : a ( : c ( ; nl ; ( ; d , thai , is , it , is char a , cl ; &lt; ~risec/ by a , ll illll ) orl , a , lll ; r ( ; ( lll ( : t ; R ) n ill pit , oh , ; 'till- '' plii ; ude and dural ; ion ( ci\ [ ' .</sentence>
				<definiendum id="0">ion</definiendum>
				<definiens id="0">t ; R ) n ill pit , oh</definiens>
			</definition>
			<definition id="4">
				<sentence>l ( , , , , , ,0\ ] a , , ( s , ) - : Vl'\ [ l ' ~ ( ; &lt; l A t'0 , ) + s , A~ , .l ( . , , , , , ) \ ] ( , ' , l ( s , ' ) _ A. ; ./ ( . , , , , , . ) Sin &lt; : ( ~ ncil , liel : ( Td lior \ ] '' o ( : tt8 ~-tre , initially givon , the third e &lt; llla , l , ioli ; tl ) ovc is uni , yl ) C ( t a.ll ( I ( : ~tliiiol ; I ) e solved t ) y I luel , 's algorit ; hnP. \ [ n I , ha.t , sit , ual , ion , we &lt; : au &lt; fit , her ilsslilliO. SOIIIC delaying inechanisnl or some exl.erision o\ [ I luet , 'n algorithm t ; hag can cope with I &gt; yi ) ( ' .</sentence>
				<definiendum id="0">tl ) ovc</definiendum>
				<definiens id="0">~tliiiol ; I ) e solved t ) y I luel , 's algorit ; hnP. \ [ n I , ha.t</definiens>
			</definition>
</paper>

		<paper id="2114">
			<definition id="0">
				<sentence>The entire corpus of 1 million words has passed through this stage of manual disambiguation and annotation , which makes it an important standard that can be used as a tool , e.g. , when training probabilistic taggers .</sentence>
				<definiendum id="0">annotation</definiendum>
				<definiens id="0">a tool , e.g. , when training probabilistic taggers</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>HorizontM relations ( between objects ) are in principle pretty much unconstrained .</sentence>
				<definiendum id="0">HorizontM</definiendum>
				<definiens id="0">relations ( between objects ) are in principle pretty much unconstrained</definiens>
			</definition>
			<definition id="1">
				<sentence>( iONS attribute -- -for to put that would be on-contact in order to make sure that the locatum argument always surfaces as the direct object ; of the verb predicate .</sentence>
				<definiendum id="0">iONS</definiendum>
				<definiens id="0">the direct object</definiens>
			</definition>
</paper>

		<paper id="2099">
			<definition id="0">
				<sentence>A linky string is a series of letters extracted from a corpus using statistical intbrmation only .</sentence>
				<definiendum id="0">linky string</definiendum>
				<definiens id="0">a series of letters extracted from a corpus using statistical intbrmation only</definiens>
			</definition>
			<definition id="1">
				<sentence>The idea of bigrams and trigrams is often used in studies on NI , P. Wgram is the information of the association between n certain events .</sentence>
				<definiendum id="0">Wgram</definiendum>
				<definiens id="0">the information of the association between n certain events</definiens>
			</definition>
			<definition id="2">
				<sentence>A score graph has the letters in a sentence on the x-axis and linking scores on the y-axis ( Figure 5 ) .</sentence>
				<definiendum id="0">score graph</definiendum>
				<definiens id="0">has the letters in a sentence on the x-axis and linking scores on the y-axis ( Figure 5 )</definiens>
			</definition>
			<definition id="3">
				<sentence>When LcN checks a three-letter morpheme ABC , with bigram data it can see the string only as A-B and B-C .</sentence>
				<definiendum id="0">LcN</definiendum>
				<definiendum id="1">ABC</definiendum>
				<definiens id="0">checks a three-letter morpheme</definiens>
			</definition>
			<definition id="4">
				<sentence>S decides to segment between AB and B-C .</sentence>
				<definiendum id="0">S</definiendum>
			</definition>
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>Informativeness and relewmce , therefore , are not only functions of the purpose ( s ) of the exchange of infornmtion but also of the knowledge of the interlocutor .</sentence>
				<definiendum id="0">Informativeness</definiendum>
				<definiens id="0">s ) of the exchange of infornmtion but also of the knowledge of the interlocutor</definiens>
			</definition>
			<definition id="1">
				<sentence>S : no departure at 7:20 .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">no departure at 7:20</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>Ahnost every character is a word and inost words are of one or two characters long but there are also abundant wor ( ls longer than two characters .</sentence>
				<definiendum id="0">Ahnost every character</definiendum>
				<definiens id="0">a word</definiens>
			</definition>
			<definition id="1">
				<sentence>All ( tut/li ( ; ate entries are coral ) trier1 , i.e. , if ( ~ is a word as well as a suflix , tilt ; two entries arc combined into one with a , n indication that it ; can serve as a word as well as a suffix .</sentence>
				<definiendum id="0">tut/li</definiendum>
				<definiendum id="1">ate entries</definiendum>
				<definiens id="0">a word as well as a suflix , tilt ; two entries arc combined into one with a , n indication that it ; can serve as a word as well as a suffix</definiens>
			</definition>
			<definition id="2">
				<sentence>a~ , ~ , __l~r/v~ where a is the standard deviation of error rates in generation i1 , and n is the number of blocks to be segmented in generation i. to.5~ , n-1 is the density function of ( 0.5a , n 1 ) degrees of freedom ( Devore , 1991 ) .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
			<definition id="3">
				<sentence>Lexical analysis is a basic process of analyzing and understanding a language .</sentence>
				<definiendum id="0">Lexical analysis</definiendum>
				<definiens id="0">a basic process of analyzing and understanding a language</definiens>
			</definition>
</paper>

		<paper id="2123">
			<definition id="0">
				<sentence>We propose a detinition of structural colnI ) h~xil ; y ( SC ) such thai ; sentences ranked by our definition as more complex are generally more difficult for humans to process than otherwise similar sentences , hi other words , suppose a pair of sentences A arid B consist of the same set of words and have essentially the same meaning , then sentence A is more difficult to process than sentence 1~ if SC ( A ) &gt; SC ( B ) .</sentence>
				<definiendum id="0">SC</definiendum>
				<definiendum id="1">SC</definiendum>
				<definiendum id="2">A ) &gt; SC</definiendum>
				<definiens id="0">a pair of sentences A arid B consist of the same set of words and have essentially the same meaning</definiens>
			</definition>
			<definition id="1">
				<sentence>In dependency grammars ( Hudson , 1984 ; Mel'Suk , 1987 ) , a dependency relationship is a primitive relationship between two words , called the head and the modifier .</sentence>
				<definiendum id="0">dependency grammars</definiendum>
				<definiens id="0">a primitive relationship between two words</definiens>
			</definition>
			<definition id="2">
				<sentence>The modifiers of a word w are the head words of the specifier , complements , and adjuncts of w. For example , Figure 1 is the X-bar structure of ( 2 ) .</sentence>
				<definiendum id="0">Figure 1</definiendum>
				<definiens id="0">the head words of the specifier , complements , and adjuncts of w. For example ,</definiens>
			</definition>
			<definition id="3">
				<sentence>Structural complexity measures how easy or di\ [ \ [ icnlt it is to establish these dependency links .</sentence>
				<definiendum id="0">Structural complexity</definiendum>
				<definiens id="0">measures how easy or di\ [ \ [ icnlt it is to establish these dependency links</definiens>
			</definition>
</paper>

		<paper id="1086">
			<definition id="0">
				<sentence>Similarity methods can be broadly divided into `` relation based '' methods which use relations in an ontology to determine similarity and `` distribution based '' methods which use statistical analysis as the basis of similarity judgements .</sentence>
				<definiendum id="0">Similarity methods</definiendum>
				<definiens id="0">relation based '' methods which use relations in an ontology to determine similarity and `` distribution based</definiens>
			</definition>
			<definition id="1">
				<sentence>'l'here are a variety of similarity measures awu\ ] able for sets of \ [ ; ~atm'es , biLL all make their eoml ) arisons t ) ase ( l on some combination of shared \ [ 'etltlH ; es , disLilleL \ [ 'eal ttres , altd sharect ttl ) sellL l'ea- .</sentence>
				<definiendum id="0">biLL</definiendum>
				<definiens id="0">ttres , altd sharect ttl</definiens>
			</definition>
			<definition id="2">
				<sentence>HI~X'I'ANT ( ( ~reii : nstette,94 ) introduce ( 1 the \Veighted 3aeeard Measure which combitms the Jaeeard Measure with weights derive ( l froth an inh ) rmation theoreti &lt; : anMysis of % ature occurfences+ '\ ] 'he we : ight of a feature is com\ [ mte ( l from a global weight ( based on the nmuber of glohal occurrences of the , wor ( l or concept ) and a \ [ ( ) ( : at weight ( based Oil the \ [ 're ( lllellcy Of tlt+ &gt; Features atlaehed to the word ) .</sentence>
				<definiendum id="0">HI~X'I'ANT</definiendum>
				<definiendum id="1">wor</definiendum>
				<definiens id="0">at weight ( based Oil the \ [ 're ( lllellcy Of tlt+ &gt; Features atlaehed to the word )</definiens>
			</definition>
			<definition id="3">
				<sentence>IFSM consists of a hierarchical conceptual thesaurus , a set of distinctive features assigned to each object and weightings of the features .</sentence>
				<definiendum id="0">IFSM</definiendum>
			</definition>
			<definition id="4">
				<sentence>J ~0 , lo/ /- &lt; so ~ , ~ h , ~.~ ki , ,.51 ... . ... ... ... ... ... ... ... i , , , , , il ... ... ... ... .. i ... ... ... ... ... W ... ... ... ... i ... ... ... . O-iso ~ , , , ,~c Jo~ ¢ , ~ , - , , ) 0 0 ( 9 0 i ..e % ed . .r , ooe Tri , l s ... ... ... ... N ... ... ... ... .. N ... ... ... ... .. ... ... . i O O O¢ O O .O O O O O O L. Deep Triples ... ... ... ... ~~ ... ... . \ [ ... ... .. ~ ... . / / / / ( SO v228 n5 n9 ... 5.3 32 ( `` dll , Se '' `` nm after '' ) ( `` dog '' `` hound '' ~ { `` cat '' `` kitty '' ) ) : ~0 0 O '' 0 0 0 0 0 i , , , II 1 1 • Abstracted Triples ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... I I I I Filtering Heuristics I 0 0 , , , Filtered Abstracted Triples ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Fig.4 Abstracted triple extraction from corpus pus as a resource of features , and apply class based abstraction ( Resnik 95a ) to triples to reduce the size of the possible feature space. As mentioned above , features extracted fi'om the corpus will be represented using synsets/concepts in IFSM. Since no large scale corpus data with semantic tags is available , the current implementation of IFSM has a word sense disambiguation problem in obtaining class probabilities. Our current basic strategy to this problem is similar to ( Resnik,95a ) in the sense that synsets associated with one word are assigned uniform frcquency or `` credit '' when that word appears in the corpus. We call this strategy the `` brute-force '' approach , like Resnik. On top of this strategy , we introduce filtering heuristics which sort out unreliable flata using heuristics based on the statistical properties of the data. This section describes the feature extraction procedure. If a sentence `` a dog chased a cat '' appears in the corpus , features representing `` chase cat '' and `` dog chase '' may be attached to `` dog '' and `` cat '' respectively. Fig 4 shows the overall process used to obtain a set of abstracted triples which are sources of feature and weighting sets for synsets. from the corpus Typed surface triples are triples of surface words holding some fixed linguistic relations ( Hereafter call this simply `` surface triples '' ) . The current implementation has one type `` SO '' which represents 510 `` subject verb object '' relation. A set of typed surface triples are extracted from a corpus with their frequencies. Surface triple set ( TYPE VERB NOUN1 NOUN2 FREQUENCY ) Fx. ( SO `` ch~se '' `` &lt; log '' `` cat '' 10 ) triples Surface triples are expanded to corresponding deep triples ( triples of synset IDs ) by expanding each surface word to its corresponding synsets. The frequency of the surface triples is divided by the number of generated deep triples and it is assigned to each deep triple. The frequency is also preserved ~ it is as an occurrence count. Surface words are also reserved for later processings. Deep triple collection ( TYPE V-SYNSE'F N1-SYNSET N2-SYNSEq ' FREQENCY OCCUttRENCE V-WORD NI-WORD N2-WORI ) ) Ex. ( SO v123 n5 n9 0.2 10 `` chase '' `` &lt; log '' `` cat '' ) `` v123 '' and `` n5 '' are synset IDs corresponding to word `` chase '' and `` dog '' respectively , These deep triples are sorted and merged. The frequencies and the occurrence counts are summed up respectively. The surface words are merged into surface word lists as the following example shows. Deep triple set ( TYPE V-SYNSET N1-SYNSEq ' N2-SYNSET FREQUENCY OCCURRENCE V-WOttDS N1-WORDS N2-WORDS ) gx. ( SO v123 n5 n9 0.7 15 ( `` ch msc '' ) ( `` dog '' `` hou nd `` ) ( `` cat `` ) ) In this example , `` dog '' and `` hound '' have same synset ID `` n9 '' . The purpose of the following phases is to extract featm : e sets for each synset in an abstracted form. In an abstracted form , the size of each lhature space becomes tractable. Abstraction of a syuset can be done by divid~ ing whole synsets into the appropriate number of synset groups and determining a representative of each group to which each member is abstracted. There are several methods to decide a set of synset groups using a hierarchical structure. One of the simplest methods is to make groups by cutting the hierarchy structure at some depth from the root. We call this the flat-depth grouping method. Another method tries to make the nmnber of synsets in a group constant , i.e. , the upper/lower bound for a number of concepts is given as a criteria ( ttearst,93 ) . We call this the flat-size grouping method. In our implementation , we introduce a new grouping method called the flat-probability grouping method in which synset groups are specified such that every group has the same class probabilities. One of the advantages of this method is that it is expected to give a grouping based on the quantity of information which will be suitable for the target task , i.e. , semantic abstraction of triples. The degree of abstraction , i.e. , the number of groups , is one of the principal factors in deciding the size of the feature space and the preciseness of the features ( power of description ) . Each synset of deep triples is abstracted based on the flat-probability grouping method. These abstracted triples are sorted and merged. Original synset IDs are maintained in this processing for feature extraction process. The result is called the abstracted deep triple set. Abstracted deep triple set ( TYPE V-ABS-SYNSET NJ-ABS-SYNSET N2-ABS-SYNSFT V-SYNSEq'-LISq ' N1-SYNSE'r-LIS'f N2-SYNSET-LIST SYN-FREQUENCY OCCURRENCE V-WORDS NI-WORDS N2-WORDS ) Ex. ( SO v28 n5 n9 ( v123 v224 ) ( n5 ) ( n9 n8 ) 5.3 32 C c ! ... . `` `` ru n `` 'after '' ) C dog '' `` hound '' ) C cat '' `` kit ty '' ) ) Synset `` v28 '' is an abstraction of synset `` v123 '' and synset `` v224 '' which corresponds to `` chase '' and `` run_after '' respectively. Synset `` ng '' con : esponding to `` cat '' is an abstraction of synset `` nS '' corresponding to `` kitty '' . heuristics Since the current implementation adepts the `` brute-force '' approach , almost all massively generated deep triples are fake triples. The filtering process reduces the number of abstracted triples using heuristics based on statistical data attached to the abstracted triples. There are three types of statistical data available ; i.e. , estimated frequency , estimated occurrences of abstracted triples and lists of surface words. \ [ ler % the length of a surface word list associated with an abstracted synset is called a surface support of the abstracted synset. A heuristics rule using some fixed frequency threshold and a surface support bound are adopted in the current implementation. abstracted triple set This section describes a method for obtaining features of each synset. Basically a feature is typed binary relation extracted from an abstracted triple. From the example triple , ( SO v28 115 n9 ( v12a v224 ) ( , ,5 ) ( , ,9 ns ) , ~ , a a~ ( `` chase '' `` run `` 'after '' ) ( `` dog '' `` hound '' ) ( `` cat '' `` kitty '' ) ) the following features are extracted for three of the synsets contained in the above data. n5 ( ov v28 n9 5.3 32 ( `` chase '' `` run '' 'after '' ) ( `` cat '' `` kitty '' ) ) i19 ( sv v2S n5 5.3 32 ( `` chase '' `` run `` 'after '' ) ( `` dog '' `` hound '' ) ) n8 ( sv v28 n5 5.3 32 ( `` chase '' `` run '' 'after '' ) ( 'dog '' `` hound '' ) ) An abstracted triple represents a set of exmnples in the text corpus and each sentence in the corpus usually describes some specific event. This means that the content of each abstracted 511 triple can not be treated as generally or universally true. For example , even if a sentence `` a man bit a dog '' exists in the corpus , we can not declare that `` biting dogs '' is a general property of `` man '' . Metaphorical expressions are typical examples. Of course , the distributional semantics approach assumes that such kind of errors or noise are hidden by the accumulation of a large number of examples. However , we think it might be a more serious problem because many uses of nouns seem to have an anaphoric aspect , i.e. , the synset which best fits the real world object is not included in the set of synsets of the noun which is used to refer to the real world object. `` The man '' can be used to express any descendant of the concept `` man '' . We call this problem the word-referent disambiguation problem. Our approach to this problem will be described elsewhc're. Preliminary experiments on feature extraction using 1010 corpus In this section , our preliminary experiments of the feature extraction process are described. In these experiments , we examine the proper granularity of abstracted concepts. We also discuss a criteria for evaluating filtering heuristics. WordNet 1.4 , 1010 corpus and Brown corpus are utilized through the exI ) eriments. The 1010 corpus is a multiqayered structured corpus constructed on top of the FRAMEIX-D knowledge representation language. More than 10 million words of news articles have been parsed using a multi-scale parser and stored in the corpus with mutual references to news article sources , parsed sentence structures , words and WordNet synsets. grouping To examine the appropriate number of abstracted synsets , we calculated three levels of abstracted synset sets using the fiat probability grouping method. Class probabilities for noun and verb synsets are calculated using the brute force method based on 280K nouns and 167K verbs extracted fl'om the Brown eortms ( 1 million words ) . We selected 500 , 1500 , 3000 synset groups for candidates of feature description level. The 500 node level is considered to be a lowest boundary and the 3000 node level is expected to be the tarI ) epth 1 2 3 4 5 6 7 8 Synsets 611 122 966 2949 5745 12293 8384 7408 Depth 9 10 11 12 13 14 15 16 Synsets 5191 3068 1417 812 314 94 36 6 Table 1. Depth/Noun_Synsels in WordNet 1.4 Level 500 ( 518 synsets ) 1 ( structure construction\ ] ( 72\ ] 9.47 4 ) : a thing constructed ; a con~.plex eonstruetioI'l or entity 2 { time_period period period_of_tilne\ ] ( 6934 3 ) : a length ef time ; `` government services began during the colonial period '' 3 { organization\ ] ( 6469.94 4 ) : a group of people who work together 4 { action } ( 6370.54 9 ) : something done ; 5 { natural_object } ( 6277.26 3 ) : an object occurring naturally ; Level 3000 ( 3001 synsets ) 1 { natural language tongue mother tongue\ ] ( 678.7 6 ) : the language of a community~ 2 { weapon arm weapon_system\ ] ( 673.7 ( ~ 6 ) : used in fighting or hunting 3 { head chief top_dog } ( 671.55 5 ) : 4 { capitalist } ( 669.45 4 ) : a person who believes in the capitalistic system 5 { point point_in_~ime } ( 669.29 8 ) : a parti.cular clock time ; Table 2 : Synsets I ) y flat-probal ) illty grouping metho ( 1 get abstraction level. This expectation is based on the observation that 3000 node granularity is empirically sulficient for deseribing the translation patterns for selecting the proper target Fmglish verb for one Japanese verb ( lkehara,93 ) . Table 1 shows the average synset node depth and the distribution of synset node depth of WordNet1.4. Table 2 lists the top five noun synsets in the fiat probability groupings of 500 and 3000 synsets. `` { } '' shows synset. The first and the second number in `` 0 '' shows the class frequency and the depth of synset respectively. Level 500 grout ) ings contain a very abs|racted level of synsets such as `` action '' , `` time_period '' and `` natural_object '' . This level seems to be too general for describing the features of objects. In contrast , the level 3000 groupings contains `` natural_language '' , `` weapotf ' , `` head , chief ' , and `` point_in_time '' which seems to be a reasonable basis for feature description. There is a relatively big depth gap between synsets in the abstracted synset group. F , ven in the 500 level synset group , there is a two-depth gap. In the 3000 level synset group , there is 4 depth gap between `` capitalist '' ( depth 4 : ) and `` point_in_time '' ( depth 8 ) . The interesting point here is that `` point_in_time '' seems to be more at ) . stract than `` capitalist , `` inluitively speaking. The actual synset numbers of each level of synset groups are 518 , 15 % 8 , and 3001. 'fhus the fiat probability grouping method can precisely control the lew'J of abstraction. Considering the possible abstraction levels available by the fiatdepth method , i.e. , depth 2 ( 122 synsets ) , depth 3 ( 966 synsets ) , depth 4 ( 2949 synsets ) , this is a great advantage over the flat probability grouping. 1010 corpus A preliminary experiment for obtaining abstract triples as a basis of features of synsets was conducted. 82,703 surface svo triples are extracted from the 101.0 corpus. Polarities of abstracted triple sets for 500 , 1500 , 3000 level abstraction are 1.20M , 2.03M and 2.30M respectively. Each 512 Level 1500 1 { organization } { talk sp , :ak utter mouth verbulize vurbi~v } { organization\ ] ( 70,4.24,1| ) 8 ) 2 { organization ) { talk spcal~ utt &lt; r inottth v , rl &gt; aliz &lt; ver/- , ify } { action } ( 5 ( ; , 'La5,112 ) 3 { organization } { change ttndergo~a_change becozlLe_ ( liitk : rent } { l &gt; ossession } ( 60,2.83,188 ) 4 torgauization } { talk speak utter mouth vez'lmlizu vcrbi~ , \ ] { ... ...</sentence>
				<definiendum id="0">abstracted triple</definiendum>
				<definiens id="0">one of the principal factors in deciding the size of the feature space and the preciseness of the features ( power of description ) . Each synset of deep triples is abstracted based on the flat-probability grouping method. These abstracted triples</definiens>
				<definiens id="1">a multiqayered structured corpus constructed on top of the FRAMEIX-D knowledge representation language. More than 10 million words of news articles have been parsed using a multi-scale parser and stored in the corpus with mutual references to news article sources , parsed sentence structures</definiens>
				<definiens id="2">an object occurring naturally</definiens>
				<definiens id="3">a relatively big depth gap between synsets in the abstracted synset group. F</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>For example , an anaphoric reference should be said to be ambiguous • if several possible referents appear in the representation ( several proper representations ) , • and also if the referent is simply marked as unknown , which causes no disjunction .</sentence>
				<definiendum id="0">unknown</definiendum>
				<definiens id="0">causes no disjunction</definiens>
			</definition>
			<definition id="1">
				<sentence>We take it for granted that , for each considered representation system , we know how to define , R~r each fragment V of an utterance U having a proper representation P , tile part of P which represents V. For example , given a context-free grammar and an associated tree structure P for U , the part of P representing a substring V of U is the smallest subtree Q containing all leaves corresponding to V. Q is 121 not necessarily the whole subtree of P rooted at the root of Q. Conversely , for each part Q of P , we suppose that we know how to define the fragment V of U represented by Q. Let P be a proper representation of U. Q is a minimal underspecifiedpart of P if it does not contain any strictly smaller underspecified part Q ' .</sentence>
				<definiendum id="0">Conversely</definiendum>
				<definiens id="0">R~r each fragment V of an utterance U having a proper representation P , tile part of P which represents V. For example , given a context-free grammar and an associated tree structure P for U , the part of P representing a substring V of U is the smallest subtree Q containing all leaves corresponding to V. Q is 121 not necessarily the whole subtree of P rooted at the root of Q.</definiens>
				<definiens id="1">a minimal underspecifiedpart of P if it does not contain any strictly smaller underspecified part Q '</definiens>
			</definition>
			<definition id="2">
				<sentence>I An ambiguity occurrence , or simply ambiguity , A , of multiplicity n ( n &gt; 2 ) relative to a representation system R , may be formally defined as : A = ( U , V , &lt; P1 , P2 ... Pm &gt; , &lt; Pl , P2 ... Pn &gt; ) , where m &gt; n and : U is a complete utterance , called the context of the ambiguity .</sentence>
				<definiendum id="0">U</definiendum>
				<definiens id="0">A = ( U , V , &lt; P1 , P2 ... Pm &gt; , &lt; Pl , P2 ... Pn &gt; )</definiens>
				<definiens id="1">a complete utterance , called the context of the ambiguity</definiens>
			</definition>
			<definition id="3">
				<sentence>V is a fragment of U , usually , but not necessarily connex , the scope of the ambiguity .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">a fragment of U , usually , but not necessarily connex</definiens>
			</definition>
			<definition id="4">
				<sentence>P1 , P2 ... Pm are all proper representations of U in R , and Pl , P2 ... Pn are the parts of them which represent V. For any fragment W of U strictly contained in V , if ql , q2 ... qn are the parts of Pl , P2 .</sentence>
				<definiendum id="0">qn</definiendum>
				<definiens id="0">the parts of them which represent V. For any fragment W of U strictly contained in V , if ql</definiens>
			</definition>
			<definition id="5">
				<sentence>Finally , An ambiguity pattern is a schem~i wfth variables which can be instantiated to a ( usually unbounded ) set of ambiguity kernels .</sentence>
				<definiendum id="0">ambiguity pattern</definiendum>
				<definiens id="0">a schem~i wfth variables which can be instantiated to a ( usually unbounded ) set of ambiguity kernels</definiens>
			</definition>
			<definition id="6">
				<sentence>UTTERANCES \ [ 1.1\ ] AA : Good morning , conference office ( 1 ) ( ambiguity EMMI10a-l-2.2.8.3 ( ( scope `` conference office '' ) ( status expert_system ) ( type address ( *speaker *hearer ) ) ( importance not-important ) ( multimodal facial-expression ) ( desambiguation_scope definitive ) ) ) \ [ 1.2\ ] AA : How can I help you ?</sentence>
				<definiendum id="0">UTTERANCES</definiendum>
				<definiens id="0">multimodal facial-expression ) ( desambiguation_scope definitive )</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>Thc system needs to know how words are clustered in semantic classes , and how semantic classes are hierarchically organised .</sentence>
				<definiendum id="0">Thc system</definiendum>
				<definiens id="0">needs to know how words are clustered in semantic classes , and how semantic classes are hierarchically organised</definiens>
			</definition>
			<definition id="1">
				<sentence>The measure ( ) 1 ' conceptual distance among concepts we are looking for should be scnsflive Io : • the length of the shortest palh that connects lhe concepts involved .</sentence>
				<definiendum id="0">conceptual distance</definiendum>
				<definiens id="0">• the length of the shortest palh that connects lhe concepts involved</definiens>
			</definition>
			<definition id="2">
				<sentence>W W0~d to be disarlJ0iguated : W Context words : wl w2 w3 w4 ... Figure 1 : senses of a word in WordNet Given a concept c , at the top of a sulfifierarchy , and given nhyp ( mean number of hyponyms per node ) , the Conceptual Density for c when its subhierarchy contains a number m ( nmrks ) of senses of the words to disambiguate is given by the \ [ ormula below : mI Z .0 20 nh37~ CI ) ( c , m ) , ::0 descendants , , ( 1 ) l ; ornlula I shows a lmralneter that was COlnputed experimentally .</sentence>
				<definiendum id="0">ornlula I</definiendum>
				<definiens id="0">senses of a word</definiens>
				<definiens id="1">mean number of hyponyms per node ) , the Conceptual Density for c when its subhierarchy contains a number m ( nmrks ) of senses of the words to disambiguate is given by the \ [ ormula below</definiens>
			</definition>
			<definition id="3">
				<sentence>t ( tree ) Figure 2 : algori ( hm for each window 17 First , the algorithm represents in a lattice the nouns present in the window , their senses and hypernyms ( step 1 ) .</sentence>
				<definiendum id="0">algorithm</definiendum>
			</definition>
			<definition id="4">
				<sentence>Precision ( that is , the percentage of actual answers which were correct ) and recall ( that is , the percentage of possible answers which were correct ) are given in terms of polysemous nouns only .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the percentage of actual answers which were correct ) and recall ( that is , the percentage of possible answers which were correct ) are given in terms of polysemous nouns only</definiens>
			</definition>
			<definition id="5">
				<sentence>Sussna disambiguates several documents from a public corpus using WordNet .</sentence>
				<definiendum id="0">Sussna</definiendum>
				<definiens id="0">disambiguates several documents from a public corpus using WordNet</definiens>
			</definition>
			<definition id="6">
				<sentence>Conceptual Density overcomes the combinatorial explosion extending the notion of conceptual distance from a pair of words to n words , and therefore can yield more than one correct sense for a word .</sentence>
				<definiendum id="0">Conceptual Density</definiendum>
				<definiens id="0">overcomes the combinatorial explosion extending the notion of conceptual distance from a pair of words to n words</definiens>
			</definition>
			<definition id="7">
				<sentence>WordNet provides sinonymy , hypernymy and meronyny relations for nouns , but other relations are missing .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">nouns , but other relations are missing</definiens>
			</definition>
			<definition id="8">
				<sentence>For instance , WordNet lacks eross-categorial semantic relations , which could he very useful to extend the notion of Conceptual Density of nouns to Conceptual Density of words .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">lacks eross-categorial semantic relations , which could he very useful to extend the notion of Conceptual Density of nouns to Conceptual Density of words</definiens>
			</definition>
</paper>

		<paper id="2158">
			<definition id="0">
				<sentence>where binds is a relation which is defined on the NP nodes of the ( surface ) phrase structure tree : Definition 2 ( the binding relation ) Node X binds Node Y if and only if X and Y arc coindexed and X e-commands Y. where ( definitions vary slightly ) : Definition 3 ( the c-command relation ) Node X e-commands node Y if and only if the next b~nnehing node which dominates X also dominates Y and neither X dominates Y , Y dominates X nor X=Y. The central part of the Binding Theory develops the notion of local domain to which binding principles A , B , and C refer as binding category : Definition 4 ( binding category ) Node X is binding category of node Y if and only if X is the next node which dominates Y , and which contains a subject that e-commands Y. Due to these definitions , the acceptability judgements for the data presented above are reproduced by binding principles A , B , and C. For each example , the subject demarcating the ( local ) binding category is just the ordinary subject of the subordinate clause .</sentence>
				<definiendum id="0">binds</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">a relation which is defined on the NP nodes of the ( surface ) phrase structure tree</definiens>
				<definiens id="1">the binding relation ) Node X binds Node Y if and only if X and Y arc coindexed and X e-commands Y. where ( definitions vary slightly ) : Definition 3 ( the c-command relation</definiens>
				<definiens id="2">central part of the Binding Theory develops the notion of local domain to which binding principles A , B , and</definiens>
				<definiens id="3">the next node which dominates Y , and which contains a subject that e-commands Y. Due to these definitions , the acceptability judgements for the data presented above are reproduced by binding principles A , B</definiens>
			</definition>
			<definition id="1">
				<sentence>( a ) For each surviving pair ( Y/ , A~ ) of anaphor and antecedent candidate : deterinine the munerical plausibility score v ( Y/ , Xj ) , which ranks Xj relatively to Y/ , based on case role inertia , recency , cataphor penalty , and subject prefe .</sentence>
				<definiendum id="0">Xj )</definiendum>
				<definiens id="0">deterinine the munerical plausibility score v ( Y/ ,</definiens>
				<definiens id="1">ranks Xj relatively to Y/ , based on case role inertia , recency , cataphor penalty</definiens>
			</definition>
</paper>

	</volume>
