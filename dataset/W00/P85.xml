<?xml version="1.0" encoding="UTF-8"?>
	<volume id="P85">

		<paper id="1009">
			<definition id="0">
				<sentence>A regular language is any language that can be generated from a formula called a regular expression .</sentence>
				<definiendum id="0">regular language</definiendum>
				<definiens id="0">any language that can be generated from a formula called a regular expression</definiens>
			</definition>
			<definition id="1">
				<sentence>eats ) ( cakesipies ) * NONE NONE NONE Johnbakes pies John bakes k=2 NONE NONE NONE NONE NONE length ) that can be found at the very beginning of some legal string in a language , and a suffix as any substring ( again , possibly zero-length ) that can be found at the very end of some legal string in a language .</sentence>
				<definiendum id="0">NONE Johnbakes</definiendum>
				<definiens id="0">pies John bakes k=2 NONE NONE NONE NONE NONE length ) that can be found at the very beginning of some legal string in a language , and a suffix as any substring ( again , possibly zero-length ) that can be found at the very end of some legal string in a language</definiens>
			</definition>
			<definition id="2">
				<sentence>To describe the inference algorithm , we make use of the fact that every regular language can be associated with a corresponding deterministic finite-state automaton ( DFA ) which accepts or generates exactly that language .</sentence>
				<definiendum id="0">DFA</definiendum>
				<definiens id="0">accepts or generates exactly that language</definiens>
			</definition>
			<definition id="3">
				<sentence>We found that the English auxiliary system can be faithfully modeled as a/c-reversible regular language for k &gt; _ I. Only zero-reversible inference overgeneralizes the full corpus as well as the active and passive corpora treated as separate languages .</sentence>
				<definiendum id="0">auxiliary system</definiendum>
				<definiens id="0">a/c-reversible regular language for k &gt; _ I. Only zero-reversible inference overgeneralizes the full corpus as well as the active and passive corpora treated as separate languages</definiens>
			</definition>
			<definition id="4">
				<sentence>-cro-reversibility is a rather simple form of generalization of sequential patterns with which we believe humans readily identify .</sentence>
				<definiendum id="0">-cro-reversibility</definiendum>
				<definiens id="0">a rather simple form of generalization of sequential patterns with which we believe humans readily identify</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>Referring is a particular type of concept activation action with relatively strict conditions on what must be mutually believed by the speaker and hearer for the action to succeed .</sentence>
				<definiendum id="0">Referring</definiendum>
				<definiens id="0">a particular type of concept activation action with relatively strict conditions on what must be mutually believed by the speaker and hearer for the action to succeed</definiens>
			</definition>
			<definition id="1">
				<sentence>These actions are SI ( shared concept activation with identification intention ) , NSI ( nonshared concept activation with identification intention ) , SNI ( shared concept activation with no identification intention ) , and NSNI ( nonshared concept activation with no identification intention . )</sentence>
				<definiendum id="0">NSNI</definiendum>
				<definiens id="0">SI ( shared concept activation with identification intention ) , NSI ( nonshared concept activation with identification intention ) , SNI ( shared concept activation with no identification intention</definiens>
			</definition>
			<definition id="2">
				<sentence>Identification Intention ( SI ) These actions are the only type of concept activation acti , ms that were considered in the earlier KAMP research .</sentence>
				<definiendum id="0">Identification Intention</definiendum>
				<definiendum id="1">SI</definiendum>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>attempt constant ) n ( xJe represents an ohlectum , and a % NePS pr ( q'x~ltn ( mal nixie represents : in , ~hlt~tnve .</sentence>
				<definiendum id="0">xJe</definiendum>
				<definiens id="0">an ohlectum</definiens>
			</definition>
			<definition id="1">
				<sentence>Tile tormer includes all `` ordinary '' properties such as : being red .</sentence>
				<definiendum id="0">Tile tormer</definiendum>
				<definiens id="0">includes all `` ordinary '' properties such as : being red</definiens>
			</definition>
			<definition id="2">
				<sentence>Consuh~tantmtnon is an equivalence relation that is u~d in the analyses of ( I ) external predication , ( 2 ) co-reference , and ( 3 ) existence : l , et a = c { ... / '' ... } be a guise and let a\ [ fi l =~f c ( { . . . 1 '' ... } u l ( ; } ) .</sentence>
				<definiendum id="0">Consuh~tantmtnon</definiendum>
				<definiens id="0">an equivalence relation that is u~d in the analyses of ( I ) external predication</definiens>
			</definition>
			<definition id="3">
				<sentence>A SNePS representation o£ `` l'he Morning Star is the Evening Star ' ( m6 ) and 'The Morning Star is a planet ' ( m9 ) on Castaneda 's theory .</sentence>
				<definiendum id="0">SNePS representation o£</definiendum>
				<definiens id="0">a planet '</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>In fact , while an elliptical subject can be handled by the hypothesizetion , as second conjunct , of a verb phrase ( this is the equivalent of treating the sit~/ation as a single sentence involving a single subject and tw3 actions , and not as tw~ coordinated sentences , the second of which has an elliptical subject ; it a perfectly acceptable choice ) , the same mechanism can not be used t~ handle sentences with an elliptical verb in the second conjunct .</sentence>
				<definiendum id="0">verb phrase</definiendum>
				<definiens id="0">the equivalent of treating the sit~/ation as a single sentence involving a single subject</definiens>
			</definition>
			<definition id="1">
				<sentence>In logic grammars ( Definite Clause Granmars ( Pereira &amp; Warren , 1980 ) , Extraposition Grammars ( Pereira , 1981 ) , M~difier Structure Grammars ( Dahl &amp; ~L-~Drd , 1983 ) ) this book-keeping need not be completely explicit , but the interpreter of the language ( usually a dialect of PROLOG ) has to keep track of the binding of the variables , of the clauses that have not been used ( but could be used in case of failure of the current path ) , and so on .</sentence>
				<definiendum id="0">Definite Clause Granmars</definiendum>
				<definiendum id="1">Extraposition Grammars</definiendum>
				<definiens id="0">but the interpreter of the language ( usually a dialect of PROLOG ) has to keep track of the binding of the variables , of the clauses that have not been used ( but could be used in case of failure of the current path ) , and so on</definiens>
			</definition>
			<definition id="2">
				<sentence>Be~nd these six types , a special node ( TOP ) has been included to identi~ Z the main verb ( s ) of the sentence .</sentence>
				<definiendum id="0">special node</definiendum>
				<definiens id="0">the main verb ( s ) of the sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>Each of them has the form : ~ITION -- - &gt; ACTION where PR~ONDITION is a boolean expression ~nose ter~tg are elementary conditions ; their predicates allow the system to inspect the current status of the analysis , i.e. the tree ( for instance : ' '' ~hat is the type of the current node ? ''</sentence>
				<definiendum id="0">PR~ONDITION</definiendum>
				<definiens id="0">a boolean expression ~nose ter~tg are elementary conditions ; their predicates allow the system to inspect the current status of the analysis</definiens>
			</definition>
			<definition id="4">
				<sentence>The right-hand side of a rule ( ACTION ) consists in a sequence of operations ; there are two operators : CRLINK ( X , Y ) which creates a new instance of the type X and links it to the nearest node of type Y existing in the rightn~Dst path of the tree ( and moving only upwards ) FILL ( X , V ) which fills the nearest node ( see above ) of type X with the value V ( which in most cases coincides with the lexical date about the current input word ) . '</sentence>
				<definiendum id="0">right-hand side of a rule</definiendum>
				<definiendum id="1">V</definiendum>
				<definiens id="0">creates a new instance of the type X and links it to the nearest node of type Y existing in the rightn~Dst path of the tree</definiens>
			</definition>
			<definition id="5">
				<sentence>a ( and REF4 is the current node ) .</sentence>
				<definiendum id="0">REF4</definiendum>
				<definiens id="0">the current node )</definiens>
			</definition>
			<definition id="6">
				<sentence>A further co~Tent concerns the relative clauses with the deleted relative pronouns ( as in ( 2 ) above ) : this gaencmenon does not occur in Italian either ; v~ believe that it could be handled by means of a 184 natural change very similar to the one described below .</sentence>
				<definiendum id="0">further co~Tent</definiendum>
				<definiens id="0">concerns the relative clauses with the deleted relative pronouns</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>The analysis , i.e. output , of the NLPS is a data structure which serves as the input to an expert system .</sentence>
				<definiendum id="0">NLPS</definiendum>
				<definiens id="0">a data structure which serves as the input to an expert system</definiens>
			</definition>
			<definition id="1">
				<sentence>The information-processing task consists of the analysis of Linguistic information into datastructures which are chronologically ordered by the NLPS .</sentence>
				<definiendum id="0">information-processing task</definiendum>
				<definiens id="0">consists of the analysis of Linguistic information into datastructures which are chronologically ordered by the NLPS</definiens>
			</definition>
			<definition id="2">
				<sentence>The resulting knowledge representation of mv NLPS consists of a series of events which are extracted from the text and chronologically ordered by the NLPS based on the stored knowledge the system has about the domain and ~enera \ [ temporal re\ [ at ions .</sentence>
				<definiendum id="0">mv NLPS</definiendum>
				<definiens id="0">consists of a series of events which are extracted from the text and chronologically ordered by the NLPS based on the stored knowledge the system has about the domain</definiens>
			</definition>
			<definition id="3">
				<sentence>The target representation consists of a temporal indicator attached to a domain-specific fact what \ [ had referred to in as `` event '' .</sentence>
				<definiendum id="0">target representation</definiendum>
				<definiens id="0">consists of a temporal indicator attached to a domain-specific fact what \ [ had referred to in as `` event ''</definiens>
			</definition>
			<definition id="4">
				<sentence>Function specialists consists of procedures attached to function words ( e.~ .</sentence>
				<definiendum id="0">Function specialists</definiendum>
			</definition>
			<definition id="5">
				<sentence>A pragmatic theory focuses on the information from the context ( e.g. , co-text , discourse situation , intentions of interlocutors ) to explain linguistic behavior .</sentence>
				<definiendum id="0">pragmatic theory</definiendum>
				<definiens id="0">focuses on the information from the context ( e.g. , co-text , discourse situation , intentions of interlocutors ) to explain linguistic behavior</definiens>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>Another constraint , the Surface Recursion Restriction ( Emends , 1976 , p. 19 ) , prohibits free recursion of a node appearing within a phrase , to the left of the phrase head .</sentence>
				<definiendum id="0">Surface Recursion Restriction</definiendum>
			</definition>
			<definition id="1">
				<sentence>Sprouting , which derives its name from the action of growing a semantic tree from a specified root , uses the results of head-finding as its raw material .</sentence>
				<definiendum id="0">Sprouting</definiendum>
				<definiens id="0">derives its name from the action of growing a semantic tree from a specified root , uses the results of head-finding as its raw material</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>LEXICAL-SENANTIC RELATIONS The semantic component of the lexicon produced by this system consists principally of a network of lexical-semantic relations .</sentence>
				<definiendum id="0">LEXICAL-SENANTIC RELATIONS</definiendum>
				<definiens id="0">The semantic component of the lexicon produced by this system consists principally of a network of lexical-semantic relations</definiens>
			</definition>
			<definition id="1">
				<sentence>We express the taxonomic relations of `` carotid ' , `` artery '' and `` blood vessel '' with the relational arcs carotid T artery artery T blood vessel Another important relation is that of the part to the whole : ventricle PART heart Broca 's area PART brain Note that taxonomy is transitive : if the carotid is an artery and an artery is a blood vessel , then the carotid is a blood vessel .</sentence>
				<definiendum id="0">artery</definiendum>
				<definiens id="0">a blood vessel</definiens>
			</definition>
			<definition id="2">
				<sentence>THE INTERACTIVE LEXICON BUILDER Commands The interactive lexicon builder consists of an operatlng-system-like environment in which the user may invoke the following commands : HELP displays a set of one-line summaries of the commands , or a paragraphlength description of a specified command .</sentence>
				<definiendum id="0">INTERACTIVE LEXICON BUILDER Commands The interactive lexicon builder</definiendum>
				<definiens id="0">consists of an operatlng-system-like environment in which the user may invoke the following commands : HELP displays a set of one-line summaries of the commands</definiens>
			</definition>
			<definition id="3">
				<sentence>UNDEF is a special form of EDIT .</sentence>
				<definiendum id="0">UNDEF</definiendum>
				<definiens id="0">a special form of EDIT</definiens>
			</definition>
			<definition id="4">
				<sentence>aphasia ( 1 ) n. definition a disorder of language due to injury to the brain attributes nonhuman collective predicate calculus have ( x , aphasia ) - '' able ( speak ( x ) ) relations TAX \ [ aphasia is a kind of x\ ] deficit disorder loss inability `` TAX Ix is a kind of aphasia\ ] anomic global gerstmann ' s semantic We rnicke ' s Sroca ' s conduction transcortical SYMPTOM \ [ aphasia is a symptom of x\ ] stroke TIA ASSOC \ [ aphasia may be associated with x\ ] apraxia _CAUSE \ [ x is a cause of aphasia\ ] injury lesion NNABLE \ [ aphasia is the inability to do x\ ] speech language Figure 3 .</sentence>
				<definiendum id="0">TAX Ix</definiendum>
				<definiens id="0">a kind of aphasia\ ] anomic global gerstmann ' s semantic We rnicke ' s Sroca ' s conduction transcortical SYMPTOM</definiens>
			</definition>
			<definition id="5">
				<sentence>ADDENTRY uses this prompt when querying the user for the occurrence of relational arcs involving this relation .</sentence>
				<definiendum id="0">ADDENTRY</definiendum>
				<definiens id="0">uses this prompt when querying the user for the occurrence of relational arcs involving this relation</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>{ 1 ) S -NP VP VP ~VNP NP -Det N Det -NP \ [ ÷Ce~l ( '2 } 0 the t boy 's 2 father 3 hit 4 the s dog s The subscripts in ( 2 } serve to locate the lexical items : they indicate that , for instance , the utterance of the word d0~ began at time t s and ended at time t s. That is , the location of the utterance `` dog '' in example ( 2 ) was the interval ( t~ , tsl .</sentence>
				<definiendum id="0">S -NP VP VP ~VNP NP -Det N Det -NP</definiendum>
				<definiens id="0">they indicate that , for instance , the utterance of the word d0~ began at time t s and ended at time t s. That is , the location of the utterance `` dog '' in example ( 2 ) was the interval ( t~ , tsl</definiens>
			</definition>
</paper>

		<paper id="1029">
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>The Head-driven Phrase Structure Grammar project ( HPSG ) is an English language database query system under development at Hewlett-Packard Laboratories .</sentence>
				<definiendum id="0">Head-driven Phrase Structure Grammar project</definiendum>
				<definiendum id="1">HPSG</definiendum>
				<definiens id="0">an English language database query system under development at Hewlett-Packard Laboratories</definiens>
			</definition>
			<definition id="1">
				<sentence>HPSG is a lexically based theory of phrase structure , so called because of the central role played by grammlttical heads and their associated complements . '</sentence>
				<definiendum id="0">HPSG</definiendum>
				<definiens id="0">a lexically based theory of phrase structure</definiens>
			</definition>
			<definition id="2">
				<sentence>Verbs are the heads of verb phrm~es ( apd sentences ) , nouns are the heads of noun phra~es , and so forth .</sentence>
				<definiendum id="0">Verbs</definiendum>
				<definiens id="0">the heads of verb phrm~es ( apd sentences ) , nouns are the heads of noun phra~es</definiens>
			</definition>
			<definition id="3">
				<sentence>Head-driven Active Chart Parser A crucial dilference between the HPSG system and its predecessor GPSG is the importance placed on the head constituent in HPSG .</sentence>
				<definiendum id="0">GPSG</definiendum>
				<definiens id="0">the importance placed on the head constituent in HPSG</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>Prepositions are defined as being either true or false in a particular state , and predicates such as `` before ( sl , s2 ) '' can be defined to order the states temporally .</sentence>
				<definiendum id="0">Prepositions</definiendum>
				<definiens id="0">being either true or false in a particular state</definiens>
			</definition>
			<definition id="1">
				<sentence>4 The last type of query Is the oertodlc query , which asks for objects to be grouped according to one or more attributes .</sentence>
				<definiendum id="0">oertodlc query</definiendum>
			</definition>
			<definition id="2">
				<sentence>The model , which Is expressed as a collection of predicates and rules written in Prolng \ [ S\ ] , consists of the following components : I. A time domain model for representing units ( days ) , intervals , lengths of time , calendar structures , and a variety of relative time descriptions .</sentence>
				<definiendum id="0">model</definiendum>
				<definiens id="0">consists of the following components : I. A time domain model for representing units ( days ) , intervals , lengths of time , calendar structures</definiens>
			</definition>
			<definition id="3">
				<sentence>A calendar ( CAt ) is a totally ordered set of Interval descriptors called `` calendar elements '' ( L'~ , CEI , CE2 .</sentence>
				<definiendum id="0">CAt</definiendum>
				<definiens id="0">a totally ordered set of Interval descriptors called `` calendar elements '' ( L'~ , CEI , CE2</definiens>
			</definition>
			<definition id="4">
				<sentence>Periodic descriptors can run either forward or backward in time , as shown by the following example : each_of_the_gas~cal_elts ( CAL , N , PO ) : PO \ [ CAL , CEP , MI , today ( DT ) , incal ( CAL , DT , CET , _ ) , dtstcal ( CAL , CEP , CET , I ) , H Is -N. To Interpret a query containing a periodic descriptor , the NL interface must first expand the structure Into a list of Intervals ( this must wait until execution time in order to ensure the right value for `` today ' ) and then perform an Iteratlve execution of the query , restricting it In turn to each interval in the list .</sentence>
				<definiendum id="0">Intervals</definiendum>
				<definiens id="0">shown by the following example : each_of_the_gas~cal_elts ( CAL , N , PO ) : PO \ [ CAL , CEP , MI</definiens>
			</definition>
</paper>

		<paper id="1016">
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>A message-based explanation generator is a two-phase processor that first generates and organizes messages based on the contents of working memory , and then maps those messages into surface strings .</sentence>
				<definiendum id="0">message-based explanation generator</definiendum>
				<definiens id="0">a two-phase processor that first generates and organizes messages based on the contents of working memory , and then maps those messages into surface strings</definiens>
			</definition>
			<definition id="1">
				<sentence>A relational link is a sJightly richer memory element which not only names the relation that holds between two or more facts , but also categorizes it .</sentence>
				<definiendum id="0">relational link</definiendum>
			</definition>
			<definition id="2">
				<sentence>predicate ON ~terml &lt; link.focus &gt; tstatus reiterate ) ) Figure 3-4 : The Generic Schema directs the message generator to create additional query .</sentence>
				<definiendum id="0">Generic Schema</definiendum>
				<definiens id="0">directs the message generator to create additional query</definiens>
			</definition>
			<definition id="3">
				<sentence>`` XPLAIN : a System for Creating and Explaining Expert Consulting Programs '' .</sentence>
				<definiendum id="0">XPLAIN</definiendum>
				<definiens id="0">a System for Creating and Explaining Expert Consulting Programs ''</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>The simple sentence consists of cases and a predicate .</sentence>
				<definiendum id="0">simple sentence</definiendum>
				<definiens id="0">consists of cases and a predicate</definiens>
			</definition>
			<definition id="1">
				<sentence>A noun phrase is defined as the recursive concatenation of noun phrase or that of embedded sentence .</sentence>
				<definiendum id="0">noun phrase</definiendum>
				<definiens id="0">the recursive concatenation of noun phrase or that of embedded sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>en 28 For an example , the causative auxiliary verb `` seru '' or % aseru '' results in ( a ) the addition of the causative agent , or ( b ) the introduction of a second-order predicate CAUSE ( x , y ) in which argument x represents the causative agent and argument y represents a predicate , as follows : ( s5 ) Taro ga Hiroko ni yasai wo tabe saseru .</sentence>
				<definiendum id="0">argument y</definiendum>
				<definiens id="0">a ) the addition of the causative agent , or ( b ) the introduction of a second-order predicate CAUSE ( x , y ) in which argument x represents the causative agent</definiens>
			</definition>
			<definition id="3">
				<sentence>-vx ( S ( x~ ~Come ( x ) ) , where the predicate S ( x ) denotes `` zen'in \ [ all the persons ) '' .</sentence>
				<definiendum id="0">-vx ( S ( x~ ~Come</definiendum>
				<definiens id="0">( x ) ) , where the predicate S ( x ) denotes `` zen'in \ [ all the persons ) ''</definiens>
			</definition>
</paper>

		<paper id="1010">
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>\ [ A is confused , examines both NOZZLE SX. , mr-VALVE \ ] \ [ E takes SLIDEVALVE\ ] and A : 9 .</sentence>
				<definiendum id="0">NOZZLE SX.</definiendum>
				<definiens id="0">mr-VALVE \ ] \ [ E takes SLIDEVALVE\ ] and A : 9</definiens>
			</definition>
			<definition id="1">
				<sentence>Excerpt 7 below demonstrates the latter type of focus confuszon that occurs when the speaker ( S ) sets up one focus the M,4\ ] NTUBE , which is the correct focus In this case but then proceeds in such a manner that the listener ( J ) thinks a focus shift to another piece , the TUBESASE , has occurred .</sentence>
				<definiendum id="0">NTUBE</definiendum>
				<definiens id="0">occurs when the speaker ( S ) sets up one focus the M,4\ ]</definiens>
			</definition>
			<definition id="2">
				<sentence>\ [ J answers this with MAINTUBE still sittint on the TABLE ; he shows no indication of what hole he thinks i8 meant the one on the MAINTUBE .</sentence>
				<definiendum id="0">J</definiendum>
				<definiens id="0">answers this with MAINTUBE still sittint on the TABLE ; he shows no indication of what hole he thinks i8 meant the one on the MAINTUBE</definiens>
			</definition>
			<definition id="3">
				<sentence>v2 ) &lt; ObjectOeecr ( d ) , Feat ureOeec r i ptor ( v ! ) , FectureOescr iptor ( v2 ) , FecturelnOeecr i pt ion ( vf .d ) . Feat urel nOesc r i pt i on ( v2 .d ) . 5 '' quo I ( syntoc t ic-f orm ( v t .d ) , `` ADJ '' ) . ; 'quo I ( synt a¢t ic-f orm ( v2.d ) , `` REL-CLS '' ) Figure 5 : A sample relaxation rule Each knowledge source produces ~ts own partial ordermg of features. The partial ordermgs are then zntegrated to form a d~rected graph. For example. perceptual knowledge may say to relax color However. ~f the color value was asserted ~n a relative clause. linguistic knowledge would rank color lower. ~.e.. placmg ~t later ~n the list of things to relax. Smce different knowledge sources generally have different partial orderlngs of features , these differences can lead to a conflict over which features to relax. It Is the job of the best candidate algorithm to resolve the d~sagreements among knowledge sources. It 's goal ts to order the referent candidates , Ci , so that relaxation ~s attempted on the best candzdates first Those candidates are the ones that conform best to a proposed feature ordering. To start , the algorithm exammes pairs of candidates and the feature order~ngs from each knowledge source. For each candidate C i. the algorithm scores the effect of relaxlng the speaker 's orlglnal descrlpt~on to C i. using the feature ordering from one knowledge source. The score reflects the goal of mln~mlz : ng the number of features relaxed whale try3ng to relax the features that are `` earhest '' sn the feature ordermg. It repeats ~ts scoring of C i for each knowledge source , and sums up its scores to form Ci 's total score. The Ci 's are then ordered by that score. Figure 6 provides a graphic description of th~s process. A set of objects ~n the real world are selected by the partial marcher as potent~a| candidates for the referent. These candidates are shown across the top of the figure. The lines on the right side of each box correspond to the set of features that describe that object. The speaker 's descrlpt~on ts represented in the center of the figure. The set of specified features and their assigned feature value ( e.g. , the pair Color-Maroon ) are also shown there. A set of partial orderings are generated that suggest which features in the speaker 's description should be relaxed first one ordering for each knowledge source ( shown as `` l~nguist~c , '' `` Perceptual. '' and `` H~erarchlcaI '' in the figure ) . These are put together to form a directed graph that represents the possible , reasonable ways to relax the features specified tn the speakers description. Finally. the referent candidates are reordered using the information expressed ~n the speaker 's description and in the directed graph of features. OQ/ecrl • *a pm-c~al FI -~ ¢o1¢*f~ tl oe fz P~ ¢ - ) N|eeet.¢tnlceJ f3 - ) F~I : I~ f2 F3 fZ oe f~ oe F , * F4 - ) Size f3 fa f4 5 O~Nct4d Ct~ of/~rtu.s I~ , *~ , ~r~ ; ~ Figure 8 : Reordering referent candldates Once a set of ordered , potential candldates are selected , the relaxation mechanlsm begms step 3 of relaxatlon ; it trles to find proper relaxation methods to relax the features that have lust been ordered ~success tn flndlng such methods `` justifies '' relaxing the descrlptlon ) . It stops at the first candidate which zs reasonable. Determine which relaxation methods to apply Relaxation can take place wlth many aspects of a speaker 's descrlptlon : wlth complex relatlons specified In the descrlptlon , wlth indlvldual features of a referent specified by the descrlptlon , and with the focus of attention in the real world where one attempts to find a match. Complex relatlons speclfted in a speaker 's descrlptlon include spatlal relations ( e.g.. `` the outlet near the top of the tube '' &gt; , comparatives ( e.g. `` the larger tube '' ) and superlatlves ( e.g. , `` the longest tube '' ) .</sentence>
				<definiendum id="0">FectureOescr iptor</definiendum>
				<definiens id="0">lines on the right side of each box correspond to the set of features that describe that object. The speaker 's descrlpt~on ts represented in the center of the figure. The set of specified features and their assigned feature value ( e.g. , the pair Color-Maroon ) are also shown there. A set of partial orderings</definiens>
				<definiens id="1">Reordering referent candldates Once a set of ordered , potential candldates are selected , the relaxation mechanlsm begms step 3 of relaxatlon ; it trles to find proper relaxation methods to relax the features that have lust been ordered ~success tn flndlng such methods `` justifies '' relaxing the descrlptlon ) . It stops at the first candidate which zs reasonable. Determine which relaxation methods</definiens>
			</definition>
			<definition id="4">
				<sentence>The KL-One Classifier compares the features specified in the speaker 's descriptions ( Descrl and the '' `` Outer '' feature of Descr2 in Figure T ) with the features speclhed for each element in the EL-One taxonomy that corresponds to one of the current objects of interest in the real world .</sentence>
				<definiendum id="0">KL-One Classifier</definiendum>
				<definiens id="0">corresponds to one of the current objects of interest in the real world</definiens>
			</definition>
			<definition id="5">
				<sentence>Rule-based relaxation provided a methodical way to use knowledge about language and the world to find a referent .</sentence>
				<definiendum id="0">Rule-based relaxation</definiendum>
				<definiens id="0">provided a methodical way to use knowledge about language and the world to find a referent</definiens>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>The term level refers to Mohanan 's theory of Level Ordered Morphology and Phonology \ [ Mohanan\ ] which builds upon a number of well-known differences between + boundary affixes ( level I ) and # boundary affixes ( level 2 ) .</sentence>
				<definiendum id="0">term level</definiendum>
				<definiens id="0">builds upon a number of well-known differences between + boundary affixes</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>PATR-II is a simple grammar formalism that can serve as the least common denominator of many of the complex-feature-based and unification-based formalisms prevalent in linguistics and computational linguistics .</sentence>
				<definiendum id="0">PATR-II</definiendum>
				<definiens id="0">a simple grammar formalism that can serve as the least common denominator of many of the complex-feature-based and unification-based formalisms prevalent in linguistics and computational linguistics</definiens>
			</definition>
			<definition id="1">
				<sentence>The PATR-II nonterminal domain is a lattice of directed , acyclic , graph structures ( dags ) .</sentence>
				<definiendum id="0">PATR-II nonterminal domain</definiendum>
				<definiens id="0">a lattice of directed , acyclic , graph structures ( dags )</definiens>
			</definition>
			<definition id="2">
				<sentence>X , and D is a dug with top-level features Xo , ... , X , and with atomic values for the eat feature of each of the top-level subdags .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">D</definiendum>
				<definiens id="0">a dug with top-level features Xo , ... , X , and with atomic values for the eat feature of each of the top-level subdags</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>In the new Head-driven Phrase Structure Grammar ( HPSG ) language processing system that is currently under development at Hewlett-Packard Laboratories , the Montagovian semantics of the earlier GPSG system ( see \ [ Gawron et al. 19821 ) is replaced by a radically different approach with a number of distinct advantages .</sentence>
				<definiendum id="0">Hewlett-Packard Laboratories</definiendum>
				<definiens id="0">Head-driven Phrase Structure Grammar ( HPSG ) language processing system that is currently under development at</definiens>
			</definition>
			<definition id="1">
				<sentence>NFLT has a number of features that make it well-suited { 'or natural language translations , including predicates of variable arity in which explicitly marked situational roles supercede order-coded argument positions , sortally restricted quantification , a compositional ( but nonextensional ) semantics that handles causal contexts , and a princip\ [ ed conceptual raising mechanism that we expect to lead to a computationally tractable account of propositional attitudes .</sentence>
				<definiendum id="0">NFLT</definiendum>
				<definiens id="0">a number of features that make it well-suited { 'or natural language translations , including predicates of variable arity in which explicitly marked situational roles supercede order-coded argument positions , sortally restricted quantification , a compositional ( but nonextensional ) semantics that handles causal contexts , and a princip\ [ ed conceptual raising mechanism that we expect to lead to a computationally tractable account of propositional attitudes</definiens>
			</definition>
			<definition id="2">
				<sentence>Simple situations consist of individuals playing roles in relations .</sentence>
				<definiendum id="0">Simple situations</definiendum>
				<definiens id="0">consist of individuals playing roles in relations</definiens>
			</definition>
			<definition id="3">
				<sentence>One might also consider the sort of state in which Ron is a person , i.e. in which the relation is PERSON , and Ron plays the INSTANCE role .</sentence>
				<definiendum id="0">Ron</definiendum>
				<definiens id="0">a person</definiens>
			</definition>
			<definition id="4">
				<sentence>Non-Extensionality According to the standard semantics for the predicate calculus , predicate symbols denote the extensions of relations ( i.e. sets of ordered n-tuples ) and sentential formulas denote truth values .</sentence>
				<definiendum id="0">Non-Extensionality</definiendum>
				<definiens id="0">According to the standard semantics for the predicate calculus , predicate symbols denote the extensions of relations ( i.e. sets of ordered n-tuples ) and sentential formulas denote truth values</definiens>
			</definition>
			<definition id="5">
				<sentence>( 4b ) NFLT Display Syntax : ( C~USE conditn : ( GENOTYPE-XYZW inst : JOHN ) result : ( BROWN-EYED bearer : JOHN } ) Now , the predicate calculus is an extensional language in the sense that the replacement of categorical subparts within an expression by new subparts having the same extension must preserve the extension of the original expression .</sentence>
				<definiendum id="0">BROWN-EYED bearer</definiendum>
				<definiens id="0">an extensional language in the sense that the replacement of categorical subparts within an expression by new subparts having the same extension must preserve the extension of the original expression</definiens>
			</definition>
			<definition id="6">
				<sentence>The result of appending the `` conceptual raising '' symbol ' l '' to the constant `` COOKIE ' is a new constant , ' TCOOKIE ' , that denotes the concept that 'COOKTE ' expresses ( i.e. ' 1 '' ' applies to a constant and forms a standard name of the sense of that constant ) .</sentence>
				<definiendum id="0">COOKIE</definiendum>
				<definiendum id="1">'COOKTE ' expresses</definiendum>
				<definiens id="0">a constant and forms a standard name of the sense of that constant )</definiens>
			</definition>
			<definition id="7">
				<sentence>`` situation semantics '' 119831 is to be provided for NFLTexpressions , insofar as those expressions involve no ascription of propositional attitudes ( the Barwise-Perry semantics for ascriptions of propositional attitudes takes a quite different approach from that to be described for NFLT in the next section ) : s For further details concerning this Fregean conceptual hierarchy , see \ [ Creary 1979 I , sections 2.2 and 2.3.1 .</sentence>
				<definiendum id="0">propositional attitudes</definiendum>
				<definiens id="0">the Barwise-Perry semantics for ascriptions of propositional attitudes takes a quite different approach from that to be described for NFLT in the next section</definiens>
			</definition>
			<definition id="8">
				<sentence>A sign is a conceptual object , shared by a group of organisms , which consist , ~ of two associated concepts that we call ( by a conventional abuse of language ) a phonolooical representation and a semantic representation .</sentence>
				<definiendum id="0">sign</definiendum>
				<definiens id="0">a conceptual object , shared by a group of organisms</definiens>
			</definition>
			<definition id="9">
				<sentence>Augmented with such information , lexical signs assume forms such as these : ( 7a ) { cookie ; COOKIE ; \ [ MAJOR : N ; AGR : 3RDSGI } ( 7b ) ( kisses ; KISS ; \ [ MAJOR : V ; VFORM : FINI } Such features as MAJOR ( major category ) , AGR ( agreement ) , and VFORM ( verb form ) encode inherent syntactic properties of signs .</sentence>
				<definiendum id="0">AGR</definiendum>
				<definiens id="0">verb form ) encode inherent syntactic properties of signs</definiens>
			</definition>
			<definition id="10">
				<sentence>Certain expressions ( heads ) characteristically combine with other expressions of specified categories ( complements ) to form larger expressions .</sentence>
				<definiendum id="0">Certain expressions</definiendum>
				<definiens id="0">heads ) characteristically combine with other expressions of specified categories ( complements ) to form larger expressions</definiens>
			</definition>
			<definition id="11">
				<sentence>For example , English grammar has a rule something like this : ( lO ) If X is a sign whose SUBCAT value contains just one category Y , and Z is a sign whose category is consistent with Y , then X and Z can be combined to form a new sign W whose expression is got by 178 concatenating the expressions of X and Z. That is , put the final complement ( subject } to the left of the head .</sentence>
				<definiendum id="0">Z</definiendum>
				<definiens id="0">a sign whose category is consistent with Y</definiens>
				<definiens id="1">got by 178 concatenating the expressions of X and Z. That is , put the final complement</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>Real understanding consists sot only of resognlzAr~ the particular surface-request or surface-lnform , but also of inferring what the speaker wants to accomplish and the relationship of each utterance to this task .</sentence>
				<definiendum id="0">Real understanding</definiendum>
				<definiens id="0">consists sot only of resognlzAr~ the particular surface-request or surface-lnform , but also of inferring what the speaker wants to accomplish and the relationship of each utterance to this task</definiens>
			</definition>
			<definition id="1">
				<sentence>SPREDS consists of all propositions along the paths from the root of the context tree to the nodes at which an element of the frasment is associated with a plan element , as well as all propositions appearing along the previous ACTIVE path .</sentence>
				<definiendum id="0">SPREDS</definiendum>
				<definiens id="0">consists of all propositions along the paths from the root of the context tree to the nodes at which an element of the frasment is associated with a plan element , as well as all propositions appearing along the previous ACTIVE path</definiens>
			</definition>
			<definition id="2">
				<sentence>A focus domain consists of a set of actions , one of which is an ancestor of all other actions in the focus domain and is called the root of the focus domain .</sentence>
				<definiendum id="0">focus domain</definiendum>
				<definiens id="0">consists of a set of actions , one of which is an ancestor of all other actions in the focus domain and is called the root of the focus domain</definiens>
			</definition>
			<definition id="3">
				<sentence>\ [ 2\ ] Obtaln-Corroboration : IS expresses surprise regarding some proposition P and requests elaboration upon and justification of it .</sentence>
				<definiendum id="0">Obtaln-Corroboration</definiendum>
				<definiens id="0">IS expresses surprise regarding some proposition P and requests elaboration upon and justification of it</definiens>
			</definition>
			<definition id="4">
				<sentence>I. Discourse Expectation Rules The top element of the discourse stack activates the discourse expectation rule with which it is associated ; this rule in turn suggests discourse goals which the information-seeker ' s utterance may pursue and activates these discourse goal rules .</sentence>
				<definiendum id="0">discourse stack</definiendum>
				<definiens id="0">activates the discourse expectation rule with which it is associated</definiens>
			</definition>
			<definition id="5">
				<sentence>These discourse goal rules use the plan-analysls component to help determine the best interpretation of the frasmentary utterance relevant to the suggested discourse goal .</sentence>
				<definiendum id="0">discourse goal rules</definiendum>
				<definiens id="0">the plan-analysls component to help determine the best interpretation of the frasmentary utterance relevant to the suggested discourse goal</definiens>
			</definition>
			<definition id="6">
				<sentence>~onflrmatlon or 2 , ~seeklng corroboration of a component of the preceding response or 3 ) seeking elaboration and corroboration of some aspect of this 195 ( I ) eEarn-Credit ( IS , _crse : &amp; COU RsE , _sem : &amp; SEmeSTERS ) such that Course-Of f ered ( _cr se : &amp; COU RSE , _sem : &amp; S~STERS ) l I ( I ) eEarn-Cr edit-Sectlon ( IS , _ss : &amp; SECTIONS ) such that IsSecti onOf ( _as : a3ECT ION S , _or se : &amp; COURSE ) Is-Offered ( _ss : &amp; SECTIONS , _sea : &amp; SE ) ~STERS ) I i ( I ) iRegl sterLate ( IS , _ss : &amp; SECTION S , _sea : &amp; S E ) ~STERS ) i I ( 2 ) eMissProReg ( IS , _sea : &amp; SEM~TEBS ) \ [ ( 2 ) Pay-Fee ( IS , LATEREG , _sere : &amp; SEI~STF~S ) t \ [ ( 2 ) Pay ( IS , _lreg : &amp; MONEY ) such that Costs ( LATERE3 , _lreg : &amp; MON ~-Y ) Figure 2 .</sentence>
				<definiendum id="0">I ) eEarn-Cr edit-Sectlon</definiendum>
				<definiendum id="1">COURSE ) Is-Offered</definiendum>
				<definiendum id="2">_lreg</definiendum>
				<definiens id="0">IS , LATEREG , _sere : &amp; SEI~STF~S ) t \ [ ( 2 ) Pay ( IS ,</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>RINA receives new figurative phrases in context and through the application of a sequence of failure-driven rules , creates and refines both the patterns and the concepts which hold syntactic and semantic information about phrases .</sentence>
				<definiendum id="0">RINA</definiendum>
			</definition>
			<definition id="1">
				<sentence>A figurative phrase such as put one 's fooc down is a linguistic pattern whose associated meaning can not be produced from the composition of its constituents .</sentence>
				<definiendum id="0">figurative phrase</definiendum>
			</definition>
			<definition id="2">
				<sentence>( 8 ) By convention , the agent is the case-frame which precedes the verb in the lexical pattern .</sentence>
				<definiendum id="0">agent</definiendum>
			</definition>
			<definition id="3">
				<sentence>Conceptual analysis is the process which involves reading input words left to right , matching them with existing linguistic patterns and instantiating or modifying in memory the associated conceptual meanings .</sentence>
				<definiendum id="0">Conceptual analysis</definiendum>
				<definiens id="0">the process which involves reading input words left to right , matching them with existing linguistic patterns and instantiating or modifying in memory the associated conceptual meanings</definiens>
			</definition>
			<definition id="4">
				<sentence>Discriminating a Pattern by Freezing a Prepoab tional Phrase A prepoMtional mismatch occurs when a preposition P matches in neither the active pattern nor in one of the lexical prepositional phrases , such as : &lt; on ? x : platform &gt; ( indicating a spatial relation ) &lt; on ? x : time-unit &gt; ( indicating a time of action ) &lt; on ? x : location &gt; ( indicating a place ) Sentence : Patternl : Failures : Pattern2 : David took on Goliath .</sentence>
				<definiendum id="0">Patternl</definiendum>
				<definiens id="0">matches in neither the active pattern nor in one of the lexical prepositional phrases</definiens>
			</definition>
			<definition id="5">
				<sentence>For example , RINA implements the behavior of the learner in vffivtd vs. c , oliffich and in Go£ng Punk in Section 1 .</sentence>
				<definiendum id="0">RINA</definiendum>
				<definiens id="0">implements the behavior of the learner in vffivtd vs. c</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>JackcndtflT considers proper nouns to be definite and the same is done here , except in certain cases t ) l '' phnal proper nouns which are interpreted a.s the plural indefinite ( scc $ 21 in figure 5 ) .</sentence>
				<definiendum id="0">JackcndtflT</definiendum>
				<definiens id="0">considers proper nouns to be definite and the same is done here , except in certain cases t ) l '' phnal proper nouns which are interpreted a.s the plural indefinite</definiens>
			</definition>
			<definition id="1">
				<sentence>1983 : 95\ ] ) d , i.d.c d. , i.d , c ( S7 ) The tiger is the fiercest I~e'ast of the jungle .</sentence>
				<definiendum id="0">tiger</definiendum>
				<definiens id="0">the fiercest I~e'ast of the jungle</definiens>
			</definition>
			<definition id="2">
				<sentence>Selection restrictions are an important type of information to accumulate because they are needed not only to distinguish different senses of words but also to recognize metaphorical uses .</sentence>
				<definiendum id="0">Selection restrictions</definiendum>
				<definiens id="0">an important type of information to accumulate because they are needed not only to distinguish different senses of words but also to recognize metaphorical uses</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>ABSTRACT INKA is a natural language interface to facilitate knowledge acquisition during expert system development for electronic instrument trouble-thooting .</sentence>
				<definiendum id="0">ABSTRACT INKA</definiendum>
				<definiens id="0">a natural language interface to facilitate knowledge acquisition during expert system development for electronic instrument trouble-thooting</definiens>
			</definition>
			<definition id="1">
				<sentence>This grammar format enables GLIB to be used with the INGLISH interface , which constrains users to create statements within a subset of English .</sentence>
				<definiendum id="0">GLIB</definiendum>
			</definition>
			<definition id="2">
				<sentence>Tereisias ( Davis , 1977 ) provides a natural language environment for debugging a knowledge base .</sentence>
				<definiendum id="0">Tereisias</definiendum>
				<definiens id="0">provides a natural language environment for debugging a knowledge base</definiens>
			</definition>
			<definition id="3">
				<sentence>INKA makes extensive use of the bit-mapped display and three-button mouse on these systems .</sentence>
				<definiendum id="0">INKA</definiendum>
				<definiens id="0">makes extensive use of the bit-mapped display and three-button mouse on these systems</definiens>
			</definition>
			<definition id="4">
				<sentence>THE NATURAL LANGUAGE INTERFACE DESIGN INGLISH INterface enGLISH ( Ph/Ilips &amp; Nicholl , 1984 ) allows a user to create sentences either by menu selection , by typing , or by a mixture of the two .</sentence>
				<definiendum id="0">NATURAL LANGUAGE INTERFACE DESIGN INGLISH INterface enGLISH</definiendum>
				<definiens id="0">allows a user to create sentences either by menu selection , by typing , or by a mixture of the two</definiens>
			</definition>
			<definition id="5">
				<sentence>When this is used , INGLISH attempts to complete the word on the basis of the characters so far typed .</sentence>
				<definiendum id="0">INGLISH</definiendum>
			</definition>
			<definition id="6">
				<sentence>The Smafltalk system contnin~ controllers that manage activity on a variety of input devices and from these a controller was readily constructed '' to coordinate mouse and key• Smalltalk is an object-oriented language .</sentence>
				<definiendum id="0">key• Smalltalk</definiendum>
				<definiens id="0">an object-oriented language</definiens>
			</definition>
			<definition id="7">
				<sentence>Smalitalk programs create objects or send messages to other objects .</sentence>
				<definiendum id="0">Smalitalk programs</definiendum>
				<definiens id="0">create objects or send messages to other objects</definiens>
			</definition>
			<definition id="8">
				<sentence>The f-strucntre of Figure 4 produces the Prolog clause rule ( state ( led-2 , on ) , ~tatus ( transistor-17 , failed ) KNOWLEDGE USE Translated rules are sent to a diagnostic engine that has been implemented ia Pmiog .</sentence>
				<definiendum id="0">Prolog clause rule</definiendum>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>~/ i~l , |fke 0 `` ases ' , ~ ionear i~lihaliv as , ,felt 158 The parse template associated with the above top-level pattern for yes/no questions is : aux kernel-casetrame + ( : query ) This template tells the parser that the input consists of the auxiliary verb matched in the first pass followed by a : kernel-caseframe .</sentence>
				<definiendum id="0">template</definiendum>
				<definiens id="0">the auxiliary verb matched in the first pass followed by a</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>~ BMB has the following properties : Proposition 9 ( BMB y x pAq ) = ( BMB y x p ) A ( DMB y x q ) Proposition 10 ( BMB y x pDq ) 3 ( ( BMD y x p ) 3 ( BMB y x q ) ) Proposition 11 1/I- , ~ 3 # then ~- ( BMB y x ~ ) :3 ( BMB y x J ) Also , we characterize mutual knowledge as : Definition 3 ( MK x y p ) d.=f P ^ ( BMB x y p ) ^ ( BMD y x p ) r 5For an exploration of the issues involved in explicit vs. implicit belief , see ilel .</sentence>
				<definiendum id="0">BMB</definiendum>
				<definiendum id="1">BMB y x p</definiendum>
				<definiens id="0">we characterize mutual knowledge as : Definition 3 ( MK x y p ) d.=f P ^ ( BMB x y p ) ^ ( BMD y x p ) r 5For an exploration of the issues involved in explicit vs. implicit belief</definiens>
			</definition>
			<definition id="1">
				<sentence>Now , we have the following as a lemma that will be used in the speech act derivations : Lemma 2 Shared Recoqnition ( BMB y x { GOAL x p ) } A ( BMB y x ( BEL x ( ALWAYS p~q ) ) ) 3 ( BMB y x ( GOAL x q ) ) The proof is a straightforward application of Lemma I and Propositions 9 and 10 .</sentence>
				<definiendum id="0">BEL x</definiendum>
				<definiendum id="1">BMB y x</definiendum>
				<definiens id="0">a lemma that will be used in the speech act derivations</definiens>
				<definiens id="1">a straightforward application of Lemma I and Propositions 9 and 10</definiens>
			</definition>
			<definition id="2">
				<sentence>Now , in order to state constraints on c.o.e. 's we define : d*f Definition T ( PREREQ x p q ) = Vc ( RESULT x ¢ q ) ~ 3 a ( a ~ c ) A ( RESULT x a p } This definition states that p is a prerequisite for x 's achieving q if all ways for x to bring about q result in a course of events in which p has been true .</sentence>
				<definiendum id="0">RESULT</definiendum>
				<definiens id="0">a prerequisite for x 's achieving q if all ways for x to bring about q result in a course of events in which p has been true</definiens>
			</definition>
			<definition id="3">
				<sentence>; act ) , where p % 'J ( P-GOAL x q ) ~ ~ ( DEL x ( AFTER act ( ALWAYS x ~p ) ) ) v ~ ( COAL x ( DONE x act ) ) l0 in other words , no deliberately shooting onessetf in the foot .</sentence>
				<definiendum id="0">p % 'J</definiendum>
				<definiendum id="1">AFTER act</definiendum>
				<definiens id="0">DONE x act ) )</definiens>
			</definition>
			<definition id="4">
				<sentence>First , NIL denotes a primitive act -the empty sequence , llence , doing it would satisfy Proposition 25 , but the agent never does anything substantive .</sentence>
				<definiendum id="0">NIL</definiendum>
				<definiens id="0">a primitive act -the empty sequence</definiens>
			</definition>
			<definition id="5">
				<sentence>First , we will say an agcnt i~ SINCERE with respect to p if whenever his goal is to get someone else to belietpe p , his goal is in fact to get that person to knom p. dec Definition tl ( SINCERE x p ) = ( GOAL x ( laEL y p ) ) D ( GOAL x ( KNOW y p ) ) An agent is HELPFUL to another if he adopts as his own persistent goal another agent 's goal that he eventually do something ( provided that potential goal does not conflict with his own I. Definition 12 ( HELPFUL x y ) a , ¢= 'Ca ( BEL x ( GOAL y ( } ( DONE y a ) ) ) ^ ~ ( GOAL x ~ ( DONE x a ) ) D ( P-GOAL x ( DONE x a ) ) Agent x thinks agent y is more EXPERT about the true of p than x if he always adopts x 's beliefs about p as his own .</sentence>
				<definiendum id="0">DONE x a )</definiendum>
				<definiens id="0">KNOW y p ) ) An agent is HELPFUL to another if he adopts as his own persistent goal another agent 's goal that he eventually do something</definiens>
				<definiens id="1">BEL x ( GOAL y ( } ( DONE y a )</definiens>
			</definition>
			<definition id="6">
				<sentence>; lIMPER x y `` do y act'\ ] ) :3 O ( DONE y act ) We will give the major steps of the proof in Fi~lre I , and point In their justifications .</sentence>
				<definiendum id="0">lIMPER x y</definiendum>
			</definition>
			<definition id="7">
				<sentence>are axioms of belief , a plan is a proof that h ( BEL x ( p. A ... ^ p~ ) ) ~ ( BEL x ( RESULT x a 'l ) ) Among tile corollaries to a plan are } ( BEL x ( ( Po a ... ^ p , ) ~ ( RESULT x a q , ) ) ) i=\ [ ... . k and } ( BEL x ( ( p , '' a ... a Pi ) ~ ( ALWAYS q~-i D qi ) ) ) i : 1 ... . k \ ] =l '' ... . k There are two main points to be made about the~e corollaries .</sentence>
				<definiendum id="0">plan</definiendum>
				<definiens id="0">a proof that h ( BEL x ( p. A ... ^ p~ ) ) ~ ( BEL x ( RESULT x a 'l ) ) Among tile corollaries to a plan are } ( BEL x ( ( Po a ... ^ p , ) ~ ( RESULT x a q , ) ) ) i=\ [ ... . k and } ( BEL x ( ( p , '' a ... a Pi ) ~ ( ALWAYS q~-i D qi</definiens>
			</definition>
			<definition id="8">
				<sentence>A 3ummary consists of a name , a list of free variables , a distingafished free variable called the agent of the summary ( who will always be list , ,d tirst ) , an Effect which is a wff , a optional Body which is either an action or a wff and finally , an optional Gate which is a wff .</sentence>
				<definiendum id="0">3ummary</definiendum>
			</definition>
			<definition id="9">
				<sentence>; lIMPER x y `` do y act*\ ] ) ( BMB y x ( GOAL x ( BEL y ( GOAL x ( P-GOAL y ( DONE y act ) ) ) ) ) ) A * ( BMB y x ( SINCERE x ( GOAL x ( P-GOAL y ( DONE y act ) ) ) ) ) ( BMB y x ( GOAL x ( P-GOAL y ( DONE y act ) ) ) ) ^ * ( BMB y x ( ALWAYS ( COMPETENT y ( DONE y act ) ) ) } ( BMB y x ( GOAL x O\ [ ( DONE y act ) v 4 .</sentence>
				<definiendum id="0">lIMPER x y</definiendum>
				<definiendum id="1">BMB y x ( GOAL x</definiendum>
				<definiendum id="2">BEL y ( GOAL x</definiendum>
				<definiendum id="3">P-GOAL y</definiendum>
				<definiendum id="4">SINCERE x ( GOAL x</definiendum>
				<definiendum id="5">P-GOAL y</definiendum>
				<definiendum id="6">BMB y x ( GOAL x</definiendum>
				<definiendum id="7">DONE y act</definiendum>
				<definiens id="0">P-GOAL y ( DONE y act ) ) ) ) ^ * ( BMB y x ( ALWAYS ( COMPETENT y ( DONE y act</definiens>
			</definition>
			<definition id="10">
				<sentence>\ [ REQUEST x y act\ ] : Gate : it ) ( BMB y x ( SINCERE x ( GOAL x ( P-GOAL y ( DONE y act ) ) ) ) ) ^ ( 2 ) ( BMB y x ( ALWAYS ( COMPETENT y ( DONE y act ) ) ) ) ( 3 ) ( BMB y x ~ ( ALWAYS ~ ( DONE y act ) ) ) Bo~i~ .</sentence>
				<definiendum id="0">BMB y x ( SINCERE x ( GOAL x</definiendum>
				<definiendum id="1">BMB y x ( ALWAYS</definiendum>
				<definiendum id="2">BMB y x ~ ( ALWAYS ~</definiendum>
				<definiens id="0">P-GOAL y ( DONE y act ) )</definiens>
			</definition>
			<definition id="11">
				<sentence>( BMB y x ( GOAL x ( BEL y ( GOAL x ( P-GOAL y { DONE y act ) ) ) ) } ) Effect : ( BMB y x ( GOAL x O ( DONE y act ) ) ) This summary allows us to conclude that any action preserving the Gate and making the Bod !</sentence>
				<definiendum id="0">BMB y x ( GOAL x ( BEL y ( GOAL x</definiendum>
			</definition>
			<definition id="12">
				<sentence>p p = ( ( : OAL speaker ( P-GOAL hearer ( DONE hearer/JUMP-INTO Laker\ ] ) ) ) .</sentence>
				<definiendum id="0">P-GOAL hearer</definiendum>
			</definition>
			<definition id="13">
				<sentence>( IJMB y x ( GOAL x ( BEL y ( GOAL x ( P-GOAL y p ) ) ) ) ) Effect : ( nMB y x ( GOAL x OPt ) Since the speaker only asks the hearer to make p true .</sentence>
				<definiendum id="0">IJMB y x ( GOAL x ( BEL y ( GOAL x</definiendum>
				<definiens id="0">P-GOAL y p ) )</definiens>
			</definition>
			<definition id="14">
				<sentence>Vy ( P-COAL y p ) A ( ALWAYS ( COMPETENT y p ) ) D O ( p v { BEL y { ALWAYS ~p ) ) ) Proo~ I , Q.E.D. { P.GOAL y ( DONE y act } } A { ALWAYS { COMPETENT y { DONE y actJ } } O { 3a { DONE y \ [ ( BEL y ( AFTER a p } } l ?</sentence>
				<definiendum id="0">Vy ( P-COAL y p ) A ( ALWAYS</definiendum>
				<definiens id="0">DONE y act } } A { ALWAYS { COMPETENT y { DONE y actJ</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>Generalized Phrase Structure grammars ( GPSG ) , Lexical Functional grunmmm ( LFG ) , Phrm~ Linking grammars ( PLG ) , and Tree Adjoining grammars ( TAG ) are some key examples of grammatical systems that have been and still continue to be investignted along theme lines .</sentence>
				<definiendum id="0">GPSG</definiendum>
				<definiendum id="1">LFG</definiendum>
				<definiendum id="2">PLG</definiendum>
				<definiendum id="3">Tree Adjoining</definiendum>
				<definiens id="0">grammars ( TAG ) are some key examples of grammatical systems that have been and still continue to be investignted along theme lines</definiens>
			</definition>
			<definition id="1">
				<sentence>We will now define T ( G ) : The set of alJ trees derived in G starting from initial trees in I. This set will be called the tree set of G. L ( G ) : The set of all terminal strinp which uppe'mr in the frontier of the trees in TIG ) .</sentence>
				<definiendum id="0">T ( G )</definiendum>
				<definiens id="0">The set of all terminal strinp which uppe'mr in the frontier of the trees in TIG</definiens>
			</definition>
			<definition id="2">
				<sentence>This set will be called the string language ( ~r langtiage ) of G. If L is the string language of s TAG G then we say that L is a Tree-Adjoinin~ I.angllage ( TAL ) .</sentence>
				<definiendum id="0">string language</definiendum>
				<definiendum id="1">TAL</definiendum>
				<definiens id="0">~r langtiage ) of G. If L is the string language of s TAG G then we say that L is a Tree-Adjoinin~ I.angllage (</definiens>
			</definition>
			<definition id="3">
				<sentence>L ( G ) , the string language of G is L- { , .</sentence>
				<definiendum id="0">L</definiendum>
				<definiens id="0">the string language of G is L- { ,</definiens>
			</definition>
			<definition id="4">
				<sentence>L I is a strictly context-sensitive language ( i.e. , s context , , sensitive language that i , not context-free ) .</sentence>
				<definiendum id="0">context-sensitive language</definiendum>
				<definiens id="0">s context , , sensitive language that i , not context-free )</definiens>
			</definition>
			<definition id="5">
				<sentence>or as O ( C ) where C is a subeet of the set of all suxifiacy trees adjoisable at u. I~ -- ~amp~ 2.4 : Let G == ( I~ . )</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a subeet of the set of all suxifiacy trees adjoisable at u. I~ -- ~amp~ 2.4 : Let G == ( I~ .</definiens>
			</definition>
			<definition id="6">
				<sentence>ght sibling is in A ( p , j , k.I I , i -m &lt; p and p ~ j , then we put their parent in A\ [ i , j , k , l I. This may be written as For n : l to J-t stop 1 do For p=u-t to J step 1 do for •11 left 8iblinp in A ( t.n.n , p\ ] and riKht 8iblinp in A\ [ p.J.k.l\ ] satlsfyins •pproprlatn rHCrlctlon8 put ~heix parents in A { £ , j , k.1\ ] .</sentence>
				<definiendum id="0">ght sibling</definiendum>
				<definiens id="0">satlsfyins •pproprlatn rHCrlctlon8 put ~heix parents in A { £ , j</definiens>
			</definition>
			<definition id="7">
				<sentence>If X is n node in A\ [ m , j , k , pJ and Y is the root of a a•xiliary tree with same symbol as that of X , such that Y is in A\ [ i , m , p , I\ ] ( ( i &lt; _ m _ &lt; p &lt; iori &lt; m_ &lt; p &lt; _lJand ( m &lt; j &lt; k ~ porto ~j ~_k &lt; p ) J. This may be writte• as for • = £ co J 8t*p t do for p = u ~o I stop t do tf t node X E A\ [ a.J.k.p\ ] and t , he root , of tuxllXary tree ~.• In k\ [ t , a.p , l\ ] t , heu put , X Xn A ( i.J , k , l\ ] Case 4 corresponds to the case where s •ode Y has only one child X If X E A~i , j , k , ll then put Y in A\ [ i , j , k , l\ [ . Repe~t Case 4 again if Y has us siblings. It is obvious that steps I0 through 15 ( cases a-e ) are completed in 0 ( •-* ) , beta•an the different cases have at most two nested for loop statements , the iterating variables taking values in the range 0 thro•gh u. They are repeated utmost 0 ( • 4 ) times , because o ( the four loop statements i• steps 6 through 9. The initialization phase ( steps 1 through 5 ) has a time complexity of 0 ( • + • : ) == 0 ( •2 ) . Step 15 is completed in O ( • ) . Therefore , the time complexity of the parsing algorithm is O ( •S ) . The main issue in proving the algorithm correct , is to show that while computing the contents of an element of the array A , we must have already determined the contents of other elements of the array needed to correctly complete this entry. We can show this inductively by considering each cue individually. We give an ; .uformal argument below. Case h We need to know the co•tents of A\ [ i , j , k.m\ [ , A\ [ m , p , p , I\ ] where m &lt; I , i &lt; m. when we are trying to compute the co•tents or Aii.j , k , l \ [ . Since I is the y &amp; riable itererated i• the outermost loop ( step 6 ) , we can assume ( by indnctio• hypothesis ) that for all m &lt; I and for all p , q , r , the coate•ts of A\ [ p , q , r , mJ are already computed. Hence , the contents of A\ [ i , j , k , mJ are known. Similarly , for all m &gt; i , and for all p , q , and r &lt; _ .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">the root of a a•xiliary tree with same symbol as that of X</definiens>
				<definiens id="1">£ co J 8t*p t do for p = u ~o I stop t do tf t</definiens>
			</definition>
			<definition id="8">
				<sentence>l gruamar X had an obligatory constraint associated with it then we retmm the obligatory constraint regarcllelm of the relationship between qt and p , mud q4 and s. if the constraint amsccinted with X is a null adjoining constraint , we seaociate ( qt , qt , CL , , q ) , and ( q , r , r , q4 ) with Y and Z resp~tively , and aamcinte the nuU adjoining enustramt with X. If the label o ( Z is a. where s E ~ , then we cboous s ~ q such that 6 ( q , a ) I s. In the nu II adjoining constr~nt c~ule , q is cheeeu such that 6 ( q , a ) == q4 .</sentence>
				<definiendum id="0">Z</definiendum>
				<definiens id="0">a null adjoining constraint</definiens>
			</definition>
			<definition id="9">
				<sentence>NaB constraint in the original grammar will force us to use null constraint ud not consider the cases where it is not the case that qt I p and q4 m s. If the label of Y is • terminal 'a ' then we chouse r such that 6* ( p , a ) m r. If the constraint at X is s nuU adjoining constraint , then • ¢ ( qt , a ) r. Case 3 : This corresponds to the cue where •either the left child V nor the right child Z of the node X is the ancestor of the foot node of a or if a is a initial tree .</sentence>
				<definiendum id="0">NaB constraint</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">the cue where •either the left child V nor the right child Z of the node</definiens>
			</definition>
			<definition id="10">
				<sentence>The induction hypothesis is that if all derived trees obtained after k &lt; _ n adjeininlg operations have the prepethy P then so will the derived after n + 1 adjoininp where P is defined as , Property P : If any node X in a derived tree -f bus the foot-node of the tree 0 to which X belongs labeDed Y as • descendant sucb that w z Y w= is the fro•tier of the s•btree of ~ rooted at X , then if ( ql , q~ , q.l , q4 ) had bee• as•oct•ted with X , 6* ( qt , wl ) m q= and 6 '' ( q3 , ws ) m q4 , and if w is the fro•tier of the subtree under the foot node of 0 i• `` /is then 6* ( q~ , w ) ~ q8if X is not the ancestor of the foot •ode of 0 then the subtree of 0 below is of the form wtw s. Suppme X has aso~inted with it ( ql , q , q , q2 ) the• 6* ( qt , wl ) -q , 5* ( q , w , ) = q , .</sentence>
				<definiendum id="0">induction hypothesis</definiendum>
				<definiendum id="1">qt , wl</definiendum>
				<definiens id="0">if all derived trees obtained after k &lt; _ n adjeininlg operations have the prepethy P then so will the derived after n + 1 adjoininp where P is defined as , Property P : If any node X in a derived tree -f bus the foot-node of the tree 0 to which X belongs labeDed Y as • descendant sucb that w z Y w= is the fro•tier of the s•btree of ~ rooted at X</definiens>
				<definiens id="1">ql , q~ , q.l , q4 ) had bee• as•oct•ted with X</definiens>
				<definiens id="2">q~ , w ) ~ q8if X is not the ancestor of the foot •ode of 0 then the subtree of 0 below is of the form wtw s. Suppme X has aso~inted with it ( ql , q , q</definiens>
			</definition>
			<definition id="11">
				<sentence>The• there is n node X ' in ~ ' such that X ' belo•p to the anxilliary tree 0f ( with the same structure as 01There are several rMes to consider Case 1 : X is the ancestor of the foot node of 01 , such that the fro•tier of the subtree of 0t rooted at X is wsYw 4 and the fro•tier of the subtree or 7 rooted at X is W|WlZW~W t. Let 6~ ( qt , w| ) an q , 6* ( q , wt ) -q , , 6* ( qa , w2 ) n r , and 6* ( r , wt ) -q4 .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">qa , w2</definiendum>
				<definiens id="0">the ancestor of the foot node of 01</definiens>
				<definiens id="1">W|WlZW~W t. Let 6~ ( qt</definiens>
			</definition>
			<definition id="12">
				<sentence>The arrow denotes the head of the string , which in turn determines where the string is split up when wrapping operation takes place .</sentence>
				<definiendum id="0">arrow</definiendum>
				<definiens id="0">the head of the string , which in turn determines where the string is split up when wrapping operation takes place</definiens>
			</definition>
			<definition id="13">
				<sentence>We shaft assume , without loss of general/t ) ' , that the constra/nts expressed at the nodes of elementary trees of G ~re I ) Nothing can be •de•heed st • node ( NA ) , 2 ) Any appropriat~ tree ( ~mbob at the node and root of the ~*uxillimry tree must marsh ) can be adjoined ( AA ) , or 3 ) Adjoining at the node is •brig•tory ( OA ) .</sentence>
				<definiendum id="0">NA</definiendum>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>l '' The logical notation then is just first-order predicate calculus , where the universe of discourse is a rich set of individuals , which are real , possible auad even impossible objects , events , conditions , eventualities , and so on .</sentence>
				<definiendum id="0">discourse</definiendum>
				<definiens id="0">a rich set of individuals , which are real , possible auad even impossible objects , events , conditions , eventualities</definiens>
			</definition>
			<definition id="1">
				<sentence>In particular , the de dieto reading of ( 5 ) would be represented by something like ( 11 ) believe ( J , P ) A spy ' ( P , S ) A believe ( J , Q ) A at ' ( Q , S , T ) where T is the next table .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">the next table</definiens>
			</definition>
			<definition id="2">
				<sentence>Then we have believe ( J , P , ) A vine ' ( P , , M S ) A believe ( J , Q : ) AMorning-b'tar ' ( Q : , MS ) This is a representation for tile paradoxical sentence ( 15 ) .</sentence>
				<definiendum id="0">MS ) This</definiendum>
				<definiens id="0">a representation for tile paradoxical sentence ( 15 )</definiens>
			</definition>
			<definition id="3">
				<sentence>Sta¢ ( Qc , F , S ) l tend r.o prefer co cl~nk o ( the vaJue o ( a ( ES , J , Qt ) as sa abstract entity .</sentence>
				<definiendum id="0">vaJue o</definiendum>
				<definiens id="0">a ( ES , J , Qt ) as sa abstract entity</definiens>
			</definition>
			<definition id="4">
				<sentence>Metonymy is a very common phenomenon in discourse , but l prefer to think o ( it as occurring irregularly , sad not 8a siKnalled systematieafly by other elemenu , in the sentence .</sentence>
				<definiendum id="0">Metonymy</definiendum>
				<definiens id="0">a very common phenomenon in discourse , but l prefer to think o ( it as occurring irregularly</definiens>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>St is a highly presuppositionai structure , since the subject and object of the original verb are often deleted during the transformation and the reader must then supply these arguments from world knowledge .</sentence>
				<definiendum id="0">St</definiendum>
				<definiens id="0">a highly presuppositionai structure , since the subject and object of the original verb are often deleted during the transformation</definiens>
			</definition>
			<definition id="1">
				<sentence>This is a lucky fact , sSnce the syntactic forms used in quoted speech are usually much less constrained than those in non-quoted portions .</sentence>
				<definiendum id="0">sSnce</definiendum>
				<definiens id="0">the syntactic forms used in quoted speech are usually much less constrained than those in non-quoted portions</definiens>
			</definition>
			<definition id="2">
				<sentence>DUMP is feted from reliance on such scripts because of the fact that the news reporter , however unconsciously , encodes key requests syntactically .</sentence>
				<definiendum id="0">DUMP</definiendum>
				<definiens id="0">encodes key requests syntactically</definiens>
			</definition>
			<definition id="3">
				<sentence>Natural Language Information Processing : A Computer Grammar of English and its Applications .</sentence>
				<definiendum id="0">Natural Language Information Processing</definiendum>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>Alphabetical ordering is the only way that a lexicographer who works by hand can keep track of his data , but an alphabetical order puts together words with similar spellings and scatters haphazardly words with similar meanings .</sentence>
				<definiendum id="0">Alphabetical ordering</definiendum>
				<definiens id="0">the only way that a lexicographer who works by hand can keep track of his data , but an alphabetical order puts together words with similar spellings and scatters haphazardly words with similar meanings</definiens>
			</definition>
			<definition id="1">
				<sentence>The Dictionary of Contemporary Enalish~ which is very popular with people learning English as a second language , uses a constrained vocabulary of about 2,000 words ( plus some specialized terms ) to write its definitions .</sentence>
				<definiendum id="0">Dictionary of Contemporary Enalish~</definiendum>
				<definiens id="0">uses a constrained vocabulary of about 2,000 words ( plus some specialized terms</definiens>
			</definition>
			<definition id="2">
				<sentence>Ordered access -- Search starts with the most frequent sense and continues serially until a sense ks found that satisfies the context .</sentence>
				<definiendum id="0">Search</definiendum>
				<definiens id="0">starts with the most frequent sense and continues serially until a sense ks found that satisfies the context</definiens>
			</definition>
			<definition id="3">
				<sentence>Although the nature of semantic primitives is a matter of considerable interest to anyone who proposes a semantic notation for writing the definitions that a language processing system will use , they have received relatively little attention from psychologists .</sentence>
				<definiendum id="0">semantic primitives</definiendum>
				<definiens id="0">a matter of considerable interest to anyone who proposes a semantic notation for writing the definitions that a language processing system will use</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>Define the depth d ( n ) of a tree node n to be 0 for the root and d ( p ) + I if p is the parent of n. Each virtual-copy array a has also a positive depth D ( a ) &gt; max { d ( n ) : n is a node of a } .</sentence>
				<definiendum id="0">Define the depth d</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">a node of a }</definiens>
			</definition>
			<definition id="1">
				<sentence>Dereferencing is the process of following such chains of rerouting pointers to reach a dug that has not been rerouted .</sentence>
				<definiendum id="0">Dereferencing</definiendum>
				<definiens id="0">the process of following such chains of rerouting pointers to reach a dug that has not been rerouted</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>ABSTRACT INTRODUCTION Anaphora resolution is the process of determining the referent of ~uaphors .</sentence>
				<definiendum id="0">ABSTRACT INTRODUCTION Anaphora resolution</definiendum>
				<definiens id="0">the process of determining the referent of ~uaphors</definiens>
			</definition>
			<definition id="1">
				<sentence>The backward-looking center links the current sentence to the preceding discourse .</sentence>
				<definiendum id="0">backward-looking center</definiendum>
				<definiens id="0">links the current sentence to the preceding discourse</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>WordSmith allows the user to explore a multidimensional space of information about words .</sentence>
				<definiendum id="0">WordSmith</definiendum>
				<definiens id="0">allows the user to explore a multidimensional space of information about words</definiens>
			</definition>
			<definition id="1">
				<sentence>PRONUNC represents pronunciations in an alphabet derived from Webster 's Seventh Collegiate Dictionary .</sentence>
				<definiendum id="0">PRONUNC</definiendum>
				<definiens id="0">pronunciations in an alphabet derived from Webster 's Seventh Collegiate Dictionary</definiens>
			</definition>
			<definition id="2">
				<sentence>277 ureide uremia uremic ureter ureteral ureteric urethan urethane urethra urethrae urethral urethritis urethroscope urethroscopic urge somebody perfidy subsidy burgundy hypertrophy courtesy discourtesy reluctancy decumbency recumbency incumbency redundancy fervency conservancy pungency pudency agency subagency regency exigency plangency tangency stringency astringency contingency pungency cogency emergency detergency convergency l-urgency ... ... ... ... ... ... ... ... ... ... ... ... ... . I I N : &gt; *R-J*N-SE3 I I u : &gt; * r : R g : d e : * n : N c : S y : E3 I i I urgent uric uricosuric uridine uriel urim and thumm urinal urinalysis urinary urinate urination urine urinogenital urinometer urinometric detergency surgeoncy insurgency convergency emergency indeterminacy pertinency impertinency repugnancy permanency impermanency currency trustworthy twopenny company insurgency deficiency efficiency inefficiency sufficiency insufficiency proficiency expediency inexpediency resiliency leniency conveniency inconvenienc incipiency pruriency APPLICATION : PRONUNC COMMAND : DIM1 : PRONUNC DIM2 : RHYME DIM3 : REVERSE DIM4 : Figure 1 .</sentence>
				<definiendum id="0">DIM1</definiendum>
				<definiens id="0">N c : S y : E3 I i I urgent uric uricosuric uridine uriel urim and thumm urinal urinalysis urinary urinate urination urine urinogenital urinometer urinometric detergency surgeoncy insurgency convergency emergency indeterminacy pertinency impertinency repugnancy permanency impermanency currency trustworthy twopenny company insurgency deficiency efficiency inefficiency sufficiency insufficiency proficiency expediency inexpediency resiliency leniency conveniency inconvenienc incipiency pruriency APPLICATION : PRONUNC COMMAND :</definiens>
			</definition>
			<definition id="3">
				<sentence>The fh'st is a main file which is keyed on the spelling of words and which contains pronunciations organized according to part of speech .</sentence>
				<definiendum id="0">fh'st</definiendum>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>The word tag disambiguation program uses these markers to reduce the probability of the less likely tags occurring Lu context ; ' @ ' results in the probability being halved , ' % ' results in the probability being divided by eight .</sentence>
				<definiendum id="0">word tag disambiguation program</definiendum>
				<definiens id="0">uses these markers to reduce the probability of the less likely tags occurring Lu context ; ' @ ' results in the probability being halved , ' % ' results in the probability being divided by eight</definiens>
			</definition>
			<definition id="1">
				<sentence>Unpublished document : Unit for Computer Research on the ~hglish language , University of lancaster .</sentence>
				<definiendum id="0">Unpublished document</definiendum>
				<definiens id="0">Unit for Computer Research on the ~hglish language</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>HPSG 's grammax contains fewer tha4z twenty ( very general ) rules ; its predecessor required over 350 to achieve roughly the same coverage .</sentence>
				<definiendum id="0">HPSG 's grammax</definiendum>
				<definiens id="0">contains fewer tha4z twenty ( very general ) rules ; its predecessor required over 350 to achieve roughly the same coverage</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , our lexicon database contains frames specifying properties of classes of words , such as the VERB class , which numbers among its subclasses BASE and FINITE .</sentence>
				<definiendum id="0">VERB class</definiendum>
				<definiens id="0">numbers among its subclasses BASE and FINITE</definiens>
			</definition>
			<definition id="2">
				<sentence>We specify that the generic VERB frame includes in its features ( MAJOR V ) , that the PRESENT-TENSE frame includes ( TENSE PRES ) , that the THIRD-SING frame includes ( AGREEMENT 3RD-SING ) , that the SUBJECT-RAISE frame includes ( CONTROL SSR ) , and the AUXILIARY frame includes ( AUX PLUS ) .</sentence>
				<definiendum id="0">AUXILIARY frame includes</definiendum>
				<definiens id="0">the generic VERB frame includes in its features ( MAJOR V ) , that the PRESENT-TENSE frame includes ( TENSE PRES ) , that the THIRD-SING frame includes</definiens>
			</definition>
			<definition id="3">
				<sentence>A prose description of this passive lexical rule follows : Passive Lexicai Rule If F0 is a trm~sitive verb frame with spelling XXX , then F1 is the corresponding passive frame , where ( I ) FI is an instance of the generic PASSIVE class frame ( 2 ) FI has as its spelling whatever the past particip|e 's spelling is for F0 ( XXXED if regular , stipulated if irregular ) ( 3 ) F1 has as its subject 's role the role of F0 's object , and assigns the role of F0 's subject to F1 's optional PP-BY .</sentence>
				<definiendum id="0">prose description</definiendum>
				<definiendum id="1">FI</definiendum>
				<definiens id="0">Passive Lexicai Rule If F0 is a trm~sitive verb frame with spelling XXX , then F1 is the corresponding passive frame</definiens>
				<definiens id="1">regular , stipulated if irregular ) ( 3 ) F1 has as its subject 's role the role of F0 's object , and assigns the role of F0 's subject to F1 's optional PP-BY</definiens>
			</definition>
			<definition id="4">
				<sentence>( TRANSITIVE ( CLASSES ( subcyclic ) ) ( OBLIGATORY ( object ) ( subject ) ) ( FEATURES ( control trans ) ) ( LEX-RULES ( passive-rule ) ) ) The generic frame of which every output from the passive rule is an instance looks as follows : ( PASSIVE ( CLASSES ( verb ) ) ( FEATURES ( predicative plus ) ( form pas ) ) ( OPTIONAL ( pp-by ) ) ) An example , then , of a verb frame which serves as input to the passive rule is the frame for the transitive verb make , whose entry in the lexicon is given below .</sentence>
				<definiendum id="0">TRANSITIVE</definiendum>
				<definiendum id="1">CLASSES</definiendum>
				<definiendum id="2">) ( LEX-RULES</definiendum>
				<definiendum id="3">FEATURES</definiendum>
				<definiens id="0">subcyclic ) ) ( OBLIGATORY ( object ) ( subject ) ) ( FEATURES ( control trans</definiens>
				<definiens id="1">predicative plus ) ( form pas ) ) ( OPTIONAL ( pp-by ) ) ) An example , then , of a verb frame which serves as input to the passive rule is the frame for the transitive verb make , whose entry in the lexicon</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>Hornstem develops a theory of tense w # th # n the Re~cnenbachlan framewcrk whtch postulates threetheoretical entit~es : S ( the moment of speech } , R ( a relerence point } , and E ( the moment of event ) .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">E (</definiendum>
				<definiens id="0">the moment of speech }</definiens>
				<definiens id="1">the moment of event )</definiens>
			</definition>
			<definition id="1">
				<sentence>A. ( &lt; = ) - &gt; B -- A. ( &gt; ) - &gt; B In each of these cases , the operation involves the addihon of new members to the adm=ss=Dle set .</sentence>
				<definiendum id="0">operation</definiendum>
				<definiens id="0">involves the addihon of new members to the adm=ss=Dle set</definiens>
			</definition>
			<definition id="2">
				<sentence>m l &lt; t ( slncetdt ' ) -3m 2Et'.m l &lt; m 2 &lt; t ( bydensltyoft=mepolnts ) Let t '' be the interval \ [ m 1 .m2\ ] Then. we have t '' ( t and t '' C t'. By ( P2 ) . we have OCCUR ( p , t '' ) . That is , 0 has occurred. -- I. The charactenzat , on of process verb by Allen ( ms O.2 ) is less sat=slactory because ~t combines both the notion of Drogresswe asDect ( his `` OCCURRING '' ) and me process verb into the same axiom Furthermore. the difference between me predicate `` OCCUR '' and `` OCCURRING '' ~s not adequately exolamed in his paper. An event verb shares an ~moortant proDerty with a brocess verb. namely. , t can be true only at a non.instantaneous interval. ( El ) : OCCUR ( e.t ) - ! bet ( t ) ( E2 ) : OCCUR ( e.t ) - ( V r ) ( per ( t ' ) A r C t - '' ~ OCCUR ( e , r ) The following theorem snows that the ~rogresslve form of an event verb entads the negal~on of the perfect form. ( E-THEOREM ) : OCCUR ( PROG ( e.t ) ) -'- , ( 3 r ) ( per ( t ' ) A r &lt; t A OCCUR ( e , t ' ) ) Proof AS in the ~roof of ( P.THEOREM ) . we can find a non-~nstantaneous interval t '' such that t '' &lt; t and t '' C t ' But |or any such t '' . we have OCCUR ( e.t '' ) Pecause of ( E2 ) . That is. it can not be the case t11at e has occurred. -- I. Again the crucial property ( El ) is not captured by Allen 's charactenzat=on of events ( ms O.1 ) . To account for the variety of aspect interpretations as presented in section 3.1.2 , I propose the following constraint on 25 situation/perspective type : ( C-ASPECT\ ] : Let `` dynamic '' stand for a process or event. ( a ) simple/dynamic .-* morn ( t ) ( b ) simple/state ..per ( t ) ( c ) progressive/dynamic -.-* per ( t ) /k _C PerspeCtive is a way of looking at the situateon type. For process or event , the simple aspect treats U~e situation as an instantaneous interval even though the situation ~tself may not be instantaneous. For state , the simple aspect retains its duration. The progressive aspect essentially views a process or event from its inter=or , thus requiring a stance in which the situation is a non.instantaneous interval and the admissible temporal relationship to be the C_ relations , i.e. , s , s~ , I , fi.d. di , eoual. Let me show graphically how C.ASPECT accounts for the aspect interpretations of sentences { 9\ ] to { 12\ ] . \ [ g'\ ] simple/process WHEN simple/event Admissible relations : ( m : mi X Y XY X YX Y ) Y X \ [ to'\ ] AOmissib\ ] e relations : progressive/process WHEN slmple/event si di fi XXX XXX XXX Y Y Y \ [ 11'\ ] simple/state WHEN s~mple/event Admissible relations : &gt; mi si di fi Y XXX YXXX XXX XXX XXX Y Y Y m &lt; XXXY XXX Y \ [ 12'\ ] prog/process WHEN prog/event Admissible relations : : f fi s si XXX XXX XXXX XXX XXXX YYY YYYY YYY YYYY YYY XX XXXX YYYY YY In this paper , I nave exam=ned two problems regarding linguistic semantics : tense and asDect .</sentence>
				<definiendum id="0">event verb</definiendum>
				<definiendum id="1">event.</definiendum>
				<definiendum id="2">_C PerspeCtive</definiendum>
				<definiens id="0">the difference between me predicate `` OCCUR '' and `` OCCURRING '' ~s not adequately exolamed in his paper. An</definiens>
				<definiens id="1">t ) ( E2 ) : OCCUR ( e.t ) - ( V r ) ( per ( t '</definiens>
				<definiens id="2">a non.instantaneous interval and the admissible temporal relationship to be the C_ relations , i.e. , s , s~ , I</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>I. INTRODUCTION Movement is an important phenomenon in natural languages .</sentence>
				<definiendum id="0">I. INTRODUCTION Movement</definiendum>
			</definition>
			<definition id="1">
				<sentence>The grammar generates the strings -- a , b. acd .</sentence>
				<definiendum id="0">grammar</definiendum>
				<definiens id="0">generates the strings -- a , b. acd</definiens>
			</definition>
			<definition id="2">
				<sentence>A pattern describes the me=age= to which a node rmponds , and the new message= and internal state= that are produced .</sentence>
				<definiendum id="0">pattern</definiendum>
				<definiens id="0">describes the me=age= to which a node rmponds , and the new message= and internal state= that are produced</definiens>
			</definition>
			<definition id="3">
				<sentence>For the current paper we will assume that exogenous sources remain fully on for the duration of the sentenco , s In Figure 3 ( b ) , another exogenous source Exog-srcl activates c , which furthers the pattern for RO .</sentence>
				<definiendum id="0">exogenous source Exog-srcl activates c</definiendum>
				<definiens id="0">furthers the pattern for RO</definiens>
			</definition>
			<definition id="4">
				<sentence>RO sends an inhibition message to QO , posts expectations for S , and relays an activation message to P0 , which rebind~ its variable to RO and a~umes a new activation value .</sentence>
				<definiendum id="0">RO</definiendum>
				<definiens id="0">sends an inhibition message to QO , posts expectations for S , and relays an activation message to P0 , which rebind~ its variable to RO and a~umes a new activation value</definiens>
			</definition>
			<definition id="5">
				<sentence>Subjacency , and the Structure Preserving Hypothesis ) from the detailed structural descriptions of earlier transformational theories ( Chomsky , 1981 ) , Our research can be viewed as an attempt tu induce the machine that embodies theae principles .</sentence>
				<definiendum id="0">Structure Preserving Hypothesis</definiendum>
			</definition>
			<definition id="6">
				<sentence>Relative Pronoun Insertion contextual processing and learning capabilities based on a formal notion of expectations .</sentence>
				<definiendum id="0">Relative Pronoun Insertion</definiendum>
				<definiens id="0">contextual processing and learning capabilities based on a formal notion of expectations</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>Suppose~o = zAz and that ~o , $ 6 : P where P is a some set of strings .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a some set of strings</definiens>
			</definition>
			<definition id="1">
				<sentence>There ; tre consider ; Lble savings of computatioaal resources in not having to compare every element of the set with every other element to generate all possible equivalent strings which would take O ( n ~ ) time where n is the cardinality of the set .</sentence>
				<definiendum id="0">Lble savings</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">of computatioaal resources in not having to compare every element of the set with every other element to generate all possible equivalent strings</definiens>
				<definiens id="1">the cardinality of the set</definiens>
			</definition>
			<definition id="2">
				<sentence>t2 ) : Generate holds when Sentence is the conjoined sentence resulting/'ram the linearization of the pair of dilFerence lists ( Sl .</sentence>
				<definiendum id="0">Sentence</definiendum>
				<definiens id="0">the conjoined sentence resulting/'ram the linearization of the pair of dilFerence lists ( Sl</definiens>
			</definition>
			<definition id="3">
				<sentence>Parse holds when Sentence is the conjoined set , tence resulting from the linearization of the pair of dilference lists ( S1 .</sentence>
				<definiendum id="0">Sentence</definiendum>
			</definition>
			<definition id="4">
				<sentence>E2 ) provided that the set of candidate pairs for conjoining ( Subset ) is a subset of the set of pairs of equivalent terminal strings ( Set ) .</sentence>
				<definiendum id="0">E2</definiendum>
			</definition>
			<definition id="5">
				<sentence>lineariz~ : d a~ : { likes X } where X is the linearization of strings { Mary .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the linearization of strings { Mary</definiens>
			</definition>
			<definition id="6">
				<sentence>13 : Example of identical leading substrings The last case occurs when the two pairs of ( qonempty ) difference lists have no common leading substring , llere , the conjoined string will be the co , tcatenation nf the co.junctinn of one of the pairs from the candidate set , with the conjoined sqring resulting fr~nl the line ; trization of the two strings with their respective candidate substrings deleted .</sentence>
				<definiendum id="0">trization</definiendum>
				<definiens id="0">of the two strings with their respective candidate substrings deleted</definiens>
			</definition>
			<definition id="7">
				<sentence>16 ) : { John and Bill X. } where X is tl~e linearization of ~ ; trin~ , s { likes Mary , likes .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">tl~e linearization of ~ ; trin~ , s { likes Mary , likes</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>Systemic grammars employ a functional vocabulary : they empha~/ze the uses to which language can be put -- how languages achieve their speakers '' goaLs-rather than its formal structure .</sentence>
				<definiendum id="0">Systemic grammars</definiendum>
				<definiens id="0">employ a functional vocabulary : they empha~/ze the uses to which language can be put -- how languages achieve their speakers '' goaLs-rather than its formal structure</definiens>
			</definition>
			<definition id="1">
				<sentence>of-m 2 &gt; tmr~y.m ) ( pareetm~ # ~ Ttumtuvm Osto-ofltc~a &gt; # ~ Lbemn Uo~ &gt; ) ) This realization specification represents the structured object which gives the toplevel plan for this utterance .</sentence>
				<definiendum id="0">realization specification</definiendum>
				<definiens id="0">represents the structured object which gives the toplevel plan for this utterance</definiens>
			</definition>
			<definition id="2">
				<sentence>port complement ; \ ] o &lt; hi ( by atsmstle. • r~ure 8 1 '' ~ path •mr attadunem From this discussion one can tee that our urea•taunt of art•thin•at usa two tt~tctuges , an attachment point and • choice , where • TAG would oedy use cme structure , an anx/lia~ tree. Tim is • amsequeace of the fact that we are working with a performance medel of generation that m , ,~ , how explicitly how coacupm~ in/ormafion units arts rendered into tea•as as part of • IxJychofinguisticafly plaus/ble process , while • TAG is • formaIiun for competence theories that oily aeed to qxcify the syntactic mnu~ : mm of the grammatical minp of a languagu. `` Vnis is a usnifa : ant cliff•race , but not one that should stand in our way in compming what the two theories have to offer each other. Comequeady in the , rest of this paper we wifl omit the of the psm aoumoa and a¢¢nchmunt point clefimtions to fs~liu~ me comptrtuxt of theoredad lames. wh-movement Earlier we illustrated the TAG mncept of `` \ ] inking '' by shemdng how one woukl , ran ~th - , ' , initial u'ee consisting of the /nmrrmo~ datum of a quest/on p/us the frooted wh-phnum and then build outward by ma : emvely •die/n/rig the des/red amdtiary phrases to the S node that intervenes baweea the wb-phram and the dame. Wh-quest/ons am thus built from the bottom up , as in fact is any sentence involving wa~ tsklng urn•retrial complements. This an•lyre has the dem~ble property of •flowing mus to state the dependencies between the W~3hrase aad the gap as a laced relation on a =ngie elementary tree , criminating the need to inducie any machinery for movemem iu the theory. Aft unbounded dependencies now derive from adjunczioas ( which , as far as the grammar is coucerned , ca• be made withemt limit ) , rather than m the exit migratkm of a c~mdtount 8cram dauses. We also find this iocaiRy property to be demable , aad an umlogous ~ in our ~m of qmsmi01m and osher kinds of W~lUesdcm and unbounded dupmdm~ axumJedm~ 100 This -ommm-u~ dmiKn haa comequencm for how the reaiizatien qmc~catiom for them comcP , ic~o~ mu~ be or~-i- , ~ Xa paxecu/ar , the logi~- 's urea/ ~tatiou of senu~d com~em~ ved~ u Id~ opw , non is am tenable m that ~e. For ~'~ , ,qde we cannm have the mu~m M , my. How may d , ~ d~d Re~m.~ r~ d , m In , # had ~ , dd it a~ac/~d ? be the ex~mssm : when ~ as , ~l~don ~x¢/ficm/ou. ~sm~ ~ ou realizn dm IJml~ opm'a~t fw~ , me ee~ o~ , -~.-~1 , ~e my thi. , d , and , ~ on. A local TAG , , , - , ym of Wk-movemen¢ requ~ , ,to have me Ltmlxla and the a singia `` hyer '' o4 the qxa~ation , otber~i~ we would be forcad to vio/am oae of me .A , ,~.S p , mcild , .of our theory of ~era~ion , aamely chat me ~ ia a reaiizabon clam may `` , ,~W ' only ~he immediam arlFuaenm of ~he , ,-it being reafiz~ ; they may ao¢ look `` ~ssicl~ '' those arguments to mbu~lUCmt levels of ~ m.uc~uru. princilde has , erred us we~l. aad we a : e to give it up without a very compe~ng P'~ , ,~a. We dec'.~l immsd to give up the iaummi ~m~ioa of ~mumt/a/ c : m~lement verb ~ u ~inKle exl : m~mo~ This move wu a. , y for , to make , /ace uw.h ~ am awkward m manil~Ltm ia the `` Era Coa~ gyle frame ~ , ,o~l~i~ ~ that we u~ ia our owu rmmmnS and we have p~m'red a ~m¢ionai myle wire r~lundant. ~ m~d ooacepma/ umB for qmte , ome ~ime. The rep~m~¢acmn we um inateacl ammmm to breaginll up d~e logical ~ into individua~ um~ , and s/lowin s ~em m inc/ud~ refm¢-nc~ m each oth~. U 1 tambd~quam/¢y-ot-sh/ps ) . anack ( lnq , qmmtiry-of-daps ) u2 `` , -- y ( -u-~ , u 0 U 3 = re~or~Reuten , U2 ) Given such a network u ~e r , .~ii~-~oa specificaaio~. d~e LC mu~ have mine l~nncip/e by wt , P.~ m ) uclSe w~e~e to start : which umt ~houJd form me ~ of ~he ~udace smu : nue to which the othe~ are then attached ? A tumuli prm¢il~e to adolx i~ to ~ ~m d~e `` oa~ '' , - , q , i~. me one that does am mention any other umm in im defimQon. We axe ~n~dermg aclopemg the po//cy that atria ~mm daouid be allowed onJy rmdizaUon~ as iaimd trees while ~mm whom defmitioa m~ `` pomunS m '' ( .- , - '' . $ ) other umm taou~d be aflowed o~y realizauem u au~ : xee , . We have rim. howe~e¢ , worked thxo~sh a/l M the ramificattom inch a poficy m/ght have on o~or parB of our l~meranon mode/ ; without ye~ ~lg whe~ it impn~ve or desra~ me o~w ~ M our mere , y , we axe relum~nt co aum't it as one of our hypoth__-._-~_ retalmS our ge~eranoa mode/ to TAG's. Given tbtt ~en ~ m , me r~indoe d the quea/en is fa~dy maiShdmward ( See F~gum 9 ) . The Lameda ¢qnemoa is amgned a realizat/oa dam for dau~ Wk oommscboss , wherentxm the emmmmd aXllummt cp -- * , *y-et-ddW is I~ '' ~ ia COMP , aad me body of me k p/aced in me H\ ] BAD pom~0u. At the mine ~me , the two m of quan~-e~-~ a : e , ~ mark~ The o~e ia COMP ~ ~mllned to the reaiiz~oa for w ; , phnu~ appmlma~ to quanuty ( e.g. it will have the choice how many X aad pmmbly related choicm such as &lt; aan~/ &gt; ~ ' w/dck and olhe¢ vaxiaum aplnopriam to rehmve chuu , m or oth~ pemtiom whe~ Wk commm~om can be m~d ) .</sentence>
				<definiendum id="0">Lameda ¢qnemoa</definiendum>
				<definiens id="0">a usnifa : ant cliff•race , but not one that should stand in our way in compming what the two theories have to offer each other. Comequeady in the</definiens>
				<definiens id="1">any sentence involving wa~ tsklng urn•retrial complements. This an•lyre has the dem~ble property of •flowing mus to state the dependencies between the W~3hrase aad the gap as a laced relation on a =ngie elementary tree , criminating the need to inducie any machinery for movemem iu the theory. Aft unbounded dependencies now derive from adjunczioas</definiens>
			</definition>
			<definition id="3">
				<sentence>McDonald &amp; Pustejovsky ( 19 &amp; ~a ) `` SAMSON : a computational theory of prose style in generation '' , ~gs of the 1985 meeting of the European Amociat/on for Computational Linguistics .</sentence>
				<definiendum id="0">~a ) `` SAMSON</definiendum>
				<definiens id="0">a computational theory of prose style in generation ''</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>In writing metamorpilosis grammars ( Colmerauer , 1978 ) , or definite clause grammars , DCG 's , ( a special case of metamorphosis grammars , Pereira and Warren .</sentence>
				<definiendum id="0">writing metamorpilosis grammars</definiendum>
				<definiendum id="1">DCG 's</definiendum>
				<definiens id="0">a special case of metamorphosis grammars , Pereira and Warren</definiens>
			</definition>
			<definition id="1">
				<sentence>The CHAT-80 system ( Pereira and Warren , 1982 , Pereira , 1983 ) is a three-pass system .</sentence>
				<definiendum id="0">CHAT-80 system</definiendum>
				<definiens id="0">a three-pass system</definiens>
			</definition>
			<definition id="2">
				<sentence>The syntactic component for an MLG consists of a declaration of the strong non-terminals , followed by a sequence of MLG syntax rules .</sentence>
				<definiendum id="0">syntactic component for an MLG</definiendum>
				<definiens id="0">consists of a declaration of the strong non-terminals , followed by a sequence of MLG syntax rules</definiens>
			</definition>
			<definition id="3">
				<sentence>A logical terminal is of the form 0p-L~ , where Op is a modification operator and LF is a logical form .</sentence>
				<definiendum id="0">Op</definiendum>
				<definiendum id="1">LF</definiendum>
				<definiens id="0">a logical form</definiens>
			</definition>
			<definition id="4">
				<sentence>The Operator is a term which determines how the semantic item can combine with other semantic items during semantic interpretation .</sentence>
				<definiendum id="0">Operator</definiendum>
				<definiens id="0">a term which determines how the semantic item can combine with other semantic items during semantic interpretation</definiens>
			</definition>
			<definition id="5">
				<sentence>an ( X ) , Q ) where @ q is an operator which causes Q to get unified wi~h the logical form of a further modificand .</sentence>
				<definiendum id="0">@ q</definiendum>
				<definiens id="0">an operator which causes Q to get unified wi~h the logical form of a further modificand</definiens>
			</definition>
			<definition id="6">
				<sentence>The syntactic structures manipulated by the compiled rules are represented as syntactic items , which are terms of the form syn ( Features , Oaughters ) where Features is a feature list ( to be defined ) , and Daughters is a list consisting of syntactic items and terminals .</sentence>
				<definiendum id="0">Features</definiendum>
				<definiendum id="1">Daughters</definiendum>
				<definiens id="0">a feature list ( to be defined ) , and</definiens>
				<definiens id="1">a list consisting of syntactic items and terminals</definiens>
			</definition>
			<definition id="7">
				<sentence>SynO represents the version before satisfying nt , and Syn represents the version after satisfying nt .</sentence>
				<definiendum id="0">SynO</definiendum>
				<definiendum id="1">Syn</definiendum>
				<definiens id="0">the version before satisfying nt</definiens>
				<definiens id="1">represents the version after satisfying nt</definiens>
			</definition>
			<definition id="8">
				<sentence>The compiled rules are as follows : np\ [ Syn , Strl , Str3 ) &lt; det ( Hod , Strl , Str2 ) &amp; npl ( syn ( np : nil , Hod : Hods ) , Syn , Hods , nil , Str2 , Str3 ) . npl ( Synl , Syn3 , Hodsl , Hods3 , Strl , Str4 ) &lt; premods ( Synl , Syn2 , Hodsl , Hod : Hods2 , Strl , Str2 ) &amp; noun ( Hod , Str2 , Str3 ) &amp; np2 ( Syn2 , Syn3 , Hods2 , Hods3 , Str3 , Str4 ) . np2 ( Synl , Syn2 , Hodsl , Hods2 , Strl , Str2 ) &lt; postmods ( Synl , Syn2 , Hodsl , Hods2 , Strl , Str2 ) . np2 ( syn ( Feas , HodsO ) , Syn , Hod : Hodsl , Hodsl , Strl , Str3 ) &lt; poss ( Mod , Strl , Str2 ) &amp; npl ( syn ( Feas , syn ( Feas , HodsO ) : Hods2 ) , Syn , Hods2 , nil , Str2 , Str3 ) . In the first compiled rule , the structure Syn to be associated with the call to 'np ' appears again in the second matrix structure argument of 'npl ' The first matrix structure argument of 'npl ' is syn ( np : nil , Mod : Hods ) . and this will turn out to be the value of Syn if no shifts are encountered. Here Hod is the 'syn ' structure associated with the determiner 'det ' , and Hods is the list of modifiers determined further by 'npi'. The feature list np : nil is constructed from the leading non-terminal 'np ' of this strong rule. ( It would have been np : Argl if np had a ( first ) argument Argl. ) \ [ n the second and third compiled rules , the matrix structure pairs ( first two arguments ) and the modifier difference list pairs are linked in a straightforward way to reflect sequencing. \ ] 'be fourth rule shows the effect of the shift. Here syn ( Feas , HodsO ) , the previous `` conjecture '' for the matrix structure , is now made simply the first modifier in the larger structure syn ( Feas , syn ( Feas , HodsO ) : Hods2 ) which becomes the new `` conjecture '' by being placed in the first argument of the further call to 'npl'. If the shift operator had been used in its binary form FO % npl , then the new conjecture would be syn ( NT : F , syn ( NT : FO , Mods0 ) : Hods2 ) where the old conjecture was syn ( NT : F , HodsO ) . \ [ n larger grammars , this allows one to have a completely correct feature list NT : FO for the leftembedded modifier. To illustrate the compilation of terminal symbols , let us look at the rule det = &gt; +O : Sdt ( D , PI , P2 , P ) : P2/Pt-P .</sentence>
				<definiendum id="0">npl ( syn ( Feas , syn</definiendum>
				<definiendum id="1">Hods</definiendum>
				<definiens id="0">Hods ) , Syn , Hods , nil</definiens>
				<definiens id="1">Synl , Syn2 , Hodsl , Hods2 , Strl , Str2 ) &lt; postmods ( Synl</definiens>
			</definition>
			<definition id="9">
				<sentence>The compiled rule is det ( syn ( det : nil , +D : P2/PI-P : nil ) , D.Str , Str ) &lt; dt ( D , PI , P2 , P ) . Note that both the surface terminal +D and the logical terminal P2/PI-P are entered as modifiers of the 'det ' node. The semantic interpretation component looks only at the logical terminals , but in certain applications it is useful to be able to see the surface terminals in the syntactic structures. As mentioned above , the display procedures for syntac=i¢ structures can optionally show only one type of terminal. 108 The display of the syntactic structure of the sentence `` Every man loves a woman '' produced by MLGRAM is as follows. sentence : nil np : Xl det : nil X2/X3-alI ( X3 , X2 ) l-man ( Xl ) l-love ( Xl , XA ) np : XA det : nil XS/X6-ex ( X6 , XS ) l-woman ( X &amp; ) Note that no 'vp ' node is shown in the parse tree ; 'vp ' is a weak non-terminal. The logical form produced for this tree by the semantic component given in the next section is all ( man ( Xl ) , ex ( woman ( X2 ) , love ( XI , X2 ) ) ) . Now let us look at the compilation of syntax rules for the one-pass mode. In this mode , syntactic structures are not built , but semantic structures are built up directly. The rule compiler adds extra arguments to non-terminals for manipulation of semantic structures , and adds calls to the top-level semantic interpretation procedure , 'semant'. The procedure 'semant ' builds complex semantic structures out of simpler ones , where the original building blocks are the logical terminals appearing in the MLG syntax rules. In this process of construction , it would be possible to work with semantic items ( and in fact a subsystem of the rules do work directly with semantic items ) , but it appears to be more efficient to work with slightly more elaborate structures which we call augmented semantic items. These ' are terms of the form sem ( Feas , Op , LP ) , where Op and \ [ 2 are such that Op-LF is an ordinary semantic item , and Fees is either a feature list or the list terminal : nil. The latter form is used for the initial augmented semantic items associated with logical terminals. As in the two-pass mode , the number of analysis structure arguments added to a non-terminal by the compiler depends on whether the non-terminal is strong or weak. If the original non-terminal is strong and has the form nt ( Xl , ... , Xn ) then in the compiled version we will have nt ( Xl , ... , Xn , Semsl , Sems2 , Strl , Str2 ) . Here ( Semsl , Sems2 ) is a difference list of augmented semantic items representing the list of semantic s~ruotures for the phrase associated by n~ with the word s~ring given by the difference list ( Strl , Sir2 ) . In the syntactic ( two-pass ) mode , only one argument ( for a 'syn ' ) is needed here , but now we need a list of structures because of a raising phenomenon necessary for proper scoping , which we will discuss in Sections A and 5. When the non-terminal nt is weak , five extra arguments are added , producing a compiled predication of the form nt ( Xl , ... , Xn , Fees , SemsO , Sems , Semsl , Sems2 , Strl , Str2 ) . Here Fees is the feature list for the matrix strong non-terminal. The pair ( SemsO , Sems ) represents the changing `` conjecture '' for the complete list of. daughter ( augmented ) semantic items for the matrix node , and is analogous to first extra argument pair in the two-pass mode. The pair ( Semsl , Sems2 ) holds a difference list for the sequence of semantic items analyzed by the weak non-terminal nt. Semsl will be a final sublist of SemsO , and Sems2 will of course be a final sub|ist of Semsl. For each strong rule , a cal-i to 'semant ' is added at the end of the compiled form of the rule. The form of the call is semant ( Feas , Sems , Semsl , Sems2 ) . Here teas is the feature list for the non-terminal on the left-hand side of the rule. Sems is the final version of the list of daughter semantic items ( after all adjustments for shifts ) and ( SemsL , Sems2 ) is the difference list of semantic items resulting from the semantic interpretation for this level. ( Think of Fees and Sems as input to 'semant ' , and ( Semsl , Sems2 ) as output. ) CSemsl , Sems2 ) will be the structure arguments for the non-terminal on the left-hand side of the strong rule. A call to 'semant ' is also generated when a shift is encountered , as we will see below. The actual working of 'semant ' is the topic of the next section. For the shift grammar fragment shown above , the compiled rules are as follows. np ( Sems , Sems0 , Strl , Str3 ) &lt; det ( Semsl , Sems2 , Strl , Str2 ) &amp; npl ( np : nil , Semsl , Sems3 , Sems2 , nil , Str2 , Scr3 ) a semant ( np : nil , Sems3 , Sems , SemsO ) . npl ( Feas , Semsl , Sems3 , Semsa , Sems7 , Strl , St\ [ ~ ) &lt; premods ( Feas , Semsl , Sems2 , SemsA , Sems5 , Strl , Str2 ) &amp; noun ( Sems5 , Sems6 , Str2 , Str3 ) &amp; np2 ( Feas , Sems2 , Sems3 , Sems6 , SemsT , Str3 , StrA ) . np2 ( Feas , Semsl , Sems2 , Sems3 , Semsd , Strl , Str2 ) &lt; postmods ( Feas , Semsl , Sems2 , Sems3 , SemsA , Strl , Str2 ) . npE ( Feas , Semsl.SemsA , SemsS , Sems6 , Strl , Str3 ) &lt; poss ( SemsS , Sems6 , Strl , Str2 ) &amp; semant ( Feas , Semsl , Sems2 , Sems3 ) &amp; npl ( Feas , Sems2 , Sems~ , Sems3 , nil , Str2 , Str3 ) . In the first compiled rule ( a strong rule ) , the pair ( Seres , SemsO ) is a difference list of the semantic items analyzing the noun phrase. ( Typically there 109 will just be one element in this list , but there can be more when modifiers of the noun phrases contain quantifiers that cause the modifiers to get promoted semantically to be sisters of the noun phrase. ) This difference list is the output of the call to 'semant ' compiled in at the end of the first rule. The input to this call is the list Sems3 ( along with the feature list np : nil ) . We arrive at Sems3 as follows. The list Semsl is started by , ! the call to det ; its first element is the determiner ( if there is one ) , and the list is continued in the list Sems2 of modifiers determined further by the call to 'npl'. In this call to 'npl ' , the initial list Semsl is given in the second argument of 'npl ' as the `` initial verslon for the final list of modifiers of the noun phrase. Sems3 , being in the next argument of 'npl ' , is the `` final version '' of the np modifier list , and this is the list given as input to 'semant'. \ [ f the processing of 'npl ' encounters no shifts , then Sems3 will just equal 5ems I. \ [ n the second compiled rule ( for 'npl ' ) , the `` versions '' of the total list of modifiers are \ [ inked in a chain ( Semsl , 5ems2 , Sems3 ) in the second and third arguments of the weak nonterminals. The actual modifiers produced by this rule are linked in a chain ( SemsA , Sems51 Sems6 , SemsT ) in the fourth and fifth arguments of the weak nonterminals and the first and second arguments of the strong non-terminals. A similar situation holds for the first of the 'np2 ' rules. \ [ n the second 'npZ ' rule , a shift is encountered , so a call to 'semant ' is generated. This is necessary because of the shift of levels ; the modifiers produced so far represent all the modifiers in an np , and these must be combined by 'semant ' to get the analysis of this np. As input to this call to 'semant ' , we take the list Semsl , which is the current version of the modifiers of the matrix np. The output is the difference list . ( Sems2 , gems3 ) . Sems2 is given to the succeeding call to 'npl ' as the new current version of the matrix modifier list. The tail Sems3 of the difference list output by 'semant ' is given to 'npl ' in its fourth argument to receive further modifiers. SemsA is the f~.nal uersion of the matrix modifier list , determined by 'npi I , and this information is also put in the third a , 'gument of 'np2'. The difference list ( Sems5 , Semsb ) contains the single element produced by 'poss ' , and this list tails off the list Semsl. When a semantic item Op-LF occurs in a rule body , the rule compiler inserts the augmented semantic item sem ( terminal : nil , Op , LF ) . As an example , the weak rule transverb ( X , Y ) ~ &gt; +V : $ tv ( V , X , Y , P ) : I-P .</sentence>
				<definiendum id="0">Xl det : nil X2/X3-alI</definiendum>
				<definiendum id="1">XA det</definiendum>
				<definiendum id="2">Op-LF</definiendum>
				<definiendum id="3">Fees</definiendum>
				<definiendum id="4">Sems )</definiendum>
				<definiendum id="5">SemsO )</definiendum>
				<definiens id="0">both the surface terminal +D and the logical terminal P2/PI-P are entered as modifiers of the 'det ' node. The semantic interpretation component looks only at the logical terminals , but in certain applications it is useful to be able to see the surface terminals in the syntactic structures. As mentioned above , the display procedures for syntac=i¢ structures can optionally</definiens>
				<definiens id="1">manipulation of semantic structures , and adds calls to the top-level semantic interpretation procedure</definiens>
				<definiens id="2">semantic items ) , but it appears to be more efficient to work with slightly more elaborate structures</definiens>
				<definiens id="3">an ordinary semantic item , and</definiens>
				<definiens id="4">the initial augmented semantic items associated with logical terminals. As in the two-pass mode</definiens>
				<definiens id="5">a difference list of augmented semantic items representing the list of semantic s~ruotures for the phrase associated by n~ with the word s~ring given by the difference list</definiens>
				<definiens id="6">the changing `` conjecture '' for the complete list of. daughter ( augmented ) semantic items for the matrix node , and is analogous to first extra argument pair in the two-pass mode. The pair ( Semsl , Sems2 ) holds a difference list for the sequence of semantic items analyzed by the weak non-terminal nt. Semsl will be a final sublist of SemsO</definiens>
				<definiens id="7">the feature list for the non-terminal on the left-hand side of the rule. Sems is the final version of the list of daughter semantic items ( after all adjustments for shifts</definiens>
				<definiens id="8">the difference list of semantic items resulting from the semantic interpretation</definiens>
				<definiens id="9">a difference list of the semantic items analyzing the noun phrase.</definiens>
				<definiens id="10">the f~.nal uersion of the matrix modifier list , determined by 'npi I</definiens>
				<definiens id="11">contains the single element produced by 'poss ' , and this list tails off the list Semsl. When a semantic item Op-LF occurs in a rule body , the rule compiler inserts the augmented semantic item sem ( terminal : nil , Op , LF )</definiens>
			</definition>
			<definition id="10">
				<sentence>compiles into the clause det ( Semsl , Sems2 , D.SemsA , Sems &amp; ) &lt; dt ( D , P1 , P2 , P ) &amp; semant ( det : nil , sem ( terminal : nil , P2/PI , P ) : nil , Semsl , Sems2 ) . The semantic interpretation schemes for both the one-pass mode and the two-pass mode share a large core of common procedures ; they differ only at the top level. In both schemes , augmented semantic items are combined with one another , forming more and more complex items , until a single item is constructed which represents the structure of the whole sentence. In this final structure , only the logical form component is of interest ; the other two components are discarded. We will describe the top levels for both modes , then describe the common core. The top level for the one-pass mode is simpler , because semantic interpretation works in tandem with the parser , and does not itself have to go through the parse tree. The procedure 'semant ' , which has interleaved calls in the compiled syntax rules , essentially is the top-level procedure , but there is some minor cleaning up that has to be done. If the top-level non-terminal is 'sentence ' ( with no arguments ) , then the top-level analysis procedure for the one-pass mode can be analyzeCSent ) &lt; sentence ( Sems , nil , Sent , nil ) &amp; semant ( top : nil , Sems , sem ( * , e , iF ) : nil , nil ) &amp; outlogform ( LF ) . Normally , the first argument , Sems , of 'sentence ' will be a list containing a single augmented semantic item , and its logical form component will be the desired logical form. However , for some grammars , the ~dditional call to 'semant ' is needed to complete the modification process. The procedure 'outlogform ' simplifies the logical form and outputs it. ~ne definition of 'semant ' itself is given in a single clause : semant ( Feas , Sems , Sems2 , Sems3 ) &lt; reorder ( Sems , Semsl ) &amp; modlist ( Semsl , sem ( Feas , id , t ) , Sem , Sems2 , Sem : Sems3 ) . Here , the procedure 'reorder ' takes the list Sems of augmented semantic items to be combined and re110 orders it ( permutes it ) , to obtain proper ( or most likely ) scoping. This procedure belongs to the common core of the two methods of semantic interpretation , and will be discussed further below. The procedure 'modlist ' does the following. A call modlist ( Sems , SemO , Sem , Semsl , Sems2 ) takes a list Sems of ( augmented ) semantic items and combines them with ( lets them modify ) the item SemO , producing an item Sem ( as the combination ) , along with a difference list ( Semsl , Sems2 ) of items which are promoted to be sisters of gem. The leftmost member of Sems acts as the outermost modifier. Thus , in the definition of 'semant ' , the result list Semsl of reordering acts on the trivial item sem ( Feas , id , t ) to form a difference list ( gems2 , Sem : Sems3 ) where the result Sem is right-appended to its sisters. 'modlist ' also belongs to the common core , and will be defined below. The top level for the two-pass system can be defined as follows. analyze2 ( Sent ) &lt; sentence ( gyn , Sent , nil ) &amp; synsem ( Syn , Sems , nil ) &amp; semant ( top : nil , gems , sem ( * , e , LF ) : nit , niI ) &amp; outlogform ( LF ) . The only difference between this and 'analyze ' above is that the call to 'sentence ' produces a syntactic item Syn , and this is given to the procedure 'synsem'. The latter is the main recursive procedure of the two-pass system. A call synsem ( Syn , SemsI , Sems2 ) takes a syntactic item Syn and produces a difference list ( Semsl , Sems2 ) of augmented semantic items representing the semantic structure of Syn. ( Typically , this list will just have one element , but it can have more if modifiers get promoted to sisters of the node. ) The definition of 'synsem ' is as follows. synsem ( syn ( Feas , Mods ) , Sems2 , Sems3 ) &lt; synsemlist ( Mods , Sems ) &amp; reorder ( Sems , Semsl ) &amp; modlist ( Semsl , sem ( Feas , id , t ) , Sem , Sems2 , Sem : Sems3 ) . Note that this differs from the definition of 'semant ' only in that 'synsem ' must first recursively process the daughters Mode of its input syntactic item before calling 'reorder ' and 'modlist ' The procedure 'synsemlist ' that processes the daughters is defined as follows. synsemlist ( syn ( Feas , Mods0 ) : Mods , Semsl ) &lt; / &amp; synsem ( syn ( Feas , ModsO ) , SemsI , Sems2 ) &amp; synsemlist ( Mods , Sems2 ) . synsemlist ( ( Op-LF ) : Mods , sem ( terminal : nil , Op , LF ) : Sems ) &lt; / &amp; synsemlist ( Mods , Sems ) . synsemlist ( Nod : Mods , Sems ) &lt; synsemlist ( Mods , Sems ) . synsemlist ( nil , nil ) . The first clause calls 'synsem ' recursively when the daughter is another 'syn ' structure. The second clause replaces a logical terminal by an augmented semantic item whose feature list is terminal : nil. The next clause ignores any other type of daughter ( this would normally be a surface terminal ) . Now we can proceed to the common core of the two semantic interpretation systems. The procedure 'modlist ' is defined recursively in a straightforward way : modlist ( Sem : Sems , Sem0 , Sem2 , Semsl , Sems3 ) &lt; modlist ( Sems , SemO , Seml , Sems2 , Sems3 ) &amp; modify ( Sem , Seml , Sem2 , Semsl , Sems2 ) . modlist ( nil , Sem , gem , Sems , Sems ) . Here 'modify ' takes a single item Sem and lets it operate on Seml , giving Sem2 and a difference list ( Semsl , Sems2 ) of sister items. Its definltion is modify ( Sem , Seml , Seml , Sem2 : Sems , Sems~ &lt; raise ( Sem , Seml , Sem2 ) &amp; /. modify ( sem ( * , Op , LF ) , sem ( Feas , Opl , LFI ) , sem ( Feas , Op2 , LF2 ) , Sems , Sems ) &lt; mod ( Op-LF , OpI-LFI , Op2-LF2 ) . Here 'raise ' is responsible for raising the item Seml so that it becomes a sister of the item Seml ; gem2 is a new version of Seml after the raising , although in most cases , gem2 equals geml. Raising occurs for a noun phrase like `` a chicken in every pot '' , where the quantifier `` every '' has higher scope than the quantifier `` a '' . The semantic item for `` every pot '' gets promoted to a left sister of that for `` a chicken '' . 'raise ' is defined basically by a system of unit clauses which look at specific types of phrases. For the small grammar MLGRAM of Section 2 , no raising is necessary , and the definition of 'raise ' can just be omitted. The procedures 'raise ' and 'reorder ' are two key ingredients of reshaping ( the movement of semantic items to handle scoping problems ) , which was discussed extensively in McCord ( 1982 , 1981 ) . \ [ n those two systems , reshaping was a separate pass of semantic interpretation , but } , ere , as in McCord ( 198 &amp; ) , reshaping is interleaved with the rest of semantic interpretation. In spite of the new toplevel organization for semantic interpretation of MLG 's , the low-level procedures for raising and reordering are basically the same as in the previous systems , and we refer to the previous reports for further discussion. The procedure 'mod ' , used in the second clause for 'modify ' , is the heart of semantic interpretation. mod ( Sem , Seml , Sem2 ) means that the ( non-augmented ) semantic item Sem modifies ( combines with ) the item Semi to give the item Sem2. 'mod ' is defined by a system consisting basically of unit clauses which key off the modification operators appearing in the semantic items. 111 In the experimental MLG described in the next section , there are 22 such clauses. For the grammar MLGRAM of Section 2 , the following set of clauses suffices. mod ( id -~ , Sem , Sem ) &lt; /. mod ( Sem , id -~ , Sem ) &lt; /. mod ( l-P , Op-Q , Op-R ) &lt; and ( P , Q , R ) . mod ( P/Q-R , Op-Q , @ P-R ) . mod ( @ P-Q , Op-P , Op-Q ) . The first two clauses say that the operator 'id ' acts like an identity. The second clause defines 'i ' as a left-conjoining operator ( its corresponding logical form gets left-conjoined to that of the modificand ) . The call and ( P , Q , R ) makes R=P &amp; Q , except that it treats 't ' ( 'true ' ) as an identity. The next clause for 'mod ' allows a quantifier semantic item like P/Q-each ( Q , P ) to operate on an item like I-man ( X ) to give the item @ P-each ( man ( X ) , P ) . The final clause then allows this item to operate on I-live ( X ) to give l-each ( man ( X ) , live ( X ) ) . The low-level procedure 'mod ' is the same ( in purpose ) as the procedure 'trans ' in HcCord ( 1981 ) , amd has close similarities to 'trans ' in McCord ( 1982 ) and 'mod ' in McCord ( 198 &amp; ) , so we refer to this previous work for more illustrations of this approach to modification. For MLGRAH , the only ingredient of semantic interpretation remaining to be defined is 'reorder'. We can define it in a way that is somewhat more general than is necessary for this small grammar , but which employs a technique useful for larger grammars. Each augmented semantic item is assigned a precedence number , and the reordering ( sorting ) is done so that wh @ n item B has higher precedence number than item A , then B is ordered to the left of A ; otherwise items are kept in their original order. The following clauses then define 'reorder ' in a way suitable for MLGRAM. reorder ( A : L , H ) &lt; reorder ( L , Ll ) &amp; insert ( A , Li , M ) . reordef ( nit , n£1 ) . insert ( A , B : L , S : Ll ) &lt; prec ( A , PA ) &amp; prec ( B , PB ) &amp; gt ( PB , PA ) &amp; / &amp; insert ( A , L , Li ) . insert ( A , L , a : L~. prec ( sem ( term~nal : * , e , ~ ) ,2 ) &lt; /. pruc ( sem ( relc ! ause : e , e , e ) , l ) &lt; /. prec ( e,3 ) . ~nus terminals are ordered to the end , except not after relative clauses. In particular , the subject and object of a sentence are ordered before the verb ( ~ terminal in the sentence ) , and this allows the ssraightforward process of modification in : mod ' to scope the quantifiers of the subject and object over the material of the verb. One can alter the definition of 'prec ' to get finer distinctions in ~coping , and for this we refer to McCord ( 1982 , 1981 ) . For a grammar as small as MLGRAM , which has no treatment of scoping phenomena , the total tomplexity of the MLG , including the semantic interpretation component we have given in this Section , is certainly greater than that of the comparable DCG in Section 2. However , for larger grammars , the modularity is definitely worthwhile -conceptually , and probably in the total size of the system. This section describes briefly an experimental MLG , called HODL , which covers the same linguistic ground as the grammar ( called HOD ) in HcCord ( 198l ) . The syntactic component of HOD , a DCG , is essentially the same as that in HcCord ( 1982 ) . One feature of these syntactic components is a systematic use of slot-filling to treat complements of verbs and nouns. This method increases modularity between syntax and lexicon , and is described in detail in McCord ( 1982 ) . One purpose of HOD , which is carried over to MODL , is a good treatment of scoping of modifiers and a good specification of logical form. The logical form language used by &gt; IODL as the target of semantic interpretation has been improved somewhat over that used for HOD .</sentence>
				<definiendum id="0">Op-LF )</definiendum>
				<definiendum id="1">LF )</definiendum>
				<definiendum id="2">synsemlist ( Mods , Sems ) . synsemlist ( Nod</definiendum>
				<definiendum id="3">gem2</definiendum>
				<definiendum id="4">MLGRAM</definiendum>
				<definiendum id="5">DCG</definiendum>
				<definiens id="0">compiles into the clause det ( Semsl , Sems2 , D.SemsA , Sems &amp; ) &lt; dt ( D , P1 , P2 , P ) &amp; semant ( det : nil , sem ( terminal : nil , P2/PI , P ) : nil , Semsl , Sems2 ) . The semantic interpretation schemes for both the one-pass mode and the two-pass mode share a large core of common procedures ; they differ only at the top level. In both schemes , augmented semantic items are combined with one another , forming more and more complex items , until a single item</definiens>
				<definiens id="1">discarded. We will describe the top levels for both modes , then describe the common core. The top level for the one-pass mode is simpler , because semantic interpretation works in tandem with the parser , and does not itself have to go through the parse tree. The procedure 'semant ' , which has interleaved calls in the compiled syntax rules , essentially is the top-level procedure , but there is some minor cleaning up that has to be done. If the top-level non-terminal is 'sentence ' ( with no arguments ) , then the top-level analysis procedure for the one-pass mode can be analyzeCSent ) &lt; sentence ( Sems , nil , Sent , nil ) &amp; semant ( top : nil , Sems , sem ( * , e , iF ) : nil , nil ) &amp; outlogform ( LF ) . Normally , the first argument , Sems , of 'sentence ' will be a list containing a single augmented semantic item , and its logical form component will be the desired logical form. However , for some grammars , the ~dditional call to 'semant ' is needed to complete the modification process. The procedure 'outlogform ' simplifies the logical form and outputs it. ~ne definition of 'semant ' itself is given in a single clause : semant ( Feas , Sems , Sems2 , Sems3 ) &lt; reorder ( Sems , Semsl ) &amp; modlist ( Semsl , sem ( Feas , id , t ) , Sem , Sems2 , Sem : Sems3 ) . Here , the procedure 'reorder ' takes the list Sems of augmented semantic items to be combined and re110 orders it ( permutes it ) , to obtain proper ( or most likely ) scoping. This procedure belongs to the common core of the two methods of semantic interpretation , and will be discussed further below. The procedure 'modlist ' does the following. A call modlist ( Sems , SemO , Sem , Semsl , Sems2 ) takes a list Sems of ( augmented ) semantic items and combines them with ( lets them modify ) the item SemO , producing an item Sem ( as the combination ) , along with a difference list ( Semsl , Sems2 ) of items which are promoted to be sisters of gem. The leftmost member of Sems acts as the outermost modifier. Thus , in the definition of 'semant ' , the result list Semsl of reordering acts on the trivial item sem ( Feas , id , t ) to form a difference list ( gems2 , Sem : Sems3 ) where the result Sem is right-appended to its sisters. 'modlist ' also belongs to the common core , and will be defined below. The top level for the two-pass system can be defined as follows. analyze2 ( Sent ) &lt; sentence ( gyn , Sent , nil ) &amp; synsem ( Syn , Sems , nil ) &amp; semant ( top : nil , gems , sem ( * , e , LF ) : nit , niI ) &amp; outlogform ( LF ) . The only difference between this and 'analyze ' above is that the call to 'sentence ' produces a syntactic item Syn</definiens>
				<definiens id="2">the main recursive procedure of the two-pass system. A call synsem ( Syn , SemsI , Sems2 ) takes a syntactic item Syn and produces a difference list ( Semsl , Sems2 ) of augmented semantic items representing the semantic structure of Syn. ( Typically , this list will just have one element , but it can have more if modifiers get promoted to sisters of the node. ) The definition of 'synsem ' is as follows. synsem ( syn ( Feas , Mods ) , Sems2 , Sems3 ) &lt; synsemlist ( Mods , Sems ) &amp; reorder ( Sems , Semsl ) &amp; modlist ( Semsl , sem ( Feas , id , t ) , Sem , Sems2 , Sem : Sems3 ) . Note that this differs from the definition of 'semant ' only in that 'synsem ' must first recursively process the daughters Mode of its input syntactic item before calling 'reorder '</definiens>
				<definiens id="3">second clause replaces a logical terminal by an augmented semantic item whose feature list is terminal : nil. The next clause ignores any other type of daughter ( this would normally be a surface terminal ) . Now we can proceed to the common core of the two semantic interpretation systems. The procedure 'modlist ' is defined recursively in a straightforward way : modlist ( Sem : Sems , Sem0 , Sem2 , Semsl , Sems3 ) &lt; modlist ( Sems , SemO , Seml , Sems2 , Sems3 ) &amp; modify ( Sem , Seml , Sem2 , Semsl , Sems2 ) . modlist ( nil , Sem , gem , Sems , Sems ) . Here 'modify ' takes a single item Sem and lets it operate on Seml , giving Sem2 and a difference list ( Semsl , Sems2 ) of sister items. Its definltion is modify ( Sem , Seml , Seml , Sem2 : Sems , Sems~ &lt; raise ( Sem , Seml , Sem2 ) &amp; /. modify ( sem ( * , Op , LF ) , sem ( Feas , Opl , LFI ) , sem ( Feas , Op2 , LF2 ) , Sems , Sems ) &lt; mod ( Op-LF , OpI-LFI , Op2-LF2 ) . Here 'raise ' is responsible for raising the item Seml so that it becomes a sister of the item Seml</definiens>
				<definiens id="4">a new version of Seml after the raising , although in most cases , gem2 equals geml. Raising occurs for a noun phrase like `` a chicken in every pot ''</definiens>
				<definiens id="5">the movement of semantic items to handle scoping problems ) , which was discussed extensively in McCord ( 1982 , 1981 ) . \ [ n those two systems , reshaping was a separate pass of semantic interpretation , but } , ere , as in McCord ( 198 &amp; ) , reshaping is interleaved with the rest of semantic interpretation. In spite of the new toplevel organization for semantic interpretation of MLG 's , the low-level procedures for raising and reordering are basically the same as in the previous systems</definiens>
				<definiens id="6">the heart of semantic interpretation. mod ( Sem , Seml , Sem2 ) means that the ( non-augmented ) semantic item Sem modifies ( combines with ) the item Semi to give the item Sem2. 'mod ' is defined by a system consisting basically of unit clauses which key off the modification operators appearing in the semantic items. 111 In the experimental MLG described in the next section</definiens>
				<definiens id="7">id -~ , Sem , Sem ) &lt; /. mod ( Sem , id -~ , Sem ) &lt; /. mod ( l-P , Op-Q , Op-R ) &lt; and ( P , Q , R ) . mod ( P/Q-R , Op-Q , @ P-R ) . mod ( @ P-Q , Op-P , Op-Q ) . The first two clauses say that the operator 'id ' acts like an identity. The second clause defines 'i ' as a left-conjoining operator ( its corresponding logical form gets left-conjoined to that of the modificand ) . The call and ( P , Q , R ) makes R=P &amp; Q , except that it treats 't ' ( 'true ' ) as an identity. The next clause for 'mod ' allows a quantifier semantic item like P/Q-each ( Q , P ) to operate on an item like I-man ( X ) to give the item @ P-each ( man ( X ) , P ) . The final clause then allows this item to operate on I-live ( X ) to give l-each ( man ( X ) , live ( X</definiens>
				<definiens id="8">the only ingredient of semantic interpretation remaining to be defined is 'reorder'. We can define it in a way that is somewhat more general than is necessary for this small grammar , but which employs a technique useful for larger grammars. Each augmented semantic item is assigned a precedence number , and the reordering ( sorting ) is done so that wh @ n item B has higher precedence number than item A , then B is ordered to the left of A ; otherwise items are kept in their original order. The following clauses then define 'reorder ' in a way suitable for MLGRAM. reorder ( A : L , H ) &lt; reorder ( L , Ll ) &amp; insert ( A , Li , M ) . reordef ( nit , n£1 ) . insert ( A , B : L , S : Ll ) &lt; prec ( A , PA ) &amp; prec ( B , PB ) &amp; gt ( PB , PA ) &amp; / &amp; insert ( A , L , Li ) . insert ( A , L , a : L~. prec ( sem ( term~nal : * , e , ~ ) ,2 ) &lt; /. pruc ( sem ( relc ! ause : e , e , e ) , l ) &lt; /. prec ( e,3 ) . ~nus terminals are ordered to the end , except not after relative clauses. In particular , the subject and object of a sentence are ordered before the verb ( ~ terminal in the sentence ) , and this allows the ssraightforward process of modification in : mod ' to scope the quantifiers of the subject and object over the material of the verb. One can alter the definition of 'prec ' to get finer distinctions in ~coping</definiens>
				<definiens id="9">has no treatment of scoping phenomena , the total tomplexity of the MLG , including the semantic interpretation component we have given in this Section , is certainly greater than that of the comparable DCG in Section 2. However , for larger grammars , the modularity is definitely worthwhile -conceptually , and probably in the total size of the system. This section describes briefly an experimental MLG , called HODL , which covers the same linguistic ground as the grammar ( called HOD ) in HcCord ( 198l ) . The syntactic component of HOD , a</definiens>
				<definiens id="10">a systematic use of slot-filling to treat complements of verbs and nouns. This method increases modularity between syntax and lexicon , and is described in detail in McCord ( 1982 ) . One purpose of HOD , which is carried over to MODL , is a good treatment of scoping of modifiers and a good specification of logical form. The logical form language used by &gt; IODL as the target of semantic interpretation has been improved somewhat over that used for HOD</definiens>
			</definition>
			<definition id="11">
				<sentence>The main predicates of LFL are word-senses for words in the natural language being analyzed , for ' example , believel ( X , Y ) in the sense `` X believes that Y holds '' .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">word-senses for words in the natural language being analyzed</definiens>
			</definition>
			<definition id="12">
				<sentence>Specifically , if P is a logical form and E is a variable , then P : E ( read `` P indexed by E '' ~ is also a logical form .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">a logical form</definiens>
			</definition>
			<definition id="13">
				<sentence>When an indexed logical form P : E appears as part of a larger logical form Q , and the index variable E is used elsewhere in Q. then E can be thought of roughly as standing for P together with its `` context '' .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">appears as part of a larger logical form Q</definiens>
			</definition>
			<definition id="14">
				<sentence>For some applications , it is sufficient to ignore contexts , and in such cases we just think of P : E as verifying P and binding E to an instantiation 112 of P. In fact , for PROLOG execution of logical forms without contexts , ' : ' can be defined by the single clause : P : P &lt; F. A specific purpose of the MOD system in McCord ( 1981 ) was to point out the importance of a class of predicates called focaiizers , and to offer a method for dealing with them in semantic interpretation. Focalizers include many determiners , adverbs , and adjectives ( or their word-senses ) , as well as certain non-lexical predicates like 'yesno'. Focalizers take two logical form arguments called the base and the fOCUS : focalizer ( Base , Focus ) . The Focus is often associated with sentence stress , hence the name. The pair ( Base , Focus ) is called the SCOpe of the focalizer. The adverbs 'only ' and 'even ' are focalizers which most clearly exhibit the connection with stress. The predication only ( P , Q ) reads `` the only case where P holds is when Q also holds '' . We get different analyses depending on focus. John only buys books at Smith's. only ( at ( smith , buy ( john , X1 ) ) , book ( X1 ) ) . John only buys books at Smith's. only ( book ( Xl ) &amp; at ( X2 , buy ( john , Xl ) ) , X2=smith ) . quantificational adverbs like 'always ' and 'seldom ' , studied by David Lewis ( 1975 ) , are also focalizers. Lewis made the point that these quantifiers are properly considered unseJKtJve , in the sense that they quantify over all the free variables in ( what we call ) their bases. For example , in John always buys books at Smith's. always ( book ( Xl ) &amp; at ( X2 , buy ( john , Xl ) ) , X2=smith ) • the quantification is over both X1 and X2. ( A paraphrase is `` Always , if X1 is a book and John buys X1 at X2 , then X2 is Smith 's '' . ) Quantificational determiners are also focalizers ( and are unselective quantifiers ) ; they correspond closely in meaning to the quantificational adverbs ( 'all ' 'always ' , 'many ' 'often ' , 'few ' 'seldom ' , etc. ) . We have the paraphrases : Leopards often attack monkeys in trees. often ( leopard ( Xl ) &amp; tree ( X2 ) &amp; in ( X2 , attack ( Xl , X3 ) ) , monkey ( X3 ) ) . Many leopard attacks in trees are ( attacks ) on monkeys. many ( leopard ( Xl ) &amp; tree ( X2 ) &amp; in ( X2 , attack ( Xi , X3 ) ) , monkey ( X3 ) ) . Adverbs and adjectives involving comparison or degree along some scale of evaluation ( a wide class ) are also focalizers. The base specifies the base of comparison , and the focus singles out what is being compared to the base. This shows up most clearly in the superlative forms. Consider the adverb `` fastest '' : John ran fastest yesterday. fastest ( run ( john ) : E , yesterday ( E ) ) . John ran fastest yesterday. fastest ( yesterday ( run ( X ) ) , X=john ) . In the first sentence , with focus on `` yesterday '' , the meaning is that , among all the events of John 's running ( this is the base ) , John 's running yesterday was fastest. The logical form illustrates the indexing operator. \ [ n the second sentence , with focus on `` John '' , the meaning is that among all the events of running yesterday ( there is an implicit location for these events ) , John 's running was fastest. As an example of a non-lexical focalizer , we have yesno ( P , q ) , which presupposes that a case of P holds , and asks whether P &amp; Q holds. ( The pair ( P , Q ) is like Topic/Comment for yes-no questions. ) Example : Did John see M @ ry yesterday ? yesno ( yesterday ( see ( john , X ) ) , X=mary ) . It is possible to give Prolog definitions for most of the focalizers discussed above which are suitable for extensional evaluation and which amount to model-theoretic definitions of them. This will be discussed in a later report on LFL. A point of the grammar HODL is to be able to produce LFL analyses of sentences using the modular semantic interpretation system outlined in the preceding section , and to arrive at the right ( or most likely ) scopes for focalizers and other modifiers. The decision on scoping can depend on heuristics involving precedences , on very reliable cues from the syntactic position , and even on the specification of loci by explicit underlining in ~he input string ( which is most relevant for adverbial focalizers ) . Although written text does not often use such explici~ specification of adverbial loci , it is important that the system can get the right logical form after having some specification of the adverbial focus , because this specification might be obtained from prosody in spoken language , or might come from the use of discourse information. \ [ t also is an indication of the modularity of the system that it can use the same syntactic rules and parse path no matter where the adverbial focus happens to lie. Most of the specific linguistic information for semantic interpretation is encoded in the procedures 'mod ' , 'reorder ' , and 'raise ' , which manipulate semantic items. In MODL there are 22 clauses for the procedure 'mod ' , most of which are unit clauses. These involve ten different modification operators , four of which were illustrated in the preceding section. The definition of 'mo &lt; l ' in MODL is taken fairly directly from the corresponding procedure 'trans ' in HOD ( McCord , 1981 ) , although there are some changes involved in handling the new version of the logical form language ( LFL ) , 113 especially the indexing operator. The definitions of 'reorder ' and 'raise ' are essentially the same as for procedures in HOD. An illustration of analysis in the two-pass mode in HODL is now given. For the sentence `` Leopards only attack monkeys in trees '' , the syntactic analysis tree is as follows. sent nounph l-leopard ( X ) avp ( P &lt; Q ) -only ( P , Q ) l-attack ( X , Y ) nounph l-monkey ( Y ) prepph @ @ R-in ( Z , R ) nounph l-tree ( Z ) Here we display complete logical terminals in the leaf nodes of the tree. An indicat\ [ on of the meanings of the operators ( P &lt; Q ) and @ @ R will be given below. \ [ n the semantic interpretation of the prepositional phrase , the 'tree ' item gets promoted ( by 'raise ' ) to be a left-sister of the the 'in ' item , and the list of daughter items ( augmented semantic items ) of the 'sent ' node is the following. nounph i leopard ( X ) avp P &lt; Q only ( P , Q ) terminal I attack ( X , Y ) nounph 1 monkey ( Y ) nounph I tree ( Z ) prepph @ @ R in ( Z , R ) . Here we di~ : play each augmented semantic item sem ( nt : Feas , Op , LF ) simply in the form nt Op LF. The material in the first field of the 'monkey ' item actually shows that it is stressed. The reshaping p~ocedure 'reorder ' rearran6es these items into the order : nounph I leopard ( X ) nounph 1 tree ( Z ) prepph @ @ R in ( Z , R ) terminal I attack ( X , Y ) avp P &lt; Q only ( P , Q ) nounph 1 monkey ( Y ) Next , these items successively modify ( according to the rules for 'mod ' ) the matrix item , sent id t , with the rightmost daughter acting as innermost oodifier. The rules for 'mod ' involving the operator ( P &lt; Q ) associated with only ( P , Q ) are designed so that the logical form material to the right of 'only ' goes into the focus Q of 'only ' and the material to the left goes into the base P. The material to the right is just monkey ( Y ) . The items on the left ( 'leopard ' , 'tree ' , 'in ' , 'attack ' ) are allowed to combine ( through 'mod ' ) in an independent way before being put into the base of 'only'. The operator ~ @ R associated with in ( Z , R ) causes R to be botmd to the logical form of the modificand -attack ( X , Y ) . The combination of items on the left of 'only ' is leopard ( X ) &amp; tree ( Z ) &amp; in ( Z , attack ( X , Y ) ) This goes into the base , so the whole logical form is only ( leopard ( X ) &amp; tree ( Z ) &amp; in ( Z , attack ( X , Y ) ) , monkey ( Y ) ) . For detailed traces of logical form construction by this method , see McCord ( 1981 ) . An illustration of the treatment of leftembedding in HODL in a two-pass analysis of the sentence `` John sees each boy 's brother 's teacher '' is as follows. sent nounph \ [ - ( X=john ) l-see ( X , W ) nounph nounph nounph determiner Q/P-each ( P , Q ) l-boy ( Y ) l-poss l-brother ( Z , Y ) 1-poss 1-teacher ( W , Z ) Logical form ... each ( boy ( Y ) , the ( brother ( Z , Y ) , the ( teacher ( W , Z ) , see ( john , W ) ) ) ) . The MODL noun phrase rules include the shift ( in a way that is an elaboration of the shift grammar fragment in Section 2 ) , as well as rules for slotfilling for nouns like 'brother ' and 'teacher ' which have more than one argument in logical form. Exactly the same logical form is obtained by MODL for the sentence `` John sees the teacher of the brother of each boy '' . Both of these analyses involve raising. \ [ n =he first , the 'poss ' node resulting from the apostrophe-s is raised to become a definite article. In the second , the prepositional phrases ( their semantic structures ) are promoted to be sisters of the `` teacher '' node , and the order of the quantlfiers ts ( correctly ) reversed. The syntactic component of MODL was adapted as closely as possible from that of HOD ( a DCG ) in order to get an idea of the efficiency of HLG's. The fact that the MLG rule compiler produces more structure-building arguments than are in the DCG would tend to |engthen analysis times , but it is hard to predic~ the effect of the different organization of the semantic interpreter ( from a threepass system to a one-pass and a two-pass version of MODL ) . 7 '' no followin E five sentences were used for timing tests. Who did John say that the man introduced Mary to ? Each book Mary said was given to Bill 114 was written by a woman. Leopards only attack monkeys in trees. John saw each boy 's brother 's teacher. Does anyone wanting to see the teacher know whether there are any hooks left in this room ? Using Waterloo Prolog ( an interpreter ) on an IBM 3081 , the following average times to get the logical forms for the five sentences were obtained ( not including ~ime for \ [ /0 and initial word separation ) : MODL , one-pass mode 40 milliseconds. MODL , two-pass mode 42 milliseconds. MOD 35 milliseconds. So there was a loss of speed , but not a significant one. MODL has also been implemented in PSC Prolog ( on a 3081 ) . Here the average one-pass analysis time for the five sentences was improved to 30 milliseconds per sentence. On the other hand , the MLG grammar ( in source form ) ls more compact and easier to understand. The syntactic components for MOD and MODL were compared numerically by a Prolog program that totals up the sizes of all the grammar rules , where the size of a compound term is defined to be I plus the sum of the sizes of its arguments , and the size of any other term is I. The total for MODL was l &amp; 33 , and for MOD was 1807 , for a ratio of 79 % . So far , nothing has been said in this report about semantic constraints in HODL. Currently , MODL exercises constraints by unification of semantic types. Prolog terms representing type requirements on slot-fillers must be unified with types of actual fillers. The types used in MODL are t % /pe trees. A type tr~ is either a variable { unspecified type ) or a term whose principal functor is an atomic type ( like 'human ' ) , and whose arguments are subordinate type trees. A type tree T1 is subordinate to a type tree T2 if either T1 is a variable or the principal functor of T1 is a subtype ( ako ) of the principal functor of T2. Type trees are a generalization of the type lists used by Dahl ( 1981 ) , which are lists of the form TI : T2 : T3 : ... , where T1 is a supertype of T2 , T2 is a supertype of TS , ... , and the tail of the list may be a variable. The point of the generalization is to allow cross-classification. Multiple daughters of a type node cross-classify it. The lexicon in MODL includes a preprocessor for lexical entries which allows the original lexical entries to specify type constraints in a compact , non-redundant way. There is a Pro|o K representation for type-hierarchies , and the \ [ exical preprocessor manufactures full type trees from a specification of their leaf nodes. \ [ n the one-pass mode for analysis with MLG 's , logical forms get built up during parsing , so logical forms are available for examination by semantic checking procedures of the sort outlined in McCord ( 198 &amp; ) . If such methods are arguably best , then there may be more argument for a one-pass system ( with interleaving of semantics ) . The general question of the number of passes in a natural language understander is an interesting one. The MLG formalism makes this easier to investigate , because the same syntactic component can he used with onepass or two-pass interpretation. In MODL , there is a small dictionary stored directly in Prolog , but MODL is also interfaced to a large dictionary/morphology system ( Byrd , 1983 , 1984 ) which produces syntactic and morphological information for words based on over 70,000 lemmata. There are plans to include enough semantic information in this dictionary to provide semantic constraints for a large MLG. Alexa HcCray is working on the syntactic component for an MLG with very wide coverage. I wish to thank her for useful conversations about the nature of the system. The Restriction Grammars ( RG 's ) of HLrschman and Puder ( 1982 ) are logic grammars that were designed with modularity \ [ n mind. Restriction Grammars derive from the Linguistic String Project { Sager , 1981 ) . An RG consists of conLexE-free phrase structure rules to which restrictions are appended. The rule compiler { written in ProIo K and compiling into Prolog ) , sees to it that derivation trees are constructed automatically during parsing. The restrictions appended to the rules are basically Prolog procedures which can walk around , during the parse , in the partially constructed parse tree , and can look at the words remaining in the input stream. Thus there is a modularity between the phrasestructure parts of the syntax rules and the restrictions. The paper contains an interesting discussion of Prolog representations of parse trees that make it easy to walk around in them. A disadvantage of RG 's is that the automatically constructed analysis tree is just a derivation tree. With MLG 's , the shift operator and the declaration of strong non-terminals produce analysis structures which are more appropriate semantically and are easier to read for large grammars. \ [ n addition , MLG analysis trees contain logical terminals as building blocks for a modular semantic interpretation system. The method of walking about in the partially constructed parse tree is powerful and is worth exploring further ; but the more common way of exercising constraints in logic grammars by parameter passing and unification seems to be adequate linguistically and notationally more compact , as well as more efficient for the compiled Prolog program. Another type of logic grammar developed with modularity in mind is the Definite Clause Translation Grammars ( DCTG 's ) of Abramson ( 1984 ) . These were inspired partially by RG 's ( Hirschman and Puder , 1982 ) , by MSG 's { Dahl and McCord , 1983 ) , and by Attribute Grammars ( Knuth , 1968 ) . A DCTG rule is like a DCG rule with an appended list of clauses which compute the semantics of the node resulting from use of the rule. The non-terminals on the right-hand side of the syntactic portion of the rule can be indexed by variables , and these index variables can be used in the semantic portion to link to the syntactic portion. For exa~le , the DCG rule 115 sent ( P ) -- &gt; np ( X , P1 , P ) : vp ( X , Pl ) .</sentence>
				<definiendum id="0">T2</definiendum>
				<definiens id="0">sufficient to ignore contexts , and in such cases we just think of P : E as verifying P and binding E to an instantiation 112 of P. In fact , for PROLOG execution of logical forms without contexts , ' : ' can be defined by the single clause : P : P &lt; F. A specific purpose of the MOD system in McCord ( 1981 ) was to point out the importance of a class of predicates called focaiizers , and to offer a method for dealing with them in semantic interpretation. Focalizers include many determiners , adverbs , and adjectives ( or their word-senses ) , as well as certain non-lexical predicates like 'yesno'. Focalizers take two logical form arguments called the base and the fOCUS : focalizer ( Base , Focus ) . The Focus is often associated with sentence stress , hence the name. The pair ( Base , Focus ) is called the SCOpe of the focalizer. The adverbs 'only ' and 'even ' are focalizers which most clearly exhibit the connection with stress. The predication only ( P , Q ) reads `` the only case where P holds is when Q also holds '' . We get different analyses depending on focus. John only buys books at Smith's. only ( at ( smith , buy ( john , X1 ) ) , book ( X1 ) ) . John only buys books at Smith's. only ( book ( Xl ) &amp; at ( X2 , buy ( john , Xl ) ) , X2=smith ) . quantificational adverbs like 'always ' and 'seldom ' , studied by David Lewis ( 1975 ) , are also focalizers. Lewis made the point that these quantifiers are properly considered unseJKtJve , in the sense that they quantify over all the free variables in ( what we call ) their bases. For example , in John always buys books at Smith's. always ( book ( Xl ) &amp; at ( X2 , buy ( john , Xl ) ) , X2=smith ) • the quantification is over both X1 and X2. ( A paraphrase is `` Always , if X1 is a book and John buys X1 at X2 , then X2 is Smith 's '' . ) Quantificational determiners are also focalizers ( and are unselective quantifiers ) ; they correspond closely in meaning to the quantificational adverbs ( 'all ' 'always ' , 'many ' 'often ' , 'few ' 'seldom ' , etc. ) . We have the paraphrases : Leopards often attack monkeys in trees. often ( leopard ( Xl ) &amp; tree ( X2 ) &amp; in ( X2 , attack ( Xl , X3 ) ) , monkey ( X3 ) ) . Many leopard attacks in trees are ( attacks ) on monkeys. many ( leopard ( Xl ) &amp; tree ( X2 ) &amp; in ( X2 , attack ( Xi , X3 ) ) , monkey ( X3 ) ) . Adverbs and adjectives involving comparison or degree along some scale of evaluation ( a wide class ) are also focalizers. The base specifies the base of comparison , and the focus singles out what is being compared to the base. This shows up most clearly in the superlative forms. Consider the adverb `` fastest '' : John ran fastest yesterday. fastest ( run ( john ) : E , yesterday ( E ) ) . John ran fastest yesterday. fastest ( yesterday ( run ( X ) ) , X=john ) . In the first sentence , with focus on `` yesterday '' , the meaning is that , among all the events of John 's running ( this is the base ) , John 's running yesterday was fastest. The logical form illustrates the indexing operator. \ [ n the second sentence , with focus on `` John '' , the meaning is that among all the events of running yesterday ( there is an implicit location for these events ) , John 's running was fastest. As an example of a non-lexical focalizer , we have yesno ( P , q ) , which presupposes that a case of P holds , and asks whether P &amp; Q holds. ( The pair ( P , Q ) is like Topic/Comment for yes-no questions. ) Example : Did John see M @ ry yesterday ? yesno ( yesterday ( see ( john , X ) ) , X=mary ) . It is possible to give Prolog definitions for most of the focalizers discussed above which are suitable for extensional evaluation and which amount to model-theoretic definitions of them. This will be discussed in a later report on LFL. A point of the grammar HODL is to be able to produce LFL analyses of sentences using the modular semantic interpretation system outlined in the preceding section , and to arrive at the right ( or most likely ) scopes for focalizers and other modifiers. The decision on scoping can depend on heuristics involving precedences , on very reliable cues from the syntactic position , and even on the specification of loci by explicit underlining in ~he input string ( which is most relevant for adverbial focalizers ) . Although written text does not often use such explici~ specification of adverbial loci , it is important that the system can get the right logical form after having some specification of the adverbial focus , because this specification might be obtained from prosody in spoken language , or might come from the use of discourse information. \ [ t also is an indication of the modularity of the system that it can use the same syntactic rules and parse path no matter where the adverbial focus happens to lie. Most of the specific linguistic information for semantic interpretation is encoded in the procedures 'mod ' , 'reorder ' , and 'raise ' , which manipulate semantic items. In MODL there are 22 clauses for the procedure 'mod ' , most of which are unit clauses. These involve ten different modification operators , four of which were illustrated in the preceding section. The definition of 'mo &lt; l ' in MODL is taken fairly directly from the corresponding procedure 'trans ' in HOD ( McCord , 1981 ) , although there are some changes involved in handling the new version of the logical form language ( LFL ) , 113 especially the indexing operator. The definitions of 'reorder ' and 'raise ' are essentially the same as for procedures in HOD. An illustration of analysis in the two-pass mode in HODL is now given. For the sentence `` Leopards only attack monkeys in trees '' , the syntactic analysis tree is as follows. sent nounph l-leopard ( X ) avp ( P &lt; Q ) -only ( P , Q ) l-attack ( X , Y ) nounph l-monkey ( Y ) prepph @ @ R-in ( Z , R ) nounph l-tree ( Z ) Here we display complete logical terminals in the leaf nodes of the tree. An indicat\ [ on of the meanings of the operators ( P &lt; Q ) and @ @ R will be given below. \ [ n the semantic interpretation of the prepositional phrase , the 'tree ' item gets promoted ( by 'raise ' ) to be a left-sister of the the 'in ' item , and the list of daughter items ( augmented semantic items ) of the 'sent ' node is the following. nounph i leopard ( X ) avp P &lt; Q only ( P , Q ) terminal I attack ( X , Y ) nounph 1 monkey ( Y ) nounph I tree ( Z ) prepph @ @ R in ( Z , R ) . Here we di~ : play each augmented semantic item sem ( nt : Feas , Op , LF ) simply in the form nt Op LF. The material in the first field of the 'monkey ' item actually shows that it is stressed. The reshaping p~ocedure 'reorder ' rearran6es these items into the order : nounph I leopard ( X ) nounph 1 tree ( Z ) prepph @ @ R in ( Z , R ) terminal I attack ( X , Y ) avp P &lt; Q only ( P , Q ) nounph 1 monkey ( Y ) Next , these items successively modify ( according to the rules for 'mod ' ) the matrix item , sent id t , with the rightmost daughter acting as innermost oodifier. The rules for 'mod ' involving the operator ( P &lt; Q ) associated with only ( P , Q ) are designed so that the logical form material to the right of 'only ' goes into the focus Q of 'only ' and the material to the left goes into the base P. The material to the right is just monkey ( Y ) . The items on the left ( 'leopard ' , 'tree ' , 'in ' , 'attack ' ) are allowed to combine ( through 'mod ' ) in an independent way before being put into the base of 'only'. The operator ~ @ R associated with in ( Z , R ) causes R to be botmd to the logical form of the modificand -attack ( X , Y ) . The combination of items on the left of 'only ' is leopard ( X ) &amp; tree ( Z ) &amp; in ( Z , attack ( X , Y ) ) This goes into the base , so the whole logical form is only ( leopard ( X ) &amp; tree ( Z ) &amp; in ( Z , attack ( X , Y ) ) , monkey ( Y ) ) . For detailed traces of logical form construction by this method , see McCord ( 1981 ) . An illustration of the treatment of leftembedding in HODL in a two-pass analysis of the sentence `` John sees each boy 's brother 's teacher '' is as follows. sent nounph \ [ - ( X=john ) l-see ( X , W ) nounph nounph nounph determiner Q/P-each ( P , Q ) l-boy ( Y ) l-poss l-brother ( Z , Y ) 1-poss 1-teacher ( W , Z ) Logical form ... each ( boy ( Y ) , the ( brother ( Z , Y ) , the ( teacher ( W , Z ) , see ( john , W ) ) ) ) . The MODL noun phrase rules include the shift ( in a way that is an elaboration of the shift grammar fragment in Section 2 ) , as well as rules for slotfilling for nouns like 'brother ' and 'teacher ' which have more than one argument in logical form. Exactly the same logical form is obtained by MODL for the sentence `` John sees the teacher of the brother of each boy '' . Both of these analyses involve raising. \ [ n =he first , the 'poss ' node resulting from the apostrophe-s is raised to become a definite article. In the second , the prepositional phrases ( their semantic structures ) are promoted to be sisters of the `` teacher '' node , and the order of the quantlfiers ts ( correctly ) reversed. The syntactic component of MODL was adapted as closely as possible from that of HOD ( a DCG ) in order to get an idea of the efficiency of HLG's. The fact that the MLG rule compiler produces more structure-building arguments than are in the DCG would tend to |engthen analysis times , but it is hard to predic~ the effect of the different organization of the semantic interpreter ( from a threepass system to a one-pass and a two-pass version of MODL ) . 7 '' no followin E five sentences were used for timing tests. Who did John say that the man introduced Mary to ? Each book Mary said was given to Bill 114 was written by a woman. Leopards only attack monkeys in trees. John saw each boy 's brother 's teacher. Does anyone wanting to see the teacher know whether there are any hooks left in this room ? Using Waterloo Prolog ( an interpreter ) on an IBM 3081 , the following average times to get the logical forms for the five sentences were obtained ( not including ~ime for \ [ /0 and initial word separation ) : MODL , one-pass mode 40 milliseconds. MODL , two-pass mode 42 milliseconds. MOD 35 milliseconds. So there was a loss of speed , but not a significant one. MODL has also been implemented in PSC Prolog ( on a 3081 ) . Here the average one-pass analysis time for the five sentences was improved to 30 milliseconds per sentence. On the other hand , the MLG grammar ( in source form ) ls more compact and easier to understand. The syntactic components for MOD and MODL were compared numerically by a Prolog program that totals up the sizes of all the grammar rules , where the size of a compound term is defined to be I plus the sum of the sizes of its arguments , and the size of any other term is I. The total for MODL was l &amp; 33 , and for MOD was 1807 , for a ratio of 79 % . So far , nothing has been said in this report about semantic constraints in HODL. Currently , MODL exercises constraints by unification of semantic types. Prolog terms representing type requirements on slot-fillers must be unified with types of actual fillers. The types used in MODL are t % /pe trees. A type tr~ is either a variable { unspecified type ) or a term whose principal functor is an atomic type ( like 'human ' ) , and whose arguments are subordinate type trees. A type tree T1 is subordinate to a type tree T2 if either T1 is a variable or the principal functor of T1 is a subtype ( ako ) of the principal functor of T2. Type trees are a generalization of the type lists used by Dahl ( 1981 ) , which are lists of the form TI : T2 : T3 : ... , where T1 is a supertype of T2</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>ABSTRACT How can grammar be viewed as a functional part of a cognitive system ) Given a neural basis for the processing control paradigm of language performance , what roles does 'Sgrammar '' play ?</sentence>
				<definiendum id="0">ABSTRACT</definiendum>
				<definiens id="0">a functional part of a cognitive system ) Given a neural basis for the processing control paradigm of language performance</definiens>
			</definition>
			<definition id="1">
				<sentence>In this type of model , a grammar is an explicit encoded representation that coordinates the integrated parallel process .</sentence>
				<definiendum id="0">grammar</definiendum>
				<definiens id="0">an explicit encoded representation that coordinates the integrated parallel process</definiens>
			</definition>
			<definition id="2">
				<sentence>INTERPRETATION results in activation of a pragmatic representation of a disambiguated word meaning .</sentence>
				<definiendum id="0">INTERPRETATION</definiendum>
				<definiens id="0">results in activation of a pragmatic representation of a disambiguated word meaning</definiens>
			</definition>
			<definition id="3">
				<sentence>The interpretation function represents a firing activation level for the `` concept '' of the meaning and includes its syntactic form .</sentence>
				<definiendum id="0">interpretation function</definiendum>
				<definiens id="0">a firing activation level for the `` concept '' of the meaning and includes its syntactic form</definiens>
			</definition>
			<definition id="4">
				<sentence>The current represented grammar in HOPE contains the following lexical categories : OET for determiner , ENOCONT for end of sentence intonation , NOUN for common noun , PAUSE for end of clause intonation , TERM for proper nouns , VIP for intrasitive verb , VTP for transitive verb .</sentence>
				<definiendum id="0">VIP</definiendum>
				<definiendum id="1">VTP</definiendum>
				<definiens id="0">intrasitive verb</definiens>
			</definition>
			<definition id="5">
				<sentence>Rules , as previously discussed , are activated during processing via spreading activation .</sentence>
				<definiendum id="0">Rules</definiendum>
				<definiens id="0">activated during processing via spreading activation</definiens>
			</definition>
			<definition id="6">
				<sentence>OET is the only active category that predicts NOUN so all active meanings of type OET will receive the feedback activity .</sentence>
				<definiendum id="0">OET</definiendum>
				<definiens id="0">the only active category that predicts NOUN so all active meanings of type OET will receive the feedback activity</definiens>
			</definition>
			<definition id="7">
				<sentence>Finally , the Categorial Grammar is one form of a Context-Free ( CF ) grammar which provides a suitable integration of syntactic and semantic processing .</sentence>
				<definiendum id="0">Categorial Grammar</definiendum>
				<definiens id="0">one form of a Context-Free ( CF ) grammar which provides a suitable integration of syntactic and semantic processing</definiens>
			</definition>
</paper>

	</volume>
