<?xml version="1.0" encoding="UTF-8"?>
	<volume id="P98">

		<paper id="1037">
			<definition id="0">
				<sentence>Word sense disambiguation for unrestricted text is one of the most difficult tasks in the fields of computational linguistics .</sentence>
				<definiendum id="0">Word sense disambiguation</definiendum>
				<definiens id="0">one of the most difficult tasks in the fields of computational linguistics</definiens>
			</definition>
			<definition id="1">
				<sentence>An adaptation step follows to combine the initial knowledge base with knowledge gleaned from the partial disambiguated text .</sentence>
				<definiendum id="0">adaptation step</definiendum>
			</definition>
			<definition id="2">
				<sentence>Word sense disambiguation for unrestricted text is one of the most difficult tasks in the fields of computational linguistics .</sentence>
				<definiendum id="0">Word sense disambiguation</definiendum>
				<definiens id="0">one of the most difficult tasks in the fields of computational linguistics</definiens>
			</definition>
			<definition id="3">
				<sentence>The current implementation of TopSense uses the topical information in Longman Lexicon of Contemporary English ( McArthur 1992 , LLOCE ) to represent WSD knowledge for LDOCE senses .</sentence>
				<definiendum id="0">TopSense</definiendum>
			</definition>
			<definition id="4">
				<sentence>MRDs Dictionary is a text whose subject matter is a language .</sentence>
				<definiendum id="0">MRDs Dictionary</definiendum>
				<definiens id="0">a text whose subject matter is a language</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>Introduction World Wide Web ( WWW ) is the most useful and powerful information dissemination system on the Internet .</sentence>
				<definiendum id="0">WWW )</definiendum>
			</definition>
			<definition id="1">
				<sentence>Coverage is one of the major problems in dictionary-based approaches ( Ballesteros and Croft , 1996 ; Davis , 1997 ; Hull and Grefenstette , 1996 ) .</sentence>
				<definiendum id="0">Coverage</definiendum>
				<definiens id="0">one of the major problems in dictionary-based approaches</definiens>
			</definition>
			<definition id="2">
				<sentence>In famous message understanding system evaluation and message understanding conferences ( MUC ) and the related multilingual entity tasks ( MET ) , named entity , which covers named organizations , people , and locations , along with date/time expressions and monetary and percentage expressions , is one of tasks for evaluating technologies .</sentence>
				<definiendum id="0">message understanding conferences</definiendum>
				<definiendum id="1">MET</definiendum>
				<definiendum id="2">entity</definiendum>
				<definiens id="0">covers named organizations , people , and locations</definiens>
			</definition>
			<definition id="3">
				<sentence>Cache records the occurrences of all the possible candidates in a paragraph .</sentence>
				<definiendum id="0">Cache</definiendum>
				<definiens id="0">records the occurrences of all the possible candidates in a paragraph</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>ctxt is the repository of contextually unified information , where conflicts result in ungrammaticality , gen holds generalizable information .</sentence>
				<definiendum id="0">ctxt</definiendum>
				<definiens id="0">the repository of contextually unified information , where conflicts result in ungrammaticality , gen holds generalizable information</definiens>
			</definition>
			<definition id="1">
				<sentence>A canonical entry for such unknown words is defined as the disjunction of maximally underspecified generic lexical entries for nouns , adjectives and verbs .</sentence>
				<definiendum id="0">canonical entry</definiendum>
			</definition>
</paper>

		<paper id="2219">
			<definition id="0">
				<sentence>Decision theoretic planning can be applied to the problem of choosing among strategies , by associating a utility U with each strategy ( action ) choice and by positing that agents should adhere to the Maximum Expected Utility Principle ( Keeney and Raiffa , 1976 ; Russell and Norvig , 1995 ) , Maximum Expected Utility Principle : An optimal action is one that maximizes the expected utility of outcome states .</sentence>
				<definiendum id="0">Decision theoretic planning</definiendum>
				<definiens id="0">An optimal action is one that maximizes the expected utility of outcome states</definiens>
			</definition>
			<definition id="1">
				<sentence>The platform consists of a speech recognizer that supports barge-in so that the user can interrupt the agent when it is speaking .</sentence>
				<definiendum id="0">platform</definiendum>
				<definiens id="0">consists of a speech recognizer that supports barge-in so that the user can interrupt the agent when it is speaking</definiens>
			</definition>
			<definition id="2">
				<sentence>Users reported their perceptions as to whether they had completed the task ( Comp ) , 2 and filled in an AVM with the information that they had acquired from the agent , e.g. the values for Email .</sentence>
				<definiendum id="0">Comp</definiendum>
				<definiens id="0">the information that they had acquired from the agent</definiens>
			</definition>
			<definition id="3">
				<sentence>Measure SYSTEM ( SI ) MIXED ( MI ) Kappa Comp User Turns System Turns Elapsed Time ( ET ) MeanRecog ( MRS ) Time Outs Help Requests Barge Ins Recognizer Rejects .81 .83 25.94 28.18 328.59 s .88 .70 .98 .83 .78 17.59 21.74 289.43 s .72 .94 User Satisfaction 26.6 23.7 Table 2 : Performance measure means per dialogue for Initiative Strategies PARADISE provides a way to calculate dialogue agent performance as a linear combination of a number of simpler metrics that can be directly measured such as those in Table 2 .</sentence>
				<definiendum id="0">Measure SYSTEM</definiendum>
				<definiens id="0">Performance measure means per dialogue for Initiative Strategies PARADISE provides a way to calculate dialogue agent performance as a linear combination of a number of simpler metrics</definiens>
			</definition>
			<definition id="4">
				<sentence>Performance for any ( sub ) dialogue D is defined by the following equation : n Performance= ( a , A/ ' ( , ~ ) ) ~_ , wi* A/ ' ( ci ) i=1 1348 where o~ is a weight on n , ci are the cost functions , which are weighted by wl , and .</sentence>
				<definiendum id="0">Performance for any ( sub</definiendum>
				<definiendum id="1">n Performance=</definiendum>
				<definiendum id="2">o~</definiendum>
				<definiens id="0">a weight on n , ci are the cost functions , which are weighted by wl , and</definiens>
			</definition>
			<definition id="5">
				<sentence>The magnitude of the coefficients in this equation demonstrates the performance of the speech recognizer ( MRS ) is the most important predictor , followed by users ' perception of Task Success ( Comp ) and efficiency ( ET ) .</sentence>
				<definiendum id="0">MRS )</definiendum>
				<definiens id="0">the most important predictor , followed by users ' perception of Task Success ( Comp ) and efficiency ( ET )</definiens>
			</definition>
			<definition id="6">
				<sentence>the following recursive equation : M ~ u ( a , s ) = R ( Sd+ F , J where R ( Si ) is a reward associated with being in state Si , a is a strategy from a finite set of strategies A that are admissable in state Si , and M~j is the probability of reaching state Sj if strategy a is selected in state Si .</sentence>
				<definiendum id="0">M~j</definiendum>
				<definiens id="0">s ) = R ( Sd+ F , J where R ( Si ) is a reward associated with being in state Si</definiens>
			</definition>
			<definition id="7">
				<sentence>The utility values can be estimated to within a desired threshold using Value Iteration , which updates the estimate of U ( a , Si ) , based on updated utility estimates for neighboring states , so that the equation above becomes : Un+l ( a , Sd = R ( Sd + ~ M~ m÷xUn ( a ' , Sj ) 3 where Un ( a , Si ) is the utility estimate for doing a in state Si after n iterations .</sentence>
				<definiendum id="0">Un+l</definiendum>
				<definiendum id="1">Si )</definiendum>
				<definiens id="0">updates the estimate of U ( a , Si ) , based on updated utility estimates for neighboring states</definiens>
			</definition>
</paper>

		<paper id="2125">
			<definition id="0">
				<sentence>The dependency parser ( Lee , 1995 ) only gives the syntactic relation mod between them , which should be regarded as subject in the relative clause .</sentence>
				<definiendum id="0">dependency parser</definiendum>
				<definiens id="0">the syntactic relation mod between them , which should be regarded as subject in the relative clause</definiens>
			</definition>
			<definition id="1">
				<sentence>Through this processing , the syntactic relational pattern ( SRP ) changes into the conceptual frequency pattern ( CFP ) , ( { &lt; C1 , fl &gt; , &lt; C2 , f2 &gt; , ... , &lt; Crn , fm &gt; } , SRj , Vk ) , where Ci represents a concept code at level four of the Kadokawa thesaurus , fi indicates the frequency of the code Ci , and SRj shows a syntactic relation between these concept codes and verb Vk .</sentence>
				<definiendum id="0">SRP</definiendum>
				<definiendum id="1">Ci</definiendum>
				<definiendum id="2">SRj</definiendum>
				<definiens id="0">a concept code at level four of the Kadokawa thesaurus , fi indicates the frequency of the code Ci , and</definiens>
			</definition>
			<definition id="2">
				<sentence>This information is represented by relative score RSk ( SRi ) of syntactic role SRi for antecedents of verb Vk as is shown bellow and is used in syntactic role determination as described in section 4 : RSk ( SRi ) freqk ( SRi ) ( 3 ) freq ( Vk ) where freq ( Vk ) are the frequency of verb Vk of relative clauses , and freqk ( SRi ) is the frequency of syntactic role SRi of antecedents in relative clauses including verb Vk in the corpus .</sentence>
				<definiendum id="0">freq</definiendum>
				<definiendum id="1">SRi )</definiendum>
				<definiens id="0">the frequency of syntactic role SRi of antecedents in relative clauses including verb Vk in the corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>We assume that an antecedent has meanings C1 , C2 , C3 , ... , Cn , and that CPi is a conceptual pattern ( { P1 , P2 , ... , Pro } , SRi , Vk ) corresponding to syntactic relation SP~ of verb Vk .</sentence>
				<definiendum id="0">CPi</definiendum>
				<definiens id="0">a conceptual pattern ( { P1 , P2 , ... , Pro } , SRi , Vk ) corresponding to syntactic relation SP~ of verb Vk</definiens>
			</definition>
			<definition id="4">
				<sentence>SIMI ( Np , Vk ) = rnax ( Csirn ( Cw , Pj ) ) 1 &lt; w &lt; n , 1 ~ j ~_ m ( 4 ) Csim ( Cw , Pj ) 2 * level ( MSCA ( Cw , Pj ) ) = • ispenalty ( 5 ) level ( Cw ) + level ( Pj ) where MSCA ( Cw , Pj ) in Csim ( Cw , Pj ) represents the most specific common ancestor ( MSCA ) of concepts Cw and Pj in the Kadokawa concept hierarchy .</sentence>
				<definiendum id="0">SIMI</definiendum>
				<definiendum id="1">MSCA</definiendum>
				<definiendum id="2">Pj )</definiendum>
				<definiens id="0">the most specific common ancestor ( MSCA ) of concepts Cw and Pj in the Kadokawa concept hierarchy</definiens>
			</definition>
</paper>

		<paper id="2154">
			<definition id="0">
				<sentence>Rule psrl denotes the relationship between three feature structures s , np , and v-p .</sentence>
				<definiendum id="0">Rule psrl</definiendum>
				<definiens id="0">the relationship between three feature structures s , np</definiens>
			</definition>
			<definition id="1">
				<sentence>POS v list `` \ [ sign AGR *1 agr PER \ [ 2ndJ ( 10 ) l agr \ [ NUM plural\ ] suB , sign \ [ AO , *1 \ ] POS n \ ] sign AGR agr\ [ NUMPER 3rdSing\ ] ( 11 ) pCsign ( v , Agr , sign ( _ , Agr , _ ) ) ) ~ -- { not_3s ( Agr ) } p ( sign ( n , ag ( ing , 3 d ) , _ ) ) not_3s ( agr ( sing , Per ) ) * -- { l st_or.2nd ( Per ) } not_3s ( ag ( pt al , _ ) ) l st_or_2nd ( l st ) ~-l st_or_2nd ( 2nd ) , -Literal p ( X ) means that variable X is a candidate for the disjunctive feature structure ( DFS ) specified by predicate p. The constraint element lst_or_2nd ( Per ) in ( 11 ) constrains variable Per to be either 1st or 2nd .</sentence>
				<definiendum id="0">ag</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a candidate for the disjunctive feature structure ( DFS ) specified by predicate p. The constraint element lst_or_2nd ( Per ) in ( 11 ) constrains variable Per to be either 1st or 2nd</definiens>
			</definition>
			<definition id="2">
				<sentence>In a similar way , not_3s ( Agr ) means that Agr is a term having the form agr ( Num , Per ) , and that either Num is sing and Per is subject to 1 st_or_2nd ( Per ) or that Num is plural .</sentence>
				<definiendum id="0">Agr</definiendum>
			</definition>
			<definition id="3">
				<sentence>LCGR consists of a set of phrase structure rules , a set of lexical items , and a database .</sentence>
				<definiendum id="0">LCGR</definiendum>
				<definiens id="0">consists of a set of phrase structure rules , a set of lexical items , and a database</definiens>
			</definition>
			<definition id="4">
				<sentence>Each phrase structure role is a triplate ( V -- , ~ , C / , where V is a variable , ~ is a list of variables , and C is a constraint on V and variables in ~ .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">a variable , ~ is a list of variables , and</definiens>
				<definiens id="1">a constraint on V and variables in ~</definiens>
			</definition>
			<definition id="5">
				<sentence>For example , ( X -- ~ Y Z , { psrl ( X , Y , Z ) } ) means if there is a set of instances x , y , and z of X , Y , and Z that satisfies { psrl ( X , Y , Z ) } , the sequence of a phrase having feature structure y and that having feature structure z can be recognized as a phrase having feature structure x. Each lexical item is a pair ( w , p ) , where w is a word and p is a predicate .</sentence>
				<definiendum id="0">w</definiendum>
				<definiendum id="1">p</definiendum>
				<definiens id="0">a set of instances x , y , and z of X</definiens>
				<definiens id="1">the sequence of a phrase having feature structure y and that having feature structure z can be recognized as a phrase having feature structure x. Each lexical item is a pair ( w , p</definiens>
				<definiens id="2">a predicate</definiens>
			</definition>
			<definition id="6">
				<sentence>This means an instance of X that satisfies { p ( X ) } can be a lexical feature structure for word w. For example , ( walk , lex_walk I means instances of X that satisfy { lex_walk ( X ) } are lexical feature structures for walk .</sentence>
				<definiendum id="0">lex_walk I</definiendum>
				<definiens id="0">means instances of X that satisfy { lex_walk ( X ) } are lexical feature structures for walk</definiens>
			</definition>
			<definition id="7">
				<sentence>Then eq ( X l , verb ) is added to the body , where eq is a predicate that represents the identity relation and that has the following definition clause .</sentence>
				<definiendum id="0">eq</definiendum>
				<definiens id="0">a predicate that represents the identity relation and that has the following definition clause</definiens>
			</definition>
			<definition id="8">
				<sentence>BUP : A bottom-up parser embedded in Prolog .</sentence>
				<definiendum id="0">BUP</definiendum>
				<definiens id="0">A bottom-up parser embedded in Prolog</definiens>
			</definition>
			<definition id="9">
				<sentence>Complex indeterminates in Prolog and its application to discourse models .</sentence>
				<definiendum id="0">Complex</definiendum>
			</definition>
</paper>

		<paper id="2240">
			<definition id="0">
				<sentence>Given a known finite alphabet of symbols I , a target finite-state language L , and a data sample D + _C L _C I ' , the task is to find an FSA A , such that L ( A ) is consistent with D + , L ( A ) is a superset of D + encoding generalisation over the structural regularities of D + , and the size of S is as small as possible .</sentence>
				<definiendum id="0">L ( A )</definiendum>
				<definiens id="0">a superset of D + encoding generalisation over the structural regularities of D +</definiens>
			</definition>
</paper>

		<paper id="1092">
			<definition id="0">
				<sentence>Its corresponding syntactic context is : NP1_of_NP2_prepLNP3 where NP is a recognised noun phrase , prep~ refers to the class of preposition not containing of and often found in the morphological composition of terms ( for , by , in , from , with ) .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiendum id="1">prep~</definiendum>
				<definiens id="0">a recognised noun phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>This transformation is based on the following noun phrase formation rule for English : DAM1 h p m Mz -- -~ D A m M2 Ml h where D , A and M are respectively strings of determiner , adjective and words whose place can be empty , h is a head noun , m is a word and p is a preposition .</sentence>
				<definiendum id="0">h</definiendum>
				<definiendum id="1">p</definiendum>
				<definiens id="0">DAM1 h p m Mz -- -~ D A m M2 Ml h where D , A and M are respectively strings of determiner , adjective and words whose place can be empty ,</definiens>
				<definiens id="1">a word and</definiens>
			</definition>
			<definition id="2">
				<sentence>It marks the transformation of a term , from a syntagmatic structure to a compound one : tI=ANMI hpmM2 t2=AmM2NMI h where tl is really found in the corpus , N is a string of words that is either empty or a noun .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a string of words that is either empty or a noun</definiens>
			</definition>
			<definition id="3">
				<sentence>• Modifier substitution ( M-Sub ) : t2 is a substitution of t~ if and only if : t~ = M 1 m M 2 h and t2 = M~ m ' M 2 h with m ' ~ m • Head substitution ( H-Sub ) : t2 is a substitution of tl if and only if : tz= Mmh andt2= Mmh ' with h ' ~ h Tzoukermann et al. ( 1997 ) considered chemical treatment against disease and disease treatment as substitution variants whereas , in our study , after transformation , they would be a case of leftexpansion ( L-Exp ) .</sentence>
				<definiendum id="0">Modifier substitution</definiendum>
				<definiendum id="1">M-Sub )</definiendum>
				<definiendum id="2">t2</definiendum>
			</definition>
			<definition id="4">
				<sentence>Expansion is the generic name designating three elementary operations of word adjunction in an existing term .</sentence>
				<definiendum id="0">Expansion</definiendum>
				<definiens id="0">the generic name designating three elementary operations of word adjunction in an existing term</definiens>
			</definition>
			<definition id="5">
				<sentence>• Left expansion ( L-Exp ) : tz is a left-expansion of t~ if and only if : tl = Mh and t2 = M ' m ' M h • Right expansion ( R-Exp ) : t2 is a right-expansion of t~ if and only if : tl =M h and t2 = M h M ' h ' • Insertion ( Ins ) : t2 is an insertion of t~ if and only if : tl =Ml mMzh t2 =M1 mm'M'MEh 566 Examples of each sub-type of expansion are given in Table 3 .</sentence>
				<definiendum id="0">L-Exp )</definiendum>
				<definiendum id="1">tz</definiendum>
				<definiendum id="2">R-Exp )</definiendum>
				<definiendum id="3">t2</definiendum>
				<definiens id="0">a right-expansion of t~ if and only if : tl =M h and t2 = M h M ' h ' • Insertion ( Ins ) : t2 is an insertion of t~ if and only if</definiens>
			</definition>
			<definition id="6">
				<sentence>The value of Sub relations was then given by the ratio Exp/Sub where Exp ( respectively Sub ) is the total number of expansions relations ( respectively substitutions ) between terms in the corpus .</sentence>
				<definiendum id="0">Exp</definiendum>
				<definiens id="0">the total number of expansions relations ( respectively substitutions ) between terms in the corpus</definiens>
			</definition>
</paper>

		<paper id="2150">
			<definition id="0">
				<sentence>Indefinite noun phrase An indefinite noun phrase denotes an arbitrary member of the class of the noun phrase .</sentence>
				<definiendum id="0">indefinite noun phrase</definiendum>
				<definiens id="0">an arbitrary member of the class of the noun phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>generic NP { NP non generic NP definite NP indefinite NP Generic noun phrase A noun phrase is classified as generic when it denotes all members of the class described by the noun phrase or the class itself of the noun phrase .</sentence>
				<definiendum id="0">noun phrase</definiendum>
				<definiens id="0">all members of the class described by the noun phrase or the class itself of the noun phrase</definiens>
			</definition>
			<definition id="2">
				<sentence>The possessor of the second `` HOO ( cheek ) '' is also determined to be `` OJIISAN ( old man ) '' because `` OJIISAN ( old man ) '' is the subject of the sentence .</sentence>
				<definiendum id="0">OJIISAN</definiendum>
				<definiens id="0">old man ) '' is the subject of the sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>R1 When a noun phrase is modified by the words `` SOREZORE-NO ( each ) '' and `` ONOONO-NO ( each ) '' , { ( Indefinite , 25 ) } R2 When a noun phrase is estimated to be a definite noun phrase , and satisfies the modifier and possessor constraints , and the same noun phrase X has already appeared , { ( The noun phrase X , 30 ) } R3 When a noun phrase is estimated to be a generic noun phrase , { ( Generic , 10 ) } R4 When a noun phrase is estimated to be an indefinite noun phrase , { ( Indefinite , 10 ) } R5 When a noun phrase X is not estimated to be a definite noun phrase , { ( A noun phrase X which satisfies the modifier and possessor constraints , P + W D + 4 ) } The values P , W , D are as defined in Section Before determining the referents of noun phrases , sentences were at first transformed into a case structure by the case structure analyzer ( Kurohashi and Nagao 1994 ) .</sentence>
				<definiendum id="0">SOREZORE-NO</definiendum>
				<definiendum id="1">ONOONO-NO</definiendum>
				<definiens id="0">satisfies the modifier and possessor constraints</definiens>
			</definition>
			<definition id="4">
				<sentence>Recall is the fraction of noun phrases which have antecedents .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">the fraction of noun phrases</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>3 A trigger pair is an ordered pair of words t and w. Knowledge that the trigger word t has occurred within some window of words in the history , changes 2A more intuitive view of entropy is provided through perplexity ( Jelinek et al. , 1977 ) which is a measure of the number of choices , on average , there are for a random variable .</sentence>
				<definiendum id="0">trigger pair</definiendum>
				<definiens id="0">an ordered pair of words t and w. Knowledge that the trigger word t has occurred within some window of words in the history</definiens>
			</definition>
			<definition id="1">
				<sentence>, at ( sent.-final , +/ -- '' : '' ) from ( sent.-final , +/- '' : '' ) 25 % , 12 '' , 9.4m3 Trigger Words Like These : Utah , Maine , Alaska Party , Council , Department yet ( conjunction ) ( not ) only , ( not ) just $ 452,983,000 , $ 10,000 , $ 19.95 913-3434 ( follows area code ) 22314-1698 ( postal zipcode ) profit , price , cost Table 4 : Selected Tag Trigger-Pairs , ATR/Lancaster General-English Treebank # la lb 2a 2b 3a 3b 4a 4b 5a 5b A Construction Like This : Interrupter Phrase - &gt; * Or Example : * , VP - &gt; Verb+Interrupter Phrase+Obj/Compl Example : starring -- surprise , surprise -- men Noun Phrase - &gt; Simple Noun Phrase+Num Example : Lows around 50 Verb Phrase - &gt; Adverb Phrase+Verb Phrase Example : just need to understand it Question - &gt; Be+NP+Object/Complement Example : Is it possible ?</sentence>
				<definiendum id="0">price</definiendum>
				<definiens id="0">starring -- surprise , surprise -- men Noun Phrase - &gt; Simple Noun Phrase+Num Example : Lows around 50 Verb Phrase - &gt;</definiens>
			</definition>
			<definition id="2">
				<sentence>A trigger pair of constructions specific to Class 1 of the Source partitioning , which contains only Associated Press newswire and Wall Street Journal articles , is the following : A sentence containing both a quoted remark and an attribution of that remark to a particular source , triggers a sentence containing simply a quoted remark , without attribution .</sentence>
				<definiendum id="0">trigger pair</definiendum>
				<definiens id="0">contains only Associated Press newswire and Wall Street Journal articles , is the following : A sentence containing both a quoted remark and an attribution of that remark to a particular source , triggers a sentence containing simply a quoted remark , without attribution</definiens>
			</definition>
</paper>

		<paper id="2249">
			<definition id="0">
				<sentence>Variable inference processes are a reflection of different thresholds , and the skepticism of an individual inference process determines how thresholds are reached .</sentence>
				<definiendum id="0">Variable inference processes</definiendum>
				<definiens id="0">a reflection of different thresholds , and the skepticism of an individual inference process determines how thresholds are reached</definiens>
			</definition>
			<definition id="1">
				<sentence>A schema is any function which maps inputs onto mental representations .</sentence>
				<definiendum id="0">schema</definiendum>
				<definiens id="0">any function which maps inputs onto mental representations</definiens>
			</definition>
			<definition id="2">
				<sentence>Coverage captures the principle of conflict resolution in production systems .</sentence>
				<definiendum id="0">Coverage captures</definiendum>
				<definiens id="0">the principle of conflict resolution in production systems</definiens>
			</definition>
			<definition id="3">
				<sentence>By contrast , Completion represents the percentage of the schema that is matched by the input ( i.e. the completeness of the match ) .</sentence>
				<definiendum id="0">Completion</definiendum>
				<definiens id="0">the percentage of the schema that is matched by the input ( i.e. the completeness of the match )</definiens>
			</definition>
			<definition id="4">
				<sentence>The tree-based learning examination methods are recursive procedures that process each tree node , performing an in-order ( or depth-first ) tree traversal .</sentence>
				<definiendum id="0">tree-based learning examination methods</definiendum>
				<definiens id="0">recursive procedures that process each tree node , performing an in-order ( or depth-first ) tree traversal</definiens>
			</definition>
			<definition id="5">
				<sentence>l ) `` z , V-~=I_M ( NC ( x ) TC ( x ) ) where M is the vector size , that is , the number of possible successors ( e.g. 27 ) ( see Fig .</sentence>
				<definiendum id="0">NC ( x ) TC</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">the vector size , that is , the number of possible successors</definiens>
			</definition>
</paper>

		<paper id="1100">
			<definition id="0">
				<sentence>A collocation is a predisposed combination of words , typically pairwise words , that tend to regularly co-occur ( e.g. orange and peel ) .</sentence>
				<definiendum id="0">collocation</definiendum>
				<definiens id="0">a predisposed combination of words</definiens>
			</definition>
			<definition id="1">
				<sentence>Word repetition is a component of the lexical cohesion class of reiteration , and collocation is a lexical cohesion class in its entirety .</sentence>
				<definiendum id="0">Word repetition</definiendum>
				<definiendum id="1">collocation</definiendum>
				<definiens id="0">a component of the lexical cohesion class of reiteration , and</definiens>
				<definiens id="1">a lexical cohesion class in its entirety</definiens>
			</definition>
			<definition id="2">
				<sentence>Collocation : Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio ( Church and Hanks , 1990 ) and outputted to a lexicon .</sentence>
				<definiendum id="0">Collocation</definiendum>
				<definiens id="0">Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio</definiens>
			</definition>
			<definition id="3">
				<sentence>A thesaurus is a collection of synonym groups , indicating that synonym relations are captured , and the hierarchical structure of RT implies that superordinate relations are also captured .</sentence>
				<definiendum id="0">thesaurus</definiendum>
				<definiens id="0">a collection of synonym groups , indicating that synonym relations are captured</definiens>
			</definition>
			<definition id="4">
				<sentence>Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex .</sentence>
				<definiendum id="0">Relation weights</definiendum>
				<definiens id="0">for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex</definiens>
			</definition>
			<definition id="5">
				<sentence>The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity .</sentence>
				<definiendum id="0">segmentation algorithm</definiendum>
				<definiens id="0">compares adjacent windows of sentences and determines their lexical similarity</definiens>
			</definition>
			<definition id="6">
				<sentence>Segmentation points , indicating a change of subject , were determined by the agreement of three or more test subjects ( Litman and Passonneau , 1996 ) .</sentence>
				<definiendum id="0">Segmentation points</definiendum>
				<definiens id="0">indicating a change of subject , were determined by the agreement of three or more test subjects</definiens>
			</definition>
			<definition id="7">
				<sentence>Discussion : The segmentation algorithm using word repetition and relation weights in combination achieved mean precision and recall rates of 0.80 and 0.69 , respectively .</sentence>
				<definiendum id="0">Discussion</definiendum>
				<definiens id="0">The segmentation algorithm using word repetition and relation weights in combination achieved mean precision and recall rates of 0.80 and 0.69 , respectively</definiens>
			</definition>
</paper>

		<paper id="2143">
			<definition id="0">
				<sentence>The collocation preference here is restricted to the patterns `` noun phrase ( pronoun ) , verb '' and `` verb , noun phrase ( pronoun ) '' .</sentence>
				<definiendum id="0">noun phrase</definiendum>
				<definiens id="0">pronoun ) , verb '' and `` verb , noun phrase ( pronoun ) ''</definiens>
			</definition>
			<definition id="1">
				<sentence>Look for noun phrases 3 only to the left of the anaphor 4 those which agree in gender and number 5 with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores ; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences , a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora ; non-anaphoric `` it '' occurring in constructions such as `` It is important '' , `` It is necessary '' is eliminated by a `` referential filter '' 5Note that this restriction may not always apply in languages other than English ( e.g. German ) ; on the other hand , there are certain collective nouns in English which do not agree in number with their antecedents ( e.g. `` government '' , `` team '' , `` parliament '' etc. can be referred to by `` they '' ; equally some plural nouns ( e.g. `` data '' ) can be referred to by `` it '' ) and are exempted from the agreement test .</sentence>
				<definiendum id="0">aggregate score</definiendum>
				<definiens id="0">agree in gender and number 5 with the pronominal anaphor and group them as a set of potential candidates tial candidate and assign scores ; the candidate with the highest</definiens>
				<definiens id="1">collective nouns in English which do not agree in number with their antecedents</definiens>
			</definition>
			<definition id="2">
				<sentence>872 In order to evaluate the effectiveness of the approach and to explore if/how far it is superior over the baseline models for anaphora resolution , we also tested the sample text on ( i ) a Baseline Model which checks agreement in number and gender and , where more than one candidate remains , picks as antecedent the most recent subject matching the gender and number of the anaphor ( ii ) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor .</sentence>
				<definiendum id="0">Baseline Model</definiendum>
				<definiens id="0">the effectiveness of the approach and to explore if/how far it is superior over the baseline models for anaphora resolution</definiens>
				<definiens id="1">a Baseline Model which checks agreement in number and gender and , where more than one candidate remains , picks as antecedent the most recent subject matching the gender and number of the anaphor ( ii ) a</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>The task of sentence alignment is to match corresponding sentences in a text from One language to sentences in a translation of that text in another language .</sentence>
				<definiendum id="0">sentence alignment</definiendum>
				<definiens id="0">to match corresponding sentences in a text from One language to sentences in a translation of that text in another language</definiens>
			</definition>
			<definition id="1">
				<sentence>The alignment space includes all possible combinations of multiple matches upto and including 3:3 alignments .</sentence>
				<definiendum id="0">alignment space</definiendum>
				<definiens id="0">includes all possible combinations of multiple matches upto and including 3:3 alignments</definiens>
			</definition>
			<definition id="2">
				<sentence>2fEj ( 1 ) Dice ( E , .1 ) fE + fJ where lea is the number of lexical items which match in E and J , fE is tile number of lexical items in E and fj is the number of lexical items in J. The translation lists for each Japanese word are used disjunctively , so if one word in the list matches then we do not consider the other terms in the list .</sentence>
				<definiendum id="0">Dice</definiendum>
				<definiendum id="1">lea</definiendum>
				<definiendum id="2">fE</definiendum>
				<definiendum id="3">fj</definiendum>
				<definiens id="0">the number of lexical items which match in E and J ,</definiens>
				<definiens id="1">tile number of lexical items in E and</definiens>
			</definition>
</paper>

		<paper id="2182">
			<definition id="0">
				<sentence>As an illustration of this last condition , neither Galileo Probe nor gray plane is a valid entry , the former because it denotes an individual and the latter because it is a class of planes based upon an incidental feature ( color ) .</sentence>
				<definiendum id="0">gray plane</definiendum>
				<definiens id="0">an individual and the latter because it is a class of planes based upon an incidental feature ( color )</definiens>
			</definition>
			<definition id="1">
				<sentence>We have outlined an algorithm in this paper that , as it stands , could significantly speed up 1114 MUC=4 corpus WSJ corpus Category Algorithm Total Valid Valid Total Valid Valid Terms Terms Terms not Terms Terms Terms not Generated Generated in Wordnet Generated Generated in Wordnet Vehicle 1 % &amp; C 249 82 52 339 123 81 Vehicle R &amp; S 200 34 4 NA NA NA Weapon R &amp; C 257 93 54 150 17 Weapon R &amp; S 200 34 NA NA Table 2 : Valid category terms found that are not in Wordnet 12 NA Crimes ( a ) : terrorism , extortion , robbery ( es ) , assassination ( s ) , arrest ( s ) , disappearance ( s ) , violation ( s ) , assault ( s ) , battery ( es ) , tortures , raid ( s ) , seizure ( s ) , search ( es ) , persecution ( s ) , siege ( s ) , curfew , capture ( s ) , subversion , good ( s ) , humiliation , evictions , addiction , demonstration ( s ) , outrage ( s ) , parade ( s ) Crimes ( b ) : action-the murder ( s ) , Justines crime ( s ) , drug trafficking , body search ( es ) , dictator Noriega , gun running , witness account ( s ) Sites ( a ) : office ( s ) , enterprise ( s ) , company ( es ) , dealership ( s ) , drugstore ( s ) , pharmacies , supermarket ( s ) , terminal ( s ) , aqueduct ( s ) , shoeshops , marinas , theater ( s ) , exchange ( s ) , residence ( s ) , business ( es ) , employment , farmland , range ( s ) , industry ( es ) , commerce , etc. , transportation-have , market ( s ) , sea , factory ( es ) Sites ( b ) : grocery store ( s ) , hardware store ( s ) , appliance store ( s ) , book store ( s ) , shoe store ( s ) , liquor store ( s ) , A1batros store ( s ) , mortgage bank ( s ) , savings bank ( s ) , creditor bank ( s ) , Deutsch-Suedamerikanische bank ( s ) , reserve bank ( s ) , Democracia building ( s ) , apartment building ( s ) , hospital-the building ( s ) Vehicle ( a ) : gunship ( s ) , truck ( s ) , taxi ( s ) , artillery , Hughes-500 , tires , jitneys , tens , Huey-500 , combat ( s ) , ambulance ( s ) , motorcycle ( s ) , Vides , wagon ( s ) , Huancora , individual ( s ) , KFIR , M-bS , T-33 , Mirage ( s ) , carrier ( s ) , passenger ( s ) , luggage , firemen , tank ( s ) Vehicle ( b ) : A-37 plane ( s ) , A-37 Dragonfly plane ( s ) , passenger plane ( s ) , Cessna plane ( s ) , twin-engined Cessna plane ( s ) , C-47 plane ( s ) , grayplane ( s ) , KFIR plane ( s ) , Avianca-HK1803 plane ( s ) , LATN plane ( s ) , Aeronica plane ( s ) , 0-2 plane ( s ) , push-and-pull 0-2 plane ( s ) , push-and-pull plane ( s ) , fighter-bomber plane ( s ) Weapon ( a ) - '' launcher ( s ) , submachinegun ( s ) , mortar ( s ) , explosive ( s ) , cartridge ( s ) , pistol ( s ) , ammunition ( s ) , carbine ( s ) , radio ( s ) , amount ( s ) , shotguns , revolver ( s ) , gun ( s ) , materiel , round ( s ) , stick ( s ) clips , caliber ( s ) , rocket ( s ) , quantity ( es ) , type ( s ) , AK-47 , backpacks , plugs , light ( s ) Weapon ( b ) : car bomb ( s ) , night-two bomb ( s ) , nuclear bomb ( s ) , homemade bomb ( s ) , incendiary bomb ( s ) , atomic bomb ( s ) , medium-sized bomb ( s ) , highpower bomb ( s ) , cluster bomb ( s ) , WASP cluster bomb ( s ) , truck bomb ( s ) , WASP bomb ( s ) , high-powered bomb ( s ) , 20-kg bomb ( s ) , medium-intensity bomb ( s ) Table 3 : Top results from ( a ) the head noun list the task of building a semantic lexicon .</sentence>
				<definiendum id="0">disappearance</definiendum>
				<definiens id="0">it stands , could significantly speed up 1114 MUC=4 corpus WSJ corpus Category Algorithm Total Valid Valid Total Valid Valid Terms Terms Terms not Terms Terms Terms not Generated Generated in Wordnet Generated Generated in Wordnet Vehicle 1 % &amp; C 249 82 52 339 123 81 Vehicle R &amp; S 200 34 4 NA NA NA Weapon R &amp; C 257 93 54 150 17 Weapon R &amp; S 200 34 NA NA</definiens>
				<definiens id="1">parade ( s ) Crimes ( b ) : action-the murder ( s ) , Justines crime ( s ) , drug trafficking , body search ( es ) , dictator Noriega , gun running</definiens>
				<definiens id="2">industry ( es ) , commerce , etc. , transportation-have , market ( s ) , sea , factory ( es ) Sites ( b ) : grocery store ( s ) , hardware store ( s ) , appliance store ( s ) , book store ( s ) , shoe store ( s ) , liquor store ( s ) , A1batros store ( s ) , mortgage bank ( s ) , savings bank ( s ) , creditor bank ( s ) , Deutsch-Suedamerikanische bank ( s ) , reserve bank ( s ) , Democracia building ( s ) , apartment building</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>\ [ Definition 3 \ ] quasi-dependency-strength : Given a baseNP set NP= { npt , np2 , ... , npM } and lexicon W= { W~ , ... , WM } , VW~ , Wj E W , the quasidependency-strength of w~wj is defined as : ~_a dep ( w i -- ~ w.i , nP t ) npt ~ NP dsCwi -- ~ w~ ) = Z co ( w~ -- ~ w j , rip , ) npt ~ NP where dep ( w i ~ wj , npk ) is the count of dependent word pair w~'w~ contained in np , , co ( w~ , w~ , np , ) is the count of cooccurent word pair ( w~ , wj ) contained in np , .</sentence>
				<definiendum id="0">~_a dep</definiendum>
				<definiendum id="1">nP t</definiendum>
				<definiendum id="2">dep ( w</definiendum>
				<definiendum id="3">npk )</definiendum>
				<definiens id="0">the count of dependent word pair w~'w~ contained in np , , co ( w~ , w~ , np , ) is the count of cooccurent word pair ( w~ , wj ) contained in np</definiens>
			</definition>
			<definition id="1">
				<sentence>On condition that the structure part of the model is fixed , the parameter optimization means to find the optimal sets of quasi-dependency-strength in order that the data description length minimized , 4 that is C -arg min LCNP I G ) G Where L ( NPIG ) is the optimal coding length of NP when G is known .</sentence>
				<definiendum id="0">parameter optimization</definiendum>
				<definiens id="0">means to find the optimal sets of quasi-dependency-strength in order that the data description length minimized , 4 that is C -arg min LCNP I G ) G Where L ( NPIG ) is the optimal coding length of NP when G is known</definiens>
			</definition>
			<definition id="2">
				<sentence>The testing target is the precision of baseNP structural analysis , that is a precision = -× 100 % ; b Where a is the count of the baseNPs which are correctly analyzed , b is the count of the baseNPs in the tesing set .</sentence>
				<definiendum id="0">testing target</definiendum>
				<definiendum id="1">b</definiendum>
				<definiens id="0">the precision of baseNP structural analysis</definiens>
				<definiens id="1">a precision = -× 100 % ; b Where a is the count of the baseNPs which are correctly analyzed</definiens>
				<definiens id="2">the count of the baseNPs in the tesing set</definiens>
			</definition>
			<definition id="3">
				<sentence>Table-3 : The performance of ML algorithm and MDL algorithm BaseNP analysis precision Close test ML algorithm MDL algorithm 89.0 % 91.5 % The most difficult problem related to the structural analysis of baseNPs is the acquisition of the quasi-dependency-strength .</sentence>
				<definiendum id="0">baseNPs</definiendum>
				<definiens id="0">the acquisition of the quasi-dependency-strength</definiens>
			</definition>
</paper>

		<paper id="2162">
			<definition id="0">
				<sentence>Our TRs axe triples ( D , E , Z ) where D is a sequence of SL WCs , E is a sequence of TL WCs and Z is a WA matrix between D and E. For using one rule in the translation process we first rewrite the probability P ( eld ) : P ( eld ) = ~ P ( E , Zld ) • P ( elE , Z , d ) ( 6 ) E , Z In order to simplify the maximization ( equation 1 ) we use only the TR which gives the maximum probability .</sentence>
				<definiendum id="0">TRs axe triples</definiendum>
				<definiendum id="1">D</definiendum>
				<definiendum id="2">Z</definiendum>
				<definiens id="0">a sequence of SL WCs , E is a sequence of TL WCs and</definiens>
			</definition>
			<definition id="1">
				<sentence>o e ( JD ) • 1-I P ( z ( k ) , E ( k ) IC ( d ( k ) ) k=l • p ( e ( jk ) IZ ( k ) , E ( k ) , d ( k ) ) ( 8 ) Here L is the number of SL units , d ( k ) is the k-th SL unit , e ( k ) is the k-th TL unit and jl , ... , ji is a permutation of the numbers 1 , ... , L. During the last decade some publications have discussed the problem of learning WCs using clustering techniques based on maximum likelihood criteria applied to single language corpora .</sentence>
				<definiendum id="0">k )</definiendum>
				<definiens id="0">the k-th TL unit and jl , ... , ji is a permutation of the numbers 1 , ... , L. During the last decade some publications have discussed the problem of learning WCs using clustering techniques based on maximum likelihood criteria applied to single language corpora</definiens>
			</definition>
			<definition id="2">
				<sentence>The function ML ( CINw_~w , ) which has to be optimized depends only on the count function Nw~w , which counts the frequency that the word w ' comes after the word w. Using two sets of WCs for the TL and SL which are independent ( method INDEP ) does not guarantee that those WCs are much correlated .</sentence>
				<definiendum id="0">function ML ( CINw_~w , )</definiendum>
				<definiens id="0">counts the frequency that the word w ' comes after the word w. Using two sets of WCs for the TL and SL which are independent ( method INDEP ) does not guarantee that those WCs are much correlated</definiens>
			</definition>
			<definition id="3">
				<sentence>When we include the WA in the EM algorithm as described in section 3 we can produce fewer lexicon entries of a much better quality : • Tuesday-+Dienstag ( 0.97 ) , dienstags ( 0.029 ) • Frankfurt -- +Frankfurt ( 1 ) The following two corresponding WCs ( out of 600 ) show a typical result of the method COMB to determine correlated WCs : • Mittwoch , Donnerstag , Freitag , Sonnabend , Friihlingsanfang , Karsamstag , Volkstrauertag , Weihnachtsferien , Sommerschule , Thomas , einschlieflen • Wednesday , Thursday , Friday , Thursdays , Fridays , Thomas , Veterans ' , mourning , national , spending , spring , summer-school 988 To evaluate the complete system we translated 200 randomly chosen sentences drawn from an independent test corpus and checked manually how many of them constituted acceptable translations .</sentence>
				<definiendum id="0">Thursdays</definiendum>
				<definiens id="0">determine correlated WCs : • Mittwoch , Donnerstag , Freitag , Sonnabend , Friihlingsanfang , Karsamstag , Volkstrauertag , Weihnachtsferien , Sommerschule , Thomas , einschlieflen • Wednesday , Thursday , Friday ,</definiens>
				<definiens id="1">the complete system we translated 200 randomly chosen sentences drawn from an independent test corpus</definiens>
			</definition>
</paper>

		<paper id="2215">
			<definition id="0">
				<sentence>2 I will use the French ( se tenir ) and English ( stand , lie ) simplified entries below , in my illustration of mismatches between the generator and the lexicons .</sentence>
				<definiendum id="0">English (</definiendum>
			</definition>
			<definition id="1">
				<sentence>I illustrate below three main types of gaps between the input semantics ( IS ) to the generator and the lexicon entries ( LEX ) of the language in which to generate .</sentence>
				<definiendum id="0">LEX</definiendum>
				<definiens id="0">of the language in which to generate</definiens>
			</definition>
			<definition id="2">
				<sentence>( iii ) IS LEX Underspecification Generating , in French , from the partial IS below ( 5 ) , ( 5 ) PsYertical ( agent : john , against : vall , time : tl ) &amp; PsHorizontal ( agent : john , against : wall , time : t2 ) &amp; tl &lt; t2 needs extra work from the lexicon processor , with respect to the entries presented here , as one does not want to end up generating John se tint contre le mur puis il se tint contre lemur ( John was against the wall then he was against the wall ) . Because of the conjunctions here , one can not just consider se tenir as vague with respect to lie and stand. This illustrates a lexicon in action , where the lexical processor must process se tenir as underspecified : PositionState -+ PsVertical V PsHorizontal The lexical processor will thus produce the divergences se tenir debout ( stand ) and se tenir allongd ( lying ) to generate ( with some generation processing such as lexical choice , ellipsis , pronominalisation , etc ) John se tenait ( debout ) eontre lemur puis s'allongea contre lui ( John was standing against the wall then he lied against it ) . Where the continuum perspective comes in , is that we do not want to `` freeze '' the meanings of words once and for all. As we just saw , in French one might want to generate se tenir debout or just se tenir depending on the semantics of its arguments and also depending on the context as in ( 5 ) . In the WYSINNWYG approach , words are allowed to have their `` meanings '' vary in context. In other words , the literal meaning ( s ) coded in the lexicon is/are the `` closest '' possible meaning ( s ) of a word within the STH context , and by enriching the discourse context ( dc ) , one ends up `` specialising '' or `` generalising '' the meaning ( s ) of the word , using formally two hierarchies : semantic ( STH ) and lexical ( LTH ) , enabling different types of lexicon representations : vagueness , underspecification and lexical rules. Multilingual lexicons are usually monolingual lexicons connected via translation links ( Tlinks ) , whereas truly multilingual lexicons , as defined by ( Cahill and Gazdar , 1995 ) , involve n 41 hierarchies , thus involving an additional abstract hierarchy containing information shared by two or more languages. Figure 3 illustrates the STH which is shared by all lexicons ( French , English , Spanish , etc ) , and the lexical MLTH which involves the abstract hierarchy shared by all LTHs. grH T A Pr.perly -~lnteiner ¢~mtacl I/ ILTH t ' L ll~4M I n I , ~C.nla~ I - , : . ... . .. .. : ... ... i . `` , , `` , : '' .f '' , .. `` . i ~ ~2 , , , , .. ~ ' ~ ' `` / / ' -prep ~ , ~ ~ , . ; ~ , ~ ... ... ~~oo ..L Figure 3 : Subset of the Multilingual Hierarchy for Prepositions The lexicons themselves are also organised as language lexical type hierarchies ( Spanish LTH , English LTH in Figure 3 ) . For instance , the English dictionary ( eng-lexeme ) has the English prepositions ( engprep ) as one of its sub-types , which itself has as subtypes all the English prepositions ( along , through , on , in , ... ) . These prepositions have in turn subtypes ( for instance , on has onl , on2 , ... ) , which can themselves have subtypes ( onll , on12 , ... ) . All these language dependent LTHs inherit part of their information from a truly Multilingual Lexical Type Hi1324 erarchy ( MLTH ) , which contains information shared by all lexicons. There might be several levels of sharing , for instance , family-related languages sharing. Lexical types are linked to the STH via their language LTH and the MLTH , so that these lexicons can be used by either monolingual or multilingual processing. The advantages of a MTLH extend to 1 ) lexicon acquisition , by allowing lexicons to inherit information from the abstract level hierarchy. This is even more useful when acquiring family-related languages ; and 2 ) robustness , as the lexical processors can try to `` make guesses '' on the assignment of a sense to a lexeme absent from a dictionary , based on similarities in morphology or orthography , with other family-related language lexemes , s Lexical Rules The STH along with the LTH allow the lexicographers to leave the meaning of some lexemes as vague or underspecified. The vagueness or underspecification typing allows the lexical processor to specialise or generalise the meaning of a lexeme , for a particular task and on a needed basis. Formally , generalisation and specialisation can be done in various ways , as specified for instance in ( Kameyama et al. , 1991 ) , ( Poesio , 1996 ) , ( Mahesh et al. , 1997 ) . A lexicon entry is considered as vague when its semantics is typed using a general monomorphic type covering multiple senses , as is the case of the French entry `` se-tenir3 '' , or the Spanish preposition en , as represented in ( 6 ) . ( 6 ) \ [ key : `` en '' , form : \ [ orth : \ [ exp : `` en '' \ ] ... . sense : \ [ sem : \ [ name : Location3 ... . \ ] It is at processing time , and only if needed , that the semantic type Location for en can be further processed as LocContact , LocContainer , ... to generate the English prepositions ( on , at , ... ) . Lexicon vagueness is represented by mapping the citation form lex of any word x appearing in a corpus to a semantic monomorphic type m , which belongs to STH. Let us consider MAPS , the function which links lex to STH , dc a discourse context where lex can appear , and _ the immediate type/sub-type relation between types of STH , then : ( 7 ) x is vague iff 3rn E STH : rn = MAPS ( dc , lex ( x ) ) A 3n , oE STH : n EmAoC_rnAn¢oA VrESTH : rErn : /~qESTH : qCr 3I have not investigated this issue yet , but see ( Cahill , 1998 ) for promising results with respect to making guesses on phonology. In other words , lex is vague , if m is in a type/subtype relation with all its immediate sub-types. The meaning of a lexeme is considered as underspecifled when its semantics is represented via a polymorphic type , which presents a disjunction of semantic types , 4 thus covering different polysemous senses , as is the case of the Spanish preposition `` por '' in ( 8 ) , and typical examples in lexical semantics , such as door which is typed as PHYSICAL_OBJECT-ORAPERTURE. 5 ( 8 ) \ [ key : `` por ' , form : \ [ orth : \ [ `` exp : `` por'\ ] ... . sense : \ [ sem : \ [ name : Through ; Along\ ] ... . \ ] It is at processing time only , and on a needed basis only , that the semantic type Through-OR-Along for pot can be further processed as either Through , or Along , ... , thus allowing the generator or analyser to find the appropriate representation depending on the task. Disambiguating `` por '' to generate English , requires that the lexeme be embedded within the discourse context , where the filled arguments of the prepositions will provide semantic information under constraints. For instance , walk and river could contribute to the disambiguation of pot as Along. Lexicon underspecification is represented by mapping lex ( the citation form of a word x ) to a semantic polymorphic type p , which belongs to STH , then : ( 9 ) x is underspecifled iff 3p E STH : rn = MAPS ( dc , Iex ( x ) ) A 3s C STH : p = Vs A Card ( s ) &gt; _2 In other words , lex is underspecified , if p is a disjunction of types , and no type/sub-type relation is required .</sentence>
				<definiendum id="0">LEX Underspecification Generating</definiendum>
				<definiendum id="1">PsYertical ( agent</definiendum>
				<definiendum id="2">PsHorizontal ( agent</definiendum>
				<definiendum id="3">lexicon entry</definiendum>
				<definiendum id="4">lex</definiendum>
				<definiendum id="5">n EmAoC_rnAn¢oA VrESTH</definiendum>
				<definiendum id="6">lex</definiendum>
				<definiendum id="7">Card</definiendum>
				<definiens id="0">a lexicon in action , where the lexical processor must process se tenir as underspecified : PositionState -+ PsVertical V PsHorizontal The lexical processor will thus produce the divergences se tenir debout ( stand ) and se tenir allongd ( lying ) to generate ( with some generation processing such as lexical choice , ellipsis , pronominalisation , etc ) John se tenait ( debout ) eontre lemur puis s'allongea contre lui</definiens>
				<definiens id="1">the literal meaning ( s ) coded in the lexicon is/are the `` closest '' possible meaning ( s ) of a word within the STH context , and by enriching the discourse context ( dc ) , one ends up `` specialising '' or `` generalising '' the meaning ( s ) of the word , using formally two hierarchies : semantic ( STH ) and lexical ( LTH ) , enabling different types of lexicon representations : vagueness , underspecification and lexical rules. Multilingual lexicons are usually monolingual lexicons connected via translation links ( Tlinks ) , whereas truly multilingual lexicons</definiens>
				<definiens id="2">involve n 41 hierarchies , thus involving an additional abstract hierarchy containing information shared by two or more languages. Figure 3 illustrates the STH which is shared by all lexicons ( French , English , Spanish , etc ) , and the lexical MLTH which involves the abstract hierarchy shared by all LTHs. grH T A Pr.perly -~lnteiner ¢~mtacl I/ ILTH t ' L ll~4M I n I , ~C.nla~ I -</definiens>
				<definiens id="3">one of its sub-types , which itself has as subtypes all the English prepositions ( along , through , on , in , ... ) . These prepositions have in turn subtypes ( for instance , on has onl , on2 , ... ) , which can themselves have subtypes ( onll , on12 , ... ) . All these language dependent LTHs inherit part of their information from a truly Multilingual Lexical Type Hi1324 erarchy ( MLTH ) , which contains information shared by all lexicons. There might be several levels of sharing , for instance , family-related languages sharing. Lexical types are linked to the STH via their language LTH and the MLTH</definiens>
				<definiens id="4">the lexical processors can try to `` make guesses '' on the assignment of a sense to a lexeme absent from a dictionary , based on similarities in morphology or orthography , with other family-related language lexemes , s Lexical Rules The STH along with the LTH allow the lexicographers to leave the meaning of some lexemes as vague or underspecified. The vagueness or underspecification typing allows the lexical processor to specialise or generalise the meaning of a lexeme , for a particular task and on a needed basis. Formally , generalisation</definiens>
				<definiens id="5">LocContact , LocContainer , ... to generate the English prepositions ( on , at , ... ) . Lexicon vagueness is represented by mapping the citation form lex of any word x appearing in a corpus to a semantic monomorphic type m</definiens>
				<definiens id="6">the function which links lex to STH , dc a discourse context where lex can appear , and _ the immediate type/sub-type relation between types of STH</definiens>
				<definiens id="7">presents a disjunction of semantic types , 4 thus covering different polysemous senses</definiens>
				<definiens id="8">either Through , or Along , ... , thus allowing the generator or analyser to find the appropriate representation depending on the task. Disambiguating `` por '' to generate English , requires that the lexeme be embedded within the discourse context , where the filled arguments of the prepositions will provide semantic information under constraints. For instance , walk and river could contribute to the disambiguation of pot as Along. Lexicon underspecification is represented by mapping lex ( the citation form of a word x ) to a semantic polymorphic type p</definiens>
				<definiens id="9">a disjunction of types</definiens>
			</definition>
</paper>

		<paper id="2187">
			<definition id="0">
				<sentence>Let us assume that any noun which can be modified by bon has a telic role in which the main function ( s ) of the object is described ( e.g. execute programmes for a computer , run for a car 5 ) , then the semantics of the compound adjective + noun can be defined as follows : Let N be a noun of semantic type a. , and of Qualia : \ [ ... , Telic : T , ... \ ] where T denotes the set of predicates associated with the telic role of the noun N. Let Y the variable associated with N and let us assume that T is a list of predicates of the form Fi ( _ , - ) .</sentence>
				<definiendum id="0">Telic</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">any noun which can be modified by bon has a telic role in which the main function ( s ) of the object is described ( e.g. execute programmes for a computer</definiens>
				<definiens id="1">the set of predicates associated with the telic role of the noun N. Let Y the variable associated with N and let us assume that T is a list of predicates of the form Fi ( _ , - )</definiens>
			</definition>
			<definition id="1">
				<sentence>From a compositional point of view , the combination Adjective + Noun is treated as follows , where R is the semantic representation of the adjective , T , the contents of the telic role of the Qualia of the noun N of type o , r , a particular element of T , and Y , the variable associated with the noun : sem-composition ( Adj ( R ) , Noun ( Qualia ( T ) ) = ) ~Y : c~ , 3F/ ( Y , _ ) E T , ( N ( Y ) A R ( Y ) ( Fi ( Y , _ ) ) ) .</sentence>
				<definiendum id="0">R</definiendum>
				<definiendum id="1">Adj</definiendum>
				<definiendum id="2">_ ) E T , ( N</definiendum>
				<definiendum id="3">R ( Y )</definiendum>
				<definiens id="0">the semantic representation of the adjective , T , the contents of the telic role of the Qualia of the noun N of type o , r , a particular element of T</definiens>
			</definition>
			<definition id="2">
				<sentence>We have here another form of representation for bon , where Fi is a CAUSE .</sentence>
				<definiendum id="0">Fi</definiendum>
				<definiens id="0">a CAUSE</definiens>
			</definition>
			<definition id="3">
				<sentence>, the values for the above variables become : A -= +char , +ident , X= \ [ ... . t/state J \ ] Y = AWAY FROMA ( \ [ prop ACTIVE ( J ) \ ] ) where ACTIVE ( J ) is an elementary property of an ontology describing the status of events .</sentence>
				<definiendum id="0">ACTIVE</definiendum>
				<definiens id="0">an elementary property of an ontology describing the status of events</definiens>
			</definition>
			<definition id="4">
				<sentence>Finally , we consider the expressions Y makes a good X , Y is a good X as collocations where good is not fully treated compositionally .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">a good X as collocations where good is not fully treated compositionally</definiens>
			</definition>
</paper>

		<paper id="2155">
			<definition id="0">
				<sentence>Ripper ( Cohen , 1995 ) , is used to acquire accent classification systems from a training corpus of correctly classified examples , each defined by a vector of feature values , or predictors .</sentence>
				<definiendum id="0">Ripper</definiendum>
			</definition>
			<definition id="1">
				<sentence>The CLAUSE TYPE feature represents global syntactic constituency information , while the BASENP TYPE feature represents local or NP-internal syntactic constituency information .</sentence>
				<definiendum id="0">CLAUSE TYPE feature</definiendum>
				<definiens id="0">represents global syntactic constituency information</definiens>
			</definition>
			<definition id="2">
				<sentence>It also appears that as suggested by earlier experiments , 5In the rules themselves , written in Prolog-style notation , the tilde character is a two-place operator , X - , ~ Y , signifying that Y is a member of the set-value for feature X. lexical features trade-off for one other as well as with form of expression information .</sentence>
				<definiendum id="0">tilde character</definiendum>
				<definiens id="0">a member of the set-value for feature X. lexical features trade-off for one other as well as with form of expression information</definiens>
			</definition>
			<definition id="3">
				<sentence>The Computational Processing of Intonational Prominence : a Functional Prosody Perspective .</sentence>
				<definiendum id="0">Computational Processing of Intonational Prominence</definiendum>
			</definition>
</paper>

		<paper id="2224">
			<definition id="0">
				<sentence>expression pattern ( a ) , ( b ) , and ( c ) described in Section 3.2 : ( a ) A + ha + predicative noun ( b ) A + ga + aru ( exist ) ( c ) A + ga + verbalized noun + suru For example , `` ryoseibana ( hermaphrodite flower ) '' `` mebana ( female flower ) '' , and `` obana ( male flower ) '' in Figure 6 are interpreted as the types of the plant part by this rule .</sentence>
				<definiendum id="0">ryoseibana</definiendum>
				<definiens id="0">male flower ) '' in Figure 6 are interpreted as the types of the plant part by this rule</definiens>
			</definition>
			<definition id="1">
				<sentence>The words in Figure 10 ( b ) , `` ryoseibana ( hermaphrodite flower ) '' , `` mebana ( female flower ) '' , and `` obana ( male flower ) '' represent the names of the plant part .</sentence>
				<definiendum id="0">ryoseibana</definiendum>
				<definiens id="0">female flower ) '' , and `` obana ( male flower ) '' represent the names of the plant part</definiens>
			</definition>
</paper>

		<paper id="2229">
			<definition id="0">
				<sentence>Verbmobil employs different approaches to machine translation .</sentence>
				<definiendum id="0">Verbmobil</definiendum>
			</definition>
			<definition id="1">
				<sentence>The VIT ( short for Verbmobil Interface Term ) was designed as a common output format for the two alternative and independently developed syntacticsemantic analysis components of the first project phase ( Bos et al. , 1998 ) .</sentence>
				<definiendum id="0">VIT</definiendum>
			</definition>
			<definition id="2">
				<sentence>( 1 ) vit ( vitID ( sid ( l , a , ge , O,20 , l , ge , y , semantics ) , \ [ word ( montag , 13 , \ [ II16\ ] ) , word ( ist,14 , \ [ ii17\ ] ) , word ( gut,15 , \ [ lllOl ) l ) , index ( lll3,1109 , il04 ) , \ [ decl ( lll2 , hl05 ) , gut ( lllO , il05 ) , dofw ( lll6 , ilO5 , mon ) , support ( lll7 , il04,1110 ) , indef ( llll , ilO5,1115 , hl06 ) \ ] , \ [ ccom_plug ( hl05,1114 ) , ccom_plug ( h106,1109 ) , in g ( ii12,1113 ) , in_g ( lll7,1109 ) , in_g ( lll6,1115 ) , in_g ( llll , lll4 ) , leq ( lll4 , hlO5 ) , leq ( llO9 , hl06 ) , leq ( llOg , hl05 ) \ ] , Is sort ( ilO5 , time ) l , \ [ \ ] , \ [ num ( ilO5 , sg ) , pers ( il05,3 ) \ ] , \ [ ta_mood ( ilO4 , ind ) , ta_tense ( ilO4 , pres ) , ta_perf ( ilO4 , nonperf ) \ ] , \ [ \ ] ) The VIT can be viewed as a theory-independent representation for underspecified semantic representations ( Bos et al. , 1996 ) .</sentence>
				<definiendum id="0">vitID</definiendum>
				<definiens id="0">a , ge , O,20 , l , ge , y , semantics ) , \ [ word ( montag , 13 , \ [ II16\ ] ) , word ( ist,14 , \ [ ii17\ ] )</definiens>
			</definition>
			<definition id="3">
				<sentence>If an utterance is structurally ambiguous , it will be represented by one VIT , which specifies the set of DRSs corresponding to the different readings of the utterance .</sentence>
				<definiendum id="0">VIT</definiendum>
				<definiens id="0">specifies the set of DRSs corresponding to the different readings of the utterance</definiens>
			</definition>
			<definition id="4">
				<sentence>Formally , a VIT is a nine-place PROLOG term .</sentence>
				<definiendum id="0">VIT</definiendum>
			</definition>
			<definition id="5">
				<sentence>The left hand side consists of a list of conditions on partial analyses , Cond2 being a condition ( or a list of conditions ) on the first partial analysis ( VIT ) , etc. , where the order of conditions parallels the expected temporal order of the analyses .</sentence>
				<definiendum id="0">left hand side</definiendum>
				<definiens id="0">consists of a list of conditions on partial analyses , Cond2 being a condition ( or a list of conditions ) on the first partial analysis ( VIT ) , etc. , where the order of conditions parallels the expected temporal order of the analyses</definiens>
			</definition>
			<definition id="6">
				<sentence>However , the WHG contains no path which includes the preposition in in an appropriate position .</sentence>
				<definiendum id="0">WHG</definiendum>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>PAS is the sole level of representation in Combinatory Categorial Grammar ( CCG ) ( Steedman , 1996 ) .</sentence>
				<definiendum id="0">PAS</definiendum>
				<definiens id="0">the sole level of representation in Combinatory Categorial Grammar ( CCG ) ( Steedman , 1996 )</definiens>
			</definition>
			<definition id="1">
				<sentence>For instance , Grimshaw ( 1990 ) defines the thematic hierarchy as : Agent &gt; Experiencer &gt; Goal / Location / Source &gt; Theme `` Thanks to Mark Steedman for discussion and material , and to the anonymous reviewer of an extended version whose comments led to significant revisions .</sentence>
				<definiendum id="0">thematic hierarchy</definiendum>
				<definiens id="0">an extended version whose comments led to significant revisions</definiens>
			</definition>
			<definition id="2">
				<sentence>This level of description also simplifies the formulation of grammatical function changing ; the primary term of a passivized predicate ( PASS p ) is the secondary term of the active p. I follow Shaumyan and Steedman ( 1996 ) also in the ordered representation of the PAS ( 1 ) .</sentence>
				<definiendum id="0">PASS p )</definiendum>
				<definiens id="0">the secondary term of the active p. I follow Shaumyan and Steedman ( 1996 ) also in the ordered representation of the PAS ( 1 )</definiens>
			</definition>
			<definition id="3">
				<sentence>Application is the only primitive of the combinatory system ; it is indicated by juxtaposition in the examples and denoted by • in the normal order evaluator ( §4 ) .</sentence>
				<definiendum id="0">Application</definiendum>
				<definiens id="0">the only primitive of the combinatory system ; it is indicated by juxtaposition in the examples</definiens>
			</definition>
			<definition id="4">
				<sentence>3 The types of case markers in the lexicon can be defined as : ( 6 ) Lexical type assignment for the case marker ( -case ) encoding argument n : -case : = C ( n ) : T ( C ( n ) ) x\N : x where T ( C ) denotes the semantic type for category C : ( 7 ) a. T ( NPn ) = I ( lower type for NPn ) b. T ( C ) = T ( if C is a type shifted category as in ( 3 ) ) c. T ( C ) = BBT ( if C is a type shifted and composed category ) ( 5 ) and ( 6 ) are schemas that yield three lexical categories per -case : one for lower type , and two for higher types which differ only in the directionality of the main function due to ( 5 ) .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">: ( 6 ) Lexical type assignment for the case marker ( -case ) encoding argument n : -case : = C ( n ) : T ( C ( n ) ) x\N : x where T ( C ) denotes the semantic type for category C : ( 7 ) a. T ( NPn ) = I ( lower type for NPn ) b. T ( C ) = T ( if</definiens>
				<definiens id="1">a type shifted and composed category ) ( 5 ) and ( 6 ) are schemas that yield three lexical categories per -case : one for lower type , and two for higher types which differ only in the directionality of the main function due to ( 5 )</definiens>
			</definition>
			<definition id="5">
				<sentence>Revealing the PAS amounts to stripping off all combinators from the combinatory form by evaluating the reducible expressions ( redexes ) .</sentence>
				<definiendum id="0">Revealing the PAS</definiendum>
				<definiens id="0">amounts to stripping off all combinators from the combinatory form by evaluating the reducible expressions ( redexes )</definiens>
			</definition>
			<definition id="6">
				<sentence>PAS is the semantic normal form of a derivation .</sentence>
				<definiendum id="0">PAS</definiendum>
				<definiens id="0">the semantic normal form of a derivation</definiens>
			</definition>
</paper>

		<paper id="1087">
			<definition id="0">
				<sentence>SSNs are a natural extension of Simple Kecurrent Networks ( SRNs ) ( Elman , I99I ) , which are in turn a natural extension of Multi-Layered Perceptrons ( MLPs ) ( Rumelhart et al. , 1986 ) .</sentence>
				<definiendum id="0">SSNs</definiendum>
			</definition>
			<definition id="1">
				<sentence>SRNs are an improvement over MLPs because they generalize what they have learned over words in different sentence positions .</sentence>
				<definiendum id="0">SRNs</definiendum>
				<definiens id="0">an improvement over MLPs because they generalize what they have learned over words in different sentence positions</definiens>
			</definition>
			<definition id="2">
				<sentence>SSNs are an improvement over SKNs because the use of TSVB gives them the additional ability to generalize over constituents in different structural positions .</sentence>
				<definiendum id="0">SSNs</definiendum>
				<definiens id="0">an improvement over SKNs because the use of TSVB gives them the additional ability to generalize over constituents in different structural positions</definiens>
			</definition>
			<definition id="3">
				<sentence>Simple Recurrent Networks ( Elman , 1991 ) are a simple extension of the most popular form of connectionist network , Multi-Layered Perceptrons ( MLPs ) ( Rumelhart et al. , 1986 ) .</sentence>
				<definiendum id="0">Multi-Layered Perceptrons</definiendum>
				<definiendum id="1">MLPs</definiendum>
				<definiens id="0">a simple extension of the most popular form of connectionist network</definiens>
			</definition>
			<definition id="4">
				<sentence>Like MLPs , SRNs consist of a finite set of units which are connected by weighted links , as illustrated in figure 1 .</sentence>
				<definiendum id="0">SRNs</definiendum>
				<definiens id="0">consist of a finite set of units which are connected by weighted links</definiens>
			</definition>
			<definition id="5">
				<sentence>A number of algorithms exist for training such networks with loops in their flow of activation ( called recurrence ) , for example Backpropagation Through Time ( Rumelhart et al. , 1986 ) .</sentence>
				<definiendum id="0">number of algorithms</definiendum>
				<definiens id="0">exist for training such networks with loops in their flow of activation ( called recurrence )</definiens>
			</definition>
			<definition id="6">
				<sentence>The first three tags all designate the constituents introduced with them as their parents , but the fourth tag ( NN ) designates the constituent introduced with the previous tag ( y ) as its parent .</sentence>
				<definiendum id="0">previous tag</definiendum>
				<definiens id="0">all designate the constituents introduced with them as their parents , but the fourth tag ( NN ) designates the constituent introduced with the</definiens>
			</definition>
			<definition id="7">
				<sentence>The Susanne corpus consists of a subset of the Brown corpus , preparsed according to the Susanne classification scheme described in ( Sampson , 1995 ) .</sentence>
				<definiendum id="0">Susanne corpus</definiendum>
				<definiens id="0">consists of a subset of the Brown corpus</definiens>
			</definition>
			<definition id="8">
				<sentence>The test set consists of 536 4485 words in 181 sentences , average length 24.78 words .</sentence>
				<definiendum id="0">test set</definiendum>
			</definition>
</paper>

		<paper id="2216">
			<definition id="0">
				<sentence>Relations Syntagmatic relations , also known as collocations , are used differently by lexicographers , linguists and statisticians denoting almost similar but not identical classes of expressions .</sentence>
				<definiendum id="0">Relations Syntagmatic relations</definiendum>
				<definiens id="0">also known as collocations , are used differently by lexicographers , linguists and statisticians denoting almost similar but not identical classes of expressions</definiens>
			</definition>
			<definition id="1">
				<sentence>Sinclair ( 1991 ) states that ' % word 1328 which occurs in close proximity to a word under investigation is called a collocate of it ... . Collocation is the occurrence of two or more words within a short space of each other in a text '' .</sentence>
				<definiendum id="0">Collocation</definiendum>
				<definiens id="0">the occurrence of two or more words within a short space of each other in a text ''</definiens>
			</definition>
			<definition id="2">
				<sentence>LFs are central to the study of collocations : A lexical function F is a correspondence which associates a lexical item L , called the key word of F , with a set of lexical items F ( L ) -the value of F. ( Mel'6uk , 1988 ) 4 We focus here on syntagmatic LFs describing cooccurrence relations such as pay attention , legitimate complaint ; from a distance .</sentence>
				<definiendum id="0">LFs</definiendum>
				<definiendum id="1">lexical function F</definiendum>
				<definiens id="0">a correspondence which associates a lexical item L , called the key word of F , with a set of lexical items F ( L ) -the value of F.</definiens>
			</definition>
			<definition id="3">
				<sentence>( 2a ) # O=\ [ key : tel : `` truth '' , \ [ syntagmatic : LSFSyn \ [ base : # 0 , collocate : \ [ key : `` plain '' , sense : adj2 , Ir : \ [ comp : no , superl : no\ ] \ ] \ ] \ ] ... \ ] ( 2b ) # 0=\ [ key : rel : `` pupil '' , \ [ syntagmatic : LSFSyn \ [ base : # 0 , collocate : \ [ key : `` teacher '' , sense : n2 , freq : \ [ value : 5\ ] \ ] \ ] \ ] ... \ ] ( 2c ) # O=\ [ key : tel : `` conference '' , \ [ syntagmatic : LSFSyn \ [ base : # 0 , collocate : \ [ key : `` student '' , sense : nl , freq : \ [ value : 9\ ] \ ] \ ] \ ] ... \ ] In examples ( 2 ) , the LSFSyn produces a new entry composed of two or more entries .</sentence>
				<definiendum id="0">LSFSyn</definiendum>
			</definition>
			<definition id="4">
				<sentence>LSFSyn signals a compositional syntax and a compositional semantics , and restricts the use of lexemes to be used in the composition .</sentence>
				<definiendum id="0">LSFSyn</definiendum>
				<definiens id="0">signals a compositional syntax and a compositional semantics , and restricts the use of lexemes to be used in the composition</definiens>
			</definition>
</paper>

		<paper id="2135">
			<definition id="0">
				<sentence>Acknowledgements Andy Merlino is the principal system developer of BNN .</sentence>
				<definiendum id="0">Acknowledgements Andy Merlino</definiendum>
				<definiens id="0">the principal system developer of BNN</definiens>
			</definition>
			<definition id="1">
				<sentence>The Alembic sub-system is the result of efforts by MITRE 's Language Processing Group including Marc Vilaln and John Aberdeen for part of speech proper name taggers , and David Day for training these on closed caption text .</sentence>
				<definiendum id="0">Alembic sub-system</definiendum>
				<definiens id="0">the result of efforts by MITRE 's Language Processing Group including Marc Vilaln and John Aberdeen for part of speech proper name taggers , and David Day for training these on closed caption text</definiens>
			</definition>
			<definition id="2">
				<sentence>Literature Abstracts : An Approach Based on the Identification of Self-Indicating Phrases .</sentence>
				<definiendum id="0">Literature Abstracts</definiendum>
				<definiens id="0">An Approach Based on the Identification of Self-Indicating Phrases</definiens>
			</definition>
			<definition id="3">
				<sentence>Zhang , H. J. ; Low , C. Y. ; Smoliar , S. W. and Zhong , D. ( 1995 ) Video Parsing , Retrieval , and Browsing : An Integrated and Content-Based Solution .</sentence>
				<definiendum id="0">Browsing</definiendum>
				<definiens id="0">An Integrated and Content-Based Solution</definiens>
			</definition>
</paper>

		<paper id="2141">
			<definition id="0">
				<sentence>TDMT makes the most of an example-based framework , which produces an output sentence by mimicking the closest translation example to an input sentence .</sentence>
				<definiendum id="0">TDMT</definiendum>
				<definiendum id="1">example-based framework</definiendum>
				<definiens id="0">produces an output sentence by mimicking the closest translation example to an input sentence</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>Tree-DOP uses two decomposition operations that produce connected subtrees of utterance representations : ( 1 ) the Root operation selects any node of a tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates ; ( 2 ) the Frontier operation then chooses a set ( possibly empty ) of nodes in the new subtree different from its root and erases all subtrees dominated by the chosen nodes .</sentence>
				<definiendum id="0">Root operation</definiendum>
				<definiens id="0">selects any node of a tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates ; ( 2 ) the Frontier operation then chooses a set ( possibly empty ) of nodes in the new subtree different from its root and erases all subtrees dominated by the chosen nodes</definiens>
			</definition>
			<definition id="1">
				<sentence>Let CP ( tlCS ) denote the probability of choosing a tree t from a competition set CS containing t. Then the probability of a derivation is P ( &lt; tl , t2 ... tk &gt; ) = l'\ ] iCP ( ti I CSi ) where the competition probability CP ( t ICS ) is given by CP ( t I CS ) = P ( t ) / : El , e CS P ( t ' ) Here , P ( t ) is the fragment probability for t in a given corpus .</sentence>
				<definiendum id="0">CP</definiendum>
				<definiens id="0">the probability of choosing a tree t from a competition set CS containing t. Then the probability of a derivation is P ( &lt; tl , t2 ... tk &gt; ) = l'\ ] iCP ( ti I CSi ) where the competition probability CP ( t ICS ) is given by CP ( t I CS ) = P ( t ) / : El , e CS P ( t ' ) Here , P ( t ) is the fragment probability for t in a given corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>As Manning and Carpenter ( 1997 ) have shown , the competition sets can be made dependent on the composition operation .</sentence>
				<definiendum id="0">competition</definiendum>
			</definition>
			<definition id="3">
				<sentence>The cstructure is a tree that describes the surface constituent structure of an utterance ; the f-structure is an attribute-value matrix marking the grammatical relations of subject , predicate and object , as well as providing agreement features and semantic forms ; and is a correspondence function that maps nodes of the c-structure into units of the f-structure ( Kaplan &amp; Bresnan 1982 ; Kaplan 1989 ) .</sentence>
				<definiendum id="0">f-structure</definiendum>
				<definiens id="0">a tree that describes the surface constituent structure of an utterance</definiens>
				<definiens id="1">an attribute-value matrix marking the grammatical relations of subject , predicate and object</definiens>
				<definiens id="2">a correspondence function that maps nodes of the c-structure into units of the f-structure</definiens>
			</definition>
			<definition id="4">
				<sentence>( 1 ) `` XlPRED K , m\ ] \ ] TENSE PRES I • PRED 'eat ( SUB J ) ' \ ] Note that the ¢ correspondence function gives an explicit characterization of the relation between the superficial and underlying syntactic properties of an utterance , indicating how certain parts of the string carry information about particular units of underlying structure .</sentence>
				<definiendum id="0">correspondence function</definiendum>
				<definiens id="0">gives an explicit characterization of the relation between the superficial and underlying syntactic properties of an utterance , indicating how certain parts of the string carry information about particular units of underlying structure</definiens>
			</definition>
			<definition id="5">
				<sentence>Nonbranching Dominance demands that no c-structure category appears twice in a nonbranching dominance chain ; Uniqueness asserts that there can be at most one value for any attribute in the f-structure ; Coherence prohibits the appearance of grammatical functions that are not governed by the lexical predicate ; and Completeness requires that all the functions that a predicate governs appear as attributes in the local f-structure .</sentence>
				<definiendum id="0">Uniqueness</definiendum>
				<definiendum id="1">Coherence</definiendum>
				<definiens id="0">the appearance of grammatical functions that are not governed by the lexical predicate</definiens>
			</definition>
			<definition id="6">
				<sentence>Let CP ( fl CS ) denote the probability of choosing a fragment f from a competition set CS containing f , then the probability of a derivation D = &lt; fl , f2 ... fk &gt; is ( 10 ) P ( &lt; fl , f2 ... fk &gt; ) = FIi CPffi I CSi ) where as in Tree-DOP , CP ( f I CS ) is expressed in terms of fragment probabilities P ( f ) by the formula ( 11 ) CP ( f I CS ) = P ( D / ~ , fe cs P ( f ) Tree-DOP is the special case where there are no conditions of validity other than the ones that are enforced at each step of the stochastic process by the composition operation .</sentence>
				<definiendum id="0">CP</definiendum>
			</definition>
			<definition id="7">
				<sentence>Suppose that Fi-I =fl °f2 o ... ofi.1 is the current subanalysis at the beginning of step i in the process , that LNC ( Fi.1 ) denotes the category of the leftmost nonterminal node of the c-structure of F i.1 , and that r ( f ) is now interpreted as the root-node category of fs c-structure component .</sentence>
				<definiendum id="0">Fi.1 )</definiendum>
				<definiens id="0">the category of the leftmost nonterminal node of the c-structure of F i.1 , and that r ( f ) is now interpreted as the root-node category of fs c-structure component</definiens>
			</definition>
			<definition id="8">
				<sentence>Moreover , M1 assigns the same probability to SG and PL , whereas M2 does n't .</sentence>
				<definiendum id="0">M1</definiendum>
				<definiens id="0">assigns the same probability to SG and PL , whereas M2 does n't</definiens>
			</definition>
</paper>

		<paper id="1112">
			<definition id="0">
				<sentence>The initial method , using WordNet ( Miller et al. 1990 ) , produced multiple cross-classification of articles , primarily due to the bushy nature of the verb tree coupled with the sense disambiguation problem .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">produced multiple cross-classification of articles</definiens>
			</definition>
			<definition id="1">
				<sentence>We developed two algorithms to : ( 1 ) explore WordNet ( WN-Verber ) to cluster related verbs and build a set of verb chains in a document , much as Morris and Hirst ( 1991 ) used Roget 's Thesaurus or like Hirst and St. Onge ( 1998 ) used WordNet to build noun chains ; ( 2 ) classify verbs according to a semantic classification system , in this case , using Levin 's ( 1993 ) English Verb Classes and Alternations ( EVCA-Yerber ) as a basis .</sentence>
				<definiendum id="0">Alternations ( EVCA-Yerber )</definiendum>
				<definiens id="0">verbs according to a semantic classification system</definiens>
			</definition>
			<definition id="2">
				<sentence>WordNet is a general lexical resource in which words are organized into synonym sets , each representing one underlying lexical concept ( Miller et al. 1990 ) .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">a general lexical resource in which words are organized into synonym sets</definiens>
			</definition>
			<definition id="3">
				<sentence>In the case of WordNet , all verb tokens ( n = 10K ) were considered in all senses , whereas in the case of EVCA , a subset of less ambiguous verbs were manually selected .</sentence>
				<definiendum id="0">EVCA</definiendum>
				<definiens id="0">a subset of less ambiguous verbs were manually selected</definiens>
			</definition>
			<definition id="4">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="5">
				<sentence>Selection and Information : A Class-Based Approach to Lexical Relationships .</sentence>
				<definiendum id="0">Selection</definiendum>
				<definiendum id="1">Information</definiendum>
			</definition>
</paper>

		<paper id="2138">
			<definition id="0">
				<sentence>Optical character recognition ( OCR ) is useful in a wide range of applications , such as office automation and information retrieval system .</sentence>
				<definiendum id="0">Optical character recognition ( OCR</definiendum>
				<definiens id="0">office automation and information retrieval system</definiens>
			</definition>
			<definition id="1">
				<sentence>Using dynamic programming technique , we can calculate P ( SIW ) ( = P ( SnlWm ) ) by the following equation : P ( SiIWj ) = max ( P ( Si_llWj ) * P ( ins ( ci ) ) , P ( SilWj_I ) • P ( del ( vj ) ) , P ( Si-llW -l ) • P ( cilv ) ) ( 4 ) where P ( ins ( c ) ) , P ( del ( v ) ) and P ( clv ) are the probabilities that letter c is inserted , letter v is deleted and letter v is substituted with c , respectively .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">letter v is deleted and letter v is substituted with c , respectively</definiens>
			</definition>
			<definition id="2">
				<sentence>Winnow is a multiplicative weight updating and incremental algorithm ( Littlestone , 1988 ; Golding and Roth , 1996 ) .</sentence>
				<definiendum id="0">Winnow</definiendum>
				<definiens id="0">a multiplicative weight updating and incremental algorithm</definiens>
			</definition>
			<definition id="3">
				<sentence>We define the confidence level of any word as all weights that vote for that word divided by all weights in the network .</sentence>
				<definiendum id="0">confidence level</definiendum>
				<definiens id="0">of any word as all weights that vote for that word divided by all weights in the network</definiens>
			</definition>
</paper>

		<paper id="2188">
			<definition id="0">
				<sentence>To circumvent a sparse data problem , we extract values of well-motivated features of utterances , such as speaker direction , punctuation marks , and a new feature , called dialogue act cues , which we find to be more effective than cue phrases and word n-grams in practice .</sentence>
				<definiendum id="0">dialogue act cues</definiendum>
				<definiens id="0">effective than cue phrases and word n-grams in practice</definiens>
			</definition>
			<definition id="1">
				<sentence>Given a tagged training corpus , TBL develops a learned model that consists of a sequence of rules .</sentence>
				<definiendum id="0">TBL</definiendum>
				<definiens id="0">consists of a sequence of rules</definiens>
			</definition>
			<definition id="2">
				<sentence>tag is SUGGEST Figure h Rules produced by the system To develop a sequence of rules from a tagged training corpus , TBL attempts to produce rules that will correctly label many of the utterances in the training data .</sentence>
				<definiendum id="0">TBL</definiendum>
				<definiens id="0">attempts to produce rules that will correctly label many of the utterances in the training data</definiens>
			</definition>
			<definition id="3">
				<sentence>Specifically , C de=f { sES \ [ H ( DIs ) &lt; 01 A # ( s ) &gt; 02 } where C is the set of dialogue act cues , S is the set of word substrings , D is the set of dialogue acts , 01 and 02 are predefined thresholds , # ( x ) is the number of times an event , x , occurs in the training corpus , and entropy 6 is defined in the standard way : 7 H ( D\ [ s ) de__f __ ~ '' ~dED P ( dls ) log 2 P ( d\ [ s ) .</sentence>
				<definiendum id="0">C</definiendum>
				<definiendum id="1">S</definiendum>
				<definiendum id="2">D</definiendum>
				<definiendum id="3">entropy 6</definiendum>
				<definiendum id="4">~dED P</definiendum>
				<definiens id="0">the set of dialogue act cues</definiens>
				<definiens id="1">the set of word substrings</definiens>
				<definiens id="2">the number of times an event , x , occurs in the training corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>Traditional cues are those cue phrases that have previously been reported in the literature , such as `` but '' and `` so '' ; potential cues consist of other useful word substrings that have not been considered , such as `` thanks '' and `` see you '' ; and for dialogues from a particular domain , there may be domain cues -for example , the appointment-scheduling corpora have dialogue act cues , such as `` what time '' and `` busy '' .</sentence>
				<definiendum id="0">Traditional cues</definiendum>
				<definiens id="0">those cue phrases that have previously been reported in the literature , such as `` but '' and `` so '' ; potential cues consist of other useful word substrings that have not been considered , such as `` thanks '' and `` see you '' ; and for dialogues from a particular domain</definiens>
			</definition>
			<definition id="5">
				<sentence>In a given pass through the training data , for each utterance that is incorrectly tagged , only R of the possible template instantiations are randomly selected , where R is a parameter that is set in advance .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">a parameter that is set in advance</definiens>
			</definition>
			<definition id="6">
				<sentence>Unlike probabilistic machine learning approaches , TBL fails to offer any measure of confidence in the tags that it produces .</sentence>
				<definiendum id="0">TBL</definiendum>
			</definition>
			<definition id="7">
				<sentence>As a direct comparison , we applied our system to Reithinger and Klesen 's training set ( 143 dialogues , 2701 utterances ) and disjoint testing set ( 20 dialogues , 328 utterances ) , which consist of utterances labeled with 18 different dialogue acts .</sentence>
				<definiendum id="0">disjoint testing</definiendum>
			</definition>
			<definition id="8">
				<sentence>11There are only 478 different cue phrases in the set , but for our system , it was necessary to manipulate the 1154 Word Substrings None Cue phrases ( from previous literature ) n Word n-grams Entropy minimization Entropy minimization with clustering Entropy minimization with filtering Entropy minimization with filtering and clustering # 0 936 16271 1053 1029 826 811 Accuracy 41.16 % ( a=O.O0 % ) 61.74 % ( a -- 0.69 % ) 69.21 % ( a=0.94 % ) 69.54 % ( a=1.97 % ) 70.18 % ( a=0.75 % ) 70.70 % ( a=1.31 % ) 71.22 % ( a=1.25 % ) Figure 3 : Tagging accuracy on held-out data , using different sets of word substrings in training As the figure shows , when the system was restricted from using any word substrings , its accuracy on unseen data was only 41.16 % .</sentence>
				<definiendum id="0">Word Substrings None Cue phrases</definiendum>
				<definiens id="0">Tagging accuracy on held-out data , using different sets of word substrings in training As the figure shows</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>The VIT encodes various pieces of information produced and used in the linguistic modules .</sentence>
				<definiendum id="0">VIT</definiendum>
				<definiens id="0">encodes various pieces of information produced and used in the linguistic modules</definiens>
			</definition>
			<definition id="1">
				<sentence>A VIT is a record-like data structure whose fields are filled with semantic , scopal , sortal , morphosyntactic , prosodic , discourse and other information ( see Table 1 ) .</sentence>
				<definiendum id="0">VIT</definiendum>
				<definiens id="0">a record-like data structure whose fields are filled with semantic , scopal , sortal , morphosyntactic , prosodic , discourse and other information</definiens>
			</definition>
			<definition id="2">
				<sentence>Besides purely linguistic information , a VIT contains a unique segment identifier that encodes the time span of the analyzed speech input , the analyzed path of the original word lattice , the producer of the VIT , which language is represented , etc .</sentence>
				<definiendum id="0">VIT</definiendum>
				<definiens id="0">contains a unique segment identifier that encodes the time span of the analyzed speech input</definiens>
			</definition>
			<definition id="3">
				<sentence>Labels denote the instantiated structures , primarily the individual predicates .</sentence>
				<definiendum id="0">Labels</definiendum>
			</definition>
			<definition id="4">
				<sentence>Resolution consists of an assignment of la3We owe the term minimal recursion to Copestake et al. ( 1995 ) , but the mechanism they describe was already in use in UDRSs .</sentence>
				<definiendum id="0">Resolution</definiendum>
				<definiens id="0">consists of an assignment of la3We owe the term minimal recursion to Copestake et al. ( 1995 ) , but the mechanism they describe was already in use in UDRSs</definiens>
			</definition>
			<definition id="5">
				<sentence>Grouping constraints take the form in_g ( L , G ) , where the `` in group '' relation ( in_g ) denotes that the label L is a member of the group G. The content of a group is , thus , defined by the set of such grouping constraints .</sentence>
				<definiendum id="0">Grouping constraints</definiendum>
				<definiens id="0">the form in_g ( L , G ) , where the `` in group '' relation ( in_g ) denotes that the label L is a member of the group G. The content of a group is , thus , defined by the set of such grouping constraints</definiens>
			</definition>
			<definition id="6">
				<sentence>A content checker is used to test language-independent structural properties , such as missing or wrongly bound variables , missing or inconsistent information , and cyclicity .</sentence>
				<definiendum id="0">content checker</definiendum>
				<definiens id="0">used to test language-independent structural properties , such as missing or wrongly bound variables , missing or inconsistent information , and cyclicity</definiens>
			</definition>
</paper>

		<paper id="1074">
			<definition id="0">
				<sentence>A flow in G is a vector = ( 91 , ~2 , `` ~m ) T~R m ( where T denotes the transpose of a matrix ) such as , for each vertex i E V : E ( 1 ) ue~+ ( i ) uew- ( i ) where w+ ( i ) denotes the set of edges entering vertex i , whereas w- ( i ) is the set of edges leaving vertex i. We can , furthermore , associate to each edge u of G = ( V , E ) two numbers , b~ , and eu with b~ , _ &lt; c , , , which will be called the lower capacity bound and the upper capacity bound of the edge .</sentence>
				<definiendum id="0">w+ ( i )</definiendum>
				<definiens id="0">a vector = ( 91 , ~2 , `` ~m ) T~R m ( where T denotes the transpose of a matrix ) such as , for each vertex i E V</definiens>
			</definition>
			<definition id="1">
				<sentence>This model defines for each pair of aligned sentences a graph G ( V , E ) as follows : • V comprises a source , a sink , all the English and French words , an empty English word , and an empty French word , • E comprises edges from the source to all the English words ( including the empty one ) , edges from all the French words ( including the empty one ) to the sink , an edge from the sink to the source , and edges from all English words ( including the empty one ) to all the French words ( including the empty one ) 2 .</sentence>
				<definiendum id="0">graph G</definiendum>
				<definiendum id="1">empty one ) to all the French words</definiendum>
				<definiens id="0">empty French word , • E comprises edges from the source to all the English words ( including the empty one ) , edges from all the French words ( including the empty one</definiens>
			</definition>
			<definition id="2">
				<sentence>445 • from the source to the empty English word , the capacity interval is \ [ O ; maz ( le , 1/ ) \ ] , where l I is the number of French words , and l~ the number of English ones , • from the English words ( including the empty one ) to the French words ( including the empty one ) , the capacity interval is \ [ 0 ; 1\ ] , • from the French words ( excluding the empty one ) to the sink , the capacity interval is \ [ 1 ; 1\ ] .</sentence>
				<definiendum id="0">l I</definiendum>
				<definiens id="0">the number of French words</definiens>
				<definiens id="1">including the empty one ) to the French words ( including the empty one )</definiens>
			</definition>
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>The CES is an application of SGML ( ISO-8879 , Standard Generalized Markup Language ) and is based on the TEl Guidelines for Electronic Text Encoding and Interchange .</sentence>
				<definiendum id="0">CES</definiendum>
				<definiens id="0">an application of SGML ( ISO-8879 , Standard Generalized Markup Language ) and is based on the TEl Guidelines for Electronic Text Encoding and Interchange</definiens>
			</definition>
			<definition id="1">
				<sentence>In addition to providing encoding conventions for elements relevant to corpus-based work , the CES provides a data architecture for linguistic corpora and their annotations .</sentence>
				<definiendum id="0">CES</definiendum>
			</definition>
			<definition id="2">
				<sentence>The Multext-East parallel corpus consists of seven translations of George Orwell 's Nineteen Eighty-Four : besides the original English version , the corpus contains translations in the six project languages .</sentence>
				<definiendum id="0">Multext-East parallel corpus</definiendum>
			</definition>
			<definition id="3">
				<sentence>A fourth document , the cesAlign document , is associated with each of the non-English versions , which includes links between sentences in the cesDoc encoding for each and the English version , thus providing a parallel alignment at the sentence level .</sentence>
				<definiendum id="0">encoding</definiendum>
				<definiens id="0">includes links between sentences in the cesDoc</definiens>
			</definition>
			<definition id="4">
				<sentence>The Multext segmenter is a language-independent and configurable tokenizer whose output includes token , paragraph and sentence boundary markers .</sentence>
				<definiendum id="0">Multext segmenter</definiendum>
				<definiens id="0">a language-independent and configurable tokenizer whose output includes token , paragraph and sentence boundary markers</definiens>
			</definition>
			<definition id="5">
				<sentence>Within Multext-East , resource data , including rules describing the form of sentence boundaries , word splitting ( cliticized forms decomposition ) , word compounding , quotations , numbers , dates , punctuation , capitalization , abbreviations etc. , was developed for the six project languages .</sentence>
				<definiendum id="0">word splitting ( cliticized</definiendum>
				<definiens id="0">forms decomposition ) , word compounding , quotations , numbers , dates , punctuation , capitalization</definiens>
			</definition>
			<definition id="6">
				<sentence>For corpus morpho-lexical processing purposes , the Multext-East consortium developed language-specific wordform dictionaries , which , for all languages except Estonian and Hungarian , contain the full inflectional paradigm for at least the lemmas appearing in the corpus .</sentence>
				<definiendum id="0">language-specific wordform dictionaries</definiendum>
				<definiens id="0">the full inflectional paradigm for at least the lemmas appearing in the corpus</definiens>
			</definition>
			<definition id="7">
				<sentence>Each dictionary entry has the following structure : wordform \ [ TAB\ ] 1emma \ [ TAB\ ] MSD \ [ TAB\ ] where wordform represents an inflected form of the lemma , characterised by a combination of feature values encoded by a Morphosyntactic Description ( MSD ) .</sentence>
				<definiendum id="0">wordform</definiendum>
				<definiens id="0">an inflected form of the lemma , characterised by a combination of feature values encoded by a Morphosyntactic Description ( MSD )</definiens>
			</definition>
			<definition id="8">
				<sentence>The Wordforms column gives the number of distinct wordforms appearing in the lexicon , irrespective of their lemma and MSD .</sentence>
				<definiendum id="0">Wordforms column</definiendum>
				<definiens id="0">gives the number of distinct wordforms appearing in the lexicon , irrespective of their lemma and MSD</definiens>
			</definition>
			<definition id="9">
				<sentence>The MSD field gives the total number of distinct MSDs used in the encoding of the lexicon stock .</sentence>
				<definiendum id="0">MSD field</definiendum>
				<definiens id="0">gives the total number of distinct MSDs used in the encoding of the lexicon stock</definiens>
			</definition>
</paper>

		<paper id="1113">
			<definition id="0">
				<sentence>picks up He picks up the ball Figure 1 : Featurisation Lexicalisation is the case when a particular subtree in the representation tree presents the meaning of some part of the string , which is not orally realized in phonological form .</sentence>
				<definiendum id="0">Featurisation Lexicalisation</definiendum>
				<definiens id="0">the case when a particular subtree in the representation tree presents the meaning of some part of the string</definiens>
			</definition>
			<definition id="1">
				<sentence>The notation used in SSTC to denote a correspondence consists of a pair of intervals X/Y attached to each node in the tree , where X ( SNODE ) denotes the interval containing the substring that corresponds to the node , and Y ( STREE ) denotes the interval containing the substring that corresponds to the subtree having the node as root \ [ 4\ ] .</sentence>
				<definiendum id="0">notation</definiendum>
				<definiendum id="1">X ( SNODE )</definiendum>
				<definiendum id="2">Y ( STREE )</definiendum>
				<definiens id="0">used in SSTC to denote a correspondence consists of a pair of intervals X/Y attached to each node in the tree</definiens>
				<definiens id="1">the interval containing the substring that corresponds to the node</definiens>
			</definition>
			<definition id="2">
				<sentence>The SSTC is a general structure that can associate , to string in a language , arbitrary tree structure as desired by the annotator to be the interpretation structure of the string , and more importantly is the facility to specify the correspondence between the string and the associated tree which can be interpreted for both analysis and synthesis in NLP .</sentence>
				<definiendum id="0">SSTC</definiendum>
				<definiens id="0">a general structure that can associate , to string in a language , arbitrary tree structure as desired by the annotator to be the interpretation structure of the string</definiens>
			</definition>
</paper>

		<paper id="2133">
			<definition id="0">
				<sentence>The core engine of LiLFeS is an Abstract Machine for Attribute-Value Logics , proposed by Carpenter and Qu .</sentence>
				<definiendum id="0">core engine of LiLFeS</definiendum>
				<definiens id="0">an Abstract Machine for Attribute-Value Logics , proposed by Carpenter and Qu</definiens>
			</definition>
			<definition id="1">
				<sentence>Structures ( TFSs ) Since Typed Feature Structures ( TFSs ) ( Carpenter , 1992 ) are the basic data structures in HPSG , the efficiency of handling TFSs has been considered as the key to improve the efficiency of an HPSG parser .</sentence>
				<definiendum id="0">Structures</definiendum>
				<definiens id="0">the basic data structures in HPSG , the efficiency of handling TFSs has been considered as the key to improve the efficiency of an HPSG parser</definiens>
			</definition>
			<definition id="2">
				<sentence>__list , for e~ample , is a sub-iype of the type my_list , and has two appropriate features , FIRST and REST .</sentence>
				<definiendum id="0">FIRST</definiendum>
				<definiens id="0">a sub-iype of the type my_list , and has two appropriate features</definiens>
			</definition>
			<definition id="3">
				<sentence>Value Logics The Abstract Machine for Attribute-Value Logics ( AMAVL ) is the unification engine of the LiLFeS system .</sentence>
				<definiendum id="0">Attribute-Value Logics</definiendum>
				<definiendum id="1">AMAVL</definiendum>
				<definiens id="0">the unification engine of the LiLFeS system</definiens>
			</definition>
			<definition id="4">
				<sentence>The representation of a TFS on memory resembles the graph notation of a TFS ; A node is represented by n+ 1 continuous data cells , where n is the number of features outgoing from the node .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of features outgoing from the node</definiens>
			</definition>
			<definition id="5">
				<sentence>Aquarius Prolog adopts an execution model with an instruction set that is fine-grained and close to an instruction set of a real machine .</sentence>
				<definiendum id="0">Aquarius Prolog</definiendum>
				<definiens id="0">adopts an execution model with an instruction set that is fine-grained and close to an instruction set of a real machine</definiens>
			</definition>
			<definition id="6">
				<sentence>We developed LiLFeS , a logic programming language for TFSs .</sentence>
				<definiendum id="0">LiLFeS</definiendum>
				<definiens id="0">a logic programming language for TFSs</definiens>
			</definition>
			<definition id="7">
				<sentence>I rev ( 1000 ) 10 times 48.7 ( Unit : seconds ) ( See notes in Table 4 for environment and other notes ) Table 6 Performance Comparison of LiLFeS Native-Code Compiler to Prolog Systems ( Some of the data is overlapped to Table 4 ) Hassan A~'t-Kaci ( 1991 ) Warren 's Abstract Machine , A Tutorial Reconstruction .</sentence>
				<definiendum id="0">Tutorial Reconstruction</definiendum>
				<definiens id="0">Some of the data is overlapped to Table 4 ) Hassan A~'t-Kaci ( 1991 ) Warren 's Abstract Machine , A</definiens>
			</definition>
			<definition id="8">
				<sentence>The XTAG Research Group ( 1995 ) A Lexicalized Tree Adjoining Grammar for English .</sentence>
				<definiendum id="0">XTAG Research Group</definiendum>
			</definition>
</paper>

		<paper id="2171">
			<definition id="0">
				<sentence>We present a linear representation of the multidimensional phonological data based on a straightforward linearization convention , which suffices to bring this conceptually multilinear data set under the scope of the well-known processing techniques for two-level morphology .</sentence>
				<definiendum id="0">linearization convention</definiendum>
				<definiens id="0">suffices to bring this conceptually multilinear data set under the scope of the well-known processing techniques for two-level morphology</definiens>
			</definition>
			<definition id="1">
				<sentence>System Our concept-to-speech generation system consists of a pipeline of modules ( Fig .</sentence>
				<definiendum id="0">System Our concept-to-speech generation system</definiendum>
			</definition>
			<definition id="2">
				<sentence>Input is a partially specified feature description which constrains the utterance to be generated .</sentence>
				<definiendum id="0">Input</definiendum>
			</definition>
			<definition id="3">
				<sentence>Output is a fully specified feature description ( in the sense of the particular grammar ) subsumed by the input structure , which is then linearized to yield a sentence .</sentence>
				<definiendum id="0">Output</definiendum>
				<definiens id="0">a fully specified feature description</definiens>
			</definition>
</paper>

		<paper id="2207">
			<definition id="0">
				<sentence>Sim ( i , j ) = max Sim ' ( i , j ) all paths = max np ( wordk ) x Xk,15 ) all paths In formula ( 5 ) , wordk is a word in the word lattice , and each selected word does not share any frames with any other selected words .</sentence>
				<definiendum id="0">Sim</definiendum>
				<definiendum id="1">wordk</definiendum>
				<definiens id="0">a word in the word lattice</definiens>
			</definition>
			<definition id="1">
				<sentence>np ( wordk ) is the number of phonemes of wordk .</sentence>
				<definiendum id="0">np ( wordk )</definiendum>
				<definiens id="0">the number of phonemes of wordk</definiens>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>In our discussion , the following notations are used : • each terminal node ( leaf node , discourse unit ) has an attached label ; • mark ( x ) is a function that takes a string of symbols x and returns each symbol in x marked in some way ( e.g. , with parentheses ) ; • simpl ( x ) is a function that eliminates all marked symbols from its argument , if they exist ; e.g. simpl ( a ( bc ) d ( e ) ) =ad ; • seq ( x , y ) is a se .</sentence>
				<definiendum id="0">terminal node</definiendum>
				<definiendum id="1">e.g. simpl</definiendum>
				<definiendum id="2">seq ( x , y )</definiendum>
				<definiens id="0">a function that takes a string of symbols x and returns each symbol in x marked in some way ( e.g. , with parentheses</definiens>
				<definiens id="1">a function that eliminates all marked symbols from its argument , if they exist ;</definiens>
				<definiens id="2">a se</definiens>
			</definition>
			<definition id="1">
				<sentence>More formally , for each terminal node u , if vein ( u ) is its vein , then accessibility from u is given by acc ( u ) = pref ( u , unmark ( vein ( u ) ) , where : vein is the function that computes the vein ; unmark ( x ) is a function that removes the markers from all symbols of its argument ; • pref is a function that retains the prefix of the second argument up to and including the first argument ( e.g. , if a and 13 are strings of labels and u is a label , pref ( u , aufl ) = ocu , referential expression , then either b directly realizes a center that appears for the first time in the discourse , or it refers back to another center realized by a referential expression aeA , such that Aeacc ( B ) .</sentence>
				<definiendum id="0">unmark ( x )</definiendum>
				<definiendum id="1">pref</definiendum>
				<definiens id="0">a function that removes the markers from all symbols of its argument ; •</definiens>
			</definition>
			<definition id="2">
				<sentence>are units , ceC is a referential expression that refers to beB , and B is not on the vein of C .</sentence>
				<definiendum id="0">ceC</definiendum>
			</definition>
			<definition id="3">
				<sentence>CT defines a set of transition types for discourse ( Grosz , Joshi , and Weinstein ( 1995 ) ; Brennan , Friedman and Pollard ( 1987 ) ) .</sentence>
				<definiendum id="0">CT</definiendum>
				<definiendum id="1">Weinstein</definiendum>
				<definiens id="0">defines a set of transition types for discourse ( Grosz , Joshi , and</definiens>
			</definition>
			<definition id="4">
				<sentence>Table 2 : Smoothness scores for transitions CENTER CONTINUATION 4 CENTER RETAINING 3 CENTER SHIFTING ( SMOOTH ) 2 CENTER SHIFTING ( ABRUPT ) 1 NO Cb 0 Conjecture C2 : The global smoothness score of a discourse when computed following VT is at least as high as the score computed following CT .</sentence>
				<definiendum id="0">VT</definiendum>
				<definiens id="0">The global smoothness score of a discourse when computed following</definiens>
			</definition>
</paper>

		<paper id="2144">
			<definition id="0">
				<sentence>Tile EDR Corpus is a Japanese version of treebank with morphological , structural , and semantic information .</sentence>
				<definiendum id="0">Tile EDR Corpus</definiendum>
				<definiens id="0">a Japanese version of treebank with morphological , structural , and semantic information</definiens>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>Although automatic word sense disambiguation ( WSD ) remains a much more difficult task than part of speech ( POS ) disambiguation , resources and automatic systems are starting to appear .</sentence>
				<definiendum id="0">WSD</definiendum>
				<definiens id="0">difficult task than part of speech ( POS ) disambiguation , resources and automatic systems are starting to appear</definiens>
			</definition>
			<definition id="1">
				<sentence>321 unambiguously tagged as place , we learn that in a structure GO to X , where GO is a verb of the same semantic class as the word go and X is a word containing place among its possible senses , then X is disambiguated as place .</sentence>
				<definiendum id="0">GO</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a verb of the same semantic class as the word go</definiens>
				<definiens id="1">a word containing place among its possible senses</definiens>
			</definition>
			<definition id="2">
				<sentence>Call R the tag Z which maximizes the following function ( where Z ranges over all the tags in ~ except Y , freq ( Y ) is the number of occurences of words unambiguously tagged with Y , freq ( Z ) is the number of occurences of words unambiguously tagged with Z , and incontext ( Z , C ) is the number of times a word unambiguously tagged with Z occurs in context C ) : freq ( Y ) *incontext ( Z , C ) R = argmaxz \ ] req ( Z ) The score assigned to the rule would then be : S : incontext ( Y , C ) freq ( Y ) *incontext ( R , C ) freq ( R ) In short , a good transformation from ~ to Y is one for which alternative tags in ~ have either very low frequency in the corpus or they seldom appear in context C. At every iteration cycle , the algorithm simply computes the best scoring transformation .</sentence>
				<definiendum id="0">tag Z</definiendum>
				<definiendum id="1">C )</definiendum>
				<definiens id="0">Z ranges over all the tags in ~ except Y</definiens>
				<definiens id="1">the number of occurences of words unambiguously tagged with Z , and incontext ( Z ,</definiens>
				<definiens id="2">the number of times a word unambiguously tagged with Z occurs in context C ) : freq ( Y ) *incontext ( Z , C ) R = argmaxz \ ] req</definiens>
			</definition>
			<definition id="3">
				<sentence>As usual , Precision is I II Precision I RecMI I F-measure Adjusted I Random 0.45 0.44 0.44 0.28 500 Goodlog 0.97 0.25 0.40 0.91 `` 500 Original 0.78 0.30 0.44 0.50 Table 1 : Precision and recall figures the number of correctly tagged words divided by the total number of tagged words ; Recall is the number of correctly tagged words divided by the number of words in the test corpus ( about 40000 ) .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiendum id="1">Recall</definiendum>
			</definition>
</paper>

		<paper id="1093">
			<definition id="0">
				<sentence>Classification makes it possible to access data from a user 's comparative viewpoints by combining 5WlH elements .</sentence>
				<definiendum id="0">Classification</definiendum>
				<definiens id="0">makes it possible to access data from a user 's comparative viewpoints by combining 5WlH elements</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , the cell specified by NEC and PC shows the number of articles containing NEC as a `` who '' element and PC as a `` what '' element .</sentence>
				<definiendum id="0">PC</definiendum>
				<definiens id="0">a `` what '' element</definiens>
			</definition>
			<definition id="2">
				<sentence>Cotp , develops a new computer .</sentence>
				<definiendum id="0">Cotp</definiendum>
				<definiens id="0">develops a new computer</definiens>
			</definition>
			<definition id="3">
				<sentence>B Inc. puts a portable terminal on the market , CommuniJ C Telecommunication starts a virtual market .</sentence>
				<definiendum id="0">CommuniJ C Telecommunication</definiendum>
				<definiens id="0">starts a virtual market</definiens>
			</definition>
			<definition id="4">
				<sentence>CBSP is a robust and effective method of analysis which uses lexical information , expression patterns and case-markers in sentences .</sentence>
				<definiendum id="0">CBSP</definiendum>
				<definiens id="0">a robust and effective method of analysis which uses lexical information , expression patterns and case-markers in sentences</definiens>
			</definition>
			<definition id="5">
				<sentence>Ltd. ) and ~-~ ( Univ. ) mean they are organizations and `` who '' elements .</sentence>
				<definiendum id="0">~-~</definiendum>
				<definiens id="0">mean they are organizations and `` who '' elements</definiens>
			</definition>
			<definition id="6">
				<sentence>A 5WlH element belongs to the same 5W1H set as the nearest predicate after it .</sentence>
				<definiendum id="0">5WlH element</definiendum>
			</definition>
			<definition id="7">
				<sentence>`` Who '' elements are `` NEC , A Co. , and B Co. '' listed on the vertical axis which is the fundamental axis in the upper frame of Figure ( data ) , [ ] ~ ( recovery ) , and ~ ( technique ) . ''</sentence>
				<definiendum id="0">NEC</definiendum>
				<definiens id="0">A Co. , and B Co. '' listed on the vertical axis which is the fundamental axis in the upper frame of Figure ( data )</definiens>
			</definition>
			<definition id="8">
				<sentence>The DEC Alpha workstation ( 300 MHz ) is a server machine providing 5WlH classification and navigation functions for 50 users through WWW browsers .</sentence>
				<definiendum id="0">DEC Alpha workstation</definiendum>
			</definition>
			<definition id="9">
				<sentence>SOM ( Self-Organization Map ) is an effective automatic classification method for any data represented by vectors ( Kohonen , 1990 ) .</sentence>
				<definiendum id="0">SOM ( Self-Organization Map</definiendum>
			</definition>
			<definition id="10">
				<sentence>GALOIS/ULYSSES is a lattice-based classification system and the user can browse information on the lattice produced by the existence of keywords ( Carpineto and Romano , 1995 ) .</sentence>
				<definiendum id="0">GALOIS/ULYSSES</definiendum>
				<definiens id="0">a lattice-based classification system and the user can browse information on the lattice produced by the existence of keywords</definiens>
			</definition>
</paper>

		<paper id="2183">
			<definition id="0">
				<sentence>Thus there are analogous classes of automata at the level of labeled three-dimensional tree manifolds , the level of labeled trees and at the level of strings ( which can be understood as twoand one-dimensional tree manifolds ) which recognize sets of structures that yield , respectively , the TALs , the CFLs , and the regular languages .</sentence>
				<definiendum id="0">TALs</definiendum>
				<definiens id="0">one-dimensional tree manifolds ) which recognize sets of structures that yield , respectively , the</definiens>
			</definition>
			<definition id="1">
				<sentence>Tree manifolds are a generalization to arbitrary dimensions of Gorn 's tree domains ( Gorn , 1967 ) .</sentence>
				<definiendum id="0">Tree manifolds</definiendum>
			</definition>
			<definition id="2">
				<sentence>A tree domain is a set of node address drawn from N* ( that is , a set of strings of natural numbers ) in which c is the address of the root and the children of a node at address w occur at addresses w0 , wl , ... , in left-to-right order .</sentence>
				<definiendum id="0">tree domain</definiendum>
				<definiens id="0">a set of node address drawn from N* ( that is , a set of strings of natural numbers ) in which c is the address of the root and the children of a node at address w occur at addresses w0 , wl , ... , in left-to-right order</definiens>
			</definition>
			<definition id="3">
				<sentence>The interpretation of a tuple ( a , q , 7 ) E A d is that if a node of a d-TM is labeled a and T encodes the assignment of states to its children , then that node may be assigned state q. A run of an d-TM automaton A on a E-labeled d-TM 7 = ( T , r ) is an assignment r : T -+ Q of states in Q to nodes in T in which each assignment is licensed by A. If we let Q0 c Q be any set of accepting states , then the set of ( finite ) Elabeled d-TM recognized by A , relative to Q0 , is that set for which there is a run of A that assigns the root a state in Q0 .</sentence>
				<definiendum id="0">interpretation of a tuple</definiendum>
				<definiens id="0">if a node of a d-TM is labeled a and T encodes the assignment of states</definiens>
				<definiens id="1">an assignment r : T -+ Q of states in Q to nodes in T in which each assignment</definiens>
			</definition>
			<definition id="4">
				<sentence>It is easy to see that in the typical `` crossproduct '' construction of the proof of closure under intersection , for instance , the dimensionality of the TMs is a parameter that determines the type of the objects being manipulated but does not affect the manner of their manipulation .</sentence>
				<definiendum id="0">TMs</definiendum>
			</definition>
			<definition id="5">
				<sentence>Uniform proofs can be obtained for closure of recognizable sets under determinization ( in a bottom-up sense ) , projection , cylindrification , Boolean operations and for decidability of emptiness .</sentence>
				<definiendum id="0">Uniform proofs</definiendum>
				<definiens id="0">closure of recognizable sets under determinization ( in a bottom-up sense ) , projection , cylindrification</definiens>
			</definition>
			<definition id="6">
				<sentence>a structure : ( T , &lt; i , ~i , &lt; ~+ , H , Pa ) l &lt; _i &lt; a , a~g , where T is a rooted , connected subset of T 3 for some n. With this signature it is easy to define the set of 3-TM that captures a TAG in the sense that their 2-dimensional yields -- the set of maximal points wrt , ~+ , ordered by 4 + and , ~+ -- form the set of trees derived by the TAG .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">a rooted</definiens>
				<definiens id="1">captures a TAG in the sense that their 2-dimensional yields -- the set of maximal points wrt , ~+ , ordered by 4 + and , ~+ -- form the set of trees derived by the TAG</definiens>
			</definition>
			<definition id="7">
				<sentence>In general , of course , SA constraints depend not only on the attributes ( the label ) of a node , but also on the elementary tree in which it occurs and its position in that tree .</sentence>
				<definiendum id="0">SA constraints</definiendum>
				<definiens id="0">the label ) of a node , but also on the elementary tree in which it occurs and its position in that tree</definiens>
			</definition>
</paper>

		<paper id="2176">
			<definition id="0">
				<sentence>We have found that a natural application for the analysis of entity-description pairs is language reuse , which includes techniques of extracting shallow structure from a corpus and applying that structure to computer-generated texts .</sentence>
				<definiendum id="0">language reuse</definiendum>
				<definiens id="0">includes techniques of extracting shallow structure from a corpus and applying that structure to computer-generated texts</definiens>
			</definition>
			<definition id="1">
				<sentence>Language reuse involves two components : a source text written by a human and a target text , that is to be automatically generated by a computer , partially making use of structures reused from the source text .</sentence>
				<definiendum id="0">Language reuse</definiendum>
				<definiens id="0">involves two components : a source text written by a human and a target text</definiens>
			</definition>
			<definition id="2">
				<sentence>WordNet ( Miller et al. , 1990 ) is an on-line hierarchical lexical database which contains semantic information about English words ( including hypernymy relations which we use in our system ) .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">an on-line hierarchical lexical database which contains semantic information</definiens>
			</definition>
			<definition id="3">
				<sentence>The synset offset is a number that uniquely identifies a concept node ( synset ) in the WordNet hierarchy .</sentence>
				<definiendum id="0">synset offset</definiendum>
				<definiens id="0">a number that uniquely identifies a concept node</definiens>
			</definition>
			<definition id="4">
				<sentence>Ripper ( Cohen , 1995 ) is an algorithm that learns rules from example tuples in a relation .</sentence>
				<definiendum id="0">Ripper</definiendum>
				<definiens id="0">an algorithm that learns rules from example tuples in a relation</definiens>
			</definition>
			<definition id="5">
				<sentence>Precision ( or P ) is defined to be the number of matches divided by the number of elements in the predicted set .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the number of matches divided by the number of elements in the predicted set</definiens>
			</definition>
			<definition id="6">
				<sentence>Recall ( or R ) is the number of matches divided by the number of elements in the correct set .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">the number of matches divided by the number of elements in the correct set</definiens>
			</definition>
			<definition id="7">
				<sentence>There are two parts to the evaluation : how well does the system performs in selecting semantic features ( WordNet nodes ) and how well it works in constraining the choice of a description .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">nodes ) and how well it works in constraining the choice of a description</definiens>
			</definition>
			<definition id="8">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="2208">
			<definition id="0">
				<sentence>Part-of-speech tagging is one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information .</sentence>
				<definiendum id="0">Part-of-speech tagging</definiendum>
				<definiens id="0">one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information</definiens>
			</definition>
			<definition id="1">
				<sentence>, Cn ; V ) , where the Ci are , in general , feature constraints on a sequence of the ambiguous parses , and V is an integer denoting the vote of the rule .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">an integer denoting the vote of the rule</definiens>
			</definition>
			<definition id="2">
				<sentence>And , among the k-grams which are equally good ( same f/n ) , those with a higher n ( hence less uncertainty ) are given higher votes .</sentence>
				<definiendum id="0">good</definiendum>
				<definiens id="0">hence less uncertainty ) are given higher votes</definiens>
			</definition>
</paper>

		<paper id="2132">
			<definition id="0">
				<sentence>Since each input Thai text can be segmented into individual words that can be further tagged with all possible POSs using an electronic Thai dictionary , the POS tagging tasks can be regarded as a kind of POS disambiguation problem using contexts as follows : IPT : ( iptdt , ... , ipt-ll , ipt_t , ipt_rl , ... , ipt_rr ) OPT : POS_t , ( 1 ) where ipt_t is the element related to the possible POSs of the target word , ( ipt_lt , ... , ipt_ll ) and ( ipt_rl , ... , ipt_rr ) are the elements related to the contexts , i.e. , the POSs of the words to the left and right of the target word , respectively , and POS_t is the correct POS of the target word in the contexts .</sentence>
				<definiendum id="0">ipt_t</definiendum>
				<definiendum id="1">POSs of the</definiendum>
				<definiendum id="2">POS_t</definiendum>
				<definiens id="0">a kind of POS disambiguation problem using contexts as follows : IPT : ( iptdt , ... , ipt-ll , ipt_t , ipt_rl , ... , ipt_rr</definiens>
			</definition>
			<definition id="1">
				<sentence>The entropy of the set S , i.e. , the average amount of information needed to identify the class ( the POS ) of an example in 5 ' , is in f o ( S ) = _ ~-~ f req ( Ci , S ) ~ ( ~\ ] i , S ) ) , ISl x In ( fre ( 2 ) where ISl is the number of examples in S and freq ( Ci , S ) is the number of examples belonging to class Ci .</sentence>
				<definiendum id="0">ISl</definiendum>
				<definiendum id="1">S )</definiendum>
				<definiens id="0">The entropy of the set S , i.e. , the average amount of information needed to identify the class ( the POS ) of an example in 5 ' , is in f o ( S )</definiens>
				<definiens id="1">the number of examples in S</definiens>
			</definition>
			<definition id="2">
				<sentence>( 5 ) Figure 1 shows a single-neuro tagger ( SNT ) which consists of a 3-layer feedforward neural network .</sentence>
				<definiendum id="0">SNT</definiendum>
				<definiens id="0">consists of a 3-layer feedforward neural network</definiens>
			</definition>
			<definition id="3">
				<sentence>When word x is given in position y ( y = t , li , or rj ) , element ipt-y of input IPT is a weighted pattern defined as ipt_y = w_y .</sentence>
				<definiendum id="0">word x is</definiendum>
				<definiendum id="1">IPT</definiendum>
				<definiens id="0">given in position y ( y = t , li , or rj ) , element ipt-y of input</definiens>
			</definition>
			<definition id="4">
				<sentence>( ezl , ex2 , '' `` - , ezn ) , = ( Ix , , I~2 , -- ' , I~n ) ( 6 ) where w_y is the weight obtained in ( 5 ) , n is the total number of POSs defined in Thai , and 803 Izi = w_y .</sentence>
				<definiendum id="0">w_y</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the weight obtained in ( 5 )</definiens>
			</definition>
			<definition id="5">
				<sentence>( 7 ) Here tile Prob ( POSi\ [ x ) is the prior probability of POSi that the word x can be and is estimated from tile training data as Prob ( PO &amp; \ [ x ) IPOSi , xl Ixl ' ( 8 ) where IPOSi , x\ [ is the number of times both POSi and x appear and Ixl is the number of times x appears in all the training data .</sentence>
				<definiendum id="0">Ixl</definiendum>
				<definiens id="0">the prior probability of POSi that the word x can be and is estimated from tile training data as Prob</definiens>
				<definiens id="1">the number of times both POSi and x appear</definiens>
				<definiens id="2">the number of times x appears in all the training data</definiens>
			</definition>
			<definition id="6">
				<sentence>If x is an unknown word , i.e. , it does not appear in the training data , each bit e , :i is obtained as follows : 1__ if POSi is a candidate = n , ' ( 9 ) exi 0 , otherwise , where nx is the number of POSs that the word x can be ( this number can be simply obtained from an electronic Thai dictionary ) .</sentence>
				<definiendum id="0">nx</definiendum>
				<definiens id="0">an unknown word , i.e. , it does not appear in the training data</definiens>
				<definiens id="1">a candidate = n</definiens>
				<definiens id="2">the number of POSs that the word</definiens>
			</definition>
			<definition id="7">
				<sentence>The OPT is a pattern defined as follows : OPT = ( O1 , O2 , '' '' , On ) .</sentence>
				<definiendum id="0">OPT</definiendum>
				<definiens id="0">a pattern defined as follows : OPT = ( O1 , O2 , '' ''</definiens>
			</definition>
			<definition id="8">
				<sentence>, D , ~ ) , ( 14 ) whose bits are defined as follows : 1 ifPOSi is a desired answer Di = 0 .</sentence>
				<definiendum id="0">ifPOSi</definiendum>
				<definiens id="0">a desired answer Di = 0</definiens>
			</definition>
			<definition id="9">
				<sentence>otherwise ( 15 ) and WOPT and WDES are respectively defined as and EOBd ( 16 ) WOPT EACT WDE S = 1 WOPT , ( 17 ) where EOBJ and EACT are the objective and actual errors , respectively .</sentence>
				<definiendum id="0">EACT</definiendum>
				<definiens id="0">the objective and actual errors , respectively</definiens>
			</definition>
</paper>

		<paper id="2156">
</paper>

		<paper id="2167">
			<definition id="0">
				<sentence>User gives the feedback to resolve the ambiguities of analysis .</sentence>
				<definiendum id="0">User</definiendum>
				<definiens id="0">gives the feedback to resolve the ambiguities of analysis</definiens>
			</definition>
			<definition id="1">
				<sentence>Part-of-Speech Tagging System ( Lim et al. , 1996 ) proposed tagging system that uses word-tag transformation rules dealing with agglutinative characteristics of Korean , and also extends the tagger by using specific transformation rule considering the lexical information of mistagged word .</sentence>
				<definiendum id="0">Part-of-Speech Tagging System</definiendum>
				<definiens id="0">uses word-tag transformation rules dealing with agglutinative characteristics of Korean</definiens>
			</definition>
</paper>

		<paper id="1055">
</paper>

		<paper id="1098">
			<definition id="0">
				<sentence>Suppose X is a category 2 in the thesaurus , for any word we X , let Sw be the set of its senses in the dictionary , and Sx = U Sw , for any se Sx , let w~X DW , be the set of the definition words in its definition , DW , = UDW ~ , and DW~ UDW w , s¢S w we X for any word w , let CODE ( w ) be the set of its semantic codes that are given in the thesaurus 3 , CODEs= UCODE ( w ) , CODE~= UCODE , , weD~ seS w and CODEx= U CODE , .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the set of its semantic codes that are given in the thesaurus 3 , CODEs= UCODE ( w )</definiens>
			</definition>
			<definition id="1">
				<sentence>For any category X , let w~X and seSw , if dis ( X , s ) &lt; T , where T is some threshold , we will tag w by s , and assign the semantic code X to s. Now we consider the problem of estimating an appropriate threshold for dis ( X , s ) to distinguish between the senses of the words in X. To do so , we first extract the words which hold only one code in the thesaurus , and have only one sense in the dictionary T , then check the distances between these senses and categories. The number of such words is 22,028. , This means that the words are regarded as univocal ones by both resources. 602 Tab.1 lists the distribution of the words with respect to the distance in 5 intervals. Intervals \ [ o.o , 0.2 ) Word num. 8,274 Percent ( % ) 37.56 \ [ 0.2 , 0.4 ) 10,655 48.37 \ [ 0.4 , 0.6 ) 339 1.54 \ [ 0.6 , 0.8 ) 1172 5.32 \ [ 0.8 , 1.0\ ] 1588 7.21 all 22,028 100 Tab. I. The distribution of univocal words with respect to dis ( X , s ) From Tab.l , we can see that for most univocal words , the distance between their senses and categories lies in \ [ 0 , 0.4\ ] . Let Wv be the set of the univocal words we consider here , for any univocal word we Wv , let sw be its unique sense , and Xw be its univocal category , we call DEN &lt; a. a &gt; point density in interval \ [ tj , t2\ ] as 9 ) , where O &lt; tj &lt; t2 &lt; l. 9 ) DEN &lt; a. a &gt; = \ [ { wlw ~ W v , t , &lt; dis ( Xw , s , , ) &lt; t 2 } 1 t 2 t , We define 10 ) as an object function , and take t '' which maximizes DEN , as the threshold .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">s ) to distinguish between the senses of the words in X. To do so</definiens>
			</definition>
			<definition id="2">
				<sentence>603 IRr n cr l IRr l scr ( Rr II where RTe is a set of the sense tags for the entries in E produced by the tagging procedure , and CT~ is a set of the sense tags for the entries in E , which are regarded as correct ones somehow .</sentence>
				<definiendum id="0">RTe</definiendum>
				<definiendum id="1">CT~</definiendum>
				<definiens id="0">a set of the sense tags for the entries in E produced by the tagging procedure , and</definiens>
				<definiens id="1">a set of the sense tags for the entries in E</definiens>
			</definition>
			<definition id="3">
				<sentence>What we concern here is to evaluate the dissimilarity between different categories , including those within one medium category , so we make use of semantic code based vectors to define their dissimilarity , which is motivated by Shuetze 's word frequency based vectors ( Shuetze , 1993 ) .</sentence>
				<definiendum id="0">dissimilarity</definiendum>
				<definiens id="0">to evaluate the dissimilarity between different categories</definiens>
			</definition>
			<definition id="4">
				<sentence>Miller K. J. ( 1990 ) Introduction to WordNet : An On-line Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1110">
			<definition id="0">
				<sentence>A term-list is a list of content words that characterize a consistent text or a concept .</sentence>
				<definiendum id="0">term-list</definiendum>
				<definiens id="0">a list of content words that characterize a consistent text or a concept</definiens>
			</definition>
			<definition id="1">
				<sentence>Our term-list translation method consists of two steps called Dictionary Lookup and Disambiguation .</sentence>
				<definiendum id="0">term-list translation method</definiendum>
			</definition>
			<definition id="2">
				<sentence>The starting point of WORD SPACE is to represent a word with an ndimensional vector whose i-th element is how many times the word wi occurs close to the word .</sentence>
				<definiendum id="0">WORD SPACE</definiendum>
				<definiens id="0">to represent a word with an ndimensional vector whose i-th element is how many times the word wi occurs close to the word</definiens>
			</definition>
			<definition id="3">
				<sentence>Suppose the given term-list consists of bank and river .</sentence>
				<definiendum id="0">Suppose the given term-list</definiendum>
				<definiens id="0">consists of bank and river</definiens>
			</definition>
			<definition id="4">
				<sentence>We prepared two test sets of term-lists : those extracted from the 400 articles from the New York Times mentioned above , and those extracted from 5The tf-idf score of a word w in a text is tfwlog ( N-~ ) , where tfwis the occurrence of w in the text , N is the number of documents in the collection , and Nw is the number of documents containing w. 672 articles in Reuters ( Reuters , 1997 ) , called Test-NYT , and Test-REU , respectively .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">Nw</definiendum>
				<definiens id="0">prepared two test sets of term-lists : those extracted from the 400 articles from the New York Times mentioned above</definiens>
				<definiens id="1">the number of documents containing w. 672 articles in Reuters ( Reuters , 1997 ) , called Test-NYT , and Test-REU , respectively</definiens>
			</definition>
</paper>

		<paper id="2186">
			<definition id="0">
				<sentence>Part of Speech tagging ( POS ) is the problem of assigning each word in a sentence the part of speech that it assumes in that sentence .</sentence>
				<definiendum id="0">Speech tagging</definiendum>
				<definiendum id="1">POS</definiendum>
				<definiens id="0">the problem of assigning each word in a sentence the part of speech that it assumes in that sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>Unlike most of the algorithms tried on this and other disambiguation tasks , SNOW is an online learning algorithm .</sentence>
				<definiendum id="0">SNOW</definiendum>
				<definiens id="0">an online learning algorithm</definiens>
			</definition>
			<definition id="2">
				<sentence>The algorithm predicts 1 ( positive ) iff ~'\ ] ie~4wi &gt; 0 , where wl is the weight on the edge connecting the ith feature to the target node .</sentence>
				<definiendum id="0">wl</definiendum>
				<definiens id="0">the weight on the edge connecting the ith feature to the target node</definiens>
			</definition>
			<definition id="3">
				<sentence>The learning procedure is a mistakedriven algorithm that produces a set of rules .</sentence>
				<definiendum id="0">learning procedure</definiendum>
				<definiens id="0">a mistakedriven algorithm that produces a set of rules</definiens>
			</definition>
			<definition id="4">
				<sentence>The hypothesis of TBL is an ordered list of transformations .</sentence>
				<definiendum id="0">hypothesis of TBL</definiendum>
				<definiens id="0">an ordered list of transformations</definiens>
			</definition>
			<definition id="5">
				<sentence>A transformation is a rule with an antecedent t and a consequent c E C. The antecedent t is a condition on the input sentence .</sentence>
				<definiendum id="0">transformation</definiendum>
				<definiens id="0">a condition on the input sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>TBL uses a manually-tagged corpus for learning the ordered list of transformations .</sentence>
				<definiendum id="0">TBL</definiendum>
				<definiens id="0">uses a manually-tagged corpus for learning the ordered list of transformations</definiens>
			</definition>
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>A MONOID AI is a set M together with a product M × 31 -- + , ll , written ( a , b ) ~+ ab , such that : • This product is associative ; • There is an element 1 E M ( the neutral element ) with la = al = a for all a 6 M. * This paper is an abridged version of Group Theory and Grammatical Description , TR-MLTT-033 , XRCE , April 1998 ; available on the CMP-LG archive at the address : http : //xxx.lanl.gov/abs/cmpIg/9805002 .</sentence>
				<definiendum id="0">MONOID AI</definiendum>
				<definiendum id="1">XRCE</definiendum>
				<definiens id="0">the neutral element ) with la = al = a for all a 6 M. * This paper is an abridged version of Group Theory and Grammatical Description , TR-MLTT-033 ,</definiens>
			</definition>
			<definition id="1">
				<sentence>A GROUP is a monoid in which every element a has an inverse a -1 such that al a = aa -1 -l. A PREORDER on a set is a reflexive and transitive relation on this set .</sentence>
				<definiendum id="0">GROUP</definiendum>
				<definiens id="0">a reflexive and transitive relation on this set</definiens>
			</definition>
			<definition id="2">
				<sentence>The fourth property above says that the set A , / of elements x 6 G such that x -41 is a set which contains along with an element all its conjugates , that is , a NORMAL subset of G. As M is clearly a submonoid of G , it will be called a NORMAL SUBMONOID of G. Conversely , it is easy to show that with any normal submonoid M of G one can associate a preorder compatible with G. Indeed let 's define x-+ y as xy -1 6 M. The relation -- ~ is clearly reflexive and transitive , hence is a preorder .</sentence>
				<definiendum id="0">hence</definiendum>
				<definiens id="0">a set which contains along with an element all its conjugates</definiens>
			</definition>
			<definition id="3">
				<sentence>the NORMAL SUBGROUP CLOSURE NG ( S ) of S in G ) .</sentence>
				<definiendum id="0">NORMAL SUBGROUP CLOSURE NG</definiendum>
			</definition>
			<definition id="4">
				<sentence>In this way , a product on F ( V ) is defined , and it is easily shown that F ( V ) becomes a ( non-commutative ) group , called the FREE GROUP over V ( Hungerford , 1974 ) .</sentence>
				<definiendum id="0">V )</definiendum>
				<definiens id="0">becomes a ( non-commutative ) group , called the FREE GROUP over V</definiens>
			</definition>
			<definition id="5">
				<sentence>If r is a relator , and if ct is an arbitrary element of F ( V ) , then ct , rc~ -1 will be called a QUASI-RELATOR of the group computation structure .</sentence>
				<definiendum id="0">ct</definiendum>
				<definiens id="0">an arbitrary element of F ( V ) , then ct , rc~ -1 will be called a QUASI-RELATOR of the group computation structure</definiens>
			</definition>
			<definition id="6">
				<sentence>A COMPUTATION relative to GCS is a finite sequence c = ( rl ... . , rn ) of quasi-relators .</sentence>
				<definiendum id="0">COMPUTATION relative to GCS</definiendum>
				<definiens id="0">a finite sequence c = ( rl ... . , rn ) of quasi-relators</definiens>
			</definition>
			<definition id="7">
				<sentence>If GCS = ( V , R ) is a group computation structure , and if A is a given subset of F ( V ) , then we will call the pair GCSA = ( GCS , A ) a GROUP COMPUTATION STRUCTURE WITH ACCEPTORS .</sentence>
				<definiendum id="0">R )</definiendum>
				<definiens id="0">a group computation structure , and if A is a given subset of F ( V )</definiens>
			</definition>
			<definition id="8">
				<sentence>A G-grammar is a group computation structure with acceptors over a vocabulary V = Vlog U ~/pho~ consisting of a set of logical forms l/~og and a disjoint set of phonological elements ( in the example , words ) l/~ho , , .</sentence>
				<definiendum id="0">G-grammar</definiendum>
				<definiens id="0">a group computation structure with acceptors over a vocabulary V = Vlog U ~/pho~ consisting of a set of logical forms l/~og and a disjoint set of phonological elements</definiens>
			</definition>
			<definition id="9">
				<sentence>We take A to be the set of elements of F ( V ) which are products of the following form : S lI/n-lWr~_1-1 ... IV1-1 where S is a logical form ( S stands for `` semantics '' ) , and where each II ' ; is a phonological element ( W stands for `` 'word '' ) .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">the set of elements of F ( V ) which are products of the following form : S lI/n-lWr~_1-1 ... IV1-1 where S is a logical form ( S stands for `` semantics '' )</definiens>
				<definiens id="1">a phonological element</definiens>
			</definition>
			<definition id="10">
				<sentence>-- , W ran -= A -1 r ( A ) saw -v A -I s ( A , B ) B -I in -- , E -I i ( E , A ) A -I the -- 7 t ( N ) N -I ever ) ' -- , o ev ( N , X , P\ [ X\ ] ) some -- , c~ sm ( N , X , P\ [ X\ ] ) that-v N -I tt ( N , X , P\ [ X\ ] ) p\ [ x\ ] -I ~-I X N -I P\ [ X\ ] -a ~-1 X N -I p\ [ x\ ] -1 ~-I X Figure 4 : Parsing-oriented rules Suppose now that we move to the right of the -- 7 arrow all elements appearing on the left of it , but for the single phonological element of each relator .</sentence>
				<definiendum id="0">-I s</definiendum>
				<definiens id="0">A , B ) B -I in --</definiens>
			</definition>
			<definition id="11">
				<sentence>By the same reasoning as in the generation case , it is easy to show that any derivation using these rules and leading to the relation PS -- , LF , where PS is a phonological string and LF a logical form , corresponds to a public result LF PS -1 in the G-grammar .</sentence>
				<definiendum id="0">PS</definiendum>
			</definition>
</paper>

		<paper id="2228">
			<definition id="0">
				<sentence>SENSUS ( Knight and Luk , 1994 ) is a largescale ontology designed for machine-translation and was produced by merging the ontological hierarchies of WordNet and LDOCE ( Bruce and Guthrie , 1992 ) .</sentence>
				<definiendum id="0">SENSUS</definiendum>
			</definition>
</paper>

		<paper id="2231">
			<definition id="0">
				<sentence>For instance , Church and Hanks ( 1990 ) calculated SA in terms of mutual information between two words wl and w2 : N * f ( wl , w2 ) I ( wl , w2 ) = log2 ( 1 ) f ( wl ) f ( w2 ) here N is the size of the corpus used in the estimation , f ( Wl , w2 ) is the frequency of the cooccurrence , f ( wl ) and f ( w2 ) that of each word .</sentence>
				<definiendum id="0">w2 )</definiendum>
				<definiendum id="1">f</definiendum>
				<definiens id="0">the size of the corpus used in the estimation</definiens>
				<definiens id="1">the frequency of the cooccurrence</definiens>
			</definition>
			<definition id="1">
				<sentence>The most frequent co-occurrence ( produce , concrete thing ) has a low t-score also reflecting the fact that it may be attributed to chance .</sentence>
				<definiendum id="0">most frequent co-occurrence</definiendum>
				<definiens id="0">a low t-score also reflecting the fact that it may be attributed to chance</definiens>
			</definition>
			<definition id="2">
				<sentence>\ ] ( a ) where Cmfie r stands for the classes that include the modifier word , Part is the particle following the modifier , mc the content word in the modificand phrase , and f the frequency .</sentence>
				<definiendum id="0">Part</definiendum>
				<definiens id="0">the particle following the modifier , mc the content word in the modificand phrase , and f the frequency</definiens>
			</definition>
			<definition id="3">
				<sentence>Determination of Most Strongly Associated Structure After calculating SA for each possible construction , we choose the construction with highest SA score as the most probable strucpm-ticle-modificand triplets in a corpus that includes 220,000 parsed Japanese sentences .</sentence>
				<definiendum id="0">Determination of Most Strongly Associated Structure After calculating SA</definiendum>
				<definiens id="0">a corpus that includes 220,000 parsed Japanese sentences</definiens>
			</definition>
			<definition id="4">
				<sentence>The training data consists of all 568,000 modifier-particle-modificand triplets in COD .</sentence>
				<definiendum id="0">data</definiendum>
				<definiens id="0">consists of all 568,000 modifier-particle-modificand triplets in COD</definiens>
			</definition>
			<definition id="5">
				<sentence>Here SA is measured by : S A ( v_attachlv , p , n2 ) = log2 \ -\ ] - ( C~ ~',2 ) ) ( 5 ) SA ( n_attachln , ,p , n , ) -log , \ 7- ( C-~ , ~-C , -- ~2 ) \ ] ( 6 ) where Cw stands for the class that includes the word w and f is the frequency in a training data containing verb-nounl-preposition-noun2 constructions .</sentence>
				<definiendum id="0">SA</definiendum>
				<definiendum id="1">f</definiendum>
				<definiens id="0">measured by : S A ( v_attachlv , p</definiens>
				<definiens id="1">the class that includes the word w</definiens>
				<definiens id="2">the frequency in a training data containing verb-nounl-preposition-noun2 constructions</definiens>
			</definition>
</paper>

		<paper id="2153">
			<definition id="0">
				<sentence>cw ( dA , tk , p , to ) a ( dA , ~k , p , t~ ) X ~ ( tk , t~ ) X 7 ( tk , /c ) X C M ( dA ) where c~ ( da , tk , p , to ) is a function expressing how near tkand t~ occur , p denotes that pth tk 's occurrence in the segment dA , and fl ( tk , t¢ ) is a normalized frequency of co-occurrence of ¢~ and ¢~ .</sentence>
				<definiendum id="0">tk , t¢ )</definiendum>
				<definiens id="0">dA , tk , p , to ) a ( dA , ~k , p</definiens>
				<definiens id="1">c~ ( da , tk , p , to ) is a function expressing how near tkand t~ occur , p denotes that pth tk 's occurrence in the segment dA , and fl</definiens>
			</definition>
			<definition id="1">
				<sentence>7 ( t~ , to ) is an inverse document frequency ( in this case `` inverse segment frequency '' ) of te which co-occurs with tk , and defined as follows .</sentence>
				<definiendum id="0">inverse document frequency</definiendum>
			</definition>
			<definition id="2">
				<sentence>N 7 ( tk , fc ) = lOg ( d-~c ) ) where N is a number of segments in a manual , and dr ( to ) is a number segments in which tc occurs with tk .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a number of segments in a manual , and dr ( to ) is a number segments in which tc occurs with tk</definiens>
			</definition>
			<definition id="3">
				<sentence>C is a weight parameter for cw .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a weight parameter for cw</definiens>
			</definition>
			<definition id="4">
				<sentence>In the definition of tf ' , Tc ( tk , dA , dB ) is a set of word which occur in both of dA and dB .</sentence>
				<definiendum id="0">dB )</definiendum>
				<definiens id="0">a set of word which occur in both of dA and dB</definiens>
			</definition>
			<definition id="5">
				<sentence>Input is an electronic manual text which can be written in plain text , I~TEXor HTML ) Output is a hypertext in HTML format .</sentence>
				<definiendum id="0">Input</definiendum>
				<definiens id="0">an electronic manual text which can be written in plain text , I~TEXor HTML ) Output is a hypertext in HTML format</definiens>
			</definition>
</paper>

		<paper id="2204">
			<definition id="0">
				<sentence>Cb ( Ui ) = Cb ( Ui-1 ) Cb ( Ui ) OR no Cb ( Ui-1 ) Cb ( Vi-1 ) CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT Table 1 : Transition Types Brennan et al. ( 1987 ) modify the second of two rules on center movement and realization which were defined by Grosz et al. ( 1983 ; 1995 ) : Rule 1 : If some element of Cf ( Ui-1 ) is realized as a pronoun in Ui , then so is Cb ( Ui ) .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiens id="0">Ui ) = Cb ( Ui-1 ) Cb ( Ui ) OR no Cb ( Ui-1 ) Cb ( Vi-1 ) CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT Table 1 : Transition Types Brennan et al. ( 1987 ) modify the second of two rules on center movement and realization which were defined by Grosz et al. ( 1983 ; 1995 ) : Rule 1 : If some element of Cf ( Ui-1 ) is realized as a pronoun in Ui , then so is Cb ( Ui )</definiens>
			</definition>
			<definition id="1">
				<sentence>OLD consists of evoked ( E ) and unused ( U ) discourse entities while NEW consists of brand-new ( BN ) discourse entities .</sentence>
				<definiendum id="0">OLD</definiendum>
			</definition>
			<definition id="2">
				<sentence>When the pronoun `` her '' in ( lc ) is encountered , FRIEDMAN is the first element of the S-list since FRIEDMAN is unused and in the current utterance .</sentence>
				<definiendum id="0">FRIEDMAN</definiendum>
				<definiens id="0">the first element of the S-list since FRIEDMAN is unused and in the current utterance</definiens>
			</definition>
			<definition id="3">
				<sentence>3The S-list consists of referring expressions which are specified for text position , agreement , sortal information , and information status .</sentence>
				<definiendum id="0">S-list</definiendum>
				<definiens id="0">consists of referring expressions which are specified for text position , agreement , sortal information , and information status</definiens>
			</definition>
			<definition id="4">
				<sentence>1254 ( 3a ) A judge S : \ [ JUDGEBN : judge\ ] ordered that Mr. Curtis S : \ [ CURTISE : Mr. Curtis , JUDGEBN : judge\ ] be released , but e S : \ [ CURTISE : Mr. Curtis , JUDGEE : e\ ] agreed with a request S : \ [ CURTISE : Mr. Curtis , JUDGEE : e , REQUESTBN : request\ ] from prosecutors S : \ [ CURTISE : Mr. Curtis , JUDGEE : e , REQUESTBN : request , PROSECUTORSBN : prosecutors\ ] that he S : \ [ CURTISE : he , JUDGEE : e , REQUESTBN : request , PROSECUTORSBN : prosecutors\ ] be re-examined each year S : \ [ CURTISE : he , JUDGEE : ~ , REQUESTBN : request , PROSECUTORSBN : prosecutors , YEARBN : year\ ] to see if his S : \ [ CURTISE : his , JUDGEE : ~ , REQUESTBN : request , PROSECUTORSBN : prosecutors , YEARBN : year\ ] condition S : \ [ CURTISE : his , JUDGEE : e , CONDITIONBNA : condition , REQUESTBN : request , PROSECUTORSBN : prosec .</sentence>
				<definiendum id="0">JUDGEE</definiendum>
				<definiendum id="1">REQUESTBN</definiendum>
				<definiendum id="2">REQUESTBN</definiendum>
				<definiendum id="3">YEARBN</definiendum>
				<definiendum id="4">YEARBN</definiendum>
				<definiens id="0">judge\ ] be released , but e S : \ [ CURTISE : Mr. Curtis , JUDGEE : e\ ] agreed with a request S : \ [ CURTISE : Mr. Curtis ,</definiens>
				<definiens id="1">prosecutors\ ] that he S : \ [ CURTISE : he , JUDGEE : e ,</definiens>
			</definition>
			<definition id="5">
				<sentence>Following Walker ( 1989 ) , a segment is defined as a paragraph unless its first sentence has a pronoun in subject position or a pronoun where none of the preceding sentence-internal noun phrases matches its syntactic features .</sentence>
				<definiendum id="0">segment</definiendum>
				<definiens id="0">a paragraph unless its first sentence has a pronoun in subject position or a pronoun where none of the preceding sentence-internal noun phrases matches its syntactic features</definiens>
			</definition>
			<definition id="6">
				<sentence>Its incremental character seems to be an answer to the question Kehler ( 1997 ) recently raised .</sentence>
				<definiendum id="0">incremental character</definiendum>
			</definition>
</paper>

		<paper id="2126">
			<definition id="0">
				<sentence>The Natural Language Understanding Engine Test Environment ( ETE ) is a GUI software tool that aids in the development and maintenance of large , modular , natural language understanding ( NLU ) systems .</sentence>
				<definiendum id="0">Natural Language Understanding Engine Test Environment ( ETE )</definiendum>
				<definiens id="0">a GUI software tool that aids in the development and maintenance of large , modular , natural language understanding ( NLU ) systems</definiens>
			</definition>
			<definition id="1">
				<sentence>The ETE addresses these problems by : sentences or messages , whether spoken or written .</sentence>
				<definiendum id="0">ETE</definiendum>
				<definiens id="0">addresses these problems by : sentences or messages , whether spoken or written</definiens>
			</definition>
			<definition id="2">
				<sentence>ETE is designed for the Unisys natural language engine ( NLE ) , a NL system implemented in Quintus Prolog .</sentence>
				<definiendum id="0">ETE</definiendum>
				<definiens id="0">designed for the Unisys natural language engine ( NLE ) , a NL system implemented in Quintus Prolog</definiens>
			</definition>
			<definition id="3">
				<sentence>ETE is comprised of two components : a common relational database that houses the test results and a GUI program that manages and displays the test resources and results .</sentence>
				<definiendum id="0">ETE</definiendum>
				<definiens id="0">a common relational database that houses the test results and a GUI program that manages and displays the test resources and results</definiens>
			</definition>
			<definition id="4">
				<sentence>ETE employs different algorithms to compute the difference between different types of data and display the disparate regions graphically The comparison routines are implemented in Prolog except for trees .</sentence>
				<definiendum id="0">ETE</definiendum>
				<definiens id="0">employs different algorithms to compute the difference between different types of data</definiens>
			</definition>
			<definition id="5">
				<sentence>Let U ( G ) denote the variable substitution of a graph G and diff ( Gx , Gy ) the set of different terms between Gx and Gy , then diff ( Gx , Gy ) = Gx U ( Gy ) and diff ( Gy , Gx ) = Gy U ( Gx ) , where ( - ) is the Prolog set difference operation .</sentence>
				<definiendum id="0">U ( G ) denote</definiendum>
				<definiens id="0">the variable substitution of a graph G and diff ( Gx , Gy ) the set of different terms between Gx and Gy</definiens>
			</definition>
			<definition id="6">
				<sentence>ETE stores all types of intermediate data as strings in the database and provides regular-expression based text search for various data .</sentence>
				<definiendum id="0">ETE</definiendum>
				<definiens id="0">stores all types of intermediate data as strings in the database and provides regular-expression based text search for various data</definiens>
			</definition>
			<definition id="7">
				<sentence>A unique feature of ETE is in-report query , which enables query options on various reports to allow an analyst to quickly zoom in to interesting data based on the diagnostic information .</sentence>
				<definiendum id="0">in-report query</definiendum>
				<definiens id="0">analyst to quickly zoom in to interesting data based on the diagnostic information</definiens>
			</definition>
			<definition id="8">
				<sentence>The tests show that ETE is capable of dealing with large sets of test items ( at an average of 1,000 records per test ) in a network environment with fast database access responses .</sentence>
				<definiendum id="0">ETE</definiendum>
				<definiens id="0">capable of dealing with large sets of test items ( at an average of 1,000 records per test</definiens>
			</definition>
			<definition id="9">
				<sentence>ETE assists analysts to identify problems and debug the system on large data sets .</sentence>
				<definiendum id="0">ETE assists</definiendum>
			</definition>
</paper>

		<paper id="2241">
</paper>

		<paper id="2226">
			<definition id="0">
				<sentence>In our ( rather simple ) example , the lexical constraints associated with the idiom ( 7 ) state that the head is a transitive lexeme whose direct object has the fixed form `` POSS pipe '' , where POSS stands for a possessive determiner coreferential with the external argument of the head ( i.e. the subject ) .</sentence>
				<definiendum id="0">POSS</definiendum>
			</definition>
			<definition id="1">
				<sentence>In automatic mode , the idiom reading takes precedence over the literal interpretation s .</sentence>
				<definiendum id="0">idiom reading</definiendum>
			</definition>
</paper>

		<paper id="2194">
			<definition id="0">
				<sentence>SDRT -an extension ofDRT ( Kamp and Reyle , 1993 ) -describes a complex propositional structure of Discourse Representation Structures ( DRSs ) connected via discourse relations .</sentence>
				<definiendum id="0">SDRT -an extension ofDRT</definiendum>
				<definiens id="0">-describes a complex propositional structure of Discourse Representation Structures ( DRSs ) connected via discourse relations</definiens>
			</definition>
			<definition id="1">
				<sentence>Formally , an SDRS is recursively defined as a pair of sets containing labelled DRSs or SDRSs , and the discourse relations holding between them .</sentence>
				<definiendum id="0">SDRS</definiendum>
			</definition>
			<definition id="2">
				<sentence>The tuple &lt; U , Con ) is an SDRS if ( a ) U is a labelled DRS and Con = O or ( b ) U = { K1 ... , Kn } and Con is a set of SDRS conditions .</sentence>
				<definiendum id="0">Con )</definiendum>
				<definiendum id="1">Con</definiendum>
				<definiens id="0">a labelled DRS and Con = O or ( b ) U = { K1 ... , Kn } and</definiens>
			</definition>
			<definition id="3">
				<sentence>An SDRS condition is a discourse relation suchas D ( K1 , ... , Kn ) , where D 6 R. For the basic case ( i.e. ( K , 0 ) ) K labels a DRS representing the semantic context of a sentence .</sentence>
				<definiendum id="0">SDRS condition</definiendum>
				<definiendum id="1">K1 , ... , Kn</definiendum>
				<definiens id="0">a DRS representing the semantic context of a sentence</definiens>
			</definition>
			<definition id="4">
				<sentence>Taking the reader 's world knowledge and Gricean-style pragmatic maxims into account , DICE provides a formal theory of discourse attachment .</sentence>
				<definiendum id="0">DICE</definiendum>
			</definition>
			<definition id="5">
				<sentence>K ) ) logic consisting of variables for the nodes , four binary relations and the logical connectives - % A , V. 5 Definition 2 ( TDG ) A Tree Description Grammar ( TDG ) is a tuple G = ( N , T , &lt; 1 , &lt; * , - .</sentence>
				<definiendum id="0">TDG</definiendum>
				<definiendum id="1">Tree Description Grammar</definiendum>
				<definiendum id="2">TDG )</definiendum>
				<definiens id="0">a tuple G =</definiens>
			</definition>
			<definition id="6">
				<sentence>( f ) S is the start description .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">the start description</definiens>
			</definition>
			<definition id="7">
				<sentence>~ K2 , where KI , K2 eL ( a ) sentential : sl : drs , where Sl 6 L , drs 6 S ( b ) segmental : K1 : P ( sl , ... , Sn ) , where P is an n-place discourse relation in R , and gl , Sl , ... , Sn 6 L Generally speaking , a discourse relation P provides the link between DRSs or SDRSs .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">an n-place discourse relation in R , and gl , Sl , ... , Sn 6 L Generally speaking , a discourse relation P provides the link between DRSs or SDRSs</definiens>
			</definition>
</paper>

		<paper id="1083">
			<definition id="0">
				<sentence>, t-C , ~b , ~ L &lt; I/ , l.t~. ~ , ~- , ~I.~R~I~'~ , ~.~1~. , ~ , .~l~ ; l~\ ] f'tit , lg'~ , $ 1 '' tf~ , t~l , .V , ¢IL ~\ [ \ ] glllql~\ ] . e~i~\ ] , n o n , k~. , .X , ~J.¢~ non , `` , ~ , ~. \ [ , \ [ . \ [ , ~ , l , `` , ' , ~ , , , I , .I , \ ] , J A ( 0 ) , B ( ; ~4 ) , C ( &gt; 5 ) 7 0 , 1 8 0 , 1 Table 4 : Values for Each Feature Type ¢3 .</sentence>
				<definiendum id="0">] , J A</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">Values for Each Feature Type ¢3</definiens>
			</definition>
			<definition id="1">
				<sentence>Tile class set consists of binary values which delineate whether a sample ( the two bunsetsu ) have a modification relation or not .</sentence>
				<definiendum id="0">Tile class set</definiendum>
				<definiens id="0">binary values which delineate whether a sample ( the two bunsetsu ) have a modification relation or not</definiens>
			</definition>
</paper>

		<paper id="2160">
			<definition id="0">
				<sentence>This paper introduces Boas , a semi-automatic knowledge elicitation system that guides a team of two people through the process of developing the static knowledge sources for a moderate-quality , broad-coverage MT system from any `` low-density '' language into English in about six months .</sentence>
				<definiendum id="0">Boas</definiendum>
				<definiens id="0">a semi-automatic knowledge elicitation system that guides a team of two people through the process of developing the static knowledge sources for a moderate-quality</definiens>
			</definition>
			<definition id="1">
				<sentence>The descriptive language knowledge , which we address in this paper , is , later in the process of Boas operation , converted into operational knowledge capable of supporting the processes of source language analysis and source-target transfer .</sentence>
				<definiendum id="0">descriptive language knowledge</definiendum>
				<definiens id="0">later in the process of Boas operation , converted into operational knowledge capable of supporting the processes of source language analysis and source-target transfer</definiens>
			</definition>
			<definition id="2">
				<sentence>Lexical parameters are viewed as language-independent lexical meanings ( ontological concepts ) , such as TABLEFuRNITUR E. The values of this parameter are the word senses corresponding to this ontological concept across the inventory of languages .</sentence>
				<definiendum id="0">ontological concepts</definiendum>
			</definition>
			<definition id="3">
				<sentence>These resources include a ) the vocabulary of the generation lexicon which can serve as the list of lexical parameters for compiling the bilingual dictionary ; b ) a world model ( ontology ) providing the terms in which the senses of the English words and phrases are expressed ( Boas uses the ontology from the Mikrokosmos project at NMSU CRL-see Mahesh and Nirenburg 1995 ) ; c ) the structure and term definitions from the text meaning representation in Mikrokosmos ( see , for instance , Onyshkevych and Nirenburg 1995 ) , to help guide parameter elicitation ; d ) the set of English closedclass lexical items and morphemes ; e ) English grammar used in text synthesis , which provides the TL side of structural transfer rules in the runtime MT system ( see Figure 1 above ) ; and f ) a set of `` ecological '' parameters and their realizations for English .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">the list of lexical parameters for compiling the bilingual dictionary ; b ) a world model ( ontology ) providing the terms in which the senses of the English words and phrases</definiens>
				<definiens id="1">provides the TL side of structural transfer rules in the runtime MT system ( see Figure 1 above )</definiens>
			</definition>
</paper>

		<paper id="2140">
			<definition id="0">
				<sentence>The maximum entropy model is a model which fits a set of pre-defined constraints and assumes maximum ignorance about everything which is not subject to its constraints thus assigning such cases with the most uniform distribution .</sentence>
				<definiendum id="0">maximum entropy model</definiendum>
				<definiens id="0">a model which fits a set of pre-defined constraints</definiens>
			</definition>
			<definition id="1">
				<sentence>Formally , the feature collocation lattice is a 3-ple : ( 0 , C_ , ~w ) where 0 is a set of nodes of the lattice which corresponds to the union of the feature space of the maximum entropy model and the configuration space : 0 = XU~ ( w ) .</sentence>
				<definiendum id="0">feature collocation lattice</definiendum>
				<definiens id="0">a 3-ple : ( 0 , C_ , ~w ) where 0 is a set of nodes of the lattice which corresponds to the union of the feature space of the maximum entropy model and the configuration space : 0 = XU~ ( w )</definiens>
			</definition>
			<definition id="2">
				<sentence>The feature frequency of a node will then be ~X ( 0k ) = ~0~e0 f0k ( 0i ) * ~0~ which is the sum of all the configuration frequency counts ( ~w ) of the descendant nodes .</sentence>
				<definiendum id="0">feature frequency of a node</definiendum>
				<definiens id="0">the sum of all the configuration frequency counts ( ~w ) of the descendant nodes</definiens>
			</definition>
			<definition id="3">
				<sentence>So the probabilities for the nodes will be : I % ~ % ¢t.u./ ¢iw =/ ( A ) / ( ec ) = p ' ( B ) / ( ABC ) =/ ( AB ) / ( C ) -~ N is the total count on the empirical lattice and { ~_~_ .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the total count on the empirical lattice</definiens>
			</definition>
			<definition id="4">
				<sentence>A period can act as the end of a sentence or be a part of an abbreviation , but when an abbreviation is the last word in a sentence , the period denotes the end of a sentence as well .</sentence>
				<definiendum id="0">period</definiendum>
				<definiens id="0">the end of a sentence or be a part of an abbreviation , but when an abbreviation is the last word in a sentence , the</definiens>
			</definition>
</paper>

		<paper id="2193">
			<definition id="0">
				<sentence>A state in VC-mode records the verb form expected by Vii n ( n + 1 ) , the infinite verb form of the last verb encountered ( rn ) , and the verb form expected by the VC verb , if the VC consists of only one verb ( n + 1 ) .</sentence>
				<definiendum id="0">VC-mode</definiendum>
			</definition>
			<definition id="1">
				<sentence>In case the VC consists of only one verb that can be interpreted as finite , the expected verb form is recorded in a new S-mode state .</sentence>
				<definiendum id="0">VC</definiendum>
				<definiens id="0">consists of only one verb that can be interpreted as finite , the expected verb form is recorded in a new S-mode state</definiens>
			</definition>
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>Scope ( appearing in quantifiers and operators ) is represented in an underspecified way by variables ( `` holes '' ) ranging over labels .</sentence>
				<definiendum id="0">Scope (</definiendum>
				<definiens id="0">appearing in quantifiers and operators ) is represented in an underspecified way by variables ( `` holes '' ) ranging over labels</definiens>
			</definition>
			<definition id="1">
				<sentence>( NB : no restrictions are placed on the particles ' relative scope . )</sentence>
				<definiendum id="0">NB</definiendum>
				<definiens id="0">no restrictions are placed on the particles ' relative scope</definiens>
			</definition>
			<definition id="2">
				<sentence>The number of readings the USR encodes equals the number of possible pluggings .</sentence>
				<definiendum id="0">USR encodes</definiendum>
				<definiens id="0">equals the number of possible pluggings</definiens>
			</definition>
			<definition id="3">
				<sentence>In addition to underspecification , we let two other principles guide the semantic construction : lexicalization ( keep as much as possible of the semantics lexicalized ) and compositionality ( a phrase 's interpretation is a function of its subphrases ' interpretations ) .</sentence>
				<definiendum id="0">semantic construction</definiendum>
				<definiendum id="1">compositionality</definiendum>
				<definiendum id="2">interpretation</definiendum>
			</definition>
			<definition id="4">
				<sentence>Trivial composition occurs in grammar rules which are semantically unary branching , i.e. , the semantics of at the most one of the daughter ( right-hand side ) nodes need to influence the interpretation of the mother ( left-hand side ) node .</sentence>
				<definiendum id="0">Trivial composition</definiendum>
				<definiens id="0">occurs in grammar rules which are semantically unary branching , i.e. , the semantics of at the most one of the daughter ( right-hand side ) nodes need to influence the interpretation of the mother ( left-hand side ) node</definiens>
			</definition>
			<definition id="5">
				<sentence>On terminals , the features in this set will normally have the values shown in ( 7 ) , indicating that the category does not contain a hole ( isahole has the value no ) , i.e. , it is a nonscopebearing element , sb-label , the semantic-head based resolution label , is the label of the element of the substructure below it having widest scope .</sentence>
				<definiendum id="0">isahole</definiendum>
				<definiens id="0">the label of the element of the substructure below it having widest scope</definiens>
			</definition>
			<definition id="6">
				<sentence>Thus , in contrast to our USR , VIT is a representation that encodes all the linguistic information of an utterance ; in addition to the USR semantic structure of Sectiom 2 , the Verbmobil Interface Term contains prosodic , syntactic , and discourse related information .</sentence>
				<definiendum id="0">VIT</definiendum>
				<definiens id="0">a representation that encodes all the linguistic information of an utterance ; in addition to the USR semantic structure of Sectiom 2 , the Verbmobil Interface Term contains prosodic , syntactic , and discourse related information</definiens>
			</definition>
			<definition id="7">
				<sentence>Thus , a VIT with one hole only trivially contains the top hole of the utterance ( i.e. , the hole for the sentence mood predicate ; introduced by the main verb ) .</sentence>
				<definiendum id="0">VIT</definiendum>
				<definiens id="0">the hole for the sentence mood predicate ; introduced by the main verb )</definiens>
			</definition>
</paper>

		<paper id="1081">
			<definition id="0">
				<sentence>It uses a number of word and context features rather similar to system M , and trains a Maximum Entropy model that assigns a weighting parameter to each feature-value and combination of features that is relevant to the estimation of the probability P ( tag\ [ features ) .</sentence>
				<definiendum id="0">Maximum Entropy model</definiendum>
				<definiens id="0">assigns a weighting parameter to each feature-value and combination of features that is relevant to the estimation of the probability P ( tag\ [ features )</definiens>
			</definition>
			<definition id="1">
				<sentence>In the basic version ( Tags ) , each case consists of the tags suggested by the component taggers and the correct tag .</sentence>
				<definiendum id="0">Tags )</definiendum>
				<definiens id="0">consists of the tags suggested by the component taggers and the correct tag</definiens>
			</definition>
</paper>

		<paper id="1114">
			<definition id="0">
				<sentence>The collocations are also classified into two classes by a grammatical criterion : one is a class of functional collocations , which work as functional words such as particles ( postpositionals ) or auxiliary verbs , the other is a class of conceptual collocations which work as nouns , verbs , adjectives , adverbs , etc .</sentence>
				<definiendum id="0">functional collocations</definiendum>
				<definiendum id="1">other</definiendum>
			</definition>
			<definition id="1">
				<sentence>The cost for the segmentation candidate is the sum of three partial costs : b-cost , c-cost and d-cost shown below .</sentence>
				<definiendum id="0">segmentation candidate</definiendum>
				<definiens id="0">the sum of three partial costs : b-cost , c-cost and d-cost shown below</definiens>
			</definition>
			<definition id="2">
				<sentence>( 3 ) a dependency cost ( d-cost ) , which has a negative value , is assigned to the strong dependency relationship between conceptual words in the candidate , representing the consistency of concurrence of conceptual words .</sentence>
				<definiendum id="0">d-cost )</definiendum>
				<definiens id="0">has a negative value , is assigned to the strong dependency relationship between conceptual words in the candidate , representing the consistency of concurrence of conceptual words</definiens>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>The root of 7 has the address 7/e where e is the empty string .</sentence>
				<definiendum id="0">e</definiendum>
				<definiens id="0">the empty string</definiens>
			</definition>
			<definition id="1">
				<sentence>Q is the set of states , E is the input alphabet , q0 is the initial state , ( ~ is the transition relation , and F is the set of final states .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">E</definiendum>
				<definiendum id="2">q0</definiendum>
				<definiendum id="3">F</definiendum>
				<definiens id="0">the set of states</definiens>
				<definiens id="1">the input alphabet</definiens>
				<definiens id="2">the initial state</definiens>
				<definiens id="3">the transition relation</definiens>
				<definiens id="4">the set of final states</definiens>
			</definition>
			<definition id="2">
				<sentence>Q = { T\ [ 'l/i\ ] , ±\ [ 'l/i\ ] I'l/i is an address in `` l } ; = { A , IA } ; F = { T\ [ ' ) '/e\ ] } ; and 6 includes the following transitions : ( ±\ [ foot ( 'l ) \ ] , _A. , T\ [ foot ( 'l ) \ ] ) if foot ( 7 ) is to the right of anchor ( 'l ) ( ±\ [ foot ( '/ ) \ ] , +A_ , T\ [ foot ( 'l ) \ ] ) , if foot ( 'l ) is to the left of anchor ( 'l ) { ( T\ [ 'l/i\ ] , e , ±\ [ next ( 'l/i ) \ ] ) I `` l/i is an address in 'l ice } { ( m\ [ 'y/i\ ] , A , T\ [ 'l/i\ ] ) I `` y/i substitution node , label ( 'l/i ) = A , `` l/i to right of anchor ( 7 ) } { ( ±\ [ 7/i\ ] , ~ , T\ [ 7/i\ ] ) I 7/i substitution node , label ( 'l/i ) = A , `` l/i to left of anchor ( 7 ) } { ( ±\ [ 'l/i\ ] , 4 , T\ [ 'l/i\ ] ) I `` l/i adjunct±on node label ( 'I/i ) = A } { ( ±\ [ 'l/i\ ] , e , T\ [ 'l/i\ ] ) \ [ 7/i adjunct±on node } { ( T\ [ 7/i\ ] , ~__+ , T\ [ 'l/i\ ] ) \ [ 7/i adjunct±on node , label ( 'l/i ) = A } In order to recover derivation trees , we also define the partial function a-site ( q , a , q ' ) for ( q , a , q ' ) E ~ which provides information about the site within the elementary tree of actions occurring in the automaton .</sentence>
				<definiendum id="0">] I'l/i</definiendum>
				<definiens id="0">to the left of anchor ( 'l ) { ( T\ [ 'l/i\ ] , e</definiens>
				<definiens id="1">an address in 'l ice } { ( m\ [ 'y/i\ ] , A</definiens>
				<definiens id="2">provides information about the site within the elementary tree of actions occurring in the automaton</definiens>
			</definition>
			<definition id="3">
				<sentence>An item has the form ( T , q , \ [ l , r , l ' , r'\ ] ) where T is a set of elementary tree names , q is a automata state and l , r , l ' , r ' • { 0 , ... , n , } such that either l &lt; _l ' &lt; _r ~ &lt; _rorl &lt; randl ~=r'=- .</sentence>
				<definiendum id="0">T</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">a set of elementary tree names</definiens>
			</definition>
			<definition id="4">
				<sentence>Roughly speaking , an item ( T , q , \ [ l , r , l ' , r\ ] ) is included in I when for every 't • T , anchored by some ak ( where I &lt; k &lt; r and ifl I ~ then k &lt; l ~ or r t &lt; k ) ; q is a state in Qk , such that some elementary subcomputation reaching q from the initial state , qk , of Mk is an initial substring of the elementary computation for 't that reaches the elementary address `` t/i , the subtree rooted at `` t/i spans Wl , r , and if't/i dominates a foot node then that foot node spans Wl , r , , otherwise l ~ = r ~ = - .</sentence>
				<definiendum id="0">q</definiendum>
				<definiendum id="1">Mk</definiendum>
				<definiendum id="2">if't/i</definiendum>
				<definiens id="0">a state in Qk , such that some elementary subcomputation reaching q from the initial state</definiens>
			</definition>
			<definition id="5">
				<sentence>The input is accepted if an item ( T , qs , \ [ O , n , - , -\ ] ) is added to I where T contains some initial tree rooted in the start symbol S and qf • Fk for some k. When adding items to I we use the procedure add ( T , q , \ [ / , r , l ' , r'\ ] ) which is defined such that if there is already an entry ( T ~ , q , \ [ / , r , l ~ , rq/ • I for some T ~ then replace this with the entry ( T U T ' , q , \ [ / , r , l ' , # \ ] ) 6 ; otherwise add the new entry { T , q , \ [ l , r , l ' , r'\ ] ) to I. I is initialized as follows .</sentence>
				<definiendum id="0">otherwise add</definiendum>
				<definiens id="0">added to I where T contains some initial tree rooted in the start symbol</definiens>
				<definiens id="1">the new entry { T , q</definiens>
			</definition>
			<definition id="6">
				<sentence>For each k • { 1 , ... , n } call add ( T , qk , \ [ k1 , k , - , -\ ] ) where T = treesof ( Mk ) and qk is the initial state of the automata Mk .</sentence>
				<definiendum id="0">qk</definiendum>
				<definiens id="0">n } call add ( T , qk , \ [ k1 , k , - , -\ ] ) where T = treesof ( Mk</definiens>
			</definition>
			<definition id="7">
				<sentence>A derivation tree for the input is returned by the call der ( ( T , ql , \ [ 0 , n , - , -\ ] ) , ~- ) where ( T , qs , \ [ O , n , - , -\ ] ) • I such that T contains some initial tree 7 rooted with the start nonterminal S and ql is the final state of some automata Mk , 1 &lt; _ k &lt; _ n. r is a derivation tree containing just one node labelled with name % In general , on a call to der ( ( T , q , \ [ l , r , l ~ , rq ) , T ) we examine I to find a rule that has caused this item to be included in I. There are six rules to consider , corresponding to the five recogniser rules , plus lexical introduction , as follows : 7Derivation trees axe labelled with tree names and edges axe labelled with tree addresses .</sentence>
				<definiendum id="0">derivation tree</definiendum>
				<definiendum id="1">ql</definiendum>
				<definiens id="0">returned by the call der ( ( T , ql , \ [ 0 , n , - , -\ ] ) , ~- ) where ( T , qs , \ [ O , n , -</definiens>
			</definition>
			<definition id="8">
				<sentence>( q~ , _A , ,q ) • 5k for some k , `` y is the label of the root of 7- , ~/ • T ' and foot ( ' ) , ) • a-sites ( q t , A÷ , q ) then make the call der ( ( T ' , q ' , \ [ l , l ' , - , -\ ] ) , r ) .</sentence>
				<definiendum id="0">y</definiendum>
				<definiens id="0">the label of the root of 7- , ~/ • T ' and foot ( ' ) , ) • a-sites ( q t , A÷ , q ) then make the call der ( ( T '</definiens>
			</definition>
			<definition id="9">
				<sentence>I , ql • Fk for some k , ( q~ , A , q ) • 5k , for some k ~ , ~ , is the label of the root of r , `` ) , • T ~ , label ( 'y~/e ) = A from some ~/ ' • T '' and `` I/i • a-sites ( q ' , A , q ) , then let T ' be the derivation tree containing a single node labelled `` /~ , and let T '' be the result of attaching der ( ( T '' , q/ , \ [ l , r , l '' , r '' \ ] ) , ~- ' ) under the root of r with an edge labelled the tree address i. We then complete the derivation tree by calling der ( ( T ' , ql , \ [ In , r 'l , l ' , r'\ ] ) , Tll ) .</sentence>
				<definiendum id="0">let T</definiendum>
				<definiens id="0">the label of the root of r , `` ) , • T ~ , label ( 'y~/e ) = A from some ~/ ' • T '' and `` I/i • a-sites</definiens>
				<definiens id="1">r , l '' , r '' \ ] ) , ~- ' ) under the root of r with an edge labelled the tree address i. We then complete the derivation tree by calling der ( ( T ' , ql</definiens>
			</definition>
</paper>

		<paper id="2201">
			<definition id="0">
				<sentence>Structural ambiguity is one of the most serious problems faced by Natural Language Processing ( NLP ) systems .</sentence>
				<definiendum id="0">Structural ambiguity</definiendum>
			</definition>
			<definition id="1">
				<sentence>WordNet is a semantic net in which each node stands for a set of synonyms ( synset ) , and domination stands for set inclusion ( IS-A links ) .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">a semantic net in which each node stands for a set of synonyms ( synset ) , and domination stands for set inclusion ( IS-A links )</definiens>
			</definition>
			<definition id="2">
				<sentence>Other mistakes come from what are known as reporting and aspectual verbs .</sentence>
				<definiendum id="0">Other mistakes</definiendum>
			</definition>
			<definition id="3">
				<sentence>Introduction to WordNet : An Online Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="2136">
			<definition id="0">
				<sentence>Recognition results are an n-best list ( top n-ranked ) of interpretations .</sentence>
				<definiendum id="0">Recognition results</definiendum>
				<definiendum id="1">n-best list</definiendum>
				<definiens id="0">top n-ranked ) of interpretations</definiens>
			</definition>
			<definition id="1">
				<sentence>Early Confirmation Message Flow After confirmation of the speech , Quickset passes the selected sentence to the parser ( 3 ) and the process of integration follows ( 4 ) .</sentence>
				<definiendum id="0">Confirmation Message Flow After confirmation</definiendum>
				<definiendum id="1">Quickset</definiendum>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>Redundancy is a good thing , at least in a learning process .</sentence>
				<definiendum id="0">Redundancy</definiendum>
				<definiens id="0">a good thing</definiens>
			</definition>
			<definition id="1">
				<sentence>This dictionary , the American Heritage First Dictionary 1 ( AHFD ) , is an example of a good source of knowledge in terms of simplicity , clarity and redundancy .</sentence>
				<definiendum id="0">AHFD</definiendum>
				<definiens id="0">an example of a good source of knowledge in terms of simplicity , clarity and redundancy</definiens>
			</definition>
			<definition id="2">
				<sentence>The CCKG is a merge of all individual CGs from the words in the cluster .</sentence>
				<definiendum id="0">CCKG</definiendum>
				<definiens id="0">a merge of all individual CGs from the words in the cluster</definiens>
			</definition>
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>The extraction tool consists of a set of batch files for use with the Corpus Query Processor ( CQP ) , which is part of the IMS corpus workbench ( cf. Christ 1994a , b ) .</sentence>
				<definiendum id="0">extraction tool</definiendum>
				<definiens id="0">consists of a set of batch files for use with the Corpus Query Processor ( CQP ) , which is part of the IMS corpus workbench ( cf. Christ 1994a , b )</definiens>
			</definition>
			<definition id="1">
				<sentence>lemma : insist CQP Search Definition search_by : lemma POS : verb np : ( y/n ) n np_np : ( y/n ) n np_ap : ( y/n ) n np_p .</sentence>
				<definiendum id="0">lemma</definiendum>
			</definition>
</paper>

		<paper id="1122">
			<definition id="0">
				<sentence>USER SAID : Go to message four eight nine SYSTEM HEARD : Go to message four please umm CODE : ERROR SYSTEM SAID : I heard goto new message 4 .</sentence>
				<definiendum id="0">USER SAID</definiendum>
				<definiens id="0">ERROR SYSTEM SAID : I heard goto new message 4</definiens>
			</definition>
			<definition id="1">
				<sentence>First we perform an automatic forced alignment of the utterance to the verbatim transcription text using the OGI CSLU CSLUsh Toolkit ( Colton , 1995 ) .</sentence>
				<definiendum id="0">OGI CSLU CSLUsh Toolkit</definiendum>
				<definiens id="0">an automatic forced alignment of the utterance to the verbatim transcription text using the</definiens>
			</definition>
</paper>

		<paper id="1102">
			<definition id="0">
				<sentence>Chart parsing involves population of a triangular matrix of well-formed constituents : chart ( i , j ) , where i and j are numbered vertices delimiting the start and end of the string .</sentence>
				<definiendum id="0">Chart parsing</definiendum>
				<definiens id="0">involves population of a triangular matrix of well-formed constituents : chart ( i</definiens>
			</definition>
			<definition id="1">
				<sentence>multichart ( X ) = U multichart ( Y ) * multichart ( Z ) where X = Y uz , Y nZ = O , Y ~ 0,2 ~ In place of numerical spans within a single dimension ( e.g. chart ( 3,5 ) ) , edges in the multidimensional chart are identified by sets ( e.g. multichart ( { \ [ s , 4 , 2\ ] , \ [ g , 6 , 1\ ] } ) ) containing the identifiers ( IDs ) of the terminal input elements they contain .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">edges in the multidimensional chart are identified by sets</definiens>
			</definition>
			<definition id="2">
				<sentence>The multichart statement enumerates all the possible combinations that need to be considered given a set of inputs whose IDs are contained in a set X. The multidimensional parsing algorithm ( Figure 4 ) runs bottom-up from the input elements , building progressively larger constituents in accordance with the ruleset .</sentence>
				<definiendum id="0">multichart statement</definiendum>
				<definiens id="0">enumerates all the possible combinations that need to be considered given a set of inputs whose IDs are contained in a set X. The multidimensional parsing algorithm</definiens>
			</definition>
			<definition id="3">
				<sentence>A timeout feature indicates the shelflife of an edge within the chart .</sentence>
				<definiendum id="0">timeout feature</definiendum>
				<definiens id="0">indicates the shelflife of an edge within the chart</definiens>
			</definition>
			<definition id="4">
				<sentence>Spoken phrases and pen gestures , which are the terminal elements of the multimodal parsing process , are referred to as lexical edges .</sentence>
				<definiendum id="0">pen gestures</definiendum>
				<definiens id="0">the terminal elements of the multimodal parsing process</definiens>
			</definition>
			<definition id="5">
				<sentence>The next stage is the formulation of declarative integration rules ( phrase structure rules ) , then comes a shift from rules to representations ( lexicalism , categorial and unification-based grammars ) .</sentence>
				<definiendum id="0">phrase structure rules</definiendum>
				<definiens id="0">a shift from rules to representations ( lexicalism , categorial and unification-based grammars )</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>67 want to decide whether the candidate sequence DT ADJ ADJ NN NNP is a noun phrase ( NP ) by comparing it to the training corpus .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">a noun phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>The input to the Memory-Based Sequence Learning ( MBSL ) algorithm is a sentence represented as a sequence of POS tags , and its output is a bracketed sentence , indicating which subsequences of the sentence are to be considered instances of the target pattern ( target instances ) .</sentence>
				<definiendum id="0">Memory-Based Sequence Learning ( MBSL</definiendum>
			</definition>
			<definition id="2">
				<sentence>The idea of the MBSL scoring algorithm is to construct a tiling of subsequences of a situated candidate which covers the entire candidate .</sentence>
				<definiendum id="0">MBSL scoring algorithm</definiendum>
				<definiens id="0">to construct a tiling of subsequences of a situated candidate which covers the entire candidate</definiens>
			</definition>
			<definition id="3">
				<sentence>The score of the candidate is a linear function of its statistics : f ( c ) = anum ( c ) 13minsize ( c ) + 3 ' maxcontext ( c ) + maxoverlap ( c ) If candidate c has no covers , we set f ( c ) = O. Note that minsize is weighted negatively , since a cover with fewer tiles provides stronger evidence for the candidate .</sentence>
				<definiendum id="0">score of the candidate</definiendum>
				<definiens id="0">a linear function of its statistics : f ( c ) = anum ( c ) 13minsize ( c ) + 3 ' maxcontext ( c ) + maxoverlap ( c ) If candidate c has no covers</definiens>
			</definition>
			<definition id="4">
				<sentence>The cover graph is a directed acyclic graph ( DAG ) whose nodes represent matching tiles of the candidate , such that an arc exists between nodes n and n ' , if tile n connects to n ' .</sentence>
				<definiendum id="0">cover graph</definiendum>
				<definiens id="0">a directed acyclic graph ( DAG ) whose nodes represent matching tiles of the candidate , such that an arc exists between nodes n and n ' , if tile n connects to n '</definiens>
			</definition>
			<definition id="5">
				<sentence>A special start node is added as the root of the DAG , that connects to all of the nodes ( tiles ) that contain an open bracket .</sentence>
				<definiendum id="0">special start node</definiendum>
				<definiens id="0">the root of the DAG , that connects to all of the nodes ( tiles ) that contain an open bracket</definiens>
			</definition>
			<definition id="6">
				<sentence>The MBSL scoring algorithm searches the training corpus for each subsequence of the sentence in order to find matching tiles .</sentence>
				<definiendum id="0">MBSL scoring algorithm</definiendum>
				<definiens id="0">searches the training corpus for each subsequence of the sentence in order to find matching tiles</definiens>
			</definition>
			<definition id="7">
				<sentence>=2 0 Examples Figure 3 : Learning curves for NP , VO , and SV by number of SV , 8~0.6 Con .</sentence>
				<definiendum id="0">SV</definiendum>
				<definiens id="0">Learning curves for NP , VO , and</definiens>
			</definition>
</paper>

		<paper id="1117">
			<definition id="0">
				<sentence>The overall ranking for all systems ( excluding the TECH corpus results ) is given in Figure 2 , in terms of the Sent and Char F-measures .</sentence>
				<definiendum id="0">overall ranking for all systems</definiendum>
				<definiens id="0">excluding the TECH corpus results</definiens>
			</definition>
</paper>

		<paper id="2146">
			<definition id="0">
				<sentence>Vocabulary learning means learning the words and their limitations , probability of occurrences , and syntactic behavior around them , Swartz &amp; Yazdani ( 1992 ) .</sentence>
				<definiendum id="0">Vocabulary learning</definiendum>
				<definiens id="0">means learning the words and their limitations , probability of occurrences , and syntactic behavior around them</definiens>
			</definition>
			<definition id="1">
				<sentence>( 1995 ) Intelligent Language Tutors , Theory Shaping Technology , Lawrence Erlbaum Associates , Mahwah , N.J. , 384 p. Hutchins , W.J. &amp; Somers , H.L. ( 1992 ) An Introduction to Machine Translation , Academic Press , San Diego , CA , 361 p. Ingraham , B. , Chanier T. &amp; Emery , C. ( 1994 ) CAMILLE : A European Project to Develop Language Training for Different Purposes , in Various Languages on a Common Hypermedia Framework , Computers and Education , 23/1 &amp; 2 , pp.107-115 .</sentence>
				<definiendum id="0">CAMILLE</definiendum>
				<definiendum id="1">Computers</definiendum>
				<definiens id="0">A European Project to Develop Language Training for Different Purposes , in Various Languages on a Common Hypermedia Framework ,</definiens>
			</definition>
</paper>

		<paper id="2180">
			<definition id="0">
				<sentence>The MindNet similarity procedure is based on the top-ranked ( by weight ) semrel paths between words .</sentence>
				<definiendum id="0">MindNet similarity procedure</definiendum>
			</definition>
			<definition id="1">
				<sentence>Several experiments were performed in which word pairs from a thesaurus and an anti-thesaurus ( the latter containing dissimilar words ) were used in a training phase to identify semrel path patterns that indicate similarity .</sentence>
				<definiendum id="0">anti-thesaurus</definiendum>
				<definiens id="0">the latter containing dissimilar words ) were used in a training phase to identify semrel path patterns that indicate similarity</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>Equivalent adverbials and prepositions English Swedish after all n~ir allt kommer , omkring trots in spite of in general i allm~inhet The problem we consider is how to find word and phrase alignments for a bitext that is already aligned at the sentence level .</sentence>
				<definiendum id="0">phrase</definiendum>
				<definiens id="0">how to find word and</definiens>
			</definition>
			<definition id="1">
				<sentence>A pair ( X , Z ) is considered to be a translation pair iff there exist strings W , F and G such that Y -- i~rF , Z = IY/G and F and G have been defined as different suffices of the same paradigm .</sentence>
				<definiendum id="0">pair ( X , Z )</definiendum>
				<definiens id="0">considered to be a translation pair iff there exist strings W , F and G such that Y -- i~rF , Z = IY/G and F and G have been defined as different suffices of the same paradigm</definiens>
			</definition>
			<definition id="2">
				<sentence>It can happen that the t-scores for two pairs &lt; s , tl &gt; and &lt; s , t ; &gt; , where t I is a multi-word expression and P is a word that is part of t 1 , will be identical or almost identical .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the t-scores for two pairs &lt; s , tl &gt; and &lt; s , t ; &gt; , where t I is a multi-word expression</definiens>
				<definiens id="1">a word that is part of t 1</definiens>
			</definition>
			<definition id="3">
				<sentence>Precision is increased , but perhaps not only ( B ) , all modules except the weights ( AMLinked source expressiones Linked multi-word expr .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">increased , but perhaps not only ( B ) , all modules except the weights ( AMLinked source expressiones Linked multi-word expr</definiens>
			</definition>
</paper>

		<paper id="2227">
			<definition id="0">
				<sentence>hd_ph : = &lt; hd_ph k @ 'HFP ' k synsem ! loc ! cat ! val ! comps ! Q. hd_nexus_ph : = &lt; hd nexus_ph k @ hd ph k @ 'SemP'. hdsubj_ph : = &lt; hd_subj_ph k @ hd_nexus_ph k @ 'VALP ' ( spr ) k @ 'VALP ' ( comps ) synsem ! loc ! cat ! val ! subj ! \ [ \ ] . hd_comp_ph : = &lt; hd_comp_ph k @ hd_nexus_ph k @ 'VALP ' ( subj ) &amp; @ 'VALP ' ( spr ) . @ hd_subj_phk phon ! PO-PN hd_dtr ! ( Head k synsem ! loc ! ca~ ! val ! subj ! \ [ S\ ] ) k subj_dtr ! ( Subj k synsem ! S ) -- - &gt; \ [ Head &amp; &lt; phrase k phon ! PI-PN , Subj k &lt; phrase k phon ! P0-Pl\ ] . @ hd_comp_phk phon ! P0-PN k hd_dtr ! ( Head &amp; synsem ! loc ! cat ! val ! comps ! \ [ C\ ] ) k comp_dtrs ! \ [ Comp k synsem ! C\ ] -- - &gt; \ [ Head &amp; &lt; word a phon ! P0-Pl , Comp a &lt; phrase k phon ! PI-PN\ ] . Figure 1 : Principles , Phrase Types , Schemata a clear definition of semantic head : in head-adjunct phrases , the adjunct daughter is the semantic head ; in other headed phrases , the syntactic head daughter is the semantic head. In both cases , the Semantics Principle basically requires the content of the semantic head to be identical to the content of the mother. If we ignore coordinate structures , and if we equate logical form with semantic content for now , then all HPSG grammar rules are SHD chain rules , meeting the requirement of the BUG algorithm. ProFIT : Prolog with Features , Inheritance and Templates ( Erbach , 1995 ) is an extension of Prolog which supports inheritance-based typed feature structures. The type hierarchy is declared in a signature , which defines subtypes and appropriate features of every type. Terms with typed feature structures can then be used alongside normal terms. Using the signature declarations , the ProFIT system compiles the typed feature structures into normal Prolog terms , which can be compiled by the Prolog system. Figure 1 shows some implementation details. We use ProFIT templates ( defined by ' : = ' ) for principies such as the Head Feature Principle ( 'HFP ' ) and Semantics Principle ( 'SemP ' ) . Templates are expanded where they are invoked ( by @ 'HFP ' or @ 'SemP ' ) . The type hierarchy includes the phrase type hierarchy of Sag ( 1997 ) . As ProFIT does not support dynamic constraints , we use templates to specify phrasal constraints. For example , for headnexus phrases , the hd__nexus_ph template specifies the &lt; hd_nexus_ph type , invokes general constraints on headed phrases ( such as HFP ) by @ hd_ph , and invokes the Semantics Principle by @ 'SetuP'. Immediate dominance schemata are implemented as PSG rules , using schematic categories word and phrase , not traditional categories ( NP , VP etc ) . To simplify the generator , the semantic head is first in the list of daughters. Linear precedence is specified by the PHON strings , implemented as Prolog difference lists. Example rules for Head-Subject and Head-Complements Schemata are shown in Figure 1. van Noord ( 1990 ) implements the BUG algorithm as BUGI in Prolog. For HPSG , we add the ProFIT interface in Figure 2. Templates identify the head features ( HF ) and logical form ( LF ) , and keep the algorithm independent from HPSG internal details. Note that link , used by van Noord ( 1990 ) to improve the efficiency of the algorithm , is replaced by the HPSG Head Feature Principle. hf ( HF ) : = synsem ! loc ! cat ! head ! HF. If ( LF ) : = synsem ! loc ! cont ! LF. predict_word ( @ If ( LF ) k @ hf ( HF ) , Word ) : lex ( Word t @ If ( LF ) k @ hf ( HF ) ) . predict_rule ( Head , Mother , Others , @ hf ( HF ) ) : ( Mother k @ hf ( HF ) -- - &gt; \ [ HeadJOthers\ ] ) .</sentence>
				<definiendum id="0">adjunct daughter</definiendum>
				<definiendum id="1">ProFIT system</definiendum>
				<definiens id="0">the content of the semantic head to be identical to the content of the mother. If we ignore coordinate structures , and if we equate logical form with semantic content for now , then all HPSG grammar rules are SHD chain rules , meeting the requirement of the BUG algorithm. ProFIT : Prolog with Features</definiens>
				<definiens id="1">logical form ( LF ) , and keep the algorithm independent from HPSG internal details. Note that link , used by van Noord ( 1990 ) to improve the efficiency of the algorithm , is replaced by the HPSG Head Feature Principle. hf</definiens>
			</definition>
			<definition id="1">
				<sentence>If logical form is restricted to semantic content as in Figure 2 , then V is the semantic head of VP and VP is the semantic head of S , not only in terms of the HPSG definition but also in terms of the BUG algorithm .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">VP</definiendum>
				<definiens id="0">the semantic head of S</definiens>
			</definition>
			<definition id="2">
				<sentence>The Quantifier Inheritance Principle requires a phrase 's QSTORE to be the set union of the QSTOREs of all daughters , minus any quantifiers in the phrase 's RETRIEVED list .</sentence>
				<definiendum id="0">Quantifier Inheritance Principle</definiendum>
				<definiens id="0">requires a phrase 's QSTORE to be the set union of the QSTOREs of all daughters , minus any quantifiers in the phrase 's RETRIEVED list</definiens>
			</definition>
			<definition id="3">
				<sentence>Lexical amalgamation of CONTEXT , proposed by Wilcock ( 1997 ) , follows the same approach .</sentence>
				<definiendum id="0">Lexical amalgamation of CONTEXT</definiendum>
				<definiens id="0">follows the same approach</definiens>
			</definition>
</paper>

		<paper id="1067">
			<definition id="0">
				<sentence>SRV assumes nothing about the structure of a field instance 1 or the text in which it is embedded -- only that an instance is an unbroken fragment of text .</sentence>
				<definiendum id="0">SRV</definiendum>
				<definiens id="0">assumes nothing about the structure of a field instance 1 or the text in which it is embedded</definiens>
			</definition>
			<definition id="1">
				<sentence>a notion of relational features , such as next-token , which map a given token to another token in its environment .</sentence>
				<definiendum id="0">next-token</definiendum>
				<definiens id="0">map a given token to another token in its environment</definiens>
			</definition>
			<definition id="2">
				<sentence>SRV uses such features to explore the context of fragments under investigation .</sentence>
				<definiendum id="0">SRV</definiendum>
				<definiens id="0">uses such features to explore the context of fragments under investigation</definiens>
			</definition>
			<definition id="3">
				<sentence>SRV constructs rules from general to specific , as in FOIL ( Quinlan , 1990 ) .</sentence>
				<definiendum id="0">SRV</definiendum>
				<definiens id="0">constructs rules from general to specific</definiens>
			</definition>
			<definition id="4">
				<sentence>The link grammar parser takes a sentence as input and returns a complete parse in which terms are connected in typed binary relations ( `` links '' ) which represent syntactic relationships ( Sleator and Temperley , 1993 ) .</sentence>
				<definiendum id="0">link grammar parser</definiendum>
				<definiens id="0">takes a sentence as input and returns a complete parse in which terms are connected in typed binary relations ( `` links '' ) which represent syntactic relationships</definiens>
			</definition>
			<definition id="5">
				<sentence>A fragment is a acqabr , if : it contains exactly one token ; the token ( T ) is capitalized ; T is followed by a lower-case token ; T is preceded by a lower-case token ; T has a right AN-link to a token ( U ) with wn_word value `` possession '' ; U is preceded by a token with wn_word value `` stock '' ; and the token two tokens before T is not a two-character token .</sentence>
				<definiendum id="0">fragment</definiendum>
			</definition>
			<definition id="6">
				<sentence>More than similar systems , SRV satisfies the criteria of generality and retargetability .</sentence>
				<definiendum id="0">SRV</definiendum>
				<definiens id="0">satisfies the criteria of generality and retargetability</definiens>
			</definition>
</paper>

		<paper id="2127">
			<definition id="0">
				<sentence>A dependency triple consists of two words and the grammatical relationship between them in the input sentence .</sentence>
				<definiendum id="0">dependency triple</definiendum>
				<definiens id="0">consists of two words and the grammatical relationship between them in the input sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>The description of a word w consists of the frequency counts of all the dependency triples that matches the pattern ( w , .</sentence>
				<definiendum id="0">description of a word w</definiendum>
				<definiens id="0">consists of the frequency counts of all the dependency triples that matches the pattern ( w ,</definiens>
			</definition>
			<definition id="2">
				<sentence>When the value of Ilw , r , w 'll is unknown , we assume that A and C are conditionally independent given B. The probability of A , B and C cooccurring is estimated by PMLE ( B ) PMLE ( A\ [ B ) PMLE ( C\ [ B ) , where PMLE is the maximum likelihood estimation of a probability distribution and P.LE ( B ) = II* , * , *ll ' P. , ~E ( AIB ) = II* , ~ , *ll ' P , LE ( CIB ) = When the value of Hw , r , w~H is known , we can obtain PMLE ( A , B , C ) directly : PMLE ( A , B , C ) = \ [ \ [ w , r , wll/\ [ \ [ * , * , *H Let I ( w , r , w ~ ) denote the amount information contained in Hw , r , w~\ ] \ ] =c. Its value can be corn769 simgindZe ( Wl , W2 ) = ~'~ ( r , w ) eTCwl ) NTCw2 ) Are { subj.of.obj-of } min ( I ( Wl , r , w ) , I ( w2 , r , w ) ) simHindte , ( Wl , W2 ) = ~ , ( r , w ) eT ( w , ) nT ( w2 ) min ( I ( wl , r , w ) , I ( w2 , r , w ) ) \ ] T ( Wl ) NT ( w2 ) I simcosine ( Wl , W2 ) = x/IZ ( w~ ) l×lZ ( w2 ) l 2x IT ( wl ) nZ ( w2 ) l simDice ( Wl , W2 ) = iT ( wl ) l+lT ( w2 ) I simJacard ( Wl , W2 ) = T ( wl ) OT ( w2 ) l T ( wl ) + T ( w2 ) l-IT ( Wl ) rlT ( w2 ) l Figure 1 : Other Similarity Measures puted as follows : I ( w , r , w ' ) = _ Iog ( PMLE ( B ) PMLE ( A\ ] B ) PMLE ( CIB ) ) -- ( -log PMLE ( A , B , C ) ) log IIw , r , wfl×ll* , r , *ll -IIw , r , *ll xll* , r , w 'll It is worth noting that I ( w , r , w ' ) is equal to the mutual information between w and w ' ( Hindle , 1990 ) .</sentence>
				<definiendum id="0">PMLE</definiendum>
				<definiendum id="1">A , B , C ) directly : PMLE ( A , B , C</definiendum>
				<definiens id="0">conditionally independent given B. The probability of A , B and C cooccurring is estimated by PMLE ( B ) PMLE ( A\ [ B ) PMLE ( C\ [ B ) , where</definiens>
				<definiens id="1">the maximum likelihood estimation of a probability distribution and P.LE ( B ) = II* , * , *ll ' P. , ~E ( AIB ) = II*</definiens>
				<definiens id="2">[ * , * , *H Let I ( w , r , w ~ ) denote the amount information contained in Hw , r</definiens>
				<definiens id="3">r , w ) eTCwl ) NTCw2 ) Are { subj.of.obj-of } min ( I ( Wl , r , w ) , I ( w2 , r , w )</definiens>
				<definiens id="4">wl , r , w ) , I ( w2 , r , w ) ) \ ] T ( Wl ) NT ( w2 ) I simcosine ( Wl , W2 ) = x/IZ ( w~ ) l×lZ ( w2 ) l 2x IT ( wl ) nZ ( w2 ) l simDice ( Wl , W2 ) = iT ( wl ) l+lT ( w2 ) I simJacard ( Wl , W2 ) = T ( wl ) OT ( w2 ) l T ( wl ) + T ( w2 ) l-IT ( Wl ) rlT ( w2 ) l Figure 1 : Other Similarity Measures puted as follows : I ( w , r , w ' ) = _ Iog ( PMLE ( B ) PMLE ( A\ ] B ) PMLE ( CIB ) ) -- ( -log PMLE ( A , B , C ) ) log IIw , r , wfl×ll* , r</definiens>
			</definition>
			<definition id="3">
				<sentence>ilarity sim ( wl , w2 ) between two words wl and w2 as follows : ) '' ~ ( r , w ) eT ( w , ) NT ( w~ ) ( I ( Wl , r , w ) + I ( w2 , r , w ) ) ~- , ( r , w ) eT ( wl ) I ( Wl , r , w ) q~ ( r , w ) eT ( w2 ) I ( w2 , r , w ) We parsed a 64-million-word corpus consisting of the Wall Street Journal ( 24 million words ) , San Jose Mercury ( 21 million words ) and AP Newswire ( 19 million words ) .</sentence>
				<definiendum id="0">NT</definiendum>
				<definiendum id="1">w~ )</definiendum>
				<definiendum id="2">w ) eT</definiendum>
				<definiendum id="3">19 million words</definiendum>
				<definiens id="0">r , w ) We parsed a 64-million-word corpus consisting of the Wall Street Journal ( 24 million words ) , San Jose Mercury ( 21 million words</definiens>
			</definition>
			<definition id="4">
				<sentence>• • , WN , 8N where pos is a part of speech , wi is a word , si=sim ( w , wi ) and si 's are ordered in descending 'We used N=200 in our experiments 2The resulting thesaurus is available at : http : //www.cs.umanitoba.caflindek/sims.htm .</sentence>
				<definiendum id="0">pos</definiendum>
				<definiens id="0">a part of speech</definiens>
			</definition>
			<definition id="5">
				<sentence>21R ( ~l ) nR ( w2 ) l simRoget ( Wl , W2 ) = IR ( wx ) l+lR ( w2 ) l where S ( w ) is the set of senses of w in the WordNet , super ( c ) is the set of ( possibly indirect ) superclasses of concept c in the WordNet , R ( w ) is the set of words that belong to a same Roget category as w. Figure 2 : Word similarity measures based on WordNet and Roget make use of the unique dependency triples and ignore their frequency counts .</sentence>
				<definiendum id="0">S ( w )</definiendum>
				<definiendum id="1">c )</definiendum>
				<definiendum id="2">R ( w )</definiendum>
				<definiens id="0">the set of senses of w in the WordNet</definiens>
			</definition>
			<definition id="6">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="7">
				<sentence>WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">An on-line lexical database</definiens>
			</definition>
</paper>

		<paper id="2239">
			<definition id="0">
				<sentence>Distance Perplexity 230 Distance 1 2 575 8 1531 3 966 9 1580 4 1157 10 1599 5 1307 11 1611 6 1410 100 1674 Perplexity 1479 Table 1 : Conditional perplexities of the longdistance WB models for different distances Given a window , we define two events : 1466 w : { w is the next word } w o : { wo occurs somewhere in the window } Considering a particular trigger ( A ~ B ) , we are interested in the correlation between the two events A o and B. A simple way to assess the significance of the correlation between the two events A o and B in the trigger ( A ~ B ) is to measure their cross product ratio ( CPR ) .</sentence>
				<definiendum id="0">B.</definiendum>
				<definiens id="0">the next word } w o : { wo occurs somewhere in the window } Considering a particular trigger</definiens>
			</definition>
			<definition id="1">
				<sentence>One often used measure is the logarithmic measure of that quality , which has units of bits and is defined as : P ( Ao , B ) P ( Ao , B ) log CPR ( Ao , B ) = log ( 2 ) P ( A o , B ) P ( A o , B ) where P ( X o , Y ) is the probability of a word pair ( X , , , Y ) occurring in the window .</sentence>
				<definiendum id="0">B ) P (</definiendum>
				<definiendum id="1">Y )</definiendum>
				<definiens id="0">has units of bits and is defined as : P ( Ao , B ) P ( Ao , B ) log CPR ( Ao , B ) = log ( 2 ) P ( A o ,</definiens>
				<definiens id="1">A o , B ) where P ( X o</definiens>
				<definiens id="2">the probability of a word pair ( X , , , Y ) occurring in the window</definiens>
			</definition>
			<definition id="2">
				<sentence>Therefore , an alternative measure of the expected benefit provided by A o in predicting B is the average mutual information ( AMI ) between the two : P ( AoB ) AMI ( Ao ; B ) = P ( A o , B ) log P ( Ao ) P ( B ) + P ( Ao , -B ) Iog P ( AoB ) P ( Ao ) P ( B ) + P ( A-'~o , B ) log P ( __AoB ) P ( Ao ) P ( B ) P ( A o B ) + P ( A o , B ) log e ( -~oo ) P ( -~ ) ( 3 ) Obviously , Equation 3 takes the joint probability into consideration .</sentence>
				<definiendum id="0">AMI</definiendum>
				<definiens id="0">the average mutual information</definiens>
			</definition>
			<definition id="3">
				<sentence>P ( B ) where P ( X ) is the probability of the word X occurred in the corpus and P ( A , B ) is the probability of the word pair ( A , B ) occurred in the window .</sentence>
				<definiendum id="0">P ( X )</definiendum>
				<definiendum id="1">P</definiendum>
				<definiendum id="2">B )</definiendum>
				<definiens id="0">the probability of the word X occurred in the corpus</definiens>
				<definiens id="1">the probability of the word pair</definiens>
			</definition>
			<definition id="4">
				<sentence>For simplicity , we represent a trigger pair as XX-ws-MI-Trigger , and call a trigger-based model as the XX-ws-MI-Trigger model , while XX represents DI or DD and ws represents the window size .</sentence>
				<definiendum id="0">XX</definiendum>
				<definiens id="0">represents DI or DD and ws represents the window size</definiens>
			</definition>
			<definition id="5">
				<sentence>In this way , we build a DD-6-MITrigger model which includes the best 1M trigger pairs .</sentence>
				<definiendum id="0">DD-6-MITrigger model</definiendum>
				<definiens id="0">includes the best 1M trigger pairs</definiens>
			</definition>
</paper>

		<paper id="2213">
			<definition id="0">
				<sentence>By applying some constraints on the chronological ordering of articles , an efficient threading algorithm that runs in O ( n ) time ( where n is the number of articles ) is obtained .</sentence>
				<definiendum id="0">efficient threading algorithm</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the number of articles</definiens>
			</definition>
			<definition id="1">
				<sentence>Known information consists of words shared by the beginning and ending points of an arc .</sentence>
				<definiendum id="0">Known information</definiendum>
				<definiens id="0">consists of words shared by the beginning and ending points of an arc</definiens>
			</definition>
			<definition id="2">
				<sentence>V is a set of nodes .</sentence>
				<definiendum id="0">V</definiendum>
			</definition>
			<definition id="3">
				<sentence>From the time constraint defined in Section 3 , MG is an upper triangle matrix .</sentence>
				<definiendum id="0">MG</definiendum>
				<definiens id="0">an upper triangle matrix</definiens>
			</definition>
			<definition id="4">
				<sentence>Other parameters are defined as follows : k : constant value Cd , ( kw ) : frequency of word kw in d ( i ) Cd , : number of words in d ( i ) Nk ( kw ) : number of articles that contain the word kw in k articles di-k , ... , di The function differentia ( d { ) returns a set of keywords that appear in dj but do not appear in the last k articles .</sentence>
				<definiendum id="0">Other parameters</definiendum>
				<definiens id="0">k : constant value Cd , ( kw ) : frequency of word kw in d ( i ) Cd , : number of words in d ( i ) Nk ( kw ) : number of articles that contain the word kw in k articles di-k , ... , di The function differentia ( d { ) returns a set of keywords that appear in dj but do not appear in the last k articles</definiens>
			</definition>
			<definition id="5">
				<sentence>The in-degree is the number of arcs leading to a node , while the out-degree is the number of arcs leading from it .</sentence>
				<definiendum id="0">in-degree</definiendum>
				<definiens id="0">the number of arcs leading to a node</definiens>
			</definition>
			<definition id="6">
				<sentence>In the set of articles V shown in Figure 9 , d7 is an index node .</sentence>
				<definiendum id="0">d7</definiendum>
				<definiens id="0">an index node</definiens>
			</definition>
			<definition id="7">
				<sentence>In this paper , an index node denotes the beginning of a new topic .</sentence>
				<definiendum id="0">index node</definiendum>
				<definiens id="0">the beginning of a new topic</definiens>
			</definition>
			<definition id="8">
				<sentence>The shortest path ( dl , d2 , , dT , dl0 ) gives a simple outline of the articles .</sentence>
				<definiendum id="0">dT , dl0 )</definiendum>
			</definition>
			<definition id="9">
				<sentence>Extended Markup Language ( XML ) is a proposed standard ( XML , 1997 ) specified by the World Wide Web Consortium ( W3C ) .</sentence>
				<definiendum id="0">Extended Markup Language</definiendum>
				<definiendum id="1">XML )</definiendum>
			</definition>
			<definition id="10">
				<sentence>An efficient threading algorithm whose complexity is O ( n ) ( where n is the number of articles ) was introduced with some constraints on the chronological ordering of articles .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of articles ) was introduced with some constraints on the chronological ordering of articles</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>From this experiment , we hoped to gauge what the upper bound is on how much we could improve upon state of the art by using very rich models ) For our experiments , we used three different speech recognizers , trained respectively on Switchboard ( spontaneous speech ) , Broadcast News ( recorded news broadcasts ) and Wall Street Journal data .</sentence>
				<definiendum id="0">, Broadcast News</definiendum>
				<definiens id="0">spontaneous speech )</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>The input to our microplanning component consists of semantic representations encoded in a minimal recursive structure following a variant of UDRT .</sentence>
				<definiendum id="0">microplanning component</definiendum>
				<definiens id="0">consists of semantic representations encoded in a minimal recursive structure following a variant of UDRT</definiens>
			</definition>
			<definition id="1">
				<sentence>; ; standard finite verb with 2 complements ( ( WORK_ACCEPTABLE ( L I ) ARG3 ( L 1 12 ) ; ; pattern PERSPECTIVE ( L % I I3 ) ) ( $ not ( $ sem-match NOM ( L I ) ) ) ; ; condition ( WORK_ACCEPTABLE ( CAT V ) ; ; action ( HEAD ( OR SUIT_V1 SUIT_V2 ) ) ( FORM ordinary ) ( TENSE Sget-tense I ) ( VOICE Sget-voice I ) ) ( I2 ( GENDER ( NOT MAS FEM ) ) ) ( REGENT-DEP-FUNC WORK_ACCEPTABLE 12 AGENT ) ( REGENT-DEP-FUNC WORK_ACCEPTABLE 13 PATIENT ) ( KEY KEY-V ) ) ; ; nominalized form . . . Figure 2 : Example Microplanning Content Rule In the condition part of the verbal mapping the existence of a NOM-condition within the semantic input information is tested .</sentence>
				<definiendum id="0">WORK_ACCEPTABLE</definiendum>
				<definiendum id="1">TENSE Sget-tense I )</definiendum>
			</definition>
			<definition id="2">
				<sentence>I2 which stands for the ARG3 of WORK_ACCEPTABLE , defined by a database of linking-information as the semantic agent is characterized as neither allowing gender masc ( uline ) nor fem ( inine ) for preventing `` he suits '' in the sense of `` he is okay '' .</sentence>
				<definiendum id="0">I2 which</definiendum>
				<definiendum id="1">fem</definiendum>
				<definiens id="0">stands for the ARG3 of WORK_ACCEPTABLE</definiens>
			</definition>
			<definition id="3">
				<sentence>The combination phase finds a successful combination of trees to build a ( derived ) phrase structure tree .</sentence>
				<definiendum id="0">combination phase</definiendum>
				<definiens id="0">finds a successful combination of trees to build a ( derived ) phrase structure tree</definiens>
			</definition>
			<definition id="4">
				<sentence>The final inflection phase uses the information in the feature structures of the leaves ( i.e. , the words ) to apply appropriate morphological functions .</sentence>
				<definiendum id="0">final inflection phase</definiendum>
				<definiens id="0">uses the information in the feature structures of the leaves ( i.e. , the words ) to apply appropriate morphological functions</definiens>
			</definition>
</paper>

		<paper id="1076">
			<definition id="0">
				<sentence>Sentence tokenization is the task of mapping sentences from character strings into streams of tokens .</sentence>
				<definiendum id="0">Sentence tokenization</definiendum>
				<definiens id="0">the task of mapping sentences from character strings into streams of tokens</definiens>
			</definition>
			<definition id="1">
				<sentence>The Chinese PH corpus is a collection of about 4 million morphemes of news articles from the single source of China 's Xinhua News Agency in 1990 and 1991 .</sentence>
				<definiendum id="0">Chinese PH corpus</definiendum>
				<definiens id="0">a collection of about 4 million morphemes of news articles from the single source of China 's Xinhua News Agency in 1990 and 1991</definiens>
			</definition>
			<definition id="2">
				<sentence>The Beihang dictionary is a collection of about 50,000 word-like tokens , each of which occurs at least 5 times in a balanced collection of more than 20 million Chinese characters .</sentence>
				<definiendum id="0">Beihang dictionary</definiendum>
				<definiens id="0">occurs at least 5 times in a balanced collection of more than 20 million Chinese characters</definiens>
			</definition>
			<definition id="3">
				<sentence>~ ) /E ( N , ) , where E ( N , ) is the expectation of the number of types whose frequency in a sample is r. What we are looking for here is the quantity of r'E ( N , ) for r=O , or E ( N~ ) , which can be closely approximated by the number of non-dictionary-entry fragments that occurred exactly once in the PH corpus .</sentence>
				<definiendum id="0">E ( N , )</definiendum>
				<definiens id="0">the expectation of the number of types whose frequency in a sample</definiens>
			</definition>
			<definition id="4">
				<sentence>The Notion of Tokens : A stretch of characters is a legitimate token to be put in tokenization dictionary if and only if it does not introduce any violation to the law of one tokenization per source .</sentence>
				<definiendum id="0">stretch of characters</definiendum>
				<definiens id="0">a legitimate token to be put in tokenization dictionary if and only if it does not introduce any violation to the law of one tokenization per source</definiens>
			</definition>
</paper>

		<paper id="2152">
			<definition id="0">
				<sentence>It can be computed from the a priori likelihood estimates for individual characters , n P ( XIC ) = II P ( xilci ) ( 3 ) i=1 where n is the string length .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
			<definition id="1">
				<sentence>The character confusion probabilities are computed from the character confusion matrix , which is a set of the frequencies of the input-output character pairs of the OCR .</sentence>
				<definiendum id="0">character confusion probabilities</definiendum>
				<definiendum id="1">character confusion matrix</definiendum>
				<definiens id="0">a set of the frequencies of the input-output character pairs of the OCR</definiens>
			</definition>
			<definition id="2">
				<sentence>Let C ( ci , cj ) be the frequency of events where ci and cj are the input and the output characters , respectively .</sentence>
				<definiendum id="0">cj</definiendum>
				<definiens id="0">the frequency of events where ci</definiens>
			</definition>
			<definition id="3">
				<sentence>Let ~ ( ci ) be the sum of the probabilities of unseen output characters where the input character is ci .</sentence>
				<definiendum id="0">Let ~ ( ci )</definiendum>
				<definiens id="0">the sum of the probabilities of unseen output characters where the input character is ci</definiens>
			</definition>
			<definition id="4">
				<sentence>It estimates the probability of observing novel events to be r/ ( n + r ) , where n is the total number of events seen previously , and r is the number of symbols that are distinct .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">r</definiendum>
				<definiens id="0">the total number of events seen previously , and</definiens>
			</definition>
			<definition id="5">
				<sentence>We then distribute the probability for unseen events in proportion to the class confusion probability , P ( cj\ [ c~ ) = a ( ci ) P ( class ( cj ) \ [ class ( c~ ) ) ( 7 ) where Z ( c~ ) ~ ( c~ ) = E~ , :c ( ~ , ,~=0 P ( d~ ( cDId~s ( ~ ) ) ( 8 ) is a normalizing constant , and class ( c { ) is the function that returns the class of character c~ .</sentence>
				<definiendum id="0">class ( c { )</definiendum>
				<definiens id="0">a normalizing constant , and</definiens>
			</definition>
			<definition id="6">
				<sentence>924 Using the language model ( 9 ) , the OCR error correction task can be defined as finding a word sequence r~d that maximizes the joint probability of word sequence given recognized character sequence P ( WIX ) .</sentence>
				<definiendum id="0">OCR error correction task</definiendum>
			</definition>
			<definition id="7">
				<sentence>We decompose it into the product of word length probability and word spelling probability , P ( wil &lt; tlNg &gt; ) = P ( Cl ... ck \ [ &lt; IJNI~ &gt; ) = P ( k ) P ( cl ... ck Ik ) ( 11 ) where k is the length of the character sequence and &lt; UNK &gt; represents unknown word .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">into the product of word length probability and word spelling probability , P ( wil &lt; tlNg &gt; ) = P ( Cl ... ck \ [ &lt; IJNI~ &gt; ) = P ( k ) P ( cl ... ck Ik</definiens>
				<definiens id="1">the length of the character sequence and &lt; UNK &gt; represents unknown word</definiens>
			</definition>
			<definition id="8">
				<sentence>e- ( X-\ ] ) ( 12 ) We approximate the spelling probability given word length P ( cl ... c~\ [ k ) by the word-based character bigram model , regardless of word length .</sentence>
				<definiendum id="0">e-</definiendum>
			</definition>
			<definition id="9">
				<sentence>Edit distance is the minimum number of editing operations ( insertions , deletions , and substitutions ) required to transform one string into another .</sentence>
				<definiendum id="0">Edit distance</definiendum>
				<definiens id="0">the minimum number of editing operations ( insertions , deletions , and substitutions ) required to transform one string into another</definiens>
			</definition>
			<definition id="10">
				<sentence>Thus , the edit distance of two words becomes c/n , where c is the number of matched characters and n is the length of the misspelled ( and the dictionary ) word .</sentence>
				<definiendum id="0">c</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the length of the misspelled ( and the dictionary ) word</definiens>
			</definition>
			<definition id="11">
				<sentence>X would be corrected by W if the following relationship holds , P ( X ) P ( XIX ) &lt; P ( W ) P ( XIW ) ( 14 ) The left hand side represents the probability that X is an unknown word and that it is correctly recognized .</sentence>
				<definiendum id="0">left hand side</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">an unknown word and that it is correctly recognized</definiens>
			</definition>
			<definition id="12">
				<sentence>The right hand side represents the probability that W is incorrectly recognized as X. The larger the product of the word unigram probability P ( W ) and the word confusion probability P ( XIW ) , the more likely word W is the correct word for X. Therefore , for two character words , we sort the list of all one edit distance words by P ( W ) P ( X I W ) , and select the top-k words as the correction candidates .</sentence>
				<definiendum id="0">right hand side</definiendum>
				<definiendum id="1">confusion probability P ( XIW</definiendum>
				<definiens id="0">the probability that W is incorrectly recognized as X. The larger the product of the word unigram probability P ( W ) and the word</definiens>
				<definiens id="1">the correct word for X. Therefore , for two character words</definiens>
			</definition>
			<definition id="13">
				<sentence>3In ( Nagata , 1996 ) , it was assumed that the rank order distribution of the correct characters is a geometric distribution whose parameter is the accuracy of the first candidate .</sentence>
				<definiendum id="0">characters</definiendum>
				<definiens id="0">a geometric distribution whose parameter is the accuracy of the first candidate</definiens>
			</definition>
			<definition id="14">
				<sentence>Wrong correction can be avoided if the certainty of the character recognition ( OCR score ) is available .</sentence>
				<definiendum id="0">Wrong correction</definiendum>
				<definiens id="0">the certainty of the character recognition ( OCR score</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>The model will operate by means of three modules : • WORD-PREDICTOR predicts the next word wk+l given the word-parse k-prefix and then passes control to the TAGGER ; • TAGGER predicts the POStag of the next word tk+l given the word-parse k-prefix and the newly predicted word and then passes control to the PARSER ; • PARSER grows the already existing binary branching structure by repeatedly generating the transitions : ( unary , NTlabel ) , ( adjoin-left , NTlabel ) or ( adjoin-right , NTlabel ) until it passes control to the PREDICTOR by taking a null transition .</sentence>
				<definiendum id="0">PARSER</definiendum>
				<definiens id="0">operate by means of three modules : • WORD-PREDICTOR predicts the next word wk+l given the word-parse k-prefix and then passes control to the TAGGER ; • TAGGER predicts the POStag of the next word tk+l given the word-parse k-prefix and the newly predicted word and then passes control to the PARSER ; •</definiens>
			</definition>
			<definition id="1">
				<sentence>The probability P ( W , T ) of a word sequence W and a complete parse T can be broken into : P ( W , T ) = 1-I `` +xr P ( wk/Wk-aTk-x ) `` P ( tk/Wk-lTk-x , wk ) '' k=X I. N~ ~I P ( Pki /Wk-xTk-a ' Wk , tk , pkx ... pLX ) \ ] ( 1 ) i=X where : • Wk-lTk-x is the word-parse ( k 1 ) -prefix • wk is the word predicted by WORD-PREDICTOR * tk is the tag assigned to wk by the TAGGER • Nk -1 is the number of operations the PARSER executes before passing control to the WORDPREDICTOR ( the Nk-th operation at position k is the null transition ) ; Nk is a function of T • pi k denotes the i-th PARSER operation carried out at position k in the word string ; p~ 6 { ( unary , NTlabel ) , ( adjoin-left , NTlabel ) , ( adjoin-right , NTlabel ) , null } , pk 6 { ( adjoin-left , NTlabel ) , ( adjoin-right , NTlabel ) } , 1 &lt; i &lt; Nk , p~ =null , i = Nk Our model is based on three probabilities : P ( wk/Wk-lTk-1 ) ( 2 ) P ( tk/wk , Wk-lTk-x ) ( 3 ) P ( p~/wk , tk , Wk -- xTk -- l , p~ .</sentence>
				<definiendum id="0">Nk</definiendum>
				<definiendum id="1">NTlabel</definiendum>
				<definiendum id="2">NTlabel</definiendum>
				<definiens id="0">tk/Wk-lTk-x , wk ) '' k=X I. N~ ~I P ( Pki /Wk-xTk-a ' Wk , tk , pkx ... pLX ) \ ] ( 1 ) i=X where : • Wk-lTk-x is the word-parse ( k 1 ) -prefix • wk is the word predicted by WORD-PREDICTOR * tk is the tag assigned to wk by the TAGGER • Nk -1 is the number of operations the PARSER executes before passing control to the WORDPREDICTOR ( the Nk-th operation at position k is the null transition ) ;</definiens>
				<definiens id="1">a function of T • pi k denotes the i-th PARSER operation carried out at position k in the word string ; p~ 6 { ( unary ,</definiens>
				<definiens id="2">adjoin-right ,</definiens>
			</definition>
			<definition id="2">
				<sentence>word = &lt; s &gt; and h_ { 0 } ~ ( &lt; /s &gt; , TOP ' ) -that is , before predicting &lt; /s &gt; -ensures that ( &lt; s &gt; , SB ) is adjoined in the last step of the parsing process ; • P ( ( adjoin-right , TOP ) /WkTk ) = 1 , if h_O = ( &lt; /s &gt; , TOP ' ) and h_ { -l } .</sentence>
				<definiendum id="0">SB</definiendum>
				<definiens id="0">before predicting &lt; /s &gt; -ensures that ( &lt; s &gt; ,</definiens>
			</definition>
			<definition id="3">
				<sentence>The probability assignment for the word at position k + 1 in the input sentence is made using : P ( Wk+l/Wk ) = ~TheS~ P ( Wk+x/WkTk ) `` p ( Wk , Tk ) , ( 8 ) p ( Wk , Tk ) = P ( W Tk ) / P ( WkTk ) ( 9 ) TkESk which ensures a proper probability over strings W* , where Sk is the set of all parses present in our stacks at the current stage k. Another possibility for evaluating the word level perplexity of our model is to approximate the probability of a whole sentence : N P ( W ) = Z P ( W , T ( k ) ) ( 10 ) k=l where T ( k ) is one of the `` N-best '' -in the sense defined by our search -parses for W. This is a deficient probability assignment , however useful for justifying the model parameter re-estimation .</sentence>
				<definiendum id="0">probability assignment</definiendum>
				<definiendum id="1">P ( Wk+x/WkTk ) `` p ( Wk</definiendum>
				<definiendum id="2">Sk</definiendum>
				<definiendum id="3">T ( k )</definiendum>
				<definiendum id="4">N-best '' -in</definiendum>
				<definiens id="0">ensures a proper probability over strings W* , where</definiens>
				<definiens id="1">the set of all parses present in our stacks at the current stage k. Another possibility for evaluating the word</definiens>
				<definiens id="2">to approximate the probability of a whole sentence : N P ( W ) = Z P ( W , T ( k ) ) ( 10 ) k=l where</definiens>
				<definiens id="3">a deficient probability assignment , however useful for justifying the model parameter re-estimation</definiens>
			</definition>
			<definition id="4">
				<sentence>~ ( m , ) ~ where : * l is the index of the current model action ; * ml is the model component -WORDPREDICTOR , TAGGER , PARSER -that takes action number l in the derivation ( W , T ) ; , y~mt ) is the action taken at position I in the derivation : if mt = WORD-PREDICTOR , then y~m , ) is a word ; if mt -TAGGER , then y~m~ ) is a POStag ; if mt = PARSER , then y~m~ ) is a parser-action ; • ~m~ ) is the context in which the above action was taken : if rat = WORD-PREDICTOR or PARSER , then _~ , na ) = ( ho .</sentence>
				<definiendum id="0">~m~ )</definiendum>
				<definiens id="0">the index of the current model action ; * ml is the model component -WORDPREDICTOR , TAGGER , PARSER -that takes action number l in the derivation</definiens>
				<definiens id="1">a parser-action ; •</definiens>
			</definition>
			<definition id="5">
				<sentence>The intuition behind this procedure is that ¢ ( W , T ( k ) ) is an approximation to the P ( T ( k ) /w ) probability which places all its mass on the parses that survived the parsing process ; the above procedure simply accumulates the expected values of the counts c ( m ) ( y ( m ) , x ( m ) ) under the ¢ ( W , T ( k ) ) conditional distribution .</sentence>
				<definiendum id="0">T ( k ) )</definiendum>
				<definiens id="0">an approximation to the P ( T ( k ) /w ) probability which places all its mass on the parses that survived the parsing process ; the above procedure simply accumulates the expected values of the counts c ( m ) ( y ( m ) , x ( m ) ) under the ¢</definiens>
			</definition>
			<definition id="6">
				<sentence>The headword of a phrase is the word that best represents the phrase , all the other words in the phrase being modifiers of the headword .</sentence>
				<definiendum id="0">headword of a phrase</definiendum>
				<definiens id="0">the word that best represents the phrase , all the other words in the phrase being modifiers of the headword</definiens>
			</definition>
</paper>

		<paper id="1075">
			<definition id="0">
				<sentence>A critical path in the development of natural language understanding ( NLU ) modules lies in the difficulty of defining a mapping from words to semantics : Usually it takes in the order of years of highly-skilled labor to develop a semantic mapping , e.g. , in the form of a semantic grammar , that is comprehensive enough for a given domain .</sentence>
				<definiendum id="0">NLU</definiendum>
				<definiens id="0">Usually it takes in the order of years of highly-skilled labor to develop a semantic mapping</definiens>
			</definition>
			<definition id="1">
				<sentence>Once the DM is defined , the Kernel Grammar Editor drives the development of the Kernel Grammar by querying the developer to instantiate into grammar rules the rule templates derived from the DM .</sentence>
				<definiendum id="0">Kernel Grammar Editor</definiendum>
				<definiens id="0">drives the development of the Kernel Grammar by querying the developer to instantiate into grammar rules the rule templates derived from the DM</definiens>
			</definition>
			<definition id="2">
				<sentence>The Kernel Grammar Editor follows a concreteto-abstract ordering of the concepts obtained via a topological sort of the DM to query the developer , after which the Kernel Grammar is complete 3 and 2Understood here as a qualified person ( e.g. , knowledge engineer or software developer ) who is familiar with the domain at hand and has access to some sample sentences that the NLU front-end is supposed to understand .</sentence>
				<definiendum id="0">Kernel Grammar Editor</definiendum>
				<definiens id="0">follows a concreteto-abstract ordering of the concepts obtained via a topological sort of the DM to query the developer , after which the Kernel Grammar is complete 3 and 2Understood here as a qualified person ( e.g. , knowledge engineer or software developer</definiens>
			</definition>
			<definition id="3">
				<sentence>Its aim is , again , to provide predictive power at run-time : upon encountering an unparsable expression , the HUM hypothesizes possible intended meanings in the form of a ranked list of the most likely parse trees , given the current state in the discourse , the subparses for the expression and the lexical items present in the expression .</sentence>
				<definiendum id="0">HUM</definiendum>
				<definiens id="0">hypothesizes possible intended meanings in the form of a ranked list of the most likely parse trees</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>Building a head transducer involves creating appropriate head transducer states and tracing hypothesized head-transducer transitions between them that are consistent with the occurrence of the pairings ( W , f ( W ) ) in each aligned bitext .</sentence>
				<definiendum id="0">head transducer</definiendum>
				<definiens id="0">involves creating appropriate head transducer states and tracing hypothesized head-transducer transitions between them that are consistent with the occurrence of the pairings ( W , f ( W ) ) in each aligned bitext</definiens>
			</definition>
			<definition id="1">
				<sentence>Here Q is an additional symbol e.g. `` initial '' for identifying a specific state of this transducer .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiens id="0">an additional symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>A bar above a substring denotes the number of words preceding the substring in the source or target string .</sentence>
				<definiendum id="0">bar above a substring</definiendum>
				<definiens id="0">the number of words preceding the substring in the source or target string</definiens>
			</definition>
			<definition id="3">
				<sentence>44 word w t without any target output : ( initial ( w : V ) , leftw , ( W : V ) , w ' , e , -1 , 0 ) , where e is the empty string .</sentence>
				<definiendum id="0">e</definiendum>
				<definiens id="0">the empty string</definiens>
			</definition>
			<definition id="4">
				<sentence>Word frequency : The frequency of occurrence of a word in the training corpus .</sentence>
				<definiendum id="0">Word frequency</definiendum>
				<definiens id="0">The frequency of occurrence of a word in the training corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>Optionality : This metric is intended to identify optional modifiers which are less likely to be heads .</sentence>
				<definiendum id="0">Optionality</definiendum>
				<definiens id="0">This metric is intended to identify optional modifiers</definiens>
			</definition>
			<definition id="6">
				<sentence>Tlle test set used in the evaluations reported here consisted of 336 held-out English sentences .</sentence>
				<definiendum id="0">Tlle test set</definiendum>
			</definition>
</paper>

		<paper id="1105">
</paper>

		<paper id="2210">
			<definition id="0">
				<sentence>The score is the same in the case where an object occurs with three different verbs with the frequencies , say , 100 , 100 , and 100 , and in the case where the frequencies of the three heads are 280 , 10 and 10 .</sentence>
				<definiendum id="0">score</definiendum>
				<definiens id="0">the same in the case where an object occurs with three different verbs with the frequencies</definiens>
			</definition>
			<definition id="1">
				<sentence>The final formula for the distributed frequency DF ( o ) of the object o in a corpus of n triples ( Fj , Vj , o ) with Fj &gt; C is the sum 4=1 nb where a , b and C are constants that may depend on the corpus and the parser .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">the sum 4=1 nb where a , b and</definiens>
			</definition>
</paper>

		<paper id="2232">
			<definition id="0">
				<sentence>Mi means the set of lneanings in the ambiguous word wi .</sentence>
				<definiendum id="0">Mi</definiendum>
				<definiens id="0">means the set of lneanings in the ambiguous word wi</definiens>
			</definition>
			<definition id="1">
				<sentence>For exampie , PH expresses the probability for the candidate { see_l , star_l } .</sentence>
				<definiendum id="0">PH</definiendum>
				<definiens id="0">expresses the probability for the candidate { see_l</definiens>
			</definition>
			<definition id="2">
				<sentence>Therefore the condition of executing the interactive disambiguatiou can be defined as the exceptional case of the limitation .</sentence>
				<definiendum id="0">interactive disambiguatiou</definiendum>
				<definiens id="0">the exceptional case of the limitation</definiens>
			</definition>
			<definition id="3">
				<sentence>Pluto ( i ) = max ( Pr ( m ) ) ( 2 ) m E M where M is the set of the node directly under node i , pd ( m\ [ M ) is the accuracy of the interactive disambiguation at node m , that is , the probability that a user selects m provided that the system shows explanations for all the elements of M to him ( her ) .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">the set of the node directly under node i</definiens>
				<definiens id="1">the accuracy of the interactive disambiguation at node m , that is , the probability that a user selects m provided that the system shows explanations for all the elements of M to him ( her )</definiens>
			</definition>
			<definition id="4">
				<sentence>( root ) P~ , ,to ( , 'oot ) = 0.9 ( Pr ( star_l ) + Pr ( star_2 ) ) = 0.9 ( Pi , tr ( s'car_l ) + Pint~ ( star_2 ) ) = 0.9 ( 0.765+0.135 ) : 0.81 = max ( Pr ( star_l ) , Pr ( sl ; ar_2 ) ) = max ( 0.10,0.75 ) = 0.75 Therefore , P , ,t~ ( root ) &gt; P~u , o ( rOot ) , and P , .</sentence>
				<definiendum id="0">rOot</definiendum>
				<definiens id="0">Pr ( star_l ) + Pr ( star_2 ) ) = 0.9 ( Pi , tr ( s'car_l ) + Pint~ ( star_2 ) ) = 0.9 ( 0.765+0.135 ) : 0.81 = max</definiens>
			</definition>
			<definition id="5">
				<sentence>For this purpose , 'RII ' is defined as follows : RII -np na nw where np , na is the number of interaction by MP and MA respectively , 71. , , , is the llumber of ambiguous words in an input sentence .</sentence>
				<definiendum id="0">na</definiendum>
				<definiens id="0">the number of interaction by MP and MA respectively , 71. , , , is the llumber of ambiguous words in an input sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>RII represents the ratio of the increase ill the number of interaction per ambiguous word .</sentence>
				<definiendum id="0">RII</definiendum>
				<definiens id="0">the ratio of the increase ill the number of interaction per ambiguous word</definiens>
			</definition>
</paper>

		<paper id="1005">
</paper>

		<paper id="1084">
			<definition id="0">
				<sentence>TraumAID ( Webber et al. , 1992 ) is a decision support system for addressing the initial definitive management of multiple trauma .</sentence>
				<definiendum id="0">TraumAID</definiendum>
				<definiens id="0">a decision support system for addressing the initial definitive management of multiple trauma</definiens>
			</definition>
			<definition id="1">
				<sentence>TraumaTIQ ( Gertner and Webber , 1996 ) is a module that infers a physician 's plan for managing patient care , compares it to TraumAID 's plan , and critiques significant differences between them .</sentence>
				<definiendum id="0">TraumaTIQ</definiendum>
				<definiens id="0">a module that infers a physician 's plan for managing patient care , compares it to TraumAID 's plan</definiens>
			</definition>
			<definition id="2">
				<sentence>RTPI contains a set of domain-independent rules , along with adjustable parameters that determine when and how rules are invoked .</sentence>
				<definiendum id="0">RTPI</definiendum>
				<definiens id="0">contains a set of domain-independent rules , along with adjustable parameters that determine when and how rules are invoked</definiens>
			</definition>
			<definition id="3">
				<sentence>In contrast , RTPI operates on sets of complete , independent text plan trees .</sentence>
				<definiendum id="0">RTPI</definiendum>
			</definition>
			<definition id="4">
				<sentence>Similarly , WISHFUL ( Zukerman and McConachy , 1995 ) includes an optimization phase during which it chooses the optimal way to achieve a set of related communicative goals .</sentence>
				<definiendum id="0">WISHFUL</definiendum>
				<definiens id="0">includes an optimization phase during which it chooses the optimal way to achieve a set of related communicative goals</definiens>
			</definition>
			<definition id="5">
				<sentence>The first rule aggregates the communicative goal ( GOAL USER ( DO USER check_med_allergies ) ) that exists in two of the text plans .</sentence>
				<definiendum id="0">communicative goal</definiendum>
				<definiendum id="1">GOAL USER</definiendum>
				<definiens id="0">DO USER check_med_allergies ) ) that exists in two of the text plans</definiens>
			</definition>
			<definition id="6">
				<sentence>A domain specific realizer traverses the tree and inserts cue words and conjunctions based on relations .</sentence>
				<definiendum id="0">domain specific realizer</definiendum>
				<definiens id="0">traverses the tree and inserts cue words and conjunctions based on relations</definiens>
			</definition>
			<definition id="7">
				<sentence>RTPI performs rule-based integration of a set of RST-style trees .</sentence>
				<definiendum id="0">RTPI</definiendum>
			</definition>
			<definition id="8">
				<sentence>RTPI aggregates communicative goals to achieve more succinct text plans , resolves conflict among text plans , and exploits the relations between communicative goals to enhance coherence .</sentence>
				<definiendum id="0">RTPI</definiendum>
				<definiens id="0">aggregates communicative goals to achieve more succinct text plans , resolves conflict among text plans</definiens>
			</definition>
</paper>

		<paper id="2221">
			<definition id="0">
				<sentence>Here the alignment parameters penalize the alignment of English words with their German translation equivalents because the translation equivalents are far away from the words .</sentence>
				<definiendum id="0">alignment parameters</definiendum>
			</definition>
			<definition id="1">
				<sentence>-- , e~l. ) = EoEIE2 ... En , where E0 is a null phrase .</sentence>
				<definiendum id="0">E0</definiendum>
				<definiens id="0">a null phrase</definiens>
			</definition>
			<definition id="2">
				<sentence>the probability P ( rili , r~ -1 , E , e ) , align it with an English phrase E~ .</sentence>
				<definiendum id="0">probability P</definiendum>
			</definition>
			<definition id="3">
				<sentence>We made the following independence assumptions : pends only on the number of phrases in the source sentence : P ( qle , E ) -- pn ( q\ [ n ) = a ( rili ) x 1-I0_ &lt; j &lt; i ( 1 5 ( ri , rj ) ) where 5 ( x , y ) = 1 when x = y , and 5 ( x , y ) = 0 otherwise. This assumption states that P ( ri I i , rio-X , E , e ) depends on i and ri. It also 1 depends on r~with the factor YI0 &lt; j &lt; i ( 1 ( f ( ri , rj ) ) to ensure that each EnglisI~ phrase is aligned with at most one German phrase. depends on its distance from the beginning position of its preceding phrase , as well as . . the length of the source phrase aligned with the preceding phrase : P ( bi l i , bio-l , r~ , e , E ) = I = o ( Ai I lEr , _ , l ) The fertility and translation tablet of a source word depend on the word only : P ( ¢ij l i , J , ¢ilj-1 , wo'~i-1 , ~o , hq rq , e , E ) = n ( ¢ij l P ( Tijk I Tkl 1,7 '' : i 1 '' , rg -1 , ¢0 , t bo , q r~ , e , E ) = levi ) The leftmost position of the translations of a source word depends on its distance from the beginning of the target phrase aligned with the source phrase that contains that source word. It also depends on the identity of the phrase , and the position of the source word in the source phrase. j-1 i-i t E ) = dl ( Trijl -bil El , j ) For a target word rijk other than the leftmost Tij 1 in the translation tablet of the source eij , its position depends on its distance from the position of another tablet word 7 '' ij ( k_l ) closest to its left , the class of the target word Tijk , and the fertility of the source word eij. p ( jkl l 1 , i-1 i l rCil , Tr o , rO , ¢o , b~ , r~ , e , E ) = d2 ( rcijk lrij ( k_l ) I 6 ( rijk ) , ¢ij ) here G ( g ) is the equivalent class for g. EM algorithm was used to estimate the seven types of parameters : Pn , a , a , ¢ , r , dl and d2. We used a subset of probable alignments in the EM learning , since the total number of alignments is exponential to the target sentence length. The subset was the neighboring alignments ( Brown et al. , 1993 ) of the Viterbi alignments discovered by Model 1 and Model 2. We chose to include the Model 1 Viterbi alignment here because the Model 1 alignment is closer to the `` ideal '' when strong skewness exists in a sentence pair. It is of little interest for the structure-based alignment model if we have to manually find 1360 the language structures and write a grammar for them , since the primary merit of statistical machine translation is to reduce human labor. In this section we introduce a grammar inference technique that finds the phrases used in the structure-based alignment model. It is based on the work in ( Ries , Bu¢ , and Wang , 1995 ) , where the following two operators are used : . . Clustering : Clustering words/phrases with similar meanings/grammatical functions into equivalent classes. The mutual information clustering algorithm ( Brown et al. , 1992 ) were used for this. Phrasing : The equivalent class sequence Cl , c2 , ... c k forms a phrase if P ( cl , c2 , ' '' `` ck ) log P ( cI , c2 , ' '' `` ck ) &gt; 8 , P ( c , ) P ( c2 ) '' `` P ( ck ) where ~ is a threshold .</sentence>
				<definiendum id="0">c k</definiendum>
				<definiens id="0">the length of the source phrase aligned with the preceding phrase : P ( bi l i , bio-l , r~ , e</definiens>
				<definiens id="1">leftmost position of the translations of a source word depends on its distance from the beginning of the target phrase aligned with the source phrase that contains that source word. It also depends on the identity of the phrase , and the position of the source word in the source phrase. j-1 i-i t E ) = dl</definiens>
			</definition>
			<definition id="4">
				<sentence>Ambiguity Reduction : A word occurring in a phrase should be less ambiguous than in other random context .</sentence>
				<definiendum id="0">Ambiguity Reduction</definiendum>
				<definiens id="0">A word occurring in a phrase</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>DTL takes each term in the query and performs dictionary lookup to produ , :e a list of possible translation terms in the document collection language .</sentence>
				<definiendum id="0">DTL</definiendum>
				<definiens id="0">takes each term in the query and performs dictionary lookup to produ , :e a list of possible translation terms in the document collection language</definiens>
			</definition>
			<definition id="1">
				<sentence>, Walt ) where Wd~ represents the weight of term k in document D. The set of documents in the collection is N , and nk represents the number of documents in which term k appears , tfdk denotes the term frequency of term k in document D. A query Q is formulated as a query description vector Q = ( wql , wq~ , .</sentence>
				<definiendum id="0">Wd~</definiendum>
				<definiens id="0">the weight of term k in document D. The set of documents in the collection is N , and nk represents the number of documents in which term k appears</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>Because interdigitation involves pattern elements being inserted between the radicals of the root morpheme , Semitic stem formation is a classic example of non-concatenative morphotactics .</sentence>
				<definiendum id="0">Semitic stem formation</definiendum>
				<definiens id="0">involves pattern elements being inserted between the radicals of the root morpheme</definiens>
			</definition>
			<definition id="1">
				<sentence>into a copy of the previous consonant is the following , where Cons is a grammar-level variable ranging freely over consonants , LongVowel is a grammar-level variable ranging freely over long vowels and diphthongs , and C is an indexed local variable ranging over the enumerated set of consonants .</sentence>
				<definiendum id="0">Cons</definiendum>
				<definiendum id="1">LongVowel</definiendum>
				<definiendum id="2">C</definiendum>
				<definiens id="0">a grammar-level variable ranging freely over consonants</definiens>
				<definiens id="1">a grammar-level variable ranging freely over long vowels and diphthongs</definiens>
				<definiens id="2">an indexed local variable ranging over the enumerated set of consonants</definiens>
			</definition>
			<definition id="2">
				<sentence>Multi-tape two-level morphology : a case study in Semitic non-linear morphology .</sentence>
				<definiendum id="0">Multi-tape two-level morphology</definiendum>
				<definiens id="0">a case study in Semitic non-linear morphology</definiens>
			</definition>
			<definition id="3">
				<sentence>Two-level morphology : A general computational model for word-form recognition and production .</sentence>
				<definiendum id="0">Two-level morphology</definiendum>
				<definiens id="0">A general computational model for word-form recognition and production</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>The Data-Oriented Parsing ( DOP ) model ( cf. Bod 1992 , 1995 ; Bod &amp; Kaplan 1998 ; Scha 1992 ; Sima'an 1995 , 1997 ; Rajman 1995 ) is a probabilistic parsing model which does not single out a narrowly predefined set of structures as the statistically significant ones .</sentence>
				<definiendum id="0">Data-Oriented Parsing</definiendum>
				<definiendum id="1">DOP ) model</definiendum>
				<definiens id="0">a probabilistic parsing model which does not single out a narrowly predefined set of structures</definiens>
			</definition>
			<definition id="1">
				<sentence>compositional frame semantics The OVIS corpus currently consists of 10,000 syntactically and semantically annotated user utterances that were collected on the basis of a pilot version of the OVIS system 2 .</sentence>
				<definiendum id="0">OVIS corpus</definiendum>
			</definition>
			<definition id="2">
				<sentence>That is , the probability of a subtree t is equal to the number of occurrences of t in the corpus divided by the number of occurrences of all subtrees t ' that can be substituted on the same node as t. The probability of a derivation D = t 1 o ... o t n is the product of the probabilities of its subtrees t i. The probability of a parse tree T is the sum of the probabilities of all derivations D that produce T. And the probability of a meaning M and a word string W is the sum of the probabilities of all parse trees T of W whose top-node meaning is logically equivalent to M ( see Bod et al. 1996 ) .</sentence>
				<definiendum id="0">o t n</definiendum>
				<definiens id="0">the sum of the probabilities of all derivations D that produce T. And the probability of a meaning M and a word string W is the sum of the probabilities of all parse trees T of W whose top-node meaning</definiens>
			</definition>
			<definition id="3">
				<sentence>Under this assumption , the most likely meaning of a string is the top-node meaning generated by the most likely derivation of that string ( see also section 5 ) .</sentence>
				<definiendum id="0">most likely meaning of a string</definiendum>
				<definiens id="0">the top-node meaning generated by the most likely derivation of that string ( see also section 5 )</definiens>
			</definition>
			<definition id="4">
				<sentence>( 2 ) Can we constrain the OVIS subtrees without loosing accuracy ?</sentence>
				<definiendum id="0">OVIS</definiendum>
			</definition>
</paper>

		<paper id="1097">
			<definition id="0">
				<sentence>a part-of-speech tagger for French described in ( Tzoukermann and Radev , 1997 ) and FASTR , a controlled indexer ( Jacquemin et al. , 1997 ) .</sentence>
				<definiendum id="0">FASTR</definiendum>
			</definition>
			<definition id="1">
				<sentence>Thus , VerbTransfl is a verbalization which transforms a Noun-Preposition-Noun term into a verb phrase represented by the variation pattern V 4 ( Adv ?</sentence>
				<definiendum id="0">VerbTransfl</definiendum>
			</definition>
			<definition id="2">
				<sentence>VerbTransfl recognizes analys~es\ [ v\ ] dans\ [ prep\ ] le\ [ nrt\ ] sOl\ [ N\ ] ( analyzed in the soil ) as a variant of analyse\ [ N\ ] de\ [ Prep\ ] sol\ [ N\ ] ( soil analysis ) .</sentence>
				<definiendum id="0">VerbTransfl</definiendum>
				<definiens id="0">recognizes analys~es\ [ v\ ] dans\ [ prep\ ] le\ [ nrt\ ] sOl\ [ N\ ] ( analyzed in the soil ) as a variant of analyse\ [ N\ ] de\ [ Prep\ ] sol\ [ N\ ] ( soil analysis )</definiens>
			</definition>
</paper>

		<paper id="1099">
			<definition id="0">
				<sentence>WordNet is a well known on-line dictionary , consisting of 121,962 unique words , 99,642 synsets ( each synset is a lexical concept represented by a set of synonymous words ) , and 173,941 senses of words .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="1">
				<sentence>COMLEX contains syntactic information for 38,000 English words .</sentence>
				<definiendum id="0">COMLEX</definiendum>
			</definition>
			<definition id="2">
				<sentence>EVCA has been designed for use by humans , not computers .</sentence>
				<definiendum id="0">EVCA</definiendum>
				<definiens id="0">designed for use by humans , not computers</definiens>
			</definition>
			<definition id="3">
				<sentence>EVCA specifies a mapping between words and word classes , associating each class with alternations and with subcategorization frames .</sentence>
				<definiendum id="0">EVCA</definiendum>
				<definiens id="0">specifies a mapping between words and word classes</definiens>
			</definition>
			<definition id="4">
				<sentence>The formatted EVCA consists of sets of applicable alternations and subcategorizations for 3,104 verbs .</sentence>
				<definiendum id="0">formatted EVCA</definiendum>
			</definition>
			<definition id="5">
				<sentence>A semantic concept is the semantic meaning that a user wants to convey , while a lexical concept is a lexical meaning that can be represented by a set I Sentence Planner I ~i uoncepts to Le×ical Concepts 11 ~01 Lexical Concepts `` ~ } \ [ Mapping from Lexicall i~ .</sentence>
				<definiendum id="0">semantic concept</definiendum>
				<definiens id="0">the semantic meaning that a user wants to convey</definiens>
			</definition>
			<definition id="6">
				<sentence>PlanDOC is an enhancement to Bellcore 's LEIS-PLAN TM network planning product .</sentence>
				<definiendum id="0">PlanDOC</definiendum>
				<definiens id="0">an enhancement to Bellcore 's LEIS-PLAN TM network planning product</definiens>
			</definition>
			<definition id="7">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1063">
</paper>

		<paper id="1104">
			<definition id="0">
				<sentence>The LNRE ( Large Number of Rare Events ) zone ( Chitashvili &amp; Baayen , 1993 ) is defined as the range of sample size where the population events ( different morphemes ) are far from being exhausted .</sentence>
				<definiendum id="0">LNRE ( Large Number of Rare Events ) zone</definiendum>
			</definition>
			<definition id="1">
				<sentence>( 4 ) where dG ( p ) = G ( pj ) G ( pj+l ) around PJ , and 0 otherwise , in which p is now grouped for the same value and indexed by the subscript j that indicates in ascending order the values of p. In using some explicit expressions such as lognormal 'law ' ( Carrol , 1967 ) for G ( p ) , we again face the problem of sample size dependency of the parameters of these 'laws ' .</sentence>
				<definiendum id="0">dG</definiendum>
				<definiens id="0">( p ) = G ( pj ) G ( pj+l ) around PJ</definiens>
			</definition>
			<definition id="2">
				<sentence>To overcome the problem , a certain distribution model for the population is assumed , which manifests itself as one of the 'laws ' at a pivotal sample size Z. By explicitly incorporating Z as a parameter , the models can be completed , and it becomes possible ( i ) to represent the distribution of population probabilities by means of G ( p ) with Z and to estimate the theoretical vocabulary size , and ( ii ) to interpolate and extrapolate V ( N ) and V ( m , N ) to the arbitrary sample size N , by such an expression : E\ [ V ( m , N ) \ ] = -- I = - ( ~ ( Z-'-P ) ) '~ ) m !</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">V</definiendum>
				<definiens id="0">p ) with Z and to estimate the theoretical vocabulary size</definiens>
			</definition>
			<definition id="3">
				<sentence>Pi reflects the growth rate of the morphemes of each type observed separately .</sentence>
				<definiendum id="0">Pi</definiendum>
				<definiens id="0">reflects the growth rate of the morphemes of each type observed separately</definiens>
			</definition>
</paper>

		<paper id="2181">
			<definition id="0">
				<sentence>Once we have obtained the first DGILE version with semantically labelled definitions , we can collect the salient words ( that is , those representative words for a particular category ) using a Mutual Information-like formula ( 2 ) , where w means word and SC semantic class .</sentence>
				<definiendum id="0">w</definiendum>
				<definiens id="0">means word and SC semantic class</definiens>
			</definition>
			<definition id="1">
				<sentence>• Filter 1 ( F1 ) removes all FOOD genus terms not assigned to the FOOD semantic file during the mapping process between the bilingual dictionary and WordNet .</sentence>
				<definiendum id="0">F1 )</definiendum>
				<definiens id="0">removes all FOOD genus terms not assigned to the FOOD semantic file during the mapping process between the bilingual dictionary</definiens>
			</definition>
</paper>

		<paper id="2169">
			<definition id="0">
				<sentence>I will use letters t , u , and v to indicate types ; capital letters to indicate type variables ; capitalized words to indicate feature names ; p , q , and r for names of parametric types ; and g to indicate ground instances of parametric types , 1028 archies that use parametric types are not `` type '' hierarchies , since they express a relationship between functions ( we can regard simple types as nullary parametric types ) : Definition 1 : A parametric ( type ) hierarchy is a finite meet semilattice , ( P , EP ) , plus a partial argument assignment function , ap : P × P × Nat -~ Nat U { 0 } , in which : • P consists of ( simple and ) parametric types , ( i.e. no ground instances of parametric types ) , including the simple most general type , _1_ , • For p , q E P , ap ( p , q , i ) , written aq ( i ) , is defined iff p EP q and 1 &lt; _ i &lt; _ arity ( p ) , and • 0 &lt; aq ( i ) &lt; _ rn , when it exists .</sentence>
				<definiendum id="0">P , EP</definiendum>
				<definiens id="0">u , and v to indicate types ; capital letters to indicate type variables ; capitalized words to indicate feature names ; p , q , and r for names of parametric types ; and g to indicate ground instances of parametric types , 1028 archies that use parametric types are not `` type '' hierarchies , since they express a relationship between functions ( we can regard simple types as nullary parametric types ) : Definition 1 : A parametric ( type ) hierarchy is a finite meet semilattice</definiens>
				<definiens id="1">plus a partial argument assignment function , ap : P × P × Nat -~ Nat U { 0 } , in which : • P consists of ( simple and ) parametric types , ( i.e. no ground instances of parametric types ) , including the simple most general type , _1_ , • For p , q E P , ap ( p , q</definiens>
			</definition>
			<definition id="1">
				<sentence>The argument assignment function encodes the identification of parameters between a parametric type and its parametric subtype .</sentence>
				<definiendum id="0">argument assignment function</definiendum>
				<definiens id="0">encodes the identification of parameters between a parametric type and its parametric subtype</definiens>
			</definition>
			<definition id="2">
				<sentence>• p ( tl , ... , tn ) EI q ( ut , ... , urn ) iff p EP q , and , for all l &lt; i &lt; n , either at ( i ) = 0 or ti Et u~ ( i ) . It can easily be shown that ( I ( P ) , EI ) is a partial order with a least element , namely .L , the least element of P. Note that I ( P ) also contains all of the simple types of P. In the case where 9rand92 are simple , gl EI g2 iff gl EP 92. Figure 4 shows a fragment of the type hierarchy induced by Figure 1. If list and nclist had nelist ( , wo d~..~eli~t ( phrase ) ... ... ' n `` '' `` ( si n '' nelist ( list ( .l_ ) ) ens ( , g / . . `` `` list ( ~ ( phrase ) l /list ~ist ( _k ) ) . . . t~sr ( szgrv ' -- -~list~ ) Figure 4 : Fragment induced by Figure 1. not shared the same type variable ? , ,nelistfl~ 0 ) , then it would have induced the type hierarchy in Figure 5. In the hierarchy induced nelist ( wor~jnelist ( phrase } i nelist ( sigr~.~nelist ( list ( _l_ ) ) list ( word phrase ) : cl ) , ,flist ( phrese ) . list ( sig ..Llist ( list ( -L ) ) list ( Z ) Figure 5 : Another possible induced hierarchy. feature values. This abstract assumes that these domains are always the set of all types in the signature. This is the most expressive case of parametric types , and the worst case , computationally. 1029 by Figure 3 , b ( e , e ) subsumes types d ( e , Y , e ) , for any type Y , for example d ( e , c ( e , e ) , e ) , or d ( e , b ( _L , e ) , e ) , but not d ( c ( _L , e ) , e , e ) , since eElc ( _l_ , e ) . Also , for any types , W , X , and Z , c ( W , e ) subsumes d ( X , e , Z ) . The present approach permits parametric types in the type signature , but only ground instances in a grammar relative to that signature. If one must refer to `` some list '' or `` every list '' within a grammar , for instance , one may use list ( I ) , while still retaining groundedness. An alternative to this approach would be to attempt to cope with type variable parameters directly within descriptions. From a processing perspective , this is problematic when closing such descriptions under total well-typing , as observed in ( Car92 ) . The most general satisfier of the description , list ( X ) A ( HEAD : HEAD `` - '' TAIL : HEAD ) , for example , is an infinite feature structure of the infinitely parametric type , nelist ( nelist ( .. , because X must be bound to nelist ( X ) . For which P does it make sense to talk about unification in I ( P ) , that is , when is I ( P ) a meet semilattice ? We can generalize the usual notion of coherence from programming languages , so that a subtype can add , and in certain cases drop , parameters with respect to a supertype : Definition 3 : ( P , EP , ap ) is semi-coherent if , for allp , q E P such thatp Ep q , all1 _ &lt; i _ &lt; arity ( p ) , 1 &lt; _ j &lt; _ arity ( q ) : • ag ( i ) = i , • either aq ( i ) = 0 or for every chain , p = Pl EP p2 EP ... EP Pn = q , aq ( i ) = , up._ , t ... aP~ ( i ) ... ) ) , and • Ifpllpq $ , then for all i and j for which there is a k &gt; _ 1 such that appUpq ( i ) = apqUpa ( j ) = k , the set , { rip Up q EP r and ( @ ( i ) = 0 or arq ( j ) = 0 ) } is empty or has a least element ( with respect to EP ) .</sentence>
				<definiendum id="0">nclist had nelist</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a partial order with a least element</definiens>
				<definiens id="1">wor~jnelist ( phrase } i nelist ( sigr~.~nelist ( list ( _l_ ) ) list ( word phrase ) : cl )</definiens>
				<definiens id="2">the most expressive case of parametric types</definiens>
				<definiens id="3">e ) , e ) , but not d ( c ( _L , e ) , e , e</definiens>
			</definition>
			<definition id="3">
				<sentence>Appropriateness constitutes an integral part of a parametric type signature 's expressive power , because the scope of its type variables extends to include it .</sentence>
				<definiendum id="0">Appropriateness</definiendum>
				<definiens id="0">constitutes an integral part of a parametric type signature 's expressive power</definiens>
			</definition>
			<definition id="4">
				<sentence>Definition 4 : A parametric ( type ) signature is a parametric hierarchy , ( P , EP , ap &gt; , along with finite set of features , Featp , and a partial ( parametric ) appropriateness function , Appropp : Featp x P -- ~ Q , where Q = UneNat Qn , and each Qn is the smallest set satisfying the equation , Qn = { 1 , ... , n } u { P ( qi , ... , qk ) lP E Parity k , qi E Qn } , such that : 6The proofs of these theorems can be found in the full version of this paper .</sentence>
				<definiendum id="0">parametric</definiendum>
				<definiens id="0">the smallest set satisfying the equation</definiens>
			</definition>
			<definition id="5">
				<sentence>1030 f E Featp , there is a most general parametric type Intro ( f ) E P such that Appropp ( f , Intro ( f ) ) is defined For any p , q E P , if Appropp ( f , p ) is defined and p EP q , then Appropp ( f , q ) is also defined and Appropp ( f , p ) EQ Appropp ( f , q ) , where EQ is defined as EI ( P ) with natural numbers interpreted as universally quantified variables ( e.g. a ( 1 ) EQ b ( 1 ) iffVx E I ( P ) .</sentence>
				<definiendum id="0">Intro ( f ) )</definiendum>
				<definiendum id="1">p ) EQ Appropp</definiendum>
				<definiens id="0">a most general parametric type Intro ( f ) E P such that Appropp ( f ,</definiens>
			</definition>
			<definition id="6">
				<sentence>One way to identify this part is to identify some set of ground instances ( a generator set ) that are necessary for computation , and close that set under Ui ( p ) : Theorem 4 : If G C I ( P ) , is finite , then the sub-algebra of I ( P ) generated by G , I ( G ) , is finite .</sentence>
				<definiendum id="0">I</definiendum>
				<definiens id="0">to identify some set of ground instances ( a generator set ) that are necessary for computation</definiens>
			</definition>
</paper>

		<paper id="1054">
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>Heuristic for I-on-H ( the shaded node in Figure 1 ) .</sentence>
				<definiendum id="0">Heuristic for I-on-H</definiendum>
			</definition>
</paper>

		<paper id="2234">
			<definition id="0">
				<sentence>By core phrases , we mean the kind of nonrecursive simplifications of the NP and VP that in the literature go by names such as noun/verb groups ( Appelt et al. , 1993 ) or chunks , and base NPs ( Ramshaw and Marcus , 1995 ) .</sentence>
				<definiendum id="0">base NPs</definiendum>
				<definiens id="0">noun/verb groups ( Appelt et al. , 1993 ) or chunks , and</definiens>
			</definition>
</paper>

		<paper id="2128">
			<definition id="0">
				<sentence>Constraint Grammar is a system for part of speech tagging and ( shallow ) syntactic dependency analysis of unrestricted text .</sentence>
				<definiendum id="0">Constraint Grammar</definiendum>
				<definiens id="0">a system for part of speech tagging and ( shallow ) syntactic dependency analysis of unrestricted text</definiens>
			</definition>
			<definition id="1">
				<sentence>The following as a typical 'reductionistic ' example of a CG rule which discards a verbal reading of a word following a word unambiguously tagged as determiner ( Tapanainen , 1996 , page 12 ) : REMOVE ( V ) IF ( -iC DET ) ; where V is the target tag to be discarded and -IC DET denotes the word immediately to the left ( -I ) , unambiguously ( C ) tagged as determiner ( DET ) .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">-IC DET</definiendum>
				<definiens id="0">discards a verbal reading of a word following a word unambiguously tagged as determiner ( Tapanainen , 1996 , page 12 ) : REMOVE ( V ) IF ( -iC DET</definiens>
			</definition>
			<definition id="2">
				<sentence>Inductive Logic Programming ( ILP ) is a combination of machine learning and logic programming , where the goal is to find a hypothesis , H , given examples , E , and background knowledge , B , such that the hypothesis along with the background knowledge logically implies the examples ( Muggleton , 1995 , page 2 ) : BAH~E The examples are usually split into a positive , E + , and a negative , E- , subset .</sentence>
				<definiendum id="0">Inductive Logic Programming</definiendum>
				<definiendum id="1">ILP )</definiendum>
				<definiens id="0">to find a hypothesis , H , given examples , E , and background knowledge</definiens>
			</definition>
			<definition id="3">
				<sentence>Progol creates , for each E + , a most specific clause -l-i and then searches through the lattice of hypotheses , from specific 775 to more general , bounded by \ [ \ ] - &lt; H - &lt; -l-i to find the clause that maximally compresses the data where `` &lt; ( 0-subsumption ) is defined as Cl .</sentence>
				<definiendum id="0">Progol</definiendum>
			</definition>
			<definition id="4">
				<sentence>The SUC tagset has 146 different tags , and the tags consist of a part of speech tag , e.g. VB ( the verb ) followed by a ( possibly empty ) set of morphological features , such as PRS ( the present tense ) and AKT ( the active voice ) , etc .</sentence>
				<definiendum id="0">SUC tagset</definiendum>
				<definiendum id="1">VB</definiendum>
				<definiendum id="2">AKT (</definiendum>
				<definiens id="0">has 146 different tags , and the tags consist of a part of speech tag</definiens>
				<definiens id="1">the verb ) followed by a ( possibly empty ) set of morphological features</definiens>
			</definition>
			<definition id="5">
				<sentence>The U0 tag is a peculiarity of the SUC tagset , and conveys no grammatical information ; it stands for 'foreign word ' and is used e.g. for the words in passages quoting text which is not in Swedish .</sentence>
				<definiendum id="0">U0 tag</definiendum>
				<definiens id="0">a peculiarity of the SUC tagset , and conveys no grammatical information</definiens>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>Various traditional information retrieval ( IR ) techniques combined with natural language processing ( NLP ) techniques have been re-targeted to enable efficient access of the WWW -- search engines , indexing , relevance feedback , query term and keyword weighting , document analysis , document classification , etc .</sentence>
				<definiendum id="0">IR</definiendum>
			</definition>
			<definition id="1">
				<sentence>Fortunately , nowadays the World Wide Web provides us with a daily increase of fresh , up-to-date multilingual material , together with the archived versions , all easily downloadable by software tools running in the background .</sentence>
				<definiendum id="0">World Wide Web</definiendum>
				<definiens id="0">provides us with a daily increase of fresh , up-to-date multilingual material</definiens>
			</definition>
			<definition id="2">
				<sentence>The TF of a context word is defined as the frequency of the word in the context of W. ( e.g. TF of virus in flu is 26 , in ~ , ~ is 147 ) .</sentence>
				<definiendum id="0">TF of a context word</definiendum>
				<definiens id="0">the frequency of the word in the context of W. ( e.g. TF of virus in flu is 26</definiens>
			</definition>
			<definition id="3">
				<sentence>In an extreme case , the function word the appears most frequently in English texts and would have the highest TF in the context of any W. In our HKStandard/Mingpao corpus , Hong Kong is the most frequent content word which appears everywhere .</sentence>
				<definiendum id="0">Hong Kong</definiendum>
				<definiens id="0">the most frequent content word which appears everywhere</definiens>
			</definition>
			<definition id="4">
				<sentence>A ranking algorithm selects the best target language candidate for a source language word according to direct comparison of some similarity measures ( Frakes and Baeza-Yates , 1992 ) .</sentence>
				<definiendum id="0">ranking algorithm</definiendum>
				<definiens id="0">selects the best target language candidate for a source language word according to direct comparison of some similarity measures ( Frakes and Baeza-Yates</definiens>
			</definition>
</paper>

		<paper id="1043">
</paper>

		<paper id="2185">
			<definition id="0">
				<sentence>( ( frame *schedule ) ( who ( ( frame *you ) ) ) ( what ( ( frame *what ) ( wh + ) ) ) ( when ( ( frame *simple-time ) ( day 26 ) ( month 5 ) ) ) ) `` Will schedule for tile twenty-sixth of May '' ( ( frame *schedule ) ( when ( ( frame *simple-time ) ( day 26 ) ( month 5 ) ) ) ) Figure 1 : Example Alternative Hypotheses ( ( f *schedule ) ( s who ) ) ( ( f *you ) ) ( ( f *schedule ) ( s what ) ) ( ( f *schedule ) ( s what ) ( f *what ) ) ( ( f *what ) ) ( ( f + ) ) ( ( f *what ) ( s wh ) ( f + ) ) ( ( f *schedule ) ( s who ) ( f *you ) ) ( ( f *schedule ) ( s what ) ( f *what ) ( s wh ) ( f + ) ) Figure 2 : Distinguishing Features used in the system to distinguish alternative meaning representation structures from one another specify paths down this tree structure .</sentence>
				<definiendum id="0">*simple-time )</definiendum>
				<definiendum id="1">*simple-time )</definiendum>
				<definiendum id="2">s wh )</definiendum>
				<definiens id="0">Distinguishing Features used in the system to distinguish alternative meaning representation structures</definiens>
			</definition>
			<definition id="1">
				<sentence>• Evaluatable : Does it ask about a single repair or set of repairs that always occur together ?</sentence>
				<definiendum id="0">Evaluatable</definiendum>
			</definition>
			<definition id="2">
				<sentence>Equation 1 is for calculating S/ , the expected search reduction of feature number f. nl s , = × ( L-t , , , l ( ii i=I L is the number of alternative hypotheses .</sentence>
				<definiendum id="0">Equation 1</definiendum>
				<definiens id="0">the number of alternative hypotheses</definiens>
			</definition>
			<definition id="3">
				<sentence>A plan-based discourse processor ( Ros6 et al. , 1995 ) provides contextual expectations that guide the system in the manner in which it formulates 1132 Sentence : What about any time but the ten to twelve slot on Tuesday the thirtieth ?</sentence>
				<definiendum id="0">plan-based discourse processor</definiendum>
				<definiens id="0">provides contextual expectations that guide the system in the manner in which it formulates 1132 Sentence</definiens>
			</definition>
</paper>

		<paper id="2235">
			<definition id="0">
				<sentence>We assume that the title of a text is the most concise statement which expresses the most essential information of the text , and that the closer a sentence relates to an important sentence , the more important this sentence is .</sentence>
				<definiendum id="0">concise statement</definiendum>
				<definiens id="0">expresses the most essential information of the text , and that the closer a sentence relates to an important sentence</definiens>
			</definition>
</paper>

		<paper id="1106">
			<definition id="0">
				<sentence>We will define a property ( more exactly a family of properties ) , weaker than projectivity , called pseudo-projectivity , which describes a subset of the set of ordered dependency trees , containing the non-projective linguistic structures .</sentence>
				<definiendum id="0">pseudo-projectivity</definiendum>
				<definiens id="0">a family of properties ) , weaker than projectivity , called</definiens>
				<definiens id="1">describes a subset of the set of ordered dependency trees</definiens>
			</definition>
			<definition id="1">
				<sentence>Suppose we have have a grammar containing the dependency rules dl ( rule 2 ) , d2 ( rule 3 ) , and d3 ( rule 4 ) ; the LP rule Pl ( rule 5 ) and p2 : p2 : Vclause : ( Ntop : + INwh : + ) ( Adv ) Nnom ( Aux ) Adv* # Adv* Vt ... . Furthermore , we have the following 1-rule : II : Vbridge : + -- -~Nc ... .. bj top : + { 'V~ridge : +VNc ... .. bj top : + } This rule says that an objective wh-noun with feature top : + which depends on a verb with no further restrictions ( the third V in the lifting path ) can raise to any verb that dominates its immediate governor as long as the raising paths contains only verb with feature bridge : + , i.e. , bridge verbs .</sentence>
				<definiendum id="0">LP rule Pl</definiendum>
				<definiens id="0">p2 : Vclause : ( Ntop : + INwh : + )</definiens>
			</definition>
			<definition id="2">
				<sentence>V~ridge : + V Nc ... . bj top : + } =~ beans Fernando thought yesterday Milagro claims V { -V~ridge : + Y Nc ... .. bj top : + } beans yesterday Fernando thought yesterday Milagro claims Nnom eats N { Y~ridge : + V Ycase : obj top : + ' } Adv : =~ beans Fernando thought yesterday Milagro claims Carlos eats slowly Vcl~us¢ N~au*e beans Fernando yester Nno m claims Vtrans Milagro Nnom eats Adv I I Carlos slowly Figure 4 : A sample PP-GDG derivation A sample derivation is shown in Figure 4 , with the sentential form representation on top 650 and the corresponding tree representation below .</sentence>
				<definiendum id="0">sample PP-GDG</definiendum>
				<definiens id="0">+ } =~ beans Fernando thought yesterday Milagro claims V { -V~ridge : + Y Nc ... .. bj top : + } beans yesterday Fernando thought yesterday Milagro claims Nnom eats N { Y~ridge : + V Ycase : obj top : + ' } Adv : =~ beans Fernando thought yesterday</definiens>
			</definition>
			<definition id="3">
				<sentence>The entries in the parse matrix M are of the form ( m , q , LM ) , where m is a rule-FSM , q a state of m , and LM is a multiset of lifting conditions as defined in Section 4 .</sentence>
				<definiendum id="0">m</definiendum>
				<definiendum id="1">LM</definiendum>
			</definition>
			<definition id="4">
				<sentence>The maximum number of entries in each square of the parse matrix is O ( GQnL ) , where G is the number of rule-FSMs corresponding to LP rules in the grammar , Q is the maximum number of states in any of the rule-FSMs , and L is the maximum number of states that the lifting rules can be in ( i.e. , the number of lifting conditions in the grammar multiplied by the maximum number of dot positions of any lifting condition ) .</sentence>
				<definiendum id="0">G</definiendum>
				<definiendum id="1">Q</definiendum>
				<definiendum id="2">L</definiendum>
				<definiens id="0">the number of rule-FSMs corresponding to LP rules in the grammar</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>Introduction In terms of syntactic categories , proper nouns are lexical NPs that can be formed by primitive proper names ( Adolfo_Battaglia ) , groups of proper nouns of different semantic categories ( San Paolo di Brescia ) , and also of non-proper nouns ( Banca dei regolamenti internazionali ) .</sentence>
				<definiendum id="0">Banca dei regolamenti internazionali</definiendum>
				<definiens id="0">proper nouns are lexical NPs that can be formed by primitive proper names ( Adolfo_Battaglia ) , groups of proper nouns of different semantic categories</definiens>
			</definition>
			<definition id="1">
				<sentence>Finally , let : ESLA be a set of esls defined as follows : for each esli ( x , PN_U ) in ESL put in ESLA the set of eslj ( x , PNj ) , in the corpus , with type=typei , x in the same position of esli , and PNi a known proper noun , in the same position as PN_U in esli , ESLB be the set of eslk defined as follows : for each esli ( x , PN_U ) in ESL put in ESLB the set of eslj ( w , PNj ) , in the corpus , with type=typei , w in the same position of x in esli , Sim ( w , x ) &gt; 8 , and PNj a known proper noun , in the same position as PNU in esl i. Sim ( w , x ) is a similarity measure between x and w. In our first experiments , Sim ( w , x ) &gt; 8 iff w is a synonym of x. For each semantic category Con i compute evidence ( Cpnj ) as shown ih-Figure 1 , where : amb ( esl ( x , PNi ) ) is a measure of the ambiguity of x and PNj in esli ; tx and 13 are experimentally determined weights ( currently , t~=0.7 and 13=0.3 ) .</sentence>
				<definiendum id="0">ESLB</definiendum>
				<definiendum id="1">PNj )</definiendum>
				<definiendum id="2">Cpnj</definiendum>
				<definiendum id="3">PNi ) )</definiendum>
				<definiens id="0">in the corpus , with type=typei , x in the same position of esli , and PNi a known proper noun , in the same position as PN_U in esli</definiens>
				<definiens id="1">follows : for each esli ( x , PN_U ) in ESL put in ESLB the set of eslj ( w ,</definiens>
				<definiens id="2">a similarity measure between x</definiens>
			</definition>
			<definition id="2">
				<sentence>The Information Gain is an informationtheoretic measure that takes into account the complexity of the classification task .</sentence>
				<definiendum id="0">Information Gain</definiendum>
				<definiens id="0">an informationtheoretic measure that takes into account the complexity of the classification task</definiens>
			</definition>
			<definition id="3">
				<sentence>000 5 ~ 0.000 0.000 0.000 6 ~ 0.000 0.000 0.000 7 ~ 0.000 0.000 0.000 S3M_ESLA= 1.333 SLIM_~= 7.274 Max evidenoe category is : PRCILL-T Figure 2 A complete example If P ( C ) is the prior ( a-priori ) probability 4 that an instance c is a member of class C , and P ' ( C ) is the probability of c e C , as computed by the classifier in a given test ti , the Information Gain I ( ti ) is defined as : I ( ti ) = log ( 1-P ( C ) ) log ( 1-P ' ( C ) ) if P ( C ) &gt; P ' ( C ) or I ( ti ) = log ( P ' ( C ) ) log ( P ( C ) ) if P ' ( C ) &gt; P ( C ) That is , if the classification is wrong , I ( ti ) is a penalty as high as the classification task 4The prior probability can be easily computed in a learning set as the ratio between the number of training instances belonging to a class C and the total number of training instances .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">Information Gain I ( ti )</definiendum>
				<definiens id="0">the prior ( a-priori ) probability 4 that an instance c is a member of class C</definiens>
				<definiens id="1">the probability of c e C , as computed by the classifier in a given test ti , the</definiens>
				<definiens id="2">if P ( C ) &gt; P ' ( C ) or I ( ti ) = log ( P ' ( C ) ) log ( P ( C ) ) if P ' ( C ) &gt; P ( C ) That is , if the classification is wrong , I ( ti ) is a penalty as high as the classification task 4The prior probability can be easily computed in a learning set as the ratio between the number of training instances belonging to a class C and the total number of training instances</definiens>
			</definition>
			<definition id="4">
				<sentence>Capturing complex nominals in absence of anchors and specific contextual rules ( here the only anchor is Fiat , which appears in the gazetteer as an Organization name ) may be difficult , and if a complex nominal is not captured as a unit , the resulting syntactic context may be misleading ( e.g. N_ADJ ( Fiat_Marea_Weekend , 2000 ) ) .</sentence>
				<definiendum id="0">Fiat</definiendum>
				<definiens id="0">appears in the gazetteer as an Organization name</definiens>
			</definition>
</paper>

		<paper id="2172">
			<definition id="0">
				<sentence>Coreference resolution is the next step on the way towards discourse understanding .</sentence>
				<definiendum id="0">Coreference resolution</definiendum>
			</definition>
			<definition id="1">
				<sentence>For story processing , both pc ( W ) and the gesture message are extremely limited , so the program has to rely only on discourse information , thus building fh'st RWc ( s ) and only afterwards RWc , using supplementary knowledge about W. The gap between RWc ( s ) and RWc is 1047 due to the speaker 's misuse of referring expressions , or to internal contradictions of the story .</sentence>
				<definiendum id="0">RWc</definiendum>
				<definiens id="0">1047 due to the speaker 's misuse of referring expressions , or to internal contradictions of the story</definiens>
			</definition>
			<definition id="2">
				<sentence>The symmetrical operation , i.e. , splitting an MR which confusingly represents two objects , is far more difficult to do , as it has to reverse a lot of decisions ; • partition : MRa ~ MRa + MRnew ( 1 ) + MRnew ( 2 ) + ... ; • grouping : MRa + MRb ~ MRa + MRb + MRnew ( a , b ) ; The last two operations ( partition/grouping ) are symmetrical , and prove necessary in order to deal with collections of objects ( plurals ) .</sentence>
				<definiendum id="0">symmetrical operation</definiendum>
				<definiens id="0">splitting an MR which confusingly represents two objects</definiens>
			</definition>
</paper>

		<paper id="2175">
			<definition id="0">
				<sentence>MoBiDic has been designed for translator workgroups , where the translators ' own glossaries ( built also with the help of the system ) may also be disseminated among the members of the group , with different access rights , if needed .</sentence>
				<definiendum id="0">MoBiDic</definiendum>
				<definiens id="0">designed for translator workgroups , where the translators ' own glossaries ( built also with the help of the system</definiens>
			</definition>
			<definition id="1">
				<sentence>To make the system able to find the stem of the input word-form automatically , MoBiDic uses a lemmatizer that provides the dictionary look-up module with the stem ( s ) to be found ( Figure 1 ) .</sentence>
				<definiendum id="0">MoBiDic</definiendum>
			</definition>
			<definition id="2">
				<sentence>An intelligent software tool , like MorphoLogic 's Helyette 1 , is the combination of a thesaurus ( synonym dictionary ) , a morphological analyzer and a generator , because the output is re-inflected according to the morphological information contained by the input word-form .</sentence>
				<definiendum id="0">intelligent software tool</definiendum>
				<definiens id="0">the combination of a thesaurus ( synonym dictionary ) , a morphological analyzer and a generator</definiens>
			</definition>
			<definition id="3">
				<sentence>MoBiDic is able to treat user dictionaries containing any type of information sources ( lexicons , encyclopedias and dictionaries ) .</sentence>
				<definiendum id="0">MoBiDic</definiendum>
				<definiens id="0">able to treat user dictionaries containing any type of information sources ( lexicons , encyclopedias and dictionaries )</definiens>
			</definition>
			<definition id="4">
				<sentence>Its server side ( Windows NT , Unix and Novell ) consists , in fact , of two servers : the linguistic server and the dictionary server .</sentence>
				<definiendum id="0">Novell )</definiendum>
				<definiens id="0">consists , in fact</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>Conceptual Density ( Agirre and Rigau 1996 ) is the paradigmatic component chosen to discriminate semantically among potential noun corrections .</sentence>
				<definiendum id="0">Conceptual Density</definiendum>
				<definiens id="0">the paradigmatic component chosen to discriminate semantically among potential noun corrections</definiens>
			</definition>
			<definition id="1">
				<sentence>This work focuses on language models , not error models ( typing errors , common misspellings , OCR mistakes , speech recognition mistakes , etc. ) .</sentence>
				<definiendum id="0">OCR</definiendum>
			</definition>
			<definition id="2">
				<sentence>CG works on a text where all possible morphological interpretations have been assigned to each word-form by the ENGTWOL morphological analyser ( Voutilainen and Heikkil~i 1995 ) .</sentence>
				<definiendum id="0">CG</definiendum>
				<definiens id="0">works on a text where all possible morphological interpretations have been assigned to each word-form by the ENGTWOL morphological analyser</definiens>
			</definition>
			<definition id="3">
				<sentence>CG shows good results on precision , but fails to choose a single proposal .</sentence>
				<definiendum id="0">CG</definiendum>
				<definiens id="0">shows good results on precision , but fails to choose a single proposal</definiens>
			</definition>
			<definition id="4">
				<sentence>H2 raises the precision of all techniques at the cost of losing coverage .</sentence>
				<definiendum id="0">H2</definiendum>
				<definiens id="0">raises the precision of all techniques at the cost of losing coverage</definiens>
			</definition>
			<definition id="5">
				<sentence>Menezo et al. ( 1996 ) present a spelling/grammar checker that adjusts its strategy dynamically taking into account different lexical agents ( dictionaries ... . ) , the user and the kind of text .</sentence>
				<definiendum id="0">spelling/grammar checker</definiendum>
				<definiens id="0">adjusts its strategy dynamically taking into account different lexical agents ( dictionaries ... . ) , the user and the kind of text</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>In transformation-based tagging ( Brill ( 1995 ) ) , every word is first assigned an initial tag , This tag is the most likely tag for a word if the word is known and is guessed based upon properties of the word if the word is not known .</sentence>
				<definiendum id="0">transformation-based tagging</definiendum>
				<definiens id="0">the most likely tag for a word if the word is known and is guessed based upon properties of the word if the word is not known</definiens>
			</definition>
			<definition id="1">
				<sentence>We define the complementary rate of taggers A and B as : The maximum-entropy framework is a probabilistic framework where a model is found that is consistent with the observed data and is 2 http : //www.cs.jhu.edu/-brill 3 http : //www.cis.upenn.edu/-adwait ambiguity resolution over all words , including words that are unambiguous .</sentence>
				<definiendum id="0">maximum-entropy framework</definiendum>
				<definiens id="0">a probabilistic framework where a model is found that is consistent with the observed data</definiens>
			</definition>
			<definition id="2">
				<sentence>* I00 # of errors in A only In other words , Comp ( A , B ) measures the percentage of time when tagger A is wrong that tagger B is correct .</sentence>
				<definiendum id="0">Comp</definiendum>
				<definiendum id="1">B )</definiendum>
				<definiens id="0">measures the percentage of time when tagger A is wrong that tagger B is correct</definiens>
			</definition>
			<definition id="3">
				<sentence>Row = A , Column = B The complementary rates are quite high , which is encouraging , since this sets the upper bound on how well we can do in combining the different classifiers .</sentence>
				<definiendum id="0">Row</definiendum>
				<definiens id="0">encouraging , since this sets the upper bound on how well we can do in combining the different classifiers</definiens>
			</definition>
</paper>

		<paper id="2139">
			<definition id="0">
				<sentence>Each transfer rule consists of a pair of tree fragments derived by `` cutting up '' the source and target trees .</sentence>
				<definiendum id="0">transfer rule</definiendum>
			</definition>
			<definition id="1">
				<sentence>Let the d ( v ) be the degree of a node v. We denote children of u by vi , i = 1 , ... , d ( v ) , and arc ( v , v { ) by if { .</sentence>
				<definiendum id="0">v )</definiendum>
				<definiens id="0">the degree of a node v. We denote children of u by vi , i = 1 , ... , d ( v ) , and arc</definiens>
			</definition>
			<definition id="2">
				<sentence>( v , v ~ ) &gt; _ 0 ( used below ) is the quality of translation , i.e. the measure of how closely the label ( word ) at source node v corresponds to the label at target node v ~ in the bilingual dictionary , and Lex~c ( ff , ff~ ) &gt; __ 0 is the corresponding measure for arc labels .</sentence>
				<definiendum id="0">Lex~c</definiendum>
				<definiens id="0">the quality of translation , i.e. the measure of how closely the label</definiens>
			</definition>
			<definition id="3">
				<sentence>= S ( vi , v ' ) Pen ( ffi ) Pen ( ffi ) &gt; _ 0 is the penalty for collapsing the edge ffi , which depends on the value of the label of that edge .</sentence>
				<definiendum id="0">edge ffi</definiendum>
			</definition>
			<definition id="4">
				<sentence>A legitimate pairing P is a set of elements of the matrix M. that conform to the following conditions : tribute at most one element to P , except that the row and the column labeled * may contribute more than one element to P ing to the node pair ( w. w ' ) , and some child node u appears in the Children-Pairing for ( w , w ' ) , then the row or column of u may not contribute any elements to P. We use/.7 ) = £7 ) ( v. v ' ) to denote the set of all legitimate pairings .</sentence>
				<definiendum id="0">legitimate pairing P</definiendum>
				<definiens id="0">a set of elements of the matrix M. that conform to the following conditions : tribute at most one element to P , except that the row and the column labeled * may contribute more than one element to P ing to the node pair ( w. w ' ) , and some child node u appears in the Children-Pairing for ( w , w '</definiens>
			</definition>
			<definition id="5">
				<sentence>such pairings , where d is the greater of the degrees of u and v ' .</sentence>
				<definiendum id="0">d</definiendum>
				<definiens id="0">the greater of the degrees of u and v '</definiens>
			</definition>
			<definition id="6">
				<sentence>An arc ( role ) al with head ( value ) h is written as al : h , where h is a fixed label ( word ) , a substructure or a variable .</sentence>
				<definiendum id="0">h</definiendum>
				<definiens id="0">a fixed label ( word ) , a substructure or a variable</definiens>
			</definition>
			<definition id="7">
				<sentence>, x~ , the target will have n of the leaves labeled with Tr ( xl ) , ... , Tr ( x~ ) , where Tr ( x ) is the texical translation function .</sentence>
				<definiendum id="0">Tr ( x )</definiendum>
				<definiens id="0">the texical translation function</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>Final score prediction for cross-validation uses the weighted predictive feature set identified during training .</sentence>
				<definiendum id="0">Final score prediction for cross-validation</definiendum>
				<definiens id="0">uses the weighted predictive feature set identified during training</definiens>
			</definition>
			<definition id="1">
				<sentence>206 E-rater uses a hybrid feature methodology that incorporates several variables either derived statistically , or extracted through NLP techniques .</sentence>
				<definiendum id="0">E-rater</definiendum>
			</definition>
			<definition id="2">
				<sentence>The GMAT issue essay asks the writer to respond to a general question and to provide `` reasons and/or examples '' to support his or her position on an issue introduced by the test question .</sentence>
				<definiendum id="0">GMAT issue essay</definiendum>
				<definiens id="0">asks the writer to respond to a general question and to provide `` reasons and/or examples '' to support his or her position on an issue introduced by the test question</definiens>
			</definition>
			<definition id="3">
				<sentence>APA uses rules tbr argument annotation and partitioning based on syntactic and paragraphbased distribution of cue words , phrases and structures to identify rhetorical structure .</sentence>
				<definiendum id="0">APA</definiendum>
				<definiens id="0">uses rules tbr argument annotation and partitioning based on syntactic and paragraphbased distribution of cue words , phrases and structures to identify rhetorical structure</definiens>
			</definition>
			<definition id="4">
				<sentence>, , is the frequency of word i in category s , max_freq~ is the frequency of the most frequent word in s ( after a stop list of words has been removed ) , n_essaystot , ,i is the total number of training essays across all six categories , and n_essays~ is the number of training essays containing word i. The first part of the weight formula represents the prominence of word i in the score category , and the second part is the log of the word 's inverse document frequency .</sentence>
				<definiendum id="0">max_freq~</definiendum>
				<definiendum id="1">,i</definiendum>
				<definiendum id="2">n_essays~</definiendum>
				<definiens id="0">the total number of training essays across all six categories , and</definiens>
				<definiens id="1">the number of training essays containing word i. The first part of the weight formula represents the prominence of word i in the score category</definiens>
			</definition>
</paper>

		<paper id="2238">
			<definition id="0">
				<sentence>Inter-dialect differences exist in pronunciation , vocabulary and syntactic rules .</sentence>
				<definiendum id="0">Inter-dialect differences</definiendum>
				<definiens id="0">exist in pronunciation , vocabulary and syntactic rules</definiens>
			</definition>
</paper>

		<paper id="1009">
</paper>

		<paper id="2137">
</paper>

		<paper id="1080">
			<definition id="0">
				<sentence>The P0S category contains only the major part of speech values ( noun ( N ) , verb ( V ) , adjective ( A ) , pronoun ( P ) , verb ( V ) , adjective ( A ) , adverb ( D ) , numeral ( C ) , preposition ( R ) , conjunction ( J ) , interjection ( I ) , particle ( T ) , punctuation ( Z ) , and `` undefined '' ( X ) ) .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">adjective</definiendum>
				<definiendum id="2">V</definiendum>
				<definiendum id="3">adjective</definiendum>
				<definiendum id="4">`` undefined ''</definiendum>
				<definiens id="0">preposition ( R ) , conjunction ( J ) , interjection ( I ) , particle ( T ) , punctuation ( Z ) , and</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , verbs ( POS : V ) are divided into simple finite form in present or future tense ( B ) , conditional ( c ) , infinitive ( f ) , imperative ( i ) , etc. 3 All the categories vary in their size as well as in their unigram entropy ( see Table 1 ) computed using the standard entropy definition Hp = ~ p ( y ) log ( p ( y ) ) ( 1 ) yEY where p is the unigram distribution estimate based on the training data , and Y is the set of possible values of the category in question .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiens id="0">verbs ( POS : V ) are divided into simple finite form in present or future tense ( B ) , conditional ( c ) , infinitive ( f )</definiens>
				<definiens id="1">the unigram distribution estimate based on the training data</definiens>
				<definiens id="2">the set of possible values of the category in question</definiens>
			</definition>
			<definition id="2">
				<sentence>Lemma disambiguation is a separate process following tagging .</sentence>
				<definiendum id="0">Lemma disambiguation</definiendum>
				<definiens id="0">a separate process following tagging</definiens>
			</definition>
			<definition id="3">
				<sentence>Such model ( when predicting an event 5 y E Y in a context x ) has the general form PAC , e ( YIX ) = exp ( ~-~in -- -- 1 Aifi ( y , x ) ) Z ( x ) ( 3 ) where fi ( Y , x ) is the set ( of size n ) of binary-valued ( yes/no ) features of the event value being predicted and its context , hi is a `` weigth '' ( in the exponential sense ) of the feature fi , and the normalization factor Z ( x ) is defined naturally as z ( x ) = exp ( z x ) ) ( 4 ) yEY i -- -- 1 ~ , Ve use a separate model for each ambiguity class AC ( which actually appeared in the training data ) of each of the 13 morphological categories 6 .</sentence>
				<definiendum id="0">fi ( Y , x )</definiendum>
				<definiendum id="1">hi</definiendum>
				<definiens id="0">actually appeared in the training data</definiens>
			</definition>
			<definition id="4">
				<sentence>full cross-product of the category being predicted ( y ) and of the x specified as a combination of : may be different from the category being predicted , or and text , or four positions away ) having a certain ambiguity class in the POS category Let now Categories = { POS , SUBPOS , GENDER , NUMBER , CASE , POSSGENDER , POSSNUMBER , PERSON , TENSE , GRADE , NEGATION , VOICE , VAR } ; then the feature function fcatAc , ~ , ~ ( Y , X ) ~ { 0 , 1 } is well-defined iff where Cat E Categories and CatAC is the ambiguity class AC ( such as AN , for adjective/noun ambiguity of the part of speech category ) of a morphological category Cat ( such as POS ) .</sentence>
				<definiendum id="0">PERSON</definiendum>
				<definiendum id="1">CatAC</definiendum>
				<definiens id="0">well-defined iff where Cat E</definiens>
				<definiens id="1">the ambiguity class AC ( such as AN , for adjective/noun ambiguity of the part of speech category</definiens>
			</definition>
			<definition id="5">
				<sentence>• SAC be the set of features selected so far for a model for ambiguity class AC , • PSac ( Yl d ) the probability , using model ( 3-5 ) with features SAC , of subtag y in a context defined by position d in the training data , and • FAC , ~ be the set ( `` batch '' ) of features sharing the same context ~ , i.e. FAc , = { f FAc : : S = ( 9 ) Note that the size of AC is equal to the size of any batch of features ( \ [ AC\ [ = \ [ FAc , ~\ [ for any z ) .</sentence>
				<definiendum id="0">• FAC , ~</definiendum>
				<definiendum id="1">FAc , ~\ [</definiendum>
				<definiens id="0">the set of features selected so far for a model for ambiguity class AC , • PSac ( Yl d ) the probability , using model ( 3-5 ) with features SAC , of subtag y in a context defined by position d in the training data , and</definiens>
				<definiens id="1">{ f FAc : : S = ( 9 ) Note that the size of AC is equal to the size of any batch of features ( \ [ AC\ [ = \ [</definiens>
			</definition>
			<definition id="6">
				<sentence>~ ( Yid ) over all y E AC .</sentence>
				<definiendum id="0">Yid</definiendum>
			</definition>
			<definition id="7">
				<sentence>The probability defined by the formula ( 11 ) can easily be computed despite its ugly general form , as the denominator is in fact the number of ( positive ) occurrences of all the features from the batch defined by the context ~ in the training data .</sentence>
				<definiendum id="0">denominator</definiendum>
				<definiens id="0">the formula ( 11 ) can easily be computed despite its ugly general form</definiens>
			</definition>
</paper>

		<paper id="2200">
			<definition id="0">
				<sentence>Whether screening whole populations of children , or assessing individual referrals , the articulation test is an important tool for the speech clinician .</sentence>
				<definiendum id="0">articulation test</definiendum>
				<definiens id="0">an important tool for the speech clinician</definiens>
			</definition>
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>Introduction Acquisition and exploitation of dictionary resources ( DRs ) ( machine-readable , on-line dictionaries , computational lexicons , etc ) have long been recognized as important and difficult problems .</sentence>
				<definiendum id="0">DRs )</definiendum>
				<definiens id="0">machine-readable , on-line dictionaries , computational lexicons</definiens>
			</definition>
			<definition id="1">
				<sentence>Our tool consists of a syntax-directed translation ( SDT ) formalism called h-grammar , and its running engine .</sentence>
				<definiendum id="0">tool</definiendum>
			</definition>
			<definition id="2">
				<sentence>An h-grammar is a context-free grammar augmented with variables and actions .</sentence>
				<definiendum id="0">h-grammar</definiendum>
				<definiens id="0">a context-free grammar augmented with variables and actions</definiens>
			</definition>
			<definition id="3">
				<sentence>The computational aspect , in which we are interested , is how to do production .</sentence>
				<definiendum id="0">computational aspect</definiendum>
				<definiens id="0">how to do production</definiens>
			</definition>
</paper>

		<paper id="2184">
</paper>

		<paper id="1094">
			<definition id="0">
				<sentence>( occlusion ) , the source elements H ( voicelessness ) , L ( non-spontaneous voicing ) and N ( nasality ) , and the resonance elements A ( low ) , I ( palatal ) , U ( labial ) and R ( coronal ) .</sentence>
				<definiendum id="0">N ( nasality</definiendum>
				<definiens id="0">labial ) and R ( coronal )</definiens>
			</definition>
			<definition id="1">
				<sentence>The normalisation is a simple form of time-warping without the computational complexity of dynamic time-warping or Hidden Markov Models ( HMMs ) .</sentence>
				<definiendum id="0">normalisation</definiendum>
			</definition>
</paper>

		<paper id="2220">
			<definition id="0">
				<sentence>The pattern matching consists of splitting those consonant clusters that can not be pronounced within the Chinese phonemic set .</sentence>
				<definiendum id="0">pattern matching</definiendum>
				<definiens id="0">consists of splitting those consonant clusters that can not be pronounced within the Chinese phonemic set</definiens>
			</definition>
			<definition id="1">
				<sentence>The Pinyin romanization consists of elements that can be described as consonants ( including three consonant clusters `` zh '' , `` ch '' and `` sh '' ) and vowels which consist of monothongs , diphthongs and vowels followed by a nasal In/ or /rj/ .</sentence>
				<definiendum id="0">Pinyin romanization</definiendum>
				<definiens id="0">consists of elements that can be described as consonants ( including three consonant clusters `` zh '' , `` ch '' and `` sh '' ) and vowels which consist of monothongs , diphthongs and vowels followed by a nasal In/ or /rj/</definiens>
			</definition>
</paper>

		<paper id="2129">
			<definition id="0">
				<sentence>We analyze our data using both traditional hypothesis testing methods and the PARADISE ( Walker et al. , 1997 ; Walker et al. , 1998 ) methodology for estimating a performance function .</sentence>
				<definiendum id="0">PARADISE</definiendum>
				<definiens id="0">Walker et al. , 1997 ; Walker et al. , 1998 ) methodology for estimating a performance function</definiens>
			</definition>
			<definition id="1">
				<sentence>TOOT is implemented using a platform for spoken dialogue agents ( Kamm et al. , 1997 ) that combines automatic speech recognition ( ASR ) , textto-speech ( TTS ) , a phone interface , and modules for specifying a dialogue manager and application functions .</sentence>
				<definiendum id="0">TOOT</definiendum>
				<definiens id="0">implemented using a platform for spoken dialogue agents ( Kamm et al. , 1997 ) that combines automatic speech recognition</definiens>
			</definition>
			<definition id="2">
				<sentence>We predicted that optimal performance would occur whenever the correct task solution was included in TOOT ' s initial reTask 1 ( Exact-Match ) : Try to find a train going to Boston from New York City on Saturday at 6:00 pro .</sentence>
				<definiendum id="0">Exact-Match )</definiendum>
				<definiens id="0">Try to find a train going to Boston from New York City on Saturday at 6:00 pro</definiens>
			</definition>
			<definition id="3">
				<sentence>Task2 ( No-Match-l ) : Try to find a train going to Chicago from Philadelphia on Sunday at 10:30 am .</sentence>
				<definiendum id="0">Task2 ( No-Match-l )</definiendum>
			</definition>
			<definition id="4">
				<sentence>Task4 ( Too-Much-Info/Early-Answer ) : Try to find a train going to Philadelphia from New York City on the weekend at 4:00 pro .</sentence>
				<definiendum id="0">Task4 ( Too-Much-Info/Early-Answer )</definiendum>
				<definiens id="0">Try to find a train going to Philadelphia from New York City on the weekend at 4:00 pro</definiens>
			</definition>
			<definition id="5">
				<sentence>We plan to evaluate additional cooperative response strategies in TOOT ( e.g. , intensional summaries ( Kalita et al. , 1986 ) , summarization and constraint elicitation in isolation ) , and to combine TOOT data with data from other agents ( Walker et al. , 1998 ) .</sentence>
				<definiendum id="0">intensional summaries</definiendum>
			</definition>
</paper>

		<paper id="2242">
			<definition id="0">
				<sentence>ILEX is an adaptive hypertext system generating museum object descriptions .</sentence>
				<definiendum id="0">ILEX</definiendum>
			</definition>
			<definition id="1">
				<sentence>A revised version of Text Structure ( TS ) ( Meteer , 1992 ) is used as an intermediate level of representation between the text planner and the sentence realiser , which provides syntactic constraints to the text planner while abstracting away from linguistic details .</sentence>
				<definiendum id="0">revised version of Text Structure ( TS )</definiendum>
				<definiendum id="1">sentence realiser</definiendum>
				<definiens id="0">provides syntactic constraints to the text planner while abstracting away from linguistic details</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , the definition of one rule that embeds a prepositional phrase is : ( def-embed-rule : name with-phrase ; the name of this rule : priority 4 : type prep-phrase ; the type of embedding : constraints ( ( : type pred Generalized-Possession ) ( : type refer ( : or demonstrative indefinite ) ) ) : RT ( ( : rel-parent Adjunct ) ( : textual-sem With-Prep-phrase ) ) ) In the definition , priority is the order in which the rule should be tried , where those rules producing simpler syntactic forms always have higher priority ( Scott and de Souza , 1990 ) ; constraints is the restrictions that must be satisfied by the predicate and arguments of the embedded fact and the realisation of the referring part .</sentence>
				<definiendum id="0">priority</definiendum>
				<definiendum id="1">constraints</definiendum>
				<definiens id="0">the restrictions that must be satisfied by the predicate and arguments of the embedded fact and the realisation of the referring part</definiens>
			</definition>
			<definition id="3">
				<sentence>Prepositional phrases fall between adjectives and relative clauses in their effect .</sentence>
				<definiendum id="0">Prepositional phrases</definiendum>
			</definition>
</paper>

		<paper id="1107">
			<definition id="0">
				<sentence>EPC uses such error pattems for correction .</sentence>
				<definiendum id="0">EPC</definiendum>
				<definiens id="0">uses such error pattems for correction</definiens>
			</definition>
			<definition id="1">
				<sentence>c &gt; 3 EPC is a simple and effective method because it detects and corrects errors only by pattern-matching .</sentence>
				<definiendum id="0">EPC</definiendum>
				<definiens id="0">a simple and effective method because it detects and corrects errors only by pattern-matching</definiens>
			</definition>
			<definition id="2">
				<sentence>For two arbitrary candidates , when one of their Error-Parts includes the other , and their frequencies are the same value , the candidate whose Error-Part includes the other is accepted .</sentence>
				<definiendum id="0">frequencies</definiendum>
				<definiens id="0">the same value , the candidate whose Error-Part includes the other is accepted</definiens>
			</definition>
			<definition id="3">
				<sentence>Instead of this portion of the regular expression , SSC uses a collection of strings , the members of which are in the corpus ( this collection we refer to as the String-Database ) .</sentence>
				<definiendum id="0">SSC</definiendum>
				<definiens id="0">uses a collection of strings , the members of which are in the corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>Here , P1 and P2 denote the start and end positions of an error-block , and L denotes the length of the input string .</sentence>
				<definiendum id="0">L</definiendum>
				<definiens id="0">the length of the input string</definiens>
			</definition>
			<definition id="5">
				<sentence>Here , S is defined as : S= ( L-N ) /L where L is the len~uh of the Similar String , and N is the minimum number of character insertions , deletions , or substitutions necessary to transform the Error-String to the Similar-String .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">S= ( L-N</definiendum>
				<definiendum id="2">L</definiendum>
				<definiendum id="3">N</definiendum>
				<definiens id="0">the minimum number of character insertions , deletions , or substitutions necessary to transform the Error-String to the Similar-String</definiens>
			</definition>
</paper>

		<paper id="1086">
</paper>

		<paper id="2218">
			<definition id="0">
				<sentence>Nu~l arc TA ... .. TB lc Figure 2 Keyword Pair Model The model consists of two sets of words ( keywordsl and keywords2 ) before and after the synchronisation point ( point B ) .</sentence>
				<definiendum id="0">model</definiendum>
			</definition>
			<definition id="1">
				<sentence>Each HMM is a three-loop , eight-mixture-distribution HM .</sentence>
				<definiendum id="0">HMM</definiendum>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>Terminology is an interesting kind of multiword expressions because such expressions are almost but not completely fixed , and there is an intuition that you wo n't loose many good anal~This integration has been done by Fr6d~rique Segond .</sentence>
				<definiendum id="0">Terminology</definiendum>
				<definiens id="0">an interesting kind of multiword expressions because such expressions are almost but not completely fixed</definiens>
			</definition>
			<definition id="1">
				<sentence>The tokenization process consists of splitting an input string into tokens , ( Grefenstette and Tapanainen , 1994 ) , ( Ait-Mokthar , 1997 ) , i.e. determining the word boundaries .</sentence>
				<definiendum id="0">tokenization process</definiendum>
				<definiens id="0">consists of splitting an input string into tokens</definiens>
			</definition>
</paper>

		<paper id="2179">
			<definition id="0">
				<sentence>Secondly , RST offers no way of capturing higher level organisational units , such as Modus Ponens , Modus Tollens , and so on .</sentence>
				<definiendum id="0">RST</definiendum>
				<definiens id="0">offers no way of capturing higher level organisational units</definiens>
			</definition>
			<definition id="1">
				<sentence>Strengthening the notion of inductive generalisation is a trivial task of increasing the minimum number of premises which must exist for the application of IG to be licensed .</sentence>
				<definiendum id="0">inductive generalisation</definiendum>
				<definiens id="0">a trivial task of increasing the minimum number of premises which must exist for the application of IG to be licensed</definiens>
			</definition>
</paper>

		<paper id="1103">
			<definition id="0">
				<sentence>To update the context with topics we introduce the Predict-Support algorithm which selects utterance topics on the basis of topic transitions described in the topic tree and words recognized in the current utterance .</sentence>
				<definiendum id="0">Predict-Support algorithm</definiendum>
				<definiens id="0">selects utterance topics on the basis of topic transitions described in the topic tree and words recognized in the current utterance</definiens>
			</definition>
			<definition id="1">
				<sentence>The topic ( focus ) is a means to describe thematically coherent discourse structure , and its use has been mainly supported by arguments regarding anaphora resolution and processing effort ( search space limits ) .</sentence>
				<definiendum id="0">topic ( focus )</definiendum>
				<definiens id="0">a means to describe thematically coherent discourse structure</definiens>
			</definition>
			<definition id="2">
				<sentence>The topic model consists of the following parts : between words and topic types ity between the predicted topics and the topics supported by the input utterance .</sentence>
				<definiendum id="0">topic model</definiendum>
				<definiens id="0">consists of the following parts : between words and topic types ity between the predicted topics and the topics supported by the input utterance</definiens>
			</definition>
			<definition id="3">
				<sentence>The focus tree is a subgraph of the world knowledge , built in the course of the discourse on the basis of the utterances that have occurred .</sentence>
				<definiendum id="0">focus tree</definiendum>
				<definiens id="0">a subgraph of the world knowledge , built in the course of the discourse on the basis of the utterances that have occurred</definiens>
			</definition>
			<definition id="4">
				<sentence>may still get an incorrect topic , even though lunch is a known word : mutual information mi ( included , RoomPrice ) may be greater than mi ( lunch , Meals ) .</sentence>
				<definiendum id="0">RoomPrice )</definiendum>
				<definiens id="0">a known word : mutual information mi ( included ,</definiens>
			</definition>
			<definition id="5">
				<sentence>PP is the corpus perplexity which represents the average branching factor of the corpus , or the number of alternatives from which to choose the correct label at a given point .</sentence>
				<definiendum id="0">PP</definiendum>
				<definiens id="0">the corpus perplexity which represents the average branching factor of the corpus</definiens>
			</definition>
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>Knowledge-based systems as Grosz and Sidner 's ( 1986 ) require an extensive manual knowledge engineering effort to create the knowledge base ( semantic network and/or frames ) and this is only possible in very limited and well-known domains .</sentence>
				<definiendum id="0">Knowledge-based systems</definiendum>
				<definiens id="0">an extensive manual knowledge engineering effort to create the knowledge base ( semantic network and/or frames</definiens>
			</definition>
			<definition id="1">
				<sentence>Descriptor values are the number of occurrences of the words in the unit , modified by the word distribution in the text .</sentence>
				<definiendum id="0">Descriptor values</definiendum>
				<definiens id="0">the number of occurrences of the words in the unit , modified by the word distribution in the text</definiens>
			</definition>
			<definition id="2">
				<sentence>A basic discourse unit , here a paragraph , is represented as a term vector Gi = ( gil , gi2 , ... , git ) where gi is the number of occurrences of a given descriptor in Gi .</sentence>
				<definiendum id="0">gi</definiendum>
				<definiens id="0">the number of occurrences of a given descriptor in Gi</definiens>
			</definition>
			<definition id="3">
				<sentence>idf which is an indicator of the importance of a term according to its distribution in a text .</sentence>
				<definiendum id="0">idf</definiendum>
				<definiens id="0">an indicator of the importance of a term according to its distribution in a text</definiens>
			</definition>
			<definition id="4">
				<sentence>log where tfij is the number of occurrences of a descriptor Tj in a paragraph i ; dfi is the number of paragraphs in which Tj occurs and 393 N the total number of paragraphs in the text .</sentence>
				<definiendum id="0">tfij</definiendum>
				<definiendum id="1">dfi</definiendum>
				<definiens id="0">the number of paragraphs in which Tj occurs and 393 N the total number of paragraphs in the text</definiens>
			</definition>
			<definition id="5">
				<sentence>The distance measure is the Dice coefficient , defined for two vectors X= ( x 1 , x2 ... .. xt ) and Y= ( Yl , Y2 ... .. Yt ) by : C ( X , Y ) = t 2 w ( xi ) w ( yi ) i=l t t w ( xi ) 2÷ w ( yi ) 2 i=l i=l where w ( xi ) is the number of occurrences of a descriptor xi weighted by tf .</sentence>
				<definiendum id="0">distance measure</definiendum>
				<definiens id="0">the number of occurrences of a descriptor xi weighted by tf</definiens>
			</definition>
</paper>

		<paper id="2205">
			<definition id="0">
				<sentence>The Service is responsible \ [ or eight species ot '' marine mammals under the jurisdiction of the Department of the Interior , as assigned by the Marine Mammal Protection Act of 1972 .</sentence>
				<definiendum id="0">Service</definiendum>
				<definiens id="0">responsible \ [ or eight species ot '' marine mammals under the jurisdiction of the Department of the Interior</definiens>
			</definition>
			<definition id="1">
				<sentence>The final score for each passage , normalized for its length , is a weighted sum of a number of minor scores , using the following formula : 5 1 score ( paragraph ) = -\ [ • E w~ • S~ ( 1 ) h where Sa is a minor score calculated using metric h ; wh is the weight reflecting how effective this metric is in general ; l is the length of the segment .</sentence>
				<definiendum id="0">wh</definiendum>
				<definiens id="0">a weighted sum of a number of minor scores , using the following formula : 5 1 score ( paragraph ) = -\ [ • E w~ • S~ ( 1 ) h where Sa is a minor score calculated using metric h ;</definiens>
				<definiens id="1">the weight reflecting how effective this metric is in general ; l is the length of the segment</definiens>
			</definition>
			<definition id="2">
				<sentence>other than the common stopwords ) are ranked by a passage-level inverted frequency distribution , e.g. , N/pf , where pf is the number of passages containing the term and N is the total number of passages contained in a document .</sentence>
				<definiendum id="0">pf</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">the number of passages containing the term</definiens>
				<definiens id="1">the total number of passages contained in a document</definiens>
			</definition>
</paper>

		<paper id="1049">
</paper>

		<paper id="2189">
			<definition id="0">
				<sentence>1159 • Let TRSH be the lexical cohesion threshold TU be the current text unit LC Tu be the current lexical cohesion score of TU ( i.e. LC Tv is the count of tokenized words TU shares with some other text unit ) CLevel .</sentence>
				<definiendum id="0">LC Tv</definiendum>
			</definition>
</paper>

		<paper id="1108">
			<definition id="0">
				<sentence>Japanese is one of them .</sentence>
				<definiendum id="0">Japanese</definiendum>
			</definition>
			<definition id="1">
				<sentence>The Decision-Tree model consists of a set of questions structured into a dendrogram with a probability distribution associated with each leaf of the tree .</sentence>
				<definiendum id="0">Decision-Tree model</definiendum>
				<definiens id="0">consists of a set of questions structured into a dendrogram with a probability distribution associated with each leaf of the tree</definiens>
			</definition>
			<definition id="2">
				<sentence>In general , a decision-tree is a complex of n-ary branching trees in which questions are associated with each parent node , and a choice or class is associated with each child node .</sentence>
				<definiendum id="0">decision-tree</definiendum>
				<definiens id="0">a complex of n-ary branching trees in which questions are associated with each parent node , and a choice or class is associated with each child node</definiens>
			</definition>
			<definition id="3">
				<sentence>Our proposed morphological analyzer processes each character in a string from left to right .</sentence>
				<definiendum id="0">morphological analyzer</definiendum>
				<definiens id="0">processes each character in a string from left to right</definiens>
			</definition>
			<definition id="4">
				<sentence>In practical contexts , the system a3Here , a dictionary is a listing of words attached to part-of-speech tags .</sentence>
				<definiendum id="0">dictionary</definiendum>
			</definition>
</paper>

		<paper id="1068">
			<definition id="0">
				<sentence>• Grammar is a set of connection rules .</sentence>
				<definiendum id="0">Grammar</definiendum>
				<definiens id="0">a set of connection rules</definiens>
			</definition>
</paper>

		<paper id="2251">
			<definition id="0">
				<sentence>Mikheev ( Mikheev , 1996 ; Mikheev , 1997 ) uses a general purpose lexicon to learn affix and word ending information to be used in tagging unknown words .</sentence>
				<definiendum id="0">Mikheev</definiendum>
				<definiens id="0">uses a general purpose lexicon to learn affix and word ending information to be used in tagging unknown words</definiens>
			</definition>
</paper>

		<paper id="2248">
			<definition id="0">
				<sentence>This paper explores the problem of lexical selection in machine translation ( MT ) : a given source language ( SL ) word can often be translated into different target language ( TL ) words , depending on the context .</sentence>
				<definiendum id="0">TL</definiendum>
				<definiens id="0">the problem of lexical selection in machine translation ( MT ) : a given source language ( SL</definiens>
			</definition>
			<definition id="1">
				<sentence>The relevance r i for each dimension is defined as the ratio of the standard deviation si of the distribution formed by dimension i , for all local context words LC , over the maximum standard deviation Smax for LC : si ri= Smax For each candidate translation t the vector representing each word c in LC is moved to a new position in the space according to a function of r and its current distance from t : c'=ci+ri ( ti-ci ) If r is large , then any difference in the value of component i between t and LC is made less prominent than if r is small .</sentence>
				<definiendum id="0">LC</definiendum>
				<definiens id="0">the ratio of the standard deviation si of the distribution formed by dimension i , for all local context words LC , over the maximum standard deviation Smax for LC : si ri= Smax For each candidate translation t the vector representing each word c in LC is moved to a new position in the space according to a function of r and its current distance from t : c'=ci+ri ( ti-ci ) If r is large , then any difference in the value of component i between t and</definiens>
			</definition>
</paper>

		<paper id="2230">
			<definition id="0">
				<sentence>An ITG consists of context-free productions where terminal symbols come in couples , for example x/y , where x is a English word and y is an Chinese translation of x , with singletons of the form x/e or e/y representing function words that are used in only one of the languages .</sentence>
				<definiendum id="0">ITG</definiendum>
				<definiendum id="1">y</definiendum>
				<definiens id="0">consists of context-free productions where terminal symbols come in couples</definiens>
				<definiens id="1">a English word</definiens>
			</definition>
			<definition id="1">
				<sentence>t. K is the maximium number of consecutive English words that can be skipped .</sentence>
				<definiendum id="0">t. K</definiendum>
				<definiens id="0">the maximium number of consecutive English words that can be skipped</definiens>
			</definition>
</paper>

		<paper id="1079">
			<definition id="0">
				<sentence>The text parser ( for an overview , cf. BrSker et al. ( 1994 ) ) yields information from the grammatical constructions in which an unknown lexical item ( symbolized by the black square ) occurs in terms of the corresponding dependency parse tree .</sentence>
				<definiendum id="0">text parser</definiendum>
				<definiens id="0">yields information from the grammatical constructions in which an unknown lexical item ( symbolized by the black square ) occurs in terms of the corresponding dependency parse tree</definiens>
			</definition>
			<definition id="1">
				<sentence>Concepts are unary predicates , roles are binary predicates over a domain A , with individuals being the elements of A. We assume a common set-theoretical semantics for C7 ) £ an interpretation Z is a function that assigns to each concept symbol ( the set A ) a subset of the domain A , Z : A -+ 2 n , to each role symbol ( the set P ) a binary relation of A , Z : P -- + 2 ~×n , and to each individual symbol ( the set I ) an element of A , Z : I -- + A. Concept terms and role terms are defined inductively .</sentence>
				<definiendum id="0">Concepts</definiendum>
				<definiendum id="1">Z</definiendum>
				<definiens id="0">unary predicates , roles are binary predicates over a domain A , with individuals being the elements of A. We assume a common set-theoretical semantics for C7 ) £ an interpretation Z is a function that assigns to each concept symbol ( the set A ) a subset of the domain A ,</definiens>
				<definiens id="1">the set P ) a binary relation of A , Z : P -- + 2 ~×n , and to each individual symbol ( the set I ) an element of A , Z : I -- + A. Concept terms and role terms</definiens>
			</definition>
			<definition id="2">
				<sentence>R z ( d ) represents the set of role fillers of the individual d , i.e. , the set of individuals e with ( d , e ) E R z. By means of terminological axioms ( for a subset , see Table 2 ) a symbolic name can be introduced for each concept to which are assigned necessary and sufficient constraints using the definitional operator ' '' = .</sentence>
				<definiendum id="0">R z</definiendum>
				<definiens id="0">the set of role fillers of the individual d , i.e. , the set of individuals e with ( d , e ) E R z. By means of terminological axioms</definiens>
			</definition>
			<definition id="3">
				<sentence>An interpretation Z is a model of an ABox with regard to a TBox , iff Z satisfies the assertional and terminological axioms .</sentence>
				<definiendum id="0">interpretation Z</definiendum>
				<definiens id="0">a model of an ABox with regard to a TBox , iff Z satisfies the assertional and terminological axioms</definiens>
			</definition>
			<definition id="4">
				<sentence>Given the structural foundations of terminological theories , two dimensions of conceptual learning can be distinguished -the taxonomic one by which new concepts are located in conceptual hierarchies , and the aggregational one by which concepts are supplied with clusters of conceptual relations ( these will be used subsequently by the terminological classifier to determine the current position of the item to be learned in the taxonomy ) .</sentence>
				<definiendum id="0">conceptual relations</definiendum>
				<definiens id="0">the terminological classifier to determine the current position of the item to be learned in the taxonomy )</definiens>
			</definition>
			<definition id="5">
				<sentence>roleSet ; gen-hypo creates a new hypothesis space by asserting the given axioms of h and outputs its identifier .</sentence>
				<definiendum id="0">gen-hypo</definiendum>
				<definiens id="0">creates a new hypothesis space by asserting the given axioms of h and outputs its identifier</definiens>
			</definition>
			<definition id="6">
				<sentence>The M-Deduction rule ( see Table 6 ) is triggered for any repetitive assignment of the same role filler to one specific conceptual relation that occurs in different hypothesis spaces .</sentence>
				<definiendum id="0">M-Deduction rule</definiendum>
				<definiens id="0">any repetitive assignment of the same role filler to one specific conceptual relation that occurs in different hypothesis spaces</definiens>
			</definition>
			<definition id="7">
				<sentence>EXISTS 01,02 , 03 , R , h : ( 01 R 02 ) h A ( 01 R 03 ) h A ( 01 : ACTION ) h ===V I TELL ( 01 R o~_ ) h : ADDFILLER Table 8 : The Rule AddFiller We give examples both for the assignmeut of an ADDFILLER as well as for a SUPPORT label : Examples : ( produces.1 : ACTION ) h A ( produces.1 AGENT ltoh ) h A ( produces.1 AGENT IBM ) h ( produces.1 AGENT Itoh ) h : ADDFILLER ( ltoh-Ci-8 : PRINTER ) h A ( Itoh-Ct : PRINTER ) h A ( Itoh SELLS Itoh-Ci-8 ) h A ( Itoh SELLS Itoh-Ct ) h A ( ltoh : -~AcTION ) h ( Itoh-Ci-8 : PRINTER ) h : SUPPORT 480 The criteria from which concept hypotheses are derived differ in the dimension from which they are drawn ( grammatical vs. conceptual evidence ) , as well as the strength by which they lend support to the corresponding hypotheses ( e.g. , apposition vs. genitive , multiple deduction vs. additional role filling , etc. ) .</sentence>
				<definiendum id="0">PRINTER ) h A ( Itoh</definiendum>
				<definiendum id="1">Itoh SELLS Itoh-Ct ) h A ( ltoh</definiendum>
				<definiens id="0">derived differ in the dimension from which they are drawn ( grammatical vs. conceptual evidence</definiens>
			</definition>
</paper>

		<paper id="2191">
			<definition id="0">
				<sentence>If there exist ( xl , Yl &gt; , ... , { XT , YT ) 6 X × Y such that each xi is translated into Yi in the parallel corpora X , Y , then its empirical probability distribution/5 obtained from observed training data is defined by : p ( x , y ) c ( x , y ) ( 1 ) Ex , c ( x , y ) where c ( x , y ) is the number of times that x is translated into y in the training data .</sentence>
				<definiendum id="0">Ex , c ( x , y ) where c</definiendum>
				<definiendum id="1">y )</definiendum>
				<definiens id="0">the number of times that x is translated into y in the training data</definiens>
			</definition>
			<definition id="1">
				<sentence>&lt; ( x , y ) c ( x , y ) = T ( 2 ) where X~ is i-th sentence in X. We denote that sentence Xi is translated into sentence Y/ in aligned parallel corpora .</sentence>
				<definiendum id="0">X~</definiendum>
				<definiens id="0">i-th sentence in X. We denote that sentence Xi is translated into sentence Y/ in aligned parallel corpora</definiens>
			</definition>
			<definition id="2">
				<sentence>This is called feature function , which expresses statistical properties of a language model .</sentence>
				<definiendum id="0">function</definiendum>
				<definiens id="0">expresses statistical properties of a language model</definiens>
			</definition>
			<definition id="3">
				<sentence>When there are feature functions fi ( i E { 1 , 2 , ... , n } ) which are important to modeling processes , the distribution p we estimate should be included in a set of distributions defined such as : C = { p E 7 9 I P ( fi ) =16 ( fi ) for i E { 1,2 , ... , n } } ( 6 ) where P is a set of all possible distributions onX×Y. For the distribution p , there is no assumption except equation ( 6 ) , so it is reasonable that the most uniform distribution is the most suitable for the training corpora .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">n } ) which are important to modeling processes</definiens>
			</definition>
			<definition id="4">
				<sentence>Then , to solve p. E C in equation ( 8 ) is equivalent to solve h. that maximize the loglikelihood : = ( x ) log zj , ( z ) + x i ( 10 ) h. = argmax kV ( h ) Such h. can be solved by one of the numerical algorithm called the Improved Iteratire Scaling Algorithm ( Berger et al. , 1996 ) .</sentence>
				<definiendum id="0">Iteratire Scaling Algorithm</definiendum>
				<definiens id="0">= argmax kV ( h ) Such h. can be solved by one of the numerical algorithm called the Improved</definiens>
			</definition>
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>Structural misalignment is treated in semantics construction involving a restriction operator ( Kaplan and Wedekind , 1993 ) where f-structures are related to ( possibly sets of ) disambiguated semantic representations .</sentence>
				<definiendum id="0">Structural misalignment</definiendum>
				<definiens id="0">sets of ) disambiguated semantic representations</definiens>
			</definition>
			<definition id="1">
				<sentence>2 ( lc ) is the set of terms representing ( la ) .</sentence>
				<definiendum id="0">lc )</definiendum>
				<definiens id="0">the set of terms representing ( la )</definiens>
			</definition>
			<definition id="2">
				<sentence>Transfer works on source language ( SL ) and target language ( TL ) sets of terms representing predicates , roles , etc. like the ones shown in ( lc ) .</sentence>
				<definiendum id="0">Transfer</definiendum>
				<definiens id="0">works on source language ( SL ) and target language ( TL ) sets of terms representing predicates , roles , etc. like the ones shown in ( lc )</definiens>
			</definition>
			<definition id="3">
				<sentence>3 Appendix B gives a relational formulation of the correspondence between f-structures and UDRSs .</sentence>
				<definiendum id="0">Appendix B</definiendum>
				<definiens id="0">gives a relational formulation of the correspondence between f-structures</definiens>
			</definition>
			<definition id="4">
				<sentence>A wide scope rule would be { hDJN ( E , X ) } # { HS ( E1 ) , XC0~IP ( E1 , E ) } ~- } { ADJN ( EI , X ) } where HS ( E1 ) is a `` marker '' on the switched adverbial 's node El .</sentence>
				<definiendum id="0">HS</definiendum>
				<definiens id="0">a `` marker '' on the switched adverbial 's node El</definiens>
			</definition>
</paper>

		<paper id="2246">
			<definition id="0">
				<sentence>A bi-directional FLLB is similar to a split FLLB , but uses all available space in the FLLB .</sentence>
				<definiendum id="0">bi-directional FLLB</definiendum>
				<definiens id="0">similar to a split FLLB , but uses all available space in the FLLB</definiens>
			</definition>
			<definition id="1">
				<sentence>The training set for the ANN consists of 1449 word/symbol pairs .</sentence>
				<definiendum id="0">training set for the ANN</definiendum>
			</definition>
</paper>

		<paper id="2195">
			<definition id="0">
				<sentence>The overall architecture of a MELISSA-based NL interface consists of the following software modules : • Speech Recognition Module ( SRM ) , which is a commercial product , providing a continuous speech interface for the other NL modules • Linguistic Processing Module ( LPM ) consisting of the linguistic processing machinery and the linguistic resources • Semantic Analysis Module ( SAM ) interpreting LPM output using application knowledge • Function Generator Module ( FGM ) converting SAM output into executable function calls • Application Knowledge Repository ( AKR ) containing all the relevant application specific knowledge being used by SAM and FGM • Front-End Module ( FEM ) responsible for invoking requested operations in the application • Controller Module ( CTR ) co-ordinating the cooperation between the previous modules • End-User Interface ( EUI ) in which the user types or dictates his NL queries to target application The focus of MELISSA is on understanding NL .</sentence>
				<definiendum id="0">overall architecture of a MELISSA-based NL interface</definiendum>
				<definiendum id="1">SAM</definiendum>
				<definiendum id="2">FGM • Front-End Module</definiendum>
				<definiendum id="3">End-User Interface</definiendum>
				<definiendum id="4">EUI</definiendum>
				<definiens id="0">consists of the following software modules : • Speech Recognition Module ( SRM ) , which is a commercial product , providing a continuous speech interface for the other NL modules • Linguistic Processing Module ( LPM ) consisting of the linguistic processing machinery and the linguistic resources • Semantic Analysis Module ( SAM ) interpreting LPM output using application knowledge • Function Generator Module ( FGM ) converting SAM output into executable function calls • Application Knowledge Repository ( AKR ) containing all the relevant application specific knowledge being used by</definiens>
			</definition>
			<definition id="1">
				<sentence>Speech is the most natural form of communication for people and is felt to greatly extend the range of potential applications suitable for an NL interface .</sentence>
				<definiendum id="0">Speech</definiendum>
				<definiens id="0">the most natural form of communication for people</definiens>
			</definition>
			<definition id="2">
				<sentence>ALEP provides the functionality for efficient NLP : a 'lean ' linguistic formalism ( with term unification ) providing typed feature structures ( TFSs ) , an efficient head scheme based parser , rule indexation mechanisms , a number of devices supporting modularization and configuration of linguistic resources , e.g. an interface format supporting information flow from SGML-encoded data structures to TFSs ( thus enabling straightforward integration of 'low-level ' processing with deep linguistic analysis ) , the refinement facility allowing for separating parsing and 'semantic decoration ' , and the specifier mechanism allowing for multi-dimensional partitioning of linguistic resources into specialized sub-modules .</sentence>
				<definiendum id="0">ALEP</definiendum>
				<definiens id="0">a 'lean ' linguistic formalism ( with term unification ) providing typed feature structures ( TFSs ) , an efficient head scheme based parser , rule indexation mechanisms , a number of devices supporting modularization and configuration of linguistic resources</definiens>
			</definition>
			<definition id="3">
				<sentence>This is achieved by a ( do1195 main-independent ) matching process , which attempts to unify each of the LPM results with one or more so-called mapping rules .</sentence>
				<definiendum id="0">matching process</definiendum>
				<definiens id="0">attempts to unify each of the LPM results with one or more so-called mapping rules</definiens>
			</definition>
			<definition id="4">
				<sentence>The SFs embody structural semantic information , but also very important constraint information , derived from the text-handling .</sentence>
				<definiendum id="0">SFs embody</definiendum>
			</definition>
			<definition id="5">
				<sentence>NEC allows for the representation of events , preconditions , postcondifions and time intervals between events .</sentence>
				<definiendum id="0">NEC</definiendum>
				<definiens id="0">allows for the representation of events , preconditions , postcondifions and time intervals between events</definiens>
			</definition>
			<definition id="6">
				<sentence>add ( open ( F ) , \ [ open ( F ) \ ] ) .</sentence>
				<definiendum id="0">add ( open</definiendum>
				<definiens id="0">( F ) , \ [ open ( F ) \ ] )</definiens>
			</definition>
			<definition id="7">
				<sentence>del ( delete ( F ) , \ [ exists ( F ) \ ] ) .</sentence>
				<definiendum id="0">del</definiendum>
			</definition>
			<definition id="8">
				<sentence>MELISSA represents a unique combination of high quality NLP and state-of-the-art softwareand knowledge-engineering techniques .</sentence>
				<definiendum id="0">MELISSA</definiendum>
			</definition>
</paper>

		<paper id="1062">
</paper>

		<paper id="2233">
			<definition id="0">
				<sentence>Exophoric information consists of a lot of elements , such as the time , the place , the speaker , and the listener of the utterance .</sentence>
				<definiendum id="0">Exophoric information</definiendum>
				<definiens id="0">consists of a lot of elements</definiens>
			</definition>
</paper>

		<paper id="2168">
</paper>

		<paper id="2123">
			<definition id="0">
				<sentence>In addition to the morphological analyzer , Morphy includes a statistical part-of-speech tagger and a context-sensitive lemmatizer .</sentence>
				<definiendum id="0">Morphy</definiendum>
				<definiens id="0">includes a statistical part-of-speech tagger and a context-sensitive lemmatizer</definiens>
			</definition>
			<definition id="1">
				<sentence>In essence , Morphy is a computer implementation of the morphological system described in the Duden grammar ( Drosdowsky , 1984 ) .</sentence>
				<definiendum id="0">Morphy</definiendum>
			</definition>
			<definition id="2">
				<sentence>To our knowledge , Morphy is the only morphology system for German whose lexicon can be expanded by users who have no specialist knowledge .</sentence>
				<definiendum id="0">Morphy</definiendum>
				<definiens id="0">the only morphology system for German whose lexicon can be expanded by users who have no specialist knowledge</definiens>
			</definition>
			<definition id="3">
				<sentence>The lemmatizer uses the output of the tagger to disambiguate word forms with more than one possible lemma .</sentence>
				<definiendum id="0">lemmatizer</definiendum>
				<definiens id="0">uses the output of the tagger to disambiguate word forms with more than one possible lemma</definiens>
			</definition>
</paper>

		<paper id="1070">
			<definition id="0">
				<sentence>X ' is the transfer result of X. The source expression of the transfer knowledge is expressed by a constituent boundary pattern , which is defined as a sequence that consists of variables and symbols representing constituent boundaries ( Furuse , 1994 ) .</sentence>
				<definiendum id="0">X '</definiendum>
			</definition>
			<definition id="1">
				<sentence>An active arc , which corresponds to an incomplete substructure , is created from a combination of patterns some of which have uninstantiated variables as right-hand neighbors to the processed string , like ( b ) and ( d ) .</sentence>
				<definiendum id="0">active arc</definiendum>
				<definiens id="0">an incomplete substructure , is created from a combination of patterns some of which have uninstantiated variables as right-hand neighbors to the processed string</definiens>
			</definition>
			<definition id="2">
				<sentence>The distance calculation mechanism aims to split an utterance into sentences correctly .</sentence>
				<definiendum id="0">distance calculation mechanism</definiendum>
				<definiens id="0">aims to split an utterance into sentences correctly</definiens>
			</definition>
</paper>

		<paper id="2202">
</paper>

		<paper id="2199">
			<definition id="0">
				<sentence>The strategic component determines what to say and the order in which to say it while the tactical component determines how to say it .</sentence>
				<definiendum id="0">strategic component</definiendum>
			</definition>
			<definition id="1">
				<sentence>Van Oirsouw ( van Oirsouw , 1987 ) , based on the literature on coordinate deletion , identified a number of rules which result in deletion under identity : Gapping , which deletes medial material ; Right-Node-Raising ( RNR ) , which deletes identical right most constituents in a syntactic tree ; VP-deletion ( VPD ) , which deletes identical verbs and handles post-auxiliary deletion ( Sag , 1976 ) .</sentence>
				<definiendum id="0">Gapping</definiendum>
				<definiendum id="1">Right-Node-Raising ( RNR</definiendum>
				<definiendum id="2">VP-deletion</definiendum>
			</definition>
			<definition id="2">
				<sentence>Conjunction Reduction ( CR ) , which deletes identical right-most or leftmost material .</sentence>
				<definiendum id="0">Conjunction Reduction</definiendum>
				<definiendum id="1">CR )</definiendum>
				<definiens id="0">deletes identical right-most or leftmost material</definiens>
			</definition>
			<definition id="3">
				<sentence>Functional Unification Grammar : A formalism for machine translation .</sentence>
				<definiendum id="0">Functional Unification Grammar</definiendum>
			</definition>
			<definition id="4">
				<sentence>Gapping : a eonstribution to Sentence Grammar .</sentence>
				<definiendum id="0">Gapping</definiendum>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>The FrameNet work is in some ways similar to efforts to describe the argument structures of lexical items in terms of case-roles or thetaroles , 5 but in FrameNet , the role names ( called frame elements or FEs ) are local to particular conceptual structures ( frames ) ; some of these are quite general , while others are specific to a small family of lexical items .</sentence>
				<definiendum id="0">FrameNet work</definiendum>
			</definition>
			<definition id="1">
				<sentence>elements ( DRIVER ( : MOVER ) , VEHICLE ( : MEANS ) , RIDER ( S ) ( : MOVER ( S ) ) , CARGO ( =MOVER ( S ) ) ) scenes ( DRIVER starts VEHICLE , DRIVER controis VEHICLE , DRIVER stops VEHICLE ) frame ( RIDING-i ) inherit ( TRANSP O RTATION ) frame .</sentence>
				<definiendum id="0">VEHICLE</definiendum>
				<definiendum id="1">RIDER ( S )</definiendum>
				<definiens id="0">MOVER ( S ) ) , CARGO ( =MOVER ( S ) ) ) scenes ( DRIVER starts VEHICLE , DRIVER controis VEHICLE , DRIVER stops VEHICLE ) frame ( RIDING-i ) inherit ( TRANSP O RTATION ) frame</definiens>
			</definition>
			<definition id="2">
				<sentence>1° reviews the skeletal lexical record created by the Vanguard , the annotated example sentences ( 5.3 ) , and the FEGs extracted from them , and builds both the entries for the lemmas in the Lexical Database ( 5.2 ) and the frame descriptions in the Frame Database ( 5.1 ) , using the Entry Writing Tools ( 4.2 ) .</sentence>
				<definiendum id="0">Database</definiendum>
				<definiens id="0">the annotated example sentences ( 5.3 ) , and the FEGs extracted from them , and builds both the entries for the lemmas in the Lexical Database ( 5.2 ) and the frame descriptions in the Frame</definiens>
			</definition>
</paper>

		<paper id="1095">
</paper>

		<paper id="2170">
			<definition id="0">
				<sentence>The Maximally Parsimonious Discrimination program ( MPD ) is a general computational tool for inferring , given multiple classes ( or , a typology ) , with attendant instances of these classes , the profiles ( =descriptions ) of these classes such that every class is contrasted from all remaining classes on the basis of feature values .</sentence>
				<definiendum id="0">Maximally Parsimonious Discrimination program</definiendum>
				<definiendum id="1">MPD</definiendum>
				<definiens id="0">a general computational tool for inferring , given multiple classes</definiens>
			</definition>
			<definition id="1">
				<sentence>KINSHIP is a specialized computer program , having as input the kinterms ( =classes ) of a language , and their attendant kintypes ( =instances ) .</sentence>
				<definiendum id="0">KINSHIP</definiendum>
				<definiens id="0">a specialized computer program , having as input the kinterms ( =classes ) of a language , and their attendant kintypes ( =instances )</definiens>
			</definition>
			<definition id="2">
				<sentence>7 Most importantly , MPD has shown that the huge number of potential componential ( -discrimination ) models -- a menace to the very foundations of the approach , which has made some linguists propose alternative analytic tools-are in fact reduced to ( nearly ) unique analyses by our 3 simplicity criteria .</sentence>
				<definiendum id="0">MPD</definiendum>
				<definiens id="0">has made some linguists propose alternative analytic tools-are in fact reduced to ( nearly ) unique analyses by our 3 simplicity criteria</definiens>
			</definition>
			<definition id="3">
				<sentence>The data consist of a sample of 30 languages with a wide genetic and areal coverage .</sentence>
				<definiendum id="0">data</definiendum>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>The scoring algorithm proceeds in five stages : hypothesis files corresponding phrases in the reference and hypothesis files of tag type , tag extent and tag content t MUC `` named entities '' include person , organization and location names , as well as numeric expressions .</sentence>
				<definiendum id="0">scoring algorithm</definiendum>
				<definiens id="0">proceeds in five stages : hypothesis files corresponding phrases in the reference and hypothesis files of tag type</definiens>
			</definition>
			<definition id="1">
				<sentence>It is very common for multiple lexemes in one text to correspond to a single lexeme in the other , in addition to multiple-to-multiple corresponMISSISSIPPI REPUBLICAN THE REPUBLICAN ref : AT THE NEW YORK DESK I 'M PHILIP BOROFF MISSISSIPPI REPUBLICAN hyp : At '' THE N~-~/~U &lt; BASK ON FILM FORUM MISSES THE REPUBLICAN Example 2 : SCLite alignment ( top ) vs. phonetic alignment ( bottom ) annotation and all punctuation is removed , and all remaining text is converted to upper-case .</sentence>
				<definiendum id="0">SCLite alignment</definiendum>
				<definiens id="0">top ) vs. phonetic alignment ( bottom ) annotation and all punctuation is removed</definiens>
			</definition>
			<definition id="2">
				<sentence>Content is the score component closest to the standard measures of word error .</sentence>
				<definiendum id="0">Content</definiendum>
				<definiens id="0">the score component closest to the standard measures of word error</definiens>
			</definition>
			<definition id="3">
				<sentence>F-measure is a single metric , a convenient way to compare systems or texts along one dimension 7 .</sentence>
				<definiendum id="0">F-measure</definiendum>
				<definiens id="0">a single metric , a convenient way to compare systems or texts along one dimension 7</definiens>
			</definition>
</paper>

		<paper id="2165">
			<definition id="0">
				<sentence>Natural language generation ( NLG ) can enhance the ability of such systems to communicate naturally and effectively by allowing the system to tailor , reorganize , or summarize lengthy database responses .</sentence>
				<definiendum id="0">Natural language generation</definiendum>
				<definiendum id="1">NLG</definiendum>
				<definiens id="0">enhance the ability of such systems to communicate naturally and effectively by allowing the system to tailor , reorganize , or summarize lengthy database responses</definiens>
			</definition>
			<definition id="1">
				<sentence>Each intonational phrase consists of one or more intermediate phrases followed by a boundary tone .</sentence>
				<definiendum id="0">intonational phrase</definiendum>
				<definiens id="0">consists of one or more intermediate phrases followed by a boundary tone</definiens>
			</definition>
</paper>

		<paper id="2212">
			<definition id="0">
				<sentence>The term bilingual lexicon denotes a collection of complex equivalences as used in Machine Translation ( MT ) transfer lexicons , not just word equivalences .</sentence>
				<definiendum id="0">bilingual lexicon</definiendum>
				<definiens id="0">a collection of complex equivalences as used in Machine Translation ( MT ) transfer lexicons , not just word equivalences</definiens>
			</definition>
			<definition id="1">
				<sentence>Transfer is a mapping between a bag of lexical items used in parsing ( the source bag ) and a corresponding bag of target lexical items ( the target bag ) , to be used in generation .</sentence>
				<definiendum id="0">Transfer</definiendum>
				<definiens id="0">a mapping between a bag of lexical items used in parsing ( the source bag ) and a corresponding bag of target lexical items ( the target bag ) , to be used in generation</definiens>
			</definition>
			<definition id="2">
				<sentence>A bilingual template is a bilingual entry in which words are left unspecified .</sentence>
				<definiendum id="0">bilingual template</definiendum>
				<definiens id="0">a bilingual entry in which words are left unspecified</definiens>
			</definition>
			<definition id="3">
				<sentence>Here , a ' '' : ' operator connects a word ( a variable , in a template ) to a description , % -~ ' connects the left and right sides of the entry , '\V introduces a transfer macro , which takes two descriptions as arguments and performs some additional transfer ( Turcato et al. , 1997 ) .</sentence>
				<definiendum id="0">'\V</definiendum>
			</definition>
			<definition id="4">
				<sentence>A bilingual lexical entry can thus be viewed as a triple &lt; Sw , Tw , T &gt; , where Sw is a list of source words , Tw a list of target words , and T a template .</sentence>
				<definiendum id="0">Sw</definiendum>
				<definiens id="0">a list of source words</definiens>
			</definition>
			<definition id="5">
				<sentence>Actually , on the target side , words are replaced by labels of the form word ( Ti , Position ) , where Ti is a template identifier and Position identifies the position of the item in the right hand side of the template .</sentence>
				<definiendum id="0">Ti</definiendum>
				<definiens id="0">a template identifier and Position identifies the position of the item in the right hand side of the template</definiens>
			</definition>
			<definition id="6">
				<sentence>Each such term has the form be ( Sw , Tw , Ti ) , where Sw is a list of source words , Tw is a list of target words and Ti is a template identifier .</sentence>
				<definiendum id="0">Sw</definiendum>
				<definiendum id="1">Tw</definiendum>
				<definiendum id="2">Ti</definiendum>
				<definiens id="0">a list of source words</definiens>
				<definiens id="1">a template identifier</definiens>
			</definition>
</paper>

		<paper id="2142">
			<definition id="0">
				<sentence>The system uses an augmented context-free grammar and employs a bidirectional chart parsing algorithm .</sentence>
				<definiendum id="0">system</definiendum>
				<definiens id="0">uses an augmented context-free grammar and employs a bidirectional chart parsing algorithm</definiens>
			</definition>
			<definition id="1">
				<sentence>Those systems identified and repaired errors in various ways , including using grammar-specific rules ( metarules ) ( Weischedel and Sondheimer , 1983 ) , least-cost error recovery based on chart parsing ( Lyon , 1974 ; Anderson and Backhouse , 1981 ) , semantic preferences ( Fass and Wilks , 1983 ) , and heuristic approaches based on a shift-reduce parser ( Vosse , 1992 ) .</sentence>
				<definiendum id="0">semantic preferences</definiendum>
				<definiens id="0">heuristic approaches based on a shift-reduce parser</definiens>
			</definition>
			<definition id="2">
				<sentence>The CHAPTER system ( CHArt Parser for Twostage Error Recovery ) , performs two-stage error recovery using generalised top-down chart parsing for the syntax phase ( cf. Mellish , 1989 ; Kato , 1994 ) .</sentence>
				<definiendum id="0">CHAPTER system</definiendum>
				<definiendum id="1">CHArt Parser</definiendum>
				<definiens id="0">performs two-stage error recovery using generalised top-down chart parsing for the syntax phase</definiens>
			</definition>
			<definition id="3">
				<sentence>Unlike other systems that 862 have focused on error recovery at a particular level ( Damerau , 1964 ; Mellish , 1989 ; Fass and Wilks , 1983 ) , CHAPTER uses an integrated agenda system , which integrates lexical , syntactic , surface case , and semantic processing .</sentence>
				<definiendum id="0">CHAPTER</definiendum>
				<definiens id="0">uses an integrated agenda system</definiens>
			</definition>
			<definition id="4">
				<sentence>The penalty score ( PS ( G ) ) of a goal ( or need-arc ) G , whose syntactic category is L and whose two positions are FROM and TO , is computed as follows : PS ( G ) = RW ( G ) MEL ( L ) where RW ( G ) is the number of remaining words to be processed , ( ie .</sentence>
				<definiendum id="0">penalty score ( PS</definiendum>
			</definition>
			<definition id="5">
				<sentence>CHAPTER uses an integrated-agenda manager that invokes subsystems incrementally at four levels : lexical , syntactic , surface case , and semantic .</sentence>
				<definiendum id="0">CHAPTER</definiendum>
				<definiens id="0">uses an integrated-agenda manager that invokes subsystems incrementally at four levels : lexical , syntactic , surface case</definiens>
			</definition>
</paper>

		<paper id="1091">
			<definition id="0">
				<sentence>The Lexicalized Tree Insertion Grammar formalism ( LTIG ) has been proposed as a way to lexicalize context-free grammars ( Schabes * This material is based upon work supported by the National Science Foundation under Grant No .</sentence>
				<definiendum id="0">Lexicalized Tree Insertion Grammar formalism</definiendum>
				<definiendum id="1">LTIG</definiendum>
				<definiens id="0">a way to lexicalize context-free grammars ( Schabes * This material is based upon work supported by the National Science Foundation under Grant No</definiens>
			</definition>
			<definition id="1">
				<sentence>LTAGs ( and LTIGs ) are tree-rewriting systems , consisting of a set of elementary trees combined by tree operations .</sentence>
				<definiendum id="0">LTAGs</definiendum>
				<definiens id="0">tree-rewriting systems , consisting of a set of elementary trees combined by tree operations</definiens>
			</definition>
			<definition id="2">
				<sentence>For a language with V vocabulary items , the number of parameters for the type of PLTIGs used in this paper is 2 ( V+I ) +2V ( K ) ( V+I ) , where K is the number of adjunction sites per tree .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">the number of adjunction sites per tree</definiens>
			</definition>
			<definition id="3">
				<sentence>The vocabulary set consists of the 48 part-of-speech tags .</sentence>
				<definiendum id="0">vocabulary set</definiendum>
			</definition>
</paper>

		<paper id="2247">
			<definition id="0">
				<sentence>Selectional preferences are represented as Association Tree Cut Models ( ATCMS ) aS described by Abe and Li ( 1996 ) .</sentence>
				<definiendum id="0">Selectional preferences</definiendum>
			</definition>
			<definition id="1">
				<sentence>MDL is a principle from information theory ( Rissanen , 1978 ) which states that the best model minimises the sum of i the number of bits to encode the model , and ii the number of bits to encode the data in the model .</sentence>
				<definiendum id="0">MDL</definiendum>
				<definiens id="0">a principle from information theory ( Rissanen , 1978 ) which states that the best model minimises the sum of i the number of bits to encode the model , and ii the number of bits to encode the data in the model</definiens>
			</definition>
			<definition id="2">
				<sentence>Introduction to WordNet : An On-Line Lezical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="2158">
			<definition id="0">
				<sentence>The task of statistical machine translation can be subdivided into two fields : tures into the probabilistic dependencies and provides methods for estimating the parameters of the models from bilingual corpora ; rithm , which performs the argmax operation in Eq .</sentence>
				<definiendum id="0">rithm</definiendum>
				<definiens id="0">performs the argmax operation in Eq</definiens>
			</definition>
			<definition id="1">
				<sentence>p ( f~le , ) ( 4 ) j=l i=1 with the following components : the sentence length probability p ( JlI ) , the mixture alignment probability p ( ilj , J , I ) and the translation probability p ( fle ) .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">translation probability p</definiendum>
				<definiens id="0">the sentence length probability p ( JlI ) , the mixture alignment probability p</definiens>
			</definition>
			<definition id="2">
				<sentence>The alignment model as described above is defined as a function that assigns exactly one target word to each source word .</sentence>
				<definiendum id="0">alignment model</definiendum>
			</definition>
			<definition id="3">
				<sentence>ST '' A word e can only be associated with a source language word f , if p ( f\ [ e ) is higher than ST. This restricts the optimization over the target language vocabulary to a relatively small set of candidate words .</sentence>
				<definiendum id="0">ST</definiendum>
				<definiens id="0">higher than ST. This restricts the optimization over the target language vocabulary to a relatively small set of candidate words</definiens>
			</definition>
			<definition id="4">
				<sentence>The Verbmobil Corpus consists of spontaneously spoken dialogs in the domain of appointment scheduling ( Wahlster , 1993 ) .</sentence>
				<definiendum id="0">Verbmobil Corpus</definiendum>
			</definition>
			<definition id="5">
				<sentence>To improve the lexicon probabilities , we interpolated them with lexicon probabilities pM ( fle ) manually created from a German-English dictionary : { 'o ~ if ( e , f ) is in the dictionary pM ( fle ) _ otherwise ' where Ne is the number of German words listed as translations of the English word e. The two lexica were combined by linear interpolation with the interpolation parameter A. For our first experiments , we set A to 0.5 .</sentence>
				<definiendum id="0">Ne</definiendum>
				<definiens id="0">the number of German words listed as translations of the English word e. The two lexica were combined by linear interpolation with the interpolation parameter</definiens>
			</definition>
			<definition id="6">
				<sentence>Acceptable translations convey the same meaning but with small grammatical mistakes or they convey most but not the entire meaning of the input .</sentence>
				<definiendum id="0">Acceptable translations</definiendum>
				<definiens id="0">convey the same meaning but with small grammatical mistakes or they convey most but not the entire meaning of the input</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>frame , which is computed Score per word by taking Length of word in frames '' When combining several word edges as we do by constructing hyperedges , the combination should be assigned a single value that reflects a certain useful aspect of the originating edges .</sentence>
				<definiendum id="0">frame</definiendum>
				<definiens id="0">computed Score per word by taking Length of word in frames '' When combining several word edges</definiens>
			</definition>
</paper>

		<paper id="1082">
			<definition id="0">
				<sentence>ligne a~rienne ( overhead line ) See_also : D~part a~rien ( overhead outlet ) Synonym : Liaison ~lectrique a~rienne ( overhead electric link ) Ligne simple ( single circuit line ) Is_a : Ligne a~rienne ( overhead line ) Ligne multiterne ( multiple circuit line ) ls_a : Ligne a~rienne ( overhead line ) Synonym : Ligne double ( double circuit line ) Figure 1 : Example of a structured terminology in the electric domain .</sentence>
				<definiendum id="0">Ligne simple</definiendum>
				<definiendum id="1">double</definiendum>
				<definiens id="0">D~part a~rien ( overhead outlet ) Synonym : Liaison ~lectrique a~rienne ( overhead electric link</definiens>
			</definition>
			<definition id="1">
				<sentence>501 Term 1 Term 2 d~t~rioration notable ( notable deterioration ) fausse manoeuvre ( wrong operation ) action de l'op~rateur ( action of the operator ) capacit~ interne ( internal capacity ) capacit~ totale ( total capacity ) capacit~ utile ( useful capacity ) limite de solubilit~ ( limit of solubility ) marche manuelle ( manual running ) tests p~riodiques ( periodic tests ) hauteur d'eau ( water height ) panneau de commande ( control panel ) d~gradation importante ( important damage ) mauvaise manipulation ( bad handling ) intervention de l'op6rateur ( intervention of the operator ) volume interne ( internal volume ) volume total ( total volume ) volume utile ( useful volume ) seuil de solubilit6 ( solubility threshold ) fonctionnement manuel ( manual working ) essais p~riodiques ( periodic trials ) niveau d'eau ( level of water ) tableau de commande ( control board ) Figure 4 : Examples of synonymy links between complex candidate terms .</sentence>
				<definiendum id="0">notable ( notable deterioration ) fausse manoeuvre</definiendum>
				<definiendum id="1">d~gradation importante</definiendum>
				<definiendum id="2">mauvaise manipulation</definiendum>
				<definiens id="0">wrong operation ) action de l'op~rateur ( action of the operator ) capacit~ interne ( internal capacity ) capacit~ totale ( total capacity ) capacit~ utile ( useful capacity ) limite de solubilit~ ( limit of solubility ) marche manuelle ( manual running ) tests p~riodiques ( periodic tests</definiens>
				<definiens id="1">intervention of the operator ) volume interne ( internal volume ) volume total ( total volume ) volume utile ( useful volume</definiens>
			</definition>
			<definition id="2">
				<sentence>Term 1 Term 2 essai en usine ( test in plant ) ligne de vidange ( draining line ) fonction d'un temps ( fonction of a time ) froid normal ( normal cold ) rapport de sfiret~ ( safety report ) solution d'acide borique ( solution of boric acid ) temperature attendue ( expected temperature ) temperature normale ( normal temperature ) organes de commande ( control devices ) gros d~bit ( big flow ) activit~ importante ( important activity ) commande m~canique ( mechanical control ) risques de corrosion ( risk of corrosion ) experience d'exploitation ( experiment of exploitation ) point de purge ( blow-down point ) effet d'une temperature ( effect of a temperature ) refroidissement correct ( correct cooling ) analyse de sfiret~ ( safety analysis ) dissolution de l'acide borique ( dissolving of the boric acid ) temps attendu ( expected time ) temps normal ( normal time ) organes d'ordre ( order devices ) plein d~bit ( full flow ) activit~ ~lev~e ( high activity ) ordre automatique ( automatic order ) risques de destruction ( risk of destruction ) Figure 5 : Examples of rejected links ture ) are thus given as synonymous .</sentence>
				<definiendum id="0">m~canique</definiendum>
				<definiens id="0">full flow ) activit~ ~lev~e ( high activity</definiens>
			</definition>
</paper>

		<paper id="1090">
			<definition id="0">
				<sentence>The Intelligent Labelling Explorer ( ILEX ) project is building a system that generates descriptions of objects displayed in a museum gallery .</sentence>
				<definiendum id="0">Intelligent Labelling Explorer</definiendum>
				<definiens id="0">building a system that generates descriptions of objects displayed in a museum gallery</definiens>
			</definition>
</paper>

		<paper id="2130">
			<definition id="0">
				<sentence>A dependency grammar is a six-tuple &lt; W , C , S , D , I , H &gt; , where W is a finite set of symbols ( words of a natural language ) ; C is a set of syntactic categories ( among which the special category E ) ; S is a non-empty set of root categories ( C ~ S ) ; D is the set of dependency relations , e.g. SUB J , OBJ , XCOMP , P-OBJ , PRED ( among which the special relation VISITOR1 ) ; I is a finite set of symbols ( among which the special symbol 0 ) , called u-indices ; H is a set of dependency rules of the form x : X ( &lt; rlYlUl'Cl &gt; ... &lt; ri-1Yi-lUi-l'Ci-1 &gt; # &lt; a'i+lYi+lUi+fl ; i+l &gt; ... &lt; rmYmum'~m &gt; ) 1 ) xe W , is the head of the rule ; 2 ) Xe C , is its syntactic category ; 3 ) an element &lt; r i Yi ui xj &gt; is a d-quadruple ( which describbs a-de-pefident ) ; the sequence of d-quads , including the symbol # ( representing the linear position of the head , # is a special symbol ) , is called the d-quad sequence .</sentence>
				<definiendum id="0">dependency grammar</definiendum>
				<definiendum id="1">W</definiendum>
				<definiendum id="2">C</definiendum>
				<definiendum id="3">H</definiendum>
				<definiens id="0">a finite set of symbols ( words of a natural language ) ;</definiens>
				<definiens id="1">a set of syntactic categories ( among which the special category E ) ; S is a non-empty set of root categories ( C ~ S ) ; D is the set of dependency relations</definiens>
				<definiens id="2">a set of dependency rules of the form x : X</definiens>
				<definiens id="3">a special symbol</definiens>
			</definition>
			<definition id="1">
				<sentence>Given that a dependency rule constrains one head and its direct dependents in the dependency tree , we have that the dependent indexed by uk is coindexed with a The relation VISITOR ( Hudson 1990 ) accounts for displaced elements and , differently from the other relations , is not semantically interpreted .</sentence>
				<definiendum id="0">relation VISITOR</definiendum>
				<definiens id="0">not semantically interpreted</definiens>
			</definition>
			<definition id="2">
				<sentence>A triple consisting of a word w ( a W ) or the trace symbol e ( ~W ) and two integers g and v is a word object of the grammar .</sentence>
				<definiendum id="0">v</definiendum>
				<definiens id="0">triple consisting of a word w ( a W ) or the trace symbol e ( ~W ) and two integers g and</definiens>
				<definiens id="1">a word object of the grammar</definiens>
			</definition>
			<definition id="3">
				<sentence>Given a grammar G , the set of derivation objects of G is Cx ( G ) = { &lt; r , Y ( y1 ) , u , y2 &gt; / re D , Ye C , u is an integer , ) '1 , T2 are strings of instantiated u-triples } .</sentence>
				<definiendum id="0">u</definiendum>
				<definiens id="0">the set of derivation objects of G is Cx ( G ) = { &lt; r</definiens>
				<definiens id="1">an integer , ) '1 , T2 are strings of instantiated u-triples }</definiens>
			</definition>
			<definition id="4">
				<sentence>Given a grammar G , L ' ( G ) is the language of sequences of word objects : L ' ( G ) = { ae Wx ( G ) * / &lt; TOP , Q ( i~ ) , 0 , 0 &gt; : = &gt; * ( x and Qe S ( G ) } where TOP is a dummy dependency relation .</sentence>
				<definiendum id="0">TOP</definiendum>
				<definiens id="0">the language of sequences of word objects : L '</definiens>
				<definiens id="1">a dummy dependency relation</definiens>
			</definition>
			<definition id="5">
				<sentence>T-stack is a stack of sets of u-triples to be satisfied yet : the sets of u-triples ( including empty sets , when applicable ) provided by the various items are stacked in order to verify the consumption of one u-triple in the appropriate subtree ( cf. the notion of derivation above ) .</sentence>
				<definiendum id="0">T-stack</definiendum>
				<definiens id="0">a stack of sets of u-triples to be satisfied yet : the sets of u-triples ( including empty sets , when applicable ) provided by the various items are stacked in order to verify the consumption of one u-triple in the appropriate subtree ( cf. the notion of derivation above )</definiens>
			</definition>
			<definition id="6">
				<sentence>The body consists of an external loop on the sets Si ( 0 &lt; i &lt; n ) and an inner loop on the single items of the set Si .</sentence>
				<definiendum id="0">body</definiendum>
			</definition>
			<definition id="7">
				<sentence>The completer looks for the items in S i which were waiting for completion ( return items ; j is the retum Position of the item P ) .</sentence>
				<definiendum id="0">j</definiendum>
				<definiens id="0">the items in S i which were waiting for completion ( return items</definiens>
			</definition>
			<definition id="8">
				<sentence>The return items must contain a dotted rule where the dot immediately precedes a d-quad &lt; R , Y , ux , Xx &gt; , where Y is the head category of the dotted rule in the item P. Their generic form is &lt; x : X ( X * &lt; R , Y , ux , Xx &gt; 4 ) , J ' , IX ' , v ' , T-stack ' &gt; .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">the head category of the dotted rule in the item P. Their generic form</definiens>
			</definition>
			<definition id="9">
				<sentence>Finally , INSERT joins the new item to the set Si .</sentence>
				<definiendum id="0">INSERT</definiendum>
				<definiens id="0">joins the new item to the set Si</definiens>
			</definition>
			<definition id="10">
				<sentence>Scanner : When the dot precedes the symbol # , the parser can scan the current input word wi ( if y , the head of the item P , is equal to it ) , or pseudoscan a trace item , respectively .</sentence>
				<definiendum id="0">Scanner</definiendum>
				<definiens id="0">if y , the head of the item P , is equal to it ) , or pseudoscan a trace item , respectively</definiens>
			</definition>
			<definition id="11">
				<sentence>T-stack has a number of elements which depends on the maximum length of the chain of predictions .</sentence>
				<definiendum id="0">T-stack</definiendum>
				<definiens id="0">a number of elements which depends on the maximum length of the chain of predictions</definiens>
			</definition>
			<definition id="12">
				<sentence>A u-triple is of the form &lt; u , R , Y &gt; : u is an integer that is ininfluent , Ra D , Ya C. Because of the PUSH-UNION operation on T-stack , the number of possible u-triples scattered throughout the elements of T-stack is IDIICI .</sentence>
				<definiendum id="0">u-triple</definiendum>
				<definiens id="0">an integer that is ininfluent , Ra D , Ya C. Because of the PUSH-UNION operation on T-stack , the number of possible u-triples scattered throughout the elements of T-stack is IDIICI</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>Human operators respond to a caller request by 1 ) routing the call to an appropriate destination , or 2 ) querying the caller for further information to determine where to route the call .</sentence>
				<definiendum id="0">Human operators</definiendum>
				<definiens id="0">a caller request by 1 ) routing the call to an appropriate destination , or 2 ) querying the caller for further information to determine where to route the call</definiens>
			</definition>
			<definition id="1">
				<sentence>Our call router consists of two components : the routing module and the disambiguation module .</sentence>
				<definiendum id="0">call router</definiendum>
				<definiens id="0">consists of two components : the routing module and the disambiguation module</definiens>
			</definition>
			<definition id="2">
				<sentence>The ignore list consists of noise words , such as uh and um , which sometimes get in the way of proper n-gram extraction , as in `` I 'd like to speak to someone about a car uh loan '' .</sentence>
				<definiendum id="0">um</definiendum>
				<definiens id="0">sometimes get in the way of proper n-gram extraction</definiens>
			</definition>
			<definition id="3">
				<sentence>A 2 1/2 ( El &lt; e &lt; n t , e ) . Our second weighting is based on the n-oti-on that a term that only occurs in a few documents is more important in discriminating among documents than a term that occurs in nearly every document. We use the inverse document frequency ( IDF ) weighting scheme ( Sparck Jones , 1972 ) whereby a term is weighted inversely to the number of documents in which it occurs , by means oflDF ( t ) = log 2 n/d ( t ) where t is a term , n is the total number of documents in the corpus , and d ( t ) is the number of documents containing the term t. Thus we obtain a weighted matrix B , whose elements are given by Bt , a = At , a x IDF ( t ) / ( ~-~x &lt; e &lt; n A2 , e ) x/2. Vector Representation To reduce the dimensionality of our vector representations for terms and documents , we applied the singular value decomposition ( Deerwester et al. , 1990 ) to the m x n matrix B of weighted term-document frequencies. Specifically , we take B = USV T , where U is an m x r orthonormal matrix ( where r is the rank of B ) , V is an n x r orthonormal matrix , and S is an r x r diagonal matrix such that Sl,1 ~_~ 82,2 ~ &gt; `` ' '' ~ &gt; Sr , r ~ O. We can think of each row in U as an r-dimensional vector that represents a term , whereas each row in V is an r-dimensional vector representing a document .</sentence>
				<definiendum id="0">inverse document frequency</definiendum>
				<definiendum id="1">IDF</definiendum>
				<definiendum id="2">n</definiendum>
				<definiendum id="3">r</definiendum>
				<definiendum id="4">V</definiendum>
				<definiendum id="5">S</definiendum>
				<definiens id="0">El &lt; e &lt; n t , e ) . Our second weighting is based on the n-oti-on that a term that only occurs in a few documents is more important in discriminating among documents than a term that occurs in nearly every document. We use the</definiens>
				<definiens id="1">the number of documents in which it occurs , by means oflDF ( t</definiens>
				<definiens id="2">the number of documents containing the term t. Thus we obtain a weighted matrix B , whose elements are given by Bt , a = At , a x IDF ( t ) / ( ~-~x &lt; e &lt; n A2 , e ) x/2. Vector Representation To reduce the dimensionality of our vector representations for terms and documents , we applied the singular value decomposition ( Deerwester et al. , 1990 ) to the m x n matrix B of weighted term-document frequencies. Specifically , we take B = USV T , where U is an m x r orthonormal matrix</definiens>
				<definiens id="3">an n x r orthonormal matrix , and</definiens>
				<definiens id="4">an r-dimensional vector that represents a term , whereas each row in V is an r-dimensional vector representing a document</definiens>
			</definition>
			<definition id="4">
				<sentence>Pseudo-Document Generation Given the extracted terms from a caller request , we can represent the request as an m-dimensional vector Q where each component Qi represents the number of times that the ith term occurred in the caller 's request .</sentence>
				<definiendum id="0">Qi</definiendum>
				<definiens id="0">the number of times that the ith term occurred in the caller 's request</definiens>
			</definition>
</paper>

		<paper id="1078">
			<definition id="0">
				<sentence>Rosetta is a machine translation system which includes Dutch as one of the source and target languages .</sentence>
				<definiendum id="0">Rosetta</definiendum>
			</definition>
			<definition id="1">
				<sentence>Approach Word Manager TM ( WM ) is a system for morphological dictionaries .</sentence>
				<definiendum id="0">Word Manager TM ( WM )</definiendum>
			</definition>
			<definition id="2">
				<sentence>The German WM dictionary consists of a comprehensive set of inflectional and word formation rules describing the full range of morphological processes in German .</sentence>
				<definiendum id="0">German WM dictionary</definiendum>
				<definiens id="0">consists of a comprehensive set of inflectional and word formation rules describing the full range of morphological processes in German</definiens>
			</definition>
			<definition id="3">
				<sentence>The specification of the string is part of the lexicographic specification , i.e. the string specification is the result of the application of the word formation rule the lexicographer chooses for the definition of an individual entry .</sentence>
				<definiendum id="0">string specification</definiendum>
				<definiens id="0">the result of the application of the word formation rule the lexicographer chooses for the definition of an individual entry</definiens>
			</definition>
</paper>

		<paper id="2145">
			<definition id="0">
				<sentence>A text consists of multiple sentences that have semantic relations with each other .</sentence>
				<definiendum id="0">text</definiendum>
			</definition>
			<definition id="1">
				<sentence>Each point , p ( n , n+l ) , is a candidate for a segment boundary and has a score scr ( n , n + 1 ) which is calculated by a weighted sum of the scores for each cue i , scri ( n , n + 1 ) , as follows : scr ( n , n+ 1 ) = Zwi X scri ( n , n+ 1 ) ( 1 ) i A point p ( n , n + 1 ) with a high score scr ( n , n + 1 ) becomes a candidate with higher plausibility .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">Zwi X scri</definiendum>
				<definiens id="0">a candidate for a segment boundary and has a score scr ( n , n + 1 ) which is calculated by a weighted sum of the scores for each cue i</definiens>
				<definiens id="1">n + 1 ) with a high score scr ( n , n + 1 ) becomes a candidate with higher plausibility</definiens>
			</definition>
			<definition id="2">
				<sentence>Considering the following equation S ( n , n + 1 ) , at each point p ( n , n + 1 ) in the training texts , p + 1 ) = a + × + 1 ) ( 2 ) i=1 where a is a constant , p is the number of the cues , and wi is the estimated weight for the i-th cue , we can obtain the above equations in the number of the points in the training texts .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">n + 1 ) , at each point p ( n , n + 1 ) in the training texts , p + 1 ) = a + × + 1 ) ( 2 ) i=1 where a</definiens>
				<definiens id="1">the number of the cues , and wi is the estimated weight for the i-th cue , we can obtain the above equations in the number of the points in the training texts</definiens>
			</definition>
			<definition id="3">
				<sentence>The partial Fstatistic is given by ( SSR SSR~ ) /q f = SSE/ ( N-p1 ) ( 3 ) where SSR denotes the regression sum of squares , SSE denotes the error sum of squares , p is the number of linguistic cues , N is the number of training data , and q is the number of cues in the model at each selection step .</sentence>
				<definiendum id="0">Fstatistic</definiendum>
				<definiendum id="1">SSR</definiendum>
				<definiendum id="2">SSE</definiendum>
				<definiendum id="3">p</definiendum>
				<definiendum id="4">N</definiendum>
				<definiendum id="5">q</definiendum>
				<definiens id="0">the regression sum of squares</definiens>
				<definiens id="1">the error sum of squares ,</definiens>
				<definiens id="2">the number of linguistic cues</definiens>
				<definiens id="3">the number of training data , and</definiens>
				<definiens id="4">the number of cues in the model at each selection step</definiens>
			</definition>
			<definition id="4">
				<sentence>We use two measures , Recall and Precision for the evaluation : Recall is the quotient of the number of correctly identified boundaries by the total number of correct boundaries .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">the quotient of the number of correctly</definiens>
			</definition>
</paper>

		<paper id="1088">
			<definition id="0">
				<sentence>Secondly , the multiplicative fragment forms the core of the system used in work by Dalrymple and colleagues for handling the semantics of LFG derivations , providing a 'glue language ' for assembling the meanings of sentences from those of words and phrases .</sentence>
				<definiendum id="0">multiplicative fragment</definiendum>
				<definiens id="0">forms the core of the system used in work by Dalrymple and colleagues for handling the semantics of LFG derivations , providing a 'glue language ' for assembling the meanings of sentences from those of words and phrases</definiens>
			</definition>
			<definition id="1">
				<sentence>Linear logic is a 'resource-sensitive ' logic : in any deduction , each assumption ( 'resource ' ) is used precisely once• The formulae of the multiplicative fragment of ( intuitionistic ) linear logic are defined by ~ '' : := A I ~'o-~ '' J 9 v ® ~ ( A a nonempty set of atomic types ) .</sentence>
				<definiendum id="0">Linear logic</definiendum>
			</definition>
			<definition id="2">
				<sentence>The following rules provide a natural deduction formulation : Ao -- B : a B : b o-E A : ( ab ) \ [ B : v\ ] A : a o -- I Ao-B : ) , v.a \ [ B : x\ ] , \ [ C : y\ ] B®C : b A : a A : a B : b ®E ®I A '' @ • E. , ~ ( b , a ) A®B : ( a ® b ) The elimination ( E ) and introduction ( I ) rules for o-correspond to steps of functional application and abstraction , respectively , as the term labelling reveals .</sentence>
				<definiendum id="0">introduction</definiendum>
				<definiens id="0">a B : b o-E A : ( ab ) \ [ B : v\ ] A : a o -- I Ao-B : ) , v.a \ [ B : x\ ] , \</definiens>
				<definiens id="1">the term labelling reveals</definiens>
			</definition>
			<definition id="3">
				<sentence>The index sets of these formulae identify precisely the assumptions from which they are derived , with appropriate indexation being ensured by the condition 7r = ¢~¢ of the rule ( where t2 stands for disjoint union , which enforces linear usage ) .</sentence>
				<definiendum id="0">t2</definiendum>
				<definiens id="0">disjoint union , which enforces linear usage )</definiens>
			</definition>
			<definition id="4">
				<sentence>Agenda items , on the other hand , instead record the way that the agenda item was produced , which is either 'presupplied ' ( by compilation ) or 'by combination ' , in which case the entries combined are recorded by their identifiers .</sentence>
				<definiendum id="0">Agenda items</definiendum>
				<definiens id="0">either 'presupplied ' ( by compilation ) or 'by combination ' , in which case the entries combined are recorded by their identifiers</definiens>
			</definition>
</paper>

		<paper id="2151">
			<definition id="0">
				<sentence>GDA ( Global Document Annotation ) is a challenging project to make WWW texts machineunderstandable on the basis of a new tag set , and to develop content-based presentation , retrieval .</sentence>
				<definiendum id="0">GDA</definiendum>
			</definition>
			<definition id="1">
				<sentence>The GDA tag set is based on XML ( Extensible Markup Language ) , and designed as compatible as possible with HTML , TEI , EAGLES , and so forth .</sentence>
				<definiendum id="0">GDA tag set</definiendum>
				<definiens id="0">XML ( Extensible Markup Language ) , and designed as compatible as possible with HTML , TEI , EAGLES , and so forth</definiens>
			</definition>
			<definition id="2">
				<sentence>A relational term denotes a binary relation , which may be a thematic role such as agent , patient , recipient , etc. , or a rhetorical relation such as cause , concession , etc .</sentence>
				<definiendum id="0">relational term</definiendum>
			</definition>
			<definition id="3">
				<sentence>re1 is an open-class attribute , potentially encompassing all the binary relations lexicalized in natural languages .</sentence>
				<definiendum id="0">re1</definiendum>
			</definition>
			<definition id="4">
				<sentence>A GDA-tagged document naturally defines an intra-document network in which nodes correspond to elements and links represent the semantic relations mentioned in the previous section .</sentence>
				<definiendum id="0">GDA-tagged document</definiendum>
				<definiens id="0">an intra-document network in which nodes correspond to elements and links represent the semantic relations mentioned in the previous section</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>Cross-document coreference occurs when the same person , place , event , or concept is discussed in more than one text source .</sentence>
				<definiendum id="0">Cross-document coreference</definiendum>
				<definiens id="0">occurs when the same person , place , event , or concept is discussed in more than one text source</definiens>
			</definition>
			<definition id="1">
				<sentence>Cross-document coreference occurs when the same person , place , event , or concept is discussed in more than one text source .</sentence>
				<definiendum id="0">Cross-document coreference</definiendum>
				<definiens id="0">occurs when the same person , place , event , or concept is discussed in more than one text source</definiens>
			</definition>
			<definition id="2">
				<sentence>Problem Cross-document coreference is a distinct technology from Named Entity recognizers like IsoQuest 's NetOwl and IBM 's Textract because it attempts to determine whether name matches are actually the same individual ( not all John Smiths are the same ) .</sentence>
				<definiendum id="0">Problem Cross-document coreference</definiendum>
				<definiens id="0">a distinct technology from Named Entity recognizers like IsoQuest 's NetOwl and IBM 's Textract because it attempts to determine whether name matches are actually the same individual ( not all John Smiths are the same )</definiens>
			</definition>
			<definition id="3">
				<sentence>Therefore , for doc.36 ( Figure 2 ) , since at least one of the three noun phrases ( `` John Perry , '' `` he , '' and `` Perry '' ) in the coreference chain of interest appears in each of the three sentences in the extract , the summary produced by SentenceExtractor is the extract itself .</sentence>
				<definiendum id="0">SentenceExtractor</definiendum>
				<definiens id="0">the extract itself</definiens>
			</definition>
			<definition id="4">
				<sentence>If $ 1 and $ 2 are the vectors for the two summaries extracted from documents D1 and D2 , then their similarity is computed as : Sim ( S1 , $ 2 ) = E wlj x w2j common terms tj where tj is a term present in both $ 1 and $ 2 , wlj is the weight of the term tj in S1 and w~j is the weight of tj in $ 2 .</sentence>
				<definiendum id="0">tj</definiendum>
				<definiendum id="1">w~j</definiendum>
				<definiens id="0">the weight of the term tj in S1 and</definiens>
			</definition>
			<definition id="5">
				<sentence>The weight of a term tj in the vector Si for a summary is given by : t f × log Wij = 2_ } _ ... +2 Jsi~ + si2 sin where tf is the frequency of the term tj in the summary , N is the total number of documents in the collection being examined , and df is the number of documents in the collection that the term tj occurs 2 is the cosine normalizain .</sentence>
				<definiendum id="0">tf</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">df</definiendum>
				<definiens id="0">the frequency of the term tj in the summary</definiens>
				<definiens id="1">the total number of documents in the collection being examined</definiens>
			</definition>
			<definition id="6">
				<sentence>• c ( S ) is the minimal number of `` correct '' links necessary to generate the equivalence class S. It is clear that c ( S ) is one less than the cardinality of S , i.e. , c ( S ) = ( IS\ [ 1 ) .</sentence>
				<definiendum id="0">• c ( S )</definiendum>
			</definition>
			<definition id="7">
				<sentence>• m ( S ) is the number of `` missing '' links in the response relative to the key set S. As noted above , this is the number of links necessary to 1The exposition of this scorer has been taken nearly entirely from ( Vilain 95 ) .</sentence>
				<definiendum id="0">S )</definiendum>
				<definiens id="0">the number of `` missing '' links in the response relative to the key set S. As noted above</definiens>
			</definition>
			<definition id="8">
				<sentence>Looking in isolation at a single equivalence class in the key , the recall error for that class is just the number of missing links divided by the number of m ( S ) correct links , i.e. , c ( S ) • c ( S ) -m ( S ) Recall in turn is c ( S ) , which equals ( ISl1 ) ( Ip ( S ) lI ) ISli The whole expression can now be simplified to ISlIp ( S ) I ISl1 Precision is computed by switching the roles of the key and response in the above formulation .</sentence>
				<definiendum id="0">recall error</definiendum>
				<definiendum id="1">S )</definiendum>
				<definiens id="0">m ( S ) correct links , i.e. , c ( S ) • c ( S ) -m ( S ) Recall in turn is c (</definiens>
			</definition>
			<definition id="9">
				<sentence>The final precision and recall numbers are computed by the following two formulae : Final Precision = N Z wi * Precisioni i=l N Final Recall = E wi * Recall~ i=l where N is the number of entities in the document , and wi is the weight assigned to entity i in the document .</sentence>
				<definiendum id="0">wi</definiendum>
				<definiens id="0">Final Precision = N Z wi * Precisioni i=l N Final Recall = E wi * Recall~ i=l where N is the number of entities in the document , and</definiens>
			</definition>
			<definition id="10">
				<sentence>It implicitly overcomes the first 83 Output MUC Algorithm B-CUBED Algorithm ( equal weights for every entity ) P : 1- % ( 90 % ) P : ~*\ [ ~+~+~+~+~+~+~+~+~+~+~+~\ ] =76 % Example 1 R : ~ ( 100 % ) R : ~*\ [ ~+~+~+~+~+~+~+~+~+~+5+51=I00 % P : 9 ( 90 % ) P : ~*\ [ 5+~+~+~+~+~+~+-~+5+5+5+5\ ] =58 % Example 2 R : ~ ( 100 % ) R : 1*\ [ ~+~+~+~+~+~+~+~+-~+~+~+~1=I00 % Precisioni = Figure 9 : Scores of Both Algorithms on the Examples number of correct elements in the output chain containing entityi Recalli = number of elements in the output chain containing entityi number of correct elements in the output chain containing entityi number of elements in the truth chain containing entityi ( t ) ( 2 ) Figure 10 : Definitions for Precision and Recall for an Entity i shortcoming of the MUC-6 algorithm by calculating the precision and recall numbers for each entity in the document ( irrespective of whether an entity is part of a coreference chain ) .</sentence>
				<definiendum id="0">chain containing entityi</definiendum>
				<definiens id="0">Definitions for Precision and Recall for an Entity i shortcoming of the MUC-6 algorithm by calculating the precision and recall numbers for each entity in the document ( irrespective of whether an entity is part of a coreference chain )</definiens>
			</definition>
</paper>

		<paper id="2163">
			<definition id="0">
				<sentence>: ~ Circumstance Manner Means Concession ( Vw , e , g , p ) go ( e , y , p ) ^ locational ( e ) A goal ( g ) D pp ( `` w ni '' , g ) A place ( g ) ( ¥w , e , g , p ) go ( e , y , p ) A posessional ( e ) ^ goal ( g ) D pp ( `` w -ni '' , g ) A thing ( g ) ( Vw , 8 , y , p ) be ( s , y , l ) A locational ( e ) A at ( l , p ) D pp ( `` w -ni '' , p ) A place ( p ) ( Vw , e , x , y ) act ( e , x , y ) D pp ( `` w-ga '' , x ) Aanimate ( x ) ( Vw , e , y , s ) become ( e , y , 8 ) ~ pp ( `` w -ga '' , y ) ( ¥w , s , y , l ) beCs , y , l ) D pp ( `` w ga '' , y ) ( Vw , e , x , y ) act ( e , x , y ) ~ W ( `` v ' -o '' , y ) ( Vw , e , ~ , y , 8 ) aS ( e , ~ , y ) ^ become ( e , y , 8 ) mo ( `` w o '' , y ) J Figure 2 : Examples of the linking rules Figure 1 : The organization of the relations with telinkage that this organization of the relations are viewed from the perspective of re-linkage .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">p ) D pp</definiendum>
				<definiens id="0">e , x , y ) act ( e , x , y ) ~ W ( `` v ' -o '' , y ) ( Vw , e , ~ , y</definiens>
			</definition>
			<definition id="1">
				<sentence>The linking rules are regular ways of ( vs , y , z , l ) be ( s , y , l ) ^ at ( l , z ) ~ State ( s ) ( re , z , y ) act ( e , x , y ) D TransAct ( e ) ( re , z ) act ( e , z ) D IntransAct ( e ) ( re , y , p ) go ( e , y , p ) A path ( p ) D Move ( e ) ( re , y , s , l , z ) become ( e , y , s ) h be ( s , y , l ) h at ( l , z ) D Achievement ( e ) ( re , e~ , e~ , , ~ , y ) act ( el , x , y ) ^ cause ( e , e , , e~ ) ^becomeCe~ , y , s ) ^ be ( s , y , l ) ^ at ( l , z ) D Accomplishment ( e ) ( Vs ) State ( s ) A thing ( y ) A place ( z ) D verb ( `` aru '' , e ) ( Vs ) State ( s ) Aanimate ( y ) hplace ( z ) D verb ( `` iru '' , e ) ( re ) Move ( e ) A mannerl D verb ( `` hashiru '' , e ) ( re ) Move ( e ) h rnanner2 D verb ( `` aruku '' , e ) ( Ve ) Accomplishment ( e ) Amanner3 D verb ( `` nuru '' , e ( re ) Accomplishment ( e ) A manner4 ^ locational ( e ) D verb ( `` sosogu '' , e ) ( re ) Accomplishment ( e ) A statelh identi f icational ( e ) D verb ( `` mitasu '' , e ) J Figure 3 : Examples of the verbs ' semantic structures 992 mapping open arguments -i.e. , variables of semantic structures whose referents can be expressed syntactically by a phrase within the same clause as the predicate -onto grammatical functions or underlying syntactic configurations by virtue of thematic roles ( thematic roles are positions in a structured semantic representation ) .</sentence>
				<definiendum id="0">IntransAct ( e )</definiendum>
				<definiens id="0">re , y , p ) go ( e , y , p ) A path ( p ) D Move ( e ) ( re , y , s , l , z ) become ( e , y , s ) h be ( s , y , l</definiens>
				<definiens id="1">s ) Aanimate ( y ) hplace ( z ) D verb ( `` iru '' , e ) ( re ) Move</definiens>
				<definiens id="2">Examples of the verbs ' semantic structures 992 mapping open arguments -i.e. , variables of semantic structures whose referents can be expressed syntactically by a phrase within the same clause as the predicate -onto grammatical functions or underlying syntactic configurations by virtue of thematic roles ( thematic roles are positions in a structured semantic representation )</definiens>
			</definition>
			<definition id="2">
				<sentence>ACT , BE , GO , and BECOME are also functions and they correspond to actions , states , movement , and inchoatives respectively .</sentence>
				<definiendum id="0">ACT</definiendum>
				<definiens id="0">also functions and they correspond to actions , states , movement , and inchoatives respectively</definiens>
			</definition>
</paper>

		<paper id="2225">
			<definition id="0">
				<sentence>The text information , on the other hand , is a secondary modality in TV newscasts , which gives us : • explanations of image information , • summaries of speech information , and • information which is not concerned with the reports ( e.g. a time signal ) .</sentence>
				<definiendum id="0">• information</definiendum>
				<definiens id="0">a secondary modality in TV newscasts , which gives us : • explanations of image information , • summaries of speech information</definiens>
			</definition>
			<definition id="1">
				<sentence>The alignment process consists of two steps .</sentence>
				<definiendum id="0">alignment process</definiendum>
			</definition>
			<definition id="2">
				<sentence>If we are given a TV news article z and a newspaper article y , we obtain the reliability score by using the words k ( k 1 ... N ) which are extracted from the TV news article z : SCORE ( z , y ) = N 4 2 ~ ~ w ( i , j ) , hap , r ( i , k ) , fTv ( j , k ) '' length ( k ) k=l i=1 j=l where w ( i , j ) is the weight which is given to according to the location of word k in each article .</sentence>
				<definiendum id="0">N</definiendum>
			</definition>
</paper>

		<paper id="2157">
			<definition id="0">
				<sentence>We use stochastic TAGs as such a structured language model in contrast with earlier work where TAGs have been exploited in a class-based n-gram language model ( Srinivas , 1996 ) .</sentence>
				<definiendum id="0">stochastic TAGs</definiendum>
				<definiens id="0">such a structured language model in contrast with earlier work where TAGs have been exploited in a class-based n-gram language model</definiens>
			</definition>
			<definition id="1">
				<sentence>A , ¢ ) where NT is a set of nonterminal symbols , E is a set of terminal symbols , 2 : is a set of initial trees and .</sentence>
				<definiendum id="0">NT</definiendum>
				<definiendum id="1">E</definiendum>
				<definiens id="0">a set of nonterminal symbols</definiens>
				<definiens id="1">a set of initial trees and</definiens>
			</definition>
			<definition id="2">
				<sentence>For each leaf N in an elementary tree , except when it is a foot , we define label ( N ) to be the label of the node , which is either a terminal from E or the empty string e. For each other node N , label ( N ) is an element from NT .</sentence>
				<definiendum id="0">N )</definiendum>
				<definiens id="0">either a terminal from E or the empty string e. For each other node N , label</definiens>
				<definiens id="1">an element from NT</definiens>
			</definition>
			<definition id="3">
				<sentence>anw ) = ~ P ( \ [ t , O , n , - , -\ ] ) , wEE* fEZ where P is a function over items recursively defined as follows : P ( \ [ t , i , j , fl , f2\ ] ) = P ( \ [ Rt , i , j , fl , f2\ ] ) ; ( 1 ) P ( \ [ t~N , i , j , - , -\ ] ) = ( 2 ) P ( \ [ a , i , k , - , -\ ] ) .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a function over items recursively defined as follows : P ( \</definiens>
			</definition>
			<definition id="4">
				<sentence>( 9 ) Term P ( \ [ t , i , j , fl , f2\ ] ) gives the inside probability of all possible trees derived from elementary tree t , having the indicated span over the input .</sentence>
				<definiendum id="0">] )</definiendum>
				<definiens id="0">gives the inside probability of all possible trees derived from elementary tree t</definiens>
			</definition>
			<definition id="5">
				<sentence>( \ [ N , i , j , - , -\ ] , \ [ t , f~ , f~\ ] ) , if a # e A `` , dfl ( aN ) ; P~o~ ( \ [ ag , i , j , ft , f2\ ] , \ [ t , f { , f~\ ] ) = ( 14 ) 6 ( fl -- -- j ) '' Pto~ ( \ [ a , i , j , - , -\ ] , \ [ t , f { , foil ) • P ( \ [ N , j , j , fl , f2\ ] ) + P ( \ [ a , i , i , - , -\ ] ) • Pto~ , ( \ [ g , i , j , fl , f2\ ] , \ [ t , f~ , f~\ ] ) , if a # e A rift ( N ) ; P , o~ ( \ [ aN , i , j , fx , f2\ ] , \ [ t , f { , f~\ ] ) = ( 15 ) P~o~ ( \ [ a , i , j , f~ , f2\ ] , \ [ t , f~ , f~\ ] ) • P ( \ [ N , j , j , - , -\ ] ) + 6 ( i = f2 ) '' P ( \ [ a , i , i , fl , f2\ ] ) `` P~o~ ( \ [ N , i , j , - , -\ ] , \ [ t , f~ , f~\ ] ) , if a # e A dft ( a ) ; P~o~ , ( \ [ N , i , j , fl , f2\ ] , \ [ t , f { , f~\ ] ) : ( 16 ) ¢ ( nil , N ) • Pzo~ ( \ [ cdn ( N ) , i , j , fl , f2\ ] , \ [ t , f { , f~\ ] ) + P~o , o ( \ [ cdn ( N ) , i , j , fl , f2\ ] , \ [ t , fl , f~\ ] ) • Et'eA ¢ ( t ' , g ) .</sentence>
				<definiendum id="0">P , o~ ( \</definiendum>
				<definiens id="0">fl , f2\ ] ) `` P~o~ ( \ [ N , i</definiens>
			</definition>
			<definition id="6">
				<sentence>Thus , Ht is the probability of all derived trees obtained from t , with no lexical node at their yields .</sentence>
				<definiendum id="0">Ht</definiendum>
			</definition>
			<definition id="7">
				<sentence>Similar quantities as above must be introduced for the case i = n. For instance , we can set H~ = P ( \ [ t , n , n , f , f\ ] ) , f specified as above , which gives the probability of all derived trees obtained from t ( with no restriction at their yields ) .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
			<definition id="8">
				<sentence>It is easy to see that such an algorithm can be made to run in time O ( n6 ) , where n is the length of the input prefix .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the input prefix</definiens>
			</definition>
			<definition id="9">
				<sentence>That is , after we have computed the prefix probability for a prefix al ... an , on input an+l we can extend the calculation to prefix al '' '' anan+l without having to recompute all intermediate steps that do not depend on an+l. This step takes time O ( n5 ) .</sentence>
				<definiendum id="0">prefix probability</definiendum>
				<definiens id="0">for a prefix al ... an , on input an+l we can extend the calculation to prefix al '' '' anan+l without having to recompute all intermediate steps that do not depend on an+l. This step takes time O ( n5 )</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>Lexicalized Tree Adjoining Grammars ( LTAG ) is a formalism integrating lexicon and grammar ( Joshi , 87 ; Schabes et al , 88 ) : its description units are lexicalized syntactic trees , the elementary trees .</sentence>
				<definiendum id="0">Lexicalized Tree Adjoining Grammars ( LTAG )</definiendum>
				<definiens id="0">a formalism integrating lexicon</definiens>
			</definition>
			<definition id="1">
				<sentence>For instance , a tree sketch is in general of depth &gt; 1 , and thus corresponds to a piece of derivation in a formalism using CF rewrite rules ( cf ( Kasper et al , 95 ) for the presentation of an LTAG as a compiled HPSG ) .</sentence>
				<definiendum id="0">CF rewrite rules</definiendum>
			</definition>
			<definition id="2">
				<sentence>Another difference is that while CF derivation uses very local units ( CF rules ) , MG uses partial descriptions of trees ( Rogers et Vijay-Shanker , 94 ) more suitable for the expression of syntactic generalizations .</sentence>
				<definiendum id="0">CF rules</definiendum>
				<definiendum id="1">MG</definiendum>
				<definiens id="0">uses partial descriptions of trees ( Rogers et Vijay-Shanker , 94 ) more suitable for the expression of syntactic generalizations</definiens>
			</definition>
			<definition id="3">
				<sentence>'IkL ' , ~ '' of cla. , q.~es language dependent Compatibility constraints ~C~ dimension 2 redistributions of functions , ¢ ... `` ~ , ,~ Crossing Translation into LTAG families 213 Figure 2 : Compilation of MG to LTAG A crossing class is a linguistic description that must fulfill the PACP .</sentence>
				<definiendum id="0">crossing class</definiendum>
				<definiens id="0">a linguistic description that must fulfill the PACP</definiens>
			</definition>
			<definition id="4">
				<sentence>Middle is characterized by a deletion of the subject , and a middle morphology ( a reflexive clitic se ) .</sentence>
				<definiendum id="0">middle morphology</definiendum>
				<definiens id="0">a reflexive clitic se )</definiens>
			</definition>
			<definition id="5">
				<sentence>( IL SE says horrible things here ) Horrible things are pronounced in here .</sentence>
				<definiendum id="0">IL SE</definiendum>
				<definiens id="0">says horrible things here ) Horrible things are pronounced in here</definiens>
			</definition>
</paper>

		<paper id="2244">
			<definition id="0">
				<sentence>where x is the actual fragment length , p is the preferred fragment length given by the user , and h is a scaling parameter that allows us to adjust the weight given to fragment length .</sentence>
				<definiendum id="0">x</definiendum>
			</definition>
			<definition id="1">
				<sentence>The average deviation davg with respect to the preferred fragment length p is defined as davg = ( ~-'~n= 1 \ [ P -lil ) /m where li is the length of fragment i , and m is the number of fragments .</sentence>
				<definiendum id="0">fragment length p</definiendum>
				<definiendum id="1">davg =</definiendum>
				<definiendum id="2">li</definiendum>
				<definiendum id="3">m</definiendum>
				<definiens id="0">the length of fragment i , and</definiens>
				<definiens id="1">the number of fragments</definiens>
			</definition>
</paper>

		<paper id="2243">
			<definition id="0">
				<sentence>For the first Nb boundaries , Nt is the number of boundaries that match with document breaks .</sentence>
				<definiendum id="0">Nt</definiendum>
			</definition>
			<definition id="1">
				<sentence>Precision is 1482 10 5 0.5 20 10 0.5 30 17 0.58 38 19 0.5 40 20 0.5 50 24 0.48 60 26 0.43 67 ( Nbmax ) 26 0.39 Table 1 : Results of the experiment given by Nt/Nb and recall , by Nt/N , where N is the number of document breaks .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">Results of the experiment given by Nt/Nb and recall</definiens>
			</definition>
</paper>

		<paper id="1066">
			<definition id="0">
				<sentence>Intermezzo employs an abstraction layer to permit simultaneous querying of multiple databases .</sentence>
				<definiendum id="0">Intermezzo</definiendum>
				<definiens id="0">employs an abstraction layer to permit simultaneous querying of multiple databases</definiens>
			</definition>
			<definition id="1">
				<sentence>An entry consists of an object ID and a weight .</sentence>
				<definiendum id="0">entry</definiendum>
				<definiens id="0">consists of an object ID and a weight</definiens>
			</definition>
			<definition id="2">
				<sentence>Exact matches ( disregarding inflectional morphology ) rank 100 .</sentence>
				<definiendum id="0">Exact matches</definiendum>
			</definition>
</paper>

		<paper id="1077">
			<definition id="0">
				<sentence>M : A -- o B ~-c R r , y : A be M\ [ y/x\ ] : B r F-c Ax .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">A -- o B ~-c R r</definiens>
			</definition>
</paper>

		<paper id="1121">
</paper>

		<paper id="2147">
			<definition id="0">
				<sentence>In particular , speech understanding applications require appropriate grammars both to constrain speech recognition and to help extract the meaning of utterances .</sentence>
				<definiendum id="0">appropriate</definiendum>
				<definiens id="0">grammars both to constrain speech recognition and to help extract the meaning of utterances</definiens>
			</definition>
			<definition id="1">
				<sentence>A dynamic substitution consists of the application of the substitution a to ~ , during the process of recognition of a word sequence .</sentence>
				<definiendum id="0">dynamic substitution</definiendum>
				<definiens id="0">consists of the application of the substitution a to ~ , during the process of recognition of a word sequence</definiens>
			</definition>
			<definition id="2">
				<sentence>The automaton M ( X ) that represents the language generated by nonterminal symbol X can be defined using K ( S ) , where S is the strongly connected component containing X , X E S. For instance , when the subgrammar of S is right-linear , M ( X ) is the automaton that has the same states , transitions , and final states as K ( S ) and has the state corresponding to X as initial state .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">X )</definiendum>
				<definiens id="0">the automaton that has the same states , transitions , and final states as K ( S )</definiens>
			</definition>
			<definition id="3">
				<sentence>Our implementation of the compilation algorithm is part of a genera\ ] set of grammar tools , the GRM Library ( Mohri , 1998b ) , currently used in speech processing projects at AT &amp; T Labs .</sentence>
				<definiendum id="0">GRM Library</definiendum>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>Our semi-automatic bracketer ( Barker , 1998 ) allows for any number of adjective or noun premodifiers .</sentence>
				<definiendum id="0">semi-automatic bracketer</definiendum>
				<definiens id="0">any number of adjective or noun premodifiers</definiens>
			</definition>
			<definition id="1">
				<sentence>If the minimum distance was 3 or 6 , there may be two candidate lists : LM contains the NMRs previously assigned to triples with matching M , L , -with matching H. The analyzer attempts to choose a set R of candidates to suggest to the user as the best NMRs for the current triple , If there is one list L of candidate NMRs , R contains the NMR ( or NMRs ) that occur most frequently in L For two lists LM and L , , R could be found in several ways , We could take R to contain the most frequent NMRs in LM u L , .</sentence>
				<definiendum id="0">LM</definiendum>
				<definiendum id="1">R</definiendum>
				<definiendum id="2">NMR</definiendum>
				<definiens id="0">contains the NMRs previously assigned to triples with matching M , L , -with matching H. The analyzer attempts to choose a set R of candidates to suggest to the user as the best NMRs for the current triple , If there is one list L of candidate NMRs ,</definiens>
			</definition>
			<definition id="2">
				<sentence>A session begins with no previous triples to match against the triple at hand .</sentence>
				<definiendum id="0">session</definiendum>
				<definiens id="0">begins with no previous triples to match against the triple at hand</definiens>
			</definition>
			<definition id="3">
				<sentence>The NMR analyzer is one part of a larger interactive semantic analysis system .</sentence>
				<definiendum id="0">NMR analyzer</definiendum>
				<definiens id="0">one part of a larger interactive semantic analysis system</definiens>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>WordNet was designed principally as a semantic network , and contains little syntactic information .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">a semantic network , and contains little syntactic information</definiens>
			</definition>
			<definition id="1">
				<sentence>Instead , in their use as split verbs , each verb manifests an extended sense that can be paraphrased as `` separate by V-ing , '' where `` V '' is the basic meaning of that verb ( Levin , 1993 ) .</sentence>
				<definiendum id="0">V ''</definiendum>
			</definition>
</paper>

		<paper id="2190">
			<definition id="0">
				<sentence>This paper derives the conditions under which a given probabilistic Tree Adjoining Grammar ( TAG ) can be shown to be consistent .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">the conditions under which a given probabilistic Tree Adjoining Grammar (</definiens>
			</definition>
			<definition id="1">
				<sentence>For each leaf A E V , label ( A ) is an element from E U { e } , and for each other node A , label ( A ) is an element from N. S is an element from N which is a distinguished start symbol .</sentence>
				<definiendum id="0">label</definiendum>
				<definiendum id="1">A )</definiendum>
				<definiens id="0">an element from N. S is an element from N which is a distinguished start symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>¢ is a function which assigns each adjunction with a probability and denotes the set of parameters 1Note that for CFGs it has been shown in ( Chaudhari et al. , 1983 ; S~nchez and Bened~ , 1997 ) that insideoutside reestimation can be used to avoid inconsistency .</sentence>
				<definiendum id="0">¢</definiendum>
				<definiens id="0">a function which assigns each adjunction with a probability and denotes the set of parameters 1Note that for CFGs it has been shown in ( Chaudhari et al. , 1983 ; S~nchez and Bened~ , 1997 ) that insideoutside reestimation can be used to avoid inconsistency</definiens>
			</definition>
			<definition id="3">
				<sentence>A probabilistic TAG G is consistent if and only if : Pr ( v ) = 1 ( 4 ) veLCG ) where Pr ( v ) is the probability assigned to a string in the language .</sentence>
				<definiendum id="0">probabilistic TAG G</definiendum>
				<definiendum id="1">Pr</definiendum>
				<definiens id="0">the probability assigned to a string in the language</definiens>
			</definition>
			<definition id="4">
				<sentence>tl ~ t2 ¢ ( A1 ~-~ t2 ) = 0.8 ¢ ( A1 ~-+ nil ) = 0.2 B1 A* I I a2 B* a3 ¢ ( A2 ~-~ t2 ) = 0.2 ¢ ( B2 ~-~ t3 ) = 0.1 ¢ ( A2~+nil ) =0.8 ¢ ( B2~nil ) =0.9 ¢ ( B1 ~+ t3 ) = 0.2 ¢ ( B1 ~-+ nil ) = 0.8 ¢ ( A3 ~-~ t2 ) = 0.4 ¢ ( A3 ~-~ nil ) = 0.6 ( 5 ) From this grammar , we compute a square matrix A4 which of size IVI , where V is the set of nodes in the grammar that can be rewritten by adjunction .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">A1 ~-~ t2 ) = 0.8 ¢ ( A1 ~-+ nil ) = 0.2 B1 A* I I a2 B* a3 ¢ ( A2 ~-~ t2</definiens>
				<definiens id="1">a square matrix A4 which of size IVI , where</definiens>
			</definition>
			<definition id="5">
				<sentence>Each AzIij contains the expected value of obtaining node Xj when node Xi is rewritten by adjunction at each level of a TAG derivation .</sentence>
				<definiendum id="0">AzIij</definiendum>
				<definiendum id="1">Xi</definiendum>
				<definiens id="0">contains the expected value of obtaining node Xj when node</definiens>
			</definition>
			<definition id="6">
				<sentence>Hence the probability of extinction is the same as the probability that a probabilistic TAG is consistent .</sentence>
				<definiendum id="0">Hence the probability of extinction</definiendum>
				<definiens id="0">the probability that a probabilistic TAG is consistent</definiens>
			</definition>
			<definition id="7">
				<sentence>For each Xj E V , where V is the set of nodes in the grammar where adjunction can occur , we define the k-argument adjunction generating \ ] unction over variables si , ... , Sk corresponding to the k nodes in V. gj ( sl , ... , 8k ) = E teAdj ( Xj ) u { niQ ¢ ( xj t ) .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">the set of nodes in the grammar where adjunction can occur</definiens>
			</definition>
			<definition id="8">
				<sentence>s4+0.2 G2 ( sl , ... ,85 ) = ¢ ( A2 ~-+ t2 ) \ [ g2 ( sy , ... , 85 ) \ ] \ [ g3 ( 81 , ... , 85 ) \ ] \ [ g4 ( 81 , ... , 85 ) \ ] -\ [ ¢ ( A2 ~ nil ) 222 222 = 0.0882838485 + 0.03828384 + 0.0482838485 + Examining this example , we can express Gi ( s1 , ... , Sk ) as a sum Di ( sl , ... , Sk ) + Ci , where Ci is a constant and Di ( . )</sentence>
				<definiendum id="0">Ci</definiendum>
				<definiens id="0">a sum Di ( sl , ... , Sk ) + Ci , where</definiens>
			</definition>
</paper>

		<paper id="2203">
			<definition id="0">
				<sentence>Natural language evolution takes place at all levels of language ( McMahon , 1994 ) .</sentence>
				<definiendum id="0">Natural language evolution</definiendum>
			</definition>
			<definition id="1">
				<sentence>A score is imposed over the members of the set of candidate words : 1 = 1 + ( 3 ) ¢I is the form-focus factor .</sentence>
				<definiendum id="0">¢I</definiendum>
				<definiens id="0">the form-focus factor</definiens>
			</definition>
			<definition id="2">
				<sentence>a maximum and lexicon coherence ( measured as the average spread in the population of the most dominant form-meaning pairs ) is high ( 100 % ) In the early stage there is important lexicon change as new form-meaning pairs need to be generated from scratch by the agents .</sentence>
				<definiendum id="0">lexicon coherence</definiendum>
				<definiens id="0">the average spread in the population of the most dominant form-meaning pairs</definiens>
			</definition>
			<definition id="3">
				<sentence>It is modeled with a second stochastic operator F , the formstochasticity , which is the probability that a character in the string constituting the word form mutates .</sentence>
				<definiendum id="0">formstochasticity</definiendum>
				<definiens id="0">the probability that a character in the string constituting the word form mutates</definiens>
			</definition>
			<definition id="4">
				<sentence>These errors are modeled using a third stochastic operator based on a parameter A , the memory-stochasticity , which alters the scores of the associations in the score matrix in a probabilistic fashion .</sentence>
				<definiendum id="0">memory-stochasticity</definiendum>
				<definiens id="0">alters the scores of the associations in the score matrix in a probabilistic fashion</definiens>
			</definition>
			<definition id="5">
				<sentence>Memory-stochasticity shifts the balance among the word-meaning associations competing for the expression of the same meaning .</sentence>
				<definiendum id="0">Memory-stochasticity</definiendum>
				<definiens id="0">shifts the balance among the word-meaning associations competing for the expression of the same meaning</definiens>
			</definition>
			<definition id="6">
				<sentence>A companion figure ( figure 4 ) focuses on the competition between different forms for the same meaning .</sentence>
				<definiendum id="0">companion figure</definiendum>
				<definiens id="0">the competition between different forms for the same meaning</definiens>
			</definition>
			<definition id="7">
				<sentence>A period of instability follows after which a new dominant winner ( `` kugo '' ) emerges .</sentence>
				<definiendum id="0">period of instability</definiendum>
			</definition>
</paper>

		<paper id="2237">
			<definition id="0">
				<sentence>Current state-of-the-art recognizers exhibit word error rates ( WER 1 ) for this corpus of approxIThe word error rate ( WEFt in % ) is defined as follows : imately 30 % -40 % ( Finke et al. , 1997 ) .</sentence>
				<definiendum id="0">Current state-of-the-art recognizers exhibit word error rates</definiendum>
			</definition>
			<definition id="1">
				<sentence>5 So , if we had an oracle that could tell the speech recognizer to always pick the hypothesis with the lowest WER from the Nbest list ( instead of the top 3A VC-chunk is a contiguous verbal segment of an utterance , whereas a VP usually comprises this verbal segment and its arguments together .</sentence>
				<definiendum id="0">Nbest list</definiendum>
				<definiens id="0">a contiguous verbal segment of an utterance</definiens>
			</definition>
			<definition id="2">
				<sentence>The chunk parser is a chart based context free parser , originally developed for the purpose of semantic frame parsing ( Ward , 1991 ) .</sentence>
				<definiendum id="0">chunk parser</definiendum>
			</definition>
</paper>

		<paper id="2198">
			<definition id="0">
				<sentence>Such answers are an inevitable consequence of any method not grammatically founded .</sentence>
				<definiendum id="0">Such answers</definiendum>
			</definition>
			<definition id="1">
				<sentence>These graphs are equivalent to FSTs with inverted representation ( FST ) ( Roche and Schabes , 1997 ) as in figure 1 , where each box represents a transition of the automaton ( input of the transducer ) , and the label under a box is an output of the transducer .</sentence>
				<definiendum id="0">FST</definiendum>
				<definiens id="0">an output of the transducer</definiens>
			</definition>
			<definition id="2">
				<sentence>val ( c ) is not defined as c is not recognized by the automaton , val ( ca ) = { d } , and val ( cb ) = { b } .</sentence>
				<definiendum id="0">val ( c )</definiendum>
				<definiens id="0">c is not recognized by the automaton , val ( ca ) = { d } , and val ( cb ) = { b }</definiens>
			</definition>
</paper>

		<paper id="2236">
</paper>

		<paper id="2222">
			<definition id="0">
				<sentence>Searchable LEAD processes over 500,000 characters ( 90 news documents ) per CPU second .</sentence>
				<definiendum id="0">Searchable LEAD</definiendum>
				<definiens id="0">processes over 500,000 characters ( 90 news documents ) per CPU second</definiens>
			</definition>
			<definition id="1">
				<sentence>A dynamic summary generator , perhaps using readers ' queries to guide it , can help readers focus on those parts of a document that are most relevant to them .</sentence>
				<definiendum id="0">dynamic summary generator</definiendum>
				<definiens id="0">those parts of a document that are most relevant to them</definiens>
			</definition>
</paper>

		<paper id="2211">
			<definition id="0">
				<sentence>Firstly , S gets associated with the meaning of a sentence via a feature CONT ( ENT ) containing all discourse referents and the conditions imposed on them .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">gets associated with the meaning of a sentence via a feature CONT ( ENT ) containing all discourse referents and the conditions imposed on them</definiens>
			</definition>
			<definition id="1">
				<sentence>The summarising yes , is n't it ( i.e. ( U6b ) ) ends the search process and closes the discourse structure at the right frontier .</sentence>
				<definiendum id="0">summarising yes</definiendum>
				<definiens id="0">U6b ) ) ends the search process and closes the discourse structure at the right frontier</definiens>
			</definition>
</paper>

		<paper id="2196">
			<definition id="0">
				<sentence>The ICICLE grammar is a broad-coverage grammar designed to parse a wide variety of both grammatical sentences and sentences containing errors .</sentence>
				<definiendum id="0">ICICLE grammar</definiendum>
				<definiens id="0">a broad-coverage grammar designed to parse a wide variety of both grammatical sentences and sentences containing errors</definiens>
			</definition>
</paper>

		<paper id="1089">
			<definition id="0">
				<sentence>Quantifier Scoping The framework of Autolexical Grammar treats a language as the intersection of numerous independent CF-PSGs , or hierarchies , each of which corresponds to a specific structural or functional aspect of the language .</sentence>
				<definiendum id="0">Autolexical Grammar</definiendum>
				<definiens id="0">treats a language as the intersection of numerous independent CF-PSGs , or hierarchies , each of which corresponds to a specific structural or functional aspect of the language</definiens>
			</definition>
			<definition id="1">
				<sentence>The function-argument hierarchy expresses that ( formal ) semantic information about a sentence which does not involve scope resolution , e.g. , semantic valency and association of referential terms with argument positions , as in Park ( 1995 ) .</sentence>
				<definiendum id="0">function-argument hierarchy</definiendum>
				<definiens id="0">expresses that ( formal ) semantic information about a sentence which does not involve scope resolution , e.g. , semantic valency</definiens>
			</definition>
</paper>

		<paper id="2192">
			<definition id="0">
				<sentence>Several methods are known that can parse languages generated by Tree Adjoining Grammars ( TAGs ) in worst case time O ( n6 ) , where n is the length of the input string ( see ( Schabes and Joshi , 1991 ) and references therein ) .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
			<definition id="1">
				<sentence>A TAG is a tuple G = ( N , ~ , I , A , S ) , where N , ~ are the finite sets of nonterminal and terminal symbols , respectively , I , A are the finite 1176 sets of initial and auxiliary trees , respectively , and S E N is the initial symbol .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiendum id="1">S E N</definiendum>
				<definiens id="0">a tuple G</definiens>
				<definiens id="1">the finite sets of nonterminal and terminal symbols , respectively , I</definiens>
				<definiens id="2">the initial symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>We provide an algorithm for the recognition of languages generated by the subclass of TAGs introduced in the previous section , and show that the worst case running time is ( .9 ( n5 ) , where n is the length of the input string .</sentence>
				<definiendum id="0">show</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">an algorithm for the recognition of languages generated by the subclass of TAGs introduced in the previous section , and</definiens>
				<definiens id="1">the length of the input string</definiens>
			</definition>
			<definition id="3">
				<sentence>We remove every node to the right ( left ) of the spine of ~3D and call ~LD ( ~RD ) the resulting tree .</sentence>
				<definiendum id="0">call ~LD</definiendum>
				<definiens id="0">every node to the right ( left ) of the spine of ~3D and</definiens>
			</definition>
			<definition id="4">
				<sentence>{ R~ , k , p ) ( ~ , i , k , q , j ) ( \ [ ~ , iD\ ] , i , p , q , j ) , ~ ' E Adj ( Wz ) , p &lt; q ; &lt; R~sD , q , k ) ( \ [ ~ , LD\ ] , i , p , k , j ) &lt; \ [ ~ , Rn\ ] , i , p , q , j ) ' p &lt; q ; R T ( O~r~ , k , j ) &lt; \ [ ~ , RD\ ] , i , p , q , k ) ( \ [ ~ , RU\ ] , i , p , q , j ) ( R~L , ,i , k ) ( \ [ ~ , RU\ ] , k , p , q , j ) .</sentence>
				<definiendum id="0">R T</definiendum>
				<definiens id="0">q , j ) ( R~L , ,i , k ) ( \ [ ~ , RU\ ] , k , p , q , j )</definiens>
			</definition>
			<definition id="5">
				<sentence>The structure of an athematic auxiliary tree may thus be described as : X n _+ Xn ... ( ymax ) ... , ( 1 ) where X n is any projection of category X , y , nax is the maximal projection of Y , and the order of the constituents is variable .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">nax</definiendum>
				<definiens id="0">X n _+ Xn ... ( ymax ) ... , ( 1 ) where X</definiens>
				<definiens id="1">the maximal projection of Y</definiens>
			</definition>
			<definition id="6">
				<sentence>The structure of a complement auxiliary tree may be • described as : Xrnax _+ ... yO . . . Xrna~ . . . , ( 2 ) where X rna~ is the maximal projection Of some category X , and y0 is the lexical projection 2The same linguistic distinction is used in the conception of 'modifier ' and 'predicative ' trees ( Schabes and Shieber , 1994 ) , but Schabes and Shieber give the trees special properties in the calculation of derivation structures , which we do not .</sentence>
				<definiendum id="0">X rna~</definiendum>
				<definiendum id="1">y0</definiendum>
				<definiens id="0">the maximal projection Of some category X</definiens>
				<definiens id="1">the lexical projection 2The same linguistic distinction is used in the conception of 'modifier ' and 'predicative ' trees ( Schabes and Shieber , 1994 ) , but Schabes and Shieber give the trees special properties in the calculation of derivation structures</definiens>
			</definition>
			<definition id="7">
				<sentence>We have specified a method that recognizes the generated languages in worst case time O ( nS ) , where n is the length of the input string .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the input string</definiens>
			</definition>
</paper>

		<paper id="1085">
</paper>

		<paper id="2214">
			<definition id="0">
				<sentence>Verb-Noun Collocation Verb-noun collocation is a data structure for the collocation of a verb and all of its argument/adjunct nouns .</sentence>
				<definiendum id="0">Verb-Noun Collocation Verb-noun collocation</definiendum>
			</definition>
			<definition id="1">
				<sentence>Subsumption Relation We introduce the subsumption relation `` ~s $ of a verb-noun collo1Although we ignore sense ambiguities of case-marked nouns in the definitions of this section , in the current implementation , we deal with sense ambiguities of casemarked nouns by deciding that a class c is superordinate to an ambiguous leaf class Cl if c is superordinate to at least one of the possible unambiguous classes of Ct .</sentence>
				<definiendum id="0">subsumption relation</definiendum>
				<definiens id="0">superordinate to at least one of the possible unambiguous classes of Ct</definiens>
			</definition>
			<definition id="2">
				<sentence>for each case-marker Pi in s and its noun class csi , there exists the same case-marker pi in e and its noun class cei is subordinate to c~i , i.e. Cei `` &lt; c Csi The subsumption relation `` ~s $ is applicable also as a subsumption relation of two subcategorization frames. from Subcategorization Frame ( s ) Suppose a verb-noun collocation e is given as : Fred : v Pl : Cel e = . ( 3 ) Pk : Cek Then , let us consider a tuple ( sl , ... , sn ) of partial subcategorization frames which satisfies the following requirements : i ) the unification sl A ... Asn of all the partial subcategorization frames has exactly the same case-markers as e has as in ( 4 ) , ii ) each semantic class Csi of a case-marked noun of the partial subcategorization frames is superordinate to the corresponding leaf semantic class eei of e as in ( 5 ) , and iii ) any pair si and si , ( i 7£ i I ) do not have common case-markers as in ( 6 ) : S 1 A • `` `` A S n ~wed : v Pl : Csl Pk : Csk csi ( i=l , ... , k ) ( 4 ) pred : v \ ] J vjvj ' pij # pi , j , si = ' ( i , i'=l , .. , n , i # i ' ) ( 6 ) Pij : Cij When a tuple ( Sl , ... , sn ) satisfies the above three requirements , we assume that the tuple ( Sl , ... , sn ) can generate the verb-noun collocation e and denote as below : ( ~ , ... , ~. ) , e ( 7 ) As we will describe in section 3.2 , we assume that the partial subcategorization frames Sl , ... , Sn are regarded as events occurring independently of each other and each of them is assigned an independent parameter. This section shows how we can incorporate case dependencies and noun class generalization into the model of generating a verb-noun collocation from a tuple of partial subcategorization frames• 1315 The Ambiguity of Case Dependencies The problem of the ambiguity of case dependencies is caused by the fact that , only by observing each verb-noun collocation in corpus , it is not decidable which cases are dependent on each other and which cases are optional and independent of other cases. Consider the following example : Example 1 Kodomo-ga kouen-de juusu-wo nomu. child-NOM park-at juice-A CC drink ( A child drinks juice at the park. ) The verb-noun collocation is represented as a feature structure e below : pred : nomu \ ] ga : Cc \ ] e = wo : cj ( 8 ) de : cp where co , cp , and cj represent the leaf classes ( in the thesaurus ) of the nouns `` kodomo ( child ) '' , `` kouen ( park ) '' , and '~uusu ( juice ) '. Next , we assume that the concepts `` human '' , `` place '' , and `` beverage '' are superordihate to `` kodomo ( child ) '' , `` kouen ( park ) '' , and '~uusu ( juice ) '' , respectively , and introduce the corresponding classes Chum , Cplc , and Cbe v as sense restriction in subcategorization frames. Then , according to the dependencies of cases , we can consider several patterns of subcategorization frames each of which can generate the verb-noun collocation e. If the three cases `` ga ( NOM ) '' , `` wo ( ACC ) '' , and `` de ( at ) '' are dependent on each other and it is not possible to find any division into several independent subcategorization frames , e can be regarded as generated from a subcategorization frame containing all of the three cases : ga : Chum : ' e ( 9 ) WO : Cbev de : Cptc Otherwise , if only the two cases `` ga ( NOM ) '' and `` wo ( A CC ) '' are dependent on each other and the `` de ( at ) '' case is independent of those two cases , e can be regarded as generated from the following two subcategorization frames independently : ga : Chu m ' de : Cpl c ~ e WO : Cbe v The Ambiguity of Noun Class Generalization The problem of the ambiguity of noun class generalization is caused by the fact that , only by observing each verb-noun collocation in corpus , it is not decidable which superordinate class generates each observed leaf class in the verb-noun collocation. Let us again consider Example 1. We assume that the concepts `` mammal '' and `` liquid '' are superordinate to `` human '' and `` beverage '' , respectively , and introduce the corresponding classes Cma m and Ctiq. If we additionally allow these superordinate classes as sense restriction in subcategorization frames , we can consider several additional patterns of subcategorization frames which can generate the verbnoun collocation e. Suppose that only the two cases `` ga ( NOM ) '' and `` wo ( ACC ) '' are dependent on each other and the `` de ( at ) '' case is independent of those two cases as in the formula ( 10 ) . Since the leaf class cc ( `` child '' ) can be generated from either Chum or cream , and also the leaf class cj ( '~uice ' ) can be generated from either Cbe v or Cliq , e can be regarded as generated according to either of the four formulas ( 10 ) and ( 11 ) , ~ ( 13 ) : ga : Cma m ~ de : Cpl c ) e WO : Cbe v ga : Chum ' de : Cpl c &gt; WO : Cliq ga : c ... . de : % to , e ( 13 ) WO : Cliq Subcategorization Preference This section describes how we apply the maximum entropy modeling approach of Della Pietra et al. ( 1997 ) and Berger et al. ( 1996 ) to model learning of subcategorization preference .</sentence>
				<definiendum id="0">subsumption relation</definiendum>
				<definiendum id="1">Sl , ... , sn</definiendum>
				<definiendum id="2">Sl , ... , sn</definiendum>
				<definiendum id="3">NOM</definiendum>
				<definiendum id="4">NOM</definiendum>
				<definiens id="0">applicable also as a subsumption relation of two subcategorization frames. from Subcategorization Frame ( s ) Suppose a verb-noun collocation e is given as : Fred : v Pl</definiens>
				<definiens id="1">additional patterns of subcategorization frames which can generate the verbnoun collocation e. Suppose that only the two cases `` ga ( NOM ) '' and</definiens>
			</definition>
			<definition id="3">
				<sentence>According to those sets , each feature function fi will be defined as follows : 1 ifxE V~iandyEVyi fi ( x , y ) = 0 otherwise Then , in the maximum entropy modeling approach , the model with the maximum entropy is selected among the possible models .</sentence>
				<definiendum id="0">feature function fi</definiendum>
				<definiens id="0">follows : 1 ifxE V~iandyEVyi fi ( x , y ) = 0 otherwise Then , in the maximum entropy modeling approach , the model with the maximum entropy is selected among the possible models</definiens>
			</definition>
			<definition id="4">
				<sentence>A verb-noun collocation e can be divided into two parts : one is the verbal part ev containing the verb v while the other is the nominal part ep containing all the pairs of case-markers p and thesaurus leaf classes c of case-marked nouns : Pk Ck Then , we define the context x of an event ( x , y ) as the verb v and the output y as the nominal part &amp; of e , and each event in the training sample is denoted as ( v , % ) : x = v , y -~ ep Features We represent each partial subcategorization frame as a feature in the maximum entropy modeling .</sentence>
				<definiendum id="0">verb-noun collocation e</definiendum>
				<definiendum id="1">context x of an event</definiendum>
				<definiens id="0">the verb v and the output y as the nominal part &amp; of e</definiens>
			</definition>
			<definition id="5">
				<sentence>Next , for the given verb-noun collocation e , tuples of partial subcategorization frames which can generate e are collected into the set SF ( e ) as below : Then , for each partial subcategorization frame s , a binary-valued feature function fs ( V , ep ) is defined to be true if and only if at least one element of the set SF ( e ) is a tuple ( sl , ... , s , ... , sn ) that contains s : 1 if 3 ( sl , ... , s , ... , sn ) f , ( v , ep ) = • SF ( e= ( \ [ pred : v\ ] A % ) ) 0 otherwise 1317 In the maximum entropy modeling approach , each feature is assigned an independent parameter , i.e. , each ( partial ) subcategorization frame is assigned an independent parameter .</sentence>
				<definiendum id="0">maximum entropy modeling approach</definiendum>
				<definiens id="0">a tuple ( sl , ... , s , ... , sn ) that contains s : 1 if 3 ( sl , ... , s , ... , sn</definiens>
			</definition>
			<definition id="6">
				<sentence>The MDL principle selects the model that minimizes the following description length l ( M , D ) of the probability model M for the data D : 1N l ( M , D ) = -logLM ( D ) + ~ MloglO I ( 17 ) where logLM ( D ) is the log-likelihood of the model M to the data D , NM is the number of the parameters in the model 21I , and IDI is the size of the data D. Description Length of Subcategorization Preference Model The description length l ( ps , £ ) of the probability model Ps ( of ( 15 ) ) for the training data set C is given as below : 4 l ( ps , C ) = ~ logps ( % Iv ) + llsIloglCI ( 18 ) ( v , e , . )</sentence>
				<definiendum id="0">MDL principle</definiendum>
				<definiendum id="1">logLM</definiendum>
				<definiendum id="2">NM</definiendum>
				<definiendum id="3">IDI</definiendum>
				<definiens id="0">the log-likelihood of the model M to the data D ,</definiens>
				<definiens id="1">the size of the data D. Description Length of Subcategorization Preference Model The description length l ( ps</definiens>
			</definition>
			<definition id="7">
				<sentence>As the training and test corpus , we used the EDR Japanese bracketed corpus ( EDR , 1995 ) , which contains about 210,000 sentences collected from newspaper and magazine articles .</sentence>
				<definiendum id="0">EDR Japanese bracketed corpus</definiendum>
				<definiens id="0">contains about 210,000 sentences collected from newspaper and magazine articles</definiens>
			</definition>
			<definition id="8">
				<sentence>BGH has a sevenlayered abstraction hierarchy and more than 60,000 words are assigned at the leaves and its nominal part contains about 45,000 words .</sentence>
				<definiendum id="0">BGH</definiendum>
				<definiens id="0">a sevenlayered abstraction hierarchy and more than 60,000 words</definiens>
			</definition>
			<definition id="9">
				<sentence>Coverage is the rate of test instances which satisfy the case covering constraint of section 4.1 .</sentence>
				<definiendum id="0">Coverage</definiendum>
			</definition>
</paper>

		<paper id="1101">
			<definition id="0">
				<sentence>f17 E ~ ( AT , e ) : A E N , 3 ' E ( N W T ) * , A -- ~ fl • P. A string w is accepted by the top-down recognition algorithm if q/ E 5* ( q0 , w ) , where 5* is the reflexive transitive closure of 6 with respect to epsilon moves .</sentence>
				<definiendum id="0">P. A string w</definiendum>
				<definiens id="0">the reflexive transitive closure of 6 with respect to epsilon moves</definiens>
			</definition>
			<definition id="1">
				<sentence>The left-corner trans/orm of a CFG G = ( N , T , P , S ) is a grammar/2C1 ( G ) = ( N ' , T , P1 , S ) , where P1 contains all productions of the form ( 1 .</sentence>
				<definiendum id="0">left-corner trans/orm</definiendum>
				<definiendum id="1">S )</definiendum>
				<definiens id="0">of a CFG G = ( N , T , P ,</definiens>
				<definiens id="1">a grammar/2C1 ( G ) = ( N ' , T , P1 , S ) , where P1 contains all productions of the form ( 1</definiens>
			</definition>
</paper>

		<paper id="2134">
			<definition id="0">
				<sentence>The utility of annotated bitexts will be demonstrated by the proposition of a methodology that crucially takes advantage of rich markup to resolve bitext correspondences , that is , the task of providing correct identification and alignment methods for text segments that are translation equivalencies of each other ( Chang &amp; Chen 97 ) .</sentence>
				<definiendum id="0">utility of annotated bitexts</definiendum>
				<definiens id="0">the task of providing correct identification and alignment methods for text segments that are translation equivalencies of each other ( Chang &amp; Chen 97 )</definiens>
			</definition>
			<definition id="1">
				<sentence>Although the alignment of smaller segments ( multi-word lexical units and collocations ) will require more expressive tagging , such as part-of-speech tagging ( POS ) , for the task of sentence alignment , this is not only unnecessary , but also inappropriate , since it would introduce undesired language dependent information .</sentence>
				<definiendum id="0">POS</definiendum>
				<definiens id="0">multi-word lexical units and collocations</definiens>
			</definition>
			<definition id="2">
				<sentence>Provided an adequate and consistent bitext mark-up , sentence alignment becomes a simple and accurate process also in the case of typologically disparate or orthographically distinct language pairs for which techniques based on lexical cognates may be problematic .</sentence>
				<definiendum id="0">sentence alignment</definiendum>
			</definition>
			<definition id="3">
				<sentence>The alignment algorithm establishes similarity metrics between candidate sentences which are delimited by corresponding mark-up .</sentence>
				<definiendum id="0">alignment algorithm</definiendum>
				<definiens id="0">establishes similarity metrics between candidate sentences which are delimited by corresponding mark-up</definiens>
			</definition>
			<definition id="4">
				<sentence>For two text segments , P and Q , one in each language , the formula for Dice 's similarity coefficient will be : Dice ( P , Q ) -2FpQ Fp + FQ where FpQ is the number of identical tags that P and Q have in common , and Fp and FQ are the number of tags contained by each text segment P and Q. Since the alignment algorithm determines the best matching on the basis of tag similarity , not only tag names used to categorise different cognate classes ( number , date , abbreviation , proper noun , etc. ) , but also attributes contained by these tags may help identify the cognate itself : &lt; num num=57 &gt; 57 &lt; /num &gt; .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">FpQ</definiendum>
				<definiendum id="2">FQ</definiendum>
				<definiens id="0">the number of identical tags that P and Q have in common</definiens>
				<definiens id="1">the number of tags contained by each text segment P and Q. Since the alignment algorithm determines the best matching on the basis of tag similarity , not only tag names used to categorise different cognate classes ( number , date , abbreviation , proper noun , etc. ) , but also attributes contained by these tags may help identify the cognate itself</definiens>
			</definition>
			<definition id="5">
				<sentence>The difference mean between Dice 's coefficients corresponding to correct alignments and next higher values is : n ~ ( DCci DCwi ) M = i=1 = 0.45 n Where for a given source sentence i , DCci represents Dice 's coefficient corresponding to its correct alignment and DCwi represents the next higher value of Dice 's coefficients for the same source sentence i. In all the cases , this difference is greater than 0.2 .</sentence>
				<definiendum id="0">DCci</definiendum>
				<definiendum id="1">DCwi</definiendum>
				<definiens id="0">the next higher value of Dice 's coefficients for the same source</definiens>
			</definition>
			<definition id="6">
				<sentence>Let G = ( V , E , U ) be a bipartite graph , such that V and U are two disjoint sets of vertices , and E is a set of edges connecting vertices from V to vertices in U. Each edge in E has associated a cost .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">a set of edges connecting vertices from V to vertices in U. Each edge in E has associated a cost</definiens>
			</definition>
</paper>

		<paper id="2177">
			<definition id="0">
				<sentence>The extracted head word tuples differ from the training data used in previous supervised attempts in an important way .</sentence>
				<definiendum id="0">extracted head word tuples</definiendum>
				<definiens id="0">differ from the training data used in previous supervised attempts in an important way</definiens>
			</definition>
			<definition id="1">
				<sentence>Unambiguous Cases Given a tagged and chunked sentence , the extraction heuristic returns head word tuples of the form ( v , p , n2 ) or ( n , p , n2 ) , where v is the verb , n is the noun , p is the preposition , n2 is the object of the preposition .</sentence>
				<definiendum id="0">v</definiendum>
				<definiendum id="1">n</definiendum>
				<definiendum id="2">p</definiendum>
				<definiendum id="3">n2</definiendum>
				<definiens id="0">a tagged and chunked sentence , the extraction heuristic returns head word tuples of the form ( v , p , n2 ) or ( n , p</definiens>
			</definition>
			<definition id="2">
				<sentence>( n =lawyers , p =in , n2 =jurisdictions ) ( v =guided , p =by , n2 =rules ) ( n =lawyer , p =in , n2 =jurisdiction ) ( v =guide , p =by , n2 =rule ) Table 1 : How to obtain training data from raw text • n2 is the first noun that occurs within K words to the right of p • No verb occurs between p and n2 ( n , p , n2 ) if • p is a preposition ( p ~ of ) • n is the first noun that occurs within K words to the left of p • No verb occurs within K words to the left of p • n2 is the first noun that occurs within K words to the right of p • No verb occurs between p and n2 Table 1 also shows the result of the applying the extraction heuristic to a sample sentence .</sentence>
				<definiendum id="0">n2</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">p =in , n2 =jurisdictions ) ( v =guided , p =by , n2 =rules ) ( n =lawyer , p =in , n2 =jurisdiction ) ( v =guide , p =by</definiens>
				<definiens id="1">How to obtain training data from raw text •</definiens>
				<definiens id="2">the first noun that occurs within K words to the right of p • No verb occurs between p and n2 ( n , p</definiens>
				<definiens id="3">the first noun that occurs within K words to the left of p • No verb occurs within K words to the left of p • n2 is the first noun that occurs within K words to the right of p • No verb occurs between p</definiens>
			</definition>
			<definition id="3">
				<sentence>The Z ( v , n ) term exists only to make the approximation a well formed probability over a E { N , V } .</sentence>
				<definiendum id="0">Z</definiendum>
				<definiens id="0">n ) term exists only to make the approximation a well formed probability over a E { N</definiens>
			</definition>
			<definition id="4">
				<sentence>c ( n , p , true ) Pr ( pltrue , n ) = ~ ( n , true ) c ( n , true ) &gt; 0 otherwise c ( v , p , true ) Pr ( pltrue , v ) = ~ ( v , tr~ , ) c ( v , true ) &gt; 0 otherwise where ~ is the set of possible prepositions , where all the counts c ( ... ) are from the extracted head word tuples .</sentence>
				<definiendum id="0">c (</definiendum>
				<definiens id="0">n , p , true ) Pr ( pltrue , n ) = ~ ( n , true ) c ( n , true ) &gt; 0 otherwise c ( v , p , true</definiens>
				<definiens id="1">the set of possible prepositions , where all the counts c ( ... ) are from the extracted head word tuples</definiens>
			</definition>
			<definition id="5">
				<sentence>This technique is similar to the one in ( Hindle and Rooth , 1993 ) , and interpolates between the tendencies of the ( v , p ) and ( n , p ) bigrams and the tendency of the type of attachment ( e.g. , N or V ) towards a particular preposition p. First , define cN ( p ) = ~n c ( n , p , true ) as the number of noun attached tuples with the preposition p , and define C N = ~'~pCN ( P ) as the number of noun attached tuples .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">n , p ) bigrams and the tendency of the type of attachment ( e.g. , N or V ) towards a particular preposition p. First , define cN ( p ) = ~n c</definiens>
			</definition>
			<definition id="6">
				<sentence>Clinterp : This classifier has the form of equation ( 1 ) , uses the method in section 4.1 to generate ¢ , and the method in section 4.2.2 to generate p. clbigram : This classifier has the form of equation ( 1 ) , uses the method in section 4.1 to generate ¢ , and the method in section 4.2.1 to generate p. Table 4 shows accuracies of the classifiers on the test set of ( Ratnaparkhi et al. , 1994 ) , which is derived from the manually annotated attachments in the Penn Treebank Wall St. Journal data .</sentence>
				<definiendum id="0">Clinterp</definiendum>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>A set of Interpretation Rules ( IRs ) applies whenever an anaphor is encountered , proposing potential antecedents from the registers , from which one is chosen using other criteria : syntactic , semantic , inferential , etc. 74 Sidner 's algorithmic account , although not exhaustively specified , has lead to the implementation of focus-based approaches to anaphora resolution in several systems , e.g. PIE ( Lin , 1995 ) .</sentence>
				<definiendum id="0">Interpretation Rules</definiendum>
				<definiens id="0">syntactic , semantic , inferential , etc. 74 Sidner 's algorithmic account , although not exhaustively specified , has lead to the implementation of focus-based approaches to anaphora resolution in several systems</definiens>
			</definition>
			<definition id="1">
				<sentence>The LaSIE system ( Gaizauskas et al. , 1995 ) and ( Humphreys et al. , 1998 ) , has been designed as a general purpose IE system which can conform to the MUC task specifications for named entity identification , coreference resolution , IE template element and relation identification , and the construction of scenario-specific IE templates .</sentence>
				<definiendum id="0">LaSIE system</definiendum>
				<definiens id="0">a general purpose IE system which can conform to the MUC task specifications for named entity identification , coreference resolution , IE template element and relation identification</definiens>
			</definition>
			<definition id="2">
				<sentence>The AF remains unchanged as the current EE lacks an agent : CF = propeller AF = witnesses FS = State Police , tell_event AFL = propeller , witnesses EE-4 : the plane descended Intra-AFL = the plane CF = the plane ( theme ) AF = witnesses ( unchanged ) FS = propeller , State Police , tell_event AFL = the plane , propeller , witnesses In the current algorithm the AFL is reset at this point , because EE-4 ends the sentence .</sentence>
				<definiendum id="0">EE</definiendum>
				<definiendum id="1">propeller</definiendum>
				<definiens id="0">lacks an agent : CF = propeller AF = witnesses FS = State Police</definiens>
			</definition>
</paper>

		<paper id="2206">
			<definition id="0">
				<sentence>Definition 1 Given a Chinese character string 'xy ' , the mutual information between characters x and 3 , ( or equally , the mutual information of the location between x and y ) is defined as : mi ( x : y ) = log 2 p ( x , y ) p ( x ) p ( y ) where p ( x , y ) is the co-occurrence probability of x and y , and p ( x ) , p ( y ) are the independent probabilities of x and y respectively .</sentence>
				<definiendum id="0">p ( x , y )</definiendum>
				<definiens id="0">a Chinese character string 'xy ' , the mutual information between characters</definiens>
				<definiens id="1">the co-occurrence probability of x and y , and p ( x ) , p ( y ) are the independent probabilities of x and y respectively</definiens>
			</definition>
			<definition id="1">
				<sentence>the t-score of the character y relevant to characters x and z is defined as : p ( zl y ) p ( y\ [ x ) tSx '' ( Y ) = ~/var ( p ( zly ) ) + var ( p ( ylx ) ) where p ( ylx ) is the conditional probability of y given x , and p ( zly ) , of z given y , and var ( p ( ylx ) ) , var ( p ( zly ) ) are variances of p ( ylx ) and of p ( zly ) respectively .</sentence>
				<definiendum id="0">p ( ylx )</definiendum>
				<definiendum id="1">var</definiendum>
				<definiens id="0">the conditional probability of y given x</definiens>
			</definition>
			<definition id="2">
				<sentence>z ( y ) &gt; 0 then y tends to be bound with z rather than with x if p ( ylx ) &gt; p ( zly ) , or tsx , ( y ) &lt; 0 then y tends to be bound with x rather than with z A distinct feature of ts is that it is contextdependent ( a relative measure ) , along with certain degree of flexibility to the context , whereas mi is context-independent ( an absolute measure ) .</sentence>
				<definiendum id="0">context-independent</definiendum>
				<definiens id="0">contextdependent ( a relative measure</definiens>
			</definition>
			<definition id="3">
				<sentence>And , the depth of the local minimum dts ( x : y ) is defined as '' d ( dts ( x : y ) ) = min { dts ( v : x ) -dts ( x.y ) , dts ( y : w ) -dts ( x : y ) } Two basic hypotheses can be easily made as the consequence of context-dependability of dts ( note : mi has not such property ) : Hypothesis 1 x and y tends to be bound ifdts ( x : y ) is a local maximum , regardless of the value of dts ( x : y ) ( even it is low ) .</sentence>
				<definiendum id="0">depth of the local minimum dts ( x : y )</definiendum>
				<definiendum id="1">d ( dts (</definiendum>
				<definiens id="0">x : y ) ) = min { dts ( v : x ) -dts ( x.y ) , dts ( y : w ) -dts ( x : y ) } Two basic hypotheses can be easily made as the consequence of context-dependability of dts ( note : mi has not such property ) : Hypothesis 1 x and y tends to be bound ifdts</definiens>
			</definition>
			<definition id="4">
				<sentence>ifdts ( x : y ) is local minimum then if d ( dts ( x : y ) ) &gt; ~l then mark ( x : y ) 'separated ' else '9 ' if ( dts ( x.y ) is local maximum ) and ( h ( dts ( x : y ) ) &gt; 6 3 ) then mark ( x : y ) 'bound ' else '9 ' if dts ( x.'y ) is local minimum then mark ( x : y ) 'separated ' else ' ? '</sentence>
				<definiendum id="0">ifdts</definiendum>
				<definiendum id="1">( h ( dts (</definiendum>
				<definiens id="0">local maximum ) and</definiens>
			</definition>
</paper>

		<paper id="2223">
			<definition id="0">
				<sentence>This is a natural extension of the example-based MT in the sense that we incorporate not only sentential correspondences ( bilingual corpora ) but every other level of linguistic ( lexical , phrasal , and collocational ) expressions into the transfer knowledge .</sentence>
				<definiendum id="0">collocational</definiendum>
				<definiens id="0">a natural extension of the example-based MT in the sense that we incorporate not only sentential correspondences ( bilingual corpora</definiens>
			</definition>
			<definition id="1">
				<sentence>( pl ) take : VERB : l a look at NP:2 =~ VP : I VP : I ¢= NP:2 wo ( dobj ) miru ( see ) : VERB : l ( p2 ) NP : I VP:2 =v S:2 S:2 ¢= NP : I ha VP:2 ( p3 ) PRON : I =~ NP : I NP : I ¢ : PRON : I The ( pl ) is a translation pattern of an English colloquial phrase `` take a look at , '' and ( p2 ) and ( p3 ) are general syntactic translation patterns .</sentence>
				<definiendum id="0">PRON</definiendum>
				<definiens id="0">a translation pattern of an English colloquial phrase `` take a look at , '' and ( p2 ) and ( p3 ) are general syntactic translation patterns</definiens>
			</definition>
			<definition id="2">
				<sentence>The source part of ( p3 ) matches `` She '' and `` him , '' the source part of ( pl ) matches a segment consisting `` take a look at '' and a NP ( `` him '' ) made from ( p3 ) , and finally the source part of ( p2 ) matches a whole sentence .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">the source part of ( p2 ) matches a whole sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>Num of Changed Sentences ( Worse/Better ) 1/2 5/4 4/6 Table 1 : Result of peformance experiment of pruning techniques ( 1-2 ) TB is a fuzzy-match term , the semantic distance of LexB and LexA is smaller than a criterion , and PosB is the same as ROSA .</sentence>
				<definiendum id="0">TB</definiendum>
				<definiendum id="1">PosB</definiendum>
				<definiens id="0">a fuzzy-match term , the semantic distance of LexB</definiens>
			</definition>
			<definition id="4">
				<sentence>( 2-2 ) LexB is a fuzzy-match term , the semantic distance of LexB and LexA is smaller than a criterion .</sentence>
				<definiendum id="0">LexB</definiendum>
				<definiens id="0">a fuzzy-match term , the semantic distance of LexB and LexA is smaller than a criterion</definiens>
			</definition>
</paper>

		<paper id="2178">
			<definition id="0">
				<sentence>This agent , named JaBot after the fact that it is a bot which has been programmed in Java , has been designed and developed by the authors in an attempt to solve common Web site problems related to information retrieval .</sentence>
				<definiendum id="0">JaBot</definiendum>
				<definiens id="0">after the fact that it is a bot which has been programmed in Java , has been designed and developed by the authors in an attempt to solve common Web site problems related to information retrieval</definiens>
			</definition>
			<definition id="1">
				<sentence>Its name is JaBot , which comes from 'Java-Based Bot ' : the word 'bot ' in turn comes from 'robot ' , both of which are alternative words for 'intelligent agent ' ; and Java is the programming language in which the bot was written .</sentence>
				<definiendum id="0">JaBot</definiendum>
				<definiendum id="1">Java</definiendum>
				<definiens id="0">the programming language in which the bot was written</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , if JaBot were placed on the Web site of a university department , users would be enquiring about subject contents , tutorial hours , exam dates , etc. , and not attempting to ask which of subjects X and Y is easier or more relevant for their careers .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">easier or more relevant for their careers</definiens>
			</definition>
</paper>

		<paper id="1115">
			<definition id="0">
				<sentence>The Penn Treebank ( PTB ) ( Marcus et al. , 1994 ) has been used for a rather simple approach to deriving large grammars automatically : one where the grammar rules are simply 'read off ' the parse trees in the corpus , with each local subtree providing the left and right hand sides of a rule .</sentence>
				<definiendum id="0">Penn Treebank</definiendum>
				<definiens id="0">subtree providing the left and right hand sides of a rule</definiens>
			</definition>
			<definition id="1">
				<sentence>NP ( 5 ) DT NN CC DT NN The idea of partiality of structure in treebanks and their grammars suggests a route by which treebank grammars may be reduced in size , or compacted as we shall call it , by the elimination of partial-structure rules .</sentence>
				<definiendum id="0">grammars</definiendum>
				<definiens id="0">suggests a route by which treebank grammars may be reduced in size , or compacted as we shall call it , by the elimination of partial-structure rules</definiens>
			</definition>
</paper>

		<paper id="2124">
			<definition id="0">
				<sentence>MDL ( Rissanen , 1989 ) is a criterion for data compression and statistical estimation proposed in information theory .</sentence>
				<definiendum id="0">MDL</definiendum>
				<definiens id="0">a criterion for data compression and statistical estimation proposed in information theory</definiens>
			</definition>
			<definition id="1">
				<sentence>We then calculate the model description length as k L ( M ) = ~ log m , where k denotes the number of free parameters in the model , and m the entire data size3 In this paper , we ignore the code length for encoding a 'discrete model , ' assuming implicitly that they are equal for all models and consider only the description length for encoding the parameters of a model as the model description length .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">the number of free parameters in the model</definiens>
			</definition>
			<definition id="2">
				<sentence>Discard those class pairs whose mutual information reduction ( 2 ) is not less than the threshold of ( k B -ka ) • logm where m denotes the total data size , ks the number of free parameters in the model before the merge , and \ ] ¢ A the number of free parameters in the model after the merge .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">the total data size , ks the number of free parameters in the model before the merge , and \ ] ¢ A the number of free parameters in the model after the merge</definiens>
			</definition>
			<definition id="3">
				<sentence>Similarly F~ is a set of not necessarily disjoint verb classes .</sentence>
				<definiendum id="0">F~</definiendum>
				<definiens id="0">a set of not necessarily disjoint verb classes</definiens>
			</definition>
</paper>

		<paper id="2250">
			<definition id="0">
				<sentence>The tree-based learning examination methods are recursive procedures that process each tree node , performing an in-order ( or depth-first ) tree traversal .</sentence>
				<definiendum id="0">tree-based learning examination methods</definiendum>
				<definiens id="0">recursive procedures that process each tree node , performing an in-order ( or depth-first ) tree traversal</definiens>
			</definition>
			<definition id="1">
				<sentence>M ( I~CCi ) TC ( I ) ) where M is the vector size , that is , the number of possible successors ( e.g. 27 ) ( see Fig .</sentence>
				<definiendum id="0">M</definiendum>
				<definiendum id="1">M</definiendum>
			</definition>
</paper>

		<paper id="2197">
			<definition id="0">
				<sentence>An anode has a label , a list of lares which it reflects , and lists of Incoming.a-ares and outgolng.a-arcs Finally , an a-arc has a pointer to its lnode , origin , extremity , and label .</sentence>
				<definiendum id="0">a-arc</definiendum>
				<definiens id="0">has a pointer to its lnode , origin , extremity , and label</definiens>
			</definition>
			<definition id="1">
				<sentence>lnode is the initial lattice node , do nothing and exit .</sentence>
				<definiendum id="0">lnode</definiendum>
				<definiens id="0">the initial lattice node , do nothing and exit</definiens>
			</definition>
			<definition id="2">
				<sentence>anodes ( the anodes , already merged so far as possible , whose reflects .</sentence>
				<definiendum id="0">anodes</definiendum>
			</definition>
			<definition id="3">
				<sentence>Conclusion and Plans The algorithm for transforming lattices into non-deterministic finite state automata which we have presented here has been successfully applied to lattices derived from dictionaries , i.e. very large corpora of strings ( MeddebHamrouni ( 1996 ) , pages 205-217 ) .</sentence>
				<definiendum id="0">Conclusion</definiendum>
				<definiens id="0">successfully applied to lattices derived from dictionaries , i.e. very large corpora of strings</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>Machine translation is a good example : the ambiguity of a sentence in the source language needs very often to be preserved and translated into the target one ( cf. ( Wedekind97 ) ) .</sentence>
				<definiendum id="0">Machine translation</definiendum>
				<definiens id="0">a good example : the ambiguity of a sentence in the source language needs very often to be preserved</definiens>
			</definition>
			<definition id="1">
				<sentence>, A &lt; lsl I~A. , ~o..ll .. // CAT adj ~ '' -ho~ L .l| L , , , , , . , &lt; , &lt; o , &lt; is , , , &lt; Figure 1 : Control relatio~-'within a lexical entry lations at the phrase-structure level. We propose for that a particular representation of hierarchical relations for ambiguous objects called unary quasi-trees. This paper is threefold. In a first section , we present the limits of the classical representation of ambiguity and in particular the technique of named disjunctions. The second section describes the controlled disjunction method applied to the lexical level. We describe in the third section the generalization of this technique to the phrase-structure level using unary quasi-trees and we show how this approach is useful for an online control of the ambiguity during the parse. Several techniques have been proposed for the interpretation and the control of disjunctive structures. For example , delaying the evaluation of the disjunctive formulae until obtaining enough information allows partial disambiguation ( cf. ( Karttunen84 ) ) . Another solution consists in converting the disjunctive formulae into a conjunctive form ( using negation ) as proposed by ( Nakazawa88 ) or ( Maxwell91 ) . We can also make use of the properties of the formula in order to eliminate inconsistencies. This approach , described in ( Maxwell91 ) , relies on the conversion of the original disjunctive formulae into a set of contexted constraints which allows , by the introduction of propositional variables ( i ) to convert the formulae into a conjunctive form , and ( ii ) to isolate a subset of formulae , the disjunctive residue ( the negation of the unsatisfiable constraints ) . The problem of the satisfiability of the initial formula is then reduced to that of the disjunctive residue. This approach is fruitful and several methods rely on this idea to refer formulae with an index ( a propositional variable , an integer , etc. ) . It is the case in particular with named disjunctions ( see ( DSrre90 ) , ( Krieger93 ) or ( Gerdemann95 ) ) which propose a compact representation of control phenomena and covariancy. A named disjunction ( noted hereafter ND ) binds several disjunctive formulae with an index ( the name of the disjunction ) . These formulae have the same arity and their disjuncts are ordered. They are linked by a covariancy relation : when one disjunct in a ND is selected ( i.e. interpreted to true ) , then all the disjuncts occurring at the same position into the other formulae of the ND also have to be true. The example ( 1 ) presents the lexical entry of the german determiner den. The covariation is indicated by three disjunctive formulae composing the named disjunction indexed by 1. ( i ) den : P= f ill L '' O'x v , , , &lt; jj But the named disjunction technique also has some limits. In particular , NDs have to represent all the relations between formulae in a covariant way. This leads to a lot of redundancy and a loss of the compactness in the sense that the disjuncts do n't contain anymore the possible values but all the possible variancies according to the other formulae. 125 Some techniques has been proposed in order to eliminate this drawback and in particular : the dependency group representation ( see ( Griffith96 ) ) and the controlled disjunctions ( see ( Blache97 ) ) . The former relies on an enrichment of the Maxwell and Kaplan 's contexted constraints. In this approach , constraints are composed of the conjunction of base constraints ( corresponding to the initial disjunctive form ) plus a control formula representing the way in which values are choosen. The second approach , described in the next section , consists in a specific representation of control relations relying on a clear distinction between ( i ) the possible values ( the disjuncts ) and ( ii ) the relations between these ambiguous values and other elements of the structure. This approach allows a direct implementation of the implication relations ( i.e. the oriented controls ) instead of simple covariancies. The controlled disjunctions ( noted hereafter CD ) implement the relations existing between ambiguous feature values. The example of the figure ( 1 ) describes a non covariant relation between GENDER and HEAD features. More precisely , this relation is oriented : if the object is a noun , then the gender is masculine and if the object is feminine , then it is an adjective. The relation between these values can be represented as implications : noun = &gt; masc and fem : =~ adj .</sentence>
				<definiendum id="0">disjunctive residue</definiendum>
				<definiens id="0">useful for an online control of the ambiguity during the parse. Several techniques have been proposed for the interpretation and the control of disjunctive structures. For example , delaying the evaluation of the disjunctive formulae until obtaining enough information allows partial disambiguation ( cf. ( Karttunen84 ) )</definiens>
				<definiens id="1">relies on the conversion of the original disjunctive formulae into a set of contexted constraints which allows , by the introduction of propositional variables ( i ) to convert the formulae into a conjunctive form</definiens>
				<definiens id="2">corresponding to the initial disjunctive form ) plus a control formula representing the way in which values</definiens>
			</definition>
			<definition id="2">
				<sentence>This example indicates that : • an adjective is a daughter of an AP which is to its turn a daughter of a NP , • a noun is a daughter of a NP which is to its turn a daughter of an unspecified phrase XP .</sentence>
				<definiendum id="0">adjective</definiendum>
				<definiendum id="1">AP</definiendum>
			</definition>
			<definition id="3">
				<sentence>As indicated before , each node represents a disjunctive formula and the set of nodes constitutes a covariant disjunction .</sentence>
				<definiendum id="0">node</definiendum>
				<definiens id="0">a disjunctive formula and the set of nodes constitutes a covariant disjunction</definiens>
			</definition>
</paper>

		<paper id="1027">
</paper>

		<paper id="1109">
			<definition id="0">
				<sentence>( 1995 ) , in which linear order constraints are taken to apply to domains distinct from the local trees formed by syntactic combination , the nonconcatenative shuffle relation is the basic operation by which these word order domains are formed .</sentence>
				<definiendum id="0">nonconcatenative shuffle relation</definiendum>
				<definiens id="0">the basic operation by which these word order domains are formed</definiens>
			</definition>
			<definition id="1">
				<sentence>The linear order of constituents in a clause is represented by an order domain ( DOM ) , which is a list of domain objects , whose relative order must satisfy a set of linear precedence ( LP ) constraints .</sentence>
				<definiendum id="0">DOM</definiendum>
				<definiens id="0">a list of domain objects , whose relative order must satisfy a set of linear precedence ( LP ) constraints</definiens>
			</definition>
			<definition id="2">
				<sentence>2 • The relation head-corner is defined as the reflexive and transitive closure of the head relation .</sentence>
				<definiendum id="0">relation head-corner</definiendum>
				<definiens id="0">the reflexive and transitive closure of the head relation</definiens>
			</definition>
			<definition id="3">
				<sentence>ConTroll is a constraint logic programming system for typed feature structures , which supports a direct implementation of HPSG .</sentence>
				<definiendum id="0">ConTroll</definiendum>
				<definiens id="0">a constraint logic programming system for typed feature structures</definiens>
				<definiens id="1">supports a direct implementation of HPSG</definiens>
			</definition>
			<definition id="4">
				<sentence>For each version of the parser , time , choice points , and calls are reported , as follows : The time measurement ( Time ) 5 is the amount of CPU seconds ( on a Sun SPARCstation 5 ) required to search for all possible parses , choice points ( ChoicePts ) records the number of instances where more than one disjunct may apply at the time when a constraint is resolved , and calls ( Calls ) lists the number of times a constraint is unfolded .</sentence>
				<definiendum id="0">time measurement</definiendum>
				<definiendum id="1">ChoicePts )</definiendum>
				<definiendum id="2">calls</definiendum>
				<definiens id="0">records the number of instances where more than one disjunct may apply at the time when a constraint is resolved</definiens>
			</definition>
</paper>

		<paper id="2149">
			<definition id="0">
				<sentence>Informally , a distinguishing description is a def'mite description which designates one and only one entity among others in a context set .</sentence>
				<definiendum id="0">distinguishing description</definiendum>
				<definiens id="0">a def'mite description which designates one and only one entity among others in a context set</definiens>
			</definition>
			<definition id="1">
				<sentence>In a guided composition point of view , the problem is then to find how to know as early as possible if a string of words ( an incomplete distinguishing description or Ida ) may or may not lead to a correct distinguishing description .</sentence>
				<definiendum id="0">Ida</definiendum>
				<definiens id="0">an incomplete distinguishing description or</definiens>
			</definition>
			<definition id="2">
				<sentence>Let us have a look at an example : Suppose we have a set of entities E ( el , e2 , e3 , e4 , e5 , e6 } , the properties of which are : e1 is a dog , e2 is a dog that barks , e3 is a dog that barks , e4 is a red bird that sings , e5 is a red bird , e6is a bird that flies .</sentence>
				<definiendum id="0">e2</definiendum>
				<definiendum id="1">e3</definiendum>
				<definiendum id="2">e4</definiendum>
				<definiendum id="3">e5</definiendum>
				<definiens id="0">a dog</definiens>
				<definiens id="1">a dog that barks</definiens>
				<definiens id="2">a dog that barks</definiens>
				<definiens id="3">a red bird that sings</definiens>
			</definition>
			<definition id="3">
				<sentence>inclusion Idd as defined in 2.1 do not depend on how they are produced .</sentence>
				<definiendum id="0">inclusion Idd</definiendum>
				<definiens id="0">defined in 2.1 do not depend on how they are produced</definiens>
			</definition>
			<definition id="4">
				<sentence>Wn is a singleton ( and if the Idd is an NP syntactically correct ) then the ldd can be considered as a dd , and its referent is the element of the set .</sentence>
				<definiendum id="0">Wn</definiendum>
				<definiens id="0">a singleton</definiens>
				<definiens id="1">the element of the set</definiens>
			</definition>
			<definition id="5">
				<sentence>Wn is a singleton or if a subset of it according to simple inclusion is a singleton .</sentence>
				<definiendum id="0">Wn</definiendum>
				<definiens id="0">a singleton or if a subset of it according to simple inclusion is a singleton</definiens>
			</definition>
			<definition id="6">
				<sentence>EREL is a software for language rehabilitation that we have designed in a collaboration with medical staffs specialised in the treatment of autistic-like children .</sentence>
				<definiendum id="0">EREL</definiendum>
				<definiens id="0">a software for language rehabilitation that we have designed in a collaboration with medical staffs specialised in the treatment of autistic-like children</definiens>
			</definition>
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>SUG is a logical formalism based on unification , which is an extension of Definite Clause Grammars ( DCG ) .</sentence>
				<definiendum id="0">SUG</definiendum>
			</definition>
			<definition id="1">
				<sentence>P is a finite set of pairs ++ &gt; 13 where ot~NT , 13~ ( TuNT ) *u { procedures calls } , and these pairs are called production rules .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a finite set of pairs ++ &gt; 13 where ot~NT , 13~ ( TuNT ) *u { procedures calls } , and these pairs are called production rules</definiens>
			</definition>
			<definition id="2">
				<sentence>Finally H is a set of production rules which only has the first member of the production rule , i.e. a , and ot 's name is either coordinated , juxtaposition , fusion , basicWord or isWord .</sentence>
				<definiendum id="0">H</definiendum>
				<definiens id="0">either coordinated , juxtaposition , fusion , basicWord or isWord</definiens>
			</definition>
			<definition id="3">
				<sentence>Each SS consists of a structure with functor the name of the constituent ( np , vp ... . ) .</sentence>
				<definiendum id="0">SS</definiendum>
				<definiens id="0">consists of a structure with functor the name of the constituent ( np , vp ... . )</definiens>
			</definition>
			<definition id="4">
				<sentence>This simple interface between the tagger and SUG is one of the advantages of the modularity that presents SUG .</sentence>
				<definiendum id="0">SUG</definiendum>
				<definiens id="0">one of the advantages of the modularity that presents SUG</definiens>
			</definition>
			<definition id="5">
				<sentence>remainingSentence ( PP , NP , P , V , C ) ++ &gt; &lt; t # # ( { ( var ( PP ) , var ( NP ) , vat ( P ) , var ( V ) , var ( C ) ) } , IVV\ ] ) , ( _ _ ) ~/f &gt; , sentence .</sentence>
				<definiendum id="0">remainingSentence</definiendum>
				<definiens id="0">var ( V ) , var ( C ) ) } , IVV\ ] ) , ( _ _ ) ~/f &gt; , sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>The SS returned by the parser will consist of a sequence of these constituents : pp , np , p , conj , verb and free words .</sentence>
				<definiendum id="0">SS</definiendum>
				<definiens id="0">pp , np , p , conj , verb and free words</definiens>
			</definition>
			<definition id="7">
				<sentence>This algorithm will take a slot structure ( SS ) that consists of a sequence of the following constituents : np , pp , p , conj and verbs and it will return a new one without anaphors .</sentence>
				<definiendum id="0">SS</definiendum>
				<definiens id="0">np , pp , p , conj and verbs and it will return a new one without anaphors</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>PATRICIA is a special kind of trie\ [ Fredkin 60\ ] .</sentence>
				<definiendum id="0">PATRICIA</definiendum>
			</definition>
			<definition id="1">
				<sentence>Node : a record of Decision hit : an integer to denote the decision bit .</sentence>
				<definiendum id="0">Node</definiendum>
				<definiens id="0">a record of Decision hit : an integer to denote the decision bit</definiens>
			</definition>
			<definition id="2">
				<sentence>Frequency : the frequency count of the prefix substring .</sentence>
				<definiendum id="0">Frequency</definiendum>
			</definition>
			<definition id="3">
				<sentence>The frequency count of an element is a simple criterion for measuring the importance of an 247 element .</sentence>
				<definiendum id="0">frequency count of an element</definiendum>
				<definiens id="0">a simple criterion for measuring the importance of an 247 element</definiens>
			</definition>
			<definition id="4">
				<sentence>The insertion process takes a free node for each element from the array in the increasing order of their indexes until the array is exhausted .</sentence>
				<definiendum id="0">insertion process</definiendum>
				<definiens id="0">takes a free node for each element from the array in the increasing order of their indexes until the array is exhausted</definiens>
			</definition>
</paper>

		<paper id="1073">
			<definition id="0">
				<sentence>Exemple 2 : lsdecomp ( w , p , i , s , root : CH ) r : B pre True post 3a , bE CH ( w=p.a.i.b.sA root = a * b ) A ( Sprefixe ( w , p ) A Ssuf fix ( w , s ) /X Sin fixe ( w , i ) ) oh B : le type bool~en Sinfixe 0 ( respectivement Sprefixe 0 et Ssu\ ] fixeO ) : un pr~dicat qui v~rifie la propri~t~ d'un infixe ( respectivementd'un pr~fixe et d'un suffixe ) pour une chaine .</sentence>
				<definiendum id="0">Ssuf fix</definiendum>
				<definiens id="0">s , root : CH ) r : B pre True post 3a</definiens>
			</definition>
</paper>

		<paper id="2131">
			<definition id="0">
				<sentence>The Pragmatic Adaptation module serves as the boundary between language and action by determining what action to take given an interpreted input utterance or a back-end response .</sentence>
				<definiendum id="0">Pragmatic Adaptation module</definiendum>
				<definiens id="0">the boundary between language and action by determining what action to take given an interpreted input utterance or a back-end response</definiens>
			</definition>
			<definition id="1">
				<sentence>The Pragmatic Adapter receives an interpretation of an input utterance with contextdependent forms resolved .</sentence>
				<definiendum id="0">Pragmatic Adapter</definiendum>
				<definiens id="0">receives an interpretation of an input utterance with contextdependent forms resolved</definiens>
			</definition>
			<definition id="2">
				<sentence>The Pragmatic Adapter combines the result of these simple tests and a set of if-then heuristics to determine whether to send through the command or to intercept the utterance and notify the Dialogue Manager to initiate a repair dialogue with the user .</sentence>
				<definiendum id="0">Pragmatic Adapter</definiendum>
				<definiens id="0">combines the result of these simple tests and a set of if-then heuristics to determine whether to send through the command or to intercept the utterance and notify the Dialogue Manager to initiate a repair dialogue with the user</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , a Speech Recognition ( SR ) error , may be detected after Natural Language Interpretation fails to parse the output of SR .</sentence>
				<definiendum id="0">Speech Recognition</definiendum>
				<definiens id="0">detected after Natural Language Interpretation fails to parse the output of SR</definiens>
			</definition>
			<definition id="4">
				<sentence>`` Data '' on Star Trek converses as a human while providing information processing of a computer and is capable of action in the physical world .</sentence>
				<definiendum id="0">Data</definiendum>
				<definiens id="0">a human while providing information processing of a computer and is capable of action in the physical world</definiens>
			</definition>
			<definition id="5">
				<sentence>A Moderator , which , in multi-party dialogues , enforces an agreed-upon interaction protocol , such as Robert 's Rules of Order or a talk-show format ( under control of the host ) .</sentence>
				<definiendum id="0">Moderator</definiendum>
				<definiens id="0">enforces an agreed-upon interaction protocol , such as Robert 's Rules of Order or a talk-show format ( under control of the host )</definiens>
			</definition>
			<definition id="6">
				<sentence>We then ported the system to a spoken interface to a battlefield simulation ( Modular Semi-Automated Forces , or ModSAF ) .</sentence>
				<definiendum id="0">ModSAF</definiendum>
				<definiens id="0">Modular Semi-Automated Forces , or</definiens>
			</definition>
			<definition id="7">
				<sentence>The Dialogue Manager receives this and passes it to the Context Tracker .</sentence>
				<definiendum id="0">Dialogue Manager</definiendum>
				<definiens id="0">receives this and passes it to the Context Tracker</definiens>
			</definition>
</paper>

		<paper id="2164">
			<definition id="0">
				<sentence>Part of Speech ( POS ) Tagging is a quite well defined NLP problem , which consists of assigning to each word in a text the proper morphosyntactic tag for the given context .</sentence>
				<definiendum id="0">Speech ( POS ) Tagging</definiendum>
				<definiens id="0">consists of assigning to each word in a text the proper morphosyntactic tag for the given context</definiens>
			</definition>
			<definition id="1">
				<sentence>Successful taggers have been built using several approaches , such as statistical techniques , symbolic machine learning techniques , neural networks , etc .</sentence>
				<definiendum id="0">Successful taggers</definiendum>
			</definition>
			<definition id="2">
				<sentence>K : Observed performance of the tagger , computed on the noisy test corpus .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">Observed performance of the tagger , computed on the noisy test corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>The tagger 2 '' 2 uses trigrams and automatically acquired context constraints and has an accuracy of K2 = evaluated on a corpus ( wsJ ) with an estimated error rate 5 C1 = C2 = 0.03 .</sentence>
				<definiendum id="0">wsJ</definiendum>
				<definiens id="0">has an accuracy of K2 = evaluated on a corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>5The ( wsJ ) corpus error rate is estimated over all words .</sentence>
				<definiendum id="0">wsJ</definiendum>
			</definition>
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>- , \ [ \ ] ~On } \ ] , adjn ( i , Q ) , ... , adjn ( i , in ) U T1 U ... U Tn &gt; withtrans &lt; \ [ \ ] ~oj , Tj &gt; ; 1 &lt; j &lt; n zrans is bidirectional , i.e. we are able to translate between f-structures and terms for using terms as transfer input , process terms in the transfer , and convert the transfer output back to f-structures which are the appropriate generator representations. 367 Transfer works on source language ( SL ) and target language ( TL ) sets of terms representing predicates , roles , etc. like the ones shown in ( 5 ) and ( 6 ) . The mapping is encoded in transfer rules as in ( 7 ) . For a rule to be applied , the set on the SL side must be a matching subset of the SL input set. If this is the case , we remove the covering set from the input and add the set on the other side of the rule to the TL output. Transfer is complete , if the SL set is empty. ( 7 ) a. treffen ( E ) &lt; - &gt; meet ( E ) .</sentence>
				<definiendum id="0">etc.</definiendum>
				<definiendum id="1">mapping</definiendum>
				<definiens id="0">able to translate between f-structures and terms for using terms as transfer input , process terms in the transfer , and convert the transfer output back to f-structures which are the appropriate generator representations. 367 Transfer works on source language ( SL ) and target language ( TL ) sets of terms representing predicates , roles ,</definiens>
			</definition>
</paper>

		<paper id="1119">
			<definition id="0">
				<sentence>Language modeling is to associate a sequence of words with a priori probability , which is a key part of many natural language applications such as speech recognition and statistical machine translation .</sentence>
				<definiendum id="0">Language modeling</definiendum>
			</definition>
			<definition id="1">
				<sentence>The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper .</sentence>
				<definiendum id="0">grammar</definiendum>
				<definiens id="0">consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper</definiens>
			</definition>
			<definition id="2">
				<sentence>N-gram model estimates the probability of a sentence as the product of the probability of each word in the sentence .</sentence>
				<definiendum id="0">N-gram model</definiendum>
				<definiens id="0">estimates the probability of a sentence as the product of the probability of each word in the sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper .</sentence>
				<definiendum id="0">grammar</definiendum>
				<definiens id="0">consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper</definiens>
			</definition>
			<definition id="4">
				<sentence>bird in the cage the bus book Figure 3 : Example complete-sequences A complete-sequence is a sequence of 0 or more adjacent complete-links that have the same direction .</sentence>
				<definiendum id="0">complete-sequence</definiendum>
				<definiens id="0">a sequence of 0 or more adjacent complete-links that have the same direction</definiens>
			</definition>
			<definition id="5">
				<sentence>• L~ ( i , j ) : { ( wi -- + wj ) , S~ ( i , m ) , St ( m+l , j ) } • Ll ( i , j ) : { ( wi e-wj ) , St ( i , m ) , St ( m+l , j ) } foram ( i &lt; m &lt; j ) . Otherwise , the set of dependencies does not satisfy the conditions of no crossing , no cycle and no multiple heads and is not a complete-link any more. Similarly , any complete-sequence on wi , j can be viewed as the following combination. • S~ ( i , j ) : { Sr ( i , m ) , L~ ( m , j ) } • St ( i , j ) : { Lt ( i , m ) , St ( m , j ) } foram ( i &lt; m &lt; j ) . In the case of complete-sequence , we can prevent multiple constructions of the same 724 complete-sequence by the above combinational restriction. Figure 4 : Abstract representation of/ ) Figure 4 shows an abstract representation of a/ ) of an n-word sentence. When wk ( 1 &lt; k &lt; _ n ) is the head of the sentence , any D of the sentence can be represented by a St ( l , EOS ) uniquely by the assumption that there is always the dependency relation , ( wk +-wEos ) . The reestimation algorithm is a variation of Inside-Outside algorithm ( Jelinek et al. , 1990 ) adapted to dependency grammar. In this section we first define the inside-outside probabilities of complete-links and complete-sequences , and then describe the reestimation algorithm based on them 2. In the followings , ~ indicates inside probability and a , is for outside probability. The superscripts , l and s , are used for `` complete-link '' and `` complete-sequence '' respectively. The subscripts indicate direction : r for `` rightward '' and I for `` leftward '' . The inside probabilities of complete-links ( n~ ( i , j ) , Lt ( i , j ) ) and complete-sequences ( Sr ( i , j ) , Sl ( i , j ) ) are as follows. j-1 /3t~ ( i , j ) = ~ p ( wi -- + wj ) /3~ ( i , m ) t3~ ( m + 1 , j ) . rn=i j -- I /3\ [ ( i , j ) = E p ( wi 6.wj ) t3~ ( i , m ) 13 ? ( m + 1 , j ) . rn=i j -- 1 fl~ ( i , j ) = ~ /3~ ( i , m ) ~t~ ( m , j ) . mini J /3 ? ( i , j ) = ~ /3\ [ ( i , m ) t3 ? ( m , j ) . m=i+l The basis probabilities are : /31r ( i , i + 1 ) = p ( wi `` ~ wi+l ) /3\ [ ( i , i + 1 ) = p ( wi ( - '' wi+l ) /3~ ( i , i ) = fl ? ( i , i ) = 1 /37 ( 1 , EO S ) = p ( wL , ) ~A little more detailed explanation of the expressions can be found in ( Lee and Choi , 1997 ) . /3~ ( i , i+ 1 ) = p ( L~ ( i , i+ 1 ) ) = p ( wi ~ wi+t ) /37 ( i , i + 1 ) = p ( Lt ( i , i + 1 ) ) = p ( wi +-wi+t ) . /37 ( 1 , EOS ) is the sentence probability because every dependency analysis , D , is represented by a St ( l , EOS ) and/37 ( 1 , EOS ) is sum of the probability of every St ( l , EOS ) . probabilities for complete ( i , j ) ) and complete-sequences are as follows. The outside links ( L , . ( i , j ) , Lt ( S~ ( i , j ) , St ( i , j ) ) i at~ ( i , j ) = n c~ ( v , j ) /3i~ ( v , i ) . a~ ( i , h ) /3 ? ( j , h ) . h=j a~ ( i , j ) = ~ a~ ( i , h ) /3tr ( j , h ) h=j+l +atr ( i , h ) /3i~ ( j + 1 , h ) p ( wi -+ Wh ) +al ( i , h ) /3 ? ( j + 1 , h ) p ( wi ~ wh ) . i-I a~ ( i , j ) = ~ a~ ( v , j ) fl~ ( v , i ) v -- -- I +dr ( v , j ) Z ; ( v , i t ) p ( wv wA +al ( v , j ) t3 ; ( v , i1 ) p ( wv ewj ) . The basis probability is ~ ( 1 , EOS ) = 1. Given a training corpus , the initial grammar is just a list of all pairs of unique words in the corpus. The initial pairs represent the tentative head-dependent relations of the words. And the initial probabilities of the pairs can be given randomly. The training starts with the initial grammar. The train corpus is analyzed with the grammar and the occurrence frequency of each dependency relation is calculated. Based on the frequencies , probabilities of dependency relations are recalculated by C ( wp -- + w~ ) The process w , ) = C ( w continues until the entropy of the training corpus becomes the minimum. The frequency of occurrence , C ( wi -- + wj ) , is calculated by w ) = -+ 1 t • • t = p ( wt , . ) a. ( , ,3 ) /3~ ( i , j ) where O~ ( wi ~ wj , D , wl , n ) is 1 if the dependency relation , ( wi -- + wj ) , is used in the D , 725 and 0 otherwise. Similarly , the occurrence frequency of the dependency relation , ( wi +wj ) , is computed by ~ -- -- L -- -o~l ( i , j ) ~\ [ ( i , j ) . We have experimented with three language models , tri-gram model ( TRI ) , bi-gram model ( BI ) , and the proposed model ( DEP ) on a raw corpus extracted from KAIST corpus 3. The raw corpus consists of 1,589 sentences with 13,139 words , describing animal life in nature. We randomly divided the corpus into two parts : a training set of 1,445 sentences and a test set of 144 sentences. And we made 15 partial training sets which include the first s sentences in the whole training set , for s ranging from 100 to 1,445 sentences. We trained the three language models for each partial training set , and tested the training and the test corpus entropies. TRI and BI was trained by counting the occurrence of tri-grams and bi-grams respectively. DEP was trained by running the reestimation algorithm iteratively until it converges to an optimal dependency grammar. On the average , 26 iterations were done for the training sets. Smoothing is needed for language modeling due to the sparse data problem. It is to compensate for the overestimated and the underestimated probabilities. Smoothing method itself is an important factor. But our goal is not to find out a better smoothing method. So we fixed on an interpolation method and applied it for the three models. It can be represented as ( McCandless , 1994 ) ... , w , -x ) = , \P , ( wilw , - , +l , ... , wi_l ) + ( 1 ... , where = C ( wl , ... , w , -1 ) C ( w , , ... , + K , '' The Ks is the global smoothing factor. The bigger the Ks , the larger the degree of smoothing. For the experiments we used 2 for Ks. We take the performance of a language model to be its cross-entropy on test corpus , 1 s IVl E-l°g2Pm ( Si ) i=1 3KAIST ( Korean Advanced Institute of Science and Technology ) corpus has been under construction since 1994. It consists of raw text collection ( 45,000,000 words ) , POS-tagged collection ( 6,750,000 words ) , and tree-tagged collection ( 30,000 sentences ) at present. where the test corpus contains a total of IV\ ] words and is composed of S sentences. &gt; '' 2.6 O. u~ 2.2 ~ ( DEP model ) o 2 a ( TRI model ) i 0 200 400 600 800 1000 1200 1400 600 No. of training sentences Figure 5 : Training corpus entropies Figure 5 shows the training corpus entropies of the three models .</sentence>
				<definiendum id="0">reestimation algorithm</definiendum>
				<definiendum id="1">EOS )</definiendum>
				<definiendum id="2">Lt ( S~</definiendum>
				<definiendum id="3">basis probability</definiendum>
				<definiens id="0">the set of dependencies does not satisfy the conditions of no crossing , no cycle and no multiple heads and is not a complete-link any more. Similarly , any complete-sequence on wi , j can be viewed as the following combination. • S~ ( i , j ) : { Sr ( i , m ) , L~ ( m , j ) } • St ( i , j ) : { Lt ( i , m ) , St ( m , j ) } foram ( i &lt; m &lt; j ) . In the case of complete-sequence</definiens>
				<definiens id="1">Abstract representation of/ ) Figure 4 shows an abstract representation of a/ ) of an n-word sentence. When wk ( 1 &lt; k &lt; _ n ) is the head of the sentence , any D of the sentence can be represented by a St ( l , EOS ) uniquely by the assumption that there is always the dependency relation , ( wk +-wEos</definiens>
				<definiens id="2">a variation of Inside-Outside algorithm ( Jelinek et al. , 1990 ) adapted to dependency</definiens>
				<definiens id="3">p ( wL , ) ~A little more detailed explanation of the expressions can be found in ( Lee and Choi , 1997 ) . /3~ ( i , i+ 1 ) = p ( L~ ( i , i+ 1 ) ) = p</definiens>
				<definiens id="4">the sentence probability because every dependency analysis</definiens>
				<definiens id="5">a list of all pairs of unique words in the corpus. The initial pairs represent the tentative head-dependent relations of the words. And the initial probabilities of the pairs can be given randomly. The training starts with the initial grammar. The train corpus is analyzed with the grammar and the occurrence frequency of each dependency relation is calculated. Based on the frequencies , probabilities of dependency relations are recalculated by C ( wp -- + w~ ) The process w , ) = C ( w continues until the entropy of the training corpus becomes the minimum. The frequency of occurrence , C ( wi -- + wj ) , is calculated by w ) = -+ 1 t • • t = p ( wt , . ) a. ( , ,3 ) /3~ ( i , j ) where O~ ( wi ~ wj , D , wl , n ) is 1 if the dependency relation , ( wi -- + wj ) , is used in the D , 725 and 0 otherwise. Similarly , the occurrence frequency of the dependency relation , ( wi +wj ) , is computed by ~ -- -- L -- -o~l ( i , j ) ~\ [ ( i , j ) . We have experimented with three language models , tri-gram model ( TRI ) , bi-gram model ( BI ) , and the proposed model ( DEP ) on a raw corpus extracted from KAIST corpus 3. The raw corpus consists of 1,589 sentences with 13,139 words , describing animal life in nature. We randomly divided the corpus into two parts : a training set of 1,445 sentences and a test set of 144 sentences. And we made 15 partial training sets which include the first s sentences in the whole training set , for s ranging from 100 to 1,445 sentences. We trained the three language models for each partial training set , and tested the training and the test corpus entropies. TRI and BI was trained by counting the occurrence of tri-grams and bi-grams respectively. DEP was trained by running the reestimation algorithm iteratively until it converges to an optimal dependency grammar. On the average , 26 iterations were done for the training sets. Smoothing is needed for language modeling due to the sparse data problem. It is to compensate for the overestimated and the underestimated probabilities. Smoothing method itself</definiens>
			</definition>
			<definition id="6">
				<sentence>The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus by the reestimation algorithm which is also introduced in this paper .</sentence>
				<definiendum id="0">grammar</definiendum>
				<definiens id="0">consists of head-dependent relations between words and can be learned automatically from a raw corpus by the reestimation algorithm which is also introduced in this paper</definiens>
			</definition>
</paper>

		<paper id="1007">
</paper>

		<paper id="2245">
			<definition id="0">
				<sentence>We used WordNet as a mediator between a conventional dictionary and a thesaurus .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">a mediator between a conventional dictionary and a thesaurus</definiens>
			</definition>
			<definition id="1">
				<sentence>It is generally accepted that applications such as word sense disambiguation ( WSD ) , machine translation ( MT ) and information retrieval ( IR ) , require a wide range of resources to supply the necessary lexical semantic information .</sentence>
				<definiendum id="0">information retrieval</definiendum>
				<definiendum id="1">IR</definiendum>
				<definiens id="0">generally accepted that applications such as word sense disambiguation ( WSD ) , machine translation</definiens>
			</definition>
			<definition id="2">
				<sentence>A similarity score B ( j , k ) is computed for the jth WN synset ( taking the synset itself , the hypernyms , and the coordinate terms ) and the k th ROGET class , according to the following : B ( j , k ) = bllSj N Pkl + b2IHyp ( Sj ) M Pkl + bHCo ( Sj ) n Pkl We have set bz = b2 = ba = 1 .</sentence>
				<definiendum id="0">similarity score B</definiendum>
				<definiens id="0">computed for the jth WN synset ( taking the synset itself , the hypernyms , and the coordinate terms</definiens>
			</definition>
			<definition id="3">
				<sentence>Reading the matrices row-wise shows how vaguely a certain sense is defined , whereas reading them column-wise reveals how polysemous a word is .</sentence>
				<definiendum id="0">Reading the matrices row-wise</definiendum>
				<definiens id="0">shows how vaguely a certain sense</definiens>
			</definition>
			<definition id="4">
				<sentence>Introduction to ~ , VordNet : An online lexical database .</sentence>
				<definiendum id="0">VordNet</definiendum>
			</definition>
</paper>

		<paper id="2174">
			<definition id="0">
				<sentence>Baggability is the number of source words consumed by an equivalence .</sentence>
				<definiendum id="0">Baggability</definiendum>
				<definiens id="0">the number of source words consumed by an equivalence</definiens>
			</definition>
			<definition id="1">
				<sentence>Rightmostness describes how far to the right an expression occurs in the sentence .</sentence>
				<definiendum id="0">Rightmostness</definiendum>
				<definiens id="0">describes how far to the right an expression occurs in the sentence</definiens>
			</definition>
</paper>

		<paper id="2159">
			<definition id="0">
				<sentence>AoxB ) , where A is a set of TFSs which have already been unified with an existing constituents , B is a set of TFSs which have not been unified yet , and x is the TFS which can be unified with the constituent in an inactive edge whose left-side is in position k. Inactive edges are in the form ( k , x , j ) , where kis the left-side position of the constituent x and j is the rightside position of the constituent x. That is , the set of all inactive edges whose left-side position is k are collected by T'k. In our algorithm , ~k is always waiting for either an active edge or an inactive edge , and performs the following procedure when receiving an edge .</sentence>
				<definiendum id="0">AoxB</definiendum>
				<definiendum id="1">B</definiendum>
				<definiendum id="2">x</definiendum>
				<definiens id="0">a set of TFSs which have not been unified yet</definiens>
				<definiens id="1">the TFS which can be unified with the constituent in an inactive edge whose left-side is in position k. Inactive edges are in the form</definiens>
				<definiens id="2">~k is always waiting for either an active edge or an inactive edge</definiens>
			</definition>
			<definition id="1">
				<sentence>• When Pk receives an active edge ( i , z -A o xB ) , 7- ) k preserve the edge and tries to find the unifiable constituent with x from the set of inactive edges that : Pk has already received .</sentence>
				<definiendum id="0">Pk</definiendum>
				<definiens id="0">receives an active edge</definiens>
			</definition>
			<definition id="2">
				<sentence>• When Pk receives an inactive edge ( k , x , j ) , : Pk preserves the edge and tries to find the unifiable constituent on the right side of the dot from the set of active edges that : Pk has already received .</sentence>
				<definiendum id="0">Pk</definiendum>
				<definiens id="0">receives an inactive edge ( k , x , j )</definiens>
			</definition>
</paper>

		<paper id="1118">
			<definition id="0">
				<sentence>FDBM~r ~ns as is a server and a daemon and is written in C ( ANSI ) and JAVA .</sentence>
				<definiendum id="0">FDBM~r ~ns</definiendum>
				<definiens id="0">a server and a daemon and is written in C ( ANSI ) and JAVA</definiens>
			</definition>
			<definition id="1">
				<sentence>\ ] • Features specification : Open list of features ( names and values ) associated with an element of presentation .</sentence>
				<definiendum id="0">• Features specification</definiendum>
				<definiens id="0">Open list of features ( names and values</definiens>
			</definition>
			<definition id="2">
				<sentence>getData ( $ OBJECT # type ) \ ] Concept : \ [ # HAS-TYPE ( # object $ OBJECT # type $ TYPE ) \ ] Desc : \ [ Describe the object type \ ] Figure 4 : Exemplar for Object Type PRESENTOR uses a conceptual dictionary for the mapping of conceptual domain-specific rep720 resentations to linguistic domain-indepenent representations .</sentence>
				<definiendum id="0">getData</definiendum>
				<definiens id="0">Exemplar for Object Type PRESENTOR uses a conceptual dictionary for the mapping of conceptual domain-specific rep720 resentations to linguistic domain-indepenent representations</definiens>
			</definition>
			<definition id="3">
				<sentence>Conceptual representations ( ConcSs ) used by PRESENTOR are inspired by the characteristics of the DSyntSs in the sense that both types of representations are unordered tree structures with labelled arcs specifying the roles ( conceptual or syntactic ) of each node .</sentence>
				<definiendum id="0">Conceptual representations</definiendum>
				<definiens id="0">unordered tree structures with labelled arcs specifying the roles ( conceptual or syntactic ) of each node</definiens>
			</definition>
			<definition id="4">
				<sentence>This rule combines clauses FDBMgr is a software component and FDBMgr is deployed on host Gauss into FDBMgr is a software component which is deployed on host Gauss .</sentence>
				<definiendum id="0">FDBMgr</definiendum>
				<definiens id="0">a software component and</definiens>
			</definition>
			<definition id="5">
				<sentence>Pronominalization rules remain hard-coded heuristics in the micro-planner but can be guided by features introduced in the presentation representations .</sentence>
				<definiendum id="0">Pronominalization rules</definiendum>
				<definiens id="0">remain hard-coded heuristics in the micro-planner but can be guided by features introduced in the presentation representations</definiens>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>FromTo/EK consists of user interface for English and Korean , translation engine , and knowledge and dictionaries .</sentence>
				<definiendum id="0">FromTo/EK</definiendum>
				<definiens id="0">consists of user interface for English and Korean , translation engine , and knowledge and dictionaries</definiens>
			</definition>
			<definition id="1">
				<sentence>If both make a set of edges respectively , `` smaller-setfirst '' policy is applied to select a preferred set , that is , the number of edges in one set should be smaller than that of the other ( e.g. if n ( LR ) =6 and n ( RL ) =5 , then n ( RL ) is selected as the first ranked parse tree , where n ( LR ) is the number of left-to-right scanned edges , and n ( RL ) is the number of right-to-left scanned edges ) .</sentence>
				<definiendum id="0">n ( LR )</definiendum>
				<definiens id="0">the number of left-to-right scanned edges , and n</definiens>
				<definiens id="1">the number of right-to-left scanned edges</definiens>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>In character substitution if the erroneous character is a neighboring key of the character on the keyboard , or if the character has a similar sound to that of the substituted character , the error weight is reduced by 10 % .</sentence>
				<definiendum id="0">character substitution</definiendum>
				<definiens id="0">a neighboring key of the character on the keyboard , or if the character has a similar sound to that of the substituted character , the error weight is reduced by 10 %</definiens>
			</definition>
			<definition id="1">
				<sentence>Let char ( n ) indicate the character at location n of the erroneous word , and char ( m ) indicate the character at location m of the word from the lexicon .</sentence>
				<definiendum id="0">Let char</definiendum>
				<definiens id="0">indicate the character at location n of the erroneous word , and char ( m ) indicate the character at location m of the word from the lexicon</definiens>
			</definition>
			<definition id="2">
				<sentence>The 3wm transforms chaose to choose in two steps : drops a and inserts an o. Solution : When the 3win detects an added character error , and char ( n+l ) =char ( m+l ) and char ( n+2 ) ~ char ( m+l ) , we change the error to character substitution type .</sentence>
				<definiendum id="0">char</definiendum>
				<definiens id="0">drops a and inserts an o. Solution : When the 3win detects an added character error , and</definiens>
				<definiens id="1">the error to character substitution type</definiens>
			</definition>
			<definition id="3">
				<sentence>Solution : When comparing S with W we partition them as S=xuz and W=xvz .</sentence>
				<definiendum id="0">Solution</definiendum>
				<definiens id="0">When comparing</definiens>
			</definition>
			<definition id="4">
				<sentence>The semantic analyzer works with one parse tree at a time and examines all senses of the words and rejects any entry that violates the sematic rules .</sentence>
				<definiendum id="0">semantic analyzer</definiendum>
				<definiens id="0">works with one parse tree at a time and examines all senses of the words and rejects any entry that violates the sematic rules</definiens>
			</definition>
</paper>

		<paper id="2209">
			<definition id="0">
				<sentence>~V~IKE is an automatic commentary system that generates a commentary of a simulated soccer game in English , French , or Japanese .</sentence>
				<definiendum id="0">~V~IKE</definiendum>
				<definiens id="0">an automatic commentary system that generates a commentary of a simulated soccer game in English , French , or Japanese</definiens>
			</definition>
			<definition id="1">
				<sentence>Sample commentaries produced by MIKE are presented and used to evaluate different methods for content selection and generation in terms of efficiency of communication .</sentence>
				<definiendum id="0">Sample commentaries</definiendum>
				<definiens id="0">produced by MIKE are presented and used to evaluate different methods for content selection and generation in terms of efficiency of communication</definiens>
			</definition>
			<definition id="2">
				<sentence>MIKE is an automatic narration system that generates spoken live commentary of a simulated soccer game in English , French , or Japanese .</sentence>
				<definiendum id="0">MIKE</definiendum>
				<definiens id="0">an automatic narration system that generates spoken live commentary of a simulated soccer game in English , French , or Japanese</definiens>
			</definition>
			<definition id="3">
				<sentence>• Logical consequences : ( PassSuccessRate player percentage ) ( PassPattern player Goal ) -- -* ( active player ) • Logical subsumption : ( Pass playerl player2 ) ( Kick playerl ) -~ ( Delete @ 2 ) • State change : ( Form team forml ) ( F0rm team form2 ) -- + ( Delete earlier-prop ) • Second order relation : ( PassSuccessRate player percentage ) ( PlayerOnVoronoiLine playr ) -- * ( Reason @ 1 @ 2 ) Figure 3 : Categories and examples of inference rules fragments , which we call propositions .</sentence>
				<definiendum id="0">Logical subsumption</definiendum>
				<definiens id="0">F0rm team form2 ) -- + ( Delete earlier-prop ) • Second order relation : ( PassSuccessRate player percentage ) ( PlayerOnVoronoiLine playr ) -- *</definiens>
			</definition>
			<definition id="4">
				<sentence>MIKE has the very basic function of uttering the most important content at any given time .</sentence>
				<definiendum id="0">MIKE</definiendum>
				<definiens id="0">the very basic function of uttering the most important content at any given time</definiens>
			</definition>
			<definition id="5">
				<sentence>The NL Generator translates the selected proposition into a natural language expression and sends it to the TTS-administrator module .</sentence>
				<definiendum id="0">NL Generator</definiendum>
				<definiens id="0">translates the selected proposition into a natural language expression and sends it to the TTS-administrator module</definiens>
			</definition>
			<definition id="6">
				<sentence>We have described how MIKE , a live commentary generation system for the game of soccer , deals with the issues of real time content selection and realization .</sentence>
				<definiendum id="0">MIKE</definiendum>
				<definiens id="0">a live commentary generation system for the game of soccer , deals with the issues of real time content selection and realization</definiens>
			</definition>
</paper>

		<paper id="2217">
			<definition id="0">
				<sentence>The tabular interpretation ensures , for all strategies , a time complexity in O ( n 6 ) and space complexity in O ( n 5 ) where n is the length of the input string .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the input string</definiens>
			</definition>
			<definition id="1">
				<sentence>O is a finite set of transitions .</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">a finite set of transitions</definiens>
			</definition>
			<definition id="2">
				<sentence>The master stack MS is a word in ( D.M ) * where 2 ) denotes the set { /~ , x.~ , -- - % ~ } of action marks used to remember which action ( w.r.t. the auxiliary stack AS ) takes place when pushing the next master stack element .</sentence>
				<definiendum id="0">master stack MS</definiendum>
				<definiens id="0">a word in ( D.M ) * where 2</definiens>
				<definiens id="1">the set { /~ , x.~ , -- - % ~ } of action marks used to remember which action ( w.r.t. the auxiliary stack AS ) takes place when pushing the next master stack element</definiens>
			</definition>
			<definition id="3">
				<sentence>The empty master stack is noted e and a non-empty master stack ~1A1 ... ~nAn where A , ~ denotes the topmost element .</sentence>
				<definiendum id="0">~</definiendum>
				<definiens id="0">the topmost element</definiens>
			</definition>
			<definition id="4">
				<sentence>The auxiliary stack AS is a word of ( K : X* ) * where K : = { ~w , ~e } is a set of two elements used to delimit session stacks in AS .</sentence>
				<definiendum id="0">auxiliary stack AS</definiendum>
				<definiens id="0">a word of ( K : X* ) * where K : = { ~w , ~e } is a set of two elements used to delimit session stacks in AS</definiens>
			</definition>
			<definition id="5">
				<sentence>Formally , a LIG G is a 5-tuple ( VT , VN , S , VI , P ) where VT is a finite set of terminals , VN is a finite set of non-terminals , S E VN is the start symbol , VI is a finite set of indices and P is a finite set of productions .</sentence>
				<definiendum id="0">LIG G</definiendum>
				<definiendum id="1">VT</definiendum>
				<definiendum id="2">VN</definiendum>
				<definiendum id="3">S E VN</definiendum>
				<definiendum id="4">VI</definiendum>
				<definiendum id="5">P</definiendum>
				<definiens id="0">a 5-tuple ( VT , VN , S , VI , P ) where</definiens>
				<definiens id="1">a finite set of terminals</definiens>
				<definiens id="2">a finite set of non-terminals ,</definiens>
				<definiens id="3">the start symbol</definiens>
				<definiens id="4">a finite set of indices</definiens>
				<definiens id="5">a finite set of productions</definiens>
			</definition>
			<definition id="6">
				<sentence>Formally , a TAG is a 5-tuple G = ( VN , VT , S , I , A ) , where VN is a finite set of nonterminal symbols , VT a finite set of terminal symbols , S the axiom of the grammar , I a finite set of initial trees and A a finite set of auxiliary trees .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiendum id="1">VN</definiendum>
				<definiens id="0">a 5-tuple G = ( VN , VT , S , I</definiens>
				<definiens id="1">a finite set of nonterminal symbols</definiens>
			</definition>
			<definition id="7">
				<sentence>IUA is the set of elementary trees .</sentence>
				<definiendum id="0">IUA</definiendum>
			</definition>
			<definition id="8">
				<sentence>iX-WRITE\ ] T = ( W , C , c ) : z ( w , CX~F , e ) ( 3 ) \ ] i°**\ [ °°\ ] CWM**\ [ oo\ ] Aw } ~ MC°\\ [ °°\ ] Fw ( 4 ) where C = ( w , C , c ) , A = ( u , A , a ) , and F = &lt; k , F , a ) . \ [ -- *-ERASE\ ] r = ( e , B -- *C , e ) , ~ , ( e , F , e ) A°MA\ [ °°\ ] \ ] ~w'4°\ ] ~°~\ [ DE\ ] Oe } ~ A°MA\ [ DE\ ] Fe ( 5 ) where C = ( w , C , c ) , b = ( v , B , b ) , ~ ' = ( k , F , c/ , and ( when D # o ) D = ( s , D , b ) . 1337 \ [ x , ~-ERASE\ ] ~= ( e , Bx~C , e ) , z ( e , f , f ) 21° B° '' ~\ [ D*\ ] C'e } ~I°*A \ [ oo\ ] -~lw =~ -/V/° O # \ [ \ ] ~C°\ ] ~'e ( 6 ) f~°o~\ [ oolBw where C ' = ( w , C , c ) , /~ = ( v , B , b ) , M = ( / , M , m ) , ~ ' = ( k , F , f ) , and ( when D ~ o ) D = ( * , * , m ) . IF-ERASE\ ] ~= ( e , B~C , ~'~ ) ~ ( m , F , e ) /~°B~\ [ oo\ ] Ce } ~ MNA\ [ DE\ ] Fm ( 7 ) MNA\ [ DE\ ] Bm where C = ( w~C , ~m ) , B = ( v , B , b ) , and ~ ' = ( k , F , a ) \ [ /Z-ERASE\ ] r = ( e , B/ZC , c ) , = ~ ( e , F , e ) MNA\ [ ~\ ] l~w/~°\ ] ~/°/Z\ [ °°\ ] Ce } ==~ MNA\ [ °°\ ] /ae ( S ) where ( ~7 = ( w , C , c/ , B = ( v , B , b/ , and ~ ' = ( k , F , b ) .B°B°/Z\ [ DE°\ ] Ce } MNA\ [ oolBw ~ MNA\ [ OPI~'e ( 9 ) MD°x , ~\ [ OP\ ] E , e where C ' = ( w , C , c ) , /~ = ( v , B , b ) , ~ ' = { k , F , b ) , and ( when O # o ) O = &lt; l , O , b ) . \ [ SWAP\ ] r = ( p , C , ~ ) , z ( q , F , ~ ) AB6\ [ DE\ ] Cm ~ AB6\ [ DEI~ 'm ( 10 ) where C ? = ( w , C , c ) , ~ ' = ( k , F , c ) , and either c=~=~°or~=e. The best way to apprehend these rules is to visualize them graphically as done for the two most complex ones ( Rules 6 and 9 ) in Figures 4 and 5. A `` -- L Figure 4 : Application of Rule 6 N C D ~M P `` ~'- '' ~ Figure 5 : Application of Rule 9 An analysis of the time complexity to apply each rule gives us polynomial complexities O ( n '' ) with u &lt; _ 6 except for Rule 9 where u = 8. However , by adapting an idea from ( Nederhof , 1997 ) , we replace Rule 9 by the alternate and equivalent Rule 11. `` B°*/\ [ b'E°\ ] C'e } , D° x , ,~\ [ OP\ ] ~'e MNA\ [ oo\ ] l~w ~ MNA\ [ OPI~e ( 11 ) M . ' % \ [ O P\ ] *e where C7 : ( w , C , c ) , B = ( v , B , b ) , ~ ' = ( k , F , b ) , and ( when O ¢ v ) O = ( l , O , b ) . Rule 11 has same complexity than Rule 9 , but may actually be split into two rules of lesser complexity O ( n6 ) , introducing an intermediary pseudo-item BB/Z\ [ \ [ OP\ ] \ ] Ce ( intuitively assimilable to a `` deeply escaped '' CF derivation ) . Rule 12 collects these pseudo-items ( independently from any transition ) while Rule 13 combines them with items ( given a/Z-ERASE transition ~- ) . BB/Z\ [ / ) E°\ ] C'e } ===~ BB/Z\ [ \ [ OP\ ] \ ] ( 3'e ( 12 ) *D°\\ [ OP\ ] E , e 1~° \ ] ~°/Z\ [ \ [ OP\ ] \ ] Ce } MNA\ [ c~\ ] I~w ~ MNA\ [ OP\ ] Fe ( 13 ) M* ~ , ~\ [ OP\ ] *e where C7 = ( w , C , c } , B = ( v , B , b ) , ~ ' = ( k , F , b ) , and ( when O ¢ o ) O = ( l , O , b ) . Theorem 5.1 The worst time complexity of the application rules ( 1,2,3,4,5,6,7,8,10,12,13 ) is O ( n 6 ) where n is the length of the input string. The worst space complexity is O ( nS ) . Two main theorems establish the correctness of derivable items w.r.t , derivable configurations. A derivable item is either the initial item or an item resulting from the application of a combination rules on derivable items. The initial item ( 0 , e ) ( 0 , e ) ~\ [ oo\ ] &lt; 0 , $ 0 , ~w &gt; w stands for the virtual derivation step ( w , 0 , e , e ) \ [ ( w , 0 , ~ $ 0 , ~w ) .</sentence>
				<definiendum id="0">F , e ) A°MA\</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">5 ) where C = ( w , C , c ) , b = ( v , B , b ) , ~ ' = ( k , F , c/ , and ( when D # o ) D = ( s , D , b )</definiens>
				<definiens id="1">] E , e where C ' = ( w , C , c ) , /~ = ( v , B , b ) , ~ ' = { k , F , b ) , and ( when O # o ) O = &lt; l</definiens>
				<definiens id="2">*e where C7 : ( w , C , c ) , B = ( v , B , b ) , ~ ' = ( k , F , b ) , and ( when O ¢ v</definiens>
				<definiens id="3">c } , B = ( v , B , b ) , ~ ' = ( k , F , b ) , and ( when O ¢ o ) O = ( l , O , b )</definiens>
				<definiens id="4">either the initial item or an item resulting from the application of a combination rules on derivable items. The initial item ( 0 , e ) ( 0 , e</definiens>
			</definition>
			<definition id="9">
				<sentence>Theorem 5.2 ( Soundness ) For every derivable item Z = AB6\ [ £IE\ ] Cm , there exists a derivation on configurations ( o , e ) I-D -- Ul~v such that H\ [ -~V is a CF or xCF derivation representable by I. Proof : By induction on the item derivation length and by case analysis .</sentence>
				<definiendum id="0">Soundness</definiendum>
				<definiens id="0">a CF or xCF derivation representable by I. Proof : By induction on the item derivation length and by case analysis</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>Consider a context-free grammar consisting of a set C of `` categories '' or non-terminal symbols , including one distinguished symbol s ( for `` sentence '' ) ; a set of terminal symbols T ( `` lexical slots '' ) , none of which are also in C , a lexicon L which is basically a set of words and an indication of the kind of lexical slot each word can fill , and a set of rewrite rules R of form c -- ~ vl - '' vn where c is a symbol in C ' , and the string on the right hand side vl ... vn consists of one or more symbols in T or C. A sentence is derived by writing s , then rewriting s by the string Ul ' '' um on the right hand side of any rule in R of form s -~ ul '' .</sentence>
				<definiendum id="0">c</definiendum>
			</definition>
			<definition id="1">
				<sentence>Condition ( iii ) , formulated as it is in terms of the rank order of constituents , must be worded differently to capture the basic idea that a constituent is labeled according to language A whenever it is out place according to a rule of language B , and viceversa .</sentence>
				<definiendum id="0">Condition ( iii )</definiendum>
				<definiens id="0">place according to a rule of language B , and viceversa</definiens>
			</definition>
</paper>

		<paper id="2173">
			<definition id="0">
				<sentence>This approach has also been adopted in two M-NLG systems : GIST ( Power and Cavallotto , 1996 ) , which generates social security forms in English , Italian and German ; and DRAFTER ( Paris et al. , 1995 ) , which generates instructions for software applications in English and French .</sentence>
				<definiendum id="0">DRAFTER</definiendum>
				<definiens id="0">generates social security forms in English</definiens>
			</definition>
</paper>

		<paper id="1120">
			<definition id="0">
				<sentence>Finally , morphological analogies can be regarded as simple equations independent of any knowledge about the language in which they are written .</sentence>
				<definiendum id="0">morphological analogies</definiendum>
				<definiens id="0">simple equations independent of any knowledge about the language in which they are written</definiens>
			</definition>
			<definition id="1">
				<sentence>In linguistic works , analogy is defined by Saussure , after Humboldt and Baudoin de Courtenay , as the operation by which , given two forms of a given word , and only one form of a second word , the missing form is coined 4 , `` honor is to hon6rem as 6r6tor is to 6rSt6rem '' noted 6r~t6rem : 6rdtor = hon6rem : honor .</sentence>
				<definiendum id="0">analogy</definiendum>
				<definiendum id="1">Saussure</definiendum>
				<definiens id="0">the operation by which , given two forms of a given word , and only one form of a second word</definiens>
			</definition>
			<definition id="2">
				<sentence>sire ( A , B ) = I A \ [ pdist ( A , B ) For instance , pdist ( unlike , like ) = 2 , while pdist ( like , unlike ) = O. l i k e u 1 1 1 1 u n l i k e n 2 2 2 2 l 2 2 2 2 I 1 1 0 0 0 0 i 3 2 2 2 i 2 2 1 0 0 0 k 4 3 2 2 k 3 3 2 1 0 0 e 5 4 3 2 e 4 4 3 2 1 0 Characters inserted into B or C may be left aside , precisely because they are those characters of B and C , absent from A , that we want to assemble into the solution , D. As A is the axis in the resolution of analogy , graphically we make it the vertical axis around which the computation of pseudo-distances takes place .</sentence>
				<definiendum id="0">sire ( A , B</definiendum>
				<definiendum id="1">B</definiendum>
			</definition>
			<definition id="3">
				<sentence>anatrices ( A , B , C , max ( \ [ A I pdist ( d , C ) , pdAB + 1 ) , xnax ( I A I pdist ( A , B ) , pdac + x ) ) end if end proc COlnpute_matrices Once enough in the matrices has been computed , the principle of the algorithm is to follow the paths along which longest common subsequences are found , simultaneously in both matrices , copying characters into the solution accordingly .</sentence>
				<definiendum id="0">anatrices</definiendum>
				<definiendum id="1">I pdist</definiendum>
				<definiens id="0">to follow the paths along which longest common subsequences are found</definiens>
			</definition>
			<definition id="4">
				<sentence>dirAc ) is the direction of the path in matrix A x B ( resp .</sentence>
				<definiendum id="0">dirAc )</definiendum>
				<definiens id="0">the direction of the path in matrix A x B ( resp</definiens>
			</definition>
			<definition id="5">
				<sentence>`` copy '' means to copy a character from a word at the beginning of D and to move to the previous character in that word .</sentence>
				<definiendum id="0">copy ''</definiendum>
				<definiens id="0">means to copy a character from a word at the beginning of D and to move to the previous character in that word</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>Numeral classifier combinations appear in seven major patterns of use ( following Asahioka et al. ( 1990 ) ) as shown below ( T refers to the quantified target noun phrase , m is a casemarker ) : Type Form XC N pre-nominal Q-no T-m + + appositive TQ-m + floating T-m Q + + Q T-m partitive T-no Q-m + + attributive QT-m + anaphoric T-m ÷ predicative T-wa Q-da ÷ Table 1 : Types of quantifier constructions Noun quantifiers can not appear in the appositive , attributive , anaphoric and predicative complement patterns .</sentence>
				<definiendum id="0">Numeral classifier combinations</definiendum>
				<definiendum id="1">m</definiendum>
			</definition>
			<definition id="1">
				<sentence>Verb arguments subcategorized for in the lexicon are noun phrases , where the case-marker is a clitic and thus can be c-commanded , whereas adjuncts are headed by their markers , to form post-positional phrases which are thus not available as targets .</sentence>
				<definiendum id="0">case-marker</definiendum>
			</definition>
			<definition id="2">
				<sentence>Noun quantifiers can be used as degree modifiers as well as quantifying some referent .</sentence>
				<definiendum id="0">Noun quantifiers</definiendum>
				<definiens id="0">degree modifiers as well as quantifying some referent</definiens>
			</definition>
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>IMAFO includes an enhanced version of ID3 and an interface to C4.5 ( we used both engines in our experimentation ) .</sentence>
				<definiendum id="0">IMAFO</definiendum>
				<definiens id="0">includes an enhanced version of ID3 and an interface to C4.5</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>Finding simple , non-recursive , base noun phrases is an important subtask for many natural language processing applications .</sentence>
				<definiendum id="0">base noun phrases</definiendum>
				<definiens id="0">an important subtask for many natural language processing applications</definiens>
			</definition>
			<definition id="1">
				<sentence>Finding base noun phrases is a sensible first step for many natural language processing ( NLP ) tasks : Accurate identification of base noun phrases is arguably the most critical component of any partial parser ; in addition , information retrieval systems rely on base noun phrases as the main source of multi-word indexing terms ; furthermore , the psycholinguistic studies of Gee and Grosjean ( 1983 ) indicate that text chunks like base noun phrases play an important role in human language processing .</sentence>
				<definiendum id="0">base noun phrases</definiendum>
				<definiendum id="1">noun phrases</definiendum>
				<definiens id="0">a sensible first step for many natural language processing ( NLP ) tasks : Accurate identification of base noun phrases is arguably the most critical component of any partial parser ; in addition , information retrieval systems rely on base noun phrases as the main source of multi-word indexing terms ; furthermore , the psycholinguistic studies of Gee and Grosjean ( 1983 ) indicate that text chunks like base</definiens>
			</definition>
			<definition id="2">
				<sentence>A notable exception is the Ramshaw &amp; Marcus work , which evaluates their transformation-based learning approach on a base NP corpus derived from the Penn Treebank WSJ , and achieves precision and recall levels of approximately 93 % .</sentence>
				<definiendum id="0">notable exception</definiendum>
				<definiens id="0">evaluates their transformation-based learning approach on a base NP corpus derived from the Penn Treebank WSJ , and achieves precision and recall levels of approximately 93 %</definiens>
			</definition>
			<definition id="3">
				<sentence>The benefit of rule r is given by B~ = C , E , where C~ 220 Training Corpus Pruning Corpus Improved Rule Set Final Rule Set Figure 3 : Pruning the Base NP Grammar is the number of NPs correctly identified by r , and E~ is the number of precision errors for which r is responsible .</sentence>
				<definiendum id="0">E~</definiendum>
				<definiens id="0">Pruning the Base NP Grammar is the number of NPs correctly identified by r , and</definiens>
			</definition>
			<definition id="4">
				<sentence>Rule ( NNP NNP , NNP / incorrectly identifies Boca Raton , Hot as a noun phrase , so its score is -1 .</sentence>
				<definiendum id="0">Rule</definiendum>
				<definiens id="0">a noun phrase</definiens>
			</definition>
			<definition id="5">
				<sentence>Finally , rule ( NNP NNP ) receives a score of 1 for correctly identifying Palm Beach as a base NP .</sentence>
				<definiendum id="0">NNP NNP )</definiendum>
				<definiens id="0">receives a score of 1 for correctly identifying Palm Beach as a base NP</definiens>
			</definition>
			<definition id="6">
				<sentence>This is the case for our initial rule set , culled from the WSJ corpus , which contains approximately 4500 base NP rules .</sentence>
				<definiendum id="0">WSJ corpus</definiendum>
			</definition>
			<definition id="7">
				<sentence>Performance is measured in terms of precision and recall .</sentence>
				<definiendum id="0">Performance</definiendum>
				<definiens id="0">measured in terms of precision and recall</definiens>
			</definition>
</paper>

		<paper id="1111">
			<definition id="0">
				<sentence>A Korean word ( called eojeol ) consists of more than one morpheme with clear-cut morpheme boundaries ( Korean is an agglutinative language ) .</sentence>
				<definiendum id="0">Korean word</definiendum>
			</definition>
			<definition id="1">
				<sentence>Grapheme-to-Phoneme Converter Part-of-speech ( POS ) tagging is a basic step to the grapheme-to-phoneme conversion since phonological changes depend on morphotactic and phonotactic environments .</sentence>
				<definiendum id="0">Grapheme-to-Phoneme Converter Part-of-speech</definiendum>
				<definiens id="0">a basic step to the grapheme-to-phoneme conversion since phonological changes depend on morphotactic and phonotactic environments</definiens>
			</definition>
			<definition id="2">
				<sentence>• Korean is a syllable-base language , i.e. , Korean syllable is the basic unit of the graphemes and consists of first consonant , vowel and final consonant ( CVC ) .</sentence>
				<definiendum id="0">Korean syllable</definiendum>
				<definiens id="0">a syllable-base language</definiens>
				<definiens id="1">the basic unit of the graphemes and consists of first consonant , vowel and final consonant ( CVC )</definiens>
			</definition>
			<definition id="3">
				<sentence>This phoneme connectivity table indicates the grammatical sound combinations in Korean 678 phonology using the defined left and right connectivity information .</sentence>
				<definiendum id="0">phoneme connectivity table</definiendum>
				<definiens id="0">indicates the grammatical sound combinations in Korean 678 phonology using the defined left and right connectivity information</definiens>
			</definition>
			<definition id="4">
				<sentence>This paper presents a new grapheme-tophoneme conversion method using phoneme connectivity and CCV conversion rules for unlimited vocabulary Korean TTS .</sentence>
				<definiendum id="0">CCV conversion</definiendum>
				<definiens id="0">rules for unlimited vocabulary Korean TTS</definiens>
			</definition>
</paper>

		<paper id="2161">
			<definition id="0">
				<sentence>An NL Toolkit enables a developer to enter such information for additional words manually .</sentence>
				<definiendum id="0">NL Toolkit</definiendum>
				<definiens id="0">enables a developer to enter such information for additional words manually</definiens>
			</definition>
			<definition id="1">
				<sentence>The WordNet concepts correspond to real-world entities and phenomena in terms of which people understand the meanings of words .</sentence>
				<definiendum id="0">WordNet concepts</definiendum>
				<definiens id="0">correspond to real-world entities and phenomena in terms of which people understand the meanings of words</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>A word order domain is a set of words , generalizing the notion of positions in DUG .</sentence>
				<definiendum id="0">word order domain</definiendum>
				<definiens id="0">a set of words , generalizing the notion of positions in DUG</definiens>
			</definition>
			<definition id="1">
				<sentence>The domain structure in Fig.2 satisfies these restrictions since nothing follows the participle , and because `` den Mann '' is not an element of d2 , which contains `` hat '' .</sentence>
				<definiendum id="0">d2</definiendum>
				<definiens id="0">contains `` hat ''</definiens>
			</definition>
			<definition id="2">
				<sentence>Def : An order domain ( over W ) m is a set of words from ~ ) where VWl , W2 , W3 E VV : ( wl - &lt; w2- &lt; w3Awl EmAw3 Era ) ~ w2 E m. Def : An order domain structure ( over W ) f14 is a set of order domains where Vm , m ~ E .£4 : mMm ~ = OVm C m'Vm ~ C m. 4One review pointed out that some verbs may allow extractions , i.e. , that this restriction is lexical , not universal. This fact can easily be accomodated because the possibility of discontinuity ( and the dependency types across which the modifier may be extracted ) is described in the lexical entry of the verb. In fact , a universal restriction could not even be stated because the treatment is completely lexicalized. Def : A dependency structure T is a tuple ( VV , Wr , R~ , VA , Vc , .A4 , VM &gt; where ( I , V , wr , Rz~ , VA , VC &gt; is a dependency tree , A4 is an order domain structure over ~V , and VAa : V~ ~ .</sentence>
				<definiendum id="0">Def</definiendum>
				<definiendum id="1">domain</definiendum>
				<definiendum id="2">A4</definiendum>
				<definiendum id="3">VAa</definiendum>
				<definiens id="0">An order</definiens>
				<definiens id="1">VWl , W2 , W3 E VV : ( wl - &lt; w2- &lt; w3Awl EmAw3 Era ) ~ w2 E m. Def : An order domain structure ( over W</definiens>
				<definiens id="2">a set of order domains where Vm</definiens>
				<definiens id="3">A dependency structure T is a tuple ( VV , Wr , R~ , VA , Vc , .A4 , VM &gt; where ( I , V , wr , Rz~ , VA , VC &gt; is a dependency tree</definiens>
			</definition>
			<definition id="3">
				<sentence>177 Syntax ( valid formulae ) Semantics ( satisfaction relation ) c • £v , Vc • C T , w a • £v , Va • A T , w &lt; d ) ¢ • £v , Vd • 79 , ¢ • £v T , w &lt; . 6 £v , T , w &lt; ~ •£9 , V6c_79 $ ~ •£v , VTcD oi single • ED , Vi • $ V , % 4 o~filled • £D , Vi • ~V D~a•£D , Vi• $ V , a•A ¢^¢ • £~ , V¢ , ¢ • £v -~¢ 6.£~ , V¢ • £v T , w T , w T , w T , w T , iv T , w T , w ~c ka &lt; d ) ¢ : ¢* c = Yc ( w ) : ¢* a e Y ( w ) : ¢~ 3w ' 6 142 : wRdW ' A T , w ' ~ ¢ : ~ 3m • M : ( V.~ ( w ) = ( ... m ... ) ^Vw ' • m : ( w = w ' Vw - &lt; w ' ) ) : ¢~ ~3w ' , w '' , w ' '' • W : places ( w ' , w ) Aplaces ( w ' , w '' ) A w ' '' R6w A w ' '' `` &lt; w $ 6 : ¢~ 3w ' , w '' • ~42 : wRvwA places ( w '' , w ) A w '' R ; w ' o ! { w'• ( , ,,11^ \ ] o~single : ¢t , w ' ~Bw '' : ( w '' RT ) w'n , , &lt; 1 w '' e k obfilled I 1 t:3~a : ¢* Vw ' • Oi ( V.M ( w ) ) : T , w ' k a ~¢A¢ : ¢~ , T , w~¢andT , w~¢ -- , ¢ : ¢¢ , not T , w ~ ¢ Figure 3 : Syntax and Semantics of Ev Formulae Vfin ~ ol ( single A filled ) A OLinitial \ [ 1\ ] A O L ( middle A norel ) \ [ 2\ ] A 0 3 single A D L ( final A norel ) \ [ 3\ ] A V2 ¢~ ( middleA &lt; , A\ [ 3~norel ) \ [ 4\ ] A VEnd ¢~ ( middleA &gt; , ) \ [ 5\ ] A Vl ¢~ ( initial A norel ) \ [ 6\ ] Figure 4 : Domain Description of finite verbs `` hat '' A Vfin \ [ 7\ ] A ( subj ) ( `` Junge '' A 1 '' 0 ) \ [ 8\ ] A ( vpart ) ( `` gesehen '' A S0 \ [ 9\ ] A ~final A &gt; { subj , obj } \ [ i0\ ] A ( obj ) ( `` Mann '' A t { vpart } ) ) \ [ 11\ ] Figure 5 : Hierachical Structure final fields by infinite verb parts such as separable prefixes or participles .</sentence>
				<definiendum id="0">Vl ¢~</definiendum>
				<definiens id="0">w T , w T , w T , w T , iv T , w T</definiens>
			</definition>
			<definition id="4">
				<sentence>Lexical-Functional Grammar : A Formal System for Grammatical Representation .</sentence>
				<definiendum id="0">Lexical-Functional Grammar</definiendum>
				<definiens id="0">A Formal System for Grammatical Representation</definiens>
			</definition>
			<definition id="5">
				<sentence>Slot Grammar : A System for Simpler Construction of Practical Natural Language Grammars .</sentence>
				<definiendum id="0">Slot Grammar</definiendum>
				<definiens id="0">A System for Simpler Construction of Practical Natural Language Grammars</definiens>
			</definition>
			<definition id="6">
				<sentence>A Formal Theory of Word Order : A Case Study in West Germanic .</sentence>
				<definiendum id="0">Formal Theory of Word Order</definiendum>
				<definiens id="0">A Case Study in West Germanic</definiens>
			</definition>
</paper>

		<paper id="1116">
			<definition id="0">
				<sentence>Language generation is an important subtask of applications like machine translation , humancomputer dialogue , explanation , and summarization .</sentence>
				<definiendum id="0">Language generation</definiendum>
				<definiens id="0">an important subtask of applications like machine translation , humancomputer dialogue , explanation , and summarization</definiens>
			</definition>
			<definition id="1">
				<sentence>A desirable solution is a generator that abstracts away from templates enough to provide the needed flexibility and scalability , and yet still requires only minimal semantic input ( and maintains reasonable efficiency ) .</sentence>
				<definiendum id="0">desirable solution</definiendum>
				<definiens id="0">a generator that abstracts away from templates enough to provide the needed flexibility and scalability , and yet still requires only minimal semantic input ( and maintains reasonable efficiency )</definiens>
			</definition>
			<definition id="2">
				<sentence>A corpus-based statistical ranker takes a set of sentences packed efficiently into a word lattice , ( a state transition diagram with links labeled by English words ) , and extracts the best path from the lattice as output , preferring fluent sentences over contorted ones .</sentence>
				<definiendum id="0">corpus-based statistical ranker</definiendum>
				<definiens id="0">takes a set of sentences packed efficiently into a word lattice , ( a state transition diagram with links labeled by English words</definiens>
			</definition>
			<definition id="3">
				<sentence>The AMR language is composed of concepts from the SENSUS knowledge base ( Knight and Luk , 1994 } , including all of WordNet 1.5 ( Miller , 1990 ) , and keywords relating these concepts to each other ) An AMR is a labeled directed graph , or feature structure , derived from the PENMAN Sentence Plan Language ( Penman , 1989 ) .</sentence>
				<definiendum id="0">AMR language</definiendum>
				<definiendum id="1">AMR</definiendum>
				<definiens id="0">a labeled directed graph , or feature structure , derived from the PENMAN Sentence Plan Language</definiens>
			</definition>
			<definition id="4">
				<sentence>The most basic AMR is of the form ( label / concept ) , e.g. : ~ ( ml / \ [ dog &lt; canidl ) The slash is shorthand for a type ( or instance ) feature , and in logic notation this AMR might be written as instance ( m1 , dog ) . This AMR can represent `` the dog , '' `` the dogs , '' `` a dog , '' or `` dog , '' etc. A concept can be modified using keywords : ( , ,2 I \ [ dog &lt; canid\ [ : quant plural ) IStrings can be used in place of concepts. If the string is not a recognized word/phrase , then the generator will add this ambiguity to the word lattice for the statistical extractor to resolve by proposing all possible part-of-speech tags. We prefer to use concepts because they make the AMR more language-lndependent , and enable semantic reasoning and inference. 2Concept names appear between vertical bars. We use a set of short , unique concept names derived from the structure of WordNet by Jonathan Graehl , and available from http : //www.isi.edu/natural-language/GAZELLE.html 705 This narrows the meaning to `` the dogs , '' or `` dogs. '' Concepts can be associated with each other in a nested fashion to form more complex meanings. These relations between conceptual meanings are also expressed through keywords. It is through them that our formalism exhibits an appealing flexibility. A client has the freedom to express the relations at various semantic and syntactic levels , using whichever level of representation is most convenient. 3 We have currently implemented shallow semantic versions of roles such as : agent , : patient , : sayer , : sensor , etc. , as well as deep syntactic roles such as : obliquel , : oblique2 , and : oblique3 ( which correspond to deep subject , object , and indirect object respectively , and serve as an abstraction for passive versus active voice ) and the straightforward syntactic roles : subject , : directobject , : indirect-object , etc. We explain further how this is implemented later in the paper. Below is an example of a slightly more complex meaning. The root concept is eating , and it has an agent and a patient , which are dogs and a bone ( or bones ) , respectively. ( m3 / lear , take inJ : agent ( m4 / Idog &lt; canidJ : quant plural ) : patient ( mS / \ [ os , boneJ ) ) Possible output includes `` The dogs ate the bone , '' `` Dogs will eat a bone , '' `` The dogs eat bones , '' `` Dogs eat bone , '' and `` The bones were eaten by dogs. '' The Sensus concept ontology is mapped to an English lexicon that is consulted to find words for expressing the concepts in an AMR. The lexicon is a list of 110,000 tuples of the form : ( &lt; word &gt; &lt; part-of-speech &gt; &lt; rank &gt; &lt; concept &gt; ) Examples : ( ( `` eat '' VERB I feat , take in\ [ ) ( `` eat '' VERB 2 Jeat &gt; eat lunch\ [ ) ° .</sentence>
				<definiendum id="0">most basic AMR</definiendum>
				<definiens id="0">the statistical extractor to resolve by proposing all possible part-of-speech tags. We prefer to use concepts because they make the AMR more language-lndependent , and enable semantic reasoning</definiens>
				<definiens id="1">correspond to deep subject , object , and indirect object respectively , and serve as an abstraction for passive versus active voice ) and the straightforward syntactic roles : subject</definiens>
			</definition>
			<definition id="5">
				<sentence>Japanese analysis provides AMR 's , which Nitrogen transforms into word lattices on the order of hundreds of nodes and thousands of arcs .</sentence>
				<definiendum id="0">AMR 's</definiendum>
				<definiens id="0">Nitrogen transforms into word lattices on the order of hundreds of nodes and thousands of arcs</definiens>
			</definition>
</paper>

		<paper id="2148">
			<definition id="0">
				<sentence>Then this relation is representing by the following form of rewriting rule of CFG : B =~ AB , where A is the attribute of the anterior bunsetsu and B is that of the posterior .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">representing by the following form of rewriting rule of CFG : B =~ AB , where A is the attribute of the anterior bunsetsu</definiens>
			</definition>
			<definition id="1">
				<sentence>The aim of word clustering is to build a language model with less cross entropy without referring to the test corpus .</sentence>
				<definiendum id="0">word clustering</definiendum>
				<definiens id="0">to build a language model with less cross entropy without referring to the test corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>Therefore the objective function , called average cross entropy , is defined as follows : m y= __1 ~ H ( Li , Mi ) , ( 5 ) m i -- -- 1 where Li is the i-th learning corpus and Mi is the language model estimated from the learning corpus excluding the i-th learning corpus .</sentence>
				<definiendum id="0">average cross entropy</definiendum>
				<definiendum id="1">Li</definiendum>
				<definiendum id="2">Mi</definiendum>
				<definiens id="0">the i-th learning corpus</definiens>
				<definiens id="1">the language model estimated from the learning corpus excluding the i-th learning corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>P ( xlT ) = 1 ) , W ( T ) =Z where to ( T ) represents the character sequence of the syntactic tree T. P ( T ) in the last line is a stochastic language model including the concept of dependency .</sentence>
				<definiendum id="0">T )</definiendum>
			</definition>
			<definition id="4">
				<sentence>It follows that a CKY method extended to SCFG , a dynamic-programming method , is applicable to calculate the best solution in O ( n 3 ) time , where n is the number of input characters .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of input characters</definiens>
			</definition>
</paper>

		<paper id="2166">
			<definition id="0">
				<sentence>In some situations , PPAs like shown in example 2 and 3 can be solved by applying semantic knowledge , since PPAs establish possessive relationships ( in concrete or figurative sense ) between objects in discourse .</sentence>
				<definiendum id="0">PPAs</definiendum>
				<definiens id="0">PPAs establish possessive relationships ( in concrete or figurative sense ) between objects in discourse</definiens>
			</definition>
			<definition id="1">
				<sentence>A reflexive agent is a rulebased entity , which acts according to the perceived environment ( the blackboard structure ) .</sentence>
				<definiendum id="0">reflexive agent</definiendum>
				<definiens id="0">a rulebased entity , which acts according to the perceived environment ( the blackboard structure )</definiens>
			</definition>
</paper>

		<paper id="1096">
			<definition id="0">
				<sentence>In information retrieval systems a common user initiative is a request for domain concept information from the database ; users specify a database object , or a set of objects , and ask for the value of a property of that object or set of objects .</sentence>
				<definiendum id="0">information retrieval</definiendum>
				<definiens id="0">a request for domain concept information from the database ; users specify a database object , or a set of objects , and ask for the value of a property of that object or set of objects</definiens>
			</definition>
			<definition id="1">
				<sentence>Domain concepts are concepts about which the system has information , mainly concepts from the database , T , but also synonyms to such concepts acquired , for instance , from the information base describing the system , S. In a database query system users also often request information by relating concepts and objects , e.g. which one is the cheapest .</sentence>
				<definiendum id="0">Domain concepts</definiendum>
				<definiens id="0">concepts about which the system has information , mainly concepts from the database , T , but also synonyms to such concepts acquired , for instance , from the information base describing the system</definiens>
			</definition>
			<definition id="2">
				<sentence>The object grammar consists of 5 rules and the property grammar consists of 21 rules .</sentence>
				<definiendum id="0">object grammar</definiendum>
				<definiens id="0">consists of 5 rules and the property grammar consists of 21 rules</definiens>
			</definition>
			<definition id="3">
				<sentence>The information base for TRAVEL consists of texts from travel brochures which contains a lot of information .</sentence>
				<definiendum id="0">information base</definiendum>
				<definiens id="0">contains a lot of information</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>o , , Z c ( wiwj ) p ( ~ ) ( wj I wi ) ( 8 ) ij where Tcro , , is the number of words in Weros , , and c ( wiwj ) the number of co-occurences of the words wi and wj in Wero , ~ .</sentence>
				<definiendum id="0">Z c</definiendum>
				<definiens id="0">ij where Tcro , , is the number of words in Weros , , and c ( wiwj ) the number of co-occurences of the words wi and wj in Wero</definiens>
			</definition>
			<definition id="1">
				<sentence>In the case of a stochastic phrase based model where the segmentation into phrases is not known a priori the above computation of the interpolation weights still applies , however , it has to be embedded in dynamic programming to solve the ambiguity on the segmentation : A ( k+l ) _ 1 S- '' e ( sis~\ ] S * ( k ) ) A ( k ) p ( sj I si ) c ( S ' ( ~ ) ) ~ p ( ~ ) ( si I si ) s,2 ( 9 ) where S `` ( k ) the most likely segmentation of Wero , s given the current estimates p ( ~ ) ( sj I si ) can be retrieved with a Viterbi algorithm , and where c ( S* ( k ) ) is the number of sequences in the segmentation S `` ( k ) .</sentence>
				<definiendum id="0">c ( S*</definiendum>
				<definiens id="0">the number of sequences in the segmentation S `` ( k )</definiens>
			</definition>
			<definition id="2">
				<sentence>The class-phrase models are trained with 5 iterations of the algorithm described in section 2.2 : each iteration consists in clustering the phrases into 300 phrase-classes ( step 1 ) , and in reestimating the phrase distribution ( step 2 ) with Eq .</sentence>
				<definiendum id="0">iteration</definiendum>
			</definition>
			<definition id="3">
				<sentence>The forward variable represents the likelihood of the first t words , where the last li words are constrained to form a sequence : = The backward variable represents the conditional likelihood of the last ( T -t ) words , knowing that they are preceded by the sequence \ [ w ( t_zi+l ) ... w ( 0\ ] : = Assuming that the likelihood of a parse is computed according to Eq .</sentence>
				<definiendum id="0">forward variable</definiendum>
			</definition>
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>A Astructure can be considered as a A-term up to consistent renaming of bound variables ( aequality ) ; a constraint of CLLS is an underspecified description of a A-structure .</sentence>
				<definiendum id="0">CLLS</definiendum>
				<definiens id="0">an underspecified description of a A-structure</definiens>
			</definition>
			<definition id="1">
				<sentence>A A-structure is a standard predicate logic tree structure which can be considered as a A-term or some other logical formula up-to consistent renaming of bound variables ( a-equality ) .</sentence>
				<definiendum id="0">A-structure</definiendum>
				<definiens id="0">a standard predicate logic tree structure which can be considered as a A-term or some other logical formula up-to consistent renaming of bound variables ( a-equality )</definiens>
			</definition>
			<definition id="2">
				<sentence>A-Structures ( CLLS ) CLLS is an ordinary first-order language interpreted over A-structures .</sentence>
				<definiendum id="0">A-Structures</definiendum>
				<definiendum id="1">CLLS ) CLLS</definiendum>
				<definiens id="0">an ordinary first-order language interpreted over A-structures</definiens>
			</definition>
			<definition id="3">
				<sentence>Every A-structure characterizes a unique A-term or a logical formula up to consistent renaming of bound variables ( a-equality ) .</sentence>
				<definiendum id="0">A-structure</definiendum>
				<definiens id="0">characterizes a unique A-term or a logical formula up to consistent renaming of bound variables ( a-equality )</definiens>
			</definition>
			<definition id="4">
				<sentence>As a means to underspecify Astructures , CLLS employs constraints for dominance X~*Y. Dominance is defined as the transitive and reflexive closure of immediate dominance .</sentence>
				<definiendum id="0">underspecify Astructures</definiendum>
				<definiendum id="1">CLLS employs constraints</definiendum>
				<definiens id="0">the transitive and reflexive closure of immediate dominance</definiens>
			</definition>
			<definition id="5">
				<sentence>We write ~r &lt; r ' for ~r being an initial segment of 7d. The dominance relation v &lt; ~v ' holds if 37r v.Tr = v'. If ~r is non-empty we have proper dominance v &lt; +v '. A A-structure L is a tree structure with two ( partially functional ) binary relations AL ( ' ) = `` , for binding , and anteL ( ' ) = ' , for anaphor-toantecedent linking. We assume that the following conditions hold : ( 1 ) binding only holds between variables ( nodes labelled var ) to A-binders ( nodes labelled lain ) ; ( 2 ) every variable has exactly one binder ; ( 3 ) variables are dominated by their binders ; ( 4 ) only anaphors ( nodel labelled ana ) are linked to antecendents ; ( 2 ) every anaphor has exactly one antecendent ; ( 5 ) antecedents are terminal nodes ; ( 6 ) there are no cyclic link chains ; ( 7 ) if a link chain ends at a variable then each anaphor in the chain must be dominated by the binder of that variable. The not so straight forward part of the semantics of CLLS is the notion of parallelism , which we define for any given A-structure L as follows : iff there is a path ~r0 such that : node of the parallel structures the the two exception positions : v { =Vl.~ro A v~=v2.~ro low Vl and v2 up-to the trees below the exception positions v { and v~ , must have the same structure and labels : Vr -~0 &lt; r ~ ( ( v , .~ $ L ~ v2.rSL ) A ( Vl.Tr.~L = : ~ l ( Vl.Tr ) -- -l ( v2.Tr ) ) ) ) texts to variables outside them : VvVv ' * + ' * ' AL ( v ' ) =v ) ~ ( Vl &lt; ~LV &lt; ~ L Vl &lt; ~LV A the two contexts : 356 V rr V rr ' -~ir o &lt; ~r A vl . Tr.L L A -~'tr o &lt; _Tr ' A vl . lr ' J~ L : =~ their context and bound outside their con~_.~. : y , . `` text must be bound by the same binder : , -- ~'~. I~-~ v , ,w- ( , ,o &gt; , , /- ' % : ; *-1 x. , ( AL ( Vl .</sentence>
				<definiendum id="0">A-structure L</definiendum>
				<definiendum id="1">parallelism</definiendum>
				<definiendum id="2">AL</definiendum>
				<definiens id="0">' ) = ' , for anaphor-toantecedent linking. We assume that the following conditions hold : ( 1 ) binding only holds between variables ( nodes labelled var ) to A-binders ( nodes labelled lain</definiens>
				<definiens id="1">dominated by their binders ; ( 4 ) only anaphors ( nodel labelled ana ) are linked to antecendents</definiens>
				<definiens id="2">the binder of that variable. The not so straight forward part of the semantics of CLLS is the notion of</definiens>
				<definiens id="3">=~ their context and bound outside their con~_.~. : y , . `` text must be bound by the same binder : , -- ~'~. I~-~ v</definiens>
			</definition>
			<definition id="6">
				<sentence>~/sue * _X CLLS allows a uniform and yet internally structured approach to semantic ambiguity .</sentence>
				<definiendum id="0">~/sue * _X CLLS</definiendum>
				<definiens id="0">allows a uniform and yet internally structured approach to semantic ambiguity</definiens>
			</definition>
</paper>

	</volume>
