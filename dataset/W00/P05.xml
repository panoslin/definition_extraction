<?xml version="1.0" encoding="UTF-8"?>
	<volume id="P05">

		<paper id="3014">
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>• Training data T is a set of input/output pairs , e.g. , T = { 〈x1 , y1〉 , ... , 〈xL , yL〉 } , where xi is an input sentence , and yi is a correct parse associated with the sentence xi .</sentence>
				<definiendum id="0">xi</definiendum>
				<definiendum id="1">yi</definiendum>
				<definiens id="0">a set of input/output pairs</definiens>
				<definiens id="1">an input sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>We incorporate the log probability into the reranking by using it as a feature : Φ ( y ) = { L ( y ) , I ( t1 ⊆ y ) , ... , I ( tm ⊆ y ) } , and w = { w0 , w1 , w2 , ... , wm } , where L ( y ) is the log probability of a tree y under the base parser and w0 is the parameter of L ( y ) .</sentence>
				<definiendum id="0">y )</definiendum>
				<definiendum id="1">w0</definiendum>
				<definiens id="0">⊆ y ) , ... , I ( tm ⊆ y ) } , and w = { w0 , w1 , w2 , ... , wm }</definiens>
				<definiens id="1">the log probability of a tree y under the base parser and</definiens>
				<definiens id="2">the parameter of L ( y )</definiens>
			</definition>
			<definition id="2">
				<sentence>The 193 TOP S ( saw ) NP ( I ) PRP I VP ( saw ) VBD saw NP ( girl ) DT a NN girl Figure 3 : Lexicalized CFG tree for WSJ parsing head word , e.g. , ( saw ) , is put as a leftmost constituent size parameter s and frequency parameter f were experimentally set at 6 and 10 , respectively .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">Lexicalized CFG tree for WSJ parsing head word</definiens>
			</definition>
			<definition id="3">
				<sentence>CBs is the average number of cross brackets per sentence .</sentence>
				<definiendum id="0">CBs</definiendum>
				<definiens id="0">the average number of cross brackets per sentence</definiens>
			</definition>
			<definition id="4">
				<sentence>Tree kernel uses the all-subtrees representation not explicitly but implicitly by reducing the problem to the calculation of the inner-products of two trees .</sentence>
				<definiendum id="0">Tree kernel</definiendum>
			</definition>
			<definition id="5">
				<sentence>195 WSJ parsing w active trees that contain the word “in” ... ... -1.1217 ( PP ( in ) ( NP ( NP ( effect ) ) ) ) -1.1634 ( VP ( yield ) ( PP ( PP ) ) ( PP ( in ) ) ) -1.3574 ( NP ( PP ( in ) ( NP ( NN ( way ) ) ) ) ) -1.8030 ( NP ( PP ( in ) ( NP ( trading ) ( JJ ) ) ) ) shallow parsing w active trees that contain the phrase “SBAR” .</sentence>
				<definiendum id="0">PP</definiendum>
				<definiendum id="1">PP</definiendum>
			</definition>
			<definition id="6">
				<sentence>In the shallow parsing task , O is a special phrase that means “out of chunk” .</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">a special phrase that means “out of chunk”</definiens>
			</definition>
</paper>

		<paper id="2021">
			<definition id="0">
				<sentence>In case of Katz back-off model , the conditional bigram word probability is estimated as a3a5a4a7a6a9a8a11a10a13a12a8a11a10a9a14a15a4a17a16a19a18a21a20a23a22 a3a24a6a9a8 a10 a12a8 a10a9a14a15a4 a16 if a25 a6a9a8 a10a9a14a15a4a27a26 a8 a10 a16a29a28a31a30 a32a34a33a34a6a9a8 a10a9a14a15a4 a16a36a35 a22 a3a24a6a9a8 a10 a16 otherwise ( 1 ) where a22a3 represents a smoothed probability distribution , a32a34a33a34a6a37a16 stands for the back-off weight , and a25 a6a39a38a40a16 denotes the count of its argument .</sentence>
				<definiendum id="0">a22a3</definiendum>
				<definiendum id="1">a25 a6a39a38a40a16</definiendum>
				<definiens id="0">a smoothed probability distribution</definiens>
			</definition>
			<definition id="1">
				<sentence>a3a24a6a9a8a11a10a55a12a8a11a10a9a14a15a4a17a16a56a18a21a20 a3 a4 a6a9a8 a10 a12a8 a10a9a14a15a4 a16 a8 a10a58a57a60a59 a32a61a33a24a6a9a8a11a10a9a14a15a4a51a16a58a35a7a62a34a6a9a8a11a10a37a16a63a8a11a10 a57a65a64 ( 2 ) a3a5a4a7a6a37a16 refers to the regular back-off model , a59 denotes the regular dictionary from which the back-off model was estimated , a64 is the supplementary dictionary which does not overlap with a59 .</sentence>
				<definiendum id="0">a59</definiendum>
				<definiendum id="1">a64</definiendum>
				<definiens id="0">the regular back-off model</definiens>
				<definiens id="1">the regular dictionary from which the back-off model was estimated</definiens>
				<definiens id="2">the supplementary dictionary which does not overlap with a59</definiens>
			</definition>
			<definition id="2">
				<sentence>The Czech Broadcast News ( Radov·a et al. , 2004 ) is a collection of both radio and TV news in Czech .</sentence>
				<definiendum id="0">Czech Broadcast News</definiendum>
				<definiens id="0">a collection of both radio and TV news in Czech</definiens>
			</definition>
			<definition id="3">
				<sentence>The test set consists of 2500 sentences .</sentence>
				<definiendum id="0">test set</definiendum>
			</definition>
			<definition id="4">
				<sentence>G denotes a language model compiled as a nite-state automaton .</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">a language model compiled as a nite-state automaton</definiens>
			</definition>
			<definition id="5">
				<sentence>CLG denotes transducer mapping triphones to words augmented with model scores .</sentence>
				<definiendum id="0">CLG</definiendum>
			</definition>
			<definition id="6">
				<sentence>Morph refers to the dictionary generated by the morphology tools from from TR .</sentence>
				<definiendum id="0">Morph</definiendum>
				<definiens id="0">the dictionary generated by the morphology tools from from TR</definiens>
			</definition>
			<definition id="7">
				<sentence>Inj denotes the loop of the rest of words of the CNC corpus and the morphologygenerated words .</sentence>
				<definiendum id="0">Inj</definiendum>
				<definiens id="0">the loop of the rest of words of the CNC corpus and the morphologygenerated words</definiens>
			</definition>
			<definition id="8">
				<sentence>G refers to a language model compiled into an automaton , CLG denotes triphone-to-word transducer .</sentence>
				<definiendum id="0">G</definiendum>
				<definiendum id="1">CLG</definiendum>
				<definiens id="0">a language model compiled into an automaton</definiens>
				<definiens id="1">triphone-to-word transducer</definiens>
			</definition>
			<definition id="9">
				<sentence>Morph represents the loop of words generated by morphology .</sentence>
				<definiendum id="0">Morph</definiendum>
				<definiens id="0">the loop of words generated by morphology</definiens>
			</definition>
			<definition id="10">
				<sentence>Inj is the loop of all words from CNC which were not included in CNC language model , moreover , Inj also contains words generated by the morphology .</sentence>
				<definiendum id="0">Inj</definiendum>
				<definiens id="0">the loop of all words from CNC which were not included in CNC language model , moreover , Inj also contains words generated by the morphology</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>The conventional Question Answering ( QA ) architecture is a cascade of the following building blocks : Question Analyzer analyzes a question sentence and identi es the question types ( or answer types ) .</sentence>
				<definiendum id="0">conventional Question Answering ( QA ) architecture</definiendum>
				<definiens id="0">a cascade of the following building blocks : Question Analyzer analyzes a question sentence and identi es the question types</definiens>
			</definition>
			<definition id="1">
				<sentence>Answer Selector ranks the answer candidates according to the syntactic and semantic conformity of each answer with the question and its context in the document .</sentence>
				<definiendum id="0">Answer Selector</definiendum>
				<definiens id="0">ranks the answer candidates according to the syntactic and semantic conformity of each answer with the question</definiens>
			</definition>
			<definition id="2">
				<sentence>Typically , question types consist of named entities , e.g. , PERSON , DATE , and ORGANIZATION , numerical expressions , e.g. , LENGTH , WEIGHT , SPEED , and class names , e.g. , FLOWER , BIRD , and FOOD .</sentence>
				<definiendum id="0">question types</definiendum>
				<definiens id="0">consist of named entities , e.g. , PERSON , DATE , and ORGANIZATION , numerical expressions , e.g. , LENGTH , WEIGHT , SPEED , and class names , e.g. , FLOWER , BIRD , and FOOD</definiens>
			</definition>
			<definition id="3">
				<sentence>A sample ( x , y ) is a pair of input x= { x1 , . . . , xm } ( xi ∈ X ) and output y ∈ Y. 1Presently , National Institute of Information and Communications Technology ( NICT ) , Japan 2A machine learning approach to hierarchical question analysis was reported in ( Suzuki et al. , 2003 ) , but training and maintaining an answer extractor for question types of ne granularity is not an easy task .</sentence>
				<definiendum id="0">sample</definiendum>
				<definiens id="0">a pair of input x= { x1 , . . . , xm } ( xi ∈ X</definiens>
			</definition>
			<definition id="4">
				<sentence>216 The Maximum Entropy Principle ( Berger et al. , 1996 ) is to nd a model p∗ = argmax p∈C H ( p ) , which means a probability model p ( y|x ) that maximizes entropy H ( p ) .</sentence>
				<definiendum id="0">Maximum Entropy Principle</definiendum>
				<definiens id="0">means a probability model p ( y|x ) that maximizes entropy H ( p )</definiens>
			</definition>
			<definition id="5">
				<sentence>Question Answering in the QBTE Model 1 involves directly classifying words wi in the document into answer words or non-answer words .</sentence>
				<definiendum id="0">Question Answering</definiendum>
				<definiens id="0">in the QBTE Model 1 involves directly classifying words wi in the document into answer words or non-answer words</definiens>
			</definition>
			<definition id="6">
				<sentence>The training phase estimates a probabilistic model from training data ( x ( 1 ) , y ( 1 ) ) , ... , ( x ( n ) , y ( n ) ) generated from the CRL QA Data .</sentence>
				<definiendum id="0">training phase</definiendum>
			</definition>
			<definition id="7">
				<sentence>The execution phase evaluates the probability of yprime ( i ) given inputxprime ( i ) using the the probabilistic model .</sentence>
				<definiendum id="0">execution phase</definiendum>
				<definiens id="0">evaluates the probability of yprime ( i ) given inputxprime ( i ) using the the probabilistic model</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>Knott uses an empirical methodology to determine the substitutability of pairs of connectives .</sentence>
				<definiendum id="0">Knott</definiendum>
				<definiens id="0">uses an empirical methodology to determine the substitutability of pairs of connectives</definiens>
			</definition>
			<definition id="1">
				<sentence>The Kullback-Leibler ( KL ) divergence function is a distributional similarity function that is of particular relevance here since it can be described informally in terms of substitutability .</sentence>
				<definiendum id="0">Kullback-Leibler ( KL ) divergence function</definiendum>
				<definiens id="0">a distributional similarity function that is of particular relevance here since it can be described informally in terms of substitutability</definiens>
			</definition>
			<definition id="2">
				<sentence>The co-occurrence data were used to construct a Gaussian classifier , by assuming the values for D and V are generated by Gaussians.2 First , normal functions were used to calculate the likelihood ratio of p and q being in the two relationships : P ( syn|data ) P ( hyp|data ) = P ( syn ) P ( hyp ) · P ( data|syn ) P ( data|hyp ) ( 8 ) = 1·n ( max ( D1 , D2 ) ; µsyn , σsyn ) n ( max ( D 1 , D2 ) ; µhyp , σhyp ) ( 9 ) 2KL divergence is right skewed , so a log-normal model was used to model D , whereas a normal model used for V .</sentence>
				<definiendum id="0">σsyn ) n</definiendum>
				<definiens id="0">by assuming the values for D and V are generated by Gaussians.2 First , normal functions were used to calculate the likelihood ratio of p and q being in the two relationships : P ( syn|data</definiens>
			</definition>
			<definition id="3">
				<sentence>Input to Gaussian SYN vs SYN/HYP vs Model HYP EX/CONT max ( D1 , D2 ) 50.0 % 76.1 % max ( V1 , V2 ) 84.8 % 60.6 % Table 4 : Accuracy on pseudodisambiguation task where n ( x ; µ , σ ) is the normal function with mean µ and standard deviation σ , and where µsyn , for example , denotes the mean of the Gaussian model for SYNONYMY .</sentence>
				<definiendum id="0">Gaussian SYN</definiendum>
				<definiendum id="1">µ , σ )</definiendum>
				<definiens id="0">vs SYN/HYP vs Model HYP EX/CONT max</definiens>
				<definiens id="1">the normal function with mean µ and standard deviation σ</definiens>
				<definiens id="2">the mean of the Gaussian model for SYNONYMY</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>Edit distance ( ED ) is a common measure of the similarity between two strings .</sentence>
				<definiendum id="0">Edit distance</definiendum>
				<definiendum id="1">ED )</definiendum>
				<definiens id="0">a common measure of the similarity between two strings</definiens>
			</definition>
			<definition id="1">
				<sentence>sji is the substring si ... sj and sji is equal to the empty string , epsilon1 , when i &gt; j. Likewise , tn1 denotes a target string over a target alphabet B , and n the length of tn1 .</sentence>
				<definiendum id="0">sji</definiendum>
				<definiendum id="1">tn1</definiendum>
				<definiens id="0">the substring si ... sj and sji is equal to the empty string , epsilon1 , when i &gt; j. Likewise ,</definiens>
				<definiens id="1">a target string over a target alphabet B , and n the length of tn1</definiens>
			</definition>
			<definition id="2">
				<sentence>We write 〈s , t〉 ( ( s , t ) negationslash= ( epsilon1 , epsilon1 ) ) to denote an edit operation in which the symbol s is replaced by t. If s=epsilon1 and tnegationslash=epsilon1 , 〈s , t〉 is an insertion .</sentence>
				<definiendum id="0">t〉</definiendum>
				<definiens id="0">s , t ) negationslash= ( epsilon1 , epsilon1 ) ) to denote an edit operation in which the symbol s is replaced by t. If s=epsilon1 and tnegationslash=epsilon1 , 〈s ,</definiens>
			</definition>
			<definition id="3">
				<sentence>A O ( m · n ) Dynamic Programming ( DP ) algorithm exists to compute the ED between two strings .</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">m · n ) Dynamic Programming ( DP ) algorithm exists to compute the ED between two strings</definiens>
			</definition>
			<definition id="4">
				<sentence>P ( Zi=zi , sm1 , tn1 |θ ) , 1 &lt; i &lt; lscript , where zi=〈z ( s ) i , z ( t ) i 〉 , in the form Q ( zi ) ∝     fins ( tbi ) for z ( s ) i =epsilon1 ; z ( t ) i =tbi fdel ( sai ) for z ( s ) i =sai ; z ( t ) i =epsilon1 fsub ( sai , tbi ) for ( z ( s ) i , z ( t ) i ) = ( sai , tbi ) 0 otherwise ( 2 ) where summationtextz Q ( z ) = 1 ; ai =summationtexti−1j=1 1 { z ( s ) j negationslash=epsilon1 } ( resp. bi ) is the index of the source ( resp. target ) string generated up to the ith edit operation ; and fins , fdel , and fsub are functions mapping to [ 0,1 ] .2 Context independence is not to be taken here to mean Zi does not depend on sai or tbi. It depends on them through the global context which forces Zlscript1 to generate ( sm1 , tn1 ) . The RY model is memoryless and context-independent ( MCI ) . Equation 2 , also implicitly enforces the consistency constraint that the pair of symbols output , ( z ( s ) i , z ( t ) i ) , agrees with the actual pair of symbols , ( sai , tbi ) , that needs to be generated at step i in order for the total yield , v ( zlscript1 ) , to equal the string pair. The RY stochastic model is similar to the one introduced earlier by Bahl and Jelinek ( 1975 ) . The difference is that the Bahl model is memoryless and context-dependent ( MCD ) ; the f functions are now indexed by sai ( or tai , or both ) such thatsummationtext z Qsai ( z ) = 1 ∀sai. In general , context dependence can be extended to include up to the whole source ( and/or target ) string , sai−11 , sai , smai+1. Several other types of dependence can be exploited as will be discussed in section 3. Both the Ristad and the Bahl transducer models give exponentially smaller probability to longer strings and edit sequences. Ristad presents an alternate explicit model of the joint probability of the length of the source and target strings. In this parametrization the probability of the length of an edit sequence does not necessarily decrease geometrically. A similar effect can be achieved by modeling the length of the hidden edit sequence explicitly ( see section 3 ) . Dynamic Bayesian Networks ( DBNs ) , of which Hidden Markov Models ( HMMs ) are the most fa2By convention , sa i = epsilon1 for ai &gt; m. Likewise , tbi = epsilon1 if bi &gt; n. fins ( epsilon1 ) = fdel ( epsilon1 ) = fsub ( epsilon1 , epsilon1 ) = 0 .</sentence>
				<definiendum id="0">zi=〈z</definiendum>
				<definiens id="0">the index of the source ( resp. target ) string generated up to the ith edit operation ; and fins , fdel , and fsub are functions mapping to [ 0,1 ] .2 Context independence is not to be taken here to mean Zi does not depend on sai or tbi. It depends on them through the global context which forces Zlscript1 to generate</definiens>
				<definiens id="1">and/or target ) string , sai−11 , sai</definiens>
				<definiens id="2">of which Hidden Markov Models ( HMMs</definiens>
			</definition>
			<definition id="5">
				<sentence>The cardinality of H determines how much the information from one frame to the other is “summarized.”</sentence>
				<definiendum id="0">cardinality of H</definiendum>
			</definition>
			<definition id="6">
				<sentence>With a deterministic implementation , we can , for example , specify the usual P ( Zi|Zi−1 ) memory model when H is a simple copy of Z or have Zi depend on the type of edit operation in the previous frame .</sentence>
				<definiendum id="0">H</definiendum>
				<definiens id="0">a simple copy of Z or have Zi depend on the type of edit operation in the previous frame</definiens>
			</definition>
			<definition id="7">
				<sentence>C ( w ) denotes the set of canonical pronunciations of w. ˆW = argmax w summationdisplay sm1 ∈C ( w ) P ( w|sm1 ) P ( sm1 , tn1 ) ( 3 ) If we assume uniform probabilities P ( w|sm1 ) ( sm1 ∈C ( w ) ) and use the max approximation in place of the sum in eq .</sentence>
				<definiendum id="0">C ( w )</definiendum>
				<definiens id="0">the set of canonical pronunciations of w. ˆW = argmax w summationdisplay</definiens>
				<definiens id="1">the max approximation in place of the sum in eq</definiens>
			</definition>
			<definition id="8">
				<sentence>Switchboard consists of spontaneous informal conversations recorded over the 342 phone .</sentence>
				<definiendum id="0">Switchboard</definiendum>
			</definition>
			<definition id="9">
				<sentence>Another source of pronunciation variability is the noise introduced during the annotation of speech segments .</sentence>
				<definiendum id="0">pronunciation variability</definiendum>
				<definiens id="0">the noise introduced during the annotation of speech segments</definiens>
			</definition>
			<definition id="10">
				<sentence>The direct model , for example , is quite fast in practice because we can restrict the cardinality of the del RV to a constant c ( i.e. we disallow long-span deletions , which for certain applications is a reasonable restriction ) and make inference linear in n with a running time constant proportional to c2 .</sentence>
				<definiendum id="0">direct model</definiendum>
				<definiens id="0">a reasonable restriction</definiens>
			</definition>
</paper>

		<paper id="1001">
</paper>

		<paper id="2014">
			<definition id="0">
				<sentence>A statistical discourse model is trained and then used to classify dialogue acts based on the observed words in an utterance .</sentence>
				<definiendum id="0">statistical discourse model</definiendum>
				<definiens id="0">used to classify dialogue acts based on the observed words in an utterance</definiens>
			</definition>
			<definition id="1">
				<sentence>The first level is a Turn , consisting of at least one Message , which consists of at least one Utterance , defined as follows : Turn : Dialogue participants normally take turns writing .</sentence>
				<definiendum id="0">first level</definiendum>
				<definiens id="0">a Turn , consisting of at least one Message , which consists of at least one Utterance , defined as follows : Turn : Dialogue participants normally take turns writing</definiens>
			</definition>
			<definition id="2">
				<sentence>Message : A message is defined as a group of words that are sent from one dialogue participant to the other as a single unit .</sentence>
				<definiendum id="0">Message</definiendum>
				<definiendum id="1">message</definiendum>
				<definiens id="0">a group of words that are sent</definiens>
			</definition>
			<definition id="3">
				<sentence>To achieve this goal , we implemented a naive Bayes classifier using bag-of-words feature representation such that the most probable dialogue act ˆd given a bag-ofwords input vector ¯v is taken to be : ˆd = argmax d∈D P ( ¯v|d ) P ( d ) P ( ¯v ) ( 1 ) P ( ¯v|d ) ≈ nproductdisplay j=1 P ( vj|d ) ( 2 ) ˆd = argmax d∈D P ( d ) nproductdisplay j=1 P ( vj|d ) ( 3 ) where vj is the jth element in ¯v , D denotes the set of all dialogue acts and P ( ¯v ) is constant for all d∈D. The use of P ( d ) in Equation 3 assumes that dialogue acts are independent of one another .</sentence>
				<definiendum id="0">vj</definiendum>
				<definiendum id="1">D</definiendum>
				<definiens id="0">the jth element in ¯v</definiens>
				<definiens id="1">the set of all dialogue acts and P ( ¯v ) is constant for all d∈D. The use of P ( d ) in Equation 3 assumes that dialogue acts are independent of one another</definiens>
			</definition>
			<definition id="4">
				<sentence>The bigram model uses the posterior probability P ( d|H ) rather than the prior probability P ( d ) in Equation 3 , where H is the n-gram context vector containing the previous dialogue act or previous 2 dialogue acts in the case of the trigram model .</sentence>
				<definiendum id="0">H</definiendum>
				<definiens id="0">the n-gram context vector containing the previous dialogue act or previous 2 dialogue acts in the case of the trigram model</definiens>
			</definition>
			<definition id="5">
				<sentence>For example , IM provides the ability for an automated agent to ask clarification questions .</sentence>
				<definiendum id="0">IM</definiendum>
				<definiens id="0">provides the ability for an automated agent to ask clarification questions</definiens>
			</definition>
</paper>

		<paper id="3004">
			<definition id="0">
				<sentence>The document exploration functionality includes identification of semantically similar concepts and dynamic ontology creation .</sentence>
				<definiendum id="0">document exploration functionality</definiendum>
				<definiens id="0">includes identification of semantically similar concepts and dynamic ontology creation</definiens>
			</definition>
			<definition id="1">
				<sentence>CL Research uses the Proximity Parser , developed by an inventor of top-down syntaxdirected parsing ( Irons , 1961 ) .3 The parser output consists of bracketed parse trees , with leaf nodes describing the part of speech and lexical entry for each sentence word .</sentence>
				<definiendum id="0">CL Research</definiendum>
				<definiens id="0">uses the Proximity Parser , developed by an inventor of top-down syntaxdirected parsing</definiens>
				<definiens id="1">consists of bracketed parse trees , with leaf nodes describing the part of speech and lexical entry for each sentence word</definiens>
			</definition>
			<definition id="2">
				<sentence>XML Analyzer includes the document number , the sentence number , and the full sentence for each noun phrase .</sentence>
				<definiendum id="0">XML Analyzer</definiendum>
				<definiens id="0">includes the document number , the sentence number</definiens>
			</definition>
			<definition id="3">
				<sentence>An XPath expression consists of two parts .</sentence>
				<definiendum id="0">XPath expression</definiendum>
			</definition>
			<definition id="4">
				<sentence>KMS is designed to identify relevant documents , to build a repository of these documents , to explore the documents , and to extract relevant pieces of information .</sentence>
				<definiendum id="0">KMS</definiendum>
				<definiens id="0">designed to identify relevant documents , to build a repository of these documents , to explore the documents , and to extract relevant pieces of information</definiens>
			</definition>
</paper>

		<paper id="2010">
</paper>

		<paper id="3007">
			<definition id="0">
				<sentence>UIMA is a software architecture that defines roles , interface , and communications of components for natural language processing .</sentence>
				<definiendum id="0">UIMA</definiendum>
				<definiens id="0">a software architecture that defines roles , interface , and communications of components for natural language processing</definiens>
			</definition>
			<definition id="1">
				<sentence>The context free tokenizer is a finite state transducer that parses the document text into the smallest meaningful spans of text .</sentence>
				<definiendum id="0">context free tokenizer</definiendum>
				<definiens id="0">a finite state transducer that parses the document text into the smallest meaningful spans of text</definiens>
			</definition>
			<definition id="2">
				<sentence>A token is a set of characters that can be classified into one of these categories : word , punctuation , number , contraction , possessive , symbol without taking into account any additional context .</sentence>
				<definiendum id="0">token</definiendum>
				<definiens id="0">a set of characters that can be classified into one of these categories : word , punctuation , number , contraction , possessive , symbol without taking into account any additional context</definiens>
			</definition>
			<definition id="3">
				<sentence>The shallow parser uses a set of rules operating on tokens and their part-ofspeech category to identify linguistic phrases in the text such as noun phrases , verb phrases , and adjectival phrases .</sentence>
				<definiendum id="0">shallow parser</definiendum>
				<definiens id="0">uses a set of rules operating on tokens and their part-ofspeech category to identify linguistic phrases in the text such as noun phrases , verb phrases , and adjectival phrases</definiens>
			</definition>
			<definition id="4">
				<sentence>The dictionary named entity annotator uses a set of enriched dictionaries ( SNOMED-CT , MeSH , RxNorm and Mayo Synonym Clusters ( MSC ) to lookup named entities in the document text .</sentence>
				<definiendum id="0">dictionary named entity annotator</definiendum>
			</definition>
			<definition id="5">
				<sentence>The ML ( Machine Learning ) Named Entity annotator is based on a Naïve Bayes classifier trained on a combination of the UMLS entry terms and the MCS where each diagnostic statement is represented as a bag-of-words and used as a training sample for generating a Naive Bayes classifier which assigns MCS id’s to noun phrases identified in the text of clinical notes .</sentence>
				<definiendum id="0">ML</definiendum>
				<definiens id="0">a bag-of-words and used as a training sample for generating a Naive Bayes classifier which assigns MCS id’s to noun phrases identified in the text of clinical notes</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>The BNC annotation includes tagging of units approximating to sentences , as identified by the CLAWS segmentation scheme ( Garside , 1987 ) .</sentence>
				<definiendum id="0">BNC annotation</definiendum>
				<definiens id="0">includes tagging of units approximating to sentences</definiens>
			</definition>
			<definition id="1">
				<sentence>Cynthia : A thousand pounds .</sentence>
				<definiendum id="0">Cynthia</definiendum>
				<definiens id="0">A thousand pounds</definiens>
			</definition>
			<definition id="2">
				<sentence>[ BNC , G4X ] Long distance short answers primarily a multilogue effect Table 4 shows the total number of short answers found in dialogue and multilogue respectively , and the proportions sorted by distance over those totals : From this it emerges that short answers are more common in multilogue than in dialogue—134 ( 71 % ) v. 2Acknowledgements and acceptances are , in principle , distinct acts : the former involves indication that an utterance has been understood , whereas the latter that an assertion is accepted .</sentence>
				<definiendum id="0">distance short answers</definiendum>
				<definiens id="0">the total number of short answers found in dialogue and multilogue respectively , and the proportions sorted by distance over those totals</definiens>
			</definition>
			<definition id="3">
				<sentence>The general pattern involves a question being asked by the tutor or session leader , the other participants then taking turns to answer that question .</sentence>
				<definiendum id="0">general pattern</definiendum>
				<definiens id="0">involves a question being asked by the tutor or session leader</definiens>
			</definition>
			<definition id="4">
				<sentence>MLDSA and MAG have a somewhat different status : whereas MLDSA is a direct generalization from the data , MAG is a negative constraint , posited given the paucity of positive instances .</sentence>
				<definiendum id="0">MLDSA</definiendum>
				<definiendum id="1">MAG</definiendum>
				<definiens id="0">a direct generalization from the data</definiens>
				<definiens id="1">a negative constraint , posited given the paucity of positive instances</definiens>
			</definition>
			<definition id="5">
				<sentence>The querying/assertion protocols ( in their most basic form ) are summarized as follows : ( 6 ) querying assertion LatestMove = Ask ( A , q ) LatestMove = Assert ( A , p ) A : push q onto QUD ; A : push p ?</sentence>
				<definiendum id="0">querying/assertion protocols</definiendum>
				<definiens id="0">in their most basic form ) are summarized as follows : ( 6 ) querying assertion LatestMove = Ask ( A , q ) LatestMove = Assert ( A , p ) A : push q onto QUD</definiens>
			</definition>
			<definition id="6">
				<sentence>from QUD ; Following ( Larsson , 2002 ; Cooper , 2004 ) , one can 3In other words , pushed onto the stack , if one assumes QUD is a stack .</sentence>
				<definiendum id="0">QUD</definiendum>
				<definiens id="0">a stack</definiens>
			</definition>
			<definition id="7">
				<sentence>( a ) Addressee B tries to anchor the contextual parameters of T. If successful , B acknowledges u ( directly , gesturally or implicitly ) and responds to the content of u. ( b ) If unsuccessful , B poses a Clarification Request ( CR ) , that arises via utterance coercion ( see ( Ginzburg and Cooper , 2001 ) ) .</sentence>
				<definiendum id="0">B poses a Clarification Request</definiendum>
				<definiens id="0">tries to anchor the contextual parameters of T. If successful , B acknowledges u ( directly , gesturally or implicitly</definiens>
			</definition>
			<definition id="8">
				<sentence>They keep track of facts concerning a particular interaction , but their context is not facilitated for them to participate : ( 7 ) Given a dialogue protocolpi , add roles C1 , . . . , Cn where each Ci is a silent participant : given an utterance u0 classified as being of type T0 , Ci updates Ci .</sentence>
				<definiendum id="0">Ci</definiendum>
				<definiens id="0">a silent participant : given an utterance u0 classified as being of type T0</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>Statistical machine translation ( SMT ) has an advantage over many other statistical natural language processing applications in that training data is regularly produced by other human activity .</sentence>
				<definiendum id="0">SMT</definiendum>
				<definiens id="0">an advantage over many other statistical natural language processing applications in that training data is regularly produced by other human activity</definiens>
			</definition>
			<definition id="1">
				<sentence>The Linguistics Data Consortium provides an excellent set of off the shelf Arabic-English and Chinese-English parallel corpora for the annual NIST machine translation evaluation exercises .</sentence>
				<definiendum id="0">Linguistics Data Consortium</definiendum>
				<definiens id="0">provides an excellent set of off the shelf Arabic-English and Chinese-English parallel corpora for the annual NIST machine translation evaluation exercises</definiens>
			</definition>
			<definition id="2">
				<sentence>But the distortion parameter is a poor explanation of word order .</sentence>
				<definiendum id="0">distortion parameter</definiendum>
				<definiens id="0">a poor explanation of word order</definiens>
			</definition>
			<definition id="3">
				<sentence>The decision concerning what length of phrases to store in the phrase table seems to boil down to a practical consideration : one must weigh the likelihood of retrieval against the memory needed to store longer phrases .</sentence>
				<definiendum id="0">retrieval</definiendum>
			</definition>
			<definition id="4">
				<sentence>Abstractly , a suffix array is an alphabetically-sorted list of all suffixes in a corpus , where a suffix is a substring running from each position in the text to the end .</sentence>
				<definiendum id="0">suffix array</definiendum>
				<definiens id="0">an alphabetically-sorted list of all suffixes in a corpus</definiens>
				<definiens id="1">a substring running from each position in the text to the end</definiens>
			</definition>
			<definition id="5">
				<sentence>The maximum likelihood estimator for the probability of a phrase is defined as p ( ¯f|¯e ) = count ( ¯f , ¯e ) summationtext ¯f count ( ¯f , ¯e ) ( 1 ) Where count ( ¯f , ¯e ) gives the total number of times the phrase ¯f was aligned with the phrase ¯e in the parallel corpus .</sentence>
				<definiendum id="0">maximum likelihood estimator for the probability of a phrase</definiendum>
				<definiens id="0">¯f , ¯e ) summationtext ¯f count ( ¯f , ¯e ) ( 1 ) Where count ( ¯f , ¯e ) gives the total number of times the phrase ¯f was aligned with the phrase ¯e in the parallel corpus</definiens>
			</definition>
			<definition id="6">
				<sentence>The computational complexity is therefore bounded by O ( 2log ( n ) ) where n is the length of the corpus .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the corpus</definiens>
			</definition>
			<definition id="7">
				<sentence>The complexity is O ( k ∗2log ( m ) ) where k is the number of occurrences of ¯e and m is the number of sentence pairs in the parallel corpus .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">the number of sentence pairs in the parallel corpus</definiens>
			</definition>
			<definition id="8">
				<sentence>That is : ˆe = argmax eI1 p ( eI1|fI1 ) ( 6 ) = argmax eI1 pLM ( eI1 ) ∗ ( 7 ) Iproductdisplay i=1 p ( ¯fi|¯ei ) d ( ai − bi−1 ) plw ( ¯fi|¯ei , a ) ( 8 ) Where pLM is a language model probability and d is a distortion probability which penalizes movement .</sentence>
				<definiendum id="0">pLM</definiendum>
			</definition>
</paper>

		<paper id="2024">
			<definition id="0">
				<sentence>JACY ( Siegel and Bender , 2002 ) is a hand-crafted Japanese HPSG grammar that provides semantic information as well as linguistically motivated analysis of complex constructions .</sentence>
				<definiendum id="0">JACY</definiendum>
				<definiens id="0">a hand-crafted Japanese HPSG grammar that provides semantic information as well as linguistically motivated analysis of complex constructions</definiens>
			</definition>
			<definition id="1">
				<sentence>The HEAD feature specifies phrasal categories , the MOD feature represents restrictions on the left and right modifiees , and the VAL feature encodes valence information .</sentence>
				<definiendum id="0">MOD feature</definiendum>
				<definiens id="0">represents restrictions on the left and right modifiees , and the VAL feature encodes valence information</definiens>
			</definition>
			<definition id="2">
				<sentence>Postpositional phrases ( PPs ) , which consist of postpositions and preceding phrases , are the most typical example of specifier-head structures .</sentence>
				<definiendum id="0">Postpositional phrases</definiendum>
				<definiendum id="1">PPs</definiendum>
				<definiens id="0">consist of postpositions and preceding phrases</definiens>
			</definition>
			<definition id="3">
				<sentence>ga/NOM kare/he ni/DAT korosa/kill re/PASSIVE ta/DECL’ ( I was killed by him ) , “korosa” takes a “ga”-marked PP as an object and a “ni”-marked PP as an agent , though without “ ( sa ) re , ” it takes a “ga”marked PP as an agent and a “wo”-marked PP as an object .</sentence>
				<definiendum id="0">ni/DAT korosa/kill re/PASSIVE ta/DECL’</definiendum>
				<definiens id="0">takes a “ga”-marked PP as an object and a “ni”-marked PP as an agent , though without “ ( sa ) re , ” it takes a “ga”marked PP as an agent and a “wo”-marked PP as an object</definiens>
			</definition>
			<definition id="4">
				<sentence>Parsing Strategies with ’Lexicalized’ Grammars : Application to Tree Adjoining Grammars .</sentence>
				<definiendum id="0">’Lexicalized’ Grammars</definiendum>
				<definiens id="0">Application to Tree Adjoining Grammars</definiens>
			</definition>
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>ACE ( Automatic Content Extraction ) 2 is a research and development program in information extraction sponsored by the U.S. Government .</sentence>
				<definiendum id="0">ACE ( Automatic Content Extraction</definiendum>
				<definiens id="0">a research and development program in information extraction sponsored by the U.S. Government</definiens>
			</definition>
			<definition id="1">
				<sentence>Each mention of an entity has a mention type : NAM ( proper name ) , NOM ( nominal ) or 1 Kambhatla also evaluated his system on the ACE relation detection task , but the results are reported for the 2003 task , which used different relations and different training and test data , and did not use hand-annotated entities , so they can not be readily compared to our results .</sentence>
				<definiendum id="0">NOM</definiendum>
				<definiens id="0">used different relations and different training and test data</definiens>
			</definition>
			<definition id="2">
				<sentence>In our approach , input text is preprocessed by the Charniak sentence parser ( including tokenization and POS tagging ) and the GLARF ( Meyers et al. , 2001 ) dependency analyzer produced by NYU .</sentence>
				<definiendum id="0">GLARF</definiendum>
				<definiens id="0">Meyers et al. , 2001 ) dependency analyzer produced by NYU</definiens>
			</definition>
			<definition id="3">
				<sentence>Based on treebank parsing , GLARF produces labeled deep dependencies between words ( syntactic relations such as logical subject and logical object ) .</sentence>
				<definiendum id="0">GLARF</definiendum>
				<definiens id="0">produces labeled deep dependencies between words ( syntactic relations such as logical subject and logical object )</definiens>
			</definition>
			<definition id="4">
				<sentence>A relation candidate R is defined as R = ( arg 1 , arg 2 , seq , link , path ) , where arg 1 and arg 2 are the two entity arguments which may be related ; seq= ( t 1 , t 2 , … , t n ) is a token vector that covers the arguments and intervening words ; link= ( t 1 , t 2 , … , t m ) is also a token vector , generated from seq and the parse tree ; path is a dependency path connecting arg 1 and arg 2 in the dependency graph produced by GLARF .</sentence>
				<definiendum id="0">relation candidate R</definiendum>
				<definiendum id="1">path</definiendum>
				<definiens id="0">R = ( arg 1 , arg 2 , seq , link , path ) , where arg 1 and arg 2 are the two entity arguments which may be related ; seq= ( t 1 , t 2 , … , t n ) is a token vector that covers the arguments and intervening words ; link= ( t 1 , t 2 , … , t m ) is also a token vector , generated from seq and the parse tree</definiens>
				<definiens id="1">a dependency path connecting arg 1 and arg 2 in the dependency graph produced by GLARF</definiens>
			</definition>
			<definition id="5">
				<sentence>A token T is defined as a string triple , T = ( word , pos , base ) , where word , pos and base are strings representing the word , part-of-speech and morphological base form of T. Entity is a token augmented with other attributes , E = ( tk , type , subtype , mtype ) , where tk is the token associated with E ; type , subtype and mtype are strings representing the entity type , subtype and mention type of E. The subtype contains more specific information about an entity .</sentence>
				<definiendum id="0">token T</definiendum>
				<definiendum id="1">tk</definiendum>
				<definiens id="0">a string triple , T = ( word , pos , base ) , where word , pos and base are strings representing the word , part-of-speech and morphological base form of T. Entity is a token augmented with other attributes , E = ( tk , type , subtype , mtype )</definiens>
				<definiens id="1">the token associated with E ; type , subtype and mtype are strings representing the entity type , subtype and mention type of E. The subtype contains more specific information about an entity</definiens>
			</definition>
			<definition id="6">
				<sentence>A dependency arc is ARC = ( w , dw , label , e ) , where w is the current token ; dw is a token connected by a dependency to w ; and label and e are the role label and direction of this dependency arc respectively .</sentence>
				<definiendum id="0">dependency arc</definiendum>
				<definiendum id="1">w</definiendum>
				<definiendum id="2">dw</definiendum>
				<definiens id="0">a token connected by a dependency to w</definiens>
			</definition>
			<definition id="7">
				<sentence>Finally , path is a vector of dependency arcs , path = ( arc 1 , ... , arc l ) , where l is the length of the path and arc i ( 1≤i≤l ) satisfies arc 1 .</sentence>
				<definiendum id="0">l</definiendum>
				<definiens id="0">a vector of dependency arcs , path = ( arc 1 , ... , arc l )</definiens>
				<definiens id="1">the length of the path</definiens>
			</definition>
			<definition id="8">
				<sentence>For example , PER-SOC is a relation mainly between two person entities , while PHYS can happen between any type of entity and a GPE or LOC entity .</sentence>
				<definiendum id="0">PER-SOC</definiendum>
				<definiens id="0">a relation mainly between two person entities</definiens>
			</definition>
			<definition id="9">
				<sentence>Using the notation just defined , we can write the two surface kernels as follows : 1 ) Argument kernel troopsareas controlled by A-POS OBJ arg 1 arg 2 SBJ OBJ path in seq link areas controlled by Syrian troops COMP 422 where K E is a kernel that matches two entities , K T is a kernel that matches two tokens .</sentence>
				<definiendum id="0">K E</definiendum>
				<definiendum id="1">K T</definiendum>
				<definiens id="0">a kernel that matches two entities</definiens>
				<definiens id="1">a kernel that matches two tokens</definiens>
			</definition>
			<definition id="10">
				<sentence>3 ) Link sequence kernel where min_len is the length of the shorter link sequence in R 1 and R 2 .</sentence>
				<definiendum id="0">min_len</definiendum>
				<definiens id="0">the length of the shorter link sequence in R 1 and R 2</definiens>
			</definition>
			<definition id="11">
				<sentence>Analyzers based on these resources can generate regularized semantic representations for lexically or syntactically related sentence structures .</sentence>
				<definiendum id="0">Analyzers</definiendum>
				<definiens id="0">based on these resources can generate regularized semantic representations for lexically or syntactically related sentence structures</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>Define n×n probability transition matrix Tij = P ( j → i ) = Wijsummationtextn k=1 Wkj , where Tij is the probability to jump from example xj to example xi .</sentence>
				<definiendum id="0">Tij</definiendum>
				<definiens id="0">the probability to jump from example xj to example xi</definiens>
			</definition>
			<definition id="1">
				<sentence>The upper moon consists of 9 points , while the lower moon consists of 13 points .</sentence>
				<definiendum id="0">upper moon</definiendum>
			</definition>
			<definition id="2">
				<sentence>Figure 2 ( b ) 2 ( f ) shows the convergence process of LP with t increasing from 1 to 100 .</sentence>
				<definiendum id="0">f )</definiendum>
				<definiens id="0">shows the convergence process of LP with t increasing from 1 to 100</definiens>
			</definition>
			<definition id="3">
				<sentence>Major is a baseline method in which they always choose the most frequent sense .</sentence>
				<definiendum id="0">Major</definiendum>
				<definiens id="0">a baseline method in which they always choose the most frequent sense</definiens>
			</definition>
			<definition id="4">
				<sentence>MB-D denotes monolingual bootstrapping with decision list as base classifier , MB-B represents monolingual bootstrapping with ensemble of Naive Bayes as base classifier , and BB is bilingual bootstrapping with ensemble of Naive Bayes as base classifier .</sentence>
				<definiendum id="0">MB-B</definiendum>
				<definiendum id="1">BB</definiendum>
				<definiens id="0">monolingual bootstrapping with decision list as base classifier</definiens>
				<definiens id="1">monolingual bootstrapping with ensemble of Naive Bayes as base classifier</definiens>
				<definiens id="2">bilingual bootstrapping with ensemble of Naive Bayes as base classifier</definiens>
			</definition>
			<definition id="5">
				<sentence>In their algorithm , c is the number of senses of ambiguous word , and b ( b = 15 ) is the number of examples added into classified data for each class in each iteration of bootstrapping .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">the number of examples added into classified data for each class in each iteration of bootstrapping</definiens>
			</definition>
			<definition id="6">
				<sentence>In ( Zhu and Ghahramani , 2002 ; Zhu et al. , 2003 ) , they suggested a label entropy criterion H ( YU ) for model selection , where Y is the label matrix learned by their semi-supervised algorithms .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">the label matrix learned by their semi-supervised algorithms</definiens>
			</definition>
			<definition id="7">
				<sentence>Another possible criterion for model selection is to measure the entropy of c × c inter-class distance matrix D calculated on labeled data ( denoted as H ( D ) ) , where Di , j represents the average distance between the ith class and the j-th class .</sentence>
				<definiendum id="0">model selection</definiendum>
				<definiendum id="1">Di , j</definiendum>
				<definiens id="0">to measure the entropy of c × c inter-class distance matrix D calculated on labeled data</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>Different metrics have been used , such as weighted Jaccard ( Grefenstette , 1994 ; Dagan , 2000 ) , cosine ( Ruge , 1992 ) , various information theoretic measures ( Lee , 1997 ) , and the widely cited and competitive ( see ( Weeds and Weir , 2003 ) ) measure of Lin ( 1998 ) for similarity between two words , w and v , defined as follows : , ) , ( ) , ( ) , ( ) , ( ) , ( ) ( ) ( ) ( ) ( ∈∈ ∩∈ + + = fvweightfwweight fvweightfwweight vwsim vFfwFf vFwFf Lin where F ( w ) and F ( v ) are the active features of the two words ( positive feature weight ) and the weight function is defined as MI .</sentence>
				<definiendum id="0">Different metrics</definiendum>
				<definiendum id="1">weight function</definiendum>
				<definiens id="0">follows : , ) , ( ) , ( ) , ( ) , ( ) , ( ) ( ) ( ) ( ) ( ∈∈ ∩∈ + + = fvweightfwweight fvweightfwweight vwsim vFfwFf vFwFf Lin where F ( w ) and F ( v ) are the active features of the two words ( positive feature weight ) and the</definiens>
			</definition>
			<definition id="1">
				<sentence>The lexical entailment prediction task of ( Geffet and Dagan , 2004 ) measures how many of the top ranking similarity pairs produced by the ∩∈= ) , ( ) ( ) ( ) , ( vwsimwNfWSvfwRFF , where sim ( w , v ) is an initial approximation of the similarity space by Lin’s measure , WS ( f ) is a set of words co-occurring with feature f , and N ( w ) is the set of the most similar words of w by Lin’s measure .</sentence>
				<definiendum id="0">sim</definiendum>
				<definiendum id="1">WS ( f )</definiendum>
				<definiendum id="2">N ( w )</definiendum>
				<definiens id="0">an initial approximation of the similarity space by Lin’s measure</definiens>
				<definiens id="1">a set of words co-occurring with feature f</definiens>
			</definition>
			<definition id="2">
				<sentence>Mutual Web-sampling Procedure : For each pair ( w , v ) and their k-subsets M ( w , v , k ) and M ( v , w , k ) execute : Search the web for sentences including v and a feature f from M ( w , v , k ) as “bag of words” , i. e. sentences where w and f appear in any distance and in either order .</sentence>
				<definiendum id="0">Mutual Web-sampling Procedure</definiendum>
				<definiens id="0">Search the web for sentences including v and a feature f from M ( w , v , k ) as “bag of words” , i. e. sentences where w and f appear in any distance and in either order</definiens>
			</definition>
</paper>

		<paper id="3015">
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>• TIMBL ( Tilburg Memory-Based Learner ) , ( Daelemans et al. , 2003 ) , which implements a memory-based learning algorithm ( IB1 ) which predicts the class of a test data point by looking at its distance to all examples from the training data , using some distance metric .</sentence>
				<definiendum id="0">TIMBL</definiendum>
				<definiens id="0">implements a memory-based learning algorithm ( IB1 ) which predicts the class of a test data point by looking at its distance to all examples from the training data , using some distance metric</definiens>
			</definition>
</paper>

		<paper id="2012">
			<definition id="0">
				<sentence>Assuming we translate from f to e , and defining ˜ei , ˜fj a certain source phrase and a target phrases ( sequences of contiguous words ) , the phrase translation model Pr ( ˜ei| ˜fj ) can be decomposed as : summationdisplay T Pr ( ˜ei|T , ˜fj ) Pr ( ˜Ei| ˜Fj , ˜fj ) Pr ( ˜Fj , ˜fj ) ( 1 ) where ˜Ei , ˜Fj are the generalized classes of the source and target phrases , respectively , and T = ( ˜Ei , ˜Fj ) is the pair of source and target classes used , which we call Tuple .</sentence>
				<definiendum id="0">a target phrases</definiendum>
				<definiens id="0">sequences of contiguous words ) , the phrase translation model Pr ( ˜ei| ˜fj</definiens>
			</definition>
			<definition id="1">
				<sentence>Preprocessing includes : 69 PP + V ( L=have ) { +RB } { +been } +V { G } V ( L=have ) { +not } +PP { +RB } { +been } +V { G } PP +V ( L=be ) { +RB } +VG V ( L=be ) { +not } +PP { +RB } +VG PP + MD ( L=will/would/ ... ) { +RB } +V MD ( L=will/would/ ... ) { +not } +PP { +RB } +V PP { +RB } +V V ( L=do ) { +not } +PP { +RB } +V V ( L=be ) { +not } +PP PP : Personal Pronoun V / MD / VG / RB : Verb / Modal / Gerund / Adverb ( PennTree Bank POS ) L : Lemma ( or base form ) { } / ( ) : optionality / instantiation Examples : leaves do you have did you come he has not attended have you ever been I will have she is going to be we would arrive Figure 1 : Some verb phrase detection rules and detected forms in English .</sentence>
				<definiendum id="0">instantiation Examples</definiendum>
				<definiens id="0">Some verb phrase detection rules and detected forms in English</definiens>
			</definition>
			<definition id="2">
				<sentence>Mathematically , they can be expressed thus : recall = |A ∩ S||S| , precision = |A ∩ P||A| AER = 1 − |A ∩ S| + |A ∩ P||A| + |S| 70 where A is the hypothesis alignment and S is the set of Sure links in the gold standard reference , and P includes the set of Possible and Sure links in the gold standard reference .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">includes the set of Possible and Sure links in the gold standard reference</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>In addition to being at least as good as the MAP classifier , the choice of the TOP kernel feature extractor is motivated by the minimization of the binary classification error of a linear classifier &lt; w , φˆθ ( z ) &gt; + b. Tsuda ( 2002 ) demonstrates that this error is closely related to the estimation error of the posterior probability P ( c=+1|z , θstar ) by the estimator g ( &lt; w , φˆθ ( z ) &gt; + b ) , where g is the sigmoid function g ( t ) = 1/ ( 1 + exp ( −t ) ) .</sentence>
				<definiendum id="0">g</definiendum>
			</definition>
			<definition id="1">
				<sentence>A complete parse consists of a sequence of these actions , d1 , ... , dm , such that performing d1 , ... , dm results in a complete phrase structure tree .</sentence>
				<definiendum id="0">complete parse</definiendum>
				<definiens id="0">consists of a sequence of these actions , d1 , ... , dm , such that performing d1 , ... , dm results in a complete phrase structure tree</definiens>
			</definition>
			<definition id="2">
				<sentence>The Voted Perceptron algorithm is an ensemble method for combining the various intermediate models which are produced during training a perceptron .</sentence>
				<definiendum id="0">Voted Perceptron algorithm</definiendum>
				<definiens id="0">an ensemble method for combining the various intermediate models which are produced during training a perceptron</definiens>
			</definition>
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>Arguments of a verb are labeled ARG0 to ARG5 , where ARG0 is the PROTO-AGENT , ARG1 is the PROTO-PATIENT , etc .</sentence>
				<definiendum id="0">ARG0</definiendum>
				<definiendum id="1">ARG1</definiendum>
			</definition>
			<definition id="1">
				<sentence>Using what is known as the ONE VS ALL classification strategy , n binary classifiers are trained , where n is number of semantic classes including a NULL class .</sentence>
				<definiendum id="0">the ONE VS ALL classification strategy</definiendum>
				<definiendum id="1">n binary classifiers</definiendum>
				<definiendum id="2">n</definiendum>
			</definition>
			<definition id="2">
				<sentence>We analyze the performance on three tasks : Argument Identification – This is the process of identifying the parsed constituents in the sentence that represent semantic arguments of a given predicate .</sentence>
				<definiendum id="0">Argument Identification – This</definiendum>
				<definiens id="0">the process of identifying the parsed constituents in the sentence that represent semantic arguments of a given predicate</definiens>
			</definition>
			<definition id="3">
				<sentence>Precision ( P ) , Recall ( R ) and F1 scores are given for the identification and combined tasks , and Classification Accuracy ( A ) for the classification task .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">Recall ( R ) and F1 scores are given for the identification and combined tasks , and Classification Accuracy ( A ) for the classification task</definiens>
			</definition>
			<definition id="4">
				<sentence>SINGLE CHARACTER PHRASE TAGS : Each phrase category is clustered to a category defined by the first character of the phrase label .</sentence>
				<definiendum id="0">SINGLE CHARACTER PHRASE TAGS</definiendum>
				<definiens id="0">Each phrase category is clustered to a category defined by the first character of the phrase label</definiens>
			</definition>
			<definition id="5">
				<sentence>FEATURE CONTEXT : Features for argument bearing constituents were added as features to the constituent being classified .</sentence>
				<definiendum id="0">FEATURE CONTEXT</definiendum>
				<definiens id="0">the constituent being classified</definiens>
			</definition>
			<definition id="6">
				<sentence>Minipar is a rule based dependency parser .</sentence>
				<definiendum id="0">Minipar</definiendum>
				<definiens id="0">a rule based dependency parser</definiens>
			</definition>
			<definition id="7">
				<sentence>POS PATH : This is the path from the predicate to the head word through the dependency tree connecting the part of speech of each node in the tree .</sentence>
				<definiendum id="0">POS PATH</definiendum>
				<definiens id="0">the path from the predicate to the head word through the dependency tree connecting the part of speech of each node in the tree</definiens>
			</definition>
			<definition id="8">
				<sentence>It has three values as ”before” , ”after” and ”-” ( for the predicate ) PATH : It defines a flat path between the token and the predicate CLAUSE BRACKET PATTERNS CLAUSE POSITION : A binary feature that identifies whether the token is inside or outside the clause containing the predicate HEADWORD SUFFIXES : suffixes of headwords of length 2 , 3 and 4 .</sentence>
				<definiendum id="0">HEADWORD SUFFIXES</definiendum>
				<definiens id="0">A binary feature that identifies whether the token is inside or outside the clause containing the predicate</definiens>
			</definition>
			<definition id="9">
				<sentence>LENGTH : the number of words in a token .</sentence>
				<definiendum id="0">LENGTH</definiendum>
				<definiens id="0">the number of words in a token</definiens>
			</definition>
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>| ( ) | , Pr ( ) | , ( 00 φφ φ φ πτ φφ πτ eef ( 3 ) 1 A cept is defined as the set of target words connected to a source word ( Brown et al. , 1993 ) .</sentence>
				<definiendum id="0">| ( ) | , Pr ( )</definiendum>
				<definiendum id="1">cept</definiendum>
			</definition>
			<definition id="1">
				<sentence>We also use a sentence-aligned in-domain English-Chinese bilingual corpus ( operation manuals for diagnostic ultrasound systems ) , which includes 5,862 bilingual sentence pairs .</sentence>
				<definiendum id="0">sentence-aligned in-domain English-Chinese bilingual corpus</definiendum>
				<definiens id="0">includes 5,862 bilingual sentence pairs</definiens>
			</definition>
			<definition id="2">
				<sentence>If we use to represent the set of alignment links identified by the proposed methods and to denote the reference alignment set , the methods to calculate the precision , recall , f-measure , and alignment error rate ( AER ) are shown in Equation ( 13 ) , ( 14 ) , ( 15 ) , and ( 16 ) .</sentence>
				<definiendum id="0">AER</definiendum>
				<definiens id="0">the set of alignment links identified by the proposed methods and to denote the reference alignment set , the methods to calculate the precision , recall , f-measure , and alignment error rate</definiens>
			</definition>
			<definition id="3">
				<sentence>G S C S |S| |SS| G CG ∩ =precision ( 13 ) 471 |S| |SS| C CG ∩ =recall ( 14 ) |||| ||2 CG CG SS SS fmeasure + ∩× = ( 15 ) fmeasure SS SS AER CG CG −= + ∩× −= 1 |||| ||2 1 ( 16 ) We use the held-out set described in section 6.1 to set the interpolation weights .</sentence>
				<definiendum id="0">G S C S |S| |SS| G CG ∩ =precision</definiendum>
				<definiens id="0">14 ) |||| ||2 CG CG SS SS fmeasure + ∩× = ( 15 ) fmeasure SS SS AER CG CG −= + ∩× −= 1 |||| ||2 1 ( 16 ) We use the held-out set described in section 6.1 to set the interpolation weights</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>MSR-MT ( Menezes &amp; Richardson , 01 ) parses both source and target languages to obtain a logical form ( LF ) , and translates source LFs using memorized aligned LF patterns to produce a target LF .</sentence>
				<definiendum id="0">MSR-MT</definiendum>
			</definition>
			<definition id="1">
				<sentence>That is , we make the following simplifying assumption ( where c is a function returning the set of nodes modifying t ) : ∏ ∈ = Tt TStcorderTSTorder ) , | ) ) ( ( P ( ) , | ) ( P ( Furthermore , we assume that the position of each child can be modeled independently in terms of a head-relative position : ) , | ) , ( P ( ) , | ) ) ( ( P ( ) ( TStmposTStcorder tcm ∏ ∈ = Figure 3a demonstrates an aligned dependency tree pair annotated with head-relative positions ; Figure 3b presents the same information in an alternate tree-like representation .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">a function returning the set of nodes modifying t ) : ∏ ∈ = Tt TStcorderTSTorder</definiens>
				<definiens id="1">a head-relative position : ) , | ) , ( P ( ) , | ) ) ( ( P ( ) ( TStmposTStcorder tcm ∏ ∈ = Figure 3a demonstrates an aligned dependency tree pair annotated with head-relative positions ; Figure 3b presents the same information in an alternate tree-like representation</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus we try to predict as follows : P ( pos ( m 1 ) = -1 | lex ( m 1 ) = '' la '' , lex ( h ) = '' propriété '' , lex ( src ( m 1 ) ) = '' the '' , lex ( src ( h ) = '' property '' , cat ( src ( m 1 ) ) =Determiner , cat ( src ( h ) ) =Noun , position ( src ( m 1 ) ) =-2 ) · P ( pos ( m 2 ) = +1 | lex ( m 2 ) = '' Cancel '' , lex ( h ) = '' propriété '' , lex ( src ( m 2 ) ) = '' Cancel '' , lex ( src ( h ) ) = '' property '' , cat ( src ( m 2 ) ) =Noun , cat ( src ( h ) ) =Noun , position ( src ( m 2 ) ) =-1 ) The training corpus acts as a supervised training set : we extract a training feature vector from each of the target language nodes in the aligned dependency tree pairs .</sentence>
				<definiendum id="0">P ( pos</definiendum>
				<definiendum id="1">lex ( src</definiendum>
				<definiendum id="2">cat ( src</definiendum>
				<definiendum id="3">position ( src</definiendum>
				<definiens id="0">a supervised training set : we extract a training feature vector from each of the target language nodes in the aligned dependency tree pairs</definiens>
			</definition>
			<definition id="3">
				<sentence>We define the set of all possible treelet translation pairs of the subtree rooted at each input node in the following manner : A treelet translation pair x is said to match the input dependency tree S iff there is some connected subgraph S’ that is identical to the source side of x. We say that x covers all the nodes in S’ and is rooted at source node s , where s is the root of matched subgraph S’ .</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">identical to the source side of x. We say that x covers all the nodes in S’ and is rooted at source node s</definiens>
			</definition>
			<definition id="4">
				<sentence>The resultant translation is the best scoring candidate for the root input node .</sentence>
				<definiendum id="0">resultant translation</definiendum>
				<definiens id="0">the best scoring candidate for the root input node</definiens>
			</definition>
			<definition id="5">
				<sentence>Note that if c is the count of children specified in the mapping and r is the count of subtrees translated via recursive calls , then there are ( c+r+1 ) !</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">the count of subtrees translated via recursive calls</definiens>
			</definition>
			<definition id="6">
				<sentence>Therefore , we need to keep the best scoring translation candidate for a given subtree for each combination of ( head , leading bigram , trailing bigram ) , which is , in the worst case , O ( V 5 ) , where V is the vocabulary size .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">the vocabulary size</definiens>
			</definition>
			<definition id="7">
				<sentence>We used a parallel English-French corpus containing 1.5 million sentences of Microsoft technical data ( e.g. , support articles , product documentation ) .</sentence>
				<definiendum id="0">parallel English-French</definiendum>
				<definiens id="0">corpus containing 1.5 million sentences of Microsoft technical data ( e.g. , support articles , product documentation )</definiens>
			</definition>
			<definition id="8">
				<sentence>Pharaoh monotone refers to Pharaoh with phrase reordering disabled .</sentence>
				<definiendum id="0">Pharaoh monotone</definiendum>
				<definiens id="0">Pharaoh with phrase reordering disabled</definiens>
			</definition>
</paper>

		<paper id="3024">
			<definition id="0">
				<sentence>Kenneth Church Bo Thiesson Microsoft Research Redmond , WA , 98052 , USA { church , thiesson } @ microsoft.com Suppose you are on a mobile device with no keyboard ( e.g. , a cell or PDA ) .</sentence>
				<definiendum id="0">PDA</definiendum>
				<definiens id="0">a cell or</definiens>
			</definition>
			<definition id="1">
				<sentence>Google Suggests makes it easy to find popular web queries ( in the standard nonmobile desktop context ) .</sentence>
				<definiendum id="0">Google Suggests</definiendum>
				<definiens id="0">makes it easy to find popular web queries</definiens>
			</definition>
			<definition id="2">
				<sentence>Rather than trying to solve the general case , Palm encourages users to work with the system to write in a way that is easier to recognize ( Graffiti ) .</sentence>
				<definiendum id="0">Palm encourages</definiendum>
				<definiens id="0">users to work with the system to write in a way that is easier to recognize</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus , S [ i ] =i , for 0≤i &lt; N. Each of these ints represents a string , starting at position i in the text and extending to the end of the string. S is then sorted alphabetically. Suffix arrays make it easy to find the frequency and location of any substring. For example , given the substring “mail , ” we find the first and last suffix in S that starts with “mail.” The gap between these two is the frequency. Each suffix in the gap points to a super-string of “mail.” To generalize suffix arrays for phone mode we replace alphabetical order ( strcmp ) with phone order ( phone-strcmp ) . Both strcmp and phonestrcmp consider each character one at a time. In standard alphabetic ordering , ‘a’ &lt; ‘b’ &lt; ‘c’ , but in code can be found at www.cs.dartmouth.edu/~doug. 95 phone-strcmp , the characters that map to the same key on the phone keypad are treated as equivalent. We generalize suffix arrays to take advantage of popularity weights. We don’t want to find all queries that contain the substring “mail , ” but rather , just the k-best ( most popular ) . The standard suffix array method will work , if we add a filter on the output that searches over the results for the kbest. However , that filter could take O ( N ) time if there are lots of matches , as there typically are for short queries. An improvement is to sort the suffix array by both popularity and alphabetic ordering , alternating on even and odd depths in the tree. At the first level , we sort by the first order and then we sort by the second order and so on , using a construction , vaguely analogous to KD-Trees ( Bentley , 1975 ) . When searching a node ordered by alphabetical order , we do what we would do for standard suffix arrays. But when searching a node ordered by popularity , we search the more popular half before the second half. If there are lots of matches , as there are for short strings , the index makes it very easy to find the top-k quickly , and we won’t have to search the second half very often. If the prefix is rare , then we might have to search both halves , and therefore , half the splits ( those split by popularity ) are useless for the worst case , where the input substring doesn’t match anything in the table. Lookup is O ( sqrt N ) .3 Wildcard matching is , of course , a different task from substring matching. Finite State Machines ( Mohri et al , 2002 ) are the right way to think about the k-best string matching problem with wildcards. In practice , the input strings often contain long anchors of constants ( wildcard free substrings ) . Suffix arrays can use these anchors to generate a list of candidates that are then filtered by a regex package. frequency splits and let A ( N ) be the work to process N items on the alphabetical splits. In the worst case , F ( N ) = 2A ( N/2 ) + C1 and A ( N ) = F ( N/2 ) + C2 , where C1 and C2 are two constants. In other words , F ( N ) = 2F ( N/4 ) + C , where C = C1 + 2C2. We guess that F ( N ) = α sqrt ( N ) + β , where α and β are constant. Substituting this guess into the recurrence , the dependencies on N cancel. Thus , we conclude , F ( N ) = O ( sqrt N ) . Memory is limited in many practical applications , especially in the mobile context. Much has been written about lossless compression of language models. For trigram models , we use a lossy method inspired by the Unix Spell program ( McIlroy , 1982 ) . We map each trigram &lt; x , y , z &gt; into a hash code h = ( V2 x + V y + z ) % P , where V is the size of the vocabulary and P is an appropriate prime .</sentence>
				<definiendum id="0">Lookup</definiendum>
				<definiendum id="1">V</definiendum>
				<definiendum id="2">P</definiendum>
				<definiens id="0">the frequency. Each suffix in the gap points to a super-string of “mail.” To generalize suffix arrays for phone mode we replace alphabetical order ( strcmp ) with phone order ( phone-strcmp ) . Both strcmp and phonestrcmp consider each character one at a time. In standard alphabetic ordering , ‘a’ &lt; ‘b’ &lt; ‘c’ , but in code can be found at www.cs.dartmouth.edu/~doug. 95 phone-strcmp , the characters that map to the same key on the phone keypad are treated as equivalent. We generalize suffix arrays to take advantage of popularity weights. We don’t want to find all queries that contain the substring “mail , ” but rather</definiens>
				<definiens id="1">half the splits ( those split by popularity ) are useless for the worst case , where the input substring doesn’t match anything in the table.</definiens>
				<definiens id="2">N/4 ) + C , where C = C1 + 2C2. We guess that F ( N ) = α sqrt ( N ) + β</definiens>
				<definiens id="3">the size of the vocabulary and</definiens>
				<definiens id="4">an appropriate prime</definiens>
			</definition>
			<definition id="4">
				<sentence>Unary is an optimal Huffman code when Pr ( z ) = ( ½ ) z+1 .</sentence>
				<definiendum id="0">Unary</definiendum>
				<definiens id="0">an optimal Huffman code when Pr ( z</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>Word Sense Disambiguation ( WSD ) is the task of determining the correct meaning ( “sense” ) of a word in context , and several efforts have been made to develop automatic WSD systems .</sentence>
				<definiendum id="0">Word Sense Disambiguation</definiendum>
				<definiendum id="1">WSD</definiendum>
				<definiens id="0">the task of determining the correct meaning ( “sense” ) of a word in context</definiens>
			</definition>
			<definition id="1">
				<sentence>PropBank is a corpus in which verbs are annotated with semantic tags , including coarse-grained sense distinctions and predicate-argument structures .</sentence>
				<definiendum id="0">PropBank</definiendum>
				<definiens id="0">a corpus in which verbs are annotated with semantic tags , including coarse-grained sense distinctions and predicate-argument structures</definiens>
			</definition>
			<definition id="2">
				<sentence>PropBank provides manual annotation of predicate-argument information for a large number of verb instances in the Senseval-2 data set .</sentence>
				<definiendum id="0">PropBank</definiendum>
				<definiens id="0">provides manual annotation of predicate-argument information for a large number of verb instances in the Senseval-2 data set</definiens>
			</definition>
			<definition id="3">
				<sentence>We show that associating WordNet semantic classes with nouns is beneficial even without explicit disambiguation of the noun senses because , given enough data , maximum entropy models are able to assign high weights to the correct hypernyms of the correct noun sense if they represent defining selectional restrictions .</sentence>
				<definiendum id="0">maximum entropy models</definiendum>
				<definiens id="0">beneficial even without explicit disambiguation of the noun senses because , given enough data</definiens>
			</definition>
</paper>

		<paper id="2025">
			<definition id="0">
				<sentence>Each feature consists of syntactic patterns ( like verb-object ) in which the word occurs .</sentence>
				<definiendum id="0">feature</definiendum>
			</definition>
			<definition id="1">
				<sentence>The rst order representation is a vector that indicates which of the features identi ed during the feature selection process occur in this context .</sentence>
				<definiendum id="0">rst order representation</definiendum>
				<definiens id="0">a vector that indicates which of the features identi ed during the feature selection process occur in this context</definiens>
			</definition>
			<definition id="2">
				<sentence>A label is a list of bigrams that act as a simple summary of the contents of the cluster .</sentence>
				<definiendum id="0">label</definiendum>
				<definiens id="0">a list of bigrams that act as a simple summary of the contents of the cluster</definiens>
			</definition>
			<definition id="3">
				<sentence>Precision is the percentage of contexts clustered correctly out of those that were attempted .</sentence>
				<definiendum id="0">Precision</definiendum>
			</definition>
			<definition id="4">
				<sentence>Recall is the percentage of contexts clustered correctly out of the total number of contexts given .</sentence>
				<definiendum id="0">Recall</definiendum>
			</definition>
			<definition id="5">
				<sentence>Note that we divide the total input data into equal-sized test and feature selection les , so the number of feature selection and test contexts is half of what is shown , with approximately the same distribution of names .</sentence>
				<definiendum id="0">test contexts</definiendum>
				<definiens id="0">the total input data into equal-sized test and feature selection les , so the number of feature selection and</definiens>
			</definition>
			<definition id="6">
				<sentence>MAJ. represents the percentage of the majority name in the data as a whole , and can be viewed as a baseline measure of performance that 147 Table 1 : Experimental Results ( F-measure ) MAJ. K Order 1 Order 2 Target Word ( M ) ; + ( N ) FSD TST FSD FSD/S TST TST/S BAYER ( 1271 ) ; 60.0 2 67.2 68.6 71.0 51.3 69.2 53.2 BOAMERICA ( 846 ) ( 2117 ) 6 37.4 33.9 47.2 53.3 42.8 49.6 BCLINTON ( 1900 ) ; 50.0 2 82.2 87.6 81.1 81.2 81.2 70.3 TBLAIR ( 1900 ) ( 3800 ) 6 58.5 61.6 61.8 71.4 61.5 72.3 MEXICO ( 1500 ) ; 50.0 2 42.3 52.4 52.7 54.5 52.6 54.5 INDIA ( 1500 ) ( 3000 ) 6 28.4 36.6 37.5 49.0 37.9 52.4 THANKS ( 817 ) ; 55.6 2 61.2 65.3 61.4 56.7 61.4 56.7 RCROWE ( 652 ) ( 1469 ) 6 36.3 41.2 38.5 52.0 39.9 47.8 BAYER ( 1271 ) ; BOAMERICA ( 846 ) ; 43.2 3 69.7 73.7 57.1 54.7 55.1 54.7 JGRISHAM ( 828 ) ; ( 2945 ) 6 31.5 38.4 32.7 53.1 32.8 52.8 BCLINTON ( 1900 ) ; TBLAIR ( 1900 ) ; 33.3 3 51.4 56.4 47.7 44.8 47.7 44.9 EBARAK ( 1900 ) ; ( 5700 ) 6 58.0 54.1 43.8 48.1 43.7 48.1 MEXICO ( 1500 ) ; INDIA ( 1500 ) ; 33.3 3 40.4 41.7 38.1 36.5 38.2 37.4 CALIFORNIA ( 1500 ) ( 4500 ) 6 31.5 38.4 32.7 36.2 32.8 36.2 THANKS ( 817 ) ; RCROWE ( 652 ) ; 35.4 4 42.7 61.5 42.9 38.5 42.7 37.6 BAYER ( 1271 ) ; BOAMERICA ( 846 ) ( 3586 ) 6 47.0 53.0 43.9 34.0 43.5 34.6 BCLINTON ( 1900 ) ; TBLAIR ( 1900 ) ; 25.0 4 48.4 52.3 44.2 50.1 44.7 51.4 EBARAK ( 1900 ) ; VPUTIN ( 1900 ) ( 7600 ) 6 51.8 47.8 43.4 49.3 44.4 50.6 MEXICO ( 1500 ) ; INDIA ( 1500 ) ; 25.0 4 34.4 35.7 29.2 27.4 29.2 27.1 CALIFORNIA ( 1500 ) ; PERU ( 1500 ) ( 6000 ) 6 31.3 32.0 27.3 27.2 27.2 27.2 Table 2 : Sense Assignment Matrix ( 2-way ) TBlair BClinton C0 784 50 834 C1 139 845 984 923 895 1818 would be achieved if all the contexts to be clustered were placed in a single cluster .</sentence>
				<definiendum id="0">MAJ.</definiendum>
				<definiendum id="1">BAYER</definiendum>
				<definiendum id="2">EBARAK</definiendum>
				<definiens id="0">the percentage of the majority name in the data as a whole</definiens>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>2 Kernel Traffic is a collection of summary digests of discussions on GNUe development .</sentence>
				<definiendum id="0">Kernel Traffic</definiendum>
				<definiens id="0">a collection of summary digests of discussions on GNUe development</definiens>
			</definition>
			<definition id="1">
				<sentence>The complete Linux Kernel Archive ( LKA ) consists of two separate downloads .</sentence>
				<definiendum id="0">complete Linux Kernel Archive ( LKA )</definiendum>
				<definiens id="0">consists of two separate downloads</definiens>
			</definition>
			<definition id="2">
				<sentence>The Kernel Traffic ( summary digests ) are in XML format and were downloaded by crawling the Kernel Traffic site .</sentence>
				<definiendum id="0">Kernel Traffic</definiendum>
			</definition>
			<definition id="3">
				<sentence>These methods can be either agglomerative , which begin with an unclustered data set and perform N – 1 pairwise joins , or divisive , which add all objects to a single cluster , and then perform N – 1 divisions to create a hierarchy of smaller clusters , where N is the total number of items to be clustered ( Frakes and Baeza-Yates , 1992 ) .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">begin with an unclustered data set and perform N – 1 pairwise joins</definiens>
				<definiens id="1">add all objects to a single cluster</definiens>
			</definition>
			<definition id="4">
				<sentence>Support vector machines ( SVMs ) have been shown to outperform other existing methods ( naïve Bayes , k-NN , and decision trees ) in text categorization ( Joachims , 1998 ) .</sentence>
				<definiendum id="0">Support vector machines</definiendum>
				<definiendum id="1">SVMs</definiendum>
			</definition>
			<definition id="5">
				<sentence>A simple baseline system takes the first sentence from each email in the sequence that they were posted , based on the assumption that people tend to put important information in the beginning of texts ( Position Hypothesis ) .</sentence>
				<definiendum id="0">simple baseline system</definiendum>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>For example , the pattern COMPANY+fired+ceo consists of three pairs : subject COMPANY , verb fired and object ceo .</sentence>
				<definiendum id="0">pattern COMPANY+fired+ceo</definiendum>
				<definiens id="0">consists of three pairs : subject COMPANY , verb fired and object ceo</definiens>
			</definition>
			<definition id="1">
				<sentence>Each pair consists of either a lexical item or semantic category , and pattern element .</sentence>
				<definiendum id="0">pair</definiendum>
				<definiens id="0">consists of either a lexical item or semantic category , and pattern element</definiens>
			</definition>
			<definition id="2">
				<sentence>Note that we assume the similarity of two element-filler pairs is symmetric , so wij = wji and , consequently , W is a symmetric matrix .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">a symmetric matrix</definiens>
			</definition>
			<definition id="3">
				<sentence>For two senses , s1 and s2 , the lowest common subsumer , lcs ( s1 , s2 ) , is defined as the sense with the highest information content ( most specific ) which subsumes both senses in the WordNet hierarchy .</sentence>
				<definiendum id="0">s1</definiendum>
				<definiens id="0">the sense with the highest information content ( most specific ) which subsumes both senses in the WordNet hierarchy</definiens>
			</definition>
			<definition id="4">
				<sentence>Jiang and Conrath used these elements to calculate the semantic distance between a pair or words , w1 and w2 , according to this formula ( where senses ( w ) is the set 1The cosine metric for a pair of vectors is given by the calculation a.b|a||b| .</sentence>
				<definiendum id="0">senses ( w )</definiendum>
				<definiens id="0">these elements to calculate the semantic distance between a pair or words</definiens>
				<definiens id="1">the set 1The cosine metric for a pair of vectors is given by the calculation a.b|a||b|</definiens>
			</definition>
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>Reading pro ciency is a fundamental component of language competency .</sentence>
				<definiendum id="0">Reading pro ciency</definiendum>
			</definition>
			<definition id="1">
				<sentence>The U.S. educational system is faced with the challenging task of educating growing numbers of students for whom English is a second language ( U.S. Dept. of Education , 2003 ) .</sentence>
				<definiendum id="0">U.S. educational</definiendum>
			</definition>
			<definition id="2">
				<sentence>Reading is a critical part of language and educational development , but nding appropriate reading material for LEP students is often dif cult .</sentence>
				<definiendum id="0">Reading</definiendum>
				<definiens id="0">a critical part of language and educational development , but nding appropriate reading material</definiens>
			</definition>
			<definition id="3">
				<sentence>Natural language processing ( NLP ) technology is an ideal resource for automating the task of selecting appropriate reading material for bilingual students .</sentence>
				<definiendum id="0">Natural language processing</definiendum>
				<definiendum id="1">technology</definiendum>
				<definiens id="0">an ideal resource for automating the task of selecting appropriate reading material for bilingual students</definiens>
			</definition>
			<definition id="4">
				<sentence>For example , perplexity ( PP ) is an information-theoretic measure often used to assess language models : PP = 2H ( t|c ) , ( 3 ) where H ( t|c ) is the entropy relative to class c of a length m word sequence t = w1 , ... , wm , de ned as H ( t|c ) = − 1m log2 P ( t|c ) .</sentence>
				<definiendum id="0">PP )</definiendum>
				<definiens id="0">an information-theoretic measure often used to assess language models</definiens>
				<definiens id="1">the entropy relative to class c of a length m word sequence t = w1 , ... , wm</definiens>
			</definition>
			<definition id="5">
				<sentence>Given P ( c|w ) , the probability of class c given word w , estimated empirically from the training set , we sorted words based on their information gain ( IG ) .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the probability of class c given word w</definiens>
			</definition>
			<definition id="6">
				<sentence>Support vector machines ( SVMs ) are a machine learning technique used in a variety of text classication problems .</sentence>
				<definiendum id="0">Support vector machines</definiendum>
				<definiendum id="1">SVMs</definiendum>
				<definiens id="0">a machine learning technique used in a variety of text classication problems</definiens>
			</definition>
			<definition id="7">
				<sentence>The dev and test sets are the same size and each consist of approximately 5 % of the data for each grade level .</sentence>
				<definiendum id="0">test sets</definiendum>
				<definiens id="0">the same size and each consist of approximately 5 % of the data for each grade level</definiens>
			</definition>
			<definition id="8">
				<sentence>Recall indicates the percentage of the total number of relevant documents in the data set that are retrieved , in this case the percentage of the total number of documents from the target level that are detected .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">indicates the percentage of the total number of relevant documents in the data set that are retrieved</definiens>
			</definition>
			<definition id="9">
				<sentence>The Flesch-Kincaid Grade Level index is a commonly used measure of reading level based on the average number of syllables per word and average sentence length .</sentence>
				<definiendum id="0">Flesch-Kincaid Grade Level index</definiendum>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>Error-correcting CRF training is much less resource intensive and has a much faster training time than a standardly formulated CRF , while decoding performance remains quite comparable .</sentence>
				<definiendum id="0">Error-correcting CRF</definiendum>
				<definiens id="0">much less resource intensive and has a much faster training time than a standardly formulated CRF</definiens>
			</definition>
</paper>

		<paper id="2007">
			<definition id="0">
				<sentence>ASL natural language generation ( NLG ) is a special form of multimodal NLG that uses multiple linguistic output channels .</sentence>
				<definiendum id="0">ASL natural language generation</definiendum>
				<definiens id="0">a special form of multimodal NLG that uses multiple linguistic output channels</definiens>
			</definition>
			<definition id="1">
				<sentence>American Sign Language ( ASL ) is a full natural language – with a linguistic structure distinct from English – used as the primary means of communication for approximately one half million deaf people in the United States ( Neidle et al. , 2000 , Liddell , 2003 ; Mitchell , 2004 ) .</sentence>
				<definiendum id="0">ASL )</definiendum>
				<definiens id="0">a full natural language</definiens>
			</definition>
			<definition id="2">
				<sentence>Some Multimodal NLG focuses on “embodied conversational agents” ( ECAs ) , computer-generated animated characters that communicate with users using speech , eye gaze , facial expression , body posture , and gestures ( Cassell et al. , 2000 ; Kopp et al. , 2004 ) .</sentence>
				<definiendum id="0">Multimodal NLG</definiendum>
			</definition>
</paper>

		<paper id="1070">
			<definition id="0">
				<sentence>Given this , if the content planner wants to convey a house with the following attributes : price , number of bedrooms , number of bathrooms , and property tax , S4 is a less desirable solution than S5 because it splits these concepts into two separate sentences .</sentence>
				<definiendum id="0">S4</definiendum>
				<definiens id="0">price , number of bedrooms , number of bathrooms , and property tax</definiens>
				<definiens id="1">a less desirable solution than S5 because it splits these concepts into two separate sentences</definiens>
			</definition>
			<definition id="1">
				<sentence>Sentence boundary cost ( SBC ) : Assuming P is a set of propositions to be conveyed and S is a collection of example sentences selected from the corpus to convey P. Then we say P can be realized by S with a sentence boundary cost that is equal to ( |S|−1 ) ∗SBC in which |S| is the number of sentences and SBC is the sentence boundary cost .</sentence>
				<definiendum id="0">SBC )</definiendum>
				<definiendum id="1">S</definiendum>
				<definiendum id="2">SBC</definiendum>
				<definiens id="0">a set of propositions to be conveyed and</definiens>
				<definiens id="1">a collection of example sentences selected from the corpus to convey P. Then we say P can be realized by S with a sentence boundary cost that is equal to ( |S|−1 ) ∗SBC in which |S| is the number of sentences and</definiens>
				<definiens id="2">the sentence boundary cost</definiens>
			</definition>
			<definition id="2">
				<sentence>In general , the SBC is a parameter that is sensitive to a generation system’s capability such as its competence in reference expression generation .</sentence>
				<definiendum id="0">SBC</definiendum>
				<definiens id="0">a parameter that is sensitive to a generation system’s capability such as its competence in reference expression generation</definiens>
			</definition>
			<definition id="3">
				<sentence>Insertion cost : Assume P is the set of propositions to be conveyed , and C i is an instance in the corpus that can be used to realize P by inserting a missing proposition p j to C i , then we say P can be realized using C i with an insertion cost of icost ( C H , p j ) , in which C H is the host sentence in the corpus containing proposition p j .</sentence>
				<definiendum id="0">Insertion cost</definiendum>
				<definiendum id="1">C i</definiendum>
				<definiens id="0">Assume P is the set of propositions to be conveyed , and</definiens>
				<definiens id="1">an instance in the corpus that can be used to realize P by inserting a missing proposition p j to C i</definiens>
				<definiens id="2">with an insertion cost of icost ( C H , p j ) , in which C H is the host sentence in the corpus containing proposition p j</definiens>
			</definition>
			<definition id="4">
				<sentence>icost ( ∗ , p j ) is defined as the minimum insertion cost among all the icost ( C H , p j ) .</sentence>
				<definiendum id="0">icost ( ∗ , p j</definiendum>
				<definiens id="0">the minimum insertion cost among all the icost ( C H , p j )</definiens>
			</definition>
			<definition id="5">
				<sentence>Deletion cost : Assume P is a set of input propositions to be conveyed and C i is an instance in the corpus that can be used to convey P by deleting an unneeded proposition p j in C i .</sentence>
				<definiendum id="0">Deletion cost</definiendum>
				<definiens id="0">an instance in the corpus that can be used to convey P by deleting an unneeded proposition p j in C i</definiens>
			</definition>
			<definition id="6">
				<sentence>Overall cost : Assume P is the set of propositions to be conveyed and S is the set of instances in the corpus that are chosen to realize P by applying a set of insertion , deletion and sentence breaking operations , the overall cost of the solution Cost ( P ) = summationdisplay C i ( W i ∗ summationdisplay j icost ( C Hj , p j ) +W d ∗ summationdisplay k dcost ( C i , p k ) ) + ( N b − 1 ) ∗SBC in which W i , W d and SBC are the insertion weight , deletion weight and sentence boundary cost ; N b is the number of sentences in the solution , C i is a corpus instance been selected to construct the solution and C Hj is the host sentence that proposition p j belongs .</sentence>
				<definiendum id="0">Overall cost</definiendum>
				<definiendum id="1">S</definiendum>
				<definiendum id="2">; N b</definiendum>
				<definiens id="0">Assume P is the set of propositions to be conveyed and</definiens>
				<definiens id="1">the set of instances in the corpus that are chosen to realize P by applying a set of insertion , deletion</definiens>
				<definiens id="2">i ∗ summationdisplay j icost ( C Hj , p j ) +W d ∗ summationdisplay k dcost ( C i , p k ) ) + ( N b − 1 ) ∗SBC in which W i , W d and SBC are the insertion weight , deletion weight and sentence boundary cost</definiens>
				<definiens id="3">a corpus instance been selected to construct the solution and C Hj is the host sentence that proposition p j belongs</definiens>
			</definition>
			<definition id="7">
				<sentence>Σ is the set of all possible propositions in an application domain .</sentence>
				<definiendum id="0">Σ</definiendum>
				<definiens id="0">the set of all possible propositions in an application domain</definiens>
			</definition>
			<definition id="8">
				<sentence>Assume S is a solution to P , then it can be represented as the overall cost plus a list of pairs like ( C i s , O i s ) , in which C i s is one of the instances selected to be used in that solution , O i s is a set of deletion , insertion operations that can be applied to C i s to transform it to a subsolution S i .</sentence>
				<definiendum id="0">Assume S</definiendum>
				<definiens id="0">one of the instances selected to be used in that solution</definiens>
				<definiens id="1">a set of deletion</definiens>
			</definition>
			<definition id="9">
				<sentence>Cost d ( P ) +W i ∗ summationdisplay p k ∈E j icost ( ∗ , p k ) + SBC + Cost ( Q ) in which Cost ( Q ) is the cost of sbd ( Q ) which recursively computes the best solution for input Q and Q ⊂ P. To facilitate dynamic programming , we remember the best solution for Q derived by sbd ( Q ) in case Q is used to formulate other solutions .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiens id="0">summationdisplay p k ∈E j icost ( ∗ , p k ) + SBC + Cost ( Q ) in which Cost</definiens>
			</definition>
			<definition id="10">
				<sentence>Repeat the same process for P prime where P prime = P − S. The solution cost is Cost ( P ) = ( N − 1 ) ∗ SBC , and N is the number of sentences in the solution .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of sentences in the solution</definiens>
			</definition>
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>483 Essentially , for any text snippet of the form A1pA2qA3 , these systems estimate the probability that a relationship r ( p , q ) holds between entities p and q , given the interstitial context , as2 P ( r ( p , q ) | pA2q ) = P ( r ( p , q ) | pA2q ) = summationtext x , y∈T c ( xA2y ) summationtext x c ( xA2 ) That is , the probability of a relationship r ( p , q ) is the number of times that pattern xA2y predicts any relationship r ( x , y ) in the training set T. c ( . )</sentence>
				<definiendum id="0">y∈T c</definiendum>
				<definiendum id="1">xA2</definiendum>
				<definiens id="0">the number of times that pattern xA2y predicts any relationship r ( x , y ) in the training set T. c</definiens>
			</definition>
			<definition id="1">
				<sentence>CRFs are undirected graphical models that estimate the conditional probability of a state sequence given an output sequence P ( s | o ) = 1Z exp parenleftbigg Tsummationdisplay t=1 summationdisplay k λkfk ( st−1 , st , o , t ) parenrightbigg 6¯r stands in for all other possible relationships ( including no relationship ) between p and q. P ( A2 | ¯r ( p , q ) ) is estimated as P ( A2 | r ( p , q ) ) is , except with spurious targets .</sentence>
				<definiendum id="0">P ( A2 | r</definiendum>
				<definiens id="0">undirected graphical models that estimate the conditional probability of a state sequence given an output sequence P ( s | o ) = 1Z exp parenleftbigg Tsummationdisplay t=1 summationdisplay k λkfk ( st−1 , st , o , t ) parenrightbigg 6¯r stands in for all other possible relationships ( including no relationship</definiens>
			</definition>
			<definition id="2">
				<sentence>13C ( x ) is either the confidence estimate ( NB+E ) or the probability score ( Rote , CRF , CRF+E ) .</sentence>
				<definiendum id="0">13C ( x )</definiendum>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>Given a set of manual summaries and another set of baseline summaries per task , together with a set of similarity metrics , QARLA provides quantitative measures to ( i ) select and combine the best ( independent ) metrics ( KING measure ) , ( ii ) apply the best set of metrics to evaluate automatic summaries ( QUEEN measure ) , and ( iii ) test whether evaluating with that test-bed is reliable ( JACK measure ) .</sentence>
				<definiendum id="0">QARLA</definiendum>
				<definiens id="0">provides quantitative measures to ( i ) select and combine the best ( independent ) metrics ( KING measure ) , ( ii ) apply the best set of metrics to evaluate automatic summaries ( QUEEN measure ) , and ( iii ) test whether evaluating with that test-bed is reliable</definiens>
			</definition>
			<definition id="1">
				<sentence>In other words , QUEENX , M ( a ) can be defined as the probability , measured over M ×M ×M , that for every metric in X the automatic summary a is closer to a model than two models to each other .</sentence>
				<definiendum id="0">QUEENX , M</definiendum>
				<definiendum id="1">×M ×M</definiendum>
				<definiens id="0">the probability , measured over M</definiens>
			</definition>
			<definition id="2">
				<sentence>A direct formulation can be : KM , A ( X ) = P ( QUEEN ( m ) &gt; QUEEN ( a ) ) According to this formula , the quality of a metric set X is the probability that the quality of a 283 model is higher than the quality of a peer according to this metric set .</sentence>
				<definiendum id="0">direct formulation</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a ) ) According to this formula , the quality of a metric set</definiens>
				<definiens id="1">the probability that the quality of a 283 model is higher than the quality of a peer according to this metric set</definiens>
			</definition>
			<definition id="3">
				<sentence>We can define a suitable K that satisfies condition 5 if we apply a universal quantifier on a. This is what we call the KING measure : KINGM , A ( X ) ≡ P ( ∀a∈A.QUEENM , X ( m ) &gt; QUEENM , X ( a ) ) KING is the probability that a model is better than any peer in a test sample .</sentence>
				<definiendum id="0">KING</definiendum>
				<definiens id="0">the probability that a model is better than any peer in a test sample</definiens>
			</definition>
			<definition id="4">
				<sentence>We have considered the following similarity metrics : ROUGESim : ROUGE is a standard measure to evaluate summarisation systems based on n-gram recall .</sentence>
				<definiendum id="0">ROUGE</definiendum>
				<definiens id="0">a standard measure to evaluate summarisation systems based on n-gram recall</definiens>
			</definition>
			<definition id="5">
				<sentence>NICOS ( key concept overlap ) : Same as VectModelSim , but using key-concepts ( manually identified by the human summarisers after producing the summary ) instead of all nonempty words .</sentence>
				<definiendum id="0">NICOS</definiendum>
				<definiens id="0">key concept overlap ) : Same as VectModelSim , but using key-concepts ( manually identified by the human summarisers after producing the summary ) instead of all nonempty words</definiens>
			</definition>
			<definition id="6">
				<sentence>BLEU ( Papineni et al. , 2001 ) and ROUGE ( Lin and Hovy , 2003a ) are the standard similarity metrics used in Machine Translation and Text Summarisation .</sentence>
				<definiendum id="0">BLEU</definiendum>
				<definiendum id="1">ROUGE</definiendum>
				<definiens id="0">the standard similarity metrics used in Machine Translation and Text Summarisation</definiens>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>The pair ( John Smith , IBM ) is a positive instance , while the pair ( Jane Smith , IBM ) is a negative instance .</sentence>
				<definiendum id="0">IBM )</definiendum>
				<definiendum id="1">IBM )</definiendum>
				<definiens id="0">a positive instance</definiens>
				<definiens id="1">a negative instance</definiens>
			</definition>
			<definition id="1">
				<sentence>For binary relations , this approach is quite tractable : if the relation schema is ( t1 , t2 ) , the number of potential instances is O ( jt1jjt2j ) , where jtj is the number of entity mentions of type t in the text under consideration .</sentence>
				<definiendum id="0">jtj</definiendum>
				<definiens id="0">the number of entity mentions of type t in the text under consideration</definiens>
			</definition>
			<definition id="2">
				<sentence>A clique C of G is a subgraph of G in which there is an edge between every pair of vertices .</sentence>
				<definiendum id="0">clique C of G</definiendum>
				<definiens id="0">a subgraph of G in which there is an edge between every pair of vertices</definiens>
			</definition>
			<definition id="3">
				<sentence>ACT : actual number of related pairs , PRD : predicted number of related pairs and COR : correctly identified related pairs .</sentence>
				<definiendum id="0">PRD</definiendum>
				<definiens id="0">actual number of related pairs</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>The input constructor takes an input specification and , using a background database of medicine information , creates a network of concepts and refor making assumptions about the validity and generalisability of his approach to English language as a whole .</sentence>
				<definiendum id="0">input constructor</definiendum>
				<definiens id="0">takes an input specification and , using a background database of medicine information , creates a network of concepts and refor making assumptions about the validity and generalisability of his approach to English language as a whole</definiens>
			</definition>
			<definition id="1">
				<sentence>Regression on Stylistic Dimension 1 For the regression model on the first stylistic dimension ( SS1 ) , the generator decisions that were used in the regression analysis7 are : imperative with one object sentences ( IMP_VNP ) , V_NP_PP agentless passive sentences ( PAS_VNPP ) , V_NP bypassives ( BYPAS_VN ) , and N_PP clauses ( NPP ) and these are all decisions that happen in the realiser , i.e. , at the third choice point in the architecture .</sentence>
				<definiendum id="0">SS1</definiendum>
				<definiendum id="1">PAS_VNPP</definiendum>
				<definiendum id="2">N_PP clauses</definiendum>
				<definiendum id="3">NPP</definiendum>
				<definiens id="0">imperative with one object sentences ( IMP_VNP ) , V_NP_PP agentless passive sentences</definiens>
			</definition>
			<definition id="2">
				<sentence>i represents a stylistic score and is the dependent variable or criterion in the regression analysis ; the GDj’s represent generator decisions and are called the independent variables or predictors ; the xj’s are weights , and ε is the error .</sentence>
				<definiendum id="0">ε</definiendum>
				<definiens id="0">a stylistic score and is the dependent variable or criterion in the regression analysis</definiens>
			</definition>
			<definition id="3">
				<sentence>The coefficient of determination ( R2 ) , which measures the proportion of the variance of the dependent variable about its mean that is explained by the independent variables , had a reasonably high value ( .895 ) 9 and the analysis of variance obtained an F test of 1701.495 .</sentence>
				<definiendum id="0">coefficient of determination</definiendum>
				<definiens id="0">measures the proportion of the variance of the dependent variable about its mean that is explained by the independent variables , had a reasonably high value ( .895 ) 9 and the analysis of variance obtained an F test of 1701.495</definiens>
			</definition>
			<definition id="4">
				<sentence>The degree of correlation ( R2 ) between the values of target and regression points is 0.9574 for SS1 and 0.942 for SS2 , which means that the search mechanism is working very satisfactorily on both dimensions.14 86420-2-4-6-8-10 8 6 4 2 0 -2 -4 -6 -8 -10 -25-30-35-40-45 -25 -30 -35 -40 -45 Figure 8 : Plotting target points versus regression results on SS1 ( left ) and SS2 ( right ) Scores In the second part of this experiment we wanted to know whether the regression equations were doing the job they were supposed to do by comparing the regression scores with stylistic scores obtained ( from the factor analysis ) for each of the generated texts .</sentence>
				<definiendum id="0">degree of correlation ( R2</definiendum>
				<definiens id="0">Plotting target points versus regression results on SS1 ( left ) and SS2 ( right ) Scores In the second part of this experiment we wanted to know whether the regression equations were doing the job they were supposed to do by comparing the regression scores with stylistic scores obtained ( from the factor analysis</definiens>
			</definition>
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>• T ( w ) is the parse tree for string w. • Φ ( a , w ) ∈ Rd is a feature-vector representation of an acoustic input a together with a string w. • ¯α ∈ Rd is a parameter vector .</sentence>
				<definiendum id="0">T ( w )</definiendum>
				<definiens id="0">the parse tree for string w. • Φ ( a , w ) ∈ Rd is a feature-vector representation of an acoustic input a together with a string w. • ¯α ∈ Rd is a parameter vector</definiens>
			</definition>
			<definition id="1">
				<sentence>In this paper , we introduce syntactic features , which may be sensitive to the parse tree for w , for example Φ3 ( a , w ) = Count of S → NP VP in T ( w ) where S → NP VP is a context-free rule production .</sentence>
				<definiendum id="0">S → NP VP</definiendum>
				<definiens id="0">a context-free rule production</definiens>
			</definition>
			<definition id="2">
				<sentence>The training set consists of examples ( ai , wi ) for i = 1 . . .m , where ai is the i’th acoustic input , and wi is the transcription of this input .</sentence>
				<definiendum id="0">training set</definiendum>
				<definiendum id="1">ai</definiendum>
				<definiendum id="2">wi</definiendum>
				<definiens id="0">the i’th acoustic input</definiens>
			</definition>
			<definition id="3">
				<sentence>C is a constant that dictates the relative weighting given to the two terms .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a constant that dictates the relative weighting given to the two terms</definiens>
			</definition>
			<definition id="4">
				<sentence>Initialization : Set α1 = α , and αj = 0 for j = 2 . . .d. Algorithm : For t = 1 . . . T , i = 1 . . . m •Calculate yi = arg maxw∈GEN ( ai ) 〈Φ ( ai , w ) , ¯α〉 • For j = 2 . . .m , set ¯αj = ¯αj + Φj ( ai , si ) − Φj ( ai , yi ) Output : Either the final parameters ¯α , or the averaged parameters ¯αavg defined as ¯αavg = Pt , i ¯αt , i/mT where ¯αt , i is the parameter vector after training on the i’th training example on the t’th pass through the training data .</sentence>
				<definiendum id="0">Initialization</definiendum>
				<definiens id="0">Set α1 = α , and αj = 0 for j = 2</definiens>
				<definiens id="1">Either the final parameters ¯α</definiens>
				<definiens id="2">the parameter vector after training on the i’th training example on the t’th pass through the training data</definiens>
			</definition>
			<definition id="5">
				<sentence>( a ) we/PRP helped/VBD her/PRP paint/VB the/DT house/NN ( b ) we/NPb helped/VPb her/NPb paint/VPb the/NPb house/NPc ( c ) we/PRP-NPb helped/VBD-VPb her/PRP-NPb paint/VB-VPb the/DT-NPb house/NN-NPc ( d ) we/NP helped/VP her/NP paint/VP house/NP Figure 3 : Sequences derived from a parse tree : ( a ) POS-tag sequence ; ( b ) Shallow parse tag sequence—the superscripts b and c refer to the beginning and continuation of a phrase respectively ; ( c ) Shallow parse tag plus POS tag sequence ; and ( d ) Shallow category with lexical head sequence tag in position i ) ; and composite tag/word features , e.g. tiwi ( where wi represents the word in position i ) or , more complicated configurations , such as ti−2ti−1wi−1tiwi .</sentence>
				<definiendum id="0">helped/VBD-VPb her/PRP-NPb paint/VB-VPb the/DT-NPb house/NN-NPc</definiendum>
				<definiendum id="1">e.g. tiwi</definiendum>
				<definiens id="0">the beginning and continuation of a phrase respectively ; ( c ) Shallow parse tag plus POS tag sequence</definiens>
			</definition>
			<definition id="6">
				<sentence>511 Feature Examples from figure 2 ( P , HCP , Ci , { + , - } { 1,2 } , HP , HCi ) ( VP , VB , NP,1 , paint , house ) ( S , VP , NP , -1 , helped , we ) ( P , HCP , Ci , { + , - } { 1,2 } , HP , HPCi ) ( VP , VB , NP,1 , paint , NN ) ( S , VP , NP , -1 , helped , PRP ) ( P , HCP , Ci , { + , - } { 1,2 } , HPP , HCi ) ( VP , VB , NP,1 , VB , house ) ( S , VP , NP , -1 , VBD , we ) ( P , HCP , Ci , { + , - } { 1,2 } , HPP , HPCi ) ( VP , VB , NP,1 , VB , NN ) ( S , VP , NP , -1 , VBD , PRP ) Table 1 : Examples of head-to-head features .</sentence>
				<definiendum id="0">VBD</definiendum>
				<definiens id="0">VP , VB , NP,1 , VB , house ) ( S , VP , NP , -1 ,</definiens>
			</definition>
			<definition id="7">
				<sentence>The rt02 set consists of 6081 sentences ( 63804 words ) and has three subsets : Switchboard 1 , Switchboard 2 , Switchboard Cellular .</sentence>
				<definiendum id="0">rt02 set</definiendum>
				<definiens id="0">consists of 6081 sentences ( 63804 words ) and has three subsets : Switchboard 1</definiens>
			</definition>
			<definition id="8">
				<sentence>The rt03 set consists of 9050 sentences ( 76083 words ) and has two subsets : Switchboard and Fisher .</sentence>
				<definiendum id="0">rt03 set</definiendum>
				<definiens id="0">consists of 9050 sentences ( 76083 words ) and has two subsets : Switchboard and Fisher</definiens>
			</definition>
			<definition id="9">
				<sentence>Note that our n-best result , using just ngram features , improves upon the perceptron result of ( Roark et al. , 2005 ) by 0.2 percent , putting us within 0.1 percent of their GCLM result for that 512 WER Trial rt02 rt03 ASR system output 37.1 36.4 Roark et al. ( 2005 ) perceptron 36.6 35.7 Roark et al. ( 2005 ) GCLM 36.3 35.4 n-gram perceptron 36.4 35.5 Table 2 : Baseline word-error rates versus Roark et al. ( 2005 ) rt02 Trial WER ASR system output 37.1 n-gram perceptron 36.4 n-gram + POS ( 1 ) perceptron 36.1 n-gram + POS ( 1,2 ) perceptron 36.1 n-gram + POS ( 1,3 ) perceptron 36.1 Table 3 : Use of POS-tag sequence derived features condition .</sentence>
				<definiendum id="0">POS</definiendum>
				<definiens id="0">Baseline word-error rates versus Roark et al. ( 2005 ) rt02 Trial WER ASR system output 37.1 n-gram perceptron 36.4 n-gram + POS ( 1 ) perceptron 36.1 n-gram + POS ( 1,2 ) perceptron 36.1 n-gram +</definiens>
			</definition>
</paper>

		<paper id="2006">
			<definition id="0">
				<sentence>Our error analysis of the C5 results indicates that , because of the relatively small numbers of training instances , C5 ignores some of the features and makes wrong decisions .</sentence>
				<definiendum id="0">C5</definiendum>
				<definiens id="0">ignores some of the features and makes wrong decisions</definiens>
			</definition>
			<definition id="1">
				<sentence>Question a49 a6 is searching for a location as its answer type and the correct answer is one which involves al Qaeda intending to purchase weapons of mass destruction .</sentence>
				<definiendum id="0">answer</definiendum>
				<definiens id="0">one which involves al Qaeda intending to purchase weapons of mass destruction</definiens>
			</definition>
</paper>

		<paper id="2017">
			<definition id="0">
				<sentence>Latin is a pro-drop language , so we can perform the merge without having an explicit subject , which is currently part of another tree .</sentence>
				<definiendum id="0">Latin</definiendum>
				<definiens id="0">a pro-drop language , so we can perform the merge without having an explicit subject , which is currently part of another tree</definiens>
			</definition>
			<definition id="1">
				<sentence>( Case : Nom , Pers:1 , Num : Sg ) , UNCH ( Wh:0 ) → unch ( Type : V ) } We once again use a feature path .</sentence>
				<definiendum id="0">UNCH</definiendum>
				<definiens id="0">→ unch ( Type : V ) } We once again use a feature path</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>It is given by : IG ( w ) = H ( C ) −p ( w ) H ( C|w ) −p ( ¯w ) H ( C| ¯w ) ( 1 ) where H ( C ) = −summationtextCc=1p ( c ) logp ( c ) denotes the entropy of the discrete gender category random variable C. Each document is represented with the Bernoulli model , i.e. a vector of 1 or 0 depending if the word appears or not in the document .</sentence>
				<definiendum id="0">H ( C</definiendum>
				<definiendum id="1">c )</definiendum>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>DIAG ( Towne , 1997 ) is a shell to build ITSs based on interactive graphical models that teach students to troubleshoot complex systems such as home heating and circuitry .</sentence>
				<definiendum id="0">DIAG</definiendum>
				<definiens id="0">a shell to build ITSs based on interactive graphical models that teach students to troubleshoot complex systems such as home heating and circuitry</definiens>
			</definition>
			<definition id="1">
				<sentence>When the student consults DIAG , the tutor sees , in tabular form , the information that DIAG would use in generating its advice — the same “fact file” that DIAG gives to DIAG-NLP1and DIAG-NLP2— and types a response that substitutes for DIAG’s .</sentence>
				<definiendum id="0">DIAG</definiendum>
				<definiens id="0">gives to DIAG-NLP1and DIAG-NLP2— and types a response that substitutes for DIAG’s</definiens>
			</definition>
			<definition id="2">
				<sentence>Objects may be functional aggregates , such as the oil burner , which is a system component that includes other components ; linguistic aggregates , which include plurals and conjunctions ; or a summary over several unspecified indicators or RUs .</sentence>
				<definiendum id="0">Objects</definiendum>
				<definiens id="0">a system component that includes other components ; linguistic aggregates , which include plurals and conjunctions</definiens>
			</definition>
</paper>

		<paper id="3022">
			<definition id="0">
				<sentence>To achieve this goal we have made several innovations , including ( 1 ) developing domain independent models of semantic and contextual interpretation , ( 2 ) developing generic dialogue management components based on an abstract model of collaborative problem solving , and ( 3 ) extensively using an ontology-mapping system that connects the domain independent representations to the representations/query languages used by the back-end applications , and which is used to automatically optimize the performance of the system in the specific domain .</sentence>
				<definiendum id="0">ontology-mapping system</definiendum>
				<definiens id="0">the representations/query languages used by the back-end applications , and which is used to automatically optimize the performance of the system in the specific domain</definiens>
			</definition>
			<definition id="1">
				<sentence>TRIPS is now being used in a wide range of diverse applications , from interactive planning ( e.g. , developing evacuation plans ) , advice giving ( e.g. , a medication advisor ( Ferguson et al. 2002 ) ) , controlling teams of robots , collaborative assistance ( e.g. , an assistant that can help you purchase a computer , as described in this paper ) , supporting human learning , and most recently having the computer learn ( or be taught ) tasks , such as learning to perform tasks on the web .</sentence>
				<definiendum id="0">medication advisor</definiendum>
				<definiens id="0">described in this paper ) , supporting human learning , and most recently having the computer learn ( or be taught ) tasks , such as learning to perform tasks on the web</definiens>
			</definition>
			<definition id="2">
				<sentence>This model forms the basis of a generic component ( the collaboration manager ) that supports both intention recognition to identify the intended speech acts and their content , planning the system 's actions to respond to the user ( or that take initiative ) , and providing utterance realization goals to the generation system .</sentence>
				<definiendum id="0">generic component</definiendum>
				<definiens id="0">the collaboration manager ) that supports both intention recognition to identify the intended speech acts and their content , planning the system 's actions to respond to the user ( or that take initiative</definiens>
			</definition>
</paper>

		<paper id="2020">
			<definition id="0">
				<sentence>Information Structure ( IS ) is a partitioning of the content of a sentence according to its relation to the discourse context .</sentence>
				<definiendum id="0">Information Structure</definiendum>
			</definition>
			<definition id="1">
				<sentence>The Prague Dependency Treebank ( PDT ) consists of newspaper articles from the Czech National Corpus ( ˇ Cerma´ak , 1997 ) and includes three layers of annotation .</sentence>
				<definiendum id="0">Prague Dependency Treebank ( PDT )</definiendum>
				<definiens id="0">consists of newspaper articles from the Czech National Corpus ( ˇ Cerma´ak , 1997 ) and includes three layers of annotation</definiens>
			</definition>
			<definition id="2">
				<sentence>In the Praguian approach to IS , the content of the sentence is divided in two parts : the Topic is “what the sentence is about” and the Focus represents the information asserted about the Topic .</sentence>
				<definiendum id="0">Focus</definiendum>
				<definiens id="0">the information asserted about the Topic</definiens>
			</definition>
			<definition id="3">
				<sentence>The following rules determine which lexical items ( CB or NB ) belong to the Topic or to the Focus ( Hajiˇcov´a et al. , 1998 ; Hajiˇcov´a and Sgall , 2001 ) : belong to the Focus if they are NB ; main verb and is subordinated to an element of Focus belongs to Focus ( where “subordinated to” is defined as the irreflexive transitive closure of “depend on” ) ; then those dependents k i of the verb which have subordinated items l m that are NB are called ‘proxi foci’ ; the items l m together with all items subordinated to them belong to Focus , where i , m &gt; 1 ; 1 – 3 belongs to Topic .</sentence>
				<definiendum id="0">“subordinated to”</definiendum>
				<definiens id="0">lexical items ( CB or NB ) belong to the Topic or to the Focus ( Hajiˇcov´a et al. , 1998 ; Hajiˇcov´a and Sgall , 2001 ) : belong to the Focus if they are NB ; main verb</definiens>
				<definiens id="1">the irreflexive transitive closure of “depend on” ) ; then those dependents k i of the verb</definiens>
			</definition>
			<definition id="4">
				<sentence>The data set consists of 1053 files ( 970,920 words ) from the pre-released version of PDT 2.0 .</sentence>
				<definiendum id="0">data set</definiendum>
				<definiens id="0">consists of 1053 files ( 970,920 words</definiens>
			</definition>
</paper>

		<paper id="3002">
			<definition id="0">
				<sentence>Though it was developed following the guidelines for creating WordNet , GermaNet features a couple of divergent design decisions , such as e.g. the use of non-lexicalized concepts , the association relation between synsets and the small number of textual definitions of word senses .</sentence>
				<definiendum id="0">GermaNet</definiendum>
				<definiens id="0">the association relation between synsets and the small number of textual definitions of word senses</definiens>
			</definition>
			<definition id="1">
				<sentence>On the word sense ( WS ) level , we have the following methods : getAntonyms ( ) retrieves all antonyms of a given WS ; getArtificial ( ) indicates whether a WS is an artificial concept ; getGrapheme ( ) gets a graphemic representation of a WS ; getParticipleOf ( ) retrieves the WS of the verb that the word sense is a participle of ; getPartOfSpeech ( ) gets the part of speech associated with a WS ; getPertonym ( ) gives the WS that the word sense is derived from ; getProperName ( ) indicates whether the WS is a proper name ; getSense ( ) yields the sense number of a WS in GermaNet ; getStyle ( ) indicates if the WS is stylistically marked ; getSynset ( ) returns the corresponding synset ; toString ( ) yields a string representing a WS .</sentence>
				<definiendum id="0">getAntonyms ( )</definiendum>
				<definiendum id="1">toString ( )</definiendum>
				<definiens id="0">retrieves all antonyms of a given WS ; getArtificial ( ) indicates whether a WS is an artificial concept ; getGrapheme ( ) gets a graphemic representation of a WS ; getParticipleOf ( ) retrieves the WS of the verb that the word sense is a participle of ; getPartOfSpeech ( ) gets the part of speech associated with a WS ; getPertonym ( ) gives the WS that the word sense is derived from ; getProperName ( ) indicates whether the WS is a proper name ; getSense ( ) yields the sense number of a WS in GermaNet ; getStyle ( ) indicates if the WS is stylistically marked ; getSynset ( ) returns the corresponding synset ;</definiens>
			</definition>
			<definition id="2">
				<sentence>On the synset level , the following information can be accessed : getAssociations ( ) returns all associations ; getCausations ( ) gets the effects that a given synset is a cause of ; getEntailments ( ) yields synsets that entail a given synset ; getHolonyms ( ) , getHyponyms ( ) , getHypernyms ( ) , getMeronyms ( ) return a list of holonyms , hyponyms , immediate hypernyms , and meronyms respectively ; getPartOfSpeech ( ) returns the part of speech associated with word senses of a synset ; getWordSenses ( ) returns all word senses constituting the synset ; toString ( ) yields a string representation of a synset .</sentence>
				<definiendum id="0">getAssociations ( ) returns all associations ; getCausations ( )</definiendum>
				<definiendum id="1">; getEntailments ( )</definiendum>
				<definiendum id="2">getHolonyms</definiendum>
				<definiendum id="3">getWordSenses ( )</definiendum>
				<definiendum id="4">toString ( )</definiendum>
				<definiens id="0">a cause of</definiens>
				<definiens id="1">return a list of holonyms , hyponyms , immediate hypernyms , and meronyms respectively ; getPartOfSpeech ( ) returns the part of speech associated with word senses of a synset ;</definiens>
				<definiens id="2">returns all word senses constituting the synset ;</definiens>
			</definition>
			<definition id="3">
				<sentence>RelatednessComparator is a class which takes two words as input and returns a numeric value indicating semantic relatedness for the two words .</sentence>
				<definiendum id="0">RelatednessComparator</definiendum>
				<definiens id="0">a class which takes two words as input and returns a numeric value indicating semantic relatedness for the two words</definiens>
			</definition>
			<definition id="4">
				<sentence>The Lesk algorithm computes the number of overlaps in the definitions of words , which are sometimes extended with the definitions of words related to the given word senses ( Patwardhan et al. , 2003 ) .</sentence>
				<definiendum id="0">Lesk algorithm</definiendum>
				<definiens id="0">computes the number of overlaps in the definitions of words</definiens>
			</definition>
</paper>

		<paper id="2022">
			<definition id="0">
				<sentence>The reasoning is as follows ( Figure 1 ) : if there is a pair of anchor words , i.e. if two words w1 i ( community in the example ) and w2 m ( communauté ) are aligned at the sentence level , and if there is a dependency relation between w1 i ( community ) and w1 j ( ban ) on the one hand , and between w2 m ( communauté ) and w2 n ( interdire ) on the other hand , then the alignment link is propagated from the anchor pair ( community , communauté ) to the syntactically connected words ( ban , interdire ) .</sentence>
				<definiendum id="0">w1 j</definiendum>
				<definiendum id="1">w2 n</definiendum>
				<definiens id="0">a pair of anchor words</definiens>
			</definition>
			<definition id="1">
				<sentence>It contains written questions on a wide variety of topics addressed by members of the European Parliament to the European Commission , as well as the corresponding answers .</sentence>
				<definiendum id="0">Commission</definiendum>
				<definiens id="0">a wide variety of topics addressed by members of the European Parliament to the European</definiens>
			</definition>
			<definition id="2">
				<sentence>SYNTEX is a dependency parser whose input is a POS tagged 3 corpus — meaning each word in the corpus is assigned a lemma and grammatical tag .</sentence>
				<definiendum id="0">SYNTEX</definiendum>
				<definiens id="0">a dependency parser whose input is a POS tagged 3 corpus — meaning each word in the corpus is assigned a lemma and grammatical tag</definiens>
			</definition>
			<definition id="3">
				<sentence>In addition to parsed English/French bitexts , the syntax-based alignment requires pairs of anchor words be identified prior to propagation .</sentence>
				<definiendum id="0">syntax-based alignment</definiendum>
				<definiens id="0">requires pairs of anchor words be identified prior to propagation</definiens>
			</definition>
			<definition id="4">
				<sentence>( 5 ) X is a model which was designed to stimulate… X est un modèle qui a été conçu pour stimuler… GOV-TO-DEP PROPAGATION TO ALIGN DEPENDENT VERBS .</sentence>
				<definiendum id="0">X</definiendum>
			</definition>
			<definition id="5">
				<sentence>Indeed , the HLT project concerned above all statistical alignment systems aiming at language modelling for machine translation .</sentence>
				<definiendum id="0">HLT project</definiendum>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>Log-linear models can be so crafted and have already achieved excellent performance when trained on annotated data , where they are known as “maximum entropy” models ( Ratnaparkhi et al. , 1994 ; Rosenfeld , 1994 ) .</sentence>
				<definiendum id="0">Log-linear models</definiendum>
			</definition>
			<definition id="1">
				<sentence>The other reason to use CE is to improve accuracy .</sentence>
				<definiendum id="0">CE</definiendum>
				<definiens id="0">to improve accuracy</definiens>
			</definition>
			<definition id="2">
				<sentence>Typically one turns to the EM algorithm ( Dempster et al. , 1977 ) , which locally maximizes productdisplay i p parenleftBig X = xi | vectorθ parenrightBig = productdisplay i summationdisplay y∈Y p parenleftBig X = xi , Y = y | vectorθ parenrightBig ( 1 ) where X is a random variable over sentences and Y a random variable over analyses ( notation is often abbreviated , eliminating the random variables ) .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a random variable over sentences</definiens>
			</definition>
			<definition id="3">
				<sentence>joint likelihood ( JL ) productdisplay i p parenleftBig xi , y∗i | vectorθ parenrightBig conditional likelihood ( CL ) productdisplay i p parenleftBig y∗i | xi , vectorθ parenrightBig classification accuracy ( Juang and Katagiri , 1992 ) summationdisplay i δ ( y∗i , ˆy ( xi ) ) expected classification accuracy ( Klein and Manning , 2002 ) summationdisplay i p parenleftBig y∗i | xi , vectorθ parenrightBig negated boosting loss ( Collins , 2000 ) − summationdisplay i p parenleftBig y∗i | xi , vectorθ parenrightBig−1 margin ( Crammer and Singer , 2001 ) γ s.t. bardbl vectorθbardbl ≤ 1 ; ∀i , ∀y negationslash= y∗i , vectorθ · ( vectorf ( xi , y∗i ) − vectorf ( xi , y ) ) ≥ γ expected local accuracy ( Altun et al. , 2003 ) productdisplay i productdisplay j p parenleftBig lscriptj ( Y ) = lscriptj ( y∗i ) | xi , vectorθ parenrightBig Table 1 : Various supervised training criteria .</sentence>
				<definiendum id="0">local accuracy</definiendum>
				<definiens id="0">Various supervised training criteria</definiens>
			</definition>
			<definition id="4">
				<sentence>Our approach instead maximizes productdisplay i p parenleftBig Xi = xi | Xi ∈ N ( xi ) , vectorθ parenrightBig ( 2 ) where the “neighborhood” N ( xi ) ⊆ X is a set of implicit negative examples plus the example xi itself .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">maximizes productdisplay i p parenleftBig Xi = xi | Xi ∈ N ( xi ) , vectorθ parenrightBig ( 2 ) where the “neighborhood” N ( xi ) ⊆</definiens>
			</definition>
			<definition id="5">
				<sentence>Log-linear models , which we will show are a natural fit for CE , assign probability to an ( example , label ) pair ( x , y ) according to p parenleftBig x , y | vectorθ parenrightBig def = 1 Z parenleftBigvector θ parenrightBigu parenleftBig x , y | vectorθ parenrightBig ( 3 ) where the “unnormalized score” u ( x , y | vectorθ ) is u parenleftBig x , y | vectorθ parenrightBig def = exp parenleftBigvector θ · vectorf ( x , y ) parenrightBig ( 4 ) The notation above is defined as follows .</sentence>
				<definiendum id="0">Log-linear models</definiendum>
				<definiens id="0">a natural fit for CE , assign probability to an ( example , label ) pair ( x , y ) according to p parenleftBig x , y | vectorθ parenrightBig def = 1 Z parenleftBigvector θ parenrightBigu parenleftBig x , y | vectorθ parenrightBig ( 3 ) where the “unnormalized score” u ( x , y | vectorθ ) is u parenleftBig x , y | vectorθ parenrightBig def = exp parenleftBigvector θ · vectorf ( x , y</definiens>
			</definition>
			<definition id="6">
				<sentence>Z ( vectorθ ) ( the partition function ) is chosen so thatsummationtext ( x , y ) p ( x , y | vectorθ ) = 1 ; i.e. , Z ( vectorθ ) = summationtext ( x , y ) u ( x , y | vectorθ ) .</sentence>
				<definiendum id="0">Z ( vectorθ )</definiendum>
				<definiens id="0">the partition function ) is chosen so thatsummationtext ( x , y ) p ( x , y | vectorθ ) = 1</definiens>
			</definition>
			<definition id="7">
				<sentence>y is a hidden path through the automaton ( determining a POS sequence ) , and x is the string it emits .</sentence>
				<definiendum id="0">y</definiendum>
				<definiendum id="1">x</definiendum>
				<definiens id="0">a hidden path through the automaton ( determining a POS sequence ) , and</definiens>
			</definition>
			<definition id="8">
				<sentence>4 and 6 where fj ( x , y ) is the number of times the path y takes the jth transition .</sentence>
				<definiendum id="0">fj</definiendum>
				<definiendum id="1">y )</definiendum>
				<definiens id="0">the number of times the path y takes the jth transition</definiens>
			</definition>
			<definition id="9">
				<sentence>Generally speaking , CE is equivalent to some kind of EM when N ( · ) is an equivalence relation on examples , so that the neighborhoods partition X. Then if q is any fixed ( untrained ) distribution over neighborhoods , CE equates to running EM on the model defined by pprime parenleftBig x , y | vectorθ parenrightBig def = q ( N ( x ) ) ·p parenleftBig x , y | N ( x ) , vectorθ parenrightBig ( 9 ) CE may also be viewed as an importance sampling approximation to EM , where the sample space X is replaced by N ( xi ) .</sentence>
				<definiendum id="0">N ( · )</definiendum>
				<definiendum id="1">CE</definiendum>
				<definiens id="0">an equivalence relation on examples , so that the neighborhoods partition X. Then if q is any fixed ( untrained ) distribution over neighborhoods</definiens>
				<definiens id="1">equates to running EM on the model defined by pprime parenleftBig x , y | vectorθ parenrightBig def = q ( N ( x ) ) ·p parenleftBig x , y | N ( x ) , vectorθ parenrightBig ( 9 ) CE may also be viewed as an importance sampling approximation to EM , where the sample space X is replaced by N ( xi )</definiens>
			</definition>
			<definition id="10">
				<sentence>A final neighborhood we will consider is LENGTH , which consists of Σm .</sentence>
				<definiendum id="0">LENGTH</definiendum>
				<definiens id="0">consists of Σm</definiens>
			</definition>
			<definition id="11">
				<sentence>9 where q is any fixed ( untrained ) distribution over lengths .</sentence>
				<definiendum id="0">q</definiendum>
				<definiens id="0">any fixed ( untrained ) distribution over lengths</definiens>
			</definition>
			<definition id="12">
				<sentence>When the vocabulary Σ is the set of words in a natural language , it is never fully known ; approximations for defining LENGTH = Σm include using observed Σ from the training set ( as we do ) or adding a special OOV symbol .</sentence>
				<definiendum id="0">vocabulary Σ</definiendum>
				<definiens id="0">the set of words in a natural language , it is never fully known</definiens>
			</definition>
</paper>

		<paper id="3030">
			<definition id="0">
				<sentence>EFL ( English as a foreign language ) learners and teachers can easily access a wide range of English reading materials on the Internet .</sentence>
				<definiendum id="0">EFL</definiendum>
				<definiens id="0">a foreign language ) learners and teachers can easily access a wide range of English reading materials on the Internet</definiens>
			</definition>
			<definition id="1">
				<sentence>We then define g ( · ) as g ( d|Vx ) = k1 +1k 1 ( ( 1−b ) +b |W ( d ) |E ( |W ( · ) | ) ) +1 |W ( d ) ∩Vx| , ( 2 ) where W ( d ) is the set of types in d , E ( |W ( · ) | ) is the average for |W ( · ) | over the whole corpus , and k1 and b are parameters that depend on the corpus .</sentence>
				<definiendum id="0">W ( d )</definiendum>
				<definiendum id="1">E ( |W</definiendum>
				<definiens id="0">the set of types in d</definiens>
				<definiens id="1">the average for |W ( · ) | over the whole corpus</definiens>
				<definiens id="2">parameters that depend on the corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>Consequently , G ( · ) uses documents that have new types of the given vocabulary in preference to documents that have covered types .</sentence>
				<definiendum id="0">G ( · )</definiendum>
				<definiens id="0">uses documents that have new types of the given vocabulary in preference to documents that have covered types</definiens>
			</definition>
			<definition id="3">
				<sentence>The numbers in parentheses indicate the document frequencies ( DFs ) of the words , where the DF of a word is the number of articles in which the word occurred .</sentence>
				<definiendum id="0">document frequencies</definiendum>
				<definiendum id="1">DFs</definiendum>
				<definiens id="0">the number of articles in which the word occurred</definiens>
			</definition>
			<definition id="4">
				<sentence>To this end , we replaced The Daily Yomiuri , which is copyrighted , with the English Wikipedia , which is a free-content encyclopedia , and developed new courseware whose statistics were presented and discussed in this paper .</sentence>
				<definiendum id="0">Daily Yomiuri</definiendum>
				<definiendum id="1">Wikipedia</definiendum>
				<definiens id="0">a free-content encyclopedia , and developed new courseware whose statistics were presented</definiens>
			</definition>
</paper>

		<paper id="2023">
			<definition id="0">
				<sentence>Lxtransduce is an updated version of fsgmatch , the core program of LT TTT ( Grover et al. , 2000 ) .</sentence>
				<definiendum id="0">Lxtransduce</definiendum>
				<definiens id="0">an updated version of fsgmatch</definiens>
			</definition>
			<definition id="1">
				<sentence>For the initial lookup , we used CELEX , a lexical database of English , German and Dutch containing full and inflected word forms as well as corresponding lemmas .</sentence>
				<definiendum id="0">CELEX</definiendum>
				<definiens id="0">a lexical database of English , German and Dutch containing full and inflected word forms as well as corresponding lemmas</definiens>
			</definition>
			<definition id="2">
				<sentence>The Google lookup module obtains the number of hits for two searches per token , one on German Web pages and one on English ones , an advanced language preference offered by Google .</sentence>
				<definiendum id="0">Google lookup module</definiendum>
				<definiens id="0">obtains the number of hits for two searches per token , one on German Web pages and one on English ones , an advanced language preference offered by Google</definiens>
			</definition>
			<definition id="3">
				<sentence>In the first experiment ( ID1 ) , we use the tagger’s standard feature set including words , character sub-strings , word shapes , POS-tags , abbreviations and NE tags ( Finkel et al. , 2005 ) .</sentence>
				<definiendum id="0">ID1</definiendum>
			</definition>
			<definition id="4">
				<sentence>ID2 involves the same setup as ID1 but eliminating all features relying on the POS-tags .</sentence>
				<definiendum id="0">ID2</definiendum>
				<definiens id="0">involves the same setup as ID1 but eliminating all features relying on the POS-tags</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>However , the MLU holds the advantage of being far easier to compute .</sentence>
				<definiendum id="0">MLU</definiendum>
				<definiens id="0">holds the advantage of being far easier to compute</definiens>
			</definition>
			<definition id="1">
				<sentence>An evaluation of our automated version of IPSyn , which searches for IPSyn structures using POS , morphology and GR information , and a comparison to the CP implementation , which uses only POS and morphology information , is presented in section 5 .</sentence>
				<definiendum id="0">IPSyn</definiendum>
				<definiens id="0">searches for IPSyn structures using POS , morphology and GR information</definiens>
				<definiens id="1">uses only POS and morphology information</definiens>
			</definition>
			<definition id="2">
				<sentence>The second is Point-to-Point Accuracy , which reflects the overall reliability over each individual scoring decision in the computation of IPSyn scores .</sentence>
				<definiendum id="0">Point-to-Point Accuracy</definiendum>
			</definition>
			<definition id="3">
				<sentence>The second set ( B ) contains 25 transcripts of children of ages ranging between eight and nine .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">contains 25 transcripts of children of ages ranging between eight and nine</definiens>
			</definition>
			<definition id="4">
				<sentence>GR is our implementation of IPSyn based on grammatical relations , CP is Long et al.’s ( 2004 ) implementation of IPSyn , and HUMAN is manual scoring .</sentence>
				<definiendum id="0">CP</definiendum>
				<definiendum id="1">HUMAN</definiendum>
				<definiens id="0">manual scoring</definiens>
			</definition>
			<definition id="5">
				<sentence>Searching for the relation COMP is a crucial part in finding propositional complements .</sentence>
				<definiendum id="0">COMP</definiendum>
				<definiens id="0">a crucial part in finding propositional complements</definiens>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>Stochastic Optimality Theory ( Boersma , 1997 ) is a widely-used model in linguistics that did not have a theoretically sound learning method previously .</sentence>
				<definiendum id="0">Stochastic Optimality Theory</definiendum>
				<definiens id="0">a widely-used model in linguistics that did not have a theoretically sound learning method previously</definiens>
			</definition>
			<definition id="1">
				<sentence>Optimality Theory ( Prince and Smolensky , 1993 ) is a linguistic theory that dominates the field of phonology , and some areas of morphology and syntax .</sentence>
				<definiendum id="0">Optimality Theory</definiendum>
				<definiens id="0">a linguistic theory that dominates the field of phonology</definiens>
			</definition>
			<definition id="2">
				<sentence>The standard version of OT contains the following assumptions : • A grammar is a set of ordered constraints ( { Ci : i = 1 , ··· , N } , &gt; ) ; • Each constraint Ci is a function : Σ∗ → { 0,1 , ··· } , where Σ∗ is the set of strings in the language ; ∗The author thanks Bruce Hayes , Ed Stabler , Yingnian Wu , Colin Wilson , and anonymous reviewers for their comments .</sentence>
				<definiendum id="0">grammar</definiendum>
				<definiendum id="1">Ci</definiendum>
				<definiens id="0">a function : Σ∗ → { 0,1</definiens>
				<definiens id="1">the set of strings in the language</definiens>
			</definition>
			<definition id="3">
				<sentence>The learning problem of Stochastic OT involves fitting a grammar G ∈ RN to a set of candidates with frequency counts in a corpus .</sentence>
				<definiendum id="0">Stochastic OT</definiendum>
				<definiens id="0">involves fitting a grammar G ∈ RN to a set of candidates with frequency counts in a corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>As an example , given the grammar and data set in Table 1 , the likelihood of d=“max { C1 , C2 } &gt; C3” can be written as P ( d|µ1 , µ2 , µ3 ) = 1−integraltext0−∞integraltext0−∞ 12piσ2 exp braceleftbigg −vectorfxy·Σ·vectorfTxy2 bracerightbigg dxdy where vectorfxy = ( x−µ1 +µ3 , y −µ2 +µ3 ) , and Σ is the identity covariance matrix .</sentence>
				<definiendum id="0">Σ</definiendum>
				<definiens id="0">the identity covariance matrix</definiens>
			</definition>
			<definition id="5">
				<sentence>M ( the number of missing data ) is not a crucial parameter .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">the number of missing data ) is not a crucial parameter</definiens>
			</definition>
			<definition id="6">
				<sentence>The prior probability p ( D ) determines the number of samples ( missing data ) that are drawn under each ordering relation .</sentence>
				<definiendum id="0">prior probability p</definiendum>
				<definiens id="0">determines the number of samples ( missing data ) that are drawn under each ordering relation</definiens>
			</definition>
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>L monolingual phoneme recognition front-ends are used in parallel to tokenize the input utterance , which is analyzed by LMs to predict the spoken language A voice tokenizer is a speech recognizer that converts a spoken document into a sequence of tokens .</sentence>
				<definiendum id="0">voice tokenizer</definiendum>
				<definiens id="0">a speech recognizer that converts a spoken document into a sequence of tokens</definiens>
			</definition>
			<definition id="1">
				<sentence>The ngram LM derives the n-local phonotactic score ˆ l T ( ) ˆ / LM ll PT λ from the language model LM l λ .</sentence>
				<definiendum id="0">ngram LM</definiendum>
				<definiens id="0">derives the n-local phonotactic score ˆ l T ( ) ˆ / LM ll PT λ from the language model LM l λ</definiens>
			</definition>
			<definition id="2">
				<sentence>One focus of IR is to extract informative features for document representation .</sentence>
				<definiendum id="0">IR</definiendum>
				<definiens id="0">to extract informative features for document representation</definiens>
			</definition>
			<definition id="3">
				<sentence>The bag-of-words paradigm represents a document as a vector of counts .</sentence>
				<definiendum id="0">bag-of-words paradigm</definiendum>
				<definiens id="0">represents a document as a vector of counts</definiens>
			</definition>
			<definition id="4">
				<sentence>The local phonotactic constraints can be typically described by the token n-grams , or phoneme n-grams as in ( Ng et al. , 2000 ) , which represents short-term statistics such as lexical constraints .</sentence>
				<definiendum id="0">local phonotactic constraints</definiendum>
				<definiens id="0">represents short-term statistics such as lexical constraints</definiens>
			</definition>
			<definition id="5">
				<sentence>The # sign is a place holder for free context .</sentence>
				<definiendum id="0"># sign</definiendum>
			</definition>
			<definition id="6">
				<sentence>We use the inverse-document frequency ( idf ) weighting scheme ( Spark Jones , 1972 ) , in which a word is weighted inversely to the number of documents in which it occurs , by means of ( ) log / ( ) idf w D d w= , where w is a word in the vocabulary of W token n-grams .</sentence>
				<definiendum id="0">inverse-document frequency</definiendum>
				<definiendum id="1">w</definiendum>
				<definiens id="0">in which a word is weighted inversely to the number of documents in which it occurs</definiens>
				<definiens id="1">a word in the vocabulary of W token n-grams</definiens>
			</definition>
			<definition id="7">
				<sentence>D is the total number of documents in the training corpus from L languages .</sentence>
				<definiendum id="0">D</definiendum>
			</definition>
			<definition id="8">
				<sentence>Through singular value decomposition ( SVD ) of H , we construct a modified matrix H Q from the Q-largest singular values : T QQQQ H USV= ( 4 ) Q U is a WQ× left singular matrix with rows ,1 w uwW≤ ≤ Q S ; is a QQ× diagonal matrix of Qlargest singular values of H ; is Q V D Q× right singular matrix with rows , 1 .</sentence>
				<definiendum id="0">SVD</definiendum>
				<definiendum id="1">T QQQQ H USV=</definiendum>
				<definiens id="0">a WQ× left singular matrix with rows ,1 w uwW≤ ≤ Q S ; is a QQ× diagonal matrix of Qlargest singular values of H</definiens>
			</definition>
			<definition id="9">
				<sentence>Applying LSA to a termdocument matrix : HW L′× , where LL assuming each language l is represented by a set of M vectors , M′=× l Φ , a new classifier , using k-nearest neighboring rule ( Duda and Hart , 1973 ) , is formulated , named k-nearest classifier ( KNC ) : ˆ arg min ( , ) l pl l l lk φ ′ ∈Λ ′∈ = v ∑ ( 8 ) where l φ is the set of k-nearest-neighbor to and p v ll φ ⊂Φ .</sentence>
				<definiendum id="0">LL</definiendum>
				<definiendum id="1">M′=× l Φ</definiendum>
				<definiens id="0">formulated , named k-nearest classifier ( KNC ) : ˆ arg min</definiens>
				<definiens id="1">the set of k-nearest-neighbor to and p v ll φ ⊂Φ</definiens>
			</definition>
			<definition id="10">
				<sentence>The class conditional probability can be described as a linear combination of , ( 1 , ... ) lm l vm∈Φ = M , ( | ) ilm p vv : , 1 ( | ) ( ) ( | ) M LM il lm ilm m , p vpvpvλ = = ∑ ) ( 9 ) the probability , ( lm p v , functionally serves as a mixture weight of , ( | ) ilm p vv .</sentence>
				<definiendum id="0">class conditional probability</definiendum>
				<definiens id="0">a mixture weight of , ( | ) ilm p vv</definiens>
			</definition>
</paper>

		<paper id="2002">
			<definition id="0">
				<sentence>The Qur’an is one of the great religious books of the world , and is at the heart of Islamic culture .</sentence>
				<definiendum id="0">Qur’an</definiendum>
				<definiens id="0">one of the great religious books of the world</definiens>
			</definition>
			<definition id="1">
				<sentence>The Qur’an consists of 114 chapters called suras which range in length from the shortest , AlKawthar , consisting of 4 ayat ( verses ) to the longest , Al-Baqarah , consisting of 286 ayat .</sentence>
				<definiendum id="0">Qur’an</definiendum>
			</definition>
			<definition id="2">
				<sentence>This was done on the following basis : ( ) ( ) ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ×= l FfreqFfreq ijij µ ' where freq ' is the adjusted frequency , F ij is the value at the ( i , j ) coordinates of the data matrix F , freq is the raw frequency , µ is the mean number of words per sura across all 114 suras , and l is the number of words in sura i. That said , it has also to be observed that , as text length decreases , so does the probability that any given word will occur even once in it , and its frequency vector will therefore become increasingly sparse , consisting mainly of zeros .</sentence>
				<definiendum id="0">freq</definiendum>
				<definiendum id="1">l</definiendum>
				<definiens id="0">the mean number of words per sura across all 114 suras , and</definiens>
			</definition>
			<definition id="3">
				<sentence>.1 , ∑ = = where j is the index to the jth element of s , i indexes the rows of the data matrix F , and n is the total number of rows in cluster G. If s is interpreted in terms of the semantics of the matrix column labels , it becomes a thematic profile for G : relative to the frequency range of s , a high-frequency word indicates that the suras which constitute G are concerned with the denotation of that word , and the indication for a low-frequency one is the obverse .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the total number of rows in cluster G. If s is interpreted in terms of the semantics of the matrix column labels</definiens>
			</definition>
			<definition id="4">
				<sentence>Initial plot of groups A and B” The suras of cluster A are strikingly more concerned with the denotation of variable 1 , the highest-variance variable in the Qur’an , than the suras of cluster B. This variable is the lexical item ‘Allah’ , which is central in Islam ; the disparity in the frequency of its occurrence in A and B is the first significant finding of the proposed methodology .</sentence>
				<definiendum id="0">‘Allah’</definiendum>
				<definiendum id="1">B</definiendum>
				<definiens id="0">the disparity in the frequency of its occurrence in A and</definiens>
			</definition>
</paper>

		<paper id="1066">
			<definition id="0">
				<sentence>Phrase-based models generalize the original IBM models by allowing multiple words in one language to correspond to multiple words in another language .</sentence>
				<definiendum id="0">Phrase-based models</definiendum>
				<definiens id="0">generalize the original IBM models by allowing multiple words in one language to correspond to multiple words in another language</definiens>
			</definition>
			<definition id="1">
				<sentence>Key to non-terminals : PPER = personal pronoun ; VAFIN = finite verb ; VVINF = infinitival verb ; KOUS = complementizer ; APPR = preposition ; ART = article ; ADJA = adjective ; ADJD = adverb ; -SB = subject ; -HD = head of a phrase ; -DA = dative object ; -OA = accusative object .</sentence>
				<definiendum id="0">-SB</definiendum>
				<definiendum id="1">-OA</definiendum>
				<definiens id="0">= subject ; -HD = head of a phrase ; -DA = dative object ;</definiens>
			</definition>
			<definition id="2">
				<sentence>We trained this system on the Europarl corpus , which consists of 751,088 sentence pairs with 15,256,792 German words and 16,052,269 English words .</sentence>
				<definiendum id="0">Europarl corpus</definiendum>
			</definition>
			<definition id="3">
				<sentence>R gives counts corresponding to translations where an annotator preferred the reordered system ; B signifies that the annotator preferred the baseline system ; E means an annotator judged the two systems to give equal quality translations .</sentence>
				<definiendum id="0">R</definiendum>
				<definiendum id="1">E</definiendum>
				<definiens id="0">gives counts corresponding to translations where an annotator preferred the reordered system ; B signifies that the annotator preferred the baseline system ;</definiens>
			</definition>
			<definition id="4">
				<sentence>Given a sample of a49 test points a38 a0 a44a51a50a53a52a53a52a53a52a51a50 a0a55a54 a42 , the sign test depends on calculation of the following counts : a56a53a24a57a26a59a58a60a38a51a61a63a62 a1a3a2 a0a55a64a65a4 a26a30a29 a42 a58 , a56a66a31a67a26a68a58a60a38a51a61a69a62 a1a3a2 a0a55a64a70a4 a26a34a33 a42 a58 , 537 and a56 a37 a26a59a58a60a38a51a61a69a62 a1a3a2 a0a55a64a70a4 a26 a0 a42 a58 , where a58a0 a58 is the cardinality of the set a0 .</sentence>
				<definiendum id="0">a58a0 a58</definiendum>
			</definition>
			<definition id="5">
				<sentence>R is the human ( reference ) translation ; C is the translation from the system with reordering ; B is the output from the baseline system .</sentence>
				<definiendum id="0">R</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">the translation from the system with reordering ; B is the output from the baseline system</definiens>
			</definition>
			<definition id="6">
				<sentence>R is the human ( reference ) translation ; C is the translation from the system with reordering ; B is the output from the baseline system .</sentence>
				<definiendum id="0">R</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">the translation from the system with reordering ; B is the output from the baseline system</definiens>
			</definition>
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>The VSM is a k-dimensional space Rk , in which the text tj 2 C is represented by means of the vector vectortj such that the ith component of vectortj is ti , j. The similarity among two texts in the VSM is estimated by computing the cosine among them .</sentence>
				<definiendum id="0">VSM</definiendum>
				<definiens id="0">a k-dimensional space Rk , in which the text tj</definiens>
			</definition>
			<definition id="1">
				<sentence>Each cluster represents a semantic domain , i.e. a set of terms that often co-occur in texts having similar topics .</sentence>
				<definiendum id="0">cluster</definiendum>
				<definiens id="0">a semantic domain</definiens>
			</definition>
			<definition id="2">
				<sentence>A DM is represented by a k kprime rectangular matrix D , containing the degree of association among terms and domains , as illustrated in Table 1 .</sentence>
				<definiendum id="0">DM</definiendum>
				<definiens id="0">containing the degree of association among terms and domains</definiens>
			</definition>
			<definition id="3">
				<sentence>where IIDF is a k k diagonal matrix such that iIDFi , i = IDF ( wi ) , vectortj is represented as a row vector , and IDF ( wi ) is the Inverse Document Frequency of wi .</sentence>
				<definiendum id="0">IIDF</definiendum>
				<definiendum id="1">IDF</definiendum>
				<definiens id="0">a k k diagonal matrix such that iIDFi , i = IDF ( wi )</definiens>
			</definition>
			<definition id="4">
				<sentence>LSA is an unsupervised technique for estimating the similarity among texts and terms in a corpus .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">an unsupervised technique for estimating the similarity among texts and terms in a corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>LSA is performed by means of a Singular Value Decomposition ( SVD ) of the term-by-document matrix T describing the corpus .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">performed by means of a Singular Value Decomposition ( SVD ) of the term-by-document matrix T describing the corpus</definiens>
			</definition>
			<definition id="6">
				<sentence>The latter captures the syntagmatic aspects of sense distinction and it is composed by two kernels : the collocation kernel ( KColl ) and is equivalent to a Latent Semantic Space ( Deerwester et al. , 1990 ) .</sentence>
				<definiendum id="0">Latent Semantic Space</definiendum>
				<definiens id="0">captures the syntagmatic aspects of sense distinction and it is composed by two kernels : the collocation kernel ( KColl ) and is equivalent to a</definiens>
			</definition>
			<definition id="7">
				<sentence>The Domain Kernel estimates the similarity among the topics ( domains ) of two texts , so to capture domain aspects of sense distinction .</sentence>
				<definiendum id="0">Domain Kernel</definiendum>
				<definiens id="0">estimates the similarity among the topics ( domains ) of two texts , so to capture domain aspects of sense distinction</definiens>
			</definition>
			<definition id="8">
				<sentence>The Domain Kernel is de ned by KD ( ti , tj ) = hD ( ti ) , D ( tj ) iradicalbighD ( t i ) , D ( tj ) ihD ( ti ) , D ( tj ) i ( 4 ) where D is the Domain Mapping de ned in equation 1 .</sentence>
				<definiendum id="0">Domain Kernel</definiendum>
				<definiendum id="1">D</definiendum>
				<definiens id="0">the Domain Mapping de ned in equation 1</definiens>
			</definition>
			<definition id="9">
				<sentence>A more traditional approach to detect topic ( domain ) similarity is to extract Bag-of-Words ( BoW ) features from a large window of text around the word to be disambiguated .</sentence>
				<definiendum id="0">similarity</definiendum>
				<definiens id="0">to extract Bag-of-Words ( BoW ) features from a large window of text around the word to be disambiguated</definiens>
			</definition>
			<definition id="10">
				<sentence>Formally , let V be the vocabulary , the feature space associated with the gap-weighted subsequence kernel of length n is indexed by a set I of subsequences over V of length n. The ( explicit ) mapping function is de ned by φnu ( s ) = summationdisplay i : u=s ( i ) λl ( i ) , u 2 V n ( 5 ) where u = s ( i ) is a subsequence of s in the positions given by the tuple i , l ( i ) is the length spanned by u , and λ 2 ] 0 , 1 ] is the decay factor used to penalize non-contiguous subsequences .</sentence>
				<definiendum id="0">l ( i )</definiendum>
				<definiens id="0">a subsequence of s in the positions given by the tuple i</definiens>
				<definiens id="1">the length spanned by u</definiens>
			</definition>
			<definition id="11">
				<sentence>In analogy we de ned the PoS kernel KnPoS , by setting s to the sequence of PoSs p−3 , p−2 , p−1 , p0 , p+1 , p+2 , p+3 , where p0 is the PoS of the word to be disambiguated .</sentence>
				<definiendum id="0">p0</definiendum>
				<definiens id="0">the PoS of the word to be disambiguated</definiens>
			</definition>
			<definition id="12">
				<sentence>The Domain kernel exploits Domain Models , acquired from external untagged corpora , to estimate the similarity among the contexts of the words to be disambiguated .</sentence>
				<definiendum id="0">Domain kernel</definiendum>
				<definiens id="0">exploits Domain Models , acquired from external untagged corpora , to estimate the similarity among the contexts of the words to be disambiguated</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>We used Minipar ( Lin 1994 ) , a broad coverage parser , to analyze text .</sentence>
				<definiendum id="0">Minipar</definiendum>
				<definiens id="0">a broad coverage parser , to analyze text</definiens>
			</definition>
			<definition id="1">
				<sentence>Precision and Recall ( Shared and Committee ) vs. Number of Returned Attachments 1 12345 K Precision ( Shared ) Recall ( Shared ) Precision ( Committee ) Recall ( Committee ) Precision and Recall ( Full and Subset ) vs. Number of Returned Attachments 1 12345 K Precision ( Full ) Recall ( Full ) Precision ( Subset ) Recall ( Subset ) Figure 6 .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiendum id="1">Recall ( Shared</definiendum>
				<definiendum id="2">K Precision ( Full ) Recall ( Full ) Precision ( Subset ) Recall</definiendum>
				<definiens id="0">and Committee ) vs. Number of Returned Attachments 1 12345 K Precision ( Shared ) Recall ( Shared ) Precision ( Committee ) Recall ( Committee ) Precision and Recall ( Full and Subset ) vs. Number of Returned</definiens>
			</definition>
</paper>

		<paper id="1076">
			<definition id="0">
				<sentence>Our method has been implemented using a decision-tree classifier which tests for the presence of grammatical relations ( GRs ) in the output of the RASP ( Robust Accurate Statistical Parsing ) system ( Briscoe and Carroll , 2002 ) .</sentence>
				<definiendum id="0">RASP</definiendum>
				<definiens id="0">implemented using a decision-tree classifier which tests for the presence of grammatical relations ( GRs ) in the output of the</definiens>
			</definition>
			<definition id="1">
				<sentence>Our acquisition process consists of two main steps : 1 ) extracting GRs from corpus data , and 2 ) feeding the GRs as input to the classifier which incrementally matches parts of the GR sets to decide which branches of a decision-tree to follow .</sentence>
				<definiendum id="0">acquisition process</definiendum>
				<definiens id="0">consists of two main steps : 1 ) extracting GRs from corpus data , and 2 ) feeding the GRs as input to the classifier which incrementally matches parts of the GR sets to decide which branches of a decision-tree to follow</definiens>
			</definition>
			<definition id="2">
				<sentence>The NP headed by “examples” is marked as the subject of the frame by ncsubj ( be [ 6 ] examples [ 2 ] ) , and ncsubj ( comprehend [ 12 ] we+ [ 10 ] ) corresponds to the coindexation marked by 3 : the subject of the 3The format is slightly more complicated than that shown in ( Carroll et al. , 1998a ) : each argument that corresponds to a word consists of three parts : the lexeme , the part of speech tag , and the position ( index ) of the word in the sentence .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiendum id="1">ncsubj</definiendum>
			</definition>
			<definition id="3">
				<sentence>Token recall is the proportion of annotated ( sentence , frame ) pairs that the system recovered correctly .</sentence>
				<definiendum id="0">Token recall</definiendum>
				<definiens id="0">the proportion of annotated ( sentence , frame ) pairs that the system recovered correctly</definiens>
			</definition>
			<definition id="4">
				<sentence>Token precision is the proportion of classified ( sentence , frame ) pairs that were correct .</sentence>
				<definiendum id="0">Token precision</definiendum>
				<definiens id="0">the proportion of classified ( sentence , frame ) pairs that were correct</definiens>
			</definition>
			<definition id="5">
				<sentence>The F-measure ( β = 1 ) is a weighted combination of precision and recall .</sentence>
				<definiendum id="0">F-measure</definiendum>
				<definiens id="0">a weighted combination of precision and recall</definiens>
			</definition>
			<definition id="6">
				<sentence>Generalization ( anti-unification ) is an intersection operation on two structures which retains the features common to both ; generalization over the sets of GRs associated with the sentences which instantiate a particular frame can produce a pattern such as we used for classification in the experiments described above .</sentence>
				<definiendum id="0">Generalization ( anti-unification )</definiendum>
				<definiens id="0">an intersection operation on two structures which retains the features common to both ; generalization over the sets of GRs associated with the sentences which instantiate a particular</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>Q/A dialogues have been evaluated in terms of ( 1 ) efficiency , defined as the number of questions that the user must pose to find particular information , ( 2 ) effectiveness , defined by the relevance of the answers returned , ( 3 ) user satisfaction .</sentence>
				<definiendum id="0">Q/A dialogues</definiendum>
				<definiens id="0">evaluated in terms of ( 1 ) efficiency , defined as the number of questions that the user must pose to find particular information , ( 2 ) effectiveness , defined by the relevance of the answers returned , ( 3 ) user satisfaction</definiens>
			</definition>
			<definition id="1">
				<sentence>The PDN consists of a large set of questions that were asked or predicted for each topic .</sentence>
				<definiendum id="0">PDN</definiendum>
				<definiens id="0">consists of a large set of questions that were asked or predicted for each topic</definiens>
			</definition>
			<definition id="2">
				<sentence>In addition , real-world scenarios also need to identify certain operational parameters as well , such as the identity of the scenario’s sponsor ( i.e. the organization sponsoring the research ) and audience ( i.e. the organization receiving the information ) , as well as a series of evidence conditions which specify how much verification information must be subject to before it can be accepted as fact .</sentence>
				<definiendum id="0">audience</definiendum>
				<definiens id="0">the organization receiving the information</definiens>
			</definition>
			<definition id="3">
				<sentence>Subtopic : Egypt’s production of toxins and BW agents Topic Signature : produce − phosphorous trichloride ( TOXIN ) house − ORGANIZATION cultivate − non−pathogenic Bacilus Subtilis ( TOXIN ) produce − mycotoxins ( TOXIN ) acquire − FACILITY Subtopic : Egypt’s allies and partners Topic Signature : provide − COUNTRY cultivate − COUNTRY supply − precursors cooperate − COUNTRY train − PERSON supply − know−how Figure 3 : Example of two topic signatures acquired for the scenario illustrated in Figure 2 .</sentence>
				<definiendum id="0">Subtopic</definiendum>
				<definiens id="0">Egypt’s production of toxins and BW agents Topic Signature : produce − phosphorous trichloride ( TOXIN ) house − ORGANIZATION cultivate − non−pathogenic Bacilus Subtilis ( TOXIN ) produce − mycotoxins ( TOXIN ) acquire − FACILITY Subtopic : Egypt’s allies and partners Topic Signature : provide − COUNTRY cultivate − COUNTRY supply − precursors cooperate − COUNTRY train − PERSON supply − know−how Figure 3 : Example of two topic signatures acquired for the scenario illustrated in Figure 2</definiens>
			</definition>
			<definition id="4">
				<sentence>Under this approach , 208 the score associated with each relation is given by : a0a2a1a4a3 a0a6a5 a7 a0a9a8a11a10 a12a14a13a16a15a18a17a20a19a22a21 a10a11 a10a24a23a26a25a28a27a14a29a9a30 a3a32a31a6a33 a7 a0a6a8 , where a34a9a35a34 represents the cardinality of the documents where the relation is identified , and a3a32a31a9a33 a7 a0a9a8 represents support associated with the relation a0 .</sentence>
				<definiendum id="0">a34a9a35a34</definiendum>
				<definiens id="0">the cardinality of the documents where the relation is identified , and a3a32a31a9a33 a7 a0a9a8 represents support associated with the relation a0</definiens>
			</definition>
			<definition id="5">
				<sentence>a0 Similarity Metric 1 is based on two processing steps : ( a ) the content words of the questions are weighted using the a0a2a1a4a3 a45 a1 measure used in Information Retrieval a5 a1 a10 a5 a7a0 a1 a8 a10 a7 a49a7a6 a25a28a27a14a29 a7 a0a8a1a2a1a90a8a57a8a10a9a12a11a2a13a15a14 a38a17a16 a63 , where a18 is the number of questions in the QUAB , a45 a1 a1 is the number of questions containing a0 a1 and a0a2a1a2a1 is the number of times a0 a1 appears in the question .</sentence>
				<definiendum id="0">a18</definiendum>
				<definiens id="0">based on two processing steps : ( a ) the content words of the questions are weighted using the a0a2a1a4a3 a45 a1 measure used in Information Retrieval a5 a1 a10 a5 a7a0 a1 a8 a10 a7 a49a7a6 a25a28a27a14a29 a7 a0a8a1a2a1a90a8a57a8a10a9a12a11a2a13a15a14 a38a17a16 a63 , where</definiens>
				<definiens id="1">the number of questions containing a0 a1 and a0a2a1a2a1 is the number of times a0 a1 appears in the question</definiens>
			</definition>
			<definition id="6">
				<sentence>5 The resultant similarity score was based on three variables : a83 = sum of the weights of all concepts matched between a user query ( a84 a13 ) and a QUAB query ( a84a86a85 ) ; a87 = sum of the weights of all unmatched concepts in a84 a13 ; a88 = sum of the weights of all unmatched concepts in a84a86a85 ; The similarity between a84 a13 and a84a89a85 was calculated as a83 a51 a7a33 a13 a39 a87 a8 a51 a7a33 a85 a39 a88 a8 , where a33 a13 and a33 a85 were used as coefficients to penalize the contribution of unmatched concepts in a84 a13 and a84a86a85 respectively .</sentence>
				<definiendum id="0">QUAB query</definiendum>
				<definiens id="0">a84a86a85 ) ; a87 = sum of the weights of all unmatched concepts in a84 a13 ; a88 = sum of the weights of all unmatched concepts in a84a86a85</definiens>
				<definiens id="1">a83 a51 a7a33 a13 a39 a87 a8 a51 a7a33 a85 a39 a88 a8 , where a33 a13 and a33 a85 were used as coefficients to penalize the contribution of unmatched concepts in a84</definiens>
			</definition>
			<definition id="7">
				<sentence>In this case , if the QUAB question ( a84 a85 ) that was deemed to be most similar to a user question ( a84 a13 ) under Similarity Metric 5 is contained in the cluster of QUAB questions deemed to be most similar to a84 a13 under Similarity Metric 6 , then a84a86a85 receives a cluster adjustment score in order to boost its ranking within its QUAB cluster .</sentence>
				<definiendum id="0">a84a86a85</definiendum>
				<definiens id="0">receives a cluster adjustment score in order to boost its ranking within its QUAB cluster</definiens>
			</definition>
			<definition id="8">
				<sentence>MRR is defined as a60a43 a37 a1 a70 a60 a60 a19 a63 , whree a0a8a1 is the lowest rank of any relevant answer for the a3 a1a3a2 user query7 .</sentence>
				<definiendum id="0">MRR</definiendum>
				<definiens id="0">a60a43 a37 a1 a70 a60 a60 a19 a63 , whree a0a8a1 is the lowest rank of any relevant answer for the a3 a1a3a2 user query7</definiens>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>Our experiments were conducted in the context of the ACE Information Extraction evaluations , and we will use the terminology of these evaluations : entity : an object or a set of objects in one of the semantic categories of interest mention : a reference to an entity ( typically , a noun phrase ) name mention : a reference by name to an entity nominal mention : a reference by a common noun or noun phrase to an entity relation : one of a specified set of relationships between a pair of entities The 2004 ACE evaluation had 7 types of entities , of which the most common were PER ( persons ) , ORG ( organizations ) , and GPE ( ‘geo-political entities’ – locations which are also political units , such as countries , counties , and cities ) .</sentence>
				<definiendum id="0">ACE Information Extraction</definiendum>
				<definiendum id="1">name mention</definiendum>
				<definiendum id="2">GPE</definiendum>
				<definiens id="0">an object or a set of objects in one of the semantic categories of interest mention</definiens>
			</definition>
			<definition id="1">
				<sentence>Our baseline name tagger consists of a HMM tagger augmented with a set of post-processing rules .</sentence>
				<definiendum id="0">baseline name tagger</definiendum>
				<definiens id="0">consists of a HMM tagger augmented with a set of post-processing rules</definiens>
			</definition>
			<definition id="2">
				<sentence>Our nominal mention tagger ( noun group recognizer ) is a maximum entropy tagger trained on the Chinese TreeBank from the University of Pennsylvania , supplemented by list matching .</sentence>
				<definiendum id="0">nominal mention tagger</definiendum>
				<definiens id="0">a maximum entropy tagger trained on the Chinese TreeBank from the University of Pennsylvania , supplemented by list matching</definiens>
			</definition>
			<definition id="3">
				<sentence>Each training / test example consists of the pair of mentions and the sequence of intervening words .</sentence>
				<definiendum id="0">test example</definiendum>
				<definiens id="0">consists of the pair of mentions and the sequence of intervening words</definiens>
			</definition>
			<definition id="4">
				<sentence>H i is the hypotheses set for S i h ij is the j-th hypothesis in S i M ij is the mention set for h ij m ijk is the k-th mention in M ij e ijk is the entity which m ijk belongs to according to the current reference resolution results For each mention we compute seven quantities based on the results of name tagging and reference resolution : 1 This constraint is relaxed for parallel structures such as “mention1 , mention2 , [ and ] mention3….”</sentence>
				<definiendum id="0">e ijk</definiendum>
				<definiens id="0">the mention set for h ij m ijk is the k-th mention in M ij</definiens>
			</definition>
			<definition id="5">
				<sentence>413 CorefNum ijk is the number of mentions in e ijk WeightSum ijk is the sum of all the link weights between m ijk and other mentions in e ijk , 0.8 for name-name coreference ; 0.5 for apposition ; FirstMention ijk is 1 if m ijk is the first name mention in the entity ; otherwise 0 Head ijk is 1 if m ijk includes the head word of name ; otherwise 0 Withoutidiom ijk is 1 if m ijk is not part of an idiom ; otherwise 0 PERContext ijk is the number of PER context words around a PER name such as a title or an action verb involving a PER ORGSuffix ijk is 1 if ORG m ijk includes a suffix word ; otherwise 0 The first three capture evidence of the correctness of a name provided by reference resolution ; for example , a name which is coreferenced with more other mentions is more likely to be correct .</sentence>
				<definiendum id="0">FirstMention ijk</definiendum>
				<definiens id="0">the sum of all the link weights between m ijk and other mentions in e ijk , 0.8 for name-name coreference</definiens>
			</definition>
			<definition id="6">
				<sentence>train , where d includes n sentences S 1 …S n For i = 1…n , let m = the number of hypotheses for S i ( 1 ) Pre-prune the candidate hypotheses using the HMM margin ( 2 ) For each hypothesis h ij , j = 1…m ( a ) Compare h ij with the key , set the prediction Value ij “Best” or “Not Best” ( b ) Run the Coreference Resolver on h ij and the best hypothesis for each of the other sentences , generate entity results for each candidate name in h ij ( c ) Generate a coreference feature vector V ij for h ij ( d ) Output V ij and Value ij 2 We set different N = 5 , 10 , 20 or 30 for different margin ranges , by crossvalidation checking the training data about the ranking position of the best hypothesis for each sentence .</sentence>
				<definiendum id="0">c ) Generate</definiendum>
				<definiens id="0">a coreference feature vector V ij for h ij ( d ) Output V ij and Value ij 2 We set different N = 5 , 10 , 20 or 30 for different margin ranges , by crossvalidation checking the training data about the ranking position of the best hypothesis for each sentence</definiens>
			</definition>
</paper>

		<paper id="3012">
			<definition id="0">
				<sentence>The COMIC DAM ( Catizone et al. , 2003 ) is a general-purpose dialogue manager which can handle different dialogue management styles such as system-driven , user-driven or mixed-initiative .</sentence>
				<definiendum id="0">COMIC DAM</definiendum>
				<definiens id="0">a general-purpose dialogue manager which can handle different dialogue management styles such as system-driven , user-driven or mixed-initiative</definiens>
			</definition>
			<definition id="1">
				<sentence>The general-purpose part of the DAM is a simple stack architecture with a control structure ; all the application-dependent information is stored in a variation of Augmented Transition Networks ( ATNs ) called Dialogue Action Forms ( DAFs ) .</sentence>
				<definiendum id="0">DAM</definiendum>
			</definition>
			<definition id="2">
				<sentence>The DAM uses the system ontology to retrieve designs according to the chosen feature , and consults the user model and dialogue history to narrow down the resulting designs to a small set to be shown and described to the user .</sentence>
				<definiendum id="0">DAM</definiendum>
				<definiens id="0">uses the system ontology to retrieve designs</definiens>
			</definition>
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>IBM translation models try to model the translation probability Pr ( fJ1 |eI1 ) , which describes the relationship between a source language sentence eI1 and a target language sentence fJ1 .</sentence>
				<definiendum id="0">IBM translation models</definiendum>
				<definiens id="0">describes the relationship between a source language sentence eI1 and a target language sentence fJ1</definiens>
			</definition>
			<definition id="1">
				<sentence>n ( φi|ei ) × mproductdisplay j=1 t ( fj|eaj ) d ( j|aj , l , m ) ( 7 ) We distinguish between two translation directions to use Model 3 as feature functions : treating English as source language and French as target language or vice versa .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">treating English as source language and French as target language or vice versa</definiens>
			</definition>
			<definition id="2">
				<sentence>e is a source language word , f is a target langauge word , and conf is a positive real-valued number ( usually , conf = 1.0 ) assigned 461 by lexicographers to evaluate the validity of the entry .</sentence>
				<definiendum id="0">e</definiendum>
				<definiendum id="1">f</definiendum>
				<definiendum id="2">conf</definiendum>
				<definiens id="0">a source language word ,</definiens>
			</definition>
			<definition id="3">
				<sentence>After the bilingual sentences in the development corpus are tokenized ( or segmented ) and POS tagged , they can be used to train POS tags transition probabilities by counting relative frequencies : p ( fT|eT ) = NA ( fT , eT ) N ( eT ) Here , NA ( fT , eT ) is the frequency that the POS tag fT is aligned to POS tag eT and N ( eT ) is the frequency of eT in the development corpus .</sentence>
				<definiendum id="0">NA ( fT , eT )</definiendum>
				<definiendum id="1">N ( eT )</definiendum>
				<definiens id="0">the frequency of eT in the development corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>A gain is defined as follows : gain ( a , l ) = exp [ summationtextM m=1 λmhm ( a∪l , e , f ) ] exp [ summationtextMm=1 λmhm ( a , e , f ) ] ( 12 ) where l = ( i , j ) is a link added to a. The greedy search algorithm for general loglinear models is formally described as follows : Input : e , f , eT , fT , and D Output : a Compute gain ( a , l ) to a. 462 The above search algorithm , however , is not efficient for our log-linear models .</sentence>
				<definiendum id="0">Compute gain</definiendum>
				<definiens id="0">follows : gain ( a , l ) = exp [ summationtextM m=1 λmhm ( a∪l , e , f ) ] exp [ summationtextMm=1 λmhm ( a , e</definiens>
				<definiens id="1">a link added to a. The greedy search algorithm for general loglinear models is formally described as follows : Input : e , f , eT , fT , and D Output : a</definiens>
			</definition>
			<definition id="5">
				<sentence>Gain threshold t is a real-valued number , which can be optimized on the development corpus .</sentence>
				<definiendum id="0">Gain threshold t</definiendum>
				<definiens id="0">a real-valued number , which can be optimized on the development corpus</definiens>
			</definition>
			<definition id="6">
				<sentence>Ney , 2003 ) for scoring the viterbi alignments of each model against gold-standard annotated alignments : precision = |A∩P||A| recall = |A∩S||S| AER = 1− |A∩S|+|A∩P||A|+|S| where A is the set of word pairs aligned by word alignment systems , S is the set marked in the gold standard as ”sure” and P is the set marked as ”possible” ( including the ”sure” pairs ) .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">scoring the viterbi alignments of each model against gold-standard annotated alignments : precision = |A∩P||A| recall = |A∩S||S| AER = 1− |A∩S|+|A∩P||A|+|S| where A is the set of word pairs aligned by word alignment systems</definiens>
			</definition>
			<definition id="7">
				<sentence>The base feature of our log-linear models , IBM Model 3 , takes the parameters generated by GIZA++ as parameters for itself .</sentence>
				<definiendum id="0">IBM Model 3</definiendum>
				<definiens id="0">takes the parameters generated by GIZA++ as parameters for itself</definiens>
			</definition>
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>Support Vector Machines ( SVMs ) are a supervised machine learning technique motivated by the statistical learning theory ( Vapnik 1998 ) .</sentence>
				<definiendum id="0">Support Vector Machines ( SVMs )</definiendum>
				<definiens id="0">a supervised machine learning technique motivated by the statistical learning theory ( Vapnik 1998 )</definiens>
			</definition>
			<definition id="1">
				<sentence>For efficiency , we apply the one vs. others strategy , which builds K classifiers so as to separate one class from all others , instead of the pairwise strategy , which builds K* ( K-1 ) /2 classifiers considering all pairs of classes .</sentence>
				<definiendum id="0">pairwise strategy</definiendum>
				<definiens id="0">builds K classifiers so as to separate one class from all others</definiens>
			</definition>
			<definition id="2">
				<sentence>• WM2 : bag-of-words in M2 • HM2 : head word of M2 • HM12 : combination of HM1 and HM2 • WBNULL : when no word in between • WBFL : the only word in between when only one word in between • WBF : first word in between when at least two words in between • WBL : last word in between when at least two words in between • WBO : other words in between except first and last words when at least three words in between • BM1F : first word before M1 • BM1L : second word before M1 • AM2F : first word after M2 • AM2L : second word after M2 This feature concerns about the entity type of both the mentions , which can be PERSON , ORGANIZATION , FACILITY , LOCATION and Geo-Political Entity or GPE : • ET12 : combination of mention entity types This feature considers the entity level of both the mentions , which can be NAME , NOMIAL and PRONOUN : • ML12 : combination of mention levels This category of features includes : • # MB : number of other mentions in between • # WB : number of words in between • M1 &gt; M2 or M1 &lt; M2 : flag indicating whether M2/M1is included in M1/M2. Normally , the above overlap features are too general to be effective alone. Therefore , they are also combined with other features : 1 ) ET12+M1 &gt; M2 ; 2 ) ET12+M1 &lt; M2 ; 3 ) HM12+M1 &gt; M2 ; 4 ) HM12+M1 &lt; M2. It is well known that chunking plays a critical role in the Template Relation task of the 7 th Message Understanding Conference ( MUC-7 1998 ) . The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper , we separate the features of base 429 phrase chunking from those of full parsing. In this way , we can separately evaluate the contributions of base phrase chunking and full parsing. Here , the base phrase chunks are derived from full parse trees using the Perl script 5 written by Sabine Buchholz from Tilburg University and the Collins’ parser ( Collins 1999 ) is employed for full parsing. Most of the chunking features concern about the head words of the phrases between the two mentions. Similar to word features , three categories of phrase heads are considered : 1 ) the phrase heads in between are also classified into three bins : the first phrase head in between , the last phrase head in between and other phrase heads in between ; 2 ) the phrase heads before M1 are classified into two bins : the first phrase head before and the second phrase head before ; 3 ) the phrase heads after M2 are classified into two bins : the first phrase head after and the second phrase head after. Moreover , we also consider the phrase path in between. • CPHBNULL when no phrase in between • CPHBFL : the only phrase head when only one phrase in between • CPHBF : first phrase head in between when at least two phrases in between • CPHBL : last phrase head in between when at least two phrase heads in between • CPHBO : other phrase heads in between except first and last phrase heads when at least three phrases in between • CPHBM1F : first phrase head before M1 • CPHBM1L : second phrase head before M1 • CPHAM2F : first phrase head after M2 • CPHAM2F : second phrase head after M2 • CPP : path of phrase labels connecting the two mentions in the chunking • CPPH : path of phrase labels connecting the two mentions in the chunking augmented with head words , if at most two phrases in between This category of features includes information about the words , part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins’ parser and linking all the other 5 http : //ilk.kub.nl/~sabine/chunklink/ fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP. • ET1DW1 : combination of the entity type and the dependent word for M1 • H1DW1 : combination of the head word and the dependent word for M1 • ET2DW2 : combination of the entity type and the dependent word for M2 • H2DW2 : combination of the head word and the dependent word for M2 • ET12SameNP : combination of ET12 and whether M1 and M2 included in the same NP • ET12SamePP : combination of ET12 and whether M1 and M2 exist in the same PP • ET12SameVP : combination of ET12 and whether M1 and M2 included in the same VP This category of features concerns about the information inherent only in the full parse tree. • PTP : path of phrase labels ( removing duplicates ) connecting M1 and M2 in the parse tree • PTPH : path of phrase labels ( removing duplicates ) connecting M1 and M2 in the parse tree augmented with the head word of the top phrase in the path. Semantic information from various resources , such as WordNet , is used to classify important words into different semantic lists according to their indicating relationships. Country Name List This is to differentiate the relation subtype “ROLE.Citizen-Of” , which defines the relationship between a person and the country of the person’s citizenship , from other subtypes , especially “ROLE.Residence” , where defines the relationship between a person and the location in which the person lives. Two features are defined to include this information : • ET1Country : the entity type of M1 when M2 is a country name • CountryET2 : the entity type of M2 when M1 is a country name 430 Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE : Parent , Grandparent , Spouse , Sibling , Other-Relative and OtherPersonal. This trigger word list is first gathered from WordNet by checking whether a word has the semantic class “person|…|relative”. Then , all the trigger words are semi-automatically 6 classified into different categories according to their related personal social relation subtypes. We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships. Two features are defined to include this information : • ET1SC2 : combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype. • SC1ET2 : combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype. This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system. The ACE corpus is gathered from various newspapers , newswire and broadcasts. In this paper , we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number. We use the official ACE corpus from LDC. The training set consists of 674 annotated text documents ( ~300k words ) and 9683 instances of relations. During development , 155 of 674 documents in the training set are set aside for fine-tuning the system. The testing set is held out only for final evaluation. It consists of 97 documents ( ~50k words ) and 1386 instances of relations. Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization ( RDC ) task , along with their frequency of occurrence in the ACE training set. It shows that the 6 Those words that have the semantic classes “Parent” , “GrandParent” , “Spouse” and “Sibling” are automatically set with the same classes without change. However , The remaining words that do not have above four classes are manually classified. ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype “Founder” under the type “ROLE”. It also shows that the ACE RDC task defines some difficult subtypes such as the subtypes “Based-In” , “Located” and “Residence” under the type “AT” , which are difficult even for human experts to differentiate. Type Subtype Freq AT ( 2781 ) Based-In 347 Located 2126 Residence 308 NEAR ( 201 ) Relative-Location 201 PART ( 1298 ) Part-Of 947 Subsidiary 355 Other 6 ROLE ( 4756 ) Affiliate-Partner 204 Citizen-Of 328 Client 144 Founder 26 General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL ( 827 ) Associate 91 Grandparent 12 Other-Personal 85 Other-Professional 339 Other-Relative 78 Parent 127 Sibling 18 Spouse 77 Table 1 : Relation types and subtypes in the ACE training data In this paper , we explicitly model the argument order of the two mentions involved. For example , when comparing mentions m1 and m2 , we distinguish between m1-ROLE.Citizen-Of-m2 and m2ROLE.Citizen-Of-m1. Note that only 6 of these 24 relation subtypes are symmetric : “RelativeLocation” , “Associate” , “Other-Relative” , “OtherProfessional” , “Sibling” , and “Spouse”. In this way , we model relation extraction as a multi-class classification problem with 43 classes , two for each relation subtype ( except the above 6 symmetric subtypes ) and a “NONE” class for the case where the two mentions are not related. In this paper , we only measure the performance of relation extraction on “true” mentions with “true” chaining of coreference ( i.e. as annotated by the corpus annotators ) in the ACE corpus. Table 2 measures the performance of our relation extrac431 tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1 % /49.5 % / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features. Table 2 also measures the contributions of different features by gradually increasing the feature set. It shows that : Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2 : Contribution of different features over 43 relation subtypes in the test data • Using word features only achieves the performance of 69.2 % /23.7 % /35.3 in precision/recall/Fmeasure. • Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase. • The usefulness of mention level features is quite limited. It only improves the F-measure by 0.8 due to the recall increase. • Incorporating the overlap features gives some balance between precision and recall. It increases the F-measure by 3.6 with a big precision decrease and a big recall increase. • Chunking features are very useful. It increases the precision/recall/F-measure by 4.1 % /5.6 % / • To our surprise , incorporating the dependency tree and parse tree features only improve the Fmeasure by 0.6 and 0.4 respectively. This may be due to the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70 % of relations exist where two mentions are embedded in each other or separated by at most one word. While short-distance relations dominate and can be resolved by above simple features , the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations. However , full parsing is always prone to long distance errors although the Collins’ parser used in our system represents the state-of-the-art in full parsing. • Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by tion subtype “ROLE.Citizen-Of” from “ROLE. Residence” by distinguishing country GPEs from other GPEs. The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes. Table 4 separately measures the performance of different relation types and major subtypes. It also indicates the number of testing instances , the number of correctly classified instances and the number of wrongly classified instances for each type or subtype. It is not surprising that the performance on the relation type “NEAR” is low because it occurs rarely in both the training and testing data. Others like “PART.Subsidary” and “SOCIAL. Other-Professional” also suffer from their low occurrences. It also shows that our system performs best on the subtype “SOCIAL.Parent” and “ROLE. Citizen-Of”. This is largely due to incorporation of two semantic resources , i.e. the country name list and the personal relative trigger word list. Table 4 also indicates the low performance on the relation type “AT” although it frequently occurs in both the training and testing data. This suggests the difficulty of detecting and classifying the relation type “AT” and its subtypes. Table 5 separates the performance of relation detection from overall performance on the testing set. It shows that our system achieves the performance of 84.8 % /66.7 % /74.7 in precision/recall/Fmeasure on relation detection. It also shows that our system achieves overall performance of 77.2 % /60.7 % /68.0 and 63.1 % /49.5 % /55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus. It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g. WordNet and gazetteers ) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al ( 2004 ) are yet to be effective on the ACE RDC task. Finally , Table 6 shows the distributions of errors. It shows that 73 % ( 627/864 ) of errors results 432 from relation detection and 27 % ( 237/864 ) of errors results from relation characterization , among which 17.8 % ( 154/864 ) of errors are from misclassification across relation types and 9.6 % ( 83/864 ) of errors are from misclassification of relation subtypes inside the same relation types. This suggests that relation detection is critical for relation extraction. # of other mentions in between # of relations 0 1 2 3 &gt; =4 Overall 0 3991 161 11 0 0 4163 1 2350 315 26 2 0 2693 2 465 95 7 2 0 569 3 311 234 14 0 0 559 4 204 225 29 2 3 463 5 111 113 38 2 1 265 &gt; =6 262 297 277 148 134 1118 # of the words in between Overall 7694 1440 402 156 138 9830 Table 3 : Distribution of relations over # words and # other mentions in between in the training data Type Subtype # Testing Instances # Correct # Error P R F AT 392 224 105 68.1 57.1 62.1 Based-In 85 39 10 79.6 45.9 58.2 Located 241 132 120 52.4 54.8 53.5 Residence 66 19 9 67.9 28.8 40.4 NEAR 35 8 1 88.9 22.9 36.4 Relative-Location 35 8 1 88.9 22.9 36.4 PART 164 106 39 73.1 64.6 68.6 Part-Of 136 76 32 70.4 55.9 62.3 Subsidiary 27 14 23 37.8 51.9 43.8 ROLE 699 443 82 84.4 63.4 72.4 Citizen-Of 36 25 8 75.8 69.4 72.6 General-Staff 201 108 46 71.1 53.7 62.3 Management 165 106 72 59.6 64.2 61.8 Member 224 104 36 74.3 46.4 57.1 SOCIAL 95 60 21 74.1 63.2 68.5 Other-Professional 29 16 32 33.3 55.2 41.6 Parent 25 17 0 100 68.0 81.0 Table 4 : Performance of different relation types and major subtypes in the test data Relation Detection RDC on Types RDC on Subtypes System P R F P R F P R F Ours : feature-based 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5 Kambhatla ( 2004 ) : feature-based 63.5 45.2 52.8 Culotta et al ( 2004 ) : tree kernel 81.2 51.8 63.2 67.1 35.0 45.8 Table 5 : Comparison of our system with other best-reported systems on the ACE corpus Error Type # Errors False Negative 462 Detection Error False Positive 165 Cross Type Error 154 Characterization Error Inside Type Error 83 Table 6 : Distribution of errors In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed .</sentence>
				<definiendum id="0">“AT”</definiendum>
				<definiendum id="1">Ours</definiendum>
				<definiens id="0">second word after M2 This feature concerns about the entity type of both the mentions , which can be PERSON , ORGANIZATION , FACILITY , LOCATION and Geo-Political Entity or GPE : • ET12 : combination of mention entity types This feature considers the entity level of both the mentions , which can be NAME , NOMIAL and PRONOUN : • ML12 : combination of mention levels This category of features includes : • # MB : number of other mentions in between • # WB : number of words in between • M1 &gt; M2 or M1 &lt; M2 : flag indicating whether M2/M1is included in M1/M2. Normally , the above overlap features are too general to be effective alone. Therefore</definiens>
				<definiens id="1">the contributions of base phrase chunking and full parsing. Here , the base phrase chunks are derived from full parse trees using the Perl script 5 written by Sabine Buchholz from Tilburg University and the Collins’ parser ( Collins 1999 ) is employed for full parsing. Most of the chunking features concern about the head words of the phrases between the two mentions. Similar to word features , three categories of phrase heads are considered : 1 ) the phrase heads in between are also classified into three bins : the first phrase head in between , the last phrase head in between and other phrase heads in between ; 2 ) the phrase heads before M1 are classified into two bins : the first phrase head before and the second phrase head before ; 3 ) the phrase heads after M2 are classified into two bins : the first phrase head after and the second phrase head after. Moreover , we also consider the phrase path in between. • CPHBNULL when no phrase in between • CPHBFL : the only phrase head when only one phrase in between • CPHBF : first phrase head in between when at least two phrases in between • CPHBL : last phrase head in between when at least two phrase heads in between • CPHBO : other phrase heads in between except first and last phrase heads when at least three phrases in between • CPHBM1F : first phrase head before M1 • CPHBM1L : second phrase head before M1 • CPHAM2F : first phrase head after M2 • CPHAM2F : second phrase head after M2 • CPP : path of phrase labels connecting the two mentions in the chunking • CPPH : path of phrase labels connecting the two mentions in the chunking augmented with head words , if at most two phrases in between This category of features includes information about the words , part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collins’ parser and linking all the other 5 http : //ilk.kub.nl/~sabine/chunklink/ fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP. • ET1DW1 : combination of the entity type and the dependent word for M1 • H1DW1 : combination of the head word and the dependent word for M1 • ET2DW2 : combination of the entity type and the dependent word for M2 • H2DW2 : combination of the head word and the dependent word for M2 • ET12SameNP : combination of ET12 and whether M1 and M2 included in the same NP • ET12SamePP : combination of ET12 and whether M1 and M2 exist in the same PP • ET12SameVP : combination of ET12 and whether M1 and M2 included in the same VP This category of features concerns about the information inherent only in the full parse tree. • PTP : path of phrase labels ( removing duplicates ) connecting M1 and M2 in the parse tree • PTPH : path of phrase labels ( removing duplicates ) connecting M1 and M2 in the parse tree augmented with the head word of the top phrase in the path. Semantic information from various resources , such as WordNet , is used to classify important words into different semantic lists according to their indicating relationships. Country Name List This is to differentiate the relation subtype “ROLE.Citizen-Of” , which defines the relationship between a person and the country of the person’s citizenship</definiens>
				<definiens id="2">the entity type of M1 when M2 is a country name • CountryET2 : the entity type of M2 when M1 is a country name 430 Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE : Parent , Grandparent , Spouse , Sibling , Other-Relative and OtherPersonal. This trigger word list is first gathered from WordNet by checking whether a word has the semantic class “person|…|relative”. Then , all the trigger words are semi-automatically 6 classified into different categories according to their related personal social relation subtypes. We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships. Two features are defined to include this information : • ET1SC2 : combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype. • SC1ET2 : combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype. This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system. The ACE corpus is gathered from various newspapers</definiens>
				<definiens id="3">relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number. We use the official ACE corpus from LDC. The training set consists of 674 annotated text documents ( ~300k words ) and 9683 instances of relations. During development , 155 of 674 documents in the training set are set aside for fine-tuning the system. The testing set is held out only for final evaluation. It consists of 97 documents ( ~50k words ) and 1386 instances of relations. Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization ( RDC ) task , along with their frequency of occurrence in the ACE training set. It shows that the 6 Those words that have the semantic classes “Parent” , “GrandParent” , “Spouse” and “Sibling” are automatically set with the same classes without change. However</definiens>
				<definiens id="4">annotated by the corpus annotators ) in the ACE corpus. Table 2 measures the performance of our relation extrac431 tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1 % /49.5 % / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features. Table 2 also measures the contributions of different features by gradually increasing the feature set. It shows that : Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level</definiens>
				<definiens id="5">different features over 43 relation subtypes in the test data • Using</definiens>
				<definiens id="6">the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70 % of relations exist where two mentions are embedded in each other or separated by at most one word. While short-distance relations dominate and can be resolved by above simple features , the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations. However , full parsing is always prone to long distance errors although the Collins’ parser used in our system represents the state-of-the-art in full parsing. • Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by tion subtype “ROLE.Citizen-Of” from “ROLE. Residence” by distinguishing country GPEs from other GPEs. The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes. Table 4 separately measures the performance of different relation types and major subtypes. It also indicates the number of testing instances , the number of correctly classified instances and the number of wrongly classified instances for each type or subtype. It is not surprising that the performance on the relation type “NEAR” is low because it occurs rarely in both the training and testing data. Others like “PART.Subsidary” and “SOCIAL. Other-Professional” also suffer from their low occurrences. It also shows that our system performs best on the subtype “SOCIAL.Parent” and “ROLE. Citizen-Of”. This is largely due to incorporation of two semantic resources , i.e. the country name list and the personal relative trigger word list. Table 4 also indicates the low performance on the relation type “AT” although it frequently occurs in both the training and testing data. This suggests the difficulty of detecting and classifying the relation type “AT”</definiens>
				<definiens id="7">precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus. It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g. WordNet and gazetteers ) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al ( 2004 ) are yet to be effective on the ACE RDC task. Finally , Table 6 shows the distributions of errors. It shows that 73 % ( 627/864 ) of errors results 432 from relation detection and 27 % ( 237/864 ) of errors results from relation characterization , among which 17.8 % ( 154/864 ) of errors are from misclassification across relation types and 9.6 % ( 83/864 ) of errors are from misclassification of relation subtypes inside the same relation types. This suggests that relation detection is critical for relation extraction. # of other mentions in between # of relations 0 1 2 3 &gt; =4 Overall 0 3991 161 11 0 0</definiens>
				<definiens id="8">Distribution of relations over # words and # other mentions in between in the training data Type Subtype # Testing Instances # Correct # Error P R F</definiens>
				<definiens id="9">Performance of different relation types and major subtypes in the test data Relation Detection RDC on Types RDC on Subtypes System P R F P R F P R F</definiens>
				<definiens id="10">tree kernel 81.2 51.8 63.2 67.1 35.0 45.8 Table 5 : Comparison of our system with other best-reported systems on the ACE corpus Error Type # Errors False Negative 462 Detection Error False Positive 165 Cross Type Error 154 Characterization Error Inside Type Error 83 Table 6 : Distribution of errors In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed</definiens>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>A likely cause is the relative productivity of German morphology compared to that of English : German has a higher type/token ratio for words , making sparse data problems more severe .</sentence>
				<definiendum id="0">likely cause</definiendum>
				<definiens id="0">German has a higher type/token ratio for words , making sparse data problems more severe</definiens>
			</definition>
			<definition id="1">
				<sentence>To maximize cases where all necessary nodes are dominated by the same parent , NEGRA uses flat ‘dependency-style’ rules .</sentence>
				<definiendum id="0">NEGRA</definiendum>
				<definiens id="0">uses flat ‘dependency-style’ rules</definiens>
			</definition>
			<definition id="2">
				<sentence>Under the NEGRA annotation scheme , the first sentence above would have a rule Sa0 NP-SB VVFIN NP-OA and the second , Sa0 NP-OA VVFIN NP-SB , where SB denotes subject and OA denotes accusative object .</sentence>
				<definiendum id="0">SB</definiendum>
			</definition>
			<definition id="3">
				<sentence>Automatic treebank transformations are an important step in developing an accurate unlexicalized parser ( Johnson , 1998 ; Klein and Manning , 2003 ) .</sentence>
				<definiendum id="0">Automatic treebank transformations</definiendum>
				<definiens id="0">an important step in developing an accurate unlexicalized parser</definiens>
			</definition>
			<definition id="4">
				<sentence>KON is the POS tag for a conjunct , and CJ denotes the function of the NP is a coordinate sister .</sentence>
				<definiendum id="0">KON</definiendum>
				<definiendum id="1">CJ</definiendum>
				<definiens id="0">a coordinate sister</definiens>
			</definition>
			<definition id="5">
				<sentence>The transformation does not add any extra nonterminals , rather it replaces rules such as S a0 KOUS NP V NP ( where KOUS is a complementizer POS tag ) with SBAR a0 KOUS NP V NP .</sentence>
				<definiendum id="0">KOUS</definiendum>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>The Inversion Transduction Grammar ( ITG ) of Wu ( 1997 ) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages .</sentence>
				<definiendum id="0">Inversion Transduction Grammar ( ITG</definiendum>
				<definiens id="0">a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages</definiens>
			</definition>
			<definition id="1">
				<sentence>475 Grammars An Inversion Transduction Grammar can generate pairs of sentences in two languages by recursively applying context-free bilingual production rules .</sentence>
				<definiendum id="0">Inversion Transduction Grammar</definiendum>
				<definiens id="0">generate pairs of sentences in two languages by recursively applying context-free bilingual production rules</definiens>
			</definition>
			<definition id="2">
				<sentence>A stochastic ITG can be thought of as a stochastic CFG extended to the space of bitext .</sentence>
				<definiendum id="0">stochastic ITG</definiendum>
				<definiens id="0">a stochastic CFG extended to the space of bitext</definiens>
			</definition>
			<definition id="3">
				<sentence>For instance , the probability of the parse in Figure 1 is : P ( S → A ) ·P ( A → [ CB ] ) ·P ( B → 〈CC〉 ) ·P ( C → I/Je ) ·P ( C → see/vois ) ·P ( C → them/les ) It is important to note that besides the bottomlevel word-pairing rules , the other rules are all nonlexical , which means the structural alignment component of the model is not sensitive to the lexical contents of subtrees .</sentence>
				<definiendum id="0">) ·P</definiendum>
				<definiens id="0">means the structural alignment component of the model is not sensitive to the lexical contents of subtrees</definiens>
			</definition>
			<definition id="4">
				<sentence>Now there are 4 forms of binary rules : X ( e/f ) → [ Y ( e/f ) Z ] X ( e/f ) → [ YZ ( e/f ) ] X ( e/f ) → 〈Y ( e/f ) Z〉 X ( e/f ) → 〈YZ ( e/f ) 〉 determined by the four possible combinations of head selections ( Y or Z ) and orientation selections ( straight or inverted ) .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">e/f ) → 〈Y ( e/f ) Z〉 X ( e/f ) → 〈YZ ( e/f ) 〉 determined by the four possible combinations of head selections ( Y or Z ) and orientation selections ( straight or inverted )</definiens>
			</definition>
			<definition id="5">
				<sentence>Assuming that the lengths of the source and target sentences are proportional , the algorithm has a complexity of O ( n8 ) , where n is the average length of the source and target sentences .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the average length of the source and target sentences</definiens>
			</definition>
			<definition id="6">
				<sentence>477 Algorithm 1 LexicalizedITG ( s , t ) for all l , m such that 0 ≤ l ≤ m ≤ Ns do for all i , j such that 0 ≤ i ≤ j ≤ Nt do for all e ∈ { el+1 ... em } do for all f ∈ { fi+1 ... fj } do for all n such that l ≤ n ≤ m do for all k such that i ≤ k ≤ j do for all rules X → YZ ∈ G do β ( X ( e/f ) , l , m , i , j ) += a3 straight rule , where Y is head P ( [ Y ( e/f ) Z ] | X ( e/f ) ) ·β ( Y ( e/f ) , l , n , i , k ) ·β ( Z , n , m , k , j ) a3 inverted rule , where Y is head + P ( 〈Y ( e/f ) Z〉 | X ( e/f ) ) ·β ( Y ( e/f ) , n , m , i , k ) ·β ( Z , l , n , k , j ) a3 straight rule , where Z is head + P ( [ YZ ( e/f ) ] | X ( e/f ) ) ·β ( Y , l , n , i , k ) ·β ( Z ( e/f ) , n , m , k , j ) a3 inverted rule , where Z is head + P ( 〈YZ ( e/f ) 〉 | X ( e/f ) ) ·β ( Y , n , m , i , k ) ·β ( Z ( e/f ) , l , n , k , j ) end for end for end for a3 word pair generation rule β ( X , l , m , i , j ) += P ( X ( e/f ) | X ) ·β ( X ( e/f ) , l , m , i , j ) end for end for end for end for We need to further restrict the space of alignments spanned by the source and target strings to make the algorithm feasible .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiendum id="2">X</definiendum>
				<definiens id="0">l , n , i , k ) ·β ( Z , n , m , k , j ) a3 inverted rule</definiens>
				<definiens id="1">l , n , k , j ) a3 straight rule , where Z is head + P ( [ YZ ( e/f ) ] |</definiens>
				<definiens id="2">n , m , k , j ) a3 inverted rule</definiens>
			</definition>
			<definition id="7">
				<sentence>λL1 , L2 is the probability of any word alignment template for a pair of L1word source string and L2-word target string , which we model as a uniform distribution of word-forword alignment patterns after a Poisson distribution of target string’s possible lengths , following Brown et al. ( 1993 ) .</sentence>
				<definiendum id="0">L2</definiendum>
				<definiens id="0">the probability of any word alignment template for a pair of L1word source string and L2-word target string , which we model as a uniform distribution of word-forword alignment patterns after a Poisson distribution of target string’s possible lengths</definiens>
			</definition>
			<definition id="8">
				<sentence>As an alternative , the summationtext operator can be replaced by the max operator as the inside operator over the translation probabilities above , meaning that we use the Model 1 Viterbi probability as our estimate , rather than the total Model 1 probability.2 A na¨ıve implementation would take O ( n6 ) steps of computation , because there are O ( n4 ) cells , each of which takes O ( n2 ) steps to compute its Model 1 probability .</sentence>
				<definiendum id="0">summationtext operator</definiendum>
			</definition>
			<definition id="9">
				<sentence>Let INS ( l , m , i , j ) denote the major factor of our Model 1 estimate of a cell’s inside probability , producttextt∈ ( i , j ) summationtexts∈ { 0 , ( l , m ) } t ( ft | es ) .</sentence>
				<definiendum id="0">INS</definiendum>
				<definiens id="0">the major factor of our Model 1 estimate of a cell’s inside probability</definiens>
			</definition>
			<definition id="10">
				<sentence>For scoring the Viterbi alignments of each system against goldstandard annotated alignments , we use the alignment error rate ( AER ) of Och and Ney ( 2000 ) , which measures agreement at the level of pairs of words : AER = 1 − |A∩GP| + |A∩GS||A| + |G S| where A is the set of word pairs aligned by the automatic system , GS is the set marked in the gold standard as “sure” , and GP is the set marked as “possible” ( including the “sure” pairs ) .</sentence>
				<definiendum id="0">AER</definiendum>
				<definiendum id="1">GS</definiendum>
				<definiendum id="2">GP</definiendum>
				<definiens id="0">the set of word pairs aligned by the automatic system ,</definiens>
				<definiens id="1">the set marked as “possible” ( including the “sure” pairs )</definiens>
			</definition>
</paper>

		<paper id="2009">
			<definition id="0">
				<sentence>Learning is automatic , with neither manual selection of best patterns , nor expert validation of patterns .</sentence>
				<definiendum id="0">Learning</definiendum>
				<definiens id="0">automatic , with neither manual selection of best patterns , nor expert validation of patterns</definiens>
			</definition>
			<definition id="1">
				<sentence>In training , terms were labelled using MMTx , which uses lexical variant generation to map noun phrases to candidate terms and concepts attested in a terminology database .</sentence>
				<definiendum id="0">MMTx</definiendum>
				<definiens id="0">uses lexical variant generation to map noun phrases to candidate terms and concepts attested in a terminology database</definiens>
			</definition>
			<definition id="2">
				<sentence>Configuration TERM continued to label terms in the training phase , but did not label new terms found during iteration ( as discussed in Section 3.1 ) .</sentence>
				<definiendum id="0">Configuration TERM</definiendum>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>Wu and Jiang ( 2000 ) calculated P ( Cat , Pos , Len ) for each character , where Cat is the POS of a word containing the character , Pos is the position of the character in that word , and Len is the length of that word .</sentence>
				<definiendum id="0">Cat</definiendum>
				<definiendum id="1">Pos</definiendum>
				<definiendum id="2">Len</definiendum>
				<definiens id="0">the POS of a word containing the character ,</definiens>
				<definiens id="1">the position of the character in that word , and</definiens>
				<definiens id="2">the length of that word</definiens>
			</definition>
			<definition id="1">
				<sentence>Rule development involves knowledge of Chinese morphology and generalizations of the training data .</sentence>
				<definiendum id="0">Rule development</definiendum>
				<definiens id="0">involves knowledge of Chinese morphology and generalizations of the training data</definiens>
			</definition>
			<definition id="2">
				<sentence>The rules developed fall into the following four types : 1 ) reduplication rules ( T1 ) , which tag reduplicated unknown words based on knowledge about the reduplication process ; 2 ) derivation rules ( T2 ) , which tag derived unknown words based on knowledge about the affixation process ; 3 ) compounding rules ( T3 ) , which tag unknown compounds based on the POS information of their component words ; and 4 ) rules based on generalizations about the training data ( T4 ) .</sentence>
				<definiendum id="0">reduplication rules</definiendum>
				<definiens id="0">T1 ) , which tag reduplicated unknown words based on knowledge about the reduplication process ; 2 ) derivation rules ( T2 ) , which tag derived unknown words based on knowledge about the affixation process ; 3 ) compounding rules ( T3 ) , which tag unknown compounds based on the POS information of their component words</definiens>
			</definition>
			<definition id="3">
				<sentence>if A equals B if A is a verb morpheme , AB is a verb else if A is a noun morpheme , AB is a noun else if A is an adjective morpheme , AB is a stative adjective/adverb else if B equals er , AB is a noun else if B is a categorizing suffix AND A is not a verb morpheme , AB is a noun else if A and B are both noun morphemes but not verb morphemes , AB is a noun else if A occurs verb-initially only AND B is not a noun morpheme AND B does not occur noun-finally only , AB is a verb else if B occurs noun-finally only AND A is not a verb morpheme AND A does not occur verb-initially only , AB is a noun Figure 1 : Rules for disyllabic words but a noun morpheme .</sentence>
				<definiendum id="0">AB</definiendum>
				<definiendum id="1">AB</definiendum>
				<definiendum id="2">AB</definiendum>
				<definiendum id="3">AB</definiendum>
				<definiendum id="4">AB</definiendum>
				<definiendum id="5">AB</definiendum>
				<definiendum id="6">AB</definiendum>
				<definiendum id="7">AB</definiendum>
				<definiens id="0">a verb else if A is a noun morpheme</definiens>
			</definition>
			<definition id="4">
				<sentence>We assume that the unknown POS depends on the previous two POS tags , and calculate the trigram probability P ( t3|t1 , t2 ) , where t3 stands for the unknown 3 POS , and t1 and t2 stand for the two previous POS tags .</sentence>
				<definiendum id="0">t3</definiendum>
			</definition>
			<definition id="5">
				<sentence>Recall ( R ) is defined as the number of correctly tagged unknown words divided by the total number of unknown words .</sentence>
				<definiendum id="0">Recall ( R</definiendum>
				<definiens id="0">the number of correctly tagged unknown words divided by the total number of unknown words</definiens>
			</definition>
			<definition id="6">
				<sentence>Precision ( P ) is defined as the number of correctly tagged unknown words divided by the number of tagged unknown words .</sentence>
				<definiendum id="0">Precision ( P</definiendum>
				<definiens id="0">the number of correctly tagged unknown words divided by the number of tagged unknown words</definiens>
			</definition>
</paper>

		<paper id="3006">
			<definition id="0">
				<sentence>We defined Descriptive Answer Type ( DAT ) as answer types for descriptive questions with two points of view : what kind of descriptive questions are in the use’s frequently asked questions ?</sentence>
				<definiendum id="0">Descriptive Answer Type ( DAT</definiendum>
			</definition>
			<definition id="1">
				<sentence>and Y n ] PRINCIPLE Osmosis is the principle , transfer of a liquid solvent through a semipermeable membrane that does not allow dissolved solids to pass .</sentence>
				<definiendum id="0">PRINCIPLE Osmosis</definiendum>
				<definiens id="0">the principle , transfer of a liquid solvent through a semipermeable membrane that does not allow dissolved solids to pass</definiens>
			</definition>
			<definition id="2">
				<sentence>[ X is the name from Y ] Descriptive Answer indexing process consists of two parts : pattern extraction from pre-tagged corpus and extraction of DIU ( Descriptive Indexing Unix ) using a pattern matching technique .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the name from Y ] Descriptive Answer indexing process consists of two parts : pattern extraction from pre-tagged corpus and extraction of DIU ( Descriptive Indexing Unix ) using a pattern matching technique</definiens>
			</definition>
			<definition id="3">
				<sentence>DIU consists of Title , DAT tag , Value , V_title , Pattern_ID , Determin_word , and Clue_word .</sentence>
				<definiendum id="0">DIU</definiendum>
			</definition>
			<definition id="4">
				<sentence>5 ETRI QA Test Set 2.0 consists of 1,047 &lt; question , answer &gt; pairs including both factoid and descriptive questions for all categories in encyclopedia 23 For performance comparisons , we used Top 1 and Top 5 precision , recall and F-score .</sentence>
				<definiendum id="0">ETRI QA Test Set 2.0</definiendum>
				<definiens id="0">consists of 1,047 &lt; question , answer &gt; pairs including both factoid and descriptive questions for all categories in encyclopedia 23 For performance comparisons , we used Top 1 and Top 5 precision , recall and F-score</definiens>
			</definition>
</paper>

		<paper id="2015">
			<definition id="0">
				<sentence>Quarc ( Riloff and Thelen , 2000 ) utilizes manually generated rules that selects a sentence deemed to contain the answer based on a combination of syntactic similarity and semantic correspondence ( i.e. , semantic categories of nouns ) .</sentence>
				<definiendum id="0">Quarc</definiendum>
				<definiens id="0">selects a sentence deemed to contain the answer based on a combination of syntactic similarity and semantic correspondence</definiens>
			</definition>
			<definition id="1">
				<sentence>Our framework , called QABLe ( QuestionAnswering Behavior Learner ) , draws on prior work in learning action and problem-solving strategies ( Tadepalli and Natarajan , 1996 ; Khardon , 1999 ) .</sentence>
				<definiendum id="0">QABLe</definiendum>
				<definiens id="0">draws on prior work in learning action and problem-solving strategies</definiens>
			</definition>
			<definition id="2">
				<sentence>Strategies ( and the procedural knowledge stored therein ) are acquired by explaining ( or deducing ) correct answers from training examples .</sentence>
				<definiendum id="0">Strategies</definiendum>
				<definiens id="0">the procedural knowledge stored therein ) are acquired by explaining ( or deducing ) correct answers from training examples</definiens>
			</definition>
			<definition id="3">
				<sentence>At the next tier , we utilize a Named Entity ( NE ) tagger for proper nouns a semantic category classifier for nouns and noun phrases , and a co-reference resolver ( that is limited to pronominal anaphora ) .</sentence>
				<definiendum id="0">co-reference resolver</definiendum>
				<definiens id="0">a Named Entity ( NE ) tagger for proper nouns a semantic category classifier for nouns and noun phrases</definiens>
			</definition>
			<definition id="4">
				<sentence>A phrase entity is defined as head ( w h , e x ) ∧ inPhrase ( w i , e x ) ∧ … ∧ inPhrase ( w j , e x ) .</sentence>
				<definiendum id="0">phrase entity</definiendum>
				<definiens id="0">head ( w h , e x ) ∧ inPhrase ( w i , e x ) ∧ … ∧ inPhrase ( w j , e x )</definiens>
			</definition>
			<definition id="5">
				<sentence>Semantic categories are similarly defined over the set of words and syntactic phrase entities – for example , sem_cat ( c x , PERSON ) ∧ head ( w h , c x ) ∧ pos ( w i , NNP ) ∧ word ( w h , “John” ) .</sentence>
				<definiendum id="0">c x ) ∧ pos</definiendum>
				<definiens id="0">the set of words and syntactic phrase entities – for example</definiens>
			</definition>
			<definition id="6">
				<sentence>The seq relation between two sentences , seq ( s i , s j ) ⇒ prior ( main ( s i ) , main ( s j ) ) , is defined as the sequential ordering in time of the corresponding events .</sentence>
				<definiendum id="0">seq relation between two sentences , seq</definiendum>
				<definiens id="0">the sequential ordering in time of the corresponding events</definiens>
			</definition>
			<definition id="7">
				<sentence>C represents the necessary condition for rule activation in the form of a conjunction over the relevant attributes of the world state .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">the necessary condition for rule activation in the form of a conjunction over the relevant attributes of the world state</definiens>
			</definition>
			<definition id="8">
				<sentence>R G represents the expected effect of the action .</sentence>
				<definiendum id="0">R G</definiendum>
				<definiens id="0">represents the expected effect of the action</definiens>
			</definition>
			<definition id="9">
				<sentence>If the current state is not a goal state and more processing time is available , QABLe passes the state to the Inference Engine ( IE ) .</sentence>
				<definiendum id="0">QABLe</definiendum>
			</definition>
			<definition id="10">
				<sentence>The SE uses the current state and its set of primitive operators to instantiate a new rule , as described in section 2.4 .</sentence>
				<definiendum id="0">SE</definiendum>
				<definiens id="0">uses the current state and its set of primitive operators to instantiate a new rule</definiens>
			</definition>
			<definition id="11">
				<sentence>Extraction As discussed in the previous section , when a goal state is generated in the abstract representation , this corresponds to a textual domain representation that contains an explicit answer in the right form to match the questions .</sentence>
				<definiendum id="0">Extraction As</definiendum>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>Specifically , we for the first time propose to utilize the web and the twin-candidate model , in addition to the previous combination of the corpus and the single-candidate model , to compute and apply the semantic information .</sentence>
				<definiendum id="0">twin-candidate model</definiendum>
				<definiens id="0">the previous combination of the corpus and the single-candidate model , to compute and apply the semantic information</definiens>
			</definition>
			<definition id="1">
				<sentence>166 compatibility with the anaphor could be represented simply in terms of frequency StatSem ( candi , ana ) = count ( candi , ana ) ( 1 ) where count ( candi , ana ) is the count of the tuple formed by candi and ana , or alternatively , in terms of conditional probability ( P ( candi , ana|candi ) ) , where the count of the tuple is divided by the count of the single candidate in the corpus .</sentence>
				<definiendum id="0">candi , ana )</definiendum>
				<definiens id="0">the count of the tuple formed by candi and ana , or alternatively , in terms of conditional probability ( P ( candi , ana|candi ) ) , where the count of the tuple is divided by the count of the single candidate in the corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>FirstNP aims to capture the salience of the candidate in the local discourse segment .</sentence>
				<definiendum id="0">FirstNP</definiendum>
			</definition>
			<definition id="3">
				<sentence>The score of a candidate is the total number of the competitors that the candidate wins over .</sentence>
				<definiendum id="0">score of a candidate</definiendum>
			</definition>
			<definition id="4">
				<sentence>Cass ( Abney , 1996 ) , a robust chunker parser was then applied to generate the shallow parse trees , which resulted in 353,085 possessive-noun tuples , 759,997 verb-object tuples and 1,090,121 subject-verb tuples .</sentence>
				<definiendum id="0">Cass</definiendum>
				<definiens id="0">resulted in 353,085 possessive-noun tuples , 759,997 verb-object tuples and 1,090,121 subject-verb tuples</definiens>
			</definition>
			<definition id="5">
				<sentence>Specifically , we proposed to utilize the web and the twin-candidate model , in addition to the common combination of the corpus and single-candidate model , to compute and apply the semantic information .</sentence>
				<definiendum id="0">twin-candidate model</definiendum>
				<definiens id="0">the common combination of the corpus and single-candidate model , to compute and apply the semantic information</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>A fine-grained PCFG is automatically induced from parsed corpora by training a PCFG-LA model using an EM-algorithm , which replaces the manual feature selection used in previous research .</sentence>
				<definiendum id="0">fine-grained PCFG</definiendum>
				<definiendum id="1">EM-algorithm</definiendum>
			</definition>
			<definition id="1">
				<sentence>PCFG-LA is a generative probabilistic model of parse trees .</sentence>
				<definiendum id="0">PCFG-LA</definiendum>
			</definition>
			<definition id="2">
				<sentence>Each non-terminal node in the complete data is labeled with a complete symbol of the form a44a45a39a46a47a42 , where a44 is the non-terminal symbol of the corresponding node in the observed tree and a46 is a latent annotation symbol , which is an element of a fixed set a48 .</sentence>
				<definiendum id="0">a44</definiendum>
				<definiendum id="1">a46</definiendum>
				<definiens id="0">Each non-terminal node in the complete data is labeled with a complete symbol of the form a44a45a39a46a47a42 , where</definiens>
				<definiens id="1">the non-terminal symbol of the corresponding node in the observed tree</definiens>
				<definiens id="2">a latent annotation symbol</definiens>
				<definiens id="3">an element of a fixed set a48</definiens>
			</definition>
			<definition id="3">
				<sentence>The probability of the observed tree a61 a51a62a38 a59 is obtained by summing a61 a51a62a38a40a39a41a43a42 a59 for all the assignments to latent annotation symbols , a41 : a61 a51a62a38 a59 a49a112a111 a113 a15a69a114a2a115 a111 a113 a22a116a114a29a115a95a117a58a117a58a117 a111 a113 a37 a114a2a115 a61 a51a62a38a40a39a41a43a42 a59a103a57 ( 1 ) Using dynamic programming , the theoretical bound of the time complexity of the summation in Eq .</sentence>
				<definiendum id="0">a41</definiendum>
				<definiens id="0">all the assignments to latent annotation symbols</definiens>
			</definition>
			<definition id="4">
				<sentence>We define a PCFG-LA a124 as a tuple a124 a49 a125 a76a31a126a116a127 a53 a76a128a127 a53 a48 a53a116a129a45a53 a64 a53a69a71a131a130 , where a76 a126a123a127a133a132 a set of observable non-terminal symbols a76a128a127 a132 a set of terminal symbols a48 a132 a set of latent annotation symbols a129 a132 a set of observable CFG rules a64a34a51a82a44a40a39a46a89a42 a59 a132 the probability of the occurrence of a complete symbol a44a40a39a46a89a42 at a root node a71 a51a17a106 a59 a132 the probability of a rule a106a135a134 a129 a39a48a136a42 a57 We use a44 a53a116a137a101a53a58a57a58a57a58a57 for non-terminal symbols in a76a31a126a116a127 ; a138 a5a58a53 a138a12a55 a53a58a57a58a57a58a57 for terminal symbols in a76a40a127 ; and a46 a53a69a139a80a53a58a57a58a57a58a57 for latent annotation symbols in a48 .</sentence>
				<definiendum id="0">PCFG-LA a124</definiendum>
				<definiens id="0">symbols a48 a132 a set of latent annotation symbols a129 a132 a set of observable CFG rules a64a34a51a82a44a40a39a46a89a42 a59 a132 the probability of the occurrence of a complete symbol a44a40a39a46a89a42 at a root node a71 a51a17a106 a59 a132 the probability of a rule a106a135a134 a129 a39a48a136a42 a57 We use a44 a53a116a137a101a53a58a57a58a57a58a57 for non-terminal symbols in a76a31a126a116a127</definiens>
				<definiens id="1">latent annotation symbols in a48</definiens>
			</definition>
			<definition id="5">
				<sentence>a76a31a126a116a127a60a39a48a136a42 denotes the set of complete non-terminal symbols , i.e. , a76a40a126a123a127a60a39a48a136a42a73a49a141a140a142a44a40a39a46a89a42a27a118a29a44a100a134a143a76a128a126a123a127 a53 a46a144a134a143a48a144a145 .</sentence>
				<definiendum id="0">a76a31a126a116a127a60a39a48a136a42</definiendum>
				<definiens id="0">the set of complete non-terminal symbols</definiens>
			</definition>
			<definition id="6">
				<sentence>In the above definition , a129 is a set of CFG rules of observable ( i.e. , not annotated ) symbols .</sentence>
				<definiendum id="0">a129</definiendum>
				<definiens id="0">a set of CFG rules of observable ( i.e. , not annotated ) symbols</definiens>
			</definition>
			<definition id="7">
				<sentence>A complete tree is denoted by a38a40a39a41a43a42 , where a41 a49a160a51a17a46 a5a54a53a58a57a58a57a58a57a142a53 a46a89a161 a59 a134a100a48 a161 is a vector of latent annotation symbols and a46a109a162 is the latent annotation symbol attached to the a107 -th non-terminal node .</sentence>
				<definiendum id="0">a46a109a162</definiendum>
				<definiens id="0">a vector of latent annotation symbols and</definiens>
				<definiens id="1">the latent annotation symbol attached to the a107 -th non-terminal node</definiens>
			</definition>
			<definition id="8">
				<sentence>The probability of a complete parse tree a38a45a39a41a63a42 is defined as a61 a51a62a38a40a39a41a43a42 a59 a49a65a64a34a51a82a44 a5 a39a46 a5 a42 a59 a164 a165 a114a2a166a168a167a133a169a170a89a171 a71 a51a17a106 a59a103a53 ( 2 ) where a44 a5 a39a46 a5 a42 is the label of the root node of a38a40a39a41a43a42 and a83a40a172a68a173a174a81a175 denotes the multiset of annotated CFG rules used in the generation of a38a40a39a41a43a42 .</sentence>
				<definiendum id="0">probability of a complete parse tree a38a45a39a41a63a42</definiendum>
				<definiens id="0">a61 a51a62a38a40a39a41a43a42 a59 a49a65a64a34a51a82a44 a5 a39a46 a5 a42 a59 a164 a165 a114a2a166a168a167a133a169a170a89a171 a71 a51a17a106 a59a103a53 ( 2 ) where a44 a5 a39a46 a5 a42 is the label of the root node of a38a40a39a41a43a42 and a83a40a172a68a173a174a81a175 denotes the multiset of annotated CFG rules used in the generation of a38a40a39a41a43a42</definiens>
			</definition>
			<definition id="9">
				<sentence>We have the probability of an observable tree a38 by marginalizing out the latent annotation symbols in a38a40a39a41a43a42 : a61 a51a62a38 a59 a49 a111 a176 a114a29a115a9a177 a64a34a51a82a44a128a5a158a39a46a73a5a69a42 a59 a164 a165 a114a2a166a168a167a133a169a170a47a171 a71 a51a17a106 a59a103a53 ( 3 ) where a159 is the number of non-terminal nodes in a38 .</sentence>
				<definiendum id="0">a159</definiendum>
				<definiens id="0">the probability of an observable tree a38 by marginalizing out the latent annotation symbols in a38a40a39a41a43a42</definiens>
				<definiens id="1">the number of non-terminal nodes in a38</definiens>
			</definition>
			<definition id="10">
				<sentence>Like the derivations of the EM algorithms for other latent variable models , the update formulas for the parameters , which update the parameters from a194 to a194a23a197a152a49a198a51 a71 a197 a53 a64a183a197 a59 , are obtained by constrained optimization of a199a84a51a82a194 a197 a118a194 a59 , which is defined as a199a84a51a82a194 a197 a118a194 a59 a49 a111 a172 a196 a114a201a200 a111 a174 a196 a114a29a115 a177 a196 a61a131a202 a51a82a41 a162 a118a38 a162a24a59a16a203a205a204a29a206 a61a131a202a93a207 a51a62a38 a162 a39a41 a162 a42 a59a103a53 where a61a131a202 and a61a131a202 a207 denote probabilities under a194 and a194 a197 , and a61 a51a82a41a155a118a38 a59 is the conditional probability of latent annotation symbols given an observed tree a38 , i.e. , a61 a51a82a41a144a118a38 a59 a49 a61 a51a62a38a45a39a41a63a42 a59a69a208 a61 a51a62a38 a59 .</sentence>
				<definiendum id="0">update formulas for the parameters</definiendum>
			</definition>
			<definition id="11">
				<sentence>In theory , we can use PCFG-LAs to parse a given sentence a138 by selecting the most probable parse : a38a183a209a211a210a93a212a24a213a168a49a65a214a2a215 a206a23a216 a214a201a217 a172 a114a2a218a168a219a221a220a109a222 a61 a51a62a38a45a118a138 a59 a49a65a214a2a215 a206a23a216 a214a201a217 a172 a114a2a218a168a219a223a220a183a222 a61 a51a62a38 a59a103a53 ( 4 ) where a224a84a51a17a138 a59 denotes the set of possible parses for a138 under the observable grammar a129 .</sentence>
				<definiendum id="0">a224a84a51a17a138 a59</definiendum>
				<definiens id="0">the set of possible parses for a138 under the observable grammar a129</definiens>
			</definition>
</paper>

		<paper id="3031">
			<definition id="0">
				<sentence>Header trees can be seen as variants of XML trees where each internal node is not an XML tag , but a header which is a part of document that can be regarded as tags annotated to other parts of the document .</sentence>
				<definiendum id="0">Header trees</definiendum>
				<definiens id="0">variants of XML trees where each internal node is not an XML tag</definiens>
			</definition>
			<definition id="1">
				<sentence>A separator is a sequence of HTML tags and symbols .</sentence>
				<definiendum id="0">separator</definiendum>
			</definition>
			<definition id="2">
				<sentence>[ [ About Me , [ NAME , John Smith ] , [ AGE , 25 ] ] , Back to Home Page ] ] Figure 3 : A List Representation of the Example Web Document A header is defined as a block that modifies subsequent blocks .</sentence>
				<definiendum id="0">Web Document A header</definiendum>
				<definiens id="0">A List Representation of the Example</definiens>
			</definition>
			<definition id="3">
				<sentence>In this case , : is a separator , ME is the left string and Jo is the right string .</sentence>
				<definiendum id="0">ME</definiendum>
				<definiens id="0">a separator</definiens>
				<definiens id="1">the left string and Jo is the right string</definiens>
			</definition>
			<definition id="4">
				<sentence>Assuming the locality of separator appearances , the model for all separators in a given document set is defined as C8B4D0BN D7BN D6B5 BP C9 C8B4D0BN D7BN D6B5 where D0 is a vector of left strings , D7 is a vector of separators , and D6 is a vector of right strings .</sentence>
				<definiendum id="0">Assuming the locality of separator appearances</definiendum>
				<definiendum id="1">D7</definiendum>
				<definiendum id="2">D6</definiendum>
				<definiens id="0">the model for all separators in a given document set is defined as C8B4D0BN D7BN D6B5 BP C9 C8B4D0BN D7BN D6B5 where D0 is a vector of left strings</definiens>
				<definiens id="1">a vector of separators , and</definiens>
				<definiens id="2">a vector of right strings</definiens>
			</definition>
			<definition id="5">
				<sentence>6 from 1,000 to 10,000 bytes 7 Src tags indicate inclusion of image files , java codes , etc 123 Algorithm Recall Precision F-measure OUR ALGORITHM 0.477 0.266 0.329 NO-CL 0.178 0.119 0.139 NO-EM 0.389 0.211 0.265 PREV 0.144 0.615 0.202 Table 1 : Macro-Averaged Recall , Precision , and Fmeasure on Test Documents to evaluate machine-made bracketings .</sentence>
				<definiendum id="0">java codes</definiendum>
				<definiens id="0">Macro-Averaged Recall , Precision , and Fmeasure on Test Documents to evaluate machine-made bracketings</definiens>
			</definition>
			<definition id="6">
				<sentence>Recall is the rate that bracketing given by hand are also given by machine , and precision is the rate that bracketing given by machine are also given by hand .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiendum id="1">precision</definiendum>
				<definiens id="0">the rate that bracketing given by hand are also given by machine , and</definiens>
				<definiens id="1">the rate that bracketing given by machine are also given by hand</definiens>
			</definition>
			<definition id="7">
				<sentence>F-measure is a harmonic mean of recall and precision that is used as a combined measure .</sentence>
				<definiendum id="0">F-measure</definiendum>
				<definiendum id="1">precision</definiendum>
				<definiens id="0">a harmonic mean of recall and</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>A learning-based coreference system can be defined by four elements : the learning algorithm used to train the coreference classifier , the method of creating training instances for the learner , the feature set 2Examples of such scoring functions include the DempsterShafer rule ( see Kehler ( 1997 ) and Bean and Riloff ( 2004 ) ) and its variants ( see Harabagiu et al. ( 2001 ) and Luo et al. ( 2004 ) ) .</sentence>
				<definiendum id="0">learning-based coreference system</definiendum>
				<definiens id="0">the learning algorithm used to train the coreference classifier , the method of creating training instances for the learner</definiens>
			</definition>
			<definition id="1">
				<sentence>Specifically , Soon et al.’s system employs a decision tree learner to train a coreference classifier on instances created by Soon’s method and represented by Soon’s feature set , coordinating the classification decisions via closest-first clustering .</sentence>
				<definiendum id="0">Soon et al.’s system</definiendum>
				<definiens id="0">employs a decision tree learner to train a coreference classifier on instances created by Soon’s method</definiens>
			</definition>
			<definition id="2">
				<sentence>Second , the MUC scorer applies the same penalty to each erroneous merging decision , whereas B-CUBED penalizes erroneous merging decisions involving two large clusters more heavily than those involving two small clusters .</sentence>
				<definiendum id="0">MUC scorer</definiendum>
				<definiens id="0">applies the same penalty to each erroneous merging decision</definiens>
			</definition>
</paper>

		<paper id="3013">
			<definition id="0">
				<sentence>TextRank ( Mihalcea and Tarau , 2004 ) , ( Mihalcea , 2004 ) is specifically designed to address this problem , by using an extractive summarization technique that does not require any training data or any language-specific knowledge sources .</sentence>
				<definiendum id="0">TextRank</definiendum>
			</definition>
			<definition id="1">
				<sentence>Unlike other graph ranking algorithms , PageRank integrates the impact of both incoming and outgoing links into one single model , and therefore it produces only one set of scores : PR ( Vi ) = ( 1 − d ) + d ∗ summationdisplay Vj∈In ( Vi ) PR ( Vj ) |Out ( Vj ) | ( 1 ) where d is a parameter that is set between 0 and 1 , and has the role of integrating random jumps into the random walking model .</sentence>
				<definiendum id="0">PageRank</definiendum>
				<definiens id="0">integrates the impact of both incoming and outgoing links into one single model , and therefore it produces only one set of scores : PR ( Vi ) = ( 1 − d ) + d ∗ summationdisplay Vj∈In ( Vi ) PR ( Vj ) |Out ( Vj ) | ( 1 ) where d is a parameter that is set between 0 and 1 , and has the role of integrating random jumps into the random walking model</definiens>
			</definition>
			<definition id="2">
				<sentence>HITS ( Hyperlinked Induced Topic Search ) ( Kleinberg , 1999 ) is an iterative algorithm that was designed for ranking Web pages according to their degree of “authority” .</sentence>
				<definiendum id="0">HITS</definiendum>
				<definiens id="0">an iterative algorithm that was designed for ranking Web pages according to their degree of “authority”</definiens>
			</definition>
</paper>

		<paper id="3005">
			<definition id="0">
				<sentence>The iProClass database is a central point for exploration of protein information , which provides summary descriptions of protein family , function and structure for all protein sequences from PIR , Swiss-Prot , and TrEMBL ( now UniProt ) .</sentence>
				<definiendum id="0">iProClass database</definiendum>
				<definiendum id="1">TrEMBL</definiendum>
				<definiens id="0">a central point for exploration of protein information , which provides summary descriptions of protein family , function and structure for all protein sequences from PIR , Swiss-Prot , and</definiens>
			</definition>
			<definition id="1">
				<sentence>The PIR-NREF database is a comprehensive database for sequence searching and protein identification .</sentence>
				<definiendum id="0">PIR-NREF database</definiendum>
				<definiens id="0">a comprehensive database for sequence searching and protein identification</definiens>
			</definition>
			<definition id="2">
				<sentence>TrEMBL consists of computationally analyzed records that await full manual annotation .</sentence>
				<definiendum id="0">TrEMBL</definiendum>
			</definition>
			<definition id="3">
				<sentence>It automatically gathers fields that contain annotation information from PSD , RefSeq , Swiss-Prot , TrEMBL , GenBank , Entrez GENE , MGI , RGD , HUGO , ENCUM , FlyBase , and WormBase for each iProClass record from the distribution website 18 Figure 2 : Screenshot of retrieving il2 from BioThesaurus of each resource .</sentence>
				<definiendum id="0">WormBase</definiendum>
				<definiens id="0">fields that contain annotation information from PSD</definiens>
			</definition>
			<definition id="4">
				<sentence>Shanker K : A biological named entity recognizer .</sentence>
				<definiendum id="0">Shanker K</definiendum>
				<definiens id="0">A biological named entity recognizer</definiens>
			</definition>
</paper>

		<paper id="3028">
			<definition id="0">
				<sentence>Growing interest in richly annotated corpora is a driving force for the development of annotation tools that can handle multiple levels of annotation .</sentence>
				<definiendum id="0">richly annotated corpora</definiendum>
			</definition>
			<definition id="1">
				<sentence>Simplified MMAXQL is a variant of the MMAXQL query language .</sentence>
				<definiendum id="0">Simplified MMAXQL</definiendum>
				<definiens id="0">a variant of the MMAXQL query language</definiens>
			</definition>
			<definition id="2">
				<sentence>Each basedata query token consists of a regular expression in single quotes , which must exactly match one basedata element .</sentence>
				<definiendum id="0">basedata query token</definiendum>
				<definiens id="0">consists of a regular expression in single quotes , which must exactly match one basedata element</definiens>
			</definition>
</paper>

		<paper id="1074">
			<definition id="0">
				<sentence>We use the heuristic for phrase alignment described in Och and Ney ( 2003 ) which aligns phrases by incrementally building longer phrases from words and phrases which have adjacent alignment points.1 1Note that while we induce the translations of phrases from 598 what is more , the relevant cost dynamic is completelyunder control im übrigen ist die diesbezügliche kostenentwicklung völlig unter kontrolle we owe it to the taxpayers to keep in checkthe costs wir sind es den steuerzahlern die kosten zu habenschuldig unter kontrolle Figure 2 : Using a bilingual parallel corpus to extract paraphrases We define a paraphrase probability p ( e2|e1 ) in terms of the translation model probabilities p ( f|e1 ) , that the original English phrase e1 translates as a particular phrase f in the other language , and p ( e2|f ) , that the candidate paraphrase e2 translates as the foreign language phrase .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the heuristic for phrase alignment described in Och and Ney ( 2003 ) which aligns phrases by incrementally building longer phrases from words and phrases</definiens>
				<definiens id="1">the original English phrase e1 translates as a particular phrase f in the other language , and</definiens>
				<definiens id="2">the foreign language phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , p ( e|f ) can be calculated straightforwardly using maximum likelihood estimation by counting how often the phrases e and f were aligned in the parallel corpus : p ( e|f ) = count ( e , f ) summationtext e count ( e , f ) ( 3 ) Note that the paraphrase probability defined in Equation 2 returns the single best paraphrase , ˆe2 , irrespective of the context in which e1 appears .</sentence>
				<definiendum id="0">p</definiendum>
			</definition>
			<definition id="2">
				<sentence>We include a simple language model probability , which would additionally rank e2 based on the probability of the sentence formed by substiuting e2 for e1 in S. A possible extension which we do not evaluate might be permitting only paraphrases that are the same syntactic type as the original phrase , which we could do by extending the translation model probabilities to count only phrase occurrences of that type .</sentence>
				<definiendum id="0">phrase</definiendum>
				<definiens id="0">a simple language model probability , which would additionally rank e2 based on the probability of the sentence formed by substiuting e2 for e1 in S. A possible extension</definiens>
			</definition>
</paper>

		<paper id="3026">
			<definition id="0">
				<sentence>The generation algorithm is an iterative process and produces these translation hypotheses incrementally .</sentence>
				<definiendum id="0">generation algorithm</definiendum>
				<definiens id="0">an iterative process and produces these translation hypotheses incrementally</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>Ratnaparkhi’s conditional maximum entropy model ( Ratnaparkhi , 1999 ) , trained to maximize conditional likelihood P ( y|x ) of the training data , performed nearly as well as generative models of the same vintage even though it scores parsing decisions in isolation and thus may suffer from the label bias problem ( Lafferty et al. , 2001 ) .</sentence>
				<definiendum id="0">Ratnaparkhi’s conditional maximum entropy model</definiendum>
				<definiens id="0">trained to maximize conditional likelihood P ( y|x ) of the training data , performed nearly as well as generative models of the same vintage even though it scores parsing decisions in isolation</definiens>
			</definition>
			<definition id="1">
				<sentence>The generic dependency tree is denoted by y. If y is a dependency tree for sentence x , we write ( i , j ) ∈ y to indicate that there is a directed edge from word xi to word xj in the tree , that is , xi is the parent of xj .</sentence>
				<definiendum id="0">y. If y</definiendum>
				<definiens id="0">i , j ) ∈ y to indicate that there is a directed edge from word xi to word xj in the tree</definiens>
			</definition>
			<definition id="2">
				<sentence>We follow the edge based factorization method of Eisner ( 1996 ) and define the score of a dependency tree as the sum of the score of all edges in the tree , s ( x , y ) = summationdisplay ( i , j ) ∈y s ( i , j ) = summationdisplay ( i , j ) ∈y w · f ( i , j ) where f ( i , j ) is a high-dimensional binary feature representation of the edge from xi to xj .</sentence>
				<definiendum id="0">f</definiendum>
				<definiens id="0">a high-dimensional binary feature representation of the edge from xi to xj</definiens>
			</definition>
			<definition id="3">
				<sentence>Finally we define dt ( x ) as the set of possible dependency trees for the input sentence x and bestk ( x ; w ) as the set of k dependency trees in dt ( x ) that are given the highest scores by weight vector w , with ties resolved by an arbitrary but fixed rule .</sentence>
				<definiendum id="0">bestk</definiendum>
				<definiens id="0">the set of possible dependency trees for the input sentence x</definiens>
				<definiens id="1">the set of k dependency trees in dt ( x ) that are given the highest scores by weight vector w , with ties resolved by an arbitrary but fixed rule</definiens>
			</definition>
			<definition id="4">
				<sentence>Crammer and Singer ( 2001 ) developed a natural method for large-margin multi-class classification , which was later extended by Taskar et al. ( 2003 ) to structured classification : minbardblwbardbl s.t. s ( x , y ) − s ( x , yprime ) ≥ L ( y , yprime ) ∀ ( x , y ) ∈ T , yprime ∈ dt ( x ) where L ( y , yprime ) is a real-valued loss for the tree yprime relative to the correct tree y. We define the loss of a dependency tree as the number of words that have the incorrect parent .</sentence>
				<definiendum id="0">yprime )</definiendum>
				<definiens id="0">2003 ) to structured classification : minbardblwbardbl s.t. s ( x , y ) − s ( x , yprime ) ≥ L ( y , yprime ) ∀ ( x , y ) ∈ T , yprime ∈ dt ( x ) where L ( y ,</definiens>
			</definition>
			<definition id="5">
				<sentence>The resulting optimization made by MIRA ( see Figure 3 , line 4 ) would then be : min vextenddoublevextenddoublew ( i+1 ) − w ( i ) vextenddoublevextenddouble s.t. s ( xt , yt ) − s ( xt , yprime ) ≥ L ( yt , yprime ) ∀yprime ∈ bestk ( xt ; w ( i ) ) reducing the number of constraints to the constant k. We tested various values of k on a development data set and found that small values of k are sufficient to achieve close to best performance , justifying our assumption .</sentence>
				<definiendum id="0">w ( i ) ) reducing</definiendum>
				<definiens id="0">the number of constraints to the constant k. We tested various values of k on a development data set and found that small values of k are sufficient to achieve close to best performance</definiens>
			</definition>
			<definition id="6">
				<sentence>Y &amp; M2003 is the SVM-shiftreduce parsing model of Yamada and Matsumoto ( 2003 ) , N &amp; S2004 is the memory-based learner of Nivre and Scholz ( 2004 ) and MIRA is the the system we have described .</sentence>
				<definiendum id="0">M2003</definiendum>
				<definiendum id="1">S2004</definiendum>
				<definiendum id="2">MIRA</definiendum>
				<definiens id="0">the the system we have described</definiens>
			</definition>
			<definition id="7">
				<sentence>Accuracy is the number of words that correctly identified their parent in the tree .</sentence>
				<definiendum id="0">Accuracy</definiendum>
				<definiens id="0">the number of words that correctly identified their parent in the tree</definiens>
			</definition>
			<definition id="8">
				<sentence>Root is the number of trees in which the root word was correctly identified .</sentence>
				<definiendum id="0">Root</definiendum>
				<definiens id="0">the number of trees in which the root word was correctly identified</definiens>
			</definition>
			<definition id="9">
				<sentence>Complete is the number of sentences for which the entire dependency tree was correct .</sentence>
				<definiendum id="0">Complete</definiendum>
				<definiens id="0">the number of sentences for which the entire dependency tree was correct</definiens>
			</definition>
			<definition id="10">
				<sentence>Acknowledgments : We thank Jan Hajiˇc for answering queries on the Prague treebank , and Joakim Nivre for providing the Yamada and Matsumoto ( 2003 ) head rules for English that allowed for a direct comparison with our systems .</sentence>
				<definiendum id="0">Yamada</definiendum>
				<definiens id="0">We thank Jan Hajiˇc for answering queries on the Prague treebank , and Joakim Nivre for providing the</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>Clearly , OVA makes no explicit use of pairwise label or item relationships .</sentence>
				<definiendum id="0">OVA</definiendum>
				<definiens id="0">makes no explicit use of pairwise label or item relationships</definiens>
			</definition>
			<definition id="1">
				<sentence>Then , it is quite natural to pose our problem as nding a mapping of instances a48 to labels a11a60a68 ( respecting the original labels of the training instances ) that minimizes a69 a68a14a70 test a71a72 a24 a43 a9a49a48a50a13a15a11 a68 a18a45a73a26a74 a69 a75 a70a77a76a78a76a80a79a42a81a82a68a51a83a78a84 a9a85a8a86a9a12a11 a68 a13a15a11 a75 a18a2a18 a65a5a66a12a67 a9a49a48a50a13a2a87a88a18a27a89a90a91a13 where a84 is monotonically increasing ( we chose a84 a9a85a8a92a18a93a20a94a8 unless otherwise speci ed ) and a74 is a trade-off and/or scaling parameter .</sentence>
				<definiendum id="0">a74</definiendum>
				<definiens id="0">respecting the original labels of the training instances ) that minimizes a69 a68a14a70 test a71a72 a24 a43 a9a49a48a50a13a15a11 a68 a18a45a73a26a74 a69 a75 a70a77a76a78a76a80a79a42a81a82a68a51a83a78a84 a9a85a8a86a9a12a11 a68 a13a15a11 a75 a18a2a18 a65a5a66a12a67 a9a49a48a50a13a2a87a88a18a27a89a90a91a13 where a84 is monotonically increasing</definiens>
				<definiens id="1">a trade-off and/or scaling parameter</definiens>
			</definition>
			<definition id="2">
				<sentence>A simple hypothesis is that ratings can be determined by the positive-sentence percentage ( PSP ) of a text , i.e. , the number of positive sentences divided by the number of subjective sentences .</sentence>
				<definiendum id="0">simple hypothesis</definiendum>
				<definiendum id="1">PSP</definiendum>
				<definiens id="0">the number of positive sentences divided by the number of subjective sentences</definiens>
			</definition>
			<definition id="3">
				<sentence>The notation Aa73 B denotes metric labeling where method A provides the initial label preference function a43 and B serves as similarity measure .</sentence>
				<definiendum id="0">notation Aa73 B</definiendum>
				<definiens id="0">denotes metric labeling where method A provides the initial label preference function a43 and B serves as similarity measure</definiens>
			</definition>
</paper>

		<paper id="3027">
			<definition id="0">
				<sentence>SenseClusters is a freely available system that identifies similar contexts in text .</sentence>
				<definiendum id="0">SenseClusters</definiendum>
				<definiens id="0">a freely available system that identifies similar contexts in text</definiens>
			</definition>
			<definition id="1">
				<sentence>Unigrams are single words that occur more than five times , bigrams are ordered pairs of words that may have intervening words between them , while co-occurrences are simply unordered bigrams .</sentence>
				<definiendum id="0">Unigrams</definiendum>
				<definiens id="0">single words that occur more than five times , bigrams are ordered pairs of words that may have intervening words between them , while co-occurrences are simply unordered bigrams</definiens>
			</definition>
			<definition id="2">
				<sentence>The discriminating labels are any descriptive labels for a cluster that are not descriptive labels of another cluster .</sentence>
				<definiendum id="0">discriminating labels</definiendum>
				<definiens id="0">any descriptive labels for a cluster that are not descriptive labels of another cluster</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>To identify the intended message of a new information graphic , the graphic is first given to a Visual Extraction Module ( Chester and Elzer , 2005 ) that is responsible for recognizing the individual components of a graphic , identifying the relationship of the components to one another and to the graphic as a whole , and classifying the graphic as to type ( bar chart , line graph , etc. ) ; the result is an XML file that describes the graphic and all of its components .</sentence>
				<definiendum id="0">graphic</definiendum>
				<definiendum id="1">Visual Extraction Module</definiendum>
				<definiens id="0">an XML file that describes the graphic and all of its components</definiens>
			</definition>
			<definition id="1">
				<sentence>Next a Caption Processing Module analyzes the caption .</sentence>
				<definiendum id="0">Caption Processing Module</definiendum>
				<definiens id="0">analyzes the caption</definiens>
			</definition>
			<definition id="2">
				<sentence>The Caption Processing Module applies a part-of-speech tagger and a stemmer to the caption in order to identify nouns , adjectives , and the root form of verbs and adjectives derived from verbs .</sentence>
				<definiendum id="0">Caption Processing Module</definiendum>
				<definiens id="0">applies a part-of-speech tagger and a stemmer to the caption in order to identify nouns , adjectives , and the root form of verbs and adjectives derived from verbs</definiens>
			</definition>
			<definition id="3">
				<sentence>The XML representation of the graphic is augmented to indicate any independent axis labels that match nouns in the caption , and the presence of a verb or adjective class in the caption .</sentence>
				<definiendum id="0">XML representation of the graphic</definiendum>
				<definiens id="0">augmented to indicate any independent axis labels that match nouns in the caption</definiens>
			</definition>
			<definition id="4">
				<sentence>An example is DJIA ( for Dow Jones Industrial Average ) that occurs in one graphic as a label but appears as Dow in the caption .</sentence>
				<definiendum id="0">DJIA</definiendum>
				<definiens id="0">occurs in one graphic as a label but appears as Dow in the caption</definiens>
			</definition>
</paper>

		<paper id="3016">
</paper>

		<paper id="3021">
			<definition id="0">
				<sentence>The TARSQI system can be used stand-alone or as a means to alleviate the tasks of human annotators .</sentence>
				<definiendum id="0">TARSQI system</definiendum>
				<definiens id="0">a means to alleviate the tasks of human annotators</definiens>
			</definition>
			<definition id="1">
				<sentence>GUTime extends TempEx to handle time expressions based on the TimeML TIMEX3 standard ( timeml.org ) , which allows a functional style of encoding offsets in time expressions .</sentence>
				<definiendum id="0">GUTime</definiendum>
				<definiens id="0">extends TempEx to handle time expressions based on the TimeML TIMEX3 standard ( timeml.org ) , which allows a functional style of encoding offsets in time expressions</definiens>
			</definition>
			<definition id="2">
				<sentence>Evita ( Events in Text Analyzer ) is an event recognition tool that performs two main tasks : robust event identification and analysis of grammatical features , such as tense and aspect .</sentence>
				<definiendum id="0">Evita ( Events</definiendum>
				<definiens id="0">an event recognition tool that performs two main tasks : robust event identification and analysis of grammatical features , such as tense and aspect</definiens>
			</definition>
			<definition id="3">
				<sentence>GUTenLINK uses default rules for ordering events ; its handling of successive past tense nonstative verbs in case ( iii ) will not correctly order sequences like Max fell .</sentence>
				<definiendum id="0">GUTenLINK</definiendum>
				<definiens id="0">uses default rules for ordering events</definiens>
			</definition>
			<definition id="4">
				<sentence>Slinket ( SLINK Events in Text ) is an application currently being developed .</sentence>
				<definiendum id="0">Slinket</definiendum>
				<definiens id="0">an application currently being developed</definiens>
			</definition>
			<definition id="5">
				<sentence>SputLink is a temporal closure component that takes known temporal relations in a text and derives new implied relations from them , in effect making explicit what was implicit .</sentence>
				<definiendum id="0">SputLink</definiendum>
				<definiens id="0">a temporal closure component that takes known temporal relations in a text and derives new implied relations from them , in effect making explicit what was implicit</definiens>
			</definition>
			<definition id="6">
				<sentence>The SputLink algorithm , like Allen’s , is basically a constraint propagation algorithm that uses a transitivity table to model the compositional behavior of all pairs of relations .</sentence>
				<definiendum id="0">constraint propagation algorithm</definiendum>
				<definiens id="0">uses a transitivity table to model the compositional behavior of all pairs of relations</definiens>
			</definition>
</paper>

		<paper id="2026">
			<definition id="0">
				<sentence>Good directiongivers respond actively to a driver’s actions and questions , and express instructions relative to a large variety of landmarks , times , and distances .</sentence>
				<definiendum id="0">Good directiongivers</definiendum>
				<definiens id="0">respond actively to a driver’s actions and questions , and express instructions relative to a large variety of landmarks , times , and distances</definiens>
			</definition>
			<definition id="1">
				<sentence>The HALogen system is a broad-coverage generator that uses a combination of statistical and symbolic techniques .</sentence>
				<definiendum id="0">HALogen system</definiendum>
				<definiens id="0">a broad-coverage generator that uses a combination of statistical and symbolic techniques</definiens>
			</definition>
			<definition id="2">
				<sentence>By Bayes’ rule , P ( T|S ) =P ( S|T ) P ( T ) /P ( S ) , where P ( S ) is a normalizing factor , P ( T ) can be easily calculated using the language model ( as the product of the probabilities of the node expansions that appear in T ) , and P ( S|T ) = summationdisplay T prime P ( S|T prime ) P ( T prime |T ) = summationdisplay T prime trianglerightS P ( T prime |T ) Since P ( T prime |T ) =P ( T|T prime ) P ( T prime ) /P ( T ) , and since P ( T|T prime ) is 1 if T prime &gt; Tand 0 otherwise , we have P ( T|S ) = 1 P ( S ) summationdisplay T prime trianglerightS P ( T|T prime ) P ( T prime ) = 1 P ( S ) summationdisplay T prime &gt; T ; T prime trianglerightS P ( T prime ) Together with Equation 1 this shows that P ( T|S ) ≥ max T prime &gt; T P ( T prime |S ) , since the maximum is one of the terms in the sum .</sentence>
				<definiendum id="0">P ( S )</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">a normalizing factor</definiens>
				<definiens id="1">1 if T prime &gt; Tand 0 otherwise</definiens>
				<definiens id="2">T|T prime ) P ( T prime ) = 1 P ( S ) summationdisplay T prime &gt; T</definiens>
				<definiens id="3">P ( T|S ) ≥ max T prime &gt; T P ( T prime |S ) , since the maximum is one of the terms in the sum</definiens>
			</definition>
			<definition id="3">
				<sentence>P ( X → y ) is the probability that at least one x ∈ X has y as a descendant , i.e. P ( X → y ) = AL1 x∈X P ( x → y ) , where AL1 is the ‘At-leastone’ function .</sentence>
				<definiendum id="0">AL1</definiendum>
				<definiens id="0">the probability that at least one x ∈ X has y as a descendant</definiens>
			</definition>
			<definition id="4">
				<sentence>2 This means that we can approximate P ( T|S ) as P ( T ) P ( S ) productdisplay y∈α ( S , T ) AL1 x∈T-set ( y ) P ( x → y ) P ( y → S y ) ( 3 ) 2 That is , given the probabilities of a set of events , the Atleast-one function gives the probability of at least one of the events occuring .</sentence>
				<definiendum id="0">Atleast-one function</definiendum>
				<definiens id="0">gives the probability of at least one of the events occuring</definiens>
			</definition>
			<definition id="5">
				<sentence>Consider the transformation T that takes a distribution Q ( x → y ) to a new distribution T ( Q ) such that T ( Q ) ( x → y ) is equal to 1 when x = y , and otherwise is equal to summationdisplay ζ∈Exp ( x ) P LM ( ζ|x ) AL1 z∈ζ Q ( z → y ) ( 4 ) Here Exp ( x ) is the set of possible expansions of x , and P LM ( ζ|x ) is the probability of the expansion ζ according to the language model .</sentence>
				<definiendum id="0">Here Exp ( x )</definiendum>
				<definiendum id="1">P LM</definiendum>
				<definiens id="0">the probability of the expansion ζ according to the language model</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>Phrases , which can be any substring and not necessarily phrases in any syntactic theory , allow these models to learn local reorderings , translation of short idioms , or insertions and deletions that are sensitive to local context .</sentence>
				<definiendum id="0">Phrases</definiendum>
				<definiens id="0">sensitive to local context</definiens>
			</definition>
			<definition id="1">
				<sentence>In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of right-hand sides : ( 9 ) X → 〈γ , α , ∼〉 where X is a nonterminal , γ and α are both strings of terminals and nonterminals , and ∼ is a one-to-one correspondence between nonterminal occurrences in γ and nonterminal occurrences in α .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">α</definiendum>
				<definiendum id="2">∼</definiendum>
				<definiens id="0">a nonterminal , γ and</definiens>
				<definiens id="1">a one-to-one correspondence between nonterminal occurrences in γ and nonterminal occurrences in α</definiens>
			</definition>
			<definition id="2">
				<sentence>265 〈S 1 , S 1 〉 ⇒ 〈S 2 X 3 , S 2 X 3 〉 ⇒ 〈S 4 X 5 X 3 , S 4 X 5 X 3 〉 ⇒ 〈X 6 X 5 X 3 , X 6 X 5 X 3 〉 ⇒ 〈Aozhou X 5 X 3 , Australia X 5 X 3 〉 ⇒ 〈Aozhou shi X 3 , Australia is X 3 〉 ⇒ 〈Aozhou shi X 7 zhiyi , Australia is one of X 7 〉 ⇒ 〈Aozhou shi X 8 de X 9 zhiyi , Australia is one of the X 9 that X 8 〉 ⇒ 〈Aozhou shi yu X 1 you X 2 de X 9 zhiyi , Australia is one of the X 9 that have X 2 with X 1 〉 Figure 1 : Example partial derivation of a synchronous CFG .</sentence>
				<definiendum id="0">Australia</definiendum>
				<definiendum id="1">Australia</definiendum>
				<definiendum id="2">Australia</definiendum>
				<definiens id="0">one of X</definiens>
			</definition>
			<definition id="3">
				<sentence>is the product of the weights of the rules used in the translation , multiplied by the following extra factors : ( 17 ) w ( D ) = productdisplay 〈r , i , j〉∈D w ( r ) × plm ( e ) λlm ×exp ( −λwp|e| ) where plm is the language model , and exp ( −λwp|e| ) , the word penalty , gives some control over the length of the English output .</sentence>
				<definiendum id="0">plm</definiendum>
				<definiens id="0">gives some control over the length of the English output</definiens>
			</definition>
			<definition id="4">
				<sentence>The training process begins with a word-aligned corpus : a set of triples 〈f , e , ∼〉 , where f is a French sentence , e is an English sentence , and ∼ is a ( manyto-many ) binary relation between positions of f and positions of e. We obtain the word alignments using the method of Koehn et al. ( 2003 ) , which is based on that of Och and Ney ( 2004 ) .</sentence>
				<definiendum id="0">word-aligned corpus</definiendum>
				<definiendum id="1">f</definiendum>
				<definiendum id="2">∼</definiendum>
				<definiens id="0">a set of triples 〈f , e , ∼〉</definiens>
				<definiens id="1">a French sentence , e is an English sentence</definiens>
			</definition>
			<definition id="5">
				<sentence>Given a French sentence f , it finds the best derivation ( or n best derivations , with little overhead ) that generates 〈f , e〉 for some e. Note that we find the English yield of the highest-probability single derivation ( 18 ) e   arg max D s.t. f ( D ) = f w ( D )   and not necessarily the highest-probability e , which would require a more expensive summation over derivations .</sentence>
				<definiendum id="0">highest-probability e</definiendum>
				<definiens id="0">would require a more expensive summation over derivations</definiens>
			</definition>
			<definition id="6">
				<sentence>Our evaluation metric was BLEU ( Papineni et al. , 2002 ) , as calculated by the NIST script ( version 11a ) with its default settings , which is to perform case-insensitive matching of n-grams up to n = 4 , and to use the shortest ( as opposed to nearest ) reference sentence for the brevity penalty .</sentence>
				<definiendum id="0">BLEU</definiendum>
				<definiens id="0">calculated by the NIST script ( version 11a ) with its default settings</definiens>
			</definition>
			<definition id="7">
				<sentence>We used the default feature set : language model ( same as above ) , p ( ¯f | ¯e ) , p ( ¯e | ¯f ) , lexical weighting ( both directions ) , distortion model , word penalty , and phrase penalty .</sentence>
				<definiendum id="0">p</definiendum>
			</definition>
			<definition id="8">
				<sentence>Hierarchical phrase pairs , which can be learned without any syntactically-annotated training data , improve translation accuracy significantly compared with a state-of-the-art phrase-based system .</sentence>
				<definiendum id="0">Hierarchical phrase pairs</definiendum>
				<definiens id="0">any syntactically-annotated training data , improve translation accuracy significantly compared with a state-of-the-art phrase-based system</definiens>
			</definition>
</paper>

		<paper id="2013">
			<definition id="0">
				<sentence>Turkish is an agglutinating language , a single word can be a sentence with tense , modality , polarity , and voice .</sentence>
				<definiendum id="0">Turkish</definiendum>
				<definiens id="0">a sentence with tense , modality , polarity , and voice</definiens>
			</definition>
			<definition id="1">
				<sentence>Combinatory Categorial Grammar ( Ades and Steedman , 1982 ; Steedman , 2000 ) is an extension to the classical Categorial Grammar ( CG ) of Ajdukiewicz ( 1935 ) and Bar-Hillel ( 1953 ) .</sentence>
				<definiendum id="0">Combinatory Categorial Grammar</definiendum>
				<definiens id="0">an extension to the classical Categorial Grammar ( CG ) of Ajdukiewicz</definiens>
			</definition>
			<definition id="2">
				<sentence>CG , and extensions to it , are lexicalist approaches which deny the need for movement or deletion rules in syntax .</sentence>
				<definiendum id="0">CG</definiendum>
				<definiens id="0">lexicalist approaches which deny the need for movement or deletion rules in syntax</definiens>
			</definition>
			<definition id="3">
				<sentence>Some examples are : 73 ( 1 ) a. a0a2a1a3a1a5a4 a1a4a3a7a6 a1a9a8a11a10a12a10a14a13 b. a1a5a4a16a15 a1a4a3a18a17a20a19a22a21a23a6a25a24a27a26a28a21a23a6a29a24 a1a9a8a31a30a33a32a4a8a31a34a35a32a37a36a39a38a41a40a43a42a44a30a35a34 In classical CG , there are two kinds of application rules , which are presented below : ( 2 ) Forward Application ( a45 ) : a46a48a47a16a49 a1a51a50 a49 a1a53a52 a54 a46 a1a51a50a55a52 Backward Application ( a56 ) : a49 a1a53a52 a46 a21 a49 a1a51a50 a54 a46 a1a51a50a55a52 In addition to functional application rules , CCG has combinatory operators for composition ( B ) , type raising ( T ) , and substitution ( S ) .1 These operators increase the expressiveness to mildly contextsensitive while preserving the transparency of syntax and semantics during derivations , in contrast to the classical CG , which is context-free ( Bar-Hillel et al. , 1964 ) .</sentence>
				<definiendum id="0">CCG</definiendum>
				<definiens id="0">Application ( a45 ) : a46a48a47a16a49 a1a51a50 a49 a1a53a52 a54 a46 a1a51a50a55a52 Backward Application ( a56 ) : a49 a1a53a52 a46 a21 a49 a1a51a50 a54 a46 a1a51a50a55a52 In addition to functional application rules ,</definiens>
			</definition>
			<definition id="4">
				<sentence>The METU-Sabancı Treebank is a subcorpus of the METU Turkish Corpus ( Atalay et al. , 2003 ; Oflazer et al. , 2003 ) .</sentence>
				<definiendum id="0">METU-Sabancı Treebank</definiendum>
			</definition>
			<definition id="5">
				<sentence>Then , conjunctive operator is given the category ( Xa21 X ) /X where X is the category of “zıplayarak” ( or whatever the category of the last conjunct is ) , and the first conjunct takes the same category as X. The information in the treebank is not enough to distinguish sentential coordination and VP coordination .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the same category as X. The information in the treebank is not enough to distinguish sentential coordination</definiens>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>By comparing empirical contexts of a word sequence and its components , the association measures rank collocations according to the asa ) a=f ( xy ) b=f ( x¯y ) f ( x∗ ) c=f ( ¯xy ) d=f ( ¯x¯y ) f ( ¯x∗ ) f ( ∗y ) f ( ∗¯y ) N b ) Cw empirical context of w Cxy empirical context of xy Clxy left immediate context of xy Crxy right immediate context of xy Table 1 : a ) A contingency table with observed frequencies and marginal frequencies for a bigram xy ; ¯w stands for any word except w ; ∗ stands for any word ; N is a total number of bigrams .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">any word ;</definiens>
				<definiens id="1">a total number of bigrams</definiens>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>Classified advertisements , such as the one in Figure 1 ( b ) , also exhibit field structure , if less rigidly : an ad consists of descriptions of attributes of an item or offer , and a set of ads for similar items share the same attributes .</sentence>
				<definiendum id="0">Classified advertisements</definiendum>
				<definiens id="0">such as the one in Figure 1 ( b ) , also exhibit field structure , if less rigidly : an ad consists of descriptions of attributes of an item or offer , and a set of ads for similar items share the same attributes</definiens>
			</definition>
			<definition id="1">
				<sentence>An HMM consists of a set of states S , a set of observations ( in our case words or tokens ) W , a transition model specifying P ( st|st−1 ) , the probability of transitioning from state st−1 to state st , and an emission model specifying P ( w|s ) the probability of emitting word w while in state s. For a good tutorial on general HMM techniques , see Rabiner ( 1989 ) .</sentence>
				<definiendum id="0">HMM</definiendum>
			</definition>
			<definition id="2">
				<sentence>First is the most-frequentfield accuracy , achieved by labeling all tokens with the same single label which is then mapped to the most frequent field .</sentence>
				<definiendum id="0">First</definiendum>
				<definiens id="0">the most-frequentfield accuracy , achieved by labeling all</definiens>
			</definition>
			<definition id="3">
				<sentence>To adjust our procedure to learn larger-scale patterns , we can constrain the parametric form of the transition model to be P ( st|st−1 ) =    σ + ( 1−σ ) |S| if st = st−1 ( 1−σ ) |S| otherwise where |S| is the number of states , and σ is a global free parameter specifying the self-loop probability : 374 ( a ) Classified advertisements ( b ) Bibliographic citations Figure 4 : Learning curves for supervised learning and unsupervised learning with a diagonal transition matrix on ( a ) classified advertisements , and ( b ) bibliographic citations .</sentence>
				<definiendum id="0">|S|</definiendum>
				<definiendum id="1">σ</definiendum>
				<definiens id="0">a global free parameter specifying the self-loop probability : 374 ( a ) Classified advertisements ( b ) Bibliographic citations Figure 4 : Learning curves for supervised learning and unsupervised learning with a diagonal transition matrix on ( a ) classified advertisements</definiens>
			</definition>
			<definition id="4">
				<sentence>This can be accomplished with the following simple hierarchical mixture emission model Ph ( w|s ) = αPc ( w ) + ( 1 −α ) P ( w|s ) where Pc is the common word distribution , and α is 4While it may be surprising that disallowing reestimation of the transition function is helpful here , the same has been observed in acoustic modeling ( Rabiner and Juang , 1993 ) .</sentence>
				<definiendum id="0">Pc</definiendum>
				<definiendum id="1">α</definiendum>
				<definiens id="0">the common word distribution , and</definiens>
			</definition>
</paper>

		<paper id="1068">
			<definition id="0">
				<sentence>The translation model links the source language sentence to the target language sentence .</sentence>
				<definiendum id="0">translation model</definiendum>
				<definiens id="0">links the source language sentence to the target language sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>Abunsetsu/eojeol consists of two sub parts : the head part composed of content words and the tail part composed of functional words agglutinated at the end of the head part .</sentence>
				<definiendum id="0">Abunsetsu/eojeol</definiendum>
				<definiens id="0">consists of two sub parts : the head part composed of content words and the tail part composed of functional words agglutinated at the end of the head part</definiens>
			</definition>
			<definition id="2">
				<sentence>The concept of the chunk-based J/K translation framework with back-off scheme can be summarized as follows : chunk level , given sentence , ing translation : AF divide the failed chunk into a head and a tail part , 551 Figure 1 : An example of ( a ) chunk alignment for chunk-based , head-tail based translation and ( b ) bilingual verb-noun collocation by using the chunk alignment and a monolingual dependency parser AF back-off the translation into the head-tail based translation model , AF if the head or tail does not have any corresponding translation , apply a word-based translation model to the chunk .</sentence>
				<definiendum id="0">chunk level</definiendum>
				<definiendum id="1">AF</definiendum>
				<definiens id="0">An example of ( a ) chunk alignment for chunk-based , head-tail based translation and ( b ) bilingual verb-noun collocation by using the chunk alignment and a monolingual dependency parser AF back-off the translation into the head-tail based translation model</definiens>
			</definition>
			<definition id="3">
				<sentence>A verb sub-categorization is the collocation of a verb and all of its argument/adjunct nouns , i.e. verb-noun collocation ( see Figure 1 ) .</sentence>
				<definiendum id="0">verb sub-categorization</definiendum>
			</definition>
			<definition id="4">
				<sentence>By adding the weight of the AWB4BUCXCEC6 CX B5 function , we refine our model as follows : DICT C3 BD B3 CPD6CVD1CPDC C9 C3 CXBPBD C8D6B4 DI CU CP CX CYDICT CX BNDICT CXA0BD B5 ( 10 ) C8D6B4DICT CX CYDICT CXA0BD DI CU CP CXA0BD B5AC CEC6B4CU CP CX B5AWB4BUCXCEC6 CX B5 where CEC6B4CU CP CX B5 is a function indicating whether the pair of a verb and its argument CWDA CU BND2 CU CX is covered with DA CU BP CU CP CX or D2 CU BP CU CP CX and BUCXCEC6 CX BP CWDA CU BM DA CT BND2 CU BM D2 CT CX is a bilingual translation pair in the hypothesis .</sentence>
				<definiendum id="0">CEC6B4CU CP CX B5</definiendum>
				<definiens id="0">a function indicating whether the pair of a verb and its argument CWDA CU BND2 CU CX is covered with DA CU BP CU CP CX or D2 CU BP CU CP CX and BUCXCEC6 CX BP CWDA CU BM DA CT BND2 CU BM D2 CT CX is a bilingual translation pair in the hypothesis</definiens>
			</definition>
			<definition id="5">
				<sentence>BLEU , which is the ratio of the n-gram for the translation results found in the reference translations with a penalty for too short sentences ( Papineni et al. , 2001 ) .</sentence>
				<definiendum id="0">BLEU</definiendum>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>241             form :     distance-src : braceleftbig 1|2|3|4|5|more bracerightbig mood : braceleftbig none|decl|polar-q|wh-q|alt-q|imp|other bracerightbig form : braceleftbig none|particle|partial|complete bracerightbig relation-antecedent : braceleftbig none|add|repet|repet-add|reformul|indep bracerightbig boundary-tone : braceleftbig none|rising|falling|no-appl bracerightbig     function :      source : braceleftBig none|acous|lex|parsing|np-ref|deitic-ref|act-ref| int+eval|relevance|belief|ambiguity|scr-several bracerightBig extent : braceleftbig none|fragment|whole bracerightbig severity : braceleftbig none|cont-conf|cont-rep|cont-disamb|no-react bracerightbig answer : braceleftBig none|ans-repet|ans-y/n|ans-reformul|ans-elab| ans-w-defin|no-react bracerightBig satisfaction : braceleftbig none|happy-yes|happy-no|happy-ambig bracerightbig                  Figure 1 : CR classification scheme Annotation Scheme : Our annotation scheme , shown in Figure 1 , is an extention of the R &amp; S scheme described in the previous section .</sentence>
				<definiendum id="0">braceleftBig none|acous|lex|parsing|np-ref|deitic-ref|act-ref| int+eval|relevance|belief|ambiguity|scr-several bracerightBig extent</definiendum>
				<definiens id="0">braceleftbig none|cont-conf|cont-rep|cont-disamb|no-react bracerightbig answer : braceleftBig none|ans-repet|ans-y/n|ans-reformul|ans-elab| ans-w-defin|no-react bracerightBig satisfaction : braceleftbig none|happy-yes|happy-no|happy-ambig bracerightbig                  Figure 1 : CR classification scheme Annotation Scheme</definiens>
			</definition>
			<definition id="1">
				<sentence>Answer : By definition , certain types of question prompt for certain answers .</sentence>
				<definiendum id="0">Answer</definiendum>
				<definiens id="0">By definition , certain types of question prompt for certain answers</definiens>
			</definition>
			<definition id="2">
				<sentence>Extent is a binary feature indicating whether the CR points out a specific element or concerns the whole utterance .</sentence>
				<definiendum id="0">Extent</definiendum>
				<definiens id="0">a binary feature indicating whether the CR points out a specific element or concerns the whole utterance</definiens>
			</definition>
</paper>

		<paper id="1003">
</paper>

		<paper id="2005">
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>To improve accuracy , dedicated WSD models typically employ features that are not limited to the local context , and that include more linguistic information than the surface form of words .</sentence>
				<definiendum id="0">WSD</definiendum>
				<definiens id="0">models typically employ features that are not limited to the local context , and that include more linguistic information than the surface form of words</definiens>
			</definition>
			<definition id="1">
				<sentence>The training scheme consists of IBM-1 , HMM , IBM-3 and IBM-4 , following ( Och and Ney , 2003 ) .</sentence>
				<definiendum id="0">training scheme</definiendum>
			</definition>
			<definition id="2">
				<sentence>The ISI ReWrite decoder ( Germann , 2003 ) , which implements an efficient greedy decoding algorithm , is used to translate the Chinese sentences , using the alignment model and language model previously described .</sentence>
				<definiendum id="0">ISI ReWrite decoder</definiendum>
				<definiens id="0">implements an efficient greedy decoding algorithm , is used to translate the Chinese sentences , using the alignment model and language model previously described</definiens>
			</definition>
			<definition id="3">
				<sentence>In order to avoid artificially penalizing the WSD system by limiting its translation candidates to the HowNet glosses , we expand the translation set using the bilexicon learned during translation model training .</sentence>
				<definiendum id="0">HowNet glosses</definiendum>
				<definiens id="0">the translation set using the bilexicon learned during translation model training</definiens>
			</definition>
</paper>

		<paper id="2011">
			<definition id="0">
				<sentence>The FrameNet database ( Baker et al. , 1998 ) is a machine-readable lexicographic database which can be found at http : //framenet.icsi.berkeley.edu/ .</sentence>
				<definiendum id="0">FrameNet database</definiendum>
			</definition>
			<definition id="1">
				<sentence>WordNet is a lexical database that uses conceptualsemantic and lexical relations in order to group lexical items and link them to other groups ( Fellbaum , 1998 ) .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="2">
				<sentence>System Architecture As figure 1 shows , the dialog manager provides the generator with a dialog manager meaning representation ( DM MR ) , which contains the content information for the answer .</sentence>
				<definiendum id="0">DM MR )</definiendum>
				<definiens id="0">contains the content information for the answer</definiens>
			</definition>
			<definition id="3">
				<sentence>The main event ( main verb ) is identified as the lexical item buy .</sentence>
				<definiendum id="0">main event</definiendum>
				<definiens id="0">the lexical item buy</definiens>
			</definition>
</paper>

		<paper id="3017">
			<definition id="0">
				<sentence>TGrep21 is a a grep-like utility for the Penn Treebank corpus of parsed Wall Street Journal texts .</sentence>
				<definiendum id="0">TGrep21</definiendum>
			</definition>
			<definition id="1">
				<sentence>TIGERSearch2 is associated with the German syntactic corpus TIGER .</sentence>
				<definiendum id="0">TIGERSearch2</definiendum>
				<definiens id="0">associated with the German syntactic corpus TIGER</definiens>
			</definition>
			<definition id="2">
				<sentence>LPath is an extension of XPath with three features : immediate precedence , subtree scoping and edge alignment .</sentence>
				<definiendum id="0">LPath</definiendum>
				<definiens id="0">an extension of XPath with three features : immediate precedence , subtree scoping and edge alignment</definiens>
			</definition>
			<definition id="3">
				<sentence>NiteQL is the query language for the MATE annotation workbench ( McKelvie et al. , 2001 ) .</sentence>
				<definiendum id="0">NiteQL</definiendum>
			</definition>
			<definition id="4">
				<sentence>Below we illustrate our Layered Query Language ( LQL ) using examples from bioscience NLP.5 3http : //agtk.sourceforge.net/ 4http : //www.nlm.nih.gov/pubs/factsheets/medline.html 5See http : //biotext.berkeley.edu/lql/ for a formal description of the language and additional examples .</sentence>
				<definiendum id="0">Layered Query Language</definiendum>
				<definiens id="0">//www.nlm.nih.gov/pubs/factsheets/medline.html 5See http : //biotext.berkeley.edu/lql/ for a formal description of the language and additional examples</definiens>
			</definition>
			<definition id="5">
				<sentence>The MeSH layer contains entities from the hierarchical medical ontology MeSH ( Medical Subject Headings ) .8 The MeSH annotations on Figure 1 are overlapping ( share the word cell ) and hierarchical both ways : spanning , since blood cell ( with MeSH id D001773 ) orthographically spans the word cell ( id A11 ) , and ontologically , since blood cell is a kind of cell and cell death ( id D016923 ) is a kind of Biological Phenomena .</sentence>
				<definiendum id="0">MeSH layer</definiendum>
				<definiendum id="1">cell death</definiendum>
				<definiens id="0">contains entities from the hierarchical medical ontology MeSH ( Medical Subject Headings ) .8 The MeSH annotations on Figure 1 are overlapping ( share the word cell</definiens>
				<definiens id="1">a kind of Biological Phenomena</definiens>
			</definition>
			<definition id="6">
				<sentence>Our basic model is similar to that of TIPSTER ( Grishman , 1996 ) : each annotation is stored as a record , which specifies the character-level beginning and ending positions , the layer and the type .</sentence>
				<definiendum id="0">record</definiendum>
				<definiens id="0">specifies the character-level beginning and ending positions</definiens>
			</definition>
			<definition id="7">
				<sentence>Columns ( 9 ) - ( 11 ) treat the word layer as atomic and require all annotations to coincide with word boundaries .</sentence>
				<definiendum id="0">Columns</definiendum>
				<definiens id="0">treat the word layer as atomic and require all annotations to coincide with word boundaries</definiens>
			</definition>
</paper>

		<paper id="1062">
			<definition id="0">
				<sentence>This system outputs predefined features , including words and named entities in 8 types ( Name , Date , Location , Organization , Phone , Number , Period , and Email ) .</sentence>
				<definiendum id="0">Email</definiendum>
				<definiens id="0">including words and named entities in 8 types ( Name , Date , Location , Organization , Phone , Number , Period , and</definiens>
			</definition>
			<definition id="1">
				<sentence>For unknown word w r , the emission probability is x/ ( M-m i ) , where M is the number of all the words appearing in training data , 502 and m i is the number of distinct words occurring in state i. Then , we use a back-off schema ( Katz , 1987 ) to deal with the data sparseness problem when estimating the probability P ( L ) ( Gao et al. , 2003 ) .</sentence>
				<definiendum id="0">emission probability</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">the number of all the words appearing in training data</definiens>
			</definition>
			<definition id="2">
				<sentence>SVM is a binary classification model .</sentence>
				<definiendum id="0">SVM</definiendum>
				<definiens id="0">a binary classification model</definiens>
			</definition>
			<definition id="3">
				<sentence>But in our IE task , it needs to classify units into N classes , where N is two times of the number of personal detailed information types .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">two times of the number of personal detailed information types</definiens>
			</definition>
			<definition id="4">
				<sentence>We use TF×IDF as feature weight , where TF means word frequency in the text , and IDF is defined as : w N N LogwIDF 2 ) ( = ( 12 ) N : the total number of training examples ; N w : the total number of positive examples that contain word w Named Entity : Similar to the HMM models , 8 types of named entities identified by LSP , i.e. , Name , Date , Location , Organization , Phone , Number , Period , Email , are selected as binary features .</sentence>
				<definiendum id="0">TF</definiendum>
				<definiendum id="1">IDF</definiendum>
				<definiendum id="2">N w</definiendum>
				<definiens id="0">the total number of training examples</definiens>
				<definiens id="1">the total number of positive examples that contain word w Named Entity : Similar to the HMM models , 8 types of named entities identified by LSP , i.e. , Name , Date , Location , Organization , Phone , Number , Period</definiens>
			</definition>
</paper>

		<paper id="1004">
</paper>

		<paper id="3003">
			<definition id="0">
				<sentence>The median number of scope readings for the sentences in the Rondane Treebank ( distributed with the ERG ) is 55 , but the treebank also contains extreme cases such as ( 1 ) below , which according to the ERG has about 2.4 trillion ( 1012 ) readings : ( 1 ) Myrdal is the mountain terminus of the Flåm rail line ( or Flåmsbana ) which makes its way down the lovely Flåm Valley ( Flåmsdalen ) to its sea-level terminus at Flåm .</sentence>
				<definiendum id="0">Myrdal</definiendum>
				<definiendum id="1">Flåmsdalen</definiendum>
				<definiens id="0">median number of scope readings for the sentences in the Rondane Treebank ( distributed with the ERG ) is 55 , but the treebank also contains extreme cases such as ( 1 ) below , which according to the ERG has about 2.4 trillion ( 1012 ) readings : ( 1 )</definiens>
				<definiens id="1">the mountain terminus of the Flåm rail line ( or Flåmsbana ) which makes its way down the lovely Flåm Valley (</definiens>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>Section 3 presents a new model , the Switching FHMM , which represents cross-sequence dependencies more effectively than FHMMs .</sentence>
				<definiendum id="0">Switching FHMM</definiendum>
				<definiens id="0">represents cross-sequence dependencies more effectively than FHMMs</definiens>
			</definition>
			<definition id="1">
				<sentence>A Factorial Hidden Markov Model ( FHMM ) is a hidden Markov model with a distributed state representation .</sentence>
				<definiendum id="0">Factorial Hidden Markov Model</definiendum>
				<definiendum id="1">FHMM )</definiendum>
				<definiens id="0">a hidden Markov model with a distributed state representation</definiens>
			</definition>
			<definition id="2">
				<sentence>Then we define the FHMM as the probabilistic model : p ( x1 : T , y1 : T , z1 : T ) ( 1 ) = pi0 Tproductdisplay t=2 p ( xt|yt , zt ) p ( yt|yt−1 , zt ) p ( zt|zt−1 ) where pi0 = p ( x0|y0 , z0 ) p ( y0|z0 ) p ( z0 ) .</sentence>
				<definiendum id="0">FHMM</definiendum>
				<definiendum id="1">zt</definiendum>
				<definiens id="0">the probabilistic model : p ( x1 : T , y1 : T , z1 : T ) ( 1 ) = pi0 Tproductdisplay t=2 p ( xt|yt ,</definiens>
			</definition>
			<definition id="3">
				<sentence>FHMM parameters can be calculated via maximum likelihood ( ML ) estimation if the values of the hidden states are available in the training data .</sentence>
				<definiendum id="0">FHMM parameters</definiendum>
				<definiens id="0">maximum likelihood ( ML ) estimation if the values of the hidden states are available in the training data</definiens>
			</definition>
			<definition id="4">
				<sentence>An idea related to the Switching FHMM is the Bayesian Multinet ( Geiger and Heckerman , 1996 ; 21 Bilmes , 2000 ) , which allows the dynamic switching of conditional variables .</sentence>
				<definiendum id="0">Switching FHMM</definiendum>
				<definiens id="0">allows the dynamic switching of conditional variables</definiens>
			</definition>
			<definition id="5">
				<sentence>Noun-phrase ( NP ) chunking is the task of finding the non-recursive ( base ) noun-phrases of sentences .</sentence>
				<definiendum id="0">Noun-phrase</definiendum>
				<definiendum id="1">chunking</definiendum>
				<definiens id="0">the task of finding the non-recursive ( base ) noun-phrases of sentences</definiens>
			</definition>
			<definition id="6">
				<sentence>The test set contains 2012 sentences and 8k vocabulary .</sentence>
				<definiendum id="0">test set</definiendum>
				<definiens id="0">contains 2012 sentences and 8k vocabulary</definiens>
			</definition>
			<definition id="7">
				<sentence>The Switching FHMM uses the following α and β mapping .</sentence>
				<definiendum id="0">Switching FHMM</definiendum>
				<definiens id="0">uses the following α and β mapping</definiens>
			</definition>
			<definition id="8">
				<sentence>Class3 and Class4 are situations where the tag is leaving or entering an NP , and Class5 is when the tag transits between consecutive NP chunks .</sentence>
				<definiendum id="0">Class5</definiendum>
			</definition>
</paper>

		<paper id="3023">
			<definition id="0">
				<sentence>In this paper we give a brief description of a two-way speech-to-speech translation system , which was created under a collaborative effort between three organizations within USC ( the Speech Analysis and Interpretation Lab of the Electrical Engineering department , the Information Sciences Institute , and the Institute for Creative Technologies ) and the Information Sciences Lab of HRL Laboratories .</sentence>
				<definiendum id="0">USC</definiendum>
			</definition>
			<definition id="1">
				<sentence>The system is targeted at a domain which may be roughly characterized as `` urgent care '' medical interactions , where the English speaker is a medical professional and the Farsi speaker is the patient .</sentence>
				<definiendum id="0">Farsi speaker</definiendum>
				<definiens id="0">a medical professional and the</definiens>
			</definition>
			<definition id="2">
				<sentence>The MT unit works in two modes : Classifier based MT and a fully Stochastic MT. Depending on the dialogue manager mode , translations can be sent to the unit selection based Text-To-Speech synthesizer ( TTS ) , to provide the spoken output .</sentence>
				<definiendum id="0">MT unit</definiendum>
				<definiens id="0">works in two modes : Classifier based MT and a fully Stochastic MT. Depending on the dialogue manager mode , translations can be sent to the unit selection based Text-To-Speech synthesizer ( TTS ) , to provide the spoken output</definiens>
			</definition>
</paper>

		<paper id="2018">
			<definition id="0">
				<sentence>A content word is usually defined as a term , although a term can also be a phrase .</sentence>
				<definiendum id="0">content word</definiendum>
				<definiens id="0">a term , although a term can also be a phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>DegreeCentrality ( n i ) = d ( n i ) / ( u-1 ) , where d ( n i ) is the degree of node i in a network and u is the number of nodes in that network .</sentence>
				<definiendum id="0">DegreeCentrality</definiendum>
				<definiendum id="1">d</definiendum>
				<definiendum id="2">u</definiendum>
				<definiens id="0">the degree of node i in a network</definiens>
				<definiens id="1">the number of nodes in that network</definiens>
			</definition>
			<definition id="2">
				<sentence>∑ jki jk j &lt; k i g ( n ) /g BetweennessCentrality ( n ) = ( u1 ) ( u2 ) / 2 , where g jk is the number of geodesics linking node j and k , g jk ( n i ) is the number of geodesics linking the two nodes that contain node i. Betweenness centrality is widely used because of its generality. This measure assumes that information flow between two nodes will be on the geodesics between them. Nevertheless , “It is quite possible that information will take a more circuitous route either by random communication or [ by being ] channeled through many intermediaries in order to 'hide ' or 'shield ' information”. ( Stephenson and Zelen , 1989 ) . Stephenson and Zelen ( 1989 ) developed information centrality which generalizes betweenness centrality. It focuses on the information contained in all paths originating with a specific actor. The calculation for information centrality of a node is in the Appendix. Recently centrality measures have started to gain attention from researchers in text processing. Corman et al. ( 2002 ) use vectors , which consist of NPs , to represent texts and hence analyze mutual relevance of two texts. The values of the elements in a vector are determined by the betweenness centrality of the NPs in a text being analyzed. Erkan and Radev ( 2004 ) use the PageRank method , which is the application of centrality concept to the Web , to determine central sentences in a cluster for summarization. Vanderwende et al. ( 2004 ) also use the PageRank method to pick prominent triples , i.e. ( node i , relation , node j ) , and then use the triples to generate event-centric summaries. To construct a network for NPs in a text , we try two ways of modeling the relation between them. One is at the sentence level : if two noun phrases can be sequentially parsed out from a sentence , a link is added between them. The other way is at the document level : we simply add a link to every pair of noun phrases which are parsed out in succession. The difference between the two ways is that the network constructed at the sentence level ignores the existence of certain connections between sentences. We process a text document in four steps. First , the text is tokenized and stored into an internal representation with structural information. Second , the tokenized text is tagged by the Brill tagging algorithm POS tagger. 1 Third , the NPs in a text document are parsed according to 35 parsing rules as shown in Figure 1. If a new noun phrase is found , a new node is formed and added to the network. If the noun phrase already exists in the network , the node containing it will be identified. A link will be added between two nodes if they are parsed out sequentially for 1 The POS tagger we used can be obtained from http : //web.media.mit.edu/~hugo/montytagger/ 104 the network formed at the document level , or sequentially in the same sentence for the network formed at the sentence level. Finally , after the text document has been processed , the centrality of each node in the network is updated. In this paper , we refer the NPs occur both in a text document and its corresponding abstract as Cooccurring NPs ( CNPs ) . In our experiment , a corpus of 183 documents was used. The documents are from the Computation and Language collection and have been marked in XML with tags providing basic information about the document such as title , author , abstract , body , sections , etc. This corpus is a part of the TIPSTER Text Summarization Evaluation Conference ( SUMMAC ) effort acting as a general resource to the information retrieval , extraction and summarization communities. We excluded five documents from this corpus which do not have abstracts. We assume that a noun phrase with high centrality is more likely to be a central topic being addressed in a document than one with low centrality. Given this assumption , we performed an experiment , in which the NPs with highest centralities are retrieved and compared with the actual NPs in the abstracts. To evaluate this method , we use Precision , which measures the fraction of true CNPs in all predicted CNPs , and Recall , which measures the fraction of correctly predicted CNPs in all CNPs. After establishing the NP network for a document and ranking the nodes according to their centralities , we must decide how many NPs should be retrieved. This number should not be too big ; otherwise the Precision value will be very low , although the Recall will be higher. If this number is very small , the Recall will decrease correspondingly. We adopted a compound metric － Fmeasure , to balance the selection : Based on our study of 178 documents in the CMP-LG corpus , we find that the number of CNPs is roughly proportional to the number of NPs in the abstract. We obtain a linear regression model for the data shown in Figure 2 and use this model to calculate the number of nodes we should retrieve from the NP network , given the number of NPs in the abstract known a priori : One could argue that the number of abstract NPs is unknown a priori and thus the proposed method is of limited use. However , the user can provide an estimate based on the desired number of words in the summary. Here we can adopt the same way of asking the user to provide a limit for the NPs in the summary. We used the actual number of NPs the author used in his/her abstract in our experiment. Figure 2. Scatter Plot of CNPs 0 5 10 15 20 25 30 35 40 0 10203040506070 Number of NPs in Abstract Num b er o f CNP s Our experiment results are shown in Figure 3 ( a ) and 3 ( b ) . In 3 ( a ) the NP network is formed at senNX -- &gt; CD NX -- &gt; CD NNS NX -- &gt; NN NX -- &gt; NN NN NX -- &gt; NN NNS NX -- &gt; NN NNS NN NX -- &gt; NNP NX -- &gt; NNP CD NX -- &gt; NNP NNP NX -- &gt; NNP NNPS NX -- &gt; NNP NN NX -- &gt; NNP NNP NNP NX -- &gt; JJ NN NX -- &gt; JJ NNS NX -- &gt; JJ NN NNS NX -- &gt; PRP $ NNS NX -- &gt; PRP $ NN NX -- &gt; PRP $ NN NN NX -- &gt; NNS NX -- &gt; PRP NX -- &gt; WP $ NNS NX -- &gt; WDT NX -- &gt; EX NX -- &gt; WP NX -- &gt; DT JJ NN NX -- &gt; DT CD NNS NX -- &gt; DT VBG NN NX -- &gt; DT NNS NX -- &gt; DT NN NX -- &gt; DT NN NN NX -- &gt; DT NNP NX -- &gt; DT NNP NN NX -- &gt; DT NNP NNP NX -- &gt; DT NNP NNP NNP NX -- &gt; DT NNP NNP NN NN Figure 1 .</sentence>
				<definiendum id="0">BetweennessCentrality</definiendum>
				<definiendum id="1">g jk</definiendum>
				<definiendum id="2">n i )</definiendum>
				<definiens id="0">generalizes betweenness centrality. It focuses on the information contained in all paths originating with a specific actor. The calculation for information centrality of a node is in the Appendix. Recently centrality measures have started to gain attention from researchers in text processing. Corman et al. ( 2002 ) use vectors , which consist of NPs , to represent texts and hence analyze mutual relevance of two texts. The values of the elements in a vector are determined by the betweenness centrality of the NPs in a text being analyzed. Erkan and Radev ( 2004 ) use the PageRank method , which is the application of centrality concept to the Web , to determine central sentences in a cluster for summarization. Vanderwende et al. ( 2004 ) also use the PageRank method to pick prominent triples , i.e. ( node i , relation , node j ) , and then use the triples to generate event-centric summaries. To construct a network for NPs in a text</definiens>
				<definiens id="1">documents are from the Computation and Language collection and have been marked in XML with tags providing basic information about the document such as title , author , abstract , body , sections , etc. This corpus is a part of the TIPSTER Text Summarization Evaluation Conference ( SUMMAC ) effort acting as a general resource to the information retrieval</definiens>
				<definiens id="2">measures the fraction of true CNPs in all predicted CNPs , and Recall , which measures the fraction of correctly predicted CNPs in all CNPs. After establishing the NP network for a document and ranking the nodes according to their centralities</definiens>
				<definiens id="3">the user can provide an estimate based on the desired number of words in the summary. Here we can adopt the same way of asking the user to provide a limit for the NPs in the summary. We used the actual number</definiens>
				<definiens id="4">a ) the NP network is formed at senNX -- &gt; CD NX -- &gt; CD NNS NX -- &gt; NN NX -- &gt; NN NN NX -- &gt; NN NNS NX -- &gt; NN NNS NN NX -- &gt; NNP NX -- &gt; NNP CD NX -- &gt; NNP NNP NX -- &gt; NNP NNPS NX -- &gt; NNP NN NX -- &gt; NNP NNP NNP NX -- &gt; JJ NN NX -- &gt; JJ NNS NX -- &gt; JJ NN NNS NX -- &gt; PRP $ NNS NX -- &gt; PRP $ NN NX -- &gt; PRP $ NN NN NX -- &gt; NNS NX -- &gt; PRP NX -- &gt; WP $ NNS NX -- &gt; WDT NX -- &gt; EX NX -- &gt; WP NX -- &gt; DT JJ NN NX -- &gt; DT CD NNS NX -- &gt; DT VBG NN NX -- &gt; DT NNS NX -- &gt; DT NN NX -- &gt; DT NN NN NX -- &gt; DT NNP NX -- &gt; DT NNP NN NX -- &gt; DT NNP NNP NX -- &gt; DT NNP NNP NNP NX -- &gt; DT NNP NNP NN NN Figure 1</definiens>
			</definition>
</paper>

		<paper id="2016">
			<definition id="0">
				<sentence>The goal of statistical machine translation ( SMT ) is to develop mathematical models of the translation process whose parameters can be automatically estimated from a parallel corpus .</sentence>
				<definiendum id="0">SMT</definiendum>
				<definiens id="0">to develop mathematical models of the translation process whose parameters can be automatically estimated from a parallel corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>Given a string of foreign words F , we seek to find the English string E which is a “correct” translation of the foreign string .</sentence>
				<definiendum id="0">string E</definiendum>
				<definiens id="0">a “correct” translation of the foreign string</definiens>
			</definition>
			<definition id="2">
				<sentence>IND ( indicator ) is a loosely-specified feature comprised of functors , where assigned , and other words or small phrases ( often prepositions ) which are attached to the node and indicate something about the node’s function in the sentence .</sentence>
				<definiendum id="0">IND</definiendum>
				<definiens id="0">a loosely-specified feature comprised of functors , where assigned , and other words or small phrases ( often prepositions ) which are attached to the node and indicate something about the node’s function in the sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>We propose to evaluate system performance with version 0.9 of the NIST automated scorer ( NIST , 2002 ) , which is a modification of the BLEU system ( Papineni et al. , 2001 ) .</sentence>
				<definiendum id="0">NIST automated scorer</definiendum>
			</definition>
</paper>

		<paper id="2008">
			<definition id="0">
				<sentence>Sentiment Classification seeks to identify a piece of text according to its author’s general feeling toward their subject , be it positive or negative .</sentence>
				<definiendum id="0">Sentiment Classification</definiendum>
				<definiens id="0">seeks to identify a piece of text according to its author’s general feeling toward their subject</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>The energy function of a spin system can be represented as E ( x , W ) = −12 summationdisplay ij wijxixj , ( 1 ) where xi and xj ( ∈ x ) are spins of electrons i and j , matrix W = { wij } represents weights between two electrons .</sentence>
				<definiendum id="0">E</definiendum>
			</definition>
			<definition id="1">
				<sentence>We next set weights W = ( wij ) to links : wij =    1√ d ( i ) d ( j ) ( lij ∈ SL ) − 1√d ( i ) d ( j ) ( lij ∈ DL ) 0 otherwise , ( 8 ) where lij denotes the link between word i and word j , and d ( i ) denotes the degree of word i , which means the number of words linked with word i. Two words without connections are regarded as being connected by a link of weight 0 .</sentence>
				<definiendum id="0">lij</definiendum>
				<definiens id="0">wij ) to links : wij =    1√ d ( i ) d ( j ) ( lij ∈ SL</definiens>
				<definiens id="1">the link between word i and word j</definiens>
				<definiens id="2">the degree of word i , which means the number of words linked with word i. Two words without connections are regarded as being connected by a link of weight 0</definiens>
			</definition>
			<definition id="2">
				<sentence>Instead of βE ( x , W ) in Equation ( 2 ) , we use the following function H ( β , x , W ) : H ( β , x , W ) = −β2 summationdisplay ij wijxixj +αsummationdisplay i∈L ( xi −ai ) 2 , ( 9 ) where L is the set of seed words , ai is the orientation of seed word i , and α is a positive constant .</sentence>
				<definiendum id="0">L</definiendum>
				<definiendum id="1">α</definiendum>
				<definiens id="0">H ( β , x</definiens>
				<definiens id="1">the set of seed words</definiens>
				<definiens id="2">the orientation of seed word i</definiens>
				<definiens id="3">a positive constant</definiens>
			</definition>
			<definition id="3">
				<sentence>¯xoldi and ¯xnewi are the averages of xi respectively before and after update .</sentence>
				<definiendum id="0">¯xnewi</definiendum>
				<definiens id="0">the averages of xi respectively before and after update</definiens>
			</definition>
			<definition id="4">
				<sentence>The whole network consists of approximately 88,000 words .</sentence>
				<definiendum id="0">whole network</definiendum>
			</definition>
			<definition id="5">
				<sentence>WordNet : An Electronic Lexical Database , Language , Speech , and Communication Series .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">An Electronic Lexical Database , Language , Speech , and Communication Series</definiens>
			</definition>
			<definition id="6">
				<sentence>The General Inquirer : A Computer Approach to Content Analysis .</sentence>
				<definiendum id="0">General Inquirer</definiendum>
				<definiens id="0">A Computer Approach to Content Analysis</definiens>
			</definition>
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>The paper presents the Position Specific Posterior Lattice , a novel representation of automatic speech recognition lattices that naturally lends itself to efficient indexing of position information and subsequent relevance ranking of spoken documents using proximity .</sentence>
				<definiendum id="0">Position Specific Posterior Lattice</definiendum>
				<definiens id="0">a novel representation of automatic speech recognition lattices that naturally lends itself to efficient indexing of position information and subsequent relevance ranking of spoken documents using proximity</definiens>
			</definition>
			<definition id="1">
				<sentence>The Position Specific Posterior 443 Lattice ( PSPL ) is a lossy but compact representation of a speech recognition lattice that lends itself to the standard inverted indexing done in text search — which retains the position as well as other contextual information for each hit .</sentence>
				<definiendum id="0">Position Specific Posterior 443 Lattice ( PSPL )</definiendum>
				<definiens id="0">a lossy but compact representation of a speech recognition lattice that lends itself to the standard inverted indexing done in text search — which retains the position as well as other contextual information for each hit</definiens>
			</definition>
			<definition id="2">
				<sentence>( 1 ) , respectively — see Figure 1 for notation : αn [ l +1 ] = qsummationdisplay i=1 αsi [ l +δ ( li , epsilon1 ) ] ·P ( li ) αstart [ l ] = braceleftbigg1.0 , l = 0 The “probability” P ( li ) of a given link li is stored as a log-probability and commonly evaluated in ASR using : logP ( li ) = FLATw· [ 1/LMw·logPAM ( li ) + logPLM ( word ( li ) ) −1/LMw·logPIP ] ( 2 ) 446 where logPAM ( li ) is the acoustic model score , logPLM ( word ( li ) ) is the language model score , LMw &gt; 0 is the language model weight , logPIP &gt; 0 is the “insertion penalty” and FLATw is a flattening weight .</sentence>
				<definiendum id="0">FLATw</definiendum>
				<definiens id="0">the acoustic model score</definiens>
				<definiens id="1">the language model score</definiens>
				<definiens id="2">a flattening weight</definiens>
			</definition>
			<definition id="3">
				<sentence>The final collection consists of 169 documents , 66,102 segments and an average document length of 391 segments .</sentence>
				<definiendum id="0">final collection</definiendum>
				<definiens id="0">consists of 169 documents , 66,102 segments and an average document length of 391 segments</definiens>
			</definition>
</paper>

		<paper id="1075">
			<definition id="0">
				<sentence>Zipf 's formulation of this relationship for single word frequency distributions ( Zipf 's first law ) postulates that the frequency of a word is inversely proportional to its rank in the frequency distribution , or more generally if we rank words by frequency and assign rank z , where the function fz ( z , N ) gives the frequency of rank z for a sample of size N , Zipf 's first law states that : fz ( z , N ) = Czα where C is a normalizing constant and is a free parameter that determines the exact degree of skew ; typically with single word frequency data , approximates 1 ( Baayen 2001 : 14 ) .</sentence>
				<definiendum id="0">C</definiendum>
			</definition>
			<definition id="1">
				<sentence>A normalized expectation for the n-gram is then calculated as follows : 1 2 1 2 ( [ , ... ] ) ( [ , ... ] ) n n p w w w FPE w w w where [ w1 , w2 ... wn ] is the phrase being evaluated and FPE ( [ w1 , w2 ... wn ] ) is : 1 2 1 1 ^1 ( [ , ... ] ) [ ... ... . ] n n i n i p w w w p w w wn = + where wi is the term omitted from the n-gram .</sentence>
				<definiendum id="0">normalized expectation</definiendum>
				<definiendum id="1">FPE</definiendum>
				<definiens id="0">the phrase being evaluated</definiens>
			</definition>
			<definition id="2">
				<sentence>The rank ratio ( RR ) for the word given the context can then be defined as : RR ( word , context ) = ( ) ( ) , ,ER word contextAR word context where ER is the expected rank and AR is the actual rank .</sentence>
				<definiendum id="0">RR</definiendum>
				<definiendum id="1">ER</definiendum>
				<definiendum id="2">AR</definiendum>
				<definiens id="0">the expected rank and</definiens>
				<definiens id="1">the actual rank</definiens>
			</definition>
			<definition id="3">
				<sentence>Expected ranks are calculated for the same set of contexts using the same algorithm , but substituting the unconditional frequency of the ( n-1 ) -gram for the gram 's frequency with the target word.1 The Lexile Corpus is a collection of documents covering a wide range of reading materials such as a child might encounter at school , more or less evenly divided by Lexile ( reading level ) rating to cover all levels of textual complexity from kindergarten to college .</sentence>
				<definiendum id="0">Expected ranks</definiendum>
				<definiendum id="1">Lexile Corpus</definiendum>
				<definiens id="0">a collection of documents covering a wide range of reading materials such as a child might encounter at school , more or less evenly divided by Lexile ( reading level</definiens>
			</definition>
			<definition id="4">
				<sentence>The formula is : 1 1 k i i PK = where Pi ( precision at i ) equals i/Hi , and Hi is the number of n-grams into the ranked n-gram list required to find the ith correct phrasal term .</sentence>
				<definiendum id="0">Pi ( precision</definiendum>
				<definiendum id="1">Hi</definiendum>
			</definition>
			<definition id="5">
				<sentence>METRIC FORMULA Frequency ( Guiliano , 1964 ) x yf Pointwise Mutual Information [ PMI ] ( Church &amp; Hanks , 1990 ) ( ) xy x y2log /P P P True Mutual Information [ TMI ] ( Manning , 1999 ) ( ) xy 2 xy x ylog /P P P P Chi-Squared ( 2χ ) ( Church and Gale , 1991 ) { } { } , , 2 ( ) i X X Y Y i j i j i j j f ζ ζ∈ ∈ − T-Score ( Church &amp; Hanks , 1990 ) 1 2 2 2 1 2 1 2 x x s s n n − + C-Values4 ( Frantzi , Anadiou &amp; Mima 2000 ) 2 is not nested 2 log ( ) log ( ) 1 ( ) ( ) a a b T a f f f b P T α α α α ∈ ⋅ ⋅ − where is the candidate string f ( ) is its frequency in the corpus T is the set of candidate terms that contain P ( T ) is the number of these candidate terms 609 1,700 of the three-word phrases are attested in the Lexile corpus .</sentence>
				<definiendum id="0">METRIC FORMULA Frequency</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">not nested 2 log ( ) log ( ) 1 ( ) ( ) a a b T a f f f b P T α α α α ∈ ⋅ ⋅ − where is the candidate string f ( ) is its frequency in the corpus</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>The 50-best parser is a probabilistic parser that on its own produces high quality parses ; the maximum probability parse trees ( according to the parser’s model ) have an f-score of 0.897 on section 23 of the Penn Treebank ( Charniak , 2000 ) , which is still state-of-the-art .</sentence>
				<definiendum id="0">50-best parser</definiendum>
				<definiendum id="1">maximum probability parse trees</definiendum>
				<definiens id="0">a probabilistic parser that on its own produces high quality parses</definiens>
			</definition>
			<definition id="1">
				<sentence>A good example of this is the Roark parser ( Roark , 2001 ) which works left-to right through the sentence , and abjures dynamic programming in favor of a beam search , keeping some large number of possibilities to extend by adding the next word , and then re-pruning .</sentence>
				<definiendum id="0">Roark parser</definiendum>
				<definiens id="0">works left-to right through the sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>Dynamic programming parsing algorithms for PCFGs require O ( m2 ) dynamic programming states , where m is the length of the sentence , so an n-best parsing algorithm requires O ( nm2 ) .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">the length of the sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>The edges are then pruned according to their marginal probability conditioned on the string s being parsed as follows : p ( nij , k | s ) = α ( n ij , k ) β ( nij , k ) p ( s ) ( 1 ) Here nij , k is a constituent of type i spanning the words from j to k , α ( nij , k ) is the outside probability of this constituent , and β ( nij , k ) is its inside probability .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">pruned according to their marginal probability conditioned on the string s being parsed as follows : p ( nij , k | s ) = α ( n ij , k ) β</definiens>
				<definiens id="1">a constituent of type i spanning the words from j to k , α</definiens>
			</definition>
			<definition id="4">
				<sentence>The final column gives the value of the function 100∗L1.5 where L is the average length of sentences in the bucket .</sentence>
				<definiendum id="0">L</definiendum>
				<definiens id="0">the average length of sentences in the bucket</definiens>
			</definition>
			<definition id="5">
				<sentence>Each feature fj is a function that maps a parse to a real number .</sentence>
				<definiendum id="0">feature fj</definiendum>
				<definiens id="0">a function that maps a parse to a real number</definiens>
			</definition>
			<definition id="6">
				<sentence>We use a MaxEnt estimator to find the feature weights ˆθ , where L is the loss function and R is a regularization penalty term : ˆθ = argmin θ LD ( θ ) + R ( θ ) .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">a MaxEnt estimator to find the feature weights ˆθ , where L is the loss function and</definiens>
				<definiens id="1">a regularization penalty term : ˆθ = argmin θ LD ( θ ) + R ( θ )</definiens>
			</definition>
			<definition id="7">
				<sentence>The training data D = ( s1 , ... , snprime ) is a sequence of sentences and their correct parses ystar ( s1 ) , ... , ystar ( sn ) .</sentence>
				<definiendum id="0">s1 , ... , snprime )</definiendum>
				<definiens id="0">a sequence of sentences and their correct parses ystar ( s1 ) , ...</definiens>
			</definition>
			<definition id="8">
				<sentence>The loss function LD proposed in Riezler et al. ( 2002 ) is just the negative log conditional likelihood of the best parses Y+ ( s ) relative to the n-best parser output Y ( s ) : LD ( θ ) = − nprimesummationdisplay i=1 logPθ ( Y+ ( si ) |Y ( si ) ) , where Pθ ( Y+|Y ) = summationdisplay y∈Y+ Pθ ( y|Y ) The partial derivatives of this loss function , which are required by the numerical estimation procedure , are : ∂LD θj = nprimesummationdisplay i=1 Eθ [ fj|Y ( si ) ] − Eθ [ fj|Y+ ( si ) ] Eθ [ f|Y ] = summationdisplay y∈Y f ( y ) Pθ ( y|Y ) In the experiments reported here , we used a Gaussian or quadratic regularizer R ( w ) = csummationtextmj=1 w2j , where c is an adjustable parameter that controls the amount of regularization , chosen to optimize the reranker’s f-score on the development set ( section 24 of the treebank ) .</sentence>
				<definiendum id="0">Pθ</definiendum>
				<definiendum id="1">y∈Y f ( y ) Pθ</definiendum>
				<definiendum id="2">c</definiendum>
				<definiens id="0">a Gaussian or quadratic regularizer R ( w ) = csummationtextmj=1 w2j</definiens>
			</definition>
</paper>

		<paper id="1067">
			<definition id="0">
				<sentence>Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees .</sentence>
				<definiendum id="0">Synchronous dependency insertion grammars</definiendum>
			</definition>
			<definition id="1">
				<sentence>A Dependency Insertion Grammars ( DIG ) is a generative grammar formalism that captures word order phenomena within the dependency representation .</sentence>
				<definiendum id="0">Dependency Insertion Grammars ( DIG )</definiendum>
				<definiens id="0">a generative grammar formalism that captures word order phenomena within the dependency representation</definiens>
			</definition>
			<definition id="2">
				<sentence>However , a synchronous derivation process can not handle two types of cross-language mappings : crossing-dependencies ( parent-descendent switch ) and broken dependencies ( descendent appears elsewhere ) , which are illustrated below : Figure 3 .</sentence>
				<definiendum id="0">cross-language mappings</definiendum>
				<definiendum id="1">crossing-dependencies</definiendum>
				<definiens id="0">parent-descendent switch ) and broken dependencies ( descendent appears elsewhere</definiens>
			</definition>
			<definition id="3">
				<sentence>Such broken and crossing dependencies can be modeled by SDIG if they appear inside a pair of elementary trees .</sentence>
				<definiendum id="0">SDIG</definiendum>
			</definition>
			<definition id="4">
				<sentence>y Inside-outside penalties : here the probabilities of the inside English subtree generating the outside foreign residual tree and outside English residual tree generating the inside English subtree are used as penalty terms .</sentence>
				<definiendum id="0">y Inside-outside penalties</definiendum>
				<definiens id="0">here the probabilities of the inside English subtree generating the outside foreign residual tree and outside English residual tree generating the inside English subtree are used as penalty terms</definiens>
			</definition>
			<definition id="5">
				<sentence>y Entropy : the entropy of the word to word translation probability of the English word i e .</sentence>
				<definiendum id="0">y Entropy</definiendum>
				<definiens id="0">the entropy of the word to word translation probability of</definiens>
			</definition>
			<definition id="6">
				<sentence>We have P ( | ) P ( ) P ( | ) P ( ) f ee ef f = , and the best translation is : *argmaxP ( | ) P ( ) e efe= ( 2 ) P ( | ) fe and P ( ) e are also known as the “translation model” ( TM ) and the “language model” ( LM ) .</sentence>
				<definiendum id="0">“language model” ( LM</definiendum>
				<definiens id="0">P ( | ) P ( ) P ( | ) P ( ) f ee ef f = , and the best translation is : *argmaxP ( | ) P ( ) e efe= ( 2 ) P ( | ) fe and P ( ) e are also known as the “translation model” ( TM ) and the</definiens>
			</definition>
			<definition id="7">
				<sentence>In the model , the left side is the input dependency tree ( foreign language ) and the right side is the output dependency tree ( English ) .</sentence>
				<definiendum id="0">side</definiendum>
				<definiens id="0">the input dependency tree ( foreign language ) and the right</definiens>
			</definition>
			<definition id="8">
				<sentence>We have : D ( T ( ) ) , D ( T ( ) ) , Tran ( ) P ( | , ) P ( T ( ) | P ( T ( ) , ) P ( | ) ufvevu feD f eD uv ∈∈∈ = = ∏ ( 4 ) For any ET v in a given ET derivation tree d , let Root ( ) d be the root ET of d , and let Parent ( ) v denote the parent ET of v .</sentence>
				<definiendum id="0">P ( T ( ) | P ( T</definiendum>
				<definiens id="0">any ET v in a given ET derivation tree d , let Root ( ) d be the root ET of d , and let Parent ( ) v denote the parent ET of v</definiens>
			</definition>
			<definition id="9">
				<sentence>We have : ( ) ( ) D ( T ( ) ) , Root ( D ( T ( ) ) P ( | ) P ( T ( ) | ) P Root D ( T ( ) P ( | Parent ( ) ) vev e eD e D e vv ∈≠ = =⋅  ⋅   ∏ ( 5 ) where , letting root ( ) v denote the root word of v , ( ) ( ) ( ) P | Parent ( ) P root ( ) | root Parent ( ) vv v v= ( 6 ) The prior probability of tree decomposition is defined as : ( ) D ( T ( ) ) PD ( T ( ) ) P ( ) uf fu ∈ = ∏ ( 7 ) Figure 7 Comparing to the HMM An analogy between our model and a Hidden Markov Model ( Figure 7 ) may be helpful .</sentence>
				<definiendum id="0">Root ( D</definiendum>
				<definiendum id="1">Hidden Markov Model</definiendum>
				<definiens id="0">T ( ) ) P ( | ) P ( T ( ) | ) P Root D ( T ( ) P ( | Parent</definiens>
				<definiens id="1">the root word of v , ( ) ( ) ( ) P | Parent ( ) P root ( ) | root Parent ( ) vv v v= ( 6 ) The prior probability of tree decomposition is defined as : ( ) D ( T ( ) ) PD ( T ( ) ) P ( ) uf fu ∈ = ∏ ( 7 ) Figure 7 Comparing to the HMM An analogy between our model and a</definiens>
			</definition>
			<definition id="10">
				<sentence>Evaluation data details The training set consists of Xinhua newswire data from LDC and the FBIS data ( mostly news ) , both filtered to ensure parallel sentence pair quality .</sentence>
				<definiendum id="0">training set</definiendum>
				<definiens id="0">Xinhua newswire data from LDC and the FBIS data ( mostly news ) , both filtered to ensure parallel sentence pair quality</definiens>
			</definition>
			<definition id="11">
				<sentence>While statistical modeling of children reordering is one possible remedy for this problem , we believe simple linguistic treatment is another , as the output of the SDIG system is an English dependency tree rather than a string of words .</sentence>
				<definiendum id="0">SDIG system</definiendum>
				<definiens id="0">an English dependency tree rather than a string of words</definiens>
			</definition>
</paper>

		<paper id="3019">
			<definition id="0">
				<sentence>Word Sense Disambiguation is the task of identifying the intended meaning of a given target word from the context in which it is used .</sentence>
				<definiendum id="0">Word Sense Disambiguation</definiendum>
			</definition>
			<definition id="1">
				<sentence>For example , the Compound Detection Module identifies sequences of tokens that form compound words that are known as concepts to WordNet ( such as “New York City” ) .</sentence>
				<definiendum id="0">Compound Detection Module</definiendum>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>Our set consists of four symbols : S ( subject ) , O ( object ) , X ( neither subject nor object ) and – ( gap which signals the entity’s absence from a given sentence ) .</sentence>
				<definiendum id="0">X (</definiendum>
				<definiens id="0">four symbols : S ( subject )</definiens>
				<definiens id="1">neither subject nor object ) and – ( gap which signals the entity’s absence from a given sentence )</definiens>
			</definition>
			<definition id="1">
				<sentence>A local entity transition is a sequence { S , O , X , – } n that represents entity occurrences and their syntactic roles in n adjacent sentences .</sentence>
				<definiendum id="0">local entity transition</definiendum>
				<definiens id="0">a sequence { S , O , X , – } n that represents entity occurrences and their syntactic roles in n adjacent sentences</definiens>
			</definition>
			<definition id="2">
				<sentence>Each grid rendering j of a document di is represented by a feature vector Φ ( xi j ) = ( p1 ( xi j ) , p2 ( xi j ) , ... , pm ( xi j ) ) , where m is the number of all predefined entity transitions , and pt ( xi j ) the probability of transition t in grid xi j. Note that considerable latitude is available when specifying the transition types to be included in a feature vector .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">xi j ) , ... , pm ( xi j ) )</definiens>
				<definiens id="1">the number of all predefined entity transitions , and pt ( xi j ) the probability of transition t in grid xi j. Note that considerable latitude is available when specifying the transition types to be included in a feature vector</definiens>
			</definition>
			<definition id="3">
				<sentence>The training set consists of ordered pairs of renderings ( xi j , xik ) , where xi j and xik are renderings 1The frequency threshold is empirically determined on the development set .</sentence>
				<definiendum id="0">training set</definiendum>
				<definiens id="0">empirically determined on the development set</definiens>
			</definition>
</paper>

		<paper id="3008">
			<definition id="0">
				<sentence>To the best of our knowledge , Clarissa is the first spoken dialogue application in space .</sentence>
				<definiendum id="0">Clarissa</definiendum>
				<definiens id="0">the first spoken dialogue application in space</definiens>
			</definition>
			<definition id="1">
				<sentence>ISS procedures are formal documents that typically represent many hundreds of person hours of preparation , and undergo a strict approval process .</sentence>
				<definiendum id="0">ISS procedures</definiendum>
				<definiens id="0">formal documents that typically represent many hundreds of person hours of preparation , and undergo a strict approval process</definiens>
			</definition>
			<definition id="2">
				<sentence>Clarissa uses a grammar-based recognition architecture .</sentence>
				<definiendum id="0">Clarissa</definiendum>
				<definiens id="0">uses a grammar-based recognition architecture</definiens>
			</definition>
			<definition id="3">
				<sentence>Interpretation uses a set of user-specified patterns , which can match either the surface strings produced by both the statistical and grammar-based architectures , or the logical forms produced by the grammar-based architecture .</sentence>
				<definiendum id="0">Interpretation</definiendum>
				<definiens id="0">uses a set of user-specified patterns , which can match either the surface strings produced by both the statistical and grammar-based architectures , or the logical forms produced by the grammar-based architecture</definiens>
			</definition>
			<definition id="4">
				<sentence>The “Error” columns show the proportion of utterances which produce no semantic interpretation ( “Reject” ) , the proportion with an incorrect semantic interpretation ( “Bad” ) , and the total .</sentence>
				<definiendum id="0">“Error” columns</definiendum>
				<definiendum id="1">semantic interpretation</definiendum>
				<definiens id="0">show the proportion of utterances which produce no semantic interpretation ( “Reject” ) , the proportion with an incorrect</definiens>
			</definition>
</paper>

		<paper id="3020">
			<definition id="0">
				<sentence>79 accident N expensive A reform N ( +V ) belief N familiar A ( +N ) rural A birth N ( +V ) finance N V screen N ( +V ) breath N grow V N ( -N ) seek V ( +N ) brief A N imagine V serve V ( +N ) broad A ( +N ) introduction N slow A V busy A V link N V spring N A V ( -A ) catch V N lovely A ( +N ) strike N V critical A lunch N ( +V ) suit N ( +V ) cup N ( +V ) maintain V surprise N V dangerous A occur V N ( -N ) tape N V discuss V option N thank V A ( -A ) drop V N pleasure N thin A ( +V ) drug N ( +V ) protect V tiny A empty A V ( +N ) prove V widely A N ( -N ) encourage V quick A ( +N ) wild A ( +N ) establish V rain N ( +V ) Table 2 .</sentence>
				<definiendum id="0">+V ) cup N ( +V</definiendum>
				<definiens id="0">) maintain V surprise N V dangerous A occur V N ( -N ) tape N V discuss V option N thank V A ( -A ) drop V N pleasure N thin A ( +V ) drug N ( +V ) protect V tiny A empty A V ( +N ) prove V widely A N</definiens>
			</definition>
			<definition id="1">
				<sentence>Acknowledgements I would like to thank Manfred Wettler and Christian Biemann for comments , Hinrich Schütze for the SVD-software , and the DFG ( German Research Society ) for financial support .</sentence>
				<definiendum id="0">DFG</definiendum>
				<definiens id="0">comments</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>As observed by Kahane et al. ( 1998 ) , any ( nonprojective ) dependency graph can be transformed into a projective one by a lifting operation , which replaces each non-projective arc wj →wk by a projective arc wi → wk such that wi →∗ wj holds in the original graph .</sentence>
				<definiendum id="0">lifting operation</definiendum>
				<definiens id="0">replaces each non-projective arc wj →wk by a projective arc wi → wk such that wi →∗ wj holds in the original graph</definiens>
			</definition>
			<definition id="1">
				<sentence>The baseline simply retains the original labels for all arcs , regardless of whether they have been lifted or not , and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme , called Head , we use a new label d↑h for each lifted arc , where d is the dependency relation between the syntactic head and the dependent in the non-projective representation , and h is the dependency relation that the syntactic head has to its own head in the underlying structure .</sentence>
				<definiendum id="0">d</definiendum>
				<definiendum id="1">h</definiendum>
				<definiens id="0">the dependency relation between the syntactic head and the dependent in the non-projective representation , and</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>The challenge is how to sample sequences efficiently from the conditional distribution defined by the model .</sentence>
				<definiendum id="0">challenge</definiendum>
				<definiens id="0">how to sample sequences efficiently from the conditional distribution defined by the model</definiens>
			</definition>
			<definition id="1">
				<sentence>A CRF is a conditional sequence model which represents the probability of a hidden state sequence given some observations .</sentence>
				<definiendum id="0">CRF</definiendum>
			</definition>
			<definition id="2">
				<sentence>way that is consistent with the Markov Network literature ( see Cowell et al. ( 1999 ) ) : we create a linear chain of cliques , where each clique , c , represents the probabilistic relationship between an adjacent pair of states2 using a clique potential φc , which is just a table containing a value for each possible state assignment .</sentence>
				<definiendum id="0">c ,</definiendum>
				<definiens id="0">represents the probabilistic relationship between an adjacent pair of states2 using a clique potential φc , which is just a table containing a value for each possible state assignment</definiens>
			</definition>
			<definition id="3">
				<sentence>The English data is a collection of Reuters newswire articles annotated with four entity types : person ( PER ) , location ( LOC ) , organization ( ORG ) , and miscellaneous ( MISC ) .</sentence>
				<definiendum id="0">ORG</definiendum>
				<definiens id="0">a collection of Reuters newswire articles annotated with four entity types : person ( PER ) , location ( LOC ) , organization</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>Variation is caused by one of two reasons : i ) ambiguity : there is a type of string with multiple possible labels and different corpus occurrences of that string realize the different options , or ii ) error : the tagging of a string is inconsistent across comparable occurrences .</sentence>
				<definiendum id="0">Variation</definiendum>
				<definiens id="0">caused by one of two reasons : i ) ambiguity : there is a type of string with multiple possible labels and different corpus occurrences of that string realize the different options , or ii ) error : the tagging of a string is inconsistent across comparable occurrences</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , the potential nucleus was werden appears as a verb phrase ( VP ) in the TIGER corpus in the string was ein Seeufer werden ; elsewhere in the corpus was and werden appear in the same sentence with 32 words between them .</sentence>
				<definiendum id="0">VP</definiendum>
				<definiens id="0">a verb phrase</definiens>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>A block is a pair of phrases which are translations of each other .</sentence>
				<definiendum id="0">block</definiendum>
				<definiens id="0">a pair of phrases</definiens>
			</definition>
			<definition id="1">
				<sentence>where a9 a24 is a block and a10 a24a83a82a85a84 a78 a14 efta21a12a19 a81 a14 ighta21a86a19 a74 a14 eutrala21a54a87 is a three-valued orientation component linked to the block a9 a24 ( the orientation a10 a24a31a30 a18 of the predecessor block is currently ignored . )</sentence>
				<definiendum id="0">a9 a24</definiendum>
				<definiens id="0">a block</definiens>
			</definition>
			<definition id="2">
				<sentence>Here , a9 is a block consisting of a source phrase a91 and a target phrase a92 .</sentence>
				<definiendum id="0">a9</definiendum>
			</definition>
			<definition id="3">
				<sentence>a101 is the source phrase length anda102 is the target phrase length .</sentence>
				<definiendum id="0">a101</definiendum>
			</definition>
			<definition id="4">
				<sentence>Using features such as those described above , we can parameterize the probability of such a sequence as a13a15a14 a9a17a16a18 a19 a10 a16a18 a29a177 a19 a94 a21 , wherea177 is a vector of unknown model parameters to be estimated from the training data .</sentence>
				<definiendum id="0">wherea177</definiendum>
				<definiens id="0">a vector of unknown model parameters to be estimated from the training data</definiens>
			</definition>
			<definition id="5">
				<sentence>The log-linear model is defined as : a27 a14 a9a4a19 a10 a82a199a84 a78 a19 a81 a87 a29 a9 a11 a19 a10 a11 a168 a177 a19 a94 a21 ( 5 ) a72 a180a43a182a129a183 a14 a177a157a184 a169a170a14 a9a4a19 a10 a168a77a9a12a11a31a19 a10 a11a86a21a54a21 a185 a14 a9 a11 a19 a10 a11 a168 a94 a21 a19 where a94 is the source sentence , a169a170a14 a9a4a19 a10 a168a77a9a12a11a164a19 a10 a11a86a21 is a locally defined feature vector that depends only on the current and the previous oriented blocks a14 a9a43a19 a10 a21 and a14 a9a12a11a167a19 a10 a11a167a21 .</sentence>
				<definiendum id="0">log-linear model</definiendum>
			</definition>
			<definition id="6">
				<sentence>The partition function is given by a185 a14 a9 a11 a19 a10 a11 a168 a94 a21 a72 a195 a187 a190 a156 a196 a193a100a194a73a195 a187 a145 a190 a156 a145a201a200 a143a167a196 a180a43a182a129a183 a14 a177 a184 a169a170a14 a9a4a19 a10 a168a77a9 a11 a19 a10 a11 a21a54a21 a5 ( 6 ) 559 The set a178a179a14 a9a12a11a167a19 a10 a11a167a168 a94 a21 is a restricted set of possible successor oriented blocks that are consistent with the current block position and the source sentence a94 , to be described in the following paragraph .</sentence>
				<definiendum id="0">partition function</definiendum>
				<definiens id="0">a restricted set of possible successor oriented blocks that are consistent with the current block position and the source sentence a94 , to be described in the following paragraph</definiens>
			</definition>
			<definition id="7">
				<sentence>560 Model The local model described in Section 3 leads to the following abstract maximum entropy training formulation : a210 a177 a72a203a211a213a212a17a214a83a215a93a216 a205 a217 a197 a24a67a25 a18 a204a191a205 a103 a193a100a194a219a218 a180a107a182a20a183 a14 a177 a184 a131 a24 a190a103 a21 a180a107a182a20a183 a14 a177 a184 a131 a24 a190a220 a218 a21 a5 ( 8 ) In this formulation , a177 is the weight vector which we want to compute .</sentence>
				<definiendum id="0">a177</definiendum>
				<definiens id="0">local model described in Section 3 leads to the following abstract maximum entropy training formulation : a210 a177 a72a203a211a213a212a17a214a83a215a93a216 a205 a217 a197 a24a67a25 a18 a204a191a205 a103 a193a100a194a219a218 a180a107a182a20a183 a14 a177 a184 a131 a24 a190a103 a21 a180a107a182a20a183 a14 a177 a184 a131 a24 a190a220 a218 a21 a5</definiens>
				<definiens id="1">the weight vector which we want to compute</definiens>
			</definition>
			<definition id="8">
				<sentence>The set a178 a24 consists of candidate labels for the a106 -th training instance , with the true label a132 a24 a82 a178 a24 .</sentence>
				<definiendum id="0">set a178 a24</definiendum>
			</definition>
			<definition id="9">
				<sentence>Description ( a ) Unigram probability ( b ) Orientation probability ( c ) LM first word probability ( d ) LM second and following words probability ( e ) Lexical weighting ( f ) Binary Block Bigram Features Table 2 : Cased BLEU translation results with confidence intervals on the MT03 test data .</sentence>
				<definiendum id="0">Description</definiendum>
				<definiendum id="1">Orientation probability</definiendum>
			</definition>
			<definition id="10">
				<sentence>During decoding , we generate a ’translation graph’ for every input sentence using a procedure similar to ( Ueffing et al. , 2002 ) : a translation graph is a compact way of representing candidate translations which are close in terms of likelihood .</sentence>
				<definiendum id="0">translation graph</definiendum>
				<definiens id="0">a compact way of representing candidate translations which are close in terms of likelihood</definiens>
			</definition>
</paper>

		<paper id="3009">
			<definition id="0">
				<sentence>The Linguist’s Search Engine ( LSE ) was designed to provide the broadest possible range of users with an intuitive , linguistically sophisticated but user-friendly way to search the Web for naturally occurring data .</sentence>
				<definiendum id="0">Linguist’s Search Engine ( LSE</definiendum>
				<definiens id="0">designed to provide the broadest possible range of users with an intuitive , linguistically sophisticated but user-friendly way to search the Web for naturally occurring data</definiens>
			</definition>
			<definition id="1">
				<sentence>The LSE adopts a strategy one can call “query by example , ” in order to provide sophisticated search functionality without requiring the user to learn a complex query language .</sentence>
				<definiendum id="0">LSE</definiendum>
				<definiens id="0">adopts a strategy one can call “query by example , ” in order to provide sophisticated search functionality without requiring the user to learn a complex query language</definiens>
			</definition>
			<definition id="2">
				<sentence>ViST : a dynamic index method for querying XML data by tree structures .</sentence>
				<definiendum id="0">ViST</definiendum>
				<definiens id="0">a dynamic index method for querying XML data by tree structures</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>In the K &amp; M model , the sentence probability is determined by combining a probabilistic context free grammar ( PCFG ) with a word-bigram score .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">determined by combining a probabilistic context free grammar (</definiens>
			</definition>
			<definition id="1">
				<sentence>We slightly modify the channel model equation to be P ( l | s ) = Pexpand ( l | s ) Pdeleted , where Pdeleted is the probability of adding the deleted subtrees back into s to get l. We determine this probability also using the Charniak language model .</sentence>
				<definiendum id="0">Pdeleted</definiendum>
				<definiens id="0">the probability of adding the deleted subtrees back into s to get l. We determine this probability also using the Charniak language model</definiens>
			</definition>
</paper>

		<paper id="1077">
			<definition id="0">
				<sentence>Cosine similarity is defined as the cosine of the angle between them : cos ( θ ( u , v ) ) .</sentence>
				<definiendum id="0">Cosine similarity</definiendum>
				<definiens id="0">the cosine of the angle between them : cos ( θ ( u , v ) )</definiens>
			</definition>
			<definition id="1">
				<sentence>|u.v| is the scalar ( dot ) product of u and v , and |u| and |v| represent the length of vectors u and v respectively .</sentence>
				<definiendum id="0">|u.v|</definiendum>
				<definiens id="0">the scalar ( dot ) product of u and v , and |u| and |v| represent the length of vectors u and v respectively</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus , B is the beam parameter of the search .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">the beam parameter of the search</definiens>
			</definition>
			<definition id="3">
				<sentence>Newspaper Corpus ( 6 GB newspaper corpus ) We set up a spider to download roughly 70 million web pages from the Internet .</sentence>
				<definiendum id="0">Newspaper Corpus</definiendum>
				<definiens id="0">GB newspaper corpus ) We set up a spider to download roughly 70 million web pages from the Internet</definiens>
			</definition>
			<definition id="4">
				<sentence>We first construct a frequency count vector C ( e ) = ( ce1 , ce2 , ... , cek ) , where k is the total number of features and cef is the frequency count of feature f occurring in word e. Here , cef is the number of times word e occurred in context f. We then construct a mutual information vector MI ( e ) = ( mie1 , mie2 , ... , miek ) for each word e , where mief is the pointwise mutual information between word e and feature f , which is defined as : mief = log cef Nsummationtext n i=1 cif N × summationtextk j=1 cej N ( 6 ) where n is the number of words and N = 5We perform this operation so that we can compare the performance of our system to that of Pantel and Lin ( 2002 ) .</sentence>
				<definiendum id="0">frequency count vector C</definiendum>
				<definiendum id="1">k</definiendum>
				<definiendum id="2">cef</definiendum>
				<definiendum id="3">cef</definiendum>
				<definiendum id="4">mief</definiendum>
				<definiendum id="5">n</definiendum>
				<definiens id="0">( e ) = ( ce1 , ce2 , ... , cek )</definiens>
				<definiens id="1">the total number of features</definiens>
				<definiens id="2">the number of times word e occurred in context f. We then construct a mutual information vector MI ( e ) = ( mie1 , mie2 , ... , miek ) for each word e</definiens>
				<definiens id="3">the pointwise mutual information between word e and feature f</definiens>
				<definiens id="4">the number of words</definiens>
			</definition>
</paper>

		<paper id="1073">
			<definition id="0">
				<sentence>In identification , our task is to classify nodes of t as either ARG , an argument ( including modifiers ) , or NONE , a non-argument .</sentence>
				<definiendum id="0">NONE</definiendum>
				<definiens id="0">an argument ( including modifiers ) , or</definiens>
			</definition>
			<definition id="1">
				<sentence>VOICE : Active or passive relative to predicate HEAD WORD OF PHRASE SUB-CAT : CFG expansion of predicate’s parent Additional Features ( Pradhan et al. , 2004 ) FIRST/LAST WORD LEFT/RIGHT SISTER PHRASE-TYPE LEFT/RIGHT SISTER HEAD WORD/POS PARENT PHRASE-TYPE PARENT POS/HEAD-WORD ORDINAL TREE DISTANCE : Phrase Type with appended length of PATH feature NODE-LCA PARTIAL PATH Path from constituent to Lowest Common Ancestor with predicate node PP PARENT HEAD WORD If parent is a PP return parent’s head word PP NP HEAD WORD/POS For a PP , retrieve the head Word / POS of its rightmost NP Selected Pairs ( Xue and Palmer , 2004 ) PREDICATE LEMMA &amp; PATH PREDICATE LEMMA &amp; HEAD WORD PREDICATE LEMMA &amp; PHRASE TYPE VOICE &amp; POSITION PREDICATE LEMMA &amp; PP PARENT HEAD WORD Table 1 : Baseline Features and classification models can be chained in a principled way , as in Equation 1 .</sentence>
				<definiendum id="0">VOICE</definiendum>
				<definiens id="0">Active or passive relative to predicate HEAD WORD OF PHRASE SUB-CAT : CFG expansion of predicate’s</definiens>
				<definiens id="1">Xue and Palmer , 2004 ) PREDICATE LEMMA &amp; PATH PREDICATE LEMMA &amp; HEAD WORD PREDICATE LEMMA &amp; PHRASE TYPE VOICE &amp; POSITION PREDICATE LEMMA &amp; PP PARENT HEAD WORD Table 1</definiens>
			</definition>
			<definition id="2">
				<sentence>Templates For a tree t , predicate v , and joint assignment L of labels to the nodes of the tree , we define the candidate argument sequence as the sequence of nonNONE labeled nodes [ n1 , l1 , ... , vPRED , nm , lm ] ( li is the label of node ni ) .</sentence>
				<definiendum id="0">li</definiendum>
				<definiens id="0">the sequence of nonNONE labeled nodes [ n1 , l1 , ... , vPRED , nm</definiens>
				<definiens id="1">the label of node ni )</definiens>
			</definition>
			<definition id="3">
				<sentence>The proposition bank : An annotated corpus of semantic roles .</sentence>
				<definiendum id="0">proposition bank</definiendum>
				<definiens id="0">An annotated corpus of semantic roles</definiens>
			</definition>
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>Arabic is a morphologically complex language.1 The morphological analysis of a word consists of determining the values of a large number of ( orthogonal ) features , such as basic part-of-speech ( i.e. , noun , verb , and so on ) , voice , gender , number , information about the clitics , and so on.2 For Arabic , this gives us about 333,000 theoretically possible completely specified morphological analyses , i.e. , morphological tags , of which about 2,200 are actually used in the first 280,000 words of the Penn Arabic Treebank ( ATB ) .</sentence>
				<definiendum id="0">Arabic</definiendum>
				<definiens id="0">basic part-of-speech ( i.e. , noun , verb , and so on ) , voice , gender , number , information about the clitics , and so on.2 For Arabic , this gives us about 333,000 theoretically possible completely specified morphological analyses , i.e. , morphological tags , of which about</definiens>
			</definition>
			<definition id="1">
				<sentence>Third , we use the SVM-based Yamcha ( which uses Viterbi decoding ) rather than an exponential model ; however , we do not consider this difference crucial and do not contrast our learner with others in this paper .</sentence>
				<definiendum id="0">SVM-based Yamcha</definiendum>
				<definiens id="0">uses Viterbi decoding ) rather than an exponential model</definiens>
			</definition>
			<definition id="2">
				<sentence>ALMORGEANA uses the databases ( i.e. , lexicon ) from the Buckwalter Arabic Morphological Analyzer , but ( in analysis mode ) produces an output in the lexeme-and-feature format ( which we need for our approach ) rather than the stem-and-affix format of the Buckwalter analyzer .</sentence>
				<definiendum id="0">ALMORGEANA</definiendum>
				<definiendum id="1">lexeme-and-feature format</definiendum>
				<definiens id="0">uses the databases ( i.e. , lexicon ) from the Buckwalter Arabic Morphological Analyzer , but ( in analysis mode ) produces an output in the</definiens>
			</definition>
			<definition id="3">
				<sentence>Our training data consists of a set of all possible morphological analyses for each word , with the unique correct analysis marked .</sentence>
				<definiendum id="0">data</definiendum>
				<definiens id="0">consists of a set of all possible morphological analyses for each word , with the unique correct analysis marked</definiens>
			</definition>
			<definition id="4">
				<sentence>The baseline BL is the most common value associated in the training corpus TR1 with every feature for a given word form ( unigram ) .</sentence>
				<definiendum id="0">baseline BL</definiendum>
				<definiens id="0">the most common value associated in the training corpus TR1 with every feature for a given word form ( unigram )</definiens>
			</definition>
			<definition id="5">
				<sentence>Recall that the Yamcha classifiers are trained on TR1 ; in addition , Rip is trained on the output of these Yamcha clasCorpus TE1 TE2 Method All Words All Words BL 92.1 90.2 87.3 85.3 Maj 96.6 95.8 94.1 93.2 Con 89.9 87.6 88.9 87.2 Add 91.6 89.7 90.7 89.2 Mul 96.5 95.6 94.3 93.4 Rip 96.2 95.3 94.8 94.0 Figure 4 : Results ( percent accuracy ) on choosing the correct analysis , measured per token ( including and excluding punctuation and numbers ) ; BL is the baseline sifiers on TR2 .</sentence>
				<definiendum id="0">BL</definiendum>
				<definiens id="0">Results ( percent accuracy ) on choosing the correct analysis , measured per token ( including and excluding punctuation and numbers</definiens>
				<definiens id="1">the baseline sifiers on TR2</definiens>
			</definition>
			<definition id="6">
				<sentence>The ATB starts with a simple tokenization , and then splits the word into four fields : conjunctions ; particles ( prepositions in the case of nouns ) ; the word stem ; and pronouns ( object clitics in the case of verbs , possessive clitics in the case of nouns ) .</sentence>
				<definiendum id="0">ATB</definiendum>
				<definiens id="0">starts with a simple tokenization , and then splits the word into four fields : conjunctions ; particles ( prepositions in the case of nouns ) ; the word stem ; and pronouns ( object clitics in the case of verbs</definiens>
			</definition>
			<definition id="7">
				<sentence>579 Corpus TE1 TE2 Method Tags All Words All Words BL PTB 93.9 93.3 90.9 89.8 Smp 94.9 94.3 92.6 91.4 Maj PTB 97.6 97.5 95.7 95.2 Smp 98.1 97.8 96.5 96.0 Figure 6 : Part-of-speech tagging accuracy measured for all tokens ( based on gold-standard tokenization ) and only for word tokens , using the Penn Treebank ( PTB ) tagset as well as the smaller tagset ( Smp ) ( see Footnote 9 ) ; BL is the baseline obtained by using the POS value from the baseline tag used in Section 6 similarly .</sentence>
				<definiendum id="0">BL</definiendum>
				<definiens id="0">Part-of-speech tagging accuracy measured for all tokens</definiens>
			</definition>
</paper>

		<paper id="3029">
			<definition id="0">
				<sentence>A punning riddle is a question-answer riddle that uses phonological ambiguity .</sentence>
				<definiendum id="0">punning riddle</definiendum>
				<definiens id="0">a question-answer riddle that uses phonological ambiguity</definiens>
			</definition>
</paper>

		<paper id="3018">
			<definition id="0">
				<sentence>Our annotation environment consists of two pieces of software : a user interface for the annotators and a visualization tool for the researchers to examine the data .</sentence>
				<definiendum id="0">annotation environment</definiendum>
			</definition>
			<definition id="1">
				<sentence>The data-collection interface asks the users to make lexical and phrasal mappings ( word alignments ) between the two languages .</sentence>
				<definiendum id="0">data-collection interface</definiendum>
				<definiens id="0">asks the users to make lexical and phrasal mappings ( word alignments</definiens>
			</definition>
			<definition id="2">
				<sentence>We use a variant of the 2Rada Mihalcea maintains an alignment resource repository ( http : //www.cs.unt.edu/~rada/wa ) that contains other downloadable interface packages that do not have companion papers .</sentence>
				<definiendum id="0">Mihalcea</definiendum>
				<definiens id="0">maintains an alignment resource repository ( http : //www.cs.unt.edu/~rada/wa ) that contains other downloadable interface packages that do not have companion papers</definiens>
			</definition>
</paper>

		<paper id="3010">
			<definition id="0">
				<sentence>CAT tools such as Trados typically require up-front investment to populate multilingual terminology and translation memory .</sentence>
				<definiendum id="0">CAT</definiendum>
				<definiens id="0">tools such as Trados typically require up-front investment to populate multilingual terminology and translation memory</definiens>
			</definition>
			<definition id="1">
				<sentence>A TU for an English name can be a syllable or consonants which corresponds to a character in the target transliteration .</sentence>
				<definiendum id="0">TU</definiendum>
				<definiens id="0">a syllable or consonants which corresponds to a character in the target transliteration</definiens>
			</definition>
			<definition id="2">
				<sentence>Transliteration probability trained on 1,800 bilingual names ( λ denotes an empty string ) .</sentence>
				<definiendum id="0">Transliteration probability</definiendum>
			</definition>
			<definition id="3">
				<sentence>τ ω P ( τ|ω ) τ ω P ( τ|ω ) τ ω P ( τ|ω ) .458 b  : .700 ye � .667 � .271 λ .133 葉 .333 .059  , .033 z � .476 a λ .051 a .033 λ .286 � .923 an � .923  { .095 an � .077 � .077 z .048 At runtime , TermMine follows the following steps to translate a given term E : ( 1 ) Webpage retrieval .</sentence>
				<definiendum id="0">TermMine</definiendum>
			</definition>
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>Our demonstration system plays a referential communication game , much like the one that pairs of human subjects play in the experiments of Clark and Wilkes-Gibbs ( 1986 ) .</sentence>
				<definiendum id="0">demonstration system</definiendum>
				<definiens id="0">plays a referential communication game , much like the one that pairs of human subjects play in the experiments of Clark</definiens>
			</definition>
			<definition id="1">
				<sentence>( 4 ) Update advances the IS symmetrically in response to intentions signaled by the system or recognized from the user ; the symmetric architecture frees the designer from programming complementary updates in a symmetrical way .</sentence>
				<definiendum id="0">Update</definiendum>
				<definiens id="0">advances the IS symmetrically in response to intentions signaled by the system or recognized from the user</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>They show that standard lexicalized models fail to outperform an unlexicalized baseline ( a vanilla PCFG ) on Negra , a German treebank ( Skut et al. , 1997 ) .</sentence>
				<definiendum id="0">unlexicalized baseline</definiendum>
			</definition>
			<definition id="1">
				<sentence>The French Treebank ( FTB ; Abeill´e et al. 2000 ) consists of 20,648 sentences extracted from the daily newspaper Le Monde , covering a variety of authors and domains ( economy , literature , politics , etc. ) .1 The corpus is formatted in XML and has a rich morphosyntactic tagset that includes part-of-speech tag , ‘subcategorization’ ( e.g. , possessive or cardinal ) , inflection ( e.g. , masculine singular ) , and lemma information .</sentence>
				<definiendum id="0">Treebank</definiendum>
				<definiens id="0">consists of 20,648 sentences extracted from the daily newspaper Le Monde , covering a variety of authors and domains ( economy , literature , politics , etc. ) .1 The corpus is formatted in XML and has a rich morphosyntactic tagset that includes part-of-speech tag , ‘subcategorization’ ( e.g. , possessive or cardinal ) , inflection ( e.g. , masculine singular ) , and lemma information</definiens>
			</definition>
			<definition id="2">
				<sentence>( 1 ) ( VN ( V sont ) ( ADV syst´ematiquement ) ( V arrˆet´es ) ) ‘are systematically arrested’ The noun phrases ( NPs ) in the FTB are also flat ; a noun is grouped together with any associated determiners and prenominal adjectives , as in example ( 2 ) .</sentence>
				<definiendum id="0">VN</definiendum>
				<definiens id="0">V sont ) ( ADV syst´ematiquement ) ( V arrˆet´es</definiens>
			</definition>
			<definition id="3">
				<sentence>We reassigned the POS for punctuation using the PTB tagset , which differentiates between commas , periods , brackets , etc .</sentence>
				<definiendum id="0">PTB tagset</definiendum>
			</definition>
			<definition id="4">
				<sentence>XP X COORD C XP X XP X C XP X XP X C X Figure 3 : Coordination in the FTB : before ( left ) and after transformation ( middle ) ; coordination in the PTB ( right ) As mentioned previously , coordinate structures have their own constituent label COORD in the FTB annotation .</sentence>
				<definiendum id="0">XP X COORD C XP X XP X C XP X XP X</definiendum>
			</definition>
			<definition id="5">
				<sentence>Each rule now has the form : P ( h ) → Ln ( ln ) . . .L1 ( l1 ) H ( h ) R1 ( r1 ) . . .Rm ( rm ) ( 1 ) Here , H is the head-daughter of the phrase , which inherits the head-word h from its parent P. L1 . . .Ln and R1 . . .Rn are the left and right sisters of H. Either n or m may be zero , and n = m for unary rules .</sentence>
				<definiendum id="0">H</definiendum>
				<definiens id="0">the head-daughter of the phrase</definiens>
			</definition>
			<definition id="6">
				<sentence>The probability of a rule is thus defined as : P ( RHS|LHS ) = P ( Ln ( ln ) . . .L1 ( l1 ) H ( h ) , R1 ( r1 ) . . .Rm ( rm ) |P ( h ) ) = Ph ( H|P , h ) ×∏m+1i=1 Pr ( Ri ( ri ) |P , h , H , d ( i ) ) ×∏n+1i=1 Pl ( Li ( li ) |P , h , H , d ( i ) ) ( 2 ) Here , Ph is the probability of generating the head , Pl and Pr are the probabilities of generating the left and right sister respectively .</sentence>
				<definiendum id="0">probability of a rule</definiendum>
				<definiendum id="1">Ph</definiendum>
				<definiens id="0">P ( RHS|LHS ) = P ( Ln ( ln ) . . .L1 ( l1 ) H ( h ) , R1 ( r1 ) . . .Rm ( rm ) |P ( h ) ) = Ph ( H|P , h ) ×∏m+1i=1 Pr ( Ri ( ri ) |P , h , H , d ( i ) ) ×∏n+1i=1 Pl ( Li ( li ) |P , h</definiens>
				<definiens id="1">the probability of generating the head , Pl and Pr are the probabilities of generating the left and right sister respectively</definiens>
			</definition>
			<definition id="7">
				<sentence>Label FTB PTB Negra Label FTB PTB Negra SENT 5.84 2.22 4.55 VPpart 2.51 – – Ssub 4.41 – – VN 1.76 – – Sint 3.44 – – PP 2.10 2.03 3.08 Srel 3.92 – – NP 2.45 2.20 3.08 VP – 2.32 2.59 AdvP 2.24 – 2.08 VPinf 3.07 – – AP 1.34 – 2.22 Table 2 : Average number of daughter nodes per constituents in three treebanks Flatness As already pointed out in Section 2.1 , the FTB uses a flat annotation scheme .</sentence>
				<definiendum id="0">Label FTB PTB Negra Label FTB PTB Negra</definiendum>
				<definiendum id="1">FTB</definiendum>
				<definiens id="0">uses a flat annotation scheme</definiens>
			</definition>
			<definition id="8">
				<sentence>The BigramFlat model , which applies the bigram model to only those labels which have a high degree of flatness , performs Model LR LP CBs 0CB 2CB Tag Cov Exp+CR 65.50 64.76 1.49 42.36 77.48 100.0 97.83 Cont+CR 69.35 67.93 1.34 47.43 80.25 100.0 96.97 Model1 81.51 81.43 0.78 64.60 89.25 98.54 99.78 Model2 81.69 81.59 0.78 63.84 89.69 98.55 99.78 SisterHead 81.08 81.56 0.79 64.35 89.57 98.51 99.57 Bigram 81.78 81.91 0.78 64.96 89.12 98.81 99.67 BigramFlat 81.14 81.19 0.81 63.37 88.80 98.80 99.67 Table 4 : Results for lexicalized and unlexicalized models ( sentences ≤40 words ) with correct POS tags supplied ; all lexicalized models used the Cont+CR data set at roughly the same level as Model 1 .</sentence>
				<definiendum id="0">BigramFlat model</definiendum>
				<definiendum id="1">same level</definiendum>
				<definiens id="0">Results for lexicalized and unlexicalized models ( sentences ≤40 words ) with correct POS tags supplied ; all lexicalized models used the Cont+CR data set at roughly the</definiens>
			</definition>
			<definition id="9">
				<sentence>Dependency accuracy is defined as the ratio of correct dependencies over the total number of dependencies in a sentence .</sentence>
				<definiendum id="0">Dependency accuracy</definiendum>
				<definiens id="0">the ratio of correct dependencies over the total number of dependencies in a sentence</definiens>
			</definition>
			<definition id="10">
				<sentence>The group who built the French Treebank ( Abeill´e et al. , 2000 ) used a rule-based chunker to automatically annotate the corpus with syntactic structures , which were then manually corrected .</sentence>
				<definiendum id="0">Treebank</definiendum>
				<definiens id="0">Abeill´e et al. , 2000 ) used a rule-based chunker to automatically annotate the corpus with syntactic structures , which were then manually corrected</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>The dictionary is the Lexeed Semantic Database of Japanese ( Kasahara et al. , 2004 ) , which consists of all words with a familiarity greater than or equal to ve on a scale of one to seven .</sentence>
				<definiendum id="0">dictionary</definiendum>
			</definition>
			<definition id="1">
				<sentence>For example , the Kyoto Corpus ( Kurohashi and Nagao , 2003 ) has part of speech information and dependency information , but not the detailed information available from an HPSG analysis .</sentence>
				<definiendum id="0">Kyoto Corpus</definiendum>
				<definiens id="0">has part of speech information and dependency information , but not the detailed information available from an HPSG analysis</definiens>
			</definition>
			<definition id="2">
				<sentence>Reject Agreement shows the proportion of sentences for which both annotators found no suitable analysis .</sentence>
				<definiendum id="0">Reject Agreement</definiendum>
				<definiens id="0">shows the proportion of sentences for which both annotators found no suitable analysis</definiens>
			</definition>
			<definition id="3">
				<sentence>When broken down by pairs of annotators and sets of 1,000 items each , which have been annotated in strict sequential order , F scores in Table 2 con rm that : ( a ) inter-annotator agreement is stable , all three annotators appear to have performed equally ( well ) ; ( b ) with growing experience , there is a slight increase in F scores over time , particularly when taking into account that set E exhibits a noticeably higher average ambiguity rate ( 1208 parses per item ) than set D ( 820 average parses ) ; and ( c ) Hinoki inter-annotator agreement compares favorably to results reported for the German NeGra ( Brants , 2000 ) and Spanish Cast3LB ( Civit et al. , 2003 ) treebanks , both of which used manual mark-up seeded from automated POS tagging and chunking .</sentence>
				<definiendum id="0">Spanish Cast3LB</definiendum>
				<definiens id="0">a slight increase in F scores over time , particularly when taking into account that set E exhibits a noticeably higher average ambiguity rate</definiens>
			</definition>
</paper>

		<paper id="3025">
			<definition id="0">
				<sentence>↔ x0 x1 x2 takes three parts of a sentence , a noun phrase ( x0 ) , a verb phrase ( x1 ) , and a period ( x2 ) and ties them together to build a complete sentence .</sentence>
				<definiendum id="0">x0</definiendum>
				<definiens id="0">a verb phrase ( x1 ) , and a period ( x2 ) and ties them together to build a complete sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>Multilevel rules can tie several of these concepts together ; the rule VP ( VBD ( was ) VP-C ( x0 : VBN PP ( IN ( by ) x1 : NP-C ) ) ) ↔ � x1 x0 takes a Chinese word � and two English constituents — x1 , a noun phrase , and x0 , a pastparticiple verb — and translates them into a phrase of the form “was [ verb ] by [ noun-phrase ] ” .</sentence>
				<definiendum id="0">VBD</definiendum>
				<definiens id="0">NP-C ) ) ) ↔ � x1 x0 takes a Chinese word � and two English constituents — x1 , a noun phrase</definiens>
				<definiens id="1">a pastparticiple verb — and translates them into a phrase of the form “was [ verb ] by [ noun-phrase ] ”</definiens>
			</definition>
</paper>

		<paper id="3011">
			<definition id="0">
				<sentence>The paper presents the Position Specific Posterior Lattice ( PSPL ) , a novel lossy representation of automatic speech recognition lattices that naturally lends itself to efficient indexing and subsequent relevance ranking of spoken documents .</sentence>
				<definiendum id="0">Position Specific Posterior Lattice ( PSPL</definiendum>
				<definiens id="0">a novel lossy representation of automatic speech recognition lattices that naturally lends itself to efficient indexing and subsequent relevance ranking of spoken documents</definiens>
			</definition>
			<definition id="1">
				<sentence>The final collection consists of 169 documents , 66,102 segments and an average document length of 391 segments .</sentence>
				<definiendum id="0">final collection</definiendum>
				<definiens id="0">consists of 169 documents , 66,102 segments and an average document length of 391 segments</definiens>
			</definition>
</paper>

		<paper id="2019">
			<definition id="0">
				<sentence>Danish is a verb-second language .</sentence>
				<definiendum id="0">Danish</definiendum>
				<definiens id="0">a verb-second language</definiens>
			</definition>
			<definition id="1">
				<sentence>weighed A cleft construction is defined as a complex construction consisting of a copula matrix clause with a relative clause headed by the object of the matrix clause .</sentence>
				<definiendum id="0">weighed A cleft construction</definiendum>
				<definiens id="0">a complex construction consisting of a copula matrix clause with a relative clause headed by the object of the matrix clause</definiens>
			</definition>
			<definition id="2">
				<sentence>The clefted element det ‘that’ , which we annotate as +cle , leaves an ‘empty slot’ , e , in the relative clause , as shown in example ( 10 ) : ( 10 ) det it er is jo after all ikke not [ +cle deti ] [ +cle thati ] du you skal shall tabe dig lose weight af from ei ei som as s˚adan such Danish sentence intertwining can be defined as a special case of extraction where a non-WH constituent of a subordinate clause occurs in the first 111 position of the matrix clause .</sentence>
				<definiendum id="0">clefted element det ‘that’</definiendum>
				<definiens id="0">+cle , leaves an ‘empty slot’ , e , in the relative clause , as shown in example ( 10 ) : ( 10 ) det it er is jo after all ikke not [ +cle deti ] [ +cle thati ] du you skal shall tabe dig lose weight af from ei ei som as s˚adan such Danish sentence intertwining can be defined as a special case of extraction where a non-WH constituent of a subordinate clause occurs in the first 111 position of the matrix clause</definiens>
			</definition>
			<definition id="3">
				<sentence>An epistemic matrix clause is defined as a matrix clause whose function it is to evaluate the truth of its subordinate clause ( such as “I think . . . ” ) .</sentence>
				<definiendum id="0">epistemic matrix clause</definiendum>
				<definiens id="0">a matrix clause whose function it is to evaluate the truth of its subordinate clause ( such as “I think . . . ” )</definiens>
			</definition>
			<definition id="4">
				<sentence>Finally , the Intersection DT contains the features pro , def , sub , and pre .</sentence>
				<definiendum id="0">Intersection DT</definiendum>
				<definiens id="0">contains the features pro , def , sub , and pre</definiens>
			</definition>
			<definition id="5">
				<sentence>Human Total NPs 449 449 386 449 Success rate 84 % 85 % 89 % 89 % Precision 0.77 0.74 0.79 0.79 Recall 0.53 0.61 0.67 0.77 F1-score 0.63 0.67 0.72 0.78 Table 2 : Success rates , Precision , Recall , and F1-scores for the three different data sets .</sentence>
				<definiendum id="0">Success</definiendum>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>The probability , D4B4D8CYD7B5 , of producing the parse result D8 from a given sentence D7 is defined as D4B4D8CYD7B5BP BD CI D7 D4 BC B4D8CYD7B5CTDCD4B4 CG CX CU CX B4D8BND7B5AL CX B4D8BND7B5B5 CI D7 BP CG D8 BC BECCB4D7B5 D4 BC B4D8 BC CYD7B5CTDCD4B4 CG CX CU CX B4D8 BC BND7B5AL CX B4D8 BC BND7B5B5BN where D4 BC B4D8CYD7B5 is a reference distribution ( usually assumed to be a uniform distribution ) , and CCB4D7B5 is a set of parse candidates assigned to D7 .</sentence>
				<definiendum id="0">CCB4D7B5</definiendum>
				<definiens id="0">of producing the parse result D8 from a given sentence D7 is defined as D4B4D8CYD7B5BP BD CI D7 D4 BC B4D8CYD7B5CTDCD4B4 CG CX CU CX B4D8BND7B5AL CX B4D8BND7B5B5 CI D7 BP CG D8 BC BECCB4D7B5 D4 BC B4D8 BC CYD7B5CTDCD4B4 CG CX CU CX B4D8 BC BND7B5AL CX B4D8 BC BND7B5B5BN where D4 BC B4D8CYD7B5 is a reference distribution ( usually assumed to be a uniform distribution ) , and</definiens>
				<definiens id="1">a set of parse candidates assigned to D7</definiens>
			</definition>
			<definition id="1">
				<sentence>The feature function CU CX B4D8BND7B5 represents the characteristics of D8 and D7 , while the corresponding model parameter AL CX B4D8BND7B5 is its weight .</sentence>
				<definiendum id="0">feature function CU CX B4D8BND7B5</definiendum>
				<definiens id="0">represents the characteristics of D8 and D7</definiens>
			</definition>
			<definition id="2">
				<sentence>Formally , a set of HPSG parse trees is represented in a chart as a tuple CWBXBNBX D6 BNABCX , where BX is a set of equivalence classes , BX D6 AI BX is a set of root nodes , and AB BM BX AX BE BXA2BX is a function to represent immediate-dominance relations .</sentence>
				<definiendum id="0">BX</definiendum>
				<definiendum id="1">BX D6 AI BX</definiendum>
				<definiens id="0">a set of equivalence classes</definiens>
				<definiens id="1">a set of root nodes</definiens>
			</definition>
			<definition id="3">
				<sentence>A feature forest is formally defined as a tuple , CWBVBNBWBNCABNADBNÆCX , where BV is a set of conjunctive nodes , BW is a set of disjunctive nodes , CA AI BV is a set of root nodes 2 , AD BM BW AX BE BV is a conjunctive daughter function , and Æ BM BV AX BE BW is a disjunctive 2 For the ease of explanation , the definition of root node is slightly different from the original .</sentence>
				<definiendum id="0">feature forest</definiendum>
				<definiendum id="1">BV</definiendum>
				<definiendum id="2">BW</definiendum>
				<definiendum id="3">CA AI BV</definiendum>
				<definiendum id="4">Æ BM BV AX BE BW</definiendum>
				<definiens id="0">a set of conjunctive nodes</definiens>
				<definiens id="1">a set of disjunctive nodes</definiens>
				<definiens id="2">a set of root nodes 2 , AD BM BW AX BE BV is a conjunctive daughter function , and</definiens>
				<definiens id="3">a disjunctive 2 For the ease of explanation , the definition of root node is slightly different from the original</definiens>
			</definition>
			<definition id="4">
				<sentence>The preliminary model in this study is a unigram model , AMD4B4D8CYD7B5 BP C9 DBBED7 D4B4D0CYDBB5BN where DB BE D7 is a word in the sentence D7 , and D0 is a lexical entry assigned to DB .</sentence>
				<definiendum id="0">DB BE D7</definiendum>
				<definiendum id="1">D0</definiendum>
				<definiens id="0">a word in the sentence D7 , and</definiens>
			</definition>
			<definition id="5">
				<sentence>86 RULE DIST COMMA SPAN SYM WORD POS LE D4 D4 D4 –– D4 D4 D4 D4 D4 D4 D4 D4 – D4 D4 D4 –– D4 – D4 D4 D4 D4 – D4 D4 –– D4 – D4 D4 – D4 D4 D4 D4 – D4 D4 – D4 D4 – D4 – D4 D4 – D4 – D4 D4 – D4 D4 D4 D4 –– D4 D4 D4 –– – D4 D4 D4 D4 D4 D4 – D4 D4 D4 –– –– D4 D4 D4 D4 – D4 ––– D4 – D4 D4 –– D4 D4 D4 – D4 D4 D4 – D4 – D4 D4 ––– D4 D4 – D4 D4 D4 ––– RULE SYM WORD POS LE D4 – D4 D4 D4 D4 – D4 D4 – D4 – D4 – D4 D4 D4 D4 –– D4 –– D4 D4 D4 D4 – D4 ––– D4 D4 D4 ––– SYM WORD POS LE – D4 D4 D4 – D4 D4 – – D4 – D4 D4 D4 –– –– D4 D4 D4 – ––– D4 D4 ––– Table 2 : Feature templates for binary schema ( left ) , unary schema ( center ) , and root condition ( right ) Avg .</sentence>
				<definiendum id="0">RULE DIST COMMA SPAN SYM WORD POS LE</definiendum>
			</definition>
			<definition id="6">
				<sentence>A dependency is defined as a tuple CWAPBNDB CW BNCPBNDB CP CX , where AP is the predicate type ( e.g. , adjective , intransitive verb ) , DB CW is the head word of the predicate , CP is the argument label ( MODARG , ARG1 , ... , ARG4 ) , and DB CP is the head word of the argument .</sentence>
				<definiendum id="0">dependency</definiendum>
				<definiendum id="1">AP</definiendum>
				<definiendum id="2">DB CW</definiendum>
				<definiendum id="3">CP</definiendum>
				<definiendum id="4">ARG4</definiendum>
				<definiendum id="5">DB CP</definiendum>
				<definiens id="0">a tuple CWAPBNDB CW BNCPBNDB CP CX , where</definiens>
				<definiens id="1">the predicate type ( e.g. , adjective , intransitive verb )</definiens>
				<definiens id="2">the head word of the predicate</definiens>
				<definiens id="3">the argument label ( MODARG , ARG1 , ... ,</definiens>
			</definition>
			<definition id="7">
				<sentence>Labeled precision/recall ( LP/LR ) is the ratio of tuples correctly identified by the parser , while unlabeled precision/recall ( UP/UR ) is the ratio of DB CW and DB CP correctly identified regardless of AP and CP .</sentence>
				<definiendum id="0">Labeled precision/recall ( LP/LR )</definiendum>
				<definiens id="0">the ratio of tuples correctly identified by the parser</definiens>
			</definition>
			<definition id="8">
				<sentence>The F-score is the harmonic mean of LP and LR .</sentence>
				<definiendum id="0">F-score</definiendum>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>The concatenation ( a2 ) operator takes two arguments , and uses the strings encoded by its argument expressions to obtain concatenated strings that respect the order of the arguments ; e.g. , a5 a2a7a6 encodes the singleton set a8a9a5a10a6a12a11 .</sentence>
				<definiendum id="0">operator</definiendum>
				<definiens id="0">takes two arguments , and uses the strings encoded by its argument expressions to obtain concatenated strings that respect the order of the arguments</definiens>
			</definition>
			<definition id="1">
				<sentence>The a29 isjunction ( a4 ) operator allows a choice among the strings encoded by its argument expressions ; e.g. , a4a30a14a16a5a7a18a4a6a31a22 encodes the set a8a9a5a7a18a4a6a12a11 .</sentence>
				<definiendum id="0">a4a30a14a16a5a7a18a4a6a31a22</definiendum>
				<definiens id="0">encodes the set a8a9a5a7a18a4a6a12a11</definiens>
			</definition>
			<definition id="2">
				<sentence>There are three factors that linearly influence the run-time complexity of our algorithms : a0 is the maximum number of nodes in a18 a14a20a19a24a22 needed to represent a state in a1 a14a20a19a24a22 –a0 depends solely on a19 ; a1 is the maximum number of nodes in a18 a14a20a19a24a22 needed to represent a state in a18 a14a20a19a24a22 –a1 depends on a19 and a12 , the length of the context used by the a12 -gram language model ; anda2 is the number of states of a18 a14a20a19a24a22 –a2 also depends on a19 and a12 .</sentence>
				<definiendum id="0">a1</definiendum>
				<definiendum id="1">anda2</definiendum>
				<definiens id="0">the maximum number of nodes in a18 a14a20a19a24a22 needed to represent a</definiens>
				<definiens id="1">the maximum number of nodes in a18 a14a20a19a24a22 needed to represent a state in a18 a14a20a19a24a22 –a1 depends on a19 and a12 , the length of the context used by the a12 -gram language model</definiens>
			</definition>
			<definition id="3">
				<sentence>We evaluate accuracy performance using two automatic metrics : an identity metric , ID , which measures the percent of sentences recreated exactly , and BLEU ( Papineni et al. , 2002 ) , which gives the geometric average of the number of uni- , bi- , tri- , and four-grams recreated exactly .</sentence>
				<definiendum id="0">BLEU</definiendum>
				<definiens id="0">an identity metric , ID , which measures the percent of sentences recreated exactly , and</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>Word Sense Disambiguation ( WSD ) is the task of determining the meaning of a word in a given context .</sentence>
				<definiendum id="0">Word Sense Disambiguation</definiendum>
				<definiendum id="1">WSD )</definiendum>
				<definiens id="0">the task of determining the meaning of a word in a given context</definiens>
			</definition>
			<definition id="1">
				<sentence>WSD System presented by Crestan et al. ( 2001 ) in SENSEVAL-2 classified words into WORDNET unique beginners .</sentence>
				<definiendum id="0">WSD System</definiendum>
				<definiens id="0">2001 ) in SENSEVAL-2 classified words into WORDNET unique beginners</definiens>
			</definition>
			<definition id="2">
				<sentence>The system consists of three classifiers , built using local context , part of speech and syntax-based relationships respectively , and combined with the mostfrequent sense classifier by using weighted majority voting .</sentence>
				<definiendum id="0">system</definiendum>
				<definiens id="0">consists of three classifiers , built using local context , part of speech and syntax-based relationships respectively , and combined with the mostfrequent sense classifier by using weighted majority voting</definiens>
			</definition>
			<definition id="3">
				<sentence>The distances are adjusted according to the formula ∆E ( X , Y ) = ∆ ( X , Y ) ew X +epsilon1 , where ∆E ( X , Y ) is the adjusted distance between instance Y and example X , ∆ ( X , Y ) is the original distance , ewX is the exemplar weight of instance X. The small constant epsilon1 is added to avoid division by zero .</sentence>
				<definiendum id="0">∆E</definiendum>
				<definiendum id="1">ewX</definiendum>
				<definiens id="0">the original distance</definiens>
			</definition>
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>In previous work , we have developed hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers that integrate textual and prosodic knowledge sources for detecting sentence boundaries .</sentence>
				<definiendum id="0">HMM</definiendum>
				<definiendum id="1">maximum entropy</definiendum>
				<definiens id="0">prosodic knowledge sources for detecting sentence boundaries</definiens>
			</definition>
			<definition id="1">
				<sentence>1 E i ) In the HMM , the forward-backward algorithm is used to determine the event with the highest posterior probability for each interword boundary : ^ E i = argmax E i P ( E i jW ; F ) ( 1 ) The HMM is a generative modeling approach since it describes a stochastic process with hidden variables ( sentence boundary ) that produces the observable data .</sentence>
				<definiendum id="0">HMM</definiendum>
				<definiendum id="1">HMM</definiendum>
			</definition>
			<definition id="2">
				<sentence>An HMM is a generative model , yet it is able to model the sequence via the forward-backward algorithm .</sentence>
				<definiendum id="0">HMM</definiendum>
				<definiens id="0">a generative model</definiens>
			</definition>
			<definition id="3">
				<sentence>Maxent is a discriminative model ; however , it attempts to make decisions locally , without using sequential information .</sentence>
				<definiendum id="0">Maxent</definiendum>
				<definiens id="0">a discriminative model ; however , it attempts to make decisions locally , without using sequential information</definiens>
			</definition>
			<definition id="4">
				<sentence>A conditional random field ( CRF ) model ( Lafferty et al. , 2001 ) combines the benefits of the HMM and Maxent approaches .</sentence>
				<definiendum id="0">conditional random field</definiendum>
				<definiendum id="1">CRF</definiendum>
			</definition>
			<definition id="5">
				<sentence>452 A CRF is a random field that is globally conditioned on an observation sequence O. CRFs have been successfully used for a variety of text processing tasks ( Lafferty et al. , 2001 ; Sha and Pereira , 2003 ; McCallum and Li , 2003 ) , but they have not been widely applied to a speech-related task with both acoustic and textual knowledge sources .</sentence>
				<definiendum id="0">CRF</definiendum>
				<definiendum id="1">McCallum</definiendum>
				<definiens id="0">a speech-related task with both acoustic and textual knowledge sources</definiens>
			</definition>
			<definition id="6">
				<sentence>The most likely event sequence ^E for the given input sequence ( observations ) O is ^ E = argmax E e P k k G k ( E ; O ) Z ( O ) ( 3 ) where the functions G are potential functions over the events and the observations , and Z is the normalization term : Z ( O ) = X E e P k k G k ( E ; O ) ( 4 ) Even though a CRF itself has no restriction on the potential functions G k ( E ; O ) , to simplify the model ( considering computational cost and the limited training set size ) , we use a first-order CRF in this investigation , as at the bottom of Figure 2 .</sentence>
				<definiendum id="0">Z</definiendum>
				<definiens id="0">^ E = argmax E e P k k G k ( E ; O ) Z ( O ) ( 3 ) where the functions G are potential functions over the events and the observations</definiens>
				<definiens id="1">the normalization term : Z ( O ) = X E e P k k G k ( E ; O ) ( 4</definiens>
			</definition>
			<definition id="7">
				<sentence>The average length of SUs is longer in BN than in CTS , that is , 12.35 words ( standard deviation 8.42 ) in BN compared to 7.37 words ( standard deviation 8.72 ) in CTS .</sentence>
				<definiendum id="0">average length of SUs</definiendum>
				<definiens id="0">longer in BN than in CTS , that is , 12.35 words ( standard deviation 8.42 ) in BN compared to 7.37 words ( standard deviation 8.72 ) in CTS</definiens>
			</definition>
			<definition id="8">
				<sentence>The SU error rate is defined as the total number of deleted or inserted SU boundary events , divided by the number of true SU boundaries .</sentence>
				<definiendum id="0">SU error rate</definiendum>
			</definition>
			<definition id="9">
				<sentence>The N-gram LM trained from the extra corpus is used to estimate posterior event probabilities for the LDC-annotated training and test sets , and these posteriors are then thresholded to yield binary features .</sentence>
				<definiendum id="0">N-gram LM</definiendum>
				<definiens id="0">the LDC-annotated training and test sets</definiens>
			</definition>
			<definition id="10">
				<sentence>We also notice from the CTS results that when only word N-gram information is used ( with or without combining with prosodic information ) , the HMM is superior to the Maxent ; only when various additional textual features are included in the feature set does Maxent show its strength compared to 456 Table 3 : Broadcast news SU detection results reported using the NIST SU error rate ( % ) and the boundarybased error rate ( % in parentheses ) using the HMM , Maxent , and CRF individually and in combination .</sentence>
				<definiendum id="0">HMM</definiendum>
				<definiendum id="1">NIST SU error rate</definiendum>
				<definiens id="0">used ( with or without combining with prosodic information ) , the</definiens>
			</definition>
</paper>

	</volume>
