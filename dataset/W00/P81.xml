<?xml version="1.0" encoding="UTF-8"?>
	<volume id="P81">

		<paper id="1027">
			<definition id="0">
				<sentence>But consider again the traditional definition , paraphrasing Scarle ( 1969:4gff ) , a speech act is the use of an utterance directed at an addreasce in the scrvicc of a set of intentions , namely , addressee , recognize the intention to produce the effect , and knowicdge of the rules governing the utterance .</sentence>
				<definiendum id="0">speech act</definiendum>
				<definiens id="0">the use of an utterance directed at an addreasce in the scrvicc of a set of intentions , namely , addressee , recognize the intention to produce the effect , and knowicdge of the rules governing the utterance</definiens>
			</definition>
			<definition id="1">
				<sentence>Cases ( b ) and ( c ) ensure that the utterance or part of utterance , it '' text initial , conuLins enough information to enable a situation-type to be derived .</sentence>
				<definiendum id="0">Cases</definiendum>
				<definiens id="0">the utterance or part of utterance , it '' text initial , conuLins enough information to enable a situation-type to be derived</definiens>
			</definition>
			<definition id="2">
				<sentence>In this case , then , there are at least three steps in thc `` semantic '' parsing of the utterance : thc initial creation of the situation-type ( the first clause ) , the interpretation of but .</sentence>
				<definiendum id="0">situation-type</definiendum>
				<definiens id="0">the first clause ) , the interpretation of but</definiens>
			</definition>
			<definition id="3">
				<sentence>R. and N. J. Nilsson ( 1971 ) STRIPS : A new approach to the application of theorem proving to problem solving , in Artificial lntelligenc£ Grosz .</sentence>
				<definiendum id="0">STRIPS</definiendum>
				<definiens id="0">A new approach to the application of theorem proving to problem solving , in Artificial lntelligenc£ Grosz</definiens>
			</definition>
			<definition id="4">
				<sentence>John ( 1 % 9 ) Speech acts an essay in the philosophy of languag~ Cambridge : Cambridge University Press .</sentence>
				<definiendum id="0">Speech</definiendum>
				<definiens id="0">acts an essay in the philosophy of languag~ Cambridge : Cambridge University Press</definiens>
			</definition>
</paper>

		<paper id="1023">
</paper>

		<paper id="1004">
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>The SIolFiller is a structure that would normally be • the filler of a slot in the top-level description ( of a message in this case ) , but the parser has been unable to determine exactly which higher .</sentence>
				<definiendum id="0">SIolFiller</definiendum>
			</definition>
			<definition id="1">
				<sentence>Since , we designed FlexP as a tool for use in natural language interfaces , we considered it unreasonable to expect the designer of such a system to have the specialized knowledge to create such obscure rules .</sentence>
				<definiendum id="0">FlexP</definiendum>
				<definiens id="0">a tool for use in natural language interfaces</definiens>
			</definition>
			<definition id="2">
				<sentence>The above construction for the description of a message , for instance , could be defined as a single unified construction without specifying any artificial intermediate constituents , as follows : \ [ StructureType : Object ObjectName : Message Schema : \ [ Sender : \ [ FillerType : &amp; Person\ ] Recipient : \ [ FillerType : &amp; Person Number : OneOrMore\ ] Date : \ [ FJllerType : &amp; Oats\ ] After : \ [ FJllerType : &amp; Date UseRestrict ion : OescrJpt ionOnly\ ] \ ] Syntax : \ [ SynType : NounPhrase Head : ( message note &lt; ? piece ? of mail &gt; ) Case : ( &lt; % from tSender &gt; &lt; ~to ~Recipient &gt; &lt; % dated toots &gt; &lt; % since ~After &gt; . )</sentence>
				<definiendum id="0">Date</definiendum>
				<definiendum id="1">Date UseRestrict ion</definiendum>
				<definiens id="0">a single unified construction without specifying any artificial intermediate constituents , as follows : \ [ StructureType : Object ObjectName : Message Schema : \ [ Sender : \ [ FillerType : &amp; Person\ ] Recipient : \ [ FillerType : &amp; Person Number : OneOrMore\ ]</definiens>
				<definiens id="1">OescrJpt ionOnly\ ] \ ] Syntax : \ [ SynType : NounPhrase Head : ( message note &lt; ? piece ? of mail &gt;</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>The base component is a set of functions that implicitly embody context free rules for creating a tree structure ( phrase marker ) in the X-bar framework ( as discussed by Chomsky ( 1970 ) , Jackendoff ( 1974 ) , Bresnan ( 1975 ) and others . )</sentence>
				<definiendum id="0">base component</definiendum>
				<definiendum id="1">X-bar framework</definiendum>
				<definiens id="0">a set of functions that implicitly embody context free rules for creating a tree structure ( phrase marker ) in the</definiens>
			</definition>
			<definition id="1">
				<sentence>Intermediate structures are also permitted ; for instance , V * ( read `` V bar '' ) is that portion of the Verb Phrase which consists of a Verb and its complements ( e.g. direct and indirect objects , clausal complements , prepositional phrases , etc. ) while V ~ ( read `` V double bar '' ) includes V ~ as well as Auxiliary elements .</sentence>
				<definiendum id="0">V *</definiendum>
			</definition>
			<definition id="2">
				<sentence>Minor categories ( such as DET ( erminer ) , AUX ( ilfary ) , NEG ( ative ) , etc. ) stand outside this system , as do S ( entence ) and S ~ ( a sort of super sentence , which contains S and clause introducing elements ( or `` subordinating conjunctions '' ) such as that ) .</sentence>
				<definiendum id="0">Minor categories</definiendum>
				<definiendum id="1">AUX ( ilfary</definiendum>
				<definiendum id="2">NEG</definiendum>
				<definiens id="0">contains S and clause introducing elements ( or `` subordinating conjunctions '' ) such as that )</definiens>
			</definition>
			<definition id="3">
				<sentence>Lexical insertion is an integral part of the construction of the tree by the base component .</sentence>
				<definiendum id="0">Lexical insertion</definiendum>
				<definiens id="0">an integral part of the construction of the tree by the base component</definiens>
			</definition>
			<definition id="4">
				<sentence>The Structural Change of each transformation consists of one or more functions , analogous to the transformational elementaries of traditional transformational theory ( Chomskv ( 1955 , pp .</sentence>
				<definiendum id="0">Structural Change of each transformation</definiendum>
				<definiens id="0">consists of one or more functions , analogous to the transformational elementaries of traditional transformational theory</definiens>
			</definition>
			<definition id="5">
				<sentence>For example , the direct object constraint ( DIROBJ ( PER 3 ) ( NU PL ) ... ) specifies all the base constraints necessary to produce the N ' '' subtree for the direct object position in the base structure .</sentence>
				<definiendum id="0">DIROBJ</definiendum>
				<definiens id="0">NU PL ) ... ) specifies all the base constraints necessary to produce the N ' '' subtree for the direct object position in the base structure</definiens>
			</definition>
			<definition id="6">
				<sentence>( Implications specify necessary dependencies ; synspecs specify possible but not necessary choices on the part of the system designers about what combinations of constraints should be invoked under a general name . )</sentence>
				<definiendum id="0">Implications</definiendum>
				<definiens id="0">specify necessary dependencies ; synspecs specify possible but not necessary choices on the part of the system designers about what combinations of constraints should be invoked under a general name</definiens>
			</definition>
</paper>

		<paper id="1011">
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>and Semantic Interpretation in the BBN Natural Language Understanding System .</sentence>
				<definiendum id="0">Semantic Interpretation</definiendum>
				<definiens id="0">in the BBN Natural Language Understanding System</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>LIFER employs special `` look down '' logic based on the current word in the sentence to eliminate obviously fruitless downward expansion when the current word can not be accepted as the leftmose element in any expansion of the currently proposed syntactic category \ [ Griffiths and Petrick , 1965\ ] and a `` well-formed substring table '' \ [ Woods , 1975\ ] to eliminate redundant pursuit of paths after back-up .</sentence>
				<definiendum id="0">LIFER</definiendum>
				<definiens id="0">employs special `` look down '' logic based on the current word in the sentence to eliminate obviously fruitless downward expansion when the current word can not be accepted as the leftmose element in any expansion of the currently proposed syntactic category \ [</definiens>
			</definition>
			<definition id="1">
				<sentence>The second parser employed , in the 5RI experiments was DIAMOND : an all-paths bottom-up parser \ [ Paxton , lg77\ ] developed at SRI as an outgrowth of the SRI Speech Understanding Project \ [ Walker , 1978\ ] .</sentence>
				<definiendum id="0">DIAMOND</definiendum>
				<definiens id="0">an all-paths bottom-up parser \ [ Paxton , lg77\ ] developed at SRI as an outgrowth of the SRI Speech Understanding Project \ [ Walker</definiens>
			</definition>
			<definition id="2">
				<sentence>DIAMOND , while implementeing the very efficient Cocke-Kasami-Younger algorithm and being augmented with an oracle and special programming tricks ( e.g. , assembly code ) intended to enhance its performance , is a rather massive program and might be considered suspect for that reason alone ; on the other hand , its predecessor was developed for the purpose of speech understanding , where efficiency issues predominate , and this strongly argues for good performance expectations .</sentence>
				<definiendum id="0">DIAMOND</definiendum>
				<definiens id="0">the very efficient Cocke-Kasami-Younger algorithm and being augmented with an oracle and special programming tricks</definiens>
			</definition>
			<definition id="3">
				<sentence>Linguistic Grammar In terms of the number of grammar rules found applicable by the parsers , DIAMOND instantiated the fewest ( averaging 58 phrases per sentence ) ; CKY , the most ( 121 ) ; and LIFER fell in between ( IO7 ) .</sentence>
				<definiendum id="0">CKY</definiendum>
				<definiendum id="1">LIFER</definiendum>
				<definiens id="0">rules found applicable by the parsers , DIAMOND instantiated the fewest ( averaging 58 phrases per sentence ) ;</definiens>
			</definition>
			<definition id="4">
				<sentence>LIFER makes copious use of CONS cells for internal processing purposes , and thus required the most storage ( averaging 5294 CQNSes per parsed sentence ) ; DIAMOND required the least ( llO7 ) ; CKY fell in between ( 1628 ) .</sentence>
				<definiendum id="0">LIFER</definiendum>
				<definiendum id="1">CKY</definiendum>
				<definiens id="0">makes copious use of CONS cells for internal processing purposes , and thus required the most storage ( averaging 5294 CQNSes per parsed sentence ) ; DIAMOND required the least ( llO7 ) ;</definiens>
			</definition>
			<definition id="5">
				<sentence>Summary of the LRC Experiment The LRC experiment involved the production of eight different instrumented systems -two parsers ( leftcorner and Cocke-Kasami-Younger ) , each with all four combinations of two independent strategy variations ( top-down filtering and early constituent tests ) -and eight test runs on the identical set of 262 sentences selected pseudo-randemly from three technical texts supplied by the MT project sponsor .</sentence>
				<definiendum id="0">Cocke-Kasami-Younger</definiendum>
				<definiens id="0">top-down filtering and early constituent tests ) -and eight test runs on the identical set of 262 sentences selected pseudo-randemly from three technical texts supplied by the MT project sponsor</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>The other box ( semamics ) represents that part of semantics that has to do with our conceptualiz .</sentence>
				<definiendum id="0">other box ( semamics )</definiendum>
				<definiens id="0">represents that part of semantics that has to do with our conceptualiz</definiens>
			</definition>
			<definition id="1">
				<sentence>AGENT ( the seller ) , RECIPIENT ( the buyer ) and OBJECT .</sentence>
				<definiendum id="0">AGENT</definiendum>
				<definiens id="0">the seller</definiens>
			</definition>
			<definition id="2">
				<sentence>SuperCategory , which gives the conceptual net a taxonomic ( or hierarchic ) structure in addition to the structure defined by the role relations .</sentence>
				<definiendum id="0">SuperCategory</definiendum>
				<definiens id="0">gives the conceptual net a taxonomic ( or hierarchic ) structure in addition to the structure defined by the role relations</definiens>
			</definition>
			<definition id="3">
				<sentence>The concept SELL ie defined by its I~lace in the taxonomy by having TRANSACTION as a SuperCate &lt; jory. If we want to , 4It ~toul¢l be eml ) t~ullz41~t ~tlt r.~lltng the cof~-.eot SELL 'u=y~l nothing wt'~lt=oe~t~r li~out ~ngli~tt exl~'qm~on for it : . ~e *'el.'lons for gz~ it filial ~ Ire I~urely fR~mo~i¢. o~ty way the conces=t elm be I~ocmted ~m ~ ~ =o/o ' is tlw~gf~ ~g ~ of I we can define a conceot that will have SELL as a SuDerCategoq ( i.e. bear the SuperCategory relation to SELL ) , for example SELLCB 'sell on the black market'. As a result , p ) art of the taxonomy of events is TRANSACTION -- SELL .-SELLOB. If TRANSACTION has a set of roles associated with it , this set may be inherited by SELL and by SELLOB .this is a generaJ feature of the SuperCategory relation. In the examples involving SELL that follow , I will concentrate on this concept and not try to generalize to its supercategones. The Semantic Subentry In the overview figure ( 1.1 ) , the semantics is shown as part of the concaptuaisThe consequence of this is that the set of semantic entries in the lexicon is a subset of the set of concepts. The subset is groper if we assume that there are concepts which have not been lexicaiized ( the assumption indicated in the figure ) . The a.csumption is I~erfectJy reasonable ; I have already invented the concept SELLOB for which there is no word in standard English : it is not surprising if we have formed concepts for which we have to create expressions rather than pick them reedy.made from our lexicon. Furthermore , if we construct a conceptual component intended to support say a bilingual speaker , there will be a number of concepts which are lexicaiized in only one of the two languages.. A semantic entry , than , is a concept in the conceptuaisFor sold , we find soil wiffi its associated roles , AGENT , RECIPIENT and OBJECT. The right ~ of figure 4.1 below ( marked `` se : ' ; after a figure from \ [ 1\ ] gives a more detailed semantic ent~ for sold : = pointer identifies the relevant part in the KR , the concept that constitutes the semantic entry ( here the concept SELL ) . The concept that constitutes the semantic entry of a lexicai item has a fairly rich structure. Roles are associated `` with the concept and the modailty ( neces~ury or optional ) , the ¢ardinaii~ of and restrictions on ( value of ) the fillers are given. Through the value restriction the linguistic notion of selection restriction is captured. The stone sold a carnation to the little girl is odd because the AGENT role of SELL is value restricted to PERSON or FRANCHISE and the concept associated with stone fails into neither type. The strategy of letting semantic entries be part of the knowledge representation would not have been possible in a notation designed to csgture specific propositions only , However , since KL-ONE pfoviles the distinction between intension and extension , the strategy is unl=rotolsmati¢ in the I=resant framework. So what is the relationship between intensional-extensionai and s~manti¢ entries ? The working aesumption is that for a large part of the '' vocaioulary , it is the concepts of the intanalonai part of the KR that may be lexicalized and thus serve as semantic entries. We have words for intenalonai obje¢= , actions and states , but not for indtviluai extensional obiects etc. with the exception of propel names. They have extensional concepts as their semantic entries. For instance , Alex denotes a particular individuated person and The War of the Roses a palrticula¢ individumed war. Both the Sul~H'Category relation and the Indiviluates relation provide ways of walking around in the KR to find expresmons for concepts. If 50 we are in the extensional part of the KR , looking at a particular individual , w~ can follow the Individuates link up to an intensional concept. There may be a word for it , in which case the concept is part of a laxical entry. If there is no word for the concept , we will have to consider the various options the grammar gives us for forming an ¢oPropriate exoressJon. The general assumption is that all the intensional vocabulary can he used for extensional concepts in the way just describe ( l : exc ) reasabi.. , 'y is inherited with the Individuates relation. Expression candidates for concepts can also be located along the SuberCate ( Jory link by going from one concept to another one higher up in the taxonomy. Consider the following example : Joan sold Arthur ice.cream. The transaction took place in tl~e perk. The SuperCate~ory link enables us to go from SELL to TRANSACTION , where we find the expression transaction. Lexical Semantic Relations The structure of the vocabulary is parasitic on the conceptual structure. In other words , laxicalized concepts are related not only to one another , but also to concepts for which there is no word , encoding in English ( i.e. non-laxicalized concepts ) . Crudely , the semantic structure of the lexicon can be described as being part of the hierarchy of intensional concepts -the intensional concepts that happen to be lexicalized in English. -The structure of English vocabulary is thus not the only principle that is reflected in the knowledge representation , but it is reflected. Very general concepts like OBJECT , THING and ACTION are at the top. In this hierarchy , roles are inherited. This corresponds to the semantic redundancy rules of a lexicon. Considering the possibility of walking around in the KR and the integration of texicalized and non.iexicalized concepts , the KR suggests itself as the natural place to state certain text-forming principles , some of which have been described under the terms lexical cohesion ( \ [ 8\ ] ) and Thematic Progression ( \ [ 6\ ] ) . I will now turn to the syntactic component in figure 1-1 , starting with a brief introduction to the framework ( Systemic Linguistics ) that does the same for that component as the notion of semantic net did for the component just discussed. 2,2. Lexicogrammar Systemic Linguistic~ stems from a British tradition and has been developed by its founder , Michael Halliday ( e.g. \ [ 7\ ] , \ [ 9\ ] , \ [ 10\ ] ) and other systemic linguists ( see e.g. \ [ 5\ ] , \ [ 4\ ] for S presentation of Fawcett 's interesting work on developing a systemic model within a cognitive model ) for over twenty years covering many areas of linguistic concern , including studies of text , ; exicogrammar , language development , and computational applications. Systemic Grammar was used in SHRDLU \ [ 15\ ] and more recently in another important contribution , Davey'a PROTEUS \ [ 3\ ] . The systemic tradition recognizes a fundamental principle in the organization of language : the distinction between cl~oice and the structures that express ( realize ) choices. Choice is taken as primary and is given special recC , ; \ ] nition in the formalization of the systemic model of language. Consequently , a description is a specification of the choices a speaker can make together with statement : ; about how he realizes a selection he has made. This realization of a set of choices is typically linear , e.g. a string of words. Each choice point is formalized as a , system ( hence the name Systemic ) . The options open to the speaker are two or more features that constitute alternatives which can ' be chosen. The preconditions for the choice are entry conciitiona to the system. Entry conditions are logical expressions whose elementary terms are features. All but one of the systems have non.emt~/ entry conditions. This causes an interdependency among the systems with the result that the grammar of English forms one network of systems , which cluster when a feature in one system is ( part of ) the entry condition to another system. This dependency gives the network depth : it starts ( at its `` root '' ) with very general choices. Other systems of choice depend on them ( i.e. have a feature from one of these systems -or st combination of features from more than one system .. as entry conditions ) so that the systems of choice become less general ( more delicate to use the , systemic term ) as we move along in the network. The network of systems is where the control of the grammar resides , its non.deterministic part. Systemic grammar thus contrasts with many other formalisms in that choice is given explicit representation and is captured in a single ruis type ( systems ) , not distributed over the grammar as e.g. optional rules of different types. This property of systemic grammar makes it s very useful component in a text-production system , seDecially in the interf3ce with semantics and in ensuring accessibility of alternatives. The rest of the grammar is deterministic .. the consequences of features chosen in the network of systems. These conse ( luences are formalized as feature realization statements whose task is to build the appropriate structure. For example , in independent indicative sentences , English offers a choice between declarative and interroaative sentences , if interrooativ~ is chosen , this leeds to a dependent system with a choice between wh-intsrrooative and ves/no-interroaative. When the latter is chosen , it is realized by having ~.he FINITE verb before the SUBJECT. Since it is the general design of the grammar that is the focus of attention , I will not go through the algorithm for generating a sentence as it has been implemented in NIGEL. The general observation is that the results are very encouraging , although it is incomplete. The algorithm generates a wide range of English structures correctly. There have not been any serious problems in implementing a grammar written in the systemic notation. Before turning to the lexico , part of lexicogrammar , I will give an example of the toplevel structure of a sentence generated by the grammar. ( I have left out the details of the internal structure of the constituents. ) iiiii ; o.i iIi i ! o t Iiiiii i\ ] \ ] iiiliiiii I ... ... ... .. ... . I ... ... ... ... . ... ... ... . In the park| Join / sold | Arthur 14ce-¢reem 51 The structure consists of three layers of function symbols , aJl of which are needed to get the result desired ... The structure is not only functional ( withfunction s/m/ools laloeling the const|tuents instead of category names like Noun Phrase and Verb Phrase ) but it is multifunctional. Each layer of function symbols shows a particular perspective on the clause structure. Layer \ [ 1\ ] gives the aspect of the sentence as a representation of our experience. The second layer structures the sentence as interaction between the speaker and the hearer ; , the fact that SUBJECT precedes FINITE signals that.the speaker is giving the hearer information. Layer \ [ 3\ ] represents a structuring of the clause as a message ; the THEME is its starting point. The functions are called experiential , inte~emonal and textual resm~-~Jvety in the systemic framework : the function symbols are said to belong to three different metafunctions , in the rest of the ! ~koar I will concentrate on the experiential metafunction , I=artiy because it will turn out to be highly relevant to the lexicon. The syntactic sut3entry. In the systemic tradition , the syntactic part of the lexicon is seen as a continuation of grammar ( hence the term lexicogrammar for both of them ) : lsxical choices are simply more detailed ( delicate ) than grammatical choices ( cf. \ [ 9\ ] ) . The vocabulary of English can be seen as one huge taxonomy , with Roget 's Thesaurus as a very rough model. A taxonomic organization of the relevant Dart of the vocabulary of English is intended for PENMAN , but this Organization is part of the conceptual organization mentioned al0ove. There is st present no separate lexicai taxonomy. The syntactic subentry potentially con~sts of two parts. There is alv~ye the class specification .. the lexical features. This is a statement of the grammatical potential of the lexicai item , i.e. of how it can be used grammatically. For sold the'ctas , ~ specification is the following : verb C'/I1~ |0 c~als 02 bemlf &amp; ct , 1re where `` benefactive '' says that sold can occur in a sentence with a BENEFICIARY , `` class 10 '' that it encodes a material pr~ ( contrasting with mental , varbai and relational processes ) and `` CMas 02 '' that it is a tnmaltive verb. In ~ldition , there is a provision for a configurationai part , which is a h'agment of a Structure the grammar can generate , more specifically the experiential part of the grammar , s The structure corresponds to the top layer ( # \ [ 1\ ] ) in the example above. In reference to this example , I can make more explicit wh~ I mean by fragment. The general point is that ( to take just one cimm as an example ) the presence and cflara~er of functions like ACTOR , BENEFICIARY and GOAL .diract t : ~'ticiplmts in the event denoted by the verb .depend on the type of verb , whereas the more circumstantial functions like LOCATION remain unaffected and a~oDlical=ie to all ~ of verb. Conse ( luently , the information about the poasibilib/ of having a LOCATION constituent is not the type of information that has to be stated for specific lsxical items. The information given for them concerns only a fragment of the experiential functional structure. The full syntactic entry for sol~ is : PROCESS • veto class IO class 02 befloflctlve ACTOR • GOAL 8EX ( FICZAR¥ `` This says that sold Can occur in a fragment of a struCtUre where it is PROCESS and there can be an ACTOR , a GOAL and a RENEF1CIARY. The usefulness of the structure fragment will be demonstrated in section 4. I will now turn to the fundamental proiolem of making a working s/stem out of the parts that have been discu~md. The problem ~ two parts to it. viz. and I will only be concerned with the 6rat aspect here. The components of the system have been presented. What remains -. and that is the problem -is to dealgn the misalng \ [ inks ; tO find the strategies that will do the job of connecting the components. Finding these strategies is a design problem in the following sense. The stnUegies do not come as accessories with the frameworks we have uasd ( the systemic framework and the KL-ONE inspired knowledge reprasentatJon ) . Moreover , th~me two frameworks stem from two quite dispm'ate traditions with different sets of goals , symbols and terms. I will state the problem for the grammar first and then for the lexicon. As it has been presented , the grammar runs wik : l and free. It is organized Mound choice , to be sure , but there is nothing to relate the choices to the rest of the Wstem , in particular to what we can take to be semantics. In other word~k although the grammar may have • ~ that faces ~emantics .. the system network , which ; in Hallldly'e worcls , is ~arnantically relevant grammar .it does not mmke direct contact with semantics. And , if we know what we want the system to ante &gt; de in a sentence , how can we indicate what goes where , that is what a constituent ( e.9 .</sentence>
				<definiendum id="0">Alex</definiendum>
				<definiendum id="1">specification</definiendum>
				<definiendum id="2">GOAL 8EX ( FICZAR¥ `` This</definiendum>
				<definiens id="0">its I~lace in the taxonomy by having TRANSACTION as a SuperCate &lt; jory. If we want to , 4It ~toul¢l be eml ) t~ullz41~t ~tlt r.~lltng the cof~-.eot SELL 'u=y~l nothing wt'~lt=oe~t~r li~out ~ngli~tt exl~'qm~on for it : . ~e *'el.'lons for gz~ it filial ~ Ire I~urely fR~mo~i¢. o~ty way the conces=t elm be I~ocmted ~m ~ ~ =o/o ' is tlw~gf~ ~g ~ of I we can define a conceot that will have SELL as a SuDerCategoq ( i.e. bear the SuperCategory relation to SELL ) , for example SELLCB 'sell on the black market'. As a result , p ) art of the taxonomy of events is TRANSACTION -- SELL .-SELLOB. If TRANSACTION has a set of roles associated with it , this set may be inherited by SELL and by SELLOB .this is a generaJ feature of the SuperCategory relation. In the examples involving SELL that follow , I will concentrate on this concept and not try to generalize to its supercategones. The Semantic Subentry In the overview figure ( 1.1 ) , the semantics is shown as part of the concaptuaisThe consequence of this is that the set of semantic entries in the lexicon is a subset of the set of concepts. The subset is groper if we assume that there are concepts which have not been lexicaiized ( the assumption indicated in the figure ) . The a.csumption is I~erfectJy reasonable ; I have already invented the concept SELLOB for which there is no word in standard English : it is not surprising if we have formed concepts for which we have to create expressions rather than pick them reedy.made from our lexicon. Furthermore , if we construct a conceptual component intended to support say a bilingual speaker , there will be a number of concepts which are lexicaiized in only one of the two languages.. A semantic entry , than , is a concept in the conceptuaisFor sold , we find soil wiffi its associated roles , AGENT , RECIPIENT and OBJECT. The right ~ of figure 4.1 below ( marked `` se : ' ; after a figure from \ [ 1\ ] gives a more detailed semantic ent~ for sold : = pointer identifies the relevant part in the KR , the concept that constitutes the semantic entry ( here the concept SELL ) . The concept that constitutes the semantic entry of a lexicai item has a fairly rich structure. Roles are associated `` with the concept and the modailty ( neces~ury or optional ) , the ¢ardinaii~ of and restrictions on ( value of ) the fillers are given. Through the value restriction the linguistic notion of selection restriction is captured. The stone sold a carnation to the little girl is odd because the AGENT role of SELL is value restricted to PERSON or FRANCHISE and the concept associated with stone fails into neither type. The strategy of letting semantic entries be part of the knowledge representation would not have been possible in a notation designed to csgture specific propositions only , However , since KL-ONE pfoviles the distinction between intension and extension , the strategy is unl=rotolsmati¢ in the I=resant framework. So what is the relationship between intensional-extensionai and s~manti¢ entries ? The working aesumption is that for a large part of the '' vocaioulary , it is the concepts of the intanalonai part of the KR that may be lexicalized and thus serve as semantic entries. We have words for intenalonai obje¢= , actions and states , but not for indtviluai extensional obiects etc. with the exception of propel names. They have extensional concepts as their semantic entries. For instance ,</definiens>
				<definiens id="1">a particular individuated person and The War of the Roses a palrticula¢ individumed war. Both the Sul~H'Category relation and the Indiviluates relation provide ways of walking around in the KR to find expresmons for concepts. If 50 we are in the extensional part of the KR , looking at a particular individual , w~ can follow the Individuates link up to an intensional concept. There may be a word for it , in which case the concept is part of a laxical entry. If there is no word for the concept , we will have to consider the various options the grammar gives us for forming an ¢oPropriate exoressJon. The general assumption is that all the intensional vocabulary can he used for extensional concepts in the way just describe ( l : exc ) reasabi.. , 'y is inherited with the Individuates relation. Expression candidates for concepts can also be located along the SuberCate ( Jory link by going from one concept to another one higher up in the taxonomy. Consider the following example : Joan sold Arthur ice.cream. The transaction took place in tl~e perk. The SuperCate~ory link enables us to go from SELL to TRANSACTION , where we find the expression transaction. Lexical Semantic Relations The structure of the vocabulary is parasitic on the conceptual structure. In other words , laxicalized concepts are related not only to one another , but also to concepts for which there is no word , encoding in English ( i.e. non-laxicalized concepts ) . Crudely , the semantic structure of the lexicon can be described as being part of the hierarchy of intensional concepts -the intensional concepts that happen to be lexicalized in English. -The structure of English vocabulary is thus not the only principle that is reflected in the knowledge representation , but it is reflected. Very general concepts like OBJECT , THING and ACTION are at the top. In this hierarchy , roles are inherited. This corresponds to the semantic redundancy rules of a lexicon. Considering the possibility of walking around in the KR and the integration of texicalized and non.iexicalized concepts , the KR suggests itself as the natural place to state certain text-forming principles , some of which have been described under the terms lexical cohesion ( \ [ 8\ ] ) and Thematic Progression ( \ [ 6\ ] ) . I will now turn to the syntactic component in figure 1-1 , starting with a brief introduction to the framework ( Systemic Linguistics ) that does the same for that component as the notion of semantic net did for the component just discussed. 2,2. Lexicogrammar Systemic Linguistic~ stems from a British tradition and has been developed by its founder , Michael Halliday ( e.g. \ [ 7\ ] , \ [ 9\ ] , \ [ 10\ ] ) and other systemic linguists ( see e.g. \ [ 5\ ] , \ [ 4\ ] for S presentation of Fawcett 's interesting work on developing a systemic model within a cognitive model ) for over twenty years covering many areas of linguistic concern , including studies of text , ; exicogrammar , language development , and computational applications. Systemic Grammar was used in SHRDLU \ [ 15\ ] and more recently in another important contribution , Davey'a PROTEUS \ [ 3\ ] . The systemic tradition recognizes a fundamental principle in the organization of language : the distinction between cl~oice and the structures that express ( realize ) choices. Choice is taken as primary and is given special recC , ; \ ] nition in the formalization of the systemic model of language. Consequently , a description is a specification of the choices a speaker can make together with statement : ; about how he realizes a selection he has made. This realization of a set of choices is typically linear , e.g. a string of words. Each choice point is formalized as a , system ( hence the name Systemic ) . The options open to the speaker are two or more features that constitute alternatives which can ' be chosen. The preconditions for the choice are entry conciitiona to the system. Entry conditions are logical expressions whose elementary terms are features. All but one of the systems have non.emt~/ entry conditions. This causes an interdependency among the systems with the result that the grammar of English forms one network of systems , which cluster when a feature in one system is ( part of ) the entry condition to another system. This dependency gives the network depth : it starts ( at its `` root '' ) with very general choices. Other systems of choice depend on them ( i.e. have a feature from one of these systems -or st combination of features from more than one system .. as entry conditions ) so that the systems of choice become less general ( more delicate to use the , systemic term ) as we move along in the network. The network of systems is where the control of the grammar resides , its non.deterministic part. Systemic grammar thus contrasts with many other formalisms in that choice is given explicit representation and is captured in a single ruis type ( systems ) , not distributed over the grammar as e.g. optional rules of different types. This property of systemic grammar makes it s very useful component in a text-production system , seDecially in the interf3ce with semantics and in ensuring accessibility of alternatives. The rest of the grammar is deterministic .. the consequences of features chosen in the network of systems. These conse ( luences are formalized as feature realization statements whose task is to build the appropriate structure. For example , in independent indicative sentences , English offers a choice between declarative and interroaative sentences , if interrooativ~ is chosen , this leeds to a dependent system with a choice between wh-intsrrooative and ves/no-interroaative. When the latter is chosen , it is realized by having ~.he FINITE verb before the SUBJECT. Since it is the general design of the grammar that is the focus of attention , I will not go through the algorithm for generating a sentence as it has been implemented in NIGEL. The general observation is that the results are very encouraging , although it is incomplete. The algorithm generates a wide range of English structures correctly. There have not been any serious problems in implementing a grammar written in the systemic notation. Before turning to the lexico , part of lexicogrammar , I will give an example of the toplevel structure of a sentence generated by the grammar. ( I have left out the details of the internal structure of the constituents. ) iiiii ; o.i iIi i ! o t Iiiiii i\ ] \ ] iiiliiiii I ... ... ... .. ... . I ... ... ... ... . ... ... ... . In the park| Join / sold | Arthur 14ce-¢reem 51 The structure consists of three layers of function symbols , aJl of which are needed to get the result desired ... The structure is not only functional ( withfunction s/m/ools laloeling the const|tuents instead of category names like Noun Phrase and Verb Phrase ) but it is multifunctional. Each layer of function symbols shows a particular perspective on the clause structure. Layer \ [ 1\ ] gives the aspect of the sentence as a representation of our experience. The second layer structures the sentence as interaction between the speaker and the hearer ; , the fact that SUBJECT precedes FINITE signals that.the speaker is giving the hearer information. Layer \ [ 3\ ] represents a structuring of the clause as a message ; the THEME is its starting point. The functions are called experiential , inte~emonal and textual resm~-~Jvety in the systemic framework : the function symbols are said to belong to three different metafunctions , in the rest of the ! ~koar I will concentrate on the experiential metafunction , I=artiy because it will turn out to be highly relevant to the lexicon. The syntactic sut3entry. In the systemic tradition , the syntactic part of the lexicon is seen as a continuation of grammar ( hence the term lexicogrammar for both of them ) : lsxical choices are simply more detailed ( delicate ) than grammatical choices ( cf. \ [ 9\ ] ) . The vocabulary of English can be seen as one huge taxonomy , with Roget 's Thesaurus as a very rough model. A taxonomic organization of the relevant Dart of the vocabulary of English is intended for PENMAN , but this Organization is part of the conceptual organization mentioned al0ove. There is st present no separate lexicai taxonomy. The syntactic subentry potentially con~sts of two parts. There is alv~ye the class specification .. the lexical features. This is a statement of the grammatical potential of the lexicai item , i.e. of how it can be used grammatically. For sold the'ctas , ~</definiens>
				<definiens id="2">the following : verb C'/I1~ |0 c~als 02 bemlf &amp; ct , 1re where `` benefactive '' says that sold can occur in a sentence with a BENEFICIARY , `` class 10 '' that it encodes a material pr~ ( contrasting with mental , varbai and relational processes ) and `` CMas 02 '' that it is a tnmaltive verb. In ~ldition , there is a provision for a configurationai part , which is a h'agment of a Structure the grammar can generate , more specifically the experiential part of the grammar , s The structure corresponds to the top layer ( # \ [ 1\ ] ) in the example above. In reference to this example , I can make more explicit wh~ I mean by fragment. The general point is that ( to take just one cimm as an example ) the presence and cflara~er of functions like ACTOR , BENEFICIARY and GOAL .diract t : ~'ticiplmts in the event denoted by the verb .depend on the type of verb , whereas the more circumstantial functions like LOCATION remain unaffected and a~oDlical=ie to all ~ of verb. Conse ( luently , the information about the poasibilib/ of having a LOCATION constituent is not the type of information that has to be stated for specific lsxical items. The information given for them concerns only a fragment of the experiential functional structure. The full syntactic entry for sol~ is : PROCESS • veto class IO class 02 befloflctlve ACTOR •</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>Specifically , the literal meaning of an utterance is one of chose things from which hearers infer speakers '' intentions .</sentence>
				<definiendum id="0">literal meaning of an utterance</definiendum>
				<definiens id="0">one of chose things from which hearers infer speakers '' intentions</definiens>
			</definition>
			<definition id="1">
				<sentence>Logic , semantic networks , frames , scripts , and production systems are all different forms of representation .</sentence>
				<definiendum id="0">Logic</definiendum>
				<definiens id="0">semantic networks , frames , scripts , and production systems are all different forms of representation</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus '~very P is q '' becomes ( EVERY X ( IMPLIES ( P X ) ( q X ) ) ) , and `` Some P is Q'* becomes ( SOME X ( AND ( e X ) ( q X ) ) ) It seems somewhat inelegant to have to use different connectives to Join ( P X ) and ( ~ X ) in the two cases , but semantically it works .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">X</definiendum>
			</definition>
			<definition id="3">
				<sentence>The arguments of COMB may be individuals or sets of individuals , and the value of COMB is the set chat contains all the individual arguments and all the elements of the set arguments ; thus , ( COMB A iS C } D { E F C } ) = { A S C D E F G } ( The notation using braces is NOT part of the logicalform language ; this example is Just an attempt to illustrate what COMB means in terms of more conventional concepts . )</sentence>
				<definiendum id="0">COMB</definiendum>
				<definiens id="0">the set chat contains all the individual arguments and all the elements of the set arguments</definiens>
			</definition>
			<definition id="4">
				<sentence>In other cases , substances seem to be more llke abstract individuals , e.g. , `` Copper is the twentyninth element in the periodic table . ''</sentence>
				<definiendum id="0">Copper</definiendum>
				<definiens id="0">the twentyninth element in the periodic table</definiens>
			</definition>
			<definition id="5">
				<sentence>For thls notion of meaning , Kaplan uses the term `` character . ''</sentence>
				<definiendum id="0">Kaplan</definiendum>
			</definition>
</paper>

		<paper id="1019">
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>`` Delete '' matches no active case filler , and hence it is parsed as the initial Segment Of a second conjoined utterance .</sentence>
				<definiendum id="0">Delete ''</definiendum>
				<definiens id="0">matches no active case filler , and hence it is parsed as the initial Segment Of a second conjoined utterance</definiens>
			</definition>
</paper>

		<paper id="1025">
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>The generalization process involves taking the 'greatest common denominator ' ( GCD ) of the positive and negative values of the segments of the environments of two separate 'rules ' .</sentence>
				<definiendum id="0">generalization process</definiendum>
				<definiens id="0">involves taking the 'greatest common denominator ' ( GCD ) of the positive and negative values of the segments of the environments of two separate 'rules '</definiens>
			</definition>
			<definition id="1">
				<sentence>NATMATCH compares these `` rules '' with the data base of common phonological processes .</sentence>
				<definiendum id="0">NATMATCH</definiendum>
				<definiens id="0">compares these `` rules '' with the data base of common phonological processes</definiens>
			</definition>
			<definition id="2">
				<sentence>RULERED RULERED takes the generated `` rules '' that have not been established .</sentence>
				<definiendum id="0">RULERED RULERED</definiendum>
				<definiens id="0">takes the generated `` rules '' that have not been established</definiens>
			</definition>
			<definition id="3">
				<sentence>DISCUSSION PHONY is a learning program .</sentence>
				<definiendum id="0">DISCUSSION PHONY</definiendum>
				<definiens id="0">a learning program</definiens>
			</definition>
			<definition id="4">
				<sentence>Independently PHONY is an expert system .</sentence>
				<definiendum id="0">PHONY</definiendum>
				<definiens id="0">an expert system</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>s arc in the class NP , then LFGs would be shown to be NP-complcte .</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">arc in the class NP</definiens>
			</definition>
			<definition id="1">
				<sentence>Predicate : 'go ( ( TSubject ) &gt; ' .</sentence>
				<definiendum id="0">Predicate</definiendum>
				<definiens id="0">'go ( ( TSubject ) &gt; '</definiens>
			</definition>
			<definition id="2">
				<sentence>Funhennore , the class NP is clearly a subset of PSP^CE ( since if a function uses Space N , it must use at least Time N ) , and it is suspected , but not known for certain , that NP is a proper subset of PSPACE .</sentence>
				<definiendum id="0">Funhennore</definiendum>
				<definiendum id="1">NP</definiendum>
				<definiens id="0">a proper subset of PSPACE</definiens>
			</definition>
			<definition id="3">
				<sentence>Lexical-funclional Grommar : A Formal System for Grammatical Representation , Cambridge , MA : MIT Cognitive Science Occasional Paper # 13 , 1981 .</sentence>
				<definiendum id="0">Lexical-funclional Grommar</definiendum>
				<definiens id="0">A Formal System for Grammatical Representation</definiens>
			</definition>
</paper>

		<paper id="1009">
</paper>

		<paper id="1008">
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>The grammar itself consists of two sets of principles .</sentence>
				<definiendum id="0">grammar itself</definiendum>
				<definiens id="0">consists of two sets of principles</definiens>
			</definition>
			<definition id="1">
				<sentence>For example~obots were analyzed as OBJ ( bullds ) ( as well as SUBJ ( fly-\ ] ~-T , then either Alice or SlCOUld be analyzed as SUBJ ( builds ) completing d. ( 18 ) Alice builds planes faster than rgbo~s fly them SU~J I '' `` F OBa S~BJq '' ~Bj PR/build PR/fly OBj PR/build could then be analyzed as SUBJ ( faster ) and all the necessary relations between arguments and predicates , and between predicates themselves ( i.e , ordination relations ) would be assigned .</sentence>
				<definiendum id="0">OBJ</definiendum>
				<definiendum id="1">SUBJ</definiendum>
				<definiens id="0">SUBJ ( faster ) and all the necessary relations between arguments and predicates , and between predicates themselves ( i.e , ordination relations</definiens>
			</definition>
			<definition id="2">
				<sentence>This particular predicational structure is defined as a Predicate Containing Term or PCT by the Term Definition~ The Comparative Object Restriction has the effect of allowing the OBJ ( CO~P P ) to be a PCT .</sentence>
				<definiendum id="0">OBJ ( CO~P P )</definiendum>
				<definiens id="0">a Predicate Containing Term or PCT by the Term Definition~ The Comparative Object Restriction has the effect of allowing the</definiens>
			</definition>
</paper>

		<paper id="1017">
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>In essence , TED is a LADDER-like natural-language processing system for accessing databases , combined with an `` automated interface expert '' that interviews users to learn the language and logical structure associated with a particular database and that automatically tailors the system for use with the particular application .</sentence>
				<definiendum id="0">TED</definiendum>
				<definiens id="0">a LADDER-like natural-language processing system for accessing databases , combined with an `` automated interface expert '' that interviews users to learn the language and logical structure associated with a particular database</definiens>
			</definition>
			<definition id="1">
				<sentence>One of the single-word co -- -- nds accepted by the TED executive system is the command NEW , which initiates a dialogue prompting the user to supply information about the structure of a new data file .</sentence>
				<definiendum id="0">NEW</definiendum>
				<definiens id="0">initiates a dialogue prompting the user to supply information about the structure of a new data file</definiens>
			</definition>
			<definition id="2">
				<sentence>The NEW dialogue allows the user to think of the file as a table of information and asks relatively simple questions about each of the fields ( columns ) in the file ( table ) .</sentence>
				<definiendum id="0">NEW dialogue</definiendum>
				<definiens id="0">allows the user to think of the file as a table of information and asks relatively simple questions about each of the fields ( columns ) in the file ( table )</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , TED asks for the heading names of the columns , for possible synonyms for the heading names , and for information about the types of values ( numeric , Boolean , or symbolic ) that each column can contain .</sentence>
				<definiendum id="0">TED</definiendum>
				<definiens id="0">numeric , Boolean , or symbolic ) that each column can contain</definiens>
			</definition>
			<definition id="4">
				<sentence>TED is progra- , ~ed wlch the knowledge that the identifier of an object must be some kind of name , rather than a numeric quantity or Boolean value .</sentence>
				<definiendum id="0">TED</definiendum>
				<definiens id="0">progra- , ~ed wlch the knowledge that the identifier of an object must be some kind of name , rather than a numeric quantity or Boolean value</definiens>
			</definition>
			<definition id="5">
				<sentence>At Point G , TED acquires possible synonyms for NAME .</sentence>
				<definiendum id="0">TED</definiendum>
				<definiens id="0">acquires possible synonyms for NAME</definiens>
			</definition>
			<definition id="6">
				<sentence>For example , as at Points J , K and ¥ , TED asks whether symbolic field values can be used as modifiers ( usually in noun-~oun combinations ) .</sentence>
				<definiendum id="0">TED</definiendum>
				<definiens id="0">asks whether symbolic field values can be used as modifiers</definiens>
			</definition>
			<definition id="7">
				<sentence>ADJ &gt; is the category for adjectives ( e.g. OLD ) associated with numeric fields .</sentence>
				<definiendum id="0">ADJ &gt;</definiendum>
				<definiens id="0">the category for adjectives ( e.g. OLD ) associated with numeric fields</definiens>
			</definition>
			<definition id="8">
				<sentence>ADJ &gt; &lt; BE &gt; &lt; ITEM &gt; how old is the youngest ship &lt; WHDET &gt; &lt; ITEM &gt; &lt; HAVE &gt; &lt; FEATURE &gt; what leahy ships have a doctor &lt; WHDET &gt; &lt; ITEM &gt; &lt; BE &gt; &lt; COMPLEMENT &gt; which ships are older then reeves &lt; PRESENT &gt; - &gt; WHAT &lt; BE &gt; PRINT &lt; ATrR &gt; - &gt; &lt; NUM.ATTR &gt; &lt; SYM.ATTR &gt; &lt; BOOL.ATTK &gt; &lt; ITEM &gt; - &gt; &lt; GENERIC &gt; ships &lt; ID.VALUE &gt; reeves THE &lt; ITEM &gt; the oldest shlp &lt; MOD.VALUE &gt; &lt; ITEM &gt; leahy ships &lt; SUPERLATIVE &gt; &lt; ITEM &gt; fastest ship with • doctor &lt; ITEM &gt; &lt; WITH &gt; &lt; FEATURE &gt; ship with a speed greater than 12 &lt; FEATURE &gt; - &gt; &lt; BOOL.ATTR &gt; doctor / poisonous &lt; NUN.ATTE &gt; &lt; NUM.COMP &gt; &lt; NUMBER &gt; age of 15 &lt; NUM.ATTR. &gt; &lt; NUM.COMP &gt; &lt; ITEM &gt; age greater than reeves &lt; NUM.COMP &gt; - &gt; &lt; COMP.ADJ &gt; THAN OF ( GREATER &gt; THAN &lt; COMPLEMENT &gt; - &gt; &lt; COMP.A/kJ &gt; THAN &lt; ITEM &gt; &lt; COMP.ADJ &gt; THAN &lt; NUMBER &gt; These pragmatic Era-mar rules are very much like the ones used in LADDER \ [ 2\ ] , but they differ from those of LADDER in two critical ways .</sentence>
				<definiendum id="0">old</definiendum>
				<definiens id="0">the youngest ship &lt; WHDET &gt; &lt; ITEM &gt; &lt; HAVE &gt; &lt; FEATURE &gt; what leahy ships have a doctor &lt; WHDET &gt; &lt; ITEM &gt; &lt; BE &gt; &lt; COMPLEMENT &gt; which ships are older then reeves &lt; PRESENT &gt; - &gt; WHAT &lt; BE &gt; PRINT &lt; ATrR &gt; - &gt; &lt; NUM.ATTR &gt; &lt; SYM.ATTR &gt; &lt; BOOL.ATTK &gt;</definiens>
			</definition>
</paper>

		<paper id="1007">
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>Some of the restrictions of this model include : ( i ) importance of the last proposition before NEW in the analysis of NEW ; ( 2 ) preference for relations with propositions closer to NEW ; ( 3 ) considering only the last brother in a set of evidence when NEW seeks to relate to prior propositions .</sentence>
				<definiendum id="0">NEW</definiendum>
			</definition>
			<definition id="1">
				<sentence>For now , we 'd say that a proposition which offers contrast to some evidence for a claim is ( counter ) evidence for that claim , and hence S is son of P. And a proposition which contrasts another directly , without evidence being presented is a ( counter ) claim , and hence S is a brother to 9 .</sentence>
				<definiendum id="0">hence S</definiendum>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>A `` partitive '' may be defined as a noun which serves as a general term for a PART of another large and often very non-homogeneous set of concepts .</sentence>
				<definiendum id="0">partitive ''</definiendum>
				<definiens id="0">a noun which serves as a general term for a PART of another large and often very non-homogeneous set of concepts</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>My assumptions are : ( t ) that the output ot the dictionary ( the Input to my realization component ) is a representation of a natural language phrase as defined by the grammar and with both words and other objects from the domain network as its terminals ( the embedded domain objects correspond to the variable parts of'the phrase , i.e. the arguments to the original network relation ) : and ( 2 ) that the planning process ( the component that decides what to say ) will specify that network objects be described either as a composition era set of other network relations that it has explicitly selected , or else will leave the de~ : riptiun to a default given in the dictionary .</sentence>
				<definiendum id="0">realization component )</definiendum>
				<definiens id="0">a representation of a natural language phrase as defined by the grammar and with both words and other objects from the domain network as its terminals ( the embedded domain objects correspond to the variable parts of'the phrase , i.e. the arguments to the original network relation</definiens>
				<definiens id="1">the component that decides what to say ) will specify that network objects be described either as a composition era set of other network relations that it has explicitly selected</definiens>
			</definition>
			<definition id="1">
				<sentence>MnLE which produces the `` compound '' phrase on the basis of the linguistic description of the argument phrases .</sentence>
				<definiendum id="0">MnLE</definiendum>
				<definiens id="0">produces the `` compound '' phrase on the basis of the linguistic description of the argument phrases</definiens>
			</definition>
</paper>

		<paper id="1024">
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>PSG is a gr -- -n-tical theory developed by Gerald Gazdar at Sussex , in collaboration with others in both the US and Britain , most notably Ivan Sag , Geoff Pull , -- , and Ewan Klein .</sentence>
				<definiendum id="0">PSG</definiendum>
				<definiens id="0">a gr -- -n-tical theory developed by Gerald Gazdar at Sussex</definiens>
			</definition>
			<definition id="1">
				<sentence>Note that while this rule is keyed off inactive edges the top-down rule was triggered by active edges being added .</sentence>
				<definiendum id="0">inactive</definiendum>
				<definiens id="0">edges the top-down rule was triggered by active edges being added</definiens>
			</definition>
			<definition id="2">
				<sentence>*Defective invocation strategies , which never invoke a needed rule , or invoke it more than once at the same place , can of course vitiate this guarantee .</sentence>
				<definiendum id="0">*Defective invocation strategies</definiendum>
				<definiens id="0">a needed rule , or invoke it more than once at the same place</definiens>
			</definition>
			<definition id="3">
				<sentence>Meta-rules are pattern-based rewrite rules which provide for the convenient expression of a class of syntactic regularities e.g. passive and subject-auxilliary inversion .</sentence>
				<definiendum id="0">Meta-rules</definiendum>
				<definiens id="0">pattern-based rewrite rules which provide for the convenient expression of a class of syntactic regularities e.g. passive and subject-auxilliary inversion</definiens>
			</definition>
			<definition id="4">
				<sentence>Rule schemata are another notational convenience , which use variables over categories ( and features ) to express compactly a large ( but finite ) family of rules .</sentence>
				<definiendum id="0">Rule schemata</definiendum>
				<definiens id="0">use variables over categories</definiens>
			</definition>
			<definition id="5">
				<sentence>For instance , the rule { S - &gt; NP\ [ PN x\ ] VP\ [ FN x\ ] } *~ where PN is the person-number feature and x is a variable , is a compact expression of the requirement that subject and verb ( -phrase ) agree in person-number , and { x - &gt; x and x } might be a simplified rule for handling conjunction .</sentence>
				<definiendum id="0">PN</definiendum>
				<definiens id="0">the person-number feature and x is a variable , is a compact expression of the requirement that subject and verb ( -phrase ) agree in person-number</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>A referentially opaque context is one in which two expressions that refer to the same real world concept can not be interchanged in the context without changing the meaning of the utterance \ [ Quine .</sentence>
				<definiendum id="0">opaque context</definiendum>
			</definition>
			<definition id="1">
				<sentence>`` Language driven inference '' is a style of natural language processing where the infcrencing process is driven ( and hence limited ) by the phrasing of the user 's request .</sentence>
				<definiendum id="0">Language driven inference ''</definiendum>
				<definiens id="0">a style of natural language processing where the infcrencing process is driven ( and hence limited ) by the phrasing of the user 's request</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>Test : Are there any topics which are requests for information which have not been answered ?</sentence>
				<definiendum id="0">Test</definiendum>
			</definition>
			<definition id="1">
				<sentence>Action : Retrieve the hypothetical part , form all `` necessary '' questions , and offer them as utterances .</sentence>
				<definiendum id="0">Action</definiendum>
				<definiens id="0">Retrieve the hypothetical part , form all `` necessary '' questions , and offer them as utterances</definiens>
			</definition>
			<definition id="2">
				<sentence>T0005 ( ( CON ( ( &lt; = &gt; ( $ QUAL &amp; AREA ( 'HARDWARE ' ) &amp; TAKER ( 'FRANK ' ) &amp; RESULT ( 'REMEDIAL ' ) ) ) ) LEADTO ( ( CON ( ( ACTOR ( 'FRANK ' ) IS ( 'CONFIDENCE '' VAL ( &gt; 0 ) ) ) MOP ( 'NEG '' `` HYPO ' ) ) LEADTO ( ( ACTOR ( 'FRANK ' ) IS ( 'HAPPINESS '' VAL ( 0 ) ) ) ) ) MOP ( 'HYPO ' ) ) ) ) ( INITIATED ( U0013 ) SUCC T0009 CPURPOSE REQINFO INITIATEDBY ( RULE2 U0002 ) ISA ( 'TOPIC ' ) PRED T0004 ) Figure 3-3 : A sample topic in detail Along with raising these topics , the rules store the utterance and script post-inferences in the semantic network , under all the nodes mentioned in them .</sentence>
				<definiendum id="0">T0005 ( ( CON</definiendum>
				<definiendum id="1">RESULT</definiendum>
				<definiens id="0">the utterance and script post-inferences in the semantic network , under all the nodes mentioned in them</definiens>
			</definition>
</paper>

		<paper id="1006">
</paper>

		<paper id="1022">
</paper>

		<paper id="1020">
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>Proper use of presupposition ( which has not yet been attempted computationally ) likewise depends on this knowledge , and many of the techniques for maintaining coherence depend on it as well .</sentence>
				<definiendum id="0">presupposition</definiendum>
				<definiens id="0">has not yet been attempted computationally ) likewise depends on this knowledge</definiens>
			</definition>
			<definition id="1">
				<sentence>In KDS , and presumably in PROTEUS as well , aggregation rules can be used to make text brief , in effect , PROTEUS already has some aggregation , since the way its uses of conjunction shorten the text is similar to effects of aggregation rules in KDS .</sentence>
				<definiendum id="0">aggregation rules</definiendum>
				<definiens id="0">similar to effects of aggregation rules in KDS</definiens>
			</definition>
			<definition id="2">
				<sentence>PROTEUS lacks the necessary text-organization methods .</sentence>
				<definiendum id="0">PROTEUS</definiendum>
				<definiens id="0">lacks the necessary text-organization methods</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>An evaluator probes the system under test to find conditions under which it fails .</sentence>
				<definiendum id="0">evaluator</definiendum>
				<definiens id="0">probes the system under test to find conditions under which it fails</definiens>
			</definition>
			<definition id="1">
				<sentence>Consequently , ther~ is a taxonomy of concepts implicit in the knowledge representation literature .</sentence>
				<definiendum id="0">ther~</definiendum>
				<definiens id="0">a taxonomy of concepts implicit in the knowledge representation literature</definiens>
			</definition>
</paper>

		<paper id="1015">
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>A CONTACT relationship implies the existence of LOCPT 's and a SAMEPLACE relationship between them .</sentence>
				<definiendum id="0">CONTACT relationship</definiendum>
				<definiens id="0">implies the existence of LOCPT 's and a SAMEPLACE relationship between them</definiens>
			</definition>
			<definition id="1">
				<sentence>Fodor conclnues , `` Whether there are any SYNTACTIC properties of case categories that Fillmore 's theory predicts but which are missed by the semantic approach is another question ... . '' It Is the thesis of thls paper that these synCactlc properties of case categories are the very cues that are used to drive the filling of semantic arguments by syntactic constituents .</sentence>
				<definiendum id="0">SYNTACTIC</definiendum>
				<definiendum id="1">semantic approach</definiendum>
				<definiens id="0">properties of case categories that Fillmore 's theory predicts but which are missed by the</definiens>
				<definiens id="1">the very cues that are used to drive the filling of semantic arguments by syntactic constituents</definiens>
			</definition>
			<definition id="2">
				<sentence>The LOCATION ( particle , strlng ) predicate is left to stand alone and is in turn expanded .</sentence>
				<definiendum id="0">LOCATION</definiendum>
				<definiens id="0">strlng ) predicate is left to stand alone and is in turn expanded</definiens>
			</definition>
</paper>

		<paper id="1026">
</paper>

	</volume>
