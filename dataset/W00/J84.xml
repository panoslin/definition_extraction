<?xml version="1.0" encoding="UTF-8"?>
	<volume id="J84">

		<paper id="3005">
			<definition id="0">
				<sentence>Schematically , oar output structure would have to look something like that in figure 2 , where R indicates a cyclic node .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">indicates a cyclic node</definiens>
			</definition>
			<definition id="1">
				<sentence>The debracketization consists of terminal elements sans right and left brackets , but with traces and PROs .</sentence>
				<definiendum id="0">debracketization</definiendum>
				<definiens id="0">consists of terminal elements sans right and left brackets , but with traces and PROs</definiens>
			</definition>
			<definition id="2">
				<sentence>As Peters and Ritchie go on to say : We now see that any grammar satisfying such a cut-elimination theorem generates a language which more than being recursive is context sensitive .</sentence>
				<definiendum id="0">cut-elimination theorem</definiendum>
				<definiens id="0">generates a language which more than being recursive is context sensitive</definiens>
			</definition>
			<definition id="3">
				<sentence>( II ) : Any of the structures of ( I ) with empty categories ( e.c. 's ) replacing NPs , subject to the following conditions , are well-formed annotated surface structures : ( i ) Every such e.c. is an atomic constituent with a numerical index and with no internal bracketing ; ( ii ) If the e.c is governed by some X ° ( lexical element such as verb , noun , and so on ) , where X governs Y iff the first branching node dominating X dominates Y , and there is no intervening maximal projection ( full phrase ) between X and Y then : antecedent ( = element with the same numerical index ) , where c-command is defined just as government but dropping the clause about maximal projections ; and another e.c. in a non-argument ( A ) position ( the complement of the A positions defined above ) ; and dent , where subjacency is defined as usual .</sentence>
				<definiendum id="0">c-command</definiendum>
				<definiendum id="1">non-argument ( A ) position</definiendum>
				<definiens id="0">Any of the structures of ( I ) with empty categories ( e.c. 's ) replacing NPs , subject to the following conditions , are well-formed annotated surface structures : ( i ) Every such e.c. is an atomic constituent with a numerical index and with no internal bracketing ; ( ii ) If the e.c is governed by some X ° ( lexical element such as verb , noun</definiens>
				<definiens id="1">intervening maximal projection ( full phrase ) between X and Y then : antecedent ( = element with the same numerical index )</definiens>
				<definiens id="2">government but dropping the clause about maximal projections</definiens>
			</definition>
			<definition id="4">
				<sentence>~3 ( III ) : Any of the structures defined by ( I ) and ( II ) , and , in addition , with a wh phrase in COMP position c-commanding a governed e.c. , or another wh phrase in COMP position and with the same index as that other e.c. or phrase , is a well-formed annotated surface structure .</sentence>
				<definiendum id="0">III )</definiendum>
				<definiens id="0">a wh phrase in COMP position c-commanding a governed e.c. , or another wh phrase in COMP position and with the same index as that other e.c. or phrase , is a well-formed annotated surface structure</definiens>
			</definition>
			<definition id="5">
				<sentence>( IV ) : Any of the structures defined by ( I ) - ( III ) , and , in addition , with one of those structures with an e.c. having an index the same as that of an element adjoined to VP ( following Baltin 1982 ) , and c-commanded and subjacent to that element , is a wellformed annotated surface structure/4 There can be at most one such adjoined position .</sentence>
				<definiendum id="0">IV )</definiendum>
			</definition>
			<definition id="6">
				<sentence>( V ) : Any of the structures defined by ( I ) - ( IV ) conjoined so as to meet Williams 's ( 1978 ) Across the Board ( ATB ) conventions is a well-formed annotated t3 Subject to constraints dictated by `` control '' theory , that is .</sentence>
				<definiendum id="0">V )</definiendum>
				<definiens id="0">a well-formed annotated t3 Subject to constraints dictated by `` control '' theory</definiens>
			</definition>
			<definition id="7">
				<sentence>Then a new f-structure e is constructed with e = { ( a , v ) I a • A 1 U A 2 and v = merge \ [ Locate I ( el , a ) \ ] , Locate \ [ ( e2 , a ) \ ] \ ] } ( Locate is an operator that actually finds the sub-f-structure with the specified attribute structure . )</sentence>
				<definiendum id="0">Locate</definiendum>
				<definiens id="0">an operator that actually finds the sub-f-structure with the specified attribute structure</definiens>
			</definition>
			<definition id="8">
				<sentence>Rounds , W. 1975 A Grammatical Characterization of the Exponential Time Languages .</sentence>
				<definiendum id="0">Grammatical Characterization</definiendum>
				<definiens id="0">of the Exponential Time Languages</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>1 The effect is one of raising the salience of these predicates in A. Thus , sentence ( 2 ) is generally understood as meaning that billboards are ugly , whereas in ( 3 ) , those predicates that have high salience for billboards ( but low salience in warts ) for example , prominence are attributed to warts ( that is , the salience is raised ) .</sentence>
				<definiendum id="0">effect</definiendum>
				<definiens id="0">one of raising the salience of these predicates in A. Thus</definiens>
			</definition>
			<definition id="1">
				<sentence>It seems probable that the B term in ( 1 ) A is like B represents the epitome of the predicate ( s ) that are true of A and of interest in a given utterance , that is , B is the prototypical representative of these predicates .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">the B term in ( 1 ) A is like B represents the epitome of the predicate ( s ) that are true of A and of interest in a given utterance</definiens>
				<definiens id="1">the prototypical representative of these predicates</definiens>
			</definition>
			<definition id="2">
				<sentence>( 19 ) Jane is a string bean , for example , makes a statement about Jane 's shape .</sentence>
				<definiendum id="0">Jane</definiendum>
				<definiens id="0">a string bean , for example , makes a statement about Jane 's shape</definiens>
			</definition>
			<definition id="3">
				<sentence>Ortony 's compactness thesis ( 1975 ) , which allows metaphors to cause the transfer of features or characteristics as a `` chunk '' from vehicle to topic , does not account for this type of metaphoric discourse .</sentence>
				<definiendum id="0">chunk</definiendum>
				<definiens id="0">allows metaphors to cause the transfer of features or characteristics as a ``</definiens>
			</definition>
			<definition id="4">
				<sentence>Temperature is the most salient feature of ICE for which John has a Role .</sentence>
				<definiendum id="0">Temperature</definiendum>
				<definiens id="0">the most salient feature of ICE for which John has a Role</definiens>
			</definition>
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>Let f ( x ) be a function , say , the recognition function for a language L. The most interesting results we could obtain regarding f would be a lower bound on the resources needed to compute f on a machine of a given architecture , say , avon Neumann computer or a parallel array of neurons .</sentence>
				<definiendum id="0">Let f ( x</definiendum>
				<definiens id="0">a lower bound on the resources needed to compute f on a machine of a given architecture , say , avon Neumann computer or a parallel array of neurons</definiens>
			</definition>
			<definition id="1">
				<sentence>Function f is said to be O ( g ) if a constant multiple of g is an upper bound for f , for all but a finite number of values of n. More precisely , f is O ( g ) if there is are constants K and n o such that for all n &gt; no , fin ) &lt; K * g ( n ) .</sentence>
				<definiendum id="0">Function f</definiendum>
			</definition>
			<definition id="2">
				<sentence>It does not say that all inputs require g ( n ) time , or Ruzzo machine that implements f. Also , if two algorithms A i and A 2 are available for a function f , and if their worstcase complexity can be given respectively as O ( g ) and O ( g ) , and gl &lt; g2 , it may still be true that for a large number of cases ( maybe even all those likely to be encountered in practice ) , A 2 will be the preferable algorithm simply because the constant K 1 for gl may be much larger than is K 2 for g2 '' A parsing-related example is given in Section 3. In examining known results pertaining to the recognition complexity of various theories , it is useful to consider how robust they are in the face of changes in the machine model from which they were derived. These models can be divided into two classes : sequential and parallel. Sequential models ( Aho et al. 1974 ) include the familiar singleand multitape Turing machines ( TM ) as well as random-access machines ( RAM ) and random-access stored-program machines ( RASP ) . A RAM is like a TM except that its working memory is random-access rather than sequential. A RASP is like a RAM but stores its program in its memory. Of all these models , the RASP is most like a yon Neumann computer. All these sequential models can simulate one another in ways that do not require great changes in time complexity. For example , a k-tape Turing Machine that runs in time O ( t ) can be simulated by a RAM in time O ( t log0 , conversely , a RAM running in O ( t ) can be simulated by a k-tape TM in time O ( t2 ) .. In fact , all the familiar sequential models are polynomially related : they can simulate one another with at most a polynomial loss in efficiency. 6 Thus , if a syntactic model is known to have a difficult recognition problem when implemented on one sequential model , execution of an equivalent algorithm on another sequential machine will not be much easier. Transforming a sequential algorithm to one on a parallel machine with a fixed number K of processors provides at most a factor K improvement in speed. More interesting results are obtained when the number of processors is allowed to grow with the size of the problem , e.g. , with the length of the string to be parsed. These processors can be viewed as connected together in a circuit , with inputs entering at one end and outputs being produced at the other. The depth of the circuit , or the maximum number of processors that data must be passed through from input to output , corresponds to the parallel time required to complete the computation. A problem that has a solution on a sequential machine in polynomial time and in space s will have a solution on a parallel machine with a polynomial number of processors and circuit depth ( and hence parallel time ) O ( s2 ) . This means that algorithms with sequential solutions requiring small space ( such as deterministic CSLs ) have fast parallel solutions. ity of O ( n logn ) and worst-case complexity of O ( n2 ) , using notation defined in the next paragraph. Similarly , let M be SPACE ( g ) if the worst-case space complexity of M is O ( g ) . their registers. These results assume that the cost of performing elementary operations on those numbers is proportional to their length , i.e. to their logarithm. 166 Computational Linguistics , Volume 10 , Numbers 3-4 , July-December 1984. C. Raymond Perrault On the Mathematical Properties of Linguistic Theories For a comprehensive survey of parallel computation , see Cook ( 1981 ) . Recognition techniques for context-free languages are well known ( Aho and Ullman 1972 ) . The so-called CKY or `` dynamic programming '' method is attributed by Hays ( 1962 ) to J. Cocke ; it was discovered independently by Kasami ( 1965 ) and Younger ( 1967 ) , who showed it to be O ( n3 ) . It requires the grammar to be in Chomsky Normal Form , and putting an arbitrary grammar in CNF may square its size. Berwick and Weinberg ( 1982 ) point out that , since the complexity of parsing algorithms is generally at least linearly dependent on the size of the grammar , this requirement may make CKY less than optimal for parsing short sentences. Earley 's algorithm recognizes strings in arbitrary CFGs in time O ( n 3 ) and space O ( n2 ) , and in time O ( n e ) for unambiguous CFGs. Graham , Harrison , and Ruzzo ( 1980 ) offer an algorithm that unifies CKY and Earley 's algorithm ( 1970 ) , and discuss implementation details. Valiant ( 1975 ) showed how to interpret the CKY algorithm as the finding of the transitive closure of a matrix and thus reduced CF recognition to matrix multiplication , for which subcubic algorithms exist. Because of the enormous constants of proportionality associated with this method , it is not likely to be of much practical use , either an implementation method or as a `` psychologically realistic '' model. Ruzzo ( 1979 ) has shown how CFLs can be recognized by Boolean circuits of depth O ( log ( n ) 2 ) , and therefore that parallel recognition can be accomplished in time O ( log ( n ) 2 ) . The required circuit size is polynomial in n. So as not to be mystified by the upper bounds on CF recognition , it is useful to remember that no known CFL requires more than linear time , nor is there even a nonconstructive proof of the existence of such a language. This is also a good place to recall the difference between recognition and parsing : if parsing requires that distinct structures be produced for all parses , it will be TIME ( 2n ) , since in some grammars sentences of length n may have 2 n parses ( Church and Patil 1982 ) . For an empirical comparison of various parsing methods , see Slocum ( 1981 ) . From its earliest days , discussions of transformational grammar ( TG ) have included consideration of matters computational. Peters and Ritchie ( 1973a ) provided some the first nontrivial results regarding the generative power of TGs. Their model reflects the Aspects version quite faithfully , including transformations that move and add constituents , and delete them subject to recoverability. All transformations are obligatory , and applied cyclically from the bottom up. They show that every r.e. set can be generated by applying a set of transformations to a contextsensitive base. The proof is quite simple : the right-hand sides of the type-0 rules that generate the r.e. set are padded with a new `` blank '' symbol to make them at least as long as their left-hand sides. Rules are added to allow the blank symbols to commute with all others. These context-sensitive rules are then used as the base of a TG whose only transformation deletes the blank symbols. Thus , if the transformational formalism itself is supposed to characterize the grammatical strings of possible natural languages , then the only languages being excluded by the formalism are those that are not enumerable under any model of computation. The characterization assumption is further discussed in Section 9. At the expense of a considerably more intricate argument , the previous result can be strengthened ( Peters and Ritchie 1971 ) to show that every r.e. set can be generated by a context-free based TG , as long as a filter an intersection with a regular set can be applied to the phrase-markers produced by the transformations. In fact , the base grammar can be independent of the language being generated. The proof involves the simulation of a TM by a TG. The transformations first generate an `` input tape '' for the TM being simulated , then apply the TM productions , one per cycle of the grammar. The filter ensures that the base grammar will generate just as many S nodes as necessary to generate the input string and do the simulation. In this case too , if the transformational formalism is supposed to characterize the possible natural languages , the universal base hypothesis ( Peters and Ritchie 1969 ) , according to which all natural languages can be generated from the same base grammar , is empirically vacuous : any recursively enumerable language can. Following Peters and Ritchie 's work , several attempts were made to find a restricted form of the transformational model that is descriptively adequate , yet whose generated languages are recursiVe ( see , for example , LaPointe 1977 ) . Since a key part of the proof in Peters and Ritchie ( 1971 ) involves the user of a filter on the final derivation trees , Peters and Ritchie ( 1973c ) examined the consequences of forbidding final filtering. They show that , if S is the recursive symbol in the CF base , the generated language L is predictably enumerable and exponentially bounded. A language L is predictably enumerable if there is an `` easily '' computable function t ( n ) that gives an upper bound on the number of tape squares needed by its enumerating TM to enumerate the first n elements of L. L is exponentially bounded if there is a constant K such that , for every string x in L , there is another string t . x m L whose length is at most K times the length of x. The class of nonfiltering languages is quite unusual , including all the CFLs ( obviously ) , but also properly intersecting the CSLs , the recursive languages , and the r.e. languages. The source of nonrecursivity in transformationally generated languages is that transformations can delete large parts of the tree , thus producing surface trees that are arbitrarily smaller than the deep structure trees they Computational Linguistics , Volume 10 , Numbers 3-4 , July-December 1984 167 C. Raymond Perrault On the Mathematical Properties of Linguistic Theories were derived from. This is what Chomsky 's `` recoverability of deletions '' condition was meant to avoid. In his thesis , Petrick ( 1965 ) defines the following condition on transformational derivations : a derivation satisfies the terminal-length-increasing condition if the length of the yield of any subtree u , resulting from the application of the transformational cycle to a subtree t , is greater than the length of the yield of any subtree u r resulting from the application of the cycle to a subtree t r of t. Petrick shows that , if all recursion in the base grammar `` passes through S '' and all derivations satisfy the terminal-length-increasing condition , then the generated language is recursive. Using a slightly more restricted model of transformations Rounds ( 1973 ) strengthens this result by showing that the resulting languages are in fact context-sensitive. In an unpublished paper , Myhill shows that , if Petrick 's condition is weakened to terminal-length-nondecreasing , the resulting languages can be recognized in space that is at most exponential in the length of the input. This implies that recognition can be done in at most double-exponential time , but Rounds ( 1975 ) proves that not only can recognition be done in exponential time , but that every language recognizable in exponential time can be generated by a TG satisfying the terminal-lengthnondecreasing condition and recoverability of deletions. This is a very strong result , because of the closure properties of the class of exponential-time languages &gt; To see why this is so requires a few more definitions .</sentence>
				<definiendum id="0">Ruzzo machine</definiendum>
				<definiendum id="1">RAM</definiendum>
				<definiendum id="2">RASP</definiendum>
				<definiendum id="3">base grammar</definiendum>
				<definiendum id="4">CFLs</definiendum>
				<definiens id="0">sequential and parallel. Sequential models ( Aho et al. 1974 ) include the familiar singleand multitape Turing machines ( TM ) as well as random-access machines</definiens>
				<definiens id="1">offer an algorithm that unifies CKY and Earley 's algorithm ( 1970 ) , and discuss implementation details. Valiant ( 1975 ) showed how to interpret the CKY algorithm as the finding of the transitive closure of a matrix and thus reduced CF recognition to matrix multiplication</definiens>
				<definiens id="2">a derivation satisfies the terminal-length-increasing condition if the length of the yield of any subtree u , resulting from the application of the transformational cycle to a subtree t</definiens>
			</definition>
			<definition id="3">
				<sentence>A TAG consists of two finite sets of finite trees , the centre trees and the adjunct trees .</sentence>
				<definiendum id="0">TAG</definiendum>
			</definition>
			<definition id="4">
				<sentence>SG postulates the existence of several coupled components , known as strata ; phonology , morphology , syntax , and semology are examples of linguistic strata .</sentence>
				<definiendum id="0">SG</definiendum>
				<definiens id="0">postulates the existence of several coupled components , known as strata ; phonology , morphology , syntax</definiens>
			</definition>
			<definition id="5">
				<sentence>Following Gleason 's model , Borgida ( 1983 ) defines the realization relation so that it couples the application of specific pairs of productions ( or sequences of productions ) in the different grammars .</sentence>
				<definiendum id="0">realization relation</definiendum>
				<definiens id="0">or sequences of productions ) in the different grammars</definiens>
			</definition>
			<definition id="6">
				<sentence>A two-level stratifieational grammar consists of two rewrite grammars G 1 and G 2 , called tactics , with sets of productions Pi and P2 , respectively , and a realization relation R , which is a finite set of pairs , each consisting of a string of productions of P1 and a string of productions of P2 .</sentence>
				<definiendum id="0">two-level stratifieational grammar</definiendum>
				<definiens id="0">consists of two rewrite grammars G 1 and G 2 , called tactics , with sets of productions Pi and P2 , respectively , and a realization relation R , which is a finite set of pairs , each consisting of a string of productions of P1 and a string of productions of P2</definiens>
			</definition>
			<definition id="7">
				<sentence>ve respectively , such that R ( u , ,v , ) , for all i from 1 to n. The language generated by a two-level SG is the set of string generated derivations in G 2 that realize derivations in Grextended to more than two strata .</sentence>
				<definiendum id="0">two-level SG</definiendum>
				<definiens id="0">the set of string generated derivations</definiens>
			</definition>
			<definition id="8">
				<sentence>In Linebarger 's syntactic analysis ( 1980 ) , the scope relation is defined on the logical forms of the government and binding theory ( GB ) : An item is in the immediate scope of NOT if ( 1 ) it occurs only in the proposition which is the entire scope of NOT and ( 2 ) within the proposition there are no logical elements intervening between it and NOT .</sentence>
				<definiendum id="0">GB )</definiendum>
				<definiens id="0">no logical elements intervening between it and NOT</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>The major problem with the computation of ~r is the design of the GENERATOR , that is , the choice of the sample of E to be used for the evaluation of the system U. The mathematically simplest case is the one where a subset B _ E is randomly generated on the basis of a given probability distribution in E ( for example , equiprobability ) ; then , qr B = ~\ ] ~ ( g ( e ) , ~ ( e ) ) • p ( e ) ecB p ( e ) eeB is a random variable such that E ( qrB ) = 7r for reasonable distributions , where E0rB ) denotes the expectation of qr B. The value of E ( qrB ) may be estimated by means of statistical techniques such as , for example , the maximum likelihood function .</sentence>
				<definiendum id="0">~r</definiendum>
				<definiendum id="1">eeB</definiendum>
				<definiens id="0">the design of the GENERATOR , that is , the choice of the sample of E to be used for the evaluation of the system U. The mathematically simplest case is the one where a subset B _ E is randomly generated on the basis of a given probability distribution in E ( for example</definiens>
			</definition>
			<definition id="1">
				<sentence>More precisely , Tennant introduces the two notions of coverage and completeness to denote , respectively , the capabilities ( both conceptual and linguistic ) that the designer has put within a system , and ( similarly to Woods , Kaplan , Nash-Webber 1972 though differing from Woods 1977 ) the degree to which the capabilities expected by a set of users can actually be found in the system coverage .</sentence>
				<definiendum id="0">Tennant</definiendum>
				<definiens id="0">introduces the two notions of coverage and completeness to denote , respectively , the capabilities ( both conceptual and linguistic ) that the designer has put within a system , and ( similarly to Woods , Kaplan , Nash-Webber 1972 though differing from Woods 1977 ) the degree to which the capabilities expected by a set of users can actually be found in the system coverage</definiens>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>Each dictionary listing for a word is made up of one or more meanings , where each meaning comprises ( a ) the word itself ; ( b ) its part of speech ; ( c ) the associated root word ; and 82 Computational Linguistics Volume 10 , Number 2 , April-June 1984 Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLP ( d ) more possible values .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLP ( d ) more possible values</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus we feel quite comfortable in writing , at the appropriate place ( s ) in a grammar , the command ( Quote ( whether if ) ) The Get command instructs the parser to find a word having one of a list of parts of speech .</sentence>
				<definiendum id="0">Quote</definiendum>
				<definiens id="0">instructs the parser to find a word having one of a list of parts of speech</definiens>
			</definition>
			<definition id="2">
				<sentence>The Alt command instructs the parser to perform exactly one of a set of commands , which it tries in the order they are given .</sentence>
				<definiendum id="0">Alt command</definiendum>
				<definiens id="0">instructs the parser to perform exactly one of a set of commands , which it tries in the order they are given</definiens>
			</definition>
			<definition id="3">
				<sentence>The simplest type of augmentation , which applies to any of the three basic commands ( i.e. , Quote , Get , Call ) , consists of a feature-value pair that supplies information on , and thus restricts the allowable values for , some `` feature '' of the current phrase .</sentence>
				<definiendum id="0">augmentation</definiendum>
				<definiens id="0">consists of a feature-value pair that supplies information on</definiens>
			</definition>
			<definition id="4">
				<sentence>Another use of feature-value pairs is to incorporate information into the phrase being processed that will be used to determine the acceptability of subsequent commands , as described in Section 3.3.4 .</sentence>
				<definiendum id="0">feature-value pairs</definiendum>
				<definiens id="0">to incorporate information into the phrase being processed that will be used to determine the acceptability of subsequent commands</definiens>
			</definition>
			<definition id="5">
				<sentence>This file gives information analogous to that provided by the First relation of conventional LL ( 1 ) compiler theory , and is created automatically by an off-line process that traverses a grammar ( see Section 6.1 ) .</sentence>
				<definiendum id="0">off-line process</definiendum>
				<definiens id="0">gives information analogous to that provided by the First relation of conventional LL ( 1 ) compiler theory</definiens>
			</definition>
			<definition id="6">
				<sentence>However , Prep takes the attached features into account , and must also account for multiple word meanings .</sentence>
				<definiendum id="0">Prep</definiendum>
				<definiens id="0">takes the attached features into account , and must also account for multiple word meanings</definiens>
			</definition>
</paper>

		<paper id="3003">
			<definition id="0">
				<sentence>In Higginbotham 's argument , it is crucial that English allows as a noun phrase any string of the form the N such that z where `` z is an ordinary English declarative sentence that contains an occurrence of a third-person pronoun that does not have to be taken as having its antecedent within z '' and `` N is any noun that agrees properly with the pronoun in number and gender '' .</sentence>
				<definiendum id="0">z</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">an ordinary English declarative sentence that contains an occurrence of a third-person pronoun that does not have to be taken</definiens>
			</definition>
</paper>

		<paper id="2002">
			<definition id="0">
				<sentence>S : that is nearest to the top , or nearest to the red handle .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">that is nearest to the top , or nearest to the red handle</definiens>
			</definition>
			<definition id="1">
				<sentence>However , Fertig ( unpublished ) found no significant differences in the frequency of miscommunication across modes .</sentence>
				<definiendum id="0">Fertig ( unpublished )</definiendum>
				<definiens id="0">found no significant differences in the frequency of miscommunication across modes</definiens>
			</definition>
			<definition id="2">
				<sentence>Modality differences in Grosz ' ( 1977 ) study can not be directly compared for the identification phenomena because the core dialogues that were analyzed in depth each employed both spoken and keyboard modalities .</sentence>
				<definiendum id="0">Modality differences</definiendum>
				<definiens id="0">the identification phenomena because the core dialogues that were analyzed in depth each employed both spoken and keyboard modalities</definiens>
			</definition>
			<definition id="3">
				<sentence>Other categories of utterance forms for identification requests include what are `` nearly direct '' requests ( i.e. , imperatives and utterances that explicitly mention searching for an object ) and what are termed `` Let 's requests '' , which explicitly change the focus of attention to an object satisfying the description .</sentence>
				<definiendum id="0">direct '' requests</definiendum>
				<definiens id="0">imperatives and utterances that explicitly mention searching for an object</definiens>
			</definition>
			<definition id="4">
				<sentence>• Precondition-action : If the agent is thought to want some proposition P to be true , and P is the precondition of an action known to the agent , consider that the agent 's goal is to perform that action .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the precondition of an action known to the agent , consider that the agent 's goal is to perform that action</definiens>
			</definition>
			<definition id="5">
				<sentence>The effect of an S-REQUESTS to do action ACT is simply that the hearer believes the speaker wants the hearer to do ACT .</sentence>
				<definiendum id="0">effect of an S-REQUESTS</definiendum>
			</definition>
			<definition id="6">
				<sentence>Now , intended plan recognition involves deriving new formulas of the form HBSW ( HBSW ( G ) ) , i.e. , the Hearer Believes the Speaker Wants him to think the speaker 's goal is G. Any goals G ' derived with the prefix HBSW ( HBSW ) are taken to have been communicated ( in the Gricean sense ) .</sentence>
				<definiendum id="0">Hearer Believes</definiendum>
				<definiens id="0">the Speaker Wants him to think the speaker 's goal is G. Any goals G ' derived with the prefix HBSW ( HBSW ) are taken to have been communicated</definiens>
			</definition>
			<definition id="7">
				<sentence>Z3 , 24 Plan recognition can also suggest how Class B utterances all convey requests for referent identification .</sentence>
				<definiendum id="0">Class B</definiendum>
				<definiens id="0">utterances all convey requests for referent identification</definiens>
			</definition>
</paper>

		<paper id="3012">
</paper>

		<paper id="3002">
			<definition id="0">
				<sentence>In ( 3 ) , as in most of the examples in the literature previously illustrating the sluicing construction , the indefinite phrase in the first clause is an indefinite pronoun , and WH is a wh-pronoun .</sentence>
				<definiendum id="0">indefinite phrase</definiendum>
				<definiendum id="1">WH</definiendum>
				<definiens id="0">a wh-pronoun</definiens>
			</definition>
			<definition id="1">
				<sentence>Then it is possible to represent all the relevant cases schematically as follows : strings ; QI is an indefinite quantifier or pronoun ; Xl is the rest of the nominal quantified by Q1 ; Q2 is a wh-quantifier or pronoun anaphorically related to Q1 ; and X2 is the rest of the nominal quantified by Q2 .</sentence>
				<definiendum id="0">QI</definiendum>
				<definiendum id="1">Xl</definiendum>
				<definiendum id="2">Q2</definiendum>
				<definiendum id="3">X2</definiendum>
				<definiens id="0">an indefinite quantifier or pronoun</definiens>
				<definiens id="1">the rest of the nominal quantified by Q2</definiens>
			</definition>
			<definition id="2">
				<sentence>EXAMPLE Q1 X1 Q2 X2 ( 8a ) some proposals how many 4 ' ( 8b ) any books which ones ( 8c ) several typewriters how many machines ( 8d ) a few physicians how many doctors ( 8e ) certain formulas which formulas does not denote the same people that doctors does ( the latter perhaps referring to nonmedical doctors ) ; and in ( 8e ) the two occurrences of formulas then denote different things ( say , mathematical formulas in the first instance and baby milk formulas in the second ) .</sentence>
				<definiendum id="0">EXAMPLE Q1 X1 Q2 X2</definiendum>
				<definiens id="0">some proposals how many 4 ' ( 8b ) any books which ones ( 8c ) several typewriters how many machines ( 8d ) a few physicians how many doctors ( 8e ) certain formulas which formulas does not denote the same people that doctors does ( the latter perhaps referring to nonmedical doctors ) ; and in ( 8e ) the two occurrences of formulas then denote different things</definiens>
			</definition>
			<definition id="3">
				<sentence>X2 is a possible anaphor of X1 .</sentence>
				<definiendum id="0">X2</definiendum>
				<definiens id="0">a possible anaphor of X1</definiens>
			</definition>
			<definition id="4">
				<sentence>However , escape can not be used as an anaphor for fire escape , even in E2 , since a fire escape , which is a certain kind of physical object , is not an escape , which is a certain kind of event ; that is , fire escape is an exocentric compound .</sentence>
				<definiendum id="0">escape</definiendum>
				<definiens id="0">an exocentric compound</definiens>
			</definition>
			<definition id="5">
				<sentence>A copying language is any language of the form : L { cxdxe \ ] x E ( a , b ) * and a , b , c , d , e are fixed strings } Given that English contains a sluicing construction characterized as in section 1 , one can prove that FWF ( E ) , E ranging over E1 and E2 , is not CF by means of the Intersection Theorem of ( 2 ) and the fact that copying languages are not CF ( Langendoen 1977 ) .</sentence>
				<definiendum id="0">copying language</definiendum>
				<definiens id="0">any language of the form : L { cxdxe \ ] x E ( a , b ) * and a , b , c , d , e are fixed strings } Given that English contains a sluicing construction characterized as in section 1 , one can prove that FWF ( E ) , E ranging over E1 and E2</definiens>
			</definition>
			<definition id="6">
				<sentence>R = { Joe discussed some bourbon x but WHICH bourbon y is unknown t x , y E ( hater , lover ) * } Since R is a concatenation of regular languages , it is itself regular .</sentence>
				<definiendum id="0">y E</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">a concatenation of regular languages , it is itself regular</definiens>
			</definition>
			<definition id="7">
				<sentence>180 Computational Linguistics , Volume 10 , Numbers 3-4 , July-December 1984 Paul M. Postal and D. Terence Langendoen English and the Class of Context-Free Languages But as Langendoen and Postal ( 1984 ) shows , NLs are proper classes not sets , so the question of WF ( L ) s as wholes being CF no longer arises .</sentence>
				<definiendum id="0">NLs</definiendum>
				<definiens id="0">proper classes not sets , so the question of WF ( L ) s as wholes being CF no longer arises</definiens>
			</definition>
			<definition id="8">
				<sentence>Restricting attention to FWF ( L ) s , the result of Section 2 shows that for any dialect E of English for which MC holds , FWF ( E ) is not CF. Since , however , neither FWF ( E ) nor any other FWF ( L ) has been shown to lie outside the domain of indexed languages ( ILs ) in the sense of Aho ( 1968 ) , it would appear that one can conclude that while the collection of all sentences in an NL K is a proper class , FWF ( K ) is an IL .</sentence>
				<definiendum id="0">FWF</definiendum>
				<definiendum id="1">NL K</definiendum>
				<definiendum id="2">FWF ( K )</definiendum>
				<definiens id="0">any dialect E of English for which MC holds , FWF ( E</definiens>
				<definiens id="1">a proper class</definiens>
			</definition>
</paper>

	</volume>
