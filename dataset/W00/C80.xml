<?xml version="1.0" encoding="UTF-8"?>
	<volume id="C80">

		<paper id="1051">
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>Sato uses in turn the semantic structures as a source to generate Japanese sentences\ [ ll\ ] .</sentence>
				<definiendum id="0">Sato</definiendum>
			</definition>
</paper>

		<paper id="1052">
</paper>

		<paper id="1083">
			<definition id="0">
				<sentence>The analyser , which is activated by the monitor when running in the U-mode , accepts the natural language query in input and generates an appropriate internal representation or , if some step of the parsing fails , a diagnostlc message to be displayed to the user .</sentence>
				<definiendum id="0">analyser</definiendum>
				<definiens id="0">generates an appropriate internal representation or , if some step of the parsing fails , a diagnostlc message to be displayed to the user</definiens>
			</definition>
			<definition id="1">
				<sentence>The generator receives in input the internal representation produced by the analyser and translates it into the formal query language QBE .</sentence>
				<definiendum id="0">generator</definiendum>
				<definiens id="0">receives in input the internal representation produced by the analyser and translates it into the formal query language QBE</definiens>
			</definition>
			<definition id="2">
				<sentence>M ~ where : P denotes a word or a simple costruct ; 0 denotes that the record is of type object ; each pair N~ .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a word or a simple costruct ; 0 denotes that the record is of type object</definiens>
			</definition>
			<definition id="3">
				<sentence>M~ denotes that the wo~d P refers to the domain IN~ in the relation M~ ( relations and domains are ~epresented by positive inte gers ) ; the = symbol separates equal domains belonging to different relations ; if P refers to a relation without specifying the domain , N~ is replaced by the special symbol $ ; each f~eld of the record contains a different meaning of the object P. The structure of a connective record is : where : P denotes a word or a simple costruct ; C denotes that the record is of type connective ; each pair X. : Y. denotes a possible meaning 1 .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">a word or a simple costruct ;</definiens>
			</definition>
			<definition id="4">
				<sentence>1 of the connectlve P ; namely , X : represents the I pattern in which P may appear ( position of P and type of the adjacent words ) , Y : denotes ( through I a pointer ) the function which must be applied or the action which must be performed during the parsing to take correctly into account the meaning of the connective P. The structure of a function record is : where : P denotes a word or a simple construct ; F denotes that the record is of type function ; each Z. denotes a possible meaning of the function ~ and represents ( through a pointer ) an action which must be performed in the parsing .</sentence>
				<definiendum id="0">X :</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiendum id="2">P</definiendum>
				<definiendum id="3">F</definiendum>
				<definiendum id="4">Z.</definiendum>
				<definiens id="0">represents the I pattern in which P may appear ( position of P and type of the adjacent words</definiens>
				<definiens id="1">denotes ( through I a pointer ) the function which must be applied or the action</definiens>
				<definiens id="2">a word or a simple construct ;</definiens>
				<definiens id="3">a possible meaning of the function</definiens>
			</definition>
			<definition id="5">
				<sentence>we get : &lt; VENDITE ( REPARTO : A , ARTICOLO : B ) &gt; &lt; FORNITORI ( ARTICOLO : B FORNITORE : '' PARKER '' ) &gt; and , after step 4. , the correct formal representation in QBE results : VENDITE ( REPARTO : P , ARTICOLO : B ) FORNITORI ( ARTICOLO : B , FORNITORE : `` PARKER '' ) PV is a pointer to a routine which puts the special symbol P in the domain which follows the function TR~A. PA is a pointer to a routine which assigns X ; to domain 2 of relation 2 ( in the case VENDONO ) , or to domain 2 of the relation 3 ( in the case FORNITI DA ) ; PA is activated if the connective to which it is bound connects an object ( 0 ) to a particular value of a domain ( V ) , as it arrives in both cases VENDONO and FORNITI DA .</sentence>
				<definiendum id="0">PV</definiendum>
				<definiens id="0">A , ARTICOLO : B ) &gt; &lt; FORNITORI ( ARTICOLO : B FORNITORE : '' PARKER ''</definiens>
				<definiens id="1">a pointer to a routine which puts the special symbol P in the domain which follows the function TR~A. PA is a pointer to a routine which assigns X</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>SABA has been fundamentally conceived as a general , flexible French language parser , which could be used as a component of a natural language interface between a human user and a given computer process 8 .</sentence>
				<definiendum id="0">SABA</definiendum>
			</definition>
			<definition id="1">
				<sentence>( WITH ( T-list ( INSTRUMENT , ... ) ) ) ( EATING ( E-iist ( AGENT ( Animate ) , OBJECT ( Comestible ) , INSrRUMENT ( Instrument ) ) ) ) the system can easily parse the sentence `` Peter eats an apple with a knife '' and produce the three relationships AGENT ( eating , Peter ) OBJECT ( eating , apple ) INSTRUMENT ( eating , knive ) Other kinds of restrictions are based on syntactic classes or on modal properties ( of verbs ) .</sentence>
				<definiendum id="0">OBJECT</definiendum>
				<definiendum id="1">apple ) INSTRUMENT</definiendum>
			</definition>
			<definition id="2">
				<sentence>A Clause consists of one and only one verb , and of a set of Groups .</sentence>
				<definiendum id="0">Clause</definiendum>
				<definiens id="0">consists of one and only one verb , and of a set of Groups</definiens>
			</definition>
			<definition id="3">
				<sentence>A Sentence is a sequence of Words delimited by a `` terminator '' ( punctuation mark like a period , a question-mark , ... ) A Sentence contains a set of Clauses .</sentence>
				<definiendum id="0">Sentence</definiendum>
				<definiendum id="1">Sentence</definiendum>
				<definiens id="0">a sequence of Words delimited by a `` terminator ''</definiens>
			</definition>
			<definition id="4">
				<sentence>b ) The dogs that you love and who love you in return become precious comrades c ) ( ( SB ) ( LES ART ) ( CHIENS NOM ) ( AUXQUELS PR-REL ) ( VOUS PR-PERS NIL ( OD OI PR S ) ) ( VOUS PR-PERS NIL ( OD OI S ) ) ( ATTACHEZ VERBE IND ) ( ET CC ) ( QUI PR-REL ) ( VOUS PR-PERS NIL ( OD OI PR S ) ) ( RENDENT VERBE IND ) ( DE PREP ) ( L ART ) ( AFFECTION NOM ) ( DEVIENNENT VERBE IND ) ( D ART ) ( INESTIMABLES ADJ ) ( COMPAGNONS NOM ) ( SB ) ) d ) ( ( SB ) ( LES ART ) ( CHIENS NOM ) ( PR ) ( ET CC ) ( QUI PR-REL ) ( VOUS PR-PERS NIL ( OD OI PR S ) ) ( RENDENT VERBE IND ) ( DE PREP ) ( L ART ) ( AFFECTION NOM ) ( DEVIENNENT VERBE IN ) ( D ART ) ( INESTIMABLES ADJ ) ( COMPAGNONS NOM ) ( SB ) ) e ) ( ( SB ) ( LES ART ) ( CHIENS NOM ) ( PR ) ( ET CC ) ( P.R ) ( DEVIENNENT VERBE IND ) ( D PREP ) ( INESTIMABLES ADJ ) ( COMPAGNONS NOM ) ( SB ) ) f ) ( ( SB ) ( PP ) ( SB ) ) Figure 3 : segmentation of a sentence a ) the original French sentence b ) the English translation c ) the input of the segmentation procedure d ) the state of the sentence after the analysis of the relative clause `` auxquels vous vous attachez '' , which is replaced by the special symbol PR e ) the state of the sentence after the analysis of the relative clause `` qui vous rendent de l'affection '' , which is replaced by PR f ) final state of the sentence : the main clause was found and replaced by PP concerning the order for parsing the internal units at a given level , two strategies are applied , one for clauses , and one for groups .</sentence>
				<definiendum id="0">AFFECTION NOM )</definiendum>
				<definiendum id="1">INESTIMABLES ADJ ) ( COMPAGNONS NOM )</definiendum>
				<definiendum id="2">LES ART )</definiendum>
				<definiens id="0">COMPAGNONS NOM ) ( SB ) ) f ) ( ( SB ) ( PP ) ( SB ) ) Figure 3 : segmentation of a sentence a ) the original French sentence b</definiens>
			</definition>
			<definition id="5">
				<sentence>Pratt , A.W. , Progress towards a medical information system for the research environment , in Fuchs , G. &amp; Wagner , G. ( Eds ) , Krankenhaus-Informationssysteme : Erstrebtes und erreichtes , F.K.Schattauer Verlag , Stuttgart , 1972 .</sentence>
				<definiendum id="0">Progress</definiendum>
				<definiens id="0">towards a medical information system for the research environment</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>The Coding Scheme for Inputting In this scheme , a code for a Chinese character consists of two strings of symbols concatenated together .</sentence>
				<definiendum id="0">Coding Scheme</definiendum>
				<definiendum id="1">Chinese character</definiendum>
				<definiens id="0">consists of two strings of symbols concatenated together</definiens>
			</definition>
			<definition id="1">
				<sentence>The radical string for a Chinese character consists of the keys corresponding to the first three radicals composing the character .</sentence>
				<definiendum id="0">radical string</definiendum>
				<definiendum id="1">Chinese character</definiendum>
				<definiens id="0">consists of the keys corresponding to the first three radicals composing the character</definiens>
			</definition>
</paper>

		<paper id="1035">
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>generation of an argumentative answer in such a context falls outside the usual scope of linguistic analysis ; it is a good example of an application of the AI paradigm in that the condition which gives rise to the generation of an argumentative answer is a certain property of a cognitive process , namely the inference process by which the answer is derived .</sentence>
				<definiendum id="0">argumentative answer</definiendum>
				<definiens id="0">a certain property of a cognitive process , namely the inference process by which the answer is derived</definiens>
			</definition>
			<definition id="1">
				<sentence>Chicago : Chicago Linguistic Society , p. 208-219 \ [ 6\ ] v. Hahn , W. , Hoeppner , W. , Jameson , A. , Wahlster , W. ( 1980 ) : The anatomy of the natural language dialogue system HAM-RPM .</sentence>
				<definiendum id="0">Chicago</definiendum>
				<definiens id="0">Chicago Linguistic Society , p. 208-219 \ [ 6\ ] v. Hahn , W. , Hoeppner , W. , Jameson , A. , Wahlster</definiens>
				<definiens id="1">The anatomy of the natural language dialogue system HAM-RPM</definiens>
			</definition>
			<definition id="2">
				<sentence>Munich : Hanser/Macmillan \ [ 7\ ] Hart , P.E. , Duda , R.O. ( 1977 ) : PROSPECTOR A computer-based consultation system for mineral exploration .</sentence>
				<definiendum id="0">Munich</definiendum>
				<definiens id="0">PROSPECTOR A computer-based consultation system for mineral exploration</definiens>
			</definition>
			<definition id="3">
				<sentence>Czech Academy of Science , Prague \ [ 18\ ] Tou\ ] min , S. ( 1969 ) : The uses of argument , Cambridge : Univ .</sentence>
				<definiendum id="0">Cambridge</definiendum>
				<definiens id="0">The uses of argument ,</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>In Japanese sentences , BUNSETSUs ( Bphrase ) are the minimal morphological units of case dependency , and the syntax of Japanese sentences consists of ( i ) the syntax of B-phrase as a string of words , and ( 2 ) the syntax of a sentence as a string of B-phrases .</sentence>
				<definiendum id="0">BUNSETSUs</definiendum>
				<definiens id="0">the minimal morphological units of case dependency</definiens>
				<definiens id="1">a string of words , and ( 2 ) the syntax of a sentence as a string of B-phrases</definiens>
			</definition>
			<definition id="1">
				<sentence>A B-phrase usually pronounced without pausing consists of two parts -- main part \ [ or equally an independent part in the conventional school grammatical term\ ] and an annex part which is post positioned .</sentence>
				<definiendum id="0">B-phrase</definiendum>
			</definition>
			<definition id="2">
				<sentence>• • w~ Cont ( Hk , Kk , Hk+1 ) ( 0=k &lt; i ) ... ( i ) Termi ( H£ , K Z ) • • • ( 2 ) where ( Wi , Hi , Ki ) is the word structure of w i ( 0~i~£ ) , Cont ( Hk , Kk , Hk+1 ) means a word whose part of speech and inflexion are Hk , K k respectively can be followed by a word whose part of speech is Hk+lin -15-B-phrases and Termi ( HQ , Kz ) means a word whose part of speech ~nd inflexion are H£ , KZ respectively can be a right-most subword of B-phrases. ( i ) , ( 2 ) are called the rules of Bphrase structure , and ( W0 , H0 , K 0 ) ( Wi , HI , K ~ ) `` '' ( Wz , H~ , K ~ ) • .. ( 3 ) is called B-phrase structure of b. If ( 3 ) satisfies the condition ( i ) , w0wlw • ..w Z is called to be a left partial 2 B-phrase. The kakariuke relation is the dependency relation between two B-phrases in a sentence. A B-phrase has the syntactic functions of governor and dependent. The function of governor is mainly represented by the independent word of B-phrase. The function of dependent is mainly represented by the string of particles which is the rightmost substring of B-phrase and by the word in front of it ( right-most nonparticle word ) . Every particle has the syntactic and partially semantic dependent function with its own degree of power. The particle whose power of dependent function is strongest of all particles appearing in the string of particles is called the representative particle. Therefore , the syntactic function of dependent of a B-phrase is mainly represented by the representative particle and by the right-most nonparticle word. Let ( W0 , H0 , K0 ) , ( Wi , Hi , Ki ) , ( W~ , H~ , K~ ) be the word structures of independent J word , right-most non-particle word and representative particle of a B-phrase , respectively. Then , &lt; W^ , H^ &gt; _ , &lt; W.. , Hi , u u ~ &amp; Hj &gt; d are called the inrormatlon or governor and the information of dependent of the B-phrase respectively , and the pair ( &lt; W0 , H0 &gt; ~ , &lt; Wi , Hi , Hj &gt; d ) is called dependency~informati6n of the B-phrase .</sentence>
				<definiendum id="0">KZ respectively</definiendum>
				<definiendum id="1">kakariuke relation</definiendum>
				<definiens id="0">means a word whose part of speech and inflexion are Hk , K k respectively can be followed by a word whose part of speech is Hk+lin -15-B-phrases</definiens>
				<definiens id="1">a word whose part of speech ~nd inflexion are H£ ,</definiens>
				<definiens id="2">the dependency relation between two B-phrases in a sentence. A B-phrase has the syntactic functions of governor</definiens>
				<definiens id="3">the rightmost substring of B-phrase and by the word in front of it ( right-most nonparticle word ) . Every particle has the syntactic and partially semantic dependent function with its own degree of power. The particle whose power of dependent function is strongest of all particles appearing in the string of particles</definiens>
				<definiens id="4">the word structures of independent J word , right-most non-particle word and representative particle of a B-phrase</definiens>
			</definition>
			<definition id="3">
				<sentence>( Uniqueness of Dependent ) 3 ) If ( i , j , a~ , a~ , c ) 6 DS ( £ , m ) and , , ~ o ( j , k , ~j , ~k , C ) % DS ( Z , m ) , then ~ = ~ ~ , O J '' ( Uniqueness of B-phrase structure ) , 4 ) If ( i , J , ~i , ~j , c ) ~ DS ( £ , m ) , ( i , j , ~f , ~j , , c ) E DS ( £ , m ) and i &gt; i ' &gt; j , then j , hj .</sentence>
				<definiendum id="0">) E DS</definiendum>
				<definiens id="0">B-phrase structure ) , 4 ) If ( i , J , ~i , ~j , c ) ~ DS ( £ , m ) , ( i , j , ~f , ~j , , c</definiens>
			</definition>
			<definition id="4">
				<sentence>DS ( £ , m/i , j ) a subset of DS ( Z , m ) , is defined as follows : DS ( £ , m/i , j ) -~ { ( p , q , av , ~o , c ) I ( p , q , ap , aq , C ) ~ DS ( ~ , my , i~p &gt; q~j } .</sentence>
				<definiendum id="0">DS</definiendum>
				<definiens id="0">a subset of DS ( Z , m ) , is defined as follows : DS ( £ , m/i , j ) -~ { ( p , q , av , ~o , c ) I ( p , q , ap , aq</definiens>
			</definition>
			<definition id="5">
				<sentence>AND ITS EFFICIENCY In this chapter , we shall give a parsing method which will parse an input sentence using time O ( n ~ ) and space O ( n~ ) , where n is the length of input sentence .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of input sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>The parsing algorithm consists of four minor algorithms that are the construction of BL , the obtaining of B-phrase structure , the construction of DL and the obtaining of dependency structure .</sentence>
				<definiendum id="0">parsing algorithm</definiendum>
				<definiens id="0">consists of four minor algorithms that are the construction of BL , the obtaining of B-phrase structure , the construction of DL and the obtaining of dependency structure</definiens>
			</definition>
			<definition id="7">
				<sentence>The B-phrase parse list of b consists of n minor lists BL ( 1 ) , BL ( 2 ) , • .</sentence>
				<definiendum id="0">B-phrase parse list of b</definiendum>
				<definiens id="0">consists of n minor lists BL ( 1 )</definiens>
			</definition>
			<definition id="8">
				<sentence>\ [ \ ] Form of items in BL ( j ) ( i , WS , DI ) where , IL_i &lt; j~n , WS is a word structure and DI is a dependency information .</sentence>
				<definiendum id="0">WS</definiendum>
				<definiendum id="1">DI</definiendum>
				<definiens id="0">a word structure</definiens>
				<definiens id="1">a dependency information</definiens>
			</definition>
			<definition id="9">
				<sentence>• w~_ I , b ( i+l ) b ( i+2 ) ... b ( j ) =w£ , and WS is the word structure of w£ .</sentence>
				<definiendum id="0">WS</definiendum>
			</definition>
			<definition id="10">
				<sentence>Form of items in PDSL ( i , j , a~ , a~ , P # ) where , Nhi ~j ~i a # ~ DI ( i ) ~ { # } , ~ % DI ( j ) U { ~ , P~ i~ a subset of C or # Oand # is specially introduced symbol .</sentence>
				<definiendum id="0">P~</definiendum>
				<definiens id="0">i~ a subset of C or # Oand # is specially introduced symbol</definiens>
			</definition>
			<definition id="11">
				<sentence>A Japanese sentence in colloquial style is parsed b Y the parsing algorithm , using time O ( n ~ ) and memory space O ( n2 ) , where n is the length of input sentence .</sentence>
				<definiendum id="0">Japanese sentence</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the length of input sentence</definiens>
			</definition>
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>EXAM is an experimental text understanding system designed to deal with ambiguities effectively and efficiently .</sentence>
				<definiendum id="0">EXAM</definiendum>
				<definiens id="0">an experimental text understanding system designed to deal with ambiguities effectively and efficiently</definiens>
			</definition>
			<definition id="1">
				<sentence>EXAM consists of three components : hierarchical knowledge sources , a semantic interpreter and a fundamentally breadthfirst augmented context-free parser .</sentence>
				<definiendum id="0">EXAM</definiendum>
				<definiens id="0">consists of three components : hierarchical knowledge sources , a semantic interpreter and a fundamentally breadthfirst augmented context-free parser</definiens>
			</definition>
			<definition id="2">
				<sentence>GIRL provides a hierarchy with several modes of property inheritance for PROTO-units , and uses this hierarchy to instantiate them in the semantic interpretation .</sentence>
				<definiendum id="0">GIRL</definiendum>
				<definiens id="0">provides a hierarchy with several modes of property inheritance for PROTO-units , and uses this hierarchy to instantiate them in the semantic interpretation</definiens>
			</definition>
			<definition id="3">
				<sentence>Therefore , EXAM delays the disambiguation until after the entire sentence has been processed .</sentence>
				<definiendum id="0">EXAM</definiendum>
				<definiens id="0">delays the disambiguation until after the entire sentence has been processed</definiens>
			</definition>
			<definition id="4">
				<sentence>EXAM collects the requirements of the referent from the interpretation of the comlete sentence .</sentence>
				<definiendum id="0">EXAM</definiendum>
				<definiens id="0">collects the requirements of the referent from the interpretation of the comlete sentence</definiens>
			</definition>
			<definition id="5">
				<sentence>( In fact , the second partial parsed tree is plausible only when the sentence is of a form such as : Taroo no , inu wa ... ) Therefore , EXAM eliminates unnecessary parsing trees in accordance with the result of the semantic interpretation , that is , `` likelihood '' .</sentence>
				<definiendum id="0">EXAM</definiendum>
				<definiens id="0">unnecessary parsing trees in accordance with the result of the semantic interpretation</definiens>
			</definition>
			<definition id="6">
				<sentence>First , EXAM attempts to recognize how sentences are related .</sentence>
				<definiendum id="0">EXAM</definiendum>
				<definiens id="0">attempts to recognize how sentences are related</definiens>
			</definition>
			<definition id="7">
				<sentence>In this case , mizu no naka 'in water ' and tokoro 'place ' correspond to one another , and EXAM discovers the omitted element ( kaeru ga un-da ) tamago 'eggs ( which are layed by frogs ) ' So far , we have discussed the question of dealing with ambiguities while parsing .</sentence>
				<definiendum id="0">EXAM</definiendum>
				<definiens id="0">discovers the omitted element ( kaeru ga un-da ) tamago 'eggs ( which are layed by frogs</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>For the model we consider three levels ; that is , i. the extra-linguistic ( real world ) level , Information on extra-linguistic situations observed by the speaker is stored in the extra-linguistic level in the form of the situation and its existing period of time .</sentence>
				<definiendum id="0">extra-linguistic</definiendum>
				<definiens id="0">extra-linguistic situations observed by the speaker is stored in the extra-linguistic level in the form of the situation</definiens>
			</definition>
			<definition id="1">
				<sentence>The extra-linguistic level stores information on situations in the following form ( Nos. 2-9 of the appended program ) .</sentence>
				<definiendum id="0">extra-linguistic level</definiendum>
				<definiens id="0">stores information on situations in the following form</definiens>
			</definition>
			<definition id="2">
				<sentence>SITUATION START-DS END-DS END-SS SPEAK ( I ) 10030930 10031000 READ ( I , A-BOOK ) 10021400 10021630 COME ( JOHN , HERE ) I0021130 10021200 10021400 GO-ON ( A-LIGHT ) 10021900 10021900 10022359 CALM ( IT , HERE ) 10012000 10012000 10030500 The information includes situation in the form of verbal expressions and their arguments , the starting time of dynamic situations , the ending time of dynamic situations , and the ending time of static situations .</sentence>
				<definiendum id="0">SITUATION START-DS END-DS END-SS SPEAK</definiendum>
				<definiendum id="1">READ</definiendum>
				<definiendum id="2">GO-ON</definiendum>
				<definiens id="0">The information includes situation in the form of verbal expressions</definiens>
			</definition>
			<definition id="3">
				<sentence>-- -- -- , -- -- -- , -- -- -- , -- -- , S , E PERFECTIVE PERFECTIVE ( INGRESSIVE ) NEGATIVE NEGATIVE PERFECTIVE IMPER FECTIVE PERFECTIVE PERFECTIVE ( INGRESSIVE ) NEGATIVE NEGATIVE PERFECTIVE ( COMPLETED ) 104-u E A __ -- -- -- , -- -- -- , -- -- -- , -- -- -- , -- -- ' S E A B S E A B S E A B S E A B mleJ~JOJloe~lJeiJioJi~ S E A S E A B S E A B S E A S E A B S E A , B E A B StE AtB S E A B S , E A B C S E A B C S E A B C A B C S , E A B C S E A B C S E A B C -- - , -- -- -- , -- -- -- , -- -- -- , .</sentence>
				<definiendum id="0">E PERFECTIVE PERFECTIVE</definiendum>
				<definiens id="0">A __ -- -- -- , -- -- -- , -- -- -- , -- -- -- , -- -- ' S E A B S E A B S E A B S E A B mleJ~JOJloe~lJeiJioJi~ S E A S E A B S E A B S E A S E A B S E A , B E A B StE AtB S E A B S , E A B C S E A B C S E A B C A B C S , E A B C S E A B C S E A B C -- - , -- -- -- , -- -- -- , -- -- -- ,</definiens>
			</definition>
			<definition id="4">
				<sentence>P = ASP = ASP = ASP = ASP = NFGATIVE ' IMPERFECTIVE ' PEFECTIVE ( COMPLETED ) ' PERFECTIVE ' PERFECTIVE ( INGRESSIVE ) ' STATIVE ' STATIVE ( COMPLETED ) ' *** JAPA~ESE TENSE ASSIGNMENT T h T1 T2 T3 PRE PST FUT TA '4 , END : F ( DCHKI ) : S ( DCHK~ ) • RE : S ( ELDATA ) : ( ELDATA ) : S ( SDATA ) : S ( FDATA ) MNT : ; ( END ) : S ( FDATA ) : S ( ABC ) : F ( KADD ) S ( FDATA ) : F ( PO ) : F ( DI ) : ( ADJST ) : F ( D2 ) : ( ADJST ) : ( ADJST ) : F ( SE ) : S ( SO ) : F ( AGO ) F ( CI ) S ( NEG ) S ( CI ) F ( $ 5 ) S ( CI ) S ( STI ) F ( STc ) S ( ~EG ) S ( NEG ) F ( C5 ) S~ F ) F ( PEF ) F C~ ) - ) S ( PEF ) F ( PEFI ) : F ( CQ ) : S ( PEFI ) F ( II ' , P ) : S ( PEF ) F ( PEF2 ) : ( Th ) : ( TN ) : ( TN ) : ( TN ) : ( TN ) : TN ) : tTN ) E~ ( S , F ) : F ( TT ) E~w ( S , .</sentence>
				<definiendum id="0">STATIVE</definiendum>
				<definiendum id="1">~EG ) S ( NEG ) F ( C5 ) S~ F ) F ( PEF</definiendum>
				<definiens id="0">F ( DCHKI ) : S ( DCHK~ ) • RE : S ( ELDATA ) : ( ELDATA ) : S ( SDATA ) : S ( FDATA ) MNT : ; ( END ) : S ( FDATA ) : S ( ABC ) : F ( KADD ) S ( FDATA ) : F ( PO ) : F ( DI ) : ( ADJST ) : F ( D2 ) : ( ADJST ) : ( ADJST ) : F ( SE ) : S ( SO ) : F ( AGO ) F ( CI ) S</definiens>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>The Flexible Run OFF ( FROFF ) system is an integrated system that is designed for setting not only ordinary texts but also figures or tables indispensable to our documents , and utilizes a dot-printer as its output device .</sentence>
				<definiendum id="0">Flexible Run OFF ( FROFF ) system</definiendum>
				<definiens id="0">an integrated system that is designed for setting not only ordinary texts but also figures or tables indispensable to our documents , and utilizes a dot-printer as its output device</definiens>
			</definition>
			<definition id="1">
				<sentence>a mini-computer system HP2108A ( 64Kbytes ) with an auxiliary memory ( 256Kbytes ) which is designed to be used asa free-list region of FLISP and an image processing memory ( see Fig .</sentence>
				<definiendum id="0">64Kbytes</definiendum>
				<definiens id="0">) with an auxiliary memory ( 256Kbytes ) which is designed to be used asa free-list region of FLISP and an image processing memory ( see Fig</definiens>
			</definition>
			<definition id="2">
				<sentence>RS % A : recover the old font • : end where gA is one of system variables , the value of which is altered by subjecting to an effect of . ''</sentence>
				<definiendum id="0">gA</definiendum>
				<definiens id="0">one of system variables</definiens>
			</definition>
			<definition id="3">
				<sentence>of NCC , pp.879-888 ( 1977 ) 6\ ] K. Hasebe , S. Nomoto and H. Ishida : An On Line Phototypesetter Support System , Computer Center , Univ. of Tokyo , ( MARCH 1979 ) 7\ ] M. Higashide : Roff User 's Manual , Osaka University , ( APR. 1978 ) 8\ ] M. Higashide : FLISP System and its Application to Run Off , Master Thesis of Osaka University , ( FEB. 1979 ) 9\ ] N. Abe , M. Higashide and S. Tsuji : An Improvemectt of FLISP System by Micro Programming , IECE-D , ( 1979 ) 10\ ] M. Higashide , K. Kontshl , N. Abe and S , Tsuji : The FLISP System for Mini Computer using B-frame Technique and M-frame Techinque , Information Processing of Japan , No.I , pp.8-16 , ( 1979 )</sentence>
				<definiendum id="0">Ishida</definiendum>
				<definiendum id="1">S. Tsuji</definiendum>
				<definiens id="0">An On Line Phototypesetter Support System</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>AMBER is a model of first language acquisition that improves its performance through a process of error recovery .</sentence>
				<definiendum id="0">AMBER</definiendum>
				<definiens id="0">a model of first language acquisition that improves its performance through a process of error recovery</definiens>
			</definition>
			<definition id="1">
				<sentence>AMBER assumes a small number of case relations such as agent , action , and possession from which Brown 's pairwise relations can be derived .</sentence>
				<definiendum id="0">AMBER</definiendum>
				<definiens id="0">assumes a small number of case relations such as agent , action , and possession from which Brown 's pairwise relations can be derived</definiens>
			</definition>
			<definition id="2">
				<sentence>AMBER reflects this distinction by representing the information expressed by contents words and grammatical morphemes in different ways .</sentence>
				<definiendum id="0">AMBER</definiendum>
				<definiens id="0">reflects this distinction by representing the information expressed by contents words and grammatical morphemes in different ways</definiens>
			</definition>
			<definition id="3">
				<sentence>The second type of proposition expresses various kinds of relations , including facts like x possesses y , y is a *ball , and `` ball '' is the word for *ball ( where concepts are preceded by `` * '' to distinguish them from their associated words ) .</sentence>
				<definiendum id="0">y</definiendum>
				<definiens id="0">a *ball , and `` ball '' is the word for *ball ( where concepts are preceded by `` * '' to distinguish them from their associated words )</definiens>
			</definition>
			<definition id="4">
				<sentence>AMBER uses this process of spreading activation primarily to retrieve information about the words associated with particular concepts .</sentence>
				<definiendum id="0">AMBER</definiendum>
				<definiens id="0">uses this process of spreading activation primarily to retrieve information about the words associated with particular concepts</definiens>
			</definition>
			<definition id="5">
				<sentence>The Production System ACTG represents procedural knowledge as a set of condition-action rules called productions .</sentence>
				<definiendum id="0">Production System ACTG</definiendum>
			</definition>
			<definition id="6">
				<sentence>The most basic of these is the designation process , which allows the creation of a new production as one of the actions of an existing rule .</sentence>
				<definiendum id="0">designation process</definiendum>
				<definiens id="0">allows the creation of a new production as one of the actions of an existing rule</definiens>
			</definition>
			<definition id="7">
				<sentence>AMBER 's Linguistic Knowledge Learning is the result of an interaction between a set of relatively general techniques for acquiring knowledge and the environment in which they find themselves .</sentence>
				<definiendum id="0">AMBER 's Linguistic Knowledge Learning</definiendum>
				<definiens id="0">the result of an interaction between a set of relatively general techniques for acquiring knowledge and the environment in which they find themselves</definiens>
			</definition>
			<definition id="8">
				<sentence>AMBER represents the meaning of a sentence as a number of propositions , each incorporating one of a small set of relations .</sentence>
				<definiendum id="0">AMBER</definiendum>
				<definiens id="0">represents the meaning of a sentence as a number of propositions</definiens>
			</definition>
			<definition id="9">
				<sentence>This would lead to three goals : ( goal 1 AMBER pause ) , ( goal 2 AMBER bounce ) , and ( goal with English , the model will have generated a rule like : If you have a goal to pause , followed by a goal to say vword2 , and you have no intermediate goals , and vword2 is the word for vtype2 , and vtoken2 is of type vtype2 , and vtoken2 is the action of vevent , and vtokenl is the agent of vevent , and Vtokenl is of type vty~el , and vwordl is the word for vtypel , then insert a goal to say vwordl between the other goals-This rule would add a goal to say the agent `` Daddy '' after the first pause and before `` bounce '' , using the proposition ( goal 1.5 AMBER Daddy ) .</sentence>
				<definiendum id="0">vword2</definiendum>
				<definiendum id="1">vtoken2</definiendum>
				<definiendum id="2">vtoken2</definiendum>
				<definiendum id="3">Vtokenl</definiendum>
				<definiendum id="4">vwordl</definiendum>
				<definiens id="0">the word for vtype2</definiens>
				<definiens id="1">the action of vevent , and vtokenl is the agent of vevent , and</definiens>
			</definition>
			<definition id="10">
				<sentence>, Representation and Meaning : Experiments with Information Processing Systems .</sentence>
				<definiendum id="0">Representation</definiendum>
				<definiendum id="1">Meaning</definiendum>
			</definition>
</paper>

		<paper id="1080">
			<definition id="0">
				<sentence>Character strings are the most difficult items to translate , because BASIC contains string-processing functions such as LEN ( ) ( length of a string ) , LEFTS ( ) , RIGHTS ( ) , MID $ ( ) ( substrings ) , and INSTRS ( ) ( searches one string for the first occurrence of another ) .</sentence>
				<definiendum id="0">Character strings</definiendum>
				<definiendum id="1">LEN ( )</definiendum>
				<definiendum id="2">INSTRS ( ) (</definiendum>
				<definiens id="0">( length of a string )</definiens>
				<definiens id="1">searches one string for the first occurrence of another )</definiens>
			</definition>
			<definition id="1">
				<sentence>String operations which involve ASCII character codes , for example CHR $ ( ) which returns the character corresponding to a given ASCII code , and CHANGE ( ) which transfers a string to an array of ASCII codes , are not implemented .</sentence>
				<definiendum id="0">ASCII character codes</definiendum>
				<definiendum id="1">CHANGE ( )</definiendum>
				<definiens id="0">returns the character corresponding to a given ASCII code</definiens>
				<definiens id="1">transfers a string to an array of ASCII codes , are not implemented</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>M2 aims to expres a more or less syntactic finding about the description of a character ( co-presence of two antinomic 'notations ' ) in terms of the 'notations ' themselves .</sentence>
				<definiendum id="0">M2</definiendum>
				<definiens id="0">aims to expres a more or less syntactic finding about the description of a character</definiens>
			</definition>
			<definition id="1">
				<sentence>tween levels of discourse independently of which character the inference is about , R3 and R5 make use of some knowledge associated either , as in the examples , with the name of the character , or with previous sentences about it , e.g. 'Krylov is an author ' could be part of the beginning of the story .</sentence>
				<definiendum id="0">'Krylov</definiendum>
				<definiens id="0">an author ' could be part of the beginning of the story</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>A syntactic structure consists of two level elements and the algebraic structures on them .</sentence>
				<definiendum id="0">syntactic structure</definiendum>
				<definiens id="0">consists of two level elements and the algebraic structures on them</definiens>
			</definition>
			<definition id="1">
				<sentence>A semantic structure consists of two level elements and the algebraic structures on them .</sentence>
				<definiendum id="0">semantic structure</definiendum>
				<definiens id="0">consists of two level elements and the algebraic structures on them</definiens>
			</definition>
			<definition id="2">
				<sentence>In an algebraic point of view , a topological space is defined by an ordered pair ( X , T ) , where X is a support ( set of points ) , and T is a topology on X. The whole topological space is generated as a dictionary , or a thesaurus , or the universe of knowledge .</sentence>
				<definiendum id="0">topological space</definiendum>
				<definiendum id="1">X</definiendum>
				<definiendum id="2">T</definiendum>
				<definiens id="0">a support ( set of points ) , and</definiens>
			</definition>
			<definition id="3">
				<sentence>S. is a subset of VNi ( initial vocabulary ) , and Pi is a T1nite set of production rules ( rewriting rules ) of the form : P1 : A ÷ ~ ( 5 ) where A eVNi ' ~e LA ( Vi , D ) .</sentence>
				<definiendum id="0">S.</definiendum>
				<definiendum id="1">Pi</definiendum>
				<definiens id="0">a subset of VNi ( initial vocabulary ) , and</definiens>
				<definiens id="1">a T1nite set of production rules ( rewriting rules ) of the form : P1 : A ÷</definiens>
			</definition>
			<definition id="4">
				<sentence>P3T : A ÷ ~ ( i0 ) e { topological spaces of each types } P3A : A * ~ ( ii ) 6 ( areas of topological spaces } G3 generates the topology of each subspace , then following types of topologies are adopted adaptively : To , Ti , T2 , R , T3 , CR , T , N , T4 , T~ , T , Compact , Metric , Uniform , SeparableVand so on .</sentence>
				<definiendum id="0">T~</definiendum>
				<definiens id="0">To , Ti , T2 , R , T3 , CR , T , N , T4 ,</definiens>
			</definition>
			<definition id="5">
				<sentence>( 13 ) Def.6 The Syntax Generative Grammar : 'G G s = &lt; V N , V T , R , S &gt; ( 14 ) s where S is an initial vocabulary , and subset of V. , R is a set of phrase structure r~writing rules .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">The Syntax Generative Grammar : 'G G s = &lt; V N , V T , R , S &gt; ( 14 ) s where S is an initial vocabulary , and subset of V. ,</definiens>
				<definiens id="1">a set of phrase structure r~writing rules</definiens>
			</definition>
			<definition id="6">
				<sentence>The other side V is divided into two : V=V U V a , where V is significant vocabulary , aYidVG is g~ammatical vocabulary .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">aYidVG</definiendum>
				<definiens id="0">significant vocabulary</definiens>
				<definiens id="1">g~ammatical vocabulary</definiens>
			</definition>
			<definition id="7">
				<sentence>Def.8 The Syntactic Algebraic S~tem : ~s The syntactic algebraic system is defined as ~s = &lt; P ' Is ' Es &gt; ( 16 ) where ~ is a set of P-markers generated by G , and I and E are the interior and exterior op~ratorsSon ~ .</sentence>
				<definiendum id="0">Syntactic Algebraic S~tem</definiendum>
				<definiens id="0">~s = &lt; P ' Is ' Es &gt; ( 16 ) where ~ is a set of P-markers generated by G , and I and E are the interior and exterior op~ratorsSon ~</definiens>
			</definition>
			<definition id="8">
				<sentence>-- 95-Def.9 The meaning affix mapping : f , ( 17 ) ~ ' : D ÷ Vp where D is the set of all areas in the space I. @ ' assigns an area ( a concept in semantic map ) to a token or string of tokens ( words ) in the syntactic level .</sentence>
				<definiendum id="0">D</definiendum>
				<definiens id="0">a concept in semantic map ) to a token or string of tokens ( words ) in the syntactic level</definiens>
			</definition>
			<definition id="9">
				<sentence>And there are three types of unifications as follows ; *Mutual dependency : composition of transitive verb with direct object , and composition of direct object with indirect object .</sentence>
				<definiendum id="0">*Mutual dependency</definiendum>
				<definiens id="0">composition of transitive verb with direct object , and composition of direct object with indirect object</definiens>
			</definition>
			<definition id="10">
				<sentence>*Uni-directional dependency : composition of the subject and verb ( phrase ) .</sentence>
				<definiendum id="0">*Uni-directional dependency</definiendum>
			</definition>
			<definition id="11">
				<sentence>There are following property-types : D : a power set of areas , H : a power set of hyperplanes , V : a power set of values P E : a power set of estimating value .</sentence>
				<definiendum id="0">H</definiendum>
				<definiendum id="1">V</definiendum>
				<definiens id="0">a power set of areas</definiens>
				<definiens id="1">a power set of values P E : a power set of estimating value</definiens>
			</definition>
</paper>

		<paper id="1085">
			<definition id="0">
				<sentence>&lt; &lt; ~ b~ -- -- &lt; ~\ ] -- -- \ [ _ ~zLgL_ : ~D -- -D -- -- ~ ... . As each dialog cycle of the eight dialogs consists of a ) inputs and b ) outputs , the dialog outputs ( air controller utterances ) can be easily copied for preFIG , 3 ASL ARCHITECTURE : system internal sensor input~ / Coder Speech Analyzer ROM for J TWIN I KFC s I ~V-A T =~KFC F=== Matcher | ... ... ... . -7/ ... ... ... . J Answer Trigger Speech Synthesizer / Amplifier / i I Answer Pool for prep .</sentence>
				<definiendum id="0">air controller utterances</definiendum>
				<definiens id="0">system internal sensor input~ / Coder Speech Analyzer ROM for J TWIN I KFC s I ~V-A T =~KFC F=== Matcher | ... ... ...</definiens>
			</definition>
</paper>

		<paper id="1043">
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>This last characteristic is the one that accounts for the 'context sensitive ' qualification of the ACSP : the automatic programming procedures are settled in environments that are susceptible to supply the parameters relevant to their execution .</sentence>
				<definiendum id="0">ACSP</definiendum>
			</definition>
			<definition id="1">
				<sentence>APLEC ( APrenti-LECteur the Learning Reader ) will associate automatically a question/answer module for all TDG submitted by the user .</sentence>
				<definiendum id="0">APLEC</definiendum>
				<definiens id="0">APrenti-LECteur the Learning Reader ) will associate automatically a question/answer module for all TDG submitted by the user</definiens>
			</definition>
			<definition id="2">
				<sentence>APLEC is in a way sold blank to its users who little by little will transform it into a more personal robot which gets better while adapting to particular textual data .</sentence>
				<definiendum id="0">APLEC</definiendum>
				<definiens id="0">gets better while adapting to particular textual data</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus APLEC learns that `` book '' is an object of value ( since it is `` offered '' in the text ) , for it learned that to `` slip '' and `` steal '' can now be taken one for the other , since that to `` slip '' has become an `` action '' on an object of `` value '' ... In this second learning strategy , the `` word-to-word '' equivalences are replaced by more or less complete patternmatching semantic networks defined in the terms of the experimented grammar and labelled to the words of the question and to those of the text .</sentence>
				<definiendum id="0">word-to-word</definiendum>
				<definiens id="0">an object of value ( since it is `` offered '' in the text</definiens>
				<definiens id="1">'' equivalences are replaced by more or less complete patternmatching semantic networks defined in the terms of the experimented grammar</definiens>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>The Japanese language uses an extensive and complicated character system .</sentence>
				<definiendum id="0">Japanese language</definiendum>
			</definition>
			<definition id="1">
				<sentence>LOw geeds systematic training ; heavily depends on training nethod .</sentence>
				<definiendum id="0">LOw</definiendum>
				<definiens id="0">geeds systematic training</definiens>
			</definition>
			<definition id="2">
				<sentence>Various kanji dictionaries use radicals ( substructures of kanji ) , the number of strokes used to draw the character , or the reading ( in phonetics ) to index a certain kanji .</sentence>
				<definiendum id="0">Various kanji dictionaries use radicals</definiendum>
				<definiens id="0">substructures of kanji ) , the number of strokes used to draw the character , or the reading ( in phonetics ) to index a certain kanji</definiens>
			</definition>
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>An inheritance net of objects ( with the root class Object ) is constructed by sending the message NEW : to the class CLASS , i.e. a concept definition in ObjTalk has the form : ( ASK CLASS NEW : &lt; concept-name &gt; SUPERC ( &lt; concept-name1 &gt; ... ) GENERIC-PROPERTIES &lt; slot-name &gt; : &lt; filler-description &gt; ME~HODS &lt; method-name &gt; : \ [ &lt; filter &gt; = &gt; &lt; body &gt; \ ] ~RIGCER-ATN &lt; Trigger-keys &gt; ; attached ATN \ [ &lt; subATN-node &gt; &lt; production &gt; \ ] ... ) The effect of sending NEW : to CLASS is to define a class with the given &lt; concept-name &gt; as a subclass of the named superelass ( es ) .</sentence>
				<definiendum id="0">inheritance net of objects</definiendum>
				<definiendum id="1">CLASS</definiendum>
				<definiendum id="2">SUPERC</definiendum>
				<definiens id="0">with the root class Object</definiens>
				<definiens id="1">the form : ( ASK CLASS NEW : &lt; concept-name &gt;</definiens>
				<definiens id="2">to define a class with the given &lt; concept-name &gt; as a subclass of the named superelass ( es )</definiens>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>An M-rule R i is a pair &lt; Ci , Ai &gt; ; where C i is a condition on n-tuples of trees &lt; t I ... . , tn &gt; and A i is an action , applicable to any tuple for which C i holds , and delivering a tree t. Each rule R i must obey the following conditions : ( i ) C i and A i are effective procedures .</sentence>
				<definiendum id="0">C i</definiendum>
				<definiendum id="1">i</definiendum>
				<definiens id="0">an action , applicable to any tuple for which C i holds</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>The automatic translation system consists of two processes ; the segmentation and the word identification processes .</sentence>
				<definiendum id="0">automatic translation system</definiendum>
				<definiens id="0">consists of two processes ; the segmentation and the word identification processes</definiens>
			</definition>
			<definition id="1">
				<sentence>The independent part consists of an independent word or its derivative , and the dependent part consists of a sequence of dependent words , given as follows : Bunsetsu= ( independent part ) .</sentence>
				<definiendum id="0">independent part</definiendum>
				<definiens id="0">consists of an independent word or its derivative , and the dependent part consists of a sequence of dependent words , given as follows : Bunsetsu= ( independent part )</definiens>
			</definition>
			<definition id="2">
				<sentence>ikanakerebanaranakatta ( had to go ) V AUX P AUX AUX AUX V : verbs , AUX : auxiliary verb , P : particle Fig.2 An example of Bunsetsu An indicative form 'ika ' of a verb 'iku ' can be concatenated not only by inflectional form 'nakere ' of auxiliary verb 'nai ' in this example but also by all of inflectional forms of 'nai ' .</sentence>
				<definiendum id="0">AUX</definiendum>
				<definiens id="0">auxiliary verb</definiens>
			</definition>
			<definition id="3">
				<sentence>N2 ga N3 wo V. where Ni , N2 and N3 denotes a noun and V denotes a transitive verb .</sentence>
				<definiendum id="0">Ni</definiendum>
				<definiendum id="1">V</definiendum>
				<definiens id="0">a transitive verb</definiens>
			</definition>
			<definition id="4">
				<sentence>Katakana denotes no analized strings .</sentence>
				<definiendum id="0">Katakana</definiendum>
				<definiens id="0">no analized strings</definiens>
			</definition>
</paper>

		<paper id="1076">
			<definition id="0">
				<sentence>MBI is a rival of a national main-frame-makers and the national industrial Ministry ITIM , which specifies market strategic policies .</sentence>
				<definiendum id="0">MBI</definiendum>
				<definiens id="0">a rival of a national main-frame-makers and the national industrial Ministry ITIM , which specifies market strategic policies</definiens>
			</definition>
			<definition id="1">
				<sentence>Frames called objects are organized into knowledge structures by message passings which activate procedural properties of the recipient object .</sentence>
				<definiendum id="0">Frames</definiendum>
				<definiens id="0">called objects are organized into knowledge structures by message passings which activate procedural properties of the recipient object</definiens>
			</definition>
			<definition id="2">
				<sentence>OBJTA\ [ K offers a simple language of the form massage ( ask &lt; default class-name &gt; generalize &lt; instance-name &gt; ) ( ask &lt; default class-name &gt; abstract &lt; individual &gt; ) ( ask &lt; object &gt; &lt; message &gt; ) By generalizing a instance or abstracting an individual we can create -- 507-a class of instances or individuals according to their perspectives , such that it prescribes property conditions to be satisfied by sample objects .</sentence>
				<definiendum id="0">OBJTA\ [ K</definiendum>
				<definiens id="0">offers a simple language of the form massage</definiens>
			</definition>
			<definition id="3">
				<sentence>A main topic is assigned to each class which may be characterized by s set of predicates to be satisfied by members of the class .</sentence>
				<definiendum id="0">main topic</definiendum>
				<definiens id="0">each class which may be characterized by s set of predicates to be satisfied by members of the class</definiens>
			</definition>
			<definition id="4">
				<sentence>-- 510- ( 1 ) Brown , J.S. and Burton , R.R. , Bell , A.G. ; SOPHIE : A step towards a reactive \ ] earning environment , International journal of man machine studies , Voi.7,1975 , pp 675-696 ( 2 ) Schank R.C. Abelson , P.R. ; Scripts , Plans , Goals and Understanding , lawrence Erlbaum Press , 1977 ( 5 ) DeJon , G. ; Prediction and Substantiation : Process that comprise understanding , IJCAI , \ ] 979 , Tokyo , pp217-222 ( 4 ) Bobrow , D.G. , Kaplan , R.M. , Kay , M.Norman , D.A.Thompson , H. , Winograd , T. ; GUS , A fram driven dialog system , Artificial Intelligence Vol.8 , No\ ] ,1977 ( 5 ) Tanaka , H. ; EXP\ [ USA semantic processing system for japanese sentenses , Trans .</sentence>
				<definiendum id="0">GUS</definiendum>
				<definiendum id="1">Artificial Intelligence Vol.8 , No\ ] ,1977</definiendum>
				<definiendum id="2">semantic processing system for japanese</definiendum>
				<definiens id="0">A step towards a reactive \ ] earning environment</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>the set of symbols ( F ) : F = # ~y~FO , where Fo is a finite set of symbols , and if Ol # o 2 then FO , ~F~=~ .</sentence>
				<definiendum id="0">Fo</definiendum>
				<definiens id="0">a finite set of symbols</definiens>
			</definition>
			<definition id="1">
				<sentence>the set of variables ( X ) : X= ~5~Xo , where Xo is a set of variables such that if ~i # o~ then X~znXa~=~ , and that intersection of F and X is empty .</sentence>
				<definiendum id="0">set of variables</definiendum>
				<definiendum id="1">Xo</definiendum>
				<definiens id="0">a set of variables such that if ~i # o~ then X~znXa~=~ , and that intersection of F and X is empty</definiens>
			</definition>
			<definition id="2">
				<sentence>the set of expressions ( E ) : E = ~ ~E~ , where E~ fills the following properties : ( i ) Xo GEo , ( ii ) FocE~ , ( iii ) if ~ gE &lt; T , Oi , ... , ~a &gt; and ~I , ... , ~gEot , ... , E~a , then the expression ~ ( ~I , ... , ~n ) £ET , ( iv ) if B gXo and ~cET , then the expression 18\ [ ~\ ] e E &lt; Y , O &gt; , where I is a distinguished symbol in E. PSN -partitioned semantic network PSN denotes the semantics of LE .</sentence>
				<definiendum id="0">PSN</definiendum>
				<definiens id="0">the set of expressions ( E ) : E = ~ ~E~ , where E~ fills the following properties : ( i ) Xo GEo , ( ii ) FocE~ , ( iii ) if ~ gE &lt; T , Oi , ... , ~a &gt; and ~I , ... , ~gEot , ... , E~a , then the expression ~ ( ~I , ... , ~n ) £ET , ( iv ) if B gXo and ~cET</definiens>
				<definiens id="1">a distinguished symbol in E. PSN -partitioned semantic network</definiens>
				<definiens id="2">the semantics of LE</definiens>
			</definition>
			<definition id="3">
				<sentence>The constituents of PSN are : a space which denotes a possible world a typed nodes an arc with a case label 85 SENTENCE NG DET NOUN / Every man VP /\ VT NG / /\ DET NOUN I I loves a woman ( ( EVERY MAN ) ( LAMBDA X ( ( A WOMAN ) ( LAMBDA Y ( LOVE X Y ) ) ) ) ) ~E~MAN ) ( LAMBDA~~ ( LAMBDA Y ( LOVE X Y ) ) ) ) EVERY MAN LOVE ( A WOMAN ) I I A A WOMAN I I Every man loves a woman ( a ) the syntax tree .</sentence>
				<definiendum id="0">MAN ) ( LAMBDA X</definiendum>
				<definiendum id="1">A WOMAN ) I I A A WOMAN</definiendum>
				<definiens id="0">a space which denotes a possible world a typed nodes an arc with a case label 85 SENTENCE NG DET NOUN / Every man VP /\ VT NG / /\ DET NOUN I I loves a woman ( ( EVERY</definiens>
			</definition>
			<definition id="4">
				<sentence>Each grammar rule consists of syntactic generation part and semantic composition part .</sentence>
				<definiendum id="0">grammar rule</definiendum>
			</definition>
			<definition id="5">
				<sentence>The LE for a sentence is a functional composition of an NG and a VP .</sentence>
				<definiendum id="0">LE for a sentence</definiendum>
				<definiens id="0">a functional composition of an NG and a VP</definiens>
			</definition>
			<definition id="6">
				<sentence>The constituents of a noun phrase are : determiner ( DET ) in E &lt; &lt; O , &lt; 0,1 &gt; &gt; , &lt; 0,1 &gt; &gt; number ( NBR ) in E &lt; &lt; 0,1 &gt; , &lt; 0,1 &gt; &gt; adjective ( ADJ ) in E &lt; &lt; 0 , i &gt; , &lt; O , i &gt; &gt; head noun ( NOUN ) in E &lt; 0,1 &gt; plural morpheme ( +S ) in E &lt; &lt; 0,1 &gt; , &lt; O , I &gt; &gt; post modifier ( Q ) in E &lt; &lt; 0,1 &gt; , &lt; 0,1 &gt; &gt; ( example ) `` the two efficient algorithms '' the ( two ( *pl ( efficient ( algorithm ) ) ) ) E E &lt; O , &lt; 0,1 &gt; &gt; the~two ( *el ( ~ef ficient ( algorithm ) ) ) ~o *m~ifef : \ [ cie~nt ( ~l ith `` efficient ( algorithm ) *pl I I the two efficient algorithm +s ( DET ) ( NBR ) ( ADJ ) ( NOUN ) ( +S ) 87 ( 4 ) Postmodifier ( i ) Relative clause A relative clause is composed of the symbol 'which ' and a sentence .</sentence>
				<definiendum id="0">DET</definiendum>
				<definiendum id="1">NBR</definiendum>
				<definiendum id="2">NOUN</definiendum>
				<definiens id="0">efficient ( algorithm ) *pl I I the two efficient algorithm +s ( DET ) ( NBR ) ( ADJ ) ( NOUN</definiens>
			</definition>
			<definition id="7">
				<sentence>EASY is a top-down parser and reads input sentences from the left to the right .</sentence>
				<definiendum id="0">EASY</definiendum>
				<definiens id="0">a top-down parser and reads input sentences from the left to the right</definiens>
			</definition>
			<definition id="8">
				<sentence>LE is a logical expression .</sentence>
				<definiendum id="0">LE</definiendum>
				<definiens id="0">a logical expression</definiens>
			</definition>
			<definition id="9">
				<sentence>T ( ~ , p ) is a meta predicate that means the object formula p is true in the possible world ( or space ) denoted by ~ .</sentence>
				<definiendum id="0">p )</definiendum>
				<definiens id="0">a meta predicate that means the object formula p is true in the possible world</definiens>
			</definition>
</paper>

		<paper id="1092">
			<definition id="0">
				<sentence>But GLAPS is capable of processing multiple answers often given to questions about word-form .</sentence>
				<definiendum id="0">GLAPS</definiendum>
				<definiens id="0">capable of processing multiple answers often given to questions about word-form</definiens>
			</definition>
			<definition id="1">
				<sentence>21 * 22 * distribution of informants ' communities 23 * 24 SIZE 25,45 25 LOCATION NORTH/SOUTH ( SOUTH ) , EAST/WEST ( WEST ) 26 PRETITLES `` NORTH/WEST NORTH/EAST '' 27 POSTTITLES `` SOUTH/WEST SOUTH/EAST '' 28 DELETE INFORMANT-NUMBER ( 2-9 ) 29 NAMES COMMUN ITY ( l =Tare ) ( 2=Shinoga~ri ) ( 3=Shinokawara ) ( 4=Hayasa ka ) 30 ( 5=Hi gashi-Hayasaka ) ( 6=Kami-Shinozaki ) 31 ( 7=Shimo-Shinozaki ) ( 8=Higashi-Shinozaki ) 32 ( 9=Nishi-Shinozaki ) 33 SYMBOLS COMMUNITY ( I=I ) ( 2=2 ) ( 3=3 ) ( 4=4 ) ( 5=5 ) ( 6=6 ) ( 7=7 ) ( 8=8 ) ( 9=9 ) 34 ATLAS COMMUNITY 35 NDELETE 36 * 37 * crosstabulations of cowlick by other variables 38 * 39 SUBTITLES COWLICK ( `` ( the whirl of hair on the head ) '' 40 , '' // rough classification // '' ) 41 IGNORE COWLICK ( 21-33,40-41,59.75,78,81-82,84-88,94-99 ) 42 RECODE COWL ICK ( I =l -l 6 ) ( 34=34,42,44,46-48 ) ( 36=35-36,43,49,83,89 ) 43 { 50 = 5052 ) ( 53 = 53 58.77 ) ( 60 = 60-65,69 74 ) ( 67=67-68,76 ) 44 ( 90=90-92 ) 45 NAMES COWLICK ( I =uzumaki ) ( 34=makizyumonzi ( 36=makiguri ) 46 ( 45=ma kibosi ) ( 50=ma kure ) ( 53=nm kurebosi ) ( 60=maruhosi 47 ( 67=ma kurezyumonzi ) ( 90=tsumuzi ) 48 RECODE AGE ( I=I-90404 ) ( 2=90404-91404 ) ( 3=91404-92404 ) ( 4=92404-93404 ) 49 ( 5=93404 -94404 ) ( 6=94404-95404 ) ( 7=95404-99999 ) 50 NAMES AGE ( l=over 70 ) ( 2=over 60 ) ( 3=over 50 ) ( 4=over 40 ) ( 5=over 30 ) 51 ( 6=over 20 ) ( 7=over 10 ) 52 NAMES NATIVE-OR-NOT ( l=nat ive ) ( 2=non-native ) 53 NAMES PRIMARY-SCHOOL-NAME ( 1=nagayama ) ( 2=nishlne ) ( 3=etc. ) 54 CROSSTABS ( NATI VE-OR-NOT , AGE , COMMUN ITY .</sentence>
				<definiendum id="0">IGNORE COWLICK ( 21-33,40-41,59.75,78,81-82,84-88,94-99 ) 42 RECODE COWL ICK</definiendum>
				<definiens id="0">the whirl of hair on the head ) '' 40 , '' // rough classification // ''</definiens>
			</definition>
			<definition id="2">
				<sentence>The NAMES statement of lines 29 to 32 identifies the meaning of numbers used in the coded data .</sentence>
				<definiendum id="0">NAMES statement</definiendum>
				<definiens id="0">identifies the meaning of numbers used in the coded data</definiens>
			</definition>
			<definition id="3">
				<sentence>... ... ... ... ... ... ... ... ... ... ÷ SOUTH/WEST SOUTH/EAST HO OF CASES l 4e @ g ) uzumaki + 3 ) makiz~umonzl X S ) makuta W 4 ) makureboa i **** intensive I nvestlgatlon at Hi shlysma **XX ( ShlzuKulahl , Iwa~e ~rel~ ; emma • COWLICK ( the whirl ot hair on the head ) // rough classification // CONTROL AGE VALUE over 40 HORTH/WEST NORTH/EAST ÷ ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... + I U I I 0 QX I I 0 XO UO 0 I I I I o 0 Oe I l eoeooo o u uI I ex , |Qo I I eO I I 0 XO +W I I XO e I IQO O0 WW WW I I I I I I I I O0 O0 CXH I I X !</sentence>
				<definiendum id="0">COWLICK</definiendum>
				<definiens id="0">the whirl ot hair on the head ) // rough classification // CONTROL AGE VALUE over 40 HORTH/WEST NORTH/EAST ÷ ... ...</definiens>
			</definition>
			<definition id="4">
				<sentence>GLAPS is a convenient system easily accessible to dialectologists .</sentence>
				<definiendum id="0">GLAPS</definiendum>
			</definition>
</paper>

		<paper id="1066">
			<definition id="0">
				<sentence>The theoretical framework is the one of a formal language , with a syntax describing the combination rules of the language units .</sentence>
				<definiendum id="0">theoretical framework</definiendum>
				<definiens id="0">the one of a formal language , with a syntax describing the combination rules of the language units</definiens>
			</definition>
			<definition id="1">
				<sentence>ARIANE is the name of the interactive monitor interfacing with the user .</sentence>
				<definiendum id="0">ARIANE</definiendum>
				<definiens id="0">the name of the interactive monitor interfacing with the user</definiens>
			</definition>
			<definition id="2">
				<sentence>Using a built-in backtracking algorithm , ROBRA finds the first possible traversal of the control graph leading to an exit ( &amp; NUL symbol ) , thereby applying each traversed TG to the object tree .</sentence>
				<definiendum id="0">ROBRA</definiendum>
				<definiens id="0">finds the first possible traversal of the control graph leading to an exit ( &amp; NUL symbol ) , thereby applying each traversed TG to the object tree</definiens>
			</definition>
			<definition id="3">
				<sentence>-~ FTR ( TI ) &amp; NUL INIT is the first grammar , and is iterative ( E ) .</sentence>
				<definiendum id="0">NUL INIT</definiendum>
			</definition>
			<definition id="4">
				<sentence>GNI builds simple nominal groups like Adj + N or Prep + N or mum + N. GN2 looks for further elements in the nominal groups , and solves certain ambiguities .</sentence>
				<definiendum id="0">GNI</definiendum>
				<definiens id="0">builds simple nominal groups</definiens>
			</definition>
			<definition id="5">
				<sentence>MARQ builds all types of subordinate verbal and infinitive clauses .</sentence>
				<definiendum id="0">MARQ</definiendum>
				<definiens id="0">builds all types of subordinate verbal and infinitive clauses</definiens>
			</definition>
			<definition id="6">
				<sentence>CASC handles all genitive imbrications , by ( provisionally ) attaching dominated groups to non-ambiguous groups .</sentence>
				<definiendum id="0">CASC</definiendum>
				<definiens id="0">handles all genitive imbrications , by ( provisionally ) attaching dominated groups to non-ambiguous groups</definiens>
			</definition>
			<definition id="7">
				<sentence>PHR marks all strongly governed groups subordinated to the utterance with logical relations as agent , patient , attribute ... If possible , this is also done on dependent groups .</sentence>
				<definiendum id="0">PHR</definiendum>
				<definiens id="0">marks all strongly governed groups subordinated to the utterance with logical relations as agent , patient</definiens>
			</definition>
			<definition id="8">
				<sentence>The image subtree ( generally consisting of only one node ) is added to the output , with values of variables computed by the assignment part .</sentence>
				<definiendum id="0">image subtree</definiendum>
				<definiens id="0">variables computed by the assignment part</definiens>
			</definition>
			<definition id="9">
				<sentence>RFPF is an assignment procedure. ``</sentence>
				<definiendum id="0">RFPF</definiendum>
				<definiens id="0">an assignment procedure.</definiens>
			</definition>
			<definition id="10">
				<sentence>PRL handles idioms , predicted in lexical transfer by generating auxiliary subtrees .</sentence>
				<definiendum id="0">PRL</definiendum>
				<definiens id="0">handles idioms , predicted in lexical transfer by generating auxiliary subtrees</definiens>
			</definition>
			<definition id="11">
				<sentence>RCTF handles non-standard government , particular uses of `` DE '' , erases some prepositions , takes care of passive-active transformations , etc .</sentence>
				<definiendum id="0">RCTF</definiendum>
				<definiens id="0">handles non-standard government , particular uses of `` DE '' , erases some prepositions , takes care of passive-active transformations , etc</definiens>
			</definition>
			<definition id="12">
				<sentence>ACTL handles particular idiom translations , like `` ESLI + Inf '' ~ `` SI ON + Present '' , etc .</sentence>
				<definiendum id="0">ACTL</definiendum>
				<definiens id="0">handles particular idiom translations</definiens>
			</definition>
			<definition id="13">
				<sentence>ART uses the remaining designators to compute the determination of nominal groups .</sentence>
				<definiendum id="0">ART</definiendum>
				<definiens id="0">uses the remaining designators to compute the determination of nominal groups</definiens>
			</definition>
			<definition id="14">
				<sentence>RC ( T ) ACI ( P ) if relative ... ... ... ... . ADJ ( E ) pronoun RELATIF ( H ) else ~____~AC2 ( EP ) ART ( E ) if ULO ~ART2 ( EP ) ULZERO ( T ) ~ ... .. ~ &amp; NUL RC copies variables from head nodes onto their fathers , and checks for number and gender correctness .</sentence>
				<definiendum id="0">RC</definiendum>
				<definiendum id="1">NUL RC</definiendum>
				<definiens id="0">copies variables from head nodes onto their fathers , and checks for number and gender correctness</definiens>
			</definition>
			<definition id="15">
				<sentence>SYGMOR realizes the composition of two transducers : the first , `` tree-to-string '' , produces the frontier of the object tree ; the second transforms this string ( of masks of variables ) into a string of characters , under the control of the linguistic data .</sentence>
				<definiendum id="0">SYGMOR</definiendum>
				<definiens id="0">realizes the composition of two transducers</definiens>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>According to the general theory of subject indexing languages ; information is the message conveyed or intended to be conveyed by a systemetlsed body of ideas , or its accepted or acceptable substitutes .</sentence>
				<definiendum id="0">information</definiendum>
				<definiens id="0">the message conveyed or intended to be conveyed by a systemetlsed body of ideas</definiens>
			</definition>
			<definition id="1">
				<sentence>A Subject Indexing Language consists of elementary constituents and rules for the formulation of admissible subject-proposltlons .</sentence>
				<definiendum id="0">Subject Indexing Language</definiendum>
				<definiens id="0">consists of elementary constituents and rules for the formulation of admissible subject-proposltlons</definiens>
			</definition>
			<definition id="2">
				<sentence>LEATHER ( type of- ) TANNED In the relation map given above , the dotted lines indicate NT/BT relationship , continuous lines indicate RT relationship and slash indicates synonym/use relatlonship .</sentence>
				<definiendum id="0">LEATHER</definiendum>
				<definiens id="0">the relation map given above , the dotted lines indicate NT/BT relationship , continuous lines indicate RT relationship and slash indicates synonym/use relatlonship</definiens>
			</definition>
</paper>

		<paper id="1088">
			<definition id="0">
				<sentence>Reduplication is the process whereby a base or some part of the base is repeated .</sentence>
				<definiendum id="0">Reduplication</definiendum>
				<definiens id="0">the process whereby a base or some part of the base is repeated</definiens>
			</definition>
			<definition id="1">
				<sentence>Affixation is a morphological process whereby a base may be extended by one or more affixes .</sentence>
				<definiendum id="0">Affixation</definiendum>
			</definition>
			<definition id="2">
				<sentence>-- 578 -IntrodUction The Malay language has been the national language of Malaysia since 1955 and with the formation of Malaysia in 1963 , it has been known as Bahasa Malaysia ( B.M. ) .</sentence>
				<definiendum id="0">Malay language</definiendum>
				<definiens id="0">the national language of Malaysia since 1955 and with the formation of Malaysia in 1963</definiens>
			</definition>
			<definition id="3">
				<sentence>AKIANE-78 is a software tool for machine-aided translation to which linguistic data ( grammars , dictionaries , heuristic ) formalised in some external artificial language is given .</sentence>
				<definiendum id="0">AKIANE-78</definiendum>
			</definition>
			<definition id="4">
				<sentence>Reduplication Reduplication is the process whereby a base or some part of the base is repeated .</sentence>
				<definiendum id="0">Reduplication Reduplication</definiendum>
				<definiens id="0">the process whereby a base or some part of the base is repeated</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>The set of ill-formed inputs for an interface will be defined as the union of these two sets for that interface .</sentence>
				<definiendum id="0">ill-formed inputs for an interface</definiendum>
				<definiens id="0">the union of these two sets for that interface</definiens>
			</definition>
			<definition id="1">
				<sentence>Absolute ill-formedness in semantics includes omitting needed information and violation of selectional restrictions .</sentence>
				<definiendum id="0">Absolute ill-formedness</definiendum>
				<definiens id="0">omitting needed information and violation of selectional restrictions</definiens>
			</definition>
			<definition id="2">
				<sentence>Overshoot is a related phenomenon .</sentence>
				<definiendum id="0">Overshoot</definiendum>
				<definiens id="0">a related phenomenon</definiens>
			</definition>
			<definition id="3">
				<sentence>Expansion ellipsis is a kind of fragmentary input no system has processed before .</sentence>
				<definiendum id="0">Expansion ellipsis</definiendum>
				<definiens id="0">a kind of fragmentary input no system has processed before</definiens>
			</definition>
			<definition id="4">
				<sentence>Harris , Larry R. , ROBOT : A High Performance Natural Language Interface for Data Base Query , Te~n~cal Report TR 77-Jlf Dartmout~ -- -Col~ege , Department of Mathematics , February , 1977 .</sentence>
				<definiendum id="0">ROBOT</definiendum>
				<definiens id="0">A High Performance Natural Language Interface for Data Base Query</definiens>
			</definition>
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>The global structure is represented by a tree called GPT ( Global Plan Tree ) , whiGh guides the succeeding analyses , That is , a node of GPT indicates what kind of transformed patterns should be used to analyze the corresponding fragment , and in what oder .</sentence>
				<definiendum id="0">whiGh</definiendum>
				<definiens id="0">guides the succeeding analyses , That is , a node of GPT indicates what kind of transformed patterns should be used to analyze the corresponding fragment , and in what oder</definiens>
			</definition>
			<definition id="1">
				<sentence>In this sense , our JIS is a mixed form of dependency structures and phrase structures .</sentence>
				<definiendum id="0">JIS</definiendum>
				<definiens id="0">a mixed form of dependency structures and phrase structures</definiens>
			</definition>
			<definition id="2">
				<sentence>The transfer prOcedure for this node arranges Ithe transfer results of the lower level into ~single ElS 's , and return them to the higher ~llevel .</sentence>
				<definiendum id="0">transfer prOcedure</definiendum>
				<definiens id="0">Ithe transfer results of the lower level into ~single ElS 's , and return them to the higher ~llevel</definiens>
			</definition>
			<definition id="3">
				<sentence>The Japanese compound word '~- ' roughly means 'the best in Japan ' , and consists of two words , B Ak ( Japan ) and ~ ( the first or one ) .</sentence>
				<definiendum id="0">B Ak</definiendum>
				<definiens id="0">consists of two words</definiens>
			</definition>
			<definition id="4">
				<sentence>Translation Results ers of natural language -- -423- ( 1 ) W.Hutchins : Machine Translation and MachineAided Translation , Jour .</sentence>
				<definiendum id="0">Translation</definiendum>
			</definition>
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>Schank ( \ ] 975 ) has systems based on the hypothesis that language understanding is driven from the semantics with minimal use of any syntactic analysis .</sentence>
				<definiendum id="0">Schank</definiendum>
				<definiens id="0">systems based on the hypothesis that language understanding is driven from the semantics with minimal use of any syntactic analysis</definiens>
			</definition>
			<definition id="1">
				<sentence>Grammar The grammar consists of a set of rules of the form shown in Figure 1 .</sentence>
				<definiendum id="0">Grammar The grammar</definiendum>
				<definiens id="0">consists of a set of rules of the form shown in Figure 1</definiens>
			</definition>
			<definition id="2">
				<sentence>J-~Do : JOB PERFOI RT-WHOLE VAR EPLACE AR CHANGE : TIll'El '' META # I LEADS-TO % T : TIRE| Figure 2 : Simplified version of semantic network with information about changing a tire .</sentence>
				<definiendum id="0">JOB PERFOI RT-WHOLE VAR EPLACE AR CHANGE</definiendum>
				<definiens id="0">Simplified version of semantic network with information about changing a tire</definiens>
			</definition>
			<definition id="3">
				<sentence>~\ ] e former uses the concepts of subject , object , verb , etc. , whereas the latter has events , states , and agents , i~struments , etc .</sentence>
				<definiendum id="0">e former</definiendum>
			</definition>
			<definition id="4">
				<sentence>TOOL6 is a token representing a group consisting of a jack ano a wrench .</sentence>
				<definiendum id="0">TOOL6</definiendum>
				<definiens id="0">a token representing a group consisting of a jack ano a wrench</definiens>
			</definition>
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>A code consists of a set of 4 digits , which represents one Chinese character .</sentence>
				<definiendum id="0">code</definiendum>
				<definiens id="0">consists of a set of 4 digits , which represents one Chinese character</definiens>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>This topic is in general specified by the composition of predicates and functions in a certain way which is expressed by the logical matrix .</sentence>
				<definiendum id="0">topic</definiendum>
				<definiens id="0">in general specified by the composition of predicates</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>HRAF ( Human Relations Area Files ) , which was developed by Dr. Murdock and now managed by HRAF Inc. at Yale University , is a unique retrieval system .</sentence>
				<definiendum id="0">HRAF</definiendum>
				<definiens id="0">a unique retrieval system</definiens>
			</definition>
			<definition id="1">
				<sentence>In a character mode , character must be defined as a dot matrix of 8X8,16Xi6,24X24,32X32 , etc .</sentence>
				<definiendum id="0">character</definiendum>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>MTS attitude demands an ideal external observer ( EO ) who is to model the system ( S ) and the system 's environment world ( W ) .</sentence>
				<definiendum id="0">MTS attitude</definiendum>
				<definiens id="0">demands an ideal external observer ( EO ) who is to model the system ( S ) and the system 's environment world ( W )</definiens>
			</definition>
			<definition id="1">
				<sentence>In order that EO should be able to form the intended models , he must possess the following kinds of knowledge about the sample situation ( and EO being an ideal observer , we assume he really does ) : ( i ) EO knows the aspect and the level at which S may perceive and describe the environment ; in other words , EO knows S 's sensitivity .</sentence>
				<definiendum id="0">EO</definiendum>
			</definition>
			<definition id="2">
				<sentence>( ii ) EO knows those fundamental aspects of W that S may describe .</sentence>
				<definiendum id="0">EO</definiendum>
				<definiens id="0">knows those fundamental aspects of W that S may describe</definiens>
			</definition>
			<definition id="3">
				<sentence>A theory T determines a subclass Mod ( T ) of models , namely those models where each sentence of T is valid .</sentence>
				<definiendum id="0">theory T</definiendum>
				<definiens id="0">determines a subclass Mod ( T ) of models</definiens>
			</definition>
			<definition id="4">
				<sentence>The mediator is an interdisciplinary theory , e.g. the language of general system theory ( see e.g. \ [ 2\ ] ) .</sentence>
				<definiendum id="0">mediator</definiendum>
				<definiens id="0">an interdisciplinary theory , e.g. the language of general system theory</definiens>
			</definition>
			<definition id="5">
				<sentence>a i ~-~T i The l~mit of a diagram D in TH ~ is a cone { a. : T~T. : T. is object of D\ ] over D such thalt fop anly other cone { Si : R~Ti : Ti is object of D\ ] over D there is a unlque morphism v : R~T such that Bi=~oa i • The colimit of D is defined exectly as above but all the arrows are reversed .</sentence>
				<definiendum id="0">Si</definiendum>
				<definiens id="0">a cone { a. : T~T. : T. is object of D\ ] over D such thalt fop anly other cone {</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>One is the factors governing the intra-sentenceal phenomena , which determines the cognitive meaning of a sentence , and is stated in terms of phrase structures , transformation , various features and the like .</sentence>
				<definiendum id="0">intra-sentenceal phenomena</definiendum>
				<definiens id="0">determines the cognitive meaning of a sentence , and is stated in terms of phrase structures , transformation</definiens>
			</definition>
			<definition id="1">
				<sentence>Given these inputs , SGS tries to generate a sentence of the specified syntactic category from the frames by considering the given 21-conditions .</sentence>
				<definiendum id="0">SGS</definiendum>
				<definiens id="0">tries to generate a sentence of the specified syntactic category from the frames by considering the given 21-conditions</definiens>
			</definition>
			<definition id="2">
				<sentence>A verb can be followed by SERU'SASERU ( causative particles ) or RERU RARERU ( particles of passive or spontaneity etc. ) .</sentence>
				<definiendum id="0">RERU RARERU</definiendum>
			</definition>
			<definition id="3">
				<sentence>CACT and GA , THEME and WO , LOCUS and NI or DE are usually used in pairs .</sentence>
				<definiendum id="0">CACT</definiendum>
				<definiens id="0">usually used in pairs</definiens>
			</definition>
			<definition id="4">
				<sentence>The case system is a basic linguistic structure in itself , but the primary objective of SGS is not the study of case system in Japanese , so SGS utilizes the case system of EXPLUS .</sentence>
				<definiendum id="0">SGS</definiendum>
				<definiens id="0">utilizes the case system of EXPLUS</definiens>
			</definition>
			<definition id="5">
				<sentence>S1 is a subordinate clause .</sentence>
				<definiendum id="0">S1</definiendum>
				<definiens id="0">a subordinate clause</definiens>
			</definition>
			<definition id="6">
				<sentence>P000068 is the frame for a sentencial object .</sentence>
				<definiendum id="0">P000068</definiendum>
			</definition>
</paper>

		<paper id="1077">
</paper>

		<paper id="1073">
</paper>

		<paper id="1029">
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>voc~au ~ing to the use of the same defining lary , LDOEI is a natural extension of LDOCE .</sentence>
				<definiendum id="0">LDOEI</definiendum>
				<definiens id="0">a natural extension of LDOCE</definiens>
			</definition>
			<definition id="1">
				<sentence>As for PROMISE ( not discussed in Akmajian and Heny 1975 ) it could be defined by means of the following feature row : + T3 , + T5 , + V 3 : I promised to go ( T3 ) I promised him to ~o ( V3 ) I promised that I would go ( T5 ) The NP between PROMISE and the TOINFINITIVE is the object ( as in the PERSUADE class ) but it is not the subject of the infinitive .</sentence>
				<definiendum id="0">TOINFINITIVE</definiendum>
				<definiens id="0">the object</definiens>
			</definition>
			<definition id="2">
				<sentence>SECTION THREE : LDOCE DEFINITIONS : AN IR APPROACH TO SEMANTIC AND KNOWLEDGE-OFTHE-WORLD INFORMATION .</sentence>
				<definiendum id="0">LDOCE DEFINITIONS</definiendum>
				<definiens id="0">AN IR APPROACH TO SEMANTIC AND KNOWLEDGE-OFTHE-WORLD INFORMATION</definiens>
			</definition>
			<definition id="3">
				<sentence>HAMMER , for instance , is defined as : `` a tool with a heavy head for driving nails into wood or for striking things to break them or move them '' ( Definition I ) No simple procedure will associate INSTRUMENT with HAMMER .</sentence>
				<definiendum id="0">HAMMER</definiendum>
				<definiens id="0">striking things to break them or move them ''</definiens>
			</definition>
			<definition id="4">
				<sentence>The operato~that STAIRS works with are the following : A ADJ B : A and B occur next to each other and in that order in the document to be retrieved .</sentence>
				<definiendum id="0">ADJ B</definiendum>
				<definiens id="0">A and B occur next to each other and in that order in the document to be retrieved</definiens>
			</definition>
			<definition id="5">
				<sentence>A SYN B : A and B are to be regarded as synonymS\ [ or a given search operation A WITH B : A and B occur in the same sentence A SAME B : A and B occur in the same paragraph NOT B : B does n't occur in the document to be retrieved A AND B : both A &amp; B A OR B : inclusive OR A XOR B : exclusive OR In our system the STAIRS hierarchy would correspond to the following : A. A DOCUMENT -A HOMOGRAPH ( e.g. DOUBT 2 ) or AN ENTRY WITHOUT HOMOGRAPHS ( e.g. PONDEROUS ) in the LDOCE file B. A PARAGRAPH -a specified FIELD within A , e.g. POS ( part of speech ) , GRAMMATICAL CODE , SEMANTIC CODE , TEXT OF THE DEFINITION , ... C. A SENTENCE -any sentence included in the text of a given definition D. A WORD -the various words within a definition or the various codes and POS within a code field or a POS field .</sentence>
				<definiendum id="0">SYN B</definiendum>
				<definiendum id="1">XOR B</definiendum>
				<definiendum id="2">DOCUMENT -A HOMOGRAPH</definiendum>
				<definiendum id="3">, GRAMMATICAL CODE</definiendum>
				<definiens id="0">A and B are to be regarded as synonymS\ [ or a given search operation A WITH B : A and B occur in the same sentence A SAME B : A and B occur in the same paragraph NOT B : B does n't occur in the document to be retrieved A AND B : both A &amp; B A OR B : inclusive OR A</definiens>
				<definiens id="1">PARAGRAPH -a specified FIELD within A , e.g. POS ( part of speech )</definiens>
				<definiens id="2">SENTENCE -any sentence included in the text of a given definition D. A WORD -the various words within a</definiens>
			</definition>
			<definition id="6">
				<sentence>Phrase structure categories are a hard nut to crack , and we will probably have to do without them in a first stage , but lexical categories such as V can be housed in a STAIRS like system : a V is the name of any document that includes V among its POS paragraph .</sentence>
				<definiendum id="0">Phrase structure categories</definiendum>
				<definiendum id="1">V</definiendum>
				<definiens id="0">the name of any document that includes V among its POS paragraph</definiens>
			</definition>
			<definition id="7">
				<sentence>The embedding of STAIRS expressions within STAIRS expressions gives rise to the use of labels such as -- 381-At , BI , etc ; the colon is to be read as `` can be defined as '' '' AI OR A2 AI : BI WITH B2 BI : ~NYTHING ' OR 'SOMETHING ' B2 : CI OR C2 OR C3 CI : -- -'USE~WITH 'FOR ' ADJ V-ING C2 : ~-~R ~ ADJ V-ING C3 : 'MADE ' ADJ 'TO ' ADJ V A2 : B3 WITH B4 B3 . '</sentence>
				<definiendum id="0">ADJ V-ING C3</definiendum>
				<definiens id="0">'' '' AI OR A2 AI : BI WITH B2 BI : ~NYTHING ' OR 'SOMETHING ' B2 : CI OR C2 OR C3 CI : -- -'USE~WITH 'FOR ' ADJ V-ING C2 : ~-~R ~</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>Pen-path consists of stroke vectors and strokewith-stroke combining vectors for a single character , and in case of character succession , character-withcharacter combining vectors add to this .</sentence>
				<definiendum id="0">Pen-path</definiendum>
				<definiens id="0">consists of stroke vectors and strokewith-stroke combining vectors for a single character , and in case of character succession , character-withcharacter combining vectors add to this</definiens>
			</definition>
			<definition id="1">
				<sentence>K.G. ~ , 1 ( 1961 ) 3 ) T. Sakai , M. Nagao , and H. Terai : A Description of Chinese Characters Using Sub-patterns , Johoshori ( Journ .</sentence>
				<definiendum id="0">H. Terai</definiendum>
				<definiens id="0">A Description of Chinese Characters Using Sub-patterns</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>A Japanese sentence consists of a continuous character string without any space between words .</sentence>
				<definiendum id="0">Japanese sentence</definiendum>
			</definition>
			<definition id="1">
				<sentence>A KANJI ( Chinese character ) is used to write nouns and the principal part of a predicate , and expresses the concepts contained in the sentence .</sentence>
				<definiendum id="0">KANJI ( Chinese character</definiendum>
				<definiens id="0">used to write nouns and the principal part of a predicate , and expresses the concepts contained in the sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>A HIRAGANA ( traditional Japanese character ) is used to write conjunctions , adverbs , JODOSHI ( mainly expresses many modalities of a predicate ) and JOSHI ( post-position , mainly expresses case relations ) .</sentence>
				<definiendum id="0">HIRAGANA</definiendum>
				<definiendum id="1">JODOSHI</definiendum>
				<definiendum id="2">JOSHI</definiendum>
				<definiens id="0">expresses many modalities of a predicate )</definiens>
			</definition>
			<definition id="3">
				<sentence>A KATAKANA ( traditional Japanese character ) is used mainly as phonetic signs to write foreign words .</sentence>
				<definiendum id="0">KATAKANA</definiendum>
			</definition>
			<definition id="4">
				<sentence>D 6 { 20t~t~O ( D ) -- 315- ( A ) is written in KATAKANA ( only for ' ~ -- D~ , ~ ' ) and HIRAGANA ( the rests ) without using KANJI .</sentence>
				<definiendum id="0">HIRAGANA</definiendum>
				<definiens id="0">the rests ) without using KANJI</definiens>
			</definition>
			<definition id="5">
				<sentence>Axis Y represents the number of different words in which the same character is used .</sentence>
				<definiendum id="0">Axis Y</definiendum>
				<definiens id="0">the number of different words in which the same character is used</definiens>
			</definition>
</paper>

		<paper id="1087">
			<definition id="0">
				<sentence>The basic hardware to support an IMS consists of a personal computer dedicated to a single user which will have a high resolution , all points addressabte display and a mouse as a pointing device .</sentence>
				<definiendum id="0">IMS</definiendum>
				<definiens id="0">consists of a personal computer dedicated to a single user which will have a high resolution , all points addressabte display and a mouse as a pointing device</definiens>
			</definition>
			<definition id="1">
				<sentence>EMACS ( STALLMAN 1979 ) is a real-time display oriented editor , which can be extended by the user .</sentence>
				<definiendum id="0">EMACS</definiendum>
				<definiens id="0">a real-time display oriented editor , which can be extended by the user</definiens>
			</definition>
			<definition id="2">
				<sentence>It contains speciaI subsystems ( `` modes '' ; see Diagram 2 ) to take advantage of the structures which occur in the systems to be edited .</sentence>
				<definiendum id="0">speciaI subsystems ( `` modes</definiendum>
				<definiens id="0">occur in the systems to be edited</definiens>
			</definition>
			<definition id="3">
				<sentence>Powerful personal computer systems ( like the LISP machine ) contribute to the extensibility and modifiability of an information manipulation system because they make the entire software system interaetively extensible by writing it in a higher level language ( eg LISP ) and allowing the user to redefine the functions composing the innards of the system ( ie they provide the linquistic uniformity which we have mentioned in 1.2 ) .</sentence>
				<definiendum id="0">Powerful personal computer systems</definiendum>
			</definition>
			<definition id="4">
				<sentence>This hypothesis indicates the evolutionary character of complex systems , it implies that linear approximation is an important methodology and that debugging processes have to be understood thoroughly 5 ) problem solving theories about planning ( which operates in a simplified abstraction space ) , analogy ( which forms the basis for recognition methods ) , debugging ( see previous point ) and multiple representations ( see 3.2 ) are not any more only directed towards the understanding of abstract and well-structured problems but investigate ill-structured problems in semantically rich domains ( SIMON 1978 ) 4 ) knowledge about human information processig capabilities ( eg about the limited capacity of our short term memory ) shows that for complex systems there exists a need to prefold information for the user so that more pieces of the whole picture can be maintained in the user 's immediate attention at once 5 ) SIMON ( 1969 ) has provided an insightful analysis of the structure of ce~le× systems ( by showing their hierarchical structure , their property of being `` nearly decomposable '' etc ) One purpose of an IMS is to support the creative aspects of the writing and programming process .</sentence>
				<definiendum id="0">IMS</definiendum>
				<definiens id="0">operates in a simplified abstraction space ) , analogy ( which forms the basis for recognition methods</definiens>
				<definiens id="1">to support the creative aspects of the writing and programming process</definiens>
			</definition>
			<definition id="5">
				<sentence>Inadequate technologies ( eg a typewriter , a batch system ) force the writer or programmer to limit himself to a small set of strategies .</sentence>
				<definiendum id="0">Inadequate technologies</definiendum>
				<definiens id="0">a batch system ) force the writer or programmer to limit himself to a small set of strategies</definiens>
			</definition>
</paper>

		<paper id="1005">
</paper>

		<paper id="1062">
</paper>

		<paper id="1078">
			<definition id="0">
				<sentence>A &lt; Islationism controls State A. &gt; ( 4 ) As mentioned above the `` role '' of a noun word is determined by considering the following three elements : i.e. , ( a ) the predicate verb whieh the noun word depends on ( b ) the meaning of the noun word ( c ) the kaku-joshi which is concatenated to the noun word The basic Japanese sentence pattern is expressed as `` NFINF2 -- NFnPV '' , where NFi , which is called `` meishi-bunsetsu '' , is composed of a noun word and case indicating words , and where PV is a predicate verb .</sentence>
				<definiendum id="0">NFi</definiendum>
				<definiendum id="1">PV</definiendum>
			</definition>
			<definition id="1">
				<sentence>Word recognition is executed in the following two steps , ( Figure 5 ) i.e. , automatic segmentation of the Kanji and Kana character string , and the matching of each segment with entries in the content word dictionary ( `` Jiritsu-go '' dictionary which contains nouns , verbs , etc. ) and the function-word table ( `` Fuzoku-go '' table ) to obtain syntactic and semantic information concerning the word .</sentence>
				<definiendum id="0">Word recognition</definiendum>
				<definiens id="0">automatic segmentation of the Kanji and Kana character string , and the matching of each segment with entries in the content word dictionary</definiens>
				<definiens id="1">contains nouns , verbs , etc. ) and the function-word table ( `` Fuzoku-go '' table ) to obtain syntactic and semantic information concerning the word</definiens>
			</definition>
</paper>

		<paper id="1068">
			<definition id="0">
				<sentence>What this figure explains are : There is `` ~7 ( show ) '' as in a class of verb whose tense is present , and the place where it occurs is explained by `` ~ ( table ) '' ; the object of '~ # show ) '' is a concept of `` ~\ [ ~ ( specification ) '' , and it is connected to `` LSI '' by a 'theme ' relation ; `` LSI '' is an object of verbial concept'i~ ( use ) '' , and has 'aspect ' relation of continuation ; `` gx 9/ , ( system ) '' has 'modifying ' relation of `` ~ : ( this ) '' Causative sentence is typically recognized by an existence of 'causer ' relation .</sentence>
				<definiendum id="0">LSI</definiendum>
				<definiendum id="1">LSI ''</definiendum>
				<definiens id="0">an object of verbial concept'i~ ( use ) ''</definiens>
			</definition>
			<definition id="1">
				<sentence>STATE ( normal ) Fig.3 Conceptual Representation with Primitive Concepts .</sentence>
				<definiendum id="0">STATE</definiendum>
			</definition>
			<definition id="2">
				<sentence>I present ; e~ position ( in~ ... . modifier ~ ... .. system I this J robject theme 'specification Tobject position ( in ) show , | _ , J ' ( table ) J Fig .</sentence>
				<definiendum id="0">e~ position</definiendum>
				<definiens id="0">in ) show , | _ , J ' ( table ) J Fig</definiens>
			</definition>
			<definition id="3">
				<sentence>As in 4 of fig.6 , 'bunsetsu ' `` ~ , '' does not modify `` ~ ~= '' nor `` ~ % '' but `` ~y ~ `` This is because , kakariuke'-condition contains a rule that `` ~ , '' only modifies a predicate `` ~ly~ `` but not others .</sentence>
				<definiendum id="0">kakariuke'-condition</definiendum>
				<definiens id="0">contains a rule</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>Particularly revealing is the comparison of the genres of Informative Prose ( A through J -henceforth INFO ) as a group with the group encompassing Imaginative Prose ( genres K through R -henceforth IMAG ) .</sentence>
				<definiendum id="0">Particularly revealing</definiendum>
			</definition>
			<definition id="1">
				<sentence>This difference is less pronounced with regard to infinitival complements : INFO has a mean of the Corpus mean is 0.27 infinitives per sentence .</sentence>
				<definiendum id="0">INFO</definiendum>
				<definiens id="0">has a mean of the Corpus mean is 0.27 infinitives per sentence</definiens>
			</definition>
</paper>

		<paper id="1075">
</paper>

		<paper id="1081">
			<definition id="0">
				<sentence>Shift codes , together with various special symbols such as ( , ) , \ [ , \ ] , ~ etc , give us useful clues for the data translation .</sentence>
				<definiendum id="0">Shift codes</definiendum>
			</definition>
			<definition id="1">
				<sentence>Data Translator/Verifier -DTV The data translation can be seen as atranslation from linearized character strings to certain organized structures .</sentence>
				<definiendum id="0">Data Translator/Verifier -DTV The data translation</definiendum>
				<definiens id="0">atranslation from linearized character strings to certain organized structures</definiens>
			</definition>
			<definition id="2">
				<sentence>ATN gives us an adequate model of a data transducer .</sentence>
				<definiendum id="0">ATN</definiendum>
				<definiens id="0">gives us an adequate model of a data transducer</definiens>
			</definition>
			<definition id="3">
				<sentence>The alphabet characters are defined as the union of ALPHASMALL and ALPHA-LARGE .</sentence>
				<definiendum id="0">alphabet characters</definiendum>
				<definiens id="0">the union of ALPHASMALL and ALPHA-LARGE</definiens>
			</definition>
			<definition id="4">
				<sentence>BUF-NAME SIZE ( BYTE ) IF-OVERFLOW-STATE SPELLING 40 SPELL-ERROR IDIOM 30 IDIOM-EXPAND One of the typical input errors is the omissions of delimiters which cause serious problems in data translation .</sentence>
				<definiendum id="0">BUF-NAME SIZE</definiendum>
				<definiendum id="1">BYTE</definiendum>
			</definition>
			<definition id="5">
				<sentence>DTV put a message to the error message file which tells at which position the illegal character is found .</sentence>
				<definiendum id="0">DTV</definiendum>
			</definition>
			<definition id="6">
				<sentence>Our data editor accompanied by the DTV is the first step toward developing such data entry systems .</sentence>
				<definiendum id="0">DTV</definiendum>
				<definiens id="0">the first step toward developing such data entry systems</definiens>
			</definition>
			<definition id="7">
				<sentence>HEADWORD , POS , etc. ) \ [ ~ointer /Tlolx : cot 4 '' : pointers to : higher-level files KEY @ TEXT l # 11 # 2 ITAG I Y/~//~ N @ TEXT : pointer to the first ... . rd of text : # 1 , # 2 : number of UNITs occupied to represent the text Text Data File NOTE : TAG indicates what type of information ( EX .</sentence>
				<definiendum id="0">TAG</definiendum>
			</definition>
			<definition id="8">
				<sentence>A TCR consists of a pointer to the corresponding text record , the number of occupied text data records , a tag field etc .</sentence>
				<definiendum id="0">TCR</definiendum>
			</definition>
			<definition id="9">
				<sentence>The tag field indicates what kind of text data are stored in the record .</sentence>
				<definiendum id="0">tag field</definiendum>
				<definiens id="0">indicates what kind of text data are stored in the record</definiens>
			</definition>
			<definition id="10">
				<sentence>al. : A Developmenal Model for Data Translation , ACM SIGFIDET Workshop on Data Description and Access , 1972 ( 2 ) Fry , J.P. , Smith , D.P. et .</sentence>
				<definiendum id="0">al.</definiendum>
				<definiens id="0">A Developmenal Model for Data Translation</definiens>
			</definition>
			<definition id="11">
				<sentence>al. : An Approach to Stored Data Definition and Translation , ACM SIGFIDET Workshop on Data Description and Access , 1972 ( 3 ) Michiels , A. , Moulin , A. , Mullenders , J. , Noel , J. : Exploiting the Longman Computer Files for MT Lexicography and other Purposes , Technical Report , University of Liege , Belgium ( 4 ) Michiels , A. , Moulin , A. , Noel , J. : Working with LDOCE , Technical Report , University of Liege , Belgium ( 5 ) Liu , S. , Heller , J. : A Record Oriented , Grammar Driven Data Translation Model , ACM SIGFIDET Workshop on Data Description , Access and Control , 1974 ( 6 ) Nagao , M. , Tsujii , J. : Data Structure of a Large Japanese Dictionary and Morphological Analysis by Using It , Journal of Information Processing Society of Japan , Vol .</sentence>
				<definiendum id="0">al.</definiendum>
				<definiendum id="1">J. : A Record Oriented</definiendum>
				<definiens id="0">An Approach to Stored Data Definition and Translation , ACM SIGFIDET Workshop on Data Description and Access , 1972 ( 3 ) Michiels , A. , Moulin , A. , Mullenders , J. , Noel , J. : Exploiting the Longman Computer Files for MT Lexicography and other Purposes</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>With a keyboard and supporting system developed at Cornell University , input methods used to identify ideographs are adaptations of wellknown schemes ; innovation is in the addition of automatic machine selection of ambiguously identified characters .</sentence>
				<definiendum id="0">innovation</definiendum>
				<definiens id="0">the addition of automatic machine selection of ambiguously identified characters</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>Therefore we shall define informally this stage of the research a semantic grammar as a set of decision rules ( functions ) mapping a structured list of symbolic expressions or sentences into another list of symbolic expressions or sentences of a language .</sentence>
				<definiendum id="0">decision rules</definiendum>
				<definiens id="0">functions ) mapping a structured list of symbolic expressions or sentences into another list of symbolic expressions or sentences of a language</definiens>
			</definition>
			<definition id="1">
				<sentence>What is here emerging , is the fact that the interpretation of a sentence of a language is the assignment to it a whole set of rules transmapping various sentences from it , that is a sentence of a language can be transmapped into one or many other sentences each of which focuses on different aspects of the original sentence and more important where each transmapping is dependant for its existence on the role and function of the others .</sentence>
				<definiendum id="0">language</definiendum>
				<definiens id="0">the fact that the interpretation of a sentence of a</definiens>
				<definiens id="1">a sentence of a language can be transmapped into one or many other sentences each of which focuses on different aspects of the original sentence and more important where each</definiens>
			</definition>
			<definition id="2">
				<sentence>Sl : ( S ( FNN ( N ( Women ) ) ( Love ) ( N ( Bachelors ) ) ) The structured sentence S O categorial grammar .</sentence>
				<definiendum id="0">Sl</definiendum>
				<definiens id="0">structured sentence S O categorial grammar</definiens>
			</definition>
			<definition id="3">
				<sentence>S 5 ( Women ( x ESSE POSIT ( @ L ) y ) &amp; ( y ESSE POSIT ( ~L ) x ) some bachelors ) Transmap of SN , S 1 with meaning representation of LOVE S 6 ( ( Louise is a woman ) &amp; ( Kate is a woman ) &amp; ( Jane is a woman ) &amp; ( John is a bachelor ) &amp; ( John is a man ) &amp; ( Peter is a man ) &amp; ( Peter is an unmarried man ) &amp; ( Andrew is a bachelor ) &amp; ( Andrew is a man ) &amp; ( Louise loves Andrew ) &amp; ( Jane loves Peter ) &amp; ( Kate loves John ) ) This sentence describes the set of properties of all individuals of this small world .</sentence>
				<definiendum id="0">S 5 ( Women ( x ESSE POSIT</definiendum>
				<definiendum id="1">Louise</definiendum>
				<definiendum id="2">Kate</definiendum>
				<definiendum id="3">Jane</definiendum>
				<definiendum id="4">Andrew</definiendum>
				<definiens id="0">a woman</definiens>
				<definiens id="1">a woman</definiens>
				<definiens id="2">a woman</definiens>
				<definiens id="3">a bachelor</definiens>
				<definiens id="4">a man ) &amp; ( Peter is an unmarried man ) &amp; ( Andrew is a bachelor</definiens>
				<definiens id="5">a man ) &amp; ( Louise loves Andrew ) &amp; ( Jane loves Peter ) &amp; ( Kate loves John ) ) This sentence describes the set of properties of all individuals of this small world</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>13 Text III is the text of Dr. King 's statement on the eve of his death , 14 and Text IV is a sermon delivered by Dr. King at his home church in Atlanta , Georgia .</sentence>
				<definiendum id="0">Text III</definiendum>
				<definiendum id="1">IV</definiendum>
				<definiens id="0">the text of Dr. King 's statement on the eve of his death , 14 and Text</definiens>
			</definition>
			<definition id="1">
				<sentence>Text III consists of 23 sentences , 34 clauses , and 256 words .</sentence>
				<definiendum id="0">Text III</definiendum>
			</definition>
			<definition id="2">
				<sentence>As stated earlier the goals of the analysis are : a ) the recognition and identification of explicit semanticsyntactic markers between sentences , b ) the recognition and identification of explicit semantic-syntactic markers indicating relationships between clauses , c ) the recognition and identification of sentences with simple structures , d ) the recognition and identification of potential terminal points in a sentence , and e ) the recognition of sequential versus embedded clauses in a sentence .</sentence>
				<definiendum id="0">c )</definiendum>
				<definiendum id="1">e</definiendum>
				<definiens id="0">the recognition and identification of sentences with simple structures , d ) the recognition and identification of potential terminal points in a sentence , and</definiens>
			</definition>
			<definition id="3">
				<sentence>It is defined as an ordered 4-tuple : &lt; V , V~ , SC , 'S ' &gt; where V is the vocabulary of terminal symbols ( or words from sentences or items from the lexicon ) ; V '' is the vocabulary of nonterminal symbols ( the semantic codes , etc. ) ; SC is a set of semantic compatibility rules ; and `` S '' is an initial string or sentence .</sentence>
				<definiendum id="0">V ''</definiendum>
				<definiendum id="1">SC</definiendum>
				<definiendum id="2">`` S ''</definiendum>
				<definiens id="0">an ordered 4-tuple : &lt; V , V~ , SC , 'S ' &gt; where V is the vocabulary of terminal symbols ( or words from sentences or items from the lexicon ) ;</definiens>
				<definiens id="1">the vocabulary of nonterminal symbols ( the semantic codes</definiens>
				<definiens id="2">a set of semantic compatibility rules ; and</definiens>
				<definiens id="3">an initial string or sentence</definiens>
			</definition>
			<definition id="4">
				<sentence>RCUTS represents a subset of relators ( R ) .</sentence>
				<definiendum id="0">RCUTS</definiendum>
				<definiens id="0">a subset of relators ( R )</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>Japanese is an agglutinative language and is very far from European languages from structural viewpoints , i.e. the information of type ( b ) or ( c ) is ordinarily given by the annex-expression agglutinated postpositionally to the conceptual expression which gives the information of type ( a ) .</sentence>
				<definiendum id="0">Japanese</definiendum>
				<definiens id="0">an agglutinative language and is very far from European languages from structural viewpoints</definiens>
				<definiens id="1">gives the information of type ( a )</definiens>
			</definition>
			<definition id="1">
				<sentence>A triple consists of a code of the ( refined ) category such as A48 or R56 , a code of the inflectional condition of the connection , and a code of the meaning ; ( 3 ) As to the inflectable expression , the dictionary includes only its stem ; ( 4 ) E-bunsetsu is decomposed from left to right on it by the `` longest-match method '' and all possible analyses are tried in the `` depth-first '' manner ; ( 5 ) The category code such as Mi3 or Y05 , of the noun or yougen is used in the input and dictionary for the actual expression in it .</sentence>
				<definiendum id="0">triple</definiendum>
				<definiens id="0">consists of a code of the ( refined ) category such as A48 or R56 , a code of the inflectional condition of the connection , and a code of the meaning ; ( 3 ) As to the inflectable expression , the dictionary includes only its stem ; ( 4 ) E-bunsetsu is decomposed from left to right on it by the `` longest-match method '' and all possible analyses are tried in the `` depth-first '' manner</definiens>
			</definition>
			<definition id="2">
				<sentence>R70 Ai8 ROi A48 Mi4 TEKI ( NA ) NO DE HA NA ( I ) 3-7 Mi4 TEKI ( NA ) NO D ( E ) HA NA ( I ) 3-8 AS for 3-6 , it was understood that the atomic expression , 'no ' was not a particle ( R70 ) which indicates a kakariuke relation between two nominal E-bunsetsus or a particle ( R01 ) of the meaning of AGENT , but a suffixal expression ( S47 ) which nominalizes the predicative expression .</sentence>
				<definiendum id="0">HA NA</definiendum>
				<definiendum id="1">R70</definiendum>
				<definiens id="0">indicates a kakariuke relation between two nominal E-bunsetsus or a particle ( R01 ) of the meaning of AGENT</definiens>
				<definiens id="1">nominalizes the predicative expression</definiens>
			</definition>
</paper>

		<paper id="1090">
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>Cette classe contient des formes comme 'souvent , rarement , quelquefois , trois fois , plusieurs lois , fr~quemment , toujours , jamais ' etc .</sentence>
				<definiendum id="0">Cette classe contient</definiendum>
			</definition>
			<definition id="1">
				<sentence>La definition admet les deux possibilit~s. Les Guillaumistes et les lecteurs de M. Martin reconnaltront une certaine affinit~ avec le schema bien connu : Si nous traduisons la phrase 'XLa toupie tournait trois fois ' , en un langage logique , nous avons au moins deux possibilit~s. 'trois fois ' est un op~rateur qui porte sur une phrase , l'imparfait est ~galement un op~rateur de phrase .</sentence>
				<definiendum id="0">La definition admet les</definiendum>
				<definiens id="0">trois fois ' , en un langage logique , nous avons au moins deux possibilit~s. 'trois fois ' est un op~rateur qui porte sur une phrase</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>( i ) action sentence , which describes actions \ [ ika wa mizuaraisi , omote no kawa o muku\ ] 'wash and peel the squids ' ( ii ) state sentence , which describes states \ [ zeNbu de yottu ni wakareru\ ] 'it has separated into four pieces ' ( iii ) action-and-state sentence , which describes actions and states \ [ nitatte kure ba sato to miriN o kuwaeru\ ] 'when the mixture comes to a boil , add the sugar and mirin ' ( iv ) feeling sentence , which describes writer 's feeling \ [ soko no fukai kobati ga yoi\ ] 'one had better use the small deep dish ' ( v ) explanation sentence , which describes an explanation about ingredients or cookwares \ [ unerigusi to iu\ ] 'it is called a winding spit ' ( vi ) mixed sentence , which describes more than two of actions , states , feelings and explanations ( except for ( iii ) ) Most sentences from Data are grouped into ( i ) or ( iii ) .</sentence>
				<definiendum id="0">action-and-state sentence</definiendum>
				<definiendum id="1">explanation sentence</definiendum>
				<definiens id="0">describes more than two of actions , states , feelings and explanations</definiens>
			</definition>
			<definition id="1">
				<sentence>A participial adjective indicates an action that should be taken place some time prior to the cooking step in which it occurs .</sentence>
				<definiendum id="0">participial adjective</definiendum>
			</definition>
			<definition id="2">
				<sentence>Nodes ; - , , ~ -C &gt; - : -Q- : Point node , which represents a junction Name node , which represents a name Condition node , which represents a condition Declaration node , which represents to be-a declaration Action node , which represents actions Data node , which represents data Statement node , which represents a statement which is mentioned at this point of a procedure Edges ; -- : Line edge , &gt; : Arrow edge -- ~ : Sequence edge , @ : Selection edge -'~ -- : Repetition edge , -- : Parallel edge Patterns ; Sequence pattern : doing procedures 2,3 , ... and n in that order under a condition C1 Selection pattern : doing one of procedures 2,3 ... . and n under a condition C1 Parallel pattern : doing all procedures 2,3 ... . and n concurrently under a condition C1 Repetition pattern : repeating procedures 2,3 , ... and n in that order under a condition Cl Declaration pattern : declaration of procedures and/or data ( procedures= &gt; actions ) Name or condition node may be omitted in each pattern .</sentence>
				<definiendum id="0">Point node</definiendum>
				<definiens id="0">represents a junction Name node , which represents a name Condition node , which represents a condition Declaration node , which represents to be-a declaration Action node , which represents actions Data node , which represents data Statement node</definiens>
				<definiens id="1">Line edge , &gt; : Arrow edge -- ~ : Sequence edge , @ : Selection edge -'~ -- : Repetition edge , -- : Parallel edge Patterns ; Sequence pattern : doing procedures 2,3 , ... and n in that order under a condition C1 Selection pattern : doing one of procedures 2,3 ... . and n under a condition C1 Parallel pattern : doing all procedures 2,3 ... .</definiens>
			</definition>
</paper>

		<paper id="1091">
			<definition id="0">
				<sentence>The concept of the variety of lexics is defined as the relation between the size of vocabulary and the size of text in the form of V/N ( type-token ratio , or coefficient of variety ) or N/V ( average frequency of word occurrences ) .</sentence>
				<definiendum id="0">concept of the variety of lexics</definiendum>
			</definition>
			<definition id="1">
				<sentence>A verification shows good agreement with empirical data in the initial stages of text formation ( in the limits of about ~ , OOr~ 5,000 tokens which correspond to a short communication ) .</sentence>
				<definiendum id="0">verification</definiendum>
				<definiens id="0">shows good agreement with empirical data in the initial stages of text formation ( in the limits of about ~ , OOr~ 5,000 tokens which correspond to a short communication )</definiens>
			</definition>
			<definition id="2">
				<sentence>Only after balancing the initial formula by the logarithmization of both variables we obtain in ( V/N ) = a ( ln N ) B and the corresponding formula for expressing the relation between V and N : V = Ne -a ( ln N ) B or V = N I a ( ln N ) b ( where b = = B l ) p , which turns out to be the most adequate formula for solving our problems .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">a ( ln N ) B and the corresponding formula for expressing the relation between V and N</definiens>
			</definition>
			<definition id="3">
				<sentence>Table i The empirical size ( V ) and the teoretical size ( V ' ) of vocabulary plotted against the length of the text ( N ) .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">V ' ) of vocabulary plotted against the length of the text ( N )</definiens>
			</definition>
			<definition id="4">
				<sentence>The formula : V '' = Ne -a ( ln N ) B a ) Latvian newspapers ( lexemes ) 6 N V V '' 50000 7065 7025 lOOOOO 98~+ 9919 2ooooo 13389 1351o 300000 16103 15912 lO 6 24000 lO 7 37000 ( a = 0.003736 , B = 2.6304 ) b ) Czech texts of technical sciences ( word forms ) 7 N V V ' 25000 4829 4827 75000 9605 9626 125000 13056 13050 175000 15858 15853 lO 6 40000 l07 ll~O00 ( a = 0.01123 , B = 2.1539 ) c ) Kazakh newspapers ( word forms ) 8 N V V , 25000 9088 9161 50000 15047 1 @ 875 I000OO 23895 23523 150000 29785 50578 106 87000 lO 7 23OOOO ( a = 0.001372 , B = 2.8488 ) d ) Polish belles-lettres ( word forms ) 9 N V V ° 12172 545 @ 3458 29787 6146 604 @ @ 8255 8026 7998 64510 9250 9398 106 33000 107 6OOO0 ( a = 0.00364 , B = 2.6081 ) 602 e ) English texts on electronics ( word forms ) l0 N V V '' 50000 5599 5457 I00000 7855 7728 150000 9361 9371 200000 10582 10682 106 20000 lO 7 58000 ( a = 0.009152 , B = 2.5057 ) g ) Russian texts on electronics ( word forms ) 12 N V V '' 50000 c~+64 9588 lO0000 14062 14168 150000 17265 17805 200000 21468 20818 106 45000 i07 94000 ( a = 0.00 @ 284 , B = 2.5058 ) f ) Rumanian texts on electronics ( word forms ) II N V V '' 50000 6785 6841 lO0000 10281 10070 150000 12477 12479 200000 14292 14454 106 50000 107 68000 ( a = 0.008148 , B = 2.5086 ) Table 2 Prediction on the basis of two empirical points ( marked with an asterisk ) a ) English : literary texts 13 ( word forms ) N V V ° 10051 5009~ 5009 101566 15706 ~ 15709 Prediction : io 9 i00 78 i000 554 2000 700-1000 917 50721 8749 8905 255558 25655 25447 101~252 50406 49280 l07 140000 ( a = 0.007879 , B = 2.2652 ) c ) Russian : A. S. Pushkin 's `` Queen of Spades '' ( lexemes ) 15 N V V '' lOOO ~62 K _ 462 2000 787 ~ 787 Prediction : 5000 1067 1068 40OO 1348 1321 5000 1541 1556 6000 1752 1776 6861 1928 1957 ( the whole book ) ( a = 0.01699 , B = 1.9747 ) b ) Estonian : A. H. Tammsaare 's novell4 `` Truth and Justice '' I ( lexemes ) N V V '' i0000 2114 ~ 2114 20000 5124 m 3124 Prediction : 114124 7548 7207 ( the whole book ) ( a = 0.006714 , B = 2.4521 ) 603 Kuraszkiewicz , W. , Statystyczne badanie slownictwa polskich tekst6wIVI wieku .</sentence>
				<definiendum id="0">Kazakh newspapers</definiendum>
				<definiens id="0">word forms ) l0 N V V '' 50000</definiens>
				<definiens id="1">word forms ) II N V V '' 50000</definiens>
				<definiens id="2">word forms</definiens>
			</definition>
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>If a character string ( ~gb~ ) fits part of an input sentence ( E~b~ l : I~ ) , then the program segments it into parts by the lengths of words in the table and adds the information about the parts of speech and conjugation .</sentence>
				<definiendum id="0">character string</definiendum>
				<definiens id="0">E~b~ l : I~ ) , then the program segments it into parts by the lengths of words in the table and adds the information about the parts of speech and conjugation</definiens>
			</definition>
			<definition id="1">
				<sentence>J ( &gt; ~ ~ lOOg ~ '' , iOOH~ &lt; EZL ' , . PANNKD ~0 1 0 0 G KA , I 0 0 ~IEIINBUNN KUDASAHI . RE TA . Figure 6. Result of Segmentation and Transliteration from Kanji to Roman character it is verb , syushi form. ( 95.8 % ) it is verb , katei form , or demonstrative pronoun , or auxiliary verb~ I ( 92.9 % ) it is verb , meirei form , or noun. ( 63.3 % ) then it is adjective , mizen form , or verb , renyo form. ( 74.2 % ) it is verb , renyo form. ( 79.6 % ) Kanji-hirasana , then it is a verb. ( 94.4 % ) If the vowel of the last hirasana is /a/ , then its conjugation is mizen or renyo form , and if it is /i/ , then it is mizen or renyo if it is /u/ , then it is syushi or rental if it is /e/ , then it is katei or meirei if it is /o/ , then it is meirei i0. If the last character is a numeral , then it is a figure and if it is a sign , then it is a sign. The third method is by word combinations. That is , in Japanese grammer word combination -especially of nouns or verbs and particles or auxiliary verb~ ~is not free. The formula given in Figure 7 is made from this rule. Its format is as follows : i. the word can be used in front of this word which can be used in front of this word plies obligatorily. Figure 8 is the result of automatic classification of parts of speech. The explanation of the codes used in it is as follows : i ( noun ) . E ( verb ) , M ( adjective ) P ( auxiliary ver~ I , R ( particle ) C ( adverb ) , A ( conjanction ) , B ( interjection ) , Y ( sighn ) , X ( figure ) ( i ) ( 2 ) f ( 3 ) # 1 / 1 ® @ ( I ) Figure 7. table for Classification of parts of speech -- 342 ( 1 ) ~'-~ , t. ) ~d ~ `` . ) `` ( I 'I 5 ( 2 ) # g ~ ~b T. L ' , ~5 . ( 3 ) ~ACLIRI H~TE 411RU . I I I ( 5 ) ~ 3 + ( 6 ) 1 R ER EY ( 7 ) 9 iFigure 8. Result of Classification of parts of speech Q ( auxiliary verb~ior particle ) 8 ( 'mizen ' form ) , 9 ( 'renyo ' form ) # ( 'mizen ' or 'renyo ' form ) + ( 'syushi ' or 'rental ' form ) char. ~D b char. ' s freq. 38404 23633 22124 18962 16383 16062 15958 15522 14710 13515 word 's freq. aux.v. &amp; part. other 32588 ( 84.9 % ) 2 ( 0 % ) 2 ( 0.0 % ) 1305 ( 5.5 % ) 64 ( 0.3 % ) 13138 ( 59.4 % ) 17037 ( 89.8 % ) 3 ( 0.0 % ) 10173 ( 62.1 % ) 0 ( 0 % ) 13324 ( 83.0 % ) 0 ( 0 % ) 10569 ( 66.2 % ) l ( 0.0 % ) 17 ( 0.i % ) o ( 0 % ) 14702 ( 99.9 % ) o ( 0 % ) 8351 ( 61.8 % ) 00 ( 0 % ) Figure 9. Result of supervisor 3 , resulting in/~ ' ) / being changed from a verb to a noun ( using the formula for/i/found in Figure 7 ) . The steps in Figure 8 are i. input data Kanji to Roman letters the parts of speech by methods i and 2 ( by table and by word form ) ( l ) ! ~ @ @ ~ ~ ~1~'~ ~ ~ ~ ~ ~ ~ ' b ~. The supervisor program checks the results of the three automatic processing programs and selects the correct results or processes feedback. It also utilizes information obtained through each program. That is , I. The results of the character check ~ttt A , t'b m . TAKUSANN 110 KI blO TA BA lIE RARE MASE Nil DESI TA . 1RiRPRO P PP PPY + O # # + 3+ TAKUSANII 110 KI NO TABANERA RE 1R1R EP PP PPY O # # + 3+ ( 2 ) i~8 &lt; ~L~ ( .J~\ ] , ~ '' '~ • i~8 &lt; ~ ~G .~ ' ~ • ~BMOSIRBKU TE ~ASOBI SUGI TA . fb ~ . MASE NN DESI TA • EMR E EPY +3 # # + ~ 8 &lt; \ [ \ ] d ~ ~ II ~OMOSIROKU TE I~ASOBISUGI TA . EMR EPY +.3 3+ Figure I0. Result of supervisor -- 343-and conversion from kana to Roman letters are used for each program. matic transliteration is used in segmentation. Namely , if the special context is applied , then the program does not segment at that point because the character string is a word. conversion from kana to Roman letters is used in segmentation. Namely , if the consonant of the Romanized Japanese is ( * ) , ( J ) , or ( Q ) -these are used as special small characters in kana -then the program dose not segment at that point. mentation is used in classification. Namely , the program obtains information concerning parts of speech and conjugation through using the table in Figure 5 in segmentation. Checking the results of the processing involves the following : i. Checking particle and auxiliary verb strings obtained by the program at classification. If these strings are impossible in Japanese , then the segmentation was mistaken. The program corrects these. of one character in Japanese except for particles and auxiliary verbs. Figure 9 gives the frequency of some characters and the frequency of words consisting of that character alone. Words of high frequency that are not particles or auxiliary verbs are produced by errors in segmentation. The program then corrects these errors , combining them into longer words. followed by another verb , then it is a compound word and the program corrects the error to produce a longer word. Figure i0 shows the results of the supervisor program. In test sentence i , the program at first segmented / ~ /L~/ ~ / ~ / as auxiliary verbs through the use of the table in Figure 5. But the supervisor program checks and corrects this string and the classification program adds th~ information of verb to/~t~'~/ , as can be seen in Figure i0. In test sentence 2 , the program at first segmented it / # ASOBI/SUGI/TA / , but the supervisor program checked this and corrected this string to the compound word , / # ASOBISUGI/ , plus /TA/. We can process Japanese sentences using these methods and obtain words and various information about these words. With this program we can obtain a rate of correct answers of approximately 90 percent.Y3 We should be able to improve this program at the level of the supervisor and the tables. However , we do n't think that it will be possible to obtain i00 percent correct answers because this system uses Japanese writing and the Japanese writing system is not i00 percent standardized. In addition , if we wish to produce a complete program , it is necessary to process on the basis of syntax and meaning. At persent , this is not the object of our efforts. The National Language Research Institute has been investigating the vocabulary of modern Japanese since 1952 , and has been using the computer in this research since 1966. As a result , some five million words are available as machine readable data. This data contains vari '' ous information such as word frequency , part of speech , class by word origin , and thesaurus number. The thesaurus , Bunrui go ih~o in Japanese , was produced by Doctor Oki Hayashi. It contains about 38,000 words in the natural language of Japanese. We will not explain this program here since we have written a separate report about it ( number 6 in the list of references below ) . Please refer to this report for further details. Figure ii is the result of this process. Acknowledgements Professor Akio Tanaka developed this plan , made a prototype for automatic transliteration from Kan~i to kana , and permitted us to use this program. Mr. Kiyoshi Egawa made a prototype for an automatic segmentation program and permitted us to use it. They also contributed to this study through our 344 discussions with them. Mr. Oki Hayashi furnished us with the opportunity to study this and provided his support for our efforts. WORD WORD ROMANIZED PARTS THESAURUS NUMBER JAPANESE SPEECH NUMBER 01421 : I : E 1 1. 202 ~ , fc 01224 =I=E g 4. 921 ~_5 00224 =I=ERU E+ ~ , £5 01769 =I=ERU E+ ~ , ~t~ 01949 = I KANAKE E8 t-tiE. 01719 =IKI E= ~ @ 01761 =IKI E= ~ 02080 =IKI E= £k~ 02495 =IKI E9 ~ 01146 : IKI 1 ~ 00469 : IKI : O=I=I 1 ~ 02070 : IKIRU E+ 2. 581 ~ $ 02827 : IKIRU E+ 2. 581 ~ka,5 02524 : IKIRU E+ 2. 581 ~5 01970 =IKIRU E+ 2. 581 ~-~,5 02128 : IKIRU E+ 2. 581 ~ &lt; 01278 =IKU E+ ~,4 00438 =IKU M9 ~ , &lt; 00520 : IKU MS ~ &amp; '5 01621 : IKO=U 1 2. 382 ~ , ~9 01667 =IKO=U 1 2. ~2 J~ , ~ 00025 : IGO 1 ~ , ,G 00840 =ISI 1 : ~ : ~ 00258 : I8IKI I ~ilI ! 00551 =ISIKI 1 ~ } 8 00950 =ISIKISA E8 £t ~f F'~\ ] ~ 00285 =ISIKINA=I 1 Notes : *i Auxiliary verb : This term means the bound form which conjugate. It is put Jodoshi in Japanese. *2 / ~ ~g ~6~/ is rightly segmented for / @ la ~/ and /6 ~/. This case is an error of program. *3 A ratio of correct answers is follows. Sample : 2500 words from a high school textbook Segmentation : 91.3 % Transliteration from Kanji to Kana : 95.7 % Clasification of parts of speech : 97.0 % KEYWORD IN CONTEXT -- .~ ' , ~b , ~9~ : ~ ~CI~ { , @ ~±~P 167 &gt; l~J~ { ~ l &lt; llO ) XAd~tll~Ijl ) iiLl @ 11ii ; iiii I~zilI ( Eb , -flJ .</sentence>
				<definiendum id="0">renyo form.</definiendum>
				<definiendum id="1">B</definiendum>
				<definiendum id="2">X</definiendum>
				<definiendum id="3">character string</definiendum>
				<definiens id="0">1 0 0 G KA , I 0 0 ~IEIINBUNN KUDASAHI . RE TA . Figure 6. Result of Segmentation and Transliteration from Kanji to Roman character it is verb</definiens>
				<definiens id="1">explanation of the codes used in it is as follows : i ( noun ) . E ( verb )</definiens>
				<definiens id="2">processing programs and selects the correct results or processes feedback. It also utilizes information obtained through each program. That is , I. The results of the character check ~ttt A</definiens>
				<definiens id="3">program corrects these. of one character in Japanese except for particles and auxiliary verbs. Figure 9 gives the frequency of some characters and the frequency of words consisting of that character alone. Words of high frequency that are not particles or auxiliary verbs are produced by errors</definiens>
				<definiens id="4">contains vari '' ous information such as word frequency , part of speech , class by word origin , and thesaurus number. The thesaurus</definiens>
			</definition>
</paper>

		<paper id="1082">
			<definition id="0">
				<sentence>Roughly speaking an S.L. is a multidimensional table , and is defined as a collection of data of one species arranged at multi-dimensional lattice points corresponding to the combinations of attribute values .</sentence>
				<definiendum id="0">S.L.</definiendum>
				<definiens id="0">a multidimensional table</definiens>
				<definiens id="1">a collection of data of one species arranged at multi-dimensional lattice points corresponding to the combinations of attribute values</definiens>
			</definition>
			<definition id="1">
				<sentence>Generally an S.L. is a mapping F of the direct product of finite sets SI , ... , Sn into an appropriate set A denoted by F : S1 x ... x Sn~ A. ( 2 ) These sets S1 , ... , Sn and their elements will be sometimes called root words and leaf words respectively .</sentence>
				<definiendum id="0">S.L.</definiendum>
				<definiens id="0">a mapping F of the direct product of finite sets SI , ... , Sn into an appropriate set A denoted by F : S1 x ... x Sn~ A. ( 2 ) These sets S1 , ... , Sn and their elements will be sometimes called root words and leaf words respectively</definiens>
			</definition>
			<definition id="2">
				<sentence>Notice that the SML expressions contain the mathematical notations to describe sets and mappings .</sentence>
				<definiendum id="0">SML expressions</definiendum>
			</definition>
			<definition id="3">
				<sentence>LIST B , C ; B = &lt; X : FI ( 1980 , X , male ) &gt; 1,000,000 &gt; ; C = COUNT ( B ) ; In this example B is defined as the set of prefecture X 's with the population value FI ( 1920 , X , male ) &gt; 1,000,000 , and C as COUNT of B , where COUNT is one of aggregate functions prepared in SCLAMS .</sentence>
				<definiendum id="0">COUNT</definiendum>
				<definiens id="0">LIST B , C ; B = &lt; X : FI ( 1980 , X , male ) &gt; 1,000,000 &gt; ; C = COUNT ( B )</definiens>
				<definiens id="1">the set of prefecture X 's with the population value FI ( 1920 , X , male ) &gt; 1,000,000 , and C as COUNT of B , where</definiens>
			</definition>
			<definition id="4">
				<sentence>The types of expressions can be classified into £he following six categories : i ) Numeral or literal constants ; e.g. 1980 , Tokyo , male , etc. 2 ) Aggregate function values ; e.g. COUNT ( x ) , SUM ( y ) , etc. 3 ) S.L. 's values ; e.g. F ( xl ... . , xn ) , etc. 4 ) Set operation formulas ; e.g. x &amp; y , xly , x-y , etc. 5 ) Set definition formulas ; e.g. &lt; 3 , 5 , 7 , ii &gt; , &lt; Tokyo , Nagoya , Osaka &gt; , &lt; xi : F ( xl , ... , xi , ... , xn ) &lt; y &gt; , etc. 6 ) Abbreviate notations for elements of a scale , i.e. leaf words ; e.g. S.l , S.II-20 , etc. • The latter , for example , represents from llth to 20th elements of a scale S. It would be easily seen , from the above explanation , that a query by SML is expressed basically as a set of `` nonprocedural '' local queries , and thus the query as a whole has also of nonprocedural nature .</sentence>
				<definiendum id="0">e.g. COUNT</definiendum>
				<definiendum id="1">e.g. F</definiendum>
				<definiens id="0">xl , ... , xi , ... , xn ) &lt; y &gt;</definiens>
			</definition>
			<definition id="5">
				<sentence>8 ) * Root : Root words , i.e. names of scales , e.g. nen , ken ( year , prefecture ) , etc. 9 ) * Leaf : Leaf words , i.e. elements of scales , e.g. 1980 , Tokyo , otoko ( male ) , etc. l0 * Unit : Words for data units , e.g. en , nin , km ( Yen , person , kilometer ) , etc. ii ) * SL : Names of S.L. 's representing the sort of the S.L. data , usually given at Storage mode , e.g. jinko , TV keiyakusha ( population , TV subscriber ) , etc. 12 ) ** Var : Variable names such as A , B , KEN , etc .</sentence>
				<definiendum id="0">TV keiyakusha</definiendum>
				<definiendum id="1">population , TV subscriber</definiendum>
				<definiens id="0">Root words , i.e. names of scales , e.g. nen , ken ( year , prefecture ) , etc. 9 ) * Leaf : Leaf words , i.e. elements of scales</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>Let S O be the set of all elementary data types e.g. integer , real or string of characters and so on , and F be the set of selector names .</sentence>
				<definiendum id="0">S O</definiendum>
				<definiens id="0">the set of all elementary data types e.g. integer , real or string of characters and so on</definiens>
			</definition>
			<definition id="1">
				<sentence>Schema declaration in our DDL is of the form Relation name = t • where t E S. Now we can define the hierarchical relational database ( HRDB ) as follows .</sentence>
				<definiendum id="0">HRDB</definiendum>
				<definiens id="0">of the form Relation name = t • where t E S. Now we can define the hierarchical relational database</definiens>
			</definition>
			<definition id="2">
				<sentence>( i ) tES 0 = &gt; Dt=E~ , where E~ is the set of data -5 L havlng elementary type t , ( 2 ) tES and t=\ [ Sl : t I ... .. Sn : t n\ ] = &gt; Dt=2Dt~ ×Dt~ × ... ×DL ' L n , where Dt~DtiU { NULL1 , NIJLL2 } .</sentence>
				<definiendum id="0">E~</definiendum>
				<definiens id="0">the set of data -5 L havlng elementary type t</definiens>
			</definition>
			<definition id="3">
				<sentence>EMPi=\ [ $ E : int , NAME : string , EDUCATION : \ [ SCHOOL : string , DEG : string , YR : int\ ] , SKILL : \ [ SNAME : string\ ] , KIDS : \ [ KNAME : string , AGE : int , SEX : string\ ] , SAL : int\ ] Fig.2 is an instance of database with this schema at some state or world .</sentence>
				<definiendum id="0">KIDS</definiendum>
				<definiendum id="1">int</definiendum>
				<definiendum id="2">SEX</definiendum>
				<definiens id="0">string , EDUCATION : \ [ SCHOOL : string</definiens>
			</definition>
			<definition id="4">
				<sentence>Dtype is the smallest set of T A satisfying ( 1 ) ~ ( 4 ) .</sentence>
				<definiendum id="0">Dtype</definiendum>
			</definition>
			<definition id="5">
				<sentence>x ( A ) ~ Tm &lt; a , b &gt; , ( 6 ) A , B E Tm a = &gt; ( A-B ) E Tm t , ( 7 ) A E Tm a = &gt; hA E Tm &lt; s , a &gt; , ( 8 ) A e Tm &lt; s , a &gt; = &gt; VA ETm a ' ( 9 ) A , B E Tm a , P E Tm t = &gt; ( P + A , B ) e Tm a , ( i0 ) A ¢ Tm a , c E CON &lt; s , b &gt; , B e Tm b = &gt; { B/Vc } A £ Tm a , ( Ii ) A e Tm t = &gt; PA e Tm t , FA e Tm t. The additional construct ( i0 ) is introduced by Janssen .</sentence>
				<definiendum id="0">E Tm</definiendum>
				<definiens id="0">a = &gt; hA E Tm &lt; s , a &gt;</definiens>
				<definiens id="1">a , c E CON &lt; s , b &gt; , B e Tm b = &gt; { B/Vc } A £ Tm a , ( Ii ) A e Tm t = &gt; PA e Tm t , FA e Tm t. The additional construct ( i0 ) is introduced by Janssen</definiens>
			</definition>
			<definition id="6">
				<sentence>Frame is defined as the indexed family ( Ma ) aE T of sets , where ( i ) Me=D u { NULL1 , NULL2 } , ( 2 ) Mt=2= { 0,1 } , ( 3 ) M &lt; a , b &gt; =MbMa= { FIF : Ma- &gt; Mb } , ( 4 ) M &lt; s , a &gt; =MaI= { FIF : I- &gt; Ma } U { NULL1 , NULL2 } .</sentence>
				<definiendum id="0">Frame</definiendum>
				<definiens id="0">the indexed family ( Ma ) aE T of sets , where ( i ) Me=D u { NULL1</definiens>
			</definition>
			<definition id="7">
				<sentence>Every descriptor is of the form &lt; R , s I , s 2 , A &gt; , where R is a relation name , s\ ] , s 2 are sequences of selectors , and A is the ~ubset of D t which is accessed by s 2 .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">the form &lt; R , s I , s 2</definiens>
				<definiens id="1">a relation name</definiens>
			</definition>
			<definition id="8">
				<sentence>t For string s , Isl denotes length of s. 223 \ [ &lt; R , s I , s 2 , A &gt; \ ] '= ~iSlk ( ~iSon ( \ [ R ( s01 '' 'S0n ) \ ] '\ [ R'\ ] \ [ is0 n\ ] ^~ .</sentence>
				<definiendum id="0">Isl</definiendum>
				<definiens id="0">denotes length of s. 223 \ [ &lt; R , s I , s 2 , A &gt; \ ] '= ~iSlk ( ~iSon ( \ [ R ( s01 '' 'S0n ) \ ] '\ [ R'\ ] \ [ is0 n\ ] ^~</definiens>
			</definition>
			<definition id="9">
				<sentence>More exactly , the set M of data manipulation statements is defined with the following four cases : I~ ( se t oriented update ) d : descriptor , &lt; f , d &gt; ~ M , where f means arbitrary operation on the object in the answer of d , ( 2 ) ( individual insertion ) ( 2-i ) &lt; R , t &gt; E M , where , R is a relation name and t is a tuple which is intended to be inserted into R , ( 2-2 ) d : descriptor , &lt; d , t &gt; £ M , ( 3 ) ( individual deletion ) ( 3-1 ) ~ &lt; R , t &gt; e M , ( 3-2 ) d : descriptor , ~ &lt; d , t &gt; £ M , ( 4 ) ( set oriented deletion ) ~ &lt; R , S , A &gt; ~ M , where , R is a relation name , s 2 is a sequence of selectors and A is the subset of D t which is accessed by s 2 .</sentence>
				<definiendum id="0">R</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">I~ ( se t oriented update ) d : descriptor , &lt; f , d &gt; ~ M , where f means arbitrary operation on the object in the answer of d</definiens>
				<definiens id="1">a relation name and t is a tuple which is intended to be inserted into R</definiens>
				<definiens id="2">set oriented deletion ) ~ &lt; R , S , A &gt; ~ M , where ,</definiens>
			</definition>
			<definition id="10">
				<sentence>An individual insertion ( 2-1 ) means insertion of a tuple t into a relation R as usual , and an individual insertion ( 2-2 ) means insertion of a tuple t into the all relations which are elements of answer for query d. We can define the meaning of individual deletions ( 3-1 ) , ( 3-2 ) in the same manner as in the case of insertion by changing the word 'insert w by 'delete v. Set oriented deletion statement ( 4 ) deletes all tuples whose values accessed by B 2 are in A from relation R. ~224 In the Montague semantics , a data manipulation statement is translated into forward predicate transformer , which is a function from a state predicate to a state predicate whose type is &lt; s , t &gt; , and which has the format of an intension of an assertion .</sentence>
				<definiendum id="0">Montague semantics</definiendum>
				<definiens id="0">An individual insertion ( 2-1 ) means insertion of a tuple t into a relation R as usual , and an individual insertion ( 2-2 ) means insertion of a tuple t into the all relations which are elements of answer for query d.</definiens>
				<definiens id="1">a function from a state predicate to a state predicate whose type is &lt; s , t &gt; , and which has the format of an intension of an assertion</definiens>
			</definition>
			<definition id="11">
				<sentence>The translation of this update statement is \ [ &lt; =Smith , &lt; EMP3 , MGR , NAME , &lt; EMP3 , MGR , NAME , { John } &gt; &gt; &gt; \ ] ' -- 225 =\ [ EMP3 : =\ [ ~ iN % iM ( ~ jM ( VEMP3 , ( iN ) ( jM ) A ( V\ [ &lt; EMP3 , MGR , NAME , { John } &gt; \ ] ' ( iN ) + JM-Smith , , JMmiM ) ) ) \ ] +\ ] ' =\ [ EMP3 : =\ [ % iNAiM ( ~JM ( vEMP3 ' ( iN ) ( jM ) A ( VA~iM2 ( ~iN2 ( VEMP3 ' ( iN2 ) ( iM2 ) ^iN2-John ' ) ) ( i N ) -+ JM-Smith , , JM-iM ) ) ) \ ] +\ ] ' =~P^~z ( { z/VEMP3 ' } VpAVEMP3 ' ( ~iN % iM ( ~OM ( z ( i N ) ( jM ) ^ ( ~ iN2 ( z ( iN2 ) ( i N ) ^ iN2-John ' ) -+ JM-Smith ' , JM'iN ) ) ) ) ) .</sentence>
				<definiendum id="0">NAME</definiendum>
				<definiendum id="1">jM ) A ( VA~iM2</definiendum>
				<definiens id="0">VEMP3 ' ( iN2 ) ( iM2 ) ^iN2-John ' ) ) ( i N ) -+ JM-Smith , , JM-iM )</definiens>
				<definiens id="1">z ( iN2 ) ( i N ) ^ iN2-John ' ) -+ JM-Smith '</definiens>
			</definition>
			<definition id="12">
				<sentence>( I ) CON s c Tm~ , ( 2 ) VAR a c Tm~ , ( 3 ) A , B E Tm~ = &gt; A+B , A-B , A'B , A÷B e Tm~ , T ' ' = &gt; A\ [ B\ ] E m b , ( 4 ) A E TmSa , b &gt; , B e Tm a ( 5 ) A E Tm~ = &gt; % XaA £ ' Tm &lt; a , b &gt; , ( 6 ) A , B ~ Tm~ = &gt; ( A-B ) ¢ Tm~ , ( 7 ) A , B e Tmta , P E Tm~ = &gt; ( P ÷ A , B ) ~ Tm~ , ( 8 ) A e Tm~ , c E CON &lt; s , b &gt; , x e VARs , B E Tm~ = &gt; { B/c\ [ x\ ] } A ~ Tm~ , ( 9 ) A ~ Tm~ = &gt; PA e Tm~ , FA e Tm~ .</sentence>
				<definiendum id="0">B e Tmta</definiendum>
				<definiens id="0">e Tm~ , c E CON &lt; s , b &gt; , x e VARs , B E Tm~ = &gt; {</definiens>
			</definition>
			<definition id="13">
				<sentence>( I ) M e = D U { NULL1 , NULL2 } , ( 2 ) M t = 2 = { 0 , 1 } , ( 3 ) M s = I , ( 4 ) M &lt; a , b &gt; = MbMa = { FIF : M a - &gt; Mb } ( for a~s ) , ( 5 ) M &lt; s , a &gt; = MaI = { FIF : I - &gt; Ms } U { NULLi , NULL2 } .</sentence>
				<definiendum id="0">MaI</definiendum>
				<definiens id="0">a &gt; =</definiens>
			</definition>
</paper>

		<paper id="1074">
			<definition id="0">
				<sentence>( 3 ) U represents the active cognitions which are relatively independent of concepts .</sentence>
				<definiendum id="0">U</definiendum>
				<definiens id="0">represents the active cognitions which are relatively independent of concepts</definiens>
			</definition>
			<definition id="1">
				<sentence>Table i Classification of words Objective expression ( Wl ) Subjective expression ( W z ) Category Common noun Attribute Dynamle Noun noun ( N ) Abstract noun Pronoun Numeral Adjective Adjective I ( A ) Adjective Common verb Verb Special verb ( Vn ) Abstract verb Uninflected noun modifier Attribute adverb I Atribute adverb Attribute adverb Prefix Nominal suffix Suffix Verbal suffix Special symbols Compound word Auxiliary verb Case postposition Postposition Dependent postposition ( X ) Adverbial postposition Conjunctive postposition Conjunction Assertive adverb Punctuation Comma points Period Symbol Example NA transistor NBA HASSlN ( oseillation ) NBB gEl ( plus ) NC MONO ( thSng ) ND KORE ( this ) NE ICHl ( one ) AA n OOKll ( large ) AB KYUUGEKI ( rapid ) VAn KUWAERU ( add ) VE n HASSIN~SURU ( oscilIate ) VCn gURU ( do ) RR ARU ( eertain ) DA SIDAINI ( gradually ) DB KIWAMETE ( very ) HE HU ( non- ) TA KA ( -ize ) TEn SASERU ( make ) EX ~ , ( , ) RE \ [ .</sentence>
				<definiendum id="0">NBB gEl</definiendum>
				<definiendum id="1">NC MONO ( thSng ) ND KORE</definiendum>
				<definiendum id="2">AA n OOKll</definiendum>
				<definiens id="0">Verbal suffix Special symbols Compound word Auxiliary verb Case postposition Postposition Dependent postposition ( X ) Adverbial postposition Conjunctive postposition Conjunction Assertive adverb Punctuation Comma points Period Symbol Example NA transistor NBA HASSlN ( oseillation )</definiens>
				<definiens id="1">large ) AB KYUUGEKI ( rapid ) VAn KUWAERU ( add ) VE n HASSIN~SURU ( oscilIate ) VCn gURU ( do ) RR ARU ( eertain ) DA SIDAINI ( gradually ) DB KIWAMETE ( very ) HE HU ( non- ) TA KA ( -ize ) TEn SASERU ( make ) EX ~ , ( , ) RE \ [</definiens>
			</definition>
			<definition id="2">
				<sentence>Their constants , such as `` RYeS ( both ) '' , `` KAN ( between ) '' , `` TAHOe ( another ) '' , `` DAI '' , `` KS '' , etc. , are given the priority based on the strength of the connectability to variable , and are stored in constant list .</sentence>
				<definiendum id="0">RYeS</definiendum>
				<definiendum id="1">KAN</definiendum>
				<definiendum id="2">TAHOe</definiendum>
				<definiens id="0">given the priority based on the strength of the connectability to variable , and are stored in constant list</definiens>
			</definition>
</paper>

		<paper id="1009">
</paper>

		<paper id="1017">
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>M3 is a P. M4 is either a T ( erse ) Q ( uestion ) or T ( erse ) l ( nformatlon ) .</sentence>
				<definiendum id="0">M3</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">a P. M4 is either a T ( erse ) Q ( uestion ) or</definiens>
			</definition>
			<definition id="1">
				<sentence>M9 is a P and an S of 3 .</sentence>
				<definiendum id="0">M9</definiendum>
				<definiens id="0">a P and an S of 3</definiens>
			</definition>
			<definition id="2">
				<sentence>MI3 is a TRANS ( posed sentence ) , followed by FS and S of 3 .</sentence>
				<definiendum id="0">MI3</definiendum>
				<definiens id="0">a TRANS ( posed sentence ) , followed by FS and S of 3</definiens>
			</definition>
			<definition id="3">
				<sentence>M27 is a P , a TQ , followed by a S of 9 .</sentence>
				<definiendum id="0">M27</definiendum>
				<definiens id="0">a P , a TQ , followed by a S of 9</definiens>
			</definition>
			<definition id="4">
				<sentence>M29 is a ~ of 3 , a P , a S of 4 .</sentence>
				<definiendum id="0">M29</definiendum>
				<definiens id="0">a ~ of 3 , a P , a S of 4</definiens>
			</definition>
			<definition id="5">
				<sentence>TR ( Terse Reply ) : An elliptical reply , also often Just a NP , e.g. , `` No. '' , `` Probably meters . ''</sentence>
				<definiendum id="0">TR</definiendum>
				<definiens id="0">An elliptical reply , also often Just a NP</definiens>
			</definition>
			<definition id="6">
				<sentence>9 E ( Echo ) : An exact or partial repetition of usually the other speaker 's string .</sentence>
				<definiendum id="0">Echo )</definiendum>
				<definiens id="0">An exact or partial repetition of usually the other speaker 's string</definiens>
			</definition>
			<definition id="7">
				<sentence>ADD ( Added Information ) : An elliptical structure , often NP , used to clarify or complete a previous utterance , often one 's own , e.g. , `` It does n't say anything here about weight , or breaking things down .</sentence>
				<definiendum id="0">ADD ( Added Information )</definiendum>
				<definiens id="0">An elliptical structure</definiens>
			</definition>
			<definition id="8">
				<sentence>~UN ( Truncated ) : An incomplete utterance , voluntarily abandoned .</sentence>
				<definiendum id="0">~UN ( Truncated )</definiendum>
				<definiens id="0">An incomplete utterance</definiens>
			</definition>
			<definition id="9">
				<sentence>FS ( False Start ) : These are also abandoned utterances , but immediately followed by usually syntactically and semantically related ones , e.g. , `` They may , they may be identical classes . ''</sentence>
				<definiendum id="0">FS</definiendum>
				<definiens id="0">These are also abandoned utterances , but immediately followed by usually syntactically and semantically related ones</definiens>
			</definition>
			<definition id="10">
				<sentence>COMP ( Completion ) : Completion of the other speaker 's utterance , distinguished from interruption by the cooperative nature of the utterance , e.g. , `` A : l 've got a lot of ... l 've got B : 2 pages .</sentence>
				<definiendum id="0">COMP</definiendum>
				<definiens id="0">Completion of the other speaker 's utterance , distinguished from interruption by the cooperative nature of the utterance</definiens>
			</definition>
			<definition id="11">
				<sentence>P ( ~hatics ) : The largest subgroup of fragments whose name is borrowed from Mallnowskl 's II term `` phat\ [ c communion '' with which he referred to those vocal utterances that serve to establish social relations rather than the direct purpose of communication .</sentence>
				<definiendum id="0">P ( ~hatics )</definiendum>
				<definiens id="0">The largest subgroup of fragments whose name is borrowed from Mallnowskl 's II term `` phat\ [ c communion '' with which he referred to those vocal utterances that serve to establish social relations rather than the direct purpose of communication</definiens>
			</definition>
			<definition id="12">
				<sentence>TQs were noun phrases which are parsed into sentences if followed by a question mark , e.g. , `` Class of culvert ? ''</sentence>
				<definiendum id="0">TQs</definiendum>
				<definiens id="0">were noun phrases which are parsed into sentences if followed by a question mark</definiens>
			</definition>
			<definition id="13">
				<sentence>`` Conwire is an item . ''</sentence>
				<definiendum id="0">Conwire</definiendum>
				<definiens id="0">an item</definiens>
			</definition>
			<definition id="14">
				<sentence>, `` feet '' in `` List the decks of each shlp with square feet capacity less than 70 . ''</sentence>
				<definiendum id="0">feet</definiendum>
			</definition>
			<definition id="15">
				<sentence>( b ) Punctuation : involves sentence final marks , commas and spaces ; they are well illustrated in Figure 2 .</sentence>
				<definiendum id="0">Punctuation</definiendum>
				<definiens id="0">involves sentence final marks , commas and spaces</definiens>
			</definition>
			<definition id="16">
				<sentence>Robinson , Jane J. , Diagram : A Grammar for Dialogue , SRI International , Technical Note 205 , 1980 .</sentence>
				<definiendum id="0">Diagram</definiendum>
				<definiens id="0">A Grammar for Dialogue</definiens>
			</definition>
</paper>

		<paper id="1070">
			<definition id="0">
				<sentence>log\ [ 1 N1s2 ( n ) \ ] Nn ~ ( 2-1 ) where N is the number of samples in one frame .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of samples in one frame</definiens>
			</definition>
			<definition id="1">
				<sentence>A principal component analysis is applied and the tongue contour vector for vowels r v may be expressed in a linear form as , r v = li=l XTiVi + rL ( 2-5 ) where vi 's ( i=l,2 ... .. p ) are eigenvectors , and r v is a mean vector for vowels which corresponds roughly to the neutral tongue contour .</sentence>
				<definiendum id="0">principal component analysis</definiendum>
				<definiendum id="1">r v</definiendum>
				<definiens id="0">a linear form as , r v = li=l XTiVi + rL ( 2-5 ) where vi 's ( i=l,2 ... .. p ) are eigenvectors , and</definiens>
				<definiens id="1">a mean vector for vowels which corresponds roughly to the neutral tongue contour</definiens>
			</definition>
			<definition id="2">
				<sentence>The length factor SFK which gives a uniform change in the vocal tract length , and the articulatory parameters can be estimated simultaneously , Once the mean vocal tract length is fixed , only the articulatory parameters are estimated .</sentence>
				<definiendum id="0">length factor SFK</definiendum>
				<definiens id="0">gives a uniform change in the vocal tract length , and the articulatory parameters can be estimated simultaneously</definiens>
			</definition>
			<definition id="3">
				<sentence>The phonemic part is an acoustic data which is recognized in the phoneme discrimination part .</sentence>
				<definiendum id="0">phonemic part</definiendum>
				<definiens id="0">an acoustic data which is recognized in the phoneme discrimination part</definiens>
			</definition>
			<definition id="4">
				<sentence>r `` 'calculate the similarity for the BL items I satisfying the R restriction \ [ \ [ Ithen , some of the most similar items , CA arel\ ] \ [ selected as candidates ___~l L ; boundary of similarity ( given ) Fig 3~3 Algorithm to decide some candidates For Kanji , redundant data , which consists of a reading of a compound word , On-yomi , and Kunyomi , are given .</sentence>
				<definiendum id="0">On-yomi</definiendum>
				<definiens id="0">'calculate the similarity for the BL items I satisfying the R restriction \ [ \ [</definiens>
				<definiens id="1">consists of a reading of a compound word ,</definiens>
			</definition>
			<definition id="5">
				<sentence>The similarity S ( D , W ) between the phonemic string D whose length is I , ans W whose length is J is calculated as follows : s ( D , w ) = g ( 1,1 ) /i g ( i , j ) = max { Ll ( i , j ) , La ( i , j ) , Lo ( i , j ) } Ll ( i , j ) = sim ( i , j ) +g ( i+l , j+l ) La ( i , j ) = sim ( i , j+l ) +g ( i+l , j+2 ) +adp ( i , j ) Lo ( i , j ) = sim ( i+l , j ) +g ( i+2 , j +I ) +omp ( j ) g ( l , j ) = sup ( J-j ) g ( i , J ) = sup ( I-i ) where sim ( i , j ) is the similarity between phoneme i and j , and given from the confusion matrix , adp ( i , j ) is the probability indicating that phoneme i is added before phoneme j , omp ( i ) is the probability omitting i , and sup ( i ) is the penalty value of unmatched length .</sentence>
				<definiendum id="0">similarity S</definiendum>
				<definiens id="0">the similarity between phoneme i and j , and given from the confusion matrix</definiens>
				<definiens id="1">the probability indicating that phoneme i is added before phoneme j</definiens>
			</definition>
			<definition id="6">
				<sentence>I.R.E. Vol.19 No.4 pp.219-234 ( 1959 ) ( 5 ) Woods , W.A.'Motivation and Overview of SPEECHLIS : An experimental Prototype for Speech Understanding Research ' IEEE Trans .</sentence>
				<definiendum id="0">W.A.'Motivation</definiendum>
				<definiens id="0">An experimental Prototype for Speech Understanding Research ' IEEE Trans</definiens>
			</definition>
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>Almost all AT systems divide the process of translation in three main logical steps , analysis , transfer and synthesis .</sentence>
				<definiendum id="0">AT systems</definiendum>
				<definiens id="0">divide the process of translation in three main logical steps , analysis , transfer and synthesis</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>And further , the notion of acceptability of sentence is defined by the use of production rules which are applied to the set of propositions stored in memory system .</sentence>
				<definiendum id="0">notion of acceptability of sentence</definiendum>
				<definiens id="0">the use of production rules which are applied to the set of propositions stored in memory system</definiens>
			</definition>
			<definition id="1">
				<sentence>One type of conceptual entities is the use of hiearchcal relation which was actively utilized by the work of Carbonell 's SCHOLAR ( 1970 ) .</sentence>
				<definiendum id="0">conceptual entities</definiendum>
			</definition>
			<definition id="2">
				<sentence>Valence is defined as the capacity of an atom to enter into chemical ( or semantic ) combination with other atoms .</sentence>
				<definiendum id="0">Valence</definiendum>
				<definiens id="0">the capacity of an atom to enter into chemical ( or semantic ) combination with other atoms</definiens>
			</definition>
			<definition id="3">
				<sentence>Simile i. M* no m* SIM M no m. ( m* of M* SIM m of M ) In the above notation , SIM represents a similarity between two nouns in Simile , and a noun denoted a small letter is a part of noun denoted a capital letter .</sentence>
				<definiendum id="0">SIM</definiendum>
				<definiens id="0">a similarity between two nouns in Simile</definiens>
				<definiens id="1">a part of noun denoted a capital letter</definiens>
			</definition>
			<definition id="4">
				<sentence>The ELINGOL is a contex free parser extended at ETL , and it has a semantic processing parts that the user can write any semantic processing program in terms of LISP .</sentence>
				<definiendum id="0">ELINGOL</definiendum>
				<definiens id="0">a contex free parser extended at ETL</definiens>
			</definition>
			<definition id="5">
				<sentence>4 Each dictionary item consists of four parts , the first is item of the word , the second is the syntactic category of the word , the third is the part used in case of some ambiguities , ( MQMIJI NQU~ ( HIL O ) `` ( MQMIJI UMIT &lt; SELF SHOKUBUTSU ) ( PRRT-OP MID ( ~EM-FERTURE &lt; SIZE = , ,4 , ~ ) CPRRT-OF-FEATURE ( H~ ( SIZE ~ CHIISQI ) ) ) ) ) ) ( TE IIOUN ( HIL O ) `` ( TE UNIT ( SELF NIL &gt; ( PRRT-OP NI NBEM ) ( SEM-FEBTURE ( SIZE = ~ ) ( P-PROPERTY = ~ , e-~ . )</sentence>
				<definiendum id="0">MQMIJI NQU~ ( HIL O ) `` ( MQMIJI UMIT &lt; SELF SHOKUBUTSU ) ( PRRT-OP MID ( ~EM-FERTURE</definiendum>
				<definiens id="0">the syntactic category of the word , the third is the part used in case of some ambiguities</definiens>
				<definiens id="1">&lt; SIZE = , ,4 , ~ ) CPRRT-OF-FEATURE ( H~ ( SIZE ~ CHIISQI ) ) ) ) ) ) ( TE IIOUN ( HIL O ) `` ( TE UNIT ( SELF NIL &gt; ( PRRT-OP NI NBEM ) ( SEM-FEBTURE ( SIZE = ~</definiens>
			</definition>
			<definition id="6">
				<sentence>( METR-NOUtl ( HS OF SH~U~UT~U ) ) ) ) ) Fig.4 Dictionary the fourth is the part for representation which the word has .</sentence>
				<definiendum id="0">METR-NOUtl</definiendum>
			</definition>
			<definition id="7">
				<sentence>IL 0 ) ( LG ) ) ( NPP ( HPK NOUH &gt; { NIL 0 ) ( CONS &lt; LG ) ( LIST ( RG &gt; ) ) ) ( IND ( JB RBJV ) &lt; NIL O ) O ) ( HPl ( MPP IMD ) ( NIL O ) £hG ) ) { ttPH CI'tOUM IND ) eMIL 0 ) ( LG ) ) ( MP ( NDUH HDUH ) ( NIL O ) ( NP~_EM ( LG ) ( RG ) ) ) ( HP ( NPN NPP ) ( MIL O ) ( MTSEM3 ( h~ ) ( RG ) ) ) ( tIP IMPI HPP ) ( NIL O ) ( MTSEMI ( LG ) ( RG ) ) ) ( ~P ( MPN HOUN &gt; ( NIL O ) ( MTSEM4 ~'LG ) ( RG ) ) ) { MP ( NPI NOUN ) ( NIL O ) ( MTSEM23 ( LG &gt; ( RG ) ) ) ( HPL ( NPK HPI ) ( MIL 0 ) ( CONS ( LG ) ( RG ) ) ) ( NP ( NPL NOUM~ ( NIL O ) ( MTSEMI ( CDR ( L6 ) ) ( COMS ( CAP ( LG ) ) ( LIST ( RG ) ) ) ) ) ( SENTENCE ~ { P ( ~ , IL 0~ , { ~G ) ) ( $ EtITEHCE ( SENTEI~CE END ) ( r11L 0 ) &lt; LG ) ) 0 Fig. 5 Grammar Each grammar consists of four parts , the first and the second parts represent a contex free rule of A -- B ( C ) , the third is used in case of some ambiguities , and in the fourth part , we describe any semantic processing procedures. In Fig. 5 , the fourth part describe a LISP function for metaphorical semantic processing which is considered in the next section. Procedure fo_j Metaj0horical semantic processing First , a input string must be parsed through ELINGOL and produce a parsing tree which is one of the control structure for semantic analysis. In order to interprete a noun phrase , a meaning of a phrase is constructed by seeking the semantic relation between noun and noun in -- 139 the noun phrase. So , at first , two nouns to be interfered must be chosen , the choice is desided in terms of a syntactic structure and semantic part-whole relation network , because , in Japanese , there are many paraphrase only one noun phrase that has same meaning. Then , a new semantic interpretation is obtained from a intersection which is accomplished through the search of the two noun definitions. When an intersection occurs , the system focuses the matched semantic features extracted in the search to construct an interpretation. Thus , the search process corresponds to the conflict resolution process to produce the `` infered meaning '' . In this way , interpretation of metaphorical use is accomplished. Here , we show the detailed semantic procedure for each cases shown before. ( I ) noun-i + noun-2 , Metaphor Top level function : NPSEM Procedore : By metaphorical interference between noun-I and noun-2 , metaphorical semantics is obtained from a intersection of semantic features between two nouns. ( II i ) M* no m* SIMM no m , Simile Top level function : MTSEMI Procedure : First , by comparing noun semantic between M* and m* to that of M and m , the system can decide the semantic of `` M* no m* '' and `` M no m '' . Then metaphorical semantics is obtained by contrasting noun phrase semantic between the semantic of `` M* no m* '' and that of `` M no m '' . ( II 2 ) M* no m* SIM M ( or m ) , Simile Top level function : MTSEM23 Procedure : First , by comparing two noun semantics between M* and m* , the system can decide the semantic of `` M* no m* '' , then metaphorical semantic is obtained by contrasting the semantic between `` M* no m* '' and M ( or m ) . ( II 3 ) M* ( or m* ) SIMM no m , Simile Top level function : MTSEM3 Procedure : First , by comparing noun semantic between M and m , the system can decide the semantic of `` M no m '' . In this type , noun phrase contrasting has three types. The first type is in case that m* of M* is omitted because of m*=m. In this case , by comparing noun semantic between M* and m* ( =m ) , the system can decide the semantics of `` M* no m* '' , and then , metaphorical semantics is obtained by contrasting noun phrase semantic between `` M* no m* '' and `` M no m '' . The second type is in case that m* of M* is omitted but m* is restricted by META-NOUN description m* ' of m. In this case , by comparing noun semantic between M* and m* ' , the system can decide the semantic of `` M* no m* ' '' , and then , metaphorical semantics is obtained by contrasting noun phrase semantic between `` M* no m ~ '' ' and `` M no m '' . The third type is other cases. In this case , by comparing semantic between M* ( or m* ) and that of `` M no m '' . ( II 4 ) M* ( or m* ) SIM M ( or m ) , Simile Top level function : MTSEM4 Procedure : In this type , semantic procedure is as same as type ( II 3 ) without comparing noun semantic between M and m. Results of case studies Results of some case studies are shown in Fig.6 , Fig. 7 , and Fig.8. \MOCH I HODS° ~EM/EMCE ! SEHTEMCE ... ... ... ... ... ... ... ... ... ... EMD ! ! MP ! ! ! H 0 Ui '' t ... ... ... ... ... .. I ~OUN ! ! ! ! MDCtl I HODF~ * , -I , METFIPHORICFIL It TERF'EREMCE *.i. , , , :MOl-H I I M\ [ T ( SELF J INK \ [ 313pJJTSU ) ( PART-OF ~tlL ) CSEM-FEOTI.IRE ( P-PROPERTY = NF~MEPAKO ~ YAWF~RAI.'A ) &lt; COLOR = WHITE ) &lt; iJEIGHT = ~.~.-i- ) ) ) ( HODF~ Ui'l I T ( SELF NIL ) ' : PORT-OF DO ( JBU TSU ) / ( SEN-FEATURE ( COLOR = $ , -i. ) ( P-PROPERTY = ~* , ~ ) ) ) -- I'IFITCHED SEMFIi'ITIC FEFIFURE -- P-PPOPERTY ===== &gt; ~P-PR\ [ IPERTY = IHRMERAI~A ~ YFIWORAKA ) -- MOTCHE\ ] \ ] SEMROTIC FEF~TUPE -- COLOR ===== &gt; ( ISOLOR = WHITE ) -- PESLILT OF METFAPHOR -- ( HODFI UNIT , ' : :ELF NIL ) , PART-OF DOI_IPBU'T SLI ) ' .</sentence>
				<definiendum id="0">PART-OF ~tlL ) CSEM-FEOTI.IRE</definiendum>
				<definiens id="0">IL 0 ) ( LG ) ) ( NPP ( HPK NOUH &gt; { NIL 0 ) ( CONS &lt; LG ) ( LIST ( RG &gt; ) ) ) ( IND ( JB RBJV ) &lt; NIL O ) O ) ( HPl ( MPP IMD ) ( NIL O ) £hG ) ) { ttPH CI'tOUM IND ) eMIL 0 ) ( LG ) ) ( MP ( NDUH HDUH ) ( NIL O ) ( NP~_EM ( LG ) ( RG ) ) ) ( HP ( NPN NPP ) ( MIL O ) ( MTSEM3 ( h~ ) ( RG ) ) ) ( tIP IMPI HPP ) ( NIL O ) ( MTSEMI ( LG ) ( RG ) ) ) ( ~P ( MPN HOUN &gt; ( NIL O ) ( MTSEM4 ~'LG ) ( RG ) ) ) { MP ( NPI NOUN ) ( NIL O ) ( MTSEM23 ( LG &gt; ( RG ) ) ) ( HPL ( NPK HPI ) ( MIL 0 ) ( CONS ( LG ) ( RG ) ) ) ( NP ( NPL NOUM~ ( NIL O ) ( MTSEMI ( CDR ( L6 ) ) ( COMS ( CAP ( LG ) ) ( LIST ( RG ) ) ) ) ) ( SENTENCE ~ { P ( ~ , IL 0~ , { ~G ) ) ( $ EtITEHCE ( SENTEI~CE END ) ( r11L 0 ) &lt; LG ) ) 0 Fig. 5 Grammar Each grammar consists of four parts , the first and the second parts represent a contex free rule of A -- B ( C ) , the third is used in case of some ambiguities , and in the fourth part , we describe any semantic processing procedures. In Fig. 5 , the fourth part describe a LISP function for metaphorical semantic processing which is considered in the next section. Procedure fo_j Metaj0horical semantic processing First , a input string must be parsed through ELINGOL and produce a parsing tree which is one of the control structure for semantic analysis. In order to interprete a noun phrase , a meaning of a phrase is constructed by seeking the semantic relation between noun</definiens>
				<definiens id="1">desided in terms of a syntactic structure and semantic part-whole relation network , because , in Japanese , there are many paraphrase only one noun phrase that has same meaning. Then , a new semantic interpretation is obtained from a intersection which is accomplished through the search of the two noun definitions. When an intersection occurs , the system focuses the matched semantic features extracted in the search to construct an interpretation. Thus , the search process corresponds to the conflict resolution process to produce the `` infered meaning ''</definiens>
				<definiens id="2">the detailed semantic procedure for each cases shown before. ( I ) noun-i + noun-2 , Metaphor Top level function : NPSEM Procedore : By metaphorical interference between noun-I and noun-2 , metaphorical semantics is obtained from a intersection of semantic features between two nouns. ( II i ) M* no m* SIMM no m , Simile Top level function : MTSEMI Procedure : First , by comparing noun semantic between M* and m* to that of M and m , the system can decide the semantic of `` M* no m* '' and `` M no m '' . Then metaphorical semantics is obtained by contrasting noun phrase semantic between the semantic of `` M* no m* '' and that of `` M no m '' . ( II 2 ) M* no m* SIM M ( or m ) , Simile Top level function : MTSEM23 Procedure : First , by comparing two noun semantics between M* and m* , the system can decide the semantic of `` M* no m* '' , then metaphorical semantic is obtained by contrasting the semantic between `` M* no m* ''</definiens>
				<definiens id="3">the system can decide the semantics of `` M* no m* '' , and then , metaphorical semantics is obtained by contrasting noun phrase semantic between `` M* no m* '' and</definiens>
				<definiens id="4">in case that m* of M* is omitted but m* is restricted by META-NOUN description m* ' of m. In this case , by comparing noun semantic between M* and m* ' , the system can decide the semantic of `` M* no m* ' '' , and then , metaphorical semantics is obtained by contrasting noun phrase semantic between `` M* no m ~ '' '</definiens>
				<definiens id="5">COLOR = WHITE ) &lt; iJEIGHT = ~.~.-i- ) ) ) ( HODF~ Ui'l I T ( SELF NIL ) ' : PORT-OF DO ( JBU TSU ) / ( SEN-FEATURE ( COLOR = $ , -i. ) ( P-PROPERTY = ~* , ~ ) ) ) -- I'IFITCHED SEMFIi'ITIC FEFIFURE -- P-PPOPERTY ===== &gt; ~P-PR\ [ IPERTY = IHRMERAI~A ~ YFIWORAKA ) -- MOTCHE\ ] \ ] SEMROTIC FEF~TUPE -- COLOR ===== &gt; ( ISOLOR = WHITE ) -- PESLILT OF METFAPHOR -- ( HODFI UNIT , ' : :ELF NIL ) , PART-OF DOI_IPBU'T SLI ) '</definiens>
			</definition>
			<definition id="8">
				<sentence>6 Metaphor processing for `` MOCHIHADA '' Result shown in Fig°6 is to deal with noun-noun metaphor `` MOCHIHADA ( a soft white skin ) '' .</sentence>
				<definiendum id="0">MOCHIHADA</definiendum>
				<definiens id="0">a soft white skin ) ''</definiens>
			</definition>
</paper>

		<paper id="1086">
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>COL-semantics hence proceeds in a way different from the semantic processing as it is found in the framework of artificial intelligence ( AI-semantics ) .</sentence>
				<definiendum id="0">COL-semantics</definiendum>
				<definiens id="0">proceeds in a way different from the semantic processing as it is found in the framework of artificial intelligence ( AI-semantics )</definiens>
			</definition>
			<definition id="1">
				<sentence>In Montague grammar this differentiation between a well defined syntactic level and an also well defined semantic level of description is a methodologically necessary consequence of the `` Fregean '' principle .</sentence>
				<definiendum id="0">description</definiendum>
				<definiens id="0">a well defined syntactic level and an also well defined semantic level of</definiens>
			</definition>
			<definition id="2">
				<sentence>g~age KS In analogy with Montague 's `` theory of translation '' in `` Universal Grammar '' we assume that the syntactic structures of natural language ( NL , here German ) and the semantic language ( here KS ) are similar , i.e. there exists a translation function f , such that the following holds : ( l.l. ) Given the categories of a categorial grammar of NL , f is mapping from these categories on the syntactic categories of KS .</sentence>
				<definiendum id="0">g~age KS</definiendum>
				<definiendum id="1">f</definiendum>
				<definiens id="0">In analogy with Montague 's `` theory of translation '' in `` Universal Grammar '' we assume that the syntactic structures of natural language ( NL , here German ) and the semantic language ( here KS ) are similar</definiens>
			</definition>
			<definition id="3">
				<sentence>If m , ~I , ... , ~n are basic categories of German , then f ( ~ ) , f ( ~I ) , ... , f ( # n ) are syntactic categories of K $ .</sentence>
				<definiendum id="0">~I , ...</definiendum>
				<definiens id="0">basic categories of German , then f ( ~ ) , f ( ~I ) , ... , f ( # n ) are syntactic categories of K $</definiens>
			</definition>
			<definition id="4">
				<sentence>A principal term in a formula containing as PRAED the translation of anon content verb is a term that is capable , according to its semantic and syntactic structure , to embed other argument terms o~ the translation of the non content verb as its arguments .</sentence>
				<definiendum id="0">principal term</definiendum>
			</definition>
			<definition id="5">
				<sentence>The operationalized version of the two principles is now after having shifted them onto the KS level : ( 1 : maximality principle ) When a NL-expression has n analysis ( n ~ 2J in level \ ] which only differ in the number of arguments , then the level 2 representation consists of the 'maximal ' level I expression , i.e. the expression containing the largest number of arguments .</sentence>
				<definiendum id="0">KS level</definiendum>
				<definiendum id="1">maximality principle</definiendum>
			</definition>
			<definition id="6">
				<sentence>We now state that PROBE and ENTHALT are 'maximal ' expressions and PROBEI and ENTHALTI must be mapped into them respectively and that further holds : VORLIEG is the translation of the noncontent verb vorliegen PROBE is the PRAED of a second order principal term with respect to a 'plant ' argument ENTHALT is the PRAED of a principal term with respect to a 'sample ' argument Then the two examples of level I are mapped into a single representation on level 2 : \ [ ENTHALT\ [ JOTA\ [ LAMBDA x\ [ PROBE G-L XJ\ ] \ ] AS1\ ] The reduction of synonymous structures in the canonical level of representation meets the criteria of economy as they are necessary in a computer system .</sentence>
				<definiendum id="0">VORLIEG</definiendum>
				<definiendum id="1">PROBE</definiendum>
				<definiens id="0">the translation of the noncontent verb vorliegen</definiens>
			</definition>
			<definition id="7">
				<sentence>Cresswell gives an analogous categorial description for verbs .</sentence>
				<definiendum id="0">Cresswell</definiendum>
			</definition>
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>* ) DEBUT Ai : = # V # # AVOIR # \ [ Bi\ ] ; A2 : =CV ( V UT TRADUITE AI ) : REMONTER FC EN EGV ; DEPLACER A2 EN CV SOUS EGV ; EFFACER ECOP ; EFFACER ECADJ ; A3 : =GP ( GPREP ( CP ( P UT TRADUITE # BOF # ) , EN SUBS GN ( PRON UT TRADUITE # 1L # ) ) ; SI PARCOURS EPH SUJ $ ESUJ ALORS DEBUT SI PARCOURS ESUJ _ OBJD ( ) $ # OBJDALORS DEBUT INSERER CIRC ( EN GP DEPLACE ( EOBJD ) ) EN CIRC\ [ i\ ] SOUS EPH ; COPIER ESUJ EN OBJD SOUS EPH ; DEPLACER A3 EN SUJ SOUS EPH ; FIN SINON DEBUT COPIER ESUJ EN OBJD SOUS EPH ; DEPLACER A3 EN SUJ SOUS EPH ; FIN FIN SINON DEBUT TRADUIRE FC PAR UT ( FC ) ; ECRIRE SOMMET ; FIN FIN SINON DEBUT TRADUIRE FC PAR UT ( FC ) ; ECRIRE SOMMET ; FIN FIN SINON DEBUT TRADUIRE FC PAR # PR2ESENT # \ [ Fi , Pi\ ] ; FIN FIN SINON ( *C. LAINE , LE 27 DECEMBRE 1978 '' ) SI NATURE ( FC ) EST V ALORS DEBUT TRADUIRE FC PAR # PR2ESENTER # \ [ B6\ ] FIN SINON DEBUT TRADUIRE FC PAR UT ( FC ) ; FIN FIN .</sentence>
				<definiendum id="0">OBJDALORS DEBUT INSERER CIRC ( EN GP DEPLACE</definiendum>
				<definiendum id="1">FIN FIN SINON DEBUT TRADUIRE FC PAR UT</definiendum>
				<definiendum id="2">FIN FIN SINON ( *C. LAINE</definiendum>
				<definiens id="0">DEPLACER A3 EN SUJ SOUS EPH ; FIN SINON DEBUT COPIER ESUJ EN OBJD SOUS EPH ; DEPLACER A3 EN SUJ SOUS EPH ; FIN FIN SINON DEBUT TRADUIRE FC PAR UT ( FC )</definiens>
			</definition>
</paper>

		<paper id="1010">
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>3 Procedure of classification `` V~~ -- ~¥ U V T : a set of concepts of under consideration Vp : a class of concepts excluded by preprocessing V V : a set of mutually different matter concepts V~ : a set of non-simple matter concepts ¥C : a set of complex concepts V A : a class of complex concepts of A ~B : a class of complex concepts of B VD : a class of derivative concepts V S : a set of simple matter concepts Vs : a class of similar matter concepts V b : a class of standard concepts Fig .</sentence>
				<definiendum id="0">~B</definiendum>
				<definiens id="0">a set of concepts of under consideration Vp : a class of concepts excluded by preprocessing V V : a set of mutually different matter concepts V~ : a set of non-simple</definiens>
				<definiens id="1">a class of derivative concepts V S : a set of simple matter concepts Vs : a class of similar matter concepts V b : a class of standard concepts Fig</definiens>
			</definition>
			<definition id="1">
				<sentence>, ... ... ... . ~ ... ... ... , IEnglish Descriptive sentences -_J control flow ~process flow -- -~ data flow -- +back-up flow Fig .</sentence>
				<definiendum id="0">IEnglish Descriptive</definiendum>
			</definition>
			<definition id="2">
				<sentence>6 Organization of the system The knowledge system consists of four components , visual , conceptual , linguistic and thesaurus .</sentence>
				<definiendum id="0">knowledge system</definiendum>
				<definiens id="0">consists of four components , visual , conceptual , linguistic and thesaurus</definiens>
			</definition>
			<definition id="3">
				<sentence>The linguistic component consists of dictionaries for the production of Japanese and English sentences .</sentence>
				<definiendum id="0">linguistic component</definiendum>
			</definition>
			<definition id="4">
				<sentence>A picture pattern at ti ( 0~i~n-l ) is paired with the one at ti+l , and processed as follows : i ) Primitive picture recognition and syntactic analysis The picture pattern reader is a curve follower that traces line segments by octagonal scanning .</sentence>
				<definiendum id="0">pattern reader</definiendum>
				<definiens id="0">paired with the one at ti+l , and processed as follows : i ) Primitive picture recognition and syntactic analysis The picture</definiens>
			</definition>
			<definition id="5">
				<sentence>6IJCAI , Tokyo , 1979 , \ [ 13\ ] Evans , T.G. : A Program for the Solution of Geometric Analogy Intelligence-Test Questions , in Minsky , M. ( ed . )</sentence>
				<definiendum id="0">T.G.</definiendum>
				<definiens id="0">A Program for the Solution of Geometric Analogy Intelligence-Test Questions , in Minsky</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>Connotation is defined to constitute structural meanings as a system by which words or sentences of a language are conceptually re &amp; ated to one another .</sentence>
				<definiendum id="0">Connotation</definiendum>
				<definiens id="0">a system by which words or sentences of a language are conceptually re &amp; ated to one another</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , synonymy of two terms x , x'~ T may be given as the equality of the two fuzzy subsets M ( x ) and M ( x ' ) repre78 senting the referential meaning in U X : X ' iff ~L ( Z , X ) = ~L ( Z , X ' ) for all z ( 5 ) Partial synonymy may be defined by a similarity formula introducing some threshold-value s x~ x ' iffl~L ( Z , X ) ~L ( Z , X ' ) &amp; s for all z ( 6 ) Hyponymy of a term x relative to x ' may be explicated as containment of the meaning representing fuzzy sets concerned x~x ' iff~L ( z , x ) ~ ( z , x ' ) for all z ( 7 ) In so far as the operations of complement , intersection and union are concerned which correspond to negation , conjunction and adjunction respectively , there has been some critical discussion lately , particularly on the grounds of experimental results .</sentence>
				<definiendum id="0">synonymy of two terms x , x'~ T</definiendum>
				<definiendum id="1">X ) ~L</definiendum>
				<definiendum id="2">Hyponymy</definiendum>
				<definiens id="0">the equality of the two fuzzy subsets M ( x ) and M ( x ' ) repre78 senting the referential meaning in U X : X ' iff ~L ( Z , X ) = ~L ( Z , X ' ) for all z ( 5 ) Partial synonymy may be defined by a similarity formula introducing some threshold-value s x~ x ' iffl~L ( Z ,</definiens>
				<definiens id="1">of a term x relative to x ' may be explicated as containment of the meaning representing fuzzy sets concerned x~x ' iff~L ( z , x ) ~ ( z , x ' ) for all z ( 7 ) In so far as the operations of complement , intersection and union are concerned which correspond to negation , conjunction and adjunction respectively</definiens>
			</definition>
			<definition id="2">
				<sentence>Negation ( complement ) : ~x : = M ( x ) = I -~L ( Z , X ) for all z ( 8 ) Conjunctio ~ ( intersection ) : x ~ , x ' : = M ( x~x ' ) = minim L ( z , x ) , ~ ( z , x ' ) \ ] for all z ( 9 ) Adjunction ( union ) : x vx ' : = M ( xox ' ) = max\ [ ~L ( Z , X\ ] , ~L ( Z , X ' ) \ ] for all z ( 10 ) Although formally satisfactory as outlined and illustrated by PRUF the approach 's basic assumption concerning the referential nature of natural meaning proves to be crucial for its empirical applicability : in order to determine the membership-grades of a fuzzy set , or fuzzy relation respectively , one has to have access to relevant empirical data defined to constitute the sets , and some operational means to calculate the numerical values from these data .</sentence>
				<definiendum id="0">Negation ( complement )</definiendum>
				<definiens id="0">in order to determine the membership-grades of a fuzzy set , or fuzzy relation respectively , one has to have access to relevant empirical data defined to constitute the sets</definiens>
			</definition>
			<definition id="3">
				<sentence>As the domain of the fuzzy relation ~ contain not only the set of terms iL of a language , T , but also the set of objects and/or processes these terms are believed to denote in the universe , U , both these sets should be accessible in order to let an empirical procedure be devised that could be assigned to ; ~T. '' All that Zadeh /24/ is offering in That respect , stays empirically rather vague .</sentence>
				<definiendum id="0">Zadeh /24/</definiendum>
				<definiens id="0">the set of terms iL of a language , T , but also the set of objects and/or processes these terms</definiens>
			</definition>
			<definition id="4">
				<sentence>This is done by a distance measure ~ , which yields real , non-negative , numeri£al values from an i interval standardized to \ [ 0,1\ ] to denote the distances between any two corpuspoints y , y'~ C. ~I can also be considered a fuzzy , binary relation in the set of all corpus-points y. defined to constitute the corpus space l C ~I : C x C -- ~ I ; I : = \ [ O,1\ ] ; C : : \ [ yi } , i = I ... .. n ( I 4 ) By conditioning this fuzzy relation ~ .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">binary relation in the set of all corpus-points y. defined to constitute the corpus space l C ~I : C x C -- ~ I ; I : = \ [ O,1\ ] ;</definiens>
			</definition>
			<definition id="5">
				<sentence>on the Yi ( or following ( 13 ) the ~ x. respectively ) we get a non-fuzzy i mapping $ 11xi : C -- tin ; U : = I n ( 15 ) This mapping assigns to each y~C ( or x6T respectively ) one and only one socalled meaningor concept-point z defined by the n-tupe !</sentence>
				<definiendum id="0">Yi</definiendum>
				<definiens id="0">following ( 13 ) the ~ x. respectively ) we get a non-fuzzy i mapping $ 11xi : C -- tin ; U : = I n ( 15 ) This mapping assigns to each y~C ( or x6T respectively</definiens>
			</definition>
			<definition id="6">
				<sentence>Given the lemmatized vocabulary V as a proper subset of T of lexical units V : = { xi~ ; i=1 , ... , n employed in a corpus K of natural language texts as specified above K : =~t } ; t=1 , ... , m where m ~st : = S S = st ; I t t=1 ( 25 ) is the sum S of all text-lengths s t measured by the number of lexical units ( tokens ) in the corpus , and m H = ~ ht !</sentence>
				<definiendum id="0">lemmatized vocabulary V</definiendum>
				<definiens id="0">a proper subset of T of lexical units V : = { xi~ ; i=1 , ... , n employed in a corpus K of natural language texts as specified above K : =~t } ; t=1 , ... , m where m ~st : = S S = st</definiens>
				<definiens id="1">the sum S of all text-lengths s t measured by the number of lexical units ( tokens ) in the corpus</definiens>
			</definition>
			<definition id="7">
				<sentence>I ... .. i=i , ~ ( x , xi ) 2 +~t ( x ' , X i ) 2 ; O-Z ~I ( Y'Y ' ) L 2 ( 29 ) and ~2 ( 18 ) reads ~2 ( z , z ) ( in=~1 ( ~1 ( Y , Yi ) 21 ( Y , Yi ) ) 2 ) I/2 I = -I ; ( 3o ) -81 As these distance measures satisfying the conditions are to be considered the metric of the corpus space C and the semantic space U respectively , it should be noted here that so far the assumption of it being Euclidean ( 30 ) is nothing but a fist ( although operational ) guess .</sentence>
				<definiendum id="0">O-Z ~I</definiendum>
				<definiens id="0">the metric of the corpus space C and the semantic space U respectively</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>% N is only matched with the massage number % n , where n is a positive integer .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">a positive integer</definiens>
			</definition>
			<definition id="1">
				<sentence>( iii ) Rule A rule is a set of programs , which carries out a job .</sentence>
				<definiendum id="0">Rule A rule</definiendum>
				<definiens id="0">a set of programs</definiens>
			</definition>
			<definition id="2">
				<sentence>A facet shows kinds of the data ; e.g. , values , constraints , procedures and so on .</sentence>
				<definiendum id="0">facet</definiendum>
				<definiens id="0">shows kinds of the data ; e.g. , values , constraints , procedures and so on</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , assume that there is a micro-actor Taro ( names of micro-actors are underlined ) which corresponds to a man named Taro .</sentence>
				<definiendum id="0">micro-actor Taro</definiendum>
			</definition>
			<definition id="4">
				<sentence>( A~O ( : VALUE ( HUMAN ) ) ) ( MIGHTY ( : VALUE ( 5 ) ) ) ( LIVING ( : VALUE ( ANIMAL ) ) ) ( HEIGHT ( : VALUE ( LITTLE ) ) ) ( PLACE ( Sl ( : VALUE ( PI6 ) ) ) ( s2 ( : VALUE ( PIT ) ) ) ) ( ACT ( Sl ( : V ( MOVE ) ) ( : O ( P20 ) ) ) ( $ 2 ( : V ( MOVE ) ) ( : O ( P20 ) ) ) ) Fig .</sentence>
				<definiendum id="0">MIGHTY</definiendum>
				<definiendum id="1">LIVING</definiendum>
				<definiens id="0">VALUE ( HUMAN ) ) )</definiens>
				<definiens id="1">VALUE ( ANIMAL ) ) ) ( HEIGHT ( : VALUE ( LITTLE ) ) ) ( PLACE ( Sl ( : VALUE ( PI6 ) ) ) ( s2 ( : VALUE ( PIT ) ) ) ) ( ACT ( Sl ( : V ( MOVE ) ) ( : O ( P20 ) ) ) ( $ 2 ( : V ( MOVE ) ) ( : O ( P20 ) ) ) ) Fig</definiens>
			</definition>
			<definition id="5">
				<sentence>In order to deal with sentence ( i ) , we make micro-actor SIMTW which stores the knowledge about a tug of war : e.g. `` A winner is a person who is more powerful than the other . ''</sentence>
				<definiendum id="0">winner</definiendum>
				<definiens id="0">stores the knowledge about a tug of war : e.g. `` A</definiens>
			</definition>
			<definition id="6">
				<sentence>When SIMTW receives both a message from Tom and one from John , it compares the power of Tom with one of John , and Judges a winner .</sentence>
				<definiendum id="0">SIMTW</definiendum>
				<definiens id="0">one of John , and Judges a winner</definiens>
			</definition>
			<definition id="7">
				<sentence>We make micro-actor SIMPUSH which compares the sum of power of Tom and John with the weight of the box , when it receives both a message from Tom and one from John .</sentence>
				<definiendum id="0">micro-actor SIMPUSH</definiendum>
				<definiens id="0">compares the sum of power of Tom and John with the weight of the box , when it receives both a message from Tom and one from John</definiens>
			</definition>
			<definition id="8">
				<sentence>The PIG generates the instructions to display the objects in the simulation in the animation style .</sentence>
				<definiendum id="0">PIG</definiendum>
				<definiens id="0">generates the instructions to display the objects in the simulation in the animation style</definiens>
			</definition>
			<definition id="9">
				<sentence>Then the PIG makes the instructions and sends them to the PDS .</sentence>
				<definiendum id="0">PIG</definiendum>
				<definiens id="0">makes the instructions and sends them to the PDS</definiens>
			</definition>
			<definition id="10">
				<sentence>( 2 ) Spot is a dog .</sentence>
				<definiendum id="0">Spot</definiendum>
				<definiens id="0">a dog</definiens>
			</definition>
			<definition id="11">
				<sentence>\ [ 7\ ] Ogawa , H. and Tanaka , K. : `` A STRUCTURE FOR THE REPRESENTATION OF KNOWLEDGE -- A PROPOSAL FOR A MICR0-ACTOR -- '' , Proc .</sentence>
				<definiendum id="0">STRUCTURE</definiendum>
				<definiens id="0">FOR THE REPRESENTATION OF KNOWLEDGE -- A PROPOSAL FOR A MICR0-ACTOR -- '' , Proc</definiens>
			</definition>
</paper>

		<paper id="1084">
</paper>

		<paper id="1067">
			<definition id="0">
				<sentence>The case structure thus obtained containes the modal part which represents various characteristics of the predicate part in almost independent form of respeTHINGS ctive languages .</sentence>
				<definiendum id="0">modal part</definiendum>
				<definiens id="0">represents various characteristics of the predicate part in almost independent form of respeTHINGS ctive languages</definiens>
			</definition>
			<definition id="1">
				<sentence>But if it is used on detailed syntactic rules of the conventional word usage with the aids of categories of an appropriate level at various stages of mechanical translation , these approaches based on the case grammar will serve fairly well to assign reasonable F EXIStence • ~ ATTRibute PHYSlcal-~POSSession -STate ~STate I RELatio n L MENTal ~PERCEPtual STate L STate -\ ] -EMOTive STate NATURal phenomenon F Physical TRANSfer PHYSical~POSSessive TRANSfer -~ATTRibute TRANSfer ~BODily ACTion a-PRODuction ACTion~ ACTi°n L MENTal ACTion ~ Mental TRANSfer PERCEPtual ACTion EMOTive ACTion THINKing ACTion LIVING object ~HUMan r~ical~ &lt; ANIMal | c~ \ [ NON , LIVING rNATURE OBJECT ~ object -~PRODUCTS ~ MENTal OBJect MENTal SUBJect Fig.l The upper part of the categorical trees. -- 447-semantic roles to various constituents in a source sentence as well as to choose their appropriate equivalents and sentential forms in the target language. From the above considerations , a case structure is assigned to a sentence based on Hornby 's verb patterns. Events of discourse are classified by categories of predicates. Fig.l shows the upper part of the categorical tree used here. The categories used for describing case structures are , in many cases , those appearing at the rightmost in Fig.l. Semantic roles of constituents are described using case labels shown in Table i. Table i Case labels PREDicate , AGent , OBJect , EXPERiencer , RECIPient , SOurce , GOal , INSTRument , DEGree , COMPARison , LOCation , TIME , RANGE , MANNer , ROLE , CAUSE , RESULT , MEANS , PURPOSE , CONDition , MODal ( FORM , TENSE , ASPECT , AFfirmation , MOOD , VOICE ) , °.. By using several pairs of categories and case -labels chosen from the above the main parts of each event are schematically described and called case-frames of events. The first row of Table 2 is the main case-frame of the PTRANS event. The m~in part of a case structure based on a case-frame is transformed into a surface structure using a verb which has a sufficient number of Table 2 Case-frame of PTRANS action and assignment of verb patterns PREDAGPTRANS THINGS VP2B VP2E SUBJect VPI2A VPI6A OBJGOPHYS.OBJ LOCAT U PHYS.OBJ Indirect Direct Object Object links to describe the main part and belongs to an appropriate verb pattern. Table 2 shows verb patterns used for English description of PTRANS. On the other hand , if a verb pattern is given , pairs of categories and case labels of dependants are almost determined every category of the predicate verb as shown in Table 3. Hence , if the verb patterns as well as the categories of a verb are recorded in a word dictionary , the possible case structure can be retrieved at parsing immediately. Table 4 shows a part of pairs of verb patterns and categories about ~take ~. Example i shows several examples of case structures obtained by parsing with the aids of Table After determination of a case structure or a dependency relation of a sentence , there still remain several problems to be solved. One of them is the choice of an appropriate equivalent or a subcategory. The word rtake~ with the verb pattern VP6A and the case structure has fairly many Japanese equivalents. However , this kind of multi-vocal problems can be dissolved in many cases by looking up subcategories of a few designated dependants involved in the case structure and hence in advance by recording them for each equivalent in the word dictionary as shown in Table 4. The other is the choice of an appropriate Japanese postposition corresponding to a case label and a category of a dependant of a verb. Though a good correspondence generally holds between them , there are some exceptions. For these cases , the postposition is recorded at the item of the associated verb in the dictionary and picked up at parsing. DEGPURPOSE MANN QUANT -ACT -ACT ADVer -bial Adjunct to-INF-initive Table 3 Case structures of verb pattern VPI2A SUBJ AGTHINGS IO GOLOCAT U PHYS. OBJ DO OBJPHYS'0BJ VPI2A PREDPTRANS PREDAGRECIPOBJPOSS'TRANS HUM HUM OBJECT PREDMTRANS AGHUM OBJEVENT U MENT.OBJ PResent PARtici -ple EXPERHUM 0 MENT-SUBJ Example i ( i ) VP6A : BODily ACTion ~He took bread. ~ ( PRED-BOD.ACT : eat , MOD : ( TENSE : past ~OICE : active ) , AG-HUM : he , OBJFOOD : bread ) tHe took a taxif ( PRED-BOD.ACT : get-on , MOD : ( TENSE : past , VOICE : active ) , AG-HUM : he , OBJ-VEHICLE : a-taxi ) ( 2 ) VP6A : Mental TRANSfer Cl took his speech. ~ ( PRED-MTRANS : make-a-record-of , MOD : ( TENSE : past , VOICE : active ) , AG-HUM : I , OBJ-EVENT : his-speech ) ( 3 ) VPI5A : Physical TRANSfer ~I took the children to the park. ( PRED-PTRANS : go-leading , MOD : ( TENSE : past , VOICE : active ) , AG-HUM : I , OBJ-HUM : the-children , GO-LOCAT : the-park ) -- 448-Table 4 A part of the item 'take ' in the word-dictionary Verb patterns Categories Meanings ( and the designated conditions ) VP6A BOD.ACT i. get into one 's hand ( OBJ-PHYS-OBJ ) PTRANS I. go carrying ( OBJ-NON.LIVING ) POSS'TRANS i. receive ( OBJ-COMMODITY ) MTRANS i. make a record of ( OBJ-MENT. OBJ : U EVENT ) VP22 PERCEP.ACT 1. suppose ( OBJ-EVENT ) ( 4 ) VP22 : PERCEPtual ACTion ~I took her intelligent. ~ ( PRED-PERCEP.ACT : suppose , MOD : ( TENSE : past , VOICE : active ) , EXPER-HUM : I , OBJ-EVENT : ( PRED-ATTR : intelligent , OBJ-HUM : she ) ) Adverbial phrases and clauses outside verb patterns constitute optional cases. Most of idiomatic phrase prepositions such as rwith respect to p and many subordinate conjunctions such as Cwhen ' and Calthough * determine the case labels and the Japanese postpositions of the following phrases and clauses. On the other hand , many prepositional phrases which respectively have only one prepositions such as c with ~ and Cfor ' require the information about frame of discourse for deter i mination of the case label and the appropriate Japanese postposition. These information is the categories of the object term of the phrase and those of main verb of the governor and furthermore , in some cases , other information about case structure of the verb pattern. For example , an adverbial phrase consisting of a prefixed preposition ~ in ' and an object term with category~PHYSical LOCation ' can be assigned a case label'LOC ' or CGOal ' , but the appropriate postposition and case label are determined by referring to the semantic category of the main verb. The necessary information for determining them are recorded each preposition and subordinate conjunction in the word dictionary. Case structures of noun phrases and clauses have similar forms to those of sentences as follows ; t ( Kr-Cl : t~ , .. • , K6-CL : t , '' • • , K~-Cn : t~ ) ( i ) where t is the main term , ~ s are the dependants and K~-~ is a pair of a case label and a category. The case structure only consisting of several individual objects and prepositions are determined by referring to the categories of the objects and their syntactic patterns linked with the individual prepositions. If a noun phrase contains some verbal nouns , the case structure is constructed based on the verbalized words of some of them. Example 2 ( i ) Creceipt of his letter ~ C-EVENT : receive ( PRED-POSS. TRANS : * , OBJ-PHYS.OBJ : Ietter ( OBJ : * , POSS-HUM : his ) ) ( 2 ) t punctual arrival of trains ' C-EVENT : arrive ( PRED-PTRANS : * , AG-VEHICLE : trains , MANN-MODAL : punctual ) where C denotes a certain case for the outside governor and the symbol * denotes the prefixed term to the case frame containing it. First , processing of negative expressions and idioms are described. They are processed in a somewhat special manner. Negative expressions of English are fairly different from those of Japanese at the points where negative adjectives or adverbs of English correspond to some negative modifiers and a negative auxiliary verb in Japanese. Hence , it -- 449 will be better to express the negative expression of an English sentence in a language-free form at the modal part of the case structure and to generate the corresponding Japanese sentence from that anew. The types of NEGation are classified to INTensified TOTal , TOTal and PARtial ones. At parsing , the value of type is put in the modal part of the case structure together with the terms directly negated except the case when the terms belong to the predicate part of the sentence. Example 3 ( i ) ~No student can solve the problem~ ( PRED-THINK.ACT : solve , MOD : ( AF : ( INT. TOT.NEG , student ) , MOOD : can ) , AG-HUM : student , OBJ -MENT. OBJ : problem ( OBJ : * , DET : def ) ) ( 2 ) ~Converses are not always true. ' ( PRED-ATTR : true , MOD : ( AF : ( PAR'NEG , always ) ) , OBJ-MENT.OBJ : converses , FREQuency : always ) Most of terms of modal cases are omitted here and hereafter except important terms for simplicity. For an efficient parsing , idiomatic expressions are processed with some priority. They are recorded near the heading of the leftmost wnrd among multivocal constituents of the idiom in the word dictionary. If the idiom has a separated form tl~t z as shown in Eq. ( 2 ) , t I s I •.. s~ t z ( 2 ) the syntactic and semantic conditions on the strings of words or phrases sl , • .. , s~ lying between tl and t 2 are recorded together with the partial case structure and the Japanese equivalent of the idiom in the word dictionary as follows ; so ~ that ~ ( DEG-QUALity : so\ [ ADVerb\ ] very* , PRED-ATTR : ~f \ [ ADJective\ ] , RESULT ( that\ [ SUBORDInate CONJunction\ ] ) -EVENT : ~\ [ SENTence\ ] ) , where ~very*~denotes the Japanese word equivalent to the English word ~so ~. When a constituent word. of an idiom is found in a sentence at parsing , it is examined whether~ or not there exist the rest parts of the idiom in the sentence by reading the sentence ahead or by examining the partial case structure already constructed. If there is , and if the intermediate part sl -- . s~ satisfies specified conditions by parsing , the partial case structure and the Japanese equivalent of the idiom are added to those already constructed by referring to the word dictionary. Otherwise , ordinary mode of parsing restarts from term t I Example 4 ~The problem was so difficult that he could not solve it. ~ ( PRED-ATTR : difficult , OBJ-MENT.OBJ : problem ( OBJ * , DET : def ) , DEG-QUAL : so ( very ) , RESULT-EVENT : ( PRED-THINK.ACT : solve , MOD : ( AF : TOT-NEG , MOOD : can ) , AG-HUM : he , OBJ-THINGS : it ) ) The rewriting rules used for parsing are described in a Chomsky-like form as follows~ V l ( K~-C , : tl , IK2-C2 : ~2 ) -- ~ V2 ( K , -CI : tl ) V3 ( r~z~Z : J~Z , VPi ) ( 3 ) V 3 ( FKz62 : ~2 ' K3-C3 : ts ' VP£ ) -- ~ V % ( \ ] K~-~z : $ 2 ' VP { ) V 5 ( K3-C 3 : t 3 ) ( 4 ) where V { ( ~ =i ~ 5 ) denotes a non-terminal symbol , ~~ : ~ a sequent of several pai~s of a case label and a category followed by a term , and VPL a label of a verb pattern. Eq. ( 3 ) represents the type of rewriting rules of a subject and a predicate part , and Eq. ( 4 ) that of a predicate part and a right dependent. The parsing is done from left to right in a bottom-up manner and case structures are constructed in the order of the predicate part , the right part of a verb pattern of the predicate part , the subject part and the optional adverbial parts. This order is considered to be efficient for restricting possible construction of case structures of many sentences of English because the central structure is generally constructed in this order. At first , the partial case structure of the predicate part is constructed. If the main verb is a constituent of an idiom , parsing is done in a manner described in the preceding section. Otherwise , all the case structures for the verb patterns are constructed using the word dictionary and a table of case frames. When a main term t6 with a category CL comes to be included in a partial case structure S } ( ~ =1,2 , ... , m ) as shown in Fig.2 , syntactic and categorical consistency is checked. If the consistency holds , the term with the category C~L is added to the partial case structure S~ with a designated case label K~L. And if the case KiL is designated for dissolving multi-Japanese equivalents of the main verb in the case structure S~ , appropriate Japanese equivalents are extracted by subcategory matching. Otherwise , if the category C~ &amp; of the main term t i does not match one of any partial case structure S~ ( ~=1,2 , ..- , n ) , the category C~L is deleted. Similarly , if a partial case structure requires a main term with a designated semantic category and a designated syntactic category or a position and can not find the term , the partial case structure is deleted. -- 450 ( SUBJ ) ( PRED ) tl t2 t3 KH I Cln , l , I C~31 i L-__4 r~-i L -_.J. , Sm.t I -- _ -- 4 t+ Fig. 2 Construction of case structures In such a manner , the number of partial case structures decreases or increases as a term is added to partial case structures and , in many cases , it reduces to unity at the end of parsing. Some typical differences between English and Japanese sentential forms do not require any case transformation , One of them is negative expressions and processed as already mentioned in section 3.1. The others are oompound noun expressions containing some verbal nouns which are prefered to be verbalized in Japanese sentences. However , these Japanese sentences are directly generated from the case structure obtained by parsing since the case structure of a compound noun expression is constructed in the same way as a sentence when it contains some verbal nouns as mentioned in section 2.3. Hence , the main differences which require some case transformations are those as mentioned as follows. state oriented descriptions Table 5 Case transformations Category of Instances of predicate predicate CAUSE cause , make ENABLE enable ENABLE allow , permit ENABLE prevent PERCEPtual look ACTion overlook PERCEPtual show ACTion EMOTive surprise ACTion please Japanese sentences avoid sentential forms in which non-living objects bring actions such as causative and perceptive ones especially when the actions directly affect human beings and some events associated with them. In Japanese , these sentential forms are replaced by noncausative sentences or by resulted-state expressions using state-oriented verbs. This problem is well known as the non-living subject problems and has been fairly well studied by many linguists. 5 It is shown here that the transformation of the expressions can be processed in terms of case structures in a systematic way. First , the value of the voiee in the modal case of such a sentence is set passive , because in such a situation a passive form is generally a little bit more acceptable than an active form in Japanese. Second , depending on the category of the main verb and the agent , the agent case is renamed to a more appropriate case so that a more natural Japanese sentence is generated. Third , if possible , the equivalent of the predicate part is modified into a more appropriate form such as state-oriented descriptions. Some examples of this modification are shown in Table 5. Case label Modification of for agent predicate CAUSE DELetion MEANS INTRANS VERB MEANS CAUSE MEANS CAUSE REPL by'capable verb *~ REPL by ~not + capable verb *~ INTRANS VERB SOURCE + capable verb REPL by MEANS Cunderstand , ~ CAUSE INTRANS VERB -- 451-The third column of this table designates some candidates of case labels to be renamed for an agent• The appropriate case label and postposition are determined by the category of the term to be filled. The fourth column designates the version into which the equivalent of the predicate part is to be modified. The versions are classified into two kinds. One of them is REPLacement of the predicate part by a new intransitive verb• The other is some modification of a passive Japanese predicate part. The latter is subdivided into DELetion of a causative verb , INTRANSitive VERBalizatio g and addition of the meaning Ccapable~ to it. The version is coded and the code is recorded together with a new Japanese intransitive verb for REPLacement in the word dictionary. In 'Fable 5 , the words with a symbol * denote the equivalent Written in Japanese• At the execution of case transformation , the routine designated by the code is called and carries out the necessary processing. Example 5 By case transformation using Table 5 , two English sentences ( i ) CThe weight of snow caused the shed to collapse ' and ( 2 ) 'Our limitted budget did not allow us to start a new project ' are translated to Japanese sentences corresponding to ( f ) 'Owing to the weight of snow , the shed collapsed ' and ( ~ ) tOwing to our limitted budget , we could not start a new project ~ respectively. The main case transformations used here are as follows ; ( i ) ( PRED-CAUSE : cause , AG-QUANT : weight ( D~ ) , OBJ-EVENT : ( PRED-PHYS.ACT : colIapse , OBJ-PHYS. OBJ : shed ( D2 ) ) ) -- -~ ( PRED-PHYS-ACT : colIapse , OBJ-PHYS.OBJ : shed ( Dz ) , CAUSE-QUANT : weight ( DI ) ) where ~DI : =OBJ-QUANT : * , DET : def , POSS-PHYS.OBJ : snow , D z : =OBJ-PHYS°OBJ : * , DET : def ( 2 ) ( PRED-ENABLE : alIow , MOD : ( TENSE : past , AF : TOT.NEG ) , AG-QUANT : budget ( D3 ) , OBJ-EVENT : ( PRED-ACT : start , AG-HUM : we , OBJ-MENT.ACT : project ( D # ) ) ) -- - &gt; ( PRED-ACT : start , MOD : ( TENSE : past , AF : TOT-NEG , MOOD : can* ) , AG-HUM : we , OBJ-MENT .</sentence>
				<definiendum id="0">MOOD</definiendum>
				<definiendum id="1">V {</definiendum>
				<definiendum id="2">POSS-PHYS.OBJ</definiendum>
				<definiendum id="3">MOD</definiendum>
				<definiendum id="4">AF</definiendum>
				<definiendum id="5">MOD</definiendum>
				<definiendum id="6">AF</definiendum>
				<definiendum id="7">MOOD</definiendum>
				<definiens id="0">used on detailed syntactic rules of the conventional word usage with the aids of categories of an appropriate level at various stages of mechanical translation , these approaches based on the case grammar will serve fairly well to assign reasonable F EXIStence • ~ ATTRibute PHYSlcal-~POSSession -STate ~STate I RELatio n L MENTal ~PERCEPtual STate L STate -\ ] -EMOTive STate NATURal phenomenon F Physical TRANSfer PHYSical~POSSessive TRANSfer -~ATTRibute TRANSfer ~BODily ACTion a-PRODuction ACTion~ ACTi°n L MENTal ACTion ~ Mental TRANSfer PERCEPtual ACTion EMOTive ACTion THINKing ACTion LIVING object ~HUMan r~ical~ &lt; ANIMal | c~ \ [ NON , LIVING rNATURE OBJECT ~ object -~PRODUCTS ~ MENTal OBJect MENTal SUBJect Fig.l The upper part of the categorical trees. -- 447-semantic roles to various constituents in a source sentence as well as to choose their appropriate equivalents and sentential forms in the target language. From the above considerations , a case structure is assigned to a sentence based on Hornby 's verb patterns. Events of discourse are classified by categories of predicates. Fig.l shows the upper part of the categorical tree used here. The categories used for describing case structures are , in many cases , those appearing at the rightmost in Fig.l. Semantic roles of constituents are described using case labels shown in Table i. Table i Case labels PREDicate , AGent , OBJect , EXPERiencer , RECIPient , SOurce , GOal , INSTRument , DEGree , COMPARison , LOCation , TIME , RANGE , MANNer , ROLE , CAUSE , RESULT , MEANS , PURPOSE , CONDition , MODal ( FORM , TENSE , ASPECT , AFfirmation , MOOD , VOICE ) , °.. By using several pairs of categories and case -labels chosen from the above the main parts of each event are schematically described and called case-frames of events. The first row of Table 2 is the main case-frame of the PTRANS event. The m~in part of a case structure based on a case-frame is transformed into a surface structure using a verb which has a sufficient number of Table 2 Case-frame of PTRANS action and assignment of verb patterns PREDAGPTRANS THINGS VP2B VP2E SUBJect VPI2A VPI6A OBJGOPHYS.OBJ LOCAT U PHYS.OBJ Indirect Direct Object Object links to describe the main part and belongs to an appropriate verb pattern. Table 2 shows verb patterns used for English description of PTRANS. On the other hand , if a verb pattern is given , pairs of categories and case labels of dependants are almost determined every category of the predicate verb as shown in Table 3. Hence , if the verb patterns as well as the categories of a verb are recorded in a word dictionary , the possible case structure can be retrieved at parsing immediately. Table 4 shows a part of pairs of verb patterns and categories about ~take ~. Example i shows several examples of case structures obtained by parsing with the aids of Table After determination of a case structure or a dependency relation of a sentence , there still remain several problems to be solved. One of them is the choice of an appropriate equivalent or a subcategory. The word rtake~ with the verb pattern VP6A and the case structure has fairly many Japanese equivalents. However , this kind of multi-vocal problems can be dissolved in many cases by looking up subcategories of a few designated dependants involved in the case structure and hence in advance by recording them for each equivalent in the word dictionary as shown in Table 4. The other is the choice of an appropriate Japanese postposition corresponding to a case label and a category of a dependant of a verb. Though a good correspondence generally holds between them , there are some exceptions. For these cases , the postposition is recorded at the item of the associated verb in the dictionary and picked up at parsing. DEGPURPOSE MANN QUANT -ACT -ACT ADVer -bial Adjunct to-INF-initive Table 3 Case structures of verb pattern VPI2A SUBJ AGTHINGS IO GOLOCAT U PHYS. OBJ DO OBJPHYS'0BJ VPI2A PREDPTRANS PREDAGRECIPOBJPOSS'TRANS HUM HUM OBJECT PREDMTRANS AGHUM OBJEVENT U MENT.OBJ PResent PARtici -ple EXPERHUM 0 MENT-SUBJ Example i ( i ) VP6A : BODily ACTion ~He took bread. ~ ( PRED-BOD.ACT : eat , MOD : ( TENSE : past ~OICE : active ) , AG-HUM : he , OBJFOOD : bread ) tHe took a taxif ( PRED-BOD.ACT : get-on , MOD : ( TENSE : past , VOICE : active ) , AG-HUM : he , OBJ-VEHICLE : a-taxi ) ( 2 ) VP6A : Mental TRANSfer Cl took his speech. ~ ( PRED-MTRANS : make-a-record-of , MOD : ( TENSE : past , VOICE : active ) , AG-HUM : I , OBJ-EVENT : his-speech ) ( 3 ) VPI5A : Physical TRANSfer ~I took the children to the park. ( PRED-PTRANS : go-leading , MOD : ( TENSE : past , VOICE : active ) , AG-HUM : I , OBJ-HUM : the-children , GO-LOCAT : the-park ) -- 448-Table 4 A part of the item 'take ' in the word-dictionary Verb patterns Categories Meanings ( and the designated conditions ) VP6A BOD.ACT i. get into one 's hand ( OBJ-PHYS-OBJ ) PTRANS I. go carrying ( OBJ-NON.LIVING ) POSS'TRANS i. receive ( OBJ-COMMODITY ) MTRANS i. make a record of ( OBJ-MENT. OBJ : U EVENT ) VP22 PERCEP.ACT 1. suppose ( OBJ-EVENT ) ( 4 ) VP22 : PERCEPtual ACTion ~I took her intelligent. ~ ( PRED-PERCEP.ACT : suppose , MOD : ( TENSE : past , VOICE : active ) , EXPER-HUM : I , OBJ-EVENT : ( PRED-ATTR : intelligent , OBJ-HUM : she ) ) Adverbial phrases and clauses outside verb patterns constitute optional cases. Most of idiomatic phrase prepositions such as rwith respect to p and many subordinate conjunctions such as Cwhen ' and Calthough * determine the case labels and the Japanese postpositions of the following phrases and clauses. On the other hand , many prepositional phrases which respectively have only one prepositions such as c with ~ and Cfor ' require the information about frame of discourse for deter i mination of the case label and the appropriate Japanese postposition. These information is the categories of the object term of the phrase and those of main verb of the governor and furthermore , in some cases , other information about case structure of the verb pattern. For example , an adverbial phrase consisting of a prefixed preposition ~ in ' and an object term with category~PHYSical LOCation ' can be assigned a case label'LOC ' or CGOal ' , but the appropriate postposition and case label are determined by referring to the semantic category of the main verb. The necessary information for determining them are recorded each preposition and subordinate conjunction in the word dictionary. Case structures of noun phrases and clauses have similar forms to those of sentences as follows ; t ( Kr-Cl : t~ , .. • , K6-CL : t , '' • • , K~-Cn : t~ ) ( i ) where t is the main term , ~ s are the dependants and K~-~ is a pair of a case label and a category. The case structure only consisting of several individual objects and prepositions are determined by referring to the categories of the objects and their syntactic patterns linked with the individual prepositions. If a noun phrase contains some verbal nouns , the case structure is constructed based on the verbalized words of some of them. Example 2 ( i ) Creceipt of his letter ~ C-EVENT : receive ( PRED-POSS. TRANS : * , OBJ-PHYS.OBJ : Ietter ( OBJ : * , POSS-HUM : his ) ) ( 2 ) t punctual arrival of trains ' C-EVENT : arrive ( PRED-PTRANS : * , AG-VEHICLE : trains , MANN-MODAL : punctual ) where C denotes a certain case for the outside governor and the symbol * denotes the prefixed term to the case frame containing it. First , processing of negative expressions and idioms are described. They are processed in a somewhat special manner. Negative expressions of English are fairly different from those of Japanese at the points where negative adjectives or adverbs of English correspond to some negative modifiers and a negative auxiliary verb in Japanese. Hence , it -- 449 will be better to express the negative expression of an English sentence in a language-free form at the modal part of the case structure and to generate the corresponding Japanese sentence from that anew. The types of NEGation are classified to INTensified TOTal , TOTal and PARtial ones. At parsing , the value of type is put in the modal part of the case structure together with the terms directly negated except the case when the terms belong to the predicate part of the sentence. Example 3 ( i ) ~No student can solve the problem~ ( PRED-THINK.ACT : solve , MOD : ( AF : ( INT. TOT.NEG , student ) ,</definiens>
				<definiens id="1">student , OBJ -MENT. OBJ : problem ( OBJ : * , DET : def ) ) ( 2 ) ~Converses are not always true. ' ( PRED-ATTR : true , MOD : ( AF : ( PAR'NEG , always ) ) , OBJ-MENT.OBJ : converses , FREQuency : always ) Most of terms of modal cases are omitted here and hereafter except important terms for simplicity. For an efficient parsing , idiomatic expressions are processed with some priority. They are recorded near the heading of the leftmost wnrd among multivocal constituents of the idiom in the word dictionary. If the idiom has a separated form tl~t z as shown in Eq. ( 2 ) , t I s I •.. s~ t z ( 2 ) the syntactic and semantic conditions on the strings of words or phrases sl , • .. , s~ lying between tl and t 2 are recorded together with the partial case structure and the Japanese equivalent of the idiom in the word dictionary as follows ; so ~ that ~ ( DEG-QUALity : so\ [ ADVerb\ ] very* , PRED-ATTR : ~f \ [ ADJective\ ] , RESULT ( that\ [ SUBORDInate CONJunction\ ] ) -EVENT : ~\ [ SENTence\ ] ) , where ~very*~denotes the Japanese word equivalent to the English word ~so ~. When a constituent word. of an idiom is found in a sentence at parsing , it is examined whether~ or not there exist the rest parts of the idiom in the sentence by reading the sentence ahead or by examining the partial case structure already constructed. If there is , and if the intermediate part sl -- . s~ satisfies specified conditions by parsing , the partial case structure and the Japanese equivalent of the idiom are added to those already constructed by referring to the word dictionary. Otherwise , ordinary mode of parsing restarts from term t I Example 4 ~The problem was so difficult that he could not solve it. ~ ( PRED-ATTR : difficult , OBJ-MENT.OBJ : problem ( OBJ * , DET : def ) , DEG-QUAL : so ( very ) , RESULT-EVENT : ( PRED-THINK.ACT : solve , MOD : ( AF : TOT-NEG , MOOD : can ) , AG-HUM : he , OBJ-THINGS : it ) ) The rewriting rules used for parsing are described in a Chomsky-like form as follows~ V l ( K~-C , : tl</definiens>
				<definiens id="2">a non-terminal symbol , ~~ : ~ a sequent of several pai~s of a case label and a category followed by a term , and VPL a label of a verb pattern. Eq. ( 3 ) represents the type of rewriting rules of a subject and a predicate part , and Eq. ( 4 ) that of a predicate part and a right dependent. The parsing is done from left to right in a bottom-up manner and case structures are constructed in the order of the predicate part , the right part of a verb pattern of the predicate part , the subject part and the optional adverbial parts. This order is considered to be efficient for restricting possible construction of case structures of many sentences of English because the central structure is generally constructed in this order. At first , the partial case structure of the predicate part is constructed. If the main verb is a constituent of an idiom , parsing is done in a manner described in the preceding section. Otherwise , all the case structures for the verb patterns are constructed using the word dictionary and a table of case frames. When a main term t6 with a category CL comes to be included in a partial case structure S } ( ~ =1,2 , ... , m ) as shown in Fig.2 , syntactic and categorical consistency is checked. If the consistency holds , the term with the category C~L is added to the partial case structure S~ with a designated case label K~L. And if the case KiL is designated for dissolving multi-Japanese equivalents of the main verb in the case structure S~ , appropriate Japanese equivalents are extracted by subcategory matching. Otherwise , if the category C~ &amp; of the main term t i does not match one of any partial case structure S~ ( ~=1,2 , ..- , n ) , the category C~L is deleted. Similarly , if a partial case structure requires a main term with a designated semantic category and a designated syntactic category or a position and can not find the term , the partial case structure is deleted. -- 450 ( SUBJ ) ( PRED ) tl t2 t3 KH I Cln , l , I C~31 i L-__4 r~-i L -_.J. , Sm.t I -- _ -- 4 t+ Fig. 2 Construction of case structures In such a manner , the number of partial case structures decreases or increases as a term is added to partial case structures and , in many cases , it reduces to unity at the end of parsing. Some typical differences between English and Japanese sentential forms do not require any case transformation , One of them is negative expressions and processed as already mentioned in section 3.1. The others are oompound noun expressions containing some verbal nouns which are prefered to be verbalized in Japanese sentences. However , these Japanese sentences are directly generated from the case structure obtained by parsing since the case structure of a compound noun expression is constructed in the same way as a sentence when it contains some verbal nouns as mentioned in section 2.3. Hence , the main differences which require some case transformations are those as mentioned as follows. state oriented descriptions Table 5 Case transformations Category of Instances of predicate predicate CAUSE cause , make ENABLE enable ENABLE allow , permit ENABLE prevent PERCEPtual look ACTion overlook PERCEPtual show ACTion EMOTive surprise ACTion please Japanese sentences avoid sentential forms in which non-living objects bring actions such as causative and perceptive ones especially when the actions directly affect human beings and some events associated with them. In Japanese , these sentential forms are replaced by noncausative sentences or by resulted-state expressions using state-oriented verbs. This problem is well known as the non-living subject problems and has been fairly well studied by many linguists. 5 It is shown here that the transformation of the expressions can be processed in terms of case structures in a systematic way. First , the value of the voiee in the modal case of such a sentence is set passive , because in such a situation a passive form is generally a little bit more acceptable than an active form in Japanese. Second , depending on the category of the main verb and the agent , the agent case is renamed to a more appropriate case so that a more natural Japanese sentence is generated. Third , if possible , the equivalent of the predicate part is modified into a more appropriate form such as state-oriented descriptions. Some examples of this modification are shown in Table 5. Case label Modification of for agent predicate CAUSE DELetion MEANS INTRANS VERB MEANS CAUSE MEANS CAUSE REPL by'capable verb *~ REPL by ~not + capable verb *~ INTRANS VERB SOURCE + capable verb REPL by MEANS Cunderstand , ~ CAUSE INTRANS VERB -- 451-The third column of this table designates some candidates of case labels to be renamed for an agent• The appropriate case label and postposition are determined by the category of the term to be filled. The fourth column designates the version into which the equivalent of the predicate part is to be modified. The versions are classified into two kinds. One of them is REPLacement of the predicate part by a new intransitive verb• The other is some modification of a passive Japanese predicate part. The latter is subdivided into DELetion of a causative verb , INTRANSitive VERBalizatio g and addition of the meaning Ccapable~ to it. The version is coded and the code is recorded together with a new Japanese intransitive verb for REPLacement in the word dictionary. In 'Fable 5 , the words with a symbol * denote the equivalent Written in Japanese• At the execution of case transformation , the routine designated by the code is called and carries out the necessary processing. Example 5 By case transformation using Table 5 , two English sentences ( i ) CThe weight of snow caused the shed to collapse ' and ( 2 ) 'Our limitted budget did not allow us to start a new project ' are translated to Japanese sentences corresponding to ( f ) 'Owing to the weight of snow , the shed collapsed '</definiens>
				<definiens id="3">main case transformations used here are as follows ; ( i ) ( PRED-CAUSE : cause , AG-QUANT : weight ( D~ ) , OBJ-EVENT : ( PRED-PHYS.ACT : colIapse , OBJ-PHYS. OBJ : shed ( D2 ) ) ) -- -~ ( PRED-PHYS-ACT : colIapse , OBJ-PHYS.OBJ : shed ( Dz ) , CAUSE-QUANT : weight ( DI ) ) where ~DI : =OBJ-QUANT : * , DET : def ,</definiens>
			</definition>
			<definition id="2">
				<sentence>, I ... . K~ : s~ ) ) I -- ~ ( KL : t ~ ( K~ : tl , Kz : tz , ... , , K~ : * ) ~ = KI : sI , • • , K~ : s~ ) 5 ) where the double-underlined case means that a thematic fronting and addition of a certain thematic postposition is done for the underlined case in generation of a Japanese sentence -- 453-I l PRE0 I principal clause referred subordinate clause the next sentence ( a ) a source sentence referred I parts of subordinate principal principal clause clause clause ( b ) a Japanese sentence the next sentence Fig .</sentence>
				<definiendum id="0">next sentence</definiendum>
			</definition>
</paper>

		<paper id="1089">
			<definition id="0">
				<sentence>Each item is defined by its coordinates ( DN , SN , IN ) where DN is the document number , SN the sentence number and IN the item number within a sentence .</sentence>
				<definiendum id="0">DN</definiendum>
				<definiens id="0">the document number , SN the sentence number and IN the item number within a sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>In order to normalize the function , we take b ( s , t ) ~R ( S , t ) ~ where f ( t ) is the number of occurrences of t for all local documents D£ 0 - &lt; ~R ( S , t ) &lt; I. Through this function , we obtain for an item s a reference vector R which is a list of items t s related to s , such as DR ( S , t ) is greater ( or equal ) than a threshold-e .</sentence>
				<definiendum id="0">f</definiendum>
				<definiens id="0">the number of occurrences of t for all local documents D£ 0</definiens>
				<definiens id="1">a list of items t s related to s</definiens>
			</definition>
			<definition id="2">
				<sentence>The main problem of updating is to take into account `` liaisons '' , `` proximities '' or `` similarities '' between the already registered items in the thesaurus and the new liaisons found after a new query .</sentence>
				<definiendum id="0">updating</definiendum>
				<definiens id="0">similarities '' between the already registered items in the thesaurus and the new liaisons found after a new query</definiens>
			</definition>
			<definition id="3">
				<sentence>Concerning an item x of T~ , three reference vectors ( and their associated functions ) can be yielded : R , PS and T which are the • X sets of items t re~ated to x x respectlvely considered in the treated local context , in one or .</sentence>
				<definiendum id="0">PS</definiendum>
				<definiens id="0">T which are the • X sets of items t re~ated to x x respectlvely considered in the treated local context , in one or</definiens>
			</definition>
</paper>

		<paper id="1079">
			<definition id="0">
				<sentence>Line ( 4 ) sorts the sequence obtained ( SORT is a built-in function ) , and Line ( 5 ) removes the word added to the beginning of the string in line ( 3 ) .</sentence>
				<definiendum id="0">SORT</definiendum>
			</definition>
			<definition id="1">
				<sentence>Sequences Post-X has a number of facilities that apply generally to sequences of items .</sentence>
				<definiendum id="0">Sequences Post-X</definiendum>
				<definiens id="0">a number of facilities that apply generally to sequences of items</definiens>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>KAUS is a logic machine based on the axiomatic set theory and it has capabilities of deductive inference and automatic program generation of database access .</sentence>
				<definiendum id="0">KAUS</definiendum>
				<definiens id="0">a logic machine based on the axiomatic set theory</definiens>
			</definition>
			<definition id="1">
				<sentence>Then the power set of PERSON ( we denote it by @ PERSON ) comprises all the subsets of PERSON : @ PERSON = \ [ MAN , WOMAN , BOY , GIRL ... . ... . # JOHN , # MARY ... . \ ] ( 2 ) Hereupon , let us consider the ordinary first order predicate logic at a moment .</sentence>
				<definiendum id="0">PERSON )</definiendum>
				<definiens id="0">comprises all the subsets of PERSON : @ PERSON = \ [ MAN , WOMAN , BOY</definiens>
			</definition>
			<definition id="2">
				<sentence>WALK ( x ) \ ] ( 3 ) where MAN ( x ) is used for the domain restriction of the variable X. Thus , this restriction can be interpreted as `` x is an element of MAN '' .</sentence>
				<definiendum id="0">WALK</definiendum>
				<definiendum id="1">MAN ( x</definiendum>
				<definiens id="0">an element of MAN ''</definiens>
			</definition>
			<definition id="3">
				<sentence>Then , an atom is defined as P ( t~ , t2 , ... , t~ ) where P is a n-place predicate symbol and t~ ( i = l , ... , n ) is a term .</sentence>
				<definiendum id="0">atom</definiendum>
			</definition>
			<definition id="4">
				<sentence>In the NTA\ ] definition , V denotes a predicate symbol .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">a predicate symbol</definiens>
			</definition>
			<definition id="5">
				<sentence>S , XI , X~ and P are terms , of which S is usually used as the subject of V , and P is a formula which denotes the modifiers concerning time , locus , goal , reason and manner of V. Some of the terms may be omitted according to the syntactic feature of a sentence .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">of which S is usually used as the subject of V</definiens>
				<definiens id="1">a formula which denotes the modifiers concerning time , locus , goal , reason and manner of V. Some of the terms may be omitted according to the syntactic feature of a sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>F and G represent premise and conclusion respectively in the formula ; namely , F -- ~ G. More clearly , F is constituted from NTAs and PTAs by using logical connectives A , V ands , and also the same is G except that no PTAs are permitted .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">ands , and also the same is G except that no PTAs are permitted</definiens>
			</definition>
			<definition id="7">
				<sentence>We can paraphrase `` x give z to y '' to `` y receive z from x '' • ( Vx/PS ) ( Vy/PS ) ( Vz/PO ) ( Vp/VMOD ) \ [ -- ~ ( GIVE x y z p ) v ( RECEIVE y z x p ) \ ] where p/VMOD specifies that p is any formula which qualifies the predicates .</sentence>
				<definiendum id="0">Vx/PS ) ( Vy/PS )</definiendum>
			</definition>
			<definition id="8">
				<sentence>We can express the meaning of `` float '' as follows : ( Vx/PO ) ( Vy/LQ ) ( Vu/RNUM ) ( Vv/RNUM ) ( Vp/VMOD ) \ [ ~ ( ( SPEC-GR x U ) A ( SPEC-GR y v ) A ' ( LT u v ) ) v ( FLOAT x y p ) \ ] This says that `` if the specific gravity of × is U , the specific gravity of y is V and U is less than V , then X float on y '' .</sentence>
				<definiendum id="0">SPEC-GR y v</definiendum>
				<definiens id="0">less than V , then X float on y ''</definiens>
			</definition>
			<definition id="9">
				<sentence>RR ( Replacement Rule ) Let F be an axiom containing P and let G be the query clause containing C. RR generates a new query clause R after substitution o has been done .</sentence>
				<definiendum id="0">RR</definiendum>
				<definiens id="0">the query clause containing C. RR generates a new query clause R after substitution o has been done</definiens>
			</definition>
			<definition id="10">
				<sentence>As the final result , we obtain the following formula : \ [ ( MEET JOHN JACK ( TIME PAST ) ~ ( PLACE ( IN HIBIYA-PARK ) ) ) A ( MEET JOHN MARY ( TIME PAST ) A ( PLACE ( IN HIBIYA-PARK ) ) ) \ ] ( 17 ) Let us consider a more complex sentence which contains a relative clause and a personal pronoun in it : JOHN GAVE A RED CAR TO MARY WHO HE LOVES .</sentence>
				<definiendum id="0">MEET JOHN MARY</definiendum>
				<definiens id="0">contains a relative clause and a personal pronoun in it : JOHN GAVE A RED CAR TO MARY WHO HE LOVES</definiens>
			</definition>
			<definition id="11">
				<sentence>Program ( LOGICAL-FORM EF S ) ' ( GET-VINF VP MVERB ; S ) ' ( GET-DON SDOM XDOM YDOM ; VP S ) ' ( CREATE-GROUP SGP VGP XGP YGP ; SDOM VP XDOM YDOM S ) ( ARGUMENT SUBJ SDOM SGP ) ( ARGUMENT XOBJ XDOM XGP ) ( ARGUMENT YOBJ YDOM YGP ) ( NOD-ATOM SHOD SUBJ SGP ) ( NOD-ATOM XMOD XOBJ XGP ) ( NOD-ATOM YMOD YOBJ YGP ) ( VMOD-ATOM VMOD VGP SGP XGP YGP ) ( KERNEL-S KS VP MVERB SUBJ XOBJ YOBJ VMOD ) ' ( CNCT-LOGICAL EF ; KS SHOD XMOD YMOD ) ' ( ALL-MARKED ; S ) 395 ( ARGUMENT EARG EDOM EGP ) ' ( MARK EGP ; EDOM EGP ) ' ( UNMARK-PREP-OBJ EGP ; EGP ) ' ( MAKE-SET EARG ; EGP ) ( ARGUMENT EARG EDOM empty ) ' ( SEARCH REL ; INPUT ) ' ( NON-EQ REL WHERE ) ' ( ANTEC X ; REL INPUT ) ' ( PRE-W Z ; REL ) ' ( SYN-CAT K ; Z ) ' ( NON-ELM K PREP ) ' ( PUT-ELM EARG ; X ) ( NOD-ATOM EMOD EARG EGP ) ' ( DET-MOD DET ; EARG EGP ) ' ( ADJ-MOD ADJ ; EARG EGP ) ' ( NPREP-MOD NPREP ; EARG EGP ) ' ( MAKE-EF EMOD ; DET ADJ NPREP ) ( VMOD-ATOM VMOD VGP SGP XGP YGP ) ( TENSE-NOD TENSE ; VGP ) ( AUX-MOD AUX ; VGP ) ( VPREP-MOD VPREP ; VGP SGP XGP YGP ) ( ADV-MOD ADV ; VGP SGP XGP YGP ) ( TO-INF-MOD INF VGP SGP XGP YGP ) ( MAKE-EF VMOD ; TENSE AUX VPREP ADV INF ) __= ********************************************* GP DON / \ J EDOM VGP EGP VP /i\ SDM~XDOM YDOM SGP XGP YGP ARG `` ARG MVERB I su J XO `` OBJ VMD~ EMOD Jl SMOD XM~D~YMOD ********************************************* IN HIBIYA-PARK , JOHN MET JACK AND MARY .</sentence>
				<definiendum id="0">Program ( LOGICAL-FORM EF S</definiendum>
				<definiendum id="1">ARGUMENT EARG EDOM empty</definiendum>
				<definiendum id="2">EARG EGP ) '</definiendum>
				<definiendum id="3">EARG EGP</definiendum>
				<definiens id="0">DET ADJ NPREP ) ( VMOD-ATOM VMOD VGP SGP XGP YGP ) ( TENSE-NOD TENSE ; VGP ) ( AUX-MOD AUX ; VGP ) ( VPREP-MOD VPREP ; VGP SGP XGP YGP ) ( ADV-MOD ADV ; VGP SGP XGP YGP ) ( TO-INF-MOD INF VGP SGP XGP YGP ) ( MAKE-EF VMOD ; TENSE AUX VPREP ADV INF</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>The latter layer is not immediately structured by the system of language , though there are certain types of regular correspondence , which we want to discuss later .</sentence>
				<definiendum id="0">regular correspondence</definiendum>
				<definiens id="0">not immediately structured by the system of language</definiens>
			</definition>
			<definition id="1">
				<sentence>Second , and most important , the structuring of the layer of content that is made by the method of trial and error , in the experimentsl systems , often leads to the necessity to postulate more and more subtle structuring ; thus e. g. for the Fillmorean case roles it appears that every small group of verbs ( of saying , of perception , of movement , of simple physical actions , of puz~chase , etc. , etc. ) has its own set of roles : no element of the set { buyer , seller , goods , pric~ is identical with any element of the set ~speaker , addressee , object spoken of , type of messag~ , etc. ( see e. g. Fillmore 7 quoting Cole ) .</sentence>
				<definiendum id="0">pric~</definiendum>
				<definiens id="0">no element of the set { buyer , seller , goods ,</definiens>
			</definition>
			<definition id="2">
				<sentence>13 The meaning of a sentence can be represented by a rather simple tree ( in -70accordance with the traditions of European linguistics we prefer dependency to categorial or phrase structure grammars ) with the following properties : ( a ) the tree has a single root , is finite , connected and projective ( cf. Hays 14 , NarcuslS ) ; ( b ) the edges are labelled by the types of modifications , which are listed partly in the lexical ( not only verbal ) frames of the `` governing '' lexical unit , and partly in a list of free modifications ( adverbials ) , common to all the units of a given part of speech ; besides the Actor/Bearer ( rather than Agentive , see Haji~ov~ 16 ) the verbal frames may contain the Patient ( Goal ) , and , if these participants both are present in the frame , then also the Addressee , the Origin and/or the Zffect may be included there ; Instrument , ~mnner , Measure , various types of Locative , Duration , Cause , Condition , etc. belong to the list of free modifications ; they can occur with every verb at least in principle , i. e. are not excluded linguistically and they may occur even more than once with a single verb token ; they have to be listed in individual frames only if they are obligatory with the given verb ; with nouns , the General Relation is a typical free modification , while the Patient has to be included in the frame of such nouns as directQr , etc. ; ( c ) the nodes are labelled by complex symbols corresponding to leXical and morphological meanings ( the latter comprise tense , aspect , modality and others with verbs , numoer and delimitire features with nouns , degrees of comparison with adjectives ) ; ( d ) the `` left-to-right '' order of the nodes is interpreted as the `` deep word order '' or communicative dynamism , which corresponds to the order of quantifiers in formal languages ; on this scale the boundary Oetween topic and focus can be established ( primarily just before or just after the verb ) ; the scope of negation i~ identical with the focus in the unmarked case .</sentence>
				<definiendum id="0">meaning of a sentence</definiendum>
				<definiendum id="1">Actor/Bearer</definiendum>
				<definiendum id="2">General Relation</definiendum>
				<definiens id="0">the verbal frames may contain the Patient ( Goal )</definiens>
				<definiens id="1">a typical free modification , while the Patient has to be included in the frame of such nouns as directQr</definiens>
			</definition>
			<definition id="3">
				<sentence>The linguistic ( s~ntactico-semantic ) analysis translating sentences to their semantic ( tectogrammatical ) representations is combined with a procedure translating these representations to a formal language based on the theory of types ; meaning postulates are used in this procedure , which also converts the patterning of obligatory and optional modifications ( dependent words ) into structures connected with the arity of predicates ; furthermore , communicative dynamism is transferred here to the -- 71-usual form of denoting the scope of the quantifiers .</sentence>
				<definiendum id="0">linguistic ( s~ntactico-semantic</definiendum>
				<definiendum id="1">semantic</definiendum>
				<definiens id="0">also converts the patterning of obligatory and optional modifications ( dependent words ) into structures connected with the arity of predicates</definiens>
			</definition>
			<definition id="4">
				<sentence>eg_of Inference ( as a means to account for factual knowledge in automatic understanding of natural language ) led us to the conclusion that rules of inference operating on the representations of the meaning of sentences ( cf. § relationships between meaning and content ( `` factual knowledge '' ) .</sentence>
				<definiendum id="0">Inference</definiendum>
				<definiens id="0">a means to account for factual knowledge in automatic understanding of natural language ) led us to the conclusion that rules of inference operating on the representations of the meaning of sentences ( cf. § relationships between meaning and content ( `` factual knowledge '' )</definiens>
			</definition>
			<definition id="5">
				<sentence>These rules range from general ones to more or less idiosyncratic cases concerning the relationships between specific words , as well as modalities , h~ponym , y , etc. -- 73-A rather general rule changes e.g. a structure of the form ( V-act ( l~Acto r ) ... ) into ( V-act ( DActor ) ( Rinstr ) ... ) , where V-act is a verb of action , D is a dummy ( for the general actor ) and N is an inanimate noun ; thus The negative feedback can_servo the voltage to zero is changed into One can servo the volta6e to zero by ... .. A rather specific rule \ [ connected with a single verb ) is that changing ( us_ .</sentence>
				<definiendum id="0">V-act</definiendum>
				<definiendum id="1">D</definiendum>
				<definiendum id="2">N</definiendum>
				<definiens id="0">a dummy ( for the general actor</definiens>
				<definiens id="1">an inanimate noun</definiens>
			</definition>
</paper>

		<paper id="1034">
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>~le operations necessary to obtain this component of the CSR ~len analyzing the initial Japanese text will evidently comprise isolating separate word forms and determining their internal structure ( in terms of lexemes and morphologic markers ) , resolving ~nbi~ities for all units established ; eliminating synonymy where it is manifested as supplementary distribution or free variation of morphologic units ; detecting phraseological word combinations ~d reducing them to a one-word symbol ; giving special labels to those word forms or parts of word forms which play ~ auxiliary role in the text analyzed and require no special translation equivalents ; filling in the units omitted in the source text if their absence obscures its structure and hinders the translation process ( due to the differences between the rules of linguistic ellipsis in the two lmlguages ) , etc .</sentence>
				<definiendum id="0">translation process</definiendum>
				<definiens id="0">supplementary distribution or free variation of morphologic units ; detecting phraseological word combinations ~d reducing them to a one-word symbol</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>Embodiment ( EMBOD ) a substitution of some more concrete and `` vivid '' element X 1 for a more abstract element X. The element X 1 has all the properties of X plus some new feature ( s ) .</sentence>
				<definiendum id="0">Embodiment</definiendum>
				<definiens id="0">all the properties of X plus some new feature ( s )</definiens>
			</definition>
			<definition id="1">
				<sentence>Repetition ( REPET ) a substitution of a series of nearly identical elements X , X 1 ... .. X n for an element X. A FORMAL GRAMMAR OF EXPRESSIVENESS FOR SACRED LEGENDS F. Dreizin , A. Shenhar , H. Bar-ltzhak Haifa University , Haifa , Israel Examples : a door ÷ the first door , the second door , the third door ... . ; a waiter ÷ the first waiter , the second waiter ... . Variation ( VAR ) a substitution of a series of substantially different elements X I , X 2 ... .. X n for an element X so that each one is the result of an EMBOD applied to X. Examples : a man ÷ a carpenter , a student , a taxi-driver ... . ; a building ÷ a church , a mosque , ... Detailization ( DET ) a substitution of a detailed description Y1 of a thing , situation or action Y , for Y. Examples : X is ill ÷ X stays in bed and X has no appetite and X has a high temperature ; an old Jew dies ÷ an old Jew is n't well , other Jews come , they pray together , the Rabbi comes to visit the old Jew , the old Jew is dead and the body is carried to the cemetery .</sentence>
				<definiendum id="0">Repetition</definiendum>
				<definiendum id="1">Variation ( VAR</definiendum>
				<definiendum id="2">X</definiendum>
				<definiendum id="3">X</definiendum>
				<definiens id="0">a substitution of a series of nearly identical elements X , X 1 ... .. X n for an element X. A FORMAL GRAMMAR OF EXPRESSIVENESS FOR SACRED LEGENDS F. Dreizin</definiens>
				<definiens id="1">a substitution of a series of substantially different elements X I</definiens>
				<definiens id="2">a substitution of a detailed description Y1 of a thing , situation or action Y</definiens>
				<definiens id="3">ill ÷ X stays in bed and X has no appetite</definiens>
			</definition>
			<definition id="2">
				<sentence>Exposition ( EXPO ) a substitution of two elements pre-X and X for X , where pre-X precedes X in the text ; pre-X may be : a ) incomplete X ( the shadow of X appears and then X itself ) ; b ) the felt absence of X ( everybody is waiting for X , then X appears ) ; c ) Anti-X followed by X. The only difference between this case and CONTR is the order of Anti-X and X. Adjustment ( ADJ ) of X to Y with respect to a feature f : X 1 is substituted for X , where X 1 has all the essential properties of X , plus some feature f of an element Y occurring in the derivation of a text .</sentence>
				<definiendum id="0">Exposition</definiendum>
				<definiendum id="1">pre-X</definiendum>
				<definiendum id="2">CONTR</definiendum>
			</definition>
			<definition id="3">
				<sentence>USE ( x , y ) is a typical , normal relation between human x and a thing y , y being conceived exclusively as a means of x 's well-being .</sentence>
				<definiendum id="0">USE</definiendum>
				<definiens id="0">a typical , normal relation between human x and a thing y , y being conceived exclusively as a means of x 's well-being</definiens>
			</definition>
			<definition id="4">
				<sentence>INJURE ( x , y ) is x 's doing any harm to y. CAUSE ( A , B ) means : the event A happens and involves the event B. RESTORE ( x , y ) means liquidation of the consequences of x 's injuring y. DISABLE and PETRIFY are two forms of paralysis , the first one partial , the second one complete .</sentence>
				<definiendum id="0">INJURE</definiendum>
				<definiens id="0">x 's doing any harm to y. CAUSE ( A , B ) means : the event A happens and involves the event B. RESTORE ( x , y ) means liquidation of the consequences of x 's injuring y. DISABLE</definiens>
			</definition>
			<definition id="5">
				<sentence>INATTACK ( x , y ) is involuntary injuring y by x. CANCELED ( A ) means : all the consequences of the event A become non-existent .</sentence>
				<definiendum id="0">INATTACK</definiendum>
				<definiens id="0">involuntary injuring y by x. CANCELED ( A ) means : all the consequences of the event A become non-existent</definiens>
			</definition>
			<definition id="6">
				<sentence>VALOBJ is a valuable object without ideological importance .</sentence>
				<definiendum id="0">VALOBJ</definiendum>
				<definiens id="0">a valuable object without ideological importance</definiens>
			</definition>
			<definition id="7">
				<sentence>SP is an object possessing sacred power .</sentence>
				<definiendum id="0">SP</definiendum>
				<definiens id="0">an object possessing sacred power</definiens>
			</definition>
			<definition id="8">
				<sentence># s ( a ) &gt; + # s ( STAT ( s ( a ) ) ) Here s is a sign .</sentence>
				<definiendum id="0">STAT</definiendum>
				<definiens id="0">a sign</definiens>
			</definition>
			<definition id="9">
				<sentence>, The principle of CONTR is exemplified also by the following rule : s ( TRY ( -t- , T ) ) ÷ s ( TR¥ ( -t- , T ) ) &gt; FAIL ( t , T ) , where s is a sign , t is a term and T is a tsection .</sentence>
				<definiendum id="0">principle of CONTR</definiendum>
				<definiendum id="1">s</definiendum>
				<definiendum id="2">T</definiendum>
				<definiens id="0">a tsection</definiens>
			</definition>
			<definition id="10">
				<sentence>The final step of our derivation : # - ( STAT ( - ( - ( LOSE ( HUMAN*COM , J , THING*SYMBOL* SINGLE*J*SP*RELA*GRAVE ( HUMAN*SINGLE*VERIMP*RELA* J*SP*MALE*RABBI ) ) ) &amp; - ( NEGLECT ( J , HUMAN*COM , THING , SYMBOL*SINGLE*J*SP*RELA*GRAVE ( HUMAN*SINGLE *VERIMP*RELA*J*SP*MALE*RABBI ) ) ) &amp; - ( NOT ( + ( USE ( J , HUMAN*CON , THING*SYMBOL*SINGLE*J*SP*RELA* GRAVE ( HUMAN*SINGLE*VERIMP*RELA*J*SP*MALE* RABBI ) ) ) ) ) ) ) ) &gt; - ( MISAPPROPRIATE ( HUMAN , GROUP , A* IMPORTANT , MALE , OFFICIAL , THING*SYMBOL , SINGLE*J* SP*RELA*GRAVE ( HUMAN*SINGLE*VERIMP*RELA*J*SP* MALE , RABBI ) ) ) ~ ~ ( TRY ( AOMAN~GROOP~J~iMPOR~ANT~ ~\ [ E~O~T~\ [ , + ( + ( FIND ( HUMAN , GROUP , J*IMPORTANT* MALE , OFFICIAL , THING*SYMBOL*SINGLE*J*SP*RELA* GRAVE ( HUMAN*SINGLE*VERIMP*RELA*J*SP*MALE * RABBI ) ) ) &amp; + ( CAREFOR ( HUMAN*GROUP*J*IMPORTANT* MALE*OFFICIAL , THING*SYMBOL*SINGLE*J , SP , RELA , GRAVE ( 'HUMAN , SI NGL'E*VERI MP*RELA*J- , SP , MALE , RABB I ) ) ) &amp; + ( US E ( HUMAN*GROUP*J*I MPORTAN=T*MA-L E*OFFI CI AL , THING*SYMBOL*SINGLE*J*SP*RELA*GRAVE* ( HUMAN* SINGLE*VERIMP*RELA~J*SP*MALE*RABBT ) ) ) ) ) ) ~ ( ABASE ( HUMAN*SlNGLE*VERIMP*A*MALE*SON ( HUMAN * SINGLE*VERIMP*A*MALE*SECA*SHEIK ) , HUMAN*GROUP* J*IMPORTANT*MALE*OFFIClAL ) ) ~ - ( FAIL ( HUMAN* GROUP*J*IMPORTANT*MALE*OFFICIAL , + ( + ( FIND ( HUMAN • GROUP*J*IMPORTANT*MALE*OFFICIAL , THING*SYMBOL* SINGLE*J*SP*RELA*GRAVE ( HUMAN*SINGLE*VERIMP*RELA* J*SP*MALE*RABBI ) ) ) &amp; + ( CAREFOR ( HUMAN*GROUP*J* IMPORTANT*MALE*OFFICIAL , THING*SYMBOL*SINGLE*J* SP*RELA*GRAVE ( HUMAN*SINGLE*VERIMP*RELA*J*SP*MALE*RABBI ) ) ) &amp; + ( USE ( HUMAN*GROUP*J*IMPORTANT* MALE*OFFICIAL , THING*SYMBOL*SINGLE*J*SP*RELA* GRAVE ( HUMAN*SINGLE*VERIMP*RELA*J*SP*MALE* RA~ ) ) ) ) ) ) &gt; + ( CAUSE ( DEFILE ( HUMAN*SINGLE* VERIMP*A*MALE*SON ( HUMAN*SINGLE*VERIMP*A*MALE* SECA*SHEIK ) , THING*SYMBOL*SINGLE*J*SP*RELA* GRAVE ( HUMAN*SINGLE*VERIMP*RELA*J*SP*MALE* RA~ ) ) , + ( MIR ( + ( DISABLE ( THING*SYMBOL*SINGLE*J* SP*RELA*GRAVE ( HUMAN*SINGLE*VERIMP*RELA*J*SP* -165 MALE*RABBI ) , HUMAN*SINGLE*VERIMP*A*MALE*SON ( NUMAN*SINGLE*VERIMP*MALE*A*SNEIK ) ) ) ) ) INATTACK ( HUMAN*GROUP*A*IMPORTANT*MALE*OFFIClAL , HUMAN*SINGLE*VERIMP*A*MALE*SECA*SHEIK ) ~ + ( QUASI ( + ( CAREFOR ( HUMAN*SINGLE*VERIMP*A*MALE*SON ( HUMAN*SINGLE*VERIMP*MALE*SECA*SHEIK*A ) , THING* SYMBOL*SlNGLE*J*SP*RELA*GRAVE ( HUMAN*SINGLE* VERIMP*RELA*J*SP*MALE*RABBI ) ) ) ) ) ~+ ( FIND ( HUMAN* GROUP*J*IMPORTANT*MALE*OFFICIAL , THING*SYMBOL* SINGLE*J*SP*RELA*GRAVE ( HUMAN*SlNGLE*VERIMP* RELA*J*SP*MALE*RABBI ) ) ) ) ) &gt; + ( GIVEBACK ( HUM~ GROUP*A*IMPORTANT*MALE*OFFICIAL , THING , SYMBOL , SlNGLE , J , SP , RELA , GRAVE~HUMAN , SINGLE , VERIMP*RELA* J*SP*MALE*RABBI ) ) ) &gt; + ( QUASI ( + ( CAREFOR ( HUMAN* GROUP*A*IMPORTANT*MALE*OFFICIAL , THING*SYMBOL* SlNGLE*J*SP*RELA*GRAVE ( HUMAN*SINGLE*VERIMP* RELA*J*SP*MALE*RABBI ) ) ) ) ) &gt; + ( MIR ( CURE ( THING* SYMBOL , SINGLE , J*SP , RELA , GRAVE ( HUMAN*SINGLE* VERIMP , RELA*J*SP*MALE*RABBI ) , HUMAN*SINGLE* VERIMP , A*MALE*SON ( HUMAN*SINGLE*VERIMP*A* MALE*SECA*SHEIK ) ) ) ~ MIR ( CANCELED ( GUILTY ( HUMAN* GROUP~A , IMPORTANT*MALE , OFFIClAL ) ) ) ) &gt; + ( PRAISE ( HUMAN , SlNGLE*VERIMP*A*MALE*SON ( HUMAN*SlNGLE* VERIMP*A*MALE*SECA*SHEIK ) , HUMAN*GROUP , J* IMPORTANT*MALE*OFFICIAL ) ~ PRAISE ( HUMAN*SINGLE* VERIMP*A*MALE*SON ( HUMAN*SINGLE*VERIMP*A*MALE* SECA*SHEIK ) , THIN G*SYMBOL*SlNGLE*J*SP*RELA* GRAVE ( HUMAN*SlNGLE*VERIMP*RELA*J*SP*MALE* RABBI ) ) ) ~+ ( STAT ( + ( + ( FINB ( ~HUMAN~COM , THING* SYMBOL*SINGLE*J*SP*RELA*GRAVE ( HUMAN*SINGLE* VERIMP*RELA*J*SP*MALE*RABBI ) ) ) &amp; + ( CAREFOR ( J , HUMAN COM , THING*SYMBOL*SINGLE*J*SP*RELA* GRAVE ( HUMAN*SINGLE*VERIMP*RELA*J*SP*MALE* RABBI ) ) ) &amp; + ( USE ( J*HUMAN*COM , THING~SYMBOL* SlNGLE*J*SP*RELA*GRAVE ( HUMAN*SINGLE*VERIMP* RELA*J*SP*MALE*RABBI ) ) ) ) ) ) # We shall informally describe some artistic features of this legend which we tried to account for by our analysis .</sentence>
				<definiendum id="0">LOSE</definiendum>
				<definiendum id="1">SYMBOL*SINGLE*J*SP*RELA*GRAVE</definiendum>
				<definiendum id="2">SI NGL'E*VERI MP*RELA*J-</definiendum>
				<definiendum id="3">THIN G*SYMBOL*SlNGLE*J*SP*RELA* GRAVE</definiendum>
				<definiens id="0">+ ( FIND ( HUMAN , GROUP , J*IMPORTANT* MALE , OFFICIAL , THING*SYMBOL*SINGLE*J*SP*RELA* GRAVE ( HUMAN*SINGLE*VERIMP*RELA*J*SP*MALE * RABBI</definiens>
				<definiens id="1">HUMAN*SINGLE*VERIMP*RELA*J*SP*MALE* RA~ ) ) ) ) ) ) &gt; + ( CAUSE ( DEFILE ( HUMAN*SINGLE* VERIMP*A*MALE*SON ( HUMAN*SINGLE*VERIMP*A*MALE* SECA*SHEIK ) , THING*SYMBOL*SINGLE*J*SP*RELA* GRAVE ( HUMAN*SINGLE*VERIMP*RELA*J*SP*MALE* RA~ ) ) , + ( MIR ( +</definiens>
				<definiens id="2">GROUP*A*IMPORTANT*MALE*OFFICIAL , THING , SYMBOL , SlNGLE , J , SP , RELA , GRAVE~HUMAN , SINGLE , VERIMP*RELA* J*SP*MALE*RABBI ) ) ) &gt; +</definiens>
			</definition>
			<definition id="11">
				<sentence>The punitive action by SP ( the Sheik 's son being petrified ) creates a knot of entangled events ; in fact , one event which comprises different actions .</sentence>
				<definiendum id="0">punitive action by SP (</definiendum>
				<definiens id="0">the Sheik 's son being petrified ) creates a knot of entangled events ; in fact , one event which comprises different actions</definiens>
			</definition>
			<definition id="12">
				<sentence>The petrification means that the Jews have found the holy grave , Tt is a sign of the holy grave being here .</sentence>
				<definiendum id="0">Tt</definiendum>
				<definiens id="0">a sign of the holy grave being here</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>A Japanese written language consists of kanji , kana ( hirakana and katakana ) and alphanumeric letters .</sentence>
				<definiendum id="0">Japanese written language</definiendum>
				<definiens id="0">consists of kanji , kana ( hirakana and katakana</definiens>
			</definition>
			<definition id="1">
				<sentence>Kana is a phonetic symbol and kanji is an ideograph .</sentence>
				<definiendum id="0">Kana</definiendum>
				<definiendum id="1">kanji</definiendum>
				<definiens id="0">a phonetic symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>KEIRYO GENGO GAKU ( gomputation ) ( language ) ( stud ) in stics I , .</sentence>
				<definiendum id="0">KEIRYO GENGO GAKU</definiendum>
				<definiens id="0">( gomputation ) ( language ) ( stud ) in stics I ,</definiens>
			</definition>
			<definition id="3">
				<sentence>~i KANAGAWA prefecture ) KANAGAWAKEN KAWASAKI SHI £ /~ ~J ( name of city ) ~r~ I % ( person 's name ) This phrase describes an address , and ' ~\ ] ( city ) ' is a suffix for the name of a place which does not connect with a person 's name .</sentence>
				<definiendum id="0">KANAGAWAKEN KAWASAKI SHI £ /~ ~J ( name of city</definiendum>
				<definiens id="0">a suffix for the name of a place which does not connect with a person 's name</definiens>
			</definition>
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>Both the recognition systems consist of two parts : an acoustic analyser and a linguistic processor .</sentence>
				<definiendum id="0">recognition systems</definiendum>
				<definiens id="0">an acoustic analyser and a linguistic processor</definiens>
			</definition>
			<definition id="1">
				<sentence>-472 Knowledge source Semantic knowledge ~~ { Defin~tion~ \ ] ema i/erb / &lt; structure ) ( network b ~Semantic information I I ) Semantic analyser I Syntactic knowledge Syntactlc State k__/ L__dsyntactic transition~ -- -- ~Inflexion ) I ~information/ networkS__ | Knowledge about vocabulary , \ ] , Li_~v°cabulary ~_~ fCharacteristic~ /Word i hl `` \information/ analyser \ [ IMatching unit III ~-~\ ] { Candidate ~ I \word strings/ Selector of ~Matching ~ I\ ] candidate words unit I ( Phoneme ~string ) I Acoustic ~ analyser\ ] Fig. 1.1 Speech recognition system. Besides an ordinary word dictionary , a characteristic phoneme dictionary ( This dictionary exists only implicitly and is automatically composed from the word dictionary which is written in Roman letters. ) is prepared and presents major acoustic features of each word. These major features are used for reduction of the number of candidate words. For matching between a phoneme string with erroneous phonemes and items of the word or characteristic phoneme dictionaries , a new matching method using graph theory is devised \ [ 7\ ] . These acoustic and matching processings are the same as the ones in the previous systems. for machine reco @ nition In order to automatically recognize continuously spoken natural languages , it is necessary to use syntactic rules. However using the original form of Japanese grammar written by grammarians is not necessarily suitable for mechanical recognition. Moreover it is very difficult to reduce the number of predicted words only by syntactic information because of the nature of Japanese language which does not require to keep the word order so rigorously. Taking account of these conditions , Japanese words are classified as described in the following article and the syntax may preferably be represented by state transition networks as shown in section 3.1.3. speech Each word is classified grammatically as given in Table 3.1. In Japanese nouns , pronouns , numerals and quasi-nouns ( KEISHIKI-MEISHI in Japanese ) are called substantives ( inflexionless parts of speech in Japanese grammar , TAIGEN in Japanese ) , and verbs , auxiliary verbs and adjectives are called inflexional words ( inflexional parts of speech ! YOGEN in Japanese ) . Meanwhile the words No. 1 No. ii in Table 3.1 are inflexionless words and the words No. 12 No. 15 are 'able 2.1 Output phonemes and their decision methods. Class Output Phoneme Decision Method Vowel i , e , a , o , u Parcor-coefficients k , Nasal m'n'9'N using Bayes decision theory Buzz denoted by B s Number of zero-crossings Fricative Variations of amplitude and h spectrum , Number of zerocrossings , and Unsimilarity to vowels and nasals r Liquid Unvoiced stop Silence p , t , k Variations of amplitude and first formant frequency , Number of zero-crossings Following after silence and Having high frequency components Small amplitude -- 473-inflexional words. In No. 16 the inflexion rules necessary for each inflexional word are written in appropriate forms. The additional word `` carriage return '' in No. 17 is a special symbol. We ask each spejker to utter the word `` carriage return '' at the end of each sentence in order to inform the recognizer of the end of a sentence. Japanese verbs , adjectives and auxiliary verbs are inflexional. The verb 's inflexion has been classified traditionally into 5 kinds of inflexion types : GODAN-KATSUYO ( inflexion ) , KAMIITCHIDAN-KATSUYO , SHIMO-ICHIDAN-KATSUYO , SAGYOHENKAKU-KATSUYO and KAGYO-HENKAKU-KATSUYO. But we classify them into 14 types as given in Table stem , a consonant following the stem and the inflexional ending of each word. Examples are shown in Fig. 3.1. By so doing the number of inflexion tables becomes smaller. The adjectives and verbal-adjectives ( KEIYODOSHI in Japanese ) have we classified into 3 types according to their inflexion. Two types of them are shown in Fig. 3.2. The inflexion of auxiliary verbs is the same as the traditional one. Some examples are Table 3.1 Classification of words by parts of speech. No.16 and 17 are exceptional. No. part of speech 1 2 3 4 5 6 7 8 9 l0 ii 12 13 14 15 16 '' 17 '' noun pronoun nmneral quasi-noun prefix suffix part modifying substantives adverb conjunction exclamation particle verb adjective auxiliary verb subsidiary verb inflexion carriage return Table 3.2 Classification of verbs. No Inflexion Example 1 2 3 4 5 6 7 8 9 i0 ii 12 13 14 GODAN-KATS UYO 1 , , 2 , , 3 `` 4 , , 5 `` 6 , , 7 `` 8 , , 9 , , i0 KAMI I CH I DANKATS UYO , SHI MOI CHI DAN-KATSUYO SAGYO-HENKAKUKATSUYO KAGY OHENKAKU -KATS UYO Verb : ARU ( be ) IKU KATS U NORU KAU SHINU YOMU YOBU SAKU OSU OYOGU OKI RU NAGE RU SURU KURU ARU shown in Fig. 3.3. YOMU Word Stem IKU I ~ ( go ) Inflexion Following vowel First I Ending consonant vowel Consonant &amp; vowel ( a ) -K ~ A ( i. negative ) I ( 2. RENYO ) U ( 3. conclusive ) U ( 4 RENTAI ) E ( 5. conditional ) E ( 6. imperative ) OU ( 7 ; volitional ) ( b ) T ~ TA ( 3 ) ( auxiliary ) TA ( 4 ) verb TE ( particle ) ( read ) Fig. 3.\ ] Inflexion Adjective I Adjective II ( Verbaladjective ) YO ~ M -the same as ( a ) \ N ( c ) ~ DA ( 3 ) ( auxiliary ) DA ( 4 ) verb DE ( particle ) Inflexion of verbs : IKU ( go ) ( No.l in Table 3.2 ) and YOMU ( read ) ( No.6 in Table following word must be inflexional or substantive respectively. The following words TA and DA are auxiliary verbs and TE and DE are particles. Word I Stem I Inflexion UTSUKUSHII UTSUKUSHI ( beutiful ) SHIZUKADA ( being quiet ) Fig. 3.2 SHIZUKA ~ ! I ( 3 ) ( 4 ) u ( 2 ) EREBA ( 5 ) AROU ( 7 ) T -TA ( 3 ) T -TA ( 4 ) DA ( 3 ) NA ( 4 ) ARA ' 5 \~DA~ou ( 7 ) ~DAT -TA ( 3 ) -- DAT -TA ( 4 ) Examples of inflexion of an adjective and a verbal-adjective. The numbers in parentheses are identified with the ones in Fig. 3.1. Word Stem Inflexion ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 7 ) Fig. 3.3 Word Stem Inflexion NAI NA~iU ( 4 ) ( 3 ) ( 2 ) \k'KERE -- BA c5 ) ~KAT -- -TA ( 3 ) 'KAT -- -TA ( 4 ) Examples of inflexion of auxiliary verbs. The numbers in parentheses are identified with the ones in Fig. 3.1. -474 functions In a Japanese sentence some words express material ( no~ma ) such as substantives and verbs , and the others express syntactic function ( no~sis ) such as particles and auxiliary verbs \ [ 9\ ] . The latter controls the syntactic function of the former , or ; in other words , gives a material word or phrase a modifying function and these two words usually appear in a pair in sentences. The pair is called a phrase , and some modifying relation is established between phrases. And those modifying relations between phrases compose a sentence. In some cases a phrase consists of only a word such as an adjective , an adverb and some inflexional word , without being accompanied by any word that expresses a syntactic function , and itself carries a syntactic function. Some examples are shown here. ( i ) WATASHI ( pronoun ) NO ( particle ) HON ~noun `` I `` `` book '' ... ... ... ... ... ... ... phrasel l modifying relation ( my books ) adjective ) H~A ( noun ) SHIROI ... ... ( white flower phrase \ [ modifying relation ( white flowers ) ISHI noun ) NO ( particle ) IE ( noun ) ( stone l house phrase modifying relation ( stone houses ) HON.noun. KONO ( 7 in Table 3.1 ) GA ( particle ) .. ph } ase this ~ ( book ) I modifying relation ... ... ... ... ... ... ... ... ... ... ... phrase ( This book ... ) ( ii ) TOKYO ( TOKYO ) n°un . E ( particle , to I~U ( verbgo ) I phrase I modifying relation ( go to TOKYO ) HON ( noun. UO ( partlcle ) KAU ( verb ) book ) l buy phrase I modifying relation ( buy a book ) The syntactic relation is classified into three categories : ( a ) Modification of a substantive word or phrase Some examples are shown in above ( i ) . ( b ) Modification of an inflexional word or phrase Some examples are shown in above ( ii ) . ( c ) Termination ( the end of a sentence ) . A syntactic state transition network is a network which represents the Japanese syntax\ [ 10\ ] . The standard form is shown in Fig. 3.4 , where each S represents a syntactic state , an arrow a transition path to the next state , C a part of speech , and I syntactic information. Therefore , if a state S O is followed by the part of speech C O then the state transits context-freely to S 1 outputting syntactic information I 0. To an inflexional word a transition network is also applied and represents the inflexion. In speech recognition it is necessary to pursue the whole transition from the stem of an inflexional word to the end of inflexion , in other words , to predict the stem of an inflexional word with its inflexional ending and to output the syntactic information comprehensively for the whole words including their inflexions. In Fig. 3.5 is shown an example of transition network and accompanying syntactic information for two verbs `` IKU ( go ) '' Fig. 3.4 c0/I ° Standard form of syntactic state transition network. SO , Sl : states , CO : part of speech or inflection , I0 : syntactic information. re re re Fig. 3.5 Transition network for verbs : `` IKU ( go ) and YOMU ( read ) '' with their inflexion and syntactic information. X/Z means that X is output letters and Z is the syntactic information. ~ : empty , CR : carriage return , P : particle , and the numbers are identified with the ones in Fig. 3.1. -- 475-and `` YOMU ( read ) '' . This procedure corresponds to predicting all possible combinations of a verb with auxiliary verbs. For example , for a word `` go '' , it may be better to predict probable combinations : go , goes , will go , will have gone , went and so on , though the number of probable combinations will be restricted. The syntactic state transition network can not only predicts combinable words but also outputs syntactic information about modifying relation between phrases. Each word is entered in a word dictionary in group according to part of speech as shown in Fig. 3.6. Each entry and its inflexion table are represented in Roman letters together with semantic information. If a part of speech is predicted using the syntactic state transition network , a word group of the predicted part of speech is picked out from the dictionary. letter strings and inflexion tables This routine translates a word written in Roman letters into a phoneme string using a table \ [ ii\ ] . A translated phoneme string of a predicted word is used as a reference for matching an input phoneme string. This routine can also extract the characteristic phoneme string of a word. A characteristic phoneme string of a word contains only phonemes to be surely extracted from the speech wave. It is composed of vowels , /s/ and silence , and represents major acoustic information of a word. Some examples of the phoneme strings are shown in Table 3.3. For matching procedure between an input phoneme string and a predicted word are used both phoneme and characteristic phoneme strings of the word. Here , these phoneme strings are not stored in the word dictionary. The system has only one word dictionary written in Roman letters and phoneme stringsnecessary for matching are produced each time from the word dictionary using the translating routine. This fact makes it very easy to enrich the entry of vocabulary. part of Word speech C0~ WOO 1 W002 CI -- -- -- ~ WI01 WI02 C2 -- -- -~ W201 W202 Fig. 3.6 Word dictionary. Table 3.3 Examples of phoneme and characteristic phoneme strings of words. P : unvoiced stop , N : nasal , B : buzz , . : silence. Word Phoneme Characteristic ( Pronunciation ) string phoneme string OZIISAN OBSIISAN OISA YAMA IEAMA AA SENTAKU SEN.PA.PU SE.A.U OOKII OO.PSI O.SI Semantic information is used for the following purposes. ( i ) Elimination of semantically inconsistent sentences which have been recognized using only acoustic and syntactic information. ( ii ) Future development to semantic understanding of natural language by forming semantic networks. ( iii ) Control of transition on the syntactic state transition network through the syntax analyser. One of the semantic information dealt with is `` knowledge about meaning '' . This knowledge involves ( i ) what each word means , ( ii ) verb-centered semantic structure , and ( iii ) schema of a story \ [ i0\ ] . The other information is , so called , `` remembrance of episode '' which means the remembrance of a topic of conversation. In the present system , meaning of a word is represented by a list structure , and the others are represented by networks. In the system the knowledge about meaning must be given from outside and can not yet be increased or updated by itself , but remembrance of episode can be increased or updated whenever new information comes in. While , if a schema has been already formed for a topic to be talked from now on , the knowledge of the topic will help recognition of the spoken topic. In the following sections how semantic information works in the recognition system will be explained. Denote a word by n , its characteristic features by fi ( i=l , ... , m ; m is the number of features ) . Then , the meaning of a word may be expressed as follows : n ( fl ' f2 ' `` ' '' fm ) ' where f. = 1 when the word has the characteristic 1 feature f , l f = 0 when the word has not the feature f . 1 1 For example , if fl = concrete , f2 = creature , f3 = animal , ... . then hill ( 1 , 0 , 0 , ... .. ) , dog ( i , i , i , ... .. ) . 476 ... . A verb plays very important semantic role in a simple sentence. A semantic representation of meaning of a verb is shown in Fig. 3.7 , where n O , n I , ... , n. are nodes , and Ar I , Ar 2 , .. , Ar. l 1 attatched to each arc are the natures of each arc. The nature of a node n is determined by a P nature Ar attatched to the arc directing to the P node n . Thus , P Structure = ( V , Arl , Ar 2 , ... , Ari ) , in I = a word or node qualified by a nature Arl , Restriction `` n. a word or node qualified by l a nature Ar. 1. For example , a verb `` IKU ( go ) '' is defined by Fig. 3.8. The form of a schema can not be determined uniquely. Dealing with a story , we may be able to represent the schema , for example , as shown in Table 3.4 and Table 3.5. tion of a semantic network Refering to the results of syntactic analysis and the relation between the nature of an arc and a case particle ( partly involving another particle ) , the system forms a semantic network for a simple sentence centering a recognized verb. For instance , if a word sequence OZIISAN WA YAMA E SHIBAKARI NI IKIMASHITA. ( An old man went to a hill for gathering ) firewoods. with syntactic information is given , a network shown in Fig. 3.9 will be formed. In Fig. 3.9 a process constructing a sentence is also shown. tence with a semantic network for an episode After a network for a sentence has been formed , the network must be linked Up with the already constructed network for the current episode. For this purpose a new node must be identified with the same node in the episode network. &lt; nl &gt; Fig .</sentence>
				<definiendum id="0">Z</definiendum>
				<definiendum id="1">m</definiendum>
				<definiens id="0">Knowledge source Semantic knowledge ~~ { Defin~tion~ \ ] ema i/erb / &lt; structure ) ( network b ~Semantic information I I ) Semantic analyser I Syntactic knowledge Syntactlc State k__/ L__dsyntactic transition~ -- -- ~Inflexion ) I ~information/ networkS__ | Knowledge about vocabulary , \ ] , Li_~v°cabulary ~_~ fCharacteristic~ /Word i hl `` \information/ analyser \ [ IMatching unit III ~-~\ ] { Candidate ~ I \word strings/ Selector of ~Matching ~ I\ ] candidate words unit I ( Phoneme ~string ) I Acoustic ~ analyser\ ] Fig. 1.1 Speech recognition system. Besides an ordinary word dictionary , a characteristic phoneme dictionary ( This dictionary exists only implicitly and is automatically composed from the word dictionary which is written in Roman letters. ) is prepared and presents major acoustic features of each word. These major features are used for reduction of the number of candidate words. For matching between a phoneme string with erroneous phonemes and items of the word or characteristic phoneme dictionaries , a new matching method using graph theory is devised \ [ 7\ ] . These acoustic and matching processings are the same as the ones in the previous systems. for machine reco @ nition In order to automatically recognize continuously spoken natural languages , it is necessary to use syntactic rules. However using the original form of Japanese grammar written by grammarians is not necessarily suitable for mechanical recognition. Moreover it is very difficult to reduce the number of predicted words only by syntactic information because of the nature of Japanese language which does not require to keep the word order so rigorously. Taking account of these conditions , Japanese words are classified as described in the following article and the syntax may preferably be represented by state transition networks as shown in section 3.1.3. speech Each word is classified grammatically as given in Table 3.1. In Japanese nouns , pronouns , numerals and quasi-nouns ( KEISHIKI-MEISHI in Japanese ) are called substantives ( inflexionless parts of speech in Japanese grammar , TAIGEN in Japanese ) , and verbs , auxiliary verbs and adjectives are called inflexional words ( inflexional parts of speech ! YOGEN in Japanese ) . Meanwhile the words No. 1 No. ii in Table 3.1 are inflexionless words and the words No. 12 No. 15 are 'able 2.1 Output phonemes and their decision methods. Class Output Phoneme Decision Method Vowel i , e , a , o , u Parcor-coefficients k , Nasal m'n'9'N using Bayes decision theory Buzz denoted by B s Number of zero-crossings Fricative Variations of amplitude and h spectrum , Number of zerocrossings , and Unsimilarity to vowels and nasals r Liquid Unvoiced stop Silence p , t , k Variations of amplitude and first formant frequency , Number of zero-crossings Following after silence and Having high frequency components Small amplitude -- 473-inflexional words. In No. 16 the inflexion rules necessary for each inflexional word are written in appropriate forms. The additional word `` carriage return '' in No. 17 is a special symbol. We ask each spejker to utter the word `` carriage return '' at the end of each sentence in order to inform the recognizer of the end of a sentence. Japanese verbs , adjectives and auxiliary verbs are inflexional. The verb 's inflexion has been classified traditionally into 5 kinds of inflexion types : GODAN-KATSUYO ( inflexion ) , KAMIITCHIDAN-KATSUYO , SHIMO-ICHIDAN-KATSUYO , SAGYOHENKAKU-KATSUYO and KAGYO-HENKAKU-KATSUYO. But we classify them into 14 types as given in Table stem , a consonant following the stem and the inflexional ending of each word. Examples are shown in Fig. 3.1. By so doing the number of inflexion tables becomes smaller. The adjectives and verbal-adjectives ( KEIYODOSHI in Japanese ) have we classified into 3 types according to their inflexion. Two types of them are shown in Fig. 3.2. The inflexion of auxiliary verbs is the same as the traditional one. Some examples are Table 3.1 Classification of words by parts of speech. No.16 and 17 are exceptional. No. part of speech 1 2 3 4 5 6 7 8 9 l0 ii 12 13 14 15 16 '' 17 '' noun pronoun nmneral quasi-noun prefix suffix part modifying substantives adverb conjunction exclamation particle verb adjective auxiliary verb subsidiary verb inflexion carriage return Table 3.2 Classification of verbs. No Inflexion Example 1 2 3 4 5 6 7 8 9 i0 ii 12 13 14 GODAN-KATS UYO 1 , , 2 , , 3 `` 4 , , 5 `` 6 , , 7 `` 8 , , 9 , , i0 KAMI I CH I DANKATS UYO , SHI MOI CHI DAN-KATSUYO SAGYO-HENKAKUKATSUYO KAGY OHENKAKU -KATS UYO Verb : ARU ( be ) IKU KATS U NORU KAU SHINU YOMU YOBU SAKU OSU OYOGU OKI RU NAGE RU SURU KURU ARU shown in Fig. 3.3. YOMU Word Stem IKU I ~ ( go ) Inflexion Following vowel First I Ending consonant vowel Consonant &amp; vowel ( a ) -K ~ A ( i. negative ) I ( 2. RENYO ) U ( 3. conclusive ) U ( 4 RENTAI ) E ( 5. conditional ) E ( 6. imperative ) OU ( 7 ; volitional ) ( b ) T ~ TA ( 3 ) ( auxiliary ) TA ( 4 ) verb TE ( particle ) ( read ) Fig. 3.\ ] Inflexion Adjective I Adjective II ( Verbaladjective ) YO ~ M -the same as ( a ) \ N ( c ) ~ DA ( 3 ) ( auxiliary ) DA ( 4 ) verb DE ( particle ) Inflexion of verbs : IKU ( go ) ( No.l in Table 3.2 ) and YOMU ( read ) ( No.6 in Table following word must be inflexional or substantive respectively. The following words TA and DA are auxiliary verbs and TE and DE are particles. Word I Stem I Inflexion UTSUKUSHII UTSUKUSHI ( beutiful ) SHIZUKADA ( being quiet ) Fig. 3.2 SHIZUKA ~ ! I ( 3 ) ( 4 ) u ( 2 ) EREBA ( 5 ) AROU ( 7 ) T -TA ( 3 ) T -TA ( 4 ) DA ( 3 ) NA ( 4 ) ARA ' 5 \~DA~ou ( 7 ) ~DAT -TA ( 3 ) -- DAT -TA ( 4 ) Examples of inflexion of an adjective and a verbal-adjective. The numbers in parentheses are identified with the ones in Fig. 3.1. Word Stem Inflexion ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 7 ) Fig. 3.3 Word Stem Inflexion NAI NA~iU ( 4 ) ( 3 ) ( 2 ) \k'KERE -- BA c5 ) ~KAT -- -TA ( 3 ) 'KAT -- -TA ( 4 ) Examples of inflexion of auxiliary verbs. The numbers in parentheses are identified with the ones in Fig. 3.1. -474 functions In a Japanese sentence some words express material ( no~ma ) such as substantives and verbs , and the others express syntactic function ( no~sis ) such as particles and auxiliary verbs \ [ 9\ ] . The latter controls the syntactic function of the former , or ; in other words , gives a material word or phrase a modifying function and these two words usually appear in a pair in sentences. The pair is called a phrase , and some modifying relation is established between phrases. And those modifying relations between phrases compose a sentence. In some cases a phrase consists of only a word such as an adjective , an adverb and some inflexional word , without being accompanied by any word that expresses a syntactic function , and itself carries a syntactic function. Some examples are shown here. ( i ) WATASHI ( pronoun ) NO ( particle ) HON ~noun `` I `` `` book '' ... ... ... ... ... ... ... phrasel l modifying relation ( my books ) adjective ) H~A ( noun ) SHIROI ... ... ( white flower phrase \ [ modifying relation ( white flowers ) ISHI noun ) NO ( particle ) IE ( noun ) ( stone l house phrase modifying relation ( stone houses ) HON.noun. KONO ( 7 in Table 3.1 ) GA ( particle ) .. ph } ase this ~ ( book ) I modifying relation ... ... ... ... ... ... ... ... ... ... ... phrase ( This book ... ) ( ii ) TOKYO ( TOKYO ) n°un . E ( particle , to I~U ( verbgo ) I phrase I modifying relation ( go to TOKYO ) HON ( noun. UO ( partlcle ) KAU ( verb ) book ) l buy phrase I modifying relation ( buy a book ) The syntactic relation is classified into three categories : ( a ) Modification of a substantive word or phrase Some examples are shown in above ( i ) . ( b ) Modification of an inflexional word or phrase Some examples are shown in above ( ii ) . ( c ) Termination ( the end of a sentence ) . A syntactic state transition network is a network which represents the Japanese syntax\ [ 10\ ] . The standard form is shown in Fig. 3.4 , where each S represents a syntactic state , an arrow a transition path to the next state , C a part of speech , and I syntactic information. Therefore , if a state S O is followed by the part of speech C O then the state transits context-freely to S 1 outputting syntactic information I 0. To an inflexional word a transition network is also applied and represents the inflexion. In speech recognition it is necessary to pursue the whole transition from the stem of an inflexional word to the end of inflexion , in other words , to predict the stem of an inflexional word with its inflexional ending and to output the syntactic information comprehensively for the whole words including their inflexions. In Fig. 3.5 is shown an example of transition network and accompanying syntactic information for two verbs `` IKU ( go ) '' Fig. 3.4 c0/I ° Standard form of syntactic state transition network. SO , Sl : states , CO : part of speech or inflection , I0 : syntactic information. re re re Fig. 3.5 Transition network for verbs : `` IKU ( go ) and YOMU ( read ) '' with their inflexion and syntactic information. X/Z means that X is output letters and</definiens>
				<definiens id="1">the syntactic information. ~ : empty , CR : carriage return , P : particle , and the numbers are identified with the ones in Fig. 3.1. -- 475-and `` YOMU ( read ) '' . This procedure corresponds to predicting all possible combinations of a verb with auxiliary verbs. For example , for a word `` go '' , it may be better to predict probable combinations : go , goes , will go , will have gone , went and so on , though the number of probable combinations will be restricted. The syntactic state transition network can not only predicts combinable words but also outputs syntactic information about modifying relation between phrases. Each word is entered in a word dictionary in group according to part of speech as shown in Fig. 3.6. Each entry and its inflexion table are represented in Roman letters together with semantic information. If a part of speech is predicted using the syntactic state transition network , a word group of the predicted part of speech is picked out from the dictionary. letter strings and inflexion tables This routine translates a word written in Roman letters into a phoneme string using a table \ [ ii\ ] . A translated phoneme string of a predicted word is used as a reference for matching an input phoneme string. This routine can also extract the characteristic phoneme string of a word. A characteristic phoneme string of a word contains only phonemes to be surely extracted from the speech wave. It is composed of vowels , /s/ and silence , and represents major acoustic information of a word. Some examples of the phoneme strings are shown in Table 3.3. For matching procedure between an input phoneme string and a predicted word are used both phoneme and characteristic phoneme strings of the word. Here , these phoneme strings are not stored in the word dictionary. The system has only one word dictionary written in Roman letters and phoneme stringsnecessary for matching are produced each time from the word dictionary using the translating routine. This fact makes it very easy to enrich the entry of vocabulary. part of Word speech C0~ WOO 1 W002 CI -- -- -- ~ WI01 WI02 C2 -- -- -~ W201 W202 Fig. 3.6 Word dictionary. Table 3.3 Examples of phoneme and characteristic phoneme strings of words. P : unvoiced stop , N : nasal , B : buzz , . : silence. Word Phoneme Characteristic ( Pronunciation ) string phoneme string OZIISAN OBSIISAN OISA YAMA IEAMA AA SENTAKU SEN.PA.PU SE.A.U OOKII OO.PSI O.SI Semantic information is used for the following purposes. ( i ) Elimination of semantically inconsistent sentences which have been recognized using only acoustic and syntactic information. ( ii ) Future development to semantic understanding of natural language by forming semantic networks. ( iii ) Control of transition on the syntactic state transition network through the syntax analyser. One of the semantic information dealt with is `` knowledge about meaning '' . This knowledge involves ( i ) what each word means , ( ii ) verb-centered semantic structure , and ( iii ) schema of a story \ [ i0\ ] . The other information is , so called , `` remembrance of episode '' which means the remembrance of a topic of conversation. In the present system , meaning of a word is represented by a list structure , and the others are represented by networks. In the system the knowledge about meaning must be given from outside and can not yet be increased or updated by itself , but remembrance of episode can be increased or updated whenever new information comes in. While , if a schema has been already formed for a topic to be talked from now on , the knowledge of the topic will help recognition of the spoken topic. In the following sections how semantic information works in the recognition system will be explained. Denote a word by n , its characteristic features by fi ( i=l , ... , m ;</definiens>
			</definition>
			<definition id="2">
				<sentence>-- shows a phrase , shows modification and RENYO in \ [ \ ] means this phrase modifies an inflexional word or phrase , ino : in order to .</sentence>
				<definiendum id="0">]</definiendum>
				<definiendum id="1">phrase</definiendum>
				<definiens id="0">means this phrase modifies an inflexional word or</definiens>
			</definition>
			<definition id="3">
				<sentence>The syntax analyser gives to each word sequence necessary syntactic information such as part of speech of each component word , phrase and modifying relation between an old man &lt; &gt; l ' ~sub ~ and Tisa / and from / ~ f~ ~ ~from } T / \tot / IL / .</sentence>
				<definiendum id="0">syntax analyser</definiendum>
				<definiens id="0">part of speech of each component word , phrase and modifying relation between an old man &lt; &gt; l</definiens>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>Kana is the Japanese phonemic writing system .</sentence>
				<definiendum id="0">Kana</definiendum>
				<definiens id="0">the Japanese phonemic writing system</definiens>
			</definition>
			<definition id="1">
				<sentence>According to /4/ the Kanji Teletype ( Kantele ) is the most commonly used encoding equipment .</sentence>
				<definiendum id="0">Kanji Teletype ( Kantele )</definiendum>
				<definiens id="0">the most commonly used encoding equipment</definiens>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>The result of our effort was the program system known as TEAM which is based on a terminology data bank which at present contains almost a million terminological units , i.e. lexical entries based on a defined concept and offering terms expressing this concept in up to eight languages , these languages being German , English , French , Spanish , Russian , Italian , /~ortuguese , and Dutch .</sentence>
				<definiendum id="0">TEAM</definiendum>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>Paul d'oter cela ) 0 S 0 The new notation oNPs denotes a noun phrase having a double function F : it must be an accept§To do so , one has , for example , to reinterpret the tree structure of the sentence ( cfo Chomsky7 ) .</sentence>
				<definiendum id="0">oNPs</definiendum>
			</definition>
			<definition id="1">
				<sentence>The passive transformation consists in matching an active phrase with its passive counterpart .</sentence>
				<definiendum id="0">passive transformation</definiendum>
				<definiens id="0">matching an active phrase with its passive counterpart</definiens>
			</definition>
			<definition id="2">
				<sentence>Suppose now that I factorize the selection rules from a set of forms that constitute an equivalence class , for example , from the 'active ' and the 'passive ' forms ; I place a separator p between the forms of the equivalence class : ( 29 ) S ÷ NP s t V 2 NPo/p/ NP o t ~tre V2~ ( par NPs ) /p/ I1 t ~tre V2e NP o ( par NPs ) ; + N~ `` NP o + N\ ] NP s In this formulation , the selection rules are no longer duplicated ; moreover , we can interpret the separator p between the members of the equivalence class as indicating a relation between the sentence schemas so separated .</sentence>
				<definiendum id="0">selection rules</definiendum>
			</definition>
			<definition id="3">
				<sentence>Generative grammars without transformation rules : a defense of phrase structure , Language , Vol .</sentence>
				<definiendum id="0">Generative grammars</definiendum>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>The ineffectiveness of QWERTY is an inevitable consequence of its origin .</sentence>
				<definiendum id="0">ineffectiveness of QWERTY</definiendum>
				<definiens id="0">an inevitable consequence of its origin</definiens>
			</definition>
			<definition id="1">
				<sentence>Typing is a highly complicated procedure deeply involving mental activities as well as physical movements .</sentence>
				<definiendum id="0">Typing</definiendum>
				<definiens id="0">a highly complicated procedure deeply involving mental activities as well as physical movements</definiens>
			</definition>
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>If we take a simpler version of the ATN-grammar , namely the RTN-grammar ( `` recursive transition networks '' ) ( cf. \ [ Wo 70\ ] ) , then an ATN-grammar is nothing else but an attributed RTN-grammar ; so we could read the letters `` ATN '' as `` attributed transition networks '' instead of `` augmented transition networks '' .</sentence>
				<definiendum id="0">ATN-grammar</definiendum>
				<definiendum id="1">ATN</definiendum>
				<definiens id="0">'' as `` attributed transition networks '' instead of `` augmented transition networks ''</definiens>
			</definition>
			<definition id="1">
				<sentence>N~ , pRNAGEp • in and N~yNG an d Y6 { N , NPR } and sort ( translat ( Y ) ) = ORT then translat ( in ) = \ [ LAMBDA X.ORT \ [ IN translat ( Y ) X.ORT\ ] \ ] else if N~p and NAy NO and in sort ( translat ( Y ) ) _~ INT then translat ( in ) = \ [ LAMBDA X. INT\ [ INTEMP translat ( Y ) X.INT\ ] \ ] else if ~ and so ranslat ( NG ) ) ~INT &amp; in then translat ( in ) = \ [ LAMBDA X.INT \ [ INTEMP translat ( NG ) X.INT\ ] \ ] else translat ( in ) = 0o Stuttgart : translat ( Stuttgart ) = STUTTGART M : if ~ and Z ' 6 { NG , PNG } and ~Z~ ' ~NG ( ( z6 { Monat , Jahr } and cat ( translat ( Y ) ) = KONST and sort ( translat ( Y ) ) = INT ) o r ( YE { N , NPR } and cat ( translat ( Y ) ) = TERM and cat ( translat ( Z ) ) = LTERM and sort ( translat ( Y ) ) = sort ( translat ( Z ) ) ) ) then translat ( N ) = The else-part here is assumed to be substituted by the general rule prescribing that whenever a TR-rule does not apply , the translation of the node will be the translation of one of its daughter nodes , e.g. here we could write : else translat ( N ) = translat ( Z ) From the TR-rule for PNG we will only give some part : PNG : i_~ ~NG and cat ( translat ( N ) ) KONST then if ~PNG an d Y6 { DET , QDET , WDET , NEGPET , ZAHL } and cat ( translat ( Y ) ) = QUANT o¥ then if cat ( translat ( PRAEP ) ) = LTERM then translat ( PNG ) = \ [ translat ( Y ) translat ( PRAEP ) \ ] else translat ( PNG ) = -\ [ translat ( Y ) translat ( N ) \ ] else if cat ( translat ( ?</sentence>
				<definiendum id="0">~NG</definiendum>
				<definiens id="0">translat ( Z ) ) = LTERM and sort</definiens>
				<definiens id="1">assumed to be substituted by the general rule prescribing that whenever a TR-rule does not apply , the translation of the node will be the translation of one of its daughter nodes</definiens>
			</definition>
			<definition id="2">
				<sentence>Let X ° + X I X 2 ... X n ( n ~ O ) be a production , where the X. ( i = l , ... , n ) are terminal or nonterminal symbols .</sentence>
				<definiendum id="0">X n</definiendum>
				<definiens id="0">l , ... , n ) are terminal or nonterminal symbols</definiens>
			</definition>
			<definition id="3">
				<sentence>j ) else if +val ( PRAEP ) = ~0 then +val ( PNG ) = +val ( NPR ) else +val ( PNG ) = +val ( PRAEP ) +tree ( PNG ) = { ( +poe ( PNG ) , PNG , +val ( PNG ) ) } U +tree ( PRAEP ) U +tree ( NPR ) +tree ( PRAEP ) = +tree ( PNG ) % tree ( NPR ) = +tree ( PNG ) % poe ( PRAEP ) = +poe ( PNG ) .</sentence>
				<definiendum id="0">NPR</definiendum>
			</definition>
			<definition id="4">
				<sentence>j ) = symb ( ( k.2.1.1 I ) .2 ) = symb ( k.2.1.2 ) = NPR -- 403-Further sort ( value ( ( +pos ( PRAEP ) -l ) .2 ) ) = sort ( value ( k.2.1.2 ) ) = sort ( STUTTGART ) = ORT The second condition holds too , thus we get +vai ( PRAEP ) = \ [ LAMBDA X.ORT\ [ IN STUTTGART X.ORT\ ] \ ] Within the production PNG ÷ PRAEP NPR the first condition needed to determine +vai ( PNG ) does not hold , so we get % vai ( PNG ) = +val ( PRAEP ) If we assume these values to be substituted in +tree , we now have the intermediate result { ( k , NG/PNG , +vai ( NG/PNG ) ) , ( k*l , NG , +val ( NG ) ) , ( k.lol , N , +val ( N ) ) , ( kolol .</sentence>
				<definiendum id="0">value ( ( +pos</definiendum>
				<definiens id="0">\ [ LAMBDA X.ORT\ [ IN STUTTGART X.ORT\ ] \ ] Within the production PNG ÷ PRAEP NPR the first condition needed to determine +vai ( PNG ) does not hold</definiens>
			</definition>
			<definition id="5">
				<sentence>Wulz , An overview of PLIDIS , a problem solving information system with German as query language in : L. Bolc ( ed . )</sentence>
				<definiendum id="0">Wulz</definiendum>
				<definiens id="0">a problem solving information system with German as query language in : L. Bolc ( ed</definiens>
			</definition>
</paper>

	</volume>
