<?xml version="1.0" encoding="UTF-8"?>
	<volume id="J96">

		<paper id="4002">
			<definition id="0">
				<sentence>Alignment is a neglected part of the computerization of the comparative method .</sentence>
				<definiendum id="0">Alignment</definiendum>
				<definiens id="0">a neglected part of the computerization of the comparative method</definiens>
			</definition>
			<definition id="1">
				<sentence>WORDSURV : A program for analyzing language survey word lists .</sentence>
				<definiendum id="0">WORDSURV</definiendum>
				<definiens id="0">A program for analyzing language survey word lists</definiens>
			</definition>
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>The limited attention constraint has been defined by some researchers by linear recency : a representation of an utterance A is linearly recent for a representation of an utterance B if A is linearly adjacent to B. Using linear recency as a model of the limited attention constraint would mean that an antecedent for an anaphor is determined by a linear backward search of the text , or of a discourse model representation of the text ( Clark and Sengul 1979 , inter alia ) .</sentence>
				<definiendum id="0">linear recency</definiendum>
				<definiens id="0">a representation of an utterance A is linearly recent for a representation of an utterance B if A is linearly adjacent to B. Using linear recency as a model of the limited attention constraint would mean that an antecedent for an anaphor is determined by a linear backward search of the text , or of a discourse model representation of the text ( Clark and Sengul 1979</definiens>
			</definition>
			<definition id="1">
				<sentence>In the remainder of this squib , I will argue that the limited attention constraint must account for three types of evidence : ( 1 ) the occurrence of informationally redundant utterances in naturally occurring dialogues ( Walker 1993 ) ; ( 2 ) the infelicity of discourses that depend on accessing discourse entities that are not linearly recent ; and ( 3 ) experiments that show that humans have limited attentional capacity ( Miller 1956 ; Baddeley 1986 ) .</sentence>
				<definiendum id="0">attentional capacity</definiendum>
				<definiens id="0">( 1 ) the occurrence of informationally redundant utterances in naturally occurring dialogues</definiens>
			</definition>
			<definition id="2">
				<sentence>The IRU may function this way since : ( 1 ) the IRU reinstantiates the necessary information in the cache ; or ( 2 ) the IRU is a retrieval cue for retrieval of information to the cache .</sentence>
				<definiendum id="0">IRU</definiendum>
				<definiens id="0">the necessary information in the cache</definiens>
				<definiens id="1">a retrieval cue for retrieval of information to the cache</definiens>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>Kowtko , Isard , and Doherty ( 1992 ; henceforth KID ) , in arguing that it is possible to mark conversational move boundaries , cite separately for each of three naive coders the ratio of the number of times they agreed with an `` expert '' coder about the existence of a boundary over the number of times either the naive coder or the expert marked a boundary .</sentence>
				<definiendum id="0">KID</definiendum>
			</definition>
			<definition id="1">
				<sentence>1 The kappa coefficient ( K ) measures pairwise agreement among a set of coders making category judgments , correcting for expected chance agreement : K P ( A ) P ( E ) 1 P ( E ) where P ( A ) is the proportion of times that the coders agree and P ( E ) is the proportion of times that we would expect them to agree by chance , calculated along the lines of the intuitive argument presented above .</sentence>
				<definiendum id="0">kappa coefficient</definiendum>
				<definiendum id="1">P ( A )</definiendum>
				<definiens id="0">the proportion of times that the coders agree and P ( E ) is the proportion of times that we would expect them to agree by chance , calculated along the lines of the intuitive argument presented above</definiens>
			</definition>
</paper>

		<paper id="3005">
</paper>

		<paper id="3002">
			<definition id="0">
				<sentence>A major improvement here , in comparison to Suppes , B ( Sttner , and Liang ( 1995 ) , is that the association relation is generalized from a unique correspondence between words and the program 's internal representation of their meaning to a many-to-one relation , which permits different words to be associated to the same internal representation .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">a unique correspondence between words and the program 's internal representation of their meaning to a many-to-one relation , which permits different words to be associated to the same internal representation</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , the phrase the nut generalizes to the grammatical form the O , where O is the category of objects .</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">the category of objects</definiens>
			</definition>
			<definition id="2">
				<sentence>If a learner is presented a sentence , then using the associations and grammatical rules stored in long-term memory , the learner attempts to construct a semantic interpretation of the sentence and respond accordingly .</sentence>
				<definiendum id="0">learner</definiendum>
				<definiens id="0">attempts to construct a semantic interpretation of the sentence and respond accordingly</definiens>
			</definition>
			<definition id="3">
				<sentence>( c ) If ( i ) XlWl `` '' Wm-lXm is a substring of g , where the Xi , i = 1 , ... , m are not necessarily distinct category names and wi are substrings , possibly empty , or words that have no association to internal symbols on the given trial , and ( ii ) 7 ' ( X~o ) ... . , X~ ( m ) ) is the minimal Lisp form of 3` containing X. ( 1 ) ... . , X~ ( m ) , then : Xl W 1 ... Wm_ l Xm , ' , , T ( XTr ( 1 ) ... .. XTc ( m ) ) , where 7c is a permutation of the numbers 1 ... .. m. To show how Axiom 1.4 ' works , assume we have arrived at the following association of grammatical forms : A1 the PROP OBI ~ ( fal A1 ( io ( fo PROP ( fo OBJ , ) ) ) ) ( 13 ) which could be obtained as a generalization , for instance , from the command Get the red screw with the words correctly associated .</sentence>
				<definiendum id="0">XlWl `` '' Wm-lXm</definiendum>
				<definiendum id="1">, X~</definiendum>
				<definiendum id="2">T ( XTr</definiendum>
				<definiendum id="3">7c</definiendum>
				<definiens id="0">a substring of g , where the Xi , i = 1 , ... , m are not necessarily distinct category names and wi are substrings , possibly empty , or words that have no association to internal symbols on the given trial</definiens>
				<definiens id="1">the minimal Lisp form of 3` containing X. ( 1 ) ... .</definiens>
			</definition>
			<definition id="4">
				<sentence>( 17 ) From ( 15 ) , Rule X of Table 1 and Form Generalization ( Axiom 1.2 ) : PROP S ~ ( /Co PROP S ) , ( 18 ) and finally from Grammar-Rule Generation ( Axiom 1.3 ) : S -- * PROP S ( 19 ) 338 Suppes , Bottner , and Liang Machine Learning Comprehension Grammars as a rule of English grammar .</sentence>
				<definiendum id="0">Form Generalization</definiendum>
				<definiendum id="1">PROP S ~</definiendum>
				<definiens id="0">Grammar-Rule Generation ( Axiom 1.3 ) : S -- * PROP S ( 19 ) 338 Suppes , Bottner , and Liang Machine Learning Comprehension Grammars as a rule of English grammar</definiens>
			</definition>
			<definition id="5">
				<sentence>For example , French has no single idiomatic word for the nonspatial meaning of the English adjective medium , so we used the circumlocutionary phrase de taille moyenne .</sentence>
				<definiendum id="0">French</definiendum>
				<definiens id="0">has no single idiomatic word for the nonspatial meaning of the English adjective medium , so we used the circumlocutionary phrase de taille moyenne</definiens>
			</definition>
			<definition id="6">
				<sentence>As might be expected , Russian has the largest number of internal symbols with more than one association , just because of its rich inflectional patterns .</sentence>
				<definiendum id="0">Russian</definiendum>
				<definiens id="0">has the largest number of internal symbols with more than one association</definiens>
			</definition>
			<definition id="7">
				<sentence>\ [ 71\ ] +S + + + + + + + + + + O -- + \ [ 77\ ] + S + X S~PROP+\ [ 3'4\ ] +S + + + + + + + + S -- + S + \ [ 3 '' 3\ ] + PROP + + + XI S -* OBJ + + + + + + + + + + For the third missing permutation , we obtained from our Korean informant the following example : K : G O A3 ku nasapati yephey ( han ) nasa-lul nohala .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">] +S + + + + + + + + S -- + S + \ [ 3 '' 3\ ] + PROP + + + XI S -* OBJ + + + + + + + + + + For the third missing permutation</definiens>
			</definition>
			<definition id="8">
				<sentence>Although Siskind ( 1994 ) concentrates entirely on lexical meaning , his seven-step procedure , which constitutes a learning algorithm , bears resemblance at the top level but not in detail to our procedure .</sentence>
				<definiendum id="0">seven-step procedure</definiendum>
				<definiens id="0">constitutes a learning algorithm , bears resemblance at the top level</definiens>
			</definition>
</paper>

		<paper id="3006">
			<definition id="0">
				<sentence>We use the term Intentional Linguistic Structure , or ILS , as a theory-neutral way of referring to the structure of a discourse determined by the speaker 's intentions .</sentence>
				<definiendum id="0">ILS</definiendum>
			</definition>
			<definition id="1">
				<sentence>Roughly speaking , an RST nucleus expresses a G &amp; S intention In , a satellite expresses another intention Is and , in G &amp; S terms , In dominates Is .</sentence>
				<definiendum id="0">RST nucleus</definiendum>
				<definiens id="0">expresses a G &amp; S intention In , a satellite expresses another intention Is and , in G &amp; S terms</definiens>
			</definition>
			<definition id="2">
				<sentence>Speakers intend for the intentions behind their utterances to be recognized and for that recognition to be part of what makes their utterances effective .</sentence>
				<definiendum id="0">Speakers</definiendum>
				<definiens id="0">intend for the intentions behind their utterances to be recognized and for that recognition to be part of what makes their utterances effective</definiens>
			</definition>
			<definition id="3">
				<sentence>In other words , I , , is the discourse segment purpose ( DSP ) of DS , .</sentence>
				<definiendum id="0">DSP</definiendum>
				<definiens id="0">the discourse segment purpose</definiens>
			</definition>
			<definition id="4">
				<sentence>DSH is embedded in another segment DS , , just when the purposes of the two segments are in the 410 Moser and Moore Discourse Structure Intentional Structure I0 : Intends ( IntendH a ) I I1 : Intends ( Believe , b ) I I2 : Intends ( BelieveH c ) DS0 Linguistic Structure ~S ome and see the LA Chamber Ballet 's concert .</sentence>
				<definiendum id="0">DSH</definiendum>
				<definiens id="0">Intends ( Believe , b ) I I2 : Intends ( BelieveH c ) DS0 Linguistic Structure ~S ome and see the LA Chamber Ballet 's concert</definiens>
			</definition>
			<definition id="5">
				<sentence>An RST analyst must judge which schema consists of RST relation definitions whose constraints and effects best describe the nucleus and satellite spans in the schema application .</sentence>
				<definiendum id="0">RST analyst</definiendum>
				<definiens id="0">RST relation definitions whose constraints and effects best describe the nucleus and satellite spans in the schema application</definiens>
			</definition>
			<definition id="6">
				<sentence>The volitional-cause relation is defined as one in which the nucleus presents a volitional action and the satellite presents a situation that could have caused the agent to perform the action .</sentence>
				<definiendum id="0">volitional-cause relation</definiendum>
				<definiens id="0">one in which the nucleus presents a volitional action and the satellite presents a situation that could have caused the agent to perform the action</definiens>
			</definition>
			<definition id="7">
				<sentence>DS0 is a segment/span designed to achieve the purpose I0 .</sentence>
				<definiendum id="0">DS0</definiendum>
			</definition>
			<definition id="8">
				<sentence>In turn , DS1 is a segment/span designed to achieve the purpose I1 by first manifesting I1 in the expression of the core/nucleus ( b ) and then providing evidence in the embedded segment/satellite ( c ) .</sentence>
				<definiendum id="0">DS1</definiendum>
				<definiendum id="1">I1</definiendum>
				<definiens id="0">a segment/span designed to achieve the purpose</definiens>
			</definition>
</paper>

		<paper id="4001">
			<definition id="0">
				<sentence>In spite of this deficiency , the Hubert-Labbe curve appears to be an optimal smoother , and this suggests that the value obtained for the coefficient of vocabulary partition p is a fairly reliable estimate of the extent to which a text is characterized by lexical specialization .</sentence>
				<definiendum id="0">Hubert-Labbe curve</definiendum>
				<definiens id="0">appears to be an optimal smoother , and this suggests that the value obtained for the coefficient of vocabulary partition p is a fairly reliable estimate of the extent to which a text is characterized by lexical specialization</definiens>
			</definition>
			<definition id="1">
				<sentence>Lexical specialization , informally defined as topic-linked concentrated word usage , and formalized in terms of underdispersion , provides us with the required tool .</sentence>
				<definiendum id="0">Lexical specialization</definiendum>
				<definiens id="0">topic-linked concentrated word usage , and formalized in terms of underdispersion , provides us with the required tool</definiens>
			</definition>
			<definition id="2">
				<sentence>The number of underdispersed types in text slice k , VU ( k ) , and the corresponding number of underdispersed tokens , NU ( k ) , can now be defined as VU ( k ) : ~_~di , k ( 7 ) i NU ( k ) = Y~di , k'fi , k. ( 8 ) i We are now in a position to investigate where underdispersed words appear and how they influence the observed growth curve of the vocabulary .</sentence>
				<definiendum id="0">NU ( k )</definiendum>
				<definiens id="0">The number of underdispersed types in text slice k , VU ( k ) , and the corresponding number of underdispersed tokens</definiens>
			</definition>
			<definition id="3">
				<sentence>The upper panels plot VU ( k ) ( left ) and NU ( k ) ( right ) , the numbers of underdispersed types and tokens appearing in the successive text chunks .</sentence>
				<definiendum id="0">upper panels plot VU ( k )</definiendum>
				<definiendum id="1">NU</definiendum>
			</definition>
			<definition id="4">
				<sentence>VU ( k ) and NU ( k ) : numbers of underdispersed types and tokens in text slice k ; ACF : auto-correlation function ; Pr ( U , type ) and Pr ( U , token ) : proportions of underdispersed types and tokens ; D ( k ) and DU ( k ) : progressive difference scores for the overall vocabulary and the underdispersed words .</sentence>
				<definiendum id="0">NU ( k )</definiendum>
				<definiendum id="1">DU</definiendum>
			</definition>
			<definition id="5">
				<sentence>Let AV ( Mk ) = V ( Mk ) V ( Mk_~ ) ( 9 ) denote the number of new types observed in text slice k , and let aVU ( Mk ) = VU ( Mk ) -VU ( Mk_ , ) ( 10 ) denote the number of new underdispersed types for text slice k. The proportion of new underdispersed types in text slice k on the total number of new types , Pr ( U , type , k ) is given by AVU ( k ) ( 11 ) Pr ( U , type , k ) AV ( k ) The plot of Pr ( U , types , k ) is shown on the third row of Figure 4 ( left-hand panel ) .</sentence>
				<definiendum id="0">AV</definiendum>
				<definiens id="0">the number of new types observed in text slice k , and let aVU ( Mk ) = VU ( Mk ) -VU ( Mk_ , ) ( 10 ) denote the number of new underdispersed types for text slice k. The proportion of new underdispersed types in text slice k on the total number of new types</definiens>
				<definiens id="1">k ) ( 11 ) Pr ( U , type , k ) AV ( k ) The plot of Pr ( U , types , k</definiens>
			</definition>
			<definition id="6">
				<sentence>VU ( k ) and NU ( k ) : numbers of underdispersed types and tokens in text slice k ; Pr ( U , type ) and Pr ( U , token ) : proportions of underdispersed types and tokens ; D ( k ) and DU ( k ) : progressive difference scores for the overall vocabulary and the underdispersed words ; f\ [ Ahab\ ] ( k ) : frequency of Ahab in text slice k. ( F ( 1,38 ) -2.11 , p &gt; .15 for VU ( k ) , F ( 1,38 ) = 1.98 , p &gt; .15 for NU ( k ) ) for underdispersion to occur more often as the novel progresses .</sentence>
				<definiendum id="0">NU ( k )</definiendum>
				<definiens id="0">progressive difference scores for the overall vocabulary and the underdispersed words</definiens>
			</definition>
			<definition id="7">
				<sentence>VU ( k ) and NU ( k ) : numbers of underdispersed types and tokens in text slice k ; ACF : auto-correlation function ; Pr ( U , type ) and Pr ( U , token ) : proportions of underdispersed types and tokens ; D ( k ) and DU ( k ) : progressive difference scores for the overall vocabulary and the underdispersed words .</sentence>
				<definiendum id="0">NU ( k )</definiendum>
				<definiendum id="1">DU</definiendum>
			</definition>
			<definition id="8">
				<sentence>E\ [ V ( N , f ) \ ] and V ( N , f ) : expected and observed frequency spectrum ; E\ [ V ( N ) \ ] and V ( N ) : expected and observed numbers of types ; Mp ( f ) : population probability mass of the types with frequency f in the sample ; MGT ( f ) : Good-Turing estimate of Mp ( f ) ; Ms ( f ) : unadjusted sample estimate of Mp ( f ) .</sentence>
				<definiendum id="0">f )</definiendum>
				<definiendum id="1">MGT</definiendum>
				<definiens id="0">population probability mass of the types with frequency f in the sample</definiens>
			</definition>
			<definition id="9">
				<sentence>It is easy to see why 7~ ( N ) is an upper bound for coherent text by focusing on its interpretation .</sentence>
				<definiendum id="0">N )</definiendum>
				<definiens id="0">an upper bound for coherent text by focusing on its interpretation</definiens>
			</definition>
			<definition id="10">
				<sentence>Notation : MCTOC , M ) : Good-Turing estimate ; Ms ( f , M ) : sample estimate ; Mh ( f , M ) : heuristic estimate ; Mp ( f ) : population mass .</sentence>
				<definiendum id="0">Mp</definiendum>
				<definiens id="0">Good-Turing estimate ; Ms ( f , M ) : sample estimate</definiens>
				<definiens id="1">heuristic estimate ;</definiens>
			</definition>
			<definition id="11">
				<sentence>477 Computational Linguistics Volume 22 , Number 4 Finally note that ( 27 ) suggests that , under randomness , and conditional on the words appearing in the sample of N tokens , f ( i , M ) can alternatively be viewed as a binomially distributed random variable with parameters M/N andf ( i , N ) forf ( i , N ) ( ( M , N ( Muller 1977 ) : E\ [ V ( M ) \ ] : V E V ( N , f ) e-My f ( , ~ V~- ' .</sentence>
				<definiendum id="0">f ) e-My f</definiendum>
				<definiens id="0">E\ [ V ( M ) \ ] : V E V ( N</definiens>
			</definition>
</paper>

		<paper id="3004">
			<definition id="0">
				<sentence>In ( b ) is a plausible segmentation for this sentence ; in ( c ) is an implausible segmentation .</sentence>
				<definiendum id="0">b )</definiendum>
				<definiens id="0">an implausible segmentation</definiens>
			</definition>
			<definition id="1">
				<sentence>Chinese word segmentation can be viewed as a stochastic transduction problem .</sentence>
				<definiendum id="0">Chinese word segmentation</definiendum>
				<definiens id="0">a stochastic transduction problem</definiens>
			</definition>
			<definition id="2">
				<sentence>This WFST represents the segmentation of the text into the words AB and CD , word boundaries being marked by arcs mapping between ¢ and part-of-speech labels .</sentence>
				<definiendum id="0">WFST</definiendum>
				<definiens id="0">the segmentation of the text into the words AB and CD , word boundaries being marked by arcs mapping between ¢ and part-of-speech labels</definiens>
			</definition>
			<definition id="3">
				<sentence>Assuming unseen objects within each class are equiprobable , their probabilities are given by the Good-Turing theorem as : pCoZ~ c~ E ( n~1~ ) N • E ( N~/s ) ( 2 ) where p~tS is the probability of one unseen hanzi in class cls , E ( n~ ts ) is the expected number of hanzi in cls seen once , N is the total number of hanzi , and E ( N~ t~ ) is the expected number of unseen hanzi in class cls .</sentence>
				<definiendum id="0">p~tS</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">the probability of one unseen hanzi in class cls</definiens>
				<definiens id="1">the expected number of hanzi in cls seen once</definiens>
				<definiens id="2">the total number of hanzi , and E ( N~ t~ ) is the expected number of unseen hanzi in class cls</definiens>
			</definition>
			<definition id="4">
				<sentence>An example of a fairly low-level relation is the affix relation , which holds between a stem morpheme and an affix morpheme , such as ~ -menO ( PL ) .</sentence>
				<definiendum id="0">affix relation</definiendum>
				<definiens id="0">holds between a stem morpheme and an affix morpheme</definiens>
			</definition>
			<definition id="5">
				<sentence>A high-level relation is agent , which relates an animate nominal to a predicate .</sentence>
				<definiendum id="0">high-level relation</definiendum>
				<definiendum id="1">agent</definiendum>
				<definiens id="0">relates an animate nominal to a predicate</definiens>
			</definition>
</paper>

		<paper id="4004">
			<definition id="0">
				<sentence>A written Chinese sentence consists of a series of evenly spaced Chinese characters .</sentence>
				<definiendum id="0">written Chinese sentence</definiendum>
				<definiens id="0">consists of a series of evenly spaced Chinese characters</definiens>
			</definition>
			<definition id="1">
				<sentence>Namely , ( i ) the conceptual network , which is a network of nodes and links representing some permanent linguistic concepts ; ( ii ) the workspace , which is the working area in which high-level linguistic structures representing the system 's current understanding of a sentence are built and modified ; ( iii ) the coderack , which is a pool of structure-building agents ( codelets ) waiting to run ; and ( iv ) the computational temperature , which is an approximate measure of the amount of disorganization in the system 's understanding of a sentence .</sentence>
				<definiendum id="0">conceptual network</definiendum>
				<definiendum id="1">workspace</definiendum>
				<definiendum id="2">; ( iii</definiendum>
				<definiendum id="3">computational temperature</definiendum>
				<definiens id="0">a network of nodes and links representing some permanent linguistic concepts</definiens>
				<definiens id="1">the working area in which high-level linguistic structures representing the system 's current understanding of a sentence are built and modified</definiens>
				<definiens id="2">a pool of structure-building agents ( codelets ) waiting to run</definiens>
				<definiens id="3">an approximate measure of the amount of disorganization in the system 's understanding of a sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>The computational temperature is an approximate measure of the amount of coherency in the system 's interpretation of a sentence : the value at a given time is a function of the amount and quality of linguistic structures that have been built in the workspace .</sentence>
				<definiendum id="0">computational temperature</definiendum>
				<definiens id="0">an approximate measure of the amount of coherency in the system 's interpretation of a sentence : the value at a given time is a function of the amount and quality of linguistic structures that have been built in the workspace</definiens>
			</definition>
			<definition id="3">
				<sentence>The temperature-regulated urgency ( Lit ) is derived in the following way : Ut = U ~120-t ) /30 ( 1 ) where t denotes the temperature , which ranges between \ [ 0,1001 .</sentence>
				<definiendum id="0">temperature-regulated urgency</definiendum>
			</definition>
			<definition id="4">
				<sentence>The affinity relation is a quantitative measure that reflects how strongly two characters co-occur statistically .</sentence>
				<definiendum id="0">affinity relation</definiendum>
				<definiens id="0">a quantitative measure that reflects how strongly two characters co-occur statistically</definiens>
			</definition>
			<definition id="5">
				<sentence>St ~ S ( 120-t ) /40 ( 4 ) where St is the temperature-regulated strength , S is the original strength , and t is the temperature .</sentence>
				<definiendum id="0">St</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">the temperature-regulated strength ,</definiens>
			</definition>
			<definition id="6">
				<sentence>Activated nodes in the conceptual network spread activation to their neighbors , and thus concepts closely related to relevant concepts also become relevant .</sentence>
				<definiendum id="0">Activated</definiendum>
				<definiens id="0">nodes in the conceptual network spread activation to their neighbors , and thus concepts closely related to relevant concepts also become relevant</definiens>
			</definition>
			<definition id="7">
				<sentence>We have demonstrated in Section 5 how we have made use of statistics ( the maximum matching heuristics and mutual information ) to avoid generating all possible word boundary combinations .</sentence>
				<definiendum id="0">statistics</definiendum>
				<definiens id="0">the maximum matching heuristics and mutual information ) to avoid generating all possible word boundary combinations</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>If , during the construction of Y , a final state is reached without violating the cut-off edit distance constraint , and ed ( X\ [ m\ ] , Y\ [ n\ ] ) &lt; t at that point , then Y is a valid correct form of the incorrect input string } Denoting the states by subscripted q 's ( q0 being the initial state ) and the symbols in the alphabet ( and labels on the directed edges ) by a , we present the algorithm for generating all Y 's by a ( slightly modified ) depth-first probing of the graph in Figure 3 .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">a valid correct form of the incorrect input string } Denoting the states by subscripted q 's ( q0 being the initial state ) and the symbols in the alphabet ( and labels on the directed edges</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , all entries required by H ( i + 1 , j + 1 ) , except H ( i , j + 1 ) , are already available in the matrix in columns i 1 and i. The computation of cuted ( X\ [ m\ ] , Y\ [ n\ ] ) involves a loop in which the minimum is computed .</sentence>
				<definiendum id="0">] )</definiendum>
				<definiens id="0">involves a loop in which the minimum is computed</definiens>
			</definition>
			<definition id="2">
				<sentence>4 The morpheme breakdown ( with morphological glosses underneath ) is : 5 uygar +la~ +tlr +ama +yabil +ecek civilized +AtoV +CAUS +NEG +POT +VtoA ( AtoN ) +ler +imiz +den +mi~ +siniz +cesine +3PL +POSS-1PL +ABL ( +NtoV ) +PAST +2PL +VtoAdv The portion of the word following the root consists of 11 morphemes , each of which either adds further syntactic or semantic information to , or changes the part-of-speech of , the part preceding it .</sentence>
				<definiendum id="0">morpheme breakdown</definiendum>
				<definiendum id="1">morphological glosses underneath )</definiendum>
				<definiendum id="2">+AtoV +CAUS +NEG +POT +VtoA</definiendum>
				<definiendum id="3">AtoN</definiendum>
				<definiens id="0">+2PL +VtoAdv The portion of the word following the root consists of 11 morphemes , each of which either adds further syntactic or semantic information to , or changes the part-of-speech of , the part preceding it</definiens>
			</definition>
			<definition id="3">
				<sentence>6 Below , we provide a transcript of a run : 7 ENTER WORD &gt; eva Threshold 0 ... i ... ela = &gt; ( ( CAT evla = &gt; ( ( CAT ava = &gt; ( ( CAT deva = &gt; ( ( CAT NOUN ) ( ROOT eda = &gt; ( ( CAT NOUN ) ( ROOT ela = &gt; ( ( CAT NOUN ) ( ROOT enva = &gt; ( ( CAT NOUN ) ( ROOT reva = &gt; ( ( CAT NOUN ) ( ROOT evi = &gt; ( ( CAT NOUN ) ( ROOT eve = &gt; ( ( CAT NOUN ) ( ROOT ev = &gt; ( ( CAT NOUN ) ( ROOT evi = &gt; ( ( CAT NOUN ) ( ROOT eza = &gt; ( ( CAT NOUN ) ( ROOT leva = &gt; ( ( CAT NOUN ) ( ROOT neva = &gt; ( ( CAT NOUN ) ( ROOT ova = &gt; ( ( CAT NOUN ) ( ROOT ova = &gt; ( ( CAT VERB ) ( ROOT ADJ ) ( ROOT ela ) ) ADJ ) ( ROOT evla ) ) NOUN ) ( ROOT av ) ( AGR 3SG ) ( POSS NONE ) ( CASE DAT ) ) deva ) ( AGR 3SG ) ( POSS NONE ) ( CASE NOM ) ) eda ) ( AGR 3SG ) ( POSS NONE ) ( CASE NOM ) ) ela ) ( AGR 3SG ) ( POSS NONE ) ( CASE NOM ) ) enva ) ( AGR 3SG ) ( POSS NONE ) ( CASE NOM ) ) reva ) ( AGR 3SG ) ( POSS NONE ) ( CASE NOM ) ) ev ) ( AGR 3SG ) ( POSS NONE ) ( CASE ACC ) ) ev ) ( AGR 3SG ) ( POSS NONE ) ( CASE OAT ) ) ev ) ( AGR 3SG ) ( POSS NONE ) ( CASE NOM ) ) ev ) ( AGR 3SG ) ( POSS 3SG ) ( CASE NOM ) ) eza ) ( AGR 3SG ) ( POSS NONE ) ( CASE NOM ) ) leva ) ( AGR 3SG ) ( POSS NONE ) ( CASE NOM ) ) neva ) ( AGR 3SG ) ( POSS NONE ) ( CASE NOM ) ) ova ) ( AGR 3SG ) ( POSS NONE ) ( CASE NOM ) ) ov ) ( SENSE POS ) ( MOOD OPT ) ( AGR 3SG ) ) ENTER WORD &gt; ak111mnnikiler proprietary compressed representation of the same transducer can process about 1,000 forms/sec on the same platform .</sentence>
				<definiendum id="0">CAT NOUN )</definiendum>
				<definiens id="0">ROOT enva = &gt; ( ( CAT NOUN ) ( ROOT reva = &gt; ( ( CAT NOUN ) ( ROOT evi = &gt; ( ( CAT NOUN ) ( ROOT eve = &gt; ( ( CAT NOUN ) ( ROOT ev = &gt; ( ( CAT NOUN ) ( ROOT evi = &gt; ( ( CAT NOUN ) ( ROOT eza = &gt; ( ( CAT NOUN ) ( ROOT leva = &gt; ( ( CAT NOUN ) ( ROOT neva = &gt; ( ( CAT NOUN ) ( ROOT ova = &gt; ( ( CAT NOUN )</definiens>
			</definition>
			<definition id="4">
				<sentence>C0NV denotes derivations to the category indicated by the second token with a suffix or derivation type denoted by the third token , if any .</sentence>
				<definiendum id="0">C0NV</definiendum>
				<definiens id="0">derivations to the category indicated by the second token with a suffix or derivation type denoted by the third token</definiens>
			</definition>
			<definition id="5">
				<sentence>81 Computational Linguistics Volume 22 , Number 1 Threshold 0 ... i ... 2 ... ak1111nlnkiler = &gt; ( ( CAT ak1111nlnkiler = &gt; ( ( CAT ak1111ndakiler = &gt; ( ( CAT NOUN ) ( ROOT ak11 ) ( CONV ADJ LI ) ( CONV NOUN ) ( AGR 3SG ) ( POSS NONE ) ( CASE GEN ) ( CONV PRONOUN REL ) ( AGR 3PL ) ( POSS NONE ) ( CASE NOM ) ) NOUN ) ( ROOT ak11 ) ( CONV AD3 LI ) ( CONV NOUN ) ( AGR 3SG ) ( POSS 2SG ) ( CASE GEN ) ( CONV PRONOUN REL ) ( AGR 3PL ) ( POSS NONE ) ( CASE NOM ) ) NOUN ) ( ROOT akxl ) ( CONV ADJ LI ) ( CONV NOUN ) ( AGR 3SG ) ( POSS 2SG ) ( CASE LOC ) ( CONV ADJ REL ) ( CONV NOUN ) ( AGR 3PL ) ( POSS NONE ) ( CASE NOM ) ) ENTER WORD &gt; eviminkinn Threshold 0 ... 1 ... eviminkini = &gt; ( ( CAT NOUN ) ( ROOT ev ) ( AGR 3SG ) ( POSS ISG ) ( CASE GEN ) ( CONV PRONOUN REL ) ( AGR 3SG ) ( POSS NONE ) ( CASE ACC ) ) eviminkine = &gt; ( ( CAT NOUN ) ( ROOT ev ) ( AGR 3SG ) ( POSS ISG ) ( CASE GEN ) ( CONV PRONOUN REL ) ( AGR 3SG ) ( POSS NONE ) ( CASE DAT ) ) eviminkinin = &gt; ( ( CAT NOUN ) ( ROOT ev ) ( AGR 3SG ) ( PGSS lSG ) ( CASE GEN ) ( CONV PRONOUN REL ) ( AGR 3SG ) ( POSS NONE ) ( CASE GEN ) ) ENTER WORD &gt; teeplerdeki Threshold 0 ... I ... tepelerdeki = &gt; ( ( CAT NOUN ) ( ROOT tepe ) ( AGR 3PL ) ( POSS NONE ) ( CASE LOC ) ( CONV ADJ REL ) ) teyplerdeki = &gt; ( ( CAT NOUN ) ( ROOT teyb ) ( AGR 3PL ) ( POSS NONE ) ( CASE LOC ) ( CONV ADJ REL ) ) ENTER WORD &gt; uygarla~tlramadlklarmllzdanml§slnlzcaslna Threshold 0 ... 1 ... uygarla§tmramadlklarlmlzdanm1~slnlzcaslna = &gt; ( ( CAT ADJ ) ( ROOT uygar ) ( CONV VERB LAS ) ( VOICE CAUS ) ( SENSE NEG ) ( CONV ADJ DIK ) ( AGR 3PL ) ( POSS IPL ) ( CASE ABL ) ( CONV VERB ) ( TENSE NARR-PAST ) ( AGR 2PL ) ( CONV ADVERB CASINA ) ( TYPE MANNER ) ) ENTER WORD &gt; okatulna Threshold 0 ... 1 ... 2 ... 82 Kemal Oflazer Error-tolerant Finite-state Recognition okutulma = &gt; ( ( CAT okutulma = &gt; ( ( CAT okutulan = &gt; ( ( CAT okutulana = &gt; ( ( CAT okutulsa = &gt; ( ( CAT okutula = &gt; VERB ) ( RODT oku ) ( VOICE CAUS ) ( VOICE PASS ) ( SENSE NEG ) ( MOOD IMP ) ( AGR=2SG ) ) VERB ) ( ROOT oku ) ( VOICE CAUS ) ( VOICE PASS ) ( SENSE POS ) ( CONV NOUN MA ) ( TYPE INFINITIVE ) ( AGE 3SG ) ( POSS NONE ) ( CASE NOM ) ) VEKB ) ( ROOT oku ) ( VOICE CAUS ) ( VOICE PASS ) ( SENSE POS ) ( CONV ADJ YAN ) ) VERB ) ( ROOT oku ) ( VOICE CAUS ) ( VOICE PASS ) ( SENSE POS ) ( CONV ADJ YAN ) ( CONV NOUN ) ( AGR 3SG ) ( POSS NONE ) ( CASE DAT ) ) VERB ) ( ROOT oku ) ( VOICE CAUS ) ( VOICE PASS ) ( SENSE POS ) ( MOOD COND ) ( AGE 3SG ) ) ( CAT VERB ) ( ROOT oku ) ( VOICE CAUS ) ( VOICE PASS ) ( SENSE POS ) ( MOOD OPT ) ( AGR 3SG ) ) In an application context , the candidates that are generated by such a morphological analyzer can be disambiguated or filtered to a certain extent by constraint-based tagging techniques ( see Oflazer and Kuru6z 1994 ; Voutilainen and Tapanainen 1993 ) that take into account syntactic context for morphological disambiguation .</sentence>
				<definiendum id="0">POSS NONE )</definiendum>
				<definiendum id="1">POSS NONE )</definiendum>
				<definiendum id="2">CASE LOC ) ( CONV ADJ REL ) ( CONV NOUN )</definiendum>
				<definiendum id="3">POSS NONE )</definiendum>
				<definiendum id="4">CAT NOUN ) ( ROOT ev )</definiendum>
				<definiendum id="5">POSS ISG ) ( CASE GEN ) ( CONV PRONOUN REL ) ( AGR 3SG )</definiendum>
				<definiendum id="6">CAT ADJ )</definiendum>
				<definiendum id="7">CONV VERB LAS ) ( VOICE CAUS ) ( SENSE NEG ) ( CONV ADJ DIK )</definiendum>
				<definiendum id="8">POSS IPL ) ( CASE ABL ) ( CONV VERB ) ( TENSE NARR-PAST )</definiendum>
				<definiendum id="9">CONV ADVERB CASINA )</definiendum>
				<definiendum id="10">Kemal Oflazer Error-tolerant Finite-state</definiendum>
				<definiens id="0">POSS NONE ) ( CASE ACC ) ) eviminkine = &gt; ( ( CAT NOUN ) ( ROOT ev ) ( AGR 3SG ) ( POSS ISG ) ( CASE GEN ) ( CONV PRONOUN REL ) ( AGR 3SG ) ( POSS NONE ) ( CASE DAT ) ) eviminkinin = &gt; ( ( CAT NOUN ) ( ROOT ev ) ( AGR 3SG ) ( PGSS lSG ) ( CASE GEN ) ( CONV PRONOUN REL ) ( AGR 3SG ) ( POSS NONE ) ( CASE GEN ) ) ENTER WORD &gt; teeplerdeki Threshold 0</definiens>
				<definiens id="1">CAT okutulan = &gt; ( ( CAT okutulana = &gt; ( ( CAT okutulsa = &gt; ( ( CAT okutula = &gt; VERB ) ( RODT oku ) ( VOICE CAUS ) ( VOICE PASS ) ( SENSE NEG ) ( MOOD IMP ) ( AGR=2SG ) ) VERB ) ( ROOT oku ) ( VOICE CAUS ) ( VOICE PASS ) ( SENSE POS )</definiens>
				<definiens id="2">SENSE POS ) ( CONV ADJ YAN ) ) VERB ) ( ROOT oku ) ( VOICE CAUS ) ( VOICE PASS ) ( SENSE POS )</definiens>
				<definiens id="3">SENSE POS ) ( MOOD COND ) ( AGE 3SG ) ) ( CAT VERB ) ( ROOT oku ) ( VOICE CAUS ) ( VOICE PASS ) ( SENSE POS ) ( MOOD OPT )</definiens>
			</definition>
			<definition id="6">
				<sentence>Veronis ( 1988 ) presents a method for handling quite complex combinations of typographical and phonographic errors ( phonographic errors are the kind usually made by language learners using computer-aided instruction ) .</sentence>
				<definiendum id="0">phonographic errors</definiendum>
				<definiens id="0">the kind usually made by language learners using computer-aided instruction )</definiens>
			</definition>
			<definition id="7">
				<sentence>The fifth column gives the average number of solutions generated from the given misspelled strings with the given edit distance .</sentence>
				<definiendum id="0">fifth column</definiendum>
				<definiens id="0">gives the average number of solutions generated from the given misspelled strings with the given edit distance</definiens>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>Perplexity is related to entropy , so our goal is to find models that estimate a low perplexity for some unseen representative sample of the language being modeled .</sentence>
				<definiendum id="0">Perplexity</definiendum>
				<definiens id="0">to find models that estimate a low perplexity for some unseen representative sample of the language being modeled</definiens>
			</definition>
			<definition id="1">
				<sentence>They note that pronouns have a disjoint classification , since the +nominative and -nominative pronouns -- for example , &lt; I ) , &lt; they ) and ( me ) , &lt; them ) respectively -- have dissimilar distributions. These effects are replicated in our experiment. They report other , more fine-grained features such as possessives , singular determiners , definite-determiners and Wh-adjuncts. Our algorithm also distinguishes these features. Brill et al. do not report any substantial adjective clustering , or noun clustering , or singular-plural differences , or co-ordinating and subordinating conjunction distinction , or verb tense differentiation. At lower levels , the only semantic clustering they report involves the group : &lt; man world time life work people years ) and the group : &lt; give make take find ) . The results described in Brown , Della Pietra , DeSouza , Lai , and Mercer ( 1992 ) are based on a training set two orders of magnitude greater than the one used in the above experiment. Even the vocabulary size is an order of magnitude bigger. As the vocabulary size is increased , the new vocabulary items tend , with a probability approaching unity , to be content words : after approximately one thousand words , few function words are left undiscovered. This increase in resources makes contexts more balanced and , simultaneously , more statistically significant. It also allows many more content words to be grouped together semantically. The authors give two tables of generated word classes , one being specially selected by them and the other containing randomly selected classes. They do not report on any overall taxonomic relations between these classes , so it is not possible to compare the broad detail of the two sets of data. The results of Finch and Chater ( 1992 , 1991 ) are also based on a substantially larger corpus. Finch and Chater also run a version of the Elman experiment ( see below ) . Their system fails to produce a complete noun-verb distinction at the highest level , though they offer an argument to suggest that the inadequacy lies in the nature of Elman 's pseudo-natural language corpus ; our system uses Elman 's corpus but succeeds in making the primary noun-verb distinction. Finch and Chater also cluster letters and phonemes -- their system succeeds in distinguishing between vowels and consonants in the letter experiment , and only the phoneme /u/ is incorrectly classified in the phoneme experiment. Conversely , our algorithm completely clusters phonemes into vowels and consonants , but performs less well with letters ( McMahon 1994 ) . Pereira and Tishby ( 1992 ) do not give details of syntactic similarity -- they concentrate on a small number of words and make fine-grained semantic differentiations between them. Their evaluation techniques include measuring how helpful their system is in making selectional restrictions and in disambiguating verb-noun pairs. Schiitze ( 1993 ) uses a standard sparse matrix algorithm with neural networks ; his system is the only one that attempts to side-step the problem of deciding what his clusters are clusters of , by producing a system that generates its own class labels. He does not report the overall structure of his one-level classification. His training set is one order of magnitude bigger than the largest one used in the present experiments. 229 Computational Linguistics Volume 22 , Number 2 between our algorithm and others , where some of the experimental parameters are controlled -- for example , corpus size. We considered it useful to compare the performance of our algorithm with others ' on precisely the same input data because we believe that factors like vocabulary , corpus size , and corpus complexity make evaluation difficult. A Recurrent Neural Network and a Regular Grammar. We redescribe the salient details of one of the experiments performed by Elman ( 1990 ) . The grammar that generates the language upon which this experiment is based is , according to the Chomsky classification , type 4 ( regular , or finite-state ) . Its production rules are shown in Figure 7. Some of the words belong to two or more word classes. The sentence frames encode a simple semantics -- noun types of certain classes engage in behavior unique to that class. Elman generates a 10,000-sentence corpus to be used as the training corpus. Each sentence frame is just as likely to be selected as any other ; similarly , each word member of a particular word group has an equiprobable chance of selection. No punctuation is included in the corpus , so sentence endings are only implicitly represented -- for example , the segment stream/cat smell cookie clog exist boy smash plate / contains a three-word sentence followed by a two-word sentence followed by another three-word sentence. After training , Elman 's net was tested on an unseen set , generated by the same underlying grammar. The network 's performance was poor -- only achieving a prediction error rate slightly above chance. Elman then presented the training data to the net a further four times , but the prediction was still poor. He claimed that , with even more training , the net could have improved its performance further. But this was not the main goal of the experiment ; instead , hierarchical cluster analysis was performed on the averaged hidden unit activations for each of the 29 words. Figure 8 reproduces the similarity tree that cluster analysis of the recurrent net produced. His analysis reveals that the network has learned most of the major syntactic differences and many of the semantic ones coded in the original language. For example , there is a clear distinction , between verbs and nouns ; within the main class of nouns , there is a clear animate-inanimate distinction ; within that , the classes of agent-patient , aggressor , and nonhuman animal have been induced. The analysis is not perfect : the most important distinction is considered to be between a handful of inanimate noun objects ( bread , cookie , sandwich , glass , and plate ) and the rest of the vocabulary. We now discuss the results obtained when our algorithm is applied to a similar test corpus. Elman 's grammar of Figure 7 was used to produce a corpus of 10,000 sentences with no sentence breaks. Unigram and bigram word-frequency statistics were generated. Our structural tag word-classification algorithm was applied to the initial mapping , which randomly assigned tag values to the 29 words. Figure 9 shows the important classification decisions made by this algorithm. Unlike the Elman classification ( see Figure 8 ) , informationally useful class structure exists from level 1 onwards. This algorithm also produces a classification some features of which are qualitatively better than Elman 's -- all nouns and all verbs are separated ; all animates and inanimates are separated. The multicontext noun/verb /break / is identified as different from other verbs ; intransitive verbs cluster together and the aggressive nouns are identified. This algorithm does not recapture the complete syntax and semantics of the language-human nouns and non-aggressive animate nouns remain mixed , and the food noun cluster failed to attract the word ( sandwich/. This experiment was repeated several times , each time resulting in a classification whose overall structure was similar but whose fine detail was slightly different. One run , for example , correctly differentiated 230 McMahon and Smith Improving Statistical Language Models S S S S S S S S S S S S S S S S • NOUN-HUMAN VERB-EAT NOUN-FOOD • NOUN-HUMAN VERB-PERCEPT NOUN-INAN • NOUN-HUMAN VERB-DESTROY NOUN-FRAGILE • NOUN-HUMAN VERB-INTRAN • NOUN-HUMAN VERB~RAN NOUN-HUMAN • NOUN-HUMAN VERB-AGPAT NOUN-INAN -- • NOUN-HUMAN VERB-AGPAT • NOUN-ANIM VERB-EAT NOUN-FOOD -- • NOUN-ANIM VERB-TRAN NOUN-ANIM • NOUN-ANIM VERB-AGPAT NOUN-INANIM • NOUN-ANIM VERB-AGPAT • NOUN-INAN VERB-AGPAT ~- } 1=. NOUN-AGRESS VERB-DESTROY NOUN-FRAGILE • NOUN-AGRESS VERB-EAT NOUN-HUMAN • NOUN-AGRESS VERB-EAT NOUN-ANIM -- -- -- • NOUN-AGRESS VERB-EAT NOUN-FOOD NOUN-HUMAN ) ~ NOUN-ANIM • NOUN-INAN • NOUN-AGRESS -- • NOUN-FRAGILE -- • NOUN-FOOD • VERB-INTRAN • VERB-TRAN • VERB-AGPAT • VERB-PERCEPT -- • VERB-DESTROY • VERB-EAT -- • Figure 7 man woman girl boy cat mouse dog man woman girl boy dragon monster lion book rock car cookie break bread sandwich glass plate dragon monster lion glass plate cookie break bread sandwich think sleep exist see chase like move break smell see break smash eat The Elman grammar. There are 16 nonterminal rules and 12 terminals. Notice also that terminals can belong to more than one word class -- for example , { break ) is an inanimate noun , a food noun , an agent-patient verb , and a destroy verb. 231 Computational Linguistics Volume 22 , Number 2 bread { -- CC cookie sandwich glass plate eat smash ~ chase like move __~break -smell s , e:0 \ [ _/ -- -think t~exist book Li -- -car L -- rock ._~.. ragon L monster __ girl an man Figure 8 Elman 's results. A Cluster analysis of the hidden units of a trained recurrent net , showing the major verb-noun distinction , as well as many other syntactic and semantic fine-grained distinctions. between small animals and humans , but failed to recognize food nouns as a complete group. Another run identified food nouns perfectly but failed to separate aggressors from other animates. Classi~cation Using a Merging Algorithm. The systems described in Brown , Della Pietra , DeSouza , Lai , and Mercer ( 1992 ) and Brill and Marcus ( 1992 ) both provide examples of bottom-up , merge-based classification systems ; a version of such a system was chosen to be implemented and tested against our algorithm , using the same input data. The Brown system uses a principle of class merging as its main clustering technique. The initial classification contains as many classes as there are words to classify , each word in its own class. Initially these classes are all mutually independent. Then two classes are chosen to merge ; the criterion of choice is based on a mutual information calculation ( see Equation 2 ) . The process is repeated until only one class remains. Next , the order of merging provides enough information for a hierarchical cluster to be constructed. A comparison experiment was designed using the 70,000-word VODIS corpus ( Cookson 1988 ) as a source of frequency information ; our system and the merging system were given a set of those words from the decapitalized and depunctuated corpus ( except for the apostrophe when it is a part of a word ) whose frequencies were greater than 30. This accounted for the 256 most frequent words. The final classifications , to a depth of five levels , are shown in Figure 10 and Figure 11 for the bottom-up and top-down systems , respectively. The difficulty of corn232 Computational Linguistics Volume 22 , Number 2 - &gt; ~-minutes o'clock I t__ adults hundred eighty sixty I I I -- P °unds ~-thirteen fifteen sixteen seventeen , -- -l -- fifty I I '' -fou , ty -- t Ltwenty ~~ ften eleven twelve ive ~-eight seven L_ two three four six nine , _~cambfidge peterborough london birmingham manchester norwich bury I I stowmarket here ipswich euston felixstowe woodbfidge \ ] ~ me have like need want know say see mean think wonder change leave |arrive leaves leaving come coming go pay travel them travelling be get -| catch make take find work ~ -to from into via I -- do n't ca n't have n't help give tell can could thank might should will would you i they we that 'll i 'll you 'd you 'll ~ are do if no did pardon actually really all as when where but so well afraid again because obviously they 're sorry now which alright fine lovely okay fight thanks then oh past and or please yes at about before by after between than is does is n't gets was of on for what 's / with .</sentence>
				<definiendum id="0">cookie</definiendum>
				<definiens id="0">the +nominative and -nominative pronouns -- for example , &lt; I ) , &lt; they ) and ( me ) , &lt; them ) respectively -- have dissimilar distributions. These effects are replicated in our experiment. They report other , more fine-grained features such as possessives , singular determiners , definite-determiners and Wh-adjuncts. Our algorithm also distinguishes these features. Brill et al. do not report any substantial adjective clustering , or noun clustering , or singular-plural differences , or co-ordinating and subordinating conjunction distinction , or verb tense differentiation. At lower levels , the only semantic clustering they report involves the group : &lt; man world time life work people years ) and the group : &lt; give make take find )</definiens>
				<definiens id="1">an order of magnitude bigger. As the vocabulary size is increased , the new vocabulary items tend , with a probability approaching unity , to be content words : after approximately one thousand words , few function words are left undiscovered. This increase in resources makes contexts more balanced and , simultaneously , more statistically significant. It also allows many more content words to be grouped together semantically. The authors give two tables of generated word classes , one being specially selected by them and the other containing randomly selected classes. They do not report on any overall taxonomic relations between these classes</definiens>
				<definiens id="2">an argument to suggest that the inadequacy lies in the nature of Elman 's pseudo-natural language corpus ; our system uses Elman 's corpus but succeeds in making the primary noun-verb distinction. Finch and Chater also cluster letters and phonemes -- their system succeeds in distinguishing between vowels and consonants in the letter experiment</definiens>
				<definiens id="3">clusters phonemes into vowels and consonants , but performs less well with letters ( McMahon 1994 ) . Pereira and Tishby ( 1992 ) do not give details of syntactic similarity -- they concentrate on a small number of words and make fine-grained semantic differentiations between them. Their evaluation techniques include measuring how helpful their system is in making selectional restrictions and in disambiguating verb-noun pairs. Schiitze ( 1993 ) uses a standard sparse matrix algorithm with neural networks</definiens>
				<definiens id="4">clusters of , by producing a system that generates its own class labels. He does not report the overall structure of his one-level classification. His training set</definiens>
				<definiens id="5">regular , or finite-state ) . Its production rules are shown in Figure 7. Some of the words belong to two or more word classes. The sentence frames encode a simple semantics -- noun types of certain classes engage in behavior unique to that class. Elman generates a 10,000-sentence corpus to be used as the training corpus. Each sentence frame is just as likely to be selected as any other ; similarly , each word member of a particular word group has an equiprobable chance of selection. No punctuation is included in the corpus , so sentence endings are only implicitly represented -- for example , the segment stream/cat smell cookie clog exist boy smash plate / contains a three-word sentence followed by a two-word sentence followed by another three-word sentence. After training , Elman 's net was tested on an unseen set</definiens>
				<definiens id="6">used to produce a corpus of 10,000 sentences with no sentence breaks. Unigram and bigram word-frequency statistics were generated. Our structural tag word-classification algorithm was applied to the initial mapping</definiens>
				<definiens id="7">intransitive verbs cluster together and the aggressive nouns are identified. This algorithm does not recapture the complete syntax and semantics of the language-human nouns and non-aggressive animate nouns remain mixed</definiens>
				<definiens id="8">Smith Improving Statistical Language Models S S S S S S S S S S S S S S S S • NOUN-HUMAN VERB-EAT NOUN-FOOD • NOUN-HUMAN VERB-PERCEPT NOUN-INAN • NOUN-HUMAN VERB-DESTROY NOUN-FRAGILE • NOUN-HUMAN VERB-INTRAN • NOUN-HUMAN VERB~RAN NOUN-HUMAN • NOUN-HUMAN VERB-AGPAT NOUN-INAN -- • NOUN-HUMAN VERB-AGPAT • NOUN-ANIM VERB-EAT NOUN-FOOD -- • NOUN-ANIM VERB-TRAN NOUN-ANIM • NOUN-ANIM VERB-AGPAT NOUN-INANIM • NOUN-ANIM VERB-AGPAT • NOUN-INAN VERB-AGPAT ~- } 1=. NOUN-AGRESS VERB-DESTROY NOUN-FRAGILE • NOUN-AGRESS VERB-EAT NOUN-HUMAN • NOUN-AGRESS VERB-EAT NOUN-ANIM -- -- -- • NOUN-AGRESS VERB-EAT NOUN-FOOD NOUN-HUMAN</definiens>
				<definiens id="9">man woman girl boy cat mouse dog man woman girl boy dragon monster lion book rock car cookie break bread sandwich glass plate dragon monster lion glass plate cookie break bread sandwich think sleep exist see chase like move break smell see break smash eat The Elman grammar. There are 16 nonterminal rules and 12 terminals. Notice also that terminals can belong to more than one word class -- for example</definiens>
				<definiens id="10">many other syntactic and semantic fine-grained distinctions. between small animals and humans , but failed to recognize food nouns as a complete group. Another run identified food nouns perfectly but failed to separate aggressors from other animates. Classi~cation Using a Merging Algorithm. The systems described in Brown</definiens>
				<definiens id="11">a source of frequency information ; our system and the merging system were given a set of those words from the decapitalized and depunctuated corpus</definiens>
				<definiens id="12">a part of a word ) whose frequencies were greater</definiens>
				<definiens id="13">do if no did pardon actually really all as when where but so well afraid again because obviously they 're sorry now which alright fine lovely okay fight thanks then oh past</definiens>
			</definition>
			<definition id="2">
				<sentence>A high-scoring cluster is one whose members are classified similarly in the tagged LOB corpus .</sentence>
				<definiendum id="0">high-scoring cluster</definiendum>
				<definiens id="0">one whose members are classified similarly in the tagged LOB corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>Both of the compared classification systems use familiar statistical measures of correlation ( Spearman 's rank correlation coefficient and Manhattan metric ) and grouping 236 McMahon and Smith Improving Statistical Language Models J s~ome the `` \ all / lu : ta ; ; ~ebde ( 1 ) Estimate Preferred Cluster Label article adverb noun adjective determiner etc. i i o 2 C 314 0 3 C 0 0 C g 0 0 0 0 1 1 LOB-class information the : &lt; article,38458 &gt; &lt; adverb,19 &gt; &lt; noun , l &gt; all : &lt; article , 1559 &gt; &lt; adverb,160 &gt; &lt; adjective , l &gt; some : &lt; det,1111 &gt; the all some ick Highest Scorer Estimated Cluster Label = article ( 2 ) Update Evaluation Score ( i ) `` the ' : matches Estimated Cluster Label with most frequent LOB-class ADD 1 ( ii ) `` all '' : also matches Estimated Cluster Label with top LOB-class ADD 1 ( iii ) `` some '' : does n't match Estimated Cluster Label with any of the top four of its LOB-classes ADD 0 Generally , for each word , if the Estimated Cluster Label matches the kth top LOB-class , ADD ( 5-k ) /4 to the score ; k &lt; 5 .</sentence>
				<definiendum id="0">Manhattan metric</definiendum>
				<definiens id="0">all '' : also matches Estimated Cluster Label with top LOB-class ADD 1 ( iii ) `` some '' : does n't match Estimated Cluster Label with any of the top four of its LOB-classes ADD 0 Generally , for each word , if the Estimated Cluster Label matches the kth top LOB-class</definiens>
			</definition>
			<definition id="4">
				<sentence>words ( &lt; ten ) , ( three/ , etc. ) then that branch gets a certain score , regardless of how spread-out the words are within that branch. On the other hand , there may well be good engineering reasons to treat linguistically homogeneous words as belonging to different classes. For example , in a corpus of conversations about train timetables , where numbers occur in two main situations -- as ticket prices and as times -- we might expect to observe a difference between , say , the numbers from 1 to 12 , and numbers up to 59 ( hour numbers and minute numbers respectively ) ; Figure 11 lends some support to this speculation. Similarly , phrases like ( five pounds ninety nine pence / could lead to different patterns of collocation for number words. This sort of effect is indeed observed ( McMahon 1994 ) . It is less clear whether our main clustering result separates number words into different classes for the same kind of reason ( in Figure 2 , class 00000 contains 4 number words and class 00101 contains 11 ) . A second limitation lies in the evaluation scheme estimating the canonical part of speech based on the rank of the parts of speech of each word in it -- a better system would make the weight be some function of the probability of the parts of speech. A third criticism of the scheme is its arbitrariness in weighting and selecting canonical classes ; the criticism is only slight , however , because the main advantage of any benchmark is that it provides a standard , regardless of the pragmatically influenced details of its construction. Automatic word-classification systems are intrinsically interesting ; an analysis of their structure and quality is itself an ongoing research topic. However , these systems can also have more immediate uses. The two types of use are related to the two types of approach to the subject -- linguistic and engineering. Consequently , indirect evaluation can be linguistic or engineering-based. Indirect linguistic evaluation examines the utility of the derived classes in solving various linguistic problems : pronoun reference ( Elman 1990 ; Fisher and Riloff 238 McMahon and Smith Improving Statistical Language Models 1992 ) , agreement , word-sense disambiguation ( Liddy and Paik 1992 ; Gale , Church , and Yarowsky 1992 ; Yarowsky 1992 ; Pereira , Tishby , and Lee 1993 ) and resolution of anaphoric reference ( Burger and Connolly 1992 ) . A classification is said to be useful if it can contribute to a more accurate linguistic parse of given sentences. If our main interest were linguistic or cognitive scientific , we would be even more concerned about the way our system can not handle multimodal word behavior and about the resulting misclassifications and fracturing of the classes. One main engineering application that can use word classes is the statistical language model. Classifications which , when incorporated into the models , lower the test set perplexity are judged to be useful. There are several ways of incorporating word-classification information into statistical language models using the structural tag representation ( McMahon 1994 ) . Here , we shall describe a method , derived from Markov model theory ( Jelinek and Mercer 1980 ) , which is based on interpolating several language components. The interpolation parameters are estimated by using a held-out corpus. We decided to build an interpolated language model partly because it has been well studied and is familiar to the research community and partly because we can examine the lambda parameters directly to see if weight is indeed distributed across multiple class levels. A poor language model component will receive virtually no weight in an interpolated system -- if we find that weight is distributed mostly with one or two components , we can conclude that interpolated language models do not find much use for multiple class information. For the following experiments , a formatted version ( punctuation removed , all words decapitalized , control characters removed ) of the one-million-word Brown corpus was used as a source of language data ; 60 % of the corpus was used to generate maximum likelihood probability estimates , 30 % to estimate frequency-dependent interpolation parameters , and the remaining 10 % as a test set. The vocabulary items extracted from the training set were clustered according to the method described earlier. For comparison , we calculated some test set perplexities of other language models. Improved performance can be obtained by making interpolation parameters depend upon some distinguishing feature of the prediction context. One easily calculated feature is the frequency of the previously processed word. In our main experiment , this resulted in 428 sets of &amp; values , corresponding to 428 different previous-word frequencies. The parameters are fitted into an interpolated language model the core of which is described by the equation : P ( Wk ) `` ~ '' ~u ( f ) X P ( Wk ) - { ) ~b ( f ) X P ( w k I Wk-1 ) q ) ~t ( f ) X P ( w k I Wk-2 , Wk-1 ) where f = f ( wj ) , the frequency of word ( wj ) if a valid wj exists and 0 otherwise-namely at the beginning of the test set , and when the previous word is not in the training vocabulary. The &amp; values are selected using a standard re-estimation algorithm ( Baum et al. 1970 ) . The resulting perplexity value for this system is 621.6. This represents a pragmatically sensible baseline value against which any variant language model should be compared. A similar word-based language model , the weighted average language model , has been developed by O'Boyle , Owens , and Smith ( 1994 ) . This 239 Computational Linguistics Volume 22 , Number 2 model is described as follows : m Wk_ i ) q~0 X PML ( Wk ) P ( Wk I W~ -1 ) = ~i=1/~i X PML ( Wk I k-1 m A ~i=0 i where there are statistically significant segments up to m + 1 words long and PML ( Wk ) is the maximum likelihood probability estimate of a word. The numerator acts as a normalizer. It has been found that : kml Ai = 2 ( Iwk-~ I ) x logf ( w~-~ ) where k-1 IWk_il is the size of the segment , results in a useful language model of this form. When applied to the Brown corpus , excluding the 30 % allocated for interpolation and only using n-grams up to 3 , the model still performs well , achieving a perplexity score of 644.6 ; adding the extra training text should remove the disadvantage suffered by the weighted average model but at the probable cost of introducing new vocabulary items , making the test set perplexity comparisons even more difficult to interpret. An important component of many statistical language-modeling systems is the bigram conditional probability estimator P ( wi \ ] wi-1 ) ( Church and Gale 1991 ) ; we shall restrict our attention to the case where both words have been seen before , though the bigram ( Wi_l , Wi ) itself may be unseen. We shall suggest an alternative to the familiar maximum likelihood bigram estimate , which estimates the probability as P ( wi I wi-1 ) f ( wi_~ , wi ) wheref ( w ) is just the frequency of occurrence of w in some f ( wi_~ ) , training corpus. The general form of the multilevel smoothed bigram model is : S P ( wi I wi-1 ) = ~/~ ) ( wi-l ) , sP ( CS ( wi ) I CS ( wi-1 ) ) P ( wi I CS ( wi ) ) ( 3 ) where there are S levels of class granularity and CS ( wi ) is the class at level s of the word wi ; ) ,4 ( w~_l ) , s is an interpolation weight for language model component s and depends upon some function ~ ( Wi_I ) of the conditioning word wi-1 ; common functions include a frequency-based interpolation ~ ( wi-1 ) = f ( wi-1 ) and a depth-s class-based interpolation , qS ( wi_l ) = CS ( wi_l ) , though ~ can partition the conditioning context in any way and this context does not necessarily have to be a recent word. The , ~ values are estimated as before , using the frequency of the previous word to partition the conditioning context. Parameter setting for the smoothed bigram takes less than a day on a Sparc-IPC. Our 16-bit structural tag representation allows us to build an interpolated bigram model containing 16 levels of bigram-class information. As suggested earlier , we can look at the spread of ) ~ values used by the smoothed bigram component as a function of the class granularity and frequency of the conditioning word. Figure 14 shows clearly that the smoothed bigram component does indeed find each class level useful , at different frequencies of the conditioning word. Next , we need to find out how much of an improvement we can achieve using this new bigram model. We can replace the maximum likelihood bigram estimator in our interpolated trigram model with the smoothed bigram estimator. When we do , we get a perplexity of 577.4 , a 7.1 % improvement on standard interpolation , which scores 621.6. Other experiments with ) ~ depending on the class ( at a certain depth ) of the previous word lead to smaller improvements and are not reported here. 240 McMahon and Smith Improving Statistical Language Models lambda 0 ' '' 1 15 1 ularity Figure 14 Bigram lambda weights. Surface showing how lambda varies with frequency ( log scale ) of previous word and bigram-class granularity. The projected contour map highlights the main feature of this relationship -- at various frequencies , each of the 16 class bigram models is used. Figure 15 summarizes the test set perplexity results. We note that our 7.1 % improvement is larger than that obtained by Brown , Della Pietra , DeSouza , Lai , and Mercer ( 1992 ) , who report a 3.3 % improvement. The smaller absolute perplexity scores they quote are a consequence of the much larger training data they use. One reason for this apparent improvement may be that their baseline model , constructed as it is out of much more training data , is already better than our equivalent baseline , so that they find improvements harder to achieve. Another reason may be due to the different vocabulary sizes used ( Ueberla 1994 ) . A third reason , and one which we consider to be important , is that multilevel class-based language models may perform significantly better than two-level ones. We carried out another experiment to support this claim. We constructed a frequency-dependent interpolated unigram and bigram model as a baseline. Its test set perplexity was 635. We then replaced the maximum likelihood bigram component with the smoothed bigram estimate. The perplexity for this system was 580 , a 9 % improvement. We also replaced the maximum likelihood bigram component with a series of 15 two-level smoothed bigram models -- from a 16-plus15 smoothed bigram to a 16-plus-1 smoothed bigram. Figure 16 details these results. The best of these two-level systems is the 16-plus-8 model , which scores 606. So , on a bigram model , the multilevel system is 4.3 % better than the best two-level system , which supports our claim. We chose bigram models in this experiment so that we could make some comparisons with similarity-based bigram models. Dagan , Markus , and Markovitch ( 1993 ) claim that word-classification systems of this type may lead to substantial information loss when compared to similarity methods ( Dagan , Pereira , and Lee 1994 ; Essen and Steinbiss 1992 ) . The similarity-based system of Dagan , Pereira , and Lee ( 1994 ) improves a baseline Turing-Good bigram model by 2.4 % and the co-occurrence system of Essen and Steinbiss ( 1992 ) leads to a 10 % improvement over an interpolated baseline bigram model. This latter result is based on a similarly sized training set and so our 9 % improvement compared to their 241 Computational Linguistics Volume 22 , Number 2 Language Model Test Set Perplexity Weighted Average 644.626 Interpolated Trigram 621.632 Interpolated Trigram ( smoothed bigram component ) 577.421 Figure 15 Test set perplexity improvements. When an interpolated trigram language model uses smoothed bigram estimates , test set perplexity reduced by approximately 7.1 % compared to a similar system with maximum likelihood bigram estimates , and 10 % compared to the weighted average language model. Language Model Test Set Perplexity Baseline bigram word plus class 1 word plus class 2 word plus class 3 word plus class 4 word plus class 5 word plus class 6 word plus class 7 word plus class 8 word plus class 9 word plus class 10 word plus class 11 word plus class 12 word plus class 13 word plus class 14 word plus class 15 Multilevel 635 634 633 626 621 616 614 609 606 609 614 618 622 627 631 633 580 Figure 16 Multilevel versus two-level bigram performances. A multilevel smoothed bigram model is 9 % better than a baseline maximum likelihood model and 4.3 % better than the best two-level class-based bigram model. 10 % suggests that language models based upon fixed-place classes can be only slightly worse than some similarity models , given approximately equal training texts. As an illustration of the kind of advantage structural tag language models can offer , we introduce nine oronyms ( word strings which , when uttered , can produce the same sound ) based upon the uttered sentence : The boys eat the sandwiches. If we assume that we already possess a perfect speech recognition acoustic model ( Jelinek , Mercer , and Roukos 1992 ) , it may be able to recover the phoneme string : /DH a b 01 z EE t DH A s AA n d w i j i z/ 242 McMahon and Smith Improving Statistical Language Models Sentence W.A. Smoothed Grammatical the boy seat the sandwiches the boys eat the sandwiches the boy seat this and which is the boys eat this and which is the buoys eat the sandwiches the buoys eat this and which is the boys eat the sand which is the buoys eat the sand which is the buoy seat this and which is 3,419 1,787 435 232 195 25 14 0 7,848 8,821 137 149 469 8 21 0 no yes no no yes no yes yes no Figure 17 Improvements in a simulated speech-recognition example. Nine versions of a phonemically identical oronym , ordered by weighted average ( W.A. ) probability ( x 10-20 ) . The W.A. language model ranks the preferred sentence second. The smoothed structural tag model successfully predicts the original utterance as the most likely. ( buoy ) is an unseen vocabulary item in this test. Also , in all but two nonzero cases , the smoothed model makes grammatically correct sentences more likely and vice versa. The original sentence is not the only speech utterance that could give rise to the observed phoneme string ; for example , the meaningless and ungrammatical sentence : *The buoy seat this and which is. can also give rise to the observed phonemic stream. Humans usually reconstruct the most likely sentence successfully , but artificial speech recognizers with no language model component can not . Nonprobabilistic models , while theoretically well-grounded , so far tend to have poor coverage. Another limitation can be seen if we consider a third hypothesized sentence : The buoys eat the sand which is. This simultaneously surreal and metaphysical sentence may be accepted by grammar systems that detect well-formedness , but it is subsequently considered just as plausible as the original sentence. A probabilistic language model should assign a relatively low probability to the third sentence. We constructed nine hypothesized sentences , each of which could have produced the phoneme string ; we presented these sentences as input to a high-quality word-based language model ( the weighted average language model ) and to another smoothed structural tag language model. Neither the Hughes system nor the Finch system are ever applied to language models ; also , the details of the Brown language model are insufficient for us to rebuild it and run our sentences through it. Figure 17 shows the normalized probability results of these experiments. The new language model successfully identifies the most likely utterance. In all but two nonzero cases , grammatically well-formed sentences are assigned a higher raw probability by the new model , and vice-versa for ungrammatical sentences. Using the top two sentences ( the boy seat the sandwiches ) and ( the boys eat the sandwiches ) , we can examine the practical benefits of class information for statistical language modeling. An important difference between the two is in the bigrams ( boy seat ) and ( boys eat ) , neither of which occurred in the training corpus. The model that uses word frequencies exclusively differentiates between the two hypothesized sentences by examining the unigrams ( boy ) , ( seat ) , ( boys ) , and ( eat ) . In our 243 Computational Linguistics Volume 22 , Number 2 training corpus , &lt; boy &gt; and &lt; seat &gt; are individually more likely than &lt; boys &gt; and &lt; eat &gt; .</sentence>
				<definiendum id="0">PML ( Wk )</definiendum>
				<definiendum id="1">IWk_il</definiendum>
				<definiendum id="2">W.A. language model</definiendum>
				<definiens id="0">treat linguistically homogeneous words as belonging to different classes. For example , in a corpus of conversations about train timetables , where numbers occur in two main situations -- as ticket prices and as times</definiens>
				<definiens id="1">the statistical language model. Classifications which , when incorporated into the models , lower the test set perplexity are judged to be useful. There are several ways of incorporating word-classification information into statistical language models using the structural tag representation</definiens>
				<definiens id="2">The interpolation parameters are estimated by using a held-out corpus. We decided to build an interpolated language model partly because it has been well studied and is familiar to the research community and partly because we can examine the lambda parameters directly to see if weight is indeed distributed across multiple class levels. A poor language model component will receive virtually no weight in an interpolated system -- if we find that weight is distributed mostly with one or two components</definiens>
				<definiens id="3">a source of language data ; 60 % of the corpus was used to generate maximum likelihood probability estimates , 30 % to estimate frequency-dependent interpolation parameters</definiens>
				<definiens id="4">parameters are fitted into an interpolated language model the core of which is described by the equation : P ( Wk ) `` ~ '' ~u</definiens>
				<definiens id="5">A similar word-based language model , the weighted average language model</definiens>
				<definiens id="6">the maximum likelihood probability estimate of a word. The numerator acts as a normalizer. It has been found that</definiens>
				<definiens id="7">the size of the segment , results in a useful language model of this form. When applied to the Brown corpus , excluding the 30 % allocated for interpolation and only using n-grams up to 3 , the model still performs well , achieving a perplexity score of 644.6 ; adding the extra training text should remove the disadvantage suffered by the weighted average model but at the probable cost of introducing new vocabulary items , making the test set perplexity comparisons even more difficult to interpret. An important component of many statistical language-modeling systems</definiens>
				<definiens id="8">estimates the probability as P ( wi I wi-1</definiens>
				<definiens id="9">the frequency of occurrence of w in some f ( wi_~ ) , training corpus. The general form of the multilevel smoothed bigram model is : S P ( wi I wi-1 ) = ~/~ ) ( wi-l ) , sP ( CS ( wi ) I CS ( wi-1 ) ) P ( wi I CS ( wi ) ) ( 3 ) where there are S levels of class granularity and CS ( wi ) is the class at level s of the word wi</definiens>
				<definiens id="10">an interpolation weight for language model component s and depends upon some function ~</definiens>
				<definiens id="11">using the frequency of the previous word to partition the conditioning context. Parameter setting for the smoothed bigram takes less than a day on a Sparc-IPC. Our 16-bit structural tag representation allows us to build an interpolated bigram model containing 16 levels of bigram-class information. As suggested earlier</definiens>
				<definiens id="12">Language Model Test Set Perplexity Weighted Average 644.626 Interpolated Trigram 621.632 Interpolated Trigram ( smoothed bigram component ) 577.421 Figure 15 Test set perplexity improvements. When an interpolated trigram language model uses smoothed bigram estimates , test set perplexity reduced by approximately 7.1 % compared to a similar system with maximum likelihood bigram estimates , and 10 % compared to the weighted average language model. Language Model Test Set Perplexity Baseline bigram word plus class 1 word plus class 2 word plus class 3 word plus class 4 word plus class 5 word plus class 6 word plus class 7 word plus class 8 word plus class 9 word plus class 10 word plus class 11 word plus class 12 word plus class 13 word plus class 14 word plus class</definiens>
				<definiens id="13">ranks the preferred sentence second. The smoothed structural tag model</definiens>
				<definiens id="14">an unseen vocabulary item in this test. Also , in all but two nonzero cases , the smoothed model makes grammatically correct sentences more likely and vice versa. The original sentence is not the only speech utterance that could give rise to the observed phoneme string ; for example , the meaningless and ungrammatical sentence : *The buoy seat this and which is. can also give rise to the observed phonemic stream. Humans usually reconstruct the most likely sentence successfully , but artificial speech recognizers with no language model component can not</definiens>
				<definiens id="15">the weighted average language model ) and to another smoothed structural tag language model. Neither the Hughes system nor the Finch system are ever applied to language models</definiens>
				<definiens id="16">the boy seat the sandwiches</definiens>
				<definiens id="17">class information for statistical language modeling. An important difference between the two is in the bigrams ( boy seat ) and ( boys eat ) , neither of which occurred in the training corpus. The model that uses word frequencies exclusively differentiates between the two hypothesized sentences by examining the unigrams ( boy )</definiens>
			</definition>
			<definition id="5">
				<sentence>Learning syntactic categories : A statistical approach .</sentence>
				<definiendum id="0">Learning syntactic categories</definiendum>
				<definiens id="0">A statistical approach</definiens>
			</definition>
			<definition id="6">
				<sentence>Selection and Information : A Class-Based Approach to Lexical Relationships .</sentence>
				<definiendum id="0">Selection</definiendum>
				<definiendum id="1">Information</definiendum>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>Our model p of the expert 's decisions assigns to each French word or phrase f an estimate , p ( f ) , of the probability that the expert would choose f as a translation of in .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">each French word or phrase f an estimate</definiens>
			</definition>
			<definition id="1">
				<sentence>A feature is a binary-valued function of ( x , y ) ; a constraint is an equation between the expected value of the feature function in the model and its expected value in the training data .</sentence>
				<definiendum id="0">feature</definiendum>
				<definiendum id="1">constraint</definiendum>
				<definiens id="0">an equation between the expected value of the feature function in the model and its expected value in the training data</definiens>
			</definition>
			<definition id="2">
				<sentence>That is , we would like p to lie in the subset C of 7 ~ defined by C , =_ { pEP\ [ p ( fi ) =P ( fi ) fori E { 1,2 ... .. n } } ( 4 ) Figure 1 provides a geometric interpretation of this setup .</sentence>
				<definiendum id="0">=P</definiendum>
				<definiens id="0">( fi ) fori E { 1,2 ... .. n } } ( 4 ) Figure 1 provides a geometric interpretation of this setup</definiens>
			</definition>
			<definition id="3">
				<sentence>In ( b ) , the constraint C1 narrows the set of allowable models to those that lie on the line defined by the linear constraint .</sentence>
				<definiendum id="0">b )</definiendum>
				<definiens id="0">the constraint C1 narrows the set of allowable models to those that lie on the line defined by the linear constraint</definiens>
			</definition>
			<definition id="4">
				<sentence>The log-likelihood L~ ( p ) of the empirical distribution/5 as predicted by a model p is defined by 3 L~ ( p ) = log H P ( Ylx ) P ( X ' y ) = ~ ~ ( x , y ) log p ( ylx ) x , y x , y ( 13 ) It is easy to check that the dual function ~ ( A ) of the previous section is , in fact , just the log-likelihood for the exponential model p~ ; that is • ( A ) = L~ ( p ; , ) ( 14 ) With this interpretation , the result of the previous section can be rephrased as : The model p , E C with maximum entropy is the model in the parametric family p : ~ ( ylx ) that maximizes the likelihood of the training sample ~ .</sentence>
				<definiendum id="0">log-likelihood L~</definiendum>
				<definiens id="0">L~ ( p ) = log H P ( Ylx ) P ( X ' y ) = ~ ~ ( x , y ) log p ( ylx ) x , y x , y ( 13 ) It is easy to check that the dual function ~ ( A ) of the previous section is , in fact , just the log-likelihood for the exponential model p~ ; that is • ( A ) = L~ ( p ; , ) ( 14 ) With this interpretation , the result of the previous section can be rephrased as : The model p , E C with maximum entropy is the model in the parametric family p : ~ ( ylx ) that maximizes the likelihood of the training sample ~</definiens>
			</definition>
			<definition id="5">
				<sentence>An optimization method specifically tailored to the maximum entropy problem is the iterative scaling algorithm of Darroch and Ratcliff ( 1972 ) .</sentence>
				<definiendum id="0">maximum entropy problem</definiendum>
			</definition>
			<definition id="6">
				<sentence>The algorithm generalizes the Darroch-Ratcliff procedure , which requires , in addition to the nonnegativity , that the feature functions satisfy ~ifi ( x , Y ) = 1 for all x , y. Algorithm 1 : Improved Iterative Scaling Input : Feature functions fl , f2 ... . fn ; empirical distribution ~ ( x , y ) Output : Optimal parameter values ) ~*i ; optimal model p~ .</sentence>
				<definiendum id="0">Darroch-Ratcliff procedure</definiendum>
			</definition>
			<definition id="7">
				<sentence>In Section 5.4 we describe a maximum entropy model that predicts how to divide a French sentence into short segments that can be translated sequentially .</sentence>
				<definiendum id="0">maximum entropy model</definiendum>
				<definiens id="0">predicts how to divide a French sentence into short segments that can be translated sequentially</definiens>
			</definition>
			<definition id="8">
				<sentence>The probability p ( FIE ) that F is the translation of E is expressed as the sum over all possible alignments A between E and F of the probability of F and A given E : p ( FIE ) = ~_ , p ( F , AIE ) ( 31 ) A The sum in equation ( 31 ) is computationally unwieldy ; it involves a sum over all IE\ ] IFI possible alignments between the words in the two sentences .</sentence>
				<definiendum id="0">probability p</definiendum>
				<definiendum id="1">FIE</definiendum>
				<definiens id="0">the translation of E is expressed as the sum over all possible alignments A between E and F of the probability of F and A given E : p ( FIE ) = ~_ , p ( F , AIE ) ( 31 ) A The sum in equation ( 31 ) is computationally unwieldy ; it involves a sum over all IE\ ] IFI possible alignments between the words in the two sentences</definiens>
			</definition>
			<definition id="9">
				<sentence>II P ( YJ \ [ ea , ) `` d ( ArE , F ) i=1 j=l ( 33 ) where n ( ei ) denotes the number of French words aligned with ei .</sentence>
				<definiendum id="0">II P ( YJ</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the number of French words aligned with ei</definiens>
			</definition>
			<definition id="10">
				<sentence>In this expression • p ( nre ) is the probability that the English word e generates n French words , • p ( yle ) is the probability that the English word e generates the French word y ; and • d ( AIE , F ) is the probability of the particular order of French words .</sentence>
				<definiendum id="0">word e</definiendum>
				<definiens id="0">the probability that the English word e generates n French words</definiens>
				<definiens id="1">generates the French word y</definiens>
				<definiens id="2">the probability of the particular order of French words</definiens>
			</definition>
			<definition id="11">
				<sentence>( OTHER ) represents a catch-all classifier for any French phrase not listed , none of which had a probability exceeding 0.0043 .</sentence>
				<definiendum id="0">OTHER )</definiendum>
				<definiens id="0">a catch-all classifier for any French phrase not listed</definiens>
			</definition>
			<definition id="12">
				<sentence>Number of Template Actual Features f ( x , y ) -- 1 if and only if ... I llv t , -o and I I 'll I I A maximum entropy model that uses only template 1 features predicts each French translation y with the probability ~ ( y ) determined by the empirical data .</sentence>
				<definiendum id="0">maximum entropy model</definiendum>
				<definiens id="0">translation y with the probability ~ ( y ) determined by the empirical data</definiens>
			</definition>
			<definition id="13">
				<sentence>\ [ verb markerJ denotes a morphological marker inserted to indicate the presence of a verb as the next word .</sentence>
				<definiendum id="0">markerJ</definiendum>
				<definiens id="0">a morphological marker inserted to indicate the presence of a verb as the next word</definiens>
			</definition>
			<definition id="14">
				<sentence>~ : For each f E ~r , set ( 42 ) c'ff ) p ( f ) G '' ( f ) 0 ( b ) For each x , do the following : For each f E $ - ( x ) , update G ' ( f ) and G '' ( f ) by G ' ( f ) ~-G ' ( f ) ~ ; ( x ) p~ , f ( flx ) ( 43 ) G '' ( f ) ~G '' ( f ) -~ ( x ) p~d ( ( fp~ , f ( flx ) ) 21 x ) ( 44 ) where p~ , / ( f Ix ) = Gy PTs , / ( Ylx ) f ( x , y ) ( c ) For each f c ~- , update c~ ( f ) by r-~ ( 1 G ' ( f ) ) ( 45 ) c~ ( f ) , -c~ ( f ) + log 1 r ( f ) G '' ( f ) Convergence for this algorithm is guaranteed just as it was for algorithm 3 -- after each iteration of step 5 , the value of c~ ( f ) for each candidate feature f is closer to its optimal value c~* ( f ) and , more importantly , the gain Gs , f is closer to the maximal gain , - , ,AL ( , S , f ) .</sentence>
				<definiendum id="0">f</definiendum>
				<definiendum id="1">Gy PTs , / ( Ylx ) f</definiendum>
				<definiendum id="2">update c~</definiendum>
				<definiens id="0">G ' ( f ) ) ( 45 ) c~ ( f ) , -c~ ( f ) + log 1 r ( f ) G ''</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>The Rosetta stone is a tablet of black basalt containing parallel inscriptions in three different scripts : Greek and two forms of ancient Egyptian writings ( demotic and hieroglyphics ) .</sentence>
				<definiendum id="0">Rosetta stone</definiendum>
				<definiens id="0">a tablet of black basalt containing parallel inscriptions in three different scripts : Greek and two forms of ancient Egyptian writings ( demotic and hieroglyphics )</definiens>
			</definition>
			<definition id="1">
				<sentence>Sentence alignment programs take a paired bilingual corpus as input and determine which sentences in the target language translate which sentences in the source language .</sentence>
				<definiendum id="0">Sentence alignment programs</definiendum>
				<definiens id="0">take a paired bilingual corpus as input and determine which sentences in the target language translate which sentences in the source language</definiens>
			</definition>
			<definition id="2">
				<sentence>In the final stage , XTRACT filters any pairs that do not consistently occur in the same syntactic relation , using a parsed version of the corpus .</sentence>
				<definiendum id="0">XTRACT</definiendum>
				<definiens id="0">filters any pairs that do not consistently occur in the same syntactic relation , using a parsed version of the corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>Specific mutual information is a good measure of independence ( which it was designed to measure ) , but good measures of independence are not necessarily good measures of similarity .</sentence>
				<definiendum id="0">Specific mutual information</definiendum>
				<definiens id="0">a good measure of independence ( which it was designed to measure</definiens>
			</definition>
			<definition id="4">
				<sentence>The fourth column gives the frequency of each French word pair in the French counterpart of the same corpus , and the fifth column gives the frequency of appearance of today and each French word pair in matched sentences .</sentence>
				<definiendum id="0">fourth column</definiendum>
				<definiens id="0">gives the frequency of each French word pair in the French counterpart of the same corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>In this way , the number of erroneous decisions made when SI is used at the final pass is a lower bound on the number of errors that would have been made if SI had also been used in the intermediate stages .</sentence>
				<definiendum id="0">SI</definiendum>
				<definiens id="0">a lower bound on the number of errors that would have been made if SI had also been used in the intermediate stages</definiens>
			</definition>
			<definition id="6">
				<sentence>X represents an English collocation ( credit card or affirmative action ) , and Y represents candidate translations in French ( for the credit cards example : cartes , cartes credit , cartes credit taux , and cartes crddit taux paient ) .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiens id="0">an English collocation ( credit card or affirmative action</definiens>
			</definition>
			<definition id="7">
				<sentence>WORD ORDER : officielles langues : selected position : -1 Figure 2 Sample output of Champollion .</sentence>
				<definiendum id="0">WORD ORDER</definiendum>
				<definiens id="0">officielles langues : selected position : -1 Figure 2 Sample output of Champollion</definiens>
			</definition>
			<definition id="8">
				<sentence>The Dice threshold Ta ( currently set at 0.10 ) is the major criterion that Champollion uses to decide which words or partial collocations should be kept as candidates for the final translation of the source collocation .</sentence>
				<definiendum id="0">Dice threshold Ta</definiendum>
				<definiens id="0">the major criterion that Champollion uses to decide which words or partial collocations should be kept as candidates for the final translation of the source collocation</definiens>
			</definition>
			<definition id="9">
				<sentence>The preparation phase reads in the database corpus and indexes it for fast future access using a commercial B-tree package ( Informix 1990 ) .</sentence>
				<definiendum id="0">preparation phase</definiendum>
			</definition>
			<definition id="10">
				<sentence>To study the average-case behavior of our algorithm , we simulated its performance with randomly selected points with integer non-negative coordinates ( nABX , nABy¢ , naf~x , nA~ ; ~ , n , ~x , nABS , nA~x ) from the hyperplane defined by the equation nABX + nAB R -b nAF~X + nA~ R qnAB X qnAuy ¢ + nA~ X = No where No is the number of `` interesting '' sentences in the corpus for the translation under consideration , that is , the number of sentences that contain at least one of X , A , or B. 13 Sampling from this six-dimensional polytope in seven-dimensional space is not easy .</sentence>
				<definiendum id="0">No</definiendum>
				<definiens id="0">randomly selected points with integer non-negative coordinates ( nABX , nABy¢ , naf~x , nA~ ; ~ , n , ~x , nABS , nA~x ) from the hyperplane defined by the equation nABX + nAB R -b nAF~X + nA~ R qnAB X qnAuy ¢ + nA~ X = No where</definiens>
				<definiens id="1">the number of `` interesting '' sentences in the corpus for the translation under consideration , that is , the number of sentences that contain at least one of X</definiens>
			</definition>
			<definition id="11">
				<sentence>The first database corpus ( DB1 ) consists of 8 months of Hansards aligned data taken from 1986 ( 16 megabytes , 3.5 million words ) and the second database corpus ( DB2 ) consists of all of the 1986 and 1987 transcripts of the Canadian Parliament ( a total of approximately 45 megabytes and 8.5 million words ) .</sentence>
				<definiendum id="0">DB1</definiendum>
				<definiendum id="1">Parliament</definiendum>
				<definiens id="0">consists of 8 months of Hansards aligned data taken from 1986 ( 16 megabytes , 3.5 million words</definiens>
				<definiens id="1">a total of approximately 45 megabytes and 8.5 million words )</definiens>
			</definition>
			<definition id="12">
				<sentence>Important occurs a total of 858 times in the French part of the corpus and only 8 times in the right context , whereas a minimum of 10 appearances is required to pass this step .</sentence>
				<definiendum id="0">Important</definiendum>
				<definiens id="0">occurs a total of 858 times in the French part of the corpus</definiens>
			</definition>
</paper>

		<paper id="4006">
			<definition id="0">
				<sentence>( ~ ) 1996 Association for Computational Linguistics Computational Linguistics Volume 22 , Number 4 CIAULA is an unsupervised learning algorithm for incremental concept formation , based on an augmented version of the well-known COBWEB ( Fisher 1987 ) .</sentence>
				<definiendum id="0">CIAULA</definiendum>
			</definition>
			<definition id="1">
				<sentence>Formally , The preference score g is a real-valued function defined by : 1 g ( syns ) = Z IS ( v ) t vCV ( syns , C ) where I I denotes cardinality .</sentence>
				<definiendum id="0">preference score g</definiendum>
			</definition>
			<definition id="2">
				<sentence>In Table I the fourth column ( Overlap Score ) is the ratio between verbs in a cluster ( column 1 ) that belong to the WordNet synset of column 3 , and the cardinality of the cluster .</sentence>
				<definiendum id="0">Overlap Score )</definiendum>
				<definiens id="0">the ratio between verbs in a cluster ( column 1 ) that belong to the WordNet synset of column 3 , and the cardinality of the cluster</definiens>
			</definition>
			<definition id="3">
				<sentence>Class # Ciaula Clusters ( cognition verbs ) WordNet Labels ( synsets ) Overlap Scores 137 represent , lie symbolize , stand for , express 1.00 indirectly , represent 397 base , study , estimate , analyze , analyse , study , 0.33 document , calculate , explore examine 429 review , compare , include judge , form an opinion of , 0.66 pass judgment on 562 estimate , increase , soil , process , change , alter 0.42 perform , observe , approach 684 determine , compute decide , make up one 's mind , 1.00 decide upon , determine 914 divide , transform , make create , make 0.66 1,196 include , base , deal , involve , study , think about , 0.18 mind , relate , measure , review , contemplate compare 1,224 derive , describe , retrieve , get , acquire , enter upon , come 0.28 document , review , compute , upon , luck into measure 1,374 calculate , provide think , cogitate , cerebrate 1.00 1,587 calculate , relate , focus think , cogitate , cerebrate 1.00 1,941 scan , compare , propose , analyze , analyse , study , 0.30 estimate , compute , analyse , examine study , experiment , base , evaluate 2,049 assess , plan , review judge , form an opinion of , 0.66 pass judgment on 2,055 determine , provide , plan , create , make 0.33 retrieve , view , show 2,102 provide , make create , make 1.00 2,147 include , propose , derive include 0.33 2,383 account , stand , situate , create , make 0.33 estimate , study , make 2,491 select , locate , analyse find , regain 0.66 2,725 infer , derive , measure , select , reason , reason out , conclude , 0.50 estimate , locate , compare , arrive at 2,797 3,518 3,637 3,758 4,080 4,170 calculate research , base , record collect , develop , map , list , record , make determine , record , compare , measure , decide scan , survey research , locate evaluate , plot , record , base investigate , look into create , make make up one 's mind , decide 0.75 upon , determine look at , take a look at , 1.00 examine , examine by sight investigate , look into 1.00 record , enter , put down , make 0.50 a record of ( iv ) class 2797 to record/ ( AFFECTED : ARTIFACT ) Some differences between the pattern in ( 3 ) and any of the feature vectors ( /-/v ) are : • Most of the syntactic relations expressed in ( 3 ) are accounted for in the semantic patterns that CIAULA detects as prototypical for the verb to 565 Computational Linguistics Volume 22 , Number 4 CLASS : 2383 Card : 7 stand ( 0.500 1.000 ) situate ( 0.500 1.000 ) estimate ( 0.500 0.037 ) study ( 0.500 0.036 ) account ( i.000 1.000 ) make ( 0.500 0 .</sentence>
				<definiendum id="0">WordNet Labels</definiendum>
				<definiendum id="1">record</definiendum>
				<definiens id="0">calculate , relate , focus think , cogitate , cerebrate 1.00 1,941 scan , compare , propose , analyze , analyse , study , 0.30 estimate , compute , analyse , examine study , experiment , base</definiens>
			</definition>
			<definition id="4">
				<sentence>For example , in pattern ( ii ) , a COGNITIVE_PROCESS rather than somebody is the agentive of to record ( e.g. , the algorithm~co records the changes ... ) and of the other members of the class labeled decide , make up one 's mind , decide upon , determine ( cluster 3,637 ) .</sentence>
				<definiendum id="0">algorithm~co</definiendum>
				<definiens id="0">records the changes ... ) and of the other members of the class labeled decide , make up one 's mind , decide upon , determine ( cluster 3,637 )</definiens>
			</definition>
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>A category consists of a set of feature equations , written : { fl=vl , f2=v2 ... . fN=vN } Feature names are atoms ; feature values can be variables ( beginning with an uppercase character ) , atoms ( beginning with a number or a lowercase character ) or categories .</sentence>
				<definiendum id="0">category</definiendum>
				<definiens id="0">consists of a set of feature equations</definiens>
				<definiens id="1">beginning with an uppercase character ) , atoms ( beginning with a number or a lowercase character ) or categories</definiens>
			</definition>
			<definition id="1">
				<sentence>To give an illustration , the following grammar generates indefinitely long , flat , NP conjunctions of the `` John , Mary , Bill , ... , and Fred '' type .</sentence>
				<definiendum id="0">Fred</definiendum>
				<definiens id="0">flat , NP conjunctions of the `` John , Mary , Bill , ... , and</definiens>
			</definition>
			<definition id="2">
				<sentence>We want to write feature equations like : f=l f=b f -- l &amp; b ; i and b f= ( a ; b ) ~2 ; either a or b , and 2 f=~2 ; not 2 f= ( 1- &gt; b ) &amp; ( ~l- &gt; c ) ; if i then b , else c f=2 &lt; - &gt; c ; 2 if and only if c To encode these values we build a term with a functor , say bv ( for Boolean vector ) with N+I variable arguments , where N is the size of the product of the sets from which f takes its values .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">write feature equations like : f=l f=b f -- l &amp; b ; i and b f= ( a</definiens>
			</definition>
			<definition id="3">
				<sentence>However , we would not want to build a term with N+I arguments where N is the number of verbs in English .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of verbs in English</definiens>
			</definition>
			<definition id="4">
				<sentence>We encode the set of values as something like : { be , have , do , anon } , where anon is some distinguished atomic value standing for any other verb .</sentence>
				<definiendum id="0">anon</definiendum>
				<definiens id="0">some distinguished atomic value standing for any other verb</definiens>
			</definition>
			<definition id="5">
				<sentence>Since everything is a subtype of itself , and btm is a subtype of everything , there is a 1 in each of the diagonal cells , and in the cell for btm on each row .</sentence>
				<definiendum id="0">btm</definiendum>
				<definiens id="0">a subtype of itself , and</definiens>
			</definition>
			<definition id="6">
				<sentence>The term will have N+I arguments , where N is the length of the bitstring , and adjacent arguments will be linked if their corresponding bitstring position is zero , and otherwise not linked .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">corresponding bitstring position</definiendum>
				<definiens id="0">the length of the bitstring</definiens>
			</definition>
			<definition id="7">
				<sentence>pp : { agent= ( something , NPsem ) , if=Vsem , vlf=Vsem } == &gt; \ [ p : { } , np : { if=NPsem } \ ] An agentive PP replaces the default agent value with that of the agentive NP and passes up the daughter VP meaning .</sentence>
				<definiendum id="0">PP</definiendum>
				<definiens id="0">replaces the default agent value with that of the agentive NP and passes up the daughter VP meaning</definiens>
			</definition>
			<definition id="8">
				<sentence>The language consists of sequences of a verb ( vabcd , vbcd , or vbd ) followed by the things it is subcategorized for , in any order : e.g. vabcd a b c d vabcd b a d c , etc. *vabcd a b c ~ .</sentence>
				<definiendum id="0">language</definiendum>
				<definiens id="0">consists of sequences of a verb ( vabcd , vbcd , or vbd ) followed by the things it is subcategorized for , in any order : e.g. vabcd a b c d vabcd b a d c</definiens>
			</definition>
			<definition id="9">
				<sentence>A frequently occurring case is the following : a particular word , W , has multiple possible realizations of some property , P1 .</sentence>
				<definiendum id="0">frequently</definiendum>
				<definiens id="0">a particular word , W , has multiple possible realizations of some property</definiens>
			</definition>
</paper>

		<paper id="4005">
</paper>

		<paper id="2002">
			<definition id="0">
				<sentence>1 DATR is a rather spartan nonmonotonic language for defining inheritance networks with path/value equations .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">a rather spartan nonmonotonic language for defining inheritance networks with path/value equations</definiens>
			</definition>
			<definition id="1">
				<sentence>A DATR description consists of a sequence of sentences corresponding semantically to a set of statements .</sentence>
				<definiendum id="0">DATR description</definiendum>
				<definiens id="0">consists of a sequence of sentences corresponding semantically to a set of statements</definiens>
			</definition>
			<definition id="2">
				<sentence>Syntactically , however , a DATR description consists of a sequence of sentences , where each sentence starts with a node name and ends with a period , and contains one or more path equations relating to that node , each corresponding to a statement in DATR .</sentence>
				<definiendum id="0">DATR description</definiendum>
				<definiens id="0">consists of a sequence of sentences , where each sentence starts with a node name and ends with a period , and contains one or more path equations relating to that node , each corresponding to a statement in DATR</definiens>
			</definition>
			<definition id="3">
				<sentence>Nodes are primitive tokens , paths are descriptor sequences ( defined below ) enclosed in angle brackets and node/path pairs consist of a node and a path , separated by a colon : Nodel &lt; descl desc2 desc3 ... &gt; Nodel : &lt; descl desc2 desc3 ... &gt; Finally there are three kinds of global inheritance descriptor , which are quoted variants of the three local types just described : `` Nodel '' `` &lt; descl desc2 desc3 ... &gt; '' `` Nodel : &lt; descl desc2 desc3 ... &gt; '' A descriptor sequence is a ( possibly empty ) sequence of descriptors .</sentence>
				<definiendum id="0">descriptor sequence</definiendum>
				<definiens id="0">primitive tokens , paths are descriptor sequences ( defined below ) enclosed in angle brackets and node/path pairs consist of a node and a path , separated by a colon : Nodel &lt; descl desc2 desc3 ... &gt; Nodel : &lt; descl desc2 desc3 ... &gt; Finally there are three kinds of global inheritance descriptor , which are quoted variants of the three local types just described : `` Nodel '' `` &lt; descl desc2 desc3 ... &gt; '' `` Nodel : &lt; descl desc2 desc3 ... &gt; '' A</definiens>
				<definiens id="1">a ( possibly empty ) sequence of descriptors</definiens>
			</definition>
			<definition id="4">
				<sentence>179 Computational Linguistics Volume 22 , Number 2 Simple extensional sentences take the form Node : Path = Ext. where Node is a node , Path is a simple path , and Ext is a simple value .</sentence>
				<definiendum id="0">Node</definiendum>
				<definiendum id="1">Path</definiendum>
				<definiendum id="2">Ext</definiendum>
				<definiens id="0">a node ,</definiens>
				<definiens id="1">a simple path</definiens>
				<definiens id="2">a simple value</definiens>
			</definition>
			<definition id="5">
				<sentence>where Node and Path are as above and Def is an arbitrary descriptor sequence .</sentence>
				<definiendum id="0">Def</definiendum>
				<definiens id="0">an arbitrary descriptor sequence</definiens>
			</definition>
			<definition id="6">
				<sentence>Second , `` defined '' here means `` defined by a definitional statement , '' that is , a `` == '' statement : local inheritance operates entirely with definitional statements , implicitly introducing new ones for Nodel : Path1 on the basis of those defined for Node2 : Path2 .</sentence>
				<definiendum id="0">definitional statement</definiendum>
			</definition>
			<definition id="7">
				<sentence>UNDEF is the name of a node that is not defined in the fragment , thus ensuring that &lt; syn args rest first &gt; , &lt; syn args rest rest &gt; , and so forth , are all undefined .</sentence>
				<definiendum id="0">UNDEF</definiendum>
				<definiens id="0">the name of a node that is not defined in the fragment</definiens>
			</definition>
			<definition id="8">
				<sentence>DATR is a language that allows the lexicon writer to define sets of partial functions from sequences of atoms to sequences of atoms .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">a language that allows the lexicon writer to define sets of partial functions from sequences of atoms to sequences of atoms</definiens>
			</definition>
			<definition id="9">
				<sentence>NONFUNC3 : &lt; a &gt; == &lt; b &gt; &lt; b &gt; == &lt; a &gt; &lt; a &gt; == i. In NONFUNC1 , UNDEF is a node with no associated definitions , so the first statement imposes no constraint on the value of &lt; a &gt; ; in NONFUNC2 , two definitions for &lt; a &gt; are provided , which happen to define the same value ; in NONFUNC3 , we establish a mutual dependence between &lt; a &gt; and &lt; b &gt; , and then define a value for one ( either ) of them .</sentence>
				<definiendum id="0">UNDEF</definiendum>
				<definiens id="0">a node with no associated definitions</definiens>
			</definition>
			<definition id="10">
				<sentence>Orthogonal multiple inheritance ( OMI ) is a desirable property of lexical representation systems .</sentence>
				<definiendum id="0">Orthogonal multiple inheritance</definiendum>
				<definiendum id="1">OMI</definiendum>
				<definiens id="0">a desirable property of lexical representation systems</definiens>
			</definition>
			<definition id="11">
				<sentence>Gibbon ( 1993 ) presents an implementation of a very flexible query language , EDQL , which allows quantification over any constituents of ( possibly complex ) DATR queries .</sentence>
				<definiendum id="0">EDQL</definiendum>
			</definition>
			<definition id="12">
				<sentence>Our title for this paper is to be taken literally -- DATR is a language for lexical knowledge representation .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">a language for lexical knowledge representation</definiens>
			</definition>
			<definition id="13">
				<sentence>It is a kind of programming language , not a theoretical framework for the lexicon ( in the way that , say , HPSG is a theoretical framework for syntax ) .</sentence>
				<definiendum id="0">HPSG</definiendum>
				<definiens id="0">a theoretical framework for syntax )</definiens>
			</definition>
			<definition id="14">
				<sentence>But DATR , of course , knows nothing about lexemes , affixes , vowels , words , lexical rules , or whatever .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">knows nothing about lexemes , affixes , vowels , words , lexical rules , or whatever</definiens>
			</definition>
			<definition id="15">
				<sentence>56 At the root of Krieger and Nerbonne 's ( 1993 ) critique of DATR is a complaint that it fails to provide all the resources of a modern , fully equipped unification grammar formalism ( p. 90-91 ) .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">a complaint that it fails to provide all the resources of a modern</definiens>
			</definition>
			<definition id="16">
				<sentence>Nerbonne appears to misconstrue Evans , Gazdar , and Moser ( 1993 ) as an attempt to augment DATR with re-entrancy and goes on to suggest that DATR is somehow forced to maintain that `` all linguistic generalizations tend to follow the lines of morphological form '' ( p. 47 ) when , in fact , the attribute ordering used in a DATR treatment of morphology is entirely independent of the use and ordering of those same attributes elsewhere in the lexicon ( see the discussion at the end of Section 4.1 , above ) .</sentence>
				<definiendum id="0">Nerbonne</definiendum>
				<definiens id="0">an attempt to augment DATR with re-entrancy and goes on to suggest that DATR is somehow forced to maintain that `` all linguistic generalizations tend to follow the lines of morphological form ''</definiens>
			</definition>
</paper>

		<paper id="3007">
			<definition id="0">
				<sentence>Kupiec extended a SCFG that worked on CNF to a general CFG ( Kupiec 1991 ) .</sentence>
				<definiendum id="0">Kupiec</definiendum>
			</definition>
			<definition id="1">
				<sentence>In this paper , we consider a probabilistic recursive transition network ( PRTN ) as an underlying grammar representation , and present an algorithm for training the probabilistic parameters , then suggest an improved version that works with reduced redundant computations .</sentence>
				<definiendum id="0">PRTN</definiendum>
				<definiens id="0">a probabilistic recursive transition network</definiens>
				<definiens id="1">training the probabilistic parameters , then suggest an improved version that works with reduced redundant computations</definiens>
			</definition>
			<definition id="2">
				<sentence>i Return i States ~ @ A is a transition matrix containing transition probabilities , and B is a word matrix containing the probability distribution of the words observable at each terminal transition .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">a word matrix containing the probability distribution of the words observable at each terminal transition</definiens>
			</definition>
			<definition id="3">
				<sentence>nonterminal ( 1 ) returns a set of nonterminal edges in layer I. denotes the edge between states i and j. \ [ i , j\ ] denotes the network segment between states i and j. Wa~b is a word sequence covering the a th to b th word .</sentence>
				<definiendum id="0">Wa~b</definiendum>
				<definiens id="0">the edge between states i</definiens>
			</definition>
			<definition id="4">
				<sentence>The Inside-Outside algorithm provides a formal basis for estimating parameters of context free languages so that the probabilities of the word sequences ( sample sentences ) may be maximized .</sentence>
				<definiendum id="0">Inside-Outside algorithm</definiendum>
				<definiens id="0">provides a formal basis for estimating parameters of context free languages so that the probabilities of the word sequences ( sample sentences ) may be maximized</definiens>
			</definition>
			<definition id="5">
				<sentence>And by definition : t P , ( i ) s~t = ~ aikb ( T~ , Ws ) P , ( k ) s+l~t + ~ Y2 aijauvP , ( j ) s~rP , ( v ) r+l~t. k j r=s ( 1 ) -- -+ ___+ where ik E terminal ( layer ( i ) ) , ij E nonterminal ( layer ( i ) ) , u = last ( layer ( j ) ) , v E bin ( layer ( j ) ) , and layer ( i ) = layer ( v ) .</sentence>
				<definiendum id="0">layer</definiendum>
				<definiens id="0">1 ) -- -+ ___+ where ik E terminal ( layer ( i ) ) , ij E nonterminal ( layer ( i ) )</definiens>
			</definition>
			<definition id="6">
				<sentence>For a pop transition : N N ~-~s=l ~-~t=s &amp; , vPl ( v ) s~taijPo ( u , J ) sNt Go , ( ij ) = P ( W I , k ) where u E bout ( layer ( i ) ) , j c bin ( layer ( i ) ) , v = first ( layer ( i ) ) , layer ( u ) = layer ( j ) , layer ( v ) = layer ( i ) , and uv is a nonterminal transition .</sentence>
				<definiendum id="0">uv</definiendum>
				<definiens id="0">a nonterminal transition</definiens>
			</definition>
			<definition id="7">
				<sentence>Since transitions of terminal and nonterminal types can occur together at a state , terminal transitions are estimated as follows : -- -+ a-ij Gk Et ( ik ) + Gk Ent ( ~ ) ( 3 ) For nonterminal transitions : E , , , ( q ) ( 4 ) And for pop transitions , notice that only pop transitions are possible at a pop state : L aq _ Go , ( q ) Y~k Epop ( ik ) ( 5 ) For a terminal transition ~ and a word symbol w : Ct ~ .</sentence>
				<definiendum id="0">Gk Ent</definiendum>
				<definiens id="0">terminal transitions are estimated as follows : -- -+ a-ij Gk Et ( ik ) +</definiens>
			</definition>
			<definition id="8">
				<sentence>It can be shown that the complexity of the Inside algorithm is O ( N3G 3 ) and that of the Outside algorithm is O ( N4G 3 ) where N is the input size and G is the number of states .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">G</definiendum>
				<definiens id="0">the input size</definiens>
				<definiens id="1">the number of states</definiens>
			</definition>
			<definition id="9">
				<sentence>For instance , the table for storing Inside computations takes O ( N2G2C ) store , where C is the number of terminal and nonterminal categories .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">the number of terminal and nonterminal categories</definiens>
			</definition>
			<definition id="10">
				<sentence>A chart item is a function of five parameters , and returns an Inside probability .</sentence>
				<definiendum id="0">chart item</definiendum>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>An ATL is an adjunct , such as yesterday , until 10:50 , or when it flew over Barcelona , that provides information about the temporal localization of an occurrence or its duration , or both at the same time .</sentence>
				<definiendum id="0">ATL</definiendum>
				<definiens id="0">an adjunct</definiens>
			</definition>
			<definition id="1">
				<sentence>In our discourse representation , which uses Segmented Discourse Representa93 Computational Linguistics Volume 22 , Number 1 tion Theory ( SDRT ) ( Asher 1993 ) , the information is cut into smaller segments each of which contains the information to be expressed by a single sentence .</sentence>
				<definiendum id="0">discourse representation</definiendum>
				<definiens id="0">uses Segmented Discourse Representa93 Computational Linguistics Volume 22 , Number 1 tion Theory ( SDRT ) ( Asher 1993 ) , the information is cut into smaller segments each of which contains the information to be expressed by a single sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>To represent temporal concepts in Pr6texte , we chose the principles of Discourse Representation Theory ( DRT ) , which offers one of the most interesting explanations of how temporal notions are conveyed by a text .</sentence>
				<definiendum id="0">DRT )</definiendum>
				<definiens id="0">offers one of the most interesting explanations of how temporal notions are conveyed by a text</definiens>
			</definition>
			<definition id="3">
				<sentence>A DRS is a structure containing a set of entities and a set of conditions on these entities .</sentence>
				<definiendum id="0">DRS</definiendum>
				<definiens id="0">a structure containing a set of entities and a set of conditions on these entities</definiens>
			</definition>
			<definition id="4">
				<sentence>LOC represents the temporal location of the occurrence reported .</sentence>
				<definiendum id="0">LOC</definiendum>
				<definiens id="0">the temporal location of the occurrence reported</definiens>
			</definition>
			<definition id="5">
				<sentence>LOC represents the information to be expressed by an ATL in the sentence and does not necessarily have a value , because a sentence may not contain an ATL .</sentence>
				<definiendum id="0">LOC</definiendum>
				<definiens id="0">the information to be expressed by an ATL in the sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>Thus , the value of LOC in the semantic representation determines the expression of the localization , giving rise to three main problems : • how to represent the temporal constants in the conceptual representation ; • how to determine the link between these conceptual temporal constants and their semantic representation , which specifies how they are to be expressed in the text ; and • how to implement the selection mechanism , which relies on pragmatic and stylistic information to choose between the many different ways of expressing the same temporal localization .</sentence>
				<definiendum id="0">semantic representation</definiendum>
			</definition>
			<definition id="7">
				<sentence>Localizer Description inclin ( \ [ ti , Ti , Ni\ ] , \ [ tj , Tj , Nj\ ] ) incl ( \ [ ti , Ti , Ni\ ] , \ [ tj , Tj , Nj\ ] ) begin ( \ [ ti , Ti , Ni\ ] , \ [ tj , Tj , Nj\ ] ) end ( \ [ ti , Ti , Ni\ ] , \ [ tj , Tj , Nj\ ] ) after ( \ [ ti , Ti , Ni\ ] , \ [ tj , Tj , Nj\ ] , D ) before ( \ [ ti , T , , Ni\ ] , \ [ tj , Tj , Nj\ ] , D ) relpos ( X , \ [ ti , T , Ni\ ] , \ [ tj , T , Nj\ ] ) extent ( \ [ ti , Ti , Ni\ ] , \ [ G Tj , Nj\ ] , \ [ tk , Tk , Nk\ ] ) t~ is a time included in the time t 7 t* is a time that includes the time tj* t~ is a time whose beginning overlaps the time t~ t~ is a time whose ending overlaps the time t ; t~ is a time after the time t~ with a temporal distance D ( expressed as a duration ) t~ is a time before time t 7 with a temporal distance D ( expressed as a duration ) t~ is the X th ( -X th , if X &lt; 0 ) time of type T after ( before , if X &lt; 0 ) time tj* t~ is a time period starting at time t7 and ending at time t ; Suppose , for example , that the discourse contains two temporal constants , corresponding to yesterday and two days after Robert 's departure .</sentence>
				<definiendum id="0">t~</definiendum>
				<definiens id="0">a time included in the time t 7 t* is a time that includes the time tj* t~ is a time whose beginning overlaps the time t~ t~ is a time whose ending overlaps the time t</definiens>
			</definition>
			<definition id="8">
				<sentence>Semantics ATL ( 1 ) relpos ( -1 , \ [ tloc , day , _\ ] , incl ( \ [ t2 , day , _\ ] , \ [ n , _ , _\ ] ) ) ( 2 ) relpos ( -1 , \ [ hoc , daY , -\ ] , incl ( \ [ t2 , day , _\ ] , \ [ tref , _ , _\ ] ) ) ( 3 ) incl ( \ [ tloc , day , _\ ] , \ [ n , _ , -\ ] ) ( 4 ) incl ( \ [ tloc , month , _\ ] , \ [ n , _ , _\ ] ) ( 5 ) incl ( \ [ tloc , month , _\ ] , \ [ tref , - , -\ ] ) ( 6 ) \ [ tloc , month , April\ ] ( 7 ) \ [ 01 , - , -\ ] ( 8 ) inclin ( \ [ tloc , moment-of-day , morning\ ] , inclin ( \ [ t2 , season , summer\ ] , \ [ t3 , year,1995\ ] ) ) ( 9 ) inclin ( \ [ tloc , half-hour,1\ ] , occurrence ( ol ) ) ( 10 ) duration ( 3 , day ) ( 11 ) end ( \ [ tloc , _ , _\ ] , inclin ( \ [ t2 , day , wednesday\ ] , incl ( \ [ t3 , week , _\ ] , \ [ n , _ , _\ ] ) ) ) ( 12 ) begin ( \ [ tloc , _ , _\ ] , relpos ( 1 , \ [ t2 , month , _\ ] , incl ( \ [ t 3 , month , -\ ] , \ [ tref , - , -\ ] ) ) ) ( 13 ) begin ( \ [ tloc , _ , _\ ] , inclin ( \ [ t2 , day , lO\ ] , \ [ t3 , month , may\ ] ) ) ( 14 ) relpos ( 3 , \ [ tloc , day , _\ ] , occurrence ( ol ) ) ( 15 ) after ( \ [ tloc , day , _\ ] , \ [ tref , _ , _\ ] , duration ( 3 , day ) ) ( 16 ) after ( \ [ tloc , _ , _\ ] , \ [ tref , _ , _\ ] , indefinite ) ( 17 ) extent ( \ [ tloc , _ , _\ ] , inclin ( \ [ t2 , day,3\ ] , \ [ t3 , month , april\ ] ) , inclin ( \ [ t4 , day , lO\ ] , \ [ ts , month , may\ ] ) ) ( 18 ) extent ( \ [ tloc , _ , _\ ] , before ( \ [ t2 , - , -\ ] , \ [ tre f , - , -\ ] , duration ( 3 , day ) ) , \ [ tre/ , - , -l ) hier ( yesterday ) la veille ( the day before ) aujourd'hui ( today ) ce mois-ci ( this month ) ce mois-la ( that month ) en avril ( in April ) quand Robert est parti ( when Robert left ) le matin du 3 avril 1995 ( the morning of April 3rd 1995 ) la premi6re demi-heure de l'6mission ( the first half hour of the program ) durant trois jours ( during three days ) jusqu'a mercredi de cette semaine ( until Wednesday of this week ) a partir du mois suivant ( from the following month ) depuis le 10 mai ( sinceMay lOth ) trois jours apr6s le d6part de Robert ( three days after Robert 's departure ) trois jours plus tard ( three days later ) puis ( then ) du 3 avril au 10 mai ( from April 3rd to May lOth ) depuis trois jours ( since three days ) Unfortunately , the combination of localizers in the semantics does not always correspond to the combination of adverbials .</sentence>
				<definiendum id="0">Semantics ATL</definiendum>
				<definiendum id="1">_\ ] )</definiendum>
				<definiens id="0">[ tref , - , -\ ] ) ( 6 ) \ [ tloc , month</definiens>
				<definiens id="1">inclin ( \ [ t2 , season</definiens>
			</definition>
			<definition id="9">
				<sentence>Adverbial QTY_ANCHORS ANCHOR ANCHOR1 ANCHOR2 ( 1 ) hier ( 3 ) aujourd'hui ( 4 ) ce mois-ci ( 11 ) jusqu'h mercredi de cette semaine ( 2 ) la veille ( 5 ) ce mois-lh ( 12 ) a partir du mois suivant ( 15 ) trois jours plus tard ( 16 ) puis ( 6 ) en avril ( 7 ) quand Robert est parti ( 8 ) le matin du 3 avril 1995 ( 9 ) la premi6re demi-heure de l'6mission ( 10 ) durant trois jours ( 13 ) depuis le 10 rnai ( 14 ) trois jours apr6s le d6part de Robert ( 17 ) du 3 avril au 10 mai ( 18 ) depuis trois jours unique umque unique umque umque unique unique unique umque umque unique unique umque unique umque umque double double deictic deictic deictic deictic anaphoric anaphonc anaphoric anaphoric anaphoric autonomous autonomous autonomous autonomous autonomous autonomous autonomous autonomous anaphonc autonomous anaphoric Positioner \ [ Jusque Until which the feature punctual has been selected ; this is a consequence of the selection of the feature relational .</sentence>
				<definiendum id="0">Jusque Until</definiendum>
				<definiens id="0">umque unique umque umque unique unique unique umque umque unique unique umque unique umque umque double double deictic deictic deictic deictic anaphoric anaphonc anaphoric anaphoric anaphoric autonomous autonomous autonomous autonomous autonomous autonomous autonomous autonomous anaphonc autonomous anaphoric Positioner \ [</definiens>
			</definition>
			<definition id="10">
				<sentence>But our approach is a good start and it could be extended by adding more systems and their selection rules , without changing the overall structure of the network .</sentence>
				<definiendum id="0">selection rules</definiendum>
				<definiens id="0">a good start</definiens>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>Obviously the lexical prior probability of this form expressing the finite plural is not zero , the MLE is a poor estimate in such cases .</sentence>
				<definiendum id="0">MLE</definiendum>
				<definiens id="0">a poor estimate in such cases</definiens>
			</definition>
			<definition id="1">
				<sentence>These verbs include unergative intransitives like walk , for which one would not expect to find the adjectival usage , given the above characterization ; but they also include clear transitives like move , try , and ask , and unaccusative intransitives like appear , which are not generally felicitous in this usage .</sentence>
				<definiendum id="0">appear</definiendum>
				<definiens id="0">unergative intransitives like walk , for which one would not expect to find the adjectival usage , given the above characterization ; but they also include clear transitives like move , try , and ask</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus , the hapax-based MLE yields an estimate that is uncontaminated by the lexical properties of individual high-frequency forms .</sentence>
				<definiendum id="0">MLE</definiendum>
				<definiens id="0">yields an estimate that is uncontaminated by the lexical properties of individual high-frequency forms</definiens>
			</definition>
			<definition id="3">
				<sentence>The probability of encountering an unseen word given that this word is a word in -en is estimated by : N1 , N ( -en ) Pr ( unseen\ [ -en ) ~ N ( _en ) , ( 1 ) where N1 , N ( -en ) denotes the number of hapax legomena in -en among the N ( -en ) tokens in -en in the training sample ; see Baayen ( 1989 ) , Baayen and Lieber ( 1991 ) , Good ( 1953 ) , and Church and Gale ( 1991 ) .</sentence>
				<definiendum id="0">N ( -en )</definiendum>
				<definiens id="0">a word in -en is estimated by : N1</definiens>
				<definiens id="1">the number of hapax legomena in -en among the N ( -en ) tokens in -en in the training sample</definiens>
			</definition>
</paper>

		<paper id="4003">
			<definition id="0">
				<sentence>The fact that the weaker generative capacity of FSTs makes them easier to 498 Gildea and Jurafsky Learning Bias and Phonological-Rule Induction learn than arbitrary context-sensitive rules has allowed the development of a number of learning algorithms including those for deterministic finite-state automata ( FSAs ) ( Freund et al. 1993 ) , deterministic transducers ( Oncina , Garcia , and Vidal 1993 ) , as well as nondeterministic ( stochastic ) FSAs ( Stolcke and Omohundro 1993 ; Stolcke and Omohundro 1994 ; Ron , Singer , and Tishby 1994 ) .</sentence>
				<definiendum id="0">nondeterministic ( stochastic ) FSAs</definiendum>
				<definiens id="0">the weaker generative capacity of FSTs makes them easier to 498 Gildea and Jurafsky Learning Bias and Phonological-Rule Induction learn than arbitrary context-sensitive rules has allowed the development of a number of learning algorithms including those for deterministic finite-state automata ( FSAs ) ( Freund et al. 1993 ) , deterministic transducers ( Oncina , Garcia</definiens>
			</definition>
			<definition id="1">
				<sentence>A subsequential relation is any relation between strings that can represented by the input to output relation of a subsequential finite-state transducer .</sentence>
				<definiendum id="0">subsequential relation</definiendum>
				<definiens id="0">any relation between strings that can represented by the input to output relation of a subsequential finite-state transducer</definiens>
			</definition>
			<definition id="2">
				<sentence>( 2 ) a.t -- *dx/gr*_V b. latter : l ael t er -- * i ael dx er c. laughter : i ael f t er -- * I ael I t er The most significant difference between our subsequential transducers and twolevel models is that the two-level transducers described by Karttunen ( 1993 ) are non502 Gildea and Jurafsky Learning Bias and Phonological-Rule Induction tc /Start state ~_ Ex : batter r V blaellt ler I c ~ T c \ , A : ,e Seen stressed V • dxV Vrl ; ; v NV¢ '' vowe , # : t k , ,~\ ] .</sentence>
				<definiendum id="0">v NV¢</definiendum>
				<definiens id="0">T c \ , A : ,e Seen stressed V • dxV Vrl ; ;</definiens>
			</definition>
			<definition id="3">
				<sentence>The push-back operation allows the two arcs to be combined into one and their destination states to be merged .</sentence>
				<definiendum id="0">push-back operation</definiendum>
			</definition>
			<definition id="4">
				<sentence>OSTIA has a tendency to produce overly `` clumped '' transducers , as illustrated by the arcs with output b ae and n d in Figure 8 , or even Figure 4 .</sentence>
				<definiendum id="0">OSTIA</definiendum>
				<definiens id="0">has a tendency to produce overly `` clumped '' transducers , as illustrated by the arcs with output b ae</definiens>
			</definition>
			<definition id="5">
				<sentence>Orgun ( 1996 ) extends the two-level optimality-theoretic concept of faithfulness to require a kind of monotonicity from the underlying to the surface form : his MATCH constraint requires that every element of an output string contain all the information in the corresponding element of an input string .</sentence>
				<definiendum id="0">MATCH constraint</definiendum>
				<definiens id="0">requires that every element of an output string contain all the information in the corresponding element of an input string</definiens>
			</definition>
			<definition id="6">
				<sentence>A decision tree takes a set of properties that describe an object and outputs a decision about that object .</sentence>
				<definiendum id="0">decision tree</definiendum>
				<definiens id="0">takes a set of properties that describe an object and outputs a decision about that object</definiens>
			</definition>
			<definition id="7">
				<sentence>It represents the process of making a decision as a rooted tree , in which each internal node represents a test of the value of a given property , and each leaf node represents a decision .</sentence>
				<definiendum id="0">leaf node</definiendum>
				<definiens id="0">the process of making a decision as a rooted tree , in which each internal node represents a test of the value of a given property</definiens>
			</definition>
			<definition id="8">
				<sentence>Our data set was constructed from the CELEX lexical database ( Celex 1993 ) , which contains pronunciations for 359,611 word forms -- including various inflected forms of the same lexeme .</sentence>
				<definiendum id="0">CELEX lexical database</definiendum>
				<definiens id="0">contains pronunciations for 359,611 word forms -- including various inflected forms of the same lexeme</definiens>
			</definition>
			<definition id="9">
				<sentence>The OSTIA algorithm as described by Oncina , Garcfa , and Vidal ( 1993 ) had a worstcase complexity of O ( nB ( m + k ) + nmk ) , where n is the sum of all the input strings ' lengths , m is the length of the longest output string , and k is the size of the input alphabet ; Oncina , Garcfa , and Vidal 's ( 1993 ) experiments showed the average case time to grow more slowly .</sentence>
				<definiendum id="0">OSTIA algorithm</definiendum>
				<definiendum id="1">n</definiendum>
				<definiendum id="2">m</definiendum>
				<definiendum id="3">k</definiendum>
				<definiens id="0">the length of the longest output string</definiens>
			</definition>
			<definition id="10">
				<sentence>His algorithm works for rules of the form ( 15 ) a -- * b/C where C is the feature matrix of the segments around a. Johnson 's algorithm sets up a system of constraint equations that C must satisfy , by considering both the positive contexts , i.e. , all the contexts Ci in which a b occurs on the surface , as well as all the negative contexts Cj in which an a occurs on the surface .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">the feature matrix of the segments around a. Johnson 's algorithm sets up a system of constraint equations that C must satisfy , by considering both the positive contexts</definiens>
			</definition>
</paper>

		<paper id="3003">
			<definition id="0">
				<sentence>M } , fly ( m ) = P ( Ot = vm \ ] qt = Sj ) ( 2 ) ~r = Or/ : i= 1 ... N } , 7ri = P ( ql = Si ) ( 3 ) where N is the number of possible hidden states and M is the number of all the observable events .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">the number of all the observable events</definiens>
			</definition>
			<definition id="1">
				<sentence>Obviously the dimension of matrix A is N x N , that of matrix B is N x M , and ~ is a vector of N elements .</sentence>
				<definiendum id="0">~</definiendum>
			</definition>
			<definition id="2">
				<sentence>In equations ( 1 ) - ( 3 ) , qt is the hidden state of the system at time t , Si is the /th possible hidden state of the system , Ot is the observation symbol at time t , and Vm is the m th possible observable symbol .</sentence>
				<definiendum id="0">Si</definiendum>
				<definiendum id="1">Ot</definiendum>
				<definiendum id="2">Vm</definiendum>
				<definiens id="0">the hidden state of the system at time t</definiens>
				<definiens id="1">the observation symbol at time t</definiens>
				<definiens id="2">the m th possible observable symbol</definiens>
			</definition>
			<definition id="3">
				<sentence>This algorithm proceeds 353 Computational Linguistics Volume 22 , Number 3 recursively from the beginning to the end of the word calculating for any time ( in the case of PTGC , time is the position of a phoneme/grapheme in the word ) the score of the best path in all possible hidden-state sequences that end at the current state .</sentence>
				<definiendum id="0">time</definiendum>
				<definiens id="0">the position of a phoneme/grapheme in the word ) the score of the best path in all possible hidden-state sequences that end at the current state</definiens>
			</definition>
			<definition id="4">
				<sentence>In general : nS ( qt-1 -- -Si , qt = Sj ) ( 4 ) aij = n ( qt = S/ ) n ' ( qt = SpOt = vm ) bj ( m ) = n ( qt = Sj ) ( 5 ) n ' ( ql = Si ) ( 6 ) ~i -n ( ql ) where n ( x ) is the number of occurrences of x in the training corpus and n ' ( x ) is an estimation of the number of occurrences of x in the application corpus .</sentence>
				<definiendum id="0">n ( x )</definiendum>
				<definiens id="0">the number of occurrences of x in the training corpus</definiens>
				<definiens id="1">an estimation of the number of occurrences of x in the application corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>The rules for the segmentation of a phoneme string to a sequence of symbols conforming to the above condition are manually defined off-line according to the procedure presented below in an informal algorithmic language ( Figure 1 ) .</sentence>
				<definiendum id="0">The rules for</definiendum>
			</definition>
			<definition id="6">
				<sentence>The following consideration is the basis of the multiple-output conversion algorithm : Let QE = { Ql ( t ) , Q2 ( t ) , ... , QE ( t ) } be the globally E best hidden-state sequences that end at state qt = Si at a given time t. By `` best '' we mean , as usual , those sequences having the highest probability .</sentence>
				<definiendum id="0">, QE</definiendum>
				<definiens id="0">the basis of the multiple-output conversion algorithm : Let QE = { Ql ( t ) , Q2 ( t ) , ...</definiens>
			</definition>
			<definition id="7">
				<sentence>Clearly Qx ( t ) ~ QE ~=~ P ( Qx ( t ) ) &lt; P ( Qi ( t ) ) , Vi E 1 ... E. The probability of the complete state sequence Qm ( T ) ( 1 &lt; m _ &lt; E ) which contains Qx ( t ) would be : P ( Qm ( T ) ) T T=2 t T `` ~ 71 '' ql/~ql ( Ol ) '' H OLq'-\ ] qT~ ' ( OT ) '' 1-'I OZq'r -\ ] q'r ~q '' ( OT ) r=2 r=t+l T = P ( QI ( t ) ) .</sentence>
				<definiendum id="0">OT</definiendum>
				<definiens id="0">1 &lt; m _ &lt; E ) which contains Qx ( t ) would be : P ( Qm ( T ) ) T T=2 t T</definiens>
			</definition>
			<definition id="8">
				<sentence>The matrix density is a way of measuring the saturation of the model , that is , whether the model is sufficiently objective or is too dependent on the nature of the training material .</sentence>
				<definiendum id="0">matrix density</definiendum>
				<definiens id="0">a way of measuring the saturation of the model , that is , whether the model is sufficiently objective or is too dependent on the nature of the training material</definiens>
			</definition>
			<definition id="9">
				<sentence>The columns have the following meaning : d : l ( s ) : l ( w ) : 1-2 , etc : Is/lw x 100 % where lw is the size of a word in error ( in characters ) , Is is the number of incorrect characters in the word , and Is/lw x 100 % is the mean value estimated over all wrong words .</sentence>
				<definiendum id="0">lw</definiendum>
				<definiens id="0">the size of a word in error ( in characters</definiens>
				<definiens id="1">the number of incorrect characters in the word</definiens>
			</definition>
			<definition id="10">
				<sentence>The legends of these figures have the form cc/n where cc is a two letter code for the corpus domain ( E1/E2/NE/N1D , as described in the beginning of this section ) and n is either 1 for a first-order model or 2 for a second-order model .</sentence>
				<definiendum id="0">cc</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">described in the beginning of this section</definiens>
			</definition>
</paper>

	</volume>
