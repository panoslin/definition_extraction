<?xml version="1.0" encoding="UTF-8"?>
	<volume id="H93">

		<paper id="1023">
			<definition id="0">
				<sentence>Results of trials of the message identification systems using the Switchboard corpus of telephone conversations are reported .</sentence>
				<definiendum id="0">Switchboard</definiendum>
				<definiens id="0">corpus of telephone conversations are reported</definiens>
			</definition>
			<definition id="1">
				<sentence>The first , P ( A \ [ W , T ) , we can think of as the contribution of the acoustic model , which assigns probabilities to acoustic observations generated from a given string of words .</sentence>
				<definiendum id="0">acoustic model</definiendum>
				<definiens id="0">assigns probabilities to acoustic observations generated from a given string of words</definiens>
			</definition>
			<definition id="2">
				<sentence>To create our `` Switchboard '' recognizer , male and female speaker-independent acoustic models were trained using a total of about 9 hours of Switchboard messages ( approximately 140 message halves ) from 8 male and 8 female speakers not involved in the test sets .</sentence>
				<definiendum id="0">Switchboard</definiendum>
				<definiens id="0">'' recognizer , male and female speaker-independent acoustic models were trained using a total of about 9 hours of Switchboard messages ( approximately 140 message halves ) from 8 male and 8 female speakers not involved in the test sets</definiens>
			</definition>
			<definition id="3">
				<sentence>`` SWITCHBOARD : Telephone Speech Corpus for Research and Development , '' Proc .</sentence>
				<definiendum id="0">SWITCHBOARD</definiendum>
				<definiens id="0">Telephone Speech Corpus for Research and Development , '' Proc</definiens>
			</definition>
</paper>

		<paper id="1010">
</paper>

		<paper id="1111">
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>MEETING LANGUAGE TECHNOLOGY NEEDS Researchers ' primary focus continues to be studying basic scientific principles and using them to expand the capabilities of language technology regardless of modality .</sentence>
				<definiendum id="0">MEETING LANGUAGE TECHNOLOGY</definiendum>
				<definiens id="0">NEEDS Researchers ' primary focus continues to be studying basic scientific principles and using them to expand the capabilities of language technology regardless of modality</definiens>
			</definition>
</paper>

		<paper id="1094">
</paper>

		<paper id="1057">
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>Gemini is a natural language understanding system developed for spoken language applications .</sentence>
				<definiendum id="0">Gemini</definiendum>
			</definition>
			<definition id="1">
				<sentence>The Gemini kernel consists of a set of compilers to interpret the high-level languages in which the lexicon and syntactic and semantic grammar rules are written , as wellI as the parser , semantic interpretation , quantifier scoping , and repair correction mechanisms , as well as all other aspects of Gemini that are not specific to a language or domain .</sentence>
				<definiendum id="0">Gemini kernel</definiendum>
				<definiens id="0">consists of a set of compilers to interpret the high-level languages in which the lexicon and syntactic and semantic grammar rules are written , as wellI as the parser , semantic interpretation , quantifier scoping , and repair correction mechanisms , as well as all other aspects of Gemini that are not specific to a language or domain</definiens>
			</definition>
			<definition id="2">
				<sentence>Here is an example : up : \ [ wh=ynq , case= ( nomVacc ) , pers_num= ( 3rdAsg ) \ ] This category can be instantiated by any noun phrase with the value ynq for its wh feature ( which means it must be a wh-bearing noun phrase like which book , who , or whose mother ) , either acc ( accusative ) or nora ( nominative ) for its case feature , and the conjunctive value 3rdAsg ( third and singular ) for its person-number feature .</sentence>
				<definiendum id="0">nora</definiendum>
				<definiens id="0">means it must be a wh-bearing noun phrase like which book , who , or whose mother ) , either acc ( accusative ) or</definiens>
			</definition>
			<definition id="3">
				<sentence>The utterance parser is a top-down back-tracking parser that uses a different grammar called the utterance grammar to glue the constituents found during constituent parsing together to span the entire utterance .</sentence>
				<definiendum id="0">utterance parser</definiendum>
				<definiens id="0">a top-down back-tracking parser that uses a different grammar called the utterance grammar to glue the constituents found during constituent parsing together to span the entire utterance</definiens>
			</definition>
			<definition id="4">
				<sentence>The scoping algorithm that we use combines syntactic and semantic information with a set of quantifier scoping preference rules to rank the possible scoped logical forms consistent with the quasi-logical form selected by parse preferences .</sentence>
				<definiendum id="0">scoping algorithm</definiendum>
				<definiens id="0">combines syntactic and semantic information with a set of quantifier scoping preference rules to rank the possible scoped logical forms consistent with the quasi-logical form selected by parse preferences</definiens>
			</definition>
</paper>

		<paper id="1106">
</paper>

		<paper id="1068">
			<definition id="0">
				<sentence>Figure lb presenls a survey of the results of the phonetic analyses for one of the test utterances .</sentence>
				<definiendum id="0">Figure lb</definiendum>
				<definiens id="0">presenls a survey of the results of the phonetic analyses for one of the test utterances</definiens>
			</definition>
</paper>

		<paper id="1115">
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>Prosody is the organization imposed onto a string of words when they are uttered as connected speech .</sentence>
				<definiendum id="0">Prosody</definiendum>
				<definiens id="0">the organization imposed onto a string of words when they are uttered as connected speech</definiens>
			</definition>
			<definition id="1">
				<sentence>Callers are real users of the information service .</sentence>
				<definiendum id="0">Callers</definiendum>
				<definiens id="0">real users of the information service</definiens>
			</definition>
</paper>

		<paper id="1024">
</paper>

		<paper id="1066">
			<definition id="0">
				<sentence>ABSTRACT Interpreting fttUy natural speech is an important goal for spoken language understanding systems .</sentence>
				<definiendum id="0">ABSTRACT Interpreting fttUy natural speech</definiendum>
				<definiens id="0">an important goal for spoken language understanding systems</definiens>
			</definition>
			<definition id="1">
				<sentence>RIM divides the repair event into three consecutive temporal intervals and identifies time points within those intervals which are computationally critical .</sentence>
				<definiendum id="0">RIM</definiendum>
				<definiens id="0">divides the repair event into three consecutive temporal intervals and identifies time points within those intervals which are computationally critical</definiens>
			</definition>
			<definition id="2">
				<sentence>The DISFLUENCY INTERVAL extends from the IS to the resumption of fluent speech , and may contain any combination of silence , pause fillers ( 'uh ' ) , or CUE PHRASES ( 'Oops ' or 'I mean ' ) , which indicate the speaker 's recognition of his/her performance error .</sentence>
				<definiendum id="0">DISFLUENCY INTERVAL</definiendum>
				<definiendum id="1">CUE PHRASES</definiendum>
				<definiens id="0">extends from the IS to the resumption of fluent speech , and may contain any combination of silence , pause fillers ( 'uh ' ) , or</definiens>
			</definition>
			<definition id="3">
				<sentence>Levelt &amp; Cutler 1983 \ [ 20\ ] claim that repairs of erroneous information ( ERROR REPAIRS ) are marked by increased intonational prominence on the correcting information , while other kinds of repairs such as additions to descriptions ( APPROPgtnTE~ESS gEPAmS ) generally are not .</sentence>
				<definiendum id="0">APPROPgtnTE~ESS gEPAmS</definiendum>
				<definiens id="0">claim that repairs of erroneous information ( ERROR REPAIRS ) are marked by increased intonational prominence on the correcting information</definiens>
			</definition>
</paper>

		<paper id="1083">
</paper>

		<paper id="1078">
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>By using a full continuum of linguistic-conceptual processing , DRLINK has the capability of producing documents which precisely match users ' needs .</sentence>
				<definiendum id="0">DRLINK</definiendum>
				<definiens id="0">the capability of producing documents which precisely match users ' needs</definiens>
			</definition>
			<definition id="1">
				<sentence>The system evaluates the correlation coefficients between the unique and most frequent-SFCs of the sentence and the multiple SFCs assigned to the word being disambiguated to determine which of the multiple SFCs has the highest correlation with a unique-SFC or fTequent-SFC .</sentence>
				<definiendum id="0">SFCs</definiendum>
				<definiens id="0">the correlation coefficients between the unique and most frequent-SFCs of the sentence and the multiple SFCs assigned to the word being disambiguated to determine which of the multiple</definiens>
			</definition>
</paper>

		<paper id="1096">
			<definition id="0">
				<sentence>next portion of ATIS MADCOW data , to be collected with the 46-city OAGderived relational database .</sentence>
				<definiendum id="0">ATIS MADCOW</definiendum>
				<definiens id="0">data , to be collected with the 46-city OAGderived relational database</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>Most HMM-based speech recognition systems have one single constant language weight that is independent of any specific acoustic or language information , and that is determined using a hill-climbing procedure on development data .</sentence>
				<definiendum id="0">HMM-based speech recognition systems</definiendum>
				<definiens id="0">independent of any specific acoustic or language information</definiens>
			</definition>
			<definition id="1">
				<sentence>Our multi-pass 84 search algorithm generates N-best hypotheses which are used to optimize language weights or implement many discriminative training methods , where recognition errors can be used as the objective function \ [ 20 , 25\ ] .</sentence>
				<definiendum id="0">recognition errors</definiendum>
				<definiens id="0">generates N-best hypotheses which are used to optimize language weights or implement many discriminative training methods</definiens>
			</definition>
			<definition id="2">
				<sentence>We can assign a variable weight to each of the n-gram probabilities such that we have a weighted language probability as : W ( W ) = rr , _ , _ , _ _ , ~ , ( x , , , , , ,w , _ , , ... ) llr , tw~lw~-rw~-2 ... ) ( 1 ) i where the weight c~ 0 is a function of acoustic data , Xi , for wi , and words wi , Wi-l , ... . For a given sentence k , a very general objective function can be defined as Lk ( A ) = EPr ( 0 ) l -- ~\ [ l°gPr ( 'V'lwi ) + i~o +a ( Xi , wi wi_ l..</sentence>
				<definiendum id="0">Lk ( A ) = EPr</definiendum>
				<definiens id="0">assign a variable weight to each of the n-gram probabilities such that we have a weighted language probability as : W ( W ) = rr , _ , _ , _ _ , ~ , ( x , , , , , ,w , _ , , ... ) llr , tw~lw~-rw~-2 ... ) ( 1 ) i where the weight c~ 0 is a function of acoustic data , Xi , for wi , and words wi , Wi-l , ... . For a given sentence k , a very general objective function</definiens>
			</definition>
			<definition id="3">
				<sentence>( 2 ) where A denotes acoustic and language model parameters as well as language weights , Pr ( O ) denotes the a priori probability of the incorrect path 0 , and Pr ( Xi \ ] wi ) denotes acoustic probability generated by word model w~ .</sentence>
				<definiendum id="0">Pr ( O )</definiendum>
				<definiens id="0">acoustic and language model parameters as well as language weights</definiens>
				<definiens id="1">the a priori probability of the incorrect path 0 , and Pr ( Xi \ ] wi ) denotes acoustic probability generated by word model w~</definiens>
			</definition>
</paper>

		<paper id="1067">
			<definition id="0">
				<sentence>Specifically , the score is the probability of acoustic features of a hypothesized word sequence given an associated syntactic parse , based on acoustic and `` language '' ( prosody/syntax ) models that represent probabilities in terms of abstract prosodic labeis .</sentence>
				<definiendum id="0">score</definiendum>
				<definiens id="0">the probability of acoustic features of a hypothesized word sequence given an associated syntactic parse , based on acoustic and `` language '' ( prosody/syntax ) models that represent probabilities in terms of abstract prosodic labeis</definiens>
			</definition>
			<definition id="1">
				<sentence>Prosody , in particular , provides information about syntactic structure ( via prosodic constituent structure ) and information focus ( via phrasal prominence ) , and is encoded in the acoustic signal in terms of timing , energy and intonation patterns .</sentence>
				<definiendum id="0">Prosody</definiendum>
				<definiendum id="1">information focus</definiendum>
				<definiens id="0">via phrasal prominence ) , and is encoded in the acoustic signal in terms of timing , energy and intonation patterns</definiens>
			</definition>
			<definition id="2">
				<sentence>More specifically , the prosody-parse score is the probability of a sequence of acoustic observations x = { zl , ... , zn } given the hypothesized parse , p ( x\ [ parse ) , where x is a sequence of 335 duration and fO measurements associated with the recognizer output .</sentence>
				<definiendum id="0">prosody-parse score</definiendum>
				<definiens id="0">the probability of a sequence of acoustic observations x = { zl , ... , zn } given the hypothesized parse , p ( x\ [ parse )</definiens>
				<definiens id="1">a sequence of 335 duration and fO measurements associated with the recognizer output</definiens>
			</definition>
			<definition id="3">
				<sentence>The break model , for example , represents the probability distribution of the different breaks at a word boundary p ( blTAb ( Z ) ) , where TAb ( z ) is the terminal node of the acoustic break tree corresponding to observation z. Assuming the observations are conditionally independent given the breaks , the probability of the observation sequence is given by p ( x\ [ b ) = ~Ip ( z , lb , ) = p ( b , lTAb ( Z , ) ) p ( zl ) i=1 i=I p ( bi ) using the decision tree acoustic model .</sentence>
				<definiendum id="0">TAb ( z )</definiendum>
				<definiens id="0">the probability distribution of the different breaks at a word boundary p ( blTAb ( Z ) )</definiens>
				<definiens id="1">the terminal node of the acoustic break tree corresponding to observation z. Assuming the observations are conditionally independent given the breaks</definiens>
			</definition>
			<definition id="4">
				<sentence>The ATIS corpus includes speech from human subjects who were given a set of air travel planning `` scenarios '' to solve via spoken language communication with a computer .</sentence>
				<definiendum id="0">ATIS corpus</definiendum>
				<definiens id="0">includes speech from human subjects who were given a set of air travel planning `` scenarios '' to solve via spoken language communication with a computer</definiens>
			</definition>
			<definition id="5">
				<sentence>Queries made by the subjects are classified differently according to whether they are evaluable in isolation ( class A ) , require contextual information ( class D ) or having no canonical database answer ( class X ) , but these distinctions are ignored in our work .</sentence>
				<definiendum id="0">Queries</definiendum>
				<definiens id="0">made by the subjects are classified differently according to whether they are evaluable in isolation</definiens>
			</definition>
			<definition id="6">
				<sentence>Our experiments will not assess understanding accuracy , which is a function of the complete speech understanding system , but rather the rank of the correct answer after prosody/parse scoring .</sentence>
				<definiendum id="0">accuracy</definiendum>
				<definiens id="0">a function of the complete speech understanding system , but rather the rank of the correct answer after prosody/parse scoring</definiens>
			</definition>
			<definition id="7">
				<sentence>In addition , TINA falls back on a robust parsing mechanism when a complete parse is not found , using a combination of the basic parser and discourse processing mechanism ap337 plied within the utterance \ [ 14\ ] .</sentence>
				<definiendum id="0">TINA</definiendum>
			</definition>
			<definition id="8">
				<sentence>Scoring with Prosodic Information : An Analysis/Synthesis Approach , '' Computer Speech and Language , to appear 1993 .</sentence>
				<definiendum id="0">Prosodic Information</definiendum>
				<definiens id="0">An Analysis/Synthesis Approach</definiens>
			</definition>
			<definition id="9">
				<sentence>S. Seneff , `` TINA : A Natural Language System for Spoken Language Applications , '' J. Association for Computational Linguistics , pp .</sentence>
				<definiendum id="0">TINA</definiendum>
				<definiens id="0">A Natural Language System for Spoken Language Applications , '' J. Association for Computational Linguistics , pp</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>To achieve the first requirement , our explanation system uses an extended version of the text planner developed by Moore 1This category breaks up into number of subcategories in our taxonomy .</sentence>
				<definiendum id="0">explanation system</definiendum>
			</definition>
			<definition id="1">
				<sentence>When the user provides input to the system , the query analyzer interprets the question and forms a communicative goal representing the system 's intended effect on the hearer 's mental state , e.g. , `` achieve the state where the hearer believes that action A is suboptimal '' or `` achieve the state where the hearer knows about the side effects of drug X. '' A linear planner synthesizes responses to achieve these goals using a library of explanation operators that map communicafive goa~s to linguistic resources ( speech acts and rhetorical strategies ) for achieving them .</sentence>
				<definiendum id="0">query analyzer</definiendum>
				<definiens id="0">a communicative goal representing the system 's intended effect on the hearer 's mental state</definiens>
			</definition>
			<definition id="2">
				<sentence>SHERLOCK is an intelligent training system that teaches avionics technicians to troubleshoot complex electronic equipment .</sentence>
				<definiendum id="0">SHERLOCK</definiendum>
				<definiens id="0">an intelligent training system that teaches avionics technicians to troubleshoot complex electronic equipment</definiens>
			</definition>
			<definition id="3">
				<sentence>The tutor expects the student to be able to make use of the explanations given in turn 6 ( and therefore turn 3 ) by indicating that it is relevant to the current situation ( `` for the same reasons given ... '' serves this purpose ) .</sentence>
				<definiendum id="0">tutor</definiendum>
				<definiens id="0">expects the student to be able to make use of the explanations given in turn 6 ( and therefore turn 3 ) by indicating that it is relevant to the current situation ( `` for the same reasons given ... '' serves this purpose )</definiens>
			</definition>
			<definition id="4">
				<sentence>Using an algorithm described below , the system determines that in this case , the action of testing pin 38 is similar to the test of pin 28 and there is an explanation ( turn 3 ) satisfying the communicative goal ( BEL H ( SUBOPTIMAL-STEP 2Following COrosz and $ idner \ [ 8\ ] , in our model intentions are the basic determiner of segmentation , and therefore each communicative goal indicates a segment boundary .</sentence>
				<definiendum id="0">intentions</definiendum>
				<definiens id="0">similar to the test of pin 28 and there is an explanation ( turn 3 ) satisfying the communicative goal</definiens>
			</definition>
			<definition id="5">
				<sentence>Inderal is a drug that is used for prophylactic Ireatment of migraine .</sentence>
				<definiendum id="0">Inderal</definiendum>
				<definiens id="0">a drug that is used for prophylactic Ireatment of migraine</definiens>
			</definition>
			<definition id="6">
				<sentence>Associated with each facet is an indication of whether that facet contributes to a good ( + ) , bad ( - ) , or neuWal ( n ) evaluation in the current problem-solving context .</sentence>
				<definiendum id="0">neuWal</definiendum>
				<definiens id="0">an indication of whether that facet contributes to a good ( + ) , bad ( - ) , or</definiens>
			</definition>
			<definition id="7">
				<sentence>Treating each student action as a `` case '' , the algorithm builds a similarity DA G representing apartial ordering of actions based on the similarity of each action to a given action .</sentence>
				<definiendum id="0">student action</definiendum>
				<definiens id="0">a similarity DA G representing apartial ordering of actions based on the similarity of each action to a given action</definiens>
			</definition>
			<definition id="8">
				<sentence>The root of the DAG represents the current action and the facets that apply to it .</sentence>
				<definiendum id="0">DAG</definiendum>
				<definiens id="0">the current action</definiens>
			</definition>
			<definition id="9">
				<sentence>The algorithm is both efficient ( complexity O ( n 2 ) where n is the number of student actions ) and accurate .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of student actions</definiens>
			</definition>
</paper>

		<paper id="1088">
</paper>

		<paper id="1092">
</paper>

		<paper id="1035">
</paper>

		<paper id="1105">
</paper>

		<paper id="1002">
</paper>

		<paper id="1060">
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>For each candidate , the parsing model provides P ( TIc ) , the probability of the candidate being a true phrase , and P ( ~TIc ) = 1 P ( TIc ) .</sentence>
				<definiendum id="0">P ( TIc</definiendum>
				<definiens id="0">the probability of the candidate being a true phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>Model XK takes the category and length of candidates into account .</sentence>
				<definiendum id="0">Model XK</definiendum>
				<definiens id="0">takes the category and length of candidates into account</definiens>
			</definition>
			<definition id="2">
				<sentence>Model C is a variant of model G , in which a small amount of context , namely , the following part of speech , is also taken into account .</sentence>
				<definiendum id="0">Model C</definiendum>
				<definiens id="0">a variant of model G , in which a small amount of context</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>The score of an SG parse , which will be described in Section 4 , is the sum of several components .</sentence>
				<definiendum id="0">score of an SG parse</definiendum>
				<definiens id="0">the sum of several components</definiens>
			</definition>
			<definition id="1">
				<sentence>Grammatical analysis of a phrase consists basically of choosing , for each word of the phrase , ( 1 ) a word sense , ( 2 ) a feature structure , and ( 3 ) filler subphrases for its slots .</sentence>
				<definiendum id="0">Grammatical analysis of a phrase</definiendum>
			</definition>
			<definition id="2">
				<sentence>A goal hf ( Feat ) tests the feature structure of the higher phrasethe phrase ( with possibly other modifiers attached ) with which the slot is associated .</sentence>
				<definiendum id="0">Feat )</definiendum>
				<definiens id="0">tests the feature structure of the higher phrasethe phrase ( with possibly other modifiers attached ) with which the slot is associated</definiens>
			</definition>
			<definition id="3">
				<sentence>The lexicon for ESG consists of a hand-coded portion for approximately 6,000 lemmas ( basically most frequent words ) , plus a large back-up lexicon of approximately 60,000 lemmas derived from UDICT \ [ 1 , 7\ ] and other sources .</sentence>
				<definiendum id="0">lexicon for ESG</definiendum>
			</definition>
			<definition id="4">
				<sentence>The special goal st ( Type ) says that Type is a semantic type of the ( sense of ) the filler phrase , and ira is a semantic type used in ESG for time words .</sentence>
				<definiendum id="0">Type</definiendum>
				<definiendum id="1">ira</definiendum>
				<definiens id="0">a semantic type of the ( sense of ) the filler phrase , and</definiens>
				<definiens id="1">a semantic type used in ESG for time words</definiens>
			</definition>
			<definition id="5">
				<sentence>The ingredient PXSG represents a contribution from language-specific coordination rules in XSG .</sentence>
				<definiendum id="0">PXSG</definiendum>
				<definiens id="0">a contribution from language-specific coordination rules in XSG</definiens>
			</definition>
			<definition id="6">
				<sentence>An approximate parse is a non-perfect one for which nevertheless all the feature structures are correct and surface structureis correct except for level of attachment of modifiers .</sentence>
				<definiendum id="0">approximate parse</definiendum>
				<definiens id="0">a non-perfect one for which nevertheless all the feature structures are correct and surface structureis correct except for level of attachment of modifiers</definiens>
			</definition>
			<definition id="7">
				<sentence>, McCord , M. , Sowa , J. F. , and Wilson , W. G. , Knowledge Systems and Prolog : A Logical Approach to Expert Systems and Natural Language Processing , Addison-Wesley , Reading , Mass. , 1987 .</sentence>
				<definiendum id="0">Prolog</definiendum>
				<definiens id="0">A Logical Approach to Expert Systems and Natural Language Processing , Addison-Wesley , Reading , Mass. , 1987</definiens>
			</definition>
</paper>

		<paper id="1104">
</paper>

		<paper id="1095">
			<definition id="0">
				<sentence>The MIT spoken language system combines SUMMIT , a segment-based speech recognition system , and TINA , a probabilistic natural language system , to achieve speech understanding .</sentence>
				<definiendum id="0">MIT spoken language system combines SUMMIT</definiendum>
				<definiendum id="1">TINA</definiendum>
				<definiens id="0">a probabilistic natural language system , to achieve speech understanding</definiens>
			</definition>
</paper>

		<paper id="1089">
			<definition id="0">
				<sentence>PROJECT GOALS The TIPSTER/SHOGUN project aims at substantive improvements in coverage and accuracy for automatic data extraction through innovative strategies in knowledge acquisition , run-time integration , and control .</sentence>
				<definiendum id="0">TIPSTER/SHOGUN project</definiendum>
				<definiens id="0">aims at substantive improvements in coverage and accuracy for automatic data extraction through innovative strategies in knowledge acquisition , run-time integration , and control</definiens>
			</definition>
			<definition id="1">
				<sentence>Data extraction systems interpret the key content of natural language text , producing a structured representation of items that range from high-level business relationships to detailed knowledge coding of technologies and industry classifications .</sentence>
				<definiendum id="0">Data extraction systems</definiendum>
				<definiens id="0">the key content of natural language text , producing a structured representation of items that range from high-level business relationships to detailed knowledge coding of technologies and industry classifications</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>the coarse mateher is a complete ( simple ) speech recognition system .</sentence>
				<definiendum id="0">coarse mateher</definiendum>
				<definiens id="0">a complete ( simple ) speech recognition system</definiens>
			</definition>
			<definition id="1">
				<sentence>The test set is a difficult 20-sentence subset of one of the development sets .</sentence>
				<definiendum id="0">test set</definiendum>
				<definiens id="0">a difficult 20-sentence subset of one of the development sets</definiens>
			</definition>
</paper>

		<paper id="1030">
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>The N-best Paradigm is a type of fast match at the sentence level .</sentence>
				<definiendum id="0">N-best Paradigm</definiendum>
				<definiens id="0">a type of fast match at the sentence level</definiens>
			</definition>
			<definition id="1">
				<sentence>We developed the Forward-Backward Search ( FBS ) algorithm in 1986 as a way to greatly reduce the computation needed to search a large language model .</sentence>
				<definiendum id="0">Forward-Backward Search</definiendum>
				<definiens id="0">a way to greatly reduce the computation needed to search a large language model</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>Evaluation has a significant cost : the comparator-based method requires the deftnition of a training corpus and its collection , defining principles of interpretation , and ( most expensively ) the annotation of data .</sentence>
				<definiendum id="0">Evaluation</definiendum>
				<definiens id="0">the annotation of data</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>INTRODUCTION FAS'rUS is a ( slightly permuted ) acronym for Finite State Automaton Text Understanding System .</sentence>
				<definiendum id="0">INTRODUCTION FAS'rUS</definiendum>
				<definiens id="0">a ( slightly permuted ) acronym for Finite State Automaton Text Understanding System</definiens>
			</definition>
			<definition id="1">
				<sentence>( Recall is percent of the possible answers the system got correct ; precision is percent of the system 's answers that were correct . )</sentence>
				<definiendum id="0">Recall</definiendum>
			</definition>
			<definition id="2">
				<sentence>A Tokenizer Phase has been added , its input consists of ascii characters and it output is tokens , usually words , numerals , and punctuation lnarks .</sentence>
				<definiendum id="0">Tokenizer Phase</definiendum>
				<definiens id="0">tokens , usually words , numerals , and punctuation lnarks</definiens>
			</definition>
			<definition id="3">
				<sentence>The incident recognition phase ( phase IV ) recognizes those ut , tera .</sentence>
				<definiendum id="0">incident recognition phase ( phase IV )</definiendum>
			</definition>
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>l the form head syntactic-relation arg where arg is the head of the argument or modifier .</sentence>
				<definiendum id="0">arg</definiendum>
				<definiens id="0">the head of the argument or modifier</definiens>
			</definition>
			<definition id="1">
				<sentence>Co-occurrence smoothing is a method which has been recently proposed for smoothing n-gram models \ [ 4\ ] .3 The core of this method involves the computation of a co-occurrence matrix ( a matrix of confusion probabilities ) Pc ( wj \ ] wi ) , which indicates the probability of word wj occurring in contexts in which word wi occurs , averaged over these contexts .</sentence>
				<definiendum id="0">Co-occurrence smoothing</definiendum>
			</definition>
			<definition id="2">
				<sentence>r ( &lt; &gt; ) In order to avoid the generation of confusion table entries from a single shared context ( which quite often is the result of an incorrect parse ) , we apply a filter in generating Pc : for i ¢ j , we generate a non-zero Pc ( wj Iwi ) only if the wi and wj appear in at least two common contexts , and there is some common context in which both words occur at least 3We wish to thank Richard Schwartz of BBN for referring us to this method and article .</sentence>
				<definiendum id="0">non-zero Pc</definiendum>
				<definiens id="0">at least 3We wish to thank Richard Schwartz of BBN for referring us to this method and article</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>SEMANTICS is entirely language-independent , and DISCOURSE isolates the small amount of languagespecific information to the discourse data module ( i.e. discourse knowledge sources ) .</sentence>
				<definiendum id="0">DISCOURSE</definiendum>
			</definition>
			<definition id="1">
				<sentence>The new analyzer consists of a morphological processing engine and morphological data for each language , as shown in Figure 2 .</sentence>
				<definiendum id="0">new analyzer</definiendum>
				<definiens id="0">consists of a morphological processing engine and morphological data for each language</definiens>
			</definition>
			<definition id="2">
				<sentence>A few basic X-bar rules for head-initial ( e.g. English and Spanish ) and head-final languages ( e.g. Japanese ) are shown in Figure 4.1 The output of the parser is a structure called a functionally labeled template ( FIX ) , which is similar to LFG 's f-structure .</sentence>
				<definiendum id="0">few basic X-bar rules for head-initial</definiendum>
				<definiendum id="1">FIX</definiendum>
				<definiens id="0">a structure called a functionally labeled template (</definiens>
			</definition>
			<definition id="3">
				<sentence>The discourse knowledge source KB consists of generators ( i.e. various ways to generate antecedent hypotheses ) , filters ( e.g. syntactic number filter , syntactic gender filter , semantic amount filter , semantic gender filter , semantic type filter , etc. ) , and orderers ( e.g. focus orderer , recency orderer , etc. ) .</sentence>
				<definiendum id="0">discourse knowledge source KB</definiendum>
				<definiens id="0">consists of generators ( i.e. various ways to generate antecedent hypotheses ) , filters ( e.g. syntactic number filter , syntactic gender filter , semantic amount filter , semantic gender filter , semantic type filter , etc. ) , and orderers ( e.g. focus orderer , recency orderer , etc. )</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>ABSTRACT This paper presents the Semantic Linker , the fallbaek component used by the the DELPHI natural language component of the BBN spoken language system HARC .</sentence>
				<definiendum id="0">Semantic Linker</definiendum>
				<definiens id="0">the fallbaek component used by the the DELPHI natural language component of the BBN spoken language system HARC</definiens>
			</definition>
			<definition id="1">
				<sentence>The Semantic Linker is invoked when DELPHI 's regular chart-based unification grammar parser is unable to parse an input ; it attempts to come up with a semantic interpretation by combining the fragmentary sub-parses left over in the chart using a domain-independent method incorporating general search algorithm driven by empirically determined probabilities and parameter weights .</sentence>
				<definiendum id="0">Semantic Linker</definiendum>
				<definiens id="0">attempts to come up with a semantic interpretation by combining the fragmentary sub-parses left over in the chart using a domain-independent method incorporating general search algorithm driven by empirically determined probabilities and parameter weights</definiens>
			</definition>
			<definition id="2">
				<sentence>This paper presents the Semantic Linker , the fallback component used by the the DELPHI natural language component of the BBN spoken language system HARC .</sentence>
				<definiendum id="0">Semantic Linker</definiendum>
				<definiens id="0">the fallback component used by the the DELPHI natural language component of the BBN spoken language system HARC</definiens>
			</definition>
			<definition id="3">
				<sentence>The Semantic Linker is invoked when DELPHI 's regular chart-based unification grammar parser is unable to parse an input ; it attempts to come up with a semantic interpretation by combining the fragmentary sub-parses left over in the chart .</sentence>
				<definiendum id="0">Semantic Linker</definiendum>
				<definiens id="0">attempts to come up with a semantic interpretation by combining the fragmentary sub-parses left over in the chart</definiens>
			</definition>
			<definition id="4">
				<sentence>PROBABILITIES The Semantic Linker first computes the link database , which is the set of all possible links between all pairs of objects in all pairs of different fragments .</sentence>
				<definiendum id="0">Semantic Linker</definiendum>
				<definiendum id="1">link database</definiendum>
				<definiens id="0">the set of all possible links between all pairs of objects in all pairs of different fragments</definiens>
			</definition>
			<definition id="5">
				<sentence>The most important is the relational probability of the link , or : P ( r , Cl , C2 ) where r is the semantic relation of the link and C1 and C2 are semantic classes of the two argument positions , where C2 may be tagged by a preposition .</sentence>
				<definiendum id="0">most important</definiendum>
				<definiendum id="1">r</definiendum>
				<definiens id="0">the semantic relation of the link and C1 and C2 are semantic classes of the two argument positions</definiens>
			</definition>
			<definition id="6">
				<sentence>If there is more than one success state , the Linker picks the the subset of them with the highest score .</sentence>
				<definiendum id="0">Linker</definiendum>
				<definiens id="0">picks the the subset of them with the highest score</definiens>
			</definition>
			<definition id="7">
				<sentence>Basically , there are a number of heuristics , including whether the determiner of a nominal object is WH , whether the sort of the the nominal is a `` priority '' domain ( in ATIS , GROUNDTRANSPORTATION is such a domain ) , and whether the nominal occurs only has the second argument of the triples in which it occurs ( making it an unconstrained nominal ) .</sentence>
				<definiendum id="0">GROUNDTRANSPORTATION</definiendum>
				<definiens id="0">including whether the determiner of a nominal object is WH , whether the sort of the the nominal is a `` priority '' domain</definiens>
			</definition>
			<definition id="8">
				<sentence>Fragment Processing in the DELPHI System Proceedings Speech and Natural Language Workshop February 1992 A Relaxation Method for Understanding Spontaneous Speech Utterances Proceedings Speech and Natural Language Workshop February 1992 A. A Portable Approach to Last Resort Parsing and Interpretation ( this volume ) A Template Marcher for Robust NL Interpretation Proceedings Speech and Natural Language Workshop February 1991 Statistical Agenda Parsing Proceedings Speech and Natural Language Workshop February 1991 Benchmark Tests for the Spoken Language Program ( this volume ) 42</sentence>
				<definiendum id="0">Fragment Processing</definiendum>
				<definiendum id="1">Language Program</definiendum>
				<definiens id="0">in the DELPHI System Proceedings Speech and Natural Language Workshop February 1992 A Relaxation Method for Understanding Spontaneous Speech Utterances Proceedings Speech and Natural Language Workshop February 1992 A. A Portable Approach to Last Resort Parsing and Interpretation ( this volume ) A Template Marcher for Robust NL Interpretation Proceedings Speech and Natural Language Workshop February 1991 Statistical Agenda Parsing Proceedings Speech and Natural Language Workshop February 1991 Benchmark Tests for the Spoken</definiens>
			</definition>
</paper>

		<paper id="1011">
</paper>

		<paper id="1055">
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>MFCDCN is an extension of a similar algorithm , FCDCN , which provides an additive environmental compensation to cepstral vectors , but in an environment-specific fashion \ [ 5\ ] .</sentence>
				<definiendum id="0">MFCDCN</definiendum>
			</definition>
			<definition id="1">
				<sentence>Multiple fixed codeword-dependent cepstral normalization ( MFCDCN ) is a simple extension to the FCDCN algorithm , with the goal of exploiting the simplicity and effectiveness of FCDCN but without the need for environment-specific training .</sentence>
				<definiendum id="0">MFCDCN</definiendum>
				<definiens id="0">a simple extension to the FCDCN algorithm , with the goal of exploiting the simplicity and effectiveness of FCDCN but without the need for environment-specific training</definiens>
			</definition>
			<definition id="2">
				<sentence>CDCN uses EM techniques to compute ML estimates of the parameters characterizing the contributions of additive noise and linear filtering that when applied in inverse fashion to the cepstra of an incoming utterance produce an ensemble of cepstral coefficients that best match ( in the ML sense ) the cepstral coefficients of the incoming speech in the testing environment to the locations of VQ codewords in the training environment .</sentence>
				<definiendum id="0">CDCN</definiendum>
				<definiens id="0">uses EM techniques to compute ML estimates of the parameters characterizing the contributions of additive noise and linear filtering that when applied in inverse fashion to the cepstra of an incoming utterance produce an ensemble of cepstral coefficients that best match ( in the ML sense ) the cepstral coefficients of the incoming speech in the testing environment to the locations of VQ codewords in the training environment</definiens>
			</definition>
			<definition id="3">
				<sentence>The SRI DECIPHER TM system , for example , uses the highpass filter described by the difference equation y\ [ n\ ] = x\ [ n\ ] -x\ [ n1\ ] +0.97y\ [ n1\ ] where x \ [ n\ ] and y \ [ n\ ] are the time-varying cepstral vectors of the utterance before and after RASTA filtering , and the index n refers to the analysis frames \ [ 11\ ] .</sentence>
				<definiendum id="0">SRI DECIPHER TM system</definiendum>
				<definiens id="0">uses the highpass filter described by the difference equation y\</definiens>
			</definition>
			<definition id="4">
				<sentence>Cepstral mean normalization ( CMN ) is an alternate way to high-pass filter cepstral coefficients .</sentence>
				<definiendum id="0">Cepstral mean normalization</definiendum>
				<definiendum id="1">CMN</definiendum>
				<definiens id="0">an alternate way to high-pass filter cepstral coefficients</definiens>
			</definition>
			<definition id="5">
				<sentence>The trigram grammar for the system was derived from 70.0 million words of text without verbalized punctuation and 11.6 million words with verbalized punctuation .</sentence>
				<definiendum id="0">trigram grammar</definiendum>
				<definiens id="0">derived from 70.0 million words of text without verbalized punctuation</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>Plan recognition is an essential part of any dialogue system .</sentence>
				<definiendum id="0">Plan recognition</definiendum>
			</definition>
			<definition id="1">
				<sentence>The connecting path ( double arrows ) indicates that moving the engine is done to move a car that will contain the oranges , thus moving them .</sentence>
				<definiendum id="0">connecting path</definiendum>
				<definiens id="0">double arrows ) indicates that moving the engine is done to move a car that will contain the oranges</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>LINGSTAT ( Dragon Systems Inc. ) is a computer-aided translation environment in which a knowledgeable nonexpert can compose English translations of Japanese by using a variety of contextual cues with word parsing and character interpretation aids ( Bamberg 1992 ) .</sentence>
				<definiendum id="0">LINGSTAT</definiendum>
				<definiens id="0">a computer-aided translation environment in which a knowledgeable nonexpert can compose English translations of Japanese by using a variety of contextual cues with word parsing and character interpretation aids</definiens>
			</definition>
			<definition id="1">
				<sentence>Performance for each of the systems scored was computed by averaging the fragment ( or sentence ) score over all fragments ( or sentences ) , passages , and test subjects .</sentence>
				<definiendum id="0">score over all fragments</definiendum>
				<definiens id="0">or sentences ) , passages , and test subjects</definiens>
			</definition>
			<definition id="2">
				<sentence>Candide HAMT receives the highest scores for adequacy , fluency and quality ; Systran Japanese FAMT receives the lowest .</sentence>
				<definiendum id="0">Candide HAMT</definiendum>
				<definiens id="0">receives the highest scores for adequacy , fluency and quality ; Systran Japanese FAMT receives the lowest</definiens>
			</definition>
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>Verbs , for example , derive more disambiguating information from their objects ( .95 ) than from their subjects ( .90 ) .</sentence>
				<definiendum id="0">Verbs</definiendum>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>V : userknows ( V ) , voiceinteracfion ( observe , V ) which is also shown in Figure 1 .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">userknows ( V ) , voiceinteracfion ( observe , V ) which is also shown in Figure 1</definiens>
			</definition>
			<definition id="1">
				<sentence>Control of Mixed-Initiative Discourse Through Meta-Locutionary Acts : A Computational Model .</sentence>
				<definiendum id="0">Mixed-Initiative Discourse Through Meta-Locutionary Acts</definiendum>
				<definiens id="0">A Computational Model</definiens>
			</definition>
			<definition id="2">
				<sentence>TINA : A Natural Language System for Spoken LanguageApplicafions .</sentence>
				<definiendum id="0">TINA</definiendum>
			</definition>
</paper>

		<paper id="1098">
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>The TOBI ( Tones and Break Indices ) system for American English \ [ 3\ ] is a prosodic transcription system that has evolved from a series of workshops where researchers met with the goal of defining a common core of transcription labels .</sentence>
				<definiendum id="0">TOBI ( Tones</definiendum>
				<definiens id="0">a prosodic transcription system that has evolved from a series of workshops where researchers met with the goal of defining a common core of transcription labels</definiens>
			</definition>
			<definition id="1">
				<sentence>Silverman attacks the problem of predicting abstract prosodic labels , while van Santen presents a model for predicting duration from text ( and optionally abstract labels ) .</sentence>
				<definiendum id="0">Silverman</definiendum>
				<definiens id="0">attacks the problem of predicting abstract prosodic labels</definiens>
			</definition>
			<definition id="2">
				<sentence>Prosody can also serve speech understanding systems in an entirely different way , as discussed in the paper by Nakatani and Hirschberg , which is to cue the presence of a disfluency and the interval of replacement .</sentence>
				<definiendum id="0">Hirschberg</definiendum>
				<definiens id="0">to cue the presence of a disfluency and the interval of replacement</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>The i-o algorithm is an extension of the finite-state based Hidden Markov Model ( by \ [ 1\ ] ) , which has been applied successfully in many areas , including speech recognition and part of speech tagging .</sentence>
				<definiendum id="0">i-o algorithm</definiendum>
				<definiens id="0">an extension of the finite-state based Hidden Markov Model ( by \ [ 1\ ] ) , which has been applied successfully in many areas , including speech recognition and part of speech tagging</definiens>
			</definition>
			<definition id="1">
				<sentence>If we wish to delete a left paren to the right of constituent X 4 , where X appears in a subtree of the form : X A YY Z 4To the fight of the rightmost terminal dominated by X if X is a nonterminal .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a nonterminal</definiens>
			</definition>
			<definition id="2">
				<sentence>Accuracy is measured in terms of the percentage of noncrossing constituents in the test corpus , as described above .</sentence>
				<definiendum id="0">Accuracy</definiendum>
				<definiens id="0">measured in terms of the percentage of noncrossing constituents in the test corpus , as described above</definiens>
			</definition>
</paper>

		<paper id="1103">
</paper>

		<paper id="1093">
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>I-I ) -'\ [ S2 , n+l ( j~+1 ) ( i ) Here , fj is the j-th component s of the vector f , the second subscript ( j ) in s~ , j likewise refers to this component , and the first subscript ( i ) refers to the fact that the model consists of twoproduct terms numbered 1 and 2 .</sentence>
				<definiendum id="0">fj</definiendum>
				<definiens id="0">the j-th component s of the vector f</definiens>
				<definiens id="1">the fact that the model consists of twoproduct terms numbered 1 and 2</definiens>
			</definition>
			<definition id="1">
				<sentence>~,3 ( s ) + .2 , ~ ( c ) x .2,2 ( P ) ( 2 ) Here , C is consonant class , P place of articulation , and S stress condition ; it is assumed that factor scales have positive values only .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">consonant class</definiens>
			</definition>
			<definition id="2">
				<sentence>The overall correlation ( over all 41,588 segments ) between observed and predicted durations was 0.93 ( 0.90 , 0.90 , and 7The number of distinct models converges to 2 2'~ -1 _ 1 , where n is the number of factors .</sentence>
				<definiendum id="0">overall correlation</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the number of factors</definiens>
			</definition>
			<definition id="3">
				<sentence>The old module consists of a list of several hundred duration rules similar to , but somewhat simpler than , the Klatt rules \ [ 5\ ] .</sentence>
				<definiendum id="0">old module</definiendum>
			</definition>
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>spoken written function p ( PRO ) subject .71 ( N=2077 ) non-subject .16 ( N=1477 ) subject .44 ( N=1195 ) non-subject .09 ( N=i088 ) Table 2 : Subject and non-subject noun phrases Dependency Grammars Dependency grammars naturally address part of the mismatch between CFG 's and predictive associations , since they are expressed in terms of relations between words ( Melcuk 1988 ) , Nevertheless , in dependency grammars as well , certain syntactic relationships are problematic .</sentence>
				<definiendum id="0">PRO</definiendum>
				<definiens id="0">Subject and non-subject noun phrases Dependency Grammars Dependency grammars naturally address part of the mismatch between CFG 's and predictive associations</definiens>
			</definition>
			<definition id="1">
				<sentence>Our use of language seems to involve the composition of variably sized partially described units expressed in terms of a variety of predicates ( only some of which are included in our database ) .</sentence>
				<definiendum id="0">language</definiendum>
				<definiens id="0">seems to involve the composition of variably sized partially described units expressed in terms of a variety of predicates</definiens>
			</definition>
			<definition id="2">
				<sentence>We 'll label this measure IS , where IS ( w ; C ) = E Pr ( wlC ) loa Pr ( wlC ) w P ( w ) In the course of processing sentence ( 1 ) , we need an estimate of Pr ( between\ ] put banks ) .</sentence>
				<definiendum id="0">IS</definiendum>
				<definiens id="0">an estimate of Pr ( between\ ] put banks )</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>During training , AMED aligns the parallel annotations , identifies deviations as `` corrections '' , and automatically generalizes these into correction rules .</sentence>
				<definiendum id="0">AMED</definiendum>
				<definiens id="0">aligns the parallel annotations , identifies deviations as `` corrections ''</definiens>
			</definition>
			<definition id="1">
				<sentence>The third component is POST , which assigns parts of speech stochastically via a Hidden Markov model , has been described elsewhere \ [ Meteer , et al. , 1991\ ] .</sentence>
				<definiendum id="0">POST</definiendum>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>DR systems illustrate every variety of indexing language , request and document description , and search mechanism .</sentence>
				<definiendum id="0">DR systems</definiendum>
				<definiens id="0">illustrate every variety of indexing language , request and document description , and search mechanism</definiens>
			</definition>
			<definition id="1">
				<sentence>Controlled languages ( CLs ) have been commonly used , across the range from only slightly restricted natural language ( NL ) to a carefully designed artificial language .</sentence>
				<definiendum id="0">Controlled languages ( CLs</definiendum>
				<definiens id="0">the range from only slightly restricted natural language ( NL ) to a carefully designed artificial language</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>First , texts are labeled by our probabilistic part of speech tagger ( POST ) which has been extended for Japanese morphological processing \ [ Matsukawa et .</sentence>
				<definiendum id="0">POST</definiendum>
				<definiens id="0">probabilistic part of speech tagger</definiens>
			</definition>
			<definition id="1">
				<sentence>The effect of domain-specific lexical entries ( e.g. , DRAM is a noun in microelectronics ) often mitigates the need to retrain .</sentence>
				<definiendum id="0">DRAM</definiendum>
				<definiens id="0">a noun in microelectronics</definiens>
			</definition>
			<definition id="2">
				<sentence>To do this , we use JUMAN from Kyoto University to segment Japanese text into words , AMED , an example-based segmentation corrector , and a Hidden Markov Model ( POST ) \ [ Matsukawa , et .</sentence>
				<definiendum id="0">AMED</definiendum>
			</definition>
			<definition id="3">
				<sentence>Sainflection nouns ( SN ) are nominalized verbs which form a verb phrase with the morpheme `` suru . ''</sentence>
				<definiendum id="0">Sainflection nouns</definiendum>
				<definiendum id="1">SN</definiendum>
			</definition>
			<definition id="4">
				<sentence>According to \ [ Hoel 1971\ ] , the likelihood ratio LAMBDA for a test of the hypothesis : p ( i ) = po ( i ) ( i = 1 , 2 ... .. k ) , where p ( i ) is the probability of case i and po ( i ) is a hypothesized probability of it , when observations are independent of each other , is given as : k , n ( i ) -2 log LAMBDA 2 ~ n ( i ) ( 1 ) = log~ i=l where n ( i ) is the number of observations of case i , and e ( i ) is its expectation , i.e. , e ( i ) = n p ( i ) , where n is the total number of observations .</sentence>
				<definiendum id="0">p ( i )</definiendum>
				<definiendum id="1">n ( i )</definiendum>
				<definiendum id="2">n</definiendum>
				<definiens id="0">the total number of observations</definiens>
			</definition>
			<definition id="5">
				<sentence>The maximum likelihood estimate of p ( ci ) and p ( cj ) is f ( ci ) /n and f ( cj ) /n , where f ( cj ) and f ( cj ) are the number of observations of words classified in ci and cj .</sentence>
				<definiendum id="0">f</definiendum>
				<definiendum id="1">f</definiendum>
				<definiens id="0">the number of observations of words classified in ci and cj</definiens>
			</definition>
			<definition id="6">
				<sentence>The maximum likelihood estimate of p ( ci , cj ) , the probability of the co-occurrences of words in ci and cj , is f ( ci , cj ) /n , where f ( ci , cj ) is the number of observations of the co-occurrences .</sentence>
				<definiendum id="0">f ( ci , cj )</definiendum>
			</definition>
</paper>

		<paper id="1114">
</paper>

		<paper id="1086">
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>JANUS is a speaker-independent system translating spoken utterances in English and also in German into one of German , English or Japanese .</sentence>
				<definiendum id="0">JANUS</definiendum>
				<definiens id="0">a speaker-independent system translating spoken utterances in English and also in German into one of German</definiens>
			</definition>
			<definition id="1">
				<sentence>The Generalized LR parsing algorithm is an extension of LR parsing with the special device called `` Graph-Structured Stack '' \ [ 14\ ] , and it can handle arbitrary context-free grammars while most of the LR efficiency is preserved .</sentence>
				<definiendum id="0">Generalized LR parsing algorithm</definiendum>
				<definiens id="0">an extension of LR parsing with the special device called `` Graph-Structured Stack '' \ [ 14\ ] , and it can handle arbitrary context-free grammars while most of the LR efficiency is preserved</definiens>
			</definition>
			<definition id="2">
				<sentence>F. D. Bu¢ , A learnable connectionist parser that outputs fstructures ( working title ) , PhD-Thesis proposal , University of Karlsruhe , in preparation .</sentence>
				<definiendum id="0">learnable connectionist parser</definiendum>
				<definiendum id="1">Karlsruhe</definiendum>
				<definiens id="0">outputs fstructures ( working title ) , PhD-Thesis proposal</definiens>
			</definition>
			<definition id="3">
				<sentence>Jain , A. Waibel , D. Touretzky , PARSEC : A Structured Connectionist Parsing System for Spoken Language , ICASSP 1992 , volume 1 , pp 205-208 .</sentence>
				<definiendum id="0">PARSEC</definiendum>
				<definiens id="0">A Structured Connectionist Parsing System for Spoken Language</definiens>
			</definition>
</paper>

		<paper id="1109">
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>We use the maximum entropy ( ME ) principle ( \ [ 4 , 5\ ] ) , which can be summarized as follows : expectation of various functions , to be satisfied by the target ( combined ) estimate .</sentence>
				<definiendum id="0">maximum entropy</definiendum>
				<definiens id="0">can be summarized as follows : expectation of various functions , to be satisfied by the target ( combined ) estimate</definiens>
			</definition>
			<definition id="1">
				<sentence>Assume we have identified for each word w in a vocabulary , V , a set of nw trigger words tw~ t~ ... t~ , , ; we further assume that we have the relative frequency of observing a trigger word , t , occurring somewhere in the history , h , ( in our case we have used a history length , K , of either 25 , 50 , 200 , or 1000 words ) and the word w just occurs after the history from some training text ; denote the observed relative frequency of a trigger and a word w by c ( t E h and w immediatelyf ollows h ) d ( t , w ) = N where c ( . )</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">a history length , K , of either 25 , 50 , 200 , or 1000 words ) and the word w just occurs after the history from some training text ; denote the observed relative frequency of a trigger and a word w by c ( t E h and w immediatelyf ollows h</definiens>
			</definition>
			<definition id="2">
				<sentence>Using Lagrange multipliers , one can easily show that the Maximum Entropy model is given by : p ( h , w ) = H ItWt t : tE h i.e. , the joint probability is the product of lh ( W ) factors one factor for each trigger t , ~ of word w that occurs in the history h ( or one factor if none of the triggers occur . )</sentence>
				<definiendum id="0">joint probability</definiendum>
				<definiens id="0">the product of lh ( W ) factors one factor for each trigger t , ~ of word w that occurs in the history h ( or one factor if none of the triggers occur</definiens>
			</definition>
			<definition id="3">
				<sentence>Then the update is given by : d ( t , w ) P~ ' = P~llm ( t , W ) where the model predicted value m ( t , w ) is given by : m ( t , w ) = E P°lt ( h ' w ) ( I ) h : tE h where pOll uses the old factor values .</sentence>
				<definiendum id="0">pOll</definiendum>
				<definiens id="0">uses the old factor values</definiens>
			</definition>
			<definition id="4">
				<sentence>i ) ~i where lh ( w ) is the set of triggers for word w that occur in h. The convexity of the log likelihood guarantees that any hill climbing method will converge to the global optimum .</sentence>
				<definiendum id="0">lh ( w )</definiendum>
				<definiens id="0">the set of triggers for word w</definiens>
			</definition>
			<definition id="5">
				<sentence>B\ ] ( 5 ) The ML/ME Solution Generalized Iterative Scaling can be used to find the ME estimate of a simple ( non-conditional ) probability distribution over some event space .</sentence>
				<definiendum id="0">ML/ME Solution Generalized Iterative Scaling</definiendum>
				<definiens id="0">used to find the ME estimate of a simple ( non-conditional ) probability distribution over some event space</definiens>
			</definition>
			<definition id="6">
				<sentence>Let P ( h , w ) be the desired probability estimate , and let P ( h , w ) be the empirical distribution of the training data .</sentence>
				<definiendum id="0">w</definiendum>
				<definiendum id="1">w )</definiendum>
				<definiens id="0">the empirical distribution of the training data</definiens>
			</definition>
			<definition id="7">
				<sentence>Instead of constraining the expectation offi ( h , w ) with regard to P ( h , w ) , we constrain its expectation with regard to a different probability distribution , say Q ( h , w ) , whose conditional Q ( wlh ) is the same as that of P , but whose marginal Q ( h ) is the same as that of P. To better understand the effect of this change , define H as the set of all possible histories h , and define Hi , as the partition of H induced byfi .</sentence>
				<definiendum id="0">expectation offi</definiendum>
				<definiendum id="1">Q ( h )</definiendum>
				<definiens id="0">the partition of H induced byfi</definiens>
			</definition>
			<definition id="8">
				<sentence>The unique ME solution that satisfies equations like ( 7 ) or ( 6 ) can be shown to also be the Maximum Likelihood ( ML ) solution , namely that function which , among the exponential family defined by the constraints , has the maximum likelihood of generating the data .</sentence>
				<definiendum id="0">unique ME solution</definiendum>
			</definition>
			<definition id="9">
				<sentence>The Maximum Entropy models were also interpolated with the conventional trigram , using yet unseen data for interpolation .</sentence>
				<definiendum id="0">Maximum Entropy</definiendum>
				<definiens id="0">models were also interpolated with the conventional trigram , using yet unseen data for interpolation</definiens>
			</definition>
</paper>

		<paper id="1075">
			<definition id="0">
				<sentence>To avoid objectionable lag in the system 's electronic ink echo , a high-performance workstation ( i.e. , Sun SPARCstation 2 ) is used to process the subject 's input .</sentence>
				<definiendum id="0">high-performance workstation</definiendum>
				<definiens id="0">used to process the subject 's input</definiens>
			</definition>
</paper>

		<paper id="1100">
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>The main component of the model , a token , was defined as any character string : a word , number , symbol , punctuation or any combination .</sentence>
				<definiendum id="0">main component of the model</definiendum>
				<definiens id="0">any character string : a word , number , symbol , punctuation or any combination</definiens>
			</definition>
			<definition id="1">
				<sentence>This test measures how consistently the methods treat individual test contexts by determining whether the classifiers are making the same classification errors in each of the senses .</sentence>
				<definiendum id="0">test</definiendum>
			</definition>
</paper>

		<paper id="1090">
</paper>

		<paper id="1101">
</paper>

		<paper id="1073">
</paper>

		<paper id="1076">
			<definition id="0">
				<sentence>EMACS Image EMACS is an editor for scanned documents in which the inputs and outputs are binary text images\ [ I\ ] .</sentence>
				<definiendum id="0">EMACS Image EMACS</definiendum>
			</definition>
			<definition id="1">
				<sentence>Tbe image generator consists of a message source , which generates a symbol string containing the informarion to be cornrnunicated , and an imager , which formats or encodes the message into an ideal bitmap .</sentence>
				<definiendum id="0">Tbe image generator</definiendum>
				<definiendum id="1">imager</definiendum>
				<definiens id="0">consists of a message source , which generates a symbol string containing the informarion to be cornrnunicated</definiens>
				<definiens id="1">formats or encodes the message into an ideal bitmap</definiens>
			</definition>
</paper>

		<paper id="1085">
</paper>

		<paper id="1112">
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>A weight was added to reduce the effective size of the task-independent training database as shown in Equation 1 : C ( w2 , wl ) Crs ( w2 , wl ) +Y*CTt ( W2 , wl ) where C ( w2 , wl ) is the counts of the nurnher of occurrences of word wl followed by w2 , CTS ( w2 , wl ) are the counts from the task-specific database and Crt ( w2 , wl ) are the counts from the task-independent database .</sentence>
				<definiendum id="0">C</definiendum>
				<definiendum id="1">Crt</definiendum>
				<definiens id="0">the counts of the nurnher of occurrences of word wl followed by w2</definiens>
			</definition>
			<definition id="1">
				<sentence>A duration-normalized likelihood score for each keyword is computed using the following Equation 2 : AP + GP + Constant KeyScore = Duration where AP is the acoustic log-likelihood score for the keyword , and GP is the log probability of the grammar transition into the keyword , and Constant is a constant added to the score to penalize keyword hypotheses that have a short duration .</sentence>
				<definiendum id="0">duration-normalized likelihood score</definiendum>
				<definiendum id="1">AP</definiendum>
				<definiendum id="2">GP</definiendum>
				<definiendum id="3">Constant</definiendum>
				<definiens id="0">the acoustic log-likelihood score for the keyword</definiens>
				<definiens id="1">the log probability of the grammar transition into the keyword</definiens>
				<definiens id="2">a constant added to the score to penalize keyword hypotheses that have a short duration</definiens>
			</definition>
</paper>

		<paper id="1113">
</paper>

		<paper id="1081">
</paper>

		<paper id="1084">
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>Alembic is a natural language-based information extraction system that has been under development for about one year .</sentence>
				<definiendum id="0">Alembic</definiendum>
				<definiens id="0">a natural language-based information extraction system that has been under development for about one year</definiens>
			</definition>
			<definition id="1">
				<sentence>RHO is a terminological classification framework that ultimately descends from KL-ONE .</sentence>
				<definiendum id="0">RHO</definiendum>
				<definiens id="0">a terminological classification framework that ultimately descends from KL-ONE</definiens>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>Brill presents a new technique for parsing which extends the symbolic POS tagger he presented last year .</sentence>
				<definiendum id="0">Brill</definiendum>
				<definiens id="0">presents a new technique for parsing which extends the symbolic POS tagger he presented last year</definiens>
			</definition>
			<definition id="1">
				<sentence>These scales play a role in human language understanding because of a phenomenon called scalar implicature , which underlies the fact that if someone asks if Tokyo is a big city , much better than replying `` yes '' is to say , `` Well , no ; it 's actually quite huge '' .</sentence>
				<definiendum id="0">scalar implicature</definiendum>
				<definiens id="0">a big city</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>The WSJ-CSR tests consist of tests of large vocabulary ( lexicons of 5,000 to more than 20,000 words ) continuous speech recognition systems .</sentence>
				<definiendum id="0">WSJ-CSR tests</definiendum>
				<definiens id="0">consist of tests of large vocabulary ( lexicons of 5,000 to more than 20,000 words ) continuous speech recognition systems</definiens>
			</definition>
			<definition id="1">
				<sentence>The ATIS tests consist of tests of ( 1 ) ATIS-domain spontaneous speech ( lexicons typically less than 2,000 words ) , ( 2 ) natural language understanding , and ( 3 ) spoken language understanding .</sentence>
				<definiendum id="0">ATIS tests</definiendum>
				<definiens id="0">consist of tests of ( 1 ) ATIS-domain spontaneous speech ( lexicons typically less than 2,000 words ) , ( 2 ) natural language understanding , and ( 3 ) spoken language understanding</definiens>
			</definition>
			<definition id="2">
				<sentence>For several years , NIST has implemented two tests of statistical significance for the results of benchmark tests of speech recognition systems : the McNemar sentence error test ( MN ) and a Matched-Pair-Sentence-Segment-WordError ( MAPSSWE ) test , on the word error rate found in sentence segments .</sentence>
				<definiendum id="0">NIST</definiendum>
				<definiendum id="1">Matched-Pair-Sentence-Segment-WordError</definiendum>
				<definiens id="0">has implemented two tests of statistical significance for the results of benchmark tests of speech recognition systems : the McNemar sentence error test</definiens>
			</definition>
			<definition id="3">
				<sentence>System : An Overview '' , Computer Speech and Language , in press ( 1993 ) .</sentence>
				<definiendum id="0">System</definiendum>
			</definition>
			<definition id="4">
				<sentence>Dowding , J. , et al. , `` Gemini : A Natural Language System for Spoken-Language Understanding '' , in Proceedings of the Human Language Technology Workshop , March 1993 ( M. Bates , ed . )</sentence>
				<definiendum id="0">Gemini</definiendum>
				<definiens id="0">A Natural Language System for Spoken-Language Understanding ''</definiens>
			</definition>
			<definition id="5">
				<sentence>Speaker Independent : Tests : Read Speech a. SI Test Set ( Baseline Tests ) W.Err U.Err IDENTIFIER 16.7 81.1 BBN NOV92 CSR BYBLOS SI-12 20K BIGRAM BASELINE 15.2 79.0 CMU NOV92 CSR SPHINX-iI SI-84 20K BASELINE 25.0 86.8 DRAGON NOV92 CSR MULTIPLE SI-12 20K NVP BASELINE 25.2 88.0 LL NOV92 CSR SI-84 20K OPEN NVP BIGRAM BASELINE Test Set ( Non-Basellne Tests ) 14.8 75.7 BBN NOV92 CSR BYBLOS SI-12 20K TRIGRAM 12.8 71.8 CMU NOV92 CSR SPHINX-iI SI-84 20K TRIGRAM 24.8 87.4 DRAGON NOV92 CSR GD SI-12 20K NVP 27.8 87.4 DRAGON NOV92 CSR GI SI-12 20K NVP 19.4 84.1 LL NOV92 CSR SI-84 20K OPEN NVP TRIGRAM ADAPTIVE b. SI Test Set ( Baseline Tests ) 14.1 78.2 DRAGON NOV92 CSR MULTIPLE SI-12 5K NVP BASELINE 15.0 78.2 LL NOV92 CSR SI-84 5K CLOSED NVP BIGRAM BASELINE 13.0 73.9 SRI NOV92 CSR DECIPHER ( TM ) SI-84 BIGRAM BASELINE EVL 20K NVP Systems bbnl-d cmul-d dragon3-d mJ .</sentence>
				<definiendum id="0">SI Test Set</definiendum>
				<definiens id="0">Tests : Read Speech a. SI Test Set ( Baseline Tests</definiens>
			</definition>
</paper>

		<paper id="1059">
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>The Translator 's Work1pANGLOSS is a joint project of the Center for Machine Translation at Carnegie Mellon University ( CMU ) , the Computing Research Laboratory of New Mexico State University ( NMSU ) , and the Information Sciences Institute of the University of Southern California ( ISI ) .</sentence>
				<definiendum id="0">Translator 's Work1pANGLOSS</definiendum>
			</definition>
			<definition id="1">
				<sentence>A CMAT menu consists of three regions , each separated by a horizontal line .</sentence>
				<definiendum id="0">CMAT menu</definiendum>
				<definiens id="0">consists of three regions , each separated by a horizontal line</definiens>
			</definition>
			<definition id="2">
				<sentence>From top to bottom these are : • The LABEL region , which contains the word or phrase in the source text that produced this particular component ) • The FUNCTION region , which contains the post-editing Move , Delete , Modify , and Finish functions .</sentence>
				<definiendum id="0">LABEL region</definiendum>
				<definiendum id="1">FUNCTION region</definiendum>
				<definiens id="0">contains the word or phrase in the source text that produced this particular component</definiens>
			</definition>
			<definition id="3">
				<sentence>When a component is Modified , the new alternative entered by the user is stored in the knowledge base , and then treated as if it had just been chosen .</sentence>
				<definiendum id="0">Modified</definiendum>
				<definiens id="0">the new alternative entered by the user is stored in the knowledge base</definiens>
			</definition>
			<definition id="4">
				<sentence>The CMAT editor , in conjunction with often fragmentary MT and word-for-word translation , allows the translator to produce high-quality translations more quickly and easily than the simple combination of a text editor and an online dictionary .</sentence>
				<definiendum id="0">CMAT editor</definiendum>
				<definiens id="0">allows the translator to produce high-quality translations more quickly and easily than the simple combination of a text editor and an online dictionary</definiens>
			</definition>
			<definition id="5">
				<sentence>The Translator 's Workbench : An Environment for Multi-Lingual Text Processing and Translation .</sentence>
				<definiendum id="0">Translator 's Workbench</definiendum>
			</definition>
</paper>

		<paper id="1074">
</paper>

		<paper id="1079">
</paper>

		<paper id="1110">
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>PHONE-BASED ACOUSTIC LIKELIHOODS The basic idea is to train a set of large phone-based ergodic hidden Markov models ( HMMs ) for each non-linguistic feature to be identified ( language , gender , speaker , ... ) .</sentence>
				<definiendum id="0">HMMs</definiendum>
				<definiens id="0">feature to be identified ( language , gender , speaker , ... )</definiens>
			</definition>
			<definition id="1">
				<sentence>The BDSONS Corpus : BDSONS , Base de Donn6es des Sons du Fran~ais\ [ 2\ ] , was designed to provide a large corpus of French speech data for the study of the sounds in the French language and to aid speech research .</sentence>
				<definiendum id="0">BDSONS Corpus</definiendum>
				<definiens id="0">designed to provide a large corpus of French speech data for the study of the sounds in the French language and to aid speech research</definiens>
			</definition>
			<definition id="2">
				<sentence>The BREF Corpus : BREF is a large read-speech corpus , containing over 100 hours of speech material , from 120 speakers ( 55m/65f ) \ [ 15\ ] .</sentence>
				<definiendum id="0">BREF Corpus</definiendum>
				<definiendum id="1">BREF</definiendum>
				<definiens id="0">a large read-speech corpus , containing over 100 hours of speech material</definiens>
			</definition>
			<definition id="3">
				<sentence>The DARPA WSJ0 Corpus : The DARPA Wall Street Journal-based Continuous-Speech Corpus ( WSJ ) \ [ 22\ ] has been designed to provide general-purpose speech data ( primarily , read speech data ) with large vocabularies .</sentence>
				<definiendum id="0">DARPA WSJ0 Corpus</definiendum>
				<definiens id="0">The DARPA Wall Street Journal-based Continuous-Speech Corpus ( WSJ ) \ [ 22\ ] has been designed to provide general-purpose speech data ( primarily , read speech data ) with large vocabularies</definiens>
			</definition>
			<definition id="4">
				<sentence>The DARPA TIMIT Corpus : The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus\ [ 4\ ] is a corpus of read speech designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems .</sentence>
				<definiendum id="0">DARPA TIMIT Corpus</definiendum>
				<definiens id="0">a corpus of read speech designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development</definiens>
			</definition>
			<definition id="5">
				<sentence>The test data consist of 1344 sentences , comprised of 8 sentences from each of the 168 test speakers ( 112m/560 .</sentence>
				<definiendum id="0">test data</definiendum>
				<definiens id="0">consist of 1344 sentences , comprised of 8 sentences from each of the 168 test speakers ( 112m/560</definiens>
			</definition>
			<definition id="6">
				<sentence>The BREF test data consists of 130 sentences from 20 speakers ( 10m/100 and for BDSONS the data is comprised of 121 sentences from 11 speakers ( 5m/60 .</sentence>
				<definiendum id="0">BREF test data</definiendum>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>The probability that he will , in this way , obtain tile list fi , ... , f , , , is Pr ( fl , ... , f , , , le ) = sl &gt; O 0 SI '' '' ( 1 ) s , ~/Pr ( sle ) 1-I~=1 Pr ( file ) s ' , where Pr ( f , le ) is the probability from our statistical model that the phrase f , occurs as a translation of e , and Pr ( sle ) is the probability that the lexicographer chooses to sample s instances of e. The multinomial coefficient is defined by s ) s !</sentence>
				<definiendum id="0">le )</definiendum>
				<definiendum id="1">Pr ( sle )</definiendum>
				<definiens id="0">obtain tile list fi , ... , f , , , is Pr ( fl , ... , f , , , le ) = sl &gt; O 0 SI '' '' ( 1 ) s , ~/Pr ( sle ) 1-I~=1 Pr ( file ) s '</definiens>
				<definiens id="1">the probability from our statistical model that the phrase f , occurs as a translation of e , and</definiens>
			</definition>
			<definition id="1">
				<sentence>The EM algorithm adjusts an initial estimate of O in a series of iterations .</sentence>
				<definiendum id="0">EM algorithm adjusts</definiendum>
				<definiens id="0">an initial estimate of O in a series of iterations</definiens>
			</definition>
			<definition id="2">
				<sentence>Each iteration consists of an estimation step in which a count is determined for each parameter , followed by a maximization step in which each parameter is replaced by a value proportional to its count .</sentence>
				<definiendum id="0">iteration</definiendum>
			</definition>
</paper>

		<paper id="1102">
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>The VOYAGER application uses an object-oriented database , although we have also accessed data in SQL and other configurations \ [ 3\ ] .</sentence>
				<definiendum id="0">VOYAGER application</definiendum>
			</definition>
			<definition id="1">
				<sentence>Each lexical entry consists of a word ID , a pronunciation , and left and right morphological categories .</sentence>
				<definiendum id="0">lexical entry</definiendum>
			</definition>
			<definition id="2">
				<sentence>Language Modeling Language modeling is an important aspect of speech recognition since it can dramatically reduce the difficulty of a task .</sentence>
				<definiendum id="0">Language Modeling Language modeling</definiendum>
				<definiens id="0">an important aspect of speech recognition since it can dramatically reduce the difficulty of a task</definiens>
			</definition>
			<definition id="3">
				<sentence>As we and others have done previously \ [ 4 , 12\ ] , the category bigram probability is smoothed by interpolating the bigram estimate with the prior probabilities of each category : : ( llr ) = c ( l ) ~+ ( 1 ~ ( r ) ) c ( all word tokens ) c ( r ) = c ( r ) + K where c ( x ) is the count of tokens of category x in the corpus .</sentence>
				<definiendum id="0">c ( x )</definiendum>
				<definiens id="0">the count of tokens of category x in the corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>The `` j-operator '' function renames the generic key `` topic '' in the semantic frame under construction to the specific semantic relationship defined by the particular operator , in our case , `` near . ''</sentence>
				<definiendum id="0">j-operator '' function</definiendum>
				<definiens id="0">renames the generic key `` topic '' in the semantic frame under construction to the specific semantic relationship defined by the particular operator</definiens>
			</definition>
			<definition id="5">
				<sentence>EVALUATION For the Japanese VOYAGER system , we defined a vocabulary of 495 words comprised of words in the training set and words determined by translating 2000 sentences from the English VOYAGER training corpus .</sentence>
				<definiendum id="0">EVALUATION</definiendum>
				<definiens id="0">vocabulary of 495 words comprised of words in the training set and words determined by translating 2000 sentences from the English VOYAGER training corpus</definiens>
			</definition>
</paper>

		<paper id="1013">
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>A linguistic scale is a set of words , of the same grammatical category , which can be ordered by their semantic strength or degree of informativeness \ [ 1\ ] .</sentence>
				<definiendum id="0">linguistic scale</definiendum>
			</definition>
			<definition id="1">
				<sentence>For example , in light blue shirt , blue is a value of the property color , while light indicates the shade* .</sentence>
				<definiendum id="0">blue</definiendum>
				<definiens id="0">a value of the property color</definiens>
			</definition>
			<definition id="2">
				<sentence>Kendall 's x is defined as = Pc-Pd where Pc and Pd are the probabilities of observing a con- '' cordance or discordance respectively , x ranges from -1 to +1 , with +1 indicating complete concordance , -1 complete discordance , and 0 no correlation between X and Y. An unbiased estimator of x is the statistic TC-Q where n is the number of paired observations in the sample and C and Q are the numbers of observed concordances and discordances respectively \ [ 8\ ] .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">= Pc-Pd where Pc and Pd are the probabilities of observing a con- '' cordance or discordance respectively , x ranges from -1 to +1 , with +1 indicating complete concordance , -1 complete discordance , and 0 no correlation between X and Y. An unbiased estimator of x is the statistic TC-Q where</definiens>
			</definition>
			<definition id="3">
				<sentence>It is interesting to note here that the expected number of adjectives per cluster is ~=2.33 , and the clustering algorithm employed discourages long groups ; nevertheless , the evidence for the adjectives in group 6 is strong enough to allow the creation of a group with more than twice the expected number of members .</sentence>
				<definiendum id="0">nevertheless</definiendum>
				<definiens id="0">strong enough to allow the creation of a group with more than twice the expected number of members</definiens>
			</definition>
			<definition id="4">
				<sentence>measures are defined : a • Recall = • 100 % a+c a • Precision =a~ '' 100 % b • Fallout = • 100 % b+d In other words , recall is the percentage of correct `` yes '' answers that the system found among the model `` yes '' answers , precision is the percentage of correct `` yes '' answers among the total of `` yes '' answers that the system reported , and fallout is the percentage of incorrect `` ~e.s. ' . '</sentence>
				<definiendum id="0">recall</definiendum>
				<definiendum id="1">precision</definiendum>
				<definiendum id="2">fallout</definiendum>
				<definiens id="0">the percentage of correct `` yes '' answers that the system found among the model `` yes '' answers</definiens>
				<definiens id="1">the percentage of correct `` yes '' answers among the total of `` yes '' answers that the system reported , and</definiens>
			</definition>
			<definition id="5">
				<sentence>answers relative to the total number of `` no '' answers We also compute a combined measure for recall and precision , the F-measure \ [ 13\ ] , which always takes a value between the values of recall and precision , and is higher when recall and precision are closer ; it is defined as *****Another measure used in information retrieval , overgeneration , is in our case always equal to ( 100 precision ) % , 275 Recall Precision Fallout F-measure ( 15=1 ) 7 clusters 50.78 % 43.56 % 7.48 % 46.89 % 8 clusters 37.31 % 38.10 % 6.89 % 37.70 % 9 clusters 49.74 % 46.38 % 6.54 % 48.00 % 10 clusters 35.23 % 41.98 % 5.54 % 38.31 % Table 2 : Evaluation results .</sentence>
				<definiendum id="0">overgeneration</definiendum>
				<definiendum id="1">Recall Precision Fallout F-measure</definiendum>
				<definiens id="0">no '' answers We also compute a combined measure for recall and precision , the F-measure \ [ 13\ ] , which always takes a value between the values of recall and precision , and is higher when recall and precision are closer ; it is defined as *****Another measure used in information retrieval</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>ABSTRACT Tied-mixture ( or semi-continuous ) distributions are an important tool for acoustic modeling , used in many highperformance speech recognition systems today .</sentence>
				<definiendum id="0">ABSTRACT Tied-mixture</definiendum>
			</definition>
			<definition id="1">
				<sentence>( 1 ) k Note that the component Gaussian densities , Pk ( x ) -'~ N ( t~k , ~k ) , are not indexed by the state , i. In this light , tied mixtures can be seen as a particular example of the general technique of tying to reduce the number of model parameters that must be trained \ [ 3\ ] .</sentence>
				<definiendum id="0">Pk ( x ) -'~ N</definiendum>
			</definition>
			<definition id="2">
				<sentence>Taking this approach and assuming microsegments are independent , the probability for a segment is P ( Y I°t ) = H E wjk P ( Yj I oqk ) , ( 2 ) j k where aik is the k th mixture component of microsegment j and Yj is the subset of frames in Y that map to microsegment j. Given the SSM 's deterministic warping and assuming the same number of distributions for all mixture components of a given microsegment , the extension of the EM algorithm for training mixtures of this type is straightforward .</sentence>
				<definiendum id="0">aik</definiendum>
				<definiendum id="1">Yj</definiendum>
				<definiens id="0">the k th mixture component of microsegment j and</definiens>
			</definition>
			<definition id="3">
				<sentence>Ostendorf , M. and Roukos , S. , `` A Stochastic Segment Model for Phoneme-Based Continuous Speech Recognition , '' IEEE Trans .</sentence>
				<definiendum id="0">A Stochastic Segment Model</definiendum>
				<definiens id="0">Phoneme-Based Continuous Speech Recognition , '' IEEE Trans</definiens>
			</definition>
</paper>

		<paper id="1043">
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>The modular architecture consists of a pipelined series of processing phases that each output multiple hypotheses filtered by statistical preference mechanisms .</sentence>
				<definiendum id="0">modular architecture</definiendum>
				<definiens id="0">consists of a pipelined series of processing phases that each output multiple hypotheses filtered by statistical preference mechanisms</definiens>
			</definition>
			<definition id="1">
				<sentence>The English grammar is a large , domain-independent unificationbased phrase-structure grammar , augmented by a small number of domain-specific rules ( Section 3.1 ) .</sentence>
				<definiendum id="0">English grammar</definiendum>
				<definiens id="0">a large , domain-independent unificationbased phrase-structure grammar , augmented by a small number of domain-specific rules ( Section 3.1 )</definiens>
			</definition>
			<definition id="2">
				<sentence>In analysis mode , the grammar is compiled into tables that drive a left-corner parser ; input is supplied in the form of a word hypothesis lattice , and output is a set of possible semantic analyses expressed in Quasi Logical Form ( QLF ) .</sentence>
				<definiendum id="0">output</definiendum>
				<definiens id="0">compiled into tables that drive a left-corner parser ; input is supplied in the form of a word hypothesis lattice , and</definiens>
			</definition>
			<definition id="3">
				<sentence>A transfer rule specifies a pair of QLF patterns ; the left hand side matches a fragment of the source language QLF and the right hand side the corresponding target QLF .</sentence>
				<definiendum id="0">transfer rule</definiendum>
				<definiens id="0">specifies a pair of QLF patterns ; the left hand side matches a fragment of the source language QLF and the right hand side the corresponding target QLF</definiens>
			</definition>
			<definition id="4">
				<sentence>Others , which are trained automatically , reflect the strengths of semantic collocations between triples of logical constants occurring in relevant configurations in QLFs .</sentence>
				<definiendum id="0">Others</definiendum>
				<definiens id="0">the strengths of semantic collocations between triples of logical constants occurring in relevant configurations in QLFs</definiens>
			</definition>
			<definition id="5">
				<sentence>A ( Q , T ) , where I ( Q , T ) is the number of string segments induced by Q and present in T , and A ( Q , T ) is the number induced by Q but absent from T. This choice of goodness function was found , by trial and error , to lead to a good correlation with the metrics .</sentence>
				<definiendum id="0">I</definiendum>
				<definiendum id="1">T )</definiendum>
				<definiendum id="2">T )</definiendum>
				<definiens id="0">the number of string segments induced by Q and present in T</definiens>
			</definition>
			<definition id="6">
				<sentence>The value of Kendall 's ranking correlation coefficient between the relativized `` goodness '' values and the scaled sum ( reflecting the degree of agreement between the orderings induced by the two criteria ) was also almost identical for the two sets of factors .</sentence>
				<definiendum id="0">scaled sum</definiendum>
				<definiens id="0">reflecting the degree of agreement between the orderings induced by the two criteria</definiens>
			</definition>
			<definition id="7">
				<sentence>The original training/development corpus is a 4600-sentence subset of the ATIS corpus consisting of sentences of length not more than 15 words .</sentence>
				<definiendum id="0">original training/development corpus</definiendum>
				<definiens id="0">a 4600-sentence subset of the ATIS corpus consisting of sentences of length not more than 15 words</definiens>
			</definition>
</paper>

		<paper id="1001">
</paper>

		<paper id="1107">
</paper>

		<paper id="1097">
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>The PANGLOSS project is a three-site collaborative effort to build a large-scale knowledge-based machine translation system .</sentence>
				<definiendum id="0">PANGLOSS project</definiendum>
				<definiens id="0">a three-site collaborative effort to build a large-scale knowledge-based machine translation system</definiens>
			</definition>
			<definition id="1">
				<sentence>The Ontology Base is a synthesis of USC/ISI 's PENMAN Upper Model \ [ Bateman , 1990\ ] and CMU 's ONTOS concept hierarchy \ [ Carlson and Nirenburg , 1990\ ] .</sentence>
				<definiendum id="0">Ontology Base</definiendum>
				<definiendum id="1">CMU</definiendum>
				<definiens id="0">a synthesis of USC/ISI 's PENMAN Upper Model \ [ Bateman , 1990\ ] and</definiens>
			</definition>
			<definition id="2">
				<sentence>LDOCE is a learner 's dictionary of English with 27,758 words and 74,113 word senses .</sentence>
				<definiendum id="0">LDOCE</definiendum>
				<definiens id="0">a learner 's dictionary of English with 27,758 words and 74,113 word senses</definiens>
			</definition>
			<definition id="3">
				<sentence>185 WordNet is a semantic word database based on psycholinguistic principles .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="4">
				<sentence>For example , `` seal '' has 5 noun senses in LDOCE , one of which is : ( seal_l_l ) `` any of several types of large fisheating animals living mostly on cool seacoasts and floating ice , with broad flat limbs ( FLIPPERs ) suitable for swimming '' WordNet has 7 definitions of `` seal , '' one of which is : For example , ( bat_l_l ) is defined as `` any of the several types of specially shaped wooden stick used for ... '' The genus term for ( bat_l_l ) is ( stick_l_l ) .</sentence>
				<definiendum id="0">flat limbs ( FLIPPERs</definiendum>
				<definiens id="0">) suitable for swimming '' WordNet has 7 definitions of `` seal , '' one of which is : For example</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>Q1 is the set of queries ( probably multiple sets ) created to help in adjusting a system to this task , create better weighting algorithms , and in general to train the system fox testing .</sentence>
				<definiendum id="0">Q1</definiendum>
				<definiens id="0">the set of queries ( probably multiple sets ) created to help in adjusting a system to this task , create better weighting algorithms , and in general to train the system fox testing</definiens>
			</definition>
			<definition id="1">
				<sentence>Q3 is the set of queries created from the test topics as ad-hoc queries for searching against the combined documents ( both training documents and test documents ) .</sentence>
				<definiendum id="0">Q3</definiendum>
			</definition>
			<definition id="2">
				<sentence>First there was a desire to allow a wide range of query construction methods by keeping the topic ( the need statement ) distinct from the query ( the actual text submitted to the system ) .</sentence>
				<definiendum id="0">topic</definiendum>
				<definiens id="0">the need statement ) distinct from the query ( the actual text submitted to the system )</definiens>
			</definition>
</paper>

		<paper id="1080">
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>Emea~ of New Item Introduction The Map Task corpus presents an excellent opportunity for examining how speakers introduce new items into a discourse .</sentence>
				<definiendum id="0">Map Task corpus</definiendum>
				<definiens id="0">presents an excellent opportunity for examining how speakers introduce new items into a discourse</definiens>
			</definition>
</paper>

		<paper id="1058">
</paper>

		<paper id="1082">
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>A semantic concordance is being constructed to use in studies of sense resolution in context ( semantic disambiguation ) .</sentence>
				<definiendum id="0">semantic concordance</definiendum>
				<definiens id="0">semantic disambiguation )</definiens>
			</definition>
			<definition id="1">
				<sentence>The Brown Corpus is the text and WordNet is the lexicon .</sentence>
				<definiendum id="0">Brown Corpus</definiendum>
				<definiendum id="1">WordNet</definiendum>
				<definiens id="0">the text</definiens>
			</definition>
			<definition id="2">
				<sentence>WordNet is an example of a more efficient combination of traditional lexicography and modern computer science .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">an example of a more efficient combination of traditional lexicography and modern computer science</definiens>
			</definition>
			<definition id="3">
				<sentence>A synset is a group of words that are synonymous , in the sense that there are contexts in which they can be interchanged without changing the meaning of the statement .</sentence>
				<definiendum id="0">synset</definiendum>
				<definiens id="0">a group of words that are synonymous , in the sense that there are contexts in which they can be interchanged without changing the meaning of the statement</definiens>
			</definition>
			<definition id="4">
				<sentence>As of the time this is written , WordNet contains more than 83,800 entries ( unique character strings , words and collocations ) and more than 63,300 lexicalized concepts ( synsets , plus defining glosses ) ; altogether there are more than 118,600 entry-concept pairs .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">contains more than 83,800 entries ( unique character strings , words and collocations ) and more than 63,300 lexicalized concepts ( synsets , plus defining glosses</definiens>
			</definition>
			<definition id="5">
				<sentence>ConText is an X-windows interface designed specifically for annotating written texts with WordNet sense tags \ [ 5\ ] .</sentence>
				<definiendum id="0">ConText</definiendum>
			</definition>
			<definition id="6">
				<sentence>Since WordNet contains only open-class words , ConText is used to tag only nouns , verbs , adjectives , and adverbs ; that is to say , only about 50 % of the running words in the Brown Corpus are semantically tagged .</sentence>
				<definiendum id="0">ConText</definiendum>
				<definiens id="0">used to tag only nouns , verbs , adjectives , and adverbs</definiens>
			</definition>
			<definition id="7">
				<sentence>The SGML markers delimit sentences &lt; s &gt; , sentence numbers &lt; stn &gt; , words in the text &lt; wd &gt; , base forms of text words &lt; mwd &gt; , comments &lt; cmt &gt; , proper nouns &lt; pn &gt; , partof-speech tags &lt; tag &gt; and semantic tags &lt; sn &gt; or &lt; msn &gt; .</sentence>
				<definiendum id="0">SGML markers</definiendum>
				<definiens id="0">delimit sentences &lt; s &gt; , sentence numbers &lt; stn &gt; , words in the text &lt; wd &gt; , base forms of text words &lt; mwd &gt; , comments &lt; cmt &gt; , proper nouns &lt; pn &gt; , partof-speech tags &lt; tag &gt; and semantic tags &lt; sn &gt; or</definiens>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>The pre-processing step consists of a number of subtasks , including : using a maximum-likelihood tokenizer in conjunction with a morphological analyzer ( de-inflector ) that recognizes all inflected forms of Japanese verbs , adjectives , and nouns words , including inflection codes and roots ( for inflected forms ) , pronunciation , English glosses ( some automatically generated from parallel text ) , and English definitions words using dynamic-programming techniques inating a large source of user errors arising from the unusual numbering conventions in Japanese ) phrases specific dictionary The user 's word-processing environment consists normally of two windows , one containing the original Japanese broken into words and annotated with pronunciation and `` best guess '' glosses , the other for entry of the English translation .</sentence>
				<definiendum id="0">pre-processing step</definiendum>
			</definition>
			<definition id="1">
				<sentence>Information extracted during pre-processing but not available in the annotated document ( longer definitions , inflection information , etc. ) can be accessed instantly from the document-specific dictionary using the keyboard or mouse , and is presented in a pop-up window .</sentence>
				<definiendum id="0">Information</definiendum>
				<definiens id="0">extracted during pre-processing but not available in the annotated document ( longer definitions , inflection information</definiens>
			</definition>
			<definition id="2">
				<sentence>Let score ( w ) and lenflh ( w ) denote the score and length of the character sequence w. For a sentence of N characters numbered from 0 to N 1 , let best\ [ i\ ] denote the score of the best tokenization of the character sequence from 0 to i1 , and initialize best\ [ O\ ] = O , best\ [ i\ ] = -oo for 1 &lt; i &lt; N. The best tokenization score for the sentence is then given by best\ [ N\ ] after : FOR i=0 to N1 DO FOR all sequences w that starl at position i DO IF best\ [ i\ ] + score ( w ) &gt; best\ [ i + length ( w ) l THEN best\ [ i + length ( w ) \ ] = best\ [ i\ ] + score ( w ) 192 Note that when two tokenizations both have a word ending at a given position , only the higher scoring solution up to that position is used in subsequent calculations .</sentence>
				<definiendum id="0">lenflh</definiendum>
				<definiens id="0">the score and length of the character sequence w. For a sentence of N characters numbered from 0 to N 1 , let best\ [ i\ ] denote the score of the best tokenization of the character sequence from 0 to i1</definiens>
			</definition>
			<definition id="3">
				<sentence>As a first pass at helping the user with Japanese sentence structure , LINGSTAT incorporates a simple finite-state parser designed to identify modifying phrases in Japanese sentences .</sentence>
				<definiendum id="0">LINGSTAT</definiendum>
				<definiens id="0">incorporates a simple finite-state parser designed to identify modifying phrases in Japanese sentences</definiens>
			</definition>
			<definition id="4">
				<sentence>Lexicalization is a useful tool for resolving attachment questions and in sense disambiguation .</sentence>
				<definiendum id="0">Lexicalization</definiendum>
				<definiens id="0">a useful tool for resolving attachment questions and in sense disambiguation</definiens>
			</definition>
</paper>

		<paper id="1108">
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>Skipping is an appropriate strategy for the data in the two domains we are working with , because parsing failures tend to be due to extraneous material such as interpolated irrelevant comments and false starts .</sentence>
				<definiendum id="0">Skipping</definiendum>
				<definiens id="0">an appropriate strategy for the data in the two domains we are working with , because parsing failures tend to be due to extraneous material such as interpolated irrelevant comments and false starts</definiens>
			</definition>
			<definition id="1">
				<sentence>Air traffic control ( ATC ) involves oral communication , as controllers interact with pilots via radio , issuing commands which govern the movements of planes both on the ground and in the air \ [ 3\ ] .</sentence>
				<definiendum id="0">Air traffic control</definiendum>
				<definiendum id="1">ATC</definiendum>
				<definiendum id="2">oral communication</definiendum>
			</definition>
			<definition id="2">
				<sentence>Users specify an amount of time in the form of a number ( possibly fractional ) of seconds per word , so that longer inputs are given more time .</sentence>
				<definiendum id="0">Users</definiendum>
				<definiens id="0">specify an amount of time in the form of a number ( possibly fractional ) of seconds per word , so that longer inputs are given more time</definiens>
			</definition>
			<definition id="3">
				<sentence>K-Pack : A programmer 's interface to KNET .</sentence>
				<definiendum id="0">K-Pack</definiendum>
				<definiens id="0">A programmer 's interface to KNET</definiens>
			</definition>
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>Each topic is a full text description , in a specific format , of an information need .</sentence>
				<definiendum id="0">topic</definiendum>
				<definiens id="0">a full text description , in a specific format , of an information need</definiens>
			</definition>
			<definition id="1">
				<sentence>The Narrative field expands on the information need , giving an overview of the classes of documents which would or &lt; top &gt; &lt; dom &gt; Domain : International Economics &lt; Title &gt; Topic : Satellite Launch Contracts &lt; desc &gt; Description : Document will cite the signing of a contract or preliminary agreement , or the making of a tentative reservation , to launch a commercial satellite .</sentence>
				<definiendum id="0">Narrative field</definiendum>
				<definiens id="0">expands on the information need , giving an overview of the classes of documents which would or &lt; top &gt; &lt; dom &gt; Domain</definiens>
			</definition>
			<definition id="2">
				<sentence>&lt; narr &gt; Narrative : A relevant document will mention the signing of a contract or preliminary agreement , or the making of a tentative reservation , to launch a commerciM satellite .</sentence>
				<definiendum id="0">Narrative</definiendum>
				<definiens id="0">A relevant document will mention the signing of a contract or preliminary agreement , or the making of a tentative reservation , to launch a commerciM satellite</definiens>
			</definition>
			<definition id="3">
				<sentence>The object network consists of object nodes ( documents ) ( o/s ) and concept representation nodes ( r , ~ 's ) .</sentence>
				<definiendum id="0">object network</definiendum>
			</definition>
			<definition id="4">
				<sentence>The # usa term tends to have unexpected effects , because a large part of the collection consists of articles from U.S. publications .</sentence>
				<definiendum id="0"># usa term</definiendum>
			</definition>
</paper>

		<paper id="1087">
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>The `` best '' class for an argument is the class that maximizes selectional association .</sentence>
				<definiendum id="0">best '' class for an argument</definiendum>
				<definiens id="0">the class that maximizes selectional association</definiens>
			</definition>
			<definition id="1">
				<sentence>The semantic similarity of nl and n2 is sim ( nl , n2 ) = Zai\ [ -logPr ( ci ) \ ] , ( 5 ) i where { el } is the set of classes dominating both nl and n2 .</sentence>
				<definiendum id="0">n2</definiendum>
				<definiens id="0">the set of classes dominating both nl and n2</definiens>
			</definition>
			<definition id="2">
				<sentence>The alternative attachment sites-verb-attachment and nounattachment -were evaluated according to the following criteria : vscore = freq ( v , PP ) I ( v ; PP ) ( 6 ) nscore = freq ( classl , PP ) I ( classl ; PP ) ( 7 ) where PP is an abbreviation for ( preposition , class2 ) , and class 1 and class2 are classes to which the object of the verb and object of the preposition belong , respectively .</sentence>
				<definiendum id="0">PP</definiendum>
				<definiens id="0">an abbreviation for ( preposition , class2 ) , and class 1 and class2 are classes to which the object of the verb and object of the preposition belong , respectively</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>DR-LINK is a multi-stage document detection system being developed under the auspices of DARPA 's TIPSTER Project .</sentence>
				<definiendum id="0">DR-LINK</definiendum>
				<definiens id="0">a multi-stage document detection system being developed under the auspices of DARPA 's TIPSTER Project</definiens>
			</definition>
			<definition id="1">
				<sentence>The overall goal of DR-LINK is to simultaneously 1 ) focus the flow of texts through the system by selecting a subset of texts on the basis of subject content and then highlighting those sub-parts of a document which are likely spots of relevant text while 2 ) enriching the semantic representation of text content by : a ) delineating each text 's discourse-level structure ; b ) detecting relations among concepts ; c ) expanding lexical representation with semantically-related terms ; and d ) representing concepts and relations in Conceptual Graphs .</sentence>
				<definiendum id="0">DR-LINK</definiendum>
				<definiens id="0">a document which are likely spots of relevant text while 2 ) enriching the semantic representation of text content by : a ) delineating each text 's discourse-level structure</definiens>
			</definition>
			<definition id="2">
				<sentence>The Text Structurer produces an enriched representation of each document by decomposing it into smaller , conceptually labelled components .</sentence>
				<definiendum id="0">Text Structurer</definiendum>
				<definiens id="0">produces an enriched representation of each document by decomposing it into smaller</definiens>
			</definition>
			<definition id="3">
				<sentence>The continuation clues are lexical clues which occur in a sentence-initial position and were observed in our coded sample data to predictably indicate either that the current sentence continues the same component as the prior sentence ( e.g. And or In addition ) or that there is a change in the component ( e.g .</sentence>
				<definiendum id="0">continuation clues</definiendum>
				<definiens id="0">lexical clues which occur in a sentence-initial position and were observed in our coded sample data to predictably indicate either that the current sentence continues the same component as the prior sentence ( e.g. And or In addition</definiens>
			</definition>
			<definition id="4">
				<sentence>We tested the Text Structurer on a set of 116 Wall Street Journal articles , consisting of over two thousand sentences .</sentence>
				<definiendum id="0">Text Structurer</definiendum>
				<definiens id="0">on a set of 116 Wall Street Journal articles , consisting of over two thousand sentences</definiens>
			</definition>
			<definition id="5">
				<sentence>Once a document is determined to be relevant , the Text Structurer can focus the data extraction process on those sentences or sentence fragments which are most likely to contain the information required to fill the database .</sentence>
				<definiendum id="0">Text Structurer</definiendum>
				<definiens id="0">focus the data extraction process on those sentences or sentence fragments which are most likely to contain the information required to fill the database</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>From these rules it is clear that the logical translation of yesterday , as an adverbial adjunct , is ZPZx ( ( adv-e ( during Yesterday ) ) Ix P\ ] ) .</sentence>
				<definiendum id="0">ZPZx</definiendum>
			</definition>
			<definition id="1">
				<sentence>In fact we will have ( 2e1 : \ [ el before ul \ ] \ [ ( ( adv-e ( during Yesterday ) ) \ [ Mary leave\ ] ) T ** eli ) , where ul denotes the utterance event for the sentence concerned , and T denotes the current tense tree .</sentence>
				<definiendum id="0">ul</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">\ [ el before ul \ ] \ [ ( ( adv-e ( during Yesterday ) ) \ [ Mary leave\ ] ) T ** eli )</definiens>
				<definiens id="1">the utterance event for the sentence concerned , and</definiens>
				<definiens id="2">the current tense tree</definiens>
			</definition>
			<definition id="2">
				<sentence>l\ [ V~ `` = I\ [ ~ '' ' , i.e. , ~lr~ ( s ) ( s ) , where S is the set of possible situations .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">the set of possible situations</definiens>
			</definition>
			<definition id="3">
				<sentence>re is a predicate which applies to a collection of temporally separated episodes .</sentence>
				<definiendum id="0">re</definiendum>
				<definiens id="0">a predicate which applies to a collection of temporally separated episodes</definiens>
			</definition>
			<definition id="4">
				<sentence>( num is an operator that maps numbers into predicate modifiers , and plur ( 'plural ' ) is a function that maps predicates applicable to individuals into predicates applicable to collections ; cf. , Link \ [ 13\ ] .</sentence>
				<definiendum id="0">num</definiendum>
				<definiendum id="1">plur</definiendum>
				<definiens id="0">an operator that maps numbers into predicate modifiers , and</definiens>
				<definiens id="1">a function that maps predicates applicable to individuals into predicates applicable to collections ; cf.</definiens>
			</definition>
			<definition id="5">
				<sentence>agtr ( 'attributive ' ) is an operator that maps predicates into predicate modifiers . )</sentence>
				<definiendum id="0">agtr</definiendum>
				<definiens id="0">an operator that maps predicates into predicate modifiers</definiens>
			</definition>
			<definition id="6">
				<sentence>b. ( past ( ( adv-e ( in-span-of ( K ( ( num 2 ) ( plur month ) ) ) ) ) ( ( adv-f ( ( num 3 ) ( plur episode ) ) ) \ [ Mary visit Paris\ ] ) ) ) c. ( 3e2 : \ [ e2 before u2\ ] \ [ \ [ \ [ e2 in-span-of ( K ( ( hum 2 ) ( plur month ) ) ) \ ] ^ \ [ e2 ( ( num 3 ) ( plur episode ) ) \ ] ^ ( mult \ [ Mary visit Paris\ ] ) \ ] ** e2\ ] ) ( 3 ) a. John regularly dated Mary for two years .</sentence>
				<definiendum id="0">K</definiendum>
				<definiendum id="1">K</definiendum>
				<definiens id="0">plur month ) ) ) \ ] ^ \ [ e2 ( ( num 3 ) ( plur episode</definiens>
			</definition>
			<definition id="7">
				<sentence>b. ( past ( ( adv-e ( lasts-for ( K ( ( num 10 ) ( pinr day ) ) ) ) ) ( ( adv-f As \ [ \ [ s ( ( attr periodic ) ( plur episode ) ) \ ] ^ \ [ ( period-of s ) = ( K ( ( num 4 ) ( plur hour ) ) ) \ ] \ ] ) \ [ John take ( K medicine ) \ ] ) ) ) c. ( 3e4 : \ [ e4 before u4\ ] \ [ \ [ \ [ e4 lasts-for ( K ( ( num 10 ) ( plur day ) ) ) \ ] \ [ e4 ( ( aUr periodic ) ( plur episode ) ) \ ] ^ \ [ ( period-of e4 ) = ( K ( ( num 4 ) ( plur hour ) ) ) \ ] n ( mult \ [ John take ( K medicine ) \ ] ) \ ] ** e4\ ] ) ( 5 ) a. Mary bakes a cake every Saturday .</sentence>
				<definiendum id="0">b. ( past ( ( adv-e ( lasts-for ( K</definiendum>
				<definiens id="0">( num 10 ) ( plur day ) ) ) \ ] \ [ e4 ( ( aUr periodic )</definiens>
			</definition>
			<definition id="8">
				<sentence>VP &lt; -VP\ [ stat , unbounded\ ] ADVL\ [ dur\ ] ; ( ADVL ' VP ' ) VP\ [ bounded\ ] ~ VP\ [ stat , unbounded\ ] ADVL\ [ dur\ ] ; ( ADVL ' VP ' ) VP ~ VP\ [ bounded\ ] ADVL\ [ span\ ] ; ( ADVL '' VP ' ) VP ~ VP\ [ bounded\ ] ADVL\ [ card\ ] ; ( ADVL '' VP ' ) VP\ [ stat , unbounded\ ] ~ VP\ [ bounded\ ] ADVL\ [ freq\ ] ; ( ADVL ' VP ' ) VP\ [ stat , unbounded\ ] ~ VP ADVL\ [ cye-time\ ] ; ( ADVL ' VP ' ) VP\ [ bounded\ ] ~-VP\ [ stat , unbounded\ ] ; ( bounded VP ' ) VP\ [ stat , unbounded\ ] ~-VP\ [ bounded\ ] ; ( iter VP ' ) VP\ [ stat , unbounded\ ] ~ VP\ [ telic\ ] ; ( result-state VP ' ) These rules allow transitions in aspectual class and VPadverbial combinations somewhat too liberally .</sentence>
				<definiendum id="0">; ( result-state VP</definiendum>
				<definiens id="0">transitions in aspectual class and VPadverbial combinations somewhat too liberally</definiens>
			</definition>
			<definition id="9">
				<sentence>Episodic Logic : A comprehensive semantic representation and knowledge representation for language understanding .</sentence>
				<definiendum id="0">Episodic Logic</definiendum>
			</definition>
			<definition id="10">
				<sentence>Episodic Logic : A situational logic for natural language processing .</sentence>
				<definiendum id="0">Episodic Logic</definiendum>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>The BYBLOS speech recognition system uses a multi-pass search strategy designed to use progressively more detailed models on a correspondingly reduced search space .</sentence>
				<definiendum id="0">BYBLOS speech recognition system</definiendum>
				<definiens id="0">uses a multi-pass search strategy designed to use progressively more detailed models on a correspondingly reduced search space</definiens>
			</definition>
			<definition id="1">
				<sentence>More recently , the system has been used for recognizing spontaneous speech from the ATIS corpus , which contains many spontaneous speech effects , such as partial words , nonspeech sounds , extraneous .</sentence>
				<definiendum id="0">ATIS corpus</definiendum>
				<definiens id="0">contains many spontaneous speech effects</definiens>
			</definition>
			<definition id="2">
				<sentence>The blind deeonvolution algorithm estimates the simple mean of each cepstral value over the utterance , and then subtracts this mean from the value in each frame .</sentence>
				<definiendum id="0">blind deeonvolution algorithm</definiendum>
				<definiens id="0">estimates the simple mean of each cepstral value over the utterance</definiens>
			</definition>
</paper>

		<paper id="1077">
</paper>

		<paper id="1091">
</paper>

		<paper id="1099">
</paper>

		<paper id="1070">
			<definition id="0">
				<sentence>The final form of a representative for a document ( or query ) is a vector D~ = ( w~ , l , w~,2 , ... , wi , ~ ) where D~ represents a document ( or query ) text and w~ , k is a term weight of term Tk attached to document Di .</sentence>
				<definiendum id="0">D~</definiendum>
				<definiendum id="1">k</definiendum>
				<definiens id="0">a vector D~ = ( w~ , l , w~,2 , ... , wi</definiens>
				<definiens id="1">a document ( or query ) text and w~</definiens>
			</definition>
			<definition id="1">
				<sentence>In this run , using the RPI feedback model developed by Fuhr\ [ 3\ ] , relevance feedback information was used for computing the feedback query term weight q~ of a term as p~ ( 1 -ri ) /\ [ ri ( 1 -Pi ) \ ] 1 Here Pi is the average document term weight for relevant documents , and ri is the corresponding factor for nonrelevant items .</sentence>
				<definiendum id="0">ri</definiendum>
				<definiens id="0">the average document term weight for relevant documents , and</definiens>
			</definition>
			<definition id="2">
				<sentence>The particular features used in TREC 1 were combinations of the following term factors : t f : within-document frequency of the term 351 logidf : log ( ( N+l ) /n ) , where N is the number of documents in the collection and n is the number of documents containing the term lognumterms : log ( number of different terms of the document ) imaxtf : 1 / ( maximum within-document frequency of a term in the document ) After using the relevance information , the final weight for a term in a TREC 1 document was W ( t , ) = 0.00042293 + -0.00150665 • tf* imaxtf + -0.00122627 * lognumterms • imaxtf .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the number of documents in the collection and</definiens>
				<definiens id="1">the number of documents containing the term lognumterms : log ( number of different terms of the document ) imaxtf : 1 / ( maximum within-document frequency of a term in the document ) After using the relevance information</definiens>
			</definition>
			<definition id="3">
				<sentence>normalized between 0.5 and b : Binary ( ie , always 1 ) • idf : n : None ( ie , always 1 ) t : Traditional ( log ( ( N+l ) /n ) ) where N is number of documents in collection and n is number of documents • normalization : n : None c : Cosine .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">None c</definiendum>
				<definiens id="0">Traditional ( log ( ( N+l ) /n ) ) where N is number of documents in collection</definiens>
			</definition>
</paper>

		<paper id="1062">
</paper>

	</volume>
