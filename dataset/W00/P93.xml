<?xml version="1.0" encoding="UTF-8"?>
	<volume id="P93">

		<paper id="1018">
			<definition id="0">
				<sentence>Finite state translation systems ( fts ' ) were introduced as a computational model of transformational grammars .</sentence>
				<definiendum id="0">Finite state translation systems</definiendum>
				<definiens id="0">a computational model of transformational grammars</definiens>
			</definition>
			<definition id="1">
				<sentence>In lexical-functional grammars ( Ifg 's ) ( Kaplan 1982 ) , associated with each node v of a derivation tree is a finite set F of pairs of attribute names and their values .</sentence>
				<definiendum id="0">lexical-functional grammars</definiendum>
			</definition>
			<definition id="2">
				<sentence>A positive integer d ( A ) is given for each nonterminal symbol A • N. ( G2 ) T is a finite set of terminal symbols which is disjoint with N. ( G3 ) F is a finite set of functions satisfying the following conditions .</sentence>
				<definiendum id="0">positive integer d</definiendum>
				<definiens id="0">A ) is given for each nonterminal symbol A • N. ( G2 ) T is a finite set of terminal symbols which is disjoint with N. ( G3 ) F is a finite set of functions satisfying the following conditions</definiens>
			</definition>
			<definition id="3">
				<sentence>( fl ) For 1 &lt; h &lt; r ( f ) , the hth component of f , denoted by f\ [ h\ ] , is defined as ; f\ [ h\ ] \ [ Xl , f~2 , - '' - , Xa ( f ) \ ] = OCh , OX # ( h , O ) rl ( h , o ) Oth,1 • .</sentence>
				<definiendum id="0">OX #</definiendum>
				<definiens id="0">For 1 &lt; h &lt; r ( f ) , the hth component of f , denoted by f\ [ h\ ] , is defined as ; f\ [ h\ ] \ [ Xl , f~2 , - '' - , Xa ( f ) \ ] = OCh ,</definiens>
			</definition>
			<definition id="4">
				<sentence>( G4 ) P is a finite set of productions of the form A -- -* f\ [ A1 , A2 , ... , Aa ( y ) \ ] where A , Aa , A2 , ... , Aa ( / ) • N , f • F , r ( f ) = d ( A ) and di ( f ) = d ( Ai ) ( 1 &lt; i &lt; a ( f ) ) .</sentence>
				<definiendum id="0">, Aa</definiendum>
				<definiens id="0">a finite set of productions of the form A -- -* f\ [ A1 , A2 , ... , Aa ( y ) \ ] where A , Aa , A2 , ...</definiens>
			</definition>
			<definition id="5">
				<sentence>The language generated by a pmcfg G = ( N , T , F , P , S ) is defined as follows .</sentence>
				<definiendum id="0">S )</definiendum>
				<definiens id="0">language generated by a pmcfg G = ( N , T , F , P ,</definiens>
			</definition>
			<definition id="6">
				<sentence>For a given string w , it is decidable whether w E L ( G ) or not in time polynomial of I~1 , where I~1 denotes the length of w. A set ~ of symbols is a ranked alphabet if , for each cr E ~ , a unique non-negative number p ( c~ ) is associated , p ( cr ) is the rank of ~ .</sentence>
				<definiendum id="0">p ( cr )</definiendum>
				<definiens id="0">decidable whether w E L ( G ) or not in time polynomial of I~1 , where I~1 denotes the length of w. A set</definiens>
			</definition>
			<definition id="7">
				<sentence>Let G ( N , T , P , S ) be a context-free grammar ( cfg ) where N , T , P and S are a set of nonterminal symbols , a set of terminal symbols , a set of productions and the initial symbol , respectively .</sentence>
				<definiendum id="0">G ( N , T , P , S )</definiendum>
				<definiens id="0">a context-free grammar ( cfg ) where N , T , P and S are a set of nonterminal symbols , a set of terminal symbols , a set of productions and the initial symbol , respectively</definiens>
			</definition>
			<definition id="8">
				<sentence>A derivation tree in cfg G is a term defined as follows .</sentence>
				<definiendum id="0">derivation tree</definiendum>
				<definiens id="0">a term defined as follows</definiens>
			</definition>
			<definition id="9">
				<sentence>( T1 ) Every a E T is a derivation tree in G. ( T2 ) Assume that there are a production p : A -- -* X1 ... X , ~ ( A E N , XI , ... , Xn E NUT ) in P and n derivation trees tl , ... t , ~ whose roots are labeled with Pl , ... , pn , respectively , and • ifXi E N , then pl is a production Xi -- ~ `` `` , whose left-hand side is Xi , and • ifXiET , thenpi=ti=Xi .</sentence>
				<definiendum id="0">• ifXi E N</definiendum>
				<definiens id="0">E N , XI , ... , Xn E NUT ) in P and n derivation trees tl , ... t , ~ whose roots are labeled with Pl , ... , pn , respectively , and</definiens>
			</definition>
			<definition id="10">
				<sentence>Let T~ ( G ) be the set of derivation trees in G , and 7¢s ( G ) C 7¢ ( G ) be the set of derivation trees whose root is labeled with a production of which left-hand side is the initial symbol S. Clearly , T~s ( G ) C_ T~ ( ¢ ) holds .</sentence>
				<definiendum id="0">T~ ( G )</definiendum>
			</definition>
			<definition id="11">
				<sentence>, A , q0 , R ) where ( 1 ) Q is a finite set of states , ( 2 ) ~ is an input ranked alphabet , ( 3 ) A is an output alphabet , ( 4 ) q0 E Q is the initial state , and ( 5 ) R is a finite set of rules of the form q\ [ c~ ( xl , ... , xn ) \ ] -- * v where q e Q , e = and v e ( Z uQ\ [ { xl , ... , xn } \ ] ) * .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">a finite set of states</definiens>
				<definiens id="1">the initial state</definiens>
				<definiens id="2">a finite set of rules of the form q\ [ c~ ( xl , ... , xn ) \ ] -- * v where q e Q , e = and v e ( Z uQ\ [ { xl , ... , xn } \ ] ) *</definiens>
			</definition>
			<definition id="12">
				<sentence>We define yL ( M , G ) , called the yield language generated by yT-fts ( M , G ) , as yL ( M , a ) ~ { w e A* 13t e ~s ( a ) , qo\ [ t\ ] ~*M w } where A is an output alphabet and q0 is the initial state of M. An fts is called deterministic ( Engelfriet 1980 ) if the transducer M is deterministic .</sentence>
				<definiendum id="0">yL ( M , G )</definiendum>
				<definiendum id="1">yL ( M</definiendum>
				<definiendum id="2">q0</definiendum>
				<definiens id="0">called the yield language generated by yT-fts</definiens>
				<definiens id="1">the initial state of M. An fts is called deterministic ( Engelfriet 1980 ) if the transducer M is deterministic</definiens>
			</definition>
			<definition id="13">
				<sentence>* : q0\ [ t\ ] wherewi• A* forl &lt; i &lt; n+l. The state sequence of t ! in derivation a is defined to be ( qi~ , ... , qi. ) . Derivation a has copying-bound k if , for every subtree of t , the length of its state sequence is at most k. An fts ( M , G ) is a finite-copying , if there is a constant k and for each w • yL ( M , G ) , there is a derivation tree t in G and a derivation q0\ [ t\ ] ~ w with copying-bound k. It is known that the determinism does not weaken the generative capacity of finite-copying fts ' ( Engelfriet 1980 ) . We note that an fts ( M , G ) can be considered to be a model of a transformational grammar : A deep-structure of a sentence is represented by a derivation tree of G , and M can be considered to transform the deep-structure into a sentence ( or its surface structure ) . grammars A simple subclass of lfg 's , called r-lfg 's , is introduced in ( Nishino 1992 ) , which is shown to generate all the recursively enumerable languages ( Nakanishi 1992 ) . Here , we define a nondeterministic copying Ifg ( nc-lfg ) as a proper subclass of r-lfg's. An nc-lfg is defined to be a 6-tuple G = ( N , T , P , S , N~t~ , A~tr~ ) where : ( 1 ) N is a finite set of nonterminal symbols , ( 2 ) T is a finite set of terminal symbols , and ( 3 ) P is a finite set of annotated productions. Sometimes , a nonterminal symbol , a terminal symbol and an annotated production are abbreviated as a nonterminal , a terminal and a production , respectively , i 4 ) S • N is the initial symbol , ( 5 ) Nat~ is a finite set of attributes , and ( 6 ) A~tm is a finite set of atoms. An equation of the form T atr =~ ( atr • Nat , ) is called an S ( structure synthesizing ) schema , and an equation of the form T atr .-= val ( atr • Natr , val • A~tm ) is called a V ( immediate value ) schema. A functional schema is either an S schema or a V schema. Each production p • P has the following form : p : A -~ B1 B2 ... Bq , ( 4.2 ) Ev ESl Es2 `` '' Esq where A • N , B1 , B2 , . '' , Bq • NUT. Ev is a finite set of V schemata and Esj ( 1 _ &lt; j &lt; _ q ) is a singleton of an S schema. A -- ~ B1B2 '' .. Bq in ( 4.2 ) is called the underlying production of p. Let P0 be the set of all the underlying productions of P. Cfg Go = ( N , T , P0 , S ) is called the underlying c/g o/ C. An f-structure of G is recursively defined as a set F -= { ( atrl , call ) , ( atr2 , val2 &gt; , ... , latrk , valk ) } where atr\ ] , atr2 , ... , and atrk are distinct attributes , and each of vail , val2 , . '' ``</sentence>
				<definiendum id="0">G )</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">T</definiendum>
				<definiendum id="3">A~tm</definiendum>
				<definiendum id="4">, Bq • NUT. Ev</definiendum>
				<definiens id="0">qi~ , ... , qi. ) . Derivation a has copying-bound k if , for every subtree of t , the length of its state sequence is at most k. An fts ( M ,</definiens>
				<definiens id="1">a derivation tree t in G and a derivation q0\ [ t\ ] ~ w with copying-bound k. It is known that the determinism does not weaken the generative capacity of finite-copying fts ' ( Engelfriet 1980 ) . We note that an fts ( M , G ) can be considered to be a model of a transformational grammar : A deep-structure of a sentence is represented by a derivation tree of G , and M can be considered to transform the deep-structure into a sentence ( or its surface structure ) . grammars A simple subclass of lfg 's , called r-lfg 's , is introduced in ( Nishino 1992 ) , which is shown to generate all the recursively enumerable languages ( Nakanishi 1992 ) . Here</definiens>
				<definiens id="2">a nondeterministic copying Ifg ( nc-lfg ) as a proper subclass of r-lfg's. An nc-lfg is defined to be a 6-tuple G = ( N , T , P , S , N~t~ , A~tr~ ) where : ( 1 )</definiens>
				<definiens id="3">a finite set of nonterminal symbols</definiens>
				<definiens id="4">a finite set of terminal symbols , and ( 3 ) P is a finite set of annotated productions. Sometimes , a nonterminal symbol , a terminal symbol and an annotated production are abbreviated as a nonterminal , a terminal and a production , respectively , i 4 ) S • N is the initial symbol</definiens>
				<definiens id="5">a finite set of attributes</definiens>
				<definiens id="6">a finite set of atoms. An equation of the form T atr =~ ( atr • Nat , ) is called an S ( structure synthesizing ) schema , and an equation of the form T atr .-= val ( atr • Natr , val • A~tm ) is called a V ( immediate value ) schema. A functional schema is either an S schema or a V schema. Each production p • P has the following form : p : A -~ B1 B2 ... Bq , ( 4.2 ) Ev ESl Es2 `` '' Esq where A • N , B1 , B2 , . ''</definiens>
				<definiens id="7">a finite set of V schemata and Esj ( 1 _ &lt; j &lt; _ q ) is a singleton of an S schema. A -- ~ B1B2 '' .. Bq in ( 4.2 ) is called the underlying production of p. Let P0 be the set of all the underlying productions of P. Cfg Go = ( N , T , P0 , S ) is called the underlying c/g o/ C. An f-structure of G is recursively defined as a set F -= { ( atrl , call ) , ( atr2 , val2 &gt; , ... , latrk , valk ) } where atr\ ] , atr2 , ... , and atrk are distinct attributes , and each of vail , val2 , . ''</definiens>
			</definition>
			<definition id="14">
				<sentence>Productions pl , ' '' , Pn are consistent iff Ul &lt; i &lt; _n E ( 0 is consistent where E ( / ) is the set of functional schemata of Pl. If productions are not consistent then they are called inconsistent. An nc-lfg G is called a deterministically copying Ifg ( dc-lfg ) , if any two productions A -- + al and A -- + a2 whoes left-hand sides are the same are inconsistent. Suppose G = ( N , T , P , S , Nat , , Aatm ) is an nc-lfg. Let { { el , e2 , -'. , en } } denote the multiset which consists of elements el , e2 , '' • • , en that are not necessarily distinct. An SPN ( SubPhrase Nonterminal ) multiset in G is recursively defined as the following 1 through 3 : Ah E N ) is an SPN multiset. Let A1 -- ~ al , 134 • . ' , Ah ~ O : h be consistent productions. For each atr E Nat , , let MS~ , ~ be the multiset consisting of all the nonterminals which appear in al , ' '' , ah and have an S schema T atr -- l. If MSat~ is not empty , then MS~t~ is also an SPN multiset. An nc-lfg such that the number of SPN multisets in G is finite is called a finite-copying lfg ( fc-lfg ) . Example 4.4 : Consider GEX s in Example 4.1. Productions /912 and P14 are inconsistent with each other and so are P13 and Ply. SPN multisets in GEX3 are { { S } } and { { A , B ) ) . Hence GEXS is a dc-lfg and is an fc-lfg. GEX5 is also a dc-lfg and is an fc-lfg by the similar reason. Similarly , GEX4 in Example 4.2 is a dc-lfg. SPN multisets in C~x~ are { { S } } , { { S , S } ) , { { S , S , S , S ) } , ... . Hence GEx4 is not an fc-lfg. NOTE : L ( GExs ) is generated by a tree adjoining grammar. Suppose that a sentence has three or more phrases which have co-occurrence relation like the one between the subject phrase and the verb phrase in the `` respectively '' sentence. Tree adjoining grammars can not generate such syntax while fc-lfg 's or dc-lfg 's can , although the authors do not know a natural language which has such syntax so far. By Lemma 2.1 and Theorem 8.1 , fc-lfg 's are polynomial-time recognizable. Hence , it is desirable that whether a given lfg G is an fc-lfg or not is decidable. Fortunately , it is decidable by the following lemma. Lemma 4.1 : For a given nc-lfg G , it is decidable whether the number of SPN multisets in G is finite or infinite. Proof. The problem can be reduced to the boundedness problem of Petri nets , which is known to be decidable ( Peterson 1981 ) . Let ~'nc-lfg , ~'dc-lfg and ~-'fc-lfg denote the classes of languages generated by nc-lfg 's , dc-lfg 's and fc-lfg 's , respectively , and let y~ # , , Y~.d-fts and YElc- # s denote the classes of yield languages generated by fts ' , deterministic fts ' and finite-copying fts ' , respectively. Let l : vmcla and £ : mcfg be the classes of languages generated by pmcfg 's and mcfg 's , respectively. Also let £ : ta9 be the class of language generated by tree adjoining grammars. Inclusion relations among these classes of languages are summarized in Figure 2. An equivalence relation *1 is shown in ( Weir 1992 ) . Relations *2 are new results which we prove in this paper. We also note that all the inclusion relations are proper ; indeed , 0 l { ala2a3a41n &gt; _ E D E a a2 n n _ ... .. a2m_la2m \ [ n &gt; E C D for m &gt; 3 , ( by ( Vijay-Shanker 1987 ) . )</sentence>
				<definiendum id="0">Productions pl , ' ''</definiendum>
			</definition>
			<definition id="15">
				<sentence>Let T ' = A td { b } where b is a newly introduced symbol and let N ' = { S ' , RI , ... , Rm , AI , ... , An } where d ( Ri ) = d ( Aj ) = t for 1 &lt; i &lt; _ m and 1 &lt; j &lt; _ n. Productions and functions of G ~ will be constructed to have the following property .</sentence>
				<definiendum id="0">b</definiendum>
				<definiens id="0">a newly introduced symbol and let N ' = { S ' , RI , ... , Rm , AI , ... , An } where d ( Ri ) = d ( Aj ) = t for 1 &lt; i &lt; _ m and 1 &lt; j &lt; _ n. Productions and functions of G</definiens>
			</definition>
			<definition id="16">
				<sentence>Step 1 : For each production Ph : Iio -- '* Y~ `` `` Yk ( Yo ~ N , Y= E NtoT for 1 &lt; u &lt; k ) of cfg G , construct nonterminating productions Rh -+ \ [ &amp; , ... , zk\ ] for every Z~ E RS ( Y~ ) ( 1 &lt; u &lt; k ) , where fph is defined as follows : For 1 &lt; i &lt; g , • if the transducer M has no rule whose lefthand side is qi~ah ( Xl , ... , xk ) \ ] , then ( 6 .</sentence>
				<definiendum id="0">Y= E NtoT</definiendum>
				<definiens id="0">for 1 &lt; u &lt; k ) of cfg G , construct nonterminating productions Rh -+ \ [ &amp; , ... , zk\ ] for every Z~ E RS ( Y~ ) ( 1 &lt; u &lt; k )</definiens>
				<definiens id="1">follows : For 1 &lt; i &lt; g , • if the transducer M has no rule whose lefthand side is qi~ah ( Xl , ... , xk ) \ ]</definiens>
			</definition>
			<definition id="17">
				<sentence>( Since M is deterministic , there exists at most one rule whose left-hand side is qi~h ( ' '' `` ) \ ] and hence the above construction is well defined• ) Step 2 : For each ah E T , construct a terminating production Ah - '' + fah where f~h is defined as follows : For 1 &lt; i &lt; i , • if M has no rule whose left-hand side is qi\ [ ah\ ] , then ~a~\ [ i\ ] ~ -- b. • ifM has a rule qi\ [ ah\ ] -- + hi , then f\ [ ~ &amp; ai .</sentence>
				<definiendum id="0">f~h</definiendum>
				<definiens id="0">no rule whose left-hand side is qi\ [ ah\ ] , then ~a~\ [ i\ ] ~ -- b. • ifM has a rule qi\ [ ah\ ] -- + hi</definiens>
			</definition>
			<definition id="18">
				<sentence>pmcfg , and hence every language in this class can be recognized in time polynomial of the length of an input string• Our result here is : there is a nondeterministic fts that generates an A/'~-complete language• In the following , a language called Unary-3SAT , which is ArT'-complete ( Nakanishi 1992 ) , is considered , and then it is shown to belong to yL : / , a. A Unary-3CNF is a ( nonempty ) 3CNF in which the subscripts of variables are represented in unary .</sentence>
				<definiendum id="0">a. A Unary-3CNF</definiendum>
				<definiens id="0">a ( nonempty ) 3CNF in which the subscripts of variables are represented in unary</definiens>
			</definition>
			<definition id="19">
				<sentence>Unary-3SAT is the set of all satisfiable Unary3CNF 's .</sentence>
				<definiendum id="0">Unary-3SAT</definiendum>
				<definiens id="0">the set of all satisfiable Unary3CNF 's</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>Introduction Bach ( 1986:12 ) summarizes THE IMPERFECTIVE PARADOX ( Dowty 1979 ) as follows : `` ... how can we characterize the meaning of a progressive sentence like ( la ) \ [ 17\ ] on the basis of the meaning of a simple sentence like ( lb ) \ [ 18\ ] when ( la ) can be true of a history without ( lb ) ever being true ? ''</sentence>
				<definiendum id="0">IMPERFECTIVE PARADOX</definiendum>
				<definiens id="0">the meaning of a progressive sentence like ( la ) \ [ 17\ ] on the basis of the meaning of a simple sentence like ( lb</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>A category such as S\NP is a function representing an intransitive verb ; the function looks for an argument of type NP on its left and results in the category S. A small set of combinatory rules serve to combine these categories while preserving a transparent relation between syntax and semantics .</sentence>
				<definiendum id="0">S\NP</definiendum>
				<definiens id="0">a function representing an intransitive verb</definiens>
			</definition>
			<definition id="1">
				<sentence>Based on the formal definition of CCGs in ( Weir-Joshi 1988 ) , a CCG , G , is denoted by ( VT , VN , S , f , R ) , where • lit is a finite set of terminals , • VN is a finite set of nonterminals , • S is a distinguished member of VN , • f is a function that maps elements of VT U { e } to finite subsets of C ( VN ) , the set of categories , where , VN C C ( VN ) and if el and c2 • C ( VN ) , then ( el\c2 ) and ( cl/c2 ) • *I would like to thank Mark Steedman , Libby Levison , Owen Rambow , and the anonymous referees for their valuable advice .</sentence>
				<definiendum id="0">lit</definiendum>
				<definiendum id="1">VN</definiendum>
				<definiendum id="2">S</definiendum>
				<definiendum id="3">VN C C ( VN</definiendum>
				<definiens id="0">a CCG , G , is denoted by ( VT , VN , S , f</definiens>
				<definiens id="1">a finite set of nonterminals , •</definiens>
			</definition>
			<definition id="2">
				<sentence>• R is a finite set of combinatory rules where X , Y , Z1 , • • .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">a finite set of combinatory rules where X</definiens>
			</definition>
			<definition id="3">
				<sentence>The derives relation in a CCG is defined as act =~ cecl c2fl if R contains the rule Cl c2 -- -+ c. The language generated by this grammar is defined as L ( G ) = { al , ... , an \ [ S : ~ c , , ... , cn , ei • f ( al ) , ai • lit U { e } , 1 &lt; i &gt; n } Under these assumptions , Weir and Joshi ( 1988 ) prove that CCGs are weakly equivalent to TAGs , HGs , and LIGs .</sentence>
				<definiendum id="0">Weir</definiendum>
				<definiens id="0">act =~ cecl c2fl if R contains the rule Cl c2 -- -+ c. The language generated by this grammar is defined as L ( G ) = { al , ... , an \ [ S : ~ c , , ... , cn , ei • f ( al ) , ai • lit U { e } , 1 &lt; i &gt; n } Under these assumptions ,</definiens>
				<definiens id="1">weakly equivalent to TAGs , HGs , and LIGs</definiens>
			</definition>
			<definition id="4">
				<sentence>The Use of Variables Linguistic Use In CCGs , a type-raising rule can be used in the lexicon to convert basic elements into functions ; for example , an NP category can be type-raised to the category S/ ( S\NP ) representing a function looking for an intransitive verb on its right .</sentence>
				<definiendum id="0">NP category</definiendum>
			</definition>
			<definition id="5">
				<sentence>Steedman uses type-raising of NPs to capture syntactic coordination and extraction facts .</sentence>
				<definiendum id="0">Steedman</definiendum>
				<definiens id="0">uses type-raising of NPs to capture syntactic coordination and extraction facts</definiens>
			</definition>
			<definition id="6">
				<sentence>This rule can only apply to these categories if i = j , j = k , k = l , and l = m where m is the number of A 's generated and i , j , k , l are as in the complex categories listed above .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">the number of A 's generated and i</definiens>
			</definition>
			<definition id="7">
				<sentence>Thus , G ' generates a language that TAGs and CCGs can not .</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">generates a language that TAGs and CCGs can not</definiens>
			</definition>
			<definition id="8">
				<sentence>Although VAR-CCGs have a greater weak generative capacity than the class including TAGs , HGs , CCGs , and LIGs , we conjecture that it is still a mildly context-sensitive grammar as defined by Joshi ( 1985 ) .</sentence>
				<definiendum id="0">LIGs</definiendum>
				<definiendum id="1">context-sensitive grammar</definiendum>
				<definiens id="0">a greater weak generative capacity than the class including TAGs , HGs , CCGs , and</definiens>
			</definition>
			<definition id="9">
				<sentence>In future research , I will investigate whether VAR-CCGs is an adequate linguistic formalism in capturing all aspects of free word order languages or whether a formalism such as { } -CCGs ( Hoffman 1992 ) , which allows sets of arguments in function categories , is better suited .</sentence>
				<definiendum id="0">VAR-CCGs</definiendum>
				<definiens id="0">allows sets of arguments in function categories</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>Words are defined as units separated by spaces and then undergo statistical approximations .</sentence>
				<definiendum id="0">Words</definiendum>
				<definiens id="0">units separated by spaces</definiens>
			</definition>
			<definition id="1">
				<sentence>Tile error appears to follow froln the operation of the stochastic process itself .</sentence>
				<definiendum id="0">Tile error</definiendum>
				<definiens id="0">appears to follow froln the operation of the stochastic process itself</definiens>
			</definition>
			<definition id="2">
				<sentence>In a trigram model the probability of each word is calculated by taking into consideration two elements : the lexical probability ( probability of the word bearing a certain tag ) and the contextual probability ( probability of a word bearing a certain tag given two previous parts of speech ) .</sentence>
				<definiendum id="0">lexical probability ( probability</definiendum>
				<definiendum id="1">contextual probability</definiendum>
				<definiens id="0">of the word bearing a certain tag</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>A subcategorization frame is a statement of what types of syntactic arguments a verb ( or adjective ) takes , such as objects , infinitives , thatclauses , participial clauses , and subcategorized prepositional phrases .</sentence>
				<definiendum id="0">subcategorization frame</definiendum>
				<definiens id="0">a statement of what types of syntactic arguments a verb ( or adjective ) takes , such as objects , infinitives , thatclauses , participial clauses</definiens>
			</definition>
			<definition id="1">
				<sentence>Dictionaries produced by hand always substantially lag real language use .</sentence>
				<definiendum id="0">Dictionaries</definiendum>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>The Discourse Administrator is a development-time tool for defining the three discourse KB 's .</sentence>
				<definiendum id="0">Discourse Administrator</definiendum>
				<definiens id="0">a development-time tool for defining the three discourse KB 's</definiens>
			</definition>
			<definition id="1">
				<sentence>The Resolution Engine , on the other hand , is the run-time processing module which actually performs anaphora resolution using these discourse KB 's .</sentence>
				<definiendum id="0">Resolution Engine</definiendum>
				<definiens id="0">the run-time processing module which actually performs anaphora resolution using these discourse KB 's</definiens>
			</definition>
			<definition id="2">
				<sentence>Each knowledge source ( KS ) is an object in the hierarchically organized KB , and information in a specific KS can be inherited from a more general KS .</sentence>
				<definiendum id="0">KS )</definiendum>
				<definiens id="0">an object in the hierarchically organized KB , and information in a</definiens>
			</definition>
			<definition id="3">
				<sentence>Most of the KS 's are language-independent ( e.g. all the generators and the semantic type filters ) , and even when they are language-specific , the function 157 ( defframe discourse-world ( discourse-d*ta-structure ) date location topics position discourse-clauses s u b-discou rse-worlds~ ; DW date of the text ; loc~tion where the text is originated ; semantic concepts which correspond to globM topics of the text ; the corresponding character position in the text ; ~ list of discourse clauses in the current DW ; a list of DWs subordinate to the current one ( defframe discourse-clause ( discourse-d~ta-structure ; D ( : discourse-markers ; ~ list of discourse m~rkers in the current D ( : ~ syntax ; ~n f-structure for the current DC parse-tree ; ~ p~rse tree of this S semantics ; ~ semantic ( KB ) object representing the current DC position ; the corresponding character position in the text d~te ; date of the current DC~ loca .</sentence>
				<definiendum id="0">defframe discourse-clause</definiendum>
				<definiens id="0">object representing the current DC position ; the corresponding character position in the text d~te ; date of the current DC~ loca</definiens>
			</definition>
			<definition id="4">
				<sentence>Since texts in different domains exhibit different sets of discourse phenomena , and since different applications even within the same domain may not have to handle the same set of discourse phenomena , the discourse domain KB is a way to customize and constrain the workload of the discourse module .</sentence>
				<definiendum id="0">discourse domain KB</definiendum>
				<definiens id="0">a way to customize and constrain the workload of the discourse module</definiens>
			</definition>
			<definition id="5">
				<sentence>The Resolution Engine is the run-time processing module which finds the best antecedent hypothesis for a given anaphor by using data in both the global discourse world and the discourse KB 's .</sentence>
				<definiendum id="0">Resolution Engine</definiendum>
				<definiens id="0">the run-time processing module which finds the best antecedent hypothesis for a given anaphor by using data in both the global discourse world and the discourse KB 's</definiens>
			</definition>
			<definition id="6">
				<sentence>The Resolution Engine uses the discourse phenomenon KB to classify an anaphor as one of the discourse phenomena ( using dp-definition values ) and to determine a set of KS 's to apply to the anaphor ( using dp-main-strategy values ) .</sentence>
				<definiendum id="0">Resolution Engine</definiendum>
				<definiens id="0">uses the discourse phenomenon KB to classify an anaphor as one of the discourse phenomena ( using dp-definition values ) and to determine a set of KS 's to apply to the anaphor ( using dp-main-strategy values</definiens>
			</definition>
			<definition id="7">
				<sentence>Figure 4 : Discourse Phenomenon KB For each anaphoric discourse marker ill the current sentence : Find-Antecedent Input : aalaphor to resolve , global discourse world Get-KSs-for-Discourse-Phenomenon Input : anaphor to resolve , discourse phenomenon KB Output : a set of discourse KS 's Apply-KSs hlput : aalaphor to resolve , global discourse world , discourse KS 's Output : the best hypothesis Output : the best hypothesis Update-Discourse-World Input : anaphor , best hypothesis , global discourse world Output : updated global discourse world Figure 5 : Resolution Engine Operations KS 's .</sentence>
				<definiendum id="0">Output</definiendum>
				<definiens id="0">Discourse Phenomenon KB For each anaphoric discourse marker ill the current sentence : Find-Antecedent Input : aalaphor to resolve , global discourse world Get-KSs-for-Discourse-Phenomenon Input : anaphor to resolve , discourse phenomenon KB Output : a set of discourse KS 's Apply-KSs hlput : aalaphor to resolve , global discourse world , discourse KS 's</definiens>
				<definiens id="1">updated global discourse world Figure 5 : Resolution Engine Operations KS 's</definiens>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>CLASSIFICATION AND ABSTRACTION VIA CONTEXT PRIMING David J. Hutches Department of Computer Science and Engineering , Mail Code 0114 University of California , San Diego La Jolla , CA 92093-0114 dhutches @ ucsd.edu In this paper we discuss the results of experiments which use a context , essentially an ordered set of lexical items , as the seed from which to build a network representing statistically important relationships among lexical items in some corpus .</sentence>
				<definiendum id="0">CLASSIFICATION AND ABSTRACTION VIA CONTEXT PRIMING</definiendum>
				<definiens id="0">to build a network representing statistically important relationships among lexical items in some corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>The LOB corpus consists of a total of 1,008,035 words , composed of 49,174 unique words .</sentence>
				<definiendum id="0">LOB corpus</definiendum>
				<definiens id="0">consists of a total of 1,008,035 words , composed of 49,174 unique words</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>A wh question is the treated as : ( 7 ) ( wh x S ( and ( P1 x ) ( P2 x ) ) ) which is interpreted as a request to display all members of S ( the semantic class of the wh-np ) which satisfy both P1 ( the modifiers of the wh-np ) and P2 ( the predicate of the clause ) .</sentence>
				<definiendum id="0">wh question</definiendum>
				<definiens id="0">the semantic class of the wh-np ) which satisfy both P1 ( the modifiers of the wh-np )</definiens>
			</definition>
			<definition id="1">
				<sentence>( wh x airlines ( exists y flights ( and ( airline-of y x ) ( fly flight-of y orig-of Boston dest-of Denver ) ) ) ) 88 The readings of the referential ( 8 ) and and the predicative ( 9 ) are in a sense inside-out versions of each other .</sentence>
				<definiendum id="0">wh x airlines</definiendum>
				<definiens id="0">exists y flights ( and ( airline-of y x ) ( fly flight-of y orig-of Boston dest-of Denver</definiens>
			</definition>
			<definition id="2">
				<sentence>The proper reading is instead : ( 12 ) ( wh x airlines ( exists y flights ( and ( airline-of y x ) ( fly flight-of y orig-of Boston dest-of Denver ) ( leave flight-of y time-of ( 3 pm ) ) ) ) ) in which the airline is related to a single flight description that has all the desired properties .</sentence>
				<definiendum id="0">wh x airlines</definiendum>
			</definition>
			<definition id="3">
				<sentence>The operation of reading generation is to pick a non-IDENTITY R from N 's table , and apply the two schemas .</sentence>
				<definiendum id="0">operation of reading generation</definiendum>
				<definiens id="0">to pick a non-IDENTITY R from N 's table , and apply the two schemas</definiens>
			</definition>
			<definition id="4">
				<sentence>( wh x AIRLINE ( exists y FLIGHT ( AIRLINE-OF y x ) ( FLY flight-of y orig-of Boston dest-of Denver ) ) ) as desired .</sentence>
				<definiendum id="0">AIRLINE</definiendum>
				<definiens id="0">exists y FLIGHT ( AIRLINE-OF y x ) ( FLY flight-of y orig-of Boston dest-of Denver ) ) ) as desired</definiens>
			</definition>
			<definition id="5">
				<sentence>( wh x FLIGHT ( exists y JET ( and ( WIDE-BODY y ) ( AIRCRAFT-OF x y ) ( SERVE flight-of x meal-of DINNER ) ) ) ) In principle , of course , a given NP 's entry in the coercion table can have more than one distinct nonIDENTITY coercion relation .</sentence>
				<definiendum id="0">wh x FLIGHT</definiendum>
				<definiens id="0">exists y JET</definiens>
			</definition>
			<definition id="6">
				<sentence>In this light , the referential reading of the sentence above : ( wh x FLIGHT ( exists y AIRLINE ( AIRLINE-OF x y ) ) ( FLY flight-of x orig-of Boston dest-of Denver ) ) has a completely redundant component , since every flight is on some airline .</sentence>
				<definiendum id="0">FLIGHT</definiendum>
				<definiens id="0">exists y AIRLINE ( AIRLINE-OF x y ) ) ( FLY flight-of x orig-of Boston dest-of Denver ) ) has a completely redundant component</definiens>
			</definition>
			<definition id="7">
				<sentence>Then the referential coercion of the NP can be written as the pairing ( R , S ) , which describes a property on the domain of R that picks out just the subset of the domain of R that is obtained by mapping S back into the domain in the `` reverse '' direction of R. Such a property is considered vacuous if it provides no constraint on the domain , or in other words if : R is a total relation and S = ( RANGE R ) holds .</sentence>
				<definiendum id="0">S )</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">describes a property on the domain of R that picks out just the subset of the domain of</definiens>
			</definition>
			<definition id="8">
				<sentence>A total , or `` into '' , relation is one which maps every element of its domain to at least one element of its range .</sentence>
				<definiendum id="0">relation</definiendum>
				<definiens id="0">one which maps every element of its domain to at least one element of its range</definiens>
			</definition>
			<definition id="9">
				<sentence>Since every flight in ATIS is on an airline , AIRLINE-OF is a total relation , and AIRLINE is its range , so a referential metonymy is clearly vacuous in this case .</sentence>
				<definiendum id="0">AIRLINE-OF</definiendum>
				<definiens id="0">a total relation</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>A discourse reference time ( tl ) is introduced in ( 2a ) with the event of John going to Mary 's house at tl ) The past perfect in ( 2b ) introduces two times : John 's stopping at the flower shop ( t3 ) precedes the time t2 ( t3 -~ t2 ) , and t2 is typically inferred to be equal to the time of going over to Mary 's house ( tl ) ; hence t3 ~ tl .</sentence>
				<definiendum id="0">t2</definiendum>
				<definiens id="0">two times : John 's stopping at the flower shop ( t3 ) precedes the time t2 ( t3 -~ t2 ) , and</definiens>
			</definition>
			<definition id="1">
				<sentence>Tense trees reflect the structural dependencies among the tense and aspect operators in the interpretation of the sentences .</sentence>
				<definiendum id="0">Tense trees</definiendum>
				<definiens id="0">reflect the structural dependencies among the tense and aspect operators in the interpretation of the sentences</definiens>
			</definition>
			<definition id="2">
				<sentence>A discourse consists of a sequence of utterances uttl , ... , uttn .</sentence>
				<definiendum id="0">discourse</definiendum>
				<definiens id="0">consists of a sequence of utterances uttl , ... , uttn</definiens>
			</definition>
			<definition id="3">
				<sentence>The sentence grammar translates the content of each utterance utti into a ( set of ) surface logical form ( s ) containing unresolved anaphoric expressions and operators .</sentence>
				<definiendum id="0">sentence grammar</definiendum>
				<definiens id="0">translates the content of each utterance utti into a ( set of ) surface logical form ( s ) containing unresolved anaphoric expressions and operators</definiens>
			</definition>
			<definition id="4">
				<sentence>Utterance interpretation takes place in a context , and outputs an updated context .</sentence>
				<definiendum id="0">Utterance interpretation</definiendum>
				<definiens id="0">takes place in a context , and outputs an updated context</definiens>
			</definition>
			<definition id="5">
				<sentence>( 4 ) a. ( PASTrl 'John goes over to Mary 's house ' ) 73 r ( PASTr¢ ) = ( 3 e ( e C r ) A ( e starts_before SuperNow ) A ( Las $ In ( r ) orients e ) \ [ e ~ r ( ¢ ) \ ] ) r ( PERFr¢ ) = ( 3 e ( e C r ) A ( e starts_before SuperRef ) A ( LastIn ( r ) orients e ) Ce ~ r ( ¢ ) \ ] ) Figure 3 : Mapping rules for PAST and PERF b. ( PASTr2 ( PERFr3 'John stops by the florist for some roses ' ) ) The temporal operators in our logical forms are translated into the language of what we call the event structure representation ( ESR ) .</sentence>
				<definiendum id="0">PASTrl 'John</definiendum>
				<definiens id="0">goes over to Mary 's house '</definiens>
			</definition>
			<definition id="6">
				<sentence>In essence , ESR represents the temporal and causal relations among the eventualities described in discourse .</sentence>
				<definiendum id="0">ESR</definiendum>
				<definiens id="0">the temporal and causal relations among the eventualities described in discourse</definiens>
			</definition>
			<definition id="7">
				<sentence>Superllo~ and SuperRef have values determined by the position of the subformula 0PC in the logical form .</sentence>
				<definiendum id="0">Superllo~</definiendum>
				<definiens id="0">values determined by the position of the subformula 0PC in the logical form</definiens>
			</definition>
			<definition id="8">
				<sentence>SuperNow evaluates to the current utterance intervalthe time interval in which the current utterance takes place .</sentence>
				<definiendum id="0">SuperNow</definiendum>
				<definiens id="0">evaluates to the current utterance intervalthe time interval in which the current utterance takes place</definiens>
			</definition>
			<definition id="9">
				<sentence>SuperNow evaluates to Ub , yielding t2 starts_before Ub .</sentence>
				<definiendum id="0">SuperNow</definiendum>
			</definition>
			<definition id="10">
				<sentence>For example , in the for75 mula ( PAST ( PEP~F ¢ ) ) , the temporal position of ( PAST ( PERF ¢ ) ) is the empty sequence ( ) , that of ( PERF ¢ ) is the sequence { PAST ) , and that of ¢ is the sequence ( PAST , PERF ) .</sentence>
				<definiendum id="0">PAST</definiendum>
				<definiens id="0">the empty sequence</definiens>
				<definiens id="1">the sequence { PAST ) , and that of ¢ is the sequence</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>Structural explanations ( in the form of constraints on syntactic movement ) have also been proposed to explain the constraint that prevents a quantifier to take scope outside the clause in which it appears , first observed by May \ [ 28\ ] and called Scope Constraint by Heim \ [ 13\ ] .</sentence>
				<definiendum id="0">Structural explanations</definiendum>
				<definiens id="0">in the form of constraints on syntactic movement ) have also been proposed to explain the constraint that prevents a quantifier to take scope outside the clause in which it appears</definiens>
			</definition>
			<definition id="1">
				<sentence>The DRS construction algorithm consists of a set of rules that map discourses belonging to the language into certain `` interpretive structures '' .</sentence>
				<definiendum id="0">DRS construction algorithm</definiendum>
				<definiens id="0">consists of a set of rules that map discourses belonging to the language into certain</definiens>
			</definition>
			<definition id="2">
				<sentence>A DRS is a pair consisting of a set of discourse referents and a set of conditions ( = predicates on the discourse referents ) .</sentence>
				<definiendum id="0">DRS</definiendum>
				<definiens id="0">a pair consisting of a set of discourse referents and a set of conditions ( = predicates on the discourse referents )</definiens>
			</definition>
			<definition id="3">
				<sentence>A situation is a set of objects and facts about these objects \ [ 8 , 18\ ] .</sentence>
				<definiendum id="0">situation</definiendum>
				<definiens id="0">a set of objects and facts about these objects</definiens>
			</definition>
			<definition id="4">
				<sentence>82 used here , and elsewhere in the paper , a linear notation to save space ) : ( 21 ) \ [ CP \ [ IP \ [ NP '~ , Q ( V s \ [ s'2 ~ SCHOOL ( s ) \ ] Q ( s ) ) \ ] \ [ vP \ [ vP \ [ v ' 'SENT \ [ NP 'Z Q ( the p \ [ s'3 ~ PRINCIPAI~ , ~ ) \ ] ^ SHARED ( spkr , hearer $ 3 ) Q ( P ) ) \ ] \ ] \ ] \ [ pp 'TO \ [ NP 'Z Q ( the m \ [ s'l ~ MEmOs ( m ) \ ] ^ SHARED ( spkr , hearer , ,q 0 Q ( m ) ) llll\ ] I propose that expressions like ( 21 ) can appear as conditions of DRSs .</sentence>
				<definiendum id="0">'SENT \</definiendum>
				<definiendum id="1">NP 'Z Q</definiendum>
				<definiens id="0">V s \ [ s'2 ~ SCHOOL ( s ) \ ] Q ( s ) ) \ ] \ [ vP \ [ vP \ [ v '</definiens>
			</definition>
			<definition id="5">
				<sentence>Q ( det x R ( x ) ) Q ( x ) \ ] \ ] EXT ( ( 22 ) ) = { ( ~ x ( det x R ( x ) ) P ( x ) ) , ( 23 ) ( P , ) ~ Q ( det x R ( x ) ) Q ( x ) ) } I omit here the definition of the EXT function implementing Cooper storage , that is rather complex .</sentence>
				<definiendum id="0">det x R</definiendum>
				<definiens id="0">det x R ( x ) ) Q ( x ) ) } I omit here the definition of the EXT function implementing Cooper storage</definiens>
			</definition>
			<definition id="6">
				<sentence>( 24 ) EXT ( ( 21 ) ) = { the function denoted by II ( the m \ [ s'~ p ~L~tNG ( m ) \ ] ^ SnARED ( spkr , hearer , g0 ( V S \ [ ~ '' 2 \ [ = SCHOOL ( s ) \ ] ( the p \ [ g3 ~ PRINCIPAL ( p , s ) \ ] ^ SHARED ( spkr , hearer , g3 ) s~ , rr ( s , p , x ) ) ) ) II , the function denoted by II ( v s \ [ ~2 h scnooL~s ) \ ] ( the m \ [ ~¢x ~ MEElqNG ( m ) \ ] ^ SI/ARED ( spkr , hearer , ~l ) ( the p \ [ ~3 P PRn~Cn'AL ( p , s ) \ ] ^ SrlARED ( spkr , hearer , g3 ) s~cr ( s , p , x ) ) ) ) II , et~ } Having done this , we can say that a DRS condition like ( 21 ) is verifies the current situation s if one of the functions denoted by ( 21 ) maps s into 1 .</sentence>
				<definiendum id="0">EXT</definiendum>
				<definiendum id="1">hearer</definiendum>
				<definiens id="0">the p \ [ ~3 P PRn~Cn'AL ( p , s ) \ ] ^ SrlARED ( spkr , hearer</definiens>
				<definiens id="1">s , p , x</definiens>
				<definiens id="2">verifies the current situation s if one of the functions denoted by ( 21 ) maps s into 1</definiens>
			</definition>
			<definition id="7">
				<sentence>A situation description is a condition of the form : \ [ - '' '' -- '3 s : l~ \ [ ( 25 ) i -I 83 whose intuitive interpretation is that • provides a partial characterization of the situation s. The semantics of situation descriptions is defined as follows , using a semantics of DRSs in terms of situation extensions , as discussed in the previous section , and interpreting discourse markers as constituents of situations : The condition s : K is satisfied wrt the situation s ' iffK is satisfied wrt the value assigned to s in s ' .</sentence>
				<definiendum id="0">situation description</definiendum>
				<definiens id="0">a condition of the form : \ [ - '' '' -- '3 s : l~ \ [ ( 25 ) i -I 83 whose intuitive interpretation is that • provides a partial characterization of the situation s. The semantics of situation descriptions is defined as follows , using a semantics of DRSs in terms of situation extensions , as discussed in the previous section , and interpreting discourse markers as constituents of situations : The condition s : K is satisfied wrt the situation s ' iffK is satisfied wrt the value assigned to s in s '</definiens>
			</definition>
			<definition id="8">
				<sentence>Q ( the y \ [ 3 I = P ( y ) \ ] Q ( y ) ) YY ANCHOR ( % S ' ) ( 29 ) s : !</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">YY ANCHOR</definiendum>
				<definiens id="0">the y \ [ 3 I = P ( y ) \ ] Q ( y ) )</definiens>
			</definition>
			<definition id="9">
				<sentence>Definite descriptions are the paradigmatic case of an operator that tends to take wide scope .</sentence>
				<definiendum id="0">Definite descriptions</definiendum>
				<definiens id="0">the paradigmatic case of an operator that tends to take wide scope</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>Statistical data about these various cooccurrence relations is employed for a variety of applications , such as speech recognition ( Jelinek , 1990 ) , language generation ( Smadja and McKeown , 1990 ) , lexicography ( Church and Hanks , 1990 ) , machine translation ( Brown et al. , ; Sadler , 1989 ) , information retrieval ( Maarek and Smadja , 1989 ) and various disambiguation tasks ( Dagan et al. , 1991 ; Hindle and Rooth , 1991 ; Grishman et al. , 1986 ; Dagan and Itai , 1990 ) .</sentence>
				<definiendum id="0">language generation</definiendum>
				<definiendum id="1">information retrieval</definiendum>
				<definiens id="0">employed for a variety of applications , such as speech recognition</definiens>
			</definition>
			<definition id="1">
				<sentence>A cooccurrence pair can be viewed as a generalization of a bigram , where a bigram is a cooccurrence pair with d = 1 ( without ignoring function words ) .</sentence>
				<definiendum id="0">cooccurrence pair</definiendum>
				<definiens id="0">a cooccurrence pair with d = 1 ( without ignoring function words )</definiens>
			</definition>
			<definition id="2">
				<sentence>The mutual information of a cooccurrence pair , which measures the degree of association between the two words ( Church and Hanks , 1990 ) , is defined as ( Fano , 1961 ) : P ( xly ) I ( x , y ) -log 2 P ( x , y ) _ log 2 ( 1 ) P ( x ) P ( y ) P ( x ) = log 2 P ( y\ [ x ) P ( Y ) where P ( x ) and P ( y ) are the probabilities of the events x and y ( occurrences of words , in our case ) and P ( x , y ) is the probability of the joint event ( a cooccurrence pair ) .</sentence>
				<definiendum id="0">mutual information of a cooccurrence pair</definiendum>
				<definiendum id="1">P ( x ) P</definiendum>
				<definiendum id="2">P (</definiendum>
				<definiens id="0">measures the degree of association between the two words</definiens>
				<definiens id="1">the probability of the joint event ( a cooccurrence pair )</definiens>
			</definition>
			<definition id="3">
				<sentence>N f ( x , y ) \ ] I ( x , y ) = log~ P~x ) P -- ( y ) ( -d f ( x ) f ( y ) `` ( 2 ) where f denotes the frequency of an eyent and N is the length of the corpus .</sentence>
				<definiendum id="0">N f</definiendum>
				<definiendum id="1">-d f ( x ) f</definiendum>
				<definiendum id="2">f</definiendum>
				<definiendum id="3">N</definiendum>
				<definiens id="0">y ) \ ] I ( x , y ) = log~ P~x ) P -- ( y )</definiens>
				<definiens id="1">the frequency of an eyent</definiens>
				<definiens id="2">the length of the corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>The similarity metric therefore measures the degree of similarity between these mutual information values .</sentence>
				<definiendum id="0">similarity metric therefore</definiendum>
				<definiens id="0">measures the degree of similarity between these mutual information values</definiens>
			</definition>
			<definition id="5">
				<sentence>We first define the similarity between the mutual information values of Wl and w2 relative to a single other word , w. Since cooccurrence pairs are directional , we get two measures , defined by the position of w in the pair .</sentence>
				<definiendum id="0">w. Since cooccurrence pairs</definiendum>
				<definiens id="0">the similarity between the mutual information values of Wl and w2 relative to a single other word ,</definiens>
			</definition>
			<definition id="6">
				<sentence>The left context similarity of wl and w2 relative to w , termed simL ( Wl , w2 , w ) , is defined as the ratio between the two mutual information values , having the larger value in the denominator : simL ( wl , w2 , w ) = min ( I ( w , wl ) , I ( w , w2 ) ) ( 4 ) max ( I ( w , wl ) , I ( w , w2 ) ) 3The frequency based estimate for the expected frequency of a cooccurrence pair , assuming independent occurrence of the two words and using their individual frequencies , is -~f ( wz ) f ( w2 ) .</sentence>
				<definiendum id="0">I</definiendum>
				<definiendum id="1">w2 ) ) ( 4 ) max ( I</definiendum>
				<definiens id="0">left context similarity of wl and w2 relative to w , termed simL ( Wl , w2 , w )</definiens>
				<definiens id="1">the ratio between the two mutual information values , having the larger value in the denominator : simL ( wl , w2 , w ) = min ( I ( w , wl )</definiens>
				<definiens id="2">w , wl ) , I ( w , w2 ) ) 3The frequency based estimate for the expected frequency of a cooccurrence pair , assuming independent occurrence of the two words and using their individual frequencies , is -~f ( wz ) f ( w2 )</definiens>
			</definition>
			<definition id="7">
				<sentence>We therefore search first for all the `` strong neighbors '' of w , which are defined as words whose cooccurrence with w has high mutual information and high frequency , and then search for all their `` strong neighbors '' .</sentence>
				<definiendum id="0">strong neighbors '' of w</definiendum>
				<definiens id="0">words whose cooccurrence with w has high mutual information and high frequency</definiens>
			</definition>
			<definition id="8">
				<sentence>Working with analogical semantics : Disambiguation techniques in DLT .</sentence>
				<definiendum id="0">analogical semantics</definiendum>
				<definiens id="0">Disambiguation techniques in DLT</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>A commuaicati~e goal ( e.g. , `` achieve the state where the hearer believes that an action could be improved '' ) is formed based upon the student 's question .</sentence>
				<definiendum id="0">commuaicati~e goal</definiendum>
				<definiens id="0">formed based upon the student 's question</definiens>
			</definition>
			<definition id="1">
				<sentence>The dialogue history is a record of the conversation that has occurred thus far and includes the user 's utterances as well as the text plans that led to the system 's responses .</sentence>
				<definiendum id="0">dialogue history</definiendum>
			</definition>
			<definition id="2">
				<sentence>SHERLOCK evaluates each student action by determining which facets apply to that action .</sentence>
				<definiendum id="0">SHERLOCK</definiendum>
				<definiens id="0">evaluates each student action by determining which facets apply to that action</definiens>
			</definition>
			<definition id="3">
				<sentence>To evaluate an action , SHERLOCK finds each facet that applies to it and determines whether that facet should be considered good ( g ) , bad ( b ) , or neutral ( n ) given the current problem-solving context .</sentence>
				<definiendum id="0">SHERLOCK</definiendum>
				<definiens id="0">finds each facet that applies to it and determines whether that facet should be considered good ( g ) , bad ( b ) , or neutral ( n ) given the current problem-solving context</definiens>
			</definition>
			<definition id="4">
				<sentence>The root of the DAG represents the current action and the facets of interest ( b-facets in our example ) that apply to it .</sentence>
				<definiendum id="0">DAG</definiendum>
				<definiens id="0">the current action</definiens>
			</definition>
			<definition id="5">
				<sentence>Basically , the similarity DAG is a partial ordering of the student 's actions based on their facet lists .</sentence>
				<definiendum id="0">DAG</definiendum>
				<definiens id="0">a partial ordering of the student 's actions based on their facet lists</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>Functional uncertainty consists of constraints of *This work was supported by a research grant , ITW 9002 0 , from the German Bundesministerium ffir Forschung und Technologic to the DFKI project DISCO .</sentence>
				<definiendum id="0">Functional uncertainty</definiendum>
				<definiens id="0">consists of constraints of *This work was supported by a research grant , ITW 9002 0 , from the German Bundesministerium ffir Forschung und Technologic to the DFKI project DISCO</definiens>
			</definition>
			<definition id="1">
				<sentence>For space limitations most of the proofs are omitted ; they can be found in the complete paper \ [ 2\ ] the form xLy , where L is a finite description of a regular language of feature paths .</sentence>
				<definiendum id="0">L</definiendum>
				<definiens id="0">a finite description of a regular language of feature paths</definiens>
			</definition>
			<definition id="2">
				<sentence>Note that a constraint zwy is a special case of a functional uncertainty constraint .</sentence>
				<definiendum id="0">constraint zwy</definiendum>
				<definiens id="0">a special case of a functional uncertainty constraint</definiens>
			</definition>
			<definition id="3">
				<sentence>Our signature consists of a set of sorts S ( A , B , ... ) , first order variables X ( z , y , ... ) , path variables 7 9 ( a , /3 , ... ) and features Jr ( f , g , ... ) .</sentence>
				<definiendum id="0">signature</definiendum>
				<definiens id="0">consists of a set of sorts S ( A , B , ... ) , first order variables X ( z , y , ... ) , path variables 7 9 ( a , /3 , ... ) and features Jr ( f , g , ... )</definiens>
			</definition>
			<definition id="4">
				<sentence>A path is a finite string of features .</sentence>
				<definiendum id="0">path</definiendum>
				<definiens id="0">a finite string of features</definiens>
			</definition>
			<definition id="5">
				<sentence>A path u is a prefix of a path v ( written u ~ v ) if there is a non-empty path w such that v = uw .</sentence>
				<definiendum id="0">path u</definiendum>
				<definiens id="0">a prefix of a path v</definiens>
			</definition>
			<definition id="6">
				<sentence>A valuation is a pair ( Vx , VT~ ) , where Vx is a standard first order valuation of the variables in X and Vv is a function V~v : P -- -+ ~'+ .</sentence>
				<definiendum id="0">valuation</definiendum>
				<definiendum id="1">Vx</definiendum>
				<definiendum id="2">Vv</definiendum>
				<definiens id="0">a standard first order valuation of the variables in X</definiens>
				<definiens id="1">a function V~v : P -- -+ ~'+</definiens>
			</definition>
			<definition id="7">
				<sentence>We define V~ , ( a.fl ) to be VT , ( a ) V~ , ( 13 ) , The validity of an atomic constraint in an interpretation 2 '' under a valuation ( Vx , V~ , ) is defined as follows : ( Vx , V~ , ) ~z Ax : ¢= : ~ Vx ( x ) e A z ( Vx , Vr ) Pz = Y : ¢=~ Vx ( = ) = Vx ( U ) ( vx , vr ) ~z zpy ( vx , vv ) ~z = .</sentence>
				<definiendum id="0">V~ ,</definiendum>
				<definiens id="0">follows : ( Vx , V~ , ) ~z Ax : ¢= : ~ Vx ( x ) e A z ( Vx , Vr ) Pz = Y : ¢=~ Vx ( = ) = Vx ( U ) ( vx , vr ) ~z zpy</definiens>
			</definition>
			<definition id="8">
				<sentence>Lemma 4.1 A pre-soived clause ¢ is consistent iff there is a path valuation V~ , with VT~ ~ Cp , where Cp is the set of path constraints in ~ .</sentence>
				<definiendum id="0">Cp</definiendum>
				<definiens id="0">the set of path constraints in ~</definiens>
			</definition>
			<definition id="9">
				<sentence>Ai ) is a set of regular languages that contains Li and is closed under decomposition .</sentence>
				<definiendum id="0">Ai )</definiendum>
				<definiens id="0">a set of regular languages that contains Li and is closed under decomposition</definiens>
			</definition>
			<definition id="10">
				<sentence>Jinx dec ( Ai ) contains each Li and is closed under decomposition .</sentence>
				<definiendum id="0">Jinx dec</definiendum>
				<definiens id="0">contains each Li and is closed under decomposition</definiens>
			</definition>
			<definition id="11">
				<sentence>Let A = fi ( A0 ) be the least set that contains A0 and is closed under intersection .</sentence>
				<definiendum id="0">A = fi ( A0</definiendum>
				<definiens id="0">the least set that contains A0 and is closed under intersection</definiens>
			</definition>
			<definition id="12">
				<sentence>A ' , O fi0 ( A~ ) , where fl is new , A~ = { a~ , ... , a~ } is a disjoint copy of A1 = { or1 , ... , an } and A fi .</sentence>
				<definiendum id="0">fl</definiendum>
				<definiens id="0">a disjoint copy of A1 = { or1 , ... , an } and A fi</definiens>
			</definition>
			<definition id="13">
				<sentence>A~ is an abbreviation for the clause { al fl'a~ , ... , c~ , fl .</sentence>
				<definiendum id="0">A~</definiendum>
			</definition>
			<definition id="14">
				<sentence>A~ is a disjoint copy of A1 .</sentence>
				<definiendum id="0">A~</definiendum>
				<definiens id="0">a disjoint copy of A1</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>105 -- semantic level ( what the speaker l has in mind ) -- karaka level I -- vibhakti level I -- surface level ( uttered sentence ) Fig .</sentence>
				<definiendum id="0">semantic level</definiendum>
				<definiens id="0">what the speaker l has in mind ) -- karaka level I -- vibhakti level I -- surface level ( uttered sentence ) Fig</definiens>
			</definition>
			<definition id="1">
				<sentence>It explains the vibhakti in sentences B.1 to B.4 , where Ram is the karta but has different vibhaktis , ¢ , he , ko , se , respectively .</sentence>
				<definiendum id="0">Ram</definiendum>
				<definiens id="0">the karta but has different vibhaktis , ¢ , he , ko , se , respectively</definiens>
			</definition>
			<definition id="2">
				<sentence>These are the word groups at the vibhakti level ( i.e. , typically each word group is a noun or verb with its vibhakti , TAM label , etc. ) .</sentence>
				<definiendum id="0">vibhakti level</definiendum>
				<definiens id="0">a noun or verb with its vibhakti</definiens>
			</definition>
			<definition id="3">
				<sentence>\ ] ED which in the worst case is O ( n 3 ) where n is the number of word groups in the sentence being parsed .</sentence>
				<definiendum id="0">ED</definiendum>
				<definiendum id="1">n</definiendum>
			</definition>
			<definition id="4">
				<sentence>Consequently , the vibhakti 'dvArA ' for karta ( Ram ) follows from the transformation already given earlier in Fig .</sentence>
				<definiendum id="0">vibhakti 'dvArA</definiendum>
				<definiendum id="1">Ram )</definiendum>
				<definiens id="0">follows from the transformation already given earlier in Fig</definiens>
			</definition>
			<definition id="5">
				<sentence>In G.1 , Ram is the karta of both the verbs : KA ( eat ) and bulA ( call ) .</sentence>
				<definiendum id="0">Ram</definiendum>
				<definiens id="0">the karta of both the verbs</definiens>
			</definition>
			<definition id="6">
				<sentence>For example , kara is the TAM label 6 of the intermediate verb groups in G.1 and G.2 ( KA ( eat ) in G.1 and kAta ( cut ) in G.2 ) , and nA 7 is the TAM label 6 , kara , TAM label roughly means 'having completed the activity ' .</sentence>
				<definiendum id="0">kara</definiendum>
				<definiens id="0">the TAM label 6 , kara , TAM label roughly means 'having completed the activity '</definiens>
			</definition>
			<definition id="7">
				<sentence>Pala ( fruit ) is the karma of the intermediate verb ( KA ) in G.1 but not in G.2 ( kAta ) .</sentence>
				<definiendum id="0">KA</definiendum>
				<definiens id="0">the karma of the intermediate verb (</definiens>
			</definition>
			<definition id="8">
				<sentence>In the latter , Pala is the karma of the main verb .</sentence>
				<definiendum id="0">Pala</definiendum>
				<definiens id="0">the karma of the main verb</definiens>
			</definition>
</paper>

		<paper id="1017">
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>Overgeneration is the main source of computational complexity in previous principle-based parsers .</sentence>
				<definiendum id="0">Overgeneration</definiendum>
				<definiens id="0">the main source of computational complexity in previous principle-based parsers</definiens>
			</definition>
			<definition id="1">
				<sentence>Unlike rule-based grammars that use a large number of rules to describe patterns in a language , Government-Binding ( GB ) Theory ( Chomsky , 1981 ; Haegeman , 1991 ; van Riemsdijk and Williams , 1986 ) ezplains these patterns in terms of more foundmental and universal principles .</sentence>
				<definiendum id="0">Government-Binding</definiendum>
			</definition>
			<definition id="2">
				<sentence>An example The message passing process for analyzing the sentence 2V : NP denotes verbs taking an NP complement .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">verbs taking an NP complement</definiens>
			</definition>
			<definition id="3">
				<sentence>Similarly , V : IP denotes verbs taking a CP complement , N : CP represents nouns taking a CP complement .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">IP</definiendum>
				<definiendum id="2">CP</definiendum>
				<definiens id="0">nouns taking a CP complement</definiens>
			</definition>
			<definition id="4">
				<sentence>18 v , Be i5 V : NP i7 was eaten &amp; The message passing process b. The parse tree retrieved 11 : &lt; \ [ 0,0\ ] ( ( cat d ) ) , { } &gt; 12 = &lt; \ [ 1,1\ ] ( ( cat n ) -plu ( nform norm ) +theta ) , { } &gt; 13 = &lt; \ [ 1,1\ ] ( ( cat n ) -plu ( nform norm ) +theta ) , { i2 } &gt; 14 = &lt; \ [ 0,1\ ] ( ( cat n ) -plu ( nform norm ) -cm +theta ) , { il , i3 } &gt; 15 = &lt; \ [ 2,2\ ] ( ( cat i ) -plu ( per 1 3 ) ( cform fin ) +be +ca +govern ( tense past ) ) , { } &gt; 16 = &lt; \ [ 2,2\ ] ( ( cat i ) -plu ( per 1 3 ) ( cform fin ) +be +ca +govern ( tense past ) ) , { i5 } &gt; 17 = &lt; \ [ 3,3\ ] ( ( cat v ) +pas ) , { } &gt; 18 -- -- &lt; \ [ 3,3\ ] ( ( cat v ) +pas +nppg -npbarrier ( np-atts NNORM ) ) , { i7 } &gt; 19 = &lt; \ [ 3,3\ ] ( ( cat v ) +pas +nppg -npbarrier ( rip-arts NNORH ) ) , { is } &gt; 110= &lt; \ [ 3,3\ ] ( ( cat v ) +pas +nppg -npbarrier ( rip-arts NNORM ) ) , { i9 } &gt; 111= &lt; \ [ 2,3\ ] ( ( cat ± ) +pas +nppg -npbarrier ( np-atts NNORH ) ( per 1 3 ) ( cform fin ) +ca +govern ( tense past ) ) ) , { i6 , ilo } &gt; i12~- &lt; \ [ 0,3\ ] , ( ( cat i ) +pas ( per 1 3 ) ( cform fin ) +ca +govern ( tense past ) ) , { i4 , ill } &gt; Figure 2 : Parsing the sentence `` The ice-cream was eaten '' ( 1 ) The ice-cream was eaten is illustrated in Figure 2 .</sentence>
				<definiendum id="0">cat i</definiendum>
				<definiens id="0">cat v ) +pas )</definiens>
			</definition>
			<definition id="5">
				<sentence>In Chomsky 's proposal , barrierhood is a property of maximal nodes ( nodes representing maximal categories ) .</sentence>
				<definiendum id="0">barrierhood</definiendum>
				<definiens id="0">a property of maximal nodes ( nodes representing maximal categories )</definiens>
			</definition>
			<definition id="6">
				<sentence>Barrierhood is a property of these links , independent of the context .</sentence>
				<definiendum id="0">Barrierhood</definiendum>
				<definiens id="0">a property of these links , independent of the context</definiens>
			</definition>
			<definition id="7">
				<sentence>If an item has +govern attribute , then non-head sources of the item and their sources are governed by the head of the item if there are paths between them and the item satisfying the conditions : on the path ( minimality condition ( Chomsky , 1986 , p.10 ) ) .</sentence>
				<definiendum id="0">minimality condition</definiendum>
				<definiens id="0">non-head sources of the item and their sources are governed by the head of the item if there are paths between them and the item satisfying the conditions : on the path</definiens>
			</definition>
			<definition id="8">
				<sentence>When V : NP receives an item representing the passive sense of the word eaten , V : NP creates another item &lt; \ [ i , i\ ] , ( ( cat v ) -npbarrier +nppg ( np-atts ( cat n ) ) ) , { } &gt; This item will not be combined with any item from NP node because the NP complement is assumed to be an np-trace .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">NP receives an item representing the passive sense of the word eaten</definiens>
			</definition>
			<definition id="9">
				<sentence>Attribute grammar interpretation Correa ( 1991 ) proposed an interpretation of GB principles based on attribute grammars .</sentence>
				<definiendum id="0">Attribute grammar interpretation Correa</definiendum>
				<definiens id="0">proposed an interpretation of GB principles based on attribute grammars</definiens>
			</definition>
			<definition id="10">
				<sentence>An attribute grammar consists of a phrase structure grammar and a set of attribution rules to compute the attribute values of the non-terminal symbols .</sentence>
				<definiendum id="0">attribute grammar</definiendum>
				<definiens id="0">consists of a phrase structure grammar and a set of attribution rules to compute the attribute values of the non-terminal symbols</definiens>
			</definition>
</paper>

		<paper id="1030">
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>Propositional representations , which they conjecture are held by short-term registers in memory ( henceforth propositional registers ) , maintain the surface syntactic constituent structure of an utterance as well as binding relations ; however , discourse anaphors are resolved .</sentence>
				<definiendum id="0">Propositional representations</definiendum>
				<definiens id="0">henceforth propositional registers ) , maintain the surface syntactic constituent structure of an utterance as well as binding relations ; however , discourse anaphors are resolved</definiens>
			</definition>
			<definition id="1">
				<sentence>The propositional representation for the source clause in these sentences is shown in representation ( 34 ) , where P denotes the passive voice : ( 34 ) \ [ P \ [ was .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the passive voice : ( 34 ) \ [ P \ [ was</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>Our raw knowledge about the relation consists of the frequencies f~n of occurrence of particular pairs ( v , n ) in the required configuration in a training corpus .</sentence>
				<definiendum id="0">raw knowledge about the relation</definiendum>
				<definiens id="0">consists of the frequencies f~n of occurrence of particular pairs ( v , n ) in the required configuration in a training corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>Therefore , if we are trying to distinguish among hypotheses qi when p is the relative frequency distribution of observations , D ( p II ql ) gives the relative weight of evidence in favor of qi .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the relative frequency distribution of observations , D ( p II ql ) gives the relative weight of evidence in favor of qi</definiens>
			</definition>
			<definition id="2">
				<sentence>In particular , we would like to find a set of clusters C such that each conditional distribution pn ( v ) can be approximately decomposed as p , ( v ) = ~p ( cln ) pc ( v ) , cEC where p ( c\ [ n ) is the membership probability of n in c and pc ( v ) = p ( vlc ) is v 's conditional probability given by the centroid distribution for cluster c. The above decomposition can be written in a more symmetric form as ~ ( n , v ) = ~_ , p ( c , n ) p ( vlc ) cEC = ~-~p ( c ) P ( nlc ) P ( Vlc ) ( 1 ) cEC assuming that p ( n ) and /5 ( n ) coincide .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">n ) p</definiendum>
				<definiens id="0">the membership probability of n in c and pc ( v ) = p ( vlc ) is v 's conditional probability given by the centroid distribution for cluster</definiens>
			</definition>
			<definition id="3">
				<sentence>We can now see that the variation ( 5 log Z~ vanishes for centroid distributions given by ( 11 ) , since it follows from ( 10 ) that 6 log = exp-rid ( , , c ) 6d ( n , e ) Ze -ri -0 n The Free Energy Function The combined minimum distortion and maximum entropy optimization is equivalent to the minimization of a single function , the free energy 1 log Zn F = -~ = &lt; D &gt; - '' Hlri , where ( D ) is the average distortion ( 5 ) and H is the cluster membership entropy ( 6 ) .</sentence>
				<definiendum id="0">H</definiendum>
				<definiens id="0">n , e ) Ze -ri -0 n The Free Energy Function The combined minimum distortion and maximum entropy optimization is equivalent to the minimization of a single function , the free energy 1 log Zn F = -~ = &lt; D &gt; - '' Hlri</definiens>
				<definiens id="1">the average distortion ( 5 ) and</definiens>
			</definition>
			<definition id="4">
				<sentence>Relative Entropy Figure 3 plots the unweighted average relative entropy , in bits , of several test sets to asymmetric clustered models of different sizes , given by 1 ~ , ,eAr , D ( t , ,ll/~- ) , where Aft is the set of direct objects in the test set and t , ~ is the relative frequency distribution of verbs taking n as direct object in the test set .</sentence>
				<definiendum id="0">relative entropy</definiendum>
				<definiendum id="1">Aft</definiendum>
				<definiens id="0">the set of direct objects in the test set and t</definiens>
				<definiens id="1">the relative frequency distribution of verbs taking n as direct object in the test set</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>Feature Structures An FA A is defined by a 5-tuple ( Q , E , 5 , q0 , F ) , where Q is a finite set of STATES , ~ a finite INPUT ALPHABET , ( ~ : Q x ~ -- -y Q is the TRANSITION FUNCTION , q0 E Q the INITIAL STATE , and F _C Q the set of FINAL STATES .</sentence>
				<definiendum id="0">Feature Structures An FA A</definiendum>
				<definiendum id="1">Q</definiendum>
			</definition>
			<definition id="1">
				<sentence>DSrre and Eisele 1989 and Backofen et al. 1990 introduce distributed disjunctions because they ( normally ) allow more efficient processing of disjunctions , sometimes obviating the need to expand to disjunctive normal form .</sentence>
				<definiendum id="0">DSrre</definiendum>
				<definiens id="0">distributed disjunctions because they ( normally ) allow more efficient processing of disjunctions , sometimes obviating the need to expand to disjunctive normal form</definiens>
			</definition>
			<definition id="2">
				<sentence>The initial state of/21 is A , that of/22 is X. The intersection of£1 and/22 is given by the unification of A and X. Unifying A and X leads to the following structure : ( 11 ) : |EDGE a \ [ NEXT BJ \ [ NEXT $ 1 { XV Y } J \ [ NEXT B A Now , testing whether w belongs to /21 N/22 is equivalent to the satisfiability ( consistency ) of ( 12 ) A A X A \ [ INPUT w\ ] , where type expansion yields a decision procedure .</sentence>
				<definiendum id="0">X A \</definiendum>
				<definiens id="0">equivalent to the satisfiability ( consistency ) of ( 12 ) A A</definiens>
			</definition>
			<definition id="3">
				<sentence>Inflectional Morphology : A Theoretical Study Based on Aspects of Latin Verb Conjugation .</sentence>
				<definiendum id="0">Inflectional Morphology</definiendum>
				<definiens id="0">A Theoretical Study Based on Aspects of Latin Verb Conjugation</definiens>
			</definition>
			<definition id="4">
				<sentence>X2MORF : A Morphological Component Based on Augmented Two-Level Morphology .</sentence>
				<definiendum id="0">X2MORF</definiendum>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>The Textual Knowledge Bank Project ( Sadler 91 ) is building lnonolingual and multilingual text bases structured by linking the elements with grammatical ( dependency ) , referential , and bilingual relations .</sentence>
				<definiendum id="0">Textual Knowledge Bank Project</definiendum>
				<definiens id="0">building lnonolingual and multilingual text bases structured by linking the elements with grammatical ( dependency )</definiens>
			</definition>
			<definition id="1">
				<sentence>Definition 1 Simple feature structures ( FS ) ( L is the sel of feature labels , and A is the set of atomic values ) are defined recursively : them up into simple sentence fragments .</sentence>
				<definiendum id="0">FS ) ( L</definiendum>
				<definiens id="0">the set of atomic values ) are defined recursively : them up into simple sentence fragments</definiens>
			</definition>
			<definition id="2">
				<sentence>e reduced DS of a DS ( , with respect to a decomposition D = { ¢1 , '' -4 ' , ~ } is constracted as follows : I. ¢i is transformed to a DS , `` pred : St ' , where Si is the set of all coT~le~l words appeari~J 9 i7~ ¢i. Th .</sentence>
				<definiendum id="0">DS</definiendum>
				<definiendum id="1">Si</definiendum>
				<definiens id="0">the set of all coT~le~l words</definiens>
			</definition>
			<definition id="3">
				<sentence>Definition 10 Given dependency structures , DS1 and DS , , , of two languages , tile structural matching problem is to find an isomorphic oT~e-to-one mapping m be*ween decompositions of DSa aT~d DS2 that maximizes the sum of the vahtes of similarity functions , f and g. That is , the problem is to find the fltnctioT~ m that maximizes ~-~m ( f ( d , re ( d ) ) + ~t g ( l , , n. ( / ) ) ) where d varies over semi-complete DS of DS1 and l varies over feature labels in D , -q .</sentence>
				<definiendum id="0">g. That</definiendum>
				<definiens id="0">to find an isomorphic oT~e-to-one mapping m be*ween decompositions of DSa aT~d DS2 that maximizes the sum of the vahtes of similarity functions , f and</definiens>
			</definition>
			<definition id="4">
				<sentence>The matching of two dependency trees starts from the top nodes and the matching process goes along edges of the trees .</sentence>
				<definiendum id="0">matching process</definiendum>
			</definition>
			<definition id="5">
				<sentence>Suppose CE is the set of contents words in the English sentence .</sentence>
				<definiendum id="0">Suppose CE</definiendum>
				<definiens id="0">the set of contents words in the English sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>Suppose that s and t are semi-colnplete DSs to be matched , and that Vs and Vt are the sets of content words in s and t. Let A be the less larger set of l~ and Vt and B be the other ( I A I &lt; l B I ) .</sentence>
				<definiendum id="0">Vt</definiendum>
				<definiens id="0">the sets of content words in s</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>The Lexical Chooser is part of the text generator \ [ 7\ ] .</sentence>
				<definiendum id="0">Lexical Chooser</definiendum>
			</definition>
			<definition id="1">
				<sentence>On receiving the hierarchy of logical forms , the Lexical Chooser determines the overall grammatical form of each sentence based on the semantic structure of the LFs ( e.g. , conditional sentences are generated for precondition-action structures ) and selects the words and phrases realizing semantic concepts of the LF .</sentence>
				<definiendum id="0">Lexical Chooser</definiendum>
				<definiens id="0">determines the overall grammatical form of each sentence based on the semantic structure of the LFs ( e.g. , conditional sentences</definiens>
			</definition>
			<definition id="2">
				<sentence>The Lexical Chooser uses a rewriting system itself implemented on top of FUF .</sentence>
				<definiendum id="0">Lexical Chooser</definiendum>
				<definiens id="0">uses a rewriting system itself implemented on top of FUF</definiens>
			</definition>
			<definition id="3">
				<sentence>When generating both initial and subsequent referring expressions , it selects a set of distinguishing properties of the referent and chooses words to express the selected 8There is a limit to how far back COMET looks in the discourse to construct a new referring expression : the discourse history is cleared after each menu request for a new explanation .</sentence>
				<definiendum id="0">8There</definiendum>
				<definiens id="0">a limit to how far back COMET looks in the discourse to construct a new referring expression : the discourse history is cleared after each menu request for a new explanation</definiens>
			</definition>
			<definition id="4">
				<sentence>EPICURE uses the user 's domain knowledge , KAMP mutual beliefs about the domain , and FN the user 's domain knowledge in conjunction with rules on implicatures .</sentence>
				<definiendum id="0">EPICURE</definiendum>
				<definiendum id="1">FN</definiendum>
				<definiens id="0">uses the user 's domain knowledge , KAMP mutual beliefs about the domain</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>First , a speaker may produce a partial word or FRAGMENT , a string of phonemes that does not form the complete intended word .</sentence>
				<definiendum id="0">FRAGMENT</definiendum>
				<definiens id="0">a string of phonemes that does not form the complete intended word</definiens>
			</definition>
			<definition id="1">
				<sentence>RIM divides the repair event into three consecutive temporal intervals and identifies time points within those intervals that are computationally critical .</sentence>
				<definiendum id="0">RIM</definiendum>
				<definiens id="0">divides the repair event into three consecutive temporal intervals and identifies time points within those intervals that are computationally critical</definiens>
			</definition>
			<definition id="2">
				<sentence>The DISFLUENCY INTERVAL ( nI ) extends from the IS to the resumption of fluent speech , and may contain any combination of silence , pause fillers ( 'uh ' , 'urn ' ) , or CUE PHRASES ( e.g. , 'Oops ' 2This is consistent with Levelt ( 1983 ) 's observation that the material to be replaced and the correcting material in a repair often share structural properties akin to those shared by coordinated constituents .</sentence>
				<definiendum id="0">DISFLUENCY INTERVAL ( nI</definiendum>
				<definiendum id="1">CUE PHRASES</definiendum>
			</definition>
			<definition id="3">
				<sentence>RIM provides a framework for testing the extent to which cues from the speech signal contribute to the identification and correction of repair utterances .</sentence>
				<definiendum id="0">RIM</definiendum>
				<definiens id="0">provides a framework for testing the extent to which cues from the speech signal contribute to the identification and correction of repair utterances</definiens>
			</definition>
			<definition id="4">
				<sentence>Our corpus for the current study consisted of 6,414 utterances produced by 123 speakers from the ARPA Airline Travel and Information System ( ATIS ) database ( MADCOW , 1992 ) collected at AT &amp; T , BBN , CMU , SRI , and TL 334 ( 5.2 % ) of these utterances contain at least one repair~ where repair is defined as the self-correction of one or more phonemes ( up to and including sequences of words ) in an utterance ) Orthographic transcriptions of the utterances were prepared by ARPA contractors according to standardized conventions .</sentence>
				<definiendum id="0">corpus for</definiendum>
				<definiendum id="1">TL 334</definiendum>
				<definiens id="0">the current study consisted of 6,414 utterances produced by 123 speakers from the ARPA Airline Travel and Information System ( ATIS ) database ( MADCOW , 1992 ) collected at AT &amp; T , BBN , CMU , SRI , and</definiens>
				<definiens id="1">the self-correction of one or more phonemes ( up to and including sequences of words ) in an utterance ) Orthographic transcriptions of the utterances were prepared by ARPA contractors according to standardized conventions</definiens>
			</definition>
			<definition id="5">
				<sentence>Levelt &amp; Cutler ( 1983 ) claim that repairs of erroneous information ( ERROR REPAIRS ) are marked by increased intonational prominence on the correcting information , while other kinds of repairs , such as additions to descriptions ( APPROPRIATENESS REPAIRS ) , generally are not .</sentence>
				<definiendum id="0">APPROPRIATENESS REPAIRS</definiendum>
				<definiens id="0">1983 ) claim that repairs of erroneous information ( ERROR REPAIRS ) are marked by increased intonational prominence on the correcting information , while other kinds of repairs , such as additions to descriptions</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>Gemini is a natural language ( NL ) understanding system developed for spoken language applications .</sentence>
				<definiendum id="0">Gemini</definiendum>
			</definition>
			<definition id="1">
				<sentence>The Gemini kernel consists of a set of compilers to interpret the high-level languages in which the lexicon and syntactic and semantic grammar rules are written , as well as the parser , semantic interpretation , quantifier scoping , repair correction mechanisms , and all other aspects of Gemini that are not specific to a language or domain .</sentence>
				<definiendum id="0">Gemini kernel</definiendum>
				<definiens id="0">consists of a set of compilers to interpret the high-level languages in which the lexicon and syntactic and semantic grammar rules are written , as well as the parser , semantic interpretation , quantifier scoping , repair correction mechanisms , and all other aspects of Gemini that are not specific to a language or domain</definiens>
			</definition>
			<definition id="2">
				<sentence>Here is an example : np : \ [ wh=ynq , case= ( nomVacc ) , pers_num= ( 3rdAsg ) \ ] This category can be instantiated by any noun phrase with the value ynq for its wh feature ( which means it must be a wh-bearing noun phrase like which book , who , or whose mother ) , either ace ( accusative ) or nora ( nominative ) for its case feature , and the conjunctive value 3rdAsg ( third and singular ) for its person-number feature .</sentence>
				<definiendum id="0">nora</definiendum>
				<definiens id="0">means it must be a wh-bearing noun phrase like which book , who , or whose mother ) , either ace ( accusative ) or</definiens>
			</definition>
			<definition id="3">
				<sentence>flict , prefer longer reduces to shorter reduces .</sentence>
				<definiendum id="0">flict</definiendum>
			</definition>
			<definition id="4">
				<sentence>The scoping algorithm that we use combines syntactic and semantic information with a set of quantifier scoping preference rules to rank the possible scoped logical forms consistent with the quasi-logical form selected by parse preferences .</sentence>
				<definiendum id="0">scoping algorithm</definiendum>
				<definiens id="0">combines syntactic and semantic information with a set of quantifier scoping preference rules to rank the possible scoped logical forms consistent with the quasi-logical form selected by parse preferences</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>1 2 3 ( II ) LR : haX + mel + lek SR : hom Orael OOek Rlla : a : o ~ C X : ( +:0 ) Rllb : X : { m , O } ~ a : ( +:0 ) { m , m } Rllc : l:0 ~ l : l ( +:0 ) Rlla transforms a to o in the proper environment , Rllb geminates m and Rllc degeminates pheme 1 -- haX -- can not be used in a general way without conflicts with the complete set of two-level rules applicable .</sentence>
				<definiendum id="0">Rllc</definiendum>
				<definiens id="0">a to o in the proper environment</definiens>
			</definition>
			<definition id="1">
				<sentence>( 12 ) LR : laX+mel+lek SR : limOmelOOek R12 : a : i ¢ : C X : ( +:0 ) Thus lax uses a rule subset P3 which consists of rules { R12 , Rllb , Rllc } .</sentence>
				<definiendum id="0">LR</definiendum>
				<definiens id="0">laX+mel+lek SR : limOmelOOek R12 : a : i ¢ : C X : ( +:0 ) Thus lax uses a rule subset P3 which consists of rules { R12 , Rllb</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>Figure 2 plotsf ( x ) as a function of x , where x is a byte position in the English text andf ( x ) is the corresponding byte position in the French text , as determined by char_align .</sentence>
				<definiendum id="0">x</definiendum>
				<definiens id="0">the corresponding byte position in the French text , as determined by char_align</definiens>
			</definition>
			<definition id="1">
				<sentence>The residuals can be computed as f ( x ) cx , where c is the ratio of the lengths of the two files ( 0.91 ) .</sentence>
				<definiendum id="0">c</definiendum>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>Figure 2 : A Bilingual Corpus P ( FpIE ) we express the translation model as a distribution P ( \ [ Ep ; Fp\ ] ) over sentence beads .</sentence>
				<definiendum id="0">Bilingual Corpus P</definiendum>
				<definiens id="0">a distribution P ( \ [ Ep ; Fp\ ] ) over sentence beads</definiens>
			</definition>
			<definition id="1">
				<sentence>Assuming that successive sentence beads are generated independently , we get L P ( C , Yr , A ) = p ( L ) H P ( \ [ E~ ; F~\ ] ) k=l where A t. 1 = ( \ [ E~ , F ; \ ] , ... , \ [ EL ; FL\ ] ) is consistent with g and ~ '' and where p ( L ) is the probability that a corpus contains L sentence beads .</sentence>
				<definiendum id="0">successive sentence beads</definiendum>
			</definition>
			<definition id="2">
				<sentence>We take n P ( \ [ E ; F\ ] ) = p ( n ) p ( m ) H p ( ei ) fi p ( fj ) i=l j=l where p ( n ) is the probability that an English sentence is n words long , p ( m ) is the probability that a French sentence is m words long , p ( ei ) is the frequency of the word ei in English , and p ( fj ) is the frequency of the word fj in French .</sentence>
				<definiendum id="0">fi p</definiendum>
				<definiendum id="1">p ( n )</definiendum>
				<definiendum id="2">French sentence</definiendum>
				<definiens id="0">the probability that an English sentence</definiens>
				<definiens id="1">the probability that a</definiens>
				<definiens id="2">the frequency of the word ei in English , and p</definiens>
				<definiens id="3">the frequency of the word fj in French</definiens>
			</definition>
			<definition id="3">
				<sentence>As a first cut , consider the following `` model '' : P* ( B ) = p ( l ) H p ( bi ) i=1 where B = { bl , ... , bl } is a multiset of word beads , p ( l ) is the probability that an English sentence and a French sentence contain l word beads , and p ( bi ) denotes the frequency of the word bead bi .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">consider the following `` model '' : P* ( B ) = p ( l ) H p ( bi ) i=1 where B = { bl , ... , bl } is a multiset of word beads</definiens>
				<definiens id="1">the probability that an English sentence and a French sentence contain l word beads</definiens>
				<definiens id="2">the frequency of the word bead bi</definiens>
			</definition>
			<definition id="4">
				<sentence>H p ( b , ) B i=1 where B ranges over beadings consistent with \ [ E ; F\ ] and l ( B ) denotes the number of beads in B. Recall that n is the length of the English sentence and m is the length of the French sentence .</sentence>
				<definiendum id="0">B )</definiendum>
				<definiens id="0">H p ( b , ) B i=1 where B ranges over beadings consistent with \ [ E</definiens>
				<definiens id="1">the number of beads in B. Recall that n is the length of the English sentence</definiens>
				<definiens id="2">the length of the French sentence</definiens>
			</definition>
			<definition id="5">
				<sentence>: 'eB , Pb ( f ' ) where Pe refers to the distribution of word beads in 1:0 sentence beads , pf refers to the distribution of word beads in 0:1 sentence beads , pb refers to the distribution of word beads in 1:1 , 2:1 , and 1:2 sentence beads , and Be and B I refer to the sets of 1:0 and 0:1 word beads in the vocabulary , respectively .</sentence>
				<definiendum id="0">Pe</definiendum>
				<definiens id="0">the distribution of word beads in 1:0 sentence beads</definiens>
				<definiens id="1">the distribution of word beads in 0:1 sentence beads</definiens>
			</definition>
			<definition id="6">
				<sentence>Dynamic programming consists of incrementally finding the best alignment of longer and longer prefixes of the bilingual corpus .</sentence>
				<definiendum id="0">Dynamic programming</definiendum>
				<definiens id="0">consists of incrementally finding the best alignment of longer and longer prefixes of the bilingual corpus</definiens>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>HPSG is a lexiealized theory , with the lexical definitions , rather then phrase structure rules , specifying most configurational constraints .</sentence>
				<definiendum id="0">HPSG</definiendum>
				<definiens id="0">a lexiealized theory , with the lexical definitions , rather then phrase structure rules</definiens>
			</definition>
			<definition id="1">
				<sentence>Kim : `` Robin promised to come at noon '' A set of plausible partial expectations generated by the pragmatic and systemic components in anticipation of Kim 's response might be : ( ( S ) ( UNMARKED-DECLARATIVE ) ) ( ( S SUBJECT ) ( PROPER ) ) ( ( S BETA ) ( NONFINITEPRED ) ) ( ( S PREDICATOR ) ( PROMISED ) ) ( ( S BETA TEMPORAL ) ( PP ) ) ( ( S BETA PREDICATOR ) ( ARRIVAL ) ) In these expectations the first list of each pair ( e.g. ( S BETA ) ) represents a functional role within the expected sentence .</sentence>
				<definiendum id="0">Kim</definiendum>
				<definiens id="0">plausible partial expectations generated by the pragmatic and systemic components in anticipation of Kim 's response might be : ( ( S ) ( UNMARKED-DECLARATIVE ) ) ( ( S SUBJECT ) ( PROPER ) ) ( ( S BETA ) ( NONFINITEPRED ) ) ( ( S PREDICATOR ) ( PROMISED ) ) ( ( S BETA TEMPORAL ) ( PP ) ) ( ( S BETA PREDICATOR</definiens>
				<definiens id="1">a functional role within the expected sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>These expected features assert expectations which are both semantic ( e.g. PROMISED ) and syntactic ( e.g. ( ( S BETA TEMPORAL ) ( PP ) ) asserts both the existance and location of a temporal adjunct PP ) .</sentence>
				<definiendum id="0">S BETA TEMPORAL )</definiendum>
			</definition>
			<definition id="3">
				<sentence>Role path match is a constituent role path compatible with the roles of its children ?</sentence>
				<definiendum id="0">Role path match</definiendum>
			</definition>
			<definition id="4">
				<sentence>EVALUATION &amp; TESTING The techniques described here have been used successfully to guide the parsing of several sentences taken from real conversations .</sentence>
				<definiendum id="0">EVALUATION</definiendum>
				<definiens id="0">techniques described here have been used successfully to guide the parsing of several sentences taken from real conversations</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>The weights C0 ( s , t ) can be interpreted as the mean number of times that npF ( t ) corresponds to apE ( s ) given the corpus and the initial assumption of equiprobable correspondences .</sentence>
				<definiendum id="0">weights C0 ( s</definiendum>
				<definiens id="0">the mean number of times that npF</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>One type of lexical knowledge which is useful for many natural language ( NL ) tasks is the semantic relatedness between words of the same or different syntactic categories .</sentence>
				<definiendum id="0">NL</definiendum>
				<definiens id="0">the semantic relatedness between words of the same or different syntactic categories</definiens>
			</definition>
			<definition id="1">
				<sentence>A linguistic scale is a set of words , of the same grammatical category , which can be ordered by their semantic strength or degree of informativeness ( Levinson , 1983 ) .</sentence>
				<definiendum id="0">linguistic scale</definiendum>
			</definition>
			<definition id="2">
				<sentence>For example , in light blue shirt , blue is a value of the property color , while light indicates the shade 1 .</sentence>
				<definiendum id="0">blue</definiendum>
				<definiens id="0">a value of the property color</definiens>
			</definition>
			<definition id="3">
				<sentence>Formally , if ( Xi , Yi ) and ( Xj , Yj ) are two pairs of observations for the adjectives X and Y on the nouns i and j respectively , we call these pairs concordant if Xi &gt; Xj and Yi &gt; Y. or if Xi &lt; X. and Yi &lt; Yj ; otherwise these pairs are ~iscordant. W/e discard ties , that is pairs of observations where Xi=Xj.or Yi=Yj. For example , Table 1 shows the frequencies observed for the co-occurrences of the nouns coordination and market and the adjectives global and international in the test corpus which is described in Section 4. From the table we observe that for i=coordination , j=market , X=global , and Y=international , we have Xi=16 &lt; 24=X : and Yi=I9 &lt; 33=Yj , so this particular pair of paired/obser vations is concordant and contributes positively to the similarity between global and international. In general , if the distributions for the two adjectives are similar , we expect a large number of concordances , and a small number of discordances. Kendall 's I : is defined as `` c = Pc-Pd where Pc and Pd are the probabilities of observing a concordance or discordance respectively. ~ ranges from -1 to +1 , with +1 indicating complete concordance , -1 complete discordance , and 0 no correlation between X and Y. An unbiased estimator of x is the statistic C-Q T=-where n is the number of paired observations in the sample and C and Q are the numbers of observed concordances and discordances respectively ( Wayne , 1990 ) . We compute T for each pair of adjectives , adjusting for possible ties in the values of each variable , so that our statistic remains an unbiased estimator of x. We determine concordances and discordances by global international coordination 16 19 market 24 33 Table 1 : Example adjective-noun frequencies. sorting the pairs of observations ( noun frequencies ) on one of the variables ( adjectives ) , and computing how many of the ( 2 ) pairs of paired observations agree or disagree with the expected order on the other adjective. We normalize the result to the range 0 to 1 using a simple linear transformation. The second similarity module utilizes the knowledge offered by the observed adjectiveadjective pairs. We know that the adjectives which appear in any such pair can not be part of the same group , so the module produces zero similarity for all such pairs. The module does not output any similarity value for pairs of adjectives which have not been observed together in the same minimal NP. The two modules produce results of a significantly different character. The adjective-noun module always outputs a similarity value for any pair of adjectives , but these values tend to be around the middle of the range of possible values ; rarely will the pattern of similarity or dissimilarity be strong enough to produce a value which has a large deviation from values can be attributed to the existence of many ties and many adjective-noun pairs with low frequencies , as would be expected by Zipf 's law ( Zipf , 1949 ) . However , the expected number of concordances and discordances which can be attributed to chance will be the same ( a random pair can produce a concordance or discordance with probability 0.5 for each ) , so the effect of chance fluctuations on T is not very significant. Furthermore , the robustness of the method guarantees that it will not be significantly influenced by any outliers ( this is true for all rank based methods ) . Therefore , although we can not have complete confidence in a statistical estimate like T , we expect the module to produce useful estimates of similarity. On the other hand , the adjective-adjective module produces similarity values with absolute certainty , since once two adjectives have been seen in the same NP even once , we can deduce that they do not belong in the same group. However , this negative knowledge is computed only for a few of the possible pairs of adjectives , and it can not be propagated to more pairs as dissimilarity is not a transitive relation. As a result we can make some inferences with very high confidence , but we can not make very many of them. 175 Similarity Estimates In stage three we combine the values produced by the various similarity modules in stage two using a pre-specified algorithm. The output of this stage is a single table of dissimilarity values ( as required by the next stage ) having one entry for each adjective pair. Currently we have only the two similarity modules described in the previous subsection , so we employ the following simple algorithm : for any pair of adjectives ( x , y ) do if the adjective-adjective module has no opinion on ( x , y ) then dissimilarity = 1 ( the similarity reported by the adjective-noun module ) else dissimilarity = some constant k &gt; 1 As can be easily seen , the algorithm has complete confidence in the results of the adjective-adjective module whenever that module has an opinion ; when it does not , the algorithm uses the similarity value produced by the adjective-noun module , after a simple linear transformation is applied to convert it to a dissimilarity .</sentence>
				<definiendum id="0">x</definiendum>
				<definiendum id="1">statistic C-Q T=-where n</definiendum>
				<definiens id="0">two pairs of observations for the adjectives X and Y on the nouns i and j respectively , we call these pairs concordant if Xi &gt; Xj and Yi &gt; Y. or if Xi &lt; X. and Yi &lt; Yj ; otherwise these pairs are ~iscordant. W/e discard ties , that is pairs of observations where Xi=Xj.or Yi=Yj. For example</definiens>
				<definiens id="1">the frequencies observed for the co-occurrences of the nouns coordination and market and the adjectives global and international in the test corpus which is described in Section</definiens>
				<definiens id="2">concordant and contributes positively to the similarity between global and international. In general , if the distributions for the two adjectives are similar , we expect a large number of concordances , and a small number of discordances. Kendall 's I : is defined as `` c = Pc-Pd where Pc and Pd are the probabilities of observing a concordance or discordance respectively. ~ ranges from -1 to +1 , with +1 indicating complete concordance , -1 complete discordance , and 0 no correlation between X and Y. An unbiased estimator of</definiens>
				<definiens id="3">the number of paired observations in the sample and C and Q are the numbers of observed concordances and discordances respectively ( Wayne , 1990 ) . We compute T for each pair of adjectives , adjusting for possible ties in the values of each variable , so that our statistic remains an unbiased estimator of x. We determine concordances and discordances by global international coordination 16 19 market 24 33 Table 1 : Example adjective-noun frequencies. sorting the pairs of observations ( noun frequencies ) on one of the variables ( adjectives ) , and computing how many of the ( 2 ) pairs of paired observations agree or disagree with the expected order on the other adjective. We normalize the result to the range 0 to 1 using a simple linear transformation. The second similarity module utilizes the knowledge offered by the observed adjectiveadjective pairs. We know that the adjectives which appear in any such pair can not be part of the same group , so the module produces zero similarity for all such pairs. The module does not output any similarity value for pairs of adjectives which have not been observed together in the same minimal NP. The two modules produce results of a significantly different character. The adjective-noun module always outputs a similarity value for any pair of adjectives , but these values tend to be around the middle of the range of possible values ; rarely will the pattern of similarity or dissimilarity be strong enough to produce a value which has a large deviation from values can be attributed to the existence of many ties and many adjective-noun pairs with low frequencies , as would be expected by Zipf 's law ( Zipf , 1949 ) . However , the expected number of concordances and discordances which can be attributed to chance will be the same ( a random pair can produce a concordance or discordance with probability 0.5 for each ) , so the effect of chance fluctuations on T is not very significant. Furthermore , the robustness of the method guarantees that it will not be significantly influenced by any outliers ( this is true for all rank based methods ) . Therefore , although we can not have complete confidence in a statistical estimate like T , we expect the module to produce useful estimates of similarity. On the other hand , the adjective-adjective module produces similarity values with absolute certainty , since once two adjectives have been seen in the same NP even once , we can deduce that they do not belong in the same group. However , this negative knowledge is computed only for a few of the possible pairs of adjectives , and it can not be propagated to more pairs as dissimilarity is not a transitive relation. As a result we can make some inferences with very high confidence , but we can not make very many of them. 175 Similarity Estimates In stage three we combine the values produced by the various similarity modules in stage two using a pre-specified algorithm. The output of this stage is a single table of dissimilarity values ( as required by the next stage ) having one entry for each adjective pair. Currently we have only the two similarity modules described in the previous subsection , so we employ the following simple algorithm : for any pair of adjectives ( x , y ) do if the adjective-adjective module has no opinion on ( x , y ) then dissimilarity = 1 ( the similarity reported by the adjective-noun module ) else dissimilarity = some constant k &gt; 1 As can be easily seen , the algorithm has complete confidence in the results of the adjective-adjective module whenever that module has an opinion ; when it does not , the algorithm uses the similarity value produced by the adjective-noun module , after a simple linear transformation is applied to convert it to a dissimilarity</definiens>
			</definition>
			<definition id="4">
				<sentence>The number of possible partitions of n objects to m nonempty subsets with m &lt; n is equal to the corresponding Stifling number of the second kind ( Knuth , 1973 ) , and this number grows exponentially with n for all but trivial values of m. For example , for our test set of adjectives presented in the next section , we have n=21 and m=9 ; the corresponding number of possible partitions is roughly 1.23 × 1014. We tested our system on a 8.2 million word corpus of stock market reports from the Associated Press news wire. A subset of 21 of the adjectives in the corpus ( Figure 2 ) was selected for practical reasons ( mainly for keeping the evaluation task tractable ) . We selected adjectives that have one modified noun in common ( problem ) to ensure some semantic relatedness , and we included only adjectives that occurred frequently so that our similarity measure would be meaningful. The partition produced by the system for 9 clusters appears in Figure 3. Before presenting a formal evaluation of the results , we note that this partition contains interesting data. First , the results contain two clusters of gradable adjectives which fall in the same scale. Groups 5 and 8 contain adjectives that indicate the size , or scope , of a problem ; by augmenting the system with tests to identify when an adjective is gradable , we could separate out these two groups from other potential scales , and perhaps consider combining them. Second , groups 1 and 6 clearly identify separate sets of non-gradable adjectives. The first contains adjectives that describe the geographical scope of the problem. Although at first sight we would classify these adjectives as non-scalar , we observed that the phrase international even global problem is acceptable while the phrase *global even international problem is not. These patterns seem to technical Figure 3 : Partition found for 9 clusters. suggest at least some degree of scalability. On the other hand , group 6 contains non-scalar relational adjectives that specify the nature of the problem. It is interesting to note here that the clustering algorithm discourages long groups , with the expected number adjectives per cluster being 9 -- -2.33 ; nevertheless , of the evidence for the adjectives in group 6 is strong enough to allow the creation of a group with more than twice the expected number of members. Finally , note that even in group 4 which is the weakest group produced , there is a positive semantic correlation between the adjectives new and unexpected. To summarize , the system seems to be able to identify many of the existent semantic relationships among the adjectives , while its mistakes are limited to creating singleton groups containing adjectives that are related to other adjectives in the test set ( e.g. , missing the semantic associations between new-old and potential-real ) and `` recognizing '' a non-significant relationship between real and new-unexpected in group 4. We produced good results with a relatively small corpus of 8.2 million words 4 , out of which only 34,359 total / 3,073 distinct adjective-noun pairs involving 1,509 distinct nouns were relevant to our test set of 21 adjectives ( Figure 2 ) . The accuracy of the results can be improved if a larger , homogeneous corpus is used to provide the raw data. Also , we can increase the size of the adjective-noun and adjectiveadjective data that we are using if we introduce more syntactic patterns in stage one to extract more complex cases of pairs. Furthermore , some of the associations between adjectives that the system reports appear to be more stable than others ; these associations remain in the same group when we vary the number of clusters in the partition. We have noticed that adjectives with a higher degree of semantic content ( e.g. international or severe ) appear to form more 4Corpora up to 366 million words have been used for similar classification tasks. 177 Answer should be Yes Answer should be No The system says Yes a b The system says No c d Table 2 : Contingency table model for evaluation. stable associations than relatively semantically empty adjectives ( e.g. little or real ) . This observation can be used to filter out adjectives which are too general to be meaningfully clustered in groups. To evaluate the performance of our system we compared its output to a model solution for the problem designed by humans. Nine human judges were presented with the set of adjectives to be partitioned , a description of the domain , and a simple example. They were told that clusters should not overlap but they could select any number of clusters ( the judges used from 6 to 11 clusters , with an average of 8.565 and a sample standard deviation of nificantly from the alternative method of asking the humans to directly estimate the goodness of the system 's results ( e.g. ( Matsukawa , 1993 ) ) . It requires an explicit construction of a model from the human judge and places the burden of the comparison between the model and the system 's output on the system instead of the judge. It has been repeatedly demonstrated that in complex evaluation tasks humans can easily find arguments to support observed data , leading to biased results and to an inflation of the evaluation scores. To score our results , we converted the comparison of two partitions to a series of yes-no questions , each of which has a correct answer ( as dictated by the model ) and an answer assigned by the system. For each pair of adjectives , we asked if they fell in the same cluster ( `` yes '' ) or not ( `` no '' ) . Since human judges did not always agree , we used fractional values for the correctness of each answer instead of 0 ( `` incorrect '' ) and 1 ( `` correct '' ) . We defined the correctness of each answer as the relative frequency of the association between the two adjectives among the human models and the incorrectness of each answer as 1 correctness ; in this way , associations receive a correctness value proportional to their popularity among the human judges. For example , in the sample set of adjectives discussed in the previous section , the association ( foreign , international ) received a correctness value of 1 , since all the humans placed these two adjectives in the same group , while the association ( legal , severe ) received a correctness value of 0. The pair ( economic , political ) on the other hand received a correctness value of 0.67 , since two thirds of the judges placed the two adjectives in the same group. Once correctness and incorrectness values have been defined , we can generalize measures such as `` the number of correct associations retrieved by the system '' by using summation of those values instead of counting. Then the contingency table model ( Swets , 1969 ) , widely used in Information Retrieval and Psychology , is applicable. Referring to the classification of the yes-no answers in Table 2 , the following measures are defined : a • Recall = • 100 % a+c a • Precision = • 100 % a+b b • Fallout = b~ -- d '' 100 % In other words , recall is the percentage of correct `` yes '' answers that the system found among the model `` yes '' answers , precision is the percentage of correct `` yes '' answers among the total of `` yes '' answers that the system reported , and fallout is the percentage of incorrect `` yes '' answers relative to the total number of `` no '' answers 6. Note that in our generalized contingency table model , the symbols a , b , c , and d do not represent numbers of observed associations but rather sums of correctness or incorrectness values. These sums use correctness values for the quantities in the first column of Table 2 and incorrectness values for the quantities in the second column of Table 2. Furthermore , the summation is performed over all pairs reported or not reported by the system for quantities in the first or second row of Table 2 respectively. Consequently , the information theoretic measures represent the generalized counterparts of their original definitions. In the case of perfect agreement between the models , or of only one model , the generalized measures reduce to their original definitions. We also compute a combined measure for recall and precision , the F-measure ( Van Rijsbergen , 1979 ) , which always takes a value between the values of recall and precision , and is higher when recall and precision are closer ; it is defined as F ( 132+1 ) x Precision x Recall 132 x Precision + Recall 5This is the reason that we presented the partition with 9 clusters , as this is the closest integer to the average number of clusters used by the humans. 6Another measure used in information retrieval , overgeneration , is in our case always equal to ( 100 precision ) % . 178 Recall Precision Fallout F-measure ( 13=1 ) 7 clusters 50.78 % 43.56 % 7.48 % 46.89 % 8 clusters 37.31 % 38.10 % 6.89 % 37.70 % 9 clusters 49.74 % 46.38 % 6.54 % 48.00 % 10 clusters 35.23 % 41.98 % 5.54 % 38.31 % Table 3 : Evaluation results. where 13 is the weight of recall relative to precision ; we use 13=1.0 , which corresponds to equal weighting of the two measures. The results of applying our evaluation method to the system output ( Figure 3 ) are shown in Table 3 , which also includes the scores obtained for several other sub-optimal choices of the number of clusters. We have made these observations related to the evaluation mechanism : precision. Decreasing the number of clusters generally increases the recall and fallout and simultaneously decreases precision. measure overall than precision , since , in addition to its decision-theoretic advantages ( Swets , 1969 ) , it appears to be more consistent across evaluations of partitions with different numbers of clusters. This has also been reported by other researchers in different evaluation problems ( Lewis and Tong , 1992 ) . of the evaluation scores in an absolute sense is a non-trivial one. For example , there has been increasing concern that the scoring methods used for evaluating the goodness of parsers are producing values which seem extremely good ( in the &gt; 90 % range ) , while in fact the parse trees produced are not so satisfactory ; the blame for this inflation of the scores can be assigned to an inadequate comparison technique , which essentially considers a tree fragment correct when it is a part of ( although not exactly matching ) the corresponding fragment in the model .</sentence>
				<definiendum id="0">recall</definiendum>
				<definiendum id="1">precision</definiendum>
				<definiendum id="2">Recall Precision Fallout F-measure</definiendum>
				<definiens id="0">The number of possible partitions of n objects to m nonempty subsets with m &lt; n is equal to the corresponding Stifling number of the second kind ( Knuth , 1973 ) , and this number grows exponentially with n for all but trivial values of m. For example , for our test set of adjectives presented in the next section</definiens>
				<definiens id="1">word corpus of stock market reports from the Associated Press news wire. A subset of 21 of the adjectives in the corpus ( Figure 2 ) was selected for practical reasons ( mainly for keeping the evaluation task tractable ) . We selected adjectives that have one modified noun in common ( problem ) to ensure some semantic relatedness , and we included only adjectives that occurred frequently so that our similarity measure would be meaningful. The partition produced by the system for 9 clusters appears in Figure 3. Before presenting a formal evaluation of the results , we note that this partition contains interesting data. First , the results contain two clusters of gradable adjectives which fall in the same scale. Groups 5 and 8 contain adjectives that indicate the size , or scope , of a problem ; by augmenting the system with tests to identify when an adjective is gradable , we could separate out these two groups from other potential scales , and perhaps consider combining them. Second , groups 1 and 6 clearly identify separate sets of non-gradable adjectives. The first contains adjectives that describe the geographical scope of the problem. Although at first sight we would classify these adjectives as non-scalar , we observed that the phrase international even global problem is acceptable while the phrase *global even international problem is not. These patterns seem to technical Figure 3 : Partition found for 9 clusters. suggest at least some degree of scalability. On the other hand , group 6 contains non-scalar relational adjectives that specify the nature of the problem. It is interesting to note here that the clustering algorithm discourages long groups , with the expected number adjectives per cluster being 9 -- -2.33 ; nevertheless , of the evidence for the adjectives in group 6 is strong enough to allow the creation of a group with more than twice the expected number of members. Finally , note that even in group 4 which is the weakest group produced , there is a positive semantic correlation between the adjectives new and unexpected. To summarize , the system seems to be able to identify many of the existent semantic relationships among the adjectives , while its mistakes are limited to creating singleton groups containing adjectives that are related to other adjectives in the test set ( e.g. , missing the semantic associations between new-old and potential-real ) and `` recognizing '' a non-significant relationship between real and new-unexpected in group 4. We produced good results with a relatively small corpus of 8.2 million words 4 , out of which only 34,359 total / 3,073 distinct adjective-noun pairs involving 1,509 distinct nouns were relevant to our test set of 21 adjectives ( Figure 2 ) . The accuracy of the results can be improved if a larger , homogeneous corpus is used to provide the raw data. Also , we can increase the size of the adjective-noun and adjectiveadjective data that we are using if we introduce more syntactic patterns in stage one to extract more complex cases of pairs. Furthermore , some of the associations between adjectives that the system reports appear to be more stable than others ; these associations remain in the same group when we vary the number of clusters in the partition. We have noticed that adjectives with a higher degree of semantic content ( e.g. international or severe ) appear to form more 4Corpora up to 366 million words have been used for similar classification tasks. 177 Answer should be Yes Answer should be No The system says Yes a b The system says No c d Table 2 : Contingency table model for evaluation. stable associations than relatively semantically empty adjectives ( e.g. little or real ) . This observation can be used to filter out adjectives which are too general to be meaningfully clustered in groups. To evaluate the performance of our system we compared its output to a model solution for the problem designed by humans. Nine human judges were presented with the set of adjectives to be partitioned , a description of the domain , and a simple example. They were told that clusters should not overlap but they could select any number of clusters ( the judges used from 6 to 11 clusters , with an average of 8.565 and a sample standard deviation of nificantly from the alternative method of asking the humans to directly estimate the goodness of the system 's results ( e.g. ( Matsukawa , 1993 ) ) . It requires an explicit construction of a model from the human judge and places the burden of the comparison between the model and the system 's output on the system instead of the judge. It has been repeatedly demonstrated that in complex evaluation tasks humans can easily find arguments to support observed data , leading to biased results and to an inflation of the evaluation scores. To score our results , we converted the comparison of two partitions to a series of yes-no questions , each of which has a correct answer ( as dictated by the model ) and an answer assigned by the system. For each pair of adjectives , we asked if they fell in the same cluster ( `` yes '' ) or not ( `` no '' ) . Since human judges did not always agree , we used fractional values for the correctness of each answer instead of 0 ( `` incorrect ''</definiens>
				<definiens id="2">the relative frequency of the association between the two adjectives among the human models and the incorrectness of each answer as 1 correctness ; in this way , associations receive a correctness value proportional to their popularity among the human judges. For example , in the sample set of adjectives discussed in the previous section , the association ( foreign , international ) received a correctness value of 1 , since all the humans placed these two adjectives in the same group , while the association ( legal , severe ) received a correctness value of 0. The pair ( economic , political ) on the other hand received a correctness value of 0.67 , since two thirds of the judges placed the two adjectives in the same group. Once correctness and incorrectness values</definiens>
				<definiens id="3">the number of correct associations retrieved by the system '' by using summation of those values instead of counting. Then the contingency table model ( Swets , 1969 ) , widely used in Information Retrieval and Psychology , is applicable. Referring to the classification of the yes-no answers in Table 2 , the following measures are defined : a • Recall = • 100 % a+c a • Precision = • 100 % a+b b • Fallout = b~ -- d '' 100 % In other words</definiens>
				<definiens id="4">the percentage of correct `` yes '' answers that the system found among the model `` yes '' answers ,</definiens>
				<definiens id="5">the percentage of correct `` yes '' answers among the total of `` yes '' answers that the system reported , and fallout is the percentage of incorrect `` yes '' answers relative to the total number of `` no '' answers 6. Note that in our generalized contingency table model , the symbols a , b , c , and d do not represent numbers of observed associations but rather sums of correctness or incorrectness values. These sums use correctness values for the quantities in the first column of Table 2 and incorrectness values for the quantities in the second column of Table 2. Furthermore , the summation is performed over all pairs reported or not reported by the system for quantities in the first or second row of Table 2 respectively. Consequently , the information theoretic measures represent the generalized counterparts of their original definitions. In the case of perfect agreement between the models , or of only one model , the generalized measures reduce to their original definitions. We also compute a combined measure for recall and precision , the F-measure ( Van Rijsbergen , 1979 ) , which always takes a value between the values of recall and precision , and is higher when recall and precision are closer ; it is defined as F ( 132+1 ) x Precision x Recall 132 x Precision + Recall 5This is the reason that we presented the partition with 9 clusters , as this is the closest integer to the average number of clusters used by the humans. 6Another measure used in information retrieval , overgeneration , is in our case always equal to ( 100 precision</definiens>
				<definiens id="6">Evaluation results. where 13 is the weight of recall relative to precision ; we use 13=1.0 , which corresponds to equal weighting of the two measures. The results of applying our evaluation method to the system output ( Figure 3 ) are shown in Table 3 , which also includes the scores obtained for several other sub-optimal choices of the number of clusters. We have made these observations related to the evaluation mechanism : precision. Decreasing the number of clusters generally increases the recall and fallout and simultaneously decreases precision. measure overall than precision , since , in addition to its decision-theoretic advantages ( Swets , 1969 ) , it appears to be more consistent across evaluations of partitions with different numbers of clusters. This has also been reported by other researchers in different evaluation problems ( Lewis and Tong , 1992 ) . of the evaluation scores in an absolute sense is a non-trivial one. For example , there has been increasing concern that the scoring methods used for evaluating the goodness of parsers are producing values which seem extremely good ( in the &gt; 90 % range ) , while in fact the parse trees produced are not so satisfactory ; the blame for this inflation of the scores can be assigned to an inadequate comparison technique , which essentially considers a tree fragment correct when it is a part of ( although not exactly matching ) the corresponding fragment in the model</definiens>
			</definition>
			<definition id="5">
				<sentence>179 Recall Precision Fallout F-measure ( 13=1 ) Without negative knowledge 33.16 % 32.32 % 7.90 % 32.74 % With both modules 49.74 % 46.38 % 6.54 % 48.00 % Table 4 : Comparison of the system 's performance ( 9 clusters ) with and without the negative knowledge module .</sentence>
				<definiendum id="0">Recall Precision Fallout F-measure</definiendum>
				<definiens id="0">Comparison of the system 's performance ( 9 clusters ) with and without the negative knowledge module</definiens>
			</definition>
			<definition id="6">
				<sentence>WordNet : An On-Line Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>• Preposition Heuristic ( PH ) : The prepositions of the PPs in which the arguments occur often convey good discrimination information for resolving thematic roles ambiguities ( see the `` Preposition in PP '' column in Table 1 ) .</sentence>
				<definiendum id="0">• Preposition Heuristic</definiendum>
				<definiens id="0">The prepositions of the PPs in which the arguments occur often convey good discrimination information for resolving thematic roles ambiguities ( see the `` Preposition in PP '' column</definiens>
			</definition>
			<definition id="1">
				<sentence>Therefore , the argument structure becomes ( AS2 ) `` { Ag } , { Go , So , Re } , { Th } '' Thematic Hierarchy Heuristic guides the learner to test the validity of the passive Form of ( 2.1 ) .</sentence>
				<definiendum id="0">Thematic Hierarchy Heuristic</definiendum>
			</definition>
			<definition id="2">
				<sentence>Preposition Heuristic suggests the learner to to resolve ambiguities based on the prel : ositions of PPs .</sentence>
				<definiendum id="0">Preposition Heuristic</definiendum>
				<definiens id="0">the learner to to resolve ambiguities based on the prel : ositions of PPs</definiens>
			</definition>
			<definition id="3">
				<sentence>VP packets and NP packets recorded syntactic properties of the arguments of verbs and nouns respectively .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">packets recorded syntactic properties of the arguments of verbs and nouns respectively</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>The inside-outside algorithm is an extension of the finite-state based Hidden Markov Model ( by ( Bak79 ) ) , which has been applied successfully in many areas , including speech recognition and part of speech tagging .</sentence>
				<definiendum id="0">inside-outside algorithm</definiendum>
				<definiens id="0">an extension of the finite-state based Hidden Markov Model ( by ( Bak79 ) ) , which has been applied successfully in many areas , including speech recognition and part of speech tagging</definiens>
			</definition>
			<definition id="1">
				<sentence>Accuracy is measured in terms of the percentage of noncrossing constituents in the test corpus , as described above .</sentence>
				<definiendum id="0">Accuracy</definiendum>
				<definiens id="0">measured in terms of the percentage of noncrossing constituents in the test corpus , as described above</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>Definition1 A feature structure is a tuple ( Q , r , 6 , O ) where • Q is a finite set of nodes , • r E Q is the root node , • 6 : QxY r -- -+ Q is a partial feature value function that gives the edges and their labels , and • ( 9 : Q ~ S is a sorting function that gives the labels of the nodes .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiens id="0">a finite set of nodes , • r E Q is the root node</definiens>
				<definiens id="1">a partial feature value function that gives the edges and their labels</definiens>
			</definition>
			<definition id="1">
				<sentence>A default theory is a pair ( D , W ) where D is a set of default inferences and W is a set of sentences from the underlying logic .</sentence>
				<definiendum id="0">default theory</definiendum>
				<definiendum id="1">D</definiendum>
				<definiendum id="2">W</definiendum>
				<definiens id="0">a set of sentences from the underlying logic</definiens>
			</definition>
			<definition id="2">
				<sentence>Such a set of beliefs , cMled an extension , is a closure of W under the usual rules of inference combined with the default rules of inference given in D. An extension E is a minimal closed set containing W and such that if c~ : M fl/7 is a default , and if ~ E E and consistent with E then 7 E E ( that is , if we believe ~x and fl is consistent with what we believe , then we also believe 7 ) .</sentence>
				<definiendum id="0">cMled an extension</definiendum>
				<definiendum id="1">M fl/7</definiendum>
				<definiens id="0">a minimal closed set containing</definiens>
			</definition>
			<definition id="3">
				<sentence>An information system ( IS ) is a triple ( A , C , b ) where A is a countable set of `` atoms , '' Cis a class of finite subsets of A , and tis a binary relation between subsets of A and elements of A. A set X is said to be consistent if every finite subset of X is an element of C. A set G is closed if for every X _C G such that X la , we have a E G. Following thestyle used for information systems , we will write G for the closure of G. In our case , A is the wffs of SFML ( except FALSE ) , and C is the class of satisfiable sets .</sentence>
				<definiendum id="0">information system</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">a triple ( A , C , b ) where A is a countable set of `` atoms , '' Cis a class of finite subsets of A , and tis a binary relation between subsets of A and elements of A. A set X is said to be consistent if every finite subset of X is an element of C. A set</definiens>
				<definiens id="1">a E G. Following thestyle used for information systems</definiens>
				<definiens id="2">the class of satisfiable sets</definiens>
			</definition>
			<definition id="4">
				<sentence>For instance , Pl `` : P2 , P2 -P3 IPl -P3 represents the transitivity of path equations .</sentence>
				<definiendum id="0">P2 -P3 IPl -P3</definiendum>
				<definiens id="0">represents the transitivity of path equations</definiens>
			</definition>
			<definition id="5">
				<sentence>B = ( A B ) II B , where A B is derived from A by eliminating any path that either gets a label or shares a value in B. In the lexicon , each template has both `` strict '' and `` default '' information .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">A B ) II B , where A</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>LCP records mutual similarity of words in a sequence of text .</sentence>
				<definiendum id="0">LCP</definiendum>
			</definition>
			<definition id="1">
				<sentence>The similarity of words , which represents their cohesiveness , is computed using a semantic network .</sentence>
				<definiendum id="0">similarity of words</definiendum>
				<definiens id="0">represents their cohesiveness , is computed using a semantic network</definiens>
			</definition>
			<definition id="2">
				<sentence>Lexical cohesiveness is defined as word similarity ( Kozima and Furugori , 1993 ) computed by spreading activation on a semantic network .</sentence>
				<definiendum id="0">Lexical cohesiveness</definiendum>
				<definiens id="0">word similarity ( Kozima and Furugori , 1993 ) computed by spreading activation on a semantic network</definiens>
			</definition>
			<definition id="3">
				<sentence>VMP is a record of the number of new vocabulary terms introduced in an interval of text .</sentence>
				<definiendum id="0">VMP</definiendum>
				<definiens id="0">a record of the number of new vocabulary terms introduced in an interval of text</definiens>
			</definition>
			<definition id="4">
				<sentence>Si is the word list which can be seen through a fixedwidth window centered on the i-th word of T : Si -- { Wl , Wl+l , `` `` `` , wi-1 , wi , Wi+l , `` `` • , Wr -- 1 , Wr } , 1 =i -- A ( ifi_ &lt; A , thenl=l ) , r = i+A ( if i &gt; N -- A , then r=N ) .</sentence>
				<definiendum id="0">Si</definiendum>
				<definiens id="0">the word list which can be seen through a fixedwidth window centered on the i-th word of T : Si -- { Wl , Wl+l , `` `` `` , wi-1 , wi</definiens>
			</definition>
			<definition id="5">
				<sentence>LCP treats the text T as a word list without any punctuation or paragraph boundaries .</sentence>
				<definiendum id="0">LCP</definiendum>
				<definiens id="0">treats the text T as a word list without any punctuation or paragraph boundaries</definiens>
			</definition>
			<definition id="6">
				<sentence>Cohesiveness of a Word List Lexical cohesiveness c ( Si ) of the word list Si is defined as follows : c ( S ) = w ) , where a ( P ( Si ) , w ) is the activity value of the node w in the activated pattern P ( Si ) .</sentence>
				<definiendum id="0">Cohesiveness of a Word List Lexical cohesiveness c ( Si ) of the word list Si</definiendum>
				<definiendum id="1">P ( Si ) , w )</definiendum>
				<definiens id="0">follows : c ( S ) = w )</definiens>
			</definition>
			<definition id="7">
				<sentence>The definition of c ( Si ) above expresses that c ( Si ) represents semantic homogeneity of S/ , since P ( Si ) represents the average meaning of w 6 S~ .</sentence>
				<definiendum id="0">definition of c ( Si</definiendum>
				<definiendum id="1">Si )</definiendum>
			</definition>
			<definition id="8">
				<sentence>The shape of the window , which defines weight of words in it for pattern production , makes LCP smooth .</sentence>
				<definiendum id="0">shape of the window</definiendum>
				<definiens id="0">defines weight of words in it for pattern production , makes LCP smooth</definiens>
			</definition>
			<definition id="9">
				<sentence>CONCLUSION This paper proposed LCP , an indicator of segment changing , which concentrates on lexical cohesion of a text segment .</sentence>
				<definiendum id="0">LCP</definiendum>
				<definiendum id="1">segment changing</definiendum>
				<definiens id="0">concentrates on lexical cohesion of a text segment</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>STRATIFIED FEATURE GRAMMAR SFG 's key innovation is the generalization of the concept \ ] eature to a sequence of so-called relational signs ( R-signs ) .</sentence>
				<definiendum id="0">FEATURE GRAMMAR SFG 's key innovation</definiendum>
				<definiens id="0">the generalization of the concept \ ] eature to a sequence of so-called relational signs ( R-signs )</definiens>
			</definition>
			<definition id="1">
				<sentence>1 For instance , in Joe gave Mary tea there are , at the clause level , four sister arcs ( arcs with the same source node ) , as shown in Figure h one arc labeled \ [ HI with target gave , indicating gave is the head of the clause ; one with label \ [ 1\ ] and target Joe , indicating Joe is both the predicateargument , and surface subject , of the clause ; one with label \ [ 3,2\ ] and target Mary , indicating that l We use the following R-signs : 1 ( subject ) , 2 ( direct object ) , 3 ( indirect object ) , 8 ( chSmeur ) , Cat ( Category ) , C ( comp ) , F ( flag ) , H ( head ) , LOC ( locative ) , M ( marked ) , as well as the special Null R-signs 0 and/ , explainedbelow .</sentence>
				<definiendum id="0">LOC</definiendum>
				<definiens id="0">arcs with the same source node )</definiens>
				<definiens id="1">direct object ) , 3 ( indirect object</definiens>
				<definiens id="2">the special Null R-signs 0 and/ , explainedbelow</definiens>
			</definition>
			<definition id="2">
				<sentence>Mary is the predicate-argument indirect object , but the surface direct object , of the clause ; and one with label \ [ 2,8\ ] and target tea , indicating tea is the predicate-argument direct object , but surface ch6meur , of the clause .</sentence>
				<definiendum id="0">Mary</definiendum>
				<definiens id="0">the predicate-argument indirect object , but the surface direct object , of the clause</definiens>
				<definiens id="1">the predicate-argument direct object , but surface ch6meur , of the clause</definiens>
			</definition>
			<definition id="3">
				<sentence>The SFG description language includes a class of linear precedence statements , e.g. , ( 1\ ] -4 ( Hi means that in a constituent `` the final subject precedes the head '' .</sentence>
				<definiendum id="0">SFG description language</definiendum>
				<definiens id="0">includes a class of linear precedence statements</definiens>
			</definition>
			<definition id="4">
				<sentence>Then : A. Left-Precedence ( A , n~ ) is true iff : a. All surface arcs which must follow F are incomplete .</sentence>
				<definiendum id="0">A. Left-Precedence</definiendum>
				<definiens id="0">true iff : a. All surface arcs which must follow F are incomplete</definiens>
			</definition>
			<definition id="5">
				<sentence>B. Right-Precedence ( A , n~ ) is true iff : a. All surface arcs which must precede F are complete .</sentence>
				<definiendum id="0">B. Right-Precedence</definiendum>
				<definiens id="0">true iff : a. All surface arcs which must precede F are complete</definiens>
			</definition>
			<definition id="6">
				<sentence>Output : A chart containing all possible parses .</sentence>
				<definiendum id="0">Output</definiendum>
				<definiens id="0">A chart containing all possible parses</definiens>
			</definition>
			<definition id="7">
				<sentence>B. Completions : For c = 1 , ... , k , do repeatedly until no more states can be added to Se : For all = ¢\ ] Se , Qj = \ [ nj , Lj , L~\ ] E SL , , such that Complete ( nj ) and A E SSR-Out-Arcs ( ni ) , such that Left-Precedence ( A , hi ) IF Unify-a~-end-of-Path ( ni , nj , A ) n~ , THEN Add \ [ n~ , Lj , c\ ] to So .</sentence>
				<definiendum id="0">, k</definiendum>
				<definiendum id="1">A E SSR-Out-Arcs</definiendum>
				<definiendum id="2">nj</definiendum>
				<definiens id="0">For c = 1 , ...</definiens>
			</definition>
			<definition id="8">
				<sentence>Each successful completion completes an arc A E SSR-OutArcs ( n~ ) by unifying nj with the target of A. Left completion operates on a state Qi = \ [ ni , Li , c\ ] in the current state-set Sc looking for a state Qj = \ [ nj , Lj , L~\ ] in state-set SL , to complete some arc A E SSR-Out-Arcs ( ni ) .</sentence>
				<definiendum id="0">E SSR-OutArcs</definiendum>
			</definition>
</paper>

		<paper id="1029">
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>Three metrics discussed in those works were the Crossing Parenthesis Score ( a count of the number of phrases in the machine produced parse which cross with one or more phrases in the hand parse ) , Recall ( the percentage of phrases in the hand parse that are also in the machine parse ) , and Precision ( the percentage of phrases in the machine parse that are in the hand parse ) .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">a count of the number of phrases in the machine produced parse which cross with one or more phrases in the hand parse )</definiens>
				<definiens id="1">the percentage of phrases in the machine parse that are in the hand parse )</definiens>
			</definition>
			<definition id="1">
				<sentence>Recall : the percentage of critiques generated from an ideal bracketed corpus that are also present among those in the unbracketed corpus .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">the percentage of critiques generated from an ideal bracketed corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>39 OVERVIEW OF SIMPLIFIED ENGLISH The SE standard consists of a set of grammar , style , format , and vocabulary restrictions , not all of which lend themselves to computational analysis .</sentence>
				<definiendum id="0">OVERVIEW OF SIMPLIFIED ENGLISH The SE standard</definiendum>
				<definiens id="0">consists of a set of grammar , style , format , and vocabulary restrictions , not all of which lend themselves to computational analysis</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>( Axl ) VxVyVz ( f ( x , y ) A I ( x , z ) -- ~ y z ) ( for every feature jr ) ( Ax2 ) W ( A ( = ) ^ B ( . )</sentence>
				<definiendum id="0">Axl ) VxVyVz ( f</definiendum>
				<definiendum id="1">B</definiendum>
				<definiens id="0">A ( = ) ^</definiens>
			</definition>
			<definition id="1">
				<sentence>Note that the third description f ( = , y ) ^ g ( y , = ) ^ h ( y , z ) A f~T is `` cyclic '' with respect to the variables x and y. A feature tree ( examples are shown in Figure 1 ) is a tree whose edges are labeled with features , and whose nodes are labeled with sorts .</sentence>
				<definiendum id="0">f~T</definiendum>
				<definiens id="0">a tree whose edges are labeled with features , and whose nodes are labeled with sorts</definiens>
			</definition>
			<definition id="2">
				<sentence>The symbol c denotes the empty path , which satisfies cp = p = pc for every path p. A path p is called a prefix of a path q , if there exists a path p ' such that pp ' = q. We also assume an infinite alphabet of variables and adopt the convention that x , y , z always denote variables , and X , Y always denote finite , possibly empty sets of variables .</sentence>
				<definiendum id="0">symbol c</definiendum>
				<definiens id="0">the empty path</definiens>
				<definiens id="1">an infinite alphabet of variables and adopt the convention that x , y , z always denote variables</definiens>
			</definition>
			<definition id="3">
				<sentence>A conjunction of atomic formulae can thus be seen as the finite multiset of these formulae , where conjunction is multiset union , and T ( the `` empty conjunction '' ) is the empty multiset .</sentence>
				<definiendum id="0">conjunction</definiendum>
				<definiens id="0">the empty multiset</definiens>
			</definition>
			<definition id="4">
				<sentence>An exclusion constraint is an additional atomic formula of the form zfI ( `` f undefined on x '' ) taken to be equivalent to -~3y ( xfy ) ( for some variable y # z ) .</sentence>
				<definiendum id="0">exclusion constraint</definiendum>
				<definiens id="0">an additional atomic formula of the form zfI ( `` f undefined on x '' ) taken to be equivalent to -~3y ( xfy ) ( for some variable y # z )</definiens>
			</definition>
			<definition id="5">
				<sentence>The theory FT is the set of all sentences that can be obtained as instances of the axiom schemes ( Axl ) , ( Ax2 ) and ( Ax3 ) .</sentence>
				<definiendum id="0">theory FT</definiendum>
				<definiens id="0">the set of all sentences that can be obtained as instances of the axiom schemes ( Axl ) , ( Ax2</definiens>
			</definition>
			<definition id="6">
				<sentence>The theory FTo is the set of all sentences that can be obtained as instances of the first two axiom schemes .</sentence>
				<definiendum id="0">theory FTo</definiendum>
				<definiens id="0">the set of all sentences that can be obtained as instances of the first two axiom schemes</definiens>
			</definition>
			<definition id="7">
				<sentence>Lemma 4.1 Suppose there exists a set of so-called prime formulae such that : straint xfy , and every equation x y such that = ~ y is a prime formula closed prime formula compute a formula 6 that is either prime or .1_ and satisfies flAi'MFT6 and ) 2 ( 6 ) C_V ( flAff ) one can compute a prime formula i ' such that 3xi MFT fl ' and Y ( t ' ) C_ Y ( 3xfl ) ft ^ 3= ( t ^ i=1 i -- -- 1 variable x one can compute a Boolean combination 6 of prime formulae such that 3~ ( fl^- , ¢ ) I~FT 6 and Vff ) C VO= ( fl^-~l ' ) ) .</sentence>
				<definiendum id="0">C_V</definiendum>
				<definiendum id="1">Vff ) C VO=</definiendum>
				<definiens id="0">a prime formula closed prime formula compute a formula</definiens>
			</definition>
			<definition id="8">
				<sentence>If ¢ is an atomic formula Ax , xfy or x y , then ¢ is either a prime formula , or ¢ is a trivial equation z z , in which case it is equivalent to the prime formula T. If ¢ is -~¢ , ¢ ^ ¢ ' or ¢ V ¢ ' , then the claim follows immediately with the induction hypothesis .</sentence>
				<definiendum id="0">¢</definiendum>
				<definiens id="0">an atomic formula Ax , xfy or x y</definiens>
				<definiens id="1">either a prime formula , or</definiens>
				<definiens id="2">a trivial equation z z , in which case it is equivalent to the prime formula T. If ¢ is -~¢ , ¢ ^ ¢ ' or ¢ V ¢ '</definiens>
			</definition>
			<definition id="9">
				<sentence>A basic formula is either 3or a possibly empty conjunction of atomic formulae of the form Ax , xfy , and x y. Note that T is a basic formula since T is the empty conjunction .</sentence>
				<definiendum id="0">basic formula</definiendum>
				<definiens id="0">a basic formula since T is the empty conjunction</definiens>
			</definition>
			<definition id="10">
				<sentence>If.4 is a model of the theory FTo , then every paths denotes a unary partial function on the universe of .4 .</sentence>
				<definiendum id="0">If.4</definiendum>
				<definiens id="0">a model of the theory FTo , then every paths denotes a unary partial function on the universe of .4</definiens>
			</definition>
			<definition id="11">
				<sentence>A feature pregraph is a pair ( x , 7 ) consisting of a variable x ( called the root ) and a solved clause 7 not containing exclusion constraints such that , for every variable y occurring in 7 , there exists a path p satisfying xpy E \ [ 7\ ] If one deletes the exclusion constraints in Figure 2 , one obtains the graphical representation of a feature pregraph whose root is x. A feature pregraph ( x , 7 ) is called a subpregraph of a feature pregraph ( y , ~ ) if 7 _C 6 and x -y or x E \ ] 2 ( ~ ) .</sentence>
				<definiendum id="0">feature pregraph</definiendum>
				<definiens id="0">a pair ( x , 7 ) consisting of a variable x ( called the root</definiens>
			</definition>
			<definition id="12">
				<sentence>The closure of a prime formula 3X7 is defined as follows : \ [ 3xv\ ] : = { ~ e \ [ 7\ ] I v ( ~ ) n x = ~ or ~ = xc~ or ~ = =¢ 1=~ } The proper closure of a prime formula/3 is defined as follows : \ [ /3\ ] * : = { Tr • \ [ /3\ ] I r is a proper path constraint } .</sentence>
				<definiendum id="0">closure of a prime formula 3X7</definiendum>
				<definiendum id="1">] I r</definiendum>
				<definiens id="0">a proper path constraint }</definiens>
			</definition>
			<definition id="13">
				<sentence>A rooted path xp consists of a variable x and a path p. A rooted path xp is called unfree in a prime formula 13 if 3 prefix p ' of p 3 yq : x 5£ y and xp ' I Yq E \ [ /3\ ] .</sentence>
				<definiendum id="0">rooted path xp</definiendum>
				<definiens id="0">consists of a variable x and a path p. A rooted path xp is called unfree in a prime formula 13 if 3 prefix p ' of p 3 yq : x 5£ y and xp</definiens>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>A violation of this principle signifies the presence of abnormal groupings ( fallacious word boundaries ) , which must be removed , a For example , the fallacious grouping rensheng 'life ' , if it exists in ( 1 ) , can be detected by observing a violation of the syntactic relation between this group and le , which is ment shi fen in ( 2 ) : adverb shifen 'very ' can not occur at the end of a sentence .</sentence>
				<definiendum id="0">le</definiendum>
				<definiens id="0">a violation of the syntactic relation between this group and</definiens>
			</definition>
			<definition id="1">
				<sentence>• radical restructuring The characters ren and sheng will be grouped as a word rensheng 'life ' by bottom-up , character-based codelets , as the associative strength between them is strong ( 3.75 ) .</sentence>
				<definiendum id="0">character-based codelets</definiendum>
			</definition>
</paper>

		<paper id="1025">
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>6 The input to the segmentation algorithm is a list of 4-tuples representing all the referential NPs in a narrative .</sentence>
				<definiendum id="0">segmentation algorithm</definiendum>
				<definiens id="0">a list of 4-tuples representing all the referential NPs in a narrative</definiens>
			</definition>
			<definition id="1">
				<sentence>FIC , * is the current clause ( at location n ) ; CD , is the set of all indices for NPs in FIC , ; F , is the set of entities that are inferrentially linked to entities in CDn ; PRO , , is the subset of CD , where NP is a third person definite pronoun ; CDn-1 is the contextual domain for the previous FIC , and CDs is the contextual domain for the current segment .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiendum id="1">CDn-1</definiendum>
				<definiendum id="2">CDs</definiendum>
				<definiens id="0">the current clause ( at location n ) ; CD , is the set of all indices for NPs in FIC , ; F , is the set of entities that are inferrentially linked to entities in CDn</definiens>
				<definiens id="1">the subset of CD , where</definiens>
				<definiens id="2">a third person definite pronoun ;</definiens>
				<definiens id="3">the contextual domain for the current segment</definiens>
			</definition>
			<definition id="2">
				<sentence>Unsurprisingly , RA performs most like humans .</sentence>
				<definiendum id="0">RA</definiendum>
				<definiens id="0">performs most like humans</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>HBG incorporates lexical , syntactic , semantic , and structural information from the parse tree into the disambiguation process in a novel way .</sentence>
				<definiendum id="0">HBG</definiendum>
				<definiens id="0">incorporates lexical , syntactic , semantic , and structural information from the parse tree into the disambiguation process in a novel way</definiens>
			</definition>
			<definition id="1">
				<sentence>We propose a probabilistic model of context for disambiguation in parsing , HBG , which incorporates the intuitions of these previous works into one unified framework .</sentence>
				<definiendum id="0">HBG</definiendum>
				<definiens id="0">a probabilistic model of context for disambiguation in parsing</definiens>
			</definition>
			<definition id="2">
				<sentence>Let p ( T , w~ ) be the joint probability of generating the word string w~ and the parse tree T. Given w~ , our parser chooses as its parse tree that tree T* for which T '' =arg maxp ( T , w~ ) ( 1 ) T6~ ( ~ ) where ~ ( w~ ) is the set of all parses produced by the grammar for the sentence w~ .</sentence>
				<definiendum id="0">w~ )</definiendum>
				<definiens id="0">the joint probability of generating the word string w~ and the parse tree T. Given w~ , our parser chooses as its parse tree that tree T* for which T '' =arg maxp ( T ,</definiens>
				<definiens id="1">the set of all parses produced by the grammar for the sentence w~</definiens>
			</definition>
			<definition id="3">
				<sentence>A probabilistic language model attempts to estimate the probability of a sequence of sentences and their respective interpretations ( parse trees ) occurring in the language , : P ( SI TI S2 T2 ... S , , T , ~ ) .</sentence>
				<definiendum id="0">probabilistic language model</definiendum>
				<definiens id="0">attempts to estimate the probability of a sequence of sentences and their respective interpretations ( parse trees ) occurring in the language , : P ( SI TI S2 T2 ... S , , T</definiens>
			</definition>
			<definition id="4">
				<sentence>Our most common test set consists of 1600 sentences that are never seen by the grammarian .</sentence>
				<definiendum id="0">common test set</definiendum>
				<definiens id="0">consists of 1600 sentences that are never seen by the grammarian</definiens>
			</definition>
			<definition id="5">
				<sentence>We also measure the parse base , which is defined as the geometric mean of the number of proposed parses on a per word basis , to quantify the ambiguity of the grammar .</sentence>
				<definiendum id="0">parse base</definiendum>
				<definiens id="0">the geometric mean of the number of proposed parses on a per word basis , to quantify the ambiguity of the grammar</definiens>
			</definition>
			<definition id="6">
				<sentence>The Vi $ erbi rate of P-CFG is 60 % on the same test corpus of 760 sentences used in our experiments .</sentence>
				<definiendum id="0">P-CFG</definiendum>
			</definition>
			<definition id="7">
				<sentence>Pearl : A Probabilistic Chart Parser .</sentence>
				<definiendum id="0">Pearl</definiendum>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>THE ALGORITHM The algorithm consists of two parts : a hypotheses construction and a response generation phase .</sentence>
				<definiendum id="0">ALGORITHM The algorithm</definiendum>
				<definiens id="0">consists of two parts : a hypotheses construction and a response generation phase</definiens>
			</definition>
			<definition id="1">
				<sentence>When a precondition to be expanded is of the form `` Know ( IS , x ) '' and the system knows the value of `` x '' , it includes such information in the response ; so , the expansion is avoided .</sentence>
				<definiendum id="0">Know (</definiendum>
				<definiens id="0">IS , x ) '' and the system knows the value of `` x '' , it includes such information in the response ; so , the expansion is avoided</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>A dimensionality reduction creates a space representing the syntactic categories of unambiguous words .</sentence>
				<definiendum id="0">dimensionality reduction</definiendum>
				<definiens id="0">creates a space representing the syntactic categories of unambiguous words</definiens>
			</definition>
			<definition id="1">
				<sentence>Buckshort applies a high-quality quadratic clustering algorithm to a random sample of size v/k-n , where k is the number of desired cluster centers and n is the number of vectors to be clustered .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">the number of desired cluster centers and n is the number of vectors to be clustered</definiens>
			</definition>
			<definition id="2">
				<sentence>Ambiguity is a problem for the vector representation scheme used here , because the two components of an ambiguous vector can add up in a way that makes it by chance similar to an unambiguous word of a different syntactic category .</sentence>
				<definiendum id="0">Ambiguity</definiendum>
				<definiens id="0">makes it by chance similar to an unambiguous word of a different syntactic category</definiens>
			</definition>
			<definition id="3">
				<sentence>The dimensionality reduction makes the global distributional pattern of a word available in a profile consisting of a dozen or so real numbers .</sentence>
				<definiendum id="0">dimensionality reduction</definiendum>
				<definiens id="0">makes the global distributional pattern of a word available in a profile consisting of a dozen or so real numbers</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>CAPERS is a massively parallel network of processing nodes that represent syntactic phrases and their attachments within a parse tree .</sentence>
				<definiendum id="0">CAPERS</definiendum>
				<definiens id="0">a massively parallel network of processing nodes that represent syntactic phrases and their attachments within a parse tree</definiens>
			</definition>
			<definition id="1">
				<sentence>The network incorporates each input phrase through a parallel atomic operation that determines both the initial attachment for the current phrase and any revision of earlier attachments .</sentence>
				<definiendum id="0">atomic operation</definiendum>
				<definiens id="0">determines both the initial attachment for the current phrase and any revision of earlier attachments</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus , CAPERS avoids the problems of purely serial or race-based models that rely on backtracking , which is cognitively implausible , or explicit revision strate266 gies , which can be unrestrictive ( Abney 89 ; Frazier 78 ; Inoue &amp; Fodor 92 ; McRoy &amp; Hirst 90 ; Pritchett 88 ) .</sentence>
				<definiendum id="0">CAPERS</definiendum>
				<definiens id="0">avoids the problems of purely serial or race-based models that rely on backtracking</definiens>
			</definition>
			<definition id="3">
				<sentence>Grammaticality of Attachments Unlike other connectionist parsers ( Cottrell 89 ; Fanty 85 ; Selman &amp; Hirst 85 ) , CAPERS is a hybrid model whose limited symbolic processing abilities support the direct representation of the grammar of a current linguistic theory .</sentence>
				<definiendum id="0">CAPERS</definiendum>
				<definiens id="0">a hybrid model whose limited symbolic processing abilities support the direct representation of the grammar of a current linguistic theory</definiens>
			</definition>
			<definition id="4">
				<sentence>Competitionbased spreading activation ( CBSA ) is a newer technique that achieves competitive behavior indirectly : competing nodes vie for output from a common neighbor , which allocates its activation between the competitors .</sentence>
				<definiendum id="0">Competitionbased spreading activation</definiendum>
				<definiendum id="1">CBSA )</definiendum>
			</definition>
			<definition id="5">
				<sentence>aj • ( 1 ) Oji = ak k where : oji is the output from node ni to node nj ; ai is the activation of node hi ; k ranges over all nodes connected to node hi .</sentence>
				<definiendum id="0">oji</definiendum>
				<definiendum id="1">ai</definiendum>
				<definiens id="0">the output from node ni to node nj</definiens>
				<definiens id="1">the activation of node hi ; k ranges over all nodes connected to node hi</definiens>
			</definition>
			<definition id="6">
				<sentence>The allowable attachment configurations are a direct consequence of the restrictions imposed by the competitive mechanism of CAPERS .</sentence>
				<definiendum id="0">allowable attachment configurations</definiendum>
				<definiens id="0">a direct consequence of the restrictions imposed by the competitive mechanism of CAPERS</definiens>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>The intentional planner builds a plan to achieve the intentional goal , and the discourse realizer generates utterances to convey information based on the intentional plan .</sentence>
				<definiendum id="0">discourse realizer</definiendum>
				<definiens id="0">generates utterances to convey information based on the intentional plan</definiens>
			</definition>
			<definition id="1">
				<sentence>Utterance ( 2 ) results from the Address-Believability action , which is a subaction of Inform , to support the claim in ( 1 ) .</sentence>
				<definiendum id="0">Address-Believability action</definiendum>
			</definition>
			<definition id="2">
				<sentence>Modify-Acts has two specializations , Remove-Act , which removes the incorrect action ( and all of its children ) , and Alter-Act , which generalizes the proposed action so that the plan will be well-formed .</sentence>
				<definiendum id="0">Remove-Act</definiendum>
				<definiendum id="1">Alter-Act</definiendum>
				<definiens id="0">removes the incorrect action ( and all of its children</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>We have been engaged in work in the area of multimodal communication for several years now , starting with the HAM-ANS ( Wahlster et al. 1983 ) and VITRA systems ( Wahlster 1989 ) , which automatically create natural language descriptions of pictures and image sequences shown on the screen .</sentence>
				<definiendum id="0">VITRA systems</definiendum>
				<definiens id="0">automatically create natural language descriptions of pictures and image sequences shown on the screen</definiens>
			</definition>
			<definition id="1">
				<sentence>The presentation goal is a formal representation of the communicative intent specified by a back-end application system .</sentence>
				<definiendum id="0">presentation goal</definiendum>
				<definiens id="0">a formal representation of the communicative intent specified by a back-end application system</definiens>
			</definition>
			<definition id="2">
				<sentence>WIP is a highly adaptive multimodal presentation system , since all of its output is generated on the fly and customized for the intended discourse situation .</sentence>
				<definiendum id="0">WIP</definiendum>
				<definiens id="0">a highly adaptive multimodal presentation system</definiens>
			</definition>
</paper>

	</volume>
