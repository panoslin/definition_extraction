<?xml version="1.0" encoding="UTF-8"?>
	<volume id="W01">

		<paper id="1501">
</paper>

		<paper id="0505">
</paper>

		<paper id="1412">
			<definition id="0">
				<sentence>Dependency-linked N-gram Dependency-linked N-gram assumes prior knowledge of dependency links among chunks .</sentence>
				<definiendum id="0">Dependency-linked N-gram Dependency-linked N-gram</definiendum>
			</definition>
			<definition id="1">
				<sentence>In fact , Dependency-linked N-gram is an enhanced model of the Chunk-bound model in that , Dependency-linked N-gram extends chunk boundaries via dependency links .</sentence>
				<definiendum id="0">Dependency-linked N-gram</definiendum>
				<definiens id="0">an enhanced model of the Chunk-bound model in that , Dependency-linked N-gram extends chunk boundaries via dependency links</definiens>
			</definition>
			<definition id="2">
				<sentence>The correlation score between translation units a41a43a42 and a41a45a44 is calculated by the weighted Dice Coefficient defined as : a46a48a47a50a49a52a51a53a41a43a42a55a54a50a41a45a44a57a56a59a58a60a51a62a61a62a63a65a64a67a66a69a68a70a42a71a44a48a56a73a72 a68a2a42a74a44 a68a70a42a76a75a77a68a65a44 model English Japanese Bound-length 8479 11587 Chunk-bound 4865 5870 Dependency-linked 8716 11068 Table 1 : Number of Translation Units where a68a55a44 and a68a70a42 are the numbers of occurrences of a41 a42 anda41 a44 in Japanese and English corpora respectively , and a68a70a42a71a44 is the number of co-occurrences of a41a43a42 and a41a45a44 .</sentence>
				<definiendum id="0">a68a70a42a71a44</definiendum>
				<definiens id="0">the numbers of occurrences of a41 a42 anda41 a44 in Japanese and English corpora respectively , and</definiens>
				<definiens id="1">the number of co-occurrences of a41a43a42 and a41a45a44</definiens>
			</definition>
			<definition id="3">
				<sentence>Accuracy is the number of correct translation pairs over the extracted translation pairs in the algorithm .</sentence>
				<definiendum id="0">Accuracy</definiendum>
				<definiens id="0">the number of correct translation pairs over the extracted translation pairs in the algorithm</definiens>
			</definition>
			<definition id="4">
				<sentence>“found” is the number of content tokens matched with correct translation pairs .</sentence>
				<definiendum id="0">“found”</definiendum>
				<definiens id="0">the number of content tokens matched with correct translation pairs</definiens>
			</definition>
			<definition id="5">
				<sentence>“ideal” is the upper bound of content tokens that should be found by the algorithm ; it is the total number of content tokens in the translation units whose co-occurrence frequency is at least “a68 a78a71a79a55a80a81a80 ” times in the original parallel corpora .</sentence>
				<definiendum id="0">“ideal”</definiendum>
				<definiens id="0">the upper bound of content tokens that should be found by the algorithm ; it is the total number of content tokens in the translation units whose co-occurrence frequency is at least “a68 a78a71a79a55a80a81a80 ” times in the original parallel corpora</definiens>
			</definition>
			<definition id="6">
				<sentence>The prefix “i ” is the fraction of found tokens over ideal tokens and the prefix “t ” is the fraction of found tokens over the total number of both content and functional tokens in the data .</sentence>
				<definiendum id="0">prefix “i ”</definiendum>
				<definiens id="0">the fraction of found tokens over ideal tokens and the prefix “t ” is the fraction of found tokens over the total number of both content and functional tokens in the data</definiens>
			</definition>
			<definition id="7">
				<sentence>a91 a92a71a93a96a95a62a95 found ( E ) ideal ( E ) i cover ( E ) t cover ( E ) found ( J ) ideal ( J ) i cover ( J ) t cover ( J ) 100.0 0 445 0 0 0 486 0 0 50.0 0 1182 0 0 0 1274 0 0 25.0 46 2562 0.0179 0.0015 46 2564 0.0179 0.0011 12.0 156 4275 0.0364 0.0051 146 4407 0.0331 0.0037 10.0 344 4743 0.0725 0.0113 334 4935 0.0676 0.0086 Table 5 : Coverage ( Bound-length N-gram ) a91 a92a71a93a96a95a62a95 found ( E ) ideal ( E ) i cover ( E ) t cover ( E ) found ( J ) ideal ( J ) i cover ( J ) t cover ( J ) 100.0 52 1374 0.0378 0.0017 52 1338 0.0388 0.0013 50.0 371 2813 0.1318 0.0122 372 2643 0.1407 0.0095 25.0 695 5019 0.1384 0.0229 696 4684 0.1485 0.0179 12.0 1251 7129 0.1754 0.0413 1246 6873 0.1812 0.0320 10.0 1478 7629 0.1937 0.0488 1463 7441 0.1966 0.0376 Table 6 : Coverage ( Chunk-bound N-gram ) a91a57a92a71a93a96a95a62a95 found ( E ) ideal ( E ) i cover ( E ) t cover ( E ) found ( J ) ideal ( J ) i cover ( J ) t cover ( J ) 100.0 52 1370 0.0379 0.0017 52 1334 0.0389 0.0013 50.0 370 2806 0.1318 0.0122 371 2629 0.1411 0.0095 25.0 693 5010 0.1383 0.0229 694 4675 0.1484 0.0178 12.0 1238 7117 0.1739 0.0409 1233 6845 0.1801 0.0317 10.0 1429 7611 0.1877 0.0472 1424 7428 0.1917 0.0366 Table 7 : Coverage ( Dependency-linked N-gram ) Bounded Chunk Dependency ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 1 ) 1992 ( 5 ) 237 ( 2 ) 115 ( 6 ) 471 ( 3 ) 1447 ( 7 ) 331 ( 4 ) 48 Figure 5 : Venn diagram Of the three models , Chunk-bound N-gram yields the best performance both in accuracy ( 83 % ) and in coverage ( 60 % ) 3 .</sentence>
				<definiendum id="0">Coverage</definiendum>
				<definiendum id="1">Chunk-bound N-gram</definiendum>
				<definiens id="0">Bound-length N-gram ) a91 a92a71a93a96a95a62a95 found ( E ) ideal ( E ) i cover ( E ) t cover</definiens>
				<definiens id="1">Chunk-bound N-gram ) a91a57a92a71a93a96a95a62a95 found ( E ) ideal ( E ) i cover ( E ) t cover ( E ) found ( J ) ideal</definiens>
				<definiens id="2">Coverage ( Dependency-linked N-gram ) Bounded Chunk Dependency</definiens>
			</definition>
			<definition id="8">
				<sentence>Dependency-linked N-gram follows a similar transition of accuracy and coverage as Chunkbound N-gram .</sentence>
				<definiendum id="0">Dependency-linked N-gram</definiendum>
			</definition>
</paper>

		<paper id="1618">
</paper>

		<paper id="1510">
			<definition id="0">
				<sentence>Strong equivalence means that both grammars generate exactly equivalent parse results , and that we can share the LTAG grammars and lexicons in HPSG applications .</sentence>
				<definiendum id="0">Strong equivalence</definiendum>
			</definition>
			<definition id="1">
				<sentence>The tree converter module is a core module of the system , which is an implementation of the grammar conversion algorithm given in Section 3 .</sentence>
				<definiendum id="0">tree converter module</definiendum>
			</definition>
			<definition id="2">
				<sentence>Adjoining Grammar ( FB-LTAG ) LTAG ( Schabes et al. , 1988 ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two operaArg : we can run ID grammar rule unify Sym : NP Arg : Sym : VP Arg : VP Sym : VP Arg : NP Arg : Sym : Arg : 2 3 2 unify 3 unify ID grammar rule we can run Sym : NP Arg : Sym : VP Arg : VP Sym : VP Arg : NP Arg : NPSym : Arg : Arg : 1 1 | 2 Arg : 2 unify we can run Sym : NP Arg : Sym : VP Arg : VP Sym : VP Arg : NP Arg : NP Arg : Figure 6 : Parsing with an HPSG grammar S NP VP V run NP N We substitution α1 α2 S NP VP V run N We Figure 3 : Substitution VP VPV can * adjunction β1 S NP VP V run N We S NP VP VPV can N We V run Figure 4 : Adjunction tions called substitution and adjunction .</sentence>
				<definiendum id="0">operaArg</definiendum>
				<definiendum id="1">VP Arg</definiendum>
				<definiens id="0">a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two</definiens>
				<definiens id="1">Parsing with an HPSG grammar S NP VP V run NP N We substitution α1 α2 S NP VP V run N We Figure 3 : Substitution VP VPV can * adjunction β1 S NP VP V run N We S NP VP VPV can N We V run Figure 4 : Adjunction tions called substitution and adjunction</definiens>
			</definition>
			<definition id="3">
				<sentence>Substitution replaces a substitution node with another initial tree ( Figure 3 ) .</sentence>
				<definiendum id="0">Substitution</definiendum>
			</definition>
			<definition id="4">
				<sentence>Adjunction grafts an auxiliary tree with the root node and foot node labeled DC onto an internal node of another tree with the same symbol DC ( Figure 4 ) .</sentence>
				<definiendum id="0">Adjunction grafts</definiendum>
			</definition>
			<definition id="5">
				<sentence>A derivation tree is a structural description in LTAG and represents the history of combinations of elementary trees .</sentence>
				<definiendum id="0">derivation tree</definiendum>
			</definition>
			<definition id="6">
				<sentence>Grammar ( HPSG ) An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( Carpenter , 1992 ) .</sentence>
				<definiendum id="0">Grammar ( HPSG</definiendum>
				<definiendum id="1">HPSG grammar</definiendum>
			</definition>
			<definition id="7">
				<sentence>An ID grammar rule represents a relation between a mother and its daughters , and is independent of lexical characteristics .</sentence>
				<definiendum id="0">ID grammar rule</definiendum>
				<definiens id="0">a relation between a mother and its daughters , and is independent of lexical characteristics</definiens>
			</definition>
			<definition id="8">
				<sentence>The conversion algorithm consists of : HPSG lexical entries .</sentence>
				<definiendum id="0">conversion algorithm</definiendum>
			</definition>
			<definition id="9">
				<sentence>A feature Sym represents the non-terminal symbol of the mother node .</sentence>
				<definiendum id="0">feature Sym</definiendum>
			</definition>
			<definition id="10">
				<sentence>The Arg feature of the substitution node must be a null list , because the substitution node must be unified only with the node corresponding to the root node of the initial tree .</sentence>
				<definiendum id="0">Arg feature of the substitution node</definiendum>
				<definiens id="0">the node corresponding to the root node of the initial tree</definiens>
			</definition>
			<definition id="11">
				<sentence>The strong equivalence holds also for conversion of non-canonical elementary trees .</sentence>
				<definiendum id="0">strong equivalence</definiendum>
			</definition>
			<definition id="12">
				<sentence>For trees violating Condition 1 , we can distinguish the cut-off Table 1 : The classification of elementary tree templates in the XTAG English grammar ( LTAG ) and converted lexical entry templates corresponding to them ( HPSG ) : BT : canonical elementary trees , BU : elementary trees violating only Condition 1 , BV : elementary trees violating only Condition 2 , BW : elementary trees violating both conditions Grammar BT BU BV BW Total LTAG 326 764 54 50 1,194 HPSG 326 1,992 1,083 2,474 5,875 nodes from the substitution nodes owing to identifiers , which recover the co-occurrence relation in the original elementary trees between the divided trees .</sentence>
				<definiendum id="0">BU</definiendum>
				<definiendum id="1">BV</definiendum>
				<definiens id="0">The classification of elementary tree templates in the XTAG English grammar ( LTAG ) and converted lexical entry templates corresponding to them ( HPSG ) : BT : canonical elementary trees ,</definiens>
			</definition>
			<definition id="13">
				<sentence>LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; Torisawa et al. , 2000 ) .</sentence>
				<definiendum id="0">LiLFeS</definiendum>
			</definition>
			<definition id="14">
				<sentence>lem 19.64 TNT 0.77 The XTAG English grammar consists of 1,194 4 elementary tree templates and around 45,000 lexical items 5 .</sentence>
				<definiendum id="0">XTAG English grammar</definiendum>
			</definition>
			<definition id="15">
				<sentence>TNT refers to the HPSG parser ( Torisawa et al. , 2000 ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .</sentence>
				<definiendum id="0">TNT</definiendum>
				<definiendum id="1">HPSG parser</definiendum>
				<definiens id="0">performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 )</definiens>
			</definition>
</paper>

		<paper id="0503">
</paper>

		<paper id="1607">
			<definition id="0">
				<sentence>Communicator is a DARPA-funded program involving major industry and academic sites , established to provide the next generation of intelligent conversational interfaces to distributed information .</sentence>
				<definiendum id="0">Communicator</definiendum>
				<definiens id="0">a DARPA-funded program involving major industry and academic sites , established to provide the next generation of intelligent conversational interfaces to distributed information</definiens>
			</definition>
			<definition id="1">
				<sentence>Expert : WHAT TIME DO YOU [ exp-init ] NEED TO DEPART User : AS SOON AS POSSIBLE [ exp-init ] AFTER FIVE P.M. Expert : THE FIRST FLIGHT AFTER [ exp-init ] FIVE P.M. ON THAT DATE IS AT FIVE THIRTY FIVE P.M. ARRIVING IN CHICAGO AT SIX OH SIX P.M. ON U.S. AIR User : IS THAT O’HARE [ user-init ] Table 3 : Initiative tagging in an HH Exchange ( 1 ) Expert : i have an American [ exp-init ] Airlines ight departing Seattle at twelve fty ve p.m. , arrives Tokyo at three p.m. the next day .</sentence>
				<definiendum id="0">AFTER FIVE P.M. Expert</definiendum>
				<definiens id="0">THE FIRST FLIGHT AFTER [ exp-init ] FIVE P.M. ON THAT DATE IS AT FIVE THIRTY FIVE P.M. ARRIVING IN CHICAGO AT SIX OH SIX P.M. ON U.S. AIR User</definiens>
			</definition>
			<definition id="2">
				<sentence>These numbers are re ective of the system designers’ decisions for their systems , and that means all DAs are not going to be used by all systems ( i.e. system’s repertoire ) .</sentence>
				<definiendum id="0">all systems</definiendum>
				<definiens id="0">re ective of the system designers’ decisions for their systems , and that means all DAs are not going to be used by</definiens>
			</definition>
</paper>

		<paper id="0724">
</paper>

		<paper id="0710">
			<definition id="0">
				<sentence>2 David Cohn , Les Atlas , Richard Ladner , Improving Generalization with Active Learning , Machine Learning , v.15 n.2 , p.201-221 , May 1994 3 Michael Collins , Three generative , lexicalised models for statistical parsing , Proceedings of the 35th annual meeting on Association for Computational Linguistics , p.16-23 , July 07-12 , 1997 , Madrid , Spain 4 Sean P. Engelson , Ido Dagan , Minimizing manual annotation cost in supervised training from corpora , Proceedings of the 34th annual meeting on Association for Computational Linguistics , p.319-326 , June 24-27 , 1996 , Santa Cruz , California 5 Yoav Freund , H. Sebastian Seung , Eli Shamir , Naftali Tishby , Selective Sampling Using the Query by Committee Algorithm , Machine Learning , v.28 n.2-3 , p.133-168 , Aug./Sept. 1997 6 Atsushi Fujii , Takenobu Tokunaga , Kentaro Inui , Hozumi Tanaka , Selective sampling for example-based word sense disambiguation , Computational Linguistics , v.24 n.4 , p.573-597 , December 1998 7 Rebecca Hwa , Stuart Shieber , Learning probabilistic lexicalized grammars for natural language processing , 2001 8 Rebecca Hwa , Sample selection for statistical grammar induction , Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora : held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics , p.45-52 , October 07-08 , 2000 , Hong Kong 9 David D. Lewis and Jason Catlett .</sentence>
				<definiendum id="0">Learning probabilistic lexicalized grammars</definiendum>
				<definiens id="0">Selective sampling for example-based word sense disambiguation</definiens>
			</definition>
</paper>

		<paper id="0721">
</paper>

		<paper id="0814">
			<definition id="0">
				<sentence>( c7 ) Anaphora/ellipsis : Each anaphoric expression and ellipsis should be reconsidered with the options including at least the following : – NP with a demonstrative adjective “kono/sono ( this/that ) ” – bare NP without a demonstrative adjective – head noun with a demonstrative adjective – demonstrative pronoun “kore/sore/ ( this/that ) ” – personal pronoun – ellipsis ( zero pronoun ) We then implemented the above choice system on our paraphrasing engine FUNE ( Fujita et al. , 2000 ) , and obtained 1,343 paraphrase instances from the 195 source instances , which were those randomly sampled from the above 275 source instances .</sentence>
				<definiendum id="0">paraphrasing engine FUNE</definiendum>
				<definiens id="0">Each anaphoric expression and ellipsis should be reconsidered with the options including at least the following : – NP with a demonstrative adjective “kono/sono ( this/that ) ” – bare NP without a demonstrative adjective – head noun with a demonstrative adjective – demonstrative pronoun “kore/sore/ ( this/that ) ” – personal pronoun – ellipsis ( zero pronoun</definiens>
			</definition>
			<definition id="1">
				<sentence>Concession Reason Concession Reason ( a ) ( b ) Presentation ( C ) Attitude ( B ) Description ( A ) ... shita-ga [ Concession ] ... node [ Reason ] ... nagara [ ParallelEvent ] Figure 1 : Hierarchical structure of Japanese sentence In order to apply this constraint to our task of clause ordering , we need to extend it in the following respects : a5 Since Minami’s theory covers only intrasentential rhetorical structures , we need to prove whether it holds beyond sentence boundaries , and also whether it holds even if the nucleus precedes the satellite ( In a Japanese sentence , a subordinate ( satellite ) clause always precedes the matrix clause ( nucleus ) it depends on . )</sentence>
				<definiendum id="0">Concession Reason Concession Reason</definiendum>
				<definiens id="0">A ) ... shita-ga [ Concession ] ... node [ Reason ] ...</definiens>
			</definition>
			<definition id="2">
				<sentence>Constraint 1.1 If three continuous discourse segments constitute either of the rhetorical patterns ( A ) or ( B ) shown in Figure 2 , relation a20a22a21 should be of a higher level of the rhetorical hierarchy than relation a20a24a23 , where : a5 ELABORATION constitutes a new class whose level in the hierarchy is higher than the level ( B ) ( e.g. REASON ) , and lower than the level ( C ) ( e.g. CONCESSION ) , and a5 the constraint holds beyond sentence boundaries , and is independent of the order of the nucleus and satellite , except that pattern ( A ) is not acceptable if a20a24a23 is ELABORATION .</sentence>
				<definiendum id="0">ELABORATION</definiendum>
				<definiens id="0">constitutes a new class whose level in the hierarchy is higher than the level ( B ) ( e.g. REASON ) , and lower than the level ( C ) ( e.g. CONCESSION</definiens>
			</definition>
			<definition id="3">
				<sentence>The result is shown in Table 1 , where the recall is the ratio of the instances that the system correctly judged positive ( 116 instances ) to all the positive instances ( 133 instances ) , whereas the precision is the ratio of the instances that the system correctly judged positive ( 116 instances ) to all the instances that the system judged positive ( 155 instances ) .</sentence>
				<definiendum id="0">the recall</definiendum>
				<definiens id="0">the ratio of the instances that the system correctly judged positive ( 116 instances ) to all the positive instances ( 133 instances ) , whereas the precision is the ratio of the instances that the system correctly judged positive ( 116 instances ) to all the instances that the system judged positive ( 155 instances )</definiens>
			</definition>
</paper>

		<paper id="1003">
</paper>

		<paper id="1627">
			<definition id="0">
				<sentence>The health professional is the \expert '' in the conventional sense , and at times conveys medical information to the less knowledgeable patient in a conventional way .</sentence>
				<definiendum id="0">health professional</definiendum>
				<definiens id="0">the \expert '' in the conventional sense , and at times conveys medical information to the less knowledgeable patient in a conventional way</definiens>
			</definition>
			<definition id="1">
				<sentence>PMG ( Maguire &amp; Faulkner 1988 ; Maguire p.c. ) have separate tagsets for Form , Function , Content , Level , Cue , Cue Management , Blocking , and Focus .</sentence>
				<definiendum id="0">PMG</definiendum>
				<definiens id="0">separate tagsets for Form , Function , Content , Level , Cue , Cue Management , Blocking , and Focus</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>The genre of a document is defined here as a label which denotes a set of conventions in the way in which information is presented .</sentence>
				<definiendum id="0">genre of a document</definiendum>
			</definition>
			<definition id="1">
				<sentence>Denote by P ( w ) the probability that a randomly chosen token k is an occurrence of the word w. Let g ( k ) be the genre of the document in which k occurs , P ( gi ) be the probability that a randomly chosen k has g ( k ) = gi , and P ( gi | w ) be the conditional probability that a randomly chosen k has g ( k ) = gi , given that k is an occurrence of the word w. Denote by |S| the number of members in any set S. An overstrike denotes the negation of a condition .</sentence>
				<definiendum id="0">P ( w )</definiendum>
				<definiens id="0">the probability that a randomly chosen token k is an occurrence of the word w. Let g ( k ) be the genre of the document in which k occurs , P ( gi ) be the probability that a randomly chosen k has g ( k ) = gi , and P ( gi | w ) be the conditional probability that a randomly chosen k has g ( k ) = gi , given that k is an occurrence of the word w. Denote by |S| the number of members in any set S. An overstrike denotes the negation of a condition</definiens>
			</definition>
			<definition id="2">
				<sentence>Bayes formula yields the conditional probability of a random variable X having value x , given that another random variable Y has value y. In adapting Bayes conditional probability formula to document classification , this work followed the treatment of Mitchell ( 1997 ) pp.174−184 .</sentence>
				<definiendum id="0">Bayes formula</definiendum>
				<definiens id="0">yields the conditional probability of a random variable X having value x</definiens>
			</definition>
			<definition id="3">
				<sentence>A document is a series of tokens denoted K ( d ) .</sentence>
				<definiendum id="0">document</definiendum>
			</definition>
			<definition id="4">
				<sentence>G denotes a set of genres .</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">a set of genres</definiens>
			</definition>
			<definition id="5">
				<sentence>Recorded here are Recall , Precision and F1 where recall is the number of correct classifications divided by number of documents , precision is the number of correct classifications divided by the number of classifications made , and F1=2 * ( Precision * Recall ) / ( Precision + Recall ) .</sentence>
				<definiendum id="0">recall</definiendum>
				<definiendum id="1">precision</definiendum>
				<definiens id="0">the number of correct classifications divided by number of documents ,</definiens>
			</definition>
			<definition id="6">
				<sentence>However , Mitchell’s formulation generalizes to real−valued functions : The conditional probability that the genre of document d is gi , given that feature f is in d , v ( f , d ) , is a real value V , given by : V = v ( f , d ) [ Σd ε D ( gi ) v ( f , d ) ] / |D ( gi ) | ( eqn.5 ) where : D ( gi ) is the subset of the training corpus tagged as being of genre gi .</sentence>
				<definiendum id="0">conditional probability</definiendum>
				<definiens id="0">a real value V , given by : V = v</definiens>
				<definiens id="1">the subset of the training corpus tagged as being of genre gi</definiens>
			</definition>
</paper>

		<paper id="1010">
</paper>

		<paper id="0812">
			<definition id="0">
				<sentence>The remaining paths are then considered for the expression of the subject relation , selecting a highly probable VP Ge0 NP VP structure , with unary rules applying at the leaves ( VP Ge0 Verb , NP Ge0 Noun and AVP Ge0 Adverb ) , until no further expansion of nodes can be made with the 2 Probabilities may be different for other lexical items , e.g. with an unambiguously intransitive verb such as “fall” , the VP Ge0 AVP VP rule ( John quickly fell ) receives the highest ranking , based on the current training set .</sentence>
				<definiendum id="0">VP Ge0 AVP VP rule</definiendum>
			</definition>
</paper>

		<paper id="1608">
</paper>

		<paper id="1609">
			<definition id="0">
				<sentence>A dialogue manager inspects the current dialogue information state to determine how best to incorporate each new utterance into the dialogue ( Lemon et al. , 2001 ) .</sentence>
				<definiendum id="0">dialogue manager</definiendum>
			</definition>
			<definition id="1">
				<sentence>OAA is a framework for coordinating multiple asynchronous communicating processes .</sentence>
				<definiendum id="0">OAA</definiendum>
				<definiens id="0">a framework for coordinating multiple asynchronous communicating processes</definiens>
			</definition>
			<definition id="2">
				<sentence>Gemini uses a single uniﬁcation grammar both for parsing strings of words into logical forms ( LFs ) and for generating sentences from LF inputs .</sentence>
				<definiendum id="0">Gemini</definiendum>
			</definition>
			<definition id="3">
				<sentence>The ESS is a tree whose parent nodes represent damage events and whose leaves represent actions taken in response to those damage events .</sentence>
				<definiendum id="0">ESS</definiendum>
				<definiens id="0">a tree whose parent nodes represent damage events and whose leaves represent actions taken in response to those damage events</definiens>
			</definition>
</paper>

		<paper id="1314">
</paper>

		<paper id="1606">
			<definition id="0">
				<sentence>The VIP ( “Virtual Interactive Presenter” ) system is a dialogue-based interface to an Electronic Programme Guide ( EPG ) .</sentence>
				<definiendum id="0">VIP</definiendum>
			</definition>
			<definition id="1">
				<sentence>U13 : A funny movie S14 : I would suggest this programme with comedy : “Analyze This” U15 : Can I have another one ?</sentence>
				<definiendum id="0">U13</definiendum>
				<definiens id="0">A funny movie</definiens>
			</definition>
			<definition id="2">
				<sentence>User : IS THERE ANYTHING FUNNY I CAN WATCH Recognised : THE IS THERE ANYTHING FUNNY I CAN WATCH Semantics : ( ( ( QUESTION ) ( EXIST ) ( PROGRAMME ( ( CHOICE+ ) ) ) ( DET ) ) ( ( GOOD ) ( SUB_GENRE COMEDY ) ) ( ( AUDIENCE USER ) ) ( ( QUESTION ) ) ( ( VIEW ) ) ) Filter : ( ( SUB_GENRE COMEDY ) ) Speech Act : ( INITIAL ( SUB_GENRE COMEDY ) SEARCH ) System : I found 4 programmes corresponding to that selection .</sentence>
				<definiendum id="0">User</definiendum>
				<definiens id="0">THE IS THERE ANYTHING FUNNY I CAN WATCH Semantics : ( ( ( QUESTION ) ( EXIST ) ( PROGRAMME ( ( CHOICE+ ) ) ) ( DET ) ) ( ( GOOD ) ( SUB_GENRE COMEDY ) ) ( ( AUDIENCE USER ) ) ( ( QUESTION ) ) ( ( VIEW ) ) ) Filter : ( ( SUB_GENRE COMEDY ) ) Speech Act : ( INITIAL ( SUB_GENRE COMEDY ) SEARCH ) System : I found 4 programmes corresponding to that selection</definiens>
			</definition>
			<definition id="3">
				<sentence>User : NO I DON’T WANT A SITCOM Recognised : AN OLD I DO N'T WANT A SITCOM Semantics : ( ( CHOICE+ ) ( NEGATIVE ) ( AUDIENCE ( ( AUDIENCE USER ) ) ) ( PROGRAMME ( ( SUB_GENRE COMEDY ) ( GENRE ENTERTAINMENT ) ( INDET ) ) ) ) Filter : ( ( GENRE ENTERTAINMENT ) ( SUB_GENRE COMEDY ) ( NEGATIVE ) ) Speech Act : ( REJECT GENRE NIL NO_SEARCH ) System : What would you like to see , then ?</sentence>
				<definiendum id="0">User</definiendum>
				<definiendum id="1">SITCOM Recognised</definiendum>
				<definiens id="0">AN OLD I DO N'T WANT A SITCOM Semantics : ( ( CHOICE+ ) ( NEGATIVE ) ( AUDIENCE ( ( AUDIENCE USER ) ) ) ( PROGRAMME ( ( SUB_GENRE COMEDY ) ( GENRE ENTERTAINMENT ) ( INDET ) ) ) ) Filter : ( ( GENRE ENTERTAINMENT ) ( SUB_GENRE COMEDY ) ( NEGATIVE ) ) Speech Act : ( REJECT GENRE NIL NO_SEARCH</definiens>
			</definition>
			<definition id="4">
				<sentence>User : A COMEDY MOVIE Recognised : A COMEDY MOVIE Semantics : ( ( ( SUB_GENRE COMEDY ) ( INDET ) ) ( ( GENRE MOVIES ) ) ) Filter : ( ( GENRE MOVIES ) ( SUB_GENRE COMEDY ) ) Speech Act : ( SPECIFY SUB_GENRE COMEDY SEARCH ) System : My first selection is this programme with comedy : “Casablanca” User : YES THIS IS FINE Recognised : YES THE IS FIND THAT Semantics : ( ( ( GOOD ) ( POSITIVE ) ) ( ( DET ) ) ( ( ATTRIBUTE ) ) ) Speech Act : ( SIMPLE-ACCEPT ) System : Here is “Casablanca” , then .</sentence>
				<definiendum id="0">User</definiendum>
				<definiens id="0">A COMEDY MOVIE Recognised : A COMEDY MOVIE Semantics : ( ( ( SUB_GENRE COMEDY ) ( INDET ) ) ( ( GENRE MOVIES ) ) ) Filter : ( ( GENRE MOVIES ) ( SUB_GENRE COMEDY ) ) Speech Act : ( SPECIFY SUB_GENRE COMEDY SEARCH ) System</definiens>
				<definiens id="1">YES THIS IS FINE Recognised : YES THE IS FIND THAT Semantics : ( ( ( GOOD ) ( POSITIVE ) ) ( ( DET ) ) ( ( ATTRIBUTE ) ) ) Speech Act : ( SIMPLE-ACCEPT ) System</definiens>
			</definition>
</paper>

		<paper id="0518">
</paper>

		<paper id="1401">
			<definition id="0">
				<sentence>EBMT uses the corpus directly .</sentence>
				<definiendum id="0">EBMT</definiendum>
				<definiens id="0">uses the corpus directly</definiens>
			</definition>
			<definition id="1">
				<sentence>The translation process consists of four steps : 1 I. Retrieve the most similar translation pair ; II .</sentence>
				<definiendum id="0">translation process</definiendum>
			</definition>
			<definition id="2">
				<sentence>According to equation ( 1 ) , dist is calculated as follows : The counts of the Insertion ( I ) , Deletion ( D ) , and Substitution ( S ) operations are summed up and the total is normalized by the sum of the length of the source and example sequences .</sentence>
				<definiendum id="0">dist</definiendum>
				<definiendum id="1">Insertion</definiendum>
				<definiens id="0">the sum of the length of the source and example sequences</definiens>
			</definition>
			<definition id="3">
				<sentence>Substitution ( S ) considers the semantic distance between two substituted words and is called SEMDIST .</sentence>
				<definiendum id="0">Substitution ( S )</definiendum>
				<definiens id="0">considers the semantic distance between two substituted words and is called SEMDIST</definiens>
			</definition>
			<definition id="4">
				<sentence>SEMDIST is defined as the division of K ( the level of the least common abstraction in the thesaurus of two words ) by N ( the height of the thesaurus ) according to equation ( 2 ) ( Sumita and Iida , 1991 ) .</sentence>
				<definiendum id="0">SEMDIST</definiendum>
				<definiendum id="1">division of K</definiendum>
			</definition>
			<definition id="5">
				<sentence>A Matching Technique in Example-Based Machine Translation .</sentence>
				<definiendum id="0">Matching Technique</definiendum>
			</definition>
			<definition id="6">
				<sentence>A Comprehensive and Practical Model of Memory-Based Machine Translation .</sentence>
				<definiendum id="0">Comprehensive</definiendum>
			</definition>
			<definition id="7">
				<sentence>MBT2 : A Method for Combining Fragments of Examples in Example-Based Translation .</sentence>
				<definiendum id="0">MBT2</definiendum>
			</definition>
			<definition id="8">
				<sentence>Gaijin : A TemplateDriven Bootstrapping Approach to Example-Based Machine Translation , in the Proceedings of NeMNLP'97 , New Methods in Natural Language Processing , Sofia , Bulgaria .</sentence>
				<definiendum id="0">Gaijin</definiendum>
			</definition>
</paper>

		<paper id="0713">
			<definition id="0">
				<sentence>The internal random variable is the more familiar and ranges over the set of rules expanding that non-terminal .</sentence>
				<definiendum id="0">internal random variable</definiendum>
				<definiens id="0">the more familiar and ranges over the set of rules expanding that non-terminal</definiens>
			</definition>
			<definition id="1">
				<sentence>We can represent this as a9a11a27a28a14a30a29a31a12 where a27 is the rule , and a29 is the index saying where in the right hand side it occurs .</sentence>
				<definiendum id="0">a27</definiendum>
				<definiendum id="1">a29</definiendum>
			</definition>
			<definition id="2">
				<sentence>So for each a23 , a24a26a25 can take only those values of a9a11a27a32a14a30a29a33a12 where a23 is the a29 th symbol on the right hand side of a27 .</sentence>
				<definiendum id="0">a23</definiendum>
				<definiens id="0">the a29 th symbol on the right hand side of a27</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , “Washington D C” which is three tokens was tagged as NP0 ZZ0 ZZ0 where ZZ0 is a tag for alphabetic symbols .</sentence>
				<definiendum id="0">ZZ0</definiendum>
				<definiens id="0">a tag for alphabetic symbols</definiens>
			</definition>
			<definition id="4">
				<sentence>UR is unlabelled recall , UP is unlabelled precision , CB is average number of crossing brackets , a96a39a3 CB is percentage with two or fewer crossing brackets .</sentence>
				<definiendum id="0">CB</definiendum>
				<definiens id="0">average number of crossing brackets , a96a39a3 CB is percentage with two or fewer crossing brackets</definiens>
			</definition>
</paper>

		<paper id="1309">
			<definition id="0">
				<sentence>Note that the seven temporal relations employed by the current version are equivalent to sets of Allen’s interval relations ( Allen , 1983 ) .4 before a3a5a4a7a6a9a8a11a10 after a3a5a4a13a12a13a6a9a8a14a12a9a10 incl a3a7a15a16a6a13a17a18a6a13a19a20a6a13a21a23a22a24a10 at a3a7a15a25a12a26a6a13a17a7a12a26a6a13a19a20a12a26a6a13a21a7a22a18a10 starts a3a5a17a27a10 finishes a3a5a19a28a10 excl a3a5a4a7a6a13a4a29a12a26a6a9a8a30a6a9a8a14a12a26a10 Table 1 : the temporal relations used Similar to other approaches to information extraction or tagging , a cascade of Finite State Transducers ( FST ) was employed .</sentence>
				<definiendum id="0">FST</definiendum>
				<definiens id="0">temporal relations employed by the current version are equivalent to sets of Allen’s interval relations</definiens>
			</definition>
			<definition id="1">
				<sentence>4Allen ( 1983 ) proposes a temporal reasoning system that contains all 13 conceivable relations between intervals : b ( efore ) , m ( eets ) , o ( verlaps ) , s ( tarts ) , d ( uring ) , f ( inishes ) , the 6 reverse relations bi , mi , oi , si , di and fi and eq ( ual ) .</sentence>
				<definiendum id="0">eq</definiendum>
				<definiens id="0">proposes a temporal reasoning system that contains all 13 conceivable relations between intervals</definiens>
			</definition>
			<definition id="2">
				<sentence>After having processed the following noun referring to a time ( i.e. Monday ) , the following semantic representation is obtained via unification : sem = [ incl , [ E , t1 ] ] , where t1 refers to the following time stamp time = [ ’Mon’ , date ( , , ) , time ( , , ) , gl ( [ , ’1 day’ , ] ) ] .11 11Note that the underscore “ ” refers to an anonymous variable in PROLOG .</sentence>
				<definiendum id="0">t1</definiendum>
				<definiens id="0">date ( , , ) , time ( , , )</definiens>
			</definition>
</paper>

		<paper id="0722">
</paper>

		<paper id="1513">
			<definition id="0">
				<sentence>FILES has been used to annotate a number of corpora of Italian within the National Project currently still work in progress .</sentence>
				<definiendum id="0">FILES</definiendum>
				<definiens id="0">used to annotate a number of corpora of Italian within the National Project currently still work in progress</definiens>
			</definition>
			<definition id="1">
				<sentence>Low level representations are integrated in a relational database and shown in the FILES environment which is an intelligent browser allowing the annotation to operate changes and create XML output automatically for each file .</sentence>
				<definiendum id="0">Low level representations</definiendum>
				<definiens id="0">an intelligent browser allowing the annotation to operate changes</definiens>
			</definition>
			<definition id="2">
				<sentence>Fig.1 Relational databases to be used as input for the Syntactic and Functional Annotation An interesting part of the browser is the availability of subcategorization frames for verbs : these are expressed in a compact format which are intended to help the annotator in the most difficult task , i.e. that of deciding whether a given constituent head must be interpreted as either an argument or an adjunct ; and in case it is an argument , whether it should be interpreted as predicative or “open” in LFG terms , or else as non-predicative or “close” .</sentence>
				<definiendum id="0">Relational databases</definiendum>
				<definiens id="0">intended to help the annotator in the most difficult task</definiens>
			</definition>
			<definition id="3">
				<sentence>However , the aim of the RTN is that of producing a structured output both for wellformed and illformed grammatical sentences of German .</sentence>
				<definiendum id="0">RTN</definiendum>
				<definiens id="0">that of producing a structured output both for wellformed and illformed grammatical sentences of German</definiens>
			</definition>
			<definition id="4">
				<sentence>SYSTEM ARCHITECTURE I° Top-Down DGC-based Grammar Rules Lexical Look-Up Or Full Morphological Analysis Deterministic Policy : Look-ahead WFST Verb Guidance From Subcategorization Frames Semantic Consistency Check for every Syntactic Constituent Starting from CP level Phrase Structure Rules == &gt; F-structure check for Completeness Coherence , UniquenessTense , Aspect and Ambiguities dealt with by the parser go from different binding solution of a pronoun contained in a subordinate clause by two possible antecedents , chosen according to semantic and pragmatic strategies based on semantic roles and meaning associated to the subordinating conjunction , as in the following examples : i.The authorities refused permission to the demonstrators because they feared violence ii .</sentence>
				<definiendum id="0">SYSTEM ARCHITECTURE I°</definiendum>
				<definiens id="0">Top-Down DGC-based Grammar Rules Lexical Look-Up Or Full Morphological Analysis Deterministic Policy : Look-ahead WFST Verb Guidance From Subcategorization Frames Semantic Consistency Check for every Syntactic Constituent Starting from CP level Phrase Structure Rules == &gt; F-structure check for Completeness Coherence , UniquenessTense , Aspect and Ambiguities dealt with by the parser go from different binding solution of a pronoun contained in a subordinate clause by two possible antecedents</definiens>
			</definition>
</paper>

		<paper id="1315">
			<definition id="0">
				<sentence>Then the distance between A and B can be defined as : d ( A , B ) = ( |M A M AB | + |M B -M AB | ) / |M AB | In other words , the distance is the number of relation pairs that are not shared by the annotations normalized by the number that they R do share .</sentence>
				<definiendum id="0">distance</definiendum>
				<definiens id="0">d ( A , B ) = ( |M A M AB | + |M B -M AB | ) / |M AB | In other words</definiens>
				<definiens id="1">the number of relation pairs that are not shared by the annotations normalized by the number that they R do share</definiens>
			</definition>
</paper>

		<paper id="0905">
			<definition id="0">
				<sentence>Tagged Questions : Named entity tags Vocabulary &amp; frequencies Named entity recognition Candidate terms Retrieved documents Tagged sentences : named entity tags and term indexation Ordered sequences of 250 and 50 characters Question analysis Search engine Questions Subset of ranked documents Corpus Re-indexing and selection of documents ( FASTR ) Question/Sentence pairing Figure 1 .</sentence>
				<definiendum id="0">Tagged Questions</definiendum>
				<definiens id="0">Named entity tags Vocabulary &amp; frequencies Named entity recognition Candidate terms Retrieved documents Tagged sentences : named entity tags and term indexation Ordered sequences of 250 and 50 characters Question analysis Search engine Questions Subset of ranked documents Corpus Re-indexing and selection of documents ( FASTR ) Question/Sentence pairing Figure 1</definiens>
			</definition>
</paper>

		<paper id="0725">
</paper>

		<paper id="1611">
</paper>

		<paper id="1508">
			<definition id="0">
				<sentence>The relevant elements for the resources themselves are : Table 1 elements for media files Table 2 elements for annotation units C : constrained OV : open vocabulary CCV : closed constrained vocabulary For annotation units multiple units may reside in one file .</sentence>
				<definiendum id="0">OV</definiendum>
				<definiens id="0">open vocabulary CCV : closed constrained vocabulary For annotation units multiple units may reside in one file</definiens>
			</definition>
</paper>

		<paper id="0803">
			<definition id="0">
				<sentence>To achieve this task , we advocate that Segmented Discourse Representation Theory ( SDRT ) is a most expressive discourse framework .</sentence>
				<definiendum id="0">SDRT</definiendum>
			</definition>
			<definition id="1">
				<sentence>The SDRS underlying ( 2a ) is in ( 3a ) in which the discourse relation Result between a42 a3 and a42 a8 expresses the predicate causea15a17a1a10a3a43a5a7a1a27a8a27a19 .</sentence>
				<definiendum id="0">SDRS underlying</definiendum>
			</definition>
			<definition id="2">
				<sentence>Similarly , if a SDRS includes a discourse relation which can not be realized in the target language ( e.g. volitionalResult proposed in ( Mann and Thompson , 1987 ) can not be linguistically realized in French ( Danlos , 2001 ) ) , it leads to nothing4 .</sentence>
				<definiendum id="0">SDRS</definiendum>
				<definiens id="0">includes a discourse relation which can not be realized in the target language</definiens>
			</definition>
			<definition id="3">
				<sentence>In formal terms , it translates as follows : a SDRS a42 built from a logical form LF is such that the logical form derived from a42 is logically equivalent to LF .</sentence>
				<definiendum id="0">LF</definiendum>
				<definiens id="0">a SDRS a42 built from a logical form</definiens>
			</definition>
			<definition id="4">
				<sentence>Avaibility ( or openness ) of constituents is a formal property that can be accounted for by the use of discourse relations .</sentence>
				<definiendum id="0">Avaibility</definiendum>
			</definition>
			<definition id="5">
				<sentence>Formally , a DRS is a couple of sets a84 U , Cona85 .</sentence>
				<definiendum id="0">DRS</definiendum>
			</definition>
			<definition id="6">
				<sentence>U ( the universe ) is the set of discourse referents .</sentence>
				<definiendum id="0">U</definiendum>
			</definition>
			<definition id="7">
				<sentence>Con is a set of conditions which describe the meaning of the discourse in a truth-conditional semantics fashion .</sentence>
				<definiendum id="0">Con</definiendum>
			</definition>
			<definition id="8">
				<sentence>DRT adopts a Davidsonian approach ( Davidson , 1967 ) : it considers that events have to be denoted by singular terms in the logical form of sentences .</sentence>
				<definiendum id="0">DRT</definiendum>
				<definiens id="0">adopts a Davidsonian approach ( Davidson , 1967 ) : it considers that events have to be denoted by singular terms in the logical form of sentences</definiens>
			</definition>
			<definition id="9">
				<sentence>This is why some discourse abstract objects ( such as facts , situations , propositions ... ) can be referred to by discourse referents ( we will say that they are reified ) and semantically characterized by ( sub- ) DRS. ( 8 ) is an example of a fact reading , where a87 is the characterization predicate ( Asher , 1993 , p. 145 ) .</sentence>
				<definiendum id="0">a87</definiendum>
				<definiens id="0">some discourse abstract objects ( such as facts , situations , propositions ...</definiens>
			</definition>
			<definition id="10">
				<sentence>b. a56a88a68 a55a12a89 f a56a34a58 Fred fa90 a55 a55 –leave a59a56a53a60 abrupta59a55a60 a68a91a58 Mary a55a76a89–upset a59fa72a17a68a43a60 A SDRS is a couple of sets a84 U , Cona85 .</sentence>
				<definiendum id="0">SDRS</definiendum>
			</definition>
			<definition id="11">
				<sentence>U is a set of labels of DRS or SDRS which may be viewed as “speech act discourse referents” ( Asher and Lascarides , 1998 ) .</sentence>
				<definiendum id="0">U</definiendum>
			</definition>
			<definition id="12">
				<sentence>Con is a set of conditions on labels of the form : a78 a42a93a92a44a94 , where a42 is a label from U and a94 is a ( S ) DRS ( labelling ) ; a78 R a15 a42a69a95 a5 a42a51a96 a19 , where a42a24a95 and a42a44a96 are labels and R a discourse relation ( structuring ) .</sentence>
				<definiendum id="0">Con</definiendum>
				<definiendum id="1">a42</definiendum>
				<definiendum id="2">a94</definiendum>
				<definiens id="0">a set of conditions on labels of the form : a78 a42a93a92a44a94 , where</definiens>
				<definiens id="1">a label from U</definiens>
				<definiens id="2">a ( S ) DRS ( labelling ) ; a78 R a15 a42a69a95 a5 a42a51a96 a19 , where a42a24a95 and a42a44a96 are labels and R a discourse relation ( structuring )</definiens>
			</definition>
			<definition id="13">
				<sentence>The set of SDRT relations includes Narration ( for temporal sequence ) , Background ( for temporal overlap ) , Elaboration ( for whole-part or topicdevelopment ) , Explanation and Result ( for causation ) , Commentary ( for gloss ) .</sentence>
				<definiendum id="0">Narration</definiendum>
				<definiens id="0">temporal overlap ) , Elaboration ( for whole-part or topicdevelopment ) , Explanation and Result ( for causation ) , Commentary ( for gloss )</definiens>
			</definition>
			<definition id="14">
				<sentence>For example , in ( RAGS Project , 1999 ; BouayadAgha et al. , 2000 ) , the Rhetorical Structure is an unordered tree in which terminal nodes represent elementary propositions , while non terminal nodes represent rhetorical relations which are abstract relations such as cause .</sentence>
				<definiendum id="0">Rhetorical Structure</definiendum>
				<definiens id="0">an unordered tree in which terminal nodes represent elementary propositions , while non terminal nodes represent rhetorical relations</definiens>
			</definition>
			<definition id="15">
				<sentence>Commentary is such a discourse relation , and it is valid here since its constraint ( one element ina42 a8 has to be coreferent with one element in a42 a3 , see section 1 ) is respected with the coreference relation a1 a46 a29a184a1a4a3 .</sentence>
				<definiendum id="0">Commentary</definiendum>
				<definiens id="0">such a discourse relation</definiens>
			</definition>
</paper>

		<paper id="1504">
			<definition id="0">
				<sentence>TRACTOR is the TELRI Research Archive of Computational Tools and Resources .</sentence>
				<definiendum id="0">TRACTOR</definiendum>
			</definition>
			<definition id="1">
				<sentence>TRACTOR is a key element of TELRI II , a panEuropean alliance of focal national language technology institutions with the emphasis on Central and Eastern European and NIS countries .</sentence>
				<definiendum id="0">TRACTOR</definiendum>
			</definition>
			<definition id="2">
				<sentence>In the future , TRACTOR aims to build up particularly parallel corpora and tools for processing and extracting meaning from such resources .</sentence>
				<definiendum id="0">TRACTOR</definiendum>
				<definiens id="0">aims to build up particularly parallel corpora and tools for processing and extracting meaning from such resources</definiens>
			</definition>
			<definition id="3">
				<sentence>TRACTOR ( www.tractor.de ) is the TELRI Research Archive of Computational Tools and Resources .</sentence>
				<definiendum id="0">TRACTOR ( www.tractor.de )</definiendum>
			</definition>
			<definition id="4">
				<sentence>TRACTOR is a key element of TELRI II .</sentence>
				<definiendum id="0">TRACTOR</definiendum>
			</definition>
			<definition id="5">
				<sentence>The EC Concerted Action TELRI II is a panEuropean alliance of currently 28 focal national language technology institutions with the emphasis on Central and Eastern European and NIS countries .</sentence>
				<definiendum id="0">EC Concerted Action TELRI II</definiendum>
			</definition>
			<definition id="6">
				<sentence>Highly standardised resources built according to carefully crafted and strict criteria , and intended to be of commercial value such as EuroWordnet , the SIMPLE ontology , the ELAN network have more resources and attention paid to the design than the quality or usefulness of the knowledge which they contain .</sentence>
				<definiendum id="0">SIMPLE ontology</definiendum>
				<definiens id="0">the quality or usefulness of the knowledge which they contain</definiens>
			</definition>
			<definition id="7">
				<sentence>TRACTOR provides ongoing help and support for the User Community , a task which chiefly involves solving simple queries regarding the nature of the resources on offer , and how to download them , and referring more complex queries to experts in the User Community , or in the wider TELRI network .</sentence>
				<definiendum id="0">TRACTOR</definiendum>
				<definiens id="0">provides ongoing help and support for the User Community , a task which chiefly involves solving simple queries regarding the nature of the resources on offer , and how to download them , and referring more complex queries to experts in the User Community , or in the wider TELRI network</definiens>
			</definition>
			<definition id="8">
				<sentence>The TUC is the TRACTOR User Community .</sentence>
				<definiendum id="0">TUC</definiendum>
			</definition>
</paper>

		<paper id="1303">
			<definition id="0">
				<sentence>The closure locus of LOC is defined as : FER ( LOC ) = INT ( LOC ) ∪ FRO ( LOC ) with the following property : FER ( FER ( LOC ) ) ⊇ FER ( LOC ) ⊇ LOC Inside FRO ( LOC ) we distinguish two other loci , internal boundary and external boundary , such that : FRO-int ( LOC ) = LOC \ INT ( LOC ) FRO-ext ( LOC ) = FER ( LOC ) \ LOC The boundary locus is then defined as : FRO ( LOC ) = FRO-int ( LOC ) ∪ FRO-ext ( LOC ) We introduce the `` anchoring relation '' ( French : repérage ) between the `` anchored entity '' ( repéré ) and the `` anchor entity '' ( repère ) .</sentence>
				<definiendum id="0">LOC</definiendum>
				<definiens id="0">FER ( LOC ) = INT ( LOC ) ∪ FRO ( LOC ) with the following property : FER ( FER ( LOC ) ) ⊇ FER ( LOC ) ⊇ LOC Inside FRO</definiens>
			</definition>
			<definition id="1">
				<sentence>The cognitive representation ( see figure 2 ) shows the net meaning of preposition sur and preverb sur- , as well as the common invariant meaning of them .</sentence>
				<definiendum id="0">cognitive representation</definiendum>
				<definiens id="0">the net meaning of preposition sur and preverb sur- , as well as the common invariant meaning of them</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus , we consider three salient phases : boundary FRO ( LOC ) ( the beginning of the book ) , interior INT ( LOC ) and boundary FRO ( LOC ) ( the end of the book and the end of reading , in the sense achievement of this action ) .</sentence>
				<definiendum id="0">FRO</definiendum>
				<definiendum id="1">LOC</definiendum>
				<definiens id="0">the beginning of the book</definiens>
			</definition>
</paper>

		<paper id="0706">
			<definition id="0">
				<sentence>The phrases are : adjective phrase ( ADJP ) , adverb phrase ( ADVP ) , conjunction phrase ( CONJP ) , interjection phrase ( INTJ ) , list marker ( LST ) , noun phrase ( NP ) , preposition phrase ( PP ) , particle ( PRT ) , subordinated clause ( SBAR ) , unlike coordinated phrase ( UCP ) , verb phrase ( VP ) .</sentence>
				<definiendum id="0">adjective phrase</definiendum>
				<definiendum id="1">ADJP</definiendum>
				<definiendum id="2">adverb phrase</definiendum>
				<definiendum id="3">ADVP</definiendum>
				<definiendum id="4">conjunction phrase</definiendum>
				<definiendum id="5">CONJP</definiendum>
				<definiendum id="6">interjection phrase</definiendum>
				<definiendum id="7">INTJ</definiendum>
				<definiendum id="8">NP</definiendum>
				<definiendum id="9">preposition phrase</definiendum>
				<definiendum id="10">PP</definiendum>
				<definiendum id="11">VP</definiendum>
				<definiens id="0">particle ( PRT ) , subordinated clause ( SBAR ) , unlike coordinated phrase ( UCP ) , verb phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>An atomic phrase represents the most basic phrase with no nested sub-phrases .</sentence>
				<definiendum id="0">atomic phrase</definiendum>
			</definition>
			<definition id="2">
				<sentence>That is , an atomic phrase denotes a tightly coupled message unit which is just above the level of single words .</sentence>
				<definiendum id="0">atomic phrase</definiendum>
				<definiens id="0">a tightly coupled message unit which is just above the level of single words</definiens>
			</definition>
			<definition id="3">
				<sentence>SNoW ( Carleson et al. , 1999 ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .</sentence>
				<definiendum id="0">SNoW</definiendum>
				<definiens id="0">a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example</definiens>
			</definition>
			<definition id="4">
				<sentence>Our robustness test ( section 3.2 ) makes use of section 4 in the Switchboard ( SWB ) data ( which contains about 57,000 tokens and 17,000 phrases ) , taken from Treebank 3 .</sentence>
				<definiendum id="0">robustness test</definiendum>
			</definition>
			<definition id="5">
				<sentence>Full Parser Shallow Parser a34a37a35 a34a36a35 Avr .89 .93 NP .95 .96 VP .86 .92 ( Bbeginning of phrase ; Iinside the phrase ; Ooutside the phrase ) — with the true annotation and Collins’ annotation .</sentence>
				<definiendum id="0">Iinside</definiendum>
				<definiens id="0">the phrase ; Ooutside the phrase ) — with the true annotation and Collins’ annotation</definiens>
			</definition>
</paper>

		<paper id="1308">
			<definition id="0">
				<sentence>The domain of TENSE is less relevant to the verb as it expresses how the EVENT TIME is related to another TIME , either to SPEECH or REFERENCE TIME .</sentence>
				<definiendum id="0">EVENT TIME</definiendum>
				<definiens id="0">less relevant to the verb as it expresses how the</definiens>
			</definition>
			<definition id="1">
				<sentence>DEONTIC MODALITY represents the VOLITION which speakers impose on the situation expressed by the proposition .</sentence>
				<definiendum id="0">DEONTIC MODALITY</definiendum>
				<definiens id="0">represents the VOLITION which speakers impose on the situation expressed by the proposition</definiens>
			</definition>
			<definition id="2">
				<sentence>Metonymy is a semantic extension within the same domain of discourse : ( 8 ) We had a glass or two .</sentence>
				<definiendum id="0">Metonymy</definiendum>
				<definiens id="0">a semantic extension within the same domain of discourse : ( 8 ) We had a glass or two</definiens>
			</definition>
			<definition id="3">
				<sentence>By inheriting this boundary , the while functions as the CONTAINER ( CONTAIN ) of the CONTENT which is expressed by the verb phrase hoping that ... . The PROGRESSIVE ASPECT ( PROGRESS ) of the form hoping is thereby bounded to what fits into the CONTAINER .</sentence>
				<definiendum id="0">PROGRESSIVE ASPECT</definiendum>
				<definiens id="0">the CONTAINER ( CONTAIN ) of the CONTENT which is expressed by the verb phrase hoping that ...</definiens>
			</definition>
			<definition id="4">
				<sentence>The ADVERSATIVE sense of WHILE is a lexicalization from the speakers’ conversational implicature of an ANTONYMY ( Grice , 1975 ) .</sentence>
				<definiendum id="0">ADVERSATIVE sense of WHILE</definiendum>
			</definition>
			<definition id="5">
				<sentence>The discourse coherence relation which the verbs refurbish and move adopt in the respective discourse domain induce the DEONTIC OBLIGATION ( OBLIG ) sense on will and the CONDITIONAL mood on the adverbial clause .</sentence>
				<definiendum id="0">discourse coherence relation</definiendum>
				<definiens id="0">the verbs refurbish and move adopt in the respective discourse domain induce the DEONTIC OBLIGATION ( OBLIG ) sense on will and the CONDITIONAL mood on the adverbial clause</definiens>
			</definition>
			<definition id="6">
				<sentence>Once the minor grammatical category is derived , the grammaticalization cline continues metaphorically by a shift from reference to the text world to reference to the internal cognitive situation of the speakers , i.e. from objective to subjective reasoning , from the speakers’ measurement of TEMPORAL periods to their measurement of EVALUATIVE and ATTITUDINAL values ( Zelinsky-Wibbelt , 2001 ) .</sentence>
				<definiendum id="0">ATTITUDINAL values</definiendum>
				<definiens id="0">objective to subjective reasoning</definiens>
			</definition>
</paper>

		<paper id="1605">
			<definition id="0">
				<sentence>Regardless of their theoretical stance , all agree that the elementary discourse units are non-overlapping spans of text .</sentence>
				<definiendum id="0">theoretical stance</definiendum>
				<definiens id="0">all agree that the elementary discourse units are non-overlapping spans of text</definiens>
			</definition>
			<definition id="1">
				<sentence>The 78 relations used in annotating the corpus can be partitioned into 16 classes that share some type of rhetorical meaning : Attribution , Background , Cause , Comparison , Condition , Contrast , Elaboration , Enablement , Evaluation , Explanation , Joint , Manner-Means , Topic-Comment , Summary , Temporal , TopicChange .</sentence>
				<definiendum id="0">Summary</definiendum>
				<definiens id="0">share some type of rhetorical meaning : Attribution , Background , Cause , Comparison , Condition , Contrast , Elaboration , Enablement , Evaluation , Explanation , Joint , Manner-Means , Topic-Comment ,</definiens>
			</definition>
			<definition id="2">
				<sentence>The kappa coefficient ( Siegel and Castellan , 1988 ) has been used extensively in previous empirical studies of discourse ( Carletta et al. , 1997 ; Flammia and Zue , 1995 ; Passonneau and Litman , 1997 ) .</sentence>
				<definiendum id="0">kappa coefficient</definiendum>
				<definiens id="0">used extensively in previous empirical studies of discourse ( Carletta et al. , 1997</definiens>
			</definition>
			<definition id="3">
				<sentence>EDUs A , B , E ( Apr 00 ) A , B , E ( Jun 00 ) A , E ( Nov 00 ) B , E ( Nov 00 ) A , B ( Nov 00 ) A , B , E ( Jan 01 ) relations when they are grouped in this manner , with November results ranging from 0.78 to improvement had to do with pre-segmenting , we asked the same three annotators to annotate five previously unseen documents in January , without reference to a pre-segmented document .</sentence>
				<definiendum id="0">EDUs A , B , E</definiendum>
				<definiendum id="1">B</definiendum>
			</definition>
			<definition id="4">
				<sentence>The RST Corpus consists of 385 Wall Street Journal articles from the Penn Treebank , representing over 176,000 words of text .</sentence>
				<definiendum id="0">RST Corpus</definiendum>
			</definition>
			<definition id="5">
				<sentence>Thus , the RST Corpus provides an additional level of linguistic annotation to supplement existing annotated resources .</sentence>
				<definiendum id="0">RST Corpus</definiendum>
			</definition>
			<definition id="6">
				<sentence>In Corpus Annotation : Linguistic Information from Computer Text Corpora , edited by R. Garside , G. Leech , and T. McEnery .</sentence>
				<definiendum id="0">Corpus Annotation</definiendum>
				<definiens id="0">Linguistic Information from Computer Text Corpora</definiens>
			</definition>
			<definition id="7">
				<sentence>Learning discourse relations with active data selection .</sentence>
				<definiendum id="0">Learning discourse</definiendum>
			</definition>
</paper>

		<paper id="0903">
			<definition id="0">
				<sentence>Qualitative evaluation consists in estimating or judging some parameter by reference to expert standards and rules .</sentence>
				<definiendum id="0">Qualitative evaluation</definiendum>
				<definiens id="0">consists in estimating or judging some parameter by reference to expert standards and rules</definiens>
			</definition>
</paper>

		<paper id="0514">
			<definition id="0">
				<sentence>Two-level morphology : a general computational model for word-form recognition and production .</sentence>
				<definiendum id="0">Two-level morphology</definiendum>
				<definiens id="0">a general computational model for word-form recognition and production</definiens>
			</definition>
</paper>

		<paper id="0719">
			<definition id="0">
				<sentence>Case 1 CNP : scientific/JJ and/CC technical/JJ articles/NNS SNP1 : scientific/JJ articles/NNS SNP2 : technical/JJ articles/NNS Case 2 CNP : scientific/JJ thesauri/NNS and databases/NNS SNP1 : scientific/JJ thesauri/NNS SNP2 : scientific/JJ databases/NNS Case 3 CNP : physics/NN and/CC biology/NN skilled/JJ researchers/NNS SNP1 : physics/NN skilled/JJ researchers/NNS SNP2 : biology/NN skilled/JJ researchers/NNS Table 1 : Resolving Coordination of NPs Of the major syntactic constituents of a sentence , e.g. noun phrases , verb phrases , and prepositional phrases , we assume that noun phrases ( NPs ) carry the most contentful information about the document , a well-supported hypothesis ( Smeaton , 1999 ; Wacholder , 1998 ) .</sentence>
				<definiendum id="0">NPs</definiendum>
				<definiens id="0">Resolving Coordination of NPs Of the major syntactic constituents of a sentence , e.g. noun phrases , verb phrases</definiens>
			</definition>
			<definition id="1">
				<sentence>The base NP is either a simple NP or a coordination of simple NPs .</sentence>
				<definiendum id="0">base NP</definiendum>
				<definiens id="0">either a simple NP or a coordination of simple NPs</definiens>
			</definition>
			<definition id="2">
				<sentence>We choose two features to characterize the head of the noun phrases : AF head tfidf : the TF*IDF measure of the head of the candidate NP .</sentence>
				<definiendum id="0">AF head tfidf</definiendum>
			</definition>
			<definition id="3">
				<sentence>AF head focc : The position of the first occurrence of the head in text ( the number of words that precede the first occurrence of the head divided by the total number of words in the document ) .</sentence>
				<definiendum id="0">AF head focc</definiendum>
				<definiens id="0">the number of words that precede the first occurrence of the head divided by the total number of words in the document )</definiens>
			</definition>
			<definition id="4">
				<sentence>AF np length words : Noun phrase length measured in number of words , normalized by dividing it with the total number of words in the candidate NP list .</sentence>
				<definiendum id="0">AF np length words</definiendum>
				<definiens id="0">Noun phrase length measured in number of words</definiens>
			</definition>
			<definition id="5">
				<sentence>AF np length chars : Noun phrase length measured in number of characters , normalized by dividing it with the total number of characters in the candidate NPs list .</sentence>
				<definiendum id="0">AF np length chars</definiendum>
				<definiens id="0">Noun phrase length measured in number of characters</definiens>
			</definition>
			<definition id="6">
				<sentence>AF sent pos : Position of the noun phrase in the sentence : the number of words that precede the noun phrase , divided by sentence length .</sentence>
				<definiendum id="0">AF sent pos</definiendum>
			</definition>
			<definition id="7">
				<sentence>AF mh tfidf : the new TF*IDF measure that takes also into consideration the importance of the modifiers .</sentence>
				<definiendum id="0">AF mh tfidf</definiendum>
			</definition>
			<definition id="8">
				<sentence>AF DB CXBD : presence in the subject line and headline AF DB CXBE : presence in the subject line AF DB CXBF : presence in headlines where DB CXBD BQDB CXBE BQDB CXBF .</sentence>
				<definiendum id="0">AF DB CXBD</definiendum>
				<definiens id="0">presence in the subject line and headline AF DB CXBE : presence in the subject line AF DB CXBF : presence in headlines where DB CXBD BQDB CXBE BQDB CXBF</definiens>
			</definition>
			<definition id="9">
				<sentence>If the attributes are numeric , the test has the form DC CX BQD8 , whereDC CX is one of the attribute of an instance and t is the threshold .</sentence>
				<definiendum id="0">whereDC CX</definiendum>
				<definiens id="0">one of the attribute of an instance</definiens>
			</definition>
			<definition id="10">
				<sentence>In the context of gisting , the headmodifier relationship is an ordered relation between semantically equal elements .</sentence>
				<definiendum id="0">headmodifier relationship</definiendum>
				<definiens id="0">an ordered relation between semantically equal elements</definiens>
			</definition>
			<definition id="11">
				<sentence>A document summary is seen as a succinct and coherent prose that captures the meaning of the text .</sentence>
				<definiendum id="0">document summary</definiendum>
				<definiens id="0">a succinct and coherent prose that captures the meaning of the text</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>The presentation of the message gist is a complex user interface issue with its independent set of problems .</sentence>
				<definiendum id="0">presentation of the message gist</definiendum>
				<definiens id="0">a complex user interface issue with its independent set of problems</definiens>
			</definition>
			<definition id="1">
				<sentence>The base NP is either a simple NP as defined by Wacholder ( 1998 ) or a conjunction of two simple NPs .</sentence>
				<definiendum id="0">base NP</definiendum>
				<definiens id="0">either a simple NP as defined by Wacholder ( 1998 ) or a conjunction of two simple NPs</definiens>
			</definition>
			<definition id="2">
				<sentence>RIPPER allows the user to specify the loss ratio , which indicates the ratio of the cost of a false positive to the cost of a false negative , thus allowing the trade off between precision and recall .</sentence>
				<definiendum id="0">RIPPER</definiendum>
				<definiens id="0">indicates the ratio of the cost of a false positive to the cost of a false negative</definiens>
			</definition>
			<definition id="3">
				<sentence>Presenting the gist of an email message by phrase extraction addresses one obvious question : can any phrasal extract represent the content of a document , or must a well defined linguistic phrasal structure be used ?</sentence>
				<definiendum id="0">Presenting the gist of an email message</definiendum>
				<definiens id="0">any phrasal extract represent the content of a document , or must a</definiens>
			</definition>
</paper>

		<paper id="1615">
</paper>

		<paper id="1203">
			<definition id="0">
				<sentence>The treebanking process includes a “sanity check” after the treebanking proper of a sentence .</sentence>
				<definiendum id="0">treebanking process</definiendum>
				<definiens id="0">includes a “sanity check” after the treebanking proper of a sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>[ SNT , PRES , Qtarget : MONETARY-QUANTITY ] ( QUANT ) [ 2 ] How much [ INTERR-ADV ] ( MOD ) [ 3 ] How [ INTERR-ADV ] ( PRED ) [ 4 ] much [ ADV ] ( SUBJ LOG-SUBJ ) [ 5 ] one ton of cement [ NP ] ( QUANT ) [ 6 ] one ton [ NP , MASS-Q ] ( PRED ) [ 7 ] one ton [ NP-N , MASS-Q ] ( QUANT ) [ 8 ] one [ CARDINAL ] ( PRED ) [ 9 ] ton [ COUNT-NOUN ] ( PRED ) [ 10 ] of cement [ PP ] ( P ) [ 11 ] of [ PREP ] ( PRED ) [ 12 ] cement [ NP ] ( PRED ) [ 13 ] cement [ NOUN ] ( PRED ) [ 14 ] does cost [ VERB , PRES ] ( AUX ) [ 15 ] does [ AUX ] ( PRED ) [ 16 ] cost [ VERB ] ( DUMMY ) [ 17 ] ?</sentence>
				<definiendum id="0">MOD</definiendum>
				<definiendum id="1">QUANT</definiendum>
			</definition>
			<definition id="2">
				<sentence>Q : Name a film in which Jude Law acted .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiens id="0">Name a film in which Jude Law acted</definiens>
			</definition>
			<definition id="3">
				<sentence>The ontology allows us to conclude that the “dog” in an answer sentence candidate matches the Qtarget animal ( while “pizza” doesn’t ) .</sentence>
				<definiendum id="0">ontology</definiendum>
				<definiendum id="1">Qtarget animal</definiendum>
				<definiens id="0">allows us to conclude that the “dog” in an answer sentence candidate matches the</definiens>
			</definition>
</paper>

		<paper id="0501">
</paper>

		<paper id="1304">
			<definition id="0">
				<sentence>That is to say that a typical semantic contribution of a simple past is assessed on a twodimensional frame centred on the time of utterance , as is the case in sentence ( 1 ) , where the past tense is understood as encoding a precedence relation between the event described by the sentence and the time of utterance n. ( 1 ) Alice took a walk by the river .</sentence>
				<definiendum id="0">utterance</definiendum>
				<definiens id="0">the case in sentence ( 1 ) , where the past tense is understood as encoding a precedence relation between the event described by the sentence and the time of utterance n. ( 1 ) Alice took a walk by the river</definiens>
			</definition>
			<definition id="1">
				<sentence>Origin Orientation Absolute G given Intrinsic G G Relative G VPT Table 1 Table 1 illustrates that while the orientation parameter of an absolute frame is given as part of the lexical / encyclopaedic knowledge that the speaker has of a preposition like east of , the orientation parameter of an intrinsic frame is defined by the properties of the ground object ( G in the table ) .</sentence>
				<definiendum id="0">Origin Orientation Absolute G</definiendum>
				<definiens id="0">given Intrinsic G G Relative G VPT Table 1 Table 1 illustrates that while the orientation parameter of an absolute frame is given as part of the lexical / encyclopaedic knowledge that the speaker has of a preposition like east of , the orientation parameter of an intrinsic frame is defined by the properties of the ground object ( G in the table )</definiens>
			</definition>
			<definition id="2">
				<sentence>Derrière Triggering configuration : α PP P β derrière Choose : Orientation point , Opt , from the following items : ( i ) ST ( β ) ( ii ) ST ( salient VPT ) &amp; polarity is switched to negative where ST ( ) is an operator which returns the spatio-temporal ‘slice’ denoted by its argument .</sentence>
				<definiendum id="0">ST ( )</definiendum>
			</definition>
</paper>

		<paper id="1402">
			<definition id="0">
				<sentence>We describe MSR-MT , a large-scale hybrid machine translation system under development for several language pairs .</sentence>
				<definiendum id="0">MSR-MT</definiendum>
				<definiens id="0">a large-scale hybrid machine translation system under development for several language pairs</definiens>
			</definition>
			<definition id="1">
				<sentence>Other variations of EBMT systems are hybrids that integrate an EBMT component as one of multiple sources of transfer knowledge ( in addition to other transfer rule or knowledge based components ) used during translation ( Frederking et al. 1994 ; Takeda et al. 1992 ) .</sentence>
				<definiendum id="0">EBMT component</definiendum>
				<definiens id="0">one of multiple sources of transfer knowledge ( in addition to other transfer rule or knowledge based components ) used during translation ( Frederking et al. 1994</definiens>
			</definition>
			<definition id="2">
				<sentence>MSR-MT is a data-driven hybrid MT system , combining rule-based analysis and generation components with example-based transfer .</sentence>
				<definiendum id="0">MSR-MT</definiendum>
				<definiens id="0">a data-driven hybrid MT system , combining rule-based analysis and generation components with example-based transfer</definiens>
			</definition>
			<definition id="3">
				<sentence>Generation receives the target LF as input , from which it produces a target sentence ( section 5.4 ) .</sentence>
				<definiendum id="0">Generation</definiendum>
			</definition>
			<definition id="4">
				<sentence>MSR-MT’s broad-coverage parsers produce conventional phrase structure analyses augmented with grammatical relations ( Heidorn et al. 2000 ) .</sentence>
				<definiendum id="0">MSR-MT’s broad-coverage</definiendum>
			</definition>
			<definition id="5">
				<sentence>LFs normalize certain syntactic alternations ( e.g. active/passive ) and resolve both intrasentential anaphora and longdistance dependencies .</sentence>
				<definiendum id="0">LFs</definiendum>
				<definiens id="0">normalize certain syntactic alternations ( e.g. active/passive ) and resolve both intrasentential anaphora and longdistance dependencies</definiens>
			</definition>
			<definition id="6">
				<sentence>English/Spanish transfer mappings from LF alignment The repository into which transfer mappings from LF alignment are stored is known as MindNet .</sentence>
				<definiendum id="0">English/Spanish transfer</definiendum>
				<definiens id="0">mappings from LF alignment The repository into which transfer mappings from LF alignment are stored is known as MindNet</definiens>
			</definition>
			<definition id="7">
				<sentence>For MSR-MT , MindNet serves as the optimal example base , specifically designed to store and retrieve the linked source and target LF segments comprising the transfer mappings extracted during LF alignment .</sentence>
				<definiendum id="0">MindNet</definiendum>
				<definiens id="0">the optimal example base , specifically designed to store and retrieve the linked source and target LF segments comprising the transfer mappings extracted during LF alignment</definiens>
			</definition>
			<definition id="8">
				<sentence>The parser produces an LF for the sentence , as described in section 3 .</sentence>
				<definiendum id="0">parser</definiendum>
			</definition>
			<definition id="9">
				<sentence>MindMeld attempts to find the best set of matching transfer mappings by first searching for LF segments in MindNet that have matching lemmas , parts of speech , and other feature information .</sentence>
				<definiendum id="0">MindMeld</definiendum>
				<definiens id="0">attempts to find the best set of matching transfer mappings by first searching for LF segments in MindNet that have matching lemmas , parts of speech</definiens>
			</definition>
			<definition id="10">
				<sentence>Among mappings of equal size , MindMeld prefers higher-frequency mappings .</sentence>
				<definiendum id="0">MindMeld</definiendum>
				<definiens id="0">prefers higher-frequency mappings</definiens>
			</definition>
			<definition id="11">
				<sentence>After an optimal set of matching transfer mappings is found , MindMeld creates Links on nodes in the source LF to copies of the corresponding target LF segments retrieved from the mappings .</sentence>
				<definiendum id="0">MindMeld</definiendum>
			</definition>
</paper>

		<paper id="0511">
			<definition id="0">
				<sentence>The Uniﬁed Medical Language System ( UMLS ) is a biomedical lexical resource produced and maintained by the National Library of Medicine ( Humphreys et al. , 1998 ) .</sentence>
				<definiendum id="0">Uniﬁed Medical Language System</definiendum>
			</definition>
			<definition id="1">
				<sentence>WhenawordmapstoageneralMeSH term ( liketreatment , Y11 ) zerosareappendedtothe end of the descriptor to stand in place of the missing values ( so , for example , treatment in Model 3 is Y 11 0 , and in Model 4 is Y 11 0 0 , etc. ) .</sentence>
				<definiendum id="0">WhenawordmapstoageneralMeSH term</definiendum>
			</definition>
</paper>

		<paper id="1000">
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>The goals of an Ontology is to reduce ( or eliminate ) conceptual and terminological confusion .</sentence>
				<definiendum id="0">Ontology</definiendum>
				<definiens id="0">to reduce ( or eliminate ) conceptual and terminological confusion</definiens>
			</definition>
			<definition id="1">
				<sentence>With respect to a Knowledge Base ( KB ) : An Ontology can be seen as a KB whose goal is the description of the concepts necessary for talking about domains ; A KB , in addition , includes the knowledge needed to model and elaborate a problem , derive new knowledge , prove theorems , or answer to intentional queries about a domain .</sentence>
				<definiendum id="0">Knowledge Base ( KB )</definiendum>
				<definiendum id="1">KB</definiendum>
				<definiens id="0">the description of the concepts necessary for talking about domains</definiens>
				<definiens id="1">includes the knowledge needed to model and elaborate a problem , derive new knowledge , prove theorems , or answer to intentional queries about a domain</definiens>
			</definition>
			<definition id="2">
				<sentence>OPAL is a methodology for the modeling and management of the Enterprise Knowledge Base and , in particular , it allows the representation of the semi-formal knowledge of an enterprise .</sentence>
				<definiendum id="0">OPAL</definiendum>
				<definiens id="0">a methodology for the modeling and management of the Enterprise Knowledge Base and</definiens>
			</definition>
			<definition id="3">
				<sentence>The first phase of the Ontology building process consists in the identification of the key concepts of the application domain .</sentence>
				<definiendum id="0">Ontology building process</definiendum>
				<definiens id="0">consists in the identification of the key concepts of the application domain</definiens>
			</definition>
			<definition id="4">
				<sentence>Terminology is the set of words or word strings , which convey a single , possibly complex , meaning within a given community .</sentence>
				<definiendum id="0">Terminology</definiendum>
				<definiens id="0">the set of words or word strings</definiens>
			</definition>
			<definition id="5">
				<sentence>idfi = log2 N df i Where dfi is the number of documents in a domain Di that include a term t , and N is the total number of documents in a collection of n domains ( D1 , … , Dn ) .</sentence>
				<definiendum id="0">dfi</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">include a term t</definiens>
			</definition>
			<definition id="6">
				<sentence>Domain consensus measures the distributed use of a term in a domain Di .</sentence>
				<definiendum id="0">Domain consensus</definiendum>
				<definiens id="0">measures the distributed use of a term in a domain Di</definiens>
			</definition>
			<definition id="7">
				<sentence>As a matter of facts , an Ontology is a basic component itself , therefore it can be formally evaluated only in the context of some specific usage of the Ontology itself .</sentence>
				<definiendum id="0">Ontology</definiendum>
				<definiens id="0">a basic component itself</definiens>
			</definition>
			<definition id="8">
				<sentence>To compute the Domain Relevance , we first collected corpora in several domains : tourism announcements and hotel descriptions , economic prose ( Wall Street Journal ) , medical news ( Reuters ) , sport news ( Reuters ) , a balanced corpus ( Brown Corpus ) and four novels by Wells .</sentence>
				<definiendum id="0">Reuters )</definiendum>
				<definiendum id="1">balanced corpus ( Brown Corpus</definiendum>
				<definiens id="0">tourism announcements and hotel descriptions , economic prose ( Wall Street Journal ) , medical news</definiens>
			</definition>
			<definition id="9">
				<sentence>The WP includes 1270 terms , but only 214 occur at least once in the WSJ .</sentence>
				<definiendum id="0">WP</definiendum>
				<definiens id="0">includes 1270 terms , but only 214 occur at least once in the WSJ</definiens>
			</definition>
			<definition id="10">
				<sentence>Similarity is one of the fields ( see Figure 1 ) in a concept definition form that are currently filled by humans .</sentence>
				<definiendum id="0">Similarity</definiendum>
				<definiens id="0">one of the fields ( see Figure 1 ) in a concept definition form that are currently filled by humans</definiens>
			</definition>
</paper>

		<paper id="1204">
</paper>

		<paper id="1616">
</paper>

		<paper id="0904">
			<definition id="0">
				<sentence>ate the apple np ( s\np ) /np np/n n np s\np s FA FA BA John Figure 1 : An Example Parse in Basic CG The CG formalism described above has been shown to be weakly equivalent to context-free phrase structure grammars ( Bar-Hillel et al. , 1964 ) .</sentence>
				<definiendum id="0">apple np</definiendum>
			</definition>
</paper>

		<paper id="0901">
</paper>

		<paper id="0705">
</paper>

		<paper id="1414">
			<definition id="0">
				<sentence>The transfer component consists of high-quality transfer patterns automatically acquired from sentence-aligned bilingual corpora using an alignment grammar and algorithm described in detail in Menezes ( 2001 ) ( see Figure 1 for an overview of the French-English MT system ) .</sentence>
				<definiendum id="0">transfer component</definiendum>
			</definition>
			<definition id="1">
				<sentence>The transfer component consists only of correspondences learned during the alignment process .</sentence>
				<definiendum id="0">transfer component</definiendum>
			</definition>
			<definition id="2">
				<sentence>A French linguist determined the best cutoff for the raw data , i.e. determined the association score which would determine the cutoff , and otherwise left the file unedited for inclusion in the transfer training stage .</sentence>
				<definiendum id="0">French linguist</definiendum>
			</definition>
</paper>

		<paper id="1313">
			<definition id="0">
				<sentence>The above procedure classifies a clause containing more than one verb as a simple clause .</sentence>
				<definiendum id="0">above procedure</definiendum>
				<definiens id="0">classifies a clause containing more than one verb as a simple clause</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>es ( si ; d ; sj ; d0 ) = P tk2si ; d\sj ; d0 idf k P tk2sj ; d0 idf k ( 1 ) The weight of a term ti is its inverse document frequency ( idf i ) , as defined in ( 2 ) , where N is the number of all segments in the set of related documents ( the topic ) and ni is the number of segments in which the term ti occurs .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">ni</definiendum>
				<definiens id="0">the number of all segments in the set of related documents</definiens>
			</definition>
			<definition id="1">
				<sentence>The entailment score es ( si ; d ; sj ; d0 ) measures how many of the words of the segment si ; d occur in sj ; d0 , and how important those words are .</sentence>
				<definiendum id="0">entailment score es</definiendum>
				<definiens id="0">measures how many of the words of the segment si</definiens>
			</definition>
			<definition id="2">
				<sentence>sim ( si ; d ; sj ; d0 ) = P tk2si ; d\sj ; d0 wk ; si ; d wk ; sj ; d0 r P tk2si ; d w2si ; d P tk2sj ; d0 w2sj ; d0 ( 3 ) Where wk ; si ; d is the weight associated with the term tk in segment si ; d. In the nominator of ( 3 ) , k ; d0 : es ( sk ; d0 ; si ; dF ) &gt; es ( si ; dF ; sk ; d0 ) if es ( sj ; d0 ; si ; dF ) &gt; es then replace si ; dF by sj ; d0 in the fused document if for all si ; dF : sim ( si ; dF ; sj ; d0 ) &lt; sim , then find the most similar si ; dF : si ; dF = arg maxs k ; dF : sim ( sj ; d0 ; sk ; dF ) and place sj ; d0 between si ; dF and si+1 ; dF Figure 1 : Sketch of the document fusion algorithm .</sentence>
				<definiendum id="0">d</definiendum>
				<definiens id="0">the weight associated with the term tk in segment si</definiens>
			</definition>
			<definition id="3">
				<sentence>Then , precision is the fraction of computed subsumption pairs that is correct : Precision = number of correct subsumption pairs computedtotal number of subsumption pairs computed : And recall is the proportion of the total number of correct subsumption pairs that were computed : Recall = number of correct subsumption pairs computedtotal number of correct subsumption pairs : Observe that precision and recall depend on the subsumption threshold that we use .</sentence>
				<definiendum id="0">precision</definiendum>
				<definiendum id="1">recall</definiendum>
				<definiens id="0">the fraction of computed subsumption pairs</definiens>
				<definiens id="1">the proportion of the total number of correct subsumption pairs that were computed : Recall = number of correct subsumption pairs computedtotal number of correct subsumption pairs : Observe that precision and recall depend on the subsumption threshold that we use</definiens>
			</definition>
			<definition id="4">
				<sentence>The test collection consists of 69 news stories categorized into 21 topics .</sentence>
				<definiendum id="0">test collection</definiendum>
			</definition>
			<definition id="5">
				<sentence>nseg is the number of segments in the fused document and nseg ; d is the number of segments stemming from document d. For our test collection , the average fusion impact factor was 0.56 .</sentence>
				<definiendum id="0">nseg</definiendum>
				<definiendum id="1">d</definiendum>
				<definiens id="0">the number of segments in the fused document and nseg ;</definiens>
				<definiens id="1">the number of segments stemming from document d. For our test collection</definiens>
			</definition>
</paper>

		<paper id="0801">
</paper>

		<paper id="1628">
			<definition id="0">
				<sentence>Interruptions are an important element in the interactive character of discourse .</sentence>
				<definiendum id="0">Interruptions</definiendum>
				<definiens id="0">an important element in the interactive character of discourse</definiens>
			</definition>
</paper>

		<paper id="0815">
			<definition id="0">
				<sentence>In a single pass , AGILE is capable of generating several types of text , each Anthony Hartley and Donia Scott Information Technology Research Institute , University of Brighton UK { firstname.lastname } @ itri.bton.ac.uk constituting a section of a typical software user manual—i.e. , overview , short instructions , full instructions , and functional descriptions—and appearing in one of two styles ( personal/direct or impersonal/indirect ) .</sentence>
				<definiendum id="0">AGILE</definiendum>
				<definiens id="0">capable of generating several types of text , each Anthony Hartley and Donia Scott Information Technology Research Institute</definiens>
			</definition>
</paper>

		<paper id="1603">
			<definition id="0">
				<sentence>Chaining is a pattern of [ A : I B : R ] [ A : I B : R ] ( speaker A initiates the exchange and speaker B responds it ) .</sentence>
				<definiendum id="0">Chaining</definiendum>
				<definiens id="0">a pattern of [ A : I B : R ] [ A : I B : R ] ( speaker A initiates the exchange and speaker B responds it )</definiens>
			</definition>
			<definition id="1">
				<sentence>Coupling is a pattern of [ A : I B : R ] [ B : I A : R ] .</sentence>
				<definiendum id="0">Coupling</definiendum>
				<definiens id="0">a pattern of [ A : I B : R ] [ B : I A : R ]</definiens>
			</definition>
			<definition id="2">
				<sentence>Elliptical coupling is a pattern of [ A : I ] [ B : I A : R ] which omits the response in the ﬁrst exchange .</sentence>
				<definiendum id="0">Elliptical coupling</definiendum>
				<definiens id="0">a pattern of [ A : I ] [ B : I A : R ] which omits the response in the ﬁrst exchange</definiens>
			</definition>
			<definition id="3">
				<sentence>The segment relation indicates the one between the preceding and the followingsegments , which is classiﬁed as clariﬁcation , interruption , and return .</sentence>
				<definiendum id="0">segment relation</definiendum>
				<definiens id="0">classiﬁed as clariﬁcation , interruption , and return</definiens>
			</definition>
			<definition id="4">
				<sentence>Transformation-based learning is a simple rule-based learning algorithm .</sentence>
				<definiendum id="0">Transformation-based learning</definiendum>
				<definiens id="0">a simple rule-based learning algorithm</definiens>
			</definition>
			<definition id="5">
				<sentence>Usingpost-pruningmethod , wegot 82 % of accuracy ( average12 nodes ; estimated accuracy 76 % ) in closed test and 78 % in open test ( see Table 4 ) .</sentence>
				<definiendum id="0">Usingpost-pruningmethod</definiendum>
				<definiens id="0">wegot 82 % of accuracy ( average12 nodes ; estimated accuracy 76 % ) in closed test</definiens>
			</definition>
</paper>

		<paper id="1509">
			<definition id="0">
				<sentence>We have also found that XML is a convenient language to store linguistic results , such as the shared derivation forests output by our TAG parsers .</sentence>
				<definiendum id="0">XML</definiendum>
			</definition>
			<definition id="1">
				<sentence>dtd , xml &lt; /f &gt; &lt; f name= '' num '' &gt; &lt; val &gt; sing &lt; /val &gt; &lt; /f &gt; &lt; /fs &gt; &lt; /lemmaref &gt; &lt; /morph &gt; &lt; lemma cat= '' v '' name= '' *DONNER* '' &gt; &lt; anchor tree_id= '' family [ @ name=tn1pn2 ] '' &gt; &lt; coanchor node_id= '' p_2 '' &gt; &lt; lex &gt; à &lt; /lex &gt; &lt; /coanchor &gt; &lt; equation node_id= '' np_0 '' type= '' top '' &gt; &lt; fs &gt; &lt; f name= '' restr '' &gt; &lt; val &gt; plushum &lt; /val &gt; &lt; /f &gt; &lt; /fs &gt; &lt; /equation &gt; &lt; equation node_id= '' np_2 '' type= '' top '' &gt; &lt; fs &gt; &lt; f name= '' restr '' &gt; &lt; val &gt; plushum &lt; /val &gt; &lt; /f &gt; &lt; /fs &gt; &lt; /equation &gt; &lt; /anchor &gt; &lt; /lemma &gt; &lt; family name= '' tn1pn2 '' &gt; &lt; tree name= '' tn1pn2 '' &gt; &lt; node cat= '' s '' adj= '' yes '' type= '' std '' &gt; &lt; node cat= '' np '' id= '' np_0 '' type= '' subst '' / &gt; &lt; node cat= '' vp '' adj= '' yes '' type= '' std '' &gt; &lt; node cat= '' v '' adj= '' yes '' type= '' anchor '' / &gt; &lt; node cat= '' np '' type= '' subst '' / &gt; &lt; node cat= '' pp '' adj= '' yes '' type= '' std '' &gt; &lt; node cat= '' p '' id= '' p_2 '' type= '' subst '' / &gt; &lt; node cat= '' np '' id= '' np_2 '' type= '' subst '' / &gt; &lt; /node &gt; &lt; /node &gt; &lt; /node &gt; &lt; /tree &gt; &lt; /family &gt; &lt; /tag &gt; Currently , we have encoded a small French grammar ( 50 tree schemata , 117 lemmas and 345 morphological entries ) and an English grammar ( 456 tree schemata , 333 lemmas and 507 morphological entries ) .</sentence>
				<definiendum id="0">node cat= '' s '' adj= '' yes '' type=</definiendum>
				<definiens id="0">node cat= '' vp '' adj= '' yes '' type= '' std '' &gt; &lt; node cat= '' v '' adj= '' yes '' type= '' anchor '' / &gt; &lt; node cat= '' np '' type= '' subst '' / &gt; &lt; node cat= '' pp '' adj= '' yes '' type= '' std '' &gt; &lt; node cat= '' p '' id= '' p_2 '' type= '' subst</definiens>
			</definition>
			<definition id="2">
				<sentence>One of them ( DyALog ) expects a prolog-like representation of the grammars ( Alonso Pardo et al. , 2000 ) while the second one expects Range Concatenation Grammars [ RCG ] ( Boullier , 2000 ) .</sentence>
				<definiendum id="0">DyALog )</definiendum>
				<definiens id="0">expects a prolog-like representation of the grammars</definiens>
			</definition>
			<definition id="3">
				<sentence>Modules LP , RCG , and XTAG read the output formats of the derivation forests produced by our parsers and by the TAG parser6 .</sentence>
				<definiendum id="0">XTAG</definiendum>
				<definiens id="0">the output formats of the derivation forests produced by our parsers and by the TAG parser6</definiens>
			</definition>
			<definition id="4">
				<sentence>Addition can be achieved by sending an XML fragment and a DBId ( stating where to attach the XML fragment ) .</sentence>
				<definiendum id="0">DBId</definiendum>
				<definiens id="0">stating where to attach the XML fragment )</definiens>
			</definition>
</paper>

		<paper id="0515">
</paper>

		<paper id="1300">
</paper>

		<paper id="0508">
			<definition id="0">
				<sentence>Naive ( Bayes ) at forty : The independence assumption in IR .</sentence>
				<definiendum id="0">Naive ( Bayes</definiendum>
				<definiens id="0">The independence assumption in IR</definiens>
			</definition>
			<definition id="1">
				<sentence>Bow : A toolkit for statistical language modeling , text retrieval , classification , and clustering .</sentence>
				<definiendum id="0">Bow</definiendum>
				<definiens id="0">A toolkit for statistical language modeling</definiens>
			</definition>
			<definition id="2">
				<sentence>Boostexter : A boosting-based system for text categorization .</sentence>
				<definiendum id="0">Boostexter</definiendum>
				<definiens id="0">A boosting-based system for text categorization</definiens>
			</definition>
</paper>

		<paper id="1622">
			<definition id="0">
				<sentence>This paper describes how , within the framework of the MATIS 1 ( Multimodal Access to Transaction and Information Services ) project we developed a prototype of a multimodal railway information system by extending a speech-only version in such a way that it supports screen output and point-and-click actions of the user as input .</sentence>
				<definiendum id="0">prototype</definiendum>
				<definiens id="0">of a multimodal railway information system by extending a speech-only version in such a way that it supports screen output</definiens>
			</definition>
			<definition id="1">
				<sentence>Our multimodal railway information system is an extended version of the mixed-initiative speech-only railway information system ( OVIS ) developed in the NWO-TST programme 2 .</sentence>
				<definiendum id="0">multimodal railway information system</definiendum>
				<definiens id="0">an extended version of the mixed-initiative speech-only railway information system ( OVIS ) developed in the NWO-TST programme 2</definiens>
			</definition>
			<definition id="2">
				<sentence>The MATIS system inherited an architecture in which modules communicate with each other using TCP socket connections under the control of a central module ( Phrisco ) ( cf. Figure 1 ) .</sentence>
				<definiendum id="0">MATIS system</definiendum>
			</definition>
</paper>

		<paper id="0512">
			<definition id="0">
				<sentence>Transformation-Based Error-Driven Learning and Natural Language Processing : A Case Study in Part-of-Speech Tagging .</sentence>
				<definiendum id="0">Transformation-Based Error-Driven Learning</definiendum>
			</definition>
</paper>

		<paper id="0809">
			<definition id="0">
				<sentence>ITS3 is a principle-based system , linguistically inspired by the Government &amp; Binding ( GB ) theory .</sentence>
				<definiendum id="0">ITS3</definiendum>
			</definition>
			<definition id="1">
				<sentence>The analysis phase consists of two steps : GB-based syntax analysis and PSS construction .</sentence>
				<definiendum id="0">analysis phase</definiendum>
			</definition>
			<definition id="2">
				<sentence>Syntax analysis is carried out by the IPS parser ( Wehrli , 1992 ) , which builds the X-bar structure of the sentence , using many filtering constraints ( on thematic roles , on cases , etc. ) to reduce overgeneration .</sentence>
				<definiendum id="0">IPS parser</definiendum>
				<definiens id="0">builds the X-bar structure of the sentence , using many filtering constraints ( on thematic roles , on cases</definiens>
			</definition>
			<definition id="3">
				<sentence>A PSS thus contains abstract linguistic values for `` closed '' features ( tense , mood , voice , number , gender , etc. ) , and lexical values for `` open '' features ( CLS Predicate , DPS Property , etc. ) .</sentence>
				<definiendum id="0">`` closed '' features</definiendum>
				<definiens id="0">tense , mood , voice , number , gender , etc.</definiens>
			</definition>
			<definition id="4">
				<sentence>ITS3 is one among few systems that can do French syntax analysis with large lexical and grammatical coverage .</sentence>
				<definiendum id="0">ITS3</definiendum>
			</definition>
			<definition id="5">
				<sentence>GBGEN supposes a 1-1 mapping in which a determiner in a language corresponds to a universal operator and vice versa , eg. : English French Operator each chaque every this , these ce , cette , ces demonstrative no aucun , aucune no `` Ces chats '' , eg. , is analyzed into a PSS like ( note the Operator slot ) : DPS [ Property : chat Operator : demonstrative Number : plural Ref .</sentence>
				<definiendum id="0">GBGEN</definiendum>
				<definiendum id="1">Operator</definiendum>
				<definiens id="0">supposes a 1-1 mapping in which a determiner in a language corresponds to a universal operator and vice versa</definiens>
			</definition>
			<definition id="6">
				<sentence>The principal strategy of GBGEN for TP construction is to create the following general frame , and attempt to fill it gradually with appropriate elements : [ TP [ T ' Modal [ VP Perfective [ VP Passive [ VP Progresive [ VP Main ] ] ] ] ] ] where Modal , Perfective , Passive , and Progressive stand for auxiliary verbs representing respectively the modal , perfective , passive , and progressive aspects of the TP , and Main is the main verb .</sentence>
				<definiendum id="0">Main</definiendum>
				<definiens id="0">the main verb</definiens>
			</definition>
			<definition id="7">
				<sentence>The Modality slot contains an abstract value for the modality of the verb , eg .</sentence>
				<definiendum id="0">Modality slot</definiendum>
				<definiens id="0">contains an abstract value for the modality of the verb , eg</definiens>
			</definition>
			<definition id="8">
				<sentence>GBGEN foresees an orthogonal combination of negation and modality ; it inserts `` not '' after the modal verb for English , or `` ne '' and `` pas '' around it for French .</sentence>
				<definiendum id="0">GBGEN</definiendum>
			</definition>
			<definition id="9">
				<sentence>GBGEN uses this information to locate the generated AdvP in an appropriate position .</sentence>
				<definiendum id="0">GBGEN</definiendum>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>( BQBU DC ) CG AX CCBPB4CCD2CGB5 F/W Type Raising ( BQCC ) CG AX CCD2B4CCBPCGB5 B/W Type Raising ( BOCC ) Table 2 : CCG Rules for Korean Combinatory Categorial Grammars ( CCGs ) are combinatory extensions to the categorial grammars ( Steedman , 2000 ) .</sentence>
				<definiendum id="0">BQBU DC ) CG AX CCBPB4CCD2CGB5 F/W Type Raising</definiendum>
			</definition>
			<definition id="1">
				<sentence>( 3 ) CFCWD3 DBCTCPD6D7 D3D0CS D7CWD3CTD7 CPD2CS CP CQD6D3DBD2 CRD3CPD8BR D2D4 B4D7D2D2D4B5BPD2D4 D2D4 CRD3D2CY D2D4 BOA8 D2 BQ D2D4 BQ D7D2D2D4 BO D7 In addition to function application utilized in examples 2 and 3 , CCGs use rules for a limited set of combinators such as B ( function composition ) , T ( type raising ) , and S ( function substitution ) to model natural language .</sentence>
				<definiendum id="0">S ( function substitution )</definiendum>
				<definiens id="0">function composition )</definiens>
			</definition>
			<definition id="2">
				<sentence>And for the same reason , we devised the operator ARB4DCBNC1B5 where DC is an argument and C1 represents a time interval variable .</sentence>
				<definiendum id="0">DC</definiendum>
				<definiendum id="1">C1</definiendum>
				<definiens id="0">an argument</definiens>
			</definition>
			<definition id="3">
				<sentence>The lexical entry consists of a lexical item and its CCG category .</sentence>
				<definiendum id="0">lexical entry</definiendum>
			</definition>
			<definition id="4">
				<sentence>The CCG category is a pair of the syntactic and semantic information that are interwoven in the following way .</sentence>
				<definiendum id="0">CCG category</definiendum>
				<definiens id="0">a pair of the syntactic and semantic information that are interwoven in the following way</definiens>
			</definition>
			<definition id="5">
				<sentence>Elementary CCG ( syntactic ) categories include D2D4 and D7 , and CCG categories are recursively defined as either CGBPCH or CGD2CH , where CG and CH are also CCG categories , including elementary categories .</sentence>
				<definiendum id="0">Elementary CCG ( syntactic</definiendum>
				<definiendum id="1">CCG categories</definiendum>
			</definition>
			<definition id="6">
				<sentence>Each elementary CCG ( syntactic ) category CG is augmented with an appropriate semantic information CH and word disambiguation information CI so that the resulting form CG BM CH BNCI is a CCG category ( Steedman , 1996 ) .</sentence>
				<definiendum id="0">CCG</definiendum>
				<definiendum id="1">BNCI</definiendum>
				<definiens id="0">augmented with an appropriate semantic information CH and word disambiguation information CI so that the resulting form CG BM CH</definiens>
			</definition>
</paper>

		<paper id="1512">
			<definition id="0">
				<sentence>The LKB system is a grammar development environment that is distributed as part of the open source LinGO tools ( http : //wwwcsli.stanford.edu/˜aac/lkb.html and http : //lingo.stanford.edu , see also Copestake and Flickinger , 2000 ) .</sentence>
				<definiendum id="0">LKB system</definiendum>
			</definition>
			<definition id="1">
				<sentence>Grammars of all sizes have been written using the LKB , for several languages , mostly within the linguistic frameworks of Categorial Grammar and HeadDriven Phrase Structure Grammar .</sentence>
				<definiendum id="0">LKB</definiendum>
			</definition>
			<definition id="2">
				<sentence>Theory : An Introduction , CSLI Publications .</sentence>
				<definiendum id="0">Theory</definiendum>
				<definiens id="0">An Introduction</definiens>
			</definition>
</paper>

		<paper id="1200">
</paper>

		<paper id="0703">
			<definition id="0">
				<sentence>Selectional preferences try to capture the fact that linguistic elements prefer arguments of a certain semantic class , e.g. a verb like ‘eat’ prefers as object edible things , and as subject animate entities , as in , ( 1 ) “She was eating an apple” .</sentence>
				<definiendum id="0">Selectional preferences</definiendum>
				<definiens id="0">object edible things , and as subject animate entities</definiens>
			</definition>
</paper>

		<paper id="1201">
			<definition id="0">
				<sentence>Figure 1 : Frequency of answers in the TREC-8 ( black bars ) and CBC ( white bars ) data sets To gather this data we manually reviewed 50 randomly chosen TREC-8 questions and identified all answers to these questions in our text collection .</sentence>
				<definiendum id="0">CBC</definiendum>
				<definiens id="0">Frequency of answers in the TREC-8 ( black bars</definiens>
			</definition>
			<definition id="1">
				<sentence>Each solid dot in the scatter plot represents one of the 50 questions we examined.4 The x-axis shows the number of answer opportunities for the question , and the y-axis represents the percentage of systems that generated a correct answer5 for the question .</sentence>
				<definiendum id="0">y-axis</definiendum>
				<definiens id="0">the percentage of systems</definiens>
			</definition>
			<definition id="2">
				<sentence>We manually annotated data for 165 TREC9 questions and 186 CBC questions to indicate perfect question typing , perfect answer sentence identification , and perfect semantic tagging .</sentence>
				<definiendum id="0">CBC</definiendum>
				<definiens id="0">questions to indicate perfect question typing , perfect answer sentence identification , and perfect semantic tagging</definiens>
			</definition>
</paper>

		<paper id="0714">
			<definition id="0">
				<sentence>For our experiments , the metric of similarity between sequences was the Jensen-Shannon divergence of the sequences’ signatures : a39 JSa8a15a7a41a40a31a42a43a7a22a44a26a11a9a27 a40 a44 a30 a39 KLa8a15a7a36a40a34a45a35a46a34a47a49a48a32a46a51a50a44 a11a34a52 a39 KLa8a15a7a20a44a53a45a35a46a34a47a54a48a32a46a51a50a44 a11a38a37 Where a39 KL is the Kullback-Leibler divergence between probability distributions .</sentence>
				<definiendum id="0">a39 KL</definiendum>
				<definiens id="0">the Kullback-Leibler divergence between probability distributions</definiens>
			</definition>
			<definition id="1">
				<sentence>The grammar a56 which maximizes a57 a8 a39 a45a58a56a59a11 depends on the corpus a39 , which , in some sense , the core of a given language’s phrase structure should not .</sentence>
				<definiendum id="0">grammar a56</definiendum>
				<definiens id="0">maximizes a57 a8 a39 a45a58a56a59a11 depends on the corpus a39</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , in the treebank data used , CD CD is a common object of a verb , but a very rare subject .</sentence>
				<definiendum id="0">CD CD</definiendum>
				<definiens id="0">a common object of a verb</definiens>
			</definition>
			<definition id="3">
				<sentence>Another serious issue with MDL is that the target grammar is presumably bounded in size , while adding more and more data will on average cause MDL methods to choose ever larger grammars .</sentence>
				<definiendum id="0">MDL</definiendum>
			</definition>
			<definition id="4">
				<sentence>GREEDY-MERGE is a precision-oriented system which , to a first approximation , can be seen as an agglomerative clustering process over sequences .</sentence>
				<definiendum id="0">GREEDY-MERGE</definiendum>
				<definiens id="0">a precision-oriented system which , to a first approximation , can be seen as an agglomerative clustering process over sequences</definiens>
			</definition>
			<definition id="5">
				<sentence>“Total” is the frequency of the sequence in the flat data .</sentence>
				<definiendum id="0">“Total”</definiendum>
				<definiens id="0">the frequency of the sequence in the flat data</definiens>
			</definition>
</paper>

		<paper id="1202">
			<definition id="0">
				<sentence>MURAX ( Kupiec , 1993 ) is one of the noun-phrase extraction systems .</sentence>
				<definiendum id="0">MURAX</definiendum>
				<definiens id="0">one of the noun-phrase extraction systems</definiens>
			</definition>
			<definition id="1">
				<sentence>MAYA uses shallow linguistic information such as a POS tagger , a lexico-syntactic parser similar to finite-state recognizer in MURAX and a Named Entity ( NE ) recognizer based on dictionaries .</sentence>
				<definiendum id="0">MAYA</definiendum>
				<definiens id="0">uses shallow linguistic information such as a POS tagger , a lexico-syntactic parser similar to finite-state recognizer in MURAX and a Named Entity ( NE ) recognizer based on dictionaries</definiens>
			</definition>
			<definition id="2">
				<sentence>However , MAYA returns answer phrases in very short time compared with those previous systems because the system extracts answer candidates and gives each answer a score using pre-defined rules on indexing time .</sentence>
				<definiendum id="0">MAYA</definiendum>
				<definiens id="0">returns answer phrases in very short time compared with those previous systems because the system extracts answer candidates and gives each answer a score using pre-defined rules on indexing time</definiens>
			</definition>
			<definition id="3">
				<sentence>MAYA has been designed as a separate component that interfaces with a traditional IR system .</sentence>
				<definiendum id="0">MAYA</definiendum>
				<definiens id="0">a separate component that interfaces with a traditional IR system</definiens>
			</definition>
			<definition id="4">
				<sentence>The searching engine identifies a user’s asking point , and selects an index DB that includes answer candidates of his/her query .</sentence>
				<definiendum id="0">searching engine</definiendum>
				<definiens id="0">identifies a user’s asking point , and selects an index DB that includes answer candidates of his/her query</definiens>
			</definition>
			<definition id="5">
				<sentence>{ 2 , } ) * $ In the next stage , the indexing engine gives scores to content words within a context window that occur with answer candidates .</sentence>
				<definiendum id="0">indexing engine</definiendum>
				<definiens id="0">gives scores to content words within a context window that occur with answer candidates</definiens>
			</definition>
			<definition id="6">
				<sentence>For example , when www.yahoo.co.kr is an answer candidate in the sentence , “ a61 a62 a63 a64 a65 ( www.yahoo.co.kr ) a66 a67 a68 a69 a70 a71 a72 a73 a74 a75 a76 a77 .</sentence>
				<definiendum id="0">www.yahoo.co.kr</definiendum>
				<definiens id="0">an answer candidate in the sentence , “ a61 a62 a63 a64 a65 ( www.yahoo.co.kr ) a66 a67 a68 a69 a70 a71 a72 a73 a74 a75 a76 a77</definiens>
			</definition>
			<definition id="7">
				<sentence>The indexing engine gives 2 points to each content word in the sentence including the answer candidate .</sentence>
				<definiendum id="0">indexing engine</definiendum>
				<definiens id="0">gives 2 points to each content word in the sentence including the answer candidate</definiens>
			</definition>
			<definition id="8">
				<sentence>The indexing engine gives 2 points to each appositive word and gives 1 point to others .</sentence>
				<definiendum id="0">indexing engine</definiendum>
			</definition>
			<definition id="9">
				<sentence>EDCBA fEfDfCfBfAts iiiii i ++++ ⋅+⋅+⋅+⋅+⋅= 54321 ( 1 ) tsi is the term score of the ith term , and fij is the score of the jth feature in the ith term .</sentence>
				<definiendum id="0">fij</definiendum>
			</definition>
			<definition id="10">
				<sentence>Max_tsj is the maximum value among term scores in the context window that is relevant to the jth answer candidate .</sentence>
				<definiendum id="0">Max_tsj</definiendum>
				<definiens id="0">the maximum value among term scores in the context window that is relevant to the jth answer candidate</definiens>
			</definition>
			<definition id="11">
				<sentence>n is the number of answer candidates that are affected by the ith term .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of answer candidates that are affected by the ith term</definiens>
			</definition>
			<definition id="12">
				<sentence>N is the number of answer candidates of the same semantic category .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of answer candidates of the same semantic category</definiens>
			</definition>
			<definition id="13">
				<sentence>The indexing engine saves the normalized term scores with the position information of the relevant answer candidate in the DB .</sentence>
				<definiendum id="0">indexing engine</definiendum>
				<definiens id="0">saves the normalized term scores with the position information of the relevant answer candidate in the DB</definiens>
			</definition>
			<definition id="14">
				<sentence>The position information includes a document number and the distance between the beginning of the document and the answer candidate .</sentence>
				<definiendum id="0">position information</definiendum>
				<definiens id="0">includes a document number and the distance between the beginning of the document and the answer candidate</definiens>
			</definition>
			<definition id="15">
				<sentence>The total system ranks the retrieved documents by using the combined similarity values , and shows the sentences including answer candidates in the documents .</sentence>
				<definiendum id="0">total system</definiendum>
				<definiens id="0">ranks the retrieved documents by using the combined similarity values</definiens>
			</definition>
			<definition id="16">
				<sentence>RDR means the reciprocal rank of the first document including the correct answers given by each question .</sentence>
				<definiendum id="0">RDR</definiendum>
				<definiens id="0">means the reciprocal rank of the first document including the correct answers given by each question</definiens>
			</definition>
</paper>

		<paper id="0906">
			<definition id="0">
				<sentence>Systems Language engineering research is carried out in areas such as speech recognition , natural language understanding , natural language generation , speech synthesis , information retrieval , information extraction , and inference ( Jurafsky &amp; Martin , 2000 ) .</sentence>
				<definiendum id="0">inference</definiendum>
				<definiens id="0">speech recognition , natural language understanding , natural language generation , speech synthesis , information retrieval , information extraction</definiens>
			</definition>
</paper>

		<paper id="0900">
</paper>

		<paper id="1406">
			<definition id="0">
				<sentence>hacer usted información hipervínculo hipervínculo clic dirección click Hyperlink_Information you address hyperlink de de en en under Dsub Dobj Mod Dsub Dobj Figure 1a : Lexical correspondences Figure 1b : Alignment Mappings hacer usted información hipervínculo hipervínculo clic dirección click Hyperlink_Information you address hyperlink de de en en under Dsub Dobj Mod Dsub Dobj A Logical Form ( LF ) is an unordered graph representing the relations among the most meaningful elements of a sentence .</sentence>
				<definiendum id="0">Logical Form ( LF )</definiendum>
				<definiens id="0">an unordered graph representing the relations among the most meaningful elements of a sentence</definiens>
			</definition>
</paper>

		<paper id="1301">
			<definition id="0">
				<sentence>The text above is an accident report from the MAIF1 corpus , which contains 87 texts in French .</sentence>
				<definiendum id="0">MAIF1 corpus</definiendum>
				<definiens id="0">contains 87 texts in French</definiens>
			</definition>
			<definition id="1">
				<sentence>In CarSim , the general accident model consists of three lists of objects : motionless objects ( STATIC ) , moving objects ( DYNAMIC ) , and flnally collisions ( ACCIDENT ) .</sentence>
				<definiendum id="0">general accident model</definiendum>
				<definiens id="0">consists of three lists of objects : motionless objects ( STATIC ) , moving objects ( DYNAMIC ) , and flnally collisions ( ACCIDENT )</definiens>
			</definition>
			<definition id="2">
				<sentence>A collision consists of a verb , an actor , a victim and of the participating parts of the two vehicles .</sentence>
				<definiendum id="0">collision</definiendum>
				<definiens id="0">consists of a verb , an actor , a victim and of the participating parts of the two vehicles</definiens>
			</definition>
			<definition id="3">
				<sentence>Generally , a trajectory consists of a number of ‘normal’ trajectory points , followed by a number of trajectory points that represent a collision .</sentence>
				<definiendum id="0">trajectory</definiendum>
				<definiens id="0">consists of a number of ‘normal’ trajectory points , followed by a number of trajectory points that represent a collision</definiens>
			</definition>
</paper>

		<paper id="1400">
</paper>

		<paper id="1505">
			<definition id="0">
				<sentence>To this end , SiSSA provides the user with a graphical environment for ( 1 ) selecting the linguistic activities which are relevant to the particular application at hand , along with the linguistic processors that execute them ; ( 2 ) checking that the chosen architectural hypothesis corresponds to the functional specifications of the application ; ( 3 ) connecting to SiSSA new linguistic processors , this way making them available for the prototyping/design activities .</sentence>
				<definiendum id="0">SiSSA</definiendum>
				<definiens id="0">provides the user with a graphical environment for ( 1 ) selecting the linguistic activities which are relevant to the particular application at hand</definiens>
			</definition>
			<definition id="1">
				<sentence>SiSSA makes crucial use of state-of-the-art software technologies ( CORBA , XML ) in order to integrate the various modules in an effective way .</sentence>
				<definiendum id="0">SiSSA</definiendum>
				<definiens id="0">makes crucial use of state-of-the-art software technologies ( CORBA , XML ) in order to integrate the various modules in an effective way</definiens>
			</definition>
			<definition id="2">
				<sentence>SiSSA consists of two parts : an autonomous application ( called SiSSA Manager ) and a set of executable modules , henceforth called processors .</sentence>
				<definiendum id="0">SiSSA</definiendum>
			</definition>
			<definition id="3">
				<sentence>The SiSSA Manager provides an infrastructure for architecture composition and processor integration .</sentence>
				<definiendum id="0">SiSSA Manager</definiendum>
				<definiens id="0">provides an infrastructure for architecture composition and processor integration</definiens>
			</definition>
			<definition id="4">
				<sentence>RDF Schema makes available tools to check that the descriptions of the processors’ characteristics comply with SiSSA Manager’s constraints .</sentence>
				<definiendum id="0">RDF Schema</definiendum>
				<definiens id="0">makes available tools to check that the descriptions of the processors’ characteristics comply with SiSSA Manager’s constraints</definiens>
			</definition>
			<definition id="5">
				<sentence>An important service provided by the SiSSA Manager is the XSL7 processing of XML documents .</sentence>
				<definiendum id="0">SiSSA Manager</definiendum>
			</definition>
			<definition id="6">
				<sentence>SiSSA is a development environment , meant to be open to the integration of new components , whereby the latter can differ among them along a number of dimension , including the input/output formats .</sentence>
				<definiendum id="0">SiSSA</definiendum>
				<definiens id="0">a development environment</definiens>
			</definition>
			<definition id="7">
				<sentence>XML allows a representation of data which is transparent 7The Extensible Style Language ( XSL ( Adler et al. , 2000 ) ) is a language that allows to transform data from one XML representation to another .</sentence>
				<definiendum id="0">XML</definiendum>
				<definiendum id="1">XSL</definiendum>
				<definiens id="0">allows a representation of data</definiens>
				<definiens id="1">a language that allows to transform data from one XML representation to another</definiens>
			</definition>
			<definition id="8">
				<sentence>The modular nature of the DTDs for XML allows a neat distinction among metadata , and data relative to classes of processors ( idiosyncratic data ) .</sentence>
				<definiendum id="0">DTDs for XML</definiendum>
			</definition>
			<definition id="9">
				<sentence>From left to right : a0 Home : a link to the starting page of SiSSA ; a0 SiSSA Manager : a link to the page of the SiSSA Manager ; a0 Progetti ( projects ) : a link to the page that allows to create , edit , and activate the user’s projects ; a0 Repository : a link to the page for interacting with the Processor Repository ; a0 Help : an online help .</sentence>
				<definiendum id="0">a0 SiSSA Manager</definiendum>
				<definiendum id="1">Help</definiendum>
				<definiens id="0">a link to the page of the SiSSA Manager ; a0 Progetti ( projects ) : a link to the page that allows to create , edit , and activate the user’s projects ; a0 Repository : a link to the page for interacting with the Processor Repository ; a0</definiens>
			</definition>
			<definition id="10">
				<sentence>Currently the SiSSA Manager is a single-user application .</sentence>
				<definiendum id="0">SiSSA Manager</definiendum>
				<definiens id="0">a single-user application</definiens>
			</definition>
			<definition id="11">
				<sentence>TAL is a project partially funded by the Italian Ministry for University and Scientific Research .</sentence>
				<definiendum id="0">TAL</definiendum>
				<definiens id="0">a project partially funded by the Italian Ministry for University and Scientific Research</definiens>
			</definition>
</paper>

		<paper id="0908">
			<definition id="0">
				<sentence>A model is an approximation or a more abstract representation of training data .</sentence>
				<definiendum id="0">model</definiendum>
				<definiens id="0">an approximation or a more abstract representation of training data</definiens>
			</definition>
			<definition id="1">
				<sentence>The bootstrap is a re-sampling technique designed for obtaining empirical distributions of estimators .</sentence>
				<definiendum id="0">bootstrap</definiendum>
				<definiens id="0">a re-sampling technique designed for obtaining empirical distributions of estimators</definiens>
			</definition>
			<definition id="2">
				<sentence>After B samples , we have a set of bootstrap samples fxb1 ; : : : ; xbngBb=1 , each of which yields an estimate ^Sb for S. The distribution of ^S is the bootstrap estimate for the distribution of S. That distribution is mostly used for estimating the standard deviation , bias , or con dence interval of S. In the present work , xi are the base-NP instances in a given corpus , and the statistic S is the recall on a test set .</sentence>
				<definiendum id="0">^S</definiendum>
				<definiendum id="1">statistic S</definiendum>
			</definition>
			<definition id="3">
				<sentence>Wall-Street Journal ( WSJ ) Sections 15-18 and 20 were used by Ramshaw and Marcus ( 1995 ) as training and test data respectively for evaluating their base-NP chunker .</sentence>
				<definiendum id="0">Wall-Street Journal</definiendum>
				<definiens id="0">training and test data respectively for evaluating their base-NP chunker</definiens>
			</definition>
			<definition id="4">
				<sentence>MBSL is a memory-based system which records , for each POS sequence containing a border ( left , right , or both ) of a base-NP , the number of times it appears with that border vs. the number of times it appears without it .</sentence>
				<definiendum id="0">MBSL</definiendum>
			</definition>
			<definition id="5">
				<sentence>Winnow is a mistake-driven algorithm for learning a linear separator , in which feature weights are updated by multiplication .</sentence>
				<definiendum id="0">Winnow</definiendum>
				<definiens id="0">a mistake-driven algorithm for learning a linear separator , in which feature weights are updated by multiplication</definiens>
			</definition>
			<definition id="6">
				<sentence>MBSL runs with the two c values were conducted on the same training samples , therefore it is possible to compare their results directly .</sentence>
				<definiendum id="0">MBSL</definiendum>
				<definiens id="0">runs with the two c values were conducted on the same training samples</definiens>
			</definition>
			<definition id="7">
				<sentence>Precision , on the other hand , may be viewed as the expected 0-1 loss on the sample of instances detected by the learning system .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the expected 0-1 loss on the sample of instances detected by the learning system</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>The dialog management component is the central part of a dialog system and of particular importance in a discussion about streamlined development processes for multimodal interfaces , because it is the bridge between the intelligent interface technologies and the underlying application logic of application servers and databases .</sentence>
				<definiendum id="0">dialog management component</definiendum>
				<definiens id="0">the central part of a dialog system</definiens>
			</definition>
			<definition id="1">
				<sentence>This can be accomplished by a modular or object-oriented design which separates the central application server into the following functions : visual communicator : a modular handler for the visual interface ; voice communicator : a modular handler for the voice interface ; transaction module : encapsulates the transaction needed for the application logic ; multimodal integrator : handles all message flows between the interface modules , and between the interface modules and the transaction module .</sentence>
				<definiendum id="0">visual communicator</definiendum>
				<definiendum id="1">voice communicator</definiendum>
				<definiendum id="2">transaction module</definiendum>
				<definiens id="0">encapsulates the transaction needed for the application logic ; multimodal integrator : handles all message flows between the interface modules</definiens>
			</definition>
			<definition id="2">
				<sentence>The multimodal integrator is the only part in the proposed architecture where information from more than one modality is processed .</sentence>
				<definiendum id="0">multimodal integrator</definiendum>
				<definiens id="0">the only part in the proposed architecture where information from more than one modality is processed</definiens>
			</definition>
			<definition id="3">
				<sentence>XML ( W3C , 2000a ) is a markup language that can be used to represent data and data schemata in text files .</sentence>
				<definiendum id="0">XML</definiendum>
				<definiens id="0">a markup language that can be used to represent data and data schemata in text files</definiens>
			</definition>
			<definition id="4">
				<sentence>A possible implementation of the voice communicator of our architecture would be an XML-based translator that takes general XMLrepresentations and produces VoiceXML dialogs that play or query these data .</sentence>
				<definiendum id="0">XML-based translator</definiendum>
			</definition>
			<definition id="5">
				<sentence>Coombs and Hull ( 1998a ) distinguish five groups of KMPs : KMPs located in the formal R &amp; D management process ; KMPs for managing intellectual property positions ; KMPs for ‘mapping’ knowledge relationships ; KMPs for serial transfer of project experience ; and KMPs contingent on information technology applications .</sentence>
				<definiendum id="0">KMPs</definiendum>
				<definiens id="0">KMPs located in the formal R &amp; D management process ; KMPs for managing intellectual property positions ; KMPs for ‘mapping’ knowledge relationships</definiens>
			</definition>
</paper>

		<paper id="0516">
			<definition id="0">
				<sentence>An abbreviation rule consists of an abbreviation pattern , a definition pattern and a formation rule .</sentence>
				<definiendum id="0">abbreviation rule</definiendum>
				<definiens id="0">consists of an abbreviation pattern , a definition pattern and a formation rule</definiens>
			</definition>
			<definition id="1">
				<sentence>The abbreviation recognizer seeks candidate abbreviations in a text and generates their patterns ( Section 1 ) .</sentence>
				<definiendum id="0">abbreviation recognizer</definiendum>
				<definiens id="0">seeks candidate abbreviations in a text and generates their patterns ( Section 1 )</definiens>
			</definition>
			<definition id="2">
				<sentence>The abbreviation matcher consists of 5 layered matching algorithms ( Section 4.2 ) .</sentence>
				<definiendum id="0">abbreviation matcher</definiendum>
			</definition>
			<definition id="3">
				<sentence>Acronyms are a special case of abbreviations which are devoted to multi-word full forms .</sentence>
				<definiendum id="0">Acronyms</definiendum>
				<definiens id="0">a special case of abbreviations which are devoted to multi-word full forms</definiens>
			</definition>
			<definition id="4">
				<sentence>An abbreviation pattern is a string of ‘c’ and ‘n’ characters .</sentence>
				<definiendum id="0">abbreviation pattern</definiendum>
			</definition>
			<definition id="5">
				<sentence>System Overview Abbreviation Recognizer Definition Finder Rule Applier Abbreviation Matcher B e s t M a t c h S e l e c t o r Rule Base Stopwords Names RuleBase Update maximally allowed distance ( offset ) between a definition and its abbreviation .</sentence>
				<definiendum id="0">Finder Rule Applier Abbreviation Matcher B e</definiendum>
			</definition>
			<definition id="6">
				<sentence>Windows98 = &gt; Windows 98 ( 3 ) separate prefixes1 ) from the headword reusable = &gt; re usable A definition pattern consists of the characters ‘w’ ( word ) , ‘s’ ( stopword ) , ‘p’ ( prefix ) , ‘h’ ( headword ) and ‘n’ ( number ) .</sentence>
				<definiendum id="0">definition pattern</definiendum>
			</definition>
			<definition id="7">
				<sentence>R = &lt; A_Pattern , D_Pattern , F_Rule &gt; A formation rule defines how each character in an abbreviation is formed from a definition .</sentence>
				<definiendum id="0">formation rule</definiendum>
				<definiens id="0">defines how each character in an abbreviation is formed from a definition</definiens>
			</definition>
			<definition id="8">
				<sentence>A word number is the sequential location of a word within the preprocessed definition .</sentence>
				<definiendum id="0">word number</definiendum>
				<definiens id="0">the sequential location of a word within the preprocessed definition</definiens>
			</definition>
			<definition id="9">
				<sentence>A formation method represents how a character ( or characters ) in the word takes part in the abbreviation .</sentence>
				<definiendum id="0">formation method</definiendum>
			</definition>
			<definition id="10">
				<sentence>‘F’ means that the first character of a word occurs in the abbreviation .</sentence>
				<definiendum id="0">‘F’</definiendum>
			</definition>
			<definition id="11">
				<sentence>Similarly , ‘I’ refers to an interior character and ‘L’ indicates the last character of a word .</sentence>
				<definiendum id="0">‘I’</definiendum>
				<definiendum id="1">‘L’</definiendum>
				<definiens id="0">an interior character and</definiens>
				<definiens id="1">the last character of a word</definiens>
			</definition>
			<definition id="12">
				<sentence>‘E’ means ‘exact match’ and ‘R’ means ‘replacement match’ .</sentence>
				<definiendum id="0">‘E’</definiendum>
				<definiens id="0">means ‘exact match’ and ‘R’ means ‘replacement match’</definiens>
			</definition>
			<definition id="13">
				<sentence>|D| is the length of a definition pattern .</sentence>
				<definiendum id="0">|D|</definiendum>
			</definition>
			<definition id="14">
				<sentence>AFP ( Acronym Finding Program ) is an early attempt to automatically find acronyms and their definitions in free text [ 13 ] .</sentence>
				<definiendum id="0">AFP</definiendum>
			</definition>
			<definition id="15">
				<sentence>AFP looks for candidate expansions in two sub-windows – the pre-window and the post-window of the acronym by applying an LCS ( longest common subsequence ) algorithm .</sentence>
				<definiendum id="0">AFP</definiendum>
				<definiendum id="1">LCS</definiendum>
				<definiens id="0">looks for candidate expansions in two sub-windows – the pre-window and the post-window of the acronym by applying an</definiens>
			</definition>
			<definition id="16">
				<sentence>TLA ( Three Letter Acronym ) [ 14 ] removes all non-alphabetic characters and breaks the text into chunks based on the occurrences of ‘ ( ’ , ‘ ) ’ and ‘.’</sentence>
				<definiendum id="0">TLA</definiendum>
			</definition>
			<definition id="17">
				<sentence>Furthermore , the abbreviation matcher consists of 5 simple match routines and each routine is dedicated to a certain type of abbreviations .</sentence>
				<definiendum id="0">abbreviation matcher</definiendum>
			</definition>
</paper>

		<paper id="0802">
			<definition id="0">
				<sentence>Content determination is the task of deciding on the information content of a generated text .</sentence>
				<definiendum id="0">Content determination</definiendum>
			</definition>
			<definition id="1">
				<sentence>While realisation , microplanning , and document structuring techniques are increasingly based on analyses of how humans perform these tasks ( including corpus analysis , psycholinguistic studies , and KA activities ) , most papers on content determination make little reference to how human experts determine the content of a text .</sentence>
				<definiendum id="0">KA</definiendum>
				<definiens id="0">content determination make little reference to how human experts determine the content of a text</definiens>
			</definition>
			<definition id="2">
				<sentence>Time-series data is a collection of values of a set of parameters over time .</sentence>
				<definiendum id="0">Time-series data</definiendum>
				<definiens id="0">a collection of values of a set of parameters over time</definiens>
			</definition>
			<definition id="3">
				<sentence>The first year of SUMTIME ( which started in April 2000 ) has mostly been devoted to knowledge acquisition , that is to trying to understand how human experts summarise timeseries data .</sentence>
				<definiendum id="0">SUMTIME</definiendum>
			</definition>
			<definition id="4">
				<sentence>While interpreting this data the expert used his meteorological knowledge ( which includes his personal experience in interpreting weather data ) to arrive at an overview of the weather .</sentence>
				<definiendum id="0">meteorological knowledge</definiendum>
				<definiens id="0">includes his personal experience in interpreting weather data ) to arrive at an overview of the weather</definiens>
			</definition>
			<definition id="5">
				<sentence>RajuGuide is a system that generates route descriptions .</sentence>
				<definiendum id="0">RajuGuide</definiendum>
			</definition>
			<definition id="6">
				<sentence>Communication Goal ( CG ) is the input to the data summarisation system in response to which it accesses DDS to produce an overview of the data using the DR. In the context of the overview produced by DR , the Data Comprehension Goal ( Derived from Comm .</sentence>
				<definiendum id="0">Communication Goal ( CG )</definiendum>
				<definiens id="0">the input to the data summarisation system in response to which it accesses DDS to produce an overview of the data using the DR. In the context of the overview produced by DR , the Data Comprehension Goal ( Derived from Comm</definiens>
			</definition>
			<definition id="7">
				<sentence>Our model has some similarities to the one proposed by Barzilay et al ( 1998 ) , in that the Domain Reasoner uses general domain knowledge similar to their RDK , while the Communication Reasoner uses communication knowledge similar to their CDK and DCK .</sentence>
				<definiendum id="0">Domain Reasoner</definiendum>
				<definiens id="0">uses general domain knowledge similar to their RDK , while the Communication Reasoner uses communication knowledge similar to their CDK and DCK</definiens>
			</definition>
			<definition id="8">
				<sentence>The overview ontology includes concepts used by experts when reasoning about a domain ( such as air masses or motivation ) , while the text ontology includes concepts useful for communicating information to the end user ( such as wind speed , or longer life expectancy ) .</sentence>
				<definiendum id="0">overview ontology</definiendum>
			</definition>
			<definition id="9">
				<sentence>SUMTIME-MOUSAM is a framework system that consists of • `` Infrastructure '' software for accessing data files , regression testing of new software versions , etc. • An ontology , which defines a conceptual level of representation of texts .</sentence>
				<definiendum id="0">SUMTIME-MOUSAM</definiendum>
				<definiendum id="1">ontology</definiendum>
				<definiens id="0">a framework system that consists of • `` Infrastructure '' software for accessing data files , regression testing of new software versions</definiens>
				<definiens id="1">defines a conceptual level of representation of texts</definiens>
			</definition>
</paper>

		<paper id="0726">
</paper>

		<paper id="0800">
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>1 Introduction Decanter is a prototype to detect and display high-level information from argumentative text .</sentence>
				<definiendum id="0">Introduction Decanter</definiendum>
				<definiens id="0">a prototype to detect and display high-level information from argumentative text</definiens>
			</definition>
			<definition id="1">
				<sentence>In a Baconian vein : The information retriever and questioner has to use Invention ( IR techniques ) and Judgment ( critical thinking ) to tap into Memory ( writing , library science ) and Tradition ( corpus of knowledge , opinions ) .</sentence>
				<definiendum id="0">Invention ( IR</definiendum>
				<definiendum id="1">Judgment</definiendum>
				<definiendum id="2">Tradition</definiendum>
			</definition>
			<definition id="2">
				<sentence>Decanter opens the way to the necessary contribution of Judgment in Invention .</sentence>
				<definiendum id="0">Decanter</definiendum>
				<definiens id="0">opens the way to the necessary contribution of Judgment in Invention</definiens>
			</definition>
</paper>

		<paper id="1302">
</paper>

		<paper id="1602">
			<definition id="0">
				<sentence>example-action contexts An interactivespoken language application constructed with the variant transduction method consists of a set of contexts .</sentence>
				<definiendum id="0">example-action</definiendum>
				<definiens id="0">contexts An interactivespoken language application constructed with the variant transduction method consists of a set of contexts</definiens>
			</definition>
			<definition id="1">
				<sentence>The actual contexts that are used at runtime are created through a four step process : small number of ) triples he ; ; a ; ; ci where e is a natural language string ( a typical user input ) , a is an application action ( back-end application API call ) .</sentence>
				<definiendum id="0">e</definiendum>
				<definiens id="0">a natural language string ( a typical user input</definiens>
			</definition>
			<definition id="2">
				<sentence>When v is a paraphrase of e , the adapted action a 0 is the same string as a. In the more general case , the meaning of variant v is differentfromthatof e , and the systemattempts ( not always correctly ) to construct a 0 so that it reects this di erence in meaning .</sentence>
				<definiendum id="0">v</definiendum>
				<definiens id="0">a paraphrase of e , the adapted action a 0 is the same string as a. In the more general case , the meaning of variant v is differentfromthatof e , and the systemattempts ( not always correctly</definiens>
			</definition>
			<definition id="3">
				<sentence>When an example-action context is active during an interaction with a user , twocomponents ( in addition to the speech recognition language model ) are compiled from the context in order to map the user inputs into the appropriate ( possibly adapted ) action : Classi er A classi er is built with training pairs hv ; ; ai where v is a variantofan example e for which the example action pair he ; ; aiis amember ofthe unexpanded pairs in the context .</sentence>
				<definiendum id="0">v</definiendum>
				<definiens id="0">active during an interaction with a user , twocomponents ( in addition to the speech recognition language model ) are compiled from the context in order to map the user inputs into the appropriate ( possibly adapted ) action : Classi er A classi er is built with training pairs hv</definiens>
				<definiens id="1">a variantofan example e for which the example action pair he ; ; aiis amember ofthe unexpanded pairs in the context</definiens>
			</definition>
			<definition id="4">
				<sentence>In the gure , f is the mapping between examples and actions in the unexpanded context ; ; r is the relation between examples and variants ; ; and g is the searchmapping implemented by the classi ermatcher .</sentence>
				<definiendum id="0">r</definiendum>
				<definiendum id="1">g</definiendum>
				<definiens id="0">the relation between examples and variants</definiens>
			</definition>
			<definition id="5">
				<sentence>Dialog control is straightforwardas the reader might expect , except for two aspects described in this section : ( i ) evaluation of nextcontext expressions , and ( ii ) generation of p ( prompt ) : say a mailreader command s ( words spoken ) : now show me messages from Bill v ( variant ) : show the message from Bill Wilson e ( example ) : read the message from John a ( associated action ) : mailAgent .</sentence>
				<definiendum id="0">Dialog control</definiendum>
				<definiens id="0">read the message from John a ( associated action ) : mailAgent</definiens>
			</definition>
			<definition id="6">
				<sentence>getWithSender ( `` wwilson @ att.com '' ) e 0 ( adapted example ) : read the message from Bill Wilson Figure 2 : Example Figure 1 : VariantTransduction mappings con rmation requests based on the examples in the context and the user 's input .</sentence>
				<definiendum id="0">getWithSender</definiendum>
				<definiens id="0">read the message from Bill Wilson Figure 2 : Example Figure 1 : VariantTransduction mappings con rmation requests based on the examples in the context and the user 's input</definiens>
			</definition>
			<definition id="7">
				<sentence>plained in the previous section to produce the triple hv ; ; a 0 ; ; c 0 i. sum of the distance computed for the matcher between s and v and the distance computed by the matcher between v and e ( where e is the example from which v was derived ) .</sentence>
				<definiendum id="0">e</definiendum>
				<definiens id="0">i. sum of the distance computed for the matcher between s and v and the distance computed by the matcher between v</definiens>
			</definition>
			<definition id="8">
				<sentence>Animportantquestion relating toourmethod is the e ect of the number of examples on system interpretation accuracy .</sentence>
				<definiendum id="0">Animportantquestion relating toourmethod</definiendum>
				<definiens id="0">the e ect of the number of examples on system interpretation accuracy</definiens>
			</definition>
			<definition id="9">
				<sentence>The dataset consists of 8,844 utterances of which 1000were held out for testing .</sentence>
				<definiendum id="0">dataset</definiendum>
				<definiens id="0">consists of 8,844 utterances of which 1000were held out for testing</definiens>
			</definition>
</paper>

		<paper id="1614">
			<definition id="0">
				<sentence>systems , maintaining the illusion usually involves utilizing a synthetic voice to output wizard responses , often through voice distortion or a text-to-speech ( TTS ) generator .</sentence>
				<definiendum id="0">text-to-speech</definiendum>
				<definiens id="0">maintaining the illusion usually involves utilizing a synthetic voice to output wizard responses</definiens>
			</definition>
			<definition id="1">
				<sentence>Because complexity is measured only in connection with the gold standard ceteris paribus , “intellectual complexity” can be defined as : Gold Impurity Graph 0 5 10 15 20 25 30 35 40 45 50 0 10203040506070809010 Word Error Rate ( % ) T a s k Co m p l e ti o n Ra te D i ff e r e n c e ( % ) |A G| |B G| System B Mass System A Mass T a s k Co m p l e ti o n Ra te D i ff e r e n c e ( % ) ∑ = −⋅= n x xgUnIC 0 ) ( where U is the upper bound value of a performance metric , n is the upper bound value for an independent variable x , andg ( x ) isthe gold standard along that variable .</sentence>
				<definiendum id="0">n c e</definiendum>
				<definiendum id="1">U</definiendum>
				<definiendum id="2">n</definiendum>
				<definiens id="0">a s k Co m p l e ti o n Ra te D i ff e r e n c e ( % ) |A G| |B G| System B Mass System A Mass T a s k Co m p l e ti o n Ra te D i ff e r e</definiens>
				<definiens id="1">the upper bound value of a performance metric</definiens>
			</definition>
			<definition id="2">
				<sentence>Then plot the least squares distance , or ∑ − i ii xfy 2 ) ) ( ( , where f ( x ) is the fitted model , against the iteration .</sentence>
				<definiendum id="0">f ( x )</definiendum>
				<definiens id="0">the fitted model , against the iteration</definiens>
			</definition>
</paper>

		<paper id="1411">
			<definition id="0">
				<sentence>The most common approach to deriving translation lexicons from empirical data ( Catizone , Russell , and Warwick , 1989 ; Gale and Church , 1991 ; Fung , 1995 ; Kumano and Hirakawa , 1994 ; Wu and Xia , 1994 ; Melamed , 1995 ) is to use some variant of the following procedure : 1 AF Pick a good measure of the degree of association between words in language C4 BD and words in language C4 BE in aligned sentences of a parallel bilingual corpus .</sentence>
				<definiendum id="0">AF Pick</definiendum>
				<definiens id="0">a good measure of the degree of association between words in language C4 BD and words in language C4 BE in aligned sentences of a parallel bilingual corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>“Token coverage” is the proportion of the total number of occurrences of items in the text represented by the types included within the type coverage .</sentence>
				<definiendum id="0">“Token coverage”</definiendum>
				<definiens id="0">the proportion of the total number of occurrences of items in the text represented by the types included within the type coverage</definiens>
			</definition>
</paper>

		<paper id="1601">
			<definition id="0">
				<sentence>TransTool ( Nivre et al. , 1998 ) is a computer tool for transcribing spoken language in accordance with the transcription standard ( Nivre 1999a and b ) .</sentence>
				<definiendum id="0">TransTool</definiendum>
				<definiens id="0">a computer tool for transcribing spoken language in accordance with the transcription standard</definiens>
			</definition>
			<definition id="1">
				<sentence>The Corpus Browser is a web interface that makes it possible to search for words , word combinations and phrases ( as regular expressions ) in the Göteborg Spoken Language Corpus .</sentence>
				<definiendum id="0">Corpus Browser</definiendum>
				<definiens id="0">a web interface that makes it possible to search for words , word combinations and phrases ( as regular expressions ) in the Göteborg Spoken Language Corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>Tractor is a coding tool which makes it possible to create new coding schemas and annotate transcriptions .</sentence>
				<definiendum id="0">Tractor</definiendum>
				<definiens id="0">a coding tool which makes it possible to create new coding schemas and annotate transcriptions</definiens>
			</definition>
			<definition id="3">
				<sentence>MultiTool is an attempt to build such a general tool for linguistic annotation and transcribing of dialogs , as well as for browsing , searching and counting .</sentence>
				<definiendum id="0">MultiTool</definiendum>
				<definiens id="0">an attempt to build such a general tool for linguistic annotation and transcribing of dialogs</definiens>
			</definition>
			<definition id="4">
				<sentence>A feedback unit can be described as `` a maximal continuous stretch of utterance ( occurring on its own or as part of a larger utterance ) , the primary function of which is to give and/or elicit feedback concerning contact , perception , understanding and acceptance of evocative function '' ( Allwood , 1993 ) .</sentence>
				<definiendum id="0">feedback unit</definiendum>
				<definiens id="0">a maximal continuous stretch of utterance ( occurring on its own or as part of a larger utterance ) , the primary function of which is to give and/or elicit feedback concerning contact , perception , understanding and acceptance of evocative function ''</definiens>
			</definition>
			<definition id="5">
				<sentence>Interruption is a code for those overlaps which aims at or succeed in changing the topic or taking away the floor from another speaker .</sentence>
				<definiendum id="0">Interruption</definiendum>
				<definiens id="0">a code for those overlaps which aims at or succeed in changing the topic or taking away the floor from another speaker</definiens>
			</definition>
			<definition id="6">
				<sentence>( B ) Intended recipient : This type of coding has four self explanatory values ( i ) particular participant ( ii ) particular group of participants ( iii ) all participants ( iv ) no other participant ( talking to oneself ) .</sentence>
				<definiendum id="0">B ) Intended recipient</definiendum>
				<definiens id="0">i ) particular participant ( ii ) particular group of participants ( iii ) all participants ( iv ) no other participant ( talking to oneself )</definiens>
			</definition>
			<definition id="7">
				<sentence>OCM means ”Own Communication Management” and stands for processes that speakers use to regulate their own contributions to communicative interaction .</sentence>
				<definiendum id="0">OCM</definiendum>
				<definiens id="0">”Own Communication Management” and stands for processes that speakers use to regulate their own contributions to communicative interaction</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>Summarization `` conceptual '' or `` content '' level diff ( email , documents , patents ) Query dependent , Multiple perspective Summarization ( representation and output ) /\ /\ /\ || || || entity discourse co-ref Multilingual interlingua deeply annotated data + ML user appropriate translations English Interlingua Multimedia 21 Feb 2007 21:45Generated by HTML_ToPDF at rustyparts.com 3 Notes personalized content based news multimedia I ( maps , gesture ) /\ || multimedia data and annotation ( images , maps , video , medical ) Standards Process Reusable interchangeable modules ( e.g. , POS , NE ) Data ( XML , text encoding , W3C ) NLP Robust , deep language processing ( e.g. LFG parsing which is fast but inaccurate still ) KM/Information Integration Integrated mining , query of mail , DB , process knowledge CORE ENABLING RESOURCES ( intelligent ) text annotation ( feeds all areas ) large annotated corpora 21 Feb 2007 21:45Generated by HTML_ToPDF at rustyparts.com 4 Notes</sentence>
				<definiendum id="0">Summarization `` conceptual</definiendum>
				<definiens id="0">'' or `` content '' level diff ( email , documents , patents ) Query dependent , Multiple perspective Summarization ( representation and output ) /\ /\ /\ || || || entity discourse co-ref Multilingual interlingua deeply annotated data + ML user appropriate translations English Interlingua Multimedia 21 Feb 2007 21:45Generated by HTML_ToPDF at rustyparts.com 3 Notes personalized content based news multimedia I/O ( maps , gesture ) /\ || multimedia data and annotation ( images , maps , video , medical ) Standards Process Reusable interchangeable modules ( e.g. , POS , NE ) Data ( XML , text encoding , W3C ) NLP Robust , deep language processing ( e.g. LFG parsing which is fast but inaccurate still ) KM/Information Integration Integrated mining , query of mail , DB , process knowledge CORE ENABLING RESOURCES ( intelligent</definiens>
			</definition>
</paper>

		<paper id="1405">
</paper>

		<paper id="1507">
			<definition id="0">
				<sentence>The ISLE project is a continuation of the long standing EAGLES initiative , carried out under the Human Language Technology ( HLT ) programme in collaboration between American and European groups in the framework of the EU-US International Research Co-operation , supported by NSF and EC .</sentence>
				<definiendum id="0">ISLE project</definiendum>
				<definiens id="0">a continuation of the long standing EAGLES initiative , carried out under the Human Language Technology ( HLT ) programme in collaboration between American and European groups in the framework of the EU-US International Research Co-operation , supported by NSF and EC</definiens>
			</definition>
			<definition id="1">
				<sentence>The aim of EAGLES/ISLE is thus to accelerate the provision of standards , common guidelines , best practice recommendations for : • very large-scale language resources ( such as text corpora , computational lexicons , speech corpora ( Gibbon et al. , 1997 ) , multimodal resources ) ; • means of manipulating such knowledge , via computational linguistic formalisms , markup languages and various software tools ; • means of assessing and evaluating resources , tools and products ( EAGLES , 1996 ) .</sentence>
				<definiendum id="0">EAGLES/ISLE</definiendum>
				<definiens id="0">such as text corpora , computational lexicons , speech corpora ( Gibbon et al. , 1997 ) , multimodal resources</definiens>
			</definition>
			<definition id="2">
				<sentence>Work The current ISLE project ( see http : //www.ilc.pi.cnr.it/EAGLES96/isle/ISLE_H ome_Page .</sentence>
				<definiendum id="0">see http</definiendum>
				<definiens id="0">//www.ilc.pi.cnr.it/EAGLES96/isle/ISLE_H ome_Page</definiens>
			</definition>
			<definition id="3">
				<sentence>• For evaluation , ISLE is working on : quality models for machine translation systems ; and maintenance of previous guidelines in an ISO based framework ( ISO 9126 , ISO 14598 ) .</sentence>
				<definiendum id="0">ISLE</definiendum>
				<definiens id="0">working on : quality models for machine translation systems</definiens>
			</definition>
			<definition id="4">
				<sentence>Multilingual ISLE Lexical Entry The main goal of the CLWG is the definition of a Multilingual ISLE Lexical Entry ( henceforth MILE ) .</sentence>
				<definiendum id="0">CLWG</definiendum>
			</definition>
			<definition id="5">
				<sentence>Such an architecture has been proven useful in previous EAGLES work , e.g in the EAGLES morphosyntactic recommendations ( Monachini and Calzolari , 1996 ) , which embody three levels of linguistic information : obligatory , recommended and optional ( optional splits furthermore into language independent and language dependent ) .</sentence>
				<definiendum id="0">EAGLES morphosyntactic recommendations</definiendum>
				<definiendum id="1">optional</definiendum>
				<definiens id="0">( optional splits furthermore into language independent and language dependent )</definiens>
			</definition>
			<definition id="6">
				<sentence>To give a concrete example , almost all theoretical frameworks claim that lexical items have a complex semantic organization , but some of them try to describe it through a multidimensional internal structure ( cf. the qualia structure in the Generative Lexicon , Pustejovsky 1995 ) , others by specifying a network of semantic relations ( cf. WordNet , Miller et al. 1990 ) , and others in terms of argumental frames ( cf FrameNet , Baker et al. 1998 ; Lexical Conceptual Structures , Jackendoff 1992 ; etc. ) .</sentence>
				<definiendum id="0">cf FrameNet</definiendum>
				<definiendum id="1">Lexical Conceptual Structures</definiendum>
				<definiens id="0">the qualia structure in the Generative Lexicon</definiens>
			</definition>
			<definition id="7">
				<sentence>The CLWG agreed to focus on two major broad categories of application : machine translation ( MT ) and cross-lingual information retrieval ( CLIR ) .</sentence>
				<definiendum id="0">CLWG agreed</definiendum>
				<definiendum id="1">CLIR</definiendum>
				<definiens id="0">to focus on two major broad categories of application : machine translation ( MT ) and cross-lingual information retrieval</definiens>
			</definition>
			<definition id="8">
				<sentence>The MILE is a modular architecture for the representation of multilingual lexical data , and aims at becoming a common parlance for the representation and encoding of lexical data .</sentence>
				<definiendum id="0">MILE</definiendum>
			</definition>
</paper>

		<paper id="1617">
			<definition id="0">
				<sentence>Vocalist : A robust , portable spoken language dialogue system for telephone applications .</sentence>
				<definiendum id="0">Vocalist</definiendum>
			</definition>
</paper>

		<paper id="1502">
			<definition id="0">
				<sentence>Registry The Natural Language Software Registry ( NLSR ) is a concise summary of the capabilities and sources of a large amount of natural language processing ( NLP ) software available to the NLP community .</sentence>
				<definiendum id="0">Natural Language Software Registry</definiendum>
			</definition>
			<definition id="1">
				<sentence>From the side of the Language Engineering there are initiatives for describing standards and ( Calzolari et al. , 2001 ) present such an initiative , the ISLE project , which is the continuation of the EAGLES initiative .</sentence>
				<definiendum id="0">ISLE project</definiendum>
				<definiens id="0">the continuation of the EAGLES initiative</definiens>
			</definition>
			<definition id="2">
				<sentence>While ( Calzolari et al. , 2001 ) concentrate on the description of the task of the ISLE computational lexicon working group and address the topic of metadata for encoding multilingual lexical resources , ( Broeder and Wittenburg , 2001 ) presents the work of the ISLE Metadata initiative ( IMDI ) , which is directly relevant for the topic addressed here .</sentence>
				<definiendum id="0">IMDI</definiendum>
				<definiens id="0">concentrate on the description of the task of the ISLE computational lexicon working group and address the topic of metadata for encoding multilingual lexical resources</definiens>
			</definition>
</paper>

		<paper id="0510">
</paper>

		<paper id="0517">
</paper>

		<paper id="1604">
			<definition id="0">
				<sentence>Examples are the following : ( 1 ) the sum of 3 and 4 ( 2 ) the first thing that comes to mind when one thinks of Aalborg ( 3 ) the April 2001 unemployment figures for Germany ( 4 ) the beauty of a sunset ( 5 ) the H2O molecule The presuppositions that are triggered by the use of such expressions are reasonably clear : ( 1 ' ) There is a unique thing that is the sum of 3 and 4 .</sentence>
				<definiendum id="0">Examples</definiendum>
				<definiens id="0">comes to mind when one thinks of Aalborg ( 3 ) the April 2001 unemployment figures for Germany</definiens>
			</definition>
</paper>

		<paper id="0723">
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>Challenges Knowledge Management is the field of enhancing organizational performance through organizational knowledge sharing , learning , and application of expertise .</sentence>
				<definiendum id="0">Challenges Knowledge Management</definiendum>
				<definiens id="0">the field of enhancing organizational performance through organizational knowledge sharing , learning , and application of expertise</definiens>
			</definition>
			<definition id="1">
				<sentence>BNN applies speech , language , and image processing methods to segment , extract , and summarize broadcast news sources to enable personalized and targeted search of the news .</sentence>
				<definiendum id="0">BNN</definiendum>
				<definiens id="0">applies speech , language , and image processing methods to segment , extract , and summarize broadcast news sources to enable personalized and targeted search of the news</definiens>
			</definition>
			<definition id="2">
				<sentence>A grand challenge is the automated generation of coordinated speech , natural language , gesture , animation , non-speech audio , generation , possibly delivered via interactive , animated lifelike agents .</sentence>
				<definiendum id="0">grand challenge</definiendum>
			</definition>
			<definition id="3">
				<sentence>Human language technology promises to deliver great value to the challenge of knowledge management .</sentence>
				<definiendum id="0">Human language technology</definiendum>
			</definition>
</paper>

		<paper id="0513">
			<definition id="0">
				<sentence>A multiword unit ( MWU ) is a connected collocation : a sequence of neighboring words “whose exact and unambiguous meaning or connotation can not be derived from the meaning or connotation of its components” ( Choueka , 1988 ) .</sentence>
				<definiendum id="0">multiword unit</definiendum>
				<definiendum id="1">MWU</definiendum>
			</definition>
			<definition id="1">
				<sentence>LSA is a technique which automatically induces semantic relationships between words .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">a technique which automatically induces semantic relationships between words</definiens>
			</definition>
			<definition id="2">
				<sentence>In the table , f and P signify frequency and probability XX of a word X. A variable XY indicates a word bigram and indicates its expected frequency at random .</sentence>
				<definiendum id="0">probability XX</definiendum>
				<definiendum id="1">variable XY</definiendum>
				<definiens id="0">indicates a word bigram and indicates its expected frequency at random</definiens>
			</definition>
			<definition id="3">
				<sentence>WordNet includes few proper noun MWUs .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">includes few proper noun MWUs</definiens>
			</definition>
			<definition id="4">
				<sentence>Non-compositionality is a key component of valid MWUs , so we may desire to emphasize n-grams that are semantically non-compositional .</sentence>
				<definiendum id="0">Non-compositionality</definiendum>
				<definiens id="0">a key component of valid MWUs , so we may desire to emphasize n-grams that are semantically non-compositional</definiens>
			</definition>
			<definition id="5">
				<sentence>“WordNet : An on-line lexical database , ” International Journal of Lexicography , 3 ( 4 ) .</sentence>
				<definiendum id="0">“WordNet</definiendum>
				<definiens id="0">An on-line lexical database , ” International Journal of Lexicography</definiens>
			</definition>
			<definition id="6">
				<sentence>Useg : A Retargetable word segmentation procedure for information retrieval .</sentence>
				<definiendum id="0">Useg</definiendum>
			</definition>
			<definition id="7">
				<sentence>Selectional constraints : an information-theoretic model and its computational realization .</sentence>
				<definiendum id="0">Selectional constraints</definiendum>
				<definiens id="0">an information-theoretic model and its computational realization</definiens>
			</definition>
</paper>

		<paper id="1629">
			<definition id="0">
				<sentence>A spoken dialogue system determines user requests from user utterances .</sentence>
				<definiendum id="0">spoken dialogue system</definiendum>
				<definiens id="0">determines user requests from user utterances</definiens>
			</definition>
</paper>

		<paper id="0519">
			<definition id="0">
				<sentence>MBT : A memory-based part of speech tagger-generator .</sentence>
				<definiendum id="0">MBT</definiendum>
				<definiens id="0">A memory-based part of speech tagger-generator</definiens>
			</definition>
</paper>

		<paper id="0712">
			<definition id="0">
				<sentence>Local Structural Context ( LSC ) is ( partial ) information about the immediate neighbourhood of a phrase in a parse .</sentence>
				<definiendum id="0">Local Structural Context</definiendum>
			</definition>
			<definition id="1">
				<sentence>ALLiS ( ( D´ejean , 2000b ) , ( D´ejean , 2000c ) ) is a inductive rule-based system using a traditional general-to-specific approach ( Mitchell , 1997 ) .</sentence>
				<definiendum id="0">ALLiS</definiendum>
			</definition>
			<definition id="2">
				<sentence>ALLiS uses data encoded in XML , and also learns rules in XML .</sentence>
				<definiendum id="0">ALLiS</definiendum>
				<definiens id="0">uses data encoded in XML , and also learns rules in XML</definiens>
			</definition>
			<definition id="3">
				<sentence>Labelled SOM and Memory Based Learning ( LSOMMBL ) is a neurally inspired technique which incorporates a modified self-organising map ( SOM , also known as a ‘Kohonen Map’ ) in memory-based learning to select a subset of the training data for comparison with novel items .</sentence>
				<definiendum id="0">Labelled SOM</definiendum>
				<definiendum id="1">Memory Based Learning ( LSOMMBL )</definiendum>
			</definition>
			<definition id="4">
				<sentence>A mixture of simple features ( consisting of one of the mentioned information sources ) and complex features ( combinations thereof ) were used .</sentence>
				<definiendum id="0">mixture of simple features</definiendum>
				<definiens id="0">consisting of one of the mentioned information sources ) and complex features ( combinations thereof ) were used</definiens>
			</definition>
			<definition id="5">
				<sentence>Inductive Logic Programming ( ILP ) Aleph is an ILP machine learning system that searches for a hypothesis , given positive ( and , if available , negative ) data in the form of ground Prolog terms and background knowledge ( prior knowledge made available to the learning algorithm ) in the form of Prolog predicates .</sentence>
				<definiendum id="0">Inductive Logic Programming</definiendum>
				<definiendum id="1">ILP ) Aleph</definiendum>
			</definition>
			<definition id="6">
				<sentence>Minimum Description Length ( MDL ) Estimation using the minimum description length principle involves finding a model which not only ‘explains’ the training material well , but also is compact .</sentence>
				<definiendum id="0">Minimum Description Length</definiendum>
				<definiendum id="1">minimum description length principle</definiendum>
			</definition>
			<definition id="7">
				<sentence>We use a probabilistic grammatical algorithm , the DDSM algorithm ( Thollard , 2001 ) , for learning automata that provide the probability of an item given the previous ones .</sentence>
				<definiendum id="0">DDSM algorithm</definiendum>
				<definiens id="0">a probabilistic grammatical algorithm , the</definiens>
			</definition>
			<definition id="8">
				<sentence>Memory-based learning methods store all training data and classify test data items by giving them the classification of the training data items which are most similar .</sentence>
				<definiendum id="0">Memory-based learning methods</definiendum>
			</definition>
			<definition id="9">
				<sentence>They are classifiers which means that they assign phrase classes such as I ( inside a phrase ) , B ( at the beginning of a phrase ) and O ( outside a phrase ) to words .</sentence>
				<definiendum id="0">I</definiendum>
				<definiendum id="1">B</definiendum>
				<definiendum id="2">O</definiendum>
				<definiens id="0">inside a phrase ) ,</definiens>
				<definiens id="1">at the beginning of a phrase ) and</definiens>
			</definition>
			<definition id="10">
				<sentence>TOTPRECISION uses classifier weights based on their accuracy .</sentence>
				<definiendum id="0">TOTPRECISION</definiendum>
			</definition>
			<definition id="11">
				<sentence>TAGPRECISION applies classification weights based on the accuracy of the classifier for that classification .</sentence>
				<definiendum id="0">TAGPRECISION</definiendum>
				<definiens id="0">applies classification weights based on the accuracy of the classifier for that classification</definiens>
			</definition>
			<definition id="12">
				<sentence>PRECISION-RECALL uses classification weights that combine the precision of the classification with the recall of the competitors .</sentence>
				<definiendum id="0">PRECISION-RECALL</definiendum>
			</definition>
			<definition id="13">
				<sentence>And finally , TAGPAIR uses classification pair weights based on the probability of a classification for some predicted classification pair ( van Halteren et al. , 1998 ) .</sentence>
				<definiendum id="0">TAGPAIR</definiendum>
			</definition>
			<definition id="14">
				<sentence>The NP chunking task is the specialisation of the chunking task in which only base noun phrases need to be detected .</sentence>
				<definiendum id="0">NP chunking task</definiendum>
				<definiens id="0">the specialisation of the chunking task in which only base noun phrases need to be detected</definiens>
			</definition>
</paper>

		<paper id="0813">
			<definition id="0">
				<sentence>Indicative multidocument summaries are an important way of helping a user discriminate between several documents returned by a search engine .</sentence>
				<definiendum id="0">Indicative multidocument summaries</definiendum>
			</definition>
			<definition id="1">
				<sentence>CENTRIFUSER is the indicative multi-document summarization system that we have developed to operate on domainand genre-specific documents .</sentence>
				<definiendum id="0">CENTRIFUSER</definiendum>
				<definiens id="0">the indicative multi-document summarization system that we have developed to operate on domainand genre-specific documents</definiens>
			</definition>
			<definition id="2">
				<sentence>During content planning , the system decides what information to convey based on the calculated information from the previous stage .</sentence>
				<definiendum id="0">content planning</definiendum>
			</definition>
			<definition id="3">
				<sentence>Document categories like prototypical whose salient feature is their high ratio of relevant topics , are considered more important than document categories that are defined by their ratio of intricate or irrelevant topics ( e.g. deep ) .</sentence>
				<definiendum id="0">Document categories</definiendum>
				<definiens id="0">ratio of intricate or irrelevant topics ( e.g. deep )</definiens>
			</definition>
			<definition id="4">
				<sentence>( S1/description+setElements ( V1 : value ‘‘be available’’ ) ( NP1/atypical : value ‘‘more information on additional topics which are not included in the extract’’ ) ( NP2/setElements : value ‘‘files ( The AMA guide and CU Guide ) ’’ ) ) ( S2/hasTopics ( V1 : value ‘‘include’’ ) ( NP1/atypicalTopics : value ‘‘topics’’ ) ( NP2/topicList : value ‘‘definition and what are the risks ? ’’ ) )</sentence>
				<definiendum id="0">S1/description+setElements</definiendum>
				<definiens id="0">‘‘more information on additional topics which are not included in the extract’’ ) ( NP2/setElements : value ‘‘files ( The AMA guide and CU Guide ) ’’</definiens>
			</definition>
			<definition id="5">
				<sentence>Linguistic realization takes the sentence plan and produces actual text by solving the remaining morphology and syntactic problems .</sentence>
				<definiendum id="0">Linguistic realization</definiendum>
				<definiens id="0">takes the sentence plan and produces actual text by solving the remaining morphology and syntactic problems</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>An ADVICE project objective is to build a generic language processing component following a language engineering approach , that is , to achieve maintainability , time optimization , robustness , flexibility , domain adaptability and generality .</sentence>
				<definiendum id="0">ADVICE project objective</definiendum>
				<definiens id="0">to build a generic language processing component following a language engineering approach , that is , to achieve maintainability , time optimization , robustness , flexibility , domain adaptability and generality</definiens>
			</definition>
			<definition id="1">
				<sentence>The Interface Agent includes the G49G47G3 G44G81G87G72G85G83G85G72G87G72G85G3 G68G81G71G3 G42G72G81G72G85G68G87G82G85 component that is in charge of analyzing the user utterances as well as of generating the appropriate answers according to the dialogue .</sentence>
				<definiendum id="0">Interface Agent</definiendum>
				<definiens id="0">includes the G49G47G3 G44G81G87G72G85G83G85G72G87G72G85G3 G68G81G71G3 G42G72G81G72G85G68G87G82G85 component that is in charge of analyzing the user utterances as well as of generating the appropriate answers according to the dialogue</definiens>
			</definition>
			<definition id="2">
				<sentence>The information gathering for the G49G47 G44G81G87G72G85G83G85G72G87G72G85G3 G68G81G71G3 G42G72G81G72G85G68G87G82G85 involves different sources : dialogues collected from users , the database of products and users and common sense about human-computer interaction .</sentence>
				<definiendum id="0">G49G47 G44G81G87G72G85G83G85G72G87G72G85G3 G68G81G71G3 G42G72G81G72G85G68G87G82G85</definiendum>
				<definiens id="0">involves different sources : dialogues collected from users</definiens>
			</definition>
			<definition id="3">
				<sentence>This had direct repercussions in the set of contextual rules ( the rules that the tagger uses to select the correct tag for a word considering the context in which it appears ) which were adapted to fit the domain peculiarities detected in the patterns , as shown in figure 4 .</sentence>
				<definiendum id="0">contextual rules</definiendum>
				<definiens id="0">word considering the context in which it appears ) which were adapted to fit the domain peculiarities detected in the patterns</definiens>
			</definition>
			<definition id="4">
				<sentence>The G79G72G91G76G70G82G81 is a long list of words with one or more possible tags associated .</sentence>
				<definiendum id="0">G79G72G91G76G70G82G81</definiendum>
				<definiens id="0">a long list of words with one or more possible tags associated</definiens>
			</definition>
</paper>

		<paper id="1515">
			<definition id="0">
				<sentence>Annotation graphs provide an efficient and expressive data model for linguistic annotations of time-series data .</sentence>
				<definiendum id="0">Annotation graphs</definiendum>
			</definition>
			<definition id="1">
				<sentence>Annotation graphs ( AGs ) provide an efficient and expressive data model for linguistic annotations of time-series data ( Bird and Liberman , Figure 1 : Architecture for Annotation Systems 2001 ) .</sentence>
				<definiendum id="0">Annotation graphs ( AGs</definiendum>
			</definition>
			<definition id="2">
				<sentence>Thus , the identifier encodes the unique membership of an object in the containing objects .</sentence>
				<definiendum id="0">identifier</definiendum>
				<definiens id="0">encodes the unique membership of an object in the containing objects</definiens>
			</definition>
			<definition id="3">
				<sentence>The transcription editor is an annotation component which is specialized for a particular coding task .</sentence>
				<definiendum id="0">transcription editor</definiendum>
				<definiens id="0">an annotation component which is specialized for a particular coding task</definiens>
			</definition>
			<definition id="4">
				<sentence>The Annotation Graph Toolkit , version 1.0 , contains a complete implementation of the annotation graph model , import filters for several formats , loading/storing data to an annotation server ( MySQL ) , application programming interfaces in C++ and Tcl/tk , and example annotation tools for dialogue , ethology and interlinear text .</sentence>
				<definiendum id="0">Annotation Graph Toolkit</definiendum>
				<definiens id="0">contains a complete implementation of the annotation graph model , import filters for several formats , loading/storing data to an annotation server ( MySQL ) , application programming interfaces in C++ and Tcl/tk , and example annotation tools for dialogue , ethology and interlinear text</definiens>
			</definition>
</paper>

		<paper id="0701">
</paper>

		<paper id="0715">
</paper>

		<paper id="0509">
			<definition id="0">
				<sentence>LASSO : A toolf for surfing the answer net .</sentence>
				<definiendum id="0">LASSO</definiendum>
			</definition>
</paper>

		<paper id="1619">
</paper>

		<paper id="1310">
</paper>

		<paper id="1306">
			<definition id="0">
				<sentence>A corollary is the partition of the set of particles traditionally classified as temporal locating into two sets : the truly locating ones – like in , during , since or until – and those that are mere heads of ( structurally complex ) time-denoting expressions – like before , after , between , when , ago , or from .</sentence>
				<definiendum id="0">corollary</definiendum>
				<definiens id="0">the partition of the set of particles traditionally classified as temporal locating into two sets : the truly locating ones – like in , during , since or until – and those that are mere heads of ( structurally complex ) time-denoting expressions – like before</definiens>
			</definition>
			<definition id="1">
				<sentence>A second ( larger ) group of apparently ambivalent phrases includes expressions that systematically occur without any ( explicit ) temporal locating particle in adverbial position .</sentence>
				<definiendum id="0">second</definiendum>
			</definition>
			<definition id="2">
				<sentence>Phrases like English X ago , X from COMPL , X before/after COMPL , and Portuguese há X , de COMPL a X , X antes/depois ( de ) COMPL ( where X is either a predicate of amounts of time – like two hours – or a quantified predicate of times , or eventualities , like three weekends , or three classes ) : ( 22 ) a. Ana will get married two months from now .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">either a predicate of amounts of time – like two hours – or a quantified predicate of times , or eventualities</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>SHOE : A knowledge representation language for internet applications .</sentence>
				<definiendum id="0">SHOE</definiendum>
			</definition>
			<definition id="1">
				<sentence>8 Douglas B. Lenat , CYC : a large-scale investment in knowledge infrastructure , Communications of the ACM , v.38 n.11 , p.33-38 , Nov. 1995 9 Beth Levin .</sentence>
				<definiendum id="0">CYC</definiendum>
				<definiens id="0">a large-scale investment in knowledge infrastructure</definiens>
			</definition>
</paper>

		<paper id="0708">
</paper>

		<paper id="0807">
			<definition id="0">
				<sentence>Hunter-Gatherer : applying constraint satisfaction , branch-and-bound solution synthesis to computational semantics .</sentence>
				<definiendum id="0">Hunter-Gatherer</definiendum>
				<definiens id="0">applying constraint satisfaction , branch-and-bound solution synthesis to computational semantics</definiens>
			</definition>
</paper>

		<paper id="1623">
			<definition id="0">
				<sentence>Discourse markers can be defined as follows : elements whose original semantic meaning tends to decrease and their use in spoken discourse becomes more pragmatic and indicative of discourse structuring are discourse markers .</sentence>
				<definiendum id="0">Discourse markers</definiendum>
			</definition>
			<definition id="1">
				<sentence>60 , 23 male and 37 female , Taipei residents of Taipei City who volunteered to participate in the project were recorded in pairs .</sentence>
				<definiendum id="0">Taipei</definiendum>
				<definiens id="0">residents of Taipei City who volunteered to participate in the project were recorded in pairs</definiens>
			</definition>
</paper>

		<paper id="0902">
			<definition id="0">
				<sentence>For spoken dialog systems , maintaining the illusion usually involves utilizing a synthetic voice to output wizard responses , often through voice distortion or a text-to-speech ( TTS ) generator .</sentence>
				<definiendum id="0">text-to-speech</definiendum>
				<definiens id="0">maintaining the illusion usually involves utilizing a synthetic voice to output wizard responses</definiens>
			</definition>
</paper>

		<paper id="1613">
</paper>

		<paper id="0720">
			<definition id="0">
				<sentence>In this paper we present a system CLL which aims to learn natural language syntax in a way that is both computationally effective and psychologically plausible .</sentence>
				<definiendum id="0">system CLL</definiendum>
				<definiens id="0">aims to learn natural language syntax in a way that is both computationally effective and psychologically plausible</definiens>
			</definition>
			<definition id="1">
				<sentence>Bickerton ( Bickerton , 1984 ) found that the creoles , developing from syntactically impoverished language examples as they do , actually contain syntactic structures not available to the learners from their pigeon environment .</sentence>
				<definiendum id="0">Bickerton</definiendum>
				<definiens id="0">developing from syntactically impoverished language examples as they do , actually contain syntactic structures not available to the learners from their pigeon environment</definiens>
			</definition>
			<definition id="2">
				<sentence>Using CG means that we are aiming to build a lexicon that contains the required CG category or categories for each word , which defines the syntactic role or roles of that word .</sentence>
				<definiendum id="0">CG</definiendum>
				<definiens id="0">defines the syntactic role or roles of that word</definiens>
			</definition>
			<definition id="3">
				<sentence>The Corpus The corpus is a set of positive examples represented in Prolog as facts containing a list of words e.g. ex ( [ mary , loved , a , computer ] ) .</sentence>
				<definiendum id="0">Corpus The corpus</definiendum>
				<definiens id="0">facts containing a list of words e.g. ex</definiens>
			</definition>
			<definition id="4">
				<sentence>Where Word is a word , Category is a Prolog representation of the CG category assigned to that word and Frequency is the number of times this category has been assigned to this word up to the current point in the learning process , or in the case of the initial closed-class words a probability distribution is predefined .</sentence>
				<definiendum id="0">Word</definiendum>
				<definiendum id="1">Category</definiendum>
				<definiendum id="2">Frequency</definiendum>
				<definiens id="0">a word ,</definiens>
			</definition>
			<definition id="5">
				<sentence>The most probable annotation of the corpus is the set of top-most parses after the final parse selection .</sentence>
				<definiendum id="0">most probable annotation of the corpus</definiendum>
				<definiens id="0">the set of top-most parses after the final parse selection</definiens>
			</definition>
			<definition id="6">
				<sentence>Wolff ( Wolff , 1987 ) using a similar ( if rather more empiricist ) setting also uses syntactic analysis and compression to build grammars .</sentence>
				<definiendum id="0">Wolff</definiendum>
			</definition>
			<definition id="7">
				<sentence>The learning appears to be supervised and occurs over parts-of-speech rather than over the actual words .</sentence>
				<definiendum id="0">learning appears</definiendum>
				<definiens id="0">to be supervised and occurs over parts-of-speech rather than over the actual words</definiens>
			</definition>
			<definition id="8">
				<sentence>Two measures are used to evaluate the parses : lexical accuracy , which is the percentage of correctly tagged words compared to the extracted gold standard corpus ( Watkinson and Manandhar , 2001 ) and average crossing bracket rate ( CBR ) ( Goodman , 1996 ) .</sentence>
				<definiendum id="0">lexical accuracy</definiendum>
				<definiendum id="1">CBR</definiendum>
				<definiens id="0">the percentage of correctly tagged words compared to the extracted gold standard corpus</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>An ontology is a knowledge base with information about concepts existing in the world or domain , their properties , and how they relate to each other .</sentence>
				<definiendum id="0">ontology</definiendum>
			</definition>
			<definition id="1">
				<sentence>To solve this problem , we propose a semi-automatic ontology construction method , which takes full advantage of already existing knowledge resources and practical usages in large corpora .</sentence>
				<definiendum id="0">semi-automatic ontology construction method</definiendum>
				<definiens id="0">takes full advantage of already existing knowledge resources and practical usages in large corpora</definiens>
			</definition>
			<definition id="2">
				<sentence>We define a total of 30 semantic relation types for WSD by referring mainly to the Sejong electronic dictionary and the Mikrokosmos ontology ( Mahesh , 1996 ) , as shown in Table 1 .</sentence>
				<definiendum id="0">Mikrokosmos ontology</definiendum>
				<definiens id="0">a total of 30 semantic relation types for WSD by referring mainly to the Sejong electronic dictionary</definiens>
			</definition>
			<definition id="3">
				<sentence>Sematic relation types in the LIP ontology Types Relation Lists Taxonomic relation is-a Case relation agent , theme , experiencer , accompanier , instrument , location , source , destination , reason , appraisee , criterion , degree , recipient Other Semantic relation has-member , has-element , contains , material-of , headedby , operated-by , controls , owner-of , represents , symbol-of , name-of , producer-of , composer-of , inventor-of , make , measured-in inserted for the readability and convenience of the ontology developer .</sentence>
				<definiendum id="0">Sematic relation</definiendum>
				<definiens id="0">types in the LIP ontology Types Relation Lists Taxonomic relation is-a Case relation agent , theme , experiencer , accompanier , instrument , location , source , destination , reason , appraisee , criterion , degree , recipient Other Semantic relation has-member , has-element , contains , material-of , headedby , operated-by , controls , owner-of , represents , symbol-of , name-of , producer-of , composer-of , inventor-of , make , measured-in inserted for the readability and convenience of the ontology developer</definiens>
			</definition>
			<definition id="4">
				<sentence>For the automatic construction of a sense-tagged corpus , we used the COBALT-J/K , which is a high-quality practical MT system developed by POSTECH in 1996 .</sentence>
				<definiendum id="0">COBALT-J/K</definiendum>
				<definiens id="0">a high-quality practical MT system developed by POSTECH in 1996</definiens>
			</definition>
			<definition id="5">
				<sentence>To measure concept association , we use an association ratio based on the information theoretic concept of mutual information ( MI ) , which is a natural measure of the dependence 071 ( life ) 072 ( upbringing ) 073 ( disease ) YES NO agent / theme 295 ( influence ) 370 ( giving &amp; receiving ) YES NO recipient 49 ( joy &amp; sorrow ) 62 ( figure ) YES NO experiencer 201 ( stable ) 202 ( vibration ) 1 ( natural condition ) 07 ( physiology ) YES NO theme 2 ( change ) 3 ( action ) YES NO agent Manual mapping by human intuition Figure 5 .</sentence>
				<definiendum id="0">theme 295</definiendum>
				<definiens id="0">a natural measure of the dependence 071 ( life ) 072 ( upbringing ) 073 ( disease ) YES NO agent /</definiens>
			</definition>
			<definition id="6">
				<sentence>Figure 7 shows the construction process for training data in the form of &lt; SC ( governer ) , SR , DC ( dependent ) , frequency &gt; and the calculation of MI between the LIP ontology concepts .</sentence>
				<definiendum id="0">construction process</definiendum>
				<definiens id="0">for training data in the form of &lt; SC ( governer ) , SR , DC ( dependent ) , frequency &gt; and the calculation of MI between the LIP ontology concepts</definiens>
			</definition>
			<definition id="7">
				<sentence>Final relation instances in the LIP ontology Types Number Taxonomic relations 1,100 Case relations 112,746 Other semantic relations 2,093 Total 115,939 One Ontology Concept to Other Concept If we regard MI as a weight between ontology concepts , we can treat the LIP ontology as a graph with weighted edges .</sentence>
				<definiendum id="0">LIP ontology</definiendum>
				<definiens id="0">a weight between ontology concepts</definiens>
			</definition>
			<definition id="8">
				<sentence>The main reason for these results is that , in the absence of secured dictionary information ( see Figure 7 ) about an ambiguous word , the ontology provides a generalized case frame ( i.e. semantic restriction ) by the concept code of the word .</sentence>
				<definiendum id="0">ontology</definiendum>
				<definiens id="0">provides a generalized case frame ( i.e. semantic restriction ) by the concept code of the word</definiens>
			</definition>
			<definition id="9">
				<sentence>Therefore , our LIP ontology is a language independent and practical knowledge base .</sentence>
				<definiendum id="0">LIP ontology</definiendum>
				<definiens id="0">a language independent and practical knowledge base</definiens>
			</definition>
</paper>

		<paper id="0709">
			<definition id="0">
				<sentence>itive class ) is a lot higher than the number of false positives ( results over the negative class ) .</sentence>
				<definiendum id="0">itive class )</definiendum>
				<definiens id="0">a lot higher than the number of false positives ( results over the negative class )</definiens>
			</definition>
</paper>

		<paper id="1612">
			<definition id="0">
				<sentence>The HTC is a collection of 577 short texts descriptive of the city of Heidelberg , which have been collected at our lab for a tourist information system in the course of the DeepMap project ( Malaka &amp; Zipf , 2000 ) .</sentence>
				<definiendum id="0">HTC</definiendum>
				<definiens id="0">a collection of 577 short texts descriptive of the city of Heidelberg</definiens>
			</definition>
			<definition id="1">
				<sentence>Finally , an annotation tool is required which implements the annotation scheme in a robust and efficient way .</sentence>
				<definiendum id="0">annotation tool</definiendum>
				<definiens id="0">implements the annotation scheme in a robust and efficient way</definiens>
			</definition>
			<definition id="2">
				<sentence>Consider the NP [ das Bauwerk ] in the following example , which denotes a super-concept of [ dem Geb¨aude ] .</sentence>
				<definiendum id="0">Consider the NP [</definiendum>
				<definiens id="0">das Bauwerk ] in the following example</definiens>
			</definition>
			<definition id="3">
				<sentence>Since bridging ( as we define it ) is a relation not between lexical items , but between extra-linguistic entities , and since cospecification is a transitive relation , a bridging relation can be sufficiently expressed by specifying any of the candidates .</sentence>
				<definiendum id="0">cospecification</definiendum>
				<definiens id="0">a relation not between lexical items , but between extra-linguistic entities</definiens>
			</definition>
			<definition id="4">
				<sentence>The Discourse Tagging Tool ( DTTool ) ( Aone &amp; Bennett , 1994 ) is a Tcl/Tk program for the annotation and display of antecedent-anaphor relations in SGMLencoded multilingual texts .</sentence>
				<definiendum id="0">Discourse Tagging Tool</definiendum>
				<definiens id="0">a Tcl/Tk program for the annotation and display of antecedent-anaphor relations in SGMLencoded multilingual texts</definiens>
			</definition>
			<definition id="5">
				<sentence>The Alembic Workbench 4 is an annotation tool which , among other tasks , directly supports cospecification annotation .</sentence>
				<definiendum id="0">Alembic Workbench 4</definiendum>
				<definiens id="0">an annotation tool which , among other tasks</definiens>
			</definition>
			<definition id="6">
				<sentence>The formal structure of a text is described by the following DTD : &lt; ! ELEMENT text ( ( headline ? ) , ( ( paragraph+ ) | ( sentence+ ) ) ) &gt; &lt; ! ELEMENT headline ( sentence* ) &gt; &lt; ! ELEMENT paragraph ( sentence* ) &gt; &lt; ! ATTLIST paragraph id ID # REQUIRED &gt; &lt; ! ELEMENT sentence ( EMPTY ) &gt; &lt; ! ATTLIST sentence id ID # REQUIRED &gt; &lt; ! ATTLIST sentence span CDATA # REQUIRED &gt; In pragmatic terms , on the other hand , a text can be regarded as a discourse , consisting of a series of discourse segments .</sentence>
				<definiendum id="0">ELEMENT sentence</definiendum>
				<definiens id="0">a discourse , consisting of a series of discourse segments</definiens>
			</definition>
</paper>

		<paper id="1620">
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>AKT is a major research project applying a variety of technologies to knowledge management .</sentence>
				<definiendum id="0">AKT</definiendum>
				<definiens id="0">a major research project applying a variety of technologies to knowledge management</definiens>
			</definition>
			<definition id="1">
				<sentence>Knowledge is a dynamic , ubiquitous resource , which is to be found equally in an expert 's head , under terabytes of data , or explicitly stated in manuals .</sentence>
				<definiendum id="0">Knowledge</definiendum>
				<definiens id="0">a dynamic , ubiquitous resource , which is to be found equally in an expert 's head , under terabytes of data , or explicitly stated in manuals</definiens>
			</definition>
			<definition id="2">
				<sentence>KA is a complex process , which traditionally is extremely time consuming .</sentence>
				<definiendum id="0">KA</definiendum>
				<definiens id="0">a complex process</definiens>
			</definition>
			<definition id="3">
				<sentence>One methodology , often used in Expert Systems , involves the time-consuming process of structured interviews ( ‘protocols’ ) , which are then analysed by knowledge engineers in order to codify and model the knowledge of an expert in a particular domain .</sentence>
				<definiendum id="0">‘protocols’ )</definiendum>
				<definiens id="0">involves the time-consuming process of structured interviews</definiens>
			</definition>
			<definition id="4">
				<sentence>Adaptivity is a major goal for Information Extraction , especially in the case of its application to knowledge management , as KM is a process that has to be distributed throughout companies .</sentence>
				<definiendum id="0">Adaptivity</definiendum>
				<definiendum id="1">KM</definiendum>
				<definiens id="0">a major goal for Information Extraction , especially in the case of its application to knowledge management</definiens>
				<definiens id="1">a process that has to be distributed throughout companies</definiens>
			</definition>
			<definition id="5">
				<sentence>Knowledge publishing is the process that allows getting knowledge to the people who need it in a form that they can use .</sentence>
				<definiendum id="0">Knowledge publishing</definiendum>
				<definiens id="0">the process that allows getting knowledge to the people who need it in a form that they can use</definiens>
			</definition>
			<definition id="6">
				<sentence>The dynamic construction of appropriate perspectives is a challenge which , in AKT , we will address from the perspective of generating automatically such presentations from the ontologies acquired by the KA and KE methods , discussed in the previous sections .</sentence>
				<definiendum id="0">KE</definiendum>
				<definiens id="0">a challenge which , in AKT</definiens>
			</definition>
			<definition id="7">
				<sentence>Natural Language Generation ( NLG ) systems automatically produce language output ( ranging from a single sentence to an entire document ) from computer-accessible data , usually encoded in a knowledge or data base ( Reiter 2000 ) .</sentence>
				<definiendum id="0">Natural Language Generation</definiendum>
			</definition>
			<definition id="8">
				<sentence>We believe that HLT can make a substantial contribution to the following issues in KM : • Cost reduction : KM is an expensive task , especially in the acquisition phase .</sentence>
				<definiendum id="0">KM</definiendum>
				<definiens id="0">an expensive task</definiens>
			</definition>
			<definition id="9">
				<sentence>• Time reduction : KM is a slow task : HLT can help in making it more efficient by reducing the need for the human effort ; • Subjectivity reduction : this is a main problem in knowledge identification and selection .</sentence>
				<definiendum id="0">KM</definiendum>
				<definiendum id="1">Subjectivity reduction</definiendum>
				<definiens id="0">a slow task : HLT can help in making it more efficient by reducing the need for the human effort ; •</definiens>
			</definition>
			<definition id="10">
				<sentence>KM constitutes a challenge for HLT as it provides a number of fields of application and in particular it challenges the integration of a set of techniques for a common goal .</sentence>
				<definiendum id="0">KM</definiendum>
				<definiens id="0">constitutes a challenge for HLT as it provides a number of fields of application</definiens>
			</definition>
</paper>

		<paper id="0711">
			<definition id="0">
				<sentence>Learning morphology : algorithms for the identification of the stem changes .</sentence>
				<definiendum id="0">Learning morphology</definiendum>
			</definition>
</paper>

		<paper id="0806">
			<definition id="0">
				<sentence>We call any such resulting tree a single-step-lexicalisation at a7 of a64 in a54a23a12a56a55a58a57a60a59 , where a7 is the node at which the composition occurred .</sentence>
				<definiendum id="0">a7</definiendum>
				<definiens id="0">the node at which the composition occurred</definiens>
			</definition>
			<definition id="1">
				<sentence>3 11 : end if 12 : end for 13 : until a54a23a12a56a55a58a57a60a59a78a73a80a71 14 : a12 is a specific-lexicalisation4 of a12a24a5 .</sentence>
				<definiendum id="0">a12</definiendum>
				<definiens id="0">a specific-lexicalisation4 of a12a24a5</definiens>
			</definition>
			<definition id="2">
				<sentence>Notice that @ &lt; publisher.name &gt; occurs as the anchor of more than one tree.6 These trees , predication part : participleclause : item name and adj pred partp : participleclause : item name which we will refer to as a64a120a81 and a64a121a82 respectively , represent the forms in which that slot may be expressed .</sentence>
				<definiendum id="0">adj pred partp</definiendum>
				<definiens id="0">the anchor of more than one tree.6 These trees , predication part</definiens>
			</definition>
			<definition id="3">
				<sentence>Specific-lexicalisation is a transformation which operates on a complete TAG a12a6a81 and its result is another TAG a12a24a82 whose string set is the same as a12a30a81 ’s .</sentence>
				<definiendum id="0">Specific-lexicalisation</definiendum>
				<definiens id="0">a transformation which operates on a complete TAG</definiens>
			</definition>
			<definition id="4">
				<sentence>So each tree in a TAG with feature structures is an abbreviation for a7 trees , where a7 is the number of possible configurations of the feature structures on its nodes .</sentence>
				<definiendum id="0">a7</definiendum>
			</definition>
</paper>

		<paper id="0502">
			<definition id="0">
				<sentence>A partial list consists of Bayesian classifiers ( Gale et al. , 1993 ) , decision lists ( Yarowsky , 1994 ) , Bayesian hybrids ( Golding , 1995 ) , HMMs ( Charniak , 1993 ) , inductive logic methods ( Zelle and Mooney , 1996 ) , memorya3 This research is supported by NSF grants IIS-9801638 , IIS0085836 and SBR-987345 .</sentence>
				<definiendum id="0">partial list</definiendum>
				<definiendum id="1">Bayesian hybrids</definiendum>
				<definiendum id="2">HMMs</definiendum>
				<definiens id="0">consists of Bayesian classifiers ( Gale et al. , 1993 ) , decision lists ( Yarowsky , 1994 ) ,</definiens>
			</definition>
			<definition id="1">
				<sentence>In the following experiment , the SM used for unknown words makes use of three different classifiers a8a7a28a75a19a80a8 a38 and a8a27a141 or a8a84a142 a141 , defined as follows : a8a7a28a133a25 : a classifier based on the lexical feature a143a144a21a22a5 .</sentence>
				<definiendum id="0">SM</definiendum>
			</definition>
			<definition id="2">
				<sentence>SNoW ( Sparse Network of Winnows ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of features taking part in decisions is very large , but in which decisions actually depend on a small number of those features .</sentence>
				<definiendum id="0">SNoW</definiendum>
				<definiendum id="1">multi-class classifier</definiendum>
				<definiens id="0">a</definiens>
			</definition>
			<definition id="3">
				<sentence>The training corpus consists of a4a87a19a70a140a53a5a39a5a20a19a80a5a39a5a7a5 words .</sentence>
				<definiendum id="0">training corpus</definiendum>
			</definition>
			<definition id="4">
				<sentence>The test corpus consists of a4a6a162a7a5a87a19a80a5a7a5a39a5 words of which a44a149a19a82a140a84a21a75a4 are unknown words ( that is , they do not occur in the training corpus .</sentence>
				<definiendum id="0">test corpus</definiendum>
				<definiens id="0">consists of a4a6a162a7a5a87a19a80a5a7a5a39a5 words of which a44a149a19a82a140a84a21a75a4 are unknown words ( that is , they do not occur in the training corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>a8a18a141 is a classifier that uses only contextual features , a8a27a141 + baseline is the same classifier with the addition of the baseline feature ( “NNP” or “NN” ) .</sentence>
				<definiendum id="0">a8a18a141</definiendum>
				<definiens id="0">a classifier that uses only contextual features , a8a27a141 + baseline is the same classifier with the addition of the baseline feature ( “NNP” or “NN” )</definiens>
			</definition>
			<definition id="6">
				<sentence>SM ( a8 a50 a19a95a8a35a167 ) denotes that a8 a167 follows a8 a50 in the sequential model .</sentence>
				<definiendum id="0">SM</definiendum>
				<definiens id="0">a8 a167 follows a8 a50 in the sequential model</definiens>
			</definition>
			<definition id="7">
				<sentence>Test : average number of seconds to evaluate a single sentence .</sentence>
				<definiendum id="0">Test</definiendum>
				<definiens id="0">average number of seconds to evaluate a single sentence</definiens>
			</definition>
			<definition id="8">
				<sentence>In addition , SM has several significant computational advantages both in training and in test , since it only needs to consider a subset of the set of candidate class labels .</sentence>
				<definiendum id="0">SM</definiendum>
				<definiens id="0">several significant computational advantages both in training and in test</definiens>
			</definition>
			<definition id="9">
				<sentence>The SM attempts to reduce the size of the candidates set .</sentence>
				<definiendum id="0">SM</definiendum>
				<definiens id="0">attempts to reduce the size of the candidates set</definiens>
			</definition>
			<definition id="10">
				<sentence>Let a215 be a learning algorithm that is trained to minimize : a216 a194 a184a27a217a219a218 a83a108a220a12a187a75a221a185a83a108a96a105a89a33a89a85a102a138a83a85a96a110a89a136a222a58a96a188a19 where a96 is an example , a220a114a97a223a16a7a40a145a21a6a19a95a212a109a21a27a23 is the true class , a221 is the hypothesis , a218 is a loss function and a102a185a83a108a96a105a89 is the probability of seeing example a96 when a96a115a224 a101 ( see ( Allwein et al. , 2000 ) ) .</sentence>
				<definiendum id="0">a96</definiendum>
				<definiendum id="1">a220a114a97a223a16a7a40a145a21a6a19a95a212a109a21a27a23</definiendum>
				<definiendum id="2">a221</definiendum>
				<definiendum id="3">a218</definiendum>
				<definiendum id="4">a102a185a83a108a96a105a89</definiendum>
				<definiens id="0">the true class ,</definiens>
				<definiens id="1">the hypothesis</definiens>
				<definiens id="2">a loss function</definiens>
				<definiens id="3">the probability of seeing</definiens>
			</definition>
</paper>

		<paper id="1206">
			<definition id="0">
				<sentence>The TREC 1The Text REtrieval Conference ( TREC ) is a series of evaluations of fully automatic Q/A systems specified two restrictions : ( 1 ) there is at least one document in the test collection that contains the answer to a test question ; and ( 2 ) the answer length is either 50 contiguous bytes ( short answers ) or 250 contiguous bytes ( long answers ) .</sentence>
				<definiendum id="0">TREC 1The Text REtrieval Conference</definiendum>
				<definiendum id="1">answer length</definiendum>
				<definiens id="0">a series of evaluations of fully automatic Q/A systems specified two restrictions : ( 1 ) there is at least one document in the test collection that contains the answer to a test question</definiens>
			</definition>
			<definition id="1">
				<sentence>The answer type is the object of the verb visit , which is a place of attraction or entertainment , defined by the semantic category LANDMARK .</sentence>
				<definiendum id="0">answer type</definiendum>
				<definiens id="0">the object of the verb visit , which is a place of attraction or entertainment , defined by the semantic category LANDMARK</definiens>
			</definition>
			<definition id="2">
				<sentence>Semantic alternations consist of hypernyms , entailments or paraphrases .</sentence>
				<definiendum id="0">Semantic alternations</definiendum>
			</definition>
			<definition id="3">
				<sentence>The CATEGORY is defined as one of the following possibilities : TAXONOMY or one of its nodes ; For expert Q/A systems , this list of categories can be extended .</sentence>
				<definiendum id="0">CATEGORY</definiendum>
				<definiendum id="1">expert Q/A systems</definiendum>
				<definiens id="0">one of the following possibilities : TAXONOMY or one of its nodes ; For</definiens>
			</definition>
			<definition id="4">
				<sentence>The DEPENDENCY is defined as the question dependency structure when the CATEGORY belongs to the ANSWER TAXONOMY or is a DEFINITION .</sentence>
				<definiendum id="0">DEPENDENCY</definiendum>
				<definiens id="0">the question dependency structure when the CATEGORY belongs to the ANSWER TAXONOMY or is a DEFINITION</definiens>
			</definition>
			<definition id="5">
				<sentence>The NUMBER is a flag indicating whether the answer should contain a single datum or a list of elements .</sentence>
				<definiendum id="0">NUMBER</definiendum>
				<definiens id="0">a flag indicating whether the answer should contain a single datum or a list of elements</definiens>
			</definition>
			<definition id="6">
				<sentence>The ANSWER TAXONOMY was created in three steps : Step 1 We devise a set of top categories modeled after the semantic domains encoded in the WordNet database , which contains 25 noun categories and 15 verb categories .</sentence>
				<definiendum id="0">ANSWER TAXONOMY</definiendum>
				<definiendum id="1">WordNet database</definiendum>
				<definiens id="0">devise a set of top categories modeled after the semantic domains encoded in the</definiens>
			</definition>
			<definition id="7">
				<sentence>Step 2 The additional categorization of the top ANSWER TAXONOMY generates a many-tomany mapping of the Named Entity categories in the tops of the ANSWER TAXONOMY .</sentence>
				<definiendum id="0">ANSWER TAXONOMY</definiendum>
			</definition>
			<definition id="8">
				<sentence>Answer : the Seto Ohashi Bridge , consisting of six suspension bridges in the style of Golden Gate Bridge .</sentence>
				<definiendum id="0">Answer</definiendum>
				<definiens id="0">the Seto Ohashi Bridge , consisting of six suspension bridges in the style of Golden Gate Bridge</definiens>
			</definition>
			<definition id="9">
				<sentence>nominate/assignOrganization ( b ) PositionPerson Organizationresign/leave PositionPerson ( a ) Position Person1 Person2 Organization Positionreplace/succeed Person1 Person2 Organization Figure 6 : Dependencies that generate templates .</sentence>
				<definiendum id="0">nominate/assignOrganization ( b ) PositionPerson Organizationresign/leave PositionPerson</definiendum>
				<definiens id="0">Dependencies that generate templates</definiens>
			</definition>
			<definition id="10">
				<sentence>answers unify the dependency graphs and find common generalizations whenever possible .</sentence>
				<definiendum id="0">answers</definiendum>
				<definiens id="0">unify the dependency graphs and find common generalizations whenever possible</definiens>
			</definition>
			<definition id="11">
				<sentence>For each text passage retrieved by the keyword-based query we define the following seven features : a0a2a1a4a3a6a5a8a7a4a9 the number of question words matched in the same phrase as the answer type CATEGORY ; a0a10a1a4a3a6a5a8a7a11a7 the number of question words matched in the same sentence as the answer type CATEGORY ; a0a12a1a4a3a6a5a14a13a15a9 : a flag set to 1 if the answer type CATEGORY is followed by a punctuation sign , and set to 0 otherwise ; a0a16a1a4a3a6a5a8a17a19a18a21a20a23a22 : the number of question words matches separated from the answer type CATEGORY by at most three words and one comma ; a0a24a1a4a3a6a5a8a7a11a22a25a7 : the number of question words occurring in the same order in the answer text as in the question ; a0a26a1a4a3a6a5a8a27a28a20a23a22 : the average distance from the answer type CATEGORY to any of the question word matches ; a0a29a1a4a3a6a5a8a30a10a31a32a22 : the number of question words matched in the answer text .</sentence>
				<definiendum id="0">a0a12a1a4a3a6a5a14a13a15a9</definiendum>
			</definition>
</paper>

		<paper id="1621">
			<definition id="0">
				<sentence>The Trains project : A case study in building a conversational planning agent .</sentence>
				<definiendum id="0">Trains project</definiendum>
				<definiens id="0">A case study in building a conversational planning agent</definiens>
			</definition>
</paper>

		<paper id="1415">
			<definition id="0">
				<sentence>Tinysvm : Support vector machines .</sentence>
				<definiendum id="0">Tinysvm</definiendum>
				<definiens id="0">Support vector machines</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>Automatic Authoring is a good example of how these two methods can be improved and used to create a hypertextual organisation of ( multilingual ) information .</sentence>
				<definiendum id="0">Automatic Authoring</definiendum>
				<definiens id="0">a good example of how these two methods can be improved and used to create a hypertextual organisation of ( multilingual ) information</definiens>
			</definition>
			<definition id="1">
				<sentence>Automatic Authoring is the activity of processing news items in streams , detecting and extracting relevant information from them and , accordingly , organising texts in a non-linear fashion .</sentence>
				<definiendum id="0">Automatic Authoring</definiendum>
				<definiens id="0">the activity of processing news items in streams , detecting and extracting relevant information from them and</definiens>
			</definition>
			<definition id="2">
				<sentence>The general architecture is presented capitalising robust methods of Information Extraction ( Cunningham et al. , 1999 ) and large-scale multilingual resources ( e.g. EuroWordNet ) .</sentence>
				<definiendum id="0">Information Extraction</definiendum>
			</definition>
			<definition id="3">
				<sentence>As Automatic Authoring is the task of automatically deriving a hypertextual structure from a set of available news articles ( in three different languages English , Spanish and Italian in our case ) , the complexity of the overall framework requires a suitable decomposition : Text processing requires at least the detection of morphosyntactic information characterising the source texts : recognition , normalisation , and assignment of roles is required for the main participants for the different events/facts described .</sentence>
				<definiendum id="0">Automatic Authoring</definiendum>
			</definition>
			<definition id="4">
				<sentence>Coreference is a significant part of Information Extraction and indeed a necessary component in Authoring .</sentence>
				<definiendum id="0">Coreference</definiendum>
				<definiens id="0">a significant part of Information Extraction and indeed a necessary component in Authoring</definiens>
			</definition>
			<definition id="5">
				<sentence>LaSIE is a Large-scale Information Extraction system , developed for MUC ( Message Understanding Conference ) competitions , comprised of a variety of modules , see ( Humphreys et al. , 1998 ; MUC , 1998 ) .</sentence>
				<definiendum id="0">LaSIE</definiendum>
			</definition>
			<definition id="6">
				<sentence>The Name Entity Matcher returns the text with the Named Entities marked .</sentence>
				<definiendum id="0">Name Entity Matcher</definiendum>
			</definition>
			<definition id="7">
				<sentence>The NAMIC ontology consists of 40 predefined object classes and 46 attribute types related to Name Entity objects and nearly 1000 objects relating to EuroWordNet base concepts .</sentence>
				<definiendum id="0">NAMIC ontology</definiendum>
				<definiens id="0">consists of 40 predefined object classes and 46 attribute types related to Name Entity objects and nearly 1000 objects relating to EuroWordNet base concepts</definiens>
			</definition>
			<definition id="8">
				<sentence>EuroWordNet ( Vossen , 1998 ) is a multilingual lexical knowledge base ( LKB ) with wordnets for several European languages ( Dutch , Italian , Spanish , German , French , Czech and Estonian ) .</sentence>
				<definiendum id="0">EuroWordNet</definiendum>
			</definition>
			<definition id="9">
				<sentence>Each wordnet represents a unique language-internal system of lexicalisations .</sentence>
				<definiendum id="0">wordnet</definiendum>
				<definiens id="0">a unique language-internal system of lexicalisations</definiens>
			</definition>
			<definition id="10">
				<sentence>These Base Concepts serve as a level of multilingual abstraction for the conceptual constraints of our events , and allow us to extend the number of semantic classes from seven ( the MUC Named Entity classifications ) to 1024 the number of base concepts in EWN .</sentence>
				<definiendum id="0">Base Concepts serve</definiendum>
				<definiens id="0">the MUC Named Entity classifications ) to 1024 the number of base concepts in EWN</definiens>
			</definition>
			<definition id="11">
				<sentence>Ambiguity is controlled by part-of-speech tagging and domain verb-subcategorisation frames that guide the dependency recognition phase .</sentence>
				<definiendum id="0">Ambiguity</definiendum>
				<definiens id="0">guide the dependency recognition phase</definiens>
			</definition>
</paper>

		<paper id="0805">
			<definition id="0">
				<sentence>Then , a labeled directed graph a50 a15a52a51a54a53a55a23a57a56a59a58 , where a53a61a60a62a12 is the set of vertices ( or nodes ) and a56a63a60a64a53a66a65a67a40a68a65a69a53 is the set of labeled directed arcs ( or edges ) .</sentence>
				<definiendum id="0">a53a61a60a62a12</definiendum>
				<definiendum id="1">a56a63a60a64a53a66a65a67a40a68a65a69a53</definiendum>
				<definiens id="0">a labeled directed graph a50 a15a52a51a54a53a55a23a57a56a59a58 , where</definiens>
				<definiens id="1">the set of vertices ( or nodes ) and</definiens>
			</definition>
			<definition id="1">
				<sentence>makeReferringExpression ( a92 ) a17 bestGraph : = a154 ; a73 : = a51a193a17a24a92a93a31a32a23a25a49a29a58 ; return findGraph ( a92 , bestGraph , a73 ) ; a31 findGraph ( a92 , bestGraph , a73 ) a17 if ( bestGraph .</sentence>
				<definiendum id="0">makeReferringExpression</definiendum>
				<definiendum id="1">return findGraph</definiendum>
				<definiens id="0">a92 ) a17 bestGraph : = a154 ; a73 : = a51a193a17a24a92a93a31a32a23a25a49a29a58</definiens>
			</definition>
			<definition id="2">
				<sentence>cost then bestGraph : = a199 fi ; rof ; return bestGraph ; a31 Figure 5 : Sketch of the main function ( makeReferringExpression ) and the subgraph construction function ( findGraph ) .</sentence>
				<definiendum id="0">subgraph construction function</definiendum>
				<definiens id="0">= a199 fi ; rof ; return bestGraph</definiens>
			</definition>
			<definition id="3">
				<sentence>neighbors ( a92 ) ; return matchHelper ( matching , a214 , a73 ) ; a31 matchHelper ( matching , a214 , a73 ) a17 if a194 matching a194 a15 a194a73 a194 then return true fi ; if a214 a15a198a49 then return false fi ; choose a fresh , unmatched a215 from a214 ; a216 a87a200a15a63a17a20a217a191a70 a50 a194 a215 might be matched to a217a169a31 ; for each a217a218a70 a216 do if a217 is a valid extension of the mapping then if matchHelper ( matching a43a219a17a24a86a105a104a106a215a191a15a42a217a72a31 , a214 , a73 ) then return true fi ; fi ; rof ; return false ; a31 Figure 7 : Sketch of the function testing for subgraph isomorphism ( matchGraphs ) .</sentence>
				<definiendum id="0">return matchHelper</definiendum>
				<definiens id="0">matching , a214 , a73 ) a17 if a194 matching a194 a15 a194a73 a194 then return true fi ; if a214 a15a198a49 then return false fi</definiens>
				<definiens id="1">a valid extension of the mapping then if matchHelper ( matching a43a219a17a24a86a105a104a106a215a191a15a42a217a72a31 , a214 , a73 ) then return true fi ; fi ; rof ; return false</definiens>
			</definition>
			<definition id="4">
				<sentence>If not ( e.g. , a92 is a dog and a94 is a doghouse ) , we can immediately discard the matching .</sentence>
				<definiendum id="0">a92</definiendum>
				<definiendum id="1">a94</definiendum>
				<definiens id="0">a dog and</definiens>
			</definition>
			<definition id="5">
				<sentence>The algorithm described in the previous section can be seen as a generalization of Dale’s ( 1992 ) Full Brevity algorithm , in the sense that there is a guarantee that the algorithm will output the shortest possible description , if one exists .</sentence>
				<definiendum id="0">algorithm</definiendum>
				<definiens id="0">described in the previous section can be seen as a generalization of Dale’s ( 1992 ) Full Brevity algorithm , in the sense that there is a guarantee that the algorithm will output the shortest possible description , if one exists</definiens>
			</definition>
			<definition id="6">
				<sentence>a26 a227 Notice incidentally that Dale’s ( 1992 ) Greedy Heuristic algorithm can also be cast in the graph framework , by sorting edges on their descriptive power ( measured as a count of the number of occurrences of this particular edge in the scene graph ) .</sentence>
				<definiendum id="0">descriptive power</definiendum>
				<definiens id="0">a count of the number of occurrences of this particular edge in the scene graph )</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>Search takes the user direct to the storyboard and video clippings .</sentence>
				<definiendum id="0">Search</definiendum>
				<definiens id="0">takes the user direct to the storyboard and video clippings</definiens>
			</definition>
			<definition id="1">
				<sentence>Where Informedia primarily focuses on special applications , MUMIS aims at the advancement and integratibility of HLT-enhanced modules to enable information filtering beyond the textual domain .</sentence>
				<definiendum id="0">Informedia primarily</definiendum>
				<definiens id="0">focuses on special applications , MUMIS aims at the advancement and integratibility of HLT-enhanced modules to enable information filtering beyond the textual domain</definiens>
			</definition>
</paper>

		<paper id="1001">
</paper>

		<paper id="0520">
</paper>

		<paper id="1626">
			<definition id="0">
				<sentence>Subjectivity tagging is distinguishing sentences used to present opinions and other forms of subjectivity ( subjective sentences ) from sentences used to objectively present factual information ( objective sentences ) .</sentence>
				<definiendum id="0">Subjectivity tagging</definiendum>
			</definition>
			<definition id="1">
				<sentence>A potential subjective element ( PSE ) is a linguistic elementthatmay be used to express subjectivity .</sentence>
				<definiendum id="0">PSE</definiendum>
				<definiens id="0">a linguistic elementthatmay be used to express subjectivity</definiens>
			</definition>
			<definition id="2">
				<sentence>The pairwise  values on this set of 88 are : MM &amp; R : 0.80 ; ; MM &amp; L : 0.75 ; ; R &amp; MM : This study provides evidence for the viability of document-level ame annotation .</sentence>
				<definiendum id="0">MM</definiendum>
				<definiens id="0">This study provides evidence for the viability of document-level ame annotation</definiens>
			</definition>
			<definition id="3">
				<sentence>WSJ-SE is the corpus of 1001 sentences of the Wall Street Journal Treebank Corpus referred to above in section 3 .</sentence>
				<definiendum id="0">WSJ-SE</definiendum>
			</definition>
			<definition id="4">
				<sentence>Note that inammatory language is a kind of subjective language .</sentence>
				<definiendum id="0">inammatory language</definiendum>
				<definiens id="0">a kind of subjective language</definiens>
			</definition>
			<definition id="5">
				<sentence>NG-FE is a subset of the Usenet newsgroup corpus used in the documentlevel ame-annotation study described in section message test set for taggers R and MM .</sentence>
				<definiendum id="0">NG-FE</definiendum>
				<definiens id="0">a subset of the Usenet newsgroup corpus used in the documentlevel ame-annotation study described in section message test set for taggers R and MM</definiens>
			</definition>
			<definition id="6">
				<sentence>Flame elements are the subset of subjective elements that are perceived to be inammatory .</sentence>
				<definiendum id="0">Flame elements</definiendum>
				<definiens id="0">the subset of subjective elements that are perceived to be inammatory</definiens>
			</definition>
</paper>

		<paper id="1408">
			<definition id="0">
				<sentence>We are given a source string fJ1 = f1 : : : fj : : : fJ , which is to be translated into a target string eI1 = e1 : : : ei : : : eI : Among all possible target strings , we will choose the string with the highest probability : ˆeI1 = argmax eI1 n Pr ( eJ1jfI1 ) o = argmax eI1 n Pr ( eI1 ) Pr ( fJ1 jeI1 ) o The argmax operation denotes the search problem , i.e. the generation of the output sentence in the target language .</sentence>
				<definiendum id="0">argmax operation</definiendum>
				<definiens id="0">given a source string fJ1 = f1 : : : fj : : : fJ , which is to be translated into a target string eI1 = e1 : : : ei : : : eI : Among all possible target strings</definiens>
				<definiens id="1">the string with the highest probability : ˆeI1 = argmax eI1 n Pr ( eJ1jfI1 ) o = argmax eI1 n Pr ( eI1</definiens>
			</definition>
			<definition id="1">
				<sentence>In Model 4 the statistical alignment model is decomposed into five sub-models : the lexicon model p ( fje ) for the probability that the source word f is a translation of the target word e , the distortion model p=1 ( j j0jC ( fj ) ; E ) for the probability that the translations of two consecutive target words have the position difference j j0 where C ( fj ) is the word class of fj and E is the word class of the first of the two consecutive target words , the distortion model p &gt; 1 ( j j0jC ( fj ) ) for the probability that the words aligned to one target words have the position difference j j0 , the fertility model p ( je ) for the probability that a target language word e is aligned to source language words , the empty word fertility model p ( 0je0 ) for the probability that exactly 0 words remain unaligned to .</sentence>
				<definiendum id="0">C ( fj )</definiendum>
				<definiendum id="1">E</definiendum>
				<definiens id="0">aligned to source language words , the empty word fertility model p ( 0je0 ) for the probability that exactly 0 words remain unaligned to</definiens>
			</definition>
			<definition id="2">
				<sentence>A scoring function Q ( n ) + h ( n ) has to be defined which assigns a score to every node n. For beam search , this is the score Q ( n ) of a best path to this node .</sentence>
				<definiendum id="0">scoring function Q</definiendum>
				<definiens id="0">assigns a score to every node n. For beam search</definiens>
			</definition>
			<definition id="3">
				<sentence>The inverse translation probability p ( e j f ) of a source word f is calculated as p ( e j f ) = p ( f j e ) p ( e ) P e0 p ( f j e0 ) p ( e0 ) ; where we use a unigram model p ( e ) to estimate the prior probability of a target word being used .</sentence>
				<definiendum id="0">inverse translation probability p</definiendum>
				<definiens id="0">a unigram model p ( e ) to estimate the prior probability of a target word being used</definiens>
			</definition>
			<definition id="4">
				<sentence>The translation model state depends on the specific model dependencies of Model 4 : a coverage set C ( n ) containing the already translated source language positions , the position j ( n ) of the previously translated source word , a flag indicating whether the hypothesis is open or closed , the number of source language words which are aligned to the empty word , a flag showing whether the hypothesis is a complete hypothesis or not .</sentence>
				<definiendum id="0">position j</definiendum>
				<definiens id="0">a coverage set C ( n ) containing the already translated source language positions</definiens>
				<definiens id="1">open or closed , the number of source language words which are aligned to the empty word , a flag showing whether the hypothesis is a complete hypothesis or not</definiens>
			</definition>
			<definition id="5">
				<sentence>Therefore , the value of the heuristic function HX ( n ) for a node n can be deduced if we have an estimation hX ( j ) of the optimal score of translating position j ( here X denotes different possibilities to choose the heuristic function ) : HX ( n ) = Y j62C ( n ) hX ( j ) ; where C ( n ) is the coverage set .</sentence>
				<definiendum id="0">C ( n )</definiendum>
				<definiens id="0">different possibilities to choose the heuristic function ) : HX ( n ) = Y j62C ( n ) hX ( j ) ; where</definiens>
				<definiens id="1">the coverage set</definiens>
			</definition>
			<definition id="6">
				<sentence>For Model 4 we compute the following heuristic function with two arguments : hD ( j0 ; j ) = maxE p ( j j0jE ; C ( fj ) ) Thus , we obtain as an estimation of the distortion probability hD ( j ) = max j062C ( n ) hD ( j0 ; j ) : This yields the following heuristic functions taking into account translation , fertility , language , and distortion model probabilities : HTFLD ( n ) = Y j62C ( n ) hTFL ( j ) hD ( j ) ( 1 ) Using these heuristic functions we have the overhead of performing this rest cost estimation for every coverage set in search .</sentence>
				<definiendum id="0">HTFLD</definiendum>
				<definiens id="0">functions taking into account translation , fertility , language</definiens>
			</definition>
</paper>

		<paper id="0810">
			<definition id="0">
				<sentence>Information structure is a means that a speaker employs to indicate that some parts of a sentence meaning are context-dependent ( “given” ) , and that others are context-affecting ( “new” ) .</sentence>
				<definiendum id="0">Information structure</definiendum>
				<definiens id="0">a means that a speaker employs to indicate that some parts of a sentence meaning are context-dependent ( “given” ) , and that others are context-affecting ( “new” )</definiens>
			</definition>
			<definition id="1">
				<sentence>In order to clarify the complementary nature of the approaches that we have adopted , it is necessary first to distinguish between two dimensions of organization that are often confused or whose difference is contested : in his Systemic Functional Grammar ( SFG ) , ( Halliday , 1970 ; Halliday , 1985 ) distinguishes between the thematic structure of a clause and its information structure : Whereas the Theme is “the starting point for the message , it is the ground from which the clause is taking off” ( Halliday , 1985 , 38 ) , information structure concerns the distinction between the Given as “what is presented as being already known to the listener” ( Halliday , 1985 , 59 ) , and the New as “what the listener is being invited to attend to as new , or unexpected , or important” ( ibid ) .</sentence>
				<definiendum id="0">SFG</definiendum>
				<definiendum id="1">information structure</definiendum>
				<definiens id="0">necessary first to distinguish between two dimensions of organization that are often confused or whose difference is contested : in his Systemic Functional Grammar (</definiens>
				<definiens id="1">“what the listener is being invited to attend to as new , or unexpected , or important” ( ibid )</definiens>
			</definition>
			<definition id="2">
				<sentence>The SPL formulas express the bits of content identified by the text plan’s leaves , and can also group one or more leaves together ( aggregation ) depending on decisions taken by the text planner concerning discourse relations .</sentence>
				<definiendum id="0">SPL formulas</definiendum>
			</definition>
</paper>

		<paper id="0716">
			<definition id="0">
				<sentence>WordNet is an electronic lexical resource organized hierarchically by relations between sets of synonyms or near-synonyms called synsets .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">an electronic lexical resource organized hierarchically by relations between sets of synonyms or near-synonyms called synsets</definiens>
			</definition>
			<definition id="1">
				<sentence>Hypernymy is the relation that holds between such word senses as DACTCWCXCRD0CT BD -D7CWCXD4 BD or CWD9D1CPD2 BD -D4D3D0CXD8CXCRCXCPD2 BD , in which the first items in the pairs are more general than the second .</sentence>
				<definiendum id="0">Hypernymy</definiendum>
				<definiens id="0">the relation that holds between such word senses as DACTCWCXCRD0CT BD -D7CWCXD4 BD or CWD9D1CPD2 BD -D4D3D0CXD8CXCRCXCPD2 BD , in which the first items in the pairs</definiens>
			</definition>
			<definition id="2">
				<sentence>Chi-square is a non-parametric test which can be used for estimating whether or not there is any difference between the frequencies of items in frequency tables ( Oakes , 1998 ) .</sentence>
				<definiendum id="0">Chi-square</definiendum>
			</definition>
			<definition id="3">
				<sentence>The formula used to calculate chi-square is : AV BE BP CG B4C7 A0BXB5 BE BX ( 1 ) where O is the observed number of cases and E the expected number of cases .</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">the observed number of cases and E the expected number of cases</definiens>
			</definition>
			<definition id="4">
				<sentence>TiMBL is a program which implements several machine learning techniques .</sentence>
				<definiendum id="0">TiMBL</definiendum>
				<definiens id="0">a program which implements several machine learning techniques</definiens>
			</definition>
			<definition id="5">
				<sentence>The accuracy is the ratio between the number of items correctly classified and the total number of items to be classified .</sentence>
				<definiendum id="0">accuracy</definiendum>
			</definition>
</paper>

		<paper id="1624">
</paper>

		<paper id="1610">
			<definition id="0">
				<sentence>It is implemented using an IVR ( interactivevoice response ) platform developed at AT &amp; T , combining ASR and textto-speech with a phone interface ( Kamm et al. , 1997 ) .</sentence>
				<definiendum id="0">IVR</definiendum>
			</definition>
			<definition id="1">
				<sentence>U : Get me the train on Sunday at eight thirtypm 1164 NA 1162 Figure 4 : Dialogue Fragment with Aware and Correction Labels .</sentence>
				<definiendum id="0">U</definiendum>
				<definiens id="0">Dialogue Fragment with Aware and Correction Labels</definiens>
			</definition>
</paper>

		<paper id="1514">
			<definition id="0">
				<sentence>At least 6 other annotations have been created at various times and more-orless widely distributed among research sites : part-of-speech annotation ( Penn ) ; syntactic structure annotation ( Penn ) ; dysfluency annotation ( Penn ) ; partial phonetic transcription ( independently at UCLA and at Berkeley ) ; and discourse function annotation ( Colorado ) .</sentence>
				<definiendum id="0">discourse function annotation</definiendum>
				<definiens id="0">part-of-speech annotation ( Penn ) ; syntactic structure annotation ( Penn ) ; dysfluency annotation ( Penn ) ; partial phonetic transcription</definiens>
			</definition>
			<definition id="1">
				<sentence>The Topic Detection and Tracking Corpus , TDT-2 ( ISBN : 1-58563-157-4 ) was created in 1998 by LDC and contains newswire and more than 600 hours of transcribed broadcast news from 8 English and 3 Chinese sources sampled daily over six months with annotations to indicate story boundaries and relevance of those stories to 100 randomly selected topics .</sentence>
				<definiendum id="0">Topic Detection</definiendum>
			</definition>
			<definition id="2">
				<sentence>The Linguistic Data Consortium , a partner in this effort , is contributing several of its large data sets including conversational and broadcast data in Arabic , English , French and German .</sentence>
				<definiendum id="0">Linguistic Data Consortium</definiendum>
			</definition>
			<definition id="3">
				<sentence>SMART is building upon the distribution model established in LDC Online , a service that provides network-based access to hundreds of gigabytes of text and audio data and annotations .</sentence>
				<definiendum id="0">SMART</definiendum>
			</definition>
			<definition id="4">
				<sentence>The Annotation Graph Toolkit , version 1.0 , contains a complete implementation of the annotation graph model , import filters for several formats , loading/storing data to an annotation server ( MySQL ) , application programming interfaces in C++ and Tcl/tk , and example annotation tools for dialogue , ethology and interlinear text .</sentence>
				<definiendum id="0">Annotation Graph Toolkit</definiendum>
				<definiens id="0">contains a complete implementation of the annotation graph model , import filters for several formats , loading/storing data to an annotation server ( MySQL ) , application programming interfaces in C++ and Tcl/tk , and example annotation tools for dialogue , ethology and interlinear text</definiens>
			</definition>
</paper>

		<paper id="0702">
			<definition id="0">
				<sentence>Assuming each unit is equally likely to be chosen , the average number of comparisons here is given bya1a3a2a5a4a7a6a9a8a11a10a12a4a14a13 wherea1 is the number of categories , a4 is the number of units in the map and a8 is the number of training items .</sentence>
				<definiendum id="0">a4</definiendum>
				<definiendum id="1">a8</definiendum>
				<definiens id="0">the number of categories</definiens>
				<definiens id="1">the number of units in the map and</definiens>
				<definiens id="2">the number of training items</definiens>
			</definition>
			<definition id="1">
				<sentence>During testing , a novel item is presented to the SOM and the top C winners chosen ( i.e. the a1 closest map units ) , where C is the number of categories .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">the number of categories</definiens>
			</definition>
			<definition id="2">
				<sentence>Training items consist of the part of speech ( POS ) tag for the current word , varying amounts of left and right context ( POS tags only ) and the classification frequencies for that combination of tags .</sentence>
				<definiendum id="0">Training items</definiendum>
			</definition>
			<definition id="3">
				<sentence>MBL uses a weighted overlap similarity metric while SOMMBL and LSOMMBL use the euclidean distance .</sentence>
				<definiendum id="0">MBL</definiendum>
				<definiens id="0">uses a weighted overlap similarity metric while SOMMBL and LSOMMBL use the euclidean distance</definiens>
			</definition>
			<definition id="4">
				<sentence>This indicates the amount of context , in the form of “left-right” where “left” is the number of words in the left context , and “right” is the number of words in the right context .</sentence>
				<definiendum id="0">“left”</definiendum>
				<definiendum id="1">“right”</definiendum>
				<definiens id="0">the number of words in the left context</definiens>
				<definiens id="1">the number of words in the right context</definiens>
			</definition>
			<definition id="5">
				<sentence>The fscore ( a20 ) is computed as a20 a15 a21a23a22a25a24 a22a25a26a27a24 where a28 is the percentage of base NPs found that are correct and a29 is the percentage of base NPs defined in the corpus that were found .</sentence>
				<definiendum id="0">a29</definiendum>
				<definiens id="0">computed as a20 a15 a21a23a22a25a24 a22a25a26a27a24 where a28 is the percentage of base NPs found that are correct and</definiens>
			</definition>
			<definition id="6">
				<sentence>This is the maximum number of comparisons per novel item computed as a1a3a2a5a4a30a6a32a31a33a13 wherea1 is the number of categories , a4 is the number of units and a31 is the maximum number of items associated with a unit in the SOM .</sentence>
				<definiendum id="0">a4</definiendum>
				<definiens id="0">the number of categories</definiens>
				<definiens id="1">the number of units</definiens>
				<definiens id="2">the maximum number of items associated with a unit in the SOM</definiens>
			</definition>
			<definition id="7">
				<sentence>As the window size increases , LSOMMBL falls behind MBL .</sentence>
				<definiendum id="0">LSOMMBL</definiendum>
			</definition>
</paper>

		<paper id="1511">
			<definition id="0">
				<sentence>The Penn Treebank II ( Marcus et al. , 1994 ) marks subjects ( SBJ ) , logical objects of passives ( LGS ) , some reduced relative clauses ( RRC ) , as well as other grammatical information , but does not mark each constituent with a grammatical role .</sentence>
				<definiendum id="0">RRC</definiendum>
				<definiens id="0">marks subjects ( SBJ ) , logical objects of passives ( LGS ) , some reduced relative clauses</definiens>
			</definition>
			<definition id="1">
				<sentence>We designed GLARF with four objectives in mind : ( 1 ) capturing regularizations — noncanonical constructions ( e.g. , passives , fillergap constructions , etc. ) are represented in terms of their canonical counterparts ( simple declarative clauses ) ; ( 2 ) representing all phenomena using one simple data structure : the typed feature structure ( 3 ) consistently labeling all arguments and adjuncts for phrases with clear heads ; and ( 4 ) producing clear and consistent PRED-ARGs for phrases that do not have heads , e.g. , conjoined structures , named entities , etc. — rather than trying to squeeze these phrases into an X-bar mold , we customized our representations to reflect their head-less properties .</sentence>
				<definiendum id="0">GLARF</definiendum>
				<definiens id="0">with four objectives in mind : ( 1 ) capturing regularizations — noncanonical constructions ( e.g. , passives , fillergap constructions , etc. ) are represented in terms of their canonical counterparts ( simple declarative clauses</definiens>
				<definiens id="1">clear heads ; and ( 4 ) producing clear and consistent PRED-ARGs for phrases that do not have heads , e.g. , conjoined structures , named entities</definiens>
			</definition>
			<definition id="2">
				<sentence>The Susanne Corpus ( Sampson , 1995 ) consists of about 1/6 of the Brown Corpus annotated with detailed syntactic information .</sentence>
				<definiendum id="0">Susanne Corpus</definiendum>
				<definiens id="0">consists of about 1/6 of the Brown Corpus annotated with detailed syntactic information</definiens>
			</definition>
			<definition id="3">
				<sentence>A GLARFstyle PTB analysis uses the roles NAME1 and NAME2 instead of PROVINCE and COUNTRY , where name roles ( NAME1 , NAME2 ) are more general than PROVINCE and COUNTRY in a subsumption hierarchy .</sentence>
				<definiendum id="0">GLARFstyle PTB analysis</definiendum>
				<definiens id="0">uses the roles NAME1 and NAME2 instead of PROVINCE and COUNTRY , where name roles ( NAME1 , NAME2 ) are more general than PROVINCE and COUNTRY in a subsumption hierarchy</definiens>
			</definition>
			<definition id="4">
				<sentence>Another novel feature of GLARF is the ability to represent paraphrases ( in the Harrisian sense ) that are not entirely syntactic , e.g. , nominalizations as sentences .</sentence>
				<definiendum id="0">GLARF</definiendum>
				<definiens id="0">the ability to represent paraphrases</definiens>
			</definition>
			<definition id="5">
				<sentence>Each arc bears a feature label which represents either a grammatical role ( SBJ , OBJ , etc. ) or some attribute of a word or phrase ( morphological features , tense , semantic features , etc. ) .1 For example , the subject of a sentence is the head of a SBJ arc , an attribute like SINGULAR is the head of a GRAM-NUMBER arc , etc .</sentence>
				<definiendum id="0">SINGULAR</definiendum>
				<definiens id="0">a feature label which represents either a grammatical role ( SBJ , OBJ , etc. ) or some attribute of a word or phrase ( morphological features , tense , semantic features</definiens>
			</definition>
			<definition id="6">
				<sentence>The following argument types will be at the head of logical ( L- ) arcs based on counterparts in canonical sentences which are at the head of SLarcs : logical arguments of passives , understood subjects of infinitives , understood fillers of gaps , and interpreted arguments of nominalizations ( In “Rome’s destruction of Carthage” , “Rome” is the logical subject and “Carthage” is the logical object ) .</sentence>
				<definiendum id="0">“Rome”</definiendum>
				<definiendum id="1">“Carthage”</definiendum>
				<definiens id="0">the logical subject and</definiens>
				<definiens id="1">the logical object )</definiens>
			</definition>
			<definition id="7">
				<sentence>Thus “the man” is the indirect object ( IND-OBJ ) and “a dollar” is the direct object ( OBJ ) in both “She gave the man a dollar” and “She gave a dollar to the man” ( the dative alternation ) .</sentence>
				<definiendum id="0">OBJ</definiendum>
				<definiens id="0">the indirect object ( IND-OBJ )</definiens>
				<definiens id="1">the direct object (</definiens>
				<definiens id="2">the dative alternation )</definiens>
			</definition>
			<definition id="8">
				<sentence>Similarly , “the people” is the logical object ( L-OBJ ) of both “The people evacuated from the town” and “The troops evacuated the people from the town” , when we assume the appropriate regularization .</sentence>
				<definiendum id="0">“the people”</definiendum>
				<definiens id="0">the logical object ( L-OBJ ) of both “The people evacuated from the town”</definiens>
			</definition>
			<definition id="9">
				<sentence>Given ( S ( NP-SBJ ( PRP they ) ) ( VP ( VP ( VBD spent ) ( NP-2 ( $ $ ) ( CD 325,000 ) ( -NONE*U* ) ) ( PP-TMP-3 ( IN in ) ( NP ( CD 1989 ) ) ) ) ( CC and ) ( VP ( NP=2 ( $ $ ) ( CD 340,000 ) ( -NONE*U* ) ) ( PP-TMP=3 ( IN in ) ( NP ( CD 1990 ) ) ) ) ) ) Figure 1 : Penn representation of gapping a trilingual treebank , suppose that a Spanish treebank sentence corresponds to a Japanese nominalization phrase and an English nominalization phrase , e.g. , Disney ha comprado Apple Computers Disney’s acquisition of Apple Computers Furthermore , suppose that the English treebank analyzes the nominalization phrase both as an NP ( Disney = possessive , Apple Computers = object of preposition ) and as a paraphrase of a sentence ( Disney = subject , Apple Computers = object ) .</sentence>
				<definiendum id="0">Disney ha comprado</definiendum>
				<definiens id="0">a Japanese nominalization phrase and an English nominalization phrase</definiens>
			</definition>
			<definition id="10">
				<sentence>For example , X is the subject of the infinitives in “X appeared to leave” and “X was likely to bring attention to the problem” .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the subject of the infinitives in “X appeared to leave”</definiens>
			</definition>
			<definition id="11">
				<sentence>The NUMBER patterns each consist of a single NUMBER ( although multiple NUMBER constituents are possible , e.g. , “one thousand” ) and one UNIT constituent .</sentence>
				<definiendum id="0">NUMBER</definiendum>
				<definiens id="0">patterns each consist of a single NUMBER ( although multiple NUMBER constituents are possible , e.g. , “one thousand”</definiens>
			</definition>
			<definition id="12">
				<sentence>The recall is the total number of correct triples divided by the total number of triples in the answer key .</sentence>
				<definiendum id="0">recall</definiendum>
				<definiens id="0">the total number of correct triples divided by the total number of triples in the answer key</definiens>
			</definition>
</paper>

		<paper id="1409">
			<definition id="0">
				<sentence>Tamil is a head-last language with a very rich morphology and therefore quite different from English .</sentence>
				<definiendum id="0">Tamil</definiendum>
				<definiens id="0">a head-last language with a very rich morphology</definiens>
			</definition>
			<definition id="1">
				<sentence>We trained IBM Translation Model 4 ( Brown et al. , 1993 ) both on our corpus alone and on the augmented corpus , using the EGYPT toolkit ( Knight et al. , 1999 ; Al-Onaizan et al. , 1999 ) , and then translated a number of texts using different translation models and different transfer methods , namely glossing ( replacing each Tamil word by the most likely candidate from the translation tables created with the EGYPT toolkit ) and Model 4 decoding ( Brown et al. , 1995 ; Germann et al. , 2001 ) .</sentence>
				<definiendum id="0">EGYPT toolkit</definiendum>
				<definiens id="0">different transfer methods , namely glossing ( replacing each Tamil word by the most likely candidate from the translation tables created with the EGYPT toolkit</definiens>
			</definition>
			<definition id="2">
				<sentence>Seven human subjects without any knowledge of Tamil were given translations of a set of 15 texts ( all from the Berkeley corpus ) and asked to categorize them according to the following topic hierarchy : AF News about Sri Lanka AF Reports about clashes between the Sri Lankan army and the Liberation Tigers AF Sri Lankan security-related news ( arrests , arms deals , etc. ) AF Sri Lankan political news ( strikes , transport , telecom ) AF concerns Sri Lanka but doesn’t fit any of the above AF News about Pakistan/India AF Nuclear tests in Pakistan and India , including their aftermath ( international reactions , etc. ) AF Corruption investigation against Benazir Buto AF News about Pakistan/India but none of the above AF International news AF Disasters , accidents AF Nelson Mandela’s birthday AF Other international news AF Impossible to tell Except for one duplicate set , each subject received a different set of translations .</sentence>
				<definiendum id="0">Liberation Tigers AF Sri Lankan security-related news</definiendum>
				<definiens id="0">arms deals , etc. ) AF Sri Lankan political news ( strikes , transport , telecom ) AF concerns Sri Lanka but doesn’t fit any of the above AF</definiens>
				<definiens id="1">international reactions , etc. ) AF Corruption investigation against Benazir Buto AF News about Pakistan/India but none of the above AF International news AF</definiens>
			</definition>
			<definition id="3">
				<sentence>Glossing , in our system , is a simple base line algorithm that provides the most likely word translation for each word of input .</sentence>
				<definiendum id="0">line algorithm</definiendum>
				<definiens id="0">a simple base</definiens>
			</definition>
			<definition id="4">
				<sentence>method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 recall precision a glossing AF AF AF AF AF AF AF AF AF AF 67 % 79 % b AF AF AF AF AF AF AF AF 53 % 80 % c AF AF AF AF AF AF AF AF AF 60 % 48 % d both AF AF AF AF AF AF AF AF AF AF 67 % 79 % f both AF AF AF AF AF AF AF AF 60 % 73 % g both AF AF AF AF AF AF AF AF AF AF AF AF AF AF 93 % 88 % 10 B+TN.2 h both AF AF AF AF AF AF AF AF AF AF AF AF AF 87 % 75 % Human Translations AF AF AF AF AF AF AF AF AF AF AF AF AF AF AF 100 % 100 % a TamilNet corpus only ; stemmed ; 1291 aligned text chunks ; 23,359 tokens on Tamil side ; 1000 training iterations .</sentence>
				<definiendum id="0">AF AF AF AF AF AF AF AF AF AF</definiendum>
				<definiendum id="1">AF AF AF AF AF AF AF AF</definiendum>
				<definiendum id="2">AF AF AF AF AF AF AF AF AF AF AF AF AF AF</definiendum>
				<definiendum id="3">AF AF AF AF AF AF AF AF AF AF AF AF AF</definiendum>
				<definiens id="0">precision a glossing AF AF AF AF AF AF AF AF AF AF 67 % 79 % b AF AF AF AF AF AF AF AF 53 % 80 % c AF AF AF AF AF AF AF AF AF 60</definiens>
				<definiens id="1">87 % 75 % Human Translations AF AF AF AF AF AF AF AF AF AF AF AF AF AF AF 100 % 100 % a TamilNet corpus only</definiens>
			</definition>
</paper>

		<paper id="1208">
			<definition id="0">
				<sentence>In this paper , we propose a question answering system which uses an encyclopedia as a knowledge base .</sentence>
				<definiendum id="0">question answering system</definiendum>
				<definiens id="0">uses an encyclopedia as a knowledge base</definiens>
			</definition>
			<definition id="1">
				<sentence>Coverage is the ratio between the number of questions answered ( disregarding their correctness ) and the total number of questions .</sentence>
				<definiendum id="0">Coverage</definiendum>
				<definiens id="0">the ratio between the number of questions answered ( disregarding their correctness ) and the total number of questions</definiens>
			</definition>
			<definition id="2">
				<sentence>Accuracy is the ratio between the number of correct answers and the total number of answers made by the system .</sentence>
				<definiendum id="0">Accuracy</definiendum>
				<definiens id="0">the ratio between the number of correct answers and the total number of answers made by the system</definiens>
			</definition>
			<definition id="3">
				<sentence>The Class II examination consists of quadruplechoice questions , among which technical term questions can be subdivided into two types .</sentence>
				<definiendum id="0">Class II examination</definiendum>
				<definiens id="0">consists of quadruplechoice questions</definiens>
			</definition>
			<definition id="4">
				<sentence>P ( c|d ) =P ( c ) · summationdisplay t P ( t|c ) · P ( t|d ) P ( t ) ( 2 ) Here , P ( t|d ) , P ( t|c ) and P ( t ) denote probabilities that word t appears in d , c and all the domains , respectively .</sentence>
				<definiendum id="0">P</definiendum>
			</definition>
			<definition id="5">
				<sentence>In this paper , we proposed a question answering system which uses an encyclopedia as a knowledge base .</sentence>
				<definiendum id="0">question answering system</definiendum>
				<definiens id="0">uses an encyclopedia as a knowledge base</definiens>
			</definition>
</paper>

		<paper id="1403">
			<definition id="0">
				<sentence>The important features of a DSyntS are as follows : • a DSyntS is an unordered tree with labeled nodes and labeled arcs ; • a DSyntS is lexicalized , meaning that the nodes are labeled with lexemes ( uninflected words ) from the target language ; • a DSyntS is a dependency structure and not a phrasestructure structure : there are no nonterminal nodes , and all nodes are labeled with lexemes ; • a DSyntS is a syntactic representation , meaning that the arcs of the tree are labeled with syntactic relations such as SUBJECT ( represented in DSyntSs as I ) , rather than conceptual or semantic relations such as AGENT ; • a DSyntS is a deep syntactic representation , meaning that only meaning-bearing lexemes are represented , and not function words .</sentence>
				<definiendum id="0">DSyntS</definiendum>
				<definiendum id="1">DSyntS</definiendum>
				<definiendum id="2">DSyntS</definiendum>
				<definiendum id="3">DSyntS</definiendum>
				<definiens id="0">an unordered tree with labeled nodes and labeled arcs ; • a DSyntS is lexicalized , meaning that the nodes are labeled with lexemes ( uninflected words ) from the target language ; • a</definiens>
			</definition>
			<definition id="1">
				<sentence>Attribute constraints can be divided into two types : • independent attribute constraints , whose scope covers only one part of a candidate transfer rule and which are the same for the source and target parts ; • concurrent attribute constraints , whose scope extends to both the source and target parts of a candidate transfer rule .</sentence>
				<definiendum id="0">Attribute constraints</definiendum>
				<definiens id="0">both the source and target parts of a candidate transfer rule</definiens>
			</definition>
			<definition id="2">
				<sentence>For each set , we then calculated tree accuracy recall and precision measures as follows : Tree accuracy recall The tree accuracy recall for a transferred parse and a corresponding target parse is determined the by C/Rq , where C is the total number of features ( attributes , lexemes and dependency relationships ) that are found in both the nodes of the transferred parse and in the corresponding nodes in the target parse , and Rq is the total number of features found in the nodes of the target parse .</sentence>
				<definiendum id="0">C</definiendum>
				<definiendum id="1">Rq</definiendum>
				<definiens id="0">tree accuracy recall and precision measures as follows : Tree accuracy recall The tree accuracy recall for a transferred parse and a corresponding target parse is determined the by C/Rq , where</definiens>
				<definiens id="1">the total number of features ( attributes , lexemes and dependency relationships ) that are found in both the nodes of the transferred parse and in the corresponding nodes in the target parse , and</definiens>
			</definition>
			<definition id="3">
				<sentence>Tree accuracy precision The tree accuracy precision for a transferred parse and a corresponding target parse is determined the by C/Rt , where C is the total number of features ( attributes , lexemes and dependency relationships ) that are found in both the nodes of the transferred parse and in the corresponding nodes in the target parse , and Rt is the total number of features found in the nodes of the transferred parse .</sentence>
				<definiendum id="0">C</definiendum>
				<definiendum id="1">Rt</definiendum>
				<definiens id="0">the total number of features ( attributes , lexemes and dependency relationships</definiens>
			</definition>
</paper>

		<paper id="0718">
</paper>

		<paper id="0707">
			<definition id="0">
				<sentence>Subcategorization information can help solve the problem , but the amount of information necessary to be encoded in such lexicons is huge , since in addition to subcategorization frames , one should encode the different senses of words and rules on how these senses can be combined , as well as the different units ( single or multi word expressions ) a language makes use of .</sentence>
				<definiendum id="0">Subcategorization information</definiendum>
				<definiens id="0">the different units ( single or multi word expressions ) a language makes use of</definiens>
			</definition>
			<definition id="1">
				<sentence>A span consists of two or more adjacent nuclei together with the dependency links among them .</sentence>
				<definiendum id="0">span</definiendum>
			</definition>
			<definition id="2">
				<sentence>Spans are combined using the “covered concatenation” operator , which connects two spans sharing a nucleus and possibly adds a dependency link between the leftmost and the rightmost nucleus , or vice-versa .</sentence>
				<definiendum id="0">“covered concatenation” operator</definiendum>
			</definition>
			<definition id="3">
				<sentence>The probability of a span is the product of the probabilities of the dependency links it contains .</sentence>
				<definiendum id="0">probability of a span</definiendum>
			</definition>
			<definition id="4">
				<sentence>The signature of a span consists of three things : a115 A flag indicating whether the span is minimal or not .</sentence>
				<definiendum id="0">signature of a span</definiendum>
			</definition>
</paper>

		<paper id="0909">
			<definition id="0">
				<sentence>Assuming that O w ; ; tu is the occurrence of the word labelled w in the thematic unit labelled tu and that O tu is the occurrence of all the words in the thematic unit tu , such that O tu = P w O w ; ; tu , each thematic unit vector component , p ( w=tu ) , is : p ( w=tu ) = O w ; ; tu O tu ( 1 ) When the thematic units are unclassed , their entropy is given by ( Cover and Thomas , 1991 ) : H TU = ; X w ; ; tu p ( w ; ; tu ) ln [ p ( wjtu ) ] ( 2 ) with p ( w ; ; tu ) = O w ; ; tu P w ; ; tu O w ; ; tu When the thematic units are gathered in K clusters , labelled k , the cluster entropy is , H K : H K = ; X w ; ; k p ( w ; ; k ) ln [ p ( wjk ) ] ( 3 ) where p ( wjk ) is defined as : p ( wjtu 2 k ) =p ( wjk ) = O w ; ; k O k ( 4 ) with O w ; ; k = P tu2k O w ; ; tu , O k = P tu2k O tu and p ( w ; ; k ) = O w ; ; k P w ; ; k O w ; ; k The cluster entropy is always higher than or equal to the unit entropy ( log-sum rule ( Cover and Thomas , 1991 ) ) , so that the Kullback-Leibleir divergence defined as : D KL = H K ; H TU ( 5 ) is always higher than or equal to 0 .</sentence>
				<definiendum id="0">tu</definiendum>
				<definiendum id="1">tu p</definiendum>
				<definiendum id="2">tu When</definiendum>
				<definiendum id="3">cluster entropy</definiendum>
				<definiendum id="4">H TU</definiendum>
				<definiens id="0">the occurrence of all the words in the thematic unit tu , such that O tu = P w O w ; ; tu , each thematic unit vector component , p ( w=tu ) , is : p ( w=tu ) = O w ; ; tu O tu ( 1 ) When the thematic units are unclassed , their entropy is given by ( Cover and Thomas , 1991 ) : H TU =</definiens>
				<definiens id="1">the thematic units are gathered in K clusters , labelled k , the cluster entropy is , H K : H K = ; X w ; ; k p ( w ; ; k ) ln [ p ( wjk ) ] ( 3 ) where p</definiens>
			</definition>
			<definition id="1">
				<sentence>W is the weight of a word and O its occurrence number .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">the weight of a word</definiens>
			</definition>
			<definition id="2">
				<sentence>The interpolated value of the prediction of a word w , knowing the domain d is p 0 ( wjd ) such that : p 0 ( wjd ) = O ( w ; ; d ) +n sw ( d ) =V O ( d ) +n sw ( d ) ( 8 ) where n sw ( d ) is the number of words seen in each domain and V the size of the vocabulary .</sentence>
				<definiendum id="0">n sw</definiendum>
				<definiens id="0">the number of words seen in each domain and V the size of the vocabulary</definiens>
			</definition>
			<definition id="3">
				<sentence>The  coefficient measures the degree of agreement among several judgements and is expressed as follows : k = 1 ; 2 1 ; 2 ( 9 ) where 1 is the proportion of times that the judgments agree and 2 is the proportion of times that we could expect the judgments to agree by chance .</sentence>
				<definiendum id="0"> coefficient</definiendum>
				<definiens id="0">measures the degree of agreement among several judgements</definiens>
			</definition>
			<definition id="4">
				<sentence>1 , which estimates the probability that the two classifications agree , is defined by : 1 = P K i=1 k i ; ; i N ( 10 ) where N is the total number of TUs .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">estimates the probability that the two classifications agree , is defined by : 1 = P K i=1 k i</definiens>
				<definiens id="1">the total number of TUs</definiens>
			</definition>
			<definition id="5">
				<sentence>In ( Legendre , 2000 ) , Legendre defines the Mantel test as ” a procedure to test the hypothesis that the distances among objects in a matrix A are linearly independent of the distances among the same objects in another matrix B. The result of this test may be used as support for or against the hypothesis that the process that generated the first set of distances is independent of the process that generated the second set .</sentence>
				<definiendum id="0">Mantel test</definiendum>
				<definiens id="0">” a procedure to test the hypothesis that the distances among objects in a matrix A are linearly independent of the distances among the same objects in another matrix</definiens>
			</definition>
</paper>

		<paper id="1625">
</paper>

		<paper id="1500">
</paper>

		<paper id="1305">
			<definition id="0">
				<sentence>Information Extraction ( IE ) is an upcoming challenging research area to cope with the increasing volume of unwieldy distributed information resources , such as information over WWW .</sentence>
				<definiendum id="0">Information Extraction</definiendum>
				<definiendum id="1">IE</definiendum>
				<definiens id="0">distributed information resources , such as information over WWW</definiens>
			</definition>
			<definition id="1">
				<sentence>The sets of TRare : TR ( T ) =fON ; ; BEGIN ; ; END ; ; PAST , FUTUER ; ; ONGOING ; ; CONTINUEDg TR ( E i ; ; E j ) =fBEFORE ; ; AFTER ; ; MEETS , METBY ; ; OVERLAPS ; ; OVERLAPPED , DURING ; ; CONTAINS ; ; STAREDBY , STARTS ; ; FINISHES ; ; FINISHEDBY , SAME ASg For an absolute relation of a single event , T is an indispensable parameter , which includes event time t e , reference time t r 4 and speech time t s : 3 OCCURis a predicate for the happening of a single event .</sentence>
				<definiendum id="0">CONTINUEDg TR</definiendum>
				<definiendum id="1">MEETS</definiendum>
				<definiendum id="2">T</definiendum>
				<definiens id="0">an indispensable parameter , which includes event time t e</definiens>
			</definition>
			<definition id="2">
				<sentence>following rules : ( 1 ) Approximation : ON ( t e )  ( ATT ( T ) =\present '' ) ) ON ( RD ) ON ( t e )  ( ATT ( T ) =\past '' ) ) PAST ( RD ) ON ( t e )  ( ATT ( T ) =\future '' ) ) FUTURE ( RD ) PAST ( t r )  ( ATT ( T ) =\past '' ) ) PAST ( RD ) FUTURE ( t r )  ( ATT ( T ) =\future '' ) ) FUTURE ( RD ) TR ( t r )  ( ATT ( T ) =\present '' ) ) TR ( RD ) TR ( ? ) )</sentence>
				<definiendum id="0">Approximation</definiendum>
				<definiendum id="1">ATT</definiendum>
				<definiens id="0">ON ( t e )  ( ATT ( T ) =\present '' ) ) ON ( RD ) ON ( t e )  ( ATT ( T ) =\past '' ) ) PAST ( RD ) ON ( t e )  ( ATT ( T ) =\future '' )</definiens>
			</definition>
			<definition id="3">
				<sentence>TR ( RD ) ( 2 ) Negation : : END ( t r ) ) CONTINUED ( t r ) : BEGIN ( t r ) ) FUTURE ( t r ) : PAST ( t r ) ) FUTURE ( t r ) : FUTURE ( t r ) ) FUTURE ( t r ) i ; ; E j ) ( multiple events ) supports the following rules : ( 3 ) Symmetry : BEFORE ( E i ; ; E j ) AFTER ( E j ; ; E i ) CONTAINS ( E i ; ; E j ) DURING ( E j ; ; E i ) OVERLAPS ( E i ; ; E j ) OVERLAPPED ( E j ; ; E i ) STARTS ( E i ; ; E j ) STAREDBY ( E j ; ; E i ) FINISHES ( E i ; ; E j ) FINISHEDBY ( E j ; ; E i ) SAME AS ( E i ; ; E j ) SAME AS ( E j ; ; E i ) s ( T ) and TR e ( T ) ( declared event ) supports the following rules : ( 4 ) TR s ( t s ) TR e ( t r ) ) TR e ( t s ) : ON ( t s ) TR ( ? ) )</sentence>
				<definiendum id="0">TR</definiendum>
				<definiendum id="1">; E j</definiendum>
				<definiendum id="2">E i ) s ( T</definiendum>
				<definiendum id="3">TR e ( T )</definiendum>
				<definiens id="0">E j ; ; E i ) SAME AS ( E i ;</definiens>
				<definiens id="1">TR s ( t s ) TR e ( t r ) ) TR e ( t s ) : ON ( t s ) TR ( ? )</definiens>
			</definition>
			<definition id="4">
				<sentence>Compared ( a )  ; ÞÆêç ( This morning , he read the newspaper ) and ( b ) % ·ÖêÜýV ( I read two books yesterday ) , the two sentences are alike as they both embody an indicator ê , which implies PAST in principle .</sentence>
				<definiendum id="0">Compared</definiendum>
				<definiens id="0">I read two books yesterday ) , the two sentences are alike as they both embody an indicator ê , which implies PAST in principle</definiens>
			</definition>
			<definition id="5">
				<sentence>FUTURE ( t e is future for t s ) FUTURE ( t s ) PAST ( t r ) ) FUTURE ( t s ) Case IV : t e to t s and t 0 r , t 0 r to t r ( t r is given ) bÆ ( vsï ) Ç ( tÛ ) ( f ) ( adv . )</sentence>
				<definiendum id="0">FUTURE</definiendum>
				<definiens id="0">future for t s ) FUTURE ( t s ) PAST ( t r ) ) FUTURE ( t s ) Case IV : t e to t s and t 0 r</definiens>
			</definition>
			<definition id="6">
				<sentence>BEGIN ( t e is begin for t 0 r ) PAST ( t s ) FUTURE ( t r ) ) FUTURE ( t s ) FUTURE ( t s ) BEGIN ( t 0 r ) ) FUTURE ( t s ) PAST ( t s ) BEGIN ( t r ) ) CONTINUED ( t s ) PAST ( t s ) END ( t r ) ) PAST ( t s ) PAST ( t s ) FUTURE ( t r ) ) FUTURE ( t s ) PAST ( t s ) ONGOING ( t r ) ) CONTINUED ( t s ) PAST ( t s ) CONTINUED ( t r ) ) CONTINUED ( t s ) FUTURE ( t s ) BEGIN ( t r ) ) FUTURE ( t s ) FUTURE ( t s ) END ( t r ) ) CONTINUED ( t s ) FUTURE ( t s ) PAST ( t r ) ) FUTURE ( t s ) FUTURE ( t s ) ONGOING ( t r ) ) FUTURE ( t s ) FUTURE ( t s ) CONTINUED ( t r ) ) CONTINUED ( t s ) Table 1 : Rule set R1 for single event statements of TwoEvents ( R2 &amp; R3 ) To express two relevantevents is straightforward .</sentence>
				<definiendum id="0">BEGIN</definiendum>
				<definiens id="0">FUTURE ( t r ) ) FUTURE ( t s ) FUTURE ( t s ) BEGIN ( t 0 r ) ) FUTURE ( t s ) PAST ( t s ) BEGIN ( t r ) ) CONTINUED ( t s ) PAST ( t s ) END ( t r ) ) PAST ( t s ) PAST ( t s ) FUTURE ( t r ) ) FUTURE ( t s ) PAST ( t s ) ONGOING ( t r ) ) CONTINUED ( t s ) PAST ( t s ) CONTINUED ( t r ) ) CONTINUED ( t s ) FUTURE ( t s ) BEGIN ( t r ) ) FUTURE ( t s ) FUTURE ( t s ) END ( t r ) ) CONTINUED ( t s ) FUTURE ( t s ) PAST ( t r ) ) FUTURE ( t s ) FUTURE ( t s ) ONGOING ( t r ) ) FUTURE ( t s ) FUTURE ( t s ) CONTINUED ( t r</definiens>
			</definition>
			<definition id="7">
				<sentence>( 3 ) for a multiple event statement or a declared multiple event statement : IF f is found in a temporal statement , nd ATT ( F ) andTR ( E2 ) , then check rule set R2 ; ; IF TRis found , return TR ; ; ELSE return TR= ; ; ENDIF ; ; ELSE check R3 ; ; IF TRis found , return TR ; ; ELSE returnTR=\BEFORE '' ( default value ) ; ; ENDIF ; ; ENDIF ; ; IF one of the events contains time de nition ( i.e. t ) , do ( 1 ) ; ; ELSE go to END ; ; ENDIF .</sentence>
				<definiendum id="0">ELSE returnTR=\BEFORE</definiendum>
				<definiens id="0">check rule set R2 ; ; IF TRis found , return TR ; ; ELSE return TR= ; ; ENDIF ; ; ELSE check R3 ; ; IF TRis found , return TR ; ;</definiens>
			</definition>
			<definition id="8">
				<sentence>TR ( E ) 6742 6249 92.69 % TR ( E i ; ; E j ) 687 643 93.60 % Overall 7429 6892 92.77 % Table 5 : Experimental results of temporal relation discovery Pattern Number Percentage 10 BEGIN &amp; END 66 0.89 % 11 SAME AS 59 0.79 % 12 CONTAINS 41 0.55 % 13 END 7 0.09 % 14 STARTEDBY 3 0.04 % Table 6 : TRclassi ed by the program in decending order ( 2 ) Ambiguous rules All the rules are de ned on the basis of indicators ' attributes .</sentence>
				<definiendum id="0">TR</definiendum>
				<definiendum id="1">Ambiguous</definiendum>
			</definition>
</paper>

		<paper id="0907">
			<definition id="0">
				<sentence>The ARC A3 Project : Terminology Acquisition Tools : Evaluation Method and Task Widad Mustafa El Hadi mustafa @ univ-lille3.fr Ismaïl Timimi timimi @ univ-lille3.fr Annette Béguin beguin @ univ-lille3.fr Marcilio De Brito mdebrito @ noos.fr UFR IDIST &amp; CERSATES ( CNRS UMR 8529 ) Université Charles De Gaulle , Lille 3 BP 149 , F-59 653 Villeneuve D'Ascq , France This paper describes the work achieved in the Concerted Research Project ARC A3 supported and coordinated by the AUF1 , former Aupelf-Uref2 .</sentence>
				<definiendum id="0">ARC A3 Project</definiendum>
				<definiens id="0">the work achieved in the Concerted Research Project ARC A3 supported and coordinated by the AUF1 , former Aupelf-Uref2</definiens>
			</definition>
			<definition id="1">
				<sentence>ARC A3 is a project of the ILEC3 group coordinated and founded by AUF .</sentence>
				<definiendum id="0">ARC A3</definiendum>
				<definiens id="0">a project of the ILEC3 group coordinated and founded by AUF</definiens>
			</definition>
			<definition id="2">
				<sentence>Participating Systems Software Affiliation Acabit IRIN ( Nantes ) Ana IRIN ( Nantes ) Conterm LANCI ( Montréal ) Iota CLIPS-IMAG ( Grenoble ) Lexter ERSS ( Toulouse ) Seek-Java CAMS-LALIC ( Paris 4 ) Loria6 LORIA ( Nancy ) Xerox-Termfinder XEROX ( Grenoble ) Terminology plays a major role in information processing and management and in specialized communication .</sentence>
				<definiendum id="0">Grenoble ) Terminology</definiendum>
				<definiens id="0">plays a major role in information processing and management and in specialized communication</definiens>
			</definition>
			<definition id="3">
				<sentence>Extractors ( SRE ) Terminology resources are increasingly seen as structured data i.e. as a network of terms organized by relations .</sentence>
				<definiendum id="0">Extractors</definiendum>
				<definiens id="0">SRE ) Terminology resources are increasingly seen as structured data i.e. as a network of terms organized by relations</definiens>
			</definition>
			<definition id="4">
				<sentence>12 INIST is the National Institute of Scientific and Technical Information .</sentence>
				<definiendum id="0">INIST</definiendum>
			</definition>
			<definition id="5">
				<sentence>The program output consists of two files : file ( a ) which contains the elements of L2 found in L1 ( the relevant terms which the software was able to find ) .</sentence>
				<definiendum id="0">program output</definiendum>
				<definiens id="0">consists of two files : file ( a ) which contains the elements of L2 found in L1 ( the relevant terms which the software was able to find )</definiens>
			</definition>
</paper>

		<paper id="0804">
			<definition id="0">
				<sentence>Suppose a38 is the target set , and a39 is the set of elements from which a38 is to be selected.2 The algorithm iterates through IP ; for each property , it checks whether it would rule out at least one member of a39 that has not already been ruled out ; if so , the property is added to a22 .</sentence>
				<definiendum id="0">a39</definiendum>
				<definiens id="0">the set of elements from which a38 is to be selected.2 The algorithm iterates through IP</definiens>
			</definition>
			<definition id="1">
				<sentence>The process of expanding a22 and contracting a39 continues until a39a41a40a41a38 ; if and when this condition is met , a22 is a distinguishing set of properties .</sentence>
				<definiendum id="0">a22</definiendum>
			</definition>
			<definition id="2">
				<sentence>Assuming ( cf. Dale and Reiter 1995 ) that the tests in the body of the loop take some constant amount of time , the worst-case running time is in the order of a5a8a40 ( i.e. , a41a43a42 a5a8a40a45a44 ) where a5a46a40 is the total number of properties .</sentence>
				<definiendum id="0">a5a46a40</definiendum>
			</definition>
			<definition id="3">
				<sentence>Satellite sets may be exploited for the construction of descriptions by forming the union of a number of expressions , each of which is the intersection of the elements of a38a56a91 ( for some a52a90a88 a38 ) .</sentence>
				<definiendum id="0">Satellite sets</definiendum>
				<definiens id="0">the intersection of the elements of a38a56a91 ( for some a52a90a88 a38 )</definiens>
			</definition>
			<definition id="4">
				<sentence>DBS is computationally cheap : it has a worstcase running time of a41a43a42 a5 a27 a0 a44 , where a5 is the number of objects in a38 , and a0 the number of atomic properties .</sentence>
				<definiendum id="0">a5</definiendum>
			</definition>
			<definition id="5">
				<sentence>Suppose brevity of descriptions is defined as follows : a52 a24 is less brief than a52a32a31 if either a52 a24 contains only atomic properties while a52a8a31 contains non-atomic properties as well , or a52 a24 contains more Boolean operators than a52 a31 .</sentence>
				<definiendum id="0">Suppose brevity of descriptions</definiendum>
				<definiens id="0">follows : a52 a24 is less brief than a52a32a31 if either a52 a24 contains only atomic properties while a52a8a31 contains non-atomic properties as well , or a52 a24 contains more Boolean operators than a52 a31</definiens>
			</definition>
			<definition id="6">
				<sentence>Then whenever a target set a43 can be described as an intersection of atomic properties , BOOL ( a43 ) would be a maximally brief intersection of atomic properties , and this is inconsistent with the intractability of Full Brevity for intersections of atomic properties .</sentence>
				<definiendum id="0">BOOL</definiendum>
				<definiens id="0">an intersection of atomic properties</definiens>
			</definition>
			<definition id="7">
				<sentence>This negative result gives rise to the question whether Full Brevity may be approximated , perhaps in the spirit of Reiter ( 1990 ) ’s ‘Local Brevity’ algorithm which takes a given intersective description and tests whether any set of properties in it may be replaced by one other property .</sentence>
				<definiendum id="0">‘Local Brevity’ algorithm</definiendum>
				<definiens id="0">takes a given intersective description and tests whether any set of properties in it may be replaced by one other property</definiens>
			</definition>
			<definition id="8">
				<sentence>Consider extensional truncations of ( a ) and ( b ) , such as may be generated from an input I ( 1 ) ( where the semantic predicate ‘dangerous’ is abbreviated as a0 and a47 is a constant referring to the button ) : I ( 1 ) a0 a42a63a47 a44 ( aa1 ) [ The red button ] is dangerous ( ba1 ) [ The rocket launching button ] is dangerous Suppose ( a ) and ( b ) are semantically interchangeable ( e.g. , when said to someone who knows the colours and functions of all objects in the domain ) , so a choice between them can only be motivated by an appeal to pragmatic principles .</sentence>
				<definiendum id="0">a47</definiendum>
			</definition>
			<definition id="9">
				<sentence>Even then , it is difficult to accept that the same choice must be made regardless whether the input to the generator is I ( 1 ) , I ( 2 ) or I ( 3 ) : ( Here a17a105a61a42a3a2 a44 says that a2 is for launching rockets ; a4 is the Russellian description operator . )</sentence>
				<definiendum id="0">a4</definiendum>
				<definiens id="0">the Russellian description operator</definiens>
			</definition>
</paper>

		<paper id="0811">
			<definition id="0">
				<sentence>A good dictionary is a place with a lot of information , structured in such a way that the relevant information is easily accessible when needed .</sentence>
				<definiendum id="0">good dictionary</definiendum>
				<definiens id="0">a place with a lot of information , structured in such a way that the relevant information is easily accessible when needed</definiens>
			</definition>
</paper>

		<paper id="1311">
</paper>

		<paper id="0704">
			<definition id="0">
				<sentence>The method combines the use of EuroWordNet’s ontological concepts and the correct sense of each word assigned by a Word Sense Disambiguation ( WSD ) module to extract three sets of patterns : subject-verb , verb-direct object and verb-indirect object .</sentence>
				<definiendum id="0">Word Sense Disambiguation</definiendum>
				<definiendum id="1">WSD</definiendum>
				<definiens id="0">) module to extract three sets of patterns : subject-verb , verb-direct object and verb-indirect object</definiens>
			</definition>
			<definition id="1">
				<sentence>A classifier obtained by means of a ME technique consist of a set of parameters or coefficients estimated by an optimization procedure .</sentence>
				<definiendum id="0">classifier</definiendum>
				<definiens id="0">obtained by means of a ME technique consist of a set of parameters or coefficients estimated by an optimization procedure</definiens>
			</definition>
			<definition id="2">
				<sentence>A feature is a function that gives information about some characteristic in a context associated to a class .</sentence>
				<definiendum id="0">feature</definiendum>
				<definiens id="0">a function that gives information about some characteristic in a context associated to a class</definiens>
			</definition>
			<definition id="3">
				<sentence>The conditional probability a10a32a12a33a4a35a34a14a36a16 is defined as in equation ( 2 ) where a39a41a40 are the parameters or weights of each feature , and a42a43a12a15a14a36a16 is a constant to ensure that the sum of probabilities for each possible class in this context is equal to 1 .</sentence>
				<definiendum id="0">a39a41a40</definiendum>
				<definiendum id="1">a42a43a12a15a14a36a16</definiendum>
				<definiens id="0">a constant to ensure that the sum of probabilities</definiens>
			</definition>
			<definition id="4">
				<sentence>EuroWordNet ( Vossen , 2000 ) is a multilingual lexical database representing semantic relations among basic concepts for West European languages .</sentence>
				<definiendum id="0">EuroWordNet</definiendum>
			</definition>
			<definition id="5">
				<sentence>EuroWordnet’s ontology consists of 63 higherlevel concepts and distinguishes three types of entities : a91 1stOrderEntity : any concrete entity ( publicly ) perceivable by the senses and located at any point in time , in a three-dimensional space , e.g. : vehicle , animal , substance , object .</sentence>
				<definiendum id="0">EuroWordnet’s ontology</definiendum>
				<definiens id="0">consists of 63 higherlevel concepts and distinguishes three types of entities</definiens>
			</definition>
</paper>

		<paper id="1503">
			<definition id="0">
				<sentence>The “Trans-European Language Resources Infrastructure” , TELRI ( http : //www.telri.de/ ) , is a pan-European alliance of focal national language ( technology ) institutions with the emphasis on Central and Eastern European and NIS countries .</sentence>
				<definiendum id="0">TELRI ( http</definiendum>
			</definition>
			<definition id="1">
				<sentence>A number of these goals is being served by the “TELRI Research Archive of Computational Tools and Resources” , TRACTOR , ( http : //www.tractor.de ) , which features monolingual , bilingual , and multilingual corpora and lexica in a wide variety of languages as well as corpusand lexicon-related software .</sentence>
				<definiendum id="0">TRACTOR , ( http</definiendum>
				<definiens id="0">features monolingual , bilingual , and multilingual corpora and lexica in a wide variety of languages as well as corpusand lexicon-related software</definiens>
			</definition>
			<definition id="2">
				<sentence>The Summer Institute of Linguistics ( http : //www.sil.org/computing/catalog/ ) also hosts a repository containing more than 60 pieces of software developed at SIL .</sentence>
				<definiendum id="0">Summer Institute of Linguistics ( http</definiendum>
				<definiens id="0">a repository containing more than 60 pieces of software developed at SIL</definiens>
			</definition>
			<definition id="3">
				<sentence>OLAC is an international project to construct an infrastructure aimed at opening the whole array of language resources , including texts , recordings , lexicons , annotations , software , protocols , models , and formats .</sentence>
				<definiendum id="0">OLAC</definiendum>
				<definiens id="0">recordings , lexicons , annotations , software , protocols , models , and formats</definiens>
			</definition>
			<definition id="4">
				<sentence>OLAC aims to develop community-specific metadata to link language archives and establish centralized catalogs .</sentence>
				<definiendum id="0">OLAC</definiendum>
				<definiens id="0">aims to develop community-specific metadata to link language archives and establish centralized catalogs</definiens>
			</definition>
			<definition id="5">
				<sentence>The form interface runs a Perl CGI script , which mails the output , encoded as the above described DocBook &lt; sect1 &gt; element , to the editors of the catalogue .</sentence>
				<definiendum id="0">Perl CGI script</definiendum>
				<definiens id="0">mails the output , encoded as the above described DocBook &lt; sect1 &gt; element</definiens>
			</definition>
			<definition id="6">
				<sentence>For presentation , we have been so far experimenting with the XML Stylesheet Language , XSL , or , more precisely , XSLT , the XSL Transformation Language , ( W3C , 2000 ) .</sentence>
				<definiendum id="0">XSLT</definiendum>
				<definiens id="0">experimenting with the XML Stylesheet Language , XSL , or , more precisely</definiens>
			</definition>
			<definition id="7">
				<sentence>XSLT is a recommendation of the W3C and is a language for transforming XML documents into other XML documents .</sentence>
				<definiendum id="0">XSLT</definiendum>
			</definition>
			<definition id="8">
				<sentence>The catalogue currently contains only a few sample entries , which , nevertheless , exemplify the kinds of software that are to be most relevant for inclusion into the catalogue : AF tools that at least one TELRI partner has experience in using and that the partner is willing to support for new users AF tools that are available free of cost , at least for academic purposes and , preferably , are open source AF tools that are language independent or adapt easily to new languages AF tools that are primarily meant for corpus processing At present , the catalogue lists the following tools : AF The morpho-syntactic tagger TnT ( Brants , 2000 ) A robust and very efficient statistical partof-speech tagger that is trainable on different languages and on virtually any tagset .</sentence>
				<definiendum id="0">AF The morpho-syntactic tagger TnT</definiendum>
				<definiens id="0">open source AF tools that are language independent or adapt easily to new languages AF tools that are primarily meant for corpus processing At present</definiens>
			</definition>
			<definition id="9">
				<sentence>The tool catalogue , as well as TRACTOR , could also be made a part of the Open Language Archives Community mentioned in the introduction .</sentence>
				<definiendum id="0">tool catalogue</definiendum>
				<definiens id="0">made a part of the Open Language Archives Community mentioned in the introduction</definiens>
			</definition>
</paper>

		<paper id="0521">
			<definition id="0">
				<sentence>This technique applies to any statistical model which estimates probabilities by backing o # 0B , that is , using probabilities from a less speci # 0Cc distribution when no data are available are available for the full distribution , as the following equations show for the general case : P # 28ejh # 29= P 1 # 28ejh # 29 if e 62 BO # 28h # 29 = # 0B # 28h # 29P 2 # 28ejh 0 # 29 if e 2 BO # 28h # 29 Here e is the event to be predicted , h is the set of conditioning events or history , # 0B is a backo # 0B weight , and h 0 is the subset of conditioning events used for the less speci # 0Cc backo # 0Bdistribution .</sentence>
				<definiendum id="0">h</definiendum>
				<definiens id="0">the event to be predicted ,</definiens>
			</definition>
</paper>

		<paper id="0504">
			<definition id="0">
				<sentence>Decision lists for lexical ambiguity resolution : Application to accent restoration in Spanish and French .</sentence>
				<definiendum id="0">Decision lists</definiendum>
			</definition>
</paper>

		<paper id="0507">
</paper>

		<paper id="1207">
			<definition id="0">
				<sentence>QALC exploits an analysis of documents based on the search for multi-word terms and their variations .</sentence>
				<definiendum id="0">QALC</definiendum>
			</definition>
			<definition id="1">
				<sentence>Question answering is thus an area of IR that calls for Natural Language Processing ( NLP ) techniques that can provide rich linguistic features as output .</sentence>
				<definiendum id="0">Question answering</definiendum>
				<definiens id="0">calls for Natural Language Processing ( NLP ) techniques that can provide rich linguistic features as output</definiens>
			</definition>
			<definition id="2">
				<sentence>In this paper , we describe how QALC uses high level indexes , made of terms and variants , to select among documents the most relevant ones with regard to a question , and then to match candidate answers with this question .</sentence>
				<definiendum id="0">QALC</definiendum>
				<definiens id="0">uses high level indexes , made of terms and variants , to select among documents the most relevant ones with regard to a question</definiens>
			</definition>
			<definition id="3">
				<sentence>Tagged Questions : Named entity tags Vocabulary &amp; frequencies Named entity recognition Candidate terms Retrieved documents Tagged sentences : named entity tags and term indexation Ordered sequences of 250 and 50 characters Question analysis Search engine Questions Subset of ranked documents Corpus Re-indexing and selection of documents ( FASTR ) Question/Sentence pairing Figure 1 .</sentence>
				<definiendum id="0">Tagged Questions</definiendum>
				<definiens id="0">Named entity tags Vocabulary &amp; frequencies Named entity recognition Candidate terms Retrieved documents Tagged sentences : named entity tags and term indexation Ordered sequences of 250 and 50 characters Question analysis Search engine Questions Subset of ranked documents Corpus Re-indexing and selection of documents ( FASTR ) Question/Sentence pairing Figure 1</definiens>
			</definition>
			<definition id="4">
				<sentence>A synset is a set of words that are synonymous for at least one of their meanings .</sentence>
				<definiendum id="0">synset</definiendum>
			</definition>
			<definition id="5">
				<sentence>For instance , the following pattern , named NtoSemArg , extracts the occurrence making many automobiles as a variant of the term car maker : VM ( 'maker ' ) RP ?</sentence>
				<definiendum id="0">NtoSemArg</definiendum>
				<definiens id="0">extracts the occurrence making many automobiles as a variant of the term car maker : VM ( 'maker '</definiens>
			</definition>
			<definition id="6">
				<sentence>( JJ�|�NN�|�NP |�VBD�|�VBG ) [ 0-3 ] NS ( 'car ' ) where RP are particles , PREP prepositions , ART articles , and VBD , VBG verbs .</sentence>
				<definiendum id="0">VBD</definiendum>
				<definiens id="0">'car ' ) where RP are particles , PREP prepositions , ART articles , and</definiens>
			</definition>
			<definition id="7">
				<sentence>Indeed , TREC organizers foresee a number of possible improvements for the future� : real-time answering , evaluation and justification of the answer , completeness of the answer which could result from answers distributed along multiple documents , and finally interactive question answering so that the user could specify her/his intention .</sentence>
				<definiendum id="0">TREC organizers</definiendum>
				<definiens id="0">real-time answering , evaluation and justification of the answer , completeness of the answer which could result from answers distributed along multiple documents , and finally interactive question answering so that the user could specify her/his intention</definiens>
			</definition>
</paper>

		<paper id="1410">
			<definition id="0">
				<sentence>Grammar Association is a technique for Machine Translation and Language Understanding introduced in 1993 by Vidal , Pieraccini and Levin .</sentence>
				<definiendum id="0">Grammar Association</definiendum>
				<definiens id="0">a technique for Machine Translation and Language Understanding introduced in 1993 by Vidal , Pieraccini and Levin</definiens>
			</definition>
			<definition id="1">
				<sentence>Basically , a Grammar Association system consists of three models : ( 1 ) an input grammar modelling the input language of the translation task ; ( 2 ) an output grammar modelling its output language ; ( 3 ) an association model describing how the use of certain elements ( rules ) of the input 1We view Language Understanding as a particular case of Machine Translation where the output language is aimed at representing the meaning of input sentences .</sentence>
				<definiendum id="0">Grammar Association system</definiendum>
			</definition>
			<definition id="2">
				<sentence>The ECGI algorithm ( Rulot and Vidal , 1987 ) is a heuristic technique for the inference of acyclic finite-state automata from positive samples , and determinism can be imposed a posteriori by a well-known transformation for regular grammars .</sentence>
				<definiendum id="0">ECGI algorithm</definiendum>
				<definiens id="0">a heuristic technique for the inference of acyclic finite-state automata from positive samples</definiens>
			</definition>
			<definition id="3">
				<sentence>Informally , ECGI works as follows .</sentence>
				<definiendum id="0">ECGI</definiendum>
			</definition>
			<definition id="4">
				<sentence>Nevertheless , ECGI chooses the solution in Figure 1 because it searches for just one path to be modified with a minimal number of new elements , and does not take into account combinations of different paths .</sentence>
				<definiendum id="0">ECGI</definiendum>
				<definiens id="0">chooses the solution in Figure 1 because it searches for just one path to be modified with a minimal number of new elements</definiens>
			</definition>
			<definition id="5">
				<sentence>6In IBM models , all words in the input sequence have the same influence in the random choice of output words ( model 1 ) or they have a relative influence depending on their positions ( model 2 ) .</sentence>
				<definiendum id="0">IBM models</definiendum>
				<definiens id="0">model 1 ) or they have a relative influence depending on their positions ( model 2 )</definiens>
			</definition>
			<definition id="6">
				<sentence>After exploring some similar alternatives ( and discarding them because of their poor results in a few translation experiments ) , Loco C was finally defined as explained below.7 The Loco C model assumes a random generation process ( of an output derivation , given an input one ) which begins with the starting symbol of the output grammar as the “current sentential form” and then , while the current sentential form contains a non-terminal , iteratively performs the following sequence of two random choices : in Choice 1 , one of the rules in the input derivation is chosen ; in Choice 2 , the non-terminal in the current sentential form is rewritten using a randomly chosen rule of the output grammar .</sentence>
				<definiendum id="0">Loco C</definiendum>
				<definiendum id="1">Loco C model</definiendum>
				<definiens id="0">begins with the starting symbol of the output grammar as the “current sentential form”</definiens>
				<definiens id="1">sentential form is rewritten using a randomly chosen rule of the output grammar</definiens>
			</definition>
			<definition id="7">
				<sentence>The corpus consists of pairs of sentences describing two-dimensional scenes with circles , squares and triangles in Spanish and English ( some examples can be found in Figure 4 , where the task is referred to as MLA Task ) .</sentence>
				<definiendum id="0">corpus</definiendum>
				<definiens id="0">consists of pairs of sentences describing two-dimensional scenes with circles , squares and triangles in Spanish</definiens>
			</definition>
			<definition id="8">
				<sentence>Since the MLA Task is an artificial task where each language can be exactly modelled by an acyclic finite-state automaton , we decided to use those exact automata in our systems in order to measure the impact of perfect language modelling .</sentence>
				<definiendum id="0">MLA Task</definiendum>
				<definiens id="0">an artificial task where each language</definiens>
			</definition>
</paper>

		<paper id="1312">
			<definition id="0">
				<sentence>The annotation process is decomposed into two steps : flagging a temporal expression in a document ( based on the presence of specific lexical trigger words ) , and identifying the time value that the expression designates , or that the speaker intends for it to designate .</sentence>
				<definiendum id="0">annotation process</definiendum>
				<definiens id="0">flagging a temporal expression in a document ( based on the presence of specific lexical trigger words ) , and identifying the time value that the expression designates , or that the speaker intends for it to designate</definiens>
			</definition>
</paper>

		<paper id="0700">
</paper>

		<paper id="1404">
			<definition id="0">
				<sentence>a28 a10 ) , satisfying the following : if a position a42 is mapped to a pair a2a44a43a45a5a8a7a32a43a46a10a14a20 , then the positions in the list a43a45a5a16a47a6a48 a42a50a49 a47a14a43a51a10 are in strictly increasing order ; we let “a47 ” denote listconcatenation , and a48a42a50a49 represents a list consisting of a single element a42 .</sentence>
				<definiendum id="0">a48a42a50a49</definiendum>
				<definiens id="0">a list consisting of a single element a42</definiens>
			</definition>
			<definition id="1">
				<sentence>If a3 is a string a71 a5a53a47a32a47a32a47 a71a73a72 , and a43 is a list a48a54a17a5a8a7a32a31a32a31a32a31a33a7a19a54a8a57 a49 of string positions in a3 , then a3a53a74a75a43 represents the string a71a35a76 a58 a47a32a47a32a47 a71a73a76a19a77 .</sentence>
				<definiendum id="0">a43</definiendum>
				<definiens id="0">a string a71 a5a53a47a32a47a32a47 a71a73a72 , and</definiens>
				<definiens id="1">a list a48a54a17a5a8a7a32a31a32a31a32a31a33a7a19a54a8a57 a49 of string positions in a3</definiens>
			</definition>
			<definition id="2">
				<sentence>Lastly , a18 is the union of a29a11a2a44a39a11a7a32a39a11a20 a34 and a subset of a28 a5a87a86 a28 a10 that relates positions in the two strings .</sentence>
				<definiendum id="0">a18</definiendum>
				<definiens id="0">the union of a29a11a2a44a39a11a7a32a39a11a20 a34 and a subset of a28 a5a87a86 a28 a10 that relates positions in the two strings</definiens>
			</definition>
			<definition id="3">
				<sentence>A ( context-free ) transduction grammar is a 5tuple a2a44a100a90a7a32a101a37a5a8a7a32a101a102a10a11a7a32a103a104a7a32a105a104a20 , where a100 is a finite set of nonterminals , a105a38a89a38a100 is the start symbol , a101a37a5 and a101a102a10 are the source and target alphabets , and a103 is a finite set of productions of the form a106a96a107 a2a44a108a13a7a19a109a56a20 , where a106 a89a110a100 , a108a111a89a110a2a44a100a81a36a56a101a68a5a60a20 a0 anda109a112a89a110a2a44a100a81a36a64a101a102a10a11a20 a0 , such that each nonterminal in a108 occurs exactly once in a109 and each nonterminal in a109 occurs exactly once in a108 .1 If we were to replace each RHS pair by only its first part a108 , we would obtain a context-free grammar for the source language , and if we were to replace each RHS pair by its second part a109 , we would obtain a context-free grammar for the target language .</sentence>
				<definiendum id="0">context-free ) transduction grammar</definiendum>
				<definiendum id="1">a100</definiendum>
				<definiendum id="2">a105a38a89a38a100</definiendum>
				<definiendum id="3">a103</definiendum>
				<definiens id="0">a finite set of nonterminals</definiens>
				<definiens id="1">the start symbol</definiens>
				<definiens id="2">a finite set of productions of the form a106a96a107 a2a44a108a13a7a19a109a56a20 , where a106 a89a110a100</definiens>
			</definition>
			<definition id="4">
				<sentence>The combination of the two halves of such a RHS indicates how a parse for 1Note that we ignore the case that a single nonterminal occurs twice or more in a113 or a114 ; if we were to include this case , some tedious complications of notation would result , without any theoretical gain such as an increase of generative power .</sentence>
				<definiendum id="0">RHS</definiendum>
				<definiens id="0">indicates how a parse for 1Note that we ignore the case that a single nonterminal occurs twice or more in a113 or a114</definiens>
			</definition>
			<definition id="5">
				<sentence>Due to this property , we may write each nonterminal as a106 a48a71 a7a32a118 a49 to indicate that it is mapped to the pair a2 a71 a7a32a118a8a20 , where a71 a89a26a101a37a5 and a118a61a89a26a101a102a10 , where a106 is a so called delexicalized nonterminal .</sentence>
				<definiendum id="0">a106</definiendum>
				<definiens id="0">a so called delexicalized nonterminal</definiens>
			</definition>
			<definition id="6">
				<sentence>We may write a105 as a106 a48a119a120a7a32a119 a49 , where a119 is a dummy symbol at the dummy string position a39 .</sentence>
				<definiendum id="0">a119</definiendum>
				<definiens id="0">a dummy symbol at the dummy string position a39</definiens>
			</definition>
			<definition id="7">
				<sentence>The initial state of the automaton is a82a167a166 and the only final state is a82 a97a166 , where a105 is the start symbol of the grammar .</sentence>
				<definiendum id="0">a105</definiendum>
				<definiens id="0">the start symbol of the grammar</definiens>
			</definition>
			<definition id="8">
				<sentence>For a pair of strings , the edit distance is defined as the minimum number of substitutions , insertions and deletions needed to turn one string into the other .</sentence>
				<definiendum id="0">edit distance</definiendum>
			</definition>
			<definition id="9">
				<sentence>The word accuracy of a string a3 with regard to a string a173 is defined to be a30a13a83a175a174 a72 , where a125 is the edit distance between a3 and a173 and a22 is the length of a173 .</sentence>
				<definiendum id="0">a125</definiendum>
				<definiens id="0">the edit distance between a3 and a173 and a22 is the length of a173</definiens>
			</definition>
			<definition id="10">
				<sentence>Determinization for the largest automata indicated in the Figure took more than 24 hours for both fa ( 2 ) and robust fa ( 2 ) , which suggests these methods become unrealistic for training corpus sizes considerably larger than 10,000 bitexts .</sentence>
				<definiendum id="0">Determinization</definiendum>
				<definiens id="0">suggests these methods become unrealistic for training corpus sizes considerably larger than 10,000 bitexts</definiens>
			</definition>
</paper>

		<paper id="1205">
			<definition id="0">
				<sentence>`` Minimal Recursion Semantics : an Introduction '' .</sentence>
				<definiendum id="0">Minimal Recursion Semantics</definiendum>
				<definiens id="0">an Introduction ''</definiens>
			</definition>
</paper>

		<paper id="1407">
			<definition id="0">
				<sentence>The training set consists of 58 322 sentence pairs .</sentence>
				<definiendum id="0">training set</definiendum>
			</definition>
			<definition id="1">
				<sentence>Singletons are types occurring only once in training .</sentence>
				<definiendum id="0">Singletons</definiendum>
				<definiens id="0">types occurring only once in training</definiens>
			</definition>
</paper>

		<paper id="0808">
			<definition id="0">
				<sentence>This tree is input to the Logical Form module , which produces a deep syntactic representation of the input sentence , called the LF ( Heidorn , G. E. , 2000 ) .</sentence>
				<definiendum id="0">LF</definiendum>
				<definiens id="0">produces a deep syntactic representation of the input sentence , called the</definiens>
			</definition>
			<definition id="1">
				<sentence>The LF uses the same basic set of relation types for all languages .</sentence>
				<definiendum id="0">LF</definiendum>
			</definition>
			<definition id="2">
				<sentence>Tree LF Figure 1 The LF is the final output of the analysis phase and the input to the transfer phase .</sentence>
				<definiendum id="0">LF</definiendum>
			</definition>
			<definition id="3">
				<sentence>Transfer extracts a set of mappings from the source-target language MindNet ( Richardson , 2000 ) , a translation knowledge database , and applies these mappings to the LF of the source sentence to produce a target LF .</sentence>
				<definiendum id="0">Transfer</definiendum>
			</definition>
			<definition id="4">
				<sentence>The translation MindNet for a language pair is a repository of aligned LFs and portions of LFs ( produced by analyzing sentence-aligned corpora ) .</sentence>
				<definiendum id="0">translation MindNet</definiendum>
				<definiens id="0">a repository of aligned LFs and portions of LFs ( produced by analyzing sentence-aligned corpora )</definiens>
			</definition>
			<definition id="5">
				<sentence>An alignment of two LFs is a set of mappings between a node or set of nodes ( and the relations between them ) in the source LF and a node or set of nodes ( and the relations between them ) in the target LF ( Menezes &amp; Richardson , 2001 ) .</sentence>
				<definiendum id="0">alignment of two LFs</definiendum>
			</definition>
			<definition id="6">
				<sentence>In the translation process , the transfer component searches the alignments in the MindNet for those that match portions of the LF of the sentence being translated .</sentence>
				<definiendum id="0">translation process</definiendum>
			</definition>
			<definition id="7">
				<sentence>The target LF fragments from the transfer mappings and dictionary mappings are stitched together to produce the target LF ( Menezes &amp; Richardson , 2001 ) .</sentence>
				<definiendum id="0">LF</definiendum>
				<definiens id="0">fragments from the transfer mappings and dictionary mappings</definiens>
			</definition>
			<definition id="8">
				<sentence>Transferred Spanish LF : Transferred Japanese LF : Transferred Chinese LF : Figure 2 The transferred LF is the input to the generation component , which we will discuss in detail below .</sentence>
				<definiendum id="0">LF</definiendum>
				<definiens id="0">the input to the generation component</definiens>
			</definition>
			<definition id="9">
				<sentence>We start with the English generation component , which was used in experimental question-answering applications before being used in MT. Among the pre-generation rules in this component is one that removes the marker indicating non-restrictive modification ( Nonrest ) from LF nodes that are not in a modification relationship to another LF node .</sentence>
				<definiendum id="0">generation component</definiendum>
				<definiens id="0">one that removes the marker indicating non-restrictive modification ( Nonrest ) from LF nodes that are not in a modification relationship to another LF node</definiens>
			</definition>
			<definition id="10">
				<sentence>Figure 6 The LF that is the input to generation in this example is a portion of the LF representation of a complete sentence that includes the phrase “Hitler , who came to power in 1933.”</sentence>
				<definiendum id="0">LF</definiendum>
			</definition>
</paper>

		<paper id="1413">
			<definition id="0">
				<sentence>‘WCRP’ is the acronym for ‘World Climate Research Programme’ , which is the 9th candidate and is translated to ‘a67a39a68a38a63a39a64a38a65a39a66a35a69a59a70 ’ which includes the original Japanese query .</sentence>
				<definiendum id="0">‘WCRP’</definiendum>
				<definiens id="0">the acronym for ‘World Climate Research Programme’ , which is the 9th candidate and is translated to ‘a67a39a68a38a63a39a64a38a65a39a66a35a69a59a70 ’ which includes the original Japanese query</definiens>
			</definition>
			<definition id="1">
				<sentence>Practically speaking , the factor that most influences the accuracy of the term translation extractor is the set of documents returned from the search engine .</sentence>
				<definiendum id="0">extractor</definiendum>
				<definiens id="0">the accuracy of the term translation</definiens>
			</definition>
</paper>

		<paper id="0717">
</paper>

		<paper id="0506">
			<definition id="0">
				<sentence>Stacked generalization ( Wolpert , 1992 ) , or stacking , is an approach for constructing classifier ensembles .</sentence>
				<definiendum id="0">Stacked generalization</definiendum>
			</definition>
			<definition id="1">
				<sentence>A classifier ensemble , or committee , is a set of classifiers whose individual decisions are combined in some way to classify new instances ( Dietterich , 1997 ) .</sentence>
				<definiendum id="0">classifier ensemble</definiendum>
			</definition>
			<definition id="2">
				<sentence>To measure the performance of a filter , weighted accuracy ( WAcc ) and its complementary weighted error rate ( WErr = 1 – WAcc ) are used ( Androutsopoulos , et al. 2000a , b , c ; Sakkis , et al. 2001 ) : SL SSLL NN NN WAcc +⋅λ +⋅λ = →→ where ZY N → is the number of messages in category Y that the filter classified as Z , SLLLL NNN →→ += , LSSSS NNN →→ += .</sentence>
				<definiendum id="0">weighted accuracy</definiendum>
				<definiendum id="1">WAcc</definiendum>
				<definiendum id="2">ZY N →</definiendum>
				<definiens id="0">the number of messages in category Y that the filter classified as Z , SLLLL NNN →→ +=</definiens>
			</definition>
			<definition id="3">
				<sentence>Our evaluation measures also include spam recall ( SR ) and spam precision ( SP ) : LSSS SS NN N SR →→ → + = SLSS SS NN N SP →→ → + = SR measures the percentage of spam messages that the filter blocks ( intuitively , its effectiveness ) , while SP measures how many blocked messages are indeed spam ( its safety ) .</sentence>
				<definiendum id="0">spam precision</definiendum>
				<definiens id="0">SP ) : LSSS SS NN N SR →→ → + = SLSS SS NN N SP →→ → + = SR measures the percentage of spam messages that the filter blocks ( intuitively , its effectiveness ) , while SP measures how many blocked messages</definiens>
			</definition>
			<definition id="4">
				<sentence>The k nearest neighbors i x G26 of x G26 are considered : ∑ ∑ = =− δ = k i i k i ii NNk S xxd xxdxCspam xW 1 3 1 3 ) , ( 1 ) , ( ) ) ( , ( ) ( G26G26 G26G26G26 G26 , where ) ( i xC G26 is the category of neighbor i x G26 , ) , ( ji xxd G26G26 is the distance between i x G26 and j x G26 , and 1 ) , ( 21 =ccδ , if 21 cc = , and 0 otherwise .</sentence>
				<definiendum id="0">G26G26</definiendum>
				<definiens id="0">the distance between i x G26 and j x G26 , and 1</definiens>
			</definition>
</paper>

		<paper id="1506">
			<definition id="0">
				<sentence>The Dublin Core consists of 15 metadata elements , where each element is optional and repeatable : Title , Creator , Subject , Description , Publisher , Contributor , Date , Type , Format , Identifier , Source , Language , Relation , Coverage , Rights .</sentence>
				<definiendum id="0">Dublin Core</definiendum>
			</definition>
			<definition id="1">
				<sentence>A refinement makes the meaning of the element more specific .</sentence>
				<definiendum id="0">refinement</definiendum>
				<definiens id="0">makes the meaning of the element more specific</definiens>
			</definition>
			<definition id="2">
				<sentence>An OAI “service provider” is a third party that provides end-user services ( such as search functions over union catalogs ) based on metadata harvested from one or more OAI data providers .</sentence>
				<definiendum id="0">OAI “service provider”</definiendum>
			</definition>
			<definition id="3">
				<sentence>Resources The OLAC Metadata Set extends the Dublin Core set only to the minimum degree required to express basic properties of language resources which are useful as finding aids .</sentence>
				<definiendum id="0">OLAC Metadata Set</definiendum>
				<definiens id="0">extends the Dublin Core set only to the minimum degree required to express basic properties of language resources which are useful as finding aids</definiens>
			</definition>
			<definition id="4">
				<sentence>The Extensible Markup Language ( XML ) is the universal format for structured documents and data on the Web [ www.w3.org/XML ] .Thekey building block of an XML document is the element .</sentence>
				<definiendum id="0">Extensible Markup Language</definiendum>
				<definiendum id="1">XML )</definiendum>
				<definiendum id="2">XML document</definiendum>
				<definiens id="0">the universal format for structured documents and data on the Web [ www.w3.org/XML ] .Thekey building block of an</definiens>
				<definiens id="1">the element</definiens>
			</definition>
			<definition id="5">
				<sentence>XML Document Type Definitions ( DTDs ) and XML schemas are grammars that define the structure of a valid XML document , and they limit the arrangement of XML elements in a document .</sentence>
				<definiendum id="0">XML Document Type Definitions</definiendum>
				<definiens id="0">DTDs ) and XML schemas are grammars that define the structure of a valid XML document , and they limit the arrangement of XML elements in a document</definiens>
			</definition>
			<definition id="6">
				<sentence>language : Bulgarian Description : 67,500 entries divided into 242 inflectional types ( including proper nouns ) , morphosyntactic information for each entry , and a morphological engine ( MS DOS and WINDOWS 95/NT ) for morphological analysis and generation Identifier : http : //www.icp.inpg.fr/ELRA/cata/text det.html # bulmodic oai : dfki : KPML Title : KPML Creator : Bateman and many others Subject .</sentence>
				<definiendum id="0">MS DOS</definiendum>
				<definiens id="0">Bulgarian Description : 67,500 entries divided into 242 inflectional types ( including proper nouns ) , morphosyntactic information for each entry</definiens>
			</definition>
			<definition id="7">
				<sentence>Contributor : An entity responsible for making contributions to the content of the resource .</sentence>
				<definiendum id="0">Contributor</definiendum>
				<definiens id="0">An entity responsible for making contributions to the content of the resource</definiens>
			</definition>
			<definition id="8">
				<sentence>Coverage : The extent or scope of the content of the resource .</sentence>
				<definiendum id="0">Coverage</definiendum>
				<definiens id="0">The extent or scope of the content of the resource</definiens>
			</definition>
			<definition id="9">
				<sentence>Creator : An entity primarily responsible for making the content of the resource .</sentence>
				<definiendum id="0">Creator</definiendum>
				<definiens id="0">An entity primarily responsible for making the content of the resource</definiens>
			</definition>
			<definition id="10">
				<sentence>Description : An account of the content of the resource .</sentence>
				<definiendum id="0">Description</definiendum>
				<definiens id="0">An account of the content of the resource</definiens>
			</definition>
			<definition id="11">
				<sentence>encoding : An encoded character set used by a digital resource .</sentence>
				<definiendum id="0">encoding</definiendum>
				<definiens id="0">An encoded character set used by a digital resource</definiens>
			</definition>
			<definition id="12">
				<sentence>Language : A language of the intellectual content of the resource .</sentence>
				<definiendum id="0">Language</definiendum>
				<definiens id="0">A language of the intellectual content of the resource</definiens>
			</definition>
			<definition id="13">
				<sentence>Publisher : An entity responsible for making the resource available .</sentence>
				<definiendum id="0">Publisher</definiendum>
				<definiens id="0">An entity responsible for making the resource available</definiens>
			</definition>
			<definition id="14">
				<sentence>Type : The nature or genre of the content of the resource .</sentence>
				<definiendum id="0">Type</definiendum>
				<definiens id="0">The nature or genre of the content of the resource</definiens>
			</definition>
			<definition id="15">
				<sentence>Type includes terms describing general categories , functions , genres , or aggregation levels for content .</sentence>
				<definiendum id="0">Type</definiendum>
				<definiens id="0">includes terms describing general categories , functions , genres , or aggregation levels for content</definiens>
			</definition>
			<definition id="16">
				<sentence>Language identification is an important dimension of language resource classification .</sentence>
				<definiendum id="0">Language identification</definiendum>
				<definiens id="0">an important dimension of language resource classification</definiens>
			</definition>
			<definition id="17">
				<sentence>A transcription is any timeordered symbolic representation of a linguistic event .</sentence>
				<definiendum id="0">transcription</definiendum>
				<definiens id="0">any timeordered symbolic representation of a linguistic event</definiens>
			</definition>
			<definition id="18">
				<sentence>An annotation is any kind of structured linguistic information that is explicitly aligned to some spatial and/or temporal extent of a linguistic record ( such as a recorded signal or an image ) .</sentence>
				<definiendum id="0">annotation</definiendum>
				<definiens id="0">any kind of structured linguistic information that is explicitly aligned to some spatial and/or temporal extent of a linguistic record ( such as a recorded signal or an image )</definiens>
			</definition>
			<definition id="19">
				<sentence>OLAC-CPU : A vocabulary for identifying the CPU ( s ) for which the software is available , in the case of binary distributions : x86 , mips , alpha , ppc , sparc , 680x0 .</sentence>
				<definiendum id="0">OLAC-CPU</definiendum>
				<definiens id="0">A vocabulary for identifying the CPU</definiens>
			</definition>
			<definition id="20">
				<sentence>OLAC-Encoding : A vocabulary for identifying the character encoding used by a digital resource , e.g. iso-8859-1 , ... &lt; ? xml version= '' 1.0 '' encoding= '' UTF-8 '' ? &gt; &lt; olac xmlns= '' http : //www.language-archives.orgLAC/0.3/ '' xmlns : xsi= '' http : //www.w3.org/2001/XMLSchema-instance '' xsi : schemaLocation= '' http : //www.language-archives.orgLAC/0.3/ http : //www.language-archives.orgLAC/olac-0.3b1.xsd '' &gt; &lt; Title &gt; KPML &lt; /Title &gt; &lt; Identifier &gt; http : //www.purl.org/net/kpml/ &lt; /Identifier &gt; &lt; Creator refine= '' Author '' &gt; Bateman , John &lt; /Creator &gt; &lt; Subject.language code= '' es '' / &gt; &lt; Subject.language code= '' ru '' / &gt; &lt; Subject.language code= '' ja '' / &gt; &lt; Subject.language code= '' el '' / &gt; &lt; Subject.language code= '' de '' / &gt; &lt; Subject.language code= '' fr '' / &gt; &lt; Subject.language code= '' en '' / &gt; &lt; Subject.language code= '' cs '' / &gt; &lt; Subject.language code= '' bg '' / &gt; &lt; Format.os code= '' MSWindows/winNT '' / &gt; &lt; Format.os code= '' MSWindows/win95 '' / &gt; &lt; Format.os code= '' MSWindows/win98 '' / &gt; &lt; Format.os code= '' Unix/Solaris '' / &gt; &lt; Type.functionality &gt; Annotation Tools , Grammars , Lexica , Development Tools , Formalisms , Theories , Deep Generation , Morphological Generation , Shallow Generation &lt; /type.functionality &gt; &lt; Relation refine= '' Requires '' &gt; Windows : none ; Solaris : CommonLisp + CLIM &lt; /Relation &gt; &lt; Description &gt; Natural Language Generation Linguistic Resource Development and Maintenance workbench for large scale generation grammar development , teaching , and experimental generation .</sentence>
				<definiendum id="0">OLAC-Encoding</definiendum>
				<definiens id="0">A vocabulary for identifying the character encoding used by a digital resource</definiens>
			</definition>
			<definition id="21">
				<sentence>OLAC-Functionality : A vocabulary for classifying the functionality of software , again using the MIME style of representation , and using the HLT Survey as a source of categories ( Cole , 1997 ) as advocated by the ACL/DFKI Natural Language Software Registry .</sentence>
				<definiendum id="0">OLAC-Functionality</definiendum>
				<definiendum id="1">HLT Survey</definiendum>
				<definiens id="0">A vocabulary for classifying the functionality of software</definiens>
				<definiens id="1">as a source of categories ( Cole , 1997 ) as advocated by the ACL/DFKI Natural Language Software Registry</definiens>
			</definition>
			<definition id="22">
				<sentence>OLAC-Rights : A vocabulary for classifying the rights held over a resource , e.g. : open , restricted , ... OLAC-Role : A vocabulary for identifying the role of a contributor or creator of the resource , e.g. : author , editor , translator , transcriber , sponsor , ... OLAC-Software-Rights : A vocabulary for classifying the rights held over a resource , e.g. : open-source , royalty-free-library , royaltyfree-binary , commercial , ... OLAC-Sourcecode : A vocabulary for identifying the programming language ( s ) used by software which is distributed in source form , e.g. : C++ , Java , Python , Tcl , VB , ... The OLAC metadata format consists of an XML schema for the element set , and a set of schemas for the controlled vocabularies .</sentence>
				<definiendum id="0">OLAC-Rights</definiendum>
				<definiendum id="1">OLAC metadata format</definiendum>
				<definiens id="0">A vocabulary for classifying the rights held over a resource , e.g. : open , restricted , ... OLAC-Role : A vocabulary for identifying the role of a contributor or creator of the resource , e.g. : author , editor , translator , transcriber , sponsor , ... OLAC-Software-Rights : A vocabulary for classifying the rights held over a resource</definiens>
				<definiens id="1">open-source , royalty-free-library , royaltyfree-binary , commercial , ... OLAC-Sourcecode : A vocabulary for identifying the programming language ( s ) used by software which is distributed in source form , e.g. : C++ , Java , Python , Tcl , VB</definiens>
				<definiens id="2">consists of an XML schema for the element set , and a set of schemas for the controlled vocabularies</definiens>
			</definition>
</paper>

		<paper id="1307">
			<definition id="0">
				<sentence>SDRT is a non trivial extension of DRT that takes discourse structure into account and offers a theory of the semantics/pragmatics interface .</sentence>
				<definiendum id="0">SDRT</definiendum>
			</definition>
			<definition id="1">
				<sentence>An SDRS is a recursive structure consisting of elementary DRSs ( i.e. , DRSs representing a single clause ) and sub-SDRSs linked together by Discourse Relations , such as Narration , Elaboration , Background , Continuation , Result , Contrast , Explanation. . . These elementary DRSs and the subSDRSs corresponding to complex discourse segments are the constituents of the SDRS representing the discourse .</sentence>
				<definiendum id="0">SDRS</definiendum>
				<definiens id="0">a recursive structure consisting of elementary DRSs ( i.e. , DRSs representing a single clause</definiens>
			</definition>
			<definition id="2">
				<sentence>SDRT defines a “Glue Logic '' and an “Update Function '' that together determine a new SDRS for a given SDRS a3 representing the context ( the discourse already processed ) , and a new constituent a4 representing the information to be integrated into that context .</sentence>
				<definiendum id="0">SDRT</definiendum>
				<definiens id="0">defines a “Glue Logic '' and an “Update Function '' that together determine a new SDRS for a given SDRS a3 representing the context ( the discourse already processed ) , and a new constituent a4 representing the information to be integrated into that context</definiens>
			</definition>
			<definition id="3">
				<sentence>The Glue Logic is specified by : a7 definitions characterizing which constituents in a3 are open for attaching a4 , a7 axioms detailing what discourse relations may be inferred , on the basis of a variety of linguistic and common knowledge clues , in order to actualize the attachment of a4 to some open constituent of a3 , a7 axioms specifying the semantic effects of those discourse relations .</sentence>
				<definiendum id="0">Glue Logic</definiendum>
				<definiens id="0">open for attaching a4 , a7 axioms detailing what discourse relations may be inferred , on the basis of a variety of linguistic and common knowledge clues</definiens>
			</definition>
			<definition id="4">
				<sentence>Narration is a relation which is based on the Gricean pragmatic constraint of orderliness .</sentence>
				<definiendum id="0">Narration</definiendum>
				<definiens id="0">a relation which is based on the Gricean pragmatic constraint of orderliness</definiens>
			</definition>
			<definition id="5">
				<sentence>3a46a48a47a50a49a52a51a54a53a38a55a56a49a52a57a59a58 a60a54a61a62a49a35a55a56a49a52a57a64a63a66a65a67a49a68a63a66a65a69a60a25a70a29a71a72a53a30a55a56a49a52a57 , where a63a54a65 denotes temporal abutment , as used in DRT , or the “meets” relation as used in Allen’s theory ( Allen , ) .</sentence>
				<definiendum id="0">a63a54a65</definiendum>
			</definition>
			<definition id="6">
				<sentence>So we propose here an improved version of ( A1 ) , where a75 denotes the “intersection” operator4 : A 2 Narrationa28a30a4a40a39a41a17a43a32a48a5 a18 a19a69a76a78a77 a28 posta28a30a18 a19 a32a79a75 prea28a30a18a27a34a37a32a41a32 a76a78a77 a18a35a34 Narration has another semantic effect .</sentence>
				<definiendum id="0">a75</definiendum>
				<definiens id="0">the “intersection” operator4 : A 2 Narrationa28a30a4a40a39a41a17a43a32a48a5 a18 a19a69a76a78a77 a28 posta28a30a18 a19 a32a79a75 prea28a30a18a27a34a37a32a41a32 a76a78a77 a18a35a34 Narration has another semantic effect</definiens>
			</definition>
			<definition id="7">
				<sentence>A 3 a28a62a80 a3 a39a50a4a81a39a72a17a42a82 a83 Narrationa28a30a4a40a39a41a17a42a32a41a32 a5 a84a86a85 a28 Contingenta28 a85 a32a64a83 a85a59a87 a4a88a83 a85a89a87 a17a42a32 where a80 a3 a39a52a4a40a39a41a17a43a82 means that a17 is to be attached to a4 in the SDRS a3 , and a87 is a subordinating discourse relation whose semantics essentially involves subsumption between the topic and the elements of the narrative it summarizes .</sentence>
				<definiendum id="0">a87</definiendum>
			</definition>
			<definition id="8">
				<sentence>In ( Bras et al. , 2001 ) , we argue that puis is a rhetorical marker which introduces a relation of Narration : A 12 a28a62a80 a3 a39a52a4a40a39a41a17a42a82a95a83a97a107 puisa112a114a17a42a32a48a5 Narrationa28a30a4a40a39a41a17a42a32 In both ( 5-a ) and ( 5-b ) , Narration will be inferred , by default with ( A4 ) for ( 5-a ) , thanks to puis with ( A12 ) for ( 5-b ) .</sentence>
				<definiendum id="0">puis</definiendum>
				<definiendum id="1">Narration</definiendum>
				<definiens id="0">a rhetorical marker which introduces a relation of</definiens>
			</definition>
</paper>

	</volume>
