<?xml version="1.0" encoding="UTF-8"?>
	<volume id="C00">

		<paper id="2106">
			<definition id="0">
				<sentence>q'ular righ , t part grammars ) are a generalization of context-free grammars ( CFG ) in which a grammar production specifies a regular set of sequences of subconstituents of its left-haM side instead of a fixed sequence of subconstituents .</sentence>
				<definiendum id="0">CFG</definiendum>
				<definiens id="0">a generalization of context-free grammars</definiens>
			</definition>
			<definition id="1">
				<sentence>ID/LP grammars are a variant of CFG 's that were introduced into linguistic tbrmalisms to encode word order generalizations ( Gazdar et al. , 1985 ) .</sentence>
				<definiendum id="0">ID/LP grammars</definiendum>
			</definition>
			<definition id="2">
				<sentence>Wc denote nonterminal symbols with A , B , terminal symbols with a , terminal and nonterminal symbols with X , states with F , strings of symbols with/3 , % and the empty string with c. An STG is defined as tbllows : Definition 1 ( ST ( \ ] ) .</sentence>
				<definiendum id="0">Wc</definiendum>
				<definiendum id="1">STG</definiendum>
				<definiendum id="2">ST</definiendum>
				<definiens id="0">denote nonterminal symbols with A , B , terminal symbols with a , terminal and nonterminal symbols with X , states with F , strings of symbols with/3 , % and the empty string with c. An</definiens>
			</definition>
			<definition id="3">
				<sentence>Art STG G is a tuple ( N , E , A~ , AJ l ; ' , He , P , S ) where • N is a finite set of nonterminal symbols , • E is a finite set of terminal symbols , • A/I is a finite set of states , , , A.4\ ] ; , c_ .</sentence>
				<definiendum id="0">Art STG G</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">E</definiendum>
				<definiendum id="3">A/I</definiendum>
				<definiens id="0">a tuple ( N , E , A~ , AJ l ; ' , He , P , S ) where •</definiens>
				<definiens id="1">a finite set of nonterminal symbols</definiens>
				<definiens id="2">a finite set of terminal symbols</definiens>
				<definiens id="3">a finite set of states</definiens>
			</definition>
			<definition id="4">
				<sentence>A4 is a set of final states , , , Ha c ( .</sentence>
				<definiendum id="0">A4</definiendum>
				<definiendum id="1">Ha c</definiendum>
				<definiens id="0">a set of final states</definiens>
			</definition>
			<definition id="5">
				<sentence>A C N ; uid ~4 is ~ multiset over V , and LP is a set ( ) f line~r l ) re ( : edence constraints .</sentence>
				<definiendum id="0">LP</definiendum>
				<definiens id="0">a set ( ) f line~r l ) re ( : edence constraints</definiens>
			</definition>
			<definition id="6">
				<sentence>We write fl ~ LP to denote that the sl ; ring fi s~d ; isties all the constraints in l , P. The derivation r ( ; l~| ; ion is defined by 7A5 ~ 7\ [ 3d i1\ [ fl = X~ ... X~ and a &gt; { X~ , ... , Xk } ~ 1 '' mM fl ~ LI ' .</sentence>
				<definiendum id="0">mM</definiendum>
				<definiens id="0">isties all the constraints in l</definiens>
				<definiens id="1">X~ and a &gt; { X~ , ... , Xk } ~ 1 ''</definiens>
			</definition>
			<definition id="7">
				<sentence>A parsing system consists of ~ finite set Z of pars ( ; items , a finite set `` H of hyt ) otheses , whi ( : h ell ( : ( ) ( \ ] ( ; the input string , mxd ~ finite set 29 of deduction stel ) s of the fbrm x~ , ... , x , ta : where xi C 2 ; U ~ and x E Z. The hypotheses can be represented as deduction steps with empty prenfises , so we can assume that , all xi m'e it ; eros , and represent a parsing system as a pair ( Z , 29 ) .</sentence>
				<definiendum id="0">parsing system</definiendum>
				<definiens id="0">input string , mxd ~ finite set 29 of deduction stel ) s of the fbrm x~ , ... , x , ta : where xi C 2</definiens>
			</definition>
			<definition id="8">
				<sentence>The fbllowing theorem is a generalization of the definition of the semantics of Earley items for CFG 's ( Sikkel , 1993 ) ( al..</sentence>
				<definiendum id="0">fbllowing theorem</definiendum>
				<definiens id="0">a generalization of the definition of the semantics of Earley items for CFG 's</definiens>
			</definition>
			<definition id="9">
				<sentence>\ ] o ' A A A\ [ q ' where X is a propositional variable and \ [ $ \ ] , \ [ -- &gt; \ ] are the dnal operators to ( .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a propositional variable and \</definiens>
			</definition>
			<definition id="10">
				<sentence>We have defined state transition grammars ( STG ) as an intermediate formalism between grammars and parsing algorithnls .</sentence>
				<definiendum id="0">state transition grammars</definiendum>
				<definiendum id="1">STG</definiendum>
				<definiens id="0">an intermediate formalism between grammars and parsing algorithnls</definiens>
			</definition>
			<definition id="11">
				<sentence>In particular , we have discussed extended contextfree grammars ( ECFG ) , II ) /LP grammars , and grammars in which admissible trees are delined by means of local tree ( 'onstraints cxI ) resscd in a simple logical language .</sentence>
				<definiendum id="0">contextfree grammars</definiendum>
				<definiendum id="1">ECFG</definiendum>
				<definiens id="0">'onstraints cxI ) resscd in a simple logical language</definiens>
			</definition>
</paper>

		<paper id="2139">
			<definition id="0">
				<sentence>ABL remembers all possible constituents , building a search space .</sentence>
				<definiendum id="0">ABL</definiendum>
				<definiens id="0">remembers all possible constituents , building a search space</definiens>
			</definition>
			<definition id="1">
				<sentence>The algorithm consists of two steps : The model learns by comparing all sentences in the intmt corpus to each other in pairs .</sentence>
				<definiendum id="0">algorithm</definiendum>
				<definiens id="0">consists of two steps : The model learns by comparing all sentences in the intmt corpus to each other in pairs</definiens>
			</definition>
			<definition id="2">
				<sentence>Ple , f ( c ) = \ ] c ' C C : yield ( c ' ) = yicld ( c ) l ICI where C is the entire set : of constituents .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">the entire set : of constituents</definiens>
			</definition>
			<definition id="3">
				<sentence>The ATIS corlms ti'om the P ( ; nn Treebank consists of 716 sentences containing 11,777 ( : onstituents .</sentence>
				<definiendum id="0">ATIS corlms</definiendum>
			</definition>
			<definition id="4">
				<sentence>The merging of the non-terminals m~y occur anywhere in the cortms , sin ( : e all merged non-terndnals are ut ) dated .</sentence>
				<definiendum id="0">sin</definiendum>
				<definiens id="0">e all merged non-terndnals are ut ) dated</definiens>
			</definition>
</paper>

		<paper id="2123">
			<definition id="0">
				<sentence>Pr ( fflel ) } ( 1 ) The argmax operation denotes the search problem , i.e. the generation of the output sentence in the target language .</sentence>
				<definiendum id="0">argmax operation</definiendum>
				<definiens id="0">the search problem , i.e. the generation of the output sentence in the target language</definiens>
			</definition>
			<definition id="1">
				<sentence>Pr ( c~ ) is the language model of tim target language , whereas Pr ( fi'le*l ) is the translation model .</sentence>
				<definiendum id="0">Pr ( c~ )</definiendum>
				<definiens id="0">the language model of tim target language , whereas Pr ( fi'le*l ) is the translation model</definiens>
			</definition>
			<definition id="2">
				<sentence>An inverted alignment is defined as follows : inverted alignment : i -+ j = bi .</sentence>
				<definiendum id="0">inverted alignment</definiendum>
			</definition>
			<definition id="3">
				<sentence>The sentence length probability p ( J\ [ 1 ) is omitted without any loss in pertbrmance .</sentence>
				<definiendum id="0">sentence length probability p</definiendum>
				<definiens id="0">omitted without any loss in pertbrmance</definiens>
			</definition>
			<definition id="4">
				<sentence>A modified language inodel probability pa ( e\ [ c ' , c '' ) is defined as follows : Pa ( ele '' e '' ) = p ( e\ ] e ' , e '' ) ifa = 1 We associate a distribution p ( 5 ) with the two cases 5 = 0 and 5 = 1 and set p ( 5 = 1 ) = 0.7 .</sentence>
				<definiendum id="0">modified language inodel probability pa</definiendum>
				<definiens id="0">follows : Pa ( ele '' e '' ) = p ( e\ ] e ' , e '' ) ifa = 1 We associate a distribution p ( 5 ) with the two cases 5 = 0 and 5 = 1 and set p ( 5 = 1 ) = 0.7</definiens>
			</definition>
			<definition id="5">
				<sentence>The complexity of the algorithm is O ( E a • j2.2.1 ) , where E is the size of the target language vocabulary .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">the size of the target language vocabulary</definiens>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>Itere , we draw a distinction between text structure elements ( TSEs ) as the elements from which a ( task-oriented ) text , is built ut ) , and text templates ' , which condition the way TSEs are to be realized linguistically .</sentence>
				<definiendum id="0">TSEs</definiendum>
				<definiens id="0">built ut ) , and text templates ' , which condition the way TSEs are to be realized linguistically</definiens>
			</definition>
			<definition id="1">
				<sentence>Figure 3 : Text Structure Elements ( TSEs ) The TSEs are placed in correspondence with the configurational concet ) ts of the DM ( cf. Figure 2 ) ; this enat ) les us to lmild a text stru ( 'ture l ; hat folh ) ws the structuring of the content in an A-1 ) ox ( cf. Figure 4 ) .</sentence>
				<definiendum id="0">Text Structure Elements ( TSEs</definiendum>
				<definiens id="0">'ture l ; hat folh ) ws the structuring of the content in an A-1</definiens>
			</definition>
			<definition id="2">
				<sentence>Divers ( ; constraints can be iraposed depending on the user 's choice of style , e.g. , personal ( featuring ppredominantly imperatives ) vs. impersonal ( tbaturing indicatives ) .</sentence>
				<definiendum id="0">Divers</definiendum>
				<definiens id="0">the user 's choice of style , e.g. , personal ( featuring ppredominantly imperatives ) vs. impersonal ( tbaturing indicatives )</definiens>
			</definition>
			<definition id="3">
				<sentence>The ditl'erence in structure only 479 shows in syntagmatic realization and is separate from the functional description : For Bulgarian , the postmodifier marker Ha ( ha : % f ' ) is inserted , and tbr Czech , the nominal group realizing the Postmodifier is attributed genitive ease .</sentence>
				<definiendum id="0">tbr Czech</definiendum>
				<definiens id="0">the nominal group realizing the Postmodifier is attributed genitive ease</definiens>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>Subjectivity is a pragmatic , sentence-level feature that has important implications for texl processing applicalions such as information exlractiou and information iclricwd .</sentence>
				<definiendum id="0">Subjectivity</definiendum>
				<definiens id="0">a pragmatic , sentence-level feature that has important implications for texl processing applicalions such as information exlractiou and information iclricwd</definiens>
			</definition>
			<definition id="1">
				<sentence>We consider two such l % atures : semantic orientation , which represents an ewdualivc characterization of a word 's deviation from the norm for its semantic group ( e.g. , beauti/'ul is positively oriented , as opposed to ugly ) ; and gradability , which characterizes a word 's ability to express a property in wlrying degrees .</sentence>
				<definiendum id="0">semantic orientation</definiendum>
				<definiendum id="1">gradability</definiendum>
				<definiens id="0">represents an ewdualivc characterization of a word 's deviation from the norm for its semantic group</definiens>
			</definition>
			<definition id="2">
				<sentence>Gradability ( or grading ) ( Sapir , 1944 ; Lyons , 1977 , p. 27 I ) is the semantic property that enables a word to participate in comparative constructs and to accept modifying expressions that act as intensitiers or diminishers .</sentence>
				<definiendum id="0">Gradability</definiendum>
				<definiens id="0">the semantic property that enables a word to participate in comparative constructs and to accept modifying expressions that act as intensitiers or diminishers</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>In order to compute this probability , we propose a hybrid language model defined as a sireple linear combination of n-gram models and a stochastic grammatical model G~ : Pr ( wklwl ... w~_l ) = c~Pr ( ~klwk-n ... wt~-l ) + ( 1 c~ ) P '' ( wklw~ ... wk- , , G~ ) , ( 1 ) where 0 &lt; c~ &lt; 1 is a weight factor which depends on the task .</sentence>
				<definiendum id="0">P ''</definiendum>
				<definiens id="0">a sireple linear combination of n-gram models and a stochastic grammatical model G~ : Pr ( wklwl ... w~_l ) = c~Pr ( ~klwk-n ... wt~-l ) + ( 1 c~ )</definiens>
				<definiens id="1">a weight factor which depends on the task</definiens>
			</definition>
			<definition id="1">
				<sentence>First , tile parameters of Cw , represented by Pr ( w\ [ c ) , are computed as : = E , o , ( 3 ) where N ( w , c ) is the number of times that the word w has been labeled with the POStagc .</sentence>
				<definiendum id="0">c )</definiendum>
				<definiens id="0">the number of times that the word w has been labeled with the POStagc</definiens>
			</definition>
			<definition id="2">
				<sentence>A Court : el-Free , Grammar G is a four-tul ) le ( N , E , P , S ) , wher ( ; N is the tinit ( ; set of nont ( ; rminals , ) 2 is the tinite sol ; of terminals ( N ~-/E = 0 ) , S ~ N is the axiom or initial symbol and 1 ' is the finite set of t ) rodu ( : tions or ruh ; s of the tbrm A -+ it , where A c N a.nd c~ C ( N U E ) + ( only grmmmtrs with non ( ; mt ) ty rules ar ( ; considered ) .</sentence>
				<definiendum id="0">Court</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">S ~ N</definiendum>
				<definiendum id="3">mt ) ty rules</definiendum>
				<definiens id="0">el-Free , Grammar G is a four-tul ) le ( N , E , P , S ) , wher ( ;</definiens>
				<definiens id="1">the axiom or initial symbol</definiens>
				<definiens id="2">A c N a.nd c~ C ( N U E ) + ( only grmmmtrs with non ( ;</definiens>
			</definition>
			<definition id="3">
				<sentence>c U'raw , w , wl '' G.~ is a pair ( G , p ) , where G is a ( : ontext-fr ( , .</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">a pair ( G , p )</definiens>
			</definition>
			<definition id="4">
				<sentence>In this cxi ) ression , Q ( A ~ D ) is the probability that D is the leftmost nol : terminal in all sentential fOHllS which are derived from A. The vahu ; Q ( A ~ BC ) is the probability that BC is th ( ; initial substring of all sentential forms deriv ( ; d from i\ .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiens id="0">the probability that D is the leftmost nol : terminal in all sentential fOHllS which are derived from A. The vahu</definiens>
				<definiens id="1">the probability that BC is th ( ; initial substring of all sentential forms deriv ( ; d from i\</definiens>
			</definition>
			<definition id="5">
				<sentence>In this expression , Q ( A ~ D ) is the t ) rob al ) ility that D is the leftmost nontermina , l in 57 the most t ) robable sentential form which is derived ti'om d. The value Q ( A ~ BC ) is the probability that BC is the initial substring of most the probable sentential form derived from A. Pr ( B &lt; i , 1 &gt; ) is the probability of the most probable parse which generates wi • • • wl froli1 B. Probability of generating a string The wflue Pr ( A &lt; i , j &gt; ) = Pr ( A d &gt; wi ... 'u ; jlG~ , Go ) is defined as the probability that the substring wi ... wj is generated from A given G~ and C , ~ .</sentence>
				<definiendum id="0">Q ( A ~ D )</definiendum>
				<definiendum id="1">&gt; )</definiendum>
				<definiendum id="2">Go )</definiendum>
				<definiens id="0">the t ) rob al ) ility that D is the leftmost nontermina , l in 57 the most t ) robable sentential form which is derived ti'om d. The value Q ( A ~ BC ) is the probability that BC is the initial substring of most the probable sentential form derived from A. Pr ( B &lt; i , 1</definiens>
				<definiens id="1">the probability of the most probable parse which generates wi • • • wl froli1 B. Probability of generating a string The wflue Pr ( A &lt; i , j &gt; ) = Pr ( A d &gt; wi ... 'u ; jlG~ ,</definiens>
			</definition>
</paper>

		<paper id="2168">
			<definition id="0">
				<sentence>Expectation-driven methodology : covering the material by collecting cross-linguistic information on lexical and grammatical parameters , including their possible values and realizations , and asking the user to choose what holds in SL ; while it is beyond the means of the current prqiect to check all extant languages fbr possible new parameters~ we have included infomlation from 25 languages .</sentence>
				<definiendum id="0">Expectation-driven methodology</definiendum>
				<definiens id="0">covering the material by collecting cross-linguistic information on lexical and grammatical parameters , including their possible values and realizations</definiens>
			</definition>
			<definition id="1">
				<sentence>Goal-driven methodology : in the spirit of the `` demand-side '' approach to NLP ( Nirenburg 1996 ) Boas was tailored lbr elicitation of Mr relevant parameters rather than any syntactic parameters that can be postulated .</sentence>
				<definiendum id="0">Goal-driven methodology</definiendum>
				<definiens id="0">tailored lbr elicitation of Mr relevant parameters rather than any syntactic parameters that can be postulated</definiens>
			</definition>
			<definition id="2">
				<sentence>For the English sentence used in the example above the Russian translation will be : the boy gives a book to his teacher -- &gt; malchik daet knigu uchitelju As soon as this is done , Boas presents the user with English phrases corresponding to clause elements of the translated sentence , so that for every English-SL pair of sentences the user types in ( or drags from the sentence translation ) corresponding SL phrases , thus aligning clause elements .</sentence>
				<definiendum id="0">Boas</definiendum>
				<definiens id="0">presents the user with English phrases corresponding to clause elements of the translated sentence , so that for every English-SL pair of sentences the user types in ( or drags from the sentence translation ) corresponding SL phrases</definiens>
			</definition>
</paper>

		<paper id="2147">
			<definition id="0">
				<sentence>Additional document and template filters have been added at tile fi'ont and back ends of the system to reduce the amount of text to be processed and to remove templates which are only sparsely filled .</sentence>
				<definiendum id="0">template filters</definiendum>
				<definiens id="0">tile fi'ont and back ends of the system to reduce the amount of text to be processed and to remove templates which are only sparsely filled</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>Tile multilingual clusterer takes input from the lnonolingual clusterers , and determines which news clusters ill which languages talk about tile same story .</sentence>
				<definiendum id="0">Tile multilingual clusterer</definiendum>
				<definiens id="0">takes input from the lnonolingual clusterers , and determines which news clusters ill which languages talk about tile same story</definiens>
			</definition>
			<definition id="1">
				<sentence>A MU is composed of several sentence segments and denotes a complete meaning .</sentence>
				<definiendum id="0">MU</definiendum>
				<definiens id="0">a complete meaning</definiens>
			</definition>
			<definition id="2">
				<sentence>A couple-linking element is a pair of words that exist in two segments .</sentence>
				<definiendum id="0">couple-linking element</definiendum>
				<definiens id="0">a pair of words that exist in two segments</definiens>
			</definition>
			<definition id="3">
				<sentence>An annotator reads all tile news articles , and connects tile MUs that discuss the same story .</sentence>
				<definiendum id="0">annotator</definiendum>
				<definiens id="0">reads all tile news articles , and connects tile MUs that discuss the same story</definiens>
			</definition>
			<definition id="4">
				<sentence>M2 is different l'ronl M1 in that the matching order of nouns itl\ ] ( l verbs are kept conditionally .</sentence>
				<definiendum id="0">M2</definiendum>
				<definiens id="0">different l'ronl M1 in that the matching order of nouns itl\ ] ( l verbs are kept conditionally</definiens>
			</definition>
			<definition id="5">
				<sentence>IA , C.N. and Thompson , S.A. ( 1981 ) Mandarin Chinese : A Functional Re\ [ erence Giwmmar , University of California Press , 1981 .</sentence>
				<definiendum id="0">IA , C.N.</definiendum>
				<definiens id="0">A Functional Re\ [ erence Giwmmar</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>99 ) al &gt; l ) ears to r ( : veal an underlying discourse mechanism resl ) onsil ) le for centering : the information si ; rH ( : ture of an utterance ( roughly the givennew i ) ai ; t ( : rn ) is ( ; he de ( 'l ) er reason for the ranking ( ) \ [ the % rward-h ) oking ( : eni : ( : rs .</sentence>
				<definiendum id="0">rH</definiendum>
				<definiens id="0">the information si</definiens>
			</definition>
</paper>

		<paper id="2089">
			<definition id="0">
				<sentence>Tile Memory-Based Sequence Learning ( MBSL ) algorithm ( Argamon et al. , 1998 ) learns substrings or sequences of POS and brackets .</sentence>
				<definiendum id="0">Tile Memory-Based Sequence Learning ( MBSL</definiendum>
				<definiens id="0">learns substrings or sequences of POS and brackets</definiens>
			</definition>
			<definition id="1">
				<sentence>In Figure 4 , we show the results of tagging on the test set in terms of the training set size using three at ) proaches : the simplest ( LEX ) is a tagging process which does not take contextual information into account , so the lexical tag associated to a word will 100 99 90 97 06 95 04 93 92 Precision • Recall ~ , + &lt; , , + , i , __ i i 100 200 300 4o0 500 600 7 ( 30 800 # Words x 1000 Figure 5 : NP-chunldng results on WSJ for incremental training sets .</sentence>
				<definiendum id="0">LEX</definiendum>
				<definiens id="0">a tagging process which does not take contextual information into account</definiens>
			</definition>
</paper>

		<paper id="2090">
			<definition id="0">
				<sentence>The total distance ( equal to 1 ) between C and I appears in the lowest right cell .</sentence>
				<definiendum id="0">total distance</definiendum>
				<definiens id="0">equal to 1 ) between C and I appears in the lowest right cell</definiens>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>Ihnthernmre , the outtlut of Q/A systems is either the actual answer identified in a text or small text ; fragments containing the answer .</sentence>
				<definiendum id="0">Ihnthernmre</definiendum>
				<definiens id="0">the actual answer identified in a text or small text ; fragments containing the answer</definiens>
			</definition>
			<definition id="1">
				<sentence>\ ] h ' , 1 ) rover ; tttelllI ) tS t ; o 1 ) rove the LFT of the question ( QLF ) corre ( : t 1 } 3 , proving from left to right each term of QLF .</sentence>
				<definiendum id="0">tttelllI</definiendum>
			</definition>
			<definition id="2">
				<sentence>Selected axiom : enable ( 75 73 76 ) : = null .</sentence>
				<definiendum id="0">Selected axiom</definiendum>
			</definition>
			<definition id="3">
				<sentence>hlcorrect ~ilswors ( no knowledge ) Correct m~swers ( KB-based ) Incorrect answers ( KB-based ) P rovell con'ect 3 127 l'roven Precision incorrect 210 98.5 % 5 96.2 % 38 90.04 % Table 4 : Prover performmme Qualitatively , we find that the integration of knowledge-based methods is very beneficial .</sentence>
				<definiendum id="0">hlcorrect ~ilswors</definiendum>
				<definiens id="0">no knowledge ) Correct m~swers ( KB-based ) Incorrect answers</definiens>
			</definition>
			<definition id="4">
				<sentence>Similarly , our knowledge-t ) ased question l ) rocessillg is a nlel'e combination of word class information and syntactic dep endencies .</sentence>
				<definiendum id="0">rocessillg</definiendum>
			</definition>
			<definition id="5">
				<sentence>Lasso : a tool for surfing the answer net .</sentence>
				<definiendum id="0">Lasso</definiendum>
			</definition>
</paper>

		<paper id="2110">
			<definition id="0">
				<sentence>The frst seglnent `` KANOJO-HA '' consists of two words , KANOJO ( She ) and HA ( subject case , , narl : er ) .</sentence>
				<definiendum id="0">frst seglnent `` KANOJO-HA ''</definiendum>
				<definiendum id="1">KANOJO</definiendum>
				<definiens id="0">consists of two words</definiens>
			</definition>
			<definition id="1">
				<sentence>JUMAN has 42 parts-of-speech ( POS ) including the minor level POSs , and we used the POS of the head word of a candidate bunsetsu .</sentence>
				<definiendum id="0">JUMAN</definiendum>
				<definiens id="0">has 42 parts-of-speech ( POS ) including the minor level POSs , and we used the POS of the head word of a candidate bunsetsu</definiens>
			</definition>
			<definition id="2">
				<sentence>Because the accuracy of the system against the training data is only 81.653 % ( without supplementation ) , it , is clear that we miss some important information , We believe the lexical relationships in verb frmne element preference ( VFEP ) is one of the most important types of information .</sentence>
				<definiendum id="0">VFEP )</definiendum>
				<definiens id="0">the lexical relationships in verb frmne element preference</definiens>
				<definiens id="1">one of the most important types of information</definiens>
			</definition>
</paper>

		<paper id="2145">
			<definition id="0">
				<sentence>According to their intelligence , these systems try to figurc out of what the meaning invariance consists in the reference text and learn an appropriate source language/target language mapping mechanism .</sentence>
				<definiendum id="0">meaning invariance</definiendum>
				<definiens id="0">consists in the reference text and learn an appropriate source language/target language mapping mechanism</definiens>
			</definition>
			<definition id="1">
				<sentence>The ReVerb EBMT system ( Collins , 1998 ) performs sub-sentential chunking and seeks to link constituents with the same function in the source and the target language .</sentence>
				<definiendum id="0">ReVerb EBMT system</definiendum>
				<definiens id="0">sub-sentential chunking and seeks to link constituents with the same function in the source and the target language</definiens>
			</definition>
			<definition id="2">
				<sentence>An understanding consists of extraction of compositionally translatable substriugs and the generation of translation templates .</sentence>
				<definiendum id="0">understanding</definiendum>
				<definiens id="0">consists of extraction of compositionally translatable substriugs and the generation of translation templates</definiens>
			</definition>
			<definition id="3">
				<sentence>McLean ( McLean , 1992 ) has proposed an austere approach where lie uses neural networks ( NN ) to translate surface strings from English to French .</sentence>
				<definiendum id="0">McLean</definiendum>
			</definition>
			<definition id="4">
				<sentence>Machine Tr'anslation ( MT ) is a lneaning preserving raapping from a source language text into a target language text .</sentence>
				<definiendum id="0">Machine Tr'anslation</definiendum>
				<definiens id="0">a lneaning preserving raapping from a source language text into a target language text</definiens>
			</definition>
			<definition id="5">
				<sentence>Automated Dictionary Extraction for `` Knowledge-Free '' Example-Based Translation .</sentence>
				<definiendum id="0">Automated Dictionary Extraction for `` Knowledge-Free</definiendum>
			</definition>
			<definition id="6">
				<sentence>Example-Based Machine Tra'nMation : An Adaptation-Guided Retrieval AppTvach .</sentence>
				<definiendum id="0">Example-Based Machine Tra'nMation</definiendum>
				<definiens id="0">An Adaptation-Guided Retrieval AppTvach</definiens>
			</definition>
</paper>

		<paper id="2171">
			<definition id="0">
				<sentence>The rhyme consists of a peak ( the vowel ) and a coda ( the final consonant cluster , which might bc split up into coda 1 , coda 2 , etc. ) .</sentence>
				<definiendum id="0">rhyme</definiendum>
			</definition>
			<definition id="1">
				<sentence>`` DATR : A Language for Lexical Knowledge Representation '' , In Contlmtational Linguistics , Vol .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">A Language for Lexical Knowledge Representation ''</definiens>
			</definition>
</paper>

		<paper id="1070">
			<definition id="0">
				<sentence>In this paper , we assume that { w I , 'w~ , ... , w ~0 } is a set of words , { tt , t'2 , ... , t ; } is a set of POS tags , a sequence of random variables l'lq , ,~ = l~q lazy ... I'E~ is a sentence of n words , and a sequence of random w~riables T1 , , , = 7~T,2 ... TT~ is a sequence of n POS tags .</sentence>
				<definiendum id="0">I'E~</definiendum>
				<definiens id="0">a sentence of n words</definiens>
			</definition>
			<definition id="1">
				<sentence>Lexicalized HMMs fbr word-unit tagging are defined 1 ) y making a less strict Markov assmnption , as tbllows : A ( T ( K , j ) , W ( I ; j ) ) ~ Pr ( tl , ,~ , wl , n ) i=\ ] x Pr ( wi I ti-L , i , wi-I , i-1 ) Ill models A ( T ( K , j ) , 14/ ( L j ) ) , the probability of the current tag ti depends on both tile previous If tags ti-K , i-i and the previous d words wi-j , i-i and the probability of the current word 'wi depends on the current tag and the previous L tags ti_L , i and the previous I words wi-l , i-~ .</sentence>
				<definiendum id="0">Lexicalized HMMs</definiendum>
				<definiens id="0">y making a less strict Markov assmnption , as tbllows : A ( T ( K , j ) , W ( I ; j ) ) ~ Pr ( tl , ,~ , wl , n ) i=\ ] x Pr ( wi I ti-L , i , wi-I , i-1 ) Ill models A ( T ( K , j ) , 14/ ( L j ) ) , the probability of the current tag ti depends on both tile previous If tags ti-K , i-i and the previous d words wi-j , i-i and the probability of the current word 'wi depends on the current tag</definiens>
			</definition>
			<definition id="2">
				<sentence>, , , , , , , , ) ( 7 ) Cl , ~tllt~l , u In the above equations , u ( _ &gt; 'n ) denotes the llllIlll ) cr of morph ( mms in a Se ( ltlell ( ; e ( 'orrespending the given word sequ ( 'ncc , c denotes a morl ) heme-mfit tag , 'm .</sentence>
				<definiendum id="0">llllIlll ) cr of morph</definiendum>
				<definiens id="0">'orrespending the given word sequ ( 'ncc , c denotes a morl ) heme-mfit tag , 'm</definiens>
			</definition>
			<definition id="3">
				<sentence>denotes a morl ) heme , aim p denotes a type of transition froln the previous tag to the current tag .</sentence>
				<definiendum id="0">aim p</definiendum>
				<definiens id="0">a type of transition froln the previous tag to the current tag</definiens>
			</definition>
			<definition id="4">
				<sentence>483 where the flmction Fq ( x ) returns the fl : equency of x in the training set .</sentence>
				<definiendum id="0">Fq ( x )</definiendum>
				<definiens id="0">returns the fl : equency of x in the training set</definiens>
			</definition>
			<definition id="5">
				<sentence>DA Degree of mnbiguity ( i.e. the number of tags per word ) .</sentence>
				<definiendum id="0">DA Degree of mnbiguity</definiendum>
				<definiens id="0">the number of tags per word )</definiens>
			</definition>
			<definition id="6">
				<sentence>Note that ML denotes a simple smoothing method where ML estimates with probability less than 10 -9 are smoothed and replaced by 10-9• Because , in the outside-test , AD ( d = 10 -2 ) performs better than ML and kD ( a ¢ 10-2 ) , we use 5 = 10 -2 in our additive smoothing .</sentence>
				<definiendum id="0">ML</definiendum>
				<definiens id="0">a simple smoothing method where ML estimates with probability less than 10 -9 are smoothed and replaced by 10-9• Because</definiens>
			</definition>
</paper>

		<paper id="2158">
			<definition id="0">
				<sentence>activMad ( ( root actividad ) ( cat n ) ( trans activity energy ) ( gender 1 ) ) comenzar ( ( root comenzar ) ( cat v ) ( trans begin start ) ( verbtype irregular 129 ) ) cuestion ( ( root cucstion ) ( cat n ) ( lrans ¢lUeStion dispute problem issue ) ( gender 1 ) ) At the cud of the dictionary lookup phase , \ [ br each word in the Spanish sentence we have a feature structure containing the information in the dictionary entry along with the parameter values that were gained from morphological analysis .</sentence>
				<definiendum id="0">activMad (</definiendum>
				<definiens id="0">( root actividad ) ( cat n ) ( trans activity energy ) ( gender 1 ) ) comenzar ( ( root comenzar ) ( cat v ) ( trans begin start ) ( verbtype irregular 129 ) ) cuestion ( ( root cucstion ) ( cat n ) ( lrans ¢lUeStion dispute problem issue ) ( gender 1 ) ) At the cud of the dictionary lookup phase , \ [ br each word in the Spanish sentence we have a feature structure containing the information in the dictionary entry along with the parameter values that were gained from morphological analysis</definiens>
			</definition>
			<definition id="1">
				<sentence>The morphological generator takes feature structures as input and produces correctly inflected English words .</sentence>
				<definiendum id="0">morphological generator</definiendum>
				<definiens id="0">takes feature structures as input and produces correctly inflected English words</definiens>
			</definition>
			<definition id="2">
				<sentence>Itochberg , M. Renals , S. , Robinson , A. and Cook , G. ( 1995 ) Recent inq~rovements to the Abbot Large Vocabulary CSR System .</sentence>
				<definiendum id="0">Recent</definiendum>
				<definiens id="0">inq~rovements to the Abbot Large Vocabulary CSR System</definiens>
			</definition>
</paper>

		<paper id="2118">
			<definition id="0">
				<sentence>Unergatives are intransitive action verbs , as in ( 1 ) , whose transitive form can be the causative counterpart of the intransitive form .</sentence>
				<definiendum id="0">Unergatives</definiendum>
				<definiens id="0">intransitive action verbs</definiens>
			</definition>
			<definition id="1">
				<sentence>In an intransitive unergalive , the subject is an Agent , and in an intransitive unaccusative , the subject is a Theme .</sentence>
				<definiendum id="0">subject</definiendum>
				<definiens id="0">an Agent , and in an intransitive unaccusative , the</definiens>
			</definition>
			<definition id="2">
				<sentence>VBN Tag Passive implies past pa.rt ; iciple use ( VBN ) , hence correlated with transitive ( and passive ) .</sentence>
				<definiendum id="0">VBN Tag Passive</definiendum>
				<definiendum id="1">iciple use</definiendum>
				<definiendum id="2">VBN</definiendum>
				<definiens id="0">implies past pa.rt</definiens>
			</definition>
			<definition id="3">
				<sentence>VBN does appear to make the expected I.hree-way distinction .</sentence>
				<definiendum id="0">VBN</definiendum>
				<definiens id="0">appear to make the expected I.hree-way distinction</definiens>
			</definition>
</paper>

		<paper id="2105">
			<definition id="0">
				<sentence>Context-Free Grammars A probabilistic context-free g~tmmar ( PCFG ) is a context-free grmnmar which assigns a probability P to each context-fl'ee grammar rule in the rule set R. The probability of a parse tree T is defined as \ [ I , .</sentence>
				<definiendum id="0">Context-Free Grammars A probabilistic context-free g~tmmar ( PCFG</definiendum>
				<definiens id="0">a context-free grmnmar which assigns a probability P to each context-fl'ee grammar rule in the rule set R. The probability of a parse tree T is defined as \ [ I ,</definiens>
			</definition>
			<definition id="1">
				<sentence>Tl : e structmal : lOtln chunk concel ) t in tim grammar is defined according to Almey 's chunk style ( Abney , 1991 ) who describes chunks as syntactic units which correspond in some way to prosodic 1 ) atterns , containing a content word surrounded t ) y some function word ( s ) : all words from the beginning of the noun 1 ) hrase to the head noun are included .</sentence>
				<definiendum id="0">s )</definiendum>
				<definiens id="0">syntactic units which correspond in some way to prosodic 1 ) atterns , containing a content word surrounded t</definiens>
			</definition>
			<definition id="2">
				<sentence>Unigram rules are rules of the form X -+ YP X , where YP is a grammatical category and X is a new category .</sentence>
				<definiendum id="0">Unigram rules</definiendum>
				<definiendum id="1">YP</definiendum>
				<definiendum id="2">X</definiendum>
				<definiens id="0">a grammatical category</definiens>
			</definition>
			<definition id="3">
				<sentence>The best chunk set of a sentence is defined as tile set of chunks ( with category , start mid end position ) 728 for which the stun of the prolmbilities of all parses which c , ontain exactly that chunk set is maximal .</sentence>
				<definiendum id="0">best chunk set of a sentence</definiendum>
				<definiens id="0">tile set of chunks ( with category , start mid end position ) 728 for which the stun of the prolmbilities of all parses which c , ontain exactly that chunk set is maximal</definiens>
			</definition>
			<definition id="4">
				<sentence>Gv is the set of vertices .</sentence>
				<definiendum id="0">Gv</definiendum>
				<definiens id="0">the set of vertices</definiens>
			</definition>
</paper>

		<paper id="2107">
</paper>

		<paper id="2113">
			<definition id="0">
				<sentence>I ~o.j Ida* I * ' , '¢* , I v , ' , , ) + If SC is N , delete art and generate : NP '-st , NP +-str If SC is VT , delete ( aux\ [ tolprn ) and art and generate : VP ( -str \ [ 1 : NP\ ] VP ~-\ [ I : NP\ ] ~ ( dobj ) st* '' `` ej-~Cdo '' ) If SC is v , delete ( auxltolprn ) generate : V + -- sO '' V *-str ~ ( `` do '' ) 3 'M ) le 2 : POS telnplates ~md corresponding SCs The templa£es are described in the l'orm of regula .</sentence>
				<definiendum id="0">VP</definiendum>
				<definiendum id="1">NP\ ] ~</definiendum>
				<definiens id="0">N , delete art and generate : NP '-st</definiens>
				<definiens id="1">V + -- sO '' V *-str ~ ( `` do ''</definiens>
			</definition>
			<definition id="1">
				<sentence>Each column consists of an input sentence , a translation without the dictionary , and a translation with the dictionary .</sentence>
				<definiendum id="0">column</definiendum>
			</definition>
			<definition id="2">
				<sentence>A Memory-Based Approach to Learning Shallow Natural Language Patterns .</sentence>
				<definiendum id="0">Memory-Based Approach</definiendum>
				<definiens id="0">to Learning Shallow Natural Language Patterns</definiens>
			</definition>
</paper>

		<paper id="2157">
			<definition id="0">
				<sentence>d on a new corl ) uS ( e.ncoded in the NeGra , Penn Treebank or XML format ) , a preprocessing tool has to convert it into the format of the description language .</sentence>
				<definiendum id="0">uS</definiendum>
				<definiens id="0">XML format ) , a preprocessing tool has to convert it into the format of the description language</definiens>
			</definition>
</paper>

		<paper id="2156">
			<definition id="0">
				<sentence>1 ) A Korean word consists of more than one morpheme with clear-cut morphenm boundaries ( Korean is all agglutinative language ) .</sentence>
				<definiendum id="0">Korean word</definiendum>
			</definition>
			<definition id="1">
				<sentence>1051 2 ) Korean is a postpositional language with many kinds of noun-endings , verb-endings , and prefinal verb-endings .</sentence>
				<definiendum id="0">Korean</definiendum>
				<definiens id="0">a postpositional language with many kinds of noun-endings , verb-endings , and prefinal verb-endings</definiens>
			</definition>
			<definition id="2">
				<sentence>Part-of speech ( POS ) tagging is a basic step to phrase break prediction .</sentence>
				<definiendum id="0">Part-of speech</definiendum>
				<definiendum id="1">POS ) tagging</definiendum>
				<definiens id="0">a basic step to phrase break prediction</definiens>
			</definition>
			<definition id="3">
				<sentence>The probability of a phrase break bi appearing after the second word POS tag is given by P ( bilt¢,2ta ) = C ( tlt2bit3 ) Ej=o , ~,2 C ( ht2bjt3 ) ' where C is a frequency count flmction and b0 , bl and b2 mean no break , minor break and major break , respectively .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a frequency count flmction and b0 , bl and b2 mean no break</definiens>
			</definition>
			<definition id="4">
				<sentence>However , Korean is a post-positional agglntinative language .</sentence>
				<definiendum id="0">Korean</definiendum>
				<definiens id="0">a post-positional agglntinative language</definiens>
			</definition>
			<definition id="5">
				<sentence>C4.5 lmilds a decision tree fl'om the t ) airs which consist of the feature vectors and their classes .</sentence>
				<definiendum id="0">C4.5</definiendum>
			</definition>
			<definition id="6">
				<sentence>r lest ( } 9 ; ... .. PI ol~ : lbilislic Iil~ , lhod only \ [ \ ] GITtee mlly Prolmbilisfic ii~01\ ] lod Pmballilistic iii0thod lind post error ~tlltl IX ) st ¢IlOl cDrl ocliOll ( 6:3 ) t'olteclJoll ( 4:5 ) Fignre 3 : The number of sentences for the probability training , the decision tree learning and the test in the experiments .</sentence>
				<definiendum id="0">... .. PI ol~</definiendum>
				<definiens id="0">The number of sentences for the probability training</definiens>
			</definition>
			<definition id="7">
				<sentence>A substitution ( S ) is an error between major break and minor break or vice versa .</sentence>
				<definiendum id="0">substitution ( S )</definiendum>
				<definiens id="0">an error between major break and minor break or vice versa</definiens>
			</definition>
			<definition id="8">
				<sentence>In the table , W means the thature vector size tbr the decision tree , and 6:3 and 4:5 mean ratio of the number of sentences used in the probabilistic train and the decision tree induction .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">means the thature vector size tbr the decision tree , and 6:3 and 4:5 mean ratio of the number of sentences used in the probabilistic train and the decision tree induction</definiens>
			</definition>
</paper>

		<paper id="2116">
			<definition id="0">
				<sentence>`` Word '' , generally , means a unit of expression which has universal intuitive recognition by native speakers .</sentence>
				<definiendum id="0">Word</definiendum>
				<definiens id="0">means a unit of expression which has universal intuitive recognition by native speakers</definiens>
			</definition>
			<definition id="1">
				<sentence>:3 Our Approach Decision tree induction algorithms have been successfully applied for NLP problems such as sentence boundary dismnbiguation ( Pahner et al. 1997 ) , parsing ( Magerman 1995 ) and word segmentation ( Mekuavin et al. 1997 ) .</sentence>
				<definiendum id="0">Decision tree induction algorithms</definiendum>
			</definition>
			<definition id="2">
				<sentence>The induction algorithm proceeds by evaluating content of a series of attributes and iteratively building a tree fiom the attribute values with the leaves of the decision tree being the value of the goal attribute .</sentence>
				<definiendum id="0">induction algorithm</definiendum>
				<definiens id="0">proceeds by evaluating content of a series of attributes</definiens>
			</definition>
			<definition id="3">
				<sentence>p ( , y ) p ( z ) where x is the leftmost character ofayz y is the lniddle substring ol'ayz is the rightmost character of : tlVz p ( ) is tile probability function .</sentence>
				<definiendum id="0">x</definiendum>
			</definition>
			<definition id="4">
				<sentence>rye is a non-word string but consists of words and characters , either of its left or right mutual information or both lnust be low .</sentence>
				<definiendum id="0">rye</definiendum>
				<definiens id="0">a non-word string but consists of words</definiens>
			</definition>
			<definition id="5">
				<sentence>Eutropy ( Shannon 1948 ) is the information measuring disorder of wu'iables .</sentence>
				<definiendum id="0">Eutropy</definiendum>
				<definiens id="0">the information measuring disorder of wu'iables</definiens>
			</definition>
			<definition id="6">
				<sentence>Left entropy ( Le ) , and right entropy ( Re ) of stringy are defined as : 803 Le ( y ) = Z p ( xy I Y ) ' Iog2p ( xYlY ) V.r~ A Re ( y ) = Z p ( yz l y ) `` log 2 p ( yz l y ) Vz~A where y is the considered string , A is the set of all alphabets x , z is any alphabets in A. Ify is a word , the alphabets that come before and aflery should have varieties or high entropy .</sentence>
				<definiendum id="0">Left entropy</definiendum>
				<definiendum id="1">Le</definiendum>
				<definiendum id="2">entropy</definiendum>
				<definiendum id="3">y</definiendum>
				<definiendum id="4">z</definiendum>
				<definiens id="0">y ) = Z p ( xy I Y ) ' Iog2p ( xYlY ) V.r~ A Re ( y ) = Z p</definiens>
				<definiens id="1">a word , the alphabets that come before and aflery should have varieties or high entropy</definiens>
			</definition>
			<definition id="7">
				<sentence>Avl Sc where s is the considered string N ( s ) is the number of the occurrences of s in corpus Sc is the size of corpus Avl is the average Thai word length .</sentence>
				<definiendum id="0">s</definiendum>
				<definiendum id="1">Avl</definiendum>
				<definiens id="0">the number of the occurrences of s in corpus Sc is the size of corpus</definiens>
			</definition>
			<definition id="8">
				<sentence>= SIS2 ... Sn_IS n N ( sls2* ) is the number of words in the dictionary that begin with s~s 2 N ( *s , _ls , , ) is the nmnber of words in the dictionary that end with s , ,_~s , , ND is the number of words in the dictionary .</sentence>
				<definiendum id="0">ND</definiendum>
				<definiens id="0">the nmnber of words in the dictionary that end with s , ,_~s , ,</definiens>
			</definition>
			<definition id="9">
				<sentence>In order to test the decision tree , another plain I-MB corpus ( the test corpus ) , which consists of 72 articles fi'om various fields , is employed .</sentence>
				<definiendum id="0">plain I-MB corpus</definiendum>
				<definiens id="0">the test corpus ) , which consists of 72 articles fi'om various fields , is employed</definiens>
			</definition>
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>Two resources we have begun to integrate for this purpose are WordNet and the Trigger Toolkit ( measuring mutual information ) .</sentence>
				<definiendum id="0">Trigger Toolkit</definiendum>
				<definiens id="0">measuring mutual information )</definiens>
			</definition>
			<definition id="1">
				<sentence>KNN is a memory-based learning algorithm wherein the model is essentially a replica of the training examples .</sentence>
				<definiendum id="0">KNN</definiendum>
				<definiens id="0">a memory-based learning algorithm wherein the model is essentially a replica of the training examples</definiens>
			</definition>
			<definition id="2">
				<sentence>Transformation-Based ErrorDriven Learning and Natural Language Processing : A Case Study in Part Of Speech Tagging .</sentence>
				<definiendum id="0">Transformation-Based ErrorDriven Learning</definiendum>
				<definiens id="0">Natural Language Processing : A Case Study in Part Of Speech Tagging</definiens>
			</definition>
</paper>

		<paper id="2104">
			<definition id="0">
				<sentence>Wordnet : An on-line lexical database .</sentence>
				<definiendum id="0">Wordnet</definiendum>
				<definiens id="0">An on-line lexical database</definiens>
			</definition>
</paper>

		<paper id="2115">
			<definition id="0">
				<sentence>match S t and S 2 /S 1 and S 2 / definitions t processing H / / / &gt; stelnlller calculate Levenshtein distance H align S~ and S 2 I `` '-I J replace strings ~ identify bindings &lt; clusters ) , ~__~ cluster bindings Figure 1 Clustering algorithm stoplist \ [ , L I find Ice @ 0 796 The strings consist ( ) 1 ' the entry term and the definition , so that etymology , part of speech , inl'lected t'orms ol ' the cntry term , examples and other inl'ormation were deleted. Subject-field labels , such as 'astronomy ' and 'meteorology ' , were preserved , either in full or slightly abbreviated , as they are helpful to resolve which sense o1 ' a word to choose , and usually constitute a l'undamental property of the concept. It should be noted that none of the 387 strings suffered any additional transformation , apart l'rom a few cases in order to complete a del'inition when it had been broken in two pm'ts by the dictionary editor , such as when a core meaning appears just once at the beginning of several subsequent senses. Althongh some abbreviations ( 'U.S.A. ' ) , initials of proper names ( 'C.T.R. Wilson ' ) and possessives ( : un s rays ' ) will come out as two or more words al'ter deleting punctuation marks and therefore can alter the efficiency el ' the algorithm , they were preserved to observe their effect. In order to compare two strings of woMs , we use the Levenshtein distance \ [ Levenshtein 196611 , a similar method to the edit distance. This method measures the edit transl'ormations that change one string into other. The Levenshtein distance arrangcs the strings in a matrix , with the words el ' Sj heading the columns and those of S 2 heading the rows. A null word is inserted at the beginning of each string S~ and S 2 , in position i=0 , .j=0. The matrix is filled with the costs of insertion , deletion and substitution using the l'ollowing formtfla • D ( ai , b i_ , ) + Di , ,. , , ( bi ) D ( ai , bj ) = rain D ( ai_j , bi ) +Di , ,. , . ( ai ) D ( aH , b / &lt; ) + D , I , ( ai , b j ) Where the cost of insertion. D~ , ,. , ( ) , is 1. and the cost of substitution. D , ,i , ( ) , is 0 or 1 , according to whether a~ and bj differ or not. Our experimental results have shown that the application of the Levenshtein distance using stem forms gives better matches than nsing full forms. Therefore , we shall fill the matrix with the cost for the stem l'orms , although the strings preserve the fnll forms both l'or the following steps and in the output table. We used the stmnming algorithm or ' Porter \ [ 1980\ ] , which removes endings l'ronl words. Building on the Levenshtein distance , Wagner and Fisher \ [ 1974\ ] propose a dynamic t~rogramming method to align the elements of two strings. Their procedure to return the ordered pairs of the alignment starts with the last cell of the matrix with cost\ [ n\ ] \ [ m\ ] and works back until either i or j equals 0 , according to which o1 ' its neighbours a cell was derived l'rom. I1 ' it is derived either from the previous horizontal or vertical cell ( \ [ i-l\ ] \ [ j\ ] or \ [ i\ ] lj-l\ ] respectively ) then the difference in cost is.just 1 , otherwise it is derived l'rom the diagonal. The alignment gives us a list of triplets formed by ~.ll , J , l~ , cost\ [ i\ ] \ [ j\ ] ) , in decreasing order according to cost\ [ i\ ] \ [ jl , where./. ) ' I , and ./\ ] ~ arc full forms from the strings S~ and S e , respectively. There are three possible pairings of words : `` Equal couple '' is defined as the pair ( 1-\ [ i , .ffj ) of full forms such that the corresponding stem forms are equal ( , s.'/ ' I = 4 ) '' `` Matched couple '' is a pair ( /. ) ~i , .Oj ) such that .sf~ # .ff~. This couple represents a potential pair ot ' similar words. `` Null couple '' is a pair ( .g , .g ) such that , s : /I ( ) r 4 is missing. With respect to the Levcnshtein distance , the equal couple means these words do not need any change to make both equal , while for the matched couple we shall replace one word with the other progressively , and for the null couple we must either insert one word into the given string or delete it from the given string. The purpose of clustering is to match different pairs of words ( matched couples ) , thus neither pairs of equal words ( equal couples ) nor pairs with a null word ( null couples ) are relevant. As a measure of the similarity between a matched couple , we quantify the surrounding equal couples above and below it. This concept is similar to the `` longest common subsequence '' of two strings suggested by Wagner and Fisher \ [ 1974\ ] , which is del'ined as the common subsequence of two strings having maximal length , although in our case both strings differ by the single matched couple. By analogy , we use longest collocation couple , henceforth 797 abbreviated lcc , since we refer to couples instead of a single string. Besides , the word `` collocation '' is more representative for a pair o1 ' words and their neighbourhood , being the core of two longest common subsequences. We define longest collocation couple as the maximal sequence of pairs of words formed by equal couples surrounding a matched couple. Given the alignment of the strings S~ and S 2 consisting of a list of triplets formed by ( ffi. , ff , cost\ [ ill/\ ] ) , in decreasing order according to cost\ [ i\ ] \ [ j\ ] , where.ff I , and fl~ are , respectively , full fomas l'rom S~ and $ 2 , the lcc is the longest consecutive sequence of triplets ( ~i. , f~ , cost\ [ i\ ] \ [ j\ ] ) formed by one matched couple , such that it meets 3 conditions : • The cost dilTerence between the first triplet and the last triplet is 1. • There is no null couple. • The matched couple is neither the first nor the last triplet. By these conditions , only the matched couple becomes the core el ' a Icc : we constrain a matched couple 1o be between two or more equal couples , and eliminate the possibility that the matched couple appears at the beginning or end o1 ' a phrase. As a result , we get a new triplet Off , .\ [ f~ , Icco ) , where ( If , J\ [ ~ ) is the matched couple and lcc , a is the length of the longest collocation couple. As an example , for the definitions of `` dynameter '' in table 1 , there is only one matched couple , `` determining-measuring '' , whose lcc is 9 ( the extent o1 ' the Icc is indicated by arrows ) . telescopes of power magnifying the determining for inslrulnent an dynameter , / , / ; telescope a of power magnifying the measuring for instrument An dynameter cost\ [ il\ [ jl 2 2 1 1 I 1 1 0 0 0 0 Table 1 Triplets for `` dynameter '' &lt; ¢U &gt; II ¢o ¢J &lt; -Ranking all triplets found by lcc in decreasing order , we observe that the greater the value o1 ' lcc , the greater the similarity between the words of the matched couple .</sentence>
				<definiendum id="0">S 2 / definitions</definiendum>
			</definition>
			<definition id="1">
				<sentence>Cambridge : The MIT Press .</sentence>
				<definiendum id="0">Cambridge</definiendum>
			</definition>
			<definition id="2">
				<sentence>Cambridge : The MIT Press .</sentence>
				<definiendum id="0">Cambridge</definiendum>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>These rules tbllow the tbllowing schemata : r~ = \ [ -S +M \ ] { exceptions } where : S is the relational suffix to be deleted from the end of an adjective .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">the relational suffix to be deleted from the end of an adjective</definiens>
			</definition>
			<definition id="1">
				<sentence>st ( w~ , i-~ , `` '' ~ , j ) + q , • ~ di , ~t ( wi , i , with w~ , m 1 ) eing the substring t ) egimfing nt tlm 1l I'h ' C\ ] I } II ' } I , CtCI '' } ~ll ( 1 tinishing after tim mth characl ; ( ; r of the word w , dis @ c , y ) = 1 i.f : c -- y = 0 if : ~ : ¢ y and q cost ; of the , inserl ; ion/de , h ' , tion of one , character p cost of '' t ; he sul ) stitution of one ( : h ; ~racter | ) y ~mothcr .</sentence>
				<definiendum id="0">st</definiendum>
				<definiens id="0">ion/de , h ' , tion of one , character p cost of '' t ; he sul ) stitution of one ( : h ; ~racter | ) y ~mothcr</definiens>
			</definition>
			<definition id="2">
				<sentence>Generally , a substitution is ( : onsidcr ( ~ , d as a dch~lion fi ) llowed 1 ) y ; m insertion , thus I ) -2 ( 1• Wc apply this alg ( ) rithm to e , a ( : h stem 1 { , ( ) l ) tahm ( t ; d'te , r the ( h~letion of tim r ( ~ , lational suffix , that had not ; 1 ) c ( m found ~s a stem ( ) f n llOllll .</sentence>
				<definiendum id="0">m found</definiendum>
			</definition>
			<definition id="3">
				<sentence>For exanti ) le , with the adjective ioniqne ( ionic ) , we generate both ionic ( 'ionia ) and ion ( ion ) , but only ion ( ion ) is an attested tbrm ; with the adjective gazeux ( gaseous ) , the noun forms gaz # as ) and gaze # auze ) ; are generated and the two of them are attested ; but , the adjective gazeux ( gaseous ) appears with the 218 Nmnber of oc ( : urrences 1 &gt; 2 Total 1 ) ase slir~l ( : l ; ures Nora1 Prep ( \ ] ) et ) Nora2 17 232 5 949 23 181 Nora Adj 12 344 4 778 17 122 Nora h Vinf 203 16 219 ' .</sentence>
				<definiendum id="0">adjective gazeux</definiendum>
				<definiens id="0">an attested tbrm</definiens>
			</definition>
			<definition id="4">
				<sentence>TN the number of terms in AGROVOC in which the noun from which has 1 ) een derived the relational adjective appears inside ~ prepositional phrase , i.e. the terms of Nounl Prep ( Det ) Nounl~Adj structure .</sentence>
				<definiendum id="0">TN</definiendum>
				<definiens id="0">the number of terms in AGROVOC in which the noun from which has 1 ) een derived the relational adjective appears inside ~ prepositional phrase</definiens>
			</definition>
			<definition id="5">
				<sentence>Np1 tbr term variant extra ( 'tion : Syn ( ; rgy between mort ) hoh ) gy , lexicon ~md synt~x. In T. StrzMkowski , editor , Nat , ural Language Processing and IT~ : formation Retrieval .</sentence>
				<definiendum id="0">Syn</definiendum>
				<definiens id="0">editor , Nat , ural Language Processing and IT~ : formation Retrieval</definiens>
			</definition>
</paper>

		<paper id="2126">
			<definition id="0">
				<sentence>Word order is defined as the order of modifiers , or the order of phrasal milts called 'bunsetsu ' which depend on the stone modifiee .</sentence>
				<definiendum id="0">Word order</definiendum>
				<definiens id="0">the order of modifiers , or the order of phrasal milts called 'bunsetsu ' which depend on the stone modifiee</definiens>
			</definition>
			<definition id="1">
				<sentence>A bunsetsu having wide dependency is defined as a bunsetsu which does not rigidly restrict its modifiee .</sentence>
				<definiendum id="0">bunsetsu having wide dependency</definiendum>
				<definiens id="0">a bunsetsu which does not rigidly restrict its modifiee</definiens>
			</definition>
			<definition id="2">
				<sentence>Here , the bunsetsu `` watashi_ga ( I ) '' is defined as a bunsetsu having wider dependency than 1 ; 11o tmnsetsu ' : Tok~ .</sentence>
				<definiendum id="0">watashi_ga ( I ) ''</definiendum>
				<definiens id="0">a bunsetsu having wider dependency than 1</definiens>
			</definition>
			<definition id="3">
				<sentence>A repetition word is a word referring to a word in a preceding sentence .</sentence>
				<definiendum id="0">repetition word</definiendum>
				<definiens id="0">a word referring to a word in a preceding sentence</definiens>
			</definition>
			<definition id="4">
				<sentence>Taro is a civil servant and Hanako is a doctor . ''</sentence>
				<definiendum id="0">Taro</definiendum>
				<definiendum id="1">Hanako</definiendum>
				<definiens id="0">a civil servant and</definiens>
			</definition>
			<definition id="5">
				<sentence>The lnethod uses a model which estimates the likelihood of the apt ) ropriate word order .</sentence>
				<definiendum id="0">lnethod</definiendum>
				<definiens id="0">uses a model which estimates the likelihood of the apt</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>Multimodal interfaces are systems that allow input and/or output to be conveyed over multiple different channels such as speech , graphics , and gesture .</sentence>
				<definiendum id="0">Multimodal interfaces</definiendum>
				<definiens id="0">conveyed over multiple different channels such as speech , graphics , and gesture</definiens>
			</definition>
			<definition id="1">
				<sentence>Finite-state transducers ( FST ) are finite-state automata ( FSA ) where each transition consists of an input and an output symbol .</sentence>
				<definiendum id="0">Finite-state transducers</definiendum>
				<definiendum id="1">FST</definiendum>
				<definiens id="0">finite-state automata ( FSA ) where each transition consists of an input and an output symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>Finite-state models are attractive n~echanisms for language processing since they are ( a ) efficiently learnable fiom data ( b ) generally effective for decoding and ( c ) associated with a calculus for composing machines which allows for straightforward integration of constraints fl'om various levels of language processing .</sentence>
				<definiendum id="0">Finite-state models</definiendum>
			</definition>
			<definition id="3">
				<sentence>Each terminal contains three components W : G : M corresponding to the n q1 tapes , where W is for the spoken language stream , G is the gesture stream , and M is the combined meaning .</sentence>
				<definiendum id="0">W</definiendum>
				<definiendum id="1">G</definiendum>
				<definiendum id="2">M</definiendum>
				<definiens id="0">for the spoken language stream</definiens>
				<definiens id="1">the gesture stream</definiens>
			</definition>
			<definition id="4">
				<sentence>Gp represents a gestural tel'erence to a person on the display , Go to an organization , and Gd lo a department .</sentence>
				<definiendum id="0">Gp</definiendum>
				<definiens id="0">a gestural tel'erence to a person on the display</definiens>
			</definition>
			<definition id="5">
				<sentence>N is the set of nonterminals .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the set of nonterminals</definiens>
			</definition>
			<definition id="6">
				<sentence>7 ' is the set ot ' terminals of the l'orm ( W U e ) : ( G U e ) : M* where W is the vocabulary of speech , G is the vocabulary of gesture=GestureSymbols U EventSymbols ; GcsturcSymbols = { G v , Go , Gpj ' , G~ .</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">the set ot ' terminals of the l'orm ( W U e ) : ( G U e ) : M* where W is the vocabulary of speech ,</definiens>
			</definition>
			<definition id="7">
				<sentence>, in each location where content needs to be transferred from the gesture tape to the meaning tape ( See Figure 3 ) .</sentence>
				<definiendum id="0">content</definiendum>
				<definiens id="0">needs to be transferred from the gesture tape to the meaning tape ( See Figure 3 )</definiens>
			</definition>
			<definition id="8">
				<sentence>Vpq : A spoken language interface to large scale directory information .</sentence>
				<definiendum id="0">Vpq</definiendum>
				<definiens id="0">A spoken language interface to large scale directory information</definiens>
			</definition>
</paper>

		<paper id="2124">
			<definition id="0">
				<sentence>ALLiS 2 ( Architecture for Learning Linguistic Structures ) is a learning system which uses theory refinement in order to learn non-recursive NP and VP structures ( Ddjean , 2000 ) .</sentence>
				<definiendum id="0">Architecture for Learning Linguistic Structures )</definiendum>
			</definition>
			<definition id="1">
				<sentence>Feature inff ) rmation gain ( nmt ; ual inforniation 1 ) etween features and class ) is used to determine the order in which features are mnt ) loyed as tests at all levels of the tree ( Quinlan , 1993 ) , With the full inlmt representation ( words and POS tags ) ~ we were not able to run comt ) lete experiments .</sentence>
				<definiendum id="0">rmation gain</definiendum>
				<definiens id="0">used to determine the order in which features are mnt ) loyed as tests at all levels of the tree</definiens>
			</definition>
			<definition id="2">
				<sentence>Each ti ; ature recc'ives a weight which is t ) ased on the amount of information whi ( : h it t/rovides fi ) r comtmting the classification of t ; t1 ( ; items in the training data .</sentence>
				<definiendum id="0">ature</definiendum>
				<definiens id="0">recc'ives a weight which is t ) ased on the amount of information whi ( : h it t/rovides fi ) r comtmting the classification of t ; t1 ( ; items in the training data</definiens>
			</definition>
			<definition id="3">
				<sentence>A mixture of simple features ( consisting of one of the mentioned information sources ) and complex features ( combinations thereof ) were used .</sentence>
				<definiendum id="0">mixture of simple features</definiendum>
				<definiens id="0">consisting of one of the mentioned information sources ) and complex features ( combinations thereof ) were used</definiens>
			</definition>
			<definition id="4">
				<sentence>SNoW uses the Open/Close model , described in Mufioz et al. ( 1999 ) .</sentence>
				<definiendum id="0">SNoW</definiendum>
				<definiens id="0">uses the Open/Close model</definiens>
			</definition>
			<definition id="5">
				<sentence>The Open/Close model consists of two SNoW predictors , one of which predicts the beginning of baseNPs ( Open predictor ) , and the other predicts the end of the ptlrase ( Close predictor ) .</sentence>
				<definiendum id="0">Open/Close model</definiendum>
				<definiens id="0">consists of two SNoW predictors , one of which predicts the beginning of baseNPs ( Open predictor ) , and the other predicts the end of the ptlrase ( Close predictor )</definiens>
			</definition>
			<definition id="6">
				<sentence>Howeves , in their later work stacked classifiers outperIbrm voting methods as well ( Van Halteren et al. , to appear ) .</sentence>
				<definiendum id="0">Howeves</definiendum>
				<definiens id="0">in their later work stacked classifiers outperIbrm voting methods as well ( Van Halteren et al. , to appear )</definiens>
			</definition>
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>chean Ethics ( Book V ) , Aristotle wrote : For proportion is equality of ra .</sentence>
				<definiendum id="0">chean Ethics</definiendum>
				<definiens id="0">For proportion is equality of ra</definiens>
			</definition>
			<definition id="1">
				<sentence>~th , ese , + = thesis : U~ , e , ~c.w l ( : af : lcave , ~ = call ' : calves give : ga , ve = si ' &lt; q : sang i'ne : cacl : exact = incapable : capable plus SOli/e tl'tlO analogies but with I10 Illea.liillg iit \ [ aligtlag ; o : 2 aa : aaa , a -aaaa : a , aaaa , a , give : gave = bid : bad walk : walkcd = go : gocd 3 and some counl ; er-examl ) les ( noted wil ; h ¢ ) : aaaa : bbbb ~ cccc : dddd 4 dJTU , :a : bzvmbz ¢ bzwnbz : dfh , ka &gt; l'his a.nah ) gy holds indel ) endently of the truth ( or falsity of ) aa : aaaa = aaaa : aaaoaaaa ( a ~ : a 4 = a 4 : a 8 ) .</sentence>
				<definiendum id="0">-aaaa</definiendum>
			</definition>
			<definition id="2">
				<sentence>\ ] ~y insl ) ection of the previous eXaml ) les , one can sta , te that there is no solution to a , n a , nalogy on the stl'illgS of syml ) ols A : 13 = C : x if sonic symbols of A apl ) ear neither in l\ ] nor in C. lhe contraposil ; ive , is tllat~ for an ana .</sentence>
				<definiendum id="0">ive</definiendum>
				<definiens id="0">x if sonic symbols of A apl</definiens>
			</definition>
			<definition id="3">
				<sentence>V ( A , t~ , C , ~ ) ) ~ ( V* ) ~ , A : B= C : I ) IAI-sim ( A , ~ ) -ICl-s .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">B= C : I ) IAI-sim ( A</definiens>
			</definition>
			<definition id="4">
				<sentence>l : om the length of the first sentence by at most a given con stan t. Definition 4 ( Bounded growth ) A la'nguage £ has the bounded growth property if ( and only ~ ' ) £ is a singleton or 3k E IN / Now , it is easy to prove ( see Appendix ) that : Theorem 7 Any language of analogical strings verifies the bounded growth property .</sentence>
				<definiendum id="0">) £</definiendum>
				<definiens id="0">Bounded growth ) A la'nguage £ has the bounded growth property if ( and only ~ '</definiens>
				<definiens id="1">Any language of analogical strings verifies the bounded growth property</definiens>
			</definition>
			<definition id="5">
				<sentence>Induction : suppose that a ' % '~ is a member of k ( { ab } , { ab aabb } ) .</sentence>
				<definiendum id="0">Induction</definiendum>
				<definiens id="0">a member of k ( { ab }</definiens>
			</definition>
			<definition id="6">
				<sentence>Naomi Sager Natural Language inJbrmation Processing : A Cbmputcr Grammar of English and Ils A.pplications Addison-Wesley , Reading , Mass. , 1981 .</sentence>
				<definiendum id="0">Naomi Sager Natural Language inJbrmation Processing</definiendum>
				<definiens id="0">A Cbmputcr Grammar of English and Ils A.pplications Addison-Wesley , Reading , Mass. , 1981</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>Document Authoring System A Document Authoring System is a tool that helps an author to writ ( ; docmnents .</sentence>
				<definiendum id="0">Document Authoring System</definiendum>
				<definiens id="0">a tool that helps an author to writ</definiens>
			</definition>
			<definition id="1">
				<sentence>d 11al ; llre ( e.g. , the ( ; omt ) lex logical fornmlas ent ; ere ( t in the wIP sysl ; em , An ( trd and II .</sentence>
				<definiendum id="0">em</definiendum>
				<definiens id="0">the ( ; omt ) lex logical fornmlas ent</definiens>
			</definition>
			<definition id="2">
				<sentence>A WYSIWYM-based System for the Authoring of Multimedia Documents ILLUSTrl , ATE is ai1 extension of PILLS producing documents that contain pictures as well as words .</sentence>
				<definiendum id="0">WYSIWYM-based System</definiendum>
				<definiendum id="1">ATE</definiendum>
			</definition>
			<definition id="3">
				<sentence>Natural Language and Exploration of an Information Space : the aLFfl'esco Interactive Systein .</sentence>
				<definiendum id="0">Exploration of an Information Space</definiendum>
			</definition>
</paper>

		<paper id="2150">
			<definition id="0">
				<sentence>Correlation profile for word pair P ( wl , w2 ) P ( w2 , wl ) The figure 7 above shows the results for the relationship between a pair of unknown ( because of tile substitution cipher approach ) content and functional words , so identified by looking at their cross-corpus statistics as described above .</sentence>
				<definiendum id="0">Correlation profile</definiendum>
				<definiens id="0">because of tile substitution cipher approach ) content and functional words , so identified by looking at their cross-corpus statistics as described above</definiens>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>Finally , let us call Here only top-N POS scquenccs are tried , where N is a pre-defined constant to limit parsing time .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a pre-defined constant to limit parsing time</definiens>
			</definition>
			<definition id="1">
				<sentence>Then , we extract `` POS Adjusting rules '' or `` PA rules '' defined as below : PA rule : W ( IPOS ) -- - &gt; W ( PPOS ) : C C : Context W : Word IPOS : Initially tagged POS PPOS : Parsable POS Means `` Give priority to the parsable POS over the initially tagged POS in a particular context shown as 'C ' . ''</sentence>
				<definiendum id="0">tagged POS</definiendum>
				<definiens id="0">POS Adjusting rules '' or `` PA rules '' defined as below : PA rule : W ( IPOS ) -- - &gt; W ( PPOS ) : C C : Context W : Word IPOS : Initially tagged POS PPOS : Parsable POS Means `` Give priority to the parsable POS over the initially</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>We used our own implementation of K-nearest neighbors algorithm ( kNN ) , mid then ( ; lie TiMBL learner ( Daelemans et al. , 1999 ) , which provides several extensions to kNN , well-suited tbr NLP 1 ) rol ) lems .</sentence>
				<definiendum id="0">kNN</definiendum>
				<definiendum id="1">TiMBL learner</definiendum>
				<definiens id="0">provides several extensions to kNN , well-suited tbr NLP 1 ) rol ) lems</definiens>
			</definition>
			<definition id="1">
				<sentence>i N ( wj , T ) Nw ( wi , T ) is tile number of times that wi co-occurs with T. • P ( wi ) is the probability of cue wi2 : Nw ( wi ) ( 6 ) = • P ( C ) is tile prior probability that a term of the corpus belongs to the category C : P ( C ) Nt ( C ) ( 7 ) Ec~ Nt ( Ci ) where Nt ( C ) is the occurrence number in training data of terms t ) elonging to C. This probability accounts for the weight of category C in the corpus .</sentence>
				<definiendum id="0">T )</definiendum>
				<definiendum id="1">P ( wi )</definiendum>
				<definiendum id="2">Nt ( C )</definiendum>
				<definiens id="0">tile number of times that wi co-occurs with T. •</definiens>
				<definiens id="1">tile prior probability that a term of the corpus belongs to the category C : P ( C ) Nt ( C )</definiens>
				<definiens id="2">the occurrence number in training data of terms t ) elonging to C. This probability accounts for the weight of category C in the corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>\ ] DPR ( 0.2111 ) , CKI ( 0.1843 ) , I 'd ) R ( 0.1654 ) II , enseigner ( Inform ) \ [ NAV\ ] CKI ( 0.24 ) , VOR ( 0.21 ) , DPR ( 0.1276 ) Entrde d'alr ( air intake ) \ [ ENG\ ] ENG ( 0.1895 ) , FUE ( 0.1214 ) , 1 ) OC ( 0.1192 ) Motet , , '' ( Engine ) \ [ ENG\ ] ENG ( 0.1494 ) , CDV ( 0.1285 ) , Dec ( 0.1095 ) Effacement de donn6es ( Data clearing ) \ [ RTL\ ] DPR ( 0.2059 ) , DOC ( 0.1357 ) , RTL ( 0.129 ) T616phone de piste ( Ground telephone ) \ [ RTL\ ] RTL ( 0.1251 ) , ELE ( 0.1131 ) , EQX ( 0.1011 ) Figure 1 : Some results of the exogeneous categorization .</sentence>
				<definiendum id="0">DPR</definiendum>
				<definiendum id="1">ENG</definiendum>
				<definiendum id="2">DOC ( 0.1357</definiendum>
			</definition>
			<definition id="3">
				<sentence>Term Extraction Term Clustering : An Integrated Platform for Computer-Aided Terminology .</sentence>
				<definiendum id="0">Term Extraction Term Clustering</definiendum>
			</definition>
</paper>

		<paper id="2121">
			<definition id="0">
				<sentence>Syntax-based methods ( referred also as knowledge-rich in contrast to the others knowledge-poor methods ) ( Pereira and Thishby , 1992 ; Grefenstette , 1993 ; Li and Abe , 1997 ) represent the words under consideration as vectors containing statistic values of their syntactic properties in relation to a given set of words ( e.g. statistics of object syntax relations referring to a set of verbs ) and cluster the considered words according to similarity of the corresponding vectors .</sentence>
				<definiendum id="0">Syntax-based methods</definiendum>
				<definiens id="0">represent the words under consideration as vectors containing statistic values of their syntactic properties in relation to a given set of words ( e.g. statistics of object syntax relations referring to a set of verbs</definiens>
			</definition>
			<definition id="1">
				<sentence>In the present work , we consider as word context the whole sentence in which the examined word appears , excluding only the semantically empty ( i.e. functional ) words such as articles , conjunctions , particles , auxiliaries .</sentence>
				<definiendum id="0">empty</definiendum>
				<definiens id="0">articles , conjunctions , particles , auxiliaries</definiens>
			</definition>
			<definition id="2">
				<sentence>/ ( 9 ) C 1 , otherwise where we used PTI , =30/ITcl , ITcl being the size of the corpus .</sentence>
				<definiendum id="0">ITcl</definiendum>
				<definiens id="0">being the size of the corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>Words or word sequences are thus classified in common semantic categories employing syntactical , morphological and coltocational intbrmation : a.Functionals ( auxiliaries , determiners ) are discarded since they do not modit ) semantically their head words .</sentence>
				<definiendum id="0">a.Functionals</definiendum>
				<definiens id="0">auxiliaries , determiners ) are discarded since they do not modit ) semantically their head words</definiens>
			</definition>
</paper>

		<paper id="2149">
			<definition id="0">
				<sentence>Let 's consider the CFG Go : S~ SAW ( ) r~ Wnlt D3 Saw -9 I ) 0 a.'gl01 i , arg2o : ~ Lt &lt; irr Lit ; in ' -- 4 '' O~H~n Inod27 light2 Gi~lit~N -- + grcelll7 \ [ green27 O~ -9 on3 arg23a hill4 W.ll -9 with5 a/g256 telcscope~i 1 ) 0 -9 seco\ ] sawo I ) 3 -9 modo3 D30 \ [ mod.2a I ) 32 D30 -4 modo5 \ ] mod4~ D32 -9 modo5 \ ] mod.2~ \ ] mod4~ Nontenninals of that grammar arc written in uppercase , terminals ( which are graph description elements ) in lowercase. It can be verified that the language generated by this grammar is the collection of commutative words IThis context-free grammar has polynomial size relative to the length of the string. While it is also possible in principle to use a linitestale model for representing lhe sallle sel of derivation trees , it can be showll Ihal such at model may be exponential relative to string length ( remark due to John Maxwell ) . 1017 corresponding precisely to all the possible analyses for the sentence. The fact that there are 20 such words can be established by a simple bottom-up computation involving multiplications and sums. I1 ' we call ambiguity degree ad ( N ) of a nonterminal N tim ntunber of words it generates , then it is obvious that , for instance , ad ( D30 ) = 2 , ad ( D3 ) = 2+3 , ad ( S ) = 4.1.1-5 = 20. In fact , it is the multiplications which appear in such computations which are responsible for the compactness of the grammar as compared to the direct listing of the words : each time a multiplication appears , a factorization is being cashed in. 2 When working with non-ambiguous structures , transfer is a rewriting process which takes as input a sourcelanguage graph and constructs a target-language graph by applying transfer rules of the form lhs -- 4 rhs , where lhs and rhs are finite sets of description elements for source graph and target graph respectively. In outline , the `` non-ambiguous '' transfer process works in the Mlowing way : for each non-overlapping covering of the source graph with left-hand sides of transfer rules , the corresponding right-hand sides are produced and taken together represent a target graph ( this is a non-deterministic ftmction as there can be several such coverings ) . In the case of ambigt , ous structures , the aim of transfer is to take as input a language of source graphs and to produce a language of target graphs. The language of target graphs should be equal to the union of all the graphs that would have obtained if one had enumerated one-by-one the source graphs , applied non-ambiguous transfer , and taken the collection of all target graphs obtained. The goal of ambiguous transfer is to perform the same task on the basis o1 ' a compact representation for the collection of source graphs , yielding a compact representation for the collection of target graphs. For illustratkm purposes , we will consider the following collection of transfer rules : seeo -- + voitb , sawo -+ sciero , gl '' eenl7 -- + Vel17 , green27 -+ gazonT , light2 , rood27 , greenl7 -- + lcu2 , rood'27 , vcrlT , light2 -- + lumi &amp; e2 , etc. We have only listed a few rules , and have assumed that the remaining ones are straighlorward one-to-one correspondences ( 11 -- + jet , medea -+ mod'o3 \ [ we prime labels such as mod , argl ... . in order to have disjointness of source and target vocabulary\ ] , etc. ) ) 2As the example shows , conlexl-flee representations of ambiguous slructures have the important properly ( related to their inte , 'aclionfreeness as described in the i , ~troduction ) of being easily `` countable '' . This is to be contrasted with other possible representations for ambiguous structures , such as ones based on propositional axioms determining which desc , 'iption elemenls can be jointly p , esent in a given analysis. In these representations , the problem of determining whether there exists one structure satisfying the specification can be of high complexity , let alone the problem of counting such structures. but are panerns containing variables instead of imlnbers ; in order to ohThe cotnmutative monoid over an alphabet A is denoted by C ( ~* ) , and its words are represented by vectors of N A , indexed by ..4 and with entries ill N. For each w E N A , the c ( mlponent indexed by a C ..4 is denoted by w\ [ , \ ] and tells how many a 's occur in w. The product ( concatenation ) ofwl and 'w2 in C ( .A* ) is the vector w E N A s.t. Vet C A : w\ [ , \ ] = wl \ [ , \ ] + `` w2\ [ , 1. A language of the commutative monoid is a subset of C ( A* ) . The subword relation is denoted by -- &lt; . For a language L , we write : v -- &lt; L iff there exists w E L s.t. v- &lt; w. The rewriting is performed from a sourcc language £s over an alphabet Es to a target language/27 , over an alEs ) w.r.t , a phabet ET ( disjoint fi'om set of rewriting rules 7~ C P , s + x P'T* ( rules have the form A-+p ) . We assume in the sequel that any a G ES appears at most once in any left-hand side of each rule of `` R. and also at most once in any word of £s. This property is preserved by all the rewritings that we are going to int , 'oduce. Let 's deline LItS ( A-+p ) = A. For R C ~. , we define L. , ~ &amp; , ( R ) = { a E P 's ' \ [ 3r e 17 , s.t. et-e , LHS ( , ' ) } . Tim rewriting is a l'unction qSre. taking £,9 and yielding L ; T , delined as : , / , ~ ( £s ) = { mp , , I ~ , , ~ £s. , ~ , , = , x~ ... % , /~ kl -- ~'pl G `` J~ A ... A Ap'-q , flp G `` R. } . In order to implement the function ( ) n , it is useful to introduce rewriting functions q~ -- +t , and q~ ? r. They apply to any language L over C ( E* ) , where E = Es , tO ET. They are detined as : ~x-+ , , ( r ) = { m '' I aw C L } O~ ( L ) = { w c L I ~v\ [ . 1 = 0 } . The ~x-~p functions are applied so that source symbols are guaranteed to be removed one by one from £.s ' : we consider E.s ' is totally ordered by &lt; and we write E.5 ' = \ [ ( /,1 , a2 , ... , aN\ ] , with ai &lt; eti+l ; then consider the partition of 7~. : 7Zl , J~2 ... .. T~N s.t.R.1 contains all ~. rules with al in LHS , `` R.2 contains all 7¢ rules with a9 but not al in LHS , etc , `` R.N contains all 7Z rules with only aN in LHS. Then we deline a third rewriting function q ' ) 7¢~ : ~l , ,e , ( L ) = qSv ( L ) U U , .eT~ , 4 , . ( c ) . Lemma. £7 ' can be obtained l ; 'om £s by applying the T~i iteratively in the following manner : ~b~/~N ( ( / ) '~N -- I ( ' '' `` { J ) '\ ] ~l ( CS ) ° '' `` ) ) = j~'\ ] ' '' PROOF SKETCII. For 1 _ &lt; j &lt; N , we deline £'J = { pl '' 'ppx \ ] ~*tJ E J~.S , z E ES* , p &gt; O , w = At ' '' .</sentence>
				<definiendum id="0">corresponding right-hand sides</definiendum>
				<definiendum id="1">subword relation</definiendum>
				<definiens id="0">S~ SAW ( ) r~ Wnlt D3 Saw -9 I ) 0 a.'gl01 i , arg2o : ~ Lt &lt; irr Lit ; in ' -- 4 '' O~H~n Inod27 light2 Gi~lit~N -- + grcelll7 \ [ green27 O~ -9 on3 arg23a hill4 W.ll -9 with5 a/g256 telcscope~i 1 ) 0 -9 seco\ ] sawo I ) 3 -9 modo3 D30 \ [ mod.2a I ) 32 D30 -4 modo5 \ ] mod4~ D32 -9 modo5 \ ] mod.2~ \ ] mod4~ Nontenninals of that grammar arc written in uppercase , terminals ( which are graph description elements ) in lowercase. It can be verified that the language generated by this grammar is the collection of commutative words IThis context-free grammar has polynomial size relative to the length of the string.</definiens>
				<definiens id="1">appear in such computations which are responsible for the compactness of the grammar as compared to the direct listing of the words : each time a multiplication appears , a factorization is being cashed in. 2 When working with non-ambiguous structures , transfer is a rewriting process which takes as input a sourcelanguage graph and constructs a target-language graph by applying transfer rules of the form lhs -- 4 rhs</definiens>
				<definiens id="2">the vector w E N A s.t. Vet C A : w\ [ , \ ] = wl \ [</definiens>
				<definiens id="3">~x-~p functions are applied so that source symbols are guaranteed to be removed one by one from £.s '</definiens>
			</definition>
			<definition id="1">
				<sentence>The algorithm consists of the procedure and functions described below and uses an agenda which contains Dew i~on-terminals to be defined in Gi .</sentence>
				<definiendum id="0">algorithm</definiendum>
				<definiens id="0">consists of the procedure and functions described below and uses an agenda which contains Dew i~on-terminals to be defined in Gi</definiens>
			</definition>
			<definition id="2">
				<sentence>fiom a : ; //each contributing ( i.e. non e ) wj is to be deleted // ( i.e. rewritten to e it , Aj ) ; non-contributing Aj 's //Jvntain tmtoudted ; attd p is inserted .</sentence>
				<definiendum id="0">//each contributing</definiendum>
				<definiendum id="1">wj</definiendum>
				<definiens id="0">to be deleted // ( i.e. rewritten to e it</definiens>
			</definition>
			<definition id="3">
				<sentence>Ambiguity Management hz Natural Language Generation .</sentence>
				<definiendum id="0">Ambiguity Management</definiendum>
				<definiens id="0">hz Natural Language Generation</definiens>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>Finally , the con ( : hlsion follows in section 5 .</sentence>
				<definiendum id="0">hlsion</definiendum>
			</definition>
			<definition id="1">
				<sentence>Statisti ( : al information , neural network and de ( : ision tree were used to imt ) lelneld ; the direct method .</sentence>
				<definiendum id="0">Statisti</definiendum>
				<definiens id="0">al information , neural network</definiens>
			</definition>
			<definition id="2">
				<sentence>qenerated words ( 5 ) C.A. = L ( 6 ) where L is the length of the original string , and i , d , mid s are the number of insertion , deletion and substitution respectively .</sentence>
				<definiendum id="0">L</definiendum>
				<definiens id="0">the length of the original string</definiens>
				<definiens id="1">the number of insertion , deletion and substitution respectively</definiens>
			</definition>
			<definition id="3">
				<sentence>We evaluated both tbr the term frequency ( tf ) and document frequency ( d J ) , where tfis the number of term appearance in the documents and df is the number of documents that contain the term .</sentence>
				<definiendum id="0">term frequency</definiendum>
				<definiendum id="1">document frequency</definiendum>
				<definiendum id="2">df</definiendum>
			</definition>
			<definition id="4">
				<sentence>A transliteration network that consists of phoneme nnit has more nodes than a transliteration network that consists of alphabet unit .</sentence>
				<definiendum id="0">transliteration network</definiendum>
				<definiens id="0">consists of alphabet unit</definiens>
			</definition>
</paper>

		<paper id="2142">
			<definition id="0">
				<sentence>The Computing Research laboratory ( CRL ) is developing a machine translation toolkit that allows rapid deployment of translation capabilities .</sentence>
				<definiendum id="0">Computing Research laboratory</definiendum>
				<definiendum id="1">CRL</definiendum>
				<definiens id="0">developing a machine translation toolkit that allows rapid deployment of translation capabilities</definiens>
			</definition>
			<definition id="1">
				<sentence>MEAT is a publicly available environmeut I that assists a linguist in rapidly developing a machine translation system , in order to keep the overhead involved in learning and using the system as low as possible , the linguist uses use simple yet powerful basic data and control structures .</sentence>
				<definiendum id="0">MEAT</definiendum>
				<definiens id="0">a publicly available environmeut I that assists a linguist in rapidly developing a machine translation system</definiens>
				<definiens id="1">possible , the linguist uses use simple yet powerful basic data and control structures</definiens>
			</definition>
			<definition id="2">
				<sentence>The core of the system consists of the formalism and the chart data representation .</sentence>
				<definiendum id="0">core of the system</definiendum>
			</definition>
			<definition id="3">
				<sentence>Turkish shows a rich derivational and inflectional morphology , which accounts for most of the system development work that was necessary to build a wrapper for integrating the Turkish morphological analyzer in the system ( approximatly 60 person-hours ) 4 .</sentence>
				<definiendum id="0">Turkish</definiendum>
				<definiens id="0">shows a rich derivational and inflectional morphology , which accounts for most of the system development work that was necessary to build a wrapper for integrating the Turkish morphological analyzer in the system ( approximatly 60 person-hours ) 4</definiens>
			</definition>
			<definition id="4">
				<sentence>PersianEnglish Machine Translation : An Overview of the Shiraz Project .</sentence>
				<definiendum id="0">PersianEnglish Machine Translation</definiendum>
			</definition>
			<definition id="5">
				<sentence>Corpus-Based annotated Test Set for Machine Translation Evaluation by an Industrial User .</sentence>
				<definiendum id="0">Corpus-Based</definiendum>
			</definition>
</paper>

		<paper id="2129">
			<definition id="0">
				<sentence>Let E ( i ) be tile equivalence class of node i , that is the set of nodes which are coreferent with i ( linked with i via eq relationships ) .</sentence>
				<definiendum id="0">E ( i</definiendum>
				<definiens id="0">the set of nodes which are coreferent with i ( linked with i via eq relationships )</definiens>
			</definition>
			<definition id="1">
				<sentence>do ( : mu ( , n ( , ,~ ~t ) ( ) ( ll , mul ( , ipl ( ; t , o\ ] ) -4°2 , qYanMi ) rma I ; i ( ) n t\ ] '\ [ l ( ; \ ] ) I ; ( ) CC , HH ( ) V f ; llllllllgtl'iZ~l , \ [ ; i ( ) ll c~l , l } } ) ( ; ( 1 ( ; ( : O111 l ) O , ~ ; ud in { , o i ; lu ' ( ; ( ' , , ~ ; l ; ~g ( ' , ~ ; ( Sl ) a , rc\ ] ~ Jones , .</sentence>
				<definiendum id="0">HH ( ) V f</definiendum>
				<definiens id="0">mu ( , n ( , ,~ ~t )</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>ncing tim relative l ) r ( ~di ( : l ; ivc l &gt; O ( ( u~iial of difl ' &lt; u ' ( mt similarity metrics over different l ) aram ( : t ( , .risations. In tiffs l ) almr , we focus on comparison of differ ( mr retrieval algorithms for non-segmenting la.nguag ( ~s , 1 ) ascd around a TI~ , I sysi ; cm from .\ ] almnese to English. Non-s ( ! gm ( ml ; ing languages are those which ( Io not involve d ( : limii ; ers ( e.g. spaces ) tmtwe ( m words , and in ( : lude .lapmms ( : , ( Jhines ( : and Thai. W ( : are tmrticularly int ( ~'r ( ~st ( : ( l in the part tim orlhog ( mal 1 mrmnet ( ~rs of s ( . , gmentnl ; ion and word order play in the st ) ( ! cd/a ( : ( : uracy trad ( ! -oti'. That is , 1 ) 3 '' doing away with segnl ( : ntai ; ion in relying soMy on ch\ [ /t ' } lc\ [ ( } lh~v ( ~l comparis ( m ( character-1 ) ased indexing ) , do w ( : signiti ( : mitly degrade match tmrt'ormance , as compared to word-level comparison ( word-based indexing ) ? Similm'ly , by ignoring word order and treating each sour ( : e language string as a `` bag of words '' , do \re genuinely lose out over word orders ( msitive apl ) roacho.s ? The. In ; fin objective of this research is thus ( ; o ( teJ ; ermine whether the COmlmi , atioiml overlmad associated with more stringent approaches ( i.e. word-based indexing and word ordersensitive alH ) roaches ) is commensura.te with the performancc gains they ott'er. To l ) rccmpt what tollows , the major contrilmtions of this research are : ( a ) empirical evaluation of difthrcnt comparison methods over actual JapaneseEnglish TM data , focusing on four orthogonal retriewfl paradigms ; ( b ) the finding that , over tile target ; data , character-based indexing is consistently superior to word-based indexing in identii\ [ ying the translation candidate most sinfilar to tile optimal translation for a given inlmt ; and ( c ) empirical verification of tim supremacy of word order-sensitive exhaustiv ( : string comparison methods over boolean inal ; ch methods. In the % llowing sections we discuss the effects 35 of segmentation and word order ( ~ 2 ) and preseut a number of both bag-el ; words and word ordersensitive sinfilarity metrics ( § 3 ) , before going on to evaluate the difl'crent lnethods with character-based and word-based indexing ( § 4 ) . We then conclude the paper in Section 5. Using segmentation to divide strings into component words or nlori ) helnes has tile obvious advmltage of clustering characters into senlantic units , which in the case of ideogrmn-based languages such as Japanese ( in the fern1 of kanji characters ) and Chinese , generally disatnbiguates character tneaning. The kanji character 'J \ [ ' , for example , can be used to mean any of `` to discern/discriminate '' , `` to speak/argue '' and `` a valve '' , but word context easily resolves such mnbiguity , hi this sense , our intuition is that segmented strings should produce better results than non-segmented strings. Looking to past research on similarity metrics for TM systelns , ahnost all systems involving aal ) anese as the source language rely on segnlentation ( e.g. ( Nakanmra , 1989 ; Sulnita and Tsutsumi , 1991 ; Kitalnura and Yamamoto , 1996 ; Tmtaka , 19971 ) , with Sate ( 1992 ) and Sate and Kawase ( 1994 ) providing rare instances of character-based systelnS. By avoiding tile need to segment text ; , we : ( a ) alleviate computational overhead ; ( b ) avoid the need to commit ourselves to a particular analysis type in the case of ambiguity ; ( c ) avoi ( 1 the issue of ' how to deal with unknown words ; ( d ) avoid the need for stemming/lenlmatisation ; and ( e ) to a large extent get around problems related to the nornmlisation of lexical alternation ( see Baldwin and Tanaka ( 1999 ) for a discussion of problems related to lexical alternation in Jal ) anese ) . Additionally , we can use the conmlonly anlbiguous na.ture of individual kanji characters to our advantage , in modelling seinantic similarity between related words with character overlap. With word-based indexing , this would only be possible with tile aid of a thesaurus. Similarly for word order , we would expect that translation records that preserve the word ( segment ) order observed in the inImt string would provide closer-matching translations than translation records containing those stone segnlents in a different order. Natur~dly , enforcing preservation of word order is going to place a significant burden on the matching mechanism , in that a number of different substring match schenlata are inevitably going to be produced between rely two strings , each of which nmst be considered on its own merits. To the authors ' knowledge , there is no TM system operating from Japanese that does not rely on word/segment/character order to some degree. Tanaka ( 1997 ) uses pivotal content words identified , by the user to search through the TM and locate translation records which contain those same content words in the stone order and preferably the stone segment distance apart. Nakamura ( 1989 ) similarly gives preference to translation records in which the content words contained in the original input occur in the same linear order , although there is tile scope to back off to translation records which do not I ) reserve the original word order. Sumita and Tsutsmni ( 19911 take the opposite tack in iteratively filtering out NPs and adverbs to leave only functional words and nlatrix-level predicates , and find trmlslation records which contain those same key words in the same ordering , preferably with the same segment types between them in the same numbers. Nirenburg et al. ( 1993 ) propose a word order-sensitive metric based on `` string composition discrepancy '' , and increlnentally relax the restriction on the quality of match required to inehlde word lenmlata , word synonynls and then word hyt ) ernylns , increasing the match penalty as they go. Sate and Kawase ( 1994 ) employ a more local model of character order in modelling similarity according to N-grams fashioned from the original string. The greatest advantage in ignoring word/segnlent order is computational , in that we significantly reduce the search space and require only a single overall comparison per string pair. Below , we analyse whether this gain in speed outweighs any losses in retrieval perfbrmance. Due to o111 '' interest in the efli~cts of both word order and seglnentation , we must have a selection of similarity lnetrics compatible with the various permutations of these two 1 ) arameter types. We choose to look at a nunlber of bag-of-words and word ordersensitive methods which are compatible with both character-based and word-based indexing , and vary the intmt to model tile etl~ects of the two indexing paradigms. The particular bag-of-word approactles we target are tlm vector space model ( Manning and Schiitze , 1.999 , p300 ) and `` token intersection '' , a silnple ratio-based similarity nletric. For word ordersensitive approaches , we test edit distance ( Wagner and Fisher , 1974 ; Planas and Furuse , 1999 ) , `` sequential correspondence '' and `` weigllted sequential correspondence '' . Each of tile similarity metrics eillpirically describes the sintilarity between two inlmt strings tmi mid i~. , 2 where we define tmi as a source language string taken fl'om the TM and i~. as the input string which we are seeking to 1hatch within the TM. One featnre of all similarity metrics given here is that they have fine-grained discriminatory potential and are able to narrow down the final set of translation candidates to a handfld of , and in nlost cases one , outlmt. This was a deliberate design decision , and aimed at example-based machine translation applications , where human judgement can not be relied upon to single out the most appropriate translation from multiple system outputs. In this , we set ourselves apart from the research of Sunlita and Tsutsumi ( 1.991 ) , for example , who judge the system to have been successful if there are a total of 100 or less outputs , aud a useful translation is contained within them. Note that it would be a relatively simple pro2Note that the ordering here is arbitrary , and that all the similarity metrics described herein are commutative for the given implementations. 36 cedure to fall ( ) lit the 11111111 ) e1 '' of Olltt ) lltS to it ill ollr case , tly taking tim top n ranking outputs. For all silnitarity metrics , we weight different .\ ] ai ) mmse segment tyl ) es according to their exl ) ected impact on translation , in the form of the sweigh , t fllnctioll : Segment type s , wcight punctuation 0 other segments 1 W ( ' exl ) erinlentally trialled intermediate swcight settings tbr ditt'erent character tyl ) es ( in the case of character-based indexing ) or segment tyl ) eS ( in the case of word-based indexing ) , none of which was fomtd to apl ) reciat ) ly iml ) rove performance. : ~ a.1 Similarity metrics used in this research Vector space model Within our imt ) lenmntation of the reactor space Inodol ( VSM ) , the segment content of each string is ( lescril ) ( '. ( l as a vector , ma ( le u l ) of 3 single dimension for each segment tok ( , n occurring within tmi or in. The. value of each vector eolnt ) onent is given as the weighted frequen ( -y of that token accor ( ling to its sweiqht vahle , such that any nulnber of 3 given i ) un ( : tuation mark will produce a fl'e ( luen ( : y of 0. The string sinfilarity of t ? H , i and in is then detined sis tim cosine of the angle l/etween vectors t\ [ \ [ ~.i and iT\ [ t , reSl ) ectivety , calculated as : tT~ , i , i~5 , cos ( t , fi , , , i ; 4 It , ll l 0 ) where dot l ) roduct and vect ( ) r length ( : oin ( : i ( le wil ; h l ; he standard detlnitions. The strings tmi of maximal similarity are th ( ) se whi ( : h i ) roduce the nmxinuun v3hw , for th ( ! v ( ~ctor cosine. Not ( ; that VSM c ( msi ( lers ( inly s ( '.gment fre ( tueney and is insensitive to word order. Token intersection The token intersection of tmi 3nd in is defined as the cumulative intersecting fl'equency of tokens appearing in each of the strings , normalised according to the combined segment lengths of tm , i and in. Forreally , this equates to : tint ( tm~ , in ) : e × ~_~ , l'lill ( f , ' { ? ( htnl ( \ [ ) , frcqilz ( , ) ) `` m~ ( l , , , ~ ) + &gt; .</sentence>
				<definiendum id="0">ncing tim relative l ) r ( ~di</definiendum>
				<definiendum id="1">ivc l &gt; O</definiendum>
				<definiendum id="2">mt similarity metrics</definiendum>
				<definiendum id="3">mr retrieval algorithms for non-segmenting la.nguag</definiendum>
				<definiendum id="4">almnese to English. Non-s</definiendum>
				<definiendum id="5">VSM c ( msi ( lers ( inly s</definiendum>
				<definiens id="0">those which ( Io not involve d ( : limii ; ers ( e.g. spaces ) tmtwe ( m words , and in ( : lude .lapmms ( : , ( Jhines ( : and Thai. W ( : are tmrticularly int ( ~'r ( ~st ( : ( l in the part tim orlhog ( mal 1 mrmnet ( ~rs of s ( . , gmentnl ; ion and word order play in the st</definiens>
				<definiens id="1">uracy trad ( ! -oti'. That is , 1 ) 3 '' doing away with segnl ( : ntai ; ion in relying soMy on ch\ [ /t ' } lc\ [ ( } lh~v ( ~l comparis ( m ( character-1 ) ased indexing ) , do w ( : signiti ( : mitly degrade match tmrt'ormance , as compared to word-level comparison ( word-based indexing ) ? Similm'ly , by ignoring word order and treating each sour ( : e language string as a `` bag of words '' , do \re genuinely lose out over word orders ( msitive apl ) roacho.s ? The. In ; fin objective of this research is thus ( ; o ( teJ ; ermine whether the COmlmi , atioiml overlmad associated with more stringent approaches ( i.e. word-based indexing and word ordersensitive alH ) roaches ) is commensura.te with the performancc gains they ott'er. To l ) rccmpt what tollows , the major contrilmtions of this research are : ( a ) empirical evaluation of difthrcnt comparison methods over actual JapaneseEnglish TM data , focusing on four orthogonal retriewfl paradigms ; ( b ) the finding that , over tile target ; data , character-based indexing is consistently superior to word-based indexing in identii\ [ ying the translation candidate most sinfilar to tile optimal translation for a given inlmt ; and ( c ) empirical verification of tim supremacy of word order-sensitive exhaustiv ( : string comparison methods over boolean inal ; ch methods. In the % llowing sections we discuss the effects 35 of segmentation and word order ( ~ 2 ) and preseut a number of both bag-el ; words and word ordersensitive sinfilarity metrics ( § 3 ) , before going on to evaluate the difl'crent lnethods with character-based and word-based indexing ( § 4 ) . We then conclude the paper in Section 5. Using segmentation to divide strings into component words or nlori ) helnes has tile obvious advmltage of clustering characters into senlantic units , which in the case of ideogrmn-based languages such as Japanese ( in the fern1 of kanji characters ) and Chinese , generally disatnbiguates character tneaning. The kanji character 'J \ [ ' , for example , can be used to mean any of `` to discern/discriminate '' , `` to speak/argue '' and `` a valve '' , but word context easily resolves such mnbiguity , hi this sense , our intuition is that segmented strings should produce better results than non-segmented strings. Looking to past research on similarity metrics for TM systelns , ahnost all systems involving aal ) anese as the source language rely on segnlentation ( e.g.</definiens>
				<definiens id="2">1992 ) and Sate and Kawase ( 1994 ) providing rare instances of character-based systelnS. By avoiding tile need to segment text ; , we : ( a ) alleviate computational overhead ; ( b ) avoid the need to commit ourselves to a particular analysis type in the case of ambiguity ; ( c ) avoi ( 1 the issue of ' how to deal with unknown words ; ( d ) avoid the need for stemming/lenlmatisation ; and ( e ) to a large extent get around problems related to the nornmlisation of lexical alternation ( see Baldwin and Tanaka ( 1999 ) for a discussion of problems related to lexical alternation in Jal ) anese ) . Additionally</definiens>
				<definiens id="3">in modelling seinantic similarity between related words with character overlap. With word-based indexing , this would only be possible with tile aid of a thesaurus. Similarly for word order , we would expect that translation records that preserve the word ( segment ) order observed in the inImt string would provide closer-matching translations than translation records containing those stone segnlents in a different order. Natur~dly , enforcing preservation of word order is going to place a significant burden on the matching mechanism , in that a number of different substring match schenlata are inevitably going to be produced between rely two strings , each of which nmst be considered on its own merits. To the authors ' knowledge , there is no TM system operating from Japanese that does not rely on word/segment/character order to some degree. Tanaka ( 1997 ) uses pivotal content words identified , by the user to search through the TM and locate translation records which contain those same content words in the stone order and preferably the stone segment distance apart. Nakamura ( 1989 ) similarly gives preference to translation records in which the content words contained in the original input occur in the same linear order , although there is tile scope to back off to translation records which do not I ) reserve the original word order. Sumita and Tsutsmni ( 19911 take the opposite tack in iteratively filtering out NPs and adverbs to leave only functional words and nlatrix-level predicates , and find trmlslation records which contain those same key words in the same ordering , preferably with the same segment types between them in the same numbers. Nirenburg et al. ( 1993 ) propose a word order-sensitive metric based on `` string composition discrepancy '' , and increlnentally relax the restriction on the quality of match required to inehlde word lenmlata , word synonynls and then word hyt ) ernylns , increasing the match penalty as they go. Sate and Kawase ( 1994 ) employ a more local model of character order in modelling similarity according to N-grams fashioned from the original string. The greatest advantage in ignoring word/segnlent order is computational , in that we significantly reduce the search space and require only a single overall comparison per string pair. Below , we analyse whether this gain in speed outweighs any losses in retrieval perfbrmance. Due to o111 '' interest in the efli~cts of both word order and seglnentation , we must have a selection of similarity lnetrics compatible with the various permutations of these two 1 ) arameter types. We choose to look at a nunlber of bag-of-words and word ordersensitive methods which are compatible with both character-based and word-based indexing , and vary the intmt to model tile etl~ects of the two indexing paradigms. The particular bag-of-word approactles we target are tlm vector space model ( Manning and Schiitze , 1.999 , p300 ) and `` token intersection '' , a silnple ratio-based similarity nletric. For word ordersensitive approaches , we test edit distance ( Wagner and Fisher , 1974 ; Planas and Furuse , 1999 ) , `` sequential correspondence '' and `` weigllted sequential correspondence '' . Each of tile similarity metrics eillpirically describes the sintilarity between two inlmt strings tmi mid i~. , 2 where we define tmi as a source language string taken fl'om the TM and i~. as the input string which we are seeking to 1hatch within the TM. One featnre of all similarity metrics given here is that they have fine-grained discriminatory potential and are able to narrow down the final set of translation candidates to a handfld of , and in nlost cases one , outlmt. This was a deliberate design decision , and aimed at example-based machine translation applications , where human judgement can not be relied upon to single out the most appropriate translation from multiple system outputs. In this , we set ourselves apart from the research of Sunlita and Tsutsumi ( 1.991 ) , for example , who judge the system to have been successful if there are a total of 100 or less outputs , aud a useful translation is contained within them. Note that it would be a relatively simple pro2Note that the ordering here is arbitrary , and that all the similarity metrics described herein are commutative for the given implementations. 36 cedure to fall ( ) lit the 11111111 ) e1 '' of Olltt ) lltS to it ill ollr case , tly taking tim top n ranking outputs. For all silnitarity metrics , we weight different .\ ] ai ) mmse segment tyl ) es according to their exl ) ected impact on translation , in the form of the sweigh , t fllnctioll : Segment type s , wcight punctuation 0 other segments 1 W ( ' exl ) erinlentally trialled intermediate swcight settings tbr ditt'erent character tyl ) es ( in the case of character-based indexing ) or segment tyl ) eS ( in the case of word-based indexing ) , none of which was fomtd to apl ) reciat ) ly iml ) rove performance. : ~ a.1 Similarity metrics used in this research Vector space model Within our imt ) lenmntation of the reactor space Inodol ( VSM ) , the segment content of each string is ( lescril ) ( '. ( l as a vector , ma ( le u l ) of 3 single dimension for each segment tok ( , n occurring within tmi or in. The. value of each vector eolnt ) onent is given as the weighted frequen ( -y of that token accor ( ling to its sweiqht vahle , such that any nulnber of 3 given i ) un ( : tuation mark will produce a fl'e ( luen ( : y of 0. The string sinfilarity of t ? H , i and in is then detined sis tim cosine of the angle l/etween vectors t\ [ \ [ ~.i and iT\ [ t , reSl ) ectivety , calculated as : tT~ , i , i~5 , cos ( t , fi , , , i ; 4 It , ll l 0 ) where dot l ) roduct and vect ( ) r length ( : oin ( : i ( le wil ; h l ; he standard detlnitions. The strings tmi of maximal similarity are th ( ) se whi ( : h i ) roduce the nmxinuun v3hw , for th ( ! v ( ~ctor cosine. Not ( ; that</definiens>
				<definiens id="4">'.gment fre ( tueney and is insensitive to word order. Token intersection The token intersection of tmi 3nd in is defined as the cumulative intersecting fl'equency of tokens appearing in each of the strings , normalised according to the combined segment lengths of tm</definiens>
			</definition>
			<definition id="1">
				<sentence>Ave , outputs 50.2 46.6 45.6 43.7 ( -0.8 % ) 43.0 ( -2.9 % ) 47.3 ( -5.9 % ) 43.1 ( -7.4 % ) 40.7 ( -10.7 % ) Ave. time Table 1 : Results for the different similarity metri ( : s under character-1 ) ased and word-based indexing single-word technical terms taken f1'Ol12 SI~ technical glossary , to multiple-sentence strings , at an average se .</sentence>
				<definiendum id="0">Ave</definiendum>
				<definiens id="0">Results for the different similarity metri ( : s under character-1 ) ased and word-based indexing single-word technical terms taken f1'Ol12 SI~ technical glossary , to multiple-sentence strings , at an average se</definiens>
			</definition>
			<definition id="2">
				<sentence>`` Accuracy '' is an indication of the prol ) ortion of intmts fbr whi ( : h 39 an optimal translation was produced ; characterbased indexing accuracies in bold indicate a significant ~ advantage over the corresponding wprd-based indexing accuracy , and figures in brackets for wordbased indexing indicate the relative pert'ormaime gain over the corresponding character-based indexing configuration .</sentence>
				<definiendum id="0">Accuracy</definiendum>
				<definiens id="0">an indication of the prol ) ortion of intmts fbr whi ( : h 39 an optimal translation was produced ; characterbased indexing accuracies in bold indicate a significant ~ advantage over the corresponding wprd-based indexing accuracy , and figures in brackets for wordbased indexing indicate the relative pert'ormaime gain over the corresponding character-based indexing configuration</definiens>
			</definition>
			<definition id="3">
				<sentence>A Matching Technique in Example-Based Machine Translation .</sentence>
				<definiendum id="0">Matching Technique</definiendum>
			</definition>
			<definition id="4">
				<sentence>CTM : An example-based translation aid system .</sentence>
				<definiendum id="0">CTM</definiendum>
				<definiens id="0">An example-based translation aid system</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>A sample dictionary enlry fl'agment for tile English noun chance illustrates what was said above : \ [ 1\ ] CHANCE1 \ [ 21 POR : S \ [ 3\ ] SYNT : COUNT , PREDTO , PREDTHAT \ [ 4\ ] DES : 'FACT ' , 'ABSTRACT ' \ [ 5\ ] D1.1 : OF , 'PERSON ' \ [ 6\ ] D2.1 : OF , 'FACT ' \ [ 7\ ] D2.2 : TO2 \ [ 8\ ] D2.3 : THAT1 \ [ 9\ ] SYN 1 : OPPORTUNITY \ [ 10\ ] MAG N : GOOD 1~FAIR 1~EXCELLENT \ [ 11\ ] ANTIMAGN : SLIGHT/SLIM/POOR/LITTLE1/ SMALL \ [ 12\ ] OPER1 : HA VE/STAND1 \ [ 13\ ] REAL1-M : TAKE \ [ 14\ ] ANTIREAL1 -M : MISSl \ [ 15\ ] INCEPOPER1 : GET \ [ 16\ ] FINOPER1 : LOSE \ [ 17\ ] CAUSFUNC1 : GIVE &lt; TOI &gt; /GIVE \ [ 18\ ] ZONE : R \ [ 19\ ] TRANS : SHANS/SLUCHAJ \ [ 20\ ] REG : TRADUCT2.00 \ [ 21\ ] TAKE : X \ [ 22\ ] LOC : R \ [ 23\ ] R : COMPOS/MODIF/POSSES \ [ 24\ ] CHECK \ [ 25\ ] 1.1 DEP-LEXA ( X , Z , PREPOS , BY1 ) \ [ 26\ ] N:01 \ [ 27\ ] CHECK \ [ 2811.1 DOM ( X , * , R ) \ [ 29\ ] DO \ [ 30\ ] 1 ZAMRUZ : Z ( PO1 ) \ [ 31\ ] 2 ZAMRUZ : X ( SLUCFtAJNOST ' ) \ [ 32\ ] N:02 \ [ 33\ ] CHECK \ [ 34\ ] 2.1 DOM ( X , * , * ) \ [ 351 DO \ [ 36\ ] I ZAMRUZ : Z ( SLUCHAJNO ) \ [ 37\ ] 2 STERUZ : X \ [ 38\ ] TRAF : RA-EXPANS.16 \ [ 39\ ] LA : THAT1 \ [ 40\ ] TRAF : RA-EXPANS.22 lane \ [ 12\ ] part of speech : a noun .</sentence>
				<definiendum id="0">DES</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">MISSl \ [ 15\ ] INCEPOPER1 : GET \ [ 16\ ] FINOPER1 : LOSE \ [ 17\ ] CAUSFUNC1 : GIVE &lt; TOI &gt; /GIVE \ [ 18\ ] ZONE : R \ [ 19\ ] TRANS : SHANS/SLUCHAJ \ [ 20\ ] REG : TRADUCT2.00 \ [ 21\ ] TAKE : X \ [ 22\ ] LOC : R \ [ 23\ ]</definiens>
			</definition>
			<definition id="1">
				<sentence>UNL is a comlmter language intended to represent infolmation in a way that allows to generate a text expressing this information in a very large number of nahtral languages .</sentence>
				<definiendum id="0">UNL</definiendum>
				<definiens id="0">a comlmter language intended to represent infolmation in a way that allows to generate a text expressing this information in a very large number of nahtral languages</definiens>
			</definition>
			<definition id="2">
				<sentence>A UNL expression is an oriented hyper-graph that corresponds to a NL sentence in the amount of information conveyed .</sentence>
				<definiendum id="0">UNL expression</definiendum>
				<definiens id="0">an oriented hyper-graph that corresponds to a NL sentence in the amount of information conveyed</definiens>
			</definition>
			<definition id="3">
				<sentence>UNL has a tim relation that holds between an event and its linle .</sentence>
				<definiendum id="0">UNL</definiendum>
				<definiens id="0">a tim relation that holds between an event and its linle</definiens>
			</definition>
</paper>

		<paper id="2133">
			<definition id="0">
				<sentence>Centering ( Grosz et al. , 1995 ) is a i } amework for predicting local attentional focus .</sentence>
				<definiendum id="0">Centering</definiendum>
				<definiens id="0">a i } amework for predicting local attentional focus</definiens>
			</definition>
			<definition id="1">
				<sentence>CB Backward-looking center |trOllOU 11 category Possible Values def= tile pronoun is one of { it , its , itself , them , dmy , thcin themselves } dcm = the inonoun is one of { that , this , these , fllose } Y = prOllOtltl subject of lllaill clause of its ullerance N : pronotm not subject of main clause I'I~ , ( ) NOUN = antecedent is pronoun NI ' = antecedent is tmse noun phrase N ( ) N-NP = antecedent is other constituent , at most one utterance long NONE = pronotm is lit'st mention or antecedent length &gt; one tttterance SAME = antecedent and pronoun in same utterance AI ) J = antecedent and pronoun in adjacent utterances RI { MOTE = antecedent more than one utterance before pronoun Y = alSteccdelll subject o1 ' the lllain chmse of its tttterance N = antecedent not subject of a main clause Y = pronoun is Cb of its utterance N = pronoun is not Cb DIST cldj .</sentence>
				<definiendum id="0">tile pronoun</definiendum>
				<definiens id="0">lit'st mention or antecedent length &gt; one tttterance SAME = antecedent</definiens>
			</definition>
			<definition id="2">
				<sentence>REFCAT is an additional variable that describes the senmntic category of a pronoun 's referent ( eg .</sentence>
				<definiendum id="0">REFCAT</definiendum>
			</definition>
			<definition id="3">
				<sentence>R : A language for data analysis and graphics .</sentence>
				<definiendum id="0">R</definiendum>
			</definition>
			<definition id="4">
				<sentence>The Computational Processing of lntonatiolml Prominence : A Functional Prosody Perspective .</sentence>
				<definiendum id="0">Computational Processing of lntonatiolml Prominence</definiendum>
			</definition>
</paper>

		<paper id="1085">
			<definition id="0">
				<sentence>Within ollr gl ; aillmar , heads arc ( usually ) ext ) lMtly marke { t. This 1110 ; /,118 W ( ~ do l\ ] ot \ ] l ; ~v { ~ to Ill &amp; k ( ~ ; lily gllossos w } lcll identit\ [ ying the head of a. local tree .</sentence>
				<definiendum id="0">aillmar , heads arc</definiendum>
				<definiens id="0">the head of a. local tree</definiens>
			</definition>
</paper>

		<paper id="2094">
			<definition id="0">
				<sentence>The semantically smoothed probability of a pair ( v , n ) is calculated in a latent class ( LC ) model as pLC ( V , n ) = ~ &lt; cPLC ( C , v , 'n ) . The joint distribution is defined by PLC ( C , v , n ) = PLC ( C ) PLc ( V\ [ C ) PLC ( nIC ) . By construction , conditioning of v and n on each other is solely made through the classes c. The parameters PLC ( C ) , PLC ( V\ [ C ) , PLC ( n\ [ c ) are estilnated by a particularily silnple version of tile EM algorithm for context-free models. Input to our clustering algorithm was a training corpus of 1,178,698 tokens ( 608,850 types ) of verb-noun pairs participating in the grammatical relations of intransitive and transitive verbs and their subjectand object-fillers. Fig. 1 shows an induced class froln a model with 35 classes. Induced classes often have a basis in lexical semantics ; class 19 can be interpreted as locative , involving location nouns `` room '' , `` are£ ' , and `` world '' and verbs as `` enter '' and `` cross '' . Classes using EM-estimation To induce latent classes tbr the object slot ; of a fixed transitive verb v , another statistical inference step was performed. Given a latent class modal PLC ( ' ) Ibr verb-noun pairs , and a sample nl , ... , nM of objects for a fixed transitive verb , we calculate tile probability of ml arbitrary object noun , I , I~ N by p ( n ) = ~ &lt; cP ( C , ~ ; , ) = ~ &lt; c P ( c ) pLc ( n'Ic ) '' This fine-tuning of the class parameters p ( c ) to tile sample of objects for a fixed verb is formalized again as a simple instance of the EM algorithm. In an experiment with English data , we used a clustering model with 35 classes. From the maximum probabil650 ity pm : ses derived fl ) r the British National Corpus with the head-lexicalized parser of Carroll and Rooth ( 1.998 ) , we extracted frequency tables tbr transitive verb-noun pairs. These tables were used to induce a small class-labeled lexicon ( 336 verbs ) . cross.aso : o 19 0.692 mind 74.2 road 30.3 line 28.1 1 ) ridge 27.5 room 20.5 t ) order 17.8 l ) oundary 16.2 river 14.6 street 11.5 atlantic 9.9 mobilize.aso : o 6 0.386 h ) rce 2.00 t ) eoi ) le 1.95 army 1.46 sector 0.90 society 0.90 worker 0.90 meinber 0.88 company 0.86 majority 0.85 party 0.80 lID 160867 ) Es gibt einigc alte Passvorschriften , die. besagen , dass man cinch Pass habcn muss , wcnn man dic Grenze iiberschreitct. There are some old provisions rega.rding passports which state that people crossing the { border/ frontier/ boundary/ limit/ periphery/ edge } shoukI have their 1 ) assl ) ort on them. lID 201946 ) Es 9ibt sehlie.sslich keinc L5sung ohne die Mobilisierung der bii~yerlichen Gesellschaft und die Solidaritiit dcr Dcmok , nten in der 9anzcn Welt. Ttmrc can be no solution , tinally , mflcss civilian { company/ society/companionship/party/associate } is mobilized and solidarity demonstrated by democrats throughout the world. Figure 3 : Exami ) les for target-word ambiguities Figure 2 : Estinmted fl : equencies of the objects of ' the transitive verbs cross and mobilize Fig. 2 shows the topmost parts of the lexical entries for the transitive verbs cross and mobilize. Class 19 is the most prol ) abh ~ , class-label for the ol ) jeet-slot of cross ( prol ) al ) ility 0.692 ) ; tl~e objects of mobilize belong with prol ) ability ( : lass for this slot. Fig. 2 shows for each verb the tell llOllllS 'It with highest estimated frequencies .l ' , , ( 'n , ) = f ( n ) p ( cln ) , where .flu ) is the fre ( \ ] ll ( ~.ll ( : y of n in the sample v , l , • • • , 'n , M. For example , the Dequency of seeing mind as object of c , ro.ss is estimated as 74.2 times , and the most fl'equent object of mobilize is estimated to be force. Cluster-Based Lexicons Ii : t the following , we will des ( : ril ) e the simt } le and natural lexicon look-up mechanism which is eml ) loyed in our disambiguation at ) t ) roach. Consider Fig. 3 which shows two bilingual sentences taken from our evaluation corlms ( see Sect. 4 ) . The source-words and their corresponding target-words are highlighted in bold thee. The correct translation of the source-noun ( e.g. Gre.nzc ) as deternfined by the actual trmlslators is replaced by the set of alterlmtive translations ( e.g. { border , frontier , b ( mndary , limit , peril ) hcry , edge } ) as proposed by the word-to-word dictionary of Fig. 5 ( see Sect. LI ) . The prol ) lem to be solved is to lind a correct l ; ranslation of the source-word using only minimal contextual intbrmation. In our apt ) roach , the decision between alternative target-nouns is done by llSillg only int'ormal , ion provided by the governing target-verb. The key idea is to back up this nfinimal information with the condensed and precise information of a probabilistic classbased lexicon. The criterion for choosing an alterlmtive target-noun is thus the best fit of the lexical and semantic information of the target : noun to the semantics of the argument-slot of the target-verb. This criterion is checked by a silnple lexicon look-up where the target-noun with highest estinmted class-based fl'equeney is determined. Fornmlly , choose l ; 11 ( ; tm'get-nom~ gt , ( and a class ~ ? ) such that j &amp; , , ) = .ax 'nC N~c~C where L- ( -. ) = f ( - , ) v ( d-. ) is the estimated frequency of 'n , in tile sample of objects of a fixed target-verb , p ( cl , n ) is the class-melnbershi t ) probability of'n in c as determined by the probabilistic lexicon , and f ( n ) is the frequency of n in the combined sample of objects and trmlslation alternatives1. Consider example ID 160867 fron , Fig. 3. The mnbiguity to be resolved concerns the direct objects of the verb cross whose lexical entry is partly shown in Fig. 2. Class 19 and the noun border is the pair yielding a higher estimated trequency than any other combination of a class and an alternative translation such as boundary. Similarly , for example ID 301946 , the pair of the 1Note that p ( 8 ) = max p ( c ) in most , but llOt all cases. cEC 651 target-noun society and class 6 gives highest estimated frequency of the objects of mobilize. We evaluated our resolution methods on a pseudo-disambiguation task sinlilar to that used in Rooth et al. ( 1999 ) for evaluating clustering models. We used a test set of 298 ( v , n , n ~ ) triples where ( v , n ) is chosen randomly from a test corpus of pairs , and n ~ is chosen randomly according to the marginal noun distribution for the test corpus. Precision was calculated as the nmnber of times the disambiguation method decided for the non-random target noun ( f~. = n ) . As shown in Fig. 4 , we obtained 88 % precision for the class-based lexicon ( ProbLex ) , which is a gain of 9 % over the best clustering model and a gain of 15 % over the hmnan baseline 2 . human clustering ProbLex mnt ) iguity baseline 2 73.5 % 79.0 % 88.3 % I Figure 4 : Evaluation on pseudo-disambiguation task for noun-ambiguity The results of the pseudo-disambiguation could be confirmed in a fllrther evaluation on a large number of randonfly selected examples of a real-world bilingual corpus. The corpus consists of sentence-aligned debates of the European parliament ( mlcc = multilingual corpus for cooperation ) with ca. 9 million tokens for German and English. From this corpus we prepared a gold standard as follows. We gathered word-to-word translations from online-available dictionaries and eliminated German nouns fbr which we could not find at least two English translations in the mice-corpus. The resulting 35 word dictionary is shown in Fig. 5. Based on this dictionary , we extracted all bilingual sentence pairs from the corpus which included both the source-noun and the target-noun. We restricted the resulting ca. 10,000 sentence pairs to those which included a source-noun from this 2Similar results for pseudo-dismnbiguation were obtained for a simpler approach which avoids another EM application for probabilistic class labeling. Here ~ ( and ~ ) was chosen such that f~ ( v , ~ ) = max ( ( fLc ( v , n ) + 1 ) pcc ( el v , n ) ) . However , the sensitivity to class-parmnetcrs was lost in this approach. dictionary in the object position of a verb. Fm'therniore , the target-object was required to be included in our dictionary mid had to appear in a similar verb-object position as the sourceobject fbr an acceptable English translation of the German verb. We marked the German noun n q in the source-sentence , its English translation ne as appearing in the corpus , and the English lexical verb re. For the 35 word dictionary of Fig. 5 this senti-automatic procedure resulted ill a test corpus of 1,340 examples. The average ambiguity in this test corpus is 8.63 translations per source-word. Furthermore , we took the semantically most distant translations for 25 words which occured with a certain fi'equency in the ew~luation corpus. This gave a corpus of 814 examples with an average ambiguity of 2.83 translations. The entries belonging to this dictionary are highlighted in bold face in Fig. 5. The dictionaries and the related test corpora are available on the web 3. We believe that an evaluation on these test corpora is a realistic simulation of the hard task of target-language disambiguation in real-word machine translation. The translation alternatives are selected fl'om online dictionaries , correct translations are deternfined as the actual translations found in the bilingual corpus , no examples are omitted , the average ambiguity is high , and the translations are often very close to each other. In constrast to this , most other evaluations are based on frequent uses of only two clearly distant senses that were deternfined as interesting by the experimenters. Fig. 6 shows the results of lexical ambiguity resolution with probabilistic lcxica in comparison to simpler methods. The rows show the results tbr evaluations on the two corpora with average ambiguity of 8.63 and 2.83 respectively. Colunm 2 shows the percentage of correct translations found by disambiguation by random choice. Column 3 presents as another baseline disambiguation with the major sense , i.e. , always choose the most frequent targetnoun as translation of the source-noun. In colunto 4 , the empirical distribution of ( v , n ) pairs in the training corpus extracted from the BNC is used as disambiguator. Note that this method yields good results in terms of precision ( P # correct / $ correct + $ incorrect ) , but is much 3http : //www. ims .uni-stuttgart. de/proj ekt e/gramot ron/ 652 Angrif\ [ Art Aufgabe Auswahl Begriff Boden Einricht ung Erweit erung Fehler Gcnehmigung Clesehichte Gesdlschaft O|-el|ze Grund Karte Lage Mangel Menge Prfifung Schwlerigkelt Seite Sicherheit Sthnme 'rerlllin `` Vet'blnd ung Verbot Verpflicht u ng `` ~'et't rallen `` Wahl `` lgVeg Widerstand Zeiehen Ziel Z \ [ isaln 111 e llll alia Zustlmmung aggression , assault , oll ) 2nce , onset , onsbmght , attack , charge , raid , whammy , inroad form~ type , way , fashion , lit , kind , wise , lllallller , species , mode , sort , wtriety abandonment~ otIieo~ task , exercise , lesson , giveup , jot ) , 1 ) roblcm , tax eligibility , selection , choice , wwlty , assortment , extract , range , sample concept , item , notion , idea ground , land , soil , floor , bottom arrangement , institution , constitution , cstablishlnellt , feature , installation , construction , setup , adjustment , composition , organization amplification , extension , enhancement , expansion , dilatation , upgr~ding , add-on , increment error~ shortcoming , blemish , I ) lunder , bug , defect , demerit , failure , fault , flaw , mistake , trouble , slip , blooper , lapsus pernlission , approval , COllSellt , acceptance , al ) l ) robation , authorization hlstory~ story , tale , saga , strip company~ society , COmlmnionshil ) , party , associate border , frontier , boundary , Ihnlt , periphery , edge nlaster~ nlatter~ reasoll~ base , catlse , grOlllld~ bottolii root card , map , ticket , chart site , situation , position , bearing , layer , tier deficiency , lack , privation , want , shortage , shortcoming , absence , dearth , demerit , desideratum , insufticimlcy , paucity , scarceness alnollnt~ deal , lot , Illass I mtlltitttde , l ) lenty , qtlalltity , quiverful~ vOhlllle 1 abull ( latlce , aplellty 1 assemblage , crowd , batch , crop , heal ) , lashings , scores , set , loads , I ) ulk examinatlon , scrutiny , verification , ordeal , test , trial , inspection , tryout , assay , canvass , check , inquiry~ perusal , reconsideration , scruting difficttlty~ trollllle 1 problenl , severity , ar ( lotlSlleSS 1 heaviness page~ party~ slde , point , aspect certainty , guarantee , safety , immunity , security , collateral , doubtlessness , sureness , deposit voice~ vote , tones elate , deadline~ meethtg , appointment , time , term assoclation , contact , link~ cha\ [ ll , ColIjtlnCtlOll~ COlll/ectioll~ fllSiOll , joint , conlpOtlll ( l~ alliance , cl~tenation , tie , lllllOIl I t ) Olld~ interface , liaison , touch , relation , incorporation ban , interdiction , I ) rohibition , forbiddance eominitment : obligation , undertaking , duty , indebtedness , onus , debt , engagement , liability , bond COllfidence~ rellance , trllst~ faith , asstlrance~ dependence , private , secret election , option , choice , ballot , alternagive , poll , list path~ road , way , alley , route , lane resistance , opposition , drag character , icon , Sigll I sigllal , Syllll ) ol , lllark , tokell~ figure , olneil ahn , destination , end , designation , target , goal , object , objective , sightings , intentimb prompt coherence , context~ COlltlgtllty , connectloli agreeinent~ approvaI~ assont , accordance , approbation , consent , afIinnation , allowance , compliance , comi ) Iiancy , acclamation Figure 5 : Dictionaries extracted from online resources ambiguity random major emlfirical sense distrib , clusl ; ering ProbLex P : 46.1 % 8.63 14.2 % 31.9 % E : 36.2 % 43.3 % 49.4 % P : 60.8 % 2.83 35.9 % 45.5 % E : 49.4 % 61.5 % 68.2 % Figure 6 : Disambig , mtion results for clustering versus probabilistic lexicon methods worse in terms of effectiveness ( E //corre ( -t / \ ] /-correct q # : incorrect t \ ] / : do n't know ) . The reason for this is that even if the distribution ( ff ( v , n ) pairs is estimated quite precisely for the pairs in the large training corpus , there are still many pairs which receive the same or no positive probability at all. These effects can'be overcome by a clustering approach to disambiguation ( column 5 ) . Here the class-smoothed probability of a ( v , n ) pair is used to decide between alternative target-nouns. Since the clustering model assigns a more fine-grained probability to nearly every pair in its domain , there are no do n't know cases for comparable precision values. However , the senmntically smoothed probability of the clustering models is still too coarse-grained when compared to a disambiguation with a prot ) abilistic lexicon. Here ~ fllrther gain in precision and equally effectiveness of ca. 7 % is obtained on both corpora ( column 6 ) . We conjecture that this gain ( : an be attrilmted to the combination of Dequency iilformation of the nouns and the fine-tuned distribution on the selection classes of the the nominal arguments of the verbs. We believe that including the set of translation alternatives in the ProbLex distribution is important for increasing efficiency , because it gives the dismnbiguation model the opportunity to choose among unseen alternatives. Furthermore , it seems that the higher precision of ProbLex can not be attributed to filling in zeroes in the empirical distribution. Rather , we speculate that ProbLex intelligently filters the empirical distribution by reducing maximal 653 counts for observations which do not fit into classes. This might help in cases where the empirical distribution has equal values for two alternatives. source target Seite Sicherheit Verbindung Verpflichtung Ziel overall precision page side guarantee safety commction link commitment obligation objective target Figure 7 : Precision for finding correct and acceptable translations by lexicon look-up Fig. 7 shows the results for disambiguation with probabilistic lexica for five sample words with two translations each. For this dictionary , a test corpus of 219 sentences was extracted , 200 of which were additionally labeled with acceptable translations. Precision is 78 % for finding correct translations and 90 % for finding acceptable translations. Furthermore , in a subset of 100 test items with average ambiguity 8.6 , a lmnlan judge having access only to the English verb and the set of candidates for the targel , -lloun , i.e. the information used by the model , selected anlong translations. On this set ; , human precision was 39 % . Fig. 8 shows a comparison of our approadl to state-of-the-art unsupervised algorithlns for word sense disambiguation. Column 2 shows the number of test examples used to evaluate the various approaches. The range is from ca. 100 examples to ca. 37,000 examples. Our method was evaluated on test corpora of sizes 219 , 814 , and 1,340. Column 3 gives the average number of senses/eranslations for the different disambiguation methods. Here the range of the ambiguity rate is from 2 to about 9 senses 4. Column 4 4The mnbiguity factor 2.27 attributed to Dagan and Itai 's ( 1994 ) experiment is calculated by dividing their average of 3.27 alternative translations by their average of 1.44 correct translations. Furthermore , we calculated the ambiguity factor 3.51 for Resnik 's ( 1997 ) experiment shows the rmldom baselines cited for the respective experiments , ranging t'rom ca. 11 % to 50 % . Precision values are given in column 5. In order to compare these results which were computed for different ambiguity factors , we standardized the measures to an evaluation for binary ambiguity. This is achieved by calculal ; ing pl/log2 arab for precision p and ambiguity factor arab. The consistency of this `` binarization '' can be seen by a standardization of the different random baselines which yields a value of ca. 50 % for all approaches 5. The standardized precision of our approach is ca. 79 % on all test corpora. The most direct point of comparison is the method of Dagan and Itai ( 1994 ) whirl1 gives 91.4 % precision ( 92.7 % standardized ) and 62.1 % effectiveness ( 66.8 % standardized ) on 103 test ; examples for target word selection in the transfer of Hebrew to English. However , colnpensating this high precision measure for the low effectiveness gives values comparable to our results. Dagan and Itai 's ( 1994 ) method is based on a large variety of gramnmtieal relations tbr verbal , nominal , and adjectival predicates , but no class-based infornmtion or slot-labeling is used. I { esnik ( 1997 ) presented a disambiguation method which yields 44.3 % precision ( 63.8 % standardized ) tbr a test set of 88 verb-object tokens. His approach is coral ) arable to ours in terlns of infbrmedness of the ( tisambiguator. Hc also uses a class-based selection measure , but based on WordNet classes. However , the task of his evaluation was to select WordNet-senses tbr the objects rather than the objects themselves , so the results can not be compared directly. The stone is true for the SENSEVAL evaluation exelcise ( Kilgarriff and Rosenzweig , 2000 ) -- there word senses from the HECTOl~-dictionary had to be disambiguated. The precision results for the ten unsupervised systems taking part in the comt ) etitive evaluation ranged Kern 20-65 % at efficiency values from 3-54 % . The SENSEVAL '~tan ( lard is clearly beaten by the earlier results of Yarowsky ( 1995 ) ( 96.5 % precision ) and Schiitze ( 1992 ) ( 92 % precision ) . However , a comparison to these refrom his random baseline 28.5 % by taking 100/28.5 ; reversely , Dagan and Itai 's ( 1994 ) random baseline can be calculated as 100/2.27 = 44.05. Tile ambiguity t ; '~ctor for SENSEVAL is calculated for tile llOUll task in the English SENSEVAL test set. 5Note that we are guaranteed to get exactly 50 % standardized random 1 ) aseline if random , arab = 100 % . 654 disambiguation corlms random precision method size aml ) iguity random 1 ) recision ( standardized ) ( standardized ) ) robLex 1 340 8.63 14.2 % 49.4 % 53.4 % 79.7 % 814 2.83 35.9 % 68.2 % 50.5 % 77.5 % 219 2 50.0 % 78.0 % 50.0 % 78.0 % ) agan , Itai 94 { esnik 97 ; ENSEVAL 00 ( m'owsky 95 ' , chiitze 92 103 88 2 756 37 000 3 000 2 2 44.1 % 28.5 % 10.9 % 50.0 % 50.0 % P : 91.4 % E : 62.1 % 44.3 % P : 20-65 % E : 3-54 % 96.5 % 92.0 % 50.0 % 50.0 % 50.0 % 50.0 % 50.0 % P : 92.7 % E : 66.8 % 63.8 % P : 60-87 % E : 33-83 % 96.5 % 92.0 % Figure 8 : Comparison of unsupervised lexical disambiguation methods. sults is again somewhat difficult. Firstly , these at ) proaches were ewfluated on words with two clearly ( tistmlt senses which were de/ ; el'nfined by the experimenters. In contrast , our method was evalutated on randonfly selected actual translations of a large t ) ilingual cortms. Furthermore , these apl ) roaches use large amounts of infbrmation in terms of linguistic ca.tegorizations , large context windows , or even 1111nual intervention such as initial sense seeding ( hqtrowsky , 1995 ) . Such information is easily obtainabh ; , e.g. , in I1\ ] . at ) tflications , but often burdensome to gather or sim.i ) ly uslavail~bh'~ in situations such as incremental parsing O1 ' translation. The disanfl3iguation method presented in this pa.per delibera.tely is restricted to the limited mnomlt of information provided by a probabilistic class-based lexicon. This intbrmation yet proves itself accurate enough to yield good empirical results , e.g. , in target-language disambiguation. The t ) rol ) al ) ilistic class-based lexica are induced in an unsupervised manner fl'om large mmnnotated corpora. Once the lexica are constructed , lexical mnbiguity resolution can be done by a simple lexicon look-up. I51 targetword selection , the nlOSt fl'equent target-noun whose semantics fits best to tit ( ; semantics of the argument-slot of the target-verb is chosen. We evaluated our method on randomly selected examities Dora real-world bilingual corpora which constitutes a realistic hard task. Dismnbiguation based on probabilistie lexica perfornmd satisfim- ' tory for this | ; ask. The lesson lem'ned tYom our experimental results is that hybrid models con &gt; bining fi : equency information and class-based t ) robabilities outlmrtbnn both pure fl'equencybased models and pure clustering models .</sentence>
				<definiendum id="0">.flu )</definiendum>
				<definiendum id="1">f ( n )</definiendum>
				<definiendum id="2">noun border</definiendum>
				<definiens id="0">The semantically smoothed probability of a pair ( v , n ) is calculated in a latent class ( LC ) model as pLC ( V , n ) = ~ &lt; cPLC ( C , v , 'n ) . The joint distribution is defined by PLC ( C , v , n ) = PLC ( C ) PLc ( V\ [ C ) PLC ( nIC ) . By construction , conditioning of v and n on each other is solely made through the classes c. The parameters PLC ( C ) , PLC ( V\ [ C ) , PLC ( n\ [ c ) are estilnated by a particularily silnple version of tile EM algorithm for context-free models. Input to our clustering algorithm was a training corpus of 1,178,698 tokens ( 608,850 types ) of verb-noun pairs participating in the grammatical relations of intransitive and transitive verbs and their subjectand object-fillers. Fig. 1 shows an induced class froln a model with 35 classes. Induced classes often have a basis in lexical semantics ; class 19 can be interpreted as locative , involving location nouns `` room '' , `` are£ ' , and `` world '' and verbs as `` enter '' and `` cross '' . Classes using EM-estimation To induce latent classes tbr the object slot ; of a fixed transitive verb v , another statistical inference step was performed. Given a latent class modal PLC ( ' ) Ibr verb-noun pairs , and a sample nl , ... , nM of objects for a fixed transitive verb , we calculate tile probability of ml arbitrary object noun , I , I~ N by p ( n ) = ~ &lt; cP ( C , ~ ; , ) = ~ &lt; c P ( c ) pLc ( n'Ic ) '' This fine-tuning of the class parameters p ( c ) to tile sample of objects for a fixed verb is formalized again as a simple instance of the EM algorithm. In an experiment with English data , we used a clustering model with 35 classes. From the maximum probabil650 ity pm : ses derived fl ) r the British National Corpus with the head-lexicalized parser of Carroll and Rooth ( 1.998 ) , we extracted frequency tables tbr transitive verb-noun pairs. These tables were used to induce a small class-labeled lexicon ( 336 verbs ) . cross.aso : o 19 0.692 mind 74.2 road 30.3 line 28.1 1 ) ridge 27.5 room 20.5 t ) order 17.8 l ) oundary 16.2 river 14.6 street 11.5 atlantic 9.9 mobilize.aso : o 6 0.386 h ) rce 2.00 t ) eoi ) le 1.95 army 1.46 sector 0.90 society 0.90 worker 0.90 meinber 0.88 company 0.86 majority 0.85 party 0.80 lID 160867 ) Es gibt einigc alte Passvorschriften , die. besagen , dass man cinch Pass habcn muss , wcnn man dic Grenze iiberschreitct. There are some old provisions rega.rding passports which state that people crossing the { border/ frontier/ boundary/ limit/ periphery/ edge } shoukI have their 1 ) assl ) ort on them. lID 201946 ) Es 9ibt sehlie.sslich keinc L5sung ohne die Mobilisierung der bii~yerlichen Gesellschaft und die Solidaritiit dcr Dcmok , nten in der 9anzcn Welt. Ttmrc can be no solution , tinally , mflcss civilian { company/ society/companionship/party/associate } is mobilized and solidarity demonstrated by democrats throughout the world. Figure 3 : Exami ) les for target-word ambiguities Figure 2 : Estinmted fl : equencies of the objects of ' the transitive verbs cross and mobilize Fig. 2 shows the topmost parts of the lexical entries for the transitive verbs cross and mobilize. Class 19 is the most prol ) abh ~ , class-label for the ol ) jeet-slot of cross ( prol ) al ) ility 0.692 ) ; tl~e objects of mobilize belong with prol ) ability ( : lass for this slot. Fig. 2 shows for each verb the tell llOllllS 'It with highest estimated frequencies .l ' , , ( 'n , ) = f ( n ) p ( cln )</definiens>
				<definiens id="1">the fre ( \ ] ll ( ~.ll ( : y of n in the sample v , l , • • • , 'n , M. For example , the Dequency of seeing mind as object of c , ro.ss is estimated as 74.2 times , and the most fl'equent object of mobilize is estimated to be force. Cluster-Based Lexicons Ii : t the following , we will des ( : ril ) e the simt } le and natural lexicon look-up mechanism which is eml ) loyed in our disambiguation at ) t ) roach. Consider Fig. 3 which shows two bilingual sentences taken from our evaluation corlms ( see Sect. 4 ) . The source-words and their corresponding target-words are highlighted in bold thee. The correct translation of the source-noun ( e.g. Gre.nzc ) as deternfined by the actual trmlslators is replaced by the set of alterlmtive translations ( e.g. { border , frontier , b ( mndary , limit , peril ) hcry , edge } ) as proposed by the word-to-word dictionary of Fig. 5 ( see Sect. LI ) . The prol ) lem to be solved is to lind a correct l ; ranslation of the source-word using only minimal contextual intbrmation. In our apt ) roach , the decision between alternative target-nouns is done by llSillg only int'ormal , ion provided by the governing target-verb. The key idea is to back up this nfinimal information with the condensed and precise information of a probabilistic classbased lexicon. The criterion for choosing an alterlmtive target-noun is thus the best fit of the lexical and semantic information of the target : noun to the semantics of the argument-slot of the target-verb. This criterion is checked by a silnple lexicon look-up where the target-noun with highest estinmted class-based fl'equeney is determined. Fornmlly , choose l ; 11 ( ; tm'get-nom~ gt , ( and a class ~ ? ) such that j &amp; , , ) = .ax 'nC N~c~C where L- ( -. ) = f ( - , ) v ( d-. ) is the estimated frequency of 'n , in tile sample of objects of a fixed target-verb , p ( cl , n ) is the class-melnbershi t ) probability of'n in c as determined by the probabilistic lexicon , and</definiens>
				<definiens id="2">the frequency of n in the combined sample of objects and trmlslation alternatives1. Consider example ID 160867 fron , Fig. 3. The mnbiguity to be resolved concerns the direct objects of the verb cross whose lexical entry is partly shown in Fig. 2. Class 19 and the</definiens>
				<definiens id="3">the pair yielding a higher estimated trequency than any other combination of a class and an alternative translation such as boundary. Similarly , for example ID 301946 , the pair of the 1Note that p ( 8 ) = max p ( c ) in most , but llOt all cases. cEC 651 target-noun society and class 6 gives highest estimated frequency of the objects of mobilize. We evaluated our resolution methods on a pseudo-disambiguation task sinlilar to that used in Rooth et al. ( 1999 ) for evaluating clustering models. We used a test set of 298 ( v , n , n ~ ) triples where ( v , n ) is chosen randomly from a test corpus of pairs , and n ~ is chosen randomly according to the marginal noun distribution for the test corpus. Precision was calculated as the nmnber of times the disambiguation method decided for the non-random target noun ( f~. = n ) . As shown in Fig. 4 , we obtained 88 % precision for the class-based lexicon ( ProbLex ) , which is a gain of 9 % over the best clustering model and a gain of 15 % over the hmnan baseline 2 . human clustering ProbLex mnt ) iguity baseline 2 73.5 % 79.0 % 88.3 % I Figure 4 : Evaluation on pseudo-disambiguation task for noun-ambiguity The results of the pseudo-disambiguation could be confirmed in a fllrther evaluation on a large number of randonfly selected examples of a real-world bilingual corpus. The corpus consists of sentence-aligned debates of the European parliament ( mlcc = multilingual corpus for cooperation ) with ca. 9 million tokens for German and English. From this corpus we prepared a gold standard as follows. We gathered word-to-word translations from online-available dictionaries and eliminated German nouns fbr which we could not find at least two English translations in the mice-corpus. The resulting 35 word dictionary is shown in Fig. 5. Based on this dictionary , we extracted all bilingual sentence pairs from the corpus which included both the source-noun and the target-noun. We restricted the resulting ca. 10,000 sentence pairs to those which included a source-noun from this 2Similar results for pseudo-dismnbiguation were obtained for a simpler approach which avoids another EM application for probabilistic class labeling. Here ~ ( and ~ ) was chosen such that f~ ( v , ~ ) = max ( ( fLc ( v , n ) + 1 ) pcc ( el v , n ) ) . However , the sensitivity to class-parmnetcrs was lost in this approach. dictionary in the object position of a verb. Fm'therniore , the target-object was required to be included in our dictionary mid had to appear in a similar verb-object position as the sourceobject fbr an acceptable English translation of the German verb. We marked the German noun n q in the source-sentence , its English translation ne as appearing in the corpus , and the English lexical verb re. For the 35 word dictionary of Fig. 5 this senti-automatic procedure resulted ill a test corpus of 1,340 examples. The average ambiguity in this test corpus is 8.63 translations per source-word. Furthermore , we took the semantically most distant translations for 25 words which occured with a certain fi'equency in the ew~luation corpus. This gave a corpus of 814 examples with an average ambiguity of 2.83 translations. The entries belonging to this dictionary are highlighted in bold face in Fig. 5. The dictionaries and the related test corpora are available on the web 3. We believe that an evaluation on these test corpora is a realistic simulation of the hard task of target-language disambiguation in real-word machine translation. The translation alternatives are selected fl'om online dictionaries , correct translations are deternfined as the actual translations found in the bilingual corpus , no examples are omitted , the average ambiguity is high , and the translations are often very close to each other. In constrast to this , most other evaluations are based on frequent uses of only two clearly distant senses that were deternfined as interesting by the experimenters. Fig. 6 shows the results of lexical ambiguity resolution with probabilistic lcxica in comparison to simpler methods. The rows show the results tbr evaluations on the two corpora with average ambiguity of 8.63 and 2.83 respectively. Colunm 2 shows the percentage of correct translations found by disambiguation by random choice. Column 3 presents as another baseline disambiguation with the major sense , i.e. , always choose the most frequent targetnoun as translation of the source-noun. In colunto 4 , the empirical distribution of ( v , n ) pairs in the training corpus extracted from the BNC is used as disambiguator. Note that this method yields good results in terms of precision ( P # correct / $ correct + $ incorrect ) , but is much 3http : //www. ims .uni-stuttgart. de/proj ekt e/gramot ron/ 652 Angrif\ [ Art Aufgabe Auswahl Begriff Boden Einricht ung Erweit erung Fehler Gcnehmigung Clesehichte Gesdlschaft O|-el|ze Grund Karte Lage Mangel Menge Prfifung Schwlerigkelt Seite Sicherheit Sthnme 'rerlllin `` Vet'blnd ung Verbot Verpflicht u ng `` ~'et't rallen `` Wahl `` lgVeg Widerstand Zeiehen Ziel Z \ [ isaln 111 e llll alia Zustlmmung aggression , assault , oll ) 2nce , onset , onsbmght , attack , charge , raid , whammy , inroad form~ type , way , fashion , lit , kind , wise , lllallller , species , mode , sort , wtriety abandonment~ otIieo~ task , exercise , lesson , giveup , jot ) , 1 ) roblcm , tax eligibility , selection , choice , wwlty , assortment , extract , range , sample concept , item , notion , idea ground , land , soil , floor , bottom arrangement , institution , constitution , cstablishlnellt , feature , installation , construction , setup , adjustment , composition , organization amplification , extension , enhancement , expansion , dilatation , upgr~ding , add-on , increment error~ shortcoming , blemish , I ) lunder , bug , defect , demerit , failure , fault , flaw , mistake , trouble , slip , blooper , lapsus pernlission , approval , COllSellt , acceptance , al ) l ) robation , authorization hlstory~ story , tale , saga , strip company~ society , COmlmnionshil ) , party , associate border , frontier , boundary , Ihnlt , periphery , edge nlaster~ nlatter~ reasoll~ base , catlse , grOlllld~ bottolii root card , map , ticket , chart site , situation , position , bearing , layer , tier deficiency , lack , privation , want , shortage , shortcoming , absence , dearth , demerit , desideratum , insufticimlcy , paucity , scarceness alnollnt~ deal , lot , Illass I mtlltitttde , l ) lenty , qtlalltity , quiverful~ vOhlllle 1 abull ( latlce , aplellty 1 assemblage , crowd , batch , crop , heal ) , lashings , scores , set , loads , I ) ulk examinatlon , scrutiny , verification , ordeal , test , trial , inspection , tryout , assay , canvass , check , inquiry~ perusal , reconsideration , scruting difficttlty~ trollllle 1 problenl , severity , ar ( lotlSlleSS 1 heaviness page~ party~ slde , point , aspect certainty , guarantee , safety , immunity , security , collateral , doubtlessness , sureness , deposit voice~ vote , tones elate , deadline~ meethtg , appointment , time , term assoclation , contact , link~ cha\ [ ll , ColIjtlnCtlOll~ COlll/ectioll~ fllSiOll , joint , conlpOtlll ( l~ alliance , cl~tenation , tie , lllllOIl I t ) Olld~ interface , liaison , touch , relation , incorporation ban , interdiction , I ) rohibition , forbiddance eominitment : obligation , undertaking , duty , indebtedness , onus , debt , engagement , liability , bond COllfidence~ rellance , trllst~ faith , asstlrance~ dependence , private , secret election , option , choice , ballot , alternagive , poll , list path~ road , way , alley , route , lane resistance , opposition , drag character , icon , Sigll I sigllal , Syllll ) ol , lllark , tokell~ figure , olneil ahn , destination , end , designation , target , goal , object , objective , sightings , intentimb prompt coherence , context~ COlltlgtllty , connectloli agreeinent~ approvaI~ assont , accordance , approbation , consent , afIinnation , allowance , compliance , comi ) Iiancy , acclamation Figure 5 : Dictionaries extracted from online resources ambiguity random major emlfirical sense distrib</definiens>
				<definiens id="4">49.4 % 61.5 % 68.2 % Figure 6 : Disambig , mtion results for clustering versus probabilistic lexicon methods worse in terms of effectiveness ( E //corre ( -t / \ ] /-correct q # : incorrect t \ ] / : do n't know ) . The reason for this is that even if the distribution ( ff ( v , n ) pairs is estimated quite precisely for the pairs in the large training corpus , there are still many pairs which receive the same or no positive probability at all. These effects can'be overcome by a clustering approach to disambiguation ( column 5 ) . Here the class-smoothed probability of a ( v , n ) pair is used to decide between alternative target-nouns. Since the clustering model assigns a more fine-grained probability to nearly every pair in its domain , there are no do n't know cases for comparable precision values. However , the senmntically smoothed probability of the clustering models is still too coarse-grained when compared to a disambiguation with a prot ) abilistic lexicon. Here ~ fllrther gain in precision and equally effectiveness of ca. 7 % is obtained on both corpora ( column 6 ) . We conjecture that this gain ( : an be attrilmted to the combination of Dequency iilformation of the nouns and the fine-tuned distribution on the selection classes of the the nominal arguments of the verbs. We believe that including the set of translation alternatives in the ProbLex distribution is important for increasing efficiency , because it gives the dismnbiguation model the opportunity to choose among unseen alternatives. Furthermore , it seems that the higher precision of ProbLex can not be attributed to filling in zeroes in the empirical distribution. Rather , we speculate that ProbLex intelligently filters the empirical distribution by reducing maximal 653 counts for observations which do not fit into classes. This might help in cases where the empirical distribution has equal values for two alternatives. source target Seite Sicherheit Verbindung Verpflichtung Ziel overall precision page side guarantee safety commction link commitment obligation objective target Figure 7 : Precision for finding correct and acceptable translations by lexicon look-up Fig. 7 shows the results for disambiguation with probabilistic lexica for five sample words with two translations each. For this dictionary , a test corpus of 219 sentences was extracted , 200 of which were additionally labeled with acceptable translations. Precision is 78 % for finding correct translations and 90 % for finding acceptable translations. Furthermore , in a subset of 100 test items with average ambiguity 8.6 , a lmnlan judge having access only to the English verb and the set of candidates for the targel , -lloun , i.e. the information used by the model , selected anlong translations. On this set ; , human precision was 39 % . Fig. 8 shows a comparison of our approadl to state-of-the-art unsupervised algorithlns for word sense disambiguation. Column 2 shows the number of test examples used to evaluate the various approaches. The range is from ca. 100 examples to ca. 37,000 examples. Our method was evaluated on test corpora of sizes 219 , 814 , and 1,340. Column 3 gives the average number of senses/eranslations for the different disambiguation methods. Here the range of the ambiguity rate is from 2 to about 9 senses 4. Column 4 4The mnbiguity factor 2.27 attributed to Dagan and Itai 's ( 1994 ) experiment is calculated by dividing their average of 3.27 alternative translations by their average of 1.44 correct translations. Furthermore , we calculated the ambiguity factor 3.51 for Resnik 's ( 1997 ) experiment shows the rmldom baselines cited for the respective experiments , ranging t'rom ca. 11 % to 50 % . Precision values are given in column 5. In order to compare these results which were computed for different ambiguity factors , we standardized the measures to an evaluation for binary ambiguity. This is achieved by calculal ; ing pl/log2 arab for precision p and ambiguity factor arab. The consistency of this `` binarization '' can be seen by a standardization of the different random baselines which yields a value of ca. 50 % for all approaches 5. The standardized precision of our approach is ca. 79 % on all test corpora. The most direct point of comparison is the method of Dagan and Itai ( 1994 ) whirl1 gives 91.4 % precision ( 92.7 % standardized ) and 62.1 % effectiveness ( 66.8 % standardized ) on 103 test ; examples for target word selection in the transfer of Hebrew to English. However , colnpensating this high precision measure for the low effectiveness gives values comparable to our results. Dagan and Itai 's ( 1994 ) method is based on a large variety of gramnmtieal relations tbr verbal , nominal , and adjectival predicates , but no class-based infornmtion or slot-labeling is used. I { esnik ( 1997 ) presented a disambiguation method which yields 44.3 % precision ( 63.8 % standardized ) tbr a test set of 88 verb-object tokens. His approach is coral ) arable to ours in terlns of infbrmedness of the ( tisambiguator. Hc also uses a class-based selection measure , but based on WordNet classes. However , the task of his evaluation was to select WordNet-senses tbr the objects rather than the objects themselves , so the results can not be compared directly. The stone is true for the SENSEVAL evaluation exelcise ( Kilgarriff and Rosenzweig , 2000 ) -- there word senses from the HECTOl~-dictionary had to be disambiguated. The precision results for the ten unsupervised systems taking part in the comt ) etitive evaluation ranged Kern 20-65 % at efficiency values from 3-54 % . The SENSEVAL '~tan ( lard is clearly beaten by the earlier results of Yarowsky ( 1995 ) ( 96.5 % precision ) and Schiitze ( 1992 ) ( 92 % precision ) . However , a comparison to these refrom his random baseline 28.5 % by taking 100/28.5 ; reversely , Dagan and Itai 's ( 1994 ) random baseline can be calculated as 100/2.27 = 44.05. Tile ambiguity t ; '~ctor for SENSEVAL is calculated for tile llOUll task in the English SENSEVAL test set. 5Note that we are guaranteed to get exactly 50 % standardized random 1 ) aseline if random , arab = 100 % . 654 disambiguation corlms random precision method size aml ) iguity random 1 ) recision ( standardized ) ( standardized ) ) robLex 1 340 8.63 14.2 % 49.4 % 53.4 % 79.7 % 814 2.83 35.9 % 68.2 % 50.5 % 77.5 % 219 2 50.0 % 78.0 % 50.0 % 78.0 % ) agan , Itai 94 { esnik 97</definiens>
				<definiens id="5">Comparison of unsupervised lexical disambiguation methods. sults is again somewhat difficult. Firstly , these at ) proaches were ewfluated on words with two clearly ( tistmlt senses which were de/ ; el'nfined by the experimenters. In contrast , our method was evalutated on randonfly selected actual translations of a large t ) ilingual cortms. Furthermore , these apl ) roaches use large amounts of infbrmation in terms of linguistic ca.tegorizations , large context windows , or even 1111nual intervention such as initial sense seeding ( hqtrowsky , 1995 ) . Such information is easily obtainabh ; , e.g. , in I1\ ] . at ) tflications , but often burdensome to gather or sim.i ) ly uslavail~bh'~ in situations such as incremental parsing O1 ' translation. The disanfl3iguation method presented in this pa.per delibera.tely is restricted to the limited mnomlt of information provided by a probabilistic class-based lexicon. This intbrmation yet proves itself accurate enough to yield good empirical results , e.g. , in target-language disambiguation. The t ) rol ) al ) ilistic class-based lexica are induced in an unsupervised manner fl'om large mmnnotated corpora. Once the lexica are constructed , lexical mnbiguity resolution can be done by a simple lexicon look-up. I51 targetword selection , the nlOSt fl'equent target-noun whose semantics fits best to tit ( ; semantics of the argument-slot of the target-verb is chosen. We evaluated our method on randomly selected examities Dora real-world bilingual corpora which constitutes a realistic hard task. Dismnbiguation based on probabilistie lexica perfornmd satisfim- ' tory for this | ; ask. The lesson lem'ned tYom our experimental results is that hybrid models con &gt; bining fi : equency information and class-based t ) robabilities outlmrtbnn both pure fl'equencybased models and pure clustering models</definiens>
			</definition>
</paper>

		<paper id="2135">
			<definition id="0">
				<sentence>count ( X ) gives tile mmlber of X returned , occur ( X ) gives the mlmber of occurrences of X in each corpus , length ( X ) gives the dependency size of X and cofrcq ( X ) gives the number of cooccurrences in the parallel corpora .</sentence>
				<definiendum id="0">count ( X )</definiendum>
				<definiens id="0">gives tile mmlber of X returned , occur ( X ) gives the mlmber of occurrences of X in each corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>6A typical Japanese sentence follows S-O-V s~ructure : Other types of long-distance translation patterns such as `` ~d `` -case ( accusative ) and verb patterns ( be held at X/X -d ~g @ .9 ; 5 ) are not extracted even candidate patterns fi'om each corpus are generated .</sentence>
				<definiendum id="0">S-O-V s~ructure</definiendum>
				<definiens id="0">not extracted even candidate patterns fi'om each corpus are generated</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>The class NS contains both masculine and neuter nouns , and these differ ill none of their inflected forms .</sentence>
				<definiendum id="0">class NS</definiendum>
				<definiens id="0">contains both masculine and neuter nouns</definiens>
			</definition>
			<definition id="1">
				<sentence>While the nouns Band 'book volume ' ( mase , NU ) , Band 'strip ' ( neut , NR ) , Band 'bond ' ( neut , NS , archaic and rare in singular ) , Band 'music band ' ( fern , NA ) , and Bande 'gang ' ( fern , NM ) may be unlikely to occur all in the same context , they ilhlstrate the dimension of the t ) roblems of segmentation and lexical assignnlent , which in turn coil52 Figure 6 : hyl ) othescs fi ) r ,5 '' u'nd -basic_Is basic ftype ( ns ; nu , nm ; u_s ) surf \ [ \ ] sund base \ [ \ ] ayfl '' moph \ [ casc_avn\ [ case /ctxt ( -a , sc tgr Lgen 'u_q hum ( , siny ; u_ , s ) gend ( fcm ; u_~ ) ~ers third synsem ... -basic_Is basic ftype dnw ; , l_ , ~ ' ) surf \ [ \ ] su'nd base \ [ \ ] moph agr synsem ... agr case_arm case /ctxt ( : as , : Lgen ( ~om ; , u_ ( j ) num ( sing ; v_s ) gend ( `` fem ; v_s ) ~ers I , hi'rd -basic_Is -basic ftype ( `` nw ; u_s ) surf \ [ \ ] sund base \ [ \ ] moph J agr sy|\ ] Se Ill ... agr \ [ casc_ ( $ ~IIII case ctxt C ( ISC .</sentence>
				<definiendum id="0">NU</definiendum>
				<definiendum id="1">lexical assignnlent</definiendum>
				<definiens id="0">'book volume ' ( mase ,</definiens>
			</definition>
			<definition id="2">
				<sentence>Lgen ( `` gen ; 'a_g ) num ( sin q ; u_s ) gend ( ~ fl 'm ; u_s ) ~ers third stitute part of the more general 1 ) robleni of disamMguation in natural language processing .</sentence>
				<definiendum id="0">Lgen</definiendum>
				<definiendum id="1">'a_g ) num</definiendum>
				<definiens id="0">third stitute part of the more general 1 ) robleni of disamMguation in natural language processing</definiens>
			</definition>
</paper>

		<paper id="2119">
			<definition id="0">
				<sentence>The most likely analysis can be obtained by searching for the path with the minimum connective cost ( Hisamitsu and Nitta 1990 ) , often supplemented by additional heuristic devices such as the longest-string-match or the least-number-of-bunsetsu ( phrase ) .</sentence>
				<definiendum id="0">most likely analysis</definiendum>
			</definition>
</paper>

		<paper id="2122">
			<definition id="0">
				<sentence>Verbmobil ( Wahlster , 2000 ) is a speech to speech machine translation system , aimed at handling a wide range of spontaneous spee ( 'h phenomena within the restricted domain of travel t ) lanning and at ) pointment sche ( hfling dialogues .</sentence>
				<definiendum id="0">Verbmobil</definiendum>
				<definiens id="0">a speech to speech machine translation system</definiens>
			</definition>
			<definition id="1">
				<sentence>The Vcrbmobil system includes four independent translations paths that operate ill parallel .</sentence>
				<definiendum id="0">Vcrbmobil system</definiendum>
				<definiens id="0">includes four independent translations paths that operate ill parallel</definiens>
			</definition>
			<definition id="2">
				<sentence>The input shared by all paths consists of sequences of annotated Word Itypothcscs Graphs ( WHG ) , produced by the speech recognizer .</sentence>
				<definiendum id="0">input shared by all paths</definiendum>
				<definiens id="0">consists of sequences of annotated Word Itypothcscs Graphs ( WHG ) , produced by the speech recognizer</definiens>
			</definition>
			<definition id="3">
				<sentence>Here the translation invariant consists of a recognized dialogue act , together with its extracted propositional content .</sentence>
				<definiendum id="0">translation invariant</definiendum>
			</definition>
			<definition id="4">
				<sentence>The linguistic analysis part consists of several parsers which , in turn , also operate ill parallel ( Ruland et al. , 1998 ) .</sentence>
				<definiendum id="0">linguistic analysis part</definiendum>
			</definition>
			<definition id="5">
				<sentence>We define tile normalized confidence of Seq as tbllows : s~Seq Tlfis induces the following order relation : Based on this relation , we define the set B of best sequences as tbllown : B ( SEQ ) = { scq E SEQI seq is a maxinmm element in ( SEQ ; _ &lt; c ) } • ( 4 ) The selection procedure consists isl generating the various possible sequences , comlmting their respective normalized confidence values , and arbitrarily choosing a member of the set of best sequences .</sentence>
				<definiendum id="0">tile normalized confidence</definiendum>
				<definiendum id="1">seq</definiendum>
				<definiens id="0">the set B of best sequences as tbllown : B ( SEQ ) = { scq E SEQI</definiens>
				<definiens id="1">a maxinmm element in ( SEQ ; _ &lt; c ) } • ( 4 ) The selection procedure consists isl generating the various possible sequences , comlmting their respective normalized confidence values , and arbitrarily choosing a member of the set of best sequences</definiens>
			</definition>
			<definition id="6">
				<sentence>Learning the Yes ( : aling coefficients in l ) erformed off-line , and shouht normally take place only once , ulsless new training data is asseml ) led , or new criteria tbr the desirable nystelll l ) eh~tvior have been tbrmulate ( l. Tim learning &lt; ; y &lt; : le consisl , n of incorporating human feedback ( training set almotation ) and finding a set of rescaling ( : oe\ [ ticients so as to yield a selection t ) ro ( : edure with optimal or close to optimal accord with the human ewfluation .</sentence>
				<definiendum id="0">Learning the Yes</definiendum>
			</definition>
			<definition id="7">
				<sentence>s by the vector d~t ( , ~ ) , where each component denotes the conditional i ) rol ) al ) ility of a certain dialogue act , given the segment .</sentence>
				<definiendum id="0">vector d~t</definiendum>
				<definiens id="0">the conditional i ) rol ) al ) ility of a certain dialogue act , given the segment</definiens>
			</definition>
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>The IBM model is represented with the formula shown in ( 1 ) l 17t v ( f , al ) = II I-I t ( fJl % ) d ( jlaj , m , l ) i=1 j=l ( 1 ) Here , n is the fertility probability that an English word generates n h'end : words , t is tim aligmnent probability that the English word c generates the French word f , and d is the distortion probability that an English word in a certain t ) osition will generate a lh'ench word in a certain 1 ) osition .</sentence>
				<definiendum id="0">IBM model</definiendum>
				<definiendum id="1">n</definiendum>
			</definition>
			<definition id="1">
				<sentence>f ( t¢ , tk ) expresses the information for predicting that te maps into ta .</sentence>
				<definiendum id="0">f</definiendum>
				<definiens id="0">expresses the information for predicting that te maps into ta</definiens>
			</definition>
			<definition id="2">
				<sentence>Most alignment algorithm consists of two steps : ( 1 ) estimate translation probabilities .</sentence>
				<definiendum id="0">alignment algorithm</definiendum>
				<definiens id="0">consists of two steps : ( 1 ) estimate translation probabilities</definiens>
			</definition>
			<definition id="3">
				<sentence>The event consists of a tag string t , , of a English ( 2 ) ( 31 • ~lm~ '' -+ ~'~Whatever Figure 2 : Tag sequence corresl ) ondences at the phrase level 1 ) OS-tagged sentence and a tag string tL~ of the corresponding Korean POS-tagged sentence and it Call be represented with indicator functions fi ( t~ , tk ) .</sentence>
				<definiendum id="0">event</definiendum>
				<definiens id="0">consists of a tag string t , , of a English ( 2 ) ( 31 • ~lm~ '' -+ ~'~Whatever Figure 2 : Tag sequence corresl ) ondences at the phrase level 1 ) OS-tagged sentence and a tag string tL~ of the corresponding Korean POS-tagged sentence and it Call be represented with indicator functions fi ( t~ , tk )</definiens>
			</definition>
			<definition id="4">
				<sentence>fi ) IS ) Note that a model PA has a , set of t ) arameters A which means weights of teatures .</sentence>
				<definiendum id="0">fi ) IS</definiendum>
				<definiens id="0">a model PA has a , set of t ) arameters A which means weights of teatures</definiens>
			</definition>
			<definition id="5">
				<sentence>The total saml ) les consists of 3,000 aligned Sellteiice pairs of English-Korean , which were extracted from news on the web site of 'Korea Times ' and a. magazine fl ) r English learning .</sentence>
				<definiendum id="0">total saml ) les</definiendum>
			</definition>
			<definition id="6">
				<sentence>El of active features in A t~ I ) T-t-NN I ) Tq-NN DT-FNN DT+NN DT-t-NN DT-FNN I ) T-FNN etc t~ NNIN2 ANI ) E+NNIN2 ANNUWNNDE2 NNIN2+PPCA1 NNIN2+NNIN2 NNIN2-FPPAU ADCO eI ; c p ( t~l*~ ) 0.524131 Table 4 : Conditional Probability string mapping using maximum entropy modeling and feature selection concept .</sentence>
				<definiendum id="0">c p</definiendum>
				<definiens id="0">Conditional Probability string mapping using maximum entropy modeling and feature selection concept</definiens>
			</definition>
</paper>

		<paper id="2112">
			<definition id="0">
				<sentence>The dereferencing of other referring expressions like ref ( AD ( named ( D , Mary ) gcard ( D , 1 ) ) ) is similar but less constrained in that we consider entities mentioned in all nodes mentioned in the discourse , tree , whether open or closed , in order of recency .</sentence>
				<definiendum id="0">AD</definiendum>
				<definiens id="0">similar but less constrained in that we consider entities mentioned in all nodes mentioned in the discourse , tree , whether open or closed</definiens>
			</definition>
			<definition id="1">
				<sentence>vx ( ( ho , , e ( x ) v car ( X ) ) qY ( of ( Y , AZ ( door ( Z ) ) , X ) ~eard ( Y , 1 ) ) ) This means that , having used utterance ( 1 ) above to update the discourse model , we have the fbllowing amongst the facts in Discourse State 1 : Discourse State 1 seel ( # 138 ) 0 ( # 138 , agent , # 94 ) 0 ( # 138 , object , # 139 ) card ( # 139 , 1 ) house ( # 139 ) ends_be for'e ( # 4 ( 1 ) , # sat ) door ( # 46 ( # 139 ) ) entrance ( # 46 ( # 139 ) ) of ( # 46 ( # 139 ) , ~d ( door ' ( A ) ) , # 139 ) card ( # 46 ( # 139 ) , 1 ) aspect ( simple , # 137 , # 138 ) In updating utterance ( 2 ) , the bridging description which needs to be dereDrenced has the tbllowing representation : ref ( AE ( door ( E ) g~ card ( E , 1 ) ) ) Since we caimot guarantee that there will only be a single entity in our model satisfying the t ) roperties kE ( door ( E ) &amp; card ( E , 1 ) ) , we want to ensure that the referent we obtain is either the most recently mentioned or that with the most recently mentioned antecedent , i.e. , in this case , the house # 139 .</sentence>
				<definiendum id="0">vx</definiendum>
				<definiendum id="1">AZ ( door</definiendum>
				<definiens id="0">the bridging description which needs to be dereDrenced has the tbllowing representation</definiens>
			</definition>
</paper>

		<paper id="2155">
			<definition id="0">
				<sentence>Approximating an HPSG through a CFG ~ is interesting for the following practical reason : assuming that we have a CFG that comes close to an HPSG , we can use the CFG as a cheap filter ( running time complexity is O ( IGI 2 x n 3 ) for an arbitrary sentence of length n ) .</sentence>
				<definiendum id="0">Approximating an HPSG</definiendum>
			</definition>
			<definition id="1">
				<sentence>In tile next section , we describe the Japanese HPSG that is used in Verbmobil , a project that deals with the translation of spontaneously spoken dialogues between English , German , and Japanese speakers .</sentence>
				<definiendum id="0">Japanese HPSG</definiendum>
				<definiens id="0">a project that deals with the translation of spontaneously spoken dialogues between English , German , and Japanese speakers</definiens>
			</definition>
</paper>

		<paper id="2153">
			<definition id="0">
				<sentence>Language/linguistic knowledge : Each language team consists of linguists and computational linguists who grew tip and were educated in the native language community .</sentence>
				<definiendum id="0">Language/linguistic knowledge</definiendum>
				<definiens id="0">Each language team consists of linguists and computational linguists who grew tip and were educated in the native language community</definiens>
			</definition>
			<definition id="1">
				<sentence>The most decisive factors stem fi'om our customer research , which informs us about what users view as their biggest grammar challenges , and the corpus analysis , which inlkmns us about what errors users actually make .</sentence>
				<definiendum id="0">corpus analysis</definiendum>
				<definiens id="0">informs us about what users view as their biggest grammar challenges</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>Re ( : ent advances in ( : onltmter har ( lware and availal ) ility of very large corpora have made ( ; t1 ( `al ) plication of s ( ; atistical techniques to natural language processing a t ) asible and a very at ) pealing resem'ch area .</sentence>
				<definiendum id="0">Re</definiendum>
				<definiens id="0">`al ) plication of s ( ; atistical techniques to natural language processing a t ) asible and a very at ) pealing resem'ch area</definiens>
			</definition>
			<definition id="1">
				<sentence>Morphologi ( : al disanlbiguation in inflecting or agglutinative languages with COlnl ) lex morphology involves more than determining the major or minor Imrts-of-sl ) cech of the lexiea .</sentence>
				<definiendum id="0">Morphologi</definiendum>
				<definiendum id="1">lex morphology</definiendum>
				<definiens id="0">involves more than determining the major or minor Imrts-of-sl ) cech of the lexiea</definiens>
			</definition>
			<definition id="2">
				<sentence>where IGi denotes relevant inflectional feal ; urcs of the inflectional groul ) s , including the 1 ) art-ofsl ) eech for the root or any of the derived forms .</sentence>
				<definiendum id="0">IGi</definiendum>
			</definition>
			<definition id="3">
				<sentence>The training data consists of the unambiguous sequences ( US ) consisting of about 650K tokens in a corpus of i million tokens , and two sets of manually dismnbiguated corpora of 12,000 and 20,000 tokens .</sentence>
				<definiendum id="0">training data</definiendum>
				<definiens id="0">consists of the unambiguous sequences ( US ) consisting of about 650K tokens in a corpus of i million tokens , and two sets of manually dismnbiguated corpora of 12,000 and 20,000 tokens</definiens>
			</definition>
			<definition id="4">
				<sentence>g ( '~s and ( : an cerl ; aJnly l ) ( ' .</sentence>
				<definiendum id="0">g</definiendum>
				<definiens id="0">an cerl</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>The , original algorithm k ( 'ei ) s on ( ; edge in th ( : ( : hart fi ) r each ( : oml ) ination of span ( start and cn ( l position ) ~md non-tcrmimd symbol ( for inactive edges ) or right-hand side l ) r ( : fixcs of ( lot ; te ( t rules ( for active edges ) .</sentence>
				<definiendum id="0">te ( t rules</definiendum>
			</definition>
			<definition id="1">
				<sentence>flI is the inside probability for inactive edges as given in eqnation 2 , P ( X ) is the a priori probability tbr non-terminal X , ( as estimated from the frequency in the training COrlmS ) and Pm is the probability of the edge tbr the non-terminal X spanning positions i to j that is used tbr ranking .</sentence>
				<definiendum id="0">flI</definiendum>
				<definiendum id="1">P ( X )</definiendum>
				<definiendum id="2">Pm</definiendum>
				<definiens id="0">the inside probability for inactive edges as given in eqnation 2 ,</definiens>
				<definiens id="1">estimated from the frequency in the training COrlmS</definiens>
				<definiens id="2">the probability of the edge tbr the non-terminal X spanning positions i to j that is used tbr ranking</definiens>
			</definition>
			<definition id="2">
				<sentence>yk+l ym , y ) k • `` ~ , ,~ ) ) `` ' '' ~A ( il , jl ( the ( tot is aI '' ter the kth symbol of llSe : the right-hand side ) we ( 7 ) = P ( YI ... Yk ) .</sentence>
				<definiendum id="0">jl</definiendum>
				<definiens id="0">the right-hand side</definiens>
			</definition>
			<definition id="3">
				<sentence>It is the a priori probability that the right-hand side of a production has the prefix y1 ... y/c , which is estilnated by f ( yl ... yt~ is prefix ) 00 ) N where N is the total number of productions in the corpus 2 , i = ij , j = j/~ and flA is the inside probability of the pretix .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">flA</definiendum>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>Precisiolz is the number of correct alignments ( again established by independent means ) divided by the number o1 ' units aligned by the alignment program ( i.e. , the numerator in the recall calculation ) .</sentence>
				<definiendum id="0">Precisiolz</definiendum>
				<definiens id="0">the number of correct alignments ( again established by independent means ) divided by the number o1 ' units aligned by the alignment program ( i.e. , the numerator in the recall calculation )</definiens>
			</definition>
			<definition id="1">
				<sentence>3 combining knowledge sources The project in which the research reported here has been carried out , the ETAP project ( see section 8 , below ) , is a parallel translation corpus project , the aim of which is to create an annotated -- understood as part-of-speech ( POS ) tagged and aligned -- multilingual translation corpus , which will be used as the basis for the development of methods and tools for the automatic extraction of translation equivalents .</sentence>
				<definiendum id="0">ETAP project</definiendum>
				<definiens id="0">a parallel translation corpus project , the aim of which is to create an annotated -- understood as part-of-speech ( POS ) tagged and aligned -- multilingual translation corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>5 Hypothesis ( 1 ) seems plausible , since the word alignment system used ( Tiedemann ( to appear a ) , Tiedenlann ( to appear b ) ) actually aheady utilizes several kinds of information to align the words in the two texts .</sentence>
				<definiendum id="0">Tiedemann</definiendum>
				<definiendum id="1">Tiedenlann (</definiendum>
				<definiens id="0">to appear b ) ) actually aheady utilizes several kinds of information to align the words in the two texts</definiens>
			</definition>
			<definition id="3">
				<sentence>The IVTI corpus has roughly 100,000 words of text in each language ; Merkel et al. ( to appear ) ) was used to produce evaluation standards ( `` gold standards '' ) for the following alignment directions : SE-+PL , SE-+SBC , PL -- +SBC , SBC -- - , 'PL in one group , and SE-+EN , SE-+ES , EN -- +ES , ES-+EN in the other .</sentence>
				<definiendum id="0">IVTI corpus</definiendum>
				<definiendum id="1">EN -- +ES</definiendum>
				<definiens id="0">has roughly 100,000 words of text in each language ; Merkel et al. ( to appear</definiens>
			</definition>
</paper>

		<paper id="2097">
			<definition id="0">
				<sentence>CommandTalk : A Spoken-Language Interface tbr Battlefield Simulations .</sentence>
				<definiendum id="0">CommandTalk</definiendum>
				<definiens id="0">A Spoken-Language Interface tbr Battlefield Simulations</definiens>
			</definition>
</paper>

		<paper id="2125">
			<definition id="0">
				<sentence>( Hindle 1983 ) Gernmn Speech Repairs German , an Indo-Europcan language , is a language with a strong emphasis on grammatical flexion .</sentence>
				<definiendum id="0">Indo-Europcan language</definiendum>
				<definiens id="0">a language with a strong emphasis on grammatical flexion</definiens>
			</definition>
			<definition id="1">
				<sentence>The following utterance in which a German repair is produced clearly illustrates this t ) henolneuon : `` lch habe eiuen Wiirfel rail einer mit emem Gewmde 4 '' , where mit einer is a phrasal liagmcnt and mit einem Gewinde , starting fi'oln the phrasal beginning , is a complete phrase repairing the previous phrasal fiagment .</sentence>
				<definiendum id="0">mit einer</definiendum>
				<definiens id="0">a complete phrase repairing the previous phrasal fiagment</definiens>
			</definition>
			<definition id="2">
				<sentence>The BAUFIX corpus ( Sagerer el al. 1994 ) consists of 22 digitally recorded German human-human dialogues .</sentence>
				<definiendum id="0">BAUFIX corpus</definiendum>
				<definiens id="0">consists of 22 digitally recorded German human-human dialogues</definiens>
			</definition>
			<definition id="3">
				<sentence>These NP-rcpairs have the following structures : NP = &gt; N NP = &gt; DET NP = &gt; DET + N NP = &gt; ADI + N NP = &gt; QUAN + CLASS NP = &gt; OUAN + CLASS + N where QUAN denotes numbers and CLASS means classifiers in Chinese .</sentence>
				<definiendum id="0">QUAN</definiendum>
				<definiendum id="1">CLASS</definiendum>
				<definiens id="0">NP = &gt; N NP = &gt; DET NP = &gt; DET + N NP = &gt; ADI + N NP = &gt; QUAN + CLASS NP = &gt; OUAN + CLASS + N where</definiens>
			</definition>
			<definition id="4">
				<sentence>Finite state automata similar to M with e-transitions denoted by a quintuple &lt; Q , E , 8 , q0 , IF &gt; defined as follows can model more than 80 % of overall German NP-repairs : Q = { q0 , ql , q2 , q3 , qf } , E = { det , adj , 11 , dct-d G , adj-d , n-d , e } , q0 is the initial state , F = { q3 } and ~5 ( q0 , det ) =ql , 8 ( q l , adj ) =q2 , 6 ( @ , n ) =q3 , 8 ( q0 , det-d ) -qf , 6 ( ql , adj-d ) =qf , 8 ( q2 , n-d ) =qf , 6 ( qf , e ) =q0 , 8 ( ql , e ) =q0 , 6 ( @ , e ) =q ( ) , 8 ( @ , e ) =q0 M is graphically illustrated in Figure 2 .</sentence>
				<definiendum id="0">q0</definiendum>
				<definiens id="0">a quintuple &lt; Q , E , 8 , q0 , IF &gt; defined as follows can model more than 80 % of overall German NP-repairs : Q = { q0 , ql , q2 , q3 , qf } , E = { det , adj , 11 , dct-d G , adj-d , n-d , e }</definiens>
				<definiens id="1">the initial state</definiens>
				<definiens id="2">@ , n ) =q3 , 8 ( q0 , det-d ) -qf , 6 ( ql , adj-d ) =qf , 8 ( q2 , n-d ) =qf , 6 ( qf , e ) =q0 , 8 ( ql , e ) =q0 , 6 ( @ , e ) =q ( ) , 8 ( @ , e ) =q0 M is graphically illustrated in Figure 2</definiens>
			</definition>
			<definition id="5">
				<sentence>vtic l ) i , vfhten~3 , Modeling : A Comparative AnaO~si , v o J '' , S~ , edish and American Engli , s'h thtman-Human and Hltnlan-Machme Dialogs .</sentence>
				<definiendum id="0">Modeling</definiendum>
				<definiens id="0">A Comparative AnaO~si , v o J ''</definiens>
			</definition>
</paper>

		<paper id="2114">
			<definition id="0">
				<sentence>The probability of a sequence of words is : P ( w 1 `` '' w , , ) = P ( w\ [ ' ) = -- P ( wl ) X P ( w2 I wl ) x '' '' P ( w '' \ [ w~'-l ) = ( 1 ) i=I where w\ [ ' = { wl , w2 , w3 , ... , w , , } is a sentence or sequence of words .</sentence>
				<definiendum id="0">}</definiendum>
				<definiens id="0">I wl ) x '' '' P ( w '' \ [ w~'-l ) = ( 1 ) i=I where w\ [ ' = { wl , w2 , w3 , ... , w ,</definiens>
				<definiens id="1">a sentence or sequence of words</definiens>
			</definition>
			<definition id="1">
				<sentence>The individual conditional probabilities are approximated by the maximum likelihoods : PML ( W~ Iwl-b -freq ( w\ [ ) _ f , 'eq ( w , ... w~_ , wp freq ( wl q ) freq ( wl `` '' ~h-l ) ( 2 ) where freq ( X ) is the frequency of the phrase X in the text .</sentence>
				<definiendum id="0">PML</definiendum>
				<definiendum id="1">freq ( X )</definiendum>
				<definiens id="0">the frequency of the phrase X in the text</definiens>
			</definition>
			<definition id="2">
				<sentence>We use the weighted average n-gram technique ( WA ) , which combines n-grain ~ phrase distributions of several orders using a series of weighting fnnctions .</sentence>
				<definiendum id="0">WA</definiendum>
				<definiens id="0">combines n-grain ~ phrase distributions of several orders using a series of weighting fnnctions</definiens>
			</definition>
			<definition id="3">
				<sentence>eq ( w ) is the frequency of the word w in the text .</sentence>
				<definiendum id="0">eq ( w )</definiendum>
			</definition>
			<definition id="4">
				<sentence>This language model ( defined by equation ( 3 ) and ( 5 ) ) is what we term a standard n-gram language model or global language model .</sentence>
				<definiendum id="0">language model</definiendum>
				<definiens id="0">a standard n-gram language model or global language model</definiens>
			</definition>
			<definition id="5">
				<sentence>The word language-training corpus to be used is tlle amalgamation of the text fiagments taken from the global training corpus in which the significant word appears .</sentence>
				<definiendum id="0">word language-training corpus</definiendum>
				<definiens id="0">tlle amalgamation of the text fiagments taken from the global training corpus in which the significant word appears</definiens>
			</definition>
			<definition id="6">
				<sentence>If the same weight is given to all the word language models but not to the global language model and if a restriction on tim lmmber of word language models to be included is enforced , the weighted model is defined as : and ~ is a parameter which is chosen to optimise the model .</sentence>
				<definiendum id="0">~</definiendum>
				<definiens id="0">a parameter which is chosen to optimise the model</definiens>
			</definition>
			<definition id="7">
				<sentence>exp ( 21d ) ) ( 9 ) l + exp ( -3/d ) + exp ( -2/d ) where Patot , ,t ( wl w ( ' ) is the conditional probability of the word w following a phrase wl `` '' w , , in the global language model .</sentence>
				<definiendum id="0">exp</definiendum>
				<definiendum id="1">Patot</definiendum>
				<definiens id="0">the conditional probability of the word w following a phrase wl `` '' w</definiens>
			</definition>
			<definition id="8">
				<sentence>Pmppy ( Wl w~ ' ) is the conditional probability of the word w following a phrase w 1 ... % word language model for the significant word Happy .</sentence>
				<definiendum id="0">Pmppy</definiendum>
				<definiens id="0">the conditional probability of the word w following a phrase w 1 ... % word language model for the significant word Happy</definiens>
			</definition>
			<definition id="9">
				<sentence>( w\ [ w ( ' ) is the conditional probability for the significant word w i and ~ is a normalizing constant .</sentence>
				<definiendum id="0">~</definiendum>
				<definiens id="0">the conditional probability for the significant word w i and</definiens>
			</definition>
			<definition id="10">
				<sentence>We select one quarter of the articles in the global training corpus as our training corpus ( since the global training corpus is large and the normalisation process takes time ) .</sentence>
				<definiendum id="0">normalisation process</definiendum>
				<definiens id="0">takes time )</definiens>
			</definition>
</paper>

		<paper id="1067">
			<definition id="0">
				<sentence>More specifically , we introduce DPL structuT '' cs~ extended trce structures that encode formulas of dynamic predicate logic ( DPL ) in much the same way as Egg et al. 's ( 1998 ) lambda structnres encode A-terms .</sentence>
				<definiendum id="0">DPL structuT</definiendum>
			</definition>
			<definition id="1">
				<sentence>In this section , we define the Constraint Language tbr DPL structures , CL ( DPL ) , a language of tree descriptions which conserw~tively extends donfinance constraints ( Marcus et al. , 1983 ; Rainbow et al. , 1995 ; Keller et al. , 2000 ) by variable binding constraints .</sentence>
				<definiendum id="0">CL ( DPL</definiendum>
				<definiens id="0">a language of tree descriptions which conserw~tively extends donfinance constraints ( Marcus et al. , 1983 ; Rainbow et al. , 1995 ; Keller et al. , 2000 ) by variable binding constraints</definiens>
			</definition>
			<definition id="2">
				<sentence>CL ( DPL ) is a close relative of the Constraint Language for 460 Lamb ( la Structures ( CLLS ) , 1 ) resented in ( Egg et al. , 1998 ) .</sentence>
				<definiendum id="0">CL</definiendum>
				<definiendum id="1">Language</definiendum>
				<definiendum id="2">la Structures ( CLLS</definiendum>
				<definiens id="0">a close relative of the Constraint</definiens>
			</definition>
			<definition id="3">
				<sentence>A wor ( t 7t is a prefiz of 7c ' ( written rc _ &lt; re ' ) if there is a word 7r u such that 7rTr tt = 7r t. A node of a tree is the word rr E N* which addresses the node .</sentence>
				<definiendum id="0">wor</definiendum>
				<definiens id="0">A node of a tree is the word rr E N* which addresses the node</definiens>
			</definition>
			<definition id="4">
				<sentence>The nodes of the graph stand for variables in tile constraint ; labels and solid edges represent labeling COl &gt; straints , dotted edges , donlinance constraints , and dashed arrows , binding constraints , hi addition , the constraint graph represents an inequality constraint X-~=Y between each two variables whose nodes carry a label .</sentence>
				<definiendum id="0">constraint graph</definiendum>
				<definiens id="0">an inequality constraint X-~=Y between each two variables whose nodes carry a label</definiens>
			</definition>
			<definition id="5">
				<sentence>Th ( ' goal is to narrow an mMerst ) ecified ( les ( ; rit ) tion such that improi ) er DPL-structure , s are removed flom the solution set .</sentence>
				<definiendum id="0">goal</definiendum>
				<definiens id="0">to narrow an mMerst ) ecified ( les ( ; rit ) tion such that improi ) er DPL-structure</definiens>
			</definition>
			<definition id="6">
				<sentence>Let us consider | ; 15 ( ; running example in Figure 3 to see how this works ; we show how to derive Y3 &lt; I*X , which specifies the relative quantifier scope. First of all , we need to make the information 463 ( Dynl ) ( Dyn2 ) ( Dyn3 ) ( Dyn4 ) a ( x ) =Y A Y : V ( z ' ) Y &lt; *X Z ( X ) =Y A Z : f ( Zl , Z2 ) A ZI &lt; 1*Y A Z2 &lt; \ ] *X A W : g ( W1 , ... , Wn ) ~dyn ~st~tt ( fe or , , e ~ ( X ) =Y A Z : f ( Z1 , ... , Zn ) A Zi &lt; :1*X A Zj &lt; : \ ] *Y -- + false A ( X ) =Y A Z : f ( Z1 , ... , Zn ) A Zi &lt; l*X A &amp; &lt; 1*Y -~ false Figure 4 : Properness axioms. ( Trans ) ( Lab.Dom ) ( NegDisj ) ( Lab.Disj ) ( Inter ) ( Inv ) ( Child.down ) ( NegDom ) ( NonI1 ) ( NonI2 ) X &lt; a* Y A Y &lt; q* Z -+ X &lt; 1* Z x : f ( ... , z , ... ) x &lt; +y X &lt; 1* Z A Y &lt; F Z -- + X~ -L Y , xs , ... ) XR1Y A XR2Y -+ XRY XRY -+ YR-1X -+ -~ ( Z~ &lt; *W &lt; a*Y ) v~dY'1 i ¢ j ) ( .fl , , ~ Econ , - , co , , , ( .fl , , e E , i &lt; j ) Xi ± Xj where i &lt; j if RINR2 C R X &lt; : \ ] +Y A X : f ( Xl , ... , Xn ) A X-~ ± Y A X J_Z -+ Z-~ &lt; a*Y -~ ( X &lt; *Y &lt; 1*Z ) A X &lt; *Y -- + Y~ &lt; *Z ~ ( X &lt; \ ] *Y &lt; I*Z ) A Y &lt; *Z -- , '' X-~ &lt; 1*Y -- - &gt; X i &lt; F Y Figure 5 : Propagation rules for dominance and non-intervention constr~dnts. Z2 &lt; 1*Za explicit by application of ( Lab.Dom ) and ( Inter ) . In this instance , ( Inter ) is used as a rule of weakening. ( Lab.Dom ) Zg : A ( Za , Z4 ) -~ Z , ~ &lt; 1+ Za ( Inter ) Z.~ &lt; Y'Za -- + Z2 &lt; F Z3 Now we can apply the rule ( Dyn2 ) to the variable binding constraint A ( Za ) = Y ( drawn in boldface in the graph ) and the V labeling constraint to derive a non-intervention constraint. ( Dyn2 ) Z0 : A ( Z : j , Z2 ) A ZI &lt; 1*X1 A X : V_ ( X1 ) A Z2 &lt; q*Za A A ( Z3 ) = 1/ All that is left to do is to make the positive dominance intbrmation contained in the new non-intervention constraint explicit. As the constraint also contains Zo &lt; 1*X , we can apply ( NonI1 ) on the new non-intervention constraint and derive X~ &lt; FY. ( NonIl ) = ( Zo &lt; 1*X &lt; 1*Y ) A Zo &lt; 1*X -- + X~ &lt; 1*Y On the other hand , we can derive nondisjointness of X and Y because ( Trans ) , ( Lab.Dom ) , and ( Inter ) allow the deriw~tion of X &lt; FW and Y &lt; 1*W : ( NegDisj ) X &lt; *W A Y &lt; I*W ~ X= £ Y We can now combiue all of our constraints tbr X and Y with the intersection rule and obtain Y &lt; 1*X , which basically determines the order of the two quantifiers : ( Inter ) X~ &lt; *Y A X-~ £ Y ~ Y &lt; *X By exploiting the fact that the constraint is overlap-ti'ee ( i.e. contains an inequality coststraint for each two labeled variables ) , we ( : an even derive Y3 &lt; I*X by repeated application of the rules ( Child.down ) , ( Lab.Disj ) , ( NegDisj ) , and ( NegDom ) . This means that we have flflly disambiguated the scope ambiguity by saturntion with deterministic inference rules. Now let us consider a more complicated example. Fig. 6 is the underspecified description of the semantics of ( 2 ) Every visitor of a company saw one of its departments. The constraint graph has five solntions , three of which are proper. Unfortunately , the constraint language is not expressive enough to describe these three solutions ill a single constraint : Both X and Z can be either above or below Y , even in a proper solution , but if X is below Y , Z must be too , and ifX is above Y , Z must be anywhere below X ( but ; may be above 464 • IX V , , Y `` '' '' -q , , , Z company . '' `` x `` . . '' i `` . , '' `` var `` researcher i `` . / of~ -- -- ~ depar.tmen-gh'~ ; , `` ~ '' . var \~ ! .. '' ' Var ~ var'~ . '' '' '' var ~ '' . '' • i .. '' `` `` , `` . L '' i . `` / var'~ vat ~ ... ... . Figure 6 : Constraint graph for ( 2 ) . YI ) . In other words , this constraint is an exampie where the inference procedure is not strong enough to narrow the description. In this case , we must still resort to pertbrming nondeterministic case distinctions ; at worst , the rules will apply on solved forms of CL ( 1 ) PL ) constraints. constraints over these set variables ; examples for set constraints are V C V ' and V = V~ U V.2. The new non-intervention constraint ~ ( X &lt; 1*Y &lt; 1*Z ) can be encoded as Y e &lt; + ( x ) u _L ( Z ) u &gt; + ( Z ) .</sentence>
				<definiendum id="0">Dynl ) ( Dyn2 ) ( Dyn3 )</definiendum>
				<definiendum id="1">Z : f ( Z1 , ... , Zn</definiendum>
				<definiendum id="2">Zi &lt; :1*X A Zj</definiendum>
				<definiendum id="3">NegDisj</definiendum>
				<definiendum id="4">ifX</definiendum>
				<definiens id="0">false A ( X ) =Y A Z : f ( Z1 , ... , Zn ) A Zi &lt; l*X A &amp; &lt; 1*Y -~ false Figure 4 : Properness axioms. ( Trans ) ( Lab.Dom ) ( NegDisj ) ( Lab.Disj ) ( Inter ) ( Inv ) ( Child.down ) ( NegDom ) ( NonI1 ) ( NonI2 ) X &lt; a* Y A Y &lt; q* Z -+ X &lt; 1* Z x : f ( ... , z</definiens>
			</definition>
			<definition id="7">
				<sentence>Constraint programlning is a technology for solving combinatoric puzzles eificiently .</sentence>
				<definiendum id="0">Constraint programlning</definiendum>
				<definiens id="0">a technology for solving combinatoric puzzles eificiently</definiens>
			</definition>
</paper>

		<paper id="2166">
			<definition id="0">
				<sentence>SC records the feal ; llres in the VIT while it uses l ; he lemma as a key to the semantic lexicon .</sentence>
				<definiendum id="0">SC</definiendum>
				<definiens id="0">records the feal ; llres in the VIT while it uses l ; he lemma as a key to the semantic lexicon</definiens>
			</definition>
			<definition id="1">
				<sentence>The set of CHs emlfloyed is ; inspired by HPSG ( Pollard and Sag , 1994 : ) : Head , Complement , Adjunct , Om~ : j'anct , SpcciZ -- _Z Z-_Z Z -- _Z S -- Z -- _7_Z -- -- _Z -- -- _Y _ _ _/ff'\ [ houk , ( o , ) 7 `` i !</sentence>
				<definiendum id="0">HPSG</definiendum>
				<definiens id="0">j'anct , SpcciZ -- _Z Z-_Z Z -- _Z S -- Z -- _7_Z -- -- _Z -- --</definiens>
			</definition>
</paper>

		<paper id="2164">
			<definition id="0">
				<sentence>CRs contain information about objects , their properties attd about events tile agent is involved in .</sentence>
				<definiendum id="0">CRs</definiendum>
				<definiens id="0">contain information about objects , their properties attd about events tile agent is involved in</definiens>
			</definition>
			<definition id="1">
				<sentence>The basic ISM SEE and MOVE serve as SC for the COlnplex ISM TUI { N-TO which identifies the turning of the robot towards an object ; .</sentence>
				<definiendum id="0">MOVE</definiendum>
				<definiens id="0">identifies the turning of the robot towards an object ;</definiens>
			</definition>
			<definition id="2">
				<sentence>The blackl ) oard is the central storing device of tile system .</sentence>
				<definiendum id="0">oard</definiendum>
			</definition>
			<definition id="3">
				<sentence>Currently , the coneeptuMization is a pure t ) ol ; l ; omup lneclmnism .</sentence>
				<definiendum id="0">coneeptuMization</definiendum>
				<definiens id="0">a pure t</definiens>
			</definition>
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>Maximum complex units fin ' Japanese are defined by the following basic pattern , ^C_Adj * ( C_Affix l C_tdv l C_Adj \ [ N ) + where ^C means that the unit should begin with either Chinese character or Katakana .</sentence>
				<definiendum id="0">^C_Adj *</definiendum>
				<definiens id="0">the following basic pattern ,</definiens>
			</definition>
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>Pen input consists of gestures and drawings which are made in electronic ink on the computer display and processed by a gesture recognizer .</sentence>
				<definiendum id="0">Pen input</definiendum>
				<definiens id="0">consists of gestures and drawings which are made in electronic ink on the computer display and processed by a gesture recognizer</definiens>
			</definition>
			<definition id="1">
				<sentence>MP uses a multidimensional chart parser to combine the interpretations of speech and gesture in accordance with a nmltimodal unil'ication-based grammar , determines the range of possible lnultimodal interpretations , selects the one with the highest joint probability , and passes it on for execution .</sentence>
				<definiendum id="0">MP</definiendum>
				<definiens id="0">uses a multidimensional chart parser to combine the interpretations of speech</definiens>
			</definition>
			<definition id="2">
				<sentence>MP uses general combinatory schelnata for nmltimodal subcategorization ( Jolmston 1998a , p. 628 ) to combine the gestures with the speech , saturate the nmltilnodal subcategorization list , and yield an executable command .</sentence>
				<definiendum id="0">MP</definiendum>
				<definiens id="0">uses general combinatory schelnata for nmltimodal subcategorization ( Jolmston 1998a , p. 628 ) to combine the gestures with the speech , saturate the nmltilnodal subcategorization list , and yield an executable command</definiens>
			</definition>
			<definition id="3">
				<sentence>and multimodal parsing There are a nmnber of different ways in which spoken language parsing ( SLP ) and multimodal parsing ( MP ) can be imerleaved : ( 1 ) SLP populates a chart with fragments , these are passed to MP which determines possible combinations with gesture , the resulting combinations are passed back to SLP which continues until a parse of the string is found , ( 2 ) SLP parses the incoming string into a series of fragments , these become edges in MP and are combined with gestures , MP is augmented with rules from SLP which operate in MP in order to complete the analysis of the phrase , ( 3 ) SLP and MP are merged and there is one single gralnmar covering both spoken language and multimodal parsing ( cf. Johnston and Bangalore 2000 ) .</sentence>
				<definiendum id="0">MP</definiendum>
				<definiendum id="1">SLP</definiendum>
				<definiendum id="2">MP</definiendum>
				<definiens id="0">populates a chart with fragments , these are passed to MP which determines possible combinations with gesture , the resulting combinations are passed back to SLP which continues until a parse of the string is found</definiens>
				<definiens id="1">augmented with rules from SLP which operate in MP in order to complete the analysis of the phrase , ( 3 ) SLP and MP are merged and there is one single gralnmar covering both spoken language and multimodal parsing</definiens>
			</definition>
			<definition id="4">
				<sentence>MP uses a general subcategorization schema to combine 'this ' with an appropriate gesture , yielding the representation in Figure 6 ( b ) .</sentence>
				<definiendum id="0">MP</definiendum>
				<definiens id="0">uses a general subcategorization schema to combine 'this ' with an appropriate gesture</definiens>
			</definition>
			<definition id="5">
				<sentence>VPQ : A spoken language interface to large scale directory information .</sentence>
				<definiendum id="0">VPQ</definiendum>
				<definiens id="0">A spoken language interface to large scale directory information</definiens>
			</definition>
</paper>

		<paper id="1083">
			<definition id="0">
				<sentence>The DM , working as a dialogue management mechanism , keeI ) s track of the dialogue ( : ontext including the user : s view and decides the , next goal ( or a ( : tion ) of the system .</sentence>
				<definiendum id="0">DM ,</definiendum>
				<definiens id="0">working as a dialogue management mechanism</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , if th ( : user requests some instruction , the DI'I { decides the sequence of steps that realizes the l ) rocedure 1 ) y refi~rring to domain knowh~dge .</sentence>
				<definiendum id="0">DI'I {</definiendum>
				<definiens id="0">decides the sequence of steps that realizes the l ) rocedure 1 ) y refi~rring to domain knowh~dge</definiens>
			</definition>
			<definition id="2">
				<sentence>Theretbre , the CP provides the scenario tbr the instruction 1 ) ased on the control 1 ) rovided by the DM .</sentence>
				<definiendum id="0">Theretbre</definiendum>
				<definiens id="0">the scenario tbr the instruction 1 ) ased on the control 1 ) rovided by the DM</definiens>
			</definition>
			<definition id="3">
				<sentence>The Sentence Plalmer generates surface , linguisti ( : expressions coordinated with action ( Kato et al. , 1996 ) .</sentence>
				<definiendum id="0">Sentence Plalmer</definiendum>
			</definition>
			<definition id="4">
				<sentence>Theretbre , the CP provides the seenm : io tbr the instruction dialogue to the DM and enables MID-3D to output coherent instructions .</sentence>
				<definiendum id="0">Theretbre</definiendum>
				<definiens id="0">the seenm : io tbr the instruction dialogue to the DM and enables MID-3D to output coherent instructions</definiens>
			</definition>
			<definition id="5">
				<sentence>Each operator consists of the name of the operator ( Header ) , the etfcct resulting from plan execution ( Effect ) , the constraints for executing the plan ( Constraints ) , the essential subgoals ( Main-acts ) , and the optional subgoals ( Subsidiary-acts ) .</sentence>
				<definiendum id="0">operator</definiendum>
				<definiens id="0">consists of the name of the operator ( Header ) , the etfcct resulting from plan execution ( Effect ) , the constraints for executing the plan ( Constraints ) , the essential subgoals ( Main-acts ) , and the optional subgoals ( Subsidiary-acts )</definiens>
			</definition>
			<definition id="6">
				<sentence>The discourse model consists of the discourse goal agenda ( agenda ) , focus stack , and dialogue history .</sentence>
				<definiendum id="0">discourse model</definiendum>
				<definiens id="0">consists of the discourse goal agenda ( agenda ) , focus stack , and dialogue history</definiens>
			</definition>
			<definition id="7">
				<sentence>The focus stack is a sta &amp; of discourse segment frames ( DSF ) .</sentence>
				<definiendum id="0">focus stack</definiendum>
				<definiens id="0">a sta &amp; of discourse segment frames</definiens>
			</definition>
			<definition id="8">
				<sentence>Each DSF is a frmne structure that stores the tbllowing inlbrmation as slot vMues : utterance content ( UC ) : A list of utterance contents constructing a discourse segment .</sentence>
				<definiendum id="0">DSF</definiendum>
			</definition>
			<definition id="9">
				<sentence>9oal state ( GS ) : A state ( or states ) whi ( 'h shouhl 1 ) e accomplished to achieve the discourse lmrpose of the segment .</sentence>
				<definiendum id="0">GS )</definiendum>
				<definiens id="0">'h shouhl 1 ) e accomplished to achieve the discourse lmrpose of the segment</definiens>
			</definition>
			<definition id="10">
				<sentence>if the current discourse purpose is accomplished , the top level DSF is popped and added to the dialogue history , q_/he 576 l I ) SFI21 DSFI2 DSFI Jf J J UV : ( ( 18 , -20 , -263 ) ( 0 , 0.3 I , 0 ) ) UC : ( ( IJseJ~act ( Ask where heal_r ) ) I ) P : ( Response-to-user-act ( Uscr-act ( ask where bootr ) ) ) GS : ( ( Know 11 ( About ( l'lace_of boot_r ) ) ) ... ) UV : ( ( -38 , -22 , -259 ) ( 0 , -0.33 , 0 ) ) UC : ( ( System-act ( lnl ' ( ~rm S 11 ( Show S ( Action rcmovc-tiemd end .</sentence>
				<definiendum id="0">Response-to-user-act ( Uscr-act</definiendum>
				<definiendum id="1">Show S</definiendum>
				<definiens id="0">ask where bootr ) ) ) GS : ( ( Know 11 ( About ( l'lace_of boot_r ) ) ) ...</definiens>
			</definition>
			<definition id="11">
				<sentence>If the current discourse purpose is contimmd , the DM updates the information of the top level DSF .</sentence>
				<definiendum id="0">DM</definiendum>
				<definiens id="0">updates the information of the top level DSF</definiens>
			</definition>
			<definition id="12">
				<sentence>DSF12 represents a discourse segment that describes how to remove the left tie rod end .</sentence>
				<definiendum id="0">DSF12</definiendum>
				<definiens id="0">a discourse segment that describes how to remove the left tie rod end</definiens>
			</definition>
			<definition id="13">
				<sentence>DSF121 represents the user-initiated interrul ) tive subdialogue about where the right \ [ 14\ ] System : The left knuckle arm is removed like this .</sentence>
				<definiendum id="0">DSF121</definiendum>
				<definiens id="0">the user-initiated interrul ) tive subdialogue about where the right</definiens>
			</definition>
			<definition id="14">
				<sentence>At this time , if the user is viewing the left side ( Figure 2 ) and the system has the goal ( Instruct-act S H remove-tierod_end_r MR ) , ( Operator 2 } in Figure 4 is applied because the target object , right tie rod end , is not visible fi'om the user 's viewpoint .</sentence>
				<definiendum id="0">Instruct-act S H remove-tierod_end_r MR</definiendum>
				<definiens id="0">not visible fi'om the user 's viewpoint</definiens>
			</definition>
			<definition id="15">
				<sentence>Explanation and Interaction : The Computer Generation of Expalanatory Dialogues .</sentence>
				<definiendum id="0">Explanation</definiendum>
				<definiendum id="1">Interaction</definiendum>
			</definition>
			<definition id="16">
				<sentence>Cosmo : A lih ; -like animated pedagogical agent witl , deictie believability .</sentence>
				<definiendum id="0">Cosmo</definiendum>
				<definiens id="0">A lih ; -like animated pedagogical agent witl , deictie believability</definiens>
			</definition>
			<definition id="17">
				<sentence>COLLAGEN : A collaboration manager for software interfhce agents .</sentence>
				<definiendum id="0">COLLAGEN</definiendum>
			</definition>
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>However , with a left-re ( : ursive grammar such l ) ars ( 'as tyl ) i ( : ally fail to termim~te .</sentence>
				<definiendum id="0">left-re</definiendum>
				<definiens id="0">ursive grammar such l ) ars ( 'as tyl</definiens>
			</definition>
			<definition id="1">
				<sentence>Although we do not investigate it in this 1 ) al ) er , the selective left- ( : orner transform should usually lmve a slnaller sear ( : h sl ) ace relative , to tim standard left-corner transform , all else being equal .</sentence>
				<definiendum id="0">selective left-</definiendum>
			</definition>
			<definition id="2">
				<sentence>Cn ( G ) = ( V1 , T , P1 , S ) , where : V1 = VU { D X : DEV , XcVUT } and P1 contains all instances of tile schemata 1 .</sentence>
				<definiendum id="0">Cn</definiendum>
				<definiendum id="1">P1</definiendum>
				<definiens id="0">contains all instances of tile schemata 1</definiens>
			</definition>
			<definition id="3">
				<sentence>D -+ a where D = &gt; * A P L A , -+ ~ G -L D-B -- + fl D C whereC -- &gt; BflcL D-B - } fl wllereD~ } , C , C~Bfl6L Moore ( 2000 ) introduces a version of the leftcorner transform called LCLIt , which al ) plies only to productions with left-recursive parent and left clfihl categories .</sentence>
				<definiendum id="0">LCLIt</definiendum>
				<definiens id="0">a where D = &gt; * A P L A , -+ ~ G -L D-B -- + fl D C whereC -- &gt; BflcL D-B - }</definiens>
			</definition>
			<definition id="4">
				<sentence>First , £CL ( G ) contains an instance of sdmma lb tbr each topdown production A -- + a and each D such that 37 .</sentence>
				<definiendum id="0">£CL ( G )</definiendum>
				<definiens id="0">contains an instance of sdmma lb tbr each topdown production A -- + a and each D such that 37</definiens>
			</definition>
			<definition id="5">
				<sentence>In l ; he tables , L/j is the st ' .</sentence>
				<definiendum id="0">L/j</definiendum>
				<definiens id="0">the st '</definiens>
			</definition>
			<definition id="6">
				<sentence>N is the sel of 1 ) roclu ( : l ; ions in 1~ whose hfft-ha\ ] M sides do not begin with a part-ofspee ( : h ( P ( ) S ) tag ; 1 ) ecause I'OS tags are distinct front other nontermimtls in l ; he tree-lmnk , N is an easily identified set of I ) roductions guaranteed to include L0 .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">an easily identified set of I ) roductions guaranteed to include L0</definiens>
			</definition>
			<definition id="7">
				<sentence>In the grammar and tree transfl ) rms , P is the set , of productions in G ( i.e. , the standard M't-corner transform ) , N is the set of all productions in P which do not begin with a POS tag , mM L0 is the set of left-recursive t ) roclu ( : tions .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">mM L0</definiendum>
				<definiens id="0">the set of all productions in P which do not begin with a POS tag</definiens>
				<definiens id="1">the set of left-recursive t</definiens>
			</definition>
			<definition id="8">
				<sentence>BUP : A 1 ) otl ; oIn-ll I ) t ) arser embedded in Prolog .</sentence>
				<definiendum id="0">BUP</definiendum>
				<definiens id="0">A 1 ) otl ; oIn-ll I ) t ) arser embedded in Prolog</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>c word level , which is conceptually regarded as a part of the subdivision level .</sentence>
				<definiendum id="0">c word level</definiendum>
				<definiens id="0">a part of the subdivision level</definiens>
			</definition>
			<definition id="1">
				<sentence>\Ve define tags Ae , ~ , l , Bext~ C T ~ : t as follows : Figure 3 : the word extended tag set We define two smoothing coetIicients : A~ is the smoothing ratio for the current position and /~j , is the smoothing ratio of the preceding position .</sentence>
				<definiendum id="0">A~</definiendum>
				<definiens id="0">the smoothing ratio of the preceding position</definiens>
			</definition>
			<definition id="2">
				<sentence>If the current position is smoothed , then the tag probability is defined as follows ( note that wi itself is an individuated tag ) : /5 ( wilti_l ) = ( ( 1 A~ ) P ( tilti_\ ] ) + A~P ( wilti_l ) ) If the word at the preceding positions is smoothed ( assume ti-\ ] is the POS of wi-1 ) : .</sentence>
				<definiendum id="0">tag probability</definiendum>
				<definiens id="0">follows ( note that wi itself is an individuated tag ) : /5 ( wilti_l ) = ( ( 1 A~ ) P ( tilti_\ ] ) + A~P ( wilti_l ) ) If the word at the preceding positions is smoothed</definiens>
			</definition>
			<definition id="3">
				<sentence>ao~ = ,4 \ 0v , ~ , ... , ~ , ,~ , , } Bcxt = / ) \ { wv~ , ... , wt , , , ~ } To estimate the probability for tile comlection A-B , tile frequency F ( Aea : t , Bext ) is used rather than the total frequency F ( A , B ) .</sentence>
				<definiendum id="0">total frequency F</definiendum>
				<definiens id="0">,~ , , } Bcxt = / ) \ { wv~ , ... , wt , , , ~ } To estimate the probability for tile comlection A-B , tile frequency F ( Aea : t</definiens>
			</definition>
			<definition id="4">
				<sentence>\Ve , assume that th ( ; grouping at the current 1 ) osit ; ion ( '7 -~ ) share the same grouping of the t ) i-grain case .</sentence>
				<definiendum id="0">\Ve</definiendum>
				<definiens id="0">assume that th ( ; grouping at the current 1 ) osit ; ion ( '7 -~ ) share the same grouping of the t</definiens>
			</definition>
			<definition id="5">
				<sentence>Transformation-Based Error-Driven Learning and Natural Language Processing : A Case Study in Part-of-Speech Tagging .</sentence>
				<definiendum id="0">Transformation-Based Error-Driven Learning</definiendum>
			</definition>
</paper>

		<paper id="2160">
			<definition id="0">
				<sentence>1072 Table 1 : Factors of less readability and their revision methods ~ factors -A-B syntactic complexity C lack of conjunctive expressions/ presence of extraneous conjunctive expressions redundant repetition lack of information revision methods add/delete conjunctive expressions combine two sentences ; divide a sentence into two prononfinalizei onfit expressions ; add demonstratives supplement omitted expressions ; replace anaphors by antecedents ; delete anaphors required techniques discourse structure analysis anaphora and ellipsis resolution add supplementary information information extraction lack of adverbial particles ; add/delete adverbial particles presence of extraneous adverbial particles D ) Lack of information ~ : ' : , .</sentence>
				<definiendum id="0">add demonstratives supplement omitted</definiendum>
				<definiens id="0">Factors of less readability and their revision methods</definiens>
			</definition>
			<definition id="1">
				<sentence>Table 2 : The number of revisions I \ ] revision methods I total I A add ( 61 ) /delete ( ll ) conjunctive expressions 72 B combine two sentences ( 2 ) divide a sentence into two ( 6 ) 8 C pronominalize ( 5 ) ; omit expressions ( 3 ) add demonstratives ( 8 ) 16 D supplement omitted expressions ( lI ) replace anaphors by antecedents ( 10 ) delete anaphors ( 15 ) 36 add supplementary information ( 26 ) 26 E delete adverbial particles ( 4 ) add adverbial particles ( 5 ) 9 167 We compared our system 's revisions with the answer set comprising revisions that more than two subjects made .</sentence>
				<definiendum id="0">supplement omitted expressions</definiendum>
				<definiens id="0">B combine two sentences ( 2 ) divide a sentence into two</definiens>
				<definiens id="1">E delete adverbial particles ( 4 ) add adverbial particles ( 5 ) 9 167 We compared our system 's revisions with the answer set comprising revisions that more than two subjects made</definiens>
			</definition>
</paper>

		<paper id="1074">
			<definition id="0">
				<sentence>2 , the neuro tagger consists of a three-layer I ) erceptron with elastic input .</sentence>
				<definiendum id="0">neuro tagger</definiendum>
				<definiens id="0">consists of a three-layer I ) erceptron with elastic input</definiens>
			</definition>
			<definition id="1">
				<sentence>Output OPT is defined as OFT= , o+ ) , ( s ) provi &lt; led that the output OI ) T is decoded as S r/ ifOi=l &amp; O/=0forj¢i YN ( Wt ) Unknown otherwise , ( .9 ) where rN ( W , ) is the £a.gging result obtained by the neuro tagger. There is more inforlnation available for constructing the input for words on the left , because they have already been tagged. In the tagging phase , instead of using ( 4 : ) - ( 6 ) , the input can be constructed simply as i/ , ,_4 = . oPT ( -i ) , ( 1 ( ) ) where i = 1 , ... ,1 , and O I ) T ( -i ) means the output of the tagger for the ith word before the target word. ltowever , in the training process , the out ; put of the tagger is not alway.a correct a.nd can not be ted back to the inputs directly. Instead , a weighted awerage of the actual output a.nd tlm desired output is used : iptt_i = 9t-i ' ( wol , T `` 0 PT ( i ) + WlOJ , :s `` I ) liS ) , ( 1.1 ) where 1 ) l ' : ,q ' is the desired output , o : , : ,5 ' : ( &amp; , / ) 2 , ... , whose bits are defined as \ ] iI ' r i is a desired answer I ) i = 0 otherwise , ( la ) and WOl , 'r and w/ ) l , \ ] , q ' are respecLh : ely de\ [ ( ned as 1'\ ] 013J ~ , :o1'~ ... . ( .14 ) 1JACT ' a , nd 'w &gt; l , ; , s , = :1 wopT , ( 15 ) where \ ] @ , uo and \ ] 'JAC'T are the objective and actual errors .</sentence>
				<definiendum id="0">Output OPT</definiendum>
				<definiendum id="1">-i ) , ( 1 ( )</definiendum>
				<definiens id="0">s ) provi &lt; led that the output OI ) T is decoded as S r/ ifOi=l &amp; O/=0forj¢i YN ( Wt ) Unknown otherwise , ( .9 ) where rN ( W , ) is the £a.gging result obtained by the neuro tagger. There is more inforlnation available for constructing the input for words on the left , because they have already been tagged. In the tagging phase</definiens>
				<definiens id="1">) where i = 1 , ... ,1 , and O I ) T ( -i ) means the output of the tagger for the ith word before the target word. ltowever , in the training process , the out ; put of the tagger is not alway.a correct a.nd can not be ted back to the inputs directly. Instead , a weighted awerage of the actual output a.nd tlm desired output</definiens>
			</definition>
			<definition id="2">
				<sentence>ns\ [ orln ; ~tion rules Change t ; ag v a to t ; ag v ° when : ( single inlm| ; ) ( input ( 'onsists of a POS ) ( inI ) n| ; consist ; s of a word ) ( AND logical inpu¢ ot '' words ) ( AND logical in .</sentence>
				<definiendum id="0">ag v</definiendum>
				<definiens id="0">'onsists of a POS ) ( inI ) n| ; consist ; s of a word ) ( AND logical inpu¢ ot '' words ) ( AND logical in</definiens>
			</definition>
</paper>

		<paper id="2099">
			<definition id="0">
				<sentence>ltere , N+ denotes the set of positive integers { 1,2 , ... } and N~_ is the set of strings over N+ .</sentence>
				<definiendum id="0">N+</definiendum>
				<definiens id="0">the set of positive integers { 1,2 , ... } and N~_ is the set of strings over N+</definiens>
			</definition>
			<definition id="1">
				<sentence>T ( ( ) ) ranging over the power bag* N D , indicating the bag of dependency types of dJ 's children : m ( 4 , ) = f = \ [ d , , ... , dl , \ ] * &gt; e~O P ( c/ , ) = v A V : ic { i ... .. v } D ( 6j ) = dj Figure 3 encodes the dependency ti ; ee ( ) 1 ' Figure 2 accordingly .</sentence>
				<definiendum id="0">T ( ( ) )</definiendum>
				<definiens id="0">ranging over the power bag* N D , indicating the bag of dependency types of dJ 's children : m ( 4 , ) = f = \ [ d , , ... , dl</definiens>
			</definition>
			<definition id="2">
				<sentence>lJ~ ( @ j ) is the probability of the label £ ( 4~J ) of a node given the label £ ( 4 ' ) of its regent and the dependency type `` D ( 0j ) linking them .</sentence>
				<definiendum id="0">lJ~ ( @ j )</definiendum>
				<definiens id="0">the probability of the label £ ( 4~J ) of a node given the label £</definiens>
			</definition>
			<definition id="3">
				<sentence>We then define tile stochastic string-realization process by letting tile 8 ( ~5 ) variables , given ¢ 's label 1 ( 40 and the bag of strings s ( ( ) j ) of ~5 's child nodes , randomly permute and concatenate them according to the probability distributions of the modeh Ps ( 0 = = s , ( s ( &lt; . )</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">tile stochastic string-realization process by letting tile 8 ( ~5 ) variables , given ¢ 's label 1 ( 40 and the bag of strings s ( ( ) j ) of ~5 's child nodes</definiens>
			</definition>
			<definition id="4">
				<sentence>= 401 c ( O , 7 ( 0 , c ( O ) ~'~ ( 4 , ) = { for¢¢~ } = s ' ( s ( ¢ , ) = 4¢ ' ) I &gt; frO , eft , ) , 7 ( ¢ ) , c ( ¢ , ) ) where c ( 4 , ) = 0 \ [ ~ ( &lt; bJ ) \ ] j=l 8 ( ~ $ ) = adjoin ( C ( g , ) , l ( ¢ ) ) adjoin ( A , /3 ) = eoncat ( permute ( A U \ [ /3\ ] ) ) The latter equations should be interpreted as defining the randorn variable 8 , rather than specifying its probability distribution or some possible outcome. This means that each dependent is realized adjacent to its regent , where wc allow intervening siblings , and that we thus stay within the expressive power of stochastic context-free grammars. We define the string-realization probability ~beAr and the tree-string probability as P ( 7 , s ) = I '' ( 7- ) ./ , ( s I 7 ) The stochastic process generating the tree structure is as described above. We then generate the string variables S using a bottom-up stochastic process. Figure 3 also shows the process realizing the surface string John ate beans fl-om the dependency tree of Figure 2 by reading the S column upwards : ,9 ( 12 ) = bca , ,s , S ( 11 ) = Job , , , ,9 ( 1 ) = s ( 11 ) ate s ( 12 ) , S ( e ) = s ( l ) . Consider cMeulating tile striug probability at node tion observed of the strings of the children and the ? le did say/1 ~bj/ ' % sc0nj Mao , /l 1 that ate/l 2 subj/ '' ~ol0bj Johnll21 Whatbeans/122 Figure 4 : Del ) endency tree for What beans did Mary say lhat John ate ? 1M ) el of the node. To overcome the sparse-data problem , we will generalize over the actual strings of tile children to their dependency types. For example , s ( subj ) denotes the string of the subject child , regardless of what it actually might be. J'~ ( 1 ) = P ( S ( 1 ) = s ( subj ) ate s ( dobj ) I I `` D ( 1 ) = m~n , s : ( 1 ) = &lt; , re , C ( I ) = \ [ s ( subj ) , s ( dobj ) \ ] ) This is the probability of the permutation ( s ( subj ) , ate , s ( dobj ) ) of the bag \ [ s ( subj ) , aic , s ( dobj ) \ ] given this bag and the fact that we wish to tbrm a main , declarative clause. This example highlights the relationship between the node strings and both Sallssure 's notion of constituency and tile l ) ositiolml schemata of , amongst others , l ) idrichsen. To accommodate long-distance dependencies , we allow a dependent to be realized adjacent to the label of rely node that dominates it , immediately or not. For example , consider the dependency tree of Figure 4 tbr the sentence l/Vhat beans did Ma'Jw say that John ate ? as encoded in Figure 5. Ilere , What beans is a dependent of that arc , which in turn is a dependent of did say , and What beans is realized between did and sag. This phenomenon is called movement in conjunction with phrase-structure gramm~rs. It makes the dependency grammar nonprojective , since it creates crossing dependency links if the dependency trees also depict the word order. We introduce variables A// ( ~ ) that randomly select from C ( 4 ) ) a , subbag CM ( 4 , ) of strings passed up to ( ) 's regent : C ( ¢ ) = O ( \ [ s ( 4 ) J ) \ ] UCM ( ¢j ) ) j=l Gd &lt; / , ) c_ c ( 4 ) ) s'~ ( 4 , ) = P ( M ( 4 ) ) = c'~ , , ( e ) I I ~ ( &lt; t , ) , s : ( ¢ ) , fifO , c ( 6 ) ) 686 N '' £ be c v \ [ whq\ ] 1 did say \ [ subj , sconj\ ] 11 Mary ( ~ :12 thai ate \ [ subj , dobj\ ] 122 What beans 0 Figure 5 : l ) ependency encoding of What beans did Mary say that John ate ? A/ M S di ( /411 ) 11 ~ Mary 12 \ [ .s ( 122 ) \ ] that s ( 121 ) ate 121 ~J John 122 0 W/tat bem~s Fi. ? ; ure 6 : Process generating What beans did Mary .sag that dohn ate ? q'he rest of the strings , Cs ( ¢ ) , are realized here : c : , ; ( ¢ ) = c ( ¢ ) \ = = I be ( C ) , c ; , ( ¢ ) ) s ' ( ¢ ) = 3,3 Discontinuous Nuch ; i We generalize the scheme to discontinuous nuclei by allowing 8 ( ¢ ) to i , mert the strings of C~. ( ~5 ) anywhere in 1 ( ¢ ) : e adjoin ( A , fl ) = = V @ m j=l fl = b~ ... b , , , Tllis means that strings can only l ) e inserted into ancestor labels , , lot into other strings , which enforces a. type of reverse islaml constraint. Note how in Figure 6 John is inserted between that and ate to form the subordina , te clause that John atc. We define tile string-realization probability , b6 ar and again define the tree-string prol ) ability ~ ' ( r , S ) = l ' ( T ) .l ' ( a I T ) 2x -~ y indicates that x precedes y in the resulting permutation , q~snihre 's original implicit definition of a nucleus actually does not require that the order be preserved when realizing it ; if has catch is a nucleus , so is eaten h.as. This is obviously a useflfl feature for nlodeling verb chains in G erln &amp; n subordinate clauses. 'lb avoid derivational ambiguity when generating a tree-string pair , i.e. , have more than one derivation generate tile same tree-string pair , we require that no string be realized adjacent to the string of any node it was passed u 1 ) through. This introduces the l ) raetica.l problem of ensuring that zero probability mass is assigned to all derivations violating this constraint. Otherwise , the result will be approxima.ting the parse probabi\ ] ity with a derivation probability , as described in detail in ( Samuelsson , 2000 ) based on the seminal work of ( Sima'an , 1996 ) . Schemes like ( Alshawi , 1996 ) tacitly make this approximation. The tree-structure variables £ and be are generated just as before. ~¥e then generate the string variables 8 and Ad using a bottom-up stochastic process , where M ( ¢ ) is generated before 8 ( ¢ ) . 'l.'he probability of the eonditkming material of \ ] o~ ( ¢ ) is then known either from the top-down process or from I 'M ( ¢j ) and Pa ( ¢j ) , and that of INTO ) is known either from the top-down process , or from 15v4 ( ¢ ) , \ [ ) dgq ( 4 ) j ) and 1~ ( ¢j ) . The coherence of S ( ~ ) a.nd f14 ( ~/ ) ) is enforced by explicit conditioning. Figure 5 shows a top-down process generating the dependency tree of Figure &lt; 1 ; the columns £ and be should be read downwards in parallel , L ; before b e. Figure 6 shows a bottom-up process generating the string l/Vhat beans did Mary say that dohn at ( : ? from the dependency description of Figure 5. The colltlll , lS d~v4 and S should be read upwards in parallel , 2t4 before $ . We have increased the expressive power of our dependency gramma.rs by nlodifying tile S variables , i.e. , by extending the adjoin opera.lion. In tile first version , the adjoin operation randomly permutes the node label and the strings of the child nodes , and concatenates the result. In the second version , it randondy inserts the strings of the child nodes , and any moved strings to be rea.lized at tile current node , into the node label. The adjoin operation can be fln : ther refined to allow handling an even wider range of phenomena , such as negation in French. Here , the dependent string is merged with the label of the regent , as ne ... pas is wrapped around portions of the verb phrase , e.g. , Ne me quitte pas ! , see ( Brel , 195. ( t ) . Figure 7 shows a dependency tree h ) r this. In addition to this , the node labels may be linguistic abstractions , e.g. `` negation '' , calling on the S variables also for their surface-string realization. Note that the expressive power of the grammar depends on the possible distributions of the string probabilities IN. Since each node label can be moved and realized at the root node , any language can be recognized to which the string probabilities allow assigning the entire probablity mass , and the gralnmar will possess at least this expressive power. 687 ! le imnl quitte/l ~g &gt; ~do~ Ne pas/l l me~12 Figure 7 : Dependency tree for Ne me quitte pas t A close approximation of the described stochastic model of dependency syntax has been realized as a type of prohabilistic bottom-up chart parser .</sentence>
				<definiendum id="0">U</definiendum>
				<definiendum id="1">Otherwise</definiendum>
				<definiens id="0">c ( O ) ~'~ ( 4 , ) = { for¢¢~ } = s ' ( s ( ¢ , ) = 4¢ ' ) I &gt; frO , eft , ) , 7 ( ¢ ) , c ( ¢ , ) ) where c ( 4 , ) = 0 \ [ ~ ( &lt; bJ ) \ ] j=l 8 ( ~ $ ) = adjoin ( C ( g , ) , l ( ¢ )</definiens>
				<definiens id="1">distribution or some possible outcome. This means that each dependent is realized adjacent to its regent , where wc allow intervening siblings , and that we thus stay within the expressive power of stochastic context-free grammars. We define the string-realization probability ~beAr and the tree-string probability as P ( 7 , s ) = I '' ( 7- ) ./ , ( s I 7 ) The stochastic process generating the tree structure is as described above. We then generate the string variables S using a bottom-up stochastic process. Figure 3 also shows the process realizing the surface string John ate beans fl-om the dependency tree of Figure 2 by reading the S column upwards : ,9 ( 12 ) = bca , ,s , S ( 11 ) = Job , , , ,9 ( 1 ) = s ( 11 ) ate s ( 12 ) , S ( e ) = s ( l ) . Consider cMeulating tile striug probability at node tion observed of the strings of the children</definiens>
				<definiens id="2">the string of the subject child , regardless of what it actually might be. J'~ ( 1 ) = P ( S ( 1 ) = s ( subj ) ate s ( dobj ) I I `` D ( 1 ) = m~n , s : ( 1 ) = &lt; , re , C ( I ) = \ [ s ( subj ) , s ( dobj ) \ ] ) This is the probability of the permutation ( s ( subj ) , ate , s ( dobj ) ) of the bag \ [ s ( subj ) , aic , s ( dobj ) \ ] given this bag and the fact that we wish to tbrm a main , declarative clause. This example highlights the relationship between the node strings and both Sallssure 's notion of constituency and tile l ) ositiolml schemata of , amongst others , l ) idrichsen. To accommodate long-distance dependencies , we allow a dependent to be realized adjacent to the label of rely node that dominates it , immediately or not. For example , consider the dependency tree of Figure 4 tbr the sentence l/Vhat beans did Ma'Jw say that John ate ? as encoded in Figure 5. Ilere , What beans is a dependent of that arc , which in turn is a dependent of did say , and What beans is realized between did and sag. This phenomenon is called movement in conjunction with phrase-structure gramm~rs. It makes the dependency grammar nonprojective , since it creates crossing dependency links if the dependency trees also depict the word order. We introduce variables A// ( ~ ) that randomly select from C ( 4 ) ) a , subbag CM ( 4 , ) of strings passed up to ( ) 's regent : C ( ¢ ) = O</definiens>
				<definiens id="3">t , ) , s : ( ¢ ) , fifO , c ( 6 ) ) 686 N '' £ be c v \ [ whq\ ] 1 did say \ [ subj</definiens>
				<definiens id="4">Process generating What beans did Mary .sag that dohn ate ? q'he rest of the strings , Cs ( ¢ ) , are realized here : c : , ; ( ¢ ) = c ( ¢ ) \ = = I be ( C ) , c ; , ( ¢ ) ) s ' ( ¢ ) = 3,3 Discontinuous Nuch ; i We generalize the scheme to discontinuous nuclei by allowing 8 ( ¢ ) to i , mert the strings of C~. ( ~5 ) anywhere in 1 ( ¢ ) : e adjoin ( A , fl ) = = V @ m j=l fl = b~ ... b , , , Tllis means that strings can only l ) e inserted into ancestor labels , , lot into other strings , which enforces a. type of reverse islaml constraint. Note how in Figure 6 John is inserted between that and ate to form the subordina , te clause that John atc. We define tile string-realization probability , b6 ar and again define the tree-string prol ) ability ~ ' ( r , S ) = l ' ( T ) .l ' ( a I T ) 2x -~ y indicates that x precedes y in the resulting permutation , q~snihre 's original implicit definition of a nucleus actually does not require that the order be preserved when realizing it ; if has catch is a nucleus , so is eaten h.as. This is obviously a useflfl feature for nlodeling verb chains in G erln &amp; n subordinate clauses. 'lb avoid derivational ambiguity when generating a tree-string pair , i.e. , have more than one derivation generate tile same tree-string pair , we require that no string be realized adjacent to the string of any node it was passed u 1 ) through. This introduces the l ) raetica.l problem of ensuring that zero probability mass is assigned to all derivations violating this constraint.</definiens>
				<definiens id="5">the string variables 8 and Ad using a bottom-up stochastic process , where M ( ¢ ) is generated before 8 ( ¢ ) . 'l.'he probability of the eonditkming material of \ ] o~ ( ¢</definiens>
				<definiens id="6">increased the expressive power of our dependency gramma.rs by nlodifying tile S variables , i.e. , by extending the adjoin opera.lion. In tile first version , the adjoin operation randomly permutes the node label and the strings of the child nodes , and concatenates the result. In the second version , it randondy inserts the strings of the child nodes , and any moved strings to be rea.lized at tile current node , into the node label. The adjoin operation can be fln : ther refined to allow handling an even wider range of phenomena , such as negation in French. Here , the dependent string is merged with the label of the regent , as ne ... pas is wrapped around portions of the verb phrase</definiens>
				<definiens id="7">abstractions , e.g. `` negation '' , calling on the S variables also for their surface-string realization. Note that the expressive power of the grammar depends on the possible distributions of the string probabilities IN. Since each node label can be moved and realized at the root node , any language can be recognized to which the string probabilities allow assigning the entire probablity mass</definiens>
				<definiens id="8">Dependency tree for Ne me quitte pas t A close approximation of the described stochastic model of dependency syntax has been realized as a type of prohabilistic bottom-up chart parser</definiens>
			</definition>
			<definition id="5">
				<sentence>According to Tesni6re , a nucleus is a unit that contains both tile syntactic and semantic head and that does not exhihit any internal syntactic structure .</sentence>
				<definiendum id="0">nucleus</definiendum>
				<definiens id="0">a unit that contains both tile syntactic and semantic head and that does not exhihit any internal syntactic structure</definiens>
			</definition>
			<definition id="6">
				<sentence>The string-realization probability is a straightforward generalization of that given at the end of Section 3.1 , and they m : e defined through regular expressions .</sentence>
				<definiendum id="0">string-realization probability</definiendum>
				<definiens id="0">a straightforward generalization of that given at the end of Section 3.1</definiens>
			</definition>
			<definition id="7">
				<sentence>Towards history-based grammars : Using richer models tbr probabilistic parsing .</sentence>
				<definiendum id="0">Towards history-based grammars</definiendum>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>TILe objects are tokens , or words , and the measm'e of cohesion is that one word follows from the other in accordance with the the nature of the language , the content of the document , and tlm idiom of the particular document element within which they may be contained and that the spatial model of the layout of the docmnent permits cohesion .</sentence>
				<definiendum id="0">TILe objects</definiendum>
				<definiens id="0">the content of the document , and tlm idiom of the particular document element within which they may be contained and that the spatial model of the layout of the docmnent permits cohesion</definiens>
			</definition>
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>In our method , the likelihood value L ( M ) of a ( partial ) tree M in Figure 1 is detined as in ( 1 ) : L ( M ) dor L ( NH ) x L ( H ) x P ( 'n ~ h ) ( 1 ) where NH is M 's non-head daughter ( whose lexical head is n ) , H is the head-daughter ( whose lexical head is h ) , and / ) ( n -~ h ) is the probability of n t ) eing related to h. For a. single lexical iteni W , L ( W ) is defined as 1.0 .</sentence>
				<definiendum id="0">H</definiendum>
				<definiendum id="1">) ( n -~ h )</definiendum>
				<definiens id="0">the likelihood value L ( M ) of a ( partial ) tree M in Figure 1 is detined as in ( 1 ) : L ( M ) dor L ( NH ) x L ( H ) x P ( 'n ~ h ) ( 1 ) where NH is M 's non-head daughter ( whose lexical head is n ) ,</definiens>
				<definiens id="1">the head-daughter ( whose lexical head is h ) , and /</definiens>
				<definiens id="2">the probability of n t ) eing related to h. For a. single lexical iteni W</definiens>
			</definition>
			<definition id="1">
				<sentence>Attributes ( I ) i and ~I , j consist of a part-of-speech ( POS ) , a lexical item , presence of a comma , and so on .</sentence>
				<definiendum id="0">Attributes</definiendum>
				<definiens id="0">a lexical item , presence of a comma , and so on</definiens>
			</definition>
			<definition id="2">
				<sentence>The 'Diplel , /Quadruplet Model calculates the likelihood of the dependency between bunsetsu i and bunsctsu cn ; P ( i -- , cn ) with the formulas ( 8 ) and ( 9 ) , where c , ~ denotes the nth candidate among b , msctsu i 's candidates ; ( I , i denotes some attributes of i ; and ~I~¢ , ~ denotes attributes of c , ~ ( including attributes between i and cn ) .</sentence>
				<definiendum id="0">/Quadruplet Model</definiendum>
				<definiens id="0">calculates the likelihood of the dependency between bunsetsu i and bunsctsu cn ; P ( i -- , cn ) with the formulas ( 8 ) and ( 9 ) , where c , ~ denotes the nth candidate among b , msctsu i 's candidates</definiens>
			</definition>
			<definition id="3">
				<sentence>• TILe Pair Model outperforms the W Restriction Model by 0.3 % .</sentence>
				<definiendum id="0">TILe Pair Model</definiendum>
			</definition>
			<definition id="4">
				<sentence>• TILe W Restriction Model outperforms tile W Grammar Model by 0.7 % .</sentence>
				<definiendum id="0">TILe W/O Restriction Model</definiendum>
			</definition>
			<definition id="5">
				<sentence>Tlfis is the main factor that contributed to the improvement of the overall parsing accuracy .</sentence>
				<definiendum id="0">Tlfis</definiendum>
				<definiens id="0">the main factor that contributed to the improvement of the overall parsing accuracy</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>Japanese is a language where most nouns can not be directly modified by numerals , instead , nouns are modified by a numeral-classifier combinatiou as shown in ( 1 ) .2 2~Vc use lhe following abbreviations : NOM : nominative ; ACC = accusative ; AI ) N = adnominal ; CI .</sentence>
				<definiendum id="0">Japanese</definiendum>
				<definiens id="0">a language where most nouns can not be directly modified by numerals</definiens>
			</definition>
			<definition id="1">
				<sentence>The Japatwse Language : An h~troduction .</sentence>
				<definiendum id="0">Japatwse Language</definiendum>
				<definiens id="0">An h~troduction</definiens>
			</definition>
			<definition id="2">
				<sentence>WordNet : An Electronic Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="2161">
			<definition id="0">
				<sentence>A TDB is a two-sorted tirst-oMer structure .</sentence>
				<definiendum id="0">TDB</definiendum>
				<definiens id="0">a two-sorted tirst-oMer structure</definiens>
			</definition>
			<definition id="1">
				<sentence>The domain consists of a Data Do'main , D , and a Temporal Domain of intervals , 7'/ , detined as follows .</sentence>
				<definiendum id="0">domain</definiendum>
				<definiens id="0">consists of a Data Do'main , D , and a Temporal Domain of intervals , 7'/ , detined as follows</definiens>
			</definition>
			<definition id="2">
				<sentence>l , et : L : :=/~ , ~ ( x , I ) ILA LI~LI~x.LI~I.LIx = yllc~J where x , y are variables over D , x is a vector of such variables , l , J are varlet ) los or constants over T\ ] , and ( r is one of the operators : precedes , meets , overlaps , equals , contains .</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">a vector of such variables , l</definiens>
				<definiens id="1">one of the operators : precedes , meets</definiens>
			</definition>
			<definition id="3">
				<sentence>LAlien is defined as tile set of ' \ [ brmulae p E L that conLain at most one free variable over TI .</sentence>
				<definiendum id="0">LAlien</definiendum>
				<definiens id="0">tile set of ' \ [ brmulae p E L that conLain at most one free variable over TI</definiens>
			</definition>
			<definition id="4">
				<sentence>The meaning representation of the main clause Mary worked in marketing is constructed as : AJ _ / ) In this formula , I denotes a Reichenbachianlike reference time , J denotes a time interval 1078 ( I , ring which Mary worked in nia , rl ( eA : \ ] nlg , which is loca , l ; od in the l ) a , sl ; ( l ; he conl ; ribution ol7 the t , onse ) a , nd is hicluded wil ; hin 1 .</sentence>
				<definiendum id="0">J</definiendum>
				<definiendum id="1">nlg</definiendum>
				<definiens id="0">a time interval 1078 ( I , ring which Mary worked in nia</definiens>
			</definition>
			<definition id="5">
				<sentence>Developin , g TimeOriented Database Applicatio'n , s in SQL .</sentence>
				<definiendum id="0">Developin</definiendum>
				<definiens id="0">s in SQL</definiens>
			</definition>
</paper>

		<paper id="2091">
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>Also suppose thatKis a Korean word , where lq is the i-th phonetic unit of K. S.~SIS 2 `` '' Sn~ K = k 1 k2 ... k , , ( l ) Let us say P ( E , K ) is the probability that an English word E is transliterated to a Korean word K. What we have to find is K where P ( E , K ) is lnaximized given E. This probability can be approximated by substituting the English word E with its prontmciation S. Thus , the following formula holds .</sentence>
				<definiendum id="0">K )</definiendum>
				<definiens id="0">the probability that an English word E is transliterated to a Korean word K. What we have to find is K where P ( E , K</definiens>
			</definition>
			<definition id="1">
				<sentence>Secondly , P ( s , ~ Ik~s~ ) and P ( s , ~ Is~ ) are substituted for p ( si+ I I ki_lsi_lkisi ) and P ( s , I I k , _ , s , _ , s , ) .</sentence>
				<definiendum id="0">P (</definiendum>
				<definiens id="0">s , I I k , _ , s , _ , s , )</definiens>
			</definition>
			<definition id="2">
				<sentence>argmaxP ( S , K ) K rnr P ( k , l s. , k , _ , ) P ( s , I k , s.OP ( s. , I k~s , ) = arglnaxl 1 x , P ( si+ , j si ) ( 7 ) English Pron : o o o ~isi-1 @ _ @ si+l Korean Notatio % e o @ `` N~ 0 0 0 &lt; Figure 3 &gt; Statistical information source used in the extended Markov window ( 1 ) \ ] ' ( k , l.s. , _\ ] k , , ) ( 2 ) l ' ( s , lk , s , _ , ) ( 3 ) P ( ss+ , \ ] kr % ) Figure 3 pictorially summarizes the final information sources that our statistical tagger utilizes .</sentence>
				<definiendum id="0">argmaxP</definiendum>
				<definiens id="0">Statistical information source used in the extended Markov window ( 1 ) \ ] ' ( k</definiens>
			</definition>
			<definition id="3">
				<sentence>Recall = cottnl ( &lt; generaled , correcl wos'dv ) ( 9 ) count ( possible , correct._ worUs ) Precision= c ( , zmt ( x , enerated con'oct_ : word , v ) ( 10 ) comet ( generated_ words ) For words not found in the pronunciation dictionary , a transcription ailtonlata is used Io lransfornl the English alphabet to the Korean alphabet , k transcription aulonlata can be helpful bocatlsO it ilsos alphabetic illfornlalioll thai otir statistical tag ; ger does ilot llso. \ ] 'tie atltom ; _ila produces one result aiid ailachos it at the end of N-best results of the statistical tagger. This automata has about 500 lranscriplion rules , based oil previous , current , and lleXt coulext window and production alphabet. I O 0.t~ ¢1 ) 0.4 0 ... ... ... ... ... ... . ~t ~t'candidates &lt; Figure 4 &gt; Reoall value for each number of candidates All experimental results are estimated by 10-fold cross validation for more accurate results .</sentence>
				<definiendum id="0">k transcription aulonlata</definiendum>
				<definiendum id="1">ger</definiendum>
				<definiendum id="2">_ila</definiendum>
				<definiens id="0">word , v ) ( 10 ) comet ( generated_ words ) For words not found in the pronunciation dictionary</definiens>
			</definition>
</paper>

		<paper id="2102">
			<definition id="0">
				<sentence>Next , we apply Yarowsky 's method tbr supervised decision list learning I ( Yarowsky , 1994 ) to 1VVe choose tile decision list learning method as the 705 Table 1 : Statistics of NE Types of IREX NE Type ORGANIZATION PERSON LOCATION ARTIFACT DATE TIME MONEY PERCENT Total frequency ( % ) Training 3676 ( 19.7 ) 3840 ( 20.6 ) 5463 ( 29.2 ) 747 ( 4.0 ) 3567 ( 19.1 ) 502 ( 2.7 ) 390 ( 2.1 ) 492 ( 2.6 ) 18677 Test 361 ( 23.9 ) 338 ( 22.4 ) 413 ( 27.4 ) 48 ( 3.2 ) 260 ( 17.2 ) 54 ( 3.5 ) 15 ( 1.0 ) 21 ( 1.4 ) 1510 Japanese named entity recognition , into which we incorporate several noun phrase chunking techniques ( sections 3 and 4 ) and experimentally evaluate their performance on the IREX , workshop 's training and test data ( section 5 ) .</sentence>
				<definiendum id="0">ORGANIZATION PERSON LOCATION ARTIFACT DATE TIME MONEY PERCENT Total frequency</definiendum>
				<definiens id="0">Statistics of NE Types of IREX NE Type</definiens>
				<definiens id="1">workshop 's training and test data ( section 5 )</definiens>
			</definition>
			<definition id="1">
				<sentence>The novel technique incorporates richer contextual information as well as p~tterns of constituent morphemes within ~ named entity , compared with the techniques proposed in previous research on named entity recognition and base noun phrase chunking .</sentence>
				<definiendum id="0">novel technique</definiendum>
				<definiens id="0">incorporates richer contextual information as well as p~tterns of constituent morphemes within ~ named entity</definiens>
			</definition>
			<definition id="2">
				<sentence>U the morpheme at the current t ) osition is a named entity consisting of only one , mort ) heine .</sentence>
				<definiendum id="0">osition</definiendum>
				<definiens id="0">a named entity consisting of only one</definiens>
			</definition>
			<definition id="3">
				<sentence>Chunking/Tagging Named Entities For each of the two schemes of enco ( li1~g chunking states of nalned entities descrit ) ed in Section 3.2 , as the l ) ossible values of the &lt; teeision D , we consider exactly the same categories of chunking states as those described in Section 3.2. The evidence E used in the decision list learning is a combination of the tbatures of preceding/subsequent inorphemes as well as the morpheme at ; the current position. The following describes how to form the evidence E fi ) r 1 ) oth the a-gram nlodel and varial ) le length model. 3- , gram Model The evidence E ret ) resents a tut ) le ( F-l , F0 , F1 ) , where F-1 and F1 denote the features of immediately t ) receding/subsequent morphemes M_~ and M1 , respectively , F0 the featm : e of the morpheme 54o at the current position ( see Fonnuta ( 1 ) in Section 3.3.1 ) . The definition of the possible values of those tbatures F_l , F0 , and 1'~ are given below , where Mi denotes the roof 1 ) \ ] mnm itself ( i.e. , including its lexicM tbrm as well as part-of-sl ) eech ) , C , i the character type ( i.e. , JaI ) anese ( hiragana or katakana ) , Chinese ( kanji ) , numbers , English alphabets , symbols , and all possible combinations of these ) of Mi , Ti the part-of-st ) eech of Mi : F_ 1 : :m_ \ ] ~// -- 1 I ( C-1 , T-l ) I T-t Inull mlsmoothed conditional probability P ( D = x \ [ E = 1 ) . Yarowsky 's training Mgoritl , m also ditfcrs somewhat in his use of the ratio * ' ( ~D= , d*~-j ) ' which is equivalent in the case of binary classifications , and also by the interpolation between the global probalfilities ( used here ) and tl , e residual prol ) abilities further conditional on higherranked patterns failing to match in the list. 17'1 : : -- ~ \ ] ~/-/1 I ( C , ,V ; ) I T* Inul\ ] F0 : :M0 I ( C0 , T0 ) lT0 As the evidence E , we consider each possible coml ) ination of the values of those three f'ealures. Variable Length Model The evidence E rel &gt; resents a tuple ( FL , FNu , FIt ) , where FL and Fl~ denote the features of the morphemes ML_2ML1 and Mff'M~ ~ in the left/right contexts of the current named entity , respectively , FNE the features of the morphemes MN~ '' '' `` MNE `` `` '' MNEm ( _ &lt; 3 ) constituting the current named entity ( see Formula ( 2 ) in Section 3.3.2 ) .</sentence>
				<definiendum id="0">Chunking/Tagging Named Entities</definiendum>
				<definiendum id="1">Mi</definiendum>
				<definiens id="0">a combination of the tbatures of preceding/subsequent inorphemes as well as the morpheme at ; the current position. The following describes how to form the evidence E fi ) r 1 ) oth the a-gram nlodel and varial ) le length model. 3- , gram Model The evidence E ret ) resents a tut</definiens>
				<definiens id="1">the features of immediately t ) receding/subsequent morphemes M_~ and M1 , respectively , F0 the featm : e of the morpheme 54o at the current position ( see Fonnuta ( 1 ) in Section 3.3.1 ) . The definition of the possible values of those tbatures F_l , F0 , and 1'~ are given below , where</definiens>
				<definiens id="2">the roof 1 ) \ ] mnm itself ( i.e. , including its lexicM tbrm as well as part-of-sl ) eech ) , C , i the character type ( i.e. , JaI ) anese ( hiragana or katakana ) , Chinese ( kanji ) , numbers , English alphabets , symbols , and all possible combinations of these ) of Mi</definiens>
				<definiens id="3">:m_ \ ] ~// -- 1 I ( C-1 , T-l ) I T-t Inull mlsmoothed conditional probability P ( D = x \ [ E = 1 ) . Yarowsky 's training Mgoritl , m also ditfcrs somewhat in his use of the ratio * ' ( ~D= , d*~-j ) ' which is equivalent in the case of binary classifications , and also by the interpolation between the global probalfilities ( used here</definiens>
				<definiens id="4">each possible coml ) ination of the values of those three f'ealures. Variable Length Model The evidence E rel &gt; resents a tuple ( FL , FNu , FIt ) , where FL and Fl~ denote the features of the morphemes ML_2ML1 and Mff'M~ ~ in the left/right contexts of the current named entity , respectively , FNE the features of the morphemes MN~ '' ''</definiens>
			</definition>
			<definition id="4">
				<sentence>The definition of the possible values of those features 1 L , FNI , : , and FI~ arc given below , where F NI~ denotes the feature of the j-th constituent morpheme M .</sentence>
				<definiendum id="0">F NI~</definiendum>
				<definiens id="0">the feature of the j-th constituent morpheme M</definiens>
			</definition>
			<definition id="5">
				<sentence>tity consists of up to three mort ) heroes , as the possible values of the feature FNIi in the definition ( 3 ) , we consider only those which are consistent with the requirement that each nlort ) heme M NE is a constituent of the cun'ent named entity .</sentence>
				<definiendum id="0">tity</definiendum>
				<definiens id="0">consists of up to three mort ) heroes , as the possible values of the feature FNIi in the definition ( 3 ) , we consider only those which are consistent with the requirement that each nlort ) heme M NE is a constituent of the cun'ent named entity</definiens>
			</definition>
</paper>

		<paper id="1084">
			<definition id="0">
				<sentence>Hiragana is a set of 71 phonetic characters , which are mostly used for flmction words , inflections and adverbs .</sentence>
				<definiendum id="0">Hiragana</definiendum>
				<definiens id="0">a set of 71 phonetic characters</definiens>
			</definition>
			<definition id="1">
				<sentence>Kanji is a set of Chinese-origin characters .</sentence>
				<definiendum id="0">Kanji</definiendum>
				<definiens id="0">a set of Chinese-origin characters</definiens>
			</definition>
			<definition id="2">
				<sentence>The test corpus is a set of email messages sent between young female friends during 1999 .</sentence>
				<definiendum id="0">test corpus</definiendum>
				<definiens id="0">a set of email messages sent between young female friends during 1999</definiens>
			</definition>
</paper>

		<paper id="2159">
			<definition id="0">
				<sentence>A parallel corpus is an important resource for corpus-based approaches to CLIR .</sentence>
				<definiendum id="0">parallel corpus</definiendum>
				<definiens id="0">an important resource for corpus-based approaches to CLIR</definiens>
			</definition>
			<definition id="1">
				<sentence>In this way , an n-dimensional vector which represents the word 's distributional behavior is produced t'or each vocabulary word .</sentence>
				<definiendum id="0">n-dimensional vector</definiendum>
				<definiens id="0">represents the word 's distributional behavior is produced t'or each vocabulary word</definiens>
			</definition>
</paper>

		<paper id="2120">
			<definition id="0">
				<sentence>In spG , intonation contours are distinguished according to their di , f fcrcntial meanings , i.e. , they label pitch movements that are commonly interpreted by the speakers of ( British ) English as having quite different pragmatic purport ( cf. Teich et al. ( 1997 ) ) .</sentence>
				<definiendum id="0">f fcrcntial</definiendum>
				<definiens id="0">meanings , i.e. , they label pitch movements that are commonly interpreted by the speakers of ( British</definiens>
			</definition>
			<definition id="1">
				<sentence>TOBI , on the other hand , is a phonetic-phonological annotation scheme tbr intonation .</sentence>
				<definiendum id="0">TOBI</definiendum>
				<definiens id="0">a phonetic-phonological annotation scheme tbr intonation</definiens>
			</definition>
			<definition id="2">
				<sentence>The phrase tone Lis low pitch following the final pitch accent of a phrase .</sentence>
				<definiendum id="0">phrase tone Lis</definiendum>
				<definiens id="0">low pitch following the final pitch accent of a phrase</definiens>
			</definition>
			<definition id="3">
				<sentence>The Tonic represents the nuclear stress and is part of the tonic segment of the tone group .</sentence>
				<definiendum id="0">Tonic</definiendum>
				<definiens id="0">the nuclear stress and is part of the tonic segment of the tone group</definiens>
			</definition>
			<definition id="4">
				<sentence>The motivation behind this is to make the two systems collaborate in concept-to-speech generation : Tom is a phonetic-phonological approach to the deseription of intonation , SFG offers a linguistic approach to intonation , tbcusing on the meaningful intonation patterns .</sentence>
				<definiendum id="0">Tom</definiendum>
				<definiendum id="1">SFG</definiendum>
				<definiens id="0">a phonetic-phonological approach to the deseription of intonation ,</definiens>
			</definition>
			<definition id="5">
				<sentence>Emu : An enhanced hierarchical speech data management system .</sentence>
				<definiendum id="0">Emu</definiendum>
				<definiens id="0">An enhanced hierarchical speech data management system</definiens>
			</definition>
			<definition id="6">
				<sentence>ToBI : A standard tbr labelling English prosody .</sentence>
				<definiendum id="0">ToBI</definiendum>
				<definiens id="0">A standard tbr labelling English prosody</definiens>
			</definition>
</paper>

		<paper id="1066">
			<definition id="0">
				<sentence>The naive Bayes classifier is one of the statistical text classifiers that use word frequencies as features .</sentence>
				<definiendum id="0">naive Bayes classifier</definiendum>
				<definiens id="0">one of the statistical text classifiers that use word frequencies as features</definiens>
			</definition>
			<definition id="1">
				<sentence>ICF is computed as follows : ICF i -Iog ( M ) Iog ( CF i ) ( 2 ) ® where CF is tile number of categories that contain t ; , and M is tile total number of categories .</sentence>
				<definiendum id="0">ICF</definiendum>
				<definiendum id="1">CF</definiendum>
				<definiendum id="2">M</definiendum>
				<definiens id="0">tile number of categories that contain t</definiens>
				<definiens id="1">tile total number of categories</definiens>
			</definition>
			<definition id="2">
				<sentence>/ = ( 4 ) N where N is the total number of words in a sentence .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the total number of words in a sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>A woM W is assumed to have a certain affinity to every sentence , which 455 is a real number between 0 and 1 .</sentence>
				<definiendum id="0">woM W</definiendum>
				<definiens id="0">a real number between 0 and 1</definiens>
			</definition>
			<definition id="4">
				<sentence>In these formulae , W ~ S means that a word belongs to a sentence : aft , , ( W , S ) = max w , es sire , , ( W , W i ) aff , , ( S , W ) = max w~s ; sire , , ( S , S~ ) ( 5 ) ( 6 ) In the above formulae , n denotes the iteration number , and the similarity values are defined by WSM , , and SSM , , .</sentence>
				<definiendum id="0">W ~ S</definiendum>
				<definiendum id="1">S , S~ )</definiendum>
				<definiens id="0">means that a word belongs to a sentence : aft ,</definiens>
				<definiens id="1">the iteration number , and the similarity values</definiens>
			</definition>
			<definition id="5">
				<sentence>It is computed as follows ( Karov Y. et al. , 1999 ) : log pr ( w ; l w ) ( lO ) Pr ( Wi ) In ( 10 ) , Pr ( Wi ) is estimated from the frequency of Wi in the total sentences , and Pr ( WilW ) fi'om the frequency of Wi in representative sentences .</sentence>
				<definiendum id="0">l w )</definiendum>
				<definiens id="0">estimated from the frequency of Wi in the total sentences , and Pr ( WilW ) fi'om the frequency of Wi in representative sentences</definiens>
			</definition>
			<definition id="6">
				<sentence>To avoid poor estimation for words with a low count in representative sentences , we nmltiply the loglikelihood by ( 11 ) where count ( Wi ) is the number of occurrences of Wi in representative sentences .</sentence>
				<definiendum id="0">count ( Wi )</definiendum>
				<definiens id="0">the number of occurrences of Wi in representative sentences</definiens>
			</definition>
			<definition id="7">
				<sentence>456 The total weight of a word is the product of the above t'actors , each norlnalized by the sum of factors of the words in a sentence as follows ( Karov Y. et al. , 1999 ) : , &amp; ctor ( Wi , S ) weight £ J'actor ( Wi , S ) IVieS ( 12 ) In ( 12 ) , factor ( W , S ) is the weight before normalization .</sentence>
				<definiendum id="0">S ) IVieS</definiendum>
				<definiendum id="1">S )</definiendum>
				<definiens id="0">the product of the above t'actors , each norlnalized by the sum of factors of the words in a sentence as follows ( Karov Y. et al. , 1999 ) : , &amp; ctor ( Wi , S ) weight £ J'actor ( Wi ,</definiens>
				<definiens id="1">the weight before normalization</definiens>
			</definition>
			<definition id="8">
				<sentence>siul ( X Si ) } ( 14 ) ( : '~ ( `` I SjcRc , In ( 13 ) and ( 14 ) , i ) X is au unclassified sentence , ii ) C = { c l , c2 ... .. c , , , } is a category set , and iii ) R , , , = { &amp; , Sa ... ...</sentence>
				<definiendum id="0">}</definiendum>
				<definiens id="0">au unclassified sentence , ii ) C = { c l</definiens>
				<definiens id="1">a category set</definiens>
			</definition>
			<definition id="9">
				<sentence>Using the two-way contingency table of a word t and a category c i ) A is the number of times t and c co-occur , ii ) B is the number of times t occurs without c , iii ) C is the number of times c occurs without t , iv ) D is the number ot ' times ueither c nor t occurs , and vi ) N is the total number of sentences the word-goodness measure is defined as follows ( Yang Y. et al. , 1997 ) : Z2 ( t , c ) = N× ( AD-CB ) 2 ( 16 ) ( A + C ) ( B + D ) ( A + B ) ( C + D ) To measure the goodness of a word in a global feature selection , we combine the category-specific scores of a word as follows : III 9 2 ZF , , .</sentence>
				<definiendum id="0">category c i</definiendum>
				<definiendum id="1">C</definiendum>
				<definiendum id="2">) D</definiendum>
				<definiendum id="3">) N</definiendum>
				<definiens id="0">the number of times t and c co-occur , ii ) B is the number of times t occurs without c , iii )</definiens>
				<definiens id="1">the number of times c occurs without t , iv</definiens>
				<definiens id="2">the number ot ' times ueither c nor t occurs , and vi</definiens>
			</definition>
</paper>

		<paper id="2095">
			<definition id="0">
				<sentence>Sumo is a formalism for universal segmentation of text .</sentence>
				<definiendum id="0">Sumo</definiendum>
				<definiens id="0">a formalism for universal segmentation of text</definiens>
			</definition>
			<definition id="1">
				<sentence>Tokenizers designed for those languages are generally very tied to a given system and language .</sentence>
				<definiendum id="0">Tokenizers</definiendum>
				<definiens id="0">designed for those languages are generally very tied to a given system and language</definiens>
			</definition>
			<definition id="2">
				<sentence>657 As an exemple , the relation of figure 1 is described by the following expression : \ [ de le { d u } \ ] Identification is the process of identifying new items froln a source graph .</sentence>
				<definiendum id="0">Identification</definiendum>
				<definiens id="0">the process of identifying new items froln a source graph</definiens>
			</definition>
			<definition id="3">
				<sentence>Ambiguity is a central issue when talking about segmentation .</sentence>
				<definiendum id="0">Ambiguity</definiendum>
				<definiens id="0">a central issue when talking about segmentation</definiens>
			</definition>
			<definition id="4">
				<sentence>Transformations are a way to modify the item graphs so that the `` good '' paths ( segmentations ) can be kept and the `` bad '' ones discarded .</sentence>
				<definiendum id="0">Transformations</definiendum>
				<definiens id="0">a way to modify the item graphs so that the `` good '' paths ( segmentations ) can be kept and the `` bad '' ones discarded</definiens>
			</definition>
			<definition id="5">
				<sentence>a.ny set of attributes ) followed by any word $ w2 with any attribute ( `` _ '' being a context separator ) , create the item returned by the fimction f ( $ ul , $ u2 ) .</sentence>
				<definiendum id="0">fimction f</definiendum>
				<definiens id="0">_ '' being a context separator ) , create the item returned by the</definiens>
			</definition>
</paper>

		<paper id="1077">
			<definition id="0">
				<sentence>The T R U C K S approach identifies different ; elements of the context which are combined to form the Information Weight , a measure of how strongly related the context is to a candidate term .</sentence>
				<definiendum id="0">Information Weight</definiendum>
			</definition>
			<definition id="1">
				<sentence>The T R U C K S approach to t e r m recognition ( Term Recognition Using Combined Knowledge Sources ) focuses on identifying relevant contextual information from a variety of sources , in order to enhance traditional statistical techniques of t e r m recognition .</sentence>
				<definiendum id="0">T R U C K</definiendum>
				<definiens id="0">focuses on identifying relevant contextual information from a variety of sources , in order to enhance traditional statistical techniques of t e r m recognition</definiens>
			</definition>
			<definition id="2">
				<sentence>This is defined formally as : is not nested l~ ( 'n , ) ~b~T~f ( b ) ) a is nested where a is the candidate string , f ( a ) is its frequency in the corpus , eT , is the set of candidate terms that contain a , P ( Ta ) is the number of these candidate terms .</sentence>
				<definiendum id="0">P ( Ta )</definiendum>
				<definiens id="0">the set of candidate terms that contain a</definiens>
				<definiens id="1">the number of these candidate terms</definiens>
			</definition>
			<definition id="3">
				<sentence>The CT weight is formally described as follows : Contextual Information : a Term 's where a is the candidate term , 7 ' , is the set : of context terms of a , d is a word from Ta , fa ( d ) is the frequency of d as a context term of a. Semantic knowledge Semantic knowledge is obtained about context terms using the UMLS Metathesaurus and Semantic Network ( NLM , 1997 ) .</sentence>
				<definiendum id="0">Contextual Information</definiendum>
				<definiens id="0">the frequency of d as a context term of a. Semantic knowledge Semantic knowledge is obtained about context terms using the UMLS Metathesaurus</definiens>
			</definition>
</paper>

		<paper id="2137">
			<definition id="0">
				<sentence>\5/\ ] lell l , \ ] le lCsllll ; s are better with the new tcchni ( lUe , a question arises as t ( ) wh ( ' , l ; h ( ; r these l : ( `-sult ; ( litl'eren ( : es are due t ( ) the new technique a ( : t ; ually 1 ) eing l ) cl ; t ( 'x or just ; due 1 ; o ( : han ( : e. Unt'ortmmtely , one usually Callll ( ) t ) directly answer the qnesl ; ion `` what is the 1 ) robatfility that 1 ; 11 ( ; now l ; ( x : hni ( luC , is t ) el ; lx~r givell l ; he results on the t ( ' , sl , dal ; a sol ; '' : I ) ( new technique is better \ [ test set results ) \ ] ~ul ; with statistics , one cml answer the following proxy question : if the new technique was a ( &gt; tually no ditt'erent than the old t ( ' , ( 'hnique ( ( ; he * This paper reports on work l ) erfonncd at the MITR1 , ; Corporation under the SUl ) porl : of the MITIlJ , ; , qponsored Research l ) rogrmn .</sentence>
				<definiendum id="0">ion</definiendum>
				<definiens id="0">e. Unt'ortmmtely , one usually Callll ( ) t ) directly answer the qnesl ;</definiens>
				<definiens id="1">hni ( luC , is t ) el ; lx~r givell l ; he results on the t ( ' , sl , dal ; a sol</definiens>
			</definition>
			<definition id="1">
				<sentence>E\ [ d\ ] is the expected difference ( which is 0 under the null hypothesis ) and Sd is an estimate of the standard deviation of d. Standard deviation is the square root of the variance , a measure of how much a random variable is expected to vary .</sentence>
				<definiendum id="0">Sd</definiendum>
				<definiens id="0">the expected difference ( which is 0 under the null hypothesis</definiens>
				<definiens id="1">an estimate of the standard deviation of d. Standard deviation is the square root of the variance , a measure of how much a random variable is expected to vary</definiens>
			</definition>
			<definition id="2">
				<sentence>A complication of using equation 1 is that one usually does not have Sd , but only st and s2 , where Sl is the estimate for Xl 'S standard deviation and similarly for s2 .</sentence>
				<definiendum id="0">Sl</definiendum>
				<definiens id="0">the estimate for Xl 'S standard deviation and similarly for s2</definiens>
			</definition>
			<definition id="3">
				<sentence>Analogously , it turns out that 2 z S d 82 -t82 -2r128182 ( 2 ) where r12 is an estimate for P12 .</sentence>
				<definiendum id="0">r12</definiendum>
				<definiens id="0">an estimate for P12</definiens>
			</definition>
			<definition id="4">
				<sentence>Precision is the fraction of the items `` Ibund '' 1 ) y some technique that are actually of interest .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the fraction of the items `` Ibund '' 1 ) y some technique that are actually of interest</definiens>
			</definition>
			<definition id="5">
				<sentence>Precision = l~ , / ( I~ , + S ) , where R is the number of items that are of interest and m'e Recalled ( fbund ) by tile technique , and S is the munber of items that are found by tile technique that turn out to be Spurious ( not of interest ) .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiendum id="1">R</definiendum>
				<definiendum id="2">S</definiendum>
				<definiens id="0">the number of items that are of interest and m'e Recalled ( fbund ) by tile technique , and</definiens>
			</definition>
			<definition id="6">
				<sentence>Precision is a non-linear time ( ion of two random wu'iables R and S , so we did not try to estimate the correlation coefficient \ ] 'or precision .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">a non-linear time ( ion of two random wu'iables R and S , so we did not try to estimate the correlation coefficient \ ] 'or precision</definiens>
			</definition>
			<definition id="7">
				<sentence>/ ( 1~ + S ) , where I~ is i ; he nmnl ) er of iWms t ; lmt ; are of inl : eresl ; that ; are '/'c'called by a W , chnique mid S is l ; he mmfl ) er of it ; e , ms ( fi ) und 1 ) y s~ technique ) that ; are nol ; of interest ; .</sentence>
				<definiendum id="0">I~</definiendum>
				<definiens id="0">ms ( fi ) und 1 ) y s~ technique ) that</definiens>
			</definition>
			<definition id="8">
				<sentence>Sinfilar is the concept of colloca , t ; ion , where the prolml ) ility of a lexeme 's al &gt; l ) earance is influenced by the .</sentence>
				<definiendum id="0">Sinfilar</definiendum>
				<definiens id="0">the concept of colloca , t ; ion , where the prolml ) ility of a lexeme 's al &gt; l</definiens>
			</definition>
</paper>

		<paper id="2172">
			<definition id="0">
				<sentence>As Mrea ( ty mentioned , ; ~ l~tngmtge modal for th ( ; tnrget l~mguag ( ; has to bc integrated into t ; h ( , scoring of the translation hyl ) othes ( , s. Fimflly , the l , rmmdu ( 'er based al ) t ) roadl to translation will 1 ) e tested on word lattice .</sentence>
				<definiendum id="0">tnrget l~mguag</definiendum>
				<definiens id="0">to bc integrated into t ; h ( , scoring of the translation hyl</definiens>
				<definiens id="1">'er based al ) t ) roadl to translation will 1 ) e tested on word lattice</definiens>
			</definition>
</paper>

		<paper id="2162">
			<definition id="0">
				<sentence>Pr ( f~ lel ) over e I l Target Language Text l J I ~ Lexicon Model Pr ( l 1 \ ] e , ) \ [ Alignment Model \ ] Language Model Figure 1. : Architecture of the translation 31 ) preach based on Bwes ' decision rule .</sentence>
				<definiendum id="0">Pr</definiendum>
				<definiens id="0">l 1 \ ] e , ) \ [ Alignment Model \ ] Language Model Figure 1. : Architecture of the translation 31 ) preach based on Bwes ' decision rule</definiens>
			</definition>
			<definition id="1">
				<sentence>Sortie verbs in German consist of a main part and a detachable prefix which can be shifted to the end of the clause , e.g. `` losfahren '' ( `` to leave '' ) in the sentence `` Ich fahre morgen los . ''</sentence>
				<definiendum id="0">Sortie</definiendum>
				<definiens id="0">verbs in German consist of a main part and a detachable prefix which can be shifted to the end of the clause</definiens>
			</definition>
			<definition id="2">
				<sentence>Th ( , VEI { BM ( ) BII , corpus consists of st ) onttmeously spoken dialogs in t ; he al ) t ) oint ; ment sch ( &gt; ( hfling domain ( Wtflflster , 1993 ) .</sentence>
				<definiendum id="0">Th</definiendum>
				<definiendum id="1">VEI { BM ( ) BII ,</definiendum>
			</definition>
			<definition id="3">
				<sentence>A DP based Search Algorithm tbr Statistical Machine Translation .</sentence>
				<definiendum id="0">DP</definiendum>
			</definition>
</paper>

		<paper id="2127">
			<definition id="0">
				<sentence>Then , the relation score is calculated as follows : Score = Srel * ( Wl*S1 + W2 '' S2 ) Here , SI and $ 2 are tile scores of the two words connected by relations .</sentence>
				<definiendum id="0">relation score</definiendum>
				<definiens id="0">tile scores of the two words connected by relations</definiens>
			</definition>
			<definition id="1">
				<sentence>Generation o.f sur~we phrases Tiffs process produces I ) AGs each of ~laich consists of one core relation and several attached iclations .</sentence>
				<definiendum id="0">Generation o.f sur~we</definiendum>
				<definiens id="0">phrases Tiffs process produces I ) AGs each of ~laich consists of one core relation and several attached iclations</definiens>
			</definition>
			<definition id="2">
				<sentence>VCOMP ) ' \ [ 1\ ] \ [ PRED `` PICORP '' \ ] PRED 'license ( i '' SUB J ) ( \ [ OBJ ) ( t TO OBJ ) ' SUBJ \ [ 11 OBJ \ [ PRED 'environment protection technology'\ ] TO E PP TO 1 OBJ \ [ PRED 'AMICO ' \ ] 2 $ SUBJ \ [ PRED `` PICORP '' \ ] OBJ \ [ PRED 'environment protection technology'\ ] TO ~ PP 10 7\ ] OBJ \ [ PRED 'AMICO ' \ ] PICORP licenses erlvironment protection technology to AMICe .</sentence>
				<definiendum id="0">OBJ</definiendum>
				<definiens id="0">] \ [ PRED `` PICORP '' \ ] PRED 'license ( i '' SUB J ) ( \ [ OBJ ) ( t TO OBJ ) ' SUBJ</definiens>
			</definition>
			<definition id="3">
				<sentence>Kaplan , R. M. and Bresnan , J. ( 1982 ) : `` 'LexicalFunctional Grammar : A Forlnal System for Grammatical Representation , '' in Bresnan , J. ( ed . )</sentence>
				<definiendum id="0">Grammar</definiendum>
				<definiens id="0">A Forlnal System for Grammatical Representation , '' in Bresnan</definiens>
			</definition>
</paper>

		<paper id="2093">
			<definition id="0">
				<sentence>uT'e ( TS ) , in which the material in the RS is distributed among I ) aragraphs , sentences , vertical lists , etc. , perhaps linked up by discourse connectives such as 'since ' and 'however ' .</sentence>
				<definiendum id="0">uT'e ( TS</definiendum>
				<definiens id="0">in which the material in the RS is distributed among I ) aragraphs , sentences , vertical lists , etc. , perhaps linked up by discourse connectives such as 'since ' and 'however '</definiens>
			</definition>
			<definition id="1">
				<sentence>By a 'candi ( late ' we mean a solution that correctly realizes the RS without violating text-structure formation rules ; it may nevertheless be stylistically inel ) t. Having generated a set of candidate text ; structures , the ICONO ( ILAST system evaluates them through rules that detect stylistic tlaws , and on this basis re : ranges them in an order of preferen ( : e. We will discuss stylistic evaluation brMly , but the focus of the paper is the 1 ) roblem of enum ( ; rating solutions .</sentence>
				<definiendum id="0">ICONO</definiendum>
				<definiens id="0">ILAST system evaluates them through rules that detect stylistic tlaws</definiens>
			</definition>
			<definition id="2">
				<sentence>N , where N is the number of sisters .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of sisters</definiens>
			</definition>
			<definition id="3">
				<sentence>~Elixir is banned by the FDA , consequently it contains gestodene .</sentence>
				<definiendum id="0">~Elixir</definiendum>
				<definiens id="0">banned by the FDA , consequently it contains gestodene</definiens>
			</definition>
</paper>

		<paper id="1068">
			<definition id="0">
				<sentence>Even when successful interpretation is not obtained o51 content-word level , the system generates system-initiative guidances based on the semantic-attribute level , which lead the next user 's utterance to successful interpretation .</sentence>
				<definiendum id="0">semantic-attribute level</definiendum>
				<definiens id="0">lead the next user 's utterance to successful interpretation</definiens>
			</definition>
			<definition id="1">
				<sentence>Next , they are transtbrnmd from log-scaled value ( &lt; t. scaledi ) to probability dimension by taking its exponential , and calculate a posteriori probability tbr each i-th candidate ( Bouwman et al. , 1999 ) . e~.scaledi Pi = ~n Co~.scaledj j=l This Pi represents a posteriori probability of the i-th sentence hypothesis. Then , we compute a posteriori probability tbr a word. If the i-th sentence contains a word w , let 5w , i = 1 , and 0 otherwise. A posteriori probability that a word w is contained ( Pw ) is derived as summation of a posteriori prob~bilities of sentences that contain the word. /L Pw = ~ Pi `` 5w , i i=1 We define this Pw as the content-word CM ( CM , , ) . This CM. , , is calculated tbr every content word. Intuitively , words that appear many times in N-best hypotheses get high CMs , and frequently substituted ones in N-best hypotheses are judged as mn'eliable. In Figure 1 , we show an example in CMw calculation with recognizer outputs ( i-th recognized candidates and their a posteriori probabilities ) tbr an utterance `` Futaishisctsu ni rcsutoran no aru yado ( Tell me hotels with restaurant facility. ) '' . It can be observed that a correct content word 'restaurant as facility ' gets a high CM value ( CMw = 1 ) . The others , which are incorrectly recognized , get low CMs , and shall be rejected. A concept category is semantic attribute assigned to content words , and it is identified by parsing with phrase-level gramnmrs that are used in speech recognition process and represented with Finite State Automata ( FSA ) . Since 468 1 2 3 4 5 6 7 8 9 10 Recognition candidates aa shisetsu ni resutmnu , no kayacho with restaurant facility / Kayacho ( location ) aa shisetsu ni rcsuto7nn no katsurn no with restaurant fimility / Katsura ( location ) aa shisctsu ni resutoran no kamigamo with restaurant facility / Kmnigamo ( location ) &lt; g &gt; shisctsu ni with restaurant &lt; g &gt; shisetsu ni with restaurant &lt; g &gt; shisetsu ni resutoran no kayacho facility / Kayacho ( location ) rcsutor'a~t 7to kat .</sentence>
				<definiendum id="0">i-th candidate</definiendum>
				<definiens id="0">a posteriori probability of the i-th sentence hypothesis. Then , we compute a posteriori probability tbr a word. If the i-th sentence contains a word w</definiens>
				<definiens id="1">the content-word CM ( CM , , ) . This CM. , , is calculated tbr every content word. Intuitively , words that appear many times in N-best hypotheses get high CMs , and frequently substituted ones in N-best hypotheses</definiens>
				<definiens id="2">semantic attribute assigned to content words , and it is identified by parsing with phrase-level gramnmrs that are used in speech recognition process</definiens>
			</definition>
</paper>

		<paper id="2098">
			<definition id="0">
				<sentence>Verbmobil ( Wahlster , 1993 ) is a spontaneous speech-to-speech translation system and translates spoken German to English/Japanese and vice versa .</sentence>
				<definiendum id="0">Verbmobil</definiendum>
				<definiens id="0">a spontaneous speech-to-speech translation system and translates spoken German to English/Japanese and vice versa</definiens>
			</definition>
			<definition id="1">
				<sentence>Our probabilistic parser is a shift-reduce parser and uses an A*-search to find the best scored path in the lattice that can be parsed by its context fi'ee grammar .</sentence>
				<definiendum id="0">probabilistic parser</definiendum>
				<definiens id="0">a shift-reduce parser and uses an A*-search to find the best scored path in the</definiens>
			</definition>
			<definition id="2">
				<sentence>Then P ( WIT ) is nothing more than P ( ~L ) , where L is the part-of-speech tag sequence for a given utterance W. If our goal is to select the best analysis T for a given tag sequence L we do not necessarily depend on a good approximation of P ( T ) , but simply select the best analysis for a given L by finding a T that maximizes P ( TIL ) ( and not P ( 7 ) ) .</sentence>
				<definiendum id="0">L</definiendum>
			</definition>
			<definition id="3">
				<sentence>the exact match rates are quite low thus we have learned transformations from the training data to improve the output of the German and English parser ( there was not enough training data to do so for Japanese ) and evaluated the results shown in the following table ( TT is an abbreviation for Tree Translfbrmations ) .</sentence>
				<definiendum id="0">TT</definiendum>
			</definition>
			<definition id="4">
				<sentence>German English Exact Match ( w/o TT ) 46,3 % 55,4 % hlcorrect parses 50,3 % 39,3 % Not parsed 3,4 % 5,3 % Exact Match ( after 77 ) 53,8 % 61,2 % Incorrect parses ( after TT ) 42,8 % 33,5 % Labeled Precision ( w/o 7T ) 90,2 % 90,6 % German English Labeled Precision ( after TT ) 90,8 % 91,4 % Labeled Recall ( all 83,5 % 78,5 % utterances , w/o TT ) Labeled Recall ( all 84,0 % 79,2 % utterances , after TT ) Labeled Recall ( parsed 91,0 % 90,9 % utterances , w/o TT ) Labeled Recall ( parsed 91,6 % 91,7 % utterances , after TT ) In this article we have extended probabilistic shiftreduce parsing to be more context-sensitive than previous works and have demonstrated that a bigger context improves the performance of a probabilistic shift-reduce parser .</sentence>
				<definiendum id="0">w/o TT ) Labeled Recall</definiendum>
				<definiens id="0">after TT ) Labeled Recall ( parsed 91,0 % 90,9 % utterances , w/o TT ) Labeled Recall ( parsed 91,6 % 91,7 % utterances</definiens>
			</definition>
</paper>

		<paper id="2154">
			<definition id="0">
				<sentence>l { eal-time communication over the hiternet has more properly been the ( lomain of ' &lt; chat '' l ) rotoeels : primarily Interact Relay Chat ( 11 { ( 3 ) ( Oikarinen and Reed , 1993 ) , and similar instant messaging protocols developed commercially ( America Online Inc. , 2000 ; Microsoft Corp. , 2000 ; ICQ Inc. , 1999 ) . While some portals have been developed to permit access to chat using the Web ( iTRiBE lnc. , 1996 ) , the primary point of access seems to be chat-specific client software. Although chat defines protocols and provides infrastructure , it is limited ill the kind of data that it can transl ) ort , and client software is tightly focussed oil the text domain. Such limitations have not , however , prevented researchers fi'om exl ) erilnenting with the possibilities of incorporating machine translation or speech into tile chat experience ( 1 , enzo , 1998 ; Seligma.n et al. , 1998 ) . The outcome of these experiments has been to show that comn-mrcial machine translation systems may 1 ) e reasonably integrated into the chat room , and that commercial speech software ca.n be connected to existing chat software to provide the desired experience. We have taken a difl~rent road. It has been noted ( Seligman , 19. ( . ) 7 ; l '' rederking et al. , 2000 ) that broadcoverage machine translation and speech recognition can not now be usefld mdess users can interact with the system to improve results. While Seligman et al. ( 1998 ) were able to etDct user editing of speech recognition by editing text before submitting it for translation , they were unable to do the same for tile translation system , prilnarily due to limitations of commercial software. Additional limitations are encountered in the communication medium : chat is not amenable to non-text interaction with translation agents , and commercial chat software does not , in any case , support such interaction. To deal with these limitations , we have developed a fully interactive , Web-based , chat-style tra.nslation system , supporting sl ) eech recognition and synthesis , local-or third-1 ) arty correction of speech reeognitioi , and machine translation , and online learning , which ca.n be used with nothing lllore than a Well browser and some simple add-ons. All intensive processing , including translation and speech recognition is performed a.t central servers , permitting access for those with limited computational resources. In a.ddition , tile modular design of t.he system and interface permit computa.tional tasks to be easily distributed and different dialog configurations to be explored. The design of the Webl ) IPLOMAT system is intended to facilitate the following kind of interaction : ( numbers correspond to Figure 1 ) played in an editing window , where it may be edited by respeaking or using the keyboard. mitted tbr translation and transfer to the other 1041 ' , ... ... .. .I ; 5 ... . ... ... .. I ' -- v ) Figure 1 : User-level perspective on information flow. See text for explanation of labels. l ) arty. a human expert , who is able to translate , correct and teach the system a correct translation. tance by the expert , a translation is delivered to the other pa.rty and synthesized. tomatically for all users , and displayed on their interfaces. Although the above is the original vision for tihe system , other configurations are easily imagined. Configurations with more than two participants , or where one of the users is also simultaneously all expert are stra.ightforwardly handled. Internationalization of the interfaces , for use in different locales , is also easily handled. Many changes of this nature are handled by easy modifications to the HTMI , code for given \¥eb pages. More COml ) licated tasks may be accomplished by modifications of underlying code. In order to produce the above configuration , the current system implements two user interthces ( UIs ) : the Client UI , which provides speech and text input capabilities to the primary end-users of the system ; and the Editor UI , which provides translation editing capabilities to a human translation expert , in the rest of this section , we describe in detail certain unique aspects of each interface. In addition to speech-input and editing capabilities , the Client UI is able to track the entire dialog as it progresses. Because the Central Communications Server ( @ ~a.l ) forwards every message to all connected clients , every component of the system can be aware of how the dialog turn is proceeding. Ill tile Client UI , this capability is used to l ) rovide a running transcript of the conversation as it occurs. By noting the identifiers on messages ( cf. ~,3.4 ) , the U1 can assign appropriate labels to each of the following : our original utterance , translation of our utterance , other person 's utterance , translation of their utterance. In ~ddition , we use knowledge about the status of the dialog to prevent the user from sending several utterances belbre the other party has responded. The F , ditor UI provides tools which make it possible for a human expert to edit translations produced by the machine translator betbre they are sent to the users. As mentioned earlier , the editing step is optional , and is intended to improve the quality of transla.tions. The Editor UI may be configured so that either of the two users , or a remote third party can act as editor. Onr motivations for providing an editing capability are twofold : • Although our MT system ( @ ~3.2 ) dots not always produce the correct answer , the correct answer is usually available a.mong the possibilities it. considers. t.al Q • , H~ MT system provides for online updates of its knowledge base which a.llows tbr translations to improve over time. In order to take advantage of ' these capabilities , we have designed two editing tools , the chart editor and a.lways-active learning , that enable a human expert to rapidly produce an accurate tlJaillslatioll aud to store tha.t translation in the MT knowledge base for future use. As discussed in ~a.2 , our MT system ma.y produce more than one translation for each part of tile input , from which it attempts to se\ ] ect the best translation. The entire set of translations is available to the WebI ) IPLOMAT system , and ix used in the cha.rt editor. By double-clicking on words in the translation , the Original English My name is John ... ... . Edited Frencll l inen nora estJehn Figure 2 : Popup Chart Editor 1042 human edit ( ) \ ] : is l ) resented a. pol ) Ul ) -menu of alterna.tire tra.nslations beginning a.t a particular location in the sentence ( see l ? igure 2 ) . When one o\ [ ' the alternatives is sek ; cted , it replaces the original word or words. In this way , a. sentence may be rapidly edited to an acceptable sta.te. In order to reduce develolmmnt \ ] line , our MT system can be used in a ra.pid-del ) loylnent style : afl ; er a. minimal knowledge base is constructed , the system is put into use with a huma.n expert supervising , so that domain-rel ( : va.nt data ma.y be elicited ( lui ( : ldy. In order to supl ) ort this , all uttera.nces a.re considered for learning. When the editor presses the 'Acccitt/Learn ' l ) utton , the original utterance and its tra.nslatiotl are exa.ntined to determine if they are suital ) le for learning. ( Turrently all utterances for which the forward tra.nslation has 1teen edited are su brat\ ] ted \ [ 'or learning , a.lthough other criteria ma.y also be entertained. More detail about online lea.r| &gt; ing may 1 ) e found ill ~3.2 .</sentence>
				<definiendum id="0">online learning</definiendum>
				<definiendum id="1">Client UI</definiendum>
				<definiendum id="2">Editor UI</definiendum>
				<definiendum id="3">Client UI</definiendum>
				<definiens id="0">tools which make it possible for a human expert to edit translations produced by the machine translator betbre they are sent to the users. As mentioned earlier</definiens>
			</definition>
			<definition id="1">
				<sentence>tion technok ) gy , and produ ( : es multit ) 1 % possibly overlal ) ping , l.ra\ ] mlations for every part of tit ( ; inl ) ut that it can translate .</sentence>
				<definiendum id="0">produ</definiendum>
				<definiens id="0">mlations for every part of tit ( ; inl ) ut that it can translate</definiens>
			</definition>
			<definition id="2">
				<sentence>Lexical Transfer uses bilingual dictionaries and phrasal glossaries to provide phrase-for-phrase translations , while EBMT uses a fllzzy matching step to produce translations froln a bilingual corpus of matched sentence pairs .</sentence>
				<definiendum id="0">EBMT</definiendum>
			</definition>
			<definition id="3">
				<sentence>As mentioned in §2.2 , the Editor UI attempts to learn from utterances that have been edited .</sentence>
				<definiendum id="0">Editor UI</definiendum>
			</definition>
</paper>

		<paper id="2144">
			<definition id="0">
				<sentence>Thistle is a parametcrizable diagram editor .</sentence>
				<definiendum id="0">Thistle</definiendum>
				<definiens id="0">a parametcrizable diagram editor</definiens>
			</definition>
			<definition id="1">
				<sentence>Second , the demote operation is the exact analog of adjunction in Tree Adjoining Grammars ( see e.g. Joshi et al , 1991 ) .</sentence>
				<definiendum id="0">demote operation</definiendum>
			</definition>
</paper>

		<paper id="2165">
			<definition id="0">
				<sentence>This situation causes as Inuch labor-intensive work as in manual ta &lt; + &lt; qlw In this paper , we propose a semi-automatic tagging method that can reduce the human labor and guarantee the consistent tagging. 2o System Requivemer~ts To develop ari efficient tool that attempts to build a large accurately armotated corpus with minimal human labor~ we must consider the following requirements : ® In order to minimize human labor , the same human intervention to tag and to correct the same word in tile same context should not be repeated. * There may be a word which was tagged inconsistently in the same context becatlse it was tagged by different human experts or at a different task time. As an elticient tool , it can prevent tile inconsistency of tile annotated ( I results and ~uarantec the consistency of the annotated results. * It must provide an effective annotating capability lbr many unknown words in the whole corpus. 1096 The proposed POG tagging tool is used to combine the manual tagging method and the automatic tagging method. They are integrated to increase the accuracy o\ [ `` the automatic tagging method and to minimize the amount of tile human labor of thc manual tagging method. Figure 1 shows the overall architecture of the proposed tagging tool : KCAT. I ... ... ... I I I ~ I P I Raw ( ..rpus ILI Post-FnJcess ~t Ire-lrocess ( 'cJrrect an ~ ... ... ... .. : ~ ; ... . , R s `` `` ... ... ... . -- 7 -- ~ i -- g ~____~ `` : . i ~'f : : : 2 aa ' : ii , :n~ ... ... Figure 1. System Architecture of KCAT As shown in figm'e 1 , KCAT consists of three modules : the pre-processing module , the automatic tagging module , and the post-processing module. In the prcoprocessing module , the disambiguation rules are acquired I % m human experts. The candidate words are Ihe target words whose disambiguation rules are acquired. The candidate words can be unknown words and also very frequent words. In addition , the words with problematic ambiguity for tlle automatic tagger can become candidates. l ) lsamblguation rules are acquired with minimal human labor using tile tool t : n'oposed in ( Lee , 1996 ) . In the automatic tagging naodule , the disambiguation rules resolve the ambiguity of { , 'very word to which they can be applied. I lowever , tile rules are certainly not sufficient to resolve all the ambiguity of the whole words in file corpus. The proper tags are assigned to the remaining ambiguous words by a stochastic &lt; t~ '' c , hLllllan lagger. After the automatic t , t~m~ , a expert corrects tile onors o\ [ the stochastic ta , me , The system presents the expert with the results of the stochastic tagger. If the result is incorrect , tile hulllan expel1 corrects the error and generates a disambiguation rule ~br the word. The rule is also saved in the role base in order to bc used later. There are many ambiguous words that are extremely difficult to resolve alnbiguities by using a stochastic tagger. Due to the problematic words , manual tagging and manual correction must be done to build a correct coqms. Such human intervention may be repeated again and again to tag or to correct tile same word in the same context. For example , a human expert should assign 'Nal ( flying ) /Verb+Neun/Ending ' to every 'NaNemf repeatedly in the following sentences : `` Keu-Nyeo-Neun Ha-Neul-Eul Na-Neun Pi-Haeng-Ki-Reul Port Ceok-i Iss-Ta. '' ( she has seen a flying plane ) `` Keu-Netm lht-Nc'ul-Eul NaoNeun t'i-Itaeng -- Ki-Reul Port Ceok-i Eops-Ta. '' ( he has never seen a flying phme ) `` Keu-Netm tta-Ne , tl-Eul Na-Neun Pi -- ttaeng -- Ki-Reul Pal-Myeong-tlaessTa. '' ( he invented a flying plane ) In the above sentences , human experts can resolve the word , 'Na-Nemf with only the previous and ttle next lexical information : 'fla-Neul-Eul ' and 'Pi-tlaengKi-Reul'. In other words , tile human expert has to waste time on tagging the same word in tile same context repeatedly. This inefficiency can also be happened in the manual correction of the ntis-tagged words. So , if the human expert can make a rule with his disambiguation knowledge and use it for tile same words in tile same context , such inefficiency can be minimized. We define the disambiguation rule as a lexical rule. Its template is as follows. \ [ P : N\ ] \ [ Current Word\ ] \ [ Context\ ] = \ [ Tagging P , esuh\ ] Context • Previous words°p * Next Words° , , Ill tile above template , p and n mean tile previous and the next context size respectively. For the present , p and n are limited to 3. '* ' 1097 represents the separating mark between the previous and next context. For example , tile rule \ [ 1:1\ ] \ [ Na- , '\ : lten\ ] \ [ Ha-Neul-Eld * Pi-Haeng-KiReul\ ] = \ [ Na/ ( flying ) /Verb iNeun/Ending \ ] says the tag 'Nal ( flying ) /Verb + Neun/Ending ' should be assigned to the word 'Na-Neun ' when the previous word and the next word is 'Ha-Neul-Eul ' and 'Pi-Haeng-Ki-Reul'. Although these lexical rules can not always correctly disambiguate all Korean words , they are enough to cover many problematic ambignous words. We can gain some advantages of using the lexical rule. First , it is very accurate because it refers to the very specific lexical information. Second , the possibility of rule conflict is very little even though the number of the rules is increased. Third , it can resolve problematic ambiguity that can not be resolved without semantic inf'onnation ( Lim , 1996 ) . Lexical rules are acquired for the unknown words and the problematic words that are likely to be tagged erroneously by an automatic tagger. Lexical rule acquisition is perlbrmed by following steps : words li ) r which the lexical rules would be acquired. The candidate list is the collection of all examples of unknown words and problematic words for an automatic tagger. list and makes a lexical rule for the word. examples of the selected word with same context and also saves the lexical rule in the rule base. examples of the candidate words can be tagged by the acquired lexical rules. In the automatic ta , ,~dn-oo ~ phase , words are disambiguated by using the lexical rules and a stochastic tagger. To armotate a word in a raw corpus , the rule-based tagger first searches the lexical rule base to find a lexical rule that can be nlatched with tile given context. If a matching rnle is found , the system assigns the result of the rule to the word. According to the corresponding rule , a proper tag is assigned to a word. With tile lexical rules~ a very precise tag can be assigned to a word. However , because the lexical rules do not resolve all the ambiguity of the whole corpus , we must make use of a stochastic tagger. We employ an HMM -- based POS tagger for this purpose ( Kim,1998 ) . The stochastic tagger assigns the proper tags to the ambiguous words afier the rule application. Alter disambiguating the raw corpus using the lexical rules and the atttomatic tagger , we arrive at the frilly disambiguated result. But the word tagged by the stochastic tagger may have a chance to be mis-tagged. Therefore , the post-processing for error correction is required for the words tagged by the stochastic tagger. The human expert carries out the error correction task for the words tagged by a stochastic tagger. This error correction also requires tile repeatecl human labor as in the manual tagging. We employ the similar way of the rule acquisition to reduce the human labor needed for manual error cmTection. The results of the automatic tagger are marked to be distinguished from tile results of the rule-based tagger. The human expert checks the marked words only. If an error is found , the ht/man expert assigns a correct tag to the word. When tile expert corrects the erroneous word , tile system automatically generates a lexicat rule and stores it in tile rnle base. File newly acquired rule is autoinatically applied to the rest of tile corpus. Thus , the expert does not need to correct the repeated errors. 1098 B ... ... .. A ... J : ~ , £ `` ~ ; ~ ~'J ~'Y , I . : 'l~ll , k ! G'~ ( ~ ~ : ) ' : 'fl , q ! ! ! ~l ) ) L '' , l ' ) l , q ' ; '. % ll.q ! ll~ ~. `` ? 1 ) ~ : d~ : ' : } 'k L ' ~i~ tl ? r31 ,31 ? 2 : ~ '2'. ' : ~ , : , i\ [ .~ YZ. `` ± ! : 'J q l : '112j X , '' ~ ? t ) I - @ ~ ! ? I `` .'.20 t ~'~tJ 2 : .c Ul I '3t3b ! : ! I ~ : ~ '~ ( IM { 3tl *,1 N ~ :31 , 'q ~£i : :i ; ' £ , ,3 ~ : ~ ~J g~ `` JH G r.NwO } .lx* I '¢v¢et~a5 : ! ~31 W~'gffll Y'~a ! tlt~0l adlTll ' , lLr'9~. r , ~ wU , t.l : &lt; t E : '¢t : S eft ~1 : '' E ( ' ; '.hTF ~ I '~ t , iH , '.t CH.q ' , ~'~ NcrsicTc~ , :'I~L.IOF ' , I~qEA ~x~.'qlOL } ~M~ ? - '' -- ' ? II~ `` ~. =i~I 'q : '' Gt~i ! ~ } 5 '' 3~d~/t , \ ] HP*~ : } IJ'2 J , kt21 ~t X } el/tlt'l ! 3 • N , ,'JF O : '' h ! T ; UIHOII ! , ~T¢~I'I/f'g'dG.OII/JC : . *. ; 9 E , ~ ; . , V~. , *°.L.'EP* /EFGF ~.~n ~ 7 ; /I , iN p. ~ , ,d X : , ~la~ At ~I '' ~INhII\ ] -.Lt/JFB &gt; @ ~ ( ~ , ~ .</sentence>
				<definiendum id="0">KCAT</definiendum>
				<definiens id="0">a semi-automatic tagging method that can reduce the human labor and guarantee the consistent tagging. 2o System Requivemer~ts To develop ari efficient tool that attempts to build a large accurately armotated corpus with minimal human labor~ we must consider the following requirements : ® In order to minimize human labor , the same human intervention to tag and to correct the same word in tile same context should not be repeated. * There may be a word which was tagged inconsistently in the same context becatlse it was tagged by different human experts or at a different task time. As an elticient tool , it can prevent tile inconsistency of tile annotated ( I results and ~uarantec the consistency of the annotated results. * It must provide an effective annotating capability lbr many unknown words in the whole corpus. 1096 The proposed POG tagging tool is used to combine the manual tagging method and the automatic tagging method. They are integrated to increase the accuracy o\ [ `` the automatic tagging method and to minimize the amount of tile human labor of thc manual tagging method. Figure 1 shows the overall architecture of the proposed tagging tool : KCAT. I ... ... ... I I I ~ I P I Raw ( ..rpus ILI Post-FnJcess ~t Ire-lrocess ( 'cJrrect an ~ ... ... ... .. : ~ ; ... . , R s `` `` ... ... ...</definiens>
				<definiens id="1">consists of three modules : the pre-processing module , the automatic tagging module , and the post-processing module. In the prcoprocessing module , the disambiguation rules are acquired I % m human experts. The candidate words are Ihe target words whose disambiguation rules are acquired. The candidate words can be unknown words and also very frequent words. In addition , the words with problematic ambiguity for tlle automatic tagger can become candidates. l ) lsamblguation rules are acquired with minimal human labor using tile tool t : n'oposed in ( Lee , 1996 ) . In the automatic tagging naodule , the disambiguation rules resolve the ambiguity of { , 'very word to which they can be applied. I lowever , tile rules are certainly not sufficient to resolve all the ambiguity of the whole words in file corpus. The proper tags are assigned to the remaining ambiguous words by a stochastic &lt; t~ '' c , hLllllan lagger. After the automatic t , t~m~ , a expert corrects tile onors o\ [ the stochastic ta , me , The system presents the expert with the results of the stochastic tagger. If the result is incorrect , tile hulllan expel1 corrects the error and generates a disambiguation rule ~br the word. The rule is also saved in the role base in order to bc used later. There are many ambiguous words that are extremely difficult to resolve alnbiguities by using a stochastic tagger. Due to the problematic words , manual tagging and manual correction must be done to build a correct coqms. Such human intervention may be repeated again and again to tag or to correct tile same word in the same context. For example , a human expert should assign 'Nal ( flying ) /Verb+Neun/Ending ' to every 'NaNemf repeatedly in the following sentences : `` Keu-Nyeo-Neun Ha-Neul-Eul Na-Neun Pi-Haeng-Ki-Reul Port Ceok-i Iss-Ta. '' ( she has seen a flying plane ) `` Keu-Netm lht-Nc'ul-Eul NaoNeun t'i-Itaeng -- Ki-Reul Port Ceok-i Eops-Ta. '' ( he has never seen a flying phme ) `` Keu-Netm tta-Ne , tl-Eul Na-Neun Pi -- ttaeng -- Ki-Reul Pal-Myeong-tlaessTa. '' ( he invented a flying plane ) In the above sentences , human experts can resolve the word , 'Na-Nemf with only the previous and ttle next lexical information : 'fla-Neul-Eul ' and 'Pi-tlaengKi-Reul'. In other words , tile human expert has to waste time on tagging the same word in tile same context repeatedly. This inefficiency can also be happened in the manual correction of the ntis-tagged words. So , if the human expert can make a rule with his disambiguation knowledge and use it for tile same words in tile same context , such inefficiency can be minimized. We define the disambiguation rule as a lexical rule. Its template is as follows. \ [ P : N\ ] \ [ Current Word\ ] \ [ Context\ ] = \ [ Tagging P , esuh\ ] Context • Previous words°p * Next Words° , , Ill tile above template , p and n mean tile previous and the next context size respectively. For the present , p and n are limited to 3. '* ' 1097 represents the separating mark between the previous and next context. For example , tile rule \ [ 1:1\ ] \ [ Na- , '\ : lten\ ] \ [ Ha-Neul-Eld * Pi-Haeng-KiReul\ ] = \ [ Na/ ( flying ) /Verb iNeun/Ending \ ] says the tag 'Nal ( flying ) /Verb + Neun/Ending ' should be assigned to the word 'Na-Neun ' when the previous word and the next word is 'Ha-Neul-Eul ' and 'Pi-Haeng-Ki-Reul'. Although these lexical rules can not always correctly disambiguate all Korean words , they are enough to cover many problematic ambignous words. We can gain some advantages of using the lexical rule. First , it is very accurate because it refers to the very specific lexical information. Second , the possibility of rule conflict is very little even though the number of the rules is increased. Third , it can resolve problematic ambiguity that can not be resolved without semantic inf'onnation ( Lim , 1996 ) . Lexical rules are acquired for the unknown words and the problematic words that are likely to be tagged erroneously by an automatic tagger. Lexical rule acquisition is perlbrmed by following steps : words li ) r which the lexical rules would be acquired. The candidate list is the collection of all examples of unknown words and problematic words for an automatic tagger. list and makes a lexical rule for the word. examples of the selected word with same context and also saves the lexical rule in the rule base. examples of the candidate words can be tagged by the acquired lexical rules. In the automatic ta , ,~dn-oo ~ phase , words are disambiguated by using the lexical rules and a stochastic tagger. To armotate a word in a raw corpus , the rule-based tagger first searches the lexical rule base to find a lexical rule that can be nlatched with tile given context. If a matching rnle is found , the system assigns the result of the rule to the word. According to the corresponding rule , a proper tag is assigned to a word. With tile lexical rules~ a very precise tag can be assigned to a word. However , because the lexical rules do not resolve all the ambiguity of the whole corpus , we must make use of a stochastic tagger. We employ an HMM -- based POS tagger for this purpose ( Kim,1998 ) . The stochastic tagger assigns the proper tags to the ambiguous words afier the rule application. Alter disambiguating the raw corpus using the lexical rules and the atttomatic tagger , we arrive at the frilly disambiguated result. But the word tagged by the stochastic tagger may have a chance to be mis-tagged. Therefore , the post-processing for error correction is required for the words tagged by the stochastic tagger. The human expert carries out the error correction task for the words tagged by a stochastic tagger. This error correction also requires tile repeatecl human labor as in the manual tagging. We employ the similar way of the rule acquisition to reduce the human labor needed for manual error cmTection. The results of the automatic tagger are marked to be distinguished from tile results of the rule-based tagger. The human expert checks the marked words only. If an error is found , the ht/man expert assigns a correct tag to the word. When tile expert corrects the erroneous word , tile system automatically generates a lexicat rule and stores it in tile rnle base. File newly acquired rule is autoinatically applied to the rest of tile corpus. Thus , the expert does not need to correct the repeated errors. 1098 B ... ... .. A ... J : ~</definiens>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>The mutual inlbrmation between terms t~ and t i is defined by the following formula : g ( ti , tj ) /~ ' g ( t , , t , ) Ml ( ti , tj ) = log_ , /i.i , { t ' ( t0/ i ~ f ( t0 } ' { f ( tj ) //j~ f ( ti , where f ( t~ ) is the occurrence frequency of term t~ , and g ( ti , ti ) is the co-occurrence frequency of terms t~ and tj .</sentence>
				<definiendum id="0">mutual inlbrmation between terms</definiendum>
				<definiendum id="1">{ f ( tj ) //j~ f</definiendum>
				<definiendum id="2">f ( t~ )</definiendum>
				<definiendum id="3">ti )</definiendum>
				<definiens id="0">the following formula : g ( ti , tj ) /~ ' g ( t , , t , ) Ml ( ti , tj ) = log_ , /i.i</definiens>
				<definiens id="1">the occurrence frequency of term t~ , and g ( ti ,</definiens>
				<definiens id="2">the co-occurrence frequency of terms t~ and tj</definiens>
			</definition>
			<definition id="1">
				<sentence>Select the first M terms in the descending order of the frequency of being contained in the characteristic term sets° Our method for zooming in on a term cluster consists of term-set expansion and term cluster~ ing .</sentence>
				<definiendum id="0">Select</definiendum>
				<definiens id="0">the first M terms in the descending order of the frequency of being contained in the characteristic term sets° Our method for zooming in on a term cluster consists of term-set expansion and term cluster~ ing</definiens>
			</definition>
</paper>

		<paper id="2108">
			<definition id="0">
				<sentence>( 3 ) LO ( 3A'rivI , ; PI { .1 , ; I'OSITION \ ] ) l { Ol &gt; A~ : r ) , m~ATION ( some verbs ) : a. They skated along the canals .</sentence>
				<definiendum id="0">m~ATION</definiendum>
			</definition>
			<definition id="1">
				<sentence>Recall was define ( l by the I ) ercentage of verbs ( verb senses ) within the correct clusters compared to the total munber of verbs ( verb senses ) to be clustered : I , ,e , 'bs ... ... ... , , .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiendum id="1">l</definiendum>
				<definiens id="0">by the I ) ercentage of verbs ( verb senses ) within the correct clusters compared to the total munber of verbs ( verb senses ) to be clustered : I , ,e , 'bs ... ... ... , ,</definiens>
			</definition>
			<definition id="2">
				<sentence>Wordnet : A Lexical Database Organized on Psycholinguistic Principles .</sentence>
				<definiendum id="0">Wordnet</definiendum>
			</definition>
</paper>

		<paper id="2175">
			<definition id="0">
				<sentence>Grammatical relationships ( Glls ) form an important level of natural language processing , but different sets of ORs are useflfl for different purposes .</sentence>
				<definiendum id="0">Grammatical relationships</definiendum>
				<definiens id="0">an important level of natural language processing , but different sets of ORs are useflfl for different purposes</definiens>
			</definition>
			<definition id="1">
				<sentence>Grmnnmtical relationships ( GRs ) , whidl include arguments ( e.g. , subject and object ) and modifiers , form an important level of natural language processing .</sentence>
				<definiendum id="0">Grmnnmtical relationships ( GRs</definiendum>
				<definiens id="0">whidl include arguments ( e.g. , subject and object ) and modifiers , form an important level of natural language processing</definiens>
			</definition>
			<definition id="2">
				<sentence>The TR system includes several types of inibrmation not used in the MB system ( some because memory-based systems have a harder time handling set-wdued attributes ) : possible syntactic ( Comlex ) and semantic ( Wordnet ) classes of a c\ ] 11111k headword , 1 ; 11 ( ' , stem ( s ) and named-entity category ( e.g. , person , h ) cation ) , if any , of a c\ ] mnk h eadword , lcxemes in a clmnk besides the headword , pp-attachment estimate and cerl ; ain verb chunk properties ( e.g. , passive , infinitive ) .</sentence>
				<definiendum id="0">Wordnet</definiendum>
			</definition>
			<definition id="3">
				<sentence>% 47 % TR 25 ( 24 % ) 64 % 35 % Recall is the number ( and percentage ) of the keys that m'e recalled .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">the number ( and percentage ) of the keys that m'e recalled</definiens>
			</definition>
			<definition id="4">
				<sentence>Precision is the number of correctly recalled keys divided by the munber of ORs the system claims to exist .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the number of correctly recalled keys divided by the munber of ORs the system claims to exist</definiens>
			</definition>
</paper>

		<paper id="1076">
			<definition id="0">
				<sentence>Given a sequence of textual units U = tq , u2 , ... , UN , a set 1U~ of rhetorical relations that hold among these units , and a set o1 ' intentional judgments IH that pertain to the same units , find all legal discourse structures ( trees ) of U , and determine the dominance , satisl'action-precedence relations , and primary intentions of each span of these trees .</sentence>
				<definiendum id="0">u2 , ... , UN</definiendum>
				<definiens id="0">a set 1U~ of rhetorical relations that hold among these units</definiens>
				<definiens id="1">all legal discourse structures ( trees ) of U , and determine the dominance , satisl'action-precedence relations</definiens>
			</definition>
			<definition id="1">
				<sentence>the text spans that that node spans over ) , the l ) romotion set ( the set of units that constitute the most `` salient '' ( irapertain ) part of the text that is spanned by that node ) , and tile i ) rima O , intelltion .</sentence>
				<definiendum id="0">romotion set</definiendum>
				<definiens id="0">the set of units that constitute the most `` salient '' ( irapertain ) part of the text that is spanned by that node ) , and tile i ) rima O , intelltion</definiens>
			</definition>
			<definition id="2">
				<sentence>• Ill , h , intention ) provides the primary intention of discourse span El , h\ ] .</sentence>
				<definiendum id="0">intention )</definiendum>
				<definiens id="0">provides the primary intention of discourse span El</definiens>
			</definition>
			<definition id="3">
				<sentence>&lt; 19 &lt; h,2 ) \ ] `` ~ &gt; { \ [ `` ~,5 ' ( 11 , hi , NONE ) A , ~ ' ( /2 , h2 , SATEI , L1TF , ) A 11 &lt; l~ &lt; h~ &lt; hlA ( s ) ~ ( ~-+'+ , , ' , ,+ ) ( , ' , + &lt; 6 &lt; z~ &lt; h~ _ &lt; h~ &lt; \ ] , ,~A ( 13 ¢ 12 V h , ,3 ¢ h,2 ) A s ( /+ , ha , SATI+LLm , : ) ) \ ] dom ( ll , hq , 12 , h2 ) } \ [ ( + &lt; h , &lt; N ) , X ( + _ &lt; h _ &lt; h , ) / , ( l _ &lt; h+ &lt; N ) A ( 9 ) ( 1 ~ 1.9 .~ 11.2 ) A do ? l+ ( l , , lt , l , 12 , //.2 ) \ ] `` -- + \ [ -~,5 ' ( h , hi , NON. : ) A S ( 6 , h_~ , SATJILUTE ) \ ] Axiom ( 8 ) specities that if segment \ [ 12 , h.2\ ] is the immediate satellite el'segment \ [ lt , lq\ ] , then there exists a dominance relation between the DSP of segment \ [ /1 , /q\ ] and the DSP of segment \ [ 12 , h2\ ] . Hence , axiom ( 8 ) explicates the relationship between the structure of discourse and intentional dominance. In contrast , axiom ( 9 ) explicates the relationship between intentional dominance and discourse structure. That is , if we know that the intention associated with span \ [ lj , 1,1\ ] dominates the intention associated with span \ [ 12 , h,2\ ] , then both those spans play an active role in the representation and , moreover , the segment \ [ 12,11,2\ ] plays a SATELLITE role. • The satisfaction-precedence rdations described by Grosz and Sidner are parataetie relations that hold between arlfitrarily large textual spans. Nevertheless , as we have seen in the examples discussed in this paper , the fact that a paratactic relation holds between spans does not imply that there exists a satisfactionprecedence relation at the intentional level between those spans. Therefore , for satisfaction-precedence relations , we will have only OnE axiom , that shown in ( I0 ) , below. \ [ ( t 5 hJ ~ N ) A ( 1 ~ 11 ~ hl ) A ( \ ] &lt; `` h,2 ~ N ) A ( 1 o ) 0 &lt; z~ _ &lt; , '+2 ) A . , + , ~ , tv. , '~4.'~ , h , ~ , z~ , \ ] , . , _ , ) \ ] -+ \ [ S ( 11 , h,1 , NUCI+EUS ) A ,5 ' ( 12 , h,2 , NUCI , EUS ) \ ] This specifiES that the spans that are arguments of a satisfaction-precedence relation have a NUCLEUS status in the linal representation. axiomatization Given the formulation discussed abovE , tinding the discourse trees and the primary intentions lkw a text such as that given in ( 1 ) amounts to finding a model for a firstorder theory that consists of formulas ( 2 ) , ( 4 ) , and the axioms enumerated in section 3. There are a number of ways in which one can proceed with an implementation : for cxalnple , a smtightforward choice is one that applies constraint-satisl'action techniques , an approach that extends that discussed in ( Marcu , 1996 ) . Given a sequence U of N textual units , one can take advantage of the structure of the domain and associate with each of the N ( N-F 1 ) /2 possible text spans a status and a type variable whose domains consist in the set of objects over which the corresponding predicates ,5 + and T , range. For each of the N ( N + 1 ) /2 possible text spans \ [ l , h.\ ] , one can also associate h , l + \ ] promolion variables. These are boolean variables that specify whether units l , 1 + \ ] , ... , h belong to the promotion set of span \ [ / , hi. For each of the N ( N + 1 ) /2 possible text spans \ [ l , hi , one can also associate h 1 + 2 intentional variables : one of these wtriables has as domain the set of rhetorical relations that are relevant for the span \ [ 1 , hi. The rest of the h -/+ 1 wwiables are boolean and specify whether unit l , l-t\ ] ... . , or h are arguments of the oracle function f~ that intentionally characterizes that span. Hence , each text of N units yields a constraintsatisfaction prohlem with N ( N + I ) ( 2N + \ ] 3 ) /6 variables ( NCN q\ ] ) ( 2N - } 13 ) / ( J = 2NCN q\ ] ) /~ - } V,2 &lt; =N V , h &lt; -- -- N I &lt; =N W , h &lt; =N ( h_l_F2 ) ) ) . ( h ' -- l+l ) +~l-1 Z- , h.=l Z- , I=1 Z~ , h=l The constl+aints associated with these wtriables arc a oneto-onE mapping o1 ' the axioms in section 3. Finding the set of RS-trees and the intentions that are associated with a given discourse reduces then to/inding all the solutions for a traditional constraint-satisfaction problem. Reasoning from text structures to intentions. Consider again the example text ( 1 ) , which was usEd throughout this paper. As we discussed in section 1 , il ' we assume that an analyst ( or a program ) determines that the rhetorical relations given in ( 2 ) hold between the elementary units of the text , there arc live valid trees that correspond to text ( 1 ) ( see figure 1 ) . If we consider now the axioms that dEscribE the relationship bEtwEen text structures and intentions , we can infer , for example , thai , for the tree I.a , the DSP of span \ [ A1,131\ ] dominates the DSP of span \ [ cj , l ) j\ ] and that the primary intention of the whole text depends on unit B1 and on the rhetorical relation of EVID\ ] \ ] NCF , . Ill such a casE , the axiomatization provides the means for drawing intentional inferences on the basis of the discourse structure. Also , although there are live discourse structures that are consistent with the rhetorical judgments in ( I ) , they yield only three intentional interpretations , i.e. , there arc only three primary intentions that one can associate to the whole text. One intention is that discussed above , which is associated with analysis I.a. Another intention depends on unit Bz and the JUSTIFICATION relation that holds between units A1 and lh ; this intention is associated with the analyses shown in ligure 1.c and l.e. And another intention depends on trait Bj and the JUSTIFICATION relation that holds between units l ) j and Bj ; this intention is associated with the analyses shown in figure 1.b and 1.d. Reasoning fronl text structures to intentions can be also beneficial hi a context such as that described by Lochbaum ( 1998 ) because the rhetorical constraints can help prune the space of shared phms that woukl characterize an intEn tional interpretati o n of a d iscou rse. 528 Using intentions lbr nmnaging rhetorical amlfiguities. Assume now that besides providing.ivdgments concerning the rhetorical rehttions that hold between various units , an analyst ( ot '' a progran0 provides judglnents of intentions as well. If , lk+t '' cxaml+le , besides the relations given in ( 2 ) a program determines that the DSP of span tAt , 1 ) 1\ ] dominates 111o DSP of unit I/i , the theory that corresponds to these judgments and 111e axioms given in section 3 yields only two wdid text structures , those presented in \ [ igure l.b and I.d. In this ease , the axiomatization provides the means of using intentional judgments for reducing the ambiguity that characterizes the discourse parsing process. hwestigating the relationship between semantic and intentional relations. In their seminal paper , Moore and Polhtck ( 1992 ) showed lhat a text may be characterized by intentional and rhetorical analyses that are not isomorphic. For example , for the text shown in ( 1 I ) below , which is taken from ( Moore and Pollack , 1992 ) , one may argue from an informational perspective that A3 is a CONI ) ITION \ [ 'or B3. However , l } 'otll an intentional perspective , one may argue thai 1',3 can be used to MOTIVATI ' ; A3. Similal + judgments can be made with respect to units 1 { 3 and c3. Hence , lhe set of relations that COlnpletely characterizes text ( 11 ) is thal shown in ( 12 ) below. ( 11 ) \ [ Come home by 5:00. ^a\ ] \ [ Then we can go to the hardware store before it closes ) ' : '\ ] \ [ That way we can linish Ihe bookshelves tonightY : ' \ ] .rhct_.rcl ( CONl ) lTlON , A : ~ 1 ' , .+. ) 'rhcI_.rcl ( MOTIVATION , B ; : , A : : ) ( 12 ) rh ( t_rcl ( { ; ONI ) lrlON , 1~ : .. , C ' : ,. ) 'r/t.CI_ , '' cl ( MOTIVATION , C : : , B : ; ) When given this discourse problenl , our implementation produces the four discourse trees shown iu figure 4 , each el + them having a different primary intention ( ./ '' / ( CONI ) ITION , C3 ) , f ! ( MOTIVATION , a3 ) , .ft ( MOTWATION , B3 ) , and ./ ) ( CONl ) rrtoN , I+ : ~ ) ) . Hence , our approach enables one to derive automatically and enumerate all possible rhetorical interpretations of a text and to study the rehttionshil~ between structure and intentions. Our approach does not provide yet the mechanisms for choosing between different interpretations , but it provides the foundations for such a study. In contrast , Moore and Pollaek 's informal approach could neither derive nor enumerate all possible interpretations : in fact , their discttssion refers only to the two trees shown in ligure 4.a and .b. Unlike Moore and Polhtck 's approach , where it is suggested that a discourse representation should reflect simultaneously both its informational and intentional interpretations , the approach presented here is capable of only enumerating these interpretations. The formal model we proposed is not rich enough to accotlllllodate conctlrretH , non-isomorphic interpretations. ... ... ... j I ... ... ... ... ... ... ... ... ... ... .. , u++ _. /~ , \ ] ? ? .c~ + ... ... ... ... ... .. + + , + A3 B3 93 C3 A3 B3 83 C2 a ) b &gt; c ) a : Figure 4 : The set of all RS-trces that can be built for text ( I 1 ) .</sentence>
				<definiendum id="0">axiomatization</definiendum>
				<definiens id="0">&lt; h , &lt; N ) , X ( + _ &lt; h _ &lt; h , ) /</definiens>
				<definiens id="1">l , , lt , l , 12 , //.2 ) \ ] `` -- + \ [ -~,5 ' ( h , hi , NON. : ) A S ( 6 , h_~ , SATJILUTE ) \ ] Axiom ( 8 ) specities that if segment \ [ 12 , h.2\ ] is the immediate satellite el'segment \ [ lt , lq\ ] , then there exists a dominance relation between the DSP of segment \ [ /1 , /q\ ] and the DSP of segment \ [ 12 , h2\ ] . Hence , axiom ( 8 ) explicates the relationship between the structure of discourse and intentional dominance. In contrast , axiom ( 9 ) explicates the relationship between intentional dominance and discourse structure. That is , if we know that the intention associated with span \ [ lj , 1,1\ ] dominates the intention associated with span \ [ 12 , h,2\ ] , then both those spans play an active role in the representation and , moreover , the segment \ [ 12,11,2\ ] plays a SATELLITE role. • The satisfaction-precedence rdations described by Grosz and Sidner are parataetie relations that hold between arlfitrarily large textual spans. Nevertheless , as we have seen in the examples discussed in this paper , the fact that a paratactic relation holds between spans does not imply that there exists a satisfactionprecedence relation at the intentional level between those spans. Therefore , for satisfaction-precedence relations</definiens>
				<definiens id="2">A ( 1 o ) 0 &lt; z~ _ &lt; , '+2 ) A . , + , ~ , tv. , '~4.'~ , h , ~ , z~ , \ ] , . , _ , ) \ ] -+ \ [ S ( 11 , h,1 , NUCI+EUS ) A ,5 ' ( 12 , h,2 , NUCI , EUS ) \ ] This specifiES that the spans that are arguments of a satisfaction-precedence relation have a NUCLEUS status in the linal representation. axiomatization Given the formulation discussed abovE , tinding the discourse trees and the primary intentions lkw a text such as that given in ( 1 ) amounts to finding a model for a firstorder theory that consists of formulas ( 2 ) , ( 4 ) , and the axioms enumerated in section 3. There are a number of ways in which one can proceed with an implementation : for cxalnple , a smtightforward choice is one that applies constraint-satisl'action techniques , an approach that extends that discussed in ( Marcu , 1996 ) . Given a sequence U of N textual units , one can take advantage of the structure of the domain and associate with each of the N ( N-F 1 ) /2 possible text spans a status and a type variable whose domains consist in the set of objects over which the corresponding predicates ,5 + and T , range. For each of the N ( N + 1 ) /2 possible text spans \ [ l , h.\ ] , one can also associate h , l + \ ] promolion variables. These are boolean variables that specify whether units l , 1 + \ ] , ... , h belong to the promotion set of span \ [ / , hi. For each of the N ( N + 1 ) /2 possible text spans \ [ l , hi , one can also associate h 1 + 2 intentional variables : one of these wtriables has as domain the set of rhetorical relations that are relevant for the span \ [ 1 , hi. The rest of the h -/+ 1 wwiables are boolean and specify whether unit l , l-t\ ] ... . , or h are arguments of the oracle function f~ that intentionally characterizes that span. Hence , each text of N units yields a constraintsatisfaction prohlem with N ( N + I ) ( 2N + \ ] 3 ) /6 variables ( NCN q\ ] ) ( 2N - } 13 ) / ( J = 2NCN q\ ] ) /~ - } V,2 &lt; =N V , h &lt; -- -- N I &lt; =N W , h &lt; =N ( h_l_F2 ) ) ) . ( h ' -- l+l ) +~l-1 Z- , h.=l Z- , I=1 Z~ , h=l The constl+aints associated with these wtriables arc a oneto-onE mapping o1 ' the axioms in section 3. Finding the set of RS-trees and the intentions that are associated with a given discourse reduces then to/inding all the solutions for a traditional constraint-satisfaction problem. Reasoning from text structures to intentions. Consider again the example text ( 1 ) , which was usEd throughout this paper. As we discussed in section 1 , il ' we assume that an analyst ( or a program ) determines that the rhetorical relations given in ( 2 ) hold between the elementary units of the text , there arc live valid trees that correspond to text ( 1 ) ( see figure 1 ) . If we consider now the axioms that dEscribE the relationship bEtwEen text structures and intentions , we can infer , for example , thai , for the tree I.a , the DSP of span \ [ A1,131\ ] dominates the DSP of span \ [ cj , l ) j\ ] and that the primary intention of the whole text depends on unit B1 and on the rhetorical relation of EVID\ ] \ ] NCF , . Ill such a casE , the axiomatization provides the means for drawing intentional inferences on the basis of the discourse structure. Also , although there are live discourse structures that are consistent with the rhetorical judgments in ( I ) , they yield only three intentional interpretations , i.e. , there arc only three primary intentions that one can associate to the whole text. One intention is that discussed above , which is associated with analysis I.a. Another intention depends on unit Bz and the JUSTIFICATION relation that holds between units A1 and lh ; this intention is associated with the analyses shown in ligure 1.c and l.e. And another intention depends on trait Bj and the JUSTIFICATION relation that holds between units l ) j and Bj ; this intention is associated with the analyses shown in figure 1.b and 1.d. Reasoning fronl text structures to intentions can be also beneficial hi a context such as that described by Lochbaum ( 1998 ) because the rhetorical constraints can help prune the space of shared phms that woukl characterize an intEn tional interpretati o n of a d iscou rse. 528 Using intentions lbr nmnaging rhetorical amlfiguities. Assume now that besides providing.ivdgments concerning the rhetorical rehttions that hold between various units , an analyst ( ot '' a progran0 provides judglnents of intentions as well. If , lk+t '' cxaml+le , besides the relations given in ( 2 ) a program determines that the DSP of span tAt , 1 ) 1\ ] dominates 111o DSP of unit I/i , the theory that corresponds to these judgments and 111e axioms given in section 3 yields only two wdid text structures</definiens>
				<definiens id="3">provides the means of using intentional judgments for reducing the ambiguity that characterizes the discourse parsing process. hwestigating the relationship between semantic and intentional relations. In their seminal paper , Moore and Polhtck ( 1992 ) showed lhat a text may be characterized by intentional and rhetorical analyses that are not isomorphic. For example , for the text shown in ( 1 I ) below , which is taken from ( Moore and Pollack , 1992 ) , one may argue from an informational perspective that A3 is a CONI ) ITION \ [ 'or B3. However , l } 'otll an intentional perspective , one may argue thai 1',3 can be used to MOTIVATI ' ; A3. Similal + judgments can be made with respect to units 1 { 3 and c3. Hence , lhe set of relations that COlnpletely characterizes text ( 11 ) is thal shown in ( 12 ) below. ( 11 ) \ [ Come home by 5:00. ^a\ ] \ [ Then we can go to the hardware store before it closes ) ' : '\ ] \ [ That way we can linish Ihe bookshelves tonightY : ' \ ] .rhct_.rcl ( CONl ) lTlON , A : ~ 1 ' , .+. ) 'rhcI_.rcl ( MOTIVATION , B ; : , A : : ) ( 12 ) rh ( t_rcl ( { ; ONI ) lrlON , 1~ : .. , C ' : ,. ) 'r/t.CI_ , '' cl ( MOTIVATION , C : : , B : ; ) When given this discourse problenl , our implementation produces the four discourse trees shown iu figure 4 , each el + them having a different primary intention ( ./ '' / ( CONI ) ITION , C3 ) , f ! ( MOTIVATION , a3 ) , .ft ( MOTWATION , B3 ) , and ./ ) ( CONl ) rrtoN , I+ : ~ ) ) . Hence , our approach enables one to derive automatically and enumerate all possible rhetorical interpretations of a text and to study the rehttionshil~ between structure and intentions. Our approach does not provide yet the mechanisms for choosing between different interpretations , but it provides the foundations for such a study. In contrast , Moore and Pollaek 's informal approach could neither derive nor enumerate all possible interpretations : in fact , their discttssion refers only to the two trees shown in ligure 4.a and .b. Unlike Moore and Polhtck 's approach , where it is suggested that a discourse representation should reflect simultaneously both its informational and intentional interpretations , the approach presented here is capable of only enumerating these interpretations. The formal model we proposed is not rich enough to accotlllllodate conctlrretH , non-isomorphic interpretations. ... ... ... j I ... ... ... ... ... ... ... ... ... ... ..</definiens>
			</definition>
</paper>

		<paper id="2088">
			<definition id="0">
				<sentence>3 An SSN consists of a network of objects together with a mapping o1 ' these objects to a set o1 ' logical contexts .</sentence>
				<definiendum id="0">SSN</definiendum>
				<definiens id="0">consists of a network of objects together with a mapping o1 ' these objects to a set o1 ' logical contexts</definiens>
			</definition>
			<definition id="1">
				<sentence>DRSs with Plurals Following Kamp &amp; Reyle ( 1993 ) , we treat singular objects and sets of objects as entities of the same kind .</sentence>
				<definiendum id="0">DRSs</definiendum>
				<definiens id="0">singular objects and sets of objects as entities of the same kind</definiens>
			</definition>
			<definition id="2">
				<sentence>( V ) A function Name mapping constants to members of U. in particular , the constants c/ , , where P is a predicate are mapped to ®Pred ( P ) , i.e. , the supremum , also known as the sum , of the interpretation of P. Notice that in our models there are separate domains for objects and eventualities ( i.e. , states and events ) .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">A function Name mapping constants to members of U. in particular , the constants c/ , , where P is a predicate are mapped to ®Pred ( P ) , i.e. , the supremum , also known as the sum , of the interpretation of P. Notice that in our models there are separate domains for objects and eventualities ( i.e. , states and events</definiens>
			</definition>
			<definition id="3">
				<sentence>Q'~m , nt , maps it to the following interpretation : '5 ( 4 ) Q~ga~ , t ( Most ) = { ( r , c ) : r c c &amp; r is a nonatomic entity of M &amp; kl - &gt; } Thus 'most ' corresponds to the set of all tuples of individuals , such that the first individual is a nonatomic part of the second one and the cardinality of the first is greater than or equal to the cardinality of the second divided by two .</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">a nonatomic entity of M &amp; kl - &gt; } Thus 'most ' corresponds to the set of all tuples of individuals</definiens>
				<definiens id="1">a nonatomic part of the second one and the cardinality of the first is greater than or equal to the cardinality of the second divided by two</definiens>
			</definition>
			<definition id="4">
				<sentence>Seeped Semantic Networks A seeped semantic network ( SSN ) is a triple ( D , L , f ) , consisting of a typed DAG ( Directed Acyclic Graph ) D , a sef of logical contexts L and a function f which assigns a logical context ( which are treated as primitive objects separate from those in the DAG ) to each of the objects in the DAG .</sentence>
				<definiendum id="0">SSN</definiendum>
				<definiens id="0">seeped semantic network</definiens>
				<definiens id="1">a triple ( D , L , f ) , consisting of a typed DAG ( Directed Acyclic Graph</definiens>
				<definiens id="2">assigns a logical context ( which are treated as primitive objects separate from those in the DAG ) to each of the objects in the DAG</definiens>
			</definition>
			<definition id="5">
				<sentence>The function f , which assigns logical contexts to objects in a typed DAG D , satisfies the following constraints : ( I ) The root object and all the objects which are direct descendants of a logical operator are assigned a unique logical context .</sentence>
				<definiendum id="0">function f</definiendum>
				<definiens id="0">The root object and all the objects which are direct descendants of a logical operator are assigned a unique logical context</definiens>
			</definition>
			<definition id="6">
				<sentence>The lnapping of SSNs which include these specialpurpose attributions and types to a l ) P , s is defined as follows : ( 1 ) For typing statements T ( x ) , where T is a subtype of del , _type : ignore the statement 7 ' ( x ) and the object x ; ( H ) For attributions quant ( x , y ) such that ~z : p ( , ,rt_of ( : , : ,z ) &amp; z is an a , ,cho , ' &amp; Tt ( x ) &amp; 7~ ( y ) , add to the box in which also x lives the lbllowing condition : .</sentence>
				<definiendum id="0">lnapping of SSNs</definiendum>
				<definiendum id="1">T</definiendum>
				<definiendum id="2">z</definiendum>
				<definiens id="0">a l ) P , s is defined as follows : ( 1 ) For typing statements T ( x )</definiens>
				<definiens id="1">a subtype of del , _type : ignore the statement 7 ' ( x ) and the object x ; ( H ) For attributions quant ( x , y ) such that ~z : p</definiens>
				<definiens id="2">an a , ,cho , ' &amp; Tt ( x ) &amp; 7~ ( y ) , add to the box in which also</definiens>
			</definition>
</paper>

		<paper id="1082">
			<definition id="0">
				<sentence>No. of t &gt; atures= 2 x 4 x 4 x 2 +2 x 4 x 4 + 4 x 4 x 2 + 4 x 4 + 4 + 4 = 152 In Figure 2 , ( ; lie feature that uses Infornultion B in the far left morl ) heme , Infbrnmtion D in the left mort ) heine , Information C in the right morpheme , and Information A in the fa .</sentence>
				<definiendum id="0">Information</definiendum>
				<definiens id="0">uses Infornultion B in the far left morl</definiens>
			</definition>
</paper>

		<paper id="1079">
			<definition id="0">
				<sentence>It is also due to the fact that Japanese correspondences have characteristics of non-spacing and continuity , which allows several words to be treated as a single word .</sentence>
				<definiendum id="0">continuity</definiendum>
				<definiens id="0">allows several words to be treated as a single word</definiens>
			</definition>
</paper>

		<paper id="2103">
			<definition id="0">
				<sentence>( 7 ) present time -~ , ~'imple/ ) re , ~'ent -+ l &gt; rcs &lt; mt t ) rogres , vi~ &lt; '. The factors involved have been thoroughly studted and classitied in the linguistic literature ( Greenbaum and Quirk , 1990 ) . So we confine ourselves to a short re.view here. State Present. Stative verb senses get simple aspect. ( 8 ) a. • We are having a house on Oxtbrd Street ; . b. We arc having dinner. Habitual Present. A habit is a set of &lt; : vents of the same type. In semantic tern : s , a habit arises from quantiti ( 'ation over events. If the events extend indefinitely into the past and fi : ture. , the lmbit is conceived as perutancu , t `` and simple aspect is used ; if the events occur over ~ limited period of time , the habit is conceived as temporary and progressive aspect is appropriate. A frequency adverbial can be used to speci\ [ y tit ( : ( relative ) number of occurrences of the event. Ge.ncral .\ [ acts cml be viewed as a special type of a habit. They arc always expressed in simple asl ) ect ( see ( 9 ) ) . ( 9 ) Because water boils at 100°C , water is boiling at 100°C in the pot. Instantaneous Present. Dynamic verb senst~s that ret'er to a single event with little or no ( lur~tion oceun'ing at the Sl ) eech time are exl ) ressed with simple aspect. This type of \ ] ) resent is used in commentm'ies ( 10a ) , self commentaries ( 10b ) and with performative verbs ( 10c ) referring to speech acts. ( 10 ) a. Joe scores a goal. b. I enclose an apt ) lication tbrm. ( : . For I ) ermission to tmblish this paper , the authors l ; tm.nk the l ) el ) : ~rtment of Economi ( : Develol ) ment. Durational Present. Dynamic verl ) senses denoting an incomph : te event with dm'ation get progressive aspect. ( 11. ) a. We are looking at ; March sixteenth. ( Verbmobil corpus ) b. This is looking interesting. ( Verbmobil corpus ) State Present ; . Disambiguation requires definition and classification of all relevant verb senses according to stativity. When in a first ap1 ) roximation only the most fl'equent verb sense of each verb are considered , a list ; of stative verbs can be extracted from a corpus. Habitual l ? resent. The presence of a fiequency adverbial points to a reading of Habitual Present. Since every event can be construed as a general fact , general facts arc very difficult to identify and will be disregarded. 713 Instantaneous Present. For disambiguation achievement verbs used in selfcommentm'ies and perfbrmative verbs need to be listed. Durational Present. Present events are usually regarded as having duration , so progressive is the default aspect for dynamic verb senses in the present. In a special case , Germm~ present tense can be rendered as English present perfect : In English , perfect is used to describe periods that begin in the past and lend up to the present ; German uses a non-perf~ctive tense in this situation. ( 1.2 ) Wir leben schon fiinf Jahre in Amsterdam. We live ah'eady five years in Amsterdam. We have lived in Amsterdam for five years. Whenever a period is described that begins before and still holds at speedl time , a limitative time adverbial i is used. ( This term is due to Bras ( 1.990 ) . ) This peculiarity makes disambiguation very easy. ( 13 ) a. Er ist erst zwei Wochen hier. lie is onlyt two weeks here. He has only been here for two weeks. b. Er ist nur zwei Wochen hier. He is only two weeks hcxe. He is here for only two weeks. For our purposes we model the time axis as the set of rational 2 numbers Q. An interval is then a pair of rational nmnbers &lt; s , c ) , such that s &lt; e. The d'uration of an interval is the distance between start and end of the interval ( d~Lr ( { s , c ) ) = e s ) . On the interval structure we define the relations of inclusion ( 143 ) and overlap ( 14t ) ) . ( 1.4 ) a. ( 81 , c1 } C ( 82 , c2 ) +- } 82 ~.~ s1 A ~1 ~ ( 22 b. @ 1 , C1 ) 0 ( 82 , ( '2 ) ~ 81 ~ ( 32 A 82 ~__ e 1 1Limigal ; ive adverbials go with the prepositions since mid for plus temporal measure nouns in English ; in German they occur with the preposition seit and as duration adverbials modified by schon and erst. '~Although natural mmlbers could have been used to ( ) , rational mm~bers are convenient since they allow free choice of the unit. A temporal noun denotes a set ; of intervals. We define the granularity of a temporal noun formally as a pair of numbers specifying the minireal and maximal duration of its intervals ( e.g. or'an ( day ) = ( 1. , 1 ) , gT ( t , /~. ( conference ) = ( 1 , 28 ) , ora'n ( senfinar ) = ( 0.00138889 , 334.812 ) if '' the unit is a dw ) . The following relation is used to compare granularity values. ( 15 ) ( d , tr~ , durl ) &gt; { d'ur'~ , dur~ ) +~ { dur'~ , d'ur~ ) 7~ &lt; &amp; , r~ , dur~ &gt; A dur I &gt; &gt; dur~ If 7t 1 has coarser grmmlarity than rt2 , then an interval of rt : l can not be included in aD .</sentence>
				<definiendum id="0">arc having dinner. Habitual Present. A habit</definiendum>
				<definiendum id="1">temporal noun</definiendum>
				<definiens id="0">present time -~ , ~'imple/ ) re , ~'ent -+ l &gt; rcs &lt; mt t ) rogres , vi~ &lt; '. The factors involved have been thoroughly studted and classitied in the linguistic literature ( Greenbaum and Quirk , 1990 ) . So we confine ourselves to a short re.view here. State Present. Stative verb senses get simple aspect. ( 8 ) a. • We are having a house on Oxtbrd Street ;</definiens>
				<definiens id="1">a set of &lt; : vents of the same type. In semantic tern : s , a habit arises from quantiti ( 'ation over events. If the events extend indefinitely into the past and fi : ture. , the lmbit is conceived as perutancu , t `` and simple aspect is used ; if the events occur over ~ limited period of time , the habit is conceived as temporary and progressive aspect is appropriate. A frequency adverbial can be used to speci\ [ y tit ( : ( relative ) number of occurrences of the event. Ge.ncral .\ [ acts cml be viewed as a special type of a habit. They arc always expressed in simple asl ) ect ( see ( 9 ) ) . ( 9 ) Because water boils at 100°C , water is boiling at 100°C in the pot. Instantaneous Present. Dynamic verb senst~s that ret'er to a single event with little or no ( lur~tion oceun'ing at the Sl ) eech time are exl ) ressed with simple aspect. This type of \ ] ) resent is used in commentm'ies ( 10a ) , self commentaries ( 10b ) and with performative verbs ( 10c ) referring to speech acts. ( 10 ) a. Joe scores a goal. b. I enclose an apt</definiens>
				<definiens id="2">Develol ) ment. Durational Present. Dynamic verl ) senses denoting an incomph : te event with dm'ation get progressive aspect. ( 11. ) a. We are looking at ; March sixteenth. ( Verbmobil corpus ) b. This is looking interesting. ( Verbmobil corpus ) State Present ; . Disambiguation requires definition and classification of all relevant verb senses according to stativity. When in a</definiens>
				<definiens id="3">a general fact , general facts arc very difficult to identify and will be disregarded. 713 Instantaneous Present. For disambiguation achievement verbs used in selfcommentm'ies and perfbrmative verbs need to be listed. Durational Present. Present events are usually regarded as having duration , so progressive is the default aspect for dynamic verb senses in the present. In a special case</definiens>
				<definiens id="4">used to describe periods that begin in the past and lend up to the present ; German uses a non-perf~ctive tense in this situation. ( 1.2 ) Wir leben schon fiinf Jahre in Amsterdam. We live ah'eady five years in Amsterdam. We have lived in Amsterdam for five years. Whenever a period is described that begins before and still holds at speedl time</definiens>
				<definiens id="5">a pair of rational nmnbers &lt; s , c ) , such that s &lt; e. The d'uration of an interval is the distance between start and end of the interval ( d~Lr ( { s , c ) ) = e s</definiens>
				<definiens id="6">ive adverbials go with the prepositions since mid for plus temporal measure nouns in English ; in German they occur with the preposition seit and as duration adverbials modified by schon and erst. '~Although natural mmlbers could have been used to ( ) , rational mm~bers are convenient since they allow free choice of the unit. A</definiens>
				<definiens id="7">d , tr~ , durl ) &gt; { d'ur'~ , dur~ ) +~ { dur'~ , d'ur~ ) 7~ &lt; &amp; , r~ , dur~ &gt; A dur I &gt; &gt; dur~ If 7t 1 has coarser grmmlarity than rt2 , then an interval of rt : l can not be included in aD</definiens>
			</definition>
			<definition id="1">
				<sentence>A quant ; ificr Q ( x , /2. , S ) is iterative iff it requires l ; h &amp; t ; con ( 'o.t ) i ; u } fl klmv , q ( ~ .</sentence>
				<definiendum id="0">ificr Q</definiendum>
				<definiens id="0">iterative iff it requires l ; h &amp; t</definiens>
			</definition>
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>A LTAG consists of a t'inite set of elementary trees of finite depth .</sentence>
				<definiendum id="0">LTAG</definiendum>
				<definiens id="0">consists of a t'inite set of elementary trees of finite depth</definiens>
			</definition>
			<definition id="1">
				<sentence>A linear type consists in a 7-tuple &lt; A , B , C , D , E , F , G &gt; where A is the root of the tree , B is the category of the anchor , C is the lexical anchor , D is a set of nodes which can receive an adjunction , E is a set of co-anchors , F a set of nodes marked for substitution , and G a potential foot node ( or nil in case the tree is initial ) .</sentence>
				<definiendum id="0">linear type</definiendum>
				<definiendum id="1">C</definiendum>
				<definiendum id="2">D</definiendum>
				<definiendum id="3">E</definiendum>
				<definiens id="0">consists in a 7-tuple &lt; A , B , C , D , E , F , G &gt; where A is the root of the tree , B is the category of the anchor</definiens>
				<definiens id="1">the lexical anchor</definiens>
				<definiens id="2">a set of nodes which can receive an adjunction</definiens>
			</definition>
			<definition id="2">
				<sentence>Candito ( 96 ) , ( 99 ) has developed a tool to generate semi-automatically elementary trees She use an additional layer of linguistic description , called the metagrammar ( MG ) , which imposes a general organization for syntactic information in a 3 dimensional hierarchy : creating a trccbank for French ( of Abcilld &amp; al 00a ) , but unfommatcly proved impossible to manually annotate .</sentence>
				<definiendum id="0">MG )</definiendum>
				<definiens id="0">developed a tool to generate semi-automatically elementary trees She use an additional layer of linguistic description , called the metagrammar (</definiens>
			</definition>
			<definition id="3">
				<sentence>inal ... ... .. ical I\ [ I L_ I ebj : nominal-canonical I\ ] define ( J. dOlitle LilLe pOlliille/ J gives an apple ) \ [ ; 4 ~li ... ...</sentence>
				<definiendum id="0">J. dOlitle LilLe pOlliille/ J</definiendum>
				<definiens id="0">gives an apple</definiens>
			</definition>
			<definition id="4">
				<sentence>Lcxik : a maintenance tool for FTAG .</sentence>
				<definiendum id="0">Lcxik</definiendum>
				<definiens id="0">a maintenance tool for FTAG</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>There is nothing inherently wrong with ( 1 ) , and one can think of special contexts ( e.g. where Tom is a circus pertbrmer whose act includes gnawing on comtmter peripherals ) where ( 1 ) is felicitous .</sentence>
				<definiendum id="0">Tom</definiendum>
				<definiens id="0">a circus pertbrmer whose act includes gnawing on comtmter peripherals</definiens>
			</definition>
			<definition id="1">
				<sentence>HPSG indices ( the boxed nmnbers ) are used as variables .</sentence>
				<definiendum id="0">HPSG indices</definiendum>
				<definiens id="0">the boxed nmnbers ) are used as variables</definiens>
			</definition>
			<definition id="2">
				<sentence>( 15 ) kcybd ( ~\ ] ) A man ( ~ ) A edible ( ~\ ] ) Given two contstraints cl , c2 on the same varial ) le , c~ subsumes c2 if the corresponding hierarchy sort of cl is an ancestor of that of c2 or if cl = c2 .</sentence>
				<definiendum id="0">edible</definiendum>
			</definition>
			<definition id="3">
				<sentence>It also lnzdntains Pollard and Sag 's distinction bel ; ween `` literal '' mid `` non-literal '' meaning ( expressed t ) y CeNT and I~ACKGIUN\ ] ) respe , ctively ) , a distinction whi ( 'h is lflm'red in the second approach ( e.g. nothing in ( 18 ) shows th~lt requiring the obje ( 't to denote an edil ) le entity is part of the non-literal meaning ; of .</sentence>
				<definiendum id="0">non-literal '' meaning</definiendum>
				<definiens id="0">'t to denote an edil ) le entity is part of the non-literal meaning</definiens>
			</definition>
			<definition id="4">
				<sentence>CYC : A Large-Scale Invest ; merit in Knowledge Infl'astructure .</sentence>
				<definiendum id="0">CYC</definiendum>
				<definiens id="0">A Large-Scale Invest ; merit in Knowledge Infl'astructure</definiens>
			</definition>
</paper>

		<paper id="2117">
			<definition id="0">
				<sentence>Kessler gives an excellent summarization of the potential applications of a text genre detector .</sentence>
				<definiendum id="0">Kessler</definiendum>
				<definiens id="0">gives an excellent summarization of the potential applications of a text genre detector</definiens>
			</definition>
			<definition id="1">
				<sentence>Dimensions of Register Variation : A Cross-linguistic Coml ) arison .</sentence>
				<definiendum id="0">Register Variation</definiendum>
			</definition>
</paper>

		<paper id="2130">
			<definition id="0">
				<sentence>pen ( Is on knowledge of relalions between objects , such as delinite descriptions thai refer to an object rehlted 1o an entity ah'eady introduced in the dis-com'se by a relation other than identity ( Prince 's 'inlerrables ' ) , as in the flat .</sentence>
				<definiendum id="0">pen (</definiendum>
				<definiens id="0">Is on knowledge of relalions between objects</definiens>
			</definition>
			<definition id="1">
				<sentence>WordNet : An electronic lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">An electronic lexical database</definiens>
			</definition>
</paper>

		<paper id="1005">
</paper>

		<paper id="2140">
			<definition id="0">
				<sentence>The following exalnl ) le com\ ] ) arcs a turn before and after t.he clean-up component : before : I MEAN WE LOSE WE LOSE I CA N'T I CA N'T DO ANYTHING ABOUT IT SO after : we lose / i ca n't do anything about it ( ; AI , I , IIOME s\ ] ) c'e ( ; h data is lll/llti-to\ ] ) ica\ ] I ) tlt does uot include mark &lt; q ) \ [ 'or pa.ragral ) hs , nor al , y tolfieinforlJ , ative headers. Tyl ) ically , we lind about 5 I0 ( .lilt'erent topics within a 10-mimd ; e segment of a di-ah ) gue , i.e. , the. topic changes about every 1 2 minutes in these conversations. To facilitate browsing and smHtlmrization , we thus have to discover topi ( : ally coherent , segl , lents automatically. This is done using a TextTiling approach , adapted t'ron~ ( l\ ] earst , \ ] 997 ) ( section ( i ) . Imst but not least , we face t.he l ) roblcm of iml ) ert'e ( : t word a ( : cura ( : y of sl ) eech recognizers , l ) articularly when ( h'.a~ling with Sl ) OUl.a\ ] mous st ) eech over a large vo ( : al ) uhu'y aud over a low I ) ; mdwi ( Ith ( : hamJe\ ] , SIIC\ ] I \ [ ~S l , h ( ~ ( ' , AI , I , IIOME ( { at ; tl ) asc 's which we Juainly used for develol ) lnent , testing , and evaluatiou of our syste/n. ( hu'r ( mt recognizers tyl ) ically exhibit word error rates \ [ 'or l , hese ( : orl ) ora ill the order of 50 % . In I ) IASUMM 's hfl'ormation condensation component , the relevaucc weights of speaker ttlr , ls ( : all be adjusted to take into acc.omd , their word confidence scores from 1.111 ; sl ) eech recognizer. That way we can reduce the likelihood of extra.eting passages with a larger amount of word lnisreeognitions ( Zeclmer and \Vaibel , 201111 ) . lu this 1 ) aper , however , the focus will be exclusively on results of our evaluations on human generated transcripts. No information from the speech recognizer nor from the acoustic signal ( other than inter-utterance pause durations ) are used. We are aware that in particular prosodic information may be of help for tasks such as the detection of sentence boundaries , speech acts , or topic boundaries ( l\ ] irschberg ~md Nakatani , 1998 ; Shriberg et al. , 1998 ; Stolcke et al. , 2000 ) , but the investigation of the integration of this additional source of infer marion is beyond the scope of this pal ) er and lel't tbr future work. The global system architecture of I ) IASUMM is a 1 ) ipeline of the tbllowing lbur major components : 969 inputtor \ ] CLEAN ~ Turn Linking and TELE ! i \ ] Clean-up Filter ! I i \ ] J input for. Topic Segmentation TRANS i l Information Condensation ~ TRANS i L 1 71\ ] 7 7~ CLEAN Telegraphic Reduction TELE Fignre 1 : System architecture turn linking ; clean-up filter ; topic segmentation ; and information condensation. A. fifth component is added a.t the end for the purpose of telegraphic reduction , so that we can maximize the information content in a given amount of space. The system architecture is shown in Figure 1. It also indicates the three major types of smnmaries which can be generated by l ) Ia SUMM : 'P\ ] ~ANS ( `` transcript '' ) : not using the linking and clean-up components ; CLEAN : rising the main four components ; 'I'EI , E ( `` telegraphic '' summary ) : additionally , using the telegraphic reduction component. The following sections describe the components of DIASUMM ill more detail. The two main objectives of this component are : ( i ) to form turns which contain a set of full ( and not partial ) clauses ; and ( ii ) to forln turn-pairs in cases where we have a question-answer pair in the dialogue. To achieve the first objective , we scan the input for adjacent turns of one speaker and link them together if their time-stamp distance is below a pre-specified threshold 0. If the threshold is too small , we do n't get most of the ( logical ) turn continuations across utterance boundaries , if it is too large , we run the risk of `` skipping '' over short but potentiMly relevant Daglnents of the speaker on the other channel. We experimented with thresholds between 0.0 and 2.0 seconds and determined a local performance maximum around 0 = 1..0. For the second objective , to form turn-pairs which comprise a question-answer information exchange between two dialogue participants , we need to detect whand yes-uo-questions in the dialogue. We tested \ ] English \ ] Spanish Annotated l ) ata turns 1603 1185 Wh-questions /12 78 yes-no-questions /t3 98 questions total 85 ( 5.3 % ) 176 ( 14.9 % ) Automatic Detection Results ( F1 ) SA classifier POS rules raudom baseline Tahle 1 : Q-A-pair distribution in the data and expel'imental results for automatic Q-A-detection two approa.ches : ( a ) a ItMM based speech a.ct ( SA ) classifier ( \ ] /Jes , \ ] 999 ) and ( b ) a set of part-of-speech ( POS ) based rules. The SA classifier was trained oll dialogues which were manually annotated for speech acts , using parts of the SWITCIIBOARI ) corpus ( Godfrey et al. , 1992 ) for Fmglish and CALLIIOMF , for Spanish. The corresponding answers for the detected questions were hypothesized in the first turn with a. different sl ) eaker , following the question-turn. Table 1 shows the results of these experiments for 5 English and 5 Spanish CAI , L\ ] IOME dialogues , cornpayed to a baseline of randomly assigning n question speech acts , n being the number of question-turns marked by human a.nnotal~ors. We report Fl-seores , where F1 ~ with P=preeision and /g -- recall. We note that while the results \ [ br the SA-classifier and the rule-based approach are very similar for English , the rule-based apl~roach yields better results tbr Spanish. The much higher random baseline for Spanish can be explained by the higher incidence of questions in the Spanish data ( 14.9°/ ( , vs. 5.3 % for English ) . The clean-up component is a sequence of modules which serve the purposes of ( a ) rendering the transcripts more readable , ( b ) simplifying the input for subsequent components , and ( c ) avoiding unwanted bias for relevance computations ( see section 2 ) . All this has to happen without losing essential information that could be relevant in a summary. While other work ( \ ] \ ] eeman et al. , 1996 ; Stolcke et al. , 1998 ) was concerned with building classifiers that can detect and possibly correct wn : ious speech disfluencies , our implementntion is of a much simpler design. It does not require as much lnanual annota.ted training data and uses individual components for every major category of disfluency.1 t While we have not yet numerically evaluated the perfof mance of this component , its output is deemed very natural to read by system users. Since the focus and goals of this contponent are somewhat different than l ) reviotts work in that area , meaningful comparisons are hard to make. 970 Single or multiple word repetitions , fillers ( e.g. , `` uhm '' ) , and discourse markers without semantic content ( e.g. , `` you know '' ) a.re removed fl : om the input , some short forms axe expanded ( e.g. , `` we 'll '' -+ `` we will '' ) , a.nd fl'cquent word sequences are combined into a single token ( e.g. , % lot of '' -+ `` a_lot_of '' ) . Longer tm'ns are segmented into shorl clauses , which are defined a.s consisting of at least a. subject and a.n inIlectcd verbal form. While ( Stolcke and Shriberg , 1996 ) use n-gram models for this task , and ( C~awald~t et al. , 1997 ) use neura.l networks , we decided to use a. rule-based approach ( using word a , nd POS information ) , whose performa.nce proved to be compat'able with the results in the cited \ ] ) ~pets ( 1 , '~ &gt; 0.85 , error &lt; 0.05 ) .</sentence>
				<definiendum id="0">Ith</definiendum>
				<definiendum id="1">Ia SUMM</definiendum>
				<definiendum id="2">'P\ ] ~ANS</definiendum>
				<definiendum id="3">CLEAN</definiendum>
				<definiendum id="4">'I'EI , E ( `` telegraphic '' summary )</definiendum>
				<definiendum id="5">time-stamp distance</definiendum>
				<definiendum id="6">] IOME</definiendum>
				<definiendum id="7">clean-up component</definiendum>
				<definiendum id="8">] ) ~pets</definiendum>
				<definiens id="0">arcs a turn before and after t.he clean-up component : before : I MEAN WE LOSE WE LOSE I CA N'T I CA N'T DO ANYTHING ABOUT IT SO after : we lose / i ca n't do anything about it ( ; AI , I , IIOME s\ ] ) c'e ( ; h data is lll/llti-to\ ] ) ica\ ] I ) tlt does uot include mark &lt; q ) \ [ 'or pa.ragral ) hs , nor al , y tolfieinforlJ , ative headers. Tyl</definiens>
				<definiens id="1">ally coherent , segl , lents automatically. This is done using a TextTiling approach , adapted t'ron~ ( l\ ] earst , \ ] 997 ) ( section ( i ) . Imst but not least , we face t.he l ) roblcm of iml ) ert'e ( : t word a ( : cura ( : y of sl ) eech recognizers , l ) articularly when ( h'.a~ling with Sl ) OUl.a\ ] mous st</definiens>
				<definiens id="2">'s hfl'ormation condensation component , the relevaucc weights of speaker ttlr , ls ( : all be adjusted to take into acc.omd , their word confidence scores from 1.111 ; sl ) eech recognizer. That way we can reduce the likelihood of extra.eting passages with a larger amount of word lnisreeognitions ( Zeclmer and \Vaibel , 201111 ) . lu this 1 ) aper , however , the focus will be exclusively on results of our evaluations on human generated transcripts. No information from the speech recognizer nor from the acoustic signal ( other than inter-utterance pause durations ) are used. We are aware that in particular prosodic information may be of help for tasks such as the detection of sentence boundaries , speech acts , or topic boundaries ( l\ ] irschberg ~md Nakatani , 1998 ; Shriberg et al. , 1998 ; Stolcke et al. , 2000 ) , but the investigation of the integration of this additional source of infer marion is beyond the scope of this pal ) er and lel't tbr future work. The global system architecture of I ) IASUMM is a 1 ) ipeline of the tbllowing lbur major components : 969 inputtor \ ] CLEAN ~ Turn Linking and TELE</definiens>
				<definiens id="3">System architecture turn linking ; clean-up filter ; topic segmentation ; and information condensation. A. fifth component is added a.t the end for the purpose of telegraphic reduction , so that we can maximize the information content in a given amount of space. The system architecture is shown in Figure 1. It also indicates the three major types of smnmaries which can be generated by l )</definiens>
				<definiens id="4">the risk of `` skipping '' over short but potentiMly relevant Daglnents of the speaker on the other channel. We experimented with thresholds between 0.0 and 2.0 seconds and determined a local performance maximum around 0 = 1..0. For the second objective , to form turn-pairs which comprise a question-answer information exchange between two dialogue participants , we need to detect whand yes-uo-questions in the dialogue. We tested \ ] English \ ] Spanish Annotated l ) ata turns 1603 1185 Wh-questions /12 78 yes-no-questions /t3 98 questions total 85 ( 5.3 % ) 176 ( 14.9 % ) Automatic Detection Results ( F1 ) SA classifier POS rules raudom baseline Tahle 1 : Q-A-pair distribution in the data and expel'imental results for automatic Q-A-detection two approa.ches : ( a ) a ItMM based speech a.ct ( SA ) classifier ( \ ] /Jes , \ ] 999 ) and ( b ) a set of part-of-speech ( POS ) based rules. The SA classifier was trained oll dialogues which were manually annotated for speech acts , using parts of the SWITCIIBOARI ) corpus ( Godfrey et al. , 1992 ) for Fmglish and CALLIIOMF , for Spanish. The corresponding answers for the detected questions were hypothesized in the first turn with a. different sl</definiens>
				<definiens id="5">dialogues , cornpayed to a baseline of randomly assigning n question speech acts , n being the number of question-turns marked by human a.nnotal~ors. We report Fl-seores , where F1 ~ with P=preeision and /g -- recall. We note that while the results \ [ br the SA-classifier and the rule-based approach</definiens>
				<definiens id="6">a sequence of modules which serve the purposes of ( a ) rendering the transcripts more readable , ( b ) simplifying the input for subsequent components , and ( c ) avoiding unwanted bias for relevance computations ( see section 2 ) . All this has to happen without losing essential information that could be relevant in a summary. While other work ( \ ] \ ] eeman et al. , 1996 ; Stolcke et al. , 1998 ) was concerned with building classifiers that can detect and possibly correct wn : ious speech disfluencies</definiens>
				<definiens id="7">much lnanual annota.ted training data and uses individual components for every major category of disfluency.1 t While we have not yet numerically evaluated the perfof mance of this component , its output is deemed very natural to read by system users. Since the focus and goals of this contponent are somewhat different than l ) reviotts work in that area , meaningful comparisons are hard to make. 970 Single or multiple word repetitions , fillers ( e.g. , `` uhm '' ) , and discourse markers without semantic content ( e.g. , `` you know '' ) a.re removed fl : om the input</definiens>
				<definiens id="8">a. rule-based approach ( using word a , nd POS information ) , whose performa.nce proved to be compat'able with the results in the cited \</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>( PDT in the sequel ) , which has been inspired by the build-up of the Penn Treebank ( Marcus , Santorini &amp; Marcinkiewicz 1993 ; Marcus , Kim , Marcinkiewicz et al. 1994 ) , is aimed at a complex annotation of ( a part of ) the Czech National Corpus ( CNC in the sequel ) , the creation of which is under progress at the Department of Czech National Corpus at the Faculty of Philosophy , Charles University ( the corpus currently comprises about 100 million tokens of word forms ) .</sentence>
				<definiendum id="0">PDT</definiendum>
				<definiens id="0">a part of ) the Czech National Corpus ( CNC in the sequel</definiens>
			</definition>
			<definition id="1">
				<sentence>PDT comprises three layers of annotations : ( i ) the morphemic layer with about 3000 morphemic tag values ; a tag is assigned to each word form of a sentence in the corpus and the process of tagging is based on stochastic procedures described by Haii6 and Hladkfi ( 1997 ) ; ( ii ) analytic tree structures ( ATSs ) with every word form and punctuation mark explicitly represented as a node of a rooted tree , with no additional nodes added ( except for the root of the tree of every sentence ) and with the edges of the tree corresponding to ( surface ) dependency relations ; ( iii ) tectogrammatical tree structures ( TGTSs ) corresponding to the underlying sentence representations , again dependency-based .</sentence>
				<definiendum id="0">ATSs</definiendum>
				<definiens id="0">except for the root of the tree of every sentence ) and with the edges of the tree corresponding to ( surface ) dependency relations</definiens>
			</definition>
</paper>

		<paper id="2128">
			<definition id="0">
				<sentence>Metonymy is a figure of st ) eech in which tile name of one thing is substituted for that of something to which it is related .</sentence>
				<definiendum id="0">Metonymy</definiendum>
			</definition>
			<definition id="1">
				<sentence>( 7 where 13 is a noun which belongs to the class C. Finally we derive f ( 13 , ~ , v ) BqC ( .0 ) In summary , we use the measure M as defined in Equation ( 3 ) , and cah : ulated by applying Equation ( 4 ) to Equation ( 9 ) , to rank nouns according to their apl ) ropriateness as possible interpretations of a metonymy .</sentence>
				<definiendum id="0">cah</definiendum>
				<definiens id="0">to rank nouns according to their apl ) ropriateness as possible interpretations of a metonymy</definiens>
			</definition>
			<definition id="2">
				<sentence>T. V. 9a in hy &amp; siki ga iu beer , cola , mizu ( water ) yu ( hot water ) , oyu ( hot water ) , nett5 ( boiling water ) zy @ Ssya ( car ) , best seller , kuruma ( vehicle ) c ( painting ) , image , aizin ( love , ' ) gensaku ( original work ) , mcisaku ( fmnous story ) , daihySsaku ( important work ) mcnuetto ( minuet ) , kyoku ( music ) , piano si ( poem ) , tyosyo ( writings ) , tyosaku ( writings ) carc , ky~tsoku ( rest ; ) , kaigo ( nursing ) hire ( person ) , tomodati ( friend ) , bySnin ( sick person ) Nihon ( Japan ) , ziko ( accident ) , kigy5 ( company ) zikanho ( assistant vice-minister ) , scikai ( political world ) , 9ikai ( Congress ) cotnlllentgtol'~ anllOllllcer I ( : ~stel '' mawari ( surrmmding ) , zugara ( design ) .</sentence>
				<definiendum id="0">c ( painting</definiendum>
				<definiens id="0">original work ) , mcisaku ( fmnous story</definiens>
			</definition>
			<definition id="3">
				<sentence>Lexically based approaches Generative Lexicon theory ( Pustejovsky , 1995 ) proposed the qualia structure which encodes semantic relations among words explicitly .</sentence>
				<definiendum id="0">Generative Lexicon theory</definiendum>
				<definiens id="0">encodes semantic relations among words explicitly</definiens>
			</definition>
			<definition id="4">
				<sentence>Metallel : An integrated approach to nonliteral phrase interpretation .</sentence>
				<definiendum id="0">Metallel</definiendum>
			</definition>
</paper>

		<paper id="2086">
			<definition id="0">
				<sentence>The co-occurrence similarity DSim ( rq , n2 ) between the nouns n , and n2 is defined as tbllows , which incorporates tile prediction that the similarity of the two nouns is higher when they are more frequently used with the same verb of the same syntactic category .</sentence>
				<definiendum id="0">co-occurrence similarity DSim</definiendum>
				<definiendum id="1">n2</definiendum>
				<definiens id="0">tbllows , which incorporates tile prediction that the similarity of the two nouns is higher when they are more frequently used with the same verb of the same syntactic category</definiens>
			</definition>
			<definition id="1">
				<sentence>calVg ( n2 ) l where 2 • G = { subj , obj , loca , last , modi } • Vg ( n ) = { v\ [ v is a verb such that fg ( n , v ) &gt; 1 } • I~ ( , ~ ) l = E~v. ( , , ) f. ( ~ , v ) 2fe ( n , v ) is the nunlber of times that noun n of type g occurs with verb v in tlm stone sentence in a corpus .</sentence>
				<definiendum id="0">calVg</definiendum>
				<definiens id="0">the nunlber of times that noun n of type g occurs with verb v in tlm stone sentence in a corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>We used PERL scripts to narrow down the sentences meeting certain basic conditions for coordination and manually identified sentences with coordination among those chosen .</sentence>
				<definiendum id="0">PERL</definiendum>
				<definiens id="0">scripts to narrow down the sentences meeting certain basic conditions for coordination and manually identified sentences with coordination among those chosen</definiens>
			</definition>
			<definition id="3">
				<sentence>ky.mekta % talki'y \ [ ttalki-lul rock\ ] ( 06 , m ) + ( 07 , i~ 1 ) ttMki-lul rock cG I cr I 8 mekt , 'C t t alld'yenghi ' \ [ \ [ yenghl-ka ttalki-lul\ ] mek\ ] , \ [ yenghi-ka \ [ ttalki-lul rock\ ] \ ] ( C5 , R2 ) q- ( cr , R.1 ) , ( cs , rtl ) + ( c6 , R2 ) s/ ( s\npn \npa ) 5 , f.ttalki'yenghi ' \ [ yenghi-ka ttMki-lul\ ] ( c5 , m ) + ( ca , R1 ) ycnghi4 &lt; a Table 5 : Sample CKY Parsing Table ( 16 ) os -- kwa cangsingkwu ( 0 ) -hll yenkwuha-nun kes ( 0.008647 ) -iess-suImit aos : 46 verbs , cangsinkwu : 2 verbs , kes : 2O83 verbs In ( 15 ) , there are three verbs that occur with 'kimchi ' ( kimchee ) , and nine verbs that occur with 'pap ' ( steamed rice ) , significantly fewer than those verbs that occur with 'kes ' ( thing ) . And in ( 16 ) , the number of verbs that occur with 'cangsinkwu ' ( accessory ) is smaller than that of the verbs that occur with other nouns. Both result in a wrong analysis. We can use a thesaurus to address this problem. In a thesaurus , words in the same class are assumed to have related meanings. We can use these class-mate words to compensate tbr data sparseness. In constructing a lexicon , we consult the thesaurus when the nmnber of verbs that occur with a given noun falls below a threshold , and let the noun share the data with those in the same class. The thesaurus has the 'word-meaning code ' tbrmat. The present thesaurus contains slightly more than 1000 nouns that are mammlly constructed. The classification follows the NTT hierarchy. We have assigned meaning codes to only the most frequently used meanings tbr polysemous entries. The depth of the hierarchy is 6. The following shows adjusted results with our thesarus. ( 17 ) kimchi-wa pap ( 0.768942 ) -man cwu-nun kes ( 0.008380 ) -ita. kimchi : 62 verbs , pat ) : 64 verbs , kes : 2083 verbs ( 18 ) os-kwa cangsinkwu ( 0.648276 ) -hfl yenkwuha-mm kes ( 0.008647 ) iess-supnita. os : 46 verbs , eangsinkwu : 69 verbs , kes : 2083 verbs Table 6 shows the comparison of the methods with w~rious co-occurrence similarity dictionaries using 84 sentences containing nomi phrase coordination and structural ambiguity. 9 It shows that a thesaurus is indeed usefifl in dealing with data sparseness. In this experi\ [ \ [ Nil M2 Precision 84.1 % 88.5 % Recall 95.3 % 92.7 % Table 6 : Comparison of l ) ifferent Methods ment , we have shared the nouns that are associated with fewer than 20 verbs. We have also set the maximum shared examples to 70 and tuned the figures for maximum precision. For tile pertbrmance evaluation , we have con &gt; pared three kinds of parsers .</sentence>
				<definiendum id="0">Sample CKY Parsing Table</definiendum>
				<definiens id="0">verbs that occur with 'kimchi ' ( kimchee ) , and nine verbs that occur with 'pap ' ( steamed rice ) , significantly fewer than those verbs that occur with 'kes ' ( thing )</definiens>
			</definition>
			<definition id="4">
				<sentence>d ) le 9 shows I-\ ] 89.8 4.3 963 32.3 245.1 13.4 83.3 TM ) h : 8 : Derived Semmlti ( : S ( ; rllt ; l ; llres the t ) arsing time ( i ) r each m ( ~tho ( t. I~ Tim reason ( ; hnt B ~q ) t ) em : s generMly faster ( ; lmn C ( ex ( 'ei ) ( ; for type d ) is tha , t ( ) sl ) ends extra time on consull ; ing the ( tiction~ry dai ; at ) : ~se for the similarity .</sentence>
				<definiendum id="0">lmn C ( ex</definiendum>
				<definiens id="0">~tho ( t. I~ Tim reason ( ; hnt B ~q ) t ) em : s generMly faster ( ;</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>FErtGUS follows Langkilde and Knight 's seminal work in using an n-gram language model , but ; we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar .</sentence>
				<definiendum id="0">FErtGUS</definiendum>
				<definiens id="0">follows Langkilde and Knight 's seminal work in using an n-gram language model , but ; we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar</definiens>
			</definition>
			<definition id="1">
				<sentence>XTAG is a tree~ adjoining grammar ( TAG ) ( Joshi , 1987a ) .</sentence>
				<definiendum id="0">XTAG</definiendum>
			</definition>
			<definition id="2">
				<sentence>N ) r example , ( q is the supertag of an all ) ha-tree anchored 1 ) y a noun that projects up to NP , wMle 72 is | ; lie superi ; ag of it gamma tree anchored by a noun that only t ) rojects 1 ; { ) N ( we 43 assume adjectives are adjoined at N ) , and , as the adjunction table shows , can right-adjoin to an N. So that estimate~ is a particular tree in our LTAG grammar .</sentence>
				<definiendum id="0">q</definiendum>
				<definiens id="0">the supertag of an all ) ha-tree anchored 1 ) y a noun that projects up to NP , wMle 72 is | ; lie superi ; ag of it gamma tree anchored by a noun that only t ) rojects 1 ; {</definiens>
			</definition>
			<definition id="3">
				<sentence>Specifically , its extended domain of locality is useflfl in generation tbr localizing syntactic properties ( including word order as well as agreement and other morphological processes ) , and lexicalization is useful tbr providing an interfime from semantics ( the deriw~tion tree represent the sentence 's predicate-argument structure ) .</sentence>
				<definiendum id="0">lexicalization</definiendum>
				<definiens id="0">useful tbr providing an interfime from semantics ( the deriw~tion tree represent the sentence 's predicate-argument structure )</definiens>
			</definition>
			<definition id="4">
				<sentence>4 Simple accuracy is the mnnber of insertion ( I ) , deletion ( D ) and substitutions ( S ) errors between the target language strings in the test corpus and the strings produced by the generation model .</sentence>
				<definiendum id="0">Simple accuracy</definiendum>
				<definiens id="0">the mnnber of insertion ( I ) , deletion ( D ) and substitutions ( S ) errors between the target language strings in the test corpus and the strings produced by the generation model</definiens>
			</definition>
			<definition id="5">
				<sentence>The simple accuracy metric , however , penalizes a mist ) lacc .</sentence>
				<definiendum id="0">simple accuracy metric</definiendum>
				<definiens id="0">penalizes a mist</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>/ ... J z ( ji : ( ( OBJ * J '' ) S\ [ JBJ ) &lt; , = 1 '' o The right-hand side of the equation stands for the sernantic representation ( c~ ) of tile functional-strncture ( '\ ] ' ) of the anaphor .</sentence>
				<definiendum id="0">J z</definiendum>
				<definiens id="0">( ( OBJ * J '' ) S\ [ JBJ ) &lt; , = 1 '' o The right-hand side of the equation stands for the sernantic representation ( c~ ) of tile functional-strncture</definiens>
			</definition>
			<definition id="1">
				<sentence>Taking n and its subcategorizing predicator p , A is the list with the reference markers of the complements of p ordered according to their relative obliqueness ; Z includes the elements of A plus the reference markers of the upstairs predicators directly or indirectly selecting the domaiu of p , observing the multiclausal obliqueness hierarchy ; and U is the list of all reference markers in the discourse context .</sentence>
				<definiendum id="0">Z</definiendum>
				<definiendum id="1">U</definiendum>
			</definition>
			<definition id="2">
				<sentence>If n is a short-distance reflexive , A ' is associated to its semantic representation , where A ' contains the reference markers of the o-commanders of n in A. If n is a long-distance reflexive , its semantic representation includes Z ' , such that Z ' contains the o-commanders of n in Z. If n is a pronoun , the set B=U\ ( A'u { refin , , } ) is coded into its representation .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">a long-distance reflexive , its semantic representation includes Z '</definiens>
				<definiens id="1">a pronoun , the set B=U\ ( A'u { refin ,</definiens>
			</definition>
			<definition id="3">
				<sentence>Under the conception of nominals as binding machines , LISTSA , LIST-Z and LIST-U stand for the input , ANTEC ( EDFNTS ) encodes the internal state , and REF ( ERENCE ) M ( ARKF .</sentence>
				<definiendum id="0">EDFNTS )</definiendum>
				<definiendum id="1">REF</definiendum>
				<definiens id="0">encodes the internal state</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>The PREFERENCE module imposes preferences on potential antecedents on the basis of their grammatical roles , parallelism , fi'equency , proximity , etc .</sentence>
				<definiendum id="0">PREFERENCE module</definiendum>
			</definition>
			<definition id="1">
				<sentence>One of the conjectures ( ) 1 ' VT is that the vein expression of an elementary discourse unit provides a coherent `` abstract '' of the discourse fi'agmcnt that contains that unit .</sentence>
				<definiendum id="0">VT</definiendum>
				<definiens id="0">the vein expression of an elementary discourse unit provides a coherent `` abstract '' of the discourse fi'agmcnt that contains that unit</definiens>
			</definition>
</paper>

		<paper id="1025">
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>morphology These are the most important features of Basque morphology ( Alegria et al. , 96 ) : • As prepositional functions are realized by case suffixes inside word-fornls , Basque presents a relatively high power to generate inflected word-forms .</sentence>
				<definiendum id="0">Basque</definiendum>
				<definiens id="0">presents a relatively high power to generate inflected word-forms</definiens>
			</definition>
			<definition id="1">
				<sentence>The PC-Kimmo-V2 system ( Antworth , 94 ) presents an architecture similar to ours applied to English , using a finite-state segmentation phase before applying a unification-based grammar .</sentence>
				<definiendum id="0">PC-Kimmo-V2 system</definiendum>
				<definiens id="0">presents an architecture similar to ours applied to English , using a finite-state segmentation phase before applying a unification-based grammar</definiens>
			</definition>
			<definition id="2">
				<sentence>ACL-MIT Press series in Natural Language Processing .</sentence>
				<definiendum id="0">ACL-MIT Press</definiendum>
			</definition>
</paper>

		<paper id="2096">
			<definition id="0">
				<sentence>worst case comple×ity analysis for this kind of approach is fairly awflfl ( O ( l y ) X 22 ( N-I ) ) where 1 ~ is the number of unsaturated edges in the initial chart and N is the length of the sentence ( Ramsay , in press ) ) .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of unsaturated edges in the initial chart</definiens>
				<definiens id="1">the length of the sentence ( Ramsay , in press ) )</definiens>
			</definition>
			<definition id="1">
				<sentence>Syntactic theory : a formal introduction .</sentence>
				<definiendum id="0">Syntactic theory</definiendum>
				<definiens id="0">a formal introduction</definiens>
			</definition>
</paper>

		<paper id="1073">
			<definition id="0">
				<sentence>Our approad~ is to work directly in a minimal but carefully designed stat ; e space ( Singh et al. , 1999 ) .</sentence>
				<definiendum id="0">e space</definiendum>
				<definiens id="0">to work directly in a minimal but carefully designed stat ;</definiens>
			</definition>
			<definition id="1">
				<sentence>NJFnn is a real-time spoken dialogue system that provides users with intbrmation about things to do in New Jersey .</sentence>
				<definiendum id="0">NJFnn</definiendum>
				<definiens id="0">a real-time spoken dialogue system that provides users with intbrmation about things to do in New Jersey</definiens>
			</definition>
			<definition id="2">
				<sentence>NJFnn uses a speech recognizer with stochastic language and understanding models trained from example user utterances , and a TTS system based on concatenative diphone synthesis .</sentence>
				<definiendum id="0">NJFnn</definiendum>
				<definiens id="0">uses a speech recognizer with stochastic language and understanding models trained from example user utterances</definiens>
			</definition>
			<definition id="3">
				<sentence>WeakComp is a relaxed version of task comt ) letion that gives partial credit : if all attribute values are either correct or wihh : ards , the value is the sum of the correct munl ) er of attrilmtes .</sentence>
				<definiendum id="0">WeakComp</definiendum>
				<definiens id="0">a relaxed version of task comt ) letion that gives partial credit</definiens>
				<definiens id="1">the sum of the correct munl ) er of attrilmtes</definiens>
			</definition>
			<definition id="4">
				<sentence>ASR is a dialogue quality llleasure that itl ) l ) roxinmtes Sl ) eech recognition act : uracy for tl , e datM ) ase query , a.nd is computed 1 : ) 3 , adding 1 for each correct attribute value altd .5 for every wihtca .</sentence>
				<definiendum id="0">ASR</definiendum>
				<definiens id="0">a dialogue quality llleasure that itl ) l ) roxinmtes Sl ) eech recognition act : uracy for tl</definiens>
			</definition>
			<definition id="5">
				<sentence>User satisfaction ( UserSat , ranging front 0-20 ) is obtained by summing the answers of the web-based user survey .</sentence>
				<definiendum id="0">User satisfaction</definiendum>
				<definiens id="0">ranging front 0-20 ) is obtained by summing the answers of the web-based user survey</definiens>
			</definition>
			<definition id="6">
				<sentence>Learning optimal dialogue strategies : A ease study of a Sl ) oken dialogue agent tbr email .</sentence>
				<definiendum id="0">Learning optimal dialogue strategies</definiendum>
				<definiens id="0">A ease study of a Sl ) oken dialogue agent tbr email</definiens>
			</definition>
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>Japanese is an agglutinative language , and several Nnction words ( auxiliary verbs , suffixes , and postpositions ) often appear together and in many cases compositionality does not hold among them .</sentence>
				<definiendum id="0">Japanese</definiendum>
				<definiens id="0">an agglutinative language , and several Nnction words ( auxiliary verbs , suffixes , and postpositions</definiens>
			</definition>
</paper>

		<paper id="2141">
			<definition id="0">
				<sentence>Definition : A word boumla O , block is lhe combination o1 ' the word ( including part-of-speech information ) and its constituent boundary tag , i.e. wbb~= &lt; % , b~ &gt; , where % is the ith word in the sentence , b~ can value 0,1,2 , which means % is at I llereafler , 'constiluent ' represents all internal or root nodes in a parse tree , i.e. phrase oF sentence tags .</sentence>
				<definiendum id="0">Definition</definiendum>
				<definiendum id="1">%</definiendum>
				<definiens id="0">A word boumla O , block is lhe combination o1 ' the word ( including part-of-speech information</definiens>
				<definiens id="1">all internal or root nodes in a parse tree</definiens>
			</definition>
			<definition id="1">
				<sentence>In the view of syntactic description capability , the WBBs defined above , the chunks defined by Abney ( 1991 ) and the phrases ( i.e , constituents ) defined in a parse tree have the following tealtions : WBBs &lt; chunks &lt; phrases Here is an example : ® The input sentence ( 10 words ) : ( My brother gives him a book . )</sentence>
				<definiendum id="0">WBBs</definiendum>
				<definiendum id="1">the phrases</definiendum>
				<definiens id="0">WBBs &lt; chunks &lt; phrases Here is an example : ® The input sentence ( 10 words ) : ( My brother gives him a book</definiens>
			</definition>
			<definition id="2">
				<sentence>Therefore , we defined the following local context templates ( LCTs ) : 1 ) Unigram POS template : t~ , BPFL , 2 ) Bigram POS templates : , Left restriction : t~_ , t~ , BI'I ; L~ ® Right restriction : L t~+ , , BPFL~ 3 ) Trigram POS template : t~_~t~t~+~ , BPFL , 4 ) Trigram POS+CN template : t~_~+cn~_~ t~+cn , ~ t~+~+cn~+~ , BPFL~ In tile above LCTs , t~ is tile POS tag of the ith word in the sentence , cn~ is its character number , and BPFL~ is the frequency distribution list of its diffetent BP ( boundary prediction ) value ( 0,1,2 ) under the local context restrictions ( I.CR ) ( the left and right word ) .</sentence>
				<definiendum id="0">t~</definiendum>
				<definiendum id="1">BPFL~</definiendum>
				<definiendum id="2">I.CR )</definiendum>
				<definiens id="0">t~ , BPFL , 2 ) Bigram POS templates : , Left restriction : t~_ , t~ , BI'I ; L~ ® Right restriction : L t~+ , , BPFL~ 3 ) Trigram POS template : t~_~t~t~+~ , BPFL , 4 ) Trigram POS+CN template : t~_~+cn~_~ t~+cn , ~ t~+~+cn~+~ , BPFL~ In tile above LCTs</definiens>
				<definiens id="1">tile POS tag of the ith word in the sentence , cn~ is its character number , and</definiens>
			</definition>
</paper>

		<paper id="2152">
			<definition id="0">
				<sentence>The Gralnmar Programming Language ( GPL ) is an imperative programming language for feature-structure-based rewrite grammars .</sentence>
				<definiendum id="0">Gralnmar Programming Language ( GPL )</definiendum>
			</definition>
			<definition id="1">
				<sentence>GPL is a l'ormalism that allows the direct expression of linguistic algorithms l'or parsing , transfer , and generation .</sentence>
				<definiendum id="0">GPL</definiendum>
				<definiens id="0">a l'ormalism that allows the direct expression of linguistic algorithms l'or parsing , transfer , and generation</definiens>
			</definition>
			<definition id="2">
				<sentence>GPL includes variables , simple and complex tests , and various manipulation operators .</sentence>
				<definiendum id="0">GPL</definiendum>
				<definiens id="0">includes variables , simple and complex tests</definiens>
			</definition>
			<definition id="3">
				<sentence>The English parsing grammar consists of 540 GPL rules .</sentence>
				<definiendum id="0">English parsing grammar</definiendum>
			</definition>
			<definition id="4">
				<sentence>The transfer grammar consists of 140 GPL rules , and its job is to specify linguistic constraints on examples , combine multiple examples , transfer informatiou that is beyond the scope of the example database , and perl'orm various other transformations .</sentence>
				<definiendum id="0">transfer grammar</definiendum>
				<definiens id="0">consists of 140 GPL rules , and its job is to specify linguistic constraints on examples , combine multiple examples , transfer informatiou that is beyond the scope of the example database</definiens>
			</definition>
</paper>

		<paper id="2101">
			<definition id="0">
				<sentence>The semantic attribute system is a sort of hierarchical concept thesaurus represented as a tree structure in which each node is called a semantic cateqory .</sentence>
				<definiendum id="0">semantic attribute system</definiendum>
				<definiens id="0">a sort of hierarchical concept thesaurus represented as a tree structure in which each node is called a semantic cateqory</definiens>
			</definition>
			<definition id="1">
				<sentence>se frame consists of one predicate a.nd one or more case elements tha .</sentence>
				<definiendum id="0">se frame</definiendum>
				<definiens id="0">consists of one predicate a.nd one or more case elements tha</definiens>
			</definition>
			<definition id="2">
				<sentence>; ach frame consists a predicate condition and several cast elements conditions .</sentence>
				<definiendum id="0">ach frame</definiendum>
			</definition>
			<definition id="3">
				<sentence>The valency dictionary also has case &lt; roles ( Table \ ] ) for : noun phrase conditions. The caseroles of adjuncts are determined by using the particles of adjuncts and the sema.ntic ca.tegories of n ou n ph ra.ses. As a result , the OUtl ) ut O\ [ ' the case a.nalysis is a set ; el '' ( ; ase fl : ames for ca.oh unit se : ntence. The noun phra.ses in \ [ 'tames are la.beled by case-roh ; s in Tal ) le 1. l ! 'or siml ) \ ] icity , we use case-role codes , such a.s N 1 and N2 , a.s the labels ( or slot ha.rues ) to represent case li : ames. The relation between sentences and case-roles is described in detail in ( Ikehara el el. , 1993 ) . We developed a logical form translator li'E1 ~ that generates semantic representations expressed a , s atomic Ibrmulae from the cast ; fi : a.mes and parse trees. For later use , document II ) and tense inlbrmation a.re also added to the case frames. For example , tile case fl : ame in 'l.'able 2 is obtained a : l'ter analyzing the following sentence of document l ) 1 : `` Jalctcu ( .lack ) h , a suts , tkesu ( suitca.se ) we 699 Table 1 : Case-Roles Name Code Description l~xampl.e Subject N1 the agent/experiencer of I throw a ball. an event/situation Objectl N2 the object of an event Object2 N3 another object of an event Loc-Source N4 source location of a movement Loc-Goal N5 goal location of a movement Purpose N6 the purpose of an action Result N7 the result of an event Locative N8 the location of an event Comitative N9 co-experiencer Quotative N10 quoted expression Material N 11 material/ingredient Cause N12 the reason for an event Instrument N13 a concrete instrument Means N14 an abstract instrument Time-Position TN1 the time of an event Time-Source TN2 the starting time of an event Time-Goal TN3 the end time of ~n event Amount QUANT quantity of something I throw a ball. I compare it with them. I start fl'om Japan. I go to Japan. I go shopping. It results in failure. it occurs at the station. I share a room with him. I say that ... . I fill the glass with water. It collapsed fr'om the weight. I speak with a microphone. I speak in Japanese. I go to bed at i0:00. I work from Monday. It continues until Monday. I spend $ 10. hok , ,ba ( the omce ) kava ( from ) o ( the air port ) , ) , i ( to ) ha~obu ( carry ) '' ( `` Jack carries a suitcase from the office to the airport. '' ) Table 2 : Case Frame of the Sample Sentence predicate : hakobu ( carry ) article : 1 ) 1 tense : present NI : Jakhu ( Jack ) N2 : sutsukesu ( suitcase ) N4 : sl , okuba ( the office ) N5 : kuko ( the airport ) Conventional ILP systems take a set of positive and negative examples , and background knowledge. The output is a set of hypotheses in the form of logic programs that covers positives and do not cover negatives. We employed the typeoriented ILP system RHB +. System RHB + The type-oriented I\ ] , P system has the tbllowing features that match the needs for learning l\ ] '' ~ rules. • A type-oriented ILP system can efficiently and effectively handle type ( or semantic category ) information in training data.. This feature is adwmtageous in controlling the generality and accuracy of learned IE rules. • It can directly use semantic representations of the text as background knowledge. , It can learn from only positive examples. • Predicates are allowed to have labels ( or keywords ) for readability and expressibility. System RHB + This section summarizes tile employed typeoriented ILP system RHB +. The input of RHB + is a set of positive examples and background knowledge including type hierarchy ( or 700 the semantic attribute system ) . The output is a set of I\ [ orn clauses ( Lloyd , 11.987 ) having vari ; tl~les with tyl ) e intbrmation. That is , the term is extended to the v-term. v-terms are the restricted form of 0-terms ( Ai'tK~tci and Nasr , 1986 ; Ait-Kaci et al. , 11994 ) . Inl'ormttlly , v-terms are Prolog terms whose variables a.re replaced with variable Var of type T , which is denoted as Var : T. Predicttte ~tnd tim ( : tion symbols ~tre allowed to h ; we features ( or labels ) . For examl ) \ ] e , speak ( agent~ X : human , objcct~ Y : language ) is a clause based on r-terms which ha.s labels agent and object , and types human and language. The algorithm of lHllI + is basically ~t greedy covering algorithm. It constructs clauses oneby-one by calling inner_loop ( Algorithm \ ] ) which returns a hypothesis clause. A hypothesis clause is tel ) resented in the form of head : -body. Covered examples are removed from 1 ) in each cycle. The inner loop consists of two phases : the head construction phase and the body construction I ) hase. It constrncts heads in a bottom-up manner and constructs the body in a top-down lna.nner , following the result described in ( Zelle el al. , 1994 ) . `` \ [ 'he search heuristic PWI is weighted inform~tivity eml ) loying the l , a.place estimate. Let 7 ' = { Head : -Body } U B K. rwz ( r , T ) _ l If'l+ J -I.f ' -- /× 1°g2 IQ-~\ ] 'i\ [ _12 2 ' where IPl denotes the number of positive examples covered by T and Q ( T ) is the empirical content. The smaller the value of PWI , the candidate clause is better. Q ( T ) is defined as the set of atoms ( 1 ) that are derivable from T ~md ( 2 ) whose predicate is the target I ) redicate , i.e. , the predicate name of the head. The dynamic type restriction , by positivc examples uses positive examples currently covered in order to determine appropriate types to wtri~bles for the current clause. Algorithm 1 inner_loop ground knowledge 1Hr. ing the lyped least general generalizations ( lgg ) of N pairs of clcmcnts in P , and select the most general head as H cad. Head. It. Let Body bc empty. 5 , Create a set of all possible literals L using variables in Head and Body. respect to the positive weighted informalivily PWI. Body for each literal lk in BEAM. the dynamic type restriction by positive exampies. ( Head : Body ) . lO. Goto 5. Now , we examine tile two short notices of ' new products release in Table 3. The following table shows a sample te : ml ) late tbr articles reporting a new product relea.se. Tom pl ate Suppose that the following semantic representations is obtained from Article 1. ( cl ) announce ( article = &gt; I , tense = &gt; past , tnl = &gt; `` this week '' , nl = &gt; `` ABC Corp. '' , nlO = &gt; ( c2 ) ) .</sentence>
				<definiendum id="0">IPl</definiendum>
				<definiens id="0">The valency dictionary also has case &lt; roles ( Table \ ] ) for : noun phrase conditions. The caseroles of adjuncts are determined by using the particles of adjuncts and the sema.ntic ca.tegories of n ou n ph ra.ses. As a result , the OUtl ) ut O\ [ ' the case a.nalysis is a set ; el '' ( ; ase fl : ames for ca.oh unit se : ntence. The noun phra.ses in \ [ 'tames are la.beled by case-roh ; s in Tal ) le 1. l ! 'or siml ) \ ] icity , we use case-role codes , such a.s N 1 and N2 , a.s the labels ( or slot ha.rues ) to represent case li : ames. The relation between sentences and case-roles is described in detail in ( Ikehara el el. , 1993 ) . We developed a logical form translator li'E1 ~ that generates semantic representations expressed a , s atomic Ibrmulae from the cast ; fi : a.mes and parse trees. For later use , document II ) and tense inlbrmation a.re also added to the case frames. For example , tile case fl : ame in 'l.'able 2 is obtained a : l'ter analyzing the following sentence of document l ) 1 : `` Jalctcu ( .lack ) h , a suts , tkesu ( suitca.se ) we 699 Table 1 : Case-Roles Name Code Description l~xampl.e Subject N1 the agent/experiencer of I throw a ball. an event/situation Objectl N2 the object of an event Object2 N3 another object of an event Loc-Source N4 source location of a movement Loc-Goal N5 goal location of a movement Purpose N6 the purpose of an action Result N7 the result of an event Locative N8 the location of an event Comitative N9 co-experiencer Quotative N10 quoted expression Material N 11 material/ingredient Cause N12 the reason for an event Instrument N13 a concrete instrument Means N14 an abstract instrument Time-Position TN1 the time of an event Time-Source TN2 the starting time of an event Time-Goal TN3 the end time of ~n event Amount QUANT quantity of something I throw a ball. I compare it with them. I start fl'om Japan. I go to Japan. I go shopping. It results in failure. it occurs at the station. I share a room with him. I say that ... . I fill the glass with water. It collapsed fr'om the weight. I speak with a microphone. I speak in Japanese. I go to bed at i0:00. I work from Monday. It continues until Monday. I spend $ 10. hok , ,ba ( the omce ) kava ( from ) o ( the air port ) , ) , i ( to ) ha~obu ( carry ) '' ( `` Jack carries a suitcase from the office to the airport. '' ) Table 2 : Case Frame of the Sample Sentence predicate : hakobu ( carry ) article : 1 ) 1 tense : present NI : Jakhu ( Jack ) N2 : sutsukesu ( suitcase ) N4 : sl , okuba ( the office ) N5 : kuko ( the airport ) Conventional ILP systems take a set of positive and negative examples , and background knowledge. The output is a set of hypotheses in the form of logic programs that covers positives and do not cover negatives. We employed the typeoriented ILP system RHB +. System RHB + The type-oriented I\ ] , P system has the tbllowing features that match the needs for learning l\ ] '' ~ rules. • A type-oriented ILP system can efficiently and effectively handle type ( or semantic category ) information in training data.. This feature is adwmtageous in controlling the generality and accuracy of learned IE rules. • It can directly use semantic representations of the text as background knowledge. , It can learn from only positive examples. • Predicates are allowed to have labels ( or keywords ) for readability and expressibility. System RHB + This section summarizes tile employed typeoriented ILP system RHB +. The input of RHB + is a set of positive examples and background knowledge including type hierarchy ( or 700 the semantic attribute system ) . The output is a set of I\ [ orn clauses ( Lloyd , 11.987 ) having vari ; tl~les with tyl ) e intbrmation. That is , the term is extended to the v-term. v-terms are the restricted form of 0-terms ( Ai'tK~tci and Nasr , 1986 ; Ait-Kaci et al. , 11994 ) . Inl'ormttlly , v-terms are Prolog terms whose variables a.re replaced with variable Var of type T , which is denoted as Var : T. Predicttte ~tnd tim ( : tion symbols ~tre allowed to h ; we features ( or labels ) . For examl ) \ ] e , speak ( agent~ X : human , objcct~ Y : language ) is a clause based on r-terms which ha.s labels agent and object , and types human and language. The algorithm of lHllI + is basically ~t greedy covering algorithm. It constructs clauses oneby-one by calling inner_loop ( Algorithm \ ] ) which returns a hypothesis clause. A hypothesis clause is tel ) resented in the form of head : -body. Covered examples are removed from 1 ) in each cycle. The inner loop consists of two phases : the head construction phase and the body construction I ) hase. It constrncts heads in a bottom-up manner and constructs the body in a top-down lna.nner , following the result described in ( Zelle el al. , 1994 ) . `` \ [ 'he search heuristic PWI is weighted inform~tivity eml ) loying the l , a.place estimate. Let 7 ' = { Head : -Body } U B K. rwz ( r , T ) _ l If'l+ J -I.f ' -- /× 1°g2 IQ-~\ ] 'i\ [ _12 2 ' where</definiens>
			</definition>
			<definition id="4">
				<sentence>H. Ai't-Kaci and R. Nasr , LOGIN : A logic programming language with built-in inheritance , Journal oJ ' Logic Programming , 3 , pp.185215 , 1986 .</sentence>
				<definiendum id="0">LOGIN</definiendum>
				<definiens id="0">A logic programming language with built-in inheritance</definiens>
			</definition>
			<definition id="5">
				<sentence>Japanese ) S. Ikehara , S. Shirai , K. Ogura , A. Yokoo , H. Nakaiwa and T. Kawaoka , ALT-J/E : A Japanese to English Machine Translation System tbr Communication with Translation , Proc .</sentence>
				<definiendum id="0">ALT-J/E</definiendum>
				<definiens id="0">A Japanese to English Machine Translation System tbr Communication with Translation , Proc</definiens>
			</definition>
			<definition id="6">
				<sentence>Y. Sasaki and M. IIaruno , RHB+ : A TypeOriented 1LP System Learning from Positive Data , Proc .</sentence>
				<definiendum id="0">RHB+</definiendum>
				<definiens id="0">A TypeOriented 1LP System Learning from Positive Data</definiens>
			</definition>
</paper>

		<paper id="2138">
			<definition id="0">
				<sentence>77 Ill the examples , each temporal expression plays a syntactically different role used as noun phrase or adverbial phrase ( The undeJ'lined is a phrase ) although they comprise the same phrasal fornls .</sentence>
				<definiendum id="0">adverbial phrase</definiendum>
				<definiendum id="1">undeJ'lined</definiendum>
				<definiens id="0">a phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>Figure 3 1KAIST corlms consists of about 50 million cojeols , l ' ; ojeol is a sl ) acing unit ; comi ) oscd of a content word and functional words .</sentence>
				<definiendum id="0">ojeol</definiendum>
				<definiens id="0">a sl ) acing unit ; comi ) oscd of a content word and functional words</definiens>
			</definition>
			<definition id="2">
				<sentence>~\ ] '~ &lt; gl~-V-~°II -~ } ~ .j-q-\ ] _~ol~xI , lu &amp; ~ o , o~ , ~ 2,2 , 2~ ... J~. , ~Xor\ ] Ol~tq , : z~ } `` J.-*~ls1-~'li ol ~ , \ ] -~-~\ ] ~ ~x~lq~1~,4 ~. ~'t-l-b ~'l~xl'4 ~ ~-~t ~1 ; 'I ~ ~1~~ ~1~ Figure 3 : Example concordance data of yeorcum ( summer ) befbre temporal noml after outlmt freq yeoreum ( suulmer ) x\ ] ~. ! -/ t~ ( ji. , . , ,~ , ~ant ) ~/tlO ( hae , yeal ' ) o\ ] ~/ld ( ibcon , thin ) o : t ~-/t22 o : t-~-/t2. , _ o-1 ~/ t.2.2 oq ~-/t,2. , o~ ~- , /~ o~ ~-/t._ , . , r~\ [ ( bam , night ) TN 2 vo~ , ( banghagyacation ) TN 7 '~ ~- ( bycoljang , villa ) TN 1 ~ ( jumal , weekends ) TN 1 J , ~ 71 ( .qam , .qi , flu ) TN 1 q\ ] q\ ] /t2~ ( nacnae , all the time ) TA 1 ~-F~- ( na , ,eunJ/TOP ) TA \ ] ~ ( .leo'ntuncun,1 ) attle/.\ [ 0I ) TA 1 Figure 4 : Temporal expression phrases selected fronl examl ) les tuple ( Ej , E2 , Q , i , F , E ) where : E1 is a finite input alphabet ; E2 in a finite output alphabet ; Q in a tinite net of states or vertices ; i E Q in the initial state ; F C_ ( 2 is the set of final staten ; E C Q × E~ : ~ E. ; x ( 2 is the set of transitions or edges. Although the syntactic function of a temporal expression would be nondeterministically selected fl'om the context , temporal expressions and the lexical data of local context can be represented in a deterministic way due to their finite length. For the deterministic FST , we define the partial functions ® and • where q®a = q ' iffd ( q , a ) = { q ' } and q , a = w ' iff ? q ' E Q such that q®a = q ' and 5 ( q , a , q ' ) = { w ' } ( R.oche and Schabes , 1995 ) . Then , a nubsequential FST is a eight-tui ) le ( El , E2 , Q , i , F , ® , * , p ) where : E1 , E2 , Q , i and F are the smnc as the FST ; ® is the deterministic state transition fimetion that maps ( ) x E1 on Q ; • is the deterministic emission fimction Figure 6 : T = ( ~ , , r , .~ , O , i , F , o , . , p } ) ) 2~ = { tl , t~2 , t~6 , wi , wj } E2 = { TN , TA , NT } 0 = { o , 1,2,3 } i = 0 , F = { 3 } 0c4tl =1 , O , t1 =TN , 1®t22 =2 , l*t~=G 2®t~ &lt; =3 , O*twi =TN-NT , 2 @ t.~6 =3 , O * t.2s = TA , 2 ® twj =3 , O * t , u = TA_NT , p ( 3 ) = Deternfinistic FST resulted from Figure 5 that maps Q x E1 on E~ ; p : F -- ) 22~ is the final outtmt fluiction. Our teniporal co-occurrence data can lie relivesented with a deterministic finite state transducer 958 } ¢~g~'tl\ [ 1111 N , bdli # ~\ [ } ~¢~CUUltl N I~ ) colJanglNI i , *. ! N I~mylHL'IN I / ) c , nc ~I'A ; . , ) : '' '' . ( &amp; , jin ; ulY\ [ N/ ~¢ , w , , , , , n X , j. , , , , , l , x I ~ c ( ~etll , l/I N ila ¢11 a , '\ [ \ [ 'A jln ; m/I N ¢ , l¢lbrlltl A ilajlclmiN I k , e/IN ) , , , reumflA ¢,25/NI kw/IN ~¢ , liclnllJ\ [ a ilk ij~tll Ik~\ ] N I \ d ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... . \ , ... ... ... ... . ( '/ Figure 5 : Fiifite sl ; al ; e nta ( 'hin ( ~ ( : onsl ; rucl ; ed wil ; h l ; he ( bd ; a in Figure d O t'/ : '' H- ( ) wl / ! l'N , NT ' t2'- ' ( ' ~ ( ) ! ~ ' ; IT A u , j /'I'A , NT ll '' : : { barn , jumol , t , altgha 9 ... . } mi ( i lI '' wj ~ 1V Figure 7 : A d ( ; 1 ; erministic tinil , e sl , aW , ( , ransduc ( 'a ' 1 ; o i ) roce , s , s temi ) oral ex\ ] ) re , ,qsion in a similar way. The , sut ) s ( 'qu ( ' , ntial FST f ( ) r our sysl , em is ( l ( , ,fined as in Figure ( i and Figur ( ~ 7 ilhLsl.ral ; es I ; he tral~sdu ( : ( ! r in l ? igur ( ~ 6. In L\ ] m tiI~me , ti is a c\ ] ass 1 : o whi ( : h lhe ( : eml ) oral w ( ) rd 1 ) elongs in l ; he lx ' , mporat ( : las , qiti ( 'at ; ion. wi is a word ol ; her l ; han l , em1 ) oral ones 1 ; hal ; has l ; he pr ( ' , ( : ( ~ ( ting t eml ) ( ) ral wor ( l 1 ) ( ' , il : s modiiier , and wj is not ; such a word 1 ; ( ) make a compound noun. TN , TA and NT are synt , aciic tags. A word t ; agged with 5/'N would modify a su ( 'ceeding llOllll like , barn ( night ) , bangh.ag ( vacati ( m ) . A word al : t , ached with TA would lnodify a predica.lx~ ' aim one with NT nmans ii ; is not ; a 1 ; emporal word. A ( : mally , individual FSTs are coml ) in ( ; d into one aim rules for tagging of temporal words are pul ; over l ; h ( ; . , J. The rule is applied according to the prioriW by freqllellcy ill case lllOrl2 t ; hall ( ) lie ( ) ll { ; l ) ill ; are \ ] ) ossible for a ( : Oilt ; ex| ; . Nmnely , it ; is a rule-l ) ased system W\ ] I ( ) I ' ( I ~\ ] le rllles al'e , ( ~xl ; ra ( ; | ; ( ? ( l frolll ( ; ort ) llS. Afl ; er the FST of l ; enlt ) oral ( ' , xt ) ressions adds I ; o woMs syntactic tags such as TN and TA , chunking is conducted with l'eSlllI ; s frolll OllI ; l ) llI ; S 1 ) y t ; h ( ' FST. As we said earlier , ( : hunldng in Korean is relal ; iv ( ; ly easy only if t ; h0 , t ; eml ) oral exi ) l'essioll wou\ ] ( t be successfully recognized. Act ; ua.lly , our ( 'hunker is also based on the , finil ; e s ( ; a , l ; e machine. The following is an exmnl ) le for ( : hunldng rules. iN1 , ) ~ ( NF ( NP ) I ( 2V ) * ( 2VIO* ( UN ) ( 'rNI , ) ~ { TN ) * ( N ) * ( XP ) \ ] \ ] ( ! re : j\r is a noun wil ; h ( ml , rely \ ] ) ost.t ) osit.ion , NP is a noun wit.h a. 1 ) oSl ; l ) OSil , ion , TN is a t ; enll ) oral noml recogniz ( ~ , d as modifying a suc ( : ox'ding n ( mn , NU is a number and UN is a uni ( ; n ( mn. Afl ; e , r t ( 'mporal l.a.gging , 1 ; he ( 'hunker l ; ransforms 'NT ' into N , NP , ( d , ( ' , . according I , o morl ) hologi ( : al consi ; itueid ; s and their I ) OS. I h'io , tty , t ; he , rule says thai ; an NP ctmnk is mad ( ' , from eil3mr NI ' or l ; emporal NIL An NP would \ ] ) ( ! ( : ( ) llsLrll ( 'l , ( ' , ( 1 wii , h on ( ; or lll ( ) l ' ( } llOllll.q ; ill ( 1 \ [ ; boil '' modilie ( ~ or with a noun ( lUanl ; ified. A TN\ ] ) , whi ( : h is r ( ' , lal : ed with lime , is made from n ( mns moditied by t ( 'ml ) oral words wlfi ( : h would 1 ) ( ' , i ( t ( ; nl ; itied by the FST. By i ( l ( mtifi ( : ation of lx ! mporal ( ' , xpressi ( ) n and chunking , tlm following ( ' , xmnl ) k ' , senl~elu : e , is chunked as | ) ( ; low. • jinan ( lasQ ycor~'um ( summer ) bau , ghag-e ( in • s * `` v , , , : , ,. ( ; io , , ) , , , . , . , ,- , ,. , ; , , , ,4 , , , ~./s un.~ ) k~o , , , ,VV , ,t ( .~o ( , .o , m , , , ~ ; ( , F ) . , ,~ , ( th~. , ,o~ ) d ( .~'- , ~ ' , ,l ( . , ,iqOl~.J ) sassda ( bought ; ) -+ \ , Vo , bought l ; hrce c ( ) mlmt ( ~'rs in the lasl ; 81111111101 '' V~IC~I ( ; i ( ) ll. • jin ( t ? Vl , N ' , ~l ( : OTC'tt'lllq'N balzgha ( j-CNl , '~tri-nc'ltlZNl , kco'vnl ) yUl , eON SCNU dae-reUINl , sassdav • \ [ jinawl'N yCOVCUmTN ban ! lh , ag-CNl , \ ] Nc \ [ 'uri-ne , wnNP\ ] NC : \ [ kcompyuteoN SeN¢ : dac ? 'C ? tlNI , \ ] N ( ; sassdav For l ; hc ext ) erinmnl ; a.bouL l ; eint ) oral expre , ssion , we e , xla'aci ; ed 300 senl ; enc ( '~s ( : onl ; a.ining temporal expressions from E\ ] ? I { I I ) ( ) S cortms. Table 2 shows the r ( ',959 ~c'*¢unnrL ~ ( y~ , ~cl , , , ,ItX { ya~cu , , , fl N ( ) ¢ , ~c , , , n/I N~ ( ) b'un*N'l ~ : : ) t ' ) ¢ '' II : ~'U'/N.I i g ; , , , ,~ , lXl~ , .¢aadD~ . xy¢l } l¢onlfI'A / ) ! + , ,+ , , , aN ) ++ , .+ , , , , , , , A &lt; 2 '' : ''L '' ''~'L ,2 , ~ , . , ax - ( y ? , ,~ , , , ,~ , L~ / , ,~j , ,. , , , .xr ( ; , ~ Figure 5 : Finite state machine constructed with the data ill Figure , 4 wi/TN , NT .. ti/TN C~ 1.22/ { F , / '' t2o/'l'A `` `` '' 0 2 \ -- / '' wj/TA , NT IV = { barn , jumal , ba~dhao , ... } Wl G IV wj ~ II / Figul'e 7 : A { lel ; ( '~rministi { : finil ; { ; stat ( ; trans { lucer t { } process temporal expression in a similar way. The subs ( xlU { '.ntial FST for our sy : stem is detined as in Figure { i and l ? igu\ ] { ~ 7 illustrates th { . ~ trans ( hl { : er in Figure. ( i. In the tigurc , ti is a { 'lass to which the tc.mporal w { } rd 1 } elongs in the. temporal classification. `` wi is a word { } the.r than temporal ones that has | ; 11 { ; prex : e.ding temporal word be its moditier , and 'wj is not such a word to make a COml ) Oui : d noun. TN , TA and NT are synta { 'ti { : tags. A word tagged with TN would mo ( lit~y a suc ( : ceding noun like barn ( night ) , ban.qh , ag ( vacation ) . A word attached with TA wouhl mo { lii2y a predicate and oi1 { ; with NT means it is not a temporal word. Actually , individual FSTs arc { : oml } ined int { ) one. an ( l rules for tagging of teml } oral wor { ts are put over the. FST. The rule is al } plied according to the priority by fro ( tllOll { ; y ill case m ( } re than o11o ( } uttmt are possible for a context. Namely , it is a rule-based system where the rules are extracted fi'om corl } us. After the FST of temporal exi } ressiolls adds to words syntactic tags such as TN and TA , chunking is { : onducted with results tiom outl ) uts 1 ) y the Fsr\ ] '. As we said earlier , { : lmnking in Kore.an is relatively easy only if the teml } oral ext } ression would l ) e suc { : e.ssfltlly recognized. Actually , our clmnker is also 1 ) ased on the finite , state lnachine. The tbllowing is an example tbr chunking rule.s. ( Nl~h , . , ~ ) -÷ ( NP ) I ( TNP ) ( NP ) -~ ( N ) * ( NP ) I ( N ) * ( Nu ) * ( uN ) ( 'rNu ) -~ ( 'rN ) * ( N ) * ( NP ) Here , N is a noun without any 1 ) ostl ) osition , N/ ? is a noun with a postposition , TN is a temporal noun recoglfized as modii~ying a succeeding 1101111 , NU is a numbe.r and UN is a unit noun. Aft , er tcmI ) oral tagging , the chunker transforms 'NT ' into N , NP , ( ~tc. according to morphological constituents and their POS. Brietly : the rule. says that an NP clmnk is made fl'om either NP or temporal NIL An NP would 1 ) e ( : onst , rll ( ; te. ( 1 with one. or lnOl'O llOlllIS and their m { } { lifie { ~ { ) r with a noun quantified. A TNP , whi { : h is re , lated with time , is made fr { ) m nouns mo ( liticd by teml } oral words which w { mld be ide , ntitied by the , FST. By identification { ) f t ( 'mt ) { } ral ex\ ] } ression and chunldng , the following exami ) le sentence is ctmnked as below. • jinan ( last ) ycorcum ( summer ) ban.qhag-c ( in vacation ) 'ari-ncun ( we/SUB3 ) kco 'm , pyu. &lt; thr , , { , ) sa.ssda ( bought ) -+ \VC bOllght 1 ; t117o.o comi ) uters in the last Sllllllller vacal0io\ ] l. • jinanTN ycoreumTN banghag-cN1 , uvi-ncunNp kcom , pyuteoN SeNU dac-vculNp sassdav • \ [ jinanTN yeorelt~lZ , l , N ban.qha.q-cNP\ ] N ( ~ ' \ [ Iwi-TtC'lt*tNP\ ] N C \ [ kcompyutcoN SCNU dacrculN P\ ] N ( ; sassdav bbr the { ; xi } erinmnt al } out temporal exI } ressi { m , we extracted 300 senten { : es containing teml } oral expressi ( ms from ETRI POS corlms. Tal } le 2 shows the r { '.959 t precision J_£gcall rate ( % ) 97.5 90.56 Table 2 : Results of identifying temporal expression no chunking-\ [ using chuifldng Table 3 : Reduction of candidates resulted from chunking sults from identit : ying temporal expressions and disaml/iguatil~g their syntactic functions. From the result in the table we see that the method is very effect , ive in that it very accurately identifies all tile temporal expressions aim assigns them syntactic tags. And , Table 2 shows the reduction resulted from chunking after temporal expression identification. We take into consideration the average numl ) cr of head candidates for each word since our parser is dependency based one. The test was conducted on the tirst file ( about 800 sentences ) of KAIST treebank ( Choict al. , 1994 ) . The number was reduced by 51 % in candidates compared to the system with no chunking , whMl makes pa.rsing efficient. Most of errors were caused by tile case where teinporal words have different syntactic roles under the same context. In this case , the global context such as the whole sentence or intcrscnt ; cntial infornlation or sometimes very soi ) histicated processing ix needed to resolve the prol ) lem , l~br instance , '82 ~tycoTt ( year ) h , yco'njac-yi ( now/Gl ; N ) ' could be used two-.way. If the speech time is the year 1982 , then h , yeou ( fl , e-yi are conlbined with 82 nycon to represent time. Otherwise , 82 does not ino ( til~y hycordac-yi , wlfich can not be recognized only with the local context. Nevertheless , the system is promising in that generally it can ilnprove e\ [ flciency without losing accuracy which is crucial for the pracl ; ical system. hi this paper , we presented a method for identification of temporal exi ) ressions and their syntactic functions based on FST aim lexical data extracted fl : om corpus. Since tenlporal words have the syntactic ambiguity when used in a sentence , it ; is impo &gt; tant to identify the syntactic functioll as well as the temporal expression itself .</sentence>
				<definiendum id="0">FST</definiendum>
				<definiendum id="1">®</definiendum>
				<definiendum id="2">ti</definiendum>
				<definiendum id="3">word al</definiendum>
				<definiendum id="4">e s</definiendum>
				<definiendum id="5">UN</definiendum>
				<definiendum id="6">NP</definiendum>
				<definiendum id="7">TN</definiendum>
				<definiendum id="8">NU</definiendum>
				<definiendum id="9">UN</definiendum>
				<definiendum id="10">hc ext</definiendum>
				<definiendum id="11">ti</definiendum>
				<definiendum id="12">N</definiendum>
				<definiendum id="13">TN</definiendum>
				<definiendum id="14">NU</definiendum>
				<definiendum id="15">UN</definiendum>
				<definiendum id="16">whMl</definiendum>
				<definiens id="0">Example concordance data of yeorcum ( summer ) befbre temporal noml after outlmt freq yeoreum</definiens>
				<definiens id="1">Temporal expression phrases selected fronl examl ) les tuple ( Ej , E2 , Q , i , F , E ) where : E1 is a finite input alphabet ; E2 in a finite output alphabet ; Q in a tinite net of states or vertices ; i E Q in the initial state</definiens>
				<definiens id="2">a eight-tui ) le ( El , E2 , Q , i , F , ® , * , p ) where : E1 , E2 , Q , i and F are the smnc as the FST ;</definiens>
				<definiens id="3">the deterministic state transition fimetion that maps ( ) x E1 on Q ; • is the deterministic emission fimction Figure 6 : T = ( ~ , , r , .~ , O , i , F , o , . , p } ) ) 2~ = { tl , t~2 , t~6 , wi , wj } E2 = { TN , TA</definiens>
				<definiens id="4">a noun wit.h a. 1 ) oSl ; l ) OSil , ion ,</definiens>
				<definiens id="5">a t</definiens>
				<definiens id="6">a number</definiens>
				<definiens id="7">a { 'lass to which the tc.mporal w { } rd 1 } elongs in the. temporal classification. `` wi is a word { } the.r than temporal ones that has | ; 11 { ; prex : e.ding temporal word be its moditier , and 'wj is not such a word to make a COml ) Oui : d noun. TN , TA and NT are synta { 'ti {</definiens>
				<definiens id="8">a rule-based system where the rules are extracted fi'om corl</definiens>
				<definiens id="9">a noun without any 1 ) ostl ) osition</definiens>
				<definiens id="10">a numbe.r and</definiens>
				<definiens id="11">dependency based one. The test was conducted on the tirst file ( about 800 sentences ) of KAIST treebank ( Choict al. , 1994 ) . The number was reduced by 51 % in candidates compared to the system with no chunking</definiens>
				<definiens id="12">makes pa.rsing efficient. Most of errors were caused by tile case where teinporal words have different syntactic roles under the</definiens>
				<definiens id="13">the whole sentence or intcrscnt ; cntial infornlation or sometimes very soi ) histicated processing ix needed to resolve the prol</definiens>
				<definiens id="14">identification of temporal exi ) ressions and their syntactic functions based on FST aim lexical data extracted fl : om corpus. Since tenlporal words have the syntactic ambiguity when used in a sentence</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>For Stochastic Tree-Substitution Grammars ( STSG ) used by DataOriented Parsing ( DOP ) models , it has been observed lhat the shortest derivation of a sentence consists of the largest subtrees seen in a treebank thai generate that sentence ( of .</sentence>
				<definiendum id="0">Stochastic Tree-Substitution Grammars</definiendum>
				<definiendum id="1">STSG</definiendum>
			</definition>
			<definition id="1">
				<sentence>l ) ifferent derivation yielding the same parse true lbr She saw tile &amp; 'ess with tile telescope Note also that , given this example corpus , the sentence we considered is ambiguous ; by combining ( Hoogweg 2000 ) , Tree-Adjoining Grammar ( Neumalm 1998 ) , Lexical-Functional Grammar ( Bed &amp; Kaplan 1998 ; Way 1999 ; Bed 2000a ) , Head-driven Phrase Structure Grammar ( Neumann &amp; Flickinger 1999 ) , and Montague Grammar ( van den Berg et al. 1994 ; Bed 1998 ) .</sentence>
				<definiendum id="0">Montague Grammar</definiendum>
				<definiens id="0">Head-driven Phrase Structure Grammar ( Neumann &amp; Flickinger 1999 ) , and</definiens>
			</definition>
			<definition id="2">
				<sentence>In Slmnnon 's Information Theory ( Shannon 194~ , ) , tile negative logaritlun ( of base 2 ) of the probability of an event is belter known as the information ( 51 ' that event .</sentence>
				<definiendum id="0">probability of an event</definiendum>
				<definiens id="0">the information ( 51 ' that event</definiens>
			</definition>
			<definition id="3">
				<sentence>`` Using an Annotated Corpus as a Stochastic Grammar '' , EACL '93 , Utrecht , The Netherlands .</sentence>
				<definiendum id="0">Annotated Corpus</definiendum>
				<definiens id="0">a Stochastic Grammar ''</definiens>
			</definition>
			<definition id="4">
				<sentence>`` Monte Carlo Sampling Ior Nl'-hard Maximization I'roblems in the l '' ramework of Wc ghtcd Parsing '' , iu Natmcd l.altguag &lt; ! l ) roce.ssing -NI.I ' 2000 , l.eclure Not , : ! . , ; i , q Arttl/h : ial lnlelligence 1835 , 1 ) . Chrislodoulakis ( ed. ) , 2000 , 106-117. Charniak , 1 ' ; . 1996. `` Tree-bank Grammars '' , Proceedings AAAI-96 , Menlo Park , Ca. Charniak , 1 ! . 1997. `` Statistical Parsing wilh a Contcxl-l : rcc Grammar and Word Statistics '' , l'roceedinw AAAI-97. Menlo Park. Ca. Charniak , E 2000. `` A Maximimum-Entropy-Inspired Parser '' , Proceedings ANLP-NAACL '2000 , Scnttlc , Washington. Chitrao , M. and P , . Grishman , 1990. `` Statistical Parsing of Messages '' , Proceedings DARPA Speech and Language Workshop 1990. Collins , M. 1997. `` Three generalive lexicalised models for statistical parsing '' , Proceedings EACL/ACL'97 , Madrid , Spain. Collins , M. 1999. llead-l ) riven Statistical Models./br Natural I , anguagc I'arsing , Phl ) -thesis , University of Pennsylvania , PA. I ) aclcmans , W. ( ed. ) 1999. Memmy-Ilased Language Processing , Journal./br l ' ; .vperimental and Theoretical Arti/i'cial Inlelligence , 11 ( 3 ) . Eisner , J. 1997. `` Bilexical Grammars and a Cubic-Time Probabilistic Parser '' , Proceedings of International Workshop on Parsing Technologies Boston , Mass. Goodman , J. 1998. Palwing lnsMe-Out , Ph.l ) . thesis , l larwu 'd University , Mass. l loogweg , 1.. 2000. Extending DOPI with the Insertion Operation , Maslel '' S thesis , University of Amsterdam , The Netherhmds. Johnson , M. 1998. `` The I ) OP Estimatkm Method is Biased and Inconsistent '' , squib. l , ari , K. and S. Young 1990. `` The Estimation of Stochastic Context-Free Granunars Using the lnside- ( ) utsidc Algorithm '' , ( ) mtl~ltlel '' , ~'1~ ( 3cc11 alld l , angttctge , d , 35-56. Magerman , D. and M. Marcus , 1991. `` Pearl : A Probabilistic Chart Parser '' , Proceedings EACL '91 , Berlin , ( iermany. Marcus , M. , B. Santorini and M. Marcinkiewicz , 1993. Building a Large Annotated Corpus of English the Penn Treebank '' , Computational Linguistics 19 ( 2 ) . Neumann , G. 1998. `` Automatic Extraction of Stochastic Lexicalized Tree Grammars from Treebanks '' , Proceedings of the 4th Workshop on Tree-Adjoining Grammars and Related Frameworks , Philadclplaia , PA. Neumann , ( 3. and I ) . Flickinger , 1999. `` l~earning Stochastic l , exicalizcd Tree ( \ ] lanllnars frolll IIPSG '' , I ) FKI Technical l ( epoit , Saartwiicken , Germany. Ilalnaparkhi , A. 1999. `` l , carning to Parse Natural 14mguage with Maximum linlropy Models '' , Machit~e /.earning 34. 151-176. S\ [ laIlllOll , C. 1948. A Mathematical Theory of Communication. IMII System Technical .Iota'hal. 27 , 379-423 , 623656. Sima'an , K. 1995. `` An optimized algorilhin for l ) ata Oriented Parsing '' , in : R. Milkov and N , Nicolov ( eds. ) , Recent Advam : es in Natttral l , alsguage l'rocessing 1995 , w &gt; lume 136 of Current lsstws in Linguistic T/woO ' .</sentence>
				<definiendum id="0">Maximimum-Entropy-Inspired Parser</definiendum>
				<definiendum id="1">Pearl</definiendum>
				<definiens id="0">A Probabilistic Chart Parser ''</definiens>
			</definition>
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>The Celex datM ) ase contains fiequen ( : y infi ) rlnation ( based on the INL corl ) uS of the hlsl ; itute fi ) r 1 ) ul ; ( : h Lexieology ) , and i ) honologi ( : al~ morl ) hologicM , and synt ; a ( : tic lexicM intbrmation tbr more l ; tmn 384.000 word forms , 327 and uses DISC as encoding tbr word pronunciation .</sentence>
				<definiendum id="0">Celex datM</definiendum>
				<definiendum id="1">itute fi</definiendum>
				<definiens id="0">tic lexicM intbrmation tbr more l ; tmn 384.000 word forms , 327 and uses DISC as encoding tbr word pronunciation</definiens>
			</definition>
			<definition id="1">
				<sentence>The Fonilex database is a list of more than 200.000 word tbrms together with their Flemish pronunciation .</sentence>
				<definiendum id="0">Fonilex database</definiendum>
				<definiens id="0">a list of more than 200.000 word tbrms together with their Flemish pronunciation</definiens>
			</definition>
			<definition id="2">
				<sentence>igure 1 : Architecture of the learning process m~king use , of TBEDL C5.0 ( Quinlnn , 1993 ) , on the other hand , wMch is a commercial version of the C4.5 t ) rogr~mh gener~l ; ( ; s a classifier in the form of ~r decision tree .</sentence>
				<definiendum id="0">wMch</definiendum>
				<definiens id="0">a commercial version of the C4.5 t ) rogr~mh gener~l ; ( ; s a classifier in the form of ~r decision tree</definiens>
			</definition>
			<definition id="3">
				<sentence>This decision tree ( : ~Lll \ ] ) e used to cLassi ( y ; ~ case 1 ) y starting a.t the root of | ; 11 ( ; I ; ree mM then moving througLl the tree untiL a le~ff node ( associated with ~ class ) is eneomltered .</sentence>
				<definiendum id="0">le~ff node</definiendum>
				<definiens id="0">~Lll \ ] ) e used to cLassi ( y ; ~ case 1 ) y starting a.t the root of |</definiens>
			</definition>
			<definition id="4">
				<sentence>The C5.0 produ ( : tion rules , Oll the other hand also descrit/e the overlapl ) ing phonelnes between Celex and Dmilex , which makes it hard to have at clear overview of the regularities in the dilt'erences 1 ) etween both variants of Dutch .</sentence>
				<definiendum id="0">C5.0 produ</definiendum>
				<definiens id="0">tion rules</definiens>
			</definition>
			<definition id="5">
				<sentence>q , ; , Q } 1 - &gt; class y \ [ 0.996\ ] Rule 683 : ( 7749/29 , lift 110.91 fl in { = , { , ~ : } f in { x , , q } - &gt; class , g \ [ 0.996\ ] Another important phenomenon is the use of p~tlatalisation in Flemish , as in the word `` aaitje '' ( Eng. : `` stroke '' ) , where Folfilex uses the t ) alatalized fortll/aljtJ'o/instead of/a : jtjo/ .</sentence>
				<definiendum id="0">Folfilex</definiendum>
			</definition>
</paper>

		<paper id="2131">
			<definition id="0">
				<sentence>-I ( p2 ) Figure 1 : Translation Example by Examt ) le-based ~li'anslation dependency structure .</sentence>
				<definiendum id="0">-I</definiendum>
				<definiens id="0">Translation Example by Examt ) le-based ~li'anslation dependency structure</definiens>
			</definition>
			<definition id="1">
				<sentence>le , ct one of candidates , rather it is a new finding of word corre , spondence by utilizing a special structm : e. For instance , in Figure 3 , if there is a word eorrespol &gt; dence 1 ) etween `` ki '' and `` period '' and there is no word correst ) ondence between `` ikou '' and `` transition , '' then I &lt; V , g ( iko'u~ transition ) will be found 1 ) 3 ' this 1 ) roeedure. These WX and WS t ) rocedures are continuously al ) plied until no new word correspondences arc t ' ( mnd. Aft ; er al ) l ) lying the above WX and I'VS pro ( : edures , there are some target words t such that t is a destination of a l , l/C ( .s `` , t ) and there ix no other 1,176 , whose destination ix t : . In this case , the lUG ( s , t ) correspondence candidate is chosen as a valid word correspondence between s and/ , , and it ; is called a HzZ word eorrest ) ondence. We call a source node or a target node of a word correspondence an anchor node in what tbllows. The above t ) rocedures for finding word corresI ) ondences are summarized as follows : Find WCs by consulting translation dictionary ; Find WAs ; while ( true ) { find WXs ; find WSs ; if no new word corresp , is found , then break ; } find WZs ; The next step is to tind phrasal correspondences based on word eorl'eSl ) ondences t ' ( mnd t ) 3 , 1 ) roce.dures described in tim previous section. What we would like to retrieve here , is a set of phrasal correspondences which ( : overs all elements of a paired dependency trees. In what follows , we ( : all a portion of a tree which consists of nodes in a 1 ) att~ from a node ? t I ( ; o alloth ( ; r node nu which is a descen ( lanl ; of n : l a lin-. ear tree denoted by LT ( v,1 , n~ ) , and we denote a minimal sul ) tree including st ) coiffed nodes hi , ... , n.~ , l ) y T ( nl , ... , n , ) . For instan ( : ( , ~ in the English tree structure ( the right tree ) in Figure 4 , LT ( tcch , nology , science ) is a rectangular area covering % eclmol• tg `` e e ~ ogy , '' and SOl , no , , anti .T ( J'acl ; or , cou'ntrjl ) is a 1 ) olygonal area covering `` factor , '' '' atDcl , , ... . t ) olicy , '' and `` country. '' The tirst step is to find a 1 ) air of word correst ) ondences W , ( .~'~ , t , ) and ~4q ( . , .~ , t , ~ ) such that . , , a. , t s2 constructs a linear tree LT ( si , s2 ) and there is no anchor node in th ( ' 1 ) al ; h from s~ to s2 other than .s'~ and .s2 , where 1UI and H~ denote any tyi ) e of word ( 'orrest ) on ( lences 2 and we assmne there is a word corresI ) ondence t ) etwee , n roots of source and ( ; arget trees by defmflt. We construct a t ) hrasal correspondence fi'om source nodes in LT ( s , ,s2 ) and target l/o ( les itl r\ ] ' ( t : l , /'2 ) , ( l ( ) llote ( t by \ ] ; ' ( l~ , ~F ' ( .q'l , . '' ; 2 ) , 5\ ] .n ( tl , t2 ) ) . For illstall ( 'e~ ill Figllre 41~ \ ] '' 11~ \ ] ~12~ 1 ) '2~ 1 ) 3 and \ ] ) 4 tu.'e source portions of phrasal et ) rrespondences found in this step. The next stel ) checks , for ea ( : h 1 ' , if all anchor llo ( les of wor ( 1 eorres1 ) Oll ( leile ( ? s wllose SOUlT ( ; e o1 ~ ; alget node is included in P are al , eo included in P. If a t ) hrasal correst ) ondenee satisiies this condition , then it is called closed , otherwise it ix called open. Further , nodes which are not included in the I ) in question are called open nodes. If a l ) ix ot ) en , then it ix merged with other 1 ) hrasal correspondences having ol ) en nodes of P so that the merged 1 ) hrasal correspondence becomes ( -losed. Next , each P~ , , is checked if there is another l ) q which shares any nodes ottmr than anchor nodes with P. , ,. If this is the case , these P : . , and 1~ are lnerged into one phrasal correspondence. In Figure 4 , t ) hrasal correspondences i 11 and P12 are merged into P1 , since their source I ) ortions LT ( haikei , koku ) and LT ( haikci , seisaku ) share `` doukou '' which is not an anchor node. Finally , any path whose nodes other than the root are not included in any 1 ) s but the root node ix included in a 1 ) is searched for. This procedure 2Since WC is not a word correspondence ( it is a candidate , of word corresi ) ondence ) , it is llOi ; conside , red here. 908 is apl ) lied I ; o 1 ) oth source a.nd ( ; arget trees. A im.th found 1 ) y this 1 ) ro ( : ( xlur ( ~ is called an open pal , h , , m~ ( t its root no ( le is called a pivot. If such an Ol ) en path is found , it is t ) rocessed as follows : l , br each 1 ) ivot node , ( a ) if the t ) ivot is not an mmhor nod ( ; , then open lmths originating fl : om the pivot is merged into a 1 ) having I ; he pivot , ( b ) if the pivot is an ~LIlChOf llo ( lo~ { ; hOll 3_ llOW t ) hl'~lS~L1 c ( ) rFos1 ) oII ( | ( ~IlC ( ~ , iS created from Ol ) ( m 1 ) ai ; hs originating from the m &gt; thor nodes of the word ( : orrcsl ) on ( l ( : ncc .</sentence>
				<definiendum id="0">I'VS pro</definiendum>
				<definiendum id="1">lUG</definiendum>
				<definiendum id="2">anti .T</definiendum>
				<definiendum id="3">LT</definiendum>
				<definiens id="0">a word eorrespol &gt; dence 1 ) etween `` ki '' and `` period '' and there is no word correst ) ondence between `` ikou '' and `` transition , '' then I &lt; V , g ( iko'u~ transition ) will be found 1 ) 3 ' this 1 ) roeedure. These WX and WS t ) rocedures are continuously al ) plied until no new word correspondences arc t '</definiens>
				<definiens id="1">called a HzZ word eorrest ) ondence. We call a source node or a target node of a word correspondence an anchor node in what tbllows. The above t ) rocedures for finding word corresI ) ondences are summarized as follows : Find WCs by consulting translation dictionary</definiens>
				<definiens id="2">to tind phrasal correspondences based on word eorl'eSl ) ondences t '</definiens>
				<definiens id="3">overs all elements of a paired dependency trees. In what follows</definiens>
				<definiens id="4">a minimal sul ) tree including st ) coiffed nodes hi , ... , n.~ , l ) y T ( nl , ... , n , )</definiens>
				<definiens id="5">a word corresI ) ondence t ) etwee , n roots of source and ( ; arget trees by defmflt. We construct a t ) hrasal correspondence fi'om source nodes in LT ( s , ,s2</definiens>
				<definiens id="6">any path whose nodes other than the root are not included in any 1 ) s but the root node ix included in a 1</definiens>
				<definiens id="7">apl ) lied I ; o 1 ) oth source a.nd ( ; arget trees. A im.th found 1 ) y this 1 ) ro ( : ( xlur ( ~ is called an open pal , h , , m~ ( t its root no ( le is called a pivot. If such an Ol ) en path is found , it is t ) rocessed as follows : l , br each 1 ) ivot node , ( a ) if the t</definiens>
			</definition>
			<definition id="2">
				<sentence>771 745 96.63 612 600 98.03 131 118 90.07 13 12 92.3 15 15 100 Table h Experimental Result of Word Correspondences are word correspondences W ( s1 , tl ) and W ( s2 , t2 ) , s2 is a direct child of St and t2 is a direct child of tl .</sentence>
				<definiendum id="0">t2</definiendum>
				<definiens id="0">a direct child of St and</definiens>
				<definiens id="1">a direct child of tl</definiens>
			</definition>
			<definition id="3">
				<sentence>LTX is a special case of LTY , since Sl and tl of LTX must have only one child node , on the other hand , ones of LTY may have more than two child nodes .</sentence>
				<definiendum id="0">LTX</definiendum>
				<definiens id="0">a special case of LTY</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>Measuring the representativeness ( i.e. , the informativeness or domain specificity ) of a term ~ is essential to various tasks in natural language processing ( NLP ) and information retrieval ( IR ) .</sentence>
				<definiendum id="0">information retrieval</definiendum>
				<definiendum id="1">IR</definiendum>
				<definiens id="0">the informativeness or domain specificity</definiens>
			</definition>
			<definition id="1">
				<sentence>Here , M measures the property and BM estinmtes the average .</sentence>
				<definiendum id="0">M</definiendum>
				<definiendum id="1">BM</definiendum>
				<definiens id="0">measures the property</definiens>
			</definition>
			<definition id="2">
				<sentence>The size of a document set is defined as the number of words it contains .</sentence>
				<definiendum id="0">size of a document set</definiendum>
			</definition>
			<definition id="3">
				<sentence>For instance , in the interval I = { x \ [ 10000 _ &lt; x &lt; 15,000 } , Iog ( LLR ( D ) ) could be approximated by 1.103 + For LLR , we define Rep ( T , LLR ) , the representativeness of T by normalizing LLR ( D ( 7 ) ) by Bu.R ( # D ( 7 ) ) as follows : Rep ( r , LLR ) = 100 x ( Iog ( LLR ( D ( T ) ) ) _ 1 ) . `` log ( Bu , ( # D ( T ) ) ) 322 For instance , when we used Nihon Keizai Shimbun 1996 , The average of I OOx ( log ( LLR ( D ) ) ~log ( BLue ( # D ) ) 1 ) , Avr , was -0.00423 and the standard deviation , cs , was about 0.465 when D varies over randomly selected doctuncnt sets. l ' ; very observed wflue fell within Avs'4-4er and 99 % ot ' observed values fell within Avl±3cs. This hapfmlled in all corpora ( 7 orpora ) we tested. Theretbrc , we can de : fine the threshold of being representative as , say , Aw '' + 40. umoooo ~ : } f'i ( economy ) .__ _ h ... JJ~nionlh ) ! ; ~i'~ ; i/.Jl ) .~ ) ( read ) i i . , ( cipher ) \ ! ! ~ , j~ ! &amp; ( do ) ) ~ 10000 1000 # 1 ) and lid ( T ) Figure 3 Baseline and sample word distribution So \ [ 'ar we have been unable to treat extremely frequent terms , such as -~-~ ( do ) . We therefore used random sampling to calculalc tile 1 @ 1 ) ( 77 LLR ) of a very li'cquent lerm T. II ' the munbcr ot ' documents in D ( 7 ) is larger than a threshold wdue N , which was calculated froln the average number of words contained in a document , N docnmcnts arc randomly chosen from D ( 2 ) ( we used N = 150 ) . This subset is denoted D ( T ) and Re/ ) ( 7 ; LLR ) is delined by 100 x ( log ( LLR ( D ( 7 ) ) ) /log ( BL~ , Se ( # 1 ) ( 7 ) ) ) -1 ) . This is effcctivc because wc can use a well-approximated part of the baseline curve ; it also reduccs thc amount of calctflation required. By using Rel ) ( 77 LLR ) detSned above , wc obtained Rel ) ( -'F g ) , LLR ) = -0.573 , Rel ) ( a ' ) &amp; TJ , llk 7~ ) , LLR ) = 4.08 , and , * ... . Re\ ] ) ( llil-o , LLR ) = 6.80 , which reflect our linguistic intuition. Rep ( T , M ) has the t bllowing advantages by virtue of its definition : ( 1 ) Its definition is mathematically clear. ( 2 ) It can compare high-frequency terms with lowficqucncy terms. ( 3 ) The threshold value of being representative can be defined systematically. ( 4 ) It can be applied to n-gram terms for any n. Taldng topic-word selection for a navigation window for IR ( see Fig. 1 ) into account , we cxamined the relation bctwecn the value of Rel ) ( 7 , M ) and a manual classification of words ( monograms ) extracted from 158,000 articles ( excluding special-styled non-sentential articles such as company-personnel-aflhir articles ) in the 1996 issties of the Nildcei Shinbun. We randolnly chose 20,000 words from 86,000 words having doculnent ficquencies larger than 2 , thcn randomly chose 2,000 of them and classified these into thrce groups : class a ( acceptable ) words uscfill for the navigation window , class d ( delete ) words not usethl for the navigation window , , and class u ( uncertain ) words whose usefulness in the navigation window was either neulral or difficult to judge. In the classification process , a judge used the DualNA VI system and examined the informativeness of each word as guidance. Classification into class d words was done conservatively because the consequences of removing informative words from lhc window are more serious than those of allowing useless words to appear. 3hblc I shows part of the chtssification of thc 2,000 words. Words marked `` p '' arc proper nouns. The difference between propcr nouns in class a and proper nouns in other classes is that the former arc wcllknown. Most words classified as `` d '' are very common verbs ( such as- , J-~ ( do ) and { J~s-~ ( have ) ) , adverbs , demonstrative pronouns , conjunctions , and numbers. It is thereti ) rc impossible to define a stop-word list by only using parts-of-spccch bccausc ahnost all parts-of speech appear in class d words. To evaluate the effectiveness of several lneasures , we compared the ability of each measure to gather ( avoid ) representative ( non-representative ) terms. We randomly sorted thc 20,000 words and then compared the results with the restllts of sorting by other criteria : Rep ( . , LLR ) , Rep ( . , DIFFNUM ) , ( f ( tern~ liequency ) , and tfid.fi The comparison was done by nsing the accunmlated number of words marked by a specified class that appeared in the first N ( 1 _ &lt; N_ &lt; 2,000 ) words. The definition we used for tjidf was Nlota\ [ .t/ira= 4771775 × log N ( r ' where T is a term , TF ( 7 ) is the term frequency of 7 , Nt , , , &lt; , l is the number of total documents , and N ( 7 ) is the number of documents that contain 7 : Figure 4 compares , for all the sorting criteria , tile 323 accumulated number of words marked `` a '' . The total number of class a words was 911. Rep ( o , LLR ) clearly outperformed the other measures. Although Rep ( . , DIFFNUM ) outperformed .tf and tf-idf up to about the first 9,000 monograms , it otherwise under-performed them. If we use the threslaold value of Rep ( . , LLR ) , from the first word to the 1,511th word is considered representative. In this case , the recall and precision of the 1,511 words against all class a words were 85 % and 50 % , respectively. When using tf-idf the recall and precision of the first 1,511 words against all class a words were 79 % and 47 % , respectively ( note that tJ'-idfdoes not have a clear threshold value , though ) . Although the degree of out-performance by Rep ( . , LLR ) is not seemingly large , this is a promising result because it has been pointed out that , in the related domains of term extraction , existing measures hardly outperform even the use of frequency ( for example , Daille et al. 1994 , Caraballo et al. 1999 ) when we use this type of comparison based on the accumulated numbers. Figure 5 compares , for all the sorting criteria , the accumulated number of words marked by d ( 454 in total ) , in this case , fewer the number of words is better. The difference is far clearer in this case : Rep ( . , LLR ) obviously outperformed the other measures. In contrast , tfidJ and frequency barely outperformed random sorting. Rep ( . , DIFFNUM ) outperformed tfand ( f-idfuntil about the first 3,000 monograms , but under-performed otherwise. Figure 6 compares , for all the sorting criteria , the accumulated number of words marked ap ( acceptable proper nouns , 216 in total ) . Comparing this figure with Fig. 4 , we see that the out-performance ofRep ( . , LLR ) is more pronounced. Also , Rep ( . , DIFFNUM ) globally outperformed tf and tf-idf while the performance of ( land tf-idfwcre nearly the same or even worse than with random sorting. IOOO 900 ~00 700 600 500 400 300 10 0 5000 10000 15000 20000 Order • random • Rep ( . , LLR ) a Rep ( . , DIFFNUM ) ~ tfidf * tf Figure 4 Sorting results on class a words 350 300 Z 250 200 . &lt; 150 100 ~g / L 0 5000 10000 15000 20000 Order • random ~ Rep ( . , LLR ) a Rcp ( . , DIFFNUM ) ~ tt : idf • tf Figure 5 Sorting results on class d words p ) a~ 150 Z 100 . &lt; o j~ , ,-o 5 ( 1 { ) 0 I0000 15000 20000 Order • random ~ Rep ( . , LLR ) z~ Rep ( . , I ) IFFNUM ) ~ tl=id\ [ `` • tf Figure 6 Sorting results on class ap words qhble 1 Examples of the classified words chtss a class u class d ~ '' 2 : L ~Y-2 '' ~ 5/ 1.. ' &lt; -~ O'/~s ( `` g ) ( chilly ) ) kT'-I'i ) J ( 83,000,000 ) ( amusement park ) ~'\ [ 'J ? J2 ( depressed ) ~ ) &lt; ? 2 ( greatly ) g ) 3~ ) ~ ( threlerfingletter ) ; ~'~'1 t ( lshigami ) p T-l'flJqM-/- : ( 1 , t46 ) /'7 '' 4 ) 'OM -- JP ( fircwall ) ~ } 5 ' , ; : ( Shigeyuki ) p ~J-~ &lt; ~ ( all ) `` \ [ ~l'~t~ ' , ( antique ) li~¢ ; i , '2 : t , '¢£ ( misdirected ) ~ '' L L ( not ... in the least ) 7 '' \ ] ~ ; /// ( Atlanta ) p ~ } J ( ~A~ ( agility ) In the experiments , proper nouns generally have a high Rep-value , and some have particularly high scores. Proper nouns having particularly high scores are , for instance , the names ofsumo wrestlers or horses. This is because they appear in articles with special formats such as sports reports. We attribute the difference of the performance between Rep ( . , LLR ) and RED ( . , DIFFNUM ) to the quantity of information used. Obviously information on the distribution of words in a document is more comprehensive than that on the number of different words. This encourages us to try other measures of document properties that incorporate even more precise information. 324 monograms When we concentrate on the nlost fi-equent terms , Re/ ) ( . , DIFFNUM ) outperfomlcd Rep ( . , LLR ) in the following sense. We marked `` clearly non-representative terms '' in the 2,000 most frequent monograms , then counted the number of marked terms that were assigned Rt7 ) -values smaller than the threshold value of a specified representativeness ulcasurc. The total number of checked terms was 563 , and 409 of them are identified as non-representative by Rep ( ' , LER ) . On the other hand , Rep ( ° , DIFFNUM ) identified 453 terms as non -- representative. We investigated the rank-correlation of the sorting results for the 20,000 terms used in the experiments described in subsection 4.1. Rank correlation was measured by Spearman 's method and Kendall 's method ( see Appendix ) using 2,000 terms randomly selected from the 20,000 terms. Table 2 shows the correlation between Rep ( , , LLR ) and other measures. It is interesting that the ranking by Rep ( . , LLR ) and that by Rep ( . , DIFFNUM ) had a very low correlation , even lower than with ( f or ( fidf This indicates that a combination of Rep ( . , LLR ) and Rep ( , , DIFFNUM ) should provide a strong discriminative ability in term classification ; this possibility deserves further investigation. Table 2 Two types of Rank correlation between term-rankings by Rep ( . , LLR ) and other measures. Rep ( . , DIFFNUM ) t/=ic ( f tf Spearman -0.00792 0.202 0.198 Kendall -0.0646 0.161 0.153 We examined the robustness of thc baseline fimctions ; that is , whether a baseline function defined from a corpus can be used for normalization in a different corpus. This was investigated by using Re/ ) ( . , LLR ) with seven different corpora. Seven baseline functions were defined from seven corpora , then were used for normalization for defining Rep ( . , LLR ) in the corpus used in the experiments described in subesction 4.1. The per % rmance of the Re/ ) ( , , LLR ) s defined using the difl'erent baseline flmctions was compared in the same way as in the snbsection 4. l. The seven corpora used to construct baseline fhnctions were as follows : NK96-ORG : 15,8000 articles used in the experiments in 4.1 NK96-50000:50,000 randomly selected articles from Ihe whole corpus N K96 ( 206,803 articles of Nikkei-shinhun 1996 ) N K96-100000 : I 0 ( } ,000 randomly selected articles fn } m N K96 NK96-200000 : 2 { } 0,00 ( } randomly selcctcd articles fiom NK96 NK98-1580 { } 0:158,0 { } ( } randomly selecled articles from articles in Nikkei-xhinhun 1998 N ( '158000:158 , { } 00 randomly selected abstracts of academic papers I\ ] 'Olll NACSIS corptl : . ; ( Kando ct al. 1999 ) NC- : \LI. : all abstracts ( 333,003 abstracts ) in the NACSIS coq ) us. Statistics on their content words are shown in Table 3. Table 3 Corpora and statistics on their content words ~~. NK96-OP , G NK96-soooo NKq6-1ooooo NK96-2ooooo fi o|'Iotal words 42,555,095 13,49S,244 26,934,068 53.816,407 ; : ofdillbrent words 210,572 127,852 172.914 233,668 ~~ NK98-158000 NC-158000 NC-AI.I. # , af total v , 'ords 39,762 , 127 30,770,682 64,806,627 # of difliarent words 196,261 231,769 350.991 Figure 7 compares , for all the baseline functions , the accumulated number of words marked `` a '' ( see subsection 4.1 ) . The pertbrmancc decreased only slightly when the baseline defned from NC-ALL was used. In other cases , the difl'erences was so small that they were almost invisible ill Fig. 7. The same results were obtained when using class d words and class ap words. tuoo 9OO 700 -j 5OO 0 2000 40011 ( dRRI XOOH I UO ( lO 12tRR } 14000 160110 I ROOt } 21111 { 11/ Order * random ~ NK96-OR ( i A NK96-5t } 000 NK96-100000 c\ ] NK96-20 { } 000 * NK98-158 { } ( 1 { } + NC-158000 x NC-ALL Figure 7 Sorting results on class a words We also examined the rank correlations between the ranking that resulted from each representativeness measure in the same way as described in subsection 4.2 ( see Table 4 ) . They were close to 100 % except when combining the Kendall 's method and NACSIS corpus baselines. Table 4 Rank correlation between the measure defined by an NK96-ORG baseline and ones defined by other baselines ( % ) NK96NK96NK96NK9g '' ~C1 5800C NC-AI.I. 500 { } 0 I.OOOO 2000 { } 0 158000 Spcarmann 0.997 0.997 0.996 0.999 0.912 0.900 Kendall 0.970 0.956 0.951 0.979 0.789 0.780 These resnhs suggest that a baseline function constructed from a corpus can be used to rank terms in considerably different corpora. This is particularly useful when we are dealing with a corpus silnilar to a known corpus but do not know the precise word distributions in the corpus. The same tdnd of robustness was observed when we used Re/ ) ( `` , 325 DIFFNUM ) . This baseline thnction robustness is an important tbature of measures defined using the baseline based. We have developed a better method -the baseline method -for defining the representativeness of a term. A characteristic value of all docmnents containing a term T , D ( T ) , is normalized by using a baseline function that estimates the characteristic value of a randomly chosen doculnent set of the same size as D ( ? ) . The normalized value is used to measure the representativeness of the term T , and a measure defined by the baseline method offers several advantages compared to classical measures : ( 1 ) its definition is mathematically simple and clean ( 2 ) it can compare high-frequency terms with low-frequency terms , ( 3 ) the threshold value for being representative can be defined systcmatically , and ( 4 ) it can be applied to n-gram terms for any n. We developed two measures : one based on the normalized distance between two word distributions ( Rep ( . , LLR ) ) and another based on the number of different words in a document set ( Rep ( o , DIFFNUM ) ) . We compared these measures with two classical measures from various viewpoints , and confirmed that Rep ( , , LLR ) was superior. Experiments showed that the newly developed measures were particularly eflizctive for discarding frequent but uninformative terms. We can expect that these measures can be used for automated construction of a stop-word list and improvement of similarity calculation of documents. An important finding was that the baseline function is portable ; that is , one defined on a corpus can be used for laormalization in a diflbrent corpus even if the two corpora have considerably diftbrent sizes or are in different domains. Wc can therefore apply the measures in a practical application when dealing with multiple similar corpora whose word distribution information is not fully known but we have the inforlnation on one particular corpus. We plan to apply Rep ( . , LLR ) and Rep ( . , DIFFNUM ) to several tasks in IR domain , such as the construction of a stop-word list for indexing and term weighting in document-similarity calculation. It will also be interesting to theoretically estimate the baseline functions by using fundalnental parameters such as the total numbcr of words in a corpus or the total different number in the corpus. The natures of the baseline functions deserve further study. Acknowledgements This project is supported in part by the Advanced Software Technology Project under the auspices of Information-technology Promotion Agency , Japan ( IPA ) . Appendix Asusume that items I1 ... .. IN are ranked by measures A and B , and that the rank of item/ : assigncd by A ( B ) is RiO '' ) ( R~ ( j ) ) , where RA ( i ) eRA ( j ) ( Rl4 ( i ) ¢Ri~ ( j ) ) if i ~j. Then , Spearman 's rank correlation between the two rankings is given as t 6x~j ( R4 ( i ) -R '' ( i ) ) 2 N ( N ~ 1 ) and Kendall 's rank correlation between the two rankings is given as I × ( { # { ( i , j ) I c~ ( &amp; . , ( i ) R A ( j ) ) = cr ( Rz , ( i ) RB ( j ) ) } N C2 # { ( i , ./ ) l cr ( R.4 ( i ) R.I ( J ) ) = -cr ( R~ ( i ) Re ( ./ ) ) } ) , where c~ ( x ) =l ifx &gt; 0 , clse ifx &lt; 0 , c~ ( x ) = -I. Caraballo , S. A. and Charniak , E. ( 1999 ) .</sentence>
				<definiendum id="0">Iog ( LLR</definiendum>
				<definiendum id="1">LLR )</definiendum>
				<definiendum id="2">LLR</definiendum>
				<definiendum id="3">LLR</definiendum>
				<definiendum id="4">BLue</definiendum>
				<definiendum id="5">N docnmcnts</definiendum>
				<definiendum id="6">LLR</definiendum>
				<definiendum id="7">LLR</definiendum>
				<definiendum id="8">LLR ) , Rep</definiendum>
				<definiendum id="9">DIFFNUM ) , ( f</definiendum>
				<definiendum id="10">T</definiendum>
				<definiendum id="11">N ( 7 )</definiendum>
				<definiendum id="12">LLR )</definiendum>
				<definiendum id="13">baseline thnction robustness</definiendum>
				<definiens id="0">Rep ( r , LLR ) = 100 x ( Iog ( LLR ( D ( T ) ) ) _ 1 ) . `` log</definiens>
				<definiens id="1"># D ) ) 1 ) , Avr , was -0.00423 and the standard deviation , cs , was about 0.465 when D varies over randomly selected doctuncnt sets. l ' ; very observed wflue fell within Avs'4-4er and 99 % ot ' observed values fell within Avl±3cs. This hapfmlled in all corpora</definiens>
				<definiens id="2">fine the threshold of being representative as , say , Aw '' + 40. umoooo ~ : } f'i ( economy ) .__ _ h ... JJ~nionlh ) ! ; ~i'~ ; i/.Jl ) .~ ) ( read ) i i .</definiens>
				<definiens id="3">do ) ) ~ 10000 1000 # 1 ) and lid ( T ) Figure 3 Baseline and sample word distribution So \ [ 'ar we have been unable to treat extremely frequent terms , such as -~-~ ( do ) . We therefore used random sampling to calculalc tile 1 @ 1 ) ( 77 LLR ) of a very li'cquent lerm T. II ' the munbcr ot ' documents in D ( 7 ) is larger than a threshold wdue N , which was calculated froln the average number of words contained in a document</definiens>
				<definiens id="4">use a well-approximated part of the baseline curve ; it also reduccs thc amount of calctflation required. By using Rel ) ( 77 LLR ) detSned above , wc obtained Rel ) ( -'F g ) , LLR ) = -0.573 , Rel ) ( a ' ) &amp; TJ , llk 7~ ) , LLR ) = 4.08 , and , * ... . Re\ ] ) ( llil-o , LLR ) = 6.80 , which reflect our linguistic intuition. Rep ( T , M ) has the t bllowing advantages by virtue of its definition : ( 1 ) Its definition is mathematically clear. ( 2 ) It can compare high-frequency terms with lowficqucncy terms. ( 3 ) The threshold value of being representative can be defined systematically. ( 4 ) It can be applied to n-gram terms for any n. Taldng topic-word selection for a navigation window for IR</definiens>
				<definiens id="5">a manual classification of words ( monograms ) extracted from 158,000 articles ( excluding special-styled non-sentential articles such as company-personnel-aflhir articles ) in the 1996 issties of the Nildcei Shinbun. We randolnly chose 20,000 words from 86,000 words having doculnent ficquencies larger than 2 , thcn randomly chose 2,000 of them and classified these into thrce groups : class a ( acceptable ) words uscfill for the navigation window , class d ( delete ) words not usethl for the navigation window , , and class u ( uncertain ) words whose usefulness in the navigation window was either neulral or difficult to judge. In the classification process , a judge used the DualNA VI system and examined the informativeness of each word as guidance. Classification into class d words was done conservatively because the consequences of removing informative words from lhc window are more serious than those of allowing useless words to appear. 3hblc I shows part of the chtssification of thc 2,000 words. Words marked `` p '' arc proper nouns. The difference between propcr nouns in class a and proper nouns in other classes is that the former arc wcllknown. Most words classified as `` d '' are very common verbs ( such as- , J-~ ( do ) and { J~s-~ ( have ) ) , adverbs , demonstrative pronouns , conjunctions , and numbers. It is thereti ) rc impossible to define a stop-word list by only using parts-of-spccch bccausc ahnost all parts-of speech appear in class d words. To evaluate the effectiveness of several lneasures , we compared the ability of each measure to gather ( avoid ) representative ( non-representative ) terms. We randomly sorted thc 20,000 words and then compared the results with the restllts of sorting by other criteria : Rep ( . ,</definiens>
				<definiens id="6">the accunmlated number of words marked by a specified class that appeared in the</definiens>
				<definiens id="7">a term</definiens>
				<definiens id="8">the term frequency of 7 , Nt , , , &lt; , l is the number of total documents , and</definiens>
				<definiens id="9">the number of documents that contain 7 : Figure 4 compares , for all the sorting criteria , tile 323 accumulated number of words marked `` a '' . The total number of class a words was 911. Rep ( o , LLR ) clearly outperformed the other measures. Although Rep ( . , DIFFNUM ) outperformed .tf and tf-idf up to about the first 9,000 monograms , it otherwise under-performed them. If we use the threslaold value of Rep ( . , LLR ) , from the first word to the 1,511th word is considered representative. In this case , the recall and precision of the 1,511 words against all class a words were 85 % and 50 % , respectively. When using tf-idf the recall and precision of the first 1,511 words against all class a words were 79 % and 47 % , respectively ( note that tJ'-idfdoes not have a clear threshold value , though ) . Although the degree of out-performance by Rep ( . , LLR ) is not seemingly large , this is a promising result because it has been pointed out that , in the related domains of term extraction , existing measures hardly outperform even the use of frequency ( for example , Daille et al. 1994 , Caraballo et al. 1999 ) when we use this type of comparison based on the accumulated numbers. Figure 5 compares , for all the sorting criteria , the accumulated number of words marked by d ( 454 in total ) , in this case , fewer the number of words is better. The difference is far clearer in this case : Rep ( . , LLR ) obviously outperformed the other measures. In contrast , tfidJ and frequency barely outperformed random sorting. Rep ( . , DIFFNUM ) outperformed tfand ( f-idfuntil about the first 3,000 monograms , but under-performed otherwise. Figure 6 compares , for all the sorting criteria , the accumulated number of words marked ap ( acceptable proper nouns , 216 in total )</definiens>
				<definiens id="10">more pronounced. Also , Rep ( . , DIFFNUM ) globally outperformed tf and tf-idf while the performance of ( land tf-idfwcre nearly the same or even worse than with random sorting. IOOO 900 ~00 700 600 500 400 300 10 0 5000 10000 15000 20000 Order • random • Rep ( . , LLR ) a Rep ( . , DIFFNUM ) ~ tfidf * tf Figure 4 Sorting results on class a words 350 300 Z 250 200 . &lt; 150 100 ~g / L 0 5000 10000 15000 20000 Order • random ~ Rep ( . , LLR ) a Rcp ( . , DIFFNUM ) ~ tt : idf • tf Figure 5 Sorting results on class d words p ) a~ 150 Z 100 . &lt; o j~ , ,-o 5 ( 1 { ) 0 I0000 15000 20000 Order • random ~ Rep ( . , LLR ) z~ Rep ( . , I ) IFFNUM ) ~ tl=id\ [ `` • tf Figure 6 Sorting results on class ap words qhble 1 Examples of the classified words chtss a class u class d ~ '' 2 : L ~Y-2 '' ~ 5/ 1.. ' &lt; -~ O'/~s ( `` g ) ( chilly ) ) kT'-I'i ) J ( 83,000,000 ) ( amusement park ) ~'\ [ 'J ? J2 ( depressed ) ~ ) &lt; ? 2 ( greatly ) g ) 3~ ) ~ ( threlerfingletter ) ; ~'~'1 t ( lshigami ) p T-l'flJqM-/- : ( 1 , t46 ) /'7 '' 4 ) 'OM -- JP ( fircwall ) ~ } 5 '</definiens>
				<definiens id="11">t , '¢£ ( misdirected ) ~ '' L L ( not ... in the least ) 7 '' \ ] ~ ; /// ( Atlanta ) p ~ } J ( ~A~ ( agility ) In the experiments , proper nouns generally have a high Rep-value , and some have particularly high scores. Proper nouns having particularly high scores are , for instance , the names ofsumo wrestlers or horses. This is because they appear in articles with special formats such as sports reports. We attribute the difference of the performance between Rep ( . , LLR ) and RED ( . , DIFFNUM ) to the quantity of information used. Obviously information on the distribution of words in a document is more comprehensive than that on the number of different words. This encourages us to try other measures of document properties that incorporate even more precise information. 324 monograms When we concentrate on the nlost fi-equent terms , Re/ ) ( . , DIFFNUM ) outperfomlcd Rep ( . , LLR ) in the following sense. We marked `` clearly non-representative terms '' in the 2,000 most frequent monograms , then counted the number of marked terms that were assigned Rt7 ) -values smaller than the threshold value of a specified representativeness ulcasurc. The total number of checked terms was 563 , and 409 of them are identified as non-representative by Rep ( ' , LER ) . On the other hand , Rep ( ° , DIFFNUM ) identified 453 terms as non -- representative. We investigated the rank-correlation of the sorting results for the 20,000 terms used in the experiments described in subsection 4.1. Rank correlation was measured by Spearman 's method and Kendall 's method ( see Appendix ) using 2,000 terms randomly selected from the 20,000 terms. Table 2 shows the correlation between Rep ( , , LLR ) and other measures. It is interesting that the ranking by Rep ( . , LLR ) and that by Rep ( . , DIFFNUM ) had a very low correlation , even lower than with ( f or ( fidf This indicates that a combination of Rep ( . , LLR ) and Rep ( , , DIFFNUM ) should provide a strong discriminative ability in term classification ; this possibility deserves further investigation. Table 2 Two types of Rank correlation between term-rankings by Rep ( . , LLR ) and other measures. Rep ( . , DIFFNUM ) t/=ic ( f tf Spearman -0.00792 0.202 0.198 Kendall -0.0646 0.161 0.153 We examined the robustness of thc baseline fimctions ; that is , whether a baseline function defined from a corpus can be used for normalization in a different corpus. This was investigated by using Re/ ) ( . , LLR ) with seven different corpora. Seven baseline functions were defined from seven corpora , then were used for normalization for defining Rep ( . , LLR ) in the corpus used in the experiments described in subesction 4.1. The per % rmance of the Re/ ) ( , , LLR ) s defined using the difl'erent baseline flmctions was compared in the same way as in the snbsection 4. l. The seven corpora used to construct baseline fhnctions were as follows : NK96-ORG : 15,8000 articles used in the experiments in 4.1 NK96-50000:50,000 randomly selected articles from Ihe whole corpus N K96 ( 206,803 articles of Nikkei-shinhun 1996 ) N K96-100000 : I 0 ( } ,000 randomly selected articles fn } m N K96 NK96-200000 : 2 { } 0,00 ( } randomly selcctcd articles fiom NK96 NK98-1580 { } 0:158,0 { } ( } randomly selecled articles from articles in Nikkei-xhinhun 1998 N ( '158000:158 , { } 00 randomly selected abstracts of academic papers I\ ] 'Olll NACSIS corptl : . ; ( Kando ct al. 1999 ) NC- : \LI. : all abstracts ( 333,003 abstracts ) in the NACSIS coq ) us. Statistics on their content words are shown in Table 3. Table 3 Corpora and statistics on their content words ~~. NK96-OP , G NK96-soooo NKq6-1ooooo NK96-2ooooo fi o|'Iotal words 42,555,095 13,49S,244 26,934,068 53.816,407 ; : ofdillbrent words 210,572 127,852 172.914 233,668 ~~ NK98-158000 NC-158000 NC-AI.I. # , af total v , 'ords 39,762 , 127 30,770,682 64,806,627 # of difliarent words 196,261 231,769 350.991 Figure 7 compares , for all the baseline functions , the accumulated number of words marked `` a '' ( see subsection 4.1 ) . The pertbrmancc decreased only slightly when the baseline defned from NC-ALL was used. In other cases , the difl'erences was so small that they were almost invisible ill Fig. 7. The same results were obtained when using class d words and class ap words. tuoo 9OO 700 -j 5OO 0 2000 40011 ( dRRI XOOH I UO ( lO 12tRR } 14000 160110 I ROOt } 21111 { 11/ Order * random ~ NK96-OR ( i A NK96-5t } 000 NK96-100000 c\ ] NK96-20 { } 000 * NK98-158 { } ( 1 { } + NC-158000 x NC-ALL Figure 7 Sorting results on class a words We also examined the rank correlations between the ranking that resulted from each representativeness measure in the same way as described in subsection 4.2 ( see Table 4 ) . They were close to 100 % except when combining the Kendall 's method and NACSIS corpus baselines. Table 4 Rank correlation between the measure defined by an NK96-ORG baseline and ones defined by other baselines ( % ) NK96NK96NK96NK9g '' ~C1 5800C NC-AI.I. 500 { } 0 I.OOOO 2000 { } 0 158000 Spcarmann 0.997 0.997 0.996 0.999 0.912 0.900 Kendall 0.970 0.956 0.951 0.979 0.789 0.780 These resnhs suggest that a baseline function constructed from a corpus can be used to rank terms in considerably different corpora. This is particularly useful when we are dealing with a corpus silnilar to a known corpus but do not know the precise word distributions in the corpus. The same</definiens>
				<definiens id="12">an important tbature of measures defined using the baseline based. We have developed a better method -the baseline method -for defining the representativeness of a term. A characteristic value of all docmnents containing a term T , D ( T ) , is normalized by using a baseline function that estimates the characteristic value of a randomly chosen doculnent set of the same size as D ( ? ) . The normalized value is used to measure the representativeness of the term T , and a measure defined by the baseline method offers several advantages compared to classical measures : ( 1 ) its definition is mathematically simple and clean ( 2 ) it can compare high-frequency terms with low-frequency terms , ( 3 ) the threshold value for being representative can be defined systcmatically , and ( 4 ) it can be applied to n-gram terms for any n. We developed two measures : one based on the normalized distance between two word distributions ( Rep ( . , LLR ) ) and another based on the number of different words in a document set ( Rep ( o , DIFFNUM ) ) . We compared these measures with two classical measures from various viewpoints , and confirmed that Rep ( , , LLR ) was superior. Experiments showed that the newly developed measures were particularly eflizctive for discarding frequent but uninformative terms. We can expect that these measures can be used for automated construction of a stop-word list and improvement of similarity calculation of documents. An important finding was that the baseline function is portable ; that is , one defined on a corpus can be used for laormalization in a diflbrent corpus even if the two corpora have considerably diftbrent sizes or are in different domains. Wc can therefore apply the measures in a practical application when dealing with multiple similar corpora whose word distribution information is not fully known but we have the inforlnation on one particular corpus. We plan to apply Rep ( . , LLR ) and Rep ( . , DIFFNUM ) to several tasks in IR domain , such as the construction of a stop-word list for indexing and term weighting in document-similarity calculation. It will also be interesting to theoretically estimate the baseline functions by using fundalnental parameters such as the total numbcr of words in a corpus or the total different number in the corpus. The natures of the baseline functions deserve further study. Acknowledgements This project is supported in part by the Advanced Software Technology Project under the auspices of Information-technology Promotion Agency , Japan ( IPA ) . Appendix Asusume that items I1 ... .. IN are ranked by measures A and B , and that the rank of item/ : assigncd by A ( B ) is RiO '' ) ( R~ ( j ) ) , where RA ( i ) eRA ( j ) ( Rl4 ( i ) ¢Ri~ ( j ) ) if i ~j. Then , Spearman 's rank correlation between the two rankings is given as t 6x~j ( R4 ( i ) -R '' ( i ) ) 2 N ( N ~ 1 ) and Kendall 's rank correlation between the two rankings is given as I × ( { # { ( i , j ) I c~ ( &amp; . , ( i ) R A ( j ) ) = cr ( Rz , ( i ) RB ( j ) ) } N C2 # { ( i , ./ ) l cr ( R.4 ( i ) R.I ( J ) ) = -cr ( R~ ( i ) Re ( ./ ) ) } ) , where c~ ( x ) =l ifx &gt; 0 , clse ifx &lt; 0 , c~ ( x ) = -I. Caraballo , S. A. and Charniak</definiens>
			</definition>
			<definition id="4">
				<sentence>DualNA VI : An intbrmation retrieval interface .</sentence>
				<definiendum id="0">DualNA VI</definiendum>
				<definiens id="0">An intbrmation retrieval interface</definiens>
			</definition>
</paper>

		<paper id="2169">
			<definition id="0">
				<sentence>n we ( 'onsl ; rucl , such a. sequence , from a wor ( l la.tl ; ic ( &lt; ? Integrating the ntodel in a lattice algoril ; h m requires three steps : • mapping the word la£tice to a. tag lattice • triggering IPs and extra.cting the possible rel ) ar ; md um/reparans l ) : ~irs • intr &lt; ) ducing new paths to represent tile plausible repa.rans The tag lattice constrnction is adapted from ( Samuelsson , 11997 ) . For every word edge and every denoted POS tag a corresponding tag edge is crea , ted and tim resulting prol ) ability is determined. \ [ I ' a tag edge already exists , tile probabilities of both edges are merged. The original words are stored together with their unique semantic class in a associated list. Paths through the tag graph a.re scored by a IX ) S-trigram. If a trigger is active , all paths through the word before tim ll ' need to be tested whether an acceptable rel ) air segmentation exists. Since the scope model takes at most \ [ 'our words for reparandum a.nd rel ) a.ra.ns in account it is sufficient to expand only partial paths. l ) ; ach of these partial paths is then processed by the scope model. To reduce the se~rch space , paths with a low score can be pruned. Repair processing is integrated into the Verbmobil system as a. filter process between speech recognition a.nd syntactic analysis. This enforces a rep~fir representation that ca.n be intograted into a lattice. It is not possible to lna.rk only the words with some additional information , because a rel ) air is a phenomenon that ( lepends on a path. Imagine that the system has detected ~ repair on ~ certain path in the btttice and marked all words by their top , fir function. Then a search process ( e.g. the parser ) selects a different D~th which shares only the words of the repa.randum. But these words are no reparandum for this path. A solution is to introduce a new path in the. lattice where reI ) arandum a.nd editing terms a.re deleted. As we said betbre , we do not want l ; o delete these segments , so they are stored in a special slot of 1 ; 11o first word of the reparans. The original path can now 1 ) e reconstruct if necessary. To ensure that these new I ) aths are coml ) ~ &gt; ra .</sentence>
				<definiendum id="0">md um/reparans</definiendum>
				<definiens id="0">a corresponding tag edge is crea , ted and tim resulting prol ) ability is determined. \ [ I ' a tag edge already exists , tile probabilities of both edges are merged. The original words are stored together with their unique semantic class in a associated list. Paths through the tag graph a.re scored by a IX ) S-trigram. If a trigger is active , all paths through the word before tim ll ' need to be tested whether an acceptable rel</definiens>
				<definiens id="1">a phenomenon that ( lepends on a path. Imagine that the system has</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>The second method , suggested by Church and Gale ( 1995 ) , models adaptation as the chance of a second lnention ( probability that a word will appear two or inore times , given that it appeared one or more times ) .</sentence>
				<definiendum id="0">probability</definiendum>
				<definiens id="0">that a word will appear two or inore times , given that it appeared one or more times )</definiens>
			</definition>
			<definition id="1">
				<sentence>Pr ( +adapt2 ) is approximated by dJ2/dfl , where c./\ [ 'k is the number of documents that contain the word/ngram k or more times .</sentence>
				<definiendum id="0">c./\</definiendum>
			</definition>
			<definition id="2">
				<sentence>Pr ( +adapt ) &gt; &gt; Pr ( prior ) &gt; Pr ( -adapt ) prior +adapt -adapt source w hostages We find that some words adapt more than others , and that words that adapt more in one year of the AP also tend to adapt more in another year of the AP .</sentence>
				<definiendum id="0">Pr</definiendum>
				<definiens id="0">-adapt source w hostages We find that some words adapt more than others , and that words that adapt more in one year of the AP also tend to adapt more in another year of the AP</definiens>
			</definition>
			<definition id="3">
				<sentence>In general , words that adapt a lot tend to have more content ( e.g. , good keywords for information retrieval ( IR ) ) and words that adapt less have less content ( e.g. , function words ) .</sentence>
				<definiendum id="0">IR</definiendum>
				<definiens id="0">words that adapt a lot tend to have more content ( e.g. , good keywords for information retrieval</definiens>
			</definition>
			<definition id="4">
				<sentence>The neighborhood is the set of words that appear in the k= 10 or k = 100 top documents returned by the retrieval engine .</sentence>
				<definiendum id="0">neighborhood</definiendum>
			</definition>
			<definition id="5">
				<sentence>Adaptation depends more on content than flequency ; adaptation is stronger for content words such as proper nouns , technical terminology and good keywords for information retrieval , and weaker for functioll words , cliches and first nalnes .</sentence>
				<definiendum id="0">Adaptation</definiendum>
				<definiens id="0">depends more on content than flequency ; adaptation is stronger for content words such as proper nouns , technical terminology and good keywords for information retrieval</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>Word sense disambiguation ( WSD ) remains an open probleln in Natural Language Processing ( NLP ) .</sentence>
				<definiendum id="0">Word sense disambiguation ( WSD</definiendum>
			</definition>
			<definition id="1">
				<sentence>In our Bayesian Hierarchical Disambiguator ( BHD ) model , we attempt to address some of the main issues faced by today 's WSD systelns , namely : 1 ) the sparse data problem ; 2 ) the selection of a fi ; ature set that can be trained upon easily without sacrificing accuracy ; and 3 ) the scalability of the systein to disambiguate um'estricted text .</sentence>
				<definiendum id="0">Bayesian Hierarchical Disambiguator</definiendum>
				<definiendum id="1">ature set</definiendum>
			</definition>
			<definition id="2">
				<sentence>The prior term represents the knowledge of how frequently a sense of an adjective is used without any contextual infornmtion .</sentence>
				<definiendum id="0">prior term</definiendum>
				<definiens id="0">represents the knowledge of how frequently a sense of an adjective is used without any contextual infornmtion</definiens>
			</definition>
			<definition id="3">
				<sentence>However , instead of classifying all English nouns , Altavista is again used to provide collocation data on 5,000 nouns for each adjective .</sentence>
				<definiendum id="0">Altavista</definiendum>
				<definiens id="0">again used to provide collocation data on 5,000 nouns for each adjective</definiens>
			</definition>
			<definition id="4">
				<sentence>s ) , the ( lisl ; ribution is rel ) resenl ( ; ( t by the ( : onditional I ) rolial ) ility tal ) les ( C , \ ] ) Ts ) at ea ( 'h nod ( , , su ( : h as I &gt; ( B I l ) , F ) , requiring a ( olal of only 2d prol ) al ) ilitie~s. Not only ( lo th ( , savings he ( : ome more significant with larger networks , lmi ; ( .</sentence>
				<definiendum id="0">ribution</definiendum>
				<definiens id="0">onditional I ) rolial ) ility tal ) les ( C , \ ] ) Ts ) at ea ( 'h nod ( , , su ( : h as I &gt;</definiens>
			</definition>
			<definition id="5">
				<sentence>WordNet hierarchy , so this probability can be seen as the percentage of children actually present , hltuitively , this probability is a form of assigning weights to parts of the network where more related nouns are present in the training set , silnilar to the concept of semantic density .</sentence>
				<definiendum id="0">probability</definiendum>
				<definiens id="0">the percentage of children actually present , hltuitively , this</definiens>
				<definiens id="1">a form of assigning weights to parts of the network where more related nouns are present in the training set , silnilar to the concept of semantic density</definiens>
			</definition>
			<definition id="6">
				<sentence>Therefore , the semantic distance can be seen as the number of traversals 11I ) the network weighted by the number of siblings present in tile tree ( and not by direct edge counting ) .</sentence>
				<definiendum id="0">semantic distance</definiendum>
				<definiens id="0">the number of traversals 11I ) the network weighted by the number of siblings present in tile tree ( and not by direct edge counting )</definiens>
			</definition>
			<definition id="7">
				<sentence>WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">An on-line lexical database</definiens>
			</definition>
</paper>

		<paper id="2170">
			<definition id="0">
				<sentence>For orthography , Mainhmd China uses the Simplified Chinese character set , and Hong Kong uses the Traditional set l~lus 4,702 special local Cantonese Chinese characters ( Hong Kong Government , 1999 ) .</sentence>
				<definiendum id="0">Hong Kong</definiendum>
				<definiens id="0">uses the Traditional set l~lus 4,702 special local Cantonese Chinese characters</definiens>
			</definition>
			<definition id="1">
				<sentence>Chinese is a logographic language .</sentence>
				<definiendum id="0">Chinese</definiendum>
				<definiens id="0">a logographic language</definiens>
			</definition>
			<definition id="2">
				<sentence>The Viterbi algorithm ( Viterbi , 1967 ) is implemented to efficiently compute the maxinmm value of ( 4 ) and ( 5 ) for different choices of 1122 character sequences .</sentence>
				<definiendum id="0">Viterbi algorithm</definiendum>
			</definition>
			<definition id="3">
				<sentence>The Viterbi algorithm substantially reduces the computational complexity flom O ( m '' ) to O ( m.-~n ) and O ( nr~n ) using bigram and trigram estimation rc : spectively where n is the number of stenograph code tokens in a sentence , and m is tile upper bound of the number of homophonous characters for a stenograph code .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of stenograph code tokens in a sentence , and m is tile upper bound of the number of homophonous characters for a stenograph code</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>A synset is a hyponym of another synset if the tbrmer has the latter as a broader concept ; for example , BEVERAGE is a hyponym of LIQ UID .</sentence>
				<definiendum id="0">synset</definiendum>
				<definiendum id="1">BEVERAGE</definiendum>
			</definition>
			<definition id="1">
				<sentence>The statistical component consists of predicate-argument pairs extracted from a corpus in which the semantic class of the words is not indicated .</sentence>
				<definiendum id="0">statistical component</definiendum>
			</definition>
			<definition id="2">
				<sentence>A Bayesian network ( Pearl , 1988 ) , or Bayesian 1 ) el|el nel ; work ( BBN ) , eonsisi ; s of a sol ; of variables and a sel ; of directed edges ( : onneel ; ing the w~riat ) les .</sentence>
				<definiendum id="0">Bayesian network</definiendum>
				<definiens id="0">s of a sol ; of variables and a sel</definiens>
			</definition>
			<definition id="3">
				<sentence>A BBN provi ( tes a coral ) act representation tbr the joinI ; disl ; rit ) ution over the set of variables senses .</sentence>
				<definiendum id="0">BBN provi</definiendum>
			</definition>
			<definition id="4">
				<sentence>If N = X1 , ... , Xn i8 a Bayesian network with variables X1 , ... , Xn , its joint distribution P ( N ) is the product of all the conditional probabilities specified in the network , P ( N ) = II P ( XJl p , , ( Xj ) ) ( 4 ) J where pa ( X ) is the set of parents of X. A BBN generates a factorization of the joint distribution over its variables .</sentence>
				<definiendum id="0">pa ( X )</definiendum>
				<definiens id="0">the product of all the conditional probabilities specified in the network</definiens>
				<definiens id="1">the set of parents of X. A BBN generates a factorization of the joint distribution over its variables</definiens>
			</definition>
</paper>

		<paper id="2134">
			<definition id="0">
				<sentence>An associated grammar rule can be sccn as a kind of example if it is described mostly by the surface level information .</sentence>
				<definiendum id="0">associated grammar rule</definiendum>
				<definiens id="0">a kind of example if it is described mostly by the surface level information</definiens>
			</definition>
			<definition id="1">
				<sentence>The insertion operation is a restricted form of the adjoining operation in the Lexicalized Tree Adjoining Grammar ( LTAG ) ( Joshi and Schabes , 1992 ) .</sentence>
				<definiendum id="0">insertion operation</definiendum>
			</definition>
			<definition id="2">
				<sentence>LEt X be a set of terminal symbols ( words ) , and NT be the set of nonterminal symbols disjoint fi'om 27 .</sentence>
				<definiendum id="0">LEt X</definiendum>
				<definiendum id="1">NT</definiendum>
				<definiens id="0">the set of nonterminal symbols disjoint fi'om 27</definiens>
			</definition>
			<definition id="3">
				<sentence>Sw is a set of nonterminal symbols associated with the word Wo They are assigned to the root of a tree when the tree is accepted by A , ~ f For each word w , the set A , , l T , , , A , v , Sw } is the set of local trees associated with w. The structure is described by Aw and 1'w~ the symbol at the root node 927 is fl-om Sw , and se .</sentence>
				<definiendum id="0">Sw</definiendum>
				<definiens id="0">a set of nonterminal symbols associated with the word Wo They are assigned to the root of a tree when the tree is accepted by A , ~ f For each word w , the set A , , l T , , , A , v</definiens>
				<definiens id="1">the set of local trees associated with w. The structure is described by Aw and 1'w~ the symbol at the root node 927 is fl-om Sw , and se</definiens>
			</definition>
			<definition id="4">
				<sentence>The local automaton is a pushdown automaton that accepts tree sequence ( Tx ) n ( Tl ) n , and accepted trees are given the nonterminal symbol S as in ( d ) .</sentence>
				<definiendum id="0">local automaton</definiendum>
				<definiens id="0">a pushdown automaton that accepts tree sequence ( Tx ) n ( Tl ) n , and accepted trees are given the nonterminal symbol S as in ( d )</definiens>
			</definition>
			<definition id="5">
				<sentence>ll\ ] \ ] Output t % K ( II'~2-~ ( I } ' I '+~ } 'okl -ll &gt; O I\ [ I\ ] ) UI'-H , ~J &lt; J `` m /h'-~ub\ ] l/llillt'l'-d++ti C+H Figut'e 8 : Trai~slatior~ Modttle 930 level node is assigned attribute-values that express their syntactic ftlnction Stlch as subject , direct object , etc , instead of a single part-of-speech symbol such as NP. This approach is suitable to capture idiosyncratic behaviour of words. Domain-specific rules are mostly pattern-like rules with special attention to aspects that are important for carrying conversations , such as modality and the degree of politeness. The English to Japanese translation dictionary contains about seventy thousand words. The number of words that required individual I , TA was a few thousand at tile time of this report. 3°3 Current Status of Implementation The system has been iml~lemented using C++ , and runs on Windows 98 and NT. The requirelnent is Pentium I\ ] 400MItz or above for tile CPU~ about 61 ) MB of memory , and 200 MB of disk space. Most of the disk space is used for statistical data for disambiguation. We performed a preliminary evaluation of the translation quality of l ' ; nglish to Japanese translation. A widely used COlnmercial systeln was chosen as a reference system , of which the dictionaries were expanded for the target domain , t : ive hundred sentellces were randomly chosen from a large ( about 40K ) pool of conversatiolml texts of the target domain. '\ [ 'hen the output of our system and the reference system were mixed , and then presented to a single evahmtor at a random order. The evaluator classified them into ( Bur levels ( natural , good , understandable and bad ) . The result showed that tile number of sentences classified to `` 'natural '' increased about 45 % compared to that of tlle reference system , i.e. tile ratio of the ntlmber of sentences was arotllld 1.45. The ntllllber o| '' sentences classified as `` bad '' decreased about 40 % in the same measure. We applied this module to an experimental speech translation system ( Watanabe et al. , 2000 ) . 4 l ) iscussions The proposed granllnar fornlalism is a kind of lexicalized granll'nar fcnTnalisnl and shares its advantages. The largest difference frolll other strongly lexicalized granunar tbrnlalisms is that it employs lexicalized tree automata ( I , TA ) to describe the tree set associated with a word , which allows a finite description of a non-finite set of local trees. These automata 's role is equivalent to additional tree operations in other formalisms. In addition , an LTA provide an extended domain of locality ( EDOL ) of the word. If all the LTAs are finite automata in the string automaton representation , then the tree language recognized by this grammar is regular and its yield is a context-free language. The grammar can accept general Tree Adjoining Language ( TAL ) if '' the LTAs belong to the class of pushdown atttomata in the string autonlaton representation. This is a reflection of tile thct that pushdown tree automata can accept the indexed languages ( Gdcseg and Steinby , 1997 ) , of which the TAL is a subclass. As shown in the section 2.4 , the control strategy of bottom-up chart parsing does not rely ell the concrete content of the I , TA , which is an adwmtage of the proposed formalism. This implies that we can alter even tile grammar class without affecting the parsing. Suppose the current L'I'As are finite automata , hence the yield language is context-free. If we want to introduce a word e that induces a non-context-fi'eeness , such as e in a '' b '' ec '' d '' , then what we have to do is to write a pushdown automaton in tile figure 4 li ) r the word e. We change neither tile grammar formalism nor the parsing algorithm , and the change is localized to the LTA of e. Writing automata by hand may seem much more complex than writing trees , but our experience shows that it is not nlucll different fronl convelltional granHllar development. As long as appropriate notations are used , writing automata for a word anlounts to detornlining possible t'olnl of trees headed by that word , a task ahvays required in gramil-iar development. In fact , thei'e is tess alllOtllli of work since tile gralllnlar writer does not need to pay attention to assigning proper nontcrminals and/or proper attributes to internal nodes of trees in order to control their growth. It is another advantage of the proposed formalisnl that it can utilize various autorriata operations , such as conlposition and intersection. For exanlple , a word can append an atltOlllatoll to thai of the headword when it becomes a child , which enables to specify a constraint fi'onl a h ) wer-positioned word to a higherq ) ositioned v~oi 'd in tile tree. Another example is cootdination. Two edges are conjoined when tile unapplied parts of I , TAs have nonempty intersection as automata , and tile conjoined edge is given with this intersection as the lTfA. Verb phrase conjunction such as `` , lohn eats cookies and drinks beer '' is handled in this manner , by conjoining `` 'eats cookies '' and `` 'drinks beer. The intersected automaton will accept the subject tree and other sentence-level trees. 931 In the proposed method , elementary trees are always anchored by the syntactic headword. For example , a verb iit a relative clause is in the EDOL of the antecedent. Then , if the embedded verb puts a constraint on the antecedent , that constraint is not expressed in a straightforward manner , which may seem a weakness of the method. We just poiltt out that this type of problem occurs when the syntactic head and the semantic head are different , and is common to lexicalized grammars as long as a tree is anchored to one word , because constraints are often reciprocal. In our current implementation , the constraint written in the verb 's dictionary is found and checked by the relative-clause-tree accepting automaton of the antecedent noun. There have been many work on syntactic analysis based on automata attached to the headword. Evans and Weir ( 1998 ) used finite state automata as representation of trees that can be merged and minimized to improve parsing efficiency , lit their method , the granlnlar is fixed to be I , TAG or seine lexicalized grammar and the automata are obtained by automatic conversion from the trees. Our nlethod differs frol11 theirs ilt the poiltt that ours employs trees as the basic object of automata , which enables to handle general recursive adjunction in LTAG , while their automata work Oll tile nonterillinal and ternlinal synlbols , lit the center O\ [ `` Ot.lr method is the notion of '' the local orallllllal of a word. `` \ [ 'he whole grammar is divided into the global part and the set of h ) cal gralltntars specific to the words , which is represented by tile LTAs. Alshawi ( 1996 ) introduced ttead Atitornata , a weighted finite machine that accepts a pair o\ [ `` sequences of relation symbols. The difference is similar as above. Since the tree automata in our method are used to define the set of the local trees , their role will be equivalent to building the head automata themselves , but not to combining the trees that are already built , like the I lead Automata. We proposed a new lexicalized grammar formalism , called Lexicalized Tree Automata-based Grammar. In this formalism , the trees anchored to a word are described by a tree automaton associated with it. We showed a chart parsing algo , ithm that does not depend on the concrete content of the automata. We have imt ) lemented a bi-directional translation module between Japanese and English liar conversational texts using this formalism. A preliminary evaluation of English to Japanese translation quality revealed a promising result. Acknowledgement We would like to thank Shinichiro Kamei tia '' useful discussions and Yoshinori Ishihara for his help in the implelnentation work. Abeilld , A. , Schabes , Y. and .loshi , A.K. ( 1990 ) . t.'si , z &lt; . , , l.exicalized 771 ( ; s fin '' Machine 77 '' an.dalion. tn f~roceedin &lt; &lt; . , ,s oi ( 'OI , LVG90. p p. 1-6. Aho. A.V.. and Ulhn : .ul..I.1 ) . ( 1969 ) . I'roFe # 'lie.v qf , Slvntax Directed 7 ) 'anslations..Iotlrllal of C ( Hllrltlicr { tlld g } stelll Sciences. vol.3 , pp.319-334. AIshawi. II. ( 19961. Ilead ..haomam and Bilini , ,ual Tilin &lt; &lt; , c 7 ) 'aHs/alioll with Minima/ Rel ) re.ventalion.v. In Proceedin.Ts of 34 If ' .4 # lnlla\ [ .lleetlll£ , ( g { 'onl/ ) lttattolla/ Linguistics , pp. 167-176. P , rown. P , .D. ( 1996 ) . ICvamlJle-Based , l/achine 7 ) 'all.v/all'oil ill the Pan gloss ,71'.s'lenl. hi Proceedinw of ( 'Ol.l , VG-96. pp. 169-174. \ ] 'vans , R. and Weir , I ) . , I. ( 1998 ) . ,4 .S'tructure- , S'harink , Parser jot Lexicalized ( ; rantmars. In tb'oc'eedi , g.s '' of COLING- , .ICL `` 98. pp.372-378. Furuse , O. and lida. It. ( 1994 ) . ( 'onstituenl Boundary Parsing .for 1Crample-Ba , sed .Wachine 77 '' culs/ation. In l'roceedin£ , s o/ ( 'Ol , lNG-94 , pp. 105I I I. Gdcsc , , F. and Stcinby , M. ( 1997 ) . Tree lA.ulguagcs. In t/andbook of Formal l.anguages. G. P , osenberg and it. Salomaa. trillers. Springer Vcrlag. Vol.3. pp. 1-68. Joshi. A.K. and Schabcs. Y. ( 1992 ) . Tree-..tcOoinin£4 ( Trammars and l , exicali_-ed Grammars. II1 `` \ [ 'I'CC i\tltOlllala and lxmguages. M. NiXal and A. l &gt; odelski , cd , , Elsevier Science l~ublishcrs B.V. , pp.409-43 I. ltuland .</sentence>
				<definiendum id="0">LTA provide</definiendum>
				<definiendum id="1">TA</definiendum>
				<definiens id="0">suitable to capture idiosyncratic behaviour of words. Domain-specific rules are mostly pattern-like rules with special attention to aspects that are important for carrying conversations , such as modality and the degree of politeness. The English to Japanese translation dictionary contains about seventy thousand words. The number of words that required individual I</definiens>
				<definiens id="1">an extended domain of locality ( EDOL ) of the word. If all the LTAs are finite automata in the string automaton representation</definiens>
				<definiens id="2">a context-free language. The grammar can accept general Tree Adjoining Language ( TAL ) if '' the LTAs belong to the class of pushdown atttomata in the string autonlaton representation. This is a reflection of tile thct that pushdown tree automata can accept the indexed languages ( Gdcseg and Steinby , 1997 ) , of which the TAL is a subclass. As shown in the section 2.4 , the control strategy of bottom-up chart parsing does not rely ell the concrete content of the I ,</definiens>
				<definiens id="3">enables to specify a constraint fi'onl a h ) wer-positioned word to a higherq ) ositioned v~oi 'd in tile tree. Another example is cootdination. Two edges are conjoined when tile unapplied parts of I</definiens>
				<definiens id="4">representation of trees that can be merged and minimized to improve parsing efficiency</definiens>
				<definiens id="5">the basic object of automata , which enables to handle general recursive adjunction in LTAG</definiens>
			</definition>
</paper>

		<paper id="2143">
			<definition id="0">
				<sentence>SirEd offers different options for building structures .</sentence>
				<definiendum id="0">SirEd</definiendum>
				<definiens id="0">offers different options for building structures</definiens>
			</definition>
			<definition id="1">
				<sentence>Coordinalive relationships serve for clauses coordinated by conjunctions : coordinative buy apples \ [ XI and peaJwlYl ; coordinative-conj unctive I ) tty apples and \ [ X\ ] l ) emw \ [ Y\ ] .</sentence>
				<definiendum id="0">Coordinalive relationships</definiendum>
				<definiens id="0">serve for clauses coordinated by conjunctions : coordinative buy apples \ [ XI and peaJwlYl</definiens>
			</definition>
</paper>

		<paper id="2148">
			<definition id="0">
				<sentence>Lexicatized Tree Adjoining Granunars consist of a finite set of initial and auxiliary elementary hees , and two operations to combine them .</sentence>
				<definiendum id="0">Lexicatized Tree Adjoining Granunars</definiendum>
			</definition>
			<definition id="1">
				<sentence>Substitution is a simple operation that replaces a leaf of a tree with a new tree .</sentence>
				<definiendum id="0">Substitution</definiendum>
				<definiens id="0">a simple operation that replaces a leaf of a tree with a new tree</definiens>
			</definition>
			<definition id="2">
				<sentence>Adjunction is a splicing operation that replaces an internal node of an elementary tree with an auxiliary tree .</sentence>
				<definiendum id="0">Adjunction</definiendum>
				<definiens id="0">a splicing operation that replaces an internal node of an elementary tree with an auxiliary tree</definiens>
			</definition>
			<definition id="3">
				<sentence>ql running , and E2 refers to the event of an entity ( argO ) causing event El .</sentence>
				<definiendum id="0">E2</definiendum>
				<definiens id="0">the event of an entity ( argO ) causing event El</definiens>
			</definition>
</paper>

		<paper id="2173">
			<definition id="0">
				<sentence>The result of our efforts , XMLTrans , takes as input a well-lbrmed XML file and a file containing a set of transformation rules and gives as output the .</sentence>
				<definiendum id="0">XMLTrans ,</definiendum>
				<definiens id="0">takes as input a well-lbrmed XML file and a file containing a set of transformation rules and gives as output the</definiens>
			</definition>
			<definition id="1">
				<sentence>For Example : LI \ [ $ a=TYPE \ ] , where TYPE is a standard XML attribute .</sentence>
				<definiendum id="0">TYPE</definiendum>
			</definition>
			<definition id="2">
				<sentence>The basic control structure of XMLTrans is the rule , consisting of a left-hand side ( LHS ) and a right-hand side ( RHS ) separated by an arrow ( `` &gt; '' ) .</sentence>
				<definiendum id="0">right-hand side</definiendum>
				<definiendum id="1">RHS</definiendum>
				<definiens id="0">the rule , consisting of a left-hand side</definiens>
			</definition>
			<definition id="3">
				<sentence>The LHS is a pattern of XML element ( s ) to match while the RHS is a specitication for a transfbrmation on those elements .</sentence>
				<definiendum id="0">LHS</definiendum>
				<definiens id="0">a pattern of XML element ( s ) to match while the RHS is a specitication for a transfbrmation on those elements</definiens>
			</definition>
			<definition id="4">
				<sentence>XMLTrans allows for complex regular expressions of elements on the tits to match over the children of the element being examined .</sentence>
				<definiendum id="0">XMLTrans</definiendum>
			</definition>
			<definition id="5">
				<sentence>lit is evident that the set of transformations described by the XMLTrans transformation language is a subset of those described by XSLT .</sentence>
				<definiendum id="0">XMLTrans transformation language</definiendum>
				<definiens id="0">a subset of those described by XSLT</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>Each Chinese morpheme ( usually a single character ) carries meanings and most are polyscincus .</sentence>
				<definiendum id="0">Chinese morpheme</definiendum>
				<definiens id="0">a single character ) carries meanings and most are polyscincus</definiens>
			</definition>
			<definition id="1">
				<sentence>( ) no ot ' the measuroulonts proposed is : SIMILAR ( Sere , Rule ) = { } ; il , k lnforlllati ( ) nLoad ( Sem ( hSemi ) * lq-eqi } / Max-vahle Where Sere is tile semantic class of X. Max-value is tile maxinlal vahle o1 ' { ~\ ] \ [ nfornlation Load ( S ~Selni ) * Freqi } for all semantic classes S. The iriax-wllue normalizes tile SIMILAR value to 0-1 .</sentence>
				<definiendum id="0">Sere</definiendum>
				<definiens id="0">tile semantic class of X. Max-value is tile maxinlal vahle o1 ' { ~\ ] \ [ nfornlation Load ( S ~Selni ) * Freqi } for all semantic classes S. The iriax-wllue normalizes tile SIMILAR value to 0-1</definiens>
			</definition>
			<definition id="2">
				<sentence>S ( hSemi denotes the least common ancestor of S and Semi .</sentence>
				<definiendum id="0">S ( hSemi</definiendum>
			</definition>
			<definition id="3">
				<sentence>~l~j¢ : t~ '' aS ; _i I1OUn conlpound which denotes a kind of nlachille .</sentence>
				<definiendum id="0">_i I1OUn conlpound</definiendum>
				<definiens id="0">t~ '' aS</definiens>
			</definition>
			<definition id="4">
				<sentence>The Chilin provides the semantic categories for morpheme and words .</sentence>
				<definiendum id="0">Chilin</definiendum>
			</definition>
</paper>

		<paper id="2151">
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>~ \ ] ' ( c'17 ) z , ( c~lT , ) ~ ' ' p ( ~lT ) _ 2 a , ~ p ( c '' l,0 p ( c'lT , ) _ Cling I = /~ So in order to estimate p ( v\ [ c , r ) , we need a way of searching for a set c ' , where c ' is a hypernym of c , which consists of concepts c '' which have similar p ( v\ ] c '' , r ) .</sentence>
				<definiendum id="0">c '</definiendum>
				<definiendum id="1">c</definiendum>
				<definiens id="0">c'lT , ) _ Cling I = /~ So in order to estimate p ( v\ [ c , r )</definiens>
				<definiens id="1">consists of concepts c '' which have similar p</definiens>
			</definition>
			<definition id="1">
				<sentence>Table 1 : Maximum Likelihood Esti : lnates freq ( c , v , r ) is the number of ( n , v , r ) triples in the data in which n is being used to denote c. fl'eq ( c , r ) Ev'EV freq ( c , v ' , r ) P ( CI ? ' )</sentence>
				<definiendum id="0">r )</definiendum>
				<definiendum id="1">r ) Ev'EV freq</definiendum>
				<definiens id="0">Maximum Likelihood Esti : lnates freq ( c , v</definiens>
				<definiens id="1">the number of ( n , v , r ) triples in the data in which n is being used to</definiens>
			</definition>
			<definition id="2">
				<sentence>The data set consists of tuples of the form ( v , ~zl , p~ ' , n2 ) , together with the attachment site for each tuple .</sentence>
				<definiendum id="0">data set</definiendum>
				<definiens id="0">consists of tuples of the form ( v , ~zl , p~ '</definiens>
			</definition>
			<definition id="3">
				<sentence>Note that the WordNet hierarchy consists of nine separate sub-hierarchies , headed by such concepts as ( entity &gt; , ( abstraction ) , ( psychological~eature ) , bnt we assume the existence of a single root which dominates each of the sub-hierarchies , which is referred to as ( root &gt; .</sentence>
				<definiendum id="0">WordNet hierarchy</definiendum>
				<definiens id="0">consists of nine separate sub-hierarchies , headed by such concepts as ( entity &gt; , ( abstraction ) , ( psychological~eature ) , bnt we assume the existence of a single root which dominates each of the sub-hierarchies , which is referred to as ( root &gt;</definiens>
			</definition>
</paper>

		<paper id="1078">
			<definition id="0">
				<sentence>, translation probability determines the translation .</sentence>
				<definiendum id="0">translation probability</definiendum>
			</definition>
			<definition id="1">
				<sentence>i~ 4 ) O b. \ ] ~en l 2 3 1 ) ' = recalculate 1 2 3 Figure 2 : A S ( 't of % -ansfer Rules Figure 1 is a pair of `` regularized '' parses t br a corresi ) onding pair of Spanish and Fmglish sentences fi'om Microsoft Excel hell ) text .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">a pair of `` regularized '' parses t br a corresi ) onding pair of Spanish and Fmglish sentences fi'om Microsoft Excel hell</definiens>
			</definition>
			<definition id="2">
				<sentence>Nodes ( wflues ) are labeled with head words and arcs ( features ) are labeled with gramma~ ; ical thnetions ( subject , object ) , 1 ) repositions ( in ) and subordinate conjunctions ( beNre ) .</sentence>
				<definiendum id="0">Nodes</definiendum>
				<definiens id="0">subject , object ) , 1 ) repositions ( in ) and subordinate conjunctions</definiens>
			</definition>
			<definition id="3">
				<sentence>Closed substructures consist of single nodes ( A , A ' , B , B ' , C ' ) or subtrees ( the left hand side of rule 3 ) .</sentence>
				<definiendum id="0">Closed substructures</definiendum>
			</definition>
			<definition id="4">
				<sentence>The right-hand sides of these comt ) atil ) le rules are also ( : ombined t ; o 1 ) reduce the translal ; iolL This is an idealized view of our system in which each node in the input tree matches the left ; hand side of exactly one transfer rule : there is no ambiguity and no combinatorial explosion .</sentence>
				<definiendum id="0">iolL This</definiendum>
				<definiens id="0">ombined t ; o 1 ) reduce the translal</definiens>
			</definition>
			<definition id="5">
				<sentence>The denominator is the combined fl'equencies of all rules that match N. aThis is somewhat det ) cndent on the way these | ; ransfer rules are derived .</sentence>
				<definiendum id="0">denominator</definiendum>
				<definiens id="0">the combined fl'equencies of all rules that match N. aThis is somewhat det ) cndent on the way these | ; ransfer rules are derived</definiens>
			</definition>
			<definition id="6">
				<sentence>`` Total Translations '' refer to the number of sen| ; ences which were translated successfully 1 ) y the system and `` Over Edge Limit '' refers to the numl ) er of sentences which caused the system to exceed the edge limit , i.e. , once the system produces over 10,000 edges , trm~slation failure is assmned .</sentence>
				<definiendum id="0">Total Translations</definiendum>
				<definiens id="0">the numl ) er of sentences which caused the system to exceed the edge limit</definiens>
			</definition>
			<definition id="7">
				<sentence>`` Actual Edges '' reibrs to the total number of edges used tbr attempting to translate every sentence in the corpus .</sentence>
				<definiendum id="0">Actual Edges</definiendum>
				<definiens id="0">the total number of edges used tbr attempting to translate every sentence in the corpus</definiens>
			</definition>
			<definition id="8">
				<sentence>As should be expected , accuracy was virtually the same with and without normalization , although normalization &lt; lid cause a slight improvement. Normalization should produce the essentially the same result in less time. These results suggest that we can probably count on a speed-up of at least 4 and a signif icant decline in failed parses by using normMization. The ditferences in performance on the two corpora are most likely due to the degree of hand-tuning for Experiment 1. `` Accuracy '' in Figure 5 is the average of the tbllowing score for each translated sentence : ITNYu ~ TMSI 1/2 x ( ITNYuI + ITMsl ) TNZU is the set of words in NYU 's translation and TMS is the set of words in the original Microsoft translation. If TNYU = `` A B C D E '' and TMS = `` A B C F '' , then the intersection set `` A B C '' is length 3 ( the numerator ) and the average length of TNZU and TMS is 4 1/2 ( the denominator ) . The accuracy score equals 3 + 4 1/2 = 2/3. This is a Dice coefficient comparison of our translation with the original. It is an inexpensive nmthod of measuring the pertbrmance of a new version of our system , hnprovements in the average accuracy score for our san &gt; ple set ; of sentences usually reflect an improvement in overall translation quality .</sentence>
				<definiendum id="0">TNZU</definiendum>
				<definiendum id="1">TMS</definiendum>
				<definiendum id="2">B C</definiendum>
				<definiendum id="3">TMS</definiendum>
				<definiens id="0">the set of words in NYU 's translation and</definiens>
				<definiens id="1">the set of words in the original Microsoft translation. If TNYU = `` A B C D E '' and TMS = `` A B C F ''</definiens>
			</definition>
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>A recoverable error is one that does not interfere with the operation of following components .</sentence>
				<definiendum id="0">recoverable error</definiendum>
				<definiens id="0">one that does not interfere with the operation of following components</definiens>
			</definition>
			<definition id="1">
				<sentence>`` Natural Language Processing : The PLNLP Approach '' , Kluwer , Boston , 993 .</sentence>
				<definiendum id="0">Natural Language Processing</definiendum>
				<definiens id="0">The PLNLP Approach ''</definiens>
			</definition>
</paper>

		<paper id="1080">
			<definition id="0">
				<sentence>A grammatical deduction system or , in Sikkel 's terminology a pal : ring schema , is defined as a set of deduction schemes and a set of axioms .</sentence>
				<definiendum id="0">grammatical deduction system</definiendum>
				<definiens id="0">a set of deduction schemes and a set of axioms</definiens>
			</definition>
			<definition id="1">
				<sentence>Whereas usually one deals with a tixed constraint domain and a specialized solver , CHR is an extension of the Prolog language which allows for the specification of userdefined constraints and arbitrary solvers .</sentence>
				<definiendum id="0">CHR</definiendum>
				<definiens id="0">an extension of the Prolog language which allows for the specification of userdefined constraints and arbitrary solvers</definiens>
			</definition>
			<definition id="2">
				<sentence>CHR allow the definition of rule sets for constraint solving with three types of rules : Firstly simplification rules ( &lt; = &gt; ) which replace a number of constraints in the store with new constraints ; secondly propagation rules ( == &gt; ) which add new constraints to the store in case a number of constraints is already present ; and thirdly `` simpagation '' rules ( &lt; = &gt; in combination with a \ in the head of the rule ) which replace only those constraints with new ones which are to the right of the backslash .</sentence>
				<definiendum id="0">simplification rules</definiendum>
				<definiendum id="1">secondly propagation rules</definiendum>
				<definiens id="0">replace a number of constraints in the store with new constraints</definiens>
			</definition>
			<definition id="3">
				<sentence>For simplicity , a gramnmr is given as Prok/g Ihcts : lexical items as lex ( Word , Category ) and gramnmr rules as rule ( RHS , LHS ) where RHS is a list of categories representing the right hand side and LHS is a single category representing the left hand side of the rule .</sentence>
				<definiendum id="0">RHS</definiendum>
				<definiendum id="1">LHS</definiendum>
				<definiens id="0">a list of categories representing the right hand side</definiens>
				<definiens id="1">a single category representing the left hand side of the rule</definiens>
			</definition>
</paper>

		<paper id="2163">
			<definition id="0">
				<sentence>The most general form of alignment distribution that we consider in the ItMM is p ( aj a.+_ , la ( % ) , G ( f~ ) , hExtension : empty word In the original formulation of the HMM alignment model there ix no 'empty ' word which generates Fren ( : h words having no directly aligned English word .</sentence>
				<definiendum id="0">hExtension</definiendum>
				<definiens id="0">generates Fren ( : h words having no directly aligned English word</definiens>
			</definition>
			<definition id="1">
				<sentence>We present results on the Verbmobil Task which is a speech translation task ill the donmin of appointnxent scheduling , travel planning , and hotel reservation ( Wahlster , 1993 ) .</sentence>
				<definiendum id="0">Verbmobil Task</definiendum>
			</definition>
			<definition id="2">
				<sentence>The WEll , correspomls to the edit distance t ) etween the produced translation and one t ) redefined reference translation .</sentence>
				<definiendum id="0">WEll ,</definiendum>
				<definiens id="0">correspomls to the edit distance t</definiens>
			</definition>
</paper>

		<paper id="2087">
			<definition id="0">
				<sentence>Whereas a feature is usually a pair ( attribute , value ) , an IG feature is a triplet ( attribute , polarity , value ) where polarity can take one of the three values -1 , 0 or -t-1 and behaves like an electrostatic charge : for instance , a noun phrase which is waiting to receive a syntactic function in a sentence , carries a negative feature of type fltnct while a finite verb which is searching for its subject , carries a positive t'eature funct with value s @ j .</sentence>
				<definiendum id="0">IG feature</definiendum>
				<definiens id="0">a triplet ( attribute , polarity , value ) where polarity can take one of the three values -1 , 0 or -t-1 and behaves like an electrostatic charge : for instance , a noun phrase which is waiting to receive a syntactic function in a sentence</definiens>
				<definiens id="1">searching for its subject , carries a positive t'eature funct with value s @ j</definiens>
			</definition>
			<definition id="1">
				<sentence>For instance , a possible lexical entry for the tinite verb volt in French has the shal ) e Dvoit = D1 ® ( D2 &amp; D3 ) ® ( D4 &amp; ; DS ) : Dj contains information related to the subject which is coiilnloll to all uses of the verb volt ; D2 expresses the canonical order subject-verb in the sentence that is headed by the vert ) voit whereas Da expresses the reverse order for which the subject must be realized under some conditions , such as in the phrase Marie que volt , lean ; D4 exl ) resses that the verb has an exl ) licit object whereas D,5 corresl ) on ( ts to circumstances where this object is not present , such as ill the sentence , Ican volt .</sentence>
				<definiendum id="0">DS )</definiendum>
				<definiendum id="1">D2</definiendum>
				<definiens id="0">expresses the canonical order subject-verb in the sentence that is headed by the vert</definiens>
			</definition>
			<definition id="2">
				<sentence>For instance , if we continue with description D1 related to the verb volt , we can assume that it contains the formula 601 ( Na &gt; \ [ N4 , N~\ ] ) ® ( N4 &gt; * No ) which is interpreted as follows : the verb phrase Na is constituted of tile verb N4 and its object N~ ; Na represents the bare verb whereas N4 represents the verb which has been possibly modified by a clitic , a negation or an adverb .</sentence>
				<definiendum id="0">Na</definiendum>
				<definiens id="0">N4 &gt; * No ) which is interpreted as follows : the verb phrase Na is constituted of tile verb N4 and its object N~</definiens>
			</definition>
			<definition id="3">
				<sentence>Deserit ) tions of tyI ) e Fcat , related to features , have the following fol 'm : Feat : := Node : Attr Pol Val \ [ Var C Dora \ [ Vat ¢ Dora gol : := &lt; -I = I + Val : := Coast I Vat A feature Node : Attr Pol Val is a triplet composed of an attribute Attr , a polarity Pol and a value Val associated with a syntactic node Nodc .</sentence>
				<definiendum id="0">Attr</definiendum>
				<definiens id="0">a triplet composed of an attribute</definiens>
			</definition>
			<definition id="4">
				<sentence>Ill all cases , Val is either a constant which is selected from an infiifite countable set Coast of feature values or a variable which is selected fl'oln an infinite countable set Vat of feature variables ; then , its def inition domain Call be constrained by two lduds of predicates : Val E Dora and Val 9 ( Dora ; Dora is a finite set of elements taken froln Co7~8t .</sentence>
				<definiendum id="0">Val</definiendum>
				<definiendum id="1">Dora</definiendum>
				<definiens id="0">a finite set of elements taken froln Co7~8t</definiens>
			</definition>
			<definition id="5">
				<sentence>Since syntactic descriptions use only a fragment of this logic and if we choose the framework of the sequent calculus , only seven ILL rules are useflll : F1 , ... , Fn IFI® ... ®Fn id P ~G FI , F2 , P IG 1 , P 1G 1L Ft @ F'2 , P lG @ L F1 , P N G J~'2 , P k GG &amp; L2 FI &amp; F2 , P G &amp; L1 /~'I &amp; F2 , r IF\ [ t/X\ ] , P tG Pl 117 F , I~2 l-G V1~ cut V X ~ ; P FPl , P2 tG 602 With respect to tile usual presentation of the ILL sequent cahmlus ( Lincolu , 1992 ) , a×iom id is defined a bit differently but this definition is equivalent to the original one tbr tile logical fragment used by IG .</sentence>
				<definiendum id="0">ILL sequent cahmlus</definiendum>
			</definition>
			<definition id="6">
				<sentence>In scheme pha , 0 is an abbreviation for ( N : ord = e ( c , ) ) ; a is a perlnutation on \ [ \ [ 1 , p~ which expresses an order ibr concatenating the phonological tbrms vl , ... , vp of the children nodes N\ ] , ... , N v of N and c ( o- ) is a bijective encoding of this permutation with an integer .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">an abbreviation for ( N : ord = e ( c , ) ) ; a is a perlnutation on \ [ \ [ 1 , p~ which expresses an order ibr concatenating the phonological tbrms vl , ... , vp of the children nodes N\ ] , ... , N v of N</definiens>
			</definition>
			<definition id="7">
				<sentence>Here , we consider three sets of variable , s , which corretl ) on ( t to tim three kin ( Is of choi ( : e 1 ) oints in the parsing proCOTS ; a/ ) les ; ery selection variable Si is an integer variable which is associated with a connective &amp; of D0 and which is used for indicating the rank of the component of the correspondent additive conjunction that is selected in the deduction .</sentence>
				<definiendum id="0">ery selection variable Si</definiendum>
				<definiens id="0">corretl ) on ( t to tim three kin ( Is of choi ( : e 1 ) oints in the parsing proCOTS ; a/ ) les</definiens>
			</definition>
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>Prediction Candidates Estimation Unit ( c ) calculates certainty factor and usefulness factor for all of retrieved results by Unit ( b ) to estimate candidates° User Learning Unit ( d ) extracts phrases adopted by the user , and automatically registers them into the user dictionary .</sentence>
				<definiendum id="0">Prediction Candidates Estimation Unit ( c )</definiendum>
				<definiens id="0">calculates certainty factor and usefulness factor for all of retrieved results by Unit ( b ) to estimate candidates° User Learning Unit ( d ) extracts phrases adopted by the user</definiens>
			</definition>
			<definition id="1">
				<sentence>343 ( ii ) User Dictionary consists of phrases learned fl'om texts which the user typed before .</sentence>
				<definiendum id="0">User Dictionary</definiendum>
				<definiens id="0">consists of phrases learned fl'om texts which the user typed before</definiens>
			</definition>
			<definition id="2">
				<sentence>L ( x ) is the length of a string x. An entry in the dictionary is denoted by W , which has kanji notation WH and kana notation Wy .</sentence>
				<definiendum id="0">L ( x )</definiendum>
				<definiens id="0">the length of a string x. An entry in the dictionary is denoted by W , which has kanji notation WH and kana notation Wy</definiens>
			</definition>
			<definition id="3">
				<sentence>When S is typed , certainty factor fox ' W in the system dictionary is calculated as follows : Certainty f actor ( WlS ) = Ft~ ( WH ) Fl ( ( Si ) ' O , when S has a right sub-string Si whid~ partiMly matches with the head of Wy otherwise where F~ : ( WH ) is the frequency of WH in kanji notation corpus , and FK ( &amp; ) is the fl'equency of Si in kana notation corpus corresponding to kanji one .</sentence>
				<definiendum id="0">FK</definiendum>
				<definiens id="0">the system dictionary is calculated as follows : Certainty f actor ( WlS ) = Ft~ ( WH ) Fl ( ( Si ) ' O , when S has a right sub-string Si</definiens>
				<definiens id="1">the frequency of WH in kanji notation corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>Hence , when S is typed , certainty factor fox ' W in the user dictionary is calculated as follows : Certainty f actor ( W IS ) = O , when S has a right sub-string &amp; which partially m attires with the head of Wy otherwise where N ( Si ) is the number of entries whose kana notations start from Si in the user dictionary , and c~ is a constant to give greater factor tbr entries in tile user dictionary than that in the system dictionary ; i.e. , tile user dictionary has priority .</sentence>
				<definiendum id="0">N ( Si )</definiendum>
				<definiendum id="1">c~</definiendum>
				<definiens id="0">follows : Certainty f actor ( W IS ) = O , when S has a right sub-string &amp; which partially m attires with the head of Wy otherwise where</definiens>
				<definiens id="1">the number of entries whose kana notations start from Si in the user dictionary</definiens>
			</definition>
			<definition id="5">
				<sentence>Two evaluation measures , an operation ratio and a precision , are defined as follows : P-Q Operation , ratio = p x 100 ( % ) R Precision , = S x 100 ( % ) where P is the length of the original ha'ha text ; , Q is the length of ha'ha chm'acters complemented by prediction~ R is the number of sllown prediction menu windows contMning appropriate choices , and S is tile number of ~dl of shown prediction menu windows .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">Q</definiendum>
				<definiendum id="2">S</definiendum>
				<definiens id="0">follows : P-Q Operation , ratio = p x 100 ( % ) R Precision , = S x 100 ( % ) where</definiens>
				<definiens id="1">the length of the original ha'ha text ; ,</definiens>
				<definiens id="2">the length of ha'ha chm'acters complemented by prediction~ R is the number of sllown prediction menu windows contMning appropriate choices , and</definiens>
				<definiens id="3">tile number of ~dl of shown prediction menu windows</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>C1 is some high-rm~ked constraint that eliminates ban .</sentence>
				<definiendum id="0">C1</definiendum>
				<definiens id="0">some high-rm~ked constraint that eliminates ban</definiens>
			</definition>
			<definition id="1">
				<sentence>An OT grammar is a pair ( Gen , C ) where • the candidate generator Gen is a relation that maps eaeh input to a nonempty set of candidate outputs ; • the hierarchy C = ( C1 , C2 , ... ) is a finite tnple of constraint functions that evaluate outputs .</sentence>
				<definiendum id="0">OT grammar</definiendum>
				<definiendum id="1">)</definiendum>
				<definiens id="0">a pair ( Gen , C ) where • the candidate generator Gen is a relation that maps eaeh input to a nonempty set of candidate outputs ; • the hierarchy C = ( C1 , C2 , ...</definiens>
				<definiens id="1">a finite tnple of constraint functions that evaluate outputs</definiens>
			</definition>
			<definition id="2">
				<sentence>We now describe how to evaluate C ( d ) where C is a WFSA .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">how to evaluate C ( d ) where</definiens>
			</definition>
			<definition id="3">
				<sentence>rSpace prevents giving the equivalent characterization as a locally weighted language ( Walther , 1999 ) .</sentence>
				<definiendum id="0">rSpace</definiendum>
			</definition>
			<definition id="4">
				<sentence>If Ci is a left-to-right constraint , we compose Ti-1 with the WFSA that l'epresents Ci , obtaining a weigh , ted finite-state transducer ( WFST ) , Ti • This transducer may be regarded as assigning a Ci-violation level ( an ( 1~1 + 1 ) -tuple ) to each cr : ( ~ it accepts .</sentence>
				<definiendum id="0">WFST</definiendum>
				<definiendum id="1">Ci-violation level</definiendum>
				<definiens id="0">a left-to-right constraint</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>Salience , in turn , is a major component of the sentence-level score that selects the sentences for extraction ( see 2.2 below ) .</sentence>
				<definiendum id="0">Salience</definiendum>
			</definition>
			<definition id="1">
				<sentence>A hierarchical representation of the document separates content and layout metadata , and makes the latter explicit in a document structure tree .</sentence>
				<definiendum id="0">hierarchical representation of the document</definiendum>
				<definiens id="0">separates content and layout metadata , and makes the latter explicit in a document structure tree</definiens>
			</definition>
			<definition id="2">
				<sentence>The salience component is the sum of the salience scores of the items in the sentence .</sentence>
				<definiendum id="0">salience component</definiendum>
				<definiens id="0">the sum of the salience scores of the items in the sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>Agglomeration is an inexpensive way of preventing dangling anaphors without having to resolve them .</sentence>
				<definiendum id="0">Agglomeration</definiendum>
				<definiens id="0">an inexpensive way of preventing dangling anaphors without having to resolve them</definiens>
			</definition>
			<definition id="4">
				<sentence>Segmentation is a document analysis function which directly exploits one of tile core text cohesion factors , patterras of h'xicaI repetition ( see Section 1.1 ) , for '' identifying some baseline data concerning tile distribution of topics in a text .</sentence>
				<definiendum id="0">Segmentation</definiendum>
				<definiens id="0">a document analysis function which directly exploits one of tile core text cohesion factors , patterras of h'xicaI repetition ( see Section 1.1 ) , for '' identifying some baseline data concerning tile distribution of topics in a text</definiens>
			</definition>
			<definition id="5">
				<sentence>Apart from the adjustments and modifications outlined above , we use essentially ltearst 's formnla for computing lexical similarity between adjacent blocks of text bl and b2 ( t denotes a discourse element term identified as such by prior processing , ranging over tim text span of the currently analyzed block ; Wt , l , N is the normalized frequency of occurrence of the term in block b~\ , ) : sim ( bl , b2 ) : ~ &gt; tWl , btwt , b~ Unlike most applications of segmentatkm to date , which are concerned with the identification of segment boundaries , we are primarily interested ira Ieveraging the content of the segments , to the extent that it is indicative of the focus of attention , and ( indirectl3 ; at least ) points at tile topical shifts to be utilized for surnmary generation .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a discourse element term identified as such by prior processing , ranging over tim text span of the currently analyzed block ; Wt , l</definiens>
				<definiens id="1">the normalized frequency of occurrence of the term in block b~\ , ) : sim ( bl</definiens>
			</definition>
			<definition id="6">
				<sentence>WORDNET : all electronic lexical database and some of its applications .</sentence>
				<definiendum id="0">WORDNET</definiendum>
			</definition>
</paper>

		<paper id="1081">
			<definition id="0">
				<sentence>- , ) + , X0 ( 2 ) where X in PYx is the check level of the first level of the tree ( N : none , P : POS , L : lexicon ) and Y is that of the second level , and lG , c-gr &lt; ~m is the uniform distribution over the vocabulary W ( -\ ] ) ~U , O -- gI 'D , I~\ ] ( */ ) ) = l/IWl ) .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">the uniform distribution over the vocabulary W ( -\ ] ) ~U , O -- gI 'D , I~\ ] ( */ ) ) = l/IWl )</definiens>
			</definition>
			<definition id="1">
				<sentence>In order to develop a model appropriate for a parser , it is better that the parameters are estimated from a syntactically annotated corlms by a maximmn likelihood estimation ( MI , E ) ( Meriaklo , 1994 : ) as follows : 1 , ( wit+ ) MZ , , j ' ( &lt; t , + -f ( &lt; t+ w , : &gt; ) P ( t+lt ) M &amp; E f ( t+ , t ) f ( t ) where f ( x ) represents the frequency of an event x in tile training corpus .</sentence>
				<definiendum id="0">f ( x )</definiendum>
				<definiens id="0">represents the frequency of an event x in tile training corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>rt English pa.rser ( Collins , 1997 ) only the words tha , t occur more tha , n d times in training data .</sentence>
				<definiendum id="0">n d</definiendum>
				<definiens id="0">times in training data</definiens>
			</definition>
			<definition id="3">
				<sentence>The accuracy is the ratio of the nnmber of the words a.nnotated with the same dependency to the numl ) er of the words as in the corpus : accuracy = # =words dependiug on tilt correct word ~words Tile last word and the second-to-last word of '' a sentence are excluded , because there is no ambiguity .</sentence>
				<definiendum id="0">accuracy</definiendum>
				<definiens id="0">the ratio of the nnmber of the words a.nnotated with the same dependency to the numl ) er of the words as in the corpus : accuracy = # =words dependiug on tilt correct word ~words Tile last word and the second-to-last word of '' a sentence are excluded</definiens>
			</definition>
</paper>

		<paper id="2146">
			<definition id="0">
				<sentence>Postpositions , such as noun-endings , verb-endings , and prefinal verbendings , are morphemes that determine the fnnctional role of NPs ( noun phrases ) and VPs ( verb phrases ) in sentences and also transform VPs into NPs or APs ( adjective phrases ) .</sentence>
				<definiendum id="0">VPs</definiendum>
				<definiens id="0">( verb phrases ) in sentences and also transform VPs into NPs or APs ( adjective phrases</definiens>
			</definition>
			<definition id="1">
				<sentence>1003 Thus , when S is a sentence consisted of a sequence of morphemes tagged for part-of-speech , ( w~ , t~ ) , ( w2 , t2 ) , ... , ( w , , , tu ) , where wi is a i th morpheme , ti is the part-of-speech tag of the morpheme wi , and cij is a category with relative position i , j , the basic statistical model will be given by : r* = arg , ~x P ( rl , S ' ) ( 1 ) ( 2 ) = argn~x P ( S ) , ~ argmaxP ( T , S ) .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">ti</definiendum>
				<definiendum id="2">cij</definiendum>
				<definiens id="0">the part-of-speech tag of the morpheme wi</definiens>
			</definition>
			<definition id="2">
				<sentence>P ( r , S ) = II P ( cij ) ( 4 ) cij ~T = H ( P ( eiilcik'ck+'J ) cij ET xP ( cik ) P ( cl~+lj ) ) , ( 5 ) i &lt; k &lt; j , if cij is a terminal , the , P ( c j ) = and frcquency ( cij , ti , wi ) frequency ( ti , wi ) ' ( 6 ) frequency ( eli , cik , Ch+lj ) ( 7 ) P ( eijleik , C~+lj ) ~ frequency ( cik , ck+lj ) The basic statistical model has been applied to morpheme/part-of-speech/category 3-tuple. Due to the sparseness of the data , we have used part-of-speech/category pairs 7 together , i.e. , collected the frequencies of the categories associated with the part-of-speeches assigned to the morpheme. Table 1 illustrates the sample entries of the category probability database. In table , 'nal ( fly ) ' has two categories with 0.6375 mid 0.3625 probability respectively. Table 2 illustrates the sample entries of the merge probability database using equation 7. frequency ( old , tl ) 7We define this as P ( cljltl ) ~ fvcq ... ... y ( tD `` Table 3 : Model Results fl'om the Basic Statistical Total sentences No-crossing Ave. crossing Labeled Recall Labeled Precision 591 74.62 % 77.02 79.15 Figure 1 : Sub-constituents for head-head cooccurrence heuristics Table 3 summarizes the results on an open test set of 591 sentences. In the basic statistical model , lexicM dependencies between morphemes that take part in merging process can not be incorporated into the model. When there is a different morpheme with the same syntactic category , it can be a miss match on merging process. This linfitation can be overcome through the co-occurrence between the head morphemes of left and right sub-constituent. When B h is a head morphenm of left subconstituent , r is a case relation , C h is a head morpheme of right sub-constituent as shown in figure 1 , head-head co-occurrence heuristics are defined by : p ( B , LI , . , Ch ) ~ frequency ( B h , r , C h ) frequency ( r , C h ) `` ( 8 ) Tile head-head co-occurrence heuristics have been augmented to equation 5 to model the lexical co-occurrence preference in category merging process. Table 4 illustrates the sample entries of the co-occurrence probability database. In Table 4 , a morpheme 'sac ( means 'bird ' ) ' , which has a `` MCK ( common noun ) '' ms POS tag , has been used a nominative of verb 'hal ( means 'fly ' ) ' with 0.8925 probability. 1004 Table 1 : Sample entries of the category probal ) ility database ( 'DII ' Ineans an '1 ' irregular verb. ) P ( ) S , morpheme category probability DII , nal v\ [ D\ ] \ { np\ [ noln\ ] } 0.6375 DI1 , hal v\ [ D\ ] \ { np\ [ noln\ ] , nl ) \ [ acc\ ] } 0.362,5 DI1 v \ [ D\ ] \ { rip \ [ nora\ ] } 0.3079 DI1 v\ [ D\ ] \ { np\ [ llOm\ ] , np\ [ acc\ ] } 0.2020 Table 2 : Sample entries of ' syntactic merge probability database left ; category ~ , / ( ~ \ , u , \ [ , ,o , ,l\ ] ) ~ , / ( ~ , \ , ,p lace\ ] ) right category v\ [ D\ ] \ { np\ [ noml , np\ [ acc\ ] } v\ [ D\ ] \ { , ,p \ [ , lo , , , \ ] , ,u , \ [ acd } inerged category v\ [ D\ ] \ { , ,p\ [ acd } v\ [ D\ ] \ { ni ) \ [ nonl\ ] } probability nl , ( v/ ( v\nont ) ) \n t , v/ ( v\np\ [ nom\ ] ) I ) .2197 The modified model has been tested Oil the same set of the open sentences as in the 1 ) asic model ext ) eriment. 'l~fl ) le 5 smnmarizes the result of these expcwiments. • Ezperimcnt : ( linear combination af th , c basic model and the head-h , cad co-occurrence heuristics ) . P ( % s ) eij { r +/~p ( \ ] / ' I , ,. , c* ' ) ) × P ( ~ , ik ) ~ ' ( ~ , k+ , ; ) ) , ( 9 ) i &lt; k &lt; j , if cij is a terminal , ~J , . , ; ' , ~ p ( c # i ) = P ( c.~ : i I~g , td. Ta , bh ; 5 : Results from the Basic : Statistical Model t ) lns head-head co-occurrence heuristics Total sentences 591 No-crossing 81.05 % Ave. crossing 0.70 Labeled Recall 84.02 Labeled Precision 85.30 If '' there is a case relation or a modification relation in two constituents , coverage heuristics designate it is easier to add the smaller tree to the larger one ttlan to merge the two medium sized trees. On the contrary , in the coordination relation , it is easier to nmrge two medium sized trees. We implemented these heuristics using / ; tie tbllowing coverage score : Case relation , modification relation : COV_scorc = left subtrec coverage + riqh , t sub/roe coverage. ( j_ ( ) ~ 4 × ~7~ , ~ , ,bt , .~ , . , ~ ~o , , , , ' , ,e ; &lt; ' , 'i : jl , ,i ~ , ,b &gt; ' .</sentence>
				<definiendum id="0">P ( r , S ) = II P</definiendum>
				<definiendum id="1">P ( c j</definiendum>
				<definiendum id="2">frcquency ( cij , ti , wi ) frequency</definiendum>
				<definiendum id="3">frequency</definiendum>
				<definiendum id="4">cik , Ch+lj )</definiendum>
				<definiendum id="5">r</definiendum>
				<definiendum id="6">C h</definiendum>
				<definiendum id="7">Ch ) ~ frequency</definiendum>
				<definiendum id="8">r , C h ) frequency</definiendum>
				<definiendum id="9">Tile head-head co-occurrence heuristics</definiendum>
				<definiendum id="10">morpheme 'sac</definiendum>
				<definiendum id="11">MCK</definiendum>
				<definiens id="0">( cij ) ( 4 ) cij ~T = H ( P ( eiilcik'ck+'J ) cij ET xP ( cik ) P ( cl~+lj ) )</definiens>
				<definiens id="1">part-of-speech/category pairs 7 together , i.e. , collected the frequencies of the categories associated with the part-of-speeches assigned to the morpheme. Table 1 illustrates the sample entries of the category probability database. In table , 'nal ( fly ) ' has two categories with 0.6375 mid 0.3625 probability respectively. Table 2 illustrates the sample entries of the merge probability database using equation 7. frequency ( old , tl ) 7We define this as P ( cljltl ) ~ fvcq ... ... y ( tD `` Table 3 : Model Results fl'om the Basic Statistical Total sentences No-crossing Ave. crossing Labeled Recall Labeled Precision 591 74.62 % 77.02 79.15 Figure 1 : Sub-constituents for head-head cooccurrence heuristics Table 3 summarizes the results on an open test set of 591 sentences. In the basic statistical model</definiens>
				<definiens id="2">a case relation</definiens>
				<definiens id="3">p ( B , LI , . ,</definiens>
				<definiens id="4">Sample entries of the category probal ) ility database ( 'DII ' Ineans an '1 ' irregular verb. ) P ( ) S , morpheme category probability DII , nal v\ [ D\ ] \ { np\ [ noln\ ] } 0.6375 DI1 , hal v\ [ D\ ] \ { np\ [ noln\ ] , nl</definiens>
				<definiens id="5">entries of ' syntactic merge probability database left ; category ~ , / ( ~ \ , u , \ [</definiens>
				<definiens id="6">lo , , , \ ] , ,u , \ [ acd } inerged category v\ [ D\ ] \ { , ,p\ [ acd } v\ [ D\ ] \ { ni ) \ [ nonl\ ] } probability nl , ( v/ ( v\nont ) ) \n t , v/ ( v\np\ [ nom\ ] ) I ) .2197 The modified model has been tested Oil the same set of the open sentences as in the 1 ) asic model ext ) eriment. 'l~fl ) le 5 smnmarizes the result of these expcwiments. • Ezperimcnt : ( linear combination af th , c basic model and the head-h , cad co-occurrence heuristics</definiens>
				<definiens id="7">a terminal , ~J , . , ; '</definiens>
				<definiens id="8">Results from the Basic : Statistical Model t ) lns head-head co-occurrence heuristics Total sentences 591 No-crossing 81.05 % Ave. crossing 0.70 Labeled Recall 84.02 Labeled Precision 85.30 If '' there is a case relation or a modification relation in two constituents , coverage heuristics designate it is easier to add the smaller tree to the larger one ttlan to merge the two medium sized trees. On the contrary , in the coordination relation , it is easier to nmrge two medium sized trees. We implemented these heuristics using / ; tie tbllowing coverage score : Case relation , modification relation : COV_scorc = left subtrec coverage + riqh</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>Similarity t ) etween the proposed analysis and tile patterns in memory is comImted according to : • the number of patterns needed to construct a tree ( to be minimized ) • the size of the patterns that are used to construct a tree ( to be maximized ) Tile nearest neighbor tbr a given analysis can be defined as the derivation that shares the largest amount of common nodes .</sentence>
				<definiendum id="0">Similarity t</definiendum>
			</definition>
			<definition id="1">
				<sentence>PMPG @ PCFG as all approximation needs to be compm'ed to actual D ( ) P~ by having DOP parse the data used in this experiment , and by having PMPG-I-I'CFG parse the data used in the exl ) erilnents described in Bod ( 1999 ) .</sentence>
				<definiendum id="0">PMPG @ PCFG</definiendum>
				<definiens id="0">compm'ed to actual D ( ) P~ by having DOP parse the data used in this experiment , and by having PMPG-I-I'CFG parse the data used in the exl</definiens>
			</definition>
</paper>

		<paper id="2092">
			<definition id="0">
				<sentence>3 The representation used in DOT is a 3-tuple ( T , , Tt , ¢ ) , where ~ is a tree in the somce language , Tt is a tree in the target language , and ¢ is a function that maps between semantic equivalent parts in both trees .</sentence>
				<definiendum id="0">Tt</definiendum>
				<definiendum id="1">¢</definiendum>
				<definiens id="0">a tree in the somce language ,</definiens>
				<definiens id="1">a tree in the target language</definiens>
			</definition>
			<definition id="1">
				<sentence>The composition of tile linked tree pair { ts,6 ) and 4The similarity between Example-based MT ( Nagao , 1984 ) and DOT is clear : EBMT uses a database of examples to form a translation , whereas DOT uses a bag of structured trees .</sentence>
				<definiendum id="0">DOT</definiendum>
				<definiendum id="1">DOT</definiendum>
			</definition>
			<definition id="2">
				<sentence>Each subtreepair consists of two trees : one in the source language and one in the target language .</sentence>
				<definiendum id="0">subtreepair</definiendum>
				<definiens id="0">consists of two trees : one in the source language and one in the target language</definiens>
			</definition>
</paper>

		<paper id="2100">
			<definition id="0">
				<sentence>The data ( PDT ) is thanks to grant No. 405/96/K214 of the Grant Agency of the Czech Republic .</sentence>
				<definiendum id="0">PDT</definiendum>
				<definiens id="0">thanks to grant No. 405/96/K214 of the Grant Agency of the Czech Republic</definiens>
			</definition>
</paper>

		<paper id="2109">
			<definition id="0">
				<sentence>The first merit is that the head of the dependency of the M-th segment is one of the seg1Of course , there are several exceptions ( S.Shirai , 1998 ) , but the frequencies of such exceptions are negligible compared to the current precision of the system .</sentence>
				<definiendum id="0">M-th segment</definiendum>
				<definiens id="0">the head of the dependency of the</definiens>
				<definiens id="1">negligible compared to the current precision of the system</definiens>
			</definition>
			<definition id="1">
				<sentence>'Sentence accuracy ' is the i ) ercentage of the sentences in which all the dependencies are analyzed correctly .</sentence>
				<definiendum id="0">'Sentence accuracy</definiendum>
				<definiens id="0">the i ) ercentage of the sentences in which all the dependencies are analyzed correctly</definiens>
			</definition>
			<definition id="2">
				<sentence>N-best accuracy is the percentage of tile sentences which have the correct analysis among its top N analyses .</sentence>
				<definiendum id="0">N-best accuracy</definiendum>
				<definiens id="0">the percentage of tile sentences</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>Semantic interpretation implies a search in the knowledge base which takes the constraints into account that derive fl'om a particular dependency parse tree .</sentence>
				<definiendum id="0">Semantic interpretation</definiendum>
				<definiens id="0">implies a search in the knowledge base which takes the constraints into account that derive fl'om a particular dependency parse tree</definiens>
			</definition>
			<definition id="1">
				<sentence>ELi m.Cto ) is added to the knowledge base , where RELi denotes tile i th reading .</sentence>
				<definiendum id="0">RELi</definiendum>
				<definiens id="0">tile i th reading</definiens>
			</definition>
			<definition id="2">
				<sentence>To match a concept definition C against tile constraints imposed by 12+ and 12_ , we define the function get-roles ( C ) = : CR , where C12 denotes the set of conceptual roles associated with C , which are then used as starting points for the path search .</sentence>
				<definiendum id="0">C12</definiendum>
				<definiens id="0">tile constraints imposed by 12+ and 12_</definiens>
				<definiens id="1">the function get-roles ( C ) = : CR , where</definiens>
				<definiens id="2">the set of conceptual roles associated with C , which are then used as starting points for the path search</definiens>
			</definition>
			<definition id="3">
				<sentence>l'pl ( ~tali ( ) ll abstla ( : ts away ' fr ( ) ill 1 } IOS ( ~ Slll'faC ( } 1 ) \ ] 1 ( UH ) I11 ( ~I1~1 ~11 ( ( \ [ CI'O , ~/L ( ~a ~l ; lIOl'IlI~diz ( ~d'~ ( : altOllical ( : OllCel ) tual ret ) resentatiolt of lhe inlmt , as need ( ~ ( t , e.g. , for mfiformly queryilig th ( } kl ( owlr ; dge lmse .</sentence>
				<definiendum id="0">UH</definiendum>
				<definiens id="0">altOllical ( : OllCel ) tual ret</definiens>
			</definition>
			<definition id="4">
				<sentence>hi this \v ( M ( , a total of about 20,000 verbs \\ ; ere subsumed by 700 categories to reflect a semantic generalization ill l : erms of a hie .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">a total of about 20,000 verbs \\ ; ere subsumed by 700 categories to reflect a semantic generalization ill l : erms of a hie</definiens>
			</definition>
			<definition id="5">
				<sentence>~ ; ia'i ( : ted by a positive list l'end ( ~red l ) y the apl ) licable production rule .</sentence>
				<definiendum id="0">ia'i</definiendum>
				<definiens id="0">ted by a positive list l'end ( ~red l ) y the apl ) licable production rule</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>inder of this l ) aper describes how to 1 ) erform bilingua.1 word clustering using standard monoh ; ngual document clustering techniques 1 ) y converting the problem space ; the va.rious clustering algorithms which were investiga .</sentence>
				<definiendum id="0">aper</definiendum>
				<definiens id="0">describes how to 1 ) erform bilingua.1 word clustering using standard monoh ; ngual document clustering techniques 1 ) y converting the problem space</definiens>
			</definition>
			<definition id="1">
				<sentence>Using a bilingual dictionary -which may be created fl'om the corl ) us using statistical meth ( ) &lt; Is , such as those of Peter \ ] ~rown el al ( 71990 ) or the author 's own l ) r ( ~viotls• work ( Brown , 11997 ) and the parallel text , create a rough ma.pping 1 ) etween the words in the source-language half of each translation example in tile corpus and tile target-language half el'that example. Whenever there is exactly one l ) ossible translation candidate listed for a word by the mapping , generate a bilingual word pair consisting of the word and its translation. This word pair will be treated as an indivisible token in further processing , adding bilingual information to the clustering process. \ ] eorming 1 ) airs in this manner causes each distinct translation of a. word to be treated as a separate sense ; although translation pairs do not exactly correspond to word senses , pairs can be formed without any additional knowledge sonrces and are what tile EBM : I ' systern requires for its equivalence classes. 1 , 'or every unique word pair found in the 1 ) revious step , we a.ccurnulate counts for each word in the surrounding context of its occurrences. The context of ~n occurrence is defined to be tile N words immediately prior to and the N words immediately following the occurrence ; N currently is set to 3. Because word order is important , counts are accumulated separately for each position within the context , i.e. for N = 3 , a particular context word may contribute to any of six different counts , depending on its location relative to the occurrence. Further , as the distance ffoln the occurrence increases , the surrounding words become less likely to be a true part of the word-pair 's context , so tile counts are weighted to give the greatest importance to the words immediately adjacent to the word pair being examined. Currently , a silnple linear decay fl'om 1.0 to -~ is used , but other decay functions such as the reciprocal of the distance are also possible. Tile resulting weighted set of word counts tbrms the above-mentioned I ) seudodocument which is converted into a term vector Ibr cosine similarity computations ( a standaM measure in information retrieval , defined as the dot product of two term vectors normalized to unit length ) , If the clustering is seeded with a. set of initial equivalence classes ( which will be discussed below ) , then the equivalences will be used to generalize the contexts as they are added to tile overall counts \ [ 'or tile word pair. Any words in the context for which a unique correspondence can be found ( and f'or which the word and its corresponding translation are one of the pah : s in an equivalence class ) will be counted as if the name of the equivMence class had been l ) resent in the text rather than the original word. For example , if days of the week are an equivalence class , then ' : ( lid he come on Fridas : ' and `` did he leave on Mends3 : ' will yield identical context vectors for `` come '' and `` leave '' , maldng it easier \ [ 'or those two terms to chlster together. To illustrate the conversion process , consider tile li'rench word `` ( 'inq '' in two examl ) les where it translates into English as : :five '' ( thus forming tile word pair `` cinq_fi ve '' ) : &lt; NUt &gt; &lt; NI/L &gt; Le ci , zq jours dcpuis la &lt; NUL &gt; &lt; NUL &gt; 73e five dags si~zce lhe ellcs com'me~ , cc~w~ , t c~z cinq jours .</sentence>
				<definiendum id="0">bilingual dictionary -which</definiendum>
				<definiens id="0">may be created fl'om the corl ) us using statistical meth ( ) &lt; Is , such as those of Peter \ ] ~rown el al ( 71990 ) or the author 's own l ) r ( ~viotls• work ( Brown , 11997 ) and the parallel text , create a rough ma.pping 1 ) etween the words in the source-language half of each translation example in tile corpus and tile target-language half el'that example. Whenever there is exactly one l ) ossible translation candidate listed for a word by the mapping , generate a bilingual word pair consisting of the word and its translation. This word pair will be treated as an indivisible token in further processing , adding bilingual information to the clustering process. \ ] eorming 1 ) airs in this manner causes each distinct translation of a. word to be treated as a separate sense ; although translation pairs do not exactly correspond to word senses , pairs can be formed without any additional knowledge sonrces and are what tile EBM : I ' systern requires for its equivalence classes. 1 , 'or every unique word pair found in the 1 ) revious step , we a.ccurnulate counts for each word in the surrounding context of its occurrences. The context of ~n occurrence is defined to be tile N words immediately prior to and the N words immediately following the occurrence ; N currently is set to 3. Because word order is important , counts are accumulated separately for each position within the context</definiens>
				<definiens id="1">a particular context word may contribute to any of six different counts , depending on its location relative to the occurrence. Further , as the distance ffoln the occurrence increases , the surrounding words become less likely to be a true part of the word-pair 's context , so tile counts are weighted to give the greatest importance to the words immediately adjacent to the word pair being examined. Currently , a silnple linear decay fl'om 1.0 to -~ is used , but other decay functions such as the reciprocal of the distance are also possible. Tile resulting weighted set of word counts tbrms the above-mentioned I ) seudodocument which is converted into a term vector Ibr cosine similarity computations ( a standaM measure in information retrieval , defined as the dot product of two term vectors normalized to unit length ) , If the clustering is seeded with a. set of initial equivalence classes ( which will be discussed below ) , then the equivalences will be used to generalize the contexts as they are added to tile overall counts \ [ 'or tile word pair. Any words in the context for which a unique correspondence can be found ( and f'or which the word and its corresponding translation are one of the pah : s in an equivalence class ) will be counted as if the name of the equivMence class had been l ) resent in the text rather than the original word. For example , if days of the week are an equivalence class , then ' : ( lid he come on Fridas : ' and `` did he leave on Mends3 : ' will yield identical context vectors for `` come '' and `` leave '' , maldng it easier \ [ 'or those two terms to chlster together. To illustrate the conversion process , consider tile li'rench word `` ( 'inq '' in two examl ) les where it translates into English as : :five '' ( thus forming tile word pair `` cinq_fi ve '' ) : &lt; NUt &gt; &lt; NI/L &gt; Le ci , zq jours dcpuis la &lt; NUL &gt; &lt; NUL &gt; 73e five dags si~zce lhe ellcs com'me~ , cc~w~ , t c~z cinq jours</definiens>
			</definition>
			<definition id="2">
				<sentence>rity measur ( ; is a l ) ov ( ~ a. l ) r ( ; del ; er nfin ( 'd threshold , the new word pair is i ) laced in tile corresponding cluster ; otherwis % a now ( ; \ ] usi ; er is crea .</sentence>
				<definiendum id="0">rity measur ( ;</definiendum>
				<definiendum id="1">er nfin</definiendum>
				<definiens id="0">a l</definiens>
			</definition>
			<definition id="3">
				<sentence>cosin ( ; similarity 1 ) ( ; 1 ; w ( ~ ( ; n 1 , h ( ~ i ) s ( ; u ( lo &lt; locumonl , a.nd the centroid o1 '' the oxisting cluster ( standard grOUl ) -a.vera.ge clusto.rillg ; ) the l ) seudo-docuni ( ; nl ; a.nd all nl ( ; nll ) ers o\ [ ' the 0xisting ( : lust ( ; , ' ( a.voragc-link clustoring ) cosine simila.r\ ] l ; io.s I ) ctweon l ; he l ) seudo ( locuinent an ( \ ] all molnl ) ( ~ , 's or l he existing ( 'hlster ( rool.-nloa.n-sqllar ( , nlo ( lifical.ion of average-liNl¢ clustering ) Thoso i ; hro ( ~ vnria.tiol , S give hlc , 'eas\ ] ngly IIl ( ) l ' ( ' : weight to 1 , ho nea.rer mcml ) ers of ' tho oxist.ing cl ust ; cr. Tim t ) o ( ; 1 ; oin-u 1 ) a.gglomera.tive algoril ; hms all funcl ; ion I ) y ( ; tea.tills a. clustor For each I ) Seudo ( \ [ o ( : unlenl , , t ; hon r ( ; i ) ( ; a.1 ; ( ; ( lly ln ( u : ging l : li ( ; two clusl ; ors witli the \ ] iighesl ; siinila.ril , y score unl , il 110 ( , WO C\ ] tlS| , orH \ ] lSt , vo , % , q\ ] iilila , ril ; y .~ ( : Ol ' ( ~ ( ~x ( '. ( ~ ( ; ding a l ) re ( Iol ; ornlino ( \ ] 1 ; hl : eshold. The three vari- ; / , IIi ; S } / , ga , ill differ ( ) lily ill 1 ; lio S\ ] liiilaril , y lllO } lStll'O O llll ) loyc ( l : \ ] . cosine simila.rity between clustor centroids ( st~ul ( la.rd agglomei : a.tivo clustering ) l ) ers of the two clusters ( a.vera.ge-tink ) Of ni ( '.nll ) oi : s of l , \ ] ie i ; wo clusl ; ( ' , rs ( single-lin\ ] { ) l '' oi : ( ; acli of the va.i : ia.tions a.bovc , the l ) r ( ~ ( l ( ; 1 , er niincd ( ; hreshol ( I is a. funci ; ion of word \ [ 'r ( xluoncy. Two words wliich each a.l ) l ) ea.r only onc ( Y in the entire tra.ining text a.nd ha.re a. high simila.rib , score a.ro more likely to ha.re a.l ) l ) ea.red in siniila.r contexl ; s I ) y cohicide.nce l : ha.n 1 ; wo wor ( ls which each a,1 ) pea.r ill 1 ; he traJliil/g 1 ; ( ; xi ; lifty tin-its. l , 'ro ( t UO I/cy 5 ( J 7 810 \ ] 2 \ ] 5 &gt; 16 Threshold1 \ ] .00 2 0.85 3 0.80 4 0.75 9 0.55 1 \ ] 0.50 I , 'igure \ ] : Chtsleling 'l'hro .</sentence>
				<definiendum id="0">oxisting cluster</definiendum>
				<definiendum id="1">ion I</definiendum>
				<definiens id="0">standard grOUl ) -a.vera.ge clusto.rillg ; ) the l ) seudo-docuni</definiens>
			</definition>
			<definition id="4">
				<sentence>llel Strait ( Is : A 1 ) re liminary lnvesCiga .</sentence>
				<definiendum id="0">Strait ( Is</definiendum>
				<definiens id="0">A 1 ) re liminary lnvesCiga</definiens>
			</definition>
			<definition id="5">
				<sentence>Gaijin : A 'lhml ) la.te-I ) riven Bootstrapl ) ing AI ) l ) roacll to Exa , ml ) Ie-Ba .</sentence>
				<definiendum id="0">Gaijin</definiendum>
				<definiens id="0">A 'lhml ) la.te-I</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>Theory refinement consists of iml ) roving an existing knowledge base so that it ; better at : cords with data .</sentence>
				<definiendum id="0">Theory refinement</definiendum>
				<definiens id="0">consists of iml ) roving an existing knowledge base so that it ; better at : cords with data</definiens>
			</definition>
			<definition id="1">
				<sentence>Changing the rules : A ( : ( mq ) rt~hensiv ( , , al ) prtmcli to th ( ' .</sentence>
				<definiendum id="0">Changing the rules</definiendum>
				<definiens id="0">A ( : ( mq ) rt~hensiv ( , , al ) prtmcli to th ( '</definiens>
			</definition>
</paper>

		<paper id="2167">
			<definition id="0">
				<sentence>\ ] 'his is a tricky prol ) lem , as opposed to the ease in English where a word is the unit of NE candidates .</sentence>
				<definiendum id="0">\ ] 'his</definiendum>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>s in the following way if the rood ( ; 1 provides the n-best del ) en1A bunsctsu phrase ( BP ) is a chunk of words ( -onsist ; ing of a content word ( noun , verl ) , adjective , etc. ) accoml ) mfied by sonic flmctional word ( s ) ( i ) arti ( : le , mlxiliary , etc. ) .</sentence>
				<definiendum id="0">en1A bunsctsu phrase</definiendum>
				<definiendum id="1">BP</definiendum>
				<definiens id="0">a chunk of words</definiens>
			</definition>
			<definition id="1">
				<sentence>When ( r is set 1 ; o be higher ( closer to 1.0 ) , t ; he accuracy is cxt ) ected to become higher , while the coverage is ext ) ecl ; ed to become lowe , : , and vi ( : e versm Here , ( ; over~ge C* and a , ( ; ctlra ( ; y A are defined as follows : # of the .</sentence>
				<definiendum id="0">vi</definiendum>
				<definiens id="0">ext ) ecl ; ed to become lowe , : , and</definiens>
			</definition>
			<definition id="2">
				<sentence>A committee consists of a set of weighting functions and a combination flmction .</sentence>
				<definiendum id="0">committee</definiendum>
				<definiens id="0">consists of a set of weighting functions and a combination flmction</definiens>
			</definition>
			<definition id="3">
				<sentence>To avoid this problem , we consider the tbllowing weighting flmction : w~J k = @ lkAM~ ( PMk ( ' , ' ( bi , b : i ) l.s ) ) ( 5 ) where AMk ( P ) is the function that returns the expected accuracy of Mk 's vote with its depenMk dency probability p , and oz i is a normalization factor .</sentence>
				<definiendum id="0">AMk ( P )</definiendum>
				<definiendum id="1">oz i</definiendum>
				<definiens id="0">the function that returns the expected accuracy of Mk 's vote with its depenMk dency probability p , and</definiens>
			</definition>
			<definition id="4">
				<sentence>% ( i ) M~ ( r ( b~ , bj ) l , s ) ) ( 6 ) where AMkcl , i ( P ) is the P-A curve of model Mk only tbr the problems of class Cb~ in training data , and flMk is a normalization factor .</sentence>
				<definiendum id="0">flMk</definiendum>
				<definiens id="0">class Cb~ in training data , and</definiens>
				<definiens id="1">a normalization factor</definiens>
			</definition>
			<definition id="5">
				<sentence>We conducted eXl ) erinmnts using the tbllowing tive statistical parsers : Table 1 : The total / l l-t ) oint accuracy achieved 1 ) y each individual model total 11-point A 0.8974 0.9607 B 0.8551 0.9281 C 0.8586 0.9291 D 0.8470 0.9266 E 0.7885 0.8567 • KANA ( Ehara , 1998 ) : a bottom-up model based oll maxinmm entropy estimation , Since dependency score matrices given by KANA have no probabilistic semantics , we normalized them tbr each row using a certain function manually tuned for this parser .</sentence>
				<definiendum id="0">KANA</definiendum>
				<definiens id="0">a bottom-up model based oll maxinmm entropy estimation</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>The parser we describe uses fragments l'rom LFG-aunotated sentences to parse new sentences and Monte Carlo techniques to compute the most probable parse .</sentence>
				<definiendum id="0">Monte Carlo</definiendum>
			</definition>
			<definition id="1">
				<sentence>A derivation for an LFG-DOP representation R is a sequence o1 ' fragments the first of which is labeled with S and for which the itcrative application of the composition operation produces R. The two-stage composition operation is illustrated by a simple example .</sentence>
				<definiendum id="0">derivation for an LFG-DOP representation R</definiendum>
				<definiens id="0">a sequence o1 ' fragments the first of which is labeled with S and for which the itcrative application of the composition operation produces R. The two-stage composition operation is illustrated by a simple example</definiens>
			</definition>
			<definition id="2">
				<sentence>Let CP ( fl CS ) denote the probability of choosing a fragment ffrom a competition set CS containing J ; then the probability of a derivation D = &lt; fJ , f2 ... Jk &gt; is C2 ) P ( &lt; ag , f , ... fk &gt; ) = Hi cpq } I csi ) where the competitio , l~robability CP0el CS ) is expressed in terms of fragment probabilities Pq ) : ( 3 ) CP ( f I CS ) PCt ) Z , , , ~ cs P ( /+ ' ) Bed &amp; Kaplan give three definitions of increasing complexity for the competition set : the first definition groups all fi'agments that only satisfy the Categorymatching condition o1 ' the composition operation ( thus leaving out the Uniqueness , Coherence and Completeuess conditions ) ; the second definition groups all fragments which satisfy both Category-matching and Uniqueness ; and the third defi , fition groups all fragments which satisfy Category-matching , Uniqueness and Coherence .</sentence>
				<definiendum id="0">CP</definiendum>
				<definiendum id="1">condition o1 ' the composition operation</definiendum>
				<definiens id="0">( fl CS ) denote the probability of choosing a fragment ffrom a competition set CS containing J ; then the probability of a derivation D = &lt; fJ , f2 ... Jk &gt; is C2 ) P ( &lt; ag , f , ... fk &gt; ) = Hi cpq } I csi ) where the competitio , l~robability CP0el CS ) is expressed in terms of fragment probabilities Pq ) : ( 3 ) CP ( f I CS ) PCt ) Z , , , ~ cs P ( /+ '</definiens>
			</definition>
			<definition id="3">
				<sentence>We accolnplish lhis by a very simple estimalor : lhe Turing-Good estimator ( Good 1953 ) which computes lhe probability mass of unseen events as nl/N where n I is the , mmber of singleton events and N is the total number of seen events .</sentence>
				<definiendum id="0">lhe Turing-Good estimator</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">the total number of seen events</definiens>
			</definition>
			<definition id="4">
				<sentence>1 ) ) where 1 -- &gt; \ [ ( SUB J=2 ) ( TENSE = PAST ) ( PRED = fall ( SUBJ ) ) \ ] 2 -- &gt; \ [ ( PRED = John ) ( NUM = SG ) \ ] The indexed trees are then fragmented by applying the Tree-DOP decomposition operations described in section Frontier and Discard are applied to the f-structure units that correspond to the indices in the e-structure subtrees .</sentence>
				<definiendum id="0">SUB</definiendum>
				<definiens id="0">the Tree-DOP decomposition operations described in section Frontier and Discard are applied to the f-structure units that correspond to the indices in the e-structure subtrees</definiens>
			</definition>
			<definition id="5">
				<sentence>Disambiguation is accomplished by computing a large number o1 ' random derivations from the chart ; this technique is known as `` Monte Carlo disambiguation '' and has been extensively described ill the literature ( e.g. Bed 1998 ; Chappelier &amp; Rajman 1998 ; Goodman 1998 ) .</sentence>
				<definiendum id="0">Disambiguation</definiendum>
			</definition>
			<definition id="6">
				<sentence>`` Lexical-Functional Grammar : A Formal System for Grammatical Representation '' , in J. Bresnan ( ed . )</sentence>
				<definiendum id="0">Lexical-Functional Grammar</definiendum>
				<definiens id="0">A Formal System for Grammatical Representation ''</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>Systematic software testing requires a match between the test subject ( module or comt ) lete system ) and a test suite ( collection of test items , i.e. , sample input ) .</sentence>
				<definiendum id="0">Systematic software testing</definiendum>
				<definiens id="0">requires a match between the test subject ( module or comt ) lete system ) and a test suite ( collection of test items</definiens>
			</definition>
			<definition id="1">
				<sentence>Lexicalfunctional grmmnar : A formal system for grammatical rel/resentation .</sentence>
				<definiendum id="0">Lexicalfunctional grmmnar</definiendum>
			</definition>
</paper>

		<paper id="2174">
			<definition id="0">
				<sentence>The whole system consists of five modules : speech recognizen translator , dialogue manageh generator and speech synthesizer .</sentence>
				<definiendum id="0">whole system</definiendum>
				<definiens id="0">consists of five modules : speech recognizen translator , dialogue manageh generator and speech synthesizer</definiens>
			</definition>
			<definition id="1">
				<sentence>Here the interlingua is an underspecified selnantic representation ( USR ) .</sentence>
				<definiendum id="0">USR</definiendum>
				<definiens id="0">an underspecified selnantic representation</definiens>
			</definition>
			<definition id="2">
				<sentence>Busemann ( Busemann 1996 ) used hybrid method to allow template , canned texts and general rules appearing in one formalism and to tackle the problem of the inefficiency of the grammar-based surface generation system .</sentence>
				<definiendum id="0">Busemann</definiendum>
				<definiens id="0">template , canned texts and general rules appearing in one formalism and to tackle the problem of the inefficiency of the grammar-based surface generation system</definiens>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>Case stacking is a phenomenom that occurs in many Australian languages ( such as Warlt ) iri and Kayardild ) and e.g. Old Georgian .</sentence>
				<definiendum id="0">Case stacking</definiendum>
				<definiens id="0">a phenomenom that occurs in many Australian languages ( such as Warlt ) iri</definiens>
			</definition>
			<definition id="1">
				<sentence>quage L over '' an alphabet E = { wi I 0 _ &lt; i &lt; n } is called a semilinear language if its image under the Parikh mapping is a semilinear set , where the PariMt mapping ~IJ : E* ~ N n is defined as follows : &lt; o , ... , o ) wi ~ e ( i ) .fi ) r O &lt; i &lt; n or/3 ~ vI , ( ( ~ ) -F'I ' ( fl ) for all ~ , fl C E* wh , ere e ( i ) is the i + 1-ttL 'unit vector ' , wh , ich , consists of zeros except for the i-th component , wh , ich , is 1. Note that given a term t the Parikh image of all A ( t ) -strings is the same since these are just concatenations of difthrent permutations of the units in A ( t ) . In the tbllowing we make use of a proof technique used in ( Michaelis and Kracht , 1997 ) to show that Old Georgian is not a semilinear language. We cite , a special instance of a proposition given therein : Proposition 10. M be a subset of the properties l~ ) 1 ( k ) Let P ( k ) = , ~k 2 + 2~ -- -- -~-k and N n , where n &gt; 2 , which has N+ there are some numbers E N for wh , ich the n-tuple ( k , P ( k ) , l~ k ) I ( k ) \ belon ( ls to M. ~ '' '' ~ '~ -- 11 upper bound .</sentence>
				<definiendum id="0">Parikh mapping</definiendum>
				<definiendum id="1">Parikh image of all A ( t</definiendum>
				<definiens id="0">an alphabet E = { wi I 0 _ &lt; i &lt; n } is called a semilinear language if its image under the</definiens>
				<definiens id="1">follows : &lt; o , ... , o ) wi ~ e ( i ) .fi )</definiens>
				<definiens id="2">consists of zeros except for the i-th component , wh , ich , is 1. Note that given a term t the</definiens>
				<definiens id="3">concatenations of difthrent permutations of the units in A ( t ) . In the tbllowing we make use of a proof technique used in ( Michaelis and Kracht , 1997 ) to show that Old Georgian is not a semilinear language. We cite , a special instance of a proposition given therein : Proposition 10. M be a subset of the properties l~ ) 1 ( k ) Let P ( k ) = , ~k 2 + 2~ -- -- -~-k and N n , where n &gt; 2 , which has N+ there are some numbers E N for wh , ich the n-tuple ( k , P ( k ) , l~ k ) I ( k ) \ belon ( ls to M. ~ '' '' ~ '~ -- 11 upper bound</definiens>
			</definition>
			<definition id="2">
				<sentence>Then LM contains all A ( s , ) -strings .</sentence>
				<definiendum id="0">LM</definiendum>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>The typical al3pl'oacll to XML authoring views an XML doctmlcnt as a mixture of wee-like strttctttre , expressed througll balanced labelled parentheses ( tim lags ) , and of sul : face , expressed llu'ough free lexi interspersed between lhe tags ( PCI ) ATA ) .</sentence>
				<definiendum id="0">XML doctmlcnt</definiendum>
				<definiens id="0">a mixture of wee-like strttctttre , expressed througll balanced labelled parentheses ( tim lags ) , and of sul : face , expressed llu'ough free lexi interspersed between lhe tags</definiens>
			</definition>
			<definition id="1">
				<sentence>For leaves , lhe type is a semanlically specilic category such as Integer , Animal , etc. , and lhe label is a specilic concept of this type , such as three or dog ) Styling responsible for producing tim text itself .</sentence>
				<definiendum id="0">lhe label</definiendum>
				<definiens id="0">such as three or dog ) Styling responsible for producing tim text itself</definiens>
			</definition>
			<definition id="2">
				<sentence>4 The tree structure of the document then becomes the sole repository of content , and can be viewed as a kind of interlingua for describing a point in the expressive space of tile DTD ( a strongly domain-dependent space ) ; it is then the responsability of the language-specific rendering mechanisms to `` display '' such content in each individual language where the document is needed .</sentence>
				<definiendum id="0">tile DTD</definiendum>
				<definiens id="0">becomes the sole repository of content , and can be viewed as a kind of interlingua for describing a point in the expressive space of</definiens>
			</definition>
			<definition id="3">
				<sentence>The second , h~teraction Grammars ( IG ) , is a specialization of Definite Clause Grammars strongly inspired by GF .</sentence>
				<definiendum id="0">IG</definiendum>
				<definiens id="0">a specialization of Definite Clause Grammars strongly inspired by GF</definiens>
			</definition>
			<definition id="4">
				<sentence>• The grammars are revelwible , that is , can be used both for generation and for parsing ; • The authoring process is an interactive process of repeatedly asking the author to further specify nodes in the absmlct tree of which only the type is known at the 1 ) oint of interacti ( m ( tyFe re/itlemeHt ) .</sentence>
				<definiendum id="0">authoring process</definiendum>
				<definiens id="0">an interactive process of repeatedly asking the author to further specify nodes in the absmlct tree of which only the type is known at the 1 ) oint of interacti ( m ( tyFe re/itlemeHt )</definiens>
			</definition>
			<definition id="5">
				<sentence>The Grammatical Framework ( GF ; ( Ranta , 2000 ) ) is a special-purpose programming hmguage combining co~zstrttctive type thee O , with an annotation hmguage for concrete syntax .</sentence>
				<definiendum id="0">Grammatical Framework</definiendum>
				<definiens id="0">a special-purpose programming hmguage combining co~zstrttctive type thee O , with an annotation hmguage for concrete syntax</definiens>
			</definition>
			<definition id="6">
				<sentence>The XML representation of the capital ( 51 ' France is &lt; City &gt; &lt; cap / &gt; &lt; Country &gt; &lt; Fra / &gt; &lt; /Country &gt; &lt; /City &gt; which is a wdid XML object w.r.t , tile given DTD .</sentence>
				<definiendum id="0">XML representation</definiendum>
				<definiens id="0">a wdid XML object w.r.t , tile given DTD</definiens>
			</definition>
			<definition id="7">
				<sentence>• An XML represents a unique GF tree .</sentence>
				<definiendum id="0">XML</definiendum>
				<definiens id="0">a unique GF tree</definiens>
			</definition>
			<definition id="8">
				<sentence>• A DTD represents a unique GF abstract grammar .</sentence>
				<definiendum id="0">DTD</definiendum>
				<definiens id="0">a unique GF abstract grammar</definiens>
			</definition>
			<definition id="9">
				<sentence>An example is the following alternative declaration of Country and City : cat Country ; cat City ( Co : Country ) ; Under tiffs definition , there are no objects of type City ( which is no longer a well-formed type ) , but of types City Ger and City Fra .</sentence>
				<definiendum id="0">cat City</definiendum>
			</definition>
			<definition id="10">
				<sentence>The expression city ( C , Co ) is usually read as the relation `` C is a city of Co '' , which is line for computational purposes , but this reading obscures the notion that the object C is being typed as a city ; more precisely , it is being typed as a city of Co .</sentence>
				<definiendum id="0">expression city</definiendum>
				<definiendum id="1">C</definiendum>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>Co-occurrences m'e the least selective filters associated with morphosyn| ; ~mti ( : varimlts ; they nre ext/ected to extract all the l ) Ossible ( 'orrect nomino-verb : fl variations ( recall value 1.0 ) .</sentence>
				<definiendum id="0">Co-occurrences</definiendum>
				<definiens id="0">m'e the least selective filters associated with morphosyn| ; ~mti ( : varimlts ; they nre ext/ected to extract all the l</definiens>
			</definition>
			<definition id="1">
				<sentence>Co-occurrences are extra ( : ted from a ll-word window ( 9 intervening words ) .</sentence>
				<definiendum id="0">Co-occurrences</definiendum>
			</definition>
			<definition id="2">
				<sentence>P0 is the proportion of times the coders agree and I~ , is the proportion of tiines we would expect them to agree by chance .</sentence>
				<definiendum id="0">P0</definiendum>
			</definition>
			<definition id="3">
				<sentence>The paraphrase judgment is evaluated in a new way , from a practical point of view : two sequences are said to be a paraphrase of each other if the user of an information system considers that they bring identical or sin &gt; ilar information content .</sentence>
				<definiendum id="0">paraphrase judgment</definiendum>
				<definiens id="0">information system considers that they bring identical or sin &gt; ilar information content</definiens>
			</definition>
</paper>

		<paper id="1062">
			<definition id="0">
				<sentence>A generator for G provides for any given fstructure F the set ; of strings that are related to it ; by the grmn111 ar : ( 4 ) Gcna ( F ) = { s \ [ ~c , ¢ s.t. ( s , c , ¢ , F ) E At , , } .</sentence>
				<definiendum id="0">generator</definiendum>
				<definiendum id="1">s.t.</definiendum>
				<definiens id="0">s , c , ¢ , F ) E At , , }</definiens>
			</definition>
			<definition id="1">
				<sentence>For our tmrposes , then , ml LFG grammar G is a 4-tuple ( N , T , S , R } where N is the set of nonterminal categories , T is the set of terminal symbols ( the lexical items ) , S E N is the root category , and 1~ , is the set ; of annotated productions .</sentence>
				<definiendum id="0">T</definiendum>
				<definiendum id="1">S E N</definiendum>
				<definiens id="0">a 4-tuple ( N , T , S , R } where N is the set of nonterminal categories</definiens>
				<definiens id="1">the set of terminal symbols ( the lexical items ) ,</definiens>
				<definiens id="2">the root category</definiens>
				<definiens id="3">the set ; of annotated productions</definiens>
			</definition>
			<definition id="2">
				<sentence>Categories in NI ; other than SF are written X : v : D , where X is a category in N , v is contained in 17 , and D is an instantiated description in Pow ( IP ( F ) ) .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">D</definiendum>
				<definiens id="0">a category in N</definiens>
			</definition>
			<definition id="3">
				<sentence>~NT ' / ' ( Vs SPEC ) I ( v PRED ) = q~aLL ( ( SUBa ) ) ' \ [ , ( 1 ) TENSE ) = PAST -- - } M : 'U'~ ( V I } REI ) ) = tFALL ( ( SUBJ ) ) t~ • \ [ ( ~ TI~NSE ) = PAST J = l ( 'Os ~ 'Us ( Us SPEC ) = INDEF -+ NP : v~ : ( v~ NUM ) = SC Us PLIED ) -- ~ ISTUDENTI ( Us SI'EC ) N : vs : VP : v : { ( 'Os I'IIH ) ) = 'STUDI , :NT'~ ( ~ TE~S~ ) = PAS~r J apply the pumt ) ing lemma to systematically produce longer strings for exmnination .</sentence>
				<definiendum id="0">ISTUDENTI</definiendum>
				<definiens id="0">v~ NUM ) = SC Us PLIED ) -- ~</definiens>
			</definition>
			<definition id="4">
				<sentence>A head-driven strategy ( e.g. van Noord 1993 ) identities the lexical heads first , finds the rules that exl ) and them , and then uses information associated with those heads , such as their grmmnatical flmetion assigmnents , to pick other categories to exlmnd .</sentence>
				<definiendum id="0">head-driven strategy</definiendum>
				<definiens id="0">finds the rules that exl ) and them , and then uses information associated with those heads , such as their grmmnatical flmetion assigmnents</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>tj ) , with to 1 -- 1 where p ( Ck ) is the probability of class Ck 1 ) eing tbund in the training corpus , and p ( vi\ [ Ck ) and p ( nj\ [ Ci~ ) define the probability that verb vi and noun nj be associated with the semantic dimension ( or meaning component ) of class C/~ .</sentence>
				<definiendum id="0">p ( Ck</definiendum>
				<definiens id="0">the probability of class Ck 1 ) eing tbund in the training corpus</definiens>
				<definiens id="1">the probability that verb vi and noun nj be associated with the semantic dimension ( or meaning component ) of class C/~</definiens>
			</definition>
			<definition id="1">
				<sentence>AP is an important generalization of the inter-substitutability assumption , as it extends tile assumption to cases of flmctionally heterogeneous verb-nonn pairs .</sentence>
				<definiendum id="0">AP</definiendum>
				<definiens id="0">an important generalization of the inter-substitutability assumption , as it extends tile assumption to cases of flmctionally heterogeneous verb-nonn pairs</definiens>
			</definition>
			<definition id="2">
				<sentence>( 5 ) , where Np is the number of different pairs attested in the training corpus .</sentence>
				<definiendum id="0">Np</definiendum>
				<definiens id="0">the number of different pairs attested in the training corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>Fig.1 illustrates a sample of noun clusters ( between curly brackets ) projected from a set of Sis , together with a list of the verbs tbund in the same Sis ( the suffix 'S ' stands tbr subject , and 'O ' for object ) .</sentence>
				<definiendum id="0">'O</definiendum>
				<definiens id="0">the suffix 'S ' stands tbr subject , and</definiens>
			</definition>
			<definition id="4">
				<sentence>Federici , S. , Montemagni , S. , Pirrelli , V. ( 1999 ) SENSE : an Analogy based Word Sense Disambiguation System .</sentence>
				<definiendum id="0">SENSE</definiendum>
				<definiens id="0">an Analogy based Word Sense Disambiguation System</definiens>
			</definition>
</paper>

		<paper id="2132">
			<definition id="0">
				<sentence>~1+ , * ... 5 , \ ] 1A term expresses a non-terminal symbol in IAIS , an ( 1 ~'~ non-terminld or a terminal symbol in l/ .</sentence>
				<definiendum id="0">1A term</definiendum>
				<definiens id="0">expresses a non-terminal symbol in IAIS</definiens>
			</definition>
			<definition id="1">
				<sentence>Given this kind of dependency information , the following conditions are imposed on Operation A and Operation B. Conditions for Operation A : Condition A1 ( when the leftmost RHS term of a rule is a head term ) : Given an inactive arc Arc1 denoted by \ [ A ~ ... \ ] and a rule which has two or more RHS terms and the leftmost RHS term is a head denoted by { X -+ A * B ... } , Operation A is executed only if there is dependency information 144 , lYb where 1,14~ is a word matching the LHS term A of Arci and lVb is a word located anywhere to the right of the end I ) oint of Arc1 .</sentence>
				<definiendum id="0">lVb</definiendum>
				<definiens id="0">a head term ) : Given an inactive arc</definiens>
				<definiens id="1">a word located anywhere to the right of the end I ) oint of Arc1</definiens>
			</definition>
			<definition id="2">
				<sentence>Condition B2 ( when the head term is on the left side of the leftinost active RHS term of an active arc ) : Given an active mc AreA denoted by \ [ X ~ ... A* ... B ... \ ] and all inactive arc Arc1 denoted by \ [ B -- + ... \ ] such that the end point of Area is the same as the start point of Arc1 , Operation B is executed only if there is det ) endency information W , , ~ Wb where 144~ is a word matching the RHS term A of AreA , and Wv is a word matching the LItS terln B of Arc1 .</sentence>
				<definiendum id="0">Wv</definiendum>
				<definiens id="0">the left side of the leftinost active RHS term of an active arc ) : Given an active mc AreA denoted by</definiens>
				<definiens id="1">a word matching the RHS term A of AreA , and</definiens>
				<definiens id="2">a word matching the LItS terln B of Arc1</definiens>
			</definition>
			<definition id="3">
				<sentence>ion I &lt; 1~ , -= &gt; lYb where 1'15~ is a word matching the RIIS term A of AreA , and lVt , is a word mat ( : hing the I~HS term , r3 of mrcl .</sentence>
				<definiendum id="0">lVt</definiendum>
				<definiens id="0">a word matching the RIIS term A of AreA</definiens>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>Equations 1 and 2 implement a linearinterpolating HMM that incorporates a mmfl ) cr 203 of sub-models ( rethrred to fl'om now by their A coefficients ) designed to reduce the effects of data sparseness .</sentence>
				<definiendum id="0">linearinterpolating HMM</definiendum>
			</definition>
			<definition id="1">
				<sentence>s uses no character feature inibrmation. )</sentence>
				<definiendum id="0">s</definiendum>
			</definition>
</paper>

		<paper id="1075">
			<definition id="0">
				<sentence>Translation templates as well as translation units are treated as paradigmatic flexible structures that depend on the available evidence .</sentence>
				<definiendum id="0">Translation templates</definiendum>
				<definiens id="0">paradigmatic flexible structures that depend on the available evidence</definiens>
			</definition>
			<definition id="1">
				<sentence>Translation alternatives of individual units ( in our case words ) are implicitly classified through their context , that is the constant part of the translation patterns they participate in .</sentence>
				<definiendum id="0">Translation alternatives of individual units</definiendum>
				<definiens id="0">the constant part of the translation patterns they participate in</definiens>
			</definition>
			<definition id="2">
				<sentence>Translation templates are compared with respect to their source and target language constituent patterns : ( a ) Conflicting templates , that is templates sharing only one of the two patterns are subsequcntly checked in terms of weight information .</sentence>
				<definiendum id="0">Translation templates</definiendum>
				<definiens id="0">templates sharing only one of the two patterns are subsequcntly checked in terms of weight information</definiens>
			</definition>
			<definition id="3">
				<sentence>MBT2 : A Method for Combining Fragments of Examples in Example-Based Machine Translation .</sentence>
				<definiendum id="0">MBT2</definiendum>
			</definition>
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>FRUMP selects a t ) articular sketchy script based on clues to styled events in news articles .</sentence>
				<definiendum id="0">FRUMP</definiendum>
				<definiens id="0">selects a t ) articular sketchy script based on clues to styled events in news articles</definiens>
			</definition>
			<definition id="1">
				<sentence>In other words , FRUMP selects an eml ) t3 ~ t ( uni ) late 1 whose slots will be tilled on the fly as t '' F\ [ UMP reads a news artMe .</sentence>
				<definiendum id="0">FRUMP</definiendum>
				<definiens id="0">selects an eml</definiens>
			</definition>
			<definition id="2">
				<sentence>The SUMMONS ( SUMMarizing Online News artMes ) system ( McKeown and Radev , 1999 ) takes teml ) late outputs of information extra ( : tion systems develofmd for MUC conference and generating smnmaries of multit ) le news artMes .</sentence>
				<definiendum id="0">SUMMONS ( SUMMarizing Online News artMes ) system</definiendum>
				<definiens id="0">takes teml ) late outputs of information extra ( : tion systems develofmd for MUC conference and generating smnmaries of multit ) le news artMes</definiens>
			</definition>
			<definition id="3">
				<sentence>SUMMARIST ( How and Lin , 1999 ) is a system designed to generate summaries of multilingual input texts .</sentence>
				<definiendum id="0">SUMMARIST</definiendum>
				<definiens id="0">a system designed to generate summaries of multilingual input texts</definiens>
			</definition>
			<definition id="4">
				<sentence>SUMMARIST uses positional importance , topic signature , and term frequency .</sentence>
				<definiendum id="0">SUMMARIST</definiendum>
				<definiens id="0">uses positional importance , topic signature , and term frequency</definiens>
			</definition>
			<definition id="5">
				<sentence>We rate a sentence as good simply if it also occurs in the ideal human-made extract , and measure it using combined recall and precision ( F-score ) .</sentence>
				<definiendum id="0">precision</definiendum>
				<definiens id="0">a sentence as good simply if it also occurs in the ideal human-made extract , and measure it using combined recall</definiens>
			</definition>
			<definition id="6">
				<sentence>N is the total mmfl ) or of document .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the total mmfl</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>`` J'\ ] '\~\ a ttril ) uÁ ; ( ~ ill I ; he l ) ro ( : ( ' , qs of synl ; a ( : i ; i ( : o-s ( 'manti ( : tagging of i~ very large ( 'orlms lit ' Cz ( ; ( : h. Dependency Treebank ( PDT ) PDT is a corpus ( a part fl'om the Czech Na .</sentence>
				<definiendum id="0">PDT ) PDT</definiendum>
			</definition>
			<definition id="1">
				<sentence>r of i ; ag G : is r/'quired 1 ) y the language , with rich intl ( ~ ( : ti ( /n ; ( :1 '' .</sentence>
				<definiendum id="0">ag G</definiendum>
				<definiens id="0">r/'quired 1 ) y the language</definiens>
			</definition>
			<definition id="2">
				<sentence>Every node in a TGTS is either contextually bound ( CB ) or non-bound ( NB ) ; this opposition is a linguistic couterpart of the cognitive dichotomy of 'given ' vs. hmw ' , where also an item , if corresponding to a 'given ' referent presented as occupying a newly characterized specific position ( often in relation to one or more 'given ' items ) , has the feature NB , cf. : ( 3 ) Give th , is to YOUR mother .</sentence>
				<definiendum id="0">TGTS</definiendum>
				<definiendum id="1">CB</definiendum>
				<definiens id="0">a linguistic couterpart of the cognitive dichotomy of 'given ' vs. hmw ' , where also an item , if corresponding to a 'given ' referent presented as occupying a newly characterized specific position ( often in relation to one or more 'given ' items ) , has the feature NB , cf. : ( 3 ) Give th , is to YOUR mother</definiens>
			</definition>
			<definition id="3">
				<sentence>Example ( with a very simplified linearized notation of the TGTS , in which every dependent is closed in its pair of parentheses ) : ( 7 ) K jdsotu ( C ) neni ( F ) nejmen .</sentence>
				<definiendum id="0">Example</definiendum>
				<definiens id="0">a very simplified linearized notation of the TGTS , in which every dependent is closed in its pair of parentheses</definiens>
			</definition>
			<definition id="4">
				<sentence>The degree of CD of a node that is being restored ( i.e. supposed to have been deleted in the surface form of the sentence ) , and thus also its position in the underlying word order , is determined on the basis of its relationship to its governing node .</sentence>
				<definiendum id="0">degree of CD</definiendum>
				<definiens id="0">of a node that is being restored ( i.e. supposed to have been deleted in the surface form of the sentence</definiens>
			</definition>
			<definition id="5">
				<sentence>Since such a node ahnost always is contextually bound ( with the exception of the specific case of coordinated structures , see the Note after point 8 in Section 2.2 above ) , it is placed to the left of its governing word ; more specifically : ( a ) if the restored node RN depends on a verb , then : ( b ) ( c ) ( aa ) if RN is not the single item depending on the given verb token , then RN is to be added in the 'Wackernagel position ' ; ( ab ) if RN has no sister nodes , then it is placed at the beginning of the clause ; if RN is restored as depending on a noun ( or adjective ) , I { N is placed as the least dynamic dependent of this governing word ; if more than one node are inserted as depending on one and the same item , then their order should confornl to tile systemic ( % anonical ' ) ordering of the valency slots ( see the remark on SO in Section 2.2 above , point 4 ) .</sentence>
				<definiendum id="0">RN</definiendum>
				<definiens id="0">the single item depending on the given verb token</definiens>
				<definiens id="1">no sister nodes , then it is placed at the beginning of the clause</definiens>
				<definiens id="2">the least dynamic dependent of this governing word ; if more than one node are inserted as depending on one and the same item</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>SUBJPASS ( revolver , seize ) SUBJPASS ( shotgun , seize ) VMODOBJ ( seize , at , party ) Shallow parser transducers al'C accessible via the XcLI ) a server enabling fast and robust execution ( Roux98 ) .</sentence>
				<definiendum id="0">SUBJPASS</definiendum>
				<definiens id="0">party ) Shallow parser transducers al'C accessible via the XcLI ) a server enabling fast and robust execution</definiens>
			</definition>
			<definition id="1">
				<sentence>It can be paraphrased as : If the lemma X , which is ambiguous between $ 1 , $ 2 , ... , S~ , appears in the dependency DEP ( X , Y ) or DEp ( Y , X ) then it can be disambiguated by assigning the sense Si .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">If the lemma</definiens>
			</definition>
			<definition id="2">
				<sentence>The resulting rule can be paraphrased as : If the lemma X , which is ambiguous between St , $ 2 , ... , Sn , appears in the dependency DEP ( X , ambiguityclass ( Y ) ) or DEl ' ( mnbiguity_class ( Y ) , X ) then it can be disambiguated by assigning the sense Si .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">DEl</definiendum>
				<definiens id="0">If the lemma</definiens>
			</definition>
			<definition id="3">
				<sentence>If one aud only one role matches the lexical context of the dependencies directly , the system uses it to disambiguate the word , i.e. to assign the sense number Si 4 to it ; otherwise , if several rules match directly at word level , the selection process uses the meta-semantic information encoded in SGMI , tags within the dictionary ( and kept in the rules oil purpose ) with the following preference strategy : rule built fl'om collocate ( &lt; C ( ) &gt; ) , from compounds examples ( &lt; LC &gt; ) , from idiomatic examples ( &lt; IA &gt; ) , t'rom structure examples ( &lt; L ( ) &gt; ) , from phrasal verb pattern examples ( &lt; IN &gt; ) , t'rom usage examples ( &lt; LU &gt; ) , and finally from general examples ( &lt; I , E &gt; ) .</sentence>
				<definiendum id="0">selection process</definiendum>
				<definiens id="0">the system uses it to disambiguate the word</definiens>
				<definiens id="1">uses the meta-semantic information encoded in SGMI , tags within the dictionary ( and kept in the rules oil purpose ) with the following preference strategy : rule built fl'om collocate ( &lt; C ( ) &gt;</definiens>
			</definition>
			<definition id="4">
				<sentence>LOCOLEX : the translation rolls off your tongue .</sentence>
				<definiendum id="0">LOCOLEX</definiendum>
			</definition>
			<definition id="5">
				<sentence>WordNet : An Electronic Lexical Database , MIT Press , Cambridge ( MA ) .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">An Electronic Lexical Database</definiens>
			</definition>
			<definition id="6">
				<sentence>SENSEVAL : An Exercise in Evaluating Word Sense Disambiguation Programs .</sentence>
				<definiendum id="0">SENSEVAL</definiendum>
			</definition>
</paper>

		<paper id="2136">
			<definition id="0">
				<sentence>This paper presents an alternative approach , based on an automatic discovery procedure , ExDIsCO , which identifies a set ; of relewmt documents and a set of event patterns from un-annotated text , starting from a small set of `` seed patterns . ''</sentence>
				<definiendum id="0">ExDIsCO</definiendum>
				<definiens id="0">identifies a set</definiens>
			</definition>
			<definition id="1">
				<sentence>Intbrmation Extraction is the selective extraction of specified types of intbrmation from natural language text .</sentence>
				<definiendum id="0">Intbrmation Extraction</definiendum>
				<definiens id="0">the selective extraction of specified types of intbrmation from natural language text</definiens>
			</definition>
			<definition id="2">
				<sentence>The pattern set is used to divide the cortins U into a set of relewmt documents , R ( which contain at ; least one instance of one of the patterns ) , and a set of non-relevant documents R = U R. • automatically convert each document in the eorIms into a set of candidate patterns , one for each clause • rank patterns by the degree to which their distribution is correlated with docmnent relevance ( i.e. , appears with higher frequency in relevant documents than in non-relewmt ones ) .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">contain at ; least one instance of one of the patterns ) , and a set of non-relevant documents R = U R. • automatically convert each document in the</definiens>
			</definition>
			<definition id="3">
				<sentence>CAppoint denotes the list of verbs { appoint , elect , promote , name , nominate } , C-Resign = { resign , depart , quit } , and C-Buy = { buy , purchase } .</sentence>
				<definiendum id="0">CAppoint</definiendum>
				<definiendum id="1">C-Buy</definiendum>
				<definiens id="0">the list of verbs { appoint , elect , promote , name , nominate }</definiens>
			</definition>
			<definition id="4">
				<sentence>c~ ( p ) ) '' '' , ' ( 5 ) ~c K ( d ) where t ; 11 ( ' , weights , wp arc ( tetint ; d using the telewm ( : ( : of the ( loeuments , a , s the total SUl ) l ) or ( ; which the pa , I ; I ; ern p receives : % = log ~ l ; .</sentence>
				<definiendum id="0">c~ ( p ) ) '' ''</definiendum>
				<definiendum id="1">wp arc</definiendum>
				<definiens id="0">the total SUl ) l ) or</definiens>
			</definition>
			<definition id="5">
				<sentence>The Seed pattern base consists of just the initial pattern set , given in the table on the previous page .</sentence>
				<definiendum id="0">Seed pattern base</definiendum>
			</definition>
			<definition id="6">
				<sentence>ExDIscO attains values within the range of the MUC participald ; S , all of which were either heavily-supervised or m~mually coded systems .</sentence>
				<definiendum id="0">ExDIscO</definiendum>
				<definiens id="0">attains values within the range of the MUC participald ; S , all of which were either heavily-supervised or m~mually coded systems</definiens>
			</definition>
</paper>

		<paper id="2111">
			<definition id="0">
				<sentence>After 3 years of specifying the UNL ( Universal Networking Language ) language and prototyping deconverters I from more than 12 languages and enconverters for about 4 , the UNL project has opened to the community by publishing the specifcations ( v2.0 ) of the UNL language , intended to encode the meaning of NL utterances as semantic hypergraphs and to be used as a `` pivot '' representation in multilingual information and communication systems .</sentence>
				<definiendum id="0">UNL</definiendum>
				<definiens id="0">UNL language , intended to encode the meaning of NL utterances as semantic hypergraphs and to be used as a `` pivot '' representation in multilingual information and communication systems</definiens>
			</definition>
			<definition id="1">
				<sentence>A UNL document is an html document with special tags to delimit the utterances and their rendering in UNL and in all natural languages currently handled .</sentence>
				<definiendum id="0">UNL document</definiendum>
			</definition>
			<definition id="2">
				<sentence>Introduction The UNL project of network-oriented multilinguat communication has proposed a standard for encoding the meaning of natural language utterances as semantic hypergraphs intended to be used as pivots in multilingual information and communication systems .</sentence>
				<definiendum id="0">UNL project</definiendum>
				<definiens id="0">encoding the meaning of natural language utterances as semantic hypergraphs intended to be used as pivots in multilingual information and communication systems</definiens>
			</definition>
			<definition id="3">
				<sentence>Then , we discuss the use of the UNL language as a linguistic or semantic pivot for highly multilingual information systems .</sentence>
				<definiendum id="0">UNL language</definiendum>
				<definiens id="0">a linguistic or semantic pivot for highly multilingual information systems</definiens>
			</definition>
			<definition id="4">
				<sentence>UNL is a project of multilingual personal networking communication initiated by the University of United Nations based in Tokyo .</sentence>
				<definiendum id="0">UNL</definiendum>
				<definiens id="0">a project of multilingual personal networking communication initiated by the University of United Nations based in Tokyo</definiens>
			</definition>
			<definition id="5">
				<sentence>768 of an utterance in the UNL interlingua ( UNL stands for `` Universal Networking Language '' ) is a hyl ) ergraph where normal nodes bear UWs CUniversal Words '' , or interlingual acceptions ) with semantic attributes , and arcs bear semantic relations ( deep cases , such as agt , obj , goal , etc. ) .</sentence>
				<definiendum id="0">UNL interlingua ( UNL</definiendum>
				<definiens id="0">a hyl ) ergraph where normal nodes bear UWs CUniversal Words '' , or interlingual acceptions ) with semantic attributes , and arcs bear semantic relations ( deep cases</definiens>
			</definition>
			<definition id="6">
				<sentence>A UW denotes a set of interlingual acceptions ( word senses ) , although we often loosely speak of `` the '' word sense demoted by a UW .</sentence>
				<definiendum id="0">UW</definiendum>
				<definiens id="0">a set of interlingual acceptions ( word senses</definiens>
			</definition>
			<definition id="7">
				<sentence>The syntax of a normal UW consists of 2 parts : a headword , a list of restrictions Because English is known by all UNL developers , tile headword is an English word or compound .</sentence>
				<definiendum id="0">tile headword</definiendum>
				<definiens id="0">an English word or compound</definiens>
			</definition>
			<definition id="8">
				<sentence>A UW denotes a collection of interlingual acceptions ( word senses ) , although we often loosely speak of `` the '' word sense denoted by an UW .</sentence>
				<definiendum id="0">UW</definiendum>
				<definiens id="0">a collection of interlingual acceptions ( word senses</definiens>
			</definition>
			<definition id="9">
				<sentence>In the UNL hmguagc , an 769 expression consists in a set of arcs , connecting the different nodes .</sentence>
				<definiendum id="0">UNL hmguagc</definiendum>
				<definiens id="0">consists in a set of arcs , connecting the different nodes</definiens>
			</definition>
			<definition id="10">
				<sentence>Deconversion is the process of transforming a UNL graph into one ( or possibly several ) utterance in a natural language .</sentence>
				<definiendum id="0">Deconversion</definiendum>
				<definiens id="0">the process of transforming a UNL graph into one ( or possibly several ) utterance in a natural language</definiens>
			</definition>
			<definition id="11">
				<sentence>An ARIANE tree is a general ( non binary ) tree with decorations on its nodes .</sentence>
				<definiendum id="0">ARIANE tree</definiendum>
			</definition>
			<definition id="12">
				<sentence>2.3 : example graph to tree convel : vion Let Z be the set of nodes of G , A the set of labels , T the created tree , and N is the set of nodes of T. Tile graph G= { ( a , b , l ) lac Y. , b6 Z , I~ A } is defined as a set of directed labelled arcs .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">example graph to tree convel : vion Let Z be the set of nodes of G , A the set of labels , T the created tree</definiens>
				<definiens id="1">the set of nodes of T. Tile graph</definiens>
			</definition>
			<definition id="13">
				<sentence>With this approach , a UNL utterance should be the encoding of the deep structure of a valid English utterance that reflects the meaning of the source utterance .</sentence>
				<definiendum id="0">UNL utterance</definiendum>
				<definiens id="0">the encoding of the deep structure of a valid English utterance that reflects the meaning of the source utterance</definiens>
			</definition>
			<definition id="14">
				<sentence>Center for Machine Translation , Carnegie Mellon University , Pittsburg , April 1989 , 286 p. \ [ 115\ ] Nyberg E. H. &amp; Mitamura T. ( 1992 ) The KANT system : Fast , Accurate , High-Quality Translation in Practical Domains .</sentence>
				<definiendum id="0">KANT system</definiendum>
				<definiens id="0">Fast , Accurate , High-Quality Translation in Practical Domains</definiens>
			</definition>
			<definition id="15">
				<sentence>\ [ 16\ ] Qu6zel-Ambrunaz M. ( 1990 ) Ariane-G5 v.3 Le moniteut : GETA , IMAG , juin 1990 , 206 p. \ [ 17\ ] Sloeum J. ( 1984 ) METAL : the LRC Machine Translation O , stem .</sentence>
				<definiendum id="0">METAL</definiendum>
				<definiens id="0">the LRC Machine Translation O , stem</definiens>
			</definition>
</paper>

	</volume>
