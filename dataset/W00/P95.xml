<?xml version="1.0" encoding="UTF-8"?>
	<volume id="P95">

		<paper id="1021">
			<definition id="0">
				<sentence>We define a new grammar formalism , called D-Tree Grammars ( DTG ) , which arises from work on TreeAdjoining Grammars ( TAG ) ( Joshi et al. , 1975 ) .</sentence>
				<definiendum id="0">D-Tree Grammars</definiendum>
				<definiendum id="1">DTG</definiendum>
			</definition>
			<definition id="1">
				<sentence>A salient feature of TAG is the extended domain of locality it provides .</sentence>
				<definiendum id="0">salient feature of TAG</definiendum>
			</definition>
			<definition id="2">
				<sentence>Following earlier work ( Becket et al. , 1991 ; Vijay-Shanker , 1992 ) , DTG provide a mechanism involving the use of domination links ( d-edges ) that ensure that parts of the subserted structure that are not substituted dominate those parts that are .</sentence>
				<definiendum id="0">DTG</definiendum>
				<definiens id="0">a mechanism involving the use of domination links ( d-edges ) that ensure that parts of the subserted structure that are not substituted dominate those parts that are</definiens>
			</definition>
			<definition id="3">
				<sentence>A d-tree is a tree with two types of edges : domination edges ( d-edges ) and immediate domination edges ( i-edges ) .</sentence>
				<definiendum id="0">d-tree</definiendum>
				<definiens id="0">a tree with two types of edges : domination edges ( d-edges ) and immediate domination edges ( i-edges )</definiens>
			</definition>
			<definition id="4">
				<sentence>A DTG is a four tuple G = ( VN , VT , S , D ) where VN and VT are the usual nonterminal and terminal alphabets , S E V~ is a distinguished nonterminal and D is a finite set of elementary d-trees .</sentence>
				<definiendum id="0">DTG</definiendum>
				<definiendum id="1">S E V~</definiendum>
				<definiendum id="2">D</definiendum>
				<definiens id="0">a four tuple G = ( VN , VT , S , D ) where VN and VT are the usual nonterminal and terminal alphabets</definiens>
				<definiens id="1">a distinguished nonterminal and</definiens>
			</definition>
			<definition id="5">
				<sentence>The elementary d-trees of a grammar G have two additionM annotations : subsertion-insertion constraints and sister-adjoining constraints• These will be described below , but first we define simultaneously DTG derivations and subsertion-adjoining trees ( SAtrees ) , which are partial derivation structures that can be interpreted as representing dependency information , the importance of which was stressed in the introduction 5 .</sentence>
				<definiendum id="0">SAtrees</definiendum>
			</definition>
			<definition id="6">
				<sentence>The tree set T ( G ) generated by G.is defined as the set of trees 7 such that : 7 ' E T/ ( G ) for some i 0 ; 7 ~ is rooted with the nonterminal S ; the frontier of 7 ' is a string in V~ ; and 7 results from the removal of all d-edges from 7 ' .</sentence>
				<definiendum id="0">tree set T ( G )</definiendum>
				<definiens id="0">the set of trees 7 such that : 7 ' E T/ ( G ) for some i 0 ; 7 ~ is rooted with the nonterminal S ; the frontier of 7 ' is a string in V~ ; and 7 results from the removal of all d-edges from 7 '</definiens>
			</definition>
			<definition id="7">
				<sentence>The string language L ( G ) associated with G is the set of terminal strings appearing on the frontier of trees in T ( G ) .</sentence>
				<definiendum id="0">string language L</definiendum>
			</definition>
			<definition id="8">
				<sentence>A SIC is a finite set of elementary node addresses ( ENAs ) .</sentence>
				<definiendum id="0">SIC</definiendum>
				<definiens id="0">a finite set of elementary node addresses</definiens>
			</definition>
			<definition id="9">
				<sentence>A SAC is a finite set of pairs , each pair identifying a direction ( left or right ) and an elementary d-tree .</sentence>
				<definiendum id="0">SAC</definiendum>
				<definiens id="0">a finite set of pairs , each pair identifying a direction ( left or right</definiens>
			</definition>
			<definition id="10">
				<sentence>The associated SA-tree is the desired , semantically motivated , dependency structure : the embedded clause depends on the matrix clause .</sentence>
				<definiendum id="0">SA-tree</definiendum>
				<definiens id="0">the desired , semantically motivated</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>We propose an O ( M ( n2 ) ) time algorithm for the recognition of Tree Adjoining Languages ( TALs ) , where n is the size of the input string and M ( k ) is the time needed to multiply two k x k boolean matrices .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">k )</definiendum>
				<definiens id="0">the time needed to multiply two k x k boolean matrices</definiens>
			</definition>
			<definition id="1">
				<sentence>The first polynomial time parsing algorithm for TALs was given by Vijayashanker and : loshi ( 1986 ) , which had a run time of O ( n6 ) , for an input of size n. Their algorithm had a flavor similar to the Cocke-Younger-Kasami ( CYK ) algorithm for context-free grammars .</sentence>
				<definiendum id="0">Cocke-Younger-Kasami ( CYK</definiendum>
			</definition>
			<definition id="2">
				<sentence>A Tree Adjoining Grammar ( TAG ) consists of a quintuple ( N , ~ U { ~ } , I , A , S ) , where N is a finite set of nonterminal symbols , is a finite set of terminal symbols disjoint from N , is the empty terminal string not in ~ , I is a finite set of labelled initial trees , A is a finite set of auxiliary trees , S E N is the distinguished start symbol The trees in I U A are called elementary trees .</sentence>
				<definiendum id="0">Tree Adjoining Grammar ( TAG )</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">S E N</definiendum>
				<definiens id="0">consists of a quintuple</definiens>
				<definiens id="1">a finite set of nonterminal symbols , is a finite set of terminal symbols disjoint from N , is the empty terminal string not in ~</definiens>
			</definition>
			<definition id="3">
				<sentence>O ( M ( n ) ) Time The CFG G = ( N , ~ , P , A1 ) , where N is a set of Nonterminals { A1 , A2 , .</sentence>
				<definiendum id="0">N</definiendum>
			</definition>
			<definition id="4">
				<sentence>, Ak } , is a finite set of terminals , P is a finite set of productions , A1 is the start symbol is assumed to be in the Chomsky Normal Form .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">A1</definiendum>
				<definiens id="0">a finite set of terminals</definiens>
				<definiens id="1">a finite set of productions</definiens>
				<definiens id="2">the start symbol is assumed to be in the Chomsky Normal Form</definiens>
			</definition>
			<definition id="5">
				<sentence>Given an input string aza2 ... . an E ~* , the recursive algorithm makes use of an ( n+l ) × ( n+l ) upper triangular matrix b defined by hi , i+1 = { Ak I ( Ak -- * a , ) E P } , bi , j = ¢ , for j i + 1 and proceeds to find the transitive closure b + of this matrix .</sentence>
				<definiendum id="0">E ~*</definiendum>
			</definition>
			<definition id="6">
				<sentence>withi &lt; s &lt; j. A k , can be thought of as a minimal node in this sense. ( The descendent relation is both reflexive and transitive ) Thus , given a string al ... a , of length n , ( say r = 2/3 ) , the following steps are done : 167 t Figure 2 : Adjunction Operation k t spanning trees which are within the first 2/3 . spanning trees which are within the last 2/3. plication ) on the nodes got as a result of Step 1 with nodes got as a result of Step 2. and find closure of this input. The point to note is that in step 3 , we can get rid of the mid 1/3 and focus on the remaining problem size. This approach does not work for TALs because of the presence of the adjunction operation. Firstly , the data structure used , i.e. the 2dimensional matrix with the given representation , is not sufficient as adjunction does not operate on contiguous strings. Suppose a node in a tree dominates a frontier which has the substring aiaj to the left of the foot node and akat to the right of the footnode. These substrings need not be a contiguous part of the input ; in fact , when this tree is used for adjunction then a string is inserted between these two suhstrings. Thus in order to represent a node , we need to use a matrix of higher dimension , namely dimension 4 , to characterize the substring that appears to the left of the footnode and the substring that appears to the right of the footnode. Secondly , the observation we made about an entry E b + is no longer quite true because of the presence of adjunction. Thirdly , the technique of getting rid of the mid 1/3 and focusing on the reduced problem size alone , does not work as shown in figure 3 : Suppose 3 ' is a derived tree in which 3 a node rn on which adjunction was done by an auxiliary tree ft. Even if we are able to identify the derived tree 71 rooted at m , we have to first identify fl before we can check for adjunction , fl need not be realised as a result of the composition operation involving the nodes from the first and last 2/3 's , ( say r =2/3 ) . Thus , if we discard the mid 1/3 , we will not be able to infer that the adjunction had indeed taken place at node m. Before we introduce the algorithm , we state the notations that will be used. We will be making use of a 4-dimensional matrix A of size ( n + 1 ) x ( n + 1 ) x ( n + 1 ) x ( n + 1 ) , where n is the size of the input string. ( Vijayashanker , Joshi , 1986 ) Given a TAG G and an input string aza2..an , n &gt; 1 , the entries in A will be nodes of the trees of G. We say , that a node m ( = &lt; 0 , node index , label &gt; ) E A ( i , j , k , l ) iff m is a node in a derived tree 7 and the subtree of 7 rooted at m has a yield given by either ai+l..</sentence>
				<definiendum id="0">descendent relation</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">both reflexive and transitive ) Thus , given a string al ... a , of length n</definiens>
			</definition>
			<definition id="7">
				<sentence>A Grown Auxiliary Tree is defined to be either a tree resulting from an adjunction involving two auxiliary trees or a tree resulting from an adjunction involving an auxiliary tree and a grown auxiliary tree .</sentence>
				<definiendum id="0">Grown Auxiliary Tree</definiendum>
				<definiens id="0">an adjunction involving two auxiliary trees or a tree resulting from an adjunction involving an auxiliary tree and a grown auxiliary tree</definiens>
			</definition>
			<definition id="8">
				<sentence>Procedure MAKELIST ( m ) Begin empty string at their frontiers ( i.e. m spans a subtree yielding e ) then ASSOC LIST ( ml ) = ASSOC LIST ( m ) u { m ) ASSOC LIST ( m2 ) = ASSOC LIST ( m ) U ( m } yielding the empty string at its frontier , then ASSOC LIST ( ml ) = ASSOC LIST ( m ) u { m ) End We initially fill A ( i , i+l , i+l , i+l ) with all nodes from Smt , Vml , where S , ~1 = { ml } O ASSOC LIST ( ml ) , ml being a node with the same label as the input hi+l , for 0 &lt; i &lt; n-1 .</sentence>
				<definiendum id="0">Procedure MAKELIST</definiendum>
				<definiens id="0">Begin empty string at their frontiers ( i.e. m spans a subtree yielding e ) then ASSOC LIST ( ml ) = ASSOC LIST ( m ) u { m ) ASSOC LIST ( m2 ) = ASSOC LIST ( m ) U ( m } yielding the empty string at its frontier</definiens>
				<definiens id="1">A ( i , i+l , i+l , i+l ) with all nodes from Smt</definiens>
			</definition>
			<definition id="9">
				<sentence>Thus , update A ( i , j , k , s ) with { m } U ASSOC LIST ( m ) .</sentence>
				<definiendum id="0">ASSOC LIST</definiendum>
				<definiens id="0">k , s ) with { m } U</definiens>
			</definition>
			<definition id="10">
				<sentence>, rp } , and 0 _ &lt; j &lt; k &lt; n. Ce ( qt , rs ) = 1 iff m E A ( q , r , s , t ) -0 otherwise Note that inC2 0 &lt; q &lt; r &lt; s &lt; t &lt; n. Clearly the dot product of the ii th row of C1 with the rs th column of Ce is a 1 iff m E A ( i , r , s , l ) . Thus , update A ( i , r , s , l ) with { m } U ASSOC LIST ( m ) . The input string ala2 ... an is in the language generated by the TAG G iff 3 a node labelled S in some A ( O , j , j , n ) , 0 &lt; _ j &lt; n. Steps la , lb and 4a can be computed in O ( neM ( p ) ) . Steps 5a and le can be computed in O ( ( ne/pe ) eM ( pg ) ) . If T ( p ) is the time taken by the procedure Compute Nodes , for an input of size p , then T ( p ) = 3T ( 2p/3 ) 4-O ( n2M ( p ) ) 4O ( ( ne /pe ) e M ( pe ) ) where n is the initial size of the input string. Solving the recurrence relation , we get T ( n ) O ( M ( ne ) ) . We will show the proof of correctness of the algorithm by induction on the length of the sequence of symbol positions. But first , we make an observation , given any two symbol positions ( r~ , rt ) , rt &gt; r~ 4-1 , and a node m spanning a tree ( i , j , k , l ) such that i &lt; rs and i _ &gt; rt with j and k in any of the possible combinations as shown in figure 4 .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">input string ala2 ... an is in the language generated by the TAG G iff 3 a node labelled S in some A ( O , j , j , n ) , 0 &lt; _ j &lt; n. Steps la , lb and 4a can be computed in O ( neM ( p ) )</definiens>
			</definition>
			<definition id="11">
				<sentence>Interestingly , the sparse version is an order of magnitude faster than the ordinary version for strings of length greater than 7 .</sentence>
				<definiendum id="0">sparse version</definiendum>
				<definiens id="0">an order of magnitude faster than the ordinary version for strings of length greater than 7</definiens>
			</definition>
			<definition id="12">
				<sentence>In this paper we have presented an O ( M ( n2 ) ) time algorithm for parsing TALs , n being the length of the input string .</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">n being the length of the input string</definiens>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>The model , which is based on Combinatory Categorial Grammars ( CCG ) ( Ades and Steedman , 1982 ; Steedman , 1985 ) , uses the morpheme as the building block of composition at all three linguistic domains .</sentence>
				<definiendum id="0">model</definiendum>
				<definiens id="0">uses the morpheme as the building block of composition at all three linguistic domains</definiens>
			</definition>
			<definition id="1">
				<sentence>Turkish is a language in which grammatical functions can be marked morphologically ( e.g. , case ) , or syntactically ( e.g. , indirect objects ) .</sentence>
				<definiendum id="0">Turkish</definiendum>
			</definition>
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>The best guess for a meaning of a word is the TLGG which overlaps with the highest percentage of sentence representations in which that word appears .</sentence>
				<definiendum id="0">best guess for a meaning of a word</definiendum>
				<definiens id="0">overlaps with the highest percentage of sentence representations in which that word appears</definiens>
			</definition>
			<definition id="1">
				<sentence>Our system , WOLFIE ( WOrd Learning From Interpreted Examples ) , learns this mapping from training examples consisting of sentences paired with their semantic representation .</sentence>
				<definiendum id="0">WOLFIE</definiendum>
				<definiens id="0">learns this mapping from training examples consisting of sentences paired with their semantic representation</definiens>
			</definition>
			<definition id="2">
				<sentence>Given : A set of sentences , S paired with representations , R. Find : A pairing of a subset of the words , W in S with representations of those words .</sentence>
				<definiendum id="0">R. Find</definiendum>
				<definiens id="0">A set of sentences , S paired with representations</definiens>
				<definiens id="1">A pairing of a subset of the words</definiens>
			</definition>
			<definition id="3">
				<sentence>A TLGG is a good candidate for a word meaning if it is part of the representation of a large percentage of sentences in which the word appears .</sentence>
				<definiendum id="0">TLGG</definiendum>
				<definiens id="0">a good candidate for a word meaning if it is part of the representation of a large percentage of sentences in which the word appears</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>`` WordNet : An On-line Lexical Database , '' in International Journal of Lexicography , 3 ( 4 ) .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">An On-line Lexical Database , '' in International</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>Centering structures and operations To explain how speakers move an entity in and out of the center of \ [ mutual\ ] attention , GJW89 formalizes attentional operations with two computational structures -the forward .</sentence>
				<definiendum id="0">GJW89</definiendum>
				<definiens id="0">formalizes attentional operations with two computational structures -the forward</definiens>
			</definition>
			<definition id="1">
				<sentence>looking center list ( Cf ) and the backward-looking center ( the Cb ) .</sentence>
				<definiendum id="0">Cf</definiendum>
				<definiendum id="1">backward-looking center</definiendum>
			</definition>
			<definition id="2">
				<sentence>Cf is a partially ordered list of centering candidates ; 2 the Cb , at the head of Cf , is the current center of attention .</sentence>
				<definiendum id="0">Cf</definiendum>
				<definiens id="0">a partially ordered list of centering candidates</definiens>
			</definition>
			<definition id="3">
				<sentence>Attentional salience measures the degree to which an item is salient , expressible as a partial ordering , e.g. , its ranking in Cf. It is a quantitative feature .</sentence>
				<definiendum id="0">Attentional salience</definiendum>
				<definiens id="0">measures the degree to which an item is salient , expressible as a partial ordering</definiens>
				<definiens id="1">a quantitative feature</definiens>
			</definition>
			<definition id="4">
				<sentence>Thus , • H* indicates instantiation of the pronominal 's cospecifier as the Cb , while L* fails to instantiate it as the Cb ; • The partially ordered set ( salient scale ) invoked by L+H is Cf ; • The inference path evoked by H+L is , for attentional purposes , a traversal of Cf. ( ~ ) And therefore , the attentionai effect of pitch accents can be formally expressed as an effect on the order of items in Cf. From these assumptions , I derive the following attentional consequences for pitch accented pronominals : • Only one pitch accent , L+H* , selects a Cb other than that predicted by centering theory and thereby reorders Cf. • L*+H appears to support an impending reordering but does not compel it .</sentence>
				<definiendum id="0">L+H</definiendum>
			</definition>
</paper>

		<paper id="1042">
</paper>

		<paper id="1002">
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>LDT was introduced for a system , where input is a logical formula , whose predicates approximately correspond to the content words of the input utterance in natural language ( lexical predicates ) .</sentence>
				<definiendum id="0">LDT</definiendum>
				<definiendum id="1">input</definiendum>
				<definiens id="0">a logical formula , whose predicates approximately correspond to the content words of the input utterance in natural language ( lexical predicates )</definiens>
			</definition>
			<definition id="1">
				<sentence>Output is a logical formula , consisting of predicates meaningful to the database engine ( database predicates ) .</sentence>
				<definiendum id="0">Output</definiendum>
			</definition>
			<definition id="2">
				<sentence>AET provides a formalism for describing how a formula consisting of lexical predicates can be tranlsated into formula consisting of database predicates .</sentence>
				<definiendum id="0">AET</definiendum>
				<definiens id="0">provides a formalism for describing how a formula consisting of lexical predicates can be tranlsated into formula consisting of database predicates</definiens>
			</definition>
			<definition id="3">
				<sentence>P ) -F ) where Pi , Ri denote atomic formulas , Q denotes a literal , F denotes a formula and V denotes universal closure .</sentence>
				<definiendum id="0">Pi , Ri denote atomic formulas</definiendum>
				<definiendum id="1">Q</definiendum>
				<definiendum id="2">V</definiendum>
				<definiens id="0">a literal , F denotes a formula and</definiens>
			</definition>
			<definition id="4">
				<sentence>Given an I~LDT T , for each pair consisting of the ground lexical atomic formula Fi , put and the ground database atomic formula Fo , ,tput from the dictionary of T , we find the set S of conditions ( A , C ) such that ( A , C , Fi , ,pu , , Fo , ,p , , ) is NCE ( T ) .</sentence>
				<definiendum id="0">I~LDT T</definiendum>
				<definiens id="0">consisting of the ground lexical atomic formula Fi , put and the ground database atomic formula Fo</definiens>
			</definition>
			<definition id="5">
				<sentence>Using the example from the previous section , the normalization algorithm when given the pairs ( student ( a ) , db_student ( a ) ) , ( unknown ( a , b ) , db_course ( a , b ) ) and ( take ( e , a , b ) , db_take ( e , a , b ) ) will produce the results { ( true , true ) } , { ( aeour , e ( b ) , true ) } and { ( acourse ( X ) , student ( a ) A unknown ( b , X ) } respectively .</sentence>
				<definiendum id="0">e</definiendum>
				<definiens id="0">produce the results { ( true , true ) }</definiens>
			</definition>
			<definition id="6">
				<sentence>Using the example above and the assignments student ( X ) X is a `` thing '' unknown ( X , S ) X is a `` thing '' take ( E , X , Y ) E is a `` thing '' , X and Y are `` attributes '' we can infer that student ( X ) can be combined with attribute take ( _ , X , _ ) but can not have an attribute take ( _ , _ , X ) .</sentence>
				<definiendum id="0">S ) X</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a `` thing ''</definiens>
			</definition>
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>In the Minimalist Program ( Chomsky , 1992 ) it is assumed that there are different types of projections ( lexical and functional ) and therefore different types of heads .</sentence>
				<definiendum id="0">Minimalist Program</definiendum>
				<definiens id="0">lexical and functional ) and therefore different types of heads</definiens>
			</definition>
			<definition id="1">
				<sentence>GT is a structure-building operation that builds trees in a bottom-up way as is illustrated in figure 2 .</sentence>
				<definiendum id="0">GT</definiendum>
			</definition>
			<definition id="2">
				<sentence>Move- ( ~ is a special kind of GT .</sentence>
				<definiendum id="0">Move- ( ~</definiendum>
				<definiens id="0">a special kind of GT</definiens>
			</definition>
			<definition id="3">
				<sentence>A is the head of B if there is a rule with B as left hand side ( LHS ) and A as the head daughter on the right hand side ( RHS ) .</sentence>
				<definiendum id="0">RHS</definiendum>
				<definiens id="0">the head daughter on the right hand side</definiens>
			</definition>
			<definition id="4">
				<sentence>Then the head ( AgrS ) that should be filled with an adjoined verb by movement from AgrO ( in a transitive sentence ) or V ( in an intransitive sentence ) is created before AgrO and V. To avoid moving constituents from a part of the tree that has not been built yet , the head-corner table for the minimalist head-corner parser is not constructed completely according to X-Theory ( see ( 1 ) ) .</sentence>
				<definiendum id="0">AgrS</definiendum>
				<definiendum id="1">V</definiendum>
				<definiens id="0">in an intransitive sentence</definiens>
			</definition>
			<definition id="5">
				<sentence>For example , instead of AgrO , VP is the headcorner of AgrO .</sentence>
				<definiendum id="0">VP</definiendum>
				<definiens id="0">the headcorner of AgrO</definiens>
			</definition>
			<definition id="6">
				<sentence>The Minimalist Program is a generation-oriented framework .</sentence>
				<definiendum id="0">Minimalist Program</definiendum>
				<definiens id="0">a generation-oriented framework</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>lationship ( i.e. , a morphology relationship between the two words ) , whenever it exists , should be an excellent predictor for semantic markedness ( Greenberg , 1966 ; Zwicky , 1978 ) .</sentence>
				<definiendum id="0">lationship</definiendum>
				<definiens id="0">a morphology relationship between the two words ) , whenever it exists</definiens>
			</definition>
			<definition id="1">
				<sentence>For these measurements , we used the MRC Psycholinguistic Database ( Coltheart , 1981 ) which contains a variety of measures for 150,837 entries counting different parts of speech or inflected forms as different words ( 115,331 distinct words ) .</sentence>
				<definiendum id="0">MRC Psycholinguistic Database</definiendum>
				<definiens id="0">contains a variety of measures for 150,837 entries counting different parts of speech or inflected forms as different words</definiens>
			</definition>
			<definition id="2">
				<sentence>A positive ( negative ) value indicates that the first ( second ) adjective is the unmarked one , except for two variables ( word length and number of syllables ) where the opposite is true .</sentence>
				<definiendum id="0">positive</definiendum>
				<definiens id="0">word length and number of syllables ) where the opposite is true</definiens>
			</definition>
			<definition id="3">
				<sentence>This is all the more remarkable in the case of the morphologically related adjectives , where frequency outperforms length of the words ; recall that the latter directly encodes the formal markedness relationship , so frequency is able to correctly classify some of the cases where formal and semantic markedness values disagree .</sentence>
				<definiendum id="0">frequency</definiendum>
				<definiendum id="1">recall</definiendum>
				<definiens id="0">outperforms length of the words ;</definiens>
			</definition>
			<definition id="4">
				<sentence>Decision trees ( Quinlan , 1986 ) is the first statistical supervised learning paradigm that we explored .</sentence>
				<definiendum id="0">Decision trees</definiendum>
				<definiens id="0">the first statistical supervised learning paradigm that we explored</definiens>
			</definition>
			<definition id="5">
				<sentence>Log-linear regression ( Santner and Duffy , 1989 ) is the second general supervised learning method that we explored .</sentence>
				<definiendum id="0">Log-linear regression</definiendum>
				<definiens id="0">the second general supervised learning method that we explored</definiens>
			</definition>
			<definition id="6">
				<sentence>201 12 '' 10 i ® ¢3 40 % 50 % 60 % 70 % 80 % 90 % Accuracy Figure 1 : Probability densities for the accuracy of the frequency method ( dotted line ) and the smoothed log-linear model ( solid line ) on the morphologically unrelated adjectives .</sentence>
				<definiendum id="0">Probability</definiendum>
				<definiens id="0">densities for the accuracy of the frequency method ( dotted line ) and the smoothed log-linear model ( solid line ) on the morphologically unrelated adjectives</definiens>
			</definition>
			<definition id="7">
				<sentence>The test statistic is the number of runs where the score of one predictor is higher than the other 's ; as is common in statistical practice , ties are broken by assigning half of them to each category .</sentence>
				<definiendum id="0">test statistic</definiendum>
				<definiens id="0">the number of runs where the score of one predictor is higher than the other 's</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>When an identical phrase ( a set of consecutive words ) is repeated in different sentences , the constituent words of those sentences tend to be associated in identical modification patterns with identical parts of speech and identical modifieemodifier relationships .</sentence>
				<definiendum id="0">identical phrase</definiendum>
				<definiens id="0">a set of consecutive words</definiens>
			</definition>
			<definition id="1">
				<sentence>Gale ( Gale et al. , 1992 ) and Nasukawa ( Nasukawa , 1993 ) reported that polysemous words within the same discourse have the same word sense with a high probability ( 98 % according to ( Gale et al. , 1992 ) , ) and the results of our analysis indicate that most content words are frequently repeated in the discourse , as is shown in Table 1 ; moreover , collocation ( modifier-modifiee relationship ) patterns are also repeated frequently in the same discourse , as is shown in Figure 1 .</sentence>
				<definiendum id="0">Gale</definiendum>
				<definiens id="0">modifier-modifiee relationship ) patterns are also repeated frequently in the same discourse</definiens>
			</definition>
			<definition id="2">
				<sentence>An identical collocation pattern is one in which both modifiee and modifier sides consist of words that are morphologically identical with those in the sentence being analyzed , and that stand in an identical relationship .</sentence>
				<definiendum id="0">identical collocation pattern</definiendum>
				<definiens id="0">one in which both modifiee and modifier sides consist of words that are morphologically identical with those in the sentence being analyzed , and that stand in an identical relationship</definiens>
			</definition>
			<definition id="3">
				<sentence>of morph , identical words Proportion of all content words of Two or more Five or more Total number of Proportion speech times ( % ) times ( % ) appearances ( words ) ( % ) Noun 90.7 76.2 99047 59.8 Verb 94.9 83.6 35622 21.5 Adjective 88.9 71.0 16941 10.2 Adverb 68.8 4993 3.0 Pronoun 85.9 98.0 94.8 8911 5.4 Total \ [ 91.6 78.0 165514 I -Rate of repetition ( % ) 100.00 -80.00 -60.0040.00 20.00J 0 200 400 600 Size of discourse 800 ( Number of sentences ) Figure 1 : Rate of finding identical or similar collocation patterns in relation to the size of the discourse shown in Figure 3 .</sentence>
				<definiendum id="0">Proportion speech times</definiendum>
				<definiens id="0">identical words Proportion of all content words of Two or more Five or more Total number of</definiens>
			</definition>
			<definition id="4">
				<sentence>( ( DECL ( SUBCL ( NP ( VERB* ( CONJ `` as '' ) ( NP ( PRON* `` you '' ( `` you '' ( SG PL ) ) ) ) ( AUXP ( VERB* `` can '' ( `` can '' PS ) ) ) ( VERB* `` see '' ( `` see '' PS ) ) ( PUNC , , , , , ) ) ( DETP ( ADJ* `` the '' ( `` the '' BS ) ) ) ( NP ( NOUN* `` help '' ( `` help '' SG ) ) ) ( NOUN* `` display '' ( `` display '' SG ) ) ) `` provides '' ( `` provide '' PS ) ) Figure 3 : Thirty-ninth sentence of Chapter 6 and a part of its parse the parse data .</sentence>
				<definiendum id="0">NP</definiendum>
			</definition>
			<definition id="5">
				<sentence>The completion procedure consists of two steps : Step 1 : Inspecting each partial parse and restructuring it on the basis of the discourse information For each word in a partial parse , the part of speech and the rood , flee-modifier relationships with other words are inspected .</sentence>
				<definiendum id="0">completion procedure</definiendum>
				<definiens id="0">consists of two steps : Step 1 : Inspecting each partial parse and restructuring it on the basis of the discourse information For each word in a partial parse , the part of speech and the rood , flee-modifier relationships with other words are inspected</definiens>
			</definition>
			<definition id="6">
				<sentence>43 Table 3 : Discourse information on modifiees and modifiers of a noun `` cursor '' Modifiers POS Relation Word ( CFRAMEs preference value ) Noun of display ( CFRAME106873 0.1 ) in protected area ( CFRAME106872 1 ) to left ( CFRAME106407 0.1 ) right ( CFRAME106338 0.1 ) DIRECT position ( CFRAME106405 1 ) Adjective up line ( CFRAME106295 0.1 ) DIRECT your ( CFRAMEI06690 CFRAMEI06550 2 ) POS Relation Verb with up SUBJ OBJ RECIPIENT Modifiees Word ( CFRAMEs preference value ) play ( CFRAME106928 0.1 ) be ( CFRAMEI06927 0.1 ) move ( CFRAME106688 1 ) stop ( CFRAME106572 1 ) reach ( CFRAME106346 1 ) move ( CFRAME106248 1 ) move ( CFRAME106402 CFKAME106335 CFRAME106292 3 ) confuse ( CFRAME106548 1 ) move ( CFRAME106304 1 ) isometric view ( n ) I ~ '' ~f. ' : ~ , ~ magazine ( n ) l taken Ivll ~ : : ~o. : ~o ' : q operator ( n ) \ ] .</sentence>
				<definiendum id="0">SUBJ OBJ RECIPIENT Modifiees Word</definiendum>
				<definiens id="0">Discourse information on modifiees</definiens>
			</definition>
			<definition id="7">
				<sentence>Furthermore , in terms of the turnaround time ( TAT ) of the whole translation procedure , the improvement in the parses achieved by using this method along with other disambiguation methods involving discourse information , as shown in another paper ( Nasukawa , 1995 ) , shortened the TAT in the late stages of the translation procedure , 45 Table 4 : Results of completing incomplete parses on the basis of discourse information Text i Text 2 Number of sentences in discourse 175 354 Incomplete parses 32 31 Unified into a single parse 18 ( 56.3 % ) 17 ( 54.8 % ) Improvement in translation Better Even 10 7 Worse 1 3 Partially joined or restructured ' '' Improvement Better in Even translation Worse 12 ( 37.5 % ) 8 ( 25.8 % ) 4 2 7 3 1 3 Not changed 2 ( 6.3 % ) 6 ( 19.4 % ) and compensated for the extra TAT required as a result of using the discourse information , provided the size of the discourse was kept to between 100 and 300 sentences .</sentence>
				<definiendum id="0">TAT</definiendum>
				<definiens id="0">a result of using the discourse information , provided the size of the discourse was kept to between 100 and 300 sentences</definiens>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>A decision tree is a decision-making device which assigns a probability to each of the possible choices based on the context of the decision : P ( flh ) , where f is an element of the future vocabulary ( the set of choices ) and h is a history ( the context of the decision ) .</sentence>
				<definiendum id="0">decision tree</definiendum>
				<definiendum id="1">f</definiendum>
				<definiendum id="2">h</definiendum>
				<definiens id="0">a decision-making device which assigns a probability to each of the possible choices based on the context of the decision : P ( flh )</definiens>
				<definiens id="1">an element of the future vocabulary ( the set of choices</definiens>
			</definition>
			<definition id="1">
				<sentence>By this definition , an n-gram model has IWI '' parameters , where IWI is the number of unique tokens generated by the process .</sentence>
				<definiendum id="0">IWI</definiendum>
				<definiens id="0">the number of unique tokens generated by the process</definiens>
			</definition>
			<definition id="2">
				<sentence>However , here let 's define an n-gram model more loosely as a model which defines a probability distribution on a random variable given the values of n1 random variables , P ( flhlh2 ... hn-1 ) .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">defines a probability distribution on a random variable given the values of n1 random variables</definiens>
			</definition>
			<definition id="3">
				<sentence>For instance , the part-of-speech tagging model P ( tilwiti_lti_2 ) can be interpreted as a 4gram model , where HI is the variable denoting the word being tagged , Ha is the variable denoting the tag of the previous word , and Ha is the variable denoting the tag of the word two words back .</sentence>
				<definiendum id="0">HI</definiendum>
				<definiendum id="1">Ha</definiendum>
				<definiendum id="2">Ha</definiendum>
				<definiens id="0">the variable denoting the word being tagged</definiens>
				<definiens id="1">the variable denoting the tag of the previous word , and</definiens>
				<definiens id="2">the variable denoting the tag of the word two words back</definiens>
			</definition>
			<definition id="4">
				<sentence>A leaf node in a decision tree can be represented by the sequence of question answers , or history values , which leads the decision tree to that leaf .</sentence>
				<definiendum id="0">leaf node</definiendum>
				<definiens id="0">answers , or history values</definiens>
			</definition>
			<definition id="5">
				<sentence>278 tree can be defined as an interpolated n-gram model where the At function is defined as : 1 if hk~hk2 .</sentence>
				<definiendum id="0">function</definiendum>
				<definiens id="0">an interpolated n-gram model where the At</definiens>
			</definition>
			<definition id="6">
				<sentence>The extension can take on any of the following five values : right the node is the first child of a constituent ; left the node is the last child of a constituent ; up the node is neither the first nor the last child of a constituent ; unary the node is a child of a unary constituent ; 279 root the node is the root of the tree .</sentence>
				<definiendum id="0">node</definiendum>
				<definiens id="0">a child of a unary constituent ; 279 root the node is the root of the tree</definiens>
			</definition>
			<definition id="7">
				<sentence>SPATTER consists of three main decision-tree models : a part-of-speech tagging model , a nodeextension model , and a node-labeling model .</sentence>
				<definiendum id="0">SPATTER</definiendum>
				<definiens id="0">consists of three main decision-tree models : a part-of-speech tagging model , a nodeextension model</definiens>
			</definition>
			<definition id="8">
				<sentence>Each of these decision-tree models are grown using the following questions , where X is one of word , tag , label , or extension , and Y is either left and right : • What is the X at the current node ?</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiens id="0">one of word , tag , label , or extension</definiens>
				<definiens id="1">either left and right</definiens>
			</definition>
			<definition id="9">
				<sentence>Because of the size of the search space , ( roughly O ( ITI '' INJ '' ) , where \ [ TJ is the number of part-ofspeech tags , n is the number of words in the sentence , and \ [ NJ is the number of non-terminal labels ) , it is not possible to compute the probability of every parse .</sentence>
				<definiendum id="0">TJ</definiendum>
				<definiens id="0">the number of part-ofspeech tags , n is the number of words in the sentence , and \ [ NJ is the number of non-terminal labels</definiens>
			</definition>
			<definition id="10">
				<sentence>The first experiment uses the IBM Computer Manuals domain , which consists of sentences extracted from IBM computer manuals .</sentence>
				<definiendum id="0">IBM Computer Manuals domain</definiendum>
				<definiens id="0">consists of sentences extracted from IBM computer manuals</definiens>
			</definition>
			<definition id="11">
				<sentence>The Lancaster treebank uses 195 part-ofspeech tags and 19 non-terminal labels .</sentence>
				<definiendum id="0">Lancaster treebank</definiendum>
				<definiens id="0">uses 195 part-ofspeech tags and 19 non-terminal labels</definiens>
			</definition>
			<definition id="12">
				<sentence>4 For this test set , SPATTER takes on average 12 2This treebank also contains coreference information , predicate-argument relations , and trace information indicating movement ; however , none of this additional information was used in these parsing experiments .</sentence>
				<definiendum id="0">SPATTER</definiendum>
				<definiens id="0">predicate-argument relations , and trace information indicating movement</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>Non-Linearity A Semitic stem consists of a root and a vowel melody , arranged according to a canonical pattern .</sentence>
				<definiendum id="0">Semitic stem</definiendum>
				<definiens id="0">consists of a root and a vowel melody , arranged according to a canonical pattern</definiens>
			</definition>
			<definition id="1">
				<sentence>• Phonetic Syncopation A consonantal segment may be omitted from the phonetic surface form , but maintained in the orthographic surface from .</sentence>
				<definiendum id="0">consonantal segment</definiendum>
				<definiens id="0">the phonetic surface form , but maintained in the orthographic surface from</definiens>
			</definition>
			<definition id="2">
				<sentence>Multi-tape two-level morphology : a case study in Semitic non-linear morphology .</sentence>
				<definiendum id="0">Multi-tape two-level morphology</definiendum>
				<definiens id="0">a case study in Semitic non-linear morphology</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>~\ ] , ... , \ [ PHON\ [ ~ ) \ [ dom-obj \ ] A ~ : I s , ,N~E~ , ~I~ LPHONI , Io ... o r- ; -I To express this more formally , let us now define an auxiliary relation , joinF , which holds of two lists L1 and L2 only if L2 is the concatenation of values for the feature F of the elements in L1 in the same order : 4 ( 3 ) joinF ( \ [ Y\ ] , \ [ ~ ) - ( V\ ] : 0 A \ [ 7\ ] : O ) V ( cons ( IF ( El ) \ ] , \ [ -~-\ ] , \ [ ~ A joinF ( \ [ ?</sentence>
				<definiendum id="0">auxiliary relation , joinF</definiendum>
				<definiens id="0">V\ ] : 0 A \ [ 7\ ] : O</definiens>
			</definition>
			<definition id="1">
				<sentence>In order to allow for the possibility of partially compacting a domain by replacing the compaction relation of ( 4 ) by the p-compaction relation , which is defined as follows : 177 I VP , IZ\ ] /REL-~ L EXTRA -4\ [ \ ] /r , e , ne. , \ ] \ ] r ' er n , erha '' \ ] DET \ [ ~oM ( \ [ ( # , ,n~ ) \ ] ) \ ] ^ p-compaction ( l-i-l , \ [ Z\ ] , lID ) ^ shume ( I\ [ Zl ) , ( ~ , l'q , ~ I REL-S EXTRA + v , .</sentence>
				<definiendum id="0">p-compaction relation</definiendum>
				<definiendum id="1">lID ) ^ shume ( I\ [ Zl ) , ( ~</definiendum>
				<definiens id="0">follows : 177 I VP , IZ\ ] /REL-~ L EXTRA -4\ [ \ ] /r , e , ne. , \ ] \ ] r ' er n , erha '' \ ] DET \ [ ~oM ( \ [</definiens>
			</definition>
			<definition id="2">
				<sentence>J J ^ shume ( m , \ [ \ ] , ~ A joineHoN ( ~J , \ [ L\ ] ) Intuitively , the p-compaction relation holds of a sign S ( ~\ ] ) , domain object O ( \ [ ~ , and a list of domain objects L ( ~\ ] ) only if O is t~-e compaction of S with L being a llst of domain objects `` liberated '' from the S 's order domain .</sentence>
				<definiendum id="0">joineHoN</definiendum>
				<definiens id="0">t~-e compaction of S with L being a llst of domain objects `` liberated '' from the S 's order domain</definiens>
			</definition>
			<definition id="3">
				<sentence>\ ] DOM\ [ 5-'\ ] i \ [ \ ] \ [ ( Neienen Hund der ftunger hai ) \ ] , \ [ ( vf£Ltteru ) \ ] \ ] \ [ ( einen ) \ [ \ ] DoM T , A p-compaction ( I-F\ ] , \ [ \ ] , 0 ) ^ shutne ( ( \ [ ~,0 , \ [ \ ] , El ) Figure 5 : Total compaction as a special case of compaction that determiners are to N domains .</sentence>
				<definiendum id="0">p-compaction</definiendum>
				<definiens id="0">Total compaction as a special case of compaction that determiners</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>Definition 1 A nonmonotonic sort is a pair ( s , A ) where s E S and A is a set of nonmonotonic rules of the form ( w , a : fl ==~ 3 ' ) where w is an atom and a , fl and 3 ' E S. It is assumed that for each nonmonotonic rule 3 ' _C fl , a -- , s , fl , ~ s , and 713s C s. As seen by the definition a nonmonotonic sort is considered to be a pair of monotonic information from the subsumption order and nonmonotonic information represented as a set of nonmonotonic rules , The user can assign nonmonotonic information to a nonmonotonic sort by calling a nonmonotonic definition as defined in the previous section .</sentence>
				<definiendum id="0">nonmonotonic sort</definiendum>
				<definiens id="0">a pair ( s , A ) where s E S and A is a set of nonmonotonic rules of the form</definiens>
			</definition>
			<definition id="1">
				<sentence>In our original definition we said that the nonmonotonic rule above should be applied if Y , ~ -- X. This can be generalized to the case where -- X is a nonmonotonic rule if we extend the definition of -~ to also mean that the application ( or explanation ) of the not rule at this node does not yield failure .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a nonmonotonic rule if we extend the definition of -~ to also mean that the application</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>The estimator consists of a simple three-layer feed forward MLP trained with the back-propagation algorithm ( see Figure 2 ) .</sentence>
				<definiendum id="0">estimator</definiendum>
				<definiens id="0">consists of a simple three-layer feed forward MLP trained with the back-propagation algorithm ( see Figure 2 )</definiens>
			</definition>
			<definition id="1">
				<sentence>The MLP is trained on phonetically hand-labeled speech ( TIMIT ) , and then further trained by an iterative Viterbi procedure ( forced-Viterbi providing the labels ) with Wall Street Journal corpora .</sentence>
				<definiendum id="0">MLP</definiendum>
				<definiens id="0">trained on phonetically hand-labeled speech ( TIMIT ) , and then further trained by an iterative Viterbi procedure ( forced-Viterbi providing the labels ) with Wall Street Journal corpora</definiens>
			</definition>
			<definition id="2">
				<sentence>• P ( d\ ] p ) be the probability of the derivation d of pronunciation p. • PRON be the set of pronunciations derived from the forced-Viterbi output .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the probability of the derivation d of pronunciation p. • PRON be the set of pronunciations derived from the forced-Viterbi output</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>Grammars A Wansduction grammar is a bilingual model that generates two output streams , one for each language .</sentence>
				<definiendum id="0">Wansduction grammar</definiendum>
				<definiens id="0">a bilingual model that generates two output streams</definiens>
			</definition>
			<definition id="1">
				<sentence>The bracket alignment includes a word alignment as a byproduct .</sentence>
				<definiendum id="0">bracket alignment</definiendum>
				<definiens id="0">includes a word alignment as a byproduct</definiens>
			</definition>
			<definition id="2">
				<sentence>All experiments reported in this paper were performed on sentence-pairs from the HKUST English-Chinese Parallel Bilingual Corpus , which consists of governmental transcripts ( Wu 1994 ) .</sentence>
				<definiendum id="0">HKUST English-Chinese Parallel Bilingual Corpus</definiendum>
			</definition>
			<definition id="3">
				<sentence>The model nonetheless retains a high degree of compatibility with more conventional monolingual formalisms and methods .</sentence>
				<definiendum id="0">model nonetheless</definiendum>
				<definiens id="0">retains a high degree of compatibility with more conventional monolingual formalisms and methods</definiens>
			</definition>
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>The English corpus consists of 320 Table 1 : When the word orders of the English and the German matrix correspond , the dot patterns of the two matrices are identical .</sentence>
				<definiendum id="0">English corpus</definiendum>
				<definiens id="0">When the word orders of the English and the German matrix correspond , the dot patterns of the two matrices are identical</definiens>
			</definition>
			<definition id="1">
				<sentence>f ( j ) Hereby f ( i &amp; j ) is the frequency of common occurrence of the two words i and j , and f ( i ) is the corpus frequency of word i. However , for comparison , the simulations described below were also conducted using the original co-occurrence matrices ( formula 2 ) and a measure similar to mutual information ( formula 3 ) .</sentence>
				<definiendum id="0">f ( i )</definiendum>
			</definition>
</paper>

		<paper id="1043">
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>Aggregation is any syntactic process that allows the expression of concise and tightly constructed text such as coordination or subordination .</sentence>
				<definiendum id="0">Aggregation</definiendum>
				<definiens id="0">any syntactic process that allows the expression of concise and tightly constructed text such as coordination or subordination</definiens>
			</definition>
			<definition id="1">
				<sentence>PLANDoc generates natural language reports based on the interaction between telephone planning engineers and LEIS-PLAN 1 , a knowledge based system .</sentence>
				<definiendum id="0">PLANDoc</definiendum>
				<definiens id="0">generates natural language reports based on the interaction between telephone planning engineers and LEIS-PLAN 1 , a knowledge based system</definiens>
			</definition>
			<definition id="2">
				<sentence>Each FD is an atomic decision about telephone equipment installation chosen by a planning engineer .</sentence>
				<definiendum id="0">FD</definiendum>
				<definiens id="0">an atomic decision about telephone equipment installation chosen by a planning engineer</definiens>
			</definition>
			<definition id="3">
				<sentence>The content planner organizes the overall narrative and determines the linear order of the messages .</sentence>
				<definiendum id="0">content planner</definiendum>
			</definition>
			<definition id="4">
				<sentence>Finally the FUF/SURGE package ( Elhadad91 ; Robin94 ) lexicalizes the messages and maps case roles into syntactic roles , builds the constituent structure of the sentence , ensures agreement , and generates the surface sentences .</sentence>
				<definiendum id="0">FUF/SURGE package</definiendum>
				<definiens id="0">lexicalizes the messages and maps case roles into syntactic roles , builds the constituent structure of the sentence , ensures agreement , and generates the surface sentences</definiens>
			</definition>
			<definition id="5">
				<sentence>For each potential distinct attribute , the system calculates its rank using the formula m d , where m is the number of messages and d is the number of distinct attributes for that particular attribute .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">the number of messages</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>Caswey et al. ( Cawsey et al. , 1993 ; Logan et al. , 1994 ) introduced the idea of utilizing a belief revision mechanism ( Galliers , 1992 ) to predict whether a set of evidence is sufficient to change a user 's existing belief and to generate responses for information retrieval dialogues in a library domain .</sentence>
				<definiendum id="0">belief revision mechanism</definiendum>
				<definiens id="0">Cawsey et al. , 1993 ; Logan et al. , 1994 ) introduced the idea of utilizing a</definiens>
			</definition>
			<definition id="1">
				<sentence>The enhanced dialogue model has four levels : the domain level which consists of the domain plan being constructed for the user 's later execution , the problem-solving level which contains the actions being performed to construct the don~n plan , the belief level which consists of the mutual beliefs pursued during the planning process in order to further the problem-solving intentions , and the discourse level which contains the communicative actions initiated to achieve the mutual beliefs ( Chu-Carroll and Carberry , 1994 ) .</sentence>
				<definiendum id="0">domain level</definiendum>
				<definiendum id="1">problem-solving level</definiendum>
				<definiendum id="2">belief level</definiendum>
				<definiens id="0">contains the communicative actions initiated to achieve the mutual beliefs ( Chu-Carroll and Carberry</definiens>
			</definition>
			<definition id="2">
				<sentence>The belief level of the dialogue model consists of mutual beliefs proposed by the agents ' discourse actions .</sentence>
				<definiendum id="0">belief level of the dialogue model</definiendum>
				<definiens id="0">consists of mutual beliefs proposed by the agents ' discourse actions</definiens>
			</definition>
			<definition id="3">
				<sentence>The system will first gather its evidence pe~aining to the belief , which includes I ) a warranted belief ~ that Dr. Smith has postponed his sabbatical until 1997 ( PostponedSabbatical ( Smith , J997 ) ) , 2 ) a warranted belief that Dr. Smith postponing his sabbatical until 1997 supports the belief that he is not going on sabbatical next year ( supports ( Postponed-Sabbatical ( Smith,1997 ) , -~On-SabbaticaI ( Smith , next year ) ) , 3 ) a strong belief that Dr. Smith will not be a visitor at IBM next year ( -~visitor ( Smith , IBM , next year ) ) , and 4 ) a warranted belief that Dr. Smith not being a visitor at IBM next aThe strength of a belief is classified as : warranted , strong , or weak , based on the endorsement of the belief .</sentence>
				<definiendum id="0">PostponedSabbatical</definiendum>
				<definiens id="0">a visitor at IBM next year ( -~visitor ( Smith , IBM , next year ) ) , and 4 ) a warranted belief that Dr. Smith not being a visitor at IBM next aThe strength of a belief is classified as : warranted , strong , or weak , based on the endorsement of the belief</definiens>
			</definition>
			<definition id="4">
				<sentence>Focus-Modlflcatlon ( _bel ) : pertaining to _bel _bel.s-attack 4system 's own evidence against _bel then _bel .</sentence>
				<definiendum id="0">Focus-Modlflcatlon</definiendum>
				<definiens id="0">pertaining to _bel _bel.s-attack 4system 's own evidence against _bel then _bel</definiens>
			</definition>
			<definition id="5">
				<sentence>sWalker ( 1994 ) has shown the importance of IRU 's Odormationally Redundant Utterances ) in efficient discourse .</sentence>
				<definiendum id="0">sWalker ( 1994 )</definiendum>
				<definiens id="0">has shown the importance of IRU 's Odormationally Redundant Utterances ) in efficient discourse</definiens>
			</definition>
			<definition id="6">
				<sentence>When SelectFocus-Modification is applied to -- , Teaches ( Smith , Al ) , the algorithm will first be recursively invoked on OnSabbatical ( Smith , next year ) to determine the focus for modifying the child belief ( step 3.1 in Figure 3 ) .</sentence>
				<definiendum id="0">SelectFocus-Modification</definiendum>
			</definition>
			<definition id="7">
				<sentence>Since the system has two pieces of evidence against OnSabbatical ( Smith , next year ) , 1 ) a warranted piece of evidence containing Postponed-Sabbatical ( Smittg1997 ) and supports ( Postponed-Sabbatical ( Smith,1997 ) , - , OnSabbatical ( Smith , next year ) ) , and 2 ) a strong piece of evidence containing -- , visitor ( Smith , IBM , next year ) and supports ( - , visitor ( Smith , IBM , next year ) , - , OnSabbatical ( Smith , next year ) ) , the evidence is predicted to be sufficient to change the user 's belief in On-Sabbatical ( Smith , next year ) , and hence - , Teaches ( Smith , A1 ) ; thus , the focus of modification will be On-Sabbatical ( Smith , next year ) .</sentence>
				<definiendum id="0">, OnSabbatical</definiendum>
				<definiens id="0">next year ) ) , and 2 ) a strong piece of evidence containing -- , visitor ( Smith , IBM , next year ) and supports ( - , visitor ( Smith , IBM , next year ) , -</definiens>
			</definition>
			<definition id="8">
				<sentence>Node ( Figure 2 ) , MB ( S , U , -~ On-Sabbatical ( Smith , next year ) ) will be posted as a mutual belief to be achieved .</sentence>
				<definiendum id="0">Node</definiendum>
				<definiendum id="1">MB ( S , U , -~ On-Sabbatical</definiendum>
				<definiens id="0">a mutual belief to be achieved</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>For example , Combinatory Categorial Grammar ( CCG ) ( Steedman , 1990 ) is a theory of syntax and semantic interpretation that has the attractive characteristic of handling many coordination constructs that other theories can not .</sentence>
				<definiendum id="0">Combinatory Categorial Grammar ( CCG )</definiendum>
				<definiens id="0">a theory of syntax and semantic interpretation that has the attractive characteristic of handling many coordination constructs that other theories can not</definiens>
			</definition>
			<definition id="1">
				<sentence>( 15 ) ( and ( run John ) ( run Bill ) ) CCG is one of several theories in which ( lb ) gets derived by raising John to be the LF AP .</sentence>
				<definiendum id="0">CCG</definiendum>
				<definiens id="0">one of several theories in which ( lb ) gets derived by raising John to be the LF AP</definiens>
			</definition>
			<definition id="2">
				<sentence>( P john ) , where P is a predicate that takes a NP as an argument to return a sentence .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a predicate that takes a NP as an argument to return a sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , the logical form in ( 3 ) would be produced , where X\rtm ( X ) is the representation of Az .</sentence>
				<definiendum id="0">X\rtm</definiendum>
				<definiendum id="1">X )</definiendum>
				<definiens id="0">the representation of Az</definiens>
			</definition>
			<definition id="4">
				<sentence>abe represents object-level abstraction Az .</sentence>
				<definiendum id="0">abe</definiendum>
			</definition>
			<definition id="5">
				<sentence>Figure 3 : Declarations for AProlog representation of CCG logical forms where N is a meta-level function of type ta -- -* tat .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">Declarations for AProlog representation of CCG logical forms where</definiens>
				<definiens id="1">a meta-level function of type ta -- -* tat</definiens>
			</definition>
			<definition id="6">
				<sentence>Space prohibits a full explanation , but essentially the fact that AProlog is a typed language leads to a good deal of formal clutter if this method is used .</sentence>
				<definiendum id="0">Space</definiendum>
				<definiens id="0">prohibits a full explanation , but essentially the fact that AProlog is a typed language leads to a good</definiens>
			</definition>
			<definition id="7">
				<sentence>apply ( abe sub\ ( walked ' sub ) ) harry ' N. It unifies with the ta -~ ta function sub\ ( walked ~ sub ) , S with harry ' and M with ( It S ) , the recta-level application of R to S , which by the built-in fi-reduction is ( walked ' harry ' ) .</sentence>
				<definiendum id="0">fi-reduction</definiendum>
				<definiens id="0">the recta-level application of R to S , which by the built-in</definiens>
			</definition>
			<definition id="8">
				<sentence>cooed ( be i B ) ( abe R ) ( abe S ) ( abe T ) `` pi x\ ( coord B ( R x ) ( S x ) ( T x ) ) .</sentence>
				<definiendum id="0">cooed</definiendum>
				<definiens id="0">i B ) ( abe R ) ( abe S ) ( abe T ) `` pi x\ ( coord B ( R x ) ( S x ) ( T x</definiens>
			</definition>
			<definition id="9">
				<sentence>They will both have the category ( fs ( bs np s ) s ) .</sentence>
				<definiendum id="0">fs</definiendum>
				<definiens id="0">bs np s ) s )</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>85 A HPSG grammar consists of two components : the declaration of the structure of the domain of linguistic objects in a signature ( consisting of the type hierarchy and the appropriateness conditions ) and the formulation of constraints on that domain .</sentence>
				<definiendum id="0">HPSG grammar</definiendum>
				<definiens id="0">consists of two components : the declaration of the structure of the domain of linguistic objects in a signature ( consisting of the type hierarchy and the appropriateness conditions ) and the formulation of constraints on that domain</definiens>
			</definition>
			<definition id="1">
				<sentence>constraining the domain A HPSGII theory consists of a set of descriptions which are interpreted as being true or false of an object in the domain .</sentence>
				<definiendum id="0">HPSGII theory</definiendum>
				<definiens id="0">consists of a set of descriptions which are interpreted as being true or false of an object in the domain</definiens>
			</definition>
			<definition id="2">
				<sentence>relational level : a simple picture There are three characteristics of HPSGII theories which we need to model on the relational level : one needs to be able to archy to organize the constraints , and theory .</sentence>
				<definiendum id="0">relational level</definiendum>
			</definition>
			<definition id="3">
				<sentence>Definition ( defined type ) A defined type is a type that occurs as antecedent of an implicational constraint in the grammar .</sentence>
				<definiendum id="0">Definition</definiendum>
				<definiens id="0">a type that occurs as antecedent of an implicational constraint in the grammar</definiens>
			</definition>
			<definition id="4">
				<sentence>As mentioned in section 2.1 , due to the closed world interpretation of type hierarchies , we know that every object in the denotation of a non-minimal type t also has to obey the constraints on one of the minimal subtypes of t. Thus , if a type t has a subtype t ' in common with a defined type d , then t ~ is a constrained type ( by virtue of being a subtype of d ) and t is a constrained type ( because it subsumes t ' ) .</sentence>
				<definiendum id="0">t</definiendum>
				<definiens id="0">a constrained type ( because it subsumes t ' )</definiens>
			</definition>
			<definition id="5">
				<sentence>Definition ( simple type ) A simple type is a type that is neither a constrained nor a hiding type .</sentence>
				<definiendum id="0">Definition ( simple type</definiendum>
				<definiens id="0">neither a constrained nor a hiding type</definiens>
			</definition>
			<definition id="6">
				<sentence>G3 ) is a wellformed list .</sentence>
				<definiendum id="0">G3 )</definiendum>
				<definiens id="0">a wellformed list</definiens>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>In such a framework , all natural language quantifiers have their meaning grounded in terms of two logical operators : V ( for all ) , and q ( there exists ) .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">all natural language quantifiers have their meaning grounded in terms of two logical operators</definiens>
			</definition>
			<definition id="1">
				<sentence>Instead , the desired reading is one in which `` a '' has a wider scope than `` every '' stating that there is a single course outline for the course CS404 , an outline that all students received .</sentence>
				<definiendum id="0">desired reading</definiendum>
				<definiens id="0">an outline that all students received</definiens>
			</definition>
			<definition id="2">
				<sentence>The basic problem is one of interpreting statements of the form every C P ( the set-theoretic counterpart of the wff Vx ( C ( x ) -- - ) P ( x ) ) , where C has an indeterminate cardinality .</sentence>
				<definiendum id="0">basic problem</definiendum>
				<definiens id="0">one of interpreting statements of the form every C P ( the set-theoretic counterpart of the wff Vx ( C ( x ) -- - ) P ( x ) ) , where C has an indeterminate cardinality</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>While HPSG has a more elaborated principle-based theory of possible phrase structures , TAG provides the means to represent lexicalized structures more explicitly .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">a more elaborated principle-based theory of possible phrase structures</definiens>
			</definition>
			<definition id="1">
				<sentence>HPSG is a feature-based grammatical framework which is characterized by a modular specification of linguistic generalizations through extensive use of principles and lexicalization of grammatical information .</sentence>
				<definiendum id="0">HPSG</definiendum>
				<definiens id="0">a feature-based grammatical framework which is characterized by a modular specification of linguistic generalizations through extensive use of principles and lexicalization of grammatical information</definiens>
			</definition>
			<definition id="2">
				<sentence>HPSG is a `` lexicalist '' framework , in the sense that the lexicon contains the information that determines which specific categories can be combined .</sentence>
				<definiendum id="0">HPSG</definiendum>
				<definiens id="0">a `` lexicalist '' framework , in the sense that the lexicon contains the information that determines which specific categories can be combined</definiens>
			</definition>
			<definition id="3">
				<sentence>Our translation process yields a lexicalized feature-based TAG ( VSJ88 ) in which feature structures are associated with nodes in the frontier of trees and two feature structures ( top and bottom ) with nodes in the interior .</sentence>
				<definiendum id="0">translation process</definiendum>
				<definiens id="0">yields a lexicalized feature-based TAG ( VSJ88 ) in which feature structures are associated with nodes in the frontier of trees and two feature structures ( top and bottom ) with nodes in the interior</definiens>
			</definition>
			<definition id="4">
				<sentence>Following ( Kas92 ) 4 , we adopt the notion of a selector daughter ( SD ) , which contains a selector feature ( SF ) whose value constrains the argument ( or non-selector ) daughter ( non-SD ) ) For example , in a head-complement structure , the SD is the HEAD-DTR , as it contains the list-valued feature coMPs ( the SF ) each of whose elements selects a C0m~-DTR , i.e. , an element of the CoMPs list is identified with the SYNSE~4 value of a COMP-DTR .</sentence>
				<definiendum id="0">SD</definiendum>
				<definiendum id="1">SD</definiendum>
				<definiens id="0">the SF ) each of whose elements selects a C0m~-DTR</definiens>
			</definition>
			<definition id="5">
				<sentence>Raising all SFs produces only fully saturated elementary trees and would require the root and foot of any auxiliary tree to share all SFs , in order to be compatible with the SF values across any domination links where adjoining can take place .</sentence>
				<definiendum id="0">Raising all SFs</definiendum>
			</definition>
			<definition id="6">
				<sentence>A tree is an auxiliary tree if the root and some frontier node ( which becomes the foot node ) have some non-empty SF value in common .</sentence>
				<definiendum id="0">tree</definiendum>
				<definiens id="0">an auxiliary tree if the root and some frontier node ( which becomes the foot node</definiens>
			</definition>
			<definition id="7">
				<sentence>Adjunction involves the identification of the foot node with the bottom of the domination link and identification of the root with top of the domination link .</sentence>
				<definiendum id="0">Adjunction</definiendum>
				<definiens id="0">involves the identification of the foot node with the bottom of the domination link and identification of the root with top of the domination link</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>The translation process consists of three phases : or bag , of source language signs instantiated with sufficiently rich linguistic information established by the parse to ensure adequate translations .</sentence>
				<definiendum id="0">translation process</definiendum>
				<definiens id="0">consists of three phases : or bag , of source language signs instantiated with sufficiently rich linguistic information established by the parse to ensure adequate translations</definiens>
			</definition>
			<definition id="1">
				<sentence>A TNCB records dominance information from derivations and is amenable to incremental updates .</sentence>
				<definiendum id="0">TNCB</definiendum>
				<definiens id="0">records dominance information from derivations and is amenable to incremental updates</definiens>
			</definition>
			<definition id="2">
				<sentence>The value of a TNCB is the sign that is formed from the combination of its children , or INCONSISTENT , representing the fact that they can not grammatically combine , or UNDETERMINED , i.e. it has not yet been established whether the signs combine .</sentence>
				<definiendum id="0">TNCB</definiendum>
				<definiens id="0">the sign that is formed from the combination of its children , or INCONSISTENT , representing the fact that they can not grammatically combine , or UNDETERMINED , i.e. it has not yet been established whether the signs combine</definiens>
			</definition>
			<definition id="3">
				<sentence>In other words , a maximal TNCB is a largest well-formed component of a TNCB .</sentence>
				<definiendum id="0">TNCB</definiendum>
				<definiens id="0">a largest well-formed component of a TNCB</definiens>
			</definition>
			<definition id="4">
				<sentence>The complexity of the test phase is the number of evaluations that have to be made .</sentence>
				<definiendum id="0">complexity of the test phase</definiendum>
				<definiens id="0">the number of evaluations that have to be made</definiens>
			</definition>
</paper>

		<paper id="1052">
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>A conj A CO A Condition : No undischarged assumptions in any conjunct .</sentence>
				<definiendum id="0">conj A CO A Condition</definiendum>
				<definiens id="0">No undischarged assumptions in any conjunct</definiens>
			</definition>
			<definition id="1">
				<sentence>Nominative Accusative Dative Genetive { ( air ) = + , ( obj ) = - } = + , ( obj ) = + } { ( air ) = - , ( obj ) = + } { ( d , r ) = - , ( obj ) = - } By assigning the NPs Mh'nner and Kindern the fully specified case features shown above , and Frauen the underspecified case feature ( obj ) = + , both the feature structure generalization and subsumption accounts of coordination fail to generate the ungrammatical ( 5a ) and ( hb ) , and correctly accept ( 5c ) , as shown in Figure 6 .</sentence>
				<definiendum id="0">Nominative Accusative Dative Genetive {</definiendum>
				<definiendum id="1">Frauen</definiendum>
				<definiens id="0">the underspecified case feature ( obj ) = + , both the feature structure generalization and subsumption accounts of coordination fail to generate the ungrammatical ( 5a ) and ( hb )</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>WordNet ( Miller , 1990 ) is an automatic source for such defining terms .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">an automatic source for such defining terms</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>In this derivation 'VPI ' abbreviates 'S\NPI ' , 'A ' is a lexieal rule which adds adjuncts to verbs , 'D ' is a lexical 'division ' rule which enables a control or raising verb to combine with arguments of higher arity , and 'D ' is a unary modal operator which diacritically marks infinitival verbs .</sentence>
				<definiendum id="0">'A '</definiendum>
				<definiens id="0">a unary modal operator which diacritically marks infinitival verbs</definiens>
			</definition>
			<definition id="1">
				<sentence>as 'H : : B ' where H is an atom ( the head ) and B is a list of atoms ( the negative literals ) .</sentence>
				<definiendum id="0">H</definiendum>
				<definiendum id="1">B</definiendum>
				<definiens id="0">a list of atoms</definiens>
			</definition>
			<definition id="2">
				<sentence>The atom x ( Cat , Left , Right ) is true iff the substring between the two string positions Left and Right can be analyzed as belonging to category Cat .</sentence>
				<definiendum id="0">atom x ( Cat</definiendum>
				<definiens id="0">true iff the substring between the two</definiens>
			</definition>
			<definition id="3">
				<sentence>A program is a set of definite clauses of the form p ( x ) ql ( Xl ) ^ ... ^ q. ( X. ) ^ ¢ where the Xi are vectors of variables , p ( X ) and qi ( Xi ) are relational atoms and ¢ is a basic constraint coming from a basic constraint language C. ¢~ will typically refer to some ( or all ) of the variables mentioned .</sentence>
				<definiendum id="0">¢</definiendum>
				<definiens id="0">a set of definite clauses of the form p ( x ) ql ( Xl ) ^ ... ^ q. ( X. ) ^ ¢ where the Xi are vectors of variables , p ( X ) and qi ( Xi ) are relational atoms and</definiens>
				<definiens id="1">a basic constraint coming from a basic constraint language C. ¢~ will typically refer to some ( or all ) of the variables mentioned</definiens>
			</definition>
			<definition id="4">
				<sentence>The language of basic constraints is closed under conjunction and comes with ( computable ) notions of consistency ( of a constraint ) and entailment ( ¢1 ~c ¢2 ) which have to be invariant under variable renaming } Given a program P and a goal G , which is a conjunction of relational atoms and constraints , a P-answer of G is defined as a consistent basic constraint ¢ such that ¢ -- + G is valid in every model of P. SLD-resolution is generalized in this setting by performing resolution only on relational atoms and simplifying ( conjunctions of ) basic constraints thus collected in the goal list .</sentence>
				<definiendum id="0">language of basic constraints</definiendum>
				<definiens id="0">closed under conjunction and comes with ( computable ) notions of consistency ( of a constraint ) and entailment ( ¢1 ~c ¢2 ) which have to be invariant under variable renaming } Given a program P and a goal G , which is a conjunction of relational atoms and constraints</definiens>
				<definiens id="1">a consistent basic constraint ¢ such that ¢ -- + G is valid in every model of P. SLD-resolution is generalized in this setting by performing resolution only on relational atoms and simplifying ( conjunctions of ) basic constraints thus collected in the goal list</definiens>
			</definition>
			<definition id="5">
				<sentence>A completed ( i.e. , atomic ) clause p ( T ) with an instantiated argument T abbreviates the non-atomic clause p ( X ) ~ X T , where the equality constraint makes the instantiation specific .</sentence>
				<definiendum id="0">completed</definiendum>
				<definiens id="0">atomic ) clause p ( T ) with an instantiated argument T abbreviates the non-atomic clause p ( X ) ~ X T , where the equality constraint makes the instantiation specific</definiens>
			</definition>
			<definition id="6">
				<sentence>Deduction memoizes every computational step ) .</sentence>
				<definiendum id="0">Deduction</definiendum>
				<definiens id="0">memoizes every computational step )</definiens>
			</definition>
			<definition id="7">
				<sentence>Definition 3 An item is a pair ( t , c ) where c is a clause and t is a tag , i.e. , one of program , solution or table ( B ) for some goal B. A lemma table for a goal G is a pair ( G , La ) where La is a finite list of items .</sentence>
				<definiendum id="0">c</definiendum>
			</definition>
			<definition id="8">
				<sentence>Definition 4 A control rule is a function from clauses G *-B to one of program , solution or table ( C ) for some goal C C B. A selection rule is a function from clauses G *-B where B contains at least one relational atom to relational atoms a , where a appears in B. Because program steps do not require memoization and given the constraints on the control rule just mentioned , the list LG associated with a lemma table ( G , LG ) will only contain items of the form ( t , G , -B ) where t is either solution or table ( C ) for some goal C C_ B. Definition 5 To add an item an item e = ( t , H ~ B ) to its table means to replace the table ( H , L ) in T with ( H , JelL\ ] ) .</sentence>
				<definiendum id="0">selection rule</definiendum>
				<definiendum id="1">lemma table</definiendum>
				<definiendum id="2">LG</definiendum>
				<definiendum id="3">t</definiendum>
				<definiens id="0">a function from clauses G *-B where B contains at least one relational atom to relational atoms a</definiens>
			</definition>
			<definition id="9">
				<sentence>Until A is empty , do : Remove an item e = it , c ) from A. Case t of program For each clause p E P such that c resolves with p on S ( c ) , choose a corresponding resolvent e ' and add iRic ' ) , c ' ) to A. table ( B ) Add e to its table , s If T contains a table ( B ' , L ) where B ' is a variant of B then for each item ( solution , d ) E L such that c resolves with d on B choose a corresponding resolvent d ' and add iR ( c '' ) , d ' ) to A. Otherwise , add a new table i B , ¢ ) to T , and add ( program , B ~-B ) to the agenda .</sentence>
				<definiendum id="0">B '</definiendum>
				<definiendum id="1">iR ( c ''</definiendum>
				<definiens id="0">an item e = it , c ) from A. Case t of program For each clause p E P such that c resolves with p on S ( c ) , choose a corresponding resolvent e ' and add iRic ' ) , c ' ) to A. table ( B ) Add e to its table , s If T contains a table ( B '</definiens>
			</definition>
			<definition id="10">
				<sentence>delay add_adjuncts ( _ , X/Y ) : vat ( X ) , vat ( Y ) .</sentence>
				<definiendum id="0">delay add_adjuncts</definiendum>
				<definiendum id="1">X/Y )</definiendum>
			</definition>
			<definition id="11">
				<sentence>The dynamic predicate table_solution ( I , S ) records all of the solution items generated for table I so far , and table_paxent ( I , T ) records the table items T , called 'parent items ' below , which are 'waiting ' for additional solution items from table I. The 'top level ' goal is prove ( G , Cs ) , where G is 105 a single atom ( the goal to be proven ) , and Cs is a list of ( unresolved ) solution constraints ( different solutions are enumerated through backtracking ) .</sentence>
				<definiendum id="0">G</definiendum>
				<definiendum id="1">Cs</definiendum>
				<definiens id="0">The dynamic predicate table_solution ( I , S ) records all of the solution items generated for table I so far</definiens>
			</definition>
			<definition id="12">
				<sentence>L is the item 's label , C its clause and I is the index of the table it belongs to .</sentence>
				<definiendum id="0">L</definiendum>
				<definiens id="0">the item 's label</definiens>
			</definition>
			<definition id="13">
				<sentence>For example , the technique can be used in chart parsing : in such a system an edge consists not only of a dotted rule and associated variable bindings ( i.e. , instantiated feature terms ) , but also contains zero or more as yet unresolved constraints that are propagated ( and simplified if sufficiently instantiated ) during application of the fundamental rule .</sentence>
				<definiendum id="0">variable bindings</definiendum>
				<definiens id="0">instantiated feature terms ) , but also contains zero or more as yet unresolved constraints that are propagated ( and simplified if sufficiently instantiated ) during application of the fundamental rule</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>The problem addressed in this research is that of improving the performance of a natural-language recogniser ( such as a recognition system for handwritten or spoken language ) .</sentence>
				<definiendum id="0">natural-language recogniser</definiendum>
				<definiens id="0">such as a recognition system for handwritten or spoken language )</definiens>
			</definition>
			<definition id="1">
				<sentence>D1 is a straightforward estimation of the conditional probability of co-occurrence .</sentence>
				<definiendum id="0">D1</definiendum>
				<definiens id="0">a straightforward estimation of the conditional probability of co-occurrence</definiens>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>Topic identification is one of two very important steps in the process of summarizing a text ; the second step is summary text generation .</sentence>
				<definiendum id="0">Topic identification</definiendum>
				<definiens id="0">one of two very important steps in the process of summarizing a text</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>Anaphora resolution is an important but still difficult problem for various large-scale natural language processing ( NLP ) applications , such as information extraction and machine tr~slation .</sentence>
				<definiendum id="0">Anaphora resolution</definiendum>
				<definiens id="0">an important but still difficult problem for various large-scale natural language processing ( NLP ) applications , such as information extraction and machine tr~slation</definiens>
			</definition>
			<definition id="1">
				<sentence>QZPRO ( `` quasi-zero pronoun '' ) is chosen when a sentence has multiple clauses ( subordinate or coordinate ) , and the zero pronouns in these clauses refer back to the subject of the initial clause in the same sentence , as shown in Figure 2 .</sentence>
				<definiendum id="0">QZPRO</definiendum>
				<definiens id="0">the subject of the initial clause in the same sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , if B refers to A and C refers to B , CA is a positive training example as well as B-A and C-B .</sentence>
				<definiendum id="0">CA</definiendum>
				<definiens id="0">a positive training example as well as B-A and C-B</definiens>
			</definition>
			<definition id="3">
				<sentence>The MDR applies different sets of generators , filters and orderers to resolve different anaphoric types ( Aone and McKee , 1993 ) .</sentence>
				<definiendum id="0">MDR</definiendum>
				<definiens id="0">applies different sets of generators</definiens>
			</definition>
			<definition id="4">
				<sentence>MLR-5 illustrates that when anaphoric type identification is turned on the MLR 's performance drops SF-measure is calculated by : F= ( ~2+1.0 ) × P x R # 2 x P+R where P is precision , R is recall , and /3 is the relative importance given to recall over precision .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">recall , and /3 is the relative importance given to recall over precision</definiens>
			</definition>
			<definition id="5">
				<sentence>Currently , the MDR uses an ordered list of multiple orderer KS 's for each anaphoric type ( cf. Table 4 ) , where the first applicable orderer KS in the list is used to pick the best antecedent when there is more than one possibility .</sentence>
				<definiendum id="0">MDR</definiendum>
				<definiens id="0">uses an ordered list of multiple orderer KS 's for each anaphoric type</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>Turning conceptual expressions into English requires the integration of large knowledge bases ( KBs ) , including grammar , ontology , lexicon , collocations , and mappings between them .</sentence>
				<definiendum id="0">KBs</definiendum>
			</definition>
			<definition id="1">
				<sentence>The FUF/SURGE ( Elhadad , 1993 ) generation system is an example where prepositional lexical restrictions in time adjuncts are encoded by hand , producing fluent expressions but at the cost of a larger grammar .</sentence>
				<definiendum id="0">FUF/SURGE</definiendum>
				<definiens id="0">an example where prepositional lexical restrictions in time adjuncts are encoded by hand , producing fluent expressions but at the cost of a larger grammar</definiens>
			</definition>
			<definition id="2">
				<sentence>• DEFAULT -- follows the topmost path in the lattice .</sentence>
				<definiendum id="0">DEFAULT --</definiendum>
				<definiens id="0">follows the topmost path in the lattice</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>The crucial factor in distinguishing between S~CE and BECAUSE is the relative order of core and contributor .</sentence>
				<definiendum id="0">BECAUSE</definiendum>
				<definiens id="0">the relative order of core and contributor</definiens>
			</definition>
			<definition id="1">
				<sentence>RDA is a synthesis of ideas from two theories of discourse structure ( Grosz and Sidner , 1986 ; Mann and Thompson , 1988 ) .</sentence>
				<definiendum id="0">RDA</definiendum>
				<definiens id="0">a synthesis of ideas from two theories of discourse structure</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>The metric used is a mutual information-like measure based on probabilities of modification relationships .</sentence>
				<definiendum id="0">metric used</definiendum>
				<definiens id="0">a mutual information-like measure based on probabilities of modification relationships</definiens>
			</definition>
			<definition id="1">
				<sentence>This will conceal any preference given by the parameters involving Q. In such cases , we observe that the test instance itself provides the information that the event t2 -- ~ t3 can occur and we recalculate the ratio using Pr ( t2 -- -* t3 ) = k for all possible categories t2 , t a where k is any non-zero constant .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">any non-zero constant</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>Bernard Lang defines parsing as ~ calculation of the intersection of a FSA ( the input ) and a CFG .</sentence>
				<definiendum id="0">FSA</definiendum>
				<definiens id="0">the input ) and a CFG</definiens>
			</definition>
			<definition id="1">
				<sentence>However , we also show that the termination properties change drastically : we show that it is undecidable whether the intersection of a FSA and a DCG is empty ( even if the DCG is off-line parsable ) .</sentence>
				<definiendum id="0">DCG</definiendum>
				<definiens id="0">off-line parsable )</definiens>
			</definition>
			<definition id="2">
				<sentence>A DCG is a simple example of a family of constraintbased grammar formalisms that are widely used in natural language analysis ( and generation ) .</sentence>
				<definiendum id="0">DCG</definiendum>
				<definiens id="0">a simple example of a family of constraintbased grammar formalisms that are widely used in natural language analysis</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus the intersection of a FSA and a CFG is a CFG that exactly derives all parse-trees .</sentence>
				<definiendum id="0">CFG</definiendum>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>score ( S , C ) = score ( CS , C ' ) score ( CS , GlobalCS ) \ [ 1\ ] where CS is the corresponding conceptual set of S , C ' is the set of conceptual expansions of all content words ( which are defined in LDOCE ) in C and GlobalCS is the conceptual set containing all the 1792 defining concepts .</sentence>
				<definiendum id="0">CS</definiendum>
				<definiendum id="1">GlobalCS</definiendum>
				<definiens id="0">the set of conceptual expansions of all content words ( which are defined in LDOCE ) in C and</definiens>
				<definiens id="1">the conceptual set containing all the 1792 defining concepts</definiens>
			</definition>
			<definition id="1">
				<sentence>f ( y ) ( using the Maximum Likelihood Estimator ) .</sentence>
				<definiendum id="0">f ( y )</definiendum>
			</definition>
			<definition id="2">
				<sentence>DBCC ( Defmition-Bascd Conceptual Cooccurrence ) and Human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the Brown corpus , respectively .</sentence>
				<definiendum id="0">DBCC</definiendum>
				<definiens id="0">Defmition-Bascd Conceptual Cooccurrence ) and Human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the Brown corpus , respectively</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>A factor in the objective function that favors smaller grammars over 228 s -- , sx ( l-e ) s x ( , ) X ~ A ( p ( A ) ) Aa -- -* a ( 1 ) VAeN- { S , X } VaET N = the set of all nonterminal symbols T = the set of all terminal symbols Probabilities for each rule are in parentheses .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">favors smaller grammars over 228 s -- , sx ( l-e ) s x ( , )</definiens>
				<definiens id="1">the set of all nonterminal symbols T = the set of all terminal symbols Probabilities for each rule are in parentheses</definiens>
			</definition>
			<definition id="1">
				<sentence>The goM of grammar induction is taken to be finding the grammar with the largest a posteriori probability given the training data , that is , finding the grammar G ~ where c ' = arg m xp ( GIo ) and where we denote the training data as O , for observations .</sentence>
				<definiendum id="0">goM of grammar induction</definiendum>
				<definiens id="0">finding the grammar with the largest a posteriori probability given the training data</definiens>
			</definition>
			<definition id="2">
				<sentence>In the case of grammatical language modeling , this corresponds to taking p ( G ) = 2 -t ( a ) where l ( G ) is the length of the description of the grammar in bits .</sentence>
				<definiendum id="0">l</definiendum>
				<definiens id="0">the length of the description of the grammar in bits</definiens>
			</definition>
			<definition id="3">
				<sentence>Initially , the set of nonterminal symbols consists of a different nonterminal symbol expanding to each terminal symbol .</sentence>
				<definiendum id="0">nonterminal symbols</definiendum>
				<definiens id="0">consists of a different nonterminal symbol expanding to each terminal symbol</definiens>
			</definition>
			<definition id="4">
				<sentence>3 A ~rigger is a phenomenon in the Viterbi parse of a sentence that is indicative that a particular move might lead to a better grammar .</sentence>
				<definiendum id="0">~rigger</definiendum>
				<definiens id="0">a phenomenon in the Viterbi parse of a sentence that is indicative that a</definiens>
			</definition>
			<definition id="5">
				<sentence>and take the smoothed igram probability to be a linear combination of the /-gram frequency in the training data and the smoothed ( i 1 ) -gram probability , that is , p ( w01W = wi_l ... w-i ) = c ( W~o0 ) + Ai , o ( w ) c ( W ) ( 1 Ai , c ( w ) ) p ( wolwi_2 . . . w-z ) where c ( W ) denotes the count of the word sequence W in the training data .</sentence>
				<definiendum id="0">c ( W )</definiendum>
				<definiens id="0">a linear combination of the /-gram frequency in the training data and the smoothed ( i 1 ) -gram probability , that is , p ( w01W = wi_l ... w-i ) = c ( W~o0 ) + Ai</definiens>
			</definition>
			<definition id="6">
				<sentence>• Xn } and the given terminal symbols , that is , all rules Xi -- -* Xj Xk i , j , k E { 1 , ... , n } Xi -- -* a i E { 1 , . . . , n } , a E T where T denotes the set of terminal symbols in the domain .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">n } , a E T where</definiens>
				<definiens id="1">the set of terminal symbols in the domain</definiens>
			</definition>
			<definition id="7">
				<sentence>For smoothing , we combine the expansion distribution of each symbol with a uniform distribution , that is , we take the smoothed parameter ps ( A -- -* a ) to be 1 p , ( A ~ a ) = ( 1 A ) p , , ( A -- -* a ) + An3 -F n\ [ T\ [ where p~ ( A -- ~ a ) denotes the unsmoothed parameter .</sentence>
				<definiendum id="0">p~</definiendum>
				<definiens id="0">the smoothed parameter ps ( A -- -* a ) to be 1 p , ( A ~ a</definiens>
				<definiens id="1">the unsmoothed parameter</definiens>
			</definition>
			<definition id="8">
				<sentence>The ideal grammar denotes the grammar used to generate the training and test data .</sentence>
				<definiendum id="0">ideal grammar</definiendum>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>Quantifying-in is a technique originally introduced to produce appropriate semantic forms for de re interpretations of NPs inside opaque operators nous `` I~ee Adjoining Grammar ( Shieber &amp; Schabes , 1990 ) without much change .</sentence>
				<definiendum id="0">Quantifying-in</definiendum>
				<definiens id="0">a technique originally introduced to produce appropriate semantic forms for de re interpretations of NPs inside opaque operators nous `` I~ee Adjoining Grammar ( Shieber &amp; Schabes , 1990 ) without much change</definiens>
			</definition>
			<definition id="1">
				<sentence>CCGs make use of a limited set of combinators , type raising ( T ) , function composition ( B ) , and function substitution ( S ) , with directionality of combination for syntactic grammaticality .</sentence>
				<definiendum id="0">CCGs</definiendum>
				<definiens id="0">make use of a limited set of combinators , type raising ( T ) , function composition ( B ) , and function substitution ( S ) , with directionality of combination for syntactic grammaticality</definiens>
			</definition>
			<definition id="2">
				<sentence>B s : q-every ( X , dlr ( X ) , shog ( X , Y , g ) /np : Z/np : Y s : q-nost ( Y , cstaw ( Y ) , q-every ( X , dlr ( X ) , show ( X , Y , Z ) ) ) /np : g three cars s : S\ ( s : S /np : s-three ( car ) ) s : q-nost ( Y , cstnr ( Y ) , q-every ( X , dlr ( X ) , show ( X , Y , s-three ( car ) ) ) ) Figure 4 : Every dealer shows most customers three cars : One sample derivation investigate two dialects of ( s : investigate ( X , g ) ~ap : X ) /np : Y np : s-two ( l ) n : lt/ ( n : il ( n : Y'tnd ( l , of ( l , Z ) ) ~n : I1 ) /n : i \n : Y'dialect ( Y ) ) /np : g ... ... ... ... ... ... ... ... ... ... ... ... ... ... . ~B n : Y'and ( dialect ( g ) , of ( g , z ) ) /np : Z ... ... ... ... ... ... ... ... .. &gt;</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">lt/ ( n : il ( n : Y'tnd ( l</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>Polyphony is a theory that models utterances with three levels of characters .</sentence>
				<definiendum id="0">Polyphony</definiendum>
				<definiens id="0">a theory that models utterances with three levels of characters</definiens>
			</definition>
			<definition id="1">
				<sentence>The signification of a sentence is defined as a disjunction of subsets of C. Given a sentence , we identify operators , connectives and modifiers , and build the A-structure of the sentence linking these linguistic clues to the TSS 's .</sentence>
				<definiendum id="0">signification of a sentence</definiendum>
				<definiens id="0">a disjunction of subsets of C. Given a sentence , we identify operators , connectives and modifiers , and build the A-structure of the sentence linking these linguistic clues to the TSS 's</definiens>
			</definition>
			<definition id="2">
				<sentence>Connectives constrain a pair of sentences or a sentence and a discursive environment , operators constrain argumentative power , and modifiers constrain only argumentative orientation and strength .</sentence>
				<definiendum id="0">Connectives</definiendum>
				<definiens id="0">constrain a pair of sentences or a sentence and a discursive environment , operators constrain argumentative power , and modifiers constrain only argumentative orientation and strength</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>For example , XTAG currently includes over 100,000 lexemes , each of which is associated with a family of trees ( typically around 20 ) drawn from a set of over 500 elementary trees .</sentence>
				<definiendum id="0">XTAG</definiendum>
			</definition>
			<definition id="1">
				<sentence>S NPI VP V o NPI PP P o NPI Figure 1 : An example LTAG tree for give The principal unit of ( syntactic ) information associated with an LTAG entry is a tree structure in which the tree nodes are labeled with syntactic categories and feature information and there is at least one leaf node labeled with a lexical category ( such lexical leaf nodes are known as anchors ) .</sentence>
				<definiendum id="0">S NPI VP V o NPI PP P o NPI Figure 1</definiendum>
				<definiendum id="1">LTAG entry</definiendum>
			</definition>
			<definition id="2">
				<sentence>Ignoring for the moment the references to TREENODE , VPTREE , NPCOMP and PTREE ( which we shall define shortly ) , we see that VERB defines basic features for all verb entries ( and can be used directly for intransitives such as Die ) , VERB+NP inherits ~om VERB butadds an NP complement to the right of the verb ( for transitives ) , VEKB+NP+PP inherits ~om VERB+NP but adds a further PP complement and so 79 on .</sentence>
				<definiendum id="0">PTREE</definiendum>
				<definiendum id="1">VERB+NP</definiendum>
				<definiens id="0">inherits ~om VERB butadds an NP complement to the right of the verb ( for transitives</definiens>
			</definition>
			<definition id="3">
				<sentence>PTREE : &lt; &gt; == TREENODE &lt; cat &gt; I= p &lt; type &gt; == anchor &lt; parent &gt; == PPTREE : &lt; &gt; Here , TREENODE represents an abstract node in an LTAG tree and provides a ( default ) type of internal .</sentence>
				<definiendum id="0">TREENODE</definiendum>
				<definiens id="0">an abstract node in an LTAG tree</definiens>
			</definition>
			<definition id="4">
				<sentence>Notice that VERB is itself a TREENODE ( but with the nondefault type anchor ) , and the other definitions here define the remaining tree nodes that arise in our small lexicon : VPTREE is the node for VERB 's parent , STREE for VEKB 's grandparent , NPCOMP defines the structure needed for NP complement substitution nodes , etc. 1° Taken together , these definitions provide a specification for Give just as we had it before , but with the addition of type and root features .</sentence>
				<definiendum id="0">TREENODE</definiendum>
				<definiendum id="1">VPTREE</definiendum>
				<definiendum id="2">NPCOMP</definiendum>
				<definiens id="0">the node for VERB 's parent</definiens>
			</definition>
			<definition id="5">
				<sentence>In our analysis , VERB is the intransitive verb class , with complements specifically marked as undefined : thus VERB : &lt; right &gt; == under is inherited from TREENODE and VERB+NP just overrides this complement specification to add an NP complement .</sentence>
				<definiendum id="0">VERB</definiendum>
				<definiens id="0">the intransitive verb class</definiens>
			</definition>
</paper>

		<paper id="1038">
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>From a theoretical standpoint , we will demonstrate that theories which postulate a strict tree structure of discourse ( henceforth , Tree Structure Theory , or TST ) on either the intentional level or the attentional level ( Grosz and Sidner 1986 ) are not totally adequate for covering spontaneous dialogues , particularly negotiation dialogues which are composed of multiple threads .</sentence>
				<definiendum id="0">TST</definiendum>
				<definiendum id="1">attentional level</definiendum>
				<definiens id="0">henceforth , Tree Structure Theory , or</definiens>
			</definition>
			<definition id="1">
				<sentence>We are not claiming that speech act recognition is the best way to evaluate the validity of a theory of discourse , but because speech act recognition is the main aspect of the discourse processor which we have implemented , and because recognizing the discourse structure is part of the process of identifying the correct speech act , we believe it was the best way to evaluate the difference between the two different focusing mechanisms in our implementation at this time .</sentence>
				<definiendum id="0">act recognition</definiendum>
				<definiens id="0">the best way to evaluate the validity of a theory of discourse , but because speech</definiens>
				<definiens id="1">because recognizing the discourse structure is part of the process of identifying the correct speech act</definiens>
			</definition>
</paper>

		<paper id="1039">
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>Defeasibili~y is a notion that is tricky to deal with , and scholars in logics and pragmatics have learned to circumvent it or live with it .</sentence>
				<definiendum id="0">Defeasibili~y</definiendum>
			</definition>
			<definition id="1">
				<sentence>L a. Formally , we say that the u level is stronger than the i level , which is stronger than the d level : u &lt; i &lt; d. At the syntactic level , we allow atomic formulas to be labelled according to the same underlying lattice. Compound formulas are obtained in the usual way. This will give us formulas such as regrets u ( John , come ( Mary , party ) ) -- - , cornel ( Mary , party ) ) , or ( Vx ) ( '- , bachelorU ( x ) -- ~ ( malea ( ) ^ The satisfaction relation is split according to the three levels of truth into u-satisfaction , i-satisfaction , and d-satisfaction : Definition 2.1 Assume ~r is an St. valuation such that t~ = di E and assume that St. maps n-ary predicates p to relations R C 7~ × ... × 79. For any atomic formula p= ( tl , t2 , ... , t , ) , and any stratified valuation a , where z E { u , i , d } and ti are terms , the z-satisfiability relations are defined as follows : • a ~u p~ ( tl , ... , tn ) iff ( dx , ... , dnl E 1~ ~ • iff ( dl , ... , dn ) E R u UR -- ffUR i • o , ~u pa ( tx , ... , t , ) iff ( dz , ... , d , ) E R '' U'R-¢URIU-~URa • tr ~ip~ ( tl , ... , t , ) iff ( dt , ... , d , ) E R i . cr ~ipi ( tt , ... , t , ) iff ( dl , ... , d , ) E R i • pd ( tl , ... , t , ) ig ( dl , ... , d , ) E R i U~TUR d • o '' ~ap~ ( tz , ... , tn ) iff ( dl , ... , dn ) E R a • ¢ ( tl , ... , t , ) iff ( all , ... , d , ) e R d 145 • o '' ~d pd ( tl , ... , tn ) iff ( di , ... , dr , ) C= R d Definition 2.1 extends in a natural way to negated and compound formulas. Having a satisfaction definition associated with each level of strength provides a high degree of flexibility. The same theory can be interpreted from a perspective that allows more freedom ( u-satisfaction ) , or from a perspective that is tighter and that signals when some defeasible information has been cancelled ( iand d-satisfaction ) . Possible interpretations of a given set of utterances with respect to a knowledge base are computed using an extension of the semantic tableau method. This extension has been proved to be both sound and complete ( Marcu , 1994 ) . A partial ordering , &lt; , determines the set of optimistic interpretations for a theory. An interpretation m0 is preferred to , or is more optimistic than , an interpretation ml ( m0 &lt; ml ) if it contains more information and that information can be more easily updated in the future. That means that if an interpretation m0 makes an utterance true by assigning to a relation R a defensible status , while another interpretation ml makes the same utterance true by assigning the same relation R a stronger status , m0 will be the preferred or optimistic one , because it is as informative as mi and it allows more options in the future ( R can be defeated ) . Pragmatic inferences are triggered by utterances. To differentiate between them and semantic inferences , we introduce a new quantifier , V vt , whose semantics is defined such that a pragmatic inference of the form ( VVtg ) ( al ( ,7 ) -- * a2 ( g ) ) is instantiated only for those objects t ' from the universe of discourse that pertain to an utterance having the form al ( ~Hence , only if the antecedent of a pragmatic rule has been uttered can that rule be applied. A recta-logical construct uttered applies to the logical translation of utterances. This theory yields the following definition : Definition 2.2 Let ~b be a theory described in terms of stratified first-order logic that appropriately formalizes the semantics of lezical items and the necessary conditions that trigger pragmatic inferences. The semantics of lezical terms is formalized using the quantifier V , while the necessary conditions that pertain to pragmatic inferences are captured using V trt. Let uttered ( u ) be the logical translation of a given utterance or set of utterances. We say that utterance u pragmatically implicates p if and only if p d or p i is derived using pragmatic inferences in at least one optimistic model of the theory ~ U uttered ( u ) , and if p is not cancelled by any stronger information ( '.p~ , -.pi _.pd ) in any optimistic model schema of the theory. Symmetrically , one can define what a negative pragmatic inference is. In both cases , W uttered ( u ) is u-consistent. Our algorithm , described in detail by Marcu ( 1994 ) , takes as input a set of first-order stratified formulas • that represents an adequate knowledge base that expresses semantic knowledge and the necessary conditions for triggering pragmatic inferences , and the translation of an utterance or set of utterances uttered ( u ) . The Mgorithm builds the set of all possible interpretations for a given utterance , using a generalization of the semantic tableau technique. The model-ordering relation filters the optimistic interpretations. Among them , the defeasible inferences that have been triggered on pragmatic grounds are checked to see whether or not they are cancelled in any optimistic interpretation. Those that are not cancelled are labelled as pragmatic inferences for the given utterance or set of utterances. We present a set of examples that covers a representative group of pragmatic inferences. In contrast with most other approaches , we provide a consistent methodology for computing these inferences and for determining whether they are cancelled or not for all possible configurations : simple and complex utterances and sequences of utterances. A factive such as the verb regret presupposes its complement , but as we have seen , in positive environments , the presupposition is stronger : it is acceptable to defeat a presupposition triggered in a negative environment ( 2 ) , but is infelicitous to defeat one that belongs to a positive environment ( 1 ) . Therefore , an appropriate formalization of utterance ( 3 ) and the req~fisite pragmatic knowledge will be as shown in ( 4 ) . ( 3 ) John does not regret that Mary came to the party. ( 4 ) uttered ( - , regrets u ( john , come ( , ,ry , party ) ) ) ( VU'= , y , z ) ( regras ( = , come ( y , co e i ( y , z ) ) ( Vu'= , y , z ) ( regret , '' ( = , come ( y , z ) ) -* corned ( y , z ) ) The stratified semantic tableau that corresponds to theory ( 4 ) is given in figure 2. The tableau yields two model schemata ( see figure 3 ) ; in both of them , it is defeasibly inferred that Mary came to the party. The model-ordering relation &lt; establishes m0 as the optimistic model for the theory because it contains as much information as ml and is easier to defeat. Model m0 explains why Mary came to the party is a presupposition for utterance ( 3 ) . 146 `` ~regrets ( john , come ( mary , party ) ) ( Vx , y , z ) ( -~regrets ( x , come ( y , z ) ) -- -* corned ( y , z ) ) ( Vx , y , z ) ( regrets ( x , come ( y , z ) ) -- * comei ( y , z ) ) I -.regrets ( john , come ( mary , party ) ) -corned ( mary , party ) regrets ( john , come ( mary , party ) ) -- * comei ( mary , party ) regrets ( john , come ( mary , party ) ) corned ( mary , party ) u-closed -.regrets ( john , come ( mary , party ) ) come i ( mary , party ) m_0 mL1 Figure 2 : Stratified tableau for John does not regret that Mary came to the party. Schema # Indefeasible Infelicitously defeasible `` , regrets ~ ( john , come ( mary , party ) -.regTets ~ ( joh. , come ( mary , party ) mo ml come ~ ( mary , party ) Felicitously defeasible corned ( mary , party ) cornea ( mary , party ) Figure 3 : Model schemata for John does not regret that Mary came to the party. Schema # Indefeasible mo went '' ( some ( boys ) , theatre ) -.went '' ( all ( boys ) , theatre ) Infelicitously Felicitously defeasible de feasible - ' , wentd ( most ( boys ) , theatre ) -.wentd ( many ( boys ) , theatre ) - , wentd ( all ( boys ) , theatre ) Figure 4 : Model schema for John says that some of thc boys went to the theatre. Schema # Indefeasible In\ ] elicitously Felicitously de\ ] easible de feasible mo we , ,t '' ( some ( boy , ) , theatre ) , oe , ,t '' ( most ( boys ) , theatre ) went~ ( many ( boys ) , theatre ) went~ ( all ( boys ) , theatre ) d `` .went ( most ( boys ) , theatre ) d -.went ( many ( boys ) , theatre ) -~wentd ( all ( boys ) , theatre ) Figure 5 : Model schema for John says that some of the boys went to the theatre. In fact all of them went to the theatre. 147 Consider utterance ( 5 ) , and its implicatures ( 6 ) . ( 5 ) John says that some of the boys went to the theatre. ( 6 ) Not { many/most/all } of the boys went to the theatre. An appropriate formalization is given in ( 7 ) , where the second formula captures the defeasible scalar implicatures and the third formula reflects the relevant semantic information for all. ( r ) uttered ( went ( some ( boys ) , theatre ) ) went '' ( some ( boys ) , theatre ) -- -* ( -~wentd ( many ( boys ) , theatre ) A `` , wentd ( most ( boys ) , theatre ) ^ -~wentd ( aii ( boys ) , theatre ) ) went '' ( all ( boys ) , theatre ) ( went '' ( most ( boys ) , theatre ) A went '' ( many ( boys ) , theatre ) ^ went '' ( some ( boys ) , theatre ) ) The theory provides one optimistic model schema ( figure 4 ) that reflects the expected pragmatic inferences , i.e. , ( Not most/Not many/Not all ) of the boys went to the theatre. Assume now , that after a moment of thought , the same person utters : ( 8 ) John says that some of the boys went to the theatre. In fact all of them went to the theatre. By adding the extra utterance to the initial theory ( 7 ) , uttered ( went ( ail ( boys ) , theatre ) ) , one would obtain one optimistic model schema in which the conventional implicatures have been cancelled ( see figure 5 ) . The Achilles heel for most theories of presupposition has been their vulnerability to the projection problem. Our solution for the projection problem does not differ from a solution for individual utterances. Consider the following utterances and some of their associated presuppositions ( 11 ) ( the symbol t &gt; precedes an inference drawn on pragmatic grounds ) : ( 9 ) Either Chris is not a bachelor or he regrets that Mary came to the party .</sentence>
				<definiendum id="0">level</definiendum>
				<definiendum id="1">bachelorU (</definiendum>
				<definiendum id="2">W uttered</definiendum>
				<definiendum id="3">`` ~regrets</definiendum>
				<definiendum id="4">z ) ( -~regrets</definiendum>
				<definiendum id="5">z ) ( regrets</definiendum>
				<definiendum id="6">party ) -.regTets ~</definiendum>
				<definiens id="0">x ) -- ~ ( malea ( ) ^ The satisfaction relation is split according to the three levels of truth into u-satisfaction , i-satisfaction , and d-satisfaction : Definition 2.1 Assume ~r is an St. valuation such that t~ = di E and assume that St. maps n-ary predicates p to relations R C 7~ × ... × 79. For any atomic formula p= ( tl , t2 , ... , t , ) , and any stratified valuation a</definiens>
				<definiens id="1">a ~u p~ ( tl , ... , tn ) iff ( dx , ... , dnl E 1~ ~ • iff ( dl , ... , dn ) E R u UR -- ffUR i • o , ~u pa ( tx , ... , t , ) iff ( dz , ... , d , ) E R '' U'R-¢URIU-~URa • tr ~ip~ ( tl , ... , t , ) iff ( dt , ... , d , ) E R i</definiens>
				<definiens id="2">tt , ... , t , ) iff ( dl , ... , d , ) E R i • pd ( tl , ... , t , ) ig ( dl , ... , d , ) E R i U~TUR d • o '' ~ap~ ( tz , ... , tn ) iff ( dl , ... , dn ) E R a • ¢ ( tl , ... , t , ) iff ( all , ... , d , ) e R d 145 • o '' ~d pd ( tl , ... , tn ) iff ( di , ... , dr , ) C= R d Definition 2.1 extends in a natural way to negated and compound formulas. Having a satisfaction definition associated with each level of strength provides a high degree of flexibility. The same theory can be interpreted from a perspective that allows more freedom ( u-satisfaction ) , or from a perspective that is tighter and that signals when some defeasible information has been cancelled ( iand d-satisfaction ) . Possible interpretations of a given set of utterances with respect to a knowledge base are computed using an extension of the semantic tableau method. This extension has been proved to be both sound and complete ( Marcu , 1994 ) . A partial ordering , &lt; , determines the set of optimistic interpretations for a theory. An interpretation m0 is preferred to , or is more optimistic than , an interpretation ml ( m0 &lt; ml ) if it contains more information and that information can be more easily updated in the future. That means that if an interpretation m0 makes an utterance true by assigning to a relation R a defensible status , while another interpretation ml makes the same utterance true by assigning the same relation R a stronger status</definiens>
				<definiens id="3">informative as mi and it allows more options in the future ( R can be defeated ) . Pragmatic inferences are triggered by utterances. To differentiate between them and semantic inferences</definiens>
				<definiens id="4">a theory described in terms of stratified first-order logic that appropriately formalizes the semantics of lezical items and the necessary conditions that trigger pragmatic inferences. The semantics of lezical terms is formalized using the quantifier V , while the necessary conditions that pertain to pragmatic inferences are captured using V trt. Let uttered ( u ) be the logical translation of a given utterance or set of utterances. We say that utterance u pragmatically implicates p if and only if p d or p i is derived using pragmatic inferences in at least one optimistic model of the theory ~ U uttered ( u )</definiens>
				<definiens id="5">takes as input a set of first-order stratified formulas • that represents an adequate knowledge base that expresses semantic knowledge and the necessary conditions for triggering pragmatic inferences , and the translation of an utterance or set of utterances uttered ( u ) . The Mgorithm builds the set of all possible interpretations for a given utterance , using a generalization of the semantic tableau technique. The model-ordering relation filters the optimistic interpretations. Among them , the defeasible inferences that have been triggered on pragmatic grounds are checked to see whether or not they are cancelled in any optimistic interpretation. Those that are not cancelled are labelled as pragmatic inferences for the given utterance or set of utterances. We present a set of examples that covers a representative group of pragmatic inferences. In contrast with most other approaches , we provide a consistent methodology for computing these inferences and for determining whether they are cancelled or not for all possible configurations : simple and complex utterances and sequences of utterances. A factive such as the verb</definiens>
				<definiens id="6">the optimistic model for the theory because it contains as much information as ml and is easier to defeat. Model m0 explains why Mary came to the party is a presupposition for utterance ( 3 ) . 146</definiens>
				<definiens id="7">mary , party ) Figure 3 : Model schemata for John does not regret that Mary came to the party. Schema # Indefeasible mo went '' ( some ( boys ) , theatre ) -.went '' ( all ( boys ) , theatre</definiens>
				<definiens id="8">all ( boys ) , theatre ) Figure 4 : Model schema for John says that some of thc boys went to the theatre. Schema # Indefeasible In\ ] elicitously Felicitously de\ ] easible de feasible mo we , ,t '' ( some ( boy , ) , theatre ) , oe , ,t '' ( most ( boys ) , theatre ) went~ ( many ( boys ) , theatre ) went~ ( all ( boys ) , theatre ) d `` .went ( most ( boys ) , theatre ) d -.went ( many ( boys ) , theatre ) -~wentd ( all ( boys ) , theatre ) Figure 5 : Model schema for John says that some of the boys went to the theatre. In fact all of them went to the theatre. 147 Consider utterance ( 5 ) , and its implicatures ( 6 ) . ( 5 ) John says that some of the boys went to the theatre. ( 6 ) Not { many/most/all } of the boys went to the theatre. An appropriate formalization is given in ( 7 ) , where the second formula captures the defeasible scalar implicatures and the third formula reflects the relevant semantic information for all. ( r ) uttered ( went ( some ( boys ) , theatre ) ) went '' ( some ( boys ) , theatre ) -- -* ( -~wentd ( many ( boys ) , theatre ) A `` , wentd ( most ( boys ) , theatre ) ^ -~wentd ( aii ( boys ) , theatre ) ) went '' ( all ( boys ) , theatre ) ( went '' ( most ( boys ) , theatre ) A went '' ( many ( boys ) , theatre ) ^ went '' ( some ( boys ) , theatre ) ) The theory provides one optimistic model schema ( figure 4 ) that reflects the expected pragmatic inferences , i.e. , ( Not most/Not many/Not all ) of the boys went to the theatre. Assume now , that after a moment of thought , the same person utters : ( 8 ) John says that some of the boys went to the theatre. In fact all of them went to the theatre. By adding the extra utterance to the initial theory</definiens>
				<definiens id="9">the symbol t &gt; precedes an inference drawn on pragmatic grounds ) : ( 9 ) Either Chris is not a bachelor or he regrets that Mary came to the party</definiens>
			</definition>
			<definition id="2">
				<sentence>( 10 ) Chris is a bachelor or a spinster .</sentence>
				<definiendum id="0">Chris</definiendum>
				<definiens id="0">a bachelor or a spinster</definiens>
			</definition>
			<definition id="3">
				<sentence>The stratified semantic tableau that corresponds to its logical theory yields 16 models , but only Chris is an adult satisfies definition 2.2 and is projected as presupposition for the utterance .</sentence>
				<definiendum id="0">semantic tableau</definiendum>
				<definiens id="0">an adult satisfies definition 2.2 and is projected as presupposition for the utterance</definiens>
			</definition>
			<definition id="4">
				<sentence>Therefore , it cancels the early presupposition ( 14 ) : ( 16 ) ~ John is an adult .</sentence>
				<definiendum id="0">John</definiendum>
				<definiens id="0">an adult</definiens>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>For every context w in D , E ( w ) is the set of symbols available in the context w and A ( ~rlw ) is the conditional probability of the symbol c~ in the context w. Note that ) -- \ ] o~ A ( c~\ [ w ) &lt; 1 for all contexts w in the dictionary D. The probability /5 ( h1¢ ) of a string h given the model ¢ , h • E ' , is calculated as a chain of conditional probabilities ( 1 ) /5 ( h { ¢ ) -- '' ~ ( hnlhl ... hn_l , ¢ ) ~ ( hl ... h , ~_ll¢ ) ( 1 ) while the conditional probability ih ( elh , ¢ ) of a single symbol ~r after the history h is defined as ( 2 ) .</sentence>
				<definiendum id="0">E ( w )</definiendum>
				<definiendum id="1">A ( ~rlw )</definiendum>
				<definiendum id="2">¢ ) ~ ( hl ... h , ~_ll¢ ) ( 1 ) while the conditional probability ih</definiendum>
				<definiens id="0">the conditional probability of the symbol c~ in the context w. Note that ) -- \ ] o~ A ( c~\</definiens>
				<definiens id="1">h1¢ ) of a string h given the model ¢ , h • E '</definiens>
			</definition>
			<definition id="1">
				<sentence>1 ) ~ ( E ( h ) \ [ h ) ( 3 ) 6 ( h ) 1~ ( E ( h ) Ih2 ... h , ~ , ¢ ) Note that E ( h ) represents a set of symbols , and so by a slight abuse of notation ) ~ ( E ( h ) Ih ) denotes ~\ ] ~eE ( h ) A ( a\ [ h ) , ie. , the sum of A ( alh ) over all ~ in E ( h ) .</sentence>
				<definiendum id="0">E</definiendum>
				<definiendum id="1">Ih</definiendum>
				<definiens id="0">( h ) Ih2 ... h , ~ , ¢ ) Note that E ( h ) represents a set of symbols , and so by a slight abuse of notation ) ~ ( E ( h )</definiens>
			</definition>
			<definition id="2">
				<sentence>Let c ( c~\ [ w ) be the number of times that the symbol followed the string w in the training corpus , and let c ( w ) be the sum ~es c ( crlw ) of all its conditional frequencies .</sentence>
				<definiendum id="0">Let c ( c~\ [ w )</definiendum>
				<definiens id="0">the number of times that the symbol followed the string w in the training corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>LID ) : Lz ( n ) +log ( n+m-lm_l ) +log ( no + nl + ... + nm 1 ) !</sentence>
				<definiendum id="0">LID )</definiendum>
			</definition>
			<definition id="4">
				<sentence>rn-1 i +Lz &lt; ( \ [ \ [ DJ\ [ , n ) i=l + log ( n + I LDJl 1 JLDJI-7 ) \ 222 where \ [ DJ is the set of all contexts in D that are proper suffixes of another context in D. The first term encodes the number n of internal vertices using the Elias code .</sentence>
				<definiendum id="0">DJ</definiendum>
			</definition>
			<definition id="5">
				<sentence>ALe ( w , E ' ) L ( ¢ U ( { w } × E ' ) ) L ( ¢ ) ( 6 ) ALT ( W , E ' ) L ( TI¢ ) L ( T\ [ ¢ U ( { w } x E ' ) ) ( 7 ) Keeping only significant terms that are monotonically nondecreasing , we approximate the incremental cost ALe ( w , E ' ) as loglDl+log IS'l + log c ( Lwj ) + log ( c ( w ) ls , i + C 'I ) The first term represents the incremental increase in the size of the context dictionary D. The second term represents the cost of encoding the candidate extensions E ( w ) = E ~ .</sentence>
				<definiendum id="0">ALT</definiendum>
				<definiens id="0">the cost of encoding the candidate extensions E ( w ) = E ~</definiens>
			</definition>
			<definition id="6">
				<sentence>Extend ( w ) determines all profitable extensions of the candidate context w , if any exist .</sentence>
				<definiendum id="0">Extend ( w )</definiendum>
				<definiens id="0">determines all profitable extensions of the candidate context w</definiens>
			</definition>
			<definition id="7">
				<sentence>Refine ( D , E , n ) Cn is the set of candidate contexts of length n , obtained from the training corpus .</sentence>
				<definiendum id="0">n ) Cn</definiendum>
				<definiens id="0">the set of candidate contexts of length n , obtained from the training corpus</definiens>
			</definition>
			<definition id="8">
				<sentence>Dn is the set of profitable contexts of length n , while En is the set of profitable extensions of those contexts .</sentence>
				<definiendum id="0">Dn</definiendum>
				<definiendum id="1">En</definiendum>
				<definiens id="0">the set of profitable contexts of length n</definiens>
				<definiens id="1">the set of profitable extensions of those contexts</definiens>
			</definition>
			<definition id="9">
				<sentence>According to the third criteria of statistical efficiency , the best model is one that achieves the lowest test message entropy for a given model order .</sentence>
				<definiendum id="0">message entropy</definiendum>
				<definiens id="0">one that achieves the lowest test</definiens>
			</definition>
			<definition id="10">
				<sentence>An extension mixture model is an extension model whose ~ ( ~lw ) parameters are estimated by linearly interpolating the empirical probability estimates for all extensions that dominate w with respect to c~ , ie. , all extensions whose symbol is and whose context is a suffix of w. Extension mixing allows us to remove the uniform flattening of zero frequency symbols in our parameter estimates ( 5 ) .</sentence>
				<definiendum id="0">extension mixture model</definiendum>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>The XTAG system consists of a morphological analyzer , a part-ofspeech tagger , a wide-coverage LTAG English grammar , a predictive left-to-right Early-style parser for LTAG ( Schabes , 1990 ) and an X-windows interface for grammar development ( Paroubek et al. , 1992 ) .</sentence>
				<definiendum id="0">XTAG system</definiendum>
			</definition>
			<definition id="1">
				<sentence>head_tree : the tree anchored by the head word ; `` - '' if the current word does not depend on any other word .</sentence>
				<definiendum id="0">head_tree</definiendum>
				<definiens id="0">the tree anchored by the head word</definiens>
			</definition>
			<definition id="2">
				<sentence>The coverage of the FST is the number of inputs that were assigned a correct generalized parse among the parses retrieved by traversing the FST .</sentence>
				<definiendum id="0">FST</definiendum>
				<definiens id="0">the number of inputs that were assigned a correct generalized parse among the parses retrieved by traversing the FST</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>A regular relation is a mapping from one regular language to another one .</sentence>
				<definiendum id="0">regular relation</definiendum>
				<definiens id="0">a mapping from one regular language to another one</definiens>
			</definition>
			<definition id="1">
				<sentence>An fst pair a : b can be thought of as the crossproduct of a and b , the minimal relation consisting of a ( the upper symbol ) and b ( the lower symbol ) .</sentence>
				<definiendum id="0">b</definiendum>
				<definiens id="0">the upper symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>The unconditional replacement of UPPER by LOWER is written \ [ 6\ ] UPPER - &gt; LOWER Here UPPER and LOWER are any regular expressions that describe simple regular languages .</sentence>
				<definiendum id="0">LOWER</definiendum>
				<definiens id="0">any regular expressions that describe simple regular languages</definiens>
			</definition>
			<definition id="3">
				<sentence>UPPER &lt; LOWER \ [ 18\ ] is defined as the inverse of the relation LOWER - &gt; UPPER .</sentence>
				<definiendum id="0">UPPER &lt; LOWER</definiendum>
			</definition>
			<definition id="4">
				<sentence>\ [ 19\ ] UPPER ( - &gt; ) LOWER is defined as UPPER - &gt; \ [ LOWER \ [ UPPER\ ] \ [ 20\ ] The optional replacement relation maps UPPER to both LOWER and UPPER .</sentence>
				<definiendum id="0">UPPER</definiendum>
				<definiens id="0">UPPER - &gt; \ [ LOWER \ [ UPPER\ ] \ [ 20\ ] The optional replacement relation maps UPPER to both LOWER and UPPER</definiens>
			</definition>
			<definition id="5">
				<sentence>We use four alternate separators , \ [ I , // , \ \ and \/ , which gives rise to four types of conditional replacement expressions : \ [ 21l ( 1 ) Upward-oriented : UPPER - &gt; LOWER J\ [ LEFT RIGHT ; ( 2 ) Right-oriented : UPPER- &gt; LOWER // LEFT RIGHT ; ( 3 ) Left-oriented : UPPER - &gt; LOWER \\ LEFT RIGHT ; ( 4 ) Downward-oriented : UPPER - &gt; LOWER \/ LEFT RIGHT ; All four kinds of replacement expressions describe a relation that maps UPPER to LOWER between LEFT and RIGHT leaving everything else unchanged .</sentence>
				<definiendum id="0">\/</definiendum>
				<definiens id="0">a relation that maps UPPER to LOWER between LEFT and RIGHT leaving everything else unchanged</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>Dynamic Time Warping , a pattern recognition technique , was proposed as a good way to match these 1This was found to be the case in the Japanese translation of the AWK manual ( Church et al. 1993 ) .</sentence>
				<definiendum id="0">AWK manual</definiendum>
				<definiens id="0">a pattern recognition technique</definiens>
			</definition>
			<definition id="1">
				<sentence>dim ( V ) is the dimension of the vector which corresponds to the occurrence count of the word .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">the dimension of the vector which corresponds to the occurrence count of the word</definiens>
			</definition>
			<definition id="2">
				<sentence>If the two were not scaled , then position i in V1 would correspond to position j in V2 where j/i is a constant .</sentence>
				<definiendum id="0">j/i</definiendum>
				<definiens id="0">a constant</definiens>
			</definition>
			<definition id="3">
				<sentence>For non-identical vectors , DTW traces the correspondences between all points in V1 and V2 ( with no penalty for deletions or insertions ) .</sentence>
				<definiendum id="0">DTW</definiendum>
				<definiens id="0">traces the correspondences between all points in V1 and V2 ( with no penalty for deletions or insertions</definiens>
			</definition>
			<definition id="4">
				<sentence>The complexity of DTW is @ ( NM ) and the complexity of the matching is O ( IJNM ) where I is the number of nouns and proper nouns in the English text , J is the number of unique words in the Chinese text , N is the occurrence count of one English word and M the occurrence count of one Chinese word .</sentence>
				<definiendum id="0">I</definiendum>
				<definiendum id="1">J</definiendum>
				<definiendum id="2">N</definiendum>
				<definiens id="0">the number of nouns and proper nouns in the English text</definiens>
				<definiens id="1">the number of unique words in the Chinese text ,</definiens>
			</definition>
			<definition id="5">
				<sentence>Evaluator E1 is a native Cantonese speaker , E2 a Mandarin speaker , and E3 a speaker of both languages .</sentence>
				<definiendum id="0">Evaluator E1</definiendum>
				<definiens id="0">a native Cantonese speaker</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>Similarly , flAT~Go~ , denotes a pair of structures for representing a scrambled object argument .</sentence>
				<definiendum id="0">flAT~Go~ ,</definiendum>
			</definition>
			<definition id="1">
				<sentence>However , for sentence ( 2 ) where the order is OSV ( the object argument is nAn additional constraint system called dominance links was added , thus giving rise to MC-TAG-DL .</sentence>
				<definiendum id="0">OSV</definiendum>
				<definiens id="0">the object argument is nAn additional constraint system called dominance links was added</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>To evaluate how well an algorithm predicted segmental structure , we used the information retrieval ( IR ) metrics described in section 3 .</sentence>
				<definiendum id="0">information retrieval</definiendum>
				<definiendum id="1">IR</definiendum>
			</definition>
			<definition id="1">
				<sentence>Cue-prosody , which encodes a combination of prosodic and cue word features , was motivated by an analysis of IR errors on our training data , as described in section 4 .</sentence>
				<definiendum id="0">Cue-prosody</definiendum>
				<definiens id="0">encodes a combination of prosodic and cue word features</definiens>
			</definition>
			<definition id="2">
				<sentence>Recall is the ratio of correctly hypothesized boundaries to target boundaries .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">the ratio of correctly hypothesized boundaries to target boundaries</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>The dialogue component uses a combination of statistical and knowledge based approaches to process dialogue acts and to maintain and to provide contextual information for the other modules of VERBMOBIL ( Maier and McGlashan , 1994 ) .</sentence>
				<definiendum id="0">dialogue component</definiendum>
				<definiens id="0">uses a combination of statistical and knowledge based approaches to process dialogue acts and to maintain and to provide contextual information for the</definiens>
			</definition>
			<definition id="1">
				<sentence>We selected the dialogue acts by examining the VERBMOBIL corpus , which consists of transliterated spoken dialogues ( German and English ) for appointment scheduling .</sentence>
				<definiendum id="0">VERBMOBIL corpus</definiendum>
				<definiens id="0">consists of transliterated spoken dialogues ( German and English</definiens>
			</definition>
			<definition id="2">
				<sentence>The method is adapted from speech recognition , where language models are commonly used to reduce the search space when determining a word that can match a part of the input signal ( Jellinek , 1990 ) .</sentence>
				<definiendum id="0">language models</definiendum>
				<definiens id="0">commonly used to reduce the search space when determining a word that can match a part of the input signal</definiens>
			</definition>
			<definition id="3">
				<sentence>-1 , s.-u ) where f are the relative frequencies computed from a training corpus and qi weighting factors with ~ '' ~qi = 1 .</sentence>
				<definiendum id="0">f</definiendum>
				<definiens id="0">the relative frequencies computed from a training corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>The x-axis represents the different diMogues , while the y-axis gives the hit rate for three predictions .</sentence>
				<definiendum id="0">x-axis</definiendum>
				<definiens id="0">the different diMogues</definiens>
			</definition>
			<definition id="5">
				<sentence>The plan recognizer uses a hierarchical depth-first left-to-right technique for dialogue act processing ( Vilain , 1990 ) .</sentence>
				<definiendum id="0">plan recognizer</definiendum>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>DATR was introduced by Evans and Gazdar ( 1989a ; 1989b ) as a simple , declarative language for representing lexical knowledge in terms of path/value equations .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">a simple , declarative language for representing lexical knowledge in terms of path/value equations</definiens>
			</definition>
			<definition id="1">
				<sentence>At the present time , DATR is probably the most widely-used formalism for representing natural language lexicons in the natural language processing ( NLP ) community .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">the most widely-used formalism for representing natural language lexicons in the natural language processing ( NLP ) community</definiens>
			</definition>
			<definition id="2">
				<sentence>DATR has been applied to problems in inflectional and derivational morphology ( Gazdar , 1992 ; Kilbury , 1992 ; Corbett and Fraser , 1993 ) , lexical semantics ( Kilgariff , 1993 ) , morphonology ( Cahill , 1993 ) , prosody ( Gibbon and Bleiching , 1991 ) and speech ( Andry et al. , 1992 ) .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">applied to problems in inflectional and derivational morphology</definiens>
			</definition>
			<definition id="3">
				<sentence>A definitional sentence N : P -- ¢ specifies a property of the node N , namely that the path P is associated with the value defined by the sequence of value descriptors ¢ .</sentence>
				<definiendum id="0">definitional sentence N</definiendum>
				<definiens id="0">P -- ¢ specifies a property of the node N , namely that the path P is associated with the value defined by the sequence of value descriptors ¢</definiens>
			</definition>
			<definition id="4">
				<sentence>For example , the definition of the Verb node gives the values of the paths ( syn cat ) and ( syn type ) directly , as verb and main , respectively .</sentence>
				<definiendum id="0">Verb node</definiendum>
				<definiens id="0">gives the values of the paths ( syn cat</definiens>
			</definition>
			<definition id="5">
				<sentence>We associate with the interpretation I = ( U , t : , F ) a partial denotation function D : DESC -'-+ ( C -+ U* ) and write \ [ d\ ] , to denote the meaning ( value ) of descriptor d in the global context c. The denotation function is defined as shown in figure 3 .</sentence>
				<definiendum id="0">denotation function</definiendum>
				<definiens id="0">associate with the interpretation I = ( U , t : , F ) a partial denotation function D : DESC -'-+ ( C -+ U* ) and write \ [ d\ ] , to denote the meaning ( value ) of descriptor d in the</definiens>
			</definition>
			<definition id="6">
				<sentence>D \ [ T/N\ ] , That is , an interpretation is a model of a DATR theory just in case ( for each global context ) the function it associates with each node respects the definition of that node within the theory .</sentence>
				<definiendum id="0">interpretation</definiendum>
				<definiens id="0">a model of a DATR theory just in case ( for each global context ) the function it associates with each node respects the definition of that node within the theory</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>Each word consists of a phone sequence and a set of sememes ( semantic symbols ) .</sentence>
				<definiendum id="0">word</definiendum>
				<definiens id="0">consists of a phone sequence and a set of sememes ( semantic symbols )</definiens>
			</definition>
</paper>

	</volume>
