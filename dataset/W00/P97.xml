<?xml version="1.0" encoding="UTF-8"?>
	<volume id="P97">

		<paper id="1010">
			<definition id="0">
				<sentence>Lexical ambiguity is a fundamental problem in natural language processing , but relatively little quantitative information is available about the extent of the problem , or about the impact that it has on specific applications .</sentence>
				<definiendum id="0">Lexical ambiguity</definiendum>
				<definiens id="0">a fundamental problem in natural language processing , but relatively little quantitative information is available about the extent of the problem , or about the impact that it has on specific applications</definiens>
			</definition>
			<definition id="1">
				<sentence>Dictionaries group senses based on part-of-speech and etymology , but as illustrated by the word review , senses can be related even though they differ in syntactic category .</sentence>
				<definiendum id="0">Dictionaries</definiendum>
				<definiens id="0">group senses based on part-of-speech and etymology , but as illustrated by the word review</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , words can differ in morphology ( authorize/authorized ) , or part-of-speech ( diabetic \ [ noun\ ] /diabetic \ [ adj\ ] ) , or in their ability to appear in a phrase ( database/data base ) .</sentence>
				<definiendum id="0">part-of-speech</definiendum>
				<definiens id="0">/diabetic \ [ adj\ ] ) , or in their ability to appear in a phrase ( database/data base</definiens>
			</definition>
			<definition id="3">
				<sentence>Our experiments compared a baseline ( no stemming ) against several different morphology routines : 1 ) a routine that grouped only inflectional variants ( plurals and tensed verb forms ) , 2 ) a routine that grouped inflectional as well as derivational variants ( e.g. , -ize , -ity ) , and 3 ) the Porter stemmer ( Porter 80 ) .</sentence>
				<definiendum id="0">Porter stemmer</definiendum>
				<definiens id="0">no stemming ) against several different morphology routines : 1 ) a routine that grouped only inflectional variants ( plurals and tensed verb forms ) , 2 ) a routine that grouped inflectional as well as derivational variants</definiens>
			</definition>
			<definition id="4">
				<sentence>A lexical phrase is a phrase that might be defined in a dictionary , such as hot line or back end .</sentence>
				<definiendum id="0">lexical phrase</definiendum>
				<definiens id="0">a phrase that might be defined in a dictionary , such as hot line or back end</definiens>
			</definition>
			<definition id="5">
				<sentence>The most striking is the interaction between phrases and morphology .</sentence>
				<definiendum id="0">most striking</definiendum>
				<definiens id="0">the interaction between phrases and morphology</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>The goal of a natural language understanding ( NLU ) system is to interpret a user 's request and respond with an appropriate action .</sentence>
				<definiendum id="0">system</definiendum>
				<definiens id="0">to interpret a user 's request and respond with an appropriate action</definiens>
			</definition>
			<definition id="1">
				<sentence>Basically , this entails augmenting the translation model with terms of the form p ( nlf ) , where n is the number of clumps generated by the formal language word f. The resulting model can be trained automatically from a bilingual corpus of English and formal language sentence pairs .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of clumps generated by the formal language word f. The resulting model can be trained automatically from a bilingual corpus of English and formal language sentence pairs</definiens>
			</definition>
			<definition id="2">
				<sentence>The alignment is a `` hidden '' quantity which is not annotated in the training data and must be inferred indirectly .</sentence>
				<definiendum id="0">alignment</definiendum>
				<definiens id="0">a `` hidden '' quantity which is not annotated in the training data and must be inferred indirectly</definiens>
			</definition>
			<definition id="3">
				<sentence>rI p ( c~I Io , ) ( 1 ) i=1 j=l e ( c ) p ( c I f ) = p ( e ( c ) I f ) 1\ ] p ( e , I fc ) ( 2 ) i=1 where p ( ni \ [ fi ) is the fertility probability of generating n i clumps by formal word f~ .</sentence>
				<definiendum id="0">rI p</definiendum>
				<definiens id="0">the fertility probability of generating n i clumps by formal word f~</definiens>
			</definition>
			<definition id="4">
				<sentence>Note that ni = L. The factorial terms combine to give an inverse multinomial coefficient which is the uniform probability distribution for the alignment A of F to C. It appears that the computation of the likelihood , which is the sum of e ( F ) ( e ( F ) + product terms , is exponential .</sentence>
				<definiendum id="0">multinomial coefficient</definiendum>
				<definiens id="0">the uniform probability distribution for the alignment A of F to C. It appears that the computation of the likelihood</definiens>
				<definiens id="1">the sum of e ( F ) ( e ( F ) + product terms</definiens>
			</definition>
			<definition id="5">
				<sentence>The Viterbi decoding algorithm ( Forney , 1973 ) is used to calculate p ( E I L , F ) from these expressions .</sentence>
				<definiendum id="0">Viterbi decoding algorithm</definiendum>
				<definiens id="0">used to calculate p ( E I L , F ) from these expressions</definiens>
			</definition>
			<definition id="6">
				<sentence>The Viterbi algorithm produces a score which is the sum over all possible clumpings for a fixed L. This score must then normalized by the exp ( -X't ( v ) z..</sentence>
				<definiendum id="0">Viterbi algorithm</definiendum>
				<definiens id="0">produces a score which is the sum over all</definiens>
			</definition>
			<definition id="7">
				<sentence>A bigram language model uses : p ( c l Y ) = p ( e ( c ) l f ) p ( el l bdy , f~ ) p ( bdy l el ( c ) , fc ) x t ( ¢ ) 1-Iv ( e , t e , -1 , fo ) i=2 where bdy is a special marker to delimit the beginning and end of the clump .</sentence>
				<definiendum id="0">bdy</definiendum>
				<definiens id="0">a special marker to delimit the beginning and end of the clump</definiens>
			</definition>
			<definition id="8">
				<sentence>The `` HW '' and `` BG '' suffixes indicate the results when p ( e\ [ f ) is computed with a headword or bigram model .</sentence>
				<definiendum id="0">HW</definiendum>
				<definiens id="0">'' and `` BG '' suffixes indicate the results when p ( e\ [ f ) is computed with a headword or bigram model</definiens>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>A collocation is a recurrent combination of words , ranging from word level to sentence level .</sentence>
				<definiendum id="0">collocation</definiendum>
				<definiens id="0">a recurrent combination of words , ranging from word level to sentence level</definiens>
			</definition>
			<definition id="1">
				<sentence>The features of collocations are defined as follows : • collocations are recurrent • collocations consist of one or several lexical units • order of units are rigid in a collocation .</sentence>
				<definiendum id="0">The features of collocations</definiendum>
				<definiens id="0">follows : • collocations are recurrent • collocations consist of one or several lexical units • order of units are rigid in a collocation</definiens>
			</definition>
			<definition id="2">
				<sentence>The probability of each possible adjacent word p ( wi ) is then : y~eq ( wi ) p ( wi ) freq ( str ) ( 1 ) At that time , the entropy of str H ( str ) is defined as : 7l H ( str ) = ~ -p ( wi ) logp ( wi ) ( 2 ) i=1 H ( str ) takes the highest value if n = freq ( str ) and 1 for all and it takes the lowest value 0 p ( wi ) = -~ wi , if n = 1 and p ( wi ) = 1 .</sentence>
				<definiendum id="0">H ( str )</definiendum>
				<definiens id="0">takes the highest value if n = freq ( str ) and 1 for all and it takes the lowest value 0 p ( wi ) = -~ wi , if n = 1 and p ( wi ) = 1</definiens>
			</definition>
</paper>

		<paper id="1071">
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>suffÉx ) u , f~y ( resp , g~y ) denotes the function which transforms x into y : as x = uv , and as y = ut , f~y substitutes a final v with a final t. ~ denotes the empty string .</sentence>
				<definiendum id="0">g~y )</definiendum>
			</definition>
			<definition id="1">
				<sentence>At the end of the learning stage , we have in hand a set A = { Ai } of functions exchanging suffixes or prefixes in the graphemic domain , and for each Ai in A : ( i ) a statistical measure Pi of its productivity , defined as the likelihood that the transform of a lexical item be another lexieal item : Pi = I { x e dom ( di ) and Ai ( x ) E 12 } 1 i dom ( &amp; ) l ( 4 ) ( ii ) a set { Bi , j } , j G { 1 ... hi } of correlated functions in the phonemic domain , and a statistical measure Pi , j of their conditional productivity , i.e. of the likelihood that the phonetic alternation Bi , j correlates with Ai .</sentence>
				<definiendum id="0">e dom</definiendum>
				<definiens id="0">hand a set A = { Ai } of functions exchanging suffixes or prefixes in the graphemic domain , and for each Ai in A : ( i ) a statistical measure Pi of its productivity , defined as the likelihood that the transform of a lexical item be another lexieal item : Pi = I { x</definiens>
			</definition>
			<definition id="2">
				<sentence>uce ( in terms of per word correctness , silence , and precision ) of various other pronunciation systems , namely PRONOUNCE ( Dedina and Nusbaum , 1991 ) , DEC ( Torkolla , 1993 ) , SMPA ( Yvon , 1 ! )</sentence>
				<definiendum id="0">uce (</definiendum>
				<definiens id="0">in terms of per word correctness , silence , and precision</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>Two common measures of performance are recall and precision , where recall is defined as the percent of words in the hand-segmented text identified by the segmentation algorithm , and precision is defined as the percentage of words returned by the algorithm that also occurred in the hand-segmented text in the same position .</sentence>
				<definiendum id="0">recall</definiendum>
				<definiendum id="1">precision</definiendum>
				<definiens id="0">the percent of words in the hand-segmented text identified by the segmentation algorithm , and</definiens>
				<definiens id="1">the percentage of words returned by the algorithm that also occurred in the hand-segmented text in the same position</definiens>
			</definition>
			<definition id="1">
				<sentence>A very simple initial segmentation for Chinese is to consider each character a distinct word .</sentence>
				<definiendum id="0">Chinese</definiendum>
				<definiens id="0">to consider each character a distinct word</definiens>
			</definition>
			<definition id="2">
				<sentence>The Chinese segmenter CHSEG developed at the Computing Research Laboratory at New Mexico State University is a complete system for highaccuracy Chinese segmentation ( Jin , 1994 ) .</sentence>
				<definiendum id="0">Chinese segmenter CHSEG</definiendum>
			</definition>
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>Our translation model consists of the hidden parameters A + and A- , and likelihood ratios L ( u , v ) .</sentence>
				<definiendum id="0">translation model</definiendum>
				<definiens id="0">consists of the hidden parameters A + and A- , and likelihood ratios L ( u , v )</definiens>
			</definition>
			<definition id="1">
				<sentence>( u , v ) k ( u. , , ) = total number of links in the bitext = Pr ( mutual translations I co-occurrence ) = Pr ( link I co-occurrence ) = Pr ( link \ [ co-occurrence of mutual translations ) = Pr ( link I co-occurrence of not mutual translations ) = Pr ( kin , p ) , where k has a binomial distribution with parameters n and p N.B. : k + and ) ~need not sum to 1 , because they are conditioned on different events .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">total number of links in the bitext = Pr ( mutual translations I co-occurrence ) = Pr ( link I co-occurrence ) = Pr ( link \ [ co-occurrence of mutual translations ) = Pr ( link I co-occurrence of not mutual translations ) = Pr ( kin , p )</definiens>
			</definition>
			<definition id="2">
				<sentence>Let B ( kln , p ) denote the probability that k links are observed out of n co-occurrences , where k has a binomial distribution with parameters n and p. Then the probability that u and v are linked k ( u , v ) times out of n ( u , v ) co-occurrences is a mixture of two binomials : Pr ( k ( u , v ) ln ( u , v ) , A + , A- ) = ( 2 ) = rB ( k ( u , v ) ln ( u , v ) , A + ) ÷ ( 1-r ) B ( k ( u , v ) ln ( u , v ) , A- ) One more variable allows us to express r in terms of A + and A : Let A be the probability that an arbitrary co-occuring pair of word tokens will be linked , regardless of whether they are mutual translations .</sentence>
				<definiendum id="0">B</definiendum>
				<definiendum id="1">k</definiendum>
				<definiendum id="2">co-occurrences</definiendum>
				<definiens id="0">a mixture of two binomials : Pr ( k ( u , v ) ln ( u , v</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>Categorical data analysis is the area of statistics that addresses categorical statistical variable : variables whose values are one of a set of categories .</sentence>
				<definiendum id="0">Categorical data analysis</definiendum>
				<definiens id="0">the area of statistics that addresses categorical statistical variable : variables whose values are one of a set of categories</definiens>
			</definition>
			<definition id="1">
				<sentence>Marginal totals ( sums for all values of some variables ) of the observed counts are used to estimate the parameters of the loglinear model ; the model in turn delivers estimated expected cell counts , which are smoother than the original cell counts .</sentence>
				<definiendum id="0">Marginal totals</definiendum>
				<definiens id="0">sums for all values of some variables ) of the observed counts are used to estimate the parameters of the loglinear model ; the model in turn delivers estimated expected cell counts</definiens>
			</definition>
			<definition id="2">
				<sentence>the term uzii ) denotes the deviation of the mean of the expected cell counts with value i of the first variable from the grand mean u. Similarly , the term Ul2 ( ij ) denotes the deviation of the mean of the expected cell counts with value i of the first variable and value j of the second variable from the grand mean u. In other words , ttl2 ( ij ) represents the combined effect of the values i and j for the first and second variables on the logarithms of the expected cell counts .</sentence>
				<definiendum id="0">term uzii )</definiendum>
				<definiendum id="1">ij )</definiendum>
				<definiens id="0">denotes the deviation of the mean of the expected cell counts with value i of the first variable from the grand mean u. Similarly , the term Ul2 ( ij ) denotes the deviation of the mean of the expected cell counts with value i of the first variable and value j of the second variable from the grand mean u. In other words</definiens>
				<definiens id="1">represents the combined effect of the values i and j for the first and second variables on the logarithms of the expected cell counts</definiens>
			</definition>
			<definition id="3">
				<sentence>In each experiment , performance IMutu ' , d Information provides an estimate of the magnitude of the ratio t ) ctw ( .</sentence>
				<definiendum id="0">IMutu</definiendum>
				<definiendum id="1">Information</definiendum>
				<definiens id="0">provides an estimate of the magnitude of the ratio t</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>SYSTRAN is a system with a development history dating back to the seventies .</sentence>
				<definiendum id="0">SYSTRAN</definiendum>
			</definition>
			<definition id="1">
				<sentence>Systran does not offer a translation of a word if it is in the lexicon with an inappropriate part of speech .</sentence>
				<definiendum id="0">Systran</definiendum>
				<definiens id="0">does not offer a translation of a word if it is in the lexicon with an inappropriate part of speech</definiens>
			</definition>
			<definition id="2">
				<sentence>T1 ( M\ [ Springpferd\ ] ) c Personal Tr .</sentence>
				<definiendum id="0">T1</definiendum>
				<definiens id="0">M\ [ Springpferd\ ] ) c Personal Tr</definiens>
			</definition>
			<definition id="3">
				<sentence>German Assistant contains a wide variety of readings although it scored badly in our tests .</sentence>
				<definiendum id="0">Assistant</definiendum>
				<definiens id="0">contains a wide variety of readings although it scored badly in our tests</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>LR is a statistical technique for modeling a binary response variable by a linear combination of one or more predictor variables , using a logit link function : g ( r ) = log ( r~ ( 1 zr ) ) and modeling variance with a binomial random variable , i.e. , the dependent variable log ( r~ ( 1 ,7 ) ) is modeled as a linear combination of the independent variables .</sentence>
				<definiendum id="0">LR</definiendum>
				<definiens id="0">a statistical technique for modeling a binary response variable by a linear combination of one or more predictor variables</definiens>
			</definition>
			<definition id="1">
				<sentence>The model has the form g ( , 'r ) = zi,8 where , 'r is the estimated response probability ( in our case the probability of a particular facet value ) , xi is the feature vector for text i , and ~q is the weight vector which is estimated from the matrix of feature vectors .</sentence>
				<definiendum id="0">'r</definiendum>
				<definiendum id="1">xi</definiendum>
				<definiendum id="2">~q</definiendum>
				<definiens id="0">the estimated response probability ( in our case the probability of a particular facet value ) ,</definiens>
				<definiens id="1">the weight vector which is estimated from the matrix of feature vectors</definiens>
			</definition>
			<definition id="2">
				<sentence>For binary decisions , such as determining whether or not a text is : NARRATIVE , the output layer consists of one sigmoidal output unit : for polytomous decisions , it consists of four ( BRow ) or six ( GENRE ) softmax units ( which implement a multinomial response model } ( Rumelhart et al. , 1995 ) .</sentence>
				<definiendum id="0">BRow</definiendum>
				<definiendum id="1">GENRE</definiendum>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>39 Letting W denote a vocabulary ( a set of words ) , and w denote a random variable representing any word in it , for each category ci ( i = 1 , ... , n ) , we define its word-based distribution P ( wIci ) as a histogram type of distribution over W. ( The number of free parameters of such a distribution is thus I W\ [ 1 ) .</sentence>
				<definiendum id="0">vocabulary</definiendum>
				<definiendum id="1">word-based distribution P ( wIci</definiendum>
				<definiens id="0">a set of words ) , and w denote a random variable representing any word in it , for each category ci ( i = 1 , ... , n )</definiens>
			</definition>
			<definition id="1">
				<sentence>f ( kjlc , ) + 0.5 , P ( k3lc~ ) = f-~ -- ~-~ x m ( 4 ) where f ( kjlci ) is the frequency of the cluster kj in ci , f ( ci ) is the total frequency of clusters in cl , and m is the total number of clusters .</sentence>
				<definiendum id="0">f ( kjlci )</definiendum>
				<definiendum id="1">m</definiendum>
				<definiens id="0">the frequency of the cluster kj in ci</definiens>
				<definiens id="1">the total frequency of clusters in cl</definiens>
				<definiens id="2">the total number of clusters</definiens>
			</definition>
			<definition id="2">
				<sentence>Q ( wlki ) ; wek i , P ( wlkj ) 0 ; w ¢ ( 5 ) where w denotes a random variable representing any word in the vocabulary .</sentence>
				<definiendum id="0">w</definiendum>
				<definiens id="0">a random variable representing any word in the vocabulary</definiens>
			</definition>
			<definition id="3">
				<sentence>That is , we view a document as a sequence of words , d= wl , `` `` , WN ( 7 ) where wt ( t = 1 , .</sentence>
				<definiendum id="0">wt</definiendum>
				<definiens id="0">a sequence of words</definiens>
			</definition>
			<definition id="4">
				<sentence>, N ) represents a word .</sentence>
				<definiendum id="0">N )</definiendum>
				<definiens id="0">a word</definiens>
			</definition>
			<definition id="5">
				<sentence>( 9 ) P ( wlkj ) = O ; w~k~ , 41 where Ikjl denotes the number of elements belonging to kj , then we will get the same classification result as in HCM .</sentence>
				<definiendum id="0">Ikjl</definiendum>
				<definiens id="0">the number of elements belonging to kj</definiens>
			</definition>
			<definition id="6">
				<sentence>In such a case , the likelihood value for each category ci becomes : L ( dlc , ) = I-I ; : x ( P ( ktlci ) x P~wtlkt ) ) = 1-It=~ P ( ktlci ) x l-It=lP ( Wtlkt ) , ( lo ) where kt is the cluster corresponding to wt .</sentence>
				<definiendum id="0">kt</definiendum>
				<definiens id="0">the cluster corresponding to wt</definiens>
			</definition>
			<definition id="7">
				<sentence>Letting 7 ( 0 _ &lt; 7 &lt; 1 ) be a predetermined threshold value , if the following inequality holds : f ( wlci ) &gt; 7 , ( t2 ) f ( w ) then we assign w to ki , the cluster related to ci , where f ( wlci ) denotes the frequency of the word w in category ci , and f ( w ) denotes the total frequency ofw .</sentence>
				<definiendum id="0">t2 ) f</definiendum>
				<definiendum id="1">f ( w )</definiendum>
				<definiens id="0">the frequency of the word w in category ci , and</definiens>
			</definition>
			<definition id="8">
				<sentence>There are two common methods for statistical estimation , the maximum likelihood estimation method 5We calculate the probabilities by employing the maximum likelihood estimator : / ( kAc0 ( 13 ) P ( kilci ) f ( ci ) ' where f ( kj\ ] cl ) is the frequency of the cluster kj in ci , and f ( cl ) is the total frequency of clusters in el .</sentence>
				<definiendum id="0">f ( cl )</definiendum>
				<definiens id="0">the frequency of the cluster kj in ci , and</definiens>
			</definition>
			<definition id="9">
				<sentence>of parameters WBM O ( n. IWl ) HCM O ( n. m ) FMM o ( Ikl+n 'm ) HCM , and WBM , where IW\ ] is the size of a vocabulary , Ikl is the sum of the sizes of word clusters m ( i.e. , Ikl -E~=I Ikil ) , n is the number of categories , and m is the number of clusters .</sentence>
				<definiendum id="0">FMM o</definiendum>
				<definiendum id="1">IW\ ]</definiendum>
				<definiendum id="2">Ikl</definiendum>
				<definiendum id="3">n</definiendum>
				<definiendum id="4">m</definiendum>
				<definiens id="0">the sum of the sizes of word clusters m</definiens>
				<definiens id="1">the number of categories , and</definiens>
				<definiens id="2">the number of clusters</definiens>
			</definition>
			<definition id="10">
				<sentence>( 3 ) With our current method of creating clusters , as the threshold 7 approaches 0 , FMM behaves much like WBM and it does not enjoy the effects of clustering at all ( the number of parameters is as large l°In micro-averaging ( Lewis and Ringuette , 1994 ) , precision is defined as the percentage of classified documents in all categories which are correctly classified .</sentence>
				<definiendum id="0">FMM</definiendum>
				<definiendum id="1">precision</definiendum>
				<definiens id="0">behaves much like WBM and it does not enjoy the effects of clustering at all ( the number of parameters</definiens>
			</definition>
			<definition id="11">
				<sentence>Recall is defined as the percentage of the total documents in all categories which are correctly classified .</sentence>
				<definiendum id="0">Recall</definiendum>
			</definition>
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>Inference One other class of bridging DDs includes cases based on a relation of reason , cause , consequence , or set-members between an anchor ( previous NP ) and the DD ( as in Republicans/Democratics -the two sides , and last week 's earthquake -the suffering people are going through ) .</sentence>
				<definiendum id="0">DD</definiendum>
				<definiens id="0">cases based on a relation of reason , cause , consequence , or set-members between an anchor</definiens>
			</definition>
			<definition id="1">
				<sentence>In our corpus , 16 % ( 7/43 ) of the cases based on events are direct nominalisations ( for instance , changes were proposed -the proposals ) , and another 16 % were based on semantic relations holding between nouns and verbs ( such as borrou~ , ed -the loan ) .</sentence>
				<definiendum id="0">direct nominalisations</definiendum>
				<definiens id="0">for instance , changes were proposed -the proposals ) , and another 16 % were based on semantic relations holding between nouns and verbs ( such as borrou~ , ed -the loan )</definiens>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>The proposed ~ammars ( CCG-GTRC ) admit lexical type-raised categories ( LTRC ) of the form `` 1 '' / ( T\a ) or'l'\ ( T/a ) where T is a variable over categories and a is a constant category ( Const ) .</sentence>
				<definiendum id="0">CCG-GTRC ) admit lexical type-raised categories</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">a variable over categories</definiens>
			</definition>
			<definition id="1">
				<sentence>T/ ( TkAkB\C\D ) '' CCG-GTRC is defined below where g , ta and ~a , rc represent the classes of the instances of CCG-Std and CCGGTRC , respectively : Definition 1 Gatrc is the collection of G 's ( extension of a G E G , ta ) such that : l. For the lexical function f of G ( from terminals to sets of categories ) , if a E f ( a ) , f ' may additionally include { ( a , T/ ( T\a ) ) , ( a , T\ ( T/a ) ) } .</sentence>
				<definiendum id="0">T/ ( TkAkB\C\D ) '' CCG-GTRC</definiendum>
				<definiendum id="1">( a , T\ ( T/a</definiendum>
				<definiens id="0">defined below where g , ta and ~a , rc represent the classes of the instances of CCG-Std and CCGGTRC , respectively : Definition 1 Gatrc is the collection of G 's ( extension of a G E G , ta ) such that : l. For the lexical function f of G ( from terminals to sets of categories</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>An interacting issue is the granularity of meaning of derived forms .</sentence>
				<definiendum id="0">interacting issue</definiendum>
			</definition>
			<definition id="1">
				<sentence>2 Multiple schemata may apply to a single compound : for example , cotton bag is an instantiation of the made-of schema , the non-derived-purposepatient schema and also the general-nn schema .</sentence>
				<definiendum id="0">cotton bag</definiendum>
				<definiens id="0">an instantiation of the made-of schema , the non-derived-purposepatient schema and also the general-nn schema</definiens>
			</definition>
			<definition id="2">
				<sentence>The predicate made-of is to be interpreted as material constituency ( e.g. Link ( 1983 ) ) .</sentence>
				<definiendum id="0">predicate made-of</definiendum>
				<definiens id="0">to be interpreted as material constituency</definiens>
			</definition>
			<definition id="3">
				<sentence>We use the following estimate for productivity ( adapted from Briscoe and Copestake ( 1996 ) ) : M+I Prod ( cmp-schema ) N ( where N is the number of pairs of senses which match the schema input and M is the number of attested two-noun output forms -we ignore compounds with more than two nouns for simplicity ) .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">the number of pairs of senses which match the schema input and</definiens>
				<definiens id="1">the number of attested two-noun output forms -we ignore compounds with more than two nouns for simplicity )</definiens>
			</definition>
			<definition id="4">
				<sentence>If a LF contains an underspecified element ( e.g. , arising from general-nn ) , this must be instantiated by pragmatics from the discourse context .</sentence>
				<definiendum id="0">LF</definiendum>
				<definiens id="0">contains an underspecified element</definiens>
			</definition>
			<definition id="5">
				<sentence>Building an SDRS invoh'es computing a rhetorical relation between the representation of the current clause and the SDRS built so far .</sentence>
				<definiendum id="0">Building an SDRS invoh'es</definiendum>
				<definiens id="0">computing a rhetorical relation between the representation of the current clause and the SDRS built so far</definiens>
			</definition>
			<definition id="6">
				<sentence>DICE specifies how various background knowledge resources interact to provide clues about which rhetorical relation holds .</sentence>
				<definiendum id="0">DICE</definiendum>
			</definition>
			<definition id="7">
				<sentence>Update ( r , a. 3 ) is an SDRS , which includes ( a ) the discourse context r , plus ( b ) the new information '3 .</sentence>
				<definiendum id="0">Update</definiendum>
				<definiens id="0">an SDRS , which includes ( a ) the discourse context r</definiens>
			</definition>
			<definition id="8">
				<sentence>n.u.B mary ( x ) , skirt ( y ) ~ bag ( z ) , cotton ( w ) , put ( e~ , x , y , z ) , hold ( e2 , to ) , t~ - &lt; n In words , the conditions in '3 require the object denoted by the definite description to be linked by some 'bridging ' relation B ( possibly identity , cf. van der Sandt ( 1992 ) ) to an object v identified in the discourse context ( Asher and Lascarides .</sentence>
				<definiendum id="0">n.u.B mary</definiendum>
				<definiens id="0">the conditions in '3 require the object denoted by the definite description to be linked by some 'bridging ' relation B ( possibly identity , cf. van der Sandt ( 1992 ) ) to an object v identified in the discourse context ( Asher and Lascarides</definiens>
			</definition>
			<definition id="9">
				<sentence>the values of u and B are computed as a byproduct of SDRT'5 Update function ( cf. Hobbs ( 1979 ) ) ; one specifies v and B by inferring the relevant new semantic content arising from R~s coherence constraints , where R is the rhetorical relation inferred via the DICE axioms .</sentence>
				<definiendum id="0">R</definiendum>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>The feature telicity distinguishes between culminative events ( +t ) and nonculminative events ( -t ) .</sentence>
				<definiendum id="0">culminative events</definiendum>
				<definiens id="0">+t ) and nonculminative events ( -t )</definiens>
			</definition>
			<definition id="1">
				<sentence>categories features examples \ [ -d\ ] \ [ +d , +a\ ] \ [ +d , -a , -p\ ] \ [ +d , -a , +p , +t\ ] \ [ +d , -a , +p , -t , -g\ ] \ [ +d , -a , +p , -t , +g\ ] aru ( be ) , sobieru ( se ) , sonzaisuru ( e=isO hirameku ( flash ) , mikakeru ( notice ) suwaru ( sit down ) , tatu ( stand up ) korosu ( kill ) , Urn ( put on~wear ) , ake , ' ( open ) aruku ( walk ) , in ( say ) , utau ( sing ) kusaru ( turn sour ) , takamaru ( become high ) 353 Atomic adverbs make any events instantaneous , such as satto , ponto , gatatto , potarito , syunkan , etc. , which express instantaneous sound emission or an instant .</sentence>
				<definiendum id="0">aru</definiendum>
				<definiendum id="1">e=isO hirameku</definiendum>
				<definiendum id="2">Urn</definiendum>
				<definiendum id="3">utau ( sing</definiendum>
				<definiens id="0">such as satto , ponto , gatatto , potarito , syunkan , etc. , which express instantaneous sound emission or an instant</definiens>
			</definition>
			<definition id="2">
				<sentence>End state modifiers express the consequent state of events , such as mapputatuni ( into two exact halves ) , konagonani ( into pieces ) , pechankoni ( be fiat ) , barabarani ( come apart ) , etc .</sentence>
				<definiendum id="0">pechankoni</definiendum>
			</definition>
			<definition id="3">
				<sentence>The form hajimeru ( begin ) can follow the verbs which have process ( -bp ) and takes up the start time of the process .</sentence>
				<definiendum id="0">form hajimeru</definiendum>
				<definiens id="0">-bp ) and takes up the start time of the process</definiens>
			</definition>
			<definition id="4">
				<sentence>Each item in the Japanese Co-occurrence Dictionary consists of a governing word .</sentence>
				<definiendum id="0">Co-occurrence Dictionary</definiendum>
				<definiens id="0">consists of a governing word</definiens>
			</definition>
			<definition id="5">
				<sentence>Brent ( Brent , 1991 ) discusses an implemented program that automatically classifies verbs into two groups , stative vs. non-stative , on the basis of their syntactic contexts .</sentence>
				<definiendum id="0">Brent</definiendum>
				<definiens id="0">discusses an implemented program that automatically classifies verbs into two groups , stative vs. non-stative , on the basis of their syntactic contexts</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>The TaAINS corpus consists of about six and half hours of speech .</sentence>
				<definiendum id="0">TaAINS corpus</definiendum>
				<definiens id="0">consists of about six and half hours of speech</definiens>
			</definition>
			<definition id="1">
				<sentence>The second term Pr ( WP ) is the factor due to the language model .</sentence>
				<definiendum id="0">WP )</definiendum>
				<definiens id="0">the factor due to the language model</definiens>
			</definition>
			<definition id="2">
				<sentence>We rewrite Pr ( WP ) as Pr ( WI , NPI , N ) , where N is the number of words in the sequence .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of words in the sequence</definiens>
			</definition>
			<definition id="3">
				<sentence>For each leaf node , it looks for the question to ask of the context such that splitting the node into two leaf nodes results in the biggest decrease in impurity , where tile impurity measures how well each leaf predicts the events in the node .</sentence>
				<definiendum id="0">tile impurity</definiendum>
				<definiens id="0">measures how well each leaf predicts the events in the node</definiens>
			</definition>
			<definition id="4">
				<sentence>Silence , as well as other acoustic information , can also give evidence as to whether an intonational phrase , speech repair , or editing term occurred .</sentence>
				<definiendum id="0">Silence</definiendum>
				<definiens id="0">acoustic information , can also give evidence as to whether an intonational phrase , speech repair , or editing term occurred</definiens>
			</definition>
			<definition id="5">
				<sentence>Example 11 ( d92a-2.1 utt95 ) will take a total of um let 's see total of sof 7 hours reparandum | et reparandum l iV iV The language model considers all possible interpretations ( at least those that do not get pruned ) and assigns a probability to each .</sentence>
				<definiendum id="0">language model</definiendum>
				<definiens id="0">considers all possible interpretations ( at least those that do not get pruned ) and assigns a probability to each</definiens>
			</definition>
			<definition id="6">
				<sentence>The recall rate of X is the number of X events that were correctly determined by the algorithm over the number of occurrences of X. The precision rate is the number of X events that were correctly determined over the number of times that the algorithm guessed X. The error rate is the number of X events that the algorithm missed plus the number of X events that it incorrectly guessed as occurring over the number of X events .</sentence>
				<definiendum id="0">recall rate of X</definiendum>
				<definiendum id="1">precision rate</definiendum>
				<definiens id="0">the number of X events that were correctly determined by the algorithm over the number of occurrences</definiens>
				<definiens id="1">the number of X events that were correctly determined over the number of times</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>QuickSet is a distributed system consisting of a collection of agents that communicate through the Open Agent Architecture 4 ( Cohen et al 1994 ) .</sentence>
				<definiendum id="0">QuickSet</definiendum>
				<definiens id="0">a distributed system consisting of a collection of agents that communicate through the Open Agent</definiens>
			</definition>
			<definition id="1">
				<sentence>Unification is an operation that determines the consistency of two pieces of partial information , and if they are consistent combines them into a single result .</sentence>
				<definiendum id="0">Unification</definiendum>
				<definiens id="0">an operation that determines the consistency of two pieces of partial information , and if they are consistent combines them into a single result</definiens>
			</definition>
			<definition id="2">
				<sentence>Gesture also compensates for errors in speech recognition .</sentence>
				<definiendum id="0">Gesture</definiendum>
				<definiens id="0">compensates for errors in speech recognition</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>A terminal node can be either non-empty ( Figure la ) , corresponding to a basic discourse unit ( usually a clause ) , or empty .</sentence>
				<definiendum id="0">terminal node</definiendum>
				<definiens id="0">corresponding to a basic discourse unit ( usually a clause ) , or empty</definiens>
			</definition>
			<definition id="1">
				<sentence>Substitution unifies the root of a substitution structure with an empty node in the discourse tree that serves as a substitution site .</sentence>
				<definiendum id="0">Substitution</definiendum>
				<definiens id="0">unifies the root of a substitution structure with an empty node in the discourse tree that serves as a substitution site</definiens>
			</definition>
			<definition id="2">
				<sentence>Figure 4A ( i ) shows the elementary tree corresponding to sentence 2a ( `` On the one hand ... '' ) : the interpretation of `` John is very generous '' I corresponds to the left daughter labelled `` a '' .</sentence>
				<definiendum id="0">)</definiendum>
				<definiens id="0">the elementary tree corresponding to sentence 2a ( `` On the one hand ... ''</definiens>
			</definition>
			<definition id="3">
				<sentence>The contrast introduced by `` On the other hand '' in sentence 5 ( b ) leads to the auxiliary tree shown in Figure 4B ( ii ) , where T stands for the elementary tree corresponding to the interpretation of `` suppose ... '' .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">stands for the elementary tree corresponding to the interpretation of `` suppose ... ''</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>Upper-case variables are Prolog-like variables that become instantiated to specific terms within the proof , and generally their scope is the entire premise .</sentence>
				<definiendum id="0">Upper-case variables</definiendum>
				<definiens id="0">Prolog-like variables that become instantiated to specific terms within the proof</definiens>
			</definition>
			<definition id="1">
				<sentence>3Note that we refer to noncompound terms as 'literal ' or 'atomic ' terms because they are atomic from the point of view of the glue language , even though these terms are in fact of the form S ' , ~ M , where S is an expression over LFG structures and M is a type-r expression in the meaning language .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">an expression over LFG structures</definiens>
			</definition>
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>The Information Gain of feature f is measured by computing the difference in uncertainty ( i.e. entropy ) between the situations without and with knowledge of the value of that feature ( Equation 3 ) .</sentence>
				<definiendum id="0">Information Gain of feature f</definiendum>
			</definition>
			<definition id="1">
				<sentence>w\ ] = H ( C ) ~-\ ] ~ev , P ( v ) x H ( Clv ) si ( f ) ( 3 ) si ( f ) = Z P ( v ) log 2 P ( v ) ( 4 ) vEVs Where C is the set of class labels , V f is the set of values for feature f , and H ( C ) = ~cec P ( c ) log 2 P ( e ) is the entropy of the class labels .</sentence>
				<definiendum id="0">C</definiendum>
				<definiendum id="1">V f</definiendum>
				<definiens id="0">the set of class labels</definiens>
				<definiens id="1">the set of values for feature f</definiens>
				<definiens id="2">the entropy of the class labels</definiens>
			</definition>
			<definition id="2">
				<sentence>f ( X ) stands for the frequency of pattern X in the training set .</sentence>
				<definiendum id="0">f ( X )</definiendum>
				<definiens id="0">the frequency of pattern X in the training set</definiens>
			</definition>
			<definition id="3">
				<sentence>If we classify pattern X by looking at its nearest neighbors , we are in fact estimating the probability P ( classlX ) , by looking at the relative frequency of the class in the set defined by simk ( X ) , where slink ( X ) is a function from X to the set of most similar patterns present in the training data 3 .</sentence>
				<definiendum id="0">slink ( X )</definiendum>
				<definiens id="0">classify pattern X by looking at its nearest neighbors , we are in fact estimating the probability P ( classlX ) , by looking at the relative frequency of the class in the set defined by simk ( X )</definiens>
			</definition>
			<definition id="4">
				<sentence>A bucket is defined by a particular number of mismatches with respect to pattern X. Each bucket can further be decomposed into a number of schemata characterized by the position of a wildcard ( i.e. a mismatch ) .</sentence>
				<definiendum id="0">bucket</definiendum>
				<definiens id="0">a particular number of mismatches with respect to pattern X. Each bucket can further be decomposed into a number of schemata characterized by the position of a wildcard ( i.e. a mismatch )</definiens>
			</definition>
			<definition id="5">
				<sentence>The importance of the 2~back-off terms is specified using only F parameters -- the IG weights- , where F is the number of features .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">the number of features</definiens>
			</definition>
			<definition id="6">
				<sentence>The MBL framework is a convenient way to further experiment with even more complex conditioning events , e.g. with semantic labels added as features .</sentence>
				<definiendum id="0">MBL framework</definiendum>
				<definiens id="0">a convenient way to further experiment with even more complex conditioning events</definiens>
			</definition>
			<definition id="7">
				<sentence>Abstraction Considered Harmful : Lazy Learning of Language Processing .</sentence>
				<definiendum id="0">Abstraction Considered Harmful</definiendum>
			</definition>
			<definition id="8">
				<sentence>MBT : A Memory-Based Part of Speech Tagger Generator .</sentence>
				<definiendum id="0">MBT</definiendum>
			</definition>
			<definition id="9">
				<sentence>IGTree : Using Trees for Compression and Classification in Lazy Learning Algorithms .</sentence>
				<definiendum id="0">IGTree</definiendum>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>Class-based methods ( Brown et al. , 1992 ; Pereira , Tishby , and Lee , 1993 ; Resnik , 1992 ) cluster words into classes of similar words , so that one can base the estimate of a word pair 's probability on the averaged cooccurrence probability of the classes to which the two words belong .</sentence>
				<definiendum id="0">Class-based methods</definiendum>
				<definiens id="0">base the estimate of a word pair 's probability on the averaged cooccurrence probability of the classes to which the two words belong</definiens>
			</definition>
			<definition id="1">
				<sentence>In what follows , we use subscript i for the i th element of a pair ; thus P ( w21wi ) is the conditional probability ( or rather , some empirical estimate , the true probability being unknown ) that a pair has second element w2 given that its first element is wl ; and P ( wllw2 ) denotes the probability estimate , according to the base language model , that wl is the first word of a pair given that the second word is w2 .</sentence>
				<definiendum id="0">P ( wllw2 )</definiendum>
				<definiens id="0">the i th element of a pair</definiens>
				<definiens id="1">the conditional probability ( or rather , some empirical estimate , the true probability being unknown ) that a pair has second element w2 given that its first element is wl</definiens>
				<definiens id="2">the probability estimate , according to the base language model , that wl is the first word of a pair given that the second word is w2</definiens>
			</definition>
			<definition id="2">
				<sentence>P ( w ) denotes the base estimate for the unigram probability of word w. A similarity-based language model consists of three parts : a scheme for deciding which word pairs require a similarity-based estimate , a method for combining information from similar words , and , of course , a function measuring the similarity between words .</sentence>
				<definiendum id="0">P ( w )</definiendum>
				<definiens id="0">the base estimate for the unigram probability of word w. A similarity-based language model consists of three parts : a scheme for deciding which word pairs require a similarity-based estimate , a method for combining information from similar words , and , of course , a function measuring the similarity between words</definiens>
			</definition>
			<definition id="3">
				<sentence>The MLE for the probability of a word pair ( Wl , w2 ) , conditional on the appearance of word wl , is simply PML ( W2\ [ wl ) -c ( wl , w2 ) ( 1 ) c ( i ) where c ( wl , w2 ) is the frequency of ( wl , w2 ) in the training corpus and c ( wl ) is the frequency of wt .</sentence>
				<definiendum id="0">MLE for the probability of a word pair</definiendum>
				<definiendum id="1">PML</definiendum>
				<definiens id="0">the frequency of ( wl , w2 ) in the</definiens>
			</definition>
			<definition id="4">
				<sentence>However , PML is zero for any unseen word pair , which leads to extremely inaccurate estimates for word pair probabilities .</sentence>
				<definiendum id="0">PML</definiendum>
				<definiens id="0">zero for any unseen word pair , which leads to extremely inaccurate estimates for word pair probabilities</definiens>
			</definition>
			<definition id="5">
				<sentence>The discounting approach is the one adopted by Katz ( 1987 ) : /Pd ( w2\ ] wx ) C ( Wl , w2 ) &gt; 0 /5 ( w2lwl ) = \ [ o~ ( wl ) Pr ( w2\ [ wl ) o.w. ( 2 ) where Pd represents the Good-Turing discounted estimate ( Katz , 1987 ) for seen word pairs , and Pr denotes the model for probability redistribution among the unseen word pairs .</sentence>
				<definiendum id="0">Pd</definiendum>
				<definiendum id="1">Pr</definiendum>
				<definiens id="0">the Good-Turing discounted estimate ( Katz , 1987 ) for seen word pairs</definiens>
				<definiens id="1">the model for probability redistribution among the unseen word pairs</definiens>
			</definition>
			<definition id="6">
				<sentence>c~ ( wl ) is a normalization factor .</sentence>
				<definiendum id="0">c~ ( wl )</definiendum>
				<definiens id="0">a normalization factor</definiens>
			</definition>
			<definition id="7">
				<sentence>The L1 norm is defined as n ( wi , wl ) : ~ IP ( w2lwj P ( w21w'Jl .</sentence>
				<definiendum id="0">L1 norm</definiendum>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>( -\ [ VBN IN , : JJ JJS JJR\ ] ) + &lt; VBN &gt; ; This rule states that a tag past participle ( VBN ) is very compatible ( 10.0 ) with a left context consisting of a % vauxiliar % ( previously defined macro which includes all forms of `` have '' and `` be '' ) provided that all the words in between do n't have any of the tags in the set \ [ VBN IN , : JJ JJS J JR\ ] .</sentence>
				<definiendum id="0">( -\ [ VBN IN</definiendum>
			</definition>
			<definition id="1">
				<sentence>Let PA ( X ) be the partition of X induced by the values of attribute A. The average information of such partition is defined as follows : I ( PA ( X ) ) = ~ , p ( X , a ) log , .</sentence>
				<definiendum id="0">PA ( X )</definiendum>
				<definiens id="0">the partition of X induced by the values of attribute</definiens>
			</definition>
			<definition id="2">
				<sentence>p ( X , a ) , aEPa ( X ) where p ( X. a ) is the probability for an element of X belonging to the set a which is the subset of X whose examples have a certain value for the attribute .4 , and it is estimated bv the ratio ~ This average • IXl ' information measure reflects the randomness of distribution of the elements of X between the classes of the partition induced by .4 .</sentence>
				<definiendum id="0">p ( X</definiendum>
				<definiendum id="1">p ( X. a )</definiendum>
				<definiens id="0">the probability for an element of X belonging to the set a which is the subset of X whose examples have a certain value for the attribute .4 , and it is estimated bv the ratio ~ This average • IXl ' information measure reflects the randomness of distribution of the elements of X between the classes of the partition induced by .4</definiens>
			</definition>
			<definition id="3">
				<sentence>PB ( X ) ) = \ [ ( Ps ( X ) iPA ( X ) ) + I ( PA ( X ) IPB ( X ) ) is a distance .</sentence>
				<definiendum id="0">PB</definiendum>
				<definiens id="0">a distance</definiens>
			</definition>
			<definition id="4">
				<sentence>In a certain set of examples , the probability of a tag ti is estimated by I~ , l+-~ ri ( 4 ) = , +~ where m is the number of possible tags and n the number of examples .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">the number of possible tags and n the number of examples</definiens>
			</definition>
			<definition id="5">
				<sentence>Relaxation labelling is a generic name for a family of iterative algorithms which perform function optimization , based on local information .</sentence>
				<definiendum id="0">Relaxation labelling</definiendum>
				<definiens id="0">a generic name for a family of iterative algorithms which perform function optimization , based on local information</definiens>
			</definition>
			<definition id="6">
				<sentence>Gp~ ( rn ) is the weight assigned to label k for variable r at time m. 241 in the constraint except ( vi , t } ) ( representing how applicable the constraint is in the current context ) multiplied by Cr which is the constraint compatibility value ( stating how compatible the pair is with the context ) .</sentence>
				<definiendum id="0">Gp~ ( rn )</definiendum>
				<definiens id="0">the constraint compatibility value ( stating how compatible the pair is with the context )</definiens>
			</definition>
			<definition id="7">
				<sentence>It could be an adjective , meaning the 243 main office , or a noun , meaning the school head ofrice , Second , the WSJ corpus contains noise ( mistagged words ) that affects both the training and the test sets .</sentence>
				<definiendum id="0">WSJ corpus</definiendum>
				<definiens id="0">contains noise ( mistagged words ) that affects both the training and the test sets</definiens>
			</definition>
			<definition id="8">
				<sentence>1989 Relaxation and Neural Learning : Points of Convergence and Divergence .</sentence>
				<definiendum id="0">Neural Learning</definiendum>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>dl may contain only one partial dependency tree , the extracted phrase , d2 contains the rest of the sentence .</sentence>
				<definiendum id="0">d2</definiendum>
				<definiens id="0">contains the rest of the sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>This condition is written as ( 5 ) governs ( h , d , m ) A DG is thus characterized by ( 6 ) G = ( Lex , C , isac , E ) The language L ( G ) includes any sequence of words for which a dependency tree can be constructed such that for each word h governing a word m in dependency d , governs ( h , d , m ) holds .</sentence>
				<definiendum id="0">DG</definiendum>
				<definiens id="0">The language L ( G ) includes any sequence of words for which a dependency tree</definiens>
			</definition>
			<definition id="2">
				<sentence>&lt; 1 Definition 3 ( DG recognition problem ) A possible instance of the DG recognition problem is a tuple ( G , a ) where G = ( Lex , C , isac , ~ ) is a dependency grammar as defined in Section 3 and a E E + .</sentence>
				<definiendum id="0">DG recognition problem</definiendum>
				<definiendum id="1">~ )</definiendum>
				<definiens id="0">a tuple ( G , a ) where G = ( Lex , C , isac ,</definiens>
			</definition>
			<definition id="3">
				<sentence>The DG recognition problem DGR consists of all instances ( G , a ) such that a E L ( G ) .</sentence>
				<definiendum id="0">DG recognition problem DGR</definiendum>
			</definition>
			<definition id="4">
				<sentence>We define a function f : E -- + V \ Vo by f ( ei ) =def s.mod ( hi ) for all ei E E. By construction f ( ei ) is an end point of edge ei , i.e. ( 2 ) V ( v , , , ,v , d e E : f ( ( v , .</sentence>
				<definiendum id="0">construction f</definiendum>
				<definiens id="0">E -- + V \ Vo by f ( ei ) =def s.mod</definiens>
				<definiens id="1">an end point of edge ei</definiens>
			</definition>
			<definition id="5">
				<sentence>Slot grammar : A system for simpler construction of practical natural language grammars .</sentence>
				<definiendum id="0">Slot grammar</definiendum>
			</definition>
</paper>

		<paper id="1070">
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>Let A be a finite alphabet of distinct symbols , \ [ A\ [ = k , and let z T 6 A T denote an arbitrary string of length T over the alphabet A. Then z~ denotes the substring of z T that begins at position i and ends at position j. For convenience , we abbreviate the unit length substring z~ as zi and the length t prefix of z T as z* .</sentence>
				<definiendum id="0">T denote</definiendum>
				<definiendum id="1">z T</definiendum>
				<definiens id="0">an arbitrary string of length T over the alphabet A. Then z~ denotes the substring of</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>This paper introduces primitive Optimality Theory ( OTP ) , a linguistically motivated formalization of OT .</sentence>
				<definiendum id="0">primitive Optimality Theory</definiendum>
				<definiens id="0">a linguistically motivated formalization of OT</definiens>
			</definition>
			<definition id="1">
				<sentence>( Filter 1 ( Gen ( input ) ) ) ) where the function Gen is fixed across languages and Gen ( input ) C_ Repns is a potentially infinite set of candidate surface forms .</sentence>
				<definiendum id="0">Gen</definiendum>
				<definiens id="0">a potentially infinite set of candidate surface forms</definiens>
			</definition>
			<definition id="2">
				<sentence>Given such an underspecified timeline as lexical input , Gen outputs the set of all fully specified timelines that are consistent with it .</sentence>
				<definiendum id="0">Gen</definiendum>
				<definiens id="0">outputs the set of all fully specified timelines that are consistent with it</definiens>
			</definition>
			<definition id="3">
				<sentence>Scoring : Constraint ( R ) = number of a 's in R that do not overlap any 8 .</sentence>
				<definiendum id="0">Scoring</definiendum>
				<definiens id="0">Constraint ( R ) = number of a 's in R that do not overlap any 8</definiens>
			</definition>
			<definition id="4">
				<sentence>Recall that the generation problem is to find the output set S , ~ , where ( 13 ) a. So = Gen ( inpu~ ) C_ Repns b. Si+l = Filteri+l ( Si ) C Si Since in OTP , the input is a partial order of edge brackets , and Sn is a set of one or more total orders ( timelines ) , a natural approach is to successively refine a partial order .</sentence>
				<definiendum id="0">Sn</definiendum>
				<definiens id="0">a partial order of edge brackets</definiens>
				<definiens id="1">a set of one or more total orders ( timelines</definiens>
			</definition>
			<definition id="5">
				<sentence>Repns is defined as the intersection of many automata exactly like ( 18 ) , called tier rules , which ensure that brackets are properly paired on a given tier such as F ( foot ) .</sentence>
				<definiendum id="0">Repns</definiendum>
				<definiens id="0">the intersection of many automata exactly like ( 18 ) , called tier rules , which ensure that brackets are properly paired on a given tier such as F ( foot )</definiens>
			</definition>
			<definition id="6">
				<sentence>Thus if V syncopates as in footnote 2 , it still violates the parse constraint _V -V. This is an explicit property of OTP : otherwise , nothing that failed to parse would ever violate PARSE , because it would be gone !</sentence>
				<definiendum id="0">OTP</definiendum>
				<definiens id="0">an explicit property of</definiens>
			</definition>
			<definition id="7">
				<sentence>A typical GA constraint is ALIGN ( F , L , Word , L ) , which sums the number of syllables between each left foot edge F\ [ and the left edge of the prosodic word .</sentence>
				<definiendum id="0">ALIGN</definiendum>
				<definiens id="0">sums the number of syllables between each left foot edge F\ [ and the left edge of the prosodic word</definiens>
			</definition>
			<definition id="8">
				<sentence>The key operation in ( 14 ) is to find Bestpaths ( A 71 C ) , where .4 is an unweighted factored automaton and C is an ordinary weighted FSA ( a constraint ) .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">an unweighted factored automaton</definiens>
				<definiens id="1">an ordinary weighted FSA ( a constraint )</definiens>
			</definition>
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>Using these dotted rules as auxiliary symbols we can work with regular languages over the alphabet E= TU { ( X , m , n ) \ ] X E V Am= I , ... , mxA n = O , ... , max { nx , m 1 , O } , z } where T is the set of terminal symbols , V is the set of nonterminals , mx is the number of productions for nonterminal X , and nx , m is the number of symbols on the right-hand side of the ruth production for X. It will be convenient to use the symbol * as a 'wildcard ' , so ( s , * , O ) means { ( X , m , n } E E IX = s , n=O } and ( * , * , z ) means { ( X , m , n ) E Eln= z } .</sentence>
				<definiendum id="0">T</definiendum>
				<definiendum id="1">V</definiendum>
				<definiendum id="2">mx</definiendum>
				<definiens id="0">m , n ) \ ] X E V Am= I , ... , mxA n = O , ...</definiens>
				<definiens id="1">the set of terminal symbols</definiens>
				<definiens id="2">the set of nonterminals</definiens>
				<definiens id="3">a 'wildcard ' , so ( s , * , O ) means { ( X , m , n } E E IX = s , n=O } and ( * , * , z ) means { ( X , m , n ) E Eln= z }</definiens>
			</definition>
			<definition id="1">
				<sentence>Consider , for example , the following grammar : S -- + aSb S -- +e Then the following is one of the strings over E that we would like to accept , corresponding to the string aabb accepted by the grammar : ( s , 1 , O ) a ( s , 1 , 1 } ( s , 1 , O } a ( s , 1 , 1 ) ( s , 2 , 0 ) ( s , 2 , z ) ( s , 1 , 2 ) b ( s , 1 , z ) ( s , 1 , 2 ) b ( s , 1 , z ) Our first approximation to the set of acceptable strings is ( S , * , 0 ) N* ( S , * , z ) , i.e. strings that start with beginning to parse an S and end with having parsed an S. From this initial approximation we subtract ( that is , we intersect with the complement of ) a series of expressions representing restrictions on the set of acceptable strings : 1 1In these expressions over regular languages set union and set difference are denoted by + and - , respectively , while juxtaposition denotes concatenation and the bar denotes complementation ( 5 E* x ) .</sentence>
				<definiendum id="0">z )</definiendum>
				<definiens id="0">strings that start with beginning to parse an S and end with having parsed an S.</definiens>
				<definiens id="1">a series of expressions representing restrictions on the set of acceptable strings : 1 1In these expressions over regular languages set union and set difference are denoted by + and - , respectively , while juxtaposition denotes concatenation and the bar denotes complementation ( 5 E* x )</definiens>
			</definition>
			<definition id="2">
				<sentence>A subgrammar consists of all the productions for nonterminals in one of the equivalence classes of S. Calculate the approximations for each nonterminal by treating the nonterminals that belong to other equivalence classes as if they were terminals .</sentence>
				<definiendum id="0">subgrammar</definiendum>
				<definiens id="0">consists of all the productions for nonterminals in one of the equivalence classes of S. Calculate the approximations for each nonterminal by treating the nonterminals that belong to other equivalence classes as if they were terminals</definiens>
			</definition>
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>The model will operate by means of two modules : • PREDICTOR predicts the next word wk+l given the word-parse k-prefix and then passes control to the PARSER ; • PARSER grows the already existing binary branching structure by repeatedly generating the transitions adjoin-left or adjoin-right until it passes control to the PREDICTOR by taking a null transition .</sentence>
				<definiendum id="0">PARSER</definiendum>
				<definiens id="0">operate by means of two modules : • PREDICTOR predicts the next word wk+l given the word-parse k-prefix and then passes control to the PARSER ; •</definiens>
			</definition>
			<definition id="1">
				<sentence>The probability P ( W , T ) can be broken into : 1+1 p P ( W , T ) = l-L=1\ [ ( wk/Wk-lTk-1 ) '' ~\ ] ~21 P ( tk l wk , Wk , Tk-1 , t~ . . . t~_l ) \ ] where : • Wk-lTk-1 is the word-parse ( k 1 ) -prefix • wk is the word predicted by PP~EDICTOR • Nk 1 is the number of adjoin operations the PARSER executes before passing control to the PREDICTOR ( the N~-th operation at position k is the null transition ) ; N~ is a function of T h_ { -2 } h_ { -I } h_O Figure 4 : Before an adjoin operation h.~ ( -z ) -h_ ( -2 ) h._o. h._ ( x ) Figure 5 : Result of adjoin-left h'_ { *t ) .</sentence>
				<definiendum id="0">position k</definiendum>
				<definiens id="0">the null transition</definiens>
			</definition>
			<definition id="2">
				<sentence>We developed and evaluated four LMs : • 2 bigram LMs P ( wk/Wk-lTk-1 ) = P ( Wk/Wk-1 ) referred to as W and w , respectively ; wk-1 is the previous ( word , POStag ) pair ; • 2 P ( wk/Wk-ITk -- 1 ) = P ( wjho ) models , referred to as H and h , respectively ; h0 is the previous exposed ( headword , POS/non-term tag ) pair ; the parses used in this model were those assigned manually in the Penn Treebank ( Marcus95 ) after undergoing headword percolation and binarization .</sentence>
				<definiendum id="0">h0</definiendum>
				<definiens id="0">the previous exposed ( headword , POS/non-term tag</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>An agent 's responses to a query are compared with a predefined key of minimum and maximum reference answers ; performance is the proportion of responses that match the key .</sentence>
				<definiendum id="0">performance</definiendum>
				<definiens id="0">the proportion of responses that match the key</definiens>
			</definition>
			<definition id="1">
				<sentence>PARADISE uses a decision-theoretic framework to specify the relative contribution of various factors to an agent 's overall performance .</sentence>
				<definiendum id="0">PARADISE</definiendum>
				<definiens id="0">uses a decision-theoretic framework to specify the relative contribution of various factors to an agent 's overall performance</definiens>
			</definition>
			<definition id="2">
				<sentence>Success at the task for a whole dialogue ( or subdialogue ) is measured by how well the agent and user achieve the information requirements of the task by the end of the 4These dialogues have been slightly modified from ( Danieli and Gerbino , 1995 ) .</sentence>
				<definiendum id="0">subdialogue )</definiendum>
				<definiens id="0">measured by how well the agent</definiens>
			</definition>
			<definition id="3">
				<sentence>Whenever an attribute value in a dialogue ( i.e. , data ) AVM matches the value in its scenario key , the number in the appropriate diagonal cell of the matrix ( boldface for clarity ) is incremented by 1 .</sentence>
				<definiendum id="0">AVM</definiendum>
				<definiens id="0">matches the value in its scenario key , the number in the appropriate diagonal cell of the matrix</definiens>
			</definition>
			<definition id="4">
				<sentence>Given a confusion matrix M , success at achieving the information requirements of the task is measured with the Kappa coefficient ( Carletta , 1996 ; Siegel and Castellan , 1988 ) : P ( A ) P ( E ) K-1 P ( E ) P ( A ) is the proportion of times that the AVMs for the actual set of dialogues agree with the AVMs for the scenario keys , and P ( E ) is the proportion of times that the AVMs for the dialogues and the keys are expected to agree by chance .</sentence>
				<definiendum id="0">Kappa coefficient</definiendum>
				<definiens id="0">A ) P ( E ) K-1 P ( E ) P ( A ) is the proportion of times that the AVMs for the actual set of dialogues agree with the AVMs for the scenario keys</definiens>
				<definiens id="1">the proportion of times that the AVMs for the dialogues and the keys are expected to agree by chance</definiens>
			</definition>
			<definition id="5">
				<sentence>274 where ti is the sum of the frequencies in column i of M , and T is the sum of the frequencies in M ( tl + • • • + tn ) .</sentence>
				<definiendum id="0">ti</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">the sum of the frequencies in M ( tl + • • • + tn )</definiens>
			</definition>
			<definition id="6">
				<sentence>Multiple linear regression produces a set of coefficients ( weights ) describing the relative contribution of each predictor factor in accounting for the variance in a predicted factor .</sentence>
				<definiendum id="0">Multiple linear regression</definiendum>
				<definiens id="0">produces a set of coefficients ( weights ) describing the relative contribution of each predictor factor in accounting for the variance in a predicted factor</definiens>
			</definition>
			<definition id="7">
				<sentence>The PARADISE methodology consists of the following steps : • definition of a task and a set of scenarios ; • specification of the AVM task representation ; • experiments with alternate dialogue agents for the task ; • calculation of user satisfaction using surveys ; • calculation of task success using ~ ; • calculation of dialogue cost using efficiency and qualitative measures ; • estimation of a performance function using linear regression and values for user satisfaction , K and dialogue costs ; • comparison with other agents/tasks to determine which factors generalize ; • refinement of the performance model .</sentence>
				<definiendum id="0">PARADISE methodology</definiendum>
				<definiens id="0">consists of the following steps : • definition of a task and a set of scenarios ; • specification of the AVM task representation ; • experiments with alternate dialogue agents for the task ; • calculation of user satisfaction using surveys</definiens>
				<definiens id="1">user satisfaction , K and dialogue costs ; • comparison with other agents/tasks to determine which factors generalize ; • refinement of the performance model</definiens>
			</definition>
			<definition id="8">
				<sentence>Assessment ( A ) -- establish the current behavior • Diagnosis ( D ) -- -establish the cause for the errant behavior • Repair ( R ) -- -establish that the correction for the errant behavior has been made • Test ( T ) -- -establish that the behavior is now correct Our informational analysis of this task results in the AVM shown in Table 7 .</sentence>
				<definiendum id="0">Assessment</definiendum>
				<definiens id="0">A ) -- establish the current behavior • Diagnosis</definiens>
			</definition>
			<definition id="9">
				<sentence>PARADISE is a general framework for evaluating spoken dialogue agents that integrates and enhances previous work .</sentence>
				<definiendum id="0">PARADISE</definiendum>
			</definition>
			<definition id="10">
				<sentence>PARADISE supports comparisons among dialogue strategies with a task representation that decouples what an agent needs to achieve in terms of the task requirements from how the agent carries out the task via dialogue .</sentence>
				<definiendum id="0">PARADISE</definiendum>
				<definiens id="0">supports comparisons among dialogue strategies with a task representation that decouples what an agent needs to achieve in terms of the task requirements from how the agent carries out the task via dialogue</definiens>
			</definition>
</paper>

		<paper id="1067">
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>Parallelism guides the interpretation process for the above discourses .</sentence>
				<definiendum id="0">Parallelism</definiendum>
				<definiens id="0">guides the interpretation process for the above discourses</definiens>
			</definition>
			<definition id="1">
				<sentence>Context constraints ~o are defined by the following abstract syntax : t : := x I f ( tl , ... , t , ) \ [ C ( t ) ~P : : : t : tl I ~A~ I A ( second-order ) term t is either a first-order variable X , a construction f ( tl , ... , tn ) where the arity off is n , or an application C ( t ) .</sentence>
				<definiendum id="0">tl I ~A~ I A ( second-order ) term t</definiendum>
				<definiens id="0">the following abstract syntax : t : := x I f ( tl , ... , t , ) \ [ C ( t ) ~P : : : t :</definiens>
			</definition>
			<definition id="2">
				<sentence>A context constraint is a conjunction of equations between second-order terms .</sentence>
				<definiendum id="0">context constraint</definiendum>
				<definiens id="0">a conjunction of equations between second-order terms</definiens>
			</definition>
			<definition id="3">
				<sentence>t where X occurs exactly once in the second-order term t. Let a be a variable assignment that maps first-order variables to finite trees and second-order variables to context functions .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">maps first-order variables to finite trees and second-order variables to context functions</definiens>
			</definition>
			<definition id="4">
				<sentence>This variable binding is applied to the remaining constraint where X8 is substituted by @ ( @ ( s , c ) , j ) .</sentence>
				<definiendum id="0">X8</definiendum>
				<definiens id="0">s , c ) , j )</definiens>
			</definition>
			<definition id="5">
				<sentence>For example the tree ( many @ language ) @ ( lamx ( ( spoken_by @ john ) @ varx ) ) represents the HOL formula ( =poke by ( j Note that the function symbol @ represents the application in HOL and the function symbols lamx the abstraction over x in HOL .</sentence>
				<definiendum id="0">HOL formula</definiendum>
				<definiens id="0">the function symbol @ represents the application in HOL and the function symbols lamx the abstraction over x in HOL</definiens>
			</definition>
			<definition id="6">
				<sentence>Cz ( many @ linguist @ lamy ( C4 ( X ) ) ) A Xs = Cs ( spoken_by @ vary @ varx ) Both solved constraints in ( 7 ) describe infinite sets of solutions which arise from freely instantiating the remaining context variables by arbitrary contexts .</sentence>
				<definiendum id="0">Cz</definiendum>
				<definiendum id="1">Xs = Cs</definiendum>
				<definiens id="0">Both solved constraints in ( 7 ) describe infinite sets of solutions which arise from freely instantiating the remaining context variables by arbitrary contexts</definiens>
			</definition>
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>A tree domain is a subset of strings over a linearly ordered set which is closed under prefix and left sister .</sentence>
				<definiendum id="0">tree domain</definiendum>
				<definiens id="0">a subset of strings over a linearly ordered set which is closed under prefix and left sister</definiens>
			</definition>
			<definition id="1">
				<sentence>Uppercase letters denote second order variables , lowercase ones first order variables , &lt; ~* reflexive domination , &lt; ~+ proper domination and -4 proper precedence : AC-Com ( xl , x2 ) % x c-commands y : ( Vz ) \ [ z &lt; ~+ x = # z &lt; ~+ y\ ] A -~ ( x &lt; 1 '' y ) A % y does not c-command x : 4 + y z 4 + x\ ] A 4 '' x ) ) A % x preceeds y : x-~y The corresponding tree automaton is shown in Figure 2 .</sentence>
				<definiendum id="0">Uppercase letters</definiendum>
				<definiens id="0">second order variables , lowercase ones first order variables , &lt; ~* reflexive domination , &lt; ~+ proper domination and -4 proper precedence : AC-Com ( xl</definiens>
			</definition>
			<definition id="2">
				<sentence>The constraint base is an automaton which represents the incremental accumulation of knowledge about the possible valuations of variables .</sentence>
				<definiendum id="0">constraint base</definiendum>
				<definiens id="0">an automaton which represents the incremental accumulation of knowledge about the possible valuations of variables</definiens>
			</definition>
			<definition id="3">
				<sentence>A derivation step consists of two parts : goal reduction , which substitutes the body of a goal for an appropriate head , and constraint solving , which means in our case that we have to check the satisfiability of the constraint associated with the clause in conjunction with the current constraint store .</sentence>
				<definiendum id="0">derivation step</definiendum>
				<definiens id="0">consists of two parts : goal reduction , which substitutes the body of a goal for an appropriate head , and constraint solving , which means in our case that we have to check the satisfiability of the constraint associated with the clause in conjunction with the current constraint store</definiens>
			</definition>
			<definition id="4">
				<sentence>parse ( Words , Tree ) { Tree ( Words ) } &amp; yield ( Words , Tree ) &amp; xbar ( Tree ) &amp; ecp ( Tree ) Figure 4 : parse as in Johnson ( 1995 ) In more detail , Words denotes a set of nodes labeled according to the input description .</sentence>
				<definiendum id="0">parse ( Words</definiendum>
				<definiendum id="1">Words</definiendum>
				<definiens id="0">a set of nodes labeled according to the input description</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>2 For the purposes of the evolutionary simulation described in §3 , GC ( U ) Gs are represented as a sequence of p-settings ( where p denotes principles or parameters ) based on a flat ( ternary ) sequential encoding of such default inheritance lattices .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">principles or parameters ) based on a flat ( ternary ) sequential encoding of such default inheritance lattices</definiens>
			</definition>
			<definition id="1">
				<sentence>A set of grammars based on typological distinctions defined by basic constituent order ( e.g. Greenberg , 1966 ; Hawkins , 1994 ) was constructed as a ( partial ) GCUG with independently varying binaryvalued parameters .</sentence>
				<definiendum id="0">set of grammars</definiendum>
				<definiens id="0">a ( partial ) GCUG with independently varying binaryvalued parameters</definiens>
			</definition>
			<definition id="2">
				<sentence>The algorithm for the parser working with a GCG which includes application , composition and permutation is given in Figure 5 .</sentence>
				<definiendum id="0">GCG</definiendum>
				<definiendum id="1">permutation</definiendum>
				<definiens id="0">includes application , composition and</definiens>
			</definition>
			<definition id="3">
				<sentence>Figure 5 : The Parsing Algorithm 421 Stack Input Buffer Operation Step WML Kim loves Sandy 0 0 Kim : NP : kim ~ loves Sandy Shift 1 1 loves : ( S\NP ) /NP : A y , x ( love ' x , y ) Sandy Shift 2 3 Kim : NP : kim ~ Sandy : NP : sandy ~ Shift 3 6 loves : ( S\NP ) /NP : A y , x ( love ' x , y ) Kim : NP : kim ~ loves Sandy : S/NP : A x ( love ' x , sandy ' ) Reduce ( A ) 4 Kim : NP : kim ~ Kim loves Sandy : S : ( love ' kim ~ , sandy ~ ) Reduce ( A ) 5 Figure 7 : WML for Kim loves Sandy After each parse step ( Shift , Reduce , Halt ( see Fig 5 ) : troduced by Shift or Reduce ) a WML value of 0 cell onto the WML-record When the parser halts , return the sum of the WMLrecord gives the total WML for a derivation Figure 6 : The WML Algorithm thus indirectly languages , by parsing each sentence type from the exemplifying data with the associated grammar and then taking the mean of the WML obtained for these sentence types .</sentence>
				<definiendum id="0">S\NP ) /NP</definiendum>
				<definiendum id="1">Reduce</definiendum>
				<definiens id="0">troduced by Shift or Reduce ) a WML value of 0 cell onto the WML-record When the parser halts , return the sum of the WMLrecord gives the total WML for a derivation Figure 6 : The WML Algorithm thus indirectly languages , by parsing each sentence type from the exemplifying data with the associated grammar and then taking the mean of the WML obtained for these sentence types</definiens>
			</definition>
			<definition id="4">
				<sentence>The parameter setting algorithm is an extension of Gibson and Wexler 's ( 1994 ) Trigger Learning Algorithm ( TLA ) to take account of the inheritancebased partial ordering and the role of memory in learning .</sentence>
				<definiendum id="0">parameter setting algorithm</definiendum>
				<definiens id="0">an extension of Gibson and Wexler 's ( 1994 ) Trigger Learning Algorithm ( TLA ) to take account of the inheritancebased partial ordering and the role of memory in learning</definiens>
			</definition>
			<definition id="5">
				<sentence>The TLA is error-driven parameter settings are altered in constrained ways when a learner can not parse trigger input .</sentence>
				<definiendum id="0">TLA</definiendum>
				<definiens id="0">error-driven parameter settings</definiens>
			</definition>
			<definition id="6">
				<sentence>Trigger input is defined as primary linguistic data which , because of its structure or context of use , is determinately unparsable with the correct interpretation ( e.g. Lightfoot , 1991 ) .</sentence>
				<definiendum id="0">Trigger input</definiendum>
			</definition>
			<definition id="7">
				<sentence>Each step for a learner can be defined in terms of three functions : P-SETTING , GRAMMAR and PARSER , as : PARSERi ( GRAMMAR/ ( P-SETTING/ ( Sentence j ) ) ) A p-setting defines a grammar which in turn defines a parser ( where the subscripts indicate theoutput of each function given the previous trigger ) .</sentence>
				<definiendum id="0">GRAMMAR</definiendum>
				<definiendum id="1">GRAMMAR/ ( P-SETTING/</definiendum>
				<definiendum id="2">p-setting</definiendum>
				<definiens id="0">defines a grammar which in turn defines a parser ( where the subscripts indicate theoutput of each function given the previous trigger )</definiens>
			</definition>
			<definition id="8">
				<sentence>The relative fitness of a LAgt is a function of the proportion of its linguistic interactions which have been successful , the expressivity of the language ( s ) spoken , and , optionally , of the mean WML for parsing during a cycle of interactions .</sentence>
				<definiendum id="0">LAgt</definiendum>
			</definition>
			<definition id="9">
				<sentence>An interaction cycle consists of a prespecified number of individual random interactions between LAgts , with generating and parsing agents also selected randomly .</sentence>
				<definiendum id="0">interaction cycle</definiendum>
				<definiens id="0">consists of a prespecified number of individual random interactions between LAgts , with generating and parsing agents also selected randomly</definiens>
			</definition>
			<definition id="10">
				<sentence>Each learner was tested against an adult LAgt initialized to generate one of seven full languages in the set which are close to an attested language ; namely , `` English '' ( SVO , predominantly right-branching ) , `` Welsh '' ( SVOvl , mixed order ) , `` Malagasy '' ( VOS , right-branching ) , `` Tagalog '' ( VSO , right-branching ) , `` Japanese '' ( SOV , left-branching ) , `` German '' ( SOVv2 , predominantly right-branching ) , `` Hixkaryana '' ( OVS , mixed order ) , and an unattested full OSV language with leftbranching syntax .</sentence>
				<definiendum id="0">Welsh ''</definiendum>
				<definiendum id="1">Tagalog</definiendum>
				<definiens id="0">tested against an adult LAgt initialized to generate one of seven full languages in the set which are close to an attested language</definiens>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>An IIMM transducer builds on the data ( probability matrices ) of the underlying HMM .</sentence>
				<definiendum id="0">IIMM transducer</definiendum>
				<definiens id="0">builds on the data ( probability matrices</definiens>
			</definition>
			<definition id="1">
				<sentence>The s-type transducer tags any corpus which contains only known subsequences , in exactly the same way , i.e. with the same errors , as the corresponding HMM tagger does .</sentence>
				<definiendum id="0">s-type transducer</definiendum>
				<definiens id="0">tags any corpus which contains only known subsequences , in exactly the same way</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>TAG ( Joshi et al. , 1975 ) is a grammar formalism built around two operations that combine pairs of trees , SUBSTITUTION and ADJOINING .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">a grammar formalism built around two operations that combine pairs of trees , SUBSTITUTION and ADJOINING</definiens>
			</definition>
			<definition id="1">
				<sentence>A TAG grammar consists of a finite set of ELEMENTARY trees , which can be combined by these substitution and adjoining operations to produce derived trees recognized by the grammar .</sentence>
				<definiendum id="0">TAG grammar</definiendum>
				<definiens id="0">consists of a finite set of ELEMENTARY trees , which can be combined by these substitution and adjoining operations to produce derived trees recognized by the grammar</definiens>
			</definition>
			<definition id="2">
				<sentence>First , we adopt an ONTOLOGICALLY PROMISCUOUS representation ( Hobbs , 1985 ) that includes a wide variety of types of entities .</sentence>
				<definiendum id="0">ONTOLOGICALLY PROMISCUOUS representation</definiendum>
				<definiens id="0">includes a wide variety of types of entities</definiens>
			</definition>
			<definition id="3">
				<sentence>Ontological promiscuity offers a simple syntax-semantics interface .</sentence>
				<definiendum id="0">Ontological promiscuity</definiendum>
			</definition>
			<definition id="4">
				<sentence>The specification of this algorithm is summarized in the following pseudocode : 201 STEM /buy/ /sell/ /purchase/ /book/ SEMANTICS S YNTAX PRAGMATICS S buyer/buy/bought/from/seller , etc. register ( informal ) S seller~sell~bought~to~buyer , etc. register ( informal ) S buyer/purchase/bought/from/seller , etc. register ( formal ) book ( x ) /a//book/ , etc. \ [ always possible\ ] S = buy ( buying , buyer , seller , bought ) Figure 4 : Sample entries from the lexicon SUBCAT FRAME TREES PRAGMATICS Intransitive Active \ [ always possible\ ] Transitive Active \ [ always possible\ ] Topicalized Object in-poset ( obj ) , in-op ( event ) Left-Dislocated Object in-poset ( obj ) Ditransitive Active \ [ always possible\ ] Topicalized Dir Object in-poset ( dir obj ) , in-op ( event ) Left-Dislocated Dir Object in-poset ( dir obj ) PP Predicative Active \ [ always possible\ ] Locative Inversion newer-than ( subj , loc ) etc .</sentence>
				<definiendum id="0">SUBCAT FRAME TREES PRAGMATICS Intransitive Active</definiendum>
				<definiens id="0">Left-Dislocated Object in-poset ( obj ) Ditransitive Active \ [ always possible\ ] Topicalized Dir Object in-poset ( dir obj</definiens>
			</definition>
			<definition id="5">
				<sentence>G-TAG : A formalism for Text Generation inspired from Tree Adjoining Grammar : TAG issues .</sentence>
				<definiendum id="0">G-TAG</definiendum>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>Depending on the discourse history , the following are two of the possible verbalizations : `` Since 1~ is the unit element of U , and u is an element of U , u * lu -u. '' `` According to the definition of unit element , u * 1U -U. '' An explicit reference to a premise or an inference method is not restricted to a nominal phrase , as opposed to many of the treatments of subsequent references found in the literature .</sentence>
				<definiendum id="0">u</definiendum>
				<definiens id="0">restricted to a nominal phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>Concretely , PROVERB uses an architecture that models text generation as a combination of hierarchical planning and focus-guided navigation .</sentence>
				<definiendum id="0">PROVERB</definiendum>
				<definiens id="0">uses an architecture that models text generation as a combination of hierarchical planning and focus-guided navigation</definiens>
			</definition>
			<definition id="2">
				<sentence>PROVERB is a text planner that verbalizes natural deduction ( ND ) style proofs ( Gentzen , 1935 ) .</sentence>
				<definiendum id="0">PROVERB</definiendum>
			</definition>
			<definition id="3">
				<sentence>While most existing systems follow one of the two approaches exclusively , PROVERB uses them as complementary techniques in an integrated framework .</sentence>
				<definiendum id="0">PROVERB</definiendum>
				<definiens id="0">uses them as complementary techniques in an integrated framework</definiens>
			</definition>
			<definition id="4">
				<sentence>PCAs are the primitive actions planned by the macroplanner of PROVERB• Like speech acts , they can be defined in terms of the communicative goals they fulfill as well as their possible verbalizations .</sentence>
				<definiendum id="0">PCAs</definiendum>
				<definiens id="0">the primitive actions planned by the macroplanner of PROVERB• Like speech acts</definiens>
			</definition>
			<definition id="5">
				<sentence>For instance , the PCA ( Begin-Cases Goal : Formula Assumptions : ( A B ) ) creates two attentional spaces with A and B as the assumptions , and Formula as the goal by producing the verbalization : `` To prove Formula , let us consider the two cases by assuming A and B. '' Hierarchical planning operators represent communicative norms concerning how a proof is to be presented can be split into subproofs , how the subproofs can be mapped onto some linear order , and how primitive subproofs should be conveyed by PCAs .</sentence>
				<definiendum id="0">PCA</definiendum>
				<definiendum id="1">B )</definiendum>
				<definiendum id="2">Formula</definiendum>
				<definiens id="0">communicative norms concerning how a proof is to be presented can be split into subproofs , how the subproofs can be mapped onto some linear order , and how primitive subproofs should be conveyed by PCAs</definiens>
			</definition>
			<definition id="6">
				<sentence>\ [ 1\ ] : P ( a , b ) \ [ 1\ ] : P ( a , b ) , \ [ 3\ ] : S ( c ) \ [ 2\ ] Q ( a ; b ) ' \ [ 4\ ] : R ( b , c ) \ [ 5\ ] : Q ( a , b ) A R ( b , c ) Assume that node \ [ 1\ ] is the local focus , { a , b } is the set of focal centers , \ [ 3\ ] is a previously presented node and node \ [ 5\ ] is the root of the proof to be presented• \ [ 2\ ] is chosen as the next node to be presented , since it does not introduce any new semantic object and its overlap with the focal centers ( { a , b } ) is larger than the overlap of \ [ 4\ ] with the focal centers ( { b } ) .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">the set of focal centers , \ [ 3\ ] is a previously presented node and node \ [ 5\ ] is the root of the proof to be presented• \ [ 2\ ] is chosen as the next node to be presented , since it does not introduce any new semantic object and its overlap with the focal centers</definiens>
			</definition>
			<definition id="7">
				<sentence>; Igroup ( F , * ) A subgroup ( U , F , * ) A unit ( F , 1 , * ) A unit ( U , lt\ ] , * ) : =~ 1 = It\ ] Reason ( Hyp ) ( Def-subgroup 7 ) ( Def-unit 7 ) ( : :1 9 ) ( Hyp ) ( Def-unit 7 11 ) ( Def-subset 8 11 ) ( Def-subset 8 9 ) ( Def-group 7 ) ( Def-sohition 12 13 14 15 ) ( Def-unit 7 13 ) ( Def-unit 7 ) ( Def-soluti0n 13 17 18 15 ) ( Th-solution 17 16 19 ) ( Choice 10 20 ) ( Ded 7:21 ) Figure 3 : Abstracted Proof about Unit Element of Subgroups of a discourse into an attentional hierarchy , since following the theory of Grosz and Sidner ( Grosz and Sidner , 1986 ) , there is a one-to-one correspondence between the intentional hierarchy and the attentional hierarchy .</sentence>
				<definiendum id="0">Igroup</definiendum>
				<definiens id="0">Abstracted Proof about Unit Element of Subgroups of a discourse into an attentional hierarchy</definiens>
			</definition>
			<definition id="8">
				<sentence>The definitions of semigroup , group , and unit are obvious , solution ( a , b , c , F , , ) stands for `` c is a solution of the equation a , z = b in F. '' Each line in the proof is of the form : Label A FConclusion ( Justification reasons ) where Justification is either an ND inference rule , a definition or theorem , which justifies the derivation of the Conclusion using as premises the formulas in the lines given as reasons .</sentence>
				<definiendum id="0">Justification</definiendum>
				<definiens id="0">The definitions of semigroup , group , and unit are obvious , solution ( a , b , c , F , , ) stands for `` c is a solution of the equation a , z = b in F. '' Each line in the proof is of the form : Label A FConclusion ( Justification reasons ) where</definiens>
			</definition>
			<definition id="9">
				<sentence>According to the definition of unit element , 1v E U. Therefore there is an X , X E U. Now suppose that u is such an X. According to the definition of unit element , u • ltr = u. Since U is a subgroup of F , U C F. Therefore lv E F. Similarly u E F , since u E U. Since F is a group , F is a semigroup .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">a group</definiens>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>a The proof ( 2 ) ( which omits lambda terms ) illustrates that 'hypothetical reasoning ' in proofs ( i.e. the use of additional assumptions that are later discharged or cancelled , such as Z here ) is driven by the presence of higher-order formulae ( such as Xo- ( yc-z ) here ) .</sentence>
				<definiendum id="0">Xo- ( yc-z</definiendum>
				<definiens id="0">omits lambda terms ) illustrates that 'hypothetical reasoning ' in proofs ( i.e. the use of additional assumptions that are later discharged or cancelled , such as Z here ) is driven by the presence of higher-order formulae</definiens>
			</definition>
			<definition id="1">
				<sentence>( ¢ : x : t ) = ( ( x+¢ ) : t,0 ) where X atomic where 0.1 ( ¢ , Xo -- Y ) = ( Z , C ) where X atomic , 7 a fresh variable where 6 , 7 fresh variables , 6 : = ¢~7 0 '' 1 ( 6 , X 1 ) = ( X2 , C ) C ' = C u { ~r c 7 } ( unless ~r = 0 , when C = C ' ) ( 6 ) i. old formula : { i } : Xo -- ( Y : { j } ) new formula : ( X+C ) o- ( Y+Tr ) constraints : { ¢ = { i } ~rr , { j } C 7r } if .</sentence>
				<definiendum id="0">X atomic</definiendum>
				<definiens id="0">~r = 0 , when C = C ' ) ( 6 ) i. old formula : { i } : Xo -- ( Y : { j } ) new formula : ( X+C ) o- ( Y+Tr ) constraints : { ¢ = { i } ~rr</definiens>
			</definition>
			<definition id="2">
				<sentence>The inference is marked as \ [ m , n\ ] , where m is the argument position of the 'functor ' ( always the lefthand premise ) that is involved in the combination , and n indicates the number of arguments inherited from the 'argument ' ( righthand premise ) .</sentence>
				<definiendum id="0">m</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the number of arguments inherited from the 'argument ' ( righthand premise )</definiens>
			</definition>
			<definition id="3">
				<sentence>( 13 ) dep ( ( X Y / Z \ [ m , n\ ] ) ) = { ( i , j , k ) } where gov ( m , X ) = ( i , k ) , fun ( Y ) = j ( 14 ) dep* ( ( n/X \ [ a\ ] ) ) -0 dep* ( ( X Y / Z \ [ re , n\ ] ) ) = { ~ } U dep* ( X ) U dep* ( Y ) where 5 = dep ( ( X Y / Z \ [ m , n\ ] ) ) The procedure dep , defined in ( 13 ) , identifies the dependency relation established by any combination , i.e. for any subproof P = ( X Y / Z \ [ m , n\ ] ) , dep ( P ) returns a triple ( i , j , k ) , where i , j identify the head and dependent assumptions for the combination , and k indicates the argument position of the head assumption that is involved ( which has now been inherited to be argument m of the functor of the combination ) .</sentence>
				<definiendum id="0">gov</definiendum>
				<definiendum id="1">n\ ] ) , dep</definiendum>
				<definiendum id="2">k</definiendum>
				<definiens id="0">identifies the dependency relation established by any combination</definiens>
				<definiens id="1">the argument position of the head assumption that is involved ( which has now been inherited to be argument m of the functor of the combination )</definiens>
			</definition>
			<definition id="4">
				<sentence>The contraction relation generates a reduction relation ( t &gt; ) such that X reduces to Y ( X \ [ &gt; Y ) if\ ] Y is obtained from X by a finite series ( possibly zero ) of contractions .</sentence>
				<definiendum id="0">contraction relation</definiendum>
				<definiens id="0">generates a reduction relation</definiens>
			</definition>
			<definition id="5">
				<sentence>A term Y is a normal form of X iff ¥ is a normal form and X \ [ &gt; Y. We again require the ordering relation &lt; &lt; defined in ( 17 ) .</sentence>
				<definiendum id="0">term Y</definiendum>
				<definiens id="0">a normal form of X iff ¥ is a normal form</definiens>
			</definition>
			<definition id="6">
				<sentence>A redex is any subproof whose final step is a combination of two well-ordered subproofs , which establishes a dependency that undermines well-orderedness .</sentence>
				<definiendum id="0">redex</definiendum>
			</definition>
			<definition id="7">
				<sentence>This re349 x X Y Z X Z Y \ [ m , n\ ] ~ is , t\ ] V where s &lt; m 1 : &gt; V ' \ [ 8 , t\ ] \ [ ( m + t 1 ) , n\ ] W W Y z X Y Z Ira , n\ ] -\ [ ( s m + 1 ) , t\ ] V where m _ &lt; s I &gt; V ' \ [ s , t\ ] -Ira , ( n + t 1 ) \ ] W s &lt; ( m+ n ) W x X Y Z X Z Y ~\ [ m , n\ ] ~\ [ ( s -n + 1 ) , t\ ] V where s_ &gt; ( re+n ) D V ' \ [ ~ , t\ ] Ira , ~\ ] W w Y Z X Y Z Ira , n\ ] -- \ [ 8 , ( t n + : ) \ ] V t &gt; V ' -\ [ s , t\ ] \ [ ( m + s 1 ) , n\ ] W W Figure 1 : Local Reordering of Combination Steps : the four cases duction system can be shown to exhibit the property ( called strong normalisation ) that every reduction is finite , from which it follows that every proof has a normal form .</sentence>
				<definiendum id="0">s_ &gt;</definiendum>
				<definiens id="0">Local Reordering of Combination Steps : the four cases duction system can be shown to exhibit the property ( called strong normalisation</definiens>
			</definition>
			<definition id="8">
				<sentence>The following metric tt can be shown to suffice : ( a ) for P = ( nIX \ [ a\ ] ) , # ( P ) = 0 , ( b ) for P= ( XY / Z \ [ m , n\ ] ) , whose final step establishes a dependency a , # ( P ) = it ( X ) + ~u ( Y ) + D , where D is the number of dependencies 5 ' such that &lt; &lt; a ' , which are established in X and Y , i.e. D = \ [ A\ ] whereA= { 5 ' \ ] 5'edep , ( X ) Udep , ( Y ) A 5 &lt; &lt; 5 ' } .</sentence>
				<definiendum id="0">D</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a ) for P = ( nIX \ [ a\ ] ) , # ( P ) = 0</definiens>
			</definition>
			<definition id="9">
				<sentence>Xo- ( Yo-Z ) , Yo -- W =~ Xo- ( Wo-Z ) Compilation of the higher-order assumption would yield Xo -- Y plus Z , of which the first formula can compose with the second assumption Yo-W to give Xo-W , thereby achieving some semantically contentful combination of their associated meanings , which would not be allowed by composition over the original formulae .</sentence>
				<definiendum id="0">Xo- ( Yo-Z</definiendum>
				<definiens id="0">thereby achieving some semantically contentful combination of their associated meanings , which would not be allowed by composition over the original formulae</definiens>
			</definition>
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>We describe a method for interpreting abstract fiat syntactic representations , LFG fstructures , as underspecified semantic representations , here Underspecified Discourse Representation Structures ( UDRSs ) .</sentence>
				<definiendum id="0">LFG fstructures</definiendum>
			</definition>
			<definition id="1">
				<sentence>The details of the LFG and UDRT formalisms are described at length elsewhere : here we briefly present the very basics of the UDRS formalism ; we define a language of wff-s ( well-formed f-structures ) ; we define a mapping 7 '' from f-structures to UDRSs together with a reverse mapping r -1 and we show correctness with respect to an independent semantics ( Dalrymple et al. , 1996 ) .</sentence>
				<definiendum id="0">wff-s</definiendum>
			</definition>
			<definition id="2">
				<sentence>, Xn ) is a condition ( c ) if li , lj E L then li : '~lj is a condition ( d ) if li , lj , Ik E L then li : lj : :¢ , l~ is a condition ( e ) if l , ll , ... , ln E L then l : V ( ll , ... , ln ) is a condition a partial ordering defining an upper semi-lattice with a top element .</sentence>
				<definiendum id="0">l~</definiendum>
				<definiens id="0">a condition</definiens>
				<definiens id="1">a condition a partial ordering defining an upper semi-lattice with a top element</definiens>
			</definition>
			<definition id="3">
				<sentence>UDRSs are pairs of a set of type 2 conditions with a set of type 1 conditions : • A UDRS /C is a pair ( L , C ) where L = ( i , &lt; ) is an upper semi-lattice of labels and C a set of conditions of type 1 above such that if li : ~lj E 1The definition abstracts away from some of the complexities in the full definitions of the UDRS language ( Reyle , 1993 ) .</sentence>
				<definiendum id="0">UDRS /C</definiendum>
				<definiendum id="1">&lt; )</definiendum>
				<definiens id="0">a pair ( L , C ) where L = ( i ,</definiens>
				<definiens id="1">an upper semi-lattice of labels</definiens>
			</definition>
			<definition id="4">
				<sentence>The basic vocabulary consists of five disjoint sets : GFs ( subcategorizable grammatical functions ) , GF , ~ ( non-subcategorizable grammatical functions ) , SF ( semantic forms ) , ATR ( attributes ) and ATOM ( atomic values ) : 2This closes Z : under the subordination relations induced by complex conditions of the form -~K and Ki =~ Kj .</sentence>
				<definiendum id="0">basic vocabulary</definiendum>
				<definiens id="0">consists of five disjoint sets : GFs ( subcategorizable grammatical functions ) , GF , ~ ( non-subcategorizable grammatical functions ) , SF ( semantic forms ) , ATR ( attributes ) and ATOM ( atomic values ) : 2This closes Z : under the subordination relations induced by complex conditions of the form -~K and Ki =~ Kj</definiens>
			</definition>
			<definition id="5">
				<sentence>, x~ ) \ [ sP c \ ] ) • r ' ( LPRED ~II ( ) : =~ \ [ SPEC every \ ] • ri ( iVRE D H0 ) : = The formulation of the reverse translation r1 from UDRSs back into f-structures depends on a map between argument positions in UDRS predicates and grammatical functions in LFG semantic forms : I1 ( ~1 , ~2 , ... , ~ , ) I I I I n ( , rl , tru , ... , , r~ } This is , of course , the province of lexical mapping theories ( LMTs ) .</sentence>
				<definiendum id="0">x~</definiendum>
				<definiendum id="1">LPRED ~II ( )</definiendum>
				<definiendum id="2">theories ( LMTs</definiendum>
				<definiens id="0">iVRE D H0 ) : = The formulation of the reverse translation r1 from UDRSs back into f-structures depends on a map between argument positions in UDRS predicates and grammatical functions in LFG semantic forms : I1 ( ~1 , ~2 , ... , ~ , ) I I I I n ( , rl , tru , ... ,</definiens>
			</definition>
			<definition id="6">
				<sentence>404 I coach ( x\ [ ~\ ] ) ~ yer ( y~ ) Figure 1 : The UDRS rT- ( ~l ) =/C~ If the lexical map between argument positions in UDRS predicates and grammatical functions in LFG semantic forms is a function it can be shown that for all ~ E wff-s : ~-l ( r ( ~ ) ) = Proof is by induction on the complexity of ~ .</sentence>
				<definiendum id="0">UDRS rT-</definiendum>
				<definiens id="0">positions in UDRS predicates and grammatical functions in LFG semantic forms</definiens>
			</definition>
			<definition id="7">
				<sentence>Here we show correctness with respect to the linear logic ( a ) s based LFG semantics of ( Dalrymple et al. , 1996 ) : \ [ r ( ~ ) \ ] -- \ [ ~ ( ~ ) \ ] Correctness is with respect to ( sets of ) disambiguations and truthfl { ulu = 6 ( r ( ~ ) ) } { ll~ ( ~ ) ~ , l } where 6 is the UDRS disambiguation and b'u the linear logic consequence relation .</sentence>
				<definiendum id="0">Correctness</definiendum>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>Head transducer models consist of collections of finite state machines that are associated with pairs of lexical items in a bilingual lexicon .</sentence>
				<definiendum id="0">Head transducer models</definiendum>
				<definiens id="0">consist of collections of finite state machines that are associated with pairs of lexical items in a bilingual lexicon</definiens>
			</definition>
			<definition id="1">
				<sentence>The transfer system follows the familiar analysis-transfer-generation architecture ( Isabelle and Macklovitch 1986 ) .</sentence>
				<definiendum id="0">transfer system</definiendum>
				<definiens id="0">follows the familiar analysis-transfer-generation architecture</definiens>
			</definition>
			<definition id="2">
				<sentence>Our transfer model involves a bilingual lexicon specifying paired source-target fragments of dependency trees .</sentence>
				<definiendum id="0">transfer model</definiendum>
			</definition>
			<definition id="3">
				<sentence>The following nondeterministic actions are involved in the tiling process : • Selection of a bilingual entry given a source language word , w. • Matching the nodes and arcs of the source fragment of an entry against a local subgraph including a node labeled by w. A head transducer is a transduction version of the finite state head acceptors employed in the transfer model .</sentence>
				<definiendum id="0">head transducer</definiendum>
				<definiens id="0">involved in the tiling process : • Selection of a bilingual entry given a source language word</definiens>
			</definition>
			<definition id="4">
				<sentence>The cost of a solution ( i.e. , a possible translation of an input string ) is the sum of costs for all choices in the derivation of that solution .</sentence>
				<definiendum id="0">cost of a solution</definiendum>
				<definiens id="0">the sum of costs for all choices in the derivation of that solution</definiens>
			</definition>
			<definition id="5">
				<sentence>Translation word error rate is defined as the number of words in the source which are judged to have been mistranslated .</sentence>
				<definiendum id="0">Translation word error rate</definiendum>
				<definiens id="0">the number of words in the source which are judged to have been mistranslated</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>Informally , NLG is the production of a natural language text from computer-internal representation of information , where NLG can be seen as a complex -- potentially cascaded -- decision making process .</sentence>
				<definiendum id="0">NLG</definiendum>
				<definiens id="0">the production of a natural language text from computer-internal representation of information</definiens>
			</definition>
			<definition id="1">
				<sentence>TDL allows the user to define hierarchicallyordered types consisting of type and feature constraints .</sentence>
				<definiendum id="0">TDL</definiendum>
				<definiens id="0">allows the user to define hierarchicallyordered types consisting of type and feature constraints</definiens>
			</definition>
			<definition id="2">
				<sentence>Using their typed feature structure notation figure 1 displays a possible MRS of the string `` Sandy gives a chair to Kim '' ( abbreviated where convenient ) .</sentence>
				<definiendum id="0">Sandy</definiendum>
				<definiens id="0">gives a chair to Kim '' ( abbreviated where convenient )</definiens>
			</definition>
			<definition id="3">
				<sentence>SGP consists of a training module TM , an application module AM , and the subgram2But note , our approach does not depend on a flat representation of logical forms .</sentence>
				<definiendum id="0">SGP</definiendum>
				<definiens id="0">consists of a training module TM , an application module AM</definiens>
			</definition>
			<definition id="4">
				<sentence>TM extracts and generalizes the derivation tree of fs ( mrs ) , which we call the template tempi ( mrs ) of fs ( mrs ) , tempi ( mrs ) is then stored in a decision tree , where indices are computed from the MRS found under the root of tempi ( mrs ) .</sentence>
				<definiendum id="0">template tempi ( mrs ) of fs</definiendum>
				<definiens id="0">the MRS found under the root of tempi ( mrs )</definiens>
			</definition>
			<definition id="5">
				<sentence>where e.g. , NAMED is the common supertype of SANDYREL and KIMREL , and ACTUNDPREP is the supertype of GIVEREL .</sentence>
				<definiendum id="0">NAMED</definiendum>
				<definiendum id="1">ACTUNDPREP</definiendum>
				<definiens id="0">the common supertype of SANDYREL and KIMREL , and</definiens>
				<definiens id="1">the supertype of GIVEREL</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>For example , relation rhet_reI ( ELABORATION , 3 , 1 ) will be ruled 97 LABORATION 1 2 Figure 1 : The discourse tree of text ( 1 ) .</sentence>
				<definiendum id="0">relation rhet_reI</definiendum>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>From the generation perspective , cue usage consists of three distinct , but interrelated problems : ( 1 ) occurrence : whether or not to include a cue in the generated text , ( 2 ) placement : where the cue should be placed in the text , and ( 3 ) selection : what lexical item ( s ) should be used .</sentence>
				<definiendum id="0">cue usage</definiendum>
				<definiens id="0">consists of three distinct , but interrelated problems : ( 1 ) occurrence : whether or not to include a cue in the generated text , ( 2 ) placement : where the cue should be placed in the text , and ( 3 ) selection : what lexical item ( s ) should be used</definiens>
			</definition>
			<definition id="1">
				<sentence>Other researchers ( R6sner and Stede , 1902 ; Scott and de Souza , 1990 ) are concerned with generating text from `` RST trees '' , hierarchical structures where leaf nodes contain content and internal nodes indicate the rt~etorical relations , as defined in Rhetorical Structure Theory ( RST ) ( Mann and Thompson , 1988 ) , that exist between subtrees .</sentence>
				<definiendum id="0">Other researchers</definiendum>
				<definiens id="0">trees '' , hierarchical structures where leaf nodes contain content and internal nodes indicate the rt~etorical relations</definiens>
			</definition>
			<definition id="2">
				<sentence>1 RDA is a scheme devised for analyzing tutorial explanations in the domain of electronics troubleshooting .</sentence>
				<definiendum id="0">RDA</definiendum>
				<definiens id="0">a scheme devised for analyzing tutorial explanations in the domain of electronics troubleshooting</definiens>
			</definition>
			<definition id="3">
				<sentence>• ( Con ) Trib ( utor ) -pos ( ition ) captures the position of a particular contributor within the larger segment in which it occurs , and encodes the structure of the segment in terms of how many contributors precede and follow the core .</sentence>
				<definiendum id="0">Con ) Trib ( utor ) -pos</definiendum>
				<definiens id="0">ition ) captures the position of a particular contributor within the larger segment in which it occurs , and encodes the structure of the segment in terms of how many contributors precede and follow the core</definiens>
			</definition>
			<definition id="4">
				<sentence>Learning turns out to be most useful for Corel , where the error reduction ( as percentage ) from baseline to the upper bound of the best result is 32 % ; ~AII our experiments are run with groupin 9 turned on , so that C4.5 groups values together rather than creating a branch per value .</sentence>
				<definiendum id="0">Learning</definiendum>
				<definiens id="0">turns out to be most useful for Corel , where the error reduction</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>The centering model ( Grosz et al. , 1995 ) has evolved as a major methodology for computational discourse analysis .</sentence>
				<definiendum id="0">centering model</definiendum>
				<definiens id="0">a major methodology for computational discourse analysis</definiens>
			</definition>
			<definition id="1">
				<sentence>Cf ( Vi-1 ) : \ [ c 1 ... .. ej ... .. cs \ ] C~ ( Vi ) : \ [ Cl ... .. ck ... .. et \ ] Cf ( Ui-1 ) : \ [ el ... .. cj ... .. cs\ ] l &lt; j &lt; s Cf ( Vd : \ [ el ... .. ek ... .. e~l Table 2 : Thematic Progression Patterns Table 2 visualizes the abstract schemata of TP patterns. In our example ( cf. Table 8 in Section 4 ) , U1 to Ua illustrate the constant theme , while U7 to U10 illustrate the linear thematization of rhemes. In the latter case , the theme changes in each utterance , from `` Handbuch '' ( manual ) via `` Inhaltsverzeichnis '' ( table of contents ) to `` Kapitel '' ( chapter ) etc. Each of the new themes are introduced in the immediately preceding utterance so that local coherence between these utterances is established. Daneg ( 1974 ) also allows for the combination and recursion of these basic patterns ; this way the global thematic coherence of a text can be described by recurrence to these structural patterns. These principles allow for a major extension of the original centering algorithm. Given a reformulation of the TP constraints in centering terms , it is possible to determine referential segment boundaries and to arrange these segments in a nested , i.e. , hierarchical manner on the basis of which reachability constraints for antecedents can be formulated. According to the segmentation strategy of our approach , the Cp of the end point ( i.e. , the last utterance ) of a discourse segment provides the major theme of the whole segment , one which is particularly salient for anaphoric reference relations. Whenever a relevant new theme is established , however , it should reside in its own discourse segment , either embedded or in parallel to another one. Anaphora resolution can then be performed ( a ) with the forwardlooking centers of the linearly immediately preceding utterance , ( b ) with the forward-looking centers of the end point of the hierarchically immediately reachable discourse segment , and ( c ) with the preferred center of the end point of any hierarchically reachable discourse segment ( for a formalization of this constraint , cf. Table 4 ) . 105 Prior to a discussion of the algorithmic procedure for hypothesizing discourse segments based on evidence from local centering data , we will introduce its basic building blocks. Let x denote the anaphoric expression under consideration , which occurs in utterance Ui associated with segment level s. The function Resolved ( x , s , Us ) ( cf. Table 3 ) is evaluated in order to determine the proper antecedent ante for x. It consists of the evaluation of a teachability predicate for the antecedent on which we will concentrate here , and of the evaluation of the predicate lsAnaphorFor which contains the linguistic and conceptual constraints imposed on a ( pro ) nominal anaphor ( viz. agreement , binding , and sortal constraints ) or a textual ellipsis ( Hahn et al. , 1996 ) , not an issue in this paper. The predicate lsReachable ( cf. Table 4 ) requires ante to be reachable from the utterance Us associated with the segment level s. 2 Reachability is thus made dependent on the segment structure DS of the discourse as built up by the segmentation algorithm which is specified in Table 6. In Table 4 , the symbol `` =str '' denotes string equality , N the natural numbers. We also introduce as a notational convention that a discourse segment is identified by its index s and its opening and closing utterance , viz. DS\ [ s.beg\ ] and DS\ [ s.end\ ] , respectively. Hence , we may either identify an utterance Ui by its linear text index , i , or , if it is accessible , with respect to its hierarchical discourse segment index , s ( e.g. , cf. Table 8 where U3 = UDs\ [ 1.end\ ] or U13 = UDs\ [ 3.end\ ] ) . The discourse segment index is always identical to the currently valid segment level , since the algorithm in Table 6 implements a stack behavior. Note also that we attach the discourse segment index s to center expressions , e.g. , Cb ( s , Us ) . Resolved ( x , s , Ui ) : = l ante if IsReachable ( ante , s , Ui ) A IsAnaphorFor ( x , ante ) under else Table 3 : Resolution of Anaphora IsReachable ( ante , s , Ui ) if ante 6 C/ ( s , Ui-1 ) else if ante E C/ ( s 1 , Uosts_ , .~ , a\ ] ) else if ( 3v E N : ante =~tr Cp ( v , UDsI ... . a\ ] ) ^ v &lt; ( s 1 ) ) A ( -~Sv ' 6 N : ante = , t , Cp ( v ' , UDst~ , .~ndl ) A v &lt; v ' ) Table 4 : Reachability of the Anaphoric Antecedent Finally , the function Lift ( s , i ) ( cf. Table 5 ) determines the appropriate discourse segment level , s , of an utter2The Cf lists in the functional centering model are totally ordered ( Strobe &amp; Hahn , 1996 , p.272 ) and we here implicitly assume that they are accessed in the total order given. ance Ui ( selected by its linear text index , i ) . Lift only applies to structural configurations in the centering lists in which themes continuously shift at three different consecutive segment levels and associated preferred centers at least ( cf. Table 2 , lower box , for the basic pattern ) . Lift ( s , i ) : = Lift ( s1 , i1 ) if s &gt; 2Ai &gt; 3 ^ c. ( s , u , _~ ) # c~ ( ~ 1 , u , _~ ) ^ c~ ( s I , u , _~ ) # c. ( s 2 , u , _~ ) ^ c~ ( s , u , _ , ) • cj ( s1 , u , _~ ) 8 else Table 5 : Lifting to the Appropriate Discourse Segment Whenever a discourse segment is created , its starting and closing utterances are initialized to the current position in the discourse .</sentence>
				<definiendum id="0">IsAnaphorFor</definiendum>
				<definiens id="0">the combination and recursion of these basic patterns</definiens>
				<definiens id="1">contains the linguistic and conceptual constraints imposed on a ( pro ) nominal anaphor</definiens>
			</definition>
			<definition id="2">
				<sentence>Hence , the centered discourse segmentation procedure works in an incremental way and revises only locally relevant , yet globally irrelevant segmentation decisions on the fly .</sentence>
				<definiendum id="0">centered discourse segmentation procedure</definiendum>
				<definiens id="0">works in an incremental way and revises only locally relevant , yet globally irrelevant segmentation decisions on the fly</definiens>
			</definition>
			<definition id="3">
				<sentence>The anaphor `` HL-1260 '' does not cospecify the Cp of the utterance which represents the end of the hierarchically preceding discourse segment ( UT ) , but it co-specifies an element of the C !</sentence>
				<definiendum id="0">UT</definiendum>
				<definiens id="0">cospecify the Cp of the utterance which represents the end of the hierarchically preceding discourse segment (</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>A Boolean matrix multiplication algorithm takes as input two m x m Boolean matrices A and B and returns their Boolean product A x B , which is the m × m Boolean matrix C whose entries c~j are defined by m = V ( a , k A bkj ) .</sentence>
				<definiendum id="0">Boolean matrix multiplication</definiendum>
				<definiens id="0">input two m x m Boolean matrices A and B and returns their Boolean product A x B , which is the m × m Boolean matrix C whose entries c~j are defined by m = V ( a , k A bkj )</definiens>
			</definition>
			<definition id="1">
				<sentence>We use the usual definition of a context-free grammar ( CFG ) as a 4-tuple G = ( E , V , R , S ) , where E is the set of terminals , V is the set of nonterminals , R is the set of productions , and S C V is the start symbol .</sentence>
				<definiendum id="0">E</definiendum>
				<definiendum id="1">V</definiendum>
				<definiendum id="2">S C V</definiendum>
				<definiens id="0">the usual definition of a context-free grammar ( CFG ) as a 4-tuple G = ( E , V , R , S )</definiens>
				<definiens id="1">the set of terminals</definiens>
				<definiens id="2">the set of nonterminals , R is the set of productions</definiens>
				<definiens id="3">the start symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>Definition 2 A c-parser is an algorithm that takes a CFG grammar G = ( E , V , R , S ) and string w E E* as input and produces output ~G , w ; J : G , w acts as an oracle about parse information , as follows : • If A c-derives w~ , then .7 : G , w ( A , i , j ) = `` yes `` .</sentence>
				<definiendum id="0">c-parser</definiendum>
				<definiens id="0">an algorithm that takes a CFG grammar G = ( E , V , R , S</definiens>
			</definition>
			<definition id="3">
				<sentence>So as it stands , the class of c-parsers includes tabular parsers ( e.g. CKY ) , where 5rG , w is the table of substring derivations , and Earley-type parsers , where ~'G , ~ is the chart .</sentence>
				<definiendum id="0">5rG , w</definiendum>
				<definiens id="0">the class of c-parsers includes tabular parsers</definiens>
				<definiens id="1">the table of substring derivations , and Earley-type parsers</definiens>
			</definition>
			<definition id="4">
				<sentence>Valiant ( personal communication ) notes that there is a reduction of m × m Boolean matrix multiplication checking to context-free recognition of strings of length m2 ; this reduction is alluded to in a footnote of a paper by Harrison and Havel ( 1974 ) .</sentence>
				<definiendum id="0">Valiant</definiendum>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>Each sentence-tree pair ( S , T ) in a language has an associated top-down derivation consisting of a sequence of rule applications of a grammar .</sentence>
				<definiendum id="0">sentence-tree pair ( S , T )</definiendum>
				<definiens id="0">in a language has an associated top-down derivation consisting of a sequence of rule applications of a grammar</definiens>
			</definition>
			<definition id="1">
				<sentence>n The re-write rules are either internal to the tree , where LHS is a non-terminal and RHS is a string 7~ ( T , S ) 17~ ( S ) is constant , hence maximising ~ is equivalent to maximising `` P ( T , S ) .</sentence>
				<definiendum id="0">LHS</definiendum>
				<definiendum id="1">RHS</definiendum>
				<definiens id="0">a non-terminal and</definiens>
				<definiens id="1">a string 7~ ( T , S ) 17~ ( S ) is constant</definiens>
			</definition>
			<definition id="2">
				<sentence>of one or more non-terminals ; or lexical , where LHS is a part of speech tag and RHS is a word .</sentence>
				<definiendum id="0">LHS</definiendum>
				<definiendum id="1">RHS</definiendum>
				<definiens id="0">a part of speech tag</definiens>
				<definiens id="1">a word</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus we write a nonterminal as X ( x ) , where x = ( w , t ) , and X is a constituent label .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a constituent label</definiens>
			</definition>
			<definition id="4">
				<sentence>Each rule now has the form3 : P ( h ) - &gt; Ln ( In ) ... ni ( ll ) H ( h ) Rl ( rl ) ... Rm ( rm ) ( 3 ) H is the head-child of the phrase , which inherits the head-word h from its parent P. L1 ... L~ and R1 ... Rm are left and right modifiers of H. Either n or m may be zero , and n = m = 0 for unary rules .</sentence>
				<definiendum id="0">H</definiendum>
				<definiens id="0">the head-child of the phrase</definiens>
				<definiens id="1">inherits the head-word h from its parent P. L1 ... L~ and R1 ... Rm are left and right modifiers of H. Either n or m may be zero</definiens>
			</definition>
			<definition id="5">
				<sentence>R , ~+l ( r , ~+l ) is defined as STOP -the STOP symbol is added to the vocabulary of nonterminals , and the model stops generating right modifiers when it is generated .</sentence>
				<definiendum id="0">R , ~+l</definiendum>
				<definiens id="0">STOP -the STOP symbol is added to the vocabulary of nonterminals , and the model stops generating right modifiers when it is generated</definiens>
			</definition>
			<definition id="6">
				<sentence>The probability of the phrase S ( bought ) - &gt; NP ( week ) NP-C ( Marks ) VP ( bought ) is now : 7 ) h ( VPIS , bought ) x to ( { NP-C } I S , VP , bought ) x t S , VP , bought ) × 7~/ ( NP-C ( Marks ) IS , VP , bought , { NP-C } ) x 7 : ~I ( NP ( week ) I S , VP , bought , { } ) x 7 ) l ( STOe I S , ve , bought , { } ) × Pr ( STOP I S , VP , bought , { } ) Here the head initially decides to take a single NP-C ( subject ) to its left , and no complements ~A rnultiset , or bag , is a set which may contain duplicate non-terminal labels .</sentence>
				<definiendum id="0">probability of the phrase S ( bought</definiendum>
				<definiendum id="1">NP-C</definiendum>
				<definiendum id="2">~I</definiendum>
				<definiendum id="3">bought , { } ) × Pr</definiendum>
				<definiendum id="4">NP-C</definiendum>
				<definiens id="0">a set which may contain duplicate non-terminal labels</definiens>
			</definition>
			<definition id="7">
				<sentence>A couple of complexities are that modification by an SBAR does not always involve extraction ( e.g. , `` the fact ( SBAR that besoboru is 19 NP ( store ) NP ( store ) SBAR ( that ) ( +gap ) The store WHNP ( that ) WDT I that ( i ) NP - &gt; NP ( 2 ) SBAR ( +gap ) - &gt; WHNP ( 3 ) S ( +gap ) - &gt; NP-C ( 4 ) VP ( +gap ) - &gt; VB S ( bought ) ( - } -gap ) N P-C ( ~ht ) ( -- { -gap ) I B~w Marks V eek ) I I bought last week SBAR ( +gap ) S-C ( +gap ) VP ( +gap ) TRACE NP Figure 5 : A +gap feature can be added to non-terminals to describe NP extraction .</sentence>
				<definiendum id="0">couple of complexities</definiendum>
				<definiendum id="1">SBAR</definiendum>
				<definiendum id="2">SBAR</definiendum>
			</definition>
			<definition id="8">
				<sentence>For example , Rule ( 2 ) , SBAR ( that ) ( +gap ) - &gt; WHNP ( that ) S-C ( bought ) ( +gap ) , has probability ~h ( WHNP I SBAR , that ) × 7~G ( Right I SBAR , WHNP , that ) x T~LC ( { } I SBAR , WHNP , that ) x T'Rc ( { S-C } \ [ SBAR , WHNP , that ) x 7~R ( S-C ( bought ) ( +gap ) \ [ SBAR , WHNP , that , { S-C , +gap } ) x 7~R ( STOP I SBAR , WHNP , that , { } ) x PC ( STOP I SBAR , WHNP , that , { } ) Rule ( 4 ) , VP ( bought ) ( +gap ) - &gt; VB ( bought ) TRACE NP ( week ) , has probability 7~h ( VB I VP , bought ) x PG ( Right I VP , bought , VB ) x PLC ( { } I VP , bought , VB ) x ~PRc ( { NP-C } I vP , bought , VB ) x 7~R ( TRACE I VP , bought , VB , { NP-C , +gap } ) x PR ( NP ( week ) I VP , bought , VB , { } ) × 7 ) L ( STOP I VP , bought , VB , { } ) x 7~R ( STOP I VP , bought , VB , { } ) In rule ( 2 ) Right is chosen , so the +gap requirement is added to RC .</sentence>
				<definiendum id="0">WHNP</definiendum>
				<definiendum id="1">WHNP</definiendum>
				<definiens id="0">+gap ) \ [ SBAR , WHNP , that , { S-C , +gap } ) x 7~R ( STOP I SBAR , WHNP , that , { } ) x PC ( STOP I SBAR , WHNP , that , { }</definiens>
			</definition>
			<definition id="9">
				<sentence>CBs is the average number of crossing brackets per sentence .</sentence>
				<definiendum id="0">CBs</definiendum>
				<definiens id="0">the average number of crossing brackets per sentence</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>To analyze the distribution of task/dialogue initiatives in collaborative planning dialogues , we annotated the TRAINS91 dialogues ( Gross , Allen , and Traum , 1993 ) as follows : each dialogue turn is given two labels , task initiative ( TI ) and dialogue initiative ( DI ) , each of which can be assigned one of two values , system or manager , depending on which agent holds the task/dialogue initiative during that turn .</sentence>
				<definiendum id="0">TRAINS91 dialogues</definiendum>
				<definiendum id="1">dialogue initiative ( DI</definiendum>
				<definiens id="0">one of two values , system or manager , depending on which agent holds the task/dialogue initiative during that turn</definiens>
			</definition>
			<definition id="1">
				<sentence>The Dempster-Shafer theory is a mathematical theory for reasoning under uncertainty which operates over a set of possible outcomes , O. Associated with each piece of evidence that may provide support for the possible outcomes is a basic probability assignment ( bpa ) , a function that represents the impact of the piece of evidence on the subsets of O. A bpa assigns a number in the range \ [ 0,1\ ] to each subset of O such that the numbers sum to 1 .</sentence>
				<definiendum id="0">Dempster-Shafer theory</definiendum>
				<definiens id="0">a mathematical theory for reasoning under uncertainty which operates over a set of possible outcomes</definiens>
				<definiens id="1">a basic probability assignment ( bpa ) , a function that represents the impact of the piece of evidence on the subsets of O. A bpa assigns a number in the range \ [ 0,1\ ] to each subset of O such that the numbers sum to 1</definiens>
			</definition>
			<definition id="2">
				<sentence>If this prediction disagrees with the actual value in the annotated data , Adjust-bpa is invoked to alter the bpa ' s for the observed cues , and Reset-current-bpa is invoked to adjust the current bpa ' s to reflect the actual initiative holder ( step 4 ) .</sentence>
				<definiendum id="0">Adjust-bpa</definiendum>
				<definiens id="0">the actual value in the annotated data</definiens>
			</definition>
			<definition id="3">
				<sentence>In the rThis is the value that yields the optimal results ( Figure 2 ) .</sentence>
				<definiendum id="0">rThis</definiendum>
				<definiens id="0">the value that yields the optimal results ( Figure 2 )</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>, ... , C , ~ ; V ) where the Ci are ( possibly hierarchical ) feature constraints on a sequence of the morphological parses , and V is an integer denoting the vote of the rule .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">an integer denoting the vote of the rule</definiens>
			</definition>
			<definition id="1">
				<sentence>The vote V is determined as n v = i=l where V ( Ci ) is the contribution of constraint Ci to the vote of the rule R. A ( generic ) constraint has the following form : C -\ [ ( fl : vl ) ( f2 : v2 ) &amp; 5 ... ( fro : vm ) \ ] where fi is the name of a morphological feature , and vi is one of the possible values for that feature .</sentence>
				<definiendum id="0">R. A ( generic</definiendum>
				<definiendum id="1">fi</definiendum>
				<definiendum id="2">vi</definiendum>
				<definiens id="0">determined as n v = i=l where V ( Ci ) is the contribution of constraint Ci to the vote of the rule</definiens>
			</definition>
			<definition id="2">
				<sentence>This unknown word processor has a ( nominal ) root lexicon which recognizes S + , where S is the Turkish surface alphabet ( in the two-level morphology sense ) , but then tries to interpret an arbitrary postfix string of the unknown word , as a sequence of Turkish suffixes subject to all morphographemic constraints ( Oflazer and Tfir , 1996 ) .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">the Turkish surface alphabet ( in the two-level morphology sense ) , but then tries to interpret an arbitrary postfix string of the unknown word , as a sequence of Turkish suffixes subject to all morphographemic constraints</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>This is a generalized linear model ( McCullagh and Nelder , 1989 ) with a linear predictor = wWx where x is the vector of the observed counts in the various conjunction categories for the particular adjective pair we try to classify and w is a vector of weights to be learned during training .</sentence>
				<definiendum id="0">x</definiendum>
				<definiens id="0">a vector of weights to be learned during training</definiens>
			</definition>
			<definition id="1">
				<sentence>where \ [ Cil stands for the cardinality of cluster i , and d ( z , y ) is the dissimilarity between adjectives z and y. We want to select the partition : Pmin that minimizes ~ , subject to the additional constraint that for each adjective z in a cluster C , 1 1 ICl1 d ( = , y ) &lt; -- IVl d ( = , y ) ( 1 ) where C is the complement of cluster C , i.e. , the other member of the partition .</sentence>
				<definiendum id="0">\ [ Cil</definiendum>
				<definiens id="0">the dissimilarity between adjectives z and y. We want to select the partition : Pmin that minimizes ~ , subject to the additional constraint that for each adjective z in a cluster C , 1 1 ICl1 d ( = , y ) &lt; -- IVl d ( = , y ) ( 1 ) where C is the complement of cluster C</definiens>
			</definition>
			<definition id="2">
				<sentence>Simulation experiments establish that very high levels of performance can be obtained with a modest number of links per word , even when the links themselves are not always correctly classified .</sentence>
				<definiendum id="0">Simulation experiments</definiendum>
				<definiens id="0">a modest number of links per word , even when the links themselves are not always correctly classified</definiens>
			</definition>
			<definition id="3">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>The word model is a set of probabilities that a word occurs with a tag ( part-of-speech ) when given the preceding words and their tags in a sentence .</sentence>
				<definiendum id="0">word model</definiendum>
				<definiens id="0">a set of probabilities that a word occurs with a tag ( part-of-speech ) when given the preceding words and their tags in a sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>Although stochastic taggers usually make use of subdivision level , part-of-speech level is remarkably robust 231 ( root ) 0i* ( noun ) ... .. ( adverb ) ( proper ) ( numeral ) ( declarative ) NTr AT &amp; T 1 2 part-of-speech level ( degree ) subdivision level word level Figure 1 : Hierarchical Tag Set against data sparseness .</sentence>
				<definiendum id="0">numeral )</definiendum>
				<definiendum id="1">part-of-speech level</definiendum>
			</definition>
			<definition id="2">
				<sentence>The bottom level is word level and is indispensable in coping with exceptional and collocational sequences of words .</sentence>
				<definiendum id="0">bottom level</definiendum>
				<definiens id="0">word level and is indispensable in coping with exceptional and collocational sequences of words</definiens>
			</definition>
			<definition id="3">
				<sentence>Tile basic tag set is a set of tile most detailed context elements that comprises the words selected above and part-of-speech subdivision level .</sentence>
				<definiendum id="0">Tile basic tag set</definiendum>
				<definiens id="0">a set of tile most detailed context elements that comprises the words selected above and part-of-speech subdivision level</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>DOP estimates the probability of substituting a subtree t on a specific node as the probability of selecting t among all subtrees in the corpus that could be substituted on that node .</sentence>
				<definiendum id="0">DOP</definiendum>
				<definiens id="0">estimates the probability of substituting a subtree</definiens>
			</definition>
			<definition id="1">
				<sentence>This probability is equal to the number of occurrences of a subtree t , divided by the total number of occurrences of subtrees t ' with the same root node label as t : P ( t ) = Itl/~t ' : root ( e ) =roo~ ( t ) It'l '' The probability of a derivation tl o ... o tn can be computed as the product of the probabilities of the subtrees this derivation consists of : P ( tl o..</sentence>
				<definiendum id="0">o tn</definiendum>
				<definiens id="0">equal to the number of occurrences of a subtree t , divided by the total number of occurrences of subtrees t ' with the same root node label as t : P ( t ) = Itl/~t ' : root ( e ) =roo~ ( t ) It'l '' The probability of a derivation tl o ...</definiens>
			</definition>
			<definition id="2">
				<sentence>The probability of an interpretation I of a string is the sum of the probabilities of the parses of this string with a top node annotated with a formula that is provably equivalent to I. Let ti4p be the i-th subtree in the derivation d that yields parse p with interpretation I , then the probability of I is given by : P ( I ) = E E H P ( t , d , ) ( 4 ) p d i We choose the most probable interpretation/ .</sentence>
				<definiendum id="0">probability of an interpretation</definiendum>
				<definiens id="0">the probability of I is given by : P ( I ) = E E H P ( t</definiens>
			</definition>
			<definition id="3">
				<sentence>tuplesO tuples ( T ) is the set of all pairs ( c , s ) in a treebank T , where c is a syntactic category , and s is the set of all semantic types that a constituent of category c in T can have .</sentence>
				<definiendum id="0">tuplesO tuples</definiendum>
				<definiendum id="1">c</definiendum>
				<definiendum id="2">s</definiendum>
				<definiens id="0">the set of all pairs ( c , s ) in a treebank T</definiens>
				<definiens id="1">the set of all semantic types that a constituent of category c in</definiens>
			</definition>
			<definition id="4">
				<sentence>apply ( ) if c is a category , s is a set of types , and T is a tree-bank then apply ( ( c , s ) , T ) yields a tree-bank T ' , by indexing each instance of category c in T , such that the c constituent is of semantic type t E s , with a unique index i. ambO if T is a tree-bank then arab ( T ) yields an n E N , such that n is the sum of the frequencies of all CFG rules R that occur in T with more than one corresponding semantic rule .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">a set of types</definiens>
				<definiens id="1">a tree-bank then apply ( ( c , s ) , T ) yields a tree-bank T ' , by indexing each instance of category c in T , such that the c constituent is of semantic type t E s , with a unique index i. ambO if T is a tree-bank then arab ( T ) yields an n E N , such that n is the sum of the frequencies of all CFG rules R that occur in T with more than one corresponding semantic rule</definiens>
			</definition>
			<definition id="5">
				<sentence>No grammar is used to determine the correct annotation ; there is a small set of guidelines , that has the degree of detail necessary to avoid an `` anything goes '' -attitude in the annotator , but leaves room for his/her perception of the structure of an utterance .</sentence>
				<definiendum id="0">anything</definiendum>
				<definiens id="0">goes '' -attitude in the annotator , but leaves room for his/her perception of the structure of an utterance</definiens>
			</definition>
			<definition id="6">
				<sentence>The interpretation of an utterance , is an update of an information state .</sentence>
				<definiendum id="0">interpretation of an utterance</definiendum>
				<definiens id="0">an update of an information state</definiens>
			</definition>
			<definition id="7">
				<sentence>An information state is a representation of objects and the relations between them , that complies to the frame structure .</sentence>
				<definiendum id="0">information state</definiendum>
				<definiens id="0">a representation of objects and the relations between them</definiens>
			</definition>
			<definition id="8">
				<sentence>An update expression is a set of paths through the frame structure , enhanced with pragmatic operators that have scope over a certain part of a path .</sentence>
				<definiendum id="0">update expression</definiendum>
				<definiens id="0">a set of paths through the frame structure , enhanced with pragmatic operators that have scope over a certain part of a path</definiens>
			</definition>
</paper>

		<paper id="1065">
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>The whole UDRS denotes the set of well-formed DRSs that can be formed by some plugging of the subUDRSs that does not violate the ordering &lt; .</sentence>
				<definiendum id="0">UDRS</definiendum>
				<definiens id="0">the set of well-formed DRSs that can be formed by some plugging of the subUDRSs that does not violate the ordering &lt;</definiens>
			</definition>
			<definition id="1">
				<sentence>Parse forests can represent an exponential number of phrase structure alternatives in o ( n 3 ) space , where n is the length of the sentence .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>Writing ¢ ( X1 , ... , Xk ) shall indicate that X1 ... . , Xk are the free variables in the constraint ~ .</sentence>
				<definiendum id="0">X1 , ... , Xk</definiendum>
				<definiens id="0">the free variables in the constraint ~</definiens>
			</definition>
			<definition id="3">
				<sentence>Therefore , let us abstract away this size by employing a function fa ( n ) that bounds the size of semantic structures ( respectively the size of its describing constraint system in normal form ) that grammar G assigns to sentences of length n. Finally , we want to assume that generalisation is simplifying and can be performed within a bound of g ( m ) steps , where m is the total size of the input constraint systems .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">Therefore , let us abstract away this size by employing a function fa ( n ) that bounds the size of semantic structures ( respectively the size of its describing constraint system in normal form ) that grammar G assigns to sentences of length n. Finally</definiens>
				<definiens id="1">the total size of the input constraint systems</definiens>
			</definition>
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>A pair of strings ( w , w ' ) is an aligned pair if IT\ [ = \ ] w'\ [ .</sentence>
				<definiendum id="0">pair of strings</definiendum>
				<definiens id="0">an aligned pair if IT\ [ = \ ] w'\ [</definiens>
			</definition>
			<definition id="1">
				<sentence>Furthermore , if factor ua with a E E is an ( implicit ) node of T such that h ( u ) but not h ( ua ) are ( implicit ) nodes of T ' , we create node u in T ( if u was an implicit node ) and establish an a-link from u to ( implicit ) node h ( u ) of T ' .</sentence>
				<definiendum id="0">E E</definiendum>
				<definiens id="0">an ( implicit ) node of T such that h ( u ) but not h ( ua ) are ( implicit ) nodes of T ' , we create node u in T ( if u was an implicit node ) and establish an a-link from u to ( implicit ) node h ( u ) of T '</definiens>
			</definition>
			<definition id="2">
				<sentence>Algorithm 2 Step 1 : construct two copies Tx and T x of the suffix tree associated with L× and align them using hi ; Step 2 : visit trees T× and T~ in post-order , and annotate each node p with the number e ( p ) computed as the sum of the counts at leaves that p dominates ; Step 3 : annotate each node p of T× with the score e ( p ) e ( p ' ) , where p ' = a-link ( p ) if a-link ( p ) is an actual node , p~ is the node immediately dominated by a-link ( p ) if a-link ( p ) is an implicit node , and e ( p ~ ) = 0 if a-link ( p ) is undefined ; make a list of the nodes with the highest annotated score .</sentence>
				<definiendum id="0">a-link</definiendum>
				<definiendum id="1">e</definiendum>
				<definiens id="0">construct two copies Tx and T x of the suffix tree associated with L× and align them using hi ; Step 2 : visit trees T× and T~ in post-order , and annotate each node p with the number e ( p ) computed as the sum of the counts at leaves that p dominates</definiens>
				<definiens id="1">annotate each node p of T× with the score e ( p ) e ( p '</definiens>
				<definiens id="2">an actual node , p~ is the node immediately dominated by</definiens>
				<definiens id="3">an implicit node , and</definiens>
			</definition>
			<definition id="3">
				<sentence>Let p be a node of Tx associated with factor u x v. Integer e ( p ) computed at Step 2 is the number of times a suffix having u x v as a prefix appears in strings in Lx .</sentence>
				<definiendum id="0">Integer e</definiendum>
				<definiens id="0">the number of times a suffix having u x v as a prefix appears in strings in Lx</definiens>
			</definition>
			<definition id="4">
				<sentence>Thus e ( p ) is the number of different positions at which factors u and v are aligned within Lx and hence the positive evidence of transformation u -- ~ v w.r.t. L , as defined in Section 2 .</sentence>
				<definiendum id="0">p )</definiendum>
				<definiens id="0">the number of different positions at which factors u and v</definiens>
			</definition>
			<definition id="5">
				<sentence>We next show that if pair ( p , q ) is found at Step 4 , then q represents a factor u x v , p represents a factor h2 ( u x v ) 7 , and transformation u7 ~ v -has the highest score among all transformations represented by nodes of Tx and Tr .</sentence>
				<definiendum id="0">p</definiendum>
			</definition>
</paper>

		<paper id="1066">
			<definition id="0">
				<sentence>the LEXTER software LEXTER is a terminology extraction software ( Bourigault et al. , 1996 ) .</sentence>
				<definiendum id="0">LEXTER software LEXTER</definiendum>
			</definition>
			<definition id="1">
				<sentence>LEXTER alSO extracts phraseological units ( PU ) which are `` informative collocations of the candidate terms '' .</sentence>
				<definiendum id="0">LEXTER alSO</definiendum>
				<definiens id="0">extracts phraseological units ( PU ) which are `` informative collocations of the candidate terms ''</definiens>
			</definition>
			<definition id="2">
				<sentence>For instance , LEXTER extracts the complex candidate term BUILT DISPATCHING LINE , and analyses it in ( BUILT ( DISPATCHING LINE ) ) ; the adjective BUILT will appear in the terminological context of DISPATCHING LINE and not in that of DISPATCHING .</sentence>
				<definiendum id="0">LEXTER</definiendum>
				<definiens id="0">extracts the complex candidate term BUILT DISPATCHING LINE , and analyses it in ( BUILT ( DISPATCHING LINE</definiens>
			</definition>
			<definition id="3">
				<sentence>LEXICLASS software LEXICLASS is a clustering tool written using C language and specialised data analysis functions from Splus TM software .</sentence>
				<definiendum id="0">LEXICLASS software LEXICLASS</definiendum>
				<definiens id="0">a clustering tool written using C language and specialised data analysis functions from Splus TM software</definiens>
			</definition>
			<definition id="4">
				<sentence>LEXICLASS is a generic clustering module , it only needs nominal ( or verbal ) compounds described by dependancy relationships .</sentence>
				<definiendum id="0">LEXICLASS</definiendum>
				<definiens id="0">nominal ( or verbal ) compounds described by dependancy relationships</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>Given a word , its context and its possible meanings , the problem of word sense disambiguation ( WSD ) is to determine the meaning of the word in that context .</sentence>
				<definiendum id="0">WSD</definiendum>
				<definiens id="0">to determine the meaning of the word in that context</definiens>
			</definition>
			<definition id="1">
				<sentence>( 4 ) spec subj /-'~ // the boy chased a brown dog The local context of a word W is a triple that corresponds to a dependency relationship in which W is the head or the modifier : ( type word position ) where type is the type of the dependency relationship , such as subj ( subject ) , adjn ( adjunct ) , compl ( first complement ) , etc. ; word is the word related to W via the dependency relationship ; and position can either be head or rood .</sentence>
				<definiendum id="0">type</definiendum>
				<definiendum id="1">word</definiendum>
				<definiens id="0">a triple that corresponds to a dependency relationship in which W is the head or the modifier : ( type word position</definiens>
				<definiens id="1">the type of the dependency relationship , such as subj ( subject ) , adjn ( adjunct ) , compl ( first complement ) , etc. ;</definiens>
			</definition>
			<definition id="2">
				<sentence>An entry in the database is a pair : ( 6 ) ( tc , C ( tc ) ) where Ic is a local context and C ( lc ) is a set of ( word frequency likelihood ) -triples .</sentence>
				<definiendum id="0">Ic</definiendum>
				<definiens id="0">a local context</definiens>
				<definiens id="1">a set of ( word frequency likelihood ) -triples</definiens>
			</definition>
			<definition id="3">
				<sentence>We use instead an information-theoretic definition of similarity that can be derived from the following assumptions : Assumption 1 : The commonality between A and B is measured by I ( common ( A , B ) ) where common ( A , B ) is a proposition that states the commonalities between A and B ; I ( s ) is the amount of information contained in the proposition s. Assumption 2 : The differences between A and B is measured by I ( describe ( A , B ) ) I ( common ( A , B ) ) where describe ( A , B ) is a proposition that describes what A and B are .</sentence>
				<definiendum id="0">B )</definiendum>
				<definiendum id="1">B ; I</definiendum>
				<definiendum id="2">B ) ) I</definiendum>
				<definiendum id="3">describe ( A , B )</definiendum>
				<definiens id="0">The commonality between A and B is measured by I ( common ( A , B ) ) where common ( A ,</definiens>
				<definiens id="1">a proposition that states the commonalities between A and</definiens>
			</definition>
			<definition id="4">
				<sentence>Assumption 3 : The similarity between A and B , sire ( A , B ) , is a function of their commonality and differences .</sentence>
				<definiendum id="0">sire</definiendum>
				<definiendum id="1">B )</definiendum>
				<definiens id="0">a function of their commonality and differences</definiens>
			</definition>
			<definition id="5">
				<sentence>Assumption 4 : Similarity is independent of the unit used in the information measure .</sentence>
				<definiendum id="0">Similarity</definiendum>
				<definiens id="0">independent of the unit used in the information measure</definiens>
			</definition>
			<definition id="6">
				<sentence>When b = 2 , I ( s ) is the number of bits needed to encode s. Since log~ , , Assumption 4 means that the funclogbx = logb , b , tion f must satisfy the following condition : Vc &gt; O , f ( x , y ) = f ( cz , cy ) Assumption 5 : Similarity is additive with respect to commonality .</sentence>
				<definiendum id="0">Similarity</definiendum>
				<definiens id="0">the number of bits needed to encode s. Since log~</definiens>
			</definition>
			<definition id="7">
				<sentence>If common ( A , B ) consists of two independent parts , then the sim ( A , B ) is the sum of the similarities computed when each part of the commonality is considered .</sentence>
				<definiendum id="0">B )</definiendum>
				<definiendum id="1">B )</definiendum>
				<definiens id="0">consists of two independent parts , then the sim ( A ,</definiens>
				<definiens id="1">the sum of the similarities computed when each part of the commonality is considered</definiens>
			</definition>
			<definition id="8">
				<sentence>A compromise between these two is sim ( Sans~er , Skew ) &gt; _ 0.27 , where 0.27 is the average similarity of 50,000 randomly generated pairs ( w , w ' ) in which w and w ~ belong to the same Roget 's category .</sentence>
				<definiendum id="0">0.27</definiendum>
				<definiens id="0">the average similarity of 50,000 randomly generated pairs ( w , w '</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>One of the most important relation to be extracted from machine-readable dictionaries ( MRD ) is the hyponym/hypernym relation among dictionary senses ( e.g. ( Amsler , 1981 ) , ( Vossen and Serail , 1990 ) ) not only because of its own importance as the backbone of taxonomies , but also because this relation acts as the support of main inheritance mechanisms helping , thus , the acquisition of other relations and semantic features ( Cohen and Loiselle , 1988 ) , providing formal structure and avoiding redundancy in the lexicon ( Briscoe et al. , 1990 ) .</sentence>
				<definiendum id="0">machine-readable dictionaries</definiendum>
				<definiendum id="1">MRD</definiendum>
				<definiens id="0">the hyponym/hypernym relation among dictionary senses</definiens>
			</definition>
			<definition id="1">
				<sentence>While DGILE is a good example of a large sized dictionary , LPPL shows to what extent the smallest dictionary is useful .</sentence>
				<definiendum id="0">LPPL</definiendum>
				<definiens id="0">a good example of a large sized dictionary ,</definiens>
			</definition>
			<definition id="2">
				<sentence>This heuristic is of limited application : LPPL lacks semantic tags , and less than 10 % of the definitions in DGILE are marked with one of the 96 different semantic domain tags ( e.g. med .</sentence>
				<definiendum id="0">LPPL</definiendum>
				<definiens id="0">lacks semantic tags , and less than 10 % of the definitions in DGILE are marked with one of the 96 different semantic domain tags ( e.g. med</definiens>
			</definition>
			<definition id="3">
				<sentence>Given a hyponym definition ( O ) and a set of candidate hypernym definitions , this method selects the candidate hypernym ( E ) which returns the maximum score following formula ( 2 ) : CV ( O , E ) = sim ( Vo , VE ) ( 2 ) The similarity ( sim ) between two definitions can be measured by the dot product , the cosine function or the Euclidean distance between two vectors ( Vo and VE ) which represent the contexts of the words presented in the respective definitions following formula ( 3 ) : t % el = eiv ( wd ( 3 ) wi6De , f The vector for a definition ( VDel ) is computed adding the cooccurrence information vectors of the words in the definition ( civ ( wi ) ) .</sentence>
				<definiendum id="0">VE</definiendum>
				<definiendum id="1">VDel</definiendum>
				<definiens id="0">computed adding the cooccurrence information vectors of the words in the definition ( civ ( wi ) )</definiens>
			</definition>
			<definition id="4">
				<sentence>This paper has presented a general technique for WSD which is a combination of statistical and knowledge based methods , and which has been applied to disambiguate all the genus terms in two dictionaries .</sentence>
				<definiendum id="0">WSD</definiendum>
				<definiens id="0">a combination of statistical and knowledge based methods</definiens>
				<definiens id="1">applied to disambiguate all the genus terms in two dictionaries</definiens>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>The IBM model 1 ( Brown et al. , 1993 ) is used to find an initial estimate of the translation probabilities .</sentence>
				<definiendum id="0">IBM model 1</definiendum>
				<definiens id="0">used to find an initial estimate of the translation probabilities</definiens>
			</definition>
			<definition id="1">
				<sentence>where E is the size of t.he target language vocabulary and I , , , ,~ .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">the size of t.he target language vocabulary and I , , , ,~</definiens>
			</definition>
			<definition id="2">
				<sentence>'K-vec : A New Approach for Aligning Parallel Texts '' , In Proc .</sentence>
				<definiendum id="0">'K-vec</definiendum>
				<definiens id="0">A New Approach for Aligning Parallel Texts ''</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>The statistical approach to machine translation ( SMT ) can be understood as a word-by-word model consisting of two sub-models : a language model for generating a source text segment S and a translation model for mapping S to its translation T. Brown et al. ( 1993 ) also recommend using a bilingual corpus to train the parameters of Pr ( S I 73 , translation probability ( TP ) in the translation model .</sentence>
				<definiendum id="0">SMT</definiendum>
				<definiens id="0">a word-by-word model consisting of two sub-models : a language model for generating a source text segment S and a translation model for mapping S to its translation T. Brown et al. ( 1993 ) also recommend using a bilingual corpus to train the parameters of Pr ( S I 73 , translation probability ( TP ) in the translation model</definiens>
			</definition>
			<definition id="1">
				<sentence>Constraints Image IP techniques Alignment Pattern Resolution Structure Edge Convolution Phrase preserving One-to-one Texture Feature Sentence extraction Non-crossing Line Hough Discourse transform information ( Ker and Chang 1996 ) , cognates ( Simard 1992 ) , K-vec ( Fung and Church 1994 ) , DTW ( Fung and McKeown 1994 ) , etc .</sentence>
				<definiendum id="0">Constraints Image IP</definiendum>
				<definiens id="0">techniques Alignment Pattern Resolution Structure Edge Convolution Phrase preserving One-to-one Texture Feature Sentence extraction Non-crossing Line Hough Discourse transform information</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>It thus becomes possible to develop and test HPSG grammars in a computational system without having to recode them as phrase structure or definite clause grammars .</sentence>
				<definiendum id="0">HPSG</definiendum>
				<definiens id="0">grammars in a computational system without having to recode them as phrase structure or definite clause grammars</definiens>
			</definition>
			<definition id="1">
				<sentence>Our example grammar consists of some universal principles , phrase structure rules and a lexicon .</sentence>
				<definiendum id="0">example grammar</definiendum>
				<definiens id="0">consists of some universal principles , phrase structure rules and a lexicon</definiens>
			</definition>
			<definition id="2">
				<sentence>phrase A head : verb A subeat : \ [ \ ] A bar : two A comp-dtr : wfs ( PO , P1 ) A head-rift : wfs ( P1 , P ) phrase A head : verb A subcat : ne-list A head-dtr : wfs ( PO , P1 ) A eomp-dtr : wfs ( P1 , P ) word A head : verb A bar : zero A subcat : \ [ head : noun , head : noun\ ] A phon : ( loves A X ) word A head : verb A bar : zero A subcat : \ [ head : noun\ ] h phon : ( sleeps A X ) word A head : noun A bar : two A subcat : \ [ \ ] A phon : ( ( arthur V tintageO A X ) Figure 7 : Phrase structure rules and the lexicon head-dtr : subcat : \ [ XIY \ ] Figure 8 : X-bar theory , head feature principle and subcat principle The advantages of such a modular encoding of grammatical principles are obvious .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">loves A X ) word A head : verb A bar : zero A subcat</definiens>
			</definition>
			<definition id="3">
				<sentence>Interpretation As a guiding principle , the interpreter follows the ideas of the Andorra Model 6 in that it always executes deterministic goals before nondeterministic ones .</sentence>
				<definiendum id="0">interpreter</definiendum>
				<definiens id="0">follows the ideas of the Andorra Model 6 in that it always executes deterministic goals before nondeterministic ones</definiens>
			</definition>
			<definition id="4">
				<sentence>ALE provides relations and type constraints ( i.e. , only types as antecedents ) , but their unfolding is neither lazy , nor can it be controlled by the user in any way .</sentence>
				<definiendum id="0">ALE</definiendum>
				<definiens id="0">provides relations and type constraints</definiens>
			</definition>
			<definition id="5">
				<sentence>The Andorra-I preprocessor : Supporting full Prolog on the Basic Andorra model .</sentence>
				<definiendum id="0">Andorra-I preprocessor</definiendum>
				<definiens id="0">Supporting full Prolog on the Basic Andorra model</definiens>
			</definition>
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>*Research supported in part by NSF grant IRI9314969 , DARPA AASERT award DAAH04-95-1-0475 , and the ATR Interpreting Telecommunications Research Laboratories .</sentence>
				<definiendum id="0">*Research</definiendum>
			</definition>
			<definition id="1">
				<sentence>First is the decay of the probability of a word t as the distance k from the most recent occurrence of its mate s increases .</sentence>
				<definiendum id="0">First</definiendum>
			</definition>
			<definition id="2">
				<sentence>Upper row : All non-self ( left ) and self triggers ( middle ) appearing fewer than 100 times in the Nikkei corpus , and the curve for the possessive particle ¢9 ( right ) .</sentence>
				<definiendum id="0">Upper row</definiendum>
				<definiendum id="1">self triggers</definiendum>
				<definiens id="0">middle ) appearing fewer than 100 times in the Nikkei corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>We then build a family of conditional exponential models of the general form p ( w I H ) = 1 ( = ) Z~-ff~ exp Aifi ( H , w ) q ( wlw_l , w_2 ) where H = w-t , w-2 ... . , w_N is the word history , and Z ( H ) is the normalization constant Z ( H ) ~= E exp ( E Aifi ( H ' , q ( w l w_l , w-2 ) The functions fl , which depend both on the word history H and the word being predicted , are called features , and each feature fi is assigned a weight Ai .</sentence>
				<definiendum id="0">w_N</definiendum>
				<definiendum id="1">Z ( H )</definiendum>
				<definiens id="0">the word history</definiens>
			</definition>
			<definition id="4">
				<sentence>Suppose first that our distance model is a simple one-parameter exponential , p ( k I sl E H , w = ti ) = # i e -m~ .</sentence>
				<definiendum id="0">distance model</definiendum>
				<definiens id="0">a simple one-parameter exponential , p</definiens>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>Of the phe394 Phenomenon Example 'Do It ' Anaphora 'Do So ' Anaphora Stripping Comparative Deletion The Theory A clause conveys a property or eventuality , or describes a situation , or expresses a proposition .</sentence>
				<definiendum id="0">clause</definiendum>
				<definiens id="0">conveys a property or eventuality , or describes a situation , or expresses a proposition</definiens>
			</definition>
			<definition id="1">
				<sentence>A property consists of a predicate applied to a number of arguments .</sentence>
				<definiendum id="0">property</definiendum>
			</definition>
			<definition id="2">
				<sentence>A crucial piece of our treatment of VP-ellipsis is the explicit representation of coreference relations , denoted with the predicate Core\ ] .</sentence>
				<definiendum id="0">VP-ellipsis</definiendum>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>Throughout processing , the algorithm maintains a constraint network N which is a pair relating ( a ) a set of constraints , which correspond to predications over variables ( properties abstracted from the individuals they apply to ) to ( b ) sets of variables each of which fulfill these constraints in view of a given knowledge base ( the context sets ) .</sentence>
				<definiendum id="0">constraint network N</definiendum>
				<definiendum id="1">constraints</definiendum>
				<definiens id="0">correspond to predications over variables ( properties abstracted from the individuals they apply to ) to ( b ) sets of variables each of which fulfill these constraints in view of a given knowledge base ( the context sets )</definiens>
			</definition>
			<definition id="1">
				<sentence>The notation N ~ p is used to signify the result of adding the constraint p to the network N. In 209 Variable Description r , gr , v , gv R C N C L FD DD List &lt; p , r &gt; refs P-props excluded local ( r ) and global referents ( gr ) and variables ( v and gv ) associated with them a specification of slots which the target referring expression may entail ( contextually-motivated ) expected category of the intended referent constraint network , a pair relating a set of constraints to sets of variables fulfilled by them context set , indexed by variables associated with referents ( e.g. , C v , Cg v ) list of attribute-value pairs which corresponds to the constraint part of N functional description that is an appropriate lexical description expressing L distinguishing description , appearing as a pair &lt; L , FD &gt; communicative goals to pursue , expressed by Describe ( r , v ) property p ascribed to referent r referents already processed properties whose images on the lexical level are likely to fill empty slots in FD property-referent combinations that can not be verbalized in the given context Table 1 : Variables used in the algorithm addition , the notation \ [ r~v\ ] p is used to signify the result of replacing every occurrence of the constant r in p by variable v ( for an algorithm to maintain consistency see AC-3 ( Mackworth , 1977 ) , as used in ( Haddock , 1991 ) ) .</sentence>
				<definiendum id="0">Cg v</definiendum>
				<definiens id="0">contextually-motivated ) expected category of the intended referent constraint network , a pair relating a set of constraints to sets of variables fulfilled by them context set , indexed by variables associated with referents</definiens>
				<definiens id="1">an appropriate lexical description expressing L distinguishing description , appearing as a pair &lt; L , FD &gt; communicative goals to pursue , expressed by Describe ( r , v ) property p ascribed to referent r referents already processed properties whose images on the lexical level are likely to fill empty slots in FD property-referent combinations that can not be verbalized in the given context Table 1 : Variables used in the algorithm addition</definiens>
			</definition>
			<definition id="2">
				<sentence>It is additionally influenced by two parameters : refs , which specifies those referent which must be directly related to the chosen descriptor , and Pprops , which entails a list of properties whose lexical images are likely to fill yet empty slots .</sentence>
				<definiendum id="0">Pprops</definiendum>
				<definiens id="0">entails a list of properties whose lexical images are likely to fill yet empty slots</definiens>
			</definition>
			<definition id="3">
				<sentence>Function Description Next-Property ( refs , ps ) A ( p ) find-best-value ( A ( p ) , V ) basic-level-value ( r , A ( p ) ) rules-out ( &lt; A ( p ) , V &gt; ) Assoc-var ( r ) Prototypical ( p , r ) Descriptors ( r ) Map-to ( Empty-Slots ( FD ) ) lnsert-Unify ( FD , &lt; v , p &gt; ) Check-Scope ( FD ) Slots-of ( mappings ( p ) ) Rel ( A ( p ) ) Salient ( A ( p ) ) selects a property , influenced by the connection to referents refs and by properties ps functor to provide access to the predicate of predication p procedure to determine the value of property p that describes r according to ( Dale , Reiter , 1992 ) yields the basic level value of property p for referent r yields the set of referents that are ruled out as distractors due to the value V of property A ( p ) function to get access to the variable associated with referent r yields true if property p is prototypical for referent r and false otherwise yields the set of predicates entailed in N and holding for referent r yields properties which map onto the set of uninstantiated slots in FD inserts a lexical description of property p of the referent associated with variable v into FD yields true if no scope problems are expected to occur and false otherwise yields the slots of the set of lexical items by which predicate p can be expressed yields true if descriptor p is a relation and false otherwise yields true if salience is assigned to property p and false otherwise Table 2 : Functions used in the algorithm 210 D_ .</sentence>
				<definiendum id="0">Function Description Next-Property</definiendum>
				<definiendum id="1">Map-to ( Empty-Slots</definiendum>
				<definiendum id="2">Rel</definiendum>
				<definiendum id="3">Salient</definiendum>
				<definiens id="0">A ( p ) )</definiens>
			</definition>
			<definition id="4">
				<sentence>( lh~ ( r , v , N , R , c ) DD ~ nil , FD ~ nil \ [ CI\ ] unique ~ false \ [ C2\ ] gr ~-r , gv 6-v \ [ C3\ ] excluded ~-nil , P-props ~-nil \ [ C4\ ] rel : ~ ~ { r } \ [ C5l Cv~ Cvn { x I c ( x ) } \ [ C6\ ] List +-\ [ Describe ( r , v ) \ ] \ [ C7\ ] if ICxvl = 1 then \ [ C9\ ] • unique + -- true \ [ CI0\ ] return &lt; L , FD &gt; ( as a distinguishing description ) \ [ CI I\ ] endif \ [ C 12\ ] if IRI = 0 then \ [ C13\ ] return &lt; L , FD &gt; \ [ C 14\ ] ( as a non-distinguishing description ) \ [ C 15\ ] endif \ [ C 16\ ] repeat \ [ C18\ ] &lt; r , p &gt; ~-Next-Property ( rely , P-props ) \ [ C19\ ] if p = nil then \ [ C20\ ] return &lt; L , FD &gt; \ [ C21\ ] ( as a non-distinguishing description ) lC22\ ] endif \ [ C23\ ] v ~ Assoc-var ( r ) \ [ C24\ ] if Prototypical ( p , r ) or \ [ C25\ ] ( ( Slots-of ( Mappings ( p ) ) n R ) = O ) \ [ C26\ ] then excluded ~ excluded u { &lt; r , p &gt; I \ [ C27\ ] elseif ( p in Taxonomic-Inferences \ [ C28\ ] ( Descriptors ( v ) ) ) or ( ICvl -1 ) then \ [ C29\ ] excluded ~excluded u { &lt; r , p &gt; } \ [ C30\ ] endif \ [ C31 \ ] endif \ [ C32\ ] if &lt; r , p &gt; e excluded then lC33\ ] goto 2 \ [ C34\ ] endif \ [ C35\ ] V = find-best-value ( A ( p ) , \ [ C36\ ] basic-level-value ( r , A ( p ) ) ) \ [ C37\ ] if not ( ( ( rules-out ( &lt; A ( p ) , V &gt; ) ~ nil ) and ( V ~ nil ) ) \ [ C38\ ] or Rel ( A ( p ) ) ) or Salient ( A ( p ) ) then \ [ C39\ ] excluded ~ excluded u { &lt; r , p &gt; } \ [ C40\ ] goto 2 \ [ C41 \ ] endif \ [ C42\ ] FDH ~ Insert-Unify ( FD , &lt; v , p &gt; ) \ [ C43\ ] if not Check-Scope ( FDH ) then \ [ C44\ ] excluded ~ excluded u { &lt; r , p &gt; } \ [ C45\ ] goto 2 \ [ C46\ ] endif \ [ C47\ ] FD ~ FDH \ [ C49\ ] R ~-R \ slots ( FD ) \ [ C50\ ] P-props ~-Map-to ( Empty-slots ( FD ) ) \ [ C51\ ] p ~ \ [ r\vlp \ [ C52\ ] if Rel ( A ( p ) ) then \ [ C53\ ] for every other constant r ' in p do \ [ C54\ ] if Assoc-var ( r ' ) = nil then \ [ C55\ ] associate r ' with a new , unique variable v ' \ [ C56\ ] p ~-\ [ r'~vqp \ [ C57\ ] ref~ ~-rel ; ~ u { r7 \ [ C58\ ] List ~ Append ( List , Describe ( r'v ' ) ) \ [ C59\ ] endif \ [ C60\ ] next \ [ C61 \ ] else set the value of attribute p to V \ [ C62\ ] endif \ [ C63\ ] N ~N @ p \ [ C64\ ] goto 1 \ [ C65\ ] Figure 2 : Detailed pseudo-code of the new algorithm The first part of the algorithm , 'Check Success ' , comprises the algorithm 's termination criteria : \ [ $ 2- $ 4\ ] , the exit in case of full success .</sentence>
				<definiendum id="0">Cv~ Cvn { x I c</definiendum>
				<definiendum id="1">FD &gt; ( as</definiendum>
				<definiendum id="2">] ( Descriptors</definiendum>
				<definiendum id="3">Rel</definiendum>
				<definiendum id="4">Salient</definiendum>
				<definiendum id="5">] P-props ~-Map-to ( Empty-slots</definiendum>
				<definiens id="0">a distinguishing description</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>First is the issue of filtering term lists ; this has been dealt with by constraints on processing and by post-processing overgenerated lists .</sentence>
				<definiendum id="0">First</definiendum>
				<definiens id="0">the issue of filtering term lists</definiens>
			</definition>
			<definition id="1">
				<sentence>• Morpho-syntaetic ( Type 2 ) : the content words of the original term or one of their derivatives are found in the variant .</sentence>
				<definiendum id="0">Morpho-syntaetic</definiendum>
				<definiens id="0">the content words of the original term or one of their derivatives are found in the variant</definiens>
			</definition>
			<definition id="2">
				<sentence>The system consists of precomputing stems , extracted from a large dictionary of French ( Boyer , 1993 ) enhanced with newspaper corpora , a total of over 85,000 entries .</sentence>
				<definiendum id="0">system</definiendum>
			</definition>
			<definition id="3">
				<sentence>Third , derivational morphology ( Tzoukermann and Jacquemin , 1997 ) is achieved to generate morphological variants of the disambiguated words .</sentence>
				<definiendum id="0">derivational morphology</definiendum>
				<definiens id="0">achieved to generate morphological variants of the disambiguated words</definiens>
			</definition>
			<definition id="4">
				<sentence>tion is the replacement of a content word by a term ; a modification is the insertion of a modifier without reference to another term .</sentence>
				<definiendum id="0">tion</definiendum>
				<definiendum id="1">modification</definiendum>
				<definiens id="0">the replacement of a content word by a term ; a</definiens>
				<definiens id="1">the insertion of a modifier without reference to another term</definiens>
			</definition>
			<definition id="5">
				<sentence>The precision and recall of the extraction of term variants are given in Table 4 where precision is the ratio of correct variants among the variants extracted and the recall is the ratio of variants retrieved among the collocates .</sentence>
				<definiendum id="0">precision</definiendum>
				<definiendum id="1">recall</definiendum>
				<definiens id="0">the ratio of variants retrieved among the collocates</definiens>
			</definition>
			<definition id="6">
				<sentence>A T NAT P ) { ( ACAv T P ) I ( pDT ATNA T CAv T pT ) ) D T A T ) Ns .</sentence>
				<definiendum id="0">T NAT P</definiendum>
				<definiens id="0">ACAv T P ) I ( pDT ATNA T CAv T pT ) ) D T A T ) Ns</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>`` ek , which postulates a source sentence of length l and its first k words .</sentence>
				<definiendum id="0">ek</definiendum>
				<definiens id="0">postulates a source sentence of length l and its first k words</definiens>
			</definition>
			<definition id="1">
				<sentence>In our experiments , we used C = PPtrain +log ( Pmax ) , where Pm== is the maximum ngram probability in the language model .</sentence>
				<definiendum id="0">Pm==</definiendum>
				<definiens id="0">the maximum ngram probability in the language model</definiens>
			</definition>
			<definition id="2">
				<sentence>So we have e ( i I J , t , m ) = e ( ilj , l ' , m ' ; e , g ) ( 9 ) ( I-l ' ) ~+ ( m-m ' ) ~ &lt; r~ ; e , g where ~ ( i I J , l , m ) is the adjusted count for the parameter a ( i I J , 1 , m ) , c ( i I J , l , m ; e , g ) is the expected count for a ( i I J , l , m ) from a paired sentence ( e g ) , and c ( ilj , l , m ; e , g ) = 0 when lel l , or Igl ¢ m , or i &gt; l , or j &gt; m. Although ( 9 ) can moderate the severity of the first data sparse problem , it does not ease the second inefficiency problem at all .</sentence>
				<definiendum id="0">g )</definiendum>
				<definiendum id="1">c</definiendum>
				<definiens id="0">the expected count for a ( i I J , l , m ) from a paired sentence ( e g ) , and</definiens>
			</definition>
			<definition id="3">
				<sentence>This results in a simplified translation model , in which the alignment parameters are independent of the sentence length 1 and m : P ( ilj , m , e ) = P ( ilj , l , m ) -a ( i l J ) here i , j &lt; Lm , and L , n is the maximum sentence length allowed in the translation system .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">in which the alignment parameters are independent of the sentence length 1 and m : P ( ilj , m , e ) = P ( ilj , l</definiens>
				<definiens id="1">the maximum sentence length allowed in the translation system</definiens>
			</definition>
			<definition id="4">
				<sentence>In this case , a hypothesis can be expressed as H = el , e2 , ... , ek , and IHI is used to denote the length of the sentence prefix of the hypothesis H , in this case , k. Since we do not make assumption of the source sentence length , the heuristics described above can no longer be applied .</sentence>
				<definiendum id="0">IHI</definiendum>
				<definiens id="0">H = el , e2 , ... , ek , and</definiens>
				<definiens id="1">used to denote the length of the sentence prefix of the hypothesis H</definiens>
			</definition>
			<definition id="5">
				<sentence>We used the Red-Black tree data structure ( Cormen , Leiserson , and Rivest , 1990 ) to implement the dynamic set , which guarantees that the above operations take O ( log n ) time in the worst case , where n is the number of search states in the set .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of search states in the set</definiens>
			</definition>
			<definition id="6">
				<sentence>370 Model 2 , Single Stack Model 2 , Multi-Stack Simplified Model Total Test Sentences Decoded Sentenced Failed sentences 120 32 88 120 83 37 120 120 0 Table 1 : Decoder Success Rate Correct Okay Incorrect German English ( target ) English ( output ) German English/target ) English ( output ) German English ( target ) English/output/ German English/target ) English ( output ) German English ( target ) English ( output ) German English ( target ) English ( output ) ich habe ein Meeting yon halb zehn bis um zwSlf I have a meeting from nine thirty to twelve I have a meeting from nine thirty to twelve versuchen wir sollten es vielleicht mit einem anderen Termin we might want to try for some other time we should try another time ich glaube nicht diis ich noch irgend etwas im Januar frei habe I do not think I have got anything open m January I think I will not free in January ich glaube wit sollten em weiteres Meeting vereinbaren I think we have to have another meeting I think we should fix a meeting schlagen Sie doch einen Termin vor why do n't you suggest a time why you an appointment ich habe Zeit fiir den Rest des Tages I am free the rest of it I have time for the rest of July Table 2 : Examples of Correct , Okay , and Incorrect Translations : for each translation , the first line is an input German sentence , the second line is the human made ( target ) translation for that input sentence , and the third line is the output from the decoder .</sentence>
				<definiendum id="0">Incorrect Translations</definiendum>
				<definiens id="0">Multi-Stack Simplified Model Total Test Sentences Decoded Sentenced Failed sentences 120 32 88 120 83 37 120 120 0 Table 1 : Decoder Success Rate Correct Okay Incorrect German English ( target ) English ( output ) German English/target ) English ( output</definiens>
			</definition>
			<definition id="7">
				<sentence>JANUS : Towards multilingual spoken language translation .</sentence>
				<definiendum id="0">JANUS</definiendum>
			</definition>
</paper>

		<paper id="1062">
			<definition id="0">
				<sentence>parse stack `` bought '' synt : verb top of top of stack list • `` &lt; input list &gt; , `` today '' synt adv ( R 2 TO S-VP AS PRED ( OBJ PAT ) ) `` reduce the 2 top elements of the parse stack to a frame with syntax 'vp ' and roles 'pred ' and 'obj and pat ' '' 1 ~ `` bought a book ... . today '' synt : vp synt : adv sub : ( pred ) ( obj pat ) / I `` bought '' synt : verb Figure 1 : Example of a parse action ( simplified ) ; boxes represent frames The central data structure for the parser consists of a parse stack and an input list .</sentence>
				<definiendum id="0">simplified ) ; boxes represent</definiendum>
				<definiens id="0">R 2 TO S-VP AS PRED ( OBJ PAT ) ) `` reduce the 2 top elements of the parse stack to a frame with syntax 'vp ' and roles 'pred '</definiens>
				<definiens id="1">frames The central data structure for the parser consists of a parse stack and an input list</definiens>
			</definition>
			<definition id="1">
				<sentence>Labeled ( l. ) precision/recall measures not only structural correctness , but also the correctness of the syntactic label .</sentence>
				<definiendum id="0">Labeled</definiendum>
				<definiens id="0">measures not only structural correctness , but also the correctness of the syntactic label</definiens>
			</definition>
			<definition id="2">
				<sentence>Correct operations ( Ops ) measures the number of correct operations during a parse that is continuously corrected based on the logged sequence .</sentence>
				<definiendum id="0">Correct operations ( Ops )</definiendum>
				<definiens id="0">measures the number of correct operations during a parse that is continuously corrected based on the logged sequence</definiens>
			</definition>
</paper>

		<paper id="1068">
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>The tagger consists of the following sequentially applied modules : ( a ) Lexical component ( b ) Rule-based guesser for unknown words The tagger uses a two-level morphological analyser with a large lexicon and a morphological description that introduces about 180 different ambiguity-forming morphological analyses , as a result of which each word gets 1.7-2.2 different analyses on an average .</sentence>
				<definiendum id="0">tagger</definiendum>
				<definiens id="0">consists of the following sequentially applied modules : ( a ) Lexical component ( b ) Rule-based guesser for unknown words The tagger uses a two-level morphological analyser with a large lexicon and a morphological description that introduces about 180 different ambiguity-forming morphological analyses , as a result of which each word gets 1.7-2.2 different analyses on an average</definiens>
			</definition>
			<definition id="1">
				<sentence>then both taggers ( a new version known as En~CG-21 with 3,600 constraints as five subgrammars- , and a statistical tagger ) are applied to the same held-out benchmark corpus of 55,000 words , and their performances are compared .</sentence>
				<definiendum id="0">both taggers</definiendum>
				<definiens id="0">a new version known as En~CG-21 with 3,600 constraints as five subgrammars- , and a statistical tagger ) are applied to the same held-out benchmark corpus of 55,000 words , and their performances are compared</definiens>
			</definition>
			<definition id="2">
				<sentence>that f. is approximately -- , N ( p , ~/~ ) , where p is the actual disagreement probability and n is the number of trials , i.e. , the corpus size .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the number of trials , i.e. , the corpus size</definiens>
			</definition>
			<definition id="3">
				<sentence>Here X , is the random variable of assigning a tag to the tth word and xj is the last tag of the tag sequence encoded as state sj .</sentence>
				<definiendum id="0">xj</definiendum>
				<definiens id="0">the random variable of assigning a tag to the tth word and</definiens>
			</definition>
			<definition id="4">
				<sentence>An English constraint grammar ( EngCG ) : a surface-syntactic parser of English .</sentence>
				<definiendum id="0">English constraint grammar</definiendum>
				<definiendum id="1">EngCG</definiendum>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>The Smooth Injective Map Recognizer ( SIMR ) algorithm presented here is a generic pattern recognition algorithm that is particularly well-suited to mapping bitext correspondence .</sentence>
				<definiendum id="0">Smooth Injective Map Recognizer ( SIMR</definiendum>
				<definiens id="0">a generic pattern recognition algorithm that is particularly well-suited to mapping bitext correspondence</definiens>
			</definition>
			<definition id="1">
				<sentence>SIMR builds bitext maps one chain at a time .</sentence>
				<definiendum id="0">SIMR</definiendum>
				<definiens id="0">builds bitext maps one chain at a time</definiens>
			</definition>
			<definition id="2">
				<sentence>Within this search rectangle , SIMR generates all the points of correspondence that satisfy the supplied matching predicate , as explained in Section 3.1 .</sentence>
				<definiendum id="0">SIMR</definiendum>
				<definiens id="0">generates all the points of correspondence that satisfy the supplied matching predicate</definiens>
			</definition>
			<definition id="3">
				<sentence>SIMR employs a simple heuristic to select regions of the bitext space to search .</sentence>
				<definiendum id="0">SIMR</definiendum>
				<definiens id="0">employs a simple heuristic to select regions of the bitext space to search</definiens>
			</definition>
			<definition id="4">
				<sentence>A matching predicate is a heuristic for deciding whether a given pair of tokens are likely to be'mutual translations .</sentence>
				<definiendum id="0">matching predicate</definiendum>
				<definiens id="0">a heuristic for deciding whether a given pair of tokens are likely to be'mutual translations</definiens>
			</definition>
			<definition id="5">
				<sentence>The TBM consists of a set of TPCs .</sentence>
				<definiendum id="0">TBM</definiendum>
			</definition>
			<definition id="6">
				<sentence>The most tedious part of the porting process is the construction of TBMs against which SIMR 's parameters can be optimized and tested .</sentence>
				<definiendum id="0">most tedious part of the porting process</definiendum>
				<definiens id="0">the construction of TBMs against which SIMR 's parameters can be optimized and tested</definiens>
			</definition>
			<definition id="7">
				<sentence>Axis generators need to be built only once per language , rather than once per language pair .</sentence>
				<definiendum id="0">Axis generators</definiendum>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>LCS composition is a fundamental operation in two applications for which the LCS serves as an interlingua : machine translation ( Dorr et al..</sentence>
				<definiendum id="0">LCS composition</definiendum>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>Katakana writing is a syllabary rather than an alphabet -- there is one symbol for ga ( ~I ) , another for gi ( 4e ) , another for gu ( P ' ) , etc .</sentence>
				<definiendum id="0">Katakana writing</definiendum>
				<definiens id="0">a syllabary rather than an alphabet -- there is one symbol for ga ( ~I )</definiens>
			</definition>
			<definition id="1">
				<sentence>Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora ( a.k.a. `` not-found words '' ) .</sentence>
				<definiendum id="0">Katakana phrases</definiendum>
				<definiens id="0">the largest source of text phrases that do not appear in bilingual dictionaries or training corpora ( a.k.a. `` not-found words '' )</definiens>
			</definition>
			<definition id="2">
				<sentence>A WFSA is an state/transition diagram with weights and symbols on the transitions , making some output sequences more likely than others .</sentence>
				<definiendum id="0">WFSA</definiendum>
				<definiens id="0">an state/transition diagram with weights and symbols on the transitions</definiens>
			</definition>
			<definition id="3">
				<sentence>III u 0.076 AO o 0.671 o o 0.257 a 0.047 AW a u 0.830 a w 0.095 o o 0.027 a o 0.020 a 0.014 AY a i 0.864 i 0.073 a 0.018 a i y 0.018 B b 0.802 b u 0.185 CH ch y 0.277 ch 0.240 tch i 0.199 ch i 0.159 tch 0.038 ch y u 0.021 tch y 0.020 DH d 0.535 d o 0.329 dd o 0.053 j 0.032 z 0.670 z u 0.125 j 0.125 a z 0.080 EH e 0.901 a 0.069 ER a a 0.719 a 0.081 a r 0.063 e r 0.042 o r 0.029 e E¥ J P ( J l e ) e e 0.641 a 0.122 e 0.114 e i 0.080 a i 0.014 F h 0.623 h u 0.331 hh 0.019 a h u 0.010 G g 0.598 g u 0.304 gg u 0.059 gg 0.010 HH h 0.959 w 0.014 IH i 0.908 e 0.071 IY i i 0.573 i 0.317 e 0.074 e e 0.016 JR j 0.329 j y 0.328 j i 0.129 jj i 0.066 e j i 0.057 z 0.032 g 0.018 jj 0.012 e 0.012 k 0.528 k u 0.238 kk u 0.150 kk 0.043 k i 0.015 k y 0.012 L r 0.621 r u 0.362 M m 0.653 m u 0.207 n 0.123 n m 0.011 N n 0.978 NG n g u 0.743 n 0.220 n g 0.023 e j P ( j I e ) OW o 0.516 o o 0.456 o u 0.011 OY o i 0.828 o o i 0.057 i 0.029 o i y 0.029 o 0.027 o o y 0.014 o o 0.014 P p 0.649 p u 0.218 pp u 0.085 pp 0.045 PAUSE pause 1.000 R r 0.661 a 0.170 o 0.076 r u 0.042 u r 0.016 a r 0.012 s u 0.539 s 0.269 sh 0.109 u 0.028 ss 0.014 8H shy 0.475 sh 0.175 ssh y u 0.166 ssh y 0.088 sh i 0.029 ssh 0.027 shy u 0.015 t 0.463 t o 0.305 tt o 0.103 ch 0.043 tt 0.021 ts 0.020 ts u 0.011 TH s u 0.418 s 0.303 sh 0.130 ch 0.038 t 0.029 e j PUle ) UH u 0.794 u u 0.098 dd 0.034 a 0.030 o 0.026 UW u u 0.550 u 0.302 y u u 0.109 y u 0.021 V b 0.810 b u 0.150 w 0.015 W w 0.693 u 0.194 o 0.039 £ 0.027 a 0.015 e 0.012 y 0.652 i 0.220 y u 0.050 u 0.048 b 0.016 z 0.296 z u 0.283 j 0.107 s u 0.103 u 0.073 a 0.036 o 0.018 s 0.015 n 0.013 i 0.011 sh 0.011 ZH j y 0.324 sh i 0.270 j i 0.173 j 0.135 a j y u 0.027 shy 0.027 s 0.027 a j i 0.016 Figure 1 : English sounds ( in capitals ) with probabilistic mappings to Japanese sound sequences ( in lower case ) , as learned by estimation-maximization .</sentence>
				<definiendum id="0">e E¥ J P ( J l e</definiendum>
				<definiens id="0">English sounds ( in capitals ) with probabilistic mappings to Japanese sound sequences</definiens>
			</definition>
			<definition id="4">
				<sentence>• ( n y e ) is a rare sound sequence , but is written -~* when it occurs .</sentence>
				<definiendum id="0">n y e )</definiendum>
			</definition>
			<definition id="5">
				<sentence>Perhaps uncharitably , we can view optical character recognition ( OCR ) as a device that garbles perfectly good katakana sequences .</sentence>
				<definiendum id="0">OCR</definiendum>
				<definiens id="0">a device that garbles perfectly good katakana sequences</definiens>
			</definition>
			<definition id="6">
				<sentence>Next comes the P ( jlk ) model , which produces a 28-state/31-arc WFSA whose highest-scoring sequence is : mas ut aazut o o ch im ent o Next comes P ( elj ) , yielding a 62-state/241-arc WFSA whose best sequence is : M AE S T AE AE DH UH T AO AO CH IH M EH N T AO 133 Next to last comes P ( wle ) , which results in a 2982state/4601-arc WFSA whose best sequence ( out of myriads ) is : masters tone am ent awe This English string is closest phonetically to the Japanese , but we are willing to trade phonetic proximity for more sensical English ; we restore this WFSA by composing it with P ( w ) and extract the best translation : masters tournament ( Other Section 1 examples are translated correctly as earth day and robert scan leonard . )</sentence>
				<definiendum id="0">P ( jlk ) model</definiendum>
				<definiens id="0">produces a 28-state/31-arc WFSA whose highest-scoring sequence is : mas ut aazut o o ch im ent o Next comes P ( elj ) , yielding a 62-state/241-arc WFSA whose best sequence is : M AE S T AE AE DH UH T AO AO CH IH M EH N T AO 133 Next to last comes P ( wle ) , which results in a 2982state/4601-arc WFSA whose best sequence</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>Features This work adopts the following notation for regular formalisms , cf. ( Kaplan and Kay , 1994 ) : r ( =~ , &lt; = , &lt; ~ } A___p ( 1 ) where T , A and p are n-way regular expressions which describe same-length relations ) ( An n-way regular expression is a regular expression whose terms 3This analysis is along the lines of ( McCarthy , 1981 ) based on autosegmental phonology ( Goldsmith , 1976 ) .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">n-way regular expressions which describe same-length relations ) ( An n-way regular expression is a regular expression whose terms</definiens>
			</definition>
			<definition id="1">
				<sentence>A compound rule takes the form r { ~ , ~ , ¢ , } ~l___pl ; ~2__p2 ; ... ( 2 ) To accommodate for rule features , each rule may be associated with an ( n -j ) -tuple of feature structures , each of the form \ [ attributel =vall , attribute , =val2 , . . .\ ] ( 3 ) i.e. , an unordered set of attribute=val pairs .</sentence>
				<definiendum id="0">compound rule</definiendum>
			</definition>
			<definition id="2">
				<sentence>As a second illustration , R6 derives the simple p'al measure , /ktab/ .</sentence>
				<definiendum id="0">R6</definiendum>
				<definiens id="0">derives the simple p'al measure</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , if S is a tuple of strings and 0p ( S ) is an operator defined on S , the operator can be extended to a relation R in the following manner op ( n ) = { Op ( 3 ) I s e n } Definition3.1 ( Identity ) Let L be a regular language .</sentence>
				<definiendum id="0">S )</definiendum>
				<definiens id="0">a tuple of strings</definiens>
				<definiens id="1">an operator defined on S , the operator can be extended to a relation R in the following manner op</definiens>
			</definition>
			<definition id="4">
				<sentence>S ' ... Sk substitutes every occurrence of I in S with S ' .</sentence>
				<definiendum id="0">Sk</definiendum>
				<definiens id="0">substitutes every occurrence of I in S with S '</definiens>
			</definition>
			<definition id="5">
				<sentence>Definition 3.4 ( Projection ) Let S = ( st ... . , s , , ) be a tuple of strings , projec'ci ( S ) , for some i 6 { 1 ... .. n } , denotes the tuple element si .</sentence>
				<definiendum id="0">Projection</definiendum>
				<definiens id="0">st ... . , s , , ) be a tuple of strings , projec'ci ( S ) , for some i 6 { 1 ... .. n } , denotes the tuple element si</definiens>
			</definition>
			<definition id="6">
				<sentence>The operator o represents mathematical composition , not necessarily the composition of transducers .</sentence>
				<definiendum id="0">operator o</definiendum>
				<definiens id="0">represents mathematical composition , not necessarily the composition of transducers</definiens>
			</definition>
			<definition id="7">
				<sentence>If a two-level grammar is compiled into an automaton , denoted by Gram , and a lexicon is compiled into an automaton , denoted by Lez , the automaton which enforces lexical constraints on the language is expressed by L = ( Proj , ctl ( ~ ) * × Lex ) A Gram ( 18 ) The first component above is a relation which accepts any surface symbol on its first tape and the lexicon on the remaining tapes .</sentence>
				<definiendum id="0">Gram</definiendum>
				<definiens id="0">a relation which accepts any surface symbol on its first tape</definiens>
			</definition>
</paper>

		<paper id="1073">
			<definition id="0">
				<sentence>Most prefixes appear in distinct but semantically related rules , resulting in polysemou , s prefixes .</sentence>
				<definiendum id="0">Most prefixes</definiendum>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>In addition , there is a subcategory headless_PP which consists of a subset of noun phrases which typically occur in a locative prepositional phrase with the preposition omitted .</sentence>
				<definiendum id="0">subcategory headless_PP</definiendum>
				<definiens id="0">consists of a subset of noun phrases which typically occur in a locative prepositional phrase with the preposition omitted</definiens>
			</definition>
			<definition id="1">
				<sentence>Once we allow the parser to take part-of-speech as the input , the partsof-speech ( rather than actual words ) will appear as the terminal symbols in the parse tree , and hence as the vocabulary items in the semantic frame representation .</sentence>
				<definiendum id="0">partsof-speech</definiendum>
				<definiens id="0">the terminal symbols in the parse tree , and hence as the vocabulary items in the semantic frame representation</definiens>
			</definition>
</paper>

	</volume>
