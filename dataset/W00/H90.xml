<?xml version="1.0" encoding="UTF-8"?>
	<volume id="H90">

		<paper id="1052">
			<definition id="0">
				<sentence>The fourth line is an instance of a noun phrase with head government , which is the object of the verb control but is followed by no preposition .</sentence>
				<definiendum id="0">head government</definiendum>
				<definiens id="0">the object of the verb control but is followed by no preposition</definiens>
			</definition>
			<definition id="1">
				<sentence>The last line represents an instance of the ambiguity we are concerned with resolving : a noun phrase ( head is concession ) , which is the object of a verb ( grant ) , followed by a preposition ( to ) .</sentence>
				<definiendum id="0">noun phrase</definiendum>
				<definiendum id="1">head</definiendum>
				<definiens id="0">the object of a verb ( grant ) , followed by a preposition ( to )</definiens>
			</definition>
			<definition id="2">
				<sentence>( 2 ) Moscow sent more than 100,000 soldiers into Afganistan . . . The idea is to contrast the probability with which into occurs with the noun soldier with the probability with which into occurs with the verb send .</sentence>
				<definiendum id="0">idea</definiendum>
				<definiens id="0">to contrast the probability with which into occurs with the noun soldier with the probability with which into occurs with the verb send</definiens>
			</definition>
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>DIRC is a kind of intelligent , natural language-capable consultant kit that can be retargeted at different domains .</sentence>
				<definiendum id="0">DIRC</definiendum>
				<definiens id="0">a kind of intelligent , natural language-capable consultant kit that can be retargeted at different domains</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , in `` John took a book from Mary '' , John is both the recipient and the agent .</sentence>
				<definiendum id="0">John</definiendum>
				<definiens id="0">both the recipient and the agent</definiens>
			</definition>
			<definition id="2">
				<sentence>The MIDAS meuic is a simple sum of two factors : ( i ) the length of the core-relationship fi'om the input source to the source of the candidate metaphor , and ( ii ) hierarchical distance between the two concepts .</sentence>
				<definiendum id="0">MIDAS meuic</definiendum>
				<definiens id="0">a simple sum of two factors : ( i ) the length of the core-relationship fi'om the input source to the source of the candidate metaphor , and ( ii ) hierarchical distance between the two concepts</definiens>
			</definition>
			<definition id="3">
				<sentence>The Very Idea : A Case-Study in Polysemy and Cross-Lexical GeneraliTation .</sentence>
				<definiendum id="0">Very Idea</definiendum>
				<definiens id="0">A Case-Study in Polysemy and Cross-Lexical GeneraliTation</definiens>
			</definition>
			<definition id="4">
				<sentence>, Machine Learning : An Artificial Intelligence Approach , vol .</sentence>
				<definiendum id="0">Machine Learning</definiendum>
				<definiens id="0">An Artificial Intelligence Approach</definiens>
			</definition>
			<definition id="5">
				<sentence>Machine Learning : An Artificial Intelligence Approach .</sentence>
				<definiendum id="0">Machine Learning</definiendum>
			</definition>
			<definition id="6">
				<sentence>The Inherent Semantics of Argument Structure : The Case of the English Ditransitive Construction .</sentence>
				<definiendum id="0">Inherent Semantics of Argument Structure</definiendum>
				<definiens id="0">The Case of the English Ditransitive Construction</definiens>
			</definition>
</paper>

		<paper id="1028">
</paper>

		<paper id="1096">
</paper>

		<paper id="1090">
</paper>

		<paper id="1074">
			<definition id="0">
				<sentence>, O~N } as , where N~ is the number of phonetic units in 81 , Sj is the jth possible sequence of N~ connecting segments traversing the utterance , p ( c~ik Isjk ) is the probability of observing a phone in a given segment , and p ( sik ) is the probability that a segment exists .</sentence>
				<definiendum id="0">N~</definiendum>
				<definiendum id="1">Sj</definiendum>
				<definiens id="0">the number of phonetic units in 81</definiens>
				<definiens id="1">the jth possible sequence of N~ connecting segments traversing the utterance , p ( c~ik Isjk ) is the probability of observing a phone in a given segment</definiens>
				<definiens id="2">the probability that a segment exists</definiens>
			</definition>
			<definition id="1">
				<sentence>We define p ( si ) to be the probability that all internal boundaries do not exist .</sentence>
				<definiendum id="0">p ( si )</definiendum>
				<definiens id="0">the probability that all internal boundaries do not exist</definiens>
			</definition>
			<definition id="2">
				<sentence>Our encouraging results suggested that MLP is a promising technique worthy of further investigation .</sentence>
				<definiendum id="0">MLP</definiendum>
				<definiens id="0">a promising technique worthy of further investigation</definiens>
			</definition>
			<definition id="3">
				<sentence>Let z~ = Ej wijxj , where zi is the input to unit i. If we assume that the weights , w~j , are randomly initialized such that they are uncorrelated with zero mean and constant variance .</sentence>
				<definiendum id="0">zi</definiendum>
				<definiens id="0">the input to unit i. If we assume that the weights , w~j , are randomly initialized such that they are uncorrelated with zero mean and constant variance</definiens>
			</definition>
			<definition id="4">
				<sentence>_ ) , ( 4 ) ~o'3 where ¢j is the standard deviation of sj , gj is the mean of sj over all the training tokens , and 7 is a positive constant .</sentence>
				<definiendum id="0">¢j</definiendum>
				<definiens id="0">the mean of sj over all the training tokens</definiens>
				<definiens id="1">a positive constant</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>In a Spoken Language System ( SLS ) we must use all available knowledge sources ( KSs ) to decide on the spoken sentence .</sentence>
				<definiendum id="0">Spoken Language System</definiendum>
				<definiens id="0">use all available knowledge sources ( KSs ) to decide on the spoken sentence</definiens>
			</definition>
</paper>

		<paper id="1087">
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>y 5 221 afte 6 157 dialy 7 94 poice 8 82 piots 9 77 spash absorbent adjusted dusted ambitious ambitions ambition compatibility compactability comparability computability after fate aft ate ante daily diary dials dial dimly dilly police price voice poise pice ponce poire pilots pivots riots plots pits pots pints pious splash smash slash spasm stash swash sash pash spas We decided to look at the 2-candidate case in more detail in order to test how often the top scoring candidate agreed with a panel of three judges .</sentence>
				<definiendum id="0">ambitious ambitions ambition compatibility compactability comparability computability</definiendum>
			</definition>
			<definition id="1">
				<sentence>The grouping variable is the expected frequency of the bigrarn if the words occurred independently .</sentence>
				<definiendum id="0">grouping variable</definiendum>
				<definiens id="0">the expected frequency of the bigrarn if the words occurred independently</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>The stack decoder is an attractive algorithm for controlling the acoustic and language model matching in a continuous speech recognizer .</sentence>
				<definiendum id="0">stack decoder</definiendum>
				<definiens id="0">an attractive algorithm for controlling the acoustic and language model matching in a continuous speech recognizer</definiens>
			</definition>
			<definition id="1">
				<sentence>The A* criterion is the difference between the actual log-likelihood of reaching a point in time on a path and an upper bound on the log-likelihood of any path reaching that point in time : hi ( t ) = Li ( t ) ubL ( t ) ( 1 ) where Ai ( t ) is the scoring function , Li ( t ) is the output log-likelihood , t denotes time , i denotes the path ( tree branch or left sentence fragment ) and ubL ( t ) is an upper bound on Li ( t ) .</sentence>
				<definiendum id="0">A* criterion</definiendum>
				<definiens id="0">the difference between the actual log-likelihood of reaching a point in time on a path and an upper bound on the log-likelihood of any path reaching that point in time : hi ( t ) = Li ( t ) ubL ( t ) ( 1 ) where Ai ( t ) is the scoring function</definiens>
				<definiens id="1">the output log-likelihood</definiens>
			</definition>
			<definition id="2">
				<sentence>The criterion of equation 3 can be improved by normalizing the observation probabilities by the A* criterion : t ubL ( t ) = ~ n~ax bj , o r at ( 4 ) r=l where ot is the observation at time t. This helps , but basic problems of equation 3 still remain .</sentence>
				<definiendum id="0">ot</definiendum>
				<definiens id="0">the observation at time t. This helps , but basic problems of equation 3 still remain</definiens>
			</definition>
			<definition id="3">
				<sentence>This limits the maximum number of theories to VT where V is the number of vocabulary words and T is the length of the input .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">the number of vocabulary words</definiens>
				<definiens id="1">the length of the input</definiens>
			</definition>
			<definition id="4">
				<sentence>Unigram and bigram language models \ [ 1\ ] are trivial -- the unigram model is not a function of the left context and the bigram model is a function of the same word which was used in determining a match .</sentence>
				<definiendum id="0">Unigram</definiendum>
				<definiens id="0">a function of the same word which was used in determining a match</definiens>
			</definition>
			<definition id="5">
				<sentence>\ [ 10\ ] A. B. Poritz , `` Hidden Markov Models : A Guided Tour , '' Proc .</sentence>
				<definiendum id="0">Hidden Markov Models</definiendum>
				<definiens id="0">A Guided Tour , '' Proc</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>Comparator Software Both REF ( reference ) and HYP ( hypothesized ) answers were to be written in a CAS ( Common Answer Specification ) format that was a slight adaptation of the CAS originally developed by BBN , and which had been agreed on by the CAS/Comparator Task Group .</sentence>
				<definiendum id="0">HYP</definiendum>
				<definiens id="0">hypothesized ) answers were to be written in a CAS ( Common Answer Specification ) format that was a slight adaptation of the CAS originally developed by BBN</definiens>
			</definition>
			<definition id="1">
				<sentence>For instance , query bdO0cls , `` WHAT IS CLASS Y '' , has the REF answer ( CY '' ) ) , and one of the HYP answers supplied is : ( ( 'Y ' '~ ' `` COACH '' `` NO '' `` YES '' `` NO '' `` NO '' `` NONE '' `` 1234567 '' ) ) Our CAS specification counts this is a correct match , although it is indeterminate which of the tuple 's two `` Y '' fields was matched .</sentence>
				<definiendum id="0">REF answer</definiendum>
				<definiens id="0">indeterminate which of the tuple 's two `` Y '' fields was matched</definiens>
			</definition>
</paper>

		<paper id="1075">
			<definition id="0">
				<sentence>The first system , called EAR ( English Alphabet Recognizer ) , recognizes letters spoken in isolation .</sentence>
				<definiendum id="0">EAR</definiendum>
				<definiens id="0">recognizes letters spoken in isolation</definiens>
			</definition>
			<definition id="1">
				<sentence>Neural network segmenters have the important advantage of being easily retrained for different databases ( e.g. , telephone speech , continuous speech ) , whereas rule-based segmenters require substantial hu389 Study Conditions Speakers Approach Letters Results Brown 20 kHz Sampling 100 speakers HMM E-set 92.0 % ( 1987 ) 16.4 dB SNR ( multi-speaker ) Euler 6.67 kHz Sam100 speakers HMM 26 letters + 93.0 % et al. i pling ( telephone ( multi-speaker ) 10 digits + ( 1990 ) bandwidth ) 3 control words Lang Brown 's data 100 speakers Neural networks B , D , E , V 93.0 % et .</sentence>
				<definiendum id="0">Neural network segmenters</definiendum>
				<definiens id="0">the important advantage of being easily retrained for different databases ( e.g. , telephone speech , continuous speech</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>Individuals differ greatly in their language skills , in their problem solving skills , and in their attention spans .</sentence>
				<definiendum id="0">Individuals</definiendum>
				<definiens id="0">differ greatly in their language skills , in their problem solving skills , and in their attention spans</definiens>
			</definition>
</paper>

		<paper id="1070">
			<definition id="0">
				<sentence>: TYPE s : PAR ( c-published-document ) : ASSOC ( edition ) ) ) ) ( issue : POS verb : G-DERIV nil : SENSES ssuei SYNTAX ( one-obj io-rec ) EXAMPLE ( the stockroom issues supplies ) TYPE p PAR ( c-giving ) ASSOC ( supply ) S-DERIV ( ( -able adj tr-ability ) ( -ance noun tr-act ) ( -er noun tr-actor ) ) ) ( issue2 : SYNTAX ( one-obj io-rec ) : EXAMPLE ( I issued instructions ) : TYPE p : PAR ( c-informing ) : ASSOC ( produce ) : S-DERIV ( ( -ance noun tr-act ) ) ssue3 SYNTAX ( one-obj no-obj ) EXAMPLE ( good smells issue from the cake ) TYPE s PAR ( c-passive-moving ) ) ) ) The lexicon , by design , includes only the coarsest distinctions anlong word senses ; thus the financial sense of `` issue '' ( e. g. , a new securit , y ) falls under the same core sense as the latest `` issue '' of a , ma , gazine .</sentence>
				<definiendum id="0">PAR</definiendum>
				<definiendum id="1">c-published-document )</definiendum>
				<definiendum id="2">G-DERIV nil</definiendum>
				<definiendum id="3">PAR</definiendum>
				<definiens id="0">S-DERIV ( ( -ance noun tr-act ) ) ssue3 SYNTAX ( one-obj no-obj ) EXAMPLE ( good smells issue from the cake</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>This ensures A ) that it is possible to retrieve the canonical answer via SQL , B ) that even if the answer is empty or otherwise limited in content , it is possible for system developers to understand what was expected by looking at the SQL , and C ) the canonical answer contains the least amount of information needed to determine that the system produced the fight answer .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">the least amount of information needed to determine that the system produced the fight answer</definiens>
			</definition>
			<definition id="1">
				<sentence>The existing Class A definition excludes all sentences whose interpretation requires context outside the sentence itself , i.e. `` Which of those flights are non-stop ? ''</sentence>
				<definiendum id="0">definition</definiendum>
				<definiens id="0">excludes all sentences whose interpretation requires context outside the sentence itself</definiens>
			</definition>
			<definition id="2">
				<sentence>In an expression of the form `` flight number N '' , where N is a number , N will always be interpreted as referring to the flight number ( flight .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a number</definiens>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>I. June 1990 RM2 ( Four Speaker ) Test Set Speaker-Dependent Systems a. Word-Pair Grammar : BBN ( 2400 train ) BBN ( 600 train ) MIT/LL ( 2400 train ) MIT/LL ( 600 train ) b. No Grammar : MIT/LL ( 2400 train ) MIT/LL ( 600 train ) Total Sent Corr Sub Del Ins Err Err 98.5 1.1 0.5 0.1 1.7 11.3 97.3 2.1 0.6 0.4 3.1 20.0 98.7 0.9 0.4 0.2 97.4 1.7 0.9 0.5 3.1 20.0 Total Sent Corr Sub Del Ins Err Err 95.9 3.3 0.9 0.8 89.5 8.3 2.2 2.2 12.7 58.3 S~eaker-Independent Systems a. Word-Pair Grammar , 109-Speaker Training : Total Corr Sub Del Ins Err AT &amp; T ( first run ) 94.8 3.9 AT &amp; T ( 2nd ruddebugged ) 94.9 3.7 1.4 0.6 CMU 96.2 MIT/LL 94.8 SRI 94.1 SRI ( 109 + 12 train ) 95.6 3.4 SSI ( VQ FE , CI HMM BE ) 81.8 11.5 SSI ( SSI FE , CI HMM BE ) 85.8 10.4 SSI ( SSI FE , CD HMM BE ) 92.4 5.3 Sent Err 32.3 31.5 27.1 31.9 32.1 27.1 69.8 59.6 41.3 b. No Grammar , 109-Speaker Training : Total Sent Corr Sub Del Ins Err Err AT &amp; T ( first run ) 77.7 16.7 5.6 1.5 23.8 78.3 AT &amp; T ( 2nd rddebugged ) 77.7 16.7 CMU 81.9 14.8 3.4 1.8 19.9 74.4 MIT/LL 79.1 16.5 4.4 SRI 75.7 18.3 6.0 1.5 25.7 77.3 Table 1 .</sentence>
				<definiendum id="0">SSI</definiendum>
				<definiens id="0">Four Speaker ) Test Set Speaker-Dependent Systems a. Word-Pair Grammar</definiens>
			</definition>
			<definition id="1">
				<sentence>Results Reported to NIST for Previous Test Sets a. AT &amp; T ( 109-speaker training 2nd ruddebugged retest ) : Total Sent Corr Sub Del Ins Err Err AT &amp; T ( Feb '89 SI WPG ) 95.5 3.4 1.1 0.7 5.2 28.0 AT &amp; T ( Feb '89 SI NG ) 80.5 15.0 4.5 2.3 21.7 75.3 AT &amp; T ( Oct '89 SI WPG ) 96.2 2.9 AT &amp; T ( Oct '89 SI NG ) 80.6 14.5 5.0 2.6 22.0 76.7 b. BBN ( Feb '89 SI set not previously reported upon ) : Total Sent Corr Sub Del Ins Err Err BBN ( Feb '89 3-12 WPG ) 93.7 4.9 1.4 1.1 7.4 37.0 BBN ( Feb '89 SI-109* WPG ) 94.8 4.3 1.0 1.2 6.5 34.3 ( log* = &gt; 4360 sentence utterances used for training ) c. BBN ( Feb '89 SD set , speaker-adaptive ) : Total Sent Corr Sub Del Ins Err Err BBN ( Feb '89 SA-1 WPG ) 95.6 3.4 1.0 0.7 5.2 25.7 BBN ( Feb '89 SA-4 WPG ) 96.4 2.5 1.1 0.7 4.3 23.3 d. CMU ( 109-speaker training retest ) : Total Sent Corr Sub Del Ins Err Err CMU ( Feb '89 SI WPG ) 96.1 3.2 0.6 0.7 4.6 24.0 CMU ( Oct '89 SI WPG ) 96.2 2.7 1.0 1.0 4.8 28.0 e. SSI : ( June '88 set , 109 speaker training ) Total Sent Corr Sub Del Ins Err Err VQ FE , CI HMM BE , WPG 80.3 14.9 4.8 2.2 22.0 71.3 SSI FEY CI HMM BE , WPG 86.3 10.8 2.9 1.5 15.2 55.7 SSI FEY CD HMM BE , WPG 93.6 5.1 1.3 0.7 7.1 36.7 Table 2 .</sentence>
				<definiendum id="0">BBN</definiendum>
				<definiendum id="1">SSI FEY CI HMM BE</definiendum>
				<definiens id="0">Results Reported to NIST for Previous Test Sets a. AT &amp; T ( 109-speaker training 2nd ruddebugged retest ) : Total Sent Corr Sub Del Ins Err Err AT &amp; T ( Feb '89 SI WPG ) 95.5 3.4 1.1 0.7 5.2 28.0 AT &amp; T ( Feb '89 SI NG ) 80.5 15.0 4.5 2.3 21.7 75.3 AT &amp; T ( Oct '89 SI WPG ) 96.2 2.9 AT &amp; T ( Oct '89 SI NG ) 80.6 14.5 5.0 2.6 22.0 76.7 b. BBN ( Feb '89 SI set not previously reported upon ) : Total Sent Corr Sub Del Ins Err Err BBN ( Feb '89 3-12 WPG ) 93.7 4.9 1.4 1.1 7.4 37.0 BBN ( Feb '89 SI-109* WPG ) 94.8 4.3 1.0 1.2 6.5 34.3 ( log* = &gt; 4360 sentence utterances used for training ) c. BBN ( Feb '89 SD set , speaker-adaptive ) : Total Sent Corr Sub Del Ins Err Err BBN ( Feb '89 SA-1 WPG</definiens>
				<definiens id="1">e. SSI : ( June '88 set , 109 speaker training ) Total Sent Corr Sub Del Ins Err Err VQ FE , CI HMM BE</definiens>
			</definition>
</paper>

		<paper id="1103">
			<definition id="0">
				<sentence>of Military Applications of Speech Technology Introduction and Summary This section summarizes and assesses a representative sampling of current work in military applications of speech technology in the following areas : ( 1 ) narrowband ( 2400 b/s-4800 b/s ) and low-bit-rate ( 50-1200 b/s ) secure digital voice communications ; ( 2 ) speech recognition systems in fighter aircraft , military helicopters , battle management , and air traffic control training ; and ( 3 ) noise and interference suppression .</sentence>
				<definiendum id="0">speech recognition</definiendum>
				<definiens id="0">systems in fighter aircraft , military helicopters , battle management , and air traffic control training ; and ( 3 ) noise and interference suppression</definiens>
			</definition>
			<definition id="1">
				<sentence>The STU-III represents a marriage of a sophisticated speech algorithm , the Linear Predictive Coding ( LPC ) technique at 2.4 kb/s , with very large-scale integration ( VLSI ) digital signal processor ( DSP ) technology to allow development of a secure terminal which is small enough and low enough in cost to be widely used for secure voice communication over telephone circuits in the United States .</sentence>
				<definiendum id="0">STU-III</definiendum>
				<definiens id="0">a marriage of a sophisticated speech algorithm , the Linear Predictive Coding ( LPC ) technique at 2.4 kb/s , with very large-scale integration ( VLSI ) digital signal processor ( DSP ) technology to allow development of a secure terminal</definiens>
			</definition>
			<definition id="2">
				<sentence>Training for military ( or civilian ) air traffic controllers ( ATC ) represents an excellent application for speech recognition systems .</sentence>
				<definiendum id="0">ATC )</definiendum>
				<definiens id="0">represents an excellent application for speech recognition systems</definiens>
			</definition>
			<definition id="3">
				<sentence>System Pilots in combat face an overwhelming quantity of incoming data or communications on which they must base life or death decisions .</sentence>
				<definiendum id="0">System Pilots</definiendum>
				<definiens id="0">in combat face an overwhelming quantity of incoming data or communications on which they must base life or death decisions</definiens>
			</definition>
			<definition id="4">
				<sentence>( Perplexity is a measure of the recognition task difficulty , and is defined as the probabilistically-weighted geometric mean branching factor of the language ( see , e.g. , \ [ 69\ ] , pp .</sentence>
				<definiendum id="0">Perplexity</definiendum>
				<definiens id="0">a measure of the recognition task difficulty</definiens>
				<definiens id="1">the probabilistically-weighted geometric mean branching factor of the language</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>We also describe an interpolated SDCN algorithm ( iSDCN ) which combines the simplicity of SDCN and the normalization capabilities of CDCN .</sentence>
				<definiendum id="0">iSDCN</definiendum>
				<definiens id="0">combines the simplicity of SDCN and the normalization capabilities of CDCN</definiens>
			</definition>
			<definition id="1">
				<sentence>Multi-Style Training Multi-style training is a technique in which the training set includes data representing different conditions so that the resulting HMM models are more robust to this variability .</sentence>
				<definiendum id="0">Multi-Style Training Multi-style training</definiendum>
				<definiens id="0">a technique in which the training set includes data representing different conditions so that the</definiens>
			</definition>
			<definition id="2">
				<sentence>CLSTK is the Sennheiser HMD224 , CRPZM is the Crown PZM6sf and MULTI means that the data from both microphones were used in training Liftering Many studies have examined several potential distortion measures for speech recognition in noise .</sentence>
				<definiendum id="0">CLSTK</definiendum>
				<definiendum id="1">CRPZM</definiendum>
				<definiendum id="2">MULTI</definiendum>
				<definiens id="0">means that the data from both microphones were used in training Liftering Many studies have examined several potential distortion measures for speech recognition in noise</definiens>
			</definition>
			<definition id="3">
				<sentence>The SNR is defined as the log-power of the signal in a frequency band minus the log-power of the noise in that band .</sentence>
				<definiendum id="0">SNR</definiendum>
				<definiens id="0">the log-power of the signal in a frequency band minus the log-power of the noise in that band</definiens>
			</definition>
			<definition id="4">
				<sentence>If we let the cepstral vectors x , n , y and q represent the Fourier series expansion of ln ex ( f ) , ln Pn ( f ) , ln Py ( f ) and in IH ( f ) 12 respectively , ( 1 ) can be rewritten as y = x + q + r ( x , n , q ) ( 2 ) where the correction vector r ( x , n , q ) is given by r ( x , n , q ) = IDFT { In ( 1 + eBb\ [ nq-x\ ] ) } ( 3 ) Let z be an estimate of y obtained through our spectral estimation algorithm .</sentence>
				<definiendum id="0">ln Py</definiendum>
				<definiens id="0">the cepstral vectors x , n , y and q represent the Fourier series expansion of ln ex ( f ) , ln Pn ( f ) ,</definiens>
			</definition>
			<definition id="5">
				<sentence>159 Although liftering provided very little improvement for our baseline system , this technique is actually complementary to SDCN : liftering techniques can be viewed as a variance normalization while SDCN is a biascompensation algorithm .</sentence>
				<definiendum id="0">SDCN</definiendum>
				<definiens id="0">a biascompensation algorithm</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>A Categoriai Plwaze Structure Grammar Development system , IBM UKSC Technical Report 198 , 1989 ( a ) Wetherill , C.S. , Probabilistic Languages : A Review and some Open Questions , Computing Surveys , vol .</sentence>
				<definiendum id="0">Probabilistic Languages</definiendum>
				<definiens id="0">Categoriai Plwaze Structure Grammar Development system</definiens>
			</definition>
			<definition id="1">
				<sentence>S : The pancreas , in addition to making digestive enzymes , is the body organ that produces insulin , a hormone that controls the level of sugar in the blood .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">the body organ that produces insulin , a hormone that controls the level of sugar in the blood</definiens>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>The spoken language system consists of the MIT SUMMIT speech recognition system ( \ [ 20\ ] ) loosely coupled to the UNISYS PUNDIT language understanding system ( \ [ 9\ ] ) with SUMMIT providing the top N candidates ( based on acoustic score ) to the PUNDIT system .</sentence>
				<definiendum id="0">spoken language system</definiendum>
			</definition>
			<definition id="1">
				<sentence>VOYAQER is a spoken language system for finding directions in Cambridge , Massachusetts .</sentence>
				<definiendum id="0">VOYAQER</definiendum>
			</definition>
			<definition id="2">
				<sentence>VFE takes SUMMIT 'S N-best output ( computed using a word-pair grammar of perplexity 60 ) , and sends it to PUNDIT for syntactic and semantic analysis .</sentence>
				<definiendum id="0">VFE</definiendum>
				<definiens id="0">takes SUMMIT 'S N-best output</definiens>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>Recognition Engine ( RE ) The Recognition Engine transforms the coded utterance stream into an ASCII string corresponding to the decoding of the input .</sentence>
				<definiendum id="0">Recognition Engine</definiendum>
				<definiendum id="1">Recognition Engine</definiendum>
				<definiens id="0">transforms the coded utterance stream into an ASCII string corresponding to the decoding of the input</definiens>
			</definition>
			<definition id="1">
				<sentence>Recognition imposes a high computational load and it is often impractical to have this process reside on a computer on which several applicatious ( themselves potentially requiring substantial resources ) are active .</sentence>
				<definiendum id="0">Recognition</definiendum>
				<definiens id="0">imposes a high computational load</definiens>
			</definition>
			<definition id="2">
				<sentence>In the current design , the intent is to communicate such information on an utterance by utterance basis .</sentence>
				<definiendum id="0">intent</definiendum>
				<definiens id="0">to communicate such information on an utterance by utterance basis</definiens>
			</definition>
			<definition id="3">
				<sentence>The Task Manager performs a control function comparable to that of the window manager in a window-oriented system .</sentence>
				<definiendum id="0">Task Manager</definiendum>
				<definiens id="0">performs a control function comparable to that of the window manager in a window-oriented system</definiens>
			</definition>
			<definition id="4">
				<sentence>The Office Manager ( OM ) To demonstrate our approach to speech interface design , we have implemented the Office Manager system , a system which is meant to provide the user with voice access to a number of common computer-based office applications .</sentence>
				<definiendum id="0">Office Manager ( OM</definiendum>
				<definiens id="0">a system which is meant to provide the user with voice access to a number of common computer-based office applications</definiens>
			</definition>
			<definition id="5">
				<sentence>In addition to the applications themselves , the OM understands a 36 word vocabulary , which is used to execute a variety of control functions , such as creating tasks , switching between them , invoking help , etc .</sentence>
				<definiendum id="0">OM</definiendum>
				<definiens id="0">understands a 36 word vocabulary</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>Delphi is BBN 's research NL system ( formerly called CFG ) , which is based on a unification grarnmax and which incorporates semantics into the unification framework .</sentence>
				<definiendum id="0">Delphi</definiendum>
				<definiens id="0">BBN 's research NL system ( formerly called CFG ) , which is based on a unification grarnmax and which incorporates semantics into the unification framework</definiens>
			</definition>
			<definition id="1">
				<sentence>Delphi is the NL component of the BBN HARC System .</sentence>
				<definiendum id="0">Delphi</definiendum>
				<definiens id="0">the NL component of the BBN HARC System</definiens>
			</definition>
			<definition id="2">
				<sentence>The Parlance TM system is a commercial NL interface to relational databases , and is based on an ATN parser and grammar .</sentence>
				<definiendum id="0">Parlance TM system</definiendum>
				<definiens id="0">a commercial NL interface to relational databases , and is based on an ATN parser and grammar</definiens>
			</definition>
</paper>

		<paper id="1073">
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>The mutual information statistic ( Fano 1961 ) is a measure of the interdependence of two signals in a message .</sentence>
				<definiendum id="0">mutual information statistic</definiendum>
				<definiens id="0">a measure of the interdependence of two signals in a message</definiens>
			</definition>
			<definition id="1">
				<sentence>bothoccurleft ( ab , cd ) = 1 if bigrams ab and cd appear in the corpus and percentageleft ( c , d ) 2 ( 11THRESHOLD * t ( a $ ) ) 0 otherwise When computing the relation between x and all other words , we use the following function , percentage , to weigh the evidence ( as described above ) , where count ( ab ) is the number of occurrences of the bigram ab in the corpus , and numright ( x ) ( numleft ( x ) ) is the total number of bigrams with x on their right hand side ( left hand side ) .</sentence>
				<definiendum id="0">11THRESHOLD * t</definiendum>
				<definiens id="0">the number of occurrences of the bigram ab in the corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>Inconsistency is measured in terms of the proportion of disagreements of each of the annotators with each other over the total number of words in the test corpus ( 5,425 words ) .</sentence>
				<definiendum id="0">Inconsistency</definiendum>
				<definiens id="0">measured in terms of the proportion of disagreements of each of the annotators with each other over the total number of words in the test corpus ( 5,425 words )</definiens>
			</definition>
</paper>

		<paper id="1095">
</paper>

		<paper id="1089">
</paper>

		<paper id="1084">
</paper>

		<paper id="1083">
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>For example , to resolve the noun phrase ships and planes we need to prove the expression ( 3 x , sl , y , s2 ) Plural ( x , sl ) A ship ( x ) A Plural ( u , s2 ) ^ plane ( u ) where Plural is taken to be a relation between the typical element of a set and the set itself .</sentence>
				<definiendum id="0">sl</definiendum>
				<definiendum id="1">ship</definiendum>
				<definiendum id="2">Plural</definiendum>
				<definiens id="0">^ plane ( u ) where Plural is taken to be a relation between the typical element of a set and the set itself</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>The HMMs accomplish non-linear time warping , otherwise known as `` dynamic time warping , '' of time domain acoustic information to properly match the rigid structure of a neural net template ( see Fig 1 ) .</sentence>
				<definiendum id="0">HMMs accomplish non-linear time warping</definiendum>
			</definition>
			<definition id="1">
				<sentence>Other work includes `` Merging Multilayer Perceptrons and Hidden Markov Models : Some Experiments in Continuous Speech Recognition '' ( ICSI , TR-89-033 , July 1989 ) , which evidently applies MLP ( a form of ANN ) to individual states inside HMM models .</sentence>
				<definiendum id="0">Hidden Markov Models</definiendum>
			</definition>
			<definition id="2">
				<sentence>Internal time variability in word pronunciation in multiple pronunciations of the same word must be managed and ANNs have no easy way to handle temporal variability without extraordinary requirements for silicon area .</sentence>
				<definiendum id="0">ANNs</definiendum>
				<definiens id="0">no easy way to handle temporal variability without extraordinary requirements for silicon area</definiens>
			</definition>
			<definition id="3">
				<sentence>The ANN uses different input parameters than the HMM triphone in SPHINX .</sentence>
				<definiendum id="0">ANN</definiendum>
				<definiens id="0">uses different input parameters than the HMM triphone in SPHINX</definiens>
			</definition>
			<definition id="4">
				<sentence>ANN TRAINING : Each word ANN is trained from time aligned data produced by the modified SPHINX .</sentence>
				<definiendum id="0">ANN TRAINING</definiendum>
				<definiens id="0">Each word ANN is trained from time aligned data produced by the modified SPHINX</definiens>
			</definition>
</paper>

		<paper id="1007">
</paper>

		<paper id="1098">
</paper>

		<paper id="1091">
</paper>

		<paper id="1031">
</paper>

		<paper id="1058">
</paper>

		<paper id="1067">
			<definition id="0">
				<sentence>INTRODUCTION Most hidden Markov model systems use minimum distortion ( MD ) vector quantizers ( encoders ) to convert continuously valued speech parameters into streams of integer codes .</sentence>
				<definiendum id="0">INTRODUCTION Most hidden Markov model systems</definiendum>
				<definiens id="0">minimum distortion ( MD ) vector quantizers ( encoders ) to convert continuously valued speech parameters into streams of integer codes</definiens>
			</definition>
			<definition id="1">
				<sentence>The encoders are trained as binary decision trees using maximization of the average mutual information I ( classes , codes ) between the set of target label classes and the set of leaf-node numbers ( codes ) , as the training criterion : I ( classes , codes ) = E Pr ( class , code ) *log ( Pr ( class , code ) / ( Pr ( class ) *Pr ( code ) ) ) , class code where Pr ( class , code ) is the joint probability of the class and the code assigned to a training sample , Pr ( class ) and Pr ( code ) are the marginal class and code probabilities , respectively .</sentence>
				<definiendum id="0">mutual information I</definiendum>
				<definiendum id="1">code )</definiendum>
				<definiendum id="2">code )</definiendum>
				<definiens id="0">the joint probability of the class and the code assigned to a training sample</definiens>
				<definiens id="1">the marginal class</definiens>
			</definition>
			<definition id="2">
				<sentence>d.fJ : lnfo ( bits ) Error No .</sentence>
				<definiendum id="0">d.fJ</definiendum>
				<definiens id="0">lnfo ( bits ) Error No</definiens>
			</definition>
</paper>

		<paper id="1042">
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>Explanation-based learning ( EBL ) is a machine-learning technique , closely connected to other techniques like macrooperator learning , chunking , and partial evaluation ; a phrase we have found useful for describing the method to logic programmers is example-guided partial evaluation .</sentence>
				<definiendum id="0">Explanation-based learning ( EBL</definiendum>
				<definiens id="0">a machine-learning technique , closely connected to other techniques like macrooperator learning , chunking</definiens>
			</definition>
			<definition id="1">
				<sentence>The normal route through the parser is extended with an EBL bypass , which contains special rules for efficient processing of common queries ; these rules are not coded by the programmer , but rather are produced automatically by inspecting the solutions to previously posed queries of the same type .</sentence>
				<definiendum id="0">EBL bypass</definiendum>
				<definiens id="0">contains special rules for efficient processing of common queries</definiens>
			</definition>
			<definition id="2">
				<sentence>These are the grammar pre-processor , which converts the grammar into a suitable pure Horn-clause representation ; the generalizer , which performs the actual extraction of learned rules ; and the simplifier , which attempts to reduce them in size by removing unnecessary calls .</sentence>
				<definiendum id="0">grammar pre-processor</definiendum>
				<definiendum id="1">generalizer</definiendum>
				<definiens id="0">attempts to reduce them in size by removing unnecessary calls</definiens>
			</definition>
			<definition id="3">
				<sentence>`` NP - &gt; the Name '' and `` NP - &gt; DET ADJ N '' Who is a member of the Vip Bypassing .</sentence>
				<definiendum id="0">NP</definiendum>
			</definition>
			<definition id="4">
				<sentence>D.R. , Etzioni , O. &amp; Gil , Y. , `` Explanation-Based Learning : A Problem-Solving Perspective '' , Artificial Intelligence 40 pp .</sentence>
				<definiendum id="0">Explanation-Based Learning</definiendum>
				<definiens id="0">A Problem-Solving Perspective ''</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>The ToActiveWord process has two inputs : it gets information from the Viterbi process associated with phones that were active in the current frame and should stay active in the next frame .</sentence>
				<definiendum id="0">ToActiveWord process</definiendum>
				<definiens id="0">has two inputs : it gets information from the Viterbi process associated with phones that were active in the current frame</definiens>
			</definition>
</paper>

		<paper id="1099">
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>Using only 40 utterances from the target speaker for adaptation , the error rate dropped to One important scenario for the use of spoken language systems ( SLS ) by new speakers is to start with a SI corpus or model and have the system adapt as the new users interact with the system .</sentence>
				<definiendum id="0">spoken language systems</definiendum>
				<definiens id="0">the new users interact with the system</definiens>
			</definition>
			<definition id="1">
				<sentence>The test set consists of 12 speakers ( 7 males ) and 25 utterances each .</sentence>
				<definiendum id="0">test set</definiendum>
				<definiens id="0">consists of 12 speakers ( 7 males ) and 25 utterances each</definiens>
			</definition>
</paper>

		<paper id="1076">
			<definition id="0">
				<sentence>IBM has a real-time isolated word recognizer with a vocabulary of 20,000 words \ [ 1\ ] giving over 95 % word recognition on an office correspondence task .</sentence>
				<definiendum id="0">IBM</definiendum>
				<definiens id="0">a real-time isolated word recognizer with a vocabulary of 20,000 words \</definiens>
			</definition>
			<definition id="1">
				<sentence>The feature vector consists of 7 reel-based cepstral coefficents \ [ 2\ ] and 8 dynamic parameters calculated by taking cepstral differences over a 40 ms interval .</sentence>
				<definiendum id="0">feature vector</definiendum>
			</definition>
			<definition id="2">
				<sentence>Davis , S.B. , and Mermelstein , P. , `` Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences , '' IEEE Transactions on Acoustics , Speech , and Signal Processing , ASSP-28 ( 4 ) , 357-365 , 1980 .</sentence>
				<definiendum id="0">Signal Processing</definiendum>
				<definiens id="0">parametric representations for monosyllabic word recognition in continuously spoken sentences , '' IEEE Transactions on Acoustics , Speech , and</definiens>
			</definition>
</paper>

		<paper id="1080">
</paper>

		<paper id="1015">
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>Introduction COMET ( COordinated Multimedia Explanation Testbed ) is an experimental system that generates interactive multimedia explanations of how to operate , maintain .</sentence>
				<definiendum id="0">COMET</definiendum>
				<definiendum id="1">COordinated Multimedia Explanation Testbed )</definiendum>
				<definiens id="0">an experimental system that generates interactive multimedia explanations of how to operate , maintain</definiens>
			</definition>
			<definition id="1">
				<sentence>System Overview COMET consists of the major components illustrated in Fig .</sentence>
				<definiendum id="0">System Overview COMET</definiendum>
			</definition>
			<definition id="2">
				<sentence>The content planner produces the full content for the explanation , represented as a hierarchy of logical forms ( LFs ) \ [ I\ ] , which are passed to the media coordinator .</sentence>
				<definiendum id="0">content planner</definiendum>
				<definiens id="0">produces the full content for the explanation , represented as a hierarchy of logical forms ( LFs ) \</definiens>
			</definition>
			<definition id="3">
				<sentence>The media coordinator refines the LFs by adding directives indicating which portions ate to be produced by each of a set of media-specific generation systems .</sentence>
				<definiendum id="0">media coordinator</definiendum>
				<definiens id="0">refines the LFs by adding directives indicating which portions ate to be produced by each of a set of media-specific generation systems</definiens>
			</definition>
			<definition id="4">
				<sentence>In our current implementation , when the verb for the sentence is selected , the text generator annotates the LF to indicate the grammatical sentence with the smallest number of constituents that can be formed .</sentence>
				<definiendum id="0">text generator</definiendum>
				<definiens id="0">annotates the LF to indicate the grammatical sentence with the smallest number of constituents that can be formed</definiens>
			</definition>
			<definition id="5">
				<sentence>It receives its input from the media coordinator and passes its output to the surface generator , which contains COMET 's grammar and constructs the grammatical slxucture of the sentence .</sentence>
				<definiendum id="0">generator</definiendum>
				<definiens id="0">contains COMET 's grammar and constructs the grammatical slxucture of the sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>( ALT ; ; here the FUL invokes the ; ; knowledge base , LOOM ( ( ( control ( ( m~-~er \ [ c \ [ discrete-knob ( loon : : supe rconcept s ( ^ roles medium object-concept ) ) \ ] ; ; if \ ] cldiscrete-knob is a ; ; superconcept select ' set '' ( verb ( ( lex 'set ' ) ( voice-class non-middle ) ( transitive-class transitive ) ) ) ) ; ; else select 'turn ' ( ( verb ( ( lex 'turn ' ) ( on-loc-prep 'onto '' ) ( voice-class non-middle ) ( transitive-class transitive ) \ ] Figure 5 : Part of the FUL encoding the choice between two verbs provides a modular and declarative lexicon that is easily extensible , and ultimately will allow for extensive interaction between the lexieal chooser and grammar through a uniform formalism .</sentence>
				<definiendum id="0">knowledge base , LOOM ( (</definiendum>
				<definiens id="0">( control ( ( m~-~er \ [ c \ [ discrete-knob ( loon : : supe rconcept s ( ^ roles medium object-concept )</definiens>
			</definition>
			<definition id="7">
				<sentence>The development of COMET is an ongoing group effort and has benefited from the contributions of Cliff Beshers ( menu interface ) , Andrea Danyhik ( learned rule base ) , Michael Elhadad ( FUF ) , David Fox ( text formatting component for media layout ) , Laura Gabbe ( static knowledge base and content planner ) , Jong IJm ( static knowledge base and content planner ) , Jacques Robin ( lexical chooser ) , Doree Selignlann ( IBIS ) , Tony Weida ( static knowledge base ) , Matt Kamerman ( user model ) , and Christine Lombardi and Yumiko Fuknmoto ( media coordinator ) .</sentence>
				<definiendum id="0">Doree Selignlann</definiendum>
				<definiens id="0">an ongoing group effort and has benefited from the contributions of Cliff Beshers ( menu interface ) , Andrea Danyhik ( learned rule base</definiens>
				<definiens id="1">text formatting component for media layout )</definiens>
				<definiens id="2">static knowledge base and content planner ) , Jong IJm ( static knowledge base and content planner ) , Jacques Robin ( lexical chooser ) ,</definiens>
			</definition>
</paper>

		<paper id="1088">
</paper>

		<paper id="1066">
			<definition id="0">
				<sentence>The optimality criterion is the minimization of the distortion as measured by the Euclidean distance between vectors of filter log energies .</sentence>
				<definiendum id="0">optimality criterion</definiendum>
				<definiens id="0">the minimization of the distortion as measured by the Euclidean distance between vectors of filter log energies</definiens>
			</definition>
			<definition id="1">
				<sentence>Minimum-mean log-spectral distance estimation The MMSE on the vector S of K filter log-energies yields the following vector estimator S = ) P ( S I S ' ) dS , ( 1 ) where ~ ' is the observed noisy vector , P ( S~ ) is~he clean speech log-spectral vector PD , and P ( S'I S ) is the conditional probability of the noisy log-spectral vector given the clean .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the conditional probability of the noisy log-spectral vector given the clean</definiens>
			</definition>
</paper>

		<paper id="1064">
</paper>

		<paper id="1033">
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>The Unisys approach to developing a spoken language system combines SUMMIT ( the MIT speech recognition system \ [ 6\ ] ) , PUNDIT ( the Unisys language understanding system \ [ 3\ ] ) and an Ingres database of air travel information for eleven cities and nine airports ( the ATIS database ) .</sentence>
				<definiendum id="0">PUNDIT</definiendum>
				<definiens id="0">the Unisys language understanding system \ [ 3\ ] ) and an Ingres database of air travel information for eleven cities and nine airports ( the ATIS database )</definiens>
			</definition>
			<definition id="1">
				<sentence>A Dialog Manager integrates the overall user-system interaction .</sentence>
				<definiendum id="0">Dialog Manager</definiendum>
				<definiens id="0">integrates the overall user-system interaction</definiens>
			</definition>
			<definition id="2">
				<sentence>An important function of the Dialog Manager is to maintain a record of the discourse context , so that the system can successfully process connected dialog .</sentence>
				<definiendum id="0">Dialog Manager</definiendum>
				<definiens id="0">to maintain a record of the discourse context</definiens>
			</definition>
			<definition id="3">
				<sentence>Intelligent Database Server The ATIS Intelligent Database Server ( see Figure 2 ) consists of the Intelligent Database Interface ( IDI ) and a server supporting the interaction beteen QTIP and a relational database .</sentence>
				<definiendum id="0">ATIS Intelligent Database Server</definiendum>
				<definiens id="0">consists of the Intelligent Database Interface ( IDI ) and a server supporting the interaction beteen QTIP and a relational database</definiens>
			</definition>
			<definition id="4">
				<sentence>The IDI provides a logic-based language for queries and transparent support for the actual interaction with an Ingres DBMS .</sentence>
				<definiendum id="0">IDI</definiendum>
			</definition>
			<definition id="5">
				<sentence>The Intelligent Database Interface is a portable , cachebased interface providing transparent , efficient access to one or more databases on one or more remote database management systems ( DBMS ) which support SQL .</sentence>
				<definiendum id="0">Intelligent Database Interface</definiendum>
			</definition>
			<definition id="6">
				<sentence>The query language of the IDI is the Intelligent Database Interface Language ( IDIL ) and is based on a restricted subset of function-free Horn clauses where the head of a clause represents the target list ( i.e. , the form of the result relation ) and the body is a conjunction of literals which denote database relations or operations on the relations and/or their attributes ( e.g. , negation , aggregation , and arithmetic operations ) .</sentence>
				<definiendum id="0">query language of the IDI</definiendum>
				<definiens id="0">the Intelligent Database Interface Language ( IDIL ) and is based on a restricted subset of function-free Horn clauses where the head of a clause represents the target list ( i.e. , the form of the result relation ) and the body is a conjunction of literals which denote database relations or operations on the relations and/or their attributes ( e.g. , negation , aggregation , and arithmetic operations )</definiens>
			</definition>
			<definition id="7">
				<sentence>The IDI Server supports QTIP 'S interaction with a relational database in several specific ways ; the IDI Server • accepts an IDIL query as input and returns Prolog tuples or Lisp tuples , the translated SQL query and statistics ; • translates IDIL queries to SQL and executes them on the database ; • manages connections transparently to the Ingres database ; • converts Ingres tuple output to Prolog or Lisp tuples , and • produces cAs evaluation output .</sentence>
				<definiendum id="0">IDIL</definiendum>
				<definiens id="0">queries to SQL and executes them on the database</definiens>
			</definition>
</paper>

		<paper id="1085">
</paper>

		<paper id="1093">
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>The HARC system incorporates the Byblos system \ [ 6\ ] as its speech recognition component and the natural language system Delphi , which consists of a bottom-up parser paired with an integrated syntax/semantics unification grammar , a discourse module , and a database question-answering backend .</sentence>
				<definiendum id="0">HARC system</definiendum>
				<definiens id="0">its speech recognition component and the natural language system Delphi , which consists of a bottom-up parser paired with an integrated syntax/semantics unification grammar , a discourse module , and a database question-answering backend</definiens>
			</definition>
			<definition id="1">
				<sentence>( NPTYPE-FILTER ( NONUN1TNP ( NONUNITNP ( NONUN1TNP ~O ( NPTYPE-FILTER ( NONUNITNP ( NONUN1TNP ( NONUNITNP -- -+0 ( NPTYPE-FILTER ( NONUNITNP ( NONUNITNP ( NONUN1TNP ~0 ( NPTYPE-FILTER ( NONUNITNP ( NONUNITNP ( NONUNITNP ~0 ( -PRO ( TIMENP ) ) ) ( -PRO ( TIMENP ) ) ) ( -PRO ( TIMENP ) ) ) ) ( -PRO ( DATENP ) ) ) ( -PRO ( DATENP ) ) ) ( -PRO ( DATENP ) ) ) ) ( -PRO ( MISCNP ) ) ) ( +PRO : PRO-TYPE ) ) ( -PRO ( MISCNP ) ) ) ) ( +PRO : PRO-TYPE ) ) ( -- PRO ( MISCNP ) ) ) ( -PRO ( MISCNP ) ) ) ) The first two rules require that if either of the conjuncts of a conjoined NP belongs to the TIMENP or DATENP subgrammar , than the other conjunct must belong to that subgrammar , as well .</sentence>
				<definiendum id="0">NPTYPE-FILTER</definiendum>
				<definiendum id="1">NONUN1TNP</definiendum>
				<definiendum id="2">NONUNITNP</definiendum>
				<definiendum id="3">) ) ( -PRO</definiendum>
				<definiens id="0">PRO-TYPE ) ) ( -- PRO ( MISCNP )</definiens>
			</definition>
			<definition id="2">
				<sentence>The PREDICATIVE-PP solution for the `` flight-on-airline '' sense is as follows : ( PREDICATIVE-PP ( PP-SEM ( : OR ( ON ) ( ABOARD ) ( ONBOARD ) ) ( : NP AIRLINE ) ) ( : SUBJ FLIGHT ) ( EQUAL ( FLIGHT-AIRLINE-OF : SUBJ ) : NP ) ) The first occurences of the variables : NP and : SUBJ above are paired with semantic types AIRLINE and FLIGHT ; this is shorthand for me actual state of affairs in which a term representing a package of information ( including encoding of semantic type ) appears in the slots of the role these variables occur in .</sentence>
				<definiendum id="0">PREDICATIVE-PP ( PP-SEM</definiendum>
				<definiendum id="1">FLIGHT</definiendum>
				<definiens id="0">SUBJ FLIGHT ) ( EQUAL ( FLIGHT-AIRLINE-OF : SUBJ ) : NP ) ) The first occurences of the variables : NP and : SUBJ above are paired with semantic types AIRLINE and</definiens>
			</definition>
			<definition id="3">
				<sentence>Recursion is allowed , as in : ( PREDICATIVE-PP ( PP-SEM : PREP ( : NP TIME ) ) ( : SUBJ FLIGHT ) : WFF ) ( PREDICATIVE-PP ( PP-SEM : PREP ( : NP TIME ) ) ( DEPARTURE-TIME-OF : SUB J ) : WFF ) This rule says that any PP relating a flight to a time should be translated as if it related the departure time of the flight to that time .</sentence>
				<definiendum id="0">Recursion</definiendum>
				<definiendum id="1">PREP</definiendum>
				<definiens id="0">SUBJ FLIGHT ) : WFF ) ( PREDICATIVE-PP ( PP-SEM : PREP ( : NP TIME</definiens>
			</definition>
			<definition id="4">
				<sentence>There are two main rules : ( VP : SUBJ ( AND : INITIAL-WFF : COMP-WFF ) ) ( V : LEX ( INTRANSOPTCOMPS : SUBJ : INITIAL-WFF ) ) ( OPTCOMPS : LEX : SUBJ ( DUMMY ) : COMP-WFF ) and ( VP : SUBJ ( AND : INIIIAL-WFF : COMP-WFF ) ) ( V : LEX ( TRANSITIVEOPTCOMPS : SUBJ : OBJ : INITIAL-WFF ) ) ( NP : OBJ ) ( OPTCOMPS : LEX : SUBJ : OBJ : COMP-WFF ) The category OPTCOMPS generates the fight-branching tree of optional complements .</sentence>
				<definiendum id="0">OBJ</definiendum>
				<definiendum id="1">INITIAL-WFF ) )</definiendum>
				<definiens id="0">COMP-WFF ) ) ( V : LEX ( INTRANSOPTCOMPS : SUBJ : INITIAL-WFF ) ) ( OPTCOMPS : LEX : SUBJ ( DUMMY ) : COMP-WFF )</definiens>
			</definition>
			<definition id="5">
				<sentence>It has the rules ( OPTCOMPS : LEX : SUBJ : OBJ ( AND : WFF1 : WFF2 ) ) ( PP : PP ) ( OPTCOMPS : LEX : SUBJ : OBJ : WFF1 ) ( OPTCOMP-PP : LEX : SUBJ : OBJ : PP : WFF2 ) and ( OPTCOMPS : LEX : SUBJ : OBJ ( TRUE ) ) ~ 0 The OPTCOMP-PP is the constraint relation ; it keys off the lexical head of the verb ( the variable : LEX ) and combines the subject , object , and PP complement translations to produce the contribution of the PP complement to the final formula that represents the sentence meaning .</sentence>
				<definiendum id="0">SUBJ</definiendum>
				<definiens id="0">the constraint relation ; it keys off the lexical head of the verb ( the variable : LEX ) and combines the subject , object , and PP complement translations to produce the contribution of the PP complement to the final formula that represents the sentence meaning</definiens>
			</definition>
</paper>

		<paper id="1097">
</paper>

		<paper id="1092">
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>The phonotactic structure of the language is modelled , to a first order approximation , by the state transition matrix , aij , which defines the probability of occurrence of state ( phoneme ) qj at time t + z conditioned on state ( phoneme ) qi at time t , where x is the duration of phoneme i. The information about the temporal structure of the hidden units is contained in the set of durational densities { dq ( t ) } inj=l. The acoustic correlates of the phonemes are the observations , denoted Or , and their distributions , which are defined by a set of observation densities { b 0 ( Or ) } \ [ j=l. The durational densities are 3-parameter gamma distributions - ( x Xmin ( i , j ) ) v°-I e -n'~ ( ~-~ ( i , y ) ) ( 4 ) d0 ( x ) r ( v0 ) where F ( x ) is the ordinary gamma function .</sentence>
				<definiendum id="0">x</definiendum>
				<definiens id="0">The phonotactic structure of the language is modelled , to a first order approximation , by the state transition matrix , aij , which defines the probability of occurrence of state ( phoneme ) qj at time t + z conditioned on state ( phoneme ) qi at time t</definiens>
			</definition>
			<definition id="1">
				<sentence>Let at ( i ) denote the maximum likelihood of O1 02 ... Ot over all state and duration sequences terminating in state i. This quantity can be evaluated recursively according to { I `` at ( j ) = max max a } i- ) x aij dij ( x ) I'I bij ( Or-0 ) ( 5 ) l ~ i ~ n xmi , ( i , j ) ~ x ~ Xm= O=0 for 1 _ &lt; j _ &lt; n , 1 _ &lt; t &lt; _ T where Xmi~ ( i , j ) is the minimum duration for which dq ( x ) is defined and 'rmax is the maximum allowable duration for any phonetic unit .</sentence>
				<definiendum id="0">Xmi~</definiendum>
				<definiendum id="1">'rmax</definiendum>
				<definiens id="0">the maximum likelihood of O1 02 ... Ot over all state and duration sequences terminating in state i. This quantity can be evaluated recursively according to { I `` at ( j ) = max max a } i-</definiens>
			</definition>
			<definition id="2">
				<sentence>Let us denote by CKL ( V ) the cost of matching the word , v , to qt+l ... .. qt+L where v has the phonetic spelling ql , q2 ... .. qg .</sentence>
				<definiendum id="0">CKL ( V )</definiendum>
			</definition>
			<definition id="3">
				<sentence>If we adopt the rhobar metric \ [ 23\ ] between bjk ( X ) and bjt ( x ) then we have Sjkt = I I.t.ik lxytl dt+t Vj ( 10a ) We use a simple heuristic for the costs of insertion and deletion .</sentence>
				<definiendum id="0">bjt</definiendum>
				<definiens id="0">a simple heuristic for the costs of insertion and deletion</definiens>
			</definition>
			<definition id="4">
				<sentence>The training set consists of 3,267 sentences spoken by 109 different speakers .</sentence>
				<definiendum id="0">training set</definiendum>
				<definiens id="0">consists of 3,267 sentences spoken by 109 different speakers</definiens>
			</definition>
			<definition id="5">
				<sentence>The tNrd test set comprises 54 sentences spoken by one of us ( SEL ) recorded using equipment similar to that used for the DARPA data .</sentence>
				<definiendum id="0">tNrd test set</definiendum>
			</definition>
			<definition id="6">
				<sentence>K ( i ) where N ( i , j ) is the total number of occurrences of the bigram qi qj and .</sentence>
				<definiendum id="0">K</definiendum>
				<definiendum id="1">N ( i</definiendum>
				<definiens id="0">the total number of occurrences of the bigram qi qj and</definiens>
			</definition>
</paper>

		<paper id="1094">
</paper>

		<paper id="1020">
</paper>

		<paper id="1002">
</paper>

		<paper id="1062">
			<definition id="0">
				<sentence>Each ( 24-element ) feature vector consists of 12 liftered cepstral coefficients and 12 delta cepstral coefficients .</sentence>
				<definiendum id="0">24-element ) feature vector</definiendum>
				<definiens id="0">consists of 12 liftered cepstral coefficients and 12 delta cepstral coefficients</definiens>
			</definition>
			<definition id="1">
				<sentence>The sentence-level match module uses a language model ( based on a set of syntactic and semantic rules ) to determine the word sequence in a sentence .</sentence>
				<definiendum id="0">sentence-level match module</definiendum>
				<definiens id="0">uses a language model ( based on a set of syntactic and semantic rules ) to determine the word sequence in a sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>The training set consists of a set of 3990 read sentences from 109 talkers ( 30-40 sentences/talker ) .</sentence>
				<definiendum id="0">training set</definiendum>
				<definiens id="0">a set of 3990 read sentences from 109 talkers ( 30-40 sentences/talker )</definiens>
			</definition>
			<definition id="3">
				<sentence>The m 'h delta-delta cepstral coefficient at frame I was approximated as Azct ( m ) = K-\ [ Act+l ( m ) -Act_l ( m ) l ( 1 ) where Act ( m ) is the estimated m 'n delta cepstral coefficient evaluated at frame l , and K is a scaling constant which was fixed to he 0.375 ( no optimization was attempted to find a better value of the normalization constant to optimize the k-means clustering part of the training algorithm ) .</sentence>
				<definiendum id="0">Act ( m )</definiendum>
				<definiendum id="1">K</definiendum>
				<definiens id="0">the estimated m 'n delta cepstral coefficient evaluated at frame l , and</definiens>
				<definiens id="1">a scaling constant which was fixed to he 0.375 ( no optimization was attempted to find a better value of the normalization constant to optimize the k-means clustering part of the training algorithm )</definiens>
			</definition>
			<definition id="4">
				<sentence>The distance between units P2 and P1 is defined as the difference between average likelihoods of observing all acoustic segments with label P2 and observing all acoustic segments with label P1 given the model for unit Pl .</sentence>
				<definiendum id="0">P1</definiendum>
			</definition>
</paper>

		<paper id="1045">
</paper>

		<paper id="1032">
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>ABSTRACT Phonetic baseforms are the basic recognition units in most large vocabulary speech recognition systems .</sentence>
				<definiendum id="0">ABSTRACT Phonetic baseforms</definiendum>
				<definiens id="0">the basic recognition units in most large vocabulary speech recognition systems</definiens>
			</definition>
			<definition id="1">
				<sentence>Phonetic baseforms are the basic recognition units in most large vocabulary speech recognition systems .</sentence>
				<definiendum id="0">Phonetic baseforms</definiendum>
				<definiens id="0">the basic recognition units in most large vocabulary speech recognition systems</definiens>
			</definition>
			<definition id="2">
				<sentence>( 1 ) where H represents the utterance ( s ) and £ represents the word spelling .</sentence>
				<definiendum id="0">H</definiendum>
				<definiens id="0">the utterance ( s ) and £ represents the word spelling</definiens>
			</definition>
			<definition id="3">
				<sentence>Visual inspection of the baseforms generated by spellingto-sound rules alone indicated that approximately one-third of the baseforms had errors in them , where errors are defined as substantial discrepancies between the artificially generated baseforms and handwritten ones that had a fair chance of resulting in recognition errors .</sentence>
				<definiendum id="0">Visual inspection of the baseforms generated by spellingto-sound rules</definiendum>
				<definiens id="0">substantial discrepancies between the artificially generated baseforms and handwritten ones that had a fair chance of resulting in recognition errors</definiens>
			</definition>
</paper>

		<paper id="1078">
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>The modified A ° algorithm used to grow a tree is summarized as foUows : 14 N-Best Tree Search ( A ° ) Algorithm INITIALIZE : put the root node in a rank ordered list ( main stack ) to form a null partial path LOOP : best first path growing loop Take the top entry ( the best partial path ) off the main stack IF the top entry is a single path ( i.e. , not a group of partial paths ) , THEN IF the best partial path is complete ( i.e. , leads to a terminal node ) , THEN output the path and increment the output hypothesis counter by one IF output counter equals N , THEN stop ENDIF ELSE ENDIF Split the partial path into two sets : the best one-arc extension and the remaining one-arc extensions .</sentence>
				<definiendum id="0">THEN IF</definiendum>
				<definiendum id="1">ENDIF ELSE ENDIF Split</definiendum>
				<definiens id="0">a single path ( i.e. , not a group of partial paths ) ,</definiens>
				<definiens id="1">the best partial path is complete ( i.e. , leads to a terminal node )</definiens>
			</definition>
			<definition id="1">
				<sentence>ELSE Split the set of partial paths into two sets : the best partial path and the remaining partial paths in the set .</sentence>
				<definiendum id="0">ELSE Split</definiendum>
				<definiens id="0">the set of partial paths into two sets : the best partial path</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>Loom provides a separation of definition ( necessary and sufficient conditions ) and constraints ( implied features ) .</sentence>
				<definiendum id="0">Loom</definiendum>
				<definiens id="0">provides a separation of definition ( necessary and sufficient conditions ) and constraints ( implied features</definiens>
			</definition>
			<definition id="1">
				<sentence>This definition is satisfied by c. In addition , Trans-Clause has a constraint : it implies the type Active OR P as sive , which means thatany object which is a member of Trans-Clause must also be a member of Active OR Passive ( that is , Active and Passive form a disjoint covering of Trans-Clause ) .</sentence>
				<definiendum id="0">Trans-Clause</definiendum>
			</definition>
			<definition id="2">
				<sentence>In order to achieve greater portability , Penman contains a general taxonomic ontology of concepts called the Upper Model \ [ 1\ ] , under which the concepts from various application domains are subordinated .</sentence>
				<definiendum id="0">Penman</definiendum>
				<definiens id="0">contains a general taxonomic ontology of concepts called the Upper Model \ [ 1\ ] , under which the concepts from various application domains are subordinated</definiens>
			</definition>
</paper>

		<paper id="1086">
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>The stochastic segment model for Y has two components \ [ 10\ ] : 1 ) a time transformation TL to model the variable-length observed segment in terms of a fixed-length unobserved sequence Y = XTz , where X = \ [ Zl z2 ... zu\ ] , and 2 ) a probabilistic representation of the unobserved feature sequence X. The conditional density of the observed segment Y given phone a and duration L is : p ( Y\ [ ~ , L ) =/x p ( XIa ) dX '' ( 1 ) : Y=XTz When the observed length , L , is less than or equal to M , the length of X , Tz is a time-warping transformation which obtains Y by selecting a subset of columns of X and the density p ( Yla ) is a marginal distribution of p ( Xla ) .</sentence>
				<definiendum id="0">Tz</definiendum>
				<definiens id="0">two components \ [ 10\ ] : 1 ) a time transformation TL to model the variable-length observed segment in terms of a fixed-length unobserved sequence Y = XTz , where X = \ [ Zl z2 ... zu\ ] , and 2 ) a probabilistic representation of the unobserved feature sequence</definiens>
				<definiens id="1">Y=XTz When the observed length , L , is less than or equal to M , the length of X</definiens>
				<definiens id="2">a time-warping transformation which obtains Y by selecting a subset of columns of X</definiens>
				<definiens id="3">a marginal distribution of p ( Xla )</definiens>
			</definition>
			<definition id="1">
				<sentence>A segmentation S of a sentence that is N frames long and t will be represented by consists of n segments s r S ~ S ( n ) = { 8~t , 81 '' 2~- , +1 , • • • , 8 ; : _t+ , } , l &lt; rl &lt; r2 &lt; ... &lt; rn_l &lt; r , ~ = N The sequence of the observed data can be partitioned in accordance to the segmentation S ( n ) as Y `` = Y ( S ( n ) ) = = { Y ( 1 , rl ) , Y ( rx + 1 , r2 ) , ... , Y ( r , ~_ , + 1 , rn ) } with Y ( r , t ) = \ [ y , - , y , -+l , ... , yt\ ] Finally , an n-long sequence of phone labels will be represented as o : ( n ) = { ~0 , CXl , ... , an-1 } When the phone boundaries are unknown , the joint likelihood of the observed data and an n-long phone sequence is tCc , Cn ) ) = ~ v ( Y ( S ( n ) ) , S ( n ) l , ~ ( n ) ) p ( , ~ ( n ) ) ( 2 ) S ( n ) and under the assumption of independent phones , p ( Y ( S ( n ) ) , S ( n ) l~ ( n ) ) p ( ~ ( n ) ) = Ti+I 'ri+l 1I\ ] ~o I p ( Y ( ri + 1 , ri+lllal , s , . , .l ) P ( S , ,+l \ [ cq ) p ( al ) where the terms p ( s~+Lla ) = p ( Lla ) represent duration probabilities. For the automatic recognition problem , the MAP rule for detecting the most likely phonetic sequence would involve calculations of the form ( 2 ) . Because of the complexity of ( 2 ) , in a similar fashion to HMM 's where Viterbi decoding is used instead of the forward algorithm , we choose to jointly select the segmentation and the phone sequence that maximize the a posteriori likelihood ( a* ( n* ) , S* ( n * ) , n * ) = arg max ( , ~o~ ) , so , ) , ,~ ) { p ( Y ( S ( n ) ) , S ( n ) la ( n ) ) p ( a ( n ) ) } In addition to the phone independence assumption , if we further assume that there is no upper bound in the range of allowable phone durations , there are 2 Nx possible segmentations. If the second assumption is dropped , and the range of phone durations is 1 &lt; L &lt; D , the number of configurations among which we optimize drops to 2 N-1 2 N-D + 1. 174 Dynamic Programming Search The joint segmentation and recognition , being a shortest path problem , has a solution using the following Dynamic Programming recursion \ [ 6\ ] under the assumption of independent phones : = 0 J~ = max { J~* + ln\ [ p ( Y ( r + 1 , t ) ls ) \ ] + ln\ [ p ( st~+lls ) \ ] `` r &lt; t~a + ln\ [ p ( s ) \ ] + C } with C being a constant that controls the insertion rate. At the last frame N we obtain the solution : N = n n ) ) , sCn ) ) p ( s ( n ) ) ) + n fez ) , n We shall express the complexity of a joint segmentation and recognition search algorithm in terms of segment score evaluations : t cr = maax { In\ [ p ( Y ( r , ~ ) Is ) \ ] + In\ [ p ( str Is ) \ ] + In\ [ p ( s ) \ ] } ( 3 ) Clearly , a segment score evaluation is dominated by the first term , which is a ( t ~+ 1 ) q dimensional Gaussian evaluation. We shall also use the number of q-dimensional Ganssian score evaluations as a measure of complexity. A segment score evaluation consists of Inl ( tr÷ 1 ) Gaussian score evaluations , with D the set of all phones. The DP solution is efficient with a complexity of O ( N 2 ) segment score evaluations , which drops to O ( DN ) if the segment length is restricted to be less than D. However , in terms of Gaussian score evaluations ( 1 ) this approach is computationally expensive. If we assume that feature vectors are independent in time , then for each frame in the sentence the scores of all models and possible positions within a segment will effectively be computed and stored. This translates to a complexity of O ( M x N x Inl ) q-dimensional Gaussian evaluations , or simply M Gaussian evaluations per frame per model , where M is the model length. This complexity is further increased when the independence assumption is dropped , in which case the number of Gaussian scores that must be computed increases drastically and is equal to the square of allowable phone durations per frame per model. For large q ( as in \ [ 2\ ] ) , the DP solution is impractical. In the following sections we present a local search algorithm that achieves a significant computation reduction. Local Search Algorithms We now give the description of a general local search algorithm for the joint segmentation and recognition problem. The set of all segmentations is N = { sis = , ÷1 } } The neighborhood of a segmentation S is defined through the mapping from ~ to the power set of 3c : N : .7 : -- _~ 2 7 A local search algorithm for the JSR problem is then defined as : given any segmentation S , the neighborhood N ( S ) is searched for a segmentation S ' with I ( S ' ) &gt; l ( S ) .</sentence>
				<definiendum id="0">segmentation S</definiendum>
				<definiendum id="1">rl ) , Y</definiendum>
				<definiendum id="2">+ 1</definiendum>
				<definiendum id="3">segment score</definiendum>
				<definiens id="0">of a sentence that is N frames long and t will be represented by consists of n segments s r S ~ S ( n ) = { 8~t , 81 '' 2~- , +1 , • • • , 8 ; : _t+ , } , l &lt; rl &lt; r2 &lt; ... &lt; rn_l &lt; r</definiens>
				<definiens id="1">r , t ) = \ [ y , - , y , -+l , ... , yt\ ] Finally</definiens>
				<definiens id="2">o : ( n ) = { ~0 , CXl , ... , an-1 } When the phone boundaries are unknown , the joint likelihood of the observed data and an n-long phone sequence is tCc , Cn ) ) = ~ v ( Y ( S ( n ) ) , S ( n ) l , ~ ( n ) ) p ( , ~ ( n ) ) ( 2 ) S ( n ) and under the assumption of independent phones , p ( Y ( S ( n ) ) , S ( n ) l~ ( n ) ) p ( ~ ( n ) ) = Ti+I 'ri+l 1I\ ] ~o I p ( Y ( ri + 1 , ri+lllal , s , . , .l ) P ( S , ,+l \ [ cq ) p ( al ) where the terms p ( s~+Lla ) = p ( Lla ) represent duration probabilities. For the automatic recognition problem , the MAP rule for detecting the most likely phonetic sequence would involve calculations of the form ( 2 ) . Because of the complexity of ( 2 ) , in a similar fashion to HMM 's where Viterbi decoding is used instead of the forward algorithm , we choose to jointly select the segmentation and the phone sequence that maximize the a posteriori likelihood ( a* ( n* ) , S* ( n * ) , n * ) = arg max ( , ~o~ ) , so , ) , ,~ ) { p ( Y ( S ( n ) ) , S ( n ) la ( n ) ) p ( a ( n ) ) } In addition to the phone independence assumption</definiens>
				<definiens id="3">a solution using the following Dynamic Programming recursion \ [ 6\ ] under the assumption of independent phones : = 0 J~ = max { J~* + ln\ [ p ( Y ( r + 1 , t ) ls ) \ ] + ln\ [ p ( st~+lls ) \ ] `` r &lt; t~a + ln\ [ p ( s ) \ ] + C } with C being a constant that controls the insertion rate. At the last frame N we obtain the solution : N = n n ) ) , sCn ) ) p ( s ( n ) ) ) + n fez ) , n We shall express the complexity of a joint segmentation and recognition search algorithm in terms of segment score evaluations : t cr = maax { In\ [ p ( Y ( r , ~ ) Is ) \ ] + In\ [ p ( str Is ) \ ] + In\ [ p ( s ) \ ] } ( 3 ) Clearly , a segment score evaluation is dominated by the first term , which is a ( t ~+ 1 ) q dimensional Gaussian evaluation. We shall also use the number of q-dimensional Ganssian score evaluations as a measure of complexity. A</definiens>
				<definiens id="4">drops to O ( DN ) if the segment length is restricted to be less than D. However , in terms of Gaussian score evaluations ( 1 ) this approach is computationally expensive. If we assume that feature vectors are independent in time , then for each frame in the sentence the scores of all models and possible positions within a segment will effectively be computed and stored. This translates to a complexity of O ( M x N x Inl ) q-dimensional Gaussian evaluations , or simply M Gaussian evaluations per frame per model , where M is the model length. This complexity is further increased when the independence assumption is dropped , in which case the number of Gaussian scores that must be computed increases drastically and is equal to the square of allowable phone durations per frame per model. For large q</definiens>
				<definiens id="5">achieves a significant computation reduction. Local Search Algorithms We now give the description of a general local search algorithm for the joint segmentation and recognition problem. The set of all segmentations is N = { sis = , ÷1 } } The neighborhood of a segmentation S</definiens>
				<definiens id="6">given any segmentation S , the neighborhood N ( S ) is searched for a segmentation S ' with I ( S ' ) &gt; l ( S )</definiens>
			</definition>
			<definition id="2">
				<sentence>Then the basic Split and Merge algorithm consists of a local search over all segmentations in the union of N6 ( S ) and Nm ( S ) .</sentence>
				<definiendum id="0">Merge algorithm</definiendum>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>A break index encodes the degree of prosodic decoupling between neighboring words .</sentence>
				<definiendum id="0">break index</definiendum>
				<definiens id="0">encodes the degree of prosodic decoupling between neighboring words</definiens>
			</definition>
			<definition id="1">
				<sentence>The mean duration for each phone is adjusted with each new observed phone according to fia m ~a ÷ ar/N where r is the speaking rate , N is a feedback coefficient that is equal to 5000 at steady state , but varies at start-up for faster initial adaptation , g is the standard deviation of the phone 's duration , unadapted , and ~ua represents the mean duration for phone a. A fully connected seven-state HMM is used to recognize break indices , given the raw break feature .</sentence>
				<definiendum id="0">r</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">g</definiendum>
				<definiendum id="3">~ua</definiendum>
			</definition>
</paper>

		<paper id="1025">
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>Figure 1 : Characteristic Machine for G1 • The other states are all the possible dotted rules of G. * There is a transition labeled X , where X is a terminal or nonterminal symbol , from dotted rule A ~ a.X/3 to A ~ aX .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a terminal or nonterminal symbol</definiens>
			</definition>
			<definition id="1">
				<sentence>• There is an e-transition from A -+ a. B/3 to B ~ `` 7 , where B is a nonterminal symbol and B ~ `` 7 a rule in G. .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">a nonterminal symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>The purpose of collapsing loops is to `` forget '' stack segments that may be arbitrarily repeated3 Clearly , each equivalence class is uniquely defined by the shortest stack in the class , and the classes can be constructed without having to consider all the ( infinitely ) many possible stacks .</sentence>
				<definiendum id="0">equivalence class</definiendum>
				<definiens id="0">having to consider all the ( infinitely ) many possible stacks</definiens>
			</definition>
			<definition id="3">
				<sentence>In what follows , G is a fixed CFG with terminal vocabulary ~ , nonterminal vocabulary N and start symbol S..</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">a fixed CFG with terminal vocabulary ~ , nonterminal vocabulary</definiens>
			</definition>
			<definition id="4">
				<sentence>M is the characteristic machine for G , with state set Q , start state so , final states F , and transition function 6 : S x ( E U N ) -- ~ S. As usual , transition functions such as 6 are extended from input symbols to input strings by defining 6 ( s , e ) = s and 6 ( s , aft ) = 6 ( 6 ( s , a ) , fl ) .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">the characteristic machine for G , with state set Q , start state so , final states F , and transition function 6 : S x ( E U N ) -- ~ S. As usual , transition functions such as 6 are extended from input symbols</definiens>
			</definition>
			<definition id="5">
				<sentence>The initial configurations of ~ are is0 , e , w ) for some input string w , and the final configurations are ( s , ( s0 , S } , e ) for some state s E F. A derivation of a string w is a sequence of configurations co , ... , cm such that co = ( so , e , w } , cm = i s , ( So , S } , e } for some final state p , and ci-1 ~ci for 1 &lt; i &lt; n. Let s be a state .</sentence>
				<definiendum id="0">derivation of a string w</definiendum>
				<definiens id="0">a sequence of configurations co , ... , cm such that co = ( so , e</definiens>
			</definition>
			<definition id="6">
				<sentence>A stack r is a loop if r = 22 Symbol Category Features s np vp args det n pron v sentence noun phrase verb phrase verb arguments determiner noun pronoun verb n ( number ) , p ( person ) n , p , c ( case ) n , p , t ( verb type ) t n n n , p , c n , p , t Table 1 : Categories of Example Grammar ( sl , X1 ) ... ( sk , Xk ) and 6 ( sk , Xk ) = sx .</sentence>
				<definiendum id="0">stack r</definiendum>
				<definiendum id="1">sk</definiendum>
				<definiens id="0">a loop if r = 22 Symbol Category Features s np vp args det n pron v sentence noun phrase verb phrase verb arguments determiner noun pronoun verb n ( number ) , p ( person ) n , p , c ( case ) n , p</definiens>
				<definiens id="1">n n , p , c n , p</definiens>
			</definition>
			<definition id="7">
				<sentence>After this factoring , the full instantiation of the example grammar has 181 rules , its characteristic machine 222 states and 922 transitions , the Feature Values n ( number ) p ( person ) ¢ ( case ) t ( verb type ) s ( singular ) , p ( plural ) 1 ( first ) , 2 ( second ) , 3 ( third ) s ( subject ) , o ( nonsubject ) i ( intransitive ) , t ( transitive ) , d ( ditransitive ) Table 2 : Features of Example Grammar unfolded and flattened FSA 3417 states and 4255 transitions , and the determinized and minimized final DFA 18 states and 67 transitions .</sentence>
				<definiendum id="0">Feature Values n</definiendum>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>VOYAGER accepts both spoken and typed input and responds in the form of text , graphics , and synthesized speech .</sentence>
				<definiendum id="0">VOYAGER</definiendum>
				<definiens id="0">accepts both spoken and typed input and responds in the form of text , graphics , and synthesized speech</definiens>
			</definition>
			<definition id="1">
				<sentence>The horizontal line ( e ) shows the percentage of utterances where a response would have been produced if the correct word string had been found by the speech recognition component .</sentence>
				<definiendum id="0">horizontal line</definiendum>
				<definiens id="0">the percentage of utterances where a response would have been produced if the correct word string had been found by the speech recognition component</definiens>
			</definition>
			<definition id="2">
				<sentence>The horizontal line ( e ) shows the percentage of utterances that produce an action given the correct word string .</sentence>
				<definiendum id="0">horizontal line</definiendum>
				<definiens id="0">the percentage of utterances that produce an action given the correct word string</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>We describe the Air Travel Information System ( ATIS ) pilot corpus , a corpus designed to measure progress in Spoken Language Systems that include both a speech and natural language component .</sentence>
				<definiendum id="0">pilot corpus</definiendum>
				<definiens id="0">a corpus designed to measure progress in Spoken Language Systems that include both a speech and natural language component</definiens>
			</definition>
			<definition id="1">
				<sentence>Introduction The ATIS corpus provides an opportunity to develop and evaluate speech systems that understand spontaneous speech .</sentence>
				<definiendum id="0">ATIS corpus</definiendum>
				<definiens id="0">provides an opportunity to develop and evaluate speech systems that understand spontaneous speech</definiens>
			</definition>
			<definition id="2">
				<sentence>The ATIS database consists of data obtained from the Official Airline Guide ( OAG , 1990 ) , organized under a relational schema .</sentence>
				<definiendum id="0">ATIS database</definiendum>
				<definiens id="0">consists of data obtained from the Official Airline Guide ( OAG , 1990 ) , organized under a relational schema</definiens>
			</definition>
			<definition id="3">
				<sentence>l lliiiiiilN¥i ilNiiiiiiiill Figure 1 : Subject Session Procedure Session Introduction The subjects were given the following instructions , both orally and in writing : The Air Travel Information System ( ATIS ) is a prototype of a voice-input information retrieval system .</sentence>
				<definiendum id="0">Air Travel Information System</definiendum>
				<definiens id="0">a prototype of a voice-input information retrieval system</definiens>
			</definition>
</paper>

		<paper id="1101">
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>One goal of the Spoken Language System ( SLS ) project is to demonstrate a real-time interactive system that integrates speech recognition and natural language processing .</sentence>
				<definiendum id="0">Spoken Language System</definiendum>
				<definiens id="0">to demonstrate a real-time interactive system that integrates speech recognition and natural language processing</definiens>
			</definition>
			<definition id="1">
				<sentence>The most promising method for speeding up the forward search is to use a phonetic tree in which the common word beginnings are shared .</sentence>
				<definiendum id="0">forward search</definiendum>
				<definiens id="0">to use a phonetic tree in which the common word beginnings are shared</definiens>
			</definition>
</paper>

		<paper id="1100">
</paper>

		<paper id="1019">
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>Acoustic Modeling DECIPHER builds and trains word models by using context-based phone models arranged according to the pronunciation networks for the word being modeled .</sentence>
				<definiendum id="0">Acoustic Modeling DECIPHER</definiendum>
				<definiens id="0">builds and trains word models by using context-based phone models arranged according to the pronunciation networks for the word being modeled</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>An acoustic cluster consists of a mean vector and a variance vector .</sentence>
				<definiendum id="0">acoustic cluster</definiendum>
				<definiens id="0">consists of a mean vector and a variance vector</definiens>
			</definition>
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>Finally , we evaluated the above system on the June 90 ( RM2 ) test set , which consists of 480 sentences spoken by four speakers .</sentence>
				<definiendum id="0">test set</definiendum>
				<definiens id="0">consists of 480 sentences spoken by four speakers</definiens>
			</definition>
</paper>

		<paper id="1082">
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>The Interaction During Training When a category model ( e.g. a context-free ( CF ) model such as a monophone ) is used to a model a set of subcategories ( e.g. context-dependent ( CD ) models such as triphones ) , the category model becomes the subcategory prior-probability weighted average of the subcategory models : Meat E PsubeatMsubcat where M denotes a model .</sentence>
				<definiendum id="0">Interaction During Training When</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">a category model ( e.g. a context-free ( CF ) model such as a monophone ) is used to a model a set of subcategories ( e.g. context-dependent</definiens>
				<definiens id="1">a model</definiens>
			</definition>
</paper>

		<paper id="1061">
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>De-Indexicalization : Traversing Tense Trees A formal theory of how indexical formulas are converted to nonindexical ones as a function of context requires an explicit specification of the type of context structure to be used .</sentence>
				<definiendum id="0">De-Indexicalization</definiendum>
				<definiens id="0">Traversing Tense Trees A formal theory of how indexical formulas</definiens>
			</definition>
			<definition id="1">
				<sentence>Then de-indexicalization of an LF formula with a top-level pres operator , relative to T , is defined by the equivalence Pres : ( pres ~ ) T ~ ( 3aT : \ [ \ [ aT at-about EmbT\ ] A \ [ Last T orients aT\ ] \ ] 37 \ [ ~OT ** eT\ ] ) , tree transformation : ( pres 9 ) '' r = 9 .</sentence>
				<definiendum id="0">de-indexicalization of an LF formula</definiendum>
			</definition>
			<definition id="2">
				<sentence>`` OT '' means `` store e T at the focal node of T. '' Emb T denotes the last-added episode at the node which directly embeds the tree containing the focal node of T. ( This is usually an episode corresponding to a performative or attitude verb . )</sentence>
				<definiendum id="0">OT</definiendum>
				<definiens id="0">store e T at the focal node of T. '' Emb T denotes the last-added episode at the node which directly embeds the tree containing the focal node of T. ( This is usually an episode corresponding to a performative or attitude verb</definiens>
			</definition>
			<definition id="3">
				<sentence>Roughly speaking , this is needed because the formula preceding `` ** ( F r/ ) '' is an atemporal characterization of the propositional content of r/ ( one that is simply true or false , rather than true of some episodes and false of others ) ; whereas ( futr 9 ) W is a temporal characterization of r/ ( `` being followed at some future time by a 9-episode '' can be true of some episodes namely , those that do precede a 9-episode and false of others ) .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">a 9-episode and false of others</definiens>
			</definition>
			<definition id="4">
				<sentence>38 Here recent-subep is another context-charged relation , which for a state-change e of type ¢ as first argument suggests the truth of \ [ ~ * ( fin r\ ] ) \ ] , whenever \ [ ¢ ** eli entails \ [ ~ • ( fin el ) \ ] for all el ( where fin means `` final moment of ' ) .</sentence>
				<definiendum id="0">fin means</definiendum>
				<definiens id="0">final moment of ' )</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>The smoothing that is done is intended on the one hand to produce a very parsimonious representation of the speech data to make further processing very quick , and on the other , to represent the data in a way that is not too sensitive to variations in the duration of phonemes , thus obviating the need A word start grouping ( WSG ) consists of a collection of words whose beginnings are acoustically similar .</sentence>
				<definiendum id="0">word start grouping</definiendum>
				<definiens id="0">consists of a collection of words whose beginnings are acoustically similar</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , a single element of a smooth frame vector y is assumed to be a random variable with a probability density of the form f ( z ) = exp ( -Iz -glm ) where g is the mean ( or median ) and a is the mean absolute deviation ( MAD ) .</sentence>
				<definiendum id="0">g</definiendum>
				<definiens id="0">the mean ( or median ) and a is the mean absolute deviation</definiens>
			</definition>
</paper>

		<paper id="1001">
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>The domain of Project MURASAKI is the disease AIDS .</sentence>
				<definiendum id="0">Project MURASAKI</definiendum>
				<definiens id="0">the disease AIDS</definiens>
			</definition>
			<definition id="1">
				<sentence>Similarly , Appendix C contains a record from the Testing Policies database that was also generated from Text # 124 .</sentence>
				<definiendum id="0">Appendix C</definiendum>
				<definiens id="0">contains a record from the Testing Policies database</definiens>
			</definition>
			<definition id="2">
				<sentence>The quantitative measure determines the number of correct ( and/or incorrect ) records that have been generated in any one database while the qualitative measure evaluates the `` correctness '' of any database record field .</sentence>
				<definiendum id="0">quantitative measure</definiendum>
				<definiens id="0">determines the number of correct ( and/or incorrect ) records that have been generated in any one database while the qualitative measure evaluates the `` correctness '' of any database record field</definiens>
			</definition>
			<definition id="3">
				<sentence>There are three types of scoring functions based upon the cardinality of the slot fillers : ( 1 ) single , ( 2 ) enumerated and ( 3 ) range , s An example of an ordered domain with single fillers is that of TEMPERATURE : ( make-frame TEMPERATURE ( instance-of ( value field ) ) ( database-in ( value z ) ) ( element-type ( value symbol ) ) ( domain-type ( value ordered ) ) ( cardinality ( value single ) ) ( elements ( value cold cool tepid lukewarm warm hot scalding ) ) ) 6l_nt'orrnal feedback thus far has indicated that these values are geared to having more emphasis placed on the records that contain easier fields and less on the harder ones , thus not rewarding those who perform well on the harder fields .</sentence>
				<definiendum id="0">) ) ( elements</definiendum>
				<definiens id="0">instance-of ( value field ) ) ( database-in ( value z ) ) ( element-type ( value symbol ) ) ( domain-type ( value ordered ) ) ( cardinality ( value single</definiens>
				<definiens id="1">these values are geared to having more emphasis placed on the records that contain easier fields and less on the harder ones</definiens>
			</definition>
			<definition id="4">
				<sentence>It is a continuous domain with ( single ) numeric fillers and its attribute entry is the following : ( make-frame CASOS_NOTIFICADOS ( instance-of ( value field ) ) ( database-in ( value I ) ) ( element-type ( value number ) ) ( domain-type ( value continuous ) ) ( cardinality ( value single ) ) ( unit-size ( value 1 ) ) ( elements ( value ( 0 1200.000 ) ) ) ) As before , suppose we are trying to match the CASOS_NOTIFICADOS slots between the actual output and the expected output : AO : ( casos_notificados ( value 2.700 ) ) EO : ( casos_notificados ( value 2.781 ) ) Since only numbers can be represented in a continuous domain , the elements of the domain are defined by giving the endpoints of the domain ( or closed interval ) and the unit size of representation is used in computing the distance between fillers .</sentence>
				<definiendum id="0">) ) EO</definiendum>
				<definiens id="0">trying to match the CASOS_NOTIFICADOS slots between the actual output and the expected output : AO : ( casos_notificados ( value 2.700</definiens>
				<definiens id="1">giving the endpoints of the domain ( or closed interval ) and the unit size of representation is used in computing the distance between fillers</definiens>
			</definition>
</paper>

		<paper id="1050">
</paper>

		<paper id="1081">
</paper>

		<paper id="1013">
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>Our supervised training experiments used a tri-tag model based on a corpus from the University of Pennsylvania consisting of Wall Street Journal articles in which each word or punctuation mark has been tagged with one of 47 parts of speech , as shown in the following example : A tri-tag model predicts the relative likelihood of a particular tag given the two preceding tags , e.g. how likely is the tag RB on the third word in the above example , given that the two previous words were tagged NNS and VBD .</sentence>
				<definiendum id="0">tri-tag model predicts</definiendum>
				<definiendum id="1">likely</definiendum>
				<definiens id="0">the relative likelihood of a particular tag given the two preceding tags</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>The two TAGs ( one for the natural language and one for the logical form language ) work synchronously , in the sense that the certain correspondences ( links ) are stated initially between the elementary trees of the two TAGs and then composition operations ( such as substitution and adjoining ) are carried out synchronously on the linked nodes of the two TAGs .</sentence>
				<definiendum id="0">two TAGs</definiendum>
				<definiens id="0">one for the natural language and one for the logical form language ) work synchronously , in the sense that the certain correspondences ( links ) are stated initially between the elementary trees of the two TAGs and then composition operations ( such as substitution and adjoining</definiens>
			</definition>
			<definition id="1">
				<sentence>The fact that both the natural language and the logical form language can be described by TAGs is a direct consequence of the extended domain of locality of TAGs as compared to LFG or GPSG .</sentence>
				<definiendum id="0">TAGs</definiendum>
				<definiens id="0">a direct consequence of the extended domain of locality of TAGs as compared to LFG or GPSG</definiens>
			</definition>
			<definition id="2">
				<sentence>Each element of the synchronous TAG is a pair consisting of two elementary trees , one from the source language ( English ) and one from the target ( logical form \ [ LF\ ] ) .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">a pair consisting of two elementary trees , one from the source language ( English ) and one from the target ( logical form \ [ LF\ ] )</definiens>
			</definition>
			<definition id="3">
				<sentence>2We use standard TAG notation , marking foot nodes in auxiliary trees with '* ' and nodes where substitution is to occur with '~ ' .</sentence>
				<definiendum id="0">substitution</definiendum>
				<definiens id="0">to occur with '~ '</definiens>
			</definition>
			<definition id="4">
				<sentence>Quantifiers In order to characterize quantifier scoping possibilities , multi-component TAGs ( as defined by Joshi , 1987 ) is used as the base formalism for synchronous TAG ( see Shieber and Schabes \ [ 1990 ( a ) \ ] for more details on quantifiers scoping with Synchronous TAG ) .</sentence>
				<definiendum id="0">multi-component TAGs</definiendum>
			</definition>
			<definition id="5">
				<sentence>The set of Tree Adjoining Languages is a strict superset of the set of Context Free Languages ( CFLs ) .</sentence>
				<definiendum id="0">Tree Adjoining Languages</definiendum>
			</definition>
			<definition id="6">
				<sentence>3 Since the set of Tree Adjoining Languages ( TALs ) is a strict superset of the set of Context Free Languages , in order to define LR-type parsers for TAGs , we need to use a more powerful configuration then a finite state automaton driving a push down stack .</sentence>
				<definiendum id="0">Tree Adjoining Languages</definiendum>
				<definiendum id="1">Free Languages</definiendum>
				<definiens id="0">a strict superset of the set of Context</definiens>
			</definition>
			<definition id="7">
				<sentence>An EPDA is similar to a pushdown automaton ( IDA ) except that the storage of an EPDA is a sequence of pushdown stores .</sentence>
				<definiendum id="0">IDA</definiendum>
				<definiendum id="1">EPDA</definiendum>
				<definiens id="0">a sequence of pushdown stores</definiens>
			</definition>
			<definition id="8">
				<sentence>52 Conclusion During the past year there have been two very significant developments in the area of Tree Adjoining Grammars ( TAGs ) : synchronous TAGs and efficient processing of TAGs .</sentence>
				<definiendum id="0">Tree Adjoining Grammars ( TAGs )</definiendum>
				<definiens id="0">synchronous TAGs and efficient processing of TAGs</definiens>
			</definition>
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>We assign Priority ( P , ~ , n ) = 10.0 x log ( Pr ( Pm , , ) ) ( 3 ) where Ni is the number of parses obtained for the i th sentence .</sentence>
				<definiendum id="0">Ni</definiendum>
				<definiens id="0">the number of parses obtained for the i th sentence</definiens>
			</definition>
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>Text classification systems include traditional information retrieval ( IR ) systems , which retrieve texts in response to a user query , as well as categorization systems , which assign texts to one or more of a fixed set of categories .</sentence>
				<definiendum id="0">Text classification systems</definiendum>
				<definiendum id="1">IR</definiendum>
			</definition>
			<definition id="1">
				<sentence>Text classification systems can be viewed as computing a function from documents to one or more class values .</sentence>
				<definiendum id="0">Text classification systems</definiendum>
				<definiens id="0">computing a function from documents to one or more class values</definiens>
			</definition>
			<definition id="2">
				<sentence>Text categorization systems can also be viewed as computing a function defined over documents , in this case a k-ary function , where k is the number of categories into which documents can be sorted .</sentence>
				<definiendum id="0">Text categorization</definiendum>
				<definiendum id="1">k</definiendum>
				<definiens id="0">computing a function defined over documents</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , the document title `` Speech and Speech Based Systems '' might be represented as 288 ( F , F , F , T , F , F , T , F , T , F , F , F..</sentence>
				<definiendum id="0">document title</definiendum>
			</definition>
			<definition id="4">
				<sentence>Recall is the percentage of all relevant documents which show up in the retrieved set , while precision is the percentage of documents in the retrieved set which are actually relevant .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">the percentage of all relevant documents which show up in the retrieved set</definiens>
				<definiens id="1">the percentage of documents in the retrieved set which are actually relevant</definiens>
			</definition>
			<definition id="5">
				<sentence>We associated with each phrase a vector of values of the form npc/~ nqc , where npc is the number of occurrences of phrase p in documents assigned to Computing Reviews category c , and the denominator is the total number of occurrences of all phrases in category c. This is the maximum likelihood estimator of the probability that a randomly selected phrase from documents in the category will be the given phrase .</sentence>
				<definiendum id="0">npc</definiendum>
				<definiendum id="1">the denominator</definiendum>
				<definiens id="0">the number of occurrences of phrase p in documents assigned to Computing Reviews category c</definiens>
				<definiens id="1">the maximum likelihood estimator of the probability that a randomly selected phrase from documents in the category will be the given phrase</definiens>
			</definition>
</paper>

		<paper id="1068">
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>*Modular unification-grammar files *High-level well-structured grammars for each language No domain-specific info in any language specification *Unifying grammar compiler ( or interpreter ) 375 *Takes language , domain and dictionary to produce runtime working MT system No human needs to cope with the output object code of unifying compiler , in the same way that no one needs to look at output of ADA compiler once verified .</sentence>
				<definiendum id="0">*Modular unification-grammar</definiendum>
				<definiens id="0">files *High-level well-structured grammars for each language No domain-specific info in any language specification *Unifying grammar compiler ( or interpreter ) 375 *Takes language , domain and dictionary to produce runtime working MT system No human needs to cope with the output object code of unifying compiler , in the same way that no one needs to look at output of ADA compiler once verified</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>SYSTEM ANSWER ( Wrong ) : NO INFORMATION SATISFIES YOUR REQUEST CORRECT ANSWER : FLT CODE FLT DAY FRM TO DEPT AREV AL FLT # CLASSES EQP MEAL STOP DC DURA ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... . 102122 1234567 ATL DEN 840 955 DL 445 FYBMQ 757 S/B 0 N 195 102123 1234567 ATL DEN 934 1054 EA 821 FYEQK 72S S/B 0 g 200 .</sentence>
				<definiendum id="0">SYSTEM ANSWER</definiendum>
				<definiens id="0">NO INFORMATION SATISFIES YOUR REQUEST CORRECT ANSWER : FLT CODE FLT DAY FRM TO DEPT AREV AL FLT # CLASSES EQP MEAL STOP DC DURA ... ...</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>Objects consist of currently active constraints , the set of flights that meet the constraints and a list of individual flights in focus .</sentence>
				<definiendum id="0">Objects</definiendum>
				<definiens id="0">consist of currently active constraints , the set of flights that meet the constraints and a list of individual flights in focus</definiens>
			</definition>
</paper>

		<paper id="1035">
</paper>

		<paper id="1048">
</paper>

		<paper id="1077">
			<definition id="0">
				<sentence>The Airborne Reconnaissance Mission Task Texts of simulated airborne reconnaissance reports were created using an automatic sentence generator based on a finite state syntax ( perplexity 6 ) and 497 word vocabulary , defined by the Royal Aerospace Establishment ( RAE ) , Farnborough UK .</sentence>
				<definiendum id="0">Aerospace Establishment ( RAE</definiendum>
				<definiens id="0">Airborne Reconnaissance Mission Task Texts of simulated airborne reconnaissance reports were created using an automatic sentence generator based on a finite state syntax ( perplexity 6 ) and 497 word vocabulary , defined by the Royal</definiens>
			</definition>
</paper>

		<paper id="1079">
</paper>

	</volume>
