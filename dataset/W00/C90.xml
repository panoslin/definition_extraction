<?xml version="1.0" encoding="UTF-8"?>
	<volume id="C90">

		<paper id="3050">
			<definition id="0">
				<sentence>Finally , we take side with Hammond ( 1988 ) on the question of how association lines are to be interpreted : transitivity is a crucial property of these relations , since the feature structures are highly hierarchical .</sentence>
				<definiendum id="0">transitivity</definiendum>
				<definiens id="0">1988 ) on the question of how association lines are to be interpreted</definiens>
			</definition>
</paper>

		<paper id="2036">
			<definition id="0">
				<sentence>The first factor , Pr ( c ) , is a prior model of word probabilities ; the second factor , Pr ( t\ [ c ) , is a model of the noisy channel that accounts for spelling transformations on letter sequences ( e.g. , insertions , deletions , substitutions and reversals ) .</sentence>
				<definiendum id="0">Pr ( c )</definiendum>
				<definiens id="0">a model of the noisy channel that accounts for spelling transformations on letter sequences ( e.g. , insertions , deletions , substitutions and reversals</definiens>
			</definition>
			<definition id="1">
				<sentence>4 del\ [ cp_l , cp_~\ ] if deletion chars\ [ cp_l , ce\ ] ' add\ [ cp_l , tp\ ] , if insertion chars \ [ ct , _ 1 \ ] Pr ( tlc ) = sub\ [ tp , cp\ ] , if substitution chars\ [ cp\ ] rev\ [ cp , Cp+t\ ] chars\ [ cp , cp+t\ ] ' if reversal where cp is the pth character of c , and likewise tp is the p ~ character of t. The five matrices are computed with a bootstrapping procedure .</sentence>
				<definiendum id="0">cp</definiendum>
				<definiendum id="1">likewise tp</definiendum>
				<definiens id="0">the pth character of c , and</definiens>
				<definiens id="1">the p ~ character of t. The five matrices are computed with a bootstrapping procedure</definiens>
			</definition>
</paper>

		<paper id="2009">
			<definition id="0">
				<sentence>GB Theory includes a series of modules that contain constraints and principles which govern the movement transformation .</sentence>
				<definiendum id="0">GB Theory</definiendum>
				<definiens id="0">includes a series of modules that contain constraints and principles which govern the movement transformation</definiens>
			</definition>
			<definition id="1">
				<sentence>The Projection Principle preserves the syntactic information and the semantic information at each level ( d-structure , s-structure , and logical form ) during the movement transformation .</sentence>
				<definiendum id="0">Projection Principle</definiendum>
				<definiens id="0">preserves the syntactic information and the semantic information at each level ( d-structure , s-structure , and logical form ) during the movement transformation</definiens>
			</definition>
			<definition id="2">
				<sentence>A Government-Binding based Logic Grammar is a 6-tuple GBLG = ( T,2 , B , S , C , R ) where : ( 1 ) T is the set of lexical terminals .</sentence>
				<definiendum id="0">Government-Binding based Logic Grammar</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">a 6-tuple GBLG = ( T,2 , B , S , C , R ) where : ( 1 )</definiens>
			</definition>
			<definition id="3">
				<sentence>( b ) Y. V is the set of virtual non-terminals .</sentence>
				<definiendum id="0">Y. V</definiendum>
				<definiens id="0">the set of virtual non-terminals</definiens>
			</definition>
			<definition id="4">
				<sentence>( c ) Y. M is the set of movement non-terminals .</sentence>
				<definiendum id="0">Y. M</definiendum>
			</definition>
			<definition id="5">
				<sentence>( d ) ~ G is the set of goals .</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">the set of goals</definiens>
			</definition>
			<definition id="6">
				<sentence>( 3 ) B C ~p is the set of bounding non-terminals .</sentence>
				<definiendum id="0">B C ~p</definiendum>
				<definiens id="0">the set of bounding non-terminals</definiens>
			</definition>
			<definition id="7">
				<sentence>A botmding non-terminal is a phrasal non-terminal with bounding node as its predicate symbol .</sentence>
				<definiendum id="0">botmding non-terminal</definiendum>
				<definiens id="0">a phrasal non-terminal with bounding node as its predicate symbol</definiens>
			</definition>
			<definition id="8">
				<sentence>A grammar element is defined rccursivcly in terms of logic connectives as follows : ( a ) A lexical tm'minal L E T is a grammar element .</sentence>
				<definiendum id="0">grammar element</definiendum>
				<definiens id="0">a grammar element</definiens>
			</definition>
			<definition id="9">
				<sentence>( e ) A goat E ~JG is a grammm '' element .</sentence>
				<definiendum id="0">goat E ~JG</definiendum>
				<definiens id="0">a grammm '' element</definiens>
			</definition>
			<definition id="10">
				<sentence>( 6 ) R is the set of production rules .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">the set of production rules</definiens>
			</definition>
			<definition id="11">
				<sentence>\ ] \ [ : or X E ~p , Y E ~v and TR is a transitive relation , X TR Y if ( 1 ) X is tile rule head of a production rule , and Y is a grammar clement in its ntle body , or ( 2 ) X is tile rule head of a production rule , 1 { Y.p is a grammar element in its rule body , and I TR Y , or ( 3 ) there exist 11 , 12 ... .. and I n E ~ , p , such that X TR I t 2 49 TR 12 TR ... TR I n TRY .</sentence>
				<definiendum id="0">TR</definiendum>
				<definiendum id="1">X</definiendum>
				<definiendum id="2">Y</definiendum>
				<definiens id="0">a transitive relation</definiens>
				<definiens id="1">tile rule head of a production rule</definiens>
				<definiens id="2">a grammar clement in its ntle body , or ( 2 ) X is tile rule head of a production rule</definiens>
			</definition>
			<definition id="12">
				<sentence>The transitive relation TRsubjacency is a subset of TR and satisfies the restrictions : for X E ~p , Y E Y~V , X TRsubjacency Y if X TR I l TR 12 TR ... TR I n TR Y , and there does not exist more than one Ij such that lj E B. Proposition 2 .</sentence>
				<definiendum id="0">transitive relation TRsubjacency</definiendum>
				<definiens id="0">a subset of TR and satisfies the restrictions : for X E ~p</definiens>
			</definition>
			<definition id="13">
				<sentence>Semantic denotes the semantic feature of tire head noun , It must be unifiable with tt~e semantic feature prey dec by the matrix verb with the type tree matching/McCord 198'7/ .</sentence>
				<definiendum id="0">Semantic</definiendum>
				<definiens id="0">the semantic feature of tire head noun</definiens>
			</definition>
</paper>

		<paper id="1015">
</paper>

		<paper id="2065">
			<definition id="0">
				<sentence>The Trip relation is defined as a triple of formal attributes : Tnp= &lt; \ [ np\ ] , Tn , Tdet &gt; where it is understood that the possible values of the first attribute are noun phrase denotations , the possible values of the second attribute are pointers to noun denotations , and the possible values of the third attribute are pointers to determiner denotations .</sentence>
				<definiendum id="0">Trip relation</definiendum>
			</definition>
			<definition id="1">
				<sentence>The way in which ( ~ is assumed to yield either truth value conforms to the account of standard formal semantics : it consists in checking whether the property , i.e. the set , denoted by the verb phrase is a member of the set of properties denoted by the noun phrase .</sentence>
				<definiendum id="0">verb phrase</definiendum>
				<definiens id="0">it consists in checking whether the property</definiens>
				<definiens id="1">a member of the set of properties denoted by the noun phrase</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , the sentence : Most men eat an apple denotes : Tdet ( `` most '' ) A Tn ( yl , '' men '' ) A Tnp ( y2 , yl , '' most '' ) A \ [ most\ ] ( yl ) = ( y2 ) A Tv ( y3 , '' eat '' , l ) ^ TdetCa '' ) ^ Tn ( y4 , '' apple '' ) ^ Tnp ( y5 , y4 , '' a '' ) A \ [ a\ ] ( y4 ) = ( y5 ) a Tvp ( y6 , '' eat '' , y3 , y5 ) A y3 _D y6 A y6 ~ ; y2 where ~ = y6 e y2 .</sentence>
				<definiendum id="0">Tvp</definiendum>
				<definiens id="0">the sentence : Most men eat an apple denotes : Tdet ( `` most '' ) A Tn ( yl , '' men '' ) A Tnp ( y2 , yl , '' most '' ) A \ [ most\ ] ( yl ) = ( y2 ) A Tv ( y3 , '' eat '' , l ) ^ TdetCa '' ) ^ Tn ( y4 , '' apple '' ) ^ Tnp ( y5 , y4 , '' a '' ) A \ [ a\ ] ( y4 ) =</definiens>
			</definition>
</paper>

		<paper id="3031">
			<definition id="0">
				<sentence>The corpus consists of 85 million English and 95 million French words from the Canadian Parliamentary Proceedings ( the ttansard corpus ) .</sentence>
				<definiendum id="0">corpus</definiendum>
				<definiens id="0">consists of 85 million English and 95 million French words from the Canadian Parliamentary Proceedings ( the ttansard corpus )</definiens>
			</definition>
			<definition id="1">
				<sentence>Klavans , J. L. ( 1988 ) `` COMPLEX : A Computational Lexicon for Natural Language Systems ' , Proceedings of the 12th International Conference on Computational Linguistics .</sentence>
				<definiendum id="0">COMPLEX</definiendum>
				<definiens id="0">A Computational Lexicon for Natural Language Systems '</definiens>
			</definition>
</paper>

		<paper id="2046">
			<definition id="0">
				<sentence>Code is the paradigmatic meaning that speakers find in the lexicon ( standard definitions ) .</sentence>
				<definiendum id="0">Code</definiendum>
			</definition>
			<definition id="1">
				<sentence>• the semantic primitives tie the lexical item to an ontological class and is relevant for the possible word readings • word order and negation is relevant for topic Generics Generics do not underly universal or nearly universal quantifiers .</sentence>
				<definiendum id="0">semantic primitives</definiendum>
				<definiendum id="1">negation</definiendum>
				<definiens id="0">relevant for topic Generics Generics do not underly universal or nearly universal quantifiers</definiens>
			</definition>
			<definition id="2">
				<sentence>Indefinite singular generic NPs are references to properties of individuals ( 3 ) , and definite singular NPs with count nouns ( 4 ) or indefinite ( without article ) mass nouns ( 5 ) are basically references to kinds .</sentence>
				<definiendum id="0">Indefinite singular generic NPs</definiendum>
				<definiens id="0">count nouns ( 4 ) or indefinite ( without article ) mass nouns ( 5 ) are basically references to kinds</definiens>
			</definition>
			<definition id="3">
				<sentence>Alex is an artist vs. Alex es artista 15 .</sentence>
				<definiendum id="0">Alex</definiendum>
				<definiens id="0">an artist vs. Alex es artista 15</definiens>
			</definition>
			<definition id="4">
				<sentence>`` me gusta besar tigres '' vs. '' I like to kiss tigers '' ( process ) `` pongo la mesa '' vs. `` Ich decke den Tisch '' `` tengo razon '' vs.Ich habe Recht '' of definiteness Definiteness is a means to satisfy coherence requirements .</sentence>
				<definiendum id="0">Definiteness</definiendum>
				<definiens id="0">a means to satisfy coherence requirements</definiens>
			</definition>
			<definition id="5">
				<sentence>Definiteness as a semantic value of definite determiners is one of its possible values ( the logical value of the quantifier `` only '' ) ; more common values are distributional , collective quantification , or a surface phenomena within generic constructions , or in connection with the predicative value .</sentence>
				<definiendum id="0">Definiteness</definiendum>
				<definiens id="0">distributional , collective quantification , or a surface phenomena within generic constructions , or in connection with the predicative value</definiens>
			</definition>
</paper>

		<paper id="2051">
			<definition id="0">
				<sentence>and Isabelle ( 1988 ) , in which an annotated detinite clause grammar is compiled differently based on the annotations , for the two purposes , ( b. ) the inversion of a systemic generator by Kasper ( 1988 ) ( in wlfic h phrase structure is said to be added mamially \ [ 'or parsing ) , ( c ) the I ) I'I\V 'generator of Caraceni and Stock ( 1988 ) , which is based on arl aug ' mented trartsition network ( ATN ) and which seems to employ a `` gm~erate and , tesC , approach to generation , and ( d ) .</sentence>
				<definiendum id="0">ATN</definiendum>
				<definiens id="0">in wlfic h phrase structure is said to be added mamially \ [ 'or parsing )</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , fbr example , in augmented phrase structure grammars ( APSGs ) , which include definite clause grammars , it is inconvenient |o specify each legal dependent ordering by a separale phrase structure rule .</sentence>
				<definiendum id="0">APSGs )</definiendum>
				<definiens id="0">inconvenient |o specify each legal dependent ordering by a separale phrase structure rule</definiens>
			</definition>
			<definition id="2">
				<sentence>, `` Radical l~exicalisnC , CSI , I Report CS1 , \ [ -86-68 ( 1986 ) 1 , ancel , J-M , Otani M. , Simonin N. , Danlos I , , `` SAGE : A Sentence Parsing and Generalion System '' , Proe COLING 88 , 359-364 McCord , M.C. , `` Slot Grammars '' , Computationa/ Linguistics , vol 6 , 31-43 ( 1980 ) McCord , M.C. `` A New Version of Slot Grammar '' , IBM Research Report RC : 14506 ( 1989a ) McCord , M.C. ~A New Version of the Machine Translation System I , MT '' , IBM Research Report RC 14710 ( 1989b ) , to appear in Proe .</sentence>
				<definiendum id="0">SAGE</definiendum>
				<definiendum id="1">IBM Research Report RC 14710</definiendum>
				<definiens id="0">the Machine Translation System I , MT '' ,</definiens>
			</definition>
</paper>

		<paper id="2019">
			<definition id="0">
				<sentence>A restrictor is a specification of a value wlfich can be computed from a feature structure ( syntactic category , for example , is often defined as a resUictor ) .</sentence>
				<definiendum id="0">restrictor</definiendum>
				<definiens id="0">a specification of a value wlfich can be computed from a feature structure ( syntactic category</definiens>
			</definition>
			<definition id="1">
				<sentence>Tt~e graulmar writer and the generator share tile responsibility for preventing additions to the input structure ( i.e. preserving coherence ) : the gramm~ writer must again select the appropriate data types , and the generator `` tTcezes '' uninstantiated variables that occur in the input .</sentence>
				<definiendum id="0">generator `` tTcezes</definiendum>
				<definiens id="0">'' uninstantiated variables that occur in the input</definiens>
			</definition>
			<definition id="2">
				<sentence>The ELU tormalism provides a generalization of the template facility of PATR-II , the `` relational abstractions '' , which are statements abstracting over sets of constraint equations .</sentence>
				<definiendum id="0">ELU tormalism</definiendum>
				<definiens id="0">provides a generalization of the template facility of PATR-II , the `` relational abstractions '' , which are statements abstracting over sets of constraint equations</definiens>
			</definition>
			<definition id="3">
				<sentence>Subcategorization lists are relational abstractions with multiple definitions ; therefore , they introduce non-detenninism in tile expansion of the rules in which they are invoked .</sentence>
				<definiendum id="0">Subcategorization lists</definiendum>
				<definiens id="0">they introduce non-detenninism in tile expansion of the rules in which they are invoked</definiens>
			</definition>
			<definition id="4">
				<sentence>Recall that when the semantics for the head daughter of a rule does not change , the rule is a chaining rule which is used bottom-up , but if the semantics of the head changes , then the rule is a non-chaining rule , which is used top-down and defines a pivot .</sentence>
				<definiendum id="0">rule</definiendum>
				<definiendum id="1">rule</definiendum>
				<definiens id="0">a chaining rule which is used bottom-up , but if the semantics of the head changes</definiens>
			</definition>
			<definition id="5">
				<sentence>In an analysis reminiscent of current work in the Government-Binding framework , 14 where a clause is IP ( Inflectional Phrase ) , the maximal projection of INFL , we take as the semantic head for our rules the element which bears tense .</sentence>
				<definiendum id="0">IP</definiendum>
				<definiens id="0">the semantic head for our rules the element which bears tense</definiens>
			</definition>
			<definition id="6">
				<sentence>`` BUG : A Directed Bottom Up Generator for Unification Based Formalisms '' .</sentence>
				<definiendum id="0">BUG</definiendum>
				<definiens id="0">A Directed Bottom Up Generator for Unification Based Formalisms ''</definiens>
			</definition>
</paper>

		<paper id="3040">
			<definition id="0">
				<sentence>Introduction Language interpretation involves mapping from a string of words to a representation of an interpretation of those words .</sentence>
				<definiendum id="0">Introduction Language interpretation</definiendum>
				<definiens id="0">involves mapping from a string of words to a representation of an interpretation of those words</definiens>
			</definition>
			<definition id="1">
				<sentence>box ( x ) s 0 ^ pen ( : ) s o ^ y ) s3 .</sentence>
				<definiendum id="0">box ( x )</definiendum>
			</definition>
			<definition id="2">
				<sentence>Conclusions Abduction is a good model for language interpretation , and commensurability is a vital component of an abduction system .</sentence>
				<definiendum id="0">Conclusions Abduction</definiendum>
				<definiendum id="1">commensurability</definiendum>
				<definiens id="0">a good model for language interpretation , and</definiens>
				<definiens id="1">a vital component of an abduction system</definiens>
			</definition>
</paper>

		<paper id="2070">
			<definition id="0">
				<sentence>In particular , Martin ( 1988 ) implemented the MIDAS system which both uses metaphoric word senses to help with language under° 407 standing , and to extend the lexicon when a new metaphoric use of a word is encountered .</sentence>
				<definiendum id="0">MIDAS system</definiendum>
				<definiens id="0">uses metaphoric word senses to help with language under° 407 standing , and to extend the lexicon when a new metaphoric use of a word is encountered</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , in `` John took a book from Mary '' , John is both the recipient and the agent .</sentence>
				<definiendum id="0">John</definiendum>
				<definiens id="0">both the recipient and the agent</definiens>
			</definition>
			<definition id="2">
				<sentence>The MIDAS metric is a simple sum of two factors : ( i ) the length of the core-relationship from the input source to the source of the candidate metaphor , and ( ii ) hierarchical distance between the two concepts .</sentence>
				<definiendum id="0">MIDAS metric</definiendum>
				<definiens id="0">a simple sum of two factors : ( i ) the length of the core-relationship from the input source to the source of the candidate metaphor</definiens>
			</definition>
			<definition id="3">
				<sentence>The Very Idea : A Case-Study in Polysemy and Cross-Lexical Generalization .</sentence>
				<definiendum id="0">Very Idea</definiendum>
			</definition>
			<definition id="4">
				<sentence>, Machine Learning : An Artificial Intelligence Approach , vol .</sentence>
				<definiendum id="0">Machine Learning</definiendum>
				<definiens id="0">An Artificial Intelligence Approach</definiens>
			</definition>
			<definition id="5">
				<sentence>Machine Learning : An Artificial Intelligence Approach .</sentence>
				<definiendum id="0">Machine Learning</definiendum>
			</definition>
			<definition id="6">
				<sentence>The Inherent Semantics of Argument Structure : The Case of the English Ditransitive Construction .</sentence>
				<definiendum id="0">Inherent Semantics of Argument Structure</definiendum>
				<definiens id="0">The Case of the English Ditransitive Construction</definiens>
			</definition>
</paper>

		<paper id="3100">
</paper>

		<paper id="3094">
			<definition id="0">
				<sentence>Original contributions include : a plan inference system that works directly from feature structures ; a plan inference system that uses an ATMS and plan schema actions with preconditions and effects to infer hierarchical and chained plans ; and , an inference engine that works with multiple featurestructure assertions and rules .</sentence>
				<definiendum id="0">ATMS</definiendum>
				<definiens id="0">a plan inference system that works directly from feature structures ; a plan inference system that uses an</definiens>
			</definition>
			<definition id="1">
				<sentence>dicld axtioJ~ infl &gt; li ( 's ea &lt; 'h of the pr ( 'con &lt; lil.ious ~I , II ( \ ] dcco ) , q &gt; , ) : d ( i &lt; ) ) is , and a ) , ( 'fl'c &lt; ' ( iml ) ti &lt; 's a l &gt; r &lt; 'dict ( ' &lt; l at\ [ ion. I ) lal~ inf ( ~renc &lt; . &lt; 'Olnl ) ris &lt; 's a ) ~m ( , &lt; ' } ) I~ ( ' ( .v¢ ( '~m r , ~ &lt; : ogniz~d and pr ( 'dic ( , ~xl ass ( , rl , ions. '2This is an ( al , fidged ) plan ~ , chcm ; ~ I ( ) ) 'cc ( ) gnize a short ~ ) , I|SW ( ~I '' ill\ [ Af ! l'.\ [ I-C ( il ) ll N ( ~ { , ill .\ ] ~tl ) ; t , ll ( ' , S~'t , ( ! .g. `` Aiill ; Li-~ho W~t ( ) , , , , , -hl , les , , k ; ( ? '' '' lie. M , , ( lei , , , as , ' , ,. '' ( `` ( I ) , , ) ( you ) I , ; , ve Ihc ~ , , , , , , , . , , : ,.. , cnt'e '' '' N. , . ( 1 ) , I , ,. ' ( h , ,v , , , '' \ [ ~i , \ ] ) . Sh , , , .t a.~w , ... . are formed in Jalwtnese by rCl ) e ; tting |he vm'l ) . AII , h ( mgh these are semantically and I ) ragmalically wvll f.rmcd , ( hey , :am.n , I ) c tran.. , latcd liLcrally Iml must Iw rccogniv , ed and lrcumferred , sin.'c I ' ; nglish f..vms sh , ,r ( ; mswcr. ' , by repealing Lhc auxiliary ( `` No. I dos'l. '' ) . The kqv fcalmc is ( h ; ~.l the w , b is rclw ; dcd wit.h ( &gt; , ~l , an ( ) l &gt; je ( : ( .</sentence>
				<definiendum id="0">ions. '2This</definiendum>
				<definiendum id="1">kqv fcalmc</definiendum>
				<definiens id="0">'dic ( , ~xl ass ( , rl</definiens>
			</definition>
			<definition id="2">
				<sentence>Assertion triggers a fast spreading activation in the truth maintenance network ( using bitvectors ) which maintains all consistent `` possible worlds '' .</sentence>
				<definiendum id="0">Assertion</definiendum>
				<definiens id="0">triggers a fast spreading activation in the truth maintenance network ( using bitvectors ) which maintains all consistent `` possible worlds ''</definiens>
			</definition>
</paper>

		<paper id="2057">
</paper>

		<paper id="3097">
</paper>

		<paper id="2017">
			<definition id="0">
				<sentence>Preferred antecedents are a subset of the possible antecedents , selected by the application of extralinguistic knowledge .</sentence>
				<definiendum id="0">Preferred antecedents</definiendum>
				<definiens id="0">a subset of the possible antecedents , selected by the application of extralinguistic knowledge</definiens>
			</definition>
			<definition id="1">
				<sentence>Definition 1 An antecedent Ant is a possible antecedent for an anaphor Ana iff Ant and Ana are compatible ~o antecedent compatible with Ana occurs iu a more recent unit The units referred to in the definition relate to units in the discourse. '</sentence>
				<definiendum id="0">Ant</definiendum>
			</definition>
</paper>

		<paper id="3070">
			<definition id="0">
				<sentence>The interactive inlormation base provides the learner with basic universal properties of the characters ( morphology , intrinsic meaning ) , extended wilh a quite comprehensive set of language-dependent aspects ( phonetics , extended semantics , contextual or pragmatic attributes ) .</sentence>
				<definiendum id="0">interactive inlormation base</definiendum>
				<definiens id="0">provides the learner with basic universal properties of the characters ( morphology , intrinsic meaning ) , extended wilh a quite comprehensive set of language-dependent aspects ( phonetics , extended semantics , contextual or pragmatic attributes )</definiens>
			</definition>
			<definition id="1">
				<sentence>Chinese characters Activities proposed in the system allow the study of a comprehensive set of properties of a character .</sentence>
				<definiendum id="0">Chinese characters Activities</definiendum>
				<definiens id="0">proposed in the system allow the study of a comprehensive set of properties of a character</definiens>
			</definition>
			<definition id="2">
				<sentence>Morphology : the etymology of the character , its iconographic origin and evolution , its generic category ( among 6 classical ones ) , the calligraphy ( the stroke order , the different writing styles , their evolution ) \ [ ZHONGGUO SHUFA DA ZIDIAN 1983\ ] , the structure ( synthetic representation of the morphological tree of the character , semantic and/or phonetic radicals within it ) , the use in derivation or composition within other characters .</sentence>
				<definiendum id="0">Morphology</definiendum>
				<definiens id="0">the etymology of the character , its iconographic origin</definiens>
				<definiens id="1">synthetic representation of the morphological tree of the character , semantic and/or phonetic radicals within it ) , the use in derivation or composition within other characters</definiens>
			</definition>
</paper>

		<paper id="2024">
			<definition id="0">
				<sentence>~ , xample : 1 s , eL det adj We have defined a language based on rewriting rules ; each rule applies to a dependency forest and produces a dependency tree .</sentence>
				<definiendum id="0">eL det adj</definiendum>
				<definiens id="0">a dependency forest and produces a dependency tree</definiens>
			</definition>
</paper>

		<paper id="2047">
			<definition id="0">
				<sentence>270 1 To formalize the concept of expected referent , : he resorts to the notion of Thema , defined as the subject of a primary predication , where x is a primary predicate of y iff x and y form a constituent which is either O-marked or \ [ + INFL\ ] .</sentence>
				<definiendum id="0">x</definiendum>
			</definition>
			<definition id="1">
				<sentence>Centering is an account of local coherence : it tries to determine the entity which an utterance most centrally concerns .</sentence>
				<definiendum id="0">Centering</definiendum>
				<definiens id="0">an account of local coherence : it tries to determine the entity which an utterance most centrally concerns</definiens>
			</definition>
			<definition id="2">
				<sentence>In \ [ GJW86\ ] the following rule R1 is proposed : in Un+l the speaker can use • a single pronoun , and that is the Cb ( Un+l ) ; • zero or more than one pronoun : then Cb ( U~+ , ) is Cb ( U~ ) if Cb ( Un ) is realized in U~+~ , -otherwise the highest ranked Cf ( Un ) which is realized in U , ~+I. In order to ensure a coherent discourse , the speaker has to apply the following rule R2 as well : Given Cb ( U~ ) = X , Cf ( U~ ) = { Y~ &gt; ... &gt; Y , ~ } , X = Yk , for some k , 1 &lt; k &lt; m : if there are pairs { Yi , Yj } , with i &lt; j , s.t. both Yi and Yj are realized in Un+l , and if \ ] Q is realized with a pronoun , then Y , has to be realized with a pronoun .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiens id="0">the Cb ( Un+l ) ; • zero or more than one pronoun</definiens>
			</definition>
			<definition id="3">
				<sentence>Cb ( U~ ) = Joh~ C : f ( U~ ) = { John &gt; Mike } U3 ) Hej was annoyed by Johni 's call .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiens id="0">Joh~ C : f ( U~ ) = { John &gt; Mike } U3 ) Hej was annoyed by Johni 's call</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>CAR is selected over ROAD as the anaphor of IT , since CAR BRAKE is a stronger collocation than ROAD BRAKE .</sentence>
				<definiendum id="0">CAR</definiendum>
			</definition>
			<definition id="1">
				<sentence>I TO SHAREHOLDERS/nn OF RECORD AUG Figure 4 : Word Pairs Tagged as to their Part of Speech 38 5 Exanlples ( 1 ) , ( 4 ) , and ( 5 ) support the hypothesis that StIAREHOLDER is an object2 ( the recipient ) of PAY .</sentence>
				<definiendum id="0">StIAREHOLDER</definiendum>
				<definiens id="0">the recipient ) of PAY</definiens>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>An identifier position is a position that admits an identifier of a phrase , such as a lexical head .</sentence>
				<definiendum id="0">identifier position</definiendum>
				<definiens id="0">a position that admits an identifier of a phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>A configuration is a triple \ [ D , C , p\ ] where I ) is an actual category , C is an actual category or a word , and p is a position .</sentence>
				<definiendum id="0">configuration</definiendum>
				<definiendum id="1">I )</definiendum>
				<definiendum id="2">C</definiendum>
				<definiendum id="3">p</definiendum>
				<definiens id="0">an actual category or a word</definiens>
			</definition>
			<definition id="2">
				<sentence>csal : ( S , - , E , Sul ) jAPredAVform ) csa2 : ( NP , -- , II , NumbAGend ) defl : deP2 : def3 : def4 : detS : def6 : deft : ( MainS , S , VEv , ) ( V2S , MainS , xGF , -- - ) ( V1S , MainS , eCI '' , ) ( PolS , V1S , NPOlex , Vform=Fin ) ( hnpS , V1S , NP-Gnex , Vfonn=hnp ) ( SubS , S , ( Comp ) ~_vAVcv ' , ~- ) ( lntS , SubS , eG\ [ `` A ( all ) Cv ANP~ ( Enex , Vfornr=lnf ) For instance , in ( def4 ) a polar interrogative clauses ( I'olS ) is detined as a verb-first clause ( V1S ) , which in ( def3 ) is deiined as a main clause ( MainS ) , which in turn is defined as a clause ( S ) .</sentence>
				<definiendum id="0">polar interrogative clauses ( I'olS</definiendum>
				<definiens id="0">( S , - , E , Sul ) jAPredAVform ) csa2 : ( NP , -- , II , NumbAGend ) defl : deP2 : def3 : def4 : detS : def6 : deft : ( MainS , S , VEv , ) ( V2S , MainS , xGF , -- - ) ( V1S , MainS , eCI '' , ) ( PolS , V1S , NPOlex , Vform=Fin ) ( hnpS , V1S , NP-Gnex , Vfonn=hnp ) ( SubS , S , ( Comp ) ~_vAVcv ' , ~- ) ( lntS , SubS , eG\ [ `` A ( all ) Cv ANP~ ( Enex , Vfornr=lnf ) For instance</definiens>
				<definiens id="1">a clause ( S )</definiens>
			</definition>
			<definition id="3">
				<sentence>A configuration rule may be written as a list of the form ( CS , F , i ) where CS is a description of a set of COllIigurations , F is a conjunction of functional constraints and i is an occurrence index .</sentence>
				<definiendum id="0">configuration rule</definiendum>
				<definiendum id="1">F</definiendum>
				<definiens id="0">a list of the form ( CS , F , i ) where CS is a description of a set of COllIigurations</definiens>
			</definition>
			<definition id="4">
				<sentence>UK*AnK* ) where A1 , ... , A , ~ are actual subcategories of A. then LD , p , ~ = Lu , vM ( K*AK*U { e } ) .</sentence>
				<definiendum id="0">A1 , ...</definiendum>
				<definiens id="0">actual subcategories of A. then LD , p , ~ = Lu , vM ( K*AK*U { e } )</definiens>
			</definition>
			<definition id="5">
				<sentence>tegory , C , takes one obligatory daughter , II , and two optional daughters A , B , according to the the Cl '' -grantmar G1 , there is no Basic FOG for L ( G1 ) that has C as an actual category .</sentence>
				<definiendum id="0">Cl</definiendum>
				<definiens id="0">takes one obligatory daughter , II , and two optional daughters A , B , according to the the</definiens>
			</definition>
			<definition id="6">
				<sentence>Such linearizations are expressed by means of partial orderings , or LP-rules , of the fern\ A &lt; B , It is obvious that this assumption is more nat urally made in a framework that works with local trees Ihat have only two or three branches than in a framework which employs fiat structures , t : 'or instance , the existence of unmarked and inverted clauses is not contradicting the FCPO-hypothesis , if the subject is regarded ~Ls a sister of the finite verb ouly in the inverted case. llowever , there are constructions that speak against it. as a universal , such as t.he order of object and verb in German main and subof dirlateclauses : Ich kauflc ein Auto ( I bm , ght a cat ' ) vs. lc\ ] ~ babe ei~..4 , ~to flckaufl ( i have a car bough1 : -1 have I ) ought a cat ' ) , and the order of verb partici-. pies and their complements in Swedish predicative and attributive constructions : Rapporlen dr '' bcatdlhl av Bor.q ( 'Fhe report is ordered by Borg ) vs. De~ av Borg beslMlda rapporten ( The by Borg ordered report = The report that Borg ordered ) . These constructions are not problematic for FCGs , however , although they necessitate a categorial split. Although the number of categorial spli { s can bc many in a FCC ; , one would not like tim number of schemas t.o 1oe very high. For a language like Swedish it seems possible to limit tl , e descriptioJ , to five schemas , one for each type ot ' pvojectiotl ( V , N , A , t ) ) and one for coordinated structures \ [ Ahrenb89\ ] . LP-rules are used also in franteworks which do not subscribe to the ECPO-property , such as IIPSG \ [ PolSag87\ ] . llowever , they need to be colnplemented by something , as they miss an important aspect of word order. As they apply to sister constituents , they fail to give any information on the position of a daughter relative to the phonological span of the mother. For instance , as a speaker of English I kt , ow that the definile article appears at the very beginning of an N1 ) and that relative clauses appear at the end. Given a set of IA~-rules ordering detcrmilLers , relative clauses and other NP-constituents we may possibly infer this information , but this is a roundabout way of doing it.. To express such facts dire.ctly we need a device that will impose a sequential strut5 5 ture on phonological spans , and it is tbr this purpose that the topological schema is useful. On the other hand partial orderings seem better suited to describe category-independent word order regularities. Consider the case of complements to a head. In the Germanic languages the norreal order would be the one expressed in ( 10 ) : NPcomplements precede PP-complements which precede verbal complements whatever the category of the head \ [ GPSG85 , p. 110\ ] . ( 10 ) NP-~ PP-~ VP The rule in ( 2 ) defining the complement field ( ObjF ) , repeated here for convenience , specifies three positions , one for bare objects , one for prepositional objects and one for verbal and adjectival complements. ObjF -- + obj 2 pobj* ( comp ) Even if we could appeal to the same or a similar field structure rule in the case of complements to the a.djective , it seems natural in this case to explain the ordering in terms of the difference in category between different complements. Thus , with the introduction of ( 1O ) ObjF could be regarded as at position , i.e. as a ternfinal of the schema in figure 1. Note however that in a FCG LP-rules receive a slightly different interpretation. They apply to positions rather than to local trees. Current work on FCG includes the implementation of a head-driven , bidirectional chart-parsing algorithm. The basic idea is to use fillers of identifier positions to trigger bottom-up predictions. FCGs have the advantage that the search for topologically different , alternative projections of a head or other identifier , can be accomplished by a single active edge. On the other hand the category of an edge is often abstract , and has to be determined on the basis of category definitions and the content of the edges that combined to introduce it. Finally it should be stressed that while FCG is a variant of a LFG , the idea of regarding the schemas of traditional field grammars as structures of partim information can of course be exploited in any unification-based formalism. \ [ Ahrenb89\ ] L. Ahrenberg : A formal field grammar. Research report LiTtI-IDA-89-46 , Link6ping university , department of Computer Science. \ [ Bresn82\ ] J. Bresnan ( ed. ) : The Mental Representation of Grammatical l~elalions , The MIT Press : Cambridge Mass. \ [ Braumn86\ ] K. Braunmtiller : Ilvor moderne er P. Diderichsens smtningsanMyse ? 1 \ [ II &amp; A 86\ ] pp. 77-98. \ [ Dider46\ ] P. Diderichsen : Elementazr DaT*sk Grammatik. Third edition. Copenhagen , Gyldendal. \ [ Drach37\ ] E. Drach : Grundgedanken der Deulschen Satzlehre. Frankfurt/M , Diesterweg ; reprint Darmstadt , Wiss. Buchgesellschaft , 1963. \ [ GPscss\ ] G. Gazdar , E. Klein , G. Pullum and I. Sag : Generalized Phrase .5'lr~zc &amp; re Grammar. Oxford , Basil Blackwell , t985. \ [ US A SS\ ] L. tleltoft and a. E. Andersson ( eds. ) : Scetningsskemaet og dets stilliny 50 dr crier. Nydanske studier &amp; ~ ahnen komm , lLnikalionsteori 16-17. Akademisk Forlag. 1986. \ [ LrGS2\ ] R. M. Kaplan and J. Bresnan : LexicalFunctional Grammar : A Formal System for Grammatical Representation , . \ [ a \ [ Bresn82\ ] , pp. 173-281. \ [ PolSag87\ ] C. Pollard and I. A. Sag : \ [ nforwalio~Based Syntaz aT~d Semantics. Volum.e i : Fundamentals. CSLI Lecture Notes , No. 13. Stanford. \ [ RueSr\ ] It. Rue : Danish field grammar in typed I~I~.OLOG. Proceedings of the Th.ird Cot &gt; ference of the European Chapter of the ACL , Copenhagen , April 1-3 , 1987 : lt37172 .</sentence>
				<definiendum id="0">MIT Press</definiendum>
				<definiens id="0">The by Borg ordered report = The report that Borg ordered )</definiens>
				<definiens id="1">NPcomplements precede PP-complements which precede verbal complements whatever the category of the head \ [ GPSG85</definiens>
				<definiens id="2">the implementation of a head-driven , bidirectional chart-parsing algorithm. The basic idea is to use fillers of identifier positions to trigger bottom-up predictions. FCGs have the advantage that the search for topologically different , alternative projections of a head or other identifier , can be accomplished by a single active edge. On the other hand the category of an edge is often abstract , and has to be determined on the basis of category definitions and the content of the edges that combined</definiens>
				<definiens id="3">A formal field grammar. Research report LiTtI-IDA-89-46 , Link6ping university , department of Computer Science. \ [ Bresn82\ ] J. Bresnan ( ed. ) : The Mental Representation of Grammatical l~elalions</definiens>
			</definition>
</paper>

		<paper id="2015">
			<definition id="0">
				<sentence>Edinburgh United Kingdom EH8 9LW email : judy @ uk.ac.ed.cogsci Phone : +44 31 668 4515 Introduction This pal~ ; r presents an analysis and synthesis of the factors relevant to the decision to use a cleft construction in discourse .</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">presents an analysis and synthesis of the factors relevant to the decision to use a cleft construction in discourse</definiens>
			</definition>
			<definition id="1">
				<sentence>The variable-containing presupposition is given as ex-pression P in the analysis of the cleft in example ( 8 ) below , while the assertion is ~ven as expression A. The notation is a linearised form of that used in Discourse Representation Theory ( cf. Kamp \ [ 1981\ ] ) , with the addition of eventuality indices as arguments standing for the events , states and processes described by the cleft : ( 8 ) It was the mango that Martin ate .</sentence>
				<definiendum id="0">variable-containing presupposition</definiendum>
				<definiens id="0">a linearised form of that used in Discourse Representation Theory ( cf. Kamp \ [ 1981\ ] ) , with the addition of eventuality indices as arguments standing for the events , states and processes described by the cleft</definiens>
			</definition>
			<definition id="2">
				<sentence>P : \ [ el , x , m\ ] ate ( el , x , m ) A : Is1 , x , y\ ] be ( s1 , x , y ) and mango ( y ) As suggested by van der Sandt \ [ 1988\ ] the presupposition serves the function in discourse of communicating to the hearer that a propositional ANTECEDENT needs to be found or constructed for the presupposed proposition .</sentence>
				<definiendum id="0">presupposition</definiendum>
				<definiens id="0">serves the function in discourse of communicating to the hearer that a propositional ANTECEDENT needs to be found or constructed for the presupposed proposition</definiens>
			</definition>
			<definition id="3">
				<sentence>Amsterdam : Mathematical Cantre Tracts .</sentence>
				<definiendum id="0">Amsterdam</definiendum>
			</definition>
			<definition id="4">
				<sentence>Reichman , R. \ [ 1981\ ] Plain Speaking : A Theory and Grammar of Spontaneous Discourse .</sentence>
				<definiendum id="0">Plain Speaking</definiendum>
				<definiens id="0">A Theory and Grammar of Spontaneous Discourse</definiens>
			</definition>
</paper>

		<paper id="3024">
			<definition id="0">
				<sentence>The study of argumentation involves the understanding of the propositional content of utterances , as well as the analysis of their linguistic structure , the relations with the preceding and following utterances , the recognition of the underlying conceptual beliefs , and general understanding within the global coherence of the discourse .</sentence>
				<definiendum id="0">study of argumentation</definiendum>
				<definiens id="0">involves the understanding of the propositional content of utterances</definiens>
			</definition>
			<definition id="1">
				<sentence>Argumentation is essentially a relational phenomenon : how do the propositions which are uttered in a discourse relate to each other .</sentence>
				<definiendum id="0">Argumentation</definiendum>
			</definition>
			<definition id="2">
				<sentence>Knowledge consists mainly of a detailed description of the properties of linguistic structures which play an argumentative role .</sentence>
				<definiendum id="0">Knowledge</definiendum>
				<definiens id="0">consists mainly of a detailed description of the properties of linguistic structures which play an argumentative role</definiens>
			</definition>
			<definition id="3">
				<sentence>Each module contributing to the general understanding process by providing a specific set of constraints resulting from the analysis of the input : the Conceptual Base contains all the domain conceptual relations .</sentence>
				<definiendum id="0">Conceptual Base</definiendum>
				<definiens id="0">contains all the domain conceptual relations</definiens>
			</definition>
			<definition id="4">
				<sentence>the Relation Finder derives appropriate relations from the conceptual knowledge represented in canonical form .</sentence>
				<definiendum id="0">Relation Finder</definiendum>
				<definiens id="0">derives appropriate relations from the conceptual knowledge represented in canonical form</definiens>
			</definition>
			<definition id="5">
				<sentence>For but and almost , we have for instance : ( A but B ) argument ( for , A , C ) argument ( against , B , C ) stronger-opposite ( B , A , opposite ( C ) ) argumentative-orientation ( operator ( but , A , B ) , opposlte ( C ) ) ( almost A ) argument ( for , A , C ) argumentative-orientation ( operator ( almost , A ) , C ) The predicate argumentative-orientation is used to assert the final orientation of an expression containing an operator or a connector .</sentence>
				<definiendum id="0">B )</definiendum>
				<definiendum id="1">operator</definiendum>
				<definiens id="0">the final orientation of an expression containing an operator or a connector</definiens>
			</definition>
			<definition id="6">
				<sentence>The basic algorithm consists of the following steps : listing the contextual constraints listing the linguistic constraints resulting from the use of clue words searching for argumentative relations coherent with the previous constraints computation of the argumentative orientation It is extended to include the computation of contextual constraints and the derivation and learning of new conceptual relations .</sentence>
				<definiendum id="0">basic algorithm</definiendum>
				<definiens id="0">consists of the following steps : listing the contextual constraints listing the linguistic constraints resulting from the use of clue words searching for argumentative relations coherent with the previous constraints computation of the argumentative orientation It is extended to include the computation of contextual constraints and the derivation and learning of new conceptual relations</definiens>
			</definition>
</paper>

		<paper id="3007">
			<definition id="0">
				<sentence>Both Mellish and Kasper \ [ 7\ ] provide translations of systemic networks into non-graphical formMisms : Mellish expresses constraints as axioms within a simple subset of predicate logic , while Kasper uses an extended version'of Functional Unification Grammar \ [ 10\ ] .</sentence>
				<definiendum id="0">Kasper</definiendum>
				<definiens id="0">provide translations of systemic networks into non-graphical formMisms : Mellish expresses constraints as axioms within a simple subset of predicate logic</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>The linguistic information resides in tlhe following sets of rules : 1 ) Japanese-to-English Automatic Dictionary ( at present 32000 entries ) , held in a relational database with seven fields for each entry ( combined key comprises the fields Entry_~ord , Translation , ~lord .</sentence>
				<definiendum id="0">Japanese-to-English Automatic Dictionary</definiendum>
			</definition>
			<definition id="1">
				<sentence>Substitution Rules ( which deal with all remaiming word order transfer , which at this stage is Entry-Specific ) Substitution is a set of functions executing rules which finalise the English word order down to the lowest level of trees , but the output remMns in the form of trees .</sentence>
				<definiendum id="0">Substitution Rules</definiendum>
				<definiendum id="1">Entry-Specific ) Substitution</definiendum>
			</definition>
			<definition id="2">
				<sentence>Generation Rules ( which produce English word forms ) Generation produces actual English sentences by scanning all tree node labels in post-order , activating Generation rules by node labels .</sentence>
				<definiendum id="0">Generation Rules</definiendum>
				<definiens id="0">produce English word forms ) Generation produces actual English sentences by scanning all tree node labels in post-order , activating Generation rules by node labels</definiens>
			</definition>
</paper>

		<paper id="2041">
			<definition id="0">
				<sentence>A basic ( bidirectional ) categorial grammar B is defined by a set of categories C : = C0U { zl ~ = x/Y or z =x\y ; x , yC C } ( C0 a finite set of basic categories , the category y is referred to as the argument category , the category x is called value category , complex categories are named functor categories ) , a goal category g ( the start symbol ) which is a basic category , a lexicon L which is a function from a finite set of lexemes onto a set of finite sets of categories , and the two combination rules `` leftward application '' ( app\ ) and `` rightward application '' ( app/ ) which state how argument positions are filled : ( app\ ) y , xky ~ x An object U -- * x where U is a sequence of categories is called a sequent .</sentence>
				<definiendum id="0">basic ( bidirectional ) categorial grammar B</definiendum>
				<definiendum id="1">yC C } ( C0</definiendum>
				<definiens id="0">a set of categories C : = C0U { zl ~ = x/Y or z =x\y</definiens>
				<definiens id="1">a finite set of basic categories , the category y is referred to as the argument category , the category x is called value category , complex categories are named functor categories ) , a goal category g ( the start symbol</definiens>
				<definiens id="2">a basic category , a lexicon L which is a function from a finite set of lexemes onto a set of finite sets of categories , and the two combination rules `` leftward application '' ( app\ ) and `` rightward application '' ( app/ ) which state how argument positions are filled : ( app\ ) y , xky ~ x An object U -- * x where U is a sequence of categories is called a sequent</definiens>
			</definition>
			<definition id="1">
				<sentence>gorial grammar A chart is a set of items .</sentence>
				<definiendum id="0">gorial grammar A chart</definiendum>
				<definiens id="0">a set of items</definiens>
			</definition>
			<definition id="2">
				<sentence>The completer step is the part of the CKYalgorithm which determines its cubic time complexity .</sentence>
				<definiendum id="0">completer step</definiendum>
				<definiens id="0">the part of the CKYalgorithm which determines its cubic time complexity</definiens>
			</definition>
			<definition id="3">
				<sentence>We assume that each value category of a cornplex argument of an emitter is marked with a unique number m. Items also have mfique nlHnbers a. A ( left ) index of an item is now a quadruple &lt; t , m , i , p ) where t stands tbr the type of miui-.chart the \tern belongs to : 1 ( left ) , r ( right ) , or ~ , ( , ~one ) ; , n is the ltumber of the subcategory which caused the assertion of this \tern ; i is a position nulnber relative to the ( re\hi- ) chart wi~h type t and number m ; p is the ~urnber of an item in another part of the &lt; : hart where the mini-chart is attached to. Por items which have been deriw , 'd on the basis of the items given by the input string , t : = n , m = 0 , and p = 0. In figure 4 , the extended algorithm is presented. From the following discussion , various details and , in particular , a proof of correctness , are omitted due to the laek of space. The rule ( compl\s ) is roughly equivalent to ( complete\ ) in the basic algorithm : two items which are adjacent in the same piece of the ( : hart are combined. The rule ( compl\r ) allows an il ; em at tile beginning of a right mini-chart to cornline witll sorne other item with number al on its left. The function newlast adds the information td~out this new attachment point at the end of the 3In the following , symmetric cases are mosl , ly omitted. right attachment chain of item a2 ( see item 6 in figure 2 where the item id 0 is replaced by the id 5 because of'a ( compl\ r ) -step which combines items 5 and 9 ) . In order to avoid cycles , the intersection o\ [ ' the mini-chart numbers which are fbund by traversing all the tbur attachment chain 's has to be empty. The work of the abstraction rule ( abstr\ ) is split into the two sub-tasks : The rule ( emit\ ) pertbrms the assertion of the mini-charts. The rule ( di , sc\ ) ( `` discharge '' ) is a specialization of the completer rule for fllnctor categories which are emitters. The conditions in the rule guarantee the following : Both minicharts which are due to the current emitter must have been used completely in deriving y. The rigi~t index of the attachment point p4 of y ( determined by a function ri ) must be equal to the left index of the emitter. The value category x must not bear any information about the involvement of the current mini-charts in its derivation. This is achieved by going back to the attachment point of the left mini-chart. Since each of the O ( n ) mini-charts fbr an input string of length n can only be used once in a derivation , and since we currently do not know any reasonable restrictions , we assume that any one of the © ( n ! ) permutations of the mini-charts possibly can be used in a specific derivation. For every item in the initial chart , O ( n ! ) completer-steps can be possible. This means that the whole parser has a time complexity of O ( n ~ x n ! ) . In order to evaluate the result for tim given algorit , hm , one has to be aware of tile fact that. this parsing procedure can handle n-fold extraction from phrases to speak linguistically. If we restrict ourselves to single extraction ( this can be implemented by checking the length of the attachment chain before performing ( compl\ r ) ) then the time complexity reduces drastically : The formula n ! / ( n k ' ) ! tbr tile variations of length k of a set of length 'n equals n tbr k ' = 1. Thus , the time cornplexity of the algorithm is O ( n : ~ ) . If we want to describe two-tbld extraction like in figure 2 , i.e. use a `` 2-restricted version of Lc '' , the algorithm needs O ( n 4 ) steps for deriving an input string of length n. Conjecture : 1-restricted Lc covers the rules of functional compositio n and type raising , whereas 2rest , rioted LC suffices to derive the rules of predictive combination. We have presented an extension of tile CKY-algorithm for an instance of the family of extended categorial grammars which uses hypothetical reasoning , i.e. for the bidirectional Lambek categorial grammars. Although the current algorithm has an undesirable time complexity in its gener~d case , one carl define restricted versions which cover tile kind of structures found in linguistic examples with time complexity O ( n 3 ) or O ( n 4 ) depending on the. fact 3 235 whether one wishes to describe single or double extraction from phrases. It is an interesting question whether better Mgorithms for the general case can be found. There are many ways to improve this new algorithm concerning its `` average complexity '' in practical applications. For example , the search for a partner item in a completer-step could be reduced by using appropriate heuristics. Further incre~es in efficiency can be expected , if one allows for arbitrary embedding of disjunctions and slashes in categories ( of. \ [ Benthem 19891 , \ [ Morrill 1989\ ] ) instead of using the flat set notation which amounts to a disjunctive normal form of categories in the lexicon. Theoretical complexity results for a group of categorial grammars reaching beyond context-free languages are known , as well. From the fact that certain extended versions of categorial grammars ( CCG ) are equivalent with the Tree Adjoining Grammars ( TAG ) ( \ [ Weir , Josh\ [ 19881 ) , the O ( n 4 log n ) time complexity of TAG-parsers ( \ [ tIarbusch 1989\ ] ) , in principle , carries over to CCG. The connection of the propositional grammar calculi as discussed above with a unification device for first order germs is done by taking care of the variable bindings in the hypothetical categories which are asserted by the abstraction rules. The recognition problem is still decidable ( cf. \ [ Pareschi 1988\ ] ) although surely not wellbehaved as this is the case for all non-propositional grammars. For example , the recognition problem for a. basic categorial grammar witti feature unification is NP-complete ( \ [ Morrill 1988\ ] ) . Higher order versions of categorial grammars like the ones being produced in the CUG/UCG-frameworks ( \ [ Uszkoreit 1986\ ] , \ [ Zeevat et at. 1986\ ] ) where variables can range over categories are in general undecidable because higher order unification itself is undecidable. The research reported in this paper is supported by the LILOG project , and a doctoral fellowship , both from IBM Deutschland OmbH , and by the Esprit Basic Research Action Project 3175 ( DYANA ) . I thank Andreas Eisele for discussion. The responsibility for errors resides with me. 236 4 `` l ( ~l~p ) ( n , o , o , o ) I~ ) '° '' '° ) _~ Wel % / whom S n , o , o , o ) n , O,5,0 ) 15 ) s/np ( n , O , l , O ) ( r , mt , l,5 ) In , O,1,0 ) _/ '' \ says ~/ ( ( ~\~p ) \~ ) n , o,2 , o ) r , m2,1,11 ) ( ',3 ) r , m2,1,11 } 12 ) ( n , o,3 , o ) ( r , ml , l , a ) `` ~. np s/ ( ( s\np ) \np ) ( n , o,2 , o ) s 1~ ; , ° , '' / { ~ , ) , ~ , o~ \.. poto~ s/ ( ~/~ ) ~ ) , o,4 , o ) { r , mL0,0 ) liebt np /~ ) , rn3,1,6 ) loves ( n , o,4 , o ) Ig , o , ~ , o~ / -- -~ ( z ' , ml , O , O ) ( r , ~ , a , o , o ) } r , ,~l , t , CJ ) ( r , ~ , a,1 , o ) 6 ) ( 8 ) ( 8\~p ) \s ( r , ,n % 0 , o ) ( r , m2,1,0 ) ( 7 ) Figure 2 : Long dist~mee dependency \ [ tv = ( s\np ) \np , iv2 = ( s\np ) \s\ ] x/\ [ ... ( y/x~ ) \xl~ ) ... \x11 ) /~21\ ] \ [ . ( y/~ ) \~ip ... \~. ) /~q Y ~.. ~ x~. ~ ~ ... ~ ; j Figure 3 : Schema for the use of the abstraction rules 5 237 ( axiom ) ( sca , O \ [ ( ~ ) ( ol \ ] I , ( n , o , o , o ) • -~ z ( n , o , , , ,o ) \ [ ( ~ ) ( o &gt; x 6 L ( w ) I , /n , o , i , o ) ( It , O , i+1,0 ) \ [ O W ~ O~ -- -~ Z o O~ -- + Z ( compl\ s ) p2=OVp3=O , ( t2 , m2 , i2 , p3 ) , ( tl , ml , il , pl ) , ooz -+ z ( t2 , rn2 , i2 , p2 ) ( t4 , m4 , i4 , p4 ) ( t4 , m4 , i4 , p4 ) \ ] : \ [ \ ] I , \ [ Y { tI , ml , ii , pl ) x\y ( t2 , m2 , i2 , p3 ) • OL -- + Z { t2 , m2 , i2 , p2 ) ( t 4 , m4,14 , p4 ) ( compl\ I ) chain ( p1 ) F1 chain ( p2 ) N chain ( p3 ) n chain ( p4 ) = { } newlast ( pl , a2 ) \ [ ( v ) ~o , l \ [ ( ~\~ ) ~o2~ l \ [ ( ~ ) ~o~ .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">re\hi- ) chart wi~h type t</definiendum>
				<definiendum id="2">p</definiendum>
				<definiendum id="3">O</definiendum>
				<definiens id="0">the ~urnber of an item in another part of the &lt; : hart where the mini-chart is attached to. Por items which have been deriw , 'd on the basis of the items given by the input string , t : = n , m = 0</definiens>
				<definiens id="1">n 4 ) steps for deriving an input string of length n. Conjecture : 1-restricted Lc covers the rules of functional compositio n and type raising , whereas 2rest , rioted LC suffices to derive the rules of predictive combination. We have presented an extension of tile CKY-algorithm for an instance of the family of extended categorial grammars which uses hypothetical reasoning</definiens>
				<definiens id="2">a disjunctive normal form of categories in the lexicon. Theoretical complexity results for a group of categorial grammars reaching beyond context-free languages are known , as well. From the fact that certain extended versions of categorial grammars ( CCG ) are equivalent with the Tree Adjoining Grammars ( TAG ) ( \ [ Weir</definiens>
				<definiens id="3">NP-complete ( \ [ Morrill 1988\ ] ) . Higher order versions of categorial grammars like the ones being produced in the CUG/UCG-frameworks</definiens>
			</definition>
</paper>

		<paper id="3084">
			<definition id="0">
				<sentence>arg ( PP ) l , where cp represents the complementizer phrase , Ihe head of the CP or S-bar , inll denotes the inflectional elemenl , tile head of S , and predicate is tile verb .</sentence>
				<definiendum id="0">cp</definiendum>
				<definiens id="0">the complementizer phrase , Ihe head of the CP or S-bar , inll denotes the inflectional elemenl , tile head of S , and predicate is tile verb</definiens>
			</definition>
			<definition id="1">
				<sentence>The indireet_arg is a prepositional phrase ( PP ) that a verb may license and there may be several of these structures depending on the lexical specifications of the verb .</sentence>
				<definiendum id="0">indireet_arg</definiendum>
				<definiendum id="1">PP</definiendum>
				<definiens id="0">a prepositional phrase</definiens>
			</definition>
			<definition id="2">
				<sentence>Word is a lexical item , an empty category PRO , trace , or variable , or an empty complementizer or inflectional element , and Featm'es represents the type of the node in terms of x-bar features_+N , _+V. Every node receives a unique index unless it is bound ( coindexed ) to another via Binding or Control Theory .</sentence>
				<definiendum id="0">Word</definiendum>
				<definiens id="0">a lexical item , an empty category PRO , trace , or variable , or an empty complementizer or inflectional element , and Featm'es represents the type of the node in terms of x-bar features_+N</definiens>
			</definition>
			<definition id="3">
				<sentence>the parser produces ( 7 ) \ [ cp ( emp , cp , 1 ) , ext_arg ( john , x_bar ( n l , vo ) ,2 ) , infl ( was , infl,3 ) , predicate ( persuaded , x_bar ( no , v t ) ,4 ) , int arg_ 1 ( trace , x bar ( n I , vo ) ,2 ) , int_arg 2 ( cp l , cp,5 ) , \ [ cp_l ( emp , cp,5 ) , ext arg ( pro , x bar ( n I , vo ) ,2 ) , in fl ( to , infl,6 ) , predicate ( leave , x bar ( no , v 1 ) ,7 ) \ ] 1 , which illustrates PP , O and trace detection and binding .</sentence>
				<definiendum id="0">ext arg</definiendum>
				<definiens id="0">illustrates PP , O and trace detection and binding</definiens>
			</definition>
</paper>

		<paper id="2002">
</paper>

		<paper id="2016">
			<definition id="0">
				<sentence>25/389 A-1040 Vienna , Austria emaih E38901 I @ AWITUW01.BITNET Appropriate formalisms for obtaining a basis for stress and pitch information were introduced by Kiparski ( 1973 ) , who proposed an algorithm for computing a stress hierarchy for a whole sentence , and Bierwisch ( 1973 ) , who showed how to determine pitch variation patterns depending on the phrasal structure of a sentence .</sentence>
				<definiendum id="0">Bierwisch</definiendum>
				<definiens id="0">who showed how to determine pitch variation patterns depending on the phrasal structure of a sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>DS ( CLAUSE ) r~ces~ -- -\ ] SS .</sentence>
				<definiendum id="0">DS</definiendum>
			</definition>
			<definition id="2">
				<sentence>at each significant node that has at least two significant successor nodes , do the following , given the index pair ( n m ) : with head stress rule : assign the pair ( n m+l ) to the first successor assign the pair ( n+m 1 ) to all the others with tail stress rule : assign the pair ( n re+l ) to the last successor assign the pair ( n+m 1 ) to all the others at the leaves of the tree ( = lexical entry ) , with assigned pair ( n m ) : n is the Kiparski marker for the lexical item If one considers the preferred successor ( head or tail , depending on the rule ) as the winner of the rule and all others as losers , algorithm ( 3 ) can be interpreted as follows : The second index of a pair ( m ) counts how often a node is on the winning side .</sentence>
				<definiendum id="0">n re+l</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">The second index of a pair ( m ) counts how often a node is on the winning side</definiens>
			</definition>
			<definition id="3">
				<sentence>A Kiparski pair ( 1 1 ) and a Bierwisch index 1 are assigned , The corresponding surface building block , S , is generated , filled with the lexical item betrdgt ( verb ) and with the two PHRASES in their correct position ( which can be determined by looking at the features and using some default heuristics as in Engel 1982 ) .</sentence>
				<definiendum id="0">Kiparski pair</definiendum>
				<definiens id="0">corresponding surface building block , S , is generated , filled with the lexical item betrdgt ( verb ) and with the two PHRASES in their correct position ( which can be determined</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>Obvviously , this space of inhomogenous connectivity is limited in our implementation and is 2 '' 2 '' n ( where n is the number of dotted rules ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of dotted rules</definiens>
			</definition>
			<definition id="1">
				<sentence>X ) is the inhomogenous processing space whose connectivity corresponds strictly to the structure of the grammar from which it is compiled .</sentence>
				<definiendum id="0">X )</definiendum>
			</definition>
</paper>

		<paper id="3039">
			<definition id="0">
				<sentence>TAMERLAN is a frame-based representation language that hat ; the following basic entity types : text , clause , relation , proposition , attitude and pointer ( to the producer 's current plan ) .</sentence>
				<definiendum id="0">TAMERLAN</definiendum>
			</definition>
			<definition id="1">
				<sentence>TPs serve as input to the semantics-tosyntax mapping rules that produce f-structures which in turn serve as input to the syntactic realization module .</sentence>
				<definiendum id="0">TPs</definiendum>
				<definiens id="0">serve as input to the semantics-tosyntax mapping rules that produce f-structures which in turn serve as input to the syntactic realization module</definiens>
			</definition>
			<definition id="2">
				<sentence>A text plan is an hierarchically organized set of frames .</sentence>
				<definiendum id="0">text plan</definiendum>
				<definiens id="0">an hierarchically organized set of frames</definiens>
			</definition>
			<definition id="3">
				<sentence>Text planning can be understood as a mapping problem between sets of expressions in TAMERLAN and TPL .</sentence>
				<definiendum id="0">Text planning</definiendum>
				<definiens id="0">a mapping problem between sets of expressions in TAMERLAN and TPL</definiens>
			</definition>
			<definition id="4">
				<sentence>The corresponding set of ( somewhat simplified ) TAMERLAN structures is in ( 17 ) and ( 18 ) , since the structures for ( 15 ) and ( 16 ) will be identical -indeed , the sentences are synonymous and differ only in what is known as register characteristics , in this case , the level of formality .</sentence>
				<definiendum id="0">corresponding set of</definiendum>
				<definiens id="0">synonymous and differ only in what is known as register characteristics</definiens>
			</definition>
			<definition id="5">
				<sentence>time ) ) ) ) ( 19 ) ( TSF ( has-as-part Sl ) ) ( Sl ( type simple ) ( clauses CI ) ) ( Cl ( head be back ) ( time ( head i0 ) ( features ( tlme-relation before ) ) ) ( modifier ( head definitely ) ) ( features ( tense future ) ( mood declarative ) ) ( agent rl ) ( destination r2 ) ) ( rl ( head PRO ) ( features ( r2 ( 2O ) ( number singular ) ( person first ) ) ) ( head *ellided* ) ) ( TSF ( has-as-part SI ) ) ( Sl ( type simple ) ( clauses CI ) ) ( Cl ( head be back ) ( features ( tense future ) ( mood declarative ) ) ( time ( head i0 ) ( features ( time-relation before ) ) ) ( agent rl ) ( destination r2 ) ) ( rl ( head PRO ) ( features ( number singular ) ( person first ) ) ) ( r2 ( head *ellided* ) ) ( 2~ ) ( TSF ( has-as-part SI ) ) ( Sl ( type complex ) ( clauses ( CI C2 ) ) { Cl ( head promisel ) ( features ( tense presen % ) ( mood declarative ) ) ( agent rl ) ( theme C2 ) ) ( rl ( head PRO ) ( features ( number singular ) ( person first ) ) ) ( C2 ( head be back ) ( time ( head i0 ) ( features ( tlme-relation before ) ) ) ( features ( tense future ) ( mood declarative ) ) ( agent r2 ) ( destination r3 ) ) ( r2 ( head PRO ) { features ( number singular ) ( person first ) ) ) ( r3 ( head *ellided* ) ) Text planning rules in DIOGF .</sentence>
				<definiendum id="0">TSF</definiendum>
				<definiendum id="1">mood declarative ) ) ( agent rl ) ( destination r2 ) ) ( rl</definiendum>
				<definiendum id="2">singular )</definiendum>
				<definiendum id="3">time-relation before ) ) ) ( agent rl ) ( destination r2 ) ) ( rl</definiendum>
				<definiens id="0">TSF ( has-as-part SI ) ) ( Sl ( type complex ) ( clauses ( CI C2 ) ) { Cl ( head promisel ) ( features ( tense presen % ) ( mood declarative ) ) ( agent rl ) ( theme C2</definiens>
			</definition>
			<definition id="6">
				<sentence>However , this methodology allows us to benefit from a complete experimentation environment and an open-ended architecture that facilitates the addition of knowledge to the system as well as testing and debugging .</sentence>
				<definiendum id="0">open-ended architecture</definiendum>
				<definiens id="0">facilitates the addition of knowledge to the system as well as testing and debugging</definiens>
			</definition>
			<definition id="7">
				<sentence>A Comparison of Surface Language Generators : A Case Study in Choice of Connectives .</sentence>
				<definiendum id="0">Comparison of Surface Language Generators</definiendum>
				<definiens id="0">A Case Study in Choice of Connectives</definiens>
			</definition>
</paper>

		<paper id="3014">
			<definition id="0">
				<sentence>PDF ( Phoneme Distinctive Feature ) &lt; -- { Nasal , Voiced ... . } ; This feature presents the practical phonetic values of the Korean language .</sentence>
				<definiendum id="0">PDF</definiendum>
				<definiens id="0">the practical phonetic values of the Korean language</definiens>
			</definition>
			<definition id="1">
				<sentence>j '' M / '' C H It where M and M ' are the mother categories , C and H are the daughter categories , in which C is the complement category , and tt is the head category , respectively .</sentence>
				<definiendum id="0">tt</definiendum>
				<definiens id="0">the daughter categories , in which C is the complement category</definiens>
			</definition>
			<definition id="2">
				<sentence>( 1-4 ) nasal assimilation ( nassalization ) a. k &gt; ~/ ... .. \ [ +na~l\ ] b. p &gt; m/ ... .. \ [ +nasal\ ] c. t &gt; n/ ... .. \ [ +nasal\ ] ( 1-4 ) is the formal descriptions of the phonological rules based on the theory of generative phonology .</sentence>
				<definiendum id="0">nasal assimilation</definiendum>
				<definiendum id="1">1-4 )</definiendum>
				<definiens id="0">the formal descriptions of the phonological rules based on the theory of generative phonology</definiens>
			</definition>
			<definition id="3">
				<sentence>Chtmg , H. S. , and Kunii , T. L. , `` NARA : A Two-way Simultaneous Interpretation System between Korean and JapanseA Methodological Study- '' , Proceedings of COLING '86 , Bonn , 1986 .</sentence>
				<definiendum id="0">NARA</definiendum>
				<definiens id="0">A Two-way Simultaneous Interpretation System between Korean and JapanseA Methodological Study- ''</definiens>
			</definition>
</paper>

		<paper id="2022">
			<definition id="0">
				<sentence>The alternative representation we adopted is closely related to D-structure in cn theory where D-structure is a level of syntactic structure which mirrors semantic functor-argument dependencies .</sentence>
				<definiendum id="0">D-structure</definiendum>
				<definiens id="0">a level of syntactic structure which mirrors semantic functor-argument dependencies</definiens>
			</definition>
			<definition id="1">
				<sentence>The syntactic field is categorial i.e. it can be either basic ( e.g s , np , n etc ) or complex in which case , it will be of the form C/Sign where C is a syntactic field and Sign is a sign .</sentence>
				<definiendum id="0">C</definiendum>
				<definiendum id="1">Sign</definiendum>
				<definiens id="0">a syntactic field and</definiens>
				<definiens id="1">a sign</definiens>
			</definition>
			<definition id="2">
				<sentence>The semantic field contains the semantics of the expression whereby the semantic representation language is a linear version of Discourse Representation Theory in which each condition is preceded by a sorted variable called the index .</sentence>
				<definiendum id="0">semantic field</definiendum>
				<definiens id="0">contains the semantics of the expression whereby the semantic representation language is a linear version of Discourse Representation Theory in which each condition is preceded by a sorted variable called the index</definiens>
			</definition>
			<definition id="3">
				<sentence>Finally , the Order field is a binary feature with value either pre or post which constrains the applicability of grammar rules .</sentence>
				<definiendum id="0">Order field</definiendum>
				<definiens id="0">a binary feature with value either pre or post which constrains the applicability of grammar rules</definiens>
			</definition>
			<definition id="4">
				<sentence>The recursive case occurs when Sign0 is a syntactic functor .</sentence>
				<definiendum id="0">Sign0</definiendum>
				<definiens id="0">a syntactic functor</definiens>
			</definition>
			<definition id="5">
				<sentence>Generating ( lb ) is a little more intricate but results naturally from the interaction of the generator with the grammar = .</sentence>
				<definiendum id="0">Generating ( lb )</definiendum>
				<definiens id="0">a little more intricate but results naturally from the interaction of the generator with the grammar =</definiens>
			</definition>
			<definition id="6">
				<sentence>\ [ 3\ ] van Noord , G. \ [ 1989\ ] BUG : A Directed Bottom Up Generator for Unification Based Formalisms .</sentence>
				<definiendum id="0">BUG</definiendum>
				<definiens id="0">A Directed Bottom Up Generator for Unification Based Formalisms</definiens>
			</definition>
</paper>

		<paper id="2068">
			<definition id="0">
				<sentence>With respect to such beliefs , Pollack draws a threeway distinction between act-types , actions ( or acts ) and occurrences .</sentence>
				<definiendum id="0">Pollack</definiendum>
			</definition>
			<definition id="1">
				<sentence>( Pollack represents an occurrence as OCCUR ( / ? )</sentence>
				<definiendum id="0">Pollack</definiendum>
				<definiens id="0">an occurrence</definiens>
			</definition>
			<definition id="2">
				<sentence>While the action description `` \ [ go\ ] left up the hill '' has an intrinsic culmination ( i.e. , when the agent gets to the top of the hill ) , it is not the intended termination of the action in the context of these instructions .</sentence>
				<definiendum id="0">intrinsic culmination</definiendum>
				<definiens id="0">the intended termination of the action in the context of these instructions</definiens>
			</definition>
			<definition id="3">
				<sentence>A free adjunct is defined as a nonfinile predicative phrase with the function of an adverbial subordinate clause \ [ 10\ ] .</sentence>
				<definiendum id="0">free adjunct</definiendum>
			</definition>
			<definition id="4">
				<sentence>Thus , Cemain is a less specific version ( i.e. , an abstraction ) of the intended action c~ that results from combining C~main and # and .</sentence>
				<definiendum id="0">Cemain</definiendum>
				<definiens id="0">a less specific version ( i.e. , an abstraction ) of the intended action c~ that results from combining C~main and # and</definiens>
			</definition>
			<definition id="5">
				<sentence>A nucleus consists of a preparatory process , a culmination and a consequent state .</sentence>
				<definiendum id="0">nucleus</definiendum>
				<definiens id="0">consists of a preparatory process , a culmination and a consequent state</definiens>
			</definition>
</paper>

		<paper id="2035">
			<definition id="0">
				<sentence>Our entire model of linguistic knowledge consists of a database of these constructions , uniforrnly representing lexieal knowledge , syntactic knowledge , and semantic knowledge .</sentence>
				<definiendum id="0">linguistic knowledge</definiendum>
				<definiens id="0">consists of a database of these constructions , uniforrnly representing lexieal knowledge , syntactic knowledge , and semantic knowledge</definiens>
			</definition>
			<definition id="1">
				<sentence>1 use a somewhat extended notion : a construction is a relation between one information structure and one or more others .</sentence>
				<definiendum id="0">construction</definiendum>
				<definiens id="0">a relation between one information structure</definiens>
			</definition>
			<definition id="2">
				<sentence>/'he construction is represented in figure 1 below : ( constr WhNonSubjectQuestion ( Freq p ) \ [ ( a Sq ( $ q AIO Question ) ( $ var -Queried $ q ) ( \ [ $ \v $ \presup\ ] -Presupposed Sq ) ) \ ] - &gt; \ [ ( a St ( St AIO Identify ) ( Svar -Specified St ) ( $ presup -Presupposed St ) ) \ ] \ [ ( a $ v ( $ V AIO SubjectSecondClause ) ) \ ] ) Figure 1 Figure 1 can be summarized as follows : There is a construction in English called WhNonSubjectQuestion .</sentence>
				<definiendum id="0">constr WhNonSubjectQuestion</definiendum>
			</definition>
			<definition id="3">
				<sentence>First is the ability to define constituents of constructions semantically as well as syntactically .</sentence>
				<definiendum id="0">First</definiendum>
				<definiens id="0">the ability to define constituents of constructions semantically as well as syntactically</definiens>
			</definition>
</paper>

		<paper id="3026">
			<definition id="0">
				<sentence>other aspects of the role of SSK in discourse analysis , we can base the algorithm of reference assignment on the following strategies : ( I ) if the subject of the sentence has a null form , the subject of the preceding clause is referred to , as long as the grammatical agreement is preserved ; ( 2 ) in case of a relative pronoun we try to find the head of the closest preceding noun phrase as the antecedent ; ( 3 ) if the referring expression is a ' weak pronoun , we look for the antecedent in the topic of the preceding clause , in case of a strong pronoun ( or `` adjective pronoun '' in the noun phrase as `` this man '' ) we investigate the focus ; ( 4 ) if there are more competitors after step ( 3 ) or if none of the steps ( I ) through ( 3 ) can be used , we apply SSK in the form of a list of NP 's from the preceding text ( from the beginning of the actual paragraph ) with their respective degrees of activity and choose the most activated item with the congruent morphological categories .</sentence>
				<definiendum id="0">strong pronoun</definiendum>
				<definiens id="0">the form of a list of NP 's from the preceding text ( from the beginning of the actual paragraph ) with their respective degrees of activity and choose the most activated item with the congruent morphological categories</definiens>
			</definition>
			<definition id="1">
				<sentence>We can divide it into three subcases by the way referring expressions are used in the following clause ( sentence ) C : study is the first step on the way to a complex account of the impact of SSK in discourse production .</sentence>
				<definiendum id="0">study</definiendum>
			</definition>
</paper>

		<paper id="3051">
			<definition id="0">
				<sentence>2 3TMS-Style Approaches An early example of adopting reason maintenance in parsing ( more precisely , story analysis ) is the system RESUND ( O'Rorke 1983 ) .</sentence>
				<definiendum id="0">RESUND</definiendum>
				<definiens id="0">Approaches An early example of adopting reason maintenance in parsing ( more precisely , story analysis</definiens>
			</definition>
			<definition id="1">
				<sentence>The purpose of the JTMS is to obtain a system which avoids ( chronological ) backtracking and instead handles inconsistent information ( ambiguities ) by choosing to believe a different portion of the previous inferences .</sentence>
				<definiendum id="0">JTMS</definiendum>
			</definition>
			<definition id="2">
				<sentence>of a phrase is a flmction of the interpretations of its syntactic constituents and their associated contexts .</sentence>
				<definiendum id="0">a phrase</definiendum>
				<definiens id="0">a flmction of the interpretations of its syntactic constituents and their associated contexts</definiens>
			</definition>
			<definition id="3">
				<sentence>In contr~st to this , an ATMS solves or circumvents the major problems posed by a J'I'MS. The ATMS supports eificient development and coml ) arison of all legal ( possibly contradictory ) analyses ; an ATMS-style parser thus seems more in accordance with computational practice as well as with psycholinguistic evidence .</sentence>
				<definiendum id="0">ATMS</definiendum>
				<definiens id="0">solves or circumvents the major problems posed by a J'I'MS. The ATMS supports eificient development and coml ) arison of all legal ( possibly contradictory ) analyses ; an ATMS-style parser thus seems more in accordance with computational practice as well as with psycholinguistic evidence</definiens>
			</definition>
</paper>

		<paper id="3017">
			<definition id="0">
				<sentence>... . combine seres : B is the semantic-head and C the semantic-dependent , or conversely ( sere_order feature ) .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">the semantic-head and C the semantic-dependent , or conversely ( sere_order feature )</definiens>
			</definition>
			<definition id="1">
				<sentence>A B ma &lt; v /'~ '' ~h D E of Te ; ; F G visited nolre dane A.subcat = \ [ \ ] A.sem : C.sem B.subcat = \ [ J = D.scm C.subcat = \ [ BJ = often ( , visit ( marv , nd ) ) D.subcat = left E.sem = F.sem E.subcat = \ [ B\ ] = visit ( mary , nd ) F.subcat = IG , B\ ] B.sem = mary G.subcat = \ [ \ ] G.sem = nd Fig. 2. A derivation in LG ( heavy lines correspond to semantic-heads ) Our notion of semantic-head is a variant of that given in \ [ SNMP89\ ] , where a daughter is said to be a semantic-head if it shares the semantics of its mother. The combine seres predicate is responsible for assigning sere -I-wad status ( versus sem dep status ) to a phrase , and for-imposing the following constraints : i. the semantic-head shares its semantics with its mother , it. the semantic-head always subeategorizes its sister ( ( b ) in Fig. 3 ) , iii. the mother 's subeategorization list is the concatenation of the semantic-dependent list and of the semantic-head list minas the element just incorporated ( ( c ) in Fig. 3 ) . 8 The subcategorization list attached to a constituent X corresponds to constituents higher in the derivation tree which are expected to fill semamic roles inside X. Subcalegorization lists are percolated flom the lexical entries up the deriw~tion tree according to iii. strUCtLIFeS Wollld result , but some strLlcltlres ' , A'otl\ [ d be described twice , Line ( a ) is simply onc means of clinlinating these spurious ambiguities. '\ [ he S~llllC el'lEcl would be produced by rephlcing ( a ) by fi.sem enter = sere \ ] wad or by B , spl ordcr = s &gt; w head .</sentence>
				<definiendum id="0">; F G</definiendum>
				<definiendum id="1">semantic-head</definiendum>
				<definiens id="0">B ma &lt; v /'~ '' ~h D E of Te ;</definiens>
				<definiens id="1">a variant of that given in \ [ SNMP89\ ] , where a daughter is said to be a semantic-head if it shares the semantics of its mother. The combine seres predicate is responsible for assigning sere -I-wad status ( versus sem dep status ) to a phrase</definiens>
				<definiens id="2">a ) by fi.sem enter = sere \ ] wad or by B , spl ordcr = s &gt; w head</definiens>
			</definition>
			<definition id="2">
				<sentence>term ( T ) : T.sem = die ( S.sem ) , T.string =\ [ died\ ] , T.cat = v , T.subcat = IS\ ] , S.string order = left , S.cat = n , S.syn_order = syn_dep .</sentence>
				<definiendum id="0">term ( T )</definiendum>
				<definiens id="0">T.sem = die ( S.sem ) , T.string =\ [ died\ ] , T.cat = v , T.subcat = IS\ ] , S.string order = left , S.cat = n , S.syn_order = syn_dep</definiens>
			</definition>
			<definition id="3">
				<sentence>term ( T ) : T.sem = often ( S.sem ) , T.string = \ [ often\ ] , T.cat = adv , T.subcat = IS\ ] , S.string_order = _ , % may be left or right S.cat = v , ,S.syn order= syn head .</sentence>
				<definiendum id="0">term ( T )</definiendum>
				<definiens id="0">T.string = \ [ often\ ] , T.cat = adv , T.subcat = IS\ ] , S.string_order = _ , % may be left or right S.cat = v , ,S.syn order= syn head</definiens>
			</definition>
			<definition id="4">
				<sentence>T has the following properties : i. T has string \ [ in\ ] , and is of category p ( preposition ) .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">has the following properties : i. T has string \ [ in\ ]</definiens>
			</definition>
			<definition id="5">
				<sentence>A guide-structure is a partially ordered set G which respects the descending chain condition , i.e the condition that in G all strictly decreasing ordered chains 11 &gt; 12 &gt; ... &gt; l i &gt; ... are , finite .</sentence>
				<definiendum id="0">guide-structure</definiendum>
				<definiens id="0">a partially ordered set G</definiens>
			</definition>
			<definition id="6">
				<sentence>( Pl ) is obtained from ( P0 ) in the following way : ( i ) guide variables ( Lin , Linte r , Lout ) have been threaded throughout ( P0 ) , and ( it ) the l-predicate t has been rcphtced by a 3-predicate t'which is assumed to be a r &lt; finement of t , ie , Jbr all A , Li , , Lot . , t ' ( A , Lip~ , Lour ) imp.lies t ( A ) . Program ( Pl ) is a more constrained version of program ( P0 ) : t ' can be seen as a version of t which is able to `` consult '' Liv ~ , thus coostraining lexical access at each step. We will be interested in programs ( Pl ) which respect two conditions : ( i ) the guide-consumption I ! Only programs of the ( P0 ) form are discussed here , but the subsequent discussion of guides generalizes easily to arbitrary definite clause programs. condition , and ( it ) the conservative extension condition. I ) ~iFlNrrlOY 3.2. Program ( PI ) is said to satisfy the guide-consumption condition if/ '' ( i ) the guide variables take their values in some guide-structure G , and ( it ) any call to t ' ( A , Lin , Lout ) with Lin fully instantiated returns with Lou t ./idly instantiated and strictly smaller in G. DEFINITION 3.3. Program ( P1 ) is said to be a conservative extension of ( PO ) iff : a ( A ) is provable in ( PO ) e : &gt; there exist Lin , Lou t such that a ' ( A , Lin , Lout ) is provable in ( P1 ) .</sentence>
				<definiendum id="0">Pl</definiendum>
				<definiendum id="1">, Li</definiendum>
				<definiendum id="2">Lip~</definiendum>
				<definiendum id="3">Pl )</definiendum>
				<definiendum id="4">PI</definiendum>
				<definiens id="0">the following way : ( i ) guide variables ( Lin</definiens>
				<definiens id="1">assumed to be a r &lt; finement of t , ie , Jbr all A</definiens>
				<definiens id="2">a more constrained version of program ( P0 ) : t '</definiens>
			</definition>
			<definition id="7">
				<sentence>The phrase predicate can be rewritten in either one of the two forms : phrase j ) , where emphasis is put on the relative linear order of constituents ( h , ft vs. right ) , and phrase_g , where emphasis is put on the relative semantic status ( semantic head vs. semantic dependent ) of constituents .</sentence>
				<definiendum id="0">phrase predicate</definiendum>
				<definiens id="0">semantic head vs. semantic dependent ) of constituents</definiens>
			</definition>
			<definition id="8">
				<sentence>( P0g ) phrase_g ( A ) : term ( A ) where G ( B , A ) stands for : G ( B.A ) -~ phrase_g ( C ) , B.sem order = head , combine ( B , C , A ) .</sentence>
				<definiendum id="0">)</definiendum>
				<definiens id="0">term ( A ) where G ( B , A</definiens>
			</definition>
			<definition id="9">
				<sentence>The guide-structure Gp is the set of character strings , ordered in the following way : st\ ] &lt; _ st2 iff stl is a suffix of st2 .</sentence>
				<definiendum id="0">guide-structure Gp</definiendum>
				<definiens id="0">the set of character strings , ordered in the following way : st\ ] &lt; _ st2 iff stl is a suffix of st2</definiens>
			</definition>
			<definition id="10">
				<sentence>Progranl ( Plg ) is a conservative extension of program ( POg ) .</sentence>
				<definiendum id="0">Progranl</definiendum>
				<definiens id="0">a conservative extension of program ( POg )</definiens>
			</definition>
			<definition id="11">
				<sentence>Let us then posit as a guide structure , instead of a list L of words , a couple &lt; L , B &gt; , where B is a variable restricted to taking values 0 or 1 .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">a variable restricted to taking values 0 or 1</definiens>
			</definition>
			<definition id="12">
				<sentence>CRITTER : a Translation System for Agricultural Market Reports .</sentence>
				<definiendum id="0">CRITTER</definiendum>
			</definition>
			<definition id="13">
				<sentence>BUP : a bottom-up parser embedded in Prolog .</sentence>
				<definiendum id="0">BUP</definiendum>
				<definiens id="0">a bottom-up parser embedded in Prolog</definiens>
			</definition>
			<definition id="14">
				<sentence>BUG : A Directed Bottom-up Generator for Unification Based Formalisms .</sentence>
				<definiendum id="0">BUG</definiendum>
			</definition>
</paper>

		<paper id="3095">
			<definition id="0">
				<sentence>Experiencet with thit planning cottll~Onent , e~pecirtlly concerning the integrntion of new plrmt anti further exiellslonN are dilcllAle &lt; l I As PIT is wholly integrated into the IAI , OG system , first some general remarks about LII , OG. In the LILOG project ( Linguistic and logic methods for the automatic understanding of German ) we aim primarily at constructing a text understanding system. F'or the analysis part we use an llPSG-lmsed ( Pollard and Sag 5'7 ) syntax and semantics that is further developed for German. For the representation of world knowledge and the knowledge extracted from the texts we have devised the representa.tiou language Lt , rt , oa ( Plelat and yon. Luck 8g ) . Lr , tt , oa is an order sorted first-order predicate logic t.hat allows to define and farther describe sorts by using a KI , -ONE like sort descript.ion langnage , i.e. sorts can be described hy supersort and subsort relation~ as well as by roles ( relations ) and featnres ( f , , , ,ctions ) . The sorts themselves can be either primitive ( at , m~s ) or complex e.g. defined as miles , intersection , c , r complement of other sorts with constraints on role : i and features. The sorts form the conceptual entities of the system ( they build au ontology ) mid they are organized as a lattice. The sentantics of a word in the lexicon is given by a pointer into th in sort ln~.tice. In order to find out what the text-mlderstaudi , g system has really understood we can ask questions about the texts. In the first prot &lt; ) type ( llollinger el al. 89 ) we could only ask. yes/no and constituent questions. In the present scenario the system in to understand and combine the information of several lyIoth author~ are indebted to F ; , duard tlovy who devlted the planning ten , portent while ~taying a. a guest ~ &lt; : ienti~t in the I , ILOG project. The refinements , th~ i , .plementrttiotl , nn &lt; l the experlencet reported here have been mri &lt; le I+y the txtithorL The vlewI expret~e¢l in thi~ paper are our ~ole re.q.mtihillt.v. paragraph length texts about places of interest ill the city of l ) iisseldorf and we also want to be able to ask questio , s of the kind What do ~ou know about the L , tmbrrtus cathedral ? Questions of this type necessitate a. textplanniug component that decides first , which entities and second , in which order they should be verbalized such I.\ ] mtl a colwrellt descriptive paragraph is generated. There have heen several approaches to the generation of co\ ] ~ereut texts t.hat can be coarsely divided into two kinds : the schema based approach and the plan based approach. The first in desc , 'ibed in detail in McKron , n 85. Schemata are representational structures for stereotypical paragraphs that describe objects. A variant of this approach , somewhere between the schema and the plan based approach is described in Novak 86 and Novak 87. } fete the structure of the whole text is l &gt; a~ed on a schema whereas the sit , ale paragraphs are c , ,~strueuted using domain restrictions and a technique called anticipated visua .</sentence>
				<definiendum id="0">LILOG project</definiendum>
				<definiendum id="1">oa</definiendum>
				<definiens id="0">Linguistic and logic methods for the automatic understanding of German ) we aim primarily at constructing a text understanding system. F'or the analysis part we use an llPSG-lmsed ( Pollard and Sag 5'7 ) syntax and semantics that is further developed for German. For the representation of world knowledge and the knowledge extracted from the texts we have devised the representa.tiou language Lt , rt</definiens>
				<definiens id="1">an order sorted first-order predicate logic t.hat allows to define and farther describe sorts by using a KI , -ONE like sort descript.ion langnage , i.e. sorts can be described hy supersort and subsort relation~ as well as by roles ( relations ) and featnres ( f , , , ,ctions ) . The sorts themselves can be either primitive ( at , m~s ) or complex e.g. defined as miles , intersection , c , r complement of other sorts with constraints on role : i and features. The sorts form the conceptual entities of the system ( they build au ontology ) mid they are organized as a lattice. The sentantics of a word in the lexicon is given by a pointer into th in sort ln~.tice. In order to find out what the text-mlderstaudi</definiens>
				<definiens id="2">the present scenario the system in to understand and combine the information of several lyIoth author~ are indebted to F ; , duard tlovy who devlted the planning ten</definiens>
				<definiens id="3">texts about places of interest ill the city of l ) iisseldorf and we also want to be able to ask questio , s of the kind What do ~ou know about the L , tmbrrtus cathedral ? Questions of this type necessitate a. textplanniug component that decides first , which entities and second , in which order they should be verbalized such I.\ ] mtl a colwrellt descriptive paragraph is generated. There have heen several approaches to the generation of co\ ] ~ereut texts t.hat can be coarsely divided into two kinds : the schema based approach and the plan based approach. The first in desc , 'ibed in detail in McKron , n 85. Schemata are representational structures for stereotypical paragraphs that describe objects. A variant of this approach , somewhere between the schema and the plan based approach is described in Novak 86 and Novak 87. } fete the structure of the whole text is l &gt; a~ed on a schema whereas the sit , ale paragraphs are c , ,~strueuted using domain restrictions and a technique called anticipated visua</definiens>
			</definition>
			<definition id="1">
				<sentence>The plmming algorithm consists of three phases : first , the text structure tree is built by a top-down hierarchical plmmer ( Sacerdoti 75 ) using reeursive descent .</sentence>
				<definiendum id="0">plmming algorithm</definiendum>
				<definiens id="0">consists of three phases : first , the text structure tree is built by a top-down hierarchical plmmer ( Sacerdoti 75 ) using reeursive descent</definiens>
			</definition>
			<definition id="2">
				<sentence>Pollard and Sag 87 Pollard , C. , Sag , I.A. : An In/ormatlon-lJased Synta~ and Semantics .</sentence>
				<definiendum id="0">I.A.</definiendum>
				<definiens id="0">An In/ormatlon-lJased Synta~ and Semantics</definiens>
			</definition>
</paper>

		<paper id="3059">
			<definition id="0">
				<sentence>FoG is of interest to computational linguists for three additional reasons : , * the conceptual input to the text generation process is derived from data that also drive a graphic display on a workstation for forecasters ; this determination of text frora a selected subset of graphically displayed data represents an important paradigm for the transformation of information ; • conceptual processing results in an `` interlingual '' representation , a kind of deep syntactic structure for both English and French in this sublanguage ; • sentence generation is carried out using a `` streamlined '' version of the Meaning-Text linguistic model ; this may represent the first time that such a general model has been adapted to the descriptive problems arising in telegraphic sublanguages .</sentence>
				<definiendum id="0">FoG</definiendum>
				<definiens id="0">of interest to computational linguists for three additional reasons : , * the conceptual input to the text generation process is derived from data that also drive a graphic display on a workstation for forecasters ; this determination of text frora a selected subset of graphically displayed data represents an important</definiens>
			</definition>
			<definition id="1">
				<sentence>Component Text planning in l , bG consists of three stages : content determination , text structuring and interlingua production .</sentence>
				<definiendum id="0">bG</definiendum>
				<definiens id="0">consists of three stages : content determination , text structuring and interlingua production</definiens>
			</definition>
			<definition id="2">
				<sentence>Text structuring consists basically of finding the optimal way of cutting each text content representsThe last part of forecast generation involves the relatively well-developed technique of sentence realization .</sentence>
				<definiendum id="0">Text structuring</definiendum>
				<definiens id="0">consists basically of finding the optimal way of cutting each text content representsThe last part of forecast generation involves the relatively well-developed technique of sentence realization</definiens>
			</definition>
</paper>

		<paper id="3021">
			<definition id="0">
				<sentence>A sentence becomes unacceptable for processing reasons if the combination of these properties produces too great a load for the working memory capacity ( cf. Fr~ier ( 1985 ) , Gibson ( 1987 ) ) : ( 1 ) tl ~ Aixi &gt; K i=I where : K is the maximum allowable processing load ( in processing load units or PLUs ) , x~ is the number of PLUs associated with property i , n is the number of properties , Ai is the number of times property i appears in the structure in question .</sentence>
				<definiendum id="0">K</definiendum>
				<definiendum id="1">PLUs</definiendum>
				<definiendum id="2">n</definiendum>
				<definiendum id="3">Ai</definiendum>
				<definiens id="0">the maximum allowable processing load ( in processing load units or</definiens>
				<definiens id="1">the number of properties</definiens>
				<definiens id="2">the number of times property i appears in the structure in question</definiens>
			</definition>
			<definition id="1">
				<sentence>t el i=1 i=1 where : P is the preference factor ( in PLUs ) , xi is the number of PLUs associated with property i , n is the number of properties , Ai is the number of times property i appeaJs in the unpreferred structure , Bi is the number of times property i appears in the preferred structure .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">Ai</definiendum>
				<definiendum id="2">Bi</definiendum>
				<definiens id="0">the number of properties</definiens>
				<definiens id="1">the number of times property i appeaJs in the unpreferred structure</definiens>
				<definiens id="2">the number of times property i appears in the preferred structure</definiens>
			</definition>
			<definition id="2">
				<sentence>( 3 ) A buffer cell is a set of structures { St , $ 2 , ... , S , } , where each Si represents the same segment of the input string .</sentence>
				<definiendum id="0">buffer cell</definiendum>
				<definiens id="0">a set of structures { St , $ 2 , ... , S , } , where each Si represents the same segment of the input string</definiens>
			</definition>
			<definition id="3">
				<sentence>Thematic categories include nouns , verbs , adjectives and prepositions ; functional categories include determiners , complementizers , and inflection markers .</sentence>
				<definiendum id="0">Thematic categories</definiendum>
				<definiens id="0">include nouns , verbs , adjectives and prepositions ; functional categories include determiners , complementizers , and inflection markers</definiens>
			</definition>
			<definition id="4">
				<sentence>Japanese is a verb final language , so that subjects and objects appear before the verb .</sentence>
				<definiendum id="0">Japanese</definiendum>
				<definiens id="0">a verb final language , so that subjects and objects appear before the verb</definiens>
			</definition>
</paper>

		<paper id="3009">
			<definition id="0">
				<sentence>A stem ~ '' consists of a sequence of syllables , without any further structure between the level of syllable and stem .</sentence>
				<definiendum id="0">stem ~ ''</definiendum>
				<definiens id="0">consists of a sequence of syllables , without any further structure between the level of syllable and stem</definiens>
			</definition>
			<definition id="1">
				<sentence>MOLUSC contains one basic operation , substitution .</sentence>
				<definiendum id="0">MOLUSC</definiendum>
				<definiens id="0">contains one basic operation , substitution</definiens>
			</definition>
			<definition id="2">
				<sentence>A substitution is expressed in the following form : \ [ LtlS ~ RItS\ ] where LIIS is an expression which specifies the node of a tree which is to be replaced and PdtS is an expression which specifies the subtree which is to replace it .</sentence>
				<definiendum id="0">substitution</definiendum>
				<definiendum id="1">LIIS</definiendum>
				<definiendum id="2">PdtS</definiendum>
				<definiens id="0">an expression which specifies the node of a tree which is to be replaced</definiens>
			</definition>
			<definition id="3">
				<sentence>The above function template consists of a single rule ( LItS =~ RHS ) , each rule defining a substitution .</sentence>
				<definiendum id="0">above function template</definiendum>
				<definiens id="0">consists of a single rule ( LItS =~ RHS ) , each rule defining a substitution</definiens>
			</definition>
			<definition id="4">
				<sentence>A function which has conditions takes the form , \ [ LItS ~ I~I~S : C\ ] where the : behaves like the context slash / standardly used in phonology and C is any number of conditions .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">any number of conditions</definiens>
			</definition>
			<definition id="5">
				<sentence>The German umlaut is a classic example of a phonological process which ha~s become fossilized in the morphology .</sentence>
				<definiendum id="0">German umlaut</definiendum>
				<definiens id="0">a classic example of a phonological process which ha~s become fossilized in the morphology</definiens>
			</definition>
			<definition id="6">
				<sentence>52 5 Clements , G.N. and S.J.Ncyser CV Phonology : A Generative Theory of the Syllable , MIT l ) ress , 1983 .</sentence>
				<definiendum id="0">CV Phonology</definiendum>
				<definiens id="0">A Generative Theory of the Syllable</definiens>
			</definition>
</paper>

		<paper id="3068">
			<definition id="0">
				<sentence>Due to the fact that there is IIO single basic word order for Arabic sentences , three basic sentence types were defined : a. Nominal sentence : a sentence that does not contain a verb or contains a verb which follows the subject .</sentence>
				<definiendum id="0">Nominal sentence</definiendum>
				<definiens id="0">a sentence that does not contain a verb or contains a verb which follows the subject</definiens>
			</definition>
			<definition id="1">
				<sentence>Verbal sentences ( vs ) can be followed by either a subject phrase , sp , or a subject phrase and a complement phrase , cop : vs ( vs ( VP ) ) -~ vp ( T , V , S , I ' , VP ) .</sentence>
				<definiendum id="0">Verbal sentences ( vs</definiendum>
				<definiens id="0">a subject phrase , sp , or a subject phrase and a complement phrase , cop : vs ( vs ( VP ) ) -~ vp ( T , V , S , I '</definiens>
			</definition>
			<definition id="2">
				<sentence>Verb phrases ( vp ) are defined as follows : vp -- ~ ( cop ) , ( particle ) , verb , ( cop ) .</sentence>
				<definiendum id="0">Verb phrases</definiendum>
				<definiens id="0">vp ) are defined as follows : vp -- ~ ( cop ) , ( particle ) , verb , ( cop )</definiens>
			</definition>
			<definition id="3">
				<sentence>Arguments are introduced in the lexical entries of words and are inherited by the phrase in which the words are constituents , h't this grammar , arguments are defined as fol\ ] OWS : T transitivity , V voice , P person , S semantic feature .</sentence>
				<definiendum id="0">Arguments</definiendum>
			</definition>
</paper>

		<paper id="2033">
			<definition id="0">
				<sentence>Grammar Generation Principle-based grammars like tlead-driven Phrase Structure Grammar ( HPSG ) or Japanese Phrase Structure Grammar ( JPSG ) seem to be inadequate for top-down operations .</sentence>
				<definiendum id="0">JPSG</definiendum>
				<definiens id="0">Grammar Generation Principle-based grammars like tlead-driven Phrase Structure Grammar ( HPSG ) or Japanese Phrase Structure Grammar (</definiens>
			</definition>
			<definition id="1">
				<sentence>Tile predicate gen produces a sentence string from the term cat ( P , F , Aa , Au , Sc , Sem , ) 5 , Tile term cat represents a set of features : P is tile feature for part-ofspeech , F is form such as verb inflection , Aa is adjacent node specification , Au is adjunct node specification , Sc is subcategorization information , and Sere is semantic information .</sentence>
				<definiendum id="0">Au</definiendum>
				<definiendum id="1">Sere</definiendum>
				<definiens id="0">produces a sentence string from the term cat ( P , F , Aa , Au , Sc , Sem , ) 5</definiens>
				<definiens id="1">tile feature for part-ofspeech , F is form such as verb inflection</definiens>
				<definiens id="2">adjacent node specification</definiens>
				<definiens id="3">adjunct node specification</definiens>
				<definiens id="4">semantic information</definiens>
			</definition>
			<definition id="2">
				<sentence>BaseLez has lexical and feature information .</sentence>
				<definiendum id="0">BaseLez</definiendum>
				<definiens id="0">has lexical and feature information</definiens>
			</definition>
			<definition id="3">
				<sentence>The predicate genl gets a lexical item , and applies principles to the item and the base item until producing a sentence , getLez extracts a lexical item that is 3This is rather misleading .</sentence>
				<definiendum id="0">predicate genl</definiendum>
				<definiens id="0">gets a lexical item , and applies principles to the item and the base item until producing a sentence</definiens>
			</definition>
			<definition id="4">
				<sentence>introduceFLez extracts an identity semantic item that is constrained by principles .</sentence>
				<definiendum id="0">introduceFLez</definiendum>
			</definition>
			<definition id="5">
				<sentence>sr ( cat ( P , F , \ [ \ ] , Au , Sc , Sem ) , cat ( Pos , Form , \ [ \ ] , Adj , \ [ cat ( P , F , \ [ \ ] , /lu , So , Sem ) lKest\ ] , SEM ) , cat ( Pes , Form , \ [ \ ] , Adj , B.est , SEM ) , \ [ subcat p\ ] ) ; sc cond ( P , Pos ) .</sentence>
				<definiendum id="0">sr ( cat</definiendum>
				<definiens id="0">cat ( Pes , Form</definiens>
			</definition>
			<definition id="6">
				<sentence>diet ( tama , cat ( n , n , \ [ \ ] , \ [ \ ] , \ [ \ ] , \ [ ball , X\ ] ) ) .</sentence>
				<definiendum id="0">diet ( tama</definiendum>
				<definiens id="0">cat ( n , n , \ [ \ ]</definiens>
			</definition>
			<definition id="7">
				<sentence>Transformational Grammar ( the antecedent of Parameter and Principle theory ) indicates the interesting piece of data that have many sentences with the same meaning .</sentence>
				<definiendum id="0">Transformational Grammar</definiendum>
				<definiens id="0">the antecedent of Parameter and Principle theory ) indicates the interesting piece of data that have many sentences with the same meaning</definiens>
			</definition>
			<definition id="8">
				<sentence>`` Dependency Propagation : a Unified Theory of Sentence Comprehension and Generation '' , Proceedings of the 11th International Conference on Computational Linguistics , Milan , 23-28 August , 664-670 .</sentence>
				<definiendum id="0">Dependency Propagation</definiendum>
				<definiens id="0">a Unified Theory of Sentence Comprehension and Generation ''</definiens>
			</definition>
</paper>

		<paper id="3034">
			<definition id="0">
				<sentence>A sample output is which uses a four-part quantifier notation , and in which no complex terms are present .</sentence>
				<definiendum id="0">sample output</definiendum>
				<definiens id="0">uses a four-part quantifier notation , and in which no complex terms are present</definiens>
			</definition>
			<definition id="1">
				<sentence>Truth is defined as satisfaction by the null assignment , N. Given the following axiom g ( x -* a ) satisfies `` f ( x ) '' iff f ' ( a ) then we can produce the following proof `` ( V x f ( x ) ) '' is true iff N satisfies `` ( V x f ( x ) ) '' for every o , N ( x o ) satisfies `` fix ) '' iff \ [ or every o , f ' ( a ) Finally~ formalising our meta .</sentence>
				<definiendum id="0">Truth</definiendum>
				<definiendum id="1">V x f</definiendum>
				<definiendum id="2">N ( x o )</definiendum>
				<definiens id="0">satisfaction by the null assignment</definiens>
				<definiens id="1">true iff N satisfies `` ( V x f ( x ) ) '' for every o</definiens>
			</definition>
			<definition id="2">
				<sentence>language gives `` ( V z f ( x ) ) '' is true iff ( V c~ f ' ( a ) ) This idea can be extended to structurally ambiguous sentences of English .</sentence>
				<definiendum id="0">V z f</definiendum>
				<definiens id="0">true iff ( V c~ f ' ( a )</definiens>
			</definition>
			<definition id="3">
				<sentence>Suppose C is some environment containing a complex term such as `` &lt; a y woman ( y ) &gt; '' , then g satisties C ( &lt; all y woman ( y ) &gt; ) if ( All a g ( y -- ~ a ) satisfies `` woman ( y ) '' g ( y a ) satisfies c ( y ) ) Here , C ( y ) indicates the environment C ( &lt; a y woman ( y ) &gt; ) with y replacing the complex term .</sentence>
				<definiendum id="0">C ( y )</definiendum>
				<definiens id="0">a y woman ( y ) &gt; ) with y replacing the complex term</definiens>
			</definition>
</paper>

		<paper id="3053">
			<definition id="0">
				<sentence>Schematization is the fundamental principle underlying the linguistic expressions of spatial configurations ( cf. Talmy 1983:225 ) .</sentence>
				<definiendum id="0">Schematization</definiendum>
			</definition>
			<definition id="1">
				<sentence>The tolerance principle controls the pragmatic conditions under which expressions chosen by the speaker are adequate .</sentence>
				<definiendum id="0">tolerance principle</definiendum>
				<definiens id="0">controls the pragmatic conditions under which expressions chosen by the speaker are adequate</definiens>
			</definition>
			<definition id="2">
				<sentence>Tile typicality principle implies the designation of a spatial configuration in dependance of typical relations existing between the entities .</sentence>
				<definiendum id="0">Tile typicality principle</definiendum>
				<definiens id="0">implies the designation of a spatial configuration in dependance of typical relations existing between the entities</definiens>
			</definition>
			<definition id="3">
				<sentence>Landmark ( the bus ) As the sentence is about MOTION , with tile PLACE of the trajector within tile space _ of the landmark , tile bus has the normal , typic 'd function of a LAROI'I VEIIIf3L } ~ ; together with this the SUR ' , ~'ACE of the floor become salient .</sentence>
				<definiendum id="0">Landmark</definiendum>
				<definiens id="0">the bus</definiens>
			</definition>
			<definition id="4">
				<sentence>Trajector ( the children ) Depending on the situation of MOTION , with the trajeetor within the space of the landmark , and tile normal VEIIICLI~ function of the bus being salient , the TRANSPORTABILITY of the children is tile typical property ill this relation .</sentence>
				<definiendum id="0">Trajector</definiendum>
				<definiens id="0">the children ) Depending on the situation of MOTION , with the trajeetor within the space of the landmark , and tile normal VEIIICLI~ function of the bus being salient , the TRANSPORTABILITY of the children is tile typical property ill this relation</definiens>
			</definition>
			<definition id="5">
				<sentence>V Language specific relation ( o~t ) The salient part of the landmark being the SURFACE of its floor and tile typical property of the trajector being its TRANSPORTABII , ITY , the relation of support is conceptualized .</sentence>
				<definiendum id="0">V Language specific relation</definiendum>
				<definiendum id="1">ITY</definiendum>
				<definiens id="0">The salient part of the landmark being the SURFACE of its floor and tile typical property of the trajector being its TRANSPORTABII ,</definiens>
			</definition>
			<definition id="6">
				<sentence>Its spatial conceptualization is the condition for the relational concept designated by `` o~t '' : Depending on the action of `` riding '' and tile TYPICAL FUNCTION Of `` bus '' being that of a LAI~O~ VElllCLI~ , the T~'LANSPORTAnlL1TY becomes the typical property of the trajcctor , which is realized by the `` chiidrepff ' ( see rule 9 below ) : This view of the entities excludes all other schematizations from being possible ( e.g. , that in which the children are on top of the bus ) .</sentence>
				<definiendum id="0">spatial conceptualization</definiendum>
				<definiens id="0">the typical property of the trajcctor</definiens>
			</definition>
			<definition id="7">
				<sentence>In this paper we are only concerned with the semantic level , the Interface Structure ( IS ) , which should contain the semantic information required for transfer , analysis and synthesis .</sentence>
				<definiendum id="0">Interface Structure</definiendum>
				<definiens id="0">should contain the semantic information required for transfer , analysis and synthesis</definiens>
			</definition>
</paper>

		<paper id="2032">
			<definition id="0">
				<sentence>A DoPS is a collection of plausible combinations of phrases or words .</sentence>
				<definiendum id="0">DoPS</definiendum>
				<definiens id="0">a collection of plausible combinations of phrases or words</definiens>
			</definition>
			<definition id="1">
				<sentence>DoPS system flow diagram 184 2 } In Figure 1 , since passage 2 is disambiguous , the DoPS system extracts two entries ( Entry 3 and Entry 4 ) listed below , Passage 3 : the voltage-temperature coefficient of a battery Entry 3 : ( voltage-temperature coefficient ) \ [ of , verb ( passive ) } ( battery ) { sere : 85 NUMB ER } { sere : 160 POWER } Passage 4 : a battery being charged Entry 4 : ( battery ) \ [ nil , BE ( passive ) } ( charge ) { sem:160 POWER } { sem:54 STORAGE\ ] sere : thesaurus category number ( e.g. Roget 's thesaurus ) 0 : independent word , \ [ 1 : intermediary The DoPS entries are similar to the dependency relationships in dependency grammar , but two expansions have been made : -semantic expansion -coordination expansion Semantic expansion ensures that for efficient use of DoPS , the dependency relationships will be expanded into semantic dependency relationships .</sentence>
				<definiendum id="0">DoPS</definiendum>
				<definiens id="0">-semantic expansion -coordination expansion Semantic expansion ensures that for efficient use of</definiens>
			</definition>
			<definition id="2">
				<sentence>In Japanese , the governor is the word units , BUNSETSU , which modifies another BUNSETSU , called the dependant .</sentence>
				<definiendum id="0">BUNSETSU</definiendum>
				<definiens id="0">the word units</definiens>
			</definition>
</paper>

		<paper id="2058">
</paper>

		<paper id="2014">
			<definition id="0">
				<sentence>M-EMDIMG ) ( ICat M-SUFFIX ) ( Mum SG ) ( ICat SG+O ) ( Case MOM ) ( ICat M-SUFFIX ) ( Mum SG ) ( ICat SG+O ) ( ICat M-SUFFIX ) ( Num PL ) ( ICot PL+\ [ EIM ) Figure 1 : hfftectional rule for a German class of nouns -- =D~ german : word-formation : ( RIVFRule SUFFIII.AI-I'ACH-TO-ROOT ) ~-FIproductivity 20 ~our'ee 2 ( WFCat SUFFIX ) tergel added-features ( Gender M ) ( RIRule MO-UMLAUT .</sentence>
				<definiendum id="0">M-EMDIMG )</definiendum>
				<definiendum id="1">ICat M-SUFFIX ) ( Mum SG ) ( ICat SG+O ) ( Case MOM ) ( ICat M-SUFFIX ) ( Mum SG ) ( ICat SG+O ) ( ICat M-SUFFIX ) ( Num PL )</definiendum>
				<definiendum id="2">Gender M )</definiendum>
				<definiens id="0">hfftectional rule for a German class of nouns -- =D~ german : word-formation : ( RIVFRule SUFFIII.AI-I'ACH-TO-ROOT</definiens>
			</definition>
			<definition id="1">
				<sentence>The DBMS is a stand-alone application intended to run as a server .</sentence>
				<definiendum id="0">DBMS</definiendum>
				<definiens id="0">a stand-alone application intended to run as a server</definiens>
			</definition>
			<definition id="2">
				<sentence>Koskenniemi Kimmo ( 1983 ) : Two-Level Morphology : A General Computational Model jor Word-Form Recognition and Production .</sentence>
				<definiendum id="0">Koskenniemi Kimmo</definiendum>
				<definiendum id="1">Two-Level Morphology</definiendum>
			</definition>
</paper>

		<paper id="3025">
			<definition id="0">
				<sentence>This insures that properties can be consistently inherited in this graph structure ( since A IS-A B allows A to inherit properties of B ) .</sentence>
				<definiendum id="0">IS-A B</definiendum>
			</definition>
			<definition id="1">
				<sentence>To do this , we have constructed a program called the Genus Disambiguator , which takes as input the subject codes ( pragmatic codes ) and box codes ( semantic category codes ) of the headword , taken from the machine readable version of LDOCE , and the spelling form of the genus word which has been identified by the parser described above .</sentence>
				<definiendum id="0">Genus Disambiguator</definiendum>
				<definiens id="0">takes as input the subject codes ( pragmatic codes</definiens>
			</definition>
			<definition id="2">
				<sentence>flute ( J : movable-solid , MU : Music ) a pipelike wooden or metal musical instrument with finger holes , played by blowing across a hole in the side ... The genus of flute is the word `` instrument '' ; therefore , the input to the Genus Disambiguator is ( flute J MU instrument ) The following , are the LDOCE definitions for instrument° instrumentol ( J : movable-solid , HWZT : Hardware/Fools ) an object used to help in work : medical instruments instrument-2 0 : movable-solid , MU : Music ) ... an object which is played to give musical sounds ( such as a piano , a horn , etc. ) ... instrument-3 ( Z : unmarked , -- ) someone or something which seems to be used by an outside force to cause something to happen : an instrument of fate -4o 141 In this case both the first and second senses of instrument are marked as `` J '' , ( movable-solid ) , which matches perfectly with the selection restriction for flute .</sentence>
				<definiendum id="0">Hardware/Fools</definiendum>
				<definiendum id="1">MU</definiendum>
				<definiens id="0">unmarked , -- ) someone or something which seems to be used by an outside force to cause something</definiens>
			</definition>
</paper>

		<paper id="2043">
			<definition id="0">
				<sentence>A lexical entry consists of a lexical representation , linguistic '' information , and a so-called continuation class , which is a list of sublexicons `` the members of which may follow '' ( Koskenniemi 1983 , p. 29 ) the lexical entry .</sentence>
				<definiendum id="0">lexical entry</definiendum>
				<definiendum id="1">so-called continuation class</definiendum>
				<definiens id="0">consists of a lexical representation , linguistic '' information</definiens>
				<definiens id="1">a list of sublexicons `` the members of which may follow ''</definiens>
			</definition>
			<definition id="1">
				<sentence>Activation level of the semantic units and tile semantic network 0 `` -l/t~ Y F f / '' % `` % ~'~ 5 10 15 20 Cycles kick as action die as idiom kick ( action ) die 5 249 Appendix The connectionist model for the retrieval of idioms as presented in section 6 is based on the mechanism of interactive activation and competition ( IAC ) .</sentence>
				<definiendum id="0">Activation level</definiendum>
				<definiens id="0">the retrieval of idioms as presented in section 6 is based on the mechanism of interactive activation</definiens>
			</definition>
			<definition id="2">
				<sentence>An ideal IAC network consists of nodes that can take on continuous values between a minimum and a maximum .</sentence>
				<definiendum id="0">IAC network</definiendum>
				<definiens id="0">consists of nodes that can take on continuous values between a minimum and a maximum</definiens>
			</definition>
			<definition id="3">
				<sentence>Ne~input = ScinhSvi , ~h + Sc , zcSvezc +Sc~=t Sv , zt + Scgat , Svo , ~t~ The activation value Av for a new timestamp t can now be computed : When Netinput is larger than zero : Av t = Av t-l -4 ( maz Avt-1 ) Netinput _decay ( Av t1 _ vest ) When Netinput is less than zero : Avt = Av t1 + ( Av t1 _ min ) Netinput _decay ( A v t1 _ ves~ ) We see that the influence of Netinput on Av decreases when Av reaches its minimum or maximum value .</sentence>
				<definiendum id="0">Netinput</definiendum>
				<definiens id="0">larger than zero : Av t</definiens>
				<definiens id="1">_ vest ) When Netinput is less than zero : Avt = Av t1 + ( Av t1 _ min ) Netinput _decay ( A v t1 _ ves~ ) We see that the influence of Netinput on Av decreases when Av reaches its minimum or maximum value</definiens>
			</definition>
			<definition id="4">
				<sentence>The values of the parameters in the model are : Sci , ,h 0.6 Se , ,c 0.6 Sc , ,t 0.6 Scaat~ 0.6 treshold 0.5 decay 0.1 bottom-up weights 0.8 top-down weights 0.25 inhibitory weights -0.8 external input weights 1.0 max 1.0 min -1.0 rest 0 A simulation consists of a number of cycles in which activation spreads through the network .</sentence>
				<definiendum id="0">The values of the parameters</definiendum>
			</definition>
</paper>

		<paper id="3008">
			<definition id="0">
				<sentence>Extraction and representation of text meaning is a central concern of natural language application developers .</sentence>
				<definiendum id="0">Extraction</definiendum>
				<definiens id="0">a central concern of natural language application developers</definiens>
			</definition>
			<definition id="1">
				<sentence>The KBMT-89 system consists of multiple components which run in separate Lisp processes ( usually on separate workstations ) in a distributed tashion .</sentence>
				<definiendum id="0">KBMT-89 system</definiendum>
				<definiens id="0">consists of multiple components which run in separate Lisp processes ( usually on separate workstations ) in a distributed tashion</definiens>
			</definition>
			<definition id="2">
				<sentence>The distinct components ( Figure 2 ) are a source-language analyzer , a source-language generator for paraphrasing ( used for verification ) , • a target-language generator , the augmentor , and file ONTOS knowledge acquisition tool \ [ 18\ ] ( used for queries or updates of the ontological domain model ) .</sentence>
				<definiendum id="0">target-language generator</definiendum>
				<definiens id="0">a source-language analyzer , a source-language generator for paraphrasing ( used for verification ) , • a</definiens>
			</definition>
			<definition id="3">
				<sentence>2 43 ( ( ( SEN ( *SEN* ( ( NUMBER-BULLET ( ISIS-TOKEN-OF ANY-NUMBER ) ( $ ID ( KID* I ) ( CARDINALITY i ) ( SNAP-DATA ( *MAP* { map-str ( ANY-NUMBER-MAP ) } ) ( CLAUSAL-MARK + ) ( MOOD IMPERATIVE ) ( TENSE PRESENT ) ( SOURCE ( ( REFERENCE DEFINITE ) ( NUMBER SINGULAR ( SNAP-DATA ( *MAP* { map-str diskette drive } ) ) ( SIS-TOKEN-OF DISKETTE-DRIVE ) ( $ IO ( *IO* 27 ) ) ) ) ( THEME ... ) ( AGENT *READER ) ( $ MAP-DATA ( *MAP* { map-str remove ) ) ) ( $ 1S-TOKEN-OF REMOVE ) ( $ ID ( *ID* 5 ) ) ) ) ) ( NUMBER-BULLET ( ( ROOT I ) ( VALUE I ) ( SEM ... ) ) ) ( OBJ ( ( CASE ACC ) ( SEN ( *SEM* ( ( REFERENCE DEFINITE ) ( NUMBER SINGULAR ) ( $ MAP-DATA ( *MAP* { map-str tape } ) ) ( SIS-TOKEN-OF STICKY-TAPE ) ( $ ID ( *ID* 6 ) ) ) ) ) ( REF DEFINITE ) ( DET ( ( ROOT THE ) ( REF DEFINITE ) ) ) ( ROOT TAPE ) ( COUNT NO ) ( PERSON 3 ) ( NUMBER SINGULAR ) ( MEAS-UNIT NO ) ( PROPER NO ) ) ) ( VALENCY TRANS ) ( MOOD IMPERATIVE ) ( TENSE PRESENT ) ( FORM INF ) ( PPADJUNCT ... ) ( ROOT REMOVE ) ( COMP-TYPE NO ) ( PASSIVE - ) ) ) Figure 3 : Abbreviated parse of 1 .</sentence>
				<definiendum id="0">SEN</definiendum>
				<definiendum id="1">NUMBER-BULLET ( ISIS-TOKEN-OF ANY-NUMBER )</definiendum>
				<definiendum id="2">NUMBER-BULLET ( ( ROOT I )</definiendum>
				<definiendum id="3">REF DEFINITE ) ( DET</definiendum>
				<definiens id="0">( ( ROOT THE ) ( REF DEFINITE ) ) ) ( ROOT TAPE ) ( COUNT NO ) ( PERSON 3 ) ( NUMBER SINGULAR ) ( MEAS-UNIT NO ) ( PROPER NO )</definiens>
			</definition>
			<definition id="4">
				<sentence>( SPEECH-ACT ) \ ] ) ( PROPOSITIONID \ [ *REMOVE ( NUMBER-BULLET \ [ *ANY-NUMBER ( CARDINALITY I ) \ ] ) ( MOOD IMPERATIVE ) ( TENSE PRESENT ) ( SOURCE \ [ *DISKETTE-DRIVE ( REFERENCE DEFINITE ) ( NUMBER SINGULAR ) \ ] ) ( THEME \ [ *STICKY-TAPE ( REFERENCE DEFINITE ) ( NUMBER SINGULAR ) \ ] ) ( AGENT tREADER ) \ ] ) ) ) MARS attempts to find the referent for each pronoun and definite noun phrase in the interlingua texts , and adds a link to the referent if found .</sentence>
				<definiendum id="0">*REMOVE ( NUMBER-BULLET \</definiendum>
				<definiens id="0">MOOD IMPERATIVE ) ( TENSE PRESENT ) ( SOURCE \ [ *DISKETTE-DRIVE ( REFERENCE DEFINITE ) ( NUMBER SINGULAR ) \ ] ) ( THEME \ [ *STICKY-TAPE ( REFERENCE DEFINITE ) ( NUMBER SINGULAR ) \ ] ) ( AGENT tREADER ) \ ] ) ) ) MARS attempts to find the referent for each pronoun and definite noun phrase in the interlingua texts</definiens>
			</definition>
			<definition id="5">
				<sentence>After a decision has been made on a menu by clicking the mouse button over the desired choices and then DONE , the augmentor examines the composite , ILT and determines which of the candidate interlingua texts conlain any of the selected values .</sentence>
				<definiendum id="0">augmentor examines</definiendum>
				<definiendum id="1">interlingua texts</definiendum>
				<definiens id="0">the composite , ILT and determines which of the candidate</definiens>
			</definition>
			<definition id="6">
				<sentence>Knowledge acquisition ( KA ) is often an integral part of an application which uses natural language .</sentence>
				<definiendum id="0">KA</definiendum>
				<definiens id="0">an integral part of an application which uses natural language</definiens>
			</definition>
</paper>

		<paper id="3072">
			<definition id="0">
				<sentence>The first method fails after some investigations , even when considering some probabilistic models ( which , using the multiple bit hash tables method ( Fiala , 1986 ) with probability of false answers below stored ) .</sentence>
				<definiendum id="0">probabilistic models</definiendum>
				<definiens id="0">with probability of false answers below stored )</definiens>
			</definition>
</paper>

		<paper id="3012">
			<definition id="0">
				<sentence>A relational network consists of a collection of nodes of various types , and connections between them~ known as wires .</sentence>
				<definiendum id="0">relational network</definiendum>
				<definiens id="0">consists of a collection of nodes of various types , and connections between them~ known as wires</definiens>
			</definition>
			<definition id="1">
				<sentence>Language processing consists of activity flowing through the network or , in the case of acquisition , growing new wires and nodes in the network .</sentence>
				<definiendum id="0">Language processing</definiendum>
				<definiens id="0">consists of activity flowing through the network or</definiens>
			</definition>
			<definition id="2">
				<sentence>Bach , Emmon , Colin Brown &amp; William Marslen-Wilson ( 1986 ) Crossed and nested dependencies in German and Dutch : A psycholinguistic study .</sentence>
				<definiendum id="0">Dutch</definiendum>
				<definiens id="0">A psycholinguistic study</definiens>
			</definition>
</paper>

		<paper id="3004">
			<definition id="0">
				<sentence>A transition label in the network consists of two feature bundles , an input transition bundle and an output transition bundle each containing C-features ( Carson , 1988 ) .</sentence>
				<definiendum id="0">transition label</definiendum>
			</definition>
			<definition id="1">
				<sentence>A deletion rule deletes a whole segment and can also modify feature values in neighbouring segments .</sentence>
				<definiendum id="0">deletion rule</definiendum>
				<definiens id="0">deletes a whole segment and can also modify feature values in neighbouring segments</definiens>
			</definition>
			<definition id="2">
				<sentence>The extension procedure consists of two stages : phonotactic extension , whereby additional restrictions are added to the phonotaotic network , and specialised word model construction which results in an extended phonological lexicon .</sentence>
				<definiendum id="0">extension procedure</definiendum>
				<definiens id="0">consists of two stages : phonotactic extension , whereby additional restrictions are added to the phonotaotic network , and specialised word model construction which results in an extended phonological lexicon</definiens>
			</definition>
			<definition id="3">
				<sentence>The compiler uses an invariantvariant processing strategy , and thus transduotion takes place in reverse , that is to say , a translation between the phonemic and the allophonic domains .</sentence>
				<definiendum id="0">compiler</definiendum>
				<definiens id="0">uses an invariantvariant processing strategy , and thus transduotion takes place in reverse</definiens>
			</definition>
</paper>

		<paper id="3046">
			<definition id="0">
				<sentence>According to \ [ 8\ ] , the ARGtI is a tbrreal system , in the sense that its elements are formal objects .</sentence>
				<definiendum id="0">ARGtI</definiendum>
				<definiens id="0">a tbrreal system</definiens>
			</definition>
			<definition id="1">
				<sentence>Formally , an argulnent is a relation between a set of propositions ( the premises of the argument ) , and another set of propositions ( the conclusion of the arguments ) .</sentence>
				<definiendum id="0">argulnent</definiendum>
				<definiens id="0">a relation between a set of propositions</definiens>
			</definition>
			<definition id="2">
				<sentence>seategowj ( x , e ) means that x 's semantic category is c. Likewise , proposition partiele ( x , p ) is introduced for edge \ [ P ~ -- , i , j , p , a : \ ] corresponding to a particle .</sentence>
				<definiendum id="0">e )</definiendum>
				<definiens id="0">means that x 's semantic category is c. Likewise , proposition partiele ( x , p ) is introduced for edge \ [ P ~ -- , i , j , p , a : \ ] corresponding to a particle</definiens>
			</definition>
			<definition id="3">
				<sentence>Pp S , i , j , V d , Cache ) , subcat ( z , role , p , c ) , ppeategory ( y , p , c ) ls'n $ ubcat relation ( z , y , role ) \ [ s , ~+a where relation ( z , y , role ) means that the postpositional phrase y is the case role of phra .</sentence>
				<definiendum id="0">Cache )</definiendum>
				<definiendum id="1">relation</definiendum>
				<definiens id="0">subcat ( z , role , p , c ) , ppeategory ( y , p , c ) ls'n $ ubcat relation</definiens>
			</definition>
			<definition id="4">
				<sentence>( a ) ppeategory ( y , p , c ) , ( b ) ppcategory ( y , p , e ' ) , isa ( c ' , c ) , ( c ) ppcategory ( y , p , c ' ) , -~i , sa ( c ' , e ) , ( d ) ppcate , aory ( y , p ' , c ) , ( e ) ppca* 9orv ( y , p ' , i , a ( c ' , if ) pp ategorv ( v , p ' , c ' ) , c ) , where isa ( e ' , c ) means that c is a super semantic category of e ' , and m ( p ' ) means that / is a modal particle .</sentence>
				<definiendum id="0">isa</definiendum>
				<definiens id="0">y , p , c ) , ( b ) ppcategory ( y , p , e ' ) , isa ( c ' , c ) , ( c ) ppcategory ( y , p , c '</definiens>
				<definiens id="1">a modal particle</definiens>
			</definition>
			<definition id="5">
				<sentence>263 5 rnember ( \ [ Np + -- , Pp Np , i , k , x y z\ ] , Cache ) , lf ( z , p ( y ' , z ' ) ) , relation ( y , z , No ) , If ( y , y ' ) Is~ a_ , _~ If ( x , p ( y ' , z ' ) ) I. % +1 Finally , we get Paint ( Hanako , O ) using the following argument , relation ( y , z , No ) , lf ( z , p ( al , a2 ) ) , lf ( c , q ( al , a~ ) ) \ [ s , ~ in_~o lf ( z , q ( al , a2 ) ) ISn+l and thereby complement the meaning of Hanako no e by extrapolating the verb Paint .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">relation</definiendum>
				<definiens id="0">al , a2 ) ) ISn+l and thereby complement the meaning of Hanako no e by extrapolating the verb Paint</definiens>
			</definition>
</paper>

		<paper id="3091">
			<definition id="0">
				<sentence>Table ( l ) gives the keys in English to the columns on the Matrix Paradigms .</sentence>
				<definiendum id="0">Table ( l )</definiendum>
				<definiens id="0">gives the keys in English to the columns on the Matrix Paradigms</definiens>
			</definition>
			<definition id="1">
				<sentence>-H -H ( b ) Intransitive Verbs : It was found out that the subject number is an additional distinguishing feature for transitive verbs .</sentence>
				<definiendum id="0">-H -H</definiendum>
			</definition>
			<definition id="2">
				<sentence>+H ( s ) and +H ( dp ) denote the sets of singular and dual/plural subjects , respectively .</sentence>
				<definiendum id="0">+H</definiendum>
				<definiens id="0">( s ) and +H ( dp ) denote the sets of singular and dual/plural subjects , respectively</definiens>
			</definition>
			<definition id="3">
				<sentence>By definition +H ( s ) U ÷H ( dp ) -H , where U denotes the union of the two feature sets• The table below shows the possible combinations of these features ; only features designated by A , E and F were found for Landau 's &lt; i &gt; shortList .</sentence>
				<definiendum id="0">U ÷H ( dp</definiendum>
				<definiendum id="1">U</definiendum>
				<definiens id="0">the union of the two feature sets• The table below shows the possible combinations of these features ; only features designated by A , E and F were found for Landau 's &lt; i &gt; shortList</definiens>
			</definition>
</paper>

		<paper id="2050">
			<definition id="0">
				<sentence>Incremental generation ( i.e. immediate verbalization of the parts of a stepwise computed conceptual structure often called `` message '' ) is an important and efficient property of human language use ( \ [ DeSmedt &amp; Kempen~7\ ] , \ [ Levelt89\ ] ) .</sentence>
				<definiendum id="0">Incremental generation</definiendum>
				<definiens id="0">an important and efficient property of human language use</definiens>
			</definition>
			<definition id="1">
				<sentence>Incremental generation imposes special requirements upon syntactic description and processing ( cf. \ [ Kempen87\ ] ) .</sentence>
				<definiendum id="0">Incremental generation</definiendum>
				<definiens id="0">imposes special requirements upon syntactic description and processing ( cf. \ [ Kempen87\ ] )</definiens>
			</definition>
			<definition id="2">
				<sentence>1 The head element is the central element of the phrase and determines the characteristic properties of the whole phrase that is defined as the projection of its head 7 .</sentence>
				<definiendum id="0">head element</definiendum>
				<definiens id="0">the central element of the phrase and determines the characteristic properties of the whole phrase that is defined as the projection of its head 7</definiens>
			</definition>
			<definition id="3">
				<sentence>Basic segments can also be defined as abstract descriptions of certain classes of lexical elements which have the same category and valence ( or subcategorization ) restrictions .</sentence>
				<definiendum id="0">Basic segments</definiendum>
			</definition>
			<definition id="4">
				<sentence>In principle this local mapping is case-frame based : A semantic head ( e.g. , a predicate ) and its deep cases ( e.g. , agent or benefactive ) am related to a corresponding syntactic head ( e.g. a verb ) and its syntactic frame ( e.g. , subject or direct object constituents ) .</sentence>
				<definiendum id="0">semantic head</definiendum>
				<definiens id="0">a verb ) and its syntactic frame ( e.g. , subject or direct object constituents )</definiens>
			</definition>
			<definition id="5">
				<sentence>MORPIIIX : A Fast Realization of a Classification-Based Approach to Morphology .</sentence>
				<definiendum id="0">MORPIIIX</definiendum>
				<definiens id="0">A Fast Realization of a Classification-Based Approach to Morphology</definiens>
			</definition>
			<definition id="6">
				<sentence>POPEL-HOW : A Distributed Parallel Model for Incremental Natural Language Production with Feedback .</sentence>
				<definiendum id="0">POPEL-HOW</definiendum>
				<definiens id="0">A Distributed Parallel Model for Incremental Natural Language Production with Feedback</definiens>
			</definition>
			<definition id="7">
				<sentence>POPEL : A Parallel and Incremental Natural Language Generation System .</sentence>
				<definiendum id="0">POPEL</definiendum>
				<definiens id="0">A Parallel and Incremental Natural Language Generation System</definiens>
			</definition>
</paper>

		<paper id="2059">
			<definition id="0">
				<sentence>Gapping : A Functional Analysis .</sentence>
				<definiendum id="0">Gapping</definiendum>
				<definiens id="0">A Functional Analysis</definiens>
			</definition>
</paper>

		<paper id="3013">
			<definition id="0">
				<sentence>Nadine ( Kogure , 1989 ; Kogure and Nagata , 1990 ) , which is inlplemented in Lisp , is the analysis and translation component of SLoTRANS , the spoken language translation system under development at ATIt Interpreting Telephony Research Laboratories , and its large ( 12,000 line ) grammar and lexicon make extensive use of disjunction .</sentence>
				<definiendum id="0">Nadine</definiendum>
				<definiens id="0">the analysis and translation component of SLoTRANS , the spoken language translation system under development at ATIt Interpreting Telephony Research Laboratories</definiens>
			</definition>
</paper>

		<paper id="2025">
			<definition id="0">
				<sentence>Categorial-Unification Grammars In this section we describe our functor-driven approach to natural language generation which pairs logical forms ( represented in first-order predicate logic ) with syntactically well-formed expressions of English .</sentence>
				<definiendum id="0">Categorial-Unification Grammars</definiendum>
				<definiens id="0">functor-driven approach to natural language generation which pairs logical forms ( represented in first-order predicate logic ) with syntactically well-formed expressions of English</definiens>
			</definition>
</paper>

		<paper id="2018">
			<definition id="0">
				<sentence>z ) of a universe of the interpretation and an interpretation function snch that : • T z : =// and -k z= \ [ ~ • for all sorts A , 11 : GLB ( A , B ) `` z = A ~ rl B z • singleton sorts are mapped onto singleton sets • for ( ; very feature f : fz is a function b/ -+ lt. • if a is a singleton sort and f is a featnre symbol , then fz maps a z into NONE `` / When interpreting a feature term with variables and named disjunctions , we have to make sure that the same value is assigned to each occurrence of a variable and that the same branch is chosen for each occurrence of a named disjunction .</sentence>
				<definiendum id="0">GLB</definiendum>
				<definiendum id="1">fz</definiendum>
				<definiendum id="2">f</definiendum>
				<definiens id="0">a featnre symbol</definiens>
			</definition>
			<definition id="1">
				<sentence>Since we limit ourselves to binary disjunctions , a branch of a disjunction can be specified by one of the symbols l or r. Definition 2 ( //-Assignment ) A lt-assignme~t a. is an element of l.t V , i.e. a Junction from V to li .</sentence>
				<definiendum id="0">//-Assignment</definiendum>
				<definiendum id="1">lt-assignme~t a.</definiendum>
				<definiens id="0">an element of l.t V</definiens>
			</definition>
			<definition id="2">
				<sentence>Two context deseripti &lt; ms k , k ' which are satisfied b ! \ ] e : ractlg the same contexts are called equivalent ( written \ ] , '=k ' ) . 2 i01 An important form of constraints for our approach are constraints like x I zl kin , x2 which expresses that x and xl have to be equal in contexts where ~ ( dl ) = 1 and so do x and x2 in contexts where ~ ( dl ) = r. Such constraints are called bifurcations and Xl , x2 are called ( the dl : land dl : r- ) variants ' of x. Assume an additional constraint xl \ [ xa kid2 x4 , then x3 will be called the dl : l A d2 : l-variant of x and so on. Now , instead of accumulating constraints on the variable x which might be effective in different contexts and could interact in complicated ways , we ( : an introduce new variables as variants of ~ and attach the information to them. We will sometimes reffer to a. variant of a variable x without having a variable name for this variant. To this end , we will use a special notation x/k to denote the k-variant of x. Such expressions will be called contextcd variables. Definition 5 ( Contexted Variables ) A contexted variable is a pair x/k where x G V and k ~ CI ) ~. V , : wilt denote the union of V with the set of eontexted vario ables. Elernents of V~ will be written with capital letters X , ? ' , Z , X1 , Y'I ... lib mark the distinction , we will some.. time.~ call the members of V pure variables. During t.he normalization of feature descriptions we will sometimes need variable substitution. If a description eontaiz ) s e.g. x \ [ y , where other constraints might express conflicting information about x and y , we want to concentrate this information on one variable ( say a : ) by m~bstituting all oceurences of y in other constraints by x. This could lea ( \ ] to problems when constraints attached to x and y are relevant in different contexts. One way to treat this situation correc'.ly would be the introduction of conditional substitution ( see iEisele/DSrre 90\ ] for details ) . The way we choose here is to rest.ric~ the use of variables in such a way that it is always safe to use conventional substitution. Our trick **'ill be to require thud. essentially all occurences of a variable x are relevant to the same set of contexts. We call this condition ( defined more precisely below ) the context° uniqaer~ess of variables. ~Ve. will set up the normal fornl and the rewrite system in such a way , that context-nniquel , ess of a description is maintained during the simplilication protess. ( See \ [ Eisele/i ) Srre 90\ ] for a more detailed motivation of context-uniqueness ) . 'The set of relevant contexts will be regarded as an inherent and invariant property of varial ) les , and we will introduce a context assignment , i.e. , ~ partial function Con : V ~ -- ~ CD~ : that maps each variable in use to a purely conjunctive description of the contexts it is relevant to. \\reextend ( 'on to context.ed varial ) lesby defining C'ou ( : ,./~ : ) : : : : co , , ( : ~ ) A ~. in order Io obtain context-unique descriptions , we generalize our feature terms so that they may also contain contexted variables. Definition 6 ( Gontexted Feature Terms ) A contexted feature term is buih ' according go definition l , but where both p~tre and contexted variables may occur. The set of contextcd feature terms will be denoted by FT~. The symbols s , t , t~ . .. may henc @ ~rth also denote contexted feature germs. The dc**otation of a contcxted feature term in a context n { I , r } D under an assignment ee ~ l/\ [ V is defined as for usual feature terms by adding : \ [ x/k\ ] ~ , ,~ : = ~ otherwise We can now define the context compatibility of a feature term. This definition is somewhat technical and the reader can skip it , since our algorithm will produce only contextunique descriptions , anyway. Definition 7 ( Context compatibility ) Given a partial assignment Con : V ~-~ CDe , a contexted feature term t is context-compatible to a context description k with respect to Con , written t , 'ocon k , according to the following conditions. A ~Con k X `` con k -~t `` -co , ; k f : t ~Co , , k s N t `` con k s Ud t `` Con k for arbitrary k E CD~ iff Con ( X ) = k lift ~Co. k lift ~Co. k iff s ~Co , k and t *~Co , * k ifJs ~Co~ kAd : l and t ~co , ~ kAd : r Definition 8 ( Context-unique feature descriptions ) A context-uniquc feature description ( m0 , CUC , Con ) is a triple such that : * xo C V , called the root variable e CUC ' is a set of context-unique constraints which either have the form &amp; \ [ k\ ] , where k E CD or Xlt , where X C V~ , t E F'£~ and t `` co , ~ Con ( X ) o Con is a context assignment which is defined for all variables in CUC The semantics of contextounique feature descriptions is given by the satisfaction relation t- : :Co~ between variable as.~ignmentfl , contexts and constraints , which is parametrized with a context assignment. ~ , '~ t : =~ : ... .. Xlt iJJ '' ~V : ~Co , . ( X ) o , '' &lt; X ) c ~t\ ] ... .. ~ , ,~ V &lt; 'o , , ±\ [ k\ ] ifl ~ &gt; ~ k 7'he denotation of a context .</sentence>
				<definiendum id="0">Contexted Variables ) A contexted variable</definiendum>
				<definiendum id="1">con k -~t</definiendum>
				<definiendum id="2">k f</definiendum>
				<definiendum id="3">Con )</definiendum>
				<definiens id="0">ractlg the same contexts are called equivalent ( written \ ] , '=k ' ) . 2 i01 An important form of constraints for our approach are constraints like x I zl kin , x2 which expresses that x and xl have to be equal in contexts where ~ ( dl ) = 1 and so do x and x2 in contexts where ~ ( dl ) = r. Such constraints are called bifurcations and Xl , x2 are called ( the dl : land dl : r- ) variants ' of x. Assume an additional constraint xl \ [ xa kid2 x4 , then x3 will be called the dl : l A d2 : l-variant of x and so on. Now , instead of accumulating constraints on the variable x which might be effective in different contexts and could interact in complicated ways</definiens>
				<definiens id="1">an introduce new variables as variants of ~ and attach the information to them. We will sometimes reffer to a. variant of a variable x without having a variable name</definiens>
				<definiens id="2">a pair x/k where x G V and k ~ CI ) ~. V , : wilt denote the union of V with the set of eontexted vario ables. Elernents of V~ will be written with capital letters X , ? ' , Z , X1 , Y'I ... lib mark the distinction , we will some.. time.~ call the members of V pure variables. During t.he normalization of feature descriptions we will sometimes need variable substitution. If a description eontaiz ) s e.g. x \ [ y , where other constraints might express conflicting information about x and y , we want to concentrate this information on one variable ( say a : ) by m~bstituting all oceurences of y in other constraints by x. This could lea ( \ ] to problems when constraints attached to x and y are relevant in different contexts. One way to treat this situation correc'.ly would be the introduction of conditional substitution ( see iEisele/DSrre 90\ ] for details ) . The way we choose here is to rest.ric~ the use of variables in such a way that it is always safe to use conventional substitution. Our trick **'ill be to require thud. essentially all occurences of a variable x are relevant to the same set of contexts. We call this condition ( defined more precisely below ) the context° uniqaer~ess of variables. ~Ve. will set up the normal fornl and the rewrite system in such a way , that context-nniquel , ess of a description is maintained during the simplilication protess. ( See \ [ Eisele/i ) Srre 90\ ] for a more detailed motivation of context-uniqueness ) . 'The set of relevant contexts will be regarded as an inherent and invariant property of varial ) les , and we will introduce a context assignment , i.e. , ~ partial function Con : V ~ -- ~ CD~ : that maps each variable in use to a purely conjunctive description of the contexts it is relevant to. \\reextend ( 'on to context.ed varial ) lesby defining C'ou</definiens>
				<definiens id="3">feature terms so that they may also contain contexted variables. Definition 6 ( Gontexted Feature Terms ) A contexted feature term is buih ' according go definition l , but where both p~tre and contexted variables may occur. The set of contextcd feature terms will be denoted by FT~. The symbols s , t , t~ . .. may henc @ ~rth also denote contexted feature germs. The dc**otation of a contcxted feature term in a context n { I , r } D under an assignment ee ~ l/\ [ V is defined as for usual feature terms by adding : \ [ x/k\ ] ~ , ,~ : = ~ otherwise We can now define the context compatibility of a feature term. This definition is somewhat technical and the reader can skip it</definiens>
				<definiens id="4">Context compatibility ) Given a partial assignment Con : V ~-~ CDe , a contexted feature term t is context-compatible to a context description k with respect to Con , written t</definiens>
				<definiens id="5">t ~Co , , k s N t `` con k s Ud t `` Con k for arbitrary k E CD~ iff Con ( X ) = k lift ~Co. k lift ~Co. k iff s ~Co , k and t *~Co , * k ifJs ~Co~ kAd : l and t ~co , ~ kAd : r Definition 8 ( Context-unique feature descriptions ) A context-uniquc feature description ( m0 , CUC ,</definiens>
				<definiens id="6">a triple such that : * xo C V , called the root variable e CUC ' is a set of context-unique constraints which either have the form &amp; \ [ k\ ] , where k E CD or Xlt , where X C V~ , t E F'£~ and t `` co , ~ Con ( X ) o Con is a context assignment which is defined for all variables in CUC The semantics of contextounique feature descriptions is given by the satisfaction relation t- : :Co~ between variable as.~ignmentfl , contexts and constraints , which is parametrized with a context assignment. ~ , '~ t : =~ : ... .. Xlt iJJ '' ~V : ~Co , . ( X ) o , '' &lt; X ) c ~t\ ] ... .. ~ , ,~ V &lt; 'o , , ±\ [ k\ ] ifl ~ &gt; ~ k 7'he denotation of a context</definiens>
			</definition>
			<definition id="3">
				<sentence>B &amp; CUC xlf : Y &amp; zlf : Z &amp; cue ±\ [ ~\ ] s~ ±\ [ ~'\ ] ~ cue x I x~ LAd x2 &amp; : x lY~ LJd y2 ~5 CUC xlx~ LAa~ x~ ~ xlyLJd~ z ~ CUC xlxxu~z~ ~= ~lt &amp; cue `` -+X 0 , Con ~-'~ x o , C o~ -~x 0 , CorL `` -~x o , Con `` -*x o , Con `` -'-~x 0 , Con -- '~Xo , COrt `` '' +x o , Co~ -- +x o , Con -- +xo , Con `` -+x o , Con _l_\ [ k\ ] &amp; CUC , if Con ( x ) A - , k is contradictory xIGLB ( A , B ) &amp; CUC x la &amp; y l-~a &amp; CUC x la g¢ Y INONE &amp; cue ±\ [ Con ( x ) \ ] ~5 CUC , if a _ &lt; B x\ [ A &amp; CUC , if GLB ( A , B ) = ± x\ [ ~B &amp; CUC , ifA &lt; B xlf : Y &amp; ZIY &amp; CUC _l_\ [ k v k'\ ] &amp; CUC xlx~ uax~ s~ ( cucw-x~ ) u : -x~_ xlxl LJdx x~ ~ xll ! /a LAd2 zl ~ xzly2 Lla~ z'2 ylylkla~y~ &amp; zlzlt2at z2 ~ CUC , where dl : / : d2 and yl , y2 , zl , z2 are new -- *~o , Co , xlxaudz2 &amp; zalt/d : l $ .~ z21t/d : r ~ CrfC where t is not a bifurcation Figure 2 : Simplification Rules is limited , the contexts associated to variables can not be arbitrarily specific and hence , this process must terminate. Due to lack of space , our example can not demonstrate all capabilities of the formalisrn , but will concentrate on the treatment of disjunction and the support of structure sharing between different disjuncts. Assume as initial feature term f : ( x N g : ta ) V3 h : ( ( x Ud y ) yl i : tI ) where ta and t1 might be themselves complex. Translation to contextunique tbrm will produce the description ( x0 , { xolf : ( x Ng : t'c ; ) Nh : ( ( x/d : lLAdy/d : r ) Ni : t 'D } , Conl ) where t'~ and t } might contain contexted variables if necessary. Partial normalization then produces x0 \ [ x01h zlx/d : lUay/d : x~It ) ' where the further decomposition of the constraints X t `` alta , x~lt~ need not interest us. Since the bifurcation for z contains eontexted variables , it is replaced by zlz , Ud z~ , zzlx/d : l , zrly/d : r , but the latter two constraints lead to the introduction of bifurcations also for x and y. Furthermore , the feature constraints on x and z are distributed over their respective variants. We eventually get : xo , ~ x\ [ .l ud z , . , 6'o~3 ~ , .1i : xz/d : r , % ~l~t Ud zr , Although the resulting description contains contexted variables which refer to variants of zc and : r~ , we do not have to introduce bifurcations for these variables. Itcnce the information contained in constraints on the variables xa and xi is not duplicated , although both variables are used within a disjunction. However , if there would be more information on the values of the gor / -- features of z~ , x~ , or z~ , for instance a constraint of the form z~lg : x ' , this would lead to the introduction of a bifurcation for xa , and some parts of the structure embedded under xa would have to be distributed over the variants of za. But the unfolding of the structure below xc. would be limited to the minimal necessary amount , since those parts of the structure that do not interact with information known about ~ ' could make use of contexted variables. Informally speaking , if we unify a structure with a disjunction , only those parts of the structure have to be copied that 104 5 interact with the information contained in the disjunction. 4,4 Algorithmic Considerations One major advant~ge of our treatment is its similarity with conventional rewrite systems for feature logic. Since we perlorm only conventionM substitution of variables ( opposed to conditional substitution as in \ [ Maxwell/Kaplan 89\ ] , see \ [ Eisele/Dgrre 90\ ] for a discussion ) , our system can be easily implemented in environments providing term unification ( PnoLoc , ) , or the ahnost linear solution of the union/find problem could be exploited ( see e.g. \ [ Aft-Kaci 84\ ] ) . The only essential extension we need concerns tire treatment of context descriptions. A context description contained in a contexted variable is always purely conjunctive , tIence the necessary operations ( comparison with TRUe ; , locating , adding or deleting a simple conjunct ) can each be implemented by one simple list operation. In the constraint expressing inconsistent contexts ( A_\ [ k\ ] ) , k is a disjunction of the inconsistencies found so far ( which themselves are purely conjunctive ) . This could be also represented in a list of ( purely conjunctive. ) contexts. However , the exclusion of irrelevant constraints ~ : It , where Con ( x ) is covered by k in ±\ [ k\ ] , and the ( final ) test if k ~ TRUe involves a bit more propositional calculation. Since these tests might occur more often than the detection of a new inconsistency , it might be worthwile to use a representation that facilitates the test for entailment. In any case , the implementation can make use of fast bit-vector operations. &lt; ! : .5 Maxwell and Kaplan 's Approach An approach which ours is especially interesting to con &gt; pare with is the disjunctive constraint satisfaction procedure given in \ [ Maxwell/Kaplan 89\ ] , because of the similar representations involved in the two approaches .</sentence>
				<definiendum id="0">Con</definiendum>
				<definiendum id="1">k</definiendum>
				<definiendum id="2">Con</definiendum>
				<definiendum id="3">TRUe</definiendum>
				<definiens id="0">A , B ) &amp; CUC x la &amp; y l-~a &amp; CUC x la g¢ Y INONE &amp; cue ±\ [ Con ( x ) \ ] ~5 CUC , if a _ &lt; B x\ [ A &amp; CUC , if GLB ( A , B ) = ± x\ [ ~B &amp; CUC</definiens>
				<definiens id="1">initial feature term f : ( x N g : ta ) V3 h : ( ( x Ud y ) yl i : tI ) where ta</definiens>
				<definiens id="2">a disjunction of the inconsistencies found so far ( which themselves are purely conjunctive )</definiens>
			</definition>
			<definition id="4">
				<sentence>But now , since context descriptions taay include conjunction and negation at any level , this test itself is an A/P-complete problem , which has to be solved before every application of a rule .</sentence>
				<definiendum id="0">A/P-complete problem</definiendum>
				<definiens id="0">has to be solved before every application of a rule</definiens>
			</definition>
</paper>

		<paper id="2030">
			<definition id="0">
				<sentence>Another example of distinct proofs assigning the same meaning is the following ( in both of which the subproof for the premise x/y , y = &gt; w/ ( w\x ) is omitted ) : z : g : ~ z. : o x/y : :g , y : i : &gt; w/ ( w\x ) : ~j. ( j ( ygi ) ) I/L ) x/y/ , :y , ~:0 , y : ~ : ~ w/ ( w\x ) : ~j. ( j ( : gi ) ) \ [ /R\ ] xly/z : f , z : g : :~ w/ ( w\x ) /y : Ai .</sentence>
				<definiendum id="0">:~ w/ ( w\x</definiendum>
				<definiens id="0">:g , y : i : &gt; w/ ( w\x ) : ~j. ( j ( ygi ) ) I/L ) x/y/</definiens>
			</definition>
			<definition id="1">
				<sentence>6Moor~gat ( 1990 ) demonstrates that cut elimination preserves the strong recogrdsing capacity of the calculus in the sense that the systems with and without cut will yield precisely the same readings for any theorem modulo logical equivalence .</sentence>
				<definiendum id="0">6Moor~gat ( 1990 )</definiendum>
				<definiens id="0">demonstrates that cut elimination preserves the strong recogrdsing capacity of the calculus in the sense that the systems with and without cut will yield precisely the same readings for any theorem modulo logical equivalence</definiens>
			</definition>
			<definition id="2">
				<sentence>In this paper I define a NF system for the sequent formulation of the ( product-free ) Lambek Calculus , which gives rise to a parsing approach that yields only normal proofs .</sentence>
				<definiendum id="0">NF system</definiendum>
				<definiens id="0">for the sequent formulation of the ( product-free ) Lambek Calculus , which gives rise to a parsing approach that yields only normal proofs</definiens>
			</definition>
			<definition id="3">
				<sentence>* ) a ( ~ = ~ ' ) ) or ( ( j &gt; m ) &amp; C n=j+k ) ) e. ~ ( ~k , :~y ) QsC¢'Lx , ¢~ ) \ [ /L\ ] ¢ , , , ~/y , ~k , ¢ =~ , where ( ( j _ &lt; m + 1 ) &amp; ( n = j ) ) or ( ( j &gt; m + 1 ) ~ ( , , = i + k ) ) The base case for the definition is where a subproof consists only of an axiom inference , in which case the head of the proof is the single antecedent member ( and hence , n = 1 ) .</sentence>
				<definiendum id="0">proof</definiendum>
				<definiens id="0">~k , :~y ) QsC¢'Lx , ¢~ ) \ [ /L\ ] ¢ , , , ~/y , ~k , ¢ =~ , where ( ( j _ &lt; m + 1 ) &amp; ( n = j ) ) or ( ( j &gt; m + 1 ) ~ ( , , = i + k ) ) The base case for the definition is where a subproof consists only of an axiom inference , in which case the head of the</definiens>
			</definition>
			<definition id="4">
				<sentence>The contraction relation generates a reduction relation ( I &gt; ) which is such that X reduces to Y ( X i &gt; Y ) if and only if Y is obtained from X by a finite series ( possibly zero ) of contractions .</sentence>
				<definiendum id="0">contraction relation</definiendum>
				<definiens id="0">generates a reduction relation</definiens>
			</definition>
			<definition id="5">
				<sentence>A term Y is a NF of X if and only if Y is ~NFandX ~Y. lus We shall next consider a set of contraction rules stated on proofs in L. 8 These together define a reductive notion of NF .</sentence>
				<definiendum id="0">term Y</definiendum>
				<definiens id="0">~NFandX ~Y. lus We shall next consider a set of contraction rules stated on proofs in L. 8 These together define a reductive notion of NF</definiens>
			</definition>
			<definition id="6">
				<sentence>( 12 ) The a. score for any proof P ( written sc ( P ) ) is as follows : if P is an axiom leaf instantiated with type x then so ( P ) = factorial ( 3 ate ( x ) ) b. ff P has a right inference at its root , wi~h premise subproof Q then sc ( P ) = sc ( Q ) + 1 c. if the root inference of P is a head left inference , with major subproof Q and minor subproof R then sc ( P ) = ( sc ( R ) + 1 ) sc ( q ) + 1 d. if the root inference of P is a non-head left inference , with major subproof Q and minor subproof R tt~ert so ( P ) = ( sc ( R ) + 1 ) so ( q ) We write CNF ( P ) and INF ( P ) to indicate that a proof is in NF under the respective systems .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">sc ( Q ) + 1 c. if the root inference of</definiens>
				<definiens id="1">a non-head left inference , with major subproof Q and minor subproof R tt~ert so ( P ) = ( sc ( R ) + 1 ) so ( q ) We write CNF ( P ) and INF ( P ) to indicate that a proof is in NF under the respective systems</definiens>
			</definition>
			<definition id="7">
				<sentence>Extensional equivalence for the Lambda Calculus is given by the /~\ ] -reduction system .</sentence>
				<definiendum id="0">Extensional equivalence for the Lambda Calculus</definiendum>
			</definition>
			<definition id="8">
				<sentence>U~ ) ( n , m &gt; 0 ) where h is a variable , and the main branch of a CNF proof is always of the following form ( starting at the root ) : zero or more right inferences , followed by zero or more left inferences , terminating with an axiom inference .</sentence>
				<definiendum id="0">U~ )</definiendum>
				<definiendum id="1">h</definiendum>
				<definiens id="0">starting at the root ) : zero or more right inferences , followed by zero or more left inferences , terminating with an axiom inference</definiens>
			</definition>
			<definition id="9">
				<sentence>Natural Deduction : A ProofTheoretical Study .</sentence>
				<definiendum id="0">Natural Deduction</definiendum>
				<definiens id="0">A ProofTheoretical Study</definiens>
			</definition>
</paper>

		<paper id="2074">
			<definition id="0">
				<sentence>XMAS ( Xpert in Morphological Analysis and Synthesis ) is a learning system \ [ 12,13,14\ ] which consists of a learning element ( Meta-XMAS ) , a knowledge base ( KB ) , and two inference engines of a morphological analyzer ( MOA ) and a morphological synthesizer ( MOS ) .</sentence>
				<definiendum id="0">XMAS</definiendum>
				<definiendum id="1">MOA</definiendum>
				<definiens id="0">a learning system \ [ 12,13,14\ ] which consists of a learning element ( Meta-XMAS ) , a knowledge base ( KB ) , and two inference engines of a morphological analyzer (</definiens>
			</definition>
			<definition id="1">
				<sentence>A training example consists of a class name , two strings of before and after , and a critique-an optional tag , indicating if the example is a specialization example .</sentence>
				<definiendum id="0">training example</definiendum>
				<definiens id="0">consists of a class name , two strings of before and after , and a critique-an optional tag , indicating if the example is a specialization example</definiens>
			</definition>
			<definition id="2">
				<sentence>A string production is a self-contained operator which consists of a left-hand side ( LHS ) and a righthand side ( RHS ) .</sentence>
				<definiendum id="0">string production</definiendum>
				<definiendum id="1">RHS</definiendum>
				<definiens id="0">a self-contained operator which consists of a left-hand side ( LHS ) and a righthand side</definiens>
			</definition>
			<definition id="3">
				<sentence>In our example , the following rule is generated ( in a simplified form ) : =y/ies % : ( PLURAL ( NOUN ) ( ( b ) ( y ) ( % ) ) -~ ( ( = ) ( i e s ) ( % ) ) ) The acquisition process of rules is the iteration of the creation of new rules and the modification of existing rules .</sentence>
				<definiendum id="0">PLURAL</definiendum>
				<definiendum id="1">rules</definiendum>
				<definiens id="0">the iteration of the creation of new rules and the modification of existing rules</definiens>
			</definition>
			<definition id="4">
				<sentence>The specialization process is a kind of rule creation procedure in which overgeneralization is checked and the overly generalized rule is recovered -- by going down the generalization tree to the leaves or eliminating a grapheme from a concept , if necessary .</sentence>
				<definiendum id="0">specialization process</definiendum>
				<definiens id="0">a kind of rule creation procedure in which overgeneralization is checked</definiens>
			</definition>
			<definition id="5">
				<sentence>But XMAS is better in effectiveness in development and maintenance .</sentence>
				<definiendum id="0">XMAS</definiendum>
				<definiens id="0">better in effectiveness in development and maintenance</definiens>
			</definition>
			<definition id="6">
				<sentence>ing Remarks In stunmary , XMAS , including Meta-XMAS as a linguistic knowledge acquisition tool , is characterized by the following capabilities and properties : tic information as prefix and suffix guistic knowledge by inductive machine learning The properties ( 2 ) , ( 3 ) , and ( 4 ) are the same as in Kaplan et al. \ [ 5\ ] , but XMAS has in addition the properties ( 5 ) - ( 9 ) .</sentence>
				<definiendum id="0">XMAS</definiendum>
				<definiens id="0">a linguistic knowledge acquisition tool , is characterized by the following capabilities and properties : tic information as prefix and suffix guistic knowledge by inductive machine learning The properties ( 2 )</definiens>
			</definition>
			<definition id="7">
				<sentence>( ( % ) ) ) - &gt; ( ( = ) ( e s ) ( % ) ) ) =f/vcs % ( PLURAL ( N ) ( ( ( a e 1 ) ) ( f ) ( ( % ) ) ) - &gt; ( ( = ) ( v e s ) ( % ) ) ) =fe/vcs % ( PLURAL ( N ) ( ( ( i ) ) ( f c ) ( ( % ) ) ) - &gt; ( ( = ) ( v e s ) ( % ) ) ) =y/its % ( PLURAL ( N ) ( ( ( p d b t ) ) ( y ) ( ( % ) ) ) - &gt; ( ( = ) ( i e s ) ( % ) ) ) =/x % ( PLURAL ( N ) ( ( ( u ) ) NIL ( ( % ) ) ) - &gt; ( ( = ) ( x ) ( % ) ) ) : =on/a % ( PLURAL ( N ) ( ( ( n ) ) ( o n ) ( ( % ) ) ) - &gt; ( ( = ) ( a ) ( % ) ) ) =i/c= ( PLURAL ( N ) ( ( ( s ) ) ( i ) ( ( s ) ) ) - &gt; ( ( = ) ( e ) ( = ) ) ) : =/e % ( PLURAL ( N ) ( ( ( a ) ) NIL ( ( % ) ) ) - &gt; ( ( = ) ( e ) ( % ) ) ) =us/i % ( PLURAL ( N ) ( ( ( g 1 c ) ) ( u s ) ( ( % ) ) ) - &gt; ( ( = ) ( i ) ( % ) ) ) =urn/a % ( PLURAL ( N ) ( ( ( d t ) ) ( u m ) ( ( % ) ) ) - &gt; ( ( = ) ( a ) ( % ) ) ) =/rcn % ( PLURAL ( N ) ( ( ( d ) ) NIL ( ( % ) ) ) - &gt; ( ( = ) ( r e n ) ( % ) ) ) =/on % ( PLURAL ( N ) ( ( ( x ) ) NIL ( ( % ) ) ) - &gt; ( ( = ) ( e n ) ( % ) ) ) : =We= ( PLURAL ( N ) ( ( ( m ) ) ( a ) ( ( n ) ) ) - &gt; ( ( = ) ( e ) ( = ) ) ) =oNcc= ( PLURAl .</sentence>
				<definiendum id="0">PLURAL</definiendum>
				<definiendum id="1">PLURAL</definiendum>
				<definiendum id="2">PLURAL</definiendum>
				<definiendum id="3">PLURAL</definiendum>
				<definiendum id="4">PLURAL</definiendum>
				<definiendum id="5">PLURAL</definiendum>
				<definiendum id="6">PLURAL</definiendum>
				<definiens id="0">% ( PLURAL ( N ) ( ( ( p d b t ) ) ( y ) ( ( % ) ) ) - &gt; ( ( = ) ( i e s ) ( % ) ) ) =/x % ( PLURAL ( N ) ( ( ( u ) ) NIL ( ( % ) ) ) - &gt; ( ( = ) ( x ) ( % ) ) ) : =on/a % ( PLURAL ( N ) ( ( ( n ) )</definiens>
			</definition>
			<definition id="8">
				<sentence>( N ) ( ( ( t g O ) ( o o ) ( ( s t ) ) ) - &gt; ( ( = ) ( c e ) ( = ) ) ) C.2 Past Tense of Korean == \ ] -'8/ ) \ ] at % ( PASTX ( ADJ ) ( ( ( \ ] .</sentence>
				<definiendum id="0">N ) ( (</definiendum>
				<definiendum id="1">PASTX</definiendum>
				<definiens id="0">( t g O ) ( o o ) ( ( s t )</definiens>
			</definition>
			<definition id="9">
				<sentence>7 ) ( '1 '' -- ) ) ( x ) ( ( % ) ) ) - &gt; ( ( = = ) ( o -\ ] at ) ( % ) ) ) ==/~ % ( PASTX ( VERB ) ( ( ( '8 ) ( } - ) ) NIL ( ( % ) ) ) - &gt; ( ( = = ) ( o : \ ] at ) ( % ) ) ) % =-i-I `` \ ] * % ( PASTX ( VERB ) ( ( ( % ) ( ~ ) ) ( T ) ( ( % ) ) ) - &gt; ( ( % = ) ( { * ) ( % ) ) ) % =__/ -\ ] at % ( PASTX ( VERB ) ( ( ( % ) Cat ) ) ( -- ) ( ( % ) ) ) - &gt; ( ( % = ) ( q ~ ) ( % ) ) ) ==1~ , % ( PASTX ( VERB ADJ ) ( ( ( .</sentence>
				<definiendum id="0">PASTX</definiendum>
				<definiendum id="1">PASTX</definiendum>
				<definiendum id="2">T )</definiendum>
				<definiendum id="3">PASTX</definiendum>
				<definiens id="0">PASTX ( VERB ) ( ( ( % ) Cat ) )</definiens>
			</definition>
			<definition id="10">
				<sentence>x. } - ) ( `` z ~ :7 i~ 7 ) ) NIL ( ( % ) ) ) - &gt; ( ( = = ) ( o I '' '~ ) ( % ) ) ) ==/~ % ( PA~FX ( VERB ) ( ( ( -\ ] \ ] -1- ) ( ~ `` ¢ ~- ) ) NIL ( ( % ) ) ) - &gt; ( ( = = ) ( o `` l `` ~ ) ( % ) ) ) 436 6 \ [ 1\ ] Bear , J. , A morphological recognizer with syntactic and phonological rules , Proc .</sentence>
				<definiendum id="0">NIL</definiendum>
				<definiens id="0">A morphological recognizer with syntactic and phonological rules , Proc</definiens>
			</definition>
</paper>

		<paper id="2010">
			<definition id="0">
				<sentence>ICG is a kind of unification-based formalism .</sentence>
				<definiendum id="0">ICG</definiendum>
				<definiens id="0">a kind of unification-based formalism</definiens>
			</definition>
			<definition id="1">
				<sentence>Chinese is a weakly marked language with no inflection .</sentence>
				<definiendum id="0">Chinese</definiendum>
			</definition>
			<definition id="2">
				<sentence>54 1 ( 2 ) C'hiuan °persuade '' : Semantic : meaning : `` persuade '' features : arguments : f AGENT : feature : +Human |GOAL : \ [ \ ] feature : + 11uman \ [ feature : +Event \ [ I'HEME : \ [ argument : AGENT : \ [ \ ] adjuncts : time , location , manner , ... . Syutactic : chtss : VNI , ,V p constraints : form : ( time \ [ { NP , DM , PP , GP , ADV } , +time\ ] \ [ location \ [ { PP , ADV } , +location\ ] l mammr \ [ ADV , + m.anner\ ] BP : AGENT \ [ NP\ ] &lt; * &lt; GOAL \ [ NP\ ] &lt; TItEME \ [ VP\ ] AP : ( 1 .</sentence>
				<definiendum id="0">f AGENT</definiendum>
				<definiendum id="1">VNI , ,V p constraints</definiendum>
				<definiendum id="2">AP</definiendum>
				<definiens id="0">] l mammr \ [ ADV , + m.anner\ ] BP : AGENT \ [ NP\ ] &lt; * &lt; GOAL \ [ NP\ ] &lt; TItEME \ [ VP\ ]</definiens>
			</definition>
			<definition id="3">
				<sentence>`` I2ms , a legal phrase can be viewed as a sequence of thematic roles arranged in a proper order defined by one of the basic patterns and satisfying all the constraints of Adjuncts Precedence when qpplicable .</sentence>
				<definiendum id="0">legal phrase</definiendum>
				<definiens id="0">a sequence of thematic roles arranged in a proper order defined by one of the basic patterns and satisfying all the constraints of Adjuncts Precedence when qpplicable</definiens>
			</definition>
			<definition id="4">
				<sentence>ICG is a kind of unification-based formalism .</sentence>
				<definiendum id="0">ICG</definiendum>
				<definiens id="0">a kind of unification-based formalism</definiens>
			</definition>
			<definition id="5">
				<sentence>Different languages differ with regard to syntactic of information .</sentence>
				<definiendum id="0">Different languages</definiendum>
				<definiens id="0">differ with regard to syntactic of information</definiens>
			</definition>
			<definition id="6">
				<sentence>58 5 ICG provides a way of generating surface seIlteilces froili thematic structures .</sentence>
				<definiendum id="0">ICG</definiendum>
				<definiens id="0">provides a way of generating surface seIlteilces froili thematic structures</definiens>
			</definition>
</paper>

		<paper id="3033">
			<definition id="0">
				<sentence>Ellipsis is a very common phenomenon and is frequently encountered in dialogues between persons .</sentence>
				<definiendum id="0">Ellipsis</definiendum>
				<definiens id="0">a very common phenomenon and is frequently encountered in dialogues between persons</definiens>
			</definition>
			<definition id="1">
				<sentence>For this reason , ellipsis has received much attention and different solutions have been proposed based on the mechanism used to analyze the sentence : semantic grammars ( the LIFER/LADDER system \ [ Hendrix , 1977\ ] ) , ATN \ [ Kwasny and Sondheimer , 1981 , Weischedel and Sondheimer , 1982\ ] , or caseframe instantiation ( the XCALIBUR system \ [ Carbonell et al. , 1983\ ] ) .</sentence>
				<definiendum id="0">caseframe instantiation</definiendum>
				<definiens id="0">much attention and different solutions have been proposed based on the mechanism used to analyze the sentence : semantic grammars ( the LIFER/LADDER system \ [</definiens>
			</definition>
			<definition id="2">
				<sentence>The algorithm tbr treatment of ellipsis consists of alternate scanning of the model chart and fragments chart .</sentence>
				<definiendum id="0">algorithm tbr treatment of ellipsis</definiendum>
			</definition>
			<definition id="3">
				<sentence>Now , we analyze the way in which tile function that selects the tasks contained in the Agenda is modified : inactive edge I is to be executed , check whether among the inactive edges leaving the vertex of Remaining-FragmentsChart there is one , I ' , of the same syntactic category as I ; if yes , go to point 3 , otherwise go to point 2 ; of edge I and exit from the function normally 2 185 executing the task selected ; made clearer at the end of the algorithm formulation ) ; those specified above ) and put a task into the Agenda that inserts the edge I ' into the model chart ; Remaining-FragmentsChart to the arrival vertex of the inactive edges selected in the model chart and fragments chart respectively ; modified ; backtrack , which means go to point 2 after having reestablished the preceding context .</sentence>
				<definiendum id="0">backtrack</definiendum>
				<definiens id="0">inserts the edge I ' into the model chart ; Remaining-FragmentsChart to the arrival vertex of the inactive edges selected in the model chart and fragments chart respectively ; modified</definiens>
			</definition>
			<definition id="4">
				<sentence>If the pm'ser uses a top-down strategy ( as is usual in ATN and logic grammars ) , it must hypothesize a structure for the second conjunct without any knowledge of its actual structure .</sentence>
				<definiendum id="0">top-down strategy</definiendum>
				<definiens id="0">usual in ATN and logic grammars</definiens>
			</definition>
</paper>

		<paper id="3088">
			<definition id="0">
				<sentence>cAT : ~ a z x I art i b J o I h i ~ I n i P r vJ u i w I wn { ~ ~HS : = ( A ... A ) A : := ( parse VAR node ) I ( parse transfer synthesis VAR ) chinese VAR : = % vi I -- \ [ % vn where `` ( `` and `` ) '' : all terminals , SEGRULE : a r~lle for long English segmentation , LHS : an augmented regular expression which is compo6ed of a regular expression and test ( s ) , RHS : parsing action ( s ) , test : : a LISP function which implements the designated test , ving : a gerund , such as going , doing , ved : a verb with endinq `` ed '' , where it indicates its past or past participle form , num : a number , english : an English word , or a symbol of punctuation , closure and plus : the functions * + corresponding to R and R where R is a regular expression , ( The function are done by matching the shortest pattern , covered by the functions , in the input sentence . )</sentence>
				<definiendum id="0">cAT</definiendum>
				<definiendum id="1">parse VAR node ) I</definiendum>
				<definiendum id="2">SEGRULE</definiendum>
				<definiendum id="3">LHS</definiendum>
				<definiens id="0">parse transfer synthesis VAR ) chinese VAR : = % vi I -- \</definiens>
				<definiens id="1">an augmented regular expression which is compo6ed of a regular expression and test ( s ) , RHS : parsing action ( s ) , test : : a LISP function which implements the designated test , ving : a gerund</definiens>
				<definiens id="2">an English word , or a symbol of punctuation , closure and plus : the functions * + corresponding to R and R where R is a regular expression</definiens>
			</definition>
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>WISBER is a fully implemented German NL system cow .</sentence>
				<definiendum id="0">WISBER</definiendum>
				<definiens id="0">a fully implemented German NL system cow</definiens>
			</definition>
			<definition id="1">
				<sentence>The dialog memory \ [ 5\ ] contains objects defined by these tbatures , thus providing a link between objects tYom the world of discourse ( the SEMS ) and the natural language expressions ( names ) used to refer to these objects ( the REFOs ) .</sentence>
				<definiendum id="0">discourse</definiendum>
			</definition>
			<definition id="2">
				<sentence>In IRS-formula ( 1 ) E-EV is a quantifier for events ( the buying event z ) and the quantifier Eencodes the cardinality ( of bond y ) and the fact that y has not been mentioned earlier in the dialog .</sentence>
				<definiendum id="0">E-EV</definiendum>
				<definiens id="0">a quantifier for events ( the buying event z ) and the quantifier Eencodes the cardinality ( of bond y</definiens>
			</definition>
			<definition id="3">
				<sentence>The term `` IBM '' constitutes a simplification ( it denotes the organization named `` IBM '' and is treated as a constant here ) and the timeintervals associated with states and events ( which are the source for the determination of tense features ) are omitted here .</sentence>
				<definiendum id="0">IBM</definiendum>
				<definiens id="0">the organization named `` IBM '' and is treated as a constant here ) and the timeintervals associated with states and events ( which are the source for the determination of tense features ) are omitted here</definiens>
			</definition>
			<definition id="4">
				<sentence>H. Horacek : Towards Principles of Ontology .</sentence>
				<definiendum id="0">H. Horacek</definiendum>
			</definition>
</paper>

		<paper id="2037">
			<definition id="0">
				<sentence>If the Engli , ; h test input is `` The chairman decides to come '' , the corresponding French sentence is equally simple , `` Le pr6sident d6cide de venir '' , but if the English is `` The chairman expects to come '' , the French equivalent of `` expect '' imposes a completely different structure ( `` Le pr6sident s'attend a ce qu'il vienne ( lui-m~me ) '' ) , and , if tile output translation is incorrect , it may be difficult to determine whether the reason is a generally wrong treatment of tile class of English verbs. , or whether it is something specific to `` expect '' .</sentence>
				<definiendum id="0">h test input</definiendum>
				<definiens id="0">incorrect , it may be difficult to determine whether the reason is a generally wrong treatment of tile class of English verbs. , or whether it is something specific to `` expect ''</definiens>
			</definition>
			<definition id="1">
				<sentence>A Practical Guide to the Evaluation o/ Machine Translation Systems .</sentence>
				<definiendum id="0">Practical Guide</definiendum>
			</definition>
</paper>

		<paper id="3030">
			<definition id="0">
				<sentence>In this general setting , CG is the formalism of the fifth stage , syntax proper .</sentence>
				<definiendum id="0">CG</definiendum>
				<definiens id="0">the formalism of the fifth stage , syntax proper</definiens>
			</definition>
			<definition id="1">
				<sentence>@ DN &gt; '' = determiner as modifier of the next noun to the right , `` @ +FMAINV '' = finite main verb , `` @ -FMAINV '' = non-finite main verb as member of a verb chain , `` @ &lt; NQM-FMAINV '' = non-finite main verb as postmodifier of a nominal : a a '' DET CENTR ART INDEF @ DN &gt; '' move move '' N NOM SG '' move `` V SUBJUNCTIVE @ +FMAINV '' move `` V IMP @ +FMAINV '' move '' V INF @ -FMAINV @ &lt; NOM-FMAINV '' described by recursive links back to the main lexicon. Consider the cohort of the Swedish word-form frukosten ( `` _ `` = compound boundary , frukost 'breakfast ' , fru 'mrs ' , kost'nutrition ' , ko 'cow ' , sten 'stone ' ) : frukosten frukost '' N UTR DEF SG NOM '' fru_kost '' N UTR DEF SG NOM `` fru ko sten '' N UTR INDEF SG NOM `` By 'local disambiguation ' we refer to constraints or strategies that make it possible to discard some readings just by local inspection of the current cohort , without invoking any contextual information. The present cohort contains three readings. An interesting local disambiguation strategy can now be stated : `` Discard all readings with more than the smallest number of compound boundaries occurring in the current cohort '' . This strategy properly discards the readings `` fru_kost '' and `` fru ko sten '' . I have found this principle to be very close to perfect. A similar principle holds for derivation : `` Discard readings with derivational elements if there is at least one non-derived reading available in the cohort '' . Other local disambiguation strategies compare multiple compound readings in terms of how probable their part of speech structure is ( NNN , ANN , NVN , AAV , etc. ) . Local disambiguation is a potent module. The Swedish morphological analyzer was applied to a text containing some 840,000 word-form tokens. The following table shows cohort size N ( r ) in the first column. The second and third columns sllow the number of cohorts with the respective number of readings before ( a ) and after ( b ) local disambiguation. E.g. , before local disambiguation there were 3830 word-forms with 6 readings but after local disambiguation only 312. Here , disambiguation refers to reduction of morphological ambiguities , optimally down to cohort size = 1. Sense disambiguation is not included ( presently our lexical items have no sense descriptions ) . The subproblems of morphosyntactic mapping , morphological disambiguation , clause boundary location , and syntactic function determination are interrelated. E.g. , for disambiguation it is useful to know the boundaries of the current clause , and to know as much as possible about its syntactic structure. An important aspect of the general problem is to work out the precise relations between these modules. Morphological ambiguities may be due to intersection of forms of the same or of different lexical entries , or to intersection of recursive compound paths. The latter phenomenon arises if productive compound formation in e.g. Finnish , German , and Swedish is N ( r ) ( a ) ( b ) 0 13957 13957 1 440035 487994 2 253779 236298 3 55857 44782 4 38062 29053 5 24135 18911 6 3830 312 7 9551 8913 8 541 23 9 232 47 10 72 2 11 46 5 12 124 13 15 14 28 15+ 33 Out of roughly 1,5 million readings assigned by morphology ( 1.8 readings/word-form ) , local disam169 biguation discards more than 100,000. Especially dramatic the drop is for highly ambiguous words. After local disambiguation , each word in the sentence undergoes morphosyntactic mapping , i.e. it is assigned at least one syntactic label , perhaps several if a unique label is not possible to assign. This mapping will be discussed in connection with the syntactic constraints in section 7. constraints The CG formalism will first be illustrated by context-dependent disambiguation constraints. Sets of grammatical features are needed in the constraints for the purpose of genei'alization. Each set declaration consists of a set name followed by the elements of that set. The elements are ( strings of ) features and/or base-forms occurring in readings : ( DET `` DET '' ) ( N `` N '' ) ( TO `` to '' ) ( PREMOD `` A '' `` DET '' ) ( NOMHEAD `` N NOM '' `` PRON NOM '' ) ( VFIN `` V PRES '' `` V PAST '' `` V IMP '' `` V SUBJUNCTIVE '' ) Each constraint is a quadruple consisting of domain , operator , target , and context condition ( s ) . An example : ( @ w =0 `` PREP '' ( -1 DET ) ) other readings. The operators are here defined in the procedural mode as performing operations. Conceptually they just express constraints. The context conditions are defined relative to the target reading in position 0. Position 1 is one word to the right of 0 , -3 three words to the left of 0 , etc. ( Such straightforward positions we call absolute. ) Each context condition is a triple consisting of polarity , position , and set. 'Polarity ' is either NOT or nothing ( i.e. positive ) , 'position ' is a legal position number , and 'set ' is a declared set name. An asterisk ... .. ( functionally and mnemotechnically reminiscent of the Kleene star ) prefixed to position number n refers to some position rightwards of n ( if n is positive ) , or some position leftwards of n ( if n is negative ) , in both cases including n , up to the next sentence boundary ( or clause boundary , if enforced in clause boundary mode , cf. below ) . The asterisk convention thus enables the description of unbounded dependencies. Examples : ( 1 N ) requires there to be a reading with the feature `` N '' for the next word-form. ( NOT *-1 VFIN ) states : nowhere leftwards in this sentence is there a reading with any of the feature combinations defining finite verbs. The condition ensemble ( 1 PREMOD ) ( 2 N ) ( 3 VFIN ) requires there to be a reading with either `` A '' or `` DET '' in position 1 , with `` N '' in position 2 , and with one of the VFIN readings in position 3. Here are two more context-dependent disambiguation constraints for English : ( @ w =0 VFIN ( -1 TO ) ) ( `` that '' = ! `` &lt; Rel &gt; '' ( -1 NOMHEAD ) ( 1 VFIN ) ) stating that if a word ( @ w ) has a reading with the feature `` PREP '' , this very reading is discarded ( =0 ) iff the preceding word ( i.e. the word in position -1 ) has a reading with the feature `` DET '' .</sentence>
				<definiendum id="0">Swedish</definiendum>
				<definiens id="0">non-finite main verb as member of a verb chain , `` @ &lt; NQM-FMAINV '' = non-finite main verb as postmodifier of a nominal : a a '' DET CENTR ART INDEF @ DN &gt; '' move move '' N NOM SG '' move `` V SUBJUNCTIVE @ +FMAINV '' move `` V IMP @ +FMAINV '' move '' V INF @ -FMAINV @ &lt; NOM-FMAINV '' described by recursive links back to the main lexicon. Consider the cohort of the Swedish word-form frukosten ( `` _ `` = compound boundary</definiens>
				<definiens id="1">frukosten frukost '' N UTR DEF SG NOM '' fru_kost '' N UTR DEF SG NOM `` fru ko sten '' N UTR INDEF SG NOM `` By 'local disambiguation ' we refer to constraints or strategies that make it possible to discard some readings just by local inspection of the current cohort , without invoking any contextual information. The present cohort contains three readings. An interesting local disambiguation strategy can now be stated : `` Discard all readings with more than the smallest number of compound boundaries occurring in the current cohort '' . This strategy properly discards the readings `` fru_kost '' and `` fru ko sten '' . I have found this principle to be very close to perfect. A similar principle holds for derivation : `` Discard readings with derivational elements if there is at least one non-derived reading available in the cohort '' . Other local disambiguation strategies compare multiple compound readings in terms of how probable their part of speech structure is ( NNN , ANN , NVN , AAV , etc. ) . Local disambiguation is a potent module. The Swedish morphological analyzer was applied to a text containing some 840,000 word-form tokens. The following table shows cohort size N ( r ) in the first column. The second and third columns sllow the number of cohorts with the respective number of readings before ( a ) and after ( b ) local disambiguation. E.g. , before local disambiguation there were 3830 word-forms with 6 readings but after local disambiguation only 312. Here , disambiguation refers to reduction of morphological ambiguities , optimally down to cohort size = 1. Sense disambiguation is not included ( presently our lexical items have no sense descriptions ) . The subproblems of morphosyntactic mapping , morphological disambiguation , clause boundary location , and syntactic function determination are interrelated. E.g. , for disambiguation it is useful to know the boundaries of the current clause , and to know as much as possible about its syntactic structure. An important aspect of the general problem is to work out the precise relations between these modules. Morphological ambiguities may be due to intersection of forms of the same or of different lexical entries , or to intersection of recursive compound paths. The latter phenomenon arises if productive compound formation in e.g. Finnish , German , and</definiens>
				<definiens id="2">33 Out of roughly 1,5 million readings assigned by morphology ( 1.8 readings/word-form ) , local disam169 biguation discards more than 100,000. Especially dramatic the drop is for highly ambiguous words. After local disambiguation , each word in the sentence undergoes morphosyntactic mapping</definiens>
				<definiens id="3">illustrated by context-dependent disambiguation constraints. Sets of grammatical features are needed in the constraints for the purpose of genei'alization. Each set declaration consists of a set name followed by the elements of that set. The elements are ( strings of ) features and/or base-forms occurring in readings : ( DET `` DET '' ) ( N `` N '' ) ( TO `` to '' ) ( PREMOD `` A '' `` DET '' ) ( NOMHEAD `` N NOM '' `` PRON NOM '' ) ( VFIN `` V PRES '' `` V PAST '' `` V IMP '' `` V SUBJUNCTIVE '' ) Each constraint is a quadruple consisting of domain , operator , target , and context condition ( s ) . An example : ( @ w =0 `` PREP '' ( -1 DET ) ) other readings. The operators are here defined in the procedural mode as performing operations. Conceptually they just express constraints. The context conditions are defined relative to the target reading in position 0. Position 1 is one word to the right of 0 , -3 three words to the left of 0 , etc. ( Such straightforward positions we call absolute. ) Each context condition is a triple consisting of polarity , position , and set. 'Polarity ' is either NOT or nothing ( i.e. positive ) , 'position ' is a legal position number , and 'set ' is a declared set name. An asterisk ... .. ( functionally and mnemotechnically reminiscent of the Kleene star ) prefixed to position number n refers to some position rightwards of n ( if n is positive ) , or some position leftwards of n ( if n is negative ) , in both cases including n , up to the next sentence boundary ( or clause boundary , if enforced in clause boundary mode , cf. below ) . The asterisk convention thus enables the description of unbounded dependencies. Examples : ( 1 N ) requires there to be a reading with the feature `` N '' for the next word-form. ( NOT *-1 VFIN ) states : nowhere leftwards in this sentence</definiens>
				<definiens id="4">a reading with either `` A '' or `` DET '' in position 1 , with `` N '' in position 2 , and with one of the VFIN readings in position 3. Here are two more context-dependent disambiguation constraints for English : ( @ w =0 VFIN ( -1 TO ) ) ( `` that '' = ! `` &lt; Rel &gt; '' ( -1 NOMHEAD ) ( 1 VFIN ) ) stating that if a word ( @ w ) has a reading with the feature `` PREP '' , this very reading is discarded ( =0 ) iff the preceding word ( i.e. the word in position -1 ) has a reading with the feature `` DET ''</definiens>
			</definition>
			<definition id="2">
				<sentence>VFIN is a declared set .</sentence>
				<definiendum id="0">VFIN</definiendum>
				<definiens id="0">a declared set</definiens>
			</definition>
			<definition id="3">
				<sentence>E.g. , given that conjunctions are lexically marked by the inherent feature `` &lt; Conj &gt; '' , the constraint : ( @ w =**CLB `` &lt; Conj &gt; '' ( 1 NOMHEAD ) ( 2 VFIN ) ) states that there is a clause boundary before conjunction instances that precede a NOMHEAD followed by a finite verb ( e.g. , before the conjunction in a sentence such as John eats and Bill drinks ) .</sentence>
				<definiendum id="0">E.g.</definiendum>
			</definition>
			<definition id="4">
				<sentence>Here are some mapping statements without context conditions , providing a maximal set of labels : ( `` PRON GEN '' NIL GN &gt; @ PCOMPL-S @ PCOM PL-O ) ( `` A '' NIL AN &gt; @ PCOMPL-S @ PCOMPL-O @ SUBJ @ OBJ @ I-OBJ ) ( `` N NOM '' NIL @ SUBJ @ OBJ @ I-OBJ @ PCOMPL-S @ PCOMPL-O @ APP @ NN &gt; @ &lt; P ) A pronoun in the genitive case is either prenominal genitival modifier , subject predicate complement , or object predicate complement. An adjective is prenominal adjectival modifier , predicate complement , subject , object , or indirect object ( the last three functions refer to occurrences of adjectives as 'nominalized heads ' ) , etc. Often morphosyntactic mappings may be considerably constrained by imposing context conditions : ( `` N NOM '' ( ( 1 N ) ) @ NN &gt; ) ( `` N NOM '' ( ( -1 PREP ) ) @ &lt; P ) ( `` INF '' ( ( -2 N ) ( -1 TO ) ) @ &lt; NOM-FMAINV ) These state that a noun in the nominative case premodifies ( @ NN &gt; ) a subsequently following noun ( in compounds , cf. computer screen ) , that a noun in the nominative case after a preposition is @ &lt; P , and that an infinitive preceded by a noun + to postmodifies that noun. In this way , the task of syntactic analysis is simplified as much as possible , as early as possible. Superfluous alternatives are not even introduced into the parsing of a particular clause if it is clear at the outset , i.e. either in the lexicon or at the stage of morphosyntactic mapping , that certain labels are incompatible with the clausal context at hand. There may be several mapping statements for the same morphological feature ( s ) , e.g. `` N NOM '' . Mapping statements with more narrowly specified contexts have precedence over more general statements. In the present implementation of CGP , the mapping statements apply in plain linear order. The last mapping statement for a particular feature 1 '' /2 provides the worst case , i.e. the maximal assortment of function labels for that feature. Every word-form will have at least one syntactic label after morphosyntactic mapping , and all possible syntactic ambiguities have also now been introduced. In step three , syntactic constraints reduce syntactic ambiguities where such exist due either to lexical information ( cf. the infinitive move above ) , or to morphosyntactic mapping. Syntactic constraints discard the remaining superfluous syntactic labels. Syntactic constraints differ from context-dependent disambiguation constraints only by having one of the syntactic operators '=s ! ' , or '=sO ' ( where s indicates that the constraint is a syntactic one ) . Their semantics is identical to that of the disambiguation constraints : ( @ w =sO `` @ +FMAINV '' ( *-1 VFIN ) ) ( @ w =sO `` @ +FMAINV '' ( '1 VFIN ) ) ( @ w =s ! `` @ SUBJ '' ( 0 NOMHEAD ) ( NOT `` 1 NOMHEAD ) ( '1 VFIN ) ( NOT *-1 NOMHEAD ) ) The first two constraints discard @ +FMAINV as a syntactic alternative if there is a unique finite main verb either to the left or to the right in the same clause. The third constraint prescribes that @ SUBJ is the correct label for a noun or pronoun ( NOMHEAD in target position , i.e. position 0 ) , with a finite verb somewhere to the right in the same clause and no similar noun or pronoun either left or right ( -woman -laughed -- ) . Maximal profit is extracted from the Uniqueness Principle. At each syntactic step ( before mapping , after mapping , and after the application of a syntactic constraint that affects the labels obeying the Uniquehess Principle ) , each clause is checked for eventual violations of this principle. In this way many ambiguous primary labels may be safely discarded. Here is an example sentence , fully analyzed and unambiguous in all respects but the one syntactic ambiguity remaining for the word in : Bill Bill `` &lt; Proper &gt; N NOM SG `` @ SUBJ saw see '' &lt; SVO &gt; V PAST '' @ +FMAINV the the '' DET '' @ DN &gt; little little '' A ABS '' @ AN &gt; dog dog '' N NOM SG '' @ OBJ in in '' PREP `` @ &lt; NOM @ ADVL the the '' DET '' @ DN &gt; park park '' N NOM SG `` @ &lt; P 5 There is no non-semantic way of resolving the attachment ambiguity of the adverbial in the park .</sentence>
				<definiendum id="0">PREP ) ) @ &lt; P )</definiendum>
				<definiendum id="1">DET</definiendum>
				<definiens id="0">either prenominal genitival modifier , subject predicate complement , or object predicate complement. An adjective is prenominal adjectival modifier , predicate complement , subject , object</definiens>
				<definiens id="1">syntactic constraints reduce syntactic ambiguities where such exist due either to lexical information ( cf. the infinitive move above ) , or to morphosyntactic mapping. Syntactic constraints discard the remaining superfluous syntactic labels. Syntactic constraints differ from context-dependent disambiguation constraints only by having one of the syntactic operators '=s ! '</definiens>
				<definiens id="2">an example sentence , fully analyzed and unambiguous in all respects but the one syntactic ambiguity remaining for the word in : Bill Bill `` &lt; Proper &gt; N NOM SG `` @ SUBJ saw see '' &lt; SVO &gt; V PAST '' @ +FMAINV the the ''</definiens>
			</definition>
			<definition id="5">
				<sentence>CGP takes two inputs , a constraint file with set declarations , mapping statements , context-dependent disambiguation constraints , syntactic constraints , etc. , and a text file with morphologically analyzed word-forms ( cf. section 2 ) .</sentence>
				<definiendum id="0">CGP</definiendum>
				<definiens id="0">takes two inputs , a constraint file with set declarations , mapping statements , context-dependent disambiguation constraints , syntactic constraints , etc. , and a text file with morphologically analyzed word-forms ( cf. section 2 )</definiens>
			</definition>
</paper>

		<paper id="3048">
			<definition id="0">
				<sentence>Tiffs lmper concerns an approach to Machine Translation whieJJ differs from the typical 'standard ' approaches crucially in .</sentence>
				<definiendum id="0">Tiffs lmper</definiendum>
				<definiens id="0">concerns an approach to Machine Translation whieJJ differs from the typical 'standard ' approaches crucially in</definiens>
			</definition>
			<definition id="1">
				<sentence>Keywords : Machine translation ; natural language interface ; dialogue Introduction Machine Translation ( M'f ) or natural lang~lge translation in general is a typical example of the 'under-constrained ' problems which we often encounter in the field of artificial intelligence 1 .</sentence>
				<definiendum id="0">Keywords</definiendum>
				<definiens id="0">Machine translation ; natural language interface ; dialogue Introduction Machine Translation ( M'f ) or natural lang~lge translation in general is a typical example of the 'under-constrained ' problems which we often encounter in the field of artificial intelligence 1</definiens>
			</definition>
			<definition id="2">
				<sentence>In S. Nirenburg ( ed ) Machine translation : theoretical and methodological issues , Cambridge : Cambridge University Press , 136-144 .</sentence>
				<definiendum id="0">Nirenburg</definiendum>
				<definiens id="0">ed ) Machine translation : theoretical and methodological issues</definiens>
			</definition>
</paper>

		<paper id="3099">
			<definition id="0">
				<sentence>To express word and sentence grammars , we have developed a grammar formalism , called Unification-based Transition Networks ( UTN ) , I~s skeleton are nondeterministic reeursive transition networks ( RTNs ) , which are equivalent to comex~free gramnmrs .</sentence>
				<definiendum id="0">Unification-based Transition Networks</definiendum>
				<definiendum id="1">RTNs )</definiendum>
			</definition>
			<definition id="1">
				<sentence>Our s~entence sets consist of 35 German sentences ( set SI , with an average sentence length of 9.8 words ) and 39 English sentences ( set SII , with an average sentence length of 15.3 words ) from Tomita \ [ 7\ ] , pp .</sentence>
				<definiendum id="0">s~entence sets</definiendum>
			</definition>
			<definition id="2">
				<sentence>Two-level Morphology : A General Computational Model for Word-Form Recognition and Production .</sentence>
				<definiendum id="0">Two-level Morphology</definiendum>
			</definition>
</paper>

		<paper id="3071">
			<definition id="0">
				<sentence>System Design The principal system components are 1 o a syntactic analyzer , using an attgmented context-free grammar , which produces a parse and a regularized syntactic structure ® a semantic analyzer , which maps clauses and nominalizations into domain-specific predicates • reference resolution , which deteirnines referents for anaphoric and omitted arguments ® data base creation , which maps predicates describing events of interest into data base entries i In addition to these principal components , there is a small semantic rcgularization component ( following semantic analysis ) , which performs some decomposition and simplification of semantic forms .</sentence>
				<definiendum id="0">attgmented context-free grammar</definiendum>
				<definiens id="0">produces a parse and a regularized syntactic structure ® a semantic analyzer , which maps clauses and nominalizations into domain-specific predicates</definiens>
				<definiens id="1">deteirnines referents for anaphoric and omitted arguments ® data base creation</definiens>
				<definiens id="2">following semantic analysis ) , which performs some decomposition and simplification of semantic forms</definiens>
			</definition>
</paper>

		<paper id="2053">
			<definition id="0">
				<sentence>\ [ Aho '72\ ] Aho , A.V.and Ulman , J.D. : The Theory o\ ] Parsing , Translation , and Compiling , Prentice-Hall , Englewood Cliffs , New Jersey ( 1972 ) \ [ Knuth 65\ ] Knuth , D.E. : On the translation o\ ] languages \ ] rom left to right , Information and Control 8:6 , pp.607-639 \ [ Johnson 89\ ] Mark Johnson : The Computational Complexity of Tomita 's Algorithm International Workshop on Parsing Technologies , pp.203-208 ( 1989 ) \ [ Matsumoto 87\ ] Matsumoto , Y. : A Parallel Parsing System for Natural Language Analysis , New Generation Computing , 1/ol.5 , No. 1 , pp.63-78 ( 1987 ) \ [ Matsumoto 89\ ] Matsumoto , Y. : Natural Language Parsing Systems based on Logic Programming , Ph.D thesis of Kyoto University , ( June 1989 ) \ [ Mellish 85\ ] Mellish , C.S. : Computer Interpretation of Natural Language Descriptions , Ellis Horwood Limited ( 1985 ) \ [ Nilsson 86\ ] Nilsson , U. : AID : An Alternative Implementation of DCGs , New Generation Computing , 4 , pp.383-399 ( 1986 ) \ [ Tanaka 89\ ] Tanaka , H. and Numazaki , H. : Parallel Generalized LR Parsing based on Logic Programming International Workshop on Parsing Technologies , pp .</sentence>
				<definiendum id="0">Knuth , D.E.</definiendum>
				<definiendum id="1">Information</definiendum>
				<definiendum id="2">Matsumoto , Y.</definiendum>
				<definiens id="0">Natural Language Parsing Systems based on Logic Programming</definiens>
				<definiens id="1">Parallel Generalized LR Parsing based on Logic Programming International Workshop on Parsing Technologies , pp</definiens>
			</definition>
</paper>

		<paper id="3081">
			<definition id="0">
				<sentence>As its name indicates , ELU is one of the growing number of systems designed to employ unification as the principal computational mechanism ; we shall mention below some respects in which it difl~rs from other such systems .</sentence>
				<definiendum id="0">ELU</definiendum>
				<definiens id="0">one of the growing number of systems designed to employ unification as the principal computational mechanism ; we shall mention below some respects in which it difl~rs from other such systems</definiens>
			</definition>
			<definition id="1">
				<sentence>ELU exploits the parser and unifier of UD ( Johnson and Rosner , 1989 ) , but differs from that earlier system in the addition of a generator ( Russell et al. , 1990 ) and a transfer component ( Estival et al. , 1990 ; Russell et al. , 1989 ) .</sentence>
				<definiendum id="0">ELU</definiendum>
				<definiens id="0">that earlier system in the addition of a generator ( Russell et al. , 1990 ) and a transfer component ( Estival et al. , 1990</definiens>
			</definition>
			<definition id="2">
				<sentence>These two properties immediately give it certain advantages : Declarativeness means that a description is a set of independent statements about the well-formed expressions of the language .</sentence>
				<definiendum id="0">Declarativeness</definiendum>
				<definiens id="0">a set of independent statements about the well-formed expressions of the language</definiens>
			</definition>
			<definition id="3">
				<sentence>This allows the system to be : ® flexible , permitting changes during development ; • incrementable , as the linguist need not be concemed with the order in which information is added or new phenomena accounted for ; • reversible : Grammar reversibility ( or bidirectional grammars ) is a highly desirable goal in the context of machine translation , i.e , using the same grammar as either source or target language description , a goal attested to in other centres working on MT ( cf. Dymetman &amp; Isabelle , 1988 ; Van Noord , to appear ; Russell et al. , 1990 ) .</sentence>
				<definiendum id="0">Van Noord</definiendum>
				<definiens id="0">the linguist need not be concemed with the order in which information is added or new phenomena accounted for ; • reversible : Grammar reversibility ( or bidirectional grammars</definiens>
			</definition>
			<definition id="4">
				<sentence>® ELU accepts terms ( trees ) and lists as attribute values in addition to feature structures .</sentence>
				<definiendum id="0">ELU</definiendum>
				<definiens id="0">accepts terms ( trees ) and lists as attribute values in addition to feature structures</definiens>
			</definition>
</paper>

		<paper id="3006">
			<definition id="0">
				<sentence>Linguistic Based MT uses : core knowledge about the language ; specific knowledge about the corpus ( domain , typology ) ; intrinsic semantics ( a term coined by J.P. Desclds to cover all information formally marked in a natural language , but which refers to its interpretation , such as semantic features or relations : concreteness , location , cause , instrument ... ) ; but not : extrinsic semantics ( static knowledge de~ribing the domain ( s ) of the text , e.g. in terms of facts and rules ) ; situational semantics ( describing the dynmnic situations and their actors ) ; pragmatics ( overt or covert intentions in the communicative context ) .</sentence>
				<definiendum id="0">intrinsic semantics</definiendum>
				<definiens id="0">static knowledge de~ribing the domain ( s ) of the text</definiens>
				<definiens id="1">describing the dynmnic situations and their actors ) ; pragmatics ( overt or covert intentions in the communicative context )</definiens>
			</definition>
			<definition id="1">
				<sentence>Certain elements ( such as menu items ) should be automatically proposed for insertion in the list .</sentence>
				<definiendum id="0">Certain elements</definiendum>
				<definiens id="0">such as menu items</definiens>
			</definition>
			<definition id="2">
				<sentence>Promotion of the National Languages is becomi~g q~fite important nowadays , but , apmt of efforts to teach a few for~zigl~ langt~ages , no technical .</sentence>
				<definiendum id="0">National Languages</definiendum>
				<definiens id="0">becomi~g q~fite important nowadays , but</definiens>
			</definition>
			<definition id="3">
				<sentence>\ [ 9\ ] RICHARDSON S. D. ( 1985 ) Enharmed Text Critiquing using a Natural Language Parser : the CRITIQUE system .</sentence>
				<definiendum id="0">Natural Language Parser</definiendum>
			</definition>
</paper>

		<paper id="2072">
			<definition id="0">
				<sentence>( NONE is a regular atomic value that is given special status by the parser . )</sentence>
				<definiendum id="0">NONE</definiendum>
			</definition>
			<definition id="1">
				<sentence>C-PATRs chart parser is a simplified version of general chart parsing algorithm .</sentence>
				<definiendum id="0">C-PATRs chart parser</definiendum>
				<definiens id="0">a simplified version of general chart parsing algorithm</definiens>
			</definition>
			<definition id="2">
				<sentence>Each template consists of a name ( designated by an Q-sign ) followed by a set of explicit path equations and references to other templates \ [ see Appendix A\ ] .</sentence>
				<definiendum id="0">template</definiendum>
			</definition>
			<definition id="3">
				<sentence>Path equations mentioned in a lexical entry should describe only the idiosyncratic properties of the word .</sentence>
				<definiendum id="0">Path equations</definiendum>
				<definiens id="0">the idiosyncratic properties of the word</definiens>
			</definition>
			<definition id="4">
				<sentence>CPATR is an order of magnitude faster than D-PATR .</sentence>
				<definiendum id="0">CPATR</definiendum>
				<definiens id="0">an order of magnitude faster than D-PATR</definiens>
			</definition>
			<definition id="5">
				<sentence>\ [ 3\ ] Koskenniemi , Kimmo , Two-Level Morphology : A General Computational Model for Word-Form Recognition and Production , Publications No. 11 , Department of General Linguistics , University of Helsinki , Helsinki , Finland , 1983 .</sentence>
				<definiendum id="0">Koskenniemi , Kimmo , Two-Level Morphology</definiendum>
				<definiens id="0">A General Computational Model for Word-Form Recognition</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>A style checker is a tool which supports authors during the process of writing : Certain style markers are analyzed , their values are compared with a given norm , deviations are detected , and recommendations are given to the author .</sentence>
				<definiendum id="0">style checker</definiendum>
				<definiens id="0">a tool which supports authors during the process of writing : Certain style markers are analyzed , their values are compared with a given norm , deviations are detected</definiens>
			</definition>
			<definition id="1">
				<sentence>Within a functional , group based definition , style is the selection of certain words , phrases , sentences or structures out of a set of grammatically correct words , phrases , sentences or structures .</sentence>
				<definiendum id="0">style</definiendum>
				<definiens id="0">the selection of certain words , phrases , sentences or structures out of a set of grammatically correct words , phrases , sentences or structures</definiens>
			</definition>
			<definition id="2">
				<sentence>Style errors can be detected on several different levels : word , phrase , sentence and text .</sentence>
				<definiendum id="0">Style errors</definiendum>
			</definition>
			<definition id="3">
				<sentence>SIMSA consists of three main parts .</sentence>
				<definiendum id="0">SIMSA</definiendum>
				<definiens id="0">consists of three main parts</definiens>
			</definition>
			<definition id="4">
				<sentence>Hirst : Stylistic Grammars in Language Translation .</sentence>
				<definiendum id="0">Hirst</definiendum>
			</definition>
</paper>

		<paper id="2060">
			<definition id="0">
				<sentence>As a further example consider the literal stibject ( AI , A2 , WHQ , h~JM , P ) where A1 and A2 arc input and output strings of words , WttQ indicates whether the subject phrase is a part of a clause within a wh-question , ~ is the number of the subject phrase , and P is the final translation .</sentence>
				<definiendum id="0">phrase</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">P ) where A1 and A2 arc input and output strings of words , WttQ indicates whether the subject</definiens>
				<definiens id="1">a part of a clause within a wh-question</definiens>
				<definiens id="2">the number of the subject phrase</definiens>
				<definiens id="3">the final translation</definiens>
			</definition>
			<definition id="1">
				<sentence>of a clause if ( A ) , or ( B ) , or ( F ) L is the top-level literal and X is `` in '' in it ( known a priori ) ; or ( G ) X occurs more than once in L and at least one of these occurrences is `` in '' ; or ( H ) for every literal L 1 : pred ( .</sentence>
				<definiendum id="0">L</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">the top-level literal and</definiens>
			</definition>
			<definition id="2">
				<sentence>6 In the following procedure , the expression VAR ( T ) , where 7 '' is a set of terms , denotes the the set of all variables occurring in the terms in T. MSEAS ( MS , MSEA , VP , i , OUT ) ( 1 ) Start with VP = VAR ( { X1 , `` `` , X. } ) , MSEA = f~ , i=1 , and OUT = O. When the computation is completed , MS is bound to the set of active MSEA 's for P. ( 2 ) Let MR 1 be the set of active MSEA 's of R 1 , and let MRU1 be obtained from MR ~ by replacing all variables in each member of MR1 by their corresponding actual arguments of R~ on the rhs of ( C1 ) .</sentence>
				<definiendum id="0">expression VAR</definiendum>
				<definiendum id="1">MS</definiendum>
				<definiens id="0">the the set of all variables occurring in the terms in T. MSEAS ( MS , MSEA , VP , i</definiens>
			</definition>
			<definition id="3">
				<sentence>MSEA 's belonging to different groups give rise to alternative sets of MSEA 's in the final set MS. Note that in this modified algorithm , MS becomes a set of sets of sets .</sentence>
				<definiendum id="0">MS</definiendum>
				<definiens id="0">becomes a set of sets of sets</definiens>
			</definition>
			<definition id="4">
				<sentence>INVERSE ( `` head : old-rhs '' , ins , outs ) ; { ins and outs are subsets of VAR ( head ) which are `` in '' and are required to be `` out '' , respectively } begin compute M the set of all MSEA 's for head ; for every MSEA m e M do begin OUT : = ~ ; if m is an active MSEA such that me ins then begin compute `` out '' arguments in head ; add them to OUT ; 4 350 if outs cOUT then DONE ( `` head : -old-rhs '' ) end else if m is a non-active MSEA and mc_ins then begin new- .</sentence>
				<definiendum id="0">INVERSE</definiendum>
				<definiens id="0">subsets of VAR ( head ) which are `` in '' and are required to be `` out ''</definiens>
				<definiens id="1">an active MSEA such that me ins then begin compute `` out '' arguments in head ; add them to OUT</definiens>
			</definition>
</paper>

		<paper id="3029">
			<definition id="0">
				<sentence>The `` Low '' of Attach bow and Parallel is an instance of a general cognitive heuristic to interpret features of the enviromnent ~ locally as possible .</sentence>
				<definiendum id="0">Parallel</definiendum>
				<definiens id="0">an instance of a general cognitive heuristic to interpret features of the enviromnent</definiens>
			</definition>
</paper>

		<paper id="2067">
			<definition id="0">
				<sentence>At present , the Vassar/CNRS data base includes , through the courtesy of several editors and research institutions , several English and French dictionaries ( the Collins English Dictionary , the Oxford Advanced Learner 's Dictionary , the COBUILD Dictionary , the Longman ) Dictionary of Contemporary English , theWebster 's 9th Dictionary , and the ZYZOMYS CD-ROM dictionary from Hachette Publishers ) as well as several other lexical and textual materials ( the Brown Corpus of American English , the CNRS BDLex data base , the MRC Psycholinguistic Data Base , etc. ) .</sentence>
				<definiendum id="0">textual materials</definiendum>
				<definiens id="0">the Collins English Dictionary , the Oxford Advanced Learner 's Dictionary , the COBUILD Dictionary , the Longman ) Dictionary of Contemporary English</definiens>
				<definiens id="1">the Brown Corpus of American English , the CNRS BDLex data base , the MRC Psycholinguistic Data Base</definiens>
			</definition>
</paper>

		<paper id="3047">
			<definition id="0">
				<sentence>Essentially , the CS level preserves the connectivity properties of the symbolic structures .</sentence>
				<definiendum id="0">CS level</definiendum>
				<definiens id="0">preserves the connectivity properties of the symbolic structures</definiens>
			</definition>
			<definition id="1">
				<sentence>, 0 , { 0 } ) where L x is the finite set of strings defined over the symbolic alphabet X , and U and .</sentence>
				<definiendum id="0">L x</definiendum>
				<definiens id="0">the finite set of strings defined over the symbolic alphabet X</definiens>
			</definition>
			<definition id="2">
				<sentence>AS-DAGs codify the way in which symbolic labels address , or map onto , the nodes and edges of ASStts .</sentence>
				<definiendum id="0">AS-DAGs</definiendum>
				<definiens id="0">codify the way in which symbolic labels address , or map onto , the nodes and edges of ASStts</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus , the memory consists of a set of superposition spaces , where each space has a label ( or address ) , and where labels can be encoded as elements of other spaces resulting in a hierarchical structure .</sentence>
				<definiendum id="0">memory</definiendum>
				<definiens id="0">consists of a set of superposition spaces , where each space has a label ( or address ) , and where labels can be encoded as elements of other spaces resulting in a hierarchical structure</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>Based on a GB-inspired parser , lexical transfer and lexical projection , STS provides real-time accurate English translations for a small but non-trivial subset of French sentences .</sentence>
				<definiendum id="0">STS</definiendum>
				<definiens id="0">provides real-time accurate English translations for a small but non-trivial subset of French sentences</definiens>
			</definition>
			<definition id="1">
				<sentence>The lexical database is the central piece of the STS system .</sentence>
				<definiendum id="0">lexical database</definiendum>
			</definition>
</paper>

		<paper id="3018">
			<definition id="0">
				<sentence>An IF contains four pragmatic features in addition to the propositional content and speech act of the proposition : argumentative orientation ( Duerot , 1983 ) , the set of conclusions that the proposition supports ; functional status ( Sinclair &amp; Coulthard , 1975 , Roulet et al , 1985 ) , its structural relationship to the remaining discourse segment ; polyphonic features ( Ducrot , 1983 ) , indicating whether the speaker attributes the utterance to himself or to others ; and a thematization procedure , which describes the connection between discourse entities in the propositions .</sentence>
				<definiendum id="0">thematization procedure</definiendum>
				<definiens id="0">four pragmatic features in addition to the propositional content and speech act of the proposition : argumentative orientation</definiens>
			</definition>
			<definition id="1">
				<sentence>Halliday ( Halliday , 1985 ) proposes that the connection between clauses can be described on three dimensions : taxis , expansion and projection .</sentence>
				<definiendum id="0">Halliday</definiendum>
				<definiens id="0">taxis , expansion and projection</definiens>
			</definition>
			<definition id="2">
				<sentence>The given/new distinction gives one interpretation of these differences : `` because '' introduces new information , whereas `` since '' introduces given inibrmation ( where given is defined as information that the listener already knows or has accessible to him ( Halliday , 1985 ) ) .</sentence>
				<definiendum id="0">given/new distinction</definiendum>
				<definiens id="0">gives one interpretation of these differences : `` because '' introduces new information , whereas `` since '' introduces given inibrmation ( where given is defined as information that the listener already knows or has accessible to him</definiens>
			</definition>
			<definition id="3">
				<sentence>Polyphony is one type of given information but it adds an additional parameter : each piece of given information is attributed to a particular utterer .</sentence>
				<definiendum id="0">Polyphony</definiendum>
				<definiens id="0">one type of given information but it adds an additional parameter : each piece of given information is attributed to a particular utterer</definiens>
			</definition>
			<definition id="4">
				<sentence>The SA thematization procedure adds the feature Speech-Act : to the theme of the proposition Q. In ( 6 ) , `` since '' links on the Utterance Act : the fact that B utters `` she is sick '' is justified by A 's insistence on knowing everything ( note that `` since '' does not justify the assertion but the fact that B is speaking at all ) .</sentence>
				<definiendum id="0">SA thematization procedure</definiendum>
				<definiens id="0">speaking at all )</definiens>
			</definition>
			<definition id="5">
				<sentence>Nigel : a Systemic Grammar for Text Generation .</sentence>
				<definiendum id="0">Nigel</definiendum>
				<definiens id="0">a Systemic Grammar for Text Generation</definiens>
			</definition>
</paper>

		<paper id="2062">
			<definition id="0">
				<sentence>The information content of each event consists of : TREE RULE-NAME PIVOT-NODE DELETED-NODES NEW-NODES ACTIVE-NODES resultant tree structure , applied rule , pivot node , list of deleted nodes , list of created nodes , list of modified nodes and their annotations .</sentence>
				<definiendum id="0">information content of each event</definiendum>
			</definition>
			<definition id="1">
				<sentence>NEOMYCIN : reconfiguring a rule-based expert system for application to teaching .</sentence>
				<definiendum id="0">NEOMYCIN</definiendum>
				<definiens id="0">reconfiguring a rule-based expert system for application to teaching</definiens>
			</definition>
</paper>

		<paper id="2048">
			<definition id="0">
				<sentence>A schema is a discourse strategy that captures a typical pattern of discourse associated with a particular discourse purpose , ( e.g. , providing an analogy or evidence ) .</sentence>
				<definiendum id="0">schema</definiendum>
				<definiens id="0">a discourse strategy that captures a typical pattern of discourse associated with a particular discourse purpose , ( e.g. , providing an analogy or evidence )</definiens>
			</definition>
			<definition id="1">
				<sentence>RST posits a small number of relations , comparable to McKeown 's rhetorical predicates , that exist between segments of text .</sentence>
				<definiendum id="0">RST</definiendum>
				<definiens id="0">posits a small number of relations , comparable to McKeown 's rhetorical predicates</definiens>
			</definition>
			<definition id="2">
				<sentence>A basic block consists of two elements : cation , and The focus is what makes a cohesive unit of the material in the block ; it is the thread cominon to all of this material , whether directly or indirectly .</sentence>
				<definiendum id="0">basic block</definiendum>
				<definiens id="0">consists of two elements : cation , and The focus is what makes a cohesive unit of the material in the block</definiens>
			</definition>
			<definition id="3">
				<sentence>The ideal case is one in which each child of the candidate accounts for approximately the same number of concepts .</sentence>
				<definiendum id="0">ideal case</definiendum>
				<definiens id="0">one in which each child of the candidate accounts for approximately the same number of concepts</definiens>
			</definition>
			<definition id="4">
				<sentence>The Linguistic Discourse Model : 7bwards a Yormal Theory of Dis° course Structure .</sentence>
				<definiendum id="0">Linguistic Discourse Model</definiendum>
				<definiens id="0">7bwards a Yormal Theory of Dis° course Structure</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>The Cooperation Architecture is a conceptual framework for AGES design and a set of mechanisms to support implementation of that design .</sentence>
				<definiendum id="0">Cooperation Architecture</definiendum>
				<definiens id="0">a conceptual framework for AGES design and a set of mechanisms to support implementation of that design</definiens>
			</definition>
			<definition id="1">
				<sentence>The Problem Solver is the module which solves the user 's problem { s ) in a financial domain and provides proof trees that the Dialogue Manager can use to generate explanations .</sentence>
				<definiendum id="0">Problem Solver</definiendum>
				<definiens id="0">the module which solves the user 's problem { s ) in a financial domain and provides proof trees that the Dialogue Manager can use to generate explanations</definiens>
			</definition>
			<definition id="2">
				<sentence>Cooperative Answering : a methodology to to provide intelligent access to a Database .</sentence>
				<definiendum id="0">Cooperative Answering</definiendum>
			</definition>
</paper>

		<paper id="2011">
			<definition id="0">
				<sentence>A word lattice is a set of word hypotheses produced by some acoustic signal processor in continuous speech recognition applications which possibly includes problems such as word boundary overlapping , lexical ambiguities , missing or extra phones , recognition uncertainty and errors , etc .</sentence>
				<definiendum id="0">word lattice</definiendum>
			</definition>
			<definition id="1">
				<sentence>A word lattice W is a partially ordered set of word hypotheses , W = { w 1 ... .. win } , where each word hypothesis w i , i=l ... . , m , is characterized by begin , the beginning point , end , the ending point , cat , the category , phone , the associated phonemes , and name , the word name of the word hypothesis .</sentence>
				<definiendum id="0">word lattice W</definiendum>
				<definiens id="0">a partially ordered set of word hypotheses</definiens>
			</definition>
			<definition id="2">
				<sentence>These word hypotheses are sorted in the order of their ending points ; that is , for every pair of word hypotheses w i and wj , i &lt; j implies end ( wi ) &lt; = end ( wj ) . Also , two word hypotheses w i and wj are said to be connected if there is no other word hypothesis located exactiy between the boundaries of the two word hypotheses , i.e. , if w i _ &lt; wj and there does not exist any other word hypothesis w k such that w i &lt; w k _ &lt; wj , where w i _ &lt; wj fff end ( wi ) &lt; = begin ( wj ) . A sentence hypothesis is then a sequence of connected word hypotheses selected from the given word lattice , and a sentence hypothesis is grammatical valid only if it can be generated by a grammar. As an example , a sample word lattice constructed for demonstration purpose is shown on the top of Fig. 1 , in which only the word sequence `` Tad does this. '' is a valid sentence hypothesis. The augmented chart is a directed uncyclic graph specified by a two-tuple &lt; V , E &gt; , where V is a sequence of vertices and E is a set of edges .</sentence>
				<definiendum id="0">sentence hypothesis</definiendum>
				<definiendum id="1">V</definiendum>
				<definiendum id="2">E</definiendum>
				<definiens id="0">a valid sentence hypothesis. The augmented chart is a directed uncyclic graph specified by a two-tuple &lt; V , E &gt; , where</definiens>
				<definiens id="1">a sequence of vertices</definiens>
				<definiens id="2">a set of edges</definiens>
			</definition>
</paper>

		<paper id="3073">
</paper>

		<paper id="3082">
			<definition id="0">
				<sentence>Idealizing considerably , SynlnL formulas consist of four types : heads , complements , modifiers and specifiers .</sentence>
				<definiendum id="0">SynlnL formulas</definiendum>
				<definiens id="0">consist of four types : heads , complements , modifiers and specifiers</definiens>
			</definition>
			<definition id="1">
				<sentence>The answer process consists of the following steps : e The question is parsed .</sentence>
				<definiendum id="0">answer process</definiendum>
			</definition>
</paper>

		<paper id="3086">
</paper>

		<paper id="3041">
</paper>

		<paper id="3076">
			<definition id="0">
				<sentence>\ [ 'ragmental utterance comprehension by a hearer can be achieved using knowledge of the dialogue sitamtion , context intbrmation , domain dependent knowledge , especially the donmin dependent action hierarchy\ [ Litman871 , universal pragmatics concerning how to advance a dialogue , maintain dialogue cooperation between dialogue participants , etc. , and language specific pragmatics \ [ Levinson831 .</sentence>
				<definiendum id="0">'ragmental utterance comprehension</definiendum>
			</definition>
			<definition id="1">
				<sentence>LevinsonS3 ) can be classified and described by the following plans : Interaction-Plan a plan basically eharaeterized by a dialogue turntaking* ( 2 ) which describes a sequence of communicative acts* ( 3 ) , Communication-Plan a plan which determines how to execute or 370 achieve an utterance goal or dialogue goals , and Dialogued : qan : a plan for establishing a dialogue construction , e.g. a cooperative dialogue* ( 4 ) .</sentence>
				<definiendum id="0">LevinsonS3</definiendum>
				<definiens id="0">determines how to execute or 370 achieve an utterance goal or dialogue goals</definiens>
			</definition>
			<definition id="2">
				<sentence>A communicative act is a decomposition element of an interaction plan .</sentence>
				<definiendum id="0">communicative act</definiendum>
				<definiens id="0">a decomposition element of an interaction plan</definiens>
			</definition>
</paper>

		<paper id="2023">
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>DRS+f-structure gives a ground for examining logical conditions based excluded from our discussion .</sentence>
				<definiendum id="0">DRS+f-structure</definiendum>
			</definition>
			<definition id="1">
				<sentence>( 4 ) &lt; ^/GF ( ADJUNCT ) GF* ' , SUBJ Ant &gt; =c &lt; A Desc &gt; This equation says that if there exists a SUBJ that fcommands GF ( s ) that may contain an adjunct which includes an f-structure in which the current pro resides , then it must be the case that the Ant value of that SUBJ and Desc value of the pro are identical .</sentence>
				<definiendum id="0">SUBJ</definiendum>
				<definiens id="0">that fcommands GF ( s ) that may contain an adjunct which includes an f-structure in which the current pro resides , then it must be the case that the Ant value of that SUBJ and Desc value of the pro are identical</definiens>
			</definition>
			<definition id="2">
				<sentence>SMS is a pair &lt; SemType , DRS &gt; , where SemType is a semantic type .</sentence>
				<definiendum id="0">SMS</definiendum>
				<definiendum id="1">SemType</definiendum>
				<definiens id="0">a semantic type</definiens>
			</definition>
			<definition id="3">
				<sentence>If they are not unifiable , the focus has been shifted , and SLSTOR stores the newly selected element as a current focus .</sentence>
				<definiendum id="0">SLSTOR</definiendum>
				<definiens id="0">stores the newly selected element as a current focus</definiens>
			</definition>
			<definition id="4">
				<sentence>The last function is SETPRO , which assigns 0 to the pronominal in the case of 2 ) , and assigns a default word-for-word translation in the case of 3 ) .</sentence>
				<definiendum id="0">SETPRO</definiendum>
				<definiens id="0">assigns 0 to the pronominal in the case of 2</definiens>
			</definition>
</paper>

		<paper id="3075">
			<definition id="0">
				<sentence>Suppose your adversary accepts the invitation to discuss , and takes the antecedent of the counterfactual as a temporary additional concession .</sentence>
				<definiendum id="0">Suppose your adversary</definiendum>
				<definiens id="0">accepts the invitation to discuss , and takes the antecedent of the counterfactual as a temporary additional concession</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>The system was implemented on VAX/VMS in Quintus Prolog and consists of the following parts : ( 1 ) The compiler , which takes as its input two-level rules and produces final state automata ( transducers ) .</sentence>
				<definiendum id="0">compiler</definiendum>
				<definiens id="0">takes as its input two-level rules and produces final state automata ( transducers )</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>Transformational rules fall into two groups : simple local transformations ( like subject-aux inversion ) and major movement rules like wh movement .</sentence>
				<definiendum id="0">Transformational rules</definiendum>
				<definiens id="0">simple local transformations ( like subject-aux inversion ) and major movement rules like wh movement</definiens>
			</definition>
			<definition id="1">
				<sentence>In general , for each type of phrase , creation of the phrase ( creating a new node on the active node stack ) and completion of the phrase ( dropping it into the buffer ) is carried out by a separate grammar rule action .</sentence>
				<definiendum id="0">creation of the phrase</definiendum>
				<definiens id="0">creating a new node on the active node stack</definiens>
			</definition>
			<definition id="2">
				<sentence>Learning itself occurs off-line and is a time-consuming process , but once learned the processing times for file system are excellent .</sentence>
				<definiendum id="0">Learning itself</definiendum>
			</definition>
			<definition id="3">
				<sentence>Each input pattern consists of two feature vectors from the buffer items and one vector from the stack .</sentence>
				<definiendum id="0">input pattern</definiendum>
				<definiens id="0">consists of two feature vectors from the buffer items and one vector from the stack</definiens>
			</definition>
</paper>

		<paper id="2069">
			<definition id="0">
				<sentence>Banfield characterizes the sentences of narration as objective or subjective .</sentence>
				<definiendum id="0">Banfield</definiendum>
				<definiens id="0">characterizes the sentences of narration as objective or subjective</definiens>
			</definition>
			<definition id="1">
				<sentence>Less commonly , the subjective character is the actor of an action denoted by a previous objective sentence .</sentence>
				<definiendum id="0">subjective character</definiendum>
				<definiens id="0">the actor of an action denoted by a previous objective sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus , the last subjective character : is an expected subjective character only if a subjective sentence has appeared in the current scene ( see Wiebe 1990 for examples ) .</sentence>
				<definiendum id="0">last subjective character :</definiendum>
				<definiens id="0">is an expected subjective character only if a subjective sentence has appeared in the current scene</definiens>
			</definition>
			<definition id="3">
				<sentence>Passage ( 4 ) shows , however , that the subjective character of a private-state sentence need not be the experiencer .</sentence>
				<definiendum id="0">Passage</definiendum>
				<definiens id="0">the subjective character of a private-state sentence</definiens>
			</definition>
			<definition id="4">
				<sentence>A subjective element is a linguistic element that actually indicates that a sentence is subjective in the context of use .</sentence>
				<definiendum id="0">subjective element</definiendum>
				<definiens id="0">a linguistic element that actually indicates that a sentence is subjective in the context of use</definiens>
			</definition>
			<definition id="5">
				<sentence>A private-state sentence that is interpreted to be a private-state report , on the other hand , is a report of a character 's private state ; it is not someone 's thought about it .</sentence>
				<definiendum id="0">private-state sentence</definiendum>
				<definiens id="0">a report of a character 's private state ; it is not someone 's thought about it</definiens>
			</definition>
			<definition id="6">
				<sentence>98-99\ ] Sentence ( 6.3 ) is a private-state report and the subjective , character is the experiencer ( Johnnie Martin ) ; this is so even though ( 6.3 ) contains the sub~ jeetive element 'old bag ' and even though there is an expected subjective character ( the girl ) when it is encountered .</sentence>
				<definiendum id="0">character</definiendum>
				<definiens id="0">a private-state report and the subjective ,</definiens>
				<definiens id="1">an expected subjective character ( the girl ) when it is encountered</definiens>
			</definition>
			<definition id="7">
				<sentence>15 Provisions for the following have been omitted for brevity : broadening and narrowing of point of view , sentences denoting private-state actions , and private-state sentences that can be objective To identify SC : If the sentence contains a narrative parenthetical then SC is the subject of the parenthetical else if the sentence is a private-state sentence then if it has a non-subordinated subjective clement or the text situation is continuing-subjective then SC is identified from the previous context else SC is the experiencer end if else SC is identified from the previous context end if To identify SC from the previous context : If there are two expected subjective characters then if the sentence is about the last active character then SC is the last subjective character else SC is the last active character end if else if there is an expected subjective character then SC is the expected subjective character else SC is unidentified end if along two avenues .</sentence>
				<definiendum id="0">SC</definiendum>
				<definiendum id="1">SC</definiendum>
				<definiendum id="2">SC</definiendum>
				<definiendum id="3">SC</definiendum>
				<definiens id="0">the subject of the</definiens>
				<definiens id="1">continuing-subjective then SC is identified from the previous context else</definiens>
				<definiens id="2">the last subjective character else</definiens>
			</definition>
</paper>

		<paper id="3057">
			<definition id="0">
				<sentence>The transducing dictionary ( TD ) , originally used for ~T between English and Czech languages , handled mostly international ( Greek-Latin ) words .</sentence>
				<definiendum id="0">transducing dictionary ( TD</definiendum>
				<definiens id="0">originally used for ~T between English and Czech languages , handled mostly international ( Greek-Latin ) words</definiens>
			</definition>
</paper>

		<paper id="2034">
			<definition id="0">
				<sentence>The scgmc~zlalion compone~t tells the program what to parse , what to skip , and where to use seinantic inlbrmation for attachment .</sentence>
				<definiendum id="0">scgmc~zlalion compone~t</definiendum>
				<definiens id="0">tells the program what to parse , what to skip , and where to use seinantic inlbrmation for attachment</definiens>
			</definition>
			<definition id="1">
				<sentence>`` Acme rejects an offer '' and `` Acme plans an offer '' place Acme in different roles ( i.e. the target and suitor , respectively ) .</sentence>
				<definiendum id="0">Acme</definiendum>
				<definiens id="0">rejects an offer '' and `` Acme plans an offer '' place Acme in different roles</definiens>
			</definition>
			<definition id="2">
				<sentence>Aetivity : Skip over empty sentences and phrases , and break the combinatorics of parsing where a single parse will do .</sentence>
				<definiendum id="0">Aetivity</definiendum>
				<definiens id="0">Skip over empty sentences and phrases</definiens>
			</definition>
			<definition id="3">
				<sentence>The segmentation algorithm includes most important noun phrases , even when separation information prevents them from being attached .</sentence>
				<definiendum id="0">segmentation algorithm</definiendum>
				<definiens id="0">includes most important noun phrases</definiens>
			</definition>
			<definition id="4">
				<sentence>The effect of the skimming algorithm is to give more weight to the semantic attachment of these phrases .</sentence>
				<definiendum id="0">effect of the skimming algorithm</definiendum>
				<definiens id="0">to give more weight to the semantic attachment of these phrases</definiens>
			</definition>
			<definition id="5">
				<sentence>The skimming algorithm , in its application of linguistic relations , must use both positive and negative intbrmation in determining where to attach phrases .</sentence>
				<definiendum id="0">skimming algorithm</definiendum>
				<definiens id="0">use both positive and negative intbrmation in determining where to attach phrases</definiens>
			</definition>
			<definition id="6">
				<sentence>The GE NUl'oolset : A software foundation for intelligence text processing .</sentence>
				<definiendum id="0">GE NUl'oolset</definiendum>
				<definiens id="0">A software foundation for intelligence text processing</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>1 17 emterr~aI ( S , EF ) relates a semantic representation S and an external form EF , e.g. a representation that constitutes the parscr 's output .</sentence>
				<definiendum id="0">EF )</definiendum>
				<definiens id="0">relates a semantic representation S and an external form EF , e.g. a representation that constitutes the parscr 's output</definiens>
			</definition>
			<definition id="1">
				<sentence>Following Pereira and Shieber ( who were in turn inspired by Montague ) , VP and N incomings are represented by terms of the form x ^ S , where X represents a referential index and S represents an S meaning .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a referential index and S represents an S meaning</definiens>
			</definition>
			<definition id="2">
				<sentence>The semantic value associated with lexical entry for a determiner in the grammars presented here is a term of the form Res~Scol ) e~Sentence , where lees is the semantic value associated with the restrictor and Scope is the semantic value associated with the scope .</sentence>
				<definiendum id="0">Scope</definiendum>
				<definiens id="0">the semantic value associated with the restrictor</definiens>
				<definiens id="1">the semantic value associated with the scope</definiens>
			</definition>
			<definition id="3">
				<sentence>The semantic representation S is the composition of S1 and $ 2 , where S2 is the semantic atom ResName =- &gt; ScopeName .</sentence>
				<definiendum id="0">semantic representation S</definiendum>
				<definiendum id="1">S2</definiendum>
				<definiens id="0">the composition of S1</definiens>
				<definiens id="1">the semantic atom ResName =- &gt; ScopeName</definiens>
			</definition>
			<definition id="4">
				<sentence>Re s is subordinate to S1 , and is itself the composition of ResO and Resl , where Res0 is the semantic representation of the restricter .</sentence>
				<definiendum id="0">Res0</definiendum>
				<definiens id="0">subordinate to S1 , and is itself the composition of ResO and Resl , where</definiens>
			</definition>
			<definition id="5">
				<sentence>S = : ( man ( X ) &amp; donkey ( Y ) &amp; own ( X , Y ) ) == &gt; beat ( X , Z ) Roughly this latter form might be interpreted as : if X is a m~m and Y is a donkey and X owns Y , then there is a Z such that X beats Z. The Sets-of-In fens Constructors The constructors for the sets-of-infons and the discourse-representation both constrain anaphora by requiring that the referential indices provided by the accessible index constructor be indices that were introduced by new index in some earlier representation ( where precedence is defined by the compose constmclor ) .</sentence>
				<definiendum id="0">Y</definiendum>
			</definition>
			<definition id="6">
				<sentence>The atom constructor introduces a new atomic proposition p as an infon Sit : P , where Sit is the situation currently being constructed .</sentence>
				<definiendum id="0">atom constructor</definiendum>
				<definiendum id="1">Sit</definiendum>
			</definition>
			<definition id="7">
				<sentence>sO X is a man , Y is a donkey and X owns Y. 2p ( \ [ every , man , owns , a , donkey\ ] , S ) .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">a man</definiens>
			</definition>
			<definition id="8">
				<sentence>A situation is of type sl if it contains individuals X and Y , and X is a man and Y is a donkey .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiens id="0">a man</definiens>
				<definiens id="1">a donkey</definiens>
			</definition>
			<definition id="9">
				<sentence>22 6 is a man , Y is a donkey and X owns Y. situation is of type a2 if X beats y.5 The Discourse-Representation Constructors A The representations built by these constructors are inspired by the `` box representations '' of Kamp 's ( 1981 ) Discourse Representation Theory \ [ 9\ ] .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">a man</definiens>
			</definition>
			<definition id="10">
				<sentence>A representation is a difference-pair of the lists of the representations of the currently open boxes ( i.e. the current box and all superordinate boxes ) , as in Johnson and Klein \ [ 81 .</sentence>
				<definiendum id="0">representation</definiendum>
				<definiens id="0">a difference-pair of the lists of the representations of the currently open boxes ( i.e. the current box and all superordinate boxes</definiens>
			</definition>
			<definition id="11">
				<sentence>The subordinate constructor introduces an empty subordinate box onto the list of ' curan additional reading in which tile man that owns the donkey beats himself ; i.e. it is taken as anaphorically dependent on every mare Simple extensions to tim grammar ( e.g. requiring tile index of a pronoun to differ from the index of all c-commanding NPs ) or Ihe semantics ( e.g. requiring the gender of tile pronoun to agree with its antecedent 's gender ) would ride out this spurious analysis .</sentence>
				<definiendum id="0">subordinate constructor</definiendum>
				<definiendum id="1">Ihe semantics</definiendum>
				<definiens id="0">introduces an empty subordinate box onto the list of ' curan additional reading in</definiens>
			</definition>
			<definition id="12">
				<sentence>S = \ [ own ( X , Y ) , donkey ( Y ) , i ( Y , man ( X ) , i ( X ) \ ] This representation is tree just m case there are two individuals X aid } i , X is a man and Y is a donkey , and X owns } I. ?</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiendum id="2">X</definiendum>
				<definiens id="0">a man</definiens>
				<definiens id="1">a donkey</definiens>
			</definition>
			<definition id="13">
				<sentence>S = \ [ \ [ man ( X ) , i ( X ) \ ] == &gt; \ [ own ( X , Y ) , donkey ( Y ) , i ( Y ) \ ] \ ] This representation is true just in case for all individuals X such that X is a man there is an individual Y such that Y is a donkey and X owns Y. 2p ( \ [ every , man , that , owns , a , donkey , beats , it\ ] , S ) .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a donkey</definiens>
			</definition>
			<definition id="14">
				<sentence>S = \ [ \ [ own ( X , Y ) , donkey ( Y ) , i ( Y ) , man ( X ) , i ( X ) \ ] == &gt; \ [ beat ( X , Y ) \ ] \ ] This representation is true just in case for all individuals X and Y such that X is a man and Y a donkey and X owns Y , it is also true that X beats Y. Extending the Grammar to handle Quantifier-Raising In this section we sketch a syntactic account of quantifier-raising inspired by the implementation of Cooper-storage ( Cooper \ [ 41 ) 7 23 presented in Pereira and Shieber \ [ 12\ ] , to which we refer the reader for details .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a man</definiens>
			</definition>
			<definition id="15">
				<sentence>Quantiticational determiners add items to the quantifier store , and at S nodes , quantifiers are removed from the store and applied to the semantic representation .</sentence>
				<definiendum id="0">Quantiticational determiners</definiendum>
				<definiens id="0">add items to the quantifier store , and at S nodes , quantifiers are removed from the store and applied to the semantic representation</definiens>
			</definition>
			<definition id="16">
				<sentence>The proposition shuffle ( LI , LI , L3 ) is true just in case L3 is a list that can be seen as having been constructed in a sequence of steps in each of which the next available item is taken from either L1 or L2 and added to the end .</sentence>
				<definiendum id="0">proposition shuffle</definiendum>
				<definiens id="0">true just in case L3 is a list that can be seen as having been constructed in a sequence of steps in each of which the next available item is taken from either L1 or L2 and added to the end</definiens>
			</definition>
			<definition id="17">
				<sentence>The first reading displayed corresponds to the quantilier-raised interpretation , which paraphrases as : Situation s0 contains the individual Y , the fact that Y is a donkey , and the fact that for all ways of making sl tale , s2 is also true , where sl contains the individual X and the fact that X is a man , and s2 conrains the fact that X owns Y. Since Y is in s ( ) , under this reading it is a potential matecedent for ~maphors in for following sentences .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">paraphrases as : Situation s0 contains the</definiens>
				<definiens id="1">a man , and s2 conrains the fact that X owns Y. Since Y is in s</definiens>
			</definition>
			<definition id="18">
				<sentence>S sO : \ [ sO : sl== &gt; s2 , s2 : beat ( X , Y ) , sl : own ( X , Y ) , sl : i ( X ) , sl : man ( X ) sO : i ( Y ) , sO : donkey ( Y ) \ ] ; S = sO : \ [ sO : sl== &gt; s2 , s2 : beat ( X , Y ) , sl : own ( X , Y ) , sl : i ( Y ) , sl : donkey Y ) , sl : i ( X ) , sl : man ( X ) \ ] The first reading displayed again corresponds to the quantifier-raiscd interpretation , which paraphrases as : Situation s0 contains an individual Y , and the facts that Y is a donkey and that every way of making S1 true also makes $ 2 tree , where S1 contains the individual X and the facts that X is a man and X owns Y , and $ 2 contains the fact that X beats Y. 9 25 Finally , the discourse-representation constructors yield the following : 2q ( \ [ every , man , owns , a , donkey\ ] , S ) .</sentence>
				<definiendum id="0">quantifier-raiscd interpretation</definiendum>
				<definiens id="0">a donkey and that every way of making S1 true also makes $ 2 tree , where S1 contains the individual X and the facts that X is a man</definiens>
			</definition>
</paper>

		<paper id="3085">
			<definition id="0">
				<sentence>ARCHITECTURAL OVERVIEW Tile architecture of NAS is modular consisting of several main subsystems , viz. , a set of preprocessors and template filters , a parser , a lexicon , a semantic interpreter , a set of concept bases , and an indexer ( Figure 1 ) .</sentence>
				<definiendum id="0">ARCHITECTURAL OVERVIEW Tile architecture of NAS</definiendum>
				<definiens id="0">modular consisting of several main subsystems , viz. , a set of preprocessors</definiens>
			</definition>
			<definition id="1">
				<sentence>For rigidly-formatted articles that are numerical or non-texttlal in form , a template filter , which is an indexing component of low-level routines , categorizes them from lhe title while deriving specifics l'rom the body of tile story .</sentence>
				<definiendum id="0">template filter</definiendum>
				<definiens id="0">an indexing component of low-level routines , categorizes them from lhe title while deriving specifics l'rom the body of tile story</definiens>
			</definition>
			<definition id="2">
				<sentence>The parser , in accordance with GB principles , produces : indexes : Company Name ACTMEDIA INC Descriptor 3RD Quarter Earnings Subject Net loss 1,674,000 Descriptor Current Earnings Subject Net loss 2,280,000 Descriptor Cumulative Earnings ( 4 ) Agent ( Alpha Corp ) Predicate ( said ) Proposition Agent ( Alpha Corp ) Predicate ( plans ) Proposition Agent ( Alpha Corp ) Predicate ( release ) Theme ( a new workstation ) In ( Japan ) A Numerical Story and Its Index Figure 2 In contrast , textual stories require grammatical processing and these are sent to the parser and semantic interpreter .</sentence>
				<definiendum id="0">Numerical Story</definiendum>
				<definiens id="0">in accordance with GB principles , produces : indexes : Company Name ACTMEDIA INC Descriptor 3RD Quarter Earnings Subject Net loss 1,674,000 Descriptor Current Earnings Subject Net loss 2,280,000 Descriptor Cumulative Earnings ( 4 ) Agent ( Alpha Corp ) Predicate ( said ) Proposition Agent ( Alpha Corp ) Predicate ( plans ) Proposition Agent ( Alpha Corp ) Predicate ( release ) Theme ( a new workstation</definiens>
			</definition>
			<definition id="3">
				<sentence>The parser which relies on the principles of Government-Binding ( GB ) Theory ( /Chomsky 1981/3 , outputs predicate-argument structure of each sentence of a sto W.a In doing so , the parser identi ties empty categories , viz. , PROs , traces , and variables , and thematic relations , and resolves antecedent and anaphor and pronominal bindings .</sentence>
				<definiendum id="0">Government-Binding</definiendum>
				<definiens id="0">outputs predicate-argument structure of each sentence of a sto W.a In doing so , the parser identi ties empty categories , viz. , PROs , traces , and variables , and thematic relations , and resolves antecedent and anaphor and pronominal bindings</definiens>
			</definition>
</paper>

		<paper id="3028">
			<definition id="0">
				<sentence>`` A Prolog Technology Theorem Prover : A New Exposition and Implementation in Prolog '' , Technical Note No. 464 .</sentence>
				<definiendum id="0">Prolog Technology Theorem Prover</definiendum>
				<definiens id="0">A New Exposition and Implementation in Prolog ''</definiens>
			</definition>
</paper>

		<paper id="2006">
			<definition id="0">
				<sentence>Certain elements ( such as menu items ) should be automatically proposed for insertion in the list .</sentence>
				<definiendum id="0">Certain elements</definiendum>
				<definiens id="0">such as menu items</definiens>
			</definition>
			<definition id="1">
				<sentence>CMT , CMU , Pittsburgh , 286 p. \ [ 8\ ] PECCOUD F. ( 1988 ) The Aims of the French National Project of Computer-Aided Translation .</sentence>
				<definiendum id="0">CMT</definiendum>
			</definition>
			<definition id="2">
				<sentence>\ [ 9\ ] RICttARDSON S. D. ( 1985 ) Enhanced Text Critiquing using a Natural Language Parser : the CRIllQUE system .</sentence>
				<definiendum id="0">Natural Language Parser</definiendum>
			</definition>
</paper>

		<paper id="3022">
			<definition id="0">
				<sentence>Binding Theory ( BT ) is a module of the Government and Binding theory ruling the distribution and the referential properties of anaphors ( such as himself ) , pronouns ( such as him and his ) and R ( eferential ) expressions ( such as John , the man I met yesterday , my sister , etc ) .</sentence>
				<definiendum id="0">BT )</definiendum>
				<definiens id="0">a module of the Government and Binding theory ruling the distribution and the referential properties of anaphors ( such as himself ) , pronouns ( such as him and his ) and R ( eferential ) expressions</definiens>
			</definition>
			<definition id="1">
				<sentence>We will also assume the following functions and predicates : s a function father from N to Nu { _t_ } ; a function siblings from N to '/ ' ( 30 ; a binary predicate agreement , defined on NxN , such that agreement ( n1 , n2 ) =TRUE iff the agreement features of n 1 and n2 are mutually compatible ; a unary predicate pt-antecedent , defined in -N , such that pt-antecedent ( n ) =TRUE iff n is a maximal projection of a head N o ( a Noun ) within T~ .</sentence>
				<definiendum id="0">=TRUE iff n</definiendum>
				<definiens id="0">a maximal projection of a head N o ( a Noun ) within T~</definiens>
			</definition>
			<definition id="2">
				<sentence>The third possibility concerns the so called government across boundaries : when a maximal projection ZP ( or a Small Clause ) is in sisterhood relation with a lexical category X ° , then the latter can govern the specifier position of Ihe former ( or the subjex : t position , in the case of a Small Clause ) .</sentence>
				<definiendum id="0">ZP</definiendum>
			</definition>
			<definition id="3">
				<sentence>X* YP = input-node Z ' Figure 3 Theorem 4 The running times of Algorithms 1 and 2 are given by two functions fAz andfA2 , such thatf4 ~ , fa2e O ( n ) , where n is the length of the sentence under analysis. '</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
			<definition id="4">
				<sentence>Now local-domain-flag=TRUE and domain ( present-node , input-node ) =TRUE ( i.e. IP is the local domain for the input anaphor ) ; therefore Algorithm 1 enters Step 4 and then stops .</sentence>
				<definiendum id="0">input-node ) =TRUE</definiendum>
				<definiens id="0">the local domain for the input anaphor</definiens>
			</definition>
			<definition id="5">
				<sentence>Ingria and Stallard 's static definition of local domain would lead to the conclusion that the pronoun , being dominated by an S type node ( the embedded sentence ) , is free in that category and , thus , could be coindexed outside , for instance with the R-expression John .</sentence>
				<definiendum id="0">S type node</definiendum>
				<definiens id="0">the embedded sentence</definiens>
			</definition>
</paper>

		<paper id="3087">
			<definition id="0">
				<sentence>PC Beta is a PC oriented tool for corpus work in this term 's broadest possible sense .</sentence>
				<definiendum id="0">PC Beta</definiendum>
				<definiens id="0">a PC oriented tool for corpus work in this term 's broadest possible sense</definiens>
			</definition>
			<definition id="1">
				<sentence>One very important feature of PC Beta is that it takes ordinary text files as input and yields ordinary text files as output ; PC Beta is a text processing system , not a database sys~ tern .</sentence>
				<definiendum id="0">PC Beta</definiendum>
				<definiens id="0">that it takes ordinary text files as input and yields ordinary text files as output ;</definiens>
				<definiens id="1">a text processing system , not a database sys~ tern</definiens>
			</definition>
			<definition id="2">
				<sentence>The state condition in a rule is again just a name , now referring to a set of positive integers ( i.e. possible states ) defined under the heading STATESET ( `` state sets '' ) in the rule file , and the condition is fulfilled if the current internal state is a member of that set .</sentence>
				<definiendum id="0">STATESET</definiendum>
				<definiens id="0">a set of positive integers ( i.e. possible states ) defined under the heading</definiens>
			</definition>
</paper>

		<paper id="3011">
			<definition id="0">
				<sentence>A leading idea of the lexicon system is the separation of four different lexicons as follows : DGLEX c c T : X ~E r PFLEX DGLEX is a lexicon of general linguistic definitions of terms .</sentence>
				<definiendum id="0">DGLEX c c T</definiendum>
				<definiens id="0">a lexicon of general linguistic definitions of terms</definiens>
			</definition>
			<definition id="1">
				<sentence>Language-internal idioms ( e.g. , keep tabs in English ) are given in the monolingual lexicons , whereas the other type , which might be called `` transfer idioms '' , are referred m at the level of tnmsfer entries only ( e.g. , have access to , which translates into one Finnish verb ) .</sentence>
				<definiendum id="0">Language-internal idioms</definiendum>
				<definiens id="0">translates into one Finnish verb )</definiens>
			</definition>
			<definition id="2">
				<sentence>( i ) ( d~scuss v slmpleobj-e ) ( 2 ) ( keskustella v sJmpJeob\ ] -ela ) ( 3 ) ( d\ ] scuss ( e ( @ e : :discnss ) ) ( f ( @ f : : keskusteila ) ) tra ) \ [ E : \ [ I , EX : Big CAT : VERB SUBJ : # 3\ [ E : \ [ LEX : I\ [ T CAT : PRON SEM : # 2\ [ ANIM : F HUM : f'\ ] \ ] E : \ [ LEX : SE CAT : PRON CASE : EI , A SEM : # 2\ ] \ ] VCOMP : # 4\ [ E : \ [ LEX : DISCUSS CAT : VERB SUBJ : # 3 PREI ) : \ [ ARGI : # 5\ [ E : \ [ LEX : *NONE*\ ] V : \ [ LEX : *NONE* SEM : \ [ HUM : T\ ] \ ] \ ] ARG2 : # 3 ARG3 : *NONE *\ ] VFORM : PASTPART VOICE : PASS\ ] F : # I0\ [ LEX : KESKUSTELLA CAT : VERB THEMA : # 3 SUBJ : # 5 OBL : # 3 PILED : \ [ ARGI : # 5 ARG2 : # 3 ARG3 : *NONE*\ ] VFORM : FINITE VOICE : PASS\ ] \ ] PRED : \ [ ARGI : # 4 ARG2 : *NONE* ARG3 : *NONE*\ ] VFORM : FIN !</sentence>
				<definiendum id="0">SEM</definiendum>
				<definiendum id="1">PASTPART VOICE</definiendum>
				<definiendum id="2">FINITE VOICE</definiendum>
				<definiens id="0">:discnss ) ) ( f ( @ f : : keskusteila ) ) tra ) \ [ E : \ [ I , EX : Big CAT : VERB SUBJ : # 3\ [ E : \ [ LEX : I\ [ T CAT : PRON SEM : # 2\ [ ANIM : F HUM : f'\ ] \ ] E : \</definiens>
				<definiens id="1"># 4\ [ E : \ [ LEX : DISCUSS CAT : VERB SUBJ : # 3 PREI ) : \ [ ARGI : # 5\ [ E : \ [ LEX : *NONE*\ ] V : \ [ LEX : *NONE* SEM : \</definiens>
				<definiens id="2">PASS\ ] F : # I0\ [ LEX : KESKUSTELLA CAT</definiens>
				<definiens id="3">PASS\ ] \ ] PRED : \</definiens>
			</definition>
			<definition id="3">
				<sentence>1 : Simplified TFS of `` it was discussed '' ( next page ) The entries are from ELEX , FLEX , and TFLEX , respectively , and together they specify the transfer relation between English discus , ; ~d its Finnish equivalent keskustella .</sentence>
				<definiendum id="0">TFLEX</definiendum>
				<definiens id="0">specify the transfer relation between English discus</definiens>
			</definition>
			<definition id="4">
				<sentence>Each of the adjuncts has a unique modifiend ( modif = the modified word ) , # 1\ [ E : 'T ... .. o t ~z , X. mXAM~ LE CAT : NOUN ADJT : # 2\ [ E : \ [ LEX : ADDITIONAL CAT : ADJ PRED : \ [ ARGI : # 1 ARG2 : *NONE ' : ARG3 : *NONE*\ ] ADJT : \ [ E : \ [ CAT : ADV MODIF : # 2\ ] \ ] MODIF : # I\ ] F : \ [ LEX : LISA CAT : NOUN ADJT : \ [ F : *NONE*\ ] MOD IF : # i NUM : SG\ ] \ ] NUM : PL PERS : 3 F : \ [ LEX : ES IMERKKI CAT : NOUN\ ] \ ] Fig .</sentence>
				<definiendum id="0">LISA CAT</definiendum>
			</definition>
</paper>

		<paper id="3102">
			<definition id="0">
				<sentence>Conversation is the root of the tree .</sentence>
				<definiendum id="0">Conversation</definiendum>
				<definiens id="0">the root of the tree</definiens>
			</definition>
			<definition id="1">
				<sentence>A conversation takes place between several participants and it is composed by one or more Dialogues .</sentence>
				<definiendum id="0">conversation</definiendum>
				<definiens id="0">takes place between several participants and it is composed by one or more Dialogues</definiens>
			</definition>
</paper>

		<paper id="2045">
			<definition id="0">
				<sentence>The morphological analyzer analyzes a consecutive sequence of characters and identifies word and phrase boundaries .</sentence>
				<definiendum id="0">morphological analyzer</definiendum>
			</definition>
</paper>

		<paper id="3069">
			<definition id="0">
				<sentence>The system was implemented on VAX/V-MS in Quintus Prolog and consists of the following parts : ( 1 ) The compiler , which takes as its input two-level rules and produces final state automata ( transducers ) .</sentence>
				<definiendum id="0">compiler</definiendum>
				<definiens id="0">takes as its input two-level rules and produces final state automata ( transducers )</definiens>
			</definition>
			<definition id="1">
				<sentence>2 , 349 With this information at its disposal , the input module must , in order to store the entry into the lexicon , do the following : extract form the word its lexical stem ; transcribe it from surface into lexical characters ; determine the continuation lexicon ( s ) ( paradigms ) and lexical alternations ; Extracting the lexical stem and assigning lexical alternations are performed by comparing the ( base and comparative ) word forms entered .</sentence>
				<definiendum id="0">Extracting</definiendum>
				<definiens id="0">the lexical stem and assigning lexical alternations are performed by comparing the ( base and comparative ) word forms entered</definiens>
			</definition>
</paper>

		<paper id="3054">
			<definition id="0">
				<sentence>MORPHOLOGY AND MORPHOSYNTACTIC ( ANALYSIS ) RULES In METAL , morphological analysis is a recursive process of lookup and segmentation that scans input words from left to right in search of their component parts .</sentence>
				<definiendum id="0">MORPHOLOGY AND MORPHOSYNTACTIC ( ANALYSIS ) RULES In METAL</definiendum>
				<definiens id="0">a recursive process of lookup and segmentation that scans input words from left to right in search of their component parts</definiens>
			</definition>
			<definition id="1">
				<sentence>The DEFAULTER system consists of three modules : ( I ) a BASIC module containing languageindependent functions ( like table manipulation , dictionary checking , creating defaulted entries in METAL format , general string manipulation , etc. ) .</sentence>
				<definiendum id="0">DEFAULTER system</definiendum>
				<definiens id="0">consists of three modules : ( I ) a BASIC module containing languageindependent functions ( like table manipulation , dictionary checking , creating defaulted entries in METAL format , general string manipulation , etc. )</definiens>
			</definition>
			<definition id="2">
				<sentence>( `` itGit '' ( NST ( CAN `` * '' ) ( ALO `` * '' ) ( GD F ) ( CL S-0 P-EN ) ( DH DE ) ) ) ) ( `` liJks '' ( AST ( ( CAN `` * '' ) ( ALO `` * '' ) ( CL P-0 P-E ) ) ) ) ( `` ieel '' ( ( NST ( ( CAN `` * '' ) ( ALO `` * '' ) ( GD N ) ( CL S-0 ) ( DH HIT ) ) ) ( AST ( ( CAN `` * '' ) ( ALO `` * '' ) ( CL S-0 ) ( DG SU ) ) ) ) ) ( `` dt '' ( VST ( ( CAN `` -on '' ) ( ALO `` t '' ) ( CL PR-T ) ) ( V-F~X ( 0 ( ( CAN t ) ( ALO T ) ( CL PR-T ) • .</sentence>
				<definiendum id="0">NST</definiendum>
				<definiendum id="1">DH DE ) ) ) ) ( `` liJks</definiendum>
				<definiendum id="2">ALO T )</definiendum>
				<definiens id="0">'' ( AST ( ( CAN `` * '' ) ( ALO `` * '' ) ( CL P-0 P-E ) ) ) ) ( `` ieel '' ( ( NST ( ( CAN `` * '' ) ( ALO `` * '' ) ( GD N ) ( CL S-0 ) ( DH HIT ) ) ) ( AST ( ( CAN `` * '' ) ( ALO `` * '' ) ( CL S-0 ) ( DG SU ) ) ) ) ) ( `` dt ''</definiens>
			</definition>
</paper>

		<paper id="2066">
			<definition id="0">
				<sentence>Lexical semantics : do n't try to seek for the 'real meaning ' of things .</sentence>
				<definiendum id="0">Lexical semantics</definiendum>
			</definition>
</paper>

		<paper id="3015">
			<definition id="0">
				<sentence>Copying ( i ) : Assimilation e.g. 1 ran \ [ hi ran quickly \ [ 9 \ ] Rule : n-~O/ { k , g } \ [ 0\ ] denotes back-of-tongue ( velar ) nasal closure e.g. 2 sandwich \ [ samwitJ'\ ] Rule : n -~ m/ { l ) , b , w etc. } 79 Copying ( ii ) : Ooarticulation e.g. keep \ [ ~_\ ] cool c rt \ [ k\ ] V Rules : k- , +k / -- i-back\ ] V v denotes advanced articulation + denotes lip-rounding denotes retracted articulation Insertion : Epenthesis e.g. mince \ [ mints\ ] pence \ [ pents\ ] Rule : ns -- ~ nts Deletion : Elision e.g. sandwich \ [ sanwitf\ ] Rule : nd -- * n Permutation : Metathesis e.g. burnt \ [ brunt\ ] Rule : ur-+ ru The problems inherent in this approach are many : Se , lsitive grammars undecidable .</sentence>
				<definiendum id="0">Rule</definiendum>
				<definiens id="0">n -~ m/ { l ) , b</definiens>
			</definition>
			<definition id="1">
				<sentence>SRS text-to-phoneme rules : a three-level rule strategy .</sentence>
				<definiendum id="0">SRS text-to-phoneme rules</definiendum>
				<definiens id="0">a three-level rule strategy</definiens>
			</definition>
</paper>

		<paper id="3089">
			<definition id="0">
				<sentence>Bottom-up processing considers the meaning of each individual word mad the modification relationships which may hold between words .</sentence>
				<definiendum id="0">Bottom-up processing</definiendum>
				<definiens id="0">considers the meaning of each individual word mad the modification relationships which may hold between words</definiens>
			</definition>
			<definition id="1">
				<sentence>Natural Language Production as a Process of Decision Making Under Constraint .</sentence>
				<definiendum id="0">Natural Language Production</definiendum>
			</definition>
</paper>

		<paper id="2028">
			<definition id="0">
				<sentence>Lakoff ( 1970 ) proposed a. test where anaphoric so in the second clause of a conjunction refers back to an antecedent containing the lu : for which the question of reading distinction arises , ms in ( 3 ) .</sentence>
				<definiendum id="0">anaphoric</definiendum>
			</definition>
			<definition id="1">
				<sentence>( 8 ) A reading of an lu is a coherent group of senses , the boundaries of which ( 'aunot be crossed by a single occurrence of the lu without losing semantic normality .</sentence>
				<definiendum id="0">reading of an lu</definiendum>
				<definiens id="0">a coherent group of senses , the boundaries of which ( 'aunot be crossed by a single occurrence of the lu without losing semantic normality</definiens>
			</definition>
</paper>

		<paper id="2071">
			<definition id="0">
				<sentence>Similarity evaluators usually employ PDP networks where semantic concepts are internally represented as distributed activation patterns over a set of ~microfeaturcs ' .</sentence>
				<definiendum id="0">Similarity evaluators</definiendum>
				<definiens id="0">usually employ PDP networks where semantic concepts are internally represented as distributed activation patterns over a set of ~microfeaturcs '</definiens>
			</definition>
			<definition id="1">
				<sentence>McClelland &amp; Kawamoto 's ( 1986 ) PDP model learns how : syntactic ( word-order ) cues affect semantic frame/case selection , yielding more principled preference integration .</sentence>
				<definiendum id="0">PDP model</definiendum>
				<definiens id="0">learns how : syntactic ( word-order ) cues affect semantic frame/case selection , yielding more principled preference integration</definiens>
			</definition>
			<definition id="2">
				<sentence>Given a nominal compound ( of arbitary length ) , an intevpretalion is defined as an instantiated construction -- including all the syntactic , semantic , and sub-construction f-structures -- such that the syntactic structure parses the nominal compound , and the semantic structure is consistent with all the ( sub- ) constructions .</sentence>
				<definiendum id="0">intevpretalion</definiendum>
				<definiens id="0">Given a nominal compound ( of arbitary length ) , an</definiens>
			</definition>
			<definition id="3">
				<sentence>The probabilistic goodness metric for an interpretation is defined as follows : the goodness of an interpretation is the probability of the entire construction given the input words of the nominal compound , e.g. , P\ [ +c : ) l + s1 , +82\ ] = P\ [ NN-constrl ( ig ) l `` afternoon '' ( ix ) ^ `` rest '' ( i2 ) \ ] .</sentence>
				<definiendum id="0">probabilistic goodness metric for an interpretation</definiendum>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>Each conceptual object has an associated attribute list ( domains ) with values which instantiate them .</sentence>
				<definiendum id="0">conceptual object</definiendum>
				<definiens id="0">has an associated attribute list ( domains ) with values which instantiate them</definiens>
			</definition>
			<definition id="1">
				<sentence>For example : POSITION Qual~ion Fu~ Bran~ / ~ marketing trade engl~a~'ee~ Icnnician compu~ oase where POSITION is an 9 .</sentence>
				<definiendum id="0">POSITION</definiendum>
				<definiens id="0">an 9</definiens>
			</definition>
			<definition id="2">
				<sentence>v. , ~ , ~ The sy. , ; tem uses morphologic and syntactic analysers and a semantic analysis engine called the `` matching machine '' ( MM ) .</sentence>
				<definiendum id="0">tem</definiendum>
				<definiens id="0">uses morphologic and syntactic analysers and a semantic analysis engine called the `` matching machine '' ( MM )</definiens>
			</definition>
			<definition id="3">
				<sentence>The MM uses functions or `` methods '' which carry out specific treatments according to the type of objects under consideration .</sentence>
				<definiendum id="0">MM</definiendum>
				<definiens id="0">uses functions or `` methods '' which carry out specific treatments according to the type of objects under consideration</definiens>
			</definition>
</paper>

		<paper id="3027">
			<definition id="0">
				<sentence>Ai is a leftmost descendant ( not necessarily the left daughter ) of Bi_l or they are identical for 1 _ &lt; i &lt; d+l. Bi is a rightmost descendant ( not necessarily the right d &amp; v.ghter ) of Ai for 1 G i &lt; d. Thus our model is similar to left-corner parser \ [ 1\ ] , though our discussion is not restricted to parsing .</sentence>
				<definiendum id="0">Ai</definiendum>
				<definiens id="0">a rightmost descendant</definiens>
			</definition>
			<definition id="1">
				<sentence>% Xm_ ~ Z , ,_ a /'- , , NP , ,-1 NP , , V , ,:1 V , ttere Vo is a finite verb and V ; is an infinite verb for 1 &lt; i &lt; n. Vi is a causative verb or a perception verb for 1 &lt; i &lt; n. NPl is the subject of Vi for 0 &lt; i &lt; n , and NPl is an object of V , forn &lt; i &lt; _ m ( m &gt; m ) .</sentence>
				<definiendum id="0">Vo</definiendum>
				<definiendum id="1">NPl</definiendum>
				<definiens id="0">,_ a /'- , , NP , ,-1 NP , , V , ,:1 V , ttere</definiens>
				<definiens id="1">a finite verb and V ; is an infinite verb for 1 &lt; i &lt; n. Vi is a causative verb or a perception verb for 1 &lt; i &lt; n. NPl is the subject of Vi for 0 &lt; i &lt; n , and</definiens>
				<definiens id="2">an object of V , forn &lt; i &lt; _ m ( m &gt; m )</definiens>
			</definition>
</paper>

		<paper id="2007">
			<definition id="0">
				<sentence>It is important to realize that this is a generative process , which goes well beyond the simple matching of features .</sentence>
				<definiendum id="0">generative process</definiendum>
				<definiens id="0">goes well beyond the simple matching of features</definiens>
			</definition>
</paper>

		<paper id="3064">
			<definition id="0">
				<sentence>The output is a list of doublets &lt; p , v &gt; , where p is either a preposition or a syntactic category , apd v the lcxical-semantic translation of the item ( s ) , The \ [ risk of the analyzer is not domain-neutral and t ' ( : t purely grammatical : d~e `` p '' clement of the doublets is in fact filtered and sometimes transformed : prepositions which trar~slate identically end up the same ( but inversely , ambiguities caused Ey plurivocal prepositions arc left for the sc~ : i : antic processor to solve , mainly by the use of domain filtering ) ; a lot of lexicon entries are complex nouns and vet-bai phrases A clause is represented in the output under the form : &lt; ap , x-np , &gt; .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">vet-bai</definiendum>
				<definiens id="0">phrases A clause is represented in the output under the form : &lt; ap , x-np , &gt;</definiens>
			</definition>
</paper>

		<paper id="2054">
			<definition id="0">
				<sentence>h typical phrase consists of a content word followed by some functional morphemes , h Japanese sentence is a sequence of phrases with a structure which can be described by a diagram as in Fig .</sentence>
				<definiendum id="0">h typical phrase</definiendum>
				<definiendum id="1">h Japanese sentence</definiendum>
				<definiens id="0">consists of a content word followed by some functional morphemes</definiens>
				<definiens id="1">a sequence of phrases with a structure which can be described by a diagram as in Fig</definiens>
			</definition>
			<definition id="1">
				<sentence>When a dependency structure X is composed of phrases Xl , X 2 ... .. x n we say that X is a dependency structure on XlX2 ... x n. The set of all the dependency structures on XlX2 ... x n is denoted as K ( XlX2 ... Xn ) : and for a sequence of phrase sets A1 , A 2 ... .. A n , we define KB ( A 1 , A 2 ... .. A n ) = { X\ [ XeK ( XlX2 ... Xn ) , xieh i ( l &lt; i &lt; n ) } . Fig.1 Example of dependency structure in Japanese. A , B ... . are phrases. 311 I For a pair of phrases x 1 and x 0 ' we can think of a penalty imposed on a modifiermodificant relation between x 1 and x 0. This non-negative value is denoted as pen ( xl ; x0 ) . The smaller value of pen ( xl ; x 0 ) represents the more natural linguistic relation. Although it is very important to establish a way of computing pen ( xl ; x0 ) , we will not go into that problem in this paper. Based on the 'local ' penalty , a 'global ' penalty P ( X ) of a dependency structure X is defined recursively as follows \ [ 0zeki 86a\ ] . Definition 2 ( 1 ) For X= &lt; x &gt; , P ( X ) =O. ( 2 ) For X= &lt; Xl ... X n xo &gt; , where Xi= &lt; ... xi &gt; ( I &lt; i &lt; n ) is a dependency structure , P ( X ) = P ( Xl ) + ... +P ( X n ) +pen ( xl ; xo ) + ... ÷pen ( xn ; XO ) . Note that P ( X ) is the sum of the penalty of all the phrase pairs which are supposed to be in modifier-modificant relation in the dependency structure X. This function is invariant under permutation of X 1 ... .. X n in accordance with the characteristic of Japanese. For simplicity , let us begin with a special type of phrase lattice composed of a sequence of phrase sets BI , B 2 ... .. B N as shown in Fig.2 , which we call phrase matrix. Suppose we are given a phrase matrix and a reliability function s : BIUB2U ... UB N -- &gt; R+ , where R+ denotes the set of non-negative real numbers .</sentence>
				<definiendum id="0">x n</definiendum>
				<definiendum id="1">A 1</definiendum>
				<definiendum id="2">x0</definiendum>
				<definiens id="0">a dependency structure on XlX2 ... x n. The set of all the dependency structures on XlX2 ...</definiens>
				<definiens id="1">the more natural linguistic relation. Although it is very important to establish a way of computing pen ( xl ;</definiens>
				<definiens id="2">the sum of the penalty of all the phrase pairs which are supposed to be in modifier-modificant relation in the dependency structure X. This function is invariant under permutation of X 1 ... .. X n in accordance with the characteristic of Japanese. For simplicity , let us begin with a special type of phrase lattice composed of a sequence of phrase sets BI , B 2 ... .. B N as shown in Fig.2 , which we call phrase matrix. Suppose we are given a phrase matrix and a reliability function s : BIUB2U ... UB N -- &gt; R+ , where R+ denotes the set of non-negative real numbers</definiens>
			</definition>
			<definition id="2">
				<sentence>In that case B i is the set of output candidates for the ith utterance , and s ( x ) is the recognition score for a candidate phrase x. For a dependency structure X on a phrase sequence XlX2 ... x N , the total reliability of X is defined as S ( X ) = S ( Xl ) + ... +S ( XN ) .</sentence>
				<definiendum id="0">x )</definiendum>
				<definiens id="0">the recognition score for a candidate phrase x. For a dependency structure X on a phrase sequence XlX2 ... x N , the total reliability of X</definiens>
			</definition>
			<definition id="3">
				<sentence>algorithm Combining two dependency structures X and Y= &lt; YI ... .. Ym , Y &gt; , a new dependency structure &lt; X , Y 1 ... .. Ym , y &gt; is obtained which is denoted as X O V. Conversely , any dependency structure Z with length greater than 1 can be decomposed as Z= X @ Y , where X is the top dependency structure in Z. Moreover , it is easily verified from the definition of the objective function that F ( Z ) = F ( X ) ÷ F ( Y ) ÷ pen ( x ; y ) , where x and y are the last phrases in X and Y , respectively .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">verified from the definition of the objective function that F ( Z ) = F ( X</definiens>
				<definiens id="1">the last phrases in X and Y , respectively</definiens>
			</definition>
			<definition id="4">
				<sentence>A Phrase lattice is a set of phrase sets , which looks like Fig.5 .</sentence>
				<definiendum id="0">Phrase lattice</definiendum>
				<definiens id="0">a set of phrase sets , which looks like Fig.5</definiens>
			</definition>
			<definition id="5">
				<sentence>For l &lt; i &lt; m &lt; j ( N and Xmj p , opt ' ( i , j , m ; p ) =the minimum value of \ [ P ( X ) iS ( X ) \ ] as X runs over all the dependency structures on all the possible phrase sequences beginning at i and ending at j with the last phrase being fixed as Xmj p , and opts ' ( i , j , m ; p ) =the dependency structure which gives the above minimum. Then recurrence equations similar to Proposition 1 and Proposition 1 ' hold for opt ' and opts'\ [ Ozeki 86bJ : Proposition 2 For l ! iJm ! j ! S and lJp &lt; lB ( m , j ) \ [ , ( 1 ) if i=m , then opt ' ( i , j , m ; p ) =S ( Xmjp ) , 313 3 ( 2 ) and if i &lt; m , then opt ' ( i , j , m ; p ) =min { f ' ( k , n , q ) l i &lt; n &lt; k &lt; m-1 , lJqJlB ( n , k ) l } , where f ' ( k , rl , q ) = ept ' ( i , k , n ; q ) ÷opt ' ( k+l , j , m ; p ) ÷pen ( xnkq : Xmjp ) Propo~ ; ition 2 ' For \ [ &lt; i &lt; mi3~N and lJpJIB ( m , J ) l , ( I ) if i=m then opts ' ( i , j , m ; p ) = &lt; Xmjp &gt; , ( 2 ) and if i &lt; m , then opts ' ( i j , m ; p ) =opts ' ( i *k , gn ; gq ) O opts ' ( gk+l , j , m ; p ) , where *k is the best segmentation point , *n is the top position of the best phrase at the segmentation point and *q is the best phrase number in B ( *n , *k ) : ( ~k , $ n , *q ) =argmin { f ( k , n , q ) li &lt; n &lt; k &lt; m-l , lJqJIB ( n , k ) \ [ } . The minimum is searched on 3 variables in this case. It is a straight forward matter to translate these recurrence equations into an algorithm similar to Fig.3 \ [ Ozeki 88b , Kohda 86\ ] . In this case , the order of amount of computation is O ( M2NS ) , where M=IB ( i , j ) I and N is the number of starting and ending positions of phrases in the top layer node ( I,1 ) bottom layer node ( 7,7 ) Fig.6 2-dimensional array of computing elements. lattice. Also , we can modify the algorithm in such a way that up to kth optimal solutions are obtained. When only one processor is available , the amount of computation dominates the processing time. On the other hand , when there is no limit as to the number of processors , the processing time depends on how much of the computation can be executed in parallel. There exists a tidy parallel and layered structure to implement the above algorithm. For simplicity , let us confine ourselves to a phrase matrix case here. Furthermore , let us first consider the case where there is only one element x i in each of the phrase set B i. If we define opt '' ( i , j ) =min { P ( X ) lXeK ( x i ... .. xj ) } then Proposition 1 is reduced to the following simpler form. Proposition 3 For lJiJjJN , ( 1 ) if i=j , then opt '' ( i , j ) =O , ( 2 ) and if i &lt; j , then opt '' ( i , j ) =min { opt '' ( i , k ) iopt '' ( k+l , j ) +pen ( xk ; xj ) \ [ i &lt; k &lt; j-1 } , It is easy to see that opt '' ( i , j ) and opt '' ( i÷m , j÷m ) ( m~O ) can be calculated independently of each other. This motivates us to devise a parallel and layered computation structure in which processing elements are arranged in a 2-dimensional array as shown in Fig.6. There are N ( N+I ) /2 processing elements in total. The node ( i , j ) has an internal structure as shown in Fig.7 , and is connected with node ( i , k ) and node ( k÷l , j ) ( lJk &lt; j-1 ) as in Fig.8. The bottom elements , node ( i , i ) 's ( l &lt; i &lt; N ) , hold value 0 and do nothing else. The node ( i , j ) calculates the value of opt '' ( i , j ) and holds the result in memory i together with the optimal segmentation point in memory 2. Within a layer all the nodes work independently in parallel and the computation proceeds from the lower to upper layer. An upper node receives information about a longer sub-sequence than a lower node : an upper node processes more global information than a lower node. When \ [ . oinio ; zatio. ... x , ' '' ut t on J 0 node ( i÷l.j ' / 0 node ( i+g , J ) 0 node ( i.i ) 0 node ( i.i+l ) memory I o~ut p min ; ut 1 , , L\ ] ~ ~utation of Fig.7 Internal structure of node ( i , j ) . 314 4 e ( i , j ) node ( i , j-1 ) node ( i+l , j ) / \ 1 \ 1 \ node ( i , i+ l ) node ( j-I , j ) dnode ( i , i ) node ( j , j ) ~ Fig.8 Nodes connected to node ( i , j ) . ( 1 , 6 ; 5 ) d : C~ / / `` //C~ '' /3C~\ &gt; 3 2nd ( ~aver/ ( D\\ 'C ) x ( \ ] , i ; ! )</sentence>
				<definiendum id="0">*k</definiendum>
				<definiendum id="1">*n</definiendum>
				<definiendum id="2">*q</definiendum>
				<definiendum id="3">M=IB</definiendum>
				<definiendum id="4">N</definiendum>
				<definiens id="0">the top position of the best phrase at the segmentation point and</definiens>
			</definition>
</paper>

		<paper id="2073">
			<definition id="0">
				<sentence>\ [ Zaharin87b\ ] Zaharin Y. , Thelinguistic s : proach at GETA , TECHNOLOGOS ( langues et artefacts ) , priatemps 1987 , no.4 , LISH-CNRS , Paris , pp.93-110 .</sentence>
				<definiendum id="0">TECHNOLOGOS</definiendum>
				<definiens id="0">proach at GETA</definiens>
			</definition>
</paper>

		<paper id="2026">
			<definition id="0">
				<sentence>; \ [ o Introduction Among currant syntactic theories , Generalized l'hrase Structure Grammars ( GPSG ) \ [ 4\ ] provide an appealing solution for describing natural languages with their modular system of composite categories , rules , constraints and feature propagation principles .</sentence>
				<definiendum id="0">Generalized l'hrase Structure Grammars ( GPSG</definiendum>
			</definition>
</paper>

		<paper id="2063">
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>THALES is a software package for plane geometry constructions , supplied with a natural language interlace .</sentence>
				<definiendum id="0">THALES</definiendum>
				<definiens id="0">a software package for plane geometry constructions , supplied with a natural language interlace</definiens>
			</definition>
			<definition id="1">
				<sentence>The abbreviations ID , OBJ , and ADJ stand lot identitier ( a unique name of an object , denoted by '- ' in THALES ) , object ( the name of an object , possibly a part of another object ) , and adjective , resp .</sentence>
				<definiendum id="0">ADJ stand lot identitier</definiendum>
				<definiens id="0">a unique name of an object , denoted by '- ' in THALES ) , object ( the name of an object , possibly a part of another object ) , and adjective , resp</definiens>
			</definition>
</paper>

		<paper id="2052">
			<definition id="0">
				<sentence>For each language there is a unification grammar that defines a reversible relation between strings and language dependent lneaning representations ( logical forms ) .</sentence>
				<definiendum id="0">unification grammar</definiendum>
			</definition>
			<definition id="1">
				<sentence>A grammar G is reversible for a relation R iff R is reversible and defined by G. For example , a grammar that relates strings to logical forms is reversible if both the parsing and generation problem is computable , and the relation between strings and logical forms is symmetric ; the parsing problem is computable if for a given string all corresponding logical forms can be enumerated by some terminating procedure ; such a procedure should halt if the given string does not have a corresponding logical form .</sentence>
				<definiendum id="0">grammar G</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">reversible for a relation R iff</definiens>
				<definiens id="1">computable , and the relation between strings</definiens>
			</definition>
			<definition id="2">
				<sentence>A relation R C A x B is computable iff for a given a E A the set { b C B\ ] R ( a , b ) } can be enumerated by some terminating procedure .</sentence>
				<definiendum id="0">relation R C A x B</definiendum>
				<definiens id="0">computable iff for a given a E A the set</definiens>
			</definition>
			<definition id="3">
				<sentence>Instead of using unification grammars CRITTER uses logic grammars ; Zajac uses a type system including an inheritance mechanism to define transfer-like rules .</sentence>
				<definiendum id="0">Zajac</definiendum>
				<definiens id="0">uses logic grammars ;</definiens>
				<definiens id="1">uses a type system including an inheritance mechanism to define transfer-like rules</definiens>
			</definition>
</paper>

		<paper id="2055">
			<definition id="0">
				<sentence>SESAME is an industrial product .</sentence>
				<definiendum id="0">SESAME</definiendum>
				<definiens id="0">an industrial product</definiens>
			</definition>
			<definition id="1">
				<sentence>The conceptual schema is a set of specifications which describe the semantic structure of the data base .</sentence>
				<definiendum id="0">conceptual schema</definiendum>
			</definition>
			<definition id="2">
				<sentence>S , ~ ; mantic constraints ( selection restrictions ) are also expressed as features equations .</sentence>
				<definiendum id="0">mantic constraints</definiendum>
				<definiens id="0">selection restrictions ) are also expressed as features equations</definiens>
			</definition>
			<definition id="3">
				<sentence>Unification which is a basic Prolog operation is thus directly and efficiently used .</sentence>
				<definiendum id="0">Unification</definiendum>
			</definition>
			<definition id="4">
				<sentence>Simplified descriptions of a grammar rule and a lexicon entry The linguistic covering of the grammar and the lexicon is the sub-language of data base query , which include the processing of expressions concerning tile sorting of information , comparisons , etc .</sentence>
				<definiendum id="0">query</definiendum>
			</definition>
			<definition id="5">
				<sentence>The translation process keeps trace of the direct link which is set between the conceptual data of the ER-SQL query and the data which will make up the answer when the query is sent to the D.B.MS. This information is kept in the form of conceptual data / data base data mapping rules .</sentence>
				<definiendum id="0">translation process</definiendum>
				<definiens id="0">the D.B.MS. This information is kept in the form of conceptual data / data base data mapping rules</definiens>
			</definition>
			<definition id="6">
				<sentence>ER-SQL is the query language of the THE RESULTS MANAGEMENT ENVIRONMENT The results management environment must be able to take the answers of the D.B.M.S. and to present them in such a form that they can be exploited by the user .</sentence>
				<definiendum id="0">ER-SQL</definiendum>
				<definiens id="0">the query language of the THE RESULTS MANAGEMENT ENVIRONMENT The results management environment must be able to take the answers of the D.B.M.S. and to present them in such a form that they can be exploited by the user</definiens>
			</definition>
</paper>

		<paper id="3052">
			<definition id="0">
				<sentence>The type system encourages an objectoriented approach to linguistic description by providing a multiple inheritance mechanism and an inference mechanism which allows the specitication of relations between levels o\ [ linguistic description defined as classes of objects .</sentence>
				<definiendum id="0">type system</definiendum>
				<definiens id="0">encourages an objectoriented approach to linguistic description by providing a multiple inheritance mechanism and an inference mechanism which allows the specitication of relations between levels o\ [ linguistic description defined as classes of objects</definiens>
			</definition>
			<definition id="1">
				<sentence>l ) where T represents the greatest element ( no information ) and 3_ the smallest element ( inconsistent information , leading to failure in unification ) .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">represents the greatest element ( no information ) and 3_ the smallest element ( inconsistent information , leading to failure in unification )</definiens>
			</definition>
			<definition id="2">
				<sentence>WORD denotes the set of word forms , and the list .</sentence>
				<definiendum id="0">WORD</definiendum>
				<definiens id="0">the set of word forms</definiens>
			</definition>
			<definition id="3">
				<sentence>A SIGN has a phonological value , represented as a list of words , and syntactic and semantic information ( omitted for this comparison ) .</sentence>
				<definiendum id="0">SIGN</definiendum>
				<definiens id="0">a phonological value , represented as a list of words</definiens>
			</definition>
			<definition id="4">
				<sentence>Ex : Lsu cA : UM \ ] Lexical entries AI , L = DET\ [ sYN IIINAD : \ [ LEX : '' aII '' , NUM : pl\ ] \ ] \ ] , MAN = : NOUN\ [ ~YN : IIn~A ) : \ [ bEX : '' man '' , NUM : sg\ ] V \ ] \ ] .</sentence>
				<definiendum id="0">NUM</definiendum>
			</definition>
			<definition id="5">
				<sentence>\ [ 1\ ] Ilassan Ait-Kaci : A Lattice Theoretic Approach 1o Computation Based on a Calculus of Partially Ordered Type Structures , Ph.D .</sentence>
				<definiendum id="0">Ilassan Ait-Kaci</definiendum>
				<definiens id="0">A Lattice Theoretic Approach 1o Computation Based on a Calculus of Partially Ordered Type Structures , Ph.D</definiens>
			</definition>
</paper>

		<paper id="2044">
			<definition id="0">
				<sentence>In an earlier study \ [ 8\ ] , we examined the use of various intonational , syntactic , and orthographic features to distinguish between discourse and sententim readings of a single cue phrase ( 'now ' ) .</sentence>
				<definiendum id="0">phrase</definiendum>
				<definiens id="0">syntactic , and orthographic features to distinguish between discourse and sententim readings of a single cue</definiens>
			</definition>
			<definition id="1">
				<sentence>A pitch accent consists either of a single tone or an ordered pair of tones , such as L*+H. The tone aligned with the stressed syllable is indicated by a star * ; thus , in an L*+H accent , the low tone L* is aligned with the stressed syUahle .</sentence>
				<definiendum id="0">pitch accent</definiendum>
				<definiens id="0">consists either of a single tone or an ordered pair of tones</definiens>
			</definition>
			<definition id="2">
				<sentence>A wellformed intermediate phrase consists of one or more pitch accents , and a simple lfigh H or low L tone that represents the phrase accent .</sentence>
				<definiendum id="0">wellformed intermediate phrase</definiendum>
				<definiens id="0">consists of one or more pitch accents , and a simple lfigh H or low L tone that represents the phrase accent</definiens>
			</definition>
</paper>

		<paper id="2012">
			<definition id="0">
				<sentence>All structures defined by the grammar with that mother at the top and those daughters at the bottom are built : specifically , VP is inserted above the verb and object .</sentence>
				<definiendum id="0">VP</definiendum>
				<definiens id="0">inserted above the verb and object</definiens>
			</definition>
			<definition id="1">
				<sentence>ological issues , CUP , Cambridge , 114-135 Arnold , D , L des Tombe &amp; L Jaspaert ( 1985 ) `` Eurotra Linguistic Specifications Version 3 '' , DG XIII , CEC , Luxembourg Arnold , D , S Krauwer , L des Tombe &amp; L Sadler ( 1988 ) , `` 'Relaxed ' Compositionality in Machine Translation '' , in Second International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages , Carnegie Mellon Univ , Pittsburgh Arnold , D , S Krauwer , M Rosner , L des Tombe &amp; G B Varile ( 1986 ) `` The iC , AL , T Framework in EU ROTRA : A Theoretically Committed Notation for MT '' , in Proceedings of the llth International Conference on Computational Linguistics ( COLING 86 ) , Association for Computational Linguistics Arnold , D , &amp; L Sadler ( 1989 ) `` MiMo : Theoretical Aspects of the System '' , Working Papers in Language Processing 6 , Dept of Language &amp; Linguistics , Univ of Essex Bech , A , &amp; A Nygaard ( 1988 ) `` The E-Framework : A Formalism for Natural Language Proces~ing ' , in Proceedings of the 12th International Confer .</sentence>
				<definiendum id="0">Theoretically Committed Notation</definiendum>
				<definiendum id="1">Nygaard</definiendum>
				<definiens id="0">Methodological Issues in Machine Translation of Natural Languages</definiens>
			</definition>
			<definition id="2">
				<sentence>ence on Computational Linguistics ( COLING 88 ) , Association for Computational Linguistics , 36-39 Isabelle , P ( 1988 ) , `` Reversible Logic Grammars for MT '' , in Second International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages , Carnegie Mellon Univ , Pittsburgh Krauwer , S , &amp; L des Tombe ( 1984 ) `` Transfer in a Multilingual MT System '' , in Proceedings of the lOth International Conference on Computational Linguistics ( COLING 84 ) , Association for Computational Linguistics , 464-467 Leermakers , R , &amp; J Rous ( 1986 ) `` The Translation Method of ROSETTA '' , in Computers and Translation 1 , 169-183 Raw , A , B Vandecapelle , &amp; F van Eynde ( 1988 ) `` Eurotra : An Overview '' , ill b~te~Jace 3 , 5-32 Sharp , tt ( 1988 ) , `` CAT-2 -- Implementing a Form~dism for Multi-Lingual MT '' , in Second International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages , Carnegie Mellon UniL Pittsburgh van Eynde ( 1986 ) `` The interface structure level of representation '' in Multilingua 5 , 1~5-i , ~6 Wanyins ; Jin &amp; R F Simmons ( 1986 ) `` Symmetric Rules for Translation of English and Chinese '' , in Computers and ~'anslation 1 , 153-168</sentence>
				<definiendum id="0">Raw</definiendum>
				<definiendum id="1">Eurotra</definiendum>
				<definiens id="0">Reversible Logic Grammars for MT '' , in Second International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages</definiens>
				<definiens id="1">An Overview ''</definiens>
			</definition>
</paper>

		<paper id="3079">
			<definition id="0">
				<sentence>In our conceptual model each WF is a set of assertions having the form ( W , T , R ) .</sentence>
				<definiendum id="0">conceptual model each WF</definiendum>
			</definition>
			<definition id="1">
				<sentence>Analysis is the process of transforming the text of a given WF into its internal representation .</sentence>
				<definiendum id="0">Analysis</definiendum>
				<definiens id="0">the process of transforming the text of a given WF into its internal representation</definiens>
			</definition>
			<definition id="2">
				<sentence>Synthesis is the process of trans= forming the internal representation of a given WF into a text in a certain NL and according to a certain scheme ( discourse structure ) .</sentence>
				<definiendum id="0">Synthesis</definiendum>
				<definiens id="0">the process of trans= forming the internal representation of a given WF into a text in a certain NL and according to a certain scheme ( discourse structure )</definiens>
			</definition>
			<definition id="3">
				<sentence>The discourse generator scans the internal representation and sorts out the assertions into the predefined sections , whereby some ~ssertions ( e.g. those referring to the whote country ) may fall into more than one section .</sentence>
				<definiendum id="0">discourse generator</definiendum>
				<definiens id="0">scans the internal representation and sorts out the assertions into the predefined sections , whereby some ~ssertions ( e.g. those referring to the whote country ) may fall into more than one section</definiens>
			</definition>
			<definition id="4">
				<sentence>Map generation is the process of transforming the internal representation of a glven NF into a weather map .</sentence>
				<definiendum id="0">Map generation</definiendum>
				<definiens id="0">the process of transforming the internal representation of a glven NF into a weather map</definiens>
			</definition>
</paper>

		<paper id="3002">
			<definition id="0">
				<sentence>Complex constrains can be expressed in a slightly restriced form of first order predicate logic , which makes CLG ( 2 ) well suited for expressing , amongst others , HPSG-style of grammars .</sentence>
				<definiendum id="0">Complex constrains</definiendum>
				<definiens id="0">makes CLG ( 2 ) well suited for expressing , amongst others , HPSG-style of grammars</definiens>
			</definition>
			<definition id="1">
				<sentence>Unification remains tlle sole building operation , under the control of complex constraints .</sentence>
				<definiendum id="0">Unification</definiendum>
				<definiens id="0">remains tlle sole building operation , under the control of complex constraints</definiens>
			</definition>
			<definition id="2">
				<sentence>, xn ) &lt; - &gt; Di , the least fixed point of the function H : PEnv -- &gt; PEnv defined by : H\ [ pi \ ] d ( vl , ... , vn ) = C\ [ Di \ ] d \ [ vi/xi\ ] o_nil where o_nil is the empty object .</sentence>
				<definiendum id="0">o_nil</definiendum>
				<definiens id="0">the empty object</definiens>
			</definition>
			<definition id="3">
				<sentence>A CLG ( 2 ) grammar is compiled by successively compiling type declarations , partial descriptions of phrasal signs , principles , user defined relations and lexical information .</sentence>
				<definiendum id="0">CLG</definiendum>
				<definiens id="0">compiled by successively compiling type declarations , partial descriptions of phrasal signs , principles , user defined relations and lexical information</definiens>
			</definition>
</paper>

		<paper id="2038">
			<definition id="0">
				<sentence>Each node is a type and represents either a concept ( Concept Class node ; CC ) or a sequence of concepts ( Concept Sequence Class node ; CSC ) .</sentence>
				<definiendum id="0">CSC</definiendum>
				<definiens id="0">a type and represents either a concept ( Concept Class node ; CC ) or a sequence of concepts ( Concept Sequence Class node ;</definiens>
			</definition>
			<definition id="1">
				<sentence>Activation Markers ( A-Markers ) are created based on the input of the source language .</sentence>
				<definiendum id="0">Activation Markers ( A-Markers</definiendum>
				<definiens id="0">created based on the input of the source language</definiens>
			</definition>
			<definition id="2">
				<sentence>Prediction Markers ( P-Markers ) are passed along the conceptual and phonemic sequences to make predictions about which nodes are to be activated next .</sentence>
				<definiendum id="0">Prediction Markers</definiendum>
				<definiens id="0">the conceptual and phonemic sequences to make predictions about which nodes are to be activated next</definiens>
			</definition>
			<definition id="3">
				<sentence>Each PMarker carries constraints , cost , and the information structure of the utterance which is built incrementally during parsing .</sentence>
				<definiendum id="0">PMarker</definiendum>
				<definiens id="0">carries constraints , cost , and the information structure of the utterance which is built incrementally during parsing</definiens>
			</definition>
			<definition id="4">
				<sentence>Generation Markers ( G-Markers ) show activation of nodes in the target language , and each contains a surface string , features , cost and an instance which the surface string represents .</sentence>
				<definiendum id="0">Generation Markers</definiendum>
			</definition>
			<definition id="5">
				<sentence>In figure lb , dl is a closed class lexical item s. When a G-Marker hits the first element , a V-Marker on the first element is moved to the third element by passing through the second element which is a closed class item .</sentence>
				<definiendum id="0">dl</definiendum>
				<definiens id="0">a closed class item</definiens>
			</definition>
			<definition id="6">
				<sentence>Suppose each CSC represents a phrase structure nile , then the dynamically organized CSC hierarchy provides productive power so that various types of structures of complex sentences can be generated .</sentence>
				<definiendum id="0">CSC</definiendum>
				<definiens id="0">a phrase structure nile</definiens>
			</definition>
			<definition id="7">
				<sentence>A G-Marker is createxl at LEXT , L containing a surface realization , cost , features , and an instance which the LEXlrc represents ( CI ) .</sentence>
				<definiendum id="0">G-Marker</definiendum>
				<definiens id="0">instance which the LEXlrc represents ( CI )</definiens>
			</definition>
			<definition id="8">
				<sentence>CSC node activation : When a CC can be represented by a phrase or sentence , a CSC node is activated and a 6LEX nodes are a kind of CSC which represent a lexical entry and phonological realization of the word .</sentence>
				<definiendum id="0">CSC node activation</definiendum>
				<definiens id="0">When a CC can be represented by a phrase or sentence , a CSC node is activated and a 6LEX nodes are a kind of CSC which represent a lexical entry and phonological realization of the word</definiens>
			</definition>
			<definition id="9">
				<sentence>Feature aggregation is an operation which combines features in the process of passing up G-Markers so that minimal features are carried up .</sentence>
				<definiendum id="0">Feature aggregation</definiendum>
				<definiens id="0">an operation which combines features in the process of passing up G-Markers so that minimal features are carried up</definiens>
			</definition>
			<definition id="10">
				<sentence>Constraint is a central notion in modern syntax theories .</sentence>
				<definiendum id="0">Constraint</definiendum>
				<definiens id="0">a central notion in modern syntax theories</definiens>
			</definition>
			<definition id="11">
				<sentence>Decision has to be made , for this case , to wait translation until these ambiguities are resolved by encountering a clause which follows the initial clause .</sentence>
				<definiendum id="0">Decision</definiendum>
				<definiens id="0">follows the initial clause</definiens>
			</definition>
			<definition id="12">
				<sentence>Our model also explains contradictory observations by \ [ Bock , 1982\ ] and \ [ Levo elt and Maassen , 1981\ ] because activation of CC nodes ( Ll-iteras ) and LEX nodes ( L2-items ) are separated with some interactions .</sentence>
				<definiendum id="0">LEX</definiendum>
				<definiens id="0">because activation of CC nodes ( Ll-iteras )</definiens>
			</definition>
			<definition id="13">
				<sentence>\ [ Kitano , 1989b\ ] Kitano , H. , `` Hybrid Parallelism : A Case of Speech-to-Speech Dialog Translation , '' In Proceedings of the IJCAI-89 Workshop on Parallel Algorithms for Machine Intelligence , 1989 .</sentence>
				<definiendum id="0">Hybrid Parallelism</definiendum>
				<definiens id="0">A Case of Speech-to-Speech Dialog Translation</definiens>
			</definition>
</paper>

		<paper id="3090">
			<definition id="0">
				<sentence>in Arabic , there is a set of object pronouns which refers to a non-human object : ( t. , ~ , ~ , a ) and this will be denoted by -H. This is a subset of the complete set of pronouns +H , which denotes human and non-human .</sentence>
				<definiendum id="0">-H. This</definiendum>
				<definiens id="0">a subset of the complete set of pronouns +H , which denotes human and non-human</definiens>
			</definition>
			<definition id="1">
				<sentence>-H -H ( b ) Intz-ansitive Verbs : It was found out that the subject number is an additional distinguishing feature for transitive verbs .</sentence>
				<definiendum id="0">-H -H</definiendum>
			</definition>
			<definition id="2">
				<sentence>+H ( s ) and +H ( dp ) denote the sets of singular and dual/plural subjects , respectively .</sentence>
				<definiendum id="0">+H</definiendum>
				<definiens id="0">( s ) and +H ( dp ) denote the sets of singular and dual/plural subjects , respectively</definiens>
			</definition>
			<definition id="3">
				<sentence>By definition +t { ( s ) U +H ( dp ) -H , where U denotes the union of the two feature sets .</sentence>
				<definiendum id="0">U +H ( dp</definiendum>
				<definiendum id="1">U</definiendum>
				<definiens id="0">the union of the two feature sets</definiens>
			</definition>
			<definition id="4">
				<sentence>~I ~I ~I 'l `` ' , ' ; ' i ~ ' , ; l &lt; sf , -i~i I ~ ' , ~ ' , ~ I ' i ' \ [ ' I ' I ~ ' , ~ i i , , al~ , , , , `` -H -- H -- ' { -- -I -- - { -- - { -- H -- H -- -i -- q ... . I ... ... ... ... ... I ~ I ~ \ [ t i `` i ' ' , `` I ' I ~ i I i . , ,I ~u , , : , ~ I ~ I ~ ' , ' { ' I ' \ [ ' i ~ I ! ' , ol , i.. -- - ' -- - ' ... . ' -- - : -- -~ -- -I -'H -'q -- '' , -'H ... ... ... ... .. ~ ~i ~i ~ ~i ~\ [ ; ' , ~i , \ [ , : ~'i : -- - { -- -\ [ -- q -- q -'- ! ... ... . ' ... ... ... .. ' ... . ~ ... ... ... { ' ~ \ [ t I ~ ~ I ~ I ~ ' , I i ~ I I ' , y'l~ , I , i~ TABLE ( 1 ) KEY TO COLUMNS ON MATRIX PARADIGM~ ... ... .. ' , ... ... ... ... ... ' , KEY DESCRIPTION ~1 ' , I &amp; S ~ ~ ~J : ' , ==== ============= \ [ J , ~ , .\ [ I &amp; S ~ ~ ~.~ ' , 1 ' J ~'I ... ... ... ... ... ... ... . { S ! i J l.~ '' i ~\ [ - , ~z , ~t I ISJ ~-~tJ ~I ... ... i ... ... ... ... ... ... ... . \ [ I i ... ... . : , J , . ' , tl c , , , , f \ [ I ~I ' , ' , S I J ~.* , i ~\ [ 4 ... ... { ... ... . I ... ... ... ... ... . { ... ... I ... ... . ' , i ... ... ... ... ... I ... ... I ... ... . il ... ... ... ... ... I 6 MODE for Verbs : Indicative ( I ) : ~'-L ' Subjunc r.ive ( S ) : Jussive ( d ) : ~ CASE for Nouns : Nominative ( N ) : ~ , - ' : Accusative ( A ) : Genitive ( G ) : .~ Person-Number-Gender ( Verbs only ) Prefix Core Subject Pronoun ( for verbs ) Case Ending ( for nouns ) Object Pronoun ( for verbs ) ======================================================= ... ... ... ... ... =============== ... ... ... ... = ... .. I &lt; ... ... ... ... ... ... ... ... .. 6 &gt; &lt; ~ - &gt; II &lt; -- I -- - &gt; ' , &lt; `` 7 - &gt; 1 = ==================================== : ====== ==~===== ============ =============================~==========*= ' , I ' , ; I , i i 1 i i ; I ct~ ' .</sentence>
				<definiendum id="0">Accusative ( A )</definiendum>
				<definiens id="0">Genitive ( G ) : .~ Person-Number-Gender ( Verbs only ) Prefix Core Subject Pronoun ( for verbs ) Case Ending ( for nouns ) Object Pronoun ( for verbs</definiens>
			</definition>
</paper>

		<paper id="3010">
			<definition id="0">
				<sentence>Calzolari , N. , ( 1984 ) , `` Detecting patterns in a I~exical Database '' , Proceedings of the tk International Conference on Computational Linguistics , Stanford ( CA ) , 170-173 .</sentence>
				<definiendum id="0">Detecting</definiendum>
				<definiens id="0">patterns in a I~exical Database ''</definiens>
			</definition>
</paper>

		<paper id="3003">
			<definition id="0">
				<sentence>10 Analysis by Synthesis There is one system for doing computational morphology , specifically for recognizing Turkish , which uses 11 It is a common abbreviatory convention that any pair of idendical segments , e.g. , a : a , can be written simply as a single segment , e.g. , a. So , in these rules the glottal stop character represents the pair : ?</sentence>
				<definiendum id="0">glottal stop character</definiendum>
				<definiens id="0">one system for doing computational morphology</definiens>
			</definition>
			<definition id="1">
				<sentence>\ [ 18\ ] Koskenniemi , Kimmo ( 1983 ) Two-Level Morphology : A General Computational Model for Word-form Recognition and Production .</sentence>
				<definiendum id="0">Two-Level Morphology</definiendum>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>Categories ( nonterminal symbols ) are also expressed by , and are closely related to , types .</sentence>
				<definiendum id="0">Categories</definiendum>
			</definition>
</paper>

		<paper id="3096">
			<definition id="0">
				<sentence>Abstract : This paper describes basic ideas of a parser for HPSG style grammars without LP component .</sentence>
				<definiendum id="0">Abstract</definiendum>
			</definition>
			<definition id="1">
				<sentence>Naturally , different levels of abstraction can be introduced among the rules as well , which allows for capturing different levels of generalization over the linguistic data described .</sentence>
				<definiendum id="0">different levels of abstraction</definiendum>
				<definiens id="0">allows for capturing different levels of generalization over the linguistic data described</definiens>
			</definition>
			<definition id="2">
				<sentence>The main problem consists in the fact that the parser working bottom-up may discover certain features of already parsed ( sub ) structures only later in the parsing process ( so to say , only when it gets `` higher in the tree '' , with regard to the way the parsing proceeds ) .</sentence>
				<definiendum id="0">main problem</definiendum>
				<definiens id="0">consists in the fact that the parser working bottom-up may discover certain features of already parsed ( sub ) structures only later in the parsing process ( so to say , only when it gets `` higher in the tree '' , with regard to the way the parsing proceeds )</definiens>
			</definition>
			<definition id="3">
				<sentence>The transitive closure inductively states that for all triples of categories X , Y , Z such that X is a left corner of Y and Y is a left corner of Z , X is also a left corner of Zo The reflexive closure finishes the picture by saying that any category is a left corner of itself .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">a left corner of Z</definiens>
				<definiens id="1">a left corner of itself</definiens>
			</definition>
			<definition id="4">
				<sentence>/* The following clause describes parsing of a category dominating a non-empty terminal string */ parse ( Goal_Category , \ [ Real_GoalCategory , Structure\ ] , \ [ Word_FormIRest Input_Strlng\ ] , Rest_String ) lexicon ( WordForm , WordFormCategory ) , perestroika ( Word_Form_Category , Real Word_Form_Category ) , is a left corner ( \ [ Real_WordFormCategory , d trs=\ [ \ ] \ ] , \ [ Real_Goal Category , Structure } , Rest_Input_String , Rest_String ) , assertz ( already_parsed ( Goal Category , \ [ Real_Goal Category , Structure\ ] , \ [ Word_FormlRest_Input_String\ ] , Rest_String ) ) .</sentence>
				<definiendum id="0">WordForm</definiendum>
				<definiens id="0">a left corner ( \ [</definiens>
			</definition>
			<definition id="5">
				<sentence>where the `` RESULT '' is the only output argument , namely , the resulting structure of the parse , the TOPMOST CATEGORY Is a skeleton category ( represented as a `` usual '' Prolog llst ) of the expected result ( most often , something like `` \ [ cat=sentence\ ] `` or `` \ [ cat=v , bar=two\ ] '' etc. ) , i.e. a category which is expected to unify with any result of the parse , the INPUT STRING is the input string represented as a Prolog list of wordforms and the empty string `` \ [ \ ] '' is the expected rest of the input string after the parsing process finished .</sentence>
				<definiendum id="0">RESULT</definiendum>
				<definiendum id="1">INPUT STRING</definiendum>
				<definiens id="0">the input string represented as a Prolog list of wordforms and the empty string</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>Note that COMMUNAL is an interactive system , rather than one that only generates monologue .</sentence>
				<definiendum id="0">COMMUNAL</definiendum>
				<definiens id="0">an interactive system</definiens>
			</definition>
			<definition id="1">
				<sentence>( b ) The realization rules , which turn the selection expressions ( or 'bundles ' ) of semantic features that are the output from passes through the system networks into quite relatively richly labelled syntactic structures and into the entities at their terminal nodes , i.e. items ( grammatical and lexical ) and markers of punctuation or intonation .</sentence>
				<definiendum id="0">realization rules</definiendum>
				<definiens id="0">turn the selection expressions ( or 'bundles ' ) of semantic features that are the output from passes through the system networks into quite relatively richly labelled syntactic structures and into the entities at their terminal nodes , i.e. items ( grammatical and lexical ) and markers of punctuation or intonation</definiens>
			</definition>
			<definition id="2">
				<sentence>ROLES ( realized in Adjuncts ) ; the handling of MANNER ADVERBS in the same network as ADJECHVES , with provision for COMPARATIVE and SUPERLATIVE meanings and forms , both regular and all types of irregular forms ; a representative range of TIME ADVERBS ; a full range of irregular verbs and noun plurals ; complex NOMINAL GROUPS with provision for mass and count nouns , for NUMBER , for appropriately sequenced multiple pre-head MODIFIERS ( which may themselves be filled by structures such as `` fairly rich '' ) , for three types of determiner selection , as in `` five of the biggest of those apples '' , ( i.e. ( a ) selection by QUANTIFICATION , including weak `` one '' and `` a ( n ) '' , ( b ) selection by SUPERLATIVISATION , and ( c ) selection by one of three types of DEIXIS , realized in `` the '' , demonstratives and possessives ) , and for post-head QUALIFIERS filled by prepositional groups ( with clauses as qualifiers ( i.e. 'relative clauses ' ) currently being added ) ; a full range of PRONOUNS ( personal , possessive , demonstrative and indefinite ) and PROPER NOUNS ( names of individuals , with their own quite complex internal grammar , names of social groups , names of places ) ; complex genitive constructions , e.g. `` the new doctor 's car 's door '' ; PREPOSITIONAL GROUPS , with a representative range of PREPOSITIONS ; a wide range of TEMPERING items for use with adjectives and adverbs ( with embedded groups and clauses as in `` bigger than him/it used to be '' currently being covered ) ; 'special ' grammars of dates , addresses and human proper names ; and much else .</sentence>
				<definiendum id="0">ROLES</definiendum>
				<definiendum id="1">PREPOSITIONAL GROUPS</definiendum>
				<definiens id="0">personal , possessive , demonstrative and indefinite ) and PROPER NOUNS ( names of individuals , with their own quite complex internal grammar , names of social groups , names of places</definiens>
			</definition>
</paper>

		<paper id="3063">
			<definition id="0">
				<sentence>The use of selectional constraints is one of the most popular methods in applying semantic information to the resolution of ambiguities in natural languages .</sentence>
				<definiendum id="0">selectional constraints</definiendum>
			</definition>
</paper>

		<paper id="2061">
			<definition id="0">
				<sentence>The type &lt; emposition is the process that specifies the internal variables of~ gradually. We introduce here the notion of 'most general unifier ( mgu ) ' by Milner \ [ 6\ ] , which combines two type expressions and the result becomes a set of variable instantiations which were contained in dmse type expressions. Let us consider an example of concatenating seem with +ed ( swim ) . First , assume that we can infer the type of +ed ( swim ) to be ~1 from F by a variable instantiation 0 , as is shown below in ( 3 ) : r0 ~ +ed ( s~im ) : V'~ ( 3 ) As for seem : ext , because 'ext ' was a mnemonic for a certain modifier of a verb phrase , we can put ext = ( ~1 -~ 5~ ) . Hence , for a new type variable 9a2 , set r/1 as ( 4 ) : ~q , = mgu ( extO , W -4 p~ ) ( 4 ) Note that we have not specified the contents of 7 h yet ; we will give them later. We can now infer the type of combined verb phrase with ( 4 ) , as is shown in fig. 4 where we can use a canonical expression ( see \ [ 5\ ] ) Ax.seem ( x ) instead of seem. \ ] - '' /1 ~ seem : extT } l r0/\ ] l \ [ +ed ( swim ) : qCl~h ( _+ E ) F'igure 4 : inference of seem ( +ed ( swim ) ) \Ve use '~ ' to denote that the type of the left-hand side is more instantiated than that of the right-hand side , so that : In order to analyze the contents of 0 in ( 3 ) in detail , we need to clarify what tensel actually does. We will argue what each mnemonic means in this subsection. Concretely saying , we will specify what kind of internal variables can occur in various verb types. Our presupposition is that a 'full-fledged ' complex verb contains 'external part ' as well as 'internal part'. So that a verb type ~ , is assumed to have two internal type variables ~oi , ,t and ~oCxt. Each of them has a 'head ' verb which incurs tense or negation if exists. We call a compound of tense and negation 'cap'. As for a cap , we assume a construction of : negation ( tense ( X ) ) a As a result , our full-fledged verb type becomes as fig. 5. aThis formalization is actually after the consideration of the following generation process. When we construct a past and negation of 'walk ' : walk ~ walked - , did n't walk is less natural than walk ~ do n't walk - , did n't walk becanse it is 'do n't ' which incurs the operation of past. The exactly sinfilar thing happens also in Japanese. 3 355 neg -- ~ tense neg -- ~ tense cape eapi / / 0 -- ,0- , O O head~ headl root external internal Figure 5 : complex verb structure Let us get back to the type inference of +ed ( swim ) here , to see the basis of our formalism of type unification. +ed is of type tensei , and swim is of type root that was a mnemonic for simple T. 0 in ( 3 ) becomes as follows : 0 = mgu ( tensei , root ~ ~a ) Because tensei itself is not a function , it must be qualified as of type capl to act as a function by itself. We call this qualification 'promotion ' , to mean that the component raises its type to connect with others. The similar thing can be said for root , which must be promoted to ~nt SO as to be in the domain of cap~. Fig. 6 depicts the type promotion. tense ; root J. J. promote capi\ [ ¢/negl\ ] p , ~ , \ [ , 'oot/head , \ ] % / capdPint ) .~ promote I 4 , q~ , ,\ [ root\ [ ¢ /neg , 1/tense\ ] / head , \ ] promote ~\ [ ¢/c2~xt , root\ [ C/ne9 , 1/ tense\ ] / headi\ ] Figure 6 : type promotion A unifier , or what we have called a set of instantiation so far , like 0 or '7 is exactly a set of promotions where some missing verb elements ( compared with fullfledged type ) are ignored or replaced by other elements. For example , the contents of 0 becomes as follows : 0 = \ [ ¢/negl , tensei/capl , root/headi , he d , l , int , In the type inference of fig. 4 , we happened to choose two items of seer. and +ed ( suim ) . Actually , we can combine any two items picked up from F. In this subsection we will show an example , in which we try to concatenate the consequence of fig. 4 with progressive : int. In the conventional generation , an internal verb element such as 'progressive ' must be concatenated to the structure , prior to an external element such as 'seem'. However , in our type expression , 'be +ing ' can join tim 'seem ( +ed ( swim ) ) ' structure , and also correctly choose the target element 'swim ' from the structure , instead of 'seem ' which exists most exteriorly. If there is such a set of instantiation 772 that : 712 = mgu ( intOth , ~Th ~ ~3 ) ( 5 ) then we can validate the inference in fig. 7. However , because the domain of int must be 'cap'-less ~i , t , we can not legalize the type inference of fig. 7 immediately. What we are required to do now is a type 'demotion ' , as opposed to the promotion. Roughly saying , a verb type of seem ( +ed ( swim ) ) is regarded as below : ( 6 ) though each of the right hand side of ( 6 ) is promoted to some qualified type. This means that the history of promotion must be included in the unifier 0. Hence , we can scrutinize the contents of 0 so that we may find where int can be embeddable. In this case , { r oot / headi , headl/ ~p i~t } in 0 ( viz. ~i~t ~ int ) should be demoted , and we can redefine a verb type of ( 6 ) as : ten e , ( root ) ) -- , tens , ( .d '' ' ( root ) ) ) Suppose that r/a replaces the history of promotions and demotions as below : ( F U { ~9 : i , lt } ) 0~71r/3 F seem ( +ed ( swim ) ) : ~4 then we can make the inference in fig. 8. In this case , { r U q~ : int } O , h tl3 fseem ( +ed ( swim ) ) : ~4 , ... ... ... ... ~-~ I ) \ [ '0rl , rl3 I~.seem ( +ed ( ~o ( swim ) ) ) : int , \ ] 3 -~ ~4 Figure 8 : inference of A-abstraction there is only a place for int to be embedded in between +ed and swim , and int operates upon a root. This time , we can make an inference from abstracted verb structure , as is shown in fig. 9. We mentioned that another superiority of type formalism is its partiality. Actually we can compose a verb stru'cture from any part of given interlingua set , and this feature is realized dynamically. The computation of verb complex generation may be stopped any time by ill-foundedness of machine translation system. Even in such a case , our formalism can offer a part of surface structure which had been partially completed so far. The type definition above gives us an important clue to how to compose verb elements. The algorithm necessarily becomes as follows : 4 356 FOr/ , 1seem ( +ed ( sw£m ) ) : ~2r/ , I '' Iprogressive : int ( __+ E ) V @ lr/2 b progressive ( seem ( +ed ( swim ) ) ) : ~o3r12 Figure 7 : ill-founded type inference F b +ed : tensei F b swim : root F0 b +ed ( swim ) : ~1 FF seem : ext F IA &lt; p.seem ( ( p ) : ¢Pl -9 ~2 F0r\ ] I lseem ( +ed ( swim ) ) : ~2r/1 F0rhr/s ? A~ , int.seem ( +ed ( ~ ( swim ) ) ) : intu3 -- , ~4 F K prog : int F0r/lr/3 1seem ( +ed. ( prog ( swim ) ) ) : ~4 Figure 9 : inference with A-calculus applied modal function. applied modal flmction. The position shift of tense , caused by the arrival of a new internal verb , is diagramed in fig. 10. Fig. 11 English ( Vp new-root . . ( tense ) .. ( vcomp .. e.. ) k______j JRpanese ( vp .. ( w : omp ..e.. ( new-vcomp .. ( tense ) .. ) ) ) q___ __9 Figure 10 : verb position shift shows that every step in the derivation process can offer the partial syntactic tree. We have shown a model of complex verb translation based upon type theory , in which verb elements in the interlingua are regarded as generation functions whose domain and range are elucidated so that each verb element is certified to acquire a correct position in the target structure. Furthermore , the flexibility of type calculus was shown on the following two points. every time , so that all the information a type owns can always be regarded as partial. This means that we can translate partially what can be done. in the type expression , the verb structure can be composed in a way of self-organization , in tile meaning that tile structure is able to be decomposed and to be reorganized in the process. In the case of complex verb translation , rephrasing to another part of speech is rather easier ; we only need to 'kick out ' the functional expression from the verb structure to point to another lexical item. Some external expressions must be translated into a complex sentence as the feature of externality , as we have discussed in section 1.3. We give here a definition of the identity in meaning between an external verb and its corresponding complex sentence in the type-theoreticM view as ( 7 ) : &lt; agent , qa ( v ) &gt; : ( 7 ) .</sentence>
				<definiendum id="0">emposition</definiendum>
				<definiens id="0">the process that specifies the internal variables of~ gradually. We introduce here the notion of 'most general unifier</definiens>
				<definiens id="1">+ed ( swim ) ) \Ve use '~ ' to denote that the type of the left-hand side is more instantiated than that of the right-hand side</definiens>
				<definiens id="2">a 'full-fledged ' complex verb contains 'external part ' as well as 'internal part'. So that a verb type ~ , is assumed to have two internal type variables ~oi , ,t and ~oCxt. Each of them has a 'head ' verb which incurs tense or negation if exists. We call a compound of tense and negation 'cap'. As for a cap</definiens>
				<definiens id="3">a set of instantiation so far , like 0 or '7 is exactly a set of promotions where some missing verb elements ( compared with fullfledged type ) are ignored or replaced by other elements. For example , the contents of 0 becomes as follows : 0 = \ [ ¢/negl , tensei/capl , root/headi , he d , l , int</definiens>
				<definiens id="4">ten e , ( root ) ) -- , tens , ( .d '' ' ( root ) ) ) Suppose that r/a replaces the history of promotions</definiens>
				<definiens id="5">offer a part of surface structure which had been partially completed so far. The type definition above gives us an important clue to how to compose verb elements. The algorithm necessarily becomes as follows</definiens>
				<definiens id="6">inference with A-calculus applied modal function. applied modal flmction. The position shift of tense , caused by the arrival of a new internal verb</definiens>
				<definiens id="7">verb position shift shows that every step in the derivation process can offer the partial syntactic tree. We have shown a model of complex verb translation based upon type theory , in which verb elements in the interlingua are regarded as generation functions whose domain and range are elucidated so that each verb element</definiens>
				<definiens id="8">a definition of the identity in meaning between an external verb and its corresponding complex sentence in the type-theoreticM view as ( 7 ) : &lt; agent</definiens>
			</definition>
			<definition id="1">
				<sentence>~ &lt; affent , `` u &gt; , ~ &gt; '' : O'comp where we denote a sentence by &lt; agent , action ( state ) &gt; informally , and cr is a type of sentence and cr~o~ v is a type of complex sentence .</sentence>
				<definiendum id="0">cr</definiendum>
				<definiens id="0">a sentence by &lt; agent , action</definiens>
				<definiens id="1">a type of sentence and cr~o~ v is a type of complex sentence</definiens>
			</definition>
</paper>

		<paper id="3038">
			<definition id="0">
				<sentence>I Unit &lt; s9 ) , , i I HL2 '' I o , ° '' ~ lu'it~06 ) I `` I .~i I .. . LU'it &lt; lm I '1 Ifiput I '. '' Input ... . i U nits ( 89 ) I Units ( 89 ) : { N-3 ) th Word `` ( N-2 ) th Word ( N-1 ) th Word ! 4-gram Trigram Bigram network Fig.3 NETgram ( Trigram , 4-gram Network ) grams increases , every new input block produced is fully connected to the lower hidden layer of one basic Bigram network. The link weight is set at wt ' as shown in Fig.3. When expanding from Trigram network to 4-gram network , one lower hidden layer block is added and the first and second input blocks are fully connected to one lower hidden layer block , and the second and third input blocks are fully connected to the other lower hidden layer block. Ilow to train a NE'Pgram , e.g. a Trigram network , is shown in Fig.4. As input data , word categories in the Brown Corpus text\ [ 5\ ] are given , in order , from the first word in the sentence to the last , In one input block , only one unit corresponding to the word category number is turned ON ( 1 ) ; The others are turned OFF ( 0 ) . As output data , only one unit corresponding to the next word category number is trained by ON ( 1 ) . The others are trained by OFF ( 0 ) . The training algorithm is the Back-Propagation algorithm\ [ 6l , which uses the gradient descent to change link-weights in order to reduce the difference between the network output vectors and the desired output vectors. First , the basic Bigram network is trained. Next , the Trigram networks are trained with the llnk weight values trained by the basic Bigram network as initial values. This task is a many-to-many mapping problem. Thus , it . is difficult to train because the updating direction of the link weight vector easily fluctuates. In a two-sentence .~ko. o°°° 0 1 0 i O ... ... 0 ... . O ! oo , pot ~_1 ... ... .. 5~ ... .. J ! 9_~ Layer Hidden Layers L_ ! ... ... .. _s3 ... .. 8_9_2 L_t ... ... ... .. 79___8_9_2 Layer ~. ..~ • , o , , , 'l i _..d Fig.4 ttow to Train NETgram ( Trigram Model ) 214 training experiment of about 50 words , we have confirmed that the output values of tbe basic Bigram network converge on the next occurrence probability distribution. ttowever , for many training data , considerable time is required for training. Therefore , in order to increase training speed , we use the next werd category occurrence probability distribution calculated for 1,024 sentences ( about 24,000 words ) as output training data in the basic Bigram network. Of course , in Trigram and 4-gram training , we use the next one-word category as output training data. Word category prediction results show that NETgrmn ( the basic Bigram network ) is comparal ) le to the statistical Bigram model. Next , we consider whether the hidden layer has obtained some linguistic structure. We Calculated the similarity of every two lower hidden layer ( HIA ) output vectors for 89 word categories and clustered them. Similarity S is calculated by ( M ( Ci ) , M ( Cj ) ) S ( ci , cj ) = ( 4.1 ) I1 M ( Ci ) it II M ( Qi ) I1 where MfOi ) is the lower hidden layer ( ILL1 ) output vector of the input word category CL ( M ( Ci ) , M ( C~\ ] ) ) is the immr product of M ( Ci ) and M ( Cj ) . It M ( Ci ) II is the norm of M ( Ci ) . The clustering result is shown in Fig.5. Clustering by the threshold of similarity , 0.985 , the word categories are classified into linguistically significant groups , which are the HAVE ' , verb group , BE verb group , subjective pronoun group , group whose categories should be before a noun , and others. Therefore the NETgram can learn linguistic structure naturally. Word category prediction results are shown in Fig.6. The NETgram ( Trigram network ) is comparable to the statistical Trigram model for test data. Furthermore , the NETgram performs effectively for unseen data whicil never appeared in the training data , although the statistical Trigram can not predict the next word category for unseen data. That is to say , NETgrams interpolate sparse training data in the same way deleted interpolation \ [ 71 does. CA'FEGORY `` 3 '' g3qV~ 37 HVD 40 EIV7 16 BEDZ 20 BER 21 BEZ 19 BEN 14 BE 17BEG 38 I-IVG ~ffPPS-86 WPS 67 PPSS `` 2gD-r -- 45 JJT 58 OD 42 JJ 48 NN $ 61 PP $ 52 NP $ 13 AT 78 VB 80 VBG 06 , 1lAP 22 CC 09 ABN 10 ABX 75 RP 81 VBN 89 DUM 23 CD 43 J JR 47 NN noun , single ) 55 NR lome , west 79 VBD ( verb , past ) 32 VSZ ( verb , -s , -es ) ~2 DTS these 55 PPO me , him , it 19 NNS ( noun , plural ) t0 RB ( adverb ) ~0NNS $ men 's } 1 N PP ATR , Tom ~thers others EXAMPLE Threshold of Similarity ( part of speech ) 1.000 0.995 0.990 0.985 0.980 had -- -1 J : has : \ -- -- ~-T_Y2-L _ ~_L_ -WaS ... ... .. / ! are ... ... . ~ i ' een Q ? ) i bei.~ ... ... . ~ j h~ ... ... ... ... ... ... 7 fie , Jt ... ... ... .. ~ whe , whicb ... ... ... ... .. ~ I ; I , we , thev ... ... .. ~ ; biggest ... . ~ /I I F F l ; first , 2nd ... . Jill| I \ [ i ( adjective ) ... ... .. 1 I I t t |i dog 's -- -- -ATR 's ... ... . 1 I I I ~ I ! a the ... ... . ~_ I I ; ! ( verb , base ~ -- t \ ] \ [ ; ( ~erU. ~.g ) -2 _~ i i i \ ] many , next ... . ~ ' , i and , or -- ~ ! ! half , all ~I ~ both -I I about , off ~-i ( verb , -ed ) ~_~ i ( dummy ) one , 2 I -- t ' , comp.adj. ) Prediction Rate ... ... ... ... . C ) NETgram for test data Statistical Model for test data data as Trigram data. I_ Fig.5 Clustering Result of iliA Output Vectors of NETgram ( Bigram ) l 1 l__J 0 \ ] 2 3 4 5 Number of Prediction Candidate Categories Fig.6 NETgram ( Trigran0 Prediction Rates 215 3 N ETgram We discuss differences between two approaches , the conventional statistical model and the NETgraln. The conventional statistical model is based on the table-lookup. In the case of the Trigram nlodel , next appearance probabilities are computed frmn the histogram , counting the next word category for the two word categories in the training sentences. The probabilities are put in an 89*89*89 size table. Thus , the 89 appearance probabilities of the next word category are obtained from the 89*89*89 size table using the argument of 89*89 symbol permutation. B 89.s9 -- - &gt; R 89 B ; binary space R ; real space In order to get 89 prediction values for the next word category , the trained NETgram procedure is as follows : First , encode from an 89*89 symbol permutation to a 16-dimensional analogue code .</sentence>
				<definiendum id="0">training algorithm</definiendum>
				<definiendum id="1">MfOi )</definiendum>
				<definiens id="0">comparable to the statistical Trigram model for test data. Furthermore , the NETgram performs effectively for unseen data whicil never appeared in the training data</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , the NETgram interpolates sparse training data in the process of bigram and trigram training .</sentence>
				<definiendum id="0">NETgram</definiendum>
				<definiens id="0">interpolates sparse training data in the process of bigram and trigram training</definiens>
			</definition>
			<definition id="2">
				<sentence>S'l'ij ( 55 ) where a~ is the weighting parameter to adjust the scaling of two kinds of likelihood .</sentence>
				<definiendum id="0">a~</definiendum>
				<definiens id="0">the weighting parameter to adjust the scaling of two kinds of likelihood</definiens>
			</definition>
			<definition id="3">
				<sentence>Jetinek , `` Continuous Speech Recognition by Statistical Methods '' , Proceedings of the iI'~EI'\ ] , Vol.64 , No.4 ( 1976.4 ) 121 K.Shikano , `` hnprovenmnt of Word Recognition Result by Trigram Model '' , \ [ CASSP 87 , 29.2 ( 1987.4 ) \ [ al T.J.Sejnowski , C.R.Rosenberg , `` NETtalk , A Parallel Network that l , earns to Read Aloud '' , Teeh .</sentence>
				<definiendum id="0">Jetinek</definiendum>
				<definiendum id="1">NETtalk</definiendum>
				<definiens id="0">A Parallel Network that l , earns to Read Aloud '' , Teeh</definiens>
			</definition>
</paper>

		<paper id="3092">
</paper>

		<paper id="2039">
			<definition id="0">
				<sentence>These formalisms were applied in the field of natural language processing and , based on these formalisms , ~ : ~ystems such as machine translation systems were developed \ [ l &lt; ol ; u , e et a l 8gJ. In such unification-based formalisms , feature ~trueture ( FS ) unification is the most fundamental and ..~ignifieant operation. The efficiency of systems based on ..~uch formalisms , such as natural language analysis and generation systems very much depends on their FS ~lnifieatlon efficiencies. Tiffs dependency is especially crucial for lexicon-driven approaches such as tlPSO\ [ Pollard and Sag 861 and JPSG\ [ Gunji 871 because rich lexieal information and phrase structure information is described in terms of FSs. For example , a spoken Present. affiliation : Infi ) rmation Science Research 1 , aboratory , NTT Basic Research i.aboratories. lh'esenl , address : 9 11 , Midori cho 3-theme , Musashino-shi , Tokyo 180 , Japan. Japanese analysis system based on llPSG\ [ Kogure 891 uses 90 % 98 % of the elapsed time in FS unification. Several FS unificatioa methods were proposed in IKarttunen 86 , l'ereira 85 , Wroblewski 871. These methods uses rooted directed graphs ( DGs ) to represent FSs. These methods take two DGs as their inputs and give a unification result DG. Previous research identified DG copying as a significant overhead. Wroblewski claims that copying is wrong when an algorithm copies too much ( over copying ) or copies too soon ( early copying ) . Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever , the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification , or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths. In natural language processing , such cases are ubiquitous. I '' or example , in unifying an FS representing constraints on phrase structures and an FS representing a daughter phrase structure , such eases occur very h'equent , ly. In Kasper 's disjunctive feature description unification \ [ Kasper 861 , such cases occur very h'equently in unifying definite and disjunct 's definite parts. Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency. I ) eveloping a method which avoids memory wastage is very important. Pereira 's structure sharing FS unification method can avoid this problem. The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721. The method uses a data structure consisting of a skeleton part to represent original information and an environment part to represent updated information. 3'he skeleton part is shared by one of the input FSs and the result FS. Therefore , Pereira 's method needs relatively few new structures when two input FSs are difference in size and which input is larger are known before unification. However , Pereira 's method can create skeletonenviromnent structures that are deeply embedded , for example , in reeursively constructing large phrase structure fl'om their parts. This causes O ( log d ) graph node access time overhead in assembling the whole DG 223 from the skeleton and environments where d is the number of nodes in the DG. Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure , but this prevents structure sharing. This paper proposes an FS unification method that allows structure sharing with constant m'der node access time. This method achieves structure sharing by introducing lazy copying to Wroblewski 's incremental copy graph unification method. The method is called the lazy i2 ! cremental copy IFaph unification reel , hod ( the LING unifieation method for short ) . In a natural language proeessing system that uses deelarative constraint rules in terms of FSs , FS unification provides constraint-checking and structurebuilding mechanisms. The advantages of such a system include : ( 1 ) rule writers are not required to describe control infimnation such as eonstraiut application order in a rule , and ( 12 ) rule descriptions can be used iu different processing directions , i.e. , analysis and general , ion. However , these advantages in describing rules are disadvantages in applying them because of tt~e lack of control information. For example , when constructing a phrase structure from its parts ( e.g. , a sentence fi'om a subject NP and VP ) , unueeessary computation can be reduced if the semantic representation is assembled after checking constraints such as grammatical agreements , which can fail. This is impossible in straightforward unification-based formalisms. In contrast , in a procedure-based system which uses IF-TItEN style rules ( i.e. , consisting of explicit test and structure-building operations ) , it is possible to construct the semantic representation ( TIIEN par'g ) after checking the agreement ( IF part ) . Such a system has the advantage of processing efficiency but the disadvantage of lacking multi-directionality. In this paper , some of the efficiency of the procedurebased system is introduced into an FS unification-based system. That is , an FS unification method is proposed that introduces a strategy called the e_arly failure £inding strategy ( the EFF strategy ) to make FS unification efficient , in this method , FS unification orders are not specified explicitly by rule wril.ers , but are controlled by learned information on tendencies of FS constraint application failures. This method is called the strategic ij ! ~crementaI copy graph unification method ( the SING unification method ) . These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method ( the SLING unification method ) . Section 2 explains typed feature structures ( TFSs ) and unification on them. Section 3 explains a TFS unification method based on Wroblewski 's method and then explains the problem with his method. The section also introduces the key idea of the EFF strategy wlfich comes from observations of his method. Section 3 and 4 introduce the LING method and the SING method , respectively. Ordinary FSs used in unification-based grammar formalisms such as PAT\ ] . { \ [ Shieher 851 arc classified into two classes , namely , atomic leSs and complex FSs. An atomic FS is represented by an atomic symbol and a complex FS is represented by a set of feature-value pairs. Complex FSs are used to partially describe objects by specifying values for certain features or attributes of described objects. Complex FSs can have complex FSs as their feature values and can share certain values among features. For ordinary FSs , unification is defined by using partial ordering based on subsumption relationships. These properties enable flexible descriptions. An extension allows complex FSs to have type symbols which define a lattice structure on them , for example , as in \ [ Pollard and Sag 8 '' 11. The type symbol lattice contains the greatest type symbol Top , which subsumes every type symbol , and the least type symbol Bottom , which is subsumed by every I.ype symbol. An example of a type symbol lattice is shown in Fig. 1. An extended complex FS is represented by a type symbol and a set of feature-value pairs. Once complex IeSs are extended as above , an atomic FS can be seen as an extended complex FS whose type symbol has only Top as its greater type symbol and only Bottom as its lesser type symbol and which has an empty set of feature value pairs. Extended complex FSs are called typed feature structures ( TFSs ) . TFSs are denoted by feature-value pair matrices or rooted directed graphs as shown in Fig. 2. Among such structures , unification c 'm be defined IAP , Kaci 861 by using the following order ; ATFS tl is less than or equal to a TFS t2 if and only if : • the type symbol of tl is less than or equal to the type syn'bol of/2 ; and • each of the features of t2 exists in t1 and. has as its value a TFS which is not less than its counterpart in tl ; and each of the coreference relationships in t2 is also held in tl. Top Sign Syn Head List POS Lexical Phrase Slgn /77 Sign NonEmpty Empty V N P ADV Li. Lis~ ust I I I I NonEmpty Emply I I i I Sign Sign I I/ / List List 5/ /5 ... . U_ Bottom Figure 1 : Exainple of a type symbol lattice 224 -- 2-TI peSymb°10 eaturel TypeSymboll \ ] \ ] \ ] I feature2 TypeSymbol2 I feature3 ? Tag T ypeSymbol3 \ ] \ ] feature4 TypeSymbol4 L \ [ .feature5 TypeSymbol5 eature3 7Tag ( a ) feature-value matrix notation `` ? '' i~ the prefix for a tag and TFSs with the same tag are token-identical. TypeSym bol/~ feo~. , o/ I TypeSymboll ~ \ [ . . TypeSymbol2 4¢ '' '~°~'~/.~ypeSymbol3 featury `` X~ature5 TypeSymbol4 4r `` ~TypeSymbol5 ( b ) directed graph notation Figure 2 : TFS notations Phrase \ [ sub ( at ? X2 SignList dtrs CHconst i 'oo Sign syn I head Syn I ubcat \ ] U ? Xl . \ ] NonEmptySignLIst | \ [ 'first ? ×3 \ ] 1 Lrest ? X2 J j Phrase -dtrs CHconst hdtr LexicalSign -syn Syn -head Head pos P orm Ga subcat NonEmptySignList Sign yn Syn ead Head L~ , os N\ ] Irest EmptySignkist ,11 Phrase `` syn Syn head ? X1 Head Fpos P Lform Ga Lsubcat ? X2 Empl.ySignList dtrs CHconst ccltr ? X3 Sign syn iyn head Head _ \ [ pos N hdtr \ ] LexicalSign l-syn Syn l I F head : x~ 7/ Lsubcat NonEinptySignList l l \ [ P '' '' ~×~ llll Lrest ? X2 JJjJ Figure 3 : Example of TFS unification Then , the unification of tl anti t2 is defined as their greatest lower bound or the meet. A unification example is shown in Fig. 3. In tile directed graph notation , TFS unification corresponds to graph mergi ng. TFSs are very convenient for describing linguistic information in unlfication-based formalisms. Method and Its Problems In TFS unification based on Wrobtewski 's method , a DG is represented by tile NODE and ARC structures corresponding to a TFS and a feature-value pair respectively , as shown in Fig. 4. The NODE structure has the slots TYPESYMBOL to represent a type symbol , ARCS to represent a set of feature-value pairs , GENERATION to specify the unification process in which the structure has been created , FORWARD , and COPY. When a NODE 's GENERATION value is equal to the global value specifying the current unit\ ] cation process , the structure has been created in the current process or that the structure is currel~l. The characteristics which allow nondestructive incremental copy are the NODE 's two different slots , FORWARD and COPY , for representing forwarding relationships. A FORWARD slot value represents an eternal relationship while a COPY slot value represents a temporary relationship. When a NODE node1 has a NODE node2 as its FORWARD value , the other contents of tile node1 are ignored and tim contents of node2 are used. t { owever , when a NODE has another NODE as its COPY value , the contents of the COPY value are used only when the COPY value is cub : rent. After the process finishes , all COPY slot values are ignored and thus original structures are not destroyed. The unification procedure based on this method takes as its input two nodes which are roots of the DGs to be unified. The procedure incrementally copies nodes and ares on the subgraphs of each input 1 ) G until a node with an empty ARCS value is found. The procedure first dereferences both root nodes of the input DGs ( i.e. , it follows up FORWARD and COPY slot values ) . If the dereferenee result nodes arc identical , the procedure finishes and returns one of the dereference result nodes. Next , the procedure calculates the meet of their type symbol. If the meet is Bottom , which means inconsistency , the procedure finishes and returns Bottom. Otherwise , the procedure obtains the output node with the meet as its TYPESYMBOL. The output node has been created only when neither input node is current ; or otherwise the output node is an existing current node. Next , the procedure treats arcs. The procedure assumes the existence of two procedures , namely , SharedArcs and ComplementArcs. The SharedArcs procedure takes two lists of arcs as its arguments and gives two lists of arcs each of which contains arcs whose labels exists in both lists with the same arc label order. The ComplementArcs procedure takes two lists of arcs as 225 NODE TYPESYMBOL : &lt; symbol &gt; \ [ ARCS : &lt; a list of ARC structures &gt; FORWARD : `` &lt; a NODE structure orNIL &gt; / COPY : &lt; a NODE structure or Nil , &gt; GENERATION : &lt; an integer &gt; ARC LABEL : &lt; symbol &gt; VALUE : &lt; : a NODE structure &gt; Figure 4 : Data Structures for Wroblewski 's method Input graph GI Input graph 62 ... ...</sentence>
				<definiendum id="0">FS ) unification</definiendum>
				<definiendum id="1">, NTT Basic Research i.aboratories. lh'esenl</definiendum>
				<definiens id="0">applied in the field of natural language processing and , based on these formalisms , ~ : ~ystems such as machine translation systems were developed \ [ l &lt; ol ; u , e et a l 8gJ. In such unification-based formalisms , feature ~trueture (</definiens>
				<definiens id="1">the most fundamental and ..~ignifieant operation. The efficiency of systems based on ..~uch formalisms , such as natural language analysis and generation systems very much depends on their FS ~lnifieatlon efficiencies. Tiffs dependency is especially crucial for lexicon-driven approaches such as tlPSO\ [ Pollard and Sag 861 and JPSG\ [ Gunji 871 because rich lexieal information and phrase structure information is described in terms of FSs. For example , a spoken Present. affiliation : Infi ) rmation Science Research 1 , aboratory</definiens>
			</definition>
			<definition id="1">
				<sentence>7 ) : ( 1 ) if nodel ' , the dereference result of node/ , is current , then CopyNode returns node l '' to indicate that the ancestor node node2 must be coiffed immediately ; ( 2 ) otherwise , CopyArcs is applied to node1 '' and if it returns , ~ ; everal arc copies , CopyNode creates a new copy node .</sentence>
				<definiendum id="0">CopyNode</definiendum>
				<definiens id="0">creates a new copy node</definiens>
			</definition>
			<definition id="2">
				<sentence>It then adds the arc copies and arcs of node/ ' that are not copied to the new node , and returns the new node ; ( 3 ) otherwise , CopyNode adds the pair consisting of the ancestor node node2 and the are arcl into the COPYDEPENDENCY slot of node 1 '' and returns Nil_ .</sentence>
				<definiendum id="0">CopyNode</definiendum>
				<definiens id="0">adds the pair consisting of the ancestor node node2 and the are arcl into the COPYDEPENDENCY slot of node 1 '' and returns Nil_</definiens>
			</definition>
			<definition id="3">
				<sentence>ENDIF ENDPROCEDURE CopyArcs PROCEDURE AlcsCopied ( node ) newarcs = OFOR ALL arc IN node .</sentence>
				<definiendum id="0">ENDPROCEDURE CopyArcs PROCEDURE AlcsCopied</definiendum>
			</definition>
</paper>

		<paper id="3080">
			<definition id="0">
				<sentence>The parser of QPATR uses a left-corner algorithm for context-free grammars and includes a facility for identifying new lexical items in input on the basis of contextual information .</sentence>
				<definiendum id="0">QPATR</definiendum>
				<definiens id="0">uses a left-corner algorithm for context-free grammars and includes a facility for identifying new lexical items in input on the basis of contextual information</definiens>
			</definition>
			<definition id="1">
				<sentence>In contrast to most existing PATR implementations such as D-PATR ( cf Karttunan 1986a , 1986b ) , QPATR runs under MS-DOS and thus makes minimal hardware demands .</sentence>
				<definiendum id="0">QPATR</definiendum>
				<definiens id="0">runs under MS-DOS and thus makes minimal hardware demands</definiens>
			</definition>
			<definition id="2">
				<sentence>Atomic well-formed formulas ( wffs ) of this logic consist of the two types of equations just introduced as well as macro heads ( see below ) ; heads of macros defined in terms of constraints are prefixed with the operator `` @ '' in atomic wffs .</sentence>
				<definiendum id="0">Atomic well-formed formulas ( wffs )</definiendum>
				<definiens id="0">of this logic consist of the two types of equations just introduced as well as macro heads ( see below ) ; heads of macros defined in terms of constraints are prefixed with the operator `` @ '' in atomic wffs</definiens>
			</definition>
			<definition id="3">
				<sentence>QPATR includes a facility of prediction whereby FSMs are proposed for new lexical items encountered in input but not contained in the lexicon .</sentence>
				<definiendum id="0">QPATR</definiendum>
				<definiens id="0">includes a facility of prediction whereby FSMs</definiens>
			</definition>
</paper>

		<paper id="2013">
			<definition id="0">
				<sentence>c , denotes the empty set .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">the empty set</definiens>
			</definition>
			<definition id="1">
				<sentence>We can characterize F1 in the following way : ( 9 ) F1 = ( GF + TA ) where GF denotes the set of grammatical function labels .</sentence>
				<definiendum id="0">GF</definiendum>
				<definiens id="0">the set of grammatical function labels</definiens>
			</definition>
</paper>

		<paper id="3020">
			<definition id="0">
				<sentence>For m &lt; k , let r. be v Pk_l.k_2 ) . K , m ( Pk-I m+l V Pk-I m+2 v ... Then ( 3-a-k , m ) is equ3 ) alent to : ' ( 3-b-k , m ) ( Pk m D Pm+l m ) &amp; ( Pk , ~n D ( Pm+2 , m v Pm+2 , m+t ) ) &amp; ( Pkm D ( Pk 2 m V rk. 1 m ) ) ( Pklm D ( Pk~l ' , m v , 'k , n~ ) ) Of course , the space required to define the r km must figure in the space required to encode an analys\ [ s of ( 3 ) along the lines of ( 3-b-k , m ) . rk , m_ l -= ( Pk-l , m v rk , m ) , so CNF prior to choosing a reading. The hypothesis is rather that fragments of a CNF representation are produced ( in some sense ) during processing. it requires quadratic space to define all the rk. m. A revised version of ( 3 ) with ( 3-b-k , m ) in place of ( 3-ak , m ) throughout requires cubic space. 6 Tree representations of single readings for examples like ( 3 ) may be viewed as follows : edges correspond to atomic propositions that comprise specifications like `` constituent i attaches to constituent j '' or `` constituent i projects to constituent j. , ,7 A non-terminal node A corresponds to a constituent , but also corresponds to the conjunction of the atomic propositions that correspond to edges that A dominates. Thus the root node of the tree corresponds to a proposition that comprises a full specification of constituent structure. The situation is essentially the same \ [ 'or shared forests. ( \ [ Tomita 87\ ] discusses shared forests and packed shared forests. ) Edges ill shared forests correspond to atomic propositions , and non-terminal nodes correspond to non-atomic propositions. To extend this perspective , shared forests compress the information in non-shared forests by exploiting the introduction of constants for non-atomic propositions. In a shared forest , the subtree that a node dominates is written only once. In effect , then , a constant is introduced that represents the conjunction that corresponds to the node. This constant is a constituent of the fornmlas that correspond to superior nodes. While shared forests are more compressed than unshared forests , the number of nodes in the shared forest representation of ( 3 ) is still exponential in the length of ( 3 ) . In a packed shared forest , a packed node that does not dominate any packed nodes corresponds to a disjunction of conjunctions of atomic propositions. Packed nodes that dominate other packed nodes correspond to disjunctions of conjunctions of atomic and non-atomic propositions. In effect , for each node ( packed or non-packed ) , a constant is introduced that abbreviates the formula that corresponds to the node. Exploitation of constants for non-atomic propositions pemfits more significant compression for packed shared forests than for shared forests. The packed root node of a packed shared forest for ( 3 ) cotxesponds to a disjunction of conjunctions whose size in atoms is exponential in the length of ( 3 ) . However , the number of nodes of a packed shared forest for ( 3 ) goes up as the square of the length of ( 3 ) . The number of edges of the packed shared forest ( a more authentic measure of the size of the forest ) goes up as the cube of the length. subscript indices. However , quantification over artifacts of representation may uncontroversially involve crossing the divide between analysis and procedure. here. For example , we will not distinguish `` X attaches to V '' from `` X attaches to VP. '' 2 110 3 A linear length encoding of an analysis of ( 1 ) is possible if we use the constant A = { John , Bill ... .. Harry } in the encoding as follows : ( l-c ) q &amp; ( antecedent ( pronoun l ) e A ) &amp; ( antecedent ( pronoun 2 ) e A ) &amp; ( antecedent ( pronounm ) e A ) Note that `` x ~ Y '' is short-hand for the disjunction of the statements `` x = y , '' where y ranges over Y , so that ( l-c ) is not very different from ( l-b ) . Examples below involve tYeer use of constants that correspond to sets of linguistic entities , I will call such constants `` structural. '' A linear analysis of ( 2 ) is possible if we introduce constants A 1 ... .. A , where A. = { John , Bill } , A ; = A 1 u { Tom } , A 3 = A 2 ~ { Fred } , ..l. , Am = Am-I U { Jim } : ( 2-b ) q &amp; ( matecedent ( pronoun t ) E A1 ) &amp; ( antecedent ( pronoun2 ) ~ A 2 ) &amp; quantifier Qi takes scope over Qi-I to its immediate left , then the quantifier Qi+l to the immediate right of Qi can not take scope over Qi '' ( See \ [ Epstein 88\ ] for a discussion of relative operator scope. ) It follows that the number of relative operator scope readings for ( 4 ) grows as the Fibonacci of the length of ( 4 ) . 8 However , a linear encoding of an exhaustive analysis of ( 4 ) is as follows : ( 4-a ) q &amp; \ [ ( ( Q1 &gt; Q2 ) &amp; ( L\ ] = Q2 ) ) v ( ( Q2 &gt; Q1 ) &amp; ( L1 = T ) ) \ [ &amp; \ [ Q1 &gt; Q3 \ ] &amp; \ [ ( ( L 1 &gt; Q3 ) &amp; ( L 2 = Q3 ) ) v ( ( Q3 &gt; L1 ) &amp; ( L2 = T ) ) \ ] &amp; \ [ Qk-2 &gt; Qk+l \ ] &amp; \ [ Qk-t &gt; Qk+l \ ] &amp; \ [ ( ( Lk_ 1 &gt; Qk+l ) &amp; ( L k = Qk+l ) ) v ( ( Qk+l &gt; Lk-1 ) &amp; ( Lk = T ) ) \ ] &amp; ( antecedent ( pronounm ) E Am ) Because A. can be defined in terms of Ai_l , only linear 1 space is required to define these constants .</sentence>
				<definiendum id="0">m_ l -=</definiendum>
				<definiendum id="1">non-terminal node A corresponds</definiendum>
				<definiendum id="2">packed node</definiendum>
				<definiens id="0">Pk-l , m v rk , m ) , so CNF prior to choosing a reading. The hypothesis is rather that fragments of a CNF representation are produced ( in some sense ) during processing. it requires quadratic space to define all the rk. m. A revised version of</definiens>
				<definiens id="1">the node. Exploitation of constants for non-atomic propositions pemfits more significant compression for packed shared forests than for shared forests. The packed root node of a packed shared forest for ( 3 ) cotxesponds to a disjunction of conjunctions whose size in atoms is exponential in the length of ( 3 ) . However , the number of nodes of a packed shared forest for ( 3 ) goes up as the square of the length of ( 3 ) . The number of edges of the packed shared forest ( a more authentic measure of the size of the forest</definiens>
				<definiens id="2">the disjunction of the statements `` x = y , '' where y ranges over Y</definiens>
			</definition>
			<definition id="1">
				<sentence>The desired solutions consist of specifications of attachment possibilities , stated in the form `` ap ( PPk ) e X '' ( `` attachment point of the k-th PP is one of the elements of X ' ) in ( 3~c ) .</sentence>
				<definiendum id="0">PP</definiendum>
				<definiens id="0">The desired solutions consist of specifications of attachment possibilities , stated in the form `` ap ( PPk ) e X ''</definiens>
			</definition>
</paper>

		<paper id="2031">
			<definition id="0">
				<sentence>Perrault describes the way assertions influence peoples beliefs by distinguishing between axioms describing strong evidence for beliefs , and default rules capturing the effects of ( sometimes failing ) speech acts .</sentence>
				<definiendum id="0">Perrault</definiendum>
			</definition>
			<definition id="1">
				<sentence>~peech act assertion is defined as follows : Aetion~ Assert P ( in SITu ) Pre~onditionm The hearer believes that the speaker is sincere and competent in SIT U. Add-list : The hearer believes that the speaker believes P. Body : Utter ~P. '' ( in SIT v ) ' : \ [ ~e effect ( as specified in the add-list ) is what we take to be the fllocutionary point : to achieve that the hearer believes that the speaker believes the propositional con~nt .</sentence>
				<definiendum id="0">~peech act assertion</definiendum>
				<definiens id="0">Aetion~ Assert P ( in SITu ) Pre~onditionm The hearer believes that the speaker is sincere and competent in SIT U. Add-list : The hearer believes that the speaker believes P. Body : Utter ~P. '' ( in SIT v ) ' : \ [ ~e effect ( as specified in the add-list ) is what we take to be the fllocutionary point : to achieve that the hearer believes that the speaker believes the propositional con~nt</definiens>
			</definition>
			<definition id="2">
				<sentence>Let us assume the following scenario : a speaker S ut4~ers `` P. '' ( referring to a situation SIT R ) in the situation SIT v. Sincerity and competence of the speaker can be judged by an observer O ( who may be identical to the ~ !</sentence>
				<definiendum id="0">Sincerity</definiendum>
				<definiens id="0">a speaker S ut4~ers `` P. '' ( referring to a situation SIT R ) in the situation SIT v.</definiens>
			</definition>
			<definition id="3">
				<sentence>Nevertheless , ifA is an action , then so is tryA , and intending to tryA is a genuine intention distinct from an intention to A. In both cases will the intending agent try-to-do A , and sometimes doA , but even if she fails to do A , she can not fail to try A. Trying A will sometimes have the effects of A-ing -and , if it does , will have caused these effects -but its only necessary result is that A has been tried .</sentence>
				<definiendum id="0">ifA</definiendum>
				<definiens id="0">an action</definiens>
			</definition>
			<definition id="4">
				<sentence>MEDIAS handles assertions as well as yes/no questions , distinguishing information-seeking from information-probing questions .</sentence>
				<definiendum id="0">MEDIAS</definiendum>
				<definiens id="0">handles assertions as well as yes/no questions , distinguishing information-seeking from information-probing questions</definiens>
			</definition>
</paper>

		<paper id="3101">
			<definition id="0">
				<sentence>Abstract : A Bilingual Knowledge Bank is a syntactically and referentially structured pair of corpora , one being a translation of the other , in which translation units are cross-codexl between the corpora .</sentence>
				<definiendum id="0">Abstract</definiendum>
				<definiens id="0">A Bilingual Knowledge Bank is a syntactically and referentially structured pair of corpora , one being a translation of the other , in which translation units are cross-codexl between the corpora</definiens>
			</definition>
			<definition id="1">
				<sentence>The pilot implementation consists of three , main parts : the parser , the `` synsemizer '' and the retrieval system .</sentence>
				<definiendum id="0">pilot implementation</definiendum>
				<definiens id="0">consists of three , main parts : the parser , the `` synsemizer '' and the retrieval system</definiens>
			</definition>
			<definition id="2">
				<sentence>The retrieval system is a tool which extracts information from a BKB that has been built using the parser and the synsemizer .</sentence>
				<definiendum id="0">retrieval system</definiendum>
				<definiens id="0">a tool which extracts information from a BKB that has been built using the parser and the synsemizer</definiens>
			</definition>
			<definition id="3">
				<sentence>Witkam , Toon ( 1988 ) : DLT an industrial R &amp; D project for multilingual MT. In : Proceedings of the 12th International Conference on Computational Linguistics , Budapest , 1988 , pp .</sentence>
				<definiendum id="0">DLT</definiendum>
			</definition>
</paper>

		<paper id="3060">
</paper>

		<paper id="2020">
			<definition id="0">
				<sentence>The concept hierarchy is the basis for the join and projection algorithms \ [ 3 , 6\ ] which provide a way to disambignate the Natural l~anguage complex sentences and to perform query/answering on Conceptual Graphs .</sentence>
				<definiendum id="0">concept hierarchy</definiendum>
				<definiens id="0">the basis for the join and projection algorithms \ [ 3 , 6\ ] which provide a way to disambignate the Natural l~anguage complex sentences and to perform query/answering on Conceptual Graphs</definiens>
			</definition>
			<definition id="1">
				<sentence>We give here the logical interpretation of the hyperonymy relation between the words with meanings wl and w2 , derived from the one given in \ [ 4\ ] : w I is hyperonym of w2 ill ' , for every sentence S true\ [ S ( w2 ) \ ] ~ true\ [ S ( w2/wl ) \ ] where : S ( w ) stands for a sentence containing an occurrence of w , S ( w2/wl ) stands R ) r the sentence S ( w2 ) in which the occurrence of w2 is replaced by w 1 .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">a sentence containing an occurrence of w , S ( w2/wl ) stands R ) r the sentence</definiens>
			</definition>
</paper>

		<paper id="2008">
			<definition id="0">
				<sentence>Abstract : Current research being undertaken at both Cambridge and IBM is aimed at the construction of substantial lexicons containing lexical semantic information capable of use in automated natural language processing ( NLP ) applications .</sentence>
				<definiendum id="0">Abstract</definiendum>
				<definiens id="0">Current research being undertaken at both Cambridge and IBM is aimed at the construction of substantial lexicons containing lexical semantic information capable of use in automated natural language processing ( NLP ) applications</definiens>
			</definition>
			<definition id="1">
				<sentence>In general , coercion is a problem in theories which take the syntactic aspect of grammatical realisation as primary , but would be a natural consequence of a theory which took the sense and rite fact that believe3 is a relation between an individual and a proposition as basic .</sentence>
				<definiendum id="0">coercion</definiendum>
				<definiens id="0">a relation between an individual</definiens>
			</definition>
			<definition id="2">
				<sentence>In Order to automatically obtain typical ( frequent ) predications of book , four corpora were searched LDOCE example sentences , the Brown corpus , 1.2 million words of Readers ' Digest , and 26 million words of tapes from the American Publishing House for the Blind .</sentence>
				<definiendum id="0">Brown</definiendum>
				<definiens id="0">corpus , 1.2 million words of Readers ' Digest , and 26 million words of tapes from the American Publishing House for the Blind</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>Induction : Processes of Inference , Learning , and Discovery .</sentence>
				<definiendum id="0">Induction</definiendum>
			</definition>
</paper>

		<paper id="2021">
			<definition id="0">
				<sentence>feature set : CAT = N + CAT -- = V \ [ _ \ [ _ SF = SUBJ_ SF = PRED _t _ whereby K is the feature of '' the phrase type , NP and VP are the values of this feature ; CAT is the feature o\ [ `` the parts of speech , N and V are the values of this feature ; SF is the feature o~ '' syntactic function , SUB .</sentence>
				<definiendum id="0">K</definiendum>
				<definiendum id="1">VP</definiendum>
				<definiendum id="2">CAT</definiendum>
				<definiendum id="3">V</definiendum>
				<definiendum id="4">SF</definiendum>
				<definiens id="0">N + CAT -- = V \ [ _ \ [ _ SF = SUBJ_ SF = PRED _t _ whereby</definiens>
				<definiens id="1">the feature of '' the phrase type</definiens>
			</definition>
			<definition id="1">
				<sentence>With the complex features , the structure `` NP + VP '' which forms a construction `` modifier + head '' can be formutarized as following : CAT = N + CAT = V SF = MODF_ SF = HEAD whereby MODF and HEAD are the values of the feature SF .</sentence>
				<definiendum id="0">HEAD</definiendum>
				<definiens id="0">the values of the feature SF</definiens>
			</definition>
			<definition id="2">
				<sentence>In English , we have VP + NP - &gt; VP1 , the VP corresponding to the predicate : , the NP to the object , and VP1 is a new verb phrase , it is a construction `` predicate + object '' .</sentence>
				<definiendum id="0">VP1</definiendum>
				<definiens id="0">a new verb phrase</definiens>
			</definition>
			<definition id="3">
				<sentence>In the structure `` NP + VP '' , if the syntactic function of NP is subject , and the semantic relation of NP is agent , then the complex features of this structure can be formularized as following : \ [ K=NP ~ K=VP \ ] CAT = N + CAT = V \ ] SF = SUBJ l k. SF = PRED J L SM AGENT _I whereby SM is the feature of semantic relation , AGENT is the values of feature SM .</sentence>
				<definiendum id="0">AGENT</definiendum>
				<definiens id="0">the feature of semantic relation</definiens>
				<definiens id="1">the values of feature SM</definiens>
			</definition>
			<definition id="4">
				<sentence>= PRED whereby PATIENT is the value o\ [ feature SM .</sentence>
				<definiendum id="0">PATIENT</definiendum>
			</definition>
			<definition id="5">
				<sentence>_sF = MODFJ ÷ I K = NP \ ] iL % d whereby TRANS represents the feature of transitb~ty of verb , IV represents intransitive verb , it is a value of feature TRANS .</sentence>
				<definiendum id="0">TRANS</definiendum>
				<definiendum id="1">IV</definiendum>
				<definiens id="0">intransitive verb</definiens>
			</definition>
			<definition id="6">
				<sentence>% '' ( high trainirg teacher ) , the VP `` ~I'~ } '' ( to engage in advanced studies ) is a transitive verb , the NP `` : g£0~ '' ( teacher ) is a title of ' professional post , thus we can decide that the syntactical function of ' VP `` : '~i~7 '' . ''</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">a transitive verb , the</definiens>
				<definiens id="1">a title of ' professional post</definiens>
			</definition>
</paper>

		<paper id="3023">
			<definition id="0">
				<sentence>The Model Query Processor accepts queries about the static model , either testing a parameter of a component or a relation ( adjacency , containment , etc. ) between two components .</sentence>
				<definiendum id="0">Model Query Processor</definiendum>
				<definiens id="0">accepts queries about the static model , either testing a parameter of a component or a relation ( adjacency , containment , etc. ) between two components</definiens>
			</definition>
</paper>

		<paper id="3077">
			<definition id="0">
				<sentence>SCISOR ( System for Conceptual Information Summarization , Organization , and Retrieval ) reads financial news stories from a news service , selects stories about mergers and acquisitions , extracts key pieces of information from those stories , and answers English questions about this information .</sentence>
				<definiendum id="0">SCISOR</definiendum>
				<definiens id="0">financial news stories from a news service , selects stories about mergers and acquisitions , extracts key pieces of information from those stories , and answers English questions about this information</definiens>
			</definition>
			<definition id="1">
				<sentence>ScBoR provides the user with information in multiple forms .</sentence>
				<definiendum id="0">ScBoR</definiendum>
				<definiens id="0">provides the user with information in multiple forms</definiens>
			</definition>
			<definition id="2">
				<sentence>SCISOR : A system tbr extracting information from on-line news .</sentence>
				<definiendum id="0">SCISOR</definiendum>
				<definiens id="0">A system tbr extracting information from on-line news</definiens>
			</definition>
</paper>

		<paper id="3056">
</paper>

		<paper id="3037">
			<definition id="0">
				<sentence>The three axioms : &lt; s ) + `` W '' -- * &lt; n , s\np ) where W : np/n ( n , s\np ) + `` W '' -- + ( s\np ) where W : n ( s\np ) + `` W '' -- * ( &gt; where W : s\np ( 'np/n ' is an np requiring a noun on its right , and 's\np ' is a sentence requiring an up on its left ) are instantiations of the axioms : &lt; X ) ® R + `` W '' -+ &lt; Z , X\Y ) • R where W : Y/Z ( X ) ® R + `` W '' -- + R where W : X ( 'X ' is the head and 'R ' , the tail of the list encoding the state. '. ' denotes concatenation , so ' ( n , s\np } ' is equivalent to ' ( n ) • &lt; s\np &gt; ' ) The 'encoded states ' will roughly correspond to 'principal ' categories in Axiomatic Grammar , and the axioms above to the axioms of Prediction and Composition .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">&lt; s ) + `` W '' -- * &lt; n , s\np ) where W : np/n ( n , s\np ) + `` W '' -- + ( s\np ) where W : n ( s\np ) + `` W '' -- * ( &gt; where</definiens>
				<definiens id="1">a sentence requiring an up on its left ) are instantiations of the axioms : &lt; X ) ® R + `` W '' -+ &lt; Z , X\Y ) • R where W : Y/Z ( X ) ® R + `` W '' -- + R where W</definiens>
				<definiens id="2">the tail of the list encoding the state. '. ' denotes concatenation , so ' ( n , s\np } ' is equivalent to ' ( n ) • &lt; s\np &gt; ' ) The 'encoded states ' will roughly correspond to 'principal ' categories in Axiomatic Grammar , and the axioms above to the axioms of Prediction and Composition</definiens>
			</definition>
			<definition id="1">
				<sentence>A parse of a sentence consists of a proof that , starting with a principal category which requires a sentence , we can end 208 2 up with a complete principal category .</sentence>
				<definiendum id="0">parse of a sentence</definiendum>
				<definiens id="0">consists of a proof that , starting with a principal category which requires a sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>The first , has already been proved , so we are left to prove : cO `` sits '' \ [ \ ] The head of the argument list of cO is an optional noun-phrase modifier , Optional categories at the head of the argument list of a principal category can be deleted by the use of the Optional Reduction Rule which is as follows : \ [ r = R '' \ ] String C r = ( 1 = L ) * String C r = R We instantiate the Optional Reduction Rule to : cl `` sits '' \ [ \ ] c0 '' sits '' \ [ \ ] in which 'el ' is cO without the optional modifier i.e. r= ( ( c=np ) ) Tile proof now consists of proving the antecedent of tile Optional Reduction Rule i.e. r = ( ( \ [ c : , p\ ] / ) This can be proved using first the Composition Axiom , then tile Sequencing Rule followed by Optional Reduction , and finally the Identity Axiom .</sentence>
				<definiendum id="0">cO</definiendum>
				<definiens id="0">an optional noun-phrase modifier</definiens>
			</definition>
			<definition id="3">
				<sentence>The Coordination Rule enforces a parallelism between conjuncts in a similar manner to the parallelism enforced by the phrase structure rule mentioned above .</sentence>
				<definiendum id="0">Coordination Rule</definiendum>
				<definiens id="0">enforces a parallelism between conjuncts in a similar manner to the parallelism enforced by the phrase structure rule mentioned above</definiens>
			</definition>
</paper>

		<paper id="3065">
			<definition id="0">
				<sentence>ment is of the form `` If X then Y '' where X is the negation of the propositional content of the conclusion and Y is some as yet unspecified harm to H. ( SP ) is to provide evidence for RP .</sentence>
				<definiendum id="0">ment</definiendum>
				<definiendum id="1">X</definiendum>
				<definiendum id="2">Y</definiendum>
				<definiens id="0">the negation of the propositional content of the conclusion</definiens>
				<definiens id="1">to provide evidence for RP</definiens>
			</definition>
			<definition id="1">
				<sentence>ASA ( Practical Arguments &amp; Speech Acts ) , a test program which accepts practical arguments as input and identifies their principal speeeh acts .</sentence>
				<definiendum id="0">ASA</definiendum>
				<definiens id="0">a test program which accepts practical arguments as input and identifies their principal speeeh acts</definiens>
			</definition>
			<definition id="2">
				<sentence>PASA is a modest example of a language driven understanding system in which the need for domain specific knowledge is minimized .</sentence>
				<definiendum id="0">PASA</definiendum>
			</definition>
</paper>

		<paper id="3032">
			<definition id="0">
				<sentence>A sentence is an adequate normalization of an ill-formed utterance , if it corresponds m our intuitions about what the speaker might have intended to say .</sentence>
				<definiendum id="0">sentence</definiendum>
				<definiens id="0">an adequate normalization of an ill-formed utterance</definiens>
			</definition>
			<definition id="1">
				<sentence>The reparandum is the part of the utterance that is to be corrected by the reparans .</sentence>
				<definiendum id="0">reparandum</definiendum>
				<definiens id="0">the part of the utterance that is to be corrected by the reparans</definiens>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>Kasper , Robert T. ( 1989b ) 'Unification and Classification : an experiment in information-based parsing ' .</sentence>
				<definiendum id="0">Classification</definiendum>
				<definiens id="0">an experiment in information-based parsing '</definiens>
			</definition>
			<definition id="1">
				<sentence>The Penman Project ( 1989 ) 'The PENMAN docuRavelli , Louise J. ( 1985 ) 'Metaphor , Mode and Complexity : an exploration of co-varying pa.tterns ' .</sentence>
				<definiendum id="0">Penman Project</definiendum>
				<definiendum id="1">Complexity</definiendum>
				<definiens id="0">an exploration of co-varying pa.tterns '</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>Text structuring consists basically of finding the optimal way of cutting each text content representaThe last part of forecast generation involves the relatively well-developed technique of sentence realization .</sentence>
				<definiendum id="0">Text structuring</definiendum>
				<definiens id="0">consists basically of finding the optimal way of cutting each text content representaThe last part of forecast generation involves the relatively well-developed technique of sentence realization</definiens>
			</definition>
</paper>

		<paper id="1025">
</paper>

		<paper id="3093">
			<definition id="0">
				<sentence>A FD represents a list of attributevalue pairs , j Rousselot 's formalism is a very extended form of the functional grammars \ [ Kay , 1985\ ] .</sentence>
				<definiendum id="0">FD</definiendum>
			</definition>
			<definition id="1">
				<sentence>We have developed an algorithm , which determines automatically the gender of the Bulgarian nouns ( consisting of 254 steps ) .</sentence>
				<definiendum id="0">algorithm</definiendum>
			</definition>
			<definition id="2">
				<sentence>Iiplementat ion GECO is a program , designed for instructional and experimental purposes .</sentence>
				<definiendum id="0">Iiplementat ion GECO</definiendum>
				<definiens id="0">a program , designed for instructional and experimental purposes</definiens>
			</definition>
</paper>

		<paper id="3058">
</paper>

		<paper id="2056">
			<definition id="0">
				<sentence>The semantic network consists of systems like threat / warning , physical punishment / mental punishment / restraint on behaviour , and so on .</sentence>
				<definiendum id="0">semantic network</definiendum>
				<definiens id="0">consists of systems like threat / warning , physical punishment / mental punishment / restraint on behaviour , and so on</definiens>
			</definition>
			<definition id="1">
				<sentence>A systemic semantics : the chooser and inquiry framework .</sentence>
				<definiendum id="0">systemic semantics</definiendum>
				<definiens id="0">the chooser and inquiry framework</definiens>
			</definition>
</paper>

		<paper id="3078">
			<definition id="0">
				<sentence>S~TEM CONFiGUR &amp; TIOR ECTST consists of a translation program , a bili,4Tual diction~w and a rule-data base .</sentence>
				<definiendum id="0">TIOR ECTST</definiendum>
				<definiens id="0">consists of a translation program , a bili,4Tual diction~w and a rule-data base</definiens>
			</definition>
			<definition id="1">
				<sentence>The transfer phase comprises rules for converting the parsed SL sentence into the TL sentence .</sentence>
				<definiendum id="0">transfer phase</definiendum>
				<definiens id="0">comprises rules for converting the parsed SL sentence into the TL sentence</definiens>
			</definition>
</paper>

		<paper id="3045">
			<definition id="0">
				<sentence>synchronous TAG is a pair consisting of two elementar2 , ' trees , one from tlie source language ( English ) and one from the target ( logical form \ [ LF\ ] ) .</sentence>
				<definiendum id="0">synchronous TAG</definiendum>
				<definiens id="0">a pair consisting of two elementar2 , ' trees , one from tlie source language ( English ) and one from the target ( logical form \ [ LF\ ] )</definiens>
			</definition>
			<definition id="1">
				<sentence>Vijay-Shanker and Joshi ( 1989 ) provide an argument that this claim follows from several assumptions concerning how a feature system for TAGs might be constrained .</sentence>
				<definiendum id="0">Vijay-Shanker</definiendum>
				<definiens id="0">provide an argument that this claim follows from several assumptions concerning how a feature system for TAGs might be constrained</definiens>
			</definition>
			<definition id="2">
				<sentence>Vijay-Shanker ( personal communication ) has noted that by placing a simple assumption on the elementary trees in the logical form component of a synchronous TAG , the proof of this claim becomes immediate .</sentence>
				<definiendum id="0">Vijay-Shanker</definiendum>
				<definiendum id="1">TAG</definiendum>
				<definiens id="0">personal communication ) has noted that by placing a simple assumption on the elementary trees in the logical form component of a synchronous</definiens>
			</definition>
			<definition id="3">
				<sentence>~ Thus , a synchronous TAG ( like the grammar presented in Figure 1 ) whose semantic component forms a TAG with this property necessarily obeys the regular language constraint on long-distance semantic dependencies .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">the grammar presented in Figure 1 ) whose semantic component forms a TAG with this property necessarily obeys the regular language constraint on long-distance semantic dependencies</definiens>
			</definition>
</paper>

		<paper id="3044">
			<definition id="0">
				<sentence>The translation process consists of three steps : ( .1 ) Make the source matching expression from lhe source sentence .</sentence>
				<definiendum id="0">translation process</definiendum>
				<definiens id="0">consists of three steps : ( .1 ) Make the source matching expression from lhe source sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>MBT2 is the second prototype system in our Memory-based Translation Project .</sentence>
				<definiendum id="0">MBT2</definiendum>
			</definition>
			<definition id="2">
				<sentence>A t~anslation example consists of three parts : • an English word-dependency tree ( EWD ) • a Japanese word-dependency tree ( JWD ) • correspondence links For example , in Prolog , ewd e ( \ [ el , \ [ buy , v\ ] , \ [ e2 , \ [ he , pron\ ] \ ] , \ [ e3 , \ [ notebook , n\ ] , \ [ e4 , \ [ a , det\ ] \ ] \ ] \ ] ) .</sentence>
				<definiendum id="0">t~anslation example</definiendum>
				<definiens id="0">consists of three parts : • an English word-dependency tree ( EWD ) • a Japanese word-dependency tree ( JWD ) • correspondence links For example , in Prolog , ewd e ( \ [ el , \ [ buy</definiens>
			</definition>
			<definition id="3">
				<sentence>Matching expression ( ME ) is defined as the following : &lt; HE &gt; : := \ [ &lt; ID &gt; I &lt; ME-Commands &gt; \ ] &lt; ME-Commands &gt; : : = \ [ \ ] or \ [ &lt; ME-Command &gt; I &lt; ME-Commands &gt; \ ] &lt; ME-Command &gt; : : = \ [ d , &lt; ID &gt; \ ] or \ [ r , &lt; ID &gt; , &lt; ME &gt; \ ] or \ [ a , &lt; ID &gt; , &lt; ME &gt; \ ] % % delete &lt; ID &gt; % % replace &lt; ID &gt; % % with &lt; ME &gt; % % add &lt; ME &gt; as a % % child of root % % node of &lt; ID &gt; Every ID in an ME should be translatable .</sentence>
				<definiendum id="0">Matching expression ( ME )</definiendum>
				<definiens id="0">the following : &lt; HE &gt; : := \ [ &lt; ID &gt; I &lt; ME-Commands &gt; \ ] &lt; ME-Commands &gt; : : = \ [ \ ] or \ [ &lt; ME-Command &gt; I &lt; ME-Commands &gt; \ ] &lt; ME-Command &gt; : : = \ [ d , &lt; ID &gt; \ ] or</definiens>
			</definition>
			<definition id="4">
				<sentence>, ompo~itiol~ \ ] Target WD ( TWD ) Figure 1 : Flow of Translaton The matching expression ( b ) consists of two transla , tion units : el-e3 , e13 .</sentence>
				<definiendum id="0">Target WD</definiendum>
			</definition>
			<definition id="5">
				<sentence>The translation process consists of three steps : decomposition , transfer , and composition .</sentence>
				<definiendum id="0">translation process</definiendum>
				<definiens id="0">consists of three steps : decomposition , transfer , and composition</definiens>
			</definition>
</paper>

		<paper id="3019">
			<definition id="0">
				<sentence>We use a hierarchy of subcategorization types where `` monosemous readings '' of lexical units are classified .</sentence>
				<definiendum id="0">monosemous</definiendum>
				<definiens id="0">readings '' of lexical units are classified</definiens>
			</definition>
			<definition id="1">
				<sentence>Penman Development Note '' , ms. ( Los Angeles : ISI ) , May 1989 \ [ BrtESNAN/KANI~ : aVA 1989\ ] Joan Bresnan , Jonni M. Kanerva : `` Locative Inversion in ChicheWa : A Case S~udy of Factorization in Grammar '' in : Linguistic Inquiry , Vol .</sentence>
				<definiendum id="0">ChicheWa</definiendum>
				<definiens id="0">A Case S~udy of Factorization in Grammar '' in : Linguistic Inquiry , Vol</definiens>
			</definition>
			<definition id="2">
				<sentence>20/1 , p. 1-50 , 1989 \ [ DANLOS 1987\ ] Laurence Danlos : The Linguistic Basis of Text Generation .</sentence>
				<definiendum id="0">Laurence Danlos</definiendum>
				<definiens id="0">The Linguistic Basis of Text Generation</definiens>
			</definition>
			<definition id="3">
				<sentence>International Conference on Fifth Generation Computer Systems , ( Tokyo ) 1988 \ [ KAMP/REYLE 1990\ ] tIans Kamp , Uwe Reyle : From Discourse to Logic , ( Dordrecht : Reidel ) : to appear \ [ KAPLAN ET AL. 1989\ ] Ronald M. t ( aplan , Klaus Netter , J/irgen Wedekind , Annie Zaenen : `` Translation by Structural Correspondences '' in : Proceedings of the 4th Conference of the A CL , European Chapter , Manchester , 10.12 April 1989 , 1989 \ [ KAY 1984\ ] Martin Kay : `` Functional Unification Grammar : a formalism for machine translation '' .</sentence>
				<definiendum id="0">Unification Grammar</definiendum>
				<definiens id="0">a formalism for machine translation ''</definiens>
			</definition>
			<definition id="4">
				<sentence>, ms. ( Pittsburgh , Pa : Center for Machine Translation , Carnegie Mellon University ) 1989 \ [ PoLLARD/SAG 1987\ ] Carl J. Pollard , Ivan A. Sag : Information-Based Syntax and Semantics , Vol.1 , Fundamentals , ( Chicago : University Press ) 1987 \ [ CSLI Lecture Notes , No. 13\ ] \ [ ZAIZNEN 1988\ ] A. Zaenen : Lexical Information in LFG -At* Overview , ms. ( Palo Alto : Xerox PARC ) 1988 \ [ ZAJAC :1989A\ ] R4mi Zajac : `` A Relational Approach to Translation '' , ( Stuttgart : IMS ) , internal paper , submitted to Third International Conference on Theoretical and Methodological Issues of Machine Translation of Languages ( Austin , 1990 ) \ [ ZAJAC 1989B\ ] R4mi Zajac : `` A Transfer Mode\ ] Using a Typed Feature Structure Rewriting System with Inheritance . ''</sentence>
				<definiendum id="0">Chicago</definiendum>
				<definiens id="0">Lexical Information in LFG -At* Overview</definiens>
				<definiens id="1">A Relational Approach to Translation '' , ( Stuttgart : IMS ) , internal paper , submitted to Third International Conference on Theoretical and Methodological Issues of Machine Translation of Languages</definiens>
			</definition>
</paper>

		<paper id="3103">
			<definition id="0">
				<sentence>garian Morphology Like the other Slavonic languages , Bulgarian is a highly inflexional language one lexeme produces an average of 10 different wordforms ; in the discussed system there exist 58 types of alternation and 102 inflexional types ( 46 of them concern nouns ) .</sentence>
				<definiendum id="0">Bulgarian</definiendum>
				<definiens id="0">a highly inflexional language one lexeme produces an average of 10 different wordforms</definiens>
			</definition>
</paper>

		<paper id="2042">
			<definition id="0">
				<sentence>One of the issues of Artificial Intelligence is the transfer of the knowledge conveyed by Natural Language into formalisms that a computer can interpret .</sentence>
				<definiendum id="0">Artificial Intelligence</definiendum>
				<definiens id="0">the transfer of the knowledge conveyed by Natural Language into formalisms that a computer can interpret</definiens>
			</definition>
			<definition id="1">
				<sentence>A Conceptual Graph is an orienled graph macle up of concept nodes related by conceptual relation edges .</sentence>
				<definiendum id="0">Conceptual Graph</definiendum>
				<definiens id="0">an orienled graph macle up of concept nodes related by conceptual relation edges</definiens>
			</definition>
			<definition id="2">
				<sentence>A. Berard -- Dugourd , J. Fargues , M.C. Landau , J.P. Rogala , Natural Language Information Retrieval from French Texts , 3rd Workshop on Conceptual Graphs sponsored by AAAI , St-Paul , Minnesota , August 27 , 1988 P. Bosch , Some Good Reasons for Shallow Pronoun Processing , IBM Conference on Natural Language Processing , Thornwood , NY , October 24-26 , 1988 L. Danlos , G~n6ration automatique de textes en langues naturelles , pp 191-208 , Masson , Paris , 1985 D.R. Dowry , R.E. Wall and S. Peters , Introduction to Montague Semantics , D. Reidel Publishing Company , Dordrecht ( Holland ) , 1981 .</sentence>
				<definiendum id="0">D.R. Dowry , R.E. Wall</definiendum>
				<definiens id="0">Natural Language Information Retrieval from French Texts , 3rd Workshop on Conceptual Graphs sponsored by AAAI , St-Paul , Minnesota , August 27 , 1988 P. Bosch , Some Good Reasons for Shallow Pronoun Processing , IBM Conference on Natural Language Processing</definiens>
			</definition>
			<definition id="3">
				<sentence>J. Fargues &amp; al. , Conceptual Graphs for semantics and knowledge processing , IBM Journal of Research and Development , Vol 30 , No. 1 , January 1986 , pp 70-79 .</sentence>
				<definiendum id="0">Conceptual Graphs</definiendum>
				<definiens id="0">for semantics and knowledge processing , IBM Journal of Research and Development</definiens>
			</definition>
</paper>

		<paper id="3036">
			<definition id="0">
				<sentence>A lowel-level network reads the segments of the sentence word by word into partially specified case-role representations of the acts .</sentence>
				<definiendum id="0">lowel-level network</definiendum>
			</definition>
			<definition id="1">
				<sentence>The system consists of four hierarchically organized subnetworks ( figure 1 ) .</sentence>
				<definiendum id="0">system</definiendum>
			</definition>
			<definition id="2">
				<sentence>The act parser reads the input words one at a time , and forms a stationary caserole representation for each act fragment ( defined as part of sentence separated by commas ) .</sentence>
				<definiendum id="0">act parser</definiendum>
			</definition>
			<definition id="3">
				<sentence>These are fed one at a time to the act generator , which generates the sequence of words for each act fragment .</sentence>
				<definiendum id="0">generator</definiendum>
				<definiens id="0">generates the sequence of words for each act fragment</definiens>
			</definition>
			<definition id="4">
				<sentence>Each subsystem consists of two hierarchically organized modules , with the case-role assignment of the act as an intermediate representation .</sentence>
				<definiendum id="0">subsystem</definiendum>
				<definiens id="0">consists of two hierarchically organized modules , with the case-role assignment of the act as an intermediate representation</definiens>
			</definition>
			<definition id="5">
				<sentence>Both types of Recurrent FGREP networks dew ; lop representations in their input layers .</sentence>
				<definiendum id="0">Recurrent FGREP</definiendum>
				<definiens id="0">networks dew ; lop representations in their input layers</definiens>
			</definition>
			<definition id="6">
				<sentence>The sentence generator network takes the internal representation as its input and produces the case-role representation of the first act fragment , l woman ( blank ) ( blank ) I as its output ( figure 5 ) .</sentence>
				<definiendum id="0">sentence generator network</definiendum>
				<definiens id="0">takes the internal representation as its input and produces the case-role representation of the first act fragment</definiens>
			</definition>
			<definition id="7">
				<sentence>A higher-level process ( the sentence parser ) keeps track of the recursive relations of tile act fragments and combines them into complete representations .</sentence>
				<definiendum id="0">higher-level process</definiendum>
				<definiens id="0">the sentence parser ) keeps track of the recursive relations of tile act fragments and combines them into complete representations</definiens>
			</definition>
</paper>

		<paper id="2027">
			<definition id="0">
				<sentence>Syntactical robustness is a desired design properry of natural language parsers .</sentence>
				<definiendum id="0">Syntactical robustness</definiendum>
			</definition>
			<definition id="1">
				<sentence>is Ssetind ~ ~a-tind , 2RNF ( A ) is in disjunctive normal form , such that DNF ( RNF ( A ) ) : RNF ( A ) Robustness in the area of av-languages is the ability to cope with inconsistent ( i.e. overspeeifled ) formulae .</sentence>
				<definiendum id="0">RNF</definiendum>
				<definiendum id="1">RNF</definiendum>
				<definiendum id="2">av-languages</definiendum>
				<definiens id="0">the ability to cope with inconsistent ( i.e. overspeeifled ) formulae</definiens>
			</definition>
			<definition id="2">
				<sentence>In general , set weakening is defined as follows : ( 4 ) Let A E cO a fonmfla in disjunctive nor-mal form and t a non-constant tenn .</sentence>
				<definiendum id="0">E cO</definiendum>
				<definiens id="0">a fonmfla in disjunctive nor-mal form and t a non-constant tenn</definiens>
			</definition>
			<definition id="3">
				<sentence>Then A 0-partial B iff : DNF ( A A I ) = DN F ( B ) otherwise 2 157 a. leNF ( A ) ¢ Tile formula I C c ) may be restricted to be a conjunction of default literals , whose predicate is marked with a. subscript a. This gives a default relation , which is a subset of a superset of subsumption between formulae .</sentence>
				<definiendum id="0">DNF</definiendum>
				<definiens id="0">a subset of a superset of subsumption between formulae</definiens>
			</definition>
			<definition id="4">
				<sentence>F ( A1 A I ) by conjoining it with a default formula I G 0 such that RNF ( A2 A I ) = I. mantics For any WACSG-Grammar G , a domain D ( G ) and its subset SDDE ( G ) of strictly derivable domain elements is defined as follows .</sentence>
				<definiendum id="0">F ( A1 A I</definiendum>
				<definiens id="0">conjoining it with a default formula I G 0 such that RNF ( A2 A I ) = I. mantics For any WACSG-Grammar G , a domain D ( G ) and its subset SDDE ( G ) of strictly derivable domain elements</definiens>
			</definition>
			<definition id="5">
				<sentence>( 7 ) die \ [ Umschaltung the \ [ switching A Einstellung\ ] des Fonts A adjustment \ ] of the font /3 ' 7 ( 8 ) Peter \ [ versuchte dann A konnte\ ] kommen Peter \ [ tried then ,4 could \ ] come From the viewpoint of robustness theory , a restart &lt; a/3 A S~7 , M &gt; 6 D ( G ) should not be in SDDE ( G ) exactly if it is defect , where G is a realistic WACSG fragment of the language in question .</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">a realistic WACSG fragment of the language in question</definiens>
			</definition>
			<definition id="6">
				<sentence>lefec obj2 obj subj + \ [ syn\ [ defec+\ ] \ ] SelTl class nllrnan I t sern \ [ pred unfilled \ ] syn ... sere \ [ pred unfilled \ ] \ [ pred interessieren ' ( arg3 , arg2 ) \ [ 3\ ] arg3 \ [ a\ ] sem \ [ 1\ ] class human syn \ [ gender fern sem \ [ 2\ ] \ [ pred sclmle ' \ ] \ ] 6 161 Bibliography DeJong , G. : An Overview of the FB .</sentence>
				<definiendum id="0">G.</definiendum>
				<definiens id="0">An Overview of the FB</definiens>
			</definition>
</paper>

		<paper id="2029">
			<definition id="0">
				<sentence>Presently , Unification is defined as extension of context-flee grmnm~s. Knowing th , e formalism of Tree Adjoining Grammars ( in the following called TAGs in short ) , which is closely related to context-free gr , 'tmmars ( in the I611owing abbreviated CFG ) , the idea arises to replace the context-free grammar in a Unification Grammar by ; t TAG .</sentence>
				<definiendum id="0">Unification</definiendum>
				<definiens id="0">extension of context-flee grmnm~s. Knowing th , e formalism of Tree Adjoining Grammars ( in the following called TAGs in short ) , which is closely related to context-free gr , 'tmmars ( in the I611owing abbreviated CFG ) , the idea arises to replace the context-free grammar in a Unification Grammar by ; t TAG</definiens>
			</definition>
			<definition id="1">
				<sentence>The ~ecursion operation for TAGs allows the replacement of nodes by a tree ( defined by a TAG-rule ) , so that larger t ; tructure u'ees are processed .</sentence>
				<definiendum id="0">~ecursion operation for TAGs</definiendum>
				<definiendum id="1">tructure u'ees</definiendum>
				<definiens id="0">allows the replacement of nodes by a tree ( defined by a TAG-rule ) , so that larger t ;</definiens>
			</definition>
			<definition id="2">
				<sentence>A TAG is a tree generation system .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">a tree generation system</definiens>
			</definition>
			<definition id="3">
				<sentence>A special leaf ( the foot node ) must exist , labelled with the same nonterminal as the root node is labelled with .</sentence>
				<definiendum id="0">special leaf</definiendum>
				<definiens id="0">the foot node ) must exist , labelled with the same nonterminal as the root node is labelled with</definiens>
			</definition>
			<definition id="4">
				<sentence>A path consists of a number uniquely referring to a constituent in the context-free rule together with a list of feature names and/or an atomic value .</sentence>
				<definiendum id="0">path</definiendum>
			</definition>
			<definition id="5">
				<sentence>To realize this partial interpretation of specification rules , the following sets for each node x are defined : l '' x : = the set of all specification rules with x father or brother of the other mentioned node in that rule , Sx : = the set of all specification rules , where x is a son of the further mentioned node in that rule and 0x : = the set of all specification rules , where a value of x is defined .</sentence>
				<definiendum id="0">x</definiendum>
				<definiens id="0">a son of the further mentioned node in that rule and 0x : = the set of all specification rules</definiens>
			</definition>
			<definition id="6">
				<sentence>\ [ Vijay-Shanker 87\ ] K. Vijay-Shanker : A Study of Tree Adjoining Grammars , PhD Thesis , University of Pennsylvania , Philadelphia , Pennsylvania , 1987 .</sentence>
				<definiendum id="0">K. Vijay-Shanker</definiendum>
				<definiens id="0">A Study of Tree Adjoining Grammars</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>The non-terminals in the phrase structure part of the rule are referenced in the equations as x0 ... xn , where x0 is the non-terminal 1Many members of CMU Center for Machine Translation have made contributions to the development of the system .</sentence>
				<definiendum id="0">x0</definiendum>
				<definiens id="0">the non-terminal 1Many members of CMU Center for Machine Translation have made contributions to the development of the system</definiens>
			</definition>
			<definition id="1">
				<sentence>Masaru Tomita School of Computer Science and Center for Machine Translation Carnegie Mellon University Pittsburgh , PA 15213 , USA mt @ cs.cmu.edu ( &lt; DEC &gt; &lt; = &gt; ( &lt; NP &gt; &lt; VP &gt; ) ( ( ( xl case ) =nom ) ( ( x2 form ) =c finite ) ( *OR* ( ( ( x2 : time ) = present ) ( ( xl agr ) = ( x2 agr ) ) ) ( ( ( x2 : time = past ) ) ) ( x0 = x2 ) ( ( x0 subj ) = xl ) ( ( x0 passive ) = - ) ) ) Figure 1-1 : A Grammar Rule for Parsing in the left hand side ( here , &lt; DEC &gt; ) and xn is the n-th non-terminal in the right hand side ( here , xl represents &lt; NP &gt; and x2 represents &lt; vP &gt; ) .</sentence>
				<definiendum id="0">xn</definiendum>
				<definiendum id="1">x2</definiendum>
				<definiens id="0">A Grammar Rule for Parsing in the left hand side</definiens>
			</definition>
			<definition id="2">
				<sentence>Grammar compilation is the key to this efficient parsing system .</sentence>
				<definiendum id="0">Grammar compilation</definiendum>
				<definiens id="0">the key to this efficient parsing system</definiens>
			</definition>
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>Conceptual Basis of the Lexicon in Machine Translation .</sentence>
				<definiendum id="0">Conceptual Basis</definiendum>
			</definition>
</paper>

		<paper id="3035">
			<definition id="0">
				<sentence>Consider a sample CFT : S ( x.y ) &lt; NP_sing_fem ( x ) &amp; VP sing_fem ( y ) . S ( x.y ) &lt; NP_sing mat ( x ) &amp; VP sing_mat ( y ) . S ( x.y ) &lt; NP sing_neu ( x ) &amp; VP sing_neu ( y ) . S ( x.y ) &lt; NP_plur_fern ( x ) &amp; VP plur_fem ( y ) . S ( x.y ) &lt; NP plur_mal ( x ) &amp; VP plur_mal ( y ) . S ( x.y ) &lt; NP_plurneu ( x ) &amp; VP plurneu ( y ) . In order to prove that agreement can not be captured here , we need to specify what that would mean. It is easy to show that well-defined relations such as numberagree ( number , x , y ) or agree ( number , x , y ) are only definable in CFT if the defining formula.uses disjunction. But traditionally disjunction in grammatical description is a standard notation for two ( or more ) unrelated phenomena. Thus , disjunction is not forbidden , but when it occurs , it impfies the factual claim about the referents of the disjuncts are distinct linguistic phenomena. In the case before us , that would be saying that singular and plural agreement are not the same phenomenon. It is not , of course , the business of logic to inquire into whether in fact number agreement in some language is a unitary phenomenon. It is rather the business of ' logic to provide the tools for the linguist scientist who , on whatever basis , makes such determinations , to capture formally the theories that he develops , Accordingly , we first assume a special notion of definability , defined as follows : Definition. A relation p is &amp; -definable in a CFT theory T if there is a formula ( * ) / ' ( x , , ... , x , , ) , - &amp; ... &amp; &amp; . B ( x , x i ... . ) &amp; ... 196 2 s.t. the tuples ( a , ... . , a~ ) , which can be proven from T+ ( * ) to satisfy P ( ... ) , are exactly those belonging to the relation p. A category c is &amp; -definable in a CFT theory T if the one argument relation corresponing to c is. ltowever , some categories ttms defined are spurious in that they can not be used in proving sentencehood. We want to rule these out. I.et Lnf ( G ) be the language that contains only those categories which appear in the formulas \ [ S , \ [ ... \ ] \ ] which are derivable in. ( the CFT con'esponding to ) the grammar. From now on , by ( &amp; - ) definability we shall understated the ( &amp; - ) definability in L~G ) . Moreover , the notion of 'category ' will be analogously restricted. Tiffs allows us to avoid spurious categories as in 2.4 below. We wil ! also refer to constructions which are I.wo~place relations between a grammatical category ( the category the construction yields ) and a string , ff grmnmatical categories ( which the construction i : ; made up of ) . ( ; onsider a g ) -amrnar like : S o &gt; NPsg VPsg ~ ; - &gt; NPplVPpl NPs- &gt; Det Ns NPp- &gt; Det Np NP &gt; Det N ins &gt; dog , cat ... . Np &gt; dogs , cats ... . N &gt; dog , dogs , cat , cats ... . It is possible to introduce a symbol that corresponds to the category NP , but it c , 'mnot be used in deriving sentences from the stm't symbol .</sentence>
				<definiendum id="0">NP_sing mat</definiendum>
				<definiens id="0">a standard notation for two ( or more ) unrelated phenomena. Thus , disjunction is not forbidden , but when it occurs , it impfies the factual claim about the referents of the disjuncts are distinct linguistic phenomena. In the case before us , that would be saying that singular and plural agreement are not the same phenomenon. It is not , of course , the business of logic to inquire into whether in fact number agreement in some language is a unitary phenomenon. It is rather the business of ' logic to provide the tools for the linguist scientist who , on whatever basis , makes such determinations , to capture formally the theories that he develops</definiens>
				<definiens id="1">the language that contains only those categories which appear in the formulas \ [ S , \ [ ... \ ] \ ] which are derivable in. ( the CFT con'esponding to ) the grammar. From now on , by ( &amp; - ) definability we shall understated the ( &amp; - ) definability in L~G ) . Moreover , the notion of 'category ' will be analogously restricted. Tiffs allows us to avoid spurious categories as in 2.4 below. We wil ! also refer to constructions which are I.wo~place relations between a grammatical category ( the category the construction yields ) and a string , ff grmnmatical categories ( which the construction i : ; made up of ) . ( ; onsider a g ) -amrnar like : S o &gt; NPsg VPsg ~ ; - &gt; NPplVPpl NPs- &gt; Det Ns NPp- &gt; Det Np NP &gt; Det N ins &gt; dog , cat ...</definiens>
				<definiens id="2">possible to introduce a symbol that corresponds to the category NP , but it c , 'mnot be used in deriving sentences from the stm't symbol</definiens>
			</definition>
			<definition id="1">
				<sentence>That is , they can not &amp; -define a relation R ( Cat , x ) , where Cat is some grammatical category and x ranges over a set of strings of categories identical except with respect to word order .</sentence>
				<definiendum id="0">Cat</definiendum>
				<definiens id="0">some grammatical category and x ranges over a set of strings of categories identical except with respect to word order</definiens>
			</definition>
			<definition id="2">
				<sentence>Now , we make one more restriction : let LnJ ( NP ) be the language that contains only those categories except NP which appear in the RHS of the formulas \ [ NP , \ [ RIfS \ ] \ ] wtfich are derivable in ( the CFT corresponding to ) the grammar .</sentence>
				<definiendum id="0">LnJ</definiendum>
				<definiens id="0">the language that contains only those categories except NP which appear in the RHS of the formulas \ [ NP</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus , we write rules like , where ont is a parameter controling 'the appearance or absence of an element ( i.e. , +ont ( X ) means X appears , -ont ( JO that it does not ) , V VP-~ \ [ c~ active\ ] ~ ont ( NP ) Now we can &amp; -define the different kinds of transitive constructions , by using ont to control whether the object NP is realized ( in the active ) or null ( in the passive ) .</sentence>
				<definiendum id="0">ont</definiendum>
				<definiens id="0">a parameter controling 'the appearance or absence of an element ( i.e. , +ont ( X ) means X appears , -ont ( JO that it does not )</definiens>
			</definition>
			<definition id="4">
				<sentence>Let q '' be a collection of trees ( over some alphabet with terminals , non-terminals , and perhaps other symbols ) , where each tree is a pair \ [ Node , Sons\ ] , where Sore is a list of trees .</sentence>
				<definiendum id="0">Sore</definiendum>
				<definiens id="0">a collection of trees ( over some alphabet with terminals , non-terminals , and perhaps other symbols ) , where each tree is a pair \ [ Node , Sons\ ] , where</definiens>
			</definition>
			<definition id="5">
				<sentence>• In the case of TGs a proof of a formula I~ is a sequence ( P , Q ) where such that each F~ is a fommla corresponding to a context free rule of the grammar or is obtained from \ [ } , ( j &lt; i ) by the rule of substitution , and l ; k is \ [ S ( ... ) , \ [ RIlS\ ] \ ] , where RIIS contains only the terminal symbols ( i.e. Fk represents a fully expanded tree ) .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">is \ [ S ( ... ) , \ [ RIlS\ ] \ ] , where RIIS contains only the terminal symbols ( i.e. Fk represents a fully expanded tree )</definiens>
			</definition>
</paper>

		<paper id="2040">
			<definition id="0">
				<sentence>The representation consists of one or mo ; e interpretations , and each interpretation , in turn , of a base form and a set of morphosyntactic features , eg .</sentence>
				<definiendum id="0">representation</definiendum>
			</definition>
			<definition id="1">
				<sentence>imperatives only in sentence initial positions , negative forms require a negation , AD-A requires an adjective or adverb to follow ; etc. ) ° Clause boundary constraints ( relative pronouns and certain conjunctions are preceded by a boundary , even other boundaries need some explicit clue to justify their possibility ) .</sentence>
				<definiendum id="0">AD-A</definiendum>
				<definiens id="0">requires an adjective or adverb to follow ; etc. ) ° Clause boundary constraints</definiens>
			</definition>
			<definition id="2">
				<sentence>( Here COMMA is a feature associated with the punctuation token , and COORD a feature present in coordinating conjunctions . )</sentence>
				<definiendum id="0">COMMA</definiendum>
				<definiens id="0">a feature associated with the punctuation token</definiens>
			</definition>
			<definition id="3">
				<sentence>Two-level morphology : A general computational model for word-form recognition and production .</sentence>
				<definiendum id="0">Two-level morphology</definiendum>
				<definiens id="0">A general computational model for word-form recognition and production</definiens>
			</definition>
</paper>

		<paper id="3049">
			<definition id="0">
				<sentence>Define 2 an n-ary nondeterminisiic finite automaton as a 5-tuple A = ( Q , qt , F , E , H ) where Q is a finite non-empty set of states , ql is a designated start state , F is a set of designated final states , E is a finite non-empty alphabet , and H is a finite subset of { Q × ( E* ) '' × Q } , where ( : E* ) n is the set of n-tuples of ( possibly empty ) words over E..</sentence>
				<definiendum id="0">ql</definiendum>
				<definiendum id="1">F</definiendum>
				<definiendum id="2">E</definiendum>
				<definiendum id="3">H</definiendum>
				<definiens id="0">a 5-tuple A = ( Q , qt , F , E , H ) where Q is a finite non-empty set of states</definiens>
				<definiens id="1">a designated start state</definiens>
				<definiens id="2">a set of designated final states</definiens>
				<definiens id="3">a finite non-empty alphabet , and</definiens>
				<definiens id="4">a finite subset of { Q × ( E* ) '' × Q }</definiens>
			</definition>
			<definition id="1">
				<sentence>Using a state labeled 1 by convention as the start state , and a state labeled 0 by convention as the ( unique ) final state , we express all of the information needed to define our automaton .4 by enumerating the arcs in H , which now can be represented as lists of 4-tuples ( qi , qj , u , v ) , where qi and qj are arbitrary identifiers for states , u is a substring of an inflected form , and v is a substring of the corresponding lemma + morphosyntactic category .</sentence>
				<definiendum id="0">v</definiendum>
				<definiens id="0">the arcs in H , which now can be represented as lists of 4-tuples ( qi , qj , u , v ) , where qi and qj are arbitrary identifiers for states , u is a substring of an inflected form , and</definiens>
			</definition>
			<definition id="2">
				<sentence>-~ } ° ° o ~ : '-,3 ~ , = ' , % &lt; Z -- iJ ltl Ol Ii &amp; ii , , . : ... , ~N ... . ... .. , . , , . ... .. ~j , . `` . : . : . : . ; , . : .. , , NNri~l~i~l~u -- .i ; i~l , ... . ~lN , l~.Jl ' , -- . '' : .~lP-.Itl II oo t~ • -~ .~ ii~- , ,~-.~- , , , ~ ~~..I la.i~. , i~ ~lll .~ , ~. , ~ , r. , ii~/ , . , ~.. ~ , t , i , ~.-~ , , , .~ ~ '' ~ .. , ~ , ~C~ 0~1 .. , .~ ..-.. ~ ui~ ' ~ Ill i~i i , ~ C ~ ~ il i~ , '' ii71 `` il~i Cll II ii~. &lt; i.i i , ,.i , i~ ill ilU Ill i~ , , : '. : : , , - , ,. tt ..~ i , ~mm .. la iI ..l= ltl.~ , - , -l~l ti.~t , ~ T J o ~ tG ~. I~ ~ , . , ,.- , , , -- , ~- , '' -~~ , ~-~ , i % ~~-'l : i~tt~.~ `` ~i ! t~ ~ , ~.I~-~ ~=~ '~ =~ ~ iti Ill i~ lfa llu II `` ~l a. , ,i ~l I~l Ill Ilii , ... .I 282 6 Casajuana R. and C. Rodr { guez 1985. Clasificacidn de los verbos castellanos pars un dic. cionario en ordenador. I congreso de lenguajes naturales y lenguajes formales. Universidad de Barcelona. Facultad de Filologin. Departamento de Lingii { stica General. Barcelona. Chomsky , N. 1962. Couiext-free Grammars and Pushdown Storage , M.I.T. Research Laboratory of E\ ] ectronics Quarterly Progress Report # 65 , pp. 187q93. Collins Spanish Dictionary : Spanish-English. Collins Pnbl ! ishers , Glasgow , 1989. Corbin D. 11 ) 87. Morphologie d &amp; ivationnelle et structural : ion du lexique. Niemeyer Verlag : Tubingen. Elgot , C.C. and J.E. Mezei 1965. On Relations Defined by Generalized Finite Automata , IBM Journal Res. 9 , pp. 47-68. Feigenbaum , J. , M.Y. Liberman , R.N. Wright ( forthcoming ) . Cryptographic Protection of Databases and Software. In Proceedings of the DIMACS Workshop on Distributed Comput~ ing and Cryptography , Feigenbaum and Merritt , Eds. AMS and ACM. Ginsburg , S. 1966. 7~e Malhema ! ical Theory of Context-Free Languages , McGraw Hill. Kay , M. 1982. When Meta-rules are not Melarules. In Spark-Jones &amp; Wiiks ( eds. ) Automatic Natural Lang~tage Processing. University of Essex , Cognitive Studies Center ( CSM\ ] 0 ) . Kartunnen , L. 1983. KIMMO : A general morphological processor. Texas Linguistic Forum , No. 22 pp 165-185. Kartunnen , L. , K. Koskenniemi , R. Kaplan 1987. A Compiler for Two-level Phonological Rules. Ms. Xerox Palo Alto Research Center. Khan R. 1983. A two-level morphological analysis of t ? .oumanian. Texas Linguistic Forum , No. 22 pp 153-170. Koskenniemi , K. 1983. Two-level morphology : A General Computational Model for WordForm Recognition and Production. University of Iielsinki , Dept. of General Linguistics , Publications , No. 11. Koskenniemi , K. , K. W. Church 1988. Complexity , Two-level morphology and Finnish. Proceedings of the 12th International Conference on Computational Linguistics. Budapest , Hungary. Lun S. 1983. A two-level morphological analysis of French. Texas Linguistic Forum , No. 22 pp 271-277. Rabin , M.O. and D. Scott , 1959. Finite Automata and their Decision Problems , IBM J. Res. 3 , pp. 114-125. Schiitzenberger , M.P. 1961. A Remark on Finite Transducers , Information and Control 4 , pp. 185-196. Tzoukermann E. , R. Byrd 1988. The Applica~ lion of a Morphological Analyzer to o~.line French Dictionaries. Proceedings of the h &gt; ternational Conference on Lexicography , Euralex .</sentence>
				<definiendum id="0">NNri~l~i~l~u</definiendum>
				<definiens id="0">il~i Cll II ii~. &lt; i.i i , ,.i , i~ ill ilU Ill i~ , , : '. : : , , - , ,. tt ..~ i , ~mm .. la iI ..l= ltl.~ , - , -l~l ti.~t , ~ T J o ~ tG ~. I~ ~ , . , ,.- , , , -- , ~- , '' -~~</definiens>
				<definiens id="1">A general morphological processor. Texas Linguistic Forum , No. 22 pp 165-185. Kartunnen , L. , K. Koskenniemi , R. Kaplan 1987. A Compiler for Two-level Phonological Rules. Ms. Xerox Palo Alto Research Center. Khan R. 1983. A two-level morphological analysis of t ? .oumanian. Texas Linguistic Forum , No. 22 pp 153-170. Koskenniemi , K. 1983. Two-level morphology : A General Computational Model for WordForm Recognition and Production. University of Iielsinki , Dept. of General Linguistics , Publications , No. 11. Koskenniemi , K. , K. W. Church 1988. Complexity , Two-level morphology and Finnish. Proceedings of the 12th International Conference on Computational Linguistics. Budapest , Hungary. Lun S. 1983. A two-level morphological analysis of French. Texas Linguistic Forum</definiens>
			</definition>
</paper>

		<paper id="2049">
			<definition id="0">
				<sentence>The knowledge base can be constructed semi-automatically , since the source of knowledge exists in the form of texts , and these sentences can be analyzed by the parser and transformed into dependency structures by the system .</sentence>
				<definiendum id="0">knowledge base</definiendum>
				<definiens id="0">since the source of knowledge exists in the form of texts</definiens>
			</definition>
			<definition id="1">
				<sentence>The Dependency Analyzer is a systenl fl ) r structural disambignation .</sentence>
				<definiendum id="0">Dependency Analyzer</definiendum>
				<definiens id="0">a systenl fl</definiens>
			</definition>
			<definition id="2">
				<sentence>A Tree consists of a Node and reeursions ( or null ) of Tree , and a Node consists of repetitions of a paired attribute name and u.ttribute value .</sentence>
				<definiendum id="0">Tree</definiendum>
				<definiens id="0">consists of a Node and reeursions ( or null ) of Tree , and a Node consists of repetitions of a paired attribute name and u.ttribute value</definiens>
			</definition>
			<definition id="3">
				<sentence>\Ve developed an interactive tree management tool , the Tree Editor .</sentence>
				<definiendum id="0">\Ve</definiendum>
			</definition>
			<definition id="4">
				<sentence>The dependency distance is computed from the following formula : Distance = \ [ Depl + ~c'o , , , x ( n1 ) ( t~ , ... .. + 1 ) x ( l @ oo~ + 1 ) ' where \ ] Dep\ ] represents the number of dependencies included in the palh , i '' c ... .. is the value of case consistency , 1 ~'oo~ .</sentence>
				<definiendum id="0">dependency distance</definiendum>
				<definiens id="0">computed from the following formula : Distance = \ [ Depl + ~c'o , , , x ( n1 ) ( t~ , ... .. + 1 ) x ( l @ oo~ + 1 ) ' where \ ] Dep\ ] represents the number of dependencies included in the palh</definiens>
			</definition>
			<definition id="5">
				<sentence>n is a real number in tile range 0 &lt; n &lt; 1 ; it is a heuristic parameter that represents the degree of unimportance of context consistency .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">a heuristic parameter that represents the degree of unimportance of context consistency</definiens>
			</definition>
			<definition id="6">
				<sentence>The constraint table is a two-dimensional matrix that represents the .</sentence>
				<definiendum id="0">constraint table</definiendum>
				<definiens id="0">a two-dimensional matrix that represents the</definiens>
			</definition>
			<definition id="7">
				<sentence>The algorithm for plauning consists of the following steps : element values ( Ai in Table 2 ) , and subtract the sum from the size of the row ( Bi ) .</sentence>
				<definiendum id="0">algorithm for plauning</definiendum>
				<definiens id="0">consists of the following steps : element values ( Ai in Table 2 ) , and subtract the sum from the size of the row ( Bi )</definiens>
			</definition>
			<definition id="8">
				<sentence>The Dependency Stracturc Builder transhttes the phrase structure int .</sentence>
				<definiendum id="0">Dependency Stracturc Builder</definiendum>
				<definiens id="0">transhttes the phrase structure int</definiens>
			</definition>
			<definition id="9">
				<sentence>The Planner , which is the component for planning , gives the Disambiguator the information on an ambiguous dependency and its candidate modifiees .</sentence>
				<definiendum id="0">Planner</definiendum>
				<definiens id="0">the component for planning , gives the Disambiguator the information on an ambiguous dependency and its candidate modifiees</definiens>
			</definition>
			<definition id="10">
				<sentence>Whereas this framework ( and also that of Wilks et al. ) was aimed at disambiguating single prepositional phrases in sentences , our approach can handle the attachments of multiple prepositional phrases in sentences , ttirst \ [ 3\ ] developed a mechanism for structural disambiguation , called the Semantic Enquiry Desk , which is based on Chraniak 's marker passing paradigm \ [ 1\ ] .</sentence>
				<definiendum id="0">Enquiry Desk</definiendum>
				<definiens id="0">ttirst \ [ 3\ ] developed a mechanism for structural disambiguation , called the Semantic</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>ANALYSIS Terminology , the discipline concerned with the formation , description and naming of concepts in specialized fields of knowledge , is a key component of the general documentation process : it is normally preceded by knowledge acquisition ( usually not formalized ) , and followed by document preparation .</sentence>
				<definiendum id="0">ANALYSIS Terminology</definiendum>
				<definiens id="0">the discipline concerned with the formation , description and naming of concepts in specialized fields of knowledge , is a key component of the general documentation process</definiens>
			</definition>
			<definition id="1">
				<sentence>CODE employs a flexible knowledge representation which permits considerable variety in style and degree of formality .</sentence>
				<definiendum id="0">CODE</definiendum>
				<definiens id="0">employs a flexible knowledge representation which permits considerable variety in style and degree of formality</definiens>
			</definition>
			<definition id="2">
				<sentence>A property is a unit of information that characterizes a concept , corresponding roughly to a succinct declarative sentence .</sentence>
				<definiendum id="0">property</definiendum>
				<definiens id="0">a unit of information that characterizes a concept , corresponding roughly to a succinct declarative sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>CODE 's browsing facility , the Property Browser , allows the terminologist to `` navigate '' easily between concepts , between properties , and between concepts and properties .</sentence>
				<definiendum id="0">Property Browser</definiendum>
			</definition>
			<definition id="4">
				<sentence>Documentation is an essential aspect of the software production process , but unlortunately it is often not treated with sufficient care .</sentence>
				<definiendum id="0">Documentation</definiendum>
				<definiens id="0">an essential aspect of the software production process , but unlortunately it is often not treated with sufficient care</definiens>
			</definition>
			<definition id="5">
				<sentence>The translation process is a great bottleneck for efficient inter-linguistic knowledge transfer : both human and machine translation are greatly enhanced by correct terminology and conceptual clarity .</sentence>
				<definiendum id="0">translation process</definiendum>
				<definiens id="0">a great bottleneck for efficient inter-linguistic knowledge transfer : both human and machine translation are greatly enhanced by correct terminology and conceptual clarity</definiens>
			</definition>
</paper>

		<paper id="3067">
			<definition id="0">
				<sentence>i. INTRODUCTION Since ARABIC is a rich and highly inflected language \ [ 3\ ] , there are grannnatical categories in ARABIC which do not exist in other foreign languages such as in ENGLISH \ [ 4\ ] .</sentence>
				<definiendum id="0">INTRODUCTION Since ARABIC</definiendum>
				<definiens id="0">a rich and highly inflected language</definiens>
			</definition>
			<definition id="1">
				<sentence>LINGUISTICS As the need increases to a model of A~ % BIC , ARABIC linguists try to re-organize the granmlatic system in a way that may help the researchers in the field of computational linguistics .</sentence>
				<definiendum id="0">ARABIC</definiendum>
				<definiens id="0">linguists try to re-organize the granmlatic system in a way that may help the researchers in the field of computational linguistics</definiens>
			</definition>
			<definition id="2">
				<sentence>It is evident that : , the ARABIC possesses a certain symmetry as regards its structure , which leads itself easily to computation .</sentence>
				<definiendum id="0">ARABIC</definiendum>
				<definiens id="0">leads itself easily to computation</definiens>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>To summarize this problem , we can say that there are two main sources of nondeterminism ( and henceforth of actual computational complexity ) in GPSG : 19 ( 1 ) ID-rules : derivation is a non-detenninistic process possibility of null transition ( rules with empty righthand sides ) , permitting large structures with few `` supporting '' terminals unordered right-hand sides ( 2 ) Metarules : induction of null transition and ambiguity exponential increase of ID-rules non-deterministic application to ID-rules There are several other factors of complexity .</sentence>
				<definiendum id="0">derivation</definiendum>
				<definiens id="0">a non-detenninistic process possibility of null transition ( rules with empty righthand sides ) , permitting large structures with few `` supporting '' terminals unordered right-hand sides ( 2 ) Metarules : induction of null transition and ambiguity exponential increase of ID-rules non-deterministic application to ID-rules There are several other factors of complexity</definiens>
			</definition>
			<definition id="1">
				<sentence>The interpretation of a linguistic theory consists in the adaptation of the abstract model to make it compulationally tractable .</sentence>
				<definiendum id="0">interpretation of a linguistic theory</definiendum>
				<definiens id="0">consists in the adaptation of the abstract model to make it compulationally tractable</definiens>
			</definition>
			<definition id="2">
				<sentence>A compilaticn of a GSPG consists in several expansion steps which transform it into a context-free equivalent grammar .</sentence>
				<definiendum id="0">compilaticn of a GSPG</definiendum>
				<definiens id="0">consists in several expansion steps which transform it into a context-free equivalent grammar</definiens>
			</definition>
			<definition id="3">
				<sentence>Hence , we can define a function head from P ( KuX ) to K ( where K is the set of categories,2J the set of terminals and P ( X ) denotes the set of all subsets of X ) as follow : let an ID-rule of the form A -- - &gt; ai C1 ... .. C , ~ , then : head ( A ) = { Ci / ( Ci ( \ [ N1 ) = A ( IN\ ] ) ) and ( Ci ( \ [ W ) = A ( \ [ W ) ) \ ] let G be a GPSG , R be the set of ID-rules of G , andr~ R , r is of the form s -- 9 t , cl ... .. cj ( with 0 _ &lt; i _~j ) , where t is the head of s we define the following operations on R : indistinguishable. 20 2 ( 1 ) left-lhand side of a rule : utS ( r ) = { s ) ( 2 ) right-hand side of a rule : RHS ( r ) = { t , ci ... .. cj } ( 3 ) reduced right-hand side of a rule : RHS ' ( r ) = { ci ... .. cj } ( 4 ) rule inclusion ( noted ~ ) let rl , r2 e R , rl ~_ r2 iff LHS ( rl ) = LtlS ( r2 ) , head ( r1 ) = head ( r2 ) and RIlS ( rl ) ~_ RHS- ( r2 ) We define a rule clustering function F from R to R as follows : F ( r ) = { r i E R / r i ~r vr ~ri } Hence , , an extensive D-rule is define as follows : let r e R , r is an extensive ID-rule iff V r ' ~ F ( r ) , r ' ~ r Such a formalization of the grammar considerably reduces the problem of non-determinism during the selection of a rule : if two rules are different , their righthand sides have at most one element in common. This allows us to establish strong selection constraints. To summm'ize , using extensive ID-rules allows a very high level of generality for the representation of a GPSG , preserving its succinctness property. The Bottom-Up Filtering strategy is based on the detection of the first constituent of a phrase. \ [ Pereira871 , in a presentation of bottom-up parsing , describes the left-corner parsing method. This strategy was first introduced in \ [ Rosenkrantz70\ ] . It consists in finding the leftmost constituent ~ of a phrase P , so as to select a phrase structure rule P -9 c~ ~ and then proving that ~ is actually the left-corner of such a phrase by application of the rest ( N ) of the selected rule. There are two stages in the process : a bottom-up one ( detecting the left corner ) and a top-down one ( parsing the rest of the phrase ) . Using both strategies is interesting , particularly for the selection of a phrase structure rule : knowledge of the leftmost constituent constrains this stage and so reduces non-determinism. Hence , this strategy , like ours , is based upon the detection of the leflmost constituent of a pttrase. But the similarity stops here : the use of unordered rules , inherent to the ID/LP formalism , would force modification and introduction of new mechanisms. Moreover , this strategy allows only a small reAuction of non-determinism , especially because the top-down stage is used in a classical way. Based on our interpretation of GPSG and the formalization of extensive ID-rules we propose a strategy that , allows the initialization of the phrase level upon determination of the leftmost constituents. After this bottom-up stage , the parse is completed by a topdown process consisting in the selection of the adequate extensive ID-rules and the generation of phrasestructure rules. We insist on the fact that we do n't use expansion or a selection function for this last stage , but a genuine generation process : the rules are actually deduced by formal operations from the grammar. This stage is largely constrained by both our formalization and the bottom-up filtering that initializes the phrases. We obtain a strategy in which non-determinism is drastically reduced. Bottom-Up Filtering parsing is achieved in three stages : ( i ) creation of prediction tables ( ii ) phrase level initialization ( iii ) generation of phrase-structure rules Using file extensive ID-rule formalization , we deduce informations that will allow us to determine the leftmost constituent. We use two main concepts : first legal daughter and immediate precedence. Definition : the first legal daughter of a constituent is a category of any level that can occur in the first position of the right-hand side of a phrase-structure rule describing thi , ~ constituent. So , according to LP constraints , a given constituent may have scwzral first lcgal daughters which we collect into a set. We note &lt; the linear precedence relation. Let P be a phrase , V oc such that P -- ~ ~ then First , the set of first legal daughters , is defined as follows : First ( P ) = { c ~ o~ ... ... .. such that Vx ~ a- { c } , then c &lt; x } t The second concept , the immediate precedence relation , allows us to determine all the constituents that can precede , according with LP constraints , a first legal 3 21 daughter ~n a right-hand side of ID-rule. These constituents can themselves be first legal daughters of the considered phrase , or not. The reason is , particularly when using the extensive ID-rules formalism , that several ID-rules describe several different constructions of a given phrase type. So , there may be constituents that can not initialize a phrase but , in some constructions , that can precede a constituent which is actually a first daughter in another construction. This relation defines sets of immediate precedence ; as follows : Let P be a phrase , k/a such that P -- ~ a , let x be a non-terminal , let c ~ First ( P ) , then IPp ( c ) , the set of immediate precedence of c for P , is defined as follows : \ [ Pp ( c ) = { x such that ( x &lt; c ) or ( x e a and neither x &lt; c nor c &lt; x exist ) } Prediction tables are made of the sets of first legal daughters for all phrases and those of immediate precedence for each first legal daughter. These sets are specified during the implementation of the grammar. Note that this is not a compilation of the grammar , because we only have to determine the leftmost constituents for the rules , whereas compilation would generate all the possible permutations for entire rules. The sets are thus kept reasonably small. With the aid of the prediction tables , we can now describe the mechanisms used in the initialization of the phrase level. This consists in determining all the first daughters in the input sentence , and so all the phrases belonging to the syntactic structure. This stage consists in two phases : categorization and actual initialization. The categorization is a trivial function , used in all bottom-up strategies , which we enhance with a special device for easier resolution of lexical ambiguities : the resulting data are stored as possible backtracking points for our parser. The initialization stage is based upon a simple principle : an element of the sequence of categories is a first daughter of a phrase if it belongs to the set of first legal daughters of this phrase and if the previous category does not belong to its immediate precedence set. We define the initialize relation as follows : Let G be a GPSG , L ( G ) the language generated by G , let I be a string such that I ~ L ( G ) , let C the list of categories of I , k/ c ~ C , -~ c ' ~ N such that c ' precedes c in C ; c initialize S iffc ~ First ( S ) and c ' ¢~ IPs ( c ) This stage yields a new list made of the lexical categories and the initialized phrases. We can give a very simple example of phrase level initialization. Let G a very small ID/LP grammar : Extensive ID-rules : S -- ) id NP , VP NP '- ) id Det , N , AP , PP NP -- ~'id N VP -- ~ id V , NP , PP AP -~id Adj PP ~id Prep , NP LP-rules ( given here in a binary formalization ) • V &lt; NP V &lt; PP NP &lt; VP Det &lt; N Det &lt; AP Det &lt; PP N &lt; SP Prep &lt; NP Sets of First Legal Daughter : First ( S ) = { NP } First ( NP ) = { Det , N } First ( VP ) = { V } First ( PP ) = { Prep } First ( AP ) = { Adj } Sets of Immediate Precedence : IPs ( NP ) = O IPvp ( V ) = O IPNp ( Det ) = O IPNp ( N ) = { Det , AP } IPAP ( Adj ) = O IPpp ( Prep ) = O PHRASE LEVEL INITIALIZATION Let the sentence : Peter walks down the street. ( 1 ) Categorization : N . V . Prep. Det . N ( 2 ) Phrase level initialization : Current cat N V Prep Det L N First ( P ) Precedent cat NP VP N ' PP V NP Prep NP Det lPp ( c ) Action N/NP O V/VP p~. ! ~ O Det / NP We obtain the following list : S. &lt; NP , N &gt; .</sentence>
				<definiendum id="0">P ( KuX ) to K ( where K</definiendum>
				<definiendum id="1">P ( X )</definiendum>
				<definiens id="0">the set of categories,2J the set of terminals</definiens>
			</definition>
			<definition id="4">
				<sentence>If a category does not belong to the pattern rule , it can be , an indirect constituent ( i.e. a category belonging to a constituent itself belonging itself to the phrase which is being parsed ) .</sentence>
				<definiendum id="0">indirect constituent</definiendum>
				<definiens id="0">a category belonging to a constituent itself belonging itself to the phrase which is being parsed )</definiens>
			</definition>
</paper>

		<paper id="3098">
			<definition id="0">
				<sentence>Leader is a generator of nalural lmlguage interfaces .</sentence>
				<definiendum id="0">Leader</definiendum>
			</definition>
			<definition id="1">
				<sentence>In this case , Leader produces a message expliciting the type associated to the expected expressions .</sentence>
				<definiendum id="0">Leader</definiendum>
				<definiens id="0">produces a message expliciting the type associated to the expected expressions</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , after the incomplete question : Give the persons who received the Nobel prize of Physics before ... Leader will synthesize : &lt; enter a year , example . '' 1945 &gt; &lt; enter a person , example : Einstein &gt; Concerning variable symlmls , Leader displays messages when running in synflmsis , but collects and parses expressions when running in analysis .</sentence>
				<definiendum id="0">Leader</definiendum>
				<definiens id="0">displays messages when running in synflmsis , but collects and parses expressions when running in analysis</definiens>
			</definition>
			<definition id="3">
				<sentence>Partial synthesis is an interesting challenge when one decides to use the same system for analysing and synthesizing sentences .</sentence>
				<definiendum id="0">Partial synthesis</definiendum>
				<definiens id="0">an interesting challenge when one decides to use the same system for analysing and synthesizing sentences</definiens>
			</definition>
</paper>

		<paper id="3061">
</paper>

		<paper id="3042">
			<definition id="0">
				<sentence>A word lattice is a set of words hypothesized t , ~y a speech recognition system from an utterance .</sentence>
				<definiendum id="0">word lattice</definiendum>
				<definiens id="0">a set of words hypothesized t , ~y a speech recognition system from an utterance</definiens>
			</definition>
			<definition id="1">
				<sentence>A typical word lattice consists of 30 200 words for a 10 word utterance , and each word has a score indicating probability of its having been actually uttered .</sentence>
				<definiendum id="0">typical word lattice</definiendum>
			</definition>
			<definition id="2">
				<sentence>Parsing : A Tabular Method .</sentence>
				<definiendum id="0">Parsing</definiendum>
			</definition>
</paper>

		<paper id="3074">
			<definition id="0">
				<sentence>XTRA : The design and implementation of a fully automatic machine translation system .</sentence>
				<definiendum id="0">XTRA</definiendum>
				<definiens id="0">The design and implementation of a fully automatic machine translation system</definiens>
			</definition>
</paper>

		<paper id="3062">
			<definition id="0">
				<sentence>These checks and repairs have been studied by people working in the field of conversation analysis ( CA ) for many years .</sentence>
				<definiendum id="0">CA</definiendum>
				<definiens id="0">studied by people working in the field of conversation analysis</definiens>
			</definition>
			<definition id="1">
				<sentence>S : OK , A light-detector-circuit is a kind of potential-divider circuit .</sentence>
				<definiendum id="0">light-detector-circuit</definiendum>
				<definiens id="0">a kind of potential-divider circuit</definiens>
			</definition>
</paper>

		<paper id="3066">
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>Cependant , ils sont en g6n6ral insuffisants pour identifier de mani~re unique un r6f6rent .</sentence>
				<definiendum id="0">Cependant</definiendum>
				<definiens id="0">ils sont en g6n6ral insuffisants pour identifier de mani~re unique un r6f6rent</definiens>
			</definition>
			<definition id="1">
				<sentence>A partir d'exp6riences faites sur l'6valuation d'un temps de r6ponse h propos de personnes cit6es dans un texte , apr~s lecture des deux phrases formant ce texte , la deuxi~me contenant un pronom r6f6rent ~ un 616ment variable de la premiere ( \ [ Corbett &amp; Chang 83\ ] ) , les auteurs d6duisent que le sujet ( en temps que r61e fonctionnel tenu par un synmgrne ) de la premiere phrase est plus accessible h la fm du texte que le nom pr6dicat ( ici , un compl~ment d'objet direct ) de cette m6me phrase , ind6pendemment des effets de la r6f6rence .</sentence>
				<definiendum id="0">ind6pendemment</definiendum>
				<definiens id="0">r61e fonctionnel tenu par un synmgrne ) de la premiere phrase est plus accessible h la fm du texte que le nom pr6dicat ( ici , un compl~ment d'objet direct ) de cette m6me phrase ,</definiens>
			</definition>
			<definition id="2">
				<sentence>Correspond au crit~res de relations formelles entre phrases utilis6 pour la synth~se dans \ [ Danlos 85\ ] ) d ) L'antEcEdent d'un pronom le plus probable est celui qui est le plus prEs ( \ [ St Dizier 86\ ] et Principe de ProximitE dans \ [ Guenthner &amp; Lehmann 83\ ] ) Remarquons d'abord que notre crit~re 2 est une gEnEralisation de a ) et b ) .</sentence>
				<definiendum id="0">prEs</definiendum>
				<definiens id="0">L'antEcEdent d'un pronom le plus probable est celui qui est le plus</definiens>
			</definition>
			<definition id="3">
				<sentence>E16ments d'Analyse dans le Rtcit '' Langue Fran¢aise 78 pp 67-128 ( 1988 ) \ [ McKoon &amp; Ratcliff 80\ ] McKoon G. et Ratcliff R. `` The Comprehension processes and Memory Structures involved in \ [ Morrow 85\ ] Morrow D.G `` Pr61~ositions and Verbe Aspects in Narrative Understanding '' Journal of Memory and Language Vol 24 , pp 390-404 ( 1985 ) \ [ Murphy 84\ ] Murphy G.L `` Establishing and Accessing Referents in Discourse '' Memory and Cognition 12 ( 5 ) , pp 489497 ( 1984 ) \ [ Reinhart 83\ ] Remhart T. `` Coreference and Bound Anaphora : A Restatement of the Anaphora Questions '' Linguistics and Philosophy Vol .</sentence>
				<definiendum id="0">Memory Structures</definiendum>
			</definition>
			<definition id="4">
				<sentence>Faeult6 des Sciences de Luminy ( 1989 ) \ [ Saint-Dizier 86\ ] Saint-Dizier P. `` R6solution des anaphores et Programrrmtion en Logique '' Papier Pr61iminaire \ [ Sedogbo 87\ ] Sedogbo C. `` SYLOG : A DRT System in Prolog '' Second International Workshop on Natural Language and Logic Programming Simon Fraser University , Vancouver , B.C Canada ( 1987 ) \ [ Sidner 83\ ] Sidner C. `` Focusing in the comprehension of definite Anaphora '' in Computational Models of Discourse Brady &amp; Berwick eds .</sentence>
				<definiendum id="0">SYLOG</definiendum>
				<definiens id="0">A DRT System in Prolog '' Second International Workshop on Natural Language and Logic Programming Simon Fraser University</definiens>
			</definition>
</paper>

		<paper id="3016">
			<definition id="0">
				<sentence>This deficiency comes out clearest in propositional attitude constructions , i.e. constructions of the form 'x V that S ' ; where V is an epistemic verb ( 'knows ' , ~believes ' ) and S a sentence .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">an epistemic verb</definiens>
			</definition>
			<definition id="1">
				<sentence>D.Lewis , for instance , used semantically interpreted phrase markers ( roughly : syntax trees with logical formulas attached to the nodes ) as meanings for natural language expressions ( \ [ Lewis 1972\ ] ) .</sentence>
				<definiendum id="0">D.Lewis</definiendum>
			</definition>
			<definition id="2">
				<sentence>Thus , each D-tree is associated with a semantic tree ( M-tree ) , whose nodes are semantic rules and whose leaves are non-logical constants .</sentence>
				<definiendum id="0">D-tree</definiendum>
				<definiens id="0">a semantic tree ( M-tree ) , whose nodes are semantic rules and whose leaves are non-logical constants</definiens>
			</definition>
			<definition id="3">
				<sentence>For instance , the English Noun Phrase 'ItMian giri ' and its Spanish equivalent 'muchach~ Italiana ' might , if we simplify , both be represented by the same M-tree : M1 / \ M2 M3 where M2 stands for the sets of italians , M3 stands for the set of girls and M1 stands for the operation of set intersection .</sentence>
				<definiendum id="0">M2</definiendum>
				<definiendum id="1">M3</definiendum>
				<definiens id="0">the operation of set intersection</definiens>
			</definition>
			<definition id="4">
				<sentence>Viewed in this way , the following definition of synonymy ( notation : '= ' ) between Dtrees ( and , derivatively , for natural language expressions ) is forthcoming : Synonymy ( first version ) : D1 ~ D2 ¢*ae/ D1 = Rl ( al , ... , a , ~ ) and D2 = R2 ( bl , ... , b , , ) , where Rt and R~ snap onto the same meaning rule , und where it holds for alll &lt; i &lt; nthat ai ~bi , or D1 and D2 are basic expressions which map onto the same basic meaning. ( Definition of synonynly for M-trees , at this stage , comes down to simple equality of the trees. ) This notion of meaning takes syntactic structure into account , but does not `` cut meanings too finely '' , since any two linguistic constructions can be designated as synonymous. For instunce , Lewis ' `` double negation '' problem can be countered as follows : the syntax rules of double negation ( Raou~ler~e~ ) and plain affirmation ( R~Hi ... . ) can be mapped onto one and the same meaning rule , if the grammar writer decides that they are semantically indistinguishable. Alternatively , the semantic relation between a D-tree of the form R-negation \ [ R-negation I D and its constituent tree D nl~y be accounted for if both trees are snapped onto one and the same M-tree. Effectively , this would come down to an extension of Rosetta with `` rules of synonynly '' for entire trees , rather than for individual syntax rules. Arguably , our grip on the notion of nleaning is incomplete if only the limiting case of structural equivalence is dealt with , leaving aside the more general case of structural consequence ( ~v/ ) . Under what conditions does , for instance , one belief follow from another ? Rosetta 's isomorphy-based notion of meaning seems ill-equipped to deal with inference , but we claim theft an extrapolation is possible. A natural boundary conditions on ~M is logical validity : no rule may lead from true premises to a false conclusion. Writing '~- ' for the relation holding between M-trees if the formulas corresponding to their top-nodes stand in the relation of logical consequence , this gives : Validity : T , , ~M Tb only if T , , ~ Tt , . Given validity as an upper bound , we seek reasonable lower bounds on structural inference. It is not generally valid to allow that a tree has all its subtrees as structural consequences. ( For instance , the 86 2 negation of a tree T does not have the subtree T as a consequence. ) However , a solution can be found if we take the dual nature of our concept of meaning into account : M-trees combine structural and logical information. Therefore , if one tree is a subtree of another tree , and also a purely logical consequence of the bigger tree , then the inference is indisputable ; for the inference is logically correct and there can be no difference in syntactic structure : Subtree Principle ( 1 't version ) : If ( i ) T , T~ , and ( ii ) T1 , is a subtree of T , ~ then T , ~ ~M Tb. However , we have to exclude as `` pathological '' cases all those situations ill which it is not one and the same subtree Tt , that takes care of the logical and the structural side : we can not allow inferences such as the following -where S abbreviates a `` paraphrase '' of S , namely a sentence that is logically , but not structurally , equivalent to S ( see below ) -- . even though they fulfil both conditions of the Subtree Principle : ( 2 ) - , a ( s - , v ~M St v s2. These inferences are not structurally valid , given the structural differences between the conclusion and the relevant part of the premise. Let an atomic sentential fragment ( asj ) be a sentential M-tree no proper part of which is sentential itseff. To be on the safe side , we might forbid that `` par &amp; phases '' of asps from the conclusion occur in the premisse : Subtree Principle ( 2 nd version ) : If ( i ) T , ~ Tt , and ( ii ) Tb is a subtree of T , ~ and ( iii ) If TI is an asf that occurs essentially in T , ~ and T2 is an asf that occurs essentially ill Tb , then T\ ] . is not a paraphrase of T2 , then Ta t=M T~ , , where a paraphrase is a logical equivalent that falls short of structural equivalence : Paraphrase ( 1 ~t version ) : T1 is a paraphrase of T2 ¢~D , j Tl ~ T2 and T2 ~ Tl but none of the two is a subtree of the other. The resulting logic is quite uncommon unless stronger lower bounds are given. For instance , if ( ii ) is a necessary condition , there can not be any tree T such that ~M T. Consequently , the Deduction Theorem will not hold. Also , if ( iii ) is a necessary condition , then Conjunction Elimination fails to hold. In fact , it holds for all Sl and $ 2 that SI &amp; S2 ~M $ 2. To remedy this defect , ( iii ) may be weakened to allow logically inessential occurrences of paraphrases : Inessential occurrence : An occurrence of T in the premisse ( conclusion ) of an ino ference is inessential if the inference goes through when T is replaced by an arbitrary T ' everywhere in the premisse ( conclusion ) . For instance , the occurrence of S in ( 1 ) and ( 2 ) is essential , but its occurrence in S &amp; S ~/~ S is inessential and therefore harmless. As a result of this change , a restricted version of Conjunction Elimination holds , to the effect that a conjunction will structurally imply ally of its eonjuncts , provided the conchslon conjunct does not contain two asf 's that are paraphrases of eachother. This concludes our formalization of the `` subtree '' intuition. If we want to cover more ground , we need a more liberal concept than the structural no~ion of one tree being a subtree of another. First , a more subtle structural notion may be employed. For instance , an inference from Each dog barks loudly to Each black dog barks must be allowed , it seems , even though none of the two M-trees is a p , 'u't of the other. Therefore , a relation of constituent-wise comparability ( ~ , , definition follows ) is called for. It is important to note that the `` direction '' of the comparison ( which of the two subsumes which ) is irrelevant , since the logical requirement ( i ) determines the direction of the inference : Subtree Principle ( 3 r'~ version ) : If ( i ) T , , Wl , and ( ii ) Ta ~ T1 , and ( iii ) ( as above ) , then T , ~ ~M Tb. If the notation ~-. stands for the symmetrical relation that holds between two trees if one of them is a subtree of the other , this is the definition of the relation Comparability : T , ~ ~ : TI , ~D~\ ] ~rn , n &gt; 0 such that T , ~ = &lt; Tal , ... , Ta , .</sentence>
				<definiendum id="0">Tb</definiendum>
				<definiendum id="1">T2</definiendum>
				<definiens id="0">bl , ... , b , , ) , where Rt and R~ snap onto the same meaning rule , und where it holds for alll &lt; i &lt; nthat ai ~bi , or D1 and D2 are basic expressions which map onto the same basic meaning. ( Definition of synonynly for M-trees</definiens>
				<definiens id="1">comes down to simple equality of the trees. ) This notion of meaning takes syntactic structure into account , but does not `` cut meanings too finely ''</definiens>
				<definiens id="2">the syntax rules of double negation ( Raou~ler~e~ ) and plain affirmation ( R~Hi ... . ) can be mapped onto one and the same meaning rule</definiens>
				<definiens id="3">logical validity : no rule may lead from true premises to a false conclusion. Writing '~- ' for the relation holding between M-trees if the formulas corresponding to their top-nodes stand in the relation of logical consequence</definiens>
				<definiens id="4">the following -where S abbreviates a `` paraphrase '' of S , namely a sentence that is logically , but not structurally , equivalent to S ( see below ) -- . even though they fulfil both conditions of the Subtree Principle : ( 2 ) - , a ( s - , v ~M St v s2. These inferences are not structurally valid , given the structural differences between the conclusion and the relevant part of the premise. Let an atomic sentential fragment</definiens>
				<definiens id="5">an asf that occurs essentially in T</definiens>
				<definiens id="6">an asf that occurs essentially ill Tb</definiens>
				<definiens id="7">Inessential occurrence : An occurrence of T in the premisse</definiens>
				<definiens id="8">a relation of constituent-wise comparability ( ~ , , definition follows ) is called for. It is important to note that the `` direction '' of the comparison ( which of the two subsumes which ) is irrelevant , since the logical requirement ( i ) determines the direction of the inference : Subtree Principle ( 3 r'~ version ) : If ( i ) T , , Wl</definiens>
			</definition>
			<definition id="5">
				<sentence>d. barks Tbl B3 /\ B1 Tbl2 Each black dog / \ B5 B2 black dog Here , T~ ~ Tb holds , for T , ~ = &lt; B1 , B2 , T , ~2 &gt; , and T~ , = &lt; B1 , T , n2 , B3 &gt; , while B2 is a subtree of Tb12 and B3 is a subtree of T , ~2 .</sentence>
				<definiendum id="0">B2</definiendum>
				<definiens id="0">a subtree of Tb12 and B3 is a subtree of T , ~2</definiens>
			</definition>
			<definition id="6">
				<sentence>If all the suggested improvements on the Subtree Principle are taken into account , one might venture the followhlg definition of structural consequence : Subtree Principle ( final version ) : T , , ~- 'M Tb ~¢ : ~Def ( i ) We , ~ W~ , and ( ii ) T , ~ ~c Tb and ( iii ) If T1 is an asf that occurs essentially in T~ and T2 is an asf that occurs essentially ill Tb , theu T1 is not a paraphrase of T2 , and ( iv ) all asf 's of T~ , occur in T~ .</sentence>
				<definiendum id="0">T2</definiendum>
				<definiens id="0">an asf that occurs essentially in T~ and</definiens>
			</definition>
			<definition id="7">
				<sentence>Assuming that a notion of inference has been established along these lines , synonymy between M-trees can now be defined as nmtual structural consequence ( synonymy of D-trees is analogous ) : Synonymy ( final version ) : T1 and T2 are synonymous ¢ &gt; D~f TI bM T~ and To .</sentence>
				<definiendum id="0">synonymy</definiendum>
				<definiens id="0">nmtual structural consequence ( synonymy of D-trees is analogous ) : Synonymy ( final version ) : T1 and T2 are synonymous ¢ &gt; D~f TI bM T~ and To</definiens>
			</definition>
</paper>

		<paper id="3005">
			<definition id="0">
				<sentence>The computational gTammar specifies a mapping from the nominals and the verb ( s ) in a sentence to karaka r'elations between them .</sentence>
				<definiendum id="0">gTammar</definiendum>
				<definiens id="0">specifies a mapping from the nominals and the verb ( s ) in a sentence to karaka r'elations between them</definiens>
			</definition>
			<definition id="1">
				<sentence>Yogyata gives the semantic type that must be satisfied by the word group that serves in the kamaka role . )</sentence>
				<definiendum id="0">Yogyata</definiendum>
				<definiens id="0">gives the semantic type that must be satisfied by the word group that serves in the kamaka role</definiens>
			</definition>
			<definition id="2">
				<sentence>g , a number of source word groups will qualify for a part : i cul ilr &lt; lem~nd . The job of the core parseF is to make an appropriate assignmerit of the candidates , subject to certain constraints such as the following : I ) one cand. { date sour 're word group can not satisfy more than orle demand of the same demand word. 2 ) every obligatory demand must be satisfied in some karaka chart of every demand word group. 3 ) every source word must have an assignment . 4 ) if more than one interpretation of a source word is available , then exactly one has to be selected. Tile above problem is transformed to an integer programming problem. Assigning 1 to a candidate variable means that the particular karaka relation between the source word group and the demand word group holds ; 0 stands for otherwise. All the various types of constraints mentioned above can be specified in a very natural manner using algebraic inequalities in integer programming. Having a set of candidate variables assigned to I not only identifies the karaka relations which can be used to get the deep cases , but also identifies the karaka chart which serves to identify the sense of the verb group , etc. Moreover Integer ' programming also permits a lingu { st to express preferences among various candidates for a particular demand. A typical example of such a preference can be given. For. example , for most of the verbs an animate thing is more likely to be the karts than inanimate things , and among animate~ human beings are more likely candldate : ~ ; to b : , karts than non-human candidates. These preferences would simply order the multiple parses if an~ in the absence of other information. The parsing strategy actually adopted in the system makes use of the merged kar'aka chart and corresponds to AnvitAbhidhanvad , a theory of mimamsa school of the Indian grammatical it-edition. In this approach , we first determine the karaka relationships among the demand and source ~ord groups. ( These are determined 3 27 A.4 Ram se khet nahi iota gaya. Ram sefarm not plough could. ( Ram could not plough the farm. ) The above principle allows us to deal with active passives. The verb forms for active and passive are just two special cases of the forms a verb can take. For example , the verb 'iota ' in Hindi has four different meanings listed in the dictionary : I ) harness ( e.g. , Ram ne bail ko kolhu me iota , or Ram harnessed the bullock for ( turning ) the crusher. ) 2 ) hitching the cart ( e.g. , Ram ne gaadii ko iota , or Ram hitched the art. ) 3 ) plough ( e.g. , Ram ne jamindar ka khet iota , or Ram ploughed the landlord 's farm. ' ) 4 ) exploit ( e.g. , Ram ne naukar ko kaam me iota diya , or Ram exploited his servant by putting him to ( hard ) work. ) For each of the four senses , a karaka chart can be created. A karaka chart specifies the mandatory karakas ( i.e. , which must be filled for the sentence to be grammatical ) , optional karakas , and desirable karakas. For each of the kara-kas , it specifies the vibhakti ( i.e. , inflection or post position marker ) , and the semantic specification ( typically in the form of semantic type ) to be satisfied by the source word ( group ) . Such a specification for a karaka in a karaka chart is called a karaka restriction. Thus , the karaka chart for the 'hitching ' sense of 'iota ' has two mandatory karaka restrictions : one for karta karaka ( p\ [ .onounced kartaa kaarak ) and the other for karma karaka ( pronounced kaFm kaaz.ak ) . The former karaka relation maps to agent and the latter to patient semantic relation. As shown in Fig. i , the restriction for karta karaka says that a source word group satisfying it must be present in the sentence , its vibhakti must be 0 , and its semantic type should be human. restriction on karta karaka : karaka : karta mandatory : yes vibhakti : 0 semantic expression : human restriction on karma karaka : karaka : karma mandatory : yes vibhakti : 0-or-ko semantic expression : cart Fig. I : Karaka Chart for Jota ( Sense 2 ) The actual grammar we use in the system is based on the model discussed above. However , it differs from it slightly so as to have a faster parser. Instead of a sepal'ate kar~ka chart for each sense of a verb , we have a single merged karaka chart. It consists of a set of karaka restrictions where a restriction for a particular karaka relation is obtained by taking the logical-or of the necessary vibhakti and semantic types for the same karaka relation in the different karaka charts. For example , semantic type in restriction for karma kanaka for the merged karaka chart is obtained by taking logical-or of semantic types in karma karaka restrictions in the different karaka charts. Fig. 2 shows the merged karaka chart for iota. Karaka Necessity Vibhakti Semantic Type ... ... ... ... ... ... ... ... ... ... ... ... ... karta m 0 animate karma m 0-ko ~ animate or instrumentor land karana d se-dvara animate Or ' instrument Fig. 2 : ~erged Karaka Chart for Jota { As 'the separate karaka charts are no longer available for distinguishing among the senses of the main verb , separate information is needed. This information is available in the form of lakshan charts or discrimination nets. These nots can be obtained by looking at the separate karaka charts and identifying features that help us in distinguishing among the different senses. An example lakshan chart for jota is given in Fig. 3. 28 4 by l ; eskin£ the sour. re wor 'd ~AFoups ag &lt; tinsl ; kar , aka \ [ `est ; p\ ] cl ; ioria irl I ; 1l~ , iue~£ed \ ] .'.~xFal &lt; ct char`t , and then : \ ] o\ ] vJI'Ig l ; h , &gt; inl .</sentence>
				<definiendum id="0">job</definiendum>
				<definiens id="0">to make an appropriate assignmerit of the candidates</definiens>
				<definiens id="1">a candidate variable means that the particular karaka relation between the source word group and the demand word group holds</definiens>
			</definition>
</paper>

		<paper id="3055">
			<definition id="0">
				<sentence>The languages spoken in NHK 's World News are English , French , German , Italian , Russian , Korean , and Chinese .</sentence>
				<definiendum id="0">languages</definiendum>
				<definiens id="0">spoken in NHK 's World News are English , French , German , Italian , Russian , Korean</definiens>
			</definition>
			<definition id="1">
				<sentence>The translation process can be divided into 4 main steps : morphological analysis , syntactic analysis , transfer , and generation .</sentence>
				<definiendum id="0">translation process</definiendum>
				<definiens id="0">morphological analysis , syntactic analysis , transfer , and generation</definiens>
			</definition>
			<definition id="2">
				<sentence>Translation System for News Sentences Examining a large body of English news consisting of more than 3.5 million words from the World News and the basic news service of AP ( Associated Press ) , we can summarize the linguistic properties of Ihe news sentences as follows : I ) About 75,000 different words are used , and they are difficult to classify by news fields .</sentence>
				<definiendum id="0">Translation System</definiendum>
				<definiens id="0">properties of Ihe news sentences as follows : I ) About 75,000 different words are used , and they are difficult to classify by news fields</definiens>
			</definition>
			<definition id="3">
				<sentence>a~iIl~ , name -- &gt; HNAME ; for defined words -- &gt; ( R_NAME ) + ; for undefined words HNAME -- &gt; ... ; names defined in the lexicon RNAME -- &gt; \ [ A-Z\ ] \ [ a-z\ ] + ; names defined from the input Figure 1 Rules for the Local Context Translation .</sentence>
				<definiendum id="0">R_NAME</definiendum>
				<definiens id="0">) + ; for undefined words HNAME -- &gt; ... ; names defined in the lexicon RNAME -- &gt; \ [ A-Z\ ] \ [ a-z\ ] + ; names defined from the input Figure 1 Rules for the Local Context Translation</definiens>
			</definition>
</paper>

		<paper id="3043">
</paper>

		<paper id="2064">
			<definition id="0">
				<sentence>Umlautung also occurs in derivation in combination with a number of derivational particles , e.g. -lich ( klagen = &gt; kl~iglich ) .</sentence>
				<definiendum id="0">e.g. -lich</definiendum>
				<definiens id="0">occurs in derivation in combination with a number of derivational particles</definiens>
			</definition>
			<definition id="1">
				<sentence>They rely on the idea to represent stem vowels which exhibit umlautung with special characters ( diacritics ) ( e.g. A ) at the lexical level .</sentence>
				<definiendum id="0">diacritics )</definiendum>
				<definiens id="0">the idea to represent stem vowels which exhibit umlautung with special characters</definiens>
			</definition>
			<definition id="2">
				<sentence>The use of a separate data structure is contrary to the intuition that umlautung is a regular phenomenon of German morphology , the treatment of which should require no extra mechanism .</sentence>
				<definiendum id="0">umlautung</definiendum>
				<definiens id="0">a regular phenomenon of German morphology , the treatment of which should require no extra mechanism</definiens>
			</definition>
			<definition id="3">
				<sentence>Rules consist of a left context , a right context and a substitution .</sentence>
				<definiendum id="0">Rules</definiendum>
				<definiens id="0">consist of a left context , a right context and a substitution</definiens>
			</definition>
			<definition id="4">
				<sentence>A substitution consists of exactly one such pair .</sentence>
				<definiendum id="0">substitution</definiendum>
				<definiens id="0">consists of exactly one such pair</definiens>
			</definition>
			<definition id="5">
				<sentence>The pair of strings ( lexical and surface ) is processed from left to right .</sentence>
				<definiendum id="0">pair of strings</definiendum>
				<definiens id="0">processed from left to right</definiens>
			</definition>
			<definition id="6">
				<sentence>The basic structure consists of a head , usually some sort of affix , and one or more complements , one of which must be some type of stem .</sentence>
				<definiendum id="0">basic structure</definiendum>
			</definition>
			<definition id="7">
				<sentence>Schiller A. , Steffens P. ( 1990 ) : A Two-Level Morphology for a German natural language understanding system , IBM Stuttgart , manuscript .</sentence>
				<definiendum id="0">IBM Stuttgart</definiendum>
				<definiens id="0">A Two-Level Morphology for a German natural language understanding system ,</definiens>
			</definition>
</paper>

		<paper id="3083">
</paper>

	</volume>
