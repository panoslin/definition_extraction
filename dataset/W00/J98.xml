<?xml version="1.0" encoding="UTF-8"?>
	<volume id="J98">

		<paper id="3005">
			<definition id="0">
				<sentence>SUMMONS generates summaries that are informative in nature .</sentence>
				<definiendum id="0">SUMMONS</definiendum>
				<definiens id="0">generates summaries that are informative in nature</definiens>
			</definition>
			<definition id="1">
				<sentence>The DARPA message understanding systems ( MUC 1992 ) , which process news articles in specific domains to extract specified types of information , also fall within this category .</sentence>
				<definiendum id="0">DARPA message understanding systems</definiendum>
				<definiens id="0">process news articles in specific domains to extract specified types of information</definiens>
			</definition>
			<definition id="2">
				<sentence>Techniques for proper noun extraction include the use of regular grammars to delimit and identify proper nouns ( Mani et al. 1993 ; Paik et al. 1994 ) , the use of extensive name lists , place names , titles and `` gazetteers '' in conjunction with partial grammars in order to recognize proper nouns as unknown words in close proximity to known words ( Cowie et al. 1992 ; Aberdeen et al. 1992 ) , statistical training to learn , for example , Spanish names , from on-line corpora ( Ayuso 474 Radev and McKeown Generating Natural Language Summaries et al. 1992 ) , and the use of concept-based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information ( Weischedel et al. 1993 ; Lehnert et al. 1993 ) .</sentence>
				<definiendum id="0">part-of-speech information ( Weischedel</definiendum>
				<definiens id="0">place names , titles and `` gazetteers '' in conjunction with partial grammars in order to recognize proper nouns as unknown words in close proximity to known words ( Cowie et al. 1992 ; Aberdeen et al. 1992 ) , statistical training to learn</definiens>
			</definition>
			<definition id="3">
				<sentence>Another system , called MURAX ( Kupiec 1993 ) , is similar to ours from a different perspective .</sentence>
				<definiendum id="0">MURAX</definiendum>
				<definiens id="0">similar to ours from a different perspective</definiens>
			</definition>
			<definition id="4">
				<sentence>Conceptual summarization is a form of content selection .</sentence>
				<definiendum id="0">Conceptual summarization</definiendum>
			</definition>
			<definition id="5">
				<sentence>ZEDDoc ( Passonneau et al. 1997 ; Kukich et al. 1997 ) generates Web traffic summaries for advertisement management software .</sentence>
				<definiendum id="0">ZEDDoc</definiendum>
				<definiens id="0">Kukich et al. 1997 ) generates Web traffic summaries for advertisement management software</definiens>
			</definition>
			<definition id="6">
				<sentence>FUF is a functional unification formalism that uses a large systemic grammar of English , called SURGE , to fill in syntactic constraints , build a syntactic tree , choose closed class words , and eventually linearize the tree as a sentence .</sentence>
				<definiendum id="0">FUF</definiendum>
				<definiens id="0">a functional unification formalism that uses a large systemic grammar of English , called SURGE , to fill in syntactic constraints , build a syntactic tree , choose closed class words , and eventually linearize the tree as a sentence</definiens>
			</definition>
			<definition id="7">
				<sentence>SUMMONS produces a summary from sets of templates that contain the salient facts reported in the input articles and that are produced by the message understanding systems .</sentence>
				<definiendum id="0">SUMMONS</definiendum>
				<definiens id="0">produces a summary from sets of templates that contain the salient facts reported in the input articles and that are produced by the message understanding systems</definiens>
			</definition>
			<definition id="8">
				<sentence>CONFIDENCE PHYS TGT : ID PHYS TGT : TYPE PHYS TGT : NUMBER PHYS TGT : FOREIGN NATION • PHYS TGT : EFFECT OF INCIDENT PHYS TGT : TOTAL NUMBER HUM TGT : NAME HUM TGT : DESCRIPTION HUM TGT : TYPE HUM TGT : NUMBER HUM TGT : FOREIGN NATION HUM TGT : EFFECT OF INCIDENT HUM TGT : TOTAL NUMBER TST3-MUC4-0010 2 01 NOV 89 EL SALVADOR ATTACK ACCOMPLISHED TERRORIST ACT `` TERRORIST '' `` THE FMLN '' REPORTED : `` THE FMLN '' `` 1 CIVILIAN '' CIVILIAN : `` 1 CIVILIAN '' 1 : '' 1 CWILIAN '' DEATH : `` 1 CIVILIAN '' Figure 2 Sample MUC-4 template .</sentence>
				<definiendum id="0">CONFIDENCE PHYS TGT</definiendum>
				<definiens id="0">ID PHYS TGT : TYPE PHYS TGT : NUMBER PHYS TGT : FOREIGN NATION • PHYS TGT : EFFECT OF INCIDENT PHYS TGT : TOTAL NUMBER HUM TGT : NAME HUM TGT : DESCRIPTION HUM TGT</definiens>
			</definition>
			<definition id="9">
				<sentence>Later the same day , Reuters reported that the radical Muslim group Hamas had claimed responsibility for the act .</sentence>
				<definiendum id="0">Reuters</definiendum>
				<definiens id="0">reported that the radical Muslim group Hamas had claimed responsibility for the act</definiens>
			</definition>
			<definition id="10">
				<sentence>The content planner produces a conceptual representation of text meaning ( e.g. , a frame , a logical form , or an internal representation of text ) and typically does not include any linguistic information .</sentence>
				<definiendum id="0">content planner</definiendum>
				<definiens id="0">produces a conceptual representation of text meaning</definiens>
			</definition>
			<definition id="11">
				<sentence>The linguistic component consists of a lexical chooser , which determines the high-level sentence structure of each sentence and the words that realize each semantic role , and the FUF/SURGE ( Elhadad 1991 ; Elhadad 1993 ) sentence generator .</sentence>
				<definiendum id="0">linguistic component</definiendum>
				<definiens id="0">consists of a lexical chooser , which determines the high-level sentence structure of each sentence and the words that realize each semantic role , and the FUF/SURGE ( Elhadad 1991 ; Elhadad 1993 ) sentence generator</definiens>
			</definition>
			<definition id="12">
				<sentence>SUMMONS 's summarization component generates a base summary , which contains facts extracted from the input set of articles .</sentence>
				<definiendum id="0">base summary</definiendum>
				<definiens id="0">contains facts extracted from the input set of articles</definiens>
			</definition>
			<definition id="13">
				<sentence>A summary operator encodes a means for linking information in two different templates .</sentence>
				<definiendum id="0">summary operator</definiendum>
				<definiens id="0">encodes a means for linking information in two different templates</definiens>
			</definition>
			<definition id="14">
				<sentence>Later the same day , Reuters reported that the radical Muslim group Hamas had claimed responsibility for the act .</sentence>
				<definiendum id="0">Reuters</definiendum>
				<definiens id="0">reported that the radical Muslim group Hamas had claimed responsibility for the act</definiens>
			</definition>
			<definition id="15">
				<sentence>Later the same day , Reuters reported that the Islamic fundamentalist group Hamas claimed responsibility .</sentence>
				<definiendum id="0">Reuters</definiendum>
				<definiens id="0">reported that the Islamic fundamentalist group Hamas claimed responsibility</definiens>
			</definition>
			<definition id="16">
				<sentence>Furthermore , some operators reduce the importance values of 484 Radev and McKeown Generating Natural Language Summaries existing templates even further ( e.g. , the refinement operator reduces the importance of chronologically earlier templates by additional increments of 20 points because they contain outdated information ) .</sentence>
				<definiendum id="0">refinement operator</definiendum>
				<definiens id="0">reduces the importance of chronologically earlier templates by additional increments of 20 points because they contain outdated information )</definiens>
			</definition>
			<definition id="17">
				<sentence>Later the same day , Reuters reported that the radical Muslim group Hamas had claimed responsibility for the act .</sentence>
				<definiendum id="0">Reuters</definiendum>
				<definiens id="0">reported that the radical Muslim group Hamas had claimed responsibility for the act</definiens>
			</definition>
			<definition id="18">
				<sentence>To do this , we must make use of a discourse model that represents the content and wording of summaries that have already been presented to the user .</sentence>
				<definiendum id="0">discourse model</definiendum>
				<definiens id="0">represents the content and wording of summaries that have already been presented to the user</definiens>
			</definition>
			<definition id="19">
				<sentence>CREP : A regular expression-matching textual corpus tool .</sentence>
				<definiendum id="0">CREP</definiendum>
				<definiens id="0">A regular expression-matching textual corpus tool</definiens>
			</definition>
			<definition id="20">
				<sentence>FUF : The universal unifier -- user manual , version 5.0 .</sentence>
				<definiendum id="0">FUF</definiendum>
			</definition>
			<definition id="21">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="22">
				<sentence>SEMTEX : A text generator for German .</sentence>
				<definiendum id="0">SEMTEX</definiendum>
			</definition>
</paper>

		<paper id="2002">
			<definition id="0">
				<sentence>The case frame ( case slot ) pattern acquisition process consists of two phases : extraction of case frame instances from corpus data , and generalization of those instances to case frame patterns .</sentence>
				<definiendum id="0">case frame ( case slot ) pattern acquisition process</definiendum>
				<definiens id="0">consists of two phases : extraction of case frame instances from corpus data , and generalization of those instances to case frame patterns</definiens>
			</definition>
			<definition id="1">
				<sentence>M = { nl , n2 ... .. nN } , V in the set of verbs V = { vl , v2 ... .. Vv } , and r in the set of slot names T~ = { rl , r2 ... .. rR } , satisfying : P ( n Iv , r ) = 1 .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">in the set of verbs V = { vl , v2 ... .. Vv }</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , suppose that we employ the maximum-likelihood estimation ( or MLE for short ) to estimate the probability parameters of a conditional probability distribution , as described above , given the co-occurrence data in Figure 1 .</sentence>
				<definiendum id="0">maximum-likelihood estimation</definiendum>
				<definiendum id="1">MLE</definiendum>
				<definiens id="0">short ) to estimate the probability parameters of a conditional probability distribution , as described above , given the co-occurrence data in Figure 1</definiens>
			</definition>
			<definition id="3">
				<sentence>The selectional association , denoted A ( C I v , r ) , is defined as follows : P ( CIv , r ) ( 2 ) A ( C I v , F ) = P ( C I v , F ) x log P ( C ) where C is a class of nouns present in a given thesaurus , v is a verb and r is a slot name , as described earlier .</sentence>
				<definiendum id="0">selectional association</definiendum>
				<definiendum id="1">A ( C I</definiendum>
				<definiendum id="2">v</definiendum>
				<definiendum id="3">r</definiendum>
				<definiens id="0">r ) , is defined as follows : P ( CIv , r ) ( 2 ) A ( C I v , F ) = P ( C I v , F ) x log P ( C ) where C is a class of nouns present in a given thesaurus ,</definiens>
			</definition>
			<definition id="4">
				<sentence>M is any collection of mutually disjoint subsets of iV '' that exhaustively cover N. The parameters specify the conditional probability P ( C I v , r ) for each class ( subset ) C in that partition , such that P ( CIv , r ) = 1 .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">any collection of mutually disjoint subsets of iV '' that exhaustively cover N. The parameters specify the conditional probability P ( C I v</definiens>
			</definition>
			<definition id="5">
				<sentence>3 MDL is a principle of data compression and statistical estimation from information theory , which states that the best probability model for given data is that which requires the least code length in bits for the encoding of the model itself and the given data observed through it .</sentence>
				<definiendum id="0">MDL</definiendum>
				<definiens id="0">a principle of data compression and statistical estimation from information theory , which states that the best probability model for given data is that which requires the least code length in bits for the encoding of the model itself and the given data observed through it</definiens>
			</definition>
			<definition id="6">
				<sentence>The model description length L ( F ) is a subjective quantity , which depends on the coding scheme employed .</sentence>
				<definiendum id="0">model description length L ( F )</definiendum>
				<definiens id="0">a subjective quantity , which depends on the coding scheme employed</definiens>
			</definition>
			<definition id="7">
				<sentence>The parameter description length L ( O I F ) is calculated by : k L ( 0 I r ) = ~ x log IsI ( 9 ) where ISI denotes the sample size and k denotes the number of free parameters in the tree cut model , i.e. , k equals the number of nodes in P minus one .</sentence>
				<definiendum id="0">parameter description length L</definiendum>
				<definiendum id="1">ISI</definiendum>
				<definiens id="0">the sample size and k denotes the number of free parameters in the tree cut model , i.e. , k equals the number of nodes in P minus one</definiens>
			</definition>
			<definition id="8">
				<sentence>Recall that P ( n ) is obtained by MLE , namely , by normalizing the frequencies : for each C c P and each n E C , where for each C c P : = d ( C ) ( 12 ) ISI wheref ( C ) denotes the total frequency of nouns in class C in the sample S , and F is a tree cut .</sentence>
				<definiendum id="0">MLE</definiendum>
				<definiendum id="1">F</definiendum>
				<definiens id="0">n E C , where for each C c P : = d ( C ) ( 12 ) ISI wheref ( C ) denotes the total frequency of nouns in class C in the sample S , and</definiens>
				<definiens id="1">a tree cut</definiens>
			</definition>
			<definition id="9">
				<sentence>Concerning the above algorithm , we show that the following proposition holds : Proposition 1 The algorithm Find-MDL terminates in time O ( N x ISI ) , where N denotes the number of leaf nodes in the input thesaurus tree T and ISI denotes the input sample size , and outputs a tree cut model of T with the minimum description length ( with respect to the encoding scheme described in Section 3.1 ) .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">ISI</definiendum>
				<definiens id="0">the number of leaf nodes in the input thesaurus tree T and</definiens>
			</definition>
			<definition id="10">
				<sentence>MLE is likely to estimate most parameters to be zero , and thus suffers from the data sparseness problem .</sentence>
				<definiendum id="0">MLE</definiendum>
				<definiens id="0">likely to estimate most parameters to be zero , and thus suffers from the data sparseness problem</definiens>
			</definition>
			<definition id="11">
				<sentence>Interpreting log P ( M ) as the model description length translates , in the Bayesian estimation , to assigning larger prior probabilities on simpler models , since it is equivalent to assuming that P ( M ) = ( ½ ) t ( a ) , where I ( M ) is the description length of M. ( Note that if we assign uniform prior probability P ( M ) to all models M , then ( 15 ) becomes equivalent to ( 13 ) , giving the maximum-likelihood estimate . )</sentence>
				<definiendum id="0">I</definiendum>
				<definiens id="0">the model description length translates , in the Bayesian estimation , to assigning larger prior probabilities on simpler models</definiens>
			</definition>
			<definition id="12">
				<sentence>Recall , that in our definition of parameter description length , we assign a shorter parameter description length to a model with a smaller number of parameters k , which admits the above interpretation .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">admits the above interpretation</definiens>
			</definition>
			<definition id="13">
				<sentence>In our experiments , we extracted verbs and their case frame slots ( verb , slot_name , slot_value triples ) from the tagged texts of the Wall Street Journal corpus ( ACL/DCI CD-ROM1 ) consisting of 126,084 sentences , using existing techniques ( specifically , those in Smadja \ [ 1993\ ] ) , then incorporation of absolute frequencies of the words ( inside and outside the particular slot in question ) .</sentence>
				<definiendum id="0">case frame slots</definiendum>
			</definition>
			<definition id="14">
				<sentence>The starting cut consists of nodes/plant ... / , /food/ , etc , which are the highest nodes containing values of the direct object slot of eat .</sentence>
				<definiendum id="0">starting cut</definiendum>
				<definiens id="0">consists of nodes/plant ... / , /food/ , etc , which are the highest nodes containing values of the direct object slot of eat</definiens>
			</definition>
			<definition id="15">
				<sentence>For LA , we estimated P ( prep \ ] verb ) and P ( prep \ ] noun1 ) from the training data of each data set and compared them for disambiguation .</sentence>
				<definiendum id="0">P</definiendum>
			</definition>
			<definition id="16">
				<sentence>Coverage ( % ) Accuracy ( % ) Default 100 56.2 MDL + Default 100 82.2 SA + Default 100 76.7 LA + Default 100 80.7 LA.t + Default 100 78.1 TEL 100 82.4 We also implemented the exact method proposed by Hindle and Rooth ( 1991 ) , which makes disambiguation judgement using the t-score .</sentence>
				<definiendum id="0">Coverage ( %</definiendum>
				<definiens id="0">makes disambiguation judgement using the t-score</definiens>
			</definition>
			<definition id="17">
				<sentence>Table 14 shows example generalization results for MDL ( with classes with probability less than 0.05 discarded ) and SA .</sentence>
				<definiendum id="0">MDL</definiendum>
				<definiens id="0">with classes with probability less than 0.05 discarded ) and SA</definiens>
			</definition>
			<definition id="18">
				<sentence>P ( Clv , r ) Since SA estimates the ratio between two probability values , namely -~y- , the generalization result may be lead astray if one of the estimates of P ( C I v , r ) and P ( C ) is unreliable .</sentence>
				<definiendum id="0">SA</definiendum>
				<definiendum id="1">P ( C</definiendum>
				<definiens id="0">estimates the ratio between two probability values</definiens>
			</definition>
			<definition id="19">
				<sentence>( Note that MT = M , ST = S. ) Then define , in general for any submodel MT , and subsample ST , , L ( ST , \ [ FT , , ~T ' ) to be the data description length of subsample ST , using submodel MT , , L ( ~T , \ [ FT , ) to be the parameter description length for the submodel MT , , and L ' ( MT , ,ST , ) to be L ( ST , I FT ' , ~T ' ) qL ( ~T , \ [ FT , ) .</sentence>
				<definiendum id="0">L</definiendum>
				<definiens id="0">the parameter description length for the submodel MT , , and</definiens>
			</definition>
			<definition id="20">
				<sentence>( 18 ) i=1 Since the number of free parameters of a model in the entire thesaurus tree equals the number of nodes in the model minus one due to the stochastic condition ( that the probability parameters must sum to one ) , when T equals the entire thesaurus tree , theoretically the parameter description length for a tree cut model of T should be : L ( g I rr ) = L r ) k = L ( 0r , I rr , ) i=1 log Isl ( 19 ) where ISI is the size of the entire sample .</sentence>
				<definiendum id="0">ISI</definiendum>
				<definiens id="0">the number of free parameters of a model in the entire thesaurus tree equals the number of nodes in the model minus one due to the stochastic condition ( that the probability parameters must sum to one ) , when T equals the entire thesaurus tree , theoretically the parameter description length for a tree</definiens>
			</definition>
			<definition id="21">
				<sentence>We will thus use the identity ( 18 ) both when T is the entire tree and when it is a proper subtree .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">the entire tree</definiens>
			</definition>
			<definition id="22">
				<sentence>Namely , if we let Clmin ( MT , ST ) denote the minimum description length ( as defined by \ [ 17\ ] and \ [ 18\ ] ) achievable for ( sub ) model Mr on ( sub ) sample ST contained in ( sub ) tree T , \ [ ) s ( ~ ) the MLE estimate for node ~\ ] 240 Li and Abe Generalizing Case Frames using the entire sample S , and root ( T ) the root node of tree T , then we have : L~nin ( MT , ST ) min L~nin ( MTi , ST i ) , k i=1 L ' ( ( \ [ root ( T ) \ ] , \ [ Ps ( root ( T ) ) \ ] ) , ST ) } ( 20 ) The rest of the proof proceeds by induction .</sentence>
				<definiendum id="0">min L~nin</definiendum>
				<definiendum id="1">ST ) }</definiendum>
				<definiens id="0">the minimum description length ( as defined by \ [ 17\ ] and \ [ 18\ ] ) achievable for ( sub ) model Mr on ( sub ) sample ST contained in ( sub ) tree T , \ [ ) s ( ~ ) the MLE estimate for node ~\ ] 240 Li and Abe Generalizing Case Frames using the entire sample S , and root ( T ) the root node of tree T</definiens>
				<definiens id="1">k i=1 L ' ( ( \ [ root ( T ) \ ] , \ [ Ps ( root ( T ) ) \ ] )</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>Section 2 describes a statistical classifier , TLC ( Topical/Local Classifier ) , that uses topical context ( the open-class words that co-occur with a particular sense ) , local context ( the openand closed-class items that occur within a small window around a word ) , or a combination of the two .</sentence>
				<definiendum id="0">context</definiendum>
				<definiens id="0">the open-class words that co-occur with a particular sense ) , local context ( the openand closed-class items that occur within a small window around a word ) , or a combination of the two</definiens>
			</definition>
			<definition id="1">
				<sentence>serve ( verb ) hard ( adj ) line ( noun ) supply with food .41 not easy ( difficult ) .80 product .54 hold an office .29 not soft ( metaphoric ) .12 phone .10 function as something .20 not soft ( physical ) .08 text .10 provide a service .10 cord .09 division .09 formation .08 Wall Street Journal corpus and from the American Printing House for the Blind corpus .</sentence>
				<definiendum id="0">serve ( verb</definiendum>
			</definition>
			<definition id="2">
				<sentence>The operation of TLC consists of preprocessing , training , and testing .</sentence>
				<definiendum id="0">operation of TLC</definiendum>
				<definiens id="0">consists of preprocessing , training , and testing</definiens>
			</definition>
			<definition id="3">
				<sentence>For this cue type , p ( cj \ ] si ) is the probability that item cj appears precisely at location j for sense si .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the probability that item cj appears precisely at location j for sense si</definiens>
			</definition>
			<definition id="4">
				<sentence>Corpus-based word sense identifiers are data hungry -- it takes them mere seconds to digest all of the information contained in training materials that take months to prepare manually .</sentence>
				<definiendum id="0">Corpus-based word sense identifiers</definiendum>
				<definiens id="0">data hungry -- it takes them mere seconds to digest all of the information contained in training materials that take months to prepare manually</definiens>
			</definition>
			<definition id="5">
				<sentence>4 Like a standard dictionary , WordNet contains the definitions of words .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">contains the definitions of words</definiens>
			</definition>
			<definition id="6">
				<sentence>The basic unit in WordNet is a synonym set , or synset , which represents a lexicalized concept .</sentence>
				<definiendum id="0">synset</definiendum>
				<definiens id="0">a synonym set</definiens>
				<definiens id="1">represents a lexicalized concept</definiens>
			</definition>
			<definition id="7">
				<sentence>For example , maple is a hyponym of tree , which is to say that a maple is a kind of tree .</sentence>
				<definiendum id="0">maple</definiendum>
				<definiens id="0">a hyponym of tree , which is to say that a maple is a kind of tree</definiens>
			</definition>
			<definition id="8">
				<sentence>A class consists of the synonyms found at a node and the synonyms at all the nodes that it dominates ( all of its hyponyms ) .</sentence>
				<definiendum id="0">class</definiendum>
				<definiens id="0">consists of the synonyms found at a node and the synonyms at all the nodes that it dominates ( all of its hyponyms )</definiens>
			</definition>
			<definition id="9">
				<sentence>In his experiment , Yarowsky uses an updated on-line version of RogeFs Thesaurus that is not generally available to the research community .</sentence>
				<definiendum id="0">Yarowsky</definiendum>
				<definiendum id="1">RogeFs Thesaurus</definiendum>
				<definiens id="0">uses an updated on-line version of</definiens>
			</definition>
</paper>

		<paper id="4001">
			<definition id="0">
				<sentence>In the dialogue of Figure 1 , the Apprentice ( participant `` A '' ) initiates the first subdialogue for two reasons : ( i ) because he believes that removing the belt of the air compressor plays a role in replacing its pump and belt and ( ii ) because he wants to enlist the Expert 's help in removing the belt .</sentence>
				<definiendum id="0">Apprentice</definiendum>
				<definiens id="0">wants to enlist the Expert 's help in removing the belt</definiens>
			</definition>
			<definition id="1">
				<sentence>Th An agent intends that a proposition hold CBA An agent can bring about an act BCBA An agent believes that it can bring about an act BEL An agent believes a proposition Multi-agent FSP PSP CBAG MBCBAG MB A set of agents have a full SharedPlan for an act A set of agents have a partial SharedPlan for an act A set of agents can bring about an act A set of agents mutually believe that they can bring about an act A set of agents mutually believe a proposition FIP ( G , c~ , Tp , T~ , P~ ) An agent G has a full individual plan at time Tp to perform act c~ at time Tc~ using recipe R~ R~ = { fll , pj } A BEL ( G , tG E Recipes ( a ) , Tp ) ( a ) G intends to perform fli Int .</sentence>
				<definiendum id="0">proposition FIP</definiendum>
				<definiendum id="1">BEL</definiendum>
				<definiens id="0">a ) G intends to perform fli Int</definiens>
			</definition>
			<definition id="2">
				<sentence>Th represents an agent 's intention that a proposition hold .</sentence>
				<definiendum id="0">Th</definiendum>
				<definiens id="0">represents an agent 's intention that a proposition hold</definiens>
			</definition>
			<definition id="3">
				<sentence>Th ( Gj , ( 3R &amp; ) CBA ( G &amp; , Z , , R &amp; , T &amp; , { pj } ) , Tp , T &amp; ) , Tp ) GR &amp; C GR such that ( a ) There is a recipe R &amp; for/31 such that i. GR &amp; mutually believe that they can perform/31 according to the recipe ( 3R &amp; ) \ [ MBCBAG ( GR &amp; , /3i , R &amp; , T &amp; , Tp , { pj } ) A ii .</sentence>
				<definiendum id="0">Th</definiendum>
				<definiens id="0">G &amp; , Z , , R &amp; , T &amp; , { pj } )</definiens>
			</definition>
			<definition id="4">
				<sentence>For example , Allen ( 1983 ) introduces a precondition-action rule stating that if agent G wants to achieve proposition P and P is a precondition of an act ACT , then G may want to perform ACT .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a precondition of an act ACT , then G may want to perform ACT</definiens>
			</definition>
			<definition id="5">
				<sentence>Heuristics , derived from both planning and natural language principles , are used to guide the application of the rules to recognize ( or construct ) the best possible plan accounting for an agent 's observations ( or desired effects ) .</sentence>
				<definiendum id="0">Heuristics</definiendum>
				<definiens id="0">derived from both planning and natural language principles , are used to guide the application of the rules to recognize ( or construct ) the best possible plan accounting for an agent 's observations ( or desired effects )</definiens>
			</definition>
			<definition id="6">
				<sentence>537 Computational Linguistics Volume 24 , Number 4 Assume : PSP ( { G1 , G2 } , o~ ) , G1 is the agent being modeled .</sentence>
				<definiendum id="0">G1</definiendum>
				<definiens id="0">the agent being modeled</definiens>
			</definition>
			<definition id="7">
				<sentence>In Case ( a ) of Step ( 2 ) , Prop indicates G2 '' s intention that the agents collaborate on an act ft .</sentence>
				<definiendum id="0">Prop</definiendum>
				<definiens id="0">indicates G2 '' s intention that the agents collaborate on an act ft</definiens>
			</definition>
			<definition id="8">
				<sentence>Th ( e , FSP ( { e , a } , replace ( pump ( acl ) &amp; belt ( acl ) , { a } ) ) ) `` E intends that the agents collaborate to replace the pump and belt of the air compressor , acl . ''</sentence>
				<definiendum id="0">FSP</definiendum>
			</definition>
			<definition id="9">
				<sentence>Thus , the agent specification of the FSP in DSP1 includes both the Expert and the Apprentice , while only the Apprentice is the agent of the replace act itself .</sentence>
				<definiendum id="0">Apprentice</definiendum>
				<definiens id="0">the agent of the replace act itself</definiens>
			</definition>
			<definition id="10">
				<sentence>541 Computational Linguistics Volume 24 , Number 4 modify_network ( NetPiece , Loc , G , T ) { type ( Net Piece , kl-o ne_net work ) , type ( Lot , screen_location ) , mpty ( Loc ) , frees pace~for ( Data , Loc ) l &lt; T2 } display ( NetPiece , G 1 , T 1 ) put ( Data , Loc , G2 , T2 ) Figure 12 A recipe for modifying a network. Figure 2 may be represented as : DSP4 = Int.Th ( u , FSP ( { u , s } , modify_network ( NetPiece , Loc , { u , s } ) ) ) `` U intends that the agents collaborate to modify the piece of network displayed at some screen location. '' Figure 12 contains one possible recipe for the act modify_network ( NetPiece , Loc , G , T ) . 14 The recipe requires that an agent display a piece of a network and then put some new data at some screen location. The constraints of the recipe require that the screen location be empty and that there be enough free space for the data at that location. The purpose of the subdialogue in Figure 2 may be represented as : 15 DSP5 =In t. Th ( u , F S P ( ( u , s } , Achieve ( freespace.for ( Data , below ( gel ) ) , { u , s } ) ) ) `` U intends that the agents collaborate to free up some space below the employee concept. '' The SharedPlan used to model DSP5 is subsidiary to that used to model DSP4 by virtue of the ability operator BCBA. As discussed in Section 2 , an agent G 's ability to perform an act fl depends in part on its ability to satisfy the constraints of its recipe for ft. A plan to satisfy one of the constraints thus contributes to the plan for fl and is therefore subsidiary to it. Because the condition freespace_for ( Data , Loc ) is a constraint in the recipe for modify_network ( NetPiece , Loc , G , T ) , the SharedPlan in DSP5 to free up space on the screen is subsidiary to the SharedPlan in DSP4 to modify the network. DSP4 thus dominates DSPs. dialogue in Figure 3 may be represented as : DSP6 = Int.Th ( nm , FSP ( { nm , np } , maintain ( node39 , { nm , np } ) ) ) `` NM intends that the agents collaborate to maintain node39 of the local computer network. '' The purpose of the first subdialogue in Figure 3 may be represented as : DSP7 = Int.Th ( np , FSP ( { nm , np } , Achieve ( has.recipe ( { nm , np } , maintain ( node39 , { nm , np } ) , R ) , { nm , np } ) ) ) `` NP intends that the agents collaborate to obtain a recipe for maintaining node39. '' The SharedPlan used to model DSP7 is subsidiary to the SharedPlan used to model DSP6 by virtue of the recipe requirement of the SharedPlan definition. As shown in Clause ( 1 ) of the definition in Figure 5 , for a group of agents to have an FSP for an act o~ , they must have mutual belief of a recipe for o~. The SharedPlan in DSP7 to obtain 14 This recipe derives from the operators used in Sidner 's ( 1985 ) and Litman 's ( 1985 ) representations of the acts and constraints underlying the exchange in Figure 2. 15 The function Achieve takes propositions to actions ( Pollack 1986a ) . 542 Lochbaum A Collaborative Planning Model a recipe for maintaining node39 thus contributes to the SharedPlan in DSP6 to do the maintenance and is therefore subsidiary to it. As a result , DSP6 dominates DSP7. The second subdialogue in Figure 3 is concerned with identifying a parameter of an act. The purpose of this subdialogue may be represented as : DSP8 = Int.Th ( nm , FSP ( { nm , np } , Achieve ( has.sat.descr ( { nm , np } , ToNode , S ( divert_traffic , ToNode ) ) , { nm , np } ) ) ) `` NM intends that the agents collaborate to obtain a suitable description of the ToNode parameter of the divert_traffic act. '' The SharedPlan used to model DSPs is subsidiary to that used to model DSP6 by virtue of the ability operator BCBA. As discussed in Section 2 , an agent G 's ability to perform an act fl depends in part on its ability to identify the parameters of ft. At this point in the agents ' discourse , NM and NP have agreed that the acts divert_traFfic ( node39 , ToNode , G1 ) and replace_switch ( node39 , Switch Type , G2 ) will be part of their recipe for maintaining node39. Because the agents ' recipe for maintaining node39 includes the act of diverting network traffic , the SharedPlan in DSP8 to identify the ToNode parameter of the divert_traffic act contributes to the SharedPlan in DSP6 to maintain node39. DSP6 thus dominates DSP8. The SharedPlan in DSP8 is not subsidiary to the SharedPlan in DSP7 , because , id.params is not a requirement of has.recipe. As we argued in Section 2.1 , knowing a recipe for an act should not require identifying the parameters of the act or the acts in its recipe. However , because an agent must have a recipe in mind before it can be concerned with identifying the parameters of the acts in that recipe , the SharedPlan in DSP7 must be completed before the SharedPlan in DSPs. 16 DSP7 thus satisfactionprecedes DSP8. Intentional structure plays a central role in discourse processing. For each utterance of a discourse , an agent must determine whether the utterance begins a new segment of the discourse , completes the current segment , or contributes to it ( Grosz and Sidner 1986 ) . If the utterance begins a new segment of the discourse , the agent must recognize the DSP of that segment , as well as its relationship to the other DSPs underlying the discourse and currently in focus. If the utterance completes the current segment , the agent must come to believe that the DSP of that segment has been satisfied. If the utterance contributes to the current segment , the agent must determine the effect of the utterance on the segment 's DSP. We now show how the SharedPlan reasoning presented in Section 3 may be mapped to the problem of recognizing and reasoning with intentional structure. Step ( 2 ) of the augmentation process in Figure 10 is divided into three cases based upon the way in which an utterance affects the SharedPlans underlying a discourse. An utterance may indicate the initiation of a subsidiary SharedPlan ( Case ( 2a ) ) , the completion 16 There are several means by which an agent can determine a recipe for an act c~. If an agent chooses a recipe for c~ from some type of manual ( e.g. , a cookbook ) , then the agent will have a complete recipe for c~ before identifying the parameters of c~ 's constituent acts. On the other hand , when being told a recipe for c~ by another agent , the ignorant agent may interrupt and ask about a parameter of a constituent act before knowing all of the constituent acts. In this case , the agent may have only a partial recipe for c~ before identifying the parameters of the acts in that partial recipe. Thus , if fli is an act in c~ 's recipe , a discourse segment concerned with identifying a parameter of fli could be linguistically embedded within a segment concerned with obtaining a recipe for c~. This case poses interesting questions for future research regarding the relationship between the two segments ' DSPs. 543 Computational Linguistics Volume 24 , Number 4 Assume : PSP ( { G1 , G2 } , c~ ) , The purpose of the current discourse segment , DSc , is thus DSP¢ =Int.Th ( ICP , FSP ( { G1 , G2 } , a ) ) G1 is the agent being modeled , S is a stack of SharedPlans used to represent G~ 's beliefs as to which portion of the intentional structure is currently in focus. Let Prop be the proposition communicated by G2 's utterance/d. ( a ) Does ld or Prop indicate the initiation of a new discourse segment ? If G1 believes that/d or Prop indicates the initiation of a subsidiary SharedPlan for an act fl , then i. G1 believes that the DSP of the new segment is Int.Th ( G2 , FSP ( { Gi , G~ } , fl ) ) . ii. G1 explains the new segment by determining the relationship of the SharedPlan in ( i ) to the SharedPlans maintained in S. ( b ) Does bl or Prop indicate the completion of the current discourse segment ? If G1 believes that/d or Prop indicates the satisfaction of DSP~ , then i. G~ believes that G2 believes DSc is complete. ii. If G1 believes that the agent s ' PSP for a is complet % then G1 will also believe that DSPc has been satisfied and thus DS¢ is complete. DSP¢ is thus popped from S. ( c ) Does Prop contribute to the current discourse segment ? Otherwise , G1 will i. ascribe to G~ a belief that Prop contributes to the agents ' PSP for ii. determine if he also believes that to be the case. Figure 13 Step ( 2 ) of the augmentation process. of the current SharedPlan ( Case ( 2b ) ) , or its continuation ( Case ( 2c ) ) . These three cases : may be mapped to the problem of determining whether an utterance begins a new segment of the discourse , completes the current segment , or contributes to it. In Figure 13 , we have recast Step ( 2 ) of the augmentation process to reflect this use. The augmentation process in Figure 13 specifies the process by which agent G1 makes sense of agent G2 's utterances given the current discourse context. We use a stack of SharedPlans S to model this context. The stack corresponds to that portion of the intentional structure that is currently in focus. It thus mirrors the attentional state component of discourse structure and contains PSPs corresponding to discourse segments that have not yet been completed. Because the augmentation process depends most heavily upon the SharedPlans that are used to represent DSPs , it simply makes use of the SharedPlans themselves , rather than the full intentions. The full intentions are easily recoverable from the stack representation. Case ( 2a ) in Figure 13 models the recognition of new discourse segments and their purposes. If G1 believes that G2 's utterance indicates the initiation of a new SharedPlan , then G1 will take G2 to be initiating a new discourse segment with her utteranceJ 7 Gt first ascribes this intention to G2 ( Step ( 2ai ) ) and then tries to explain it given the 17 As discussed in Section 7.3 , the DSP of the new segment may be only abstractly specified at this point. 544 Lochbaum A Collaborative Planning Model current discourse context ( Step ( 2aii ) ) . Whereas at the utterance level , a hearer must explain why a speaker said what he did ( Sidner and Israel 1981 ) , at the discourse level , an OCP must explain why an ICP engages in a new discourse segment at a particular juncture in the discourse. The latter explanation depends upon the relationship of the new segment 's DSP to the other DSPs underlying the discourse. In Step ( 2aii ) of the augmentation process , G1 must thus determine whether the new SharedPlan would contribute to the agents ' SharedPlan for o~ or to some other plan on the stack S. If the new SharedPlan does not contribute to any of the plans on the stack , then it is taken as an interruption. If it does not contribute to the agents ' SharedPlan for o~ , but to another plan on the stack , one for 7 say , then G1 must also determine whether the plans that are above 7 on the stack have been completed. Case ( 2b ) in Figure 13 models the recognition of a segment 's completion. If G1 believes that G2 's utterance signals the completion of the current segment , then G1 must reason whether he too believes the segment to be complete. For that to be the case , G1 must believe that all of the beliefs and intentions required of an FSP have been established over the course of the segment. The completion of a segment may be signaled in either the linguistic structure or the intentional structure. For example , in the linguistic structure , cue phrases such as `` but anyway '' may indicate the satisfaction of a DSP ( as well as a pop of the focus space stack ) . In the intentional structure , the completion of a segment may be signaled by the initiation of a new SharedPlan , as described above. Case ( 2c ) models the recognition of an utterance 's contribution to the current discourse segment. When a speaker produces an utterance within a segment , a hearer must determine why the speaker said what he did. Step ( 2c ) models the hearer 's reasoning by trying to ascribe appropriate beliefs to the speaker. These beliefs are ascribed based on the hearer 's beliefs about the state of the agents ' SharedPlans and the steps necessary to complete them. Figure 13 contains a high-level specification of the process of reasoning with intentional structure. It provides a framework in which to develop further mechanisms for modeling the various steps of this process. In this section , we present two such mechanisms. The first mechanism presents a method for recognizing the initiation of a new discourse segment ( Step ( 2a ) in Figure 13 ) ; the second describes an algorithm for reasoning about the contribution of an utterance to the current segment ( Step ( 2c ) ) . These two mechanisms are central to the augmentation process , but are not complete ; they each model just one aspect of their respective steps of the process. The complete specification of these steps , as well as that of the augmentation process in general , requires further research , as is discussed in Section 10. process involves recognizing agent G2 's intention that G1 and G2 form a full SharedPlan for an act ft. This intention may be recognized using a conversational default rule , CDRA , shown in Figure 14. TM The antecedent of this rule consists of two parts : ( la ) G1 must believe that G2 communicated her desire for the performance of act fl to G1 , and ( lb ) G1 must believe that G2 believes they can together perform ft. The second condition precludes the case where G2 is stating her desire to perform the act herself 18 This rule extends Grosz and Sidner 's ( 1990 ) original conversational default rule , CDR1. 545 Computational Linguistics Volume 24 , Number 4 ( la ) BEL ( Gi , \ [ communicates ( G2 , Gi , Desires ( G2 , occurs ( fl ) ) , T ) A ( lb ) BEL ( G~ , ( BR~ ) CBAG ( { Gi , G2 } , f~ , R~ ) , T ) \ ] , T ) ( 2 ) BEL ( Gi , Int.Th ( G~ , ESP ( { G1 , G : } , fl ) ) , T ) Figure 14 Conversational default rule CDRA. de f~lt or for G1 to perform the act. If conditions ( la ) and ( lb ) are satisfied , then in the absence of evidence to the contrary , G1 will believe that G2 intends that they form a full SharedPlan for ft. As given in Figure 14 , CDRA is used to recognize an agent 's intention based upon its desire for the performance of a particular act ft. The rule may also be used when an agent expresses its desire for a particular state of affairs P. In this case , the expressions OCCUrS ( fl ) 19 and fl are replaced in Figure 14 by P and Achieve ( P , { G1 , G2 } , T ) respectively. augmentation process involves recognizing an utterance 's contribution to the current SharedPlan. The SharedPlan definitions place requirements on recipes , abilities , plans , and commitments. A SharedPlan may thus be affected by utterances containing a variety of information. We will focus here , however , on utterances that communicate information about a single action fl that can be taken to play a role in the recipe of the agents ' plan for o~. We thus do not deal with utterances concerning warnings ( e.g. , `` Do not clog or close the stem vent under any circumstances '' \ [ Ansari 1995\ ] ) or utterances involving multiple actions that are related in particular ways ( e.g. , `` To reset the printer , flip the switch. '' \ [ Balkanski 1993\ ] ) . As with the other cases of Step ( 2 ) of the augmentation process , Step ( i ) of Case ( c ) involves ascribing a particular belief to agent G2 regarding the relationship of her utterance to the agents ' plans. For the types of utterances we are considering here , this belief is concerned with the relationship of the act fl to the objective of the agents ' current plan , i.e. , o~. In particular , G2 's reference to fl is understood as indicating belief of a Contributes relation between fl and oL. Contributes holds of two actions if the performance of the first action plays a role in the performance of the second action ( Lochbaum , Grosz , and Sidner 1990 ; Lochbaum 1994 ) . It is defined as the transitive closure of the D ( irectly ) -Contributes relation. One act D-Contributes to another if the first act is an element of the second act 's recipe ( Lochbaum , Grosz , and Sidner 1990 ; Lochbaum 1994 ) . 2o An agent ascribes belief in a Contributes relation irrespective of his own beliefs about this relationship. Once he has ascribed this belief , he then reasons about whether ihe also believes fl to contribute to o~ and in what way. Step ( 2cii ) of the augmentation process corresponds to this reasoning. To model this step , we introduce an algorithm based on the construction of a dynamic recipe representation called a recipe graph 19 The predicate occurs ( fl ) is true if fl was , is , or will be performed at the time associated with fl as one of its parameters ( Balkanski 1993 ) . 20 The term `` contributes '' is overloaded in this paper. The use of Contributes here refers to a relation between actions. Grosz and Sidner ( 1986 ) also describe a contributes relation between DSPs that is the inverse of the dominates relation. In addition , we have been using contributes informally to refer to the inverse of a subsidiary relationship between plans. 546 Lochbaum A Collaborative Planning Model A recipe for c~ is comprised of a set of immediate constituent acts ( { ill , ... , fin } ) and constraints ( { pl , ... , pro } ) . 12 An rgraph for c~ is comprised of a set of constituent acts and a set of constraints. ~~ `` ' '' ~q } il lJ lp Tijl 7ijl Figure 15 Graphical recipe and rgraph representations. 12 I~ l ~i ~ , /~'~ ' , N.. { E1 `` '' e r } 8 n 1 8nk or rgraph. 21 We first describe the rgraph representation and then indicate its role in modeling Gl 'S reasoning concerning G2 's utterances. Rgraphs result from composing recipes. Whereas a recipe includes only one level of action decomposition , an rgraph may include multiple levels. On analogy with parsing constructs , one can think of a recipe as being like a grammar rule , while an rgraph is like a ( partial ) parse tree. 2a Whereas a recipe represents information about the abstract performance of an action , an rgraph represents more specialized information by including instantiations of parameters , agents , and times , as well as multiple levels of decomposition. The graphical representations in Figure 15 contrast the structure of these two constructs. The construction of an rgraph corresponds to the reasoning that an agent performs in determining whether or not the performance of a particular act fl makes sense given the agent 's beliefs about recipes and the state of its individual and shared plans. The process of rgraph construction can thus be used to model the process by which agent G1 explains G2 's presumed belief in a Contributes relation. In explaining this belief , however , G1 must reason about more than just the agents ' immediate SharedPlan. In 2l This terminology was chosen to parallel Kautz's. He uses the term explanation graph or egraph for his representation relating event occurrences ( Kautz 1990 ) . A comparison of our representation and algorithms with Kautz 's can be found elsewhere ( Lochbaum 1991 , 1994 ) . In short , Kautz 's work is based on assumptions that are inappropriate for collaborative discourse. In particular , Kautz assumes a model of keyhole recognition ( Cohen , Perrault , and Allen 1982 ) in which one agent is observing another agent without that second agent 's knowledge. In such a situation , only actual event occurrences performed by a single agent are reasoned about ; Kautz 's representation and algorithms include no means for reasoning about hypothetical , partially specified , or multiagent actions. In addition , in keyhole recognition , no assumptions can be made about the interdependence of observed actions. Because the agent is not aware that it is being observed , it does not structure its actions to facilitate the recognition of its motives. A separate egraph must thus be created for each observation. 22 Barrett and Weld ( 1994 ) and Vilain ( 1990 ) provide further discussion of the use of parsing in planning and plan recognition. 547 Computational Linguistics Volume 24 , Number 4 Assume : PSP ( { G1 , G2 } , a ) , G1 is the agent being modeled , R~ is the set of recipes that G1 knows for a , H is an rgraph explaining the acts underlying the discourse up to this point , /3 is the act referred to by G2. a ) , expand H by choosing a recipe from R~ and adding it to the rgraph. previously been used to explain another act. If no such act exists , then fail. Otherwise , let r I be the result of identifying fl with fll in r. the subtree r in H by r ' , otherwise , fail. Figure 16 The rgraph construction algorithm. particular , he must also take into account any other collaborations of the agents , as well as any individual plans of his own. In so doing , G1 verifies that fl is compatible with the rest of the acts the agents have agreed upon , as well as those G1 intends to perform himself. 23 The rgraph construction algorithm is given in Figure 16. It is based on the assumption that agents G1 and G2 are collaborating on an act a and models Gl 's reasoning concerning G2 '' s reference to an act ft. While PSP ( { G1 , G2 } , oL ) provides the immediate context for interpreting G2 '' s utterance , an rgraph H models the remaining context established by the agents ' dialogue. H represents Gl 'S hypothesis as to how all of the acts underlying the agents ' discourse are related. To make sense of G2 's utterance concerning t , G1 must determine whether fl directly contributes to a while being consistent with H. Steps ( 1 ) and ( 2 ) of the algorithm model the immediate explanation of t , while Step ( 3 ) ensures that this explanation is consistent with the rest of the rgraph. The algorithm in Figure 16 is nondeterministic. Step ( 0 ) involves choosing a recipe from G ( s recipe library , while Step ( 2 ) involves choosing an act from that recipe. The failures in Steps ( 2 ) and ( 3 ) do not imply failure of the entire algorithm , but rather failure of a single nondeterministic execution. In Step ( 0 ) of the algorithm , Gl 'S hypothesis rgraph is initialized to some recipe that he knows for a. As will be discussed in Section 6.2.3 , this recipe may involve physical actions , such as those involved in lifting a piano , as well as informationgathering actions , such as those involved in satisfying a knowledge precondition. At the start of the agents ' collaboration , G1 may or may not have any beliefs as to how the agents will perform a. If he believes that the agents will use a particular recipe , the ihypothesis rgraph is initialized to that recipe. Otherwise , a recipe is selected arbitrarily from Gl '' s recipe library. The initial hypothesis will be refined , and possibly replaced , on the basis of G2 's utterances. In Step ( 1 ) of the algorithm , the recipe for a is first isolated from the remainder of the rgraph. This recipe , r , represents Gl 'S current beliefs as to how the agents are going to perform a. Step ( 2 ) of the algorithm involves identifying fl with a particular 23 On the basis of this reasoning , G 1 thus attributes belief in more than just a Contributes relation to G 2. In particular , G1 assumes that G2 also believes that fl is compatible with the other acts the agents have agreed upon. 548 Lochbaum A Collaborative Planning Model act fli in r resulting in a new recipe r'. If an appropriate fli can be found , it provides an explanation for G2 's reference to the act ft. If an appropriate fli can not be found , then r can not be the recipe that G2 has in mind for performing oz. The algorithm thus fails in this case and backtracks to select a different recipe for 0~. The new recipe must account for fl as well as all of the other acts previously accounted for by r. Step ( 3 ) of the algorithm ensures that the recipe and act chosen to account for a and fl are compatible with the other acts the agents have already discussed in support of a or the objectives of their other plans. This is done by adding the constraints of the recipe r t to the constraints of the rgraph H and checking that the resulting set is satisfiable. For G1 to agree to the performance of t , the recipe r t must be both internally and externally consistent. That is , the constraints of the recipe must be consistent themselves , as well as being consistent with the constraints of the recipes that G1 believes the agents will use to accomplish their other objectives. 24 The rgraph construction algorithm fails to produce an explanation for an act fl in the context of a PSP for a if the algorithm fails for all of the nondeterministic possibilities. This failure corresponds to a discrepancy between agent Gl 's beliefs and those G1 has attributed to agent G2. The failure thus indicates that further communication and replanning are necessary. To further elucidate the augmentation process , we now return to the dialogues given in Section 1 and show that the processes presented in this section capture the properties highlighted by the informal analyses given in the Introduction. We present each analysis from the perspective of one of the two discourse participants. Each analysis thus indicates the type of reasoning that is required for a system to assume the role of that participant in the dialogue. Figure 1 ) contains two subtask subdialogues. In Section 1 we noted that an OCP must recognize the purpose underlying each subdialogue , as well as the relationship of each purpose to the preceding discourse , in order to respond appropriately to the ICP. The OCP 's recognition of DSPs and their interrelationships is modeled by Case ( 2a ) of the augmentation process in Figure 13. We illustrate its use by modeling the Apprentice 's reasoning concerning the Expert 's first utterance in the second segment in Figure 17 , i.e. , ( 2a ) E : Now remove the pump. At this point in the agents ' discourse , the stack S consists only of a PSP to replace the air compressor 's pump and belt. This PSP corresponds to the overall discourse in Figure 17. The SharedPlan corresponding to the first embedded segment has been completed at this point in the discourse and is thus no longer in focus. 24 Another distinction between our work and Kautz 's ( 1990 ) relates to Step ( 3 ) of the algorithm in Figure 16 and the use of constraints. Whereas rgraphs include an explicit representation of constraints , Kautz 's egraphs do not. Constraints are used to guide egraph construction , but are not part of the representation itself. As a result , Kautz 's algorithms can only check for constraint satisfaction locally. In our algorithm , that would correspond to checking the satisfiability of a recipe 's constraints before adding it to an rgraph , but not afterwards. By checking the satisfiability of the constraint set that results from combining the recipe 's constraints with the rgraph 's constraints , the rgraph construction algorithm is able to detect unsatisfiability earlier than an algorithm that checks constraints only locally. 549 Computational Linguistics Volume 24 , Number 4 E : Replace the pump and belt please. ~ , AA~ OK , I found belt in the back. a Is that where it should be ? • .. \ [ A removes belt\ ] It 's done. E : Now remove the pump. E : First you have to remove the flywheel. • . . E : Now take the pump off the base plate. A : Already did. Figure 17 Sample subtask subdialogues ( Grosz 1974 ) . In utterance ( 2a ) , the Expert expresses her desire that the action remove ( pump ( acl ) , { a } ) be performed , where acl represents the air compressor the agents are working on. The Apprentice 's reasoning concerning this utterance may be modeled using CDRA. Condition ( la ) of CDRA is satisfied by the communication of this utterance to the Apprentice. Condition ( lb ) is satisfied by the context surrounding the agents ' collaboration. Because the Expert is in another room and can only instruct the Apprentice as to which actions to perform , the Expert 's utterance can not be expressing her intention to perform the desired action herself• In addition , because the Apprentice and Expert are both aware that the Apprentice does not have the necessary expertise to perform the action himself , the Apprentice can assume that the Expert must believe the agents can perform the act together , thus satisfying Condition ( lb ) and sanctioning the default conclusion , Bel ( a , Int.Th ( e , FSP ( { a , e } , remove ( pump ( acl ) , { a } ) ) ) ) . Thus , on the basis of the Expert 's utterance and her presumed beliefs concerning the agents ' capabilities to act , the Apprentice may reason that the Expert is initiating a new discourse segment with this utterance. The purpose of this segment is recognized as : DSP3 =Int. Th ( e , FSP ( { a , e } , remove ( pump ( acl ) , { a } ) ) ) . Once the Apprentice recognizes the DSP of this new discourse segment , he must determine its relationship to the other DSPs underlying the discourse• Subsidiary relationships between plans provide the basis for modeling the Apprentice 's reasoning• In particular , if the Apprentice believes that a plan for removing the pump would further some other plan of the agents ' , then he will believe that DSP3 is dominated by the DSP involving that other plan. As discussed in Section 5•1.1 , the subsidiary relation in question in this example derives from the constituent plan requirement of the SharedPlan definition. The Apprentice will succeed in recognizing the relationship of the second subdialogue to the remainder of the discourse , if he believes that removing the pump of the air compressor could be an act in the agents ' recipe for replacing its pump and belt. If the Apprentice does not have any beliefs about the relationship between these two acts , he may choose to assume the necessary D-Contributes relation on the basis of the Expert 's utterance and the current discourse context , or he may choose to query the Expert further. The rgraph construction algorithm may be used to model the Apprentice 's reasoning. In particular , Steps ( 1 ) and ( 2 ) of the algorithm in Figure 16 model the reasoning 550 Lochbaum A Collaborative Planning Model P1 ( a ) ( b ) FSP ( { a , e } , remov PSP ( { a , e } , replace ( pump ( acl ) &amp; belt ( acl ) , { a } ) ) { remove ( pump ( acl ) . { a } ) , remove ( belt ( acl ) , { a } ) } in \ [ 1\ ] Recipe ( replace ( pump ( acl ) &amp; belt ( acl ) , { a } ) ) FSP ( { a , e } , remove ( belt ( acl ) , { a } ) ) ~ \ [ 3aii\ ] ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... A explains P3 in terms of the role it plays in completing P1 , namely bringing about the condition marked ( b ) ) ( pump ( acl ) , { a } ) ) Figure 18 Analysis of the dialogue in Figure 17. \ [ 3ai ( E explains P2 in terms of the role it plays in completing P1 , namely bringing about the condition marked ( a ) FSP ( { a , e } , remove ( belt ( acl ) , { a } ) ) P2 The utterances of the first subdialogue are understood .~_q.q.d.pz_~_uced in this context PSP ( { a , e } , remove ( purnp ( ac 1 ) , { a } ) ) The utterances of the second subdialogue are understood P3 necessary for determining that a D-Contributes relation holds between two actions. If the OCP is able to infer such a D-Contributes relation , he will thus succeed in determining the subsidiary relationship necessary for explaining a subtask subdialogue. If the OCP is unable to infer such a relationship , then the algorithm will fail. This failure indicates that the OCP may need to query the ICP further about the appropriateness of her utterance. For example , as we noted in Section 1 , if the OCP has reason to believe that the proposed subtask will not in fact play a role in the agents ' overall task , then the OCP should communicate that information to the ICP. In addition , if the OCP has reason to believe that the performance of the subtask will conflict with the agents ' other plans and intentions , then the OCP should communicate that information as well. The latter reasoning is modeled by Step ( 3 ) of the rgraph construction algorithm. Step ( 3 ) ensures that the subtask is consistent with the objectives of the agents ' other plans. Figure 18 contains a graphical representation of the SharedPlans underlying the discourse in Figure 17. It is a snapshot representing the Apprentice 's view of the agents ' plans just after he explains the initiation of segment ( 3 ) . Each box in the figure corresponds to a discourse segment and contains the SharedPlan used to model the segment 's purpose. The plan used to model DSP3 is marked P3 in this figure , while the plans used to model DSP1 and DSP2 are labeled P1 and P2 , respectively. We will continue to follow the convention of co-indexing DSPs with the SharedPlans used to model them in the remainder of this paper. The information represented within each SharedPlan in Figure 18 is separated into two parts. Those beliefs and intentions that have been established at the time of the snapshot are shown above the dotted line , while those that remain to be established , but are used in determining subsidiary relationships , are shown below the line. Because the last utterance of segment ( 2 ) signals the end of the agents ' SharedPlan for removing the belt , the FSP for that act occurs above the dotted line. The agents ' plan for removing the belt is complete and thus no longer in focus at the start of segment ( 3 ) . We have included it in the figure for illustrative purposes. The index in square brackets 551 Computational Linguistics Volume 24 , Number 4 ( 1 ) User : Show me the generic concept called `` employee '' . ( 2 ) System : OK. &lt; system displays network &gt; F User : I ca n't fit a new ic below it .</sentence>
				<definiendum id="0">modify_network ( NetPiece , Loc</definiendum>
				<definiendum id="1">Loc )</definiendum>
				<definiendum id="2">S</definiendum>
				<definiendum id="3">G1</definiendum>
				<definiendum id="4">R~</definiendum>
				<definiendum id="5">H</definiendum>
				<definiendum id="6">acl</definiendum>
				<definiendum id="7">Bel (</definiendum>
				<definiens id="0">NetPiece , Loc , G , T ) { type ( Net Piece , kl-o ne_net work ) , type ( Lot , screen_location ) , mpty ( Loc ) , frees pace~for ( Data , Loc ) l &lt; T2 } display ( NetPiece , G 1 , T 1 ) put ( Data , Loc , G2 , T2 ) Figure 12 A recipe for modifying a network. Figure 2 may be represented as : DSP4 = Int.Th ( u , FSP ( { u , s }</definiens>
				<definiens id="1">s } ) ) ) `` U intends that the agents collaborate to modify the piece of network displayed at some screen location. '' Figure 12 contains one possible recipe for the act modify_network ( NetPiece , Loc , G , T ) . 14 The recipe requires that an agent display a piece of a network and then put some new data at some screen location. The constraints of the recipe require that the screen location be empty and that there be enough free space for the data at that location. The purpose of the subdialogue in Figure 2 may be represented as : 15 DSP5 =In t. Th ( u , F S P ( ( u , s } , Achieve ( freespace.for ( Data , below ( gel ) ) , { u , s } ) ) ) `` U intends that the agents collaborate to free up some space below the employee concept. '' The SharedPlan used to model DSP5 is subsidiary to that used to model DSP4 by virtue of the ability operator BCBA. As discussed in Section 2 , an agent G 's ability to perform an act fl depends in part on its ability to satisfy the constraints of its recipe for ft. A plan to satisfy one of the constraints thus contributes to the plan for fl and is therefore subsidiary to it. Because the condition freespace_for ( Data ,</definiens>
				<definiens id="2">a constraint in the recipe for modify_network ( NetPiece , Loc , G , T ) , the SharedPlan in DSP5 to free up space on the screen is subsidiary to the SharedPlan in DSP4 to modify the network. DSP4 thus dominates DSPs. dialogue in Figure 3 may be represented as : DSP6 = Int.Th ( nm , FSP ( { nm , np } , maintain ( node39 , { nm , np } ) ) ) `` NM intends that the agents collaborate to maintain node39 of the local computer network. '' The purpose of the first subdialogue in Figure 3 may be represented as : DSP7 = Int.Th ( np , FSP ( { nm , np } , Achieve ( has.recipe ( { nm , np } , maintain ( node39 , { nm , np } ) , R ) , { nm , np } ) ) ) `` NP intends that the agents collaborate to obtain a recipe for maintaining node39. '' The SharedPlan used to model DSP7 is subsidiary to the SharedPlan used to model DSP6 by virtue of the recipe requirement of the SharedPlan definition. As shown in Clause ( 1 ) of the definition in Figure 5 , for a group of agents to have an FSP for an act o~ , they must have mutual belief of a recipe for o~. The SharedPlan in DSP7 to obtain 14 This recipe derives from the operators used in Sidner 's ( 1985 ) and Litman 's ( 1985 ) representations of the acts and constraints underlying the exchange in Figure 2. 15 The function Achieve takes propositions to actions ( Pollack 1986a ) . 542 Lochbaum A Collaborative Planning Model a recipe for maintaining node39 thus contributes to the SharedPlan in DSP6 to do the maintenance and is therefore subsidiary to it. As a result , DSP6 dominates DSP7. The second subdialogue in Figure 3 is concerned with identifying a parameter of an act. The purpose of this subdialogue may be represented as : DSP8 = Int.Th ( nm , FSP ( { nm , np } , Achieve ( has.sat.descr ( { nm , np } , ToNode , S ( divert_traffic , ToNode ) ) , { nm , np } ) ) ) `` NM intends that the agents collaborate to obtain a suitable description of the ToNode parameter of the divert_traffic act. '' The SharedPlan used to model DSPs is subsidiary to that used to model DSP6 by virtue of the ability operator BCBA. As discussed in Section 2 , an agent G 's ability to perform an act fl depends in part on its ability to identify the parameters of ft. At this point in the agents ' discourse , NM and NP have agreed that the acts divert_traFfic ( node39 , ToNode , G1 ) and replace_switch ( node39 , Switch Type , G2 ) will be part of their recipe for maintaining node39. Because the agents ' recipe for maintaining node39 includes the act of diverting network traffic , the SharedPlan in DSP8 to identify the ToNode parameter of the divert_traffic act contributes to the SharedPlan in DSP6 to maintain node39. DSP6 thus dominates DSP8. The SharedPlan in DSP8 is not subsidiary to the SharedPlan in DSP7 , because , id.params is not a requirement of has.recipe. As we argued in Section 2.1 , knowing a recipe for an act should not require identifying the parameters of the act or the acts in its recipe. However , because an agent must have a recipe in mind before it can be concerned with identifying the parameters of the acts in that recipe , the SharedPlan in DSP7 must be completed before the SharedPlan in DSPs. 16 DSP7 thus satisfactionprecedes DSP8. Intentional structure plays a central role in discourse processing. For each utterance of a discourse , an agent must determine whether the utterance begins a new segment of the discourse , completes the current segment , or contributes to it ( Grosz and Sidner 1986 ) . If the utterance begins a new segment of the discourse , the agent must recognize the DSP of that segment , as well as its relationship to the other DSPs underlying the discourse and currently in focus. If the utterance completes the current segment , the agent must come to believe that the DSP of that segment has been satisfied. If the utterance contributes to the current segment , the agent must determine the effect of the utterance on the segment 's DSP. We now show how the SharedPlan reasoning presented in Section 3 may be mapped to the problem of recognizing and reasoning with intentional structure. Step ( 2 ) of the augmentation process in Figure 10 is divided into three cases based upon the way in which an utterance affects the SharedPlans underlying a discourse. An utterance may indicate the initiation of a subsidiary SharedPlan ( Case ( 2a ) ) , the completion 16 There are several means by which an agent can determine a recipe for an act c~. If an agent chooses a recipe for c~ from some type of manual ( e.g. , a cookbook ) , then the agent will have a complete recipe for c~ before identifying the parameters of c~ 's constituent acts. On the other hand , when being told a recipe for c~ by another agent , the ignorant agent may interrupt and ask about a parameter of a constituent act before knowing all of the constituent acts. In this case , the agent may have only a partial recipe for c~ before identifying the parameters of the acts in that partial recipe. Thus , if fli is an act in c~ 's recipe , a discourse segment concerned with identifying a parameter of fli could be linguistically embedded within a segment concerned with obtaining a recipe for c~. This case poses interesting questions for future research regarding the relationship between the two segments ' DSPs. 543 Computational Linguistics Volume 24 , Number 4 Assume : PSP ( { G1 , G2 } , c~ ) , The purpose of the current discourse segment , DSc , is thus DSP¢ =Int.Th ( ICP , FSP ( { G1 , G2 } , a ) ) G1 is the agent being modeled ,</definiens>
				<definiens id="3">a stack of SharedPlans used to represent G~ 's beliefs as to which portion of the intentional structure is currently in focus. Let Prop be the proposition communicated by G2 's utterance/d. ( a ) Does ld or Prop indicate the initiation of a new discourse segment ? If G1 believes that/d or Prop indicates the initiation of a subsidiary SharedPlan for an act fl , then i. G1 believes that the DSP of the new segment is Int.Th ( G2 , FSP ( { Gi , G~ } , fl ) ) . ii. G1 explains the new segment by determining the relationship of the SharedPlan in ( i ) to the SharedPlans maintained in S. ( b ) Does bl or Prop indicate the completion of the current discourse segment ? If G1 believes that/d or Prop indicates the satisfaction of DSP~ , then i. G~ believes that G2 believes DSc is complete. ii. If G1 believes that the agent s ' PSP for a is complet % then G1 will also believe that DSPc has been satisfied and thus DS¢ is complete. DSP¢ is thus popped from S. ( c ) Does Prop contribute to the current discourse segment ? Otherwise , G1 will i. ascribe to G~ a belief that Prop contributes to the agents ' PSP for ii. determine if he also believes that to be the case. Figure 13 Step ( 2 ) of the augmentation process. of the current SharedPlan ( Case ( 2b ) ) , or its continuation ( Case ( 2c ) ) . These three cases : may be mapped to the problem of determining whether an utterance begins a new segment of the discourse , completes the current segment , or contributes to it. In Figure 13 , we have recast Step ( 2 ) of the augmentation process to reflect this use. The augmentation process in Figure 13 specifies the process by which agent G1 makes sense of agent G2 's utterances given the current discourse context. We use a stack of SharedPlans S to model this context. The stack corresponds to that portion of the intentional structure that is currently in focus. It thus mirrors the attentional state component of discourse structure and contains PSPs corresponding to discourse segments that have not yet been completed. Because the augmentation process depends most heavily upon the SharedPlans that are used to represent DSPs , it simply makes use of the SharedPlans themselves , rather than the full intentions. The full intentions are easily recoverable from the stack representation. Case ( 2a ) in Figure 13 models the recognition of new discourse segments and their purposes. If G1 believes that G2 's utterance indicates the initiation of a new SharedPlan , then G1 will take G2 to be initiating a new discourse segment with her utteranceJ 7 Gt first ascribes this intention to G2 ( Step ( 2ai ) ) and then tries to explain it given the 17 As discussed in Section 7.3 , the DSP of the new segment may be only abstractly specified at this point. 544 Lochbaum A Collaborative Planning Model current discourse context ( Step ( 2aii ) ) . Whereas at the utterance level , a hearer must explain why a speaker said what he did ( Sidner and Israel 1981 ) , at the discourse level , an OCP must explain why an ICP engages in a new discourse segment at a particular juncture in the discourse. The latter explanation depends upon the relationship of the new segment 's DSP to the other DSPs underlying the discourse. In Step ( 2aii ) of the augmentation process , G1 must thus determine whether the new SharedPlan would contribute to the agents ' SharedPlan for o~ or to some other plan on the stack S. If the new SharedPlan does not contribute to any of the plans on the stack , then it is taken as an interruption. If it does not contribute to the agents ' SharedPlan for o~ , but to another plan on the stack , one for 7 say , then G1 must also determine whether the plans that are above 7 on the stack have been completed. Case ( 2b ) in Figure 13 models the recognition of a segment 's completion. If G1 believes that G2 's utterance signals the completion of the current segment , then G1 must reason whether he too believes the segment to be complete. For that to be the case , G1 must believe that all of the beliefs and intentions required of an FSP have been established over the course of the segment. The completion of a segment may be signaled in either the linguistic structure or the intentional structure. For example , in the linguistic structure , cue phrases such as `` but anyway '' may indicate the satisfaction of a DSP ( as well as a pop of the focus space stack ) . In the intentional structure , the completion of a segment may be signaled by the initiation of a new SharedPlan , as described above. Case ( 2c ) models the recognition of an utterance 's contribution to the current discourse segment. When a speaker produces an utterance within a segment , a hearer must determine why the speaker said what he did. Step ( 2c ) models the hearer 's reasoning by trying to ascribe appropriate beliefs to the speaker. These beliefs are ascribed based on the hearer 's beliefs about the state of the agents ' SharedPlans and the steps necessary to complete them. Figure 13 contains a high-level specification of the process of reasoning with intentional structure. It provides a framework in which to develop further mechanisms for modeling the various steps of this process. In this section , we present two such mechanisms. The first mechanism presents a method for recognizing the initiation of a new discourse segment ( Step ( 2a ) in Figure 13 ) ; the second describes an algorithm for reasoning about the contribution of an utterance to the current segment ( Step ( 2c ) ) . These two mechanisms are central to the augmentation process , but are not complete ; they each model just one aspect of their respective steps of the process. The complete specification of these steps , as well as that of the augmentation process in general , requires further research , as is discussed in Section 10. process involves recognizing agent G2 's intention that G1 and G2 form a full SharedPlan for an act ft. This intention may be recognized using a conversational default rule , CDRA , shown in Figure 14. TM The antecedent of this rule consists of two parts : ( la ) G1 must believe that G2 communicated her desire for the performance of act fl to G1 , and ( lb ) G1 must believe that G2 believes they can together perform ft. The second condition precludes the case where G2 is stating her desire to perform the act herself 18 This rule extends Grosz and Sidner 's ( 1990 ) original conversational default rule , CDR1. 545 Computational Linguistics Volume 24 , Number 4 ( la ) BEL ( Gi , \ [ communicates ( G2 , Gi , Desires ( G2 , occurs ( fl ) ) , T ) A ( lb ) BEL ( G~ , ( BR~ ) CBAG ( { Gi , G2 } , f~ , R~ ) , T ) \ ] , T ) ( 2 ) BEL ( Gi , Int.Th ( G~ , ESP ( { G1 , G : } , fl ) ) , T ) Figure 14 Conversational default rule CDRA. de f~lt or for G1 to perform the act. If conditions ( la ) and ( lb ) are satisfied , then in the absence of evidence to the contrary , G1 will believe that G2 intends that they form a full SharedPlan for ft. As given in Figure 14 , CDRA is used to recognize an agent 's intention based upon its desire for the performance of a particular act ft. The rule may also be used when an agent expresses its desire for a particular state of affairs P. In this case , the expressions OCCUrS ( fl ) 19 and fl are replaced in Figure 14 by P and Achieve ( P , { G1 , G2 } , T ) respectively. augmentation process involves recognizing an utterance 's contribution to the current SharedPlan. The SharedPlan definitions place requirements on recipes , abilities , plans , and commitments. A SharedPlan may thus be affected by utterances containing a variety of information. We will focus here , however , on utterances that communicate information about a single action fl that can be taken to play a role in the recipe of the agents ' plan for o~. We thus do not deal with utterances concerning warnings ( e.g. , `` Do not clog or close the stem vent under any circumstances '' \ [ Ansari 1995\ ] ) or utterances involving multiple actions that are related in particular ways ( e.g. , `` To reset the printer , flip the switch. '' \ [ Balkanski 1993\ ] ) . As with the other cases of Step ( 2 ) of the augmentation process , Step ( i ) of Case ( c ) involves ascribing a particular belief to agent G2 regarding the relationship of her utterance to the agents ' plans. For the types of utterances we are considering here , this belief is concerned with the relationship of the act fl to the objective of the agents ' current plan , i.e. , o~. In particular , G2 's reference to fl is understood as indicating belief of a Contributes relation between fl and oL. Contributes holds of two actions if the performance of the first action plays a role in the performance of the second action ( Lochbaum , Grosz , and Sidner 1990 ; Lochbaum 1994 ) . It is defined as the transitive closure of the D ( irectly ) -Contributes relation. One act D-Contributes to another if the first act is an element of the second act 's recipe ( Lochbaum , Grosz , and Sidner 1990 ; Lochbaum 1994 ) . 2o An agent ascribes belief in a Contributes relation irrespective of his own beliefs about this relationship. Once he has ascribed this belief , he then reasons about whether ihe also believes fl to contribute to o~ and in what way. Step ( 2cii ) of the augmentation process corresponds to this reasoning. To model this step , we introduce an algorithm based on the construction of a dynamic recipe representation called a recipe graph 19 The predicate occurs ( fl ) is true if fl was , is , or will be performed at the time associated with fl as one of its parameters ( Balkanski 1993 ) . 20 The term `` contributes '' is overloaded in this paper. The use of Contributes here refers to a relation between actions. Grosz and Sidner ( 1986 ) also describe a contributes relation between DSPs that is the inverse of the dominates relation. In addition , we have been using contributes informally to refer to the inverse of a subsidiary relationship between plans. 546 Lochbaum A Collaborative Planning Model A recipe for c~ is comprised of a set of immediate constituent acts ( { ill , ... , fin } ) and constraints ( { pl , ... , pro } ) . 12 An rgraph for c~ is comprised of a set of constituent acts and a set of constraints. ~~ `` ' '' ~q } il lJ lp Tijl 7ijl Figure 15 Graphical recipe and rgraph representations. 12 I~ l ~i ~ , /~'~ ' , N.. { E1 `` '' e r } 8 n 1 8nk or rgraph. 21 We first describe the rgraph representation and then indicate its role in modeling Gl 'S reasoning concerning G2 's utterances. Rgraphs result from composing recipes. Whereas a recipe includes only one level of action decomposition , an rgraph may include multiple levels. On analogy with parsing constructs , one can think of a recipe as being like a grammar rule , while an rgraph is like a ( partial ) parse tree. 2a Whereas a recipe represents information about the abstract performance of an action , an rgraph represents more specialized information by including instantiations of parameters , agents , and times , as well as multiple levels of decomposition. The graphical representations in Figure 15 contrast the structure of these two constructs. The construction of an rgraph corresponds to the reasoning that an agent performs in determining whether or not the performance of a particular act fl makes sense given the agent 's beliefs about recipes and the state of its individual and shared plans. The process of rgraph construction can thus be used to model the process by which agent G1 explains G2 's presumed belief in a Contributes relation. In explaining this belief , however , G1 must reason about more than just the agents ' immediate SharedPlan. In 2l This terminology was chosen to parallel Kautz's. He uses the term explanation graph or egraph for his representation relating event occurrences ( Kautz 1990 ) . A comparison of our representation and algorithms with Kautz 's can be found elsewhere ( Lochbaum 1991 , 1994 ) . In short , Kautz 's work is based on assumptions that are inappropriate for collaborative discourse. In particular , Kautz assumes a model of keyhole recognition ( Cohen , Perrault , and Allen 1982 ) in which one agent is observing another agent without that second agent 's knowledge. In such a situation , only actual event occurrences performed by a single agent are reasoned about ; Kautz 's representation and algorithms include no means for reasoning about hypothetical , partially specified , or multiagent actions. In addition , in keyhole recognition , no assumptions can be made about the interdependence of observed actions. Because the agent is not aware that it is being observed , it does not structure its actions to facilitate the recognition of its motives. A separate egraph must thus be created for each observation. 22 Barrett and Weld ( 1994 ) and Vilain ( 1990 ) provide further discussion of the use of parsing in planning and plan recognition. 547 Computational Linguistics Volume 24 , Number 4 Assume : PSP ( { G1 , G2 } , a )</definiens>
				<definiens id="4">the agent being modeled ,</definiens>
				<definiens id="5">the set of recipes that G1 knows for a ,</definiens>
				<definiens id="6">an rgraph explaining the acts underlying the discourse up to this point , /3 is the act referred to by G2. a ) , expand H by choosing a recipe from R~ and adding it to the rgraph. previously been used to explain another act. If no such act exists , then fail. Otherwise , let r I be the result of identifying fl with fll in r. the subtree r in H by r ' , otherwise , fail. Figure 16 The rgraph construction algorithm. particular , he must also take into account any other collaborations of the agents , as well as any individual plans of his own. In so doing , G1 verifies that fl is compatible with the rest of the acts the agents have agreed upon , as well as those G1 intends to perform himself. 23 The rgraph construction algorithm is given in Figure 16. It is based on the assumption that agents G1 and G2 are collaborating on an act a and models Gl 's reasoning concerning G2 '' s reference to an act ft. While PSP ( { G1 , G2 } , oL ) provides the immediate context for interpreting G2 '' s utterance , an rgraph H models the remaining context established by the agents ' dialogue. H represents Gl 'S hypothesis as to how all of the acts underlying the agents ' discourse are related. To make sense of G2 's utterance concerning t , G1 must determine whether fl directly contributes to a while being consistent with H. Steps ( 1 ) and ( 2 ) of the algorithm model the immediate explanation of t , while Step ( 3 ) ensures that this explanation is consistent with the rest of the rgraph. The algorithm in Figure 16 is nondeterministic. Step ( 0 ) involves choosing a recipe from G ( s recipe library , while Step ( 2 ) involves choosing an act from that recipe. The failures in Steps ( 2 ) and ( 3 ) do not imply failure of the entire algorithm , but rather failure of a single nondeterministic execution. In Step ( 0 ) of the algorithm , Gl 'S hypothesis rgraph is initialized to some recipe that he knows for a. As will be discussed in Section 6.2.3 , this recipe may involve physical actions , such as those involved in lifting a piano , as well as informationgathering actions , such as those involved in satisfying a knowledge precondition. At the start of the agents ' collaboration , G1 may or may not have any beliefs as to how the agents will perform a. If he believes that the agents will use a particular recipe , the ihypothesis rgraph is initialized to that recipe. Otherwise , a recipe is selected arbitrarily from Gl '' s recipe library. The initial hypothesis will be refined , and possibly replaced , on the basis of G2 's utterances. In Step ( 1 ) of the algorithm , the recipe for a is first isolated from the remainder of the rgraph. This recipe , r , represents Gl 'S current beliefs as to how the agents are going to perform a. Step ( 2 ) of the algorithm involves identifying fl with a particular 23 On the basis of this reasoning , G 1 thus attributes belief in more than just a Contributes relation to G 2. In particular , G1 assumes that G2 also believes that fl is compatible with the other acts the agents have agreed upon. 548 Lochbaum A Collaborative Planning Model act fli in r resulting in a new recipe r'. If an appropriate fli can be found , it provides an explanation for G2 's reference to the act ft. If an appropriate fli can not be found , then r can not be the recipe that G2 has in mind for performing oz. The algorithm thus fails in this case and backtracks to select a different recipe for 0~. The new recipe must account for fl as well as all of the other acts previously accounted for by r. Step ( 3 ) of the algorithm ensures that the recipe and act chosen to account for a and fl are compatible with the other acts the agents have already discussed in support of a or the objectives of their other plans. This is done by adding the constraints of the recipe r t to the constraints of the rgraph H and checking that the resulting set is satisfiable. For G1 to agree to the performance of t , the recipe r t must be both internally and externally consistent. That is , the constraints of the recipe must be consistent themselves , as well as being consistent with the constraints of the recipes that G1 believes the agents will use to accomplish their other objectives. 24 The rgraph construction algorithm fails to produce an explanation for an act fl in the context of a PSP for a if the algorithm fails for all of the nondeterministic possibilities. This failure corresponds to a discrepancy between agent Gl 's beliefs and those G1 has attributed to agent G2. The failure thus indicates that further communication and replanning are necessary. To further elucidate the augmentation process , we now return to the dialogues given in Section 1 and show that the processes presented in this section capture the properties highlighted by the informal analyses given in the Introduction. We present each analysis from the perspective of one of the two discourse participants. Each analysis thus indicates the type of reasoning that is required for a system to assume the role of that participant in the dialogue. Figure 1 ) contains two subtask subdialogues. In Section 1 we noted that an OCP must recognize the purpose underlying each subdialogue , as well as the relationship of each purpose to the preceding discourse , in order to respond appropriately to the ICP. The OCP 's recognition of DSPs and their interrelationships is modeled by Case ( 2a ) of the augmentation process in Figure 13. We illustrate its use by modeling the Apprentice 's reasoning concerning the Expert 's first utterance in the second segment in Figure 17 , i.e. , ( 2a ) E : Now remove the pump. At this point in the agents ' discourse , the stack S consists only of a PSP to replace the air compressor 's pump and belt. This PSP corresponds to the overall discourse in Figure 17. The SharedPlan corresponding to the first embedded segment has been completed at this point in the discourse and is thus no longer in focus. 24 Another distinction between our work and Kautz 's ( 1990 ) relates to Step ( 3 ) of the algorithm in Figure 16 and the use of constraints. Whereas rgraphs include an explicit representation of constraints , Kautz 's egraphs do not. Constraints are used to guide egraph construction , but are not part of the representation itself. As a result , Kautz 's algorithms can only check for constraint satisfaction locally. In our algorithm , that would correspond to checking the satisfiability of a recipe 's constraints before adding it to an rgraph , but not afterwards. By checking the satisfiability of the constraint set that results from combining the recipe 's constraints with the rgraph 's constraints , the rgraph construction algorithm is able to detect unsatisfiability earlier than an algorithm that checks constraints only locally. 549 Computational Linguistics Volume 24 , Number 4 E : Replace the pump and belt please. ~ , AA~ OK , I found belt in the back. a Is that where it should be ? • .. \ [ A removes belt\ ] It 's done. E : Now remove the pump. E : First you have to remove the flywheel. • . . E : Now take the pump off the base plate. A : Already did. Figure 17 Sample subtask subdialogues ( Grosz 1974 ) . In utterance ( 2a ) , the Expert expresses her desire that the action remove ( pump ( acl ) , { a } ) be performed</definiens>
				<definiens id="7">the air compressor the agents are working on. The Apprentice 's reasoning concerning this utterance may be modeled using CDRA. Condition ( la ) of CDRA is satisfied by the communication of this utterance to the Apprentice. Condition ( lb ) is satisfied by the context surrounding the agents ' collaboration. Because the Expert is in another room and can only instruct the Apprentice as to which actions to perform , the Expert 's utterance can not be expressing her intention to perform the desired action herself• In addition , because the Apprentice and Expert are both aware that the Apprentice does not have the necessary expertise to perform the action himself , the Apprentice can assume that the Expert must believe the agents can perform the act together , thus satisfying Condition ( lb ) and sanctioning the default conclusion</definiens>
			</definition>
			<definition id="11">
				<sentence>Th ( u , FSP ( { u , s } , Achieve ( freespace~for ( Data , below ( gel ) ) , { u , s } ) ) ) 552 Lochbaum A Collaborative Planning Model modify_network ( NetPiece , Loc , G , T ) { type ( NetPieee , kl-one_.network ) , type ( Loc , ~creen_location ) , l &lt; T2 } display ( NetPiece , G 1 , T1 ) delete ( Data , Loc , G2 , T2 ) modify network ( NetPiece , Loc , G , T ) { type ( NetPiece , kl-one_network ) , type ( Loe , screen_iocation ) , l &lt; T2 } display ( NetPiece , G 1 , T1 ) change ( Data , Loc , G2 , T2 ) modify_network ( NetPiece , Loc , G , T ) { ttype ( NetPiece , kl-one network ) , type ( Loc , screen location ) , mpty ( Loc ) , freespacefor ( Data , Loc ) l &lt; T2 } display ( NetPiece , G 1 , T1 ) put ( Data , Loc , G2 , T2 ) Figure 20 Recipes for modifying a network. modffy_network ( gel , Loc , { u , s } ) { type ( gel , M-onenetwork ) , e ( Loc , screen_location ) } display ( gel , { s } ) delete ( Data , Loc , { u } ) Figure 21 Initial rgraph explaining utterances ( 1 ) - ( 2 ) of the dialogue in Figure 19. where gel represents `` the generic concept called 'employee'. '' To explain the User 's initiation of the subdialogue , the System must determine how the SharedPlan in DSP5 will further the agents ' plan in ( P4 ) . The System 's current beliefs as to how the agents will modify the network , as represented by the rgraph in Figure 21 , do not provide an explanation for the User 's utterance in ( 3 ) . The System 's recipe does not include any type of `` fit '' act. The rgraph construction algorithm thus fails at this point and backtracks to nondeterministically select a different recipe for modifying the network. Suppose that this time the third recipe in Figure 20 is selected ; this is the recipe that includes adding data to the network. The rgraph that results from using this recipe to explain the User 's first utterance is shown in Figure 22. This new rgraph also provide an explanation for the User 's utterance in ( 3 ) ; the `` fit '' act referred to by the User corresponds to the `` put '' act in the rgraph. In addition , the constraints of the recipe , along with the requirements of the ability operators , provide the explanation for the new discourse segment. As discussed in Section 5.1.2 , an agent G 's ability to perform an act fl depends in part on its ability to satisfy the constraints of the recipe in which fl is a constituent. Thus , to perform the act put ( Data , below ( gel ) , { u } ) , the User must be able to satisfy the constraints empty ( below ( gel ) ) and freespace_for ( Data , below ( gel ) ) . The need to satisfy the latter constraint provides the System with an explanation for DSPs. In particular , 553 Computational Linguistics Volume 24 , Number 4 modify_network ( gel , Lee , { u , s } ) ~ { type ( gel , kl-one_network ) , ttype ( Loc , screen_location ) , empty ( Loc ) , freespace_for ( Dat , a , Loc ) } display ( ge 1 , { s } ) put ( Data , Loc , { u } ) Figure 22 Second rgraph explaining utterances ( 1 ) - ( 2 ) of the dialogue in Figure 19. P4 ( a ) PSP ( { u , s } , rnodify_network ( gel , below ( gel ) , { u , s } ) ) { { display ( gel , { s } ) , put ( Data , below ( gel ) , { u } ) } , \ [ 111 { empty ( below ( gel ) ) , freespace_for ( Data , below ( gel ) ) } } in | / ... ... .. ... ... ... ... ... ... ... ... . d / BCBA ( u , put ( Data , below ( gel ) , { u } ) , R , \ [ 2ai\ ] I { empty ( belo ~ ( gel ) ) , freespace for ( Data , below ( gel ) ) } ) J U engages S in P5 because she needs to satisfy ( a ) S explains P5 in terms of the role it plays in completing P4 , namely bnnging about the condition marked ( a ) ... C.~ PSP ( { u , s } , Achieve ( freespace_for ( Data , below ( gel ) ) , { u , s } ) ) | move ( gel , up , { s } ) in \ [ 1\ ] Recipe ( Achieve ( f reespace_for ( Data , below ( gel ) ) , { u , s } ) ) Utterances ( 4 ) - ( 5 ) are understood and produced in this context Figure 23 Analysis of the dialogue in Figure 19. P5 the System can reason that the User initiated the new discourse segment in order to satisfy one of the ability requirements of the agents ' SharedPlan to modify the network. The SharedPlan in DSP5 is thus subsidiary to that in ( P4 ) by virtue of the BCBA requirements of the latter plan. Figure 23 summarizes our analysis of the dialogue. Whereas subtask subdialogues are explained in terms of constituent plan requirements of SharedPlans ( Clause ( 3aii ) ) , correction subdialogues are explained in terms of ability requirements ( Clause ( 2ai ) ) . Once the System recognizes , and explains , the initiation of the new segment , it will interpret the User 's subsequent utterances in the context of its DSP , rather than the previous one. It will thus understand utterance ( 4 ) to contribute to freeing up space on the screen , rather than to modifying the network. This reasoning is modeled by Case ( 3a ) of the augmentation process as follows : First , on the basis of its explanation of DSP5 , the System will take the agents to have a PSP for the act Achieve ( freespace.for ( Data , below ( gel ) ) , { u , s } ) . This plan is marked ( P5 ) in Figure 23 and is pushed onto the stack S above the plan in ( P4 ) . As a result , the System will now take the agents to be focused on the plan in ( P5 ) , rather than that in ( P4 ) , and thus will interpret the User 's subsequent utterances in terms of the information they contribute towards completing the plan in ( P5 ) , rather than that in ( P4 ) . The User 's utterance in ( 4 ) makes reference to an act move ( gel , up , { s } ) . Using the rgraph construction algorithm , this act is understood to directly contribute to the objective of the plan in ( P5 ) , i.e. , Achieve~freespace~or ( Data , below ( gel ) ) , { u , s } ) . The resulting rgraph is shown in Figure 24. This rgraph provides an explanation for utterance ( 4 ) in the context of all of the acts involved in the agents ' plans. 554 Lochbaum A Collaborative Planning Model modify_network ( gel , below ( gel ) , { u , s } ) ~ { type ( gel , kl-one network ) , type ( below ( ge 1 ) , screenJocation ) , empty ( below ( gel ) ) , freespace foffData , below ( ge 1 ) ) } display ( ge 1 , { s } ) put ( Data , below ( gel ) , { u } ) I Achieve ( freespace_for ( Data , below ( gel ) ) , { u , s 1 ) I move ( gel , up , { s } ) Figure 24 Rgraph explaining utterances ( 1 ) - ( 4 ) of the dialogue in Figure 19. As noted in Section 1 , the System 's response to the User 's request in ( 4 ) should take the context of the agents ' entire discourse into account and not simply the context of freeing up space on the screen. In particular , the System should not clear the currently displayed network from the screen to help the User perform the task of putting up some new data , but rather should leave the displayed network visible. The discourse context modeled by the SharedPlans in ( P4 ) and ( P5 ) , as well as the rgraph in Figure 24 , enables the System to respond correctly. In particular , by examining the plans currently in focus and determining what needs to be done to complete them , the System can reason that it should perform an act in support of Achieve ( freespaceqCor ( Data , below ( gel ) ) , { u , s } ) . The System will most likely select the requested act of moving gel up , but if it decides to modify that act in some way or to select a different act , the new act must be compatible with the other acts the agents have agreed upon. By inserting the new act into the rgraph and determining that the resulting rgraph constraints will not be violated by this addition , the System can ensure that its response is in accord with the larger discourse context. ( repeated from Figure 3 ) contains two embedded knowledge precondition subdialogues. We will assume the role of the Network Presenter , NP , in analyzing this example. The overall purpose of the dialogue may be represented as : DSP6 = Int.Th ( nm , FSP ( { nm , np } , maintain ( node39 , { nm , np } ) ) ) and can be recognized on the basis of NM 's utterance in ( 1 ) and CDRA. The purpose of the first subdialogue in Figure 25 can be represented as : DSP7 = Int.Th ( np , FSP ( { nm , np } , Achieve ( has.recipe ( { nm , np } , maintain ( node39 , { nm , np } ) , R ) , { nm , np } ) ) ) . This first subdialogue is initiated by agent NP , the agent whose reasoning we are modeling. We must thus account for NP 's generation of an utterance in this example , rather than his interpretation of another agent 's utterance. As will be discussed in Section 10 , the use of SharedPlans in generation is an area for future research ; however , the basic principles used in interpretation apply here as well. The current state of the agents ' plans provides the basis for an agent 's communication. DSP7 represents NP 's intention that the agents determine a means of diverting network traffic. As discussed in Section 5.1.3 , for a group of agents G to have a col555 Computational Linguistics Volume 24 , Number 4 ( 1 ) NM : It looks like we need to do some maintenance on node39. ( 2 ) NP : Right. ( 3 ) How shall we proceed ? ( 4 ) NM : Well , first we need to divert the traffic to another node. ( 5 ) NP : Okay. ( 6 ) Then we can replace node39 with a higher capacity switch. ( 7 ) NM : Right. ( 8 ) NP : Okay good. FNM : nodes could we divert the traffic to ? Which I ( 10 ) NP : \ [ puts up diagram\ ] ( 11 ) ode41 looks like it could temporarily handle the extra load. ( I~NM : I agree. ( 13 ) Why do n't you go ahead and divert the traffic to node41 and then we can do the replacement. ( 14 ) NP : Okay. ( 15 ) \ [ NP changes network traffic patterns\ ] ( 16 ) That 's done. , ° , Figure 25 Sample knowledge precondition subdialogues. ( Adapted from Lochbaum , Grosz , and Sidner \ [ 1990\ ] . ) laborative plan for an act a , the group must have mutual belief of a recipe for o~. It is this requirement that leads NP to initiate the first subdialogue ; deciding upon a means of performing the objective of the agents ' collaboration is a necessary first step to furthering that collaboration. The plan in DSP7 to agree on a recipe for maintaining node39 thus contributes to the plan in DSP6 to do the maintenance , and is therefore subsidiary to it. Figure 26 provides a graphical representation of this relationship. Once NM agrees to the subsidiary collaboration , either explicitly or implicitly as in utterance ( 4 ) , NP will assume that the agents have a partial SharedPlan to obtain the recipe : ( P7 ) PSP ( { nm , np } , Achieve ( has.recipe ( { nm , np } , rnaintain ( node39 , { nrn , np } ) , R ) , { nm , np } ) ) NP will thus produce his next utterances in the context of the SharedPlan in ( P7 ) , rather than that in DSP6 and will assume that NM will do the same. To make sense of NM 's utterance in ( 4 ) , NP must provide an explanation for it in the context of the agents ' SharedPlan in ( P7 ) . The rgraph construction algorithm is used in modeling NP 's reasoning. Whereas in the case of a subtask subdialogue , the algorithm makes uses of recipes for performing a subtask , in the case of a knowledge precondition subdialogue , it makes use of recipes for satisfying a knowledge precondition. Figure 27 contains two recipes an agent might know to obtain a recipe for an act o~. The first is a single-agent recipe that involves looking up a procedure for a in a manual. The second recipe is a multiagent recipe that involves the agents communicating to come to agreement about the acts and constraints that will comprise their recipe for o~. We use these recipes to model NM 's reasoning concerning utterance ( 4 ) as follows : In Step ( 0 ) of the rgraph construction algorithm , a recipe for the act Achieve ( has.recipe ( { nm , np } , maintain ( node39 , { nm , np } ) , R ) ) is first selected from NP 's recipe library. For illustrative purposes , we will assume that the second recipe in Figure 27 is selected. 556 Lochbaum A Collaborative Planning Model NP engages NM in P7 because the condition marked ( a ) needs to be satisfied PSP ( { n m , np } , A¢hleve ( has.reclpe ( { nrn , np } , malntaln ( node39 , { nm , n p } ) , R ) , { rim , rip } ) ) Utterances ( 4 ) - ( 8 ) are understcod and produced in this context Figure 26 Analysis of the first subdialogue in Figure 25. Achieve ( has.recipe ( G , et , R , T ) , G , T ) I { BeI ( G , R ~ Recipes ( a ) , T ) } look_up ( G , R , Manual , T ) P7 Achieve ( has.recipe ( { G1 , G2 } ,0t , R , T ) , { G1 , G2 } , T ) I { MB ( { G1 , G2 } , Re Recipes ( ~ ) , T ) , MB ( { G1 , G2 } Exists R\ [ { 13i , Oj } ~ Re Recipes ( o0\ ] , T ) l communicate ( Gk , Gm , { 13i , pj } , T ) Figure 27 Recipes for obtaining recipes. Next , we try to identify NM 's communicative act in utterance ( 4 ) with some act in that recipe , and succeed by appropriately instantiating the communicate act. NP is thus able to make sense of NM 's utterance based on his beliefs about ways of obtaining recipes. Now , however , he must decide whether the act that NM is proposing to include as part of their recipe for maintaining node39 is compatible with his beliefs about ways of performing that act. This reasoning is modeled by Step ( 3 ) of the augmentation process in which the constraints of the rgraph are checked for satisfiability. The recipe for obtaining recipes that was selected in Step ( 0 ) of the algorithm indicates that to have a recipe for maintaining node39 , the agents must have mutual belief that some set of acts and constraints constitute a recipe for that act. If NP does not believe that the act divert_traffic ( Nodel , Node2 , G ) should play a role in maintaining node39 , then the constraint will not hold and the algorithm will fail. NP will then communicate his dissent to NM and possibly propose an alternative act. In this instance , however , NP is in agreement with NM , as evidence by his `` Okay '' in utterance , ( 5 ) . The rgraph that results from his reasoning is shown in Figure 28. To produce utterance ( 6 ) , NP must reason about the state of the agents ' SharedPlans and determine what needs to be done to complete them. At this point in the discourse , the agents are focused on obtaining a recipe for maintaining node39 and have agreed that the act of diverting network traffic will be included in that recipe. NP might thus propose the performance of another act as part of their recipe. He does this in utterance ( 6 ) . In utterance ( 7 ) , NM agrees to the inclusion of that act. 557 Computational Linguistics Volume 24 , Number 4 Achieve ( has.recipe ( { nm , np } , maintain ( node39 , { nm , np } ) , R ) , { nm , np } ) { MB ( { nm , np } , Re Recipes ( maintain ( node39 , { nm , np } ) ) , MB ( { nm , np } Exists R \ [ { { divert_traffic ( node39 , ToNode , G 1 ) } , { ty pe ( ToNode , node ) } } Re Recipes ( maintain ( node39 , { nm , np } ) ) \ ] ) } communicate ( nm , np , { { divert traffic ( node39 , ToNode , G 1 ) } , { type ( ToNode , node ) } } ) Figure 28 Rgraph explaining utterances ( 1 ) - ( 5 ) of the dialogue in Figure 25. maintain ( node39 , { nm , np } ) { type ( node39 , node ) , type ( ToNode , node ) , ype , switeh_type ) , T1 &lt; T2 } diverttraffic ( node39 , ToNode , G 1 , T1 ) replace_switch ( node39 , SwitchType , G2 , T2 ) Figure 29 Rgraph explaining utterances ( 1 ) - ( 8 ) of the dialogue in Figure 25. To produce utterance ( 8 ) , NP must once again reason about the state of the agents ' plans. If he believes that diverting network traffic from node39 and then replacing that node with a higher capacity switch will result in maintaining node39 , then he will believe that the agents have completed their SharedPlan in ( P7 ) to obtain a recipe for maintaining the node. His utterance in ( 8 ) indicates that this is the case. Unless agent NM indicates her disagreement , NP will thus assume that the agents have completed their SharedPlan in ( P7 ) and will update his beliefs accordingly. First , he will remove the SharedPlan in ( P7 ) from further consideration ; the agents have completed that plan and have thus satisfied the corresponding discourse purpose. The plan in ( P7 ) is thus popped from NP 's representation of the intentional structure. Second , NP will update his beliefs about the dominating plan in DSP6 based on the knowledge gained during the subdialogue. In particular , the recipe that was decided upon to maintain node39 will be added to the plan and the rgraph will be updated accordingly. Figure 29 contains the rgraph representing the new discourse context after utterance ( 8 ) . Utterance ( 9 ) indicates the initiation of a new discourse segment , the purpose of which can be recognized as : DSP8 = Int.Th ( nm , FSP ( { nm , np } , Achieve ( has.sat.descr ( { nm , np } , ToNode , .T ( divert_traffic , ToNode ) ) , ( nm , np } ) ) ) . using CDRA. As with the other types of subdialogues discussed above , once agent NP recognizes this DSP , he must determine its relationship to the other DSPs underlying the discourse. In this instance , the only other DSP is that underlying the entire discourse. To model agent NP 's reasoning , we must thus determine the relationship of the SharedPlan in DSP8 to that in DSP6. The knowledge precondition requirements of the latter plan provide that explanation. 558 Lochbaum A Collaborative Planning Model P6 PSP ( { nm , npl , maint ain ( node39 , { nm , np } ) ) I { { diverLtraffic ( node39 , ToNede , G1 , T1 ) , \ [ 1\ ] J replace_switch ( node39 , Switch Type , G2 , T 2 ) } , I ( type ( node39 , node ) , type ( ToNode , node ) , I ... . h_ ZI_ : } _ iy_ ? . ... ... I ( a ) BCBA ( Gl , dived traffie ( node39 , ToNode , G1 , T1 ) , R ) \ [ 2ai\ ] l 1 ' I NM engages NP in P8 because NP explains P8 in terms of the role it plays the condition marked ( a ) needs in completing P6 , namely bringing about the to be satisfied condition marked ( a ) I PSP ( { nm , np } , Achieve ( hae.eat.descr ( G1 , ToNode , F ( divert_traffic , ToNode ) ) , { nm , np } ) ) Utterances ( 10 ) - ( 12 ) are understood and produced in this context Figure 30 Analysis of the second subdialogue in Figure 25. Achieve ( has.s at.descr ( O , pi , F ( ~ , pi ) , T ) , { G , G2 } , T ) I { has.sat.descr ( G , D , F ( ~ , p ) : I3 } communicate ( G2 , G , D , T ) Figure 31 A recipe for obtaining a parameter description. P8 As discussed in Section 5.1.3 , an agent G 's ability to perform an act fl depends in part on its ability to identify the parameters of ft. Thus to perform the act divert_traffic ( node39 , ToNode , G ! ) as part of the agents ' Shared.Plan to maintain node39 , the agents must be able to identify the ToNode parameter of the act. The need to identify this parameter thus provides NP with an explanation for DSP8. In particular , NP can reason that NM initiated the new discourse segment in order to satisfy one of the ability requirements of the agents ' SharedPlan to maintain node39. The SharedPlan in DSPs is thus subsidiary to that in DSP6 by virtue of the BCBA requirements of the latter plan. Figure 30 summarizes our analysis of the subdialogue. Once NP recognizes , and explains , the initiation of the new segment , he will produce his subsequent utterances in the context of its DSP , rather than the previous one , and will expect NM to do the same. The rgraph construction algorithm is used in modeling NP 's reasoning. Whereas in the previous example , the algorithm makes use of recipes for obtaining recipes , in this case it makes use of recipes for obtaining parameter descriptions. Figure 31 contains an example of such a recipe. The recipe is derived from the definition of has.sat.descr in Figure 8 and represents that an agent G can bring about has.sat.descr of a parameter Pi by getting another agent G2 to give it a description D of pi. The recipe 's constraints , however , require that D be of the appropriate sort , according to the constraint .T ( 6 , Pi ) , for the identification of the parameter to be successful ( Appelt 1985 ; Kronfeld 1986 , 1990 ; Hintikka 1978 ) . Given the discourse context represented by Figure 30 then , NP should respond to NM 's utterance in ( 9 ) on the basis of his beliefs about ways in which to identify parameters. For example , if NP knows the recipe in Figure 31 , then he might respond to NM by communicating some node description to her. As we noted in Section 1 , however , the description that NP uses must be one that is appropriate for the current circumstances. In particular , NP should respond to NM with a description that will 559 Computational Linguistics Volume 24 , Number 4 enable both of the agents to identify the node for the purposes of diverting network traffic. The rgraph in Figure 29 and the constraints of the recipe in Figure 31 provide the necessary context for modeling NP 's behavior. Because NP knows that the agents are trying to divert network traffic as part of maintaining node39 , as represented by the rgraph in Figure 29 , he should first choose a node that is appropriate for that circumstance. For example , he might choose a node that is spatially close to node39 , rather than one that , while lightly loaded , is more distant. After selecting the node , NP should then choose a means of identifying it for NM. For example , he might present her with a diagram of the network and then tell her how to identify the particular node on the diagram ; NP 's response in utterances ( 10 ) and ( 11 ) takes this form. It would not be appropriate , however , for NP to respond to NM with some internal node name , or with a description like `` the node with the lightest traffic , '' unless he believed that NM could identify the node on the basis of that description. The constraints of the recipe in Figure 31 model this requirement. They represent that the description communicated by an agent should be one that will allow the other agent to identify the object in question for the purpose of the act to be performed. Grosz and Sidner ( 1990 ) have argued that a theory of DSP recognition depends upon an underlying theory of collaborative plans. Although SharedPlans provide that latter theory , the connection between SharedPlans and DSPs was never specified. In this paper , we have presented a SharedPlan model for recognizing DSPs and their interrelationships. We now show that this model satisfies the requirements set out by Grosz and Sidner 's ( 1986 ) theory of discourse structure. We first discuss the process by which intentional structure is recognized. Next , we discuss the way in which intentional structure interacts with the attentional state component of discourse structure. And finally , we discuss the contextual use of intentional structure in interpretation. course structure , Grosz and Sidner give several examples of the types of intentions that could serve as DSPs ( Grosz and Sidner 1986 , 179 ) : agent intend to perform some physical task. agent believe some fact. agent believe that one fact supports another. agent intend to identify an object. agent know some property of an object. Intentions such as these , as well as segment beginnings and endings , might be recognized on the basis of linguistic markers , utterance-level intentions , or knowledge about actions and objects in the domain of discourse ( Grosz and Sidner 1986 ) . In our model , DSPs take the form Int.Th ( ICP , FSP ( { ICP , OCP } , fl ) ) . This type of DSP addresses several problems with the above examples -- problems that motivated Grosz and Sidner 's ( 1990 ) subsequent work on SharedPlans -- namely the case of one agent intending another to do something and the so-called master/slave assumption. We recognize DSPs using the conversational default rule , CDRA. This rule provides a means of recognizing the initiation of new segments and their purposes based on the 560 Lochbaum A Collaborative Planning Model propositional content of utterances. Although this use of CDRA is admittedly limited-it requires an ICP to communicate the act that it desires to collaborate on at the outset of a segment -- other sources of information , such as those cited above , could also be incorporated into the model to aid in the recognition of new segments and their corresponding SharedPlans. SharedPlans can also be used in recognizing the completion of discourse segments. Case ( 2b ) of the augmentation process in Figure 13 outlines the required reasoning. A discourse segment is complete when all of the beliefs and intentions required to complete its corresponding SharedPlan have been established. This use of SharedPlans also appears at first glance to be of limited use -- -the mental attitudes required of a full SharedPlan may not all be explicitly established over the course of a dialogue or subdialogue. However , the OCP may be able to infer the completion of a SharedPlan , and thus the corresponding segment , in combination with information from other sources. For example , suppose an OCP has some reason to expect the end of a segment based on a linguistic signal such as an intonational feature ( e.g. , as described by Grosz and Hirschberg \ [ 1992\ ] ) . If additionally the OCP is able to ascribe the various mental attitudes `` missing '' from the SharedPlan that corresponds to that segment , then the OCP has further evidence for the segment boundary. These mental attitudes may be ascribed on the basis of those of the OCP 's beliefs that are in accord with the mental attitudes comprising the SharedPlan ( Pollack 1986a ; Grosz and Sidner 1990 ) . nizes the initiation of a new discourse segment , it must determine the relationship of that segment 's DSP to the other DSPs underlying the discourse ( Grosz and Sidner 1986 ) . In our model , relationships between SharedPlans provide the basis for determining the corresponding relationships between DSPs. An OCP must determine how the SharedPlan used to model a segment 's DSP is related to the other SharedPlans underlying the discourse. The information that an OCP considers in determining this relationship is delineated by the beliefs and intentions that are required to complete each of the other plans. In this way , our model provides a more detailed account of the relationships that can hold between DSPs than did Grosz and Sidner 's original formulation. One DSP dominates another if the second provides part of the satisfaction of the first. In our model , subsidiary relationships between SharedPlans provide a means of determining dominance relationships between DSPs. If one plan is subsidiary to another , then the DSP that is modeled using the first plan is dominated by that modeled using the second. One DSP satisfaction-precedes another if the first must be satisfied before the second. This relationship corresponds to a temporal dependency between SharedPlans. When one SharedPlan must be completed before another , the DSP that is modeled using the first satisfaction-precedes that modeled using the second. The attentional state component of discourse structure is an abstraction of the discourse participants ' focus of attention ; it is modeled using a stack of focus spaces , one for each segment. Each focus space contains its segment 's DSP , as well as those objects , properties , and relations that become salient over the course of the segment. One of the primary roles of the focus space stack is to constrain the range of DSPs to which a new DSP can be related ; a new DSP can only be dominated by a DSP in some space on the stack. In our model , a segment 's focus space contains a DSP of the form Int.Th ( ICP , FSP ( { ICP , OCP } , fl ) ) . The operations on the focus space stack depend upon subsidiary rela561 Computational Linguistics Volume 24 , Number 4 tionships between SharedPlans in the same way that Grosz and Sidner ( 1986 ) describe the operations as depending upon DSP relationships. As each SharedPlan corresponding to a discourse segment is completed , the segment 's focus space is popped from the stack. Only those SharedPlans in some space on the stack are candidates for subsidiary relationships. The use of the SharedPlan stack S in the augmentation process of Figure 13 reflects the operations of the focus space stack. An utterance of a discourse can either begin a new segment of the discourse , complete the current segment , or contribute to it ( Grosz and Sidner 1986 ) . Each of these possibilities is modeled by a separate case within the augmentation process given in Figure 13. The initiation and completion of discourse segments was discussed in Section 7.1. Our discussion here is thus restricted to the case of an utterance 's contributing to a discourse segment. Under Grosz and Sidner 's theory , each utterance of a discourse segment contributes some information towards achieving the purpose of that segment. In our model , each utterance is understood in terms of the information it contributes towards completing the corresponding SharedPlan. The FSP definition in Figure 5 constrains the range of information that an utterance of a segment can contribute towards the segment 's SharedPlan. Hence , if an utterance can not be understood as contributing information to the current SharedPlan , then it can not be part of the current discourse segment. That is , the utterance must begin a new segment of the discourse or complete the current segment , but it can not contribute to it. In this way , our model provides a more detailed account of the role that intentional structure plays as context in interpreting utterances than did Grosz and Sidner 's original formulation. Because each utterance of a discourse segment contributes some information towards the purpose of that segment , the segment 's DSP may not be completely determined until the last utterance of the segment. However , as Grosz and Sidner ( 1986 ) have argued , the OCP must be able to recognize initially at least a generalization of the DSP so that the proper moves of attentional state can be made. Although CDRA provides a limited method of recognizing new segments and their purposes , it does conform to this aspect of Grosz and Sidner 's theory. In particular , the initial purpose of a segment , as recognized by CDRA , is quite generally specified ; it consists only of the intention that the agents form a SharedPlan. However , as the utterances of a discourse segment provide information about the details of that plan , the segment 's purpose becomes more completely determined. In particular , the purpose comes to include the mental attitudes required of a full SharedPlan and established by the dialogue. Additionally , although the objective of the agents ' plan may only be abstractly specified when it is initially recognized , it too may be further refined by the utterances of the segment. Early work on plan recognition in discourse ( Allen and Perrault 1980 ; Cohen , Perrault , and Allen 1982 ) focused on the problem of reasoning about single utterances. 25 Subsequent work ( Sidner and Israel 1981 ; Sidner 1983 , 1985 ; Carberry 1987 ) extended the earlier approaches to recognize speaker 's intentions across multiple utterances. All of 25 More recent work in the area of single utterance reasoning includes that of Cohen and Levesque ( 1990 ) and Perrault ( 1990 ) . Their work provides a detailed mental state model of speech act processing and is thus focused at a different level of granularity than the work discussed in this paper. 562 Lochbaum A Collaborative Planning Model ( 1 ) User : Show me the generic concept called `` employee '' . ( 2 ) System : OK. &lt; system displays network &gt; ( 3 ) User : I ca n't fit a new ic below it .</sentence>
				<definiendum id="0">Th</definiendum>
				<definiendum id="1">FSP</definiendum>
				<definiendum id="2">Achieve ( freespace~for ( Data</definiendum>
				<definiendum id="3">Lochbaum A Collaborative Planning Model modify_network ( NetPiece , Loc , G , T ) { type</definiendum>
				<definiendum id="4">NetPiece , G 1</definiendum>
				<definiendum id="5">T1 ) put ( Data , Loc , G2 , T2</definiendum>
				<definiendum id="6">gel</definiendum>
				<definiendum id="7">PSP</definiendum>
				<definiendum id="8">collaboration</definiendum>
				<definiendum id="9">rgraph construction algorithm</definiendum>
				<definiendum id="10">R ) )</definiendum>
				<definiendum id="11">MB</definiendum>
				<definiendum id="12">NP</definiendum>
				<definiendum id="13">Okay</definiendum>
				<definiendum id="14">MB</definiendum>
				<definiendum id="15">Collaborative Planning Model P6 PSP</definiendum>
				<definiendum id="16">... ... I</definiendum>
				<definiendum id="17">p )</definiendum>
				<definiendum id="18">DSPs</definiendum>
				<definiendum id="19">discourse segment</definiendum>
				<definiendum id="20">OCP</definiendum>
				<definiendum id="21">OCP</definiendum>
				<definiendum id="22">SharedPlan</definiendum>
				<definiens id="0">change ( Data , Loc , G2 , T2 ) modify_network ( NetPiece , Loc , G , T ) { ttype ( NetPiece , kl-one network )</definiens>
				<definiens id="1">an abstraction of the discourse participants ' focus of attention ; it is modeled using a stack of focus spaces</definiens>
				<definiens id="2">contributing information to the current SharedPlan , then it can not be part of the current discourse segment. That is , the utterance must begin a new segment of the discourse or complete the current segment</definiens>
			</definition>
			<definition id="12">
				<sentence>PLAN5 IDENTIFY-PARAMETER ( user , system , MI , CI , PLAN4 ) I INFORMREF ( user , syst~m , MI , WANT ( user , Ml ) ) REQUEST ( USer , system , M1 ) I SURFACE-REQUEST ( user , system , INFORMIF ( system , user , CANDO ( system , Ml ) ) ) where PARAMETER ( Mi , Ci ) STEP ( CI , PLAN4 ) PARAMETER ( MI , WANT ( user , MI ) ) WANT ( system , PLAN4 ) PLAN4 PLAN2 CI : CORRECT-PLAN ( user , system , DI , Mi , FI , PLAN2 ) I REQUEST ( use~ , system , Fl ) SURFACE-INFORM ( user , system , -CANDO ( user , FI ) ) ADD-DATA ( user , EI , ?</sentence>
				<definiendum id="0">PLAN5 IDENTIFY-PARAMETER</definiendum>
				<definiendum id="1">PARAMETER</definiendum>
				<definiens id="0">CORRECT-PLAN ( user , system , DI , Mi , FI , PLAN2 ) I REQUEST ( use~ , system , Fl ) SURFACE-INFORM ( user , system , -CANDO ( user</definiens>
			</definition>
			<definition id="13">
				<sentence>`` Discourse intentions are purposes of the speaker , expressed in terms of both the task plans of the speaker ( the domain plans ) and the plans recursively generated by these plans ( the discourse plans ) '' ( Litman and Allen , 1990 , 376 ) .</sentence>
				<definiendum id="0">Discourse intentions</definiendum>
				<definiens id="0">the discourse plans ) ''</definiens>
			</definition>
			<definition id="14">
				<sentence>On the basis of his agenda , G1 chooses an item to which to direct his attention , decides what he wants to say about that item , and then does so ( Step ( 5b ) of the process in Figure 37 ) .</sentence>
				<definiendum id="0">G1</definiendum>
				<definiens id="0">chooses an item to which to direct his attention , decides what he wants to say about that item</definiens>
			</definition>
			<definition id="15">
				<sentence>G1 is the speaker and must decide what to communicate .</sentence>
				<definiendum id="0">G1</definiendum>
				<definiens id="0">the speaker and must decide what to communicate</definiens>
			</definition>
			<definition id="16">
				<sentence>( a ) If the Agenda is empty , then G1 believes that the agents ' PSP is complete and so communicates that belief to G2 .</sentence>
				<definiendum id="0">PSP</definiendum>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>Sense discrimination divides the occurrences of a word into a number of classes by determining for any two occurrences whether they belong to the same sense or not .</sentence>
				<definiendum id="0">Sense discrimination</definiendum>
				<definiens id="0">divides the occurrences of a word into a number of classes by determining for any two occurrences whether they belong to the same sense or not</definiens>
			</definition>
			<definition id="1">
				<sentence>1 Contextgroup discrimination groups the occurrences of an ambiguous word into clusters , where clusters consist of contextually similar occurrences .</sentence>
				<definiendum id="0">Contextgroup discrimination</definiendum>
				<definiens id="0">groups the occurrences of an ambiguous word into clusters</definiens>
			</definition>
			<definition id="2">
				<sentence>The cosine is equivalent to the normalized correlation coefficient : corr ( fi , ~ ) = ~iN=l ViWi where ff and ~ are vectors and N is the dimension of the vector space .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the dimension of the vector space</definiens>
			</definition>
			<definition id="3">
				<sentence>A neighbor is any word that occurs at a distance of at most 25 words from the ambiguous word ( that is , in a 50word window centered on the ambiguous word ) .</sentence>
				<definiendum id="0">neighbor</definiendum>
				<definiens id="0">any word that occurs at a distance of at most 25 words from the ambiguous word ( that is , in a 50word window centered on the ambiguous word )</definiens>
			</definition>
			<definition id="4">
				<sentence>A context vector is the centroid ( or sum ) of the vectors of the words occurring in the context .</sentence>
				<definiendum id="0">context vector</definiendum>
				<definiens id="0">the centroid ( or sum ) of the vectors of the words occurring in the context</definiens>
			</definition>
			<definition id="5">
				<sentence>A rough measure of how well word wi discriminates between different topics is the log inverse document frequency used in information retrieval ( Salton and Buckley 1990 ) : ai = l°g/~ / where ni is the number of documents that wi occurs in and N is the total number of documents .</sentence>
				<definiendum id="0">ni</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">A rough measure of how well word wi discriminates between different topics is the log inverse document frequency used in information retrieval</definiens>
				<definiens id="1">the number of documents that wi occurs in and</definiens>
				<definiens id="2">the total number of documents</definiens>
			</definition>
			<definition id="6">
				<sentence>Sense vectors are derived by clustering the context vectors of an ambiguous word ( here , cl , c2 , c3 , c4 , c5 , c6 , c7 , and cs ) , and computing sense vectors as the centroids of the resulting clusters .</sentence>
				<definiendum id="0">Sense vectors</definiendum>
				<definiens id="0">clustering the context vectors of an ambiguous word ( here , cl , c2 , c3 , c4 , c5 , c6 , c7 , and cs ) , and computing sense vectors as the centroids of the resulting clusters</definiens>
			</definition>
			<definition id="7">
				<sentence>SVD is a form of dimensionality reduction that finds the major axes of variation in Word Space .</sentence>
				<definiendum id="0">SVD</definiendum>
				<definiens id="0">a form of dimensionality reduction that finds the major axes of variation in Word Space</definiens>
			</definition>
			<definition id="8">
				<sentence>Context-group discrimination uses word vectors and sense vectors as follows to discriminate occurrences of the ambiguous word .</sentence>
				<definiendum id="0">Context-group discrimination</definiendum>
				<definiens id="0">uses word vectors and sense vectors as follows to discriminate occurrences of the ambiguous word</definiens>
			</definition>
			<definition id="9">
				<sentence>Long queries ( as they may arise in an IR system after relevance feedback ) provide more context than the short queries Voorhees worked with in her experiments .</sentence>
				<definiendum id="0">Long queries</definiendum>
				<definiens id="0">they may arise in an IR system after relevance feedback</definiens>
			</definition>
			<definition id="10">
				<sentence>The test corpus in Sch~tze and Pedersen ( 1995 ) is the Category B TREC-1 collection ( about 170,000 documents from the Wall Street Journal ) in conjunction with its queries 51-75 ( Harman 1993 ) .</sentence>
				<definiendum id="0">test corpus</definiendum>
				<definiens id="0">the Category B TREC-1 collection ( about 170,000 documents from the Wall Street Journal</definiens>
			</definition>
			<definition id="11">
				<sentence>Kelly and Stone ( 1975 ) consider hand-constructed disambiguation rules ; Lesk ( 1986 ) , Krovetz and Croft ( 1989 ) , Guthrie et al. ( 1991 ) , and Karov and Edelman ( 1996 ) use on-line dictionaries ; Hirst ( 1987 ) constructs knowledge bases ; Cottrell ( 1989 ) uses syntactic and semantic structure encoded in a connectionist net ; Brown et al. ( 1991 ) and Church and Gale ( 1991 ) exploit bilingual corpora ; Dagan , Itai , and Schwall ( 1991 ) use a bilingual dictionary ; Hearst ( 1991 ) , Leacock , Towell , and Voorhees ( 1993 ) , Niwa and Nitta ( 1994 ) , and Bruce and Wiebe ( 1994 ) exploit a hand-labeled training set ; and Yarowsky ( 1992 ) and Walker and Amsler ( 1986 ) perform computations based on a hand-constructed semantic categorization of words ( Roget 's Thesaurus and Longman 's subject codes , respectively ) .</sentence>
				<definiendum id="0">Schwall</definiendum>
				<definiendum id="1">Voorhees</definiendum>
				<definiens id="0">1987 ) constructs knowledge bases</definiens>
				<definiens id="1">use a bilingual dictionary ; Hearst ( 1991 ) , Leacock , Towell , and</definiens>
				<definiens id="2">exploit a hand-labeled training set ; and Yarowsky ( 1992 ) and Walker and Amsler ( 1986 ) perform computations based on a hand-constructed semantic categorization of words ( Roget 's Thesaurus and Longman 's subject codes , respectively )</definiens>
			</definition>
			<definition id="12">
				<sentence>criminal robe Word Space SVD Space Word Space SVD Space gangster 0.17 0.61 0.15 -0.52 criminal 0.41 0.37 where p = min { m , n } , U ( the left matrix ) is an orthonormal m-by-p matrix , V ( the right matrix ) is an orthonormal n-by-p matrix and diag ( o.1 ... . , o.p ) is a matrix with the diagonal elements o.1 _ &gt; o'2 &gt; `` '' _ &gt; o.p ~_ 0 ( and the value zero for nondiagonal elements ) ( Golub and van Loan 1989 ) .</sentence>
				<definiendum id="0">matrix )</definiendum>
				<definiendum id="1">o.p )</definiendum>
				<definiens id="0">an orthonormal m-by-p matrix</definiens>
				<definiens id="1">the right</definiens>
			</definition>
			<definition id="13">
				<sentence>The EM algorithm is an iterative procedure that , starting from an initial hypothesis of the cluster parameters , improves the estimates of the parameters in each iteration .</sentence>
				<definiendum id="0">EM algorithm</definiendum>
				<definiens id="0">an iterative procedure that , starting from an initial hypothesis of the cluster parameters , improves the estimates of the parameters in each iteration</definiens>
			</definition>
			<definition id="14">
				<sentence>p ( ~l~j ; o k ) -~j ( ~ ; ) P ( ~j ) G~ P ( ~ I ~ , ; o~ ) The M step computes the most likely parameters of the distribution given the cluster membership probabilities : ~\ ] i=1 /j k+l E/N1 hq ( 2 : i lif ) ( Y : i 11~ ) T ~j = ~N=lhq These are the well-known maximum-likelihood estimates for mean and variance of a Gaussian .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the well-known maximum-likelihood estimates for mean and variance of a Gaussian</definiens>
			</definition>
			<definition id="15">
				<sentence>The initial cluster parameters are computed by applying group-average agglomerative clustering to a sample of size v'N. Appendix C : Agglomerative Clustering Agglomerative clustering is a clustering technique that starts by assigning each element to a different cluster and then iteratively merges clusters according to a goodness criterion until the desired number of clusters has been reached .</sentence>
				<definiendum id="0">initial cluster parameters</definiendum>
				<definiendum id="1">Agglomerative Clustering Agglomerative clustering</definiendum>
				<definiens id="0">a clustering technique that starts by assigning each element to a different cluster</definiens>
			</definition>
			<definition id="16">
				<sentence>30 Marti A. Hearst , Christian Plaunt , Subtopic structuring for full-length document access , Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval , p.59-68 , June 27-July 01 , 1993 , Pittsburgh , Pennsylvania , United States 31 Graeme Hirst , Semantic interpretation and the resolution of ambiguity , Cambridge University Press , New York , NY , 1987 32 Anil K. Jain , Richard C. Dubes , Algorithms for clustering data , Prentice-Hall , Inc. , Upper Saddle River , NJ , 1988 33 Karov , Yael and Shimon Edelman .</sentence>
				<definiendum id="0">Subtopic structuring</definiendum>
				<definiens id="0">the 16th annual international ACM SIGIR conference on Research and development in information retrieval</definiens>
			</definition>
			<definition id="17">
				<sentence>36 Robert Krovetz , Homonymy and polysemy in information retrieval , Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics , p.72-79 , July 07-12 , 1997 , Madrid , Spain 37 R. Krovetz , W. B. Croft , Word sense disambiguation using machine-readable dictionaries , Proceedings of the 12th annual international ACM SIGIR conference on Research and development in information retrieval , p.127-136 , June 25-28 , 1989 , Cambridge , Massachusetts , United States 38 Robert Krovetz , W. Bruce Croft , Lexical ambiguity and information retrieval , ACM Transactions on Information Systems ( TOIS ) , v.10 n.2 , p.115-141 , April 1992 39 Leacock , Claudia , Geoffrey Towell , and Ellen Voorhees .</sentence>
				<definiendum id="0">Proceedings</definiendum>
				<definiendum id="1">information retrieval</definiendum>
				<definiens id="0">of the 12th annual international ACM SIGIR conference on Research and development in information retrieval</definiens>
			</definition>
			<definition id="18">
				<sentence>47 Fernando Pereira , Naftali Tishby , Lillian Lee , Distributional clustering of English words , Proceedings of the 31st annual meeting on Association for Computational Linguistics , p.183-190 , June 22-26 , 1993 , Columbus , Ohio 48 Yonggang Qiu , Hans-Peter Frei , Concept based query expansion , Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval , p.160-169 , June 27-July 01 , 1993 , Pittsburgh , Pennsylvania , United States 49 Gerda Ruge , Experiment on linguistically-based term associations , Information Processing and Management : an International Journal , v.28 n.3 , p.317-332 , 1992 50 Salton , Gerard .</sentence>
				<definiendum id="0">Information Processing</definiendum>
				<definiendum id="1">Management</definiendum>
				<definiens id="0">Hans-Peter Frei , Concept based query expansion , Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval , p.160-169</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>The disambiguator is a particular formulation of feed-forward neural networks ( Rumelhart , Hinton , and Williams 1986 ) that separately extract topical and local contexts of a target word from a set of sample sentences that are tagged with the correct sense of the target .</sentence>
				<definiendum id="0">disambiguator</definiendum>
				<definiens id="0">a particular formulation of feed-forward neural networks</definiens>
			</definition>
			<definition id="1">
				<sentence>The topical component consists of substantive words that are likely to co-occur with a given sense of the target word .</sentence>
				<definiendum id="0">topical component</definiendum>
				<definiens id="0">consists of substantive words that are likely to co-occur with a given sense of the target word</definiens>
			</definition>
			<definition id="2">
				<sentence>The non-input 1 where x is the sum of the incoming activations units compute the function y l+e-x weighted by the links and y is the output activation .</sentence>
				<definiendum id="0">x</definiendum>
				<definiens id="0">the sum of the incoming activations units compute the function y l+e-x weighted by the links and y is the output activation</definiens>
			</definition>
			<definition id="3">
				<sentence>In networks that extract topical context , the number of input units is equal to the number of tokens that appear three or more times in the training set , where a token is the string remaining after text processing .</sentence>
				<definiendum id="0">token</definiendum>
				<definiens id="0">the string remaining after text processing</definiens>
			</definition>
			<definition id="4">
				<sentence>`` Correct ( all ) '' gives the accuracy over all examples .</sentence>
				<definiendum id="0">Correct ( all ) ''</definiendum>
				<definiens id="0">gives the accuracy over all examples</definiens>
			</definition>
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>Whether our end-goal is the construction of artifacts that use natural languages intelligently , the formal characterization of phenomena in human languages , or the computational modeling of the human language processing mechanism , we can not ignore the fact that language is both spoken ( or written ) and heard ( or read ) .</sentence>
				<definiendum id="0">end-goal</definiendum>
				<definiens id="0">the construction of artifacts that use natural languages intelligently , the formal characterization of phenomena in human languages , or the computational modeling of the human language processing mechanism</definiens>
			</definition>
			<definition id="1">
				<sentence>A more recent definition with a slightly different emphasis can be found in Reiter and Dale ( 1997 , 57 ) : Natural language generation is the subfield of artificial intelligence and computational linguistics that is concerned with the construction of computer systems that can produce understandable texts in ... human languages from some underlying non-linguistic representation of information .</sentence>
				<definiendum id="0">Natural language generation</definiendum>
				<definiens id="0">the subfield of artificial intelligence and computational linguistics that is concerned with the construction of computer systems that can produce understandable texts in ... human languages from some underlying non-linguistic representation of information</definiens>
			</definition>
			<definition id="2">
				<sentence>An overview of SURGE : A reusable comprehensive syntactic realisation component .</sentence>
				<definiendum id="0">overview of SURGE</definiendum>
				<definiens id="0">A reusable comprehensive syntactic realisation component</definiens>
			</definition>
			<definition id="3">
				<sentence>Natural Language Production as a Process of Decision Making under Constraint .</sentence>
				<definiendum id="0">Natural Language Production</definiendum>
				<definiens id="0">a Process of Decision Making under Constraint</definiens>
			</definition>
			<definition id="4">
				<sentence>The Generation Gap : The Problem of Expressibility in Text Planning .</sentence>
				<definiendum id="0">Generation Gap</definiendum>
				<definiens id="0">The Problem of Expressibility in Text Planning</definiens>
			</definition>
</paper>

		<paper id="3003">
			<definition id="0">
				<sentence>In this approach , lexical entries therefore have two main components : a denotation that defines the applicability conditions of the lexeme with respect to the domain model ( i.e. , it can be matched against the generator 's input ) , and a partial semantic specification ( PSemSpec ) , which specifies the contribution that the lexeme makes to sentence meaning .</sentence>
				<definiendum id="0">PSemSpec</definiendum>
				<definiens id="0">specifies the contribution that the lexeme makes to sentence meaning</definiens>
			</definition>
			<definition id="1">
				<sentence>The UM is a taxonomy of event ( e.g. , to inflame ) or its coming to an end .</sentence>
				<definiendum id="0">UM</definiendum>
				<definiens id="0">a taxonomy of event ( e.g. , to inflame</definiens>
			</definition>
			<definition id="2">
				<sentence>With circumstances , the situation is different : A SitSpec is complete and wellformed without the information on , for instance , the location of an event .</sentence>
				<definiendum id="0">SitSpec</definiendum>
				<definiens id="0">complete and wellformed without the information on</definiens>
			</definition>
			<definition id="3">
				<sentence>Partial SemSpec ( PSemSpec ) : The contribution that the lexeme can make to a sentence SemSpec .</sentence>
				<definiendum id="0">Partial SemSpec</definiendum>
				<definiens id="0">The contribution that the lexeme can make to a sentence SemSpec</definiens>
			</definition>
			<definition id="4">
				<sentence>To compute these configurations automatically , such that valency and meaning are changed in parallel , we define an alternation or extension rule as a 5-tuple with the following components : NAM : a unique name ; DXT : extension of denotation ; C0V : additions to the covering-list ; ROC : role changes in PSemSpec ; NR0 : additional PSemSpec roles and fillers .</sentence>
				<definiendum id="0">NAM</definiendum>
				<definiens id="0">a unique name</definiens>
			</definition>
			<definition id="5">
				<sentence>reading , which we do not handle here , reverses the directionality of the path involved .</sentence>
				<definiendum id="0">reading</definiendum>
			</definition>
			<definition id="6">
				<sentence>Applying the changes to the PSemSpec results in ( X / nondirected-action : lex fill : inclusive B : actor A &lt; : destination C &gt; ) which corresponds to the sentence The tank filled with the water .</sentence>
				<definiendum id="0">lex fill</definiendum>
				<definiens id="0">inclusive B : actor A &lt; : destination C &gt; ) which corresponds to the sentence The tank filled with the water</definiens>
			</definition>
			<definition id="7">
				<sentence>Levin discusses a causative/inchoative alternation that applies to a large number of verbs .</sentence>
				<definiendum id="0">Levin</definiendum>
			</definition>
			<definition id="8">
				<sentence>NAM : durative-causative DXT : ( activity ( CAUSER X ) ) COY : ( ) ROC : ( ( : actor : actee ) ) NRO : ( : actor X ) NAM : resultative-causative DXT : ( event ( ACTIVITY ( X ( CAUSER Y ) ) ) ) COY : ( ) ROC : ( ( : actor : actee ) ) NRO : ( : actor Y ) The first rule derive~for examplG Tom walked the dog from The dog walke~ andthe second Tom closed thedoor ~omThe doorclosed .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">durative-causative DXT : ( activity ( CAUSER X ) ) COY : ( ) ROC : ( ( : actor : actee ) ) NRO : ( : actor X</definiens>
			</definition>
			<definition id="9">
				<sentence>The exact interpretation of completion-state is the open question that Levin ( 1993 ) referred to , and that Jackendoff treated with his d subscript .</sentence>
				<definiendum id="0">exact interpretation of completion-state</definiendum>
				<definiens id="0">the open question that Levin ( 1993 ) referred to , and that Jackendoff treated with his d subscript</definiens>
			</definition>
			<definition id="10">
				<sentence>NAM : locative-transitive DXT : ( event ( MOVE ( OBJECT X ) ( PATH ( DESTINATION Y ) ) ) ( POST-STATE ( Z completion-state ( OB3ECT Y ) ) ) ) COY : ( Z ) ROC : ( ( : actee &lt; : inclusive &gt; ) ( : destination : actee ) ) NRO : ( ) 416 Stede Verb Alternations Levin distinguishes two kinds of locative alternation : the spray/load alternation just discussed and the clear ( transitive ) alternation .</sentence>
				<definiendum id="0">MOVE</definiendum>
				<definiendum id="1">OBJECT X ) ( PATH</definiendum>
			</definition>
			<definition id="11">
				<sentence>It derives , for example , Tom drained the container of the water from Tom drained the water from the container , n NAM : clear-transitive DXT : ( event ( MOVE ( OBJECT X ) ( PATH ( SOURCE Y ) ) ) ( POST-STATE ( Z completion-state ( OBJECT Y ) ) ) ) C0V : ( Z ) ROC : ( ( : actee &lt; : of-matter &gt; ) ( : source : actee ) ) NR0 : ( ) The clear verbs , except for to clean , can in addition be intransitive , and Levin states a separate alternation for them .</sentence>
				<definiendum id="0">MOVE</definiendum>
				<definiendum id="1">Levin states</definiendum>
				<definiens id="0">a separate alternation for them</definiens>
			</definition>
			<definition id="12">
				<sentence>Resultative-causative of ( 1 ) : Denotation : ( event ( ACTIVITY ( OBJECT A ) ( PATH ( SOURCE B ) ) ( CAUSER C ) ) ( POST-STATE ( C ( OBJECT B ) ) ) ) PSemSpec : ( xl / directed-action : lex drain : of-matter A : actee B : actor C ) ( 3 ) Tom drained the engine of the oil .</sentence>
				<definiendum id="0">ACTIVITY ( OBJECT A )</definiendum>
				<definiendum id="1">PATH</definiendum>
				<definiendum id="2">POST-STATE ( C</definiendum>
				<definiens id="0">of-matter A : actee B : actor C ) ( 3 ) Tom drained the engine of the oil</definiens>
			</definition>
			<definition id="13">
				<sentence>The SemSpec language is a subset of the input representation language that was developed for Penman , the sentence plan language ( SPL ) ( Kasper 1989 ) .</sentence>
				<definiendum id="0">SemSpec language</definiendum>
				<definiens id="0">a subset of the input representation language that was developed for Penman , the sentence plan language ( SPL )</definiens>
			</definition>
			<definition id="14">
				<sentence>An SPL expression consists of variables , types , and case roles ; an example was given in Figure 3 .</sentence>
				<definiendum id="0">SPL expression</definiendum>
				<definiens id="0">consists of variables , types , and case roles</definiens>
			</definition>
			<definition id="15">
				<sentence>Penman and SPL are based on the upper model ( UM ) ( Bateman et al. 1990 ) introduced 12 Fr6hlich and van de Riet ( 1997 ) describe how MOOSE is employed in the generation component of an information system .</sentence>
				<definiendum id="0">UM )</definiendum>
				<definiens id="0">describe how MOOSE is employed in the generation component of an information system</definiens>
			</definition>
			<definition id="16">
				<sentence>•/ i~ : SitSpec Lexicon Morphosyntax Morphosyntax Mowhosyntax Alternations Alternations Alternations Partial SemSpec Partial SemSpec Partial SemSpec Connotation Connotation Connotation Denotation Denotation Denotation Alternation/ Extension rules -Ve lzatlon @ ... ... ... ... ... .</sentence>
				<definiendum id="0">SemSpec Partial SemSpec Connotation Connotation Connotation Denotation Denotation Denotation Alternation/ Extension</definiendum>
				<definiens id="0">SitSpec Lexicon Morphosyntax Morphosyntax Mowhosyntax Alternations Alternations Alternations Partial SemSpec Partial</definiens>
			</definition>
			<definition id="17">
				<sentence>Thus , a SemSpec is a lexicalized structure , and accordingly , Moose interprets the upper model as a taxonomy of lexical classes .</sentence>
				<definiendum id="0">SemSpec</definiendum>
				<definiens id="0">a lexicalized structure</definiens>
			</definition>
			<definition id="18">
				<sentence>This contradicts the Penman philosophy of viewing the UM as abstract semantics and clearly distinct from the generation grammar , which in accordance with systemic-functional linguistics is an integrated lexicogrammar , with `` lexis as most delicate grammar '' ( Hasan 1987 ) .</sentence>
				<definiendum id="0">generation grammar</definiendum>
				<definiens id="0">in accordance with systemic-functional linguistics is an integrated lexicogrammar</definiens>
			</definition>
			<definition id="19">
				<sentence>SemSpec , then , is an intermediate level of representation that reflects sentence semantics and that mediates between the language-neutral conceptual representation and linguistic realization .</sentence>
				<definiendum id="0">SemSpec</definiendum>
				<definiens id="0">is an intermediate level of representation that reflects sentence semantics and that mediates between the language-neutral conceptual representation and linguistic realization</definiens>
			</definition>
			<definition id="20">
				<sentence>When text planning has been completed and linearization as well as the `` chunking '' of the material into sentence-size pieces is accomplished , Moose takes over to perform the necessary sentence planning , which includes lexicalization .</sentence>
				<definiendum id="0">Moose</definiendum>
				<definiens id="0">takes over to perform the necessary sentence planning , which includes lexicalization</definiens>
			</definition>
			<definition id="21">
				<sentence>Learnability and Cognition : The Acquisition of Argument Structure .</sentence>
				<definiendum id="0">Learnability</definiendum>
				<definiendum id="1">Cognition</definiendum>
			</definition>
</paper>

		<paper id="4004">
			<definition id="0">
				<sentence>The relative frequency estimator provides a straightforward way of inducing these grammars from treebank corpora , and a broad-coverage parsing system can be obtained by using a parser to find a maximum-likelihood parse tree for the input string with respect to such a treebank gram_mar .</sentence>
				<definiendum id="0">relative frequency estimator</definiendum>
				<definiens id="0">provides a straightforward way of inducing these grammars from treebank corpora</definiens>
			</definition>
			<definition id="1">
				<sentence>The corpus used as the source for the empirical study is version II of the Wall Street Journal ( WSJ ) corpus constructed at the University of Pennsylvania , modified as described in Charniak ( 1996 ) , in that : • root nodes ( labeled ROOT ) were inserted , • the terminal or lexical items were deleted ( i.e. , the terminal items in the trees were POS tags ) , • node labels consisted solely of syntactic category information ( e.g. , grammatical function and coindexation information was removed ) , 614 Mark Johnson PCFG Models • the POS tag of auxiliary verbs was replaced with AUX , • empty nodes ( i.e. , nodes dominating the empty string ) were deleted , and • any resulting unary branching nodes dominating a single child with the same node label ( i.e. , which are expanded by a production X ~ X ) were deleted .</sentence>
				<definiendum id="0">AUX , • empty nodes</definiendum>
				<definiens id="0">the source for the empirical study is version II of the Wall Street Journal ( WSJ ) corpus constructed at the University of Pennsylvania , modified as described in Charniak ( 1996 ) , in that : • root nodes ( labeled ROOT ) were inserted , • the terminal or lexical items were deleted ( i.e. , the terminal items in the trees were POS tags ) , • node labels consisted solely of syntactic category information ( e.g. , grammatical function</definiens>
				<definiens id="1">nodes dominating the empty string ) were deleted</definiens>
				<definiens id="2">dominating a single child with the same node label ( i.e. , which are expanded by a production X ~ X ) were deleted</definiens>
			</definition>
			<definition id="2">
				<sentence>A PCFG defines a probability distribution over the ( finite ) parse trees generated by the grammar , where the probability of a tree T is given by PO- ) = H P ( A-+ c~ ) cT ( A~ ) A-*rxEP where Cr ( A ~ o~ ) is the number of times the production A ~ oL is used in the derivation T. The PCFG that assigns maximum likelihood to the sequence ~ of trees in a treebank corpus is given by the relative frequency estimator .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiendum id="1">Cr</definiendum>
				<definiens id="0">the number of times the production A ~ oL is used in the derivation T. The PCFG that assigns</definiens>
			</definition>
			<definition id="3">
				<sentence>G ( A ~ ~ ) = C~ ( A -~ ~ ) ~ , V~ ( NuT ) * C~ ( A ~ o/ ) Here C~ ( A ~ o~ ) is the number of times the production A -- ~ oz is used in derivations of the trees in ~ .</sentence>
				<definiendum id="0">G</definiendum>
				<definiendum id="1">C~</definiendum>
				<definiens id="0">the number of times the production A -- ~ oz is used in derivations of the trees in ~</definiens>
			</definition>
			<definition id="4">
				<sentence>616 Mark Johnson PCFG Models As noted above , the WSJ corpus represents PP modification to NPs using the twolevel representation .</sentence>
				<definiendum id="0">WSJ corpus</definiendum>
				<definiens id="0">represents PP modification to NPs using the twolevel representation</definiens>
			</definition>
			<definition id="5">
				<sentence>Thus the estimated likelihoods using P1 of the tree structures ( A1 ) and ( B1 ) are : PI ( A1 ) - ( 2 +f ) a 4 ( 1 -f ) PI ( B1 ) - ( 2 +f ) 2 Clearly Pl ( al ) &lt; f and \ ] 51 ( B1 ) &lt; ( 1 -f ) except at f= 0 andf = 1 , so in general the estimated frequencies using \ ] ~1 differ from the frequencies of ( A~ ) and ( B1 ) in the training corpus. This is not too surprising , as the PCFG Pl assigns nonzero probability to trees not in the training corpus ( e.g. , to trees with more than one PP ) . In any case , in the parsing applications mentioned earlier the absolute magnitude of the probability of a tree is not of direct interest ; rather we are concerned with its probability relative to the probabilities of other , alternative tree structures for the same yield. Thus it is arguably more reasonable to ignore the `` spurious '' tree structures 619 Computational Linguistics Volume 24 , Number 4 0 1 / / `` ' '' ! /= ... . , . ... ' '' ; / \ ] 3 ... ... .. / . : , 4 / , .2 • / / , : , / / • `` ' / / ..'. '' / / , ' '' / / d '' / I 7 : / I* . , ' 1 , ~ `` `` .. , '' I II ..f / • • * '' / • / / -/ . '' / , '' • , o '' / , /• ** , * j./ / , '1 / • _.~. ~ I ~ ~ I I I 0 0.2 0.4 0.6 0.8 I Figure 4 The estimated relative frequency f of NP attachment. This graph shows f as a function of the relative frequency f of NP attachment in the training data for various models discussed in the text. generated by Pl but not present in the training corpus , and compare the estimated relative frequencies of ( A1 ) and ( Ba ) under Pl to their frequencies in the training data. Ideally the estimated relative frequency fl of ( A1 ) = PI ( T = Al : r • { A1 , B1 } ) Pl ( A1 ) Pi ( A1 ) qPi ( B1 ) f2 2-f will be close to its actual frequencyf in the training corpus. The relationship between f and fl is plotted in Figure 4. As inspection of Figure 4 makes clear , the value of fl can diverge substantially fromf. For example , atf = 0.48 ( the estimate obtained from the WSJ corpus presented above ) fl = 0.15. Thus a PCFG language model induced from the simple two-tree corpus above can underestimate the relative frequency of NP attachment by a factor of more than 3. Now suppose that the corpus contains the following two trees ( A2 ) and ( B2 ) of Figure 5 , which are the Chomsky adjunction representations of NP attached and VP attached PP 's , respectively , with relative frequencies f and 1 -f as before. Note that unlike the Penn II representations , the Chomsky adjunction representation represents NP and VP modification by PPs symmetrically. 620 Mark Johnson PCFG Models ( A2 ) VP ( B2 ) VP V NP VP PP NP PP V NP P NP Det N P NP Det N Det N Det N Figure 5 The training corpus ~2. This two-tree corpus , which uses Chomsky adjuncfion tree representations , consists of the trees ( A2 ) with relative frequencyf and the trees ( B2 ) with relative frequency 1 -f. The PCFG P2 is estimated from this corpus. The counts C2 and the nonunit production probability estimates P2 for the PCFG induced from this two-tree corpus are as follows : R C2 ( R ) P2 ( R ) VP ~ V NP 1 1/ ( 2 -f ) VP ~ VP PP 1 ~f ( 1 -f ) / ( 2 -f ) NP ~ Det N 2/ ( 2 +f ) NP ~ NP PP f f/ ( 2 +f ) The estimated likelihoods using P2 of the tree structures ( A2 ) and ( B2 ) are : 4f P2 ( A2 ) = ( 4 -f2 ) ( 2 +f ) 2 P2 ( B2 ) = 4 ( 1 -f ) ( 4 _f2 ) 2 As in the previous subsection , P2 ( A2 ) Kf and P2 ( B2 ) &lt; ( 1 -f ) because the PCFG assigns nonzero probability to trees not in the training corpus. Again , we calculate the estimated relative frequencies of ( A2 ) and ( B2 ) under P2. = P2 ( T-~ A2 : T E { A2 , B2 } ) _ f-af 2f 2 -f-2 The relationship between/and f2 is also plotted in Figure 4. The value off2 can diverge from f , although not as widely as fl. For example , atf = 0.48f2 = 0.36. Thus the precise tree structure representations used to train a PCFG can have a marked effect on the probability distribution that it generates. The previous subsection showed that inserting additional nodes into the tree structure can result in a PCFG language model that better models the distribution of trees in the training corpus. This subsection investigates the effect of removing the lower NP node in the WSJ NP modification structure , again resulting in a pair of more symmetric tree 621 Computational Linguistics Volume 24 , Number 4 VP /N V NP Det N PP /N P NP Det N ( B3 ) VP V NP PP Det N P NP Det N Figure 6 The training corpus ~3. The NP modification tree representation used in the Penn II WSJ corpus is `` flattened '' to make it similar to the VP modification representation. The PCFG P3 is estimated from this corpus. structures , as shown in Figure 6. As explained in Section 1 , flattening the tree structures in general corresponds to weakening the independence assumptions in the induced PCFG models , so one might expect this to improve the induced language model. The counts C3 and the nonunit production probability estimates P3 for the PCFG induced from this two-tree corpus are as follows : a C3 ( R ) G ( R ) VP ~ V NP f f VP ~ VP NP PP 1 -- f 1 -f NP-*DetN 2d 1-d/2 NP ~ Det N PP f f/2 The estimated likelihoods using P3 of the tree structures ( A3 ) and ( B3 ) are : Y P3 ( A3 ) 2 ( 1 -f/2 ) P3 ( B3 ) = ( 1 -f ) ( 1 -f/2 ) 2 As before P3 ( A3 ) &lt; f and P3 ( B3 ) &lt; ( 1 -- f ) , again because the PCFG assigns nonzero probability to trees not in the training corpus. The estimated relative frequency f3 of ( a3 ) relative to ( B3 ) under P3 is : = P3 ( T ~A3 : T C { A3 , B3 } ) d 2 2 3f+2f 2 The relationship between f and f3 is also plotted in Figure 4. The value of f3 diverges from f , as before : atf = 0.48f3 = 0.23. As Figure 4 shows , the estimated relative frequency f3 using the flattened tree representations is always closer to f than the estimated relative frequency fl using the Penn II representations , but is only closer to f than the estimated relative frequency f2 using the Chomsky adjunction representations for f greater than approximately 0.7. 622 Mark Johnson PCFG Models ( A4 ) VP ^S V NP~VP NP^NP PP^NP Det N P NP^PP Det N ( B4 ) VP^S i_i-7 -- -- -._ ... V NP^VP PP^VP Det N P NP^PP Det N Figure 7 The training corpus ~4. This corpus , which uses Penn II treebank tree representations in which each preterminal node 's parent 's category is appended onto its own label , consists of the trees ( A4 ) with relative frequencyf and the trees ( B4 ) with relative frequency 1 -f. The PCFG P4 is estimated from this corpus. As mentioned in Section 1 , another way of relaxing the independence assumptions implicit in a PCFG model is to systematically encode more information in node labels about their context. This subsection explores a particularly simple kind of contextual encoding : the label of the parent of each nonroot nonpreterminal node is appended to that node 's label. The labels of the root node and the terminal and preterminal nodes are left unchanged. For example , assuming that the Penn II format trees ( A1 ) and ( B1 ) of Section 4.1 are immediately dominated by a node labeled S , this relabeling applied to those trees produces the trees ( A4 ) and ( B4 ) depicted in Figure 7. We can perform the same theoretical analysis on this two-tree corpus that we applied to the previous corpora to investigate the effect of this relabeling on the PCFG modeling of PP attachment structures. The counts C4 and the nonunit production probability estimates P4 for the PCFG induced from this two-tree corpus are as follows : R VP ^S -+ V NP ^ VP VP ^ S ~ V NP ^ VP PP ^ VP NP^VP ~ Det N NP ^ VP -- + NP ~ NP PP ^ NP C4 ( R ) P4 ( R ) 11 ( ; f 1-f ; 1-ff The estimated likelihoods using P4 of the tree structures ( A4 ) and ( B4 ) are : P4 ( A4 ) .~ f2 P4 ( B4 ) : ( l-f ) 2 As in the previous subsection P4 ( a4 ) &lt; f and P4 ( B4 ) &lt; ( 1 -- f ) . Again , we calculate the estimated relative frequencies of ( a4 ) and ( B4 ) under P4. = P4 ( 7 '' : A4 : 9e { A4 , B4 } ) f f2 + ( 1 -f ) 2 623 Computational Linguistics Volume 24 , Number 4 The relationship between f and f4 is plotted in Figure 4. The value of f4 can diverge from f , just like the other estimates. For example , atf = 0.48 f4 = 0.46 , which is closer to f than any of the other relative frequency estimates presented earlier. ( However , forf less than approximatel^y 0.38 , the relative fre^quency estimate using the Chomsky adjunction representations f2 is closer to f than f4 ) . Thus as expected , increasing the context information in the form of an enriched node-labeling scheme can improve the performance of a PCFG language model. The previous section presented theoretical evidence that varying the tree representations used to estimate a PCFG language model can have a noticeable impact on that model 's performance. However , as anyone working with statistical language models knows , the actual performance of a language model on real language data can often differ dramatically from one 's expectations , even when it has an apparently impeccable theoretical basis. For example , on the basis of the theoretical models presented in the last section ( and , undoubtedly , a background in theoretical linguistics ) I expected that PCFG models induced from Chomksy adjunction tree representations would perform better than models induced from the Penn II representations. However , as shown in this section , this is not the case , but some of the other tree representations investigated here induce PCFGs that do perform noticeably better than the Penn II representations. It is fairly straightforward to mechanically transform the Penn II tree representations in the WSJ corpus into something close to the alternative tree representations described above , although the diversity of local trees in the WSJ corpus makes this task more difficult. For example , what is the Chomsky adjunction representation of a VP with no apparent verbal head ? In addition , the Chomsky adjunction representation requires argument PPs to be attached as sisters of the lexical head , while adjunct PPs are attached as sisters of a nonlexical projection. Argument PPs are not systematically distinguished from adjunct PPs in the Penn II tree representations , and reliably determining whether a particular PP is an argument or an adjunct is extremely difficult , even for trained linguists. Nevertheless , the tree transformations investigated below should give at least an initial idea as to the influence of different kinds of tree representation on the induced PCFG language models. The tree transformations investigated in this section are listed below. Each is given a short name , which is used to identify it in the rest of the paper. Designing the tree transformations is complicated by the fact that there are in general many different tree transformations that correctly transform the simple cases discussed in Section 4 , but behave differently on more complex constructions that appear in the WSJ corpus. The actual transformations investigated here have the advantage of simplicity , but many other different transformations would correctly transform the trees discussed in Sections 3 and 4 and be just as linguistically plausible as the transforms below , yet would presumably induce PCFGs with very different properties. Id is an identity transformation , i.e. , it does not modify the trees at all. This condition studies the behavior of the Penn II tree representation used in the WSJ corpus. NP-VP produces trees that represent PP modification of both NPs and VPs using Chomsky adjunction. The NP-VP transform is the result of exhaustively 624 Mark Johnson PCFG Models NP NP NP ~ ~/x.N2 NP p~pp , NP a PP ~ ~ NP a PP ~ ~ A ~ A NP ~ A ~ A dx NP A , /-k A /-k VP VP vP p fl pp c~.J ~ ~ fl pp L~.J Figure 8 Producing Chomsky adjunction tree representations. The four tree transforms depicted here are exhaustively reapplied to produce the Chomsky adjunction tree representations from Penn II tree representations in the NP-VP transformation. In the N'-V ' transformation the boxed NP and VP nodes are relabeled with N ' and V ' respectively. In these schema a is a sequence of trees of length 1 or greater and fl is a sequence of trees of length 2 or greater. N ! .V ! Flatten applying all four of the tree transforms depicted in Figure 8. The first and fourth transforms turn NP and VP nodes whose rightmost child is a PP into Chomsky adjunction structures , and the second and third transforms adjoin final PPs with a following comma punctuation into Chomsky adjunction structures. The constraints that a &gt; 1 and fl &gt; 2 ensures that these transforms will only apply a finite number of time to any given subtree .</sentence>
				<definiendum id="0">PP</definiendum>
				<definiendum id="1">NP-VP transform</definiendum>
				<definiendum id="2">fl</definiendum>
				<definiens id="0">a function of the relative frequency f of NP attachment in the training data for various models discussed in the text. generated by Pl but not present in the training corpus , and compare the estimated relative frequencies of ( A1 ) and ( Ba ) under Pl to their frequencies in the training data. Ideally the estimated relative frequency fl of</definiens>
				<definiens id="1">the Chomsky adjunction representations of NP attached and VP attached PP 's , respectively , with relative frequencies f</definiens>
				<definiens id="2">a C3 ( R ) G ( R</definiens>
				<definiens id="3">uses Penn II treebank tree representations in which each preterminal node 's parent 's category is appended onto its own label , consists of the trees ( A4 ) with relative frequencyf and the trees</definiens>
				<definiens id="4">sisters of a nonlexical projection. Argument PPs are not systematically distinguished from adjunct PPs in the Penn II tree representations , and reliably determining whether a particular</definiens>
				<definiens id="5">an identity transformation</definiens>
				<definiens id="6">a PP into Chomsky adjunction structures</definiens>
			</definition>
			<definition id="6">
				<sentence>The labeled precision and recall figures are obtained by regarding the sequence of trees e produced by a parser as a multiset or bag E ( e ) of edges , i.e. , triples IN , 1 , r / where N is a nonterminal label and 1 and r are left and right string positions in yield of the entire corpus .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the sequence of trees e produced by a parser as a multiset or bag E ( e ) of edges</definiens>
				<definiens id="1">a nonterminal label and 1 and r are left and right string positions in yield of the entire corpus</definiens>
			</definition>
			<definition id="7">
				<sentence>IE ( e ) n E ( e ' ) l Precision ( e ) IE ( e ) l IE ( e ) n E ( e ' ) l Recall ( e ) IE ( e ' ) l Thus , precision is the fraction of edges in the tree sequence to be evaluated that also appear in the test tree sequence , and recall is the fraction of edges in the test tree sequence that also appear in tree sequence to be evaluated .</sentence>
				<definiendum id="0">IE ( e ) n E</definiendum>
				<definiendum id="1">precision</definiendum>
				<definiendum id="2">recall</definiendum>
				<definiens id="0">( e ' ) l Precision ( e ) IE ( e ) l IE ( e ) n E ( e ' ) l Recall ( e ) IE ( e '</definiens>
				<definiens id="1">the fraction of edges in the tree sequence to be evaluated that also appear in the test tree sequence</definiens>
				<definiens id="2">the fraction of edges in the test tree sequence that also appear in tree sequence to be evaluated</definiens>
			</definition>
			<definition id="8">
				<sentence>Randomization tests for paired sample data were performed to assess the significance of the difference between the labeled precision and recall scores for the output of the Id PCFG and the other PCFGs ( Cohen 1995 ) .</sentence>
				<definiendum id="0">Randomization</definiendum>
				<definiendum id="1">other PCFGs</definiendum>
				<definiens id="0">tests for paired sample data were performed to assess the significance of the difference between the labeled precision and recall scores for the output of the Id PCFG and the</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>Ide and V4ronis Introduction In general terms , word sense disambiguation involves the association of a given word in a text or discourse with a definition or meaning ( sense ) which is distinguishable from other meanings potentially attributable to that word .</sentence>
				<definiendum id="0">word sense disambiguation</definiendum>
				<definiens id="0">involves the association of a given word in a text or discourse with a definition or meaning ( sense</definiens>
			</definition>
			<definition id="1">
				<sentence>The complexity of the context , and in particular the role of syntactic relations , was also recognized ; for example , Reifler ( 1955 ) says : Grammatical structure can also help disambiguate , as , for instance , the word keep , which can be disambiguated by determining whether its object is gerund ( He kept eating ) , adjectival phrase ( He kept calm ) , or noun phrase ( He kept a record ) .</sentence>
				<definiendum id="0">adjectival phrase</definiendum>
			</definition>
			<definition id="2">
				<sentence>The network consists of nodes representing noun senses and links represented by verb senses ; case frames impose IS-A and PART-OF relations on the network .</sentence>
				<definiendum id="0">network</definiendum>
				<definiens id="0">consists of nodes representing noun senses and links represented by verb senses ; case frames impose IS-A and PART-OF relations on the network</definiens>
			</definition>
			<definition id="3">
				<sentence>Wilks ' preference semantics ( \ [ 1968 , 1969 , 1973 , 1975a , 1975b , 1975c , 1975d\ ] ; see the survey by Wilks and Fass \ [ 1990\ ] ) , which uses Masterman 's primitives , is essentially a case-based approach to natural language understanding and one of the first specifically designed to deal with the problem of sense disambiguation .</sentence>
				<definiendum id="0">] )</definiendum>
				<definiens id="0">uses Masterman 's primitives , is essentially a case-based approach to natural language understanding and one of the first specifically designed to deal with the problem of sense disambiguation</definiens>
			</definition>
			<definition id="4">
				<sentence>Preference semantics specifies selectional restrictions for combinations of lexical items in a sentence that can be relaxed when a word with the preferred restrictions does not appear , thus enabling , especially , the handling of metaphor ( as in My car drinks gasoline , where the restrictions on drink prefer an animate subject but allow an inanimate one ) .</sentence>
				<definiendum id="0">Preference semantics</definiendum>
				<definiens id="0">specifies selectional restrictions for combinations of lexical items in a sentence that can be relaxed when a word with the preferred restrictions does not appear</definiens>
			</definition>
			<definition id="5">
				<sentence>Roget 's International Thesaurus , which was put into machine-tractable form in the 1950s and has been used in a variety of applications including machine translation ( Masterman 1957 ) , information retrieval ( Sparck-Jones 1964 , 1986 ) , and content analysis ( Sedelow and Sedelow \ [ 1969\ ] , see also Sedelow and Sedelow \ [ 1986 , 1992\ ] ) , also supplies an explicit concept hierarchy consisting of up to eight increasingly refined levels/Typically , each occurrence of the same word under different categories of the thesaurus represents different senses of that word ; i.e. , the categories correspond roughly to word senses ( Yarowsky 1992 ) .</sentence>
				<definiendum id="0">information retrieval</definiendum>
			</definition>
			<definition id="6">
				<sentence>Among enumerative lexicons , WordNet ( Miller et al. 1990 ; Fellbaum , forthcoming-a , forthcoming-b ) is at present the best-known and the most utilized resource for word sense disambiguation in English .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">at present the best-known and the most utilized resource for word sense disambiguation in English</definiens>
			</definition>
			<definition id="7">
				<sentence>A hood for a given word w is defined as the largest connected subgraph that contains w. For each content and the Longman Lexicon of Contemporary English ( LLOCE ) ( Chen and Chang , this volume ) .</sentence>
				<definiendum id="0">hood for a given word w</definiendum>
				<definiens id="0">the largest connected subgraph that contains w. For each content and the Longman Lexicon of Contemporary English ( LLOCE ) ( Chen and Chang , this volume )</definiens>
			</definition>
			<definition id="8">
				<sentence>12 Ide and Vdronis Introduction word in a document collection , Voorhees computes the number of times each synset appears above that word in the WordNet noun hierarchy , which gives a measure of the expected activity ( global counts ) ; she then performs the same computation for words occurring in a particular document or query ( local counts ) .</sentence>
				<definiendum id="0">Voorhees</definiendum>
				<definiens id="0">gives a measure of the expected activity ( global counts</definiens>
			</definition>
			<definition id="9">
				<sentence>The most frequently cited problem is the fine-grainedness of WordNet 's sense distinctions , which are often well beyond what may be needed in many language-processing applications ( see Section 3.2 ) .</sentence>
				<definiendum id="0">most frequently cited problem</definiendum>
			</definition>
			<definition id="10">
				<sentence>Hearst ( 1991 ) proposed an algorithm ( CatchWord ) that includes a training phase during which each occurrence of a set of nouns to be disambiguated is manually sense-tagged in several occurrences .</sentence>
				<definiendum id="0">CatchWord</definiendum>
				<definiens id="0">includes a training phase during which each occurrence of a set of nouns to be disambiguated is manually sense-tagged in several occurrences</definiens>
			</definition>
			<definition id="11">
				<sentence>The best-known smoothing methods are that of Turing-Good ( Good 1953 ) , which hypothesizes a binomial distribution of events , and that of Jelinek and Mercer ( 1985 ) , which combines estimated parameters on distinct subparts of the training corpus .</sentence>
				<definiendum id="0">best-known smoothing methods</definiendum>
				<definiens id="0">hypothesizes a binomial distribution of events</definiens>
				<definiens id="1">combines estimated parameters on distinct subparts of the training corpus</definiens>
			</definition>
			<definition id="12">
				<sentence>Context is the only means to identify the meaning of a polysemous word .</sentence>
				<definiendum id="0">Context</definiendum>
				<definiens id="0">the only means to identify the meaning of a polysemous word</definiens>
			</definition>
			<definition id="13">
				<sentence>Based on this definition , a significant collocation can be defined as a syntagmatic association among lexical items , where the probability of item x co-occurring with items a , b , c..</sentence>
				<definiendum id="0">significant collocation</definiendum>
				<definiens id="0">a syntagmatic association among lexical items , where the probability of item x co-occurring with items a , b</definiens>
			</definition>
			<definition id="14">
				<sentence>distinctions provided by established lexical resources , such as machine-readable dictionaries or WordNet ( which uses the OALD 's senses ) , because they are widely available .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">uses the OALD 's senses</definiens>
			</definition>
			<definition id="15">
				<sentence>Word sense ambiguation : Clustering related senses .</sentence>
				<definiendum id="0">Word sense ambiguation</definiendum>
				<definiens id="0">Clustering related senses</definiens>
			</definition>
			<definition id="16">
				<sentence>WordNet : An Electronic Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="17">
				<sentence>SELECT : A computer program to identify associationally rich words for content analysis .</sentence>
				<definiendum id="0">SELECT</definiendum>
				<definiens id="0">A computer program to identify associationally rich words for content analysis</definiens>
			</definition>
			<definition id="18">
				<sentence>SELECT : A computer program to identify associationally rich words for content analysis .</sentence>
				<definiendum id="0">SELECT</definiendum>
				<definiens id="0">A computer program to identify associationally rich words for content analysis</definiens>
			</definition>
			<definition id="19">
				<sentence>WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">An on-line lexical database</definiens>
			</definition>
			<definition id="20">
				<sentence>Word concepts : A theory and simulation of some basic semantic capabilities .</sentence>
				<definiendum id="0">Word concepts</definiendum>
				<definiens id="0">A theory and simulation of some basic semantic capabilities</definiens>
			</definition>
			<definition id="21">
				<sentence>WordNet and distributional analysis : A class-based approach to statistical discovery .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiendum id="1">distributional analysis</definiendum>
				<definiens id="0">A class-based approach to statistical discovery</definiens>
			</definition>
			<definition id="22">
				<sentence>Selection and Information : A Class-based Approach to Lexical Relationships .</sentence>
				<definiendum id="0">Selection</definiendum>
				<definiendum id="1">Information</definiendum>
			</definition>
			<definition id="23">
				<sentence>Detecting Subject Boundaries Within Text : A Language Independent Statistical Approach .</sentence>
				<definiendum id="0">Within Text</definiendum>
			</definition>
			<definition id="24">
				<sentence>TINA , A natural language system for spoken language applications .</sentence>
				<definiendum id="0">TINA</definiendum>
			</definition>
			<definition id="25">
				<sentence>The General Inquirer : A Computer Approach to Content Analysis .</sentence>
				<definiendum id="0">General Inquirer</definiendum>
				<definiens id="0">A Computer Approach to Content Analysis</definiens>
			</definition>
			<definition id="26">
				<sentence>Preference semantics : A family history .</sentence>
				<definiendum id="0">Preference semantics</definiendum>
				<definiens id="0">A family history</definiens>
			</definition>
			<definition id="27">
				<sentence>Decision lists for lexical ambiguity resolution : Application to accent restoration in Spanish and French .</sentence>
				<definiendum id="0">Decision lists</definiendum>
			</definition>
			<definition id="28">
				<sentence>The Psycho-biology of Language : An Introduction to Dynamic Biology .</sentence>
				<definiendum id="0">Psycho-biology of Language</definiendum>
				<definiens id="0">An Introduction to Dynamic Biology</definiens>
			</definition>
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>For each A E V , let F ( A ; w ) be the number of instances of A in w and let F ( A ; w ) be the number of nonroot instances of A in w. Given oz E ( V U T ) * , let nA ( cZ ) be the number of instances of A in the string o~ , and , finally , let ai be the ith component of the string o~ .</sentence>
				<definiendum id="0">E V</definiendum>
				<definiendum id="1">F</definiendum>
			</definition>
			<definition id="1">
				<sentence>`` E~ nn ( ol ) f ( A `` -- -~ Ol ; Wi ) } Z ( A~cQER = qB n BEV Zi=I F ( A ; ¢0i ) Z qB ~ Z nB ( OOf ( m ~ OGCdi ) BEV i=1 a s.t. Be~ ( A~o , ) ER 303 Computational Linguistics Volume 24 , Number 2 Sum over A E V : E qA E F ( A ; wi ) _ &lt; E qB ~ E E nB ( ol ) f ( A ~ c~ ; wi ) AEV i~-1 BEV i=1 AEV c~ s.t. B~a ( A~c~ ) ER n = ~ qB ~l : ( B ; wi ) BEV i=1 i.e./ H qA E ( Ie ( A ; wi ) F ( A ; wi ) ) ~ 0 AEV i=1 Clearly , for every i = 1,2 , ... , n F ( A ; wi ) = F ( A ; wi ) whenever A # S and F ( S ; wi ) &lt; F ( S ; wi ) .</sentence>
				<definiendum id="0">E~ nn</definiendum>
				<definiendum id="1">wi ) F ( A ; wi</definiendum>
				<definiendum id="2">... , n F</definiendum>
				<definiens id="0">A E V : E qA E F ( A ; wi ) _ &lt; E qB ~ E E nB ( ol ) f ( A ~ c~ ; wi ) AEV i~-1 BEV i=1 AEV c~ s.t. B~a ( A~c~ ) ER n = ~ qB ~l : ( B ; wi ) BEV i=1 i.e./ H qA E ( Ie ( A ;</definiens>
				<definiens id="1">A ; wi ) = F ( A ; wi ) whenever A # S and F ( S ; wi ) &lt; F ( S ; wi )</definiens>
			</definition>
			<definition id="2">
				<sentence>AEV i=1 ( 6 ) In the absence of unit productions and null productions , F ( A ; w ) &lt; 21w \ [ ( twice the length of the string w ) .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">twice the length of the string w )</definiens>
			</definition>
</paper>

		<paper id="3006">
			<definition id="0">
				<sentence>502 Oberlander Do the Right Thing in hand ; people use a simple algorithm , which does n't waste excessive resources on computing possible misinterpretations : The principle that has emerged from our study of the referring expression generation task is that a simple and nonliteral interpretation of the Gricean maxims is to be preferred ... Perhaps it may some day be possible to make a very general statement such as `` human speakers in general use very simple ( in computational terms ) interpretations of the maxims of conversational implicature , and hence computer natural language generation systems should also use such interpretations . ''</sentence>
				<definiendum id="0">simple algorithm</definiendum>
				<definiendum id="1">Gricean maxims</definiendum>
				<definiens id="0">a simple and nonliteral interpretation of the</definiens>
			</definition>
			<definition id="1">
				<sentence>Continuation studies examine the kinds of ( written ) utterances that people prefer to generate in given , carefully controlled contexts .</sentence>
				<definiendum id="0">Continuation studies</definiendum>
				<definiens id="0">the kinds of ( written ) utterances that people prefer to generate in given</definiens>
			</definition>
			<definition id="2">
				<sentence>In all the examples like ( 3 ) , Bill is the person a generator is expected to talk about -- and for whom a pronoun would thus make good sense-but this thematically based expectation must be played off against the centering-based expectation ; there are multiple factors underlying any expectation .</sentence>
				<definiendum id="0">Bill</definiendum>
				<definiens id="0">the person a generator is expected to talk about</definiens>
			</definition>
</paper>

		<paper id="4002">
			<definition id="0">
				<sentence>Let the input be { no1 reel , nc2 mc2 , nc3 rnc3 , v } , where nci denotes the case filler for the case ci , and mci denotes the case marker for ci , and assume that the interpretation candidates for v are derived from the database as sl , s2 and s3 .</sentence>
				<definiendum id="0">nci</definiendum>
			</definition>
			<definition id="1">
				<sentence>577 Computational Linguistics Volume 24 , Number 4 Table 1 The relation between the length of the path between two nouns nl and n2 in the Bunruigoihyo thesaurus ( len ( nl , n2 ) ) , and their relative similarity ( sire ( n1 , n2 ) ) .</sentence>
				<definiendum id="0">relative similarity</definiendum>
				<definiens id="0">relation between the length of the path between two nouns nl and n2 in the Bunruigoihyo thesaurus ( len ( nl</definiens>
			</definition>
			<definition id="2">
				<sentence>Formally , this is expressed by Equation ( 1 ) , where Score ( s ) is the score of sense s of the input verb , and SIM ( nc , G , c ) is the maximum similarity degree between the input case filler nc and the corresponding case fillers in the database example set ~s , c ( calculated through Equation ( 2 ) ) .</sentence>
				<definiendum id="0">c )</definiendum>
				<definiendum id="1">c</definiendum>
				<definiens id="0">the score of sense s of the input verb</definiens>
				<definiens id="1">the maximum similarity degree between the input case filler nc and the corresponding case fillers in the database example set ~s</definiens>
			</definition>
			<definition id="3">
				<sentence>CCD ( c ) is the weight factor of case c , which we will explain later in this section .</sentence>
				<definiendum id="0">CCD ( c )</definiendum>
				<definiens id="0">the weight factor of case c</definiens>
			</definition>
			<definition id="4">
				<sentence>TF ( term frequency ) gives each context ( a case marker/verb pair ) importance proportional to the number of times it occurs with a given noun .</sentence>
				<definiendum id="0">TF ( term frequency )</definiendum>
				<definiens id="0">gives each context ( a case marker/verb pair ) importance proportional to the number of times it occurs with a given noun</definiens>
			</definition>
			<definition id="5">
				<sentence>This notion is expressed by Equation ( 4 ) , wheref ( ( n , c , v ) ) is the frequency of the tuple ( n , c , v ) , nf ( ( c , v ) ) is the number of noun types which collocate with verb v in the case c , and N is the number of noun types within the overall co-occurrence data .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">n , c , v ) ) is the frequency of the tuple ( n , c , v ) , nf ( ( c , v</definiens>
				<definiens id="1">the number of noun types which collocate with verb v in the case c</definiens>
				<definiens id="2">the number of noun types within the overall co-occurrence data</definiens>
			</definition>
			<definition id="6">
				<sentence>579 Computational Linguistics Volume 24 , Number 4 In Equation ( 1 ) , CCD ( c ) expresses the weight factor of the contribution of case c to ( current ) verb sense disambiguation .</sentence>
				<definiendum id="0">CCD ( c )</definiendum>
				<definiens id="0">expresses the weight factor of the contribution of case c to ( current ) verb sense disambiguation</definiens>
			</definition>
			<definition id="7">
				<sentence>C ' 1 r~-I __ ~ CCD ( c ) = ~,7~ ~ ~ Igs '' ¢l + \ ] £s , ,cl 2\ [ £s~ , c r '' l £s , ,~l j=i+l I &amp; , ,I 7 I'G*I ) ( 6 ) Here , o~ is a constant for pararneterizing the extent to which CCD influences verb sense disambiguation .</sentence>
				<definiendum id="0">o~</definiendum>
				<definiens id="0">a constant for pararneterizing the extent to which CCD influences verb sense disambiguation</definiens>
			</definition>
			<definition id="8">
				<sentence>580 Fujii , Inui , Tokunaga , and Tanaka Selective Sampling Here , A ( s , c , r ) is the association degree between verb sense s and class r ( selectional restriction candidate ) with respect to case c. P ( rls , c ) is the conditional probability that a case filler example associated with case c of sense s is dominated by class r in the thesaurus .</sentence>
				<definiendum id="0">c )</definiendum>
			</definition>
			<definition id="9">
				<sentence>P ( xls ) argmaxP ( s ) II P ( ncls ) c ( 8 ) Here , P ( ncls ) is the probability that a case filler associated with sense s for case c in the training data is nc .</sentence>
				<definiendum id="0">P ( xls</definiendum>
				<definiendum id="1">P ( ncls )</definiendum>
				<definiens id="0">the probability that a case filler associated with sense s for case c in the training data is nc</definiens>
			</definition>
			<definition id="10">
				<sentence>We estimated P ( s ) based on the distribution of the verb senses in the training data .</sentence>
				<definiendum id="0">P (</definiendum>
				<definiens id="0">s ) based on the distribution of the verb senses in the training data</definiens>
			</definition>
			<definition id="11">
				<sentence>581 Computational Linguistics Volume 24 , Number 4 Table 2 The verbs contained in the corpus used , and the accuracy of the different verb sense disambiguation methods ( LB : lower bound , RB : rule-based method , NB : Naive-Bayes method , VSM : vector space model , BGH : the Bunruigoihyo thesaurus ) .</sentence>
				<definiendum id="0">RB</definiendum>
				<definiendum id="1">NB</definiendum>
				<definiendum id="2">BGH</definiendum>
				<definiens id="0">the Bunruigoihyo thesaurus )</definiens>
			</definition>
			<definition id="12">
				<sentence>Third , a number of existing NLP tools such as JUMAN ( a morphological analyzer ) ( Matsumoto et al. 1993 ) and QJP ( a morphological and syntactic analyzer ) ( Kameda 1996 ) could broaden the coverage of our system , as inputs are currently limited to simple , morphologically analyzed sentences .</sentence>
				<definiendum id="0">JUMAN</definiendum>
				<definiendum id="1">QJP</definiendum>
				<definiens id="0">a morphological analyzer )</definiens>
				<definiens id="1">inputs are currently limited to simple , morphologically analyzed sentences</definiens>
			</definition>
			<definition id="13">
				<sentence>Based on the above two conditions , we compute interpretation certainties using Equation ( 10 ) , where C ( x ) is the interpretation certainty of an example x , Scorel ( x ) and Score2 ( x ) are the highest and second highest scores for x , respectively , and , ~ , which ranges from 0 to 1 , is a parametric constant used to control the degree to which each condition affects the computation of C ( x ) .</sentence>
				<definiendum id="0">C ( x )</definiendum>
				<definiens id="0">the interpretation certainty of an example x</definiens>
				<definiens id="1">the highest and second highest scores for x , respectively , and , ~ , which ranges from 0 to 1 , is a parametric constant used to control the degree to which each condition affects the computation of C ( x )</definiens>
			</definition>
			<definition id="14">
				<sentence>Note that in this experiment , accuracy is the ratio of the number of correct outputs and the number of cases where the interpretation certainty of the output is above a certain threshold .</sentence>
				<definiendum id="0">accuracy</definiendum>
				<definiens id="0">the ratio of the number of correct outputs and the number of cases where the interpretation certainty of the output</definiens>
			</definition>
			<definition id="15">
				<sentence>Coverage is the ratio of the number of cases where the interpretation certainty of the output is above a certain threshold and the number of inputs .</sentence>
				<definiendum id="0">Coverage</definiendum>
				<definiens id="0">the ratio of the number of cases where the interpretation certainty of the output is above a certain threshold and the number of inputs</definiens>
			</definition>
			<definition id="16">
				<sentence>The training utility of an example a is greater than that of another example b when the total interpretation certainty of unsupervised examples increases more after training with example a than with example b. Let us consider Figure 9 , in which the x-axis mono-dimensionally denotes the semantic similarity between two unsupervised examples , and the y-axis denotes the interpretation certainty of each example .</sentence>
				<definiendum id="0">training utility of an</definiendum>
				<definiendum id="1">x-axis mono-dimensionally</definiendum>
				<definiendum id="2">y-axis</definiendum>
				<definiens id="0">the semantic similarity between two unsupervised examples</definiens>
			</definition>
			<definition id="17">
				<sentence>The training utility of a is greater as it has more neighbors than b. On the other hand , in Figure 9 ( b ) , b has more neighbors than a. However , since b is semantically similar to e , which is already contained in the database , the total increase in interpretation certainty of its neighbors , i.e. the training utility of b , is smaller than that of a. Let AC ( x = s , y ) be the difference in the interpretation certainty of y c X after training with x c X , taken with the sense s. TU ( x = s ) , which is the training utility function for x taken with sense s , can be computed by Equation ( 11 ) .</sentence>
				<definiendum id="0">y</definiendum>
				<definiens id="0">the difference in the interpretation certainty of y c X after training with x c X</definiens>
			</definition>
			<definition id="18">
				<sentence>We estimate TU ( x ) , by the expected value of x , calculating the average of each TU ( x = s ) , weighted by the probability that x takes sense s. This can be realized by Equation ( 12 ) , where P ( slx ) is the probability that x takes the sense s. TU ( x ) = E P ( slx ) `` TU ( x = s ) ( 12 ) s Given the fact that ( a ) P ( sIx ) is difficult to estimate in the current formulation , and ( b ) the cost of computation for each TU ( x = s ) is not trivial , we temporarily approximate 587 Computational Linguistics Volume 24 , Number 4 TU ( x ) as in Equation ( 13 ) , where K is a set of the k-best verb sense ( s ) of x with respect to the interpretation score in the current state .</sentence>
				<definiendum id="0">TU</definiendum>
				<definiendum id="1">P ( slx )</definiendum>
				<definiendum id="2">TU</definiendum>
				<definiendum id="3">TU ( x = s</definiendum>
				<definiendum id="4">K</definiendum>
				<definiens id="0">( x ) , by the expected value of x , calculating the average of each TU ( x = s ) , weighted by the probability that x takes sense</definiens>
				<definiens id="1">difficult to estimate in the current formulation</definiens>
				<definiens id="2">a set of the k-best verb sense ( s ) of x with respect to the interpretation score in the current state</definiens>
			</definition>
			<definition id="19">
				<sentence>~n order to investigate the effectiveness of our example sampling method , we conducted an experiment in which we compared the following four sampling methods : • a control ( random ) , in which a certain proportion of a given corpus is randomly selected for training , • uncertainty sampling ( US ) , in which examples with minimum interpretation certainty are selected ( Lewis and Gale 1994 ) , • committee-based sampling ( CBS ) ( Engelson and Dagan 1996 ) , • our method based on the notion of training utility ( TU ) .</sentence>
				<definiendum id="0">uncertainty sampling</definiendum>
				<definiens id="0">in which examples with minimum interpretation certainty are selected ( Lewis and Gale 1994 ) , • committee-based sampling ( CBS )</definiens>
			</definition>
</paper>

		<paper id="4003">
			<definition id="0">
				<sentence>Katakana writing is a syllabary rather than an alphabet -- there is one symbol for ga ( ~ '' ) , another for gi ( 4 ~ '' ) , another for gu ( Y '' ) , etc .</sentence>
				<definiendum id="0">Katakana writing</definiendum>
				<definiens id="0">a syllabary rather than an alphabet -- there is one symbol for ga ( ~ ''</definiens>
			</definition>
			<definition id="1">
				<sentence>Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora ( a.k.a. `` notfound words '' ) , but very little computational work has been done in this area .</sentence>
				<definiendum id="0">Katakana phrases</definiendum>
				<definiens id="0">the largest source of text phrases that do not appear in bilingual dictionaries or training corpora ( a.k.a. `` notfound words '' ) , but very little computational work has been done in this area</definiens>
			</definition>
			<definition id="2">
				<sentence>A WFSA is a state/transition diagram with weights and symbols on the transitions , making some output sequences more likely than others .</sentence>
				<definiendum id="0">WFSA</definiendum>
				<definiens id="0">a state/transition diagram with weights and symbols on the transitions</definiens>
			</definition>
			<definition id="3">
				<sentence>Perhaps uncharitably , we can view optical character recognition ( OCR ) as a device that garbles perfectly good katakana sequences .</sentence>
				<definiendum id="0">OCR</definiendum>
				<definiens id="0">a device that garbles perfectly good katakana sequences</definiens>
			</definition>
			<definition id="4">
				<sentence>Next comes the P ( jlk ) model , which produces a 28-state/31-arc WFSA whose highest-scoring sequence is : masutaazut oochiment o Next comes P ( elj ) , yielding a 62-state/241-arc WFSA whose best sequence is : M AE S T AE AE DH UH T A0 A0 CH IH M EH N T A0 Next to last comes P ( wle ) , which results in a 2982-state/4601-arc WFSA whose best sequence ( out of roughly three hundred million ) is : masters tone am ent awe This English string is closest phonetically to the Japanese , but we are willing to trade phonetic proximity for more sensical English ; we rescore this WFSA by composing it with P ( w ) and extract the best translation : masters tournament Other Section I examples ( aasudee and robaato shyoon renaado ) are translated correctly as earth day and robert sean leonard .</sentence>
				<definiendum id="0">P ( jlk ) model</definiendum>
				<definiens id="0">produces a 28-state/31-arc WFSA whose highest-scoring sequence is : masutaazut oochiment o Next comes P ( elj ) , yielding a 62-state/241-arc WFSA whose best sequence is : M AE S T AE AE DH UH T A0 A0 CH IH M EH N T A0 Next to last comes P ( wle ) , which results in a 2982-state/4601-arc WFSA whose best sequence ( out of roughly three hundred million ) is : masters tone am ent awe This English string is closest phonetically to the</definiens>
				<definiens id="1">masters tournament Other Section I examples ( aasudee and robaato shyoon renaado</definiens>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>The evaluation we are considering typically takes the form of experiments in which human subjects are asked to annotate texts from a corpus ( or recordings of spoken conversations ) according to a given classification scheme , and the agreement among their annotations is measured ( see , for example , Passonneau and Litman 1993 or the papers in Moore and Walker 1997 ) .</sentence>
				<definiendum id="0">annotations</definiendum>
			</definition>
			<definition id="1">
				<sentence>For the experiments , we used a set of randomly selected articles from the Wall Street Journal contained in the ACL/DCI CD-ROM , rather than a corpus of transcripts of spoken language corpora such as the HCRC MapTask corpus ( Anderson et al. 1991 ) or the TRAINS corpus ( Heeman and Allen 1995 ) .</sentence>
				<definiendum id="0">HCRC MapTask corpus</definiendum>
				<definiendum id="1">TRAINS corpus</definiendum>
				<definiens id="0">a set of randomly selected articles from the Wall Street Journal contained in the ACL/DCI CD-ROM</definiens>
			</definition>
			<definition id="2">
				<sentence>Finally , Prince proposes a category for noun phrases that are like inferrables , but whose connection with previous hearer 's knowledge is specified as part of the noun phrase itself -- her example is the door of the Bastille in the following example : ( 15 ) The door of the Bastille was painted purple .</sentence>
				<definiendum id="0">Prince</definiendum>
				<definiens id="0">proposes a category for noun phrases that are like inferrables , but whose connection with previous hearer 's knowledge is specified as part of the noun phrase itself -- her example is the door of the Bastille in the following example : ( 15 ) The door of the Bastille was painted purple</definiens>
			</definition>
			<definition id="3">
				<sentence>Secondly , none of the classification schemes just discussed , nor any of the alternatives proposed in the literature , consider so-called generic uses of definite descriptions , such as the use of the tiger in the generic sentence The tiger is a fierce animal that lives in the jungle .</sentence>
				<definiendum id="0">tiger</definiendum>
				<definiens id="0">a fierce animal that lives in the jungle</definiens>
			</definition>
			<definition id="4">
				<sentence>Fraurud simplified matters in this way because she was primarily interested in verifying the empirical basis for the claim that familiarity is the defining property of definite descriptions ; she also observed , however , that some of the distinctions introduced by Hawkins and Prince led to ambiguities of classification .</sentence>
				<definiendum id="0">familiarity</definiendum>
				<definiens id="0">the defining property of definite descriptions ; she also observed , however , that some of the distinctions introduced by Hawkins and Prince led to ambiguities of classification</definiens>
			</definition>
			<definition id="5">
				<sentence>Idiom 0 0 1 1 0 2 V. Doubt 3 0 4 0 0 7 Total A 294 160 546 39 1 1,040 In order to measure the agreement in a more precise way , we used the Kappa statistic ( Siegel and Castellan 1988 ) , recently proposed by Carletta as a measure of agreement for discourse analysis ( Carletta 1996 ) .</sentence>
				<definiendum id="0">Kappa statistic</definiendum>
				<definiens id="0">a measure of agreement for discourse analysis ( Carletta 1996 )</definiens>
			</definition>
			<definition id="6">
				<sentence>The kappa coefficient of agreement between c annotators is defined as : K P ( A ) P ( E ) 1 P ( E ) where P ( A ) is the proportion of times the annotators agree and P ( E ) is the proportion 195 Computational Linguistics Volume 24 , Number 2 Table 5 An example of the Kappa test .</sentence>
				<definiendum id="0">kappa coefficient of agreement between c annotators</definiendum>
				<definiens id="0">K P ( A ) P ( E ) 1 P ( E ) where P ( A ) is the proportion of times the annotators agree and P ( E ) is the proportion</definiens>
			</definition>
			<definition id="7">
				<sentence>The numbers in each nq entry of the matrix indicate the number of classifiers that assigned the description in row i to the class in column j. The final colunm ( labeled S ) represents the percentage agreement for each definite description ; we explain below how this percentage agreement is calculated .</sentence>
				<definiendum id="0">S )</definiendum>
				<definiens id="0">the percentage agreement for each definite description</definiens>
			</definition>
			<definition id="8">
				<sentence>Massimo Poesio holds an Advanced Research Fellowship from EPSRC , UK ; Renata Vieira is supported by a fellowship from CNPq , Brazil .</sentence>
				<definiendum id="0">Massimo Poesio</definiendum>
				<definiens id="0">holds an Advanced Research Fellowship from EPSRC</definiens>
			</definition>
</paper>

		<paper id="3002">
			<definition id="0">
				<sentence>Allen ( 1991 ) proposed a discourse model that differentiates among the shared and individual beliefs that agents might hold during collaboration .</sentence>
				<definiendum id="0">discourse model</definiendum>
				<definiens id="0">differentiates among the shared and individual beliefs that agents might hold during collaboration</definiens>
			</definition>
			<definition id="1">
				<sentence>The enhanced dialogue model ( Chu-Carroll and Carberry 1994 ) has four levels : the domain level , which consists of the domain plan being constructed to achieve the agents ' shared domain goal ( s ) ; the problem-solving level which contains the actions being performed to construct the domain plan ; the belief level , which consists of the mutual beliefs pursued to further the problem-solving intentions ; and the discourse level which contains the communicative actions initiated to achieve the mutual beliefs .</sentence>
				<definiendum id="0">domain level</definiendum>
				<definiendum id="1">problem-solving level</definiendum>
				<definiendum id="2">belief level</definiendum>
				<definiens id="0">consists of the mutual beliefs pursued to further the problem-solving intentions ; and the discourse level which contains the communicative actions initiated to achieve the mutual beliefs</definiens>
			</definition>
			<definition id="2">
				<sentence>n fo-Sharing ) reject I • r select an information-sharing strategy -Sec 5.2.2 Output : discourse acts initiating information-sharing Identify subset of beliefs to be addressed in conflict resolution -Sec 6.1 ( Select-Focus-Modification ) Select evidence for claims -Sec 6.2 ( Select-Justification ) Output : discourse acts presenting proposed modifications along with justification Figure 1 Schematic diagram of Propose-Evaluate-Modify process. '</sentence>
				<definiendum id="0">n fo-Sharing</definiendum>
			</definition>
			<definition id="3">
				<sentence>The beliefs captured by the nodes in the tree may be of three forms : 1 ) MB ( _agentl , _agent2 , _prop ) , representing that _agent1 and _agent2 come to mutually believe _prop , 2 ) MknowrefCagentl , _agent2 , _var , _prop ) , meaning that _agent1 and _agent2 come to mutually know the referent of _var which will satisfy _prop , where _var is a variable in _prop , and 3 ) Mknowifl_agentl , _agent2 , _prop ) , representing that _agent1 and _agent2 come to mutually know whether or not _prop is true .</sentence>
				<definiendum id="0">_var</definiendum>
				<definiens id="0">a variable in _prop , and 3</definiens>
			</definition>
			<definition id="4">
				<sentence>The belief level of a dialogue model consists of one or more belief trees .</sentence>
				<definiendum id="0">belief level of a dialogue model</definiendum>
			</definition>
			<definition id="5">
				<sentence>This is because the top-level proposed belief is the main belief that EA ( the executing agent ) is attempting to establish between the agents , while its descendents are only intended to provide support for establishing that belief ( Young , Moore , and Pollack 1994 ) .</sentence>
				<definiendum id="0">EA</definiendum>
			</definition>
			<definition id="6">
				<sentence>In calculating whether to accept a belief , Evaluate-Belief invokes DetermineAcceptance , which performs the following functi6ns ( Chu-Carroll 1996 ) : 1 ) it utilizes a simplified version of Galliers ' belief revision mechanism ( Galliers 1992 ; Logal et al. 1994 ) to determine the system 's strength of belief in _bel ( or its negation ) given a set of evidence , by comparing the strengths of the pieces of evidence supporting and attacking _bel , ~1 and 2 ) it determines whether to accept , reject , or remain uncertain about the acceptance of _bel based on the resulting strength .</sentence>
				<definiendum id="0">Evaluate-Belief invokes DetermineAcceptance</definiendum>
				<definiens id="0">Galliers 1992 ; Logal et al. 1994 ) to determine the system 's strength of belief in _bel ( or its negation ) given a set of evidence</definiens>
			</definition>
			<definition id="7">
				<sentence>Determine-Acceptance follows Walker 's weakest link assumption ( Walker 1992 ) and computes the strength of the evidence as the weaker of the strengths of the antecedent belief and the evidential relationship .</sentence>
				<definiendum id="0">Determine-Acceptance</definiendum>
				<definiens id="0">follows Walker 's weakest link assumption ( Walker 1992 ) and computes the strength of the evidence as the weaker of the strengths of the antecedent belief and the evidential relationship</definiens>
			</definition>
			<definition id="8">
				<sentence>A recipe includes a header specifying the action defined by the recipe , the recipe type , the applicability conditions and preconditions of the action , the subactions comprising the body of the recipe , and the goal of performing the action .</sentence>
				<definiendum id="0">recipe</definiendum>
				<definiens id="0">includes a header specifying the action defined by the recipe , the recipe type , the applicability conditions and preconditions of the action , the subactions comprising the body of the recipe</definiens>
			</definition>
			<definition id="9">
				<sentence>In a decomposition recipe , the body consists of 371 Computational Linguistics Volume 24 , Number 3 Action : Recipe-Type : Appl Conds : Precondition : Body : Goal : Share-Info-Reevaluate-Beliefs ( _agentl , _agent2 , _proposed-belief-tree ) Specialization uncertain ( _agentl , _proposed-belief-tree ) focus-of-info-sharing ( docus , _proposed-belief-tree ) Reevaluate-After-Invite-Attack ( _agentl , _agent2 , _focus , _proposed-belief-tree ) Reevaluate-After-Ask-Why ( _agentl , _agent2 , _focus , _proposed-belief-tree ) Reevaluate-After-Invite-Attack-and-Ask-Why ( -agent1 -agent2 -f cus -proposed-belief-tree ) Reevaluate-After-Express-Uncertainty ( _agent1 , _agent2 , _focus , _proposed-belief-tree ) acceptance-determined ( _proposed-belief-tree ) Figure 5 The Share-Info-Reevaluate-Beliefs recipe .</sentence>
				<definiendum id="0">_focus</definiendum>
				<definiendum id="1">_proposed-belief-tree</definiendum>
				<definiendum id="2">_proposed-belief-tree ) acceptance-determined</definiendum>
				<definiens id="0">Recipe-Type : Appl Conds : Precondition : Body : Goal : Share-Info-Reevaluate-Beliefs ( _agentl , _agent2 , _proposed-belief-tree ) Specialization uncertain ( _agentl , _proposed-belief-tree ) focus-of-info-sharing ( docus , _proposed-belief-tree ) Reevaluate-After-Invite-Attack ( _agentl , _agent2 , _focus , _proposed-belief-tree ) Reevaluate-After-Ask-Why ( _agentl , _agent2 ,</definiens>
			</definition>
			<definition id="10">
				<sentence>The recipe for Share-Info-Reevaluate-Beliefs is of type specialization and its body consists of four subactions that correspond to four alternative information-sharing strategies that _agent1 may adopt in attempting to resolve its uncertainty in the acceptance of the selected focus .</sentence>
				<definiendum id="0">recipe for Share-Info-Reevaluate-Beliefs</definiendum>
				<definiens id="0">consists of four subactions that correspond to four alternative information-sharing strategies that _agent1 may adopt in attempting to resolve its uncertainty in the acceptance of the selected focus</definiens>
			</definition>
			<definition id="11">
				<sentence>produced by the Select-Focus-Info-Sharing algorithm , is a set of one or more proposed beliefs that the system can not decide whether to accept and whose acceptance ( or rejection ) will affect the system 's acceptance of the top-level proposed belief .</sentence>
				<definiendum id="0">Select-Focus-Info-Sharing algorithm</definiendum>
				<definiens id="0">a set of one or more proposed beliefs that the system can not decide whether to accept and whose acceptance ( or rejection ) will affect the system 's acceptance of the top-level proposed belief</definiens>
			</definition>
			<definition id="12">
				<sentence>This prediction process involves the system first selecting a subset of the rejected evidence that it predicts it can successfully refute , and then predicting whether eliminating this subset of the rejected evidence is sufficient to cause EA to accept ~_bel .</sentence>
				<definiendum id="0">prediction process</definiendum>
				<definiens id="0">involves the system first selecting a subset of the rejected evidence that it predicts it can successfully refute</definiens>
			</definition>
			<definition id="13">
				<sentence>Our algorithm , Select-Focus-Modification ( Figure 12 ) , is based on the above principles and is invoked with _bel instantiated as the root node of the candidate foci tree .</sentence>
				<definiendum id="0">Select-Focus-Modification</definiendum>
				<definiens id="0">based on the above principles and is invoked with _bel instantiated as the root node of the candidate foci tree</definiens>
			</definition>
			<definition id="14">
				<sentence>_rain-set is the minimum subset of evidence proposed to support _bel that the system believes it must address in order to change EA 's belief in _bel .</sentence>
				<definiendum id="0">_rain-set</definiendum>
				<definiens id="0">the minimum subset of evidence proposed to support _bel that the system believes it must address in order to change EA 's belief in _bel</definiens>
			</definition>
			<definition id="15">
				<sentence>When justification chains have been constructed for an antecedent belief _beli and the evidential relationship between _beli and _rob , the algorithm uses a function Make-Evidence to construct a justification chain with _rob as its root node , the root node of _beli-chain as its child node , and the root node of _reli-chain as the relationship between _beli and _mb ( step 2.3 ) .</sentence>
				<definiendum id="0">justification chains</definiendum>
			</definition>
			<definition id="16">
				<sentence>CORE has two pieces of evidence against Dr. Jones being the professor of CS821 : 1 ) a very strong piece of evidence consisting of the beliefs that Dr. Jones is going on sabbatical in 1998 and that professors on sabbatical do not teach courses , and 2 ) a strong piece of evidence consisting of the beliefs that Dr. Jones ' expertise is compilers , that CS821 is a database course , and that professors generally do not teach courses outside of their areas of expertise .</sentence>
				<definiendum id="0">CS821</definiendum>
				<definiens id="0">a database course</definiens>
			</definition>
			<definition id="17">
				<sentence>To satisfy the precondition of Modify-Node , CORE posts MB ( CA , EA , -~Professor ( CS821 , Jones ) ) as a mutual belief to be achieved .</sentence>
				<definiendum id="0">CORE posts MB</definiendum>
				<definiens id="0">a mutual belief to be achieved</definiens>
			</definition>
			<definition id="18">
				<sentence>Inform has two subactions : Tell which conveys a belief to EA , and Address-Acceptance , which invokes the Select-Justification algorithm ( Figure 13 ) to select justification for the intended mutual belief .</sentence>
				<definiendum id="0">Inform</definiendum>
				<definiendum id="1">Address-Acceptance</definiendum>
				<definiens id="0">has two subactions : Tell which conveys a belief to EA , and</definiens>
				<definiens id="1">invokes the Select-Justification algorithm ( Figure 13 ) to select justification for the intended mutual belief</definiens>
			</definition>
			<definition id="19">
				<sentence>The three dialogue segments in which CORE initiated collaborative negotiation ( CN1 , CN2 , and CN3 ) , however , yielded less uniform results .</sentence>
				<definiendum id="0">CN3</definiendum>
				<definiens id="0">segments in which CORE initiated collaborative negotiation ( CN1 , CN2 , and</definiens>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>More precisely , contextual grammars include contexts ( pairs of words ) , associated with * University of Bucharest , Faculty of Mathematics , Str .</sentence>
				<definiendum id="0">contextual grammars</definiendum>
				<definiens id="0">include contexts ( pairs of words ) , associated with * University of Bucharest</definiens>
			</definition>
			<definition id="1">
				<sentence>This is the case with { wc mi ( w ) I w c { a , b } * } , where mi ( w ) is the mirror image of w. Also the language { w mi ( w ) I w E { a , b } * } can be generated when the maximal use of selectors is considered , but not without involving this feature .</sentence>
				<definiendum id="0">mi ( w )</definiendum>
				<definiendum id="1">w mi</definiendum>
				<definiens id="0">the mirror image of w. Also the language {</definiens>
			</definition>
			<definition id="2">
				<sentence>The effect of hi , h~ -I can also be achieved by a sequential transducer ( with finite memory ) , hence we may state the theorem in the form : every recursively enumerable language is a sequential translation of a contextual language ( generated with maximal use of selectors ) .</sentence>
				<definiendum id="0">enumerable language</definiendum>
				<definiens id="0">a sequential translation of a contextual language ( generated with maximal use of selectors )</definiens>
			</definition>
			<definition id="3">
				<sentence>As usual , given an alphabet V ( which we also call vocabulary ) , we denote by V* the set of all words ( equivalently : strings ) over V , including the empty one , which is denoted by A. The set of all nonempty words over V , hence V* { A } , is denoted by V + .</sentence>
				<definiendum id="0">alphabet V</definiendum>
				<definiens id="0">the set of all words ( equivalently : strings ) over V , including the empty one</definiens>
			</definition>
			<definition id="4">
				<sentence>3 A contextual grammar ( with choice ) is a construct : G = ( V , A , ( S1 , C1 ) , ... , ( Sn , Cn ) ) , n &gt; 1 , where V is an alphabet , A is a finite language over V , $ 1 ... .. Sn are languages over V , and C1 ... .. Cn are finite subsets of V* x V* .</sentence>
				<definiendum id="0">contextual grammar</definiendum>
				<definiendum id="1">, ( S1</definiendum>
				<definiendum id="2">V</definiendum>
				<definiens id="0">a construct : G = ( V , A</definiens>
				<definiens id="1">languages over V , and C1 ... .. Cn are finite subsets of V* x V*</definiens>
			</definition>
			<definition id="5">
				<sentence>C ( `` is an element of '' ) , 0 ( the empty set ) , 2 x ( the family of all subsets of the set X ) .</sentence>
				<definiendum id="0">C ( ``</definiendum>
				<definiens id="0">an element of ''</definiens>
			</definition>
			<definition id="6">
				<sentence>Conversely , from a grammar given as G = ( V , A , C , ~ ) with : C -- ~ { ( Ul , Vl ) ... .. ( Un , Vn ) } , we can pass , for instance , to G ' = ( V , A , ( $ 1 , C1 ) , ... , ( S , ,C , ) ) , taking , for each i , 1 &lt; i &lt; n : ci = { ( ui , vi ) } , and Si the set of strings in V* to which the context ( ui , vi ) can be adjoined , that is : si = { x E V* I ( u~ , vi ) E ~ ( x ) } . The two grammars G and G ' are clearly equivalent in both cases. Thus , in the proofs below we shall use that presentation of a contextual grammar which is more appropriate ( economical ) for that case. Remark 2 The derivation relation defined above has been denoted by ~in in order to distinguish it from the external derivation defined for G , where the context is adjoined at the ends of the derived word : x ==~'ex y iff y = uxv for ( u , v ) E Ci , x E Si , for some i , 1 &lt; i &lt; n. In Marcus ( 1969 ) , only the external derivation is considered , for grammars presented in the functional form , without restrictions on the selection mapping. Contextual grammars with internal derivation were introduced in P~un and Nguyen ( 1980 ) . We do not investigate the external derivation here. Two natural variants of the relation ~in defined above were considered by Mart/n-Vide et al. ( 1995 ) : x : :=-~Mt Y iff x = XlX2X3 , y = XlUX2VX3 , for X2 E Si , ( u , v ) E Ci , for some 1 &lt; i &lt; n , and there are ! ! ! no X~ , x2 , ' X 3 ! E V* such that x = xlx2x3 , x~ E Si , and Ix~l _ &lt; IXll , Ix~I &lt; _ I/B1 , Ix~l &gt; Ix2\ ] ; x ~MR y 250 Marcus , Martin-Vide , and P~un Contextual Grammars iff x = XlX2X3 , y = XlUX2VX3 , for x2 E Si , ( u , v ) E Ci , for some 1 &lt; i &lt; n , and there I !</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">Si</definiendum>
				<definiens id="0">the set of strings in V* to which the context</definiens>
				<definiens id="1">si = { x E V* I ( u~ , vi</definiens>
			</definition>
			<definition id="7">
				<sentence>We say that ~Ml is a derivation in the maximal local mode ( the word-selector x2 is maximal in Si ) and ~Mx is a derivation in the maximal global mode ( the wordselector x2 is maximal with respect to all selectors $ 1 ... .. Sn ) .</sentence>
				<definiendum id="0">~Ml</definiendum>
				<definiendum id="1">~Mx</definiendum>
				<definiens id="0">a derivation in the maximal global mode</definiens>
			</definition>
			<definition id="8">
				<sentence>However , using a selector Si means deciding the membership of a substring of the current string with respect to Si ; when Si is a regular language , this question can be solved in real time , using the simplest type of recognizers : a deterministic finite automaton .</sentence>
				<definiendum id="0">selector Si</definiendum>
				<definiendum id="1">Si</definiendum>
				<definiens id="0">a regular language</definiens>
			</definition>
			<definition id="9">
				<sentence>Consequently , the obtained string is of the form ( bba ) ma p , where m is the number of times of using the production ( { abb } , ( ( bb , a ) } ) minus one , and p is the number of initial occurrences of a , that is p = n. This implies that the occurrence of a immediately to the left-hand side of the initial pair bb has crossed one pair bb ( doubling it ) , the next one has crossed two pairs bb ( doubling them ) , and so on until the leftmost occurrence of a , the n-th one .</sentence>
				<definiendum id="0">obtained string</definiendum>
				<definiendum id="1">p</definiendum>
				<definiens id="0">the number of times of using the production ( { abb } , ( ( bb , a ) } ) minus one</definiens>
				<definiens id="1">the number of initial occurrences of a , that is p = n. This implies that the occurrence of a immediately to the left-hand side of the initial pair bb has crossed one pair bb ( doubling it ) , the next one has crossed two pairs bb ( doubling them ) , and so on until the leftmost occurrence of a</definiens>
			</definition>
			<definition id="10">
				<sentence>Given a language L over the alphabet V , each context ( u , v ) over V selects a set of 256 Marcus , Martin-Vide , and PSun Contextual Grammars strings x such that uxv c L. We say in this case that x is accepted by ( u , v ) in L or that ( u , v ) accepts x in L. Any set C of contexts over V selects the set X of those strings that are accepted in L by any context in C. Obviously , X is here maximal , because it is the set of all strings with the relevant property .</sentence>
				<definiendum id="0">X</definiendum>
			</definition>
			<definition id="11">
				<sentence>The dual phenomenon is the following : each string x over V selects the set C ( x ) of those contexts that accept x in L. To any set E of strings over V we associate the set of contexts accepting in L any string in E. In short , given a language L , each set of contexts ( strings ) selects , with respect to L , a set of strings ( contexts ) ; in other words , each language over V determines a precise interplay of strings and contexts over V. A natural question can be raised : could we now follow an inverse itinerary , by starting from a finite stock A of strings ( over V ) simple enough to be considered primitive well-formed strings ( axioms ) , and by considering a finite set of couples ( Si , Ci ) , 1 &lt; i &lt; n , where Si is a set of strings , while Ci is a finite set of contexts , to ask what is ( are ) the language ( s ) with respect to which Ci selects Si , 1 &lt; i &lt; n ?</sentence>
				<definiendum id="0">Si</definiendum>
				<definiens id="0">the following : each string x over V selects the set C ( x ) of those contexts that accept x in L. To any set E of strings over V we associate the set of contexts accepting in</definiens>
				<definiens id="1">determines a precise interplay of strings and contexts over V. A natural question can be raised</definiens>
				<definiens id="2">A of strings ( over V ) simple enough to be considered primitive well-formed strings ( axioms ) , and by considering a finite set of couples ( Si , Ci</definiens>
				<definiens id="3">a set of strings</definiens>
			</definition>
			<definition id="12">
				<sentence>The corresponding formal language consists of words of the form xcx , for x an arbitrarily long word over an alphabet not containing the symbol c ( this symbol corresponds to the separator o in the Bambara construction ) .</sentence>
				<definiendum id="0">corresponding formal language</definiendum>
				<definiendum id="1">symbol c</definiendum>
				<definiendum id="2">construction</definiendum>
				<definiens id="0">consists of words of the form xcx , for x an arbitrarily long word over an alphabet not containing the</definiens>
			</definition>
			<definition id="13">
				<sentence>This also follows from grammar G2 , for which we have LiR ( G2 ) = LMg ( G2 ) = LMi ( G2 ) : the two selectors are disjoint and their elements are `` marked strings , '' bounded by fixed symbols , hence no selector string is the subword of another selector string .</sentence>
				<definiendum id="0">selector string</definiendum>
				<definiens id="0">follows from grammar G2 , for which we have LiR ( G2 ) = LMg ( G2 ) = LMi ( G2 ) : the two selectors are disjoint and their elements are `` marked strings , '' bounded by fixed symbols</definiens>
			</definition>
			<definition id="14">
				<sentence>It is known that the language M3 mentioned above can not be generated by a tree adjoining grammar ( TAG ) in the pure form introduced by Joshi , Levy , and Takahashi ( 1975 ) , but CF c TAL , where TAL is the family of languages generated by TAGs without additional features ( see also Section 21.2 in Partee , ter Meulen , and Wall \ [ 1990\ ] ) .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiendum id="1">Takahashi</definiendum>
				<definiendum id="2">TAL</definiendum>
			</definition>
			<definition id="15">
				<sentence>A string w E ( V U B ) * , where V is an alphabet , is said to be minimally Dyck covered if : .</sentence>
				<definiendum id="0">string w E</definiendum>
				<definiens id="0">V U B ) * , where V is an alphabet</definiens>
			</definition>
			<definition id="16">
				<sentence>We denote by MDC ( V ) the language of all minimally Dyck covered strings over the alphabet V. To any string x E MDC ( V ) we can associate a tree T ( x ) with labeled edges in the following way : • draw a dot representing the root of the tree ; the tree will be represented with the root up and the leaves down ; • scan x from the left to the right and grow 7- ( x ) according to the following two rules : • for each maximal substring \ [ w of x , for w E V* ( hence after w we find either \ [ or \ ] ) , we draw a new edge , starting from the current point of the partially constructed ~- ( x ) , marked with w on its left side , and placed to the right of the currently constructed tree ; • for each maximal w\ ] , w E V* , not scanned yet ( hence , either before w we find \ ] , or w = , ~ and to the left of \ ] we have a substring \ [ z for some z E V* already scanned ) , we climb the current edge , writing w on its right side .</sentence>
				<definiendum id="0">MDC ( V )</definiendum>
				<definiens id="0">the language of all minimally Dyck covered strings over the alphabet V. To any string x E MDC ( V ) we can associate a tree T ( x ) with labeled edges in the following way : • draw a dot representing the root of the tree</definiens>
				<definiens id="1">a new edge , starting from the current point of the partially constructed ~- ( x ) , marked with w on its left side , and placed to the right of the currently constructed tree ; • for each maximal w\ ] , w E V* , not scanned yet</definiens>
			</definition>
			<definition id="17">
				<sentence>where V is an alphabet , A is a finite subset of MDC ( V ) , Sic MDC ( V ) , and Ci are finite subsets of V '' x V* ( A , A ) , for all 1 &lt; i &lt; n ; in turn , n &gt; 1 .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">an alphabet , A is a finite subset of MDC ( V ) , Sic MDC ( V ) , and Ci are finite subsets of V</definiens>
			</definition>
			<definition id="18">
				<sentence>The string language generated by a bracketed contextual grammar G = ( V , A , ( $ 1 , C1 ) ... .. ( Sn , C , ) ) is defined by : L ( G ) = { pry ( z ) I w == &gt; ~ z , for some w E A } , where pry ( z ) denotes the projection of z E ( V U B ) * on V , that is the string in V* obtained by removing \ [ and \ ] from z. We can also associate to G the bracketed language BL ( G ) defined by : BL ( G ) -= { ( prv ( z ) , T ( z ) ) I w ==~ z , for some w E A } .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">pry</definiendum>
				<definiendum id="2">z ) , T ( z</definiendum>
				<definiens id="0">the projection of z E ( V U B ) * on V , that is the string in V* obtained by removing \ [ and \ ] from z. We can also associate to G the bracketed language BL ( G ) defined by : BL ( G ) -= {</definiens>
			</definition>
			<definition id="19">
				<sentence>Now , a structured contextual grammar is a construct : G = ( V , A , P ) , where V is an alphabet , A is a finite set of structured strings over V , and P is a finite set of triples of the form Ix , ( u , v ) ; puxvl , with x E V + , ( u , v ) c V* x V* , and p , xv a dependence relation over uxv such that p , xv\ [ x = O. The elements of A are called axioms , the triples in P are called productions ; in a production Ix , ( u , v ) ; puxvl , the string x is the selector , ( u , v ) is the context and puxv is a relation defining the structure of uxv ; note that no dependence is considered between the elements of x. ( Thus , we consider here only grammars with finite selectors . )</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">P</definiendum>
				<definiendum id="2">p</definiendum>
				<definiens id="0">a construct : G = ( V , A , P ) , where</definiens>
				<definiens id="1">an alphabet , A is a finite set of structured strings over V</definiens>
			</definition>
			<definition id="20">
				<sentence>The derivation relation is defined ( only for structured strings ) as follows : for ( x , px ) , ( y , py ) , x , y E V + , we write : ( x , px ) =~c ( y , py ) iff x = XlX2X3 , y = XlUX2VX3 , for xl , x3 C V* and ( X2 , ( U , P ) ; Pux2vl E P , such that pylx , x~x3 = Px , and Py\ ] ux2v = PUXRV .</sentence>
				<definiendum id="0">derivation relation</definiendum>
				<definiens id="0">only for structured strings ) as follows : for ( x , px ) , ( y , py ) , x , y E V +</definiens>
			</definition>
			<definition id="21">
				<sentence>Take L C_ T* , L E RE , and a type-0 Chomsky grammar Go = ( N , T , S , P ) for L. Consider the new symbols \ [ , \ ] , t- , and construct the contextual grammar G with the alphabet : V = N U T U { \ [ , \ ] ,1- } , the starting string S , and the following productions : ° ( { u } , { ( \ [ , Iv ) } ) , for u ~ v E P , ( { ol\ [ u\ ] } , { 0- , ol ) } ) , for o~ E N U T , u -- ~ v E P , ( { o~-fl } , { ( t- , o~ ) } ) , for a , fl E N UT .</sentence>
				<definiendum id="0">Iv</definiendum>
				<definiens id="0">the new symbols \ [ , \ ] , t- , and construct the contextual grammar G with the alphabet : V = N U T U { \</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>It is a nontrivial task to divide the senses of a word and determine this set , for word sense is an abstract concept frequently based on subjective and subtle distinctions in topic , register , dialect , collocation , part of speech , and valency ( McRoy 1992 ) .</sentence>
				<definiendum id="0">valency</definiendum>
				<definiens id="0">a nontrivial task to divide the senses of a word and determine this set</definiens>
				<definiens id="1">topic , register , dialect , collocation , part of speech</definiens>
			</definition>
			<definition id="1">
				<sentence>Dolan proposes a heuristic algorithm for forming unlabeled clusters of closely related senses in the LDOCE to eliminate distinctions that are unnecessarily fine for WSD .</sentence>
				<definiendum id="0">Dolan</definiendum>
				<definiens id="0">proposes a heuristic algorithm for forming unlabeled clusters of closely related senses in the LDOCE to eliminate distinctions that are unnecessarily fine for WSD</definiens>
			</definition>
			<definition id="2">
				<sentence>The current implementation of TopSense uses the topical information in the Longman Lexicon of Contemporary English ( LLOCE ) ( McArthur 1992 ) to cluster LDOCE senses .</sentence>
				<definiendum id="0">TopSense</definiendum>
			</definition>
			<definition id="3">
				<sentence>WordNet is organized as a set of hierarchical , conceptual taxonomies of nouns , verbs , adjectives , and adverbs called synsets .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">a set of hierarchical , conceptual taxonomies of nouns , verbs , adjectives</definiens>
			</definition>
			<definition id="4">
				<sentence>The LLOCE is a hierarchical thesaurus that organizes word senses primarily according to subject matter .</sentence>
				<definiendum id="0">LLOCE</definiendum>
				<definiens id="0">a hierarchical thesaurus that organizes word senses primarily according to subject matter</definiens>
			</definition>
			<definition id="5">
				<sentence>The LLOCE contains over 23,000 different senses for some 15,000 distinct words .</sentence>
				<definiendum id="0">LLOCE</definiendum>
				<definiens id="0">contains over 23,000 different senses for some 15,000 distinct words</definiens>
			</definition>
			<definition id="6">
				<sentence>Subject Set Gloss for Subjects A 1-158 B 1-181 C 1-357 D 1-186 E 1-143 F 1-283 G 1-293 H 1-252 I 1-148 J 1-240 K 1-207 L 1-273 M 1-225 N 1-367 Life and living things Body ; its function and welfare People and family Buildings , houses , home , clothes , belongings , personal care Food , drink , and farming Feeling , emotions , attitudes , and sensations Thought , communication , language , and grammar Substance , materials , objects , and equipment Arts/Crafts , science/technology , industry/education Numbers , measurement , money , and commerce Entailment , sports and games Space and time Movement , location , travel , and transportation General and abstract terms to subject L ( Space and time ) .</sentence>
				<definiendum id="0">science/technology</definiendum>
				<definiens id="0">communication , language , and grammar Substance , materials , objects , and equipment Arts/Crafts</definiens>
			</definition>
			<definition id="7">
				<sentence>Subject Range Gloss Examples L 001-019 a The universe L 020-039 b Light and color L 040-079 c Weather and temperature L 080-129 d Geography L 130-169 e Time generally L 170-199 f Beginning and ending L 200-219 g Old , new , and young L 220-249 h Period/Measure of time L 250-273 i Function words ( time ) sun , moon , star , left , right , etc. light , dark , ray , color , white , black , etc. weather , sky , rain , snow , rain , ice , etc. stream , sea , lake , flood , to flow , etc. time , history , frequent , permanent , etc. start , stop , late , last , etc. ancient , modem , future , age , etc. day , night , second , minute , etc. now , soon , always , ever , after , etc. `` L9 &gt; O , o , ... . ... .. ½ ... ...</sentence>
				<definiendum id="0">history</definiendum>
				<definiens id="0">color , white , black , etc. weather , sky , rain , snow , rain , ice , etc. stream , sea , lake , flood , to flow</definiens>
			</definition>
			<definition id="8">
				<sentence>A dictionary definition , in general , begins with a genus term ( that is , conceptual ancestor of the sense ) , followed by a set of differentiae that are words semantically related to the sense to provide the specifics .</sentence>
				<definiendum id="0">dictionary definition</definiendum>
				<definiens id="0">begins with a genus term ( that is , conceptual ancestor of the sense</definiens>
			</definition>
			<definition id="9">
				<sentence>The relevancy of VDi to the sense S according to term tj is therefore given by the following weight : wij = x = × log ( N/dR ) , where N is the number of documents in the collection .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of documents in the collection</definiens>
			</definition>
			<definition id="10">
				<sentence>Assume 0 is the threshold and 0 is an estimator of 8 , and B is the bound on the error of estimation .</sentence>
				<definiendum id="0">Assume 0</definiendum>
				<definiendum id="1">B</definiendum>
				<definiens id="0">the bound on the error of estimation</definiens>
			</definition>
			<definition id="11">
				<sentence>The Appendix gives a sense-by-sense rundown of all senses tested and evaluated .</sentence>
				<definiendum id="0">Appendix</definiendum>
				<definiens id="0">gives a sense-by-sense rundown of all senses tested and evaluated</definiens>
			</definition>
			<definition id="12">
				<sentence>Wilks et al. ( 1990 ) call the defining words in the LDOCE definition semantic primitives ( SP ) and suggest that a semantic network constructed on the strength of co-occurrence of SPs in definitions can be useful for a variety of NLP tasks , ranging from WSD , to machine translation , to message understanding .</sentence>
				<definiendum id="0">LDOCE definition semantic primitives</definiendum>
				<definiendum id="1">SP</definiendum>
				<definiens id="0">a semantic network constructed on the strength of co-occurrence of SPs in definitions can be useful for a variety of NLP tasks , ranging from WSD , to machine translation , to message understanding</definiens>
			</definition>
			<definition id="13">
				<sentence>For instance , the LLOCE lists the following cross-references for the topic of Eb ( Food ) : Ac ( Animals~Mammals ) , Ad ( Birds ) , Af ( Fish and other water creatures ) , Ah ( Parts of animal ) , Ai ( Kinds of parts of plants ) , Aj ( Plant in general ) , Jg ( Shopkeepers and shops selling food ) .</sentence>
				<definiendum id="0">LLOCE</definiendum>
				<definiendum id="1">Aj ( Plant</definiendum>
				<definiens id="0">Kinds of parts of plants</definiens>
			</definition>
			<definition id="14">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>The feedback set for sense si of word W is the union of all contexts that contain some noun found in the entry of si ( W ) in an MRD .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">the union of all contexts that contain some noun found in the entry of si (</definiens>
			</definition>
			<definition id="1">
				<sentence>Affinity ( a real number between 0 and 1 ) reflects the contextual relationships between W and the words of the sentence .</sentence>
				<definiendum id="0">Affinity</definiendum>
				<definiens id="0">a real number between 0 and 1 ) reflects the contextual relationships between W and the words of the sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>We say that a word belongs to a sentence , denoted as W E S , if it is textually contained there ; in this case , sentence is said to include the word : S 9 W. Affinity is then defined as follows : affn ( W , S ) = max simn ( W , Wi ) ( 1 ) WiCS aff , ( S , W ) = max simn ( S , Sj ) ( 2 ) 8j~w where n denotes the iteration number , and the similarity values are defined by the word and sentence similarity matrices , WSMn and SSMn .3 The initial representation of a sentence , as the set of words that it directly contains , is now augmented by a sentence should determine the affinity between the two .</sentence>
				<definiendum id="0">S )</definiendum>
				<definiendum id="1">max simn ( S , Sj</definiendum>
				<definiendum id="2">n</definiendum>
				<definiendum id="3">WSMn</definiendum>
				<definiens id="0">said to include the word : S 9 W. Affinity is then defined as follows : affn ( W ,</definiens>
				<definiens id="1">the iteration number , and the similarity values are defined by the word and sentence similarity matrices</definiens>
				<definiens id="2">initial representation of a sentence , as the set of words that it directly contains , is now augmented by a sentence should determine the affinity between the two</definiens>
			</definition>
			<definition id="3">
				<sentence>52 Karov and Edelman Similarity-based Word Sense Disambiguation verbs , adjectives of the target word , bigrams , trigrams , and subject-verb or verb-object pairs , ( 2 ) discarding features with a low weight ( cf. Section A.3 of the appendix ) , and ( 3 ) using the remaining features instead of single words ( i.e. , by representing a sentence by the set of significant features it contains , and a feature by the set of sentences in which it appears ) .</sentence>
				<definiendum id="0">Edelman Similarity-based Word Sense Disambiguation</definiendum>
				<definiens id="0">contains , and a feature by the set of sentences in which it appears )</definiens>
			</definition>
			<definition id="4">
				<sentence>probabilistic language modeling , that require a probability density to be estimated from data .</sentence>
				<definiendum id="0">probabilistic language modeling</definiendum>
				<definiens id="0">a probability density to be estimated from data</definiens>
			</definition>
			<definition id="5">
				<sentence>frequency is max { 0 , 1 max5xfreq ( X ) J ' where max5xfreq ( X ) is a function of the five highest frequencies in the global corpus , and X is any noun , or verb , or adjective there .</sentence>
				<definiendum id="0">frequency</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">a function of the five highest frequencies in the global corpus</definiens>
			</definition>
			<definition id="6">
				<sentence>The total weight of a word is the product of the above factors , each normalized by factor ( Wi , S ) the sum of factors of the words in the sentence : weight ( Wi , $ ) = z_ , ~'wjcs factor ( W j , S ) ' Yarowsky ( 1992 ) suggested to interpolate between probabilities computed within the subcorpus and probabilities computed over the entire corpus .</sentence>
				<definiendum id="0">total weight of a word</definiendum>
				<definiens id="0">the sum of factors of the words in the sentence : weight ( Wi , $ ) = z_ , ~'wjcs factor ( W j , S ) ' Yarowsky ( 1992 ) suggested to interpolate between probabilities computed within the subcorpus and probabilities computed over the entire corpus</definiens>
			</definition>
			<definition id="7">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="2006">
			<definition id="0">
				<sentence>3 Such a UR is fed as input to the function OEN , which produces as output the set of all possible surface realizations ( SRs ) for this UR , called the candidate set .</sentence>
				<definiendum id="0">OEN</definiendum>
				<definiens id="0">produces as output the set of all possible surface realizations ( SRs ) for this UR , called the candidate set</definiens>
			</definition>
			<definition id="1">
				<sentence>To do this , OT imposes a set of well-formedness constraints on the elements of the candidate set .</sentence>
				<definiendum id="0">OT</definiendum>
				<definiens id="0">imposes a set of well-formedness constraints on the elements of the candidate set</definiens>
			</definition>
			<definition id="2">
				<sentence>Definition An optimality system ( OS ) is a triple G = ( E , tEN , C ) , where E is a finite alphabet , GEN is a relation over E* x E '' and C = ( cl ... .. Cp ) , p &gt; 1 , is an ordered sequence of total functions from E* to N. The basic idea underlying this definition is as follows : If w is a well-formed UR , \ [ GEN\ ] ( W ) is the nonempty set of all associated SR , otherwise \ [ ¢nN\ ] ( W ) = 0 .</sentence>
				<definiendum id="0">optimality system</definiendum>
				<definiendum id="1">OS )</definiendum>
				<definiendum id="2">GEN</definiendum>
				<definiens id="0">a triple G = ( E , tEN , C ) , where E is a finite alphabet</definiens>
				<definiens id="1">a relation over E* x E ''</definiens>
				<definiens id="2">the nonempty set of all associated SR , otherwise \ [ ¢nN\ ] ( W ) = 0</definiens>
			</definition>
			<definition id="3">
				<sentence>Since the question that we focus on in this research is that of determining whether the class of mappings specifiable in OT is beyond the formal power of finite-state transducers , allowing ann to be beyond the power of a 310 Frank and Satta Constraint Violability finite-state transducer would decide the question byfiat , s In addition , we assume that each constraint c in C is regular in that it satisfies the following requirement : For each k E N , the set { w I w E ~* , c ( w ) = k } ( i.e. , the inverse image of k under c ) is a regular language .</sentence>
				<definiendum id="0">c )</definiendum>
				<definiens id="0">a regular language</definiens>
			</definition>
			<definition id="4">
				<sentence>Then OTc is a rational relation .</sentence>
				<definiendum id="0">OTc</definiendum>
				<definiens id="0">a rational relation</definiens>
			</definition>
			<definition id="5">
				<sentence>Since L ( ci ) is a regular language , from an already mentioned property it follows that : R1 = Rrst ( R , L ( ci ) ) is a rational relation as well .</sentence>
				<definiendum id="0">ci )</definiendum>
				<definiens id="0">a regular language , from an already mentioned property it follows that : R1 = Rrst</definiens>
				<definiens id="1">a rational relation as well</definiens>
			</definition>
			<definition id="6">
				<sentence>Then OTG is a rational relation .</sentence>
				<definiendum id="0">OTG</definiendum>
				<definiens id="0">a rational relation</definiens>
			</definition>
			<definition id="7">
				<sentence>We have shown that when GUN is a rational relation and the constraints have a finite codomain , constraint ranking as defined by OT yields a system whose generative capacity does not exceed that of rational relations .</sentence>
				<definiendum id="0">GUN</definiendum>
				<definiendum id="1">constraint ranking</definiendum>
				<definiens id="0">a rational relation</definiens>
				<definiens id="1">a system whose generative capacity does not exceed that of rational relations</definiens>
			</definition>
			<definition id="8">
				<sentence>The following example ( due to P. Smolensky , after an idea of M. Hiller who first proved this separation result ) shows this fact using only a single constraint : G = { a , b } , GEN = { ( anbm , anbm ) l n , m E N } U { ( a'bm , b'a m ) l n , m E N } , c ( w ) = # a ( W ) , where # a ( w ) denotes the number of occurrences of a within w. ( Constraint c can be seen as a prohibition against the occurrence of the letter a in an SR. ) Clearly GEN is a rational relation and c satisfies our previous assumptions .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">this fact using only a single constraint : G = { a , b }</definiens>
				<definiens id="1">the number of occurrences of a within w. ( Constraint c can be seen as a prohibition against the occurrence of the letter a in an SR. ) Clearly GEN is a rational relation</definiens>
			</definition>
</paper>

		<paper id="3004">
			<definition id="0">
				<sentence>It describes an implemented system that integrates two robust systems : SAGE -- an intelligent graphics presentation system ( Roth et al. 1994 ) , and a natural language generator , consisting of a text planner ( Young and Moore 1994 ; Young 1997 ) , a microplanner implementing tactical decisions , and a sentence realizer ( Elhadad and Robin 1992 ) .</sentence>
				<definiendum id="0">sentence realizer</definiendum>
				<definiens id="0">SAGE -- an intelligent graphics presentation system ( Roth et al. 1994 ) , and a natural language generator , consisting of a text planner</definiens>
			</definition>
			<definition id="1">
				<sentence>The referring expression module uses a well-known domain-independent algorithm that , given an intended referent , builds a description uniquely identifying it .</sentence>
				<definiendum id="0">well-known domain-independent algorithm</definiendum>
				<definiens id="0">an intended referent , builds a description uniquely identifying it</definiens>
			</definition>
			<definition id="2">
				<sentence>SAGE is a knowledge-based presentation system that designs graphical displays of combinations of diverse information ( e.g. , quantitative , relational , temporal , hierarchical , categorical , geographic ) .</sentence>
				<definiendum id="0">SAGE</definiendum>
				<definiens id="0">a knowledge-based presentation system that designs graphical displays of combinations of diverse information ( e.g. , quantitative , relational , temporal , hierarchical , categorical , geographic )</definiens>
			</definition>
			<definition id="3">
				<sentence>SAGE 'S output consists of one or more coordinated sets of 2-D information graphics that use a variety of graphical lechniques to integrate multiple data attributes in a single display .</sentence>
				<definiendum id="0">SAGE 'S output</definiendum>
				<definiens id="0">consists of one or more coordinated sets of 2-D information graphics that use a variety of graphical lechniques to integrate multiple data attributes in a single display</definiens>
			</definition>
			<definition id="4">
				<sentence>Each symbol class consists of a definition of the spatial relationship among a set of graphemes and the correspondence between the parameters of this set and attributes types in a data set .</sentence>
				<definiendum id="0">symbol class</definiendum>
				<definiens id="0">consists of a definition of the spatial relationship among a set of graphemes and the correspondence between the parameters of this set and attributes types in a data set</definiens>
			</definition>
			<definition id="5">
				<sentence>Each encoder class consists of a definition of the relation between a family of data set attributes and a particular graphical type .</sentence>
				<definiendum id="0">encoder class</definiendum>
				<definiens id="0">consists of a definition of the relation between a family of data set attributes and a particular graphical type</definiens>
			</definition>
			<definition id="6">
				<sentence>In addition to this knowledge about graphemes , symbols , and encoders , SAGE uses knowledge of the characteristics of data relevant to graphic design ( Roth and Mattis 1990 ; Roth and Hefley 1993 ) , including knowledge of data types and scales of measurement ( e.g. , quantitative , interval , ordinal , or nominal data sets ) , structural 436 Mittal , Moore , Carenini , and Roth Generating Chart Captions House BEECHWOOD-ii74 BEECHWOOD-1950 BEECHWOOD-2266 BEECHWOOD-3237 BOULEVARD-3931 COLLEGE-637 DOUGLAS-5919 MOREWOOD-508 PENHAM-6828 SHADY-1205 SHADY-263 SHADY-2908 WALNUT-6343 WELFER-1027 WIGHTMAN-1236 WILKINS-5735 WOODWELL-6663 $ 0K £3D m \ [ mS q3 q Q C~ m ~3 4£Z3 m £13 48 $ 80K $ 160K $ 240K House Price $ 320K 7-Feb 5-May E ) • l I • \ [ \ ] : i : i : ' : '' ' , , i • \ [ \ ] • r • i ~l • r • • I • • i i • I I • i I • 24-Ju\ [ 12-0ct 31-Dec HH C21 CB Date Listing Agency Figure 2 A sample graphic generated by SAGE .</sentence>
				<definiendum id="0">SAGE</definiendum>
				<definiens id="0">uses knowledge of the characteristics of data relevant to graphic design</definiens>
			</definition>
			<definition id="7">
				<sentence>Finall3¢ SAGE has a library of graphical techniques , knowledge of the appropriateness of the techniques for different data and tasks , and design knowledge for as437 Computational Linguistics Volume 24 , Number 3 sembling these techniques into composites that can integrate information in a single display .</sentence>
				<definiendum id="0">SAGE</definiendum>
				<definiens id="0">a library of graphical techniques , knowledge of the appropriateness of the techniques for different data and tasks</definiens>
			</definition>
			<definition id="8">
				<sentence>SAGE uses this graphic design knowledge together with the data characterization knowledge to generate displays of information .</sentence>
				<definiendum id="0">SAGE</definiendum>
				<definiens id="0">uses this graphic design knowledge together with the data characterization knowledge to generate displays of information</definiens>
			</definition>
			<definition id="9">
				<sentence>The X-axis shows the house prices , whereas the Y-axis shows the house 's number of days on the market .</sentence>
				<definiendum id="0">X-axis</definiendum>
				<definiens id="0">shows the house prices , whereas the Y-axis shows the house 's number of days on the market</definiens>
			</definition>
			<definition id="10">
				<sentence>White indicates a transition point .</sentence>
				<definiendum id="0">White</definiendum>
				<definiens id="0">indicates a transition point</definiens>
			</definition>
			<definition id="11">
				<sentence>Anchor mappings refer to the mapping between a functionally independent attribute ( FIA ) -- -usually the key in the database schema -- and the axis it is mapped to .</sentence>
				<definiendum id="0">Anchor mappings</definiendum>
				<definiens id="0">the mapping between a functionally independent attribute ( FIA ) -- -usually the key in the database schema -- and the axis it is mapped to</definiens>
			</definition>
			<definition id="12">
				<sentence>Decompositional refinement selects a composite action and creates a subplan for that action by adding instances of the steps listed in the decomposition operator to the current plan .</sentence>
				<definiendum id="0">Decompositional refinement</definiendum>
				<definiens id="0">selects a composite action and creates a subplan for that action by adding instances of the steps listed in the decomposition operator to the current plan</definiens>
			</definition>
			<definition id="13">
				<sentence>Causal refinement selects an unsatisfied precondition of a step in the plan and adds a causal link to establish the needed condition .</sentence>
				<definiendum id="0">Causal refinement</definiendum>
			</definition>
			<definition id="14">
				<sentence>12 • Cp ( U ) : Highest ranking element of Cf ( U ) • Cb ( U ) : The backward-looking center ( unique ) is the highest ranking Cf ( Ui_l ) realized in the current utterance Ui .</sentence>
				<definiendum id="0">U )</definiendum>
				<definiendum id="1">backward-looking center</definiendum>
				<definiens id="0">the highest ranking Cf ( Ui_l ) realized in the current utterance Ui</definiens>
			</definition>
			<definition id="15">
				<sentence>Cb ( U ) is a discourse construct , therefore the same utterance in different discourse segments may have a different Cb .</sentence>
				<definiendum id="0">Cb</definiendum>
				<definiens id="0">a discourse construct , therefore the same utterance in different discourse segments may have a different Cb</definiens>
			</definition>
			<definition id="16">
				<sentence>463 Computational Linguistics Volume 24 , Number 3 STEP : ( SPEECH-ACT ( ( PROCESS PRESENT ) ( CIRCUM : NONE ) ( AGENT ( CHART ) ) ( AFFECTED ( DATA-SET ( NAME ( : SET ( TS-2480 ) ) ) ( RELATION ( : SET ( HOUSE-SALES-INF0 ) ) ) ) ( ANCHOR DOM-2516 ) ) ) ) STEP : ( SPEECH-ACT ( ( PROCESS SHOW ) ( CIRCUM ( :0NLY-0NE CHART ) ) ( AGENT ( Y-AXIS ) ) ( AFFECTED ( ANCHOR ( Y-AXIS ) DOM-2589 ) ) ) ) STEP : ( SPEECH-ACT ( ( PROCESS SHOW ) ( CIRCUM ( :0NLY-0NE CHART ) ) ( GRAPHEME HORIZONTAL-INTERVAL-BAR-SET ) ( AGENT ( : PROP ( XI ) ( HORIZONTAL-INTERVAL-BAR-SET ) ) ) ( AFFECTED ( : PROP ( ( : SET ( DOM-2512 ) ) ) ( DOM-2589 DOM-2516 ) ) ) ( ANCHOR DOM-2516 ) ) ) STEP : ( SPEECH-ACT ( ( PROCESS SHOW ) ( CIRCUM ( :0NLY-0NE CHART ) ) ( GRAPHEME HORIZONTAL-INTERVAL-BAR-SET ) ( AGENT ( : PROP ( X2 ) ( HORIZONTAL-INTERVAL-BAR-SET ) ) ) ( AFFECTED ( : PR0P ( ( : SET ( DOM-2517 ) ) ) ( DOM-2589 DOM-2516 ) ) ) ( ANCHOR DOM-2516 ) ) ) STEP : ( SPEECH-ACT ( ( PROCESS SHOW ) ( CIRCUM ( :0NLY-0NE CHART ) ) ( GRAPHEME NIL ) ( AGENT ( MARK-SET ) ) ( AFFECTED ( : PROP ( ( : SET ( DOM-2590 ) ) ) ( DOM-2589 DOM-2516 ) ) ) ( ANCHOR DOM-2516 ) ) ) This chart presents information about house sales from data-set TS-2480 .</sentence>
				<definiendum id="0">PROCESS PRESENT )</definiendum>
				<definiendum id="1">NAME</definiendum>
				<definiendum id="2">SPEECH-ACT ( ( PROCESS SHOW ) ( CIRCUM</definiendum>
				<definiendum id="3">SPEECH-ACT ( ( PROCESS SHOW ) ( CIRCUM ( :0NLY-0NE CHART ) ) ( GRAPHEME HORIZONTAL-INTERVAL-BAR-SET ) ( AGENT</definiendum>
				<definiens id="0">ANCHOR ( Y-AXIS ) DOM-2589 ) ) ) ) STEP : ( SPEECH-ACT ( ( PROCESS SHOW ) ( CIRCUM ( :0NLY-0NE CHART ) ) ( GRAPHEME HORIZONTAL-INTERVAL-BAR-SET ) ( AGENT ( : PROP ( XI ) ( HORIZONTAL-INTERVAL-BAR-SET ) ) )</definiens>
				<definiens id="1">PROP ( X2 ) ( HORIZONTAL-INTERVAL-BAR-SET ) ) ) ( AFFECTED ( : PR0P ( ( : SET ( DOM-2517 )</definiens>
				<definiens id="2">SPEECH-ACT ( ( PROCESS SHOW ) ( CIRCUM ( :0NLY-0NE CHART ) ) ( GRAPHEME NIL ) ( AGENT ( MARK-SET ) ) ( AFFECTED ( : PROP ( ( : SET ( DOM-2590 )</definiens>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>The chart is a data structure that contains all of the constituents for which subtrees have been found , that is , constituents for which a derivation has been found and which may therefore appear in some complete parse of the sentence .</sentence>
				<definiendum id="0">chart</definiendum>
				<definiens id="0">a data structure that contains all of the constituents for which subtrees have been found , that is , constituents for which a derivation has been found and which may therefore appear in some complete parse of the sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>Ideally , we would like to use as our figure of merit the conditional probability of that constituent , given the entire sentence , in order to choose a constituent that not only appears likely in isolation , but is most likely given the sentence as a whole ; that is , we would like to pick the constituent that maximizes the following quantity : P ( N~ , k I to , n ) where t0 , n is the sequence of the n tags , or parts of speech , in the sentence ( numbered to ... .. tn-1 ) , and N~ , k is a nonterminal of type i covering terms tj ... tk-1 .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the conditional probability of that constituent , given the entire sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>Edge creation is a good measure of CFG parser effort , since it is independent of platform and implementation .</sentence>
				<definiendum id="0">Edge creation</definiendum>
				<definiens id="0">a good measure of CFG parser effort</definiens>
			</definition>
			<definition id="3">
				<sentence>CPU time : The total CPU time ( in seconds ) needed to get 95 % of the probability mass for all of the 500 sentences .</sentence>
				<definiendum id="0">CPU time</definiendum>
				<definiens id="0">The total CPU time ( in seconds</definiens>
			</definition>
			<definition id="4">
				<sentence>Inside probability is defined as the probability of the words or tags in the constituent given that the constituent is dominated by a particular nonterminal symbol ; see Figure 2 .</sentence>
				<definiendum id="0">Inside probability</definiendum>
				<definiens id="0">the probability of the words or tags in the constituent given that the constituent is dominated by a particular nonterminal symbol</definiens>
			</definition>
			<definition id="5">
				<sentence>Outside probability o~ of a constituent N~ , k is defined as the probability of that constituent and the rest of the words in the sentence ( or rest of the tags in the tag sequence , in our case ) ; see Figure 3 .</sentence>
				<definiendum id="0">Outside probability o~ of a constituent N~ , k</definiendum>
				<definiens id="0">the probability of that constituent and the rest of the words in the sentence ( or rest of the tags in the tag sequence</definiens>
			</definition>
			<definition id="6">
				<sentence>O~L ( N~ , k ) lhs ( e ) eEq , , N~q , ~ Given a complete parse of the sentence , the formula above gives an exact value for aL .</sentence>
				<definiendum id="0">k ) lhs</definiendum>
				<definiens id="0">a complete parse of the sentence , the formula above gives an exact value for aL</definiens>
			</definition>
			<definition id="7">
				<sentence>We can derive an estimate similar to the prefix estimate but containing a much simpler model of the context as follows : P ( N~ , k , tO , n ) p ( X ) 'k It0 , . )</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">derive an estimate similar to the prefix estimate but containing a much simpler model of the context as follows : P ( N~ , k , tO , n ) p (</definiens>
			</definition>
			<definition id="8">
				<sentence>p ( to , n ) p ( to , j , tk , , , ) p ( N~ , k I to , j , tk , , ) p ( tj , k \ [ Nji , k , to , j , tk , , , ) p ( to , j , tk , n ) p ( tj , k I to , j , tk , , , ) Once again applying the usual independence assumption that given a nonterminal , the tag sequence it generates depends only on that nonterminal , we can rewrite the figure of merit as follows : p ( Nii , k I to , n ) , ~ P ( N~ , k I to , j , tk , n ) fl ( N~ , k ) p ( tj , k l to , j , tk , , ) As usual , we use a trigram model for the tags , giving p ( tj , k I to , j , tk , , ) ~ p ( tj , k \ [ tj-2 , tj-1 ) .</sentence>
				<definiendum id="0">N~ , k ) p</definiendum>
				<definiens id="0">N~ , k I to , j , tk , , ) p ( tj , k \ [ Nji , k , to , j , tk , ,</definiens>
				<definiens id="1">p ( Nii , k I to , n ) , ~ P ( N~ , k I to , j , tk</definiens>
			</definition>
			<definition id="9">
				<sentence>We can derive a similar estimate using context on both sides of the constituent as follows : p ( N~ , k \ [ to , , , ) p ( Nji , k , t0 , , ) p ( t0 , n ) p ( to , j ) p ( N~ , k I to , j ) p ( tj , k INS , k , to , j ) p ( tk \ [ i i to , j , N ; , k , tj , k ) p ( tk + l , n I to , j , N ; , k , tj , k , tk ) p ( to , j ) p ( tj , k l to , j ) p ( tk \ [ tO , k ) p ( tk+l , n I tO , k+l ) p ( Njik \ [ to , j ) p ( tj , k INS , k , to , j ) p ( tk \ [ i to , j , N ; , k , tj , k ) p ( tk+l , n \ [ tO , k+1 , N~ , k ) p ( tj , kItO , / ) p ( tk\ ] tO , k ) p ( tk+l , , I tO , k+1 ) Once again applying the usual independence assumption that given a nonterminal , the tag sequence it generates depends only on that nonterminal and also assuming that the probability of tk+l , n depends only on the previous tags , we can rewrite the figure of merit as follows : p ( Nj , k I to , j ) fl ( N~ , k ) p ( tk \ [ t0 , k , N ) i , k ) p ( N ; i , k \ [ to , , , ) ~ p ( tj , k+l \ [ to , j ) Now we add some new independence assumptions .</sentence>
				<definiendum id="0">k ) p</definiendum>
				<definiens id="0">n I tO , k+l ) p ( Njik \ [ to , j ) p ( tj , k INS , k , to , j ) p ( tk \ [ i to , j</definiens>
				<definiens id="1">tk+l , , I tO , k+1 ) Once again applying the usual independence assumption that given a nonterminal , the tag sequence it generates depends only on that nonterminal and also assuming that the probability of tk+l</definiens>
			</definition>
			<definition id="10">
				<sentence>k ) p ( tk ) which is simply the product of the two boundary statistics described in the previous section .</sentence>
				<definiendum id="0">k ) p ( tk</definiendum>
				<definiens id="0">the product of the two boundary statistics described in the previous section</definiens>
			</definition>
			<definition id="11">
				<sentence>Recall is defined as the percentage of constituents in the treebank test data that are found by our parser .</sentence>
				<definiendum id="0">Recall</definiendum>
			</definition>
</paper>

	</volume>
