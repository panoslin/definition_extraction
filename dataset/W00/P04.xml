<?xml version="1.0" encoding="UTF-8"?>
	<volume id="P04">

		<paper id="1086">
			<definition id="0">
				<sentence>Discriminative learning methods , such as Maximum Entropy Markov Models ( McCallum et al. , 2000 ) , Projection Based Markov Models ( Punyakanok and Roth , 2000 ) , Conditional Random Fields ( Lafferty et al. , 2001 ) , Sequence AdaBoost ( Altun et al. , 2003a ) , Sequence Perceptron ( Collins , 2002 ) , Hidden Markov Support Vector Machines ( Altun et al. , 2003b ) and Maximum-Margin Markov Networks ( Taskar et al. , 2004 ) , overcome the limitations of HMMs .</sentence>
				<definiendum id="0">Discriminative learning methods</definiendum>
				<definiendum id="1">Conditional Random Fields</definiendum>
				<definiendum id="2">Sequence AdaBoost</definiendum>
				<definiendum id="3">Hidden Markov Support Vector Machines</definiendum>
				<definiens id="0">Altun et al. , 2003b ) and Maximum-Margin Markov Networks ( Taskar et al. , 2004 ) , overcome the limitations of HMMs</definiens>
			</definition>
			<definition id="1">
				<sentence>Among these methods , CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging ( Lafferty et al. , 2001 ) , Named-Entity Recognition ( Collins , 2002 ) and shallow parsing ( Sha and Pereira , 2003 ; McCallum , 2003 ) .</sentence>
				<definiendum id="0">CRFs</definiendum>
				<definiendum id="1">Named-Entity Recognition</definiendum>
				<definiens id="0">the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging ( Lafferty et al. , 2001 ) ,</definiens>
			</definition>
			<definition id="2">
				<sentence>They define a conditional probability distribution of a label sequence y given an observation sequence x. In this paper , x = ( x1 ; x2 ; : : : ; xn ) denotes a sentence of length n and y = ( y1 ; y2 ; : : : ; yn ) denotes the label sequence corresponding to x. In pitch accent prediction , xt is a word and yt is a binary label denoting whether xt is accented or not .</sentence>
				<definiendum id="0">xn )</definiendum>
				<definiendum id="1">xt</definiendum>
				<definiendum id="2">yt</definiendum>
				<definiens id="0">denotes a sentence of length n and y = ( y1</definiens>
				<definiens id="1">a word and</definiens>
			</definition>
			<definition id="3">
				<sentence>CRFs specify a linear discriminative function F parameterized by over a feature representation of the observation and label sequence ( x ; y ) .</sentence>
				<definiendum id="0">CRFs</definiendum>
				<definiens id="0">specify a linear discriminative function F parameterized by over a feature representation of the observation and label sequence ( x ; y )</definiens>
			</definition>
			<definition id="4">
				<sentence>The model is assumed to be stationary , thus the feature representation can be partitioned with respect to positions tin the sequence and linearly combined with respect to the importance of each feature k , denoted by k. Then the discriminative function can be stated as in Equation 1 : F ( x ; y ; ) = X t h ; t ( x ; y ) i ( 1 ) Then , the conditional probability is given by p ( yjx ; ) = 1Z ( x ; ) F ( x ; y ; ) ( 2 ) where Z ( x ; ) = P yF ( x ; y ; ) is a normalization constant which is computed by summing over all possible label sequences y of the observation sequence x. We extract two types of features from a sequence pair : vation sequence , such as part-of-speech tag of a word that is within a window centered at the word currently labeled , e.g. Is the current word pitch accented and the part-of-speech tag of the previous word=Noun ?</sentence>
				<definiendum id="0">)</definiendum>
				<definiens id="0">2 ) where Z ( x ; ) = P yF ( x ; y ;</definiens>
				<definiens id="1">a normalization constant which is computed by summing over all possible label sequences y of the observation sequence x. We extract</definiens>
			</definition>
			<definition id="5">
				<sentence>In CRFs , the objective function is the log-loss of the model with parameters with respect to a training set D. This function is defined as the negative sum of the conditional probabilities of each training label sequence yi , given the observation sequence xi , where D f ( xi ; yi ) : i = 1 ; : : : ; mg .</sentence>
				<definiendum id="0">objective function</definiendum>
				<definiens id="0">the log-loss of the model with parameters with respect to a training set D. This function</definiens>
				<definiens id="1">the negative sum of the conditional probabilities of each training label sequence yi , given the observation sequence xi , where D f ( xi ; yi ) : i = 1 ; : : : ; mg</definiens>
			</definition>
			<definition id="6">
				<sentence>The data for this study were taken from the Switchboard Corpus ( Godfrey et al. , 1992 ) , which consists of 2430 telephone conversations between adult speakers ( approximately 2.4 million words ) .</sentence>
				<definiendum id="0">Switchboard Corpus</definiendum>
				<definiens id="0">consists of 2430 telephone conversations between adult speakers ( approximately 2.4 million words )</definiens>
			</definition>
			<definition id="7">
				<sentence>The SWDB corpus is open domain conversational speech .</sentence>
				<definiendum id="0">SWDB corpus</definiendum>
				<definiens id="0">open domain conversational speech</definiens>
			</definition>
			<definition id="8">
				<sentence>We did not test directly the probabilistic measures ( or collocation measures ) that have been used before for this task , namely information content ( IC ) ( Pan and McKeown , 1999 ) and mutual information ( Pan and Hirschberg , 2001 ) .</sentence>
				<definiendum id="0">IC )</definiendum>
				<definiens id="0">the probabilistic measures ( or collocation measures</definiens>
			</definition>
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>Part of the reason for the Relationship Definition and Example Cure TREAT cures DIS 810 ( 648 , 162 ) Intravenous immune globulin for recurrent spontaneous abortion Only DIS TREAT not mentioned 616 ( 492 , 124 ) Social ties and susceptibility to the common cold Only TREAT DIS not mentioned 166 ( 132 , 34 ) Flucticasone propionate is safe in recommended doses Prevent TREAT prevents the DIS 63 ( 50 , 13 ) Statins for prevention of stroke Vague Very unclear relationship 36 ( 28 , 8 ) Phenylbutazone and leukemia Side Effect DIS is a result of a TREAT 29 ( 24 , 5 ) Malignant mesodermal mixed tumor of the uterus following irradiation NO Cure TREAT does not cure DIS 4 ( 3 , 1 ) Evidence for double resistance to permethrin and malathion in head lice Total relevant : 1724 ( 1377 , 347 ) Irrelevant TREAT and DIS not present 1771 ( 1416 , 355 ) Patients were followed up for 6 months Total : 3495 ( 2793 , 702 ) Table 1 : Candidate semantic relationships between treatments and diseases .</sentence>
				<definiendum id="0">DIS</definiendum>
				<definiens id="0">safe in recommended doses Prevent TREAT prevents the DIS 63 ( 50 , 13 ) Statins for prevention of stroke Vague Very unclear relationship 36 ( 28 , 8 ) Phenylbutazone and leukemia Side Effect</definiens>
				<definiens id="1">a result of a TREAT 29 ( 24 , 5 ) Malignant mesodermal mixed tumor of the uterus following irradiation NO Cure</definiens>
			</definition>
			<definition id="1">
				<sentence>In Agichtein and Gravano ( 2000 ) the goal is to extract pairs such as ( Microsoft , Redmond ) , where Redmond is the location of the organization Microsoft .</sentence>
				<definiendum id="0">Redmond</definiendum>
				<definiens id="0">the location of the organization Microsoft</definiens>
			</definition>
			<definition id="2">
				<sentence>The GENIES system ( Friedman et al. , 2001 ) uses a hand-built semantic grammar along with hand-derived syntactic and semantic constraints , and recognizes a wide range of relationships between biological molecules .</sentence>
				<definiendum id="0">GENIES system</definiendum>
				<definiens id="0">Friedman et al. , 2001 ) uses a hand-built semantic grammar along with hand-derived syntactic and semantic constraints , and recognizes a wide range of relationships between biological molecules</definiens>
			</definition>
			<definition id="3">
				<sentence>Precision is a measure of how many of the roles extracted by the system are correct and recall is the measure of how many of the true roles were extracted by the system .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">a measure of how many of the roles extracted by the system are correct and recall is the measure of how many of the true roles were extracted by the system</definiens>
			</definition>
			<definition id="4">
				<sentence>The F-measure is a weighted combination of precision and recall4 .</sentence>
				<definiendum id="0">F-measure</definiendum>
			</definition>
			<definition id="5">
				<sentence>D2 encodes the fact that even when the roles are given , the observations depend on the relation .</sentence>
				<definiendum id="0">D2</definiendum>
				<definiens id="0">encodes the fact that even when the roles are given , the observations depend on the relation</definiens>
			</definition>
			<definition id="6">
				<sentence>Model D1 defines the following joint probability distribution over relations , roles , words and word features , assuming the leftmost Role node is a28a30a29a32a31a34a33a36a35 , and a37 is the number of words in the sentence : a8a38a3a22a10a13a39a41a40a20a42a43a10a13a44a21a40a45a39a47a46a21a42a49a48a50a48a50a42a43a10a13a44a36a40a22a39a47a51a52a42a54a53a56a55a43a46a57a42a57a48a50a48a58a42a54a53a41a59a60a46a41a42a54a48a50a48a50a48a58a42a54a53a56a55a61a51a62a42a54a48a50a48a50a42a54a53a41a59a60a51a9a18 a63 a8a38a3a22a10a13a39a41a40a5a18a22a8a38a3a22a10a13a44a21a40a22a39a2a46a65a64a41a10a13a39a57a40a5a18 a59 a66 a67a69a68 a55 a8a38a3a20a53 a67 a46a7a64a41a10a13a44a21a40a22a39a2a46a36a18 ( 1 ) a51 a66 a70 a68 a55 a8a38a3a22a10a65a44a36a40a22a39 a70 a64a41a10a65a44a36a40a22a39 a70a43a71 a55a49a42a43a10a13a39a57a40a5a18 a59 a66 a67a72a68 a55 a8a38a3a20a53 a67 a70 a64a41a10a65a44a36a40a22a39 a70 a18 Model D1 is similar to the model in Thompson et al. ( 2003 ) for the extraction of roles , using a different domain .</sentence>
				<definiendum id="0">a37</definiendum>
				<definiens id="0">the following joint probability distribution over relations , roles , words and word features</definiens>
				<definiens id="1">the number of words in the sentence : a8a38a3a22a10a13a39a41a40a20a42a43a10a13a44a21a40a45a39a47a46a21a42a49a48a50a48a50a42a43a10a13a44a36a40a22a39a47a51a52a42a54a53a56a55a43a46a57a42a57a48a50a48a58a42a54a53a41a59a60a46a41a42a54a48a50a48a50a48a58a42a54a53a56a55a61a51a62a42a54a48a50a48a50a42a54a53a41a59a60a51a9a18 a63 a8a38a3a22a10a13a39a41a40a5a18a22a8a38a3a22a10a13a44a21a40a22a39a2a46a65a64a41a10a13a39a57a40a5a18 a59 a66 a67a69a68 a55 a8a38a3a20a53 a67 a46a7a64a41a10a13a44a21a40a22a39a2a46a36a18 ( 1 ) a51 a66 a70 a68 a55 a8a38a3a22a10a65a44a36a40a22a39 a70 a64a41a10a65a44a36a40a22a39 a70a43a71 a55a49a42a43a10a13a39a57a40a5a18 a59 a66 a67a72a68 a55 a8a38a3a20a53 a67 a70 a64a41a10a65a44a36a40a22a39 a70 a18 Model D1 is similar to the model in Thompson</definiens>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>In this framework , we have a set of M+1 feature functions f i ( S , W ) , i = 0 , … , M. They are derived from the context model ( i.e. f 0 ( W ) ) and M class models , each for one word class , as shown in Figure 1 : For probabilistic models such as the context model or person name model , the feature functions are defined as the negative logarithm of the corresponding probabilistic models .</sentence>
				<definiendum id="0">M class models</definiendum>
				<definiens id="0">For probabilistic models such as the context model or person name model , the feature functions are defined as the negative logarithm of the corresponding probabilistic models</definiens>
			</definition>
			<definition id="1">
				<sentence>IWP is a real valued feature .</sentence>
				<definiendum id="0">IWP</definiendum>
			</definition>
			<definition id="2">
				<sentence>The IWP of a single character is the likelihood for this character to appear as an independent word in texts ( Wu and Jiang , 2000 ) : ) ( ) , ( ) ( xC WxC xIWP = .</sentence>
				<definiendum id="0">IWP of a single character</definiendum>
				<definiens id="0">the likelihood for this character to appear as an independent word in texts</definiens>
			</definition>
			<definition id="3">
				<sentence>( 4 ) where C ( x , W ) is the number of occurrences of the character x as an independent word in training data , and C ( x ) is the total number of x in training data .</sentence>
				<definiendum id="0">C ( x , W )</definiendum>
				<definiendum id="1">C ( x )</definiendum>
				<definiens id="0">the number of occurrences of the character x as an independent word in training data , and</definiens>
				<definiens id="1">the total number of x in training data</definiens>
			</definition>
			<definition id="4">
				<sentence>AWP is a binary feature derived from IWP .</sentence>
				<definiendum id="0">AWP</definiendum>
				<definiens id="0">a binary feature derived from IWP</definiens>
			</definition>
			<definition id="5">
				<sentence>WFA is a binary feature .</sentence>
				<definiendum id="0">WFA</definiendum>
			</definition>
			<definition id="6">
				<sentence>Given a character pair ( x , y ) , a character ( or a multi-character string ) z is called the common stem of ( x , y ) if at least one of the following two conditions hold : ( 1 ) character strings xz and yz are lexical words ( i.e. x and y as prefixes ) ; and ( 2 ) character strings zx and zy are lexical words ( i.e. x and y as suffixes ) .</sentence>
				<definiendum id="0">character pair</definiendum>
				<definiens id="0">y ) , a character ( or a multi-character string ) z is called the common stem of ( x , y ) if at least one of the following two conditions hold : ( 1 ) character strings xz and yz are lexical words ( i.e. x and y as prefixes ) ; and ( 2 ) character strings zx and zy are lexical words ( i.e. x and y as suffixes</definiens>
			</definition>
			<definition id="7">
				<sentence>The value of WFA for a given NW_11 candidate ab is defined as : WFA ( ab ) = 1 if there exist an affix pair ( a , x ) ( or ( b , x ) ) and the string xb ( or ax ) is a lexical word , 0 otherwise .</sentence>
				<definiendum id="0">affix pair</definiendum>
				<definiendum id="1">string xb</definiendum>
				<definiendum id="2">ax )</definiendum>
				<definiens id="0">a lexical word , 0 otherwise</definiens>
			</definition>
			<definition id="8">
				<sentence>α and β are parameters that can be fit using the observed mean λ and the observed inverse document frequency IDF as follow : N cf =λ , df N IDF log= , df dfcf IDF − =−×= 12λβ , and β λ α = , where cf is the total number of occurrence of word w i in training data , df is the number of documents in training data that w i occurs in , and N is the total number of documents .</sentence>
				<definiendum id="0">inverse document frequency IDF</definiendum>
				<definiendum id="1">cf</definiendum>
				<definiendum id="2">df</definiendum>
				<definiendum id="3">N</definiendum>
				<definiens id="0">the total number of occurrence of word w i in training data</definiens>
				<definiens id="1">the number of documents in training data that w i occurs in , and</definiens>
				<definiens id="2">the total number of documents</definiens>
			</definition>
			<definition id="9">
				<sentence>In short , our adaptive Chinese word segmenter consists of two components : ( 1 ) a generic segmenter that is capable of adapting to the vocabularies of different domains and ( 2 ) a set of output adaptors , learned from application data , for adapting to different “application-specific” standards We evaluated the proposed adaptive word segmentation system ( henceforth AWS ) using five different standards .</sentence>
				<definiendum id="0">adaptive Chinese word segmenter</definiendum>
				<definiens id="0">consists of two components : ( 1 ) a generic segmenter that is capable of adapting to the vocabularies of different domains and ( 2 ) a set of output adaptors , learned from application data , for adapting to different “application-specific” standards We evaluated the proposed adaptive word segmentation system ( henceforth AWS ) using five different standards</definiens>
			</definition>
			<definition id="10">
				<sentence>The training and test corpora of these standards are detailed in Table 1 , where MSR is defined by ourselves , and the other four are standards used in SIGHAN’s First International Chinese Word Segmentation Bakeoff ( Bakeoff test sets for brevity , see Sproat and Emperson ( 2003 ) for details ) .</sentence>
				<definiendum id="0">MSR</definiendum>
				<definiens id="0">standards used in SIGHAN’s First International Chinese Word Segmentation Bakeoff ( Bakeoff test sets for brevity</definiens>
			</definition>
			<definition id="11">
				<sentence>The NE class models , as shown in Figure 1 , were trained on the corresponding NE lists that were collected separately .</sentence>
				<definiendum id="0">NE class models</definiendum>
				<definiens id="0">shown in Figure 1 , were trained on the corresponding NE lists that were collected separately</definiens>
			</definition>
			<definition id="12">
				<sentence>The performance of word segmentation is measured through test precision ( P ) , test recall ( R ) , F score ( which is defined as 2PR/ ( P+R ) ) , the OOV rate for the test corpus ( on Bakeoff corpora , OOV is defined as the set of words in the test corpus not occurring in the training corpus . )</sentence>
				<definiendum id="0">performance of word segmentation</definiendum>
				<definiendum id="1">F score</definiendum>
				<definiendum id="2">P+R ) )</definiendum>
				<definiendum id="3">OOV rate for the test corpus</definiendum>
				<definiendum id="4">OOV</definiendum>
				<definiens id="0">measured through test precision ( P ) , test recall ( R ) ,</definiens>
				<definiens id="1">the set of words in the test corpus not occurring in the training corpus</definiens>
			</definition>
			<definition id="13">
				<sentence>We also perform NWI on Bakeoff AWS w/o NW AWS w/ NW ( post-processor ) AWS w/ NW ( unified approach ) word segmentation word segmentation NW word segmentation NW # of NW P % R % P % R % P % R % P % R % P % R % Uniform 5,682 92.6 94.5 94.7 95.2 64.1 66.8 95.1 95.5 68.1 78.4 Poisson 3,862 93.4 95.6 94.5 95.9 61.4 45.6 95.0 95.7 57.2 60.6 K-Mixture 2,915 94.7 96.4 95.1 96.2 44.1 41.5 95.6 96.2 46.2 60.4 Table 2 : NWI results on MSR test set , NWI as post-processor versus unified approach PK CTB P R F OOV Roov Riv P R F OOV Roov Riv Table 3 : Comparison scores for PK open and CTB open .</sentence>
				<definiendum id="0">NWI</definiendum>
				<definiens id="0">scores for PK open and CTB open</definiens>
			</definition>
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>We now define an a23 -induced block ( or a23 -block for short ) as a substring a21 a0a52a51a53a51a53a51 a21 a36 of a sentence in a22 a0 , such that the union over all a23 -images ( a54 a0a56a55a57a55a36 a23a33a25 a21 a36a58a27 ) forms a contiguous substring in a22 a1 , modulo the words from a23a26a25 NULLa27 .</sentence>
				<definiendum id="0">a23 -induced block</definiendum>
				<definiens id="0">a substring a21 a0a52a51a53a51a53a51 a21 a36 of a sentence in a22 a0 , such that the union over all a23 -images ( a54 a0a56a55a57a55a36 a23a33a25 a21 a36a58a27 ) forms a contiguous substring in a22 a1 , modulo the words from a23a26a25 NULLa27</definiens>
			</definition>
			<definition id="1">
				<sentence>The inside-outside algorithm reads in the weight factor array and uses it in the computation of expected rule counts .</sentence>
				<definiendum id="0">inside-outside algorithm</definiendum>
				<definiens id="0">reads in the weight factor array and uses it in the computation of expected rule counts</definiens>
			</definition>
			<definition id="2">
				<sentence>We follow an evaluation criterion that ( Klein and Manning , 2002 , footnote 3 ) discuss for the evaluation of a not fully supervised grammar induction approach based on a binary grammar topology : bracket multiplicity ( i.e. , non-branching projections ) is collapsed into a single set of brackets ( since what is relevant is the constituent structure that was induced ) .11 For comparison , we provide baseline results that a uniform left-branching structure and a uniform right-branching structure ( which encodes some nontrivial information about English syntax ) would give rise to .</sentence>
				<definiendum id="0">bracket multiplicity</definiendum>
				<definiens id="0">encodes some nontrivial information about English syntax</definiens>
			</definition>
</paper>

		<paper id="1087">
			<definition id="0">
				<sentence>ADDITIVE TEMPORAL CAUSAL and , but , whereas after , as soon as , before , ever since , now , now that , once , until , when , whenever although , because , even though , for , given that , if , if ever , in case , on condition that , on the assumption that , on the grounds that , provided that , providing that , so , so that , supposing that , though , unless Table 3 : Discourse markers used in the type experiment The data for the experiments comes from a database of sentences collected automatically from the British National Corpus and the world wide web ( Hutchinson , 2004 ) .</sentence>
				<definiendum id="0">ADDITIVE TEMPORAL CAUSAL</definiendum>
				<definiens id="0">on the assumption that , on the grounds that , provided that , providing that , so , so that , supposing that , though</definiens>
			</definition>
			<definition id="1">
				<sentence>NEG-SUBJ and NEGVERB indicated the presence of subject negation ( e.g. nothing ) or verbal negation ( e.g. n’t ) .</sentence>
				<definiendum id="0">NEG-SUBJ</definiendum>
				<definiendum id="1">NEGVERB</definiendum>
			</definition>
			<definition id="2">
				<sentence>STRUCTURAL-SKELETON identified the major constituents under the S or VP nodes , e.g. a simple double object construction gives “NP VB NP NP” .</sentence>
				<definiendum id="0">STRUCTURAL-SKELETON</definiendum>
				<definiens id="0">identified the major constituents under the S or VP nodes , e.g. a simple double object construction gives “NP VB NP NP”</definiens>
			</definition>
			<definition id="3">
				<sentence>The features and the possible values for each clause were as follows : MODALITY : one of FUTURE , ABILITY or NULL ; MOOD : one of DECL , IMP or INTERR ; PERFECT : either YES or NO ; PROGRESSIVE : either YES or NO ; TENSE : either PAST or PRESENT .</sentence>
				<definiendum id="0">TENSE</definiendum>
			</definition>
			<definition id="4">
				<sentence>a33a34a4a7a35a27a8a11a10a45a12a15a14a9a16a6a18 a20a46a21 a10a47a8a26a25a31a16a49a48a51a50a9a52 a10a24a8a26a25a27a16 a36a45a14a30a8a26a25a27a16a54a53a55a8a57a56a58a28a59a36a47a16a60a10a24a8a26a25a27a16 ( 7 ) The third metric , a61a54a62a49a63a64a63a64a65 , is a a66 -test weighted adaption of the Jaccard coefficient ( Curran and Moens , 2002 ) .</sentence>
				<definiendum id="0">adaption</definiendum>
				<definiendum id="1">Jaccard coefficient</definiendum>
				<definiens id="0">a a66 -test weighted</definiens>
			</definition>
</paper>

		<paper id="3026">
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>The rst model , which we call the Sense model , builds on the work of Diab and Resnik ( 2002 ) that uses both parallel text and a sense inventory for the target language , and recasts their approach in a probabilistic framework .</sentence>
				<definiendum id="0">rst model</definiendum>
				<definiens id="0">uses both parallel text and a sense inventory for the target language , and recasts their approach in a probabilistic framework</definiens>
			</definition>
			<definition id="1">
				<sentence>Word sense disambiguation ( WSD ) has been a central question in the computational linguistics community since its inception .</sentence>
				<definiendum id="0">Word sense disambiguation ( WSD</definiendum>
				<definiens id="0">a central question in the computational linguistics community since its inception</definiens>
			</definition>
			<definition id="2">
				<sentence>WSD is fundamental to natural language understanding and is a useful intermediate step for many other language processing tasks ( Ide and Veronis , 1998 ) .</sentence>
				<definiendum id="0">WSD</definiendum>
				<definiens id="0">fundamental to natural language understanding and is a useful intermediate step for many other language processing tasks</definiens>
			</definition>
			<definition id="3">
				<sentence>An interesting by-product of the Concept Model is a semantic structure for the secondary language .</sentence>
				<definiendum id="0">Concept Model</definiendum>
				<definiens id="0">a semantic structure for the secondary language</definiens>
			</definition>
			<definition id="4">
				<sentence>A translation pair is ( a0a2a1 , a0a2a3 ) where the subscript a4 and a5 indicate the primary language ( English ) and the secondary language ( Spanish ) .</sentence>
				<definiendum id="0">translation pair</definiendum>
				<definiens id="0">the subscript a4 and a5 indicate the primary language ( English ) and the secondary language ( Spanish )</definiens>
			</definition>
			<definition id="5">
				<sentence>The Sense Model makes the assumption , inspired by ideas in Diab and Resnik ( 2002 ) and Bengio and Kermorvant ( 2003 ) , that the English word a0a33a1 and the Spanish word a0a37a3 in a translation pair share the same precise sense .</sentence>
				<definiendum id="0">Sense Model</definiendum>
				<definiens id="0">makes the assumption , inspired by ideas in Diab</definiens>
			</definition>
			<definition id="6">
				<sentence>The rst step maximizes the expected log-likelihood of the joint probability of the observed data with the current parameter settings a46 a47 .</sentence>
				<definiendum id="0">rst step</definiendum>
			</definition>
			<definition id="7">
				<sentence>Subsequent normalization takes care of the sum constraints .</sentence>
				<definiendum id="0">Subsequent normalization</definiendum>
				<definiens id="0">takes care of the sum constraints</definiens>
			</definition>
			<definition id="8">
				<sentence>We estimate the prediction accuracy and recall of our models on Senseval data.2 In addition , the Concept Model learns a sense structure for the Spanish 2Accuracy is the ratio of the number of correct predictions and the number of attempted predictions .</sentence>
				<definiendum id="0">Concept Model</definiendum>
				<definiens id="0">the ratio of the number of correct predictions and the number of attempted predictions</definiens>
			</definition>
			<definition id="9">
				<sentence>Recall is the ratio of the number of correct predictions and the size of the test set .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">the ratio of the number of correct predictions and the size of the test set</definiens>
			</definition>
			<definition id="10">
				<sentence>The most important distinction is that the Sense Model is a probabilistic generative model for parallel corpora , where interaction between different words stemming from the same sense comes into play , even if the words are not related through translations , and this interdependence of the senses through common words plays a role in sense disambiguation .</sentence>
				<definiendum id="0">Sense Model</definiendum>
				<definiens id="0">a probabilistic generative model for parallel corpora , where interaction between different words stemming from the same sense comes into play , even if the words are not related through translations</definiens>
			</definition>
			<definition id="11">
				<sentence>The critical difference in the Concept Model can be appreciated directly from the corresponding joint probability a27a29a28 a66 a30a21a27a29a28 a40 a1a70a49 a66 a30a21a27a29a28 a10 a1a79a49 a40 a1a31a30a21a27a29a28 a40 a3a50a49 a66 a30a21a27a29a28 a10 a3a50a49 a40 a3a54a30 , where a66 is the relevant concept in the model .</sentence>
				<definiendum id="0">a66</definiendum>
				<definiens id="0">the relevant concept in the model</definiens>
			</definition>
			<definition id="12">
				<sentence>actos accidente accidentes supremas muertes ( deaths ) decisi·on decisiones casualty gobernando gobernante matar ( to kill ) matanzas ( slaughter ) muertes-le gubernamentales slaying gobernaci·on gobierno-proporciona derramamiento-de-sangre ( spilling-of-blood ) prohibir prohibiendo prohibitivo prohibitiva cachiporra ( bludgeon ) obligar ( force ) obligando ( forcing ) gubernamental gobiernos asesinato ( murder ) asesinatos linterna-el·ectrica linterna ( lantern ) man·ia craze faros-autom·ovil ( headlight ) culto ( cult ) cultos proto-senility linternas-portuarias ( harbor-light ) delirio delirium antorcha ( torch ) antorchas antorchas-pino-nudo rabias ( fury ) rabia farfulla ( do hastily ) oportunidad oportunidades diferenciaci·on ocasi·on ocasiones distinci·on distinciones riesgo ( risk ) riesgos peligro ( danger ) especializaci·on destino sino ( fate ) maestr·ia ( mastery ) fortuna suerte ( fate ) peculiaridades particularidades peculiaridades-inglesas probabilidad probabilidades especialidad especialidades diablo ( devil ) diablos modelo parang·on dickens ideal ideales heller santo ( saint ) santos san lucifer satan satan·as idol idols ·idolo deslumbra ( dazzle ) dios god dioses cromo ( chromium ) divinidad divinity meteoro meteoros meteor meteoros-blue inmortal ( immortal ) inmortales meteorito meteoritos teolog·ia teolog pedregosos ( rocky ) deidad deity deidades variaci·on variaciones minutos minuto discordancia desacuerdo ( discord ) discordancias momento momentos un-momento desviaci·on ( deviation ) desviaciones desviaciones-normales minutos momentos momento segundos discrepancia discrepancias fugaces ( eeting ) variaci·on diferencia instante momento disensi·on pesta neo ( blink ) gui na ( wink ) pesta nean adhesi·on adherencia ataduras ( tying ) pasillo ( corridor ) enlace ( connection ) ataduras aisle atadura ataduras pasarela ( footbridge ) conexi·on conexiones hall vest·ibulos conexi·on une ( to unite ) pasaje ( passage ) relaci·on conexi·on callej·on ( alley ) callejas-ciegas ( blind alley ) callejones-ocultos implicaci·on ( complicity ) envolvimiento tire set of senses for the corpus .</sentence>
				<definiendum id="0">rabia farfulla</definiendum>
				<definiendum id="1">divinidad divinity meteoro meteoros meteor meteoros-blue inmortal</definiendum>
				<definiens id="0">vest·ibulos conexi·on une ( to unite ) pasaje ( passage ) relaci·on conexi·on callej·on ( alley ) callejas-ciegas ( blind alley ) callejones-ocultos implicaci·on ( complicity ) envolvimiento tire set of senses for the corpus</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>Conceptually , convolution kernels K ( X ; Y ) enumerate all substructures occurring in X and Y and then calculate their inner product , which is simply written as : K ( X ; Y ) = h ( X ) ; ( Y ) i = X i i ( X ) i ( Y ) : ( 1 ) represents the feature mapping from the discrete object to the feature space ; that is , ( X ) = ( 1 ( X ) ; : : : ; i ( X ) ; : : : ) : With sequence kernels ( Lodhi et al. , 2002 ) , input objects X and Y are sequences , and i ( X ) is a sub-sequence .</sentence>
				<definiendum id="0">i ( X</definiendum>
				<definiens id="0">the feature mapping from the discrete object to the feature space</definiens>
				<definiens id="1">a sub-sequence</definiens>
			</definition>
			<definition id="1">
				<sentence>By using the above notations , sequence kernels can be defined as : KSK ( S ; T ) = X u2 n X iju=S [ i ] ( i ) X jju=T [ j ] ( j ) ; ( 3 ) where is the decay factor that handles the gap present in a common sub-sequence u , and ( i ) = l ( i ) juj .</sentence>
				<definiendum id="0">sequence kernels</definiendum>
				<definiens id="0">KSK ( S ; T ) = X u2 n X iju=S [ i ] ( i ) X jju=T [ j ] ( j ) ; ( 3 ) where is the decay factor that handles the gap present in a common sub-sequence u , and ( i ) = l ( i ) juj</definiens>
			</definition>
			<definition id="2">
				<sentence>Let Jm ( Si ; Tj ) be a function that returns the value of common sub-sequences if si = tj .</sentence>
				<definiendum id="0">Tj</definiendum>
				<definiens id="0">a function that returns the value of common sub-sequences if si = tj</definiens>
			</definition>
			<definition id="3">
				<sentence>Jm ( Si ; Tj ) = J0m 1 ( Si ; Tj ) I ( si ; tj ) ( 5 ) I ( si ; tj ) is a function that returns a matching value between si and tj .</sentence>
				<definiendum id="0">Jm</definiendum>
				<definiendum id="1">tj )</definiendum>
				<definiens id="0">a function that returns a matching value between si and tj</definiens>
			</definition>
			<definition id="4">
				<sentence>Then , J0m ( Si ; Tj ) and J00m ( Si ; Tj ) are introduced to calculate the common gapped sub-sequences between Si and Tj .</sentence>
				<definiendum id="0">Tj</definiendum>
			</definition>
			<definition id="5">
				<sentence>The sequence kernel with feature selection ( FSSK ) can be defined as follows : KFSSK ( S ; T ) = X 2 ( u ) ju2 n X iju=S [ i ] ( i ) X jju=T [ j ] ( j ) : ( 9 ) The difference between Equations ( 3 ) and ( 9 ) is simply the condition of the first summation .</sentence>
				<definiendum id="0">feature selection</definiendum>
				<definiendum id="1">FSSK</definiendum>
				<definiens id="0">follows : KFSSK ( S ; T ) = X 2 ( u ) ju2 n X iju=S [ i ] ( i ) X jju=T [ j ] ( j ) : ( 9 ) The difference between Equations ( 3 ) and ( 9 ) is simply the condition of the first summation</definiens>
			</definition>
			<definition id="6">
				<sentence>FSSK selects significant sub-sequence u by using the condition of the statistical metric 2 ( u ) .</sentence>
				<definiendum id="0">FSSK</definiendum>
				<definiens id="0">selects significant sub-sequence u by using the condition of the statistical metric 2 ( u )</definiens>
			</definition>
			<definition id="7">
				<sentence>First , we denote uv , which is the concatenation of sequences u and v. Then , u is a specific sequence and uv is any sequence that is constructed by u with any suffix v. The upper bound of the 2 value of uv can be defined by the value of u ( Morishita and Sese , 2000 ) .</sentence>
				<definiendum id="0">uv</definiendum>
				<definiendum id="1">uv</definiendum>
				<definiens id="0">the concatenation of sequences u and v. Then , u is a specific sequence and</definiens>
			</definition>
			<definition id="8">
				<sentence>m ( Si ; Tj ) = fu j u 2 b m ( Si ; Tj ) ; 2 ( u ) g ( 15 ) b m ( Si ; Tj ) = 8 &lt; : ( b 0m 1 ( Si ; Tj ) ; si ) if si = tj ; otherwise ( 16 ) ( F ; w ) = fuw j u 2 F ; b 2 ( uw ) g ; ( 17 ) where F represents a set of sub-sequences .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">a set of sub-sequences</definiens>
			</definition>
			<definition id="9">
				<sentence>Notice that m ( Si ; Tj ) and b m ( Si ; Tj ) have only subsequences u that satisfy 2 ( uw ) or b 2 ( uw ) , respectively , if si = tj ( = w ) ; otherwise they become empty sets .</sentence>
				<definiendum id="0">Tj</definiendum>
				<definiens id="0">uw ) , respectively , if si = tj ( = w ) ; otherwise they become empty sets</definiens>
			</definition>
			<definition id="10">
				<sentence>Support Vector Machine ( SVM ) was selected as the kernel-based classifier for training and classification .</sentence>
				<definiendum id="0">Support Vector Machine ( SVM</definiendum>
				<definiens id="0">the kernel-based classifier for training and classification</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>Formally , a kernel function K is a mapping K : X × X → [ 0 , ∞ ] from instance space X to a similarity score K ( x , y ) = summationtexti φi ( x ) φi ( y ) = φ ( x ) · φ ( y ) .</sentence>
				<definiendum id="0">kernel function K</definiendum>
				<definiens id="0">a mapping K</definiens>
			</definition>
			<definition id="1">
				<sentence>A simple kernel function takes the dot product of the vector representation of instances being compared .</sentence>
				<definiendum id="0">simple kernel function</definiendum>
				<definiens id="0">takes the dot product of the vector representation of instances being compared</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus , φi ( x ) is the number of times string x contains the subsequence referenced by i. These matches can be found efficiently through a dynamic program , allowing string kernels to examine long-range features that would be computationally infeasible in a feature-based method .</sentence>
				<definiendum id="0">φi ( x )</definiendum>
				<definiens id="0">the number of times string x contains the subsequence referenced by i. These matches can be found efficiently through a dynamic program , allowing string kernels to examine long-range features that would be computationally infeasible in a feature-based method</definiens>
			</definition>
			<definition id="3">
				<sentence>A support vector machine ( SVM ) is a type of classifier that formulates the task of finding the separating hyperplane as the solution to a quadratic programming problem ( Cristianini and Shawe-Taylor , 2000 ) .</sentence>
				<definiendum id="0">support vector machine</definiendum>
				<definiendum id="1">SVM</definiendum>
				<definiens id="0">a type of classifier that formulates the task of finding the separating hyperplane as the solution to a quadratic programming problem</definiens>
			</definition>
			<definition id="4">
				<sentence>A dependency tree is a representation that denotes grammatical relations between words in a sentence ( Figure 1 ) .</sentence>
				<definiendum id="0">dependency tree</definiendum>
			</definition>
			<definition id="5">
				<sentence>Feature Example word troops , Tikrit part-of-speech ( 24 values ) NN , NNP general-pos ( 5 values ) noun , verb , adj chunk-tag NP , VP , ADJP entity-type person , geo-political-entity entity-level name , nominal , pronoun Wordnet hypernyms social group , city relation-argument ARG A , ARG B Table 3 : List of features assigned to each node in the dependency tree .</sentence>
				<definiendum id="0">Tikrit part-of-speech</definiendum>
				<definiens id="0">List of features assigned to each node in the dependency tree</definiens>
			</definition>
			<definition id="6">
				<sentence>The tree kernel is a function K ( T1 , T2 ) that returns a normalized , symmetric similarity score in the range ( 0,1 ) for two trees T1 and T2 .</sentence>
				<definiendum id="0">tree kernel</definiendum>
			</definition>
			<definition id="7">
				<sentence>For two dependency trees T1 , T2 , with root nodes r1 and r2 , we define the tree kernel K ( T1 , T2 ) as follows : K ( T1 , T2 ) =    0 if m ( r1 , r2 ) = 0 s ( r1 , r2 ) + Kc ( r1 [ c ] , r2 [ c ] ) otherwise where Kc is a kernel function over children .</sentence>
				<definiendum id="0">T2 )</definiendum>
				<definiendum id="1">K</definiendum>
				<definiendum id="2">Kc</definiendum>
				<definiens id="0">a kernel function over children</definiens>
			</definition>
			<definition id="8">
				<sentence>A matching subsequence of children is a sequence of children a and b such that m ( ai , bi ) = 1 ( ∀i &lt; n ) .</sentence>
				<definiendum id="0">matching subsequence of children</definiendum>
			</definition>
			<definition id="9">
				<sentence>( B ) denotes binary classification .</sentence>
				<definiendum id="0">B )</definiendum>
				<definiens id="0">binary classification</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>Nigam studied an Expected Maximization ( EM ) technique for combining labeled and unlabeled data for text categorization in his dissertation .</sentence>
				<definiendum id="0">Nigam</definiendum>
				<definiens id="0">studied an Expected Maximization ( EM ) technique for combining labeled and unlabeled data for text categorization in his dissertation</definiens>
			</definition>
			<definition id="1">
				<sentence>The score of semantic similarity between a title word , T , and a word , W , is calculated by the cosine metric as follows : ∑∑ ∑ == = × × = n i i n i i n i ii wt wt WTsim 1 2 1 2 1 ) , ( ( 1 ) where t i and w i represent the occurrence ( binary value : 0 or 1 ) of words T and W in i-th document respectively , and n is the total number of documents in the collected documents .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">binary value : 0 or 1 ) of words T and W in i-th document respectively , and</definiens>
				<definiens id="1">the total number of documents in the collected documents</definiens>
			</definition>
			<definition id="2">
				<sentence>Since the keywords for text categorization must have the power to discriminate categories as well as similarity with the title words , we assign a word to the keyword list of a category with the maximum similarity score and recalculate the score of the word in the category using the following formula : ) ) , ( ) , ( ( ) , ( ) , ( maxsecmaxmaxmax WTsimWTsimWTsimcWScore ond −+= ( 2 ) where T max is the title word with the maximum similarity score with a word W , c max is the category of the title word T max , and T secondmax is other title word with the second high similarity score with the word W. This formula means that a word with high ranking in a category has a high similarity score with the title word of the category and a high similarity score difference with other title words .</sentence>
				<definiendum id="0">c max</definiendum>
				<definiendum id="1">T secondmax</definiendum>
				<definiens id="0">the power to discriminate categories as well as similarity with the title words , we assign a word to the keyword list of a category with the maximum similarity score and recalculate the score of the word in the category using the following formula : ) ) , ( ) , ( ( ) , ( ) , ( maxsecmaxmaxmax WTsimWTsimWTsimcWScore ond −+= ( 2 ) where T max is the title word with the maximum similarity score with a word W</definiens>
			</definition>
			<definition id="3">
				<sentence>The list of keywords in the WebKB data set Category Title Word Keywords course course assignments , hours , instructor , class , fall faculty professor associate , ph.d , fax , interests , publications project project system , systems , research , software , information student student graduate , computer , science , page , university We choose contexts with a keyword or a title word of a category as centroid-contexts .</sentence>
				<definiendum id="0">list of keywords</definiendum>
				<definiens id="0">in the WebKB data set Category Title Word Keywords course course assignments , hours , instructor , class , fall faculty professor associate , ph.d , fax , interests , publications project project system , systems , research , software , information student student graduate , computer , science , page</definiens>
			</definition>
			<definition id="4">
				<sentence>First of all , weights ( W ij ) of word w i in j-th category are calculated using Term Frequency ( TF ) within a category and Inverse Category Frequency ( ICF ) ( Cho and Kim , 1997 ) as follows : ) ) log ( ) ( log ( iijiijij CFMTFICFTFW −×=×= ( 3 ) where CF i is the number of categories that contain w i and M is the total number of categories .</sentence>
				<definiendum id="0">CF i</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">the total number of categories</definiens>
			</definition>
			<definition id="5">
				<sentence>Using word weights ( W ij ) calculated by formula 3 , the score of a centroid-context ( S k ) in j-th category ( c j ) is computed as follows : N WWW cSScore Njjj jk +++ = ... ) , ( 21 ( 4 ) where N is the number of words in the centroidcontext .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">W ij ) calculated by formula 3 , the score of a centroid-context ( S k</definiens>
				<definiens id="1">the number of words in the centroidcontext</definiens>
			</definition>
			<definition id="6">
				<sentence>Affinity formulae are defined as follows ( Karov and Edelman , 1998 ) : ) , ( max ) , ( inXWn WWsimXWaff i ∈ = ( 5 ) ( 6 ) ) , ( max ) , ( jnXWn XXsimWXaff j ∈ = In the above formulae , n denotes the iteration number , and the similarity values are defined by WSM n and CSM n .</sentence>
				<definiendum id="0">Affinity formulae</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">follows ( Karov and Edelman , 1998 ) : ) , ( max ) , ( inXWn WWsimXWaff i ∈ = ( 5 ) ( 6 ) ) , ( max ) , ( jnXWn XXsimWXaff j ∈ = In the above formulae</definiens>
				<definiens id="1">the iteration number , and the similarity values are defined by WSM n and CSM n</definiens>
			</definition>
			<definition id="7">
				<sentence>Similarity formulae are defined as follows : ) , ( ) , ( ) , ( 21211 1 XWaffXWweightXXsim n XW n ⋅= ∑ ∈ + ( 7 ) ( 8 ) ) , ( ) , ( ) , ( 1 ) , ( 21211 211 21 1 WXaffWXweightWWsim else WWsim WWif n XW n n ⋅= = = ∑ ∈ + + The weights in formula 7 are computed as reflecting global frequency , log-likelihood factors , and part of speech as used in ( Karov and Edelman , 1998 ) .</sentence>
				<definiendum id="0">Similarity formulae</definiendum>
			</definition>
			<definition id="8">
				<sentence>The voting ratio of each category c j in a feature t m is calculated by the following formula : ∑∑ ∈∈ ⋅= mmmm jj Ilt lm Ilt mlmm dtwltcydtwtcr ) ( ) ( ) , ( ) ) ( , ( ) , ( ) , ( rr ( 14 ) In formula 14 , w ) , ( dt m r is the weight of term t m in document d , I m denotes a set of elements selected for voting and is a function ; if the category for an element t is equal to c , the output value is 1 .</sentence>
				<definiendum id="0">dt m r</definiendum>
				<definiens id="0">voting ratio of each category c j in a feature t m is calculated by the following formula : ∑∑ ∈∈ ⋅= mmmm jj Ilt lm Ilt mlmm dtwltcydtwtcr ) ( ) ( ) , ( ) ) ( , ( ) , ( )</definiens>
				<definiens id="1">the weight of term t m in document d , I m denotes a set of elements selected for voting and is a function</definiens>
			</definition>
			<definition id="9">
				<sentence>Finally , the voting score of each category c in the m-th feature t j m of a test document d is calculated by the following formula : ) ) ( 1log ( ) , ( ) , ( ) , ( 2 mmmm ttcrdttwtcvs jj χ+⋅⋅= r ( 15 ) where tw ( t m , d ) denotes a modified term weight by the co-occurrence frequency and denotes the calculated χ ) ( 2 m tχ m 2 statistics value of .</sentence>
				<definiendum id="0">voting score</definiendum>
				<definiens id="0">a modified term weight by the co-occurrence frequency and denotes the calculated χ</definiens>
			</definition>
</paper>

		<paper id="3019">
			<definition id="0">
				<sentence>Collocations are a phenomenon of word combination occurring together relatively often .</sentence>
				<definiendum id="0">Collocations</definiendum>
				<definiens id="0">a phenomenon of word combination occurring together relatively often</definiens>
			</definition>
			<definition id="1">
				<sentence>Smadja ( 1993 ) also detailed techniques for collocation extraction and developed a program called XTRACT , which is capable of computing flexible collocations based on elaborated statistical calculation .</sentence>
				<definiendum id="0">XTRACT</definiendum>
				<definiens id="0">capable of computing flexible collocations based on elaborated statistical calculation</definiens>
			</definition>
			<definition id="2">
				<sentence>TANGO is a concordancer capable of answering users’ queries on collocation use .</sentence>
				<definiendum id="0">TANGO</definiendum>
				<definiens id="0">a concordancer capable of answering users’ queries on collocation use</definiens>
			</definition>
			<definition id="3">
				<sentence>Currently , TANGO supports two text collections : a monolingual corpus ( BNC ) and a bilingual corpus ( SPC ) .</sentence>
				<definiendum id="0">SPC</definiendum>
				<definiens id="0">a monolingual corpus ( BNC ) and a bilingual corpus (</definiens>
			</definition>
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>In this configuration , the CP is the referent of the first NP within the unit that is annotated as a subject for its gf.3 Example ( 2 ) shows the relevant annotation features of unit u210 which corresponds to utterance ( a ) in example ( 1 ) .</sentence>
				<definiendum id="0">CP</definiendum>
			</definition>
			<definition id="1">
				<sentence>The second column of Table 1 shows how the utterances in example ( 1 ) are automatically translated by the scripts developed by Poesio et al. ( 2004 ) into a 1For example , one could equate “utterance” with sentence ( Strube and Hahn , 1999 ; Miltsakaki , 2002 ) , use indirect realisation for the computation of the CF list ( Grosz et al. , 1995 ) , rank the CFs according to their information status ( Strube and Hahn , 1999 ) , etc. 2Our definition includes titles which are not always finite units , but excludes finite relative clauses , the second element of coordinated VPs and clause complements which are often taken as not having their own CF lists in the literature .</sentence>
				<definiendum id="0">CF list</definiendum>
				<definiens id="0">includes titles which are not always finite units , but excludes finite relative clauses , the second element of coordinated VPs and clause complements which are often taken as not having their own CF lists in the literature</definiens>
			</definition>
			<definition id="2">
				<sentence>The first step of seec is to search through the space of possible orderings defined by the permutations of the CF lists that B consists of , and to divide the explored search space into sets of orderings that score better , equal , or worse than B according to M. Then , the classification rate is defined according to the following generation scenario .</sentence>
				<definiendum id="0">seec</definiendum>
				<definiendum id="1">classification rate</definiendum>
				<definiens id="0">to search through the space of possible orderings defined by the permutations of the CF lists that B consists of , and to divide the explored search space into sets of orderings that score better , equal , or worse than B according to M. Then , the</definiens>
			</definition>
			<definition id="3">
				<sentence>Hence , half of the orderings with the same score will have better chances than B to be selected by M. The classification rate υ of a metric M on B expresses the expected percentage of orderings with a higher probability of being generated than B according to the scores assigned by M and the additional biases assumed by the generation scenario as follows : ( 3 ) Classification rate : υ ( M , B ) = Better ( M ) + Equal ( M ) 2 Better ( M ) stands for the percentage of orderings that score better than B according to M , whilst Equal ( M ) is the percentage of orderings that score equal to B according to M. If υ ( Mx , B ) is the classification rate of Mx on B , and υ ( My , B ) is the classification rate of My on B , My is a more suitable candidate than Mx for generating B if υ ( My , B ) is smaller than υ ( Mx , B ) .</sentence>
				<definiendum id="0">whilst Equal ( M )</definiendum>
				<definiendum id="1">B )</definiendum>
				<definiendum id="2">B )</definiendum>
				<definiendum id="3">My</definiendum>
				<definiens id="0">the percentage of orderings that score equal to B according to M. If υ ( Mx ,</definiens>
			</definition>
			<definition id="4">
				<sentence>In order to restrict the scope of the experiment to the text-type most relevant to our study , we selected 20 “museum labels” , i.e. , short texts that describe a concrete artefact , which served as the input to seec together with the metrics in section 3.10 In specifying the performance of the metrics we made use of a simple permutation heuristic exploiting a piece of domain-specific communication knowledge ( Kittredge et al. , 1991 ) .</sentence>
				<definiendum id="0">concrete artefact</definiendum>
			</definition>
			<definition id="5">
				<sentence>Hence , M.NOCB is the most suitable among the investigated metrics for structuring the CF lists in gnome .</sentence>
				<definiendum id="0">M.NOCB</definiendum>
				<definiens id="0">the most suitable among the investigated metrics for structuring the CF lists in gnome</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>A language game is a routinised interaction between two agents about a shared situation in the world that involves the exchange of symbols .</sentence>
				<definiendum id="0">language game</definiendum>
				<definiens id="0">a routinised interaction between two agents about a shared situation in the world that involves the exchange of symbols</definiens>
			</definition>
			<definition id="1">
				<sentence>A semantic structure consists of a set of units where each unit has a referent , which is the object or event to which the unit draws attention , and a meaning , which is a set of clauses constraining the referent .</sentence>
				<definiendum id="0">semantic structure</definiendum>
				<definiens id="0">consists of a set of units where each unit has a referent , which is the object or event to which the unit draws attention , and a meaning , which is a set of clauses constraining the referent</definiens>
			</definition>
			<definition id="2">
				<sentence>The interpretation process matches these variables against the facts in the world model .</sentence>
				<definiendum id="0">interpretation process</definiendum>
				<definiens id="0">matches these variables against the facts in the world model</definiens>
			</definition>
			<definition id="3">
				<sentence>A syntactic category ( such as noun , verb , nominative ) is a categorisation of a word or a group of words , which can be used to constrain the syntactic side of grammatical rules .</sentence>
				<definiendum id="0">syntactic category</definiendum>
				<definiens id="0">a categorisation of a word or a group of words</definiens>
			</definition>
</paper>

		<paper id="3014">
			<definition id="0">
				<sentence>Word-level alignment is a key infrastructural technology for multilingual processing .</sentence>
				<definiendum id="0">Word-level alignment</definiendum>
			</definition>
			<definition id="1">
				<sentence>We start by running the Collins parser ( Collins , 1999 ) on the English side of both training and testing data , and apply our source-language-specific heuristics to the Language VP AP NP English VO AO AN , NR Hindi OV OA AN , RN Korean OV OA AN , RN Chinese VO AOA AN , RN Romanian VO AO NA , NR Table 1 : Basic word order for three major phrase types – VP : verb phrases with Verb and Object , AP : appositional ( prepositional or postpositional ) phrases with Apposition and Object , and NP : noun phrases withNoun andAdjective orRelative clause .</sentence>
				<definiendum id="0">AP</definiendum>
				<definiens id="0">Basic word order for three major phrase types – VP : verb phrases with Verb and Object</definiens>
				<definiens id="1">appositional ( prepositional or postpositional ) phrases with Apposition and Object , and NP : noun phrases withNoun andAdjective orRelative clause</definiens>
			</definition>
			<definition id="2">
				<sentence>This is a plausible low-resource language scenario 25 30 35 40 45 50 55 3 3.5 4 4.5 5 F-measure log ( number of training sentences ) E’ Method Direct Figure 6 : Chinese alignment performance 35 40 45 50 55 60 65 70 75 3 3.2 3.4 3.6 3.8 4 4.2 4.4 4.6 F-measure log ( number of training sentences ) E’ Method Direct Figure 7 : Romanian alignment performance # Train Direct Englishprime Sents P R F P R F Hindi Dict only 16.4 13.8 15.0 18.5 15.6 17.0 1000 26.8 23.0 24.8 28.4 24.4 26.2 3162 35.7 31.6 33.5 38.4 33.5 35.8 10000 46.6 42.7 44.6 50.4 45.2 47.6 31622 60.1 56.0 58.0 63.6 58.5 61.0 63095 64.7 61.7 63.2 66.3 62.2 64.2 Korean Dict only 26.6 12.3 16.9 27.5 12.9 17.6 1000 9.4 7.3 8.2 11.3 8.7 9.8 3162 13.2 10.2 11.5 16.0 12.4 14.0 10000 15.2 12.0 13.4 17.0 13.3 14.9 30199 21.5 16.9 18.9 21.9 17.2 19.3 Chinese Dict only 44.4 30.4 36.1 44.5 30.5 36.2 1000 33.0 22.2 26.5 30.8 22.6 26.1 3162 44.6 28.9 35.1 41.7 30.0 34.9 10000 51.1 34.0 40.8 50.7 35.8 42.0 31622 60.4 39.0 47.4 55.7 39.7 46.4 100000 66.0 43.7 52.6 63.7 45.4 53.0 Romanian 1000 49.6 27.7 35.6 50.1 28.0 35.9 3162 57.9 33.4 42.4 57.6 33.0 42.0 10000 72.6 45.5 55.9 71.3 45.0 55.2 48441 84.7 57.8 68.7 83.5 57.1 67.8 Table 2 : Performance in Precision , Recall , and Fmeasure ( per cent ) of all systems .</sentence>
				<definiendum id="0">Fmeasure</definiendum>
				<definiens id="0">Romanian alignment performance # Train Direct Englishprime Sents P R F P R F Hindi</definiens>
			</definition>
</paper>

		<paper id="1083">
			<definition id="0">
				<sentence>ago ( Aho &amp; Ullman , 1969 ) , but here we articulate it in more detail : ordinary parsing is a special case of synchronous parsing , which is a special case of translation .</sentence>
				<definiendum id="0">synchronous parsing</definiendum>
				<definiens id="0">a special case of translation</definiens>
			</definition>
			<definition id="1">
				<sentence>The vehicle for the present guided tour shall be multitext grammar ( MTG ) , which is a generalization of context-free grammar to the synchronous case ( Melamed , 2003 ) .</sentence>
				<definiendum id="0">multitext grammar</definiendum>
				<definiendum id="1">MTG</definiendum>
			</definition>
			<definition id="2">
				<sentence>Such specifications are nondetera7a9a8a11a10a13a12a15a14a17a16a19a18 a8a11a10a16a11a14a20a12a20a18a22a21 a7a24a23 a23 a21 a7a24a25a27a26a29a28a31a30 a32a9a33 a21 a7 Wash a32a9a33 a21 a7 a32a9a33 a34a27a35a37a36 a21 a7 a32a9a33 moya21 a7a39a38a41a40 a10a13a12a15a14a42a16a43a18 a38a41a40 a21 a7a39a44 a32a9a33 a21 a7 the a32a9a33 a21 a7a39a38 a38a45a21 a7a39a44 a35 a28a46a30 a32a9a33 a21 a7 dishes a32a9a33 a21 a7 a32a9a33 a40a47a26a29a28a48a21 a7 a32a9a33 Pasudua21 Figure 2 : Above : A tree generated by a 2-MTG in English and ( transliterated ) Russian .</sentence>
				<definiendum id="0">Above</definiendum>
			</definition>
			<definition id="3">
				<sentence>Parser C begins with an empty chart .</sentence>
				<definiendum id="0">Parser C</definiendum>
				<definiens id="0">begins with an empty chart</definiens>
			</definition>
			<definition id="4">
				<sentence>Composition proceeds as before , except that there are no constraints on the role templates in the output dimensions – the role templates in a28 a41 a48 a49 a51 are free variables .</sentence>
				<definiendum id="0">Composition proceeds</definiendum>
				<definiens id="0">the output dimensions – the role templates in a28 a41 a48 a49 a51 are free variables</definiens>
			</definition>
			<definition id="5">
				<sentence>Under the various derivation semirings ( Goodman , 1999 ) , Translator CT can store the output role templates a28 a41 a48 a49 a51 in each internal node of the tree .</sentence>
				<definiendum id="0">Translator CT</definiendum>
				<definiens id="0">store the output role templates a28 a41 a48 a49 a51 in each internal node of the tree</definiens>
			</definition>
			<definition id="6">
				<sentence>Multitree-based statistical machine translation ( MTSMT ) is an architecture for SMT that revolves around multitrees .</sentence>
				<definiendum id="0">MTSMT</definiendum>
				<definiens id="0">an architecture for SMT that revolves around multitrees</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>Klein and Manning ( 2002 ) argue that these results show a pattern where discriminative probability models are inferior to generative probability models , but that improvements can be achieved by keeping a generative probability model and training according to a discriminative optimization criteria .</sentence>
				<definiendum id="0">discriminative probability models</definiendum>
				<definiens id="0">inferior to generative probability models , but that improvements can be achieved by keeping a generative probability model and training according to a discriminative optimization criteria</definiens>
			</definition>
			<definition id="1">
				<sentence>As with the generative model , we use the chain rule to decompose the entire conditional probability into a sequence of probabilities for individual parser decisions , where yield ( dj ; : : : ; dk ) is the sequence of words wi from the shift ( wi ) actions in dj ; : : : ; dk .</sentence>
				<definiendum id="0">yield</definiendum>
				<definiendum id="1">dk )</definiendum>
				<definiens id="0">the sequence of words wi from the shift ( wi ) actions in dj ; : : : ; dk</definiens>
			</definition>
			<definition id="2">
				<sentence>Once it has computed h ( d1 ; : : : ; di 1 ) and ( for the discriminative model ) l ( yield ( di ; : : : ; dm ) ) , the SSN uses standard methods ( Bishop , 1995 ) to estimate a probability distribution over the set of possible next decisions di given these representations .</sentence>
				<definiendum id="0">SSN</definiendum>
				<definiens id="0">uses standard methods ( Bishop , 1995 ) to estimate a probability distribution over the set of possible next decisions di given these representations</definiens>
			</definition>
			<definition id="3">
				<sentence>LR LP F =1 DSSN-Freq 5 84.9 86.0 85.5 GSSN-Freq 200 87.6 88.9 88.2 DGSSN-Freq 200 87.8 88.8 88.3 GSSN-Freq 20 88.2 89.3 88.8 DGSSN-Freq 200 , rerank 88.5 89.6 89.0 DGSSN-Freq 20 88.5 89.7 89.1 DGSSN-Freq 20 , rerank 89.0 90.3 89.6 Table 1 : Percentage labeled constituent recall ( LR ) , precision ( LP ) , and a combination of both ( F =1 ) on validation set sentences of length at most 100 .</sentence>
				<definiendum id="0">precision</definiendum>
				<definiens id="0">on validation set sentences of length at most 100</definiens>
			</definition>
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>A topic signature is defined as : TS = f ( t1 ; w1 ) ; : : : ; ( ti ; wi ) ; : : : g , where ti is a term highly correlated to a target topic ( or concept ) with association weight wi , which can be omitted .</sentence>
				<definiendum id="0">topic signature</definiendum>
				<definiendum id="1">ti</definiendum>
				<definiens id="0">a term highly correlated to a target topic ( or concept ) with association weight wi , which can be omitted</definiens>
			</definition>
			<definition id="1">
				<sentence>At the end of this step , we have produced a set C , which consists of Chinese words fc1 ; c2 ; : : : ; cng , where ci is the translation corresponding to sense si of w , and n is the number of senses that w has .</sentence>
				<definiendum id="0">ci</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the translation corresponding to sense si of w , and</definiens>
				<definiens id="1">the number of senses that w has</definiens>
			</definition>
			<definition id="2">
				<sentence>For efficiency purposes , we extract our topic signatures mainly from the Mandarin portion of the Chinese Gigaword Corpus ( CGC ) , produced by the LDC3 , which contains 1:3GB of newswire text drawn from Xinhua newspaper .</sentence>
				<definiendum id="0">Chinese Gigaword Corpus</definiendum>
				<definiens id="0">produced by the LDC3 , which contains 1:3GB of newswire text drawn from Xinhua newspaper</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus , the Concept Space was formed by collecting a n-by-2 ; 500 matrix M , such that element mij records the number of times that concept i and j co-occur in a window , where n is the number of concept vectors that occur in the corpus .</sentence>
				<definiendum id="0">Concept Space</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the number of concept vectors that occur in the corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>Row l of matrix M represents concept vector l. We measure the similarity of two vectors by the cosine score : corr ( ~v ; ~w ) = PN i=1 ~vi ~wiq PN i=1 ~vi 2PN i=1 ~wi 2 where ~v and ~w are vectors and N is the dimension of the vector space .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the dimension of the vector space</definiens>
			</definition>
			<definition id="5">
				<sentence>A context vector is the sum of the vectors of concepts that occur in a context window .</sentence>
				<definiendum id="0">context vector</definiendum>
				<definiens id="0">the sum of the vectors of concepts that occur in a context window</definiens>
			</definition>
			<definition id="6">
				<sentence>A vector of sense si is the sum of the vectors of contexts in which the ambiguous word realises si .</sentence>
				<definiendum id="0">vector of sense si</definiendum>
				<definiens id="0">the sum of the vectors of contexts in which the ambiguous word realises si</definiens>
			</definition>
			<definition id="7">
				<sentence>For example , a Chinese string ABC may be segmented as either A+BC or AB +C ; assuming the former is correct whereas AB + C was produced by the segmenter , distributions of words A , AB , BC , and C are all affected accordingly .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">correct whereas AB + C was produced by the segmenter , distributions of words A , AB , BC , and</definiens>
			</definition>
			<definition id="8">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="2011">
			<definition id="0">
				<sentence>The Hidden Markov Model ( HMM ) used for partof-speech ( POS ) tagging is usually a second-order model , using tag trigrams , implementing the idea that a limited number of preceding tags provide a considerable amount of information on the identity of the current tag .</sentence>
				<definiendum id="0">Hidden Markov Model</definiendum>
				<definiendum id="1">HMM</definiendum>
				<definiens id="0">a considerable amount of information on the identity of the current tag</definiens>
			</definition>
			<definition id="1">
				<sentence>The diversity of an n-gram is the number of different tags that appear in the position following this n-gram in the training data .</sentence>
				<definiendum id="0">diversity of an n-gram</definiendum>
				<definiens id="0">the number of different tags that appear in the position following this n-gram in the training data</definiens>
			</definition>
			<definition id="2">
				<sentence>Test data is a collection of 3686 sentences ( 59K words ) from the Parool newspaper .</sentence>
				<definiendum id="0">Test data</definiendum>
				<definiens id="0">a collection of 3686 sentences ( 59K words</definiens>
			</definition>
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>A context vector for each NE pair consists of the bag of words formed from all intervening words from all co-occurrences of two named entities .</sentence>
				<definiendum id="0">context vector for each NE pair</definiendum>
				<definiens id="0">consists of the bag of words formed from all intervening words from all co-occurrences of two named entities</definiens>
			</definition>
			<definition id="1">
				<sentence>Term frequency is the number of occurrences of a word in the collected context words .</sentence>
				<definiendum id="0">Term frequency</definiendum>
				<definiens id="0">the number of occurrences of a word in the collected context words</definiens>
			</definition>
			<definition id="2">
				<sentence>If a word a11a13a12 occurred a14 times in context a0a9a1a4a3a5a3a5a3a6a0a4a7 and a15 times in context a0a16a7a2a3a5a3a5a3a6a0a2a1 , the term frequency a0a2a1a2a12 of the word a11 a12 is defined as a14a4a3 a15 , where a0a9a1 and a0a8a7 are named entities .</sentence>
				<definiendum id="0">term frequency a0a2a1a2a12</definiendum>
				<definiens id="0">If a word a11a13a12 occurred a14 times in context a0a9a1a4a3a5a3a5a3a6a0a4a7 and a15 times in context a0a16a7a2a3a5a3a5a3a6a0a2a1</definiens>
			</definition>
			<definition id="3">
				<sentence>Document frequency is the number of documents which include the word .</sentence>
				<definiendum id="0">Document frequency</definiendum>
				<definiens id="0">the number of documents which include the word</definiens>
			</definition>
			<definition id="4">
				<sentence>Recall is defined as follows : a4 a25 a33 a34a16a36a42a38a42a38a41a40a42a34a44a43 a33a5a0 a40a3a2 Precision ( P ) How many correct pairs are detected among the pairs clustered automatically ?</sentence>
				<definiendum id="0">Recall</definiendum>
			</definition>
			<definition id="5">
				<sentence>Precision is defined as follows : a6 a25 a33 a34a16a36a42a38a42a38a41a40a42a34a44a43 a33 a34a16a36a42a38a2a38a41a40a39a34a44a43a8a7a4a33 a12a46a45a47a34a37a36a39a38a2a38a41a40a42a34a44a43 F-measure ( F ) F-measure is defined as a combination of recall and precision according to the following formula : a9 a25 a10 a4a11a6 a4 a7 a6 These values vary depending on the threshold of cosine similarity .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">follows : a6 a25 a33 a34a16a36a42a38a42a38a41a40a42a34a44a43 a33 a34a16a36a42a38a2a38a41a40a39a34a44a43a8a7a4a33 a12a46a45a47a34a37a36a39a38a2a38a41a40a42a34a44a43 F-measure ( F ) F-measure is defined as a combination of recall and precision according to the following formula : a9 a25 a10 a4a11a6 a4 a7 a6 These values vary depending on the threshold of cosine similarity</definiens>
			</definition>
			<definition id="6">
				<sentence>The relative frequency for a word is the number of such links , relative to the maximal possible number of links ( a33 a17 a33 a3 a32 a21a1a0 a10 for a cluster of a33 pairs ) .</sentence>
				<definiendum id="0">relative frequency for a word</definiendum>
				<definiens id="0">the number of such links , relative to the maximal possible number of links ( a33 a17 a33 a3 a32 a21a1a0 a10 for a cluster of a33 pairs )</definiens>
			</definition>
			<definition id="7">
				<sentence>This research was supported in part by the Defense Advanced Research Projects Agency as part of the Translingual Information Detection , Extraction and Summarization ( TIDES ) program , under Grant N66001-001-1-8917 from the Space and Naval Warfare Systems Center , San Diego , and by the National Science Foundation under Grant ITS00325657 .</sentence>
				<definiendum id="0">Extraction</definiendum>
				<definiens id="0">supported in part by the Defense Advanced Research Projects Agency as part of the Translingual Information Detection</definiens>
			</definition>
</paper>

		<paper id="1068">
			<definition id="0">
				<sentence>Compilation of translation lexicons is a crucial process for machine translation ( MT ) ( Brown et al. , 1990 ) and cross-language information retrieval ( CLIR ) systems ( Nie et al. , 1999 ) .</sentence>
				<definiendum id="0">CLIR ) systems</definiendum>
				<definiens id="0">a crucial process for machine translation ( MT ) ( Brown et al. , 1990 ) and cross-language information retrieval</definiens>
			</definition>
			<definition id="1">
				<sentence>The correct translation t can then be extracted if it can be found as a translation of m. The transitive translation model , therefore , combines the processes of both direct translation and indirect translation , and is defined as : �� ��� ��= &gt; = � `` otherwise ) , ( ) , ( ) , ( ) , ( ) , ( if ) , , ( ) , ( mtmSmsStsS tsStsS tsS directdirectindirect directdirect m trans v q where m is one of the top k most probable intermediate translations of s in language lm , and v is the confidence value of m’s accuracy , which can be estimated based on m’s probability of occurring in the corpus , and q is a predefined threshold value .</sentence>
				<definiendum id="0">m</definiendum>
				<definiendum id="1">v</definiendum>
				<definiendum id="2">q</definiendum>
				<definiens id="0">a translation of m. The transitive translation model</definiens>
				<definiens id="1">one of the top k most probable intermediate translations of s in language lm , and</definiens>
				<definiens id="2">the confidence value of m’s accuracy , which can be estimated based on m’s probability of occurring in the corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>In this section , we will describe the details of the direct translation process , i.e. the way to compute Sdirect ( s , t ) .</sentence>
				<definiendum id="0">translation process</definiendum>
				<definiens id="0">the way to compute Sdirect ( s , t )</definiens>
			</definition>
			<definition id="3">
				<sentence>The Chi-square Method : A number of statistical measures have been proposed for estimating term association based on co-occurrence analysis , including mutual information , DICE coefficient , chi-square test , and log-likelihood ratio ( Rapp , 1999 ) .</sentence>
				<definiendum id="0">Chi-square Method</definiendum>
				<definiens id="0">A number of statistical measures have been proposed for estimating term association based on co-occurrence analysis , including mutual information , DICE coefficient , chi-square test , and log-likelihood ratio</definiens>
			</definition>
			<definition id="4">
				<sentence>Chisquare test ( χ2 ) is adopted in our study because the required parameters for it can be obtained by submitInternet Technology 網際網路 ( Internet ) 技術 ( Technology ) 瀏覽器 ( Browser ) 電腦 ( Computer ) 資訊 ( Information ) t1 t2 s2 eij s3 s4 s5 s1 ting Boolean queries to search engines and utilizing the returned page counts ( number of pages ) .</sentence>
				<definiendum id="0">Chisquare test</definiendum>
				<definiens id="0">Internet ) 技術 ( Technology ) 瀏覽器 ( Browser ) 電腦 ( Computer ) 資訊 ( Information ) t1 t2 s2 eij s3 s4 s5 s1 ting Boolean queries to search engines and utilizing the returned page counts ( number of pages )</definiens>
			</definition>
			<definition id="5">
				<sentence>Herein , we adopt the conventional tf-idf weighting scheme to estimate the significance of features and define it as : ) log ( ) , ( max ) , ( n Nptf ptfw jj i t i �= , where f ( ti , p ) is the frequency of term ti in search-result page p , N is the total number of Web pages , and n is the number of the pages containing ti .</sentence>
				<definiendum id="0">p )</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">n</definiendum>
				<definiens id="0">the conventional tf-idf weighting scheme to estimate the significance of features</definiens>
				<definiens id="1">the frequency of term ti in search-result page p</definiens>
				<definiens id="2">the total number of Web pages , and</definiens>
				<definiens id="3">the number of the pages containing ti</definiens>
			</definition>
			<definition id="6">
				<sentence>We train a uni-gram language model for each language of concern and perform language identification based on a discrimination function , which locates maximum character or word entropy and is defined as : � � � � � �= � �� ) | ( ln ) | ( maxarg ) ( ) ( lwplwptlang tjNwLl j , where N ( tj ) is the collection of the snippets containing tj and L is a set of languages to be identified .</sentence>
				<definiendum id="0">N ( tj )</definiendum>
				<definiendum id="1">L</definiendum>
				<definiens id="0">a uni-gram language model for each language of concern and perform language identification based on a discrimination function , which locates maximum character or word entropy and is defined as : � � � � � �= � �� ) | ( ln ) | ( maxarg ) ( ) ( lwplwptlang tjNwLl j</definiens>
				<definiens id="1">the collection of the snippets containing tj and</definiens>
			</definition>
			<definition id="7">
				<sentence>Performance Metric : The average top-n inclusion rate was adopted as a metric on the extraction of translation equivalents .</sentence>
				<definiendum id="0">Performance Metric</definiendum>
				<definiens id="0">a metric on the extraction of translation equivalents</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>The Unihan database , which currently contains 54,728 kanji characters found in Chinese , Japanese , and Korean , provides rich information about these kanji characters , such as the definition of the character , its values in different encoding systems , and the pronunciation ( s ) of the character in Chinese ( listed under the feature kMandarin in the Unihan database ) , in Japanese ( both the On reading and the Kun reading 9 : kJapaneseKun and kJapaneseOn ) , and in Korean ( kKorean ) .</sentence>
				<definiendum id="0">Unihan database</definiendum>
				<definiens id="0">the definition of the character , its values in different encoding systems , and the pronunciation ( s ) of the character in Chinese ( listed under the feature kMandarin in the Unihan database</definiens>
			</definition>
			<definition id="1">
				<sentence>The procedure consists of the following steps : ( 1 ) romanji name segmentation , ( 2 ) kanji name generation , ( 3 ) kanji name filtering via monolingual Japanese corpus , and ( 4 ) kanjiromanji combination filtering via WWW .</sentence>
				<definiendum id="0">procedure</definiendum>
				<definiens id="0">consists of the following steps : ( 1 ) romanji name segmentation</definiens>
			</definition>
</paper>

		<paper id="1056">
</paper>

		<paper id="3008">
</paper>

		<paper id="1067">
			<definition id="0">
				<sentence>We adopt here a geometric view on bilingual lexicon extraction from comparable corpora which allows one to re-interpret the methods proposed thus far and formulate new ones inspired by latent semantic analysis ( LSA ) , which was developed within the information retrieval ( IR ) community to treat synonymous and polysemous terms ( Deerwester et al. , 1990 ) .</sentence>
				<definiendum id="0">LSA</definiendum>
			</definition>
			<definition id="1">
				<sentence>In addition , notice that M can be rewritten as S &gt; T , with S an n p and T an n q matrix encoding the relations between words and pairs in the bilingual dictionary ( e.g. Ski is 1 iff si is in the kth translation pair ) .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">an n p and T an n q matrix encoding the relations between words and pairs in the bilingual dictionary</definiens>
			</definition>
			<definition id="2">
				<sentence>Assuming that C encodes the co-occurrences between vocabulary words w and translation pairs d , PLSA models the probability of co-occurrence w and d via latent classes : P ( w ; d ) = X P ( ) P ( wj ) P ( dj ) ( 7 ) where , for a given class , words and translation pairs are assumed to be independently generated from class-conditional probabilities P ( wj ) and P ( dj ) .</sentence>
				<definiendum id="0">PLSA models</definiendum>
				<definiens id="0">the probability of co-occurrence w</definiens>
			</definition>
			<definition id="3">
				<sentence>The Fisher kernel performs a dot-product in a vector space defined by the parameters of the model .</sentence>
				<definiendum id="0">Fisher kernel</definiendum>
				<definiens id="0">performs a dot-product in a vector space defined by the parameters of the model</definiens>
			</definition>
			<definition id="4">
				<sentence>Infrequent words ( occurring less than 5 times ) were discarded when building the indexing terms and the dictionary entries .</sentence>
				<definiendum id="0">Infrequent words</definiendum>
				<definiens id="0">occurring less than 5 times ) were discarded when building the indexing terms and the dictionary entries</definiens>
			</definition>
			<definition id="5">
				<sentence>standard 0.14 0.20 0.24 0.29 0.30 0.33 0.35 0.38 0.40 0.35 Ext ( N=500 ) 0.11 0.21 0.27 0.32 0.34 0.38 0.41 0.45 0.50 0.40 CCA ( l=300 ) 0.04 0.10 0.14 0.20 0.22 0.26 0.29 0.35 0.41 0.25 NFK ( k=1 ) 0.10 0.15 0.20 0.23 0.26 0.27 0.28 0.32 0.34 0.30 Ext + standard 0.16 0.26 0.32 0.37 0.40 0.44 0.45 0.47 0.50 0.44 Ext + NFK ( k=1 ) 0.13 0.23 0.28 0.33 0.38 0.42 0.44 0.48 0.50 0.42 Ext + NFK ( k=4 ) 0.13 0.22 0.26 0.33 0.37 0.40 0.42 0.47 0.50 0.41 Ext + NFK ( k=16 ) 0.12 0.20 0.25 0.32 0.36 0.40 0.42 0.47 0.50 0.40 Table 1 : Results of the different methods ; F-1 score at different number of candidate translations .</sentence>
				<definiendum id="0">CCA</definiendum>
				<definiendum id="1">NFK</definiendum>
				<definiens id="0">Results of the different methods</definiens>
			</definition>
</paper>

		<paper id="3030">
			<definition id="0">
				<sentence>Wysiwym ( What You See Is What You Meant ) is a user-interface technology through which a domain expert can formally encode knowledge by structured editing of an automatically generated feedback text ( Power and Scott , 1998 ) .</sentence>
				<definiendum id="0">Wysiwym</definiendum>
			</definition>
			<definition id="1">
				<sentence>In the first case , Wysiwym editing encodes the desired content of the document in an interlingua , from which versions can be generated in mutliple languages ; in the second case , it yields a query encoded in a formal query language such as SQL .</sentence>
				<definiendum id="0">Wysiwym editing</definiendum>
				<definiens id="0">encodes the desired content of the document in an interlingua</definiens>
			</definition>
</paper>

		<paper id="3012">
			<definition id="0">
				<sentence>Table 3 shows the mean and standard deviation for all de subcorpora ( CC is Computing Corpus ) .</sentence>
				<definiendum id="0">CC</definiendum>
			</definition>
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>Corpus data , however , are more likely while Section 6 presents an evaluation of to include instantiated participants , which may SemFrame’s ability to identify semantic verb not generalize to the frame element .</sentence>
				<definiendum id="0">Corpus data</definiendum>
				<definiens id="0">may SemFrame’s ability to identify semantic verb not generalize to the frame element</definiens>
			</definition>
			<definition id="1">
				<sentence>Words corresponding to the Money ( money , value ) , the Merchandise ( property , goods ) , and the Buyer ( buyer , buyers ) are present in , and to some extent shared across , the definitions ; however , no words corresponding to the Seller are present .</sentence>
				<definiendum id="0">Merchandise</definiendum>
				<definiendum id="1">Buyer</definiendum>
				<definiens id="0">buyer , buyers ) are present in , and to some extent shared across , the definitions</definiens>
			</definition>
			<definition id="2">
				<sentence>forall synset lines in input file return ( synset , related_synset ) pairs for all synsets directly related through hyponymy , antonymy , entailment , or cause_to relationships in WordNet ( for extended relationship pairs , also return ( synset , related_synset ) pairs for all synsets within hyponymy tree , i.e. , no matter how many levels removed ) f. Relates LDOCE verb senses that map to the same WordNet synset Input .</sentence>
				<definiendum id="0">forall synset</definiendum>
			</definition>
			<definition id="3">
				<sentence>However , it is clearly Incompleteness in the listing of evoking verbs specific word senses that evoke frames , and in FrameNet and SemFrame precludes a straightforward detection of correspondences between incrust , and ornament .</sentence>
				<definiendum id="0">SemFrame</definiendum>
				<definiens id="0">the listing of evoking verbs specific word senses that evoke frames , and in FrameNet</definiens>
			</definition>
			<definition id="4">
				<sentence>FrameNet frame names ( e.g. , ABUNDANCE , A C T I V I T Y _ S T A R T , C A U S E _ T O _ B E _ W E T , INCHOATIVE_ATTACHING ) , however , exhibit considerable variation .</sentence>
				<definiendum id="0">FrameNet frame names</definiendum>
				<definiens id="0">A C T I V I T Y _ S T A R T , C A U S E _ T O _ B E _ W E T</definiens>
			</definition>
			<definition id="5">
				<sentence>The majority-related function computes the precision ratio of the SemFrame frame for each pair of FrameNet and SemFrame frames being compared .</sentence>
				<definiendum id="0">majority-related function</definiendum>
				<definiens id="0">computes the precision ratio of the SemFrame frame for each pair of FrameNet and SemFrame frames being compared</definiens>
			</definition>
			<definition id="6">
				<sentence>WordNet : An Electronic Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>The model consists of an online and an offline version , the former taking care of the expertise level changes during the same session , the latter modelling the overall user expertise as a function of time and repeated interactions .</sentence>
				<definiendum id="0">model</definiendum>
			</definition>
			<definition id="1">
				<sentence>AthosMail is an interactive speech-based e-mail system being developed for mobile telephone use in the project DUMAS ( Jokinen and Gambäck , 2004 ) .</sentence>
				<definiendum id="0">AthosMail</definiendum>
				<definiendum id="1">DUMAS</definiendum>
				<definiens id="0">an interactive speech-based e-mail system being developed for mobile telephone use in the project</definiens>
			</definition>
			<definition id="2">
				<sentence>The purpose of the AthosMail user model is to provide flexibility and variation in the system utterances .</sentence>
				<definiendum id="0">AthosMail user model</definiendum>
				<definiens id="0">to provide flexibility and variation in the system utterances</definiens>
			</definition>
			<definition id="3">
				<sentence>The user model consists of different subcomponents , such as Message Prioritizing , Message Categorization and User Preference components ( Jokinen et al , 2004 ) .</sentence>
				<definiendum id="0">user model</definiendum>
			</definition>
			<definition id="4">
				<sentence>The Cooperativity Model utilizes two parameters , explicitness and dialogue control ( i.e. initiative ) , and the combination of their values then guides utterance generation .</sentence>
				<definiendum id="0">Cooperativity Model</definiendum>
				<definiens id="0">utilizes two parameters , explicitness and dialogue control ( i.e. initiative ) , and the combination of their values then guides utterance generation</definiens>
			</definition>
			<definition id="5">
				<sentence>The former is an estimate of the user’s competence level , and is described in the following sections .</sentence>
				<definiendum id="0">former</definiendum>
				<definiens id="0">an estimate of the user’s competence level , and is described in the following sections</definiens>
			</definition>
			<definition id="6">
				<sentence>AthosMail uses a three-level user expertise scale to encode varied skill levels of the users .</sentence>
				<definiendum id="0">AthosMail</definiendum>
			</definition>
			<definition id="7">
				<sentence>DASEX ( dialogue act specific explicitness ) : The value is modified during sessions .</sentence>
				<definiendum id="0">DASEX</definiendum>
			</definition>
			<definition id="8">
				<sentence>HLP ( the occurrence of a help request by the user ) : The system incorporates a separate help function ; this parameter is only used to notify the offline side about the frequency of help requests .</sentence>
				<definiendum id="0">HLP</definiendum>
				<definiens id="0">the occurrence of a help request by the user ) : The system incorporates a separate help function</definiens>
			</definition>
			<definition id="9">
				<sentence>TIM ( the occurrence of a timeout on the user 's turn ) : If TIM = 'yes ' , then DASEX +1 .</sentence>
				<definiendum id="0">TIM</definiendum>
				<definiens id="0">the occurrence of a timeout on the user 's turn ) : If TIM = 'yes '</definiens>
			</definition>
			<definition id="10">
				<sentence>INT ( occurrence of a user interruption during system turn ) : Can be either a barge-in or an interruption by telephone keys .</sentence>
				<definiendum id="0">INT</definiendum>
				<definiens id="0">occurrence of a user interruption during system turn</definiens>
			</definition>
			<definition id="11">
				<sentence>DDASEX ( default dialogue act specific explicitness ) : Every system dialogue act has its own default explicitness value invoked at the beginning of a session .</sentence>
				<definiendum id="0">DDASEX</definiendum>
				<definiens id="0">Every system dialogue act has its own default explicitness value invoked at the beginning of a session</definiens>
			</definition>
			<definition id="12">
				<sentence>GEX ( general expertise ) : General expertise .</sentence>
				<definiendum id="0">GEX</definiendum>
				<definiens id="0">general expertise ) : General expertise</definiens>
			</definition>
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>The Question Decomposition Unit and the Answer Recomposition Unit are the units that conform the Temporal Q.A. layer which process the temporal questions , before and after using a General Purpose Q.A. system .</sentence>
				<definiendum id="0">Question Decomposition Unit</definiendum>
				<definiens id="0">the units that conform the Temporal Q.A. layer which process the temporal questions , before and after using a General Purpose Q.A. system</definiens>
			</definition>
			<definition id="1">
				<sentence>a4 The Question Decomposition Unit is a preprocessing unit which performs three main tasks .</sentence>
				<definiendum id="0">Question Decomposition Unit</definiendum>
				<definiens id="0">a preprocessing unit which performs three main tasks</definiens>
			</definition>
			<definition id="2">
				<sentence>For the example above , a current Q.A. system returns the following answers : – Q1 Answers : Georgetown University ( 1964-68 ) // Oxford University ( 1968-70 ) // Yale Law School ( 1970-73 ) – Q2 Answer : 1968 a4 The Answer Recomposition Unit is the last stage in the process .</sentence>
				<definiendum id="0">Recomposition Unit</definiendum>
				<definiens id="0">the last stage in the process</definiens>
			</definition>
			<definition id="3">
				<sentence>The Type Identification Unit classifies the question in one of the four types of the taxonomy proposed in section 2 .</sentence>
				<definiendum id="0">Type Identification Unit</definiendum>
			</definition>
</paper>

		<paper id="1075">
			<definition id="0">
				<sentence>Support Vector Machines ( SVM ) is a powerful machine learning method , which has been applied successfully in NER tasks , such as ( Kazama et al. 2002 ; Lee et al. 2003 ) .</sentence>
				<definiendum id="0">Support Vector Machines ( SVM</definiendum>
				<definiens id="0">a powerful machine learning method , which has been applied successfully in NER tasks</definiens>
			</definition>
			<definition id="1">
				<sentence>In NER , SVM is to cla ssify a word into positive class “1” indicating that the word is a part of an entity , or negative class “-1” in dicating that the word is not a part of an entity .</sentence>
				<definiendum id="0">SVM</definiendum>
				<definiens id="0">to cla ssify a word into positive class “1” indicating that the word is a part of an entity , or negative class “-1” in dicating that the word is not a part of an entity</definiens>
			</definition>
			<definition id="2">
				<sentence>The semantic trigger features consist of some special head nouns for an entity class which is supplied by users .</sentence>
				<definiendum id="0">semantic trigger features</definiendum>
				<definiens id="0">consist of some special head nouns for an entity class which is supplied by users</definiens>
			</definition>
			<definition id="3">
				<sentence>N is the number of the support vectors in current model .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of the support vectors in current model</definiens>
			</definition>
			<definition id="4">
				<sentence>The density of NEi is defined as the average similarity between NEi and all the other entities NEj in NESet as follows .</sentence>
				<definiendum id="0">density of NEi</definiendum>
				<definiens id="0">the average similarity between NEi and all the other entities NEj in NESet as follows</definiens>
			</definition>
			<definition id="5">
				<sentence>Info_Min Strategy1 Strategy2 51.9K 40K 31K Table 4 : Comparisons of training data sizes for the multicriteria-based selection strategies and the informativenesscriterion-based selection ( Info_Min ) to achieve the same performance level as the supervised learning .</sentence>
				<definiendum id="0">informativenesscriterion-based selection</definiendum>
				<definiens id="0">Comparisons of training data sizes for the multicriteria-based selection strategies</definiens>
			</definition>
</paper>

		<paper id="3006">
			<definition id="0">
				<sentence>Tsim uses a machine-readable bilingual dictionary to find word-to-word matches between two halves of a bitext .</sentence>
				<definiendum id="0">Tsim</definiendum>
				<definiens id="0">uses a machine-readable bilingual dictionary to find word-to-word matches between two halves of a bitext</definiens>
			</definition>
			<definition id="1">
				<sentence>The SIMR algorithm attempts to construct a piecewise linear approximation to the True Bitext Map ( TBM ) of a bitext by greedily searching for small chains of points of correspondence .</sentence>
				<definiendum id="0">SIMR algorithm</definiendum>
				<definiens id="0">attempts to construct a piecewise linear approximation to the True Bitext Map ( TBM ) of a bitext by</definiens>
			</definition>
			<definition id="2">
				<sentence>SIMR-cl uses the standard SIMR parameters plus the additional chain length parameter discussed above .</sentence>
				<definiendum id="0">SIMR-cl</definiendum>
				<definiens id="0">uses the standard SIMR parameters plus the additional chain length parameter discussed above</definiens>
			</definition>
			<definition id="3">
				<sentence>Once the current state is set SIMR-cl generates a bitext map and calculates the density of the map .</sentence>
				<definiendum id="0">SIMR-cl</definiendum>
				<definiens id="0">generates a bitext map and calculates the density of the map</definiens>
			</definition>
			<definition id="4">
				<sentence>The bitext map density is defined as the number of points in the bitext map divided by the length of the main diagonal of the bitext space .</sentence>
				<definiendum id="0">bitext map density</definiendum>
				<definiens id="0">the number of points in the bitext map divided by the length of the main diagonal of the bitext space</definiens>
			</definition>
			<definition id="5">
				<sentence>The LCSR ratio is the length of the longest common subsequence of two tokens , divided by the length of the longer token .</sentence>
				<definiendum id="0">LCSR ratio</definiendum>
			</definition>
			<definition id="6">
				<sentence>For this purpose , we used the STRAND corpus , which consists of 326 candidate bitexts in French and English1 ( Resnik and Smith , 2003 ) .</sentence>
				<definiendum id="0">STRAND corpus</definiendum>
			</definition>
			<definition id="7">
				<sentence>I.e. , even in sections where the order of ideas was largely the same in the two languages , the English wording was much more terse ( the informant said ”compressed” ) , and omitted many details .</sentence>
				<definiendum id="0">I.e.</definiendum>
				<definiens id="0">the informant said ”compressed”</definiens>
			</definition>
</paper>

		<paper id="3020">
			<definition id="0">
				<sentence>HITS ( Hyperlinked Induced Topic Search ) ( Kleinberg , 1999 ) is an iterative algorithm that was designed for ranking Web pages according to their degree of “authority” .</sentence>
				<definiendum id="0">HITS</definiendum>
				<definiens id="0">an iterative algorithm that was designed for ranking Web pages according to their degree of “authority”</definiens>
			</definition>
			<definition id="1">
				<sentence>For each vertex , HITS produces two sets of scores – an “authority” score , and a “hub” score : HITSA ( Vi ) = X Vj2In ( Vi ) HITSH ( Vj ) ( 1 ) HITSH ( Vi ) = X Vj2Out ( Vi ) HITSA ( Vj ) ( 2 ) Introduced by ( Herings et al. , 2001 ) , the positional power function is a ranking algorithm that determines the score of a vertex as a function that combines both the number of its successors , and the score of its successors .</sentence>
				<definiendum id="0">HITSH</definiendum>
				<definiendum id="1">positional power function</definiendum>
			</definition>
			<definition id="2">
				<sentence>Unlike other ranking algorithms , PageRank integrates the impact of both incoming and outgoing links into one single model , and therefore it produces only one set of scores : PR ( Vi ) = ( 1 d ) + d X Vj2In ( Vi ) PR ( Vj ) jOut ( Vj ) j ( 5 ) where d is a parameter that is set between 0 and 1 1 .</sentence>
				<definiendum id="0">PageRank</definiendum>
				<definiens id="0">integrates the impact of both incoming and outgoing links into one single model , and therefore it produces only one set of scores : PR ( Vi ) = ( 1 d ) + d X Vj2In</definiens>
				<definiens id="1">a parameter that is set between 0 and 1 1</definiens>
			</definition>
			<definition id="3">
				<sentence>Formally , given two sentences Si and Sj , with a sentence being represented by the set of Ni words that appear in the sentence : Si = W i1 ; W i2 ; : : : ; W iNi , the similarity of Si and Sj is defined as : Similarity ( Si ; Sj ) = jWkjWk2Si &amp; Wk2Sjjlog ( jSij ) +log ( jSjj ) The resulting graph is highly connected , with a weight associated with each edge , indicating the strength of the connections between various sentence pairs in the text2 .</sentence>
				<definiendum id="0">W iNi</definiendum>
				<definiens id="0">with a sentence being represented by the set of Ni words that appear in the sentence : Si = W i1 ; W i2 ; : : : ;</definiens>
			</definition>
			<definition id="4">
				<sentence>Intuitively , TextRank works well because it does not only rely on the local context of a text unit ( vertex ) , but rather it takes into account information recursively drawn from the entire text ( graph ) .</sentence>
				<definiendum id="0">TextRank</definiendum>
				<definiens id="0">works well because it does not only rely on the local context of a text unit ( vertex ) , but rather it takes into account information recursively drawn from the entire text ( graph )</definiens>
			</definition>
</paper>

		<paper id="1073">
			<definition id="0">
				<sentence>QA-by-Dossier-with-Constraints is a new approach to Question Answering whereby candidate answers’ confidences are adjusted by asking auxiliary questions whose answers constrain the original answers .</sentence>
				<definiendum id="0">QA-by-Dossier-with-Constraints</definiendum>
				<definiens id="0">auxiliary questions whose answers constrain the original answers</definiens>
			</definition>
			<definition id="1">
				<sentence>Our new approach of QA-by-Dossier-with-Constraints ( QDC ) uses the answers to additional questions to provide more information that can be used in ranking candidate answers to the original question .</sentence>
				<definiendum id="0">QDC )</definiendum>
				<definiens id="0">uses the answers to additional questions to provide more information that can be used in ranking candidate answers to the original question</definiens>
			</definition>
			<definition id="2">
				<sentence>The LCC system ( Moldovan &amp; Rus , 2001 ) uses a Logic Prover to establish the connection between a candidate answer passage and the question .</sentence>
				<definiendum id="0">LCC system</definiendum>
				<definiens id="0">uses a Logic Prover to establish the connection between a candidate answer passage and the question</definiens>
			</definition>
			<definition id="3">
				<sentence>QA-by-Dossier-with-Constraints is an extension of on-going work of ours called QA-by-Dossier ( QbD ) ( Prager et al. , 2004 ) .</sentence>
				<definiendum id="0">QA-by-Dossier-with-Constraints</definiendum>
			</definition>
</paper>

		<paper id="3021">
			<definition id="0">
				<sentence>Finitestate models are attractive mechanisms for language processing since they ( a ) provide an ef cient data structure for representing weighted ambiguous hypotheses ( b ) generally effective for decoding ( c ) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper , we describe the compilation process for a particular classi er model into an WFST and validate the accuracy of the compilation process on a one-best input in a call-routing task .</sentence>
				<definiendum id="0">Finitestate models</definiendum>
				<definiens id="0">attractive mechanisms for language processing since they ( a ) provide an ef cient data structure for representing weighted ambiguous hypotheses ( b ) generally effective for decoding ( c ) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech</definiens>
			</definition>
			<definition id="1">
				<sentence>Boostexter is a machine learning tool which is based on the boosting family of algorithms rst proposed in ( Freund and Schapire , 1996 ) .</sentence>
				<definiendum id="0">Boostexter</definiendum>
				<definiens id="0">a machine learning tool</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus , ifa79a81a80 is a function to produce alla2 -grams up toa2 of its argument , then the set of predicates for the weak learners is a25 a12 a79a45a80 a26 a4a82a31 .</sentence>
				<definiendum id="0">ifa79a81a80</definiendum>
				<definiens id="0">a function to produce alla2 -grams up toa2 of its argument , then the set of predicates for the weak learners is a25 a12 a79a45a80 a26 a4a82a31</definiens>
			</definition>
			<definition id="3">
				<sentence>a117 a65a116a88 : test features of the word a117 a65 a84 : test features of the left context a117 a65 a85 : test features of the right context We use the representation of context-dependent rewrite rules ( Johnson , 1972 ; Kaplan and Kay , 1994 ) and their weighted version ( Mohri and Sproat , 1996 ) to represent these weak learners .</sentence>
				<definiendum id="0">context-dependent rewrite rules</definiendum>
				<definiendum id="1">weighted version</definiendum>
				<definiens id="0">test features of the word a117 a65 a84 : test features of the left context a117 a65 a85 : test features of the right</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>Add n edges ( s , vi ) , each with weight ind1 ( xi ) , and n edges ( vi , t ) , each with weight ind2 ( xi ) .</sentence>
				<definiendum id="0">Add n edges</definiendum>
				<definiens id="0">xi ) , and n edges ( vi , t ) , each with weight ind2 ( xi )</definiens>
			</definition>
			<definition id="1">
				<sentence>We can set the individual scores ind1 ( si ) to PrNBsub ( si ) and ind2 ( si ) to 1 − PrNBsub ( si ) , as shown in Figure 3 , where PrNBsub ( s ) denotes Naive Bayes’ estimate of the probability that sentence s is subjective ; or , we can use the weights produced by the SVM classifier instead.7 If we set all the association scores to zero , then the minimum-cut classification of the sentences is the same as that of the basic subjectivity detector .</sentence>
				<definiendum id="0">PrNBsub</definiendum>
				<definiens id="0">( si ) , as shown in Figure 3 , where PrNBsub ( s ) denotes Naive Bayes’ estimate of the probability that sentence s is subjective ; or , we can use the weights produced by the SVM classifier instead.7 If we set all the association scores to zero</definiens>
			</definition>
</paper>

		<paper id="3033">
			<definition id="0">
				<sentence>MATCHKiosk is a multimodal interactive city guide which provides users with the freedom to interact using speech , pen , touch or multimodal inputs .</sentence>
				<definiendum id="0">MATCHKiosk</definiendum>
				<definiens id="0">a multimodal interactive city guide which provides users with the freedom to interact using speech , pen , touch or multimodal inputs</definiens>
			</definition>
			<definition id="1">
				<sentence>The MATCHKiosk runs on a Windows PC mounted in a rugged cabinet ( Figure 1 ) .</sentence>
				<definiendum id="0">MATCHKiosk</definiendum>
			</definition>
			<definition id="2">
				<sentence>System output consists of coordinated presentations combining synthetic speech with graphical actions on the map .</sentence>
				<definiendum id="0">System output</definiendum>
				<definiens id="0">consists of coordinated presentations combining synthetic speech with graphical actions on the map</definiens>
			</definition>
			<definition id="3">
				<sentence>Help messages consist of multimodal presentations combining spoken output with ink drawn on the display by the system .</sentence>
				<definiendum id="0">Help messages</definiendum>
				<definiens id="0">consist of multimodal presentations combining spoken output with ink drawn on the display by the system</definiens>
			</definition>
			<definition id="4">
				<sentence>The underlying architecture of MATCHKiosk consists of a series of re-usable components which communicate using XML messages sent over sockets through a facilitator ( MCUBE ) ( Figure 3 ) .</sentence>
				<definiendum id="0">MATCHKiosk</definiendum>
			</definition>
			<definition id="5">
				<sentence>These are then combined and assigned a meaning representation using a multimodal language processing architecture based on finite-state techniques ( MMFST ) ( Johnston and Bangalore , 2000 ; Johnston et al. , 2002b ) .</sentence>
				<definiendum id="0">MMFST )</definiendum>
				<definiens id="0">a meaning representation using a multimodal language processing architecture based on finite-state techniques</definiens>
			</definition>
			<definition id="6">
				<sentence>If additional information or confirmation is required , the MDM uses the virtual agent to enter into a short information gathering dialogue with the user .</sentence>
				<definiendum id="0">MDM</definiendum>
				<definiens id="0">uses the virtual agent to enter into a short information gathering dialogue with the user</definiens>
			</definition>
			<definition id="7">
				<sentence>Once a command or query is complete , it is passed to the multimodal generation component ( MMGEN ) , which builds a multimodal score indicating a coordinated sequence of graphical actions and TTS prompts .</sentence>
				<definiendum id="0">MMGEN</definiendum>
				<definiens id="0">builds a multimodal score indicating a coordinated sequence of graphical actions</definiens>
			</definition>
			<definition id="8">
				<sentence>The Multimodal UI passes prompts to a visual text-to-speech component ( Cosatto and Graf , 2000 ) which communicates with the AT &amp; T Natural Voices TTS engine ( Beutnagel et al. , 1999 ) in order to coordinate the lip movements of the virtual agent with synthetic speech output .</sentence>
				<definiendum id="0">Multimodal UI</definiendum>
				<definiens id="0">passes prompts to a visual text-to-speech component ( Cosatto and Graf , 2000 ) which communicates with the AT &amp; T Natural Voices TTS engine ( Beutnagel et al. , 1999 ) in order to coordinate the lip movements of the virtual agent with synthetic speech output</definiens>
			</definition>
			<definition id="9">
				<sentence>The subway route server is an application server which identifies the best route between any two locations .</sentence>
				<definiendum id="0">subway route server</definiendum>
				<definiens id="0">an application server which identifies the best route between any two locations</definiens>
			</definition>
			<definition id="10">
				<sentence>The August system ( Gustafson et al. , 1999 ) is a multimodal dialog system mounted in a public kiosk .</sentence>
				<definiendum id="0">August system</definiendum>
				<definiens id="0">a multimodal dialog system mounted in a public kiosk</definiens>
			</definition>
			<definition id="11">
				<sentence>SmartKom-Public ( Wahlster , 2003 ) is an interactive public information kiosk that supports multimodal input through speech , hand gestures , and facial expressions .</sentence>
				<definiendum id="0">SmartKom-Public</definiendum>
				<definiens id="0">an interactive public information kiosk that supports multimodal input through speech , hand gestures , and facial expressions</definiens>
			</definition>
			<definition id="12">
				<sentence>The mVPQ kiosk system ( Narayanan et al. , 2000 ) provides access to corporate directory information and call completion .</sentence>
				<definiendum id="0">mVPQ kiosk system</definiendum>
				<definiens id="0">provides access to corporate directory information and call completion</definiens>
			</definition>
			<definition id="13">
				<sentence>SmartKom uses an animated character , August a model-based talking head , and MATCHKiosk a sample-based videorealistic talking head .</sentence>
				<definiendum id="0">SmartKom</definiendum>
				<definiendum id="1">MATCHKiosk</definiendum>
				<definiens id="0">uses an animated character</definiens>
			</definition>
</paper>

		<paper id="3017">
			<definition id="0">
				<sentence>MODIFY denotes the modiﬁed PAS .</sentence>
				<definiendum id="0">MODIFY</definiendum>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>Where unique feature introduction is not assumed , it can be added automatically in O ( F T ) time , whereF is the number of features and T is the number of types ( Penn , 2001 ) .</sentence>
				<definiendum id="0">whereF</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">the number of features</definiens>
				<definiens id="1">the number of types</definiens>
			</definition>
			<definition id="1">
				<sentence>As mentioned above , the ERG uses the same form , but where can only be a type description , , and is the trivial goal , true .</sentence>
				<definiendum id="0">ERG</definiendum>
				<definiens id="0">uses the same form</definiens>
			</definition>
			<definition id="2">
				<sentence>On the other hand , MERGE uses delaying for all three of the purposes that have been discussed in this paper : complex antecedents , explicit whenfs/2 calls to avoid non-termination problems , and explicit whenfs/2 calls to avoid expensive nondeterministic searches .</sentence>
				<definiendum id="0">MERGE</definiendum>
				<definiens id="0">complex antecedents , explicit whenfs/2 calls to avoid non-termination problems , and explicit whenfs/2 calls to avoid expensive nondeterministic searches</definiens>
			</definition>
</paper>

		<paper id="3005">
			<definition id="0">
				<sentence>The Springer corpus consists of 9640 documents ( titles plus abstracts of medical journal articles ) each in English and in German , with 25 queries in both languages , and relevance judgments made by native German speakers who are medical experts and are fluent in English .</sentence>
				<definiendum id="0">Springer corpus</definiendum>
				<definiens id="0">consists of 9640 documents ( titles plus abstracts of medical journal articles ) each in English and in German , with 25 queries in both languages , and relevance judgments made by native German speakers who are medical experts and are fluent in English</definiens>
			</definition>
			<definition id="1">
				<sentence>In addition to Springer , we have used four other English-German parallel corpora for training : • NEWS is a collection of 59K sentence aligned news stories , downloaded from the web ( 1996-2000 ) , and available at http : //www.isi.edu/~koehn/publications/denews/ • WAC is a small parallel corpus obtained by mining the web ( Nie et al. , 2000 ) , in no particular domain • EUROPARL is a parallel corpus provided by ( Koehn ) .</sentence>
				<definiendum id="0">NEWS</definiendum>
				<definiendum id="1">WAC</definiendum>
				<definiendum id="2">EUROPARL</definiendum>
				<definiens id="0">a collection of 59K sentence aligned news stories , downloaded from the web ( 1996-2000 )</definiens>
				<definiens id="1">a small parallel corpus obtained by mining the web ( Nie et al. , 2000 ) , in no particular domain •</definiens>
			</definition>
			<definition id="2">
				<sentence>The crosslingual retrieval consists of the following steps : feedback product between the query vector ( with weights from step 1 ) and a translation matrix obtained by calculating translation probabilities or term-term similarity using the parallel corpus .</sentence>
				<definiendum id="0">crosslingual retrieval</definiendum>
				<definiens id="0">consists of the following steps : feedback product between the query vector ( with weights from step 1 ) and a translation matrix obtained by calculating translation probabilities or term-term similarity using the parallel corpus</definiens>
			</definition>
</paper>

		<paper id="3022">
			<definition id="0">
				<sentence>For the Template Relations task of MUC-7 , BBN researchers ( Miller et al. , 2000 ) augmented syntactic parse trees with semantic information corresponding to entities and relations and built generative models for the augmented trees .</sentence>
				<definiendum id="0">BBN</definiendum>
			</definition>
			<definition id="1">
				<sentence>Automatic Content Extraction ( ACE , 2004 ) is an evaluation conducted by NIST to measure Entity Detection and Tracking ( EDT ) and relation detection and characterization ( RDC ) .</sentence>
				<definiendum id="0">Automatic Content Extraction</definiendum>
				<definiens id="0">an evaluation conducted by NIST to measure Entity Detection and Tracking ( EDT ) and relation detection and characterization ( RDC )</definiens>
			</definition>
			<definition id="2">
				<sentence>Entity Type The entity type ( one of PERSON , ORGANIZATION , LOCATION , FACILITY , Geo-Political Entity or GPE ) of both the mentions .</sentence>
				<definiendum id="0">GPE</definiendum>
				<definiens id="0">one of PERSON , ORGANIZATION , LOCATION , FACILITY , Geo-Political Entity or</definiens>
			</definition>
			<definition id="3">
				<sentence>Overlap one-mention-in-between ( the word “its” ) , two-words-apart , in-same-noun-phrase .</sentence>
				<definiendum id="0">Overlap one-mention-in-between</definiendum>
				<definiens id="0">the word “its”</definiens>
			</definition>
			<definition id="4">
				<sentence>The ACE value is a NIST metric that assigns 0 % value for a system which produces no output and 100 % value for a system that extracts all the relations and produces no false alarms .</sentence>
				<definiendum id="0">ACE value</definiendum>
				<definiens id="0">a NIST metric that assigns 0 % value for a system which produces no output and 100 % value for a system that extracts all the relations and produces no false alarms</definiens>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>After a short relation to previous work , Section 3 introduces the WITAS multimodal dialogue system , which we use to collect data ( Section 4 ) and to derive baseline results ( Section 5 ) .</sentence>
				<definiendum id="0">WITAS multimodal dialogue system</definiendum>
				<definiens id="0">we use to collect data ( Section 4 ) and to derive baseline results ( Section 5 )</definiens>
			</definition>
			<definition id="1">
				<sentence>The WITAS dialogue system ( Lemon et al. , 2002 ) is a multimodal command and control dialogue system that allows a human operator to interact with a simulated “unmanned aerial vehicle” ( UAV ) : a small robotic helicopter .</sentence>
				<definiendum id="0">WITAS dialogue system</definiendum>
				<definiens id="0">a multimodal command and control dialogue system that allows a human operator to interact with a simulated “unmanned aerial vehicle” ( UAV ) : a small robotic helicopter</definiens>
			</definition>
			<definition id="2">
				<sentence>The WITAS dialogue system is part of a larger family of systems that implement the Information State Update ( ISU ) approach to dialogue management ( Traum et al. , 1999 ) .</sentence>
				<definiendum id="0">WITAS dialogue system</definiendum>
			</definition>
			<definition id="3">
				<sentence>Here , we briefly introduce parts of the IS which are needed to understand the system’s basic workings , and from which we will extract dialogue-level and task-level information for our learning experiments : • Dialogue Move Tree ( DMT ) : a tree-structure , in which each subtree of the root node represents a “thread” in the conversation , and where each node in a subtree represents an utterance made either by the system or the user .</sentence>
				<definiendum id="0">node</definiendum>
				<definiens id="0">parts of the IS which are needed to understand the system’s basic workings , and from which we will extract dialogue-level and task-level information for our learning experiments : • Dialogue Move Tree ( DMT ) : a tree-structure , in which each subtree of the root</definiens>
			</definition>
			<definition id="4">
				<sentence>1 • Active Node List ( ANL ) : a list that records all “active” nodes in the DMT ; active nodes indi1A tree is used in order to overcome the limitations of stackbased processing , see ( Lemon and Gruenstein , 2004 ) .</sentence>
				<definiendum id="0">Active Node List</definiendum>
				<definiens id="0">a list that records all “active” nodes in the DMT</definiens>
			</definition>
			<definition id="5">
				<sentence>• Salience List ( SL ) : a list of NPs introduced in the current dialogue ordered by recency .</sentence>
				<definiendum id="0">SL )</definiendum>
				<definiens id="0">a list of NPs introduced in the current dialogue ordered by recency</definiens>
			</definition>
			<definition id="6">
				<sentence>• Modality Buffer ( MB ) : a temporary store that registers click events on the GUI .</sentence>
				<definiendum id="0">Modality Buffer ( MB )</definiendum>
				<definiens id="0">a temporary store that registers click events on the GUI</definiens>
			</definition>
			<definition id="7">
				<sentence>The recognition ( REC ) feature group includes the position of a hypothesis in the n-best list ( nbestRank ) , its length in words ( hypothesisLength ) , and five features representing the recognizer’s confidence assessment .</sentence>
				<definiendum id="0">REC</definiendum>
				<definiens id="0">includes the position of a hypothesis in the n-best list ( nbestRank ) , its length in words ( hypothesisLength ) , and five features representing the recognizer’s confidence assessment</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>A noun , a4 , is thus described by a set of co-occurrence triples a94 a4a7a14 a55 a14a32a95a97a96 and associated frequencies , where a55 is a grammatical relation and a95 is a possible cooccurrence with a4 in that relation .</sentence>
				<definiendum id="0">a55</definiendum>
				<definiens id="0">a grammatical relation and a95 is a possible cooccurrence with a4 in that relation</definiens>
			</definition>
			<definition id="1">
				<sentence>The Reuters corpus ( Rose et al. , 2002 ) is a collection of about 810,000 Reuters , English Language News stories .</sentence>
				<definiendum id="0">Reuters corpus</definiendum>
			</definition>
			<definition id="2">
				<sentence>The SPORTS corpus consists of 35317 documents ( about 9.1 million words ) .</sentence>
				<definiendum id="0">SPORTS corpus</definiendum>
			</definition>
			<definition id="3">
				<sentence>The FINANCE corpus consists of 117734 documents ( about 32.5 million words ) .</sentence>
				<definiendum id="0">FINANCE corpus</definiendum>
			</definition>
</paper>

		<paper id="3023">
			<definition id="0">
				<sentence>We define a weighted finite-state automata ( WFST ) T over a set of weights K by an 8-tuple ( Σ , Ω , Q , I , F , E , λ , ρ ) where Σ and Ω are two finite sets of symbols ( alphabets ) , Q is a finite set of states , I ⊆ Q is the set of initial states , F ⊆ Q is the set of final states , E ⊆ Q×Σ∪ { ε } ×Ω∪ { ε } ×K×Q is the set of transitions , and λ : I → K and ρ : F → K are the initial and final weight functions .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiens id="0">a weighted finite-state automata ( WFST ) T over a set of weights K by an 8-tuple ( Σ , Ω , Q , I , F , E , λ , ρ ) where Σ and Ω are two finite sets of symbols ( alphabets )</definiens>
				<definiens id="1">a finite set of states</definiens>
			</definition>
			<definition id="1">
				<sentence>A transition e ∈ E has a label l ( e ) ∈ Σ∪ { epsilon1 } ×Ω∪ { epsilon1 } , a weight w ( e ) ∈Kand a destination δ ( e ) ∈ Q. The set of weights is a semi-ring , that is a system ( K , ⊕ , ⊗ , ¯0 , ¯1 ) where ¯0 is the identity element for ⊕ , ¯1 is the identity element for ⊗ , and ⊕ is commutative ( Berstel and Reteunauer , 1988 ) .</sentence>
				<definiendum id="0">weights</definiendum>
				<definiendum id="1">⊕</definiendum>
				<definiens id="0">a label l ( e ) ∈ Σ∪ { epsilon1 } ×Ω∪ { epsilon1 } , a weight w ( e ) ∈Kand a destination δ ( e ) ∈ Q. The set of</definiens>
			</definition>
			<definition id="2">
				<sentence>In NLP , the tropical semi-ring ( R+ ∪ { ∞ } , min , + , ∞,0 ) is very often used : weights are added along a path , and if several paths match the same relation , the total cost is the cost of the path with minimal cost .</sentence>
				<definiendum id="0">total cost</definiendum>
				<definiens id="0">the cost of the path with minimal cost</definiens>
			</definition>
			<definition id="3">
				<sentence>In our implementation described in section 5 we have generalized the use of this epsilon-free composition operation to handle two operations that are defined EQUIV ( p , wp , q , wq , S ) 1 equiv ← FALSE 2 if S [ { p , q } ] negationslash= NIL 3 then { wprimep , wprimeq } ← S [ { p , q } ] 4 equiv ← wprimep = wp ∧wprimeq = wq 5 else if SIGNATURE ( p ) = SIGNATURE ( q ) 6 then if FINAL ( p ) 7 then equiv ← wp ⊗ρ ( p ) = wq ⊗ρ ( q ) 9 for ep ∈ E ( p ) , eq ∈ E ( q ) , l ( ep ) = l ( eq ) 10 do { wprimep , wprimeq } ← REMAINDER ( wp ⊗w ( ep ) , wq ⊗w ( eq ) ) 11 equiv ← equiv ∧EQUIV ( δ ( ep ) , wprimep , δ ( eq ) , wprimeq , S ) 12 DELETE ( S [ { p , q } ] ) 13 return equiv Figure 2 : The equivalence algorithm on automata only , that is intersection and crossproduct .</sentence>
				<definiendum id="0">, l</definiendum>
				<definiens id="0">p , wp , q , wq</definiens>
				<definiens id="1">p ) = SIGNATURE ( q ) 6 then if FINAL ( p ) 7 then equiv ← wp ⊗ρ ( p ) = wq ⊗ρ ( q ) 9 for ep ∈ E ( p ) , eq ∈ E ( q )</definiens>
				<definiens id="2">ep ) , wq ⊗w ( eq ) ) 11 equiv ← equiv ∧EQUIV ( δ ( ep ) , wprimep , δ ( eq ) , wprimeq</definiens>
			</definition>
			<definition id="4">
				<sentence>Intersection is a simple variant of the composition of the identity transducers corresponding to the operand automata .</sentence>
				<definiendum id="0">Intersection</definiendum>
				<definiens id="0">a simple variant of the composition of the identity transducers corresponding to the operand automata</definiens>
			</definition>
			<definition id="5">
				<sentence>matches any symbol ; “x” is a special espilonsymbol introduced in the final states of the operand automata at preprocessing .</sentence>
				<definiendum id="0">“x”</definiendum>
				<definiens id="0">a special espilonsymbol introduced in the final states of the operand automata at preprocessing</definiens>
			</definition>
</paper>

		<paper id="2008">
			<definition id="0">
				<sentence># S ( EPATTERN : TARGET |yield| : SUBCAT ( VSUBCAT NP ) : CLASSES ( ( 24 51 161 ) 5293 ) : RELIABILITY 0 : FREQSCORE 0.26861903 : FREQCNT 1 : TLTL ( VV0 ) : SLTL ( ( |route| NN1 ) ) : OLT1L ( ( |result| NN2 ) ) : OLT2L NIL : OLT3L NIL : LRL 0 ) ) Figure 1 : An acquired SCF for a verb “yield” In their study , they first acquire fine-grained SCFs using the unsupervised method proposed by Briscoe and Carroll ( 1997 ) and Korhonen ( 2002 ) .</sentence>
				<definiendum id="0">CLASSES</definiendum>
				<definiens id="0">TLTL ( VV0 ) : SLTL ( ( |route| NN1 ) ) : OLT1L ( ( |result| NN2 ) ) : OLT2L NIL : OLT3L NIL : LRL 0 )</definiens>
			</definition>
			<definition id="1">
				<sentence>One intuitive way to estimate a confidence value is to assume an observed probability , i.e. , relative frequency , is equal to a probability θ ij of SCF s j for a word w i ( θ ij = freq ij /∑ j freq ij where freq ij is a frequency that a word w i appears with SCF s j in corpora ) .</sentence>
				<definiendum id="0">freq ij</definiendum>
				<definiens id="0">to assume an observed probability , i.e. , relative frequency , is equal to a probability θ ij of SCF s j for a word</definiens>
				<definiens id="1">a frequency that a word w i appears with SCF s j in corpora )</definiens>
			</definition>
			<definition id="2">
				<sentence>In this context , a posteriori distribution of the probability θ ij of an SCF s j for a word w i is given by : p ( θ ij |D ) = P ( θ ij ) P ( D|θ ij ) P ( D ) = P ( θ ij ) P ( D|θ ij ) integraltext 1 0 P ( θ ij ) P ( D|θ ij ) dθ ij , ( 1 ) where P ( θ ij ) is a priori distribution , and D is the data we have observed .</sentence>
				<definiendum id="0">D</definiendum>
				<definiens id="0">a posteriori distribution of the probability θ ij of an SCF s j for a word w i is given by : p ( θ ij |D ) = P ( θ ij ) P ( D|θ ij ) P ( D ) = P ( θ ij ) P ( D|θ ij ) integraltext 1 0 P ( θ ij ) P ( D|θ ij ) dθ ij , ( 1 ) where P ( θ ij ) is a priori distribution</definiens>
			</definition>
			<definition id="3">
				<sentence>Input : a set of SCF confidence-value vectors V= { v 1 , v 2 , ... , v n } ⊆R m a distance function d : R m ×Z m → R a function to compute a centroid µ : { v j 1 , v j 2 , ... , v j l } →Z m initial centroids C= { c 1 , c 2 , ... , c k } ⊆Z m Output : a set of clusters { C j } while cluster members are not stable do foreach cluster C j C j = { v i |∀c l , d ( v i , c j ) ≥ d ( v i , c l ) } ( 1 ) end foreach foreach clusters C j c j = µ ( C j ) ( 2 ) end foreach end while return { C j } Figure 3 : Clustering algorithm for SCF confidence-value vectors and recomputing centroids until cluster members become stable , as depicted in Figure 3 .</sentence>
				<definiendum id="0">Input</definiendum>
				<definiens id="0">a set of SCF confidence-value vectors V= { v 1 , v 2 , ... , v n } ⊆R m a distance function d : R m ×Z m → R a function to compute a centroid µ</definiens>
			</definition>
			<definition id="4">
				<sentence>I applied my method to SCFs acquired from 135,902 sentences of mobile phone newsgroup postings archived by Google.com , which is the same data used in ( Carroll and Fang , 2004 ) .</sentence>
				<definiendum id="0">Google.com</definiendum>
			</definition>
			<definition id="5">
				<sentence>0 1 0 0.2 0.4 0.6 0.8 1 Recall Precision A B C D A : frequency cut-off B : confidence cut-off 0.01 C : confidence cut-off 0.03 D : confidence cut-off 0.05 0 1 0 0.2 0.4 0.6 0.8 1 Recall Precision A B C D A : frequency cut-off B : confidence cut-off 0.01 C : confidence cut-off 0.03 D : confidence cut-off 0.05 XTAG ERG Figure 4 : Precision and recall of the resulting SCFs using confidence cut-offs and frequency cut-off : the XTAG English grammar ( left ) the LinGO ERG ( right ) 0 1 0 0.2 0.4 0.6 0.8 1 Recall Precision A B C D A : frequency cut-off B : centroid cut-off* 0.05 C : centroid cut-off 0.05 D : confidence cut-off 0.05 0 1 0 0.2 0.4 0.6 0.8 1 Recall Precision A B C D A : frequency cut-off B : centroid cut-off* 0.05 C : centroid cut-off 0.05 D : confidence cut-off 0.05 XTAG ERG Figure 5 : Precision and recall of the resulting SCFs using confidence cut-off and centroid cut-off : the XTAG English grammar ( left ) the LinGO ERG ( right ) testing SCFs , we can estimate to what extent my method can preserve reliable SCFs for words unknown to the grammar .</sentence>
				<definiendum id="0">Recall Precision A B C</definiendum>
				<definiendum id="1">LinGO ERG ( right</definiendum>
				<definiens id="0">the XTAG English grammar ( left ) the LinGO ERG ( right ) 0 1 0 0.2 0.4 0.6 0.8 1 Recall Precision A B C D A : frequency cut-off B : centroid cut-off* 0.05 C : centroid cut-off 0.05 D : confidence cut-off 0.05 0 1 0 0.2 0.4 0.6 0.8 1 Recall Precision A B C D A</definiens>
			</definition>
</paper>

		<paper id="3018">
			<definition id="0">
				<sentence>Although the Web consists of mostly unstructured and loosely structured information , the available structured data is a valuable resource for question answering .</sentence>
				<definiendum id="0">Web</definiendum>
				<definiens id="0">consists of mostly unstructured and loosely structured information , the available structured data is a valuable resource for question answering</definiens>
			</definition>
			<definition id="1">
				<sentence>The CIA World Factbook is a database containing geographical , political , and economical profiles of all the countries in the world .</sentence>
				<definiendum id="0">CIA World Factbook</definiendum>
				<definiens id="0">a database containing geographical , political , and economical profiles of all the countries in the world</definiens>
			</definition>
			<definition id="2">
				<sentence>Gazetteers offer high precision answers , but have limited recall since they only cover a limited number of questions ( See Table 1 ) .</sentence>
				<definiendum id="0">Gazetteers</definiendum>
				<definiens id="0">offer high precision answers , but have limited recall since they only cover a limited number of questions</definiens>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>Support Vector Machines ( SVMs ) , using a combination of such kernels and the at feature kernel , classify PropBank predicate arguments with accuracy higher than the current argument classi cation stateof-the-art .</sentence>
				<definiendum id="0">Support Vector Machines ( SVMs</definiendum>
				<definiens id="0">using a combination of such kernels and the at feature kernel , classify PropBank predicate arguments with accuracy higher than the current argument classi cation stateof-the-art</definiens>
			</definition>
			<definition id="1">
				<sentence>Position : Indicates if the constituent , i.e. the potential argument , appears before or after the predicate in the sentence , e.g. after for Arg1 and before for Arg0 .</sentence>
				<definiendum id="0">Position</definiendum>
				<definiens id="0">appears before or after the predicate in the sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>Voice : This feature distinguishes between active or passive voice for the predicate phrase , e.g. active for every argument .</sentence>
				<definiendum id="0">Voice</definiendum>
				<definiens id="0">This feature distinguishes between active or passive voice for the predicate phrase</definiens>
			</definition>
			<definition id="3">
				<sentence>Predicate Word : This feature consists of two components : ( 1 ) the word itself , e.g. gives for all arguments ; and ( 2 ) the lemma which represents the verb normalized to lower case and in nitive form , e.g. give for all arguments .</sentence>
				<definiendum id="0">Predicate Word</definiendum>
			</definition>
			<definition id="4">
				<sentence>Given a vector space in &lt; n and a set of positive and negative points , SVMs classify vectors according to a separating hyperplane , H ( ~x ) = ~w ~x + b = 0 , where ~w 2 &lt; n and b 2 &lt; are learned by applying the Structural Risk Minimization principle ( Vapnik , 1995 ) . To apply the SVM algorithm to Predicate Argument Classi cation , we need a function : F ! &lt; n to map our features space F = ff1 ; : : ; fjFjg and our predicate/argument pair representation , Fp ; a = Fz , into &lt; n , such that : Fz ! ( Fz ) = ( 1 ( Fz ) ; : : ; n ( Fz ) ) From the kernel theory we have that : H ( ~x ) = X i=1 : :l i~xi ~x+b = X i=1 : :l i~xi ~x+b = = X i=1 : :l i ( Fi ) ( Fz ) + b : where , Fi 8i 2 f1 ; : : ; lg are the training instances and the product K ( Fi ; Fz ) = &lt; ( Fi ) ( Fz ) &gt; is the kernel function associated with the mapping .</sentence>
				<definiendum id="0">Structural Risk Minimization principle</definiendum>
				<definiens id="0">a set of positive and negative points , SVMs classify vectors according to a separating hyperplane</definiens>
			</definition>
			<definition id="5">
				<sentence>As suggested in Section 2 we can map them into vectors in &lt; n and evaluate implicitly the scalar product among them. Kernel ( PAK ) Given the semantic objects de ned in the previous section , we design a convolution kernel in a way similar to the parse-tree kernel proposed in ( Collins and Du y , 2002 ) . We divide our mapping in two steps : ( 1 ) from the semantic structure space F ( i.e. PAF or SCF objects ) to the set of all their possible sub-structures element in Fp ; a with an abuse of notation we use it to indicate the objects themselves. NP D N a talk NP D N NP D N a D N a talk NP D N NP D N VP V delivers a talk V delivers NP D N VP V a talk NP D N VP V NP D N VP V a NP D VP V talk N a NP D N VP V delivers talk NP D N VP V delivers NP D N VP V delivers NP VP V NP VP V delivers talk Figure 4 : All 17 valid fragments of the semantic structure associated with Arg 1 of Figure 2. F0 = ff01 ; : : ; f0jF0jg and ( 2 ) from F0 to &lt; jF0j. An example of features in F0 is given in Figure 4 where the whole set of fragments , F0deliver ; Arg1 , of the argument structure Fdeliver ; Arg1 , is shown ( see also Figure 2 ) . It is worth noting that the allowed sub-trees contain the entire ( not partial ) production rules. For instance , the sub-tree [ NP [ D a ] ] is excluded from the set of the Figure 4 since only a part of the production NP ! D N is used in its generation. However , this constraint does not apply to the production VP ! V NP PP along with the fragment [ VP [ V NP ] ] as the subtree [ VP [ PP [ ... ] ] ] is not considered part of the semantic structure. Thus , in step 1 , an argument structure Fp ; a is mapped in a fragment set F 0p ; a. In step 2 , this latter is mapped into ~x = ( x1 ; : : ; xjF0j ) 2 &lt; jF0j , where xi is equal to the number of times that f0i occurs in F0p ; a2. In order to evaluate K ( ( Fx ) ; ( Fz ) ) without evaluating the feature vector ~x and ~z we dene the indicator function Ii ( n ) = 1 if the substructure i is rooted at node n and 0 otherwise. It follows that i ( Fx ) = Pn2Nx Ii ( n ) , where Nx is the set of the Fx’s nodes. Therefore , the kernel can be written as : K ( ( Fx ) ; ( Fz ) ) = jF0jX i=1 ( X nx2Nx Ii ( nx ) ) ( X nz2Nz Ii ( nz ) ) = X nx2Nx X nz2Nz X i Ii ( nx ) Ii ( nz ) where Nx and Nz are the nodes in Fx and Fz , respectively. In ( Collins and Du y , 2002 ) , it has been shown that Pi Ii ( nx ) Ii ( nz ) = ( nx ; nz ) can be computed in O ( jNxj jNzj ) by the following recursive relation : ( 1 ) if the productions at nx and nz are di erent then ( nx ; nz ) = 0 ; 2A fragment can appear several times in a parse-tree , thus each fragment occurrence is considered as a di erent element in F 0p ; a. ( 2 ) if the productions at nx and nz are the same , and nx and nz are pre-terminals then ( nx ; nz ) = 1 ; ( 3 ) if the productions at nx and nz are the same , and nx and nz are not pre-terminals then ( nx ; nz ) = nc ( nx ) Y j=1 ( 1 + ( ch ( nx ; j ) ; ch ( nz ; j ) ) ) ; where nc ( nx ) is the number of the children of nx and ch ( n ; i ) is the i-th child of the node n. Note that as the productions are the same ch ( nx ; i ) = ch ( nz ; i ) . This kind of kernel has the drawback of assigning more weight to larger structures while the argument type does not strictly depend on the size of the argument ( Moschitti and Bejan , 2004 ) . To overcome this problem we can scale the relative importance of the tree fragments using a parameter for the cases ( 2 ) and ( 3 ) , i.e. ( nx ; nz ) = and ( nx ; nz ) = Qnc ( nx ) j=1 ( 1 + ( ch ( nx ; j ) ; ch ( nz ; j ) ) ) respectively. It is worth noting that even if the above equations de ne a kernel function similar to the one proposed in ( Collins and Du y , 2002 ) , the substructures on which it operates are di erent from the parse-tree kernel. For example , Figure 4 shows that structures such as [ VP [ V ] [ NP ] ] , [ VP [ V delivers ] [ NP ] ] and [ VP [ V ] [ NP [ DT ] [ N ] ] ] are valid features , but these fragments ( and many others ) are not generated by a complete production , i.e. VP ! V NP PP. As a consequence they would not be included in the parse-tree kernel of the sentence. Features In this section we compare standard features with the kernel based representation in order to derive useful indications for their use : First , PAK estimates a similarity between two argument structures ( i.e. , PAF or SCF ) by counting the number of sub-structures that are in common. As an example , the similarity between the two structures in Figure 2 , F '' delivers '' ; Arg0 and F '' delivers '' ; Arg1 , is equal to 1 since they have in common only the [ V delivers ] substructure. Such low value depends on the fact that di erent arguments tend to appear in di erent structures. On the contrary , if two structures di er only for a few nodes ( especially terminals or near terminal nodes ) the similarity remains quite high. For example , if we change the tense of the verb to deliver ( Figure 2 ) in delivered , the [ VP [ V delivers ] [ NP ] ] subtree will be transformed in [ VP [ VBD delivered ] [ NP ] ] , where the NP is unchanged. Thus , the similarity with the previous structure will be quite high as : ( 1 ) the NP with all sub-parts will be matched and ( 2 ) the small di erence will not highly affect the kernel norm and consequently the nal score. The above property also holds for the SCF structures. For example , in Figure 3 , KPAK ( ( Fflush ) ; ( Fbuckle ) ) is quite high as the two verbs have the same syntactic realization of their arguments. In general , at features do not possess this conservative property. For example , the Parse Tree Path is very sensible to small changes of parse-trees , e.g. two predicates , expressed in di erent tenses , generate two di erent Path features. Second , some information contained in the standard features is embedded in PAF : Phrase Type , Predicate Word and Head Word explicitly appear as structure fragments. For example , in Figure 4 are shown fragments like [ NP [ DT ] [ N ] ] or [ NP [ DT a ] [ N talk ] ] which explicitly encode the Phrase Type feature NP for the Arg 1 in Figure 2.b. The Predicate Word is represented by the fragment [ V delivers ] and the Head Word is encoded in [ N talk ] . The same is not true for SCF since it does not contain information about a speci c argument. SCF , in fact , aims to characterize the predicate with respect to the overall argument structures rather than a speci c pair &lt; p ; a &gt; .</sentence>
				<definiendum id="0">Nx</definiendum>
				<definiendum id="1">nc ( nx )</definiendum>
				<definiendum id="2">i )</definiendum>
				<definiendum id="3">PAK</definiendum>
				<definiens id="0">the number of times that f0i occurs in F0p ; a2. In order to evaluate K ( ( Fx ) ; ( Fz ) ) without evaluating the feature vector ~x and ~z we dene the indicator function Ii ( n ) = 1 if the substructure i is rooted at node n and 0 otherwise. It follows that i ( Fx ) = Pn2Nx Ii ( n ) , where</definiens>
				<definiens id="1">estimates a similarity between two argument structures</definiens>
			</definition>
			<definition id="6">
				<sentence>On the contrary , SCF lacks important information , thus , alone it may be used only to classify verbs in syntactic categories .</sentence>
				<definiendum id="0">SCF</definiendum>
				<definiens id="0">lacks important information</definiens>
			</definition>
</paper>

		<paper id="2002">
			<definition id="0">
				<sentence>Once the rule set has been selected , Theorem 1 shows how to select a question from the selected rule set that minimizesa22a24a23a26a25a28a27 .</sentence>
				<definiendum id="0">Theorem 1</definiendum>
				<definiens id="0">shows how to select a question from the selected rule set that minimizesa22a24a23a26a25a28a27</definiens>
			</definition>
			<definition id="1">
				<sentence>a22a29a23a26a25a71a27 is the expected length for the ordering with a0a69a61 at position a40 .</sentence>
				<definiendum id="0">a22a29a23a26a25a71a27</definiendum>
				<definiens id="0">the expected length for the ordering with a0a69a61 at position a40</definiens>
			</definition>
</paper>

		<paper id="3002">
			<definition id="0">
				<sentence>Bilingual word alignment is first introduced as an intermediate result in statistical machine translation ( SMT ) ( Brown et al. , 1993 ) .</sentence>
				<definiendum id="0">Bilingual word alignment</definiendum>
			</definition>
			<definition id="1">
				<sentence>These GTMS are usually employed to translate various documents such as user manuals , computer operation guides , and mechanical operation manuals .</sentence>
				<definiendum id="0">computer operation</definiendum>
				<definiens id="0">guides , and mechanical operation manuals</definiens>
			</definition>
			<definition id="2">
				<sentence>G S C S |S| |SS| G CG ∩ =precision ( 1 ) |S| |SS| C CG ∩ =recall ( 2 ) |||| ||*2 CG CG SS SS fmeasure + ∩ = ( 3 ) fmeasure SS SS AER CG CG −= + ∩ −= 1 |||| ||*2 1 ( 4 ) Method Precision Recall AER Ours 0.8363 0.7673 0.1997 Gen+Spec 0.8276 0.6758 0.2559 Gen 0.8668 0.6428 0.2618 Spec 0.8178 0.4769 0.3974 Table 1 .</sentence>
				<definiendum id="0">G S C S |S| |SS| G CG ∩ =precision</definiendum>
				<definiens id="0">||*2 CG CG SS SS fmeasure + ∩ = ( 3 ) fmeasure SS SS AER CG CG −= + ∩ −= 1 |||| ||*2 1 ( 4 ) Method Precision Recall AER</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>Type This paper Dienes &amp; DubeyP R f P R f PRO-NP 73.1 63.89 68.1 68.7 70.4 69.5 COMP-SBAR 82.6 83.1 82.8 93.8 78.6 85.5 COMP-WHNP 65.3 40.0 49.6 67.2 38.3 48.8 UNIT 95.4 91.8 93.6 99.1 92.5 95.7 For comparison we use the notation of Dienes and Dubey : PRO-NP for uncontrolled PROs ( nodes ‘*’ in the WSJ ) , COMP-SBAR for empty complementizers ( nodes ‘0’ with dependency label VP a0 SBAR ) , COMP-WHNP for empty relative pronouns ( nodes ‘0’ with dependency label X a0 SBAR , where X a0a1 VP ) and UNIT for empty units ( nodes ‘*U*’ ) .</sentence>
				<definiendum id="0">UNIT</definiendum>
				<definiens id="0">nodes ‘0’ with dependency label X a0 SBAR , where X a0a1 VP ) and</definiens>
			</definition>
</paper>

		<paper id="3016">
</paper>

		<paper id="2006">
			<definition id="0">
				<sentence>It follows the SERT ( SelfExplanation Reading Training ) methodology developed by McNamara ( in press ) as a way to improve high school students’ reading ability by teaching them to use active reading strategies in self-explaining difficult texts .</sentence>
				<definiendum id="0">McNamara (</definiendum>
				<definiens id="0">in press ) as a way to improve high school students’ reading ability by teaching them to use active reading strategies in self-explaining difficult texts</definiens>
			</definition>
			<definition id="1">
				<sentence>The Reporter provides the final summary of the total paraphrase matches , noting unmatched information in either the sentence or the explanation .</sentence>
				<definiendum id="0">Reporter</definiendum>
				<definiens id="0">provides the final summary of the total paraphrase matches , noting unmatched information in either the sentence or the explanation</definiens>
			</definition>
			<definition id="2">
				<sentence>A parse from the Link Grammar consists of triplets : starting word , an ending word , and a connector type between these two words .</sentence>
				<definiendum id="0">Link Grammar</definiendum>
				<definiens id="0">consists of triplets : starting word , an ending word</definiens>
			</definition>
			<definition id="3">
				<sentence>As can be easily seen , all sentences have a similar CG triplet of “ [ Person : $ ] fi ( Char ) fi [ Happy ] ” in their CGs .</sentence>
				<definiendum id="0">Char</definiendum>
			</definition>
			<definition id="4">
				<sentence>ExtrAns ( Extracting answers from technical texts ) by ( Molla et al , 2003 ) and ( Rinaldi et al , 2003 ) uses minimal logical forms ( MLF ) to represent both texts and questions .</sentence>
				<definiendum id="0">ExtrAns</definiendum>
				<definiendum id="1">MLF</definiendum>
				<definiens id="0">Extracting answers from technical texts</definiens>
			</definition>
			<definition id="5">
				<sentence>Automated Conceptual Graph Generator : is a C++ program that calls the Link Grammar API to get the parse result for the input sentence , and generates a CG .</sentence>
				<definiendum id="0">Automated Conceptual Graph Generator</definiendum>
				<definiens id="0">a C++ program that calls the Link Grammar API to get the parse result for the input sentence , and generates a CG</definiens>
			</definition>
</paper>

		<paper id="3015">
			<definition id="0">
				<sentence>OMOI ( feeling ) : ureshii ( glad ) , kanashii ( sad ) , shiawasena ( happy ) , … KANTEN ( viewpoint ) : igakutekina ( medical ) , rekishitekina ( historical ) , ... The complementary similarity measure ( CSM ) is used in a character recognition method for binary images which is robust against heavy noise or graphical designs ( Sawaki and Hagita , 1996 ) .</sentence>
				<definiendum id="0">OMOI</definiendum>
				<definiens id="0">used in a character recognition method for binary images which is robust against heavy noise or graphical designs</definiens>
			</definition>
			<definition id="1">
				<sentence>The CSM of F to T is defined as dcban tfdtfc tfbtfa dbca bcad TFCSM n i ii n i ii n i ii n i ii +++= −⋅−=−⋅= ⋅−=⋅= ++ − = ∑∑ ∑∑ == == , ) 1 ( ) 1 ( , ) 1 ( , ) 1 ( , ) ) ( ( ) , ( 11 11 The CSM of F to T represents the degree to which F includes T ; that is , the inclusion relation between the appearance patterns of two words .</sentence>
				<definiendum id="0">CSM of F to T</definiendum>
				<definiens id="0">CSM of F to T represents the degree to which F includes T ; that is , the inclusion relation between the appearance patterns of two words</definiens>
			</definition>
			<definition id="2">
				<sentence>Therefore , n is the number of adjectives in the corpus , a indicates the number of adjectives cooccurring with both abstract nouns , b and c indicate the number of adjectives co-occurring with either abstract noun , and d indicates the number of adjectives co-occurring with neither abstract noun .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of adjectives in the corpus , a indicates the number of adjectives cooccurring with both abstract nouns , b and c indicate the number of adjectives co-occurring with either abstract noun</definiens>
			</definition>
			<definition id="3">
				<sentence>OVLP of F and T is defined as ) , ( ) , ( ) , ( cabaMIN a TFMIN TF TFOVLP ++ == I The EDR Electronic Dictionary ( 1995 ) was developed for advanced processing of natural language by computers and is composed of eleven sub-dictionaries .</sentence>
				<definiendum id="0">cabaMIN</definiendum>
				<definiens id="0">a TFMIN TF TFOVLP ++ == I The EDR Electronic Dictionary ( 1995 ) was developed for advanced processing of natural language by computers and is composed of eleven sub-dictionaries</definiens>
			</definition>
			<definition id="4">
				<sentence>We express the pair as ( X , Y ) , where X is a hypernym of Y and Y is a hyponym of X. ties and reduce the pairs where the similarity is less than TH .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">Y</definiendum>
			</definition>
			<definition id="5">
				<sentence>A ) Choose a pair ( B , C ) where word B is the hypernym with the highest value .</sentence>
				<definiendum id="0">word B</definiendum>
				<definiens id="0">the hypernym with the highest value</definiens>
			</definition>
			<definition id="6">
				<sentence>B ) Choose a pair ( C , D ) where hyponym D is not contained in the current hierarchy and has the highest value in pairs where the last word of the current hierarchy C is a hypernym .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">not contained in the current hierarchy and has the highest value in pairs where the last word of the current hierarchy</definiens>
			</definition>
			<definition id="7">
				<sentence>These results show that CSM builds a deeper hierarchy than OVLP , though the number of hierarchies is less than OVLP .</sentence>
				<definiendum id="0">CSM</definiendum>
				<definiens id="0">builds a deeper hierarchy than OVLP , though the number of hierarchies is less than OVLP</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>The EM clustering algorithm ( Hofmann and Puzicha , 1998 ) used here is an unsupervised machine learning algorithm that has been applied in many NLP tasks , such as inducing a semantically labeled lexicon and determining lexical choice in machine translation ( Rooth et al. , 1998 ) , automatic acquisition of verb semantic classes ( Schulte im Walde , 2000 ) and automatic semantic labeling ( Gildea and Jurafsky , 2002 ) .</sentence>
				<definiendum id="0">EM clustering algorithm</definiendum>
				<definiens id="0">inducing a semantically labeled lexicon and determining lexical choice in machine translation ( Rooth et al. , 1998 ) , automatic acquisition of verb semantic classes</definiens>
			</definition>
			<definition id="1">
				<sentence>Therefore the joint probability of the observed variables ( features ) for each verb instance , i.e. , each parsed sentence containing the target verb , is defined in equation ( 1 ) , ∑ ∏ = = c m i im cfpcpfffp 1 21 ) | ( ) ( ) , ... , , ( ( 1 ) The i f �s are discrete-valued features that can take multiple values .</sentence>
				<definiendum id="0">i f �s</definiendum>
				<definiens id="0">joint probability of the observed variables ( features ) for each verb instance , i.e. , each parsed sentence containing the target verb</definiens>
			</definition>
			<definition id="2">
				<sentence>We have two Chinese electronic semantic dictionaries : the Hownet dictionary , which assigns 26,106 nouns to 346 semantic categories , and the Rocling dictionary , which assigns 4,474 nouns to 110 semantic categories .</sentence>
				<definiendum id="0">Hownet dictionary</definiendum>
				<definiens id="0">assigns 26,106 nouns to 346 semantic categories</definiens>
				<definiens id="1">assigns 4,474 nouns to 110 semantic categories</definiens>
			</definition>
			<definition id="3">
				<sentence>Assuming a verb has l senses , the clustering model assigns n instances of the verb into k clusters , i n is the size of the ith cluster , j n is the number of instances hand-tagged with the jth sense , and j i n is the number of instances with the jth sense in the ith cluster , purity is defined in equation ( 4 ) : ∑ = = k i j i j n n purity 1 max 1 ( 4 ) 10 The sense-tagged PDN data we used here are the same as in ( Dang et al. , 2002 ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">j i n</definiendum>
				<definiens id="0">the size of the ith cluster</definiens>
				<definiens id="1">the number of instances hand-tagged with the jth sense</definiens>
				<definiens id="2">the number of instances with the jth sense in the ith cluster</definiens>
			</definition>
			<definition id="4">
				<sentence>Treating the verb sense and the cluster as random variables S and C , the MI between them is defined in equation ( 5 ) : ∑∑ ∑ == = = l j k i j i j i j i cs nn nn n n cpsp csp cspCSMI 11 , log ) ( ) ( ) , ( log ) , ( ) , ( ( 5 ) MI ( S , C ) characterizes the reduction in uncertainty of one random variable S ( or C ) due to knowing the other variable C ( or S ) .</sentence>
				<definiendum id="0">C )</definiendum>
				<definiens id="0">characterizes the reduction in uncertainty of one random variable S ( or C ) due to knowing the other variable C ( or S )</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>Turney’s model introduces the further consideration of incorporating human-provided knowledge about language .</sentence>
				<definiendum id="0">Turney’s model</definiendum>
				<definiens id="0">introduces the further consideration of incorporating human-provided knowledge about language</definiens>
			</definition>
			<definition id="1">
				<sentence>^ ( w ) = log N ( w ; excellent ) =NexcellentN ( w ; poor ) =Npoor ( 1 ) Here , Na is the total number of sites on the Internet that contain an occurrence of a – a feature that can be a word type or a phrase .</sentence>
				<definiendum id="0">Na</definiendum>
				<definiens id="0">the total number of sites on the Internet that contain an occurrence of a – a feature that can be a word type or a phrase</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>A language model denes a probability distribution P ( X ) over the source sentences X , which do not contain repairs .</sentence>
				<definiendum id="0">language model</definiendum>
				<definiens id="0">denes a probability distribution P ( X ) over the source sentences X , which do not contain repairs</definiens>
			</definition>
			<definition id="1">
				<sentence>In the work reported here , X is a word string and Y is a speech transcription not containing punctuation or partial words .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiens id="0">a word string and</definiens>
				<definiens id="1">a speech transcription not containing punctuation or partial words</definiens>
			</definition>
			<definition id="2">
				<sentence>The channel model is a stochastic TAG-based transducer ; it is responsible for generating the repairs in the transcript Y , and it uses the ability of TAGs to straightforwardly model crossed dependencies .</sentence>
				<definiendum id="0">channel model</definiendum>
				<definiens id="0">a stochastic TAG-based transducer</definiens>
			</definition>
			<definition id="3">
				<sentence>The TAG denes a language whose vocabulary is the set of pairs ( [ f ; g ) ( [ f ; g ) , where is the vocabulary of the observed sentences Y .</sentence>
				<definiendum id="0">TAG</definiendum>
			</definition>
			<definition id="4">
				<sentence>A string Z in this language can be interpreted as a pair of strings ( Y ; X ) , where Y is the concatenation of the projection of the rst components of Z and X is the concatenation of the projection of the second components .</sentence>
				<definiendum id="0">string Z</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiendum id="2">X</definiendum>
				<definiens id="0">the concatenation of the projection of the rst components of Z and</definiens>
				<definiens id="1">the concatenation of the projection of the second components</definiens>
			</definition>
			<definition id="5">
				<sentence>The nonterminals in this grammar are of the form Nwx , Rwy : wx and I , where wx is a word appearing in the source string and wy is a word appearing in the observed string .</sentence>
				<definiendum id="0">wx</definiendum>
				<definiendum id="1">wy</definiendum>
				<definiens id="0">a word appearing in the source string</definiens>
				<definiens id="1">a word appearing in the observed string</definiens>
			</definition>
			<definition id="6">
				<sentence>The interregnum consists of the braced ( 1 ) Nwant a : a Na # 1 Pn ( repairja ) ( 2 ) Na ight : ight R ight : ight I # Pn ( repairj ight ) ( 3 ) NDenver on : on Non # 1 Pn ( repairjon ) ( 5 ) I uh I I mean Pi ( uhImean ) ( 1 ) R ight : ight to : ; Rto : to R ?</sentence>
				<definiendum id="0">interregnum</definiendum>
			</definition>
			<definition id="7">
				<sentence>Pn ( repairjW ) is the probability of a repair beginning after a word W in the source sentence X ; it is estimated from the training sentences with reparandums and interregnums removed .</sentence>
				<definiendum id="0">Pn</definiendum>
				<definiens id="0">the probability of a repair beginning after a word W in the source sentence X</definiens>
			</definition>
			<definition id="8">
				<sentence>Mi is the ith reparandum word and Ri is the corresponding repair word , so both of these range over [ f ; g. We de ne M0 and R0 to be source sentence word that preceded the repair ( which is ‘ $ ’ if the repair begins at the beginning of a sentence ) .</sentence>
				<definiendum id="0">Mi</definiendum>
				<definiendum id="1">Ri</definiendum>
				<definiens id="0">the ith reparandum word and</definiens>
			</definition>
			<definition id="9">
				<sentence>Pr ( TijM0i 1 ; R0i 1 ) is the probability of seeing repair type Ti following the reparandum word M0i 1 and repair word R0i 1 ; e.g. , Pr ( nonrepjBoston ; Denver ) is the probability of the repair ending when Boston is the last reparandum word and Denver is the last repair word .</sentence>
				<definiendum id="0">Denver</definiendum>
				<definiens id="0">the probability of seeing repair type Ti following the reparandum word M0i 1 and repair word R0i 1 ; e.g.</definiens>
				<definiens id="1">the probability of the repair ending when Boston is the last reparandum word</definiens>
				<definiens id="2">the last repair word</definiens>
			</definition>
			<definition id="10">
				<sentence>Pr ( MijTi = ins ; M0i 1 ; R0i ) is the probability that Mi is the word that is inserted into the reparandum ( i.e. , Ri = ; ) given that some word is substituted , and that the preceding reparandum and repair words are M0i 1 and R0i .</sentence>
				<definiendum id="0">R0i )</definiendum>
				<definiens id="0">the probability that Mi is the word that is inserted into the reparandum ( i.e. , Ri = ; ) given that some word is substituted , and that the preceding reparandum and repair words</definiens>
			</definition>
			<definition id="11">
				<sentence>Pr ( MijTi = subst ; M0i 1 ; R0i ) is the probability that Mi is the word that is substituted in the reparandum for R0i , given that some word is substituted .</sentence>
				<definiendum id="0">R0i )</definiendum>
				<definiens id="0">the probability that Mi is the word that is substituted in the reparandum for R0i , given that some word is substituted</definiens>
			</definition>
			<definition id="12">
				<sentence>This algorithm has n5 running time , where n is the length of the string .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the string</definiens>
			</definition>
			<definition id="13">
				<sentence>If nc is the number of reparandum words the model correctly classi ed , nt is the number of true reparandum words given by the manual annotations and nm is the number of words the model predicts to be reparandum words , then the precision is nc=nm , recall is nc=nt , and f is 2pr= ( p + r ) .</sentence>
				<definiendum id="0">nm</definiendum>
				<definiendum id="1">recall</definiendum>
				<definiendum id="2">f</definiendum>
				<definiens id="0">the number of true reparandum words given by the manual annotations and</definiens>
				<definiens id="1">the number of words the model predicts to be reparandum words</definiens>
			</definition>
</paper>

		<paper id="1081">
			<definition id="0">
				<sentence>Nonlinear principal components ( Diamantaras and Kung , 1996 ) may be defined as follows .</sentence>
				<definiendum id="0">Nonlinear principal components</definiendum>
			</definition>
			<definition id="1">
				<sentence>Then the lth nonlinear principal component of any test vector xt is defined as ylt = MX i=1 ^ li ( ( xi ) ( xt ) ) ( 6 ) where ^ li is the lth element of ^ l .</sentence>
				<definiendum id="0">lth nonlinear principal component of any test vector xt</definiendum>
				<definiens id="0">ylt = MX i=1 ^ li ( ( xi ) ( xt ) ) ( 6 ) where ^ li is the lth element of ^ l</definiens>
			</definition>
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>The most widely studied way of addressing these constructions within TAG-based linguistic theory ( Kroch and Joshi , 1987 ; Kroch , 1989 ; Frank , 2002 ) has been to assume some sort of multi-component adjoining ( MCTAG ( Weir , 1988 ) ) , in which elementary structures are factored into sets of trees that are adjoined simultaneously at multiple points .</sentence>
				<definiendum id="0">MCTAG</definiendum>
				<definiens id="0">in which elementary structures are factored into sets of trees that are adjoined simultaneously at multiple points</definiens>
			</definition>
</paper>

		<paper id="1088">
			<definition id="0">
				<sentence>LSA is a statistical method that is ordinarily trained on words only ; FLSA adds to LSA the richness of the many other linguistic features that a corpus may be labeled with .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiendum id="1">FLSA</definiendum>
				<definiens id="0">a statistical method that is ordinarily trained on words only</definiens>
			</definition>
			<definition id="1">
				<sentence>LSA is based on Single Value Decomposition ( SVD ) , a mathematical technique that causes the semantic space to be arranged so as to reflect the major associative patterns in the data .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">a mathematical technique that causes the semantic space to be arranged so as to reflect the major associative patterns in the data</definiens>
			</definition>
			<definition id="2">
				<sentence>With respect to a baseline of choosing the most frequent dialogue act ( DA ) , LSA reduces error rates between 33 % and 52 % , and FLSA reduces error rates between 60 % and 78 % .</sentence>
				<definiendum id="0">LSA reduces error rates</definiendum>
				<definiendum id="1">FLSA</definiendum>
				<definiens id="0">reduces error rates between 60 %</definiens>
			</definition>
			<definition id="3">
				<sentence>The input to LSA is a Word-Document matrix W with a row for each word , and a column for each document ( for us , a document is a unit , e.g. an utterance , tagged with a DA ) .</sentence>
				<definiendum id="0">document</definiendum>
				<definiens id="0">a unit</definiens>
			</definition>
			<definition id="4">
				<sentence>The product of the resulting matrices is a matrix ˆW of rank k which is approximately equal to W ; it is the matrix of rank k with the best possible least-squares-fit to W. The number of dimensions k retained by LSA is an empirical question .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">an empirical question</definiens>
			</definition>
			<definition id="5">
				<sentence>The DA annotation augments a basic tag such as statement along several dimensions , such as whether the statement describes a psychological state of the speaker .</sentence>
				<definiendum id="0">DA annotation</definiendum>
				<definiens id="0">augments a basic tag such as statement along several dimensions</definiens>
			</definition>
			<definition id="6">
				<sentence>CallHome37 maintains some subcategorizations , e.g. whether a question is yes/no or rhetorical .</sentence>
				<definiendum id="0">CallHome37</definiendum>
				<definiens id="0">maintains some subcategorizations</definiens>
			</definition>
			<definition id="7">
				<sentence>The MapTask coding scheme uses 13 DAs ( called moves ) , that include : Instruct ( a request that the partner carry out an action ) , Explain ( one of the partners states some information that was not explicitly elicited by the other ) , Queryyn/-w , Acknowledge , Reply-y/-n/-w and others .</sentence>
				<definiendum id="0">DAs</definiendum>
				<definiens id="0">a request that the partner carry out an action</definiens>
			</definition>
			<definition id="8">
				<sentence>DIAG-NLP is a corpus of computer mediated tutoring dialogues between a tutor and a student who is diagnosing a fault in a mechanical system with a tutoring system built with the DIAG authoring tool ( Towne , 1997 ) .</sentence>
				<definiendum id="0">DIAG-NLP</definiendum>
			</definition>
			<definition id="9">
				<sentence>S’s entropy H can be seen as an indicator of how uncertain the outcome of the classification is , and is given by : H ( S ) = − nsummationdisplay i=1 pilog2 ( pi ) ( 1 ) If feature F divides S into k subsets S1 ... Sk , then IG is the expected reduction in entropy caused by partitioning the data according to the values of F : IG ( S , A ) = H ( S ) − ksummationdisplay i=1 |Si| |S| H ( Si ) ( 2 ) In our case , we first computed the entropy of the corpora with respect to the classification induced by the DA tags ( see Table 6 , which also includes the LSA accuracy for convenience ) .</sentence>
				<definiendum id="0">S’s entropy H</definiendum>
				<definiendum id="1">IG</definiendum>
				<definiens id="0">an indicator of how uncertain the outcome of the classification is , and is given by : H ( S ) = − nsummationdisplay i=1 pilog2 ( pi</definiens>
				<definiens id="1">the expected reduction in entropy caused by partitioning the data according to the values of F : IG ( S , A ) = H ( S ) − ksummationdisplay</definiens>
			</definition>
</paper>

		<paper id="3031">
			<definition id="0">
				<sentence>The Natural Language Toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing .</sentence>
				<definiendum id="0">Natural Language Toolkit</definiendum>
				<definiens id="0">a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing</definiens>
			</definition>
			<definition id="1">
				<sentence>NLTK is written in Python and distributed under the GPL open source license .</sentence>
				<definiendum id="0">NLTK</definiendum>
			</definition>
			<definition id="2">
				<sentence>As an object-oriented language , Python permits data and methods to be encapsulated and re-used easily .</sentence>
				<definiendum id="0">Python</definiendum>
				<definiens id="0">permits data and methods to be encapsulated and re-used easily</definiens>
			</definition>
			<definition id="3">
				<sentence>Each Token instance represents a unit of text such as a word , sentence , or document , and is defined by a ( partial ) mapping from property names to values .</sentence>
				<definiendum id="0">Token instance</definiendum>
				<definiens id="0">a unit of text such as a word , sentence , or document , and</definiens>
			</definition>
			<definition id="4">
				<sentence>In addition to the Token class and its derivatives , NLTK defines a variety of other data types .</sentence>
				<definiendum id="0">NLTK</definiendum>
				<definiens id="0">defines a variety of other data types</definiens>
			</definition>
			<definition id="5">
				<sentence>NLTK includes the following modules : cfg , corpus , draw ( cfg , chart , corpus , featurestruct , fsa , graph , plot , rdparser , srparser , tree ) , eval , featurestruct , parser ( chart , chunk , probabilistic ) , probability , sense , set , stemmer ( porter ) , tagger , test , token , tokenizer , tree , and util .</sentence>
				<definiendum id="0">NLTK</definiendum>
				<definiens id="0">cfg , corpus , draw ( cfg , chart , corpus , featurestruct , fsa , graph , plot , rdparser , srparser , tree ) , eval , featurestruct , parser ( chart , chunk , probabilistic ) , probability , sense , set , stemmer ( porter ) , tagger , test , token , tokenizer , tree , and util</definiens>
			</definition>
			<definition id="6">
				<sentence>The full distribution consists of four packages : the Python source code ( nltk ) ; the corpora ( nltk-data ) ; the documentation ( nltk-docs ) ; and third-party contributions ( nltk-contrib ) .</sentence>
				<definiendum id="0">full distribution</definiendum>
			</definition>
			<definition id="7">
				<sentence>NLTK is an open source project , and we welcome any contributions .</sentence>
				<definiendum id="0">NLTK</definiendum>
				<definiens id="0">an open source project</definiens>
			</definition>
			<definition id="8">
				<sentence>NLTK is a broad-coverage natural language toolkit that provides a simple , extensible , uniform framework for assignments , demonstrations and projects .</sentence>
				<definiendum id="0">NLTK</definiendum>
				<definiens id="0">a broad-coverage natural language toolkit that provides a simple , extensible , uniform framework for assignments , demonstrations and projects</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>Subcategorisation requirements are enforced through semantic forms specifying the governable grammatical functions required by a particular predicate ( e.g. FOCUS〈 ( ↑ SUBJ ) ( ↑ OBLon ) 〉 ) .</sentence>
				<definiendum id="0">Subcategorisation requirements</definiendum>
			</definition>
			<definition id="1">
				<sentence>F-structures are feature structures which represent abstract syntactic information , approximating to basic predicate-argumentmodifier structures .</sentence>
				<definiendum id="0">F-structures</definiendum>
				<definiens id="0">feature structures which represent abstract syntactic information , approximating to basic predicate-argumentmodifier structures</definiens>
			</definition>
			<definition id="2">
				<sentence>Distinguishing between argument and adjunct is an inherent step in the automatic assignment of functional annotations .</sentence>
				<definiendum id="0">adjunct</definiendum>
			</definition>
			<definition id="3">
				<sentence>The Penn Treebank employs a rich arsenal of traces and empty productions ( nodes which do not realise any lexical material ) to co-index displaced material with the position where it should be interpreted semantically .</sentence>
				<definiendum id="0">Penn Treebank</definiendum>
				<definiens id="0">employs a rich arsenal of traces and empty productions ( nodes which do not realise any lexical material ) to co-index displaced material with the position where it should be interpreted semantically</definiens>
			</definition>
			<definition id="4">
				<sentence>Adjuncts ( e.g. ADJ , APP etc ) are not included in the semantic forms .</sentence>
				<definiendum id="0">Adjuncts</definiendum>
				<definiens id="0">e.g. ADJ , APP etc ) are not included in the semantic forms</definiens>
			</definition>
			<definition id="5">
				<sentence>Just as OBL includes the prepositional head of the PP , PART includes the actual particle which occurs e.g. add ( [ subj , obj , part : up ] ) .</sentence>
				<definiendum id="0">PART</definiendum>
			</definition>
			<definition id="6">
				<sentence>We have extracted frames from one domain ( the WSJ ) whereas COMLEX was built using examples from the San Jose Mercury News , the Brown Corpus , several literary works from the Library of America , scientific abstracts from the U.S. Department of Energy , and the WSJ .</sentence>
				<definiendum id="0">WSJ</definiendum>
				<definiens id="0">the Brown Corpus , several literary works from the Library of America , scientific abstracts from the U.S. Department of Energy , and the WSJ</definiens>
			</definition>
			<definition id="7">
				<sentence>As COMLEX does not provide explicit passive entries , we applied Lexical Redundancy Rules ( Kaplan and Bresnan , 1982 ) to automatically convert the active COMLEX frames to their passive counterparts .</sentence>
				<definiendum id="0">COMLEX</definiendum>
				<definiens id="0">frames to their passive counterparts</definiens>
			</definition>
</paper>

		<paper id="3034">
			<definition id="0">
				<sentence>Leta31a32a6a34a33a35a1a2a16a37a36a37a36a37a36a38a16a1a4a40a39 , and a41 denotes the set of possible classes .</sentence>
				<definiendum id="0">a41</definiendum>
				<definiens id="0">the set of possible classes</definiens>
			</definition>
</paper>

		<paper id="3025">
			<definition id="0">
				<sentence>Here , the term semantic orientation ( SO ) ( Hatzivassiloglou and McKeown , 2002 ) refers to a real number measure of the positive or negative sentiment expressed by a word or phrase .</sentence>
				<definiendum id="0">semantic orientation</definiendum>
				<definiens id="0">a real number measure of the positive or negative sentiment expressed by a word or phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>PMI is defined by Church and Hanks ( 1989 ) as follows : a0a2a1a4a3a6a5a8a7a10a9a12a11a13a7a15a14a17a16a19a18a21a20a23a22a25a24 a14a27a26a29a28 a5a8a7a10a9a19a30a31a7a15a14a17a16 a28 a5a8a7 a9 a16 a28 a5a8a7 a14 a16a33a32 ( 1 ) where a28 a5a8a7a10a9a19a30a31a7a15a14a12a16 is the probability that a7a34a9 and a7a35a14 co-occur .</sentence>
				<definiendum id="0">PMI</definiendum>
			</definition>
			<definition id="2">
				<sentence>The dataset consists of 100 record reviews from the Pitchfork Media online record review publication,2 topic-annotated by hand .</sentence>
				<definiendum id="0">dataset</definiendum>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>It will be important in what follows to see that such a representation is isomorphic ( in terms of strong generative capacity ) to a restricted form of phrase structure grammar , where the set of terminals and nonterminals is identical , and every rule is of the form X → X Y or X → Y X ( Miller , 1999 ) , giving the isomorphic representation of figure 1 ( a ) shown in figure 1 ( b ) .1 Depending on the model , part-of1Strictly , such phrase structure trees are isomorphic not to flat dependency structures , but to specific derivations of those NN Factory NNS payrolls VBD fell IN in NN September ROOT VBD NNS NN Factory NNS payrolls VBD VBD fell IN IN in NN September S NP NN Factory NNS payrolls VP VBD fell PP IN in NN September ( a ) Classical Dependency Structure ( b ) Dependency Structure as CF Tree ( c ) CFG Structure Figure 1 : Three kinds of parse structures .</sentence>
				<definiendum id="0">isomorphic</definiendum>
				<definiens id="0">isomorphic not to flat dependency structures</definiens>
				<definiens id="1">VBD VBD fell IN IN in NN September S NP NN Factory NNS payrolls VP VBD fell PP IN in NN September ( a ) Classical Dependency Structure ( b ) Dependency Structure as CF Tree ( c ) CFG Structure Figure 1 : Three kinds of parse structures</definiens>
			</definition>
			<definition id="1">
				<sentence>WSJ is the entire Penn English Treebank WSJ portion .</sentence>
				<definiendum id="0">WSJ</definiendum>
				<definiens id="0">the entire Penn English Treebank WSJ portion</definiens>
			</definition>
			<definition id="2">
				<sentence>WSJ10 is the subset of sentences which contained 10 words or less after the removal of punctuation .</sentence>
				<definiendum id="0">WSJ10</definiendum>
				<definiens id="0">the subset of sentences which contained 10 words or less after the removal of punctuation</definiens>
			</definition>
			<definition id="3">
				<sentence>CTB10 is the sentences of the same length from the Penn Chinese treebank ( v3 ) .</sentence>
				<definiendum id="0">CTB10</definiendum>
				<definiens id="0">the sentences of the same length from the Penn Chinese treebank ( v3 )</definiens>
			</definition>
			<definition id="4">
				<sentence>NEGRA10 is the same , for the German NEGRA corpus , based on the supplied conversion of the NEGRA corpus into Penn treebank format .</sentence>
				<definiendum id="0">NEGRA10</definiendum>
				<definiens id="0">the same , for the German NEGRA corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>The NEGRA corpus , however , does supply hand-annotated dependency structures .</sentence>
				<definiendum id="0">NEGRA corpus</definiendum>
				<definiens id="0">does supply hand-annotated dependency structures</definiens>
			</definition>
			<definition id="6">
				<sentence>For example , to re-estimate an entry of PSTOP ( STOP|h , left , non-adj ) according to a current model Theta1 , we calculate two quantities.5 The first is the ( expected ) number of trees headed by hceilingright whose rightmost edge i is strictly left of h. The second is the number of trees headed by ceilinglefthceilingright with rightmost edge i strictly left of h. The ratio is the MLE of that local probability factor : PSTOP ( STOP|h , left , non-adj ) = summationtext s∈S summationtext i &lt; loc ( h ) summationtext k c ( hceilingright : i , k ) summationtext s∈S summationtext i &lt; loc ( h ) summationtext k c ( ceilinglefthceilingright : i , k ) This can be intuitively thought of as the relative number of times a tree headed by h had already taken at least one argument to the left , had an opportunity to take another , but didn’t.6 Initialization is important to the success of any local search procedure .</sentence>
				<definiendum id="0">k c ( hceilingright</definiendum>
				<definiendum id="1">k c ( ceilinglefthceilingright : i</definiendum>
				<definiens id="0">the ( expected ) number of trees headed by hceilingright whose rightmost edge i</definiens>
				<definiens id="1">the relative number of times a tree headed by h had already taken at least one argument to the left</definiens>
			</definition>
			<definition id="7">
				<sentence>The CCM works as follows .</sentence>
				<definiendum id="0">CCM</definiendum>
			</definition>
</paper>

		<paper id="3027">
			<definition id="0">
				<sentence>In this formula , V means a set of vocabulary , N is the size of the contextual window that is an integer , and C means a set of corpus .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">C</definiendum>
				<definiens id="0">means a set of vocabulary ,</definiens>
				<definiens id="1">the size of the contextual window that is an integer</definiens>
			</definition>
			<definition id="1">
				<sentence>, , { ( ) ( x Iiixxg ∈= is a word sense assignment function that gives the word senses numbered i of the word x. I x is the word sense indexing function of x that gives an index to each sense of the word x. All contextual words x i ±j of a central word x have their own contextual words in their collocation , and they also have multiple senses .</sentence>
				<definiendum id="0">{ ( ) ( x Iiixxg ∈=</definiendum>
			</definition>
			<definition id="2">
				<sentence>Followings are the used clustering methods : z K-means clustering ( K ) ( Ray and Turi , 1999 ) z Buckshot ( B ) ( Jensen , Beitzel , Pilotto , Goharian and Frieder , 2002 ) z Committee based clustering ( CBC ) ( Patrick and Lin , 2002 ) z Markov clustering ( M1 , M2 ) 1 ( Stijn van Dongen , 2000 ) z Fuzzy clustering ( F1 , F2 ) 2 ( Song , Cao and Bruza , 2003 ) Used clustering methods cover both the popularity and the variety of the algorithms – soft and hard clustering and graph clustering etc .</sentence>
				<definiendum id="0">Followings</definiendum>
			</definition>
			<definition id="3">
				<sentence>WC and WF are the average number of senses by the part of speech .</sentence>
				<definiendum id="0">WF</definiendum>
				<definiens id="0">the average number of senses by the part of speech</definiens>
			</definition>
			<definition id="4">
				<sentence>Wortchartz is the collocation dictionary with the assumption that Collocation of a word expresses 7 English lexical sample for the same central words the meaning of the word ( Heyer , Quasthoff and Wolff , 2001 ) .</sentence>
				<definiendum id="0">Wortchartz</definiendum>
				<definiens id="0">the collocation dictionary with the assumption that Collocation of a word expresses 7 English lexical sample for the same central words the meaning of the word</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>A weighted finite-state transducer T is an 8-tuple T = ( Σ , ∆ , Q , I , F , E , λ , ρ ) where Σ is the finite input alphabet of the transducer , ∆ is the finite output alphabet , Q is a finite set of states , I ⊆ Q the set of initial states , F ⊆ Q the set of final states , E ⊆ Q × ( Σ ∪ { epsilon1 } ) × ( ∆ ∪ { epsilon1 } ) × R × Q a finite set of transitions , λ : I → R the initial weight function , and ρ : F → R the final weight function mapping F to R. In our statistical framework , the weights can be interpreted as log-likelihoods , thus there are added along a path .</sentence>
				<definiendum id="0">Σ</definiendum>
				<definiendum id="1">Q</definiendum>
				<definiens id="0">an 8-tuple T = ( Σ , ∆ , Q , I , F , E , λ , ρ ) where</definiens>
				<definiens id="1">the finite output alphabet</definiens>
			</definition>
			<definition id="1">
				<sentence>Since we use the standard Viterbi approximation , the weight associated by T to a pair of strings ( x , y ) ∈ Σ∗ × ∆∗ is given by : [ [ T ] ] ( x , y ) = min pi∈R ( I , x , y , F ) λ [ p [ pi ] ] + w [ pi ] + ρ [ n [ pi ] ] where R ( I , x , y , F ) denotes the set of paths from an initial state p ∈ I to a final state q ∈ F with input label x and output label y , w [ pi ] the weight of the path pi , λ [ p [ pi ] ] the initial weight of the origin state of pi , and ρ [ n [ pi ] ] the final weight of its destination .</sentence>
				<definiendum id="0">R</definiendum>
				<definiendum id="1">F )</definiendum>
				<definiens id="0">the weight associated by T to a pair of strings ( x , y ) ∈ Σ∗ × ∆∗ is given by : [ [ T ] ] ( x , y ) = min pi∈R ( I , x , y</definiens>
				<definiens id="1">the set of paths from an initial state p ∈ I to a final state q ∈ F with input label x and output label y , w [ pi ] the weight of the path pi , λ [ p [ pi ] ] the initial weight of the origin state of pi , and ρ [ n [ pi ] ] the final weight of its destination</definiens>
			</definition>
			<definition id="2">
				<sentence>( c ) T1 ◦T2 , the result of the composition of T1 and T2 .</sentence>
				<definiendum id="0">c ) T1 ◦T2</definiendum>
				<definiens id="0">the result of the composition of T1 and T2</definiens>
			</definition>
			<definition id="3">
				<sentence>The composition of two transducers T1 and T2 is a weighted transducer denoted by T1 ◦T2 and defined by : [ [ T1 ◦T2 ] ] ( x , y ) = min z∈∆∗ { [ [ T1 ] ] ( x , z ) + [ [ T2 ] ] ( z , y ) } There exists a simple algorithm for constructing T = T1 ◦ T2 from T1 and T2 ( Pereira and Riley , 1997 ; Mohri et al. , 1996 ) .</sentence>
				<definiendum id="0">T2</definiendum>
				<definiens id="0">y ) } There exists a simple algorithm for constructing T = T1 ◦ T2 from T1 and T2 ( Pereira and Riley , 1997</definiens>
			</definition>
			<definition id="4">
				<sentence>Figure 1 ( c ) gives the resulting of the composition of the weighted transducers given figure 2 ( a ) and ( b ) .</sentence>
				<definiendum id="0">c )</definiendum>
				<definiendum id="1">figure 2</definiendum>
				<definiens id="0">gives the resulting of the composition of the weighted transducers given</definiens>
			</definition>
			<definition id="5">
				<sentence>Let W be the weighted automaton obtained by composition of p with ˆG and projection on the output : W = Π2 ( p◦ ˆG ) ( 7 ) W represents the set of candidate unit sequences with their respective grammar costs .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">represents the set of candidate unit sequences with their respective grammar costs</definiens>
			</definition>
			<definition id="6">
				<sentence>A state s1 in p corresponds in the composed transducer p◦ ˆG to a set of states ( s1 , s2 ) , s2 ∈ S2 , where S2 is a subset of the states of ˆG .</sentence>
				<definiendum id="0">S2</definiendum>
				<definiens id="0">a subset of the states of ˆG</definiens>
			</definition>
			<definition id="7">
				<sentence>Thus , at each state identified with h in ˆG , a transition with input label x is pruned when the following condition holds : summationdisplay w∈Xxh log ( tildewideP ( w|h ) ) − summationdisplay w∈Xxhprime log ( αhtildewideP ( w|hprime ) ) ≤ γc ( hw ) where hprime is the backoff sequence associate with h and Xxk is the set of output labels of all the outgoing transitions with input label x of the state identified with k. We used the AT &amp; T Natural Voices Product speech synthesis system to synthesize 107,987 AP news articles , generating a large corpus of 8,731,662 unit sequences representing a total of 415,227,388 units .</sentence>
				<definiendum id="0">tildewideP</definiendum>
				<definiendum id="1">αhtildewideP</definiendum>
				<definiendum id="2">hprime</definiendum>
				<definiendum id="3">Xxk</definiendum>
				<definiens id="0">the set of output labels of all the outgoing transitions with input label x of the state identified with k. We used the AT &amp; T Natural Voices Product speech synthesis system to synthesize 107,987 AP news articles</definiens>
			</definition>
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>; empty ) is a stack of state tuples of T1 and T2 with push , pop and empty test operations .</sentence>
				<definiendum id="0">empty )</definiendum>
				<definiens id="0">a stack of state tuples of T1 and T2 with push , pop and empty test operations</definiens>
			</definition>
			<definition id="1">
				<sentence>Therefore the RWTH FSA toolkit offers alphabets that define mappings between strings and label indices .</sentence>
				<definiendum id="0">RWTH FSA toolkit</definiendum>
				<definiens id="0">offers alphabets that define mappings between strings and label indices</definiens>
			</definition>
			<definition id="2">
				<sentence>class Alphabet f public : virtual LabelId begin ( ) ; virtual LabelId end ( ) ; virtual LabelId next ( LabelId ) ; virtual string getSymbol ( LabelId ) ; g ; Figure 4 : Pseudo-C++ code fragment for the abstract datatype of alphabets .</sentence>
				<definiendum id="0">virtual string getSymbol</definiendum>
				<definiens id="0">Pseudo-C++ code fragment for the abstract datatype of alphabets</definiens>
			</definition>
			<definition id="3">
				<sentence>The current implementation of the toolkit offers a wide range of well-known algorithms defined on weighted finite-state transducers : † basic operations sort ( by input labels , output labels or by tocompose ( T1 ; T2 ) = simple-compose ( cache ( sort-output ( map-output ( T1 ; AT2 ; I ) ) ) , cache ( sort-input ( T2 ) ) ) Figure 3 : Optimized composition where AT2 ; I denotes the input alphabet of T2 : Six algorithmic transducers are used to gain maximum efficiency .</sentence>
				<definiendum id="0">I</definiendum>
				<definiens id="0">Optimized composition where AT2 ;</definiens>
			</definition>
			<definition id="4">
				<sentence>The command-line interface is a single executable and uses a stack-based execution model ( postfix notation ) for the application of operations .</sentence>
				<definiendum id="0">command-line interface</definiendum>
				<definiens id="0">a single executable and uses a stack-based execution model ( postfix notation ) for the application of operations</definiens>
			</definition>
			<definition id="5">
				<sentence>This segmentation can be directly calculated from the alignments A : Then we can formulate the problem of finding the best translation ˆeI1 of a source sentence as follows : ˆeI1 = argmax eI1 Pr ( fJ1 ; eI1 ) … argmax A ; epJp1 Pr ( fJ1 ; epJp1 ) = argmax A ; epJp1 Y fj : j=1 : :J Pr ( fj ; epjjfj¡11 ; epj¡1p1 ) … argmax A ; epJp1 Y fj : j=1 : :J Pr ( fj ; epjjfj¡1j¡n ; epj¡1pj¡n ) The last line suggests to solve the translation problem by estimating a language model on a bilanguage ( see also ( Bangalore and Riccardi , 2000 ; Casacuberta et al. , 2001 ) ) .</sentence>
				<definiendum id="0">; epJp1 Pr</definiendum>
				<definiens id="0">ˆeI1 = argmax eI1 Pr ( fJ1 ; eI1 ) … argmax A</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>3Medical Subject Headings ( MeSH ) is a controlled vocabulary for manually annoting PubMed articles .</sentence>
				<definiendum id="0">MeSH</definiendum>
				<definiens id="0">a controlled vocabulary for manually annoting PubMed articles</definiens>
			</definition>
			<definition id="1">
				<sentence>The taxonomy consists of gene names , cue words relevant for entity recognition , and classes of verbs for relation extraction .</sentence>
				<definiendum id="0">taxonomy</definiendum>
				<definiens id="0">consists of gene names , cue words relevant for entity recognition , and classes of verbs for relation extraction</definiens>
			</definition>
			<definition id="2">
				<sentence>The GENIA 3.0 corpus consists of PubMed abstracts and has 466,179 manually annotated tokens .</sentence>
				<definiendum id="0">GENIA 3.0 corpus</definiendum>
				<definiens id="0">consists of PubMed abstracts and has 466,179 manually annotated tokens</definiens>
			</definition>
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>TT2 is an innovative tool for speeding up and facilitating the work of translators by automatically suggesting translation completions .</sentence>
				<definiendum id="0">TT2</definiendum>
				<definiens id="0">an innovative tool for speeding up and facilitating the work of translators by automatically suggesting translation completions</definiens>
			</definition>
			<definition id="1">
				<sentence>TransType2 ( TT2 ) 1 is an innovative tool for speeding up and facilitating the work of translators by automatically suggesting translation completions .</sentence>
				<definiendum id="0">TransType2</definiendum>
				<definiens id="0">an innovative tool for speeding up and facilitating the work of translators by automatically suggesting translation completions</definiens>
			</definition>
			<definition id="2">
				<sentence>The project is an extension of the TransType project that was developed from 1997 to 2000 by the RALI at Université de Montréal ( Foster 1997 , Langlais 2002 ) , which demonstrated the interest of target text mediated computer aided translation .</sentence>
				<definiendum id="0">TransType project</definiendum>
			</definition>
			<definition id="3">
				<sentence>TT2 is a RTD project funded by the European Commission under the Information Society Technologies Programme and includes five European partners : Atos Origin ( Spain ) : administrative and technical coordinator , system design and integration .</sentence>
				<definiendum id="0">TT2</definiendum>
				<definiens id="0">a RTD project funded by the European Commission under the Information Society Technologies Programme and includes five European partners : Atos Origin ( Spain ) : administrative and technical coordinator , system design and integration</definiens>
			</definition>
			<definition id="4">
				<sentence>TransType is a tool that observes a translator as he or she is typing , tries to predict what will be typed next and displays its predictions to the user .</sentence>
				<definiendum id="0">TransType</definiendum>
				<definiens id="0">a tool that observes a translator as he or she is typing</definiens>
			</definition>
			<definition id="5">
				<sentence>The target window is a normal text editing window , except that after each character typed by the user , the system displays a pop-up menu of suggestions for completing the current input .</sentence>
				<definiendum id="0">target window</definiendum>
				<definiens id="0">a normal text editing window , except that after each character typed by the user , the system displays a pop-up menu of suggestions for completing the current input</definiens>
			</definition>
			<definition id="6">
				<sentence>The TT2 system consists of two major subsystems that interact closely : user interface ( UI ) , written in Java , provides the typing and pointing modalities ; a second UI supplements those with speech for operating the prototype via short commands uttered by the user .</sentence>
				<definiendum id="0">TT2 system</definiendum>
				<definiens id="0">consists of two major subsystems that interact closely : user interface ( UI ) , written in Java , provides the typing and pointing modalities ; a second UI supplements those with speech for operating the prototype via short commands uttered by the user</definiens>
			</definition>
			<definition id="7">
				<sentence>prediction engine ( PE ) , written in C/C++ , of which there are multiple realizations available , several per language pair and specific domain ( either technical documentation , EC official documents or Hansards ) .</sentence>
				<definiendum id="0">PE</definiendum>
				<definiens id="0">written in C/C++ , of which there are multiple realizations available , several per language pair and specific domain ( either technical documentation , EC official documents or Hansards )</definiens>
			</definition>
			<definition id="8">
				<sentence>to work with , the UI produces a list of text segments ( sentences ) and displays them in the source text pane of the interface .</sentence>
				<definiendum id="0">UI</definiendum>
				<definiens id="0">produces a list of text segments ( sentences ) and displays them in the source text pane of the interface</definiens>
			</definition>
			<definition id="9">
				<sentence>TT2 is the outcome of a successful cooperation between European countries and Canada to develop an innovative approach to machine aided translation .</sentence>
				<definiendum id="0">TT2</definiendum>
				<definiens id="0">the outcome of a successful cooperation between European countries</definiens>
			</definition>
			<definition id="10">
				<sentence>TT2 is a RTD project funded by the European Commission under the Information Society Technologies Programme ( IST-2001-32091 ) .</sentence>
				<definiendum id="0">TT2</definiendum>
			</definition>
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>Also , attribute classi cation is a hard problem and there is no existing classi cation scheme that can be used for open domains like newswire ; for example , WordNet ( Miller et al. , 1993 ) organises adjectives as concepts that are related by the non-hierarchical relations of synonymy and antonymy ( unlike nouns that are related through hierarchical links such as hyponymy , hypernymy and metonymy ) .</sentence>
				<definiendum id="0">attribute classi cation</definiendum>
				<definiens id="0">open domains like newswire ; for example</definiens>
			</definition>
			<definition id="1">
				<sentence>The idea is that if X is a synonym of Y and Y is a synonym of Z , then X is likely to be similar to Z. The degree of similarity between two adjectives depends on how many steps must be made through WordNet synonymy lists to get from one to the other .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">a synonym of Z</definiens>
			</definition>
			<definition id="2">
				<sentence>The set C2 consists of strict WordNet antonyms of members of S1 and WordNet synonyms of members of C1 .</sentence>
				<definiendum id="0">set C2</definiendum>
				<definiens id="0">consists of strict WordNet antonyms of members of S1 and WordNet synonyms of members of C1</definiens>
			</definition>
			<definition id="3">
				<sentence>It checks that it hasn’t entered a loop ( 6a ) , generates a new contrast set for the object of the relation ( 6 ( a ) i ) , recursively generates a referring expression for the object of the preposition ( 6 ( a ) ii ) , recalculates DQ ( 6 ( a ) iii ) and either incorporates the relation in the referring expression or shifts the relation down the *preferred* list ( 6 ( a ) iv ) .</sentence>
				<definiendum id="0">DQ</definiendum>
				<definiens id="0">generates a new contrast set for the object of the relation</definiens>
			</definition>
			<definition id="4">
				<sentence>Krahmer and Theune ( 2002 ) propose an extension to the IA which treats the context set as a combination of a discourse domain and a salience function .</sentence>
				<definiendum id="0">IA</definiendum>
			</definition>
			<definition id="5">
				<sentence>It is also , as far as we know , the rst algorithm that allows for the incremental incorporation of relations and the rst that handles nominals .</sentence>
				<definiendum id="0">rst algorithm</definiendum>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>C-structure captures surface grammatical configurations , fstructure encodes abstract syntactic information approximating to predicate-argument/dependency structure or simple logical form ( van Genabith and Crouch , 1996 ) .</sentence>
				<definiendum id="0">C-structure captures</definiendum>
				<definiens id="0">surface grammatical configurations , fstructure encodes abstract syntactic information approximating to predicate-argument/dependency structure or simple logical form</definiens>
			</definition>
			<definition id="1">
				<sentence>The Penn-II treebank employs CFG trees with additional “functional” node annotations ( such as -LOC , -TMP , -SBJ , -LGS , . . . ) as well as traces and coindexation ( to indicate LDDs ) as basic data structures .</sentence>
				<definiendum id="0">Penn-II treebank</definiendum>
				<definiens id="0">employs CFG trees with additional “functional” node annotations ( such as -LOC , -TMP , -SBJ , -LGS , . . . ) as well as traces and coindexation ( to indicate LDDs ) as basic data structures</definiens>
			</definition>
			<definition id="2">
				<sentence>The annotation algorithm is modular with four components ( Figure 2 ) : left-right ( L-R ) annotation principles ( e.g. leftmost NP to right of V head of VP type rule is likely to be an object etc. ) ; coordination annotation principles ( separating these out simplifies other components of the algorithm ) ; traces ( translates traces and coindexation in trees into corresponding reentrancies in f-structure ( 1 in Figure 3 ) ) ; catch all and clean-up .</sentence>
				<definiendum id="0">coordination annotation principles</definiendum>
				<definiens id="0">left-right ( L-R ) annotation principles</definiens>
			</definition>
			<definition id="3">
				<sentence>Subcategorisation requirements are provided lexically in terms of semantic forms ( subcat lists ) and coherence and completeness conditions ( all GFs specified must be present , and no others may be present ) on f-structure representations .</sentence>
				<definiendum id="0">Subcategorisation requirements</definiendum>
				<definiens id="0">provided lexically in terms of semantic forms ( subcat lists</definiens>
			</definition>
			<definition id="4">
				<sentence>Given a set of semantic forms s with probabilities P ( s|l ) ( where l is a lemma ) , a set of paths p with P ( p|t ) ( where t is either TOPIC , TOPIC-REL or FOCUS ) and an f-structure f , the core of the algorithm to resolve LDDs recursively traverses f to : find TOPIC|TOPIC-REL|FOCUS : g pair ; retrieve TOPIC|TOPIC-REL|FOCUS paths ; for each path p with GF1 : ... : GFn : GF , traverse f along GF1 : . . . : GFn to sub-f-structure h ; retrieve local PRED : l ; add GF : g to h iff ∗ GF is not present at h wh-less TOPIC-REL # wh-less TOPIC-REL # subj 5692 adjunct 1314 xcomp : adjunct 610 obj 364 xcomp : obj 291 xcomp : xcomp : adjunct 96 comp : subj 76 xcomp : subj 67 Table 5 : Most frequent wh-less TOPIC-REL paths 02–21 23 23 / ( 02–21 ) TOPIC 26 7 2 FOCUS 13 4 0 TOPIC-REL 60 22 1 Table 6 : Number of path types extracted ∗ h together with GF is locally complete and coherent with respect to a semantic form s for l rank resolution by P ( s|l ) × P ( p|t ) The algorithm supports multiple , interacting TOPIC , TOPIC-REL and FOCUS LDDs .</sentence>
				<definiendum id="0">l</definiendum>
				<definiendum id="1">retrieve TOPIC|TOPIC-REL|FOCUS</definiendum>
				<definiendum id="2">retrieve local PRED</definiendum>
				<definiens id="0">g to h iff ∗ GF is not present at h wh-less TOPIC-REL # wh-less TOPIC-REL # subj 5692 adjunct 1314 xcomp : adjunct 610 obj 364 xcomp : obj 291 xcomp : xcomp : adjunct 96 comp : subj 76 xcomp : subj 67 Table 5 : Most frequent</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>P ( A , W ) =P ( A|W ) P ( W ) ( 1 ) The noisy channel model for speech is presented in Equation 1 , where A represents the acoustic data extracted from a speech signal , and W represents a word string .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">A</definiendum>
				<definiendum id="2">W</definiendum>
				<definiens id="0">represents the acoustic data extracted from a speech signal</definiens>
			</definition>
			<definition id="1">
				<sentence>The word-lattice is a weighted directed acyclic graph where a path in the graph corresponds to a string predicted by the acoustic recognizer .</sentence>
				<definiendum id="0">word-lattice</definiendum>
				<definiens id="0">a weighted directed acyclic graph where a path in the graph corresponds to a string predicted by the acoustic recognizer</definiens>
			</definition>
			<definition id="2">
				<sentence>The HUB–1 is a collection of 213 word-lattices resulting from an acoustic recognizer’s analysis of speech utterances .</sentence>
				<definiendum id="0">HUB–1</definiendum>
				<definiens id="0">a collection of 213 word-lattices resulting from an acoustic recognizer’s analysis of speech utterances</definiens>
			</definition>
			<definition id="3">
				<sentence>We ran the first stage parser with 4-times overparsing for each string in 7 The n–best lists were provided by Brian Roark ( Roark , 2001 ) 8 A local-tree is an explicit expansion of an edge and its children .</sentence>
				<definiendum id="0">local-tree</definiendum>
				<definiens id="0">an explicit expansion of an edge and its children</definiens>
			</definition>
			<definition id="4">
				<sentence>Attention shifting is a simple technique that attempts to make word-lattice parsing more efficient .</sentence>
				<definiendum id="0">Attention shifting</definiendum>
				<definiens id="0">a simple technique that attempts to make word-lattice parsing more efficient</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>The False Reject Rate ( FRR ) is the percentage of cases in which this happens , the percentage being taken from the cases which should be accepted .</sentence>
				<definiendum id="0">False Reject Rate ( FRR )</definiendum>
				<definiens id="0">the percentage of cases in which this happens , the percentage being taken from the cases which should be accepted</definiens>
			</definition>
			<definition id="1">
				<sentence>Similarly , the False Accept Rate ( FAR ) is the percentage of cases where somebody who has not written the test text is accepted as having written the text .</sentence>
				<definiendum id="0">False Accept Rate</definiendum>
				<definiendum id="1">FAR )</definiendum>
				<definiens id="0">the percentage of cases where somebody who has not written the test text is accepted as having written the text</definiens>
			</definition>
			<definition id="2">
				<sentence>The ABC-NL1 consists of 72 Dutch texts by 8 authors , controlled for age and educational level of the authors , and for register , genre and topic of the texts .</sentence>
				<definiendum id="0">ABC-NL1</definiendum>
				<definiens id="0">consists of 72 Dutch texts by 8 authors , controlled for age and educational level of the authors , and for register , genre and topic of the texts</definiens>
			</definition>
			<definition id="3">
				<sentence>For the authorship task , the profile reference corpus consists of the collection of all attributed and non-attributed texts , i.e. the entire ABC-NL1 corpus .</sentence>
				<definiendum id="0">profile reference corpus</definiendum>
				<definiens id="0">consists of the collection of all attributed and non-attributed texts , i.e. the entire ABC-NL1 corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>The test data consists of the unattributed texts for the ninth essay topic .</sentence>
				<definiendum id="0">test data</definiendum>
				<definiens id="0">consists of the unattributed texts for the ninth essay topic</definiens>
			</definition>
</paper>

		<paper id="3032">
			<definition id="0">
				<sentence>Dyna has many uses but was designed especially for rapid development of new statistical NLP systems .</sentence>
				<definiendum id="0">Dyna</definiendum>
			</definition>
			<definition id="1">
				<sentence>A Dyna program is a small set of equations , resembling Prolog inference rules , that specify the abstract structure of a dynamic programming algorithm .</sentence>
				<definiendum id="0">Dyna program</definiendum>
				<definiens id="0">a small set of equations , resembling Prolog inference rules</definiens>
			</definition>
			<definition id="2">
				<sentence>1 shows a simple Dyna program that corresponds to the inside algorithm for PCFGs ( i.e. , the probabilistic generalization of CKY parsing ) .</sentence>
				<definiendum id="0">PCFGs</definiendum>
				<definiens id="0">the probabilistic generalization of CKY parsing )</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus , Dyna can be regarded as computing a function F ( vectorθ ) , where vectorθ is a vector of axiom values and F ( vectorθ ) is an objective function such as the probability of one’s training data .</sentence>
				<definiendum id="0">vectorθ</definiendum>
				<definiendum id="1">F ( vectorθ )</definiendum>
				<definiens id="0">computing a function F ( vectorθ )</definiens>
				<definiens id="1">an objective function such as the probability of one’s training data</definiens>
			</definition>
			<definition id="4">
				<sentence>np/Needed is syntactic sugar for slash ( np , Needed ) , which is the label of a partial np constituent that is still missing the list of subconstituents in Needed .</sentence>
				<definiendum id="0">np/Needed</definiendum>
				<definiendum id="1">Needed )</definiendum>
				<definiens id="0">the label of a partial np constituent that is still missing the list of subconstituents in Needed</definiens>
			</definition>
			<definition id="5">
				<sentence>Dyna is a declarative programming language for building efficient systems quickly .</sentence>
				<definiendum id="0">Dyna</definiendum>
				<definiens id="0">a declarative programming language for building efficient systems quickly</definiens>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>Then we define virtual concepts as linear functions which estimate degree of membership of documents in concept-formative clusters .</sentence>
				<definiendum id="0">virtual concepts</definiendum>
				<definiens id="0">linear functions which estimate degree of membership of documents in concept-formative clusters</definiens>
			</definition>
			<definition id="1">
				<sentence>The GRA works in iterations and gradually increases the number of non-zero elements in the resulting vector , i.e. the number of words with non-zero weight in the resulting mixture .</sentence>
				<definiendum id="0">GRA</definiendum>
				<definiens id="0">works in iterations</definiens>
			</definition>
</paper>

		<paper id="1074">
			<definition id="0">
				<sentence>The probability of z belonging to c is : ∑∑ ≈= ll zlplcpzclpzcp ) | ( ) | ( ) | , ( ) | ( ( 1 ) let n BBBl ... 21 = , by MLE we have : ) ( ) , ( ) | ( ) | ( n n n Bf Bcf Bcplcp =≈ ( 2 ) ) , ( n Bcf is the count of the items whose leaf nodes are B n and belonging to class c. And Linguistic Feature Symbol POS Tag Effect Example With/Without punctuations PT Not Applicable Not Applicable Not Applicable Speech verbs VS TI_vs Tense 报告 , 表示 , 称 Trend verbs TR TI_tr Aspect 起来 , 下去 Preposition words P TI_p Discourse Structure/Aspect 当 , 到 , 继 Position words PS TI_f Discourse Structure 底 , 后 , 开始 Verbs with verb objects VV TI_vv Tense/Aspect 继续 , 进行 , 续 Verbs expressing wish/hope VA TI_va Tense 必须 , 会 , 可 Verbs related to causality VC TI_vc Discourse Structure 导致 , 致使 , 引起 Conjunctive words C TI_c Discourse Structure 并 , 并且 , 不过 Auxiliary words U TI_u Aspect 着 , 了 , 过 Time words T TI_t Tense 过去 , 今后 , 今年 Adverbs D TI_d Tense/Aspect/Discourse Structure便 , 并 , 并未 , 不 Event class EC E0/E1/E2/E3 Event Classification State , Punctual Event , Developing Event , Process Table 1 .</sentence>
				<definiendum id="0">z belonging to c</definiendum>
				<definiendum id="1">n Bcf</definiendum>
				<definiens id="0">the count of the items whose leaf nodes are B n and belonging to class c. And Linguistic Feature Symbol POS Tag Effect Example With/Without punctuations PT Not Applicable Not Applicable Not Applicable Speech verbs VS TI_vs Tense 报告 , 表示 , 称 Trend verbs TR TI_tr Aspect 起来 , 下去 Preposition words P TI_p Discourse Structure/Aspect 当 , 到 , 继 Position words PS TI_f Discourse Structure 底 , 后 , 开始 Verbs with verb objects VV TI_vv Tense/Aspect 继续 , 进行 , 续 Verbs expressing wish/hope VA TI_va Tense 必须 , 会 , 可 Verbs related to causality VC TI_vc Discourse Structure 导致 , 致使 , 引起 Conjunctive words C TI_c Discourse Structure 并 , 并且 , 不过 Auxiliary words U TI_u Aspect 着 , 了 , 过 Time words T TI_t Tense 过去 , 今后 , 今年 Adverbs D TI_d Tense/Aspect/Discourse Structure便 , 并 , 并未 , 不 Event class EC E0/E1/E2/E3 Event Classification State</definiens>
			</definition>
			<definition id="1">
				<sentence>Objects are classified into classes based on their attributes .</sentence>
				<definiendum id="0">Objects</definiendum>
			</definition>
			<definition id="2">
				<sentence>, ... , , , | ( maxarg 321 * n c AAAAcscorec = ( 4 ) ) , ... , , , | ( ) , ... , , , | ( ) , ... , , , | ( 321 321 321 n n n AAAAcp AAAAcp AAAAcscore = ( 5 ) Apply Bayesian rule to ( 5 ) , we have : ) , ... , , , | ( ) , ... , , , | ( ) , ... , , , | ( 321 321 321 n n n AAAAcp AAAAcp AAAAcscore = ) ( ) | , ... , , , ( ) ( ) | , ... , , , ( 321 321 cpcAAAAp cpcAAAAp n n = ) ( ) | ( ) ( ) | ( 1 1 cpcAp cpcAp n i i n i i ∏ ∏ = = ≈ ( 6 ) ) | ( cAp i and ) | ( cAp i are estimated by MLE from training data with Dirichlet Smoothing method : ∑ = ×+ + = n j j i i nucAc ucAc cAp 1 ) , ( ) , ( ) | ( ( 7 ) ∑ = ×+ + = n j j i i nucAc ucAc cAp 1 ) , ( ) , ( ) | ( ( 8 ) PDT and NB are both supervised learning approach .</sentence>
				<definiendum id="0">NB</definiendum>
				<definiens id="0">maxarg 321 * n c AAAAcscorec = ( 4 ) ) , ... , , , | ( ) , ... , , , | ( ) , ... , , , | ( 321 321 321 n n n AAAAcp AAAAcp AAAAcscore</definiens>
				<definiens id="1">... , , , | ( ) , ... , , , | ( ) , ... , , , | ( 321 321 321 n n n AAAAcp AAAAcp AAAAcscore = ) ( ) | , ... , , , ( ) ( ) | , ... , ,</definiens>
			</definition>
			<definition id="3">
				<sentence>PDT ( a non-linear classifier ) and NBC ( a linear classifier ) are under consideration .</sentence>
				<definiendum id="0">PDT</definiendum>
				<definiens id="0">a non-linear classifier</definiens>
				<definiens id="1">a linear classifier ) are under consideration</definiens>
			</definition>
			<definition id="4">
				<sentence>SC2 ( Separate Connecting words 2 ) : it is the same as SC1 except that it combines the connecting word pairs ( i.e. as a single pattern ) into one attribute .</sentence>
				<definiendum id="0">SC2</definiendum>
				<definiendum id="1">connecting word pairs</definiendum>
				<definiens id="0">a single pattern ) into one attribute</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>A mention is an instance of reference to an object , and the collection of mentions referring to the same object in a document form an entity .</sentence>
				<definiendum id="0">mention</definiendum>
				<definiens id="0">an instance of reference to an object , and the collection of mentions referring to the same object in a document form an entity</definiens>
			</definition>
			<definition id="1">
				<sentence>The root node is the initial state of the process , which consists of a partial entity containing the first mention of a document .</sentence>
				<definiendum id="0">root node</definiendum>
				<definiens id="0">consists of a partial entity containing the first mention of a document</definiens>
			</definition>
			<definition id="2">
				<sentence>The Bell Number a0a2a1a4a3a6a5 is the number of ways of partitioning a3 distinguishable objects ( i.e. , mentions ) into non-empty disjoint subsets ( i.e. , entities ) .</sentence>
				<definiendum id="0">Bell Number a0a2a1a4a3a6a5</definiendum>
				<definiens id="0">the number of ways of partitioning a3 distinguishable objects ( i.e. , mentions</definiens>
			</definition>
			<definition id="3">
				<sentence>The initial node consists of the first partial entity [ 1 ] ( i.e. , node ( a ) in Figure 1 ) .</sentence>
				<definiendum id="0">initial node</definiendum>
			</definition>
			<definition id="4">
				<sentence>We use maximum entropy model ( Berger et al. , 1996 ) for both the mention-pair model ( 9 ) and the entity-mention model ( 8 ) : a83a84a1a86a85a88a87 a43 a44 a71 a43 a16 a5a13a7 a55a35a34a23a36 a6a35a37 a6a39a38a40a6a42a41 a31a44a43a3a45a31 a6 a45a46a48a47a24a49 a50 a1 a43 a44 a71 a43 a16 a5 a71 ( 10 ) a83a84a1a4a85 a87 a55 a81 a71 a43 a16 a5a13a7 a55a35a34 a36 a6 a37 a6a39a38a40a6a42a41 a11a7a32 a45a31 a6 a45a46a48a47 a49 a50 a1 a55a39a81 a71 a43 a16 a5 a71 ( 11 ) wherea57 a16 a1a51a8 a71a52a8 a71a90a85a73a5 is a feature and a53 a16 is its weight ; a50 a1a33a8 a71a54a8a5 is a normalizing factor to ensure that ( 10 ) or ( 11 ) is a probability .</sentence>
				<definiendum id="0">maximum entropy model</definiendum>
				<definiendum id="1">a50 a1a33a8 a71a54a8a5</definiendum>
				<definiens id="0">a83a84a1a86a85a88a87 a43 a44 a71 a43 a16 a5a13a7 a55a35a34a23a36 a6a35a37 a6a39a38a40a6a42a41 a31a44a43a3a45a31 a6 a45a46a48a47a24a49 a50 a1 a43 a44 a71 a43 a16 a5 a71 ( 10 ) a83a84a1a4a85 a87 a55 a81 a71 a43 a16 a5a13a7 a55a35a34 a36 a6 a37 a6a39a38a40a6a42a41 a11a7a32 a45a31 a6 a45a46a48a47 a49 a50 a1 a55a39a81 a71 a43 a16 a5 a71 ( 11 ) wherea57 a16 a1a51a8 a71a52a8 a71a90a85a73a5 is a feature and a53 a16 is its weight ;</definiens>
			</definition>
			<definition id="5">
				<sentence>The “Count” feature calculates how many times a mention string is seen .</sentence>
				<definiendum id="0">“Count” feature</definiendum>
				<definiens id="0">calculates how many times a mention string is seen</definiens>
			</definition>
			<definition id="6">
				<sentence>In Algorithm 1 , a0 contains all the hypotheses , or paths from the root to the current layer of nodes .</sentence>
				<definiendum id="0">a0</definiendum>
				<definiens id="0">contains all the hypotheses , or paths from the root to the current layer of nodes</definiens>
			</definition>
			<definition id="7">
				<sentence>The last line returns top a8 results , where a79 a41a10a9 a47 denotes the a11 a81a13a12 result ranked by a1 a1a51a8a5 : a1 a1a86a79 a41 a9 a47 a5a15a14 a1 a1a4a79 a41a10a2 a47 a5a15a14 a8a54a8a54a8a16a14 a1 a1a86a79 a41a10a17 a47 a5a22a34 Algorithm 1 Search Algorithm Input : mentions a18 a7 a42a24a43a59a44a54a46 a38a98a71a24a34 a34a24a34a18a71a41a3a54a53 ; a8 Output : top a8 entity results 1 : Initialize : a0 a46a7 a42 a79 a9 a46a7 a42a29a91a43 a9 a93a26a53a98a53a20a19 a1 a1a4a79 a9 a5a13a7a102a38 2 : for a63 a7a15a25 to a3 3 : foreach node a79 a82a5a0 4 : compute a6 a31 .</sentence>
				<definiendum id="0">a79 a41a10a9 a47</definiendum>
				<definiens id="0">the a11 a81a13a12 result ranked by a1 a1a51a8a5 : a1 a1a86a79 a41 a9 a47 a5a15a14 a1 a1a4a79 a41a10a2 a47 a5a15a14 a8a54a8a54a8a16a14 a1 a1a86a79 a41a10a17 a47 a5a22a34 Algorithm 1 Search Algorithm Input : mentions a18 a7 a42a24a43a59a44a54a46 a38a98a71a24a34 a34a24a34a18a71a41a3a54a53</definiens>
			</definition>
			<definition id="8">
				<sentence>Since the ACE-value is an entity-level metric and is weighted heavily toward NAME entities , we also measure our system’s performance by an entity-constrained mention F-measure ( henceforth “ECM-F” ) .</sentence>
				<definiendum id="0">entity-constrained mention F-measure</definiendum>
				<definiens id="0">an entity-level metric and is weighted heavily toward NAME entities</definiens>
			</definition>
			<definition id="9">
				<sentence>The ECM-F measures the percentage of mentions that are in the “right” entities .</sentence>
				<definiendum id="0">ECM-F</definiendum>
				<definiens id="0">measures the percentage of mentions that are in the “right” entities</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>In this paper we present a trainable model which incorporates coreferential information of candidates into pronoun resolution .</sentence>
				<definiendum id="0">trainable model</definiendum>
				<definiens id="0">incorporates coreferential information of candidates into pronoun resolution</definiens>
			</definition>
			<definition id="1">
				<sentence>F-measure is the harmonic mean of the two measurements .</sentence>
				<definiendum id="0">F-measure</definiendum>
			</definition>
			<definition id="2">
				<sentence>From Testing Backward feature MUC-6 MUC-7 Experiments classifler assigner* R P F S R P F S Baseline DTpron NIL 77.2 83.4 80.2 70.0 71.9 68.6 70.2 59.0 Optimal DTpron¡opt ( Annotated ) 83.6 84.7 84.1 74.7 76.0 77.6 76.8 62.5 RealResolve-1 DTpron¡opt DTpron¡opt 75.8 83.8 79.5 73.1 62.3 77.7 69.1 53.8 RealResolve-2 DTpron¡opt DTpron 75.8 83.8 79.5 73.1 63.0 77.9 69.7 54.9 RealResolve-3 DT0pron DTpron 79.3 86.3 82.7 74.7 74.7 67.3 70.8 60.8 RealResolve-4 DT0pron DT0pron 79.3 86.3 82.7 74.7 74.7 67.3 70.8 60.8 Table 4 : Results of difierent systems for pronoun resolution on MUC-6 and MUC-7 ( *Here we only list backward feature assigner for pronominal candidates .</sentence>
				<definiendum id="0">MUC-6 MUC-7</definiendum>
				<definiens id="0">Experiments classifler assigner* R P F S R P F</definiens>
			</definition>
			<definition id="3">
				<sentence>n ] : = 0 for i = 1 to N for j = i 1 downto 0 if ( Mi is a non-pron and DTnon¡pron ( ifMi ; Mjg ) == + ) or ( Mi is a pron and DTpron ( ifMi ; Mj ; Ante [ j ] g ) == + ) then Ante [ i ] : = Mj break return Ante Figure 2 : The pronoun resolution algorithm by incorporating coreferential information of candidates to be employed to obtain the closest antecedent for a candidate .</sentence>
				<definiendum id="0">Mi</definiendum>
				<definiens id="0">a non-pron and DTnon¡pron ( ifMi ; Mjg ) == +</definiens>
				<definiens id="1">a pron</definiens>
			</definition>
			<definition id="4">
				<sentence>Here the purpose of DTpron and DTnon¡pron is to provide backward feature values for training and testing instances .</sentence>
				<definiendum id="0">DTnon¡pron</definiendum>
				<definiens id="0">to provide backward feature values for training and testing instances</definiens>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>Tasks in computational linguistics ( CL ) normally focus on the content of a document while paying little attention to the context in which it was produced .</sentence>
				<definiendum id="0">computational linguistics</definiendum>
			</definition>
			<definition id="1">
				<sentence>IDF ) : a0a2a1a4a3a6a5a8a7a9a1a4a3a11a10a13a12a15a14a17a16a19a18a15a20a22a21a24a23a25a1a27a26 where a0a2a1a28a3 is the weight of term k in document d , a7a9a1a28a3 is the frequency ofkin d , N is the total number of documents in the corpus , and a23a29a1 is the total number of documents containing k. Very frequent terms ( such as function words ) that occur in many documents are downweighted , while those that are fairly unique have their weights boosted .</sentence>
				<definiendum id="0">IDF )</definiendum>
				<definiendum id="1">a0a2a1a28a3</definiendum>
				<definiendum id="2">a7a9a1a28a3</definiendum>
				<definiendum id="3">N</definiendum>
				<definiendum id="4">a23a29a1</definiendum>
				<definiens id="0">the weight of term k in document d ,</definiens>
				<definiens id="1">the total number of documents in the corpus</definiens>
				<definiens id="2">the total number of documents containing k. Very frequent terms ( such as function words</definiens>
			</definition>
			<definition id="2">
				<sentence>Lexical variants ( such as plurals ) are omitted .</sentence>
				<definiendum id="0">Lexical variants</definiendum>
				<definiens id="0">such as plurals ) are omitted</definiens>
			</definition>
			<definition id="3">
				<sentence>Text categorization ( TC ) is the problem of assigning documents to one or more pre-defined categories .</sentence>
				<definiendum id="0">Text categorization</definiendum>
				<definiens id="0">the problem of assigning documents to one or more pre-defined categories</definiens>
			</definition>
			<definition id="4">
				<sentence>TFM is a two step process that is captured by this pseudocode : VOCABULARY ADDITIONS : for each class C : for each year y : PreModList ( C , y , L ) = OddsRatio ( C , y , L ) ModifyList ( y ) = DecisionRule ( PreModList ( C , y , L ) ) for each term k in ModifyList ( y ) : Add pseudo-term `` k+y '' to Vocab DOCUMENT MODIFICATIONS : for each document : y = year of doc for each term k : if `` k+y '' in Vocab : replace k with `` k+y '' classify modified document PreModList ( C , y , L ) is a list of the top L lexemes that , by the odds ratio measure2 , are highly associated with category C in year y. We test the hypothesis that these come from a perturbed generator in year y , as opposed to the atemporal generator Gk , by comparing the odds ratios of termcategory pairs in a PreModList in year y with the same pairs across the entire corpus .</sentence>
				<definiendum id="0">TFM</definiendum>
				<definiendum id="1">VOCABULARY ADDITIONS</definiendum>
				<definiendum id="2">L )</definiendum>
			</definition>
			<definition id="5">
				<sentence>Terms which pass this test are added to the final ModifyList ( y ) for year y. For the results that we report , DecisionRule is a simple ratio test with threshold factor f. Suppose f is 2.0 : if the odds ratio between C and k is twice as great in year y as it is atemporally , the decision rule is “passed” .</sentence>
				<definiendum id="0">DecisionRule</definiendum>
				<definiendum id="1">decision rule</definiendum>
				<definiens id="0">a simple ratio test with threshold factor f. Suppose f is 2.0 : if the odds ratio between C and k is twice as great in year y as it is atemporally , the</definiens>
			</definition>
			<definition id="6">
				<sentence>We tested TFM on corpora representing genres from academic publications to Usenet postings , 2Odds ratio is defined as a46a48a47a32a49a15a50a4a51a53a52a37a47a28a54a56a55a57a52a36a47a32a49a15a50a4a51a58a46a48a47a59a54 , where p is Pr ( k|C ) , the probability that term k is present given category C , and q is Pr ( k| !</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">q</definiendum>
				<definiens id="0">the probability that term k is present given category C , and</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>Flexibility is one of the reasons for the successful employment of indexing in databases ( Elmasri and Navathe , 2000 ) and automated reasoning ( Ramakrishnan et al. , 2001 ) .</sentence>
				<definiendum id="0">Flexibility</definiendum>
				<definiens id="0">one of the reasons for the successful employment of indexing in databases</definiens>
			</definition>
			<definition id="1">
				<sentence>In Figure 1 , Agr is an active external variable in the second daughter , but it is inactive in the first daughter .</sentence>
				<definiendum id="0">Agr</definiendum>
				<definiens id="0">an active external variable in the second daughter , but it is inactive in the first daughter</definiens>
			</definition>
			<definition id="2">
				<sentence>VariableCut is the largest subset of nodes x a7 M such that : s a2 t exists .</sentence>
				<definiendum id="0">VariableCut</definiendum>
				<definiens id="0">the largest subset of nodes x a7 M such that : s a2 t exists</definiens>
			</definition>
			<definition id="3">
				<sentence>Case A is trivial , and D is a generalization of B and C. Case B. It will be shown that a8 t a7 Type such that a6 a4y a7 a11 a4z a12 a10 a12 a1 a4 D and for a0 a4xa4 a4 a11a4za12a11a10a13a12 a1 a4 M , t a3 θa7 a4ya9 and t a3 θa7 a4xa9 .</sentence>
				<definiendum id="0">D</definiendum>
				<definiens id="0">a generalization of B and C. Case</definiens>
			</definition>
			<definition id="4">
				<sentence>The compatibility is defined as type unification : the type pointed to by the element Vi a2 j a7 na9 of an edge’s vector Vi a2 j should unify with the type pointed to by the element Vi a2 j a7 na9 of the path index vector Vi a2 j of the daughter on position D j in a rule Ri .</sentence>
				<definiendum id="0">compatibility</definiendum>
				<definiens id="0">type pointed to by the element Vi a2 j a7 na9 of the path index vector Vi a2 j of the daughter on position D j in a rule Ri</definiens>
			</definition>
			<definition id="5">
				<sentence>MERGE is an adaptation of the ERG which uses types more conservatively in favour of relations , macros and complex-antecedent constraints .</sentence>
				<definiendum id="0">MERGE</definiendum>
				<definiens id="0">an adaptation of the ERG which uses types more conservatively in favour of relations , macros and complex-antecedent constraints</definiens>
			</definition>
			<definition id="6">
				<sentence>MERGE was tested on 550 sentences of lengths between 6 and 16 words , extracted from the Wall Street Journal annotated parse trees ( where phrases not covered by MERGE’s vocabulary were replaced by lexical entries having the same parts of speech ) , and from MERGE’s own test corpus .</sentence>
				<definiendum id="0">MERGE</definiendum>
				<definiens id="0">vocabulary were replaced by lexical entries having the same parts of speech ) , and from MERGE’s own test corpus</definiens>
			</definition>
			<definition id="7">
				<sentence>This discrepancy in the number of failure-causing paths could be resulting in an overfitted quick-check vector , or , perhaps the 30 paths chosen for MERGE really are not the best 30 ( quick-check uses a greedy approximation ) .</sentence>
				<definiendum id="0">quick-check</definiendum>
				<definiens id="0">uses a greedy approximation )</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>The anchor for a BD with head noun NBD was chosen among the subset of all potential antecedents ( PA ) in the previous five sentences that had been CBs or CPs by calling Google ( by hand ) with the query “the NBD of the NPA” , where NPA is the head noun of the potential antecedent , and choosing the PA with the highest hit count .</sentence>
				<definiendum id="0">NPA</definiendum>
			</definition>
			<definition id="1">
				<sentence>Then and NPA ; the Most Specific Common Subsumer scommij ( this is the closest concept which is an hypernym of both senses ) .</sentence>
				<definiendum id="0">Most Specific Common Subsumer scommij</definiendum>
				<definiens id="0">an hypernym of both senses</definiens>
			</definition>
			<definition id="2">
				<sentence>Types of Classifiers Used Multi-layer perceptrons ( MLPs ) have been claimed to work well with small datasets ; we tested both our own implementation of an MLP with back-propagation in MatLab 6.5 , experimenting with different configurations , and an off-the-shelf MLP included in the Weka Machine Learning Library8 , Weka-NN .</sentence>
				<definiendum id="0">MLPs</definiendum>
			</definition>
			<definition id="3">
				<sentence>The GNOME corpus contains 58 mereological BDs .</sentence>
				<definiendum id="0">GNOME corpus</definiendum>
				<definiens id="0">contains 58 mereological BDs</definiens>
			</definition>
			<definition id="4">
				<sentence>Centering : A parametric theory and its instantiations .</sentence>
				<definiendum id="0">Centering</definiendum>
			</definition>
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>We formalise this using the notion of cepts : a cept is a central pivot through which a subset of ewords is aligned to a subset of f-words .</sentence>
				<definiendum id="0">cept</definiendum>
				<definiens id="0">a central pivot through which a subset of ewords is aligned to a subset of f-words</definiens>
			</definition>
			<definition id="1">
				<sentence>An immediate consequence is that Pi Fik : Fil = 0 : columns of F are orthogonal , that is F is an orthogonal matrix ( and similarly , E is orthogonal ) .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">an orthogonal matrix</definiens>
				<definiens id="1">orthogonal )</definiens>
			</definition>
			<definition id="2">
				<sentence>The evaluation is done using the performance measures described in ( Mihalcea and Pedersen , 2003 ) : precision , recall and F-score on the probable and sure alignments , as well as the Alignment Error Rate ( AER ) , which in our case is a weighted average of the recall on the sure alignments and the precision on the probable .</sentence>
				<definiendum id="0">Alignment Error Rate ( AER )</definiendum>
			</definition>
			<definition id="3">
				<sentence>As the AER is a weighted average of RS and PP , the resulting AER are relatively low for our method .</sentence>
				<definiendum id="0">AER</definiendum>
				<definiens id="0">a weighted average of RS</definiens>
			</definition>
			<definition id="4">
				<sentence>no NULL alignments with NULL alignments Method PS RS FS AER PS RS FS AER ONMF + AIC 70.34 % 65.54 % 67.85 % 32.15 % 62.65 % 62.10 % 62.38 % 37.62 % ONMF + BIC 55.88 % 67.70 % 61.23 % 38.77 % 51.78 % 64.07 % 57.27 % 42.73 % HLT-03 best 82.65 % 62.44 % 71.14 % 28.86 % 82.65 % 54.11 % 65.40 % 34.60 % Table 4 : Performance on the 248 Romanian-English test sentences ( only sure alignments ) , for orthogonal non-negative matrix factorisation ( ONMF ) using the AIC and BIC criterion for choosing the number of cepts .</sentence>
				<definiendum id="0">PS RS FS AER PS RS FS AER ONMF</definiendum>
				<definiens id="0">Performance on the 248 Romanian-English test sentences ( only sure alignments ) , for orthogonal non-negative matrix factorisation ( ONMF ) using the AIC and BIC criterion for choosing the number of cepts</definiens>
			</definition>
			<definition id="5">
				<sentence>For solving the latter problem , we propose an algorithm for ONMF , which guarantees both proper alignments and good coverage .</sentence>
				<definiendum id="0">ONMF</definiendum>
				<definiens id="0">guarantees both proper alignments and good coverage</definiens>
			</definition>
</paper>

		<paper id="3004">
			<definition id="0">
				<sentence>TotalRecall is a bilingual concordancer that support search query in English or Chinese for relevant sentences and translations .</sentence>
				<definiendum id="0">TotalRecall</definiendum>
				<definiens id="0">a bilingual concordancer that support search query in English or Chinese for relevant sentences and translations</definiens>
			</definition>
			<definition id="1">
				<sentence>Central to TotalRecall is a bilingual corpus and a set of programs that provide the bilingual analyses to yield a translation memory database out of the bilingual corpus .</sentence>
				<definiendum id="0">TotalRecall</definiendum>
				<definiens id="0">a bilingual corpus and a set of programs that provide the bilingual analyses to yield a translation memory database out of the bilingual corpus</definiens>
			</definition>
</paper>

		<paper id="3024">
			<definition id="0">
				<sentence>The Naive Bayes classifier is a popular machine learning technique for text classification because it performs well in many domains , despite its simplicity ( Domingos and Pazzani , 1997 ) .</sentence>
				<definiendum id="0">Naive Bayes classifier</definiendum>
			</definition>
			<definition id="1">
				<sentence>Given model parameters p ( wtjcj ) and class prior probabilities p ( cj ) and assuming independence of the words , the most likely class for a document di is computed as c ( di ) = argmax j p ( cj ) p ( djcj ) = argmax j p ( cj ) jV jY t=1 p ( wtjcj ) n ( wt ; di ) ( 1 ) where n ( wt ; di ) is the number of occurrences of wt in di .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of occurrences of wt in di</definiens>
			</definition>
			<definition id="2">
				<sentence>p ( wtjcj ) and p ( cj ) are estimated from training documents with known classes , using maximum likelihood estimation with a Laplacean prior : p ( wtjcj ) = 1 + Pdi2cj n ( wt ; di ) jV j + PjV jt=1 Pdi2cj n ( wt ; di ) ( 2 ) p ( cj ) = jcjjPjCj j0=1 jcj0j ( 3 ) It is common practice to use only a subset of the words in the training documents for classification to avoid overfitting and make classification more efficient .</sentence>
				<definiendum id="0">p ( wtjcj</definiendum>
			</definition>
			<definition id="3">
				<sentence>The mutual information between two random variables , MI ( X ; Y ) , measures the amount of information that the value of one variable gives about the value of the other ( Cover and Thomas , 1991 ) .</sentence>
				<definiendum id="0">MI</definiendum>
				<definiens id="0">mutual information between two random variables</definiens>
			</definition>
			<definition id="4">
				<sentence>p ( x ; cj ) , p ( x ) and p ( cj ) are estimated from the training documents by counting how often wt occurs in each class .</sentence>
				<definiendum id="0">p ( x ; cj ) , p</definiendum>
				<definiens id="0">estimated from the training documents by counting how often wt occurs in each class</definiens>
			</definition>
			<definition id="5">
				<sentence>There is a strong connection between Naive Bayes and KL-divergence ( Kullback-Leibler divergence , relative entropy ) .</sentence>
				<definiendum id="0">KL-divergence</definiendum>
				<definiens id="0">Kullback-Leibler divergence , relative entropy )</definiens>
			</definition>
			<definition id="6">
				<sentence>It is defined ( for discrete distributions ) by KL ( p ; q ) = X x p ( x ) log p ( x ) q ( x ) : ( 5 ) By viewing a document as a probability distribution over words , Naive Bayes can be interpreted in an information-theoretic framework ( Dhillon et al. , 2002 ) .</sentence>
				<definiendum id="0">Naive Bayes</definiendum>
				<definiens id="0">a probability distribution over words</definiens>
			</definition>
			<definition id="7">
				<sentence>Taking logarithms and dividing by the length of d , ( 1 ) can be rewritten as c ( d ) = argmax j log p ( cj ) + jV jX t=1 n ( wt ; d ) log p ( wtjcj ) = argmax j 1 jdj log p ( cj ) + jV jX t=1 p ( wtjd ) log p ( wtjcj ) ( 6 ) Adding the entropy of p ( Wjd ) , we get c ( d ) = argmax j 1 jdj log p ( cj ) jV jX t=1 p ( wtjd ) log p ( wtjd ) p ( w tjcj ) = argmin j KL ( p ( Wjd ) ; p ( Wjcj ) ) 1jdj log p ( cj ) ( 7 ) This means that Naive Bayes assigns to a document d the class which is “most similar” to d in terms of the distribution of words .</sentence>
				<definiendum id="0">Naive Bayes</definiendum>
				<definiens id="0">assigns to a document d the class which is “most similar” to d in terms of the distribution of words</definiens>
			</definition>
			<definition id="8">
				<sentence>Let p ( wt ) be the number of occurrences of wt in all training documents , divided by the total number of words , q ( wt ) = PjCjj=1 Njt=jSj and define eKt ( S ) = p ( wt ) log q ( wt ) : ( 12 ) eKt ( S ) can be interpreted as an approximation of the average divergence of the distribution of wt in the individual training documents from the global distribution ( averaged over all training documents in all classes ) .</sentence>
				<definiendum id="0">Let p ( wt</definiendum>
				<definiendum id="1">eKt ( S )</definiendum>
				<definiens id="0">the number of occurrences of wt in all training documents , divided by the total number of words</definiens>
			</definition>
			<definition id="9">
				<sentence>Precision is the percentage of positive documents among all positively classified documents .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the percentage of positive documents among all positively classified documents</definiens>
			</definition>
			<definition id="10">
				<sentence>Recall is the percentage of positive documents that are classified as positive .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">the percentage of positive documents that are classified as positive</definiens>
			</definition>
			<definition id="11">
				<sentence>Microaveraged recall is the percentage of all positive documents ( in all topics ) that are classified as positive .</sentence>
				<definiendum id="0">Microaveraged recall</definiendum>
				<definiens id="0">the percentage of all positive documents ( in all topics ) that are classified as positive</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>Proponents of traditional ‘deep’ or ‘precise’ approaches to syntax , such as GB , CCG , HPSG , LFG , or TAG , have argued that sophisticated grammatical formalisms are essential to resolving various hidden relationships such as the source phrase of moved whphrases in questions and relativizations , or the controller of clauses without an overt subject .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">GB , CCG , HPSG , LFG , or</definiens>
			</definition>
			<definition id="1">
				<sentence>The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English ( WSJ ) and the context-free version of the NEGRA ( version 2 ) corpus of German ( Skut et al. , 1997b ) .</sentence>
				<definiendum id="0">datasets</definiendum>
			</definition>
			<definition id="2">
				<sentence>Each phase involves the identification of a certain subset of tree nodes to be operated on , followed by the application of the appropriate operation to the node .</sentence>
				<definiendum id="0">phase</definiendum>
				<definiens id="0">involves the identification of a certain subset of tree nodes to be operated on , followed by the application of the appropriate operation to the node</definiens>
			</definition>
			<definition id="3">
				<sentence>For WSJ , the three phases are : COMPlementizers4 ( IDENTNULL ) ( b ) For each COMP insertion node , determine position of each insertion and insert COMP ( INSERTNULL ) ( IDENTMOVED ) ( b ) For each DISLOCATED node , choose an ORIGIN node ( RELOCMOVED ) ( c ) For each pair 〈DISLOCATED , origin〉 , choose a position of insertion and insert dislocated ( INSERTRELOC ) ( IDENTLOCUS ) ( b ) For each LOCUS , determine position of insertion and insert LOCUS ( INSERTLOCUS ) ( c ) For each LOCUS , determine CONTROLLER ( if any ) ( FINDCONTROLLER ) Note in particular that phase 2 involves the classification of overt tree nodes as dislocated , followed by the identification of an origin site ( annotated in the treebank as an empty node ) for each dislocated element ; whereas phase 3 involves the identification of ( empty ) control loci first , and of controllers later .</sentence>
				<definiendum id="0">insert LOCUS</definiendum>
				<definiens id="0">b ) For each LOCUS , determine position of insertion and</definiens>
				<definiens id="1">dislocated , followed by the identification of an origin site ( annotated in the treebank as an empty node</definiens>
			</definition>
			<definition id="4">
				<sentence>PATH is the syntactic path between relative and base node , defined as the list of the syntactic categories on the ( inclusive ) node path linking the relative node to the node in question , paired with whether the step on the path was upward or downward .</sentence>
				<definiendum id="0">PATH</definiendum>
				<definiens id="0">the syntactic path between relative and base node , defined as the list of the syntactic categories on the ( inclusive ) node path linking the relative node to the node in question</definiens>
			</definition>
			<definition id="5">
				<sentence>The italicized node is the node for which a given feature is recorded ; underscores indicate variables that can match any category ; and the angle-bracketed parts of the tree fragment , together with an index for the pattern , determine the feature value.8 Our algorithm’s performance can be compared with the work of Johnson ( 2002 ) and Dienes and Dubey ( 2003a ) on WSJ .</sentence>
				<definiendum id="0">italicized node</definiendum>
				<definiens id="0">the node for which a given feature is recorded ; underscores indicate variables that can match any category ; and the angle-bracketed parts of the tree fragment</definiens>
			</definition>
			<definition id="6">
				<sentence>Valid comparisons exist for the insertion of uncoindexed empty nodes ( COMP and ARB-SUBJ ) , identification of control and raising loci ( CONTROLLOCUS ) , and pairings of dislocated and controller/raised nodes with their origins ( DISLOC , CONTROLLER ) .</sentence>
				<definiendum id="0">Valid comparisons</definiendum>
			</definition>
			<definition id="7">
				<sentence>A dependency relation , commonly employed for evaluation in the statistical parsing literature , is defined at a node N of a lexicalized parse tree as a pair 〈wi , wj〉 where wi is the lexical head of N and wj is the lexical head of some non-head daughter of N. Dependency relations may further be typed according to information at or near the relevant tree node ; Collins ( 1999 ) , for example , reports dependency scores typed on the syntactic categories of the mother , head daughter , and dependent daughter , plus on whether the dependent precedes or follows the head .</sentence>
				<definiendum id="0">dependency relation</definiendum>
				<definiendum id="1">wi</definiendum>
				<definiendum id="2">wj</definiendum>
				<definiens id="0">information at or near the relevant tree node</definiens>
			</definition>
			<definition id="8">
				<sentence>ID is correct identification of nodes as +/– dislocated ; Rel is relocation of node to correct mother given gold-standard data on which nodes are dislocated ( only applicable for gold trees ) ; Combo is both correct identification and remapping .</sentence>
				<definiendum id="0">ID</definiendum>
				<definiendum id="1">Combo</definiendum>
				<definiens id="0">correct identification of nodes as +/– dislocated ; Rel is relocation of node to correct mother given gold-standard data on which nodes are dislocated ( only applicable for gold trees ) ;</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>The components GEN ; and define a mapping from an input x to an output F ( x ) through F ( x ) = arg max y2GEN ( x ) ( x ; y ) ( 1 ) where ( x ; y ) is the inner product Ps s s ( x ; y ) .</sentence>
				<definiendum id="0">; y )</definiendum>
				<definiens id="0">the inner product Ps s s ( x ; y )</definiens>
			</definition>
			<definition id="1">
				<sentence>The decoding algorithm is a method for searching for the arg max in Eq .</sentence>
				<definiendum id="0">decoding algorithm</definiendum>
				<definiens id="0">a method for searching for the arg max in Eq</definiens>
			</definition>
			<definition id="2">
				<sentence>In this paper we are interested in parsing , where ( xi ; yi ) , GEN , and can be defined as follows : Each training example ( xi ; yi ) is a pair where xi is a sentence , andyi is the gold-standard parse for that sentence .</sentence>
				<definiendum id="0">yi )</definiendum>
				<definiendum id="1">andyi</definiendum>
				<definiens id="0">interested in parsing , where ( xi ; yi ) , GEN , and can be defined as follows : Each training example ( xi ;</definiens>
				<definiens id="1">a pair where xi is a sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>Given an input sentence x , GEN ( x ) is a set of possible parses for that sentence .</sentence>
				<definiendum id="0">GEN ( x )</definiendum>
				<definiens id="0">a set of possible parses for that sentence</definiens>
			</definition>
			<definition id="4">
				<sentence>For example , GEN ( x ) could be defined as the set of possible parses for x under some context-free grammar , perhaps a context-free grammar induced from the training examples .</sentence>
				<definiendum id="0">GEN ( x )</definiendum>
				<definiens id="0">the set of possible parses for x under some context-free grammar , perhaps a context-free grammar induced from the training examples</definiens>
			</definition>
			<definition id="5">
				<sentence>We can then state the following theorem ( see ( Collins , 2002 ) for a proof ) : Theorem 1 For any training sequence ( xi ; yi ) that is separable with margin , for any value of T , then for the perceptron algorithm in figure 1 Ne R 2 2 where R is a constant such that 8i ; 8z 2 GEN ( xi ) jj ( xi ; yi ) ( xi ; z ) jj R. This theorem implies that if there is a parameter vector U which makes zero errors on the training set , then after a finite number of iterations the training algorithm will converge to parameter values with zero training error .</sentence>
				<definiendum id="0">R</definiendum>
				<definiendum id="1">yi )</definiendum>
				<definiens id="0">separable with margin , for any value of T</definiens>
				<definiens id="1">a constant such that 8i</definiens>
			</definition>
			<definition id="6">
				<sentence>The input to the parser is a sentence x with length n. A hypothesis is a triple hx ; t ; ii such that x is the sentence being parsed , t is a partial or full analysis of that sentence , and i is an integer specifying the number of words of the sentence which have been processed .</sentence>
				<definiendum id="0">hypothesis</definiendum>
				<definiens id="0">the sentence being parsed</definiens>
				<definiens id="1">an integer specifying the number of words of the sentence which have been processed</definiens>
			</definition>
			<definition id="7">
				<sentence>We assume an “advance” function ADV which takes a hypothesis triple as input , and returns a set of new hypotheses as output .</sentence>
				<definiendum id="0">“advance” function ADV</definiendum>
				<definiens id="0">takes a hypothesis triple as input</definiens>
			</definition>
			<definition id="8">
				<sentence>The advance function will absorb another word in the sentence : this means that if the input to ADV ishx ; t ; ii , then each member of ADV ( hx ; t ; ii ) will have the formhx ; t0 ; i+1i. Each new analysis t0 will be formed by somehow incorporating the i+1’th word into the previous analysis t. With these definitions in place , we can iteratively define the full set of partial analysesHi for the firstiwords of the sentence as H0 ( x ) = fhx ; ; ; 0ig , and Hi ( x ) = [ h02Hi 1 ( x ) ADV ( h0 ) for i = 1 : : : n. The full set of parses for a sentencexis then GEN ( x ) =Hn ( x ) where n is the length of x. Under this definition GEN ( x ) can include a huge number of parses , and searching for the highest scoring parse , arg maxh2Hn ( x ) ( h ) , will be intractable .</sentence>
				<definiendum id="0">x ) ADV</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the length of x. Under this definition GEN ( x ) can include a huge number of parses , and searching for the highest scoring parse , arg maxh2Hn ( x ) ( h )</definiens>
			</definition>
			<definition id="9">
				<sentence>For this reason we introduce one additional function , FILTER ( H ) , which takes a set of hypothesesH , and returns a much smaller set of “filtered” hypotheses .</sentence>
				<definiendum id="0">FILTER ( H )</definiendum>
				<definiens id="0">takes a set of hypothesesH , and returns a much smaller set of “filtered” hypotheses</definiens>
			</definition>
			<definition id="10">
				<sentence>Search errors , where arg maxh2Fn ( h ) 6= arg maxh2Hn ( h ) , will create errors in decoding test sentences , and also errors in implementing the perceptron training algorithm in Figure 1 .</sentence>
				<definiendum id="0">arg maxh2Fn</definiendum>
				<definiens id="0">create errors in decoding test sentences , and also errors in implementing the perceptron training algorithm in Figure 1</definiens>
			</definition>
			<definition id="11">
				<sentence>Adjoining either of the VP chains under the S creates two triples , hS , NP , VPiandhNP , NN , Si , which are both in the set B. Note that the “allowable chains” in our grammar are what Costa et al. ( 2001 ) call “connection paths” from the partial parse to the next word .</sentence>
				<definiendum id="0">VP chains</definiendum>
				<definiens id="0">under the S creates two triples , hS , NP , VPiandhNP , NN , Si</definiens>
			</definition>
			<definition id="12">
				<sentence>Hence , L00 is the node label itself ; L10 is the label of parent of the current node ; L01 is the label of the sibling of the node , immediately to its left ; L11 is the label of the sibling of the parent node , etc .</sentence>
				<definiendum id="0">L00</definiendum>
				<definiendum id="1">L10</definiendum>
				<definiendum id="2">L01</definiendum>
				<definiendum id="3">L11</definiendum>
				<definiens id="0">the node label itself</definiens>
				<definiens id="1">the label of parent of the current node ;</definiens>
				<definiens id="2">the label of the sibling of the node</definiens>
			</definition>
			<definition id="13">
				<sentence>The incremental parser is Input : A gold-standard parse = g for sentence k of N. A set of candidate parses F. Current parameters .</sentence>
				<definiendum id="0">Input</definiendum>
			</definition>
			<definition id="14">
				<sentence>A Cache of triples hgj ; Fj ; cji for j = 1 : : : N where each gj is a previously generated gold standard parse , Fj is a previously generated set of candidate parses , and cj is a counter of the number of times that has been updated due to this particular triple .</sentence>
				<definiendum id="0">Cache of triples</definiendum>
				<definiendum id="1">Fj</definiendum>
				<definiendum id="2">cj</definiendum>
				<definiens id="0">a previously generated gold standard parse ,</definiens>
				<definiens id="1">a previously generated set of candidate parses , and</definiens>
			</definition>
			<definition id="15">
				<sentence>For this paper , we used POS tags that were provided either by the Treebank itself ( gold standard tags ) or by the perceptron POS tagger3 presented in Collins ( 2002 ) .</sentence>
				<definiendum id="0">POS</definiendum>
			</definition>
</paper>

		<paper id="3003">
			<definition id="0">
				<sentence>Each row in the confusion matrix consists of a set of syllables in the source language that are ( correctly or erroneously ) matched phonetically and statistically to a syllable in the target language .</sentence>
				<definiendum id="0">confusion matrix</definiendum>
				<definiens id="0">consists of a set of syllables in the source language that are ( correctly or erroneously ) matched phonetically and statistically to a syllable in the target language</definiens>
			</definition>
			<definition id="1">
				<sentence>The Internet is one of the largest distributed databases in the world .</sentence>
				<definiendum id="0">Internet</definiendum>
				<definiens id="0">one of the largest distributed databases in the world</definiens>
			</definition>
			<definition id="2">
				<sentence>First , it is converted into /poldə/ using the letter-to-phoneme system , and then according to the phoneme-based syllabification algorithm ( PSA ) , it is divided into /po/ , /l/ , and /də/ , where /l/ is an isolated consonant .</sentence>
				<definiendum id="0">PSA</definiendum>
				<definiendum id="1">/l/</definiendum>
				<definiens id="0">an isolated consonant</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>The translation model , p ( f|e ) defined as a marginal probability obtained by summing over word-level alignments , a , between the source and target sentences : p ( f|e ) = summationdisplay a p ( f , a|e ) .</sentence>
				<definiendum id="0">p</definiendum>
			</definition>
			<definition id="1">
				<sentence>We used the Verbmobil German-English parallel corpus as a source of training data because it has been used extensively in evaluating statistical translation and alignment accuracy .</sentence>
				<definiendum id="0">Verbmobil German-English parallel corpus</definiendum>
				<definiens id="0">a source of training data because it has been used extensively in evaluating statistical translation and alignment accuracy</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>Specifically , each training/test coreference instance i ( NPi , NPj ) ( created from NPj and a preceding NP NPi ) is augmented with a feature whose value is the anaphoricity of NPj as computed by the anaphoricity classifier .</sentence>
				<definiendum id="0">NPj ) (</definiendum>
				<definiens id="0">created from NPj and a preceding NP NPi ) is augmented with a feature whose value is the anaphoricity of NPj as computed by the anaphoricity classifier</definiens>
			</definition>
			<definition id="1">
				<sentence>System Variation BNEWS ( dev ) NPAPER ( dev ) NWIRE ( dev ) Experiments L R P F C R P F C R P F C Table 3 : Results of the coreference systems using a constraint-based , globally-optimized approach to anaphoricity determination on the three ACE held-out development data sets .</sentence>
				<definiendum id="0">System Variation BNEWS</definiendum>
			</definition>
			<definition id="2">
				<sentence>In comparison to the locally-optimized approaches , CBGO achieves better F-measure scores in almost all cases .</sentence>
				<definiendum id="0">CBGO</definiendum>
				<definiens id="0">achieves better F-measure scores in almost all cases</definiens>
			</definition>
			<definition id="3">
				<sentence>The high discriminating power of HEAD MATCH and STR MATCH is a probable consequence of the fact that an NP is likely to be anaphoric if there is a lexically similar noun phrase preceding it in the text .</sentence>
				<definiendum id="0">STR MATCH</definiendum>
				<definiens id="0">a probable consequence of the fact that an NP is likely to</definiens>
			</definition>
			<definition id="4">
				<sentence>Extensive experiments on the three ACE coreference data sets using a symbolic learner ( RIPPER ) and a statistical learner ( MaxEnt ) for training coreference classifiers demonstrate the effectiveness of the constraint-based , globally-optimized approach to anaphoricity determination , which employs our conservativeness-based anaphoricity model .</sentence>
				<definiendum id="0">symbolic learner</definiendum>
				<definiendum id="1">RIPPER</definiendum>
				<definiendum id="2">MaxEnt</definiendum>
				<definiens id="0">employs our conservativeness-based anaphoricity model</definiens>
			</definition>
</paper>

		<paper id="1078">
			<definition id="0">
				<sentence>Inspired by the work of Papineni et al. ( 2002 ) on BLEU , we define a precision-focused family of metrics , using as parameter a non-negative integer N. Part of the definition includes a list of stopwords ( SW ) and a function for extracting the stem of a given word ( ST ) .</sentence>
				<definiendum id="0">ST</definiendum>
				<definiens id="0">a precision-focused family of metrics , using as parameter a non-negative integer N. Part of the definition includes a list of stopwords ( SW ) and a function for extracting the stem of a given word (</definiens>
			</definition>
			<definition id="1">
				<sentence>Suppose we have a given NLP application for which we want to evaluate the candidate answer set Candidates for some input sequences , given a Figure 1 : Evaluation plane for NLP applications adequacy evaluation TIDES−MT ( 2002 ) precision recall precision recall faithfulness compactness low faithfulness compactness AS MT fluency evaluation TIDES−MT ( 2002 ) QA ( 2004 ) correctness evaluation coverage evaluation DUC−AS ( 2001 ) Guideline Axis QA low high high Application Axis where Count ( ngram ) is the number of n-gram counts , and Count clip ( ngram ) is the maximum number of co-occurrences of ngram in the candidate answer and its reference answer .</sentence>
				<definiendum id="0">Count ( ngram )</definiendum>
				<definiendum id="1">Count clip</definiendum>
				<definiens id="0">a given NLP application for which we want to evaluate the candidate answer set Candidates for some input sequences , given a Figure 1 : Evaluation plane for NLP applications adequacy evaluation TIDES−MT</definiens>
				<definiens id="1">the number of n-gram counts</definiens>
				<definiens id="2">the maximum number of co-occurrences of ngram in the candidate answer</definiens>
			</definition>
			<definition id="2">
				<sentence>Because the denominator in the P ( n ) formula consists of a sum over the proposed candidate answers , this formula is a precision-oriented formula , penalizing verbose candidates .</sentence>
				<definiendum id="0">P ( n ) formula</definiendum>
				<definiens id="0">consists of a sum over the proposed candidate answers , this formula is a precision-oriented formula , penalizing verbose candidates</definiens>
			</definition>
			<definition id="3">
				<sentence>This is offset by adding a brevity penalty , BP :    &lt; ⋅ ≥⋅ = − |||| , ||||,1 | ) |/||1 ( rcBife rcBif BP cBr where |c| equals the sum of the lengths of the proposed answers , |r| equals the sum of the lengths of the reference answers , and B is a brevity constant .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">|c| equals the sum of the lengths of the proposed answers , |r| equals the sum of the lengths of the reference answers</definiens>
				<definiens id="1">a brevity constant</definiens>
			</definition>
			<definition id="4">
				<sentence>The BLEU proposed by Papineni et al. ( 2002 ) for automatic evaluation of machine translation is part of the family of metrics PS ( N ) , as the particular metric obtained when N=4 , w n –s are 1/N , the brevity constant B=1 , the list of stop-words SW is empty , and the stemming function ST is the identity function .</sentence>
				<definiendum id="0">BLEU</definiendum>
				<definiendum id="1">stemming function ST</definiendum>
				<definiens id="0">empty , and the</definiens>
			</definition>
			<definition id="5">
				<sentence>As before , suppose we have a given NLP application for which we want to evaluate the candidate answer set Candidates for some input sequences , given a reference answer set where , as before , Count ( ngram ) is the number of n-gram counts , and Count clip ( ngram ) is the maximum number of co-occurrences of ngram in the reference answer and its corresponding candidate answer .</sentence>
				<definiendum id="0">Count clip ( ngram )</definiendum>
				<definiens id="0">the number of n-gram counts</definiens>
				<definiens id="1">the maximum number of co-occurrences of ngram in the reference answer and its corresponding candidate answer</definiens>
			</definition>
			<definition id="6">
				<sentence>Because the denominator in the R ( n ) formula consists of a sum over the reference answers , this formula is essentially a recalloriented formula , which penalizes incomplete candidates .</sentence>
				<definiendum id="0">R ( n ) formula</definiendum>
				<definiens id="0">consists of a sum over the reference answers , this formula is essentially a recalloriented formula , which penalizes incomplete candidates</definiens>
			</definition>
			<definition id="7">
				<sentence>This is offset by adding a wordiness penalty , WP :    &gt; ⋅ ≤⋅ = − |||| , ||||,1 | ) |/||1 ( rcWife rcWif WP rcW where |c| and |r| are defined as before , and W is a wordiness constant .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">a wordiness constant</definiens>
			</definition>
			<definition id="8">
				<sentence>The ROUGE metric proposed by Lin and Hovy ( 2003 ) for automatic evaluation of machineproduced summaries is part of the family of metrics RS ( N ) , as the particular metric obtained when N=1 , w n –s are 1/N , the wordiness constant W=∞ , the list of stop-words SW is their own , and the stemming function ST is the one defined by the Porter stemmer ( Porter 1980 ) .</sentence>
				<definiendum id="0">ROUGE metric</definiendum>
			</definition>
			<definition id="9">
				<sentence>Evaluation The precision-focused metric family PS ( N ) and the recall-focused metric family RS ( N ) defined in the previous sections are unified under the metric family AEv ( α , N ) , defined as : ) ( ) 1 ( ) ( ) ( ) ( ) , ( NPSNRS NPSNRS NAEv ⋅−+⋅ = αα α This formula extends the well-known F-measure that combines recall and precision numbers into a single number ( van Rijsbergen , 1979 ) , by combining recall and precision metric families into a single metric family .</sentence>
				<definiendum id="0">precision-focused metric family PS</definiendum>
				<definiendum id="1">recall-focused metric family RS</definiendum>
				<definiendum id="2">) ( ) ( ) ( )</definiendum>
				<definiens id="0">NPSNRS NPSNRS NAEv ⋅−+⋅ = αα α This formula extends the well-known F-measure that combines recall and precision numbers into a single number ( van Rijsbergen , 1979 ) , by combining recall and precision metric families into a single metric family</definiens>
			</definition>
			<definition id="10">
				<sentence>For the rest of the paper , we restrict the parameters of the AEv ( α , N ) family as follows : α varies continuously in [ 0,1 ] , N varies discretely in { 1,2,3,4 } , the linear weights w n are 1/N , the brevity constant is 1 , the wordiness constant is 2 , the list of stop-words SW is our own 626 stop-word list , and the stemming function ST is the one defined by the Porter stemmer ( Porter 1980 ) .</sentence>
				<definiendum id="0">brevity constant</definiendum>
				<definiendum id="1">wordiness constant</definiendum>
				<definiendum id="2">ST</definiendum>
				<definiendum id="3">Porter stemmer</definiendum>
				<definiens id="0">the parameters of the AEv ( α , N ) family as follows : α varies continuously in [ 0,1 ] , N varies discretely</definiens>
				<definiens id="1">our own 626 stop-word list , and the stemming function</definiens>
			</definition>
			<definition id="11">
				<sentence>Since metric AEv ( 1,4 ) is almost the same as the BLEU metric ( modulo stemming and stop word elimination for unigrams ) , our results confirm the current practice in the Machine Translation community , which commonly uses BLEU for automatic evaluation .</sentence>
				<definiendum id="0">BLEU metric</definiendum>
				<definiens id="0">results confirm the current practice in the Machine Translation community</definiens>
			</definition>
			<definition id="12">
				<sentence>The high correlation of metric AEv ( 0.3,2 ) with human judgment , however , suggests that such a metric is a good candidate for performing automatic evaluation of QA systems that go beyond answering factoid questions .</sentence>
				<definiendum id="0">high correlation of metric AEv</definiendum>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>Researchers depend on these electronic resources to keep abreast of their rapidly changing field .</sentence>
				<definiendum id="0">Researchers</definiendum>
				<definiens id="0">depend on these electronic resources to keep abreast of their rapidly changing field</definiens>
			</definition>
			<definition id="1">
				<sentence>Careful analysis of this extensive corpus presented some interesting facts about what we have termed “Explicit Metalinguistic Operations” ( or EMOs ) in specialized discourse : A ) EMOs usually do not follow the genusdifferentia scheme of aristotelian definitions , nor conform to the rigid and artificial structure of dictionary entries .</sentence>
				<definiendum id="0">Metalinguistic Operations”</definiendum>
				<definiens id="0">or EMOs ) in specialized discourse : A ) EMOs usually do not follow the genusdifferentia scheme of aristotelian definitions , nor conform to the rigid and artificial structure of dictionary entries</definiens>
			</definition>
			<definition id="2">
				<sentence>to be split ( at least methodologically ) into two distinct systems that share the same rules and elements : a metalanguage , which is a language that is used to talk about another one , and an object language , which in turn can refer to and describe objects in the mind or in the physical world .</sentence>
				<definiendum id="0">metalanguage</definiendum>
				<definiens id="0">a language that is used to talk about another one , and an object language , which in turn can refer to and describe objects in the mind or in the physical world</definiens>
			</definition>
			<definition id="3">
				<sentence>For our learning experiments ( an approach we have called contextual feature language models ) , we selected two well-known algorithms that showed promise for this classification task.7 The naive Bayes ( NB ) algorithm estimates the conditional probability of a set of features given a label , using the product of the probabilities of the individual features given that label .</sentence>
				<definiendum id="0">naive Bayes</definiendum>
				<definiens id="0">estimates the conditional probability of a set of features given a label , using the product of the probabilities of the individual features given that label</definiens>
			</definition>
			<definition id="4">
				<sentence>The Maximum Entropy model establishes a probability distribution that favors entropy , or uniformity , subject to the constraints encoded in the feature-label correlation .</sentence>
				<definiendum id="0">Maximum Entropy model</definiendum>
				<definiens id="0">establishes a probability distribution that favors entropy , or uniformity , subject to the constraints encoded in the feature-label correlation</definiens>
			</definition>
			<definition id="5">
				<sentence>When training our ME classifiers , Generalized ( GISMax ) and Improved Iterative Scaling ( IISMax ) algorithms are used to estimate the optimal maximum entropy of a feature set , given a corpus .</sentence>
				<definiendum id="0">Generalized ( GISMax</definiendum>
				<definiens id="0">used to estimate the optimal maximum entropy of a feature set , given a corpus</definiens>
			</definition>
			<definition id="6">
				<sentence>MIDs are semi-structured resources ( midway between raw corpora and structured lexical bases ) that can be further processed to convert them into usable data sources , along the lines suggested by Vossen and Copestake ( 1993 ) for the syntactic kernels of lexicographic definitions , or by Pustejovsky et al. ( 2002 ) using corpus analytics to increase the semantic type coverage of the NLM UMLS ontology .</sentence>
				<definiendum id="0">MIDs</definiendum>
				<definiens id="0">semi-structured resources ( midway between raw corpora and structured lexical bases ) that can be further processed to convert them into usable data sources , along the lines suggested by Vossen and Copestake ( 1993 ) for the syntactic kernels of lexicographic definitions</definiens>
			</definition>
</paper>

		<paper id="3009">
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>A crucial component of any speech recognizer is the language model ( LM ) , which assigns scores or probabilities to candidate output strings in a speech recognizer .</sentence>
				<definiendum id="0">LM</definiendum>
				<definiens id="0">assigns scores or probabilities to candidate output strings in a speech recognizer</definiens>
			</definition>
			<definition id="1">
				<sentence>Recognition performance is a direct measure of the effectiveness of a language model ; an indirect measure which is frequently proposed within these approaches is the perplexity of the LM ( i.e. , the log probability it assigns to some held-out data set ) .</sentence>
				<definiendum id="0">Recognition performance</definiendum>
				<definiendum id="1">LM</definiendum>
				<definiens id="0">a direct measure of the effectiveness of a language model</definiens>
				<definiens id="1">the log probability it assigns to some held-out data set )</definiens>
			</definition>
			<definition id="2">
				<sentence>The components GEN ; and define a mapping from an input x to an output F ( x ) through F ( x ) = argmax y2GEN ( x ) ( x ; y ) ( 1 ) where ( x ; y ) is the inner product Ps s s ( x ; y ) .</sentence>
				<definiendum id="0">; y )</definiendum>
				<definiens id="0">the inner product Ps s s ( x ; y )</definiens>
			</definition>
			<definition id="3">
				<sentence>The decoding algorithm is a method for searching for the y that maximizes Eq .</sentence>
				<definiendum id="0">decoding algorithm</definiendum>
			</definition>
			<definition id="4">
				<sentence>CRFs use the parameters to define a conditional distribution over the members of GEN ( x ) for a given input x : p ( yjx ) = 1Z ( x ; ) exp ( ( x ; y ) ) where Z ( x ; ) = Py2GEN ( x ) exp ( ( x ; y ) ) is a normalization constant that depends on x and .</sentence>
				<definiendum id="0">CRFs</definiendum>
				<definiendum id="1">Z (</definiendum>
				<definiendum id="2">y ) )</definiendum>
			</definition>
			<definition id="5">
				<sentence>The derivative of the objective function with respect to a parameter s at parameter values is @ LLR @ s = NX i=1 2 4 s ( xi ; yi ) X y2GEN ( xi ) p ( yjxi ) s ( xi ; y ) 3 5 s 2 ( 4 ) Note that LLR ( ) is a convex function , so that there is a globally optimal solution and the optimization method will find it .</sentence>
				<definiendum id="0">LLR ( )</definiendum>
				<definiens id="0">a convex function</definiens>
			</definition>
			<definition id="6">
				<sentence>In the language modeling setting we takeX to be the set of all possible acoustic inputs ; Y is the set of all possible strings , , for some vocabulary .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">the set of all possible strings , , for some vocabulary</definiens>
			</definition>
			<definition id="7">
				<sentence>Each xi is an utterance ( a sequence of acoustic feature-vectors ) , and GEN ( xi ) is the set of possible transcriptions under a first pass recognizer .</sentence>
				<definiendum id="0">GEN ( xi )</definiendum>
				<definiens id="0">an utterance ( a sequence of acoustic feature-vectors ) , and</definiens>
				<definiens id="1">the set of possible transcriptions under a first pass recognizer</definiens>
			</definition>
			<definition id="8">
				<sentence>( GEN ( xi ) is a huge set , but will be represented compactly using a lattice – we will discuss this in detail shortly ) .</sentence>
				<definiendum id="0">GEN ( xi )</definiendum>
				<definiens id="0">a huge set</definiens>
			</definition>
			<definition id="9">
				<sentence>In the general case , each component i ( x ; y ) could be essentially any function of the acoustic input x and the candidate transcription y. The first feature we define is 0 ( x ; y ) as the log-probability of y givenxunder the lattice produced by the baseline recognizer .</sentence>
				<definiendum id="0">component i ( x ; y )</definiendum>
				<definiens id="0">the log-probability of y givenxunder the lattice produced by the baseline recognizer</definiens>
			</definition>
			<definition id="10">
				<sentence>For our purpose , a WFA A = ( ; Q ; qs ; F ; E ; ) , where is the vocabulary , Q is a ( finite ) set of states , qs 2 Q is a unique start state , F Q is a set of final states , E is a ( finite ) set of transitions , and : F !</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">E</definiendum>
				<definiens id="0">a unique start state</definiens>
				<definiens id="1">a set of final states</definiens>
				<definiens id="2">a ( finite ) set of transitions</definiens>
			</definition>
			<definition id="11">
				<sentence>R is a function from final states to final weights .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">a function from final states to final weights</definiens>
			</definition>
			<definition id="12">
				<sentence>Each transition e2E is a tuple e = ( l [ e ] ; p [ e ] ; n [ e ] ; w [ e ] ) , where l [ e ] 2 is a label ( in our case , words ) , p [ e ] 2Q is the origin state of e , n [ e ] 2Q is the destination state of e , and w [ e ] 2 R is the weight of the transition .</sentence>
				<definiendum id="0">transition e2E</definiendum>
				<definiendum id="1">2Q</definiendum>
				<definiens id="0">the weight of the transition</definiens>
			</definition>
			<definition id="13">
				<sentence>Given a WFA A , a string y , and an error-function E ( y ; w ) , this operation returns ^ = argmin 2 AE ( y ; l [ ] ) .</sentence>
				<definiendum id="0">w )</definiendum>
				<definiens id="0">a string y , and an error-function E ( y ;</definiens>
			</definition>
			<definition id="14">
				<sentence>The latticeLx is an acyclic WFA , representing a weighted set of possible transcriptions of x under the baseline recognizer .</sentence>
				<definiendum id="0">latticeLx</definiendum>
				<definiens id="0">an acyclic WFA , representing a weighted set of possible transcriptions of x under the baseline recognizer</definiens>
			</definition>
			<definition id="15">
				<sentence>Before describing methods for training a discriminative language model using perceptron and CRF algorithms , we give a little more detail about the structure of D , focusing on how n-gram language models can be implemented with finite-state techniques .</sentence>
				<definiendum id="0">CRF</definiendum>
				<definiens id="0">training a discriminative language model using perceptron</definiens>
			</definition>
			<definition id="16">
				<sentence>Feature selection can be critical in our domain , as training and applying a discriminative language model over all n-grams seen in the training data ( in either correct or incorrect transcriptions ) may be computationally very demanding .</sentence>
				<definiendum id="0">Feature selection</definiendum>
				<definiens id="0">training and applying a discriminative language model over all n-grams seen in the training data ( in either correct or incorrect transcriptions</definiens>
			</definition>
			<definition id="17">
				<sentence>The GRM library , which was presented in Allauzen et al. ( 2003 ) , has a direct implementation of the function ExpCount , which simultaneously calculates the expected value of all n-grams of order less than or equal to a given n in a latticeL .</sentence>
				<definiendum id="0">GRM library</definiendum>
				<definiens id="0">has a direct implementation of the function ExpCount , which simultaneously calculates the expected value of all n-grams of order less than or equal to a given n in a latticeL</definiens>
			</definition>
			<definition id="18">
				<sentence>Then we wish to compute Ei for i = 1 : : : N , where Ei = X wn12Li Pi ( wn1 ) log Qi ( wn1 ) = X wn12Li X k=1 : : : n Pi ( wn1 ) log Qi ( wkjwk 11 ) ( 9 ) The approximation is to make the following Markov assumption : Ei X wn12Li X k=1 : : : n Pi ( wn1 ) log Qi ( wkjwk 1k 2 ) = X xyz2Si ExpCount ( L0i ; xyz ) log Qi ( zjxy ) ( 10 ) where Si is the set of all trigrams seen in Li .</sentence>
				<definiendum id="0">Si</definiendum>
				<definiens id="0">: : N , where Ei = X wn12Li Pi ( wn1 ) log Qi ( wn1 ) = X wn12Li X k=1 : : : n Pi ( wn1 ) log Qi ( wkjwk 11 ) ( 9 ) The approximation is to make the following Markov assumption : Ei X wn12Li X k=1 : : : n Pi ( wn1 ) log Qi ( wkjwk 1k 2</definiens>
				<definiens id="1">the set of all trigrams seen in Li</definiens>
			</definition>
			<definition id="19">
				<sentence>The rt02 set consists of 6081 sentences ( 63804 words ) and has three subsets : Switchboard 1 , Switchboard 2 , Switchboard Cellular .</sentence>
				<definiendum id="0">rt02 set</definiendum>
				<definiens id="0">consists of 6081 sentences ( 63804 words ) and has three subsets : Switchboard 1</definiens>
			</definition>
			<definition id="20">
				<sentence>The rt03 set consists of 9050 sentences ( 76083 words ) and has two subsets : Switchboard and Fisher .</sentence>
				<definiendum id="0">rt03 set</definiendum>
				<definiens id="0">consists of 9050 sentences ( 76083 words ) and has two subsets : Switchboard and Fisher</definiens>
			</definition>
			<definition id="21">
				<sentence>The training set consists of 276726 transcribed utterances ( 3047805 words ) , with an additional 20854 utterances ( 249774 words ) as held out data .</sentence>
				<definiendum id="0">training set</definiendum>
				<definiens id="0">consists of 276726 transcribed utterances ( 3047805 words ) , with an additional 20854 utterances ( 249774 words ) as held out data</definiens>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>Context-free grammars ( CFGs ) are standardly used in computational linguistics as formal models of the syntax of natural language , associating sentences with all their possible derivations .</sentence>
				<definiendum id="0">Context-free grammars ( CFGs</definiendum>
				<definiens id="0">standardly used in computational linguistics as formal models of the syntax of natural language , associating sentences with all their possible derivations</definiens>
			</definition>
			<definition id="1">
				<sentence>However , such probability assignments are outside the reach of the usual training algorithms for PDAs , which always produce proper PDAs .</sentence>
				<definiendum id="0">such probability assignments</definiendum>
				<definiens id="0">are outside the reach of the usual training algorithms for PDAs , which always produce proper PDAs</definiens>
			</definition>
			<definition id="2">
				<sentence>A PCFG is a pair ( G ; p ) consisting of a CFG G and a probability function p from R to real numbers in the interval [ 0 ; 1 ] .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">a pair ( G ; p ) consisting of a CFG G and a probability function p from R to real numbers in the</definiens>
			</definition>
			<definition id="3">
				<sentence>XY , called a push transition , YX 7 !</sentence>
				<definiendum id="0">XY</definiendum>
				<definiens id="0">called a push transition</definiens>
			</definition>
			<definition id="4">
				<sentence>A configuration of a PDT is a triple ( ; w ; v ) , where 2 Q is a stack , w 2 1 is the remaining input , and v 2 2 is the output generated so far .</sentence>
				<definiendum id="0">configuration of a PDT</definiendum>
				<definiens id="0">a triple</definiens>
				<definiens id="1">a stack</definiens>
			</definition>
			<definition id="5">
				<sentence>If ( Xin ; w ; '' ) ‘c ( Xfin ; '' ; v ) , then c is a complete computation of w , and the output string v is denoted out ( c ) .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">a complete computation of w</definiens>
			</definition>
			<definition id="6">
				<sentence>A PDT is reduced if each transition in occurs in some complete computation .</sentence>
				<definiendum id="0">PDT</definiendum>
				<definiens id="0">reduced if each transition in occurs in some complete computation</definiens>
			</definition>
			<definition id="7">
				<sentence>A PPDT is a pair ( A ; p ) consisting of a PDT A and a probability functionpfrom to real numbers in the interval [ 0 ; 1 ] .</sentence>
				<definiendum id="0">PPDT</definiendum>
				<definiens id="0">a pair ( A ; p ) consisting of a PDT A and a probability functionpfrom to real numbers in the</definiens>
			</definition>
			<definition id="8">
				<sentence>A probabilistic parsing strategy is defined to be a function S that maps a reduced , proper and consistent PCFG ( G ; pG ) to a triple S ( G ; pG ) = ( A ; pA ; f ) , where ( A ; pA ) is a reduced , proper and consistent PPDT , with the same properties as a ( non-probabilistic ) parsing strategy , and in addition : For each complete derivation d and each complete computation c such that f ( c ) = d , pG ( d ) equals pA ( c ) .</sentence>
				<definiendum id="0">probabilistic parsing strategy</definiendum>
				<definiendum id="1">pG</definiendum>
				<definiens id="0">a function S that maps a reduced , proper and consistent PCFG ( G</definiens>
				<definiens id="1">a reduced , proper and consistent PPDT , with the same properties as a ( non-probabilistic ) parsing strategy</definiens>
			</definition>
			<definition id="9">
				<sentence>Informally , a dead computation is a computation that can not be continued to become a complete computation .</sentence>
				<definiendum id="0">dead computation</definiendum>
				<definiens id="0">a computation that can not be continued to become a complete computation</definiens>
			</definition>
			<definition id="10">
				<sentence>Lemma 1 For each reduced CFG G , there is a probability function pG such that PCFG ( G ; pG ) is proper and consistent , and pG ( d ) &gt; 0 for all complete derivations d. Proof .</sentence>
				<definiendum id="0">pG</definiendum>
			</definition>
			<definition id="11">
				<sentence>Furthermore , it has been shown in ( Chi and Geman , 1998 ; S´anchez and Bened´ı , 1997 ) that a PCFG ( G ; pG ) is consistent if pG was obtained by maximum-likelihood estimation using a set of derivations .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">consistent if pG was obtained by maximum-likelihood estimation using a set of derivations</definiens>
			</definition>
			<definition id="12">
				<sentence>It follows from Lemma 1 that there is a probability function pG such that ( G ; pG ) is a proper and consistent PCFG and pG ( d ) &gt; 0 for all complete derivations d. Assume we also have a probability function pA such that ( A ; pA ) is a proper and consistent PPDT andpA ( c0 ) = pG ( f ( c0 ) ) for each complete computation c0 .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiendum id="1">pG</definiendum>
				<definiendum id="2">pA )</definiendum>
				<definiens id="0">a proper and consistent</definiens>
				<definiens id="1">a proper and consistent PPDT andpA ( c0 ) = pG ( f ( c0 ) ) for each complete computation c0</definiens>
			</definition>
			<definition id="13">
				<sentence>Lemma 4 Given a non-proper PCFG ( G ; pG ) , G = ( ; N ; S ; R ) , there is a probability functionp0G such that PCFG ( G ; p0G ) is proper and , for every complete derivation d , p0G ( d ) = 1C pG ( d ) , where C =P S ) d0w ; w2 pG ( d 0 ) .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">a probability functionp0G such that PCFG ( G ; p0G ) is proper and , for every complete derivation d , p0G ( d ) = 1C pG ( d ) , where C =P S ) d0w ; w2 pG ( d 0 )</definiens>
			</definition>
			<definition id="14">
				<sentence>We will show that there is a probability function pA such that ( A ; pA ) is a proper and consistent PPDT , and pA ( c ) = pG ( f ( c ) ) for all complete computations c. We first construct a PPDT ( A ; p0A ) as follows .</sentence>
				<definiendum id="0">PPDT</definiendum>
				<definiendum id="1">p0A</definiendum>
				<definiens id="0">a proper and consistent PPDT</definiens>
			</definition>
</paper>

		<paper id="3013">
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>The graph matching is an optimization process that finds the best matching between two graphs based on constraints modeled as links or nodes in these graphs .</sentence>
				<definiendum id="0">graph matching</definiendum>
				<definiens id="0">an optimization process that finds the best matching between two graphs based on constraints modeled as links or nodes in these graphs</definiens>
			</definition>
			<definition id="1">
				<sentence>An ARG consists of a set of nodes that are connected by a set of edges .</sentence>
				<definiendum id="0">ARG</definiendum>
				<definiens id="0">consists of a set of nodes that are connected by a set of edges</definiens>
			</definition>
			<definition id="2">
				<sentence>The function EdgeSim ( r xy , γ mn ) measures the similarity between r xy and γ mn , which depends on the semantic and temporal constraints of the corresponding edges .</sentence>
				<definiendum id="0">mn</definiendum>
				<definiens id="0">depends on the semantic and temporal constraints of the corresponding edges</definiens>
			</definition>
			<definition id="3">
				<sentence>When the algorithm converges , P ( a x , α m ) gives the matching probabilities between the referent node a x and the referring node α m that maximizes the overall compatibility function .</sentence>
				<definiendum id="0">P (</definiendum>
				<definiens id="0">a x , α m ) gives the matching probabilities between the referent node a x and the referring node α m that maximizes the overall compatibility function</definiens>
			</definition>
			<definition id="4">
				<sentence>Id ( a x , α m ) captures the constraint of the compatibilities between identifiers specified in a x and α m .</sentence>
				<definiendum id="0">Id</definiendum>
				<definiens id="0">a x , α m ) captures the constraint of the compatibilities between identifiers specified in a x</definiens>
			</definition>
			<definition id="5">
				<sentence>Because identifiers are usually unique , the corresponding constraint is a greater indicator of node matching if the identifier expressed from a referring expression matches the identifier of a potential referent .</sentence>
				<definiendum id="0">corresponding constraint</definiendum>
				<definiens id="0">a greater indicator of node matching if the identifier expressed from a referring expression matches the identifier of a potential referent</definiens>
			</definition>
			<definition id="6">
				<sentence>Thus the temporal constraint in our approach is a graded constraint , which is defined as follows : ) 2000 | ) ( ) ( | exp ( ) , ( mx mx BeginTimeaBeginTime aTemp α α − −= This constraint indicates that the closer a referring expression and a potential referent in terms of their temporal alignment ( regardless of the absolute precedence relationship ) , the more compatible they are .</sentence>
				<definiendum id="0">graded constraint</definiendum>
				<definiens id="0">follows : ) 2000 | ) ( ) ( | exp ( ) , ( mx mx BeginTimeaBeginTime aTemp α α − −= This constraint indicates that the closer a referring expression and a potential referent in terms of their temporal alignment ( regardless of the absolute precedence relationship</definiens>
			</definition>
			<definition id="7">
				<sentence>Optimality Theory ( OT ) is a theory of language and grammar , developed by Alan Prince and Paul Smolensky ( Prince and Smolensky , 1993 ) .</sentence>
				<definiendum id="0">Optimality Theory ( OT )</definiendum>
				<definiens id="0">a theory of language and grammar</definiens>
			</definition>
			<definition id="8">
				<sentence>An innovation of Optimality Theory is the conception of these constraints as soft , which means violable and conflicting .</sentence>
				<definiendum id="0">innovation of Optimality Theory</definiendum>
				<definiens id="0">the conception of these constraints as soft , which means violable and conflicting</definiens>
			</definition>
			<definition id="9">
				<sentence>In each entry form “a ( b ) , c ( d ) ” , “a” indicates the number of inputs in which the referring expressions were correctly recognized by the speech recognizer ; “b” indicates the number of inputs in which the referring expressions were correctly recognized and were correctly resolved ; “c” indicates the number of inputs in which the referring expressions were not correctly recognized ; “d” indicates the number of inputs in which the referring expressions also were not correctly recognized , but were correctly resolved .</sentence>
				<definiendum id="0">“b”</definiendum>
			</definition>
</paper>

		<paper id="1070">
			<definition id="0">
				<sentence>‘S , where S is the former start symbol , and the new nonterminal Sy becomes the start symbol of the augmented grammar .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">the former start symbol , and the new nonterminal Sy becomes the start symbol of the augmented grammar</definiens>
			</definition>
			<definition id="1">
				<sentence>Q ( a swap transition ) , and of the formP Q a ; b7 !</sentence>
				<definiendum id="0">Q</definiendum>
			</definition>
			<definition id="2">
				<sentence>R ( a pop transition ) .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">a pop transition</definiens>
			</definition>
			<definition id="3">
				<sentence>For the PDT that implements the LR strategy , the stack symbols are the LR states , plus symbols of the form [ p ; X ] , wherepis an LR state andX is a grammar symbol , and symbols of the form ( p ; A ; m ) , where p is an LR state , A is the left-hand side of some rule , and m is the length of some prefix of the right-hand side of that rule .</sentence>
				<definiendum id="0">stack symbols</definiendum>
				<definiendum id="1">andX</definiendum>
				<definiendum id="2">p</definiendum>
				<definiendum id="3">m</definiendum>
				<definiens id="0">the LR states , plus symbols of the form [ p ; X ] , wherepis an LR state</definiens>
				<definiens id="1">a grammar symbol , and symbols of the form ( p ; A ; m )</definiens>
				<definiens id="2">the length of some prefix of the right-hand side of that rule</definiens>
			</definition>
			<definition id="4">
				<sentence>The computed output consists of the string of terminals b1 bn0 from the output components of the applied transitions .</sentence>
				<definiendum id="0">computed output</definiendum>
				<definiens id="0">consists of the string of terminals b1 bn0 from the output components of the applied transitions</definiens>
			</definition>
			<definition id="5">
				<sentence>[ p0 ; A ] q. This is a harmless modification , which increases the number of steps in any computation by at most a factor 2 .</sentence>
				<definiendum id="0">] q. This</definiendum>
				<definiens id="0">a harmless modification , which increases the number of steps</definiens>
			</definition>
			<definition id="6">
				<sentence>Generated language consists of right-most derivations in reverse .</sentence>
				<definiendum id="0">Generated language</definiendum>
				<definiens id="0">consists of right-most derivations in reverse</definiens>
			</definition>
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>Hence , grammar induction consists of two parts : choosing the class of languages amongst which to search and designing the procedure for performing the search .</sentence>
				<definiendum id="0">grammar induction</definiendum>
				<definiens id="0">consists of two parts : choosing the class of languages amongst which to search and designing the procedure for performing the search</definiens>
			</definition>
			<definition id="1">
				<sentence>A probabilistic constrained W-grammar ( PCWgrammar ) consists of two different sets of PCF-like rules called pseudo-rules and meta-rules respectively and three pairwise disjoint sets of symbols : variables , non-terminals and terminals .</sentence>
				<definiendum id="0">probabilistic constrained W-grammar ( PCWgrammar )</definiendum>
				<definiens id="0">consists of two different sets of PCF-like rules called pseudo-rules and meta-rules respectively and three pairwise disjoint sets of symbols : variables , non-terminals and terminals</definiens>
			</definition>
			<definition id="2">
				<sentence>Let W = ( V , NT , T , S , m−→ , s−→ ) be a CWgrammar such that the set of variable , non-terminals meta-rules pseudo-rules Adj m−→0.5 AdjAdj S s−→1 AdjNoun Adj m−→0.5 Adj Adj s−→0.1 big Noun s−→1 ball ... and terminals are defined as follows : V = { Adj } , NT = { S , Adj , Noun } , T = { ball , big , fat , red , green , ... } .</sentence>
				<definiendum id="0">NT , T , S , m−→ , s−→ )</definiendum>
				<definiens id="0">be a CWgrammar such that the set of variable , non-terminals meta-rules pseudo-rules Adj m−→0.5 AdjAdj S s−→1 AdjNoun Adj m−→0.5 Adj Adj s−→0.1 big Noun s−→1 ball ... and terminals are defined as follows : V = { Adj }</definiens>
				<definiens id="1">big , fat , red , green , ... }</definiens>
			</definition>
			<definition id="3">
				<sentence>We build our final grammar G with starting symbol S , by defining its meta-rules as the disjoint union of all rules in GwL and GwR ( for all POS w ) , its set of pseudo-rules as the union of the sets { W s−→1 SwLwSwR and S s−→1 SwLwSwR } , where W is a unique new variable symbol associated to w. When we use two automata for all parts of speech , the grammar is defined as follows .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">final grammar G with starting symbol S , by defining its meta-rules as the disjoint union of all rules in GwL and GwR ( for all POS w ) , its set of pseudo-rules as the union of the sets { W s−→1 SwLwSwR and S s−→1 SwLwSwR } , where</definiens>
				<definiens id="1">a unique new variable symbol associated to w. When we use two automata for all parts of speech</definiens>
			</definition>
			<definition id="4">
				<sentence>Its starting symbol is S , its set of meta-rules is the disjoint union of all rules in GwL and GwR ( for all POS w ) , its set of pseudorules is { W s−→1 SwLwSwR , S s−→1 SwLwSwR : w is a POS in the PTB and W is a unique new variable symbol associated to w } .</sentence>
				<definiendum id="0">S s−→1 SwLwSwR</definiendum>
				<definiendum id="1">W</definiendum>
				<definiens id="0">the disjoint union of all rules in GwL and GwR ( for all POS w</definiens>
				<definiens id="1">a unique new variable symbol associated to w }</definiens>
			</definition>
			<definition id="5">
				<sentence>The Minimum Discrimination Information ( MDI ) algorithm ( Thollard et al. , 2000 ) improves over ALERGIA and uses Kullback-Leibler divergence for deciding when to merge states .</sentence>
				<definiendum id="0">Minimum Discrimination Information</definiendum>
				<definiens id="0">improves over ALERGIA and uses Kullback-Leibler divergence for deciding when to merge states</definiens>
			</definition>
			<definition id="6">
				<sentence>Specifically , assume that A1 is a temporary solution of the algorithm and that A2 is a tentative new solution derived from A1 .</sentence>
				<definiendum id="0">A2</definiendum>
				<definiens id="0">a temporary solution of the algorithm and that</definiens>
				<definiens id="1">a tentative new solution derived from A1</definiens>
			</definition>
			<definition id="7">
				<sentence>∆ ( A1 , A2 ) = D ( A0||A2 ) − D ( A0||A1 ) denotes the divergence increment while going from A1 to A2 , where D ( A0||Ai ) is the Kullback-Leibler divergence or relative entropy between the two distributions generated by the corresponding automata ( Cover and Thomas , 1991 ) .</sentence>
				<definiendum id="0">A0||A1 )</definiendum>
			</definition>
			<definition id="8">
				<sentence>The first , called test sample perplexity ( PP ) , is based on the per symbol loglikelihood of strings x belonging to a test sample according to the distribution defined by the automaton .</sentence>
				<definiendum id="0">called test sample perplexity ( PP )</definiendum>
				<definiens id="0">based on the per symbol loglikelihood of strings x belonging to a test sample according to the distribution defined by the automaton</definiens>
			</definition>
			<definition id="9">
				<sentence>Formally , LL = − 1|S|summationtextx∈S log ( P ( x ) ) , where P ( x ) is the probability assigned to the string x by the automata .</sentence>
				<definiendum id="0">P ( x )</definiendum>
				<definiens id="0">the probability assigned to the string x by the automata</definiens>
			</definition>
			<definition id="10">
				<sentence>A missed sample is a string in the test sample that the automaton failed to accept .</sentence>
				<definiendum id="0">missed sample</definiendum>
				<definiens id="0">a string in the test sample that the automaton failed to accept</definiens>
			</definition>
			<definition id="11">
				<sentence>MDI presents a way to compact rule information on the PTB ; of course , other approaches exists .</sentence>
				<definiendum id="0">MDI</definiendum>
				<definiens id="0">presents a way to compact rule information on the PTB</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>A collocation is an arbitrary and recurrent word combination ( Benson , 1990 ) .</sentence>
				<definiendum id="0">collocation</definiendum>
				<definiens id="0">an arbitrary and recurrent word combination</definiens>
			</definition>
			<definition id="1">
				<sentence>N erefreq ep e tri ) , , ( ) ( 21 = ( 2 ) where ) , , ( 21 erefreq e represents the frequency of triple tri e .</sentence>
				<definiendum id="0">erefreq e</definiendum>
				<definiens id="0">the frequency of triple tri e</definiens>
			</definition>
			<definition id="2">
				<sentence>N represents the total counts of all the English triples in the training corpus .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the total counts of all the English triples in the training corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>For the acquired collocations , we try to extract their translations from the other monolingual Train language model for English triple ) ( tri ep ; Initialize word translation probabilities ) | ( ecp head and ) | ( ecp dep uniformly as in Equation ( 8 ) ; Iterate Set ) | ( ecscore head and ) | ( ecscore dep to 0 for all dictionary entries ( c , e ) ; for all Chinese triples ) , , ( 21 crcc ctri = for all candidate English triple translations ) , , ( 21 eree etri = compute triple translation probability ) | ( tritri cep by ) | ( ) | ( ) | ( ) ( 2211 ecdepheadtri rrpecpecpep end for normalize ) | ( tritri cep , so that their sum is 1 ; for all triple translation ) , , ( 21 eree etri = add ) | ( tritri cep to ) | ( 11 ecscore head add ) | ( tritri cep to ) | ( 22 ecscore dep endfor endfor for all translation pairs ( c , e ) set ) | ( ecp head to normalized ) | ( ecscore head ; set ) | ( ecp dep to normalized ) | ( ecscore dep ; endfor enditerate corpus using the triple translation model trained with the method proposed in section 3 .</sentence>
				<definiendum id="0">endfor enditerate</definiendum>
				<definiens id="0">ecscore dep to 0 for all dictionary entries ( c , e ) ; for all Chinese triples ) , , ( 21 crcc ctri = for all candidate English triple translations ) , , ( 21 eree etri = compute triple translation probability ) | ( tritri cep by ) | ( ) |</definiens>
				<definiens id="1">tritri cep , so that their sum is 1 ; for all triple translation ) , , ( 21 eree etri = add ) | ( tritri cep to ) | ( 11 ecscore head add ) | ( tritri cep to ) | ( 22 ecscore dep endfor endfor for all translation pairs ( c , e ) set ) |</definiens>
			</definition>
			<definition id="4">
				<sentence>baseline experiment , Model A selects the highestfrequency translation for each word in triple ; Model B selects translation with the maximal target triple probability , as proposed in ( Dagan 1994 ) ; Model C selects translation using both language model and translation model , but the translation probability is simulated by a similarity score which is estimated from monolingual corpus using mutual information measure ( Zhou et al. , 2001 ) .</sentence>
				<definiendum id="0">Model A</definiendum>
			</definition>
			<definition id="5">
				<sentence>The oracle score is the upper bound accuracy under the conditions of current translation dictionary and standard test set .</sentence>
				<definiendum id="0">oracle score</definiendum>
				<definiens id="0">the upper bound accuracy under the conditions of current translation dictionary and standard test set</definiens>
			</definition>
			<definition id="6">
				<sentence>Top N accuracy is defined as the percentage of triples whose selected top N translations include correct translations .</sentence>
				<definiendum id="0">Top N accuracy</definiendum>
				<definiens id="0">the percentage of triples whose selected top N translations include correct translations</definiens>
			</definition>
</paper>

		<paper id="3011">
			<definition id="0">
				<sentence>Gagné and Shoben ( 1997 ) found that combinations involving a relation used frequently with the modifier were easier to interpret than combinations involving a less frequent relation , while the frequency distribution of the head noun had no influence .</sentence>
				<definiendum id="0">Gagné</definiendum>
				<definiens id="0">found that combinations involving a relation used frequently with the modifier were easier to interpret than combinations involving a less frequent relation</definiens>
			</definition>
			<definition id="1">
				<sentence>Gagné concluded that when the prime and target share the same modifier , relation priming increases the availability of a selected relation within the modifier’s relational distribution .</sentence>
				<definiendum id="0">relation priming</definiendum>
				<definiens id="0">increases the availability of a selected relation within the modifier’s relational distribution</definiens>
			</definition>
			<definition id="2">
				<sentence>Participants sat in front of a computer screen and placed the index finger of their left hand on the F key of the keyboard and the index finger of their right hand on the J key .</sentence>
				<definiendum id="0">Participants</definiendum>
				<definiens id="0">sat in front of a computer screen and placed the index finger of their left hand on the F key of the keyboard and the index finger of their right hand on the J key</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>For the dependency model , we define the probability of a dependency structure as follows : P ( jS ) = X d2 ( ) P ( d ; jS ) ( 1 ) where is a dependency structure , S is a sentence and ( ) is the set of derivations which lead to .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">( )</definiendum>
				<definiens id="0">the probability of a dependency structure as follows : P ( jS ) = X d2 ( ) P ( d ; jS ) ( 1 ) where is a dependency structure</definiens>
				<definiens id="1">a sentence and</definiens>
				<definiens id="2">the set of derivations which lead to</definiens>
			</definition>
			<definition id="1">
				<sentence>Each argument slot in a CCG lexical category represents a dependency relation , and a dependency is defined as a 5-tuple hh f ; f ; s ; ha ; li , where h f is the head word of the lexical category , f is the lexical category , s is the argument slot , ha is the head word of the argument , and l indicates whether the dependency is long-range .</sentence>
				<definiendum id="0">CCG lexical category</definiendum>
				<definiendum id="1">h f</definiendum>
				<definiendum id="2">ha</definiendum>
				<definiens id="0">represents a dependency relation</definiens>
				<definiens id="1">a 5-tuple hh f</definiens>
			</definition>
			<definition id="2">
				<sentence>Log-linear models ( also known as Maximum Entropy models ) are popular in NLP because of the ease with which discriminating features can be included in the model .</sentence>
				<definiendum id="0">Log-linear models</definiendum>
				<definiens id="0">Maximum Entropy models ) are popular in NLP because of the ease with which discriminating features can be included in the model</definiens>
			</definition>
			<definition id="3">
				<sentence>The function fi is a feature of the parse which can be any real-valued function over the space of parses .</sentence>
				<definiendum id="0">function fi</definiendum>
				<definiens id="0">a feature of the parse which can be any real-valued function over the space of parses</definiens>
			</definition>
			<definition id="4">
				<sentence>ZS is a normalising constant which ensures that P ( !</sentence>
				<definiendum id="0">ZS</definiendum>
				<definiens id="0">a normalising constant which ensures that P (</definiens>
			</definition>
			<definition id="5">
				<sentence>jS ) is a probability distribution : ZS = X !</sentence>
				<definiendum id="0">jS )</definiendum>
			</definition>
			<definition id="6">
				<sentence>A feature is a count of the number of times some configuration occurs in d or the number of times some dependency occurs in .</sentence>
				<definiendum id="0">feature</definiendum>
			</definition>
			<definition id="7">
				<sentence>nX i=1 2i 2 2i where n is the number of features .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of features</definiens>
			</definition>
			<definition id="8">
				<sentence>L-BFGS is an iterative algorithm which requires the gradient of the objective function to be computed at each iteration .</sentence>
				<definiendum id="0">L-BFGS</definiendum>
				<definiens id="0">an iterative algorithm which requires the gradient of the objective function to be computed at each iteration</definiens>
			</definition>
			<definition id="9">
				<sentence>A feature forest is a tuplehC ; D ; R ; ; iwhere : a0 C is a set of conjunctive nodes ; a0 D is a set of disjunctive nodes ; a0 R D is a set of root disjunctive nodes ; a0 : D !</sentence>
				<definiendum id="0">feature forest</definiendum>
				<definiendum id="1">C</definiendum>
				<definiendum id="2">a0 R D</definiendum>
				<definiens id="0">a set of conjunctive nodes ; a0 D is a set of disjunctive nodes ;</definiens>
			</definition>
			<definition id="10">
				<sentence>2D is a disjunctive daughter function .</sentence>
				<definiendum id="0">2D</definiendum>
			</definition>
			<definition id="11">
				<sentence>cdeps ( c ) is the number of correct dependencies on conjunctive node c , and takes the value 1 if there are any incorrect dependencies on c. dmax ( c ) is 3A more complete description of CCG feature forests is given in Clark and Curran ( 2003 ) .</sentence>
				<definiendum id="0">c )</definiendum>
				<definiens id="0">the number of correct dependencies on conjunctive node c</definiens>
			</definition>
			<definition id="12">
				<sentence>The feature set for the dependency model consists of the following types of features : dependency features ( with and without distance measures ) , rule instantiation features ( with and without a lexical head ) , lexical category features , and root category features .</sentence>
				<definiendum id="0">feature set for the dependency model</definiendum>
				<definiens id="0">consists of the following types of features : dependency features ( with and without distance measures ) , rule instantiation features ( with and without a lexical head ) , lexical category features , and root category features</definiens>
			</definition>
			<definition id="13">
				<sentence>Let L be the number of correct dependencies in with respect to a gold standard dependency structure G ; then the dependency structure , max , which maximises the expected recall rate is : max = arg max E ( L =jGj ) ( 10 ) = arg max X i P ( ijS ) j \ ij where S is the sentence for gold standard dependency structure G and i ranges over the dependency structures for S .</sentence>
				<definiendum id="0">max</definiendum>
				<definiens id="0">maximises the expected recall rate is : max = arg max E ( L =jGj ) ( 10 ) = arg max X i P ( ijS ) j \ ij where S is the sentence for gold standard dependency structure G and i ranges over the dependency structures for S</definiens>
			</definition>
			<definition id="14">
				<sentence>This latter sum can be calculated efficiently using inside and outside scores : max =arg max X 2 1 ZS X c2C c c if 2deps ( c ) ( 12 ) where c is the inside score and c is the outside score for node c ( see Clark and Curran ( 2003 ) ) ; C is the set of conjunctive nodes in the packed chart for sentence S and deps ( c ) is the set of dependencies on conjunctive node c. The intuition behind the expected recall score is that a dependency structure scores highly if it has dependencies produced by high scoring derivations.4 The algorithm which finds max is a simple variant on the Viterbi algorithm , efficiently finding a derivation which produces the highest scoring set of dependencies .</sentence>
				<definiendum id="0">c</definiendum>
				<definiendum id="1">C</definiendum>
				<definiendum id="2">c )</definiendum>
				<definiens id="0">efficiently using inside and outside scores : max =arg max X 2 1 ZS X c2C c c if 2deps ( c ) ( 12 ) where c is the inside score and</definiens>
				<definiens id="1">the set of conjunctive nodes in the packed chart for sentence S and deps</definiens>
				<definiens id="2">the set of dependencies on conjunctive node c. The intuition behind the expected recall score is that a dependency structure scores highly if it has dependencies produced by high scoring derivations.4 The algorithm which finds max is a simple variant on the Viterbi algorithm , efficiently finding a derivation which produces the highest scoring set of dependencies</definiens>
			</definition>
</paper>

		<paper id="3010">
			<definition id="0">
				<sentence>Part-of-speech ( POS ) tagging is a job to assign a proper POS tag to each linguistic unit such as word for a given sentence .</sentence>
				<definiendum id="0">Part-of-speech</definiendum>
				<definiens id="0">a job to assign a proper POS tag to each linguistic unit such as word for a given sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>A solid line represents a transition between two morphemes across a word boundary and a dotted line represents a transition between two morphemes in a word .</sentence>
				<definiendum id="0">solid line</definiendum>
				<definiens id="0">represents a transition between two morphemes across a word boundary and a dotted line represents a transition between two morphemes in a word</definiens>
			</definition>
			<definition id="2">
				<sentence>The morpheme-unit POS tagging model is to find the most likely sequence of morphemes C5 and corresponding POS tags CC for a given sentence CF , as follows ( Kim et al. , 1998 ; Lee et al. , 2000 ) : A0B4CFB5 CSCTCU BP CPD6CVD1CPDC C5BNCC C8B4C5BNCCCYCFB5 BP CPD6CVD1CPDC D1 BDBND9 BND8 BDBND9 C8B4D1 BDBND9 BND8 BDBND9 CYDB BDBND2 B5 ( 1 ) AP CPD6CVD1CPDC D1 BDBND9 BND8 BDBND9 C8B4D1 BDBND9 BND8 BDBND9 B5 ( 2 ) In the equation , D9B4BQBP D2B5 denotes the number of morphemes in the sentence .</sentence>
				<definiendum id="0">D9B4BQBP D2B5</definiendum>
				<definiens id="0">a given sentence CF , as follows ( Kim et al. , 1998 ; Lee et al. , 2000 ) : A0B4CFB5 CSCTCU BP CPD6CVD1CPDC C5BNCC</definiens>
				<definiens id="1">the number of morphemes in the sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>A sequence of CF BP DB BDBND2 BP DB BD DB BE A1A1A1DB D2 is a sentence of D2 words , and a sequence of C5 BP D1 BDBND9 BP D1 BD D1 BE A1A1A1D1 D9 and a sequence of CC BP D8 BDBND9 BP D8 BD D8 BE A1A1A1D8 D9 denote a sequence of D9 lexical forms of morphemes and a sequence of D9 morpheme categories ( POS tags ) , respectively .</sentence>
				<definiendum id="0">sequence of CF BP DB BDBND2 BP DB BD DB BE A1A1A1DB D2</definiendum>
				<definiens id="0">a sentence of D2 words , and a sequence of C5 BP D1 BDBND9 BP D1 BD D1 BE A1A1A1D1 D9 and a sequence of CC BP D8 BDBND9 BP D8 BD D8 BE A1A1A1D8 D9 denote a sequence of D9 lexical forms of morphemes and a sequence of D9 morpheme categories ( POS tags ) , respectively</definiens>
			</definition>
			<definition id="4">
				<sentence>To simplify Equation 2 , a Markov assumption is usually used as follows : A0B4CFB5 AP CPD6CVD1CPDC D1 BDBND9 BND8 BDBND9 D9 CH CXBPBD C8B4D8 CX CYD8 CXA0BD BND4B5C8B4D8 CX CYD1 CX B5 ( 3 ) where , D8 BC is a pseudo tag which denotes the beginning of word and is also written as BUC7CF .</sentence>
				<definiendum id="0">D8 BC</definiendum>
				<definiens id="0">a Markov assumption is usually used as follows : A0B4CFB5 AP CPD6CVD1CPDC D1 BDBND9 BND8 BDBND9 D9 CH CXBPBD C8B4D8 CX CYD8 CXA0BD BND4B5C8B4D8 CX CYD1 CX B5 ( 3 ) where ,</definiens>
				<definiens id="1">a pseudo tag which denotes the beginning of word and is also written as BUC7CF</definiens>
			</definition>
			<definition id="5">
				<sentence>D4 denotes a type of transition from the previous tag to the current tag .</sentence>
				<definiendum id="0">D4</definiendum>
				<definiens id="0">a type of transition from the previous tag to the current tag</definiens>
			</definition>
			<definition id="6">
				<sentence>The probability C8B4CACYCFB5 is given as follows : C8B4CACYCFB5 BP C8B4D6 BDBND2 CYDB BDBND2 B5 ( 9 ) BP D2 CH CXBPBD C8B4D6 CX CYDB BDBND2 BND6 BDBNCXA0BD B5 ( 10 ) AP D2 CH CXBPBD C8B4D6 CX CYDB CX BND6 CXA0BD B5 ( 11 ) where , D6 CX denotes the tagging result of CXth word ( DB CX ) , and D6 BC denotes a pseudo variable to indicate the beginning of word .</sentence>
				<definiendum id="0">D6 CX</definiendum>
				<definiendum id="1">DB CX</definiendum>
				<definiendum id="2">D6 BC</definiendum>
				<definiens id="0">given as follows : C8B4CACYCFB5 BP C8B4D6 BDBND2 CYDB BDBND2 B5 ( 9 ) BP D2 CH CXBPBD C8B4D6 CX CYDB BDBND2 BND6 BDBNCXA0BD B5 ( 10 ) AP D2 CH CXBPBD C8B4D6 CX CYDB CX BND6 CXA0BD B5 ( 11 ) where ,</definiens>
				<definiens id="1">a pseudo variable to indicate the beginning of word</definiens>
			</definition>
			<definition id="7">
				<sentence>The transition model is a form of point-wise mutual information .</sentence>
				<definiendum id="0">transition model</definiendum>
				<definiens id="0">a form of point-wise mutual information</definiens>
			</definition>
			<definition id="8">
				<sentence>C8B4D6 CXA0BD BND6 CX B5 C8B4D6 CXA0BD B5C8B4D6 CX B5 BP C8B4C5 CXA0BD BNCC CXA0BD BNC5 CX BNCC CX B5 C8B4C5 CXA0BD BNCC CXA0BD B5C8B4C5 CX BNCC CX B5 ( 16 ) BP C8B4D1 CXA0BD BDBNCY BND8 CXA0BD BDBNCY BND1 CX BDBNCZ BND8 CX BDBNCZ B5 C8B4D1 CXA0BD BDBNCY BND8 CXA0BD BDBNCY B5C8B4D1 CX BDBNCZ BND8 CX BDBNCZ B5 ( 17 ) where , a superscript CX in D1 CX BDBNCZ and D8 CX BDBNCZ denotes the position of the word in a sentence .</sentence>
				<definiendum id="0">C8B4D6 CXA0BD BND6 CX B5 C8B4D6 CXA0BD B5C8B4D6 CX B5 BP C8B4C5 CXA0BD BNCC CXA0BD BNC5 CX BNCC CX B5 C8B4C5 CXA0BD BNCC CXA0BD B5C8B4C5 CX BNCC CX B5</definiendum>
				<definiendum id="1">D8 CX BDBNCZ</definiendum>
				<definiens id="0">the position of the word in a sentence</definiens>
			</definition>
			<definition id="9">
				<sentence>C8B4D1 BDBNCZ BND8 BDBNCZ B5 BP CZ CH D0BPBD C8B4D8 D0 CYD8 D0A0BD B5C8B4D1 D0 CYD8 D0 B5 ( 18 ) C8B4D1 CXA0BD BDBNCY BND8 CXA0BD BDBNCY BND1 CX BDBNCZ BND8 CX BDBNCZ B5 BP CY CH D0BPBD AI C8B4D8 CXA0BD D0 CYD8 CXA0BD D0A0BD B5 C8B4D1 CXA0BD D0 CYD8 CXA0BD D0 B5 AJ A2 C8 CXD2D8CTD6 B4D8 CX BD CYD8 CXA0BD CY B5 A2C8B4D1 CX BD CYD8 CX BD B5 A2 CZ CH D1BPBE AI C8B4D8 CX D1 CYD8 CX D1A0BD B5 C8B4D1 CX D1 CYD8 CX D1 B5 AJ ( 19 ) where , C8 CXD2D8CTD6 B4D8 CX BD CYD8 CXA0BD CY B5 means a transition probability between the last morpheme of the B4CXA0BDB5th word and the first morpheme of the CXth word .</sentence>
				<definiendum id="0">C8 CXD2D8CTD6 B4D8 CX BD CYD8 CXA0BD CY B5</definiendum>
				<definiens id="0">CYD8 CXA0BD D0 B5 AJ A2 C8 CXD2D8CTD6 B4D8 CX BD CYD8 CXA0BD CY B5 A2C8B4D1 CX BD CYD8 CX BD B5 A2 CZ CH D1BPBE AI C8B4D8 CX D1 CYD8 CX D1A0BD B5 C8B4D1 CX D1 CYD8 CX D1 B5 AJ ( 19 ) where ,</definiens>
			</definition>
</paper>

		<paper id="2010">
			<definition id="0">
				<sentence>In addition to labels , BoosTexter assigns confidence weights that reflect the reliability of the decisions .</sentence>
				<definiendum id="0">BoosTexter</definiendum>
				<definiens id="0">assigns confidence weights that reflect the reliability of the decisions</definiens>
			</definition>
			<definition id="1">
				<sentence>Figure 1 : System Architecture The HTC is a collection of 250 short texts ( 30-700 tokens ) describing architecture , historical events and people associated with the city of Heidelberg .</sentence>
				<definiendum id="0">HTC</definiendum>
				<definiens id="0">a collection of 250 short texts ( 30-700 tokens ) describing architecture , historical events and people associated with the city of Heidelberg</definiens>
			</definition>
			<definition id="2">
				<sentence>The chunker is based on a head-lexicalised probabilistic context free grammar ( H-L PCFG ) and achieves an F-measure of 92 for range only and 83 for range and label , whereby a range of a noun chunk is defined as ”all words from the beginning of the noun phrase to the head noun” .</sentence>
				<definiendum id="0">noun chunk</definiendum>
				<definiens id="0">a head-lexicalised probabilistic context free grammar ( H-L PCFG ) and achieves an F-measure of 92 for range only and 83 for range and label , whereby a range of a</definiens>
			</definition>
</paper>

		<paper id="1066">
			<definition id="0">
				<sentence>Model 1 is defined as a particularly simple instance of this framework , by assuming all possible lengths for T ( less than some arbitrary upper bound ) have a uniform probability epsilon1 , all possible choices of source sentence generating words are equally likely , and the translation probability tr ( t j |s i ) of the generated target language word depends only on the generating source language word—which Brown et al. ( 1993a ) show yields the following equation : p ( T|S ) = epsilon1 ( l +1 ) m m productdisplay j=1 l summationdisplay i=0 tr ( t j |s i ) ( 1 ) Equation 1 gives the Model 1 estimate for the probability of a target sentence , given a source sentence .</sentence>
				<definiendum id="0">Model 1</definiendum>
				<definiens id="0">less than some arbitrary upper bound ) have a uniform probability epsilon1 , all possible choices of source sentence generating words are equally likely , and the translation probability tr ( t j |s i ) of the generated target language word depends only on the generating source language word—which Brown et al. ( 1993a ) show yields the following equation : p ( T|S ) = epsilon1 ( l +1 ) m m productdisplay j=1 l summationdisplay i=0 tr ( t j |s i ) ( 1 ) Equation 1 gives the Model 1 estimate for the probability of a target sentence , given a source sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>To estimate the smoothed probabilties we use the following formula : tr ( t|s ) = C ( t , s ) +n C ( s ) +n ·|V | ( 3 ) where C ( t , s ) is the expected count of s generating t , C ( s ) is the corresponding marginal count for s , |V | is the hypothesized size of the target vocabulary V , and n is the added count for each target word in V .</sentence>
				<definiendum id="0">|V |</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">t|s ) = C ( t , s ) +n C ( s ) +n ·|V | ( 3 ) where C ( t , s ) is the expected count of s generating t</definiens>
				<definiens id="1">the hypothesized size of the target vocabulary V , and</definiens>
				<definiens id="2">the added count for each target word in V</definiens>
			</definition>
			<definition id="2">
				<sentence>Add-n smoothing is a way of smoothing with a uniform distribution , so it is not surprising that it performs poorly in language modeling when it is compared to smoothing with higher order models ; e.g , smoothing trigrams with bigrams or smoothing bigrams with unigrams .</sentence>
				<definiendum id="0">Add-n smoothing</definiendum>
				<definiens id="0">a way of smoothing with a uniform distribution</definiens>
			</definition>
			<definition id="3">
				<sentence>1 These LLR scores can range in value from 0 to N ·log ( 2 ) , where N is the number of sentence pairs in the training data .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of sentence pairs in the training data</definiens>
			</definition>
			<definition id="4">
				<sentence>All non-EM-trained parameters were re-optimized on the trial data for each version of Model 1 tested , with the exception Model Trial Test Test Test LLR Init EM Add EM ( Ablation ) AER AER Recall Precision Exp NW NW n Iter Standard 0.311 0.298 0.810 0.646 NA NA 1.0 0.0000 17 Smoothed 0.261 0.271 0.646 0.798 NA NA 10.0 0.0100 15 ( EM NW ) 0.285 0.273 0.833 0.671 NA NA 1.0 0.0100 20 ( Add n ) 0.302 0.300 0.638 0.751 NA NA 13.0 0.0000 14 Heuristic 0.234 0.255 0.655 0.844 1.3 2.4 NA NA NA ( LLR Exp ) 0.257 0.259 0.655 0.844 1.0 2.4 NA NA NA ( Init NW ) 0.300 0.308 0.740 0.657 1.5 1.0 NA NA NA Combined 0.203 0.215 0.724 0.839 1.3 2.4 7.0 0.005 1 ( LLR Exp ) 0.258 0.272 0.636 0.809 1.0 2.4 10.0 0.0035 3 ( Init NW ) 0.197 0.209 0.722 0.854 1.5 1.0 10.0 0.0005 1 ( EM NW ) 0.281 0.267 0.833 0.680 1.3 2.4 1.0 0.0080 8 ( Add n ) 0.208 0.221 0.724 0.826 1.3 2.4 8.0 0.0000 1 Table 1 : Evaluation Results .</sentence>
				<definiendum id="0">NA NA NA</definiendum>
				<definiendum id="1">NA NA NA</definiendum>
				<definiens id="0">with the exception Model Trial Test Test Test LLR Init EM Add EM ( Ablation ) AER AER Recall Precision Exp NW NW n Iter Standard</definiens>
			</definition>
			<definition id="5">
				<sentence>These three performance statistics are defined as recall = |A ∩ S| |S| ( 5 ) precision = |A ∩ P| |A| ( 6 ) AER = 1 − |A ∩ S| + |A ∩ P| |A| + |S| ( 7 ) where S denotes the annotated set of sure alignments , P denotes the annotated set of possible alignments , and A denotes the set of alignments produced by the model under test .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">P</definiendum>
				<definiendum id="2">A denotes</definiendum>
				<definiens id="0">recall = |A ∩ S| |S| ( 5 ) precision = |A ∩ P| |A| ( 6 ) AER = 1 − |A ∩ S| + |A ∩ P| |A| + |S| ( 7 ) where</definiens>
				<definiens id="1">the annotated set of sure alignments</definiens>
				<definiens id="2">the annotated set of possible alignments</definiens>
				<definiens id="3">the set of alignments produced by the model under test</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>Moreover , the NPN classi cation yields the lowest accuracies and the lowest improvements over its baseline .</sentence>
				<definiendum id="0">NPN classi cation</definiendum>
				<definiens id="0">yields the lowest accuracies and the lowest improvements over its baseline</definiens>
			</definition>
</paper>

		<paper id="1079">
			<definition id="0">
				<sentence>The Adequacy parameter captures how much of the original content of a text is conveyed , regardless of how grammatically imperfect the output might be .</sentence>
				<definiendum id="0">Adequacy parameter</definiendum>
				<definiens id="0">captures how much of the original content of a text is conveyed</definiens>
			</definition>
			<definition id="1">
				<sentence>idf ( i , j ) = ( 1 + log ( tf i , j ) ) log ( N / df i ) , if tf i , j ≥ 1 ; where : – tf i , j is the number of occurrences of the word w i in the document d j ; – df i is the number of documents in the corpus where the word w i occurs ; – N is the total number of documents in the corpus .</sentence>
				<definiendum id="0">idf</definiendum>
				<definiendum id="1">df i</definiendum>
				<definiendum id="2">– N</definiendum>
				<definiens id="0">the number of occurrences of the word w i in the document d j ; –</definiens>
				<definiens id="1">the number of documents in the corpus where the word w i occurs ;</definiens>
				<definiens id="2">the total number of documents in the corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>On the level of corpus statistics the weighted Recall scores go in line with Adequacy , and weighted Precision scores ( as well as the Precision-based BLEU scores ) – with Fluency , which confirms such interpretation of weighted Precision and Recall scores in the example above .</sentence>
				<definiendum id="0">weighted Precision scores</definiendum>
				<definiens id="0">confirms such interpretation of weighted Precision and Recall scores in the example above</definiens>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>The total score ( combined acoustic and language model scores ) of candidate edges are compared against edge with the same span and category .</sentence>
				<definiendum id="0">total score</definiendum>
				<definiens id="0">combined acoustic and language model scores ) of candidate edges are compared against edge with the same span and category</definiens>
			</definition>
			<definition id="1">
				<sentence>Given the task of simply generating a transcription of speech , WER is a useful and direct way to measure language model quality for ASR .</sentence>
				<definiendum id="0">WER</definiendum>
				<definiens id="0">a useful and direct way to measure language model quality for ASR</definiens>
			</definition>
			<definition id="2">
				<sentence>Sentence error rate is the percentage of sentences for which the proposed utterance has at least one error .</sentence>
				<definiendum id="0">Sentence error rate</definiendum>
				<definiens id="0">the percentage of sentences for which the proposed utterance has at least one error</definiens>
			</definition>
			<definition id="3">
				<sentence>Perplexity is related to the entropy of the source model which the language model attempts to estimate .</sentence>
				<definiendum id="0">Perplexity</definiendum>
				<definiens id="0">the language model attempts to estimate</definiens>
			</definition>
			<definition id="4">
				<sentence>The WER scores for this , the first application of the Collins ( 1999 ) model to parsing word lattices , are comparable to other recent work in syntactic language modelling , and better than a simple trigram model trained on the same data .</sentence>
				<definiendum id="0">WER scores for this</definiendum>
				<definiens id="0">comparable to other recent work in syntactic language modelling , and better than a simple trigram model trained on the same data</definiens>
			</definition>
			<definition id="5">
				<sentence>CB is the average number of crossing brackets per sentence .</sentence>
				<definiendum id="0">CB</definiendum>
				<definiens id="0">the average number of crossing brackets per sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>a0 section “1987” of the BLLIP corpus ( Charniak et al. , 1999 ) [ 20 million words ] The BLLIP corpus is a collection of Penn Treebank-style parses of the three-year ( 1987-1989 ) Wall Street Journal collection from the ACL/DCI corpus ( approximately 30 million words ) .6 The parses were automatically produced by the parser of Charniak ( 2001 ) .</sentence>
				<definiendum id="0">BLLIP corpus</definiendum>
				<definiens id="0">a collection of Penn Treebank-style parses of the three-year ( 1987-1989 ) Wall Street Journal collection from the ACL/DCI corpus ( approximately 30 million words</definiens>
			</definition>
			<definition id="7">
				<sentence>The total edge weight in the parser is a scaled combination of these scores with the parser score derived with the model parameters : loga2 wa5 a1 αloga2 aa5 a4 βloga2 lma5 a4 s ( 4 ) where w is the edge weight , and s is the score assigned by the parameters of the parsing model .</sentence>
				<definiendum id="0">w</definiendum>
				<definiendum id="1">s</definiendum>
				<definiens id="0">a scaled combination of these scores with the parser score derived with the model parameters : loga2 wa5 a1 αloga2 aa5 a4 βloga2 lma5 a4 s ( 4 ) where</definiens>
				<definiens id="1">the score assigned by the parameters of the parsing model</definiens>
			</definition>
</paper>

		<paper id="1085">
			<definition id="0">
				<sentence>The ICSI Meeting corpus ( Janin et al. , 2003 ) is a collection of 75 meetings collected at the International Computer Science Institute ( ICSI ) , one among the growing number of corpora of humanto-human multi-party conversations .</sentence>
				<definiendum id="0">ICSI Meeting corpus</definiendum>
				<definiens id="0">a collection of 75 meetings collected at the International Computer Science Institute ( ICSI ) , one among the growing number of corpora of humanto-human multi-party conversations</definiens>
			</definition>
			<definition id="1">
				<sentence>identification as follows : given the second element ( B ) of an adjacency pair , determine who is the speaker of the first element ( A ) .</sentence>
				<definiendum id="0">identification</definiendum>
				<definiens id="0">given the second element ( B ) of an adjacency pair</definiens>
			</definition>
			<definition id="2">
				<sentence>We view the problem as an instance of statistical ranking , a general machine learning paradigm used for example in statistical parsing ( Collins , 2000 ) and question answering ( Ravichandran et al. , 2003 ) .3 The problem is to select , given a set of a0 possible candidates a1a3a2a5a4a7a6a7a8a9a8a9a8a9a6a10a2a12a11a14a13 ( in our case , potential A speakers ) , the one candidate a2a16a15 that maximizes a given conditional probability distribution .</sentence>
				<definiendum id="0">question answering</definiendum>
			</definition>
			<definition id="3">
				<sentence>To rank all speakers ( aside from the B speaker ) and to determine how likely each one is to be the A speaker of the adjacency pair involving speaker B , we use four categories of features : structural , durational , lexical , and dialog act ( DA ) information .</sentence>
				<definiendum id="0">rank all speakers</definiendum>
				<definiens id="0">structural , durational , lexical , and dialog act ( DA ) information</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>Two main results are presented : a ) the creation of an Information Synthesis testbed with 72 reports manually generated by nine subjects for eight complex topics with 100 relevant documents each ; and b ) an empirical comparison of similarity metrics between reports , under the hypothesis that the best metric is the one that best distinguishes between manual and automatically generated reports .</sentence>
				<definiendum id="0">b</definiendum>
				<definiens id="0">a ) the creation of an Information Synthesis testbed with 72 reports manually generated by nine subjects for eight complex topics with 100 relevant documents each</definiens>
			</definition>
			<definition id="1">
				<sentence>A good similarity metric provides a way of evaluating Information Synthesis systems ( comparing their output with manually generated reports ) , and should also shed some light on the common properties of manually generated reports .</sentence>
				<definiendum id="0">good similarity metric</definiendum>
				<definiens id="0">provides a way of evaluating Information Synthesis systems ( comparing their output with manually generated reports ) , and should also shed some light on the common properties of manually generated reports</definiens>
			</definition>
			<definition id="2">
				<sentence>We have compared several similarity metrics , including a few baseline measures ( based on document , sentence and vocabulary overlap ) and a stateof-the-art measure to evaluate summarization systems , ROUGE ( Lin and Hovy , 2003 ) .</sentence>
				<definiendum id="0">ROUGE</definiendum>
				<definiens id="0">based on document , sentence and vocabulary overlap ) and a stateof-the-art measure to evaluate summarization systems</definiens>
			</definition>
			<definition id="3">
				<sentence>We propose to use the probability that , given any manual report Mref , any other manual report M is closer to Mref than any other automatic report A : QARLA ( sim ) = P ( sim ( M ; Mref ) &gt; sim ( A ; Mref ) ) where M ; Mref 2M ; A2A where M is the set of manually generated reports , A is the set of automatically generated reports , and “sim” is the similarity metric being evaluated .</sentence>
				<definiendum id="0">M</definiendum>
				<definiendum id="1">“sim”</definiendum>
				<definiens id="0">A ; Mref ) ) where M</definiens>
				<definiens id="1">the similarity metric being evaluated</definiens>
			</definition>
			<definition id="4">
				<sentence>DocSim ( Mr ; M ) = jDoc ( Mr ) \Doc ( M ) jjDoc ( M r ) j where Mr is the reference report , M a second report and Doc ( Mr ) ; Doc ( M ) are the documents to which the sentences in Mr ; M belong to .</sentence>
				<definiendum id="0">DocSim</definiendum>
				<definiendum id="1">Mr</definiendum>
				<definiens id="0">the documents to which the sentences in Mr</definiens>
			</definition>
			<definition id="5">
				<sentence>The units can be sentences or discourse units : ROUGEn = P C2fMUg P n-gram2CCountmP C2fMUg P n-gram2CCount where MU is the set of model units , Countm is the maximum number of n-grams co-ocurring in a peer summary and a model unit , and Count is the number of n-grams in the model unit .</sentence>
				<definiendum id="0">MU</definiendum>
				<definiendum id="1">Countm</definiendum>
				<definiendum id="2">Count</definiendum>
				<definiens id="0">the set of model units</definiens>
				<definiens id="1">the number of n-grams in the model unit</definiens>
			</definition>
			<definition id="6">
				<sentence>The key concept similarity NICOS ( Nuclear Informative Concept Similarity ) between two reports M and Mr can then be defined as the inverse of the Euclidean distance between their associated concept vectors : NICOS ( M ; Mr ) = 1j~kc ( M r ) ~kc ( M ) j In our experiment , the dimensions of kc vectors correspond to the list of key concepts provided by our test subjects ( see Section 2.3 ) .</sentence>
				<definiendum id="0">key concept similarity NICOS ( Nuclear Informative Concept Similarity</definiendum>
			</definition>
			<definition id="7">
				<sentence>Our empirical comparison uses a quantitative criterion ( the QARLA estimation ) based on the hypothesis that a good similarity metric will be able to distinguish between manual and automatic reports .</sentence>
				<definiendum id="0">quantitative criterion</definiendum>
				<definiens id="0">the QARLA estimation ) based on the hypothesis that a good similarity metric will be able to distinguish between manual and automatic reports</definiens>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>SALAAM is a WSD system that exploits parallel corpora for sense disambiguation of words in running text .</sentence>
				<definiendum id="0">SALAAM</definiendum>
			</definition>
			<definition id="1">
				<sentence>To date , SALAAM yields the best scores for an unsupervised system on the SENSEVAL2 English All-Words task ( Diab , 2003 ) .</sentence>
				<definiendum id="0">SALAAM</definiendum>
			</definition>
			<definition id="2">
				<sentence>In the study , Mihalcea compares results yielded by the supervised learning system trained on the automatically generated data , GenCor , against the same system trained on manually annotated data .</sentence>
				<definiendum id="0">Mihalcea</definiendum>
				<definiendum id="1">GenCor</definiendum>
				<definiens id="0">compares results yielded by the supervised learning system trained on the automatically generated data</definiens>
			</definition>
			<definition id="3">
				<sentence>The weight calculation is a variant on the Inverse Document Frequency ( IDF ) measure in Information Retrieval .</sentence>
				<definiendum id="0">weight calculation</definiendum>
				<definiens id="0">a variant on the Inverse Document Frequency ( IDF ) measure in Information Retrieval</definiens>
			</definition>
			<definition id="4">
				<sentence>a37 L1 words that translate into the same L2 word are grouped into clusters ; a37 SALAAM identifies the appropriate senses for the words in those clusters based on the words senses’ proximity in WordNet .</sentence>
				<definiendum id="0">a37 SALAAM</definiendum>
				<definiens id="0">identifies the appropriate senses for the words in those clusters based on the words senses’ proximity in WordNet</definiens>
			</definition>
			<definition id="5">
				<sentence>We report the results using two metrics , the harmonic mean of precision and recall , ( a1a3a2a5a4a7a6 ) score , and the Performance Ratio ( PR ) , which we define as the ratio between two precision scores on the same test data where precision is rendered using scorer2 .</sentence>
				<definiendum id="0">Performance Ratio ( PR )</definiendum>
				<definiens id="0">the ratio between two precision scores on the same test data where precision</definiens>
			</definition>
			<definition id="6">
				<sentence>PR is measured as follows : a8a10a9 a34 a11a13a12a15a14a13a14a5a16 a14 a8 a39a18a17a20a19 a13a22a21a31a13a24a23a26a25 a11a13a12a15a14a13a14a5a16 a27 a8 a39a18a17a20a19 a13a22a21a31a13a24a23a26a25 ( 1 ) 4Originally , there are 48 conditions , 9 of which are excluded due to extreme sparseness in training contexts .</sentence>
				<definiendum id="0">PR</definiendum>
			</definition>
			<definition id="7">
				<sentence>Mihalcea reports high PR scores for six test items only : art , chair , channel , church , detention , nation .</sentence>
				<definiendum id="0">Mihalcea</definiendum>
				<definiens id="0">reports high PR scores for six test items only : art , chair , channel , church , detention , nation</definiens>
			</definition>
			<definition id="8">
				<sentence>Entropy is measured as follows : a0 a25a47a46 a28 a34a5a4a49a48 a12a7a50a52a51 a10a7a25a47a53 a28 a12 a23 a15a3a54 a25a55a10a7a25a47a53 a28 a28 ( 2 ) where a53 is a sense for a polysemous noun and a46 is the set of all its senses .</sentence>
				<definiendum id="0">Entropy</definiendum>
				<definiendum id="1">a46</definiendum>
				<definiens id="0">measured as follows : a0 a25a47a46 a28 a34a5a4a49a48 a12a7a50a52a51 a10a7a25a47a53 a28 a12 a23 a15a3a54 a25a55a10a7a25a47a53 a28 a28 ( 2 ) where a53 is a sense for a polysemous noun</definiens>
				<definiens id="1">the set of all its senses</definiens>
			</definition>
			<definition id="9">
				<sentence>Semantic translation entropy ( STE ) ( Melamed , 1997 ) is a special characteristic of the SALAAMtagged training data , since the source of evidence for SALAAM tagging is multilingual translations .</sentence>
				<definiendum id="0">Semantic translation entropy</definiendum>
				<definiens id="0">a special characteristic of the SALAAMtagged training data , since the source of evidence for SALAAM tagging is multilingual translations</definiens>
			</definition>
			<definition id="10">
				<sentence>STE is a variant on the entropy measure .</sentence>
				<definiendum id="0">STE</definiendum>
				<definiens id="0">a variant on the entropy measure</definiens>
			</definition>
			<definition id="11">
				<sentence>STE is expressed as follows : a0 a25 a6a1a0 a21 a28 a34a34a4 a48 a6 a50 a21 a10a7a25a40a19 a0 a21 a28 a42a12 a23 a15 a54 a25a55a10a7a25a40a19 a0 a21 a28 a28 ( 3 ) where a19 is a translation in the set of possible translations a6 in L2 ; and a21 is L1 word .</sentence>
				<definiendum id="0">STE</definiendum>
				<definiendum id="1">a19</definiendum>
				<definiendum id="2">a21</definiendum>
			</definition>
			<definition id="12">
				<sentence>Perplexity difference ( PerpDiff ) is a measure of the absolute difference in sense perplexity between the test data items and the training data items .</sentence>
				<definiendum id="0">Perplexity difference</definiendum>
				<definiendum id="1">PerpDiff</definiendum>
			</definition>
			<definition id="13">
				<sentence>We observe cases with a low PerpDiff such as holiday ( PerpDiff of a41a43a42a41a1a0 ) , yet the PR is a low a41a43a42a41a1a56 .</sentence>
				<definiendum id="0">PR</definiendum>
				<definiens id="0">a low a41a43a42a41a1a56</definiens>
			</definition>
			<definition id="14">
				<sentence>Sense Distributional Correlation ( SDC ) results from comparing the sense distributions of the test data items with those of SALAAM-tagged training data items .</sentence>
				<definiendum id="0">Sense Distributional Correlation</definiendum>
			</definition>
			<definition id="15">
				<sentence>Overall , the data suggests that SDC is a very good direct indicator of PR .</sentence>
				<definiendum id="0">SDC</definiendum>
				<definiens id="0">a very good direct indicator of PR</definiens>
			</definition>
</paper>

		<paper id="1080">
			<definition id="0">
				<sentence>In section 2 we will introduce our word sense learning algorithm , which incorporates unsupervised feature selection and model order identification technique .</sentence>
				<definiendum id="0">algorithm</definiendum>
				<definiens id="0">incorporates unsupervised feature selection and model order identification technique</definiens>
			</definition>
			<definition id="1">
				<sentence>W is used to denote bag of words occurring in context set D , then W = fwigMi=1 , where wi denotes a word occurring in D , and M is the total number of different contextual words .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">used to denote bag of words occurring in context set D , then W = fwigMi=1 , where wi denotes a word occurring in D</definiens>
				<definiens id="1">the total number of different contextual words</definiens>
			</definition>
			<definition id="2">
				<sentence>HT is a weight matrix of contextual word subset T , T W. Then each entry hi ; j represents the weight of word wj in di , wj 2 T , 1 • i • N. We use binary term weighting method to derive context vectors : hi ; j = 1 if word wj occurs in di , otherwise zero .</sentence>
				<definiendum id="0">HT</definiendum>
				<definiendum id="1">j</definiendum>
				<definiens id="0">a weight matrix of contextual word subset T</definiens>
				<definiens id="1">word wj occurs in di</definiens>
			</definition>
			<definition id="3">
				<sentence>Let CT = fcTi gNi=1 be a set of context vectors in feature space T , where cTi is the context vector of the i-th occurrence .</sentence>
				<definiendum id="0">cTi</definiendum>
				<definiens id="0">the context vector of the i-th occurrence</definiens>
			</definition>
			<definition id="4">
				<sentence>cTi is defined as : cTi = X j ( hi ; jv ( wj ) ) ; wj 2 T ; 1 • i • N : ( 1 ) The feature subset selection in word set W can be formulated as : ˆT = argmax T fcriterion ( T ; H ; V ; q ) g ; T W ; ( 2 ) subject to coverage ( D ; T ) ‚ ¿ , where ˆT is the optimal feature subset , criterion is the cluster validation based evaluation function ( the function in Table 1 ) , q is the resampling frequency for estimate of stability , and coverage ( D ; T ) is the proportion of contexts with occurrences of features in T. This constrained optimization results in a solution which maximizes the criterion and meets the given constraint at the same time .</sentence>
				<definiendum id="0">cTi</definiendum>
				<definiendum id="1">ˆT</definiendum>
				<definiendum id="2">T )</definiendum>
				<definiens id="0">The feature subset selection in word set W can be formulated as : ˆT = argmax T fcriterion ( T ; H</definiens>
				<definiens id="1">the optimal feature subset , criterion is the cluster validation based evaluation function</definiens>
				<definiens id="2">the resampling frequency for estimate of stability , and coverage ( D ;</definiens>
				<definiens id="3">the proportion of contexts with occurrences of features in T. This constrained optimization results in a solution which maximizes the criterion and meets the given constraint at the same time</definiens>
			</definition>
			<definition id="5">
				<sentence>The average of stability over q resampling is the estimation of the score of T. Function criterion ( T , H , V , q ) Input parameter : feature subset T , weight matrix H , second order co-occurrence matrix V , resampling frequency q ; ( 1 ) ST = 0 ; ( 2 ) For i = 1 to q do ( 2:1 ) Randomly split CT into disjoint halves , denoted as CTA and CTB ; ( 2:2 ) Estimate GMM parameter and cluster number on CTA using Cluster , and the parameter set is denoted as ˆ A ; The solution ˆ A can be used to construct a predictor ‰A ; ( 2:3 ) Estimate GMM parameter and cluster number on CTB using Cluster , and the parameter set is denoted as ˆ B , The solution ˆ B can be used to construct a predictor ‰B ; ( 2:4 ) Classify CTB using ‰A and ‰B ; The class labels assigned by ‰A and ‰B are denoted as LA and LB ; ( 2:5 ) ST+ = max… 1jCT Bj P i 1f… ( LA ( c T Bi ) ) = LB ( c T Bi ) g , where … denotes possible permutation relating indices between LA and LB , and cTBi 2 CTB ; ( 3 ) ST = 1qST ; ( 4 ) Return ST ; 1998 ) , to estimate cluster structure and cluster number .</sentence>
				<definiendum id="0">q resampling</definiendum>
				<definiendum id="1">LA</definiendum>
				<definiens id="0">the estimation of the score of T. Function criterion ( T , H , V , q ) Input parameter : feature subset T , weight matrix H , second order co-occurrence matrix V , resampling frequency q</definiens>
				<definiens id="1">GMM parameter and cluster number on CTA using Cluster</definiens>
				<definiens id="2">A can be used to construct a predictor ‰A ; ( 2:3 ) Estimate GMM parameter and cluster number on CTB using Cluster</definiens>
				<definiens id="3">( c T Bi ) ) = LB ( c T Bi ) g , where … denotes possible permutation relating indices between LA and LB</definiens>
			</definition>
			<definition id="6">
				<sentence>The initialization of mixture parameter ( 1 ) is given by : … ( 1 ) k = 1K o ( 6 ) „ ( 1 ) k = yn ; wheren = b ( k¡1 ) ( N ¡1 ) = ( Ko¡1 ) c+1 ( 7 ) R ( 1 ) k = 1N ΣNn=1ynytn ( 8 ) Ko is a given initial subclass number .</sentence>
				<definiendum id="0">Ko</definiendum>
				<definiens id="0">a given initial subclass number</definiens>
			</definition>
			<definition id="7">
				<sentence>Our assessment measure is defined as : M ( T ) = 1jTj X w2T X l2L p ( w ; l ) log p ( w ; l ) p ( w ) p ( l ) ; ( 17 ) where T is the feature subset to be evaluated , T W , L is class label set , p ( w ; l ) is the joint distribution of two variables w and l , p ( w ) and p ( l ) are marginal probabilities .</sentence>
				<definiendum id="0">T</definiendum>
				<definiendum id="1">l )</definiendum>
				<definiens id="0">the feature subset to be evaluated</definiens>
			</definition>
			<definition id="8">
				<sentence>In this paper , we applied their method to assign different sense tags to only min ( jUj ; jCj ) clusters by maximizing the accuracy , where jUj is the number of clusters , and jCj is the number of ground truth classes .</sentence>
				<definiendum id="0">jUj</definiendum>
				<definiendum id="1">jCj</definiendum>
				<definiens id="0">the number of clusters</definiens>
				<definiens id="1">the number of ground truth classes</definiens>
			</definition>
</paper>

		<paper id="1003">
</paper>

		<paper id="1084">
			<definition id="0">
				<sentence>Generalized Multitext Grammar ( GMTG ) is a synchronous grammar formalism that is weakly equivalent to Linear Context-Free Rewriting Systems ( LCFRS ) , but retains much of the notational and intuitive simplicity of Context-Free Grammar ( CFG ) .</sentence>
				<definiendum id="0">Generalized Multitext Grammar ( GMTG )</definiendum>
			</definition>
			<definition id="1">
				<sentence>GMTG allows both synchronous and independent rewriting .</sentence>
				<definiendum id="0">GMTG</definiendum>
			</definition>
			<definition id="2">
				<sentence>Generalized Multitext Grammar ( GMTG ) offers a way to synchronize Mildly Context-Sensitive Grammar ( MCSG ) , while satisfying both of the above criteria .</sentence>
				<definiendum id="0">Generalized Multitext Grammar ( GMTG )</definiendum>
				<definiens id="0">offers a way to synchronize Mildly Context-Sensitive Grammar ( MCSG )</definiens>
			</definition>
			<definition id="3">
				<sentence>GMTG is a generalization of MTG , which is itself a generalization of CFG to the synchronous case .</sentence>
				<definiendum id="0">GMTG</definiendum>
				<definiens id="0">a generalization of MTG , which is itself a generalization of CFG to the synchronous case</definiens>
			</definition>
			<definition id="4">
				<sentence>Formalisms that do not allow independent rewriting require a corresponding a17 to appear in the second component on the right-hand side ( RHS ) of Production ( 5 ) , and this a17 would eventually generate the empty string .</sentence>
				<definiendum id="0">RHS</definiendum>
				<definiens id="0">the empty string</definiens>
			</definition>
			<definition id="5">
				<sentence>each a66a102a98a116a103 consists of one indexed nonterminal and all of these nonterminals are coindexed .</sentence>
				<definiendum id="0">a66a102a98a116a103</definiendum>
				<definiens id="0">consists of one indexed nonterminal and all of these nonterminals are coindexed</definiens>
			</definition>
			<definition id="6">
				<sentence>A generalized multitext grammar with a17 dimensions ( a17 -GMTG for short ) is a tuple a117a118a42 a5 a37a39a38 a1a75a37 a58 a1a6a119 a1 a18 a9 where a37a39a38 , a37 a58 are finite , disjoint sets of nonterminal and terminal symbols , respectively , a18a120a52 a37 a38 is the start symbol and a119 is a finite set of productions .</sentence>
				<definiendum id="0">a119</definiendum>
				<definiens id="0">finite , disjoint sets of nonterminal and terminal symbols , respectively , a18a120a52 a37 a38 is the start symbol</definiens>
				<definiens id="1">a finite set of productions</definiens>
			</definition>
			<definition id="7">
				<sentence>Each production has the form a121 a3 a122 , where a121 is a a17 -dimensional link and a122 is a a17 dimensional ITV such that a113 a5 a121 a0 a106a27a2a24a9a123a42a124a113 a5 a122 a0 a106a27a2a24a9 for a95a83a105a125a106a112a105 a17 .</sentence>
				<definiendum id="0">a121</definiendum>
				<definiendum id="1">a122</definiendum>
				<definiens id="0">a a17 -dimensional link</definiens>
			</definition>
			<definition id="8">
				<sentence>Definition 5 A Linear Context-Free Rewriting System ( LCFRS ) is a quadruple a117 a42 a5 a37 a38 a1a75a37a59a58 a1a6a119 a1 a18 a9 where a37 a38 , a37a59a58 and a18 are as in GMTGs , every a45 a52 a37 a38 is associated with an integer a113 a5a26a45 a9a124a93 a95 with a113 a5a19a18 a9a53a42 a95 , and a119 is a finite set of productions of the form a45 a3 a36 a5a53a52 a7 a1 a52 a8 a1a14a86a14a86a14a86a10a1 a52a55a54 a47a57a56a75a51 a9 , where a145 a5a37a36 a9 a93 a100 , a45 a1 a52 a98 a52 a37a39a38 , a95a43a105a78a106a84a105 a145 a5a37a36 a9 and where a36 is a linear regular function having rank a145 a5a37a36 a9 and fan-out a113 a5a26a45 a9 , defined on a5 a37a67a60 a58 a9a36a58 a47a12a59 a88 a51 a33 a148a14a148a14a148 a33 a5 a37a61a60 a58 a9 a58 a47a12a59a51a60a18a61a62a36a63a26a51 .</sentence>
				<definiendum id="0">Linear Context-Free Rewriting System</definiendum>
				<definiendum id="1">a119</definiendum>
				<definiens id="0">a quadruple a117 a42 a5 a37 a38 a1a75a37a59a58 a1a6a119 a1 a18 a9 where a37 a38</definiens>
			</definition>
			<definition id="9">
				<sentence>We set a117 a79 a42 a5 a37 a79 a38 a1a80a37a59a58 a1a143a119 a79 a1 a0a18 a2a24a9 , where a37 a79 a38 a42 a44 a0 a127 a1a89a54a13a2 a28a39a127 a52 a119 a1a99a54 a52 a68a12a69a71a70a147a72a75a74 a5 a117 a9a85a56a143a64 a44 a0a18 a2a30a56 , a68a50a69a146a70a147a72a75a74 a5 a117 a9 is the set of all indexes appearing in the productions of a117 , and a119a67a79 is constructed as follows .</sentence>
				<definiendum id="0">a119a67a79</definiendum>
				<definiens id="0">the set of all indexes appearing in the productions of a117 , and</definiens>
			</definition>
			<definition id="10">
				<sentence>Then a44 a5 a127 a1a89a106 a1a2a108 a9a76a42 a25 a79 a7 a25 a79 a8 a148a14a148a14a148 a25 a79 a14 , where a17 a25 a79 a16 a42 a25a18a16 in case a25a19a16a67a52 a37a59a58 ; and a17 a25 a79 a16 a42 a43 a49a12a20a21 in case a25a19a16a101a52 a41 a5 a37 a38 a9 , where a54 is the index of a25a19a16 and the indicated occurrence of a25a15a16 is the a22 -th occurrence of such symbol appearing from left to right in string a121 a154 .</sentence>
				<definiendum id="0">a54</definiendum>
			</definition>
			<definition id="11">
				<sentence>For each nullable link , we create a21 a7 versions of the link , where a10 is the number of nullable strings of that link .</sentence>
				<definiendum id="0">a10</definiendum>
				<definiens id="0">the number of nullable strings of that link</definiens>
			</definition>
			<definition id="12">
				<sentence>Generalized Multitext Grammar is a convenient and intuitive model of parallel text .</sentence>
				<definiendum id="0">Generalized Multitext Grammar</definiendum>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>MRS is the underspecification language which is used in large-scale HPSG grammars , such as the English Resource Grammar ( ERG ) ( Copestake and Flickinger , 2000 ) .</sentence>
				<definiendum id="0">MRS</definiendum>
			</definition>
			<definition id="1">
				<sentence>An MRS M is a configuration if it satisfies conditions C1 and C2 : C1 The graph of M is a tree of solid edges : ( i ) all handles are labels i. e. , arg ( M ) =/0 and M contains no handle constraints , ( ii ) handles don’t properly outscope themselve , and ( iii ) all handles are pairwise connected by EPs in M. C2 If h : Q x ( h 1 , h 2 ) and h prime : P ( ... , x , ... ) belong to M , then h outscopes h prime in Mi .</sentence>
				<definiendum id="0">MRS M</definiendum>
				<definiens id="0">a configuration if it satisfies conditions C1 and C2 : C1 The graph of M is a tree of solid edges : ( i ) all handles are labels i. e. , arg ( M ) =/0 and M contains no handle constraints</definiens>
				<definiens id="1">h 2 ) and h prime : P ( ... , x , ... ) belong to M , then h outscopes h prime in Mi</definiens>
			</definition>
			<definition id="2">
				<sentence>An MRS M contains an EP-conjunction if it contains different EPs with the same label h.The intuition is that EP-conjunctions are interpreted by object language conjunctions .</sentence>
				<definiendum id="0">MRS M</definiendum>
				<definiens id="0">contains an EP-conjunction if it contains different EPs with the same label h.The intuition is that EP-conjunctions are interpreted by object language conjunctions</definiens>
			</definition>
			<definition id="3">
				<sentence>Dominance constraints are the core language underlying CLLS ( Egg et al. , 2001 ) which adds parallelism and binding constraints .</sentence>
				<definiendum id="0">Dominance constraints</definiendum>
				<definiens id="0">Egg et al. , 2001 ) which adds parallelism and binding constraints</definiens>
			</definition>
			<definition id="4">
				<sentence>We assume a possibly infinite signature Σ = { f , g , ... } of function symbols with fixed arities ( written ar ( f ) ) and an infinite set of variables ranged over by X , Y , Z. A dominance constraint ϕ is a conjunction of dominance , inequality , and labeling literals of the following form , where ar ( f ) =n : ϕ : := X triangleleft ∗ Y | X negationslash= Y | X : f ( X 1 , ... , X n ) |ϕ∧ϕ prime Dominance constraints are interpreted over finite constructor trees i. e. , ground terms constructed from the function symbols in Σ .</sentence>
				<definiendum id="0">dominance constraint ϕ</definiendum>
				<definiendum id="1">ar</definiendum>
				<definiens id="0">written ar ( f ) ) and an infinite set of variables ranged over by X</definiens>
			</definition>
			<definition id="5">
				<sentence>A solution for a dominance constraint ϕ consists of a tree τ and an assignment α that maps the variables in ϕ to nodes of τ such that all constraints are satisfied : labeling literals X : f ( X 1 , ... , X n ) are satisfied iff α ( X ) is labeled with f and its daughters are α ( X 1 ) , ... , α ( X n ) in this order ; dominance literals X triangleleft ∗ Y are satisfied iff α ( X ) dominates α ( Y ) in τ ; and inequality literals X negationslash= Y are satisfied iff α ( X ) and α ( Y ) are distinct nodes .</sentence>
				<definiendum id="0">solution for a dominance constraint ϕ</definiendum>
				<definiendum id="1">X</definiendum>
				<definiendum id="2">... , α ( X n</definiendum>
				<definiens id="0">consists of a tree τ and an assignment α that maps the variables in ϕ to nodes of τ such that all constraints are satisfied : labeling literals X : f ( X 1 , ... , X n ) are satisfied iff α (</definiens>
			</definition>
			<definition id="6">
				<sentence>N3 ( a ) if X triangleleft ∗ Y occurs in ϕ , Y is a root in ϕ .</sentence>
				<definiendum id="0">N3</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiens id="0">a root in ϕ</definiens>
			</definition>
			<definition id="7">
				<sentence>( b ) if X triangleleft ∗ Y occurs in ϕ , X is a hole in ϕ .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a hole in ϕ</definiens>
			</definition>
			<definition id="8">
				<sentence>The dominance graph of ϕ is a directed graph G = ( V , E T unionmultiE D ) defined as follows .</sentence>
				<definiendum id="0">dominance graph of ϕ</definiendum>
				<definiens id="0">a directed graph G = ( V , E T unionmultiE D ) defined as follows</definiens>
			</definition>
			<definition id="9">
				<sentence>An MRS constraint M can be represented as a corresponding dominance constraint ϕ M as follows : The variables of ϕ M are the handles of M , and the literals of ϕ M correspond ... ... ... ... ... ( a ) strong ( b ) weak ( c ) island Figure 5 : Fragment Schemata of Nets those of M in the following sence : h : P ( x 1 , ... , x n , h 1 , ... , h k ) mapsto→ h : P x 1 , ... , x n ( h 1 , ... , h k ) h : Q x ( h 1 , h 2 ) mapsto→ h : Q x ( h 1 , h 2 ) h = q h prime mapsto→ h triangleleft ∗ h prime Additionally , dominance literals h triangleleft ∗ h prime are added to ϕ M for all h , h prime s. t. h : Q x ( h 1 , h 2 ) and h prime : P ( ... , x , ... ) belong to M ( cf. C2 ) , and literals h negationslash= h prime are added to ϕ M for all h , h prime in distinct label position in M. Lemma 1 .</sentence>
				<definiendum id="0">MRS constraint M</definiendum>
				<definiendum id="1">... , x n</definiendum>
				<definiendum id="2">Q x</definiendum>
				<definiendum id="3">Q x ( h</definiendum>
				<definiens id="0">Fragment Schemata of Nets those of M in the following sence : h : P ( x 1 , ... , x n , h 1 , ... , h k ) mapsto→ h : P x 1 ,</definiens>
				<definiens id="1">dominance literals h triangleleft ∗ h prime are added to ϕ M for all h , h prime s. t. h : Q x ( h 1 , h 2 ) and h prime : P ( ... , x , ... ) belong to M ( cf. C2 ) , and literals h negationslash= h prime are added to ϕ M for all h , h prime in distinct label position in M. Lemma 1</definiens>
			</definition>
			<definition id="10">
				<sentence>Let M be an MRS and ϕ M be the translation of M.IfM is a connected MRS-net , then the merging-free configurations of M bijectively correspond to the minimal solved forms of the ϕ M .</sentence>
				<definiendum id="0">M.IfM</definiendum>
				<definiens id="0">a connected MRS-net , then the merging-free configurations of M bijectively correspond to the minimal solved forms of the ϕ M</definiens>
			</definition>
			<definition id="11">
				<sentence>ϕ M prime must also be more specific than the graph of ϕ M because the graph of M prime satisfies all dominance requirements of the handle constraints in M , hence ϕ M prime is a solved form of ϕ M .</sentence>
				<definiendum id="0">M prime</definiendum>
				<definiens id="0">a solved form of ϕ M</definiens>
			</definition>
</paper>

		<paper id="1046">
</paper>

		<paper id="1062">
			<definition id="0">
				<sentence>§3 shows how DA can be used for parameter estimation for models of language structure that use dynamic programming to compute posteriors over hidden structure , such as hidden Markov models ( HMMs ) and stochastic context-free grammars ( SCFGs ) .</sentence>
				<definiendum id="0">§3</definiendum>
				<definiens id="0">shows how DA can be used for parameter estimation for models of language structure that use dynamic programming to compute posteriors over hidden structure , such as hidden Markov models ( HMMs ) and stochastic context-free grammars ( SCFGs )</definiens>
			</definition>
			<definition id="1">
				<sentence>Suppose our data consist of a pairs of random variables X and Y , where the value of X is observed and Y is hidden .</sentence>
				<definiendum id="0">Suppose our data</definiendum>
				<definiens id="0">consist of a pairs of random variables X and Y , where the value of X is observed and Y is hidden</definiens>
			</definition>
			<definition id="2">
				<sentence>2 : The DA algorithm : a generalization of EM .</sentence>
				<definiendum id="0">DA algorithm</definiendum>
				<definiens id="0">a generalization of EM</definiens>
			</definition>
			<definition id="3">
				<sentence>Finally , as β → +∞ , ˜p tends to place nearly all of the probability mass on the single most likely vectory .</sentence>
				<definiendum id="0">˜p</definiendum>
				<definiens id="0">tends to place nearly all of the probability mass on the single most likely vectory</definiens>
			</definition>
			<definition id="4">
				<sentence>Deterministic annealing moves the randomness “inside” the objective function by taking expectations .</sentence>
				<definiendum id="0">Deterministic annealing</definiendum>
				<definiens id="0">moves the randomness “inside” the objective function by taking expectations</definiens>
			</definition>
			<definition id="5">
				<sentence>4That is , assuming the usual generative parameterization of such models ; if we generalize to Markov random fields ( also known as log-linear or maximum entropy models ) the M step , while still concave , might entail an auxiliary optimization routine such as iterative scaling or a gradient-based method .</sentence>
				<definiendum id="0">4That</definiendum>
				<definiens id="0">the usual generative parameterization of such models ; if we generalize to Markov random fields ( also known as log-linear or maximum entropy models ) the M step , while still concave , might entail an auxiliary optimization routine such as iterative scaling or a gradient-based method</definiens>
			</definition>
			<definition id="6">
				<sentence>The skewed DA ( SDA ) E step is given by : ˜p ( y ) ← 1Z ( β ) Pr ( x , y|θ ) β´p ( y ) 1−β ( 3 ) When β is close to 0 , the E step will choose ˜p to be very close to ´p. With small β , SDA is a “cautious” EM variant that is wary of moving too far from the initializing posterior ´p ( or , equivalently , the initial parameters vectorθ ( 0 ) ) .</sentence>
				<definiendum id="0">SDA</definiendum>
				<definiens id="0">close to 0 , the E step will choose ˜p to be very close to ´p. With small β ,</definiens>
				<definiens id="1">a “cautious” EM variant that is wary of moving too far from the initializing posterior ´p</definiens>
			</definition>
			<definition id="7">
				<sentence>The constituent-context model ( CCM ) they present is a generative , deficient channel model of POS tag strings given binary tree bracketings .</sentence>
				<definiendum id="0">constituent-context model</definiendum>
				<definiens id="0">a generative , deficient channel model of POS tag strings given binary tree bracketings</definiens>
			</definition>
			<definition id="8">
				<sentence>The CCM gives to a pair ( x , y ) the following probability : Pr ( x , y ) = Pr ( y ) · productdisplay 1≤i≤j≤|x| bracketleftBig ψ parenleftBig xji vextendsinglevextendsingle vextendsingleyi , j parenrightBig ·χ ( xi−1 , xj+1|yi , j ) bracketrightBig where ψ is a conditional distribution over possible tag-sequence yields ( given whether the yield is a constituent or not ) and χ is a conditional distribution over possible contexts of one tag on either side of the yield ( given whether the yield is a constituent or not ) .</sentence>
				<definiendum id="0">CCM</definiendum>
				<definiendum id="1">χ</definiendum>
				<definiens id="0">a conditional distribution over possible contexts of one tag</definiens>
				<definiens id="1">a constituent or not )</definiens>
			</definition>
			<definition id="9">
				<sentence>UR is unlabeled recall , UP is unlabeled precision , F is their harmonic mean , and CB is the average number of crossing brackets per sentence .</sentence>
				<definiendum id="0">CB</definiendum>
				<definiens id="0">the average number of crossing brackets per sentence</definiens>
			</definition>
			<definition id="10">
				<sentence>15Binomial sign test , with significance defined as p &lt; 0.05 , though all significant results had p &lt; 0.001 .</sentence>
				<definiendum id="0">15Binomial sign test</definiendum>
				<definiens id="0">p &lt; 0.05 , though all significant results had p &lt; 0.001</definiens>
			</definition>
</paper>

		<paper id="2009">
			<definition id="0">
				<sentence>Ellipsis is a linguistic phenomenon that has received considerable attention , mostly focusing on its interpretation .</sentence>
				<definiendum id="0">Ellipsis</definiendum>
				<definiens id="0">a linguistic phenomenon that has received considerable attention , mostly focusing on its interpretation</definiens>
			</definition>
			<definition id="1">
				<sentence>It is difficult to extrapolate how successful MBL GIS-MaxEnt L-BFGS-MaxEnt Rec Prec F1 Rec Prec F1 Rec Prec F1 Charniak Words + POS 54.00 62.30 57.85 38.66 79.45 52.01 56.66 71.42 63.19 + features 58.00 65.41 61.48 50.66 73.78 60.07 65.33 72.05 68.53 RASP Words + POS 55.92 66.92 60.93 43.42 56.89 49.25 51.63 79.00 62.45 + features 57.23 71.31 63.50 61.84 72.30 66.66 62.74 73.84 67.84 Table 9 : Results on re-parsed data from the Treebank MBL GIS-MaxEnt L-BFGS-MaxEnt Rec Prec F1 Rec Prec F1 Rec Prec F1 Charniak Words + POS 66.50 63.63 65.03 55.00 75.86 63.76 71.00 70.64 70.82 + features 67.50 67.16 67.33 65.00 75.58 69.89 71.00 73.19 72.08 RASP Words + POS 61.92 63.21 62.56 64.46 54.04 58.79 65.34 70.96 68.04 + features 71.06 73.29 72.16 73.09 61.01 66.51 70.29 67.29 68.76 Table 11 : Results on parsed data from the BNC MBL GIS-MaxEnt L-BFGS-MaxEnt Rec Prec F1 Rec Prec F1 Rec Prec F1 Charniak Words + POS 62.28 69.20 65.56 54.28 77.86 63.97 65.14 69.30 67.15 + features 65.71 71.87 68.65 63.71 72.40 67.78 70.85 69.85 70.35 RASP Words + POS 63.61 67.47 65.48 59.31 55.94 57.37 57.46 71.83 63.84 + features 68.48 69.88 69.17 67.61 71.47 69.48 70.14 72.17 71.14 Table 13 : Results on parsed data using the combined dataset such approaches would be based on current work , but it can be expected that they would be feasible , albeit with lower performance .</sentence>
				<definiendum id="0">MBL GIS-MaxEnt L-BFGS-MaxEnt Rec</definiendum>
				<definiens id="0">Results on re-parsed data from the Treebank</definiens>
				<definiens id="1">Results on parsed data from the BNC MBL GIS-MaxEnt</definiens>
			</definition>
</paper>

		<paper id="2012">
			<definition id="0">
				<sentence>I define a candidate inflection class ( CIC ) to be a set of c-suffixes for which there exists at least one c-stem , t , such that each c-suffix in the CIC concatenated to t produces a word form in the vocabulary .</sentence>
				<definiendum id="0">candidate inflection class</definiendum>
				<definiendum id="1">CIC</definiendum>
				<definiens id="0">a set of c-suffixes for which there exists at least one c-stem , t , such that each c-suffix in the CIC concatenated to t produces a word form in the vocabulary</definiens>
			</definition>
			<definition id="1">
				<sentence>Here I will look at the χ2 independence test , which computes the probability that two random variables are independent by calculating a statistic Q distributed as χ2 by comparing the expected distributions of the two random variables , assuming their independence with their actual distribution .</sentence>
				<definiendum id="0">independence test</definiendum>
				<definiens id="0">computes the probability that two random variables are independent by calculating a statistic Q distributed as χ2 by comparing the expected distributions of the two random variables , assuming their independence with their actual distribution</definiens>
			</definition>
</paper>

		<paper id="3029">
			<definition id="0">
				<sentence>The wheel metaphor presents the items as a list on a conveyor belt , which can be easily and quickly rotated .</sentence>
				<definiendum id="0">wheel metaphor</definiendum>
				<definiens id="0">presents the items as a list on a conveyor belt , which can be easily and quickly rotated</definiens>
			</definition>
			<definition id="1">
				<sentence>The MIAMM system uses the standard architecture for dialogue systems with analysis and generation layers , interaction management and application interface ( see figure 3 ) .</sentence>
				<definiendum id="0">MIAMM system</definiendum>
				<definiens id="0">uses the standard architecture for dialogue systems with analysis and generation layers , interaction management and application interface</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>The collected corpus consists of 66 dialog logfiles , containing on average 12 turns .</sentence>
				<definiendum id="0">collected corpus</definiendum>
			</definition>
			<definition id="1">
				<sentence>In particular , mathematical expressions ( or parts thereof ) may lie within the scope of quantifiers or negation expressed in natural language : A aucha53a55a54a57a56a59a58a49a60 [ a61a63a62a3a64a65a67a66a68a53a63a69a70a56a72a71a49a60 ] Aa73 B ista74 von Ca75 ( Aa73 B ) [ ... isa74 of . . . ] ( da ja Aa73 B=a76 ) [ ( because Aa73 B=a76 ) ] B enthaelt kein xa74 A [ B contains no xa74 A ] For parsing , this means that the mathematical content has to be identified before it is interpreted within the utterance .</sentence>
				<definiendum id="0">mathematical expressions</definiendum>
				<definiens id="0">lie within the scope of quantifiers or negation expressed in natural language : A aucha53a55a54a57a56a59a58a49a60</definiens>
				<definiens id="1">the mathematical content has to be identified before it is interpreted within the utterance</definiens>
			</definition>
			<definition id="2">
				<sentence>where PROP-LOC denotes the HasProperty relation of type Location , GENREL is a general relation as in complementation , and PROP-FROM is a HasProperty relation of type Direction-From or From-Source .</sentence>
				<definiendum id="0">PROP-LOC</definiendum>
				<definiendum id="1">GENREL</definiendum>
				<definiendum id="2">PROP-FROM</definiendum>
				<definiens id="0">a general relation as in complementation</definiens>
			</definition>
			<definition id="3">
				<sentence>An HLDS term is a relational structure where dependency relations between heads and dependents are encoded as modal relations .</sentence>
				<definiendum id="0">HLDS term</definiendum>
				<definiens id="0">a relational structure where dependency relations between heads and dependents are encoded as modal relations</definiens>
			</definition>
			<definition id="4">
				<sentence>FORMULA represents the default lexical entry for identified mathematical expressions categorized as “formula” ( cf. Section 4.1 ) .</sentence>
				<definiendum id="0">FORMULA</definiendum>
				<definiens id="0">the default lexical entry for identified mathematical expressions categorized as “formula” ( cf. Section 4.1 )</definiens>
			</definition>
			<definition id="5">
				<sentence>The LM is represented by the following HLDS term : @ h1 ( contain a18 a6 ACTa7 ( f1 a18 FORMULA : B ) a18 a6 PATa7 ( f2 a18 FORMULA : a19a21a20 a26 ) where h1 is the state where the proposition contain is true , and the nominals f1 and f2 represent dependents of the head contain , which stand in the tectogrammatical relations Actor and Patient , respectively .</sentence>
				<definiendum id="0">h1</definiendum>
				<definiens id="0">dependents of the head contain , which stand in the tectogrammatical relations Actor and Patient , respectively</definiens>
			</definition>
			<definition id="6">
				<sentence>11In prior discourse , there may have been an assignment B : = a43 , where a43 is a formula , in which case , B would be known from discourse context to be of type FORMULA ( similarly for term assignment ) ; by CONST we mean a set or element variable such as A , x denoting a set A or an element x respectively .</sentence>
				<definiendum id="0">a43</definiendum>
				<definiens id="0">a formula , in which case</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>Sentence ranking is a crucial part of generating text summaries .</sentence>
				<definiendum id="0">Sentence ranking</definiendum>
				<definiens id="0">a crucial part of generating text summaries</definiens>
			</definition>
			<definition id="1">
				<sentence>idf weight of sentence i in document j , n si is the number of words in sentence i , k is the kth word in sentence i , tf jk is the frequency of word k in document j , n d is the number of documents in the reference corpus , and df k is the number of documents in the reference corpus in which word k appears .</sentence>
				<definiendum id="0">n si</definiendum>
				<definiendum id="1">jk</definiendum>
				<definiendum id="2">df k</definiendum>
				<definiens id="0">the frequency of word k in document j , n d is the number of documents in the reference corpus , and</definiens>
			</definition>
			<definition id="2">
				<sentence>The Nucleus is the equivalent of the arc destination , and the Satellite is the equivalent of the arc origin in the non-treebased approach .</sentence>
				<definiendum id="0">Nucleus</definiendum>
				<definiendum id="1">Satellite</definiendum>
				<definiens id="0">the equivalent of the arc destination</definiens>
			</definition>
			<definition id="3">
				<sentence>Sim represents a similarity relation , and elab an elaboration relation .</sentence>
				<definiendum id="0">Sim</definiendum>
				<definiens id="0">a similarity relation , and elab an elaboration relation</definiens>
			</definition>
			<definition id="4">
				<sentence>Figure 4 shows Marcu ( 2000 ) ’s algorithm , where r ( s , D , d ) is the rank of a sentence s in a discourse tree D with depth d. Every node in a discourse tree D has a promotion set promotion ( D ) , which is the union of all Nucleus children of that node .</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">the rank of a sentence s in a discourse tree D with depth d. Every node in a discourse tree D has a promotion set promotion</definiens>
				<definiens id="1">the union of all Nucleus children of that node</definiens>
			</definition>
			<definition id="5">
				<sentence>A node represents a sentence , and the in-degree of a node represents the number of sentences that relate to that sentence .</sentence>
				<definiendum id="0">node</definiendum>
				<definiens id="0">the number of sentences that relate to that sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>Unlike just determining the in-degree of a node , PageRank takes into account the importance of sentences that relate to a sentence .</sentence>
				<definiendum id="0">PageRank</definiendum>
				<definiens id="0">takes into account the importance of sentences that relate to a sentence</definiens>
			</definition>
			<definition id="7">
				<sentence>PR n is the PageRank of the current sentence , PR n-1 is the PageRank of the sentence that relates to sentence n , o n-1 is the out-degree of sentence n-1 , and α is a damping parameter that is set to a value between 0 and 1 .</sentence>
				<definiendum id="0">PR n</definiendum>
				<definiendum id="1">PR n-1</definiendum>
				<definiendum id="2">α</definiendum>
				<definiens id="0">the PageRank of the current sentence</definiens>
				<definiens id="1">the PageRank of the sentence that relates to sentence n , o n-1 is the out-degree of sentence n-1 , and</definiens>
				<definiens id="2">a damping parameter that is set to a value between 0 and 1</definiens>
			</definition>
			<definition id="8">
				<sentence>MarcuAvg refers to the version of Marcu ( 2000 ) ’s algorithm where we calculated sentence rankings as the average of the rankings of all discourse segments that constitute that sentence ; for MarcuMin , sentence rankings were the minimum of the rankings of all discourse segments in that sentence ; for MarcuMax we selected the maximum of the rankings of all discourse segments in that sentence .</sentence>
				<definiendum id="0">MarcuAvg</definiendum>
				<definiens id="0">the version of Marcu ( 2000 ) ’s algorithm where we calculated sentence rankings as the average of the rankings of all discourse segments that constitute that sentence ; for MarcuMin , sentence rankings were the minimum of the rankings of all discourse segments in that sentence</definiens>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>A term is a meaningful unit that represents a specific concept within a domain ( Wright , 1997 ) .</sentence>
				<definiendum id="0">term</definiendum>
			</definition>
			<definition id="1">
				<sentence>12 ( ) ( ) ( ) ( ) Spec Y Spec X Spec W Spec Wα β= +⋅ +⋅ ( 2 ) where Spec ( X ) , Spec ( W 1 ) , and Spec ( W 2 ) are specificity values of X , W 1 , W 2 respectively .</sentence>
				<definiendum id="0">Spec</definiendum>
				<definiens id="0">specificity values of X</definiens>
			</definition>
			<definition id="2">
				<sentence>{ |1 } k Tt kn=≤≤ ( 3 ) where t k is a term and n is total number of terms .</sentence>
				<definiendum id="0">t k</definiendum>
				<definiens id="0">a term and n is total number of terms</definiens>
			</definition>
			<definition id="3">
				<sentence>{ |1 } ( ) Prob ( ) k kk Xx kn px X x = ≤≤ == ( 4 ) where x k is an event of a term t k occurs in corpus , p ( x k ) is the probability of event x k .</sentence>
				<definiendum id="0">x k</definiendum>
				<definiendum id="1">p ( x k )</definiendum>
				<definiens id="0">an event of a term t k occurs in corpus</definiens>
				<definiens id="1">the probability of event x k</definiens>
			</definition>
			<definition id="4">
				<sentence>( ) ( ) ( ) ( ) i iMLEi j j freq w py p w freq w ≈= ∑ ( 9 ) where freq ( w ) is frequency of word w in corpus , P MLE ( w i ) is maximum likelihood estimation of P ( w i ) , and j is index of all words in corpus .</sentence>
				<definiendum id="0">P MLE</definiendum>
				<definiens id="0">frequency of word w in corpus</definiens>
				<definiens id="1">index of all words in corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>( ) ( , ) log ( , ) modk ik ik i Ht pmodt pmodt=− ∑ ( 11 ) where p ( mod i , t k ) is the probability of mod i modifies t k and is estimated as equation ( 12 ) .</sentence>
				<definiendum id="0">p ( mod i</definiendum>
				<definiens id="0">the probability of mod i modifies t k and is estimated as equation ( 12 )</definiens>
			</definition>
			<definition id="6">
				<sentence>( , ) ( , ) ( , ) ik MLE i k j k j freq mod t pmodt freq mod t = ∑ ( 12 ) where freq ( mod i , t k ) is number of frequencies that mod i modifies t k in corpus , j is index of all modifiers of t k in corpus .</sentence>
				<definiendum id="0">freq ( mod</definiendum>
				<definiens id="0">number of frequencies that mod i modifies t k in corpus , j is index of all modifiers of t k in corpus</definiens>
			</definition>
			<definition id="7">
				<sentence>“metabolic diseases ( C18.452 ) ” node is root of the subtree , and the subtree consists of 436 disease names which are target terms of specificity measuring .</sentence>
				<definiendum id="0">“metabolic diseases</definiendum>
			</definition>
			<definition id="8">
				<sentence>Coverage is the fraction 2 MEDLINE is a database of biomedical articles serviced by National Library of Medicine , USA .</sentence>
				<definiendum id="0">Coverage</definiendum>
				<definiendum id="1">MEDLINE</definiendum>
				<definiens id="0">a database of biomedical articles serviced by National Library of Medicine , USA</definiens>
			</definition>
			<definition id="9">
				<sentence>Precision is the fraction of relations with correct specificity values as equation ( 16 ) .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the fraction of relations with correct specificity values as equation ( 16 )</definiens>
			</definition>
			<definition id="10">
				<sentence># ( , ) # ( , ) of R p c with correct specificity p of all R p c = ( 16 ) where R ( p , c ) is a parent-child relation in MeSH thesaurus , and this relation is valid only when specificity of two terms are measured by given method .</sentence>
				<definiendum id="0">R</definiendum>
				<definiendum id="1">c )</definiendum>
				<definiens id="0">a parent-child relation in MeSH thesaurus , and this relation is valid only when specificity of two terms are measured by given method</definiens>
			</definition>
</paper>

		<paper id="1082">
			<definition id="0">
				<sentence>State-of-the-art statistical parsers ( e.g. Charniak , 2000 ) typically provide a labeled bracketing only ; i.e. , a parse tree without empty categories .</sentence>
				<definiendum id="0">State-of-the-art statistical parsers</definiendum>
				<definiens id="0">a parse tree without empty categories</definiens>
			</definition>
			<definition id="1">
				<sentence>The PTB annotation in general , but especially the annotation of empty categories , follows a modified version of Government-Binding ( GB ) theory ( Chomsky , 1981 ) .</sentence>
				<definiendum id="0">PTB annotation</definiendum>
			</definition>
			<definition id="2">
				<sentence>For example , the GB categories NPtrace and PRO ( which are conflated to a single category in the PTB ) occur only in argument positions in which an overt NP could not occur , namely as the object of a passive verb or as the subject of certain kinds of infinitive .</sentence>
				<definiendum id="0">PRO</definiendum>
				<definiens id="0">the object of a passive verb or as the subject of certain kinds of infinitive</definiens>
			</definition>
			<definition id="3">
				<sentence>GB principles , which are enforced in the annotation guidelines , dictate that an empty category must be inserted as the subject of the infinitival S ; but exactly which empty category , NP* or NP*T* , depends on properties of the governing verb , including whether it is a raising or control verb , such as seem or try , or an ECM verb , such as believe .</sentence>
				<definiendum id="0">GB principles</definiendum>
				<definiens id="0">empty category , NP* or NP*T* , depends on properties of the governing verb , including whether it is a raising or control verb</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>Transliteration is a process that takes a character string in source language as input and generates a character string in the target language as output .</sentence>
				<definiendum id="0">Transliteration</definiendum>
				<definiens id="0">a process that takes a character string in source language as input and generates a character string in the target language as output</definiens>
			</definition>
			<definition id="1">
				<sentence>Then , the E2C transliteration can be formulated as ) , , ( maxarg , γβαβ γβ P= ( 4 ) and similarly the C2E back-transliteration as ) , , ( maxarg , γβαα γα P= ( 5 ) An n-gram transliteration model is defined as the conditional probability , or transliteration probability , of a transliteration pair k ce &gt; &lt; , depending on its immediate n predecessor pairs : ) , , ( ) , ( γβαPCEP = ∏ = − +− &gt; &lt; &gt; &lt; = K k k nkk ceceP 1 1 1 ) , | , ( ( 6 ) A bilingual dictionary contains entries mapping English names to their respective Chinese transliterations .</sentence>
				<definiendum id="0">E2C transliteration</definiendum>
				<definiens id="0">maxarg , γβαβ γβ P= ( 4 ) and similarly the C2E back-transliteration as ) , , ( maxarg , γβαα γα P= ( 5 ) An n-gram transliteration model is defined as the conditional probability , or transliteration probability , of a transliteration pair k ce &gt; &lt; , depending on its immediate n predecessor pairs : ) , , ( ) , ( γβαPCEP = ∏ = − +− &gt; &lt; &gt; &lt; = K k k nkk ceceP 1 1 1 ) , | , ( ( 6 ) A bilingual dictionary contains entries mapping English names to their respective Chinese transliterations</definiens>
			</definition>
			<definition id="2">
				<sentence>The number of parameters in the bigram TM is potentially 2 T , while in the noisy channel model ( NCM ) it’s 2 CT + , where T is the number of transliteration pairs and C is the number of Chinese transliteration units .</sentence>
				<definiendum id="0">NCM</definiendum>
				<definiendum id="1">T</definiendum>
				<definiendum id="2">C</definiendum>
				<definiens id="0">the number of transliteration pairs</definiens>
				<definiens id="1">the number of Chinese transliteration units</definiens>
			</definition>
			<definition id="3">
				<sentence>The cross-entropy ) ( WH p of a model on data W is defined as ) ( log 1 ) ( 2 Wp W WH T p −= where T W is the total number of aligned transliteration pair tokens in the data W. The perplexity ) ( WPP p of a model is the reciprocal of the average probability assigned by the model to each aligned pair in the test set W as ) ( 2 ) ( WH p p WPP = .</sentence>
				<definiendum id="0">cross-entropy ) ( WH p</definiendum>
			</definition>
			<definition id="4">
				<sentence>The character error rate is the sum of deletion , insertion and substitution errors .</sentence>
				<definiendum id="0">character error rate</definiendum>
				<definiens id="0">the sum of deletion , insertion and substitution errors</definiens>
			</definition>
			<definition id="5">
				<sentence>n-gram TM is a successful realization of DOM paradigm .</sentence>
				<definiendum id="0">n-gram TM</definiendum>
			</definition>
</paper>

		<paper id="1077">
			<definition id="0">
				<sentence>Skip-bigram is any pair of words in their sentence order .</sentence>
				<definiendum id="0">Skip-bigram</definiendum>
				<definiens id="0">any pair of words in their sentence order</definiens>
			</definition>
			<definition id="1">
				<sentence>N-gram precision in BLEU is computed as follows : ∑∑ ∑∑ ∈∈− ∈∈− − − = } { } { ) ( ) ( CandidatesCCgramn CandidatesCCgramn clip n gramnCount gramnCount p ( 1 ) Where Count clip ( n-gram ) is the maximum number of n-grams co-occurring in a candidate translation and a reference translation , and Count ( ngram ) is the number of n-grams in the candidate translation .</sentence>
				<definiendum id="0">Count ( ngram )</definiendum>
				<definiens id="0">the number of n-grams in the candidate translation</definiens>
			</definition>
			<definition id="2">
				<sentence>To prevent very short translations that try to maximize their precision scores , BLEU adds a brevity penalty , BP , to the formula : ) 2 ( 1 | ) |/||1 ( ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ ≤ &gt; = − rcife rcif BP cr Where |c| is the length of the candidate translation and |r| is the length of the reference translation .</sentence>
				<definiendum id="0">BLEU</definiendum>
				<definiendum id="1">|c|</definiendum>
				<definiendum id="2">|r|</definiendum>
				<definiens id="0">translations that try to maximize their precision scores</definiens>
				<definiens id="1">adds a brevity penalty , BP , to the formula</definiens>
				<definiens id="2">the length of the candidate translation and</definiens>
				<definiens id="3">the length of the reference translation</definiens>
			</definition>
			<definition id="3">
				<sentence>Third , BLEU is a geometric mean of unigram to N-gram precisions .</sentence>
				<definiendum id="0">BLEU</definiendum>
				<definiens id="0">a geometric mean of unigram to N-gram precisions</definiens>
			</definition>
			<definition id="4">
				<sentence>Melamed ( 1995 ) used the ratio ( LCSR ) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them .</sentence>
				<definiendum id="0">LCSR</definiendum>
				<definiens id="0">the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them</definiens>
			</definition>
			<definition id="5">
				<sentence>We propose using LCS-based F-measure to estimate the similarity between two translations X of length m and Y of length n , assuming X is a reference translation and Y is a candidate translation , as follows : R lcs m YXLCS ) , ( = ( 4 ) P lcs n YXLCS ) , ( = ( 5 ) F lcs lcslcs lcslcs PR PR 2 2 ) 1 ( β β + + = ( 6 ) Where LCS ( X , Y ) is the length of a longest common subsequence of X and Y , and β = P lcs /R lcs when ∂F lcs /∂R lcs _=_∂F lcs /∂P lcs .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiendum id="2">LCS</definiendum>
				<definiens id="0">a candidate translation</definiens>
				<definiens id="1">the length of a longest common subsequence of X and Y , and β</definiens>
			</definition>
			<definition id="6">
				<sentence>Notice that ROUGE-L is 1 when X = Y since LCS ( X , Y ) = m or n ; while ROUGE-L is zero when LCS ( X , Y ) = 0 , i.e. there is nothing in common between X and Y. Fmeasure or its equivalents has been shown to have met several theoretical criteria in measuring accuracy involving more than one factor ( Van Rijsbergen 1979 ) .</sentence>
				<definiendum id="0">ROUGE-L</definiendum>
				<definiendum id="1">ROUGE-L</definiendum>
			</definition>
			<definition id="7">
				<sentence>ROUGE-L as defined in Equation 6 has the property that its value is less than or equal to the minimum of unigram F-measure of X and Y. Unigram recall reflects the proportion of words in X ( reference translation ) that are also present in Y ( candidate translation ) ; while unigram precision is the proportion of words in Y that are also in X. Unigram recall and precision count all co-occurring words regardless their orders ; while ROUGE-L counts only in-sequence co-occurrences .</sentence>
				<definiendum id="0">ROUGE-L</definiendum>
				<definiendum id="1">unigram precision</definiendum>
				<definiens id="0">the proportion of words in Y that are also in X. Unigram recall and precision count all co-occurring words regardless their orders</definiens>
			</definition>
			<definition id="8">
				<sentence>Given two sentences X and Y , the WLCS score of X and Y can be computed using the following dynamic programming procedure : ( 1 ) For ( i = 0 ; i &lt; =m ; i++ ) c ( i , j ) = 0 // initialize c-table w ( i , j ) = 0 // initialize w-table ( 2 ) For ( i = 1 ; i &lt; = m ; i++ ) For ( j = 1 ; j &lt; = n ; j++ ) If x i = y j Then // the length of consecutive matches at // position i-1 and j-1 k = w ( i-1 , j-1 ) c ( i , j ) = c ( i-1 , j-1 ) + f ( k+1 ) – f ( k ) // remember the length of consecutive // matches at position i , j w ( i , j ) = k+1 Otherwise If c ( i-1 , j ) &gt; c ( i , j-1 ) Then c ( i , j ) = c ( i-1 , j ) w ( i , j ) = 0 // no match at i , j Else c ( i , j ) = c ( i , j-1 ) w ( i , j ) = 0 // no match at i , j ( 3 ) WLCS ( X , Y ) = c ( m , n ) Where c is the dynamic programming table , c ( i , j ) stores the WLCS score ending at word x i of X and y j of Y , w is the table storing the length of consecutive matches ended at c table position i and j , and f is a function of consecutive matches at the table position , c ( i , j ) .</sentence>
				<definiendum id="0">c</definiendum>
				<definiendum id="1">w</definiendum>
				<definiendum id="2">f</definiendum>
				<definiens id="0">the length of consecutive matches at // position i-1 and j-1 k = w ( i-1 , j-1 ) c ( i , j ) = c ( i-1 , j-1 ) + f ( k+1 ) – f ( k ) // remember the length of consecutive // matches at position i , j w ( i , j ) = k+1 Otherwise If c ( i-1 , j ) &gt; c ( i , j-1</definiens>
				<definiens id="1">the table storing the length of consecutive matches ended at c table position i and j , and</definiens>
				<definiens id="2">a function of consecutive matches at the table position</definiens>
			</definition>
			<definition id="9">
				<sentence>Statistics Skip-bigram is any pair of words in their sentence order , allowing for arbitrary gaps .</sentence>
				<definiendum id="0">Statistics Skip-bigram</definiendum>
				<definiens id="0">any pair of words in their sentence order , allowing for arbitrary gaps</definiens>
			</definition>
			<definition id="10">
				<sentence>Given translations X of length m and Y of length n , assuming X is a reference translation and Y is a candidate translation , we compute skip-bigram-based F-measure as follows : R skip2 ) 2 , ( ) , ( 2 mC YXSKIP = ( 13 ) P skip2 ) 2 , ( ) , ( 2 nC YXSKIP = ( 14 ) F skip2 2 2 2 22 2 ) 1 ( skipskip skipskip PR PR β β + + = ( 15 ) Where SKIP2 ( X , Y ) is the number of skip-bigram matches between X and Y , β = P skip2 /R skip2 when ∂F skip2 /∂R skip2 _=_∂F skip2 /∂P skip2 , and C is the combination function .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiendum id="2">C</definiendum>
				<definiens id="0">a reference translation</definiens>
				<definiens id="1">a candidate translation</definiens>
			</definition>
			<definition id="11">
				<sentence>ROUGE-S* is skip-bigram-based co-occurrence statistics with any skip distance limit , ROUGESN is skip-bigram-based F-measure ( β = 1 ) with maximum skip distance of N , PER is position independent word error rate , and WER is word error rate .</sentence>
				<definiendum id="0">ROUGESN</definiendum>
				<definiendum id="1">WER</definiendum>
				<definiens id="0">skip-bigram-based co-occurrence statistics with any skip distance limit</definiens>
				<definiens id="1">skip-bigram-based F-measure ( β = 1 ) with maximum skip distance of N , PER is position independent word error rate</definiens>
			</definition>
			<definition id="12">
				<sentence>Among them , ROUGE-L is the best metric in both adequacy and fluency correlation with human judgment according to Spearman’s correlation coefficient and is statistically indistinguishable from the best metrics in both adequacy and fluency correlation with human judgment according to Pearson’s correlation coefficient .</sentence>
				<definiendum id="0">ROUGE-L</definiendum>
				<definiens id="0">the best metric in both adequacy and fluency correlation with human judgment according to Spearman’s correlation coefficient</definiens>
			</definition>
			<definition id="13">
				<sentence>BLEU : a Method for Automatic Evaluation of Machine Translation .</sentence>
				<definiendum id="0">BLEU</definiendum>
			</definition>
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>The phrase van goeden huize ( of good family ) is a frozen expression with archaic inflection .</sentence>
				<definiendum id="0">phrase van goeden huize</definiendum>
				<definiens id="0">a frozen expression with archaic inflection</definiens>
			</definition>
			<definition id="1">
				<sentence>Alpino employs a maximum entropy disambiguation component ; the first parse is the most promising parse according to this statistical model .</sentence>
				<definiendum id="0">Alpino</definiendum>
				<definiens id="0">employs a maximum entropy disambiguation component</definiens>
			</definition>
			<definition id="2">
				<sentence>The maximum entropy disambiguation component of Alpino assigns a score S ( x ) to each parse x : S ( x ) = X i ifi ( x ) ( 1 ) wherefi ( x ) is the frequency of a particular featurei in parse x and i is the corresponding weight of that feature .</sentence>
				<definiendum id="0">maximum entropy disambiguation</definiendum>
				<definiendum id="1">score S</definiendum>
				<definiens id="0">the frequency of a particular featurei in parse x</definiens>
			</definition>
			<definition id="3">
				<sentence>The probability of a parse x for sentence w is then defined as follows , where Y ( w ) are all the parses of w : p ( xjw ) = exp ( S ( x ) ) P y2Y ( w ) exp ( S ( y ) ) ( 2 ) The disambiguation component is described in detail in Malouf and van Noord ( 2004 ) .</sentence>
				<definiendum id="0">probability of a parse x for sentence w</definiendum>
				<definiens id="0">follows , where Y ( w ) are all the parses of w : p ( xjw ) = exp ( S ( x ) ) P y2Y ( w ) exp ( S ( y ) ) ( 2 ) The disambiguation component is described in detail in Malouf and van Noord ( 2004 )</definiens>
			</definition>
			<definition id="4">
				<sentence>The most demanding part of the implementation consists of the computation of the frequency of ngrams .</sentence>
				<definiendum id="0">most demanding part of the implementation</definiendum>
				<definiens id="0">consists of the computation of the frequency of ngrams</definiens>
			</definition>
</paper>

		<paper id="3028">
			<definition id="0">
				<sentence>The Wrapper Approach , introduced by John et al. ( 1994 ) and refined later by Kohavi and John ( 1997 ) , is a method that searches for a good subset of relevant features using an induction algorithm as part of the evaluation function .</sentence>
				<definiendum id="0">Wrapper Approach</definiendum>
				<definiens id="0">a method that searches for a good subset of relevant features using an induction algorithm as part of the evaluation function</definiens>
			</definition>
			<definition id="1">
				<sentence>Forward Selection is a greedy search algorithm that begins with an empty set of features , and greedily adds features to the set .</sentence>
				<definiendum id="0">Forward Selection</definiendum>
				<definiens id="0">a greedy search algorithm that begins with an empty set of features</definiens>
			</definition>
			<definition id="2">
				<sentence>bestFeatures = [ ] while dim ( bestFeatures ) &lt; MINFEATURES for iterations = 1 : MAXITERATIONS split train into training/development parameters = computeParameters ( training ) for feature = 1 : MAXFEATURES evaluate ( parameters , development , [ bestFeatures + feature ] ) keep validation performance end end average_performance and keep average_performance end B = best average_performance bestFeatures  B ¨ bestFeatures end Figure 2 .</sentence>
				<definiendum id="0">bestFeatures</definiendum>
				<definiens id="0">MAXITERATIONS split train into training/development parameters = computeParameters ( training ) for feature = 1 : MAXFEATURES evaluate ( parameters , development</definiens>
			</definition>
</paper>

		<paper id="3007">
</paper>

		<paper id="1076">
			<definition id="0">
				<sentence>As far as names are concerned , co-reference consists of two sub-tasks : ( i ) name disambiguation to handle the problem of different entities happening to use the same name ; ( ii ) alias association to handle the problem of the same entity using multiple names ( aliases ) .</sentence>
				<definiendum id="0">co-reference</definiendum>
				<definiens id="0">consists of two sub-tasks : ( i ) name disambiguation to handle the problem of different entities happening to use the same name ; ( ii ) alias association to handle the problem of the same entity using multiple names ( aliases )</definiens>
			</definition>
			<definition id="1">
				<sentence>The InfoXtract engine contains a named entity tagger , an aliasing module , a parser and an entity relationship extractor .</sentence>
				<definiendum id="0">InfoXtract engine</definiendum>
				<definiens id="0">contains a named entity tagger</definiens>
			</definition>
			<definition id="2">
				<sentence>( log* ) , ( ) , ( idf D jitfjiweight = ( 7 ) where ) , ( jitf is the frequency of word i in the j-th surface string sequence ; D is the number of documents in the pool ; and ) ( idf is the number of documents containing the word i. Then , the cosine of the angle between the two resulting vectors is used as the context similarity measure .</sentence>
				<definiendum id="0">jitf</definiendum>
				<definiendum id="1">D</definiendum>
				<definiendum id="2">idf</definiendum>
				<definiens id="0">the number of documents in the pool</definiens>
				<definiens id="1">the number of documents containing the word i. Then , the cosine of the angle between the two resulting vectors is used as the context similarity measure</definiens>
			</definition>
			<definition id="3">
				<sentence>LSA is a technique to uncover the underlining semantics based on co-occurrence data .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">a technique to uncover the underlining semantics based on co-occurrence data</definiens>
			</definition>
			<definition id="4">
				<sentence>SVD yields the following Matrix decomposition : T DSTMatrix 000 = ( 8 ) where T and D are orthogonal matrices ( the row vector is called singular vectors ) , and S is a diagonal matrix with the diagonal elements ( called singular values ) sorted decreasingly .</sentence>
				<definiendum id="0">SVD</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">yields the following Matrix decomposition : T DSTMatrix 000 = ( 8 ) where T and D are orthogonal matrices ( the row vector is called singular vectors ) , and</definiens>
				<definiens id="1">a diagonal matrix with the diagonal elements ( called singular values ) sorted decreasingly</definiens>
			</definition>
			<definition id="5">
				<sentence>The optimization process consists of two steps .</sentence>
				<definiendum id="0">optimization process</definiendum>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>SPaRKy consists of a randomized sentence plan generator ( SPG ) and a trainable sentence plan ranker ( SPR ) ; these are described in Sections 3 strategy : recommend items : Chanpen Thai relations : justify ( nuc:1 ; sat:2 ) ; justify ( nuc:1 ; sat:3 ) ; justify ( nuc:1 ; sat:4 ) content : 1 .</sentence>
				<definiendum id="0">SPaRKy</definiendum>
				<definiens id="0">consists of a randomized sentence plan generator ( SPG ) and a trainable sentence plan ranker ( SPR ) ; these are described in Sections 3 strategy : recommend items : Chanpen Thai relations : justify ( nuc:1</definiens>
			</definition>
			<definition id="1">
				<sentence>Following the bottom-up approach to text-planning described in ( Marcu , 1997 ; Mellish , 1998 ) , each presentation consists of a set of assertions about a set of restaurants and a speci cation of the rhetorical relations that hold between them .</sentence>
				<definiendum id="0">presentation</definiendum>
			</definition>
			<definition id="2">
				<sentence>On the other hand , Carmine’s is an Italian restaurant .</sentence>
				<definiendum id="0">Carmine’s</definiendum>
				<definiens id="0">an Italian restaurant</definiens>
			</definition>
			<definition id="3">
				<sentence>SPaRKy , the sentence planner , gets the content plan , and then a sentence plan generator ( SPG ) generates one or more sentence plans ( Figure 7 ) and a sentence plan ranker ( SPR ) ranks the generated plans .</sentence>
				<definiendum id="0">SPaRKy</definiendum>
				<definiens id="0">the sentence planner , gets the content plan , and then a sentence plan generator ( SPG ) generates one or more sentence plans ( Figure 7 ) and a sentence plan ranker ( SPR ) ranks the generated plans</definiens>
			</definition>
			<definition id="4">
				<sentence>A prosody assignment module uses the prior levels of linguistic representation to determine the appropriate prosody for the utterance , and passes a markedup string to the text-to-speech module .</sentence>
				<definiendum id="0">prosody assignment module</definiendum>
				<definiens id="0">uses the prior levels of linguistic representation to determine the appropriate prosody for the utterance , and passes a markedup string to the text-to-speech module</definiens>
			</definition>
			<definition id="5">
				<sentence>As in SPoT , the basis of the SPG is a set of clause-combining operations that operate on tptrees and incrementally transform the elementary predicate-argument lexico-structural representations ( called DSyntS ( Melcuk , 1988 ) ) associated with the speech-acts on the leaves of the tree .</sentence>
				<definiendum id="0">SPG</definiendum>
				<definiens id="0">a set of clause-combining operations that operate on tptrees and incrementally transform the elementary predicate-argument lexico-structural representations ( called DSyntS ( Melcuk , 1988 ) ) associated with the speech-acts on the leaves of the tree</definiens>
			</definition>
			<definition id="6">
				<sentence>For example , with-reduction ( Above is a New American restaurant ; Above has good decor ) yields Above is a New American restaurant , with good decor .</sentence>
				<definiendum id="0">with-reduction ( Above</definiendum>
				<definiendum id="1">Above</definiendum>
				<definiens id="0">a New American restaurant ;</definiens>
			</definition>
			<definition id="7">
				<sentence>HOWEVER , Above is a New American restaurant ) .</sentence>
				<definiendum id="0">Above</definiendum>
			</definition>
			<definition id="8">
				<sentence>We evaluated SPaRKy on the test sets by comparing three data points for each text plan : HUMAN ( the score of the top-ranked sentence plan ) ; SPARKY ( the score of the SPR’s selected sentence ) ; and RANDOM ( the score of a sentence plan randomly selected from the alternate sentence plans ) .</sentence>
				<definiendum id="0">HUMAN</definiendum>
				<definiendum id="1">SPARKY</definiendum>
				<definiendum id="2">RANDOM</definiendum>
				<definiens id="0">the score of the SPR’s selected sentence</definiens>
			</definition>
			<definition id="9">
				<sentence>Da Andrea is an Italian restaurant , with very good service , it has good decor , and its price is 28 dollars .</sentence>
				<definiendum id="0">Da Andrea</definiendum>
				<definiens id="0">an Italian restaurant , with very good service</definiens>
			</definition>
			<definition id="10">
				<sentence>John’s Pizzeria is an Italian , Pizza restaurant .</sentence>
				<definiendum id="0">John’s Pizzeria</definiendum>
				<definiens id="0">an Italian , Pizza restaurant</definiens>
			</definition>
			<definition id="11">
				<sentence>Uguale is a French , Italian restaurant , with very good service .</sentence>
				<definiendum id="0">Uguale</definiendum>
				<definiens id="0">a French , Italian restaurant , with very good service</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>Spoken dialog systems have traditionally been difficult to instantiate because of the extensive efforts required for configuring each component from the natural language ( NL ) understanding to the domainspecific context tracking and dialog engines .</sentence>
				<definiendum id="0">Spoken dialog systems</definiendum>
				<definiens id="0">difficult to instantiate because of the extensive efforts required for configuring each component from the natural language ( NL ) understanding to the domainspecific context tracking and dialog engines</definiens>
			</definition>
			<definition id="1">
				<sentence>User : A restaurant in Dorchester .</sentence>
				<definiendum id="0">User</definiendum>
				<definiens id="0">A restaurant in Dorchester</definiens>
			</definition>
			<definition id="2">
				<sentence>Hence , the usual development process consists of multiple iterations of expensive data collections and incremental system improvements .</sentence>
				<definiendum id="0">usual development process</definiendum>
				<definiens id="0">consists of multiple iterations of expensive data collections and incremental system improvements</definiens>
			</definition>
			<definition id="3">
				<sentence>Following this , Genesis provides a summary of the characteristics of the data set , utilizing context information provided by the dialog manager and the frequency statistics .</sentence>
				<definiendum id="0">Genesis</definiendum>
			</definition>
			<definition id="4">
				<sentence>A speech recognizer ( Glass , 2003 ) is built using the utterances produced by the text mode as training data for the language model .</sentence>
				<definiendum id="0">speech recognizer</definiendum>
				<definiens id="0">built using the utterances produced by the text mode as training data for the language model</definiens>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>Coherence is the property of a good human-authored text that makes it easier to read and understand than a randomlyordered collection of sentences .</sentence>
				<definiendum id="0">Coherence</definiendum>
				<definiens id="0">the property of a good human-authored text that makes it easier to read and understand than a randomlyordered collection of sentences</definiens>
			</definition>
			<definition id="1">
				<sentence>Intuitively , cT ( un|u1 , ... , ud−1 ) is the cost of the transition ( ud−1 , ud ) given that the immediately preceding units were u1 , ... , ud−2 .</sentence>
				<definiendum id="0">cT</definiendum>
				<definiens id="0">the cost of the transition ( ud−1 , ud ) given that the immediately preceding units were u1 , ...</definiens>
			</definition>
			<definition id="2">
				<sentence>Intuitively , cI ( u1 , ... , ud ) is the cost for the fact that the discourse starts with the sequence u1 , ... , ud .</sentence>
				<definiendum id="0">cI ( u1 , ... , ud )</definiendum>
				<definiens id="0">the cost for the fact that the discourse starts with the sequence u1 , ... , ud</definiens>
			</definition>
			<definition id="3">
				<sentence>The d-place discourse ordering problem is defined as follows : Given a set U = { u1 , ... , un } , a d-place transition cost function cT and a ( d − 1 ) place initial cost function cI , compute a permutation pi of { 1 , ... , n } such that cI ( upi ( 1 ) , ... , upi ( d−1 ) ) + n−d+1summationdisplay i=1 cT ( upi ( i+d−1 ) |upi ( i ) , ... , upi ( i+d−2 ) ) is minimal .</sentence>
				<definiendum id="0">d-place discourse ordering problem</definiendum>
			</definition>
			<definition id="4">
				<sentence>The backward-looking centre Cb ( ui ) of ui is defined as the highest ranked element of Cf ( ui ) which also appears in Cf ( ui−1 ) , and serves as the link between the two subsequent utterances ui−1 and ui .</sentence>
				<definiendum id="0">backward-looking centre Cb ( ui</definiendum>
				<definiendum id="1">ui</definiendum>
				<definiens id="0">the highest ranked element of Cf ( ui ) which also appears in Cf ( ui−1 ) , and serves as the link between the two subsequent utterances ui−1 and ui</definiens>
			</definition>
			<definition id="5">
				<sentence>d-place discourse ordering problem ( dPDOP ) : Given a directed graph G = ( V , E ) , a node s ∈ V and a function c : V d → a82 , compute a simple directed path P = ( s = v0 , v1 , ... , vn ) from s through all vertices in V which minimises summationtextn−d+1i=0 c ( vi , vi+1 , ... , vi+d−1 ) .</sentence>
				<definiendum id="0">dPDOP</definiendum>
				<definiens id="0">a node s ∈ V and a function c : V d → a82 , compute a simple directed path P = ( s = v0 , v1 , ... , vn ) from s through all vertices in V which minimises summationtextn−d+1i=0 c</definiens>
			</definition>
			<definition id="6">
				<sentence>Generalised asymmetric TSP ( GATSP ) : Given a directed graph G = ( V , E ) , edge weights c : E → a82 , and a partition ( V1 , ... , Vk ) of the nodes V , compute the shortest directed cycle that visits exactly one node of each Vi .</sentence>
				<definiendum id="0">Generalised asymmetric TSP</definiendum>
			</definition>
			<definition id="7">
				<sentence>The reduction is approximation preserving , i.e. if we can find a solution of 2PDOP that is worse than the optimum only by a factor of epsilon1 ( an epsilon1-approximation ) , it translates to a solution of ATSP that is also an epsilon1-approximation .</sentence>
				<definiendum id="0">reduction</definiendum>
				<definiens id="0">an epsilon1-approximation</definiens>
			</definition>
			<definition id="8">
				<sentence>Integer linear programs consist of a set of linear equations and inequalities , and are solved by integer variable assignments which maximise or minimise a goal function while satisfying the other conditions .</sentence>
				<definiendum id="0">Integer linear programs</definiendum>
				<definiens id="0">consist of a set of linear equations and inequalities</definiens>
			</definition>
</paper>

		<paper id="2007">
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>In IR ( information retrieval ) , we find some efforts going ( under the name of distributed IR or meta-search ) to selectively fuse outputs from multiple search engines on the Internet ( Callan et al. , 2003 ) .</sentence>
				<definiendum id="0">information retrieval</definiendum>
			</definition>
			<definition id="1">
				<sentence>FLM dictates that the quality of j as a translation of e be determined by : FLM ( e , j ) = logPl ( j ) ( 1 ) Pl ( j ) is the probability of j under a particular language model ( LM ) l.1 What FLM says is that the quality of a translation essentially depends on its log likelihood ( or fluency ) and has nothing to do with what it is a translation of .</sentence>
				<definiendum id="0">FLM</definiendum>
				<definiens id="0">the quality of j as a translation of e be determined by</definiens>
				<definiens id="1">the probability of j under a particular language model ( LM ) l.1 What FLM says is that the quality of a translation essentially depends on its log likelihood ( or fluency ) and has nothing to do with what it is a translation of</definiens>
			</definition>
			<definition id="2">
				<sentence>ALM does this by using alignment models from the statistical machine translation literature ( Brown et al. , 1993 ) .</sentence>
				<definiendum id="0">ALM</definiendum>
			</definition>
			<definition id="3">
				<sentence>ALM ( e , j ) = logPl ( j ) Q ( e | j ) Q ( e | j ) is the probability estimated using IBM Model 1 .</sentence>
				<definiendum id="0">ALM</definiendum>
				<definiens id="0">the probability estimated using IBM Model 1</definiens>
			</definition>
			<definition id="4">
				<sentence>SVR is a multi-dimensional regressor , and works pretty much like its enormously popular counterpart , Support Vector classification , except that we are going to work with real numbers for target values and construct the margin , using Vapnik’s epsilon1-insensitive loss function ( Sch¨olkopf et al. , 1998 ) .</sentence>
				<definiendum id="0">SVR</definiendum>
				<definiens id="0">a multi-dimensional regressor , and works pretty much like its enormously popular counterpart</definiens>
			</definition>
			<definition id="5">
				<sentence>With confidence models in place , define a MEMT model Ψ by : Ψ ( e , J , l ) = arg maxj∈J ( θ ( e , j | l ) ) Here e represents a source sentence , J a set of translations for e generated by OTSs , and θ denotes some confidence model under an LM l. Throughout the rest of the paper , we let FLMψ and ALMψ denote MEMT systems based on FLM and ALM , respectively , and similarly for others .</sentence>
				<definiendum id="0">θ</definiendum>
				<definiens id="0">a source sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>Here ψm represents some MEMT system , ψm ( e ) denotes a particular translation ψm chooses for sentence e , i.e. , ψm ( e ) = Ψ ( e , J , l ) .</sentence>
				<definiendum id="0">ψm</definiendum>
				<definiens id="0">represents some MEMT system , ψm ( e ) denotes a particular translation ψm chooses for sentence e</definiens>
			</definition>
			<definition id="7">
				<sentence>N is the number of source sentences .</sentence>
				<definiendum id="0">N</definiendum>
			</definition>
			<definition id="8">
				<sentence>δ ( · ) says that you get 1 if a particular translation the MEMT chooses for a given sentences happens to rank highest among can3For a reference translation r and a machine-generated translation t , m-precision is defined as : m-precision = NX i P v∈Sit C ( v , r ) P v∈Sit C ( v , t ) , which is nothing more than Papineni et al. ( 2002 ) ’s modified n-gram precision applied to a pair of a single reference and the associated translation .</sentence>
				<definiendum id="0">δ ( · )</definiendum>
				<definiendum id="1">2002 ) ’s</definiendum>
			</definition>
			<definition id="9">
				<sentence>d ( ψm ) gives the average ratio of the times ψm hits a right translation .</sentence>
				<definiendum id="0">d ( ψm )</definiendum>
				<definiens id="0">gives the average ratio of the times ψm hits a right translation</definiens>
			</definition>
			<definition id="10">
				<sentence>CPC is a huge set of semi-automatically aligned pairs of English and Japanese texts from a Japanese news paper which contains as many as 150,000 sentences ( Utiyama and Isahara , 2002 ) , EJP represents a relatively small parallel corpus of English/Japanese phrases ( totaling 15,187 ) for letter writing in business ( Takubo and Hashimoto , 1999 ) , PAT is a bilingual corpus of 336,971 abstracts from Japanese patents filed in 1995 , with associated translations in English ( a.k.a NTCIR-3 PATENT ) .5 LIT contains 100 Japanese literary works from the early 20th century , and NIKMAI 1,536,191 sentences compiled from several Japanese news paper sources .</sentence>
				<definiendum id="0">CPC</definiendum>
				<definiendum id="1">EJP</definiendum>
				<definiendum id="2">PAT</definiendum>
				<definiens id="0">a huge set of semi-automatically aligned pairs of English</definiens>
			</definition>
			<definition id="11">
				<sentence>V-by-M chooses a most-votedfor LM among those in L , given the set J of translations for e. MEMT ( e , S , L ) begin J = { j | j is a translation of e generated by s ∈ S. } l = V-by-M ( J , L ) jk = arg maxj∈J ( θ ( e , j | l ) ) return jk end up an LM voted for by the majority .</sentence>
				<definiendum id="0">V-by-M</definiendum>
				<definiens id="0">chooses a most-votedfor LM among those in L , given the set J of translations for e. MEMT ( e , S , L ) begin J = { j | j is a translation of e generated by s ∈ S. } l = V-by-M</definiens>
				<definiens id="1">j | l ) ) return jk end up an LM voted for by the majority</definiens>
			</definition>
			<definition id="12">
				<sentence>HFLMψ below denotes a FLMψ based on a composite LM trained over CPC , LIT , PAT , NIKMAI , and EJP .</sentence>
				<definiendum id="0">FLMψ</definiendum>
				<definiens id="0">based on a composite LM trained over CPC , LIT , PAT , NIKMAI , and EJP</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>Galaxy is a distributed , message-based , hub-and-spoke infrastructure , optimized for spoken dialogue systems .</sentence>
				<definiendum id="0">Galaxy</definiendum>
				<definiens id="0">a distributed , message-based , hub-and-spoke infrastructure , optimized for spoken dialogue systems</definiens>
			</definition>
			<definition id="1">
				<sentence>GATE is a pure Java-based architecture developed over the past eight years in the University of Sheffield Natural Language Processing group .</sentence>
				<definiendum id="0">GATE</definiendum>
				<definiens id="0">a pure Java-based architecture developed over the past eight years in the University of Sheffield Natural Language Processing group</definiens>
			</definition>
			<definition id="2">
				<sentence>ANNIE includes customizable components necessary to complete the IE task – tokenizer , gazetteer , sentence splitter , part of speech tagger and a named entity recognizer based on a powerful engine named JAPE ( Java Annotation Pattern Engine ; Cunningham et al. , 2000 ) .</sentence>
				<definiendum id="0">ANNIE</definiendum>
				<definiens id="0">includes customizable components necessary to complete the IE task – tokenizer , gazetteer , sentence splitter , part of speech tagger and a named entity recognizer based on a powerful engine named JAPE ( Java Annotation Pattern Engine</definiens>
			</definition>
			<definition id="3">
				<sentence>P ( rec ) is the previous estimate of the probability that the record is the target record .</sentence>
				<definiendum id="0">P ( rec )</definiendum>
				<definiens id="0">the previous estimate of the probability that the record is the target record</definiens>
			</definition>
			<definition id="4">
				<sentence>Task Completion Rates Call duration was found to reflect the complexity of each scenario , where complexity is defined as the number of “concepts” needed to complete each task .</sentence>
				<definiendum id="0">complexity</definiendum>
				<definiens id="0">the number of “concepts” needed to complete each task</definiens>
			</definition>
</paper>

	</volume>
