<?xml version="1.0" encoding="UTF-8"?>
	<volume id="J92">

		<paper id="2002">
			<definition id="0">
				<sentence>The Typed Feature Structure language ( TFS ) is an attempt to provide a synthesis of several key concepts stemming from unification-based grammar formalisms ( feature structure : Kay 1984 ) knowledge representation languages ( inheritance ) , and logic programming ( narrowing ) .</sentence>
				<definiendum id="0">Typed Feature Structure language</definiendum>
				<definiendum id="1">TFS</definiendum>
				<definiendum id="2">logic programming</definiendum>
				<definiens id="0">an attempt to provide a synthesis of several key concepts stemming from unification-based grammar formalisms</definiens>
			</definition>
			<definition id="1">
				<sentence>Feature structures are partially ordered according to a subsumption ordering interpreted as an ordering on the amount of conveyed information ; the combination of information is defined as the unification of feature structures .</sentence>
				<definiendum id="0">Feature structures</definiendum>
				<definiens id="0">partially ordered according to a subsumption ordering interpreted as an ordering on the amount of conveyed information</definiens>
			</definition>
			<definition id="2">
				<sentence>Referential transparency is one of the characteristic properties of declarative languages ( Stoy 1977 ) , where the meaning of each language construct is given by a few simple and general rules .</sentence>
				<definiendum id="0">Referential transparency</definiendum>
				<definiens id="0">one of the characteristic properties of declarative languages ( Stoy 1977 ) , where the meaning of each language construct is given by a few simple and general rules</definiens>
			</definition>
			<definition id="3">
				<sentence>Recursivity is an integral part of the language , giving the necessary expressive power for describing complex recursive linguistic structures .</sentence>
				<definiendum id="0">Recursivity</definiendum>
				<definiens id="0">an integral part of the language , giving the necessary expressive power for describing complex recursive linguistic structures</definiens>
			</definition>
			<definition id="4">
				<sentence>Using types to model this taxonomic hierarchy , the type symbol VP denotes the set of verb phrases , the symbol PH denotes the set of phrases , and we define VP as a subtype of I'H. This description implies that , if we know that a linguistic object is a verb phrase , we can deduce that it is a phrase .</sentence>
				<definiendum id="0">symbol PH</definiendum>
				<definiens id="0">the set of verb phrases</definiens>
				<definiens id="1">the set of phrases</definiens>
				<definiens id="2">a verb phrase</definiens>
			</definition>
			<definition id="5">
				<sentence>The answer is the empty set when the query does not satisfy the constraints defined by the network .</sentence>
				<definiendum id="0">answer</definiendum>
				<definiens id="0">the empty set when the query does not satisfy the constraints defined by the network</definiens>
			</definition>
			<definition id="6">
				<sentence>The backbone of the network is defined by a finite set of type symbols T together with a partial ordering &lt; on T : the partially ordered set ( poset ) ( T , - &lt; /The ordering _ defines the subtype relation : for A , B E T we read A &lt; B as `` A is a subtype of B. '' We call the smallest types of T the minimal types. To have a well-behaved type hierarchy , we require that ( T , _ &lt; ) be such that : • T contains the symbols T and _1_ , where T is the greatest element and _1_ is the least element of 7-.4 any two type symbols A and B of T have a greatest common lower bound written glb ( A , B } . A poset where greatest common lower bounds exist is a meet semi-lattice : we introduce a new operation A A B = glb ( A , B } , where A A B is called the meet of A and B. Since we allow the user to specify any finite poset , a technicality arises when two types do not have a unique greatest common lower bound : in that case , the set of greatest common lower bounds is interpreted disjunctively using the following powerlattice construction , which preserves the ordering and the existing meets. The poset ( T , &lt; ) is embedded in ( crowns ( T ) , C-H ) . The set crowns ( T ) is the set of all nonempty subsets of incomparable elements of T ( the `` crowns '' of T ) . These subsets are partially ordered by the Hoare ordering GH : VX , Y E crowns ( T ) , X GH Y iff Vx E X , 3y C Y such that x &lt; y. The canonical injection of T in crowns ( T ) , which takes any element x of T into the singleton { x } trivially preserves the ordering : { x } GH { y } iff x ~ y. The meet between two elements X and Y of crowns ( T ) , X ~ Y , is defined as the union of the intersection of each of the principal ideals generated by the elements of X with each of the principal ideals generated by the elements of Y and then extracting the maximal elements. It is easy to see that existing meets are preserved : { z } - { x } ~ { y } iffz=xAy. Some meets are added , as in the following example. Let ( { a , b , c , d } , _ &lt; ) be a poset where d &lt; b , d &lt; a , c &lt; b , and c &lt; a ( this poset is represented using a Hasse diagram in Figure 1 ) . The meet a A b does not exist , but { a } R { b } -~ { c , d } , and this meet is interpreted disjunctively. The join between two elements X and Y of crowns ( T ) , X U Y , is defined as the set of maximal elements of the union of X and Y. It can be shown that , equipped with R and U , ( crowns ( T ) , ~-H/is a distributive lattice. This construction is carried over to typed feature structures , with the property that this definition of the join does not lose information , contrary to the strict generalization of feature structures : this definition of 163 Computational Linguistics Volume 18 , Number 2 b { a , c , d c d { el \ [ b , c , d } \ [ d } Figure 1 A poset and the set of the principal ideals generated by the elements of the poset ordered by set inclusion. the join is appropriate to represent disjunctive information as , for example , generated by a nondeterministic computation ( see Section 2.4 ) . This powerlattice construction is completely transparent to the user , and to simplify the presentation , we will assume in the following that ( T , ~ ) is a meet semilattice. We use the attribute-value matrix ( AVM ) notation for feature structures , and we write the type symbol for each feature structure in front of the opening square bracket of the AVM. In the remainder of this section , we shall implicitly refer to some given signature ( T , &lt; , .T / where ( T , ~ , ) is a type hierarchy and ~ '' is a set of feature symbols , and we shall also assume a set of variables V. A typed feature structure t is then an expression of the form tl\ ] \ [ \ ] A ... : tn where \ [ \ ] is a variable in a set of variables V , A is a type symbol in T , fl , ... , fn ( with n _ &gt; 0 ) are features in ~- , and h , .</sentence>
				<definiendum id="0">backbone of the network</definiendum>
				<definiendum id="1">_1_</definiendum>
				<definiens id="0">the partially ordered set ( poset ) ( T , - &lt; /The ordering _ defines the subtype relation : for A , B E T we read A &lt; B as `` A is a subtype of B. '' We call the smallest types of T the minimal types. To have a well-behaved type hierarchy , we require that ( T , _ &lt; ) be such that : • T contains the symbols T and _1_ , where T is the greatest element and</definiens>
				<definiens id="1">the least element of 7-.4 any two type symbols A and B of T have a greatest common lower bound written glb ( A , B } . A poset where greatest common lower bounds exist is a meet semi-lattice : we introduce a new operation A A B = glb ( A , B } , where A A B is called the meet of A and B. Since we allow the user to specify any finite poset , a technicality arises when two types do not have a unique greatest common lower bound : in that case , the set of greatest common lower bounds is interpreted disjunctively using the following powerlattice construction , which preserves the ordering and the existing meets. The poset ( T , &lt; ) is embedded in ( crowns ( T ) , C-H ) . The set crowns ( T ) is the set of all nonempty subsets of incomparable elements of T ( the `` crowns '' of T ) . These subsets are partially ordered by the Hoare ordering GH : VX , Y E crowns ( T ) , X GH Y iff Vx E X , 3y C Y such that x &lt; y. The canonical injection of T in crowns ( T ) , which takes any element x of T into the singleton { x } trivially preserves the ordering : { x } GH { y } iff x ~ y. The meet between two elements X and Y of crowns ( T )</definiens>
				<definiens id="2">the union of the intersection of each of the principal ideals generated by the elements of X with each of the principal ideals generated by the elements of Y and then extracting the maximal elements. It is easy to see that existing meets are preserved : { z } - { x } ~ { y } iffz=xAy. Some meets are added , as in the following example. Let ( { a , b , c , d } , _ &lt; ) be a poset where d &lt; b , d &lt; a , c &lt; b , and c &lt; a ( this poset is represented using a Hasse diagram in Figure 1 ) . The meet a A b does not exist , but { a } R { b } -~ { c , d } , and this meet is interpreted disjunctively. The join between two elements X and Y of crowns ( T )</definiens>
				<definiens id="3">the set of maximal elements of the union of X and Y. It can be shown that , equipped with R and U , ( crowns ( T ) , ~-H/is a distributive lattice. This construction is carried over to typed feature structures , with the property that this definition of the join does not lose information , contrary to the strict generalization of feature structures : this definition of 163 Computational Linguistics Volume 18 , Number 2 b { a , c , d c d { el \ [ b , c , d } \ [ d } Figure 1 A poset and the set of the principal ideals generated by the elements of the poset ordered by set inclusion. the join is appropriate to represent disjunctive information as , for example , generated by a nondeterministic computation</definiens>
				<definiens id="4">a meet semilattice. We use the attribute-value matrix ( AVM ) notation for feature structures</definiens>
				<definiens id="5">a type hierarchy and ~ '' is a set of feature symbols , and we shall also assume a set of variables V. A typed feature structure t is then an expression of the form tl\ ] \ [ \ ] A ... : tn where \ [ \ ] is a variable in a set of variables V , A is a type symbol in T , fl , ... , fn ( with n _ &gt; 0 ) are features in ~- , and h ,</definiens>
			</definition>
			<definition id="7">
				<sentence>The subnetwork on the right defines a domain of lists expressed as feature structures : the set of all possible lists is defined by the type LIST , which has no associated constraints .</sentence>
				<definiendum id="0">LIST</definiendum>
				<definiens id="0">has no associated constraints</definiens>
			</definition>
			<definition id="8">
				<sentence>This type has two subtypes : • NIL is an atomic type and represents the empty list ; • CONS is a complex type and represents the set of all possible nonempty lists and defines the following constraints , ik feature structure of type 166 R6mi Zajac Inheritance and Constraint-Based Grammar Formalisms 1 : LIST 1 LIST , \ [ , o1 APPEND0\ [ 2 : \ [ \ ] \ ] LIST I APPENDli2 : ~\ ] LIST : APPEND 2 : \ [ \ ] t3 : J /3 : Figure 2 Type hierarchy for LIST and APPEND ( T and 3_ omitted ) .</sentence>
				<definiendum id="0">NIL</definiendum>
				<definiens id="0">an atomic type and represents the empty list</definiens>
			</definition>
			<definition id="9">
				<sentence>A rewrite step for a structure t is defined as follows : if u is a substructure of t at path p and u is of type A , and there exists a rewrite rule A\ [ a\ ] -- , B\ [ b\ ] such that • A\ [ a\ ] A u 3_ , and • A\ [ a\ ] A u &lt; A\ [ a\ ] then the right-hand side B\ [ b\ ] is unified with the substructure u at path p , giving a new structure t ' that is more specific than t ( Figure 4 ) .</sentence>
				<definiendum id="0">rewrite step</definiendum>
			</definition>
			<definition id="10">
				<sentence>When there are several substructures that could be rewritten , the computation rule is to choose one of the outermost ones , i.e. , one closest to the root of the feature structure ( innermost strategies are usually nonterminating ) .</sentence>
				<definiendum id="0">computation rule</definiendum>
				<definiens id="0">to choose one of the outermost ones , i.e. , one closest to the root of the feature structure ( innermost strategies are usually nonterminating )</definiens>
			</definition>
			<definition id="11">
				<sentence>In comparison , PROLOG uses a leftmost computation rule .</sentence>
				<definiendum id="0">PROLOG</definiendum>
				<definiens id="0">uses a leftmost computation rule</definiens>
			</definition>
			<definition id="12">
				<sentence>In Saint-Dizier ( 1991 ) , linear precedence rules are defined as constraints in a language based on typed feature structures and SLD-resolution , which is used to experiment with GB theory .</sentence>
				<definiendum id="0">Saint-Dizier ( 1991</definiendum>
				<definiendum id="1">linear precedence rules</definiendum>
			</definition>
			<definition id="13">
				<sentence>In a similar way , HEAD-FP encodes the HPSG Head Feature Principle and SEM-FP encodes the Semantics Principle .</sentence>
				<definiendum id="0">HEAD-FP</definiendum>
				<definiens id="0">encodes the HPSG Head Feature Principle and SEM-FP encodes the Semantics Principle</definiens>
			</definition>
			<definition id="14">
				<sentence>The definition of a phrase recursively relates subphrases and substrings , and defines the phrase as a composition of subphrases and the string as the concatenation of substrings .</sentence>
				<definiendum id="0">definition of a phrase</definiendum>
				<definiens id="0">relates subphrases and substrings , and defines the phrase as a composition of subphrases and the string as the concatenation of substrings</definiens>
			</definition>
			<definition id="15">
				<sentence>176 R6mi Zajac Inheritance and Constraint-Based Grammar Formalisms The TFS system is implemented using rewriting techniques in a constraint-based architecture adapted to feature structures : • The language is a logical language directly based on typed feature structures , and supports an object-oriented style based on multiple inheritance .</sentence>
				<definiendum id="0">language</definiendum>
				<definiens id="0">a logical language directly based on typed feature structures</definiens>
			</definition>
			<definition id="16">
				<sentence>• X* denotes the free iteration of element X ( zero , one or more occurrences ) .</sentence>
				<definiendum id="0">• X*</definiendum>
			</definition>
			<definition id="17">
				<sentence>• X + denotes the iteration of element X ( one or more occurrences ) .</sentence>
				<definiendum id="0">X +</definiendum>
			</definition>
</paper>

		<paper id="4005">
			<definition id="0">
				<sentence>2 Solutions The source of the bottom-up path building problem is the active edge E1 above , whose ~g+ } path is of indeterminate length .</sentence>
				<definiendum id="0">path</definiendum>
				<definiens id="0">the active edge E1 above , whose ~g+ }</definiens>
			</definition>
			<definition id="1">
				<sentence>G4 does not , however , have the termination problem of G2 : ( G4 ) Rule 'base case ' S -- &gt; T : Rule 'recursive clause ' To -* TI : Rule 'copy path '' T-+A : Rule 'lexical insertion a ' A-*a : Rule 'lexical insertion b ' A-+b : ( Tg ) = e. ( Tog ) = ( Ylgg ) .</sentence>
				<definiendum id="0">A-*a</definiendum>
				<definiens id="0">Rule 'lexical insertion a '</definiens>
				<definiens id="1">Rule 'lexical insertion b ' A-+b : ( Tg ) = e. ( Tog ) = ( Ylgg )</definiens>
			</definition>
			<definition id="2">
				<sentence>Mathematically , if P is some finite set of paths , one can define a positive restrictor q~+ to be the relation defined by : p @ +l iff 3 q E P , p~q where p &lt; q means that p is a ( perhaps improper ) prefix to the path q. By contrast , the negative restriction regime proposed here allows paths not explicitly disallowed : p~pliff ( 3rEP~ r &lt; p ) =~qEP , pGq ( 1 ) In other words , whereas the positive restrictor admits those paths obeying the condition `` is a prefix of a member of the restrictor set , '' the negative restrictor only applies 529 Computational Linguistics Volume 18 , Number 4 this condition to paths extending members of the restrictor set .</sentence>
				<definiendum id="0">q</definiendum>
				<definiens id="0">some finite set of paths , one can define a positive restrictor q~+ to be the relation defined by : p @ +l iff 3 q E P , p~q where p &lt;</definiens>
				<definiens id="1">a prefix of a member of the restrictor set</definiens>
			</definition>
</paper>

		<paper id="3002">
			<definition id="0">
				<sentence>The phenomena involved in the analysis of the easy adjective class illustrate ( obligatory and optional ) subcategorization , control , long-distance dependence , optional modification , and specification ( the last in its interaction with adjectival gradation with too and enough ) .</sentence>
				<definiendum id="0">specification</definiendum>
				<definiens id="0">the last in its interaction with adjectival gradation with too and enough</definiens>
			</definition>
			<definition id="1">
				<sentence>We emphasize that HPSG is found very close to the lexical extreme , because this highlights the significance of the present work -- HPSG is a framework whose lexical demands are very nearly maximal .</sentence>
				<definiendum id="0">HPSG</definiendum>
				<definiens id="0">a framework whose lexical demands are very nearly maximal</definiens>
			</definition>
			<definition id="2">
				<sentence>3 We assume familiarity with the mechanisms of lexical inheritance and lexical rules in the analysis to follow , but we provide an overview of these mechanisms for lexical during the past several years ; we know of implementations at Hewlett-Packard Laboratories , The German AI Center ( DFKI ) , Stanford University , Carnegie Mellon University , The Ohio State University , Simon Fraser University , University of Edinburgh , ICOT , University of Stuttgart , the IBM LILOG project in Stuttgart , and ATR .</sentence>
				<definiendum id="0">German AI Center</definiendum>
				<definiens id="0">the IBM LILOG project in Stuttgart , and ATR</definiens>
			</definition>
			<definition id="3">
				<sentence>In the word class hierarchy we assume , sketched in Appendix A , there is a word class CONTROL , which introduces a verbal complement subcategorization , and which serves as the superclass from which both of the classes IT-EASY and SLASH-EASY inherit .</sentence>
				<definiendum id="0">CONTROL</definiendum>
				<definiens id="0">introduces a verbal complement subcategorization</definiens>
			</definition>
			<definition id="4">
				<sentence>Compare Tom is a linguist .</sentence>
				<definiendum id="0">Compare Tom</definiendum>
				<definiens id="0">a linguist</definiens>
			</definition>
			<definition id="5">
				<sentence>It is this property , shared by the NPs they give rise to , that explains ( i ) their ability to be controlled , e.g. , by the verb be -- only unsaturated phrases are subject to control ; ( ii ) their inability to function in normal NPs , e.g. , in the subject position of any intransitive verb ; and finally ( iii ) the fact that they can stand in construction with the main verb be without being asserted to be identical to its subject .</sentence>
				<definiendum id="0">e.g. ,</definiendum>
				<definiens id="0">in the subject position of any intransitive verb</definiens>
			</definition>
			<definition id="6">
				<sentence>c. For us , Nureyev is a real pleasure to watch .</sentence>
				<definiendum id="0">Nureyev</definiendum>
				<definiens id="0">a real pleasure to watch</definiens>
			</definition>
			<definition id="7">
				<sentence>We will focus on explaining the grammaticality of ( 31 ) , assuming that the right syntactic structure for the sentence is the binary-branching structure given in ( 34 ) , where easy forms a constituent with man , and where to talk to is sister to the phrase easy man .</sentence>
				<definiendum id="0">easy</definiendum>
				<definiens id="0">forms a constituent with man , and where to talk to is sister to the phrase easy man</definiens>
			</definition>
			<definition id="8">
				<sentence>Mary is a person to underestimate .</sentence>
				<definiendum id="0">Mary</definiendum>
				<definiens id="0">a person to underestimate</definiens>
			</definition>
			<definition id="9">
				<sentence>A translation step converts these to sets ( or -- with more information -- lists ) for use in the feature system .</sentence>
				<definiendum id="0">translation step</definiendum>
				<definiens id="0">converts these to sets ( or -- with more information -- lists ) for use in the feature system</definiens>
			</definition>
			<definition id="10">
				<sentence>We employ HPSG in the grammatical analysis presented here because it is a useful grammatical framework , and because it makes strenuous lexical demands ; but the lexicon framework does not presuppose HPSG ( for example , PATR-II systems can make use of structured lexicons , and nearly do , in the form of templates .</sentence>
				<definiendum id="0">PATR-II</definiendum>
				<definiens id="0">systems can make use of structured lexicons , and nearly do , in the form of templates</definiens>
			</definition>
			<definition id="11">
				<sentence>303 Computational Linguistics Volume 18 , Number 3 As the hierarchy of word classes sketched above indicates , this class inherits from the INCOMPLETE class ( which specifies an obligatory subject complement ) , and introduces a second obligatory complement that is a verb phrase ( not complete , which would be its maximal projection , a sentence ; and not lexical , which would be just a verb without any complements ) .</sentence>
				<definiendum id="0">INCOMPLETE class</definiendum>
				<definiens id="0">specifies an obligatory subject complement</definiens>
				<definiens id="1">a verb phrase</definiens>
			</definition>
			<definition id="12">
				<sentence>INCOMPLETE Superclasses Complementation Complements Subject Subject-Features ( Complete+ ) ( Category Noun ) Subject-Role Source TRANSITWE Superclasses Incomplete Complements Object Object-Features ( Complete+ ) ( Case Accusative ) ( Category Noun ) Object-Role Theme Thus INCOMPLETE assigns its subject the role source ; TRANSITIVE inherits this role assignment and extends it by assigning theme to objects .</sentence>
				<definiendum id="0">INCOMPLETE Superclasses Complementation Complements Subject Subject-Features ( Complete+ )</definiendum>
				<definiendum id="1">TRANSITIVE</definiendum>
				<definiens id="0">this role assignment and extends it by assigning theme to objects</definiens>
			</definition>
			<definition id="13">
				<sentence>( 48 ) REL-EASY Superclasses Features Complements XComp-features Control , Adjective ( Predicative - ) ( VForm Infinitival ) ( REL ( Category Noun ) ( Complete + ) ( NForm Normal ) ( Predicative - ) ( Case Accusative Dative ) ) ( 49 ) a. *Bill is easy for whom to work .</sentence>
				<definiendum id="0">REL</definiendum>
				<definiens id="0">easy for whom to work</definiens>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>( ~ ) 1992 Association for Computational Linguistics Computational Linguistics Volume 18 , Number 2 VERB &lt; category &gt; = verb &lt; past participle &gt; ~ /e d/ I &lt; transitive &gt; : yes &lt; transit|ve &gt; = no &lt; form = /1 o v el &lt; form &gt; = /h a t e/ &lt; ferm &gt; = /e t a p s e/ &lt; form = /e x p i r e/ Figure 1 Monotonic single inheritance. from the entries for elapse and expire and replace it with a notation that points to your list of intransitive verb properties. As you inspect your handiwork , you notice that the lists of properties associated with TRANSITIVE VERB and INTRANSITIVE VERB now exhibit exactly the kind of duplication that you first saw when you wrote down your entries for love and hate. Indeed , the number of their commonalities exceeds the number of their differences. Once again you decide to invoke the style of solution that you have used before : you collect the common properties together , give the collection the name VERB and then rework your formulation of TRANSITIVE VERB and INTRANSITIVE VERB so as to strip the shared material and replace it with a notation indicating that each is an instance of VERB. Although you may not realize it , what you have done is build an inheritance network to represent the information that you are including in your lexicon -- see Figure 1. The root node of this network is VERB and it has two daughters , TRANSITIVE VERB and INTRANSITIVE VERB , which inherit all the properties associated with the root. Each of these two nodes has further daughters ( Love , Elapse , etc. ) . The latter inherit all the properties of VERB together with all the properties of their immediate parent. These inherited properties are added to the properties listed as idiosyncratic to the lexical item itself ( e.g. , the property of being orthographically represented as /1 o v e/ ) . This very simple lexical network has a couple of characteristics that it is worth drawing attention to. Firstly , each node has a single parent , and there is thus only one path through which properties may be inherited. A network of this kind either consists of a single tree of nodes , or of a set of ( unconnected ) trees of nodes , and we will call such a network a single inheritance network. 1 Secondly , in describing our example , we have been assuming that each node inherits all the properties associated unconnected trees can always be trivially converted into an equivalent single tree by adding a new root for all the trees , but one that has no properties associated with it. 206 Walter Daelemans et al. Inheritance in Natural Language Processing with its parent node which , in logician 's parlance , means that property inheritance is monotonic. Neither single inheritance nor monotonicity is a necessary characteristic of inheritance networks. Suppose you try to add Beat to the network we have been describing. The obvious thing to do is to insert it as a daughter of TRANSITIVE VERB. But this is likely to entail that your network will claim that the past participle is *beated. One potential solution to this problem would be to define a node called EN TRANSITIVE VERB and attach Beat as a daughter to this. However , this strategy simply pushes the problem further up the inheritance tree : EN TRANSITIVE VERB can not be a daughter of the TRANSITIVE VERB node since it contains a property ( past participle = /e n/ ) that is inconsistent with a property associated with the latter ( past participle = /e d/ ) . Nor can our new node be attached as a daughter of VERB , for exactly the same reason. It seems , therefore , as if the new node may have to be defined wholly from scratch , duplicating all but one of the properties of TRANSITIVE VERB. To avoid this disagreeable conclusion , we might consider another potential solution in which we remove any reference to the past participle suffix at the level of the VERB node , and specify it instead at the level of that node 's daughters. At first sight , this appears to be a most attractive option. In fact , by adopting it , we have embarked on a slippery slope that will result in our stripping VERB of almost all the properties canonically associated with verbs. For each property you might expect it to have , if there is a single verb in English that is exceptional with respect to that property , then the property can not appear at the VERB node. In the case of morphological properties , this is likely to mean that `` present participle =/i n g/ '' is the only property that can be associated with the VERB node. And , in the case of syntactic properties , it is likely to mean that banalities such as `` category = verb '' will be all we are able to list. How are we to avoid these rather dismal alternatives ? There are ( at least ) two possibilities. One is to abandon single inheritance. Suppose we reorganize our network so that TRANSITIVE VERB and INTRANSITIVE VERB only encode syntactic properties of verbs. We then introduce two further nodes , ED VERB and EN VERB , which only encode morphological properties. Then we allow Beat to have both TRANSITIVE VERB and EN VERB as its parents. A network of this kind can no longer be represented as a tree ( or set of unconnected trees ) and is said to employ multiple inheritance-see Figure 2. Another possibility is to abandon monotonicity. We leave Beat where we first attached it , under TRANSITIVE VERB in our original network , and we associate the property `` past participle =/e n/ '' with it. If inheritance continues to be construed monotonically , then the network will make contradictory claims about the past participle of Beat. But if we adopt a nonmonotonic interpretation of inheritance , in which properties that are attached to a node take precedence over those that are inherited from a parent , then no contradiction will arise. Such nonmonotonic inheritance is known as `` default inheritance ' -- see Figure 3. Monotonic single inheritance networks are easy to build and easy to understand. If one designs a notation for defining them , then it is straightforward to say what the semantics of that notation is : translation into first order logic , for example , is quite trivial. Unfortunately , for the reasons hinted at in the example considered above , monotonic single inheritance networks are not really very well suited to the description of natural languages. As a result , as we shall see below , most researchers who have employed inheritance techniques in NLP have chosen to use either default inheritance or multiple inheritance or , very commonly , both. Networks that employ default and/or multiple inheritance are also quite easy to build , but they are much less easy to understand. The combination of default and multiple inheritance is especially problematic : `` despite a decade of study , with increasingly subtle examples and counterexamples being 207 Computational Linguistics Volume 18 , Number 2 j ER VERB &lt; past participte &gt; = /e n/ .</sentence>
				<definiendum id="0">( ~ ) 1992 Association for Computational Linguistics Computational Linguistics Volume 18</definiendum>
				<definiens id="0">category &gt; = verb &lt; past participle &gt; ~ /e d/ I &lt; transitive &gt; : yes &lt; transit|ve &gt; = no &lt; form = /1 o v el &lt; form &gt; = /h a t e/ &lt; ferm &gt; = /e t a p s e/ &lt; form = /e x p i r e/ Figure 1 Monotonic single inheritance. from the entries for elapse and expire and replace it with a notation that points to your list of intransitive verb properties. As you inspect your handiwork , you notice that the lists of properties associated with TRANSITIVE VERB and INTRANSITIVE VERB now exhibit exactly the kind of duplication that you first saw when you wrote down your entries for love and hate. Indeed , the number of their commonalities exceeds the number of their differences. Once again you decide to invoke the style of solution that you have used before : you collect the common properties together , give the collection the name VERB and then rework your formulation of TRANSITIVE VERB and INTRANSITIVE VERB so as to strip the shared material and replace it with a notation indicating that each is an instance of VERB. Although you may not realize it , what you have done is build an inheritance network to represent the information that you are including in your lexicon -- see Figure 1. The root node of this network is VERB and it has two daughters , TRANSITIVE VERB and INTRANSITIVE VERB , which inherit all the properties associated with the root. Each of these two nodes has further daughters ( Love , Elapse , etc. ) . The latter inherit all the properties of VERB together with all the properties of their immediate parent. These inherited properties are added to the properties listed as idiosyncratic to the lexical item itself ( e.g. , the property of being orthographically represented as /1 o v e/ ) . This very simple lexical network has a couple of characteristics that it is worth drawing attention to. Firstly , each node has a single parent , and there is thus only one path through which properties may be inherited. A network of this kind either consists of a single tree of nodes , or of a set of ( unconnected ) trees of nodes , and we will call such a network a single inheritance network. 1 Secondly , in describing our example , we have been assuming that each node inherits all the properties associated unconnected trees can always be trivially converted into an equivalent single tree by adding a new root for all the trees , but one that has no properties associated with it. 206 Walter Daelemans et al. Inheritance in Natural Language Processing with its parent node which , in logician 's parlance , means that property inheritance is monotonic. Neither single inheritance nor monotonicity is a necessary characteristic of inheritance networks. Suppose you try to add Beat to the network we have been describing. The obvious thing to do is to insert it as a daughter of TRANSITIVE VERB. But this is likely to entail that your network will claim that the past participle is *beated. One potential solution to this problem would be to define a node called EN TRANSITIVE VERB and attach Beat as a daughter to this. However , this strategy simply pushes the problem further up the inheritance tree : EN TRANSITIVE VERB can not be a daughter of the TRANSITIVE VERB node since it contains a property ( past participle = /e n/ ) that is inconsistent with a property associated with the latter ( past participle = /e d/ ) . Nor can our new node be attached as a daughter of VERB , for exactly the same reason. It seems , therefore , as if the new node may have to be defined wholly from scratch , duplicating all but one of the properties of TRANSITIVE VERB. To avoid this disagreeable conclusion , we might consider another potential solution in which we remove any reference to the past participle suffix at the level of the VERB node , and specify it instead at the level of that node 's daughters. At first sight , this appears to be a most attractive option. In fact , by adopting it , we have embarked on a slippery slope that will result in our stripping VERB of almost all the properties canonically associated with verbs. For each property you might expect it to have , if there is a single verb in English that is exceptional with respect to that property , then the property can not appear at the VERB node. In the case of morphological properties , this is likely to mean that `` present participle =/i n g/ '' is the only property that can be associated with the VERB node. And , in the case of syntactic properties , it is likely to mean that banalities such as `` category = verb '' will be all we are able to list. How are we to avoid these rather dismal alternatives ? There are ( at least ) two possibilities. One is to abandon single inheritance. Suppose we reorganize our network so that TRANSITIVE VERB and INTRANSITIVE VERB only encode syntactic properties of verbs. We then introduce two further nodes , ED VERB and EN VERB , which only encode morphological properties. Then we allow Beat to have both TRANSITIVE VERB and EN VERB as its parents. A network of this kind can no longer be represented as a tree ( or set of unconnected trees ) and is said to employ multiple inheritance-see Figure 2. Another possibility is to abandon monotonicity. We leave Beat where we first attached it , under TRANSITIVE VERB in our original network , and we associate the property `` past participle =/e n/ '' with it. If inheritance continues to be construed monotonically , then the network will make contradictory claims about the past participle of Beat. But if we adopt a nonmonotonic interpretation of inheritance , in which properties that are attached to a node take precedence over those that are inherited from a parent , then no contradiction will arise. Such nonmonotonic inheritance is known as `` default inheritance ' -- see Figure 3. Monotonic single inheritance networks are easy to build and easy to understand. If one designs a notation for defining them , then it is straightforward to say what the semantics of that notation is : translation into first order logic , for example , is quite trivial. Unfortunately , for the reasons hinted at in the example considered above , monotonic single inheritance networks are not really very well suited to the description of natural languages. As a result , as we shall see below , most researchers who have employed inheritance techniques in NLP have chosen to use either default inheritance or multiple inheritance or , very commonly , both. Networks that employ default and/or multiple inheritance are also quite easy to build , but they are much less easy to understand. The combination of default and multiple inheritance is especially problematic : `` despite a decade of study , with increasingly subtle examples and counterexamples being 207 Computational Linguistics Volume 18 , Number 2 j ER VERB &lt; past participte &gt; = /e n/</definiens>
			</definition>
			<definition id="1">
				<sentence>The third is the notion of `` markedness '' in linguistics , which originates in the Prague School phonology of the 1930s , reappears in the `` generative phonology '' of Chomsky and Halle ( 1968 ) and Hetzron 's ( 1975 ) and Jackendoff 's ( 1975 ) models of the lexicon , and shows up in syntax in the `` feature specification defaults '' of Gazdar , Klein , Pullum , and Sag ( 1985 ) .</sentence>
				<definiendum id="0">Sag</definiendum>
				<definiens id="0">originates in the Prague School phonology of the 1930s , reappears in the `` generative phonology '' of</definiens>
			</definition>
			<definition id="2">
				<sentence>Word Grammar is a feature-based variant of dependency grammar , one that makes pervasive use of a ( multiple ) inheritance relation .</sentence>
				<definiendum id="0">Word Grammar</definiendum>
				<definiens id="0">a feature-based variant of dependency grammar , one that makes pervasive use of a ( multiple</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , Barnett et al. ( 1990 ) use the same language ( CycL ) for linguistic semantic representation as is used in the encyclopedic inheritance network for which they are providing a natural language interface .</sentence>
				<definiendum id="0">CycL</definiendum>
				<definiens id="0">used in the encyclopedic inheritance network for which they are providing a natural language interface</definiens>
			</definition>
			<definition id="4">
				<sentence>Weischedel ( 1989 ) uses the taxonomic language NKL ( based on KL-ONE ) to express selectional restrictions , while Andry et al. ( in press ) use DATR for the same purpose .</sentence>
				<definiendum id="0">taxonomic language NKL</definiendum>
				<definiens id="0">in press ) use DATR for the same purpose</definiens>
			</definition>
			<definition id="5">
				<sentence>A semantic analog of the monotonic type hierarchies discussed above in connection with syntax is manifested in the situation theoretic `` infon lattices '' introduced by Kameyama et al. ( 1991 ) to deal with meaning mismatches in machine translation .</sentence>
				<definiendum id="0">semantic analog</definiendum>
			</definition>
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>Categorial Grammar ( CG ) has an interest in efficient and psychologically plausible , at least incremental , processing .</sentence>
				<definiendum id="0">Categorial Grammar ( CG )</definiendum>
			</definition>
			<definition id="1">
				<sentence>Windowing is a technique for incrementally accessing the hierarchical lexicon .</sentence>
				<definiendum id="0">Windowing</definiendum>
				<definiens id="0">a technique for incrementally accessing the hierarchical lexicon</definiens>
			</definition>
			<definition id="2">
				<sentence>Various approaches have been proposed for deduction in L. In its standard representation the L-calculus is a sequent calculus .</sentence>
				<definiendum id="0">L-calculus</definiendum>
				<definiens id="0">a sequent calculus</definiens>
			</definition>
			<definition id="3">
				<sentence>The elimination of a type constructor is denoted by E ; introduction by I. Definition 1 ( Lambek , sequent calculus ) U , ( ( X/ ( W , g , ,b ) ) gb , a ) , T , V = &gt; Z if T : = &gt; ( W , g. , ,b ) and U , ( X , qS+~ , a ( b ) ) , V : = &gt; Z \ [ /El U , T , ( ( ( W , g. , ,b ) \X ) , Ga ) , V : = &gt; Z if T : = &gt; ( W , g. , ,b ) and U , ( X , ~+gS , a ( b ) ) , V : = &gt; Z \ [ \E\ ] T =~ ( ( X/ ( W , e , b ) ) , g , ,Ab .</sentence>
				<definiendum id="0">Lambek</definiendum>
				<definiendum id="1">T , ( (</definiendum>
				<definiendum id="2">U , ( X , ~+gS</definiendum>
				<definiens id="0">a ( b ) ) , V : = &gt; Z \ [ /El U ,</definiens>
			</definition>
			<definition id="4">
				<sentence>Example 1 ( np , john , john ) ( ( np\ s ) /np , loves , loves ) ( np , mary , mary } : :~ ( s , john+loves+mary , loves ( mary ) ( john ) ) if { np , mary , mary ) =~ { np , mary , mary ) and ( np , john , john ) ( np\s , loves+mary , loves ( mary ) } =~ ( s , john+loves+mary , loves ( mary ) ( john ) ) if ( np , john , john ) =~ ( np , john , john ) and ( s , john+loves+mary , loves ( mary ) ( john ) ) =~ ( s , john+loves+mary , loves ( mary ) ( john ) ) \ [ /El \ ] Axiom\ ] \ [ \ El \ [ Axiom\ ] \ [ Axiom\ ] The product-free version of the Lambek calculus includes two connectives , / and \ .</sentence>
				<definiendum id="0">john+loves+mary , loves ( mary )</definiendum>
				<definiendum id="1">mary )</definiendum>
				<definiendum id="2">loves ( mary )</definiendum>
				<definiens id="0">Axiom\ ] \ [ \ El \ [ Axiom\ ] \ [ Axiom\ ] The product-free version of the Lambek calculus includes two connectives</definiens>
			</definition>
			<definition id="5">
				<sentence>XTY denotes a category X that has an argument of category Y missing somewhere within X. The constituent John put on the table in what John put on the table has as its syntactic category s T np .</sentence>
				<definiendum id="0">XTY</definiendum>
			</definition>
			<definition id="6">
				<sentence>The A-connective ( Morrill 1990 ) is one of a set of boolean connectives that can be used to denote that a certain lexical item can occur in different categories : square can be n/n and n , and is therefore assigned the category ( n/n ) An .</sentence>
				<definiendum id="0">A-connective</definiendum>
				<definiens id="0">one of a set of boolean connectives that can be used to denote that a certain lexical item can occur in different categories : square can be n/n and n , and is therefore assigned the category ( n/n ) An</definiens>
			</definition>
			<definition id="7">
				<sentence>The verb itself does not specify prosodic information for the argument and the idiom is a specialization of the verb because it does specify prosodic information .</sentence>
				<definiendum id="0">idiom</definiendum>
				<definiens id="0">a specialization of the verb because it does specify prosodic information</definiens>
			</definition>
			<definition id="8">
				<sentence>Example 3 a. KICK : ( KICK_TV ~KICK_THE_BUCKET , kick , AxAykick ( x ) ( y ) ) b. KICK_TV : ( ( np\s ) /np , _ , _ } c. KICK_THE_BUCKET : ( _/ ( _ , the + bucket , _ ) , _ , &amp; x &amp; ydie ( y ) ) similar to priority union ( Kaplan 1987 , p. 180 ) or default unification ( Bouma 1990 ) , denoted by n. N is defined as a function from pairs of mother and daughter signs to fully specified daughter signs and runs as follows .</sentence>
				<definiendum id="0">AxAykick</definiendum>
				<definiens id="0">( x ) ( y ) ) b. KICK_TV : ( ( np\s ) /np , _ , _ } c. KICK_THE_BUCKET : ( _/ ( _ , the + bucket , _ ) , _ , &amp; x &amp; ydie ( y ) ) similar to priority union ( Kaplan 1987 , p. 180 ) or default unification</definiens>
			</definition>
			<definition id="9">
				<sentence>Daelemans ( 1987 ) and De Smedt ( 1990 ) show that in a hierarchical lexicon structure , blocking is equivalent to the prevalence of more specific information over more general information .</sentence>
				<definiendum id="0">Daelemans</definiendum>
				<definiens id="0">equivalent to the prevalence of more specific information over more general information</definiens>
			</definition>
			<definition id="10">
				<sentence>np ) ) would imply that kick is in principle an intransitive verb , that has one optional argument , whereas in fact the reverse is true : kick is a two-placefunctor of which one argument may be left unspecified syntactically .</sentence>
				<definiendum id="0">kick</definiendum>
				<definiens id="0">a two-placefunctor of which one argument may be left unspecified syntactically</definiens>
			</definition>
			<definition id="11">
				<sentence>kick ( the ( y ) ) ( john ) ) ( n , boy ) : :~ ( s , kicks ( the ( boy ) ) ( john ) ) \ ] /El if ( n , boy ) =~ ( n , boy ) \ [ Axiom\ ] and ( s , kicks ( the ( boy ) ) ( john ) ) = : ~ ( s , kicks ( the ( boy ) ) ( john ) ) \ ] Axiom\ [ All words , also the function words such as the are in principle processed and thus interpreted immediately ; that is , their semantic representation is accessed from the lexicon and combined with the semantic representation of the input so far .</sentence>
				<definiendum id="0">kick</definiendum>
				<definiens id="0">the are in principle processed and thus interpreted immediately</definiens>
			</definition>
			<definition id="12">
				<sentence>b ( a ) if ( ZAb , c ) , ( W , _ , d ) 3 '' ( Y , _ , a ) \ [ M3/I ( ZAb , c ) , ( Y\X , Gb ) o* ( W\X , qo+G ) ~d.b ( a ) ) if ( W , _ , d ) , ( ZAb , c ) 3 '' ( Y , _ , a ) \ [ M3\\ ] U , ( X , Ga ) , ( YAb , b ) , V O ( Z ) if ( X , Ga ) , ( Y , ~b , b ) ~* ( Cut , x , c ) and U , ( Cut , x , c ) , V o ( Z ) \ [ M-Cut\ ] M is also capable of processing a sentence in an incremental fashion , as each word is added to the semantic structure as it is encountered .</sentence>
				<definiendum id="0">V O</definiendum>
				<definiens id="0">Cut , x , c ) , V o ( Z ) \ [ M-Cut\ ] M is also capable of processing a sentence in an incremental fashion , as each word is added to the semantic structure as it is encountered</definiens>
			</definition>
</paper>

		<paper id="4004">
			<definition id="0">
				<sentence>( ~ ) 1992 Association for Computational Linguistics Computational Linguistics Volume 18 , Number 4 TAG is defined as a tree rewriting system .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">a tree rewriting system</definiens>
			</definition>
			<definition id="1">
				<sentence>In the definition given traditionally , a TAG is defined by a finite set of trees and an operation called adjoining to compose trees .</sentence>
				<definiendum id="0">TAG</definiendum>
			</definition>
			<definition id="2">
				<sentence>A Tree Adjoining Grammar ( TAG ) as defined traditionally is said to be specified by a finite set of elementary trees .</sentence>
				<definiendum id="0">Tree Adjoining Grammar ( TAG</definiendum>
			</definition>
			<definition id="3">
				<sentence>Adjunction is thus defined as a pair of simultaneous substitutions .</sentence>
				<definiendum id="0">Adjunction</definiendum>
			</definition>
			<definition id="4">
				<sentence>Figure 8 shows the result of adjoining at the paired VP quasi-nodes in c~4 by the auxiliary quasi-trees t3 ( resulting in q/4 ) and t6 ( 75 ) '' The substitution operation used in TAG is the same as that used in context-free grammars ( CFGs ) , where one considers a CFG as a tree-rewriting formalism rather than a string-rewriting formalism .</sentence>
				<definiendum id="0">( CFGs</definiendum>
				<definiens id="0">adjoining at the paired VP quasi-nodes in c~4 by the auxiliary quasi-trees t3 ( resulting in q/4 ) and t6 ( 75 ) '' The substitution operation used in TAG is the same as that used in context-free grammars</definiens>
			</definition>
			<definition id="5">
				<sentence>In this case , given two trees , the substitution operation can be defined as the tree obtained by identifying the root node of one tree with the target 491 Computational Linguistics Volume 18 , Number 4 E foot-1 Figure 9 The substitution operation .</sentence>
				<definiendum id="0">substitution operation</definiendum>
				<definiens id="0">the tree obtained by identifying the root node of one tree with the target</definiens>
			</definition>
			<definition id="6">
				<sentence>Recall that an SA constraint of a node lists a subset of auxiliary trees that can be adjoined at this node .</sentence>
				<definiendum id="0">SA constraint of a node</definiendum>
				<definiens id="0">lists a subset of auxiliary trees that can be adjoined at this node</definiens>
			</definition>
			<definition id="7">
				<sentence>Firstly , the set of terms is defined as t : := a x l ( tl ) where a is an atomic value where x is an individual variable where 1 is a label ( or attribute ) and tl is a term .</sentence>
				<definiendum id="0">tl</definiendum>
				<definiens id="0">an atomic value where x is an individual variable where 1 is a label</definiens>
			</definition>
			<definition id="8">
				<sentence>So given a grammar , we can specify that the domination relationship is actually defined by Dom ( x , y ) 4==~ x ~ y V fll ( x~y ) V ... V -'fin ( X~y ) where { ill , ... , fin } are the quasi-auxiliary trees in the grammar .</sentence>
				<definiendum id="0">Dom (</definiendum>
				<definiens id="0">x , y ) 4==~ x ~ y V fll ( x~y ) V ... V -'fin ( X~y ) where { ill , ... , fin } are the quasi-auxiliary trees in the grammar</definiens>
			</definition>
			<definition id="9">
				<sentence>, tn are terms and P is a n-ary predicate symbol where ¢1 , ¢2 are formulae where ¢1 , ¢2 are formulae .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a n-ary predicate symbol where ¢1</definiens>
			</definition>
			<definition id="10">
				<sentence>We define a Value function as a partial function that returns the denotation of a term ( an automaton ) relative to an environment ( mapping variables to automata ) • • Valuep ( x ) = p ( x ) where x is a variable• • Valuep ( a ) = .</sentence>
				<definiendum id="0">Value function</definiendum>
				<definiendum id="1">x</definiendum>
				<definiens id="0">a partial function that returns the denotation of a term ( an automaton ) relative to an environment ( mapping variables to automata</definiens>
			</definition>
			<definition id="11">
				<sentence>G where a is an atom , where Aa is the atomic structure that corresponds to the atom a. • Valuep ( l ( t ) ) -fit~l , if fit/l is defined , where 1 is an attribute , t is a term and Valuep ( t ) = fit .</sentence>
				<definiendum id="0">Aa</definiendum>
			</definition>
			<definition id="12">
				<sentence>504 K. Vijay-Shanker Using Descriptions of Trees in a Tree Adjoining Grammar Definition ( I , p ) b ~1 A ~2 iff ( I , p ) ~ ~1 and ( I , p ) ~ $ 2 ( I , P ) V ~bl V ~b2 iff ( I , p ) ~ ~1 or ( I , p ) ~ ~b2 ( I , p ) ~ tl ~ t2 iff Valuep ( tl ) and Valuep ( t2 ) are defined and Valuep ( tl ) = Valuep ( t2 ) ( I , p ) ~ P ( h , ... , tn ) iff Valuep ( ti ) is defined ( 1 &lt; i &lt; n ) and ( Valuep ( tl ) , ... , Valuep ( t , ) ) E I ( P ) .</sentence>
				<definiendum id="0">... , tn ) iff Valuep</definiendum>
				<definiens id="0">a Tree Adjoining Grammar Definition ( I , p ) b ~1 A ~2 iff ( I , p</definiens>
				<definiens id="1">tl ) = Valuep ( t2 ) ( I , p ) ~ P ( h ,</definiens>
			</definition>
			<definition id="13">
				<sentence>We define the transformation function , To , such that given an interpretation , / , Tc returns an interpretation TG ( I ) given by Definition For all substitutions , p , where Valuep ( ti , j ) is defined for 1 &lt; _ j &lt; _ ni , ( Valuen ( tia ) , ... , Valuep ( ti , nl ) ) E TG ( I ) ( Pi ) iff ( I , p ) ~ ¢5i .</sentence>
				<definiendum id="0">Tc</definiendum>
				<definiendum id="1">Valuep</definiendum>
				<definiens id="0">returns an interpretation TG ( I ) given by Definition For all substitutions , p</definiens>
			</definition>
			<definition id="14">
				<sentence>Since Tc is continuous , the least fixed point of T6 can be obtained as U Ti ( I.a_ ) i~o where I± is the least interpretation function and is given by I_L ( P ) the empty set for all predicate symbols P. Let Ic be the fixed point of TG .</sentence>
				<definiendum id="0">I±</definiendum>
				<definiens id="0">the empty set for all predicate symbols P. Let Ic be the fixed point of TG</definiens>
			</definition>
			<definition id="15">
				<sentence>Then the set of structures derived by a grammar G is given by It ( Grammar ) , where Grammar is the distinguished predicate symbol as defined earlier .</sentence>
				<definiendum id="0">Grammar</definiendum>
			</definition>
			<definition id="16">
				<sentence>We defined the adjoining operation as an operation that fits a structure in the gap between a pair of associated quasi-nodes .</sentence>
				<definiendum id="0">adjoining operation</definiendum>
				<definiens id="0">an operation that fits a structure in the gap between a pair of associated quasi-nodes</definiens>
			</definition>
			<definition id="17">
				<sentence>Multi-Component Tree Adjoining Grammar ( MCTAG ) differs from ( the traditional definition of ) TAG in that the elementary objects of the grammar are sets of trees rather than trees , and multi-component adjoining involves the composition of these elementary sets of trees 4 ( rather than elementary trees ) .</sentence>
				<definiendum id="0">Multi-Component Tree Adjoining Grammar</definiendum>
			</definition>
</paper>

		<paper id="4001">
			<definition id="0">
				<sentence>Gazdar ( 1988 ) calls attention to a subcategory of context-sensitive grammars called indexed languages and illustrates some applicability to natural languages , and Joshi illustrates an application of `` mild context-sensitivity '' ( Joshi 1987 ) , but in general , NL computation with context-sensitive grammars is a largely unexplored area .</sentence>
				<definiendum id="0">Joshi</definiendum>
				<definiens id="0">calls attention to a subcategory of context-sensitive grammars called indexed languages and illustrates some applicability to natural languages , and</definiens>
			</definition>
			<definition id="1">
				<sentence>The example in Figure 2 shows how a sentence `` Treatment is a complete rest and a special diet '' is parsed by a context sensitive shift/reduce parser .</sentence>
				<definiendum id="0">Treatment</definiendum>
				<definiens id="0">a complete rest</definiens>
			</definition>
			<definition id="2">
				<sentence>Cdg is the given CDG grammar rules .</sentence>
				<definiendum id="0">Cdg</definiendum>
				<definiens id="0">the given CDG grammar rules</definiens>
			</definition>
			<definition id="3">
				<sentence>Stack : = empty do until ( Input = empty and Stack = ( SNT ) ) Windowed-context : = Append ( Top-five ( stack ) , First-ilve ( input ) ) Operation : = Consult_CDG ( Window-context , Cdg ) if First ( Operation ) = SHIFT then Stack : = Push ( First ( Input ) , Stack ) Input : = Rest ( Input ) else Stack : = Push ( Second ( Operation ) , Pop ( Pop ( Sta~k ) ) ) end do The functions , Top_five and First-five , return the lists of top ( or first ) five elements of the Stack and the Input respectively .</sentence>
				<definiendum id="0">Top_five</definiendum>
				<definiens id="0">= empty do until ( Input = empty</definiens>
			</definition>
			<definition id="4">
				<sentence>The scoring function we use is developed as follows : Let T4 be the set of vectors { RI~R2 , ... , Rn } where Ri is the vector \ [ rl , r2 , ... , rl0\ ] Let C be the vector \ [ Cl , Ca , ... , c10\ ] Let # ( ci , ri ) be a matching function whose value is 1 if ci = ri , and 0 otherwise .</sentence>
				<definiendum id="0">Ri</definiendum>
				<definiendum id="1">ri</definiendum>
				<definiens id="0">the vector \ [ rl , r2 , ...</definiens>
			</definition>
			<definition id="5">
				<sentence>TZ is the entire set of rules , Ri is ( the left half of ) a particular rule , and C is the parse context .</sentence>
				<definiendum id="0">TZ</definiendum>
				<definiendum id="1">Ri</definiendum>
				<definiendum id="2">C</definiendum>
				<definiens id="0">the entire set of rules</definiens>
				<definiens id="1">the parse context</definiens>
			</definition>
			<definition id="6">
				<sentence>Although 345 sentences totaling 8594 words is a small selection from the infinite set of possible English sentences , it is large enough to assure us that the CDG is a reasonable form of grammar .</sentence>
				<definiendum id="0">CDG</definiendum>
			</definition>
			<definition id="7">
				<sentence>This conclusion is somewhat at variance with those AI approaches that require a syntactic analysis to be followed by a semantic operation that filters and transforms syntactic constituents to compute case-labeled propositions ( e.g. Rim 1990 ) , but it is consistent with the neural network experience of directly mapping from sentence to case structure , and with the AI research that seeks to integrate syntactic and semantic processing while translating sentences to propositional structures .</sentence>
				<definiendum id="0">semantic operation</definiendum>
				<definiens id="0">seeks to integrate syntactic and semantic processing while translating sentences to propositional structures</definiens>
			</definition>
			<definition id="8">
				<sentence>( because Ante ( ate Agt she Obj ( fish And chips ) Vmod earlier ) Conse ( was Vmod not Objl mary State hungry ) ) Verbs are subcategorized as vao , vabo , vo , va , vhav , vbe where a is agent , o is object , b is beneficiary and vhav is a form of have and vbe a form of be .</sentence>
				<definiendum id="0">vhav</definiendum>
				<definiens id="0">fish And chips ) Vmod earlier</definiens>
			</definition>
			<definition id="9">
				<sentence>For example , in `` fish were eaten by birds , '' the CS parse is as follows : * n vbe ppart by n ; shift n * vbe ppart by n ; shift n vbe * ppart by n ; shift n vbe ppart * by n ; I vbe 2 , vpasv ( eaten Vbe were ) n vpasv * by n ; I obj 2 ( eaten Vbe were Obj fish ) vpasv * by n ; shift vpasv by * n ; shift vpasv by n * ; i prep 2 ( birds Prep by ) vpasv n * ; 2 agt 1 ( eaten Vbe were Obj fish Agt ( birds Prep by ) ) Here , it was necessary to rename the combination of a past participle and its auxiliary as a passive verb , vpasv , so that the syntactic subject and object could be recognized as Obj and Agent , respectively .</sentence>
				<definiendum id="0">agt 1</definiendum>
				<definiens id="0">the combination of a past participle and its auxiliary as a passive verb , vpasv , so that the syntactic subject and object could be recognized as Obj and Agent , respectively</definiens>
			</definition>
			<definition id="10">
				<sentence>As an aid to the reader the first two 408 Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English CS-CASE-Parser ( input , cdg ) Input is a string of syntactic classes for the given sentence .</sentence>
				<definiendum id="0">cdg ) Input</definiendum>
				<definiens id="0">a string of syntactic classes for the given sentence</definiens>
			</definition>
			<definition id="11">
				<sentence>Cdg is the given CDG grammar rules .</sentence>
				<definiendum id="0">Cdg</definiendum>
				<definiens id="0">the given CDG grammar rules</definiens>
			</definition>
			<definition id="12">
				<sentence>skipping now to BECAUSE ( VAOPASV UNTIL N * BECAUSE-OF A CRUSHED ELECTRICAL ... SAID ) options-axe h b s case-trans or cr for default : ( S ) 3 2 1 ( VAOPASV * BECAUSE-OF A CRUSHED ELECTRICAL PART ... SAID ) options-axe h b s case-trans or cr for default : ( S ) ( VAOPASV BECAUSE-OF * A CRUSHED ELECTRICAL PART ON .</sentence>
				<definiendum id="0">VAOPASV</definiendum>
			</definition>
			<definition id="13">
				<sentence>Context-Dependent Grammars ( CDGs ) are computationally and conceptually tractable formalisms that can be composed easily by a linguist and effectively used by a deterministic parser to compute phrase structures and case analyses for subsets of newspaper English .</sentence>
				<definiendum id="0">Context-Dependent Grammars ( CDGs )</definiendum>
				<definiens id="0">are computationally and conceptually tractable formalisms that can be composed easily by a linguist and effectively used by a deterministic parser to compute phrase structures and case analyses for subsets of newspaper English</definiens>
			</definition>
			<definition id="14">
				<sentence>416 Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English Appendix A. Rules from the Case Acquisition Session Blanks in the 10-symbol vectors are signified by the letter B. ( ( THE LAUNCH OF DISCOVERY AND ITS FIVE ASTRONAUTS HAS BEEN DELAYED AT-LEAST TWO DAYS UNTIL MARCH ELEVENTH BECAUSE-OF A CRUSHED ELECTRICAL PART ON A MAIN ENGINE COMMA OFFICIALS SAID ) ( ART N OF N AND PPRON ADJ N VHAV VBE VAO AT-LEAST ADJ N UNTIL N N BECAUSE-OF ART PPART ADJ N ON ART N N COMMA N VAO ) ( ( ( B B B B B ART N OF N AND ) ( S ) ) ( ( B B B B ART N OF N AND PPRON ) ( S ) ) ( ( B B B ART N OF N AND PPRON ADJ ) ( I DET 2 ) ) ( ( B B B B N OF N AND PPRON ADJ ) ( S ) ) ( ( B B B N OF N AND PPRON ADJ N ) ( S ) ) ( ( B B N OF N AND PPRON ADJ N VHAV ) ( S ) ) ( ( B N OF N AND PPRON ADJ N VHAV VBE ) ( S ) ) ( ( N OF N AND PPRON ADJ N VHAV VBE VAO ) ( S ) ) ( ( OF N AND PPRON ADJ N VHAV VBE VAO AT-LEAST ) ( S ) ) ( ( N AND PPRON ADJ N VHAV VBE VAO AT-LEAST ADJ ) ( i MOD 2 ) ) ( ( OF N AND PPRON N VHAV VBE VAO AT-LEAST ADJ ) ( I POSSBY 2 ) ) ( ( N OF N AND N VHAV VBE VAO AT-LEAST ADJ ) ( 3 2 i ) ) ( ( B B N OF N VHAV VBE VAO AT-LEAST ADJ ) ( 3 2 I ) ) ( ( B B B B N VHAV VBE VAO AT-LEAST ADJ ) ( S ) ) ( ( B B B N VHAV VBE VAO AT-LEAST ADJ N ) ( S ) ) ( ( B B N VHAV VBE VAO AT-LEAST ADJ N UNTIL ) ( i AUX 2 ) ) ( ( B B B N VBE VAO AT-LEAST ADJ N UNTIL ) ( S ) ) ( ( B B N VBE VAO AT-LEAST ADJ N UNTIL N ) ( I VBE 2 VAOPASV ) ) ( ( B B B N VAOPASV AT-LEAST ADJ N UNTIL N ) ( i OBJ 2 ) ) ( ( B B B B VAOPASV AT-LEAST ADJ N UNTIL N ) ( S ) ) ( ( B B B VAOPASV AT-LEAST ADJ N UNTIL N N ) ( S ) ) ( ( B B VAOPASV AT-LEAST ADJ N UNTIL N N BECAUSE-OF ) ( S ) ) ( ( B VAOPASV AT-LEAST ADJ N UNTIL N N BECAUSE-OF ART ) ( i MOD 2 ) ) ( ( B B VAOPASV AT-LEAST N UNTIL N N BECAUSE-OF ART ) ( 3 2 i ) ) ( ( B B B B VAOPASV UNTIL N N BECAUSE-OF ART ) ( S ) ) ( ( B B B VAOPASV UNTIL N N BECAUSE-OF ART PPART ) ( S ) ) ( ( B B VAOPASV UNTIL N N BECAUSE-OF ART PPART ADJ ) ( S ) ) ( ( B VAOPASV UNTIL N N BECAUSE-OF ART PPART ADJ N ) ( i NMOD 2 ) ) ( ( B B VAOPASV UNTIL N BECAUSE-OF ART PPART ADJ N ) ( 3 2 i ) ) ( ( B B B B VAOPASV BECAUSE-OF ART PPART ADJ N ) ( S ) ) ( ( B B B VAOPASV BECAUSE-OF ART PPART ADJ N ON ) ( i CONSE 2 ) ) ( ( B B B B BECAUSE-OF ART PPART ADJ N ON ) ( S ) ) ( ( B B B BECAUSE-OF ART PPART ADJ N ON ART ) ( S ) ) ( ( B B BECAUSE-OF ART PPART ADJ N ON ART N ) ( S ) ) ( ( B BECAUSE-OF ART PPART ADJ N ON ART N N ) ( S ) ) ( ( BECAUSE-OF ART PPART ADJ N ON ART N N COMMA ) ( i MOD 2 ) ) ( ( B BECAUSE-OF ART PPART N ON ART N N COMMA ) ( i MOD 2 ) ) 417 Computational Linguistics Volume 18 , Number 4 ( ( B B BECAUSE-OF ART N ON ART N N COMMA ) ( i DET 2 ) ) ( ( B B B BECAUSE-OF N ON ART N N COMMA ) ( S ) ) ( ( B B BECAUSE-OF N ON ART N N COMMA N ) ( S ) ) ( ( B BECAUSE-OF N ON ART N N COMMA N VAO ) ( S ) ) ( ( BECAUSE-OF N ON ART N N COMMA N VAO B ) ( S ) ) ( ( N ON ART N N COMMA N VAO B B ) ( i NMOD 2 ) ) ( ( BECAUSE-OF N ON ART N COMMA N VAO B B ) ( i DET 2 ) ) ( ( B BECAUSE-OF N ON N COMMA N VAO B B ) ( 3 2 I ) ) ( ( B B B BECAUSE-OF N COMMA N VAO B B ) ( 2 ANTE i ) ) ( ( B B B B BECAUSE-OF COMMA N VAO B B ) ( S ) ) ( ( B B B BECAUSE-OF COMMA N VAO B B B ) ( S ) ) ( ( B B BECAUSE-OF COMMA N VAO B B B B ) ( S ) ) ( ( B BECAUSE-OF COMMA N VAO B B B B B ) ( i AGT 2 ) ) ( ( B B BECAUSE-OF COMMA VAO B B B B B ) ( I OBJ 3 ) ) ) ) 418</sentence>
				<definiendum id="0">MAIN ENGINE COMMA OFFICIALS SAID )</definiendum>
				<definiendum id="1">PPRON ADJ N VHAV VBE VAO AT-LEAST ADJ N UNTIL N N BECAUSE-OF ART PPART ADJ</definiendum>
				<definiendum id="2">PPRON ) ( S ) )</definiendum>
				<definiens id="0">B B B B B ART N OF N AND ) ( S ) ) ( ( B B B B ART N OF N AND</definiens>
				<definiens id="1">B B B B BECAUSE-OF ART PPART ADJ N ON ) ( S ) ) ( ( B B B BECAUSE-OF ART PPART ADJ N ON ART ) ( S ) ) ( ( B B BECAUSE-OF ART PPART ADJ N ON ART N ) ( S )</definiens>
				<definiens id="2">B B B B BECAUSE-OF COMMA N VAO B B ) ( S ) ) ( ( B B B BECAUSE-OF COMMA N VAO B B B ) ( S ) ) ( ( B B BECAUSE-OF COMMA N VAO B B B B ) ( S ) ) ( ( B BECAUSE-OF COMMA N VAO B B B B B</definiens>
				<definiens id="3">B B BECAUSE-OF COMMA VAO B B B B B )</definiens>
			</definition>
</paper>

		<paper id="3004">
			<definition id="0">
				<sentence>Let d ( u , v ) be a dissimilarity measure or simply dissimilarity between the two words as it is described below .</sentence>
				<definiendum id="0">v</definiendum>
				<definiens id="0">a dissimilarity measure or simply dissimilarity between the two words as it is described below</definiens>
			</definition>
			<definition id="1">
				<sentence>Automatic Clustering of Languages I R IS H vile olc bolg dubh JAPANESE zenbu warui hara kuroi KA N N A DA yella ketta hoatti kahri LATIN totus malus venter niger LATVIAN visi slikts veders melns LIT H UA N I A N vise blogas pilvas jaudas MACEDONIAN site los stomak crn MALAYALAM ellam cheetta vayaru karuppu MALTESE kollox trazin zaqq iswed MAORI katoa kino hoopara hiwahiwa MARAATHI sarva waeet poat kaale NORWEGIAN alle daarlig mage svart ORIYA sabu kharap peta kala PANJABI sab bura pet kala PERSIAN hame bad shekam siah POLISH wszystko zly brzuch czarny PO RTU G U ES E todo mau barriga negro RAJASTHANI sab kharab pet kalo ROMANIAN tot rau burta negru RUSSIAN vse plokhoi brjukho cjornji SANSKRIT sara bura paat kala S E R B I A N sve los trbuh crn SLOVAK vsetko zly brucho cierny SLOVENIAN vse slab trebuh crn SPA N IS H todo mal vientre negro SWA HI L I ote baya tumbo karipia SWEDISH alla daolig mage svart TAMIL ellaam keduthy vayiru karuppu T E L U G U antha chedda kadupu nalla T U R K IS H butun fena karin kara U K RAI N I A N vse pohane zhevit chorne WELSH C pawb drwg bola du ALBANIAN asht dite vdes pi AR .</sentence>
				<definiendum id="0">A N sve los</definiendum>
				<definiens id="0">E todo mau barriga negro RAJASTHANI sab kharab pet kalo ROMANIAN tot rau burta negru RUSSIAN vse plokhoi brjukho cjornji SANSKRIT sara bura paat kala S E R B I</definiens>
				<definiens id="1">keduthy vayiru karuppu T E L U G U antha chedda kadupu nalla T U R K IS H butun fena karin kara U K RAI N I A N vse pohane zhevit</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>Koskenniemi ( ,1985 ) refined the notation and sketched a compilation method , although he still did not provide a detailed declarative definition of the meaning of the high-level rules .</sentence>
				<definiendum id="0">Koskenniemi</definiendum>
				<definiens id="0">refined the notation and sketched a compilation method , although he still did not provide a detailed declarative definition of the meaning of the high-level rules</definiens>
			</definition>
			<definition id="1">
				<sentence>The second level consists of a notional automaton `` tape '' made up of any concatenation of strings from the first level ( the lexicon ) , with special null symbols inserted anywhere between the symbols of the lexical alphabet ( optionally ) .</sentence>
				<definiendum id="0">second level</definiendum>
				<definiens id="0">consists of a notional automaton `` tape '' made up of any concatenation of strings from the first level ( the lexicon ) , with special null symbols inserted anywhere between the symbols of the lexical alphabet ( optionally )</definiens>
			</definition>
			<definition id="2">
				<sentence>The fourth level , at the bottom in Figure 1 , represents the word as spelt ( or phonetically represented ) in the surface alphabet ; the relationship between the surface tape and the surface form is that the latter can be produced from the former by removing all occurrences of the special null symbol .</sentence>
				<definiendum id="0">fourth level</definiendum>
				<definiens id="0">removing all occurrences of the special null symbol</definiens>
			</definition>
			<definition id="3">
				<sentence>1 These relations bear the same relationship to finite-state transducers that regular languages bear to finitestate machines , and his formal definition is exactly analogous to that of a regular language : the empty set is a regular relation ; the set consisting of the empty string is a regular relation ; the set consisting of a single ordered pair of symbols , either of which may be the empty string , is a regular relation ; if R1 and R2 are regular relations , so are R1 U R2 , R1R2 , and R~ ( i.e. the union of the two sets , the set consisting of concatenations of elements from the two formally published details that I am aware of .</sentence>
				<definiendum id="0">R~</definiendum>
				<definiens id="0">the empty set is a regular relation ; the set consisting of the empty string is a regular relation ; the set consisting of a single ordered pair of symbols , either of which may be the empty string , is a regular relation ; if R1 and R2 are regular relations</definiens>
			</definition>
			<definition id="4">
				<sentence>A context restriction rule of the form a : b = &gt; LC ___ RC defines a regular relation that can be expressed as : IF ( zc*LC , a : bzr* ) M IF ( ( zr*a : b ) ' , ( RCzr* ) ' ) where 7r is the set of all possible symbol-pairs , and the prime denotes complementation with respect to 7r* .</sentence>
				<definiendum id="0">LC ___ RC</definiendum>
				<definiendum id="1">7r</definiendum>
				<definiens id="0">defines a regular relation that can be expressed as : IF ( zc*LC , a : bzr* ) M IF ( ( zr*a : b ) ' , ( RCzr* ) '</definiens>
				<definiens id="1">the set of all possible symbol-pairs</definiens>
			</definition>
			<definition id="5">
				<sentence>Kaplan did not explicitly define the language that would be generated by a set of two-level rules ( a full grammar ) , but it would be characterized by an intersection of all the regular relations defined by the individual rules .</sentence>
				<definiendum id="0">two-level rules</definiendum>
				<definiens id="0">a full grammar ) , but it would be characterized by an intersection of all the regular relations defined by the individual rules</definiens>
			</definition>
			<definition id="6">
				<sentence>Given two alphabets A and A ' , a two-level morphological rule over A and A ' consists of a pair ( P , C ) where P is a symbol-pair from A and A ' , and C is a nonempty set of pairs ( LC , RC ) where LC and RC are sets of symbol-pair sequences from A and A ' ; each such set of symbol-pair sequences is called a context set .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a two-level morphological rule over A and A ' consists of a pair ( P , C ) where P is a symbol-pair from A and A '</definiens>
				<definiens id="1">a nonempty set of pairs ( LC , RC ) where LC and RC are sets of symbol-pair sequences from A and A ' ; each such set of symbol-pair sequences is called a context set</definiens>
			</definition>
			<definition id="7">
				<sentence>A set R of two-level morphological rules contextually allows a symbol-pair sequence ~ iff , for every partition ( P1 , a : a t , P2 ) of ~ , either there is no rule of the form ( a : a ' , C ) c R , or there is at least one rule ( a : a ' , C ) c R such that C contains a pair ( LC , RC ) such that LC matches P1 at the right end and RC matches P2 at the left end .</sentence>
				<definiendum id="0">RC</definiendum>
				<definiendum id="1">RC</definiendum>
				<definiens id="0">a pair ( LC ,</definiens>
			</definition>
			<definition id="8">
				<sentence>Explicit listing of additional feasible pairs can be represented by including context restriction rules of the form { ( { } , { where c denotes the empty sequence .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">the empty sequence</definiens>
			</definition>
			<definition id="9">
				<sentence>( The exact definition of matching is important here -- -matching the empty string or sequence means that there is some partition of the surrounding string in which the portion of the partition next to the symbol-pair of interest is the empty sequence ; it does not mean that there are no adjacent symbol-pairs ) .</sentence>
				<definiendum id="0">sequence</definiendum>
				<definiens id="0">important here -- -matching the empty string or</definiens>
			</definition>
			<definition id="10">
				<sentence>The two-level formalism , as defined above ( and as originally defined by Koskenniemi ) has no word-boundary symbol to mark the end of a sequence of symbols .</sentence>
				<definiendum id="0">two-level formalism</definiendum>
				<definiens id="0">defined above ( and as originally defined by Koskenniemi ) has no word-boundary symbol to mark the end of a sequence of symbols</definiens>
			</definition>
			<definition id="11">
				<sentence>Given two sets A and N , a symbol rx E A , a symbol fl c N , and a two-level morphological grammar G based on alphabets A and A ~ , then a symbol-pair sequence E ( A x A~ ) * is said to be generated by G with boundary a : fl iff Notice that under this definition , a sequence ~ may well be `` generated by G with boundary c~ : fl '' even though G itself is not generated by G. Theorem 2 Let A and A ~ be sets of symbols , let ~ be some symbol not in A , fl some symbol not in A ~ and let L be some set of symbol-pair sequences from A x AE Then there is a two-level morphological grammar G based on A U { ( ~ } and A ' U { fl } such that L - { G I G generates G with boundary rx : fl } 54 Graeme Ritchie Languages Generated by Two-Level Morphological Rules Proof Consider the two-level grammar ( written in the usual textual notation ) : ( ~ : fl =~ -- Ca : fl ors : tiC -- where C is some expression denoting the set L ( in the usual notation based on regular sets , C would be a regular expression ) .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a symbol rx E A , a symbol fl c N , and a two-level morphological grammar G based on alphabets A and A ~</definiens>
				<definiens id="1">a two-level morphological grammar G based on A U { ( ~ } and A ' U { fl } such that L - { G I G generates G with boundary rx : fl } 54 Graeme Ritchie Languages Generated by Two-Level Morphological Rules Proof Consider the two-level grammar ( written in the usual textual notation ) : ( ~ : fl =~ -- Ca : fl ors : tiC -- where</definiens>
				<definiens id="2">some expression denoting the set L ( in the usual notation based on regular sets</definiens>
			</definition>
			<definition id="12">
				<sentence>More formally , this grammar G ( = ( CR , SC ) ) could be written set-theoretically as ( { ( ~ : fl , { ( { e } , L1 ) ( L2 , ( e } ) } ) } , O ) where L1 is the set consisting of all possible concatenations of elements of L with a : fl , and L2 is the set consisting of all possible concatenations of c~ : fl with elements of L. It is straightforward to verify that this grammar generates all and only strings of the form o~ : flGc~ : fl for ~ E L. • This theorem may sound rather odd , since it implies that any set whatsoever can be generated in this manner ( in contrast to the earlier result that there are some regular languages of symbol-pairs that can not be generated by two-level rules ) .</sentence>
				<definiendum id="0">L1</definiendum>
				<definiendum id="1">L2</definiendum>
				<definiens id="0">the set consisting of all possible concatenations of elements of L with a : fl , and</definiens>
				<definiens id="1">the set consisting of all possible concatenations of c~ : fl with elements of L. It is straightforward to verify that this grammar generates all and only strings of the form o~</definiens>
			</definition>
			<definition id="13">
				<sentence>It should be clear from this that ( a ) any symbol-pair a : b occurs in the rules in G iff it occurs in the rules in G ' ; ( b ) any pair of context sets ( LC , RC ) appears in a rule in CR iff it appears in a rule in CR ' with the same symbol-pair ; ( c ) any pair of context sets ( LC , RC ) appears in a rule in SC iff it appears in a rule in SC ' with the same symbol-pair .</sentence>
				<definiendum id="0">LC , RC</definiendum>
				<definiens id="0">b occurs in the rules in G iff it occurs in the rules in G '</definiens>
			</definition>
			<definition id="14">
				<sentence>To create the context restriction rules for the intersection grammar G , take CR = CR ' U CR '' where CR ' is defined to be : ( ( p , { ( ( I ) *LC1 N c~*LC2 , RClC~ * f '' l RC2 ( I ) * ) } ) I ( p~ { ( LCI~RC1 ) } ) E CRI~ and ( p~ { ( LC2~RC2 ) } ) E ca2 } and CR '' is defined as : { ( a : b , { ( { a ' : a ' } , c ) } \ [ a : b is feasible in exactly one of G1 and G2 } where a ' is a new symbol that is not in a feasible pair in either G1 or G2 .</sentence>
				<definiendum id="0">context restriction rules for the intersection grammar G</definiendum>
				<definiendum id="1">CR</definiendum>
				<definiendum id="2">E CRI~</definiendum>
				<definiendum id="3">CR</definiendum>
				<definiens id="0">p , { ( ( I ) *LC1 N c~*LC2 , RClC~ * f '' l RC2 ( I ) * ) } ) I ( p~ { ( LCI~RC1 ) } )</definiens>
			</definition>
			<definition id="15">
				<sentence>There are at least two methods of compilation and subsequent interpretation for regular two-level grammars ( i.e. those whose context sets can be written as regular expressions , which is the normal practice ) , both of which rely heavily on finite-state transducers in their compiled form .</sentence>
				<definiendum id="0">regular expressions</definiendum>
				<definiens id="0">the normal practice ) , both of which rely heavily on finite-state transducers in their compiled form</definiens>
			</definition>
</paper>

		<paper id="4007">
			<definition id="0">
				<sentence>Effect : H recognizes the situation presented in Satellite as a cause for the volitional action presented in Nucleus .</sentence>
				<definiendum id="0">H</definiendum>
				<definiens id="0">recognizes the situation presented in Satellite as a cause for the volitional action presented in Nucleus</definiens>
			</definition>
			<definition id="1">
				<sentence>RST includes relations of both types , but commits to discourse analyses in which a single relation holds between each pair of elements .</sentence>
				<definiendum id="0">RST</definiendum>
				<definiens id="0">includes relations of both types , but commits to discourse analyses in which a single relation holds between each pair of elements</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>Generation mode uses the same routines as those used by the parser , but chooses a small subset of the permissible paths based on the outcome of a random-number generator , rather than exploring all paths and relying on an input word stream to resolve the correct one .</sentence>
				<definiendum id="0">Generation mode</definiendum>
				<definiens id="0">uses the same routines as those used by the parser , but chooses a small subset of the permissible paths based on the outcome of a random-number generator</definiens>
			</definition>
			<definition id="1">
				<sentence>This section describes how TINA handles several issues that are often considered to be part of the task of a parser .</sentence>
				<definiendum id="0">TINA</definiendum>
				<definiens id="0">handles several issues that are often considered to be part of the task of a parser</definiens>
			</definition>
			<definition id="2">
				<sentence>The gap mechanism resembles the Hold register idea of ATNs ( Woods 1970 ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ft. ) , but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes .</sentence>
				<definiendum id="0">grammars ( LFGs )</definiendum>
				<definiens id="0">the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes</definiens>
			</definition>
			<definition id="3">
				<sentence>The logic function is one of AND , OR , or SET , and the cycle is either top-down or bottom-up .</sentence>
				<definiendum id="0">logic function</definiendum>
				<definiens id="0">one of AND , OR , or SET</definiens>
			</definition>
			<definition id="4">
				<sentence>Perplexity , roughly defined as the geometric mean of the number of alternative word hypotheses that may follow each word in the sentence , is of particular concern in spoken language tasks .</sentence>
				<definiendum id="0">Perplexity</definiendum>
				<definiens id="0">the geometric mean of the number of alternative word hypotheses that may follow each word in the sentence , is of particular concern in spoken language tasks</definiens>
			</definition>
			<definition id="5">
				<sentence>The VOYAGER system can answer a number of different types of questions concerning navigation within a city , as well as provide certain information about hotels , restaurants , libraries , etc. , within the region .</sentence>
				<definiendum id="0">VOYAGER system</definiendum>
				<definiens id="0">provide certain information about hotels , restaurants , libraries , etc. , within the region</definiens>
			</definition>
			<definition id="6">
				<sentence>N Perplexity = 2 i=1 where the wi are the sequence of all words in all sentences , N is the total number of words , including an `` end '' word after each sentence , and P ( wi I Wi -- I~ '' 'Wl ) is the probability of the ith word given all preceding wordsJ 4 If all words are assumed equally likely , then P ( wi \ ] wi-1 , .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">the total number of words</definiens>
				<definiens id="1">the probability of the ith word given all preceding wordsJ 4 If all words</definiens>
			</definition>
			<definition id="7">
				<sentence>The recognizer for these systems is the SUMMIT system ( Zue et al. 1989 ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .</sentence>
				<definiendum id="0">SUMMIT system</definiendum>
				<definiens id="0">uses a segmental-based framework and includes an auditory model in the front-end processing</definiens>
			</definition>
			<definition id="8">
				<sentence>We have decided to limit semantic frame types to a small set of choices such as CLAUSE ( for a sentence-level concept , such as request ) , PREDICATE ( for a functional operation ) , REFERENCE ( essentially proper noun ) , and QSET ( for a set of objects ) .</sentence>
				<definiendum id="0">REFERENCE</definiendum>
				<definiendum id="1">QSET</definiendum>
				<definiens id="0">a sentence-level concept , such as request ) , PREDICATE ( for a functional operation )</definiens>
			</definition>
			<definition id="9">
				<sentence>Included is a set of initial contextfree rules , a set of training sentences , an illustration of how to compute the path probabilities from the training sentences , and an illustration of both parsing and perplexity computation for a test sentence .</sentence>
				<definiendum id="0">Included</definiendum>
				<definiens id="0">an illustration of how to compute the path probabilities from the training sentences , and an illustration of both parsing and perplexity computation for a test sentence</definiens>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>The Appendix contains a fragment of English grammar from which all examples are drawn .</sentence>
				<definiendum id="0">Appendix</definiendum>
				<definiens id="0">contains a fragment of English grammar from which all examples are drawn</definiens>
			</definition>
			<definition id="1">
				<sentence>A positive proposition consists of a predicate and two arguments .</sentence>
				<definiendum id="0">positive proposition</definiendum>
			</definition>
			<definition id="2">
				<sentence>Proposition ( 11 ) identifies the subject of a passive verb with iust one of its complements ; without 'a , ' it would identify the subject with every one of the complements , and lead to chaos .</sentence>
				<definiendum id="0">Proposition ( 11 )</definiendum>
				<definiens id="0">identifies the subject of a passive verb with iust one of its complements ; without 'a</definiens>
			</definition>
			<definition id="3">
				<sentence>If A isa B , then A inherits all the properties of B ( except those that are blocked as we explain in the next section ) .</sentence>
				<definiendum id="0">A inherits</definiendum>
				<definiens id="0">all the properties of B</definiens>
			</definition>
			<definition id="4">
				<sentence>( 26b ) A proposition P may be inherited iff a. Q is valid and b. at every point where P differs from Q , by containing Y instead of X , X subsumes Y. ( 26c ) A name X subsumes another name Y iff a. Y isa X , or b. Y is a compound name ( A B ) , where B is subsumed by X , or c. X =Y. ( Allowing inheritance to apply to compound names allows multiple inheritance i.e. , one concept may inherit down more than one path .</sentence>
				<definiendum id="0">b. Y</definiendum>
				<definiens id="0">a compound name ( A B ) , where B is subsumed by X</definiens>
				<definiens id="1">one concept may inherit down more than one path</definiens>
			</definition>
			<definition id="5">
				<sentence>preposition is a kind of adverb , and a determiner is a kind of pronoun , which in turn is a kind of noun .</sentence>
				<definiendum id="0">preposition</definiendum>
				<definiens id="0">a kind of pronoun , which in turn is a kind of noun</definiens>
			</definition>
			<definition id="6">
				<sentence>An operational definition of 'adjacent-to ' checks that no word between the words concerned has a head outside the phrase : ( 35a ) A is adjacent-to B iff every word between A and B is a subordinate of B. ( 35b ) A is a subordinate of B iff A is B or A is a dependent of a subordinate of B. But what if a word has more than one head ?</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">An operational definition of 'adjacent-to ' checks that no word between the words concerned has a head outside the phrase : ( 35a ) A is adjacent-to B iff every word between A and</definiens>
			</definition>
			<definition id="7">
				<sentence>A fake diamond is an object that inherits some of the properties of diamonds , especially the visible ones , but not all , and in particular not those that are criterial in the trade .</sentence>
				<definiendum id="0">fake diamond</definiendum>
				<definiens id="0">an object that inherits some of the properties of diamonds , especially the visible ones , but not all , and in particular not those that are criterial in the trade</definiens>
			</definition>
			<definition id="8">
				<sentence>The Case for Lexicase : An Outline of Lexicase Grammatical Theory .</sentence>
				<definiendum id="0">Case for Lexicase</definiendum>
			</definition>
</paper>

		<paper id="4006">
</paper>

		<paper id="4003">
			<definition id="0">
				<sentence>In automatic speech recognition , y is an acoustic signal ; in machine translation , y is a sequence of words in another language ; and in spelling correction , y is a sequence of characters produced by a possibly imperfect typist .</sentence>
				<definiendum id="0">y</definiendum>
				<definiendum id="1">y</definiendum>
				<definiens id="0">a sequence of words in another language ; and in spelling correction</definiens>
				<definiens id="1">a sequence of characters produced by a possibly imperfect typist</definiens>
			</definition>
			<definition id="1">
				<sentence>If C ( w ) is the number of times that the string w occurs in the string t~ , then for a 1-gram language model the maximum likelihood estimate for the parameter Pr ( w ) is C ( w ) /T. To estimate the parameters of an n-gram model , we estimate the parameters of the ( n 1 ) -gram model that it contains and then choose the order-n parameters so as to maximize Pr ( tn T \ [ t~- ' ) .</sentence>
				<definiendum id="0">w )</definiendum>
			</definition>
			<definition id="2">
				<sentence>The IBM Tangora speech recognition system has a vocabulary of about 20,000 words and employs a 3-gram language model with over eight trillion parameters ( Averbuch et al. 1987 ) .</sentence>
				<definiendum id="0">IBM Tangora speech recognition system</definiendum>
			</definition>
			<definition id="3">
				<sentence>We measure the performance of our model on the Brown corpus , which contains a variety of English text and is not included in either our training or held-out data ( Ku~era and Francis 1967 ) .</sentence>
				<definiendum id="0">Brown corpus</definiendum>
			</definition>
			<definition id="4">
				<sentence>For a 2-gram class model , the sequential maximum likelihood estimates of the order-2 parameters maximize Pr ( t T \ ] tl ) or , equivalently , log Pr ( t2 r I h ) and are given by C ( clc2 ) Pr ( c2 I Cl ) -= y-~cC ( ClC ) • ( 6 ) By definition , Pr ( clc2 ) = Pr ( Cl ) Pr ( c2 \ ] Cl ) , and so , for sequential maximum likelihood estimation , we have c ( cl ) Pr ( CLC2 ) C ( CLC2 ) x ( 7 ) T ~ , c C ( clc ) Since C ( Cl ) and ~c C ( ClC ) are the numbers of words for which the class is Cl in the strings t T and t r-1 respectively , the final term in this equation tends to 1 as T tends to infinity .</sentence>
				<definiendum id="0">c C</definiendum>
				<definiens id="0">the sequential maximum likelihood estimates of the order-2 parameters maximize Pr ( t T \ ] tl</definiens>
			</definition>
			<definition id="5">
				<sentence>( 8 ) Y Pr ( w2 ) Therefore , since Y~w C ( ww2 ) / ( T-1 ) tends to the relative frequency of w2 in the training text , and hence to Pr ( w2 ) , we must have , in the limit , L ( Tr ) = ZPr ( w ) logPr ( w ) +ZPr ( ClCa ) log Pr ( c2 Icl ) Pr ( c2 ) W CiC 2 = -H ( w ) ÷ I ( Cl , C2 ) , ( 9 ) 471 Computational Linguistics Volume 18 , Number 4 where H ( w ) is the entropy of the 1-gram word distribution and I ( cl , C2 ) is the average mutual information of adjacent classes .</sentence>
				<definiendum id="0">C2 )</definiendum>
				<definiens id="0">the relative frequency of w2 in the training text</definiens>
				<definiens id="1">the entropy of the 1-gram word distribution and I ( cl ,</definiens>
				<definiens id="2">the average mutual information of adjacent classes</definiens>
			</definition>
			<definition id="6">
				<sentence>As we shall see , we can make the computation of the average mutual information remaining after a merge in constant time , independent of V. Suppose that we have already made Vk merges , resulting in classes Ck ( 1 ) , Ck ( 2 ) , ... , Ck ( k ) and that we now wish to investigate the merge of Ck ( i ) with Ck ( j ) for 1 _ &lt; i &lt; j &lt; _ k. Let pk ( l , m ) = Pr ( Ck ( 1 ) , Ck ( m ) ) , i.e. , the probability that a word in class Ck ( m ) follows a word in class Ck ( 1 ) .</sentence>
				<definiendum id="0">Ck ( k</definiendum>
			</definition>
</paper>

		<paper id="3003">
			<definition id="0">
				<sentence>In particular , an important objective is to preserve as far as possible the flavor of this type of environment : a specialized programming language for linguistic descriptions , suitable for interpretation by a variety of programs performing tasks such as analysis and generation of sentences and phrases , lexical lookup , etc. , which associates with natural language expressions representations defined by the writer of a linguistic description and employs unification as the method of combining information and enforcing constraints .</sentence>
				<definiendum id="0">important objective</definiendum>
				<definiens id="0">to preserve as far as possible the flavor of this type of environment : a specialized programming language for linguistic descriptions</definiens>
				<definiens id="1">associates with natural language expressions representations defined by the writer of a linguistic description</definiens>
			</definition>
			<definition id="1">
				<sentence>An ELU lexicon consists of a number of 'classes , '' each of which is a structured collection of constraint equations encoding information common to a set of words , together with links to other more general `` superclasses . '</sentence>
				<definiendum id="0">ELU lexicon</definiendum>
				<definiens id="0">consists of a number of 'classes , '' each of which is a structured collection of constraint equations encoding information common to a set of words</definiens>
			</definition>
			<definition id="2">
				<sentence>indicates the empty string : ( i ) Prefix : '' '' Suffix : abc ( ii ) Prefix : a Suffix : bc ( iii ) Prefix : ab Suffix : c ( iv ) Prefix : abc Suffix : `` '' We introduce here a variety of notational conventions used in the examples below .</sentence>
				<definiendum id="0">empty string</definiendum>
				<definiens id="0">a Suffix : bc ( iii ) Prefix : ab Suffix : c ( iv ) Prefix : abc Suffix : `` '' We introduce here a variety of notational conventions used in the examples below</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus , Var is a variable , and both atom and 'Var ' are atomic FSs .</sentence>
				<definiendum id="0">Var</definiendum>
				<definiens id="0">a variable</definiens>
			</definition>
			<definition id="4">
				<sentence># Word A ( B D ) # Class B ( C ) # Class C ( F ) # Class D ( E ) # Class E ( F ) # Class F ( ) The ELU lexicon employs the class precedence algorithm of the Common Lisp Object System ( CLOS ) to derive a total order on the superclasses of each lexical class ; we adopt some of the CLOS terminology here .</sentence>
				<definiendum id="0">ELU lexicon</definiendum>
				<definiens id="0">employs the class precedence algorithm of the Common Lisp Object System ( CLOS ) to derive a total order on the superclasses of each lexical class</definiens>
			</definition>
			<definition id="5">
				<sentence>Similarly , we refer to the result of applying both default and variant set information in a class C to some FS F as the superclass extension of F with respect to C. The set containing ( 2 ) and ( 3 ) is then the superclass extension of ( 1 ) with respect to A , that consisting of just ( 5 ) is the superclass extension of ( 2 ) with respect to B , and the set containing ( 7 ) and ( 8 ) is the superclass extension of ( 5 ) with respect to C. Finally , since C is the most general superclass of A , ( 7 ) and ( 8 ) together comprise the global extension of the lexical class A. Figure 4 shows the pattern of inheritance arising from this lexicon ; classes are labeled 'A , ' 'B , ' and 'C , ' and within each the defaultand variant sets are identified as 'd ' and 'v , ' respectively .</sentence>
				<definiendum id="0">FS F</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">'d ' and 'v , ' respectively</definiens>
			</definition>
			<definition id="6">
				<sentence># Class Reg_Verb_Present_Exception ( Verb ) &lt; agr &gt; = sg3 &lt; form &gt; = &lt; stem &gt; ~ s I &lt; agr &gt; = sgl/sg2/pll/pl2/pl3 # Class Verb ( ) &lt; form &gt; = &lt; stem &gt; The lexicon will generally be called upon to establish a pairing of morphosyntactic information ( tense , case , agreement , etc. ) and phonological or graphological information ( the form taken by the variant of a word that expresses a particular morphosyntactic property ) .</sentence>
				<definiendum id="0">morphosyntactic information</definiendum>
				<definiendum id="1">graphological information</definiendum>
				<definiens id="0">the form taken by the variant of a word that expresses a particular morphosyntactic property )</definiens>
			</definition>
			<definition id="7">
				<sentence>If v ( c ) is the number of variant sets in the class c , and f is the function in the natural numbers such that f ( 0 ) = 1 and f ( k ) = k if k &gt; 0,15 the maximum number of FSs constructed for a lexical class with the CPL ( cl , ... cn ) is given by H IIf ( v ( ci ) ) .</sentence>
				<definiendum id="0">c )</definiendum>
				<definiendum id="1">f</definiendum>
				<definiens id="0">the number of variant sets in the class c , and</definiens>
			</definition>
			<definition id="8">
				<sentence>Lexical entries may be defined by means of templates that have other templates in their definitions .</sentence>
				<definiendum id="0">Lexical entries</definiendum>
			</definition>
			<definition id="9">
				<sentence># Class VInfl ( Verb ) &lt; morph psp prefix &gt; = ge &lt; morph psp suffix &gt; = t &lt; morph psp stem &gt; = &lt; morph bse stem &gt; &lt; syn infl &gt; = psp/bse/pres_sg_3/past_sg_3 i &lt; syn infl &gt; = psp &lt; form &gt; = &lt; morph psp prefix &gt; ~ &amp; &lt; morph psp stem &gt; ~ &amp; &lt; morph psp suffix &gt; f &lt; syn infl &gt; = bse &lt; form &gt; = &lt; morph prefix &gt; ~ &amp; &lt; morph bse stem &gt; ~ &amp; en i &lt; syn infl &gt; = pres_sg_3 &lt; morph suffix &gt; = t I &lt; syn infl &gt; = `` psp/bse/pres_sg_3 333 Computational Linguistics Volume 18 , Number 3 The most general class , Verb , establishes default values for prefix , stem , and suffix attributes , and states that the default &lt; form &gt; value is the result of concatenating whatever values these attributes have been assigned , either in this class or in more specific ones .</sentence>
				<definiendum id="0">value</definiendum>
				<definiens id="0">prefix , stem , and suffix attributes , and states that the default &lt; form &gt;</definiens>
			</definition>
			<definition id="10">
				<sentence>If there are n possible word forms implicit in a lexicon , then analyzing one form will take time proportional to c. log2 ( n ) where c is a small constant , plus in the average case a constant time for lookup with the candidate lexical classes .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">a small constant , plus in the average case a constant time for lookup with the candidate lexical classes</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>TRUMP obtains the necessary knowledge from a moderately sized lexicon ( 8,775 unique roots ) , specifically designed for use in language understanding , and a hierarchy of nearly 1,000 higher-level concepts , overlaid with approximately 40 concept-cluster definitions .</sentence>
				<definiendum id="0">TRUMP</definiendum>
				<definiens id="0">obtains the necessary knowledge from a moderately sized lexicon ( 8,775 unique roots ) , specifically designed for use in language understanding , and a hierarchy of nearly 1,000 higher-level concepts , overlaid with approximately 40 concept-cluster definitions</definiens>
			</definition>
			<definition id="1">
				<sentence>( c-ent c-made-of-rel : DESC ( a relationship between an object and what it is made of ) : PAR ( c-phys-prop-rel ) : PREF ( ( r-statevalue c-phys-obj ) ( r-stateholder ( or c-phys-obj c-whole ) ) ) : RELS ( ( *held-by* c-phys-obj ) ) ) Figure 6 The conceptual definition of c-made-of-rel .</sentence>
				<definiendum id="0">DESC</definiendum>
				<definiendum id="1">RELS</definiendum>
				<definiens id="0">a relationship between an object and what it is made of ) : PAR ( c-phys-prop-rel ) : PREF ( ( r-statevalue c-phys-obj ) ( r-stateholder ( or c-phys-obj c-whole )</definiens>
			</definition>
			<definition id="2">
				<sentence>Collocation is the relationship among any group of words that tend to co-occur in a predictable configuration .</sentence>
				<definiendum id="0">Collocation</definiendum>
				<definiens id="0">the relationship among any group of words that tend to co-occur in a predictable configuration</definiens>
			</definition>
			<definition id="3">
				<sentence>To recognize collocations during preprocessing , TRUMP uses a set of patterns , each of which lists the root words or syntactic categories that make up the collocation .</sentence>
				<definiendum id="0">TRUMP</definiendum>
				<definiens id="0">uses a set of patterns , each of which lists the root words or syntactic categories that make up the collocation</definiens>
			</definition>
			<definition id="4">
				<sentence>A cluster is a set of the senses associated with some central concept .</sentence>
				<definiendum id="0">cluster</definiendum>
				<definiens id="0">a set of the senses associated with some central concept</definiens>
			</definition>
			<definition id="5">
				<sentence>These clusters consist of the sets of all senses sharing a particular conceptual parent .</sentence>
				<definiendum id="0">These clusters</definiendum>
				<definiens id="0">consist of the sets of all senses sharing a particular conceptual parent</definiens>
			</definition>
			<definition id="6">
				<sentence>The third type of cluster , the situational cluster , encodes general relationships among senses on the basis of their being associated with a common setting , event , 13 Computational Linguistics Volume 18 , Number 1 ( take : POS verb : SPECIAL ( ( take50 : S-COMPOUNDS ( ( vc ( or ( member c-verb_advise2-obj c-act-of-verb_blamel c-act-of-verb_losel noun_profit2 ) c-giving ) ) ) : EXAMPLE ( take delivery ) : PAR ( c-receiving ) ) ( take51 : S-COMPOUNDS ( ( vc ( or ( member noun_effort1 ) c-temporal-obj c-energy ) ) ) : EXAMPLE ( the job takes up time ) ) : PAR ( c-require-tel ) ) ( take52 : S-COMPOUNDS ( ( vc ( member noun_news1 noun_burden1 noun_load2 noun_pressure3 noun_pressure2 noun_stress1 noun stress2 c-act-of-verb_strain1 ) ) ) : EXAMPLE ( he could n't take the presssure ) : PAR ( c-managing ) ) ( take58 : S-COMPOUNDS ( ( vc ( or ( member noun_office2 noun_advantagel noun_charge1 c-act-of-verb_control1 noun_command2 noun_responsibility1 ) c-structure-tel c-shape-tel ) ) ) : EXAMPLE ( they took advantage of the situation ) : PAR ( c-contracting ) ) ( ts_ke59 : S-COMPOUNDS ( ( vc ( member noun_effect1 ) ) ) : EXAMPLE ( the new rules take effect today ) : PAR ( c-transpire ) ) ( take60 : S-COMPOUNDS ( ( vc ( or c-task ) ) ) : EXAMPLE ( he took the assignment ) : PAR ( c-deciding ) ) ) Figure 9 Some compositional collocations involving take .</sentence>
				<definiendum id="0">situational cluster</definiendum>
				<definiendum id="1">PAR</definiendum>
				<definiendum id="2">) ) : PAR</definiendum>
				<definiendum id="3">c-require-tel ) )</definiendum>
				<definiendum id="4">S-COMPOUNDS</definiendum>
				<definiens id="0">the job takes up time</definiens>
				<definiens id="1">c-managing ) ) ( take58 : S-COMPOUNDS ( ( vc ( or ( member noun_office2 noun_advantagel noun_charge1 c-act-of-verb_control1 noun_command2 noun_responsibility1 ) c-structure-tel c-shape-tel ) ) ) : EXAMPLE ( they took advantage of the situation ) : PAR ( c-contracting ) ) ( ts_ke59 : S-COMPOUNDS</definiens>
				<definiens id="2">c-deciding ) ) ) Figure 9 Some compositional collocations involving take</definiens>
			</definition>
			<definition id="7">
				<sentence>Situational clusters capture the associations found in generic descriptions ( cf. Dahlgren , McDowell , and Stabler 1989 ) or dictionary examples ( cf. Janssen 1990 ) , but are more compact because clusters may include whole categories of objects ( such as c-law-action ) as members and need not specify relationships between the members .</sentence>
				<definiendum id="0">Situational clusters</definiendum>
				<definiens id="0">such as c-law-action ) as members</definiens>
			</definition>
			<definition id="8">
				<sentence>To recognize a collocation , the preprocessor relies on a set of simple patterns , which match the general syntactic context in which the collocation occurs .</sentence>
				<definiendum id="0">simple patterns</definiendum>
				<definiens id="0">match the general syntactic context in which the collocation occurs</definiens>
			</definition>
			<definition id="9">
				<sentence>Similarly , the score for a cluster is the specificity of that cluster ( as defined in Section 3.4 ) .</sentence>
				<definiendum id="0">score for a cluster</definiendum>
				<definiens id="0">the specificity of that cluster ( as defined in Section 3.4 )</definiens>
			</definition>
			<definition id="10">
				<sentence>Together , the data in the tables reveal the following sources of preference strength : The 'arrival ' sense ( reachl ) gains support from the fact that there is a sense of by meaning AGENT , which is a role that arrivals expect ( line 3 of column 3 of Figure 17 ) , and the state and the EPA make reasonably good agents ( line 5 of column 3 of Figure 17 ) .</sentence>
				<definiendum id="0">AGENT</definiendum>
				<definiens id="0">a role that arrivals expect</definiens>
			</definition>
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>DATR is a declarative language for representing inheritance networks that support multiple default inheritance .</sentence>
				<definiendum id="0">DATR</definiendum>
				<definiens id="0">a declarative language for representing inheritance networks that support multiple default inheritance</definiens>
			</definition>
			<definition id="1">
				<sentence>The syntax of paths is a superset of that found in the PATR-II language ( Shieber 1986 ) .</sentence>
				<definiendum id="0">syntax of paths</definiendum>
				<definiens id="0">a superset of that found in the PATR-II language</definiens>
			</definition>
			<definition id="2">
				<sentence>The modus feature specifies a number of constraints imposed on the interpretation of semantic objects , such as polarity , aspect , and tense .</sentence>
				<definiendum id="0">modus feature</definiendum>
				<definiens id="0">specifies a number of constraints imposed on the interpretation of semantic objects , such as polarity , aspect , and tense</definiens>
			</definition>
			<definition id="3">
				<sentence>A transitive verb takes a subject and an object .</sentence>
				<definiendum id="0">transitive verb</definiendum>
			</definition>
			<definition id="4">
				<sentence>The next line enables the complement to share the subject of the matrix verb , i.e. , in a sentence like Amy wants to fly , Amy is the subject of both want and fly .</sentence>
				<definiendum id="0">Amy</definiendum>
				<definiens id="0">the complement to share the subject of the matrix verb , i.e. , in a sentence like Amy wants to fly ,</definiens>
				<definiens id="1">the subject of both want and fly</definiens>
			</definition>
			<definition id="5">
				<sentence>Fortunately , DATR allows the creation of intermediate nodes between the NOUN node and the common nouns , and these nodes specify distinctive properties of each distinct class of nouns .</sentence>
				<definiendum id="0">DATR</definiendum>
			</definition>
			<definition id="6">
				<sentence>The form of encoded DAGs is known as normal form ( Bouma 1990 ) ; that is , if two DAGs share a common sub-DAG , this is explicitly represented in both , with the exception of unevaluated sharing sub-DAGs that are represented as Prolog variables .</sentence>
				<definiendum id="0">form of encoded DAGs</definiendum>
			</definition>
</paper>

		<paper id="4002">
			<definition id="0">
				<sentence>Because LF is a component of a computer model for language comprehension , it must be designed with two goals in mind .</sentence>
				<definiendum id="0">LF</definiendum>
				<definiens id="0">a component of a computer model for language comprehension , it must be designed with two goals in mind</definiens>
			</definition>
			<definition id="1">
				<sentence>Pronouns are a source of underspecification in a sentence : the antecedent of a pronoun can not be determined using syntactic information alone , but requires a combination of syntactic , semantic , and contextual information .</sentence>
				<definiendum id="0">Pronouns</definiendum>
				<definiens id="0">determined using syntactic information alone , but requires a combination of syntactic , semantic , and contextual information</definiens>
			</definition>
			<definition id="2">
				<sentence>A discourse entity is a designator for the entity or set of entities the NP evokes in the discourse model of the speaker or hearer .</sentence>
				<definiendum id="0">discourse entity</definiendum>
				<definiens id="0">a designator for the entity or set of entities the NP evokes in the discourse model of the speaker or hearer</definiens>
			</definition>
			<definition id="3">
				<sentence>Pronouns are a source of ambiguity in verb phrase ellipsis ( VPE ) .</sentence>
				<definiendum id="0">Pronouns</definiendum>
				<definiens id="0">a source of ambiguity in verb phrase ellipsis ( VPE )</definiens>
			</definition>
			<definition id="4">
				<sentence>To signal a VPE , a full verb phrase ( VP ) is replaced with an auxiliary , as in the second sentence of Example 3 .</sentence>
				<definiendum id="0">VP</definiendum>
				<definiens id="0">a full verb phrase</definiens>
			</definition>
			<definition id="5">
				<sentence>Node A c ( constituent ) -commands node B iff the branching node ~1 most immediately dominating A either dominates B or is immediately dominated by a node a2 which dominates B , and a2 is of the same category type as al. ( Reinhart 1983 , p. 23 ) To illustrate the concept of c-command , consider the parse tree for the sentence in Example 5 , shown in Figure 2 .</sentence>
				<definiendum id="0">a2</definiendum>
				<definiens id="0">c ( constituent ) -commands node B iff the branching node ~1 most immediately dominating A either dominates B or is immediately dominated by a node a2 which dominates B , and</definiens>
			</definition>
			<definition id="6">
				<sentence>Vx : ( man x ) x , A ( y ) ( and ( show y ( picture-of ( his2 y z ) ) \ [ 3z : ( boy z ) z\ ] ) ( = ( his2 y z ) z ) ) Simplification : Vx : ( man x ) x , A ( y ) ( show y ( picture-of z ) \ [ 3z : ( boy z ) z\ ] ) To indicate that the antecedent for his is a boy , the pronoun function ( his2 yz ) is equated with the existentially quantified variable z ( by pronoun update case 1 ) .</sentence>
				<definiendum id="0">Vx</definiendum>
			</definition>
			<definition id="7">
				<sentence>Any pronoun embedded in a definite NP can affect the definite 's meaning : a possessive pronoun , one contained in a prepositional phrase ( PP ) , or one contained in a relative clause attached to the definite .</sentence>
				<definiendum id="0">PP</definiendum>
				<definiens id="0">a possessive pronoun , one contained in a prepositional phrase</definiens>
			</definition>
			<definition id="8">
				<sentence>( ( def4 ) I ( name ( def4 ) George ) ) , Mx ) ( and ( see x ( ( def2 x ) I ( dog ( def2 x ) ) ) ) ( = ( def2 x ) ( def3 ) ) ) Simplification : ( ( def4 ) I ( name ( def4 ) George ) ) , Mx ) ( and ( see x ( def3 ) ) ( dog ( def3 ) ) ) Because the final meaning for the dog is ( def3 ) in 34 and 35 , it denotes the same dog .</sentence>
				<definiendum id="0">Mx )</definiendum>
				<definiendum id="1">Mx )</definiendum>
				<definiens id="0">= ( def2 x ) ( def3 ) ) ) Simplification : ( ( def4 ) I ( name ( def4 ) George ) ) ,</definiens>
			</definition>
			<definition id="9">
				<sentence>( ( defs ) \ [ ( name ( defs ) George ) ) , A ( x ) ( love x ( ( def2 x ) \ [ ( and ( wife ( def2 x ) ) ( possess ( his3 x ) ( def2 x ) ) ( = ( his3 x ) x ) ) ) ) Simplification : ( ( defs ) I ( name ( def5 ) George ) ) , Mx ) ( love x ( ( def2 x ) I ( and ( wife ( def2 x ) ) ( possess x ( def2 x ) ) ) ) ) Notice that the function def2 denotes a different individual in the trigger and elided sentences , depending on the value of x. On the other hand , suppose that his refers directly to Fred .</sentence>
				<definiendum id="0">( ( defs</definiendum>
				<definiendum id="1">def2</definiendum>
				<definiens id="0">A ( x ) ( love x ( ( def2 x ) \ [ ( and ( wife ( def2 x ) ) ( possess ( his3 x ) ( def2 x ) ) ( = ( his3 x ) x ) ) ) ) Simplification : ( ( defs ) I ( name ( def5 ) George ) ) , Mx ) ( love x ( ( def2 x ) I ( and ( wife ( def2 x ) ) ( possess x ( def2 x ) ) ) ) ) Notice that the function</definiens>
				<definiens id="1">a different individual in the trigger and elided sentences , depending on the value of x. On the other hand , suppose that his refers directly to Fred</definiens>
			</definition>
			<definition id="10">
				<sentence>( ( defl ) I ( name ( defl ) Fred ) ) , ) ffx ) ( and ( love x ( ( def2 x ) I ( and ( wife ( def2 x ) ) ( possess ( his3 x ) ( def2 x ) ) ( = ( his3 x ) ( defl ) ) ) ) ) ( = ( def2 x ) ( def4 ) ) ) Simplification : ( ( deh ) I ( name ( defl ) Fred ) ) , ) ffx ) ( and ( love x ( def4 ) ) ( wife ( def4 ) ) ( possess ( defO ( def4 ) ) Using this representation of the VP , we derive the strict reading of the elided sentence .</sentence>
				<definiendum id="0">defO</definiendum>
				<definiens id="0">def2 x ) ( def4 ) ) ) Simplification : ( ( deh ) I ( name ( defl ) Fred ) )</definiens>
			</definition>
			<definition id="11">
				<sentence>Possible Meanings : a. ~x : ( woman x ) Not ( see Fred x ) b. Not 3x : ( woman x ) ( see Fred x ) = Vx : ( woman x ) Not ( see Fred x ) Whenever there is negation in a sentence containing an indefinite , two meanings of the sentence are possible .</sentence>
				<definiendum id="0">Possible Meanings</definiendum>
				<definiens id="0">a. ~x : ( woman x</definiens>
			</definition>
			<definition id="12">
				<sentence>453 Computational Linguistics Volume 18 , Number 4 The definite description , ( ~x ) ( dog x ) , which stands for the object x such that the property ( dog x ) is true names a unique object , and hence , is translated into the formula , 3x ( and ( dog x ) Vy ( ( dog y ) *-4 ( = x y ) ) ) .</sentence>
				<definiendum id="0">dog x )</definiendum>
				<definiendum id="1">Vy</definiendum>
				<definiens id="0">true names a unique object</definiens>
			</definition>
			<definition id="13">
				<sentence>Kamp ( 1981 ) introduces a discourse theory similar to Heim 's , called Discourse Representation Theory ( DRT ) , providing a model-theoretic interpretation for discourse models .</sentence>
				<definiendum id="0">Discourse Representation Theory ( DRT</definiendum>
				<definiens id="0">introduces a discourse theory similar to Heim 's , called</definiens>
			</definition>
			<definition id="14">
				<sentence>Klein represents the trigger VP as a boxed structure named P. Within this box is a distinguished variable x2 ( distinguished variables are marked with brackets ) , which corresponds to the abstracted subject .</sentence>
				<definiendum id="0">Klein</definiendum>
				<definiendum id="1">VP</definiendum>
				<definiens id="0">the trigger</definiens>
				<definiens id="1">a distinguished variable x2 ( distinguished variables are marked with brackets ) , which corresponds to the abstracted subject</definiens>
			</definition>
			<definition id="15">
				<sentence>The elided sentence is represented initially as Q ( xl ) , where xl is the discourse referent for the subject of the elided sentence .</sentence>
				<definiendum id="0">xl</definiendum>
				<definiens id="0">the discourse referent for the subject of the elided sentence</definiens>
			</definition>
			<definition id="16">
				<sentence>Hence , Klein derives the two expected readings for the elided sentence in 3 .</sentence>
				<definiendum id="0">Klein</definiendum>
				<definiens id="0">derives the two expected readings for the elided sentence in 3</definiens>
			</definition>
			<definition id="17">
				<sentence>are defined by the following rules : ( a ) If T is a constant , T is a term .</sentence>
				<definiendum id="0">T</definiendum>
			</definition>
			<definition id="18">
				<sentence>For example , \ [ V~ : ~ ~\ ] is a q-term , where ~ is a variable and ~ is a well-formed formula .</sentence>
				<definiendum id="0">~</definiendum>
				<definiens id="0">a variable and ~ is a well-formed formula</definiens>
			</definition>
			<definition id="19">
				<sentence>If ¢ is a well-formed formula and r is a term , q-term , or f-term , then T , A ( X ) ¢ is a well-formed formula .</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">a well-formed formula and</definiens>
				<definiens id="1">a well-formed formula</definiens>
			</definition>
			<definition id="20">
				<sentence>, an , cr\ ] is an element in the set of n + 1 tuples that M maps to 8 , then cr is the value of T. To this we add : any Ti that is a variable bound by A ( Ti ) , the lambda operator must have 463 Computational Linguistics Volume 18 , Number 4 .</sentence>
				<definiendum id="0">cr</definiendum>
				<definiens id="0">an element in the set of n + 1 tuples that M maps to 8</definiens>
			</definition>
			<definition id="21">
				<sentence>are interpretations with identical domains and fl is a constant , M is a fl-variant of M ' if M and M ' differ only in what they assign to ft. 1 ) If ~b has the form Va~b , M ~ ~ if and only if , for all M t , if Mt is a fl-variant of M , M ' ~ ~ ( fl/c~ ) , where ~b ( fl/c~ ) is the expression obtained by substituting fl for all free occurrences c~ in ~b. 2 ) If ~ has the form ~a~b , M ~ q~ if and only if , for some M ' , if M ' is a fl-variant of M , M ' ~ ~b ( fl/c~ ) , where ~b ( fl/o~ ) is the expression obtained by substituting fl for all free occurrences c~ in ~b. ~ !</sentence>
				<definiendum id="0">M</definiendum>
				<definiendum id="1">Mt</definiendum>
				<definiendum id="2">M</definiendum>
				<definiendum id="3">~b</definiendum>
				<definiendum id="4">~b ( fl/o~ )</definiendum>
				<definiens id="0">a fl-variant of M ,</definiens>
				<definiens id="1">the expression obtained by substituting fl for all free occurrences c~ in ~b. 2 ) If ~ has the form ~a~b</definiens>
			</definition>
			<definition id="22">
				<sentence>If ~ has the form r , , ~ ( x ) 42 and r is the f-term ( ( 0 rl r2 ... rn ) I 41 ) , then M ~ ~ if and only if M ~ ( and 41 4~ ) ( where 4~ is 42 ( ( 0 rl r2 ... rn ) /X ) Additionally : equivalent to ( and 8 ' 41 ) , where 8 ' is the formula obtained by replacing the f-term with ( 0 rl r2 ... rn ) in 8 .</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">the formula obtained by replacing the f-term with ( 0 rl r2 ... rn ) in 8</definiens>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>Unification is an information-combining , monotonic , operation on feature structures , whereas the implementation of default devices typically requires some form of nonmonotonicity .</sentence>
				<definiendum id="0">Unification</definiendum>
				<definiens id="0">an information-combining , monotonic , operation on feature structures</definiens>
			</definition>
			<definition id="1">
				<sentence>Evans and Gazdar ( 1989a , b ) present the DATR-formalism , which , among other things , contains a nonmonotonic inference system that enables an elegant account of the blocking-phenomenon just described .</sentence>
				<definiendum id="0">DATR-formalism</definiendum>
				<definiens id="0">contains a nonmonotonic inference system that enables an elegant account of the blocking-phenomenon just described</definiens>
			</definition>
			<definition id="2">
				<sentence>Priority union of two f-structures A and B is defined as an operation that extends A with information from B that is not included ( or filled in ) in A. As not all information in B is present in the priority union of A and B , this operation introduces nonmonotonicity .</sentence>
				<definiendum id="0">Priority union of two</definiendum>
				<definiendum id="1">B</definiendum>
				<definiendum id="2">B</definiendum>
				<definiens id="0">an operation that extends A with information from B that is not included ( or filled in ) in A. As not all information in B is present in the priority union of A and</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus , given two feature structures FSD ( the default ) and FSND ( the nondefault ) , adding FSp to FSND conservatively would be equivalent to overwriting FSD with FSND , and to the priority union of FSND and FSD ( i.e. FSND/FSD in the notation of Kaplan \ [ 1987\ ] ) .</sentence>
				<definiendum id="0">FSND</definiendum>
				<definiens id="0">the nondefault ) , adding FSp to FSND conservatively would be equivalent to overwriting FSD with FSND , and to the priority union of FSND and FSD ( i.e. FSND/FSD in the notation of Kaplan \ [ 1987\ ] )</definiens>
			</definition>
			<definition id="4">
				<sentence>Unification is defined in terms of subsumption , a relation that imposes a partial ordering on automata : Definition An automaton A subsumes an automaton B ( A _ B ) iff there is a homomorphism h from A to B such that : Intuitively , A u B if B extends the information in A. A = B if A _ B and B U A. Unification of two automata A and B ( A U B ) is the least upper bound of these automata under subsumption .</sentence>
				<definiendum id="0">Unification</definiendum>
				<definiens id="0">defined in terms of subsumption , a relation that imposes a partial ordering on automata : Definition An automaton A subsumes an automaton B ( A _ B ) iff there is a homomorphism h from A to B such that : Intuitively , A u B if B extends the information in A. A = B if A _ B and B U A. Unification of two automata A and B ( A U B ) is the least upper bound of these automata under subsumption</definiens>
			</definition>
			<definition id="5">
				<sentence>The semantics of descriptions ( sets of formulae of the description language ) is given in terms of satisfaction : Definition An automaton A = ( Q , G , F , 6 , q0 , F , ; ~ ) satisfies a description D ( A ~ D ) or a formula ( A ~ q~ ) in the following cases : A~D A~a A ~ ( p ) -D A ~ ( pl ) = ( p2 ) iff for all q~ E D : A ~ q~ , iff Q = F = { q0 } and &amp; ( q0 ) = a , iff 6 ( qo , p ) is defined and qo/P ~ D , iff 6 ( qo , pl ) = ~ ( q0 , p2 ) .</sentence>
				<definiendum id="0">A~D A~a A ~</definiendum>
				<definiendum id="1">qo , p</definiendum>
				<definiens id="0">sets of formulae of the description language ) is given in terms of satisfaction : Definition An automaton A = ( Q , G , F , 6 , q0 , F</definiens>
			</definition>
			<definition id="6">
				<sentence>B = A ~ U B , where A ~ is the maximal ( i.e. most specific ) element in the subsumption ordering such that A ' rA and A ~ U B is defined .</sentence>
				<definiendum id="0">A ~</definiendum>
				<definiendum id="1">rA</definiendum>
				<definiens id="0">the maximal ( i.e. most specific ) element in the subsumption ordering such that A '</definiens>
			</definition>
			<definition id="7">
				<sentence>B in terms of the difference of the two arguments A and B. Definition Difference ( first version ) The difference of two automata A and B is the maximal element A B that meets the following conditions : 6B ( qO , P ' ) E FB , Definition Default Unification ( second version ) Atd !</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">the maximal element A B that meets the following conditions : 6B ( qO , P ' ) E FB , Definition Default Unification ( second version</definiens>
			</definition>
			<definition id="8">
				<sentence>Case ( 1 ) : If A ( 6o ( qo , p ) ) = a , then ( i ) A ( ~AuB ( qo , p ) ) = a and thus A ( 6A ( qo , p ) ) = a or A ( ~B ( q0 , p ) ) = a ( since there are no reentrancies ) and ( ii ) there is no prefix p ' of p such that 6c ( qo , p ' ) c Fc , nor is 8c ( qo , p ) defined .</sentence>
				<definiendum id="0">( ~AuB</definiendum>
				<definiens id="0">qo , p ' ) c Fc , nor is 8c ( qo , p</definiens>
			</definition>
			<definition id="9">
				<sentence>Thus , the difference operation can be extended as follows : Definition Difference ( final version ) The difference of A and B is the maximal element A B in the subsumption ordering that meets the following conditions : 6B ( qo , p ' ) E FB or 6B ( qo , P ' ) = 6B ( qo , P '' ) ( p ' p '' ) , undefined .</sentence>
				<definiendum id="0">difference operation</definiendum>
				<definiendum id="1">B</definiendum>
				<definiens id="0">extended as follows : Definition Difference ( final version ) The difference of A and</definiens>
				<definiens id="1">the maximal element A B in the subsumption ordering that meets the following conditions</definiens>
			</definition>
			<definition id="10">
				<sentence>Case ( 3 ) : If 6A-c ( qo , p ) is defined and ~A-C ( q0 , p ) ~ FA-C ( otherwise this case reduces to case ( 1 ) ) and 6A-C ( qO , P ) not reentrant ( otherwise this case reduces to case ( 2 ) ) , it follows that ( i ) ~B ( qo , P ) is defined , and ( ii ) there is no prefix p ' of p such that 6c ( qo , p ' ) E Fc or 6c ( qo , p ' ) = ~c ( q0 , p '' ) .</sentence>
				<definiendum id="0">p )</definiendum>
				<definiens id="0">qo , p ' ) E Fc or 6c ( qo , p '</definiens>
			</definition>
			<definition id="11">
				<sentence>The extension of A relative to B ( Ext ( A , B ) ) is the minimal ( i.e. most general ) element Ext ( A , B ) such that ~Ext ( A , B ) ( qo , pql ' ) = ~Ext ( A , B ) ( qo , p'ql ' ) ( wherever possible ) for all 1 / E G. The automaton A is extended , sometimes somewhat redundantly , with reentrant paths that are extensions of paths already reentrant in A. Ext ( A , B ) is nevertheless usually more informative than A itself , as the addition of a path pl blocks unification with feature structures in which p receives an atomic value .</sentence>
				<definiendum id="0">extension of A relative to B ( Ext</definiendum>
				<definiendum id="1">B ) )</definiendum>
				<definiendum id="2">B )</definiendum>
				<definiendum id="3">~Ext ( A , B )</definiendum>
				<definiendum id="4">A , B )</definiendum>
				<definiendum id="5">addition of a path pl</definiendum>
				<definiens id="0">the minimal ( i.e. most general ) element Ext ( A , B ) such that ~Ext ( A ,</definiens>
			</definition>
			<definition id="12">
				<sentence>198 Gosse Bouma Feature Structures and Nonmonotonicity template ( which is a feature structure ) .</sentence>
				<definiendum id="0">Nonmonotonicity template</definiendum>
				<definiens id="0">a feature structure )</definiens>
			</definition>
			<definition id="13">
				<sentence>The denotation of the template AUX is the feature structure in 16 ( i.e. , whether it is defined as in 14 or as in 18 is irrelevant ) , and from that it is impossible to conclude that AUX inherits from VERB , and thus the kind of reasoning used to justify the resolution of feature conflicts used in Touretzky ( 1986 ) is not applicable in our case .</sentence>
				<definiendum id="0">denotation of the template AUX</definiendum>
				<definiens id="0">the feature structure in 16 ( i.e. , whether it is defined as in 14 or as in 18 is irrelevant )</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>Let P denote the probability distribution of X and let Ep denote expectations * P.O. Box 704 , Yorktown Heights , NY 10598 ( ~ ) 1992 Association for Computational Linguistics Computational Linguistics Volume 18 , Number 1 with respect to P. The entropy of X is defined by H ( X ) =_ H ( P ) =_ -EplogP ( Xo l X_I , X_2 , ... ) .</sentence>
				<definiendum id="0">H ( X ) =_ H ( P ) =_ -EplogP ( Xo l X_I</definiendum>
				<definiens id="0">the probability distribution of X and let Ep denote expectations * P.O. Box 704 , Yorktown Heights , NY 10598 ( ~ ) 1992 Association for Computational Linguistics Computational Linguistics Volume 18</definiens>
			</definition>
			<definition id="1">
				<sentence>( 3 ) n -- -* cx~ y/ Thus , for an ergodic process , an estimate of H ( P ) can be obtained from a knowledge of P on a sufficiently long sample drawn randomly according to P. When P is not known , an upper bound to H ( P ) can still be obtained from an approximation to P. Suppose that the stationary stochastic process M is a model for P. The cross-entropy of P as measured by M is defined by H ( P , M ) = -- Ep logM ( Xo l X_I , X_2 , ... ) .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">-* cx~ y/ Thus , for an ergodic process , an estimate of H ( P ) can be obtained from a knowledge of P on a sufficiently long sample drawn randomly according to P. When P is not known , an upper bound to H ( P ) can still be obtained from an approximation to P. Suppose that the stationary stochastic process</definiens>
			</definition>
			<definition id="2">
				<sentence>It is well known that for any uniquely decodable coding scheme ( Cover and Thomas 1991 ) , Ep I ( XIX2 ... Xn ) ~ -Ep log e ( XlX2 ... Xn ) , ( 9 ) where I ( X1X2 ... Xn ) is the number of bits in the encoding of the string X1X2 ... Xn .</sentence>
				<definiendum id="0">Ep I</definiendum>
				<definiendum id="1">I</definiendum>
			</definition>
			<definition id="3">
				<sentence>An Estimate of an Upper Bound for the Entropy of English On the other hand , an arithmetic coding scheme ( Bell , Cleary , and Witten 1990 ) using model M will encode the sequence xlx2 ... Xn in IM ( XlX2 ... Xn ) = r -logM ( XlX2 ... Xn ) + 11 ( 11 ) bits , where \ [ r\ ] denotes the smallest integer not less than r. Combining Equations ( 7 ) and ( 11 ) we see that H ( P , M ) is the number of bits per symbol achieved by using model M to encode a long string of text drawn from P : H ( P , M ) = lim llM ( X1X2 ... Xn ) .</sentence>
				<definiendum id="0">Estimate of an Upper Bound</definiendum>
				<definiens id="0">the number of bits per symbol achieved by using model</definiens>
			</definition>
			<definition id="4">
				<sentence>Using Equation ( 8 ) we can estimate an upper bound on the entropy of characters in English as follows : H ( English ) &lt; _ __1 log M ( test sample ) , n ( 13 ) where n is the number of characters in the sample .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">estimate an upper bound on the entropy of characters in English as follows : H ( English ) &lt; _ __1 log M ( test sample )</definiens>
				<definiens id="1">the number of characters in the sample</definiens>
			</definition>
			<definition id="5">
				<sentence>The token vocabulary consists of character ; The spelling model generates a spelling $ 1s2 ... Sk given a token .</sentence>
				<definiendum id="0">token vocabulary</definiendum>
				<definiendum id="1">spelling model</definiendum>
				<definiens id="0">consists of character ; The</definiens>
			</definition>
			<definition id="6">
				<sentence>The cross-entropy of a language model and a test sample provides a natural quantitative measure of the predictive power of the model .</sentence>
				<definiendum id="0">test sample</definiendum>
				<definiens id="0">provides a natural quantitative measure of the predictive power of the model</definiens>
			</definition>
			<definition id="7">
				<sentence>The ASCII cod4 for the characters in the Brown Corpus has 8 bits per character .</sentence>
				<definiendum id="0">ASCII cod4 for</definiendum>
			</definition>
</paper>

	</volume>
