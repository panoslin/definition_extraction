<?xml version="1.0" encoding="UTF-8"?>
	<volume id="C94">

		<paper id="1052">
			<definition id="0">
				<sentence>Within Acquilex IP Project , a unification framework based on typed feature structures \ [ 4\ ] was ddveloped , the LKB ( Lexical Knowledge Base ) , in order to represent conceptual units corresponding to lexieal senses , lexical and phrasal rules , multilingual rclalionships , elc .</sentence>
				<definiendum id="0">LKB</definiendum>
			</definition>
			<definition id="1">
				<sentence>The LKB formalism uses a typed feature structure ( FS ) system for representing lexical knowledge .</sentence>
				<definiendum id="0">LKB formalism</definiendum>
			</definition>
			<definition id="2">
				<sentence>The establisment of tlinks can be obviously perlbrmed manually , but the multiplicity of possible cases and the existence of several Knowledge St ) nrces ( such as bilingual dictionaries , monolingt , al LDBs , or a mtddlingual \ [ .</sentence>
				<definiendum id="0">establisment of tlinks</definiendum>
				<definiens id="0">such as bilingual dictionaries , monolingt , al LDBs , or a mtddlingual \ [</definiens>
			</definition>
			<definition id="3">
				<sentence>The t'ormer is the modification of translation-in , popping its first element , and the latter performs the creation of another object , named translation .</sentence>
				<definiendum id="0">t'ormer</definiendum>
				<definiens id="0">the creation of another object , named translation</definiens>
			</definition>
</paper>

		<paper id="2176">
</paper>

		<paper id="1074">
			<definition id="0">
				<sentence>The parsing method has been implemented for different corpora , which exhibit very different linguistic styles : a corpus of commercial activities ( CD ) , in telegraphic style , a legal domain ( LD ) on taxation norms and lows , and remote sensing ( RSD ) abstracts .</sentence>
				<definiendum id="0">remote sensing</definiendum>
				<definiens id="0">a corpus of commercial activities ( CD ) , in telegraphic style , a legal domain ( LD ) on taxation norms and lows , and</definiens>
			</definition>
			<definition id="1">
				<sentence>Tile analyzer includes over 7000 elementary lemmata ( stems without affixes , e.g. flex is the elementary lemma for de448 flex , in-flex , re-fiex ) anti has been experimented since now on economic , financial , commercial and legal domains .</sentence>
				<definiendum id="0">Tile analyzer</definiendum>
				<definiendum id="1">e.g. flex</definiendum>
				<definiens id="0">includes over 7000 elementary lemmata ( stems without affixes</definiens>
				<definiens id="1">experimented since now on economic , financial , commercial and legal domains</definiens>
			</definition>
			<definition id="2">
				<sentence>An entry in the lexicon is as follows : lexicon ( len~na , stem , ending_class , syntactic feature ) where l emma iS the elementary lemma ( e.g. ancora for ancor-aggio ( anchor-age ) ) , stem is the lemma without ending ( ancor ) , ending_class iS one over about 60 types of inflections .</sentence>
				<definiendum id="0">stem</definiendum>
				<definiens id="0">len~na , stem , ending_class , syntactic feature ) where l emma iS the elementary lemma ( e.g. ancora for ancor-aggio ( anchor-age ) ) ,</definiens>
			</definition>
			<definition id="3">
				<sentence>`` Ihere are two group of such rules : ( i ) Rules to disambiguate ambiguous nounadjective ( N/Agg ) interpretations ( e.g. acid ) ( ii ) Rules to disambiguate ambiguous verb-noun ( V/N ) interpretations ( e.g. study ) One example of heuristics for N/Agg is : If N/Agg is neither preceded nor followed by a noun , or N/Agg , before a verb is reached , Then it is a noun .</sentence>
				<definiendum id="0">N/Agg</definiendum>
				<definiens id="0">two group of such rules : ( i ) Rules to disambiguate ambiguous nounadjective ( N/Agg ) interpretations ( e.g. acid ) ( ii ) Rules to disambiguate ambiguous verb-noun ( V/N ) interpretations ( e.g. study</definiens>
			</definition>
			<definition id="4">
				<sentence>Given an initial string NL_segment , BACKTRACK force the system to analyse all the possible solutions of the predicate LOOKRIGHT ( i.e. one-step rigth skips ) to derive all the N P N groups , headed by the first norm ( i.e. wl ) .</sentence>
				<definiendum id="0">LOOKRIGHT</definiendum>
				<definiens id="0">BACKTRACK force the system to analyse all the possible solutions of the predicate</definiens>
			</definition>
			<definition id="5">
				<sentence>Given the set I2 of all syntactically valid esl and the set m of esl derived applying SSA , the precision of the system can be defined as the ratio cardinality ( f2 m co ) / cardinality ( Q ) , while its recall can be expressed by : cardinality ( co n ~2 ) / cardinality ( ~ } ) , Global evaluations of the precision and recall are estimated by the mean values over the whole corpora .</sentence>
				<definiendum id="0">ratio cardinality</definiendum>
				<definiens id="0">evaluations of the precision and recall are estimated by the mean values over the whole corpora</definiens>
			</definition>
			<definition id="6">
				<sentence>The global rewriting procedure of SSA depends on the length n of the incoming text segment according to the following expression : *t i=l where e ( x ) is the cost of the application of a grammar rule , as for in Figure 1 , to a segment of length x. e ( x ) is easily seen to depend on : word ( e.g. noun ( w1 ) ) , whose cost is equal to that of a simple unification procedure i.e. `` t ; than `` ~*n , where n is the substring length .</sentence>
				<definiendum id="0">e ( x )</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the cost of the application of a grammar rule</definiens>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>The lexical base initially m ( xlelled provides a good coverage of character properties \ [ 3\ ] : using one view of the character base , the user may explore language-free morphology ( pictogram , stroke number , overall structural vignette , semantic radical , confusing sinfilarities ... ) and universal semantics , on the other hand -- m~d on the other view-an author can discover language-relevant morphological properties ( phonetic component ill its structural valency , homographs , positional wtriants for compound characters , use in composition ... ) , language-tied phonetics ( written and voiced pinyin and tone , homophones ... ) , h'ulguage-related semautics .</sentence>
				<definiendum id="0">language-tied phonetics</definiendum>
				<definiens id="0">provides a good coverage of character properties \ [ 3\ ] : using one view of the character base</definiens>
			</definition>
			<definition id="1">
				<sentence>The first prototype implemented the conceptmd m &lt; u.lcl of the property base as a highly structured hypertext , using llyperC , 'u 'd on Macintosh. We considered this platform a g ( ~ trade-off between a vahmble interaction management system , and a temperate framework for expressing the object-oriented and reusable view of the base. We then adopted a twofold development scheme , with both incremental prototyping ill llyperCard , for surface multimodal data and for tile interlace fimctional layer of the application , and parallel prototyping on different development platlk ) nrts , to explore different structuring and searching methods for tile character base. Three tracks were experimented to express character search modalities in more relincd interactive ways : first a Data Base Management System approach , flmmgh Iwo varianls : ( 1 ) ollject-oricnled modelling of tile base ( in tOOl 'S on a Xerox workstatioq ) , ( 2 ) a rehltional scheme on a standard medium ( an Oracle encapsulation in l lypercard on the Macintosh ) ; ( 3 ) various sketches of a knowledge base were prototyped in F'rolog with a simplified user interface. FLI~XIBLE EXPORAT1ON The prototype may here administer different users ( learners , didacticians ) with data protection , manage a standard static ch , 'u'acter properly base , maintain session journMs and user profiles , tracing their work in the base. There are two main access schemes to characters. Direct designation is based on a simple selection of the character icons Oil character boards. The surface organization of file base in character series or lessons matches here structural and pedagogical motivations initially expressed by a didactician ( Fig. 5 ) . 288 7bwards linguistic knowledge discovery assistants ... G. ~tfiotte &amp; F. Tcheou Multicriterial search ( MCS ) allows the user , starting from a partial description of the character , from tentatively diserimiuaut properties lie may recall , to refine or focus his request. Partially erroneous demands should be nmuaged adequately by the system ( while suggesting dcfault or alternate detenniuatious for dubious prol~ ) sals , or suppressing irrelevant ones ) . The elements remembered or ew~k~ by the learner are put forward in a criteria array orgimized in 3 lexical property subclitsses : l~teutial characteristics about thc sought character itself , about its semantic key , and about its phonogr , 'unme if it exists ( the phonetic marker component , very often structurally present ) . See Fig. 6. The grid presents main discrimiuaut criteria ( left ) and secondary characteristics ( right ) . In the MCS of complex characters , we put some emphasis on using structural vignettes and positional morphology aspects. Lolllll~rl 2/.a ( ~,4 r : h~raetera orprtmttlv~=t ) Fig 5 : l ) irect character selection ( series of characters with the semantic key MAN ) Meenhlg : Plnyln : Radlcal ? Yes NO Edlt a ~tructoro vlonotto : ~ILtm_cm t_l~ _ K , g_ g ttpnntnn : Plnylo : Position In the chnrt~cter : E~tlr~.~. ~ i'leanlng : PIngln : Position Ill tile charocter : Indlflororlt -\ [ Tone : Stroke Nur0hor : Select e ~lrtlcturo on the II~t : Nelghbour of form : IKo ! jwotd~ In Ilnemonlcs : \ [ ... .. lKeyword~ in P ... ... l Notos : ~0 ko~ In the II~t ! _~ Fig 6 : ll~e mtdticriteHal search grid Results of the sc~wch are available as character icons. 'lhe user may collect some of them ti~r later study. Direct observation of a chmracter , straightaway , is possible too. flexible session planning facility In its surface design , the standard main clmractcr base is originally segmented ill lessons , or collections , according to lteisig 's view of a pedagogical progression for a methodical discovery of kanji. On the I ) BMS driven prototypes , surface recontiguralion of the base is made possible , using in turn Ixlth multicriterial search or direct character collecting. It may allow dMacticians to express different views of the intrinsic property base , to restructure oh ; tractor lessons for pedagogic ; fl reasous , to elaborate alternate progression schemes ( involving for instance use frequency , series with a comnu ) n semantic key , series with a shared phonogrmnme , ctc ) . They arc here enabled to propose new palettes of 'predcfined lessons ' with alternate discovery paradigms or mnem ( mic systems related to various linguistic and cultural vicws , on which learners may express preferences its wcll. The learner himself may build and maintain one or several personal ( sub ) bases , or collections : series of characters that he selects using coherent criteria , which he plans to explore in future sessions , collections reflecting a personal thematic organisatitm of the discovery , simple reservoirs of characters built by free picking or rational collecting , through digressive or systematic navigation in the bitse. The leluner might here express , discover , refine , some personal discovery 'customs ' according to a cognitive style. Both types of users are thus allowed ( with appropriate access rights to such restructuriug resources ) to enrich file collection of existing views on the property basE , to edit and to reshape predcfiued lessons or collections into 'personal lessons'. Along with instant feedback and regular reviewing of tile actual work , the systenl here has some inccmivcs to more intention-driven , sell'guided learner activity , through sllort-tern~ session plmming and long-range curricnhmt self-organization. Case by case spontaneous consultatiou of lexical information is of course still advised. In our view such a function is essential on the way It1 inteq ) reting and rot×lolling user activity. On one of the prototypes , the tracing resources providcxl a first basis to el~d~ ) rate llexiblc , analytic and synthetic feed-back or witness functions for the user-discoverer interface , to build up an infommlion pool about user I ) chaviour mid discovery smltegies. While extracting data from file sessions base , we may sketch synthetic session journals , synthetic views on each character ( or character property ) for a user orI~lr all users. The DBMS view and the MCS ( MultiCritcrial Search ) were prototyped on diflerent development plat forms. a ) an Object Oriented prototyping ( LOOPS ) A COCOA rclcase of the AAOCC project was rewritten iu a homogeneous Object-Oriented fi'mne h ) r a co~pus of al× ) ut 100 characters. Results and pcrlonnances were quite encouraging , though on this small scale model. b ) a classical relational framework ( Oracle ) Wc also adopttal , on another prototype ( CACAO-4 ) , an integrative scheme merging two functionally specialized environments : IlypcrCard fi ) r multimodal interactive Ii'out-eud resources , and ml ( hacle kernel lor mmlagiug the bases and the user queries. We here aimed at exploring implementation paradigms for htrgcr scale character bases. The system first configurates entities in an Oracle property base , while extracting relevant data from the llyperCard hypcrdocument fields. At query time , arguments sent from the interface layer will generate SQL requests. System response is displayed back to the user , who then collects characters for litter use , or directly picks up n~ded multimedia data on the properties sought. ~lhough on a still small scale prototype , the relatioual DBMS scheme cased data security , coherence qualities , as well as some quautitative devclopmeut aspects. 2~9 Towards linguistic knowledge discovery assistants ... G. Fafiotte &amp; F. Tcheou IDENTIFICATION We prototyped a similar 'scale model ' corpus into smidl knowledge bases ( facts , structural aud other property rules ) , providing for deductive and explanatory fnuctious. Data-driven and goal-driven schemes were experimented in a small Expert Assistant for Character Identification. We try to initiate more interactive multicritcrial searches , while coupling very discriminant semantic characteristics ( the meanings of the expected character , of its semantic or phonetic keys ) , less selective indexiug ( stroke number , pinyin ... ) , with a tentative iconic specification of structural properties. When efficient criteria , are missing , we tbiuk such a visual structural recall to be helpful , with or without strong spatial positioning and applied to subcomponents with semantic or phonetic key functions , through a progressive opening or refinement of structural vignettes. Later the user should be able to express preferences regarding the prompting profile or the search strategies adopted by the Discovery Assistant ( I ) A ) in cases of underspecified or possibly erroneous queries. The DA could group results or hints , in order to prevent overstepped talkative dialogues. System explanation , if activated , could justify or illustrate side-hints with details. In the Annex example , the user tries to recall a ch~waeter , ~. , , , a morphological tree of which is shown below ( the semantic keys of the subcomponents being squared ) -- -but he actually knows very little of all this. ~ z~a~utb Fig 7 : A morphological tree for the character ILLUMINATI~ A first scenario ( see the Annex ) exemplifies a cooperative dialogue with a beginner user whose spontaneous search strategy will favour visual structural characteristics ( with no particular attention here to any phonetic unit in the churacter ) . The I ) A repeatedly asks for any semantic recall of another compouent ( a most diseriminant criterion ) , but the user decliues , lie is therefore asked and helped to visually refine the overall finally I : ~. structure , sucessively , ~ then , In a second scenario , the user soon proposes ( at dialogue step 2 ) a component kmfe ( ~ ) , which he figures to be present -- and which really belongs to the investigated character. With the current kauji base , adding this very discriminant element directly produces the unique final solution : to illuminate. In a third scenario , the user erroneously recalls ( at step 2 ) the possible presence of the radical strengh , ( ~ ) , instead of the very similar knife radical. The system will first exhibit , an empty respond , which is correct here. But knowing about this misleading similarity ( a 'faux , 'mils ' property ) , it then suggests chm~giug strength into knife. If the user acknowledges the proposal , the proper character to illuminate is reached right away. In the context of our prototyping effort , we would like ideally to design the application with a three-fold functional architecture : a highly interactive interface layer developed on an appropriate authoriug environment , for tile surface multim ( xlal representation of the lcxical lo : owlege , , an object oriented DBMS to express the core of the structural knowledge , to implement efficieutly heavy data searching and to structure and update the user history profiles mid personid bases , a declarative or deductive progriunming environment or expert system generator , in order to express both the strategy m ( xlels of a coached or error-compensatory multicritcrial character search , and first elements towards typed behavioural profiles and users ' discovery strategies. This could possit ) ly lead to 'client-server ' m'chitectures with distributed logical resources in the way of 'whiteboard ' schemes \ [ 8\ ] . lit seems that the iutcropcrability expected from multiplatform developmeut cnviroucmcnts will further such luuctionally distributed design. To summarize , we intend to develop the first draft of the exploration assistant , towards free surface resUucturing by the leander of his personal kuowledge base , according to a thematic or methodologic : d view he lkdlows , personal management -- -plauniug , monitoring and reviewing-of sessions or iuquiry sequences , synthetic or analytical follow-up of the discovery , working on a metaphor of the subbase bciug explored , with qualitative indicators on the actual navigation , productiou ( according to user preferences ) of session journals , profile status , global cun'iculum surveys , issuing of persoualized written , magnetic or audio documents , for remediation and iu-depth work. Lexical Knowledge Bases Our system can be viewed as a facet of a broader 'environment for an encyclop : edic discovery ' with other mcxles of activity : sell-review , semi-tutored lessons , where character thematic 'collections ' would drive the discovery. It would be desirable to be able to find , througb different views , in one and the same knowledge base , all the information that the I Ialperu Japanese-English dictionary \ [ 7\ ] offers , with the words built from clmraeters , Japanese pronunciation , a sound thesaun~s , the data of a large 'Chiuesc-usual language ' dictionary , character etymology \ [ 10\ ] , classical , usual calligrapby , a language-independent view of the hanzi / kanji , angmented with a progressive and comprehensive proposal of mnemonics in lteisig 's style \ [ 7\ ] , but culturally related to tile user 's native or usual language , the resources and mod : dities modelled in the AAOCC prototypes , for accompanied hyperdocumental navigation , expert character identification , for the creation and updating of pe~onal subbases or thematic character collections , among other features to appear. 290 7bwards linguistic knowledge discovery assistants ... G. 1 , '~ffi'otte &amp; 1 ' : Tcheott CONCLUSION We advocate the development tff system components lot helping authors to access the underlying liuguistic knowledge , mnong others in l : 'ei~onal or DBMT systems. Such Discovery Assistants ( DA ) slmuld certainly be highly cooperative , namely show sensible interactivity ( within multimodal hyperdocuments and object I ) BMS frameworks ) , provide some ways to tentatively adapt to users ' nmemouic and cognitive customs , and preleralfly first be user-tunable : i.e. they could offer means for the users themselves to refine and express their prcli : rences in terms of search strategies ( Slmnlaneous , self-phmned or coached ) , their planning intentions for a working sequence , as well ~ts means for an efficient follow-up of their activity. DAs should in our view rather first enhance Imth user 's natural intelligence towards more refleclive interactitm m ( ules , and user 's self-gui &amp; mce aptitudes. Iu the framework of a lexical PrOlverty base of hanzi / kanji , we have developed , as very first steps , .'t multiple prototyping of such functions , while exphMng object orienled , relalional , aud dcduclive ( rule-driven ) schemes. We expect progress from patient observation and modelling or user activity , and from the availalfility of multiplatform sol'tw~tre development tools , merging different classes of functionals , heading lowards Ixflymoqflfic or multiple-view knowledge bases. RlgFERENCES \ [ 1\ ] Boltet Ch. ( 19~ ) ) 7bwards Personal MT. '' on some aspects of the LII ) IA projecr Prc , ceedings of Coling-.90 , 08/90 , vol. 3/3 , pp30-35. \ [ 2\ ] l|oltel Ch. &amp; Blanchon 11. ( 1993 ) Dialogue based MTJor monolingual authors attd the LII ) IA project. Proceedings ( ~f NLPRS'93 , Fukuoka , Dec 6~7 1993 , wfl. 1/1 , pp2tJ8-222. \ [ 3\ ] li'aflotte G. ( 1990 ) A Self-IJmrning System Jot '' Chinese Characters. Proceedings of COLING 90 , IIelsinki , Aug 20~25 le~ ) 0 , II. Karlglvn ed. , ACL , vol. 3/3 , pp351-354 . \ [ 4\ ] Fafiotte G. ( 1990 ) Appretttissage assist ( par ordinateur des earact~res ehinois. Proceedings of 10~mes Journdes Internationales `` l.es systt~mes experts et leurs applications '' , Avignon , May 28-Jun 1 1990 , liC2 , vol. 8/8 , pp61-70. I5\ ] ( ; luse 1 ) . ( 1988 ) Conqtuter-Based hltelligent Tutoring for I : oreign lztnguages. Proceedings of Asia-Pacific Conference on Computers in Education , Shangtmi China , Oct 88. \ [ 6\ ] ilMperla J. ( 19911 ) New Japanese-l,2t~gli , rh ( ~taracter Dictionary. Kenkyusha , Tokyo , 1992 p. \ [ 71 tleislg J. W. ( 1977 ) Remembering the Kanji I ~ A complete course on Japanese characters. Japan Publications Trading Co , Tokyo , 2 vol. , 495 p. 181 Sellgm'an M. &amp; lhfitet Ch. ( 1994 ) 77w 'Whiteboard '' At ( 'hilet'lare : a way to itlteqrale helerogetteolts comltotletllS Of NLI ' systems. Proceedings of Coling 94 , Kyoto , Aug 5-9 1994. \ [ 91 , '' ; 6rzsset ( ; . &amp; Blanc 1~ ; . ( 1993 ) Utte approche par accq~tion pour les bases lexicah 's multilingues. Pro¢. of 1'.-'I'A-TAt ) '93 , Montrdal , Sep 3it-Oct 19 ! ) 3 , Ihfiversit6 de Monh'dal. \ [ 10\ ] Wleger l , . ( 1972 ) Caract~ ) res chinois. Etymologie. Graphies. l.exique , Kuangchi Press , Taichung. \ [ 11\ ] Yokol T , ( 1993 ) Very LatT , ,e-Scale Knowledge Bases Embodying Intelligence Space. Proceedings of KB &amp; KS'93 , Tokyo , Dec 1-2 1993 , JIPDI ; ; C , ppl 1-20. ANNEX An example of cooperative search , through stepwk~e structural vignette relinement ( First ~e~mrio ) `` lhe u~r interface was scl~matized here ft~ ' a more compact linear reading. I leading numlv , trs indicate dialognc stele ; . DA strolls for Di , ~overy AssistanL 'User : ' ann~xmces a seleetimlor an entry , 'I ) A -- &gt; ' a I ) A i~'ompting al~t 'DA. , . '</sentence>
				<definiendum id="0">MCS</definiendum>
				<definiendum id="1">MCS</definiendum>
				<definiendum id="2">LOOPS</definiendum>
				<definiendum id="3">empty respond</definiendum>
				<definiens id="0">tile character base. Three tracks were experimented to express character search modalities in more relincd interactive ways : first a Data Base Management System approach</definiens>
				<definiens id="1">learners , didacticians ) with data protection , manage a standard static ch , 'u'acter properly base</definiens>
				<definiens id="2">l~teutial characteristics about thc sought character itself , about its semantic key</definiens>
				<definiens id="3">synthetic views on each character ( or character property</definiens>
				<definiens id="4">the meanings of the expected character , of its semantic or phonetic keys ) , less selective indexiug</definiens>
				<definiens id="5">] , but culturally related to tile user 's native or usual language , the resources and mod : dities modelled in the AAOCC prototypes , for accompanied hyperdocumental navigation , expert character identification , for the creation and updating of pe~onal subbases or thematic character collections</definiens>
			</definition>
</paper>

		<paper id="1087">
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>Morpheme identification is an importmlt issue in some languages where two or more morphemes ave combined to make a word , a compound word , or a sentence without any delimiters between morphemes \ [ Abe86 , Chen92 , Paeh92\ ] .</sentence>
				<definiendum id="0">Morpheme identification</definiendum>
				<definiens id="0">an importmlt issue in some languages where two or more morphemes ave combined to make a word , a compound word , or a sentence without any delimiters between morphemes \ [ Abe86 , Chen92</definiens>
			</definition>
			<definition id="1">
				<sentence>Each character is a meaning unit and words are represented by the combination of characters .</sentence>
				<definiendum id="0">character</definiendum>
				<definiens id="0">a meaning unit and words are represented by the combination of characters</definiens>
			</definition>
			<definition id="2">
				<sentence>A written syllable is a combination of two or three sound symbols , which corresponds to a spoken syllable in a one-to-one fashion\ [ Chun90\ ] .</sentence>
				<definiendum id="0">written syllable</definiendum>
				<definiens id="0">a combination of two or three sound symbols</definiens>
			</definition>
			<definition id="3">
				<sentence>Suppose that X is a set of syllables that are used at the first position of grammatical morphemes .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a set of syllables that are used at the first position of grammatical morphemes</definiens>
			</definition>
			<definition id="4">
				<sentence>f : X -- -- .2 &gt; N fix ) = 27 ( CAi ( X ) *W ( i ) ) , where W ( i ) : :2 i 1 i Mot'l ) hological analysis system is formalized as a function F. Tile domain of function F is a set of words and the range of F is a Cartesian l~roduct of a set of morl~hemes and their morl ) ho-synlactic features .</sentence>
				<definiendum id="0">CAi</definiendum>
				<definiendum id="1">W ( i )</definiendum>
				<definiendum id="2">F</definiendum>
				<definiens id="0">a Cartesian l~roduct of a set of morl~hemes and their morl ) ho-synlactic features</definiens>
			</definition>
			<definition id="5">
				<sentence>~uffix-n ) 1 ) res , llat~l , ful : , Ill ) , hen ... . : : ( -~ , ) A syllable-based rule consists of loft-hand side ( IA\ [ S ) and right -- hand , ~ , ide ( l { \ [ IS ) .</sentence>
				<definiendum id="0">syllable-based rule</definiendum>
			</definition>
			<definition id="6">
				<sentence>CAr ( sill ) = 1 , head &lt; -- subsyl ( word , 1 , i-i ) , change ( head\ [ i-i\ ] , null , 'p ( tl ) ' , FINAL ) , verb ( head ) &lt; -IRREG_B tail &lt; -- subsyl ( word , i , n-i-l ) , change ( tail\ [ I\ ] , 'we ( M ) ' , 'e ( q ) ' , M~maO The b-irregular rule is described as a syllable-based formalism and it is applied after the isolation of stem parts .</sentence>
				<definiendum id="0">CAr</definiendum>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>mes c ( msisting of e ( ) nSollatlts att ( l pattern morphemes consisting of vowels and affixes .</sentence>
				<definiendum id="0">l pattern</definiendum>
				<definiens id="0">morphemes consisting of vowels and affixes</definiens>
			</definition>
			<definition id="1">
				<sentence>A verb , such as /nkutib/ ( form 7 ) , has the lexical entries n E1 u El i El where El is the alphabet of the root and E~ the alphabet of the vocalism/affixes .</sentence>
				<definiendum id="0">El</definiendum>
				<definiens id="0">the alphabet of the root and E~ the alphabet of the vocalism/affixes</definiens>
			</definition>
			<definition id="2">
				<sentence>\ [ Black et al. 1987\ ] pointed out ttmt previous two-level rules ( cf. , ~a.1 ) affect one character at a time and proposed a formalism wtfich maps tletween ( equal ram &gt; bered ) sequences of surface and lexical characters of the form , SURF ~ LEX alnidal consonant clusters , CC , take a prosthetic /Pi/ .</sentence>
				<definiendum id="0">SURF ~ LEX alnidal consonant clusters</definiendum>
				<definiendum id="1">CC</definiendum>
				<definiens id="0">cf. , ~a.1 ) affect one character at a time and proposed a formalism wtfich maps tletween ( equal ram &gt; bered ) sequences of surface and lexical characters of the form</definiens>
			</definition>
			<definition id="3">
				<sentence>182 This work follows \ [ Kay 1987\ ] in using I ; hree I , apes l ) ) l '' the lexical level : pattern tape ( PT ) , root tal ) e ( liT ) and voeallsm tape ( VT ) , and &lt; m &lt; : sm'face , tape ( ST ) . Ill syntliesis , the lexical tapes are in read mode and the surface l ; aI ) e is in write mode ; in recognil ; ion , the opposite state of affairs holds. One of the lexieal tapes is called the prhnary lexieal tape ( PILF ) through wtfieh all lexical morphentes which fall out of the donlain of rool ; -and-pattern morl &gt; hology are passed ( e.g. pretixes , sutlixes , I~artic : les , prepositi &lt; ms ) . Since char : acters in P'.I ' correspond to those on ST , P'F was chosen as PLT. There is linguisti &lt; : SUl ) pnrt for n lexical l.apes maI ) l ) ing to &lt; ) ne surface tape. As described } ) y \ [ McCarthy 1986\ ] , when a word is uttered , it is pronounced in a linear string of segmmits ( eorrespondinf , ; to the linear ST in this model ) , i.e. the multi-tier representatioll is linearised. McCarthy ealls this process tier eonllation. ism The l'ulnuul-Ih ; pl ) le/lhmssink/lllaek ct aL fornialisnl is adopted here with l ; wo extensions. The first extension is that all expressions in the lexical side of ( .he rules ( i.e. LLC , LBX and RLC ) are n-tuple regular expressions of the form : ( ; 1~1 ) ; i ; 2 ) • • , ) `` lT~'t ) If a regular expression ignores all tapes lint Pl ; I ? , the parentheses can 1 ) e ignored ; hence , ( x ) is the sanlt ! ; ts : . ) : where x is on PIfF. llaving n-tuI ) le lexical exI ) r ( ! ssions and 1-tuple surface expression corresponds to having n-tapes on the lexieal level and one ( ) it the surface. The second extension is giving LI , C the ability to contain ellipsis , ... , which indicates the ( ol ) tional ) omission li'om LLC of tvples , provided that the t.uples to tlt ( : left of ... are the first to apl &gt; ear Oil { ; h ( !</sentence>
				<definiendum id="0">lexical level</definiendum>
				<definiendum id="1">ion</definiendum>
				<definiendum id="2">PILF</definiendum>
				<definiens id="0">pattern tape ( PT ) , root tal</definiens>
			</definition>
			<definition id="4">
				<sentence>- ( Pl , Xl , ) -v I - ( \ [ ~ , X2 , ) X = vowel , P1,1~ C { el , &lt; , , c : ~ , c4 } , Xt , X ) = radical ( I7 ) Not : e thai , the segments in SIJItF iIl the above rules do not appear in LI. ; X , rather in L\ [ , C. This means \ [ ; hat , if rllles are to } ) e eoml &gt; ile &lt; l ill { ; ( ) alltolllata , the alll ; Omata } lave t ; o rcmember i ; he segments from LLC. 9 This leads us on thinking about what sorl ; of allI ; Olllal ; a are needed to describe a mull , i-tape two-level grammar. We define the following antomat , ou iul ; o which rules can he cmnpiled : A multLtape f-register auxiliary finite-state automaton ( AFSA ) with n-tapes consists of : n read tapes and heads , a linite state control , and a readwrite storage tape of length g , where f &lt; w , and w is the length of the inlml ; strings ( of. APDA in \ [ I\ ] opcrofl. and Ulhmm 1979\ ] ) . The auLomal ; on is illustrated iu Fig. 5 ( next page ) . I° In cme mow~ , depending on the state of the finite control , along with the symbols scanned by the input aml storage heads , the AFSA may do any ( n ' all of the following : 'qlf the h'aph ! mental , ion works dh'e ( % ly on ru\ [ es~ this can he achieved by unification. lI ) ~ : : : A ill LhO dla , P ; rRHL 184 Fig. 5 input tapes AFST ... ... . F II 7_Jt172 storage • clumgc ~ state ; • mow~ its ~t input heads independently c , n , :~ l ) osil.iou to the right ; • print a symbol on the coil scanned by the sLot'age head and ( optionally ) move that ; head ont , l ) osition to the right or loft. More fern , ally iLI/ AFSA is a se.xtui ) lo of tim fOl'lli ( Q , ) ; , F , 6 , q0 , F ' ) , whore : • Q is a finite sot ; of states ; • E is the machine 's alphabet ; • it ' C ) \ ] is the storage alphahot ; • ~ $ is the transition function , a map from Q × a x F t , o Q x I ' x { L , / { } , where o '' is ( al , ... , o , , ) and a i C Y ; ; • qll El Q is t.h , ' , initial sl.~tl.e ; • 1 , ' C Q is the. sot of final st ; ares. The transil ; ion function a ( l , , ~ , r ) -= ( q , , ,. , . , ) iff t.he machine emt move from state p to state q wlfile s ( : antfin Z the n-tuplo cr from the input tapes and r from the current storage cell , and upon ente.ring state q , writes the symbol w onto the. cllrrent sl , or ; Lg ( 1 cell ; m ( I moves the storage head according to m E { L , l~ } . A multi-t : ape , t ? -reglstm ' auxiliary finite-state transducm ' ( AFST ) wit ; ll n inlmt tapes and k outlntt tapes is ml AFSA with ( t+ + k ) -tapos. AFSTs lw.httvo like AFSAs , but scan t.uple pairs. Note that an AFST with n = k -= I and ~ ? = : 0 is equivalent to a. FST. The rules are comIfiled into AFSTs in the same lines of standm 'd two-level morphology. We shall ttso. a special ease of AFSTs : We hypothosise that , in lilms with tie.r confl : +A : ion , for all tnortJtcJogical processes , k=l ( i.o. on ( '. surface tape ) ; further , wo . : msmno l , hat , m &gt; less one proves otherwise , all morphological processes require that f &lt; 1 ( hence , we shall ignore m in a ) .</sentence>
				<definiendum id="0">Omata }</definiendum>
				<definiendum id="1">AFSA</definiendum>
				<definiens id="0">-v I - ( \ [ ~ , X2 , ) X = vowel , P1,1~ C { el , &lt; , , c : ~ , c4 } , Xt , X ) = radical ( I7 ) Not : e thai , the segments in SIJItF iIl the above rules do not appear in LI.</definiens>
				<definiens id="1">segments from LLC. 9 This leads us on thinking about what sorl ; of allI ; Olllal ; a are needed to describe a mull , i-tape two-level grammar. We define the following antomat , ou iul ; o which rules can he cmnpiled : A multLtape f-register auxiliary finite-state automaton ( AFSA ) with n-tapes consists of : n read tapes and heads , a linite state control , and a readwrite storage tape of length g , where f &lt; w , and w is the length of the inlml ; strings ( of. APDA in \ [ I\ ] opcrofl. and Ulhmm 1979\ ] ) . The auLomal ; on is illustrated iu Fig. 5 ( next page ) . I° In cme mow~ , depending on the state of the finite control , along with the symbols scanned by the input aml storage heads</definiens>
				<definiens id="2">~t input heads independently c , n , :~ l ) osil.iou to the right ; • print a symbol on the coil scanned by the sLot'age head and ( optionally ) move that ; head ont , l ) osition to the right or loft. More fern</definiens>
			</definition>
			<definition id="5">
				<sentence>{ lq ) olo } on one tape and the tonal morphemes { L } , { M } , { LH } and { tI } on a second tape with the lbllowing rules : *-C-* = &gt; *-C-* ( 18 ) *-V-* : ~ * -- V-* ( :19 ) *-T -- * ~ : ; , ( V , ) - ( , T ) -* ( 20 ) where C is a consonant , V is a vowel and T is a tonal segment ( these rules are for the al ) ove data only ) .</sentence>
				<definiendum id="0">C</definiendum>
				<definiendum id="1">V</definiendum>
				<definiendum id="2">T</definiendum>
				<definiens id="0">a consonant</definiens>
				<definiens id="1">a vowel</definiens>
				<definiens id="2">a tonal segment</definiens>
			</definition>
</paper>

		<paper id="2174">
			<definition id="0">
				<sentence>Texts vary along st ' .</sentence>
				<definiendum id="0">Texts</definiendum>
				<definiens id="0">vary along st '</definiens>
			</definition>
			<definition id="1">
				<sentence>`` Miscellaneous '' , the most problematic category , is a loose grouping of different informative texts .</sentence>
				<definiendum id="0">most problematic category</definiendum>
				<definiens id="0">a loose grouping of different informative texts</definiens>
			</definition>
</paper>

		<paper id="1082">
			<definition id="0">
				<sentence>LIIIP provides a processing method wlfich allows selected portions of the input to be ignored or handled differently .</sentence>
				<definiendum id="0">LIIIP</definiendum>
				<definiens id="0">provides a processing method wlfich allows selected portions of the input to be ignored or handled differently</definiens>
			</definition>
			<definition id="1">
				<sentence>Span : The span of a grammar rule R is the length of the longest island ( tl , ... tj ) such that ternfinals tl and t i are both consumed ( directly or indirectly ) by R. Cow .</sentence>
				<definiendum id="0">Span</definiendum>
				<definiens id="0">The span of a grammar rule R is the length of the longest island</definiens>
			</definition>
			<definition id="2">
				<sentence>Heads serve as anchor-points in the input string around which islands may be formed , and are accordingly treated before non-head items ( RHS items are re-ordered during compilation-see below ) .</sentence>
				<definiendum id="0">Heads</definiendum>
				<definiendum id="1">RHS items</definiendum>
				<definiens id="0">serve as anchor-points in the input string around which islands may be formed</definiens>
			</definition>
			<definition id="3">
				<sentence>LHIP graxnmars are an extended form of Prolog DCG graznmars .</sentence>
				<definiendum id="0">LHIP graxnmars</definiendum>
			</definition>
			<definition id="4">
				<sentence>A LtlIP granunar rule has the form : lhiVrute ~ \ [ \ ] term \ [ # T \ ] ~~__~ &gt; U~i~body where T is a value between zero and one .</sentence>
				<definiendum id="0">LtlIP granunar rule</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">a value between zero and one</definiens>
			</definition>
			<definition id="5">
				<sentence>A rule name is a Prolog term , and only rules and terminal items may act as heads within a rule body .</sentence>
				<definiendum id="0">rule name</definiendum>
				<definiens id="0">a Prolog term , and only rules and terminal items may act as heads within a rule body</definiens>
			</definition>
			<definition id="6">
				<sentence>s ( J , A. P , C , ~ , E , U , s-El , _ ) , F is U+Q+T , F/ ( E-D ) &gt; =M. The important points to note about this convetted form are the following : fore either of the two s clauses ; conjunction clause is the stone as that for the rule 's LIIS ( B-C ) : its island extends from 0 to p and covers Q items ; ( i.e. from tile start of tile LHS search region to tile start of the conjunction island ) , its island starts at D and covers T items ; P-C ( i.e. from the end of the conjunction island to the end of the LIIS search region ) , its island ends at E and covers II items ; extends from D to E and covers F items , whereFisU+ Q + T ; ment M with the current global threshold value .</sentence>
				<definiendum id="0">F</definiendum>
				<definiendum id="1">F/</definiendum>
				<definiendum id="2">conjunction clause</definiendum>
				<definiendum id="3">P-C</definiendum>
				<definiens id="0">its island extends from 0 to p and covers Q items ;</definiens>
			</definition>
			<definition id="7">
				<sentence>Ps is the set of parses of S by C with coverage Coy .</sentence>
				<definiendum id="0">Ps</definiendum>
				<definiens id="0">the set of parses of S by C with coverage Coy</definiens>
			</definition>
			<definition id="8">
				<sentence>lhip maxT_phras os ( +C , +S , -MaxT ) MaxT is the set of parses of S by C that have the tfighest threshold value .</sentence>
				<definiendum id="0">MaxT</definiendum>
				<definiens id="0">the set of parses of S by</definiens>
			</definition>
			<definition id="9">
				<sentence>( -1 ) \ [ 1 -- 2 ) /1 `` ' &gt; ©have ( -1 ) \ [ 8 -- 9 ) /1 `` ' &gt; ©that ( 4 ) \ [ 2 -- 8 ) /4 -- &gt; np ( nppp ( you , pp ( by , np ( the , brook , J ) ) ) ) ( 4 ) \ [ 3 -- 8 ) /5 `` ' &gt; np ( nppp ( np ( the , tree , I { ) , pp ( by , np ( the , brook , I ) ) ) ) ( 5 ) \ [ 3 -- 8 ) /2 `` - &gt; np ( np ( the , brook , K ) ) yes Here , two unattached lexical items have been identified , together with two instances of rule 4 , which combines a NP with a postmodifying PP .</sentence>
				<definiendum id="0">np</definiendum>
			</definition>
			<definition id="10">
				<sentence>It combines the advantages of Prolog-interpreted DCGs ( ease of modification , parser output suitable for direct use by other programs , etc. ) with the ability to relax tile adjacency constraints of that form &amp; llsm in a flexible and dynamic manner .</sentence>
				<definiendum id="0">Prolog-interpreted DCGs</definiendum>
				<definiens id="0">the ability to relax tile adjacency constraints of that form &amp; llsm in a flexible and dynamic manner</definiens>
			</definition>
</paper>

		<paper id="2201">
			<definition id="0">
				<sentence>The Lambek calctflus Our general fran , ework is the associative I , ambelc calculus ( L : l , mnbek , 1958 ) , a system which falls within the class of formalisms known as Categorial Grammars .</sentence>
				<definiendum id="0">Lambek calctflus Our general fran , ework</definiendum>
				<definiens id="0">the associative I , ambelc calculus ( L : l , mnbek , 1958 ) , a system which falls within the class of formalisms known as Categorial Grammars</definiens>
			</definition>
			<definition id="1">
				<sentence>, i.e. where • is an associative , notl-coulmtltative binary operator , with two-sided identity e , and E is the set Of non-eHlpty ( -7 t : g ) strings over some vocabulary .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">the set Of non-eHlpty ( -7 t : g ) strings over some vocabulary</definiens>
			</definition>
			<definition id="2">
				<sentence>An exh'aclion fimctor X\ ] Y is one whose argument corresponds to a non-peripheral ( or more precisely , nor necessaribj peripheral ) suhstring of the result of con &gt; binaries , as it , ( lc ) .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">a non-peripheral ( or more precisely , nor necessaribj peripheral ) suhstring of the result of con &gt; binaries</definiens>
			</definition>
			<definition id="3">
				<sentence>'lPhis system allows versions of T and ~ operators to be specified , hut ones whose interpretive definitions differ from Moortgat 's .</sentence>
				<definiendum id="0">'lPhis system</definiendum>
				<definiens id="0">allows versions of T and ~ operators to be specified , hut ones whose interpretive definitions differ from Moortgat 's</definiens>
			</definition>
			<definition id="4">
				<sentence>( ~ X ' , v. ( ~v ) | ) `` This system can be extended to cover product using the inference rules ( 3 ) , and the additional ) 2 elanses shown following ( with the obvious implicit extensions of the directional lambda system , and of Buszkowski 's semantics ) .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the obvious implicit extensions of the directional lambda system , and of Buszkowski 's semantics )</definiens>
			</definition>
			<definition id="5">
				<sentence>Quantifiers behave distributionally like simple NPs , but semantically are of a higher type .</sentence>
				<definiendum id="0">Quantifiers</definiendum>
				<definiens id="0">behave distributionally like simple NPs , but semantically are of a higher type</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>The lexical meaning representations or semantic markers in Swetra tire of the form re ( S , G ) , where m denotes meaning , S is the main meaning of the word denoted by a kind of Machincse English and G the grammatical meaning .</sentence>
				<definiendum id="0">m</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">the main meaning of the word denoted by a kind of Machincse English and G the grammatical meaning</definiens>
			</definition>
			<definition id="1">
				<sentence>The representation lm ( oblige , nonf ) , re ( passive , pres ) \ ] gives is obliged to as a synonym as is generally suggested in grammars .</sentence>
				<definiendum id="0">representation lm</definiendum>
				<definiens id="0">generally suggested in grammars</definiens>
			</definition>
			<definition id="2">
				<sentence>The need for attention to negation becomes even more conspicuous when considering tile effects of the interplay between negation and aspect in translation between Russian and English ( cf. Isa~enko 1962 : I98 ) : a. nado vernut ' knigu 'must ' relurn-perf book-ace 'one has to return the book/lbe book must be returned ' b. nado vozvra , ~at ' knigi 'must ' return-imp books-nom/acc 'one ought to return books ' c. ne nado vozvrag~ : at ' dtu knigu neg 'must ' return-imp tiffs book-ace 'one does not need/have to return tills book ' Tile problem of translation between English and Russian can be solved by lexically encoded negation and aspect control , according to patterns like the following : elex ( lm ( oblige , nonl ) , re ( passive , pros ) \ ] , v , aux , fin , _ , 1 , inf , i , 1711 ) -- &gt; \ [ must\ ] .</sentence>
				<definiendum id="0">re</definiendum>
				<definiens id="0">passive , pros ) \ ] , v , aux , fin , _ , 1 , inf</definiens>
			</definition>
			<definition id="3">
				<sentence>restruct ( A , B ) : A= \ [ subj ( N ) , pred ( \ [ m ( X , non0 , Im ( Epist , _ ) l , re ( passive , T ) \ ] \ ] ) \ ] , % N may come B=\ [ subj ( impers ) , pred ( lm ( Epist , _ ) , re ( passive , pres ) \ ] ) , obj ( lsubj ( N ) , pred ( m ( X , T ) ) \ ] ) I. % It is l~ossible Ih ( lt N comes In translation from and into Russian , there is a need for rendering an impersonal construction , like nado vernut ' knig , , into an English or Swedish construction with an overtly realized subject ( E. The book must be returned , S. Boken m &amp; 'te liimnas tillbaka ) .</sentence>
				<definiendum id="0">B )</definiendum>
				<definiens id="0">passive , pres ) \ ] )</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>b. Whiteboard approach Igor each object to be hanslated , the LJDIA Kernel creates a mirror object ( it text file ) in which arc sto , ed all information required by lhe lranslalion process aml necessary for Ihe construction of tile target stack .</sentence>
				<definiendum id="0">LJDIA Kernel</definiendum>
				<definiens id="0">creates a mirror object ( it text file ) in which arc sto , ed all information required by lhe lranslalion process aml necessary for Ihe construction of tile target stack</definiens>
			</definition>
</paper>

		<paper id="2138">
			<definition id="0">
				<sentence>PRTN is a RTN with probabilistic transitions and words 1 that can be estimated from sample sentences by means of statistical techniques , we present a reestimation algorithm for obtaining the probabilities of transitions and the observation symbols ( words ) defined at each terminal transition .</sentence>
				<definiendum id="0">PRTN</definiendum>
				<definiendum id="1">observation symbols</definiendum>
				<definiens id="0">a RTN with probabilistic transitions and words 1 that can be estimated from sample sentences by means of statistical techniques</definiens>
			</definition>
			<definition id="1">
				<sentence>, ~t is a slight wriation of Inside probability in which PI ( f ) =~b 'S in the Inside probability formula are replaced by P~ ( f , i ) a~b. \ [ ts actual computation is done as follows : PI ( f ) , ~t ifs_ &lt; t , 1 ifs &gt; tandf=i , 0 ifs &gt; t and f ) Ai .</sentence>
				<definiendum id="0">~t</definiendum>
				<definiens id="0">a slight wriation of Inside probability in which PI</definiens>
			</definition>
</paper>

		<paper id="2125">
			<definition id="0">
				<sentence>The inte~opretation of noun sequences ( henceforth NSs , and also known as noun compounds or complex nominals ) has long been a topic of research in natural language processing ( NLP ) ( Finin , 1980 ; Sparck Jones , 1983 ; Leonard , 1984 ; Isabelle , 1984 ; Lehnert , 1988 ; and Riloff , 1989 ) .</sentence>
				<definiendum id="0">inte~opretation of noun sequences</definiendum>
				<definiens id="0">henceforth NSs , and also known as noun compounds or complex nominals ) has long been a topic of research in natural language processing ( NLP ) ( Finin , 1980 ; Sparck Jones , 1983 ; Leonard , 1984 ; Isabelle , 1984</definiens>
			</definition>
			<definition id="1">
				<sentence>Matching is a general procedure which returns a weight to reflect how closely related two words are , in this case how related the value of an attribute is to a given lemma .</sentence>
				<definiendum id="0">Matching</definiendum>
				<definiens id="0">a general procedure which returns a weight to reflect how closely related two words</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>Machine Translation ( MT ) is considered the paradigm task of Natural Language Processing ( NLP ) hy some researchers because it combines almost all NLP research : treas : syntactic parsing , semantic disambigt , ation , knowledge rel ) reseutation , language generation , lexical acquisition , and morphological analysis and synthesis .</sentence>
				<definiendum id="0">Machine Translation</definiendum>
				<definiens id="0">considered the paradigm task of Natural Language Processing ( NLP ) hy some researchers because it combines almost all NLP research : treas : syntactic parsing , semantic disambigt , ation , knowledge rel ) reseutation , language generation , lexical acquisition , and morphological analysis and synthesis</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>o , JN+a+b &lt; ( A &lt; D/Nom/Adi ) p , .o , ~ &lt; I IILMI , &lt; N+d_~ , /N_~+~ &lt; &lt; ( N~o , ,/N+a+~ ) +fo~/ ( A &lt; D ) , ,~o , ~+to~ , .~ &lt; ( A &lt; D ) -I-~+ &lt; ~ &lt; G , , , . ... . &lt; N_d_~ &lt; ( A &lt; l ) ) + , e_~ &lt; &lt; M~ , .~o , ,~ ( ~_~s ) &lt; M , it ( , ~ , , ) _.m ) &lt; mneo ( .ll ) &lt; \ [ Mmod ( ,12-.m ) &lt; &lt; POp , .o , ~ &lt; ( a &lt; I ) ) _~t_ , &lt; 17 ' ( ) .j.d_ , .~ &lt; PO+a_~ , &lt; P ( ) - , ~+ , ~ &lt; P ( ) - , ~-~ &lt; ( ~I , ~o , ,~ &lt; &lt; &lt; &lt; SIT/DIR/I , ; XP &lt; ( Nom/Adj ) _ , , , .o , ~ &lt; ( N/A/D/G/I'O ) svc , Table 1 : ~Thematieally-tagged ' Canonical Form for German Before showing some example sentences generated by ~his CF~ we have to mention one particularity of German , which is that the verb is in second position in declarative matrix clauses ( verb-second , or V2 position ) , and in final position in subordinate clauses ( verbfinal , or VI e position ) . Nearly any element can take the one position preceding the verb in V'2 , , -ailed the. Vo~J'd~t ( `` p , 'e- ( verbal ) field ' ) . Normally a thematic element is placed into the Vorfeld. According to IIoberg 's ( 1.981 ) analysis of the Mannheimer l ) 'uden Korpus , in 63 % of al\ [ V2 sentences the nominative complement ( sub jet6 ) takes this place. A convenient way of seeing it is that all elements fol|ow the. verb in V2 position according to tile CF , and that one ( thematic ) element is moved int ; o the Vorfeld position. We suggest that if the analysis of the source language fails to recognise the theme of the sentence , the subject takes this place. In our model , most elements can cith ( , r I ) e thematic , rhematic , or neutral ( i.e. unnmrked with respect to theme and theme ) . Sent ( ? nce variations as different as shown in the examples 5a to 5d can be generated using tim canonical form presented above , depending on t ; he parameterisation of the features theme , rheme and focus for the different constituents. The order of elements in 5a corresponds to the defaull , order. Itowever , the same order would be general ; ed if the personal pronoun was marked as being thematic , and/or if the adverb gest.ern was rhematic. We put the information -t-theme in 5a to 5e in brackets to indicate that this feaLm'e is not a requirement to generate I , he respect ; ire word orders. The relaJ ; ive order el ' the adverb and the accusative NP in 51 ) difhn 's fi'om the one in 5a , becaus ( `` I , he object den Mann is rhematic. In 5c and 5 ( I , 9estcr'n and den Mam~ arc thematic , respectively , in ad-dition t ; o this , the persorm.l pronoun in 5 ( l is marked as being stressed contrastiw'.ly. We used eapii ; al letters 1 , o express the obligatory h~cus , ll ; is easy to think of more phrase order ' combinations caused by further parameterisalions. 5a lch ( +o~ , ,~. ) habe den Mann ge.sl , ern~ ( + , .h~ , ,~ ) gesehen. ( A+a+~ -Mun ) I have lhe man 9eslerdaj/see~ 51 ) Ich ( +0~e , ,~e ) babe gest ; e.rn2 ( i den Mallllq. , .he , n e gese\ [ lell. 1 have yeslerday Ilze mm~ seen 5 ( : ( ' , OSlx~.rl126+theme hill ) ( ? i¢ ; 11 deal Mallll ( _brheme ) gesehen. Yesterday have 1 th ( man seen 5d Den M ann+the , ,~e hal ) e , ge.s t e r n : ~_I_ t h ~m e. \ [ ( 21\ [ +\ ] o~u~ gesehen. The ma 'll , have yesterday I seem Modi tiers shou l &lt; l 1 ) e classified according to ltoberg 's ( 71981 ) d4 modifier position classes , which partly coincide with the common semant , ic classi\ [ ications , and partly not. Ilobcrg 's modifier indexes are l ; he r ( .'sult of the stal ; istical veril~cai , ion of lintel s intuitive classes ( 1970 ) . As modifiers do not alw~ys follow in l , he same order , ltoberg chose a classl fication which lead to least deviations between her cla.ssiflcation 72 and i ; he order in the corpus used ( Mannheimer Duden Korpus ) . The following sentet~ces exemplify the order of Lhe CI ~ for modifiers : 6a~ Ich babe deshalb.,2 gestern. , a mit Wolf.v2 fin'nge.sehen. I have therefore yester'day wilh Wolf watched-to 6b lch habe deshalb.22 ntit ~ , ~701f4.2 gest.ernu~+ , .h~ , , , e fern gesehen. I have therefore with Wolf yesterday watched-iv 7 \ ] ) amals2~+th~m~ bin ich l , 'rauen ohnehin9 ofl,37 iiberstiirzt~a davongelmffen. Then am I women anyway often o'verhastyly ranaway ( Then , I often ran away fl'om women overhastily anyway ) l ) ue to the procedure descril ) ed iu this section , ungrammatical sentences such ~s 2c and 2 ( 1 c~m be. ~voided successhflly. ~l'he generation of contextultlly embedded sentenees is based on the succoss\ [ lll analysis of l ; heme ~md rheme constiLuen~s. U'he recognio Lion of contrasLive sl ; ress is even more imporl , anl , . A basic fa ( : l ; l , hat can be used h ) r the m~tomlttie recognition of these cal , egories is i ; lt~L not only the conl , ext determines the orde.ring of constituents in m~ eml ) edded seng ( .'nc ( ' , but ; also ~ given se.ntence carries inforrn ; tl , ion on Clte contexL to which it 1 ) elongs. When Clerrna.n n~l , ive spe~tkers see ( , he sentence 3 ; ~/dl ) ~ for instance , Lhey h &amp; ve ~t st ; tong feeling a|toul ; the context ; in which it occurs. It is very liko.ly thai ; 1 ; 11 ( ; NP ihr'en Mann is stressed , ll ; is e.ither rhenutLic , or it c~trries contra.stive focus , le is even more restricted. 'Che personM pronoun ich must be contrastively stressed ( I ~tzyselfam the person who visits him ) . in every conl , ext requiring another stress , le is ungr~mmu~tica\ ] , \ ] I ; is l , hus possible to extr~tcL inform~tlfion on the context of ~ given seTiLen ( ; e , wil , hout halving ~ccess to the prec ( ; ding se.nLences. Analysis grammars must , allow mosl ; constituent order w~ri ; ~t ; ions , its the number of phrase orders theft c~m be excluded is very limited. q'he diiDrence with generation gr~m~mm 's is l ; h~tL it is suttqcient to generate one 'goo ( t ' phr~tse order for e ; ~ch context , whereas in ana.1ysis all possible vari~ttions h~ve to \ ] ) e ~dlowe. ( t. For this red , son , ~he CF is of no use~ for ~malysis. hlstea.d , mlMysis grammars should Mlow ~dl gramm~tic , M orders ~md ide.niAfy / , hem~tic , rhemal ; ic mld focussed I ) hri~ses. In our : tlgovithm , the number of possil ) le thenms lind rhe~mes is limited to on ( : constituenl , cinch , as l , his is sufficient Co generate l , he. w~ria.tions in 5 to 7. Firstly , focus should 1 ) e identified , a.nd ~l ' { ; er this theme ~n ( l rhenm. Some pe.rmul ; ~l , ions are. only possible if one consLitucnl ; is stressed conLrastively. These construcl ; ions include l ; he V-orJ'eld posit ; ion of some i ; yl ) i ( : idly rhern~t , ic elernerfl , s ( 8 , 9 ) , l , he right , movemenl ; of ( : onstil ; uerlts which h~we a. strong t.ender~c : y I ; o ( , he left ( of. 1 ( '. mt ( l 5 ( l altove ) , ~md ol.hers ( SI , einberger 1.99,1 ) . To l , 'raT~ee is Vah/~ Jle 'm ( Vah.g flew to l+a'uce ) 9 l ! finen INder+io , : , ,. , Iml , Anne geheir~lxd ; . Au Indian has ATJne married ( , 'ln~e has married an htdian ) In i , he nexl ; step , i , he theme category is iderfl , ified. \ ] ' ; v ( ery element i~l , the I ) eginrting of the chmse is marked i~s ~ the.me if i ( , has not 1 ) e.en idenLified as ~ focus in Lhe preceding sl ; ep ( J0 , 11 ) : 10 I ) mlmls+tu~ , ,~ le/ ) t , e. \ [ lendrix noch. Then lived llendri , '' still ( llendrix was still alive the.u ) I1 lch glauhe , .dal3 'l'ina+u , ~ , ,~ ofl , koe.hl ; . 1 believe lhal `` l'i , ~a often cooks Simil~ , ' t , o lla.jig : ovd , el , a.l. 's ( 1993 ) suggestion for I ) ; nglish , and I , o Mi~Lsul ) a.r~ el , al. 's ( 1993 ) for .la.l ) amese , tim h~sL ( -ollsLiLuent of the senl , ence will l ) e re ( 'ognised its rherru~tic , its rllemes Lend to occur sc'ntence-fina~lly ( cf. 5 ; ~ and 61 ) ) . Our approach differs from tllkii~ : ovA et a.l. 's , howe.ver , in theft we prohibit some ele.ments from 1 ) eing rho.m~tie. In Germ~m , 1 ; hese inhere.rH , ly nou-rhemi~t , ic eleme.rM ; s include personM pronouns , as we 'll as a limited set of too ( lifters such as 'wohl in 12. Although some modifier groups tend to be potential rhcmes , m~d ot , hers do n ( ) t , mosL modifiers muM , b ( : coded individually in thel dictionary ( Slx'AnI ) erger , 1994 ) . Not ( '. I ; h~l ; ' if inherently nonr\ ] lem &amp; tic elemenLs occur seml ; e~n ( : e-ihmlly , it , is Z3 likely that either the verb in V2 position , or the Vorfeld element , carry heavy stress ( 12a vs. 12b ) . 12a Er LAS+/o~ , ,~ den Artikel iiber Worl.stelhmg ( lann wohl-rheme • He read the article on word-order then presumably 12t } ? ? Er las den ArTlkel iiber Wortsl , elhmg ( hmn wohl- , .heme • Haji ( : ov~ et el. ( 1993 ) suggest that verbs are generally marked as rhemes , except if they have very general lexical meaning ( su ( : h as be , have , happen , carT'y oul , become ) . As our main concern is word order , and German verb pie { : ( &gt; ment is restricted by rules which do not allow variation , our algorithm does not allow the recognition of verbs as rhemes .</sentence>
				<definiendum id="0">basic fa</definiendum>
				<definiens id="0">sentences the nominative complement ( sub jet6 ) takes this place. A convenient way of seeing it is that all elements fol|ow the. verb in V2 position according to tile CF</definiens>
			</definition>
</paper>

		<paper id="2171">
</paper>

		<paper id="2155">
			<definition id="0">
				<sentence>b , , be MMI ) s. Then , S ( A , tJ ) nt { 'nti { &gt; u &lt; xl I ) revi ( } usly can l ) e defined as folh } ws : I ) EF.2 : The silnila.rily l ) eiwc , en M M I ) s LC &gt; ' { A , l~ ) '- ' , ~ ' ( A , , ) ( : ~ ) Iltll ( ( } &lt; / ' : ; ( A , B ) &lt; \ ] = £ ( A , A ) ) 'rhlls~ w ( 2 Ii¢ ( : d lt ( } l ; { .</sentence>
				<definiendum id="0">MMI ) s. Then , S</definiendum>
				<definiens id="0">folh } ws : I ) EF.2 : The silnila.rily l ) eiwc , en M M I ) s LC &gt; ' { A , l~ ) '- '</definiens>
			</definition>
			<definition id="1">
				<sentence>simila , ril , i0~ 965 a.re close the nmxinmm ( S ( A , B ) = 1 ) , gains of noise factor ( i.e. , inflection ) can be ignored .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">A , B ) = 1 ) , gains of noise factor ( i.e. , inflection ) can be ignored</definiens>
			</definition>
</paper>

		<paper id="1093">
			<definition id="0">
				<sentence>Because the objective of the gramYnar is to extract the syntactic structures el Japanese sentences automatically and elIiciently , COml~OUnd words that funel .</sentence>
				<definiendum id="0">gramYnar</definiendum>
				<definiens id="0">to extract the syntactic structures el Japanese sentences automatically and elIiciently , COml~OUnd words that funel</definiens>
			</definition>
			<definition id="1">
				<sentence>dgc S ( -ItlI'CI~S , 571 Morphological Unification-Based ATR Dialog Information Japanese Gralnlnar Datal ) euse Training Set sentences 1,000 1,000 ( words ) ( I0,510 ) ( 10,723 ) Test Set ( Full ) sentences ( words ) Test Set ( Sub ) sentences ( words ) Vocabulary 350 ( 3,804 ) 148 ( 904 ) 350 ( 4,060 ) ... . 148 ( 949 ) 1,284 1,168 POS System 75 Word Bigram ' 41325 POS Bigram 503 26 4,292 262 Table 1 : Experimental Condition test sentences , but also the training sentences ( close experiment ) and the sentences having no imknown words ( a subset of the test set ) .</sentence>
				<definiendum id="0">dgc S</definiendum>
			</definition>
			<definition id="2">
				<sentence>last+l ; end end ; function LENGTH ( A : uordlJst ) ; { This function returns the total length of word\ ] ist .</sentence>
				<definiendum id="0">function LENGTH</definiendum>
				<definiens id="0">the total length of word\ ] ist</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>A bead contains some sentences of source and largct texls .</sentence>
				<definiendum id="0">bead</definiendum>
				<definiens id="0">contains some sentences of source and largct texls</definiens>
			</definition>
			<definition id="1">
				<sentence>( 7 ) 7 } 'ansition I/ector ( TV ) : A transition vector consists of 4 components ( H , N , IV , D ) .</sentence>
				<definiendum id="0">'ansition I/ector ( TV )</definiendum>
			</definition>
</paper>

		<paper id="2188">
			<definition id="0">
				<sentence>In Japanese , therc ; ire basically two l , yl ) cS ofl ) ronolninal Cxl/ressions : the zero i ) ronoul~ and the ( overt ) pr &lt; mou , L Zero l ) ronouns ( mu I ) c dcfiucd as fbllows \ [ Yoshimoto 8 ( 3\ ] : A zero-pronoun is a noun phrase whi ( : h is of an obligatory case ; rod which is not , expressed but can Iw. understood I , hrough disCOlll ? SP. mid COlltexL There has been much work on han ( lling zeropl : O , ,O , .l~ , .~.'.h , , , ~ \ [ l &lt; ~ , m , ; y. , ~l. S,5\ ] , \ [ Vo~hi , ,lot , o Sq , \ [ Walker 92\ ] , mid \ [ N ( ) moto ! ) 3\ ] . Among ; l.hcm , M. l ( anmy~un~ showed in \ [ Kamcyama 85\ ] t.hat zero llrollOtlllS ill , I ; ~I\ ] ) ltlICSC , q ( mi , ell ( 'c , q could I ) c illierl ) rc.Lcd using a concept , called `` ( : Chic.ring '' \ [ 3oshi 81\ ] . In ( , he centei'ing inodel , there is one cnt , il , y I , hal. all tltt , ora , nce iliOS| , Colll , i'a\ ] ty concerns. This 0nl , it , y is rcfc : rro ( I to as t , hc backw~ird-looking ceutor ( ( , 'b ) , Any other ( mtity appearing hi all Ill , tCl ' ; tll ( 'O is a. \ [ } . ) r ' , var ( l-lookillg center ( Cf ) which niny I ) ccomc a ( it ) later Oil ill ttic discourse , Cfs arc ordered by grammatical flmctions according to the , Jr degrees of salience as follows : Topic &gt; Subject ; &gt; Objectbject2 &gt; Others ( Oblique , Possessor , etc ) Kmneymn~ showed that t ; he zeroq ) ronoun corr ( &gt; Sllonds I , o the ( 3 ) in ; lalmnc .</sentence>
				<definiendum id="0">zero-pronoun</definiendum>
				<definiendum id="1">rod</definiendum>
				<definiens id="0">a noun phrase whi ( : h is of an obligatory case</definiens>
				<definiens id="1">l-lookillg center ( Cf ) which niny I ) ccomc a ( it ) later Oil ill ttic discourse , Cfs arc ordered by grammatical flmctions according to the , Jr degrees of salience as follows : Topic &gt; Subject</definiens>
			</definition>
			<definition id="1">
				<sentence>In sentence ( 2 ) , the entity with the highest degree of salience fi'o , n the previous sentence ( Taro ) is chosen as tile zeropronoun 's antecedent , and becomes the Cb , with Saburo becoming a Cf. In tile third sentence , after Taro is chosen as the subject of the sentence , since there is only Saburo left , , Saburo becomes the antecedent of tile object zero-pronoun , assuming that there is some sort of knowledge preventing Taro from becoming the object .</sentence>
				<definiendum id="0">Saburo</definiendum>
				<definiens id="0">the entity with the highest degree of salience fi'o , n the previous sentence ( Taro ) is chosen as tile zeropronoun 's antecedent , and becomes the Cb , with Saburo becoming a Cf. In tile third sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>The Center List holds entities that 'appeared ' in the sentence as either an overt pronoun or a zero-pronoun .</sentence>
				<definiendum id="0">Center List</definiendum>
				<definiens id="0">holds entities that 'appeared ' in the sentence as either an overt pronoun or a zero-pronoun</definiens>
			</definition>
</paper>

		<paper id="2143">
			<definition id="0">
				<sentence>A typed feature structure consists of a set of feature-value pairs in which each value is a typed feature structure .</sentence>
				<definiendum id="0">typed feature structure</definiendum>
				<definiens id="0">consists of a set of feature-value pairs in which each value is a typed feature structure</definiens>
			</definition>
			<definition id="1">
				<sentence>1 , where syn , agr , sg , and 3rd are type symbols ; agree , hum , per , and subj are feature symbols ; and X is a tag symbol .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a tag symbol</definiens>
			</definition>
</paper>

		<paper id="2204">
			<definition id="0">
				<sentence>A description is an er , tity that can be interpreted as true or false of an object .</sentence>
				<definiendum id="0">description</definiendum>
				<definiens id="0">an er , tity that can be interpreted as true or false of an object</definiens>
			</definition>
			<definition id="1">
				<sentence>E is a siguature iff E is a sextuple ( ~ , % , ~ , G , ffl , ~ ) , is a set , ( % , - &lt; } is a partial order , { foreachrE72 , } = crC72 .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">a siguature iff E is a sextuple ( ~ , % , ~ , G , ffl</definiens>
			</definition>
			<definition id="2">
				<sentence>1 is an interpretation iff l is a triple ( U , S , A ) , U is a set , S is a total time|ion from U to A is a total function from ~ { to the set of partial functions from U to U , tbr each ( t C ~\ [ and each u C U , if a ( ( : ~ ) ( ~ , ) is deC , ned then ~ ( S ( u ) , a ) is defined , and ; ~ ( s ' ( ~ , ) , , , ) ~ , V ( A ( ~ ) ( *O ) , and for each cY G ~ ( and each u E U , if~ ( X ( u ) , a ) is d ( , Jined Suppose that 1 is an interpretation ( U , £ ' , A ) .</sentence>
				<definiendum id="0">U</definiendum>
				<definiendum id="1">S</definiendum>
				<definiendum id="2">V</definiendum>
				<definiendum id="3">cY G ~</definiendum>
				<definiens id="0">a set</definiens>
				<definiens id="1">X ( u ) , a ) is d ( , Jined Suppose that 1 is an interpretation ( U , £ ' , A )</definiens>
			</definition>
			<definition id="3">
				<sentence>Appropriateness encodes ~t rcbttionship between l ; he dcnotaLions of species and atl : ributes : ifa ( cr , , v ) is deliued then the den ( ) tt~tion of a.ttributc ( v acts upoi~ each ol~jecl , il , the , denota .</sentence>
				<definiendum id="0">Appropriateness</definiendum>
				<definiens id="0">encodes ~t rcbttionship between l ; he dcnotaLions of species</definiens>
			</definition>
			<definition id="4">
				<sentence>1 ' is the path interl~retati ( m fimctlon under 1 ill '' I is an interpretation ( U , £ ' , A ) , 1 ' is a tol , al timctim~ l ) 'om q3 to the s.t , f l ) a , rtia , l fimctions from U 1 , o U , alld lbr each ( ( vl ... .. ( v , , ) 6 ~ , / ' ( m , ... , 'v , ~ ) is the timcti &lt; mal coml , o , siti , m of d ( m ) ... .. A ( ( ~ , , ) . 1 write t~ for the path iute , 'prctal.ion flu , orion mMer l. De.finition 4. l , ' is a \ [ baturc structm.c ill '' I , '' is a quadrulde ( Q , q , 5 , 0 ) , Q is a tinite subset o1'~\ ] , q~Q , 8 is a. finite pa.rtia.I function from the , Ca , rtesian l , rgduct ot '' Q mM c2\ [ to Q , 0 is a totM l ) mction from Q to % , and for each q/ ~ Q , &amp; n '' some re ( 5 q3 , re rlm.s to q ' in I c , where ( , 'vt , ... , ; M ) z't/zzs l , o q ' ill 1 '' ill ' q ' 6 Q , and ~. '' son. , { qo , ... , q , } Cq , q = qo , for each i &lt; . , 8 ( qi , o'i41 ) iS de , lined , and 3 ( qi , ( Vi4-1 ) : = qi+l , ~ltl ( l q , , -. q/. } '' , ; tch \ [ ' ( ! ; tl ; llr ( ! Stl'tlC\ [ , llr ( ~ is a COllllCC~ , C ( l f~\ ] 001 '' ( ! machine ( see \ [ MooRI , ; 1 ( ; 56\ ] ) with finitely mauy st~tes , input alphabet 9..\ [ , and output Mplm.bet X. Definition 5. 1 ; is true of u under 1 iff F is a featnre structure ( Q , q , 5 , O ) , 1 is a.n interpretation ( U , S , A ) , u is an object in 1 , and for each re1 6 q3 , ca.oh rc 2 C q3 and each q ' ~ ( O , if rot runs to q/ in t '' , and rr.2 runs to q~ in l '' tl , , , , , : , , ( ~ , ) ( , , ) i , ~ , mi , ,.a , J~ ( ~ ) ( , , ) i~ , &gt; t/ , ,.4 0 ( q ' ) ~ s ( v , ( , ~ , ) ( u ) ) .</sentence>
				<definiendum id="0">... .. A (</definiendum>
				<definiendum id="1">Q</definiendum>
				<definiens id="0">a tinite subset o1'~\ ] , q~Q , 8 is a. finite pa.rtia.I function from the</definiens>
			</definition>
			<definition id="5">
				<sentence>M is a semi-morph ill '' M is a triple ( A , l ' , A ) , A is a nonemlH .</sentence>
				<definiendum id="0">M</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">a semi-morph ill ''</definiens>
			</definition>
			<definition id="6">
				<sentence>u is a standard ohject i\ [ r u is a quadruple ( A , P , A , E ) , ( &amp; , 1 ' , A ) is a morph , and E is an equivalence c/ass under 1 ' .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">an equivalence c/ass under 1 '</definiens>
			</definition>
			<definition id="7">
				<sentence>\ [ write U for the set of standard objects , write ~ for the total function fi'om U to ~ , where for each a E O and each ( A , I ' , A , E ) C U , S ( &amp; , F , A , E ) = cr iff for some rr G E , Afro ) = or , and write A for the total function fi'om ~t to the set of partial functions fi'om U to U , where for each &lt; v E 9.1 , each ( &amp; , F , A , F , ) E U and each ( &amp; ' , F ' , A ' , E ' ) G U , X ( c~ ) ( A , r , A , E ) is defined , and / ( , ~ ) ( a , r , A , E ) = ( a ' , r ' , A ' , E ' ) iff ( A , I ' , A ) = ( a ' , F ' , A* ) , and for some re G E , rea. E F , '. Lemma 12. ( U , S , A ) is an interpretation. I write 7 for ( U , ,5 ' , A } . Lemma 13. For each ( A , I ' , A , E ) E ( ) , ea.ch ( A ' , r ' , A ' , E ' ) E 9 a , .~ each re C q~ , ~'/~ ( re ) ( A , r , A , r. ) is ( le~. , e~l , a. , ,t ~5~ ( re ) ( A , r , A , ~ ) = ( a ' , r ' , A ' , ~ ' ) ia '' ( a , r , A ) = ( ~ ' , r ' , A ' ) , a , , ( ~ for some re ' G 1'3 , re % G E'. ProoL By induction on the length of re. ' , Lemma 14. For each ( A , F , A , E } EU , if E is the equivalence class of the. empty path under 1 ' then the abstraction of ( A , F , A , E ) under is ( A , F , A ) . Proposition 15. I'br each morph M , for some interl &gt; retation \ [ and some object u in I , M is the abstraction ofu under I. Definition 16 .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">&amp; , F , A , E ) = cr iff for some rr G E , Afro ) = or , and write A for the total function fi'om ~t to the set of partial functions fi'om U to U , where for each &lt; v E 9.1 , each ( &amp; , F , A , F , ) E U and each ( &amp; ' , F ' , A ' , E ' ) G U</definiens>
				<definiens id="1">the equivalence class of the. empty path under 1 ' then the abstraction of ( A , F , A , E ) under is ( A , F , A )</definiens>
			</definition>
			<definition id="8">
				<sentence>1 ; ' approximates M iff F is a l } ature structure ( Q , q,6,0 ) , M is a morph ( A , I ' , A ) , and for each re1 E e43 , each re'2 C q3 and each q ' EQ , il'rel runs to q~ in I '' , and re2 runs to q ' in F then ( ~rt , rr2 ) E r , and o ( q ' ) ~ a ( ~ ) .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">a morph</definiens>
			</definition>
			<definition id="9">
				<sentence>R is a resolved feature structure itr R is a feature structure ( Q , q , a , p } , p is a total function from Q to 6 , and for each ~ E 91 and each q ' G Q , if ~ ( q I , ct ) is defined then ~ ( p ( q ' ) , ~r ) is defined , and ( ~ ( p ( q ' ) , oz ) ~_ p ( a ( q ' , c~ ) ) .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">a resolved feature structure itr R is a feature structure ( Q , q , a , p } , p is a total function from Q to 6 , and for each ~ E</definiens>
			</definition>
			<definition id="10">
				<sentence>is a resolvant off iff R is a resolved lbature structure ( Q , q , 6 , p ) , F is a feature structure ( Q , q , ~ , O ) , and rot each q ' e Q , o ( q ' ) ~_ p ( q ' ) .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">a resolvant off iff R is a resolved lbature structure ( Q , q , 6 , p )</definiens>
				<definiens id="1">a feature structure ( Q , q</definiens>
			</definition>
			<definition id="11">
				<sentence>cr -p ( q ) For each n ~ IN , let An+l = An U rrcr ~r ff An , and ~ ( A , , ( rr ) , , e ) is defined l'n+l : l'~ , z U ( Trl~V , `` /r2Cg ) 7Cl ( 'g ~ An+l ' gtll ( t 7r. , ev ~ An4.1 , and ' ( ; 1,7r2 ) C \ [ 'n , An+l =z . '</sentence>
				<definiendum id="0">~</definiendum>
				<definiens id="0">let An+l = An U rrcr ~r ff An , and</definiens>
			</definition>
			<definition id="12">
				<sentence>&lt; 'a , \ ] ~r~v ~ A , ~+ , \ A. , and t A , ~U ( Trcr , fl ( ~ ) ) c is the least ordinal f ' ill ( such | , hat \ [ a ( A , , ( ~ ) , ,* ) ~ ; ~ ( ~ ) J For each n ~ IN , ( A , , , I ' , , , A , ) is a semi-morph .</sentence>
				<definiendum id="0">)</definiendum>
				<definiens id="0">a semi-morph</definiens>
			</definition>
</paper>

		<paper id="2160">
			<definition id="0">
				<sentence>INTRODUCTION Modern text-to-speech ( TTS ) systems are quite good at word level synthesis , but tend to perform badly on connected word sequences .</sentence>
				<definiendum id="0">INTRODUCTION Modern text-to-speech</definiendum>
				<definiens id="0">word level synthesis , but tend to perform badly on connected word sequences</definiens>
			</definition>
			<definition id="1">
				<sentence>The sccond component seeks to determine the boundary strengths via a scries of rules .</sentence>
				<definiendum id="0">sccond component</definiendum>
				<definiens id="0">seeks to determine the boundary strengths via a scries of rules</definiens>
			</definition>
			<definition id="2">
				<sentence>Bachenko and Fi~patrick descril ) e a verb-balancing rule which attempts to bahmce matcrial around a vcrb , and a verb adjacency rule which in effect extends the verb balancing rule , using 'lmndling ' ( the adjoining of adjaccnt phrases ) to continue to centre material round a verb .</sentence>
				<definiendum id="0">verb-balancing rule</definiendum>
			</definition>
			<definition id="3">
				<sentence>The change to the algorithm is more subtle , yielding thc rule : if Count ( X ) + Cmmt ( Y ) &lt; Count ( Z ) then Join to the Left ( Y ) else Join to tile Right ( Y ) where : Count ( a ) = Number of Phonological Words beneath Node 'a ' X = Previous Nnde Y = Current N ( ~le Z = Next Node This makes explicit the assumption in Bachenko and Fitzpatrick 's algorithm that tile adjoining of phrases pr ( xluces a balancezt tree .</sentence>
				<definiendum id="0">Count</definiendum>
				<definiendum id="1">phrases pr</definiendum>
				<definiens id="0">Z = Next Node This makes explicit the assumption in Bachenko and Fitzpatrick 's algorithm that tile adjoining of</definiens>
			</definition>
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>A multiset here is a set in which each element : has : ~ weight that is a m~tural nmnl ) er .</sentence>
				<definiendum id="0">multiset</definiendum>
				<definiens id="0">a set in which each element : has : ~ weight that is a m~tural nmnl</definiens>
			</definition>
			<definition id="1">
				<sentence>The nol ; ; tl ; ion ~b ( X , X ) represents the sum of the weights of the elements that contain PWs of : c in Inultiset X. For instance. , if PWs are defined as k~mji , 5 / ... .. ~a , bLot* , ~ .</sentence>
				<definiendum id="0">ion ~b</definiendum>
				<definiendum id="1">X )</definiendum>
				<definiens id="0">the sum of the weights of the elements that contain PWs of : c in Inultiset X. For instance.</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>METllOI ) S These methods were used in the beginning on early computers when storage space was expensive .</sentence>
				<definiendum id="0">METllOI</definiendum>
				<definiens id="0">These methods were used in the beginning on early computers when storage space was expensive</definiens>
			</definition>
			<definition id="1">
				<sentence>• FSM ( Finite-State Machine ) Compression : Using file Lexc ( Finite State Lexicon Compiler ) which allows the conversion of a list of surface forms inlo a transducer which is then minimized \ [ 81 .</sentence>
				<definiendum id="0">FSM ( Finite-State Machine ) Compression</definiendum>
				<definiens id="0">Using file Lexc ( Finite State Lexicon Compiler ) which allows the conversion of a list of surface forms inlo a transducer which is then minimized \ [ 81</definiens>
			</definition>
</paper>

		<paper id="2179">
			<definition id="0">
				<sentence>f ( Taroo ) ~ s0 : R ~ R ( 3.2 ) This can be writen for short as Taroo ga ~ A f.f ( Taroo ) ff so : ( T or 12 ) -+ tt ( 4 ) In the above , t ~_ s0 means that t is a typed A-term component of the logieM formula so .</sentence>
				<definiendum id="0">f ( Taroo</definiendum>
				<definiens id="0">a typed A-term component of the logieM formula so</definiens>
			</definition>
			<definition id="1">
				<sentence>( Taroo is a student . )</sentence>
				<definiendum id="0">Taroo</definiendum>
				<definiens id="0">a student</definiens>
			</definition>
			<definition id="2">
				<sentence>f ( Taroo ) E sl , i &lt; j : ( T or R ) -+ R H Taroo E , sj : T ( 16 ' ) where si denotes the logleM h ) rm correspond ing to the i-th sentence of ~ discourse .</sentence>
				<definiendum id="0">f ( Taroo</definiendum>
				<definiendum id="1">si</definiendum>
				<definiens id="0">( T or R ) -+ R H Taroo E</definiens>
			</definition>
</paper>

		<paper id="2194">
			<definition id="0">
				<sentence>Jan ) , the system 's boss ( Jan ) , Jan 's office , their coworker Les , their common corporate employer ( WidgetCorp ) , and another 'random ' person , Steph , who does not belong to WidgetCorp .</sentence>
				<definiendum id="0">Jan )</definiendum>
				<definiendum id="1">WidgetCorp</definiendum>
				<definiens id="0">the system 's boss ( Jan ) , Jan 's office , their coworker Les , their common corporate employer</definiens>
			</definition>
			<definition id="1">
				<sentence>WidgetCorp ~~ ~'I nonce5 Figure 9 : WidgetCorp expands An interesting consequence of not needing to nlodel all members of a conversational group is that it becomes unnecessary to identify them .</sentence>
				<definiendum id="0">WidgetCorp</definiendum>
				<definiens id="0">expands An interesting consequence of not needing to nlodel all members of a conversational group is that it becomes unnecessary to identify them</definiens>
			</definition>
</paper>

		<paper id="2151">
			<definition id="0">
				<sentence>iuncts ( the items being coordinated by a eonju n ( 'tioil such as amt el : o'r ) ea .</sentence>
				<definiendum id="0">iuncts</definiendum>
				<definiens id="0">the items being coordinated by a eonju n ( 'tioil such as amt el : o'r ) ea</definiens>
			</definition>
			<definition id="1">
				<sentence>loc ( 'm -- ~ C.f Sl , 'or exmnple , for tile shift reduce parser , Ihe word .</sentence>
				<definiendum id="0">loc</definiendum>
				<definiens id="0">tile shift reduce parser , Ihe word</definiens>
			</definition>
			<definition id="2">
				<sentence>7For the shift reduce parser , the initiM st~tc is tile ( 'Alll ) ty list , &lt; &gt; , the final state is &lt; s &gt; .</sentence>
				<definiendum id="0">initiM st~tc</definiendum>
			</definition>
</paper>

		<paper id="2169">
			<definition id="0">
				<sentence>sern : Seml \ ] `` '' \ [ pred : Nn In this notation , V is the verb , Pl , ... , P~ are the Japanese surface ease markers , N1 , ... , N , ~ are ease eb ement norms , and Semi , ... , Sem , ~ are the semantic categories of each case element in a thesaurus .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">P~</definiendum>
				<definiens id="0">the verb</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , in this paper , we define the smJaee case structure e of a sentence a.q the set of pairs { p , Sere ) where p is a surface case marker and Sere is the leaf semantic category of the case element noun : 1 e : { ( pl , Seg~gl ) ... .. ( pn , Se~Ttn &gt; } 1In the remainder of this paper , for brevity 's sake , we regard lloans as analllbigllOllS I~tlld assume that a noun has only one leaf semantic category in the thesaurus , although noan8 ( : an be ambiguous and have more than one semantic category in the current implementation .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">Sere</definiendum>
				<definiens id="0">a surface case marker</definiens>
				<definiens id="1">an be ambiguous and have more than one semantic category in the current implementation</definiens>
			</definition>
			<definition id="2">
				<sentence>( r. ) v/\ [ MI x : '' ~ &lt; -r~ ... ... .. IMI x ~IFMI ( x VIe=/'/\ [ ~ where IMI is the number of the corresponding causes , and I , ~ , l and I~=i a , 'e the number of case , ~ in e , ~md e2 respectively .</sentence>
				<definiendum id="0">IMI</definiendum>
				<definiens id="0">the number of the corresponding causes , and I</definiens>
			</definition>
			<definition id="3">
				<sentence>A retriewd query consists of the number of cases of the e×ample to be retrieved , cases which the example to be retrieved should have , and semantic restrictions of ease element nouns .</sentence>
				<definiendum id="0">retriewd query</definiendum>
			</definition>
			<definition id="4">
				<sentence>Similarities A retrieval query q is defined as a pair ( ldb , csp ) , where ldb is the number of cases of the example to be retrieved , and csp is the requirement on cases and semantic restriction of case element nouns , which we call a case structure pattern .</sentence>
				<definiendum id="0">retrieval query q</definiendum>
				<definiendum id="1">ldb</definiendum>
				<definiendum id="2">csp</definiendum>
				<definiens id="0">a pair ( ldb , csp )</definiens>
				<definiens id="1">the number of cases of the example to be retrieved , and</definiens>
				<definiens id="2">the requirement on cases and semantic restriction of case element nouns</definiens>
			</definition>
			<definition id="5">
				<sentence>CS is the multiset of the similarities between corresponding ease element nouns .</sentence>
				<definiendum id="0">CS</definiendum>
			</definition>
			<definition id="6">
				<sentence>This sequence Tl , ... , Tn can be regarded as a table of similarity templates and is called similarity table .</sentence>
				<definiendum id="0">Tl , ... , Tn</definiendum>
				<definiens id="0">a table of similarity templates and is called similarity table</definiens>
			</definition>
</paper>

		<paper id="2111">
			<definition id="0">
				<sentence>'File I , 'SP includes a separate n/odule lbr each kind of slmCltlral analysis .</sentence>
				<definiendum id="0">'SP</definiendum>
				<definiens id="0">includes a separate n/odule lbr each kind of slmCltlral analysis</definiens>
			</definition>
			<definition id="1">
				<sentence>Using a n\ ] OUSE tile user highlights tile process , drags tile selected texl ~ ~l~ to tile \ [ Processl tcxlbox alld drops it in .</sentence>
				<definiendum id="0">OUSE tile user</definiendum>
				<definiens id="0">highlights tile process , drags tile selected texl ~ ~l~ to tile \</definiens>
			</definition>
			<definition id="2">
				<sentence>Kifs unificalional aclive charl parser ( UniPurc ( 'harl ) is based on ~1 Chillesc l , cxical-l : unclional Grammar ( CI .</sentence>
				<definiendum id="0">UniPurc</definiendum>
			</definition>
</paper>

		<paper id="2180">
			<definition id="0">
				<sentence>Each knowledge source ( KS ) is an object in the hierarchically organized KB , and infl ) rmation can be inherited from more general to more specific KS 's .</sentence>
				<definiendum id="0">KS )</definiendum>
				<definiens id="0">an object in the hierarchically organized KB , and infl</definiens>
			</definition>
			<definition id="1">
				<sentence>This KB consists of three kinds of KS 's : generators , \ [ liters and orderers .</sentence>
				<definiendum id="0">KB</definiendum>
				<definiens id="0">consists of three kinds of KS 's : generators , \ [ liters and orderers</definiens>
			</definition>
			<definition id="2">
				<sentence>Finally , the Discourse Domain KB contains discourse domain objects each of which defines a set of discourse phenomena to hmldle in a particular domain .</sentence>
				<definiendum id="0">Discourse Domain KB</definiendum>
				<definiens id="0">contains discourse domain objects each of which defines a set of discourse phenomena to hmldle in a particular domain</definiens>
			</definition>
			<definition id="3">
				<sentence>Since texts in different domains exhibit different sets of discourse phenomena , and since dilt'erent applications even within the same domain may not have to handle the same set of discourse phenomena , the discourse domain KB is a way to customize , and constrain the workload of tile discourse module .</sentence>
				<definiendum id="0">discourse domain KB</definiendum>
				<definiens id="0">a way to customize</definiens>
			</definition>
			<definition id="4">
				<sentence>The Resolution Engine is the run-time processing module which finds tile best , antecedent hypothesis tot a given ~maphor by using the discourse KB 's described above .</sentence>
				<definiendum id="0">Resolution Engine</definiendum>
				<definiens id="0">the run-time processing module which finds tile best , antecedent hypothesis tot a given ~maphor by using the discourse KB 's described above</definiens>
			</definition>
			<definition id="5">
				<sentence>Tiffs generator generates all the possible antecedenl hypotheses up to the current sentence .</sentence>
				<definiendum id="0">Tiffs generator</definiendum>
				<definiens id="0">generates all the possible antecedenl hypotheses up to the current sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>For English , a filter named Englis\ [ i-N , 'une-Filter , which matches an anaphor ( e.g. `` Crown '' ) with a subsequence of a~ mttecedent nane string ( e.g. `` Crown Coal &amp; Coke CO '' ) , is currently employed .</sentence>
				<definiendum id="0">'une-Filter</definiendum>
				<definiens id="0">matches an anaphor</definiens>
			</definition>
</paper>

		<paper id="2147">
			<definition id="0">
				<sentence>The interactive relaxation algorithm consists of the following steps ( Howells 1988 ) : 1 ) add nodes , 2 ) spread actiwltion and 3 ) decay .</sentence>
				<definiendum id="0">interactive relaxation algorithm</definiendum>
				<definiens id="0">consists of the following steps ( Howells 1988 ) : 1 ) add nodes</definiens>
			</definition>
			<definition id="1">
				<sentence>A grammar node which has more activation than the predefined threshold 6 ) makes new nodes ( expectations ) .</sentence>
				<definiendum id="0">grammar node</definiendum>
				<definiens id="0">has more activation than the predefined threshold 6 ) makes new nodes ( expectations )</definiens>
			</definition>
			<definition id="2">
				<sentence>A node generates hypotheses if it gathers enough bottom-up activation ( evidence ) and each hypothesis locally searches for constituents , l ) ecay with penalty removes failed and incorrect hypotheses .</sentence>
				<definiendum id="0">node</definiendum>
				<definiens id="0">generates hypotheses if it gathers enough bottom-up activation ( evidence ) and each hypothesis locally searches for constituents , l ) ecay with penalty removes failed and incorrect hypotheses</definiens>
			</definition>
			<definition id="3">
				<sentence>The use of the phoneme lattice and the morpheme lattice removes the redundancy in postprocessing and parsing of spoken Korean , and makes the whole interactions among the phonemes and morphemes possible .</sentence>
				<definiendum id="0">morpheme lattice</definiendum>
				<definiens id="0">removes the redundancy in postprocessing and parsing of spoken Korean , and makes the whole interactions among the phonemes and morphemes possible</definiens>
			</definition>
</paper>

		<paper id="1096">
</paper>

		<paper id="2164">
			<definition id="0">
				<sentence>\ ] ( GT '' ammar for spontaneous speech in Japanese ) , which enables the syntactic and semantic processing modules of t~he Ensemble Model to deal with some of the phenomena peculiar to spontaneous speech .</sentence>
				<definiendum id="0">\ ] ( GT '' ammar</definiendum>
				<definiens id="0">enables the syntactic and semantic processing modules of t~he Ensemble Model to deal with some of the phenomena peculiar to spontaneous speech</definiens>
			</definition>
			<definition id="1">
				<sentence>A Grass-J-based parser takes an utterance unit as input and outputs the representation of the speech act ( illoeutionary act ) performed by the unit .</sentence>
				<definiendum id="0">Grass-J-based parser</definiendum>
				<definiens id="0">takes an utterance unit as input and outputs the representation of the speech act ( illoeutionary act</definiens>
			</definition>
			<definition id="2">
				<sentence>\ [ 'he Ensemble Model has been partially implemented , and Ensemble/Trio-I consists of syntactic , semantic , and syntactic-semantic modules , it can handle ( p2 ) above as described in detail elsewhere ( Shimazu et al. , 1993b ) .</sentence>
				<definiendum id="0">Ensemble/Trio-I</definiendum>
				<definiens id="0">consists of syntactic , semantic , and syntactic-semantic modules</definiens>
			</definition>
			<definition id="3">
				<sentence>conjunctive particles and sentence-final particles • noml phrases , which may be followed by particles • interjections • conjunctions Grass-J it : chides a bundle of phrase structure rules used to derive speech act re .</sentence>
				<definiendum id="0">noml phrases</definiendum>
				<definiens id="0">a bundle of phrase structure rules used to derive speech act re</definiens>
			</definition>
			<definition id="4">
				<sentence>A Grass-J-based parser inputs an utterance unit and outputs the rel ) resentt¢ lion of the speech act performed by the unit , which is then input to the discourse processing system .</sentence>
				<definiendum id="0">Grass-J-based parser</definiendum>
				<definiens id="0">inputs an utterance unit and outputs the rel ) resentt¢ lion of the speech act performed by the unit , which is then input to the discourse processing system</definiens>
			</definition>
</paper>

		<paper id="1062">
			<definition id="0">
				<sentence>1 contains Rules 2 , 3 , and 4 , there will be a state containing the dotted items VP -~ V. VP -- ~ V. NP VP -~ V. NP NP This state corresponds to just having found a verb ( V ) .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">a state containing the dotted items VP -~ V. VP --</definiens>
			</definition>
			<definition id="1">
				<sentence>In Simple LIt ( SLR ) the h ) okahead is any termiual symbol that can imnlediately follow any symbol of the saltle tylie as the LIIS of tile rule .</sentence>
				<definiendum id="0">Simple LIt ( SLR</definiendum>
				<definiendum id="1">okahead</definiendum>
				<definiendum id="2">saltle tylie</definiendum>
				<definiens id="0">any termiual symbol that can imnlediately follow any symbol of the</definiens>
			</definition>
			<definition id="2">
				<sentence>The mapping of tic phrases to CF symbols used in the experiments was the naive one , where UG phrases mapl ) ed to their syntactic categories , ( i.e. Prolog terms mapped to their \ [ 'unctors ) , save that vert ) s with different complements ( intransitive , transitive , etc. ) were distinguished .</sentence>
				<definiendum id="0">UG</definiendum>
				<definiens id="0">i.e. Prolog terms mapped to their \ [ 'unctors ) , save that vert ) s with different complements ( intransitive , transitive , etc. ) were distinguished</definiens>
			</definition>
			<definition id="3">
				<sentence>Tiffs is the dual of uniIication it constructs tim least general term that subsumes two giwm terms -- and was first described in \ [ 7\ ] .</sentence>
				<definiendum id="0">Tiffs</definiendum>
				<definiens id="0">the dual of uniIication it constructs tim least general term that subsumes two giwm terms</definiens>
			</definition>
			<definition id="4">
				<sentence>Anti-uniflcation is a built-in predicate of SICStus Prolog and quite acceptably fast .</sentence>
				<definiendum id="0">Anti-uniflcation</definiendum>
				<definiens id="0">a built-in predicate of SICStus Prolog and quite acceptably fast</definiens>
			</definition>
			<definition id="5">
				<sentence>Tliey are the generalizations of all relevant similar UG phrases .</sentence>
				<definiendum id="0">Tliey</definiendum>
			</definition>
			<definition id="6">
				<sentence>In it em ( Ruqe , LHS , RltS0 , RIts ) , Rule is an atomic rule identifier and RltS0 and RHS form a difference list marking the position of the ( lot .</sentence>
				<definiendum id="0">Rule</definiendum>
				<definiendum id="1">RHS</definiendum>
				<definiens id="0">an atomic rule identifier</definiens>
			</definition>
</paper>

		<paper id="2130">
			<definition id="0">
				<sentence>In his socalled sequence union approach , the standard concept of phrase structure grammar ( i.e. that a string is defined as the terminal yield of a phrase structure tree ) is abandoned .</sentence>
				<definiendum id="0">phrase structure grammar</definiendum>
				<definiens id="0">the terminal yield of a phrase structure tree</definiens>
			</definition>
			<definition id="1">
				<sentence>The value of HI , ; STIq which abbreviates 'restrictions ' : is a set of constraints on the value of this parameter .</sentence>
				<definiendum id="0">STIq</definiendum>
				<definiens id="0">a set of constraints on the value of this parameter</definiens>
			</definition>
			<definition id="2">
				<sentence>, C= , lI\ [ &lt; ov ( ) , u , :x-t\ ] ( 6 ) XiH. : x I\ ] ~ II\ [ ( , w ( ( : ~ ) , , , , , : ×+\ ] , Ci 'Phe second schenta is in a sense not a `` phrase '' structure schema but is instead a `` cluster-formation'schema. This is because normally the combination of two or more words leads to a sign which is I , I. ; x- , a phrasal sign , but here it leads to a 'complex word ' which is I , \ ] , ; X F. Also ( 6 ) is strictly binary : it takes one argument , namely the argument which is the value of fActually , our analysis also presupposes the IIead Featm'e Principle and Semantics Principle from \ [ P &amp; S ( 1994 ) \ ] ; cf. Figures 1 and 2 for informal illustration. aFollowing discussions of Weheltmth , Ackerman , Sag and Pollard at WCCFI , XlII , suggesting tiffs for German , and Chang of Ohio State University originally suggesting this for Korean. 819 COY. We arrange the lexicon so that any value of coy will always be an unsaturated base form verb which is defined as LEX+ as well. By the Valency Principle , this selection requirement of the governing verb will be appropriately 'cancelled ' after string concatenation during parsing. Central to our analysis of the case-markings of NPs in the Dutch Mittelfeld is the assumption from \ [ Pollard ( fc. ) \ ] that base forms of verbs do not assign any case to their subject. The value for the subjectNP 's CASE-feature in ( 3 ) , `` c^s~ '' , is the supertype in the type hierarchy for those atomic types that are appropriate values of the feature CASE. So , the value cast. '' is the supertype of NOM and Ace in Dutch and English , and in German also of DA'r and G~N. The result of assigning the subject-NP this supertype for case in practice bolls down to giving this NP some kind of `` any '' -value for case ; the case-value CASE of such an NP will unify with any other possible case value. In our analysis , the discontinuous relation between arguments and verbs in DCSDs is brought about firstly by lexically defining finite pereeptuals like zag ( and finite causatives ) as argument composition verbs , along the following lines : 9 ( 7 ) PHON ( ~o~ ) \ [ MAJOR ~ \ ] HEAD VFORM bUN I SUBJ ( NP\ [ NOM\ ] I~ ) COMPS ~NP\ [ ~ocl ) ~ \ [ \ ] ~OV ( `` V\ [ I\ ] ASE\ ] \ ] ) SUBJ ( ~\ ] NP ) COMPS \ [ \ ] CONT \ [ \ ] LEX + CONTENT 'bET ~VnNT PARA \ [ \ ] N / / \ [ \ ] / \ [ \ ] J LEX + The finite argument composition verb zag selects a singular nominative NP through its suB J-feature. As non-subject arguments it selects through its COMPSfeature first the NP tagged as \ [ \ ] which is unified with the SUB J-value of the governed verb ( s ) , and secondly the list \ [ ~\ ] of zero or more non-subject arguments of the governed verb ( s ) . And crucially , being a governing verb , zag selects through cov a governed base form verb , 1° with as SUB J-value `` \ [ ~ ' , as COMPS-value 9In this entry and throughout the paper , @ stands for concatenation of arbitrary-length lists of arguments. 1°One base form verb , or a base form verb-headed verbal clus '' \ [ ~2 ' and as semantics `` \ [ \ ] '. Note that , since the governed v\ [ nsi. : \ ] is selected as missing a subject and a list of complements , it must not 'find ' this subject or these complements , which it indeed does n't ( cf. the tree in Figure 1 ) . As it were in passing , the governing perceptual verb ( or causative verb alike ) imposes accusative case on the NP which denotes the suloject-argument of the governed verb. The unification of \ [ CASE CASK\ ] and \ [ CASE Acc\ ] will be forced through the structure-sharing indicated in ( 7 ) as `` \ [ \ ] ' , and will result in the more specific restriction \ [ CASE , cc\ ] . This accounts for the accusative case-marking on hasp ( `` her '' ) in examples ( 1 ) and ( 2 ) , and in general on all non-subject arguments in such constructions. The second and crucial step in our account of the discontinuity is accounting for the linear order in the verb cluster with DCSDs. The linear order of the verb cluster in Dutch we account for through ( 8 ) : ( 8 ) Linear Precedence l=tule Dutch Verb Clusters \ [ c~ov ( x ) \ ] &lt; x ( 9 ) Linear Precedence Rule German Verb Clusters X &lt; \ [ ( ~ov ( x ) \ ] By these LP-rnles , in each part of the binary branching verb cluster the governing verb will appear headinitial in Dutch , and head-final in German. n It is straightforward to show that the above approach has the desired effect also for the sentence ( 2 ) mentioned in the introduction if we define a lexical entry for the causative helpen with a syntax and semantics along the same lines as the perceptual zag. The only difference must be that such nonfinite entries do not assign NOM to their subject , but `` CASl. : '' . Other than that , there will just be additional embeddings in the semantics as well as in the verb cluster. Thus , by the ID-rule in ( 6 ) and the lexical entries for causatives and perceptuals , we account for the recursiveness of the phenomenon , cf. the tree in Figure 2. We extended the \ [ H &amp; N ( 1989 ) \ ] -analysis of German to Dutch , accounting for the difference , resp. nested vs. cross serial dependencies , through one single LPparameter. Also , we argued that such an argument composition approach is to be preferred over several alternative approaches , since argument composition is n't an 'additional ' mechanism. Further linguistic advantages of this approach , i.e. accounts of irregular case assignments and constraints on double infinitives , are discussed in \ [ Rentier ( 1994 ) \ ] . We are able to derive verb second constructions by standard application of ter ; due to the ID-schema in ( 6 ) either will be LEX-\ [ - , so that we are able to recltrsively build up bigger and bigger LEX+complexes. u LP-rules like these are common in HPSG , cf. for instance the rule XP &lt; SUBJ ( XP ) which orders subjects before VPs in English ( \ [ Borsley ( 1987 ) \ ] ) . 820 Figure 1 : The discontinuous relation : Valence Principle , schema 's ( 5 ) &amp; ( 6 ) , entries ( 3 ) &amp; ( 7 ) , LP-rule ( 8 ) . S NP\ [ NOM\ ] : \ [ ~\ ] NP\ [ A ( ; ( ; \ ] : 2~ NP\ [ ACC\ ] : a~ ik haar n. suBa NP\ [ NOMI : ~\ ] ) COMPS NP\ [ ACC\ ] : ~ , NP\ [ Accl : \ [ ~ ) GOV ) -I ) ET EVENT PA I1 , A \ [ 6~ ( X ) NT gES , pH , / \ [ : &gt; ) ~A '' ) ~ NT m , ; x + 'V\ [ l , qN\ ] S/'BJ ( NP\ [ NOM\ ] : ~\ ] ) 'Vbsg GOV ( ~\ ] ( 'OMPS ( ~ ) ) ( \ ] ON'F \ [ \ ] m , :x + `` DET EVENT PAISA \ [ -6J /P CON'\ [ ' INST \ [ Ll~x qI zag ~3 `` v\ [ ns , , : \ ] SU\ [ IJ COMPS GOV ( C ( ) NZ ~ m , :x + N* ' : XI &gt; NPbcc\ ] : a~j ) DET EVENT PARA f\ [ 7 'J\ ] I voeren 821 Figure 2 : Recarsion in the Verb Cluster ( Sentence ( 2 ) ) .</sentence>
				<definiendum id="0">x-</definiendum>
				<definiendum id="1">Mittelfeld</definiendum>
				<definiens id="0">lI\ [ &lt; ov ( ) , u</definiens>
				<definiens id="1">~ ) , , , , , : ×+\ ] , Ci 'Phe second schenta is in a sense not a `` phrase '' structure schema but is instead a `` cluster-formation'schema. This is because normally the combination of two or more words leads to a sign which is I , I. ;</definiens>
				<definiens id="2">a phrasal sign , but here it leads to a 'complex word ' which is I , \ ] , ; X F. Also ( 6 ) is strictly binary : it takes one argument , namely the argument which is the value of fActually , our analysis also presupposes the IIead Featm'e Principle and Semantics Principle from \ [ P &amp; S ( 1994 ) \ ] ; cf. Figures 1 and 2 for informal illustration. aFollowing discussions of Weheltmth , Ackerman , Sag and Pollard at WCCFI , XlII , suggesting tiffs for German , and Chang of Ohio State University originally suggesting this for Korean. 819 COY. We arrange the lexicon so that any value of coy will always be an unsaturated base form verb which is defined as LEX+ as well. By the Valency Principle , this selection requirement of the governing verb will be appropriately 'cancelled ' after string concatenation during parsing. Central to our analysis of the case-markings of NPs in the Dutch</definiens>
			</definition>
</paper>

		<paper id="2120">
			<definition id="0">
				<sentence>A sentence snch as the following has 4862 different syntactic parses due solely to attachment ambiguity ( Stabler 1991 ) .</sentence>
				<definiendum id="0">sentence snch</definiendum>
			</definition>
			<definition id="1">
				<sentence>\ ] to is ( ) lily ai , tile sentence lew ; l iu , siniple na , rrative texLs 0ha , t I , he l ) l ; esenl ; al , ion ordor itlld I , ho iw , l , ur~d order o\ [ 4 '' wahl3J ; ion necx~s , &lt; sarily coincide. The orderhig of a , n~tl ) hors and theii : autel : edent , s is o\ [ l ; en used inl'orinMly/ , o jusl , ify lefl , -l , o-riglll ; i , hreadiug or thi : eadilig through selllaait , ic sl ; rtlC ( , llrO , llowew ' , r , ( , \ ] iroa ( lingj fl ; Olll \ ] el'\ [ , -to-righ { disa , llows cX ; /llllJlcs ( 3\ ] '' Ol ) ( ; tonal c ; d ; aphoi ; a , , as ill example ( 1'3 ) , mid examI ) les o\ [ ' c : oiilpll|sory c , a.l ; a , p } lora , a , s ill : 14 ) I lesitle her &gt; every girl could see a , large cl : acl~ Sitnihu : ly , l ; \ ] irei~ding li'oin the aui ; ccedeui , s o1 ' conditio-ua , lS into t ; he COllSl'x\ [ llOll\ [ ; fails f ' ( ) r ( !</sentence>
				<definiendum id="0">theii</definiendum>
				<definiendum id="1">rtlC ( , llrO , llowew ' , r ,</definiendum>
				<definiens id="0">mid examI ) les o\ [ ' c : oiilpll|sory c</definiens>
			</definition>
			<definition id="2">
				<sentence>FOI : c.X ; llll I ) le , \ [ \ [ ' ( I 7 ) is converl , ed into t , he ~ ( ionl , :ey ' selil , el\ ] ce : 1 8 ) I , \ ] vel'y ill &amp; It who loves a. wollH/ , ll who lives wMi hiin al ) preciai , es hot Whe , n we consider t\ ] lreading ( 7\ [ ' possible wor\ ] ds , as hi ( JIMal , e Seuia , ui ; \ ] cs ( Ve\ ] tman 19 { ) 0 ) &gt; l : he noed I ; o ( \ ] isi , inguish bci ; woen L\ ] le orcler o\ [ eva , \ [ uai , ion aitd l ; he oi ; ( ler of I ) resentat , ion I ) ccolues inore cl ( ~ar cut , .</sentence>
				<definiendum id="0">FOI</definiendum>
				<definiendum id="1">\ ] cs</definiendum>
				<definiens id="0">Ve\ ] tman 19 { ) 0 ) &gt; l : he noed I</definiens>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>In addition to the nature of different punctuation marks , there are several phenomena described by Nunberg \ [ 1990\ ] which it is useful to consider before implementing any treatment of punctuation : Point absorption : strong point symbols ( comma , dash , semicolon , etc. ) absorb weaker adjacent ones ( 4 ) .</sentence>
				<definiendum id="0">Point absorption</definiendum>
				<definiens id="0">strong point symbols ( comma , dash , semicolon , etc. ) absorb weaker adjacent ones ( 4 )</definiens>
			</definition>
</paper>

		<paper id="2122">
			<definition id="0">
				<sentence>vii = mu ( vi , t , j ) if'mu ( vi , n5 ) &gt; _ a , 0 otherwise ( 1 ) IIere , mu ( vi , n j ) is the vahle of mutual informae tion defined in \ [ Chur Jr , 1991\ ] between t~i and nj .</sentence>
				<definiendum id="0">n j )</definiendum>
				<definiens id="0">the vahle of mutual informae tion defined in \ [ Chur Jr , 1991\ ] between t~i and nj</definiens>
			</definition>
			<definition id="1">
				<sentence>\Vh ( 'n th { ' algorithn } t ( ~ : 'mi ll ; Ltt 's , CCS is th ( , outllut of th { , algorithm .</sentence>
				<definiendum id="0">\Vh</definiendum>
				<definiendum id="1">CCS</definiendum>
				<definiens id="0">th ( , outllut of th { , algorithm</definiens>
			</definition>
			<definition id="2">
				<sentence>Recognition-ofPolysemy This procc ( lure , which recogniscs a polysemous vt , r\ ] ~ , also tal , : ( , s two ~trgult~ ( 'nts : th ( ' pair ( } f v ( 'rl/s from ICS and a set of chlst ( 'rs : ' ( 'tri ( , v ( 'd l/y MakeTemporary-Cluster-Set .</sentence>
				<definiendum id="0">lure</definiendum>
				<definiens id="0">recogniscs a polysemous vt</definiens>
			</definition>
			<definition id="3">
				<sentence>If both of th ( ' S ( 'lllallti ( ' ( ' ( lllll ) a ( 'tll ( 'SS vahl ( 'S of ( 'a ( : h sot sh ( swl| ill ( 6 ) are smalh , r lhall : -i set shown ill ( 7 ) , the srts ( 6 ) a.r ( ' s ( 'h ' ( 'te ( 1 , ( } th ( , rwis ( , , ( 7 ) is scl ( , ( 't ( , ( 1 avd stored i , , CCS as a newly ol ) taiu ( 'd ( 'lnst ( , r. If Bh ( ' newly ol ) tain ( , ( l ( 'luster ( lo ( , s not contain all th ( , verbs il } input , the n ( , xt p~tir ( ) f v ( , rl ) s is l ak { !</sentence>
				<definiendum id="0">l</definiendum>
			</definition>
			<definition id="4">
				<sentence>Th ( ' groups of v ( 'rlls are divid ( ' ( l into two diff { 'r ( 'nt tylst 's , `` tyl ) ( '\ ] ' and `` @ 1 } ( '2 '' ; % yl ) c\ ] ' is a sel : of v ( , r\ ] ) s ( 'Oilraining ( ) nr or mort , l/olys { m : ous v ( , tbs , mt ( I `` tyl ) ( '2 ' ( loos not ( 'ontain any l ) o|ys ( , mous verbs .</sentence>
				<definiendum id="0">'Oilraining ( ) nr</definiendum>
				<definiendum id="1">mt</definiendum>
			</definition>
			<definition id="5">
				<sentence>( 'orr ( ' ( 't ill ( 'orr ( ' ( 'l~\ ] t , , , , , , V t ~ , , , ll ( % ) ~ &gt; 0 ( ) fs ( 00. , ) s ( a ( is ) II__ ' : ... . l~S ( ' ; : ) A __ : I hi Tal ) h ' ; 1~ 'groul &gt; ' uieaiis the nundler &lt; /t ' each group , ly\ ] ) c\ ] and t.yl ) e2 ; ~ ( ' ( 11'l ' ( 'Cl ; ' lll ( ' &amp; llS thc llllllt\ ] ) ( 'l ' ( ) \ [ ' &lt; gl't/ltl ) S of verl ) s which are &lt; 'lustcrt , ( { c &lt; ) rrc ( 'tly : `` in ( 'orrc ( 'i '' means lhai. they are not.. Figure 3 shows t ! acL s : -/lllill ( ` of Ihe results , i.e. tyt ) el-c &lt; ) rrect , tyl ) o2-correct. tyl ) el-incorrect , a.n ( I type2-incorrect. \ ] ; \ ] a ( 'h valu ( ' iu Figure 3 shows the vahte of 111 ( ' .SClllStilli ( ' ( 'Ollil ) a ( 'l ; tl ( 'SS ( ) \ [ , h , g~l'Oll\ ] ) ( ) \ [ verbs. in lqgltre `` 3 , under the heading tyl ) el-correct , we uan set , thai 'lake ' is re ( 'ogn\ ] sed ns a p ( ll ) 'SCltlOliS v0rb siJl ( \ [ lias lhre ( ' ( liff ( 'rent S ( 'ltS ( '.'-J , 's| ) ( 'ltd ' , `` btly ' , ali ( I 'ol &gt; i : ain ' .</sentence>
				<definiendum id="0">'t ill</definiendum>
				<definiens id="0">-/lllill ( ` of Ihe results , i.e. tyt ) el-c &lt; ) rrect , tyl</definiens>
				<definiens id="1">'SS ( ) \ [ , h</definiens>
			</definition>
</paper>

		<paper id="1086">
			<definition id="0">
				<sentence>This DAG rellecls rellccls lhe proposithmal contents of the potcnti ; d document paris , Ihe intcntkmal gems behind tim parts as well as tim rhetoric~d relationships between them , lot details see ( Andr6 and Rist , 1993 ) .</sentence>
				<definiendum id="0">d document</definiendum>
				<definiens id="0">paris , Ihe intcntkmal gems behind tim parts as well as tim rhetoric~d relationships between them</definiens>
			</definition>
</paper>

		<paper id="2127">
			<definition id="0">
				<sentence>Prolog terms , f : X denotes a feature structure where X is the value of h : ature f , and X ~ Y denotes the conjunction sign ( X ) &lt; phrasal_sign ( X ) . sign ( X ) &lt; lexical sign ( X ) . phrasal sign ( X ~ dtrs : ( head dtr : RD &amp; comp_dtr : CD ) sign ( RD ) , sign ( CD ) , principles ( X , HD , CD ) . ) &lt; principles ( X , HD , CD ) &lt; constituent_order_principle ( X , HD , CD ) , head_featureprinciple ( X , RD ) , constituent order principle ( phon : X_Ph , phon : HD_Ph , phon : CD_Ph ) &lt; sequence_union ( CD_Ph , HD_Ph , X_Ph ) . The predicate sign/1 is defined recursively , and the base case is the predicate lexical_sign/1. But , clearly it is not restrictive enough to find only the predicate name of the base case for a given goal. The base cases must also be instantiated in order to find those that are useful for proving a given goal. In the case of parsing , the lookup of base cases ( lexical items ) will depend on the words that are present in the input string. This is implied by the first goal of the predicate principles/3 , the constituent order principle , which determines ihow the l'nON value of a constituent is construtted from the eflON values of its daughters. in general , we assume that the constituent order principle makes use of a linear and non-erasing oI ) eratkm tor combining stringsJ If this is the case , then M1 the words contained in the PnON value of the goal can have their lexical items selected as unit clauses to start bottom-up processing. l % r generation , an analogous condition on logical forms has been proposed by Shieber \ [ 13\ ] as the `` semantic monotonicity condition , '' which requires that the : logical form of every base case must subsume some portion of the goal 's logical form. Base case lookup must be defined specifically tbr different grammatical theories and directions of processing by the predicate lookup/2 , whose first argument is the goal and whose second argument is the selected base case. The following of the feature terms X and Y. r'There is an obvions connection to the Linear ContextFree Rewriting Systems ( LCFRS ) \ [ 15 , 16\ ] . 797 clause defines the lookup relation for parsing with HPSG. lookup ( phon : PhonList , lexical_sign ( phon : \ [ Word\ ] ~ synsem : X ) ) &lt; member ( Word , PhonList ) , lexicon ( Word , X ) . Note that the base case clauses can become further instantiated in this step. If concatenation ( of difference lists ) is used as the operation on strings , then each base case clause can be instantiated with the string that follows it. This avoids combination of items that are not adjacent in the input string. lookup ( phon : PhonLis t , lexical_sign ( phon : \ [ Word\ [ Suf\ ] -Suf synsem : Synsem ) ) &lt; append ( _ , \ [ Word I Suf\ ] , PhonList ) , lexicon ( Word , Synsem ) . In bottom-up Earley deduction , the first step towards proving a goal is perform lookup for the goal , and to add all the resulting ( unit ) clauses to the chart. Also , all non-unit clauses of the program , which can appear as internal nodes in the proof tree of the goal , are added to the chart. The scanning step achieves a certain degree of goal-directedness for bottom-up algorithms because only those clauses which can appear as leaves in the proof tree of the goal are added to the chart. An item in normal context-free chart parsing can be regarded as a pair ( R , S ) consisting of a dotted rule R and the substring S that the item covers ( a pair of starting and ending position ) . The fundamental rule of chart parsing makes use of these string positions to ensure that only adjacent substrings are combined and that the result is the concatenation of the substrings. In grammar formalisms like DCG or IIPSG , the complex nonterminals have an argument or a feature ( PtION ) that represents the covered substring explicitly. The combination of the substrings is explicit in the rules of the grammar. As a consequence , Earley deduction does not need to make use of string positions for its clauses , as Pereira and Warren \ [ 10\ ] point out. Moreover , the use of string positions known from chart parsing is too inflexible because it el= lows only concatenation of adjacent contiguous substrings. In linguistic theory , the interest has shifted from phrase structure rules that combine adjacent and contiguous constituents to • principle-based approaches to grammar that state general well-formedness conditions instead of describing particular constructions ( e.g. IIPSG ) • operations on strings that go beyond concatenation ( head wrapping \ [ 11\ ] , tree adjoining \ [ 15\ ] , sequence uuion \ [ 12\ ] ) . The string positions known from chart parsing are also inadequate for generation , as pointed out by Shieber \ [ 13\ ] in whose generator all items go from position 0 to 0 so that any item can be combined with any item. Itowever , the string positions are useful as an indexing of the items so that it can be easily detected whether their combination can contribute to a proof of the goal. This is especially important for a bottom-up algorithm which is not goal-directed like top-down processing. Without indexing , there are too many combinations of items which are useless for a proof of the goal , in fact there may be infinitely many items so that termination problems can arise. For example , in an order-monotonic grammar formalism that uses sequence union as the operation for combining strings , a combination of items would be useless which results in a sign in which the words are not in the same order as in the input string \ [ 14\ ] . We generalize the indexing scheme from chart parsing in order to allow different operations for the combination of strings. Indexing improves efficiency by detecting combinations that would fall anyway and by avoiding combinations of items that are useless for a proof of the goal. We define an item as a pair of a clause Cl and an index Idx , written as ( Cl~ Idx } . 798 Below , we give some examples of possible indexing schemes. Other indexing schemes can be used if they are needed. LCFRS , where no word of the input string can be used twice in a proof , or for generation where no part of the goal logical form should be verbalized twice in a derivation. scheme is useful for order-monotonic grammars. This indexing is used if only adjacent constituents can be combined , but the order of comhination is not prescrihcd ( e.g. nondirectional basic categorial grammars ) . This is used tbr grammars with a `` contextflee backbone. '' used several times in a proof , tor example for the non-unit clauses of the program , which would be represented as items of the form &lt; X ~- ( 11 A ... h Gn , fret ; ) . The following table summarizes the properties of these live coml ) ination schemes. Index 1 ( 11 ) is the index associated with the non-unit clause , Index 2 ( 12 ) is associated with the unit clause , and I1 * 12 is tit ( ; result of coml ) ining tile indices. I Index 1 Index 2 Result \ [ Nolc 12 11,12 \ [ Y+Z X+Y X+Z I ... ... . + 4. X-Y Y-Z X-Z I In case 2 ( `` non-adjacent combinatiou '' ) , the indices X and Y consist of a set of string positions , and tile operation ( : ) is the union of these string positions , provided that no two string positions fi'om X and Y do overlap. In ( 2 ) , the reduction rule is augmented to handle indices. X , Y denotes the combination of the indices X and Y. ( X +G Afl , ll ) ( c ' +-,12/ +a ) , n. 12 &gt; ( 2 ) With the use of indices , the lookup relation becomes a relation between goals and items .</sentence>
				<definiendum id="0">Prolog terms , f : X</definiendum>
				<definiendum id="1">X</definiendum>
				<definiendum id="2">X ~ Y</definiendum>
				<definiendum id="3">HD , CD ) . ) &lt; principles ( X , HD , CD ) &lt; constituent_order_principle ( X , HD , CD</definiendum>
				<definiendum id="4">constituent order principle</definiendum>
				<definiendum id="5">predicate sign/1</definiendum>
				<definiendum id="6">the base case</definiendum>
				<definiendum id="7">Y</definiendum>
				<definiens id="0">a feature structure where</definiens>
				<definiens id="1">the value of h : ature f , and</definiens>
				<definiens id="2">the conjunction sign ( X ) &lt; phrasal_sign ( X ) . sign ( X ) &lt; lexical sign ( X ) . phrasal sign ( X ~ dtrs : ( head dtr : RD &amp; comp_dtr : CD ) sign ( RD ) , sign ( CD ) , principles ( X ,</definiens>
				<definiens id="3">X_Ph , phon : HD_Ph , phon : CD_Ph ) &lt; sequence_union ( CD_Ph , HD_Ph , X_Ph</definiens>
				<definiens id="4">the predicate lexical_sign/1. But , clearly it is not restrictive enough to find only the predicate name of the base case for a given goal. The base cases must also be instantiated in order to find those that are useful for proving a given goal. In the case of parsing , the lookup of base cases ( lexical items ) will depend on the words that are present in the input string. This is implied by the first goal of the predicate principles/3 , the constituent order principle , which determines ihow the l'nON value of a constituent is construtted from the eflON values of its daughters. in general , we assume that the constituent order principle makes use of a linear and non-erasing oI ) eratkm tor combining stringsJ If this is the case , then M1 the words contained in the PnON value of the goal can have their lexical items selected as unit clauses to start bottom-up processing. l % r generation , an analogous condition on logical forms has been proposed by Shieber \ [ 13\ ] as the `` semantic monotonicity condition , '' which requires that the : logical form of every base case must subsume some portion of the goal 's logical form. Base case lookup must be defined specifically tbr different grammatical theories and directions of processing by the predicate lookup/2 , whose first argument is the goal and whose second argument is the selected base case. The following of the feature terms X and Y. r'There is an obvions connection to the Linear ContextFree Rewriting Systems ( LCFRS ) \ [ 15 , 16\ ] . 797 clause defines the lookup relation for parsing with HPSG. lookup ( phon : PhonList , lexical_sign ( phon : \ [ Word\ ] ~ synsem : X ) ) &lt; member ( Word , PhonList ) , lexicon ( Word , X ) . Note that the base case clauses can become further instantiated in this step. If concatenation ( of difference lists ) is used as the operation on strings , then each base case clause can be instantiated with the string that follows it. This avoids combination of items that are not adjacent in the input string. lookup ( phon : PhonLis t , lexical_sign ( phon : \ [ Word\ [ Suf\ ] -Suf synsem : Synsem ) ) &lt; append ( _ , \ [ Word I Suf\ ] , PhonList ) , lexicon ( Word , Synsem ) . In bottom-up Earley deduction , the first step towards proving a goal is perform lookup for the goal , and to add all the resulting ( unit ) clauses to the chart. Also , all non-unit clauses of the program , which can appear as internal nodes in the proof tree of the goal , are added to the chart. The scanning step achieves a certain degree of goal-directedness for bottom-up algorithms because only those clauses which can appear as leaves in the proof tree of the goal are added to the chart. An item in normal context-free chart parsing can be regarded as a pair ( R , S ) consisting of a dotted rule R and the substring S that the item covers ( a pair of starting and ending position ) . The fundamental rule of chart parsing makes use of these string positions to ensure that only adjacent substrings are combined and that the result is the concatenation of the substrings. In grammar formalisms like DCG or IIPSG , the complex nonterminals have an argument or a feature ( PtION ) that represents the covered substring explicitly. The combination of the substrings is explicit in the rules of the grammar. As a consequence , Earley deduction does not need to make use of string positions for its clauses , as Pereira and Warren \ [ 10\ ] point out. Moreover , the use of string positions known from chart parsing is too inflexible because it el= lows only concatenation of adjacent contiguous substrings. In linguistic theory , the interest has shifted from phrase structure rules that combine adjacent and contiguous constituents to • principle-based approaches to grammar that state general well-formedness conditions instead of describing particular constructions ( e.g. IIPSG ) • operations on strings that go beyond concatenation ( head wrapping \ [ 11\ ] , tree adjoining \ [ 15\ ] , sequence uuion \ [ 12\ ] ) . The string positions known from chart parsing are also inadequate for generation , as pointed out by Shieber \ [ 13\ ] in whose generator all items go from position 0 to 0 so that any item can be combined with any item. Itowever , the string positions are useful as an indexing of the items so that it can be easily detected whether their combination can contribute to a proof of the goal. This is especially important for a bottom-up algorithm which is not goal-directed like top-down processing. Without indexing , there are too many combinations of items which are useless for a proof of the goal , in fact there may be infinitely many items so that termination problems can arise. For example , in an order-monotonic grammar formalism that uses sequence union as the operation for combining strings , a combination of items would be useless which results in a sign in which the words are not in the same order as in the input string \ [ 14\ ] . We generalize the indexing scheme from chart parsing in order to allow different operations for the combination of strings. Indexing improves efficiency by detecting combinations that would fall anyway and by avoiding combinations of items that are useless for a proof of the goal. We define an item as a pair of a clause Cl and an index Idx , written as ( Cl~ Idx } . 798 Below , we give some examples of possible indexing schemes. Other indexing schemes can be used if they are needed. LCFRS , where no word of the input string can be used twice in a proof , or for generation where no part of the goal logical form should be verbalized twice in a derivation. scheme is useful for order-monotonic grammars. This indexing is used if only adjacent constituents can be combined , but the order of comhination is not prescrihcd ( e.g. nondirectional basic categorial grammars ) . This is used tbr grammars with a `` contextflee backbone. '' used several times in a proof , tor example for the non-unit clauses of the program , which would be represented as items of the form &lt; X ~- ( 11 A ... h Gn , fret ; ) . The following table summarizes the properties of these live coml ) ination schemes. Index 1 ( 11 ) is the index associated with the non-unit clause , Index 2 ( 12 ) is associated with the unit clause , and I1 * 12 is tit ( ; result of coml ) ining tile indices. I Index 1 Index 2 Result \ [ Nolc 12 11,12 \ [ Y+Z X+Y X+Z I ... ... . + 4. X-Y Y-Z X-Z I In case 2 ( `` non-adjacent combinatiou '' ) , the indices X and Y consist of a set of string positions , and tile operation ( : ) is the union of these string positions , provided that no two string positions fi'om X and Y do overlap. In ( 2 ) , the reduction rule is augmented to handle indices. X</definiens>
			</definition>
			<definition id="1">
				<sentence>The Subcategorization Principle involves an operation on lists ( appond/3 or dolot+/3 in different formalizations ) that does not need bottom-up processing , but can better be evaluated by top-down resolution if its arguments are sulficiently instantiated .</sentence>
				<definiendum id="0">Subcategorization Principle</definiendum>
				<definiens id="0">involves an operation on lists ( appond/3 or dolot+/3 in different formalizations</definiens>
			</definition>
			<definition id="2">
				<sentence>( r is the most general unifier of G and G ~ , and the substitution v is the solution which results from proving the sequence of goals ~ , ( X+ -- GA~A~ , I1 ) ( a ' ~- , x2 ) ( ra ( X ~ f~ ) , 11 * 12 ) ( a ) In order to show the correctness of the system , we must show that the scanning step only adds consequences of the program to the chart , and that any items derived by the inference rule are consequences of the program clauses .</sentence>
				<definiendum id="0">v</definiendum>
				<definiendum id="1">I1 )</definiendum>
				<definiendum id="2">x2 ) ( ra</definiendum>
				<definiens id="0">the most general unifier of G and G ~ , and the substitution</definiens>
			</definition>
			<definition id="3">
				<sentence>Unit clauses are associated with a numerical preference value , and non-unit clauses with a formula that determines how its preference value is computed fi'om the preference values of the goals in the body of the clause .</sentence>
				<definiendum id="0">Unit clauses</definiendum>
				<definiens id="0">determines how its preference value is computed fi'om the preference values of the goals in the body of the clause</definiens>
			</definition>
			<definition id="4">
				<sentence>GeLD ( GeneraJized Linguistic Deduction ) is an extension of Prolog which provides typed feature descriptions and preference values as additions to the expressivity of the language , and partial evaluation , topdown , head-driven , and bottom-up Barley deduction as processing strategies .</sentence>
				<definiendum id="0">GeLD ( GeneraJized Linguistic Deduction )</definiendum>
				<definiens id="0">an extension of Prolog which provides typed feature descriptions and preference values as additions to the expressivity of the language</definiens>
			</definition>
</paper>

		<paper id="2118">
			<definition id="0">
				<sentence>Wo cou ( hlct , ed a , ll ( '~Xl ) Cl ; ili ; iolll~ Oll A -- KINI ) - ( ) I '' links in which , \ [ br ; 'til a , liCi\ ] ) igUOllS delh\ ] ienduln , we acccl ) l ; ed ; is t , hc ( : orl'oXfl ; sciises l , hos0 ( le\ [ iilil ; iolls w|i ( ) se geilllS I ; ( ~rllhq ha , d SOlli ( ~ tnodiliers in 0oiii\ ] iloii wiLh i ; he ( tetiui0nduin in iLs original conl , exL Oul ; of tho 860 eiiLrios t , osl , ed , l , hc : rc we !</sentence>
				<definiendum id="0">iolll~ Oll</definiendum>
				<definiens id="0">A -- KINI ) - ( ) I '' links in which</definiens>
			</definition>
</paper>

		<paper id="2129">
			<definition id="0">
				<sentence>The director act is the speech act that provides the general sense of the intervention , that is to say , its ilocutive force .</sentence>
				<definiendum id="0">director act</definiendum>
				<definiens id="0">the speech act that provides the general sense of the intervention</definiens>
			</definition>
</paper>

		<paper id="2139">
			<definition id="0">
				<sentence>They assumed COml ) ound nouns consist of only one character words and two character words .</sentence>
				<definiendum id="0">COml</definiendum>
			</definition>
			<definition id="1">
				<sentence>The procedures consist of the following four steps : duct pairs of two kanzi charactcr words ; if one is not in the thesaurus , this four kanzi charaeter word is discarded ( section 2.1 ) character word ( section 2.2 ) locations ( section 2.3 ) We use a corpus of four kanzi character words as the knowledge source of collocational information .</sentence>
				<definiendum id="0">procedures</definiendum>
				<definiens id="0">consist of the following four steps : duct pairs of two kanzi charactcr words ; if one is not in the thesaurus , this four kanzi charaeter word is discarded ( section 2.1 ) character word ( section 2.2 ) locations ( section 2.3 ) We use a corpus of four kanzi character words as the knowledge source of collocational information</definiens>
			</definition>
</paper>

		<paper id="2134">
			<definition id="0">
				<sentence>SAX uses the concepts of act , iw~ ' and inactive edges of Chart P~lrsing and analyses an input sentence with a bottom-up and parallel algoril ; hm .</sentence>
				<definiendum id="0">SAX</definiendum>
				<definiens id="0">uses the concepts of act</definiens>
			</definition>
</paper>

		<paper id="1077">
			<definition id="0">
				<sentence>For example , Earley deductiml ( Pereir : t &amp; Warren , 1983 ) is a general procedure for dealing with Horn clauses which gives rise to Earlcy-like parsing when given a context-free grammar and a word string as the inlmt .</sentence>
				<definiendum id="0">Earley deductiml</definiendum>
				<definiens id="0">a general procedure for dealing with Horn clauses which gives rise to Earlcy-like parsing when given a context-free grammar and a word string as the inlmt</definiens>
			</definition>
			<definition id="1">
				<sentence>The purpose of computation is to tell whether any goal is satisfiable , and if so obtain an answer substitution for the terms ( variat ) les ) in a satisfiabh~ ' goal .</sentence>
				<definiendum id="0">computation</definiendum>
				<definiens id="0">to tell whether any goal is satisfiable , and if so obtain an answer substitution for the terms</definiens>
			</definition>
			<definition id="2">
				<sentence>The premise of a clause ( i.e. , the conjunction of the atomic formuhts and bindings which appear ~ negative literals ) is a hypothesis .</sentence>
				<definiendum id="0">premise of a clause</definiendum>
				<definiens id="0">a hypothesis</definiens>
			</definition>
			<definition id="3">
				<sentence>A chart is a graph whose node , s denote positions between words ill a selltenee and whose ares are regarded as context-free rules each instant ) areal partially with respect to at most two such positions .</sentence>
				<definiendum id="0">chart</definiendum>
				<definiens id="0">a graph whose node , s denote positions between words ill a selltenee and whose ares are regarded as context-free rules each instant</definiens>
			</definition>
			<definition id="4">
				<sentence>stantlated dau : ~c derived fi'om a program enc , } dlng 469 A subsumption operation is to extend subsumplion relation by possibly creating a partially instantiated clause .</sentence>
				<definiendum id="0">subsumption operation</definiendum>
				<definiens id="0">to extend subsumplion relation by possibly creating a partially instantiated clause</definiens>
			</definition>
			<definition id="5">
				<sentence>'ds ; trc coml ) atible when there is an instance of the obtuse , which involves all instance , of each of those literals .</sentence>
				<definiendum id="0">trc coml</definiendum>
				<definiens id="0">involves all instance</definiens>
			</definition>
</paper>

		<paper id="1067">
</paper>

		<paper id="2154">
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>TECHI ) OC is an inll ) lemellted system demonstrating the flmsibility of gep , erating multilingual technical doeulnellts on the hasis of a lallguage-ill ( lel ) ( mdellt knowledge base .</sentence>
				<definiendum id="0">TECHI ) OC</definiendum>
				<definiens id="0">an inll ) lemellted system demonstrating the flmsibility of gep , erating multilingual technical doeulnellts on the hasis of a lallguage-ill ( lel ) ( mdellt knowledge base</definiens>
			</definition>
			<definition id="1">
				<sentence>The Knowledge Base is encoded in I , OOM .</sentence>
				<definiendum id="0">Knowledge Base</definiendum>
				<definiens id="0">encoded in I</definiens>
			</definition>
			<definition id="2">
				<sentence>In addition to the standard KL-ONE functionality ( structured inheritance , separation of terminological and assertional knowledge ) , LOOM supl ) orts object-oriente .</sentence>
				<definiendum id="0">LOOM supl</definiendum>
				<definiens id="0">structured inheritance , separation of terminological and assertional knowledge )</definiens>
			</definition>
			<definition id="3">
				<sentence>In addition to tile 'Upper Model ' of the Penman generator ( a basic ontology that reflects semantic distinctions made by language , \ [ Bateman , 19901 ) more .</sentence>
				<definiendum id="0">Penman generator</definiendum>
				<definiens id="0">a basic ontology that reflects semantic distinctions made by language , \ [ Bateman , 19901 ) more</definiens>
			</definition>
			<definition id="4">
				<sentence>~ , ~ : ~ % '-~ , t &lt; ? ! ; ? ~ , ,'.~£ ! i~ : ~ . ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... .. il ii• m~ '' ... ... . `` { lU~ Auto au~ ' ebenem Boder ab£t.e.\ [ len. ; i:1 9ual text 9enerator , rt , nnin9 ( ~o `` $ 1m32 '' : Sgs~m In£ormat|on : I : SPEECHflCI PENN~N : :IHPERflTIVE ) ( R~ / COGNITION : LEX RE~D ; SENSER ( PENI , ~ : NEARER I PENMhN~KB~PERSON ) : PHENOHENON ( LEVEL INSTANCE I LEVEL ) Olala9 id den Motor aus steller , Bann das 1 , :l ttoroel ueoerprue£en. Ben ~ ! ~1 ~.1 ~~ent£ernen. thn abu.tschen. ~1 IIn wledereln£uehre~ , ihn t , Jieder it£ernen , und den Stand ablesen. Er ~ I1 'l lle zwischen der °berer ' PeBelmarke il ; o unteren Pegelmarke sein. i~ \ [ l~h : Checking Park the car on level ground , and i , :\ ] st~itch the engine o£f. Then. check the ~1 enBine oil , Remove the l s~ wipe ~iJ off it. retnsert it. remove Lt again. ~il and read the level. It should be ~ , iI i i ) ! ! French : , :~ ... ... ... ... ... ... ... . , ... ... ... , ,'oh Gaper le volture sur Ia Sur ? ace de ~i : : niveau , puts machen le motetlr atJtOUr. . Puts. veri-rl~r i '' hulle moteur , Re'c.trer \ ] a jali~e , essuyer Ja. reintrodulre \ [ a. i. retirer 1 '' a nouveau , pills VOIP \ [ e I , nlveau. It devoir ' etre entre le repere superieur ot repere io2erieLlr , l : igure \ ] : Trilingual output and interactiw~ , graphic roLl ) port 342 TRACTION The rhetorical structure represents logical relations between sentences or blocks of sentences of each section of the document. A rhetorical structure analysis determines logical relations between sentences based on linguistic clues , such .as connectives , anaphoric expressions , and idiomatic expressions ill the input text , and then recognizes an argumentative chunk of sentences. Rhetorical structure extraction consists of six major sub-processes : ( 1 ) Sentence analysis accomplishes morphological and syntactic analysis for each sentence. ( 2 ) Rhetorical relation extraction detects rhetorical relations and constructs tile sequence of sentence identifiers and relations. ( 3 ) Segmentation detects rhetorical expressions between distant sentences which define rhetorical strncture. They are added onto tile sequence produced in step 2 , and form restrictions for generating structures in step 4. For example , expressions like `` ... 3 reasons. First , ... Second ... . Third , :.. '' , and `` ... Of course ... . • ..But , ... '' are extracted and the structural constraint is added onto the sequence so ~s to form a chunk between the expressions. ( 4 ) Candidate generation generates all possible rhetorical strnctures described by binary trees which do not violate segmentatio , , restrictions. ( 5 ) Preference judgement selects tile structure candidate with the lowest penalty score , a wdue determined based on l ) reference rules on every two neighboring relations in tile ca , ldidate. This process selects tile structure candklate with the lowest penalty score , a value determi , wd based on preference rules on every two neighboring relations in the candkhtte. A preference rule used in this process represents a heuristic local preference on consecutive rhetorical relations between sentences. Couskler the sequence \ [ P &lt; EG &gt; t~ &lt; SR &gt; R\ ] , where P , Q , R are arbitrary ( blocks of ) sentences .</sentence>
				<definiendum id="0">Segmentation</definiendum>
				<definiens id="0">Sgs~m In£ormat|on : I : SPEECHflCI PENN~N : :IHPERflTIVE ) ( R~ / COGNITION : LEX RE~D ; SENSER ( PENI , ~ : NEARER I PENMhN~KB~PERSON ) : PHENOHENON ( LEVEL INSTANCE I LEVEL ) Olala9 id den Motor aus steller , Bann das 1 , :l ttoroel ueoerprue£en. Ben ~ ! ~1 ~.1 ~~ent£ernen. thn abu.tschen. ~1 IIn wledereln£uehre~ , ihn t , Jieder it£ernen , und den Stand ablesen. Er ~ I1 'l lle zwischen der °berer ' PeBelmarke il ; o unteren Pegelmarke sein. i~ \ [ l~h : Checking Park the car on level ground , and i , :\ ] st~itch the engine o£f. Then. check the ~1 enBine oil , Remove the l s~ wipe ~iJ off it. retnsert it. remove Lt again. ~il and read the level.</definiens>
				<definiens id="1">Trilingual output and interactiw~ , graphic roLl ) port 342 TRACTION The rhetorical structure represents logical relations between sentences or blocks of sentences of each section of the document. A rhetorical structure analysis determines logical relations between sentences based on linguistic clues , such .as connectives , anaphoric expressions , and idiomatic expressions ill the input text</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>1 ) is used to measure the distance between words , q'he network is a graph that shows which words are used in the .</sentence>
				<definiendum id="0">q'he network</definiendum>
				<definiens id="0">used to measure the distance between words</definiens>
				<definiens id="1">a graph that shows which words are used in the</definiens>
			</definition>
			<definition id="1">
				<sentence>In our exl~eriments we used mi ( Idle fi'equency words : the 51st to 1050th most frequent words in the reference Collins English I ) ictiotmry ( CI ' ; D ) , The distance w~ctor fl ) r diclionary is deriwM it'* fob lOWS : ~ ) ... disti~uc , , ( ( ticl. , 01 ) dictionary ~ 1 ... distance ( dict. , 0'2 ) 2 ... distance ( dicL , Oa ) The i-4 , h element is the distance ( the length of the shortest path ) between diclionary and the i-th origin , Oi .</sentence>
				<definiendum id="0">mi</definiendum>
				<definiendum id="1">h element</definiendum>
			</definition>
			<definition id="2">
				<sentence>, ) n ' This shows the length of the links between words Wi ( i = 1,2 ) ill Fig , 2 , where Ni denotes the total minibet of links front and to } Vi and n denotes the uulnlmr of direct links bt .</sentence>
				<definiendum id="0">Ni</definiendum>
				<definiens id="0">the total minibet of links front and to } Vi and n denotes the uulnlmr of direct links bt</definiens>
			</definition>
			<definition id="3">
				<sentence>A co-occurence vector of a word is defined as the list of co-occtlrrellce likelihood of the word with a certahi set o\ [ 'orighi words .</sentence>
				<definiendum id="0">co-occurence vector of a word</definiendum>
			</definition>
			<definition id="4">
				<sentence>The precision is the simple average of the respective precisions for the two senses .</sentence>
				<definiendum id="0">precision</definiendum>
				<definiens id="0">the simple average of the respective precisions for the two senses</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>Tagging problem can be formulated ( Church , 1989 ; IAn , Chiang &amp; Su , 1992 ) as .7~ i=i ( '1 ( 1 ) where ~ is the category sequence selected by the tagging model , wi is the i-th word , ci is the possible corresponding category for the i-th word and c't ~ is tim Stiort-hand notation of tile category sequence Cl~ c2~ `` • • , on .</sentence>
				<definiendum id="0">wi</definiendum>
				<definiendum id="1">ci</definiendum>
				<definiens id="0">the possible corresponding category for the i-th word</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , a probabilistic classification model , which uses all the features selected by CART in an independent way , is proposed in this section to robustly re-tag the lexical categories of the error dominant words .</sentence>
				<definiendum id="0">probabilistic classification model</definiendum>
				<definiens id="0">uses all the features selected by CART in an independent way</definiens>
			</definition>
			<definition id="2">
				<sentence>After constructing the feature vectors , the problem becomes to find a most probable category according to the given feature vector and it can be formulated as = a rgmax_P ( cI r , , ... , D , ) , ( 3 ) ( 2 where c is a possible tag for the word to be retagged .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">a possible tag for the word to be retagged</definiens>
			</definition>
</paper>

		<paper id="2178">
			<definition id="0">
				<sentence>Table 9 : K-vec results French English The K-vec algorithm generates a quick-and-dirty estimate of a bilingual lexicon .</sentence>
				<definiendum id="0">K-vec algorithm</definiendum>
				<definiens id="0">generates a quick-and-dirty estimate of a bilingual lexicon</definiens>
			</definition>
</paper>

		<paper id="2177">
			<definition id="0">
				<sentence>nt , ; ~t ; ion fornmlisln depends basically on two major fact , ors : • it , s declarative c : cpres .</sentence>
				<definiendum id="0">ion fornmlisln</definiendum>
				<definiens id="0">depends basically on two major fact , ors : • it , s declarative c : cpres</definiens>
			</definition>
			<definition id="1">
				<sentence>A standard query consists of a node and a path , e.g. Sheep : &lt; orth plur &gt; , an &lt; l evaluates to a sequence , of atoms ( value ) , e.g. sheep. A reverse query , on the other hand , starts with the value , e.g. sheep , and queries the set of node-path pairs which evaluate to it , for instance , Sheep : &lt; orth sing &gt; and Sheep : &lt; orth plur &gt; .</sentence>
				<definiendum id="0">standard query</definiendum>
				<definiens id="0">consists of a node and a path , e.g. Sheep : &lt; orth plur &gt; , an &lt; l evaluates to a sequence , of atoms ( value ) , e.g. sheep. A reverse query , on the other hand , starts with the value , e.g. sheep , and queries the set of node-path pairs which evaluate to it , for instance , Sheep : &lt; orth sing &gt; and Sheep : &lt; orth plur &gt;</definiens>
			</definition>
			<definition id="2">
				<sentence>C is the set of path suffixes of N : P. A DATR terminal symbol of a theory 0 is an atom that has at least one occurence in a sentence in 0 where it is not an attribute , i.e. where it does not occur in a path .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">the set of path suffixes of N : P. A DATR terminal symbol of a theory 0 is an atom that has at least one occurence in a sentence in 0</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus , the constraint of a path P is the set of path suffixes extending P of those paths that have P as a prefix .</sentence>
				<definiendum id="0">constraint of a path P</definiendum>
				<definiens id="0">the set of path suffixes extending P of those paths that have P as a prefix</definiens>
			</definition>
			<definition id="4">
				<sentence>Now having defined some basic notions , we can give the rules that map standard DATR notation ont ; o our representation : Mapping rules N : P =-0 N : P == atonl : :~ N : P == N2:1'2 : : &gt; N : P == N2 = &gt; N : P == P2 : : : &gt; N : P =- '' N2 : P2 '' N : P =- '' N2 '' N : P == `` P2 '' : = &gt; \ [ N , P , C , N ' , P'\ ] -~ e \ [ N , P , C , N ' , P'\ ] -+ atom \ [ N , P , C , N ' , P'\ ] -- + \ [ N2 , P2 , C , N ' , P'\ ] \ [ N , P , C , N ' , P'\ ] -+ \ [ N2 , P , C , N ' , P'\ ] \ [ N , P , C , N ' , P'\ ] -- ; .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">-~ e \ [ N , P , C , N ' , P'\ ] -+ atom \ [ N , P , C , N ' , P'\ ] -- + \ [ N2 , P2 , C</definiens>
			</definition>
			<definition id="5">
				<sentence>A chart parser is an abstract machine that performs exactly one action .</sentence>
				<definiendum id="0">chart parser</definiendum>
				<definiens id="0">an abstract machine that performs exactly one action</definiens>
			</definition>
			<definition id="6">
				<sentence>Thus an active item has the structure : ( START , END , CAT0 , CATj ... CAT , , SUFFIX ) Consider the following examples : the inactive item ( 0 , 1 , \ [ House , &lt; orth sing &gt; , { &lt; gen &gt; } , House , P'\ ] ) represents the intbrmation that the substring of the input string consisting of the first symbol is the vahm of the query House : &lt; orth sing &gt; ( with arty extensional path suffix , but not gcn ) in the global environment that consists of the node House and some still uninstantiated path P ' .</sentence>
				<definiendum id="0">] )</definiendum>
				<definiens id="0">the inactive item ( 0 , 1 , \ [ House , &lt; orth sing &gt; , { &lt; gen &gt; }</definiens>
				<definiens id="1">the intbrmation that the substring of the input string consisting of the</definiens>
				<definiens id="2">consists of the node House and some still uninstantiated path P '</definiens>
			</definition>
			<definition id="7">
				<sentence>VER3~EX ( integer ) SI ... Sn ( string of t ) A'I'\ ] : L symbols ) data : A DATR theory 0 I ) egin if n &gt; 0 then NEX'\ [ ' : VIBIIlI'I+ , X : -VI , ; I/I\ [ 'EX + 1 call-proe ad &lt; l : epsilon ( VEI { ; I'EX ) call-proc reduce ( VEllSl'EX , $ 1 , NI , 'XT-VFI/TI , ; X ) call-proe complete ( VEI ( Pl , ; X , $ 1 , NEX'I'-VEIIlFI , ; N ) eall-proe parse. ( NEXT-VEIIS\ [ 'F , X , S2 ... S , , ) else add-e.psilon ( VEI/flT ; X ) en ( i The 17r &lt; &gt; ce &lt; hn'e add-cpsilo'n ill , '- ; ( ! l't ; s ar &lt; : s For the epsihm pr { } ( lu { : ti { } ns inLo l ; he charL : procedure addc.p , +il &lt; m ( V E\ ] { SI TBX ) variables : VI , ; R/IT ; X ( integer ) data : A I ) ATR , the , { Try 0 i } egin for-each rule CAT ~ e : in 0 ca.ll-proc redu { : e ( VEl { l'EX , CAT , V\ ] , ; II3T ; X ) call-proc ( : omplete ( Vl , ; R'l ? EX , CAT , VER\ [ PIBX ) end The , l } lO ( : &lt; ~durc 'reduce Lakes all inactive item as tim inl ) tll ; a , rgumcnL and s { ~ ; ~l { ; h { ! s l , } lO I ) ATll , Llmory for tulcs thai ; have a mat ( : hinp ; le , fl ; -c { &gt; ruer &lt; : at &lt; ~g ( } ry .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">add-e.psilon</definiendum>
				<definiendum id="2">X</definiendum>
				<definiens id="0">integer ) SI ... Sn ( string of t ) A'I'\ ] : L symbols ) data : A DATR theory 0 I ) egin if n &gt; 0 then NEX'\ [</definiens>
				<definiens id="1">procedure addc.p , +il &lt; m ( V E\ ] { SI TBX ) variables : VI</definiens>
			</definition>
</paper>

		<paper id="1091">
			<definition id="0">
				<sentence>A unit classifier is any classifier which has a special relationship with one or more concrete nouns .</sentence>
				<definiendum id="0">unit classifier</definiendum>
				<definiens id="0">any classifier which has a special relationship with one or more concrete nouns</definiens>
			</definition>
			<definition id="1">
				<sentence>A collective classifier is , any classifier which shows general group or set of mass nouns , un a~ ~ /nok soong lung/ 'two flocks of bird ' .</sentence>
				<definiendum id="0">collective classifier</definiendum>
				<definiens id="0">any classifier which shows general group or set of mass nouns</definiens>
			</definition>
			<definition id="2">
				<sentence>A metric classifier is any classifier which occurs in enumerations that modify predicates as well as nouns , v lh l~1~ , u~/nam saam kaew/ 'three glasses of water ' .</sentence>
				<definiendum id="0">metric classifier</definiendum>
				<definiens id="0">any classifier which occurs in enumerations that modify predicates as well as nouns , v lh l~1~ , u~/nam saam kaew/ 'three glasses of water '</definiens>
			</definition>
			<definition id="3">
				<sentence>A frequency classifier is any classifier which is used to express the frequency of event that occurs , ~u ~ ~mJ /bin sii roob/ 'fly four rounds ' .</sentence>
				<definiendum id="0">frequency classifier</definiendum>
				<definiens id="0">any classifier which is used to express the frequency of event that occurs</definiens>
			</definition>
			<definition id="4">
				<sentence>A verbal classifier is any classifier which is derived from a verb and usually used in construction with mass nouns , n~z~q~a # a ~ 11 '' ) 11 /kradaad haa muan/ 'five rolls of paper ' .</sentence>
				<definiendum id="0">verbal classifier</definiendum>
				<definiens id="0">any classifier which is derived from a verb and usually used in construction with mass nouns</definiens>
			</definition>
			<definition id="5">
				<sentence>557 Expressions -Definite demonstration -Indefinite demonstration -Referential -Relative pronoun -Interrogative pronoun -Ordinal pronoun -Pronoun Patterns N/V-NCNM-CL N-CL-/tii/-NCNM a ) N-CL-DET a ) N-CL-DET b ) N-DET-CL a ) N-CL-DET N-CL-VA'Iq '' CL-N a ) CL-REL_M b ) CL-ITR_M c ) CL-DONM d ) CL-DDAC Samples /nakrian 3 khon/ ( N ) ( N ) ( CL ) student 3 &lt; student &gt; 'three students ' /kaew bai thii4/ ( N ) ( CL ) ( N ) glass &lt; glass &gt; 4th 'the fourth gl~Lss I a ) /raw chop kruangkhidlek kruang nii/ ( N ) ( CL ) ( DEW ) we like calculator &lt; calculator &gt; this 'we like this c~dculator ' a ) /phukhawfung khon nung sadaeng ( N ) ( CL ) ( DEW ) participant &lt; participant &gt; one express khwamhen nai thiiprachum/ opinion in conference 'A participant expressed his opinion in the conference . '</sentence>
				<definiendum id="0">-Definite demonstration -Indefinite demonstration -Referential -Relative pronoun -Interrogative pronoun -Ordinal pronoun -Pronoun Patterns N/V-NCNM-CL N-CL-/tii/-NCNM</definiendum>
				<definiens id="0">N ) ( N ) ( CL ) student 3 &lt; student &gt; 'three students ' /kaew bai thii4/ ( N ) ( CL )</definiens>
				<definiens id="1">khon nung sadaeng ( N ) ( CL ) ( DEW ) participant &lt; participant</definiens>
			</definition>
			<definition id="6">
				<sentence>/dinsoo theng san/ ( N ) ( CL ) ( VAT'I ' ) pencil &lt; shape &gt; short ~ncil ' /kana naktongtiew/ ( CL ) ( N ) group tourist of tourist ' a ) /nakbanchii khon thii thamngan ( N ) ( CL ) ( REL-M ) ( V ) accountant who work thii borisat nii/ at company this 'the accountant who works at this company ' b ) /sing nail ( CL ) ( nR-M ) &lt; thing &gt; which 'which one ' c ) /tua raek/ ( CL ) ( DONM ) one first 'the first one ' d ) /khon nil chop hia mak/ ( CI , ) ( DDAC ) the one like beer very 'The one likes b~much ' Fig .</sentence>
				<definiendum id="0">N ) ( CL )</definiendum>
				<definiendum id="1">REL-M )</definiendum>
				<definiens id="0">/sing nail ( CL ) ( nR-M ) &lt; thing &gt; which 'which one ' c ) /tua raek/ ( CL )</definiens>
			</definition>
			<definition id="7">
				<sentence>Output : A list of nouns-classifiers with frequency intormatiou of co-occurrences .</sentence>
				<definiendum id="0">Output</definiendum>
				<definiens id="0">A list of nouns-classifiers with frequency intormatiou of co-occurrences</definiens>
			</definition>
			<definition id="8">
				<sentence>( Relative/Interrogative pronoun ) where N denotes noun , CL denotes classifier , NCNM denotes c~u'dinal number , DET denotes determiner , A , .</sentence>
				<definiendum id="0">Relative/Interrogative pronoun</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">CL</definiendum>
				<definiendum id="3">NCNM</definiendum>
				<definiendum id="4">DET</definiendum>
				<definiens id="0">c~u'dinal number</definiens>
			</definition>
			<definition id="9">
				<sentence>.4 VATF denotes attributive verb , ~l/tu/ , ~ /sung/ and '\ [ u /nai/ are specific Thai words , A-B denotes a consecutive pair of A and 1t , aud A -- B denotes a possibly separated pair .</sentence>
				<definiendum id="0">VATF</definiendum>
				<definiendum id="1">A-B</definiendum>
				<definiendum id="2">-- B</definiendum>
				<definiens id="0">attributive verb</definiens>
				<definiens id="1">a consecutive pair of A and 1t , aud A</definiens>
			</definition>
			<definition id="10">
				<sentence>Each entry is of the form ( W_N1 , CLN2 , Freq ) where W denotes a noun , N1 denotes a number representing semantic class of W , CL denotes the associated classifier , N2 is a number indicating whether CL is a unit or collective classifier ( 1 for unit , 2 for collective ) and Freq denotes the frequency of cooccurrence between W and CL .</sentence>
				<definiendum id="0">W</definiendum>
				<definiendum id="1">N1</definiendum>
				<definiendum id="2">CL</definiendum>
				<definiendum id="3">N2</definiendum>
				<definiendum id="4">Freq</definiendum>
				<definiens id="0">a noun</definiens>
				<definiens id="1">a number representing semantic class of W</definiens>
				<definiens id="2">the associated classifier</definiens>
				<definiens id="3">a number indicating whether CL is a unit or collective classifier</definiens>
			</definition>
</paper>

		<paper id="2172">
			<definition id="0">
				<sentence>Siml &gt; ly put , it is this : a document is one of several types , and a machine processing of the document is to determine of wbicll type .</sentence>
				<definiendum id="0">document</definiendum>
				<definiens id="0">to determine of wbicll type</definiens>
			</definition>
			<definition id="1">
				<sentence>This text consists of m types of things , namely words from each of the W i. The frequencies ( pit , Pi~ , ... , pin , ) give the proportion of words from the classes W1 , W'2 , ... , Wm in text of type 7~ .</sentence>
				<definiendum id="0">, Wm</definiendum>
				<definiens id="0">frequencies ( pit , Pi~ , ... , pin , ) give the proportion of words from the classes W1 , W'2 , ...</definiens>
			</definition>
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>A perspective is a wording which is tailored to the lexical repertoire of an entity ; it is realizable as a clause , a phrase , or as a single lexeme .</sentence>
				<definiendum id="0">perspective</definiendum>
				<definiens id="0">a wording which is tailored to the lexical repertoire of an entity ; it is realizable as a clause , a phrase , or as a single lexeme</definiens>
			</definition>
			<definition id="1">
				<sentence>COURSE Formally speaking , an L~ f is a standard semanticolexical relation which holds between a lexeme 1,1 ( the keyword of f ) and a set of lexemes f ( t , ) ( the wdue of f ) .</sentence>
				<definiendum id="0">L~ f</definiendum>
				<definiens id="0">a standard semanticolexical relation which holds between a lexeme 1,1 ( the keyword of f ) and a set of lexemes f ( t , ) ( the wdue of f )</definiens>
			</definition>
			<definition id="2">
				<sentence>Vo : 'action ' Magn : 'intens @ y ) ' Operl : 'perform ' Incep : 'beginning ' Fin : % rid ' anus : ~C~llsa tioll ~ Manlf : 'manifest~ttion ' Syn ( bible ) = God 's Book Anti ( victory ) = defeat Gener ( lamb ) = meat Figur ( fog ) = wall \ [ of f &lt; ,9\ ] Cony71 ( to include ) : l/o\ ] belonfl So ( to teach ) : teachin9 $ 1 ( lie ) = liar Ao ( sun ) = solar V0 ( aeal ) = \ [ to\ ] , leal Magn ( beautg ) = real , stunnin 9 Operl ( cry ) = \ [ to\ ] let , , , ,t { a , : , 'y ) Ineep ( to sleep ) = \ [ to\ ] fall asleep Vln ( to sleep ) = \ [ t , ,\ ] , , , ak~ , ,i , Cans ( to sleep ) = \ [ to\ ] put to sleep Manif ( happy ) : \ [ to\ ] beam with joy Mel ' &amp; lk distinguishes about sixty simple t , l , 's on the above Idnd .</sentence>
				<definiendum id="0">Operl</definiendum>
				<definiendum id="1">leal Magn</definiendum>
				<definiens id="0">'intens @ y ) '</definiens>
			</definition>
			<definition id="3">
				<sentence>tinct classes of lexical discourse structure relations ; as , e.g. , the instantiation of the aST relation CONTRAST in ( this plan is also highly simplified ) : ( R2ONTRAST : actimn ( OCCUPATION/ SITUATION : actor Roman/ nation : actoe Gaul/ state : obligatory-roles ( : actor , actee , : situation ) ) : action2 ( DeCUPATION/ SITUATION : actor Roman/ nation : actee ( village /location : part-of : Gaul ) : negation + : obligatory-roles ( : actor , actee , : situation ) ) ) may be realized either as CONTI~ASTIVE CI , ARIFICATION ( ga ) or as CONTR .</sentence>
				<definiendum id="0">DeCUPATION/ SITUATION</definiendum>
				<definiendum id="1">ARIFICATION ( ga</definiendum>
				<definiens id="0">part-of : Gaul ) : negation + : obligatory-roles ( : actor , actee</definiens>
			</definition>
</paper>

		<paper id="2133">
			<definition id="0">
				<sentence>ints aniong categories , for exaniple , the following descriptiotl expresses a constraint that pronoun category and pointing category nnist be both instantiated wittliu a five se ( : onds~ Tl'pronoun , T2 '' point £ng , { Dill is T2 TI , Diff &lt; 5 } Timeout is a constraint of intervals belween an input and its succeeding input of n. streanl ( See Figure 4 ) .</sentence>
				<definiendum id="0">Dill</definiendum>
				<definiendum id="1">Timeout</definiendum>
			</definition>
			<definition id="1">
				<sentence>trails\ [ aLes into : noun ( Ts , Te , \ [ input ( Ts , Te , + &lt; window ' ' ) IN\ ] , N ) . A funcLor input/3 is inseri , cd into the third argunmnt forlllili , ~ the input , s\ [ , rCalll of { , he+ predicate. The third al'~lllllOnl , of t , he t'llllCl\ [ , or input/3 is the act , ual input item , the `` wimhm , '' string in this example. The first and second al'gUillelll , of input/3 is unitied wiLh the first and second argument of this unit clause r { ~spectiw ! ly , Th , ~refore , if a string `` window '' is input via lhe keyboard ~t , reum , the noun category is instant|areal , and the beginldng and end time of the noun category is Llle same as t , lle start and ( ! lid Lime attached to the `` window '' input. S trealliS Exl , ension frol. a single st , ream to nmltiple streams is easy. E ; t ( 'h stream needs four extra arguments two for t , imiug iuformnt , iou and two for expressi.g a consumed input Si.l'Calll , Thus , i\ [ ' there are n modes , 4n arguments arc ~.ldcd into head and goals argunlenl ; s. For e×anll~lc : , if l , hcre are two streams , the noun_phrase defiuitioa in Lhc previous section is translated into the following prolog l , 'edicaws with eight ( 2 x 4 ) extra argillllell\ [ , S : llOUU_i3hras e ( TxO , Tx , ~IxO , Nx , Ty O , Ty , NyO , Ny ) : article ( TxO , Txl , NxO , Nxi , TyO , Tyl , NyO , Nyl ) , adjective ( Tx2 , Tx3 , Nxl , Nx2 , Ty2 , Ty3 , Nyi , Ny2 ) , noun ( Tx4 , Tx , Nx2 , Nx , Ty4 , Ty , Ny2 , Ny ) . If there is at variable bindi\ ] tg within a goal like , Tinle -goal the goal is t , ranslat , cd into a con , jullcl , ion of two body goals ( for u single mode ) : ( goal ( T0 , `` 1'1 , R0 , R ) , Time- ( T0 , T1 ) ) 835 Ifthere exist n streams , tim variable Time is bound to a list of n time pairs , such as ~n'two modes : ( goal ( TxO , Txl , NxO , gxl , TyO , Tyl , NyO , Nyl ) , Time = \ [ ( TxO , Txl ) , ( TyO , Tyl ) \ ] ) The idea of understandillg multi-modal inputs in conjunction with each other , as presented in this paper , is not particularly new. The idea of a nnllti-n/odal input combining motions and pointing has been explored in a number of contexts. The classic 1980 paper `` I &gt; ut-ThatThere '' \ [ Bolt , 1980\ ] describes an early system that procedurally combined voice and gesture inputs .</sentence>
				<definiendum id="0">funcLor input/3</definiendum>
				<definiens id="0">describes an early system that procedurally combined voice and gesture inputs</definiens>
			</definition>
			<definition id="2">
				<sentence>MM-I ) ( : G is an extension of 1 ) CG for rnull.i-modal inpuls processing .</sentence>
				<definiendum id="0">MM-I ) ( : G</definiendum>
				<definiens id="0">an extension of 1 ) CG for rnull.i-modal inpuls processing</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>Transfer-Driven Machine Translation ( TDMT ) achieves efficient aitd robust translation within the example-based framework by adopting this parsing method .</sentence>
				<definiendum id="0">Transfer-Driven Machine Translation ( TDMT )</definiendum>
				<definiens id="0">achieves efficient aitd robust translation within the example-based framework by adopting this parsing method</definiens>
			</definition>
			<definition id="1">
				<sentence>A pattern represents meaningful units for linguistic structure and transfer in TDMT , and is defined as a sequence that consists of variables and synrbols representing constituent boundaries .</sentence>
				<definiendum id="0">pattern</definiendum>
				<definiens id="0">represents meaningful units for linguistic structure and transfer in TDMT</definiens>
				<definiens id="1">a sequence that consists of variables</definiens>
			</definition>
			<definition id="2">
				<sentence>An expression consists of words .</sentence>
				<definiendum id="0">expression</definiendum>
			</definition>
			<definition id="3">
				<sentence>ltion consists of head words in variable parts .</sentence>
				<definiendum id="0">ltion</definiendum>
				<definiens id="0">consists of head words in variable parts</definiens>
			</definition>
			<definition id="4">
				<sentence>Pattern-Based Translation : Conlcxt-Free Transducer and Its Al } t ) lication to Practical NI .</sentence>
				<definiendum id="0">Pattern-Based Translation</definiendum>
				<definiens id="0">Conlcxt-Free Transducer and Its Al } t ) lication to Practical NI</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>For instance , the distance ( the inverse of similarity ) between two Japanese lexicalforms is expressed by the difference of their values in a Japanese thesaurus called Bunrui-Goi-lIyou \ [ 5\ ] 3 as follows : I @ hcode ( w , ) bgheode ( ~ , ,~ ) \ [ + distance ( wl , w2 ) = bghmax + b '' where bghcode ( w ) is the code vMue in the BunruiGoi-Hyou , bghma : c is the maximal difference of the bghcodes , and 6 is a penalty value incurred when wl and w2 are not identical .</sentence>
				<definiendum id="0">bghcode</definiendum>
				<definiendum id="1">c</definiendum>
				<definiens id="0">expressed by the difference of their values in a Japanese thesaurus called Bunrui-Goi-lIyou \ [ 5\ ] 3 as follows : I @ hcode ( w , ) bgheode ( ~ , ,~ ) \ [ + distance</definiens>
			</definition>
			<definition id="1">
				<sentence>Not MI nodes in the source part of an exceptional translation pattern are necessarily single-quoted strings ; single-quoted string nodes and don bh+-quoted string nodes may be mixecl in a translation pattern , ht Figure 2 , ( tpl ) is an exceptional tr ; ulslation pattern and ( tp2 ) is a general translation pattern .</sentence>
				<definiendum id="0">MI</definiendum>
				<definiens id="0">nodes in the source part of an exceptional translation pattern are necessarily single-quoted strings ; single-quoted string nodes and don bh+-quoted string nodes may be mixecl in a translation pattern</definiens>
				<definiens id="1">an exceptional tr</definiens>
				<definiens id="2">a general translation pattern</definiens>
			</definition>
			<definition id="2">
				<sentence>s extra elements~ then the translation patterns of gl are marked extm-exeeptionaL Step 4 For each non-exceptional translation pattern group gl , if there is another general translation pattern group g~ such that any pattern ( Pl ) of gl is equivMent to any pattern of g2 other than the root node in the target part of Pt , then the translation patterns of gt are marked itth'aexceptional .</sentence>
				<definiendum id="0">Pl</definiendum>
				<definiens id="0">any pattern of g2 other than the root node in the target part of Pt</definiens>
			</definition>
			<definition id="3">
				<sentence>Further , step 4 identifies ( tpS ) as at , iutra-exceptional translation pattern , because ( tp5 ) is equivalent to the general translation patterns ( tp2 ) , ( tp3 ) and ( tp4 ) , other than `` use '' and `` practice '' in the root nodes of the target parts .</sentence>
				<definiendum id="0">tpS</definiendum>
				<definiens id="0">equivalent to the general translation patterns</definiens>
			</definition>
</paper>

		<paper id="2212">
			<definition id="0">
				<sentence>A construction consists of a set of features of form and a description of meaning in a context .</sentence>
				<definiendum id="0">construction</definiendum>
				<definiens id="0">consists of a set of features of form and a description of meaning in a context</definiens>
			</definition>
			<definition id="1">
				<sentence>The system consists of a NL grammar , a parser , an on-line calendar , a domain knowledge base ( about dates , times and meetings ) , an application knowledge base ( about the calendar ) , a speech recognizer , a speech generator .</sentence>
				<definiendum id="0">on-line calendar</definiendum>
				<definiens id="0">a domain knowledge base ( about dates , times and meetings ) , an application knowledge base ( about the calendar ) , a speech recognizer , a speech generator</definiens>
			</definition>
			<definition id="2">
				<sentence>tures A construction is given by the matrix : N : name_of_construction C : context \ ] V : structure M : ntessage The vehicle V consists of formulas describing presence ( or perhaps absence ) of certain taxemes , or features of form , within the structure of the ( 'onstruction .</sentence>
				<definiendum id="0">construction</definiendum>
			</definition>
			<definition id="3">
				<sentence>The feature struc is a list of variables and/or words/tokens ; it is used to describe the structure of a construction , and its role is similar to a rule in a generative grammar .</sentence>
				<definiendum id="0">feature struc</definiendum>
				<definiens id="0">a list of variables and/or words/tokens ; it is used to describe the structure of a construction</definiens>
			</definition>
			<definition id="4">
				<sentence>S ) V &lt; S cons_n &gt; -scnt ( a , s~rt , * ) &lt; p_sent truth_'value &gt; 0 \ ] M &lt; SM &gt; 4 At we can sue , the construction applies only in the context of a previously asked question , and its message says that the answer to the question is negative , after which it elaborates tile answer with a sentence S. The parts MINCAL consists of a NI , grarmnal ' , a t ) arser , a tit ) main knowledge base ( about dates , times and meetings ) , an on : line calendar ( Xdiary ) , an application knowledge base ( about Xdiary ) , a continuous speech recognizer ( IBM , ICSS ) , a speech generator ( Sl ) eech Plus , Text to Speech Converter ) , and the interfat : es .</sentence>
				<definiendum id="0">continuous speech recognizer ( IBM , ICSS )</definiendum>
				<definiendum id="1">Text to Speech Converter</definiendum>
				<definiens id="0">of a previously asked question , and its message says that the answer to the question is negative , after which it elaborates tile answer with a sentence S. The parts MINCAL consists of a NI , grarmnal ' , a t ) arser , a tit ) main knowledge base ( about dates , times and meetings ) , an on : line calendar ( Xdiary ) , an application knowledge base</definiens>
			</definition>
			<definition id="5">
				<sentence>The referential level is a t ) artially ordered collection of theories ; it encodes background knowledge in a way resembling a dictionary t ) r art encyelopedia .</sentence>
				<definiendum id="0">referential level</definiendum>
				<definiens id="0">a t ) artially ordered collection of theories ; it encodes background knowledge in a way resembling a dictionary t</definiens>
			</definition>
</paper>

		<paper id="2137">
			<definition id="0">
				<sentence>I. Characterization of Applicative Universal Grammar Applicative Universal Grmnmar ( AUG ) is a linguistic theory that uses the lormalism of catcgorial g~unmar , as a means for representing the structure of language .</sentence>
				<definiendum id="0">I. Characterization of Applicative Universal Grammar Applicative Universal Grmnmar ( AUG )</definiendum>
				<definiens id="0">a linguistic theory that uses the lormalism of catcgorial g~unmar , as a means for representing the structure of language</definiens>
			</definition>
			<definition id="1">
				<sentence>AUG includes a system of combinators ( Curry and Feys , 1958 ) , 'rod fi ) nnulates semiotic concepts , principles , and laws that dctcnninc tile fimctioning of natur~d languages , as sign systems ( for a complete description of AUG , see Shaumyan , 1974 , 1977 , 1987 ; Dcsci6s , 1990 ; Scgond , 1990a ; some applications of AI \ ] G arc discussed in Shaumyan 1989 , 1991 ) .</sentence>
				<definiendum id="0">AUG</definiendum>
				<definiens id="0">includes a system of combinators</definiens>
			</definition>
			<definition id="2">
				<sentence>AUG recognizes two primitive types -- terms ( nouns and noun-phrases ) and sentences , denoted by t mid s , respectively .</sentence>
				<definiendum id="0">AUG</definiendum>
				<definiens id="0">recognizes two primitive types -- terms ( nouns and noun-phrases</definiens>
			</definition>
			<definition id="3">
				<sentence>2 ) If x and y m'e O-types , then Oxy is an O-lype .</sentence>
				<definiendum id="0">Oxy</definiendum>
				<definiens id="0">an O-lype</definiens>
			</definition>
			<definition id="4">
				<sentence>The basic rule of combination of phrases is the Rule of Phrase Application , which is defined as follows : Phrase A of type Oxy , called an operator , combines with phrase B of type x , called its operand , to form phrase AB of type y , called its resultant : Oxy A x B y ( AB ) ( 2 ) The applicative tree of ( 2 ) has the form : y ( AB ) Oxy A x B ( 3 ) `` llm concept of immediate constituents is defined as : If phrase A is ml operator and phm~ B is its operand , then they , are inunediate constituenls of file resultant ( AB ) .</sentence>
				<definiendum id="0">basic rule of combination of phrases</definiendum>
				<definiendum id="1">Oxy A x B y ( AB</definiendum>
				<definiendum id="2">y ( AB ) Oxy A x B</definiendum>
				<definiens id="0">the Rule of Phrase Application , which is defined as follows : Phrase A of type Oxy , called an operator , combines with phrase B of type x , called its operand , to form phrase AB of type y</definiens>
				<definiens id="1">If phrase A is ml operator and phm~ B is its operand , then they , are inunediate constituenls of file resultant ( AB )</definiens>
			</definition>
			<definition id="5">
				<sentence>In terms of `` algebra , the Principle of Uniqueness of hnmediate Constituents con'esponds to non-associativity : AUG is a non-associative system .</sentence>
				<definiendum id="0">AUG</definiendum>
				<definiens id="0">a non-associative system</definiens>
			</definition>
			<definition id="6">
				<sentence>To make the AUG notation compact , we introduce recursively defined adjoined symbols ( Shaumyan 1987 : 199 ) : A type symbol is called adjoined if it is introduced into tile type system by a definition of file form : Z = Oxy where z denotes , 'm adjoined type and Oxy denotes a type where x and y are either other , adjoined type symbols , oft , ors .</sentence>
				<definiendum id="0">Oxy</definiendum>
				<definiens id="0">A type symbol is called adjoined if it is introduced into tile type system by a definition of file form : Z = Oxy where z denotes , 'm adjoined type</definiens>
			</definition>
			<definition id="7">
				<sentence>Sleedman 's Combiuatory Categorial Grammar ( CCG ) proposes file following , -malysis of the phrase Apples which Harty eats ( 1987:415 ; presented hcrc in the AUG notation ) : ( apples ) which llarty OOtsOtt OOtss Ott eats OtOts ¢OIII|X ) Se backward Ots apply forward ( 15 ) In ( 15 ) , subject type raising ( assigning OOtss to Harry ) in coujuuction with composition is used to resolve tile difficulty caused by gapping involved ill the extraction of the direct object of the finite verb eats .</sentence>
				<definiendum id="0">CCG</definiendum>
				<definiens id="0">file following , -malysis of the phrase Apples which Harty eats ( 1987:415</definiens>
			</definition>
			<definition id="8">
				<sentence>( 16 ) Type rai~ing is defiue ( l as , 'm operation whereby an oper , 'rod acquires a new type that turus it into , 'm operator over its operator .</sentence>
				<definiendum id="0">'rod</definiendum>
				<definiens id="0">acquires a new type that turus it into , 'm operator over its operator</definiens>
			</definition>
			<definition id="9">
				<sentence>The general rule of type raising in tile AUG notation is : x -~ OOxyy ( 17 ) 854 For exmnple , subject type raising is delined in lerms of AUG as follows : Subject type raising is a proccss by which a subject of type t acquires the type OOtss , which turns it into an opcmtor over the predicate ( 51 type Ots .</sentence>
				<definiendum id="0">OOtss</definiendum>
				<definiens id="0">turns it into an opcmtor over the predicate ( 51 type Ots</definiens>
			</definition>
			<definition id="10">
				<sentence>Unfortunately CCG fails to distinguish I ) ctween thc two interpretations .</sentence>
				<definiendum id="0">CCG</definiendum>
			</definition>
			<definition id="11">
				<sentence>qtfis sentence is an exmnple of long-distance dependency because the subject intervenes between the direct object and the predicate .</sentence>
				<definiendum id="0">qtfis sentence</definiendum>
				<definiens id="0">an exmnple of long-distance dependency because the subject intervenes between the direct object and the predicate</definiens>
			</definition>
			<definition id="12">
				<sentence>Polymorphism is a situation when a word is assigned several types , having equal syntactic weight .</sentence>
				<definiendum id="0">Polymorphism</definiendum>
				<definiens id="0">a situation when a word</definiens>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>Restrictive adverbials ( such as locatives and time adverbials ) will generally be encoded as presented , where R0 is a meta-wu'iable that is instantiated by the restriction introduced by the adjunct .</sentence>
				<definiendum id="0">Restrictive adverbials</definiendum>
				<definiendum id="1">R0</definiendum>
				<definiens id="0">such as locatives and time adverbials</definiens>
				<definiens id="1">a meta-wu'iable that is instantiated by the restriction introduced by the adjunct</definiens>
			</definition>
</paper>

		<paper id="2166">
			<definition id="0">
				<sentence>In contrast to traditionM Pfi , which only has 3 and V , the language of generalized qnantifiers L ( GQ ) specilies no limitation of the number of primitives to express quantification .</sentence>
				<definiendum id="0">Pfi</definiendum>
				<definiens id="0">the language of generalized qnantifiers L ( GQ ) specilies no limitation of the number of primitives to express quantification</definiens>
			</definition>
			<definition id="1">
				<sentence>DBSR , which stands for DataBase specitic Semantic Representation , is a declarative relational database query language that is both close to GSR and easily translatable to any of the commercialized \ ] { .</sentence>
				<definiendum id="0">DBSR</definiendum>
				<definiens id="0">stands for DataBase specitic Semantic Representation , is a declarative relational database query language</definiens>
			</definition>
</paper>

		<paper id="2153">
			<definition id="0">
				<sentence>Further morc , if you want to obtain the co-occurrence frcqucncy of each two adjacent part of speeches , which is helpful to the study of part of speech ( POS ) tagging , you must annotate the corpora with POS inIbrmation .</sentence>
				<definiendum id="0">POS</definiendum>
				<definiens id="0">if you want to obtain the co-occurrence frcqucncy of each two adjacent part of speeches , which is helpful to the study of part of speech (</definiens>
			</definition>
			<definition id="1">
				<sentence>Formally , context dependent rule is represented as : c~xyfl~ s ( Shif't ) c~xy\ [ l~ ( z , y , h ) ( Reduce Where x , y are the top two elements in the stack , and cqfl are the context on the left hand ofx and the context on the right hand of y respectively .</sentence>
				<definiendum id="0">cqfl</definiendum>
				<definiens id="0">the context on the left hand ofx and the context on the right hand of y respectively</definiens>
			</definition>
			<definition id="2">
				<sentence>Ilere are two rulcs obtained : -- - &lt; R &gt; &lt; VY &gt; -~ &lt; R &gt; &lt; USDE &gt; &lt; A &gt; &lt; NG &gt; &lt; + &gt; -~ ( SV , SUB , A ) -- &lt; SV &gt; &lt; R &gt; &lt; USDE &gt; ~4z &lt; A &gt; &lt; NG &gt; &lt; o &gt; -~s After the reduction , the phrasc structurc formed rcplaces the top two elements in the stack .</sentence>
				<definiendum id="0">SUB</definiendum>
				<definiens id="0">-- - &lt; R &gt; &lt; VY &gt; -~ &lt; R &gt; &lt; USDE &gt; &lt; A &gt; &lt; NG &gt; &lt; + &gt; -~</definiens>
			</definition>
			<definition id="3">
				<sentence>NUMBER-OF-ACTION records the number of total actions ( either shift or reduce ) during tagging , NUMBER-OF-AUTOMATION is the number of actions ( given by the system itselt ) which are conlirmed to bc right by human .</sentence>
				<definiendum id="0">NUMBER-OF-ACTION</definiendum>
				<definiendum id="1">NUMBER-OF-AUTOMATION</definiendum>
				<definiens id="0">records the number of total actions ( either shift or reduce ) during tagging</definiens>
			</definition>
			<definition id="4">
				<sentence>In this interactive process , human is asked to dctermine what action should be taken , he first inspect the rule-list to see if there is already a rule correctly confirming with current context , if not , he should tell the system whether `` shift '' or '/reduce '' , if `` reduce '' , he is requested to tell the system what phrase structure and what dependency relation is to be built , and which element , the top element of the stack , or the second is the head .</sentence>
				<definiendum id="0">dependency relation</definiendum>
				<definiens id="0">reduce '' , he is requested to tell the system what phrase structure and what</definiens>
			</definition>
			<definition id="5">
				<sentence>NUMBER-OF-ACTIONS records the total times of operation ( shift or reduce ) during tagging .</sentence>
				<definiendum id="0">NUMBER-OF-ACTIONS</definiendum>
				<definiens id="0">records the total times of operation ( shift or reduce ) during tagging</definiens>
			</definition>
			<definition id="6">
				<sentence>For example , ~ ( VG ) ~L , ( NG1 ) I'/ , J ( USDE ) ~'~ ( NG2 ) is an ambiguous phrasc ( which may be { ( VG , nil , GOV ) , ( NG1 , USDE , DEP ) , ( USDE , NG2 , ATTA ) , ( NG2 , VG , OBJ ) } which means `` killcd the hunter 's dog ' , or { ( VG , USDE , DEP ) , ( NG1 , VG , OBJ ) , ( USDE , NG2 , ATTA ) , ( NG2 , nil , GOV ) } which means the dog which killed the hunter .</sentence>
				<definiendum id="0">NG1</definiendum>
				<definiens id="0">an ambiguous phrasc ( which may be { ( VG , nil</definiens>
			</definition>
</paper>

		<paper id="1084">
			<definition id="0">
				<sentence>A technical lexicon consists of simple as well as complex lexical units .</sentence>
				<definiendum id="0">technical lexicon</definiendum>
			</definition>
			<definition id="1">
				<sentence>The horizontM axis grows with the score .</sentence>
				<definiendum id="0">horizontM axis</definiendum>
				<definiens id="0">grows with the score</definiens>
			</definition>
			<definition id="2">
				<sentence>ol , ~4 &gt; 1 &gt; ) x c ( &gt; ) where C ( ) means `` count of '' , and p `` pattern of '' ; probes stands for the probabilities we estimated , and sent represents the set of aligned sentences in which both the source candidate , s , and the target candidate , t , appear .</sentence>
				<definiendum id="0">C ( )</definiendum>
				<definiens id="0">means `` count of '' , and p `` pattern of '' ; probes stands for the probabilities we estimated , and sent represents the set of aligned sentences in which both the source candidate , s , and the target candidate , t</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>Transfer-Driven Machine Translation\ [ ll , \ [ 2\ ] , \ [ 9\ ] , ( TDMT ) is a translation technique which utilizes empirical transfer knowledge compiled from actual translation examples .</sentence>
				<definiendum id="0">Transfer-Driven Machine Translation\</definiendum>
				<definiendum id="1">TDMT</definiendum>
				<definiens id="0">a translation technique which utilizes empirical transfer knowledge compiled from actual translation examples</definiens>
			</definition>
			<definition id="1">
				<sentence>The transfer module , which is the heart of the TDMT system , transfers source language expressions into target language expressions using bilingual translation knowledge .</sentence>
				<definiendum id="0">transfer module</definiendum>
				<definiens id="0">the heart of the TDMT system , transfers source language expressions into target language expressions using bilingual translation knowledge</definiens>
			</definition>
			<definition id="2">
				<sentence>1 Configuration of Bidirectional TDMT System 64 The TDMT system utilizes an example-based framework to translate a sentence .</sentence>
				<definiendum id="0">TDMT system</definiendum>
				<definiens id="0">utilizes an example-based framework to translate a sentence</definiens>
			</definition>
</paper>

		<paper id="2187">
			<definition id="0">
				<sentence>Now we take a very simple view about tile gk ) bal structure of discourse : syntactically , discourse is just lllerc and throughout we use 01 for a subject ( NOMinative ) zero ; 02 for a object ( ACCusative ) zero ; TOP for a topic case ; I ) AT fox '' a dative ( indirect object ) c~me ; PASS for a passive mor\ ] . )</sentence>
				<definiendum id="0">TOP for</definiendum>
				<definiens id="0">a topic case ; I ) AT fox '' a dative ( indirect object</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus a discourse should look like Figure 1 , where G denotes a discourse segment .</sentence>
				<definiendum id="0">G</definiendum>
			</definition>
			<definition id="2">
				<sentence>qi represents a nominal occurrence in Gi .</sentence>
				<definiendum id="0">qi</definiendum>
				<definiens id="0">a nominal occurrence in Gi</definiens>
			</definition>
			<definition id="3">
				<sentence>where tf ( term frequency ) is the number of occurrences of a term Tj in a document Di ; df ( document frequency ) is tile number of documents in a collection of N documents in which 7 ) occurs ; and the importance , wljis given ~s the product of if and the inverse dffactor , or idf , log N/dfi .</sentence>
				<definiendum id="0">tf</definiendum>
				<definiendum id="1">term frequency )</definiendum>
				<definiendum id="2">df ( document frequency )</definiendum>
				<definiens id="0">the number of occurrences of a term Tj in a document Di ;</definiens>
				<definiens id="1">tile number of documents in a collection of N documents in which 7 ) occurs ; and the importance , wljis given ~s the product of if and the inverse dffactor , or idf</definiens>
			</definition>
			<definition id="4">
				<sentence>For term vectors X = ( xl , x~ , ... , x~ ) and Y = ( Yt , Y2 , ... , Yt ) , let the coherence be defined by : t /=1 C ( X , Y ) = t t i- .</sentence>
				<definiendum id="0">Yt )</definiendum>
				<definiens id="0">xl , x~ , ... , x~ ) and Y = ( Yt , Y2 , ... ,</definiens>
			</definition>
			<definition id="5">
				<sentence>where : k+A-I ifk &lt; n-A v n otherwise Measurements are made at interwfls I1 through/n-l. 11 { o7 140 '' \ ] 2o 100 '' 80 '' 60 '' 40= 0 • • .1 '' .. N '' -'~ '' `` , , , r ' .u.-.'T ' , .I , ,-I- , u- , ,u ' , u 200 400 600 800 1.0001200140016001800 WORDS ( TOKENS ) Figure 5 : A VMP Analysis What we like to see is how the scheme ( 'Omlmres with 1147 0D9OD80D70D60D5004003-0D20D1-O ' , o • , , • i a i g t i i n a a I o i i i ! ! i ! l I l I I I I i I I I I I I ' : ; I : i i ffi i ! ^ , I i ; I i I I l l , , , o : i ' \ [ ! ~ : : . . a i | | i i J i a l t i | I I I `` I ' ' I I I I I 100 200 , . i I I q J 300 i I I I ! l l : \ [ ' ' I ! a 4OO | i | | i 500 Figure 6 : The Dice on the Nikkei corpus ' I 600 ours. Figure 5 shows the results of a VMP analysis for the same nominal collection as above. The interval is set to 300 words , or the average length of a paired window in the previous analysis. The y-axis corresponds to the number of new words ( TYro , : ) and the x-axis to an interval position ( TOK~ ; N ) . As it tnrns out , the VMP fails to detect any significant pattern in the corpus. One of the problems with tim analysis has to do with its generality ( Kozima , 1993 ) ; a text with many repetitions and/or limited vocabulary would yield a flattened VMP , which , of course , does not tell us much about its inner strncturings. Indeed , this could be the case with Figure 5. We suspect that the VMP scheme fares better with a short-term coherency than with a long-term or global coherency. Evaluation l , 'igure 6 demonstrates the results of the Dice analysis on the nikkei collection of editorial articles. What we see here is a close correspondence between the Dice curve an ( l the global discourse structure. Evaluation here simply consists of finding major minima on the graph and locating them on the list of those discourse segments which comprise the corpus. The procedure is performed by hand. Correspondences evaluation has been l ) roblematical , since it requires human judgments o~ how discourse is structnred , whose reliability is yet to be demonstrated. It was decided here to use copious boundary indicators such as an article or paragraph break for evalnating matches between the Dice analysis and the discourse. For us , discourse structure reduces to just an orthographic structure 7. In the figure , article boundaries are marked by dashes. 7 out of 27 local minima are found to be incorrect , which puts the error rate at around 25 % . We obtained similar results for the Jaccard and cosine coefficient. A possible improvement would include adjusting the document frequency ( d\ ] ) factor for index terms ; the average df factor we had for the Nikkei corpns is around 1.6 , which is so low as to be negligible s. ; 'Yet , there is some evidence that an orthographic structure is liuguislically significant ( Fujisawa et al. , 1993 ; Nunberg , 19 ( 00 ) . 8IIowever , the averagc df factor would increase in proportion Another interesting possibilty is to use an alternative weighting policy , the weighled invet~se documenl frequency ( Tokunaga and Iwayama , 1994 ) . A widfvalue of a term is its frequency within the docmnent divided by its frequency throughout the entire document collection. `` \ [ 'he widf policy is reported to have a marked advantage over the idffor the text categorization task. Recall and Precision As with the document analysis , the effectiveness of text segmentation appears to be dictated by recall/precision parameters where : number of correct boundaries retrieved Fecall = total number o/'correct boundaries number of correct boundaries retrieved precision = total number of boundaries retrieved A boundary here is meant to be a minimum on the coherence graph. Precision is strongly affected by the size of block or intervalg ; a large-block segmentation yields less boundaries than a small-block segmentation. ( Table 1 ) . Experiments were made on the Nikkci corBlock Size 5 10 15 20 25 30 35 Boundm'ies 52 25 24 20 21 19 12 Table h Block size ( in word ) and the nnmber of boundaries retrieved. pus to examine the effccts of the block size parameter on text segmentation. The corpus was divided into equMly sized blocks , ranging from 5 to 35 words in length. The window size was kept to 10 blocks. Shown in Figure 7 are the results given in terms of recall and precision. Also , a partitioning task with discourse segments , whose length varies widely , is measured for recall and precision , and the result is represented as G. Averaging recall and precision values for each size gives to the growth of corpus size. It is likely , therefore , that with a 1148 ftTO40~ ( LI0 0 th'ecision + 35 30 15 $ + + 20 10 + 25 G + 5+ Recall `` 1 I ' J Figure 7 : Rec.all and Precision an ordering : 35 &lt; 25 &lt; 20 &lt; 5 &lt; 30 &lt; 15 &lt; 10 &lt; ( ; . O ranks highest , whose average value , 0.66 , is higher than any other. ~10 ' comes second ( 0.61 ) 1° . ( It is an interesting coincidence that the average length of dis= course segments is \ [ 3.7 words. ) The results demon : strate in quantitative terms the significance of diseotlrse segments. It is worth t ) ointing out that l , he method here ix rather selectlw'~ about a level of granularity it detects , namely , that of a news article. It is possible , however , to have a much smaller granularity ; as shown in Table 1 , decreasing the block size would give a segmentation of a smaller granularity. Still , we chose not to work on fine-grained segmentations I ) ( ; cause they lack a reliable evaluation metric ~ t. Conclusion In this l ) aper , we haw ; described a method for I ) art , ition ing , qll unstructured eorlms into coherent textual milts. We have adoptc~d the view that discourse consists of contiguous , non-overlal ) ping discourse segments. We have referred to a vector sl ) acc model for a statistical representation of discourse seglnellt. ( Joherence b ( &gt; tween segments is determined by the Dice coefficient with the If .</sentence>
				<definiendum id="0">k+A-I ifk &lt; n-A v n otherwise Measurements</definiendum>
				<definiendum id="1">document frequency</definiendum>
				<definiens id="0">2o 100 '' 80 '' 60 '' 40= 0 • • .1 '' .. N '' -'~ '' `` , , , r ' .u.-.'T ' , .I , ,-I- , u- , ,u ' , u 200 400 600 800 1.0001200140016001800 WORDS ( TOKENS ) Figure 5 : A VMP Analysis What we like to see is how the scheme ( 'Omlmres with 1147 0D9OD80D70D60D5004003-0D20D1-O ' , o • ,</definiens>
				<definiens id="1">The Dice on the Nikkei corpus ' I 600 ours. Figure 5 shows the results of a VMP analysis for the same nominal collection as above. The interval is set to 300 words , or the average length of a paired window in the previous analysis. The y-axis corresponds to the number of new words ( TYro , : ) and the x-axis to an interval position ( TOK~ ; N ) . As it tnrns out , the VMP fails to detect any significant pattern in the corpus. One of the problems with tim analysis has to do with its generality ( Kozima , 1993 ) ; a text with many repetitions and/or limited vocabulary would yield a flattened VMP , which , of course</definiens>
				<definiens id="2">a close correspondence between the Dice curve an ( l the global discourse structure. Evaluation here simply consists of finding major minima on the graph and locating them on the list of those discourse segments which comprise the corpus. The procedure is performed by hand. Correspondences evaluation has been l ) roblematical , since it requires human judgments o~ how discourse is structnred , whose reliability is yet to be demonstrated. It was decided here to use copious boundary indicators such as an article or paragraph break for evalnating matches between the Dice analysis and the discourse. For us</definiens>
				<definiens id="3">d\ ] ) factor for index terms ; the average df factor we had for the Nikkei corpns is around 1.6 , which is so low as to be negligible s.</definiens>
				<definiens id="4">to use an alternative weighting policy , the weighled invet~se documenl frequency ( Tokunaga and Iwayama , 1994 ) . A widfvalue of a term is its frequency within the docmnent divided by its frequency throughout the entire document collection. `` \ [ 'he widf policy is reported to have a marked advantage over the idffor the text categorization task. Recall and Precision As with the document analysis , the effectiveness of text segmentation appears to be dictated by recall/precision parameters where : number of correct boundaries retrieved Fecall = total number o/'correct boundaries number of correct boundaries retrieved precision = total number of boundaries retrieved A boundary here is meant to be a minimum on the coherence graph. Precision is strongly affected by the size of block or intervalg ; a large-block segmentation yields less boundaries than a small-block segmentation. ( Table 1 ) . Experiments were made on the Nikkci corBlock Size 5 10 15 20 25 30 35 Boundm'ies 52 25 24 20 21 19 12 Table h Block size ( in word ) and the nnmber of boundaries retrieved. pus to examine the effccts of the block size parameter on text segmentation. The corpus was divided into equMly sized blocks</definiens>
				<definiens id="5">the results given in terms of recall and precision. Also , a partitioning task with discourse segments , whose length varies widely , is measured for recall and precision , and the result is represented as G. Averaging recall and precision values for each size gives to the growth of corpus size. It is likely , therefore , that with a 1148 ftTO40~ ( LI0 0 th'ecision + 35 30 15 $ + + 20 10 + 25 G + 5+ Recall `` 1 I ' J Figure 7 : Rec.all and Precision an ordering : 35 &lt; 25 &lt; 20 &lt; 5 &lt; 30 &lt; 15 &lt; 10 &lt; ( ; . O ranks highest , whose average value , 0.66 , is higher than any other. ~10 ' comes second ( 0.61 ) 1° . ( It is an interesting coincidence that the average length of dis= course segments is \ [ 3.7 words. ) The results demon : strate in quantitative terms the significance of diseotlrse segments. It is worth t ) ointing out that l , he method here ix rather selectlw'~ about a level of granularity it detects , namely , that of a news article. It is possible , however , to have a much smaller granularity</definiens>
			</definition>
</paper>

		<paper id="2136">
			<definition id="0">
				<sentence>x l ) ~ttLerl , COl , Version knowledge , which helps user to COilll ) OSe F , nglish greatly , retricwd designed to reduce the number of nperi~ .</sentence>
				<definiendum id="0">Version knowledge</definiendum>
				<definiens id="0">helps user to COilll ) OSe F , nglish greatly , retricwd designed to reduce the number of nperi~</definiens>
			</definition>
			<definition id="1">
				<sentence>TWP brings about both users ' contentment with functions that machine translation has failed to realize , and large cost reduction ibr sentence generation in target language that simple dictionary consultation facility has not fulfilled .</sentence>
				<definiendum id="0">TWP</definiendum>
			</definition>
			<definition id="2">
				<sentence>TWP peribrms automatic recognition of a word , a phrase , and a simple sentence , and immediately after that , successive conversion based on bilingual word dictionary ~ and simple phrase and simple sentence translation functim , is executed .</sentence>
				<definiendum id="0">TWP</definiendum>
				<definiens id="0">peribrms automatic recognition of a word , a phrase , and a simple sentence , and immediately after that , successive conversion based on bilingual word dictionary ~ and simple phrase and simple sentence translation functim</definiens>
			</definition>
			<definition id="3">
				<sentence>TWP makes users feel content with processes on a screen by confirming every step of translation with their eyes .</sentence>
				<definiendum id="0">TWP</definiendum>
			</definition>
			<definition id="4">
				<sentence>The scope determination tbr conversion is one of key issues to specify eemfortable user interfa .</sentence>
				<definiendum id="0">scope determination tbr conversion</definiendum>
				<definiens id="0">one of key issues to specify eemfortable user interfa</definiens>
			</definition>
			<definition id="5">
				<sentence>TWP adopts such a simple strategy as a scope tbr conversion is the biggest logical unit of words , phrases , and clauses just adjacent to C ( cursor ) on the left part of input line .</sentence>
				<definiendum id="0">TWP</definiendum>
				<definiens id="0">adopts such a simple strategy as a scope tbr conversion is the biggest logical unit of words , phrases</definiens>
			</definition>
			<definition id="6">
				<sentence>TWP makes users feel content with processes on a screen by confirming every step of translation with their eyes .</sentence>
				<definiendum id="0">TWP</definiendum>
			</definition>
</paper>

		<paper id="2182">
			<definition id="0">
				<sentence>Now , allhough he can not refashion the expression himself , he does have the ability to help the initiator by suggesting a good way to expand it ; suggestion is a conversational move in which an agent suggests a new attribute that he deems would increase his confidence in the cxpressiou 's adequacy if the expression were expanded to include the attribute .</sentence>
				<definiendum id="0">suggestion</definiendum>
				<definiens id="0">a conversational move in which an agent suggests a new attribute that he deems would increase his confidence in the cxpressiou 's adequacy if the expression were expanded to include the attribute</definiens>
			</definition>
</paper>

		<paper id="2167">
			<definition id="0">
				<sentence>Term formation follows certain guidelines ( procedures ) which vary in sophistication depending on the subject domain .</sentence>
				<definiendum id="0">Term formation</definiendum>
				<definiens id="0">follows certain guidelines ( procedures ) which vary in sophistication depending on the subject domain</definiens>
			</definition>
			<definition id="1">
				<sentence>-* term - { word_suffix , Compounding operates in a similar fashion : term -~ term + word term -+ term -Iterm term -- ~ word -F term word -~ word -F word .</sentence>
				<definiendum id="0">Compounding</definiendum>
				<definiens id="0">operates in a similar fashion : term -~ term + word term -+ term -Iterm term -- ~ word -F term word -~ word -F word</definiens>
			</definition>
			<definition id="2">
				<sentence>Underline denotes a feature variahle , whose name indicates the set of possible values taken by the feature .</sentence>
				<definiendum id="0">Underline</definiendum>
			</definition>
			<definition id="3">
				<sentence>Subcategorisation and makes information is stored in the morphosyntax tleld of an afflx 's lexical entry .</sentence>
				<definiendum id="0">Subcategorisation</definiendum>
				<definiens id="0">makes information is stored in the morphosyntax tleld of an afflx 's lexical entry</definiens>
			</definition>
</paper>

		<paper id="2124">
			<definition id="0">
				<sentence>I The upper row ( the direct meaning row ) it : this tel &gt; resentation shows the state wherifin the number of solved probh ; m.~ is the number that appear .</sentence>
				<definiendum id="0">upper row</definiendum>
				<definiendum id="1">m.~</definiendum>
				<definiens id="0">the direct meaning row ) it : this tel &gt; resentation shows the state wherifin the number of solved probh ;</definiens>
				<definiens id="1">the number that appear</definiens>
			</definition>
			<definition id="1">
				<sentence>This intersection operation is a simple and naturM way to calculate possible answers to a question which includes a number .</sentence>
				<definiendum id="0">intersection operation</definiendum>
				<definiens id="0">a simple and naturM way to calculate possible answers to a question which includes a number</definiens>
			</definition>
			<definition id="2">
				<sentence>Interpretation ( C ) The above is a possible utterance , which requires ~nother interpretation .</sentence>
				<definiendum id="0">Interpretation ( C</definiendum>
				<definiens id="0">a possible utterance , which requires ~nother interpretation</definiens>
			</definition>
			<definition id="3">
				<sentence>779 nl &amp; lly { - , M , - , - , - ; A , M , , , J ( 1 ) row Ileverse each { A , - , S , ~ , N } { - , - , S , S , N } / ( 2 ) COMMON / { - , - , S , F , l - , - , S , F , NJ Usual interpretation \ ( 2 ) DIFFERENT \ { 7 , ' ' ' '- } Unusual interpretation Figure 7 : Not many ~ : L few - , - , - , F , A , M , S , F , } ( 1 ) Reverse each row { A , M , S , - , N } / ( 2 ) COMMON / { - , , , , ; } Unusual interpretatiou , N } \ ( 2 ) DIFFERENT \ A , M , S , - , -J Usual interpretation Figure 8 : Not a few This paper introduced eight basic degree primitives for degree concept , that is , 'A , ' 'M , ' 'S , ' 'F , ' 'N , ' ' &gt; n , ' '=n , ' and ' &lt; n. ' ttowever , the authors do not claim that these eight primitives are sufficient to indicate all degree concepts. Instead~ the authors clMm that people comprehend degree concepts in a discrete way , and that degree concepts are identified by their relatlw~ positions in the fl'amework of understanding. Consider the following cxan~ples concerning , ~nother degree concept 'several , ' which differs from these eight degree concepts. ( 13 ) They legally have several wives. Quantities , which are refl.'rred to by 'sew'.ral ' and 'a few , ' seem to be close. It is often said that quantities ret &gt; rred to by 'several ' include fiw . '</sentence>
				<definiendum id="0">'M</definiendum>
				<definiendum id="1">'N</definiendum>
				<definiens id="0">basic degree primitives for degree concept</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>MNI'ES/EK is a typical transfi'x-based system , which does English sentence analysis , tnmsforms the result ( parse tree ) into an intermediate repre- .</sentence>
				<definiendum id="0">MNI'ES/EK</definiendum>
				<definiens id="0">a typical transfi'x-based system , which does English sentence analysis</definiens>
			</definition>
			<definition id="1">
				<sentence>rall configuration of MNI'ES/EK , which has R ) llowing features : * Morphological Analysis Using l'¢-gr ; ml : We resolve tile category ambiguities by combining the N-gram and the rules .</sentence>
				<definiendum id="0">MNI'ES/EK</definiendum>
				<definiens id="0">has R ) llowing features : * Morphological Analysis Using l'¢-gr</definiens>
			</definition>
			<definition id="2">
				<sentence>The morphologicM generator is an automaton that does the synthesis and separatim , of morphemes according to the context and Korean morpheme combination rule .</sentence>
				<definiendum id="0">morphologicM generator</definiendum>
				<definiens id="0">an automaton that does the synthesis and separatim , of morphemes according to the context</definiens>
			</definition>
			<definition id="3">
				<sentence>Je ( ft obji~\ [ i fief ) % pipelinhlg perfomumce lb st jfct ot , jeqt missing parMlclism Figure 3 : An example of English syntactic structure and the corresponding English dependency structure which is described in LSS , where ; PILED ( PREDicate ) the head of a sentence , normMly verbs or adjectives are selected , COMN ( COMplement Noun ) a node that leads a noun phrase , PREA ( PREdicate Adjective ) corresponds to a verb or an adjective in an adjective phrase , PREB ( Pll , Edicate adverB ) corresponds to a verb or an adjective in an adverb phrase .</sentence>
				<definiendum id="0">Je</definiendum>
				<definiendum id="1">PREB</definiendum>
				<definiens id="0">the head of a sentence , normMly verbs or adjectives are selected , COMN ( COMplement Noun ) a node that leads a noun phrase</definiens>
				<definiens id="1">a verb or an adjective in an adjective phrase</definiens>
			</definition>
</paper>

		<paper id="2209">
			<definition id="0">
				<sentence>Wn be a span of ambiguous words in scntence and Wl , W n are unanlbiguous , C -- Ct ... Cn be a possible tag sequence for the span , where Ci is a category of Wi .</sentence>
				<definiendum id="0">Ci</definiendum>
			</definition>
			<definition id="1">
				<sentence>Let Cu is a possible tag set for unregistered word , CI is the tag of its left word and the Cr is the tag of its right word .</sentence>
				<definiendum id="0">Cu</definiendum>
				<definiendum id="1">CI</definiendum>
				<definiens id="0">a possible tag set for unregistered word ,</definiens>
				<definiens id="1">the tag of its right word</definiens>
			</definition>
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>and Ide N.M. , 1990 ; Ide N.M. and Veronis J. , 1993 ) use sew~ral dictionaries and to improve the ratio of words disambiguated to ambiguous words .</sentence>
				<definiendum id="0">Ide N.M.</definiendum>
				<definiens id="0">the ratio of words disambiguated to ambiguous words</definiens>
			</definition>
			<definition id="1">
				<sentence>• Memory capacity is O ( mn ) compared to O ( n `` 2 ) of correlative Inodel , where m is average immber of wards per scene , and n is the total number af possible words .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">average immber of wards per scene , and</definiens>
				<definiens id="1">the total number af possible words</definiens>
			</definition>
			<definition id="2">
				<sentence>Here we estimate the selectivity by the ratio of successfld cases to all of possible cases , as follaws ( n is the mlml } er of total elements , k is the number of elements related to each scene , aim m is the total number of scenes ; incomplete information is dellned as a partial vector of elements number s ( 0 &lt; s &lt; k ) ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">k</definiendum>
				<definiens id="0">the mlml } er of total elements</definiens>
				<definiens id="1">the total number of scenes ; incomplete information is dellned as a partial vector of elements number s ( 0 &lt; s &lt; k ) )</definiens>
			</definition>
			<definition id="3">
				<sentence>Without input 8.59 bits 1 word input 0.80 bits 7.79 bits 2 words inl ) ut 0.32 bits 0.48 bits Mutual-lnformation entropy : Mutual-information entropy ( MIE ) can lye defined as the contribution of additional words to identify a scene , and consequently , tile selectiveness of the target word or scene .</sentence>
				<definiendum id="0">Mutual-lnformation entropy</definiendum>
				<definiendum id="1">Mutual-information entropy</definiendum>
				<definiendum id="2">MIE</definiendum>
				<definiens id="0">the contribution of additional words to identify a scene , and consequently , tile selectiveness of the target word or scene</definiens>
			</definition>
			<definition id="4">
				<sentence>Mutualinformation entrot &gt; y per word is calculated by following formula : MIE ( O ; O ' ) = CU ( C l O ) -CE ( CIO ' ) Here , 0 is a set of previous state parameters , and 0 ~ is that of next one .</sentence>
				<definiendum id="0">Mutualinformation entrot &gt; y</definiendum>
				<definiens id="0">per word is calculated by following formula : MIE ( O ; O ' ) = CU</definiens>
			</definition>
			<definition id="5">
				<sentence>Another suggestive analysis is using Shannon 's information or entropy , which gives us more accurate .</sentence>
				<definiendum id="0">entropy</definiendum>
				<definiens id="0">gives us more accurate</definiens>
			</definition>
</paper>

		<paper id="2202">
			<definition id="0">
				<sentence>, , and that accordingly in tile Dutch iexical entry Ior sleutel we instant|ate sleutelbos as the vahle of tile nlerged 1,1 ; //Mult , then we can use the paraphraslug rule to effect a nlaplfing between tile two 13 ; 's and hence arrive at an iuterlingual approach to tile trauslation of tile example , despite structural |nisu\ ] atches , i.e. , key + bunch\ [ Mult ( key ) \ ] sleutel bos\ [ llMuit ( sleutel ) l l ; u~lher examples exist where productive nlorphological processes ( e.g. , affixation 'q ) lead to tile lexicali sat|tin in one language of concepts that exist as syntagnla|ic constructs ill another .</sentence>
				<definiendum id="0">sleutel</definiendum>
				<definiens id="0">a nlaplfing between tile two 13 ; 's and hence arrive at an iuterlingual approach to tile trauslation of tile example , despite structural |nisu\ ] atches , i.e. , key + bunch\ [ Mult ( key ) \ ]</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>~2 ) /\ [ ORPHOLOGICAL ANALYSIS A morpheme is the smallest refit of a string of characters which has a certain linguistic l/leaning itself .</sentence>
				<definiendum id="0">morpheme</definiendum>
			</definition>
			<definition id="1">
				<sentence>It includes both content words and flmction words , in this l ) aper the definition of a morl ) heme is a string of characters which is looked u I ) in tile dictionary .</sentence>
				<definiendum id="0">heme</definiendum>
				<definiens id="0">includes both content words and flmction words , in this l ) aper the definition of a morl )</definiens>
			</definition>
			<definition id="2">
				<sentence>A mutual information ( MI ) \ [ 1\ ] \ [ 2\ ] \ [ 3\ ] is tile information of the ~ussociation of several things .</sentence>
				<definiendum id="0">mutual information ( MI</definiendum>
				<definiens id="0">tile information of the ~ussociation of several things</definiens>
			</definition>
			<definition id="3">
				<sentence>A bigram is a possibility of having two certain words together in a corpus , as you see in the expression ( l ) .</sentence>
				<definiendum id="0">bigram</definiendum>
				<definiens id="0">a possibility of having two certain words together in a corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>A bigram is the information of the association between two certain words and a trigram is the information among three .</sentence>
				<definiendum id="0">bigram</definiendum>
				<definiendum id="1">trigram</definiendum>
				<definiens id="0">the information of the association between two certain words and a</definiens>
				<definiens id="1">the information among three</definiens>
			</definition>
			<definition id="5">
				<sentence>A d-bigram is the possibility that two words wt and w2 come out together at a distance of d words in a corpus .</sentence>
				<definiendum id="0">d-bigram</definiendum>
				<definiens id="0">the possibility that two words wt and w2 come out together at a distance of d words in a corpus</definiens>
			</definition>
			<definition id="6">
				<sentence>M , qS takes a lliragana sentence as its input .</sentence>
				<definiendum id="0">qS</definiendum>
				<definiens id="0">takes a lliragana sentence as its input</definiens>
			</definition>
			<definition id="7">
				<sentence>That is , MSS is available in many lmrposes with very simple , easy preparation .</sentence>
				<definiendum id="0">MSS</definiendum>
				<definiens id="0">available in many lmrposes with very simple , easy preparation</definiens>
			</definition>
</paper>

		<paper id="2115">
			<definition id="0">
				<sentence>That is , the Input Integrator sends some messages to Drawing Tool to carry out the sequences of operations .</sentence>
				<definiendum id="0">Input Integrator</definiendum>
				<definiens id="0">sends some messages to Drawing Tool to carry out the sequences of operations</definiens>
			</definition>
			<definition id="1">
				<sentence>ther tile event is an independent input or whether it follows the related voice input .</sentence>
				<definiendum id="0">tile event</definiendum>
				<definiens id="0">an independent input or whether it follows the related voice input</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>s o\ [ ' the &lt; lel ) endent supertags for each supertag is ma.intained. A supertag is dependent on another supert~g i\ [ ' the former sul ) stitutes or adjoins into tit ( . ' latter `` ~. Table 3 shows the data required for the dependency model of supertag disambigua.tion. Ideally each entry would be in ( lexed by a ( word , sui ) ertag ) pair I ) ut , due to si ) arseness o\ [ ' ( lata , we have backed-off to a. ( I ) ( ) S , supertag ) pa.ir , l'3a ( : h entry contains the following information. • POS and Supertag p~dr. IJst ol ' + aml - , representing the ( lirectioll of the ( h , peIM ( mt superta , gs with resl ) e ( : t to the indexed supe.rtag. ( Size of this list iiMicates the total number of dependeltt SUl ) e , 'ta.gs required. ) • l ) ependent supertag. Signed numl ) er representhig the direction a.nd the ordinal position of the l ) a.rticul ; u ' dependent SUl ) e.rtag mentioned in the entry from the position ( ff the indexed su\ [ ) ertag. aWe are computing dependencies between words with respect to supertags associated with the words , although the complete structure of the supcrtags is not used. It is of interest to COml ) ~U : e our work with some other dependencybased appro~ches as described by , for example , Sle~tor ( Sleator and Teml ) erley , 1990 ) , l\ [ indle ( llindle , \ ] 993 ) , Milward ( M ilward , 1 ! ) ! ) 2 ) . 158 • A probal ) ility of occnrrence of such : t ( lependency. The sum probability over all the de pendent supertags at all ordinal positions in the same direction is one. For example , the fourth entry in the T : d ) le ; I reads that the tree ( ~2 , a.nehored 1 ) y a verl ) ( V ) , has a left and a right dependent ( - , + ) and the tirst word to the left ( -1 ) , with the 1 ; ree. ( ~s , is dependent on the current word. The strength of this association is rel ) resented by the i ) robal ) ility The dependency model of ( lisaunl ) iguation works as follows. Stil ) l ) ose ( ~'2 iS a , llleiillie.r of tile ' set of super ( ass associa.te ( l with : t word a.t posi ties n in the senten ( : e. The : d &lt; e ; orithul proceeds to slttisfy the depende.ncy req ' &lt; lh'e.ment of &lt; t,2 I ) y pieldng up the dependency entries for e : t &lt; : h ( &gt; \ [ the directions .</sentence>
				<definiendum id="0">ree.</definiendum>
				<definiens id="0">'ta.gs required. ) • l ) ependent supertag. Signed numl ) er representhig the direction a.nd the ordinal position of the l ) a.rticul ; u ' dependent SUl ) e.rtag mentioned in the entry from the position ( ff the indexed su\ [</definiens>
				<definiens id="1">y a verl ) ( V ) , has a left and a right dependent ( - , +</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>frequency and V is a uniform probability that each tag will occur .</sentence>
				<definiendum id="0">frequency</definiendum>
				<definiendum id="1">V</definiendum>
				<definiens id="0">a uniform probability that each tag will occur</definiens>
			</definition>
			<definition id="1">
				<sentence>pos is tile part of speech tag , which is a list of part of speech , conjugation type , and conjugation form in our system for Japanese .</sentence>
				<definiendum id="0">pos</definiendum>
				<definiens id="0">tile part of speech tag , which is a list of part of speech , conjugation type , and conjugation form in our system for Japanese</definiens>
			</definition>
			<definition id="2">
				<sentence>Parse.nth-order-~tate is a list of the last two parts of speech tags including that of the current word .</sentence>
				<definiendum id="0">Parse.nth-order-~tate</definiendum>
				<definiens id="0">a list of the last two parts of speech tags including that of the current word</definiens>
			</definition>
			<definition id="3">
				<sentence>Parse.prob-so-far is the score of the best partial path from the beginning of the sentence to the word .</sentence>
				<definiendum id="0">Parse.prob-so-far</definiendum>
				<definiens id="0">the score of the best partial path from the beginning of the sentence to the word</definiens>
			</definition>
			<definition id="4">
				<sentence>The backward search uses a table called path-map , whose key is the end position of tile parse structure , and whose value is a list of parse structures that have the best partial path scores for each distinct combin~ ties of the start position and the combined state .</sentence>
				<definiendum id="0">backward search</definiendum>
				<definiens id="0">uses a table called path-map , whose key is the end position of tile parse structure</definiens>
			</definition>
			<definition id="5">
				<sentence>, c , ~ denote the sequence of n characters that constitute word zv whose part of speech is t. We approximate the probability of the word given part of speech P ( wlt ) by tile trigram probabilities , p ( , , , Iz ) = P , ( C ) -f ' , ( ~ , l # , # ) ~ ' , ( ~1 # , &lt; ) u IX P , ( ~ , lc+-= , ~ , -~ ) r , ( # 1c .</sentence>
				<definiendum id="0">Iz</definiendum>
				<definiens id="0">the sequence of n characters that constitute word zv whose part of speech is t. We approximate the probability of the word given part of speech P ( wlt ) by tile trigram probabilities , p ( , , ,</definiens>
			</definition>
			<definition id="6">
				<sentence>~ , ~i ) ( lO ) where Nt ( ci_2 , ci_~ , ci ) is tile total number of times character trigram ci_2ci_~el appears in words tagged as t in the training corpus .</sentence>
				<definiendum id="0">Nt</definiendum>
				<definiens id="0">tile total number of times character trigram ci_2ci_~el appears in words tagged as t in the training corpus</definiens>
			</definition>
			<definition id="7">
				<sentence>A word hypothesis is a list of word boundary , part of speech assignment , and word probability that matches tile leftmost substrings starting at a given position in tile input sentence .</sentence>
				<definiendum id="0">word hypothesis</definiendum>
				<definiens id="0">a list of word boundary , part of speech assignment , and word probability that matches tile leftmost substrings starting at a given position in tile input sentence</definiens>
			</definition>
</paper>

		<paper id="2189">
			<definition id="0">
				<sentence>Pronoml resolution is a typical proceeding that utilizes this information .</sentence>
				<definiendum id="0">Pronoml resolution</definiendum>
				<definiens id="0">a typical proceeding that utilizes this information</definiens>
			</definition>
</paper>

		<paper id="2200">
			<definition id="0">
				<sentence>f ) UOGONGN\ [ ~NG is the shoitt : ned ltanm : itn tht : oHgimtl Chmt : sc dictionaD .</sentence>
				<definiendum id="0">f ) UOGONGN\ [ ~NG</definiendum>
				<definiens id="0">the shoitt : ned ltanm : itn tht : oHgimtl Chmt : sc dictionaD</definiens>
			</definition>
</paper>

		<paper id="2186">
			<definition id="0">
				<sentence>Sentence ( s2 ) describes an eventnality of deleting , Ed , and sentence ( s4 ) describes an eventuality of forwarding , F/ .</sentence>
				<definiendum id="0">Sentence</definiendum>
				<definiens id="0">an eventnality of deleting , Ed , and sentence ( s4 ) describes an eventuality of forwarding , F/</definiens>
			</definition>
			<definition id="1">
				<sentence>Sentence ( s4 ) involves nominal reference to M : tile object of `` tensousnrn ( forward ) '' is zero-pronominalized and refers to M. Moreover , sentence ( s4 ) involves the interaction between nominal and temporal reference .</sentence>
				<definiendum id="0">Sentence</definiendum>
				<definiens id="0">nominal reference to M : tile object of `` tensousnrn ( forward ) '' is zero-pronominalized and refers to M. Moreover , sentence ( s4 ) involves the interaction between nominal and temporal reference</definiens>
			</definition>
			<definition id="2">
				<sentence>, ... } , Tlg &gt; , where 1 , ; 1~ is a represent , ttion for the eventuality , { \ [ R , ... } is a set of representations of the existen- .</sentence>
				<definiendum id="0">}</definiendum>
				<definiens id="0">a represent , ttion for the eventuality , { \ [ R , ...</definiens>
				<definiens id="1">a set of representations of the existen-</definiens>
			</definition>
			<definition id="3">
				<sentence>tial status of the individuals , and Tt~ is the teulporal relationship between them .</sentence>
				<definiendum id="0">Tt~</definiendum>
			</definition>
			<definition id="4">
				<sentence>e\ ] , Forward ) &amp; , Agcnt ( *c : , *agcnt ) &amp; ltccp ( *cj , *recp ) &amp; Objcct ( *c\ ] , *obj ) &amp; 7'ime ( *c : , *t : ) , iiQa &lt; le=r 7'ypc ( *Cobj , t ; 'xi~t ) &amp; , Object ( *eo~j , *obj ) , ~.'. Loe ( *eobj , *lobj ) &amp; Owner ( *lobj , .agent ) &amp; Time ( *covj , *toni ) , 1 l~\ ] b d~=r ! l'ypc ( *c~.ovy , Exist ) &amp; Objeet ( *e¢o~ ) v , *copy ) &amp; CopyO f ( *copy , *obj ) &amp; Loc ( *C~o~ , y , *l~ovy ) &amp; Owncr ( *l~ol , y , * ' , 'eep ) &amp; `` l~imc ( *e~op , j , *t~ot , ,j ) , T I~ , \ ] de=f ! I'll.el ( During , *t j , *tobj ) ~ 7 ' l ? .cl ( Bc f orc , *t j , *teovy ) . Loe ( *x , *1 ) means that individual *x exists at lo.. cation *l. in this dialogue domain , the location where an e-mail message exists is a mail box. Owner ( , l , .p ) means that person *p owns location */. I assulne here that the owner of a mail box is uniquely identified. /it the ahove , \ [ l~\ ] a specifies the existenti~fi status of *obj , whM1 is the object being forwarded , and says that *obj exists at time *to~j and at location *lobd , which the agent of forwarding owns. lleZb specifies the existential status of *copy , which is a copy of *obj aud is generated by forwarding *obj. The object thai ; the recipient of forwarding receives is , lot identified with *obj becmlse of domestic constraints concerning an e-mail system , lJgIi , says that *copy exists at time *tcopu and at lo ( 'ation *le~py , which the recipient of forwarding owns. 3'l¢.j says that the time : of forwarding , *t j , must be during the time , *to , j , when .obj exists , and that the time of forwarding , *t : nmst be before the time , *tcopy , when *copy exists. l ( nowledge about an eventuality of deletiug is written as follows. ( r3 ) &lt; l ' ; l { d , { \ [ ll.d } , 'Pl { d &gt; , where L'tQ d : -f 7'ypc ( *ed , Delete ) &amp; A : lcnt ( *ed , *agent ) &amp; Object ( *cd , *obj ) &amp; 7'imc ( *Cd , *td ) , def Itgd = Typc ( *Cobj , \ ] '\ ] xi .</sentence>
				<definiendum id="0">whM1</definiendum>
				<definiendum id="1">L'tQ d</definiendum>
				<definiens id="0">the object being forwarded</definiens>
			</definition>
			<definition id="5">
				<sentence>in ( rg ) , M represents an individual , which is an e-mail message , Td represents the time of Ed , and ?</sentence>
				<definiendum id="0">M</definiendum>
				<definiendum id="1">Td</definiendum>
				<definiens id="0">an e-mail message</definiens>
			</definition>
			<definition id="6">
				<sentence>\ [ '~p represents the time when sentence ( s2 ) is uttered .</sentence>
				<definiendum id="0">\ [ '~p</definiendum>
				<definiens id="0">represents the time when sentence ( s2 ) is uttered</definiens>
			</definition>
			<definition id="7">
				<sentence>Constant Copy represents the copy of the e-mail message M , generated by forwarding M. l , 'inally , the following representations are introduced into the context .</sentence>
				<definiendum id="0">Constant Copy</definiendum>
				<definiens id="0">the copy of the e-mail message M</definiens>
			</definition>
</paper>

		<paper id="1083">
			<definition id="0">
				<sentence>This allows us to 508 CII ~Cll nl-n sl-sLxLi n In n , n \ s ts 17 L\ ] n , ( n \ s ) / n , n is \ [ /R\ ] n , ( n \ s ) / n ts / n s I `` s n , ( n \s ) l n , ( sl n ) \ sk s \ [ \ L\ ] \ [ \ \ ] ~:1 ( n \ s ) ln , ( sln ) \ s V n \ s CH ~ ell s / ( n \ s ) , ( n \ s ) / n , ( s / n ) \ s F s ( s / ( n \ s ) ) / on , or , ( n \ s ) / n , ( s / n ) \ s F s S t `` S IlL\ ] \ [ I L\ [ IlL\ ] ( s / ( n \ s ) ) I on , cn , ( n \ s ) I n , ( ( s I n ) \ s ) I cn , cn Fs fig .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">n \ s ) / n ts / n s I `` s n , ( n \s ) l n , ( sl n ) \ sk s \ [</definiens>
				<definiens id="1">V n \ s CH ~ ell s / ( n \ s ) , ( n \ s ) / n , ( s / n ) \ s F s ( s / ( n \ s ) ) / on , or , ( n \ s ) / n , ( s / n ) \ s F s S t `` S IlL\ ] \ [ I L\ [ IlL\ ] ( s / ( n \ s ) ) I on , cn , ( n \ s ) I n , ( ( s I n ) \ s ) I cn , cn Fs fig</definiens>
			</definition>
			<definition id="1">
				<sentence>A sequent is an expression 7 '' lc , where T is a not &gt; empty finite sequence of categories ( the antecedent ) and c ( the succedent ) is a category .</sentence>
				<definiendum id="0">sequent</definiendum>
				<definiendum id="1">T</definiendum>
				<definiendum id="2">c</definiendum>
				<definiens id="0">a not &gt; empty finite sequence of categories</definiens>
			</definition>
			<definition id="2">
				<sentence>i ( R ( 171l '~ ( `` f2 ) \ ] '' ) i ~ ) is v 2 for ) d ( ( v/ ) '' = ( v2 ) ) , if 1 ' is it term of type et , R is a term of type e ( et ) and the z 's are either discourse referents or terms of type e. This gives us our basic conditions of the DRT language as terms of type st. In order to have complex conditions and boxes as well , we shall write not • for , a.i-,3jO ( O0 ) , • or 'I t for M3j ( O ( i ) ( j ) v ff* ( O ( J ) ) , q ' ~ lit for ) dVj ( O ( i ) ( \ ] ) -- + 3k~P ( j ) ( k ) ) , \ [ ul ... u , , Ib ... .. y , , , \ [ for ZiZj ( itu , ... .. u , ,lj A yIQ/ ) A..</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">a term of type e</definiens>
				<definiens id="1">j ) ( k ) ) , \ [ ul ... u , , Ib ... .. y , , , \ [ for ZiZj ( itu , ... .. u</definiens>
			</definition>
			<definition id="3">
				<sentence>le ( v3 ( v ) ) ) ( 3 ) ZP ( \ [ Ul\ [ \ ] ; \ [ Iman it1\ ] ; D ' ( P ' ) ( Zv ' .</sentence>
				<definiendum id="0">v3</definiendum>
				<definiens id="0">( v ) ) ) ( 3 ) ZP ( \ [ Ul\ [ \ ] ; \ [ Iman it1\ ] ; D ' ( P ' ) ( Zv '</definiens>
			</definition>
</paper>

		<paper id="1070">
			<definition id="0">
				<sentence>KEYWORDS : Distributed NLP systems , Software architectures , Whiteboard .</sentence>
				<definiendum id="0">Software architectures</definiendum>
				<definiens id="0">Distributed NLP systems</definiens>
			</definition>
			<definition id="1">
				<sentence>A ncxle N spanning It132\ ] is implicitly connected to another node N ' spanning \ [ t'l , t'2\ ] iff its time span begins earlier 01 gt'l ) , ends strictly earlier ( t2 &lt; t'2 ) , and the respective sp , 'ms ( a ) are not too far apart anti ( b ) do n't overlap tc~ ) much ( t2-max-gap_ &lt; t'l ~t2+max-ovorlap ) . maxgap and max-overlap are gapping and overlapping threshokts \ [ 12\ ] . Because t2 &lt; t'2 , there can be no cycles. ht a lattice , by contrast , nodes and arcs are explicit. Cycles are also forhiddcn , and there must be a unique first node and a unique last node. ( ; rids have often been used in NLP. l '' or example , Ihe output of the phonetic component of Kt~AL \ [ 121 was a word grid , and certain speech recognition programs at NI'R l~r ( ×luce phoneme grids 1. In gener~d , each uc~le bears a time span , a label , and a score. Grids can also be used to represent an input text obtained by scanning a bad original , or a stenotypy tape \ [ 9\ ] , and to implement some working structures ( like flint of the Cocke algorithm ) . llowever , we will require explicit arcs in order to explicitly model possible sequences , sometimes with associated information concerning sequence probability. Thus mw grkls am insufficient for our whiteboards. Two kinds of quasi-lattices have been used extensively , in two wtrietics. First , chart structures have origi , mlly been intr ( ~luccd by M. Kay in the MIND system around 1965 \ [ 8\ ] , In a ch : ut , as understood tt~lay ( Kay 's charts were more general ) , the nodes , are arranged in a row , so that there is always a path between any two given nodes. The arcs bear the information ( label , score ) , not the nodes. Ch\ [ u'ls are also used by many unification-based natural hmguage analyzers \ [ 141. Chart structures are unsuitable for represcnting restflts on a whiteboard , however , because they are tmable to represent alternate sequences. Consider the alternate word sequences of Figure 4. It is not possible to arr.'mge the words in a single mw so that all and only the proper sequences can be read out , 1 1 it if you came I would like you to come emly tomorrow earlier Figure 4 : A sentence with alternate formtdations A second type of quasi-lattice is the Q-graphs of \ [ 15\ ] and their exteasiou \ [ 17\ ] , the basic data structure for text representation in tile METI~ , O \ [ 14\ ] aud TAUM-Aviation \ [ 71 systems. A Q-graph is a loop-free graph wilh a tmique entry node and a uni ( lue exit node. As iu charts , the inlonnalion is carried on the arcs. It cousisls in labeled or atmotaled trees. As there may be no l ) ath between two nixies , Q-graphs can indeed faithfully represent alternate sequences like those of Figure 4. But in this case it is necess ; uy to use , on more thau one arc , identical labels referring to the same span of the input. For representation on a whitcl×mrd , such duplication is a drawback. To simplify bookkeeping and visual presentation , we prefer a representation in which a given label referring to a given span appeaJw in only one place. A true lattice , like flint of Figure 5 , makes this possible. `` lhe decomposition of the laltice in htyers seems natural , aud leads to more clarity. Fach layer contains results of 1115 , 16\ ] . By contrast , tile IIWIM \ [ 20\ ] system used a `` phonetic lattice '' on which an extended ATN operated. 427 The `` Whiteboard '' Architecture : a way to integrate ... Boitet &amp; Seligmcm , COLING-94 one component , selected to the `` appropriate level of detail '' . Its time-aligned character makes it possible to organize it in such a way that everything which has been computed on a certain time interval at a certain layer may be found in the same region. Each layer has three dimensions , time , depth and label ( or `` class '' ) . A node at position ( i , j , k ) corresponds to the input segment of length j ending at time i and is of label k. All realizations of label k corresponding to this segment are to be packed in this node , and all nodes corresponding to approximately equ , 'd input segments am thus geometrically clustered. In other words , ambiguities are packed so that dynamic programming techniques may be applied on direct images of the whiteboard. Figure 6 gives an ex , 'unple , Where the main NP has been obtained in two ways. G Q III II % Figure 5 : A word lattice ( representing a sentence with alternate fornudations Arcs may optionally be augmented with activation or realistic choice of layers , however. inhibition weights , so that ideas from the fast-developing lield of neural networks may be applied. language u~q~ layers layers Figure 6 : The whiteboard as a factorizing data structure The true lattice , then , is our preferred structure for the whiteboard. We said that the whiteboard could be a central place for transp , 'u'ent inspection , at suitable levels of detail. We use the notion of `` shaded nodes '' for this. `` White '' nodes are the real nodes of the lattice. They contain results of the computation of the component associated with their layer : a white node contains at least a label , legal in its layer , such as NP , AP , CARDP , VP ... in the example above , and possibly more complex information , as allowed by the declaration of the layer in the whitelx~ard. `` Grey '' nodes may be added to show how the white nodes have been constructed. They do n't belong to the lattice structure proper. In the example above , they stand for rule instances , with the possibility of m -- &gt; n rules .</sentence>
				<definiendum id="0">Q-graph</definiendum>
				<definiens id="0">a loop-free graph wilh a tmique entry node and a uni</definiens>
				<definiens id="1">a way to integrate ... Boitet &amp; Seligmcm , COLING-94 one component , selected to the `` appropriate level of detail '' . Its time-aligned character makes it possible to organize it in such a way that everything which has been computed on a certain time interval at a certain layer may be found in the same region. Each layer has three dimensions , time , depth and label ( or `` class ''</definiens>
				<definiens id="2">A word lattice ( representing a sentence with alternate fornudations Arcs may optionally be augmented with activation or realistic choice of layers , however. inhibition weights</definiens>
			</definition>
			<definition id="2">
				<sentence>A connection consists of a pair of in and out mailboxes , with associated locks , mid is opened with certain paraneters , such as its sleep time and codes indicating pre-agreed import and export formats .</sentence>
				<definiendum id="0">connection</definiendum>
				<definiens id="0">consists of a pair of in and out mailboxes , with associated locks , mid is opened with certain paraneters , such as its sleep time and codes indicating pre-agreed import and export formats</definiens>
			</definition>
			<definition id="3">
				<sentence>Connection request includes codes showing in which format ( s ) the client is expecting to deposit data in the iu box and read data from tile out box , lbr that connection .</sentence>
				<definiendum id="0">Connection request</definiendum>
				<definiens id="0">includes codes showing in which format ( s ) the client is expecting to deposit data in the iu box and read data from tile out box</definiens>
			</definition>
			<definition id="4">
				<sentence>, EC provides , lor each of 40 prerecorded bunsetsu ( elementary phrase ) , a set of about 25 phoneme malrices , one for each phoneme .</sentence>
				<definiendum id="0">EC</definiendum>
				<definiens id="0">provides , lor each of 40 prerecorded bunsetsu ( elementary phrase ) , a set of about 25 phoneme malrices</definiens>
			</definition>
</paper>

		<paper id="1068">
			<definition id="0">
				<sentence>NLyacc , which is a free and sharable software , runs on UNIX workstations and personal computers .</sentence>
				<definiendum id="0">NLyacc</definiendum>
				<definiens id="0">a free and sharable software , runs on UNIX workstations and personal computers</definiens>
			</definition>
			<definition id="1">
				<sentence>NLyacc consists of three modules ; a reader , a parsing table constructor , and a drive routine for the gene .</sentence>
				<definiendum id="0">NLyacc</definiendum>
				<definiens id="0">consists of three modules ; a reader , a parsing table constructor , and a drive routine for the gene</definiens>
			</definition>
</paper>

		<paper id="2105">
			<definition id="0">
				<sentence>NSHIP : &lt; ENTITYRELATIONSHIP 0403-8 &gt; LOCATION : Untied States ( C~UNTItY ) ~ELA'I IONSHIP0403-I 1 &gt; TYPE : COMPANY ~ \ [ ENTITY RELATIONSHIP : &lt; EN~ITY P , ELATIONSHIP 0403-10 &gt; I &lt; ENTITY RELATIONSHIP-O~ ) 3~-I 1 &gt; \ ] &lt; ACTIVITY 0403-6 &gt; : = INDUSTRY : &lt; INDUSTRY-0403-8 &gt; &lt; INDUSTRY~0403-8 &gt; : = PRODUCT/SERVICE : ( `` gas turbines '' ) Figure 1 : The TII'STI , ' , \ ] ~ , ( MUC-5 ) tlabt extraction task TIPS'I~Et { is a program of the U.S. government Adwinced I { eseareh Projects Agency ( AI~ , PA ) .</sentence>
				<definiendum id="0">TYPE</definiendum>
			</definition>
			<definition id="1">
				<sentence>wil , h l ; ilcsc &lt; q'ucb 'd dill'o+rcnccs \ ] u word usa , ge i.u a geuct's.I way. This in &lt; tar lmrtilAouing tim knowledge o1 Lhc sysl ; e\ ] n lilt , ( ) f'O/il ' coml ) ouenLs : ( 1 ) gcm~ric , ( : g ) doma.in dul , eu &lt; hmL , ( 3 ) la.ll.gu : -tgedepctMot~L , aud ( d ) ( \ ] o\ [ i'l : aiil a.il ( l , latlgtlal , ; c depcnd ( mL. WiLh Lhc d ( q ; a.il ( ) 17 a+llarlySis t , ha l imri , s oF Lll , ~ task requir , c ; , such ax I ; hosc~ de+scribed al ' , , : ) ve , it , i ; ; esscnl ; iM not ; only l , c , minimize l ; hc ; -ttUO/llll ; of ktlov , q , :~ ( lgc that , is d &lt; 't ) c'n &lt; hmt , on elf , her language or domain , bul ; aim ( ) to minimizv the off'oft , of acquiring knowledge tha , t , is dependent , on ciLhcr domain or bmgt ) age , aud , eSl ) , :'. &lt; ; ialiy , knowlodgc ( ha.l , is d , ~ ! lmndc\ ] ll , on bol ; h. The sccl , ions thai , follow will covc.r t , hcse astmcl , s c~f Otll ; so667 lntion to the TII'STEI { , problem. The previous section flamed some of the problems of data extraction in TIPSTEI { . with an emphasis on the aspects of the task that require substantial amounts of knowledge. We also presented our approach to the task by explaining tire synergistic objectives of creating generic resources and developing knowledge acquisition methods. This section will focus on the generic resources , while the next section will concentrate on acquisition methods. The main generic resource of SIIO ( \ ] UN is its core ontology of about 1,000 concepts , which was developed to support GE 's NLToolset lexicon \ [ , lacobs and Rau , 1993 ; Mcl { oy , 1992\ ] and had been tested fairly thoroughly on a variety of data extraction tusks prior to 'HPSTEI { . We augmented the core ontology using the CMU ontology from machine l ; ranslation \ [ KBM , 1989\ ] and used the extended ontology as the basis tbr Japanese lexicon development. The idea of this effort was that the Japanese lexicon would mirror the existing English lexicon , allowing fbr sharing of tire domain independent components of the knowledge base across langnages as well as the sharing of any ( lomain-specific knowledge that would be added. For example , the following is the English entry for the verb esiablish and its related forms : ( establish : POS verb : G-DERI1/S ( ( -er noun tr actor ) ( -ment noun ... ) ) : SENSES ( ( establishl : EXAMPLES ( she established superiority * ... ) : SYNTAX ( one-obj thatcomp whcomp prespart ) : TYPE *primary* : PAR ( c-causal-event ) : SYNONYMS ( set_up ) ) ( est ablish2 : EXAMPLES ( the court established fault ) : SYNTAX ( one-obj thatcomp whcomp prespart ) : TYPE *primary* : PAR ( c-deciding ) : SYNONYMS ( determine ) ) ) : X-DERIVS ( ( establish-ment-x : X-DEKIVS ( -meat noun ) : EXAMPLES ( the eating establishment ) : EXPRESS c-organizat ion ) ) ) The , lapanese lexicon now consists of about \ ] 3,000 words. This is somewhat more than the. 10,000 unique roots of the English lexicon , but tire / , ; nglish lexicon is still much richer in morphology and more thoroughly tested than the Japanese. Nevertheless , the two lexicons are roughly comparable and certainly eOmlmJ ; ible. For example , the Japanese entry for sclsurilsu ( ~-~. ) is the following : : POS usa : G-DENIES ( ) : SENSES ( ( setsnritsul : SYNTAX ( ) : TYPE *primary* : PAR ( c~causa\ ] .-event ) : SYNONYMS ( establish set_up ) : NOTE ( : nttd-kana ( ' '' 15\ [ '~ ) ~ _'U ) , , ) : jv-dom ) ) ) The main link between the English and Japanese lexicons is through the : PAR field ( for parent ) in each word sense , which joins that sense to its parent in the ontology. In this case , the common parent betweeJt establish and selsurilsu , c-causal-event ( the bringing about of events or effects ) , is a t'airly general category that includes two senses of ope~t as well as a variety of others like duplicalc iloll ( \ [ bridge. The reason that eslablish ends up in this general class is that it is very hard to confine any sense of the word to ereation events. I\ ] aving a shared ontology and lexicon format has certain adw~ , ntages. It is a requirement for using a common language processing framework across langaages , and it ensm : es that words with similar meat &gt; ings in different languages end up with similar representations and ontological restrictions .</sentence>
				<definiendum id="0">aiil a.il</definiendum>
				<definiendum id="1">c depcnd</definiendum>
				<definiendum id="2">SYNTAX</definiendum>
				<definiendum id="3">c-causal-event ) : SYNONYMS</definiendum>
				<definiendum id="4">EXAMPLES</definiendum>
				<definiendum id="5">SYNTAX</definiendum>
				<definiendum id="6">c-deciding ) : SYNONYMS</definiendum>
				<definiendum id="7">EXAMPLES</definiendum>
				<definiendum id="8">G-DENIES ( ) : SENSES</definiendum>
				<definiendum id="9">.-event ) : SYNONYMS</definiendum>
				<definiendum id="10">PAR field</definiendum>
				<definiens id="0">its core ontology of about 1,000 concepts , which was developed to support GE 's NLToolset lexicon \ [ , lacobs and Rau , 1993 ; Mcl { oy , 1992\ ] and had been tested fairly thoroughly on a variety of data extraction tusks prior to 'HPSTEI { . We augmented the core ontology using the CMU ontology from machine l</definiens>
				<definiens id="1">the bringing about of events or effects ) , is a t'airly general category that includes two senses of ope~t as well as a variety of others like duplicalc iloll</definiens>
			</definition>
			<definition id="2">
				<sentence>I ) ) vs. ( 20 ) Figure 4 : Some results of corpus analysis JJV are the F , nglish and Japanese joint venture tests , and EME and JME are the two microelectronics test sets .</sentence>
				<definiendum id="0">JME</definiendum>
				<definiens id="0">the two microelectronics test sets</definiens>
			</definition>
			<definition id="3">
				<sentence>Precision is the percentage of information produced by the system that is correct .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the percentage of information produced by the system that is correct</definiens>
			</definition>
			<definition id="4">
				<sentence>The F-measure is the geometric mean of recall and precision .</sentence>
				<definiendum id="0">F-measure</definiendum>
				<definiens id="0">the geometric mean of recall and precision</definiens>
			</definition>
			<definition id="5">
				<sentence>In addition , 670 Domain~Language English joint ventures Effort~Skiff Level 1 person-year , system developers , native English speakers Japanese joint ventures 1.5 person-years , mostly Japanese college students with non-native developers English micro-electronics 3 person-months , system developers , native speakers , no knowledge of ME Japanese micro-electronics 2 person-months , non-developers , non-native speakers ( with some help from natives , developers ) Other Notes Some effort not reflected in results Difficult to measure because of many experiments Least efficient , but most interesting effort Best overall results Lowest overafl results ( but explained by sample variation ) Last configuration done , least work , good results ( but not refined ) Figure 6 : I : ,\ [ \ [ 'ort required for eax ; h dOlilaili lind language l , here \ ] ms I ) c ( : n oliher signific , aut relidx~d work hi rolmst processing of I ; exts , notM ) ly \ [ llobbs cl al. &gt; 1992\ ] ; how ever , this rc , semmh has gener~dly emphasize ( l synt , ; u % ic coverage rather th ; m lcxical ( : overage .</sentence>
				<definiendum id="0">non-native speakers</definiendum>
				<definiens id="0">with some help from natives , developers</definiens>
			</definition>
</paper>

		<paper id="2112">
</paper>

		<paper id="2108">
			<definition id="0">
				<sentence>Optical character recognition ( OCR ) is a technique for converting scanned document images into character codes .</sentence>
				<definiendum id="0">Optical character recognition ( OCR</definiendum>
				<definiens id="0">a technique for converting scanned document images into character codes</definiens>
			</definition>
			<definition id="1">
				<sentence>Part-el-speech tagging is a technique that has been developed and refined over the past several years , and it provides an inexpensive , last , and reliable source of inlormation for recognizing noun phlases and other syntax-related text features which help characterize a doeunlen\ [ rs content .</sentence>
				<definiendum id="0">Part-el-speech tagging</definiendum>
				<definiens id="0">a technique that has been developed and refined over the past several years</definiens>
			</definition>
			<definition id="2">
				<sentence>Output consists of pairs of surface forms and tags .</sentence>
				<definiendum id="0">Output</definiendum>
			</definition>
			<definition id="3">
				<sentence>We are also investigating computationally inexpensive ways of making finer distinctions between characte , s that map to the character shape codes x and A. Initially , parentheses and brackets were always classified as A anti distorted any word shape they were adjacent to : for example , `` ( USA ) '' woukl be shape converted to A A A A A. Recently we have made progress m recognizing these nora alphabetic characters as wnrd shape token delimiters , rather than parts of the word shape tokens Ihemselves .</sentence>
				<definiendum id="0">USA</definiendum>
				<definiendum id="1">A A A A. Recently</definiendum>
				<definiens id="0">wnrd shape token delimiters</definiens>
			</definition>
</paper>

		<paper id="1089">
			<definition id="0">
				<sentence>IDIIS is a dictionary help system iuteuded to assist a human user in hulguage comprehension or production tasks .</sentence>
				<definiendum id="0">IDIIS</definiendum>
				<definiens id="0">a dictionary help system iuteuded to assist a human user in hulguage comprehension or production tasks</definiens>
			</definition>
			<definition id="1">
				<sentence>s ( specific relators ) , whose corresponding semantic structure is built in a specific way and which deserve also spccilic patterns in the hierarchk : s. KN ( IWLI , ; I ) ( ; E : TIlE I ) KB .</sentence>
				<definiendum id="0">s</definiendum>
				<definiendum id="1">I )</definiendum>
				<definiens id="0">specific relators ) , whose corresponding semantic structure is built in a specific way and which deserve also spccilic patterns in the hierarchk : s. KN ( IWLI , ;</definiens>
			</definition>
			<definition id="2">
				<sentence>As we have just seen , the knowledge reprcscntadou scheme chosen for the I ) KB of IDIIS is composcd of three clemenls , each of thetn structured as a dilTerent knowledge base : KI3-TIII~SAURUS is the reprcsentatio , l el the diclionary as a semautic network of frames , where each li'alne rcprcsenls a Olle-word collcepl ( word seuse ) or a phrasal concept .</sentence>
				<definiendum id="0">Olle-word collcepl</definiendum>
				<definiens id="0">a semautic network of frames</definiens>
			</definition>
			<definition id="3">
				<sentence>Phrasal concepts rcpresent phrase structures associated to the occurrence of concepts in meaning definilions .</sentence>
				<definiendum id="0">Phrasal</definiendum>
				<definiens id="0">concepts rcpresent phrase structures associated to the occurrence of concepts in meaning definilions</definiens>
			</definition>
			<definition id="4">
				<sentence>• PIIRASAL-CONCEPTS is a class that includes concepts similar to Quilliau 's `` tokens '' .</sentence>
				<definiendum id="0">PIIRASAL-CONCEPTS</definiendum>
				<definiens id="0">a class that includes concepts similar to Quilliau 's `` tokens ''</definiens>
			</definition>
			<definition id="5">
				<sentence>Phrasal concepts , 'u'e the representation of phrase stn~ctures which are composed by several concepts with semantic content .</sentence>
				<definiendum id="0">Phrasal concepts</definiendum>
				<definiens id="0">'u'e the representation of phrase stn~ctures which are composed by several concepts with semantic content</definiens>
			</definition>
			<definition id="6">
				<sentence>KB-TIIESAUI~ , US stores the concept network that is implemented as a network of frames .</sentence>
				<definiendum id="0">US</definiendum>
				<definiens id="0">stores the concept network that is implemented as a network of frames</definiens>
			</definition>
			<definition id="7">
				<sentence>The representatiou of the followiug dclinition gdranium I 1 : une plante d'ornement 546 rcqltires the el'cation of two new conccplual units in TIII ' ; SAURUS : Illo OIkO which coircspotlds to Ihc dcfiuicndum and Ihc phrasal COlkCCpt which rcpresculs Ihc nOkltk phrase of tim dclinition .</sentence>
				<definiendum id="0">Ihc phrasal COlkCCpt</definiendum>
				<definiens id="0">rcpresculs Ihc nOkltk phrase of tim dclinition</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>N is tim nunlber of words in the sequence , i.e. the corpus size .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">tim nunlber of words in the sequence</definiens>
			</definition>
			<definition id="1">
				<sentence>`` Phe term p ( w¢\ ] ll ) is the probability of a word w¢ in the context of the assigned tag tl .</sentence>
				<definiendum id="0">Phe term p</definiendum>
				<definiens id="0">the probability of a word w¢ in the context of the assigned tag tl</definiens>
			</definition>
</paper>

		<paper id="2123">
			<definition id="0">
				<sentence>'iFhis knowledge may be expressed at different levels of abstraction depending on the phenomena involved : selecLional re~ strictions ( SR~ ) , lexical preferences , eel-locations , etc .</sentence>
				<definiendum id="0">'iFhis knowledge</definiendum>
			</definition>
			<definition id="1">
				<sentence>Learning process The computational process is divided in three stages : ( 1 ) Guessing the possible semantic classes , i.e. creation of the space of candidates .</sentence>
				<definiendum id="0">Guessing</definiendum>
				<definiens id="0">the possible semantic classes</definiens>
			</definition>
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>The output is an ordered sc'quenee of proof communicative act iuteu~ions ( PCAs ) .</sentence>
				<definiendum id="0">PCAs</definiendum>
				<definiens id="0">an ordered sc'quenee of proof communicative act iuteu~ions</definiens>
			</definition>
			<definition id="1">
				<sentence>Rather than recording |he semantic ohjeets and their properties , our discourse model consists basically of the part of the input proof tree which has already been conveyed .</sentence>
				<definiendum id="0">discourse model</definiendum>
			</definition>
</paper>

		<paper id="2183">
			<definition id="0">
				<sentence>s1-17 : the heuristic rule SUl~porliug elaboration relaI , ion after cxelnplifical , ion-i ) resent relal , iou , s1-18 : the clm~ expressiou `` ( SONO ) I ( H ( IEA ( -WA ) , ( the reslt\ ] l , is ) '' which corresl ) ou &lt; ls to `` lead '' in seillanl , ics. sl-19 : theehain of '' synthet.icalq ) roach '' + The text from s4-1 to s4-7 in Appendix was also transformed to the structure , in Figure 1-b as follows. s4-2 : the cluc expressions : `` q~A '' ( a suffix indicating au interrogative senteuce ) in s.'t-I and `` ( the ) answer `` ill S '' I+2. s4-3 : thechaiu o\ [ ' `` douhle star '' . s4-4 : the chain \ [ 'rein `` shriuk '' in s4 -- : + to `` this process '' in s4- , l ( sonte expressions like % his process '' ate rega.rded as tttal , clting ally vet'\ ] ) ill a , previous SOIl\ [ ellCe ) . s4-5 : the chain of '' uuclcar fusion '' . s4-6 : the large , similarity wdue between s4-a and s4- ( i and the due exl ) ressiou `` similarly '' . s4-7 : this NS could uot be analyzed correctly. List relation wil , h s , 'l- ( ~ was ( h ' , tected incorrectly because of ' their sinlilarity value , lu s.I-6 and s.t-7 , while the same , vord `` Ileal , '' is us~ 'd in I ; ' , nglish , IIle prior `` heat , '' was translated inl.o `` ON I ) O ( tenllmral.lu'e ) - ( ' , A J ( ) \ [ ) 'Sll ( ) UStJl/tI ( rise ) '' in .lal ) ancse. lu oMcr to detect the chain l'or I , heir t.ol ) ic-doluiuant chaining relation , we must , in\ [ 'er that the risiug o\ [ 'telulmrat.ure i ) roduce a \ ] leat. Such a l ) t'ol ) leln is igrlored in t , his research. 1126 We have l ) rOl ) OSe ( I a. lne.Lhod of detecl.iug I ) S aulonlal , ~ ically usilig surface inforllialioli ill s ( ! nleiices : chl ( ! ex i ) re.ssions , word/i ) \ ] lrase chniils , and siulilarii , y I ) e£ : vecu s ( ~.ril &gt; eliC .</sentence>
				<definiendum id="0">J</definiendum>
				<definiens id="0">the heuristic rule SUl~porliug elaboration relaI , ion after cxelnplifical</definiens>
			</definition>
</paper>

		<paper id="2159">
			<definition id="0">
				<sentence>CATI consists of CAT2 al : d CAT3 .</sentence>
				<definiendum id="0">CATI</definiendum>
			</definition>
			<definition id="1">
				<sentence>In the rule set Pause we prepared about d5 l &gt; hrases dmt can end will : a pause : postpositionaI phrases , COllj : lllCt , ive phrases , adnominM verbal phrases marked with a special conjugation form , 989 phrases that end with a conjunctive postposition , adnominal phrases with the genitive postposition no , and coordinate verbal phrases .</sentence>
				<definiendum id="0">adnominal phrases</definiendum>
				<definiens id="0">a pause : postpositionaI phrases , COllj : lllCt , ive phrases</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>( 4 ) svare SYNILOC I E : .1 \ [ SUBCAT &lt; NP\ [ T \ ] , NP\ [ -- ~ SEMICONT I ANSWERER kANSW RED J IIl HPSG , syntactic categories are signs and are represented as attribute-value matrices , with three features , PHON ( OLOGY ) 2 , SYN ( TAX ) and SEM ( ANTICS ) , which ( with the exception of PHON ) can have feature structures as values. The path SYNILOCISUBCAT takes a list of categories as value. The leftmost element in this list corresponds to the lexical bead 's most oblique complement ( in ( 4 ) the object NP ) , the rightmost to the least oblique complement , ( the subject in ( 4 ) ) . The feature SEM in this simple example is specified for the semantic relation expressed by the verb , as well as the roles the verb selects. The categories in the subcategorisation list are signs , for which the labels in ( 4 ) are a shorthand notation. The indices in the subcategorisation list indicate that the complement signs ' SEM values are to bind the variables with which they are coindexed , and which are associated with the semantic roles in the relation. The lexical entry for the Spanish verb corresponding to svare , responder , is illustrated below : ( 5 ) -i~HON res ~onder DA , SYNILOC HEAD LLEX SUBCAT &lt; PP\ [ \ ] \ ] , NP~ &gt; \ [ 'RELN AN~\ ] WERII SEMICONT I ANSwEREP_ \ [ _ANSWERED The signs in ( 4 ) and ( 5 ) are rather similar , with the exception of the first elements in the subcategorisation lists ) ( 6 ) 1,1 sign IL sign The intuition is that whereas an L1 sign ( in the sense of Saussurean sign 4 ) has two sides , a concept side ( corresponds to the feature SEM in I IPSG ) and an expression side ( here called PIION ) , an IL sign has three sides : a common convenience , represent the valuc of PllON orthographically .</sentence>
				<definiendum id="0">PHON</definiendum>
				<definiendum id="1">feature SEM</definiendum>
				<definiens id="0">the semantic relation expressed by the verb</definiens>
			</definition>
			<definition id="1">
				<sentence>My idea is to exploit the relation between Lt and L1 subcat values in a chart parser , so if a parse fails because of a mismatch between a complement in the input string and the Lt complement needed by a lexical head , the chart can be modified such that a complement licensed by the L1 subcat specification is introduced into the chart as an alternative to the incompatible complement specification in the Lt subcat list .</sentence>
				<definiendum id="0">L1 subcat specification</definiendum>
				<definiens id="0">to exploit the relation between Lt and L1 subcat values in a chart parser</definiens>
			</definition>
</paper>

		<paper id="2128">
			<definition id="0">
				<sentence>iu and al ) l ) li ( : ation indcl ) end ( mt ( : oncel ) ts *Also on indefinite I &lt; tv ( ~ 1'1 '' 01\ [ 1 the \ ] 'vnma.n I ) rojc ( : t , 1Lq ( J/hffornta.tion Sciences Institute , Mariner dcl lie.y , I , os A ngcles. arranged in a subsumption latl ; icc , spraining dis ~iuct t , ypcs of processes ( menl ; M , communication , re lational , aclions ) , aud diverse quMitics and object ; s. Both onl , ologies ha.vc I~ ( '.cu used in ~t nmnber or ( Io. mait , s and show good re-usalfility characteristics mainly due to the l.tct that they : m : liuguistically motiw/.ted. Thus , for exauiple , if language generatioN is required there is tylfically I00 % r ( '. usabili { : y across doniahis it , co , fl. , 'ast to the 50 % clescrib ( ~d by \ [ I ) irlein , 1 ! ) ga\ ] fo , . l ; he , largely , ion-linguistically niotivated , I , II , OG onl , ology , l 'l'here are a mlnilyer of sugg , ; stions for the eval ual.iou o\ [ ' ontologies in t ( ; rlttS of \ [ 'orlnal prop ( ! r ties of consistel , ce and cohorenc , e of the inform ; ) tioN those outologics contain ( e.g. , \ [ lloracek , 1989 , ( : hmrino , 1994\ ] ) . With a restriction 1.o linguisticMly mol iw~l ; ed onl.ologies , we can now sg~i. , ; rurthcr ( 1 ( ; sigu principles collcerning whal ; is to 1oe represented and how. All ; hough Lhese l ) rinciplcs were origiually develolmd in oMer to carry out a detailed coull ) ar ison of I , he IqUM and th ( ; GUM , they are g ( ; nerally apl ) lical ) l ( ~ for all liuguistically molJvated ontologi ( .s ; ewdua.ting t ; Im couc ( ; pt.s l ) ropos ( ; d within su ( : h an on tology accordiHg to the principles ( les ( : l'il ) ( ! ( I in t.his paper should iml ) rovc { .lie sl.al ; us of thud , ontology overall. The main rosull , of the IqUM-C ; UM comp~rison is a zIhrged Upp ( r Mo ( hl presently used within I ; h ( : KOMI.'/I ' pro jecl ; as the basis rot multiliugual sentence gel ) eration in I'hlglish , ( ~ermau and I ) utch. ~ This l ; \ ] letl also i ) rovi ( les ~m early answer to a quesi.ion cot , ( : eruing a ( lilferenl ; kind of re-usability of liitgtiisl ; i. ( : ally motiva , ted ontologics , i.e. , l ; hc ( ! xtcnt to which they can be. re-used across distinct languag~.s , 'aLher tha.u across distinct donia.ins. Merging dislfiuct ontologies is a I ) rot ) lcm thaI , will ocCllr lllOFO fr ( ? ( lllelll ; ly as llCW plX ) poS\ [ -/.lS ~'ul'C ~ , o \ [ ) e \ [ ' ( '. ( 'oIloiled. \ [ tlovy aud Nirenl ) m'g , 1992\ ] i ) rol ) ose a gelll ! ral nlethod for creating a merged ontology out or dif fercnt ontologics where it. does not mattc'r whel , hcr I lun Is smq &gt; ly lmca .</sentence>
				<definiendum id="0">UM comp~rison</definiendum>
				<definiens id="0">a merged ontology out or dif fercnt ontologics where it. does not mattc'r whel , hcr I lun Is smq &gt; ly lmca</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , phrases ( 1 ) and ( 2 ) can only be generated from dill &gt; rent semantic input expressed here in the form of the typed semantic assertions of the Penman Sentence t ) lan Language ( sPt , ) \ [ Kasper , 1989\ ] : 4 ( 1 ) Das Mitdchen ist kra .</sentence>
				<definiendum id="0">lan Language</definiendum>
				<definiens id="0">the form of the typed semantic assertions of the Penman Sentence t )</definiens>
			</definition>
			<definition id="2">
				<sentence>l grounds without a change in experiential perspective , lilt the FUM the only semantie distinction in &lt; his area of con &gt; reunite &lt; ion processes is betwe.en 'telling ' like ewmts ( address &lt; c-orient &lt; d ) and 'saying'-like events ( nonaddressee-oriented ) , which is ~ dill &lt; fence in experiential lmrspective. 's In the I ' ; UM , concept discrimination is made with respeet to difIi'.ring possible realizalious of rob : s , not strMghtlbrward absence/presence of roles as in the GUM. 'Missing ' surface par &lt; Jell ) ants ean be modelled more adequately by an tipper mode.l-grammar interface which Mlows detined sem~mtic roles to ha.ve zero realization. '\ [ 'his is an elegant way to deal with optional participants , passive , and iml ) ersollal eonstructions. The net effect of both kinds of violations ( ; of principle I is tha , t the number of concepts is increased and itecessary decisions eone ( ; rnillg lexi ( 'ogralllmatical realizations arc avoided. 'lPhe. proliferation of concepts if Mlowed would complicate considerably the task of 'identifieation ' of similar coneepts in ontologies to be merged. Principle 2 : Intelligent cross classification linen \ [ bllowing extended i ( lentifieation of concel ) ts , it is uo~ suili ( : ient to provide cross products \ [ br those eoneepts that ~re not identifiable but which classify overlapping sema. , ttie areas. '\ [ 'his is clarified l ) y the following concrete , al { , hough \ [ ll/ICtl abbreviated , ex-. ample , of merging in tim al'e &amp; L of material ( action ) process types. TIll ; decisions that ar ( ; required here are typical of the merging process as ~t whole. The l ' ; -Malerial-Proccss hierarchy distiuguishes processes more or less with regard to transitivity pattcrning. Au l ' ; -Nondirecled-Aclion is a process without external eausation ( n , ostly intransitive , although transitive sentences where the objee.t is not alD ( ; te.d or created by t , he aetion Mso fall into this class ) , liAmbienl-Process and l ' ; -Motion-Process are not ex.. haustive subconeepts of L'-Nondirected-Aclion. '\ ] 'he wtse broke. Nondirected-Aetion i pla.y pi~um. Nondircctcd-Action The to , &lt; fist ran. Molion-t'roccss \ ] t rains here. Ambient-Process An E-Directed-Action in contrast is a proee.ss with an e×ternal causer as additionM tmrticipanL. / ' , '\ ] ) ireclcd-Actions divide into I ' : -Crealive-MahrialAction and I ' , '-l ) isposilive-MateriaI-Aclion. 5'\ [ 'he \ [ ( ) \ ] ? nlel '' neeess ; trily illvolws a.n addressee n ( ! lll~l , ll-. ticM\ [ y , sonleone who in intended to be listeuing , while the latter does l|ol ; . This diiR~rence is gramma.ticized tn \ [ , ; nglish in the a.cceptabillty/lmn-a.ceel ) tMfi\ ] ity of '\ [ told him thai ... . '/'1 s~tid hint tha.t ... '. Ill order I , o gratntmaticize a.n *Mdressee iu a. s~tying-like event , it , is necessary to respect its lens centrM role ~tnd 1.o use the form '1 s ; tid to hi*n fl~tt ... '. ~'l'he , :e ~tr ( . ' further similar cases ; for ex~tlnph~ tit ( ! GUM also inelmles interpersonally motive.ted eon ( : el ) t discrimin~Ltions such ; ts negativc @ : ctlurc.-ascriplion ~t , td ncgativc-quality. These govern the general , ion of m : ga.tiw~ assertions , Lhus pre erupting at lllOITc *q ) propriate speech function co &lt; tirol of ncg~ttion. The chihl broke , the vase. The lion chased the tourist. Mary baked a cake. 1 ) ispositivcM aterialA ction l ) ispositiveMaterialA ct ion Creative-Material-Action The GUM differentiates G-Agenl-eenlered , GAffected-centered , G-Agent-only and G-Affected-only as disjoint ( 7-Action subtypes. \ ] lere , we have at first a classifieation with regard to kind and number of partic.ipants. Example.s for the semantic representations of the intransitive process types are given in ( 4 ) and ( 5 ) , again ill Sel , notation : ( 4 ) I ) er'fimrint ramtte. ( The tonrint ran. ) ( r / action : lex rennen : agent ( t / tourist ) ) ( 5 ) I ) ie \ ] 'tlanze geht ein. ( The plant is dying. ) ( e / action : lex eingehan : affected ( p / pflanze ) ) The transitive processes ( with t ; wo participants ) ~rc I'urther broken up into G-Agent-centered and G'Affected-centered. The G-Affected-centered process type is a. very special case of a transitive process. 'l'he detinition is giwm ill \ [ Steiner el el. , 19881 thus : `` X affe.clcd-cenlered-vcrb Y iff X causes that Y a.lfc.cted~ &lt; chief ' &lt; d-verb '' . I ' ; xtunplcs are : I ) as Kind zerbricht ( lie Vase. l ) as Kind bewirkt , class die Vase zerbricht. The chihl breM ( s the vase. ~ * The ehihl brings it about that the vase bremen. Thus a process is called G-Alfected-cenlered if the reMizing verl ) is ahh'. to \ [ 'orln an ergal : ive pMr. All C-Ajfected-ccnlcred processes have at least two participants , the G-A : lenl and the G-Aft &lt; &lt; led. The ( ; -Agent-centered process is di ! l'crenti~ted with regaM to the different p~rtieipant types for &lt; , he second participant : Air &lt; cling l : ; ffecti ng Ranging l ) er Bauer fii.llt den I~aann. ( The. farmer is felling the tree. ) ( ; -Agent ( ; -AJl~c~ed Die Mutt , er malt o , in lbms , ( The mother is painting a house , ) G-Agent G-l ' : J.fccled leh spiel &lt; K l~vier. ( 1 play piano. ) ( ; -Agent ( \ [ Process-range At first , sight , there are few con.nonalities t ) etween the.se two onl , ologies. Without deeper hltrospecCion , one can only state an identity E-AmbientProcess = : GNaturalI'henomcnen mid could me &lt; haul &lt; ally build a cross classification as shown in Figure 1. Sonic ereated concepts should , howew : r , bc omitted from this 'eross produet ontology'. The \ [ hrsl , obvkms argumen~ is tile mmdmr of par tieipants. 'Phese are contradictory in the R ) llowing cross concepts : liI ) ircctedA &lt; lion/0 ? Agcnb. oMy and I'L Direcled-Aetion/ G-Affcctcd-only. A compari-son of the low level eoneepts shows further that the 805 I ! 'igure l : Mechanical merge of the material processes by ( : ross classiticahon following GUM and EUM concepts can in fact be identified : I ' ; Oispositive~ M aterialA c tion = G-Affcctiu 9 + G-Affcctcd-ccntercd l'LCreative-Matcrial-Action = G-Effecliug. This rules out the cross concepts : \ ] ' ; -l ) ispositive-Matcrial-Action/ g-Effecting , EI ) ispositivcM ateriaIA ction/ GRanging , EUreative. MatcriaI-Action/ G-A ffected-centered , ECreative.MaterialAction/G-Affecting , ECreative-Material-Action/G-Ranging. I , 'urthermore , it is known fron-i tile definition of ENondirecled-Aclion in \ [ l : ~ateman el al. , 1990\ ] that such processes axe either intransitive or they have a second pm : ticipant which is in meaning nothing else than t ; he G-Process-range participant , llence , the cross concepts : \ ] 'L N ondirected-Action/ G-A ffecting , liNondirectedA ( lion/ ( ~I ' ; ffecting , ENondirected-A ction/ GA ff ectcd-ccntcrcd as well ~s its subconco.pts E-Motion-Process~ ( 7-Affecting , 1 , ; -Motion-l~roccss/ G'-ldffccting ~re rnled out. Fin , nlly , tile exhaustive coverage of the low level subtypes in the I , ; UM and GITM supports the tbllowing identities : l ' % Nondirected-Action/ GNatural-phenomenon E-A mbientProcess/G-Natural-phenomenon ENondirected-Action/ GA.qent-ecntered = l ' % Nondirccted-Action/G-t~anging , EDirectedAction/G-AJ.'fected-centered -E-Dispo.~itive-MateriaI-Action/ G-Affected-ccntcred , E-Directed-Action/G-Agent-ccnlered = E-Dispositive-Material-Action/G-Affectin9 -IEC~vative-Material-Action/ G-Effectlng. By these kinds of deta.ilexl considerations , we have filtered an intelligent merge out ; of the mechanicM merge. Within the intelligent merge , we omit the German differences concerning tile participant nnmbet ( G-dge~tl-o~dy , GGganging ) since these violate principle I , and do not estM ) lish the very subtle ( L AJfccled-cente~vd type. Preferring the I~nglish terminology the result is giwm in l ! 'ignre 2. This turns ont i~o I ) 0 mainly tile EITM subhierarchy for material processes. 'lb also cover the German requirements , the Nondirecled-Aclion concept is difDrentiated into Nondirecled-I ) oing and Nondircclcd-Happenin 9 according to the distinctiou between Agent-only mM Affecled-only. q'herefbre we do not need to preserw ' , the Clerman participant types d.qe , tt and A Jr ( clod , and can inDr the releva , t inff ) rmation from tim new Nondireclcd-Action sub cottcepts. The ( .4erman SPL examples ( 4 ) and ( 5 ) then have the revised semantic form : ( 4 ' ) ( 53 ( r / nondirected-doing : \ ] .ex rennen : actor ( t / tourist ) ) ( e / nondirected-happening : lex eingehen : actor ( p / pflanze ) ) Because we have \ [ ixed the semantic differences between the G-Aqcnl and the G~AJ\ ] ? .clcd participant in the process types we do not need this differentialion as partMpant roles again , tlence , we choose the l ' ; nglish p~rticipant types h'-Aclor and I ' ; -dclee , tim correspondence of which to the German G-Agent , GAffected , G-Eft ( ( ted and G-Process-range ditDrs with the process type ( see l '' igure 2 ) . For further dermis of l , he merging of all 12 top-level regions of the two ontok ) gies , see \ [ Henschel , 1993\ ] . Principle , 3 : Flexible seman| ; ics-grmnmar interface One peculiarity of the proposed merging is that we do not assume a. strMghfforward correspondence be.tween concepts ( especially process types ) and sets of surface sentences , fh*tt means , disjoint concepts 806 ~tertal-I ) rocess / Action~ C Nondirech~l I ) irecte @ z &lt; _ ... . / ( Ta , ~ , , , , , , , ,0~ , , , , , -~ ~-No. , , , ,.. , ~ , _ -- -~ / ~me~ ' : '.__- &lt; ~ I ) oili~ J ~_j / ilct , 'o = clle { tM P /~ D , SlmSii\ ] ve -- -- . ~0t lm~l -- -- -- actee ~ : al fected I , 'igure 2 : Mergiug t ) rol ) osal for t , he materiM t ) roc ( : ss t , yl ) { ~ in the Merged O l ) ltcr Model do nol , m~ ( : ess~Mly correspond ( , ( ) disjoint sets of su\ ] 'fa ( : ( ~ SCIIt , ( HIC ( 2S o\ ] \ ] ly t , o disioinl. S ( HTI~-I , II| , i ( '. \ [ ) { n'Sl ) { ~cLives ( tit LIt ( HIt. Tim i ) &gt; ( , erfa , ce \ [ ) cl , ween \ [ , ho ii\ [ ) \ ] ) ( !</sentence>
				<definiendum id="0">-Nondirecled-Aclion</definiendum>
				<definiens id="0">an elegant way to deal with optional participants , passive , and iml ) ersollal eonstructions. The net effect of both kinds of violations</definiens>
			</definition>
			<definition id="3">
				<sentence>ross l ) l'o ( lltc { , t ; ypes ( as il , would be I , hc ( rose in ( , l\ ] ( : simple merging sLral , cgy ) , bul , by giving l ; he Ul ) lmr model -- gi : ~mmmr inI ; ( 21'f &amp; cc ItlOl'O. \ [ Iexil ) ilil , y r. As a ( ; O\ ] ISC ( \ ] U { 211C { 2\ ] ~l , hOIt ( ; CIlC ( B Sllch ~ts ( 6 ) c~ti/ iiow \ ] i : l , vc \ [ , WO { lisl , i~ct scman .</sentence>
				<definiendum id="0">ypes</definiendum>
				<definiendum id="1">hOIt</definiendum>
				<definiens id="0">simple merging sLral , cgy</definiens>
			</definition>
			<definition id="4">
				<sentence>nt~-tl ; k ) ns ( 7 ; t-\ ] ) ) ~u : ( : oMing | , ( ) tim Mcrg ( x\ [ \ [ Jl ) l ) er Model ; ( , h ( ~ COI/ { : ( 2I ) ( , dcsliltalioll ill ( ' ( \ ] ) ) iS it sul &gt; conc { 2l ) t of relalional-lrrOCCSs , which i. % disjoinl .</sentence>
				<definiendum id="0">relalional-lrrOCCSs</definiendum>
				<definiens id="0">oMing | , ( ) tim Mcrg ( x\ [ \ [ Jl ) l ) er Model</definiens>
			</definition>
			<definition id="5">
				<sentence>mcs ( excluding Chc Ics'r-rclal &amp; ms ) , from which ouly 87 concepts cm~ ~1 ( ~ally be identiIicd .</sentence>
				<definiendum id="0">mcs</definiendum>
				<definiens id="0">excluding Chc Ics'r-rclal &amp; ms ) , from which ouly 87 concepts cm~ ~1 ( ~ally be identiIicd</definiens>
			</definition>
			<definition id="6">
				<sentence>The ehmse/PP distinction , for example , which is often a concept discrinliliation criterion in the GUM violates Principle 1 and so this &lt; tiscrim : , nation is not preserved in the Merged Upper Model. Therefore , leaving out of account the clause/PP distinct , on , identical concepts then amount to 163. The number and distribution of concepts identical after union is shown by the numbers in brackets in Figure 3. 106 concepts are strongly identical and 57 merged coneepts are identical with tire unions of dill fcrent GUM con ( : epts. Extension Extension can be Pound in both directions. Because of the emphasis we have given to the EUM , most of the extensions are I ' ; UM concepts which extend tile GUM further. These are 60 concepts , 11 for the Mental-Process , ll Participants and 38 others from the Relational-Process hierarchy. On the other hand , only 4 German participant , concepts have found their way into the Merged Upper Model. Cross classification An essentiM field tbr cross classification has been avoided by the relaxation of the upper modelgrammar interface stated in Principle : { in Section 2.3. For exainple , whereas the cross class , Ifcation ( liscussed for the MaleriaI-Proccss/Aclion liierarchy in Section 2.3 would have cross classified and their subhierarchies respectively , resulting in 42 merged concepts , 9 concepts are suflicient to cover all distinctions expressed in tile EUM and the GUM. Slllillrlary Summarizing the urerging statistics , strong identity can be found for 41 % . \ [ f we allow identification of unified concepts , identity can be stated for 63 % . About 25 % of ' the merged UM are created by extension , and only 3.6 % by cross classification. Beside this , there is a small part of t.he Merged Upper Model ( 8 % ) where the concepts are not crea.ted by identification , extension and cross classification , but by preferring IgUM concepts over GUM ( ) lies. In the current merging proce , ss , we have only h ) oked tbr identities and dift~rences between tile given English and German Upper Models. Wc did not try to improve the inherent consistency of both , although it becanie clear during the merge thai , ccrtain distinctions should be removed and others tilrther developed ; these local improvements are detailed in \ [ llensehel , 1993\ ] and will be incorporated in future vet'sions of tile Merged Upper Model. In addition , one of our tints with the Merged Upper Model is to provide a stable basis for fllrther +.__Material 2 /llr , i &lt; ess 41 ( 98 ) ~'-Mental UM-Thing / Thin 106 ( 163 ) Object/Entity 35 Rdalional 33 ( 90 ) Quality ~0 Figure 3 : Identity statistics and distribntion extension both to include , tim ; her linguistic I ) he , nolnelaa and to cover further languages. We cx-. pect that an organization of information based on the requirements of natural language gram mars will provide a inert st~d ) le and re-ilsable resnlt than of ganizations based on the requirements of individual cornl ) utational systems. We are already using the Merged Upper Model as the basis for sentence generation in I ) utch and l , here is suggestion here that , again , few additional concepts appear necessary. Of more interest is the extension to rather ( lilt &gt; rent lan .</sentence>
				<definiendum id="0">ehmse/PP distinction</definiendum>
				<definiendum id="1">UM concepts</definiendum>
				<definiendum id="2">Merged Upper Model</definiendum>
				<definiens id="0">concepts have found their way into the Merged Upper Model. Cross classification An essentiM field tbr cross classification has been avoided by the relaxation of the upper modelgrammar interface stated in Principle : { in Section 2.3. For exainple , whereas the cross class</definiens>
				<definiens id="1">not crea.ted by identification , extension and cross classification , but by preferring IgUM concepts over GUM ( ) lies. In the current merging proce</definiens>
				<definiens id="2">tile given English and German Upper Models. Wc did not try to improve the inherent consistency of both , although it becanie clear during the merge thai</definiens>
				<definiens id="3">cx-. pect that an organization of information based on the requirements of natural language gram mars will provide a inert st~d ) le and re-ilsable resnlt than of ganizations based on the requirements of individual cornl ) utational systems. We are already using the Merged Upper Model as the basis for sentence generation in I ) utch and l</definiens>
			</definition>
</paper>

		<paper id="1079">
			<definition id="0">
				<sentence>A lexical item is a triple : &lt; \ [ i , j\ ] , av~lf , av ... .. p &gt; , where \ [ i , j\ ] is an interval denoting the position of the word in the sentence ; av~lf is the attribute values of the word sense ; and av , :o , , , , is the attribute values of the complements of the word sense .</sentence>
				<definiendum id="0">lexical item</definiendum>
				<definiens id="0">a triple : &lt; \ [ i , j\ ] , av~lf , av ... .. p &gt;</definiens>
				<definiens id="1">an interval denoting the position of the word in the sentence ; av~lf is the attribute values of the word sense</definiens>
				<definiens id="2">the attribute values of the complements of the word sense</definiens>
			</definition>
			<definition id="1">
				<sentence>The parse forest consists of the links of the grammar network that are traversed during the tracing process .</sentence>
				<definiendum id="0">parse forest</definiendum>
				<definiens id="0">consists of the links of the grammar network that are traversed during the tracing process</definiens>
			</definition>
			<definition id="2">
				<sentence>Each lexical entry consists of ai1 eIltry word or phrase and a , list of functions with a , r~tllllClltS : ( &lt; en~ ; ry-~ord-or-phras e &gt; ( &lt; tune-name &gt; &lt; arg &gt; ... &lt; arg &gt; ) ( &lt; gunc-name &gt; &lt; arg &gt; ... &lt; art &gt; ) ( &lt; -June-name &gt; &lt; arg &gt; .</sentence>
				<definiendum id="0">lexical entry</definiendum>
				<definiens id="0">consists of ai1 eIltry word or phrase and a , list of functions with a , r~tllllClltS : ( &lt; en~ ; ry-~ord-or-phras e &gt; ( &lt; tune-name &gt; &lt; arg &gt; ... &lt; arg &gt; ) ( &lt; gunc-name &gt; &lt; arg &gt; ... &lt; art &gt; ) ( &lt; -June-name &gt; &lt; arg &gt;</definiens>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>The KBMT-89 project ( Knowledge Based Machine Translation ) at Carnegie Mellon University in the US and the EI ) R. ( Electronic Dictionary Research ) project in Japan use tiffs approach .</sentence>
				<definiendum id="0">KBMT-89 project</definiendum>
				<definiens id="0">Knowledge Based Machine Translation ) at Carnegie Mellon University in the US and the EI ) R. ( Electronic Dictionary Research ) project in Japan use tiffs approach</definiens>
			</definition>
			<definition id="1">
				<sentence>NADIA is the continuation of a work done for the Multilex ESPRIT project .</sentence>
				<definiendum id="0">NADIA</definiendum>
				<definiens id="0">the continuation of a work done for the Multilex ESPRIT project</definiens>
			</definition>
			<definition id="2">
				<sentence>A MLI ) B consists of two kinds of diclionarics : lhc monolingual dictionaries and the acception dictionary .</sentence>
				<definiendum id="0">MLI ) B</definiendum>
				<definiens id="0">consists of two kinds of diclionarics : lhc monolingual dictionaries and the acception dictionary</definiens>
			</definition>
			<definition id="3">
				<sentence>Browser : this tool gives ways to browse through Ihe database .</sentence>
				<definiendum id="0">Browser</definiendum>
				<definiens id="0">this tool gives ways to browse through Ihe database</definiens>
			</definition>
			<definition id="4">
				<sentence>In the following example , an Entry consists in a feature structure with two fcalures ( a graphic-lbmi and \ [ i categHry ) .</sentence>
				<definiendum id="0">Entry</definiendum>
				<definiens id="0">consists in a feature structure with two fcalures ( a graphic-lbmi and \ [ i categHry )</definiens>
			</definition>
			<definition id="5">
				<sentence>The body of tile rule is a lisp expression that must t'CltJrn T or nil .</sentence>
				<definiendum id="0">tile rule</definiendum>
				<definiens id="0">a lisp expression that must t'CltJrn T or nil</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>KANT is an interlingua-based system : the sonrce language analyzer produces an interlingua expression for each source sentence , and this interlingua is processed to produce the corresponding target sentence .</sentence>
				<definiendum id="0">KANT</definiendum>
				<definiens id="0">an interlingua-based system : the sonrce language analyzer produces an interlingua expression for each source sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>`` File lexicon consists of closed-class general words , open-class general words , idioms , and nomenclature phrases .</sentence>
				<definiendum id="0">File lexicon</definiendum>
				<definiens id="0">consists of closed-class general words , open-class general words , idioms , and nomenclature phrases</definiens>
			</definition>
			<definition id="2">
				<sentence>Idioms ( e.g. on and off ) and nomencl &gt; tnre phrases ( e.g. summing valve ) are domain-specilic and are limited to those phrases identilied in the domain corpus .</sentence>
				<definiendum id="0">Idioms</definiendum>
				<definiens id="0">e.g. on and off ) and nomencl &gt; tnre phrases</definiens>
			</definition>
			<definition id="3">
				<sentence>The constrained lexicon consists of 10,000 words and 50,000 phrases talk ) red to the application domain .</sentence>
				<definiendum id="0">constrained lexicon</definiendum>
				<definiens id="0">consists of 10,000 words and 50,000 phrases talk ) red to the application domain</definiens>
			</definition>
			<definition id="4">
				<sentence>Test LEX GRA N-N DM P LEX : Lexicon GEN : General GILA : Grammar CON : Constrained N-N : Noun-Noun Compounding DM : Semantic Restriction with l ) omain Model Figure 7 : Testing Disambiguation Methods ( 12/17/93 ) The results of this testing ~u'e shown in Figure 7 .</sentence>
				<definiendum id="0">Test LEX GRA N-N DM P LEX</definiendum>
				<definiens id="0">Lexicon GEN : General GILA : Grammar CON : Constrained N-N : Noun-Noun Compounding DM : Semantic Restriction with l ) omain Model Figure 7 : Testing Disambiguation Methods</definiens>
			</definition>
</paper>

		<paper id="1099">
			<definition id="0">
				<sentence>The sorts tool provides the option of harvesting sort rules in one of two ways , either from all generated logical forms , or only from the Preferred Logical I'brm ( PLF ) .</sentence>
				<definiendum id="0">sorts tool</definiendum>
				<definiendum id="1">Logical I'brm</definiendum>
				<definiens id="0">provides the option of harvesting sort rules in one of two ways , either from all generated logical forms , or only from the Preferred</definiens>
			</definition>
			<definition id="1">
				<sentence>In addition to this , the possibility of a systematic examination for all predicates with crosschecldng tools such as sentence visualisation and funetor browing helps the linguist to establish strict aquisition methods for the knowledge base in new donlgins .</sentence>
				<definiendum id="0">funetor browing</definiendum>
				<definiens id="0">helps the linguist to establish strict aquisition methods for the knowledge base in new donlgins</definiens>
			</definition>
</paper>

		<paper id="2195">
			<definition id="0">
				<sentence>riter learns tra.nsfor-Ill~ttiollS \ [ ¥onl a C , ) l : l &gt; tls O\ [ 4-tuples of the \ [ 'orm ( v I11 I\ ] 1|9 ) , where v is ~1 w ; rl ) , nl is the head of its objecl , llolni \ ] phrase , i ) is the \ ] ) l'epositioll , and 11:2 is the head of the noun phrase , governed by the prel ) c , sition ( for e , - : anq~le , sce/v :1~ ' bo : q/ , l o , /p the h711/~2 ) .</sentence>
				<definiendum id="0">nl</definiendum>
				<definiendum id="1">i )</definiendum>
				<definiens id="0">the head of the noun phrase , governed by the prel</definiens>
			</definition>
</paper>

		<paper id="2211">
			<definition id="0">
				<sentence>Therefore , WM is a tool that may be seen as an ideal supplement for today 's finite-state analyzers and transducers and thus a further step towards the development of truly reusable dictionaries .</sentence>
				<definiendum id="0">WM</definiendum>
				<definiens id="0">a tool that may be seen as an ideal supplement for today 's finite-state analyzers and transducers</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>( \ ] ellL is a , seL of it .</sentence>
				<definiendum id="0">ellL</definiendum>
				<definiens id="0">a</definiens>
			</definition>
			<definition id="1">
				<sentence>Condition ( =c ) : ( permanent ) a condition that must |lold for a State or Event to come about .</sentence>
				<definiendum id="0">Condition</definiendum>
				<definiendum id="1">=c )</definiendum>
				<definiens id="0">a condition that must |lold for a State or Event to come about</definiens>
			</definition>
			<definition id="2">
				<sentence>Permauent links are those that are augmented to tile lletwork and COlllleCt uodes cue I , o tile other in accordance with the features found in those nodes .</sentence>
				<definiendum id="0">Permauent links</definiendum>
			</definition>
			<definition id="3">
				<sentence>% decide between a definite and indefinite article in \ ] '~nglish , a simple rule of thumb in the present system is that once an object has been specified in a coiitexL , 6ttSstlnlillg that the noun is defined as phu'al l ) y sonic ( ~lher process , oLherwlse a vtdeo is als~ a l ) osslbilily ; ill subsequent references to that parLichh~r object in the saine context will be definite r. In the method proposed here , as objects are analysed , they are giwm a unique reference number ( re* ) that separates them l'ron~ all other objects of file same type .</sentence>
				<definiendum id="0">y sonic ( ~lher process</definiendum>
				<definiens id="0">a definite and indefinite article in \ ] '~nglish , a simple rule of thumb in the present system is that once an object has been specified in a coiitexL , 6ttSstlnlillg that the noun</definiens>
				<definiens id="1">objects are analysed , they are giwm a unique reference number ( re* ) that separates them l'ron~ all other objects of file same type</definiens>
			</definition>
</paper>

		<paper id="1075">
			<definition id="0">
				<sentence>The framework consists in a uniform theoretic description of parsing algorithms , and provides the structure for decomposing the system into logical components , with possibly several interchangeable implementations .</sentence>
				<definiendum id="0">framework</definiendum>
				<definiens id="0">consists in a uniform theoretic description of parsing algorithms , and provides the structure for decomposing the system into logical components , with possibly several interchangeable implementations</definiens>
			</definition>
			<definition id="1">
				<sentence>Definition 1 ConstTnint-11ased Grammar A constraint-based grammar is a 7-tuple { Nt , T , ( ~ , V , Am , C L , R } where • Nt is a set of symbols called non-terminals • 7 ' is a set of symbols called terminals • a is a flmetion from Nt O 7 ' to the natm'al integers called the arity of the symbol , s • V is an infinite set of variables • Aa : is an element of Nt called the a : dom • CL is a constraint language ( see definition beloin ) having V as variable set and being closed it~'tder renaming a~td conjunction • R is a finite set of rules of the form : - , ( 2 ' , ) ... . , &lt; 2 ; , ) such that so E Nt , sl ~ Nt U 7 ' for 0 &lt; i _ &lt; .</sentence>
				<definiendum id="0">constraint-based grammar</definiendum>
				<definiendum id="1">Nt</definiendum>
				<definiendum id="2">V</definiendum>
				<definiendum id="3">CL</definiendum>
				<definiendum id="4">R</definiendum>
				<definiens id="0">a set of symbols called non-terminals</definiens>
				<definiens id="1">an infinite set of variables • Aa : is an element of Nt called the a : dom •</definiens>
				<definiens id="2">a constraint language ( see definition beloin ) having V as variable set and being closed it~'tder renaming a~td conjunction •</definiens>
			</definition>
			<definition id="2">
				<sentence>Definition 2 Constrnint Language A constraint Language is a 4-tuple ( V , C , u , I ) such that : • V is an infinite set of variables • C is a decidable set whose elements are called cons traints • u is fanction that associates a finite set of variables to eaeh constraint • I is a non-empty set of interpretations Ii'or bt &lt; : k of Slm &lt; : e we &lt; lo not recall in detail what itll interpret &amp; tioll Jill ( | the `` &lt; 'losuro lllldel '' I ' ( ! IlH.III ~ ing '' pr &lt; ) perty are , and refer to \ [ IIS88\ ] . The semantics of Constra.int-Based Gnmmlars is defined by the .'- ; ( ? lllalltics of the constra.int language ~tll ( l l , ho notion of syntax tree. A synta.x trce is a tree which \ ] ms at grammttr rule ( remtmed with fi'esh v~triables ) as latml of ea.ch nodo. A constraint is associatted to at parse tree : it is the conjunction of all the constr~dnts of the labels and the oqualities between the tUllle of wtriables from the non-termilml , if the loft-hand side of a label and the tlq ) le of the relewmt symbol of tim right &gt; hand side of tim l~dml of its p~trent .</sentence>
				<definiendum id="0">constraint Language</definiendum>
				<definiendum id="1">C</definiendum>
				<definiendum id="2">Constra.int-Based Gnmmlars</definiendum>
				<definiendum id="3">synta.x trce</definiendum>
				<definiens id="0">a 4-tuple ( V , C , u , I ) such that : • V is an infinite set of variables</definiens>
				<definiens id="1">a decidable set whose elements are called cons traints • u is fanction that associates a finite set of variables to eaeh constraint • I is a non-empty set of interpretations Ii'or bt &lt; : k of Slm &lt; : e we &lt; lo not recall in detail what itll interpret &amp; tioll Jill ( | the `` &lt; 'losuro lllldel '' I '</definiens>
				<definiens id="2">a tree which \ ] ms at grammttr rule ( remtmed with fi'esh v~triables ) as latml of ea.ch nodo. A constraint is associatted to at parse tree : it is the conjunction of all the constr~dnts of the labels and the oqualities between the tUllle of wtriables from the non-termilml , if the loft-hand side of a label and the tlq ) le of the relewmt symbol of tim right &gt; hand side of tim l~dml of its p~trent</definiens>
			</definition>
			<definition id="3">
				<sentence>455 syntax trees are built ( e.g. top- &lt; lown , bottomIll ) ) , and the control strategy that &lt; lcals with the non-determinism of the parsing ( e.g. backtracking , tabulation ) . This separation is based on an intermediate representation that describes how a grammar is used following a given parsing strategy. This intermediate representation is a Push-Down Automaton. It is known that most context-free parsers can be encoded with such a stack machine. Of course , the usual formalism has to be extended to take constraints into account , and possibly use them to disambiguate the parsing. We. call Extended Push-Down Automaton ( EPDA ) the extended formalism. For lack of space , we do not give here the formal definition of EPDA. hfformally , it is a machine using three data structures : a stack containing at each level a stack symbol and its tuple of variables ; a representation of the terminal string that distinguishes those that have already been used and those that are still to be read ; finally a constraint. A configuration of an automaton is a triple of these three data. Transitions are partial fimctions from configurations to configurations. We add some restrictions to these transitions : the only clmnge allowed for the string is that at most one more terminal is read ; only the top of the stack is accessible and at most one symbol can be added or removed from it at once. These restrictions are needed to employ directly the generic tabular techniques for automata execution described in \ [ BVdlC92\ ] . EPDAs may be non-deterministic , i.e. several transitions are applicable on a given configuration. Parsing for Constraint-Based Grammars blen ( ls two tasks : • The structural part , that consists in buihling the skeleton of parse trees. This l ) art is similar to a context-free parsing with the underlying context-free projection of the grammar. • Solving the constraints of this skeleton. The two tasks are related in the following way : constraints appear at the nodes of the tree ; the structure is not a valid syntax tree if the constraint set is unsatisfiable. Each task can be performed in several ways : there are several contextfree parsing methods ( e.g. LL , LR ) and constraints sets can be solved globally or incrementally , using various orders , and several ways of mixing the two tasks are valid. Tree construction involves a stack mechanism , and constraint solving results in a constraint. The different parsing teelmiques can be described as computations on these two data structures. EPDAs are thus able to enco &lt; le various l ) arsers for Constraint C~ramnlars. Automatic translation of grammars into EPDAs is possible using extensions of usual contextfree teelmiques \ [ Bar93\ ] . Thanks to the intermediate representation ( EPDA ) , parsing can be divi &lt; led into two independent passes : tile compilation that translates a granlnlar into an extended autonlaton ; tim execution that takes an EPDA and a string and produees a forest of syntax trees. To achieve the independence , the compihw is not allowed to make any assumptions about the way the automata it produces will lie executed , and the interpreter in charge of the execution is not allowed to make assumptions about the automata it executes. We add to this scheme reused from contextfree parsing a thir &lt; l component : the solver ( in an extensive meaning ) in charge of all the oi &gt; erations related to constraints and wu'iables .</sentence>
				<definiendum id="0">automaton</definiendum>
				<definiens id="0">a machine using three data structures : a stack containing at each level a stack symbol and its tuple of variables ; a representation of the terminal string that distinguishes those that have already been used and those that are still to be read ; finally a constraint. A configuration of an</definiens>
				<definiens id="1">constraints appear at the nodes of the tree</definiens>
				<definiens id="2">the intermediate representation ( EPDA ) , parsing can be divi &lt; led into two independent passes : tile compilation that translates a granlnlar into an extended autonlaton ; tim execution that takes an EPDA and a string and produees a forest of syntax trees. To achieve the independence , the compihw is not allowed to make any assumptions about the way the automata it produces will lie executed</definiens>
			</definition>
			<definition id="4">
				<sentence>We will try to make it as in &lt; lel ) en &lt; teilt from the other two modules ( compiler and interpreter ) as possible. There is not a fidl in &lt; lependenee , since both the compiler and the interpreter involve constraints and related operations , that are : l ) erfornmd by the solver. We just want to define a ( : lear interface between the solver and the other modules , an interface independent from the kind of the constraints and from the solving algorithms being used. rl'be same coml ) iler ( resp. interl ) reter ) used with different solvers will work on ditl'erent classes of grammars. For instance , the same compiler can compih~ Unilh : ation Grammars an &lt; l Definite Clause Grammars , using two solvers , one implenmnting feature unilieation , the second one iml ) lementing tirst-order unilieation. We can see a complete parsing system as the eoml ) ination of three modules , compiler , interprefer , solver. When ea ( : h module has several implementations , we wouhl like to take any combination of three modules. This schematic abstraction captures l ) arsing algorithms we are interested in. However , actually defining interfaces for a practical system without restricting openendedness or the abstraction ( interehangeability of components ) was the most difficult technical task of this work. 456 The main problem lies in the dclinition of the solver 's interface. Some of the required ol ) eralions are ol ) vious : renaming of constraints and tul ) les , constraint lmilding , extraction of the varial ) les from a constraint , etc. By the way , remark that constraint solving can be hidden within the solver , and thus not appear in the interface. There is an equivalence relation between constraints given by their interpretations. This relation can lie used to replace a constraint by another eqniwdent one , l ) ossibly siml ) ler. The solving call also be explicitly used to enR ) ree the simplification of constraints at some points of tile parsing. Unfortunately some special techniques require more specific operations on constraints. For instance , a family of parsing strategies related to Earley 's algorithm m~tke use of the restrictio~ operator defined by Shieber in \ [ Shi85\ ] . Another examl ) le : some tabular techni ( lues take Benetit from a projectioil operator that restricts constraints with respect to a subset of their variat ) les. We. could define the solver 's inte.rface as the cartesian product of all the operations used by ; tt least one technique. There are two reasons to re } cot such an apI ) roaeh. The first one is that some seldom used operations are ditli ( : ult to deline on some constraints domains , it is the case , among others , of tile projection. The second reason is that it woul ( \ [ restrict to the techniques aI : ready existing and known by us at the moment when we design tile interface. This contradicts the open-endedness requirement. A new olleration can appear , useful for a new parsing method or for optimizing the old ones. We prefer a flexible detlnition of the interface. Instead of defining one single interface , we will allow each alternative iniF , lenlentation of the solver to define exactly what it ol\ [ 'ers and each iml ) h~nmntation of the compiler or of the interpreter to detine what it demands. The conll ) ination of modules will involve the checking that the @ r &lt; '.r encompasses the demand , that all tile needed operations are implemented. This imposes restrictions on the combination of niodules : it is the overhead to obtain an open-ended system , opened to new developments. We found it language providing the. kind of llexil ) le modularity we needed : Alcool -- 90. We now present this language. 4 '\ ] 'IIE LANGUAGE ALCOOL 90 Alcool-90 is an experimental extension of the functional language ML with run-time overloading \ [ I { ou90\ ] . Overloading is used as a tool for seamless integration of abstract data types ill the ML type system , retaining strong typing , and type inference prollerties. Abstract data types ( encapsulating a data structure representation and its constructors ~uld interpretive flmctiol , s ) i ) rovide wdues for overloaded symbols , as classes provide methods for messages ill objecto , 'ientcd terminology , i { owever , strong typing means that the compiler guarantees that errors ( ) f kind `` method not found '' never hal ) pen. Abstract programs axe programs referring to overloaded syml ) ols , which vahles will be deternfined at run-time , consistently with the calling environment. By grouping Mlstract l ) rograms , we obtain parameterized abstra.ct data types ( or fllnctors ) , the calling environment being here a~ particular instantiation of the I ) arameterized adt. Thus , we obtain Jut environment equivalent to a module system , each module being an adt , eventually llarameterized. D ) r instance , ill APOC-II , ( : ompilers h~tve an abstract data type parameterized by a solver. Alcool-90 also proposes an innow~tive environment where we exploit anlbiguities due to overloading for semi-automated 1 ) rogram configuration : the type iufin'elice eoullnltes interfaces of % llissing '' COIllpollents to colnplete a progralll , aecording to the use of overloaded synlbols in the program. A search algo , 'ithm finds components satisfying those interfaces , eventually by tinding suitable parameters for parameterized components. Naturally , instantiatiot , of parameterized coml ) onents is also type-safe : actual parameters must have interfaces matching formal parameters ( schematically : the actual parameter must provide at least the functions required by the interface of the formal parameter ) . For instance , only the solvers provi ( lil , g Shieber 's restriction can } ) e used as the. aetlial pa.ramcter of Earley with restriction compiler. But these solvers can also be '.lse ( l l ) y a.ll the eoml ) ilers that do not use the restriction. Simple module systems have severe limitations when several implementations of components with simil~tr interfaces ( : ( ) exist in a system , or when some component Inay be employed in different contexts. Ada generics provided a first step to lnodule parameterization , th ( mgh at the cost of heavy declar~tions a.nd difficulties with type equiwdence. SML pral ) oses a very powerful module system with paranleterization , but lacks separate comllilation and still requires a large amount of user decl~u'ations to detine and use functors. Object-oriented languages lack the type security that Alcoo\ [ -90 guarantees. 457 The Alcool-90 approach benefits from the simplification ot modules as abstract data types by adding inference facilities : the compiler is able to infer the interfaces of parameters required by a module. Moreover , the instantiation of a functor is simply seen as a type application , thus no efforts are required from the programmer , while its consistency is checked by the compiler. This approacl , is mostly useful when multiple implementations with similar interfaces are available , whether they will coexist in the program or they will be used to generate several configurations. Components may have similar interfaces but different semantics , although they are interchangeable. Choosing a configuration is simply choosing fl'om a set of solutions to missing emnponents , computed by the compiler. Several other features of Alcool-90 have not linen used in this experiment , namely the inheritance operator on abstract data types , and an extension of tile type system with dynamics ( where some type checking occurs at run-time ) . APOC-II is a system written in Alcool-90 , implementing numerous parsing techniques within the framework described in section 3. The user can choose between these techniques to buihl a parser. By adding new modules written in Alcool-90 to the library , new techniques can freely be added to the system. APOC-II has two levels of modularity : the first one is that of the three main components distinguished above , compiler , interpreter and solver. Each of these components is implemented by several alternative modules , that are combinable using Alcool-90 discipline. Tile second level of modularity consist in splitring each of the three main components i , lto severa.1 modules. This makes the sharing of common parts of different hnplementations possible. We give now examples of splitting APOC-ql uses at the moment , in order to give an idea of this second level of modularity. This splitting has proved convenient so far , but it is not fixed and imposed to fllrther developments : ~t new implementation can be added even if it uses a completely different internal structure. A solver is made of : • a module for wtriables , variabh : generation and renaming , • a parser for constraints , • a pretty-printer for constraints , • a constraint builder ( creation of abstract syntax trees for constraints , e.g. building constraints expressing equality of variables ) , • a solver ill the restrictive meaning , in charge of constraint reduction , • an interface that encapsulate all the other modules. A compiler includes : • a grammar parser ( that uses tile constrMnt parser given by the solver ) , • a module for look-ahead ( for computation of look-ahead sets by static anMysis of the gramI\ [ lar ) , • a module for EPDA representation and handling , • ~t transition generator which translates grammar rules into EPDA tra.nsitions therefore deternfining the p~trsing strategy ( cf. figure 1 ) , • Control code , using previous modules , defining the `` compih ? ' function , tile only one exported. The two interpreters implemented so far have very different structures. The tlrst one uses backtracking and the second one uses tabulation. They share some modules however , such as a module handling transitions and a lexer of inlmt strings. Tile interest of the modular architecture is in tile eomtfin~ttorhtl effect of module composition. It leads to many diiferent parsing algorithms. The tigure 1 summarizes the different ~spects of the parsing algorithms that can vary more or less independently. For example , the built-in parsing method of Prolog for DCGs is ol~t.ained by combining tim solver for \ ] ) CGs , the top-down strategy , 0 symbol of look-ahead a.nd a backtracking interpreter ( and other modules not mentioned in Iigure 1 because they do not change the algorithm , but a.t most its implenmntation ) . Some remarks about : figure 1 : • we call Earle ? \ ] parsing strategy the way Earley deduction \ [ PW8a\ ] builds a tree , *tot the control method it uses. It difl'e.rs from top-down by the way constrMnts are taken into account. • the difference between garley-like tabulation and graph-structure stacks is the data structure used for item storage. Several variants are possible , that actually change the parser 's behavior. 458 Solver Context-tYee Grammars 1 ) etinite Clause Grammars ( grammar class ) Tree Adjoining Grammars Uni\ ] ication Grammars ... parsing strategy top-down pure bottom-up Earley Earley with restriction ( transition generator ) left-corner LR precedence PLR ... look-ahead eontext-lYee look-ahead of 0 or 1 symbol context-free look-ahead of k symbols contca't-scnsitivc look-ahead interpreter backtracking Earley-like tabulation Graph-str'acturcd Stacks ... Agenda management Synchronization lifo fifo wLrio'as weights ... ( for tabulation only ) Figure 1 : modules of APOC-II Modules written iii. bold font are ah'eady iml ) lemented , where.as modules written in italic m'e possible extensions to the system. • we call synchronization sL kind of breadth-first se~trch where sc~tnnlng a terminal is performed only whe.n it is needed by all the paths of the search-tree. The search is synchronized with the. input string. It is the order used by l , ; strh.'y 's algorithin. • at the moment , only generic look-ahead , that is look-ahestd based on the first and follow sets , has been considered. Some more aCCllrate look-ahead techniques such as the ones involved in SLR ( k ) pa , 'sing are probal &gt; ly not indepen &lt; lent fi'om the parsing strategy and &lt; : armor be an independent mo &lt; lule. Building a parsing system with APOC-II consists roughly in choosing one module of each row of figure 1 and combining them. Some of the combinations are not possible. Thanks to typechecking , Alcool-90 will detect the incompatibility and provide a tyl ) e-based explanation of the probh ; m. At the moment , APOC-II otDrs more than 60 ditDrent parsing algorithms. Given a g , ralrHn. % r , there is a choice of more than 20 different parsers. Adding one module does not add only one more algorithm , but sewn 'M new vstri ; tltts. The techniques iinplemented by APOC-II are not original. For instance , the LR conq ) ilation strategy comes from a paper I ) y Nilsson , \ [ Nil86\ ] , left-corner parsing has been used 1 ) y Matsumoto and Tanaka in \ [ MT83\ ] . As far as we know , however , LR and left-era'her p~trsers have not been prolmsed for Tree-Adjoining C , rammars before. Notice that the modularity is also useful to vary implementation of algorithms. D ) r instance , a first prototype can be quickly written by implementing constraints reduction in a naive way. A refined version can be written later , if needed. APOC-II has several advantages. First of all , it provides comparable implementations of the most comnmn parsing Mgorithms. Their efficiency can be abstractly measured , for instance by counting the number of eomlmtation step ( EPDA transition applicatiol 0 performed to eomlmte a tree or a complete forest of parse trees. We call this kind of measm'ements abstract \ ] ) ecallse it does not rely neither on the implementlttion nor on the machine that runs the parser. Other comparisons could be done statically , on the automaton or on the pstrse forest ( e..g. number of transitions , alllOllllt , ) f determi~lisnl , size of the forest , alllOllllt of structure slurring ) . ( ) therwise , APOC-II cstn be. used as a to ( ~lkit that provides : t library of modules usefld to implelllent quickly ll ( ! W parse.r generators. For instance , one has only to write a solver to obtain up to 22 parsing a.lgorithms ( perhaps less if tit ( ' , solw ! r provides only basic operations ) . The library contains tools to deal with some constraints , look-ahead , lexing , tabulation , etc. Reusing these tools whenever it is possible saves a lot of work. The limitations of APOC-II are that it is mainly convenient for parsing strategies that stre somehow static , i.e. statically determined at compih ! time. Also , al ) stractloll ( full independence between coral &gt; tiers and i , ~terpreters ) can not Im achieved for some optimized algorithms .</sentence>
				<definiendum id="0">interpreter involve constraints</definiendum>
				<definiendum id="1">. APOC-II</definiendum>
				<definiens id="0">try to make it as in &lt; lel ) en &lt; teilt from the other two modules ( compiler and interpreter</definiens>
				<definiens id="1">l ) erfornmd by the solver. We just want to define a ( : lear interface between the solver and the other modules , an interface independent from the kind of the constraints and from the solving algorithms being used. rl'be same coml ) iler ( resp. interl ) reter ) used with different solvers will work on ditl'erent classes of grammars. For instance , the same compiler can compih~ Unilh : ation Grammars an &lt; l Definite Clause Grammars , using two solvers , one implenmnting feature unilieation , the second one iml ) lementing tirst-order unilieation. We can see a complete parsing system as the eoml ) ination of three modules , compiler , interprefer , solver. When ea ( : h module has several implementations , we wouhl like to take any combination of three modules. This schematic abstraction captures l ) arsing algorithms we are interested in. However , actually defining interfaces for a practical system without restricting openendedness or the abstraction ( interehangeability of components ) was the most difficult technical task of this work. 456 The main problem lies in the dclinition of the solver 's interface. Some of the required ol ) eralions are ol ) vious : renaming of constraints and tul ) les , constraint lmilding , extraction of the varial ) les from a constraint , etc. By the way , remark that constraint solving can be hidden within the solver , and thus not appear in the interface. There is an equivalence relation between constraints given by their interpretations. This relation can lie used to replace a constraint by another eqniwdent one , l ) ossibly siml ) ler. The solving call also be explicitly used to enR ) ree the simplification of constraints at some points of tile parsing. Unfortunately some special techniques require more specific operations on constraints. For instance , a family of parsing strategies related to Earley 's algorithm m~tke use of the restrictio~ operator defined by Shieber in \ [ Shi85\ ] . Another examl ) le : some tabular techni ( lues take Benetit from a projectioil operator that restricts constraints with respect to a subset of their variat ) les. We. could define the solver 's inte.rface as the cartesian product of all the operations used by ; tt least one technique. There are two reasons to re } cot such an apI ) roaeh. The first one is that some seldom used operations are ditli ( : ult to deline on some constraints domains , it is the case , among others , of tile projection. The second reason is that it woul ( \ [ restrict to the techniques aI : ready existing and known by us at the moment when we design tile interface. This contradicts the open-endedness requirement. A new olleration can appear , useful for a new parsing method or for optimizing the old ones. We prefer a flexible detlnition of the interface. Instead of defining one single interface , we will allow each alternative iniF , lenlentation of the solver to define exactly what it ol\ [ 'ers and each iml ) h~nmntation of the compiler or of the interpreter to detine what it demands. The conll ) ination of modules will involve the checking that the @ r &lt; '.r encompasses the demand , that all tile needed operations are implemented. This imposes restrictions on the combination of niodules : it is the overhead to obtain an open-ended system , opened to new developments. We found it language providing the. kind of llexil</definiens>
				<definiens id="2">an experimental extension of the functional language ML with run-time overloading \ [ I { ou90\ ] . Overloading is used as a tool for seamless integration of abstract data types ill the ML type system , retaining strong typing , and type inference prollerties. Abstract data types ( encapsulating a data structure representation and its constructors ~uld interpretive flmctiol , s ) i ) rovide wdues for overloaded symbols , as classes provide methods for messages ill objecto , 'ientcd terminology , i { owever , strong typing means that the compiler guarantees that errors ( ) f kind `` method not found '' never hal ) pen. Abstract programs axe programs referring to overloaded syml ) ols , which vahles will be deternfined at run-time , consistently with the calling environment. By grouping Mlstract l ) rograms , we obtain parameterized abstra.ct data types ( or fllnctors ) , the calling environment being here a~ particular instantiation of the I ) arameterized adt. Thus , we obtain Jut environment equivalent to a module system , each module being an adt , eventually llarameterized. D ) r instance , ill APOC-II , ( : ompilers h~tve an abstract data type parameterized by a solver. Alcool-90 also proposes an innow~tive environment where we exploit anlbiguities due to overloading for semi-automated 1 ) rogram configuration : the type iufin'elice eoullnltes interfaces of % llissing '' COIllpollents to colnplete a progralll , aecording to the use of overloaded synlbols in the program. A search algo , 'ithm finds components satisfying those interfaces , eventually by tinding suitable parameters for parameterized components. Naturally , instantiatiot , of parameterized coml ) onents is also type-safe : actual parameters must have interfaces matching formal parameters ( schematically : the actual parameter must provide at least the functions required by the interface of the formal parameter ) . For instance , only the solvers provi ( lil , g Shieber 's restriction can } ) e used as the. aetlial pa.ramcter of Earley with restriction compiler. But these solvers can also be '.lse ( l l ) y a.ll the eoml ) ilers that do not use the restriction. Simple module systems have severe limitations when several implementations of components with simil~tr interfaces ( : ( ) exist in a system , or when some component Inay be employed in different contexts. Ada generics provided a first step to lnodule parameterization , th ( mgh at the cost of heavy declar~tions a.nd difficulties with type equiwdence. SML pral ) oses a very powerful module system with paranleterization , but lacks separate comllilation and still requires a large amount of user decl~u'ations to detine and use functors. Object-oriented languages lack the type security that Alcoo\ [ -90 guarantees. 457 The Alcool-90 approach benefits from the simplification ot modules as abstract data types by adding inference facilities : the compiler is able to infer the interfaces of parameters required by a module. Moreover , the instantiation of a functor is simply seen as a type application , thus no efforts are required from the programmer , while its consistency is checked by the compiler. This approacl , is mostly useful when multiple implementations with similar interfaces are available , whether they will coexist in the program or they will be used to generate several configurations. Components may have similar interfaces but different semantics , although they are interchangeable. Choosing a configuration is simply choosing fl'om a set of solutions to missing emnponents , computed by the compiler. Several other features of Alcool-90 have not linen used in this experiment , namely the inheritance operator on abstract data types , and an extension of tile type system with dynamics ( where some type checking occurs at run-time )</definiens>
				<definiens id="3">a system written in Alcool-90 , implementing numerous parsing techniques within the framework described in section 3. The user can choose between these techniques to buihl a parser. By adding new modules written in Alcool-90 to the library , new techniques can freely be added to the system. APOC-II has two levels of modularity : the first one is that of the three main components distinguished above , compiler , interpreter and solver. Each of these components is implemented by several alternative modules , that are combinable using Alcool-90 discipline. Tile second level of modularity consist in splitring each of the three main components i , lto severa.1 modules. This makes the sharing of common parts of different hnplementations possible. We give now examples of splitting APOC-ql uses at the moment , in order to give an idea of this second level of modularity. This splitting has proved convenient so far , but it is not fixed and imposed to fllrther developments : ~t new implementation can be added even if it uses a completely different internal structure. A solver is made of : • a module for wtriables , variabh : generation and renaming , • a parser for constraints , • a pretty-printer for constraints , • a constraint builder ( creation of abstract syntax trees for constraints , e.g. building constraints expressing equality of variables ) , • a solver ill the restrictive meaning , in charge of constraint reduction , • an interface that encapsulate all the other modules. A compiler includes : • a grammar parser ( that uses tile constrMnt parser given by the solver ) , • a module for look-ahead ( for computation of look-ahead sets by static anMysis of the gramI\ [ lar ) , • a module for EPDA representation and handling , • ~t transition generator which translates grammar rules into EPDA tra.nsitions therefore deternfining the p~trsing strategy ( cf. figure 1 ) , • Control code , using previous modules , defining the `` compih ? ' function , tile only one exported. The two interpreters implemented so far have very different structures. The tlrst one uses backtracking and the second one uses tabulation. They share some modules however , such as a module handling transitions and a lexer of inlmt strings. Tile interest of the modular architecture is in tile eomtfin~ttorhtl effect of module composition. It leads to many diiferent parsing algorithms. The tigure 1 summarizes the different ~spects of the parsing algorithms that can vary more or less independently. For example , the built-in parsing method of Prolog for DCGs is ol~t.ained by combining tim solver for \ ] ) CGs , the top-down strategy , 0 symbol of look-ahead a.nd a backtracking interpreter ( and other modules not mentioned in Iigure 1 because they do not change the algorithm , but a.t most its implenmntation ) . Some remarks about : figure 1 : • we call Earle ? \ ] parsing strategy the way Earley deduction \ [ PW8a\ ] builds a tree , *tot the control method it uses. It difl'e.rs from top-down by the way constrMnts are taken into account. • the difference between garley-like tabulation and graph-structure stacks is the data structure used for item storage. Several variants are possible , that actually change the parser 's behavior. 458 Solver Context-tYee Grammars 1 ) etinite Clause Grammars ( grammar class ) Tree Adjoining Grammars Uni\ ] ication Grammars ... parsing strategy top-down pure bottom-up Earley Earley with restriction ( transition generator ) left-corner LR precedence PLR ... look-ahead eontext-lYee look-ahead of 0 or 1 symbol context-free look-ahead of k symbols contca't-scnsitivc look-ahead interpreter backtracking Earley-like tabulation Graph-str'acturcd Stacks ... Agenda management Synchronization lifo fifo wLrio'as weights ... ( for tabulation only</definiens>
				<definiens id="4">modules of APOC-II Modules written iii. bold font are ah'eady iml ) lemented , where.as modules written in italic m'e possible extensions to the system. • we call synchronization sL kind of breadth-first se~trch where sc~tnnlng a terminal is performed only whe.n it is needed by all the paths of the search-tree. The search is synchronized with the. input string. It is the order used by l , ; strh.'y 's algorithin. • at the moment , only generic look-ahead , that is look-ahestd based on the first and follow sets , has been considered. Some more aCCllrate look-ahead techniques such as the ones involved in SLR ( k ) pa , 'sing are probal &gt; ly not indepen &lt; lent fi'om the parsing strategy and &lt; : armor be an independent mo &lt; lule. Building a parsing system with APOC-II consists roughly in choosing one module of each row of figure 1 and combining them. Some of the combinations are not possible. Thanks to typechecking , Alcool-90 will detect the incompatibility and provide a tyl ) e-based explanation of the probh ; m. At the moment , APOC-II otDrs more than 60 ditDrent parsing algorithms. Given a g</definiens>
				<definiens id="5">a choice of more than 20 different parsers. Adding one module does not add only one more algorithm , but sewn 'M new vstri ; tltts. The techniques iinplemented by APOC-II are not original. For instance , the LR conq ) ilation strategy comes from a paper I ) y Nilsson , \ [ Nil86\ ] , left-corner parsing has been used 1 ) y Matsumoto and Tanaka in \ [ MT83\ ] . As far as we know , however , LR and left-era'her p~trsers have not been prolmsed for Tree-Adjoining C , rammars before. Notice that the modularity is also useful to vary implementation of algorithms. D ) r instance , a first prototype can be quickly written by implementing constraints reduction in a naive way. A refined version can be written later , if needed. APOC-II has several advantages. First of all , it provides comparable implementations of the most comnmn parsing Mgorithms. Their efficiency can be abstractly measured , for instance by counting the number of eomlmtation step ( EPDA transition applicatiol 0 performed to eomlmte a tree or a complete forest of parse trees. We call this kind of measm'ements abstract \ ] ) ecallse it does not rely neither on the implementlttion nor on the machine that runs the parser. Other comparisons could be done statically , on the automaton or on the pstrse forest ( e..g. number of transitions , alllOllllt , ) f determi~lisnl , size of the forest , alllOllllt of structure slurring ) . ( ) therwise , APOC-II cstn be. used as a to ( ~lkit that provides : t library of modules usefld to implelllent quickly ll ( ! W parse.r generators. For instance , one has only to write a solver to obtain up to 22 parsing a.lgorithms ( perhaps less if tit ( ' , solw ! r provides only basic operations ) . The library contains tools to deal with some constraints , look-ahead , lexing , tabulation , etc. Reusing these tools whenever it is possible saves a lot of work. The limitations of APOC-II are that it is mainly convenient for parsing strategies that stre somehow static , i.e. statically determined at compih ! time. Also , al ) stractloll ( full independence between coral &gt; tiers and i , ~terpreters ) can not Im achieved for some optimized algorithms</definiens>
			</definition>
</paper>

		<paper id="1090">
			<definition id="0">
				<sentence>&lt; d Ih/ : ./o t : ~ ! t+l ( l ( h ) * ,10. Befole , . : olllilUdl|y willx lhc. s ( : niaJdic dest : +ipiiotl , a_ t ) tic ! stlllllllaly of the al~xtr~lcl syntax of lira ( ) I , F t ( : ain is givclL The shnclure of file tom* Ihat shall be 10cussed ( m ( t ; e : \ [ m ( ... ) ) llas live llt ; /jof cOlllpOllellt+ , ; . The imlex ( :0. q\ ] fis is a unique idel~filicf associated wilh a particular t e~ ) : m. , e 'l'hc catu , t~ &lt; : O~ ( C ) . 'l.\ ] Je linguistic category ( ) l '' lhc CXlllCS-. sion : a list of feature value tuplcs. , , The restriction ( It ) . A li~st order one placv l ) redica { ( : dcscribing lhc tenn. • , The quantifier° A gcneralised qum ) litich i.e. a cmdb. nality predicate holding of two properties. , , The n.'fi &lt; re/H. An expression of rcl'erctlCC , either a con-. Stall\ [ O1 ' ; 1 lC , fill index. `` l'cfms may also be variables , indices or constants. 'l\ ] le QI , I , ' Ibrmula is similar in su uclure Io the term ( : ~o+_ 'm ( ... ) is dclined in \ [ A ( '.921 ) . Combitmtions of the liclds may be uninst~mtiated and it is Ihc instanliafion of lhese meta-variables that is tim cffcct of resolution. In lhe case of leuns , in the R } rwmd dircctiou , lhc quanlilier and rcli~rcnt will be uninslautiatcd ; in file backward direction , file category , reslriction autd quanlilier will be tminstantialed. Tim scmamics of I11¢ ' , QI , I , ' sis presented ill \ [ AC92\ ] arc extended ill tilt* , InilllllOf described. As showll ahoy ( 3 , a 5 : ; I QLF may contain any numt ) er of recta-variables and so tile scm , 'mtics of the language is slighlly different from the traditional one. Instead of a function from models to truth values , a partial function is delined. The denotation of a formula F is a partial function \ [ \ [ F\ ] \ ] from models to truth values. This partial fimction is defined by W , a relation between a formula , a model axtd ; t truth value. Tile quality of a monotonic semantics is apparent in the case whcn \ [ \ [ F\ ] \ ] with rcspcct to seine model is undefined , i.e. W ( F , m , 1 ) and W ( F , m , 0 ) . This situation occurs when recta-variables are present in the QLF and consequentially apartial interpretation is being considcrcd. The incremcat~d analysis provides an 'extension ' of \ [ \ [ F\ ] \ ] in \ [ \ [ F'\ ] \ ] whenever F ' is a more resolved version of F. The semantic rules that diseha , ge recta-variables arc of the general fonn : W ( F , m , v ) if W ( U , m , v ) where F ' is an 'extension ' of F. A relation CAT ef category is de ! incd wiLh the following arguments. ~ , category • index &lt; `` , 'of trent , quanliJicr &gt; ira term , o \ [ prope'rly ifa~orra ® restriclion , , context Rules describing the discharge of quantifier and reJerent meta-variables for teems are defined as follows .</sentence>
				<definiendum id="0">e</definiendum>
				<definiendum id="1">... ) ) llas live llt ; /jof cOlllpOllellt+</definiendum>
				<definiendum id="2">imlex</definiendum>
				<definiendum id="3">Ibrmula</definiendum>
				<definiendum id="4">F '</definiendum>
				<definiens id="0">s ( : niaJdic dest : +ipiiotl , a_ t ) tic ! stlllllllaly of the al~xtr~lcl syntax of lira ( ) I , F t ( : ain is givclL The shnclure of file tom* Ihat shall be 10cussed ( m ( t ;</definiens>
				<definiens id="1">a unique idel~filicf associated wilh a particular t e~ ) : m. , e 'l'hc catu , t~ &lt; : O~ ( C ) . 'l.\ ] Je linguistic category ( ) l '' lhc CXlllCS-. sion : a list of feature value tuplcs. , , The restriction ( It ) . A li~st order one placv l ) redica { ( : dcscribing lhc tenn. • , The quantifier° A gcneralised qum ) litich i.e. a cmdb. nality predicate holding of two properties. , , The n.'fi &lt; re/H. An expression of rcl'erctlCC , either a con-. Stall\ [ O1 ' ; 1 lC , fill index. `` l'cfms may also be variables , indices or constants. 'l\ ] le QI , I , '</definiens>
				<definiens id="2">similar in su uclure Io the term ( : ~o+_ 'm ( ... ) is dclined in \ [ A ( '.921 ) . Combitmtions of the liclds may be uninst~mtiated and it is Ihc instanliafion of lhese meta-variables that is tim cffcct of resolution. In lhe case of leuns , in the R } rwmd dircctiou , lhc quanlilier and rcli~rcnt will be uninslautiatcd ; in file backward direction , file category , reslriction autd quanlilier will be tminstantialed. Tim scmamics of I11¢ ' , QI , I , ' sis presented ill \ [ AC92\ ] arc extended ill tilt* , InilllllOf described. As showll ahoy ( 3 , a 5 : ; I QLF may contain any numt ) er of recta-variables and so tile scm , 'mtics of the language is slighlly different from the traditional one. Instead of a function from models to truth values</definiens>
				<definiens id="3">a partial function \ [ \ [ F\ ] \ ] from models to truth values. This partial fimction is defined by W , a relation between a formula , a model axtd ; t truth value. Tile quality of a monotonic semantics is apparent in the case whcn \ [ \ [ F\ ] \ ] with rcspcct to seine model is undefined , i.e. W ( F , m , 1 ) and W ( F , m , 0 ) . This situation occurs when recta-variables are present in the QLF and consequentially apartial interpretation is being considcrcd. The incremcat~d analysis provides an 'extension ' of \ [ \ [ F\ ] \ ] in \ [ \ [ F'\ ] \ ] whenever F ' is a more resolved version of F. The semantic rules that diseha , ge recta-variables arc of the general fonn : W ( F , m , v ) if W ( U , m , v ) where</definiens>
			</definition>
			<definition id="1">
				<sentence>Modes ponens , and intro and the special case , discharge of terms .</sentence>
				<definiendum id="0">Modes</definiendum>
				<definiens id="0">ponens , and intro and the special case , discharge of terms</definiens>
			</definition>
</paper>

		<paper id="2152">
			<definition id="0">
				<sentence>Thematic inJbrmation is one of the information sources that can bridge the gap between syntactic and semantic processing phases .</sentence>
				<definiendum id="0">Thematic inJbrmation</definiendum>
				<definiens id="0">one of the information sources that can bridge the gap between syntactic and semantic processing phases</definiens>
			</definition>
			<definition id="1">
				<sentence>Take the two-verb-candidates case as an example , let the two verb candidates be vl , v2 , there arc five combination : ( 1 ) only vl is a verb whilc v2 is not , ( 2 ) only v2 is a verb , ( 3 ) both vl aml v2 arc vcrbs , while there is not any subordination relation bclweeu them .</sentence>
				<definiendum id="0">Take the two-verb-candidates case</definiendum>
				<definiens id="0">a verb</definiens>
			</definition>
			<definition id="2">
				<sentence>Combination Geucrator consisls of two submodules : Verb-string Generator and Subordination-relation Tagger .</sentence>
				<definiendum id="0">Combination Geucrator</definiendum>
				<definiens id="0">consisls of two submodules : Verb-string Generator and Subordination-relation Tagger</definiens>
			</definition>
			<definition id="3">
				<sentence>Whenever Combination Filter passes a feasible case into Score Evaluator , the Score Evaluator utilizes a scoring function to compute the score of the input case and then , passes the evaluated score to lhe structure selector .</sentence>
				<definiendum id="0">Score Evaluator</definiendum>
				<definiens id="0">utilizes a scoring function to compute the score of the input case and then , passes the evaluated score to lhe structure selector</definiens>
			</definition>
			<definition id="4">
				<sentence>S-function is defined as in lfigurc 21 , where RWR is the abbreviation of `` Ratio of Words included in some phrase with Roles assigned '' , RRF , `` Ratio of Roles Found '' , OBR , `` OBligatory Role '' , and OPR , `` OPtional Role '' ( Note that OBR and OPR indicate those roles registered in theta grids ' . )</sentence>
				<definiendum id="0">S-function</definiendum>
				<definiendum id="1">RWR</definiendum>
				<definiens id="0">the abbreviation of `` Ratio of Words included in some phrase with Roles assigned '' , RRF , `` Ratio of Roles Found '' , OBR , `` OBligatory Role '' , and OPR , `` OPtional Role '' ( Note that OBR and OPR indicate those roles registered in theta grids '</definiens>
			</definition>
			<definition id="5">
				<sentence>: :i : :i vl : -- =~5~ , v2 : i~f~ vl , v2 vl &gt; v2 1.00 iiiiii ; ~ !</sentence>
				<definiendum id="0">v2</definiendum>
				<definiens id="0">:i vl : -- =~5~ ,</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>A countable noun phrase is defined as follows : I If the head constituent of an NP falls within the scope ofa denumerator it is countable .</sentence>
				<definiendum id="0">countable noun phrase</definiendum>
				<definiens id="0">follows : I If the head constituent of an NP falls within the scope ofa denumerator it is countable</definiens>
			</definition>
			<definition id="1">
				<sentence>They normally have the same number and countability as the noun phrase whose property they are describing .</sentence>
				<definiendum id="0">countability</definiendum>
				<definiens id="0">the noun phrase whose property they are describing</definiens>
			</definition>
</paper>

		<paper id="2192">
			<definition id="0">
				<sentence>&lt; ) 87\ ] is a t ; hcory of int , er-s &lt; ml , eni , ial ( or inter : clausal ) ) 'elationships in a text. All ; hough RS'\ [ ' is int ; ended to serve both as a framework for taxi , analysis and l , ext generation , it has so flu ' been used exclusiw~ly in | ; exl , generation \ [ Hovy et al. , 1992 } \ [ Linden et al. , 1 ( , ) 92\ ] \ [ Ri ; sner and Stede , i992\ ] . Several resea.rehers recognise that RST has de % ors as an analyti &lt; : al frame.work. Moore and Pollack \ [ Moore and Pollack , 19921 , for example , claim that the assmul ) tion of a single relation bel.ween discourse elCIIICtL| , S is Olle &lt; ) \ [ `` I , he reasons why lIST analyses are inherently ambiguous. They also claim that the under : s\ [ ) ecilieil , y of the rhel ; orieal relal , ion delinil , ions causes problems , Our elaim is thai , the main cause ( ) 17 I , he di\ [ liculties of applying liST t ; o t ; exl ; i ) rocessing systems is that SOlile of the relat ; ions are delined on the basis of l ; he elfeets which they have OIL a reader , This is particularly the case for the relations classitied as prcseulatioual rclalion.s , the relal ; ions whose intended etfects are to increase some inclination in a reader. I ) , ackgrotmd relatiolh for examph ; , is defined as a relation whose Satellite increases the al ) ility of a reader tO ( ; ( ) lLll ) reh ( ~lld all elelnelll , in Nucleus and the reader will not tully comprehend Nucleus before reading the t , ext ; of Satellite , This delinition is problematic because there are many ways of increasing the ability of a reader to comprehend Nucleus. More seriously , \ [ , he delinition *Supporte.d by Old Elcctri ( : Industry Co. , btd. itself does not predict anything about textual forlus of Nucleus and Satellite. In order to use RST in actual text processing systems , we have to break down st &lt; ( '.h detinitions to relate them with I , extual forms. In this paper , we show how the defitfitions can be broken down and be ass ( ) eiated wil ; h semaul ; ic consl ; raints betweell eonstituelg , S ( clauses ) , in order t , o relate them with constraints on surface linguisl , ic forms. Among tile 24 rhetorical relations detined in \ [ Mann and Thompson , 1987\ ] , we focus on presentational relations ( 7 relations are elassilied as such ) which are the most prol ) lematic. The re.suits of al ) plying our met ; hod t &lt; ) leading ari ; icles in a aal ) anesc newspal ) er are also discussed. In RST , 24 relations are divided into two groups : pres &lt; ntational relations and subject matter rchdious. According to Mmm and Thomps ( m \ [ Malta and Thompson , 1987\ ] , subject matter relations are those whose intended effect is that the reader recogldses the relal ; ion in question and presentational relations are those whose intended eft'cot is to increase some inclination in the reader. Moore and Pollack \ [ Moore and Pollack , 1992\ ] eotllllletll , i ; hai ; subject matter relations are infof mational and l ) resentat , ional relations are intentional. Table 1 shows what kind of inclination each presentational relation is inI ; ended to increase. One can see that tile detinitions are highly abstract and have not , hing to do with the surface realisations of the relations. On the other hand , it has been observed that there are wu : ious surface cues in texts which are useful \ [ or ideld ; ifying inl ; er-senl , enl ; ia\ [ ( or inter-clausal ) units. Ilalliday and llasan \ [ llalli ( lay and llasan , 1985\ ] identitled a set of linguistic devices for linking one part of a text 1 , o another , such as reJcrcnce , subslihtlion and ellipsis , conjuncliou , and \ [ exical cohesion. From the view point of text processing , these linguist , it devices can be used as cues tor segment.ing a text into structural units ( Satellite and Nucleus ) . However , these cues hardly give any clue about which clause of a unit is Satellite , which clause is Nucleus , and which 1177 Table 1 : RS relation Background 1 , hiablement Motivation l , ; videnee Justify Antithesis Colleessiolt relations and their iuclination type ldnd of inclination ability of R to comprehend an element potential ability to perlbrm action iu N desire to perforln action in N belieJ ' of N readiness to accept writer 's right in N positive regard for situation presented in N positive regard tbr situation presented in N RS relation combines the two clauses into a single unit. For determining these , we have to look for other kinds of surface cues. Because RS relations are delined pragmatically , their ultimate recognition requires understanding of texts which in turn requires detailed knowledge about the world. Furthermore , the condition that the presentational relations are inherently intcutioual , implies that their recognition requires knowledge about the writer 's intention , l ) lans , etc Because this kind of information is implicit in texts , its recognition often causes prohlelns. Ilowew , r , though the writer 's intention is implicit , certain linguistic devices give us chtes to infer it. Modality inforntation in a clause , for example , expresses the writer 's attitude , toward an event/state described , attd therefore , often gives us clues to recoguise a I { S relation. Let us consider the following two examples : \ [ Example 1\ ] ( 1 ) I prepared documents for a meeting. ( 2 ) I sent them to the head ol\ [ ice. \ [ Example 2\ ] ( 1 ) ' 1 am preparing documents tbr a meeting. ( 2 ) ' 1 have to send them to the head office. Though these two examples describe pairs of similar events , the relation between ( 1 ) and ( 2 ) in l~3xample 1 is ( temporal ) Sequence ( a subject matter relation ) because they simply describe two events which happened in sequence. On the other hand , in l~3xample 2 , ( l ) ' describes an event occurring simultaneously with the utterance , and ( 2 ) ' concerns what the writer plans to do. While the two events , preparing documents and sending them , may halH ) 0.n in this sequence , the rela1 ; ion is not regarded as Sequence but as Background. ( 2 ) ' gives the reason why the writer is perfomling the action described by ( l ) '. This change of I { .S relation occurs due to the difference of , nodality of ( 2 ) and ( 2 ) '. Our basic elai , n in that , though they cmmot determine RS relations uniquely , inlbrmation of modality and tense of clauses imposes significant constraints on possible I { S relations , and , being used together with other surface cues like clausal conjunctions , it ; Call reasonably restrict a set of possible discourse structures of texts without resorting to detailed knowledge about the world mid the writer 's plan. IIowever , the contribution of modality and tense to the constraints of RS relations is uot straightforward. Both these granunatical \ [ i~'atures are intertwiued with the propositional content of clauses. There\ [ ore , in of der to formulate the co~ , straints on them properly , we have first to reveal how the intended effects of RS relations can be attained. This leads to our breaking down single RS relations into sets of subschemas , each of which is formulated in terms of the semantic relntionshil ) s between propositional eontents of clauses , their modality and temporal relationshil ) s. Like Mann and Thompson , we use clauses as the basic constituents which are related by RS relations , except that clausal subjects and completneuts and restrictive relative clauses are considered parts of their host clause. The constraints which we formulate for each RS relation are exl ) ressed in terms of propcrtic , s of clauses. In order to express these eonstra.ints tbrreally , we first introduce the l ) asic terms. A clause comprises its Contents and Modality. Modality is the part which expresses the writer 's attitude toward the Contents. While individual languages have their own linguistic devices or grmmnatical forms of modality , what sorts of modality are exl ) ressed by such linguistic devices does not vary from one language to another. For example , although the major linguistic device for nlodality are modal auxiliary verbs both in l , 'mglish anil in Japanese , some kinds of modality expressed in Japanese by modal auxiliary v ( '.rt ) s are expressed by lexical verbs in English , and vice versa. 1 Furtherntore , we find nla , ny phrasal or quasiq ) hrasal expressions which consist of several words , and which collectively express the writer 's attitude toward lhe event/state described. In order to treat them , we adopt a semantics-based view tbr the delinition of Modality. That is , we treat expressions which concern the writer 's attitude as modal expressions , whichew ; r linguistic forms they may take. We \ [ irst establish a classification schema of Modality based oH semantic considerations ( See Section 3.3 ) and then treat all expressions whose functions can be classified under this schema as modal expressions. 1The concepts expressed 1 ) y E ; nglish lexlcM verbs like wish , hope , be ! l , urqe , etc. , for exmnple , ave often expressed by lnodal auxiliaries in .\ ] apanese , when the subject is Ihe writer or speaker. 1778 ( / &lt; intents &lt; ) f a clause is delined as the part which relna.ills aft , el ; r ( : lllOV0 , l of | ; be \ [ llod\ [ ll ex| ) uessiOll. ( ~olltents COll\ [ , aiIl exl ) ressi ( &gt; ns COilcerllillg t.ellSe gild aspect ; , which also cont .</sentence>
				<definiendum id="0">Our elaim</definiendum>
				<definiendum id="1">o t ; exl ; i ) rocessing systems</definiendum>
				<definiendum id="2">be ass ( ) eiated wil ; h semaul ; ic consl ; raints betweell eonstituelg , S (</definiendum>
				<definiendum id="3">Modality. Modality</definiendum>
				<definiendum id="4">aiIl exl ) ressi ( &gt; ns COilcerllillg t.ellSe gild aspect ;</definiendum>
				<definiens id="0">a t ; hcory of int , er-s &lt; ml , eni , ial ( or inter : clausal ) ) 'elationships in a text. All ; hough RS'\ [ ' is int ; ended to serve both as a framework for taxi , analysis and l , ext generation , it has so flu ' been used exclusiw~ly in | ; exl , generation \ [ Hovy et al. , 1992 } \ [ Linden et al. , 1 ( , ) 92\ ] \ [ Ri ; sner and Stede , i992\ ] . Several resea.rehers recognise that RST has de % ors as an analyti &lt; : al frame.work. Moore and Pollack \ [ Moore and Pollack , 19921 , for example , claim that the assmul ) tion of a single relation bel.ween discourse elCIIICtL| , S is Olle &lt; ) \ [ `` I , he reasons why lIST analyses are inherently ambiguous. They also claim that the under : s\ [ ) ecilieil , y of the rhel ; orieal relal , ion delinil , ions causes problems</definiens>
				<definiens id="1">SOlile of the relat ; ions are delined on the basis of l ; he elfeets which they have OIL a reader , This is particularly the case for the relations classitied as prcseulatioual rclalion.s , the relal ; ions whose intended etfects are to increase some inclination in a reader. I ) , ackgrotmd relatiolh for examph ; , is defined as a relation whose Satellite increases the al ) ility of a reader tO</definiens>
				<definiens id="2">problematic because there are many ways of increasing the ability of a reader to comprehend Nucleus. More seriously , \ [ , he delinition *Supporte.d by Old Elcctri ( : Industry Co. , btd. itself does not predict anything about textual forlus of Nucleus and Satellite. In order to use RST in actual text processing systems</definiens>
				<definiens id="3">'.h detinitions to relate them with I</definiens>
				<definiens id="4">clauses ) , in order t , o relate them with constraints on surface linguisl , ic forms. Among tile 24 rhetorical relations detined in \ [ Mann and Thompson , 1987\ ] , we focus on presentational relations ( 7 relations are elassilied as such ) which are the most prol ) lematic. The re.suits of al ) plying our met ; hod t &lt; ) leading ari ; icles in a aal ) anesc newspal ) er are also discussed. In RST , 24 relations are divided into two groups : pres &lt; ntational relations and subject matter rchdious. According to Mmm and Thomps ( m \ [ Malta and Thompson , 1987\ ] , subject matter relations are those whose intended effect is that the reader recogldses the relal ; ion in question and presentational relations are those whose intended eft'cot is to increase some inclination in the reader. Moore and Pollack \ [ Moore and Pollack , 1992\ ] eotllllletll , i ; hai ; subject matter relations are infof mational and l ) resentat , ional relations are intentional. Table 1 shows what kind of inclination each presentational relation is inI ; ended to increase. One can see that tile detinitions are highly abstract and have not , hing to do with the surface realisations of the relations. On the other hand , it has been observed that there are wu : ious surface cues in texts which are useful \ [ or ideld ; ifying inl ; er-senl , enl ; ia\ [ ( or inter-clausal ) units. Ilalliday and llasan \ [ llalli ( lay and llasan , 1985\ ] identitled a set of linguistic devices for linking one part of a text 1 , o another , such as reJcrcnce , subslihtlion and ellipsis , conjuncliou , and \ [ exical cohesion. From the view point of text processing , these linguist , it devices can be used as cues tor segment.ing a text into structural units ( Satellite and Nucleus ) . However , these cues hardly give any clue about which clause of a unit is Satellite , which clause is Nucleus , and which 1177 Table 1 : RS relation Background 1 , hiablement Motivation l , ; videnee Justify Antithesis Colleessiolt relations and their iuclination type ldnd of inclination ability of R to comprehend an element potential ability to perlbrm action iu N desire to perforln action in N belieJ ' of N readiness to accept writer 's right in N positive regard for situation presented in N positive regard tbr situation presented in N RS relation combines the two clauses into a single unit. For determining these , we have to look for other kinds of surface cues. Because RS relations are delined pragmatically , their ultimate recognition requires understanding of texts which in turn requires detailed knowledge about the world. Furthermore , the condition that the presentational relations are inherently intcutioual , implies that their recognition requires knowledge about the writer 's intention , l ) lans , etc Because this kind of information is implicit in texts , its recognition often causes prohlelns. Ilowew , r , though the writer 's intention is implicit , certain linguistic devices give us chtes to infer it. Modality inforntation in a clause , for example , expresses the writer 's attitude , toward an event/state described , attd therefore , often gives us clues to recoguise a I { S relation. Let us consider the following two examples : \ [ Example 1\ ] ( 1 ) I prepared documents for a meeting. ( 2 ) I sent them to the head ol\ [ ice. \ [ Example 2\ ] ( 1 ) ' 1 am preparing documents tbr a meeting. ( 2 ) ' 1 have to send them to the head office. Though these two examples describe pairs of similar events , the relation between ( 1 ) and ( 2 ) in l~3xample 1 is ( temporal ) Sequence ( a subject matter relation ) because they simply describe two events which happened in sequence. On the other hand , in l~3xample 2 , ( l ) ' describes an event occurring simultaneously with the utterance , and ( 2 ) ' concerns what the writer plans to do. While the two events , preparing documents and sending them , may halH ) 0.n in this sequence , the rela1 ; ion is not regarded as Sequence but as Background. ( 2 ) ' gives the reason why the writer is perfomling the action described by ( l ) '. This change of I { .S relation occurs due to the difference of , nodality of ( 2 ) and ( 2 ) '. Our basic elai , n in that , though they cmmot determine RS relations uniquely , inlbrmation of modality and tense of clauses imposes significant constraints on possible I { S relations , and , being used together with other surface cues like clausal conjunctions , it ; Call reasonably restrict a set of possible discourse structures of texts without resorting to detailed knowledge about the world mid the writer 's plan. IIowever , the contribution of modality and tense to the constraints of RS relations is uot straightforward. Both these granunatical \ [ i~'atures are intertwiued with the propositional content of clauses. There\ [ ore , in of der to formulate the co~ , straints on them properly , we have first to reveal how the intended effects of RS relations can be attained. This leads to our breaking down single RS relations into sets of subschemas , each of which is formulated in terms of the semantic relntionshil ) s between propositional eontents of clauses , their modality and temporal relationshil ) s. Like Mann and Thompson</definiens>
				<definiens id="5">the basic constituents which are related by RS relations , except that clausal subjects and completneuts and restrictive relative clauses are considered parts of their host clause. The constraints which we formulate for each RS relation are exl ) ressed in terms of propcrtic , s of clauses. In order to express these eonstra.ints tbrreally</definiens>
				<definiens id="6">the part which expresses the writer 's attitude toward the Contents. While individual languages have their own linguistic devices or grmmnatical forms of modality , what sorts of modality are exl ) ressed by such linguistic devices does not vary from one language to another. For example , although the major linguistic device for nlodality are modal auxiliary verbs both in l , 'mglish anil in Japanese , some kinds of modality expressed in Japanese by modal auxiliary v ( '.rt ) s are expressed by lexical verbs in English , and vice versa. 1 Furtherntore , we find nla , ny phrasal or quasiq ) hrasal expressions which consist of several words , and which collectively express the writer 's attitude toward lhe event/state described. In order to treat them , we adopt a semantics-based view tbr the delinition of Modality. That is , we treat expressions which concern the writer 's attitude as modal expressions , whichew ; r linguistic forms they may take. We \ [ irst establish a classification schema of Modality based oH semantic considerations ( See Section 3.3 ) and then treat all expressions whose functions can be classified under this schema as modal expressions. 1The concepts expressed 1 ) y E ; nglish lexlcM verbs like wish , hope , be ! l , urqe , etc. , for exmnple , ave often expressed by lnodal auxiliaries in .\ ] apanese , when the subject is Ihe writer or speaker. 1778 ( / &lt; intents &lt; ) f a clause is delined as the part which relna.ills aft , el ; r ( : lllOV0 , l of |</definiens>
			</definition>
			<definition id="1">
				<sentence>• Commissive ( M-de ... .. ) Commissive is concerned with an action which a writer commits him/herself to perform or to ellsure that an event takes place .</sentence>
				<definiendum id="0">Commissive</definiendum>
				<definiens id="0">concerned with an action which a writer commits him/herself to perform or to ellsure that an event takes place</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>inguistic theories ( I.FG , HPSG , GB ) , lhe inventory of language particular information left in the lexicon teserves special attention .</sentence>
				<definiendum id="0">inguistic theories</definiendum>
				<definiens id="0">lhe inventory of language particular information left in the lexicon teserves special attention</definiens>
			</definition>
			<definition id="1">
				<sentence>word sense : KAZARU-DECORATE syntactic information : Verb Alternation Type 1 : { \ [ AGENT\ ] NOM ( /Y : ) , \ [ TI IEME\ ] ACC ( ~ ) , \ [ MATERIAL\ ] WlTH ( ~ ) } Verb Alternation Type 2 : { \ [ AGENT\ ] NOM ( /0 : : ) , ITHEME\ ] ACC ( ~ ) , \ [ GOAL\ ] DATNVEI~ .</sentence>
				<definiendum id="0">NOM</definiendum>
				<definiens id="0">KAZARU-DECORATE syntactic information</definiens>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>,7 ) e tic A -C/saF* fAy `` ~aj ~ ' 21 } ( * denotes the mmsitive closure ) can be derived which explicitly states the range of concepts that can actually be related .</sentence>
				<definiendum id="0">-C/saF* fAy</definiendum>
				<definiens id="0">the mmsitive closure ) can be derived which explicitly states the range of concepts that can actually be related</definiens>
			</definition>
			<definition id="1">
				<sentence>The grammatical styecifieation of a loxical entry consists of structural criteria ( valencies ) ~ behavioral descriptions ( protocols ) .</sentence>
				<definiendum id="0">grammatical styecifieation of a loxical entry</definiendum>
				<definiens id="0">consists of structural criteria ( valencies ) ~ behavioral descriptions ( protocols )</definiens>
			</definition>
			<definition id="2">
				<sentence>An ( explicit ) receipt message is a direct message containing a set of actor identities as a parameter .</sentence>
				<definiendum id="0">receipt message</definiendum>
				<definiens id="0">a direct message containing a set of actor identities as a parameter</definiens>
			</definition>
			<definition id="3">
				<sentence>A reception task consists of a set of partial descriptions of the messages that must be received ( implicit as well as explicit ) , and an action to be executed after all receipts have arrived ( usually , sending a message ) .</sentence>
				<definiendum id="0">reception task</definiendum>
				<definiens id="0">consists of a set of partial descriptions of the messages that must be received ( implicit as well as explicit ) , and an action to be executed after all receipts have arrived ( usually , sending a message )</definiens>
			</definition>
			<definition id="4">
				<sentence>The protocol for tx ) ttom-up establishment of det ) endencies consists el three steps : The search for a head ( searchHead ) , the reply of a suitable head to the initiator of tile search ( headFound ) , and tile acceplancc by the initiator ( headAccepted ) , thereby Ix'coming a modifier of the head , The corrcslxmding method dc\ [ initions are given in Table 3 ( note lhal Ihese mcth ( xls are ( lefincd for one actor type here , but tire executed by different actors during parsing ) .</sentence>
				<definiendum id="0">protocol for tx</definiendum>
				<definiendum id="1">headAccepted</definiendum>
				<definiens id="0">el three steps : The search for a head ( searchHead ) , the reply of a suitable head to the initiator of tile search ( headFound ) , and tile acceplancc by the initiator (</definiens>
				<definiens id="1">lefincd for one actor type here , but tire executed by different actors during parsing )</definiens>
			</definition>
			<definition id="5">
				<sentence>The develop , nent of conceptual parsers ( Riesbeck &amp; Schank , 1978 ) , however , was entirely dominated by conceptual expectations driving the parsing process and specifically provided no mechanisms to integrate linguistic knowledge into such a lexical parser in a systematic way .</sentence>
				<definiendum id="0">conceptual parsers</definiendum>
				<definiens id="0">specifically provided no mechanisms to integrate linguistic knowledge into such a lexical parser in a systematic way</definiens>
			</definition>
			<definition id="6">
				<sentence>Cnnnectionist methodology ( cf. a survey by Selman ( 1989 ) of some now classical connectionist natural language parsing systems ) is restricted in two ways compared with object-oriented comlmting .</sentence>
				<definiendum id="0">Cnnnectionist methodology</definiendum>
				<definiens id="0">now classical connectionist natural language parsing systems</definiens>
			</definition>
			<definition id="7">
				<sentence>The parallel expert parser ( I ) FA~ ) : a thoroughly revised descendent of tile wool expert F , arser ( WEP ) .</sentence>
				<definiendum id="0">FA~ )</definiendum>
				<definiens id="0">a thoroughly revised descendent of tile wool expert F</definiens>
			</definition>
			<definition id="8">
				<sentence>SMAIA. , S. &amp; R1EGFA~ , , C. ( 19821 , Parsing and comprehending with s ; 'onl experts ( a theory and its realization ) , ht W. Lchuert &amp; M. Ringle , Eds .</sentence>
				<definiendum id="0">'onl experts</definiendum>
			</definition>
</paper>

		<paper id="1102">
			<definition id="0">
				<sentence>As a matter of fact , a string is a forest with only o.lle level , and a tree is a forest with only one node on the highest level .</sentence>
				<definiendum id="0">string</definiendum>
				<definiens id="0">a forest with only one node on the highest level</definiens>
			</definition>
			<definition id="1">
				<sentence>They are syntactic constituency trees and are exactly projective , which means that each leaf in the tree corresponds to a word in the sentence in the same order .</sentence>
				<definiendum id="0">projective</definiendum>
				<definiens id="0">a word in the sentence in the same order</definiens>
			</definition>
			<definition id="2">
				<sentence>o'Utlmt is a board , built from pieces of the data base boards , and minimising the distance to the input , lC is important to stress the point that the ini ) ut never enters Che data base of board : ; .</sentence>
				<definiendum id="0">o'Utlmt</definiendum>
				<definiendum id="1">lC</definiendum>
				<definiens id="0">a board , built from pieces of the data base boards</definiens>
			</definition>
			<definition id="3">
				<sentence>y , which is more than hi-directionality and self-assesslnent. , independently of il ; s internal knowledge .</sentence>
				<definiendum id="0">y</definiendum>
				<definiens id="0">internal knowledge</definiens>
			</definition>
</paper>

		<paper id="2106">
			<definition id="0">
				<sentence>\ [ Japanese Verb : tsutsumu `` wrap '' l ( l ) NI ( SUBJECTS ) -ga N2 ( CONCRETE OBJECTS or PEOPLE ) -wo N3 ( CLOTHES or PAPERS ) -de IsuIsumu = &gt; N1 wrap N2 in/withN3 Verbal Semantic Attribute : NI 's bodily action ( 2 ) NI ( FIRE , ATMOSI'tlERI~ or AlR ) -ga N2 ( CONCRETE OBJECTS , CULTtJRE or PLACES ) -wo Isulsumu = &gt; Ni envelop N2 Verbal Semantic Attribute : N1 clmnges N2 's attributes ( 3 ) NI ( FOG ) -ga N2 ( CONCRETE OBJECTS or PI , ACES ) -wo ISI~tsIImu = &gt; NI veil N2 Verbal Semanlic Attribute : Natural Plmnomena Fig .</sentence>
				<definiendum id="0">NI ( SUBJECTS</definiendum>
				<definiens id="0">N1 clmnges N2 's attributes ( 3 ) NI ( FOG ) -ga N2 ( CONCRETE OBJECTS or PI</definiens>
			</definition>
			<definition id="1">
				<sentence>`` motsu '' ( to have ) -Possession `` kaihatsusuru '' ( to develop ) -Production The verb `` motsu '' indicates that there is an act of possession within the context .</sentence>
				<definiendum id="0">motsu</definiendum>
				<definiens id="0">an act of possession within the context</definiens>
			</definition>
			<definition id="2">
				<sentence>Fig.4 shows how many transfer patterns were created for each verb in the semantic Relationship between Verbs and Cases -SUBJ exist SUBJ not exist Relation between SUBJ and DIR-OBJ Relation between SUFIJ and IND-OBJ SUBJ cause INI ) -OBJ SUBJ cause I ) IR-OBJ tc , ; iiiiiiiiiiii .</sentence>
				<definiendum id="0">Fig.4</definiendum>
				<definiens id="0">shows how many transfer patterns were created for each verb in the semantic Relationship between Verbs and Cases -SUBJ exist SUBJ not exist Relation between SUBJ and DIR-OBJ Relation between SUFIJ and IND-OBJ SUBJ cause INI ) -OBJ SUBJ cause I ) IR-OBJ tc</definiens>
			</definition>
</paper>

		<paper id="2119">
			<definition id="0">
				<sentence>Wc began by generating triples from the entire corpus and cwdmLt , ing the selectional patterns as &lt; lescribed above ; tile resulth/g recall/l ) recision curve generated by wu'ying the threshold is shown in Figure 1. To see how pattern coverage iwl ) roves with corpus size , we divided our training corpus into 8 segments and coHll/uted sets of tril ) lcs based on the lirst Seglllell| , , the Ih'st two segments , etc. We show iu Figure 2 a plot of recall vs. corpus size , both at ~ consl , ant precision of 72 % and for maximum recall regardless of precision .7 The rate of g ; rowth of the maximum recall cau be understood in teruls of the frequency distribution of triples. In our earlier work \ [ 4\ ] we lit the growth data to curw~s of the form 1 -exp ( -fia : ) , on tile assumpt.ion that all selectional imtterns are t~qually likely. This lttay have 1 ) ee|l a roughly accurate assumption for that app\ ] ication , involving semantic-class based patterns ( rather t , han word-based l ) ; -ttl ; erns ) , and a rather sharply circumscribed sublanguage ( m ( xlical reports ) . For the ( word level ) pal ; i , crlls described here , howevcr , the distribution is quite skewed , with a small number of very-high-frequency l ) atl , erns , a which results in di\ [ : rN , , ( 1 , tta point is shown for 72 % precision for the first seglll ( : ilt &amp; lone } ; e ( : allSe we ~tl'c nl ) \ [ able to re &amp; oh ; t prcci. % lOll of 72~ with a single seglnent. a'l'hc number of highq 're ( luency patterns is m : ( : enl , u ; tted by 745 1000 number 100 of triples with this frequency i0 % % % ** ..-.. I00 f 10 fl'equency of triple in training corpus l '' igure 3 : Distribution of fre ( tuencies of triples in training corpus. Vertical scale shows number of triples with a given frequency. fereat growth curves. Figure 3 plots the number of distinct triples per unit frequency , as a function of fi'equency , for the entire training corpus , This data can be very closely approximated by a fimction of tile form N ( t , ' ) = al ; '-~ , where r~ = 2.9. 9 q'o derive a growth curve for inaxinmln recall , we will assunle that the fl'equeney distribution for triples selected at random follows the same tbrm. Let I ) ( 7 ) represent the probability that a triple chosen at randorn is a particular triple T. l , et P ( p ) be the density of triples with a given probability ; i.e. , the nmnber of triples with probal ) ilities between p and p + ( is eP ( p ) ( for small e ) . Then we are ass , ,ming that P ( p ) = ~p-~ , for p ranging fl'om some minimum probability Pmin to at least one instance of it in n corpus of w triples is approximately i -c -~p ( T ) . The lnaximum recall for a corpus of ~triples is the probability of a given triple ( the `` test triple '' ) being selected at random , multiplied by the probability that that triple was found in the training corpus , summed over a.ll triples : ~ ) ( r ) . ( 1 e-~ '' ( `` ' ) ) 7 ' which can be coral ) uteri using the density function ~ 1 P ' P ( P ) ' ( 1 e- '' V ) dp m , n f l -~ ( 1 c -T~ ' = rap. p . ) alp , ,~ia By selecting an appropriate value of a ( and corresponding l ) , ~i , ~ so that the total probability is 1 ) , we can get a the fact that our lcxicM scmmcr replaces all identitiablc COllllYally lllLllleS by tile token a-company , all C/llTellcy values by acurrency , etc. Many of the highest frequency triples involve such tokens. 9Thls is quite shnilm ' to a Zipf 's law distribution , for which w f'c ( bondlw ) eurobond foray mortgage objective marriage note maturity subsidy veteran commitment debenture activism mile coupon security yield issue ( ) .038 Figure 4 : Nouns closely related to the IIOUII 'q ) ond '' : ranked by t ) : . good match to the actual maximum recall values ; these computed values are shown as x in Figure 2. Except \ [ 'or the smallest data set , the agreement is quite good considering the wwy simple assumpt , ions made. In order to increase our coverage ( recall ) , we then applied the smoothing procedure to the triples fi'om our training corpus. In testing our procedure , we lirst generated the confusioll matrix Pc and examined some of the entries , l '' igure 4 shows the largest entries in f'c for the noun `` bond '' , a common word in the Wall Street Journal. It is clear that ( with some odd exceptions ) most of tile words with high t ) : wtlues are semantically related to the original word. 'lk ) evaluate the etl\~ctiveness of our smoothing procedure , we have plotted recall vs. precision graphs for both unsmoothed and smoothed frequency data. The results are shown in l , 'igure 5. Over tile range of precisions where the two curves overlap , the smoothed data performs better at low precision/high recall , whereas the unsmoothed data is better at high precision/low recall. In addition , smoothing substantially extends the level of recall which can be achieved for a given corpus size , although at some sacrilice in precision. Intuitively we can understand why these curves should ( : ross as they do. Smoothing introduces a certain degree of additional error. As is evident from Figure 4 , some of the confllsion matrix entries arc spurious , arising from such SOllrces as incorrect l ) arses and the conIlation of word senses. In addition , some of the triples being generalized are themselves incorrect ( note that even at high threshold the precision is below 90 % ) . The net result is that a portion ( roughly 1/3 to 1/5 ) of 746 recldl O.3O o ~ ' .+ l , ,ll , + + °~°o o o *~. OaO 0 o o *. o `` t 8 • , • °o o -\ [ ... . I -T-• precision l '' ip ; ure 5 : Benel : its of stnoot , hiug for la.r , gcsl eorl ) us : o -unstn &lt; &gt; othe ( I da .</sentence>
				<definiendum id="0">lnaximum recall</definiendum>
				<definiens id="0">the probability of a given triple ( the `` test triple '' ) being selected at random , multiplied by the probability that that triple was found in the training corpus , summed over a.ll triples</definiens>
			</definition>
</paper>

		<paper id="1101">
			<definition id="0">
				<sentence>text string ( /characters : 21bytes ) V ' ' '' '' 't '' '°r~ ... ...</sentence>
				<definiendum id="0">text string</definiendum>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>A Word style is a group of paragraph and characters format with a name ( e.g. tile title of this section has the style 'Titlel ' which includes tile information about the rendering of this lille ) .</sentence>
				<definiendum id="0">Word style</definiendum>
				<definiens id="0">includes tile information about the rendering of this lille )</definiens>
			</definition>
			<definition id="1">
				<sentence>Phonetic transcriptions These conversions first conccra the prol ) lem of special characters used in some fonts , especially the chqraclers used at USM ( standard macintosh toots , i.e. courier er times ) to approximate the intcmatielml phonetic alphabet ( IPA ) .</sentence>
				<definiendum id="0">USM</definiendum>
				<definiens id="0">the prol ) lem of special characters used in some fonts</definiens>
			</definition>
			<definition id="2">
				<sentence>Ph2 is the format where special characters are replaced by others which appear in qll usual fonts ( characters corresponding with tile letters , the numbers and with the '+ ' , '- ' signs , i.e. 7-bits ASCII ) .</sentence>
				<definiendum id="0">Ph2</definiendum>
				<definiens id="0">the format where special characters are replaced by others which appear in qll usual fonts ( characters corresponding with tile letters</definiens>
			</definition>
</paper>

		<paper id="1066">
			<definition id="0">
				<sentence>The lexical form consists of a canonical representation of the word and a sequence of tags that show the morphological characteristics of the form in question and its syntactic category .</sentence>
				<definiendum id="0">lexical form</definiendum>
				<definiens id="0">consists of a canonical representation of the word and a sequence of tags that show the morphological characteristics of the form in question</definiens>
			</definition>
			<definition id="1">
				<sentence>The Xerox lexicon compiler ( Karttunen 1993 ) is designed to carry out the computations in this optimal way .</sentence>
				<definiendum id="0">Xerox lexicon compiler</definiendum>
				<definiens id="0">designed to carry out the computations in this optimal way</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>So , the word-based metrics compare individual words of the two sentences in terms of their morphological paradigms , synonyms , hyperonyms , hyponyms , antonyms , pos tags ... \ [ Nirenburg 93\ ] or use a semantic distance d ( 0~d &lt; l ) which is determinM by the Most Specific Common Abstraction ( MSCA ) obtained from a thesaurus abstraction hierarchy \ [ Sumita 91\ ] .</sentence>
				<definiendum id="0">word-based metrics</definiendum>
			</definition>
			<definition id="1">
				<sentence>A similarity metric is defined between two such vectors , and is used in both the Learuing and Recognition phases .</sentence>
				<definiendum id="0">similarity metric</definiendum>
			</definition>
			<definition id="2">
				<sentence>The outcome of the DP-algorithm is the similarity score between two vectors which allows for different lengths of the two sentences , similarity of different parts of the two sentences ( last part of one with the first part of the other ) and finally variable number of additions and deletions .</sentence>
				<definiendum id="0">DP-algorithm</definiendum>
				<definiens id="0">allows for different lengths of the two sentences , similarity of different parts of the two sentences ( last part of one with the first part of the other ) and finally variable number of additions and deletions</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>A source pattern ( SP ) is a template to be compared with a source sentence , while a target pattern ( TP ) is used to generate a target sentence .</sentence>
				<definiendum id="0">SP</definiendum>
				<definiens id="0">a template to be compared with a source sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>There are three types of user-defined rt , les available : ( R1 ) Rules for verbs ( R2 ) P , ules for functional phrases ( R3 ) Rules for conjunctional phrases Rule ( R1 ) determines a translation equivalent of a verb based on its case fillers .</sentence>
				<definiendum id="0">functional phrases</definiendum>
			</definition>
</paper>

		<paper id="2132">
			<definition id="0">
				<sentence>This is equivalent to saying that each word representation is a vector of length one in an n-dimensional space , where n is the nmnber of features which are used in the lexicon as a whole .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the nmnber of features which are used in the lexicon as a whole</definiens>
			</definition>
			<definition id="1">
				<sentence>WordNet : A Lexical Database Organised on Psycholinguistie l'rineiples .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="2">
				<sentence>A Conncctionist Machine Tractable Dictionary : q'he Very Idea .</sentence>
				<definiendum id="0">Conncctionist Machine Tractable Dictionary</definiendum>
			</definition>
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>Despite the fact that specific languages show less varmtion and complexity in such rules ( e.g. those characterized by either fixed , or totall 3 , free , WO ) , the vast majority of world languages lie somewhere in-between these two extremes ( e.g. Steele 1981 ) .</sentence>
				<definiendum id="0">WO )</definiendum>
				<definiens id="0">those characterized by either fixed</definiens>
			</definition>
			<definition id="1">
				<sentence>Consider a grammar describing sentences with a reflexive verb and a reflexive particle ( the NP-subject and the adverb being optional ) , responsible for expressions whose English equivalent is e.g. `` ( Ivan ) shaved himself ( yesterday ) '' .</sentence>
				<definiendum id="0">reflexive particle</definiendum>
				<definiens id="0">the NP-subject and the adverb being optional ) , responsible for expressions whose English equivalent</definiens>
			</definition>
			<definition id="2">
				<sentence>( 5 ) a. Brasna ( V\ [ refl\ ] ) shaved se ( Part\ [ refl\ ] ) vcera ( Adv ) himself yesterday b. Vcera ( Adv ) se ( Part\ [ mill ) brasna ( V\ [ refl\ ] ) Yesterday himself shaved ( meaning : ( Someone ) shaved himself yesterday ) LP rules however can not enforce exactly these orderings because the CFG , corresponding to ( Sa-b ) , viz .</sentence>
				<definiendum id="0">] ) shaved se</definiendum>
			</definition>
			<definition id="3">
				<sentence>EFOG ( Extended Flexible word Order Grammar ) extends the expressive power of the ID/LP format .</sentence>
				<definiendum id="0">EFOG</definiendum>
				<definiens id="0">Flexible word Order Grammar ) extends the expressive power of the ID/LP format</definiens>
			</definition>
			<definition id="4">
				<sentence>The following atomic we constraints have been defined : Precedence constraints : • precedes ( e.g. a &lt; b ) • immediately precedes ( a &lt; &lt; b ) ( we also maintain the notation , &gt; and &gt; &gt; , for ( immediately ) follows ; see commenta W below ) Adjacency constraints : • is adjacent ( a &lt; &gt; b ) Position conxtraints : • is positioned first/last ( e. g. first ( a , Node ) , where Node is a node ; e.g. first ( a , s ) designates that a is sentence-initial .</sentence>
				<definiendum id="0">Node</definiendum>
				<definiens id="0">a node</definiens>
			</definition>
			<definition id="5">
				<sentence>Bulgarian clitics fall into different categories : ( 1 ) nominals ( short accusative pronouns : me `` me '' , te `` you '' , etc. ; short dative prononns : mi `` to me '' , ti `` to you '' , etc. ) ; ( 2 ) verbs ( the present tense forms of `` to be '' sam `` am '' , si `` ( you ) are '' , etc. ) ; ( 3 ) adjectives ( short possessive pronouns : mi `` my '' , ti `` your '' , etc. ; short ml\ ] exive pronoun : si `` one 's own '' ) ; and ( 4 ) particles ( inten ; ogative li `` do '' , reflexive se `` myself/yourself .</sentence>
				<definiendum id="0">Bulgarian clitics</definiendum>
				<definiens id="0">short accusative pronouns : me `` me ''</definiens>
			</definition>
</paper>

		<paper id="1100">
			<definition id="0">
				<sentence>( 2 ) lexicon-based word nonnldizntion : e.g. , retrieval is reduced to retrieve ; ( 3 ) operator-argument representation of phr'tses : e.g. , information retrieval , retrievhlg of information , and retrieve relewmt information , are , all assigned the slune representation , retrieve+btformation ; ( 4 ) conlext-blmed term clustering into synonymy classes and subsumption hierarchies : e.g. , takeover is a kind of acquisition ( in business ) , luld Fortran is a programming language .</sentence>
				<definiendum id="0">takeover</definiendum>
				<definiendum id="1">luld Fortran</definiendum>
				<definiens id="0">information retrieval , retrievhlg of information , and retrieve relewmt information</definiens>
				<definiens id="1">a kind of acquisition ( in business ) ,</definiens>
				<definiens id="2">a programming language</definiens>
			</definition>
			<definition id="1">
				<sentence>MAX ( W ( \ [ x , att \ ] ) , W ( \ [ y , att \ ] ) ~ltt with W ( \ [ x , y 1 ) = aEW ( x ) *tog ( f. , a ) GEW ( x ) =I+ ny v ~nyj| tog ( N ) 1 In rite above , f~ , y stands for absolute fi'equency of pair \ [ x , y\ ] in tile corpus , ny is the frequency of term y , and N is ttte number of single-word terms .</sentence>
				<definiendum id="0">MAX ( W</definiendum>
				<definiendum id="1">ny</definiendum>
				<definiendum id="2">N</definiendum>
				<definiens id="0">the frequency of term y , and</definiens>
				<definiens id="1">ttte number of single-word terms</definiens>
			</definition>
</paper>

		<paper id="2142">
			<definition id="0">
				<sentence>+ in th+~ 5 ; V , 'udy of L &lt; m,9uage ( eds. J. Groenendijl ( , J. , la , n , ss ( ; n &amp; M , S1 ; okho\ [ ) : IJ'oris l ) ulilications , \ ] ) ( u ' ( Irechl , : 27'7 372. t ( ri\ [ l , :a , M. ( I , ( ) 93 ) : I '' ocil , % Pre , ~il pposil ; ion a , n ( t l ) yna , iiliC Ititc~i'l ) rel ; ~lJ , iOli , .Jo'al'l~ ( l , \ [ of D'emanlic , s \ ] 0. l~ , a , uisa , y A.M. ( 1 ! ) 92 : a , ) : lla , r ( ! \ ] ) lu : i : a , I Nl ) s a , n ( l Ila , l ) itua.I Vl ) s , ( 'OI , ING-92 , Na , l\ ] tes : 226-231. Ranlsay A.M. ( 1992b ) : \ ] ) resuppositjolls a.ud W l I-clauses , , /ourltal of , ~'~nzanlics 9:251 186. ll.a.uisay A.M. ( 1 { ) . ( ) ; I ) : Asp+cl mul AkZim~ , ~arl I , ViU~ , o~I Co ( reims , slll ) uiitt ( , d I , o A C , \ [ , 9,1. l { ; , i , iiisay A.M. ( 19 ! ) 4 ) : Mea~ninKs a , s ( ~ , oiisl , railll , S on Infl ) rina , I ; i ( m SLa , lJes , in ( ) 'ol~ &lt; ~lraild , , % l , alz ( .lltCl.fJe alzd C'om , p'ul , ation ( eds. C. , J. Rut ) p , M.A. l { osiler &amp; 11.1~. , lohJison ) : A ( ; .~l , ( lerni ( ; Press , \ ] , Oil ( ion : 249 2 ? ( 7. 'l'urii ( ! r I~ , . ( 1.987 ) : A 'l : heory of I'roperl ; ies , Journal c U '' ,6'ymbo/ic I , ogic 52 ( 2 ) : d,55 /172. Vesl , re I ' ; . ( 1991 ) : Ali Algorit , lml for Geuera , i , iug N ( ) ii-r ( ! dunci , ~uil , Qua , nl , iJier gco\ ] ) in~ ; s &gt; I , 'iJTh Co~ @ ~ ' , ~n &lt; : c of the Iqlu'opr'a'l~ ( ; 'tm .</sentence>
				<definiendum id="0">~lJ , iOli , .Jo'al'l~</definiendum>
				<definiendum id="1">I )</definiendum>
				<definiendum id="2">o~I Co</definiendum>
				<definiens id="0">a , n ( l Ila , l ) itua.I Vl ) s , ( 'OI , ING-92</definiens>
				<definiens id="1">Mea~ninKs a , s ( ~ , oiisl , railll</definiens>
			</definition>
</paper>

		<paper id="2184">
			<definition id="0">
				<sentence>AF DF distinction Although taking Sidner 's algorithms as a starting point , Cormack renounces the distinction between actor focus and discourse focus , in the final part of her work .</sentence>
				<definiendum id="0">Cormack</definiendum>
				<definiens id="0">renounces the distinction between actor focus and discourse focus , in the final part of her work</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>E.g. , if the user wishes to delete a GM-element which forms pazt of some of the TIcombinations present in the database , VOCOPS lists these with a warning that they will also be deleted .</sentence>
				<definiendum id="0">E.g.</definiendum>
				<definiendum id="1">VOCOPS</definiendum>
				<definiens id="0">if the user wishes to delete a GM-element which forms pazt of some of the TIcombinations present in the database</definiens>
			</definition>
</paper>

		<paper id="2162">
</paper>

		<paper id="2117">
</paper>

		<paper id="2141">
			<definition id="0">
				<sentence>q. Tim \ [ irs ( ; sl ; ( ; p in I ; ho , prel ) ro ( : essing stage of several ( ; omi/ilal ; ion algorithms requires ( ; h ( ; sohltion of ( ; wo \ [ 'un ( ' , tions normally ( : al\ ] e ( l FII { , ST and F ( ) \ ] AA ) W. Intui ( ; iv ( ~ly , lrIILS'T ( X ) gives us ( ; h ( ; ( ; ( ' , rmina , l symbols ( ; ha ( ; may app ( ; nr in hfil , ial posi ( ; iou in substrings de , riv ( ; ( I froiD , c ; L| ; ( ~gory X. FOLLOW ( X ) gives us ( ; Ira ( ; c'rminals whi ( : h may imm ( ; dia , t ( ; ly follow a su| ) string o1 ' ( : ~ ( ; e , gory X. For ( , ~xami ) lc , in l ; h ( ; gra , mmar S &gt; NI ' VP ; NP &gt; ( lel ; II ( tltlI ; VP -- } vtra NP , wc go ( ; : l~'H~sr ( s ' ) v'lies'-r ( N/ , ) - : ( J , c~ , } , v~llc , s'~r ( yl , ) { , , , ~ , , , .</sentence>
				<definiendum id="0">ST</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">~gory X. FOLLOW ( X ) gives us ( ; Ira ( ; c'rminals whi ( : h may imm</definiens>
			</definition>
			<definition id="1">
				<sentence>Betbre describing the algorithm , we give a well known procedure for coinputing FIRST for CF grammars ( taken from ( Aho et al. , 1986 ) :1189 , where e is the empty string ) : `` rio conlpute FIRST ( X ) for all grammar symbols X , apply the following rules until no more terminals or e can be added to ally FIRST set : .</sentence>
				<definiendum id="0">e</definiendum>
				<definiens id="0">the empty string</definiens>
			</definition>
			<definition id="2">
				<sentence>pairs ( X ' , { - ) or ( X\ [ ... X.\ [ ~ , ¢ ) should have bindings between their two elements ; this creates the problem of deciding which of the cs in the FIRST pairs to use , since it ; is possit ) le in principle that each of these will have a difl : erent value for ( .</sentence>
				<definiendum id="0">pairs</definiendum>
				<definiens id="0">have bindings between their two elements ; this creates the problem of deciding which of the cs in the FIRST pairs to use</definiens>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>Example ( A ) : Brother takes the medicine in the morning .</sentence>
				<definiendum id="0">Example</definiendum>
				<definiens id="0">A ) : Brother takes the medicine in the morning</definiens>
			</definition>
			<definition id="1">
				<sentence>The similarity between concepts used in the above similarity is defined as follows ; Sc ( Cl , C2 ) = the ~lumber of common ancesters the number of ancesters of CI + the number of ancesters of C2 Ilere , ancestors until three layers above are used .</sentence>
				<definiendum id="0">similarity between concepts</definiendum>
				<definiendum id="1">Sc</definiendum>
				<definiens id="0">the ~lumber of common ancesters the number of ancesters of CI + the number of ancesters of C2 Ilere , ancestors until three layers above are used</definiens>
			</definition>
			<definition id="2">
				<sentence>get ( EVl9 tak\ [ E ) ( F.V F+ ) ( others are omitted ) Ext~riment 2 ( 3ceae3 ) ) get ( EVE ) grew ( EVE ) fall ( EVE ) ( others are omitted ) Experiment 3 ( Ofdc5f ) ) accept ( EVE ) acknowledgie } ( EVF . )</sentence>
				<definiendum id="0">EVE</definiendum>
				<definiens id="0">) fall ( EVE ) ( others are omitted</definiens>
			</definition>
</paper>

		<paper id="2175">
			<definition id="0">
				<sentence>`` Statistical estim~ttioa '' at text ; level indicates that lengtl &gt; based statistical techniques arc applied if necessary .</sentence>
				<definiendum id="0">level</definiendum>
				<definiens id="0">lengtl &gt; based statistical techniques arc applied if necessary</definiens>
			</definition>
			<definition id="1">
				<sentence>I077 Problem Ill this section , we formally define the problem of bilingual sentence Mignment ) Let S be a text of n sentences of a language , and T be a text of m sentences of another language and suppose that S and T are translation of each other : -- __ Sl , S2~ ... ~S n T = tl , t2 , ... , tm Let p be a pair of minimal corresponding segments in texts S and T. Suppose that p consists of x sentences sa-~+l , • • • , Sa in S and y sentences t~l , .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">the problem of bilingual sentence Mignment ) Let S be a text of n sentences of a language</definiens>
			</definition>
			<definition id="2">
				<sentence>Then , bilingual sentence alignment problem can be defined as an optimization problem that finds a sequence P of sentence beads which optimizes the total score H of the sequence P : H ( P ) = IIh ( h ( pl ) ... .. h ( pk ) ) ing Word Correspondence Information In this section , we describe the specification of our sentence alignment method based-on word correspondence information .</sentence>
				<definiendum id="0">bilingual sentence alignment problem</definiendum>
				<definiens id="0">an optimization problem that finds a sequence P of sentence beads which optimizes the total score H of the sequence P : H ( P ) = IIh ( h ( pl ) ... .. h ( pk</definiens>
				<definiens id="1">the specification of our sentence alignment method based-on word correspondence information</definiens>
			</definition>
			<definition id="3">
				<sentence>First , supposing p= ( a , x ; b , y ) , and let n~ ( a , x ) and nt ( b , y ) be the numbers of content words in the sequences of sentences s~4 , ... , s , and t~v~ , , ... , tb respectively , and n~t ( p ) be the nunlber of corresponding word pairs in p. Then , the score h ofp is defined as the ratio of n~t ( p ) to the sum of n~ ( a , x ) and nt ( b , y ) : t , , ( p ) ~ , ( p ) Let Pi be the sequence of sentence beads fi'om the begbming of the bilingual text up to the bead pi : Pi = pa , p2 , ... , pi Then , we assume that the score H ( Pi ) of Pi follows the recursion equation below : H ( Pi ) = ll ( Pi-l ) + h ( pl ) ( 1 ) Let Hm ( ai , bl ) be the maximum score of aligning a part of S ( from the beginning up to the ai ( =ai_l+xi ) th sentence ) and a part of T ( f , 'om the beginning up to bi ( =bi-l+yi ) th sentence ) .</sentence>
				<definiendum id="0">First , supposing p=</definiendum>
				<definiendum id="1">let n~ ( a</definiendum>
				<definiendum id="2">nt</definiendum>
				<definiendum id="3">y )</definiendum>
				<definiendum id="4">p ) Let Pi</definiendum>
				<definiens id="0">the numbers of content words in the sequences of sentences s~4 , ... , s , and t~v~ , , ... , tb respectively , and n~t ( p ) be the nunlber of corresponding word pairs in p. Then</definiens>
				<definiens id="1">the sequence of sentence beads fi'om the begbming of the bilingual text up to the bead pi : Pi = pa , p2 , ... , pi Then , we assume that the score H ( Pi ) of Pi follows the recursion equation below : H ( Pi ) = ll ( Pi-l ) + h ( pl ) ( 1 ) Let Hm ( ai , bl ) be the maximum score of aligning a part of S ( from the beginning up to the ai ( =ai_l+xi ) th sentence ) and a part of T ( f</definiens>
			</definition>
			<definition id="4">
				<sentence>The BICORD System : Combining lexical information from bilingual corpora and machine readable dictionaries , Proceedings of the 13th COLING , Vol .</sentence>
				<definiendum id="0">BICORD System</definiendum>
				<definiens id="0">Combining lexical information from bilingual corpora and machine readable dictionaries</definiens>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>For example , tile omission of the goal argnment for a verb such as drive , push or carmj induces an atelic ( process ) interpretation as indicated by incompatibility with a terminative adverbial : ( 6 ) a. , \ ] ohn drove his car to London in one hour b. John drove his ear ( *in one hour ) Within a ( partial ) deeomposltional approach to verb semantics ( Tahny , 1985 ; Jackendolr , 1990 ; Sanfilippo , 1993 ; Sanlilil ) po el al. , 1992 ) ) , this contrast can be explained with reference to the rneaniug component path .</sentence>
				<definiendum id="0">atelic</definiendum>
				<definiens id="0">*in one hour ) Within a ( partial ) deeomposltional approach to verb semantics ( Tahny , 1985</definiens>
			</definition>
			<definition id="1">
				<sentence>Assuming that , the compositional meaning of tile sentence involves establishing a homomorphism between tile event described by the verb and the path along which such an event takes place ( l ) owty , 1991 ; Sanlilippo , 1991 ) , it follows that with an unbounded path ( e.g. ( 6b ) ) only a process interpretation is possible , whereas with a bounded path ( e.g. ( 6a ) ) a relic interpretation is more likely .</sentence>
				<definiendum id="0">tile sentence</definiendum>
				<definiens id="0">involves establishing a homomorphism between tile event described by the verb and the path along which such an event takes place ( l ) owty , 1991 ; Sanlilippo , 1991 ) , it follows that with an unbounded path</definiens>
			</definition>
</paper>

		<paper id="2140">
			<definition id="0">
				<sentence>Wh ( , u ( +v ( +r a , liltKui , -it ; h : t ) b. ject i , &lt; + r ( &gt; ( : o &lt; c , ; idz , '.+ , rl , it , is thr ( ) wl/ i.tu t , he , soh+l , i+m &lt; : Z ( JhAM , att ( I a ( ' , t , '-i a , , , + a , ? no\ [ could. \ : ( ~rl ) , ,-+ a , lld H ( ) IIlO other ~+LtlXil\ [ +l.l'y VOI'i ) ,9 itlt ; l+O ( tllC ( + % 7121712\ [ ) 7 '' lLI : C , '+'. : l'hese melnl ) ra , tm+ l : ..~ ( : onles ( ; heir '~ ( : ( ) pe , ~+ \ [ ' ( : ,r ( : a++e ( or role ) ( Mlui , a , tic ) u ; nattMy , ca , oh verl ) , '-,0ar ( : tl ( &gt; , &lt; + for + inole ( : ule,4 ( u ( &gt; utJ i ) hr++se , s ) LhaJ ; are nec ( + .</sentence>
				<definiendum id="0">Wh</definiendum>
				<definiendum id="1">; heir '~</definiendum>
				<definiens id="0">u ( &gt; utJ i ) hr++se , s ) LhaJ</definiens>
			</definition>
</paper>

		<paper id="2198">
			<definition id="0">
				<sentence>The Chinese language has more than 10,000 character categories .</sentence>
				<definiendum id="0">Chinese language</definiendum>
			</definition>
			<definition id="1">
				<sentence>Let T= Wl , W2 , ... , wL be a text corpus with L words ; V = vl , v~ , ... , VNv be the vocabulary composed of the NV distinct words in T ; and C = C1 , C2 , ... , CNc be the set of classes , where NC is a predefined number of classes .</sentence>
				<definiendum id="0">T= Wl , W2 , ... , wL</definiendum>
				<definiendum id="1">C2 , ... , CNc</definiendum>
				<definiendum id="2">NC</definiendum>
				<definiens id="0">a text corpus with L words ; V = vl , v~ , ... , VNv be the vocabulary composed of the NV distinct words in T</definiens>
				<definiens id="1">a predefined number of classes</definiens>
			</definition>
			<definition id="2">
				<sentence>Perplexity , PP , is a well-known quality metric for language models in speech recognition : PP = /5 ( T ) -~ .</sentence>
				<definiendum id="0">Perplexity</definiendum>
				<definiens id="0">a well-known quality metric for language models in speech recognition : PP = /5 ( T ) -~</definiens>
			</definition>
			<definition id="3">
				<sentence>The new configuration is accepted if cxp ( APP/cp ) is greater than a random number between 0 and 1 , where APP is the difference of perplexities for two consecutive steps .</sentence>
				<definiendum id="0">APP</definiendum>
				<definiens id="0">the difference of perplexities for two consecutive steps</definiens>
			</definition>
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>A4 is a DAG that represents a finite set £'et ( A4 ) of strings , llere , this set only contains one elmnent , namely £'et ( A4 ) = { ( J ohn ) N O ( said ) V O ( t hat ( M a , 'y ) N O ( le f t. ) V O ) That , ~ ' } .</sentence>
				<definiendum id="0">A4</definiendum>
				<definiens id="0">a DAG that represents a finite set £'et ( A4 ) of strings</definiens>
				<definiens id="1">J ohn ) N O ( said ) V O ( t hat ( M a</definiens>
			</definition>
			<definition id="1">
				<sentence>FORMAL DESCRIPTION The algorithm Formally , a transducer T is defined by a 6-uplet ( A , Q , i , F , d , 6 ) where A is a finite all ) habet , Q is a finite set of states , i G Q is the initial state , F C Q is the set o\ [ `` t , ermina\ [ states , d the transition ftmcl .</sentence>
				<definiendum id="0">6-uplet</definiendum>
				<definiendum id="1">Q</definiendum>
				<definiendum id="2">F C Q</definiendum>
				<definiens id="0">a finite set of states , i G Q is the initial state</definiens>
				<definiens id="1">the set o\ [ `` t , ermina\ [ states , d the transition ftmcl</definiens>
			</definition>
			<definition id="2">
				<sentence>ion maps ( ~ ) × A to the set ofsuhsets of Q and ~5 the etnission function nmps Q x A x Q to A. The core of the procedure consists in apl ) lying a transducer to a FSA , the algorithm is well known , we give it here for the sake of readability .</sentence>
				<definiendum id="0">ion maps</definiendum>
				<definiendum id="1">FSA</definiendum>
				<definiens id="0">the set ofsuhsets of Q and ~5 the etnission function nmps Q x A x Q to A. The core of the procedure consists in apl ) lying a transducer to a</definiens>
			</definition>
			<definition id="3">
				<sentence>/2 ) I.h &lt; m 9 e = p ; lO , m~ , ,V\ [ , ' = , * + +\ ] = ( : /t , : , : . , ) ; II add e to d ( q , Sl ( xt , s , a:2 ) ) ; 12 q-l-4- ; 13 } while ( q &lt; n ) ; 1,11'I/.UNE ( A ) ; ( t , his line is ol ) tional ) The a , nalysis algorithm is then the following olin : ANAIA : SE_ICA , T ) 1 fin = NO ; 2 while fin ¢ Yl'2 , q ' do 3 fin = Appl ! /l ' ? 'ansduccr ( A , `` 1 ' , A ) ; Transducers v.s. Context Free Grammars It should be pointed out that , given a ( } ontext-Free ( ; ranlmar , it is alw { tys fmssible to buihl a transducer such that this method applies , h , other words , any c &lt; ml , eXt { 'reo il. , ~l ' ; I.iil|llD.r C ; lll I ) ( ~ ( , rltllsl ; t ( , ed illtO &amp; tl'~tllSdl , cer such thai , the illgorithill pltr ; te the \ [ Illlgllli.g , , de.. scribed by this graimu ; tr. Moreover , |.he olmration that ti'ailSI'orIltS ; t ( ~l '' ( l into its related t.ransdttcer is itself a v ' ; ~ , thmal tra.nsdt , ction. Although i.hi : - : ca , tool , I ) , ' d , w , Aopped here dlle I.o the \ [ ~tck of place , this resnlL colnes naturally when looking +~t. the example of section 3.1. Moreover the met , hod has trmch more expressive power t , h ; m ( 'F ( ; , in fact computing a fixed point of a , r ; +t , ionM traxlsdtlc.t ; ion has the sarne power as applying ; t 'l'uring Machine ( althottghl , ( ; here might , nol , be. any practical interest for that ) . TItE SECOND ALGORITHM : A DETERMINISTIC DEVICE ( liven ; t transducer representing the ~l ' &amp; ll\ ] |l/\ [ tr \ [ . } lore 3A'O tWO dilferenl , ways of ol ) t.ahiing new I ) m'sing I ) rogra.llls. The lil'sl , solution is to buihl a transducer 'F ' equivalent to : I ' from the view point of their Iixed points , 7 ' ~Ji= , ,d-poi , ,t 7 '' . Namely 7 ' ~/i. : ~a-poi , . 7 '' ill '' for each * e A* , V ' ( * ) = * &lt; * V '' ( ~ , ) = , ,. l '' o , ' il , ~ta , ,ee , if 7 ' is such that for each x G A* , T n ( a : ) converges then T 2 ~\ ] i~ed-point r. The second approach is to try using a different representation of T or to apply it differently. In this section , we shall give an algorithm illustrating this second al~l~roaeh. The basic idea is to transform tile finite-state transducer into a deterministic device called bimaehine \ [ 1\ ] . We will detail that latter but , basically , a bimaehine stands for a left sequential fimetion ( i.e deterministic from left to righQ composed to a right sequential function ( i.e. deterministic from right to left ) . Such a decomposition always exists. The interest of this concept appears when one looks at how tile algorithm ApplyTransdueer performs. In fact the output DAG of this algorithm has a lot of states that lead to nothing , i.e. states that are not eoaceessible , thus tile PR , UNE function ( called on live 14 of the ApplyTransducer function ) has to remove most of the states ( around 90 % in our parser of French ) . Let us for instance consider tile following example : SUl ) l ) ose the transducer 7 ; is tile one represented ligure 2 and that we want to compute 7 : , ( A ) where A is the DAG giwm \ [ igure 2. a'b c : d . dC : e e : l tu a c .q % X Figure 2 : left : initial transducer ; 7-ighl : initial DAG Following the algorithm described in ApplyTransducer up to line 14 exelnded provides the I ) AG A ' of tigure 3. A ' 1 d A tl Figure 3 : left : before pruning ; right : after i ) runing Tile PRUNE flmction has then to remove 3 of tile six states to give tile DA ( -I A '' of figure 3 A way to avoid the overhead of computing unnecessary states is to ilrst ~q ) ply a left sequential transducer 71 , , , ( that is a transducer deterministic in term of its input when read from left to right ) given figtire 4 and then apply a right sequential transducer :1 ' , ~ ( i.e. deterministic in term of its input when read from right to left ) given figure 4. We shall call the pair B , = ( T , , , , 7'a~ ) the bimaehine flmctionally equivalent to 7a , i.e. Ba ~function ~/\ ] ~. With the same input A we first obtain Aa = 7~a ( A ) of figure 5 and then Ab = A '' = , 'e~ , '.~. ( V : ,b ( , '~ , e , '~4 A , , ) ) ) -- - : ~ ' ( A ) = r~ , , ( A ) . c : d c : c a : b~o* '' -- '' , o~I 3 : q °.~ a ' % gT.. , a/ 7 : ,. 7 ; . , Figure 4 : left : left sequential function ; right : right sequential function a c g Figu , 'e 5 : A. It should be pointed oul , that both 7'.. and T.b are deterministic in term of their input , i.e.t.heir left , labels , which was not the ease to : l'a , Just like for FSA , the fact that it is deterministic implies that it , can l ) e applied faster ( and sometime much faster ) than nondeternlinistic devices , on the other hand the size of the bimachine might be , in the worst case , exponential ill term of the original tralls ( nleer , q'he following algorithm formalizes the analysis by mean of a bimaehine 7. ANAI , YSE_2 ( A , ,9 = ( 'Fi , 7:2 ) ) 1 fin = NO ; 2 while fin ~ YES do { 3 fin = ApplyT'ransdueer ( A , : l'1 , A ) ; 4 if finT~ YI , ' , S ' { 5 reverse ( A ) ; 7 reverse ( A ) ; s } 9 } IMPLEMENTATION AND RESULTS The main motivation for this work eo , nes from the linguistic claim that the syntactic rules , roughly the sentence structures , are mostly lexieal. The gralnmar of Freueh we lind at our disposal was so large that noue of the awdlable parsers could handle it. Although the inq ) lement.ed l ) art of the gramnlar is still inc ( mll ) \ [ el.e , it ah'ea ( ly describes 2,878 sentential verbs ( coming from \ [ 6\ ] ) , I.Imt is verl ) s tlutt can l.ake a sentence as argument , leading to 2 ( 11,722 lexieal rulesS ; 1,359 intransitiw , w.~rbs \ [ 2\ ] leading to 3,153 lexical rules ; 2,109 transitive verbs \ [ 3\ ] leading to 9,785 lexical rules ; 2,920 frozen expression ( coming from \ [ 7\ ] ) leading to 9,342 lexieal rules and 1,213 partly frozen adwwbials leading to 5,032 lexieal rules. Thus , t.he grammar describes 10,479 entries aud 229,035 lexieal rnles. This `` : 'l~he FSA reverse ( A ) is A where the transitions have been reversed and tile initial and Ihlal st~ttes exclumged. ~For a verb like ( former tile set o\ [ `` rules inchlde Nhu'mo : lo , me , '' Nhum~ as well as Nhumo avoi ; dto , md Nhum~ , N humo ~t , 'e ~ : tonn : pa , '' N hum , or N humo s 'dlo , me aupr~s de Nhuml de ee Qut~2 which gives an idea of how these complexe verbs generate ~ttl average of 10O rules , or sentence structures , even if no embbeding is iuvolved art this stage. 434 grammar is reprcsenLed by nne tA'~tilsdtlcer ( , ~ '' 13,408 states and d7,119 transitions stored ill { ) ( ) &lt; ~1 ( 1~ , The following illp/ll , ; Jean est ; a.gacd l ) ar le fail : que son anil , darts la ( : rain~ ( : ( t'i '' .Lre lmnl l ) ar S ( } S | ) iU'O,1It ; S~ ll ( ~ |OlII '' aii ; 1 ) as IiV ( llI ( ! S ( ~S IIIlIIIVIliS ( ~S llOt ; ( } S. is parsed in the fi ) llowing way in 0.95s s wiih a l/rogram inqflementing the Mgorithm ANALYSE_I. ( N Jean ) N esL &amp; VpI ) 0 aga : 'd par hLhdt : _QuP le filit : ( QuP qne ( N sml II alnl IlIlll ) N &gt; ( ADV darts llt Cl'I/illt ; l~ th !</sentence>
				<definiendum id="0">lll I )</definiendum>
				<definiendum id="1">ion</definiendum>
				<definiendum id="2">DETERMINISTIC DEVICE</definiendum>
				<definiendum id="3">UNE function</definiendum>
				<definiens id="0">gives an idea of how these complexe verbs generate ~ttl average of 10O rules , or sentence structures</definiens>
			</definition>
</paper>

		<paper id="1073">
			<definition id="0">
				<sentence>In the top-down phase , SEP uses the parser 's existing knowledge to perform top-down prediction of missing knowledge .</sentence>
				<definiendum id="0">SEP</definiendum>
				<definiens id="0">uses the parser 's existing knowledge to perform top-down prediction of missing knowledge</definiens>
			</definition>
			<definition id="1">
				<sentence>Thin phase is for the case in which the parser has knowledge for constructing all the individual components ( e.g. an NP and a VP ) of a constituent ( e.g. Sinai ) excepts the knowledge for grouping these components ( e.g. Smaj may be constructed by the NP followed by the VP ) .</sentence>
				<definiendum id="0">Thin phase</definiendum>
			</definition>
			<definition id="2">
				<sentence>If there is only one hypothesis in any one of the hypothesis sets , SEP returns the hypothesis as the tar443 get missing knowledge ( Step 1 ) .</sentence>
				<definiendum id="0">SEP</definiendum>
			</definition>
			<definition id="3">
				<sentence>After considering all the hypothesis sets , SEP returns the hypothesis with the highest frequency of occurrence ( Step 3 ) .</sentence>
				<definiendum id="0">SEP</definiendum>
			</definition>
			<definition id="4">
				<sentence>An example of the hypotheses generated by SEP As an example , consider the sentence `` Lead is a soft metal that serves rnany purposes in home '' in the corpus .</sentence>
				<definiendum id="0">Lead</definiendum>
				<definiens id="0">a soft metal that serves rnany purposes in home '' in the corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>For verifying the hypotheses generated for parsing the above sentencc `` Lead is a soft metal that serves many purposes in home '' ( Fig.4 ) , 1000 sentences containing `` that '' were extracted from 19200 sentences ( 71294 words ) of the DJ corpus .</sentence>
				<definiendum id="0">Lead</definiendum>
			</definition>
			<definition id="6">
				<sentence>SEP collects observations from the parser which may grow through learning .</sentence>
				<definiendum id="0">SEP</definiendum>
			</definition>
</paper>

		<paper id="2107">
			<definition id="0">
				<sentence>I. &lt; Since lla.nal~.o I } cll~wed like feeling ( : old , I closed the window. ' clos ( ; d tim window. ' If contextua.lly wc ( : an I.ake only ltalmko and tit ( ; Sl ) ( ; aker of this senteHce as candidates of anl.ccedent of ( / ) l &lt; subj eli '' { i ) '2 , ~ubj , inl.uit.ively the following two inI.crl ) re ( .ati ( } ns ; it ' ( ; equally likely. a. ¢ i , ,o , i I lan ; Lk ( ) , q52.~ubj : : Sl ) Cakcr h. dJt , ,~ , ; speak0r , ( /JS~ubj : II ; uiako 'l'hercfore ( llisub j &amp; lid { //Ssub j ~/ , l'C hotli \ [ ) re. |11 fa ( ; l. this I'act is well klloWil lHlll ) llg~ , I zq ) a.ncso \ ] ingllisl.s , i.e. \ [ Sells 85 , 'Pdcul ) o 87\ ] . As ; t roslillg zero iHtaq ) hora , rcsolut.ion o1 ' con , I ) lcx s ( ml ; cn ( : c is l , ot only to I ) ( ; don ( ; sylll ; acl ; ically , t ) ul. also 1 ; o } ) e done l ) ra.g ; lnal ; ically a.n ( l/or SCil\ ] a.ni.ica.lly. Oil { , . o\ [ ' th { . i ) roluising can ( ti ( la.tc ror this is the ( ; cnl.cl'hlg tdieory \ [ Ih'onnan eL a.l 87~ Walker 90\ ] , To apply the ceill ; ering l ; lieory thai , is orighially for : ~ SC { lilCU ( : e of s { mt ; eil ( : es , liarllely disco'~H '' , ' ; c , WC i'egm 'd Lhc sul ) oi ; diuMx ; ( ; lmlsc a , n ( I the Ill , till ( ; lmisc as ~t scgflrlOllt or ( liscourse rcsl ) o ( ; tivoly. Moreover llaimko who is marked 1 ) y &lt; wa ' is regarded as l ; he topic for t.hcse two cla.uses. Then , the l.oplc 'lhmc ( fforth , +\ ] , $ $ $ lile~lilS ; 7,1 ! 1 '' O $ $ $ , . , where $ $ $ .. is ell , her grinmna.l.h ; al , so. , ilittil~iC { ir l ) \ [ ' &amp; ginatlc role. FoP hlsL ; ti , cc , ( / } .~ubj I\ [ I ( '.D~IIP ; Z ( ~l'O Sll\ ] ) j ( ! cl~ , ( /Jagl lllt ! &amp; litl % ( \ ] I'o fsq ( ~7t , T , f./Je ; L'/ ) llll ) illlS 7,01'O L3JIJtsTit '' lI.C¢'7 '' , ; LIII'I SO l } } l'l } l. 2 qlanako ' is ; i typical giN 's li &amp; iil ( L 679 Hanako is the strongest candidate for the backward center of the subordinate clause. Therefore the hackward center of the subordinatc clause is tlanak % arm consequently zero subject qh , , , bj refers to Itanako. By the same way as the subordinate clause case is dealt with , the zero subject o\ [ ' the main clause 4~z , ~bj is known to refer to tlanako , too. This result is neither interpretation a nor b shown above. Another candidate is the property sharing thoery \ [ Kameyama 88\ ] . In her theory , since the both of zero subjects share the snbjecthood , both of them finally are known to refer to Hanako that is the topic for both of these clauses. Therefore the prol ) erty sharing theory also fails to account for the iutuitive interpretations. Then we shift our attention to more microscopic one , ill which , roughly speaking , the important part of semantics of complex sentence is tbrmalized as relations among semantic roles that appear ill tile main clause or the subordinate clause. At the first glance , the constraints about these relations are not local in terms of mMn or subordinate clauses , hr other words , semantic roles that appear in subordinate clause and semantic roles that appear ill the main clause seem to be directly constrained by the constraints of complex sentence. However , looking more carefiflly , we find that the constraints of subordinate clause and tile cons ( fronts of main cla.use are represented as local constraints by introduciug the new notion of motivated which is characterized as a llerson who has euollgh reason to act as the lllain clause describes. More precisely , moZivated is one of tire pragmatic roles that appear in a subordinate clause , and the constraints in subordinate chmse are stated as identity relations between molivaled and other semantic/pragmatic roles appearing ill subordinate clause. Therefore these constraints are local in subordinate clause. The constraints ill main clause are stated as identity relations hetween molivaled which con|es from subordinate clause , and other semantic roles al / pearing in main clause. Therefore in understanding the mail ( clause we ( h ) u't have to be care M ) out semantic/pragmatic roles in subordinate cla.use other than a molivaled. In this sense , the constraints ill the main clause can be treated as almost local constraints of the main clause. The next question is how to represent tile semantics of complex sentence in feature structure ( called lPS henceforth ) . l ? or this , we shouhl write down the constraints about these relations among semantic/pragmatic roles ill a feature structure formalism. Due to the space limitation , in this l ) aper we ma.inly pursue the constraints about semantic feature structares. Complex Sentence We pay our attention to the general structure of 3apanese utterance which is helphfl to rel ) resent semantics of complex sentence. Several Japanese linguists have Mready proposed the general structm'e of Japanese utterances \ [ Mikami 53~ Minami 74 , Takuho 87 , ( ~Ullji 89\ ] . Mikami categorized clauses into three ( : lasses , namely 'open ' , 'semi-open ' and 'closed. ' '\ [ 'his categorization indicates how freely the content of clause interacts with the outside o/ ' clause. For instance , they arc categorized by the degree of possibilities of coreference between zero pronouns inside the subordinate clause and nominM or topic that appear in tile main clause. Following Mikami 's idea. , Minami proposed four levels , namely level A , B , C and D which correspond roughly to VP , proposition , sentence without eommuni ( -~tion mood and utterance which takes into a.ccount a hearer , respectively. \ [ Takubo 87\ ] divided level A into two levels. One of them corresponds to VI ' , the other corresponds to VP + a certain kind of subject which is called `` objective subject. '' g~unji proposed the more detailed structure , in which starting from predicate , say , verb and adjective , objects , voice , subject , aspect , tense , modality , topic and mood are or might be sequin &gt; tially added to make all inIbrmatioually more fultilled sentence component , l '' iually , it ; ends up with all utterallce .</sentence>
				<definiendum id="0">Sl )</definiendum>
				<definiendum id="1">moZivated</definiendum>
				<definiens id="0">say , verb and adjective , objects , voice , subject , aspect , tense , modality , topic and mood</definiens>
			</definition>
			<definition id="1">
				<sentence>Definition 2 ( Motiwlted ) Motivated is a person who is aj\ ] ~cled by the situation described by the subordinale clause deeply enough Io feel or acl as the main clause describes .</sentence>
				<definiendum id="0">Motiwlted ) Motivated</definiendum>
				<definiens id="0">a person who is aj\ ] ~cled by the situation described by the subordinale clause deeply enough Io feel</definiens>
			</definition>
			<definition id="2">
				<sentence>hi ( ; as ( of ~ul : ijc , c1 , iv0 a , dj &lt; , ' &lt; : tive wil , houl ; garu , l , he , COll~traiill , &lt; lnolitlale.d -erqmdencer ' hohls ~dso for type I ex &lt; : cpt for tit ( ; case whe.re directionally auxiliary w'~l 'D `` yaru ( .~ive ) '' , `` kurcrla ( I ) c given ) '' are used. Analysis for l.hese ca~e.~ in Oil0 O\ [ `` ottr flit ( Ire t ) roi : iJenl. As for '\ [ 'abh ; .4 , stat , e + is a slat , c , e×c ( ! itl , |7 ) r the/ : a~c that thct'c exists a third l : iarty who ix a molival ( ( I l &gt; UtS t , hc ( &gt; /EpCl'iClI , Cff*f '' iliLO l , ha , i , s ( ; ai , e. For ins\ [ , attce , tim c , ; e\ ] , .</sentence>
				<definiendum id="0">ottr flit</definiendum>
				<definiens id="0">a slat , c</definiens>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>which indicates incomplete ( you never know ~dl about the readers ) or inconsistent ( a text can be meant simultaneously for novices and experts ) knowledge .</sentence>
				<definiendum id="0">inconsistent</definiendum>
				<definiens id="0">indicates incomplete ( you never know ~dl about the readers</definiens>
			</definition>
			<definition id="1">
				<sentence>The Architecture of a Generation Component in a Complete NL Dialogue System .</sentence>
				<definiendum id="0">Architecture of</definiendum>
				<definiens id="0">a Generation Component in a Complete NL Dialogue System</definiens>
			</definition>
			<definition id="2">
				<sentence>Meaning-Text M~×lcls ( 1982 ) : A Recent Trend iu Soviet Linguistics .</sentence>
				<definiendum id="0">Meaning-Text M~×lcls</definiendum>
			</definition>
</paper>

		<paper id="2113">
			<definition id="0">
				<sentence>The problem of word sense disambiguation is one which has received increased attention in recent work on Natural Language Processing ( NLP ) and hfformation Retrieval ( IR ) .</sentence>
				<definiendum id="0">word sense disambiguation</definiendum>
				<definiendum id="1">hfformation Retrieval</definiendum>
				<definiens id="0">one which has received increased attention in recent work on Natural Language Processing ( NLP ) and</definiens>
			</definition>
</paper>

		<paper id="2109">
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>In tile beginning of the section , the basic &lt; h'sign of the sysi ; em is outlined. Lator on , default handling is included and exemplified for two general cases. In tile final section we summarize the main results of the paper , l , 'urthermore , we discuss how default handling can I ) e adapt¢ 'd to multilingual generatim~ , as required by l : llc speech-to-speech translation system \/I~I { .IL MOBIl , ( Block et al. , I ! ) 92 ) . DEFAULTS In the literature of norl-incremental generation , the need for defaults is hardly ever taken into account. The conunon point of view restricts the iulmt to be sulIicient for generation ( see , e.g. , the Te : ct Slructure by ( Meteer , 1990 ) for a syntactic generator ) . In incremental gm , eration , most authors agree on the necessity of using defaults ( see , e.g. , ( l ) e Smedt , 1990 ; Kitano , 1990 ; Ward , 1991 ) ) . Nevertheless , they do not in sufficient depth answer the question of how to guide the processes of default handling an ( l repair wil ; hin a generator. This I~roblem is the starting -- point tbr the following considerations. We assume tlm.t generation is a decision-making process witll the aim o\ [ ' producing it phmsiMe ul : t ( wance 1 ) ased on given information. As mentioned in section 1 , there are cases where this I ) rocess stops ( caused by underspccifical.ioll of the input ) before finishing its output. We define a module named d @ tv.l ! handh : r which tries to resume the process by giving advice to il ; , i.e. , by maldng assumptions about tile missing input specification. With respect to this task it is discussed section 2.1 ) , sb , st : cm ( see sectiou 2.2 ) , ~h ! scrihed ( see. s &lt; 'ctioll 2.3 ) , and d. how assumptious arc cancelled when they I.urll out 1.o Im inconsistent wil , \ ] , newly arriving input ; ( see section 2.4 ) . hi incremental generation , as mentioned in section 1 , interleaved input consumption and OUtlml. l ) rodllcl.ion causes spc'cific default situations. An incremental processing scheme allows for an increase of elficiency and flexibility , e.g. , by making tlm analysis and general.ion l~rocesses of a system \ [ 'or simultane , ous interl~ret , al : ion ow~rlap in Lime. There are two COmlml.ing goals of incremental generai ; i ( m for spoken oul , imt , thai , mnsl ; be tal , :en 357 into account when estimating the usefulness of defaults : Fluency : Long hesitations should be a.voided during the production of an utl ; erance , in order to be aeceptalde to the hearer 2. Reliabillty : Errors in an utterance may cause misunderstanding. In most cases , errors should be recovered by appropriate selfcorrections a. Excessive use of self-corrections or erroneous expressions should be avoided because they decrease intdligibility of the utterance. Obviously there is a trade-off between fluency and reliability : maximal reliability requires 'secure ' decisions and theretbre leads to output delay. On the other hand , maximal fluency necessitates the use of assuml ) tions and repair , respectively. We define as de.fa.~dt sit'ltatio~z the situation where a generation system has not yet finished the utterance but at the same t ; ime has consumed all given iul ) ut and is not ahle to continue processing. In non-incremental gel &gt; eration , this corresponds to the fact that the input lacks necessary informatiou , because the entire input is assumed to be given at one time ( e.g. , the undecidable number wdue of the example described in section 1 ) .</sentence>
				<definiendum id="0">em</definiendum>
				<definiens id="0">they do not in sufficient depth answer the question of how to guide the processes of default handling an ( l repair wil</definiens>
				<definiens id="1">de.fa.~dt sit'ltatio~z the situation where a generation system has not yet finished the utterance but at the same t</definiens>
			</definition>
			<definition id="1">
				<sentence>just desribes ~t verb category without conclel ; c filler ( a dummy verb ) plus a subject cmnpleuwnt and active voice for the verbM phrase .</sentence>
				<definiendum id="0">c filler</definiendum>
				<definiens id="0">a dummy verb ) plus a subject cmnpleuwnt and active voice for the verbM phrase</definiens>
			</definition>
</paper>

		<paper id="2173">
			<definition id="0">
				<sentence>The template pattern search recognizes relationships between matched objects in the defined pattern a.s well a.s recognizing the .</sentence>
				<definiendum id="0">template pattern search</definiendum>
				<definiens id="0">recognizes relationships between matched objects in the defined pattern a.s well a.s recognizing the</definiens>
			</definition>
			<definition id="1">
				<sentence>The discourse processor links information identified a.t different stages o\ [ `` processing .</sentence>
				<definiendum id="0">discourse processor</definiendum>
				<definiens id="0">links information identified a.t different stages</definiens>
			</definition>
			<definition id="2">
				<sentence>The selection in ( lone by applying three heuristic rules in the following or &lt; let : • sele ( : t l ) a , tterns th~tt in ( : hide the IlK ) st I/lli\ [ lbei ' of ' nla , tched COlll\ ] ) al/y-ll311K ) Val'i Inc. '' a bles in which there is at least one company llaDle~ , sele ( : t l ) atterns tha.t COllSili\ [ le tilt fewest input seglnents ( the shortest string match ) , and • select patterns that include the lnost llliiiil ) er of variables and defined words. Another important feature of the pa.t : tern Iilat ( : hor is tha , t rules can be groupe ( 1 accordilig to their COilCel ) t. A rule lla.iile `` JohltVeliturel '' iii Fig. 2 , for example , represents a concel ) t `` JointVelitllre'. Ushlg this groupbig , the best nlatched pattern can be selected fl'on-i nlatched patterns of a particular concept group instea.d of choosing from all the matched patterns. This feature enables the discourse and template g ( meration processes to look at the , best infortnation necessary whet , tilling in a particular slot. TION EXTRACTION PROCESS This sectkm describes how concepts ~tnd patterns identifh ; d by tile matcher are used for tenll ) late filling. Concepts are often useful to fill in the `` set fill '' ( choice fi'om a. given set ) sh ) ts. An entity type slot , for examl ) le , ha , s four giw , i choices : COMPANY , I ) EI { SON , GOVERNM/i ; NT , ~tnd OTIIt { ; R. The matcher assigns concepts related to each entity type except ( ) TflEIL Thus , from the given set , the output generator chooses an entity type corresponding to the identified concept. There axe ca.ses when discourse processing is necessa.ry to link identified concepts and patterns. The f.ollowing text ; : `` X Inc. created a joint w ; ntm'e with Y Corp. last yea.r. X announced yesterday that it. terminated the venture. '' in used to describe the extraction process illustrated in Fig. 3. In the preprocessing , two company na.tnes in the first sentence `` X hic. '' and `` Y Corp. '' are identified either I ) y MAJESTY or the name recognizer. In the first sentence , the templa.te pa.tteru search locates t he , I ointgenture 1 pattern shown in Fig. 2. Now , the , INTV I ' ; NT \ [ J 1 { 1'3 con cepl ; I ) etweeii `` X inc. '' and `` Y C'ol'l ) . '' is recognized. In tim second Sellt ( ~.it ( X~. , 7067 `` X Inc. created a `` X announced joint venture with yesterday that it Y Corp. last year. '' i terminated the venture. '' I Precompany1 : `` X Inc. '' `` X '' company2 : Ip rocesslng company3 : `` Y Corp. '' : : : : : : : : : : : : : : : : : : : : : : D , ssoLveD '' IVENTURE* &amp; LCorp '' Discourse `` X Inc. '' ( companyl=company3 ) process ng } JOINT-VENTURE* y DISSOLVED* `` Y Corp. '' * concept name Fig. 3 : FxamI ) le of the inforntatiou extraction process the company name `` X '' is also identitied by the preprocessor , a Next , the concept `` DISSOLVEi ) '' is recognized by the key word terminate in the concept search. ( The key word list is shown in Section 4.11. ) After sentencelevel processing , discourse processing recognizes that `` X '' in the second sentence is a reference to `` X Inc. '' found in the first sentence. Thus , the `` DISSOLVI ' ; I ) '' concept is joined to the , joint venture relationship between `` X inc. '' and `` Y Corp. '' . in this way , TI ! iXTRACT recognizes that the two companies dissolved the , joint venture. SupI ) ose that the second sentence is replace ( t with another sentence : `` Shortly after , X terminated a contract to supply rice to Z Corp. '' . Although it does not mentiot~ the dissolved relationship nor anything a hottt `` Y Corp. '' , the system incorrectly recognizes the dissolved joint ventttre rela.tionship between `` X In ( '. '' and `` V Corp. '' due to the existence of the word terminate. When this undesirable matching is often seen , more complicated template patterns must be used instead of tile simple word list. A dissolved concept , lbr example , could he identified using the fo\ ] \ [ owing template pattern : aWhen it is an unknown word to tim prepro ( x~ssor , the discourse processor idcnti\ [ ies it IM.er. ( Dissolvedl 2 OCNAMEPARTNER_SUBJ dissolve I terminate l cancel : :V @ skip venture : :N @ CNAME_PARTNER_WITH ) . Then , discourse processing must check if com+ panies identified in this pattern are the same as the current joint venture comi ) anies in order to recognize their dissolved relationship. MANCE A total of 250 newspaper articles , 100 about Japanese mi ( 'roelectronics and 150 about Japanese corl ) orate joint ventures were provided by ARI ) A fl ) r nse in the Tll ) SrI'FA/./MU ( L5 system evalua.tion. Five microelectronics and six joint ventures systems were presented in the Japanese , system eva.luation a.t MUC-5. 4 Scoring was done in a , semi-atttomatie nta , nner , rl'he scoring program automatically compared the system output with answer tetnl ) lates created by humat~ analysts , then , when a Mman decision was necessa.ry , analysts instructed the scof ing progratu whether tlt ( , , two strings in coral ) arisen were completely matched , pa.rtially matched , or unumtched. Finally , it calculated an overall score combined from all the newspaper article scores. Although various evalnat ; ion tnetrics were tneasured in the evaluatiou \ [ C\ [ lillchor and Sun ( lheim 93\ ] , only the following error and reeall-l ) recision metrics are discussed in this pa.per. The ha.sic scoring categories use ( l are : correct ( CO R ) , partially correct ( PAR ) , itlcorrect ( INC ) , ntissing ( MIS ) , and spurious ( SPIJ ) , counted as the tmml ) er of i ) ieces of inl'orma.tion in the system output eompa.red to tile possil ) le ( answer ) information. ( l ) 1 ' ; 1'1 ; o1 ' metrics * l ' ; rror 1 ) el . resl ) onse fill ( I~l { II ) : wrong _ INC+ ILdR/2 + \ ] I'IIS + HPU lotal COR+ t ) AR+ INC+ MLS'+,5'PU 4These numbers include TI ; ; X'FRACT , aA| optional sb'stmn of ( ; I'LCMU SIIO ( ; UN. 7068 TaJ ) h , 1 : Scores of `` rI ' ; XTItACT and two other l.Ol &gt; ra .</sentence>
				<definiendum id="0">DISSOLVEi</definiendum>
				<definiendum id="1">DISSOLVI</definiendum>
				<definiens id="0">t l ) atterns tha.t COllSili\ [ le tilt fewest input seglnents ( the shortest string match ) , and • select patterns that include the lnost llliiiil ) er of variables and defined words. Another important feature of the pa.t : tern Iilat ( : hor is tha , t rules can be groupe ( 1 accordilig to their COilCel ) t. A rule lla.iile `` JohltVeliturel '' iii Fig. 2</definiens>
				<definiens id="1">the best nlatched pattern can be selected fl'on-i nlatched patterns of a particular concept group instea.d of choosing from all the matched patterns. This feature enables the discourse and template g</definiens>
				<definiens id="2">L5 system evalua.tion. Five microelectronics and six joint ventures systems were presented in the Japanese , system eva.luation a.t MUC-5. 4 Scoring was done in a , semi-atttomatie nta , nner , rl'he scoring program automatically compared the system output with answer tetnl ) lates created by humat~ analysts , then , when a Mman decision was necessa.ry , analysts instructed the scof ing progratu whether tlt ( , , two strings in coral ) arisen were completely matched , pa.rtially matched , or unumtched. Finally , it calculated an overall score combined from all the newspaper article scores. Although various evalnat ; ion tnetrics were tneasured in the evaluatiou \ [ C\ [ lillchor</definiens>
				<definiens id="3">the tmml ) er of i ) ieces of inl'orma.tion in the system output eompa.red to tile possil ) le ( answer ) information. ( l ) 1 ' ; 1'1 ; o1 ' metrics * l ' ; rror 1 ) el . resl ) onse fill ( I~l { II ) : wrong _ INC+ ILdR/2 + \ ] I'IIS + HPU lotal COR+ t ) AR+ INC+ MLS'+,5'PU 4These numbers include TI ; ; X'FRACT , aA| optional sb'stmn of</definiens>
			</definition>
</paper>

		<paper id="2191">
			<definition id="0">
				<sentence>The algorithm uses an inference engine based on Bayes ' theorem : P ( HK ) P ( AII-IK ) P ( I t\ [ dA ) ... ... ... ... ... ... . P ( I KOP ( AIH~ ) forK = 1,2 , ... Under the conditions of our model Bayes ' theorem allows the following 1173 interpretation : there are only two possible hypotheses for a certain noun ( verb ) phrase that it is the center of the current sentence ( clause ) or that it is not .</sentence>
				<definiendum id="0">algorithm</definiendum>
				<definiendum id="1">P ( I KOP</definiendum>
				<definiens id="0">uses an inference engine based on Bayes ' theorem : P ( HK</definiens>
				<definiens id="1">the center of the current sentence ( clause ) or that it is not</definiens>
			</definition>
			<definition id="1">
				<sentence>By analogy P ( AIHN ) is the probability of the symptom being observed with a phrase which is not the center ( henceforth referred to as PN ) .</sentence>
				<definiendum id="0">analogy P ( AIHN )</definiendum>
			</definition>
			<definition id="2">
				<sentence>The aposteriori probability P ( HKtA ) is defined in the light of the new piece of evidence the presence of an empirically obtained symptom , indicating the higher probability the examined phrase to be in the center of the discourse segment .</sentence>
				<definiendum id="0">aposteriori probability P ( HKtA</definiendum>
			</definition>
			<definition id="3">
				<sentence>An estimation of the probability of a subject , ( direct or indirect ) object or verb phrase ( the only possible centers in our texts ) to be centers , can be represented as a predicate with arguments : center ( X , PI , \ [ symptoml ( weight factorl 1 ' weight factorl2 ) ... . symptomN ( weight factorN~ , weight factolNz ) \ ] ) where center ( X , I , list ) represents the estimated probability of X to be the center of a sentence ( clause ) , X E { subjec t , objectl , object2 ... . verb phrase } and Pl is the initial probability of X to be the center of the sentence ( clause ) .</sentence>
				<definiendum id="0">center ( X</definiendum>
				<definiendum id="1">Pl</definiendum>
				<definiens id="0">direct or indirect ) object or verb phrase</definiens>
				<definiens id="1">the estimated probability of X to be the center of a sentence ( clause ) , X E { subjec t , objectl</definiens>
			</definition>
			<definition id="4">
				<sentence>Weight factorl is the probability of the symptom being observed with a noun ( verb ) phrase which is the center ( Py ) .</sentence>
				<definiendum id="0">Weight factorl</definiendum>
				<definiens id="0">the probability of the symptom being observed with a noun ( verb ) phrase which is the center ( Py )</definiens>
			</definition>
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>Wc illustrate the static knowledge sources ( primarily the lexicon ) and the representations that are l ) roduccd ( syntax , lexical semantics , and TMR ) .</sentence>
				<definiendum id="0">Wc</definiendum>
				<definiens id="0">illustrate the static knowledge sources ( primarily the lexicon</definiens>
			</definition>
			<definition id="1">
				<sentence>Lexical-cotlcel~lual SlrUCtures ( LCSs ) have been suggested its meaning represenlalions for n , 'ilural language sentences produced in accordance with the semantic theory developed by I lale and Jaekendo11 '' ( e.g. , Jackendoff , 1983 ) and used in MT-related experimenls by Dorr ( l ) orr , 1993 ) .</sentence>
				<definiendum id="0">Lexical-cotlcel~lual SlrUCtures</definiendum>
				<definiendum id="1">LCSs</definiendum>
				<definiens id="0">suggested its meaning represenlalions for n , 'ilural language sentences produced in accordance with the semantic theory developed by I lale</definiens>
			</definition>
			<definition id="2">
				<sentence>The inlerlingtmtext ( or texl meaning representation , TMR ) is a slructt~re which represents meaning of texts in accor ( laL~ce with Ihe ontologyoriented ; .</sentence>
				<definiendum id="0">TMR )</definiendum>
				<definiens id="0">a slructt~re which represents meaning of texts in accor ( laL~ce with Ihe ontologyoriented ;</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , to make a TMR for John began to read we need to identify a nnmbcr of meaning elements , prilnarily Ihat something look place hefore the time of speech , which was the beginning plut , ; e of a re &lt; iding evenl carried ont by John. 2 I low do we lind tllese pieces of information ? Tinle before the time of speecll is indicated by the mos7Jfiolog j of '' began '' . The beginning phase is typically intlicnled h , xically by the verb begin in English. We know that it is the beginning phase of reading becanse the syntax module tells us thai to reed is the complement of begin. We know Ihat John is reading because John is the snbject of begin ( once again , the sytllaelic module produced this element of informalion ) , whose lexical properties tell us thai John is also nnderstood as the subject of the complcment clause. In oilier words , it is the predicate argnnienl structure of begin ( prodnced by the synlax-to-S DLS mapping procedure ill the lexicon entry for begin ) Ihat tells ils where to lind ulany of the relevant pieces of information. l laving lhns served the purpose of identifying a part of the selnanlic dependency Io be represeuled in tile linal TMR ( just as the liudings of other syslenl modtnles played their assigned roles as clnes for delermining paris of Ihe TMR strnctnre ) , Ihe predicate : u'gulnent slnctnre can then be disc : tided. In Ihe l % ~llowing seclion we give sonic delailed exanlplcs of Ihe nlappings involved in prodncing SDLS OUllnll strnctures and TMl , ts : ts well : is relevant paris of lexicon erllries. Examples in Figures 2 , 3 and 4 contaiu a ntnnl'~er of representative phenomena which nnderscore the diflerences between SDLSs aud TMRs : is well ~ts ilhistrale how tile two structures co-exist in the Mikmkosmos processing model. In doing so , we also describe a lexicon design which accommodates both Stl'nctures. In all three exampies the SDLS is jilSi one of the cities for dclcrtnining a COlilponcnl of nlealling , and is not pre , served is01norphiI\ [ l~cidcntally , therefore verb clilsses are nell suilable its sclnantio hierarchies fi~r ontology ( Mitamura 1989 ) . ... . it COliN also be the I ) eglllllllll~ p I se ofa i I citer I'etldlng iIIstesld jtlgl cite instance of reading-Ihei'e is no way l { i tlelerlnine which in the abSellCe ( if coillexl. 350 cally ill tile TMR. Tile examples also illustrate the use of constructions ( Filhnore et al. 19gg , Filhnore and Kay 1992 ) as a nnit of analysis alongside words , and show that treatment of MT divergences in this apl ) roaeh simply falls out of tile general iltodel. The languages used ior illustration are English , RussimL and Japanese. Since the system is symmelrical , we do not identify which is the source langtlage and which is the target langnage ill each exmnple. Pot '' each example , we list a TMR , which is the s ; une for all of tile l { mguages , as well as synlaclic slruclures , semmltic role ~tssignments ( SDLS ) , and lexicifl entries for each lmlguage. It should be appment that tile TMR is not necessarily isomorphic Io the SDLS of any of the languages , and that sentences Irom different languages cml correspond to the same TMR even if their syntactic , 'uld SDLS representations are not isomorphic. The Mikrokosmos TMR structure consists of clauses which roughly correspond to the `` who did what to whont '' component of meaning but also includes such components as speech acts , speaker altitudes , indices of the speech situalien , slylistic factors its well Its relalions ( e.g. , temporal titles ) allIOtlg amy el + the above , alld other elements. The lexical enlries include three zones -- -syntax , semantic role , qssignment , and maPlting to TMR. ( The lirsl and third zones are discussed by Meyer et aI. 199 t . ) The lirst zone specilies an LFG-style ( Bresnan 1982 ) syntactic subcategorizalion frame of a predicate , including which grammatical functions ( subject , object , COmlllentcnt , etc. ) the predicate mtlst appear with and any requirements the predicate has of those funclions ( case , syntactic calegory , specilic lexical items , etc. ) . The second zone , also in the spirit of LFG , specilies a mapping belween tile gralnlnalical fair , talons governed by it pfcdic.'ttc arRl the sctllatltiC roles it ~.tssigns. Semantic 1olo : kssignmenl is indicated by coindexing of a sy111actic slol and a semantic role slot. The semantic role munes used in lhe exantples are simply labels lot argument positions in lexical conceptual slrtlcItlres , which are not showtl here. The syntax iuld selnallliC role assignlnent Zotles serve+ the pllr\ [ ) ose of Iocaling the imporlant participators in the sentence. For example , they might tell us thal the experiencer : u'gLnltcnt is in the SLIbjCCl slot with dative case , or Ihat the phrase functioning as file lheme argument is lound ill the object position. They arc also imporlalll ill capltniltg bolh Inrlgtiagc-specilic gcncralizalions about verb classes and universals of SClllttnlic role ilSs\ [ gnllle\ [ l\ [ . For these leaSOl/S , the syntax and Selllillltic role zones are entcial , and therefore ii/tlSl be inch\ ] deal even in cases in which they differ drastically from the TMR. The third zone of tile lexical entry spccilies portion of TMR that is associated wilh a lexical item and how the componcnls of the TMR corrcsltond to the components of the syntactic alld Semanlic role zones. We have chosen examples in which the TMR is not isomorphic to the synlactie and lexical senlantic zones , ill tnost of the examples , a lexical item specilies Ihat title of its cmltplemenls heads Ihe associated TMR. Ill these cases , Ihe syntactic head of the sentence corresponds 1o some kind of scope-taking operator or a simple feature-vahm pair ill TMR.. The examples , inci ( lcnttdly , illustrate our treatment of MT divergences -- situations in which It source langlmgc sentence and its target lmlguage translation differ signilicanlly in synlactic structure , syntaclic category , or l ) ret`licale-argun/ent slrttcltnc. No special mechanisms are needed to lreat MT divergences ill oln IllO ( JCL All lhat is needed in order to translate a sentence involving a divergence are source and target language lexical enlries of tile sort ilhlstrated here Ihat mall dil\ ] erent synlaCliC strtlcttnes elite lhe S\ [ llllO TMR. `` File reitreseltlillions i|lld nlechanisnts shown ill the lexical enhies are tllotivaled lor non-divergelll ex : tnlples and do nol have lo lie IllOdifled to deat with divergent examples. This is because source and larger language sentences : .Ire not normally eXl ; Ccled 1o be isomorllhic to tile `` FMR or to each other. Another inlportanI fealuxe of ottr model is that it considers constructions to be basic lexical unils ah ) ng with words. Following Filhnore el al. , 19gg , we deline constructions as ( possibly , discontiguous ) synlaclic structure or produclive synlaclic pattern whose meaning it is often impossible to derive solely based on the meanlings of its components. In other words , a COllSlrtlction is a COlIIbJll , 'tIt ( lit of a syl/laCfiC Sll 'll ( : lln'e : ll/d tile associaled sgln~|n\ [ \ ] c altd pragmatic representations which , once dereliCt`l , tie not have Io lie composiliortally itroduced by a 'I'MP , exIraclof. CotlSlrllCtions are typically ways of expressing it ilteillling that are CttllvenliOll : ll ill tile sense thai they are I'rozcn , lind t/or synchronically deriwdlle from general prhlciplcs , even il ' they once were. Note thltl il Iorlnalisllt Stlch its the I IPSG-IIke siglt of tile dictitlllal+y slrucltlre of tile ACQUILEX project can lie nlatle to SUpF , Oft such an idea , its lqlhnore and Kay ( 1992 ) show. tff Form-Meaning Correslmmlence The MikrokosmcJs project is based on a theory of formmeaning cc , rrcspondcnce , whose underlying assumptions can Im statcd as follows : • Meanings are exlrac/ed from lexls on lho basis of all and any available clues ( e.g. , syntactic , mof\ ] /hological , illltl lexical properties of an ilttelance ) . The exl.la¢liotl of i|leallir~g consists of collslrtlcling Ihe most platlsible , thollgh usually delbasilile , hyl ) othesis that is conlpalible with tile evidence , makhlg il an abt`hlctive process ( Hobbs , 1991 ) . • TIlE processing of clues in Mikrokosmos is grouped lille inicrolheorics for clcnlelllS ill ' Illearlillg SIICll IIS predicatc-afglllllCllt relaliollS , aspect , lelnporill lel ; .llions , modality , evidcltliality , etc. Elicit nlicrolhcory spccilics the ways to COllSlrllcI `` I'MRs for some : .ispccl of nlcallhlg by idcnlifying the Val'it/llS SylIIacIiC , morphologic 'd , : rod lexical clues Ior Ihal element of meillling ill individual lallgtlagcs. • Ill integrating the microtheories , Mikrokc , smos rejects lhe lnu'e slratilicaliona\ [ apprcmch shared by such otherwise diverse models its AI NLF ' semantics ( e.g. , l \ [ irsl , 1987 ) or Mel'~ , uk 's MTM ( e.g. , Mel'~uk , 19el ) . Knowledge from all kinds of areas coexists in tile stone rules for the determinalion of meaning units. • The clues ( pieces of evidence ) for an element of meaning can interact in complex ways. Cities can reinforce or contradict each t ) thel : Coercion is possible in sittlations ill which tile clues conIlicI. Inlcrprctalion o\ [ a clue can be dependent on which other lines are pl'cSellI. 351 e~ 8 v o.~ .~.~ a8 rj ~ O0~J • &amp; ~ ~i .~ ~o ~ ; # .~z ~ ~ . '' - : . o °. `` 4~ ~ oo~ ~ ° '' i ~L o , ,4 , ,4~ ~* ~ '~ ~ ~* `` ~ , .z , ~ '' o , ,. , Q~ , ? , _1 ~ ~J '° ~o~ be '5 ~8. : ~ ~. o o~ . , q u~ ~o .o o o ~ ' ~'o.~ o `` r ~ ~ o 352 e4 °~ o o db~ © ~ ° ~'~ • , ' ~ , °° ¢.1 ~ ~ .~ ~'~ .. 0 o ~ ~a~.~ .° • ~ , \ [ ~ ; 2 r~ o , Z~ .~ ~.~ ~ ? ~ o.~ sN '-G '~ u I : :1 1 m ~.~ o ~ NS~ 353 = ~. , m I=:1 ~ .. ~.~ N E E ~ I~ i~ ~~ ,1t • = ~ .~. ~ ~o-~.~ ~.~.~ E a , ~ { . ~o i~ .~..~ ~i ~ ~ ~~ ~ • ! ~ ~.~ 88 ~ • ~ ~ s2 ~.~N .~ .~ o o _ .~ ~ 8 '' ~ ~.~ ~ ~.~.~ 354 • Mikrokosnlos is anlonablo 1o working wilh inconil ) lole in|i. ) rlliatiOli. If IlOl all of lho hlpul condilioiis of lho rules are 1 ) rosenl , sonlo \ [ inclines will slill be possible , This properly is iniporlanl boCailSO we iilleiid Ill deal with real Io×ls , and we canllol tlol ) t~ thai COlilplote knowledge will he available. In the absence of spot ( lie knowledge , Mikrokosinos falls back Oll probabilislic and statistical devices. • An iilil ) O ) -lalli lilctor ht the design of lho iliicrolheories is iho , idelllilication of forms ( above the Ioxical level ) lhal are associalod with st ) me aSl } Ot : l of moaning by convention , rathe , r than through conlposilional on-prt ) ( hlclivo rlllos. We li/llow FilhilorO el al. , 1988 in adopting lho conslruclion as a basic , unil of al ) aly-sis. lit COllChlSioll , ilOlO how the cxaniplos ill Figilres 2 , 3 ~ul ( l 4 relalc ' , It ) the al ) ovo backgroinld assilniptioils o\ [ Mikrokosnlos. The o×ainl/les ilhlslrate how SDLS is ilsed ~.IS ~1 SOllrUc c ) l chios for various inicrolheorios , inohlding lhat of Ioxical-soniaillic dopelidenoy , aspect , nio ( lalily , speech acts , olc. \ ] 'lie nlajor lhiding of fills paper is lhal TMRs are not idonlical It ) SDLS oulptit slruclilros , hill lhai the latlcr are slill nomess~u+y in Ihat they are essential li ) r Ilio oxliiiclion t ) f lllOaning \ [ roll ( a le×l. \ ] 'lie OXalllpies also ilhistrale the coniillox iillOracli ( ) li of lhe , wirii ) llS cities ( llorigllchi 1993 ) . f ; or it ( sial ( co , the Japanese verb DlOl ( lll Call sigiial a reqilOsl-acli ( ) n Sl ) oech act but olily fill appo~us in a spccilic niorpho-synlactic t~nvirolllnOlll 0iolipasl , qiloSlion , Slleakor is subjecl , hearer is second object ) . In this onvironlnonl , olhor chios lake t ) ll Sllocial lileailings. l : or OXalilplo negation and poiOlllialily serve ollly It ) st ) f toil tile assertiveness of Ihe ieqileSl. Convolllit ) lialily is also ilhislratcd in the abt ) vo exalt ( pies. Mally of the oxaniplcs illuslraic coilslructiolls lhal ai+o associated will ) SClnantio alid llragltlalic nioanhlgs I ) y coiivolllioll. We leave tllt~ iSSilOS of ll ( ill-Slralilicationalily and working , ,villi inconlplelo hifornialion It ) r fllltlro papers which deal priniarily wilh tile control slrucluro of Mikrokosmos. Another iml ) Orlanl Ct ) lllribilliOn of this paper is to silggosl a liamowork in which MT divergoliOOS arc lieu ( + died tlsiiig Ollly 1he liiochaniSlilS Ihal are , noodt ' , ( I for liOlldivergent solllences , ( ) urlheory l } loclicls that divergences will arise bocailSO the SalllO olclli¢lll of tlaliiiig ill difforolll lal ) gilages will ilOl Ileeossariiy be expressed will ) ist ) tnorphic synlax , nlo ) \ ] ) hl , flogy , and lexicai itch ( S. Tile MikrokoSllli ) S TMR and Iho sot of nlicrothoorios for all lho rolOvalll languages naturally liandlo 1he so-called tli+ vorgencos without any addilional lnecli ; , inisnls. ileferences \ [ 11 Ihesilan , J. 1982. The Menial llepresenialion of ( ; rainalnlical Ilelalions Canlbridge , MA : MIT l'rcss. 121 Carbenell , J , , T. Milannira and F , . Nyborg. I992. \ ] 'lie KANT MT Project. Ih ' ( ) ceedings ol TMI-92. Monheal. \ [ 3\ ] l ) orr , II. 1993. The Use c ) I Lexical Senmntics in Inlerlillgual Machine Tianslalilm , Machine `` l ) 'anslation , 7:135194. \ [ 41 I ) orr , II. 1992. Classil\ ] catiut~ t ) l '' tnachine h'allslalion divcrgellces at/el a pU ) l ) ( ) sod sohlli ( lll. ( 7O , il~Utationa\ ] LmguisUcs. 151 ( h~odnlan , K. and S. Niienhur1~ ( eds. ) 1992. KIIIMT-89 : A Case Sillily in Knowledge-llased Maehhle Teanslalkm. San Mateu : Mmgan Kalllmal ( tl. \ [ 61 Fillmore , 11. , P. Kay and M.C. ( ) 'ConJifn. 1988. I &lt; teguhuity and ldioniaticily it ( Oranlll ) alical ConslitlcliOllS : the Case rifLer Alone. Language , 64:501-38. \ [ 7\ ] Filtmclre , C. and I : Kay. 1992. Cmlstrucli { ) n Olallllll~l.i. Course Materials. University { ) f California at Beikeley. \ [ 8\ ] llirst , ( 3. 1987. Senlantie hlterprelation and Res~lhutiull of Amlfiguity. Cambridge Univcrsfly l'rcss. 19\ ] ttohbs , J. 199 I. hiteiprelatim ) as Ahd ) lclilm. l'u ( ) ceedings of ACL-91. \ [ 101 1 loriguchi , K. 1993. Extraction { ) f t'ragm alic Inli : ) rnlati~l n in a CALL System. NLP 111 Piojecl l~epoI\ [ , Carnegie Melhm l.lniversily. \ [ 11\ ] Jackendoll , R. 1983. Selnantics and C ( igniiion. Cam. bridge , MA : MIT hess. \ [ 12\ ] Levm , B. 1993. English Verb Classes and AIlernao lions : A Prelilliinary Investigation.Chicago : The \ [ Jni velsily of Chicago l'ress. \ [ 131 \ [ , cvin , L. and S. Nirenburg. 1994 ( to appear ) . Ccmshuctiml-liased MT Lexicons. in M. Palmer , led. ) \ [ 141 Mahl ( ulud , A.T. 1989. A ( \ ] llmpan'aiive Study ( if Middle alid lncholitive Allernalions in Arable and l , 'Aiglish. l'h.I ) . \ ] ) isserhdion. Univelsity of l ) itlsburgh. I151 Malsllliioto , Y. 1992. ( in the Wordliolid ( ) f , |al ) alle , se Complex I~l'eilieales. Ph.I ) . Dissertatiun , Stanf ( ml Uni versiiy. \ [ 161 Mel'~.uk , I.A. 1981. Meaning-Text Models : A Reccnl '\ [ 'iend in Soviet Linguistics. 7'lie Annual Review of An. thrrq ) ology. \ [ 171 Meyer , i. , 11. ( ) nystikevych and L. Carls ( m. 1990. Lexicogial~hic l'linciples and Design for Knowledge-Bascd Machine Transhdiml. CMU CMT Technical Repel ( 90 118. 1181 Mitalllura , T. 1989. The llierarchical Orwulizalion ( if P ) 'edielile Fl'allieS for |illerpl'elive Mallpil ) 14 in Natllral Languilge P ) 'oeessilill. Pti.D. Disseltati ( ) n. { \ ] nivei'+ sily uf l &gt; ittsburgh , 1191 Nirellbmg &gt; S , , J. farbonell , M. qim/ita and K , ( J ( md I ) l &lt; &lt; tl ( .</sentence>
				<definiendum id="0">SDLS</definiendum>
				<definiendum id="1">RussimL</definiendum>
				<definiendum id="2">TMR</definiendum>
				<definiendum id="3">COllSlrtlction</definiendum>
				<definiens id="0">to make a TMR for John began to read we need to identify a nnmbcr of meaning elements , prilnarily Ihat something look place hefore the time of speech , which was the beginning plut , ; e of a re &lt; iding evenl carried ont by John. 2 I low do we lind tllese pieces of information ? Tinle before the time of speecll is indicated by the mos7Jfiolog j of '' began '' . The beginning phase is typically intlicnled h , xically by the verb begin in English. We know that it is the beginning phase of reading becanse the syntax module tells us thai to reed is the complement of begin. We know Ihat John is reading because John is the snbject of begin ( once again , the sytllaelic module produced this element of informalion ) , whose lexical properties tell us thai John is also nnderstood as the subject of the complcment clause. In oilier words , it is the predicate argnnienl structure of begin ( prodnced by the synlax-to-S DLS mapping procedure ill the lexicon entry for begin ) Ihat tells ils where to lind ulany of the relevant pieces of information. l laving lhns served the purpose of identifying a part of the selnanlic dependency Io be represeuled in tile linal TMR ( just as the liudings of other syslenl modtnles played their assigned roles as clnes for delermining paris of Ihe TMR strnctnre ) , Ihe predicate : u'gulnent slnctnre can then be disc : tided. In Ihe l % ~llowing seclion we give sonic delailed exanlplcs of Ihe nlappings involved in prodncing SDLS OUllnll strnctures and TMl , ts : ts well : is relevant paris of lexicon erllries. Examples in Figures 2 , 3 and 4 contaiu a ntnnl'~er of representative phenomena which nnderscore the diflerences between SDLSs aud TMRs : is well ~ts ilhistrale how tile two structures co-exist in the Mikmkosmos processing model. In doing so</definiens>
				<definiens id="1">jilSi one of the cities for dclcrtnining a COlilponcnl of nlealling , and is not pre , served is01norphiI\ [ l~cidcntally , therefore verb clilsses are nell suilable its sclnantio hierarchies fi~r ontology ( Mitamura 1989 ) . ... . it COliN also be the I ) eglllllllll~ p I se ofa i I citer I'etldlng iIIstesld jtlgl cite instance of reading-Ihei'e is no way l { i tlelerlnine which in the abSellCe ( if coillexl. 350 cally ill tile TMR. Tile examples also illustrate the use of constructions ( Filhnore et al. 19gg , Filhnore and Kay 1992 ) as a nnit of analysis alongside words , and show that treatment of MT divergences in this apl ) roaeh simply falls out of tile general iltodel. The languages used ior illustration are English ,</definiens>
				<definiens id="2">the s ; une for all of tile l { mguages , as well as synlaclic slruclures , semmltic role ~tssignments ( SDLS ) , and lexicifl entries for each lmlguage. It should be appment that tile TMR is not necessarily isomorphic Io the SDLS of any of the languages , and that sentences Irom different languages cml correspond to the same TMR even if their syntactic , 'uld SDLS representations are not isomorphic. The Mikrokosmos TMR structure consists of clauses which roughly correspond to the `` who did what to whont '' component of meaning but also includes such components as speech acts , speaker altitudes , indices of the speech situalien , slylistic factors its well Its relalions ( e.g. , temporal titles ) allIOtlg amy el + the above , alld other elements. The lexical enlries include three zones -- -syntax , semantic role , qssignment , and maPlting to TMR. ( The lirsl and third zones are discussed by Meyer et aI. 199 t . ) The lirst zone specilies an LFG-style ( Bresnan 1982 ) syntactic subcategorizalion frame of a predicate , including which grammatical functions ( subject , object , COmlllentcnt , etc. ) the predicate mtlst appear with and any requirements the predicate has of those funclions ( case , syntactic calegory , specilic lexical items , etc. ) . The second zone , also in the spirit of LFG , specilies a mapping belween tile gralnlnalical fair , talons governed by it pfcdic.'ttc arRl the sctllatltiC roles it ~.tssigns. Semantic 1olo : kssignmenl is indicated by coindexing of a sy111actic slol and a semantic role slot. The semantic role munes used in lhe exantples are simply labels lot argument positions in lexical conceptual slrtlcItlres , which are not showtl here. The syntax iuld selnallliC role assignlnent Zotles serve+ the pllr\ [ ) ose of Iocaling the imporlant participators in the sentence. For example , they might tell us thal the experiencer : u'gLnltcnt is in the SLIbjCCl slot with dative case , or Ihat the phrase functioning as file lheme argument is lound ill the object position. They arc also imporlalll ill capltniltg bolh Inrlgtiagc-specilic gcncralizalions about verb classes and universals of SClllttnlic role ilSs\ [ gnllle\ [ l\ [ . For these leaSOl/S , the syntax and Selllillltic role zones are entcial , and therefore ii/tlSl be inch\ ] deal even in cases in which they differ drastically from the TMR. The third zone of tile lexical entry spccilies portion of TMR that is associated wilh a lexical item and how the componcnls of the TMR corrcsltond to the components of the syntactic alld Semanlic role zones. We have chosen examples in which the TMR is not isomorphic to the synlactie and lexical senlantic zones , ill tnost of the examples , a lexical item specilies Ihat title of its cmltplemenls heads Ihe associated TMR. Ill these cases , Ihe syntactic head of the sentence corresponds 1o some kind of scope-taking operator or a simple feature-vahm pair ill TMR.. The examples , inci ( lcnttdly , illustrate our treatment of MT divergences -- situations in which It source langlmgc sentence and its target lmlguage translation differ signilicanlly in synlactic structure , syntaclic category , or l ) ret`licale-argun/ent slrttcltnc. No special mechanisms are needed to lreat MT divergences ill oln IllO ( JCL All lhat is needed in order to translate a sentence involving a divergence are source and target language lexical enlries of tile sort ilhlstrated here Ihat mall dil\ ] erent synlaCliC strtlcttnes elite lhe S\ [ llllO TMR. `` File reitreseltlillions i|lld nlechanisnts shown ill the lexical enhies are tllotivaled lor non-divergelll ex : tnlples and do nol have lo lie IllOdifled to deat with divergent examples. This is because source and larger language sentences : .Ire not normally eXl ; Ccled 1o be isomorllhic to tile `` FMR or to each other. Another inlportanI fealuxe of ottr model is that it considers constructions to be basic lexical unils ah ) ng with words. Following Filhnore el al. , 19gg , we deline constructions as ( possibly , discontiguous ) synlaclic structure or produclive synlaclic pattern whose meaning it is often impossible to derive solely based on the meanlings of its components. In other words , a</definiens>
				<definiens id="3">a COlIIbJll , 'tIt ( lit of a syl/laCfiC Sll 'll ( : lln'e : ll/d tile associaled sgln~|n\ [ \ ] c altd pragmatic representations which , once dereliCt`l , tie not have Io lie composiliortally itroduced by a 'I'MP , exIraclof. CotlSlrllCtions are typically ways of expressing it ilteillling that are CttllvenliOll : ll ill tile sense thai they are I'rozcn , lind t/or synchronically deriwdlle from general prhlciplcs , even il ' they once were. Note thltl il Iorlnalisllt Stlch its the I IPSG-IIke siglt of tile dictitlllal+y slrucltlre of tile ACQUILEX project can lie nlatle to SUpF , Oft such an idea , its lqlhnore and Kay ( 1992 ) show. tff Form-Meaning Correslmmlence The MikrokosmcJs project is based on a theory of formmeaning cc , rrcspondcnce , whose underlying assumptions can Im statcd as follows : • Meanings are exlrac/ed from lexls on lho basis of all and any available clues ( e.g. , syntactic , mof\ ] /hological , illltl lexical properties of an ilttelance ) . The exl.la¢liotl of i|leallir~g consists of collslrtlcling Ihe most platlsible , thollgh usually delbasilile , hyl ) othesis that is conlpalible with tile evidence , makhlg il an abt`hlctive process ( Hobbs , 1991 ) . • TIlE processing of clues in Mikrokosmos is grouped lille inicrolheorics for clcnlelllS ill ' Illearlillg SIICll IIS predicatc-afglllllCllt relaliollS , aspect , lelnporill lel ; .llions , modality , evidcltliality , etc. Elicit nlicrolhcory spccilics the ways to COllSlrllcI `` I'MRs for some : .ispccl of nlcallhlg by idcnlifying the Val'it/llS SylIIacIiC , morphologic 'd , : rod lexical clues Ior Ihal element of meillling ill individual lallgtlagcs. • Ill integrating the microtheories , Mikrokc , smos rejects lhe lnu'e slratilicaliona\ [ apprcmch shared by such otherwise diverse models its AI NLF ' semantics ( e.g. , l \ [ irsl , 1987 ) or Mel'~ , uk 's MTM ( e.g. , Mel'~uk , 19el ) . Knowledge from all kinds of areas coexists in tile stone rules for the determinalion of meaning units. • The clues ( pieces of evidence ) for an element of meaning can interact in complex ways. Cities can reinforce or contradict each t ) thel : Coercion is possible in sittlations ill which tile clues conIlicI. Inlcrprctalion o\ [ a clue can be dependent on which other lines are pl'cSellI. 351 e~ 8 v o.~ .~.~ a8 rj ~ O0~J • &amp; ~ ~i .~ ~o ~</definiens>
			</definition>
</paper>

		<paper id="1042">
</paper>

		<paper id="2121">
			<definition id="0">
				<sentence>By checking \ ] exical coheshm between the current word and lexical chains in the order of the salience , in tandem with getmration of lexica\ ] chains~ we realize incretnental word sense disam biguation based on contextual infl ) rmation that lexical chains , reveah Next ; , we &lt; le~ &lt; : ribe how set men &lt; boundaries of a text can be determined with the aid of lexical cohesion. Wc can measure the plausibility of each point in the text as a segment boundary by computing a degree of agreement of the start and end points of lexical chaihs. A text is not a mere set of unrelated sentences. Rather , sentences in a text are about the same thing and connected to each other\ [ l ( ) \ ] . Cohesion and cohere'nee are said to contribute to such connection of the sentences. While coherence is a semantic relationship and needs computationally expensive processing for identification , cohesion is a surface relationship among words iu a text and more accessible than coherence. Cohesion is roughly classitled into reference t , co'r @ tnction , and lezical coh , esion 2. Except conjmwtion that explicitly indicates l ; he relationship between sentences , l ; he other two &lt; : lasses are considered to t &gt; e similar in that the relationship hetweer~ sentences is in &lt; licated by two semantically same ( or related ) words. But lexical nation of segnlent boundaries of the text\ [ 10\ ] . \ ] n this paper , we first describe how word sense ambiguity can t ) e resolved with the aid of lexical cohesion. During the process of generating lexi &lt; 'al chains incrementally , they are recorded in a register in the order of the salience. The salie'ncc of lexical chains is based on their recency and length. Since the more salient lexical chain rep resents the nearby local context , by checking lexi : ca\ [ cohesion between the current word and lexieal chains in the order of tile salience , in tandem with generatiou of lexical chains , we realize incremen. tal word sense disambiguation based on contextual information that lexical chains reveal. Next ; , we describe how segment boundaries of a text can be determined with the aid of lexical cohesion. Since the start and end points of lexical chains it , the text tend to indicate the start and end points of the segment , we can measure the plausibility o\ [ ' each point in the text as a segment boundary by computing a degree of agreement of the sta.rt and end points of lexical chains. 755 Morris and Itirst\ [ 10\ ] pointed out the above two importance of lexical cohesion for discourse analysis and presented a way of computing lexical chains by using Roger 's International Thesaurus\ [ 15\ ] . IIowever , in spite of their mention to the importance , they did not present the way of word sense disambiguation based on lexical cohesion and they only showed the correspondences between lexical chains and segment boundaries by their intuitive analysis. McRoy 's work\ [ 8\ ] can be considered as the one that uses the information of lexical cohesion for word sense disambiguation , but her method does not ; take into account the necessity to arrange lexical chains dynamically. Moreover , her word sense disambignation method based on lexical cohesion is not evaluated fully. In section two we outline what lexical cohesion is. In section three we explain the way of incremental generation of lexical chains in tandem with word sense disambiguation and describe the result of the evaluation of our disambiguation method. In section four we explain the measure of the plausibility of segment boundaries and describe the result of the evaluation of our measure. Consider the following example , which is the English translation of the fragment of one of Japanese texts that we use for the experiment later. In the universe that continues expancb ing , a number of stars have appeared aml disappeared again and again. And about ten billion years after tile birth of the universe , in the same way as the other stars , a primitive galaxy was formed with the primitive sun as the center. Words { nniverse , star , universe , star , galaxy , sun } seem to be semantically same or related to each other and they are included in the same category in Roget 's International Thesaurus. Like Morris and tIirst , we compute such sequences of related words ( lexical chains ) by using a thesaurus as the knowledge base to take into account not only the repetition of the same word but the use of superordinates , subordinates , and synonyms. We. use a Japanese thesaurus 'Bnnruigoihyo'\ [ 1\ ] . Bunrui-goihyo has a similar organization to Roger 's : it consists of 798 categories and has a hierarchical structure above this level. For each word , a list of category numbers which corresponds to its multiple word senses is given. We count a sequence of words which are included in the same category as a lexical chain. It might be ( : lear that this task is computationally trivial. Note that we regard only a sequence of words in the same category as a lexical chain , rather than using the complete Morris and Hirst 's framework with five types of thesaural relations. The word sense of a word can be determined in its context. For example , in the context { universe , star , universe , star , galaxy , sun } , the word 'earth ' has a 'planet ' sense , not a 'ground ' one. As clear from this example , lexical chains ( 'an be used as a contextual aid to resolve word sense ambiguity\ [ 10\ ] . In the generation process of lexical chains , by choosing the lexical chain that the current word is added to , its word sense is determined. Thus , we regard word sense disambiguation as selecting the most likely category number of the thesaurus , as similar to \ [ 16\ ] . l ' ; arlier we proposed incremental disambiguation method that uses intrasentential information , such as selectional restrictions and case frames\ [ l 2\ ] . In the next section , we describe incremental disambiguation method that uses lexical chains as intersentential ( contextual ) information. In the last section , we showed that lexical chains carl play a role of local context , t\ ] owever , multiple lexical chains might cooccur in portions of a text and they might vary in their plausibility as local context. For this reason , for lexical chains to function truly as local context , it is necessary to arrange them in the order of the salience that indicates the degree of tile plausibility. We base the salience on the following two factors : the recency and the length. The more recently updated chains are considered to be the more activated context in the neighborhood and are given more salience. The longer chains are considered to be more about the topic in the neighborhood and are given more salience. By checking lexical cohesion between the cu &gt; rent word and lexical chains in the order of the salience , the lexical chain that is selected to add the current word determines its word sense and plays a role of local context .</sentence>
				<definiendum id="0">cohesion</definiendum>
				<definiendum id="1">lexical chains</definiendum>
				<definiendum id="2">intersentential</definiendum>
				<definiens id="0">a semantic relationship and needs computationally expensive processing for identification</definiens>
			</definition>
</paper>

		<paper id="2206">
			<definition id="0">
				<sentence>A lexical transducer , lirst described by Karttunen , Kaplan , and Zaencn ( KKZ ) ( Karttunen , 1992a ) , is a speeialized tinite state transducer ( FST ) that maps canonical citation forms of words and morphological categories to infh .</sentence>
				<definiendum id="0">lexical transducer</definiendum>
				<definiens id="0">a speeialized tinite state transducer ( FST ) that maps canonical citation forms of words and morphological categories to infh</definiens>
			</definition>
			<definition id="1">
				<sentence>A I , F is a eoncateuation of stems and morphemes in their canonical dictionary representation .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">a eoncateuation of stems and morphemes in their canonical dictionary representation</definiens>
			</definition>
			<definition id="2">
				<sentence>~i~ , ~ , l , ... , o ( , phological alternat , ions : ( i ) the realizatiou of a stem final p in irregular predicates as a vowel in front of vowel-initial suffixes ; ( ii ) let| to-right voweJ harnlony \ [ ) °sell on partitioning of vowels into 'lighl , ' ( \ [ +light\ ] : a , o , oa ) , 'dark ' nnd 'neutral'|l-light ; l ) ; ( iii ) tile realizal ; ion of i~ morpheme boundary as a syllable boun ( lary or as nothing .</sentence>
				<definiendum id="0">syllable boun</definiendum>
				<definiens id="0">l , ... , o ( , phological alternat , ions : ( i ) the realizatiou of a stem final p in irregular predicates as a vowel in front of vowel-initial suffixes</definiens>
			</definition>
			<definition id="3">
				<sentence>n &lt; ling ell ( ; olitex\ [ , . With the hell ) C ' the Xerox two-level rule eolnpiler ( % wolc ' ) ( Karttunen , 1992b ) the rules °.an bc compiled to finil ; e state transducers ~md int ; erseeted to a single trans ( lueer. I ) escribillg reich phenomena as paral\ [ ( '.l rules may be eomplie~t , ed he°°use eaeh rule may be a t'ormul~tion of effed ; s caused by several t ) honologieal rules. For example , i , I f'orlnalizing ( ii ) as a t ; wo-h.'vcl rill ( ; we |nus| , take into aceoun\ [ , bol , h irregular eonjugw t , ion C'-p v ( 'rbs/n ( ljt'ci , ives and vowel harmony. This is a not a desirable state of ~tfl'airs. We will coln ( ~ back t , o this l ) oiut later. Lexical rI~-ansducer ( LT ) The first , st ( q ) in the coustruet ; ion of a lexieM transdueer is to create a simple linite -- state automaton for all wdid k'.xical tbrms of Korean. The lexical aul , omaton ( I , A ) is eomllosed wit ; h l , he first set of rule transducers ( R ; I ' ) . The result ; ing transducer has on its `` Ul/ per '' side , | , he valid lexical forms , and on the `` lower '' si ( le , interm0. ( tiate represenl ; aJ , ions derived fly the lirst set C ' rules. This inl ; ermediate transducer is composed with | , he second set of rule trmlsducers and tim in'o tess is itera| , ed several l ; imes. At each stage ill tit ( ! process , the lexicaI si ( le remains unchanged and the iut , erme ( liate re\ [ ) resenl , atious are changed by the new set C ' rules. The \ [ ilml result is a transducer tim| asso clare 's the valid lexic~d forms with their proper surface realizations. Concel ) tually this is similar to what hap. | ) ells ill a traditional phonologic.~d deriw~tion. Ill ) wever , note thai , rul ( 's a.pply to | , he lexicon as a whole r~ther than 1 , o individual words an ( I ( ; It ( : result ; of e~ ( : h application is ~L new transducer. /~ecaus ( ' th ( `` intermediate levels ( lisa , ° ) pear in the eomposition , the resulting l/l ' is equaJly well suited for morphological aualysis as it is for general ; lolL The compila| , ioll aml int ; ei : seel , ion of rule d ; lNtlls ( ltleers was done with the I.wole eompihw , the cousl ; ruetio , 126.3 of the LA and the compositions we carried out with the Xerox interactive finite-state calculus ( 'ifsm ' ) . ton ( Lh ) The ifsm-utility enabled us to assemble the LA incrementally. The first step was to divide the total list of morphemes into snblexicons on the basis of their morphological type and to make a text file for each sublexieon. We added diacritic markers to the edges of certain types of morphemes in order to be able to enforce morphotactie constraints on valid morpheme sequences. Each sublexicon was compiled separately to a finitestate automaton. The sublexicons were used to con struct the LA with the help of the regular expression facility in the ifsm-toolkit..For example , having compiled a simple automaton from the list ofsm @ le nouns , we could expand it to an infinite lexicon of compound nouns with the regular expression `` noun.auto '' \ [ # `` noun.auto '' l* '\ ] 'his regular expression reads the noun automaton from a file and concatenates it with itself any number of times and marks the internal word boundaries with # . The first version of the LA was made in this way by combining sublexicons with regular operations ( concatenation , union , iteration ) . In order to enforce morphotactic constraints on the concatenation of some classes of snflixes , we wrote a set of two-level rules that require or prohibit the occurfence of particular diacritics at certain suffix boundaries. Lexieal forms that do not satisfy the morphotactic constraints get eliminated in the composition with the well-formedness rules. The diacritics themselves are realized as zero so that they are not present in the lower side of the resulting transducer. The final form of the lexical automaton is obtained by extracting the lower-side from that transducer as a simple automaton. We believe that this incremental method of lexicon construction is better suited to morphologically complex languages than the lexicon format commonly used in two-level morphology. In standard two-level lexicons , individual entries contain intbrmation about which sublexicon they may concatenate with. The entire lexical structure is compiled in one step to large letter tree ( Karttunen , 1993 ; Antworth , 1990 ) . Our method is more tractable in two ways. Firstly , the lexicon can be developed and refined stepwise. Secondly , the morphotactic rules of the language are described explicitly as the regular expressions that construct the LA in conjunction with the well-formedness constraints that eliminate certain types of concatenations. In two-level lexicons of the standard variety , the morphotactic structure of the language is not described explicitly at ; all. l~ , ather , it is expressed in a very opaque and indirect way , in the sequences of links between entries and snblexicons. Sproat argued thai ; two-level morphology of morphotactics leads to a somewhat inelegant model of long-distance dependencies and suggested the unficalion scheme , due to Bear , as a solution ( Sproat , 1992 ) . But unification scheme introduces additional runtime overhead. The above approach can easily and explicitly describe the fact that `` -able '' attaches to verbs formed with the prefix `` en- '' and does not require additional runtime overhead. We give a few examples of the difficulties in the description of Korean morphol ; artics. There are two different types of endings : ( i ) non-tinM ( verbal ) endings for tense , modality , subject honorific or aspect , and ( it ) final ( verbal ) endings as cornplementizer , nominalizer and adjectivizer. The non-tinal endings are placed in fl'ont of final endings and must be followed by a suflix of the second type. ( 4 ) shows the ordering restrictions of non-finM endings. The parentheses indicate optionality. ( 4. ) ( + lion ) ( 4t'~st 4Perf ( 4Will ) I ( + Past ) ( + Will ) ( 4Beta'o ) ) ( Hon : Honorific ; \ ] ~etro : I { ei~rospection ; Perf : Perfect Aspect ) ( 4 ) compiles to a lexicon covering 20 difi &gt; rent compound non-final ending sequenees including null .</sentence>
				<definiendum id="0">ion C'-p v ( 'rbs/n ( ljt'ci</definiendum>
				<definiendum id="1">lexicaI si</definiendum>
				<definiendum id="2">lNtlls</definiendum>
				<definiens id="0">sublexicons were used to con struct the LA with the help of the regular expression facility in the ifsm-toolkit..For example , having compiled a simple automaton from the list ofsm @ le nouns</definiens>
				<definiens id="1">a file and concatenates it with itself any number of times and marks the internal word boundaries with # . The first version of the LA was made in this way by combining sublexicons with regular operations ( concatenation , union</definiens>
				<definiens id="2">( i ) non-tinM ( verbal ) endings for tense , modality , subject honorific or aspect , and ( it ) final ( verbal ) endings as cornplementizer , nominalizer</definiens>
			</definition>
			<definition id="4">
				<sentence>I ' ; ach of the rules iu the lla 'm # ld sl , au ( hu 'd orl , hogr ; q ) hy Imhl\ ] shed iu M { u'ch of 1988 ( l ( or &lt; m M inisl ; ry o\ [ Education , l ! ) 88 ) is descril ) cd in the corresl ) oudiug l , wo level r~fle Sel ) ~t rarely in our inli ) lemcut , ~l , ion. The order of rules Lakes l ; hc roh ' oF rule iutcr~cl ; ions. In this casct~de , qw Mteri , ; ~l , ious described in sccl , iou 2 ~s e×~mqqe ( 3 ) ~\ ] , I : Q split , het , ween three levels : ( s ) ( i ) Rule , q E : .r irregular predicates : A s.ylia , bh~ boumhu : y is introduced be\ [ brc the. stem \ [ ina , I p in irreguhu '' -p vcrbs/~djective , s when ; r vowcl.iuitJal suffix follows. The t'ol lowing morph ( mc homuhlry is deleted ~md p is rcMized as the harmo\ ] li/ , hlg arcldl ) houeme WU 1265 ( ii ) Vowel harmony rules : ( a ) The WU is realized as o if the vowel of the preceding surface syllable is \ [ -flight\ ] , otherwise wu ( b ) The 15 ' is realized as a if the w ) wel of the preceding surfi~ce syllable is \ [ ÷light\ ] , otherwise e. ( iii ) Contraction rules : The morpheme boundary `` ~ '' ( : an optionally be deleted between wu and e. The etfect of these rules with respect to the irregular -p verb cwup 'to pick up ' is shown in ( 9 ) . ( ! t ) ( a ) ... .. 0 p { vWr~ , } + * ' : .~ , -~ ( b ) ... .. WU 0 0 tO.~ , , ( c ) ~ ~ ... . ~ , 0 0 ... .. The intermediate level , ( b ) , is eliminated in the cascade , thus the final lexical transducer maps ( a ) directly to ( e ) . The success of our work on Korean further underscores the point made by KKZ ( Karttnncn , 1992a ) thai ; the most salient property of two-level morphology is not the number of levels but the fact two-level rules describe regular relations ( just like classical phonological rewrite rnles ) ( Kaplan , 1988 ; Ritehie , 1992 ) . Consequently , it is possible to combine sets of parallel twolevel rules by intersection and merge them with the lexicon and other rule systems in a cascade , of compositions. The complexities of Korean morphology make it desirable both tbr linguistic and computational reasons to allow for many more intermediate levels than assumed in previous works on English and t , ~reneh , l { .egardless of the nnmber of intervening levels , the outcome is a single lexieal transduce.r thaw directly maps lexical forms to their intlected surface realizations , and vice versa. In the construction of the lexieal automaton for Ko-. rean , we have put two-level rules to a novel use as wellformedness constraints on lexical tbrms. The sublexicons from which the LA is constructed contain ( tiacritic marks on the outer edge that identify the type of morphoh ) gical constituents that the lexicon contains. The role of rules in the \ [ , A constructions is to enforce morphotaetics and , at the same time , to eliminate the diacritics that encode them. Theoretically , we can get the same LT to compose the morphotactic and phonological &gt; ties all together into one .</sentence>
				<definiendum id="0">hc roh</definiendum>
				<definiens id="0">s ) ( i ) Rule , q E : .r irregular predicates : A s.ylia , bh~ boumhu : y is introduced be\ [ brc the. stem \ [ ina</definiens>
			</definition>
</paper>

		<paper id="2146">
			<definition id="0">
				<sentence>ECA has a two-level storage , with a two-way linear '' list on each level composed of data items ( fields , cells ) .</sentence>
				<definiendum id="0">ECA</definiendum>
			</definition>
			<definition id="1">
				<sentence>rs ( P , w ) = { w I , where w I is a word , which is a result of one cycle performed on the word w by the automaton P } 907 We can see that the following two facts hold , due to the restarting nature of computations of P : Fact 1 : Let w be a word fi'om L ( P ) , then rs ( P , w ) c~ L ( P ) ve Q. Fact 2 : If w is not a word fi'om L ( P ) , then rs ( P , w ) n L ( P ) = Q. Two basic principles how to formulate rules for the automaton P for a given natural language L lk~llow from the above mentioned facts : 1 ) P contains only those ( deleting ) rules , for which there exists a sentence in L which will remain syntactically correct after the application of the rule .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">P , w ) n L ( P ) = Q. Two basic principles how to formulate rules for the automaton</definiens>
			</definition>
</paper>

		<paper id="1085">
			<definition id="0">
				<sentence>A user is reading a document in a bitmap browser ( left panel ) , and comes across a topic of interest .</sentence>
				<definiendum id="0">user</definiendum>
				<definiens id="0">reading a document in a bitmap browser ( left panel ) , and comes across a topic of interest</definiens>
			</definition>
			<definition id="1">
				<sentence>The bottom image is the sum of all 13 instances .</sentence>
				<definiendum id="0">bottom image</definiendum>
			</definition>
</paper>

		<paper id="2165">
			<definition id="0">
				<sentence>A collocation consists of a word and the word that immediate @ follows .</sentence>
				<definiendum id="0">collocation</definiendum>
				<definiens id="0">consists of a word and the word that immediate @ follows</definiens>
			</definition>
			<definition id="1">
				<sentence>The mutual information ratio ( in Steier &amp; Belew , 1991 ) is expressed : , Formula 1 : The mutual information ratio where 'p ' defines the probability function , p ( \ [ wl ... w2\ ] ) is read as `` the probability of finding word w2 after word wl '' .</sentence>
				<definiendum id="0">mutual information ratio</definiendum>
				<definiens id="0">the probability of finding word w2 after word wl ''</definiens>
			</definition>
			<definition id="2">
				<sentence>F ( wx ) is the frequency of word Wx .</sentence>
				<definiendum id="0">F ( wx )</definiendum>
			</definition>
</paper>

		<paper id="1092">
			<definition id="0">
				<sentence>The whole 200 million word col pns is being annotated morphologically and syntactically during 1993-94 at the Research Unit for Cor , ,Imtational Linguistics ( IL/I ( 3L ) , University of Ilelsinkl , using the Fmglish nmrphological analyser ( ENC , TW ( ) I , ) and English Constraint ( : h'ammar ( EN ( : I ( : . '</sentence>
				<definiendum id="0">ENC , TW (</definiendum>
				<definiendum id="1">EN</definiendum>
				<definiens id="0">being annotated morphologically and syntactically during 1993-94 at the Research Unit for Cor</definiens>
			</definition>
			<definition id="1">
				<sentence>I'~N ( ~I ( , 'G has been developed so that it , takes lute ; tccotllll , Variolls Lexttl , 'd codhlg cotwelfl ; iofts \ [ l ( al'\ ] sson , 1994\ ] .</sentence>
				<definiendum id="0">I'~N ( ~I</definiendum>
				<definiens id="0">'G has been developed so that it , takes lute</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>entry corlllnents Mizenkei I ( Irrealis l : orm ) Renyookei ( Adverbial Form ) Rentaikei ( Attribntive Fom~ ) Shuushikei ( Basic Form ) Kateikei ( Hypothetical Form ) Meireikei ( Imperative Form ) Mizenkei 2 ( Cohortative Form ' Negative Causative Passive Past Table S Examples of Lexical Entries ( SG-I ) If SG-I is employed , an additional 11652 allomorphs requires to be registered in our dictionary to cope with Katsuyou transformation of regular verbs .</sentence>
				<definiendum id="0">entry corlllnents Mizenkei I</definiendum>
			</definition>
			<definition id="1">
				<sentence>m k : Morpheme Figm-e l Illustration of Chart Parsing Here s denotes an input string , r~ ' '' o '' n. A candidate-word lattice { Ml , ' '' , Mn } is used for recording candidate morphelnes , where MJ records the morphemes extracted at position j. Partial path lists { T1 , ' '' , T n } are used for recording the fragments of partial solutions , where Tj contains fragments of partial solutions which reach the j-th position in s. An element in T k ( 1- &lt; 2_k &lt; n ) has the form { m , C , { &lt; mr , C1 &gt; , ... &lt; mr , , Ck &gt; } } where m is the last morpheme of partial solutions al , `` '' a k , C is their common cost , and &lt; mj , C j &gt; is the preceding morpheme of m at a , ; j { &lt; m ,1 Cl &gt; , `` ' , &lt; m , k Ck &gt; } is regarded as a `` pointer '' for tracing solutions backward .</sentence>
				<definiendum id="0">MJ</definiendum>
				<definiendum id="1">Tj</definiendum>
				<definiendum id="2">m</definiendum>
				<definiens id="0">an input string , r~ ' '' o '' n. A candidate-word lattice { Ml , ' ''</definiens>
				<definiens id="1">records the morphemes extracted at position j. Partial path lists { T1 , ' ''</definiens>
				<definiens id="2">contains fragments of partial solutions which reach the j-th position in s. An element in T k ( 1- &lt; 2_k &lt; n ) has the form { m</definiens>
			</definition>
			<definition id="2">
				<sentence>Figure 4 ( a ) illustrates the linguistically possible incidence between our lexical entries including a verbal stem v. To construct a probabilistic likelihood function , one needs to estimate all of the free parameters Pwv , where Pwv denotes the transition probability from word w to v. Since a verbal stem can succeed almost all grammatical categories , the number of parameters { Pwv } ( = N ( v ) ) is almost equal to the number of all categories .</sentence>
				<definiendum id="0">Pwv</definiendum>
				<definiens id="0">entries including a verbal stem v. To construct a probabilistic likelihood function , one needs to estimate all of the free parameters Pwv , where</definiens>
				<definiens id="1">the transition probability from word w to v. Since a verbal stem can succeed almost all grammatical categories , the number of parameters { Pwv } ( = N</definiens>
			</definition>
</paper>

		<paper id="2156">
			<definition id="0">
				<sentence>field Extracting the prommciation field from an MRD is one of the most obvious uses of a dictionary .</sentence>
				<definiendum id="0">MRD</definiendum>
			</definition>
			<definition id="1">
				<sentence>A run-on is defined as a morphological variant of a headword , included in the entry .</sentence>
				<definiendum id="0">run-on</definiendum>
				<definiens id="0">a morphological variant of a headword , included in the entry</definiens>
			</definition>
			<definition id="2">
				<sentence>A per tion of the llansard French corpus ( over 2.3 million words ) wa .</sentence>
				<definiendum id="0">per tion</definiendum>
				<definiens id="0">of the llansard French corpus ( over 2.3 million words</definiens>
			</definition>
</paper>

		<paper id="2208">
			<definition id="0">
				<sentence>The Hungarian morphological analyzer which is the lalgest and most precise implementation needs 900 Kbytes disk space and around 100 Kbytes of core memory .</sentence>
				<definiendum id="0">Hungarian morphological analyzer</definiendum>
				<definiens id="0">the lalgest and most precise implementation needs 900 Kbytes disk space and around 100 Kbytes of core memory</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>A PKS consists of preference infi~rmatitm on word sense , l ) hrasal attachment , and word selection for translation .</sentence>
				<definiendum id="0">PKS</definiendum>
				<definiens id="0">consists of preference infi~rmatitm on word sense , l ) hrasal attachment , and word selection for translation</definiens>
			</definition>
			<definition id="1">
				<sentence>ses ( the modifier and modifiee phrases ) had to match the rule exactly .</sentence>
				<definiendum id="0">ses</definiendum>
				<definiens id="0">the modifier and modifiee phrases ) had to match the rule exactly</definiens>
			</definition>
			<definition id="2">
				<sentence>nlllei|ts consist of four IBM AS/400 computer IuanllalS with about , 2'2,000 sentences and a CAD manual with about 10,000 sentences .</sentence>
				<definiendum id="0">nlllei|ts</definiendum>
				<definiens id="0">consist of four IBM AS/400 computer IuanllalS with about , 2'2,000 sentences and a CAD manual with about 10,000 sentences</definiens>
			</definition>
</paper>

		<paper id="2144">
			<definition id="0">
				<sentence>The mMn idea of representii~g as much linguistic knowledge as possit ) h~ through a mfique dater type &lt; died fi'atu*'e struetur , , Cdlows the inl , egl : ,ttion of differenl ; des ( : l ; i 1 ) lion levels withoul , taking care of interface probh ! lnS. While the tirst N ) l ) roaehes relied ( m almotate ( t I ) hrase sLIJll ( : l ; lll ? e FIlles ( e.g. , PAr\ [ '\ [ { , • \ [ l ) , 1110 ( 1 ( 21711 formalisms try l , o specify grmnmal , ical knowledge as well as lexicon entries entirely through feature , sLruetures , h , order to au : hieve t , his goal , one must enrich the exl ) ressive power of the lirst imilication-based formMisms with different forms of disjunctive deseril ) tions , l , ater , oLher operations came int , o play , e.g. , ( classical ) negation. Ol , her proposals consider the integration of funcl ; ionM/relatiomd del ) endencies into t , he \ [ brmMism which make them in gelleral 'l'uriug-c.omph2te ( ( e.g. , AI , I ' ; \ [ 4\ ] ) . However l , he mosl ; important ext ( msion 1 ; o \ [ 'ormalisms eonsisl ; s of tire incorporation of types , lbl : instance in modern systems like TI '' S \ [ 15\ ] , CUI e \ [ tl\ ] , or `` FD£ \ [ 7\ ] . Types are ordered hierm : ehically its it is known front ol ) jeet-oriented t ) rogranmdng languages. This leads l , o multiple inheritlmee in the description of linguistic entities. Finally , reeursive types are nee-. essary 1 ; o describe at lelust phrase-structure rccursion which is inherent in all gramnta.r ibrmalisms which are nol ; l ) rovidcd with a context-free loaekbone. ht the next section , we argue for the need and rel-. evmtce of using types in CL and Nl , l ) . AO ; er that , we give an overview of 7 '' 1 ) £ and il , s specialized inl~ : rence modules. EspeeiMly , we have it closer look ( ) it the novel features of J'D.~ and ltresenl , the techniques we h~tve employed in iulltlein ( mLing `` ~1 ) £ , Mode.rn tylmd mdtieaJ , ion-.ba , sed granunax foruudimns differ from etMy unt , yped systcnis in that they high-light tile notion of a fi ! ature type. Tyl ) es C~L\ [ 1 t ) e 3,1'ranged hierarchically , where a subtype inherits monotonicMly all the inlbrmation frolu its supertypes and unification plays l ; he role of the primary in % rm~ttioncoml ) imng operation. A tN ) e definition elm be seen as ; m M ) breviation for ~ ( : Oml~lex exl ) ressi ( m , ( : ( msisl.ing of I.ype eonstraiuts ( eoncerning the sub-/sup ( ~rl.yp ( : rehLtionship ) ; rod feature constraints ( stat , ing the : ~1 ' propriate M , i ; ribut.es and t ; he , ir values ) over the c ( m ueet.ives A , V , and - , . Types serve its abbrcvi~tfions Ibr lexicon e , ntries , 11 ) rule s ( : helu ; d. ; 'L , and mfiv ( ~rsa.I its well as kmguage-specilic principles as is l'amilinr Doln IlPSG. Ih~sides using Lyt ) cs as an abl ) revia|.ion~ \ [ tl lt| ( ! ~ltS ~tS temlthd , es rare , I , } lere are o| , hel ; ~t ( lv~ttlliages as well which cmmot be a ( : ( ; Oml ) lished by te.nl ) la.i.es : • STRU ( JTUIt.ING KNOWI , b ; D ( ' , I ! ; Types together with the l ) ossibility to order then , hier ; u'ehieally allow for a luodul ; u '' aHd ele~m way to r , ~l ) rcs ( '.nl , lingulsLic kuowle ( lge nd equ~d , ely. Moreow : r , generalizntions can be put , a.t the apl ) l'Ol~ri ; d , e h : vels of re.13resenl ; atioti. • I , ~FFI ( 'IENT I'I { , OCI , 'SSIN ( I Certain I , yl ) e eonsLrainl , s ( ; all I ) e ( : ompih~d iltl , o el : ficient represenl , al ; ious like. bit veeLors \ [ I\ ] , where a ( l\ [ , l/ ( grcgd ; esL h ) wer bOUlld ) , L \ [ Jl~ , ( leaM ; Ul ) p ( ~r I ) , mnd ) , or a , ~ ( Lyl , e s , d ) SUml ) liot 0 eOmlml ; m , iot~ reduces to low-h'vel bit Inanilmlatio , i ; see Seel , iou fi'om eXlmnSive COmlmI.M ; iou through lhe i ) ossi bility to declare them incoml ) al ; ilde , lu iuhligi ( m , working with t.yf ) e ua.mes only or with partiMly expanded l ; ypes minimizes the costs of copying sl , ruet ; ures during processing. 'Phis can only be a.ccomplished i\ [ ' the sysLent m~ukes at Uleeh ; LILiSln for type exlmnsion available ; see Se ( : l , ion ; L4. • TYPE ( JIIECKIN ( I 'Fype deliniti ( ms allow n gramm~riml to ( leelar ( ~ which attributes are al ) l ) rOl ) riate lkq ' a given l.yl ) e and which types m : e a.l ) prol ) riate for a given at.. tribute , therelb.'e disallowiug one to write il~ ( : ( m sistent , feat , m'e structures. Again , type expansioll is necess ; try to determine the glol ) M etmsist , eney of it given description. • RECIJltSIVI , \ ] TYI'ES l { ecursive l , ypes give it glmlmnar writ.or the opporl.unity to formulnl.e cerl.Mn fimel.ion.s or re-lations as recm'sivc type specific. ; ttions. \York ing in the type deduel , io|l i ) ~-tra ( ligm el\ ] i'orecs a , grammar writer 1 , o rel ) la ( : e the eonl ; exl ; ..fl'ee back. 89,3 bone through recursive types. Here , parameterized delayed type expansion is the ticket to the world of controlled linguistic deduction \ [ 13\ ] ; see Section 3.4. TDZ : is a unificatiol , -based grammar development environment and run time system snpporting HPSGlike grammars. Work on TD£ has started within the DISCO project of the DFKI \ [ 14\ ] ( this volume ) . The DISCO grammar currently consists of approx. 900 type specifications written in TD£ and is the largest HPSG grammar for German \ [ 9\ ] . The core engine of DISCO consists of T/I£ and the feature constraint solver //D/A~ \ [ 3\ ] . ND/~ itself is a powerful untyped unification machinery which allows the use of distributed disjunctions , general negation , and fllnctional dependencies. The modules communicate through an interface , and this connection mirrors exactly the way an abstract typed unification algorithm works : two typed feature structures can only be unified if the attached types are definitely compatible. This is accomplished by the unifier in that ~ handles over two typed feature structures to TD£ which gives back a simplified form ( plus additional information ; see Fig. 1 ) . The motivation for separating type and featnre constraints and processing them in specialized modules ( which again might consist of specialized components as is the case in 73 ) £ ) is twofold : ( i ) this strategy reduces the complexity of tile whole system , thus making the architecture clear , and ( ii ) leads to a higher performance of the whole system because every module is designed to cover only a specialized task. 7 '' D£ supports type definitions consisting of type constraints and feature constraints over the operators A , V , -1 , and ® ( xor ) . The operators are generalized in that they can connect feature descriptions , coreference tags ( logical variables ) as well as types. 77 ) £ distinguishes between arm types ( open-world semantics ) , sort types ( closed-world semantics ) , built-in types ( being made available by the underlying COMMON LISP system ) , and atoms. Recursive types are explicitly allowed and handled by a sophisticated lazy type expansion mechanism. In asking for the greatest lower bound of two awn types a and b which share no common subtype , TD£ always returns a A b ( open-world reasoning ) , and not 2_. The reason for assuming this is manifold : ( i ) partiality of our linguistic knowledge , ( ii ) approach is in harmony with terminological ( KL-ONE-like ) languages which share a similar semantics , ( iii ) important during incremental grammar/lexicon construetion ( which has been shown usefid in our project ) , and ( iv ) one must not write superfluous type definitions to guarantee successful type unifications during processing. The opposite case holds for the C , LB of sort types ( closed-world approach ) . Furthermore , sort types differ in another point from avm types in that they arc not fllrther structured , as is the case for atoIns. Moreover , 779£ oilers the possibility to declare partitions , a feature heavily used in IfPSG. In addition , one can declare sets of types as incompatible , meaning that the conjunction of them yields ± , so that specific avm types can be closed. 7 '' D£ allows a grammarian to define and use parameterized templates ( macros ) . There exists a special instance definition facility to ease the writing of lexicon entries which differ from nor , hal types in that they are not entered into the type hierarchy. Input given to TD£ is parsed by a Zebu-generated LALR ( 1 ) parser \ [ 8\ ] to allow for an intuitive , hi9h-level input syntax and to abstract fi'om uninteresting details imposed by the unifier and the underlying Lisp systenr. The kernel of TD£-. ( and of most other monotonic systems ) can be given a set-theoretical semantics Mong the lines of \ [ 12\ ] . It is easy to translate TD£. statements into denotation-preserving expressions of Smolka 's feature logic , thus viewing 7 '' D£ only as syntactic sugar for a restricted ( decidable ) subset of firstorder logic. Take for instance the following feature description O written as an attribute-vMue matrix : np \ [ agr'eement \ ] ¢= A~\ [ \ ] NUM sO PERS 3rd SUBJ \ [ \ ] It is not hard to rewrite this two-dimensionM description to a flat first-order formula , where attributes/features ( e.g. , .~GR ) are interpreted as binary relations and types ( e.g. , up ) as unary predicates : 3~. , ~p ( ¢ ) A , Ga ( e , , ~ ) A , ,a , °~em~ , ,t ( ~ ) A RUM ( x , sg ) A PERS ( x , o°7 '' ( 1 ) A SUBJ ( ¢ , x ) The corresponding VD£ type definition of ¢ looks as follows ( actually &amp; ; is used on the keyboard instead of A , \ [ instead of V , ~instead of ~ ) : ¢ : = np A \ [ AGR # x A agreement A \ [ NUll st , PERS at'd\ ] , SUBJ # ~\ ] . The type hierarchy is either called directly by the control machinery of TD£ during the detinition of a type ( type classification ) or indirectly via the simplitier both at definition and at run time ( type unification ) . The implementation of the type hierarchy is based on A'/t-Kaci 's encoding technique for partial orders \ [ 1\ ] . Every type t is assigned a code 7 ( t ) ( represented via a bit vector ) such that 7 ( 0 reflects tile reflexive transitive closure of the subsumption relation with respect to t. Decoding a code c is realized either by a look-up OFF 3t . 7-1 ( c ) = t ) or by computing the `` maximal restriction '' of the set of types whose codes are less than c. l ) eper , ding on the encoding method , the hierarchy occupies O ( n logn ) ( compact encoding ) resp. O ( n 2 ) ( transitive closure encoding ) bits. ltere , GLB/LUB operations directly correspond to bit-or/and instructions. GI , B , I , UB and ~ computations 1-1ave the nice property that they can be carried out in this tYamework in O ( n ) , where n is the 894 ~\ ] , \ [ ... \ ] ... ... ... .. _ &gt; Query { ~ 1 , \ [ ... 1 up/ .</sentence>
				<definiendum id="0">mMn idea of representii~g</definiendum>
				<definiendum id="1">lingulsLic kuowle</definiendum>
				<definiendum id="2">RUM</definiendum>
				<definiendum id="3">PERS</definiendum>
				<definiendum id="4">SUBJ</definiendum>
				<definiendum id="5">type hierarchy</definiendum>
				<definiendum id="6">n</definiendum>
				<definiens id="0">much linguistic knowledge as possit ) h~ through a mfique dater type &lt; died fi'atu*'e struetur , , Cdlows the inl , egl : ,ttion of differenl ; des ( : l ; i 1 ) lion levels withoul , taking care of interface probh</definiens>
				<definiens id="1">1 ( 21711 formalisms try l , o specify grmnmal , ical knowledge as well as lexicon entries entirely through feature , sLruetures , h , order to au : hieve t , his goal , one must enrich the exl ) ressive power of the lirst imilication-based formMisms with different forms of disjunctive deseril ) tions , l , ater , oLher operations came int , o play , e.g. , ( classical ) negation. Ol , her proposals consider the integration of funcl ; ionM/relatiomd del ) endencies into t , he \ [ brmMism which make them in gelleral 'l'uriug-c.omph2te ( ( e.g. , AI , I ' ; \ [ 4\ ] ) . However l , he mosl ; important ext ( msion 1 ; o \ [ 'ormalisms eonsisl ; s of tire incorporation of types , lbl : instance in modern systems like TI '' S \ [ 15\ ] , CUI e \ [ tl\ ] , or `` FD£ \ [ 7\ ] . Types are ordered hierm : ehically its it is known front ol ) jeet-oriented t ) rogranmdng languages. This leads l , o multiple inheritlmee in the description of linguistic entities. Finally , reeursive types are nee-. essary 1 ; o describe at lelust phrase-structure rccursion which is inherent in all gramnta.r ibrmalisms which are nol ; l ) rovidcd with a context-free loaekbone. ht the next section , we argue for the need and rel-. evmtce of using types in CL and Nl , l ) . AO ; er that , we give an overview of 7 '' 1 ) £ and il , s specialized inl~ : rence modules. EspeeiMly , we have it closer look ( ) it the novel features of J'D.~ and ltresenl , the techniques we h~tve employed in iulltlein ( mLing `` ~1 ) £ , Mode.rn tylmd mdtieaJ , ion-.ba , sed granunax foruudimns differ from etMy unt , yped systcnis in that they high-light tile notion of a fi ! ature type. Tyl ) es C~L\ [ 1 t ) e 3,1'ranged hierarchically , where a subtype inherits monotonicMly all the inlbrmation frolu its supertypes and unification plays l ; he role of the primary in % rm~ttioncoml ) imng operation. A tN ) e definition elm be seen as ; m M ) breviation for ~ ( : Oml~lex exl ) ressi ( m , ( : ( msisl.ing of I.ype eonstraiuts ( eoncerning the sub-/sup ( ~rl.yp ( : rehLtionship ) ; rod feature constraints ( stat , ing the : ~1 ' propriate M , i ; ribut.es and t ; he , ir values ) over the c ( m ueet.ives A , V , and - , . Types serve its abbrcvi~tfions Ibr lexicon e , ntries , 11 ) rule s ( : helu ; d. ; 'L , and mfiv ( ~rsa.I its well as kmguage-specilic principles as is l'amilinr Doln IlPSG. Ih~sides using Lyt ) cs as an abl ) revia|.ion~ \</definiens>
				<definiens id="2">I Certain I , yl ) e eonsLrainl , s ( ; all I ) e ( : ompih~d iltl , o el : ficient represenl , al ; ious like. bit veeLors \ [ I\ ] , where a ( l\ [ , l/ ( grcgd ; esL h ) wer bOUlld ) , L \ [ Jl~ , ( leaM ; Ul ) p ( ~r I ) , mnd ) , or a , ~ ( Lyl , e s , d ) SUml ) liot 0 eOmlml ; m , iot~ reduces to low-h'vel bit Inanilmlatio , i ; see Seel , iou fi'om eXlmnSive COmlmI.M ; iou through lhe i ) ossi bility to declare them incoml ) al ; ilde , lu iuhligi ( m , working with t.yf ) e ua.mes only or with partiMly expanded l ; ypes minimizes the costs of copying sl , ruet ; ures during processing. 'Phis can only be a.ccomplished i\ [ ' the sysLent m~ukes at Uleeh</definiens>
				<definiens id="3">ms allow n gramm~riml to ( leelar ( ~ which attributes are al ) l ) rOl ) riate lkq ' a given l.yl ) e and which types m : e a.l ) prol ) riate for a given at.. tribute , therelb.'e disallowiug one to write il~ ( : ( m sistent , feat , m'e structures. Again , type expansioll is necess ; try to determine the glol ) M etmsist , eney of it given description. • RECIJltSIVI</definiens>
				<definiens id="4">a unificatiol , -based grammar development environment and run time system snpporting HPSGlike grammars. Work on TD£ has started within the DISCO project of the DFKI \ [ 14\ ] ( this volume ) . The DISCO grammar currently consists of approx. 900 type specifications written in TD£ and is the largest HPSG grammar for German \ [ 9\ ] . The core engine of DISCO consists of T/I£ and the feature constraint solver //D/A~ \ [ 3\ ] . ND/~ itself is a powerful untyped unification machinery which allows the use of distributed disjunctions , general negation , and fllnctional dependencies. The modules communicate through an interface , and this connection mirrors exactly the way an abstract typed unification algorithm works : two typed feature structures can only be unified if the attached types are definitely compatible. This is accomplished by the unifier in that ~ handles over two typed feature structures to TD£ which gives back a simplified form ( plus additional information ; see Fig. 1 ) . The motivation for separating type and featnre constraints and processing them in specialized modules ( which again might consist of specialized components as is the case in 73 ) £ ) is twofold : ( i ) this strategy reduces the complexity of tile whole system , thus making the architecture clear , and ( ii ) leads to a higher performance of the whole system because every module is designed to cover only a specialized task. 7 '' D£ supports type definitions consisting of type constraints and feature constraints over the operators A , V , -1 , and ® ( xor ) . The operators are generalized in that they can connect feature descriptions , coreference tags ( logical variables ) as well as types. 77 ) £ distinguishes between arm types ( open-world semantics ) , sort types ( closed-world semantics ) , built-in types ( being made available by the underlying COMMON LISP system ) , and atoms. Recursive types are explicitly allowed and handled by a sophisticated lazy type expansion mechanism. In asking for the greatest lower bound of two awn types a and b which share no common subtype , TD£ always returns a A b ( open-world reasoning )</definiens>
				<definiens id="5">linguistic knowledge , ( ii ) approach is in harmony with terminological ( KL-ONE-like ) languages which share a similar semantics , ( iii ) important during incremental grammar/lexicon construetion ( which has been shown usefid in our project ) , and ( iv ) one must not write superfluous type definitions to guarantee successful type unifications during processing. The opposite case holds for the C , LB of sort types ( closed-world approach ) . Furthermore , sort types differ in another point from avm types in that they arc not fllrther structured</definiens>
				<definiens id="6">incompatible , meaning that the conjunction of them yields ± , so that specific avm types can be closed. 7 '' D£ allows a grammarian to define and use parameterized templates ( macros ) . There exists a special instance definition facility to ease the writing of lexicon entries which differ from nor , hal types in that they are not entered into the type hierarchy. Input given to TD£ is parsed by a Zebu-generated LALR ( 1 ) parser \ [ 8\ ] to allow for an intuitive , hi9h-level input syntax and to abstract fi'om uninteresting details imposed by the unifier and the underlying Lisp systenr. The kernel of TD£-. ( and of most other monotonic systems ) can be given a set-theoretical semantics Mong the lines of \ [ 12\ ] . It is easy to translate TD£. statements into denotation-preserving expressions of Smolka 's feature logic</definiens>
				<definiens id="7">an attribute-vMue matrix : np \ [ agr'eement \ ] ¢= A~\ [ \ ] NUM sO PERS 3rd SUBJ \ [ \ ] It is not hard to rewrite this two-dimensionM description to a flat first-order formula , where attributes/features ( e.g. , .~GR ) are interpreted as binary relations and types ( e.g. , up</definiens>
				<definiens id="8">looks as follows ( actually &amp; ; is used on the keyboard instead of A , \ [ instead of V , ~instead of ~ ) : ¢ : = np A \ [ AGR # x A agreement A \ [ NUll st</definiens>
				<definiens id="9">either called directly by the control machinery of TD£ during the detinition of a type ( type classification ) or indirectly via the simplitier both at definition and at run time ( type unification ) . The implementation of the type hierarchy is based on A'/t-Kaci 's encoding technique for partial orders \ [ 1\ ] . Every type t is assigned a code 7 ( t ) ( represented via a bit vector ) such that 7 ( 0 reflects tile reflexive transitive closure of the subsumption relation with respect to t. Decoding a code c is realized either by a look-up OFF 3t . 7-1 ( c ) = t ) or by computing the `` maximal restriction '' of the set of types whose codes are less than c. l ) eper , ding on the encoding method , the hierarchy occupies O ( n logn ) ( compact encoding ) resp. O ( n 2 ) ( transitive closure encoding ) bits. ltere , GLB/LUB operations directly correspond to bit-or/and instructions. GI , B , I</definiens>
			</definition>
			<definition id="1">
				<sentence>Exl ) anding append with no additional information supplied ( especiMly no path leading inside appcndl , e.g. , PATCH I PATCH I PATCH ) yields a disjunctive feature structure where both append o and append I are substituted by their definitiorl .</sentence>
				<definiendum id="0">especiMly</definiendum>
				<definiens id="0">no path leading inside appcndl</definiens>
			</definition>
			<definition id="2">
				<sentence>|'he type system i~ , self consists of several inference components , each designed to cover etficiently a specific task : ( i ) a tilt vector encoding o\ [ the hierarchy , ( ii ) a fast symbolic simplilier for complex type expressions , ( iii ) memo ization t ; ( 7 cache preeomI ) uted results , and ( iv ) a sophisticated type expansion nmchanism .</sentence>
				<definiendum id="0">self</definiendum>
				<definiens id="0">consists of several inference components , each designed to cover etficiently a specific task : ( i ) a tilt vector encoding o\ [ the hierarchy</definiens>
			</definition>
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>Eilicient reasoning in the system is accomplished through specialized modules : ( i ) bit vector encoding of the type subsumption hierarchy ; ( it ) f~ust symbolic simplification for complex type expressions ; ( iii ) memoization to cache precomputed results ; and ( iv ) type expansion to make constraints explicit , to determine the global satisfiability of a description , and to work with partially expanded types during processing .</sentence>
				<definiendum id="0">Eilicient reasoning</definiendum>
				<definiens id="0">expansion to make constraints explicit , to determine the global satisfiability of a description , and to work with partially expanded types during processing</definiens>
			</definition>
			<definition id="1">
				<sentence>Tile core of tile linguistic resources consists of a twolevel morphology with feature constraints , all Ill ) S ( \ ] oriented grammar of German with integrated syntax and semantics , and a module for surface speech act recognition , all implemented ill 7 '' D/2 .</sentence>
				<definiendum id="0">Tile core of tile linguistic resources</definiendum>
				<definiens id="0">consists of a twolevel morphology with feature constraints , all Ill ) S ( \ ] oriented grammar of German with integrated syntax and semantics , and a module for surface speech act recognition , all implemented ill 7 '' D/2</definiens>
			</definition>
			<definition id="2">
				<sentence>The diagnostics tool ( DiTo ) \ [ 11\ ] containing close to 1500 annotated diagnostic sentences of German fimilitates consistency maintenance and measuring of con &gt; petenee .</sentence>
				<definiendum id="0">diagnostics tool</definiendum>
				<definiens id="0">DiTo ) \ [ 11\ ] containing close to 1500 annotated diagnostic sentences of German fimilitates consistency maintenance and measuring of con &gt; petenee</definiens>
			</definition>
			<definition id="3">
				<sentence>Each COSMA instance functions as a personal secretarial assistant providing the following services : ( i ) storage and organization of a personal aplmint ment date-book ; ( ii ) graphical display and manil ) ulation of appointment data ; and ( iii ) natural language understanding and generation in communication with other agents via electronic mail .</sentence>
				<definiendum id="0">COSMA instance functions</definiendum>
				<definiens id="0">graphical display and manil ) ulation of appointment data ; and ( iii ) natural language understanding and generation in communication with other agents via electronic mail</definiens>
			</definition>
			<definition id="4">
				<sentence>The translation process ( maintaining the substantial difference in expressive power between A/'££ and the restricted planner language ) builds on ideas from current compiler technology with a limited set of domainand application-specific inference rules \ [ 10\ ] .</sentence>
				<definiendum id="0">translation process</definiendum>
				<definiens id="0">maintaining the substantial difference in expressive power between A/'££ and the restricted planner language</definiens>
			</definition>
</paper>

		<paper id="2181">
			<definition id="0">
				<sentence>However , DRT lacks a deductive ( prooftheoretic ) formulation to serve as an abstract interpreter for discourse understanding , since it is formalized by means of the notion of partial models .</sentence>
				<definiendum id="0">DRT</definiendum>
				<definiens id="0">lacks a deductive ( prooftheoretic ) formulation to serve as an abstract interpreter for discourse understanding</definiens>
			</definition>
			<definition id="1">
				<sentence>A discourse representation ( DR ) K is expressed as a pair ( UK , ConE ) , where UE is a set of discourse re\ ] erents , and Conic is a set of conditions .</sentence>
				<definiendum id="0">discourse representation ( DR ) K</definiendum>
				<definiendum id="1">UE</definiendum>
				<definiendum id="2">Conic</definiendum>
				<definiens id="0">a set of discourse re\ ] erents</definiens>
			</definition>
			<definition id="2">
				<sentence>Complex conditions are of the form : K1 : ~ K2 , KI V K2 or ~K1 , where both K1 and K2 are Dl~s. A discourse representation structure ( DRS ) is a partially ordered set of DRs , which can be constructed by means of DRS construction rules whose application reflects the syntactic composition of the sentences in the discourse .</sentence>
				<definiendum id="0">DRS )</definiendum>
				<definiens id="0">a partially ordered set of DRs , which can be constructed by means of DRS construction rules whose application reflects the syntactic composition of the sentences in the discourse</definiens>
			</definition>
			<definition id="3">
				<sentence>When each DR of a DRS is maximal , the DRS is called a complete DRS. Intuitively speaking , each stage in the construction algorithm can be viewed as a partial possible worlds , in which more information resulting from the processing of a further bit of the discourse changes it into a more precise description of the world .</sentence>
				<definiendum id="0">DRS</definiendum>
				<definiens id="0">maximal , the</definiens>
				<definiens id="1">a partial possible worlds , in which more information resulting from the processing of a further bit of the discourse changes it into a more precise description of the world</definiens>
			</definition>
			<definition id="4">
				<sentence>A model for DRL is an ordered pair ( DM , FM ) , where DM is the domain of M and FM is an interpretation function of constants and predicates .</sentence>
				<definiendum id="0">model for DRL</definiendum>
				<definiendum id="1">FM</definiendum>
				<definiendum id="2">DM</definiendum>
				<definiendum id="3">FM</definiendum>
				<definiens id="0">an ordered pair ( DM ,</definiens>
				<definiens id="1">the domain of M and</definiens>
				<definiens id="2">an interpretation function of constants and predicates</definiens>
			</definition>
			<definition id="5">
				<sentence>An embedding \ ] 'unction for a DR K in a model M is a mapping from discourse referents in UK into the domain of M. An extension of an embedding flmction f for K in M to an embedding function g for K ' in M is defined as g : ( Dora ( f ) U UE , ) -- ~ DM .</sentence>
				<definiendum id="0">)</definiendum>
				<definiens id="0">An embedding \ ] 'unction for a DR K in a model M is a mapping from discourse referents in UK into the domain of M. An extension of an embedding flmction f for K in M to an embedding function g for K ' in M is defined as g : ( Dora ( f ) U UE ,</definiens>
			</definition>
			<definition id="6">
				<sentence>A DR K is shown to be true in a model M iff there is a proper embedding of K in M. A DR K implies a DR K ' iff every model in which K is true is also a model in which K ' is true .</sentence>
				<definiendum id="0">DR K</definiendum>
			</definition>
			<definition id="7">
				<sentence>We denote by GC1 the sequent calculus for GC1 , which contains the axioms of the form : ( A1 ) A } A and ( A2 ) A , -- ~A ~- , with the right and left rules for ( &amp; ) , ( V ) , ( ~ ) , ( ~ &amp; ) and ( ~ V ) together with ( Weakening ) and ( Cut ) .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">contains the axioms of the form : ( A1 ) A } A and ( A2 ) A , -- ~A ~- , with the right</definiens>
			</definition>
</paper>

		<paper id="1094">
			<definition id="0">
				<sentence>The Text Encoding Initiative ( TEl ) is an international project established in 1988 to develop guidelines for the preparation and interchange of electronic texts for research , and to satisfy a broad range of uses by the language industries more generally .</sentence>
				<definiendum id="0">Text Encoding Initiative ( TEl )</definiendum>
				<definiens id="0">an international project established in 1988 to develop guidelines for the preparation and interchange of electronic texts for research , and to satisfy a broad range of uses by the language industries more generally</definiens>
			</definition>
			<definition id="1">
				<sentence>574 In 1988 , the Text Encoding Initiative ( 'I'EI ) was established as an international co-operative research project to develop a general and tlexible set of guidelines for the preparation and interchange of electronic texts .</sentence>
				<definiendum id="0">Text Encoding Initiative</definiendum>
				<definiens id="0">an international co-operative research project to develop a general and tlexible set of guidelines for the preparation and interchange of electronic texts</definiens>
			</definition>
			<definition id="2">
				<sentence>C/I ) ( J XIII ) , tile Andrew W. Mellon Foundation , and tile Social Science and tlumanities Research Council of Canada .</sentence>
				<definiendum id="0">C/I ) ( J XIII</definiendum>
				<definiens id="0">tile Andrew W. Mellon Foundation , and tile Social Science and tlumanities Research Council of Canada</definiens>
			</definition>
			<definition id="3">
				<sentence>In January 1994 , the `` I'E\ ] ~ issued its Guidelines for the Encoding and hiterchange of Machine-Readable Texts , which provide standardized encoding conventions for a large range of text types and features relevant for a broad range of applications , inchlding natural language processing , infommtion retrieval , hypertext , electronic publishing , various forms of literary and historical analysis , lexicography , etc .</sentence>
				<definiendum id="0">Machine-Readable Texts</definiendum>
			</definition>
			<definition id="4">
				<sentence>Each TEl base tagset determines the basic structure of all the documents with which it is to be used .</sentence>
				<definiendum id="0">TEl base tagset</definiendum>
				<definiens id="0">determines the basic structure of all the documents with which it is to be used</definiens>
			</definition>
			<definition id="5">
				<sentence>Elements which can appear at the same position within a document are regarded as forming a model class : for example , the class phrase includes all elements which can appear within paragraphs but not spanning them , the class chunk includes all elements which can not appear within paragraphs ( e.g. , paragraphs ) , etc .</sentence>
				<definiendum id="0">class phrase</definiendum>
			</definition>
			<definition id="6">
				<sentence>A number of optional additional tagsets are defined by tile Guidelines , inlcuding tagscts for special application areas such as alignment and linkage of text segments to form hypertexts ; a wide range of other analytic elements and attributes ; a tassel for detailed manuscril ) l transcription and another for the recording of an electronic variorum modelled on the tradition , 'd critical apparatus ; tagscts fc ) r the detailed encoding of names and dates ; abstractions such as netw¢~rks , graphs or trees ; nmthematic-d formulae and tables Etc .</sentence>
				<definiendum id="0">tile Guidelines</definiendum>
			</definition>
			<definition id="7">
				<sentence>The syntax defined by the Guidelines formalizes the way in which such features are encoded and provides for a detailed specification of legal feature wdue/pair combinations and rules ( a feature system declaration ) determining , for example , the implication of under-specified or defaulted features .</sentence>
				<definiendum id="0">Guidelines</definiendum>
				<definiens id="0">formalizes the way in which such features are encoded and provides for a detailed specification of legal feature wdue/pair combinations and rules ( a feature system declaration ) determining , for example , the implication of under-specified or defaulted features</definiens>
			</definition>
</paper>

		<paper id="2193">
			<definition id="0">
				<sentence>\ ] ) RT focuses on the semantic interpretation of discourses .</sentence>
				<definiendum id="0">RT</definiendum>
				<definiens id="0">focuses on the semantic interpretation of discourses</definiens>
			</definition>
			<definition id="1">
				<sentence>A major aspect of I ) RT is the use of Discourse Representation Structures that hold the seinantic content of sentences as a pair &lt; U , C &gt; , in which U is a set of discourse markers ( referents ) and C a set of conditions upon them .</sentence>
				<definiendum id="0">RT</definiendum>
			</definition>
			<definition id="2">
				<sentence>K* , where K* is an alphabetic variant of a part of K. conditions .</sentence>
				<definiendum id="0">K*</definiendum>
			</definition>
</paper>

		<paper id="1098">
			<definition id="0">
				<sentence>INTRODUCTION Self-repair ( Levelt 1988 ) is a repair of utterance by speaker him/herself .</sentence>
				<definiendum id="0">INTRODUCTION Self-repair</definiendum>
				<definiens id="0">a repair of utterance by speaker him/herself</definiens>
			</definition>
			<definition id="1">
				<sentence>Speaking without self-repair is the most difficult modality of natural language communication .</sentence>
				<definiendum id="0">self-repair</definiendum>
			</definition>
			<definition id="2">
				<sentence>In this paper we propose a parser called SERUP ( SElf-Repaired Utterance Parser ) , which plays a major part in understanding a self-repaired utterance .</sentence>
				<definiendum id="0">SElf-Repaired Utterance Parser )</definiendum>
				<definiens id="0">plays a major part in understanding a self-repaired utterance</definiens>
			</definition>
			<definition id="3">
				<sentence>Normal Parser is a parser that parses wellformed utterances .</sentence>
				<definiendum id="0">Normal Parser</definiendum>
				<definiens id="0">a parser that parses wellformed utterances</definiens>
			</definition>
			<definition id="4">
				<sentence>Categories printed in italics have no clue , i.e. , SERUP fails to parse utterances in those categories .</sentence>
				<definiendum id="0">SERUP</definiendum>
			</definition>
			<definition id="5">
				<sentence>We think SERUP is effective to Japanese self-repaired utter &amp; rices .</sentence>
				<definiendum id="0">SERUP</definiendum>
			</definition>
			<definition id="6">
				<sentence>SERUP uses some linguistic clues and translates a self-repaired utterance into well-formed version and parses it .</sentence>
				<definiendum id="0">SERUP</definiendum>
			</definition>
			<definition id="7">
				<sentence>Gemini : A natural language system for spoken-language understanding .</sentence>
				<definiendum id="0">Gemini</definiendum>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>Kupiec proposes an Mgorithm for finding ~loun phrases in bilingual corpora ( Kupiec , 1993 ) .</sentence>
				<definiendum id="0">Kupiec</definiendum>
			</definition>
			<definition id="1">
				<sentence>TIONS The translation likelihood ( TL ) of one translation candidate EWCi for the term JW is defined as : TL ( JW , EWCi ) = F ( TLS ( JW , EWCi ) , TLL ( JW , EWCi ) ) where TI~S ( JW , EWCi ) is `` 'Franslation Likelihood based on Statistical information , '' and TLL ( JW , EWCi ) `` Translatiou Likelihood based on Linguistic info rmat ion 2 TLS ( JW , EWCi ) is the frequency score based on the statistical information from Hypothesis 1 that a word which appears as often in tile corresponding units as JW in Japanese units is more likely to be EW .</sentence>
				<definiendum id="0">JW</definiendum>
				<definiendum id="1">TI~S</definiendum>
				<definiendum id="2">EWCi )</definiendum>
				<definiens id="0">translation likelihood ( TL ) of one translation candidate EWCi for the term</definiens>
				<definiens id="1">the frequency score based on the statistical information from Hypothesis 1 that a word which appears as often in tile corresponding units</definiens>
			</definition>
			<definition id="2">
				<sentence>~_ TLS ( JW , EWCi ) = F3U ( JW ) where FEU ( EWCi ) is the number of corresponding units in which EWCi appears .</sentence>
				<definiendum id="0">~_ TLS ( JW , EWCi</definiendum>
				<definiendum id="1">FEU ( EWCi )</definiendum>
			</definition>
			<definition id="3">
				<sentence>TLL ( JW , EWCi ) is tile word similarity score based on the accuracy of the correspondence term JW and the translation candidate EWCi obtained by using linguistic information in tile MT bilingual dictionary .</sentence>
				<definiendum id="0">TLL</definiendum>
				<definiens id="0">tile word similarity score based on the accuracy of the correspondence term JW and the translation candidate EWCi obtained by using linguistic information in tile MT bilingual dictionary</definiens>
			</definition>
			<definition id="4">
				<sentence>Term JW and translation candidate EWCi have the same length k ( -I ) , and all of their component words correspond in the dictionary , wJi : ~we i indicates that we i is included in wJi 's translation candidates in the MT bilingual dictionary .</sentence>
				<definiendum id="0">wJi</definiendum>
				<definiens id="0">included in wJi 's translation candidates in the MT bilingual dictionary</definiens>
			</definition>
			<definition id="5">
				<sentence>~ ( ~J 5 ' \ ] , , T F 1t ~ , l. , ~ .7 '' ) because `` : &lt; I. ~-7 '' '' is unknown and `` -~ 80 Table 2 : Aeeur'lcy of transl'dion estimates Compound nouns ( occurrences ) -total Tl-i~'t cstq , n -- at~'~ to , ; ~-e~-tq m : ZteT '' 1 text , , 7 { ext_s~ 3,224_~.9 % ( 2,349 ) 83.3 % ( 2,680 ) Unkilown words occtnrences ) -t-ot~al -- \ [ first estimate top 3 estimates • I O -- 55 6 I 30.1~ , ( 16.7 ) 52.4 % ( 29.1 ) I 389 | 54.0 % , ( 210 ) 65.0 % ( 253 ) k 1- '' is known as `` strike . ''</sentence>
				<definiendum id="0">~</definiendum>
				<definiens id="0">5 ' \ ] , , T F 1t ~ , l. , ~ .7 '' ) because `` : &lt; I. ~-7 '' '' is unknown and `` -~ 80 Table 2 : Aeeur'lcy of transl'dion estimates Compound nouns ( occurrences ) -total Tl-i~'t cstq , n -- at~'~ to , ; ~-e~-tq m : ZteT '' 1 text , , 7 { ext_s~ 3,224_~.9 % ( 2,349 ) 83.3 % ( 2,680 ) Unkilown words occtnrences ) -t-ot~al -- \ [ first estimate top 3 estimates • I O -- 55 6 I 30.1~</definiens>
			</definition>
			<definition id="6">
				<sentence>Katakana is the phonetic alphabet in Jal ) anese for spelling foreign words• Since many compound nourLs in a technical field consist of Katakana 's with no space between component words , much larger lexicon will contribute to more accurate segmelltation .</sentence>
				<definiendum id="0">Katakana</definiendum>
				<definiens id="0">the phonetic alphabet in Jal ) anese for spelling foreign words• Since many compound nourLs in a technical field consist of Katakana 's with no space between component words</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>Abstract : This paper is an introdnction to KASSYS , a system that has been designed to extract information from detining statements in natural language .</sentence>
				<definiendum id="0">Abstract</definiendum>
				<definiens id="0">an introdnction to KASSYS , a system that has been designed to extract information from detining statements in natural language</definiens>
			</definition>
			<definition id="1">
				<sentence>Ilyperonymy is thus defined by the gencral relation : ( 1 ) ( A is a hyperonym of B ) ~ ( V x , B ( x ) D A ( x ) ) The equiwflcnt in natural hmguage of ( 1 ) is : ( 2 ) ( A is a hyperonyna of B ) ~ ( All P , is A ) For example , lhe definition of the concept bee : ( 3 ) bee : a social insect which produces wax and honey .</sentence>
				<definiendum id="0">B ( x ) D A ( x )</definiendum>
				<definiens id="0">the gencral relation : ( 1 ) ( A is a hyperonym of B ) ~ ( V x ,</definiens>
				<definiens id="1">a social insect which produces wax and honey</definiens>
			</definition>
			<definition id="2">
				<sentence>Let us take the following definition patterns : ( 6 ) V x , A ( x ) D ( B ( x ) A C ( x ) ) ( 7 ) V x , A ( x ) : ) 0r ( x ) ^ C ' ( x ) ) In ( 6 ) and ( 7 ) , concept A has been deIined by the hypcronyms B and B ' respectively and the specific differet~ces C and C ' .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">deIined by the hypcronyms B and B ' respectively and the specific differet~ces C and C '</definiens>
			</definition>
			<definition id="3">
				<sentence>Starling froth tire cm/cept pistol , then performhlg type expansion on its hyperonym small-arm , followed by a second type expansion on tile conceptfirear , t , KASSYS buihls the graph of the following detinition : ( 21 ) A pistol is a short portable arm which Ih'es shots , etc .</sentence>
				<definiendum id="0">pistol</definiendum>
			</definition>
			<definition id="4">
				<sentence>K : Je sais d6j'l qn'une \ [ 'LISEe est un adronef qui sert h eavuycr une charge dans I'espace .</sentence>
				<definiendum id="0">Je</definiendum>
			</definition>
</paper>

		<paper id="1028">
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>The EI ) R I-lectronic l ) ictionary ( El ) R , 1993 ; Yokoi , 1990 ) is designed as the first true machine-readable dictionary that contains , it\ ] a readily qccessible lorm , tile information required for a colnputer to understand and generate natural hmguage .</sentence>
				<definiendum id="0">EI ) R I-lectronic l</definiendum>
				<definiens id="0">the first true machine-readable dictionary that contains</definiens>
			</definition>
			<definition id="1">
				<sentence>The part of the dictionary that handles surface information is kept separate from the section that handles semantic intbrlnation : surface inlormation that is heavily dependent upon a particular language is stored in the Word I ) ictionary , attd sclnantic infornmtion is stored in the Concept Dictionary .</sentence>
				<definiendum id="0">semantic intbrlnation</definiendum>
			</definition>
			<definition id="2">
				<sentence>Word entries consist of headwords , grammatical information that indicates tile grammatical characteristics of tile word , and concept identifiers that indicate tile concepts represented by a given word in different contexts .</sentence>
				<definiendum id="0">Word entries</definiendum>
				<definiens id="0">consist of headwords , grammatical information that indicates tile grammatical characteristics of tile word , and concept identifiers that indicate tile concepts represented by a given word in different contexts</definiens>
			</definition>
			<definition id="3">
				<sentence>The Word Diction , 'uy is a collection of word entries that contain entry information as shown in Fig .</sentence>
				<definiendum id="0">Word Diction</definiendum>
				<definiendum id="1">'uy</definiendum>
				<definiens id="0">a collection of word entries that contain entry information as shown in Fig</definiens>
			</definition>
			<definition id="4">
				<sentence>A headword consists of notation ( the orthographic spelling of a word containing all the characters common to all inflected forms of tile word ) and adjacency attributes .</sentence>
				<definiendum id="0">headword</definiendum>
			</definition>
			<definition id="5">
				<sentence>Grammatical information consists of part of speech , syntax tree , inflection information , grammatical attributes and function word information .</sentence>
				<definiendum id="0">Grammatical information</definiendum>
				<definiens id="0">consists of part of speech , syntax tree , inflection information , grammatical attributes and function word information</definiens>
			</definition>
			<definition id="6">
				<sentence>Included under those words tbat are treated as single word entries are the following tylx~s of words : -foreign words -proper nouns -common nouns derived from prope , ' nouns ( `` New Mexican '' ) -idiomatic expressions which do not fit into a generalized phrase structure l ) atteru ( `` on the cheap , '' `` open sesame '' ) -function word equivalents For noun phrase entries in the English Word Dictionary information is provided for the phrase as a whole as well as far the individual constituents that comprise the F , hrase .</sentence>
				<definiendum id="0">nouns</definiendum>
				<definiens id="0">Dictionary information is provided for the phrase as a whole as well as far the individual constituents that comprise the F , hrase</definiens>
			</definition>
			<definition id="7">
				<sentence>The categoricM labels used to mark the grouping of the phrasal are shown in the example below : EAJ ( traveling ) /EN 1 ( post ) /EN 1 ( o ffice ) ~ &gt; EAJ ( traveling ) /EN 1 ( F , N 1 ( post ) /EN 1 ( @ office ) ) In this syntactic notation a slash ( / ) divides constituents at the same level , ENI is an English common noun , EAJ au English adjective , etc. ; and lhe bracketing structure is a linearized tree in a standard for , n , e.g. , in ( iii ) above the tree expands to the right , while in ( iv ) it expands to tile left .</sentence>
				<definiendum id="0">ENI</definiendum>
				<definiens id="0">an English common noun</definiens>
				<definiens id="1">a linearized tree in a standard for , n</definiens>
			</definition>
			<definition id="8">
				<sentence>traveling : EAPOS ; EANOCMP ; EANOS UP - &gt; EAPOS ; EANOCMP ; EANOSUP post : ENSG ; ECNI ; ENC - &gt; ENSG ; ENU office : ENSG ; ECN 1 ; ENC - &gt; ENSG ; ECN 1 ; ENC The coding ECN1 ( takes plural ending -s ) and ENC ( Countable ) is changed to ENU ( Uncountable ) to indicate that the word `` post '' when used in the context of the phrase , does not inflect .</sentence>
				<definiendum id="0">EANOSUP post</definiendum>
				<definiendum id="1">ENC The coding ECN1</definiendum>
				<definiendum id="2">ENC</definiendum>
				<definiens id="0">takes plural ending -s</definiens>
			</definition>
			<definition id="9">
				<sentence>260 # Entries Pattern ExamAl~le EN10/ENI ( @ ) tmmmock chair EAJ0/EN 1 ( @ ) blue jay *EN20/ENI ( @ ) Doppler effect *EN 1 ( @ ) /EPP ( EPR0/EN 10 ) piece of cake EAJ ( EVE0~EV ( ) ) ~NI ( @ ) circulating library ENI ( EVE0/EEV0 ) /EN 1 ( @ ) changing room *EN 1 ( EN 10/EEN : ENPOS 0 ) /EN 1 ( @ ) teacher ' s pet *ENI denotes a common noun , EN2 denotes a proper noun , EEN : ENPOS a noun possessive ending 's and ' , EPR a preposition , and EPP a prepositional phrase .</sentence>
				<definiendum id="0">EN2</definiendum>
				<definiendum id="1">EEN</definiendum>
				<definiens id="0">a proper noun</definiens>
			</definition>
			<definition id="10">
				<sentence>Noun phrases consisting of a word in the '-ing ' form anti another noun are treated by using one of the following four patterns , where EVE denotes an English verb and EEV a verb ending : ( l ) EN 10/EN 1 ( @ ) `` hunting knife '' 261 ( 2 ) EAJ0/ENI ( @ ) `` flying fox '' `` man-eating shark '' ( 3 ) EAJ ( EVE0/EEV0 ) /EN 1 ( @ ) `` intervening sequence '' `` circulating medium '' ( 4 ) EN 1 ( EVE0/EEV0 ) /EN 1 ( @ ) `` changing room '' `` participating insurance '' If a phrasal in the form of '-ing + noun ' could be reworded as 'a noun that is v-ing ' or 'a noun that v-s ' the entry was coded using either pattern 2 or pattern 3 .</sentence>
				<definiendum id="0">EVE</definiendum>
				<definiens id="0">an English verb</definiens>
			</definition>
</paper>

		<paper id="2135">
			<definition id="0">
				<sentence>This constructor means that the elements in the class are defined as objects below it .</sentence>
				<definiendum id="0">constructor</definiendum>
				<definiens id="0">the elements in the class</definiens>
			</definition>
</paper>

		<paper id="2148">
			<definition id="0">
				<sentence>The uew arguments are : a set of currently uninstantiated variables ( tile so called guide 's set component ) and a non-negative integer ( the guide 's 977 numeric component ) .</sentence>
				<definiendum id="0">non-negative integer</definiendum>
				<definiens id="0">a set of currently uninstantiated variables ( tile so called guide 's set component</definiens>
			</definition>
			<definition id="1">
				<sentence>`` BUG : A Directed Bottom-Up Generator lbr Unification Based Formalisms '' .</sentence>
				<definiendum id="0">BUG</definiendum>
				<definiens id="0">A Directed Bottom-Up Generator lbr Unification Based Formalisms ''</definiens>
			</definition>
</paper>

		<paper id="1095">
			<definition id="0">
				<sentence>ABSTRACT INTEX is a text processor ; it is usually used to parse corpora of several megabytes .</sentence>
				<definiendum id="0">ABSTRACT INTEX</definiendum>
				<definiens id="0">a text processor ; it is usually used to parse corpora of several megabytes</definiens>
			</definition>
			<definition id="1">
				<sentence>INTEX builds collcordances and indexes of all types of patterns ; it is used by linguists to analyse corpora , but can also be viewed as an information feb'lewd system .</sentence>
				<definiendum id="0">INTEX</definiendum>
				<definiens id="0">builds collcordances and indexes of all types of patterns ; it is used by linguists to analyse corpora , but can also be viewed as an information feb'lewd system</definiens>
			</definition>
			<definition id="2">
				<sentence>LAI ~ diclio , mry contains over 700,000 simple words , basically all the simple words of the language 2 .</sentence>
				<definiendum id="0">mry</definiendum>
				<definiens id="0">contains over 700,000 simple words , basically all the simple words of the language 2</definiens>
			</definition>
			<definition id="3">
				<sentence>In order to disambiguate words in texts , INTEX uses cache dictionaries and local grammars .</sentence>
				<definiendum id="0">INTEX</definiendum>
				<definiens id="0">uses cache dictionaries and local grammars</definiens>
			</definition>
</paper>

		<paper id="1088">
			<definition id="0">
				<sentence>Collocation has been established as an essential tool in computational linguistics ( Church and Mercer 1993 ) .</sentence>
				<definiendum id="0">Collocation</definiendum>
			</definition>
			<definition id="1">
				<sentence>N-granl Collocation ( By Frequency ) Bi -- syllabic Collocation with the verb jin4xing2 gon\ [ , lzuo4 'to work ' diao4cha2 'to investigate ' 354 gong I cheng2 'engineering work ' 233 437 tao31un4 'to discuss ' 223 l. , ou ltong I 'to communicate ' 198 xie2tiao2 'to coordinate ' 185 yan2j iu4 'to study ' 185 liao3jie3 'to understand ' 166 guclhua4 'to plan ' 156 xie2shangl 'to negotiate ' 154 Last , the user can specify a character string in the context as a filter .</sentence>
				<definiendum id="0">N-granl Collocation</definiendum>
				<definiens id="0">a character string in the context as a filter</definiens>
			</definition>
			<definition id="2">
				<sentence>The automatic segmentation proccdurc is an revised version of the program reported in Chen and Liu ( 1992 ) .</sentence>
				<definiendum id="0">automatic segmentation proccdurc</definiendum>
			</definition>
			<definition id="3">
				<sentence>The on-line lexicon is the CKIP lexicon of more than 80 thousand cntries ( Chen 1994 ) .</sentence>
				<definiendum id="0">on-line lexicon</definiendum>
			</definition>
</paper>

		<paper id="2196">
			<definition id="0">
				<sentence>A discourse slralegy is a strategy for communicaling with another agent .</sentence>
				<definiendum id="0">discourse slralegy</definiendum>
				<definiens id="0">a strategy for communicaling with another agent</definiens>
			</definition>
			<definition id="1">
				<sentence>A WARRANT is always optional ; this is consistent with the RST fl'anlewolk in which all satellites are optional information .</sentence>
				<definiendum id="0">WARRANT</definiendum>
				<definiens id="0">with the RST fl'anlewolk in which all satellites are optional information</definiens>
			</definition>
			<definition id="2">
				<sentence>The All-lmplicil slralcgy is an explmsion of a disCOIII'Se 151 ; Ill Io lllilke a I'R/ ) I'O.RAI. , in which a PiU ) I'OSAI .</sentence>
				<definiendum id="0">All-lmplicil slralcgy</definiendum>
			</definition>
			<definition id="3">
				<sentence>AWM consists of a three dimensional slsace in which propositions acquired Dora perceiving the world are stored in chronological sequence according 1o tile localion o1 ' a moving memory pointef .</sentence>
				<definiendum id="0">AWM</definiendum>
				<definiens id="0">consists of a three dimensional slsace in which propositions acquired Dora perceiving the world are stored in chronological sequence according 1o tile localion o1 ' a moving memory pointef</definiens>
			</definition>
			<definition id="4">
				<sentence>Thus PERFORMANCE is a measnre of LEAS'I '' COLI , ABORATIVE EFFORT \ [ Cl~u'k and Scbaeler , 1989 ; Brennan , 19901 .</sentence>
				<definiendum id="0">PERFORMANCE</definiendum>
				<definiens id="0">a measnre of LEAS'I '' COLI</definiens>
			</definition>
			<definition id="5">
				<sentence>Here we compared two discourse strategies : Allhnplicit ~ul ( l Explicit-Warr~mh Explicit-W ; uranl is a type of discourse stralegy called ~m Attelition strategy in \ [ W ; dkel ; 1993\ ] because its main lunction is to manipulate agents ' altenlional slate .</sentence>
				<definiendum id="0">uranl</definiendum>
				<definiens id="0">a type of discourse stralegy called ~m Attelition strategy in \ [ W ; dkel ; 1993\ ] because its main lunction is to manipulate agents ' altenlional slate</definiens>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>tiacb catcgorial feature string is supposed to contain lexico-semanlie information ( c+g. , a capsule representation of the ineanitlg of the relevant Icmn-a 0 and morfJhosyntactie inR+rmation ( such as grammatical category , gender , number , tense etc. ) : the content of thc feature string , however , will be relatively neglected here .</sentence>
				<definiendum id="0">morfJhosyntactie inR+rmation</definiendum>
				<definiens id="0">the content of thc feature string</definiens>
			</definition>
</paper>

		<paper id="2197">
			<definition id="0">
				<sentence>where FRISCO is the name of a record store .</sentence>
				<definiendum id="0">FRISCO</definiendum>
				<definiens id="0">the name of a record store</definiens>
			</definition>
			<definition id="1">
				<sentence>The sentenees labeled PRE , express that the user knows the attrilLutc t ) rior to the &lt; lialogue session , while those labeled POST , express that the user has come to know it during the session. For instan &lt; : e , PRE : location ( store001 ) means that the user have ah'eady knows the h ) catiou of store001 betorc the interaction starts , whih. ' POST : location ( store001 ) means the user has &lt; : ome to know the location through the system 's explanation. The sentences labeled JUDGE , express the user 's ( : urrent knowledge and is used to exploit tile user mo &lt; lel by other coml &gt; &lt; ments in the dialogue system. For instance , JUDGE : location ( store001 ) means the use.r now knows tit ( . ' location of store001. The sentences labele &lt; l TOLD an ( l TELL , express the evi &lt; le.nce , gained by the user 's utterance and the system 's explanation. F &lt; Lr instance , TOLD : name ( store001 ) means the user has iLLdicated by the clues that she knows the name of store001 , while TELL : name ( store001 ) means the system has explai , m &lt; t the name. For exception , in the case of location , the form TELL : location ( X ) ( whcre X is some obje ( : t \ [ l ) ) is not usc &lt; l because a location is explained in terms of the relative h ) cation of another object. Instead , the form TELL : relation ( X , Y ) ( where X and Y are some ol ) ject IDs ) is used. Tit ( . ' sentences representing objects and exi ) ertisc fields have only the label PRE. The sentence representing an object ( e.g. PRE : store001 ) means that the user knows the object , that is she knows , nost of the attributes of the object. The sentence representing an expertise rich\ [ ( e.g. PRE : records_collector ) means thai : the user is an exl ) ert of the field , that is she knows the objects related to the expertise field. As mentioned , arcs of the Bayesian network represent direct probablistic influence between linked variables. Tim directionality of the arcs is essential for rei ) resenting nontransitive dependencies. In order to represent the knowledge in terms of Bayesian Network , we must interpret the qualitative relation betwee.n the sentences that are represented by our language as a directed arc or some such combination of arcs. In our ease , the network has two sub-networks. One represents the user 's knowledge be.fore the dialog session , which is used to guess the user 's model fronl her utterances . The sentences assigne &lt; l to the nodes in this part have either the label PRE or TOLD. We call this subnetwork the prior part. The other sulmetwork in which the nodes have either the label POST oi ' TELL is used to deal wil ; h tit ( ' , influence of the system 's utterances. This sulmetwork we call the posterior part. It is important t ; o make a clear distinction. Considering that the system explains a concept , it is not proper to assume that the user knows some other related concepts. For example , if tile user utters that she knows some location x then it can be inferred that she also knows locations that are ( : los ( ; to x. But that is not true if the location x is explained by the system. The relations ill the prior part of the network are categorized into four types as follows : ( 1 ) tl , e relations between objects in an expertise field ( 2 ) the relations between attributes of obje ( : ts ( 3 ) the relations lmtween an ol ) je &lt; -t and its attributes ( 4 ) the relations betwee.n an att , 'ibute of an object and the evi &lt; lence that the user knows it The relations ( 1 ) are ( : oncerL , ed with the expertise fiehl. The objects ill the same expertise field are related through the expertise field node. We introduce the arcs that go from the expertise tMd no &lt; le to the obje &lt; : t nodes belonging to that fiel ( 1. For example , ares go Dora the node of `` records collector '' to that of `` Compact Disk '' , '' Tower Records '' ( name of a record store ) and so on. The level of expertise can be controlled by the conditi &lt; mal probal ) ilities assigned to the object nodes conditioned by tile ext ) ertise tMd node. In this framework , we can intro &lt; hLce arbitrary numbers of expertise fiekls , all of which can be assigned the level of expertise. '\ ] /he re.lations ( 2 ) are conce.rned with the &lt; lolnain knowledge. In our domain , those are the relations between the locations , whi &lt; : h are based on the assumption that the user l ) robably knows the locations close to the location she known. TILe relations are assunn. 'd to be symmetric. A single directe &lt; l arc of Bayesian networks does not represent a symmetric relation. In ordeL ' to rel ) resent a symmetric relation , we introduce a dummy evi ( tence node , whereby two arcs go forth from the two location nodes as shown in figure 1. The prior 1214 O O `` - , .®/ dlllnlfiy lit ) do Figure 1 : Symmetric rel~d ; iolt conditional probabilities of l ; hc dummy node lutve high wdue it ' the two parent nodes h~tve the same wdue. The relations ( 3 ) are ( : on ( : erned with g ( : ner~d knowledge , such ; ts knowing ; m obj ( ! ct well imt ) li ( : ~d ; cs know. ing its ; d ; tril ) utes. In order to rel ) resent such kiltd of I ' ( ! l ; ttio , ls , WC ill\ [ to ( hi ( : ( ; the ~tl ' ( : s to go fl'Olll the , lode of ~m object to the nodes of its ; tttributcs. The arc ec ) rresponding to the relation ( 4 ) is introdu ( : e ( l , to go frmn the node of an al.trilmte of an ollj ( ~ct to an evidence node. The ; ~ttribul.e nolle ~utd the eviden ( : e node have the s~mm ( 'ontent , whih , they h ; Lve the different bd~els , PRE and TOLD. Iu tim l ) OSterior l ) i~rt of the network , the.re ~tr ( , . only ; ~rcs rci ) resenting the relations ( 4 ) . The ; d ; tribul ; e nodes ~md the evidence lmdes are lalmle ( l POST ~md TELL. In a ( hlition , tile TELL node. Ill ; -ty ll.~tve lllOl'e. I ; h ; tn Ol , ( ! it ; \ [ reid ; , lode \ ] ) ( ! CaAlS ( ~ th ( '. ( ! Xl ) l } tll } ttiOll8 of the att|'ilmt ( ; are m ; t ( le l ) y referring to the other attributes. Actually , ill ( ) Ill ' towtl gllid~t , l ( : ( ! ( lonudn , the syst ( ; m explains the new ht ( : ~ttkm using | ; Ill ; locations that the user already knows. Fro ' instance , the nodes POST : h ) cation ( store001 ) and POST : location ( store0 ( ) 2 ) ~tre l ) iU'ei , ts of the. llode TELL : relation ( store001~ store002 ) whe.n the system ( '.xIll ; tin till ! location of store001 by using the lo ( : ~tti ( m of store002. The. more the system shows the l'el~d : ions , the deeper the user 's un ( lerst ; ul ( ting bc ( : on~ ( ~s. The ~unbiguous e.videnee ( : ~ul lm dealt with str~ightforwardly ill tit ( ; tl ; tyesi ; ul al ) l ) ro~ ( : h. All evidence l , o ( le Citll luwe lllore th~tll Ol , ( ! l ) a , l'eltt llo ( le , to re,1 ) r ( &gt; sent the ambiguity .</sentence>
				<definiendum id="0">h tit</definiendum>
				<definiens id="0">the user have ah'eady knows the h ) catiou of store001 betorc the interaction starts , whih. ' POST : location ( store001 ) means the user has &lt; : ome to know the location through the system 's explanation. The sentences labeled JUDGE , express the user 's ( : urrent knowledge and is used to exploit tile user mo</definiens>
				<definiens id="1">important t</definiens>
			</definition>
			<definition id="2">
				<sentence>sessi ( ) n or because it has been exi ) l~dned by the system during 1 ; he session .</sentence>
				<definiendum id="0">sessi ( )</definiendum>
				<definiens id="0">n or because it has been exi ) l~dned by the system during 1 ; he session</definiens>
			</definition>
			<definition id="3">
				<sentence>l : ering the sent : torte , the syste : n adds the evidence , TELL : name ( hands ) = 1 , TELL : relation ( hands , frisco ) : : 1 , t ; o the nel ; work .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">adds the evidence</definiens>
			</definition>
</paper>

		<paper id="1097">
			<definition id="0">
				<sentence>MIJLTEXT ( Multilingual Text Tools and Corpora ) ix a recently initiated large-scale project funded under tim Commission of European Communities Linguistic Research and Engineering Program , which is intended to address these problems .</sentence>
				<definiendum id="0">MIJLTEXT</definiendum>
				<definiens id="0">a recently initiated large-scale project funded under tim Commission of European Communities Linguistic Research and Engineering Program , which is intended to address these problems</definiens>
			</definition>
			<definition id="1">
				<sentence>Level 1 includes universal text elements down to the level c , f paragraph , which is the smallest unit that can be identified languageindependently .</sentence>
				<definiendum id="0">f paragraph</definiendum>
				<definiens id="0">the smallest unit that can be identified languageindependently</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>l\ [ ere , \ ] ' ( n , c ) is defined as follows : fail ( ' , , . )</sentence>
				<definiendum id="0">c )</definiendum>
			</definition>
			<definition id="1">
				<sentence>The algorithm for consulting the dictionary is quite simple : 1 n +-Root 2 for each character position i = 11,2 , ... k , 2-1 while n 7~ Root and forward ( n , ci ) = nil do n ~-fail ( n ) 2-2 n = forward ( n , cl ) 2-3 ifn is the end of some word ( s ) , output them where ci is the character at position i. We applied the TRIE with fail pointers to our 70,491-word dictionary for Japanese morphological analysis ( in which the average word length is 2.8 characters ) and compared it with a conventional TRIE-based system , using two sets of data : newspapers articles ( 44,113 characters ) and computer manuals ( 235,104 characters ) .</sentence>
				<definiendum id="0">ci</definiendum>
				<definiens id="0">in which the average word length is 2.8 characters ) and compared it with a conventional TRIE-based system , using two sets of data : newspapers articles ( 44,113 characters ) and computer manuals ( 235,104 characters )</definiens>
			</definition>
</paper>

		<paper id="2170">
			<definition id="0">
				<sentence>GCE uses for its purpose large GENELEX lexicons ( French 55000 simple words and 18 000 compound words , English 40 000 words ) and a constraint grammar like approach .</sentence>
				<definiendum id="0">GCE</definiendum>
				<definiens id="0">French 55000 simple words and 18 000 compound words</definiens>
			</definition>
			<definition id="1">
				<sentence>The most basic operation consists in manually creating domains of information and manually ( either by typing them in , or by mouse selection in source text ) inserting items 2 into them .</sentence>
				<definiendum id="0">most basic operation</definiendum>
				<definiens id="0">consists in manually creating domains of information and manually ( either by typing them in , or by mouse selection in source text ) inserting items 2 into them</definiens>
			</definition>
</paper>

		<paper id="2203">
			<definition id="0">
				<sentence>The character string ~flN is an example of intersection combination ; Ntfl ( paddle ) is a word and ~fl~ ( sell.-at-sate-price ) is also a word , whereas tfl is the intersection character .</sentence>
				<definiendum id="0">character string ~flN</definiendum>
				<definiens id="0">a word</definiens>
				<definiens id="1">the intersection character</definiens>
			</definition>
			<definition id="1">
				<sentence>The , weight is a vector of four elements representing the importance of morphology , syntax , semantics and pragmatits , respectively , which total 1 , i.e. Cl , ; \ [ h , , e\ ] w~ , CF\ [ h , ~\ ] where Wi is the weight of the certainty fac-tor CFi in hypothesis h supported by the evidence e with respect to one of the linguistic 1246 a , specl ; s. Suppose , the weight ; vecl ; or ( O.l , 0.2 , scma , ni ; i ( : s a , nd pr~gtnal ; i ( ; s , r ( , speci ; ivcly , Lh ( ; n I ; hc following exa .</sentence>
				<definiendum id="0">Wi</definiendum>
				<definiens id="0">a vector of four elements representing the importance of morphology , syntax , semantics and pragmatits</definiens>
				<definiens id="1">the weight of the certainty fac-tor CFi in hypothesis h supported by the evidence</definiens>
			</definition>
</paper>

		<paper id="2131">
			<definition id="0">
				<sentence>As a rule , a lexeme consists of the base Cr oot '' ) of the word and of the semar tic information associated with this base .</sentence>
				<definiendum id="0">lexeme</definiendum>
				<definiens id="0">consists of the base Cr oot '' ) of the word and of the semar tic information associated with this base</definiens>
			</definition>
</paper>

		<paper id="2190">
			<definition id="0">
				<sentence>The content of an utterance is represented in tile following style : ( 1 ) K : a , b , x , y , z ... . Bel ( a , K ) ~cl ( b , IO ... ... ... ... ... ... ... ... ... ... ... ... ... A ( ~ : ) , B ( v ) , C ( ~ ) , ... We call K discourse representation structure ( DRS ) , { a , b , x , y , z ... . } K 's domain ( UK ) , the elements of UI &lt; discourse referents , tile boxed area below tile unbroken line K 's condition part ( CA- ) , and CK 's elements conditions .</sentence>
				<definiendum id="0">content of an utterance</definiendum>
				<definiendum id="1">K</definiendum>
			</definition>
			<definition id="1">
				<sentence>By contrast , DB ( K ) is empty when a dialogue starts off .</sentence>
				<definiendum id="0">DB ( K</definiendum>
				<definiens id="0">empty when a dialogue starts off</definiens>
			</definition>
			<definition id="2">
				<sentence>At this t , oint , both ( 5 ) and ( 7 ) are shared beliefs , which me , ms ( 4 ) is a belief shared by a and b. This transition is tbrumlated as the axiom of shared belief : ( 8 ) The axiom of shared belief Whet , DUCK ) cont~ns P , q ( , * , v ) , , ~nd 13~l ( b , v ) , DB ( K ' ) obtained from DB ( K ) 1 ) y the substitution of p for them is equivahmt to DB ( K ) .</sentence>
				<definiendum id="0">DB</definiendum>
			</definition>
			<definition id="3">
				<sentence>Bel ( b , Sato ( x ) ) = y ) , which gives rise to ( 34 ) tx .</sentence>
				<definiendum id="0">Bel</definiendum>
			</definition>
			<definition id="4">
				<sentence>Bel ( b , T ( x ) ) b 's intended referent , and tx .</sentence>
				<definiendum id="0">Bel</definiendum>
				<definiendum id="1">T</definiendum>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>Key words : Markov model , error detection , error correction , bunsetsu , substitution , deletion , insertion In order to improve the man-machine interface with computers , the &lt; tevelopment of input devices such as optical cha.racter tea &lt; lets ( OCR ) or speech recognition devices are expected , llowew ; r , it is not easy to input Japanese sentences J ) y these devices , because. they are written by many kinds of characters , especially thousands of `` kanji '' characters. The sentences input through an OCR. or a speech recognition device usuMly contain erroneous character strings. The techniques of natural language processing are expected to find and correct these errors. tIowever , since current technologies of natural language analysis have been developed for correct sentences , they can not directly be applied to these problems. Up to now , statistical approaches have been made to this problem. Markov mo &lt; lels are considered to be one of '' machine learning models , sinfilar to neural networks a.nd fuzzy models. They have been applied to character chains of natural lang , ,a~ges ( e.g. , l ) ; nglish ) \ [ l\ ] , \ [ 2\ ] , a.nd to phoneme reco~gnition 3 . \ [ 41 cha.ins in continuous speech. . \ [ 1~1. ¢1 ' 2nd-orde.r Markov model nt bunsets ' , l is known to be useful to correct errors in `` kanjikana. '' `` /m nsetsu '' \ [ ( ; \ ] , to choose a correct syllable chain from Japa.nese syllable `` bunsetsu '' candidates \ [ 7\ ] , and to re ( ! nce the ambiguities in translation processing of non-segmented `` kana. '' sentences into `` kanji-kana '' sentences \ [ 8\ ] . The erroneous characters can be classilied Ul , O three types , lhe hrst is w~ongly recognized chal ; aclers instead of correct ( haracters. The second and the third are wrongly inserted and deleted ( skipped ) characters respectively. Markov chain mode.Is above mentioned were restricted to tind and correct the first type of errors\ [ 5\ ] , \ [ 6\ ] . No method has been proposed for correcting errors of the second and the. third types. 'Phe. rea.son might be considered to be I.he di\ [ ticulties of finding the error location and distinguishing between deletion and insertion er I'ors. On the other hand , contextual algorithm utilizing , ,-g , 'atn letl.er statistics ( e.g.\ [ . ( ) \ ] ) a , ,d a dictionary look-ul ) algorithm\ [ 10\ ] have been discussed to detect a.nd correct erroneous characters in English sentences , which is segmented into words. This paper proposes new methods , which are able to be applied to a nor &gt; segmented chains or '' characters , to judge three types of the errors , which are characters wrongly subst .</sentence>
				<definiendum id="0">erroneous characters</definiendum>
				<definiendum id="1">'atn letl.er statistics</definiendum>
				<definiens id="0">sentences input through an OCR. or a speech recognition device usuMly contain erroneous character strings. The techniques of natural language processing are expected to find and correct these errors. tIowever , since current technologies of natural language analysis have been developed for correct sentences , they can not directly be applied to these problems. Up to now , statistical approaches have been made to this problem. Markov mo &lt; lels are considered to be one of '' machine learning models , sinfilar to neural networks a.nd fuzzy models. They have been applied to character chains of natural lang , ,a~ges</definiens>
			</definition>
			<definition id="1">
				<sentence>ituted , deleted a.nd inserted in a Japanese `` bunsetsu '' , and to correct these errors in Japanese `` kanji-l &lt; ana '' chains using m-th of der Markov chain model. The methods are based on the idea about the relation between the types of errors and the length of a chain in which the wdnes of Markov joint probability remain small , l , 'urthermore , this method is ap187 plied to detect and correct errors in segmented English words• Experiments were conducted for the case of 2nd-order and 3rd-order Markov model , and they were applied to Japanese and English newspaper arhcles. Relevance Factor 1 and `` Reeall Factor '' R for erroneous characters detected and corrected by this method were experimentally evaluated using statistical data for 70 issues of a daily Japanese newspaper and 5 issues of a daily English newspaper. Method of Error Detection and Error Correction using 2nd-Order Markov Model 2,1 Basic Definitions In this paper , two types of natural language 's sentences are discussed. One is a Japanese sentence , which is non-segmented sentence and the other is an English sentence , which is segmented into words. A Japanese sentence can be separated into syntactic units called ' % unsetsu '' , where a { ~ '' ( `` bunsetsu '' is composed of one m lependent word '' and ~ sequence of n ( greater than equal to 0 ) `` dependent words '' . A `` bunsetsu '' is a chain of Japanese `` kanjikana '' characters or an English word is a chain of alphabets , and are represented by 3 ' = sl s2 ... s , ~ , where s~ is a `` kanji-kana '' character or an alphabet. In particular , a chain , 7 , is called a `` J-bunsetsu '' when all of its elements are `` kanji-kana '' characters , and is called a `` \ [ iJword '' when all of its elements are English alphabets. The set of eorre , ct .lapanese `` bunsetsu '' or English words is represented by Pc. Three types of erroneous `` J-bunsetsu '' or E-word are dehned as follows : First , a chain ce = N , ¢~ ... s\ [ Zlg ; .. , s , ; ~ is called a `` ( i , k ) -Erroneous J-bunsetsu or Eword Wrongly Substituted `` ( ( i , k ) -EWS ) if a subehain fl = tltu ... Ih is wrongly substituted at the location i of ce , that is 3 7 Cre , -y = ~ ( o11/ &lt; Here ~ ( Ollf3 donotes substitution of a subchain fl at , the location i in a chain c~ , that is , d01i/ 8-iS2 • `` '' Si -- lll¢ 2 `` '' • \ [ , kSi-+k `` ' '' , S'~n , and l I 6- &amp; , '' ' , tk ~-siq~- , . Next , a chain c~ = &amp; g. , ... si~_lgi ... s ; , ~ is called a `` ( i , k ) -Erroneous J-bunsetsu or I ' ; word Wrongly Deleted '' ( ( i , ~ ) -~WD ) if a subchMn fl = t~t= ... tk is wrongly deleted at the location i of a , that is ~7 ~ l'c , `` y = c~ ( 1 ) &lt; &lt; ft. Here { , ( 0 &lt; &lt; fl denotes insertion of a subchain fl at the location i in a chain c , , that is , a ( 0 &lt; &lt; fl -s't.sS '' ' Si~-lltl2 `` '' `` lk , qi '' '' S~n. Finally , a chain cr = .¢t • `` '' s\ [ -lgi '' '' si- ( k-1 s ; +k '' ' s ; , ~ is also called `` ( i , k ) -Erroneous Jbunsetsu or F~word Wrongly Inserted '' ( ( i , k ) -EWI ) if a sul ) chain /3 = tlt~ ... tk is wrongly inserted at the location i of % that is 37 E Pc ' , 7 = d ; ) &gt; &gt; ft. tIere c~ ( 1 ) &gt; &gt; fl denotes deletion of a subchain f3 at the location i in a chain c~ , that is , at0 &gt; &gt; fl = .</sentence>
				<definiendum id="0">s~</definiendum>
				<definiens id="0">kanji-l &lt; ana '' chains using m-th of der Markov chain model. The methods are based on the idea about the relation between the types of errors and the length of a chain in which the wdnes of Markov joint probability remain small , l , 'urthermore , this method is ap187 plied to detect and correct errors in segmented English words• Experiments were conducted for the case of 2nd-order and 3rd-order Markov model , and they were applied to Japanese and English newspaper arhcles. Relevance Factor 1 and `` Reeall Factor '' R for erroneous characters detected and corrected by this method were experimentally evaluated using statistical data for 70 issues of a daily Japanese newspaper and 5 issues of a daily English newspaper. Method of Error Detection and Error Correction using 2nd-Order Markov Model 2,1 Basic Definitions In this paper , two types of natural language 's sentences are discussed. One is a Japanese sentence , which is non-segmented sentence and the other is an English sentence</definiens>
				<definiens id="1">a `` kanji-kana '' character or an alphabet. In particular , a chain , 7 , is called a `` J-bunsetsu '' when all of its elements are `` kanji-kana '' characters , and is called a `` \ [ iJword '' when all of its elements are English alphabets. The set of eorre</definiens>
				<definiens id="2">kSi-+k `` ' '' , S'~n , and l I 6- &amp; , '' ' , tk ~-siq~- , . Next , a chain c~ = &amp; g. , ... si~_lgi ... s ; , ~ is called a `` ( i , k ) -Erroneous J-bunsetsu or I '</definiens>
			</definition>
			<definition id="2">
				<sentence>Here p } D ) denotes the `` Relevance Factor '' for tile `` error detection prob~ ( k ) ( c ) lem '' of \ ] ~ ' , and R D denotes the `` Recall Factor '' for the `` error correction problem '' of p ( k ) respectively .</sentence>
				<definiendum id="0">Relevance Factor</definiendum>
				<definiendum id="1">R D</definiendum>
			</definition>
</paper>

		<paper id="2210">
			<definition id="0">
				<sentence>By the well-know Bayes theorem , it is possible to rewrite this expression as : max \ [ P ( WI S ) \ ] = max 1_t ' ( 5'1W ) P ( S ) I v '' 1 s L p ( nO J In this equation it is interesting to interpret the P ( SIW ) as a probability distribution capturing the facts of syllable division , while the P ( S ) is a different distribution capturing the facts of syllable sequences .</sentence>
				<definiendum id="0">P ( S )</definiendum>
				<definiens id="0">interesting to interpret the P ( SIW ) as a probability distribution capturing the facts of syllable division</definiens>
				<definiens id="1">a different distribution capturing the facts of syllable sequences</definiens>
			</definition>
</paper>

		<paper id="1081">
			<definition id="0">
				<sentence>Lexical-functional gramnmr ( LFG ) is a linguistic theory which fits nicely into computational approaches that use unification IShieber 1986\ ] .</sentence>
				<definiendum id="0">Lexical-functional gramnmr</definiendum>
				<definiendum id="1">LFG</definiendum>
			</definition>
			<definition id="1">
				<sentence>Constituent structures ( c-structures ) characterize the phrase structure configurations as a conventional phrase structure tree , while surface grammatical functions such as subject , object , and adjuncts are represented in functional structure ( f-structure ) , Because of space limitations we will not go into the details of the theory .</sentence>
				<definiendum id="0">Constituent structures</definiendum>
				<definiens id="0">a conventional phrase structure tree</definiens>
			</definition>
			<definition id="2">
				<sentence>Turkish is an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words \ [ Ottazer 1993\ ] .</sentence>
				<definiendum id="0">Turkish</definiendum>
				<definiens id="0">an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words \</definiens>
			</definition>
			<definition id="3">
				<sentence>Note th'tt although ( 61 ) ) is L , ramnmtical iyi is no more an adverbial complentent , bill is an adjective that modities yeme~ , i. Note also that ( 6c ) is ambiguous : iyi can be interpreted either as an adjective modifying yemek or as an 5In Turkish , a lransilive vmb that subcategolizes lor a direct object can take eilher fill acctlsalive marked t ) r a IlOIIlillatiVC Ill/Irked ( tllllllIIl'ketl Oll \ [ \ ] lC Slll'ftlce ) I1OUll phi'lisle for l~lli\ [ objecl , The IllllC\ [ it ) ll of accusativo case marking isl to indicale Ihat the uh .</sentence>
				<definiendum id="0">bill</definiendum>
				<definiendum id="1">IlOIIlillatiVC Ill/Irked</definiendum>
				<definiens id="0">an adjective that modities yeme~ , i. Note also that ( 6c ) is ambiguous : iyi can be interpreted either as an adjective modifying yemek or as an 5In Turkish , a lransilive vmb that subcategolizes lor a direct object can take eilher fill acctlsalive marked t ) r a</definiens>
			</definition>
			<definition id="4">
				<sentence>ject tctkrs tu a Imtticular dclinilc enlity , though Ihere are very rate cases where Ihis is ni~t die case .</sentence>
				<definiendum id="0">Ihis</definiendum>
				<definiens id="0">ni~t die case</definiens>
			</definition>
			<definition id="5">
				<sentence>Complex sentences are those that include such dependent ( subordinate ) clauses as their constituents , or as modifiers of their constituents .</sentence>
				<definiendum id="0">Complex sentences</definiendum>
			</definition>
			<definition id="6">
				<sentence>In such a case , die nlorphological analyzer returns all of \ [ lie possible nlorllhological strilctllres ill a list , lind tile parser takes care of the ambiguity regarding the gramnmr rules .</sentence>
				<definiendum id="0">lind tile parser</definiendum>
				<definiens id="0">takes care of the ambiguity regarding the gramnmr rules</definiens>
			</definition>
			<definition id="7">
				<sentence>Htzlandt is the verb of the sentence , and its voice is active , tense is past , agreement is third person singular , etc .</sentence>
				<definiendum id="0">Htzlandt</definiendum>
				<definiens id="0">the verb of the sentence</definiens>
			</definition>
</paper>

		<paper id="1076">
			<definition id="0">
				<sentence>ed its follows , using a notation similar to that of All ) ern et al. \ [ 1\ ] : Assume given a problenr instance P ( a representation of the current input ) , a solution S ( the current output ) , and a modification Ap to p.2 The modification results in a new problem instance P ' = P • At , , where • is a composition operator .</sentence>
				<definiendum id="0">•</definiendum>
				<definiens id="0">a representation of the current input</definiens>
				<definiens id="1">the current output</definiens>
				<definiens id="2">a composition operator</definiens>
			</definition>
			<definition id="1">
				<sentence>Definition 1 ( Chart ) A chart is a directed graph C = ( V , E ) such that V is a linite , non-empty set of vertices and E C V x V x R is a finite set of edges , where R iv the set of dotted context-free rules obtained from the grammar .</sentence>
				<definiendum id="0">Chart</definiendum>
				<definiendum id="1">chart</definiendum>
				<definiens id="0">a directed graph C = ( V , E ) such that V is a linite , non-empty set of vertices and E C V x V x R is a finite set of edges , where R iv the set of dotted context-free rules obtained from the grammar</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>A part-of-speech tagger is a system which automatically assigns the part of speech to words using contextual information .</sentence>
				<definiendum id="0">part-of-speech tagger</definiendum>
				<definiens id="0">a system which automatically assigns the part of speech to words using contextual information</definiens>
			</definition>
			<definition id="1">
				<sentence>Artificial neural networks consist of a large number of simple processing units .</sentence>
				<definiendum id="0">Artificial neural networks</definiendum>
			</definition>
			<definition id="2">
				<sentence>o p~gation algorithm which performs a gradient descent search on the error surface , The weight update ~XlOij , i.e. the difference between the old and the new value of weight wij , is here defined , ~s : AWij -rlapi6pj , where { , ,pj ( 1 -- , , , ) ( t , ,j `` pJ ) , if j is an output unit a , ,~ = , ,vj ( l _avs ) ~vk , oik , ( a ) k if j is a hidden unit Ilere , Zp is the target output vector which the network lnnst learn t .</sentence>
				<definiendum id="0">weight update ~XlOij</definiendum>
				<definiendum id="1">oik</definiendum>
				<definiendum id="2">Zp</definiendum>
				<definiens id="0">a hidden unit Ilere</definiens>
				<definiens id="1">the target output vector which the network lnnst learn t</definiens>
			</definition>
			<definition id="3">
				<sentence>The network learns during the training to activate that output unit which represents the correct tag and to deactivate all other output units , llence , in the trained network , the output unit with the higlu .</sentence>
				<definiendum id="0">network</definiendum>
				<definiendum id="1">llence</definiendum>
				<definiens id="0">learns during the training to activate that output unit which represents the correct tag and to deactivate all other output units</definiens>
			</definition>
			<definition id="4">
				<sentence>suffix ess 10 I gp/ &lt; iS l_5 __1 2 t~a~ 143 sufllx ness suffix less 1 85 2 8 45 0 0 2 48 95 labeled i. This node is a leaf and the attached tag probability vector ( which is not shown in lib. 3 ) is returned. The suffix lexicon was automatically built from the training corpus. First , a sujJiz tree wits constructed from the suffices of length 5 of sill words wliich were annotated with an open class l ) art-of-speecli s. Then tag frequencies were cotlnted for all suffices and stored at the corresponding tree nodes. In the next step , an information measure I ( S ) was calculated for each node of the tree : I ( S ) = ~ P ( posiS ) tomd ' ( p , &gt; , qS ) ( 7 ) po* IIere , S is the suffix which corresponds to the current node and P ( poslS ) is the probability of tag pos given a word with suffix S. Using this information measure , the suffix tree has been pruned .</sentence>
				<definiendum id="0">attached tag probability vector</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">the suffices of length 5 of sill words wliich were annotated with an open class l ) art-of-speecli s. Then tag frequencies were cotlnted for all suffices</definiens>
				<definiens id="1">an information measure I ( S ) was calculated for each node of the tree : I ( S ) = ~ P ( posiS ) tomd ' ( p , &gt; , qS ) ( 7 ) po* IIere ,</definiens>
				<definiens id="2">the suffix which corresponds to the current node</definiens>
				<definiens id="3">the probability of tag pos given a word with suffix S. Using this information measure , the suffix tree has been pruned</definiens>
			</definition>
			<definition id="5">
				<sentence>For each leaf , the weighted information gain G ( aS ) was calculated : a ( aS ) = V ( aS ) ( S ( S ) S ( &lt; , S ) ) , ( 8 ) where S is the suffix of the parent node , aS is the suffix of the current node and F ( aS ) is the frequency of suffix nS .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">S</definiendum>
				<definiendum id="2">aS</definiendum>
				<definiendum id="3">aS )</definiendum>
				<definiens id="0">the suffix of the parent node</definiens>
				<definiens id="1">the suffix of the current node</definiens>
				<definiens id="2">the frequency of suffix nS</definiens>
			</definition>
			<definition id="6">
				<sentence>To illustrate this process consider the following example , where ess is the suffix of the parent node , less is tim suffix of one child node and hess is the suffix of the other child node .</sentence>
				<definiendum id="0">ess</definiendum>
				<definiendum id="1">hess</definiendum>
				<definiens id="0">the suffix of the parent node , less is tim suffix of one child node</definiens>
				<definiens id="1">the suffix of the other child node</definiens>
			</definition>
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>Bup : A bottom-up parser embedded in prolog .</sentence>
				<definiendum id="0">Bup</definiendum>
				<definiens id="0">A bottom-up parser embedded in prolog</definiens>
			</definition>
</paper>

		<paper id="2149">
			<definition id="0">
				<sentence>XTAG is a large on-going project to develop a widecoverage grammar for English , based on the l , exicalized Tree Adjoining Grammar ( I 3 '' AG ) tbrmalism .</sentence>
				<definiendum id="0">XTAG</definiendum>
				<definiens id="0">a large on-going project to develop a widecoverage grammar for English , based on the l , exicalized Tree Adjoining Grammar ( I 3 '' AG ) tbrmalism</definiens>
			</definition>
			<definition id="1">
				<sentence>LTAG is a lexicalized mildly-context sensitive tree rewriting system \ [ Joshi et al. , 1975 ; Schabes , 1990\ ] that is closely related to Dependency Grammars and Categorial Grammars .</sentence>
				<definiendum id="0">LTAG</definiendum>
				<definiens id="0">a lexicalized mildly-context sensitive tree rewriting system \ [ Joshi et al. , 1975 ; Schabes , 1990\ ] that is closely related to Dependency Grammars and Categorial Grammars</definiens>
			</definition>
			<definition id="2">
				<sentence>Each syntactic entry consists of an INDEX feld , the uninflected form under which the entry is compiled in the database , an ENTRY field , which contains all of the lexical items that will anchor the associated tree ( s ) , a pos field , which gives the part-of-speech for the lexical item ( s ) in the ENTRY feld , and then either ( but not both ) a TREES or FAM field .</sentence>
				<definiendum id="0">syntactic entry</definiendum>
				<definiendum id="1">ENTRY field</definiendum>
				<definiendum id="2">pos field</definiendum>
				<definiens id="0">consists of an INDEX feld , the uninflected form under which the entry is compiled in the database</definiens>
				<definiens id="1">contains all of the lexical items that will anchor the associated tree ( s ) , a</definiens>
				<definiens id="2">gives the part-of-speech for the lexical item ( s ) in the ENTRY feld , and then either ( but not both ) a TREES or FAM field</definiens>
			</definition>
			<definition id="3">
				<sentence>INDEX : map/I \ ] gNTRY : NP0 map out NP1 POS : NP0 V PL NP1 FAM : Tnx0Vplnxl INDEX : map/3 ENTRY : map POS : N TREES : { rN , ( ~NXdxN , /iNn FS : # N_wh- , # N_ .</sentence>
				<definiendum id="0">INDEX</definiendum>
				<definiens id="0">map/I \ ] gNTRY : NP0 map out NP1 POS : NP0 V PL NP1 FAM : Tnx0Vplnxl INDEX : map/3 ENTRY : map POS</definiens>
			</definition>
			<definition id="4">
				<sentence>Precision is the number of bracketed constituents the system got right divided by the number of bracketed constituents in the system 's parse .</sentence>
				<definiendum id="0">Precision</definiendum>
			</definition>
</paper>

		<paper id="2185">
			<definition id="0">
				<sentence>The third method is in fact a set of the second method , and both of them use semantic information ( i.e. company name , human name. , title ) , syntactic patterns ( i.e. where a conll ) any name , a human name , or a title appears in a sentence ) and several specific lexical items which come immediately after tim company Ilallies .</sentence>
				<definiendum id="0">syntactic patterns</definiendum>
			</definition>
</paper>

		<paper id="2161">
			<definition id="0">
				<sentence>Instead , the operation of the parser consists of a sequence of transactions .</sentence>
				<definiendum id="0">operation of the parser</definiendum>
				<definiens id="0">consists of a sequence of transactions</definiens>
			</definition>
</paper>

		<paper id="2126">
			<definition id="0">
				<sentence>Chierchia represents functional anaphoric links by co-indexing a functional antecedent with a superscript and FDD with a subscript as shown below .</sentence>
				<definiendum id="0">Chierchia</definiendum>
				<definiens id="0">functional anaphoric links by co-indexing a functional antecedent with a superscript</definiens>
			</definition>
			<definition id="1">
				<sentence>Relational FDD denotes relations between objects and these relations are lexical properties of head nouns of FDD .</sentence>
				<definiendum id="0">Relational FDD</definiendum>
				<definiens id="0">relations between objects and these relations are lexical properties of head nouns of FDD</definiens>
			</definition>
			<definition id="2">
				<sentence>Following Abney 's ( 1987 ) DI ' hypothesis , Barker proposes the following syntactic analysis of possessives .</sentence>
				<definiendum id="0">Barker</definiendum>
				<definiens id="0">proposes the following syntactic analysis of possessives</definiens>
			</definition>
			<definition id="3">
				<sentence>A selection set is a maximal set of objects in a given discourse that satisfies descriptions in the head nouns of the non-relational FDD .</sentence>
				<definiendum id="0">selection set</definiendum>
				<definiens id="0">a maximal set of objects in a given discourse that satisfies descriptions in the head nouns of the non-relational FDD</definiens>
			</definition>
</paper>

		<paper id="2150">
			<definition id="0">
				<sentence>The probability of a derivation ( tree ) is the product of the probabilities of bile derivation steps that produces the tree .</sentence>
				<definiendum id="0">probability of a derivation</definiendum>
				<definiens id="0">the product of the probabilities of bile derivation steps that produces the tree</definiens>
			</definition>
			<definition id="1">
				<sentence>'Che stochast , ic language generated by a stochastic grammar ( 7 , HL ( G ) , is defined as the set of all sentences generated by the grammar with their probabilities .</sentence>
				<definiendum id="0">HL ( G )</definiendum>
			</definition>
			<definition id="2">
				<sentence>A stochastic gr~mmlar G is an adequate model of a language L if on its basis we can correctly compute tile probabilities of the sentences in the hmgnage L. Of course this assumes a statistical analysis of a language corpus .</sentence>
				<definiendum id="0">stochastic gr~mmlar G</definiendum>
				<definiens id="0">an adequate model of a language L if on its basis we can correctly compute tile probabilities of the sentences in the hmgnage L. Of course this assumes a statistical analysis of a language corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>Definition 2.1 A weakly restricted stochastic grammat '' Gzv is a pair '' ( Cc , A ) , where Cc = ( VN , ~ , } , , P , X ) is a conlex # free flrammar and A is a set of functions A = { p~lA~ C VN } where , if j E 1 ... t~ ( Ai ) and k E 1 ... \ [ CA , I , pi ( j , k ) = Pij~ '' G \ [ 0 , 1\ ] The set of productions P contains ezacily one produclion for start symbol S. In words , Plj~ stands for the probability that the k-th production with left~hand side Ai is used for rewriting the j-th right-hand side-occurrence of nonterminal Ai .</sentence>
				<definiendum id="0">A</definiendum>
				<definiendum id="1">Plj~</definiendum>
				<definiens id="0">a pair ''</definiens>
				<definiens id="1">a conlex # free flrammar</definiens>
			</definition>
			<definition id="4">
				<sentence>, ti , ~ ( q ) j , ( q ) \ ] , in which q is a production , n ( q ) is the number of nonterminals in the right-hand side of q and tij denotes a ( sub ) tree with the j-th 930 occurrence of ' nonterminal Ai at its root .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
			<definition id="5">
				<sentence>l ) efinition 2.3 73e probability of a string x Z~l L ( G~ ) is defined as The distribution langnage \ ] ) L ( Ow ) and stochastic language ,5'/ ; ( ( \ ] ~ ) of a wealdy restrieted gramuxar ( ( ; ~ , A ) are detined ana\ ] oguous to : ; l~e distribution language and stochastic language of an nnrestricted granllilar .</sentence>
				<definiendum id="0">l ) efinition 2.3 73e probability of a string x Z~l L ( G~ )</definiendum>
			</definition>
			<definition id="6">
				<sentence>qij results in at most one term consisting of the form po , f ( si , : ... . , s # , /~ ( Ak ) ) where f does not depend on s ... .. and Pij , is one of the probabilities resulting from applying Pi to j and some h in 1 ... \ ] ( '~fA , \ ] .</sentence>
				<definiendum id="0">Pij</definiendum>
				<definiens id="0">s # , /~ ( Ak ) ) where f does not depend on s ... .. and</definiens>
			</definition>
			<definition id="7">
				<sentence>tvj~ 0 ~ i &lt; j &lt; IWl denotes the substring wi+i..</sentence>
				<definiendum id="0">IWl</definiendum>
			</definition>
			<definition id="8">
				<sentence>( p~ , ) and } p ( q.r ) ( ps ) estimates the probability that a derivation of a string w C E will involve at least one expansion of the nonterminal occurrence Ap ( q.~ ) .</sentence>
				<definiendum id="0">q.r ) ( ps )</definiendum>
			</definition>
			<definition id="9">
				<sentence>Probabilistic Languages : A Review and Some Open Questions .</sentence>
				<definiendum id="0">Probabilistic Languages</definiendum>
			</definition>
</paper>

		<paper id="1104">
			<definition id="0">
				<sentence>The sentence axis is a pattern that describes the sentence structure at an appropriate level .</sentence>
				<definiendum id="0">sentence axis</definiendum>
				<definiens id="0">a pattern that describes the sentence structure at an appropriate level</definiens>
			</definition>
			<definition id="1">
				<sentence>630 We can also repeat longer sequences , for instance the set { -- FMAINV &lt; N ( ) M-I , 'MAINV +FAUXV SUBJ OBJ } provides the axis SUBJ +FAUXV ... . FMAINV ... oBa ... .. FMAINV ... oBa ... . FMAINV OBJ • .. &lt; NOM-FMAINV ... OBa ... -FMAINV SUBJ ... . FMAINV ... And we lbrm a generalisation SUBJ +FAUXV \ [ ... .. FMAINV ... OBJ \ ] + • .. &lt; NOM-FMAINV ... OBJ ... -FMAINV SUBJ ... . FMAINV ... Note that we added silently an extra ( tot be.tweeu one -FMAINV and OBY in order not to make , distinctions between -FMAINV OBg and -FMAINV. . . OBJ here. Another generalisation can be made using equivalence clauses. We can , assign several syntactic tags to the same equivalence class ( for instance -I '' MAINV , &lt; NOM-FMAIN V arrd &lt; P-FMA \ [ N V ) , and then gen-. crate axes as above. 'l'he result would be SUBJ +FAUXV \ [ ... nonfinv ... OBJ \ ] -I• .. nontinv SUBJ ... nonfinv ... where nonfinv denotes both -FMAINV ; u , d &lt; NOM FMAINV ( and also &lt; P-.1 , 'MAINV ) . The equivalence classes are essential in the present tag set because the syntactic arguments of finite verbs are not distinguished from the arguments of nontlnite verbs. Using equivalence classes for the finite attd nonfinite verbs , we may tmiht an generallsation that ; tl ) plies to both types of clauses. Another way to solve the problem , is to add new tags for the arguments of the nontinite clauses , arid make several axes for them. In the second phase of the pattern parsing scheme we apply local patterns , the joints. They contain ioformation about what kinds of modifiers have what kinds of heads , and vice versa. For instance , in the following sentence 4 the words fair and crack are both three ways ambiguous before the axes are applied. He_SUBJ gives_-t-FMAINV us I-OBa a l ) N &gt; fairAN &gt; /SUBJ/NN &gt; crack_OBJ/+FMAINV/SUIU theT , fll ) VL we .</sentence>
				<definiendum id="0">nonfinv</definiendum>
				<definiendum id="1">NOM FMAINV</definiendum>
				<definiendum id="2">equivalence classes</definiendum>
				<definiens id="0">V arrd &lt; P-FMA \ [ N V )</definiens>
			</definition>
			<definition id="2">
				<sentence>re to be exactly one subject complement , there are only two possil ) le paths which the joints can select from : less AD-A &gt; attenlive_l'COMPLS and less J'COMPl , S attentive &lt; NOM. Generating the joints is quite straightforward. We produce different alternative variants for each syntac : tic tag and select some of them. Wc use a couple of parameters to validate possible joint candidates. * q'he error margin provides the probability for checking if the context is relevant , i.e. , there is enottg\ ] t evidence for it among the existing contexts of the tag. This probability may be used in two ways : l , 'or a syntactic tag , generate all contexts ( of length n ) tl , at appear in the corpora. Select all those contexts that are frequent enough. Do this with all n 's wdues : 1 , 2 , ... -First generate all contexts of length t. Select those contexts that are fregnent enough among the generated contexts. Next , , lengtlmn all contexts selected in the previous step by one word. Select those contexts that are frequent enough among the new generated context , s. R.epeat this sulficient malty times. lloth algorithms l ) roduce a set of contexts of different lengths. Characteristic for t ) oth the algorithms is that if they haw ; gene.rated a context of length n that matches a syntactic function in a sentence , there is also a context of length n 1 that matches. • The absolute , margin mmd ) er of cases that is needed for the evidence , of till : generated context. If therc is less cvidencc , it is not taken into account arm a shorter context is generated. '.\ [ 'his is used to prevent strange behaviour with syntactic tags that are not very common or with a corpus that is not big enough. ® 'l'he maximum length of the context to be generated. l ) uring the parsing , longer contexts are preferred to shorter ones. The parsing problem is thus a kind of pattern matching problem : we have to match a pattern ( context ) arouml each tag and tlnd a sequence of syntactic tags ( analysis of the sentence ) that h~m the best score. The scoring fimetion depends on the lengths of the matched patterns. 631 I text. II words\ ] ambiguity rate I error rate \ ] bbl 1734 ' 12.4 % 2.4 % bb2 1674 14.2 % 2.8 % 1599 18.6 % 1.6 % wsj `` 2309 16.2 % 2.9 % \ ] . totall\ ] 7316 I 15.3 % \ ] 2.2 % -1 Figure 1 : Test corpora after syntactical analysis of ENGCG. CORPORA Information concerning the axes was acquired from a manually checked and fully disambiguated corpus 5 of about 30,000 words and 1,300 sentences. Local context information was derived from corpora that were analysed by ENGCG. We generated three different parsers using three different corpora 6. Each corpus contains about 10 million words. For evaluation we used four test samples ( in Figure 1 ) . Three of them were taken frmn corpora that we used to generate the parsers and one is an additional sample. The samples that are named bbl , today and wsj belong to the corpora from which three different joint parsers , called BB1 , TODAY and WSJ respectively , were generated. Sample bb~ is the additional sample that is not used during development of the parsers. The ambiguity rate tells us how much ambiguity is left after ENGCG analysis , i.e. how many words still have one or more alternative syntactic tags. The error rate shows us how many syntactic errors ENGCG has made while analysing the texts. Note that the ambiguity denotes the amount of work to be done , and the error rate denotes the number of errors that already exist in the input of our parser. All the samples were analysed with each generated parser ( in Figure 2 ) . The idea is to find out about the effects of different text types on the generation of the parsers. The present method is applied to reduce the syntactic ambiguity to zero. Success rates variate from 88.5 % to 94.3 % in ditferent samples. There is maximally a 0.5 percentage points difference in the success rate between the parsers when applied to the same data. Applying a parser to a sample from the same corpus of which it was generated does not generally show better results. Some of the distinctions left open by ENGCG may not be structurally resolvable ( see \ [ Karlsson et al. , 1994\ ] ) . A case in point is the prepositional attachment ambiguity , which alone represents about 20 % of the ambiguity in the ENGCG output. The proper way to deal with it in the CG framework is probably using lexical information. Therefore , as long as there still is structurally unresolvable ambiguity in the ENGCG output , a certain amount of processing before the present system SOonsisting of 15 individual texts from the Bank of English project \ [ J~.rvinen , 1994\ ] . The texts were chosen to cover a variety of text types but due to small size and intuitive sampling it can not be truly representative. 6We use here Today newspaper , The t '' , conomist -k Wall Street Journal and British Books. _T_Text s ~s e r s \ [ riB1 .I TODAY ~~ 92.5 % \ [ 92~ % __1 91.9-W-o Figure 2 : Overall parsing success rate in syntactically amdysed samples might improve the results considerably , e.g. , convertins structurally unresolvable syntactic tags to a single underspecified tag. \ [ , 'or instance , resolving prepositional attachment ambiguity by other means would iruprove the success rate of the current system to 90.5 % 95.5 % . In the wsj sample ttLe improvement would be as much a.s 2.0 percentage points. The differences between success rates in different samples are partly explained by tile error types that are characteristic of the samples. For example , in the Wall Street Journal adverbials of time are easily parsed erroneously. This may cause an accumulation effect , ms happens in tile following sentence MAN AG Tuesday said fiscal 1989 net income rose 25 % and said it , will raise its dividend for lhe year ended June 30 by about the same percentage. Tile phrase the year ended June 30 gets the analysis the_DN &gt; year_NN &gt; ended_AN &gt; June_NN &gt; 30_ &lt; P while the correct ( or wanted ) result is lhe DN &gt; year_ &lt; P ended_ &lt; NOM-FMAINV June_ADVL 30 &lt; NOM Different kind of errors appear in text bb ! which contains incomplete sentences. The parser prefers complete sentences and produces errors in sentences like There w~s Provence in mid-autumn. Gold Zints. Air so serene you could look out over the sea for tens of miles. Rehabilitalion walks with him along tim woodland l ) aths. The errors are : gold tints is parsed a.s svbjeel main verb ~s well ~m r'ehabililation walks , and air is analysed , as a main verb , Other words have the appropriate analyses. The strict sequentiality of morphological and syntactic analysis in ENGCG does not allow the use of syntactic information in morphological disambiguation. The present method makes it possible to prune the remaining morphological ambiguities , i.e. do some part-of-speech tagging. Morphological ambiguity remains unresoNed if the chosen syntactic tag is present in two or more morphological readings of the same word. Morphological ambiguity 7 is reduced close to zero ( about 0.3 % in all the samples together ) and the overall success rate of ENGCG + our pattern parser is 98.7 % . r After ENGCG the amount of nmrphologic ' , d ambiguity in the test data was 2.9 % , with au error rate of 0.4 % . 632 We discussed combining a linguistic rule-based parser and a corpus-based empirical parser. We divide the parsing process into two parts : applying linguistic information and applying corpus-based patterns. The linguistic rules are regarded ms more reliable than the corpus-based generalisations. They are therefore applied first. The idea is to use reliable linguistic information as long as it is possible. After certain phase it comes harder and harder to make new linguistic constraints to eliminate the remaining ambiguity. Therefore we use corpus-based patterns to do the remaining disand ) iguation. The overall success rate of the combination of the linguistic rule-based parser and the corpus-based pattern parser is good. If some unrcsolvable ambiguity is left pending ( like prepositional attachment ) , the total success rate of our morphological and surface-syntactic analysis is only slightly worse than that of many probabilistic part-of-speech taggers. It is a good result because we do more than just label each word with a morphological tags ( i.e. noun , verb , etc. ) , we label them also with syntactic fimction tags ( i.e. subject , object , subject complement , etc. ) . Some improvements might be achieved by modifying the syntactic tag set of ENGCG. As discussed above , the ( syntactic ) tag set of the ENGCG is not probably optimal. Some ambiguity is not resolvable ( like prepositional attachment ) and some distinctions arc not made ( like subjects of the finite and the nonfinite clauses ) . A better tag set for surface-syntactic parsing is presented in \ [ Voutilainen and Tapanainen , 1993\ ] . But we have not modified the present tag set because it is not clear whether small changes would improve the result significantly when compared to the effort needed. Although it is not possible to fully disambiguate the syntax in ENGCG , the rate of disambiguation can be improved using a more powerful linguistic rule tbrmalism ( see \ [ Koskenniemi el al. , 1992 ; Koskenniemi , 1990 ; Tapanainen , 1991\ ] ) . The results reported in this sudy can most likely be improved by writing a syntactic grammar in the finite-state framework. The same kind of pattern parser could then be used for disambiguating the resulting analyses. The Constraint Grammar framework was originally proposed by Fred Karlsson \ [ 1990\ ] . The extensive work on the description of English was ( tone by Atro Voutilainen , Juha tleikkil~ and Arto Anttila \ [ 1992\ ] . Timo J~rvinen \ [ 1994\ ] has developed the syntactic constraint system further. ENGCG uses Kimmo Koskenniemi 's \ [ 1983\ ] two-level morphological analyser and Past Tapanainen 's implementation of Constraint Grammar parser. We want to thank Fred Karlsson , Lauri Karttunen , Annie Za~nen , Atro Voutilainen and Gregory Grefenstette for commenting this paper. Oriented Introduction. Publications nr. 21. Dept. of General Linguistics , University of Ilelsinki , 1992. \ [ Voutilainen and Tapanainen , 199q\ ] Atro Voutilainen and Past Tapanainen. Ambiguity resolution in a rednctionistic parser. In Proceedings of of Sixth Conference of the European Chapter of the Association for Computational Linguistics. EACL-93. pp. 394403 , Utrecht , Netherlands. 1993. A TIIE TAG SET 2'his appendix contains the syntactic tags we have used. Tbe list is adopted from \ [ Voutilainen et al. , 1992\ ] . To obtain also the morphological `` part-ofspeech '' tags you can send an empty e-mail message to engcg-info @ ling.helsinki.fi. 633 +FAUXV = Finite Auxiliary Predicator : lie ca_~n read. , -FAUXV = Nonfinite Auxiliary Predicator : Fie may have_ read. , +FMAINV = Finite Main Predicator : He reads. , -FMAINV = Nonfinite Main Predicator : He has re a~d. , NPHIL = Stray NP : Volume I : Syntax , SUBJ = Subject : H__~e reads. , F-SUBJ = Formal Subject : There was some argument about that. I_tt is raining. , OBJ = Object : He read a book. , I-OBJ = Indirect Object : He gave Mary a book. , PCOMPL-S = Subject Complement~ is a fool. , PCOMPL-O = Object Complement : I eonsid~ -- him a fool. , AI~V-L = Adverbial : He came home late. He is in the Ca r. , O-ADVL = Object Adverbial : lie ran two miles. APP = Apposition : Helsinki , the capital of Finland , N = Title : King George and Mr. DN &gt; Dete~ner : He read the book. , NN &gt; = Premodifying Noun : The car park was full. , AN &gt; -Premodifying Adjective : The bh , e car is mine. , QN &gt; -- Premodifying Quantifier : He had two sandwiches and some coffee. , GN &gt; = Premodifying Genitive : M._yy car and flill 's bike are blue. , AD-A &gt; = Premodifying Ad-Adjective : She is very intelligent. , &lt; NOM-OF = Postmodifying Of : Five of you will pass. , &lt; NOM-FMAINV = Postmodifying Nonfinite Verb : He has the licence lo kill. John is easy to please. The man drinking coffee is my uncle. , &lt; AD -- -A -- -~= Postmodifying Ad-Adjective : This is good enough. , &lt; ~ = Other Postmodifier : The man with glasses is my uncle. He is the president elect. The man i_~n the moon fell down too soon. , INFMAP~K &gt; = Infinitive Marker : John wants t~ read. , &lt; P-FMAINV = Nonfinite Verb as Complement of Preposition : This is a brush for cleaning. , &lt; P = Other Complement of Prel ) ~ He is in tile car. , CC -- Coordinator : John and Bill are friends. , CS = Subordinator : /f Johu is there , we shall go , too. , 634</sentence>
				<definiendum id="0">error rate</definiendum>
				<definiendum id="1">&gt; -Premodifying Adjective</definiendum>
				<definiendum id="2">Preposition</definiendum>
				<definiens id="0">tic tag and select some of them. Wc use a couple of parameters to validate possible joint candidates. * q'he error margin provides the probability for checking if the context is relevant , i.e. , there is enottg\ ] t evidence for it among the existing contexts of the tag. This probability may be used in two ways : l , 'or a syntactic tag , generate all contexts ( of length n ) tl , at appear in the corpora. Select all those contexts that are frequent enough. Do this with all n 's wdues : 1 , 2 , ... -First generate all contexts of length t. Select those contexts that are fregnent enough among the generated contexts. Next , , lengtlmn all contexts selected in the previous step by one word. Select those contexts that are frequent enough among the new generated context</definiens>
				<definiens id="1">matches a syntactic function in a sentence</definiens>
				<definiens id="2">used to prevent strange behaviour with syntactic tags that are not very common or with a corpus that is not big enough. ® 'l'he maximum length of the context to be generated. l ) uring the parsing , longer contexts are preferred to shorter ones. The parsing problem is thus a kind of pattern matching problem : we have to match a pattern ( context ) arouml each tag and tlnd a sequence of syntactic tags ( analysis of the sentence</definiens>
				<definiens id="3">Test corpora after syntactical analysis of ENGCG. CORPORA Information concerning the axes was acquired from a manually checked and fully disambiguated corpus 5 of about 30,000 words and 1,300 sentences. Local context information was derived from corpora that were analysed by ENGCG. We generated three different parsers using three different corpora 6. Each corpus contains about 10 million words. For evaluation we used four test samples ( in Figure 1 ) . Three of them were taken frmn corpora that we used to generate the parsers and one is an additional sample. The samples that are named bbl , today and wsj belong to the corpora from which three different joint parsers , called BB1 , TODAY and WSJ respectively , were generated. Sample bb~ is the additional sample that is not used during development of the parsers. The ambiguity rate tells us how much ambiguity is left after ENGCG analysis , i.e. how many words still have one or more alternative syntactic tags. The error rate shows us how many syntactic errors ENGCG has made while analysing the texts. Note that the ambiguity denotes the amount of work to be done</definiens>
				<definiens id="4">the number of errors that already exist in the input of our parser. All the samples were analysed with each generated parser ( in Figure 2 ) . The idea is to find out about the effects of different text types on the generation of the parsers. The present method is applied to reduce the syntactic ambiguity to zero. Success rates variate from 88.5 % to 94.3 % in ditferent samples. There is maximally a 0.5 percentage points difference in the success rate between the parsers when applied to the same data. Applying a parser to a sample from the same corpus of which it was generated does not generally show better results. Some of the distinctions left open by ENGCG may not be structurally resolvable ( see \ [ Karlsson et al. , 1994\ ] )</definiens>
				<definiens id="5">a certain amount of processing before the present system SOonsisting of 15 individual texts from the Bank of English project \ [ J~.rvinen , 1994\ ] . The texts were chosen to cover a variety of text types but due to small size and intuitive sampling it can not be truly representative. 6We use here Today newspaper , The t '' , conomist -k Wall Street Journal and British Books. _T_Text s ~s e r s \ [</definiens>
				<definiens id="6">Overall parsing success rate in syntactically amdysed samples might improve the results considerably , e.g. , convertins structurally unresolvable syntactic tags to a single underspecified tag. \ [</definiens>
				<definiens id="7">differences between success rates in different samples are partly explained by tile error types that are characteristic of the samples. For example , in the Wall Street Journal adverbials of time are easily parsed erroneously. This may cause an accumulation effect , ms happens in tile following sentence MAN AG Tuesday</definiens>
				<definiens id="8">Different kind of errors appear in text bb ! which contains incomplete sentences. The parser prefers complete sentences and produces errors in sentences like There w~s Provence in mid-autumn. Gold Zints. Air so serene you could look out over the sea for tens of miles. Rehabilitalion walks with him along tim woodland l ) aths. The errors are : gold tints is parsed a.s svbjeel main verb ~s well ~m r'ehabililation walks , and air is analysed , as a main verb , Other words have the appropriate analyses. The strict sequentiality of morphological and syntactic analysis in ENGCG does not allow the use of syntactic information in morphological disambiguation. The present method makes it possible to prune the remaining morphological ambiguities , i.e. do some part-of-speech tagging. Morphological ambiguity remains unresoNed if the chosen syntactic tag is present in two or more morphological readings of the same word. Morphological ambiguity 7 is reduced close to zero ( about 0.3 % in all the samples together</definiens>
				<definiens id="9">rule-based parser and a corpus-based empirical parser. We divide the parsing process into two parts : applying linguistic information and applying corpus-based patterns. The linguistic rules are regarded ms more reliable than the corpus-based generalisations. They are therefore applied first. The idea is to use reliable linguistic information as long as it is possible. After certain phase it comes harder and harder to make new linguistic constraints to eliminate the remaining ambiguity. Therefore we use corpus-based patterns to do the remaining disand ) iguation. The overall success rate of the combination of the linguistic rule-based parser and the corpus-based pattern parser is good. If some unrcsolvable ambiguity is left pending ( like prepositional attachment ) , the total success rate of our morphological and surface-syntactic analysis is only slightly worse than that of many probabilistic part-of-speech taggers. It is a good result because we do more than just label each word with a morphological tags ( i.e. noun , verb , etc. ) , we label them also with syntactic fimction tags ( i.e. subject , object , subject complement , etc. ) . Some improvements might be achieved by modifying the syntactic tag set of ENGCG. As discussed above , the ( syntactic ) tag set of the ENGCG is not probably optimal. Some ambiguity is not resolvable ( like prepositional attachment ) and some distinctions arc not made ( like subjects of the finite and the nonfinite clauses</definiens>
				<definiens id="10">] has developed the syntactic constraint system further. ENGCG uses Kimmo Koskenniemi 's \ [ 1983\ ] two-level morphological analyser and Past Tapanainen 's implementation of Constraint Grammar parser. We want to thank Fred Karlsson , Lauri Karttunen , Annie Za~nen , Atro Voutilainen and Gregory Grefenstette for commenting this paper. Oriented Introduction. Publications nr. 21. Dept. of General Linguistics , University of Ilelsinki , 1992. \ [ Voutilainen and Tapanainen , 199q\ ] Atro Voutilainen and Past Tapanainen. Ambiguity resolution in a rednctionistic parser. In Proceedings of of Sixth Conference of the European Chapter of the Association for Computational Linguistics. EACL-93. pp. 394403 , Utrecht , Netherlands. 1993. A TIIE TAG SET 2'his appendix contains the syntactic tags we have used. Tbe list is adopted from \ [ Voutilainen et al. , 1992\ ] . To obtain also the morphological `` part-ofspeech '' tags you can send an empty e-mail message to engcg-info @ ling.helsinki.fi. 633 +FAUXV = Finite Auxiliary Predicator : lie ca_~n read. , -FAUXV = Nonfinite Auxiliary Predicator : Fie may have_ read. , +FMAINV = Finite Main Predicator : He reads. , -FMAINV = Nonfinite Main Predicator : He has re a~d. , NPHIL = Stray NP : Volume I : Syntax</definiens>
				<definiens id="11">a fool. , PCOMPL-O = Object Complement : I eonsid~ -- him a fool. , AI~V-L = Adverbial : He came home late. He is in the Ca r. , O-ADVL = Object Adverbial : lie ran two miles. APP = Apposition : Helsinki , the capital of Finland</definiens>
				<definiens id="12">The man with glasses is my uncle. He is the president elect. The man i_~n the moon fell down too soon. , INFMAP~K &gt; = Infinitive Marker : John wants t~ read. , &lt; P-FMAINV = Nonfinite Verb as Complement of</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>Many rules ( or patterns ) of word fo , 'mation are highly productive , which makes it impossible to store all complex lexical entries in a static lexicon .</sentence>
				<definiendum id="0">Many rules</definiendum>
				<definiens id="0">makes it impossible to store all complex lexical entries in a static lexicon</definiens>
			</definition>
			<definition id="1">
				<sentence>Cf. : the following examples , taken from a corpus of medical abstracks : ragweed allergic dffnitis house-thtst-allergic a , vthma house chtst asthma patient daily symptom diary cards fluticasone propionate aqueous nasal spray In most grammatical descriptions , strings consisting of nouns ( like house dust asthma ) are treated as compound nouns , whereas a complex including an adjective followed by a ' noun is normally labelled as an NP .</sentence>
				<definiendum id="0">Cf.</definiendum>
				<definiens id="0">strings consisting of nouns ( like house dust asthma</definiens>
			</definition>
			<definition id="2">
				<sentence>/* patlem : allergen , complex ( a ) syndrom : grass pollen ( allergic ) asthma */ flcx ( Tlex , mean ( Com plex ) , n , \ [ syndrom\ ] , A , B , C ) : append ( \ [ Attr , Alll,1 ) is , Tlex ) , ( lex ( Dis , n , \ [ syndroml , A , B , C ) , tlex ( \ [ Attr , All\ ] , M , n , \ [ al lergenl ... ... ... . ) , append ( Dis , \ [ because_ofl , New ) , append ( New , \ [ allergcn ( 1Atlr , All \ ] ) l , Complex ) ) ; ( lex ( Dis , a , Sem ... ...</sentence>
				<definiendum id="0">grass pollen</definiendum>
				<definiendum id="1">, B , C )</definiendum>
				<definiendum id="2">Dis</definiendum>
				<definiens id="0">mean ( Com plex ) , n , \ [ syndrom\ ] , A</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , the semantic representation of grass pollen asthma l ) atieHt becomes associated with the Polish pattern ( simplified notation ) : paticnt , su ffcrin~from , syndrom ( \ [ X , bccause of , allcrgen ( attr , All ) \ ] ) -- &gt; n ( paticnt , Agr , nom ) , ln'tact ( su ffer , Agr , nom ) , prep ( su flEr , Prcp , Case ) , n ( X , Agr2 , Case ) , prtpass ( cause , Agr2 , Case ) , n ( allergy , Agr3 , ins ) , prcp ( _ , na , ack ) , n ( All , Agr3 , ack ) , n ( Attr , agr ( Gen , pl ) , gen ) .</sentence>
				<definiendum id="0">Polish pattern</definiendum>
				<definiendum id="1">paticnt</definiendum>
				<definiendum id="2">n ( X</definiendum>
				<definiendum id="3">prtpass</definiendum>
			</definition>
</paper>

		<paper id="2207">
			<definition id="0">
				<sentence>Assume that a model M is an ordered pair ( D , F ) where D is a domain , a non-empty set and F an interpretation function assigning a semantic value to each non-logical constant of the language .</sentence>
				<definiendum id="0">D</definiendum>
				<definiens id="0">an ordered pair</definiens>
				<definiens id="1">a domain , a non-empty set and F an interpretation function assigning a semantic value to each non-logical constant of the language</definiens>
			</definition>
			<definition id="1">
				<sentence>W is a possible world .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">a possible world</definiens>
			</definition>
			<definition id="2">
				<sentence>( 7 ) It~ { o11= { &lt; ~1 , ~ ) 1~= , ( n= &lt; a , g ) , g=g , \ [ xl , &lt; n , nDell { 011 ) } . ( 8 ) II { 0 ( x ) ll= { &lt; ~ , , ~DI~ , =~ , g~= g2 , g , ( x ) ~F ( ep ) } . Where F ( ~0 ) is a set of individuals , F is the function in a model M. { p is an atomic formula. Here obviously , conjunction is treated like composition ( i.e. , compound statement { S~ , S~ } ) . Intuitively , the conjunction is treated in the sequential sense. The meanings of the formulas a , \ [ l/0\ ] , ¢l\ [ term t/0\ ] , g , \ [ x\ ] follow the statements in the preceding section. Assume that initially for all i , j , ai ( j ) ~t0 and a~ , gi in n~ denoted components concerned. Hence ( 9 ) &lt; n , , nD~il ( 2 ) li iff , by ( 6 ) , for some n3 , n3 ' , g.~ ( =gs ' ) , ( lO ) ( n~ , n.OellP_~key ( x ) ll and ( 11 ) &lt; ns ' , n2 ) ~liNOW lost ( x ) ii. ( 10 ) holds iff , by ( 5 ) , for some t ( &lt; al ( 0 ) ) , ( 12 ) ( ( al\ [ termt/0J , g , ) , n~ ) ~ \ ] \ ] ~xkey ( x ) iP iff , by ( 7 ) , for some h ( =g , \ [ x\ ] ) , ( 13 ) ( ( a , \ [ term t/0\ ] , gl\ [ x\ ] ) , 7~ ) e tlkey ( x ) H iff , by ( 8 ) , ( 14 ) ns= &lt; a , \ [ termt\ ] , g , \ [ x\ ] ) ( i.e. a.~ -=~r~\ [ term t/0\ ] , gs=gl\ [ x\ ] ) , and g , \ [ x\ ] ( x ) eF ( key ) , where g , \ [ x\ ] ( x ) =h ( x ) . ( 11 ) holds , iff , by ( 4 ) , ( 15 ) &lt; &lt; ~3'\ [ 1/0\ ] , g3 ' ) , nDeljlost ( x ) lt , iff by ( 8 ) , n2=as'\ [ 1/0\ ] , g , ~ ' ) ( i.e. , a2= ( r3'\ [ l/0\ ] and g2=g.~'=g.~ , ( by ( 6 ) ) =g , \ [ x\ ] ( by ( 14 ) ) , and gs ( x ) eF ( lost ) . It means that g.~ ( x ) ( =gl\ [ x\ ] ( x ) =-individual kn , say ) is a key at t ( &lt; t0 ) and gs ( x ) ( = k0 ) is lost at t~. On the other hand , for the formula ( 3 ) , ( 16 ) ( n , , n.0ei\ ] ( 3 ) ll iff , by ( 5 ) , for some t ( &lt; a , ( 0 ) ) , ( 17 ) ( &lt; ( rl\ [ term t/0\ ] , gl ) , n2 ) ll~ ( key ( x ) &amp; NOW lost ( x ) ) H , iff , by ( 6 ) for some h ( : =gl\ [ x\ ] ) , ( 18 ) ( ( a , \ [ term t/0J , g , \ [ x\ ] ) , n2 ) e Jlkey ( x ) &amp; NOW lost ( x ) H , iff , by ( 6 ) for some n3 , n.~ ' , g3 ( -- -g.~ ' ) , ( 19 ) ( @ , \ [ term t/0\ ] , g , \ [ x\ ] ) , us ) e IIkey ( x ) el and ( 20 ) &lt; n.~ ' , nDc\ ] tNOW lost ( x ) ll , ( 19 ) holds iff n.~= &lt; a , \ [ termt/0\ ] , g~\ [ x\ ] ) , and g , \ [ x\ ] ( x ) cF ( key ) , i.e. , ~ -- -a , \ [ term t/0~ and g3 =g , \ [ x\ ] , ( 20 ) holds iff , by ( 4 ) , ( 2t ) &lt; &lt; as'\ [ l/0\ ] , g.~ ' ) , nD~Hlost ( x ) \ ] \ [ iff , 7268 by ( 8 ) , 7r2= &lt; m~'\ [ 1/0\ ] , g3 ' &gt; , and g. , ' ( x ) c F ( lost ) , i.e. , ~2=~s'\ [ l/0J and g2=ga '= g3 = g , \ [ x\ ] .</sentence>
				<definiendum id="0">F</definiendum>
				<definiendum id="1">c F</definiendum>
				<definiens id="0">a set of individuals</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>RST has to be supplenlonted with a richer hltoulional context .</sentence>
				<definiendum id="0">RST</definiendum>
				<definiens id="0">has to be supplenlonted with a richer hltoulional context</definiens>
			</definition>
</paper>

		<paper id="2199">
			<definition id="0">
				<sentence>The transformation consists in two steps : ( 1 ) removal of empty-productions , followed by : ( 2 ) left-recursion elimination .</sentence>
				<definiendum id="0">transformation</definiendum>
				<definiens id="0">consists in two steps : ( 1 ) removal of empty-productions , followed by : ( 2 ) left-recursion elimination</definiens>
			</definition>
			<definition id="1">
				<sentence>aThe GGNF factorizes an arbitrary DCG into two components : a `` unit sub-DCG on the empty string '' , and another paa't consisting of rules whose right-hand side starts with a tm'minal .</sentence>
				<definiendum id="0">GGNF</definiendum>
				<definiens id="0">factorizes an arbitrary DCG into two components : a `` unit sub-DCG on the empty string ''</definiens>
			</definition>
			<definition id="2">
				<sentence>then the replacement of these two rules by the three rules ( where d_tc is a new nonterminal symbol , which represents a kind of `` transitive c ( osure '' of d ) : g ( X ) - , t ( Y ) , d_tc ( r , X ) .</sentence>
				<definiendum id="0">d_tc</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiens id="0">a new nonterminal symbol , which represents a kind of `` transitive c ( osure '' of d ) : g ( X ) -</definiens>
			</definition>
			<definition id="3">
				<sentence>elinlination ( t can be proven that , if I ) CG0 is an OP ( ) CG , the tb ( lowing transformation , which involves repeated partial evaluation of rules that rewrite into the empty string , terminates after a finite number of steps and produces a grammm : I ) CG without empty-l ) roductions which is equivalent to the initial grammar on noneml ) ty strings : s inimt : an otllineq ) ars~tble DC ( -II .</sentence>
				<definiendum id="0">OP ( ) CG</definiendum>
				<definiens id="0">involves repeated partial evaluation of rules that rewrite into the empty string</definiens>
			</definition>
			<definition id="4">
				<sentence>It involves the creation of a generic nonterminal g ( X ) , of arity one , which performs a task equivalent to the original nonterminals s ( X1 , ... , Xn ) , vp ( X1 , ... , Xra ) , ... . The goal g ( s ( X1 , ... , Xn ) ) , for instance , plays the same role for parsing a sentenee as did the goal s ( X1 , ... , Xn ) in the original grammar .</sentence>
				<definiendum id="0">Xn ) , vp ( X1 , ... , Xra</definiendum>
				<definiendum id="1">X1 , ... , Xn )</definiendum>
				<definiendum id="2">Xn</definiendum>
				<definiens id="0">performs a task equivalent to the original nonterminals s ( X1 , ... ,</definiens>
				<definiens id="1">X1 , ... ,</definiens>
			</definition>
			<definition id="5">
				<sentence>d ( ~p ( W ) , s ( s ( , ~p ( np ( ~ ( yo~ ) ) , nil ) , VP ) ) ) -~ \ [ \ ] , DCG '' g ( x ) -~ t ( y ) , d_te ( Y , X ) .</sentence>
				<definiendum id="0">d</definiendum>
				<definiens id="0">s ( s ( , ~p ( np ( ~ ( yo~ ) ) , nil ) , VP ) ) ) -~ \ [ \ ] , DCG '' g ( x ) -~ t ( y )</definiens>
			</definition>
</paper>

		<paper id="2157">
			<definition id="0">
				<sentence>Ad &lt; lod Co that was Clio silence symbol in initial and final position , which adds a , lioChor 70 phoneniic \ [ ) aii : s , \ [ gl'OIH this iniCial sol ; , l , he pairs of se.lni-vowels wcrc relnow ; d. All the ottior &lt; x ) mt ) inations were kept. Even though all of th ( '.Ill do llOt oecllr ill French lexical strueCure , they &lt; : a. still app &lt; ! ar in tile intcr-wor &lt; l boundaries. For oxaml &gt; lc , the sequence /lr/ is not permiCted word internally , but imist be handled since it appears in the interwor &lt; l assimilation in /val r.jc/ `` valenC rion '' cost 'n , othiny. This is partieularly iinportant in French sin &lt; : e inter-word liaison is comnion as in /el z 5/ `` ell &lt; ; s ont '' they have vs /el s5/ `` olios sont '' they are , whero the final consonant/s/eithor undergo0s liaison wiC\ ] l the vowo,1 /5/ rosulting in /z/ , or undergoes linking with the consonani , ts/ 977 resulting in the devoiced sibilant. tion of Carrier Word This section discusses the nature , of the diphone set and the manner in which diphones were collected. Diphones are structured as \ [ bllows : *V , *C , g* , C* , CV , VC , CO , gV where * is a silence , C a consonant , and V a vowel. Semi vowels were treated in the same fashion as consonants. Diphones were recorded following two ( lif\ [ erent strategies : the first one consisted of picking existing words from a dictionary list. The second consisted of deciding on a neutral phonetic context in using logatornes or nonexisting words. Logatomes are phonotactiically well-formed strings , which do not exist as words in the current French language. machine-readable dictionary A word list was extracted from a subset of the Robert French dictionary \ [ ? \ ] and the pronunciation fields were extracted. The dictionary contains almost 89,000 entries , of which 85,796 entries conLain a headword , a phonemic transcription , and a part of speech. The remaining entries are prefixes and suffixes. The first task consisted of converting and mapping the dictionary phonemic symbols to the ones adopted in our system ( shown in table 1 ) . This was not straightforward since there was not always a one-to-one mapping between the two sets. For handling symbol mapping , a program was written that converts any set of characters to any other set of characters I. The program is developed so that characters coded in octal or decimal code not only can he translated in either code , but also can be input in ascii format for being converted 2 Quite often , there was more than one pronunciation in the phonetic field and the. pattern matching program chose the pronunciation corresponding to the one required. Moreover , dictionary pro11 am very grateflfl to Mike Tanenblatt who wrote this program and made a succession of changes until complete flexibility of character conversion was obtained. written on Macintosh , PC , or Unix. Additionally , we used it to convert all the French textual databases into latin1 8 bit encoding format. nouneiation fields are often not phonetically linegrained enough for acceptable speech output ( see \ [ ? \ ] for a discussion on machine-readable dictionaries in text-to-speech systems ) . Finally , due to the lack of explicit inflectional information for nouns and adjectives , only the non-inflected forms of the entries were extracted during dictionary lookup. Sirnilarly for verbs , only the infinil ; iwd forms were used since the dictionary does not list the intleeted forms as headwords. A program was written to search through the dictionary pronunciation field and select the longest word where the phonenm pairs would be in mid-syllable position in order to avoid the extraction of ' phonemes occuring at the beginning or end of words. In this way , l , he influence of lexicM stress was reduced. The orthography/prommciation pair \ [ headword_orth , headwordq~ ) hon\ ] was extracted and headword_orth was placed in a carrier sentence for recording. Out of 1225 original phonemic pairs , 874 words wet ' { '. found with at least one occurence of the pair. Be cause 1225 is the number of all phonemic pairs in French whether they are allowed or not , it is interesting to notice that only 874 pairs occur within real words in the Robert dictionary. For the logato.tes , two phonen , es /a/ attd /t/ were used to encompass the selected diphone , since they appear to be fairly stable from a phonetic-acousLic standpoint. In order to balance the alternation of vowel and consonant , the words were constructed as follows : Logatome position initial vow. initial cons. final vow. final cons. COILS. VOW. VOWel ( ; ons. cons.1 cons.2 vow.1 vow.2 Structure Example *V-ta ota *C-ata bata at-V* at ( ) ta-C* Lab ata-CV-ta atabota at-VC-aba atit } ata ata-C , C-ata at.akrata at-VV-ta atoata `` Fable 2 : Phonotactic structure of logatomes All strings were generated in this way , ewm if they were not phonotaeticMly well-formed for isolated words in the language. Nonetheless , these R ) rms were generated and used since they were necessary for interword phenomena. Approximately 1225 words were constructed following the ab ow~ ' model. 978 l { .esearchers disagree as to whether to use logatomes or real words for synthesis. The argmnent for using logatomcs is that it is t ) etter to collect non-real words so that the diphone ix recorded as neutrally as possible and does uot undergo any real word stress. Those against argue thai ; the ( \ [ iphone is ov ( ~r-articulated in a logatome environment and that it reduces l , he naturahmss of the synthesized sl ) eech. The choice is more corn plex in the sense that it greatly depends on the speaker , the articulation , and the comfort in read ing the two diff ( '.rent sets. Given the controversy , in the present system , we decided to record the l ) houemie t ) airs in bot ; h environments , so thai we ( : ould choose the best ones , I ) ue to the variability of liquids and semi-vowels , synthesis based only on ( liphones will uot give good results. Indeed , such systems have provcu to be insut\ [ icient. Researchers \ [ ? \ ] argue l ; ha| di\ ] ) holle COllcat ( ? ll~ttion alolle is llOt a ( l ( 2 ( ltlate or sl/fticient , particularly for complex transitions. \ [ ? \ ] claims that `` Meal diphones with perfect ( ; oncatcnation would giw~ imperf ( ~ct results '' . Complex polypho , ~es are not equivalent to concatenated allphones. Therefore , louger concatcnativ ( ~ units are necessary. Polyphones are defined by \ [ '/\ ] as being a segmental unit where the initial and linal phoneme are not subject to variability , thus , excluding liquids and semi-vowels. The strategy chosen in the Fre.nch system relies on some phonetic ge.neralities to build a set of tril ) honcs. It was decided a to form a ( : lass of triphoues , based on the following transition : I'VC'~ , where 1 ) is a phoneme , V a vowel , and Cc a conso naut rel ) resenl , ative of the ~trticulatory locations , i.e. one velar , one dental , and one nasal. The set consisted then of 35 phones x 14 vowels x 3 consonants = t47 ( ) triphon ( : s. The same methodology used for building the set of ( liptlones was used for the triphon ( ~'s. These were inchMe ( I in a carrier word for the logatomes and extracted from the dictionary for the real words. Researchers disagree on which criteria are best for the selection of triphones ; should the selectiou rely on phonectic-a &lt; : oustic &lt; widence , or on statistical evidence , related to tl , e fi'equency of occurrence of triphones in the language ? Then , once the ( : riteria is defined , which triphones shouhl be selected ? Can candidates of a class ( say the phoneme /p/ 3 personal communication with Joe Olive representing all the stops , the phoneme./v/ represeutmg all the fficatiw~s ) be picked to rel ) resent a class or sit ( mid all the phonemes belonging to the class he sekwted ? Resenreh is underway in this a , ~a using a phone , , , clustering approach \ [ ' : \ ] , \ [ ' : \ ] that allows the sehx : tion of segnwaltal units fi'om a database of I ) honemes containing several instances of the same phoneme. Tim extraction is made at a spectral point common to the pho , wmes. Finally , he.cause the nnml ) er of selected units atfects results , the choi ( 'e of polyphones must be Ilia ( h ! with care. 'l'aking illto accotlrlt the size limita lion , one has to balanc ( ~ out the choice of the poly phones considering its frequency in ~he. language. This brings in the additional complexity of cort ) us selection ( its language properties , dialects , socio linguisl , ic tyl ) e of language , topic , and size ) . \ [ ? \ ] applies a series of rules on phoneum colnbination to exclude inter-word concatenation that would not occur in French. For example , one can not lind a glide , in I'~rench that ix not in the left or right cont ; ext of a vowel ; therefore , the combination consonant-glide-consonant is excluded. An optimal set of polyphone combinations is computed that re.aches a tmmber of 7725 units. Calculated from texts , statistics are then run on these illlits to ( teterlllille the most freqllellt oc ( ; iH'elH ; es in French , and the numbex of units is lowered to 3000 , It remains to be seen whether this al ) proacll is successfidl iu a workiug system. A carrier sentence `` C'est CAI~I~ItgI~_WOnl ) qlle JC dis '' was selected to fulfill the following require i'qeill.s : • short sentelice~ to record , • ability t ( ) surrourid the ' carrier word to avoid selfl , ential accent and effects , • phonetically neutral environment. l ? ive male natiw : speakers of Continental French were interviewed for selc'cting tile voice of the lq'eneh synthesizer. A sample of text representing highly o ( ' ( ; uring graphemic trigrams wax prepare.d to be used in this task. The corpus wax run through a greedy algorithm 4 that returned the most frequent words within their sentences 4'|'hanks to .Inn Van Santen for developing and running his greedy algorithm. 979 along with a measure corresponding to the coverage of the graphemic triphone. Once tile sample was recorded by tire 5 speakers , the natural voices were run through LPC analysis and re-synthesize.d in order to judge the resistance of tile voice to synthesis. Five subjects were asked to give their judgcrnent on the following criteria : clear articulation : tile voice was carefully listened to evaluate tire articulation of the speaker. Subjective perceptual judgements were lnade. asked about tile areas of Franc ( : where he grew up. The central area of France `` l'Ile de France '' is known for its neutral accent and is regarded as being a well-received accent. Additionally , for French native speakers residing in the USA , particular attention was paid to the influence of English in tire prommcialion of French , especially for English borrowings , such as for example , the company name AT &amp; T to be pronounced/a te re/ ( the French way ) and not ; /el t n t/ as in English. regularity : special attention was given to ensure that the speaker would have a reasonable degree of regularity in uttering French phonemes. ph : asantness of the voic ( . ' : the subjects doing the evaluation were asked to give their opinion on the pleasantness of the voice , in particular the timber , the level of nasality , and the intonation. Of course , this is a highly subjective matter but a critical one for success. The recording was done on four non-consecutive days under the following conditions. Thc sen tences were recorded directly onto the computer through a 1 ) AT ( Digital audio 'rape ) tape recorder , using interactive software allowing easy reading and repetition of the sentences Lo be recorded. Additional time was devoted to the recording of triphones as well as the re-recording of sentences that were improperly uttered. The same carrier sentence and a regular prosodic context was carefully maintained so that there was minireal suprasegmental variation. Once the recording was done , the 48 kHz digitized acoustic signal was downsized to 12 kllz. terial For the recording , all sentences were transcribed from the phonetic alphabet to an orthographic Ibrrnat. This was done to allow tile speaker to utter sent ( ; nees with more naturalness. Once the recording was dorlc'~ th ( `` sentences were setniautomatically re-transcribed into phonc % c form. For some~ utterances , the phon ( 'tic transcription was manually adjusted to the idiosynerasi ( ; s of the speaker. For example , it often happened that confusion arises between open and closed vowels , such in the ~ord '~zoologique '' zoological that can be pronomtced either/zooloaik/or/zaalosik/. In case the output was /zooloaik/ instead of the expe ( % ed /zaalosik/ , the transcription was readjusted. Segmentation is presently in progress ; efforts are being pursued to adapt an automatic segmentor for English to French and other languages. In the meantime , rnannal segmentation is being done as a pilot experiment in order to cheek the accuracy of automatic segmentation. Beyond the scope of this paper are many complex issues raised in segmenting French , such as the segmentation of semivoweds ( /j/ , /w/ , and /q/ ) and liquids ( /l/ and /r/ ) , each of these phonemes being quite unstable f¥om a phonetic-~eoustic standpoint. These issues will be addressed in hmm ; work. transcriber A grapheme-to-phoneme transcriber \ [ ? \ ] was acquired to convert French orthography to a phone.mie representation. The software performs some syntactic and partial semantic analysis of the sentence in order to disambiguate the input string. Once performed , spellings at0. converted in a series of steps into a phonernic representation. We have t ) nrsued work in the text analysis of French in order to obtain linguistic data for intonation and prosody ; additionally , the output of the work will be used in the translation project. This aspect of the work has entailed several points : • acquisition of a large French dictionary : lt.obcrt Encyclopedic dictionary ( containing 980 over 851¢ ent , rie.s , 80k articles , 160k eitatious , analogical terms ( synonyms , hon|onlylns , el , ( ; ) , and conjugatiou tables for illOSt l ? rerl ( ; \ [ l verl ) s ) , • collectiol~ of French corpora : i , 'rcneh news from LI ' ; M ( ) NI ) I ' ; \ [ ? \ ] I/'retlch news daily &lt; : ompih'.d by the French embassy in Washington DC ( 24657K byt ; es arc now en &lt; : oded , and a monthly update is being done. ) . Tim data are in ascii and aeeeltts were re= stored using oue of the features of the gral &gt; hetue : l , o : pholmlne software .</sentence>
				<definiendum id="0">`` valenC rion</definiendum>
				<definiendum id="1">e inter-word liaison</definiendum>
				<definiendum id="2">C-ata at.akrata at-VV-ta atoata</definiendum>
				<definiens id="0">Clio silence symbol in initial and final position , which adds a , lioChor 70 phoneniic \ [ ) aii : s , \ [ gl'OIH this iniCial sol ; , l , he pairs of se.lni-vowels wcrc relnow ; d. All the ottior &lt; x ) mt ) inations were kept. Even though all of th ( '.Ill do llOt oecllr ill French lexical strueCure , they &lt; : a. still app &lt; ! ar in tile intcr-wor &lt; l boundaries. For oxaml &gt; lc , the sequence /lr/ is not permiCted word internally , but imist be handled since it appears in the interwor &lt; l assimilation in /val r.jc/</definiens>
				<definiens id="1">ell &lt; ; s ont '' they have vs /el s5/ `` olios sont '' they are , whero the final consonant/s/eithor undergo0s liaison wiC\ ] l the vowo,1 /5/ rosulting in /z/ , or undergoes linking with the consonani , ts/ 977 resulting in the devoiced sibilant. tion of Carrier Word This section discusses the nature , of the diphone set and the manner in which diphones were collected. Diphones are structured as \ [ bllows : *V , *C , g* , C* , CV , VC , CO , gV where * is a silence , C a consonant , and V a vowel. Semi vowels were treated in the same fashion as consonants. Diphones were recorded following two ( lif\ [ erent strategies : the first one consisted of picking existing words from a dictionary list. The second consisted of deciding on a neutral phonetic context in using logatornes or nonexisting words. Logatomes are phonotactiically well-formed strings , which do not exist as words in the current French language. machine-readable dictionary A word list was extracted from a subset of the Robert French dictionary \ [ ? \ ] and the pronunciation fields were extracted. The dictionary contains almost 89,000 entries , of which 85,796 entries conLain a headword , a phonemic transcription , and a part of speech. The remaining entries are prefixes and suffixes. The first task consisted of converting and mapping the dictionary phonemic symbols to the ones adopted in our system ( shown in table 1 ) . This was not straightforward since there was not always a one-to-one mapping between the two sets. For handling symbol mapping , a program was written that converts any set of characters to any other set of characters I. The program is developed so that characters coded in octal or decimal code not only can he translated in either code , but also can be input in ascii format for being converted 2 Quite often , there was more than one pronunciation in the phonetic field and the. pattern matching program chose the pronunciation corresponding to the one required. Moreover , dictionary pro11 am very grateflfl to Mike Tanenblatt who wrote this program and made a succession of changes until complete flexibility of character conversion was obtained. written on Macintosh , PC , or Unix. Additionally , we used it to convert all the French textual databases into latin1 8 bit encoding format. nouneiation fields are often not phonetically linegrained enough for acceptable speech output ( see \ [ ? \ ] for a discussion on machine-readable dictionaries in text-to-speech systems ) . Finally , due to the lack of explicit inflectional information for nouns and adjectives , only the non-inflected forms of the entries were extracted during dictionary lookup. Sirnilarly for verbs , only the infinil ; iwd forms were used since the dictionary does not list the intleeted forms as headwords. A program was written to search through the dictionary pronunciation field and select the longest word where the phonenm pairs would be in mid-syllable position in order to avoid the extraction of ' phonemes occuring at the beginning or end of words. In this way , l , he influence of lexicM stress was reduced. The orthography/prommciation pair \ [ headword_orth , headwordq~ ) hon\ ] was extracted and headword_orth was placed in a carrier sentence for recording. Out of 1225 original phonemic pairs , 874 words wet ' { '. found with at least one occurence of the pair. Be cause 1225 is the number of all phonemic pairs in French whether they are allowed or not , it is interesting to notice that only 874 pairs occur within real words in the Robert dictionary. For the logato.tes , two phonen , es /a/ attd /t/ were used to encompass the selected diphone , since they appear to be fairly stable from a phonetic-acousLic standpoint. In order to balance the alternation of vowel and consonant , the words were constructed as follows : Logatome position initial vow. initial cons. final vow. final cons. COILS. VOW. VOWel ( ; ons. cons.1 cons.2 vow.1 vow.2 Structure Example *V-ta ota *C-ata bata at-V* at ( ) ta-C* Lab ata-CV-ta atabota at-VC-aba atit } ata ata-C ,</definiens>
				<definiens id="2">Phonotactic structure of logatomes All strings were generated in this way , ewm if they were not phonotaeticMly well-formed for isolated words in the language. Nonetheless , these R ) rms were generated and used since they were necessary for interword phenomena. Approximately 1225 words were constructed following the ab ow~ ' model. 978 l { .esearchers disagree as to whether to use logatomes or real words for synthesis. The argmnent for using logatomcs is that it is t ) etter to collect non-real words so that the diphone ix recorded as neutrally as possible and does uot undergo any real word stress. Those against argue thai ; the ( \ [ iphone is ov ( ~r-articulated in a logatome environment and that it reduces l , he naturahmss of the synthesized sl ) eech. The choice is more corn plex in the sense that it greatly depends on the speaker , the articulation , and the comfort in read ing the two diff ( '.rent sets. Given the controversy , in the present system , we decided to record the l ) houemie t ) airs in bot ; h environments</definiens>
				<definiens id="3">ould choose the best ones , I ) ue to the variability of liquids and semi-vowels , synthesis based only on ( liphones will uot give good results. Indeed , such systems have provcu to be insut\ [ icient. Researchers \ [ ? \ ] argue l</definiens>
				<definiens id="4">~ct results '' . Complex polypho , ~es are not equivalent to concatenated allphones. Therefore , louger concatcnativ ( ~ units are necessary. Polyphones are defined by \ [ '/\ ] as being a segmental unit where the initial and linal phoneme are not subject to variability</definiens>
				<definiens id="5">strategy chosen in the Fre.nch system relies on some phonetic ge.neralities to build a set of tril ) honcs. It was decided a to form a ( : lass of triphoues , based on the following transition : I'VC'~ , where 1 ) is a phoneme , V a vowel , and Cc a conso naut rel ) resenl , ative of the ~trticulatory locations , i.e. one velar , one dental , and one nasal. The set consisted then of 35 phones x 14 vowels x 3 consonants = t47 ( ) triphon ( : s. The same methodology used for building the set of ( liptlones was used for the triphon ( ~'s. These were inchMe ( I in a carrier word for the logatomes and extracted from the dictionary for the real words. Researchers disagree on which criteria are best for the selection of triphones ; should the selectiou rely on phonectic-a &lt; : oustic &lt; widence , or on statistical evidence , related to tl , e fi'equency of occurrence of triphones in the language ? Then , once the ( : riteria is defined , which triphones shouhl be selected ? Can candidates of a class ( say the phoneme /p/ 3 personal communication with Joe Olive representing all the stops , the phoneme./v/ represeutmg all the fficatiw~s ) be picked to rel ) resent a class or sit ( mid all the phonemes belonging to the class he sekwted ? Resenreh is underway in this a , ~a using a phone , , , clustering approach \ [ ' : \ ] , \ [ ' : \ ] that allows the sehx : tion of segnwaltal units fi'om a database of I ) honemes containing several instances of the same phoneme. Tim extraction is made at a spectral point common to the pho</definiens>
				<definiens id="6">'e of polyphones must be Ilia ( h ! with care. 'l'aking illto accotlrlt the size limita lion , one has to balanc ( ~ out the choice of the poly phones considering its frequency in ~he. language. This brings in the additional complexity of cort ) us selection ( its language properties , dialects , socio linguisl , ic tyl ) e of language , topic , and size ) . \ [ ? \ ] applies a series of rules on phoneum colnbination to exclude inter-word concatenation that would not occur in French. For example , one can not lind a glide , in I'~rench that ix not in the left or right cont ; ext of a vowel ; therefore , the combination consonant-glide-consonant is excluded. An optimal set of polyphone combinations is computed that re.aches a tmmber of 7725 units. Calculated from texts , statistics are then run on these illlits to ( teterlllille the most freqllellt oc ( ; iH'elH ; es in French , and the numbex of units is lowered to 3000 , It remains to be seen whether this al ) proacll is successfidl iu a workiug system. A carrier sentence `` C'est CAI~I~ItgI~_WOnl ) qlle JC dis '' was selected to fulfill the following require i'qeill.s : • short sentelice~ to record , • ability t ( ) surrourid the ' carrier word to avoid selfl , ential accent and effects , • phonetically neutral environment. l ? ive male natiw : speakers of Continental French were interviewed for selc'cting tile voice of the lq'eneh synthesizer. A sample of text representing highly o ( ' ( ; uring graphemic trigrams wax prepare.d to be used in this task. The corpus wax run through a greedy algorithm 4 that returned the most frequent words within their sentences 4'|'hanks to .Inn Van Santen for developing and running his greedy algorithm. 979 along with a measure corresponding to the coverage of the graphemic triphone. Once tile sample was recorded by tire 5 speakers , the natural voices were run through LPC analysis and re-synthesize.d in order to judge the resistance of tile voice to synthesis. Five subjects were asked to give their judgcrnent on the following criteria : clear articulation : tile voice was carefully listened to evaluate tire articulation of the speaker. Subjective perceptual judgements were lnade. asked about tile areas of Franc ( : where he grew up. The central area of France `` l'Ile de France '' is known for its neutral accent and is regarded as being a well-received accent. Additionally , for French native speakers residing in the USA , particular attention was paid to the influence of English in tire prommcialion of French , especially for English borrowings , such as for example , the company name AT &amp; T to be pronounced/a te re/ ( the French way ) and not ; /el t n t/ as in English. regularity : special attention was given to ensure that the speaker would have a reasonable degree of regularity in uttering French phonemes. ph : asantness of the voic ( . ' : the subjects doing the evaluation were asked to give their opinion on the pleasantness of the voice</definiens>
				<definiens id="7">recording was done on four non-consecutive days under the following conditions. Thc sen tences were recorded directly onto the computer through a 1 ) AT ( Digital audio 'rape ) tape recorder , using interactive software allowing easy reading and repetition of the sentences Lo be recorded. Additional time was devoted to the recording of triphones as well as the re-recording of sentences that were improperly uttered. The same carrier sentence and a regular prosodic context was carefully maintained so that there was minireal suprasegmental variation. Once the recording was done , the 48 kHz digitized acoustic signal was downsized to 12 kllz. terial For the recording , all sentences were transcribed from the phonetic alphabet to an orthographic Ibrrnat. This was done to allow tile speaker to utter sent ( ; nees with more naturalness. Once the recording was dorlc'~ th ( `` sentences were setniautomatically re-transcribed into phonc % c form. For some~ utterances , the phon ( 'tic transcription was manually adjusted to the idiosynerasi ( ; s of the speaker. For example , it often happened that confusion arises between open and closed vowels , such in the ~ord '~zoologique '' zoological that can be pronomtced either/zooloaik/or/zaalosik/. In case the output was /zooloaik/ instead of the expe ( % ed /zaalosik/ , the transcription was readjusted. Segmentation is presently in progress ; efforts are being pursued to adapt an automatic segmentor for English to French and other languages. In the meantime , rnannal segmentation is being done as a pilot experiment in order to cheek the accuracy of automatic segmentation. Beyond the scope of this paper are many complex issues raised in segmenting French , such as the segmentation of semivoweds ( /j/ , /w/ , and /q/ ) and liquids ( /l/ and /r/ ) , each of these phonemes being quite unstable f¥om a phonetic-~eoustic standpoint. These issues will be addressed in hmm ; work. transcriber A grapheme-to-phoneme transcriber \ [ ? \ ] was acquired to convert French orthography to a phone.mie representation. The software performs some syntactic and partial semantic analysis of the sentence in order to disambiguate the input string. Once performed , spellings at0. converted in a series of steps into a phonernic representation. We have t ) nrsued work in the text analysis of French in order to obtain linguistic data for intonation and prosody ; additionally , the output of the work will be used in the translation project. This aspect of the work has entailed several points : • acquisition of a large French dictionary : lt.obcrt Encyclopedic dictionary ( containing 980 over 851¢ ent , rie.s , 80k articles , 160k eitatious , analogical terms ( synonyms , hon|onlylns , el , ( ; ) , and conjugatiou tables for illOSt l ? rerl ( ; \ [ l verl ) s ) , • collectiol~ of French corpora : i , 'rcneh news from LI ' ; M ( ) NI ) I '</definiens>
				<definiens id="8">ompih'.d by the French embassy in Washington DC ( 24657K byt ; es arc now en &lt; : oded , and a monthly update is being done. ) . Tim data are in ascii and aeeeltts were re= stored using oue of the features of the gral &gt; hetue : l , o : pholmlne software</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus , a latinl compatible window wouhl dis play lg'eneh corpora with accents ; in the following examph : , the l &gt; rogram returns all in= tall ( : es of the word `` ( ; ore '' ( ( l/iotatioll , ratit~g ) in the database `` l , e Mend { `` .</sentence>
				<definiendum id="0">in= tall</definiendum>
				<definiens id="0">dis play lg'eneh corpora with accents ; in the following examph : , the l &gt; rogram returns all</definiens>
			</definition>
</paper>

		<paper id="1056">
</paper>

		<paper id="1019">
</paper>

		<paper id="2205">
			<definition id="0">
				<sentence>Adjunction , as showed in Fig 2 , inserts an auxiliary tree on the correspondent node in an elementary or derived tree .</sentence>
				<definiendum id="0">Adjunction</definiendum>
				<definiens id="0">an auxiliary tree on the correspondent node in an elementary or derived tree</definiens>
			</definition>
			<definition id="1">
				<sentence>`` 1256 S Fig 1 Substitution / \ Fig 2 Adjunction Z~ The adjunction operation can be recursive , then an auxiliary tree can receive adjunction in itself .</sentence>
				<definiendum id="0">adjunction operation</definiendum>
				<definiens id="0">an auxiliary tree can receive adjunction in itself</definiens>
			</definition>
			<definition id="2">
				<sentence>Adjunction makes TAGs a little more powerful then Context-Free Grammars ( CFG ) , placing it in a class of grammars called Midly Context-Sensitive Grammars \ [ JOSHI 85\ ] .</sentence>
				<definiendum id="0">Adjunction</definiendum>
				<definiens id="0">makes TAGs a little more powerful then Context-Free Grammars ( CFG ) , placing it in a class of grammars called Midly Context-Sensitive Grammars</definiens>
			</definition>
</paper>

		<paper id="2116">
			<definition id="0">
				<sentence>Example categories are : V ( declarative sentence ) , PVQ ( polm question ) , IMV ( imperative sentence ) , PASV ( passive sentence ) , and IV ( to-infinitive clause ) .</sentence>
				<definiendum id="0">IMV</definiendum>
				<definiens id="0">declarative sentence</definiens>
				<definiens id="1">imperative sentence ) , PASV ( passive sentence ) , and IV ( to-infinitive clause )</definiens>
			</definition>
			<definition id="1">
				<sentence>The process consists of three steps : corpus construction , data conversion , and maclfine learning .</sentence>
				<definiendum id="0">process</definiendum>
				<definiens id="0">consists of three steps : corpus construction , data conversion , and maclfine learning</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>AJIixation , revolves the addition of the inflection to the root ( or st , , , , , ) , either I , efore ( '~ , , , 'efixatio , O , after ( su/fizatlon ) , within ( infixation ) , or both before and after ( circun~Ji : r.ation ) the root .</sentence>
				<definiendum id="0">AJIixation</definiendum>
				<definiens id="0">revolves the addition of the inflection to the root ( or st , , , , , ) , either I</definiens>
			</definition>
			<definition id="1">
				<sentence>tIere a word ( er stem ) consists of a root and a pattern of segments which are intercalated 1 ) ctween the root segments in a way which is specified within the pattern .</sentence>
				<definiendum id="0">er stem )</definiendum>
				<definiens id="0">consists of a root and a pattern of segments which are intercalated 1 ) ctween the root segments in a way which is specified within the pattern</definiens>
			</definition>
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>l~ec : allse of the enJarl , e ( I geIterat , lve capacity , it is not sutprising that the coutllh : xity of analysing I : tl &lt; gIHtges gener~tled by ( Jouph : d-Context-l '' ree ( h'anm , ars is larger than it is in the context free case. \ [ t even increa.ses with growi , m rattk ( c : L \ [ 11P9,1\ ] ) . Therefore , we aim to characterize suhclasses of the set of all lall , ~ll~t.ff , es geller~tl.ed hy ( , 'Oul ) led ( .~oiHexl , -.Vree ( ~'4t'atllllltaFs which are powerful ellOltgh to model the important phenomena of natural liu , guages , I ) ut which are of at lower c ( mq ) lexity. The deternliuistic ( : onlext-free parsing with Llt ( k ) ~ , ~ '' ... ... ... ... leads t ... . li ... ... r tinl ... ... ... lysis ( of. \ [ Knu65\ ] ) , the best lmssible. Therefbre , its generalization is very attractive. A fh'st altempt in this direction was done in \ [ .'qVg ( ) \ ] . I~u ( . there , only TA ( Is are ilwestiga|.ed. \ ] \ ] ere , we invesilgatc~ the whole hi ( Hallchy Of Couph : d Context-l '' ree ( lramma~s. Allhou , ; h their enlarged generative ( Stl ) a { : ity seems to I ) e c ( mlradictory Io a. Ihm ; u ' time cOnll ) \ ] exlty of q\ ] le pals { rig algmithm , we can present an Ll~ ( k ) -notion t ' { , ( : oup\ [ ( : d-Co , ttext-l '' vee Crammars dcs { : ril , ing a class of laugual'j : s , which can actnal\ ] y bc anMysed in linear time. This increase ill power a.s to the linear time ~tlHtlysis is ilaid hy an expensive ln'eprocessil~g. It in taking into account ( he comple× rclntlons \ ] ) etwe { 'n parewthese.s that involves ILL ( : ill ( : rease ill conll ) lexity , l\ [ owever , these costs are to he paid only once for each grammar. The suhchuss described hy our I , l~ ( k ) -notion for a lixed k i.rows with the rank. `` \ [ 'he al~orithm of \ [ .q\:90\ ] fnr l , l¢ ( k ) /l'AGs does not fullill theimp ( , tant Valid Prefix I ) roperty. This means that for any prefix o1 '' the iUl ) Ul : aheady accepted , there exists a suffix such that the whoh ' , word is in the language analysed. It allows to detect illegal inputs as soon its possible , which is necessary for efllcient parsing. Our algorithm fulfills this property. Addition ; lily , the algorithm its well as the notion defined here , 'eprescnt genera.lizations o\ ] their context-free C &lt; ) lllt ( , et'pltrts which are ll~ttllra\ ] ill tit &lt; : sense that they strictly contain tit { : context-free sitnation as the special case of ( ; ouple ( \ [ - ( Jonl , exl.-Free ( \ ] ranLulars o\ ] rank 1. 401 An example of an important LR ( k ) -Coupled-ContextFree Grammar is the one generating the language { w $ w I w ~ { a , b } * } which reflects the syntactical construction of cross-serial dependencies. The paper starts by defining the Coupled-Context-Free Grammars. Thcn , wc shortly recall the context-free Lib parsing procedure. Subsequently , the deterministic finite automaton used there to guide the analysis is modified such that it can handle Couplcd-Cm~text-Frec Grammars. Based on it , the parsing algorithm for LR ( O ) CoupledContext-Free Grammars is derived. Tiffs results in the generalized definition of the LR ( O ) -notlon. As for CFCs , the LR ( k ) -Coupled-Context-Free Grammars result from the LR ( 0 ) -ones by resolving decision conflicts using a lookahead of at most k symbols. Coupled-Context-Free Grammars are defined over extended semi-Dyck sets which are a generalization of scmlDyck sets. Elements of these sets can be regarded as sequences of parentheses that are correctly nested. SentiDyek sets play an important role in the theory of formal languages. To extend the family of context-free languages by using them wc consider parentheses of arbitrary finite order define ( \ [ as follows : Definition I ( Parentheses Set ) A finite sctK : = ( ( ki , ~ ... .. ki , , , , , ) li , mi ~ N } is a Parentheses Set iff it satisfies ki , j # kt , , , , Jot '' i ¢ I or j # m. The elements eric are erdled Parentheses. All parentheses of a fixed length r are summarized as ~ : \ [ ~\ ] : = ( ( ~ , . , ... .. ~ , , , , , , ) ~ ~ : I , n , = ~ } , ,hct.e ~C\ [ O\ ] : = { ~ } . ( ~ de.otes O , e e. , Vt , a , ,otd. ) `` the s , .2 of all ( first ) components of parenthesis in K is denoted by eon , p ( ~C ) : = { ~ , I ( ~ , ... .. ~ , ... .. ~ , ) e ~c } , esp. co , marc ) : = { ~ , I ( ~ , ... .. ~ , ) e ~c } . Straightforward frmn this , we get Definition 2 ( Extended Semi-Dyck Set ) Let ~ be a parentheses set and 7 ' an arbiteary set where 7 '' VI K = T m comp ( K ) = O. E D ( K , T ) , the extended semiDyck set over E and T , is indnetively defined by ( El ) T* C ED ( K , T ) . ( E2 ) ~C\ [ ~1 c ~D ( ~C , T ) . ( E3 ) u~ ... .. u , E ED ( IC , T ) , ( k , ... .. k , + , ) e K\ [ , '' + 11 ==~ k~u~ `` '' kru~k~+~ G ED ( K , T ) . ( E4 ) u , v e ED ( K , T ) ~ u. v ~ E1 ) ( K , T ) . ( E5 ) ED ( K , 7 '' ) is the smallest set f , ,lfilling em , ditions ( E1 ) - ( E4 ) . Now , we define how to generate new elements in ED ( K , T ) starting from given ones. Definition 3 ( Parenthesis Rewriting System ) A Parenthesis Rewriting System over ED ( /C , T ) is a fiaite , nonempty set P of productions of the form { ( k , ... .. k~ ) ~ ( .~ ... .. o , . ) I ( ~ , ... .. ~ , ) C , ~ , . , ... .. , ,. e ED ( ~ , `` r ) } . The left and the right side of p : = ( X~ , ... , X~ ) -* ( o:1 ... .. o6 , ) G P is denoted by • S ( p ) : = ( x , ... .. x~ ) , thc sonrce oh , , a , ,d • V ( p ) : = ( or , ... .. e~ , . ) , the drain ofp. Now , we can deline our grammars. The term `` coupled '' expresses that a certain number of ( : ontext-free rewritings is executed in parallel and controlled by K\ [ . '' Definition 4 ( Couph ; d-Context-l , Yeo Grammar ) A Coupled-Context-Free Grammar over ED ( K , T ) is au ordered ~-luple ( IC , T , l 7 , S ) whcr'e l ' is a Parentheses Rewritin 9 Syste , n over ED ( K~,7 ' ) and S 6 KIll. Therefore , IC can be regaeded as a set of couph : d nonterminctls. The set of ( all these granmmrs is denoted by CG ' I , 'G. ~ ( ~ At last , we give the definition of derivation in CCI , . Let ( ; ' = ( K , T , P , S ) C , ( .1 G and V : = cornp ( l£ ) UT. We define the relation : :~zc ; as a subset of V* × V ' consisting of all derivation steps of rank r for G ' with `` r &gt; l. p = &gt; a '0 holds for 99 , ~/ , G V* if and only if there ex\ [ st ( k , ... . , k , ) -~ ( -* ... .. - , ) e P , , * , , , * , , , , e V ' , and u2 , ... , Ur E ED ( K , T ) such that 9~ = ulkau .</sentence>
				<definiendum id="0">geller~tl.ed hy ( , 'Oul ) led</definiendum>
				<definiendum id="1">-.Vree</definiendum>
				<definiendum id="2">h their enlarged generative</definiendum>
				<definiendum id="3">-Coupled-ContextFree Grammar</definiendum>
				<definiendum id="4">LR ( k</definiendum>
				<definiendum id="5">p</definiendum>
				<definiendum id="6">El ) T* C ED</definiendum>
				<definiendum id="7">T )</definiendum>
				<definiendum id="8">) I</definiendum>
				<definiendum id="9">u2 , ... , Ur E ED</definiendum>
				<definiens id="0">onlext-free parsing with Llt ( k ) ~ , ~ '' ... ... ... ... leads t ... . li ... ... r tinl ... ... ... lysis ( of. \ [ Knu65\ ] ) , the best lmssible. Therefbre</definiens>
				<definiens id="1">hy an expensive ln'eprocessil~g. It in taking into account ( he comple× rclntlons \ ] ) etwe { 'n parewthese.s that involves ILL ( : ill ( : rease ill conll</definiens>
				<definiens id="2">k ) /l'AGs does not fullill theimp ( , tant Valid Prefix I ) roperty. This means that for any prefix o1 '' the iUl ) Ul : aheady accepted , there exists a suffix such that the whoh ' , word is in the language analysed. It allows to detect illegal inputs as soon its possible</definiens>
				<definiens id="3">'eprescnt genera.lizations o\ ] their context-free C &lt; ) lllt ( , et'pltrts which are ll~ttllra\ ] ill tit &lt; : sense that they strictly contain tit { : context-free sitnation as the special case of</definiens>
				<definiens id="4">extended semi-Dyck sets which are a generalization of scmlDyck sets. Elements of these sets can be regarded as sequences of parentheses that are correctly nested. SentiDyek sets play an important role in the theory of formal languages. To extend the family of context-free languages by using them wc consider parentheses of arbitrary finite order define ( \ [ as follows : Definition I ( Parentheses Set ) A finite sctK : = ( ( ki , ~ ... .. ki , , , , , ) li</definiens>
				<definiens id="5">an arbiteary set where 7 '' VI K = T m comp ( K ) = O. E D ( K , T ) , the extended semiDyck set over E and T</definiens>
				<definiens id="6">a fiaite , nonempty set P of productions of the form { ( k , ... .. k~ ) ~ ( .~ ... .. o , .</definiens>
				<definiens id="7">d-Context-l , Yeo Grammar ) A Coupled-Context-Free Grammar over ED ( K , T ) is au ordered ~-luple ( IC , T , l 7</definiens>
				<definiens id="8">K , T , P , S ) C , ( .1 G and V : = cornp ( l£ ) UT. We define the relation : :~zc ; as a subset of V* × V ' consisting of all derivation steps of rank r for G ' with `` r &gt; l. p = &gt; a '0 holds for 99 , ~/ , G V* if and only if there ex\ [ st ( k , ... . , k , ) -~ ( -* ... .. - , ) e P , , * , , , * , , , , e V ' , and</definiens>
			</definition>
			<definition id="1">
				<sentence>The states of the d.fa for a given LR ( 0 ) -CI '' G consist of subsets of the set of all context-free items for ( ; ( N , T , P , S ' ) , i.e. of the set { \ [ X -- ~ ~ , .</sentence>
				<definiendum id="0">The states of the d.fa for</definiendum>
				<definiens id="0">a given LR ( 0 ) -CI '' G consist of subsets of the set of all context-free items for ( ; ( N , T , P , S ' ) , i.e. of the set { \ [ X -- ~ ~</definiens>
			</definition>
			<definition id="2">
				<sentence>e ~o , , , p ( ~c ) \ eo , s , m ( ~ : ) , x - , , ~ , z - , ~5 , /e or ( p ) , ) Definition 9 ( Lie ( o ) in COl , ' ( ; ) ( I G C ( 71 , ' ( I is .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">s , m ( ~ : ) , x - , , ~ , z - , ~5 , /e or ( p )</definiens>
			</definition>
</paper>

		<paper id="2114">
			<definition id="0">
				<sentence>First , , SENA searches for examples whose heads match the candidate .</sentence>
				<definiendum id="0">SENA</definiendum>
				<definiens id="0">searches for examples whose heads match the candidate</definiens>
			</definition>
			<definition id="1">
				<sentence>CB1 = { I , user , cradle , rock } ( for D1 ) CB2 = { storage , transient data } ( for D2 ) CB3 = { condition , format , path , 1916 , technique , control area } ( for DO ) CB4 = { systema8 , facility } ( for D4 ) CB5 = { reeord } ( for DS ) CB6 = { } ( for D6 ) The total set of candidate words ( CB ) of the `` repository '' is an union of CB1 through CB6 .</sentence>
				<definiendum id="0">CB1</definiendum>
				<definiendum id="1">CB</definiendum>
				<definiens id="0">= { I , user , cradle , rock } ( for D1 ) CB2 = { storage , transient data } ( for D2 ) CB3 = { condition , format , path , 1916 , technique , control area } ( for DO ) CB4 = { systema8 , facility } ( for D4</definiens>
			</definition>
</paper>

		<paper id="1078">
			<definition id="0">
				<sentence>A clepth-first realization of this abstract top-down algorithm would work line as long ms tl , e semantic rep4Z5 all leaves of the syntax tree are labeled with terminals ( success ) Xo Xo Figure 1 : Top-Down Generation ( G grammar description ; xl syntactic category ; Xi semantic representation ) resentations of the leaves are always strictly smaller in size as the semantic form of the root node .</sentence>
				<definiendum id="0">clepth-first realization</definiendum>
				<definiens id="0">Top-Down Generation ( G grammar description ; xl syntactic category ; Xi semantic representation ) resentations of the leaves are always strictly smaller in size as the semantic form of the root node</definiens>
			</definition>
			<definition id="1">
				<sentence>G and Xi substructure of X xo Xo xo Xo \ G Figure 2 : Bottom-Up Generation ( G grammar description ; s start symbol ; X input semantics ; xl syntactic category ; X , '' semantic representatiou ) all leaves are labeled with terminals and the tree does not contain any dotted lines ( + ; + ) ~ , ~ ' ( + +1 @ tol , .</sentence>
				<definiendum id="0">semantic representatiou</definiendum>
				<definiens id="0">Bottom-Up Generation ( G grammar description ; s start symbol</definiens>
			</definition>
</paper>

		<paper id="2158">
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>Dilemma forms part of a tool kit for t , 'anslation where focus is on text structure and over-all consistency in large text volumes rather than on framing sentences , on interaction between many actors in a large project rather than on retriewd of machine-stored data and on decision making rather than on application of give , ) rules .</sentence>
				<definiendum id="0">Dilemma</definiendum>
				<definiens id="0">forms part of a tool kit for t</definiens>
			</definition>
</paper>

		<paper id="2145">
			<definition id="0">
				<sentence>Evidently , one of the most basic hurdles to translating any of the other formMisms into UD is the reconstruction of the type system .</sentence>
				<definiendum id="0">UD</definiendum>
				<definiens id="0">the reconstruction of the type system</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>This is tmrticularly important because tile iHllllense Illllllb ( w of translation rules ( currently over 10,000 ) requires employing a team of experts over an extended l ) eriod of tim ( !</sentence>
				<definiendum id="0">translation rules</definiendum>
				<definiens id="0">currently over 10,000 ) requires employing a team of experts over an extended l</definiens>
			</definition>
			<definition id="1">
				<sentence>n ( Is mt ) lmy ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">Is mt ) lmy )</definiens>
			</definition>
</paper>

		<paper id="1080">
			<definition id="0">
				<sentence>While the issue of lexiealize ( I control has early becn invcstigatcd in the l ) aradigm of conceptual parsers ( e.g. , Riesbcck &amp; Schank 1978 ) , and word expell parsing in particular ( Small &amp; Ricger , 1982 ) , we here elaborate on improving ils lcxical commtmication facilities by formalizing the parscr 's message passing protocol according to actor computation principles .</sentence>
				<definiendum id="0">conceptual parsers</definiendum>
				<definiens id="0">lcxical commtmication facilities by formalizing the parscr 's message passing protocol according to actor computation principles</definiens>
			</definition>
			<definition id="1">
				<sentence>A dependency is thus not trealed a~ : a relation between words ( as in Word Grammar ( lludson , 199 ( \ ] , p.1171 , but between a word and a dependent phrase ( as in l ) epcndency Unification ( \ ] rammar ( llellwig , 1988 ) ) .</sentence>
				<definiendum id="0">dependent phrase</definiendum>
				<definiens id="0">a relation between words</definiens>
			</definition>
			<definition id="2">
				<sentence>Given a program , event types , written as I* &lt; key\ ] , can be syntactically deterlnined by inspecting the method delinitions wilhin the program. Let an actor type aName I ) e defined by : .£kelg£J.~ aName ( acquaintance 1 ... acquaintancek ) meth key 1 ( param 1 ... paramrn ) ( action1 ) ... meth key n ( param 1 ... paraml ) ( actionn ) with action i delincd by the following grammar fragment : action : := action ; action \ ] if condition ( action ) \ [ ~ ( action ) \ ] I .keJ3£L actor messageKey ( param* ) \ ] becom_____~e ( acquaintance* ) Wc may now map message keys to sets of message keys , defining the function sctipt~ , r~zm e as follows : 490 scriptaNa , ne : Keys -4 2 7 ( ¢~. scriptaA ( ome ( keYi ) = send ( action ! ) with sent { ( action ) : = { msgKey } if action = send actor msgKoy ( param , ... ) setu { ( al ) u sertd { a2 ) if action = \ [ \ [ condition a I else a 2 sent { ( al ) if action = if condition a 1 sentf ( ( at ) u senaC ( a2 ) if action = al ; a2 else For a program P , script is the union of all given script , m , , e with name e { aroma I P contains a delinition for aNamo } and yields a set containing the keys of those messages that can be provoked by a , nessage with the key mKey. Now , a relation between event types is delined hy causes ! : ( \ [ * ¢= mKey\ ] , \ [ * ¢=nKey\ ] ) &lt; cattses t : &lt; = &gt; nKey ~ scrit ) t ( mKey ) .</sentence>
				<definiendum id="0">¢~. scriptaA ( ome</definiendum>
				<definiendum id="1">mKey. Now</definiendum>
				<definiens id="0">.£kelg£J.~ aName ( acquaintance 1 ... acquaintancek ) meth key 1 ( param 1 ... paramrn ) ( action1 ) ... meth key n ( param 1 ... paraml ) ( actionn ) with action i delincd by the following grammar fragment : action : := action</definiens>
				<definiens id="1">the union of all given script , m , , e with name e { aroma I P contains a delinition for aNamo } and yields a set containing the keys of those messages that can be provoked by a , nessage with the key</definiens>
			</definition>
</paper>

		<paper id="2163">
			<definition id="0">
				<sentence>Sulnmary : Optimality Theory is a constraint-based Iheoly of phonology which allows constraints to conllict aad to be violated .</sentence>
				<definiendum id="0">Optimality Theory</definiendum>
				<definiens id="0">a constraint-based Iheoly of phonology which allows constraints to conllict aad to be violated</definiens>
			</definition>
			<definition id="1">
				<sentence>A derivation in OT consists of an original candidate set produced by a \ [ unction called GI , ; N , and the subsequent application of constraints to reduce tile candidate set , eliminating non-optimal candidates and I ) reserving those with Ihe greatest harmony .</sentence>
				<definiendum id="0">derivation in OT</definiendum>
				<definiens id="0">consists of an original candidate set produced by a \ [ unction called GI , ; N , and the subsequent application of constraints to reduce tile candidate set , eliminating non-optimal candidates and I ) reserving those with Ihe greatest harmony</definiens>
			</definition>
			<definition id="2">
				<sentence>As an example , ( 12 ) is the product of the transducers corresponding to the constraints ONS ( 9 ) and FmL ( 8 ) in that order .</sentence>
				<definiendum id="0">constraints ONS</definiendum>
				<definiens id="0">the product of the transducers corresponding to the</definiens>
			</definition>
			<definition id="3">
				<sentence>s is the set of states in the transducer and arcs the set o\ [ arcs .</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">the set of states in the transducer and arcs the set o\ [ arcs</definiens>
			</definition>
			<definition id="4">
				<sentence>After the al } plication o\ [ LAImI , NOI ) ES , for all states N on which harmony ( N ) is deiined , harmony ( N ) is the harmony of the optimal path to N. Proof .</sentence>
				<definiendum id="0">N )</definiendum>
				<definiens id="0">the harmony of the optimal path to N. Proof</definiens>
			</definition>
			<definition id="5">
				<sentence>But , by theorem ( 16 ) , harnlony ( N ) is the harmony of the optimal path to N. So harlnony ( M ) +hannony ( a ) &lt; harmony ( N ) , and the path must be optimal .</sentence>
				<definiendum id="0">N )</definiendum>
				<definiens id="0">the harmony of the optimal path to N. So harlnony</definiens>
			</definition>
			<definition id="6">
				<sentence>is One-Level Phonology ( Bird and l illison 1994 ) , which is a constraint-based phonology that defines inviolable constraints with automata .</sentence>
				<definiendum id="0">One-Level Phonology</definiendum>
				<definiens id="0">a constraint-based phonology that defines inviolable constraints with automata</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>7 below ) -- produce the re &lt; luired behavior. In particular , these rules relate surface Nit to lexical $ rAt÷t $ 2. X2Mold '~ can be seen as a re*These rules as well as other data presented in the e× : amples are simplified for the purl ) ose of denmnstration 2The lexical character A may have the surface realiza742 ( i ) A : ( L ~ _ j ' \ [ MORPIIIMIII ' : AI ) IUMLAIIT aolt-lt111la*tl\ ] ( ii ) t : O ¢= : :V _ 4.:0 t ( iii ) + : e ¢=e , . dental _ +:0 \ [ s It\ ] ; \ [ MORPHIMtII. ; ADIEPl. ; NTIIESI. ; -\ [ -\ ] Figure \ [ : Three extended two-level Rules lation between a surface string ( the word i~orm ) , a lexical string , and a feature structure ( tim interpretation of the word form ) . Relevant for sentence level processing is the morl ) hosyntactic information and the stem , found as the values of paths MOltPlI \ [ MIIEAI ) and MOR , Pll \ [ STEM resl ) ectively ( of. Fig. 9 below ) . This is supplemented by lexeme specific information in the value of SYNSF : M ( for a detailed description see Trost 93 ) . Framework IIPSG employs strongly typed feature structures together with principles constraining them further. Well-typcdness requirements restrict the space of valid feature structures ( cf. Carl ) enter 92 ) : Every feature structure must I ) e associated with a type , and every type restricts its associated feature structure in that only certain features are allowed and the values of these features must be of a certain type. Appropriateness and value restrictions are inherited along the type hierarchy. The second source of constraints , in order to admit only linguistically valid feature structures , are the principles of grammar. Pollard , ~ Sag 87 allow general implicative and negative constraints in the form of conditional feature structures. In Pollard h Sag in press principles are given only in verbal form. Recent work on formalizing the basis of IIPSG models them as constraints attached to types ( e.g. , Carpenter et al. 91 ) . Iiowever , these distinctions affect only how the applicability of a principle is specified. More iml ) ortant for our present purpose is the form which the constraints expressed by a principle may take. Besides constraints enforcing simple structure sharing ( e.g. , the Head Featnre Principle given in Fig.2 ) there are also complex relational dependencies ( e.g. , in the Subcategorization Principlea ) . Constraints tions a and d. The rule ha.s an empty phonological context but a morphological filter. This is an example for the treatmeat of non-concatenative phenomena in X2MonF. 3 , in a headed phrase ( i.e. , a phrmsal sign whose DTRS value is of sort head-struc ) , the suncAT value of the head like these go beyond the exl ) ressivity of l ) ure Dature formalisms alone and need to be defined in a recursive manner. In order to integrate such complex constraints in the feature unification framework we interpret unitication of typed feature structures under the restrictions of princil ) led constra.ints as constraint solving in the CLI ' paradigm ( Jafl'ar , ~ Lassez g7 ) . In CI , P the notion of unification is replaced by the more general notion of constraint solving. Constraint solvers may be embedded into a logic l ) rogramufing language either by writing a ntetaitlterl ) reter or by urn.king use of a system which allows \ [ or the impletn ( mtation el + unification ONtollS\ ] OilS. The s~ , cond approacls is taken by I ) MCAI ( : l , l ) 4 ( l\ [ olzbaur 92 ) , it l ) rolog system whose unitication mechanlsnl is extended in such a way that the user may introduce interl ) reted terms and specify their meaning with regard to uni\ [ ication through l'rolog predicates. The basic mechanism to a chiow , this behavior is the use of attributed variables , which may l ) e qualified by ar1 ) itrary user-defined attributes. Attributed variables behave like ordinary l ) rolog variables with two notal ) le exceptions : when an attributed variable is to be unified with a non-wu'iable term or another attril ) uted variable the unifi ( '.atk ) n extensions come into play. For either case the user has to supply a predicate which explicitly specifies how the attril ) utes interact and how they should 1 ) e interpreted with respect to the semantics of the al ) l ) lication domain. Unilication succeeds only if these constraint solving clauses managing the cond ) inati , m -el ' vm'ification -- af the involved attril ) utes are successfid. The iml ) hm~entati ( m of typed feature structures in our system makes use of the CLP facilities provided by this enhanced Prolog system. Feature structures are imlflemented by the attril ) ute daughter is the concatenation of the phrase 's SUBCAT llst with tile list ( in order of incre~Lslng obliqueness ) of SYNSEM values of the COml ) hmlent daughters. '' ( Pollard &amp; .gag in I ) ress ) awdlahle by anonymous ftp from f tp. ai.univie , ac. at 143 : fs ( Type , Dag , Goals ) , where Dag is a list of featurevalue pairs ( which is empty in case of atomic types ) or a marker indicating uninstantiatedness of the substructure ( feature structures are instantiated lazily ) . Goals is a list of delayed constraints ( see below ) . Well-typed unification of two feature structures is implemented via the constraint solving clauses mentioned above , taking into account type hierarchy and feature appropriateness ( for a detailed description cf. Matiasek &amp; Ileinz 93 ) . Constraints imposed onto feature structures by the principles of grammar are stated in a conditional form where the antecedent is restricted to contain only typing requirements. 5 In order to account for these conditional constraints we adopt a licensing view : Every node of a feature structure has to be licensed by all principles of grammar. A node is licensed by a principle if either ( i ) the feature structure F rooted in that node satisfies the applicability conditions of the l ) rinciple and the constraints expressed by the l ) rinciple successfully unify with F , or ( ii ) the feature structure F rooted in that node is incompatible with the applicability conditions of the principle. The interesting case arises when a feature structure does not satisfy the applicability conditions of the l ) rinciple but is compatible with them. Thus applicability of tile principle can be decided only later , after further instantiation or unification steps have restricted the ( sub ) structure rooted at that node. In precisely this case the application ( or the al ) andoning ) of the constraint has to be delayed. The delay mechanism utilizes tim Goals slot in the fs/3 6 attribute , which is dedicated to hold the delayed constraints. As an example take the well known Ilead Feature Principle of IIPSG ( Fig.2 ) r. The conditional operator === &gt; is translated at read time via terra_expansion/2 and implements the delay mechanism by coml ) iling l &gt; recon &lt; lition checks into the principle. These antecedent checks trigger either the application of the princiltle , its abandomnent , or its delay ( by annotating the variables wlfich are not sufficiently constrained to decide on the antecedent with the delayed goals ) . Two advantages of this approach to implement SThis is only a syntactic variant of attaching constraints solely to types ( Carpenter et al. 91 ) and does not permit general conditional structnres ms used in Pollard &amp; Sag 87. 6pred/n is the usual notation for a n-ary Prolog predicate. VThe operators : : , , , : : , : , === are defined for typing of a node , path restriction , path concatenation aim value restriction ( type or coreference ) respectively. A VM : \ [ SYNSEMILOCICATII'E D \ [ \ ] headedLD'I'ItS\ [ I \ [ I ' ; AD-DTIII SYNSEMILO &lt; ' : ICATII1EAI ) \ [ ~\ ] \ ] phrase Proloq : head_feature_principle ( X ) : X : :=headed_phrase === &gt; X : :synsem : loc : cat : head===H , X : :dtrs : head_dtr : synsem : loc : cat : head===H. Figure 2 : Head Feature Princil ) le principled constraints are especially important for our present purpose : First , stating redundant typing re &lt; luirements for embedded structures ( i.e. type restrictions that would follow automatically from well-typing ) forces delay of the conditional constraint until these sul ) structures are instantiated. This device can , e.g. , be used to block in : finite recursion in recursively detlned constraints. Second , the right hand part of the conditional is not restricted to feature logical expressions , but instead can contain arbitrary Prolog goals. In this way constraints involving relational dependencies ( such as the Subcategorization Principle and the morl &gt; hological relation between a lexical and a surface string ) can be expressed within the feature fornmlism and there is no need for external devices controllh~g this interaction .</sentence>
				<definiendum id="0">Dag</definiendum>
				<definiendum id="1">X</definiendum>
				<definiendum id="2">X</definiendum>
				<definiens id="0">a re*These rules as well as other data presented in the e× : amples are simplified for the purl</definiens>
				<definiens id="1">:V _ 4.:0 t ( iii ) + : e ¢=e , . dental _ +:0 \ [ s It\ ] ; \ [ MORPHIMtII. ; ADIEPl. ; NTIIESI. ; -\ [ -\ ] Figure \ [ : Three extended two-level Rules lation between a surface string ( the word i~orm ) , a lexical string</definiens>
				<definiens id="2">the morl ) hosyntactic information and the stem , found as the values of paths MOltPlI \ [ MIIEAI ) and MOR , Pll \ [ STEM resl</definiens>
				<definiens id="3">the use of attributed variables , which may l ) e qualified by ar1 ) itrary user-defined attributes. Attributed variables behave like ordinary l ) rolog variables with two notal ) le exceptions : when an attributed variable is to be unified with a non-wu'iable term or another attril ) uted variable the unifi ( '.atk ) n extensions come into play. For either case the user has to supply a predicate which explicitly specifies how the attril ) utes interact and how they should 1 ) e interpreted with respect to the semantics of the al ) l</definiens>
				<definiens id="4">the concatenation of the phrase 's SUBCAT llst with tile list ( in order of incre~Lslng obliqueness ) of SYNSEM values of the COml ) hmlent daughters. ''</definiens>
				<definiens id="5">a list of featurevalue pairs ( which is empty in case of atomic types ) or a marker indicating uninstantiatedness of the substructure ( feature structures are instantiated lazily</definiens>
				<definiens id="6">Every node of a feature structure has to be licensed by all principles of grammar. A node is licensed by a principle if either ( i ) the feature structure F rooted in that node satisfies the applicability conditions of the l ) rinciple and the constraints expressed by the l</definiens>
				<definiens id="7">conditional operator === &gt; is translated at read time via terra_expansion/2 and implements the delay mechanism by coml ) iling l &gt; recon &lt; lition checks into the principle. These antecedent checks trigger either the application of the princiltle , its abandomnent , or its delay ( by annotating the variables wlfich are not sufficiently constrained to decide on the antecedent with the delayed goals ) . Two advantages of this approach to implement SThis is only a syntactic variant of attaching constraints solely to types ( Carpenter et al. 91 ) and does not permit general conditional structnres ms used in Pollard &amp; Sag 87. 6pred/n is the usual notation for a n-ary Prolog predicate. VThe operators : : , , , : : , : , === are defined for typing of a node , path restriction , path concatenation aim value restriction ( type or coreference ) respectively. A VM : \ [ SYNSEMILOCICATII'E D \ [ \ ] headedLD'I'ItS\ [ I \ [ I ' ; AD-DTIII SYNSEMILO &lt; ' : ICATII1EAI ) \ [ ~\ ] \ ] phrase Proloq : head_feature_principle ( X ) : X : :=headed_phrase === &gt;</definiens>
				<definiens id="8">finite recursion in recursively detlned constraints. Second , the right hand part of the conditional is not restricted to feature logical expressions , but instead can contain arbitrary Prolog goals. In this way constraints involving relational dependencies ( such as the Subcategorization Principle and the morl &gt; hological relation between a lexical and a surface string</definiens>
			</definition>
			<definition id="1">
				<sentence>This architecture has computational as well as linguistic advantages , integrating morphology and morphophonology directly into the gralnmar is in the spirit of IIPSG , which views gramm : cr as a relation between the I ) honological ( or gr~qllielnic ) form of an utterance and its syntactic/seniantic rel ) resentation .</sentence>
				<definiendum id="0">IIPSG</definiendum>
			</definition>
</paper>

		<paper id="2110">
			<definition id="0">
				<sentence>ctuse for tyl ) es ( many details are omitted tor e~u. ~ , of exposition ) ( lyn_L 've is a sort for non-stative eventualities ( i.e. it subsumes processes and relic events ) -pred is either a lexical or logical predicate ( l ( : x_pred , e.g. swim ; log_t ) rcd , e.g. aud ) loc_chng is a thematic sort which characterizes participants undergoing change of location ( lir_t ) re .</sentence>
				<definiendum id="0">non-stative eventualities</definiendum>
				<definiendum id="1">loc_chng</definiendum>
			</definition>
			<definition id="1">
				<sentence>For exanlple , given a definition of list as in ( 3a ) , the listmembership predicate can be defined as in ( 3b ) where X is a typed feature structure ( Carl ) enter , 1992b : ch .</sentence>
				<definiendum id="0">X</definiendum>
			</definition>
</paper>

		<paper id="2168">
			<definition id="0">
				<sentence>For each of the semantic classes we have considered , we have defined a relatively small set of thematic grids , which define the 'regular ' thematic distributions .</sentence>
				<definiendum id="0">thematic grids</definiendum>
				<definiens id="0">define the 'regular ' thematic distributions</definiens>
			</definition>
			<definition id="1">
				<sentence>Articulation triggers belong to different linguistic domains : ( 1 ) lexical , where triggers are just words , e.g. 'devoted to ' , 'in the context of ' , 'propose ' , for THEME , ( 2 ) grammatical , where triggers can be phrases , or related to grammatical information ( such as tense and aspect , e.g. 'in the past years ' , 'since 1989 ' , for THEME ) , or verbs or nouns of certain semantic class , e.g. verbs of volition , of creation ( Levin 93 ) , ( 3 ) discursive , where triggers are mainly propositional connectors such as 'therefore ' , 'because ' , etc. , ( 4 ) pragmatic , where the relative positions of sentences and more generally , the physical form of texts ( e.g. enumerations ) can determine articulations .</sentence>
				<definiendum id="0">Articulation triggers</definiendum>
				<definiens id="0">THEME ) , or verbs or nouns of certain semantic class</definiens>
			</definition>
			<definition id="2">
				<sentence>The EDF dictionary includes the specification of nouns derived from verbs .</sentence>
				<definiendum id="0">EDF dictionary</definiendum>
				<definiens id="0">includes the specification of nouns derived from verbs</definiens>
			</definition>
</paper>

		<paper id="1103">
			<definition id="0">
				<sentence>In addition , CLAWS alXThe BNC is the result of a collaboration , supported by the Science mid Engineering Research Council ( SERC Grant No .</sentence>
				<definiendum id="0">CLAWS alXThe BNC</definiendum>
				<definiens id="0">the result of a collaboration , supported by the Science mid Engineering Research Council ( SERC Grant No</definiens>
			</definition>
			<definition id="1">
				<sentence>MATICAL TAGGER ( CLAWS4 ) The CLAWS4 tagger is a successor of the CLAWS 1 tagger described in outline in Marshall ( 1983 ) , and more fully in Garside et al ( 1987 ) , and has the same basic architecture .</sentence>
				<definiendum id="0">MATICAL TAGGER</definiendum>
				<definiens id="0">a successor of the CLAWS 1 tagger described in outline in Marshall ( 1983 ) , and more fully in Garside et al ( 1987 ) , and has the same basic architecture</definiens>
			</definition>
			<definition id="2">
				<sentence>The pre-processing section ( a ) is not trivial , since , in any large and varied corpus , there is a need to handle unusual text structures ( such as those of many popular and technical magazines ) , less usual graphic features ( e.g. non-roman alphabetic characters , mathematical symbols ) , and features of conversation transcriptions : e.g. false starts , incomplete words and utterances , unusual expletives , unplanned repetitions , and ( sometimes multiple ) overlapping speech .</sentence>
				<definiendum id="0">pre-processing section</definiendum>
				<definiens id="0">such as those of many popular and technical magazines ) , less usual graphic features ( e.g. non-roman alphabetic characters , mathematical symbols ) , and features of conversation transcriptions : e.g. false starts , incomplete words and utterances</definiens>
			</definition>
			<definition id="3">
				<sentence>Sections ( b ) and ( d ) apply essentially a Hidden Markov Model ( HMM ) to the assignmeut and disambiguation of tags .</sentence>
				<definiendum id="0">Sections</definiendum>
				<definiens id="0">a Hidden Markov Model ( HMM ) to the assignmeut and disambiguation of tags</definiens>
			</definition>
			<definition id="4">
				<sentence>Then : ww TT represents a word and its associated tag , separates a word from its predecessor \ [ TT\ ] represents : m already assigned tag \ [ WICI represents an unspecified word with a Word Initial Capilal `` I '' I'/'I'T means 'either '71 '' or TT ' ; ww/ww means 'either WW or WW ' ww'13'TT represenls an unresolved ambiguity between `` lq ' and TI '' TT* represents a tag wilh * marking the location of unspecified ch : u'acters ( \ [ TTI ) n represents the number of words ( up to n ) which may optionally intervene at a giveq point in the template TTnm represents the 'ditto tag ' attached to ~m orthographic word to indicate it is part of a complex sequence ( e.g.so that is tagged so CJS21 , that CJS22 ) .</sentence>
				<definiendum id="0">ww TT</definiendum>
				<definiendum id="1">WICI</definiendum>
				<definiendum id="2">ww'13'TT represenls</definiendum>
				<definiens id="0">a word and its associated tag , separates a word from its predecessor \ [ TT\ ] represents : m already assigned tag \ [</definiens>
				<definiens id="1">an unresolved ambiguity between `` lq ' and TI '' TT* represents a tag wilh * marking the location of unspecified ch : u'acters ( \ [ TTI ) n represents the number of words ( up to n ) which may optionally intervene at a giveq point in the template TTnm represents the 'ditto tag ' attached to ~m orthographic word to indicate it is part of a complex sequence</definiens>
			</definition>
			<definition id="5">
				<sentence>The BNC tagging takes place within the context of a larger project , in which a major task ( undertaken by OUCS at Oxford ) is to encode the texts in a TEIconformant mark-up ( CDIF ) .</sentence>
				<definiendum id="0">BNC tagging</definiendum>
				<definiens id="0">takes place within the context of a larger project , in which a major task</definiens>
			</definition>
</paper>

	</volume>
