<?xml version="1.0" encoding="UTF-8"?>
	<volume id="C02">

		<paper id="1057">
			<definition id="0">
				<sentence>Followed is a detailed description of the three criteria .</sentence>
				<definiendum id="0">Followed</definiendum>
			</definition>
			<definition id="1">
				<sentence>The edit distance between two strings s1 and s2 , is defined as the minimum number of operations to become the same ( Levenshtein1965 ) .</sentence>
				<definiendum id="0">edit distance</definiendum>
			</definition>
			<definition id="2">
				<sentence>“Good % ” is the sum of percent of “Error1” and “Perfect” .</sentence>
				<definiendum id="0">“Good % ”</definiendum>
			</definition>
			<definition id="3">
				<sentence>“Score” is the weighted sum of scores of the 6 kinds of translations .</sentence>
				<definiendum id="0">“Score”</definiendum>
				<definiens id="0">the weighted sum of scores of the 6 kinds of translations</definiens>
			</definition>
			<definition id="4">
				<sentence>The correlation coefficient between variable X and Y is defined as YXss YXCOV YXr ) , ( ) , ( = ( 7 ) where COV ( X , Y ) is the covariance defined by ∑ −− − = ) ) ( ( 1 1 ) , ( YYXX n YXCOV ii ( 8 ) The symbol meanings are as follows : sX : sample standard deviation of variable X sY : sample standard deviation of variable Y n : sample size Xi ( Yi ) : the ith component of variable X ( Y ) X ( Y ) : the sample mean of variable X ( Y ) From its definition , we know that the correlation coefficient is scale-independent and 11 ≤≤− r .</sentence>
				<definiendum id="0">correlation coefficient</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiendum id="2">YXss YXCOV YXr</definiendum>
				<definiendum id="3">COV ( X</definiendum>
				<definiendum id="4">sample size Xi ( Yi )</definiendum>
				<definiendum id="5">X</definiendum>
				<definiens id="0">the covariance defined by ∑ −− − = ) ) ( ( 1 1 ) , ( YYXX n YXCOV ii ( 8 ) The symbol meanings are as follows : sX : sample standard deviation of variable X sY : sample standard deviation of variable Y n</definiens>
				<definiens id="1">the sample mean of variable</definiens>
			</definition>
</paper>

		<paper id="1091">
</paper>

		<paper id="1166">
			<definition id="0">
				<sentence>The correspondence between concept classes across languages helps us write the probability P ( t|s ) of selecting word t as a translation of word s in the following general way , where C represents a multilingual concept class in MeSH ( we omit the derivation , which is mainly technical , and uses the fact that the correspondence between concept classes in MeSH is one-to-one ) : P ( t|s ) = a0 C P ( C|s ) P ( t|C , s ) ( 1 ) a formula which can be interpreted as follows : from a source word s of the source corpus , select a ( interlingual ) concept class in the thesaurus , according to P ( C|s ) , then generate a target word t of the target corpus from the concept class and the source word , according to P ( t|C , s ) .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">one-to-one ) : P ( t|s ) = a0 C P ( C|s ) P ( t|C , s ) ( 1 ) a formula which can be interpreted as follows : from a source word s of the source corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>The final estimate of P ( t|s ) is then based on the following mixture of models : P ( t|s ) = a3 i P ( i|f ( s ) ) Pi ( t|s ) ( 3 ) where i is an integer used to index the different models ( here 1 a4 i a5 3 ) , and P ( i|f ( s ) ) denotes the probability of selecting model i based on characteristics of s ( f is a function mapping the source word to a set of relevant features ) .</sentence>
				<definiendum id="0">f</definiendum>
				<definiens id="0">a function mapping the source word to a set of relevant features )</definiens>
			</definition>
</paper>

		<paper id="1070">
			<definition id="0">
				<sentence>Information extraction ( IE ) systems are costly to build because they require development texts , parsing tools , and specialized dictionaries for each application domain and each natural language that needs to be processed .</sentence>
				<definiendum id="0">Information extraction</definiendum>
				<definiens id="0">needs to be processed</definiens>
			</definition>
			<definition id="1">
				<sentence>Information extraction ( IE ) is an important application for natural language processing , and recent research has made great strides toward making IE systems easily portable across domains .</sentence>
				<definiendum id="0">Information extraction</definiendum>
				<definiens id="0">an important application for natural language processing , and recent research has made great strides toward making IE systems easily portable across domains</definiens>
			</definition>
			<definition id="2">
				<sentence>AutoSlog-TS is a derivative of AutoSlog that automatically generates extraction patterns by gathering statistics from a corpus of relevant texts ( within the domain ) and irrelevant texts ( outside the domain ) .</sentence>
				<definiendum id="0">AutoSlog-TS</definiendum>
				<definiens id="0">a derivative of AutoSlog that automatically generates extraction patterns by gathering statistics from a corpus of relevant texts ( within the domain</definiens>
			</definition>
			<definition id="3">
				<sentence>Each extraction pattern represents a linguistic expression that can extract noun phrases from one of three syntactic positions : subject , direct object , or object of a prepositional phrase .</sentence>
				<definiendum id="0">extraction pattern</definiendum>
				<definiens id="0">a linguistic expression that can extract noun phrases from one of three syntactic positions : subject , direct object , or object of a prepositional phrase</definiens>
			</definition>
			<definition id="4">
				<sentence>Lexical N-gram rule templates change the annotation of a word if the word ( s ) immediately surrounding it exactly match the rule .</sentence>
				<definiendum id="0">Lexical N-gram rule</definiendum>
				<definiens id="0">templates change the annotation of a word if the word ( s ) immediately surrounding it exactly match the rule</definiens>
			</definition>
			<definition id="5">
				<sentence>English TEST 140K words ( English ) SUNDANCE English ( plain ) English ( plain ) ( English ) SUNDANCE 4/5Eng TEST Eng TEST1/5 French TEST1/5 French TEST4/5 TBL ES S1 ES1 TS1 Train TBL ( 1 ) ( 3 ) Test TBL ( 4 ) ( French ) TBL TF0 Train TBL Test TBL English TEST S0 140K words 280K words 280K words Autoslog−TS Autoslog−TS [ + 280K words irrel .</sentence>
				<definiendum id="0">TEST 140K</definiendum>
				<definiens id="0">words ( English ) SUNDANCE English ( plain ) English ( plain ) ( English ) SUNDANCE 4/5Eng TEST Eng TEST1/5 French TEST1/5 French TEST4/5 TBL ES S1 ES1 TS1 Train TBL ( 1 ) ( 3 ) Test TBL</definiens>
			</definition>
			<definition id="6">
				<sentence>Next , a French TBL classifier ( TBL1 ) is trained on the projected MT-French annotations and the learned French TBL rules are subsequently applied to the native-French test data .</sentence>
				<definiendum id="0">TBL classifier</definiendum>
			</definition>
			<definition id="7">
				<sentence>An alternative path ( T E 4 ⇒ P4 ⇒ French-Test ) is more direct , in that the English TBL classifier is applied immediately to the word-aligned MTEnglish translation of the French test data .</sentence>
				<definiendum id="0">alternative path</definiendum>
				<definiens id="0">more direct , in that the English TBL classifier is applied immediately to the word-aligned MTEnglish translation of the French test data</definiens>
			</definition>
			<definition id="8">
				<sentence>5 French TBL Training and Transfer to Test Data English Annotation P2 P1 T1 T2 S1 S2 SUNDANCE ( English ) English ( plain ) TBL1 TBL2 English ( antd ) Projection Language Cross− P4 ( MT−English ) SUNDANCE MT−English Tst P 3 S 4 P 4 MT MT MT French ( plain ) S4 S 3 MT S3 P3 TBL3 T3 TBL3m T 3 MT Autoslog−TS Autoslog −TS ( plain ) MT−French MT−English ( plain ) MT−French ( annotated ) French Test Figure 4 : Sundance-based projection pathways 5 This is a “fair” gain , in that the MT-trained AutoSlog-TS patterns didn’t use translations of any of the French test data .</sentence>
				<definiendum id="0">Cross− P4</definiendum>
				<definiens id="0">Transfer to Test Data English Annotation P2 P1 T1 T2 S1 S2 SUNDANCE ( English ) English ( plain ) TBL1 TBL2 English ( antd ) Projection Language</definiens>
			</definition>
			<definition id="9">
				<sentence>Projection and Training Route P R F AutoSlog-TS trained on native English ( AS E ) S2 : Apply AS E to English-Antd P2 : Project to MT-French ( English-Antd ) .39 .24 .29 T2 : Train TBL FP2 &amp; Apply to FrTest S ( 1+2 ) : Apply AS E to English Antd+Plain P ( 1+2 ) : Project to MT-French ( Eng-Ant+Pl ) .43 .23 .30 T ( 1+2 ) : Train TBL FP1+2 &amp; Apply to FrTest S3 : Apply AS E to MT-Eng ( FrenchPlain ) P3 : Project to French-Plain .45 .04 .07 T3 : Train TBL FP3 &amp; Apply to FrTest S4 : Apply AS E to MT-Eng ( FrenchTest ) P4 : Direct Project to French-Test .48 .07 .13 AutoSlog-TS trained on MT English ( AS MTE ) S MT 3 : Apply AS MTE to MT-Eng ( FrPlain ) P MT 3 : Project to French-Plain .46 .25 .32 T MT 3 : Train TBL FMT3 &amp; Apply to FrTest S MT 4 : Apply AS MTE to MT-Eng ( FrTest ) P MT 4 : Direct Project to French-Test .55 .28 .37 Table 4 : Sundance-based IE projection performance 6 Table 4 shows that the best Sundance pathway achieved an F-measure of .37 .</sentence>
				<definiendum id="0">Projection</definiendum>
				<definiendum id="1">Training Route P R F AutoSlog-TS</definiendum>
				<definiens id="0">Project to MT-French ( Eng-Ant+Pl ) .43 .23 .30 T ( 1+2 ) : Train TBL FP1+2 &amp; Apply to FrTest S3 : Apply AS E to MT-Eng</definiens>
			</definition>
</paper>

		<paper id="1164">
</paper>

		<paper id="1145">
			<definition id="0">
				<sentence>Introduction The Penn Chinese Treebank ( CTB ) is an ongoing project , with its objective being to create a segmented Chinese corpus annotated with POS tags and syntactic brackets .</sentence>
				<definiendum id="0">CTB )</definiendum>
				<definiens id="0">an ongoing project , with its objective being to create a segmented Chinese corpus annotated with POS tags and syntactic brackets</definiens>
			</definition>
			<definition id="1">
				<sentence>The first installment of the project ( CTB-I ) consists of Xinhua newswire between the years 1994 and 1998 , totaling 100,000 words , fully segmented , POS-tagged and syntactically bracketed and it has been released to the public via the Penn Linguistic Data Consortium ( LDC ) .</sentence>
				<definiendum id="0">CTB-I )</definiendum>
			</definition>
			<definition id="2">
				<sentence>Specifically , we tagged the characters as LL ( left ) , RR ( right ) , MM ( middle ) and LR ( single-character word ) , based on their distribution within words .</sentence>
				<definiendum id="0">MM ( middle</definiendum>
				<definiendum id="1">LR</definiendum>
				<definiens id="0">LL ( left ) , RR ( right )</definiens>
				<definiens id="1">single-character word ) , based on their distribution within words</definiens>
			</definition>
</paper>

		<paper id="2011">
			<definition id="0">
				<sentence>The entire learning process consists of five main steps : In the preprocessing phase , the input text is tagged , lemmatised and chunked .</sentence>
				<definiendum id="0">entire learning process</definiendum>
				<definiens id="0">consists of five main steps</definiens>
			</definition>
			<definition id="1">
				<sentence>The FrameNet Project : Tools for Lexicon Building .</sentence>
				<definiendum id="0">FrameNet Project</definiendum>
				<definiens id="0">Tools for Lexicon Building</definiens>
			</definition>
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>K.Tanaka and Iwasaki ( 1996 ) also assumed the resemblance between co-occurring words in a source language and those in a target language , and performed experiments to flnd irrelevant translations intentionally added to a dictionary .</sentence>
				<definiendum id="0">K.Tanaka</definiendum>
			</definition>
			<definition id="1">
				<sentence>( subject-predicate , predicate-object , modiflcation , etc. ) † ... flerce price competition by exporters ... Japanese CN N CN a31 ( no ) N CN a19 ( ga ) V CN a6 ( o ) V CN a11 ( ni ) V CN a19 ( ga ) Adj English CN ( prep ) N CN V CN ( be ) Adj N CN Adj CN Ving CN CN : a target compound , N : noun , V : verb , Adj : adjective Figure 2 : Templates for syntactic dependence ( part ) † ... price competition was intensifying in this three months ... target word .</sentence>
				<definiendum id="0">CN V CN</definiendum>
				<definiens id="0">ga ) V CN a6 ( o ) V CN a11 ( ni ) V CN a19 ( ga ) Adj English CN ( prep ) N</definiens>
			</definition>
			<definition id="2">
				<sentence>„w ( t ; r ) = ‰ L ( t ; r ) : f ( t ; r ) 6= 0 0 : f ( t ; r ) = 0 ( 1 ) L ( t ; r ) = X i ; j21 ; 2 kij log kijNC iRj = k11 log k11NC 1R1 +k12 log k12NC 1R2 + k21 log k21NC 2R1 +k22 log k22NC 2R2 ( 2 ) k11 = f ( t ; r ) k12 = f ( t ) ¡ k11 k21 = f ( r ) ¡ k11 k22 = N ¡ k11 ¡ k12 ¡ k21 ( 3 ) C1 = k11 +k12 C2 = k21 +k22 R1 = k11 +k21 R2 = k12 +k22 where f ( t ) and f ( r ) are frequencies of compound noun t and co-occurring word r , respectively .</sentence>
				<definiendum id="0">f</definiendum>
				<definiens id="0">frequencies of compound noun t and co-occurring word r , respectively</definiens>
			</definition>
			<definition id="3">
				<sentence>f ( t ; r ) is the co-occurring frequency between t and r , and N is the total frequencies of all words in a corpus .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the total frequencies of all words in a corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>w = 1+ fd ( t ; r ) f ( t ; r ) ⁄ const ( 6 ) Here , fd ( t ; r ) is the frequency of word r that has dependency on t. The constant is determined experimentally , and later evaluation is done with const = 2 .</sentence>
				<definiendum id="0">r )</definiendum>
				<definiens id="0">the frequency of word r that has dependency</definiens>
			</definition>
			<definition id="5">
				<sentence>The similarity Sw ( ts ; tt ) between compound nouns ts in the source language and tt in the a24a2a26a2a27a2a28 „ w=„a operating proflt „w=„a business interest „w=„a [ independent words ] a27a18a28 3478 proflt 117 a8a10a9a12a61a7a52 654 slash 16.3 reduce 7.8 a53a12a11a14a13 508 expectation 137 a15a17a16 a22 455 rationalize 46.8 a18a17a19 363 design 11.2 a20a22a21 353 share 130 [ dependent words ] a23a14a24 a52 1866 increase 49.7 increase 6.3 a9a17a25a12a61a7a52 727 decline 5.9 diminish 14.1 a35a18a37a12a61a7a52 709 estimate 51.2 estimate 3.6 a26a28a27 422 division 49.2 division 7.1 a29a31a30 a52 321 contrast 3.2 compete 9.4 a32a31a33 266 connect 4.9 link 5.4 [ semantic attributes ] ( * ) [ 2262 ] 8531 131 11 [ 1867 ] 7290 321 93 [ 2694 ] 4936 13 19 [ 1168 ] 3855 83 [ 1395 ] 3229 695 110 [ 1920 ] 1730 810 428 ( * ) [ 2262 : increase/decrease ] , [ 1867 : transaction ] , [ 2694 : period/term ] , [ 1168 : economic system ] , [ 1395 : consideration ] , [ 1920 : labor ] Table2 : Comparisonofco-occurrencewordand semantic attributes target language is deflned by context word vectors and translation matrix T as follows .</sentence>
				<definiendum id="0">similarity Sw</definiendum>
				<definiens id="0">economic system ]</definiens>
				<definiens id="1">Comparisonofco-occurrencewordand semantic attributes target language is deflned by context word vectors and translation matrix T as follows</definiens>
			</definition>
</paper>

		<paper id="1088">
			<definition id="0">
				<sentence>Jae-Ho Kim* , In-Ho Kang , Key-Sun Choi* Korea Advanced Institute of Science and Technology ( KAIST ) / Korea Terminology Research Center for Language and Knowledge Engineering* ( KORTERM ) 373-1 , Guseong-dong , Yuseong-gu Daejeon , KOREA , 305-701 { jjaeh @ world , ihkang @ csone , kschoi @ world } .kaist.ac.kr This paper proposes an unsupervised learning model for classifying named entities .</sentence>
				<definiendum id="0">Knowledge Engineering*</definiendum>
				<definiens id="0">Key-Sun Choi* Korea Advanced Institute of Science and Technology ( KAIST ) / Korea Terminology Research Center for Language</definiens>
			</definition>
			<definition id="1">
				<sentence>Named entity extraction is an important step for various applications in natural language processing .</sentence>
				<definiendum id="0">Named entity extraction</definiendum>
				<definiens id="0">an important step for various applications in natural language processing</definiens>
			</definition>
			<definition id="2">
				<sentence>Named entity extraction involves identifying named entities in the text and classifying their types such as person , organization , location , time expressions , numeric expressions , and so on ( Sekine and Eriguchi , 2000 ) .</sentence>
				<definiendum id="0">Named entity extraction</definiendum>
				<definiens id="0">involves identifying named entities in the text and classifying their types such as person , organization , location , time expressions , numeric expressions</definiens>
			</definition>
			<definition id="3">
				<sentence>As it is more robust and requires less human intervention , several statistical methods based on a hidden Markov model ( Bikel et al. , 1997 ) , a Maximum Entropy model ( Borthwich et al. , 1998 ) and a Decision Tree model ( Béchet et al. 2000 ) have been studied .</sentence>
				<definiendum id="0">Decision Tree model</definiendum>
				<definiens id="0">requires less human intervention , several statistical methods based on a hidden Markov model ( Bikel et al. , 1997 ) , a Maximum Entropy model ( Borthwich et al. , 1998 ) and a</definiens>
			</definition>
			<definition id="4">
				<sentence>Josa 3 is a postposition that follows the target word and te predicate is a verb that predicates the target word .</sentence>
				<definiendum id="0">te predicate</definiendum>
				<definiens id="0">a postposition that follows the target word and</definiens>
				<definiens id="1">a verb that predicates the target word</definiens>
			</definition>
			<definition id="5">
				<sentence>Feature-vector format lexical morpheme ( w ) Modifier POS tag ( t ) lexical morpheme ( w ) Target word POS tag ( t ) lexical morpheme ( w ) Modifiee POS tag ( t ) Josa lexical morpheme ( w ) Predicate lexical morpheme ( w ) Category Label tag Training example : [ w , t , w , t , w , t , w , w , person ] Test example : [ w , t , w , t , w , t , w , w , Blank ] Figure 3 : The format of an example for learning The ensemble of several classifiers can be improve the performance .</sentence>
				<definiendum id="0">Feature-vector format lexical morpheme ( w ) Modifier POS tag</definiendum>
				<definiens id="0">Target word POS tag ( t ) lexical morpheme ( w ) Modifiee POS tag ( t ) Josa lexical morpheme ( w ) Predicate lexical morpheme ( w ) Category Label tag Training example : [ w , t , w</definiens>
			</definition>
			<definition id="6">
				<sentence>As features , SNoW uses a modifier and a target word , Timbl uses a modifiee and a target word , and MEMT uses a josa , a predicate and a target word .</sentence>
				<definiendum id="0">SNoW</definiendum>
				<definiendum id="1">Timbl</definiendum>
				<definiendum id="2">MEMT</definiendum>
				<definiens id="0">uses a modifier and a target word ,</definiens>
			</definition>
			<definition id="7">
				<sentence>`` Nymble : a High-Performance Learning Name-finder '' , In Proceedings of the Fifth Conference on Applied Natural Language Processing .</sentence>
				<definiendum id="0">Nymble</definiendum>
				<definiens id="0">a High-Performance Learning Name-finder '' , In Proceedings of the Fifth Conference on Applied Natural Language Processing</definiens>
			</definition>
			<definition id="8">
				<sentence>`` NYU : Description of the MENE Named Entity System as Used in MUC-7 '' , In Proceedings of the Seventh Message Understanding Conference ( MUC-7 ) .</sentence>
				<definiendum id="0">NYU</definiendum>
				<definiens id="0">Description of the MENE Named Entity System as Used in MUC-7 ''</definiens>
			</definition>
</paper>

		<paper id="1152">
			<definition id="0">
				<sentence>H ( A ) = X x2fyes ; ; nog p ( A x ) log 2 1 p ( A x ) By calculating H ( A ) for all conditions A that can be added to the current query condition , thesystemgeneratesthequestionthathasthe maximumvalueof H ( A ) .</sentence>
				<definiendum id="0">H</definiendum>
				<definiendum id="1">nog p</definiendum>
				<definiens id="0">A ) = X x2fyes</definiens>
			</definition>
</paper>

		<paper id="1094">
			<definition id="0">
				<sentence>Examining the Spoken English Corpus ( SEC ) , Knowles et al. ( 1996a , p.111 ) found that speakers insert breaks after about five syllables in most of the cases and that they almost never utter more than 15 syllables without a break .</sentence>
				<definiendum id="0">Examining the Spoken English Corpus</definiendum>
				<definiens id="0">found that speakers insert breaks after about five syllables in most of the cases and that they almost never utter more than 15 syllables without a break</definiens>
			</definition>
			<definition id="1">
				<sentence>htmland consists of approximately 52k words of contemporary spoken British English drawn from various genres .</sentence>
				<definiendum id="0">htmland</definiendum>
				<definiens id="0">consists of approximately 52k words of contemporary spoken British English drawn from various genres</definiens>
			</definition>
			<definition id="2">
				<sentence>Recall is the percentage of breaks in the corpus that our model finds : recall a0 a1a3a2a5a4a1 a6a8a7a10a9a11a9a13a12 where B is the total number of breaks in the test corpus and D is the number of deletion errors ( breaks which the model does not assign , even though they are in the test corpus ) .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiendum id="1">D</definiendum>
				<definiens id="0">the percentage of breaks in the corpus that our model finds : recall a0 a1a3a2a5a4a1 a6a8a7a10a9a11a9a13a12 where B is the total number of breaks in the test corpus</definiens>
				<definiens id="1">the number of deletion errors ( breaks which the model does not assign , even though they are in the test corpus )</definiens>
			</definition>
			<definition id="3">
				<sentence>Precision is the percentage of breaks assigned by the model which is correct according to the corpus : precision a0a15a14 a2a17a16 a14 a6a18a7a10a9a11a9a13a12 where S is the total number of breaks which our model assigns to the corpus and I is the number of insertion errors ( breaks that the model assigns even though no break occurs in the test corpus ) .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">the percentage of breaks assigned by the model which is correct according to the corpus : precision a0a15a14 a2a17a16 a14 a6a18a7a10a9a11a9a13a12 where</definiens>
				<definiens id="1">breaks that the model assigns even though no break occurs in the test corpus )</definiens>
			</definition>
			<definition id="4">
				<sentence>Performance structures : A psycholinguistic and linguistic appraisal .</sentence>
				<definiendum id="0">Performance structures</definiendum>
				<definiens id="0">A psycholinguistic and linguistic appraisal</definiens>
			</definition>
</paper>

		<paper id="1169">
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>An ontology mainly consists of a set of concepts and a set of relations that describe relationships among the concepts .</sentence>
				<definiendum id="0">ontology mainly</definiendum>
			</definition>
			<definition id="1">
				<sentence>According to Gruber’s definition , an ontology basically consists of a set of concepts , i.e. the so-called objects , which represent classes of objects , and a set of relations , i.e. the so-called interrelationships , which are binary relations defined on concepts .</sentence>
				<definiendum id="0">ontology</definiendum>
				<definiens id="0">consists of a set of concepts , i.e. the so-called objects , which represent classes of objects , and a set of relations , i.e. the so-called interrelationships , which are binary relations defined on concepts</definiens>
			</definition>
			<definition id="2">
				<sentence>A special transitive relation “subClassOf” represents a subsumption relation between concepts and structures a taxonomy .</sentence>
				<definiendum id="0">special transitive relation “subClassOf”</definiendum>
				<definiens id="0">a subsumption relation between concepts and structures a taxonomy</definiens>
			</definition>
			<definition id="3">
				<sentence>In their case , they formally defined an ontology as an 8-tuple &lt; L , C , C H , R , R H , F , G , A &gt; , in which the first primitive L denotes a set of strings that describe lexical entries for concepts and relations , the middle 6 primitives structure the taxonomy of the ontology , and the last primitive A is a set of axioms that describe additional constraints on the ontology .</sentence>
				<definiendum id="0">ontology</definiendum>
				<definiendum id="1">primitive L</definiendum>
				<definiens id="0">an 8-tuple &lt; L , C , C H , R , R H , F , G , A &gt; , in which the first</definiens>
				<definiens id="1">a set of strings that describe lexical entries for concepts and relations , the middle 6 primitives structure the taxonomy of the ontology , and the last primitive A is a set of axioms that describe additional constraints on the ontology</definiens>
			</definition>
			<definition id="4">
				<sentence>HTML is the most popular markup language for structuring documents .</sentence>
				<definiendum id="0">HTML</definiendum>
				<definiens id="0">the most popular markup language for structuring documents</definiens>
			</definition>
			<definition id="5">
				<sentence>RDF ( Resource Description Framework ) developed by the W3C ( World Wide Web Consortium ) is also an instance of XML .</sentence>
				<definiendum id="0">RDF</definiendum>
				<definiens id="0">Resource Description Framework ) developed by the W3C ( World Wide Web Consortium ) is also an instance of XML</definiens>
			</definition>
			<definition id="6">
				<sentence>By the taxonomy , we construct a fundamental ontology that consists of a set of concepts and a special relation – “subClassOf.”</sentence>
				<definiendum id="0">fundamental ontology</definiendum>
			</definition>
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>The distance of instances in k-NN is computed by estimating the similarity measured by LSA and PLSA .</sentence>
				<definiendum id="0">k-NN</definiendum>
				<definiens id="0">computed by estimating the similarity measured by LSA and PLSA</definiens>
			</definition>
			<definition id="1">
				<sentence>Latentsemanticanalysis ( LSA ) ( Landauer et al. , 1998 ) and probabilistic latent semantic analysis ( PLSA ) ( Hofmann , 2001 ) fall under the model .</sentence>
				<definiendum id="0">Latentsemanticanalysis</definiendum>
				<definiens id="0">LSA ) ( Landauer et al. , 1998 ) and probabilistic latent semantic analysis ( PLSA ) ( Hofmann , 2001 ) fall under the model</definiens>
			</definition>
			<definition id="2">
				<sentence>LSA is a theory and method for extracting and representing the contextual-usage meaning of words .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">a theory and method for extracting and representing the contextual-usage meaning of words</definiens>
			</definition>
			<definition id="3">
				<sentence>SVD is a form of factor analysis and is deflned as A = U§V T ( 1 ) , where § is a diagonal matrix composed of nonzero eigen values of AAT or ATA , and U and V are the orthogonal eigenvectors associatedwiththe r nonzeroeigenvaluesof AAT and ATA , respectively .</sentence>
				<definiendum id="0">SVD</definiendum>
				<definiendum id="1">§</definiendum>
				<definiens id="0">a form of factor analysis</definiens>
				<definiens id="1">a diagonal matrix composed of nonzero eigen values of AAT or ATA , and U and V are the orthogonal eigenvectors associatedwiththe r nonzeroeigenvaluesof AAT and ATA , respectively</definiens>
			</definition>
			<definition id="4">
				<sentence>A word-document co-occurrence event , ( d ; w ) , is modelled in a probabilistic way where it is parameterized as in P ( d ; w ) = X z P ( z ) P ( d ; wjz ) = X z P ( z ) P ( wjz ) P ( djz ) : ( 3 ) Here , w and d are assumed to be conditionally independent given a speciflc z. P ( wjz ) and P ( djz ) are topic-speciflc word distribution and document distribution , respectively .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a probabilistic way where it is parameterized as in P ( d ; w ) = X</definiens>
				<definiens id="1">( djz ) are topic-speciflc word distribution and document distribution , respectively</definiens>
			</definition>
			<definition id="5">
				<sentence>The structure of the dictionary is as follows ( Kim and Kim , 1998 ) : T ( Si ) = 8 &gt; &gt; &lt; &gt; &gt; : T1 if Cooc ( Si ; S1 ) T2 if Cooc ( Si ; S2 ) : : : Tn otherwise ; ( 6 ) where Cooc ( Si ; Sj ) denotes grammatical cooccurrenceofsourcewordsSi andSj , whichone means an input word to be translated and the other means an argument word to be used in translation , and Tj is the translation result of the source word .</sentence>
				<definiendum id="0">Sj )</definiendum>
				<definiendum id="1">whichone</definiendum>
				<definiendum id="2">Tj</definiendum>
				<definiens id="0">follows ( Kim and Kim , 1998 ) : T ( Si ) = 8 &gt; &gt; &lt; &gt; &gt; : T1 if Cooc ( Si ; S1 ) T2 if Cooc ( Si ; S2 ) : : : Tn otherwise ; ( 6 ) where Cooc ( Si ;</definiens>
			</definition>
			<definition id="6">
				<sentence>T ( ¢ ) denotes the translation process .</sentence>
				<definiendum id="0">T ( ¢ )</definiendum>
				<definiens id="0">the translation process</definiens>
			</definition>
			<definition id="7">
				<sentence>One of the fundamental di–culties in cooccurrence-based approaches to word sense disambiguation ( translation selection in this case ) is the problem of data sparseness or unseen words .</sentence>
				<definiendum id="0">translation selection</definiendum>
				<definiens id="0">the problem of data sparseness or unseen words</definiens>
			</definition>
			<definition id="8">
				<sentence>Table 1 : Examples of co-occurrence word lists for a verb ‘build’ in the dictionary Meaning of ‘build’ in Korean ( Tj ) Collocated Object Noun ( Sj ) ‘geon-seol-ha-da’ ( = ‘construct’ ) plant facility network ... ‘geon-chook-ha-da’ ( = ‘design’ ) house center housing ... ‘che-chak-ha-da’ ( = ‘produce’ ) car ship model ... ‘seol-lip-ha-da’ ( = ‘establish’ ) company market empire ... ‘koo-chook-ha-da’ ( = ‘develop’ ) system stake relationship ... Table 2 : Examples of translation of ‘build’ source words translated words ( in Korean ) sense of the verb ‘build a plant’ ) ‘gong-jang-eul geon-seol-ha-da’ ‘construct’ ‘build a car’ ) ‘ja-dong-cha-reul che-chak-ha-da’ ‘produce’ ‘build a company’ ) ‘hoi-sa-reul seol-lip-ha-da’ ‘establish’ The k-nearest neighbor learning algorithm ( Cover and Hart , 1967 ) ( Aha et al. , 1991 ) assumes all instances correspond to points in the n-dimensional space Rn .</sentence>
				<definiendum id="0">k-nearest neighbor learning algorithm</definiendum>
				<definiens id="0">Examples of translation of ‘build’ source words translated words</definiens>
			</definition>
			<definition id="9">
				<sentence>Then the distance between two instances xi and xj , D ( xi ; xj ) , is deflned to be D ( xi ; xj ) = q ( a ( xi ) ¡a ( xj ) ) 2 ( 7 ) and a ( xi ) denotes the value of instance xi , similarly to cosine computation between two vectors .</sentence>
				<definiendum id="0">xj )</definiendum>
				<definiens id="0">the value of instance xi , similarly to cosine computation between two vectors</definiens>
			</definition>
			<definition id="10">
				<sentence>Selection errors taking place in LSA and PLSA models were caused mainly by the following reasons .</sentence>
				<definiendum id="0">Selection errors</definiendum>
				<definiens id="0">taking place in LSA and PLSA models were caused mainly by the following reasons</definiens>
			</definition>
</paper>

		<paper id="1116">
			<definition id="0">
				<sentence>We analyse ( 1 ) to ( 6 ) as alternative realizations of a thematic role , because one semantic argument ( the PATIENT ) can either be realized as indirect object or PP , while the THEME is always realized as a direct object NP .</sentence>
				<definiendum id="0">PATIENT</definiendum>
				<definiens id="0">always realized as a direct object NP</definiens>
			</definition>
			<definition id="1">
				<sentence>The advantage of Davis’s model , in contrast , is the lexical inheritance architecture which is a formal means to capture generalizations .</sentence>
				<definiendum id="0">Davis’s model</definiendum>
				<definiens id="0">a formal means to capture generalizations</definiens>
			</definition>
			<definition id="2">
				<sentence>So far , TDG is only concerned with syntax : every TDG analysis consists of an unordered dependency tree ( ID tree ) and an ordered and projective topology tree ( LP tree ) .</sentence>
				<definiendum id="0">TDG</definiendum>
				<definiens id="0">only concerned with syntax : every TDG analysis consists of an unordered dependency tree ( ID tree ) and an ordered and projective topology tree ( LP tree )</definiens>
			</definition>
			<definition id="3">
				<sentence>A = fvalID ; valTH ; linkg is the set of lexical features α , and E the set of lexical entries e , having the following signature:5 2 4 valID : 2R Π valTH : 2T Π link : 2T R 3 5 E is a lattice of TDGTH-lexicon entries ; lexical entries either correspond to words or to lexical types which can be inherited ( see below ) .</sentence>
				<definiendum id="0">linkg</definiendum>
				<definiendum id="1">E</definiendum>
				<definiens id="0">the set of lexical features α ,</definiens>
			</definition>
			<definition id="4">
				<sentence>THw2 is a TH edge from node w1 to node w2 labeled with θ .</sentence>
				<definiendum id="0">THw2</definiendum>
				<definiens id="0">a TH edge from node w1 to node w2 labeled with θ</definiens>
			</definition>
			<definition id="5">
				<sentence>When we collapse nodes , we also collapse their lexical entries : the value of the lexical feature α of a collapsed node w1 = fw1 ; : : : ; wng is the set union of the values assigned to the individual nodes : α ( w1 ) = α ( w1 ) [ : : : [ α ( wn ) ( 21 ) In the example TH graph in ( 15 ) above , the two nodes will and live are collapsed into the equivalence class will live .</sentence>
				<definiendum id="0">lexical entries</definiendum>
				<definiendum id="1">wng</definiendum>
				<definiens id="0">the value of the lexical feature α of a collapsed node w1 = fw1 ; : : : ;</definiens>
				<definiens id="1">the set union of the values assigned to the individual nodes : α ( w1 ) = α ( w1 ) [ : : : [ α</definiens>
			</definition>
			<definition id="6">
				<sentence>Albany : State Univ of NY .</sentence>
				<definiendum id="0">Albany</definiendum>
				<definiens id="0">State Univ of NY</definiens>
			</definition>
</paper>

		<paper id="1127">
			<definition id="0">
				<sentence>Location normalization is a special application of word sense disambiguation ( WSD ) .</sentence>
				<definiendum id="0">Location normalization</definiendum>
				<definiens id="0">a special application of word sense disambiguation ( WSD )</definiens>
			</definition>
			<definition id="1">
				<sentence>The merging process consists of several steps including checking information compatibility such as checking synonyms , name aliases and co-reference of anaphors , time and location normalization .</sentence>
				<definiendum id="0">merging process</definiendum>
				<definiens id="0">consists of several steps including checking information compatibility such as checking synonyms , name aliases and co-reference of anaphors , time and location normalization</definiens>
			</definition>
			<definition id="2">
				<sentence>It is defined as an Attribute Value Matrix ( AVM ) to represent key aspects of information about entities , including their relationships with other entities .</sentence>
				<definiendum id="0">Attribute Value Matrix ( AVM )</definiendum>
				<definiens id="0">to represent key aspects of information about entities , including their relationships with other entities</definiens>
			</definition>
			<definition id="3">
				<sentence>The LocNZ process constructs a weighted graph where each node represents a location sense , and each edge represents similarity weight between location names .</sentence>
				<definiendum id="0">LocNZ process</definiendum>
				<definiens id="0">constructs a weighted graph where each node represents a location sense , and each edge represents similarity weight between location names</definiens>
			</definition>
			<definition id="4">
				<sentence>The Tipster Gazetteer ( http : //crl.nmsu.edu/ cgi-bin/Tools/CLR/clrcat ) used in our system has 171,039 location entries with 237,916 total senses that cover most location names all over the world .</sentence>
				<definiendum id="0">Tipster Gazetteer ( http</definiendum>
				<definiens id="0">//crl.nmsu.edu/ cgi-bin/Tools/CLR/clrcat ) used in our system has 171,039 location entries with 237,916 total senses that cover most location names all over the world</definiens>
			</definition>
			<definition id="5">
				<sentence>Suppose a location word w has several city senses si : Sense ( w ) indicates the default sense of w ; sim ( wi , xjk ) means the similarity value between two senses of the word w and the jth co-occuring word xj ; num ( w ) is the number of w in the document , and NumAll is the total number of locations .</sentence>
				<definiendum id="0">NumAll</definiendum>
				<definiens id="0">the total number of locations</definiens>
			</definition>
</paper>

		<paper id="1131">
</paper>

		<paper id="1029">
</paper>

		<paper id="1039">
</paper>

		<paper id="2021">
			<definition id="0">
				<sentence>The ATT-Meta system handles all these types of uncertainty and conflict .</sentence>
				<definiendum id="0">ATT-Meta system</definiendum>
				<definiens id="0">handles all these types of uncertainty and conflict</definiens>
			</definition>
			<definition id="1">
				<sentence>To this end , ATT-Meta handles mixed metaphor in a manner consistent with the way it handles simple metaphors .</sentence>
				<definiendum id="0">ATT-Meta</definiendum>
				<definiens id="0">handles mixed metaphor in a manner consistent with the way it handles simple metaphors</definiens>
			</definition>
			<definition id="2">
				<sentence>Conclusion The ATT-Meta project is making headway in showing how metaphorical utterances can be computationally processed .</sentence>
				<definiendum id="0">ATT-Meta project</definiendum>
				<definiens id="0">making headway in showing how metaphorical utterances can be computationally processed</definiens>
			</definition>
			<definition id="3">
				<sentence>Barnden , J.A. ( 2001a ) Application of the ATT-Meta Metaphor-Understanding Approach to Various Examples in the ATT-Meta Project Databank .</sentence>
				<definiendum id="0">Application</definiendum>
				<definiens id="0">of the ATT-Meta Metaphor-Understanding Approach to Various Examples in the ATT-Meta Project Databank</definiens>
			</definition>
			<definition id="4">
				<sentence>Barnden , J.A. and Lee , M.G. ( 2001a ) Understanding open-ended usages of familiar conceptual metaphors : An approach and artificial intelligence system .</sentence>
				<definiendum id="0">Barnden , J.A.</definiendum>
				<definiens id="0">open-ended usages of familiar conceptual metaphors : An approach and artificial intelligence system</definiens>
			</definition>
			<definition id="5">
				<sentence>Technical Report CSRP-0109 , School of Computer Science , The University of Birmingham , U.K. Carbonell , J.G. ( 1982 ) Metaphor : an inescapable phenomenon in natural-language comprehension .</sentence>
				<definiendum id="0">Metaphor</definiendum>
				<definiens id="0">an inescapable phenomenon in natural-language comprehension</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>For the ranking of the TECs and their filtering we experimented with 4 scoring functions : MI ( pointwise mutual information ) , DICE , LL ( loglikelihood ) , and χ 2 ( chi-square ) .</sentence>
				<definiendum id="0">DICE , LL</definiendum>
				<definiens id="0">mutual information ) ,</definiens>
			</definition>
			<definition id="1">
				<sentence>Rec * is defined as the number of source lemma types in the correct TEPs divided by the number of lemma types in the source language with at least 3 occurrences .</sentence>
				<definiendum id="0">Rec *</definiendum>
			</definition>
			<definition id="2">
				<sentence>Rec is defined as the number of source lemma types in the correct TEPs divided by the number of lemma types in the source language .</sentence>
				<definiendum id="0">Rec</definiendum>
				<definiens id="0">the number of source lemma types in the correct TEPs divided by the number of lemma types in the source language</definiens>
			</definition>
			<definition id="3">
				<sentence>The filtering condition in case of ties was the following : max ( COGN ( T j S , T j T ) ≥0.4 ) ∨min ( DIST ( T j S , T j T ) ≤2 ) The results show that the Rec ( 72.7 % ) almost doubled compared with the best Rec obtained by the BASE algorithm for nouns ( 39.9 % ) .</sentence>
				<definiendum id="0">COGN</definiendum>
				<definiens id="0">T j S , T j T ) ≥0.4 ) ∨min ( DIST ( T j S , T j T ) ≤2 ) The results show that the Rec ( 72.7 % ) almost doubled compared with the best Rec obtained by the BASE algorithm for nouns</definiens>
			</definition>
			<definition id="4">
				<sentence>212-219 Hiemstra , D. ( 1997 ) Deriving a bilingual lexicon for cross language information retrieval .</sentence>
				<definiendum id="0">Deriving</definiendum>
				<definiens id="0">a bilingual lexicon for cross language information retrieval</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>It automatically built and incrementally extended a BN from propositions read ML ( IG |IG ) Arg SysInt ML ( Arg | IG ) Arg ML ( IG |SysInt ) = 0SysInt ( d ) SysInt ( Top Candidate ) N reported argument N heard argument G argued with B G and B were enemies G was in garden at 11 G was in garden at time of death G had motiveG had opportunity G murdered B N reported argument N heard argument G argued with B G and B were enemies G was in garden at 11 G was in garden at time of death G had motiveG had opportunity G murdered B ( c ) IG for best SysIntSysInt ( b ) Top-ranked IG N reported argumentG was in garden at 11 G murdered B ( a ) Original argument ( Arg ) Arg The neighbour reported a argument between Mr Green and Mr Body last week Mr Green was in the garden at 11 Mr Body was murdered by Mr Green AND - &gt; [ likely ] heated seen Figure 1 : Interpretation and MML evaluation in a story , so that the BN represented hypotheses that became plausible as the story unfolded .</sentence>
				<definiendum id="0">IG N reported argumentG</definiendum>
				<definiens id="0">Top Candidate ) N reported argument N heard argument G argued with B G and B were enemies G was in garden at 11 G was in garden at time of death G had motiveG had opportunity G murdered B N reported argument N heard argument G argued with B G and B were enemies G was in garden at 11 G was in garden at time of death G had motiveG had opportunity G murdered B ( c</definiens>
			</definition>
			<definition id="1">
				<sentence>The MML principle is a model selection technique which applies information-theoretic criteria to trade data fit against model complexity .</sentence>
				<definiendum id="0">MML principle</definiendum>
				<definiens id="0">a model selection technique which applies information-theoretic criteria to trade data fit against model complexity</definiens>
			</definition>
			<definition id="2">
				<sentence>The MML criterion is derived from Bayes Theorem : Pra1a3a2a5a4a7a6a9a8a11a10 Pra1a3a6a9a8a13a12 Pra1a3a2a9a14a6a15a8 , where a2 is the data and a6 is a hypothesis which explains the data .</sentence>
				<definiendum id="0">a2</definiendum>
				<definiendum id="1">a6</definiendum>
			</definition>
			<definition id="3">
				<sentence>An Implication Graph is a graphical representation of an argument , which represents a basic “understanding” of the argument .</sentence>
				<definiendum id="0">Implication Graph</definiendum>
			</definition>
			<definition id="4">
				<sentence>a4a6a5 SysInt represents an understanding of a candidate interpretation .</sentence>
				<definiendum id="0">a4a6a5 SysInt</definiendum>
				<definiens id="0">an understanding of a candidate interpretation</definiens>
			</definition>
			<definition id="5">
				<sentence>The interpretation process obtains a4a6a5 Arg from the input , and SysInt from a4a6a5 Arg ( left-hand side of Figure 1 ) .</sentence>
				<definiendum id="0">interpretation process</definiendum>
				<definiens id="0">obtains a4a6a5 Arg from the input</definiens>
			</definition>
			<definition id="6">
				<sentence>Thus the message length for SysInt in the context of a BN is a20a23a22a25a24 a26 C # nodes ( domainBN ) # nodes ( SysInt ) a29 a20a23a22a25a24 a26 C # incident arcs ( SysInt ) # arcs ( SysInt ) ( 2 ) The message which describes a4a7a5 Arg in terms of SysInt ( or rather in terms of a4a6a5 SysInt ) conveys how a4a6a5 Arg differs from the system’s interpretation in two respects : ( 1 ) belief , and ( 2 ) argument structure .</sentence>
				<definiendum id="0">message length for SysInt</definiendum>
				<definiens id="0">message which describes a4a7a5 Arg in terms of SysInt ( or rather in terms of a4a6a5 SysInt ) conveys how a4a6a5 Arg differs from the system’s interpretation in two respects : ( 1 ) belief</definiens>
			</definition>
			<definition id="7">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>Introduction Natural Language Processing ( NLP ) is an emerging technology with a variety of real-world applications .</sentence>
				<definiendum id="0">Introduction Natural Language Processing ( NLP )</definiendum>
				<definiens id="0">an emerging technology with a variety of real-world applications</definiens>
			</definition>
			<definition id="1">
				<sentence>Computer-Assisted Language Learning/Teaching ( CALL/CALT ) is one area that NLP techniques can contribute to .</sentence>
				<definiendum id="0">Computer-Assisted Language Learning/Teaching ( CALL/CALT )</definiendum>
				<definiens id="0">one area that NLP techniques can contribute to</definiens>
			</definition>
			<definition id="2">
				<sentence>Intuition is a conventional tool in teaching one’s native language , but from a student’s perspective , a well-developed systematic method of instruction can be more convincing .</sentence>
				<definiendum id="0">Intuition</definiendum>
				<definiens id="0">a conventional tool in teaching one’s native language , but from a student’s perspective</definiens>
			</definition>
			<definition id="3">
				<sentence>Japanese is a head-final language .</sentence>
				<definiendum id="0">Japanese</definiendum>
				<definiens id="0">a head-final language</definiens>
			</definition>
			<definition id="4">
				<sentence>ZD employs a rule-based approach , with theoretically sound heuristics .</sentence>
				<definiendum id="0">ZD</definiendum>
				<definiens id="0">employs a rule-based approach , with theoretically sound heuristics</definiens>
			</definition>
			<definition id="5">
				<sentence>GTVD is a semantic valency dictionary , originally designed for transfer-based Japanese-to-English machine translation , so it includes as many valency pattern entries for each predicate as are necessary for effective transfer .</sentence>
				<definiendum id="0">GTVD</definiendum>
				<definiens id="0">a semantic valency dictionary , originally designed for transfer-based Japanese-to-English machine translation , so it includes as many valency pattern entries for each predicate as are necessary for effective transfer</definiens>
			</definition>
			<definition id="6">
				<sentence>Once zeros are identified , ZD decides where to insert the identified zeros in the original text , by keeping canonical ordering as listed in the valency pattern .</sentence>
				<definiendum id="0">ZD</definiendum>
				<definiens id="0">decides where to insert the identified zeros in the original text</definiens>
			</definition>
			<definition id="7">
				<sentence>Figure 7 : Predicate-argument structure with zeros Finally , ZD outputs the original series of clauses with zeros inserted in the most plausible positions , along with adjuncts , output ( D ) , as in Figure 8 .</sentence>
				<definiendum id="0">ZD</definiendum>
				<definiens id="0">outputs the original series of clauses with zeros inserted in the most plausible positions</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>The SLUITK generates semantic representations of each input sentence .</sentence>
				<definiendum id="0">SLUITK</definiendum>
				<definiens id="0">generates semantic representations of each input sentence</definiens>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>We describe a discovery program , called UNIVAUTO ( UNIVersals AUthoringTOol ) , whose domain of application is the study of language universals , a classic trend in contemporary linguistics .</sentence>
				<definiendum id="0">UNIVAUTO ( UNIVersals AUthoringTOol</definiendum>
				<definiens id="0">the study of language universals , a classic trend in contemporary linguistics</definiens>
			</definition>
			<definition id="1">
				<sentence>UNIVAUTO accepts as input the following , manually prepared , information : ( 1 ) A database ( =a table ) , usually comprising a sizable number of languages , described in terms of some properties ( featurevalue pairs ) , as well as a list of the abbreviations used in the database .</sentence>
				<definiendum id="0">UNIVAUTO</definiendum>
				<definiendum id="1">A database</definiendum>
				<definiens id="0">=a table ) , usually comprising a sizable number of languages , described in terms of some properties ( featurevalue pairs ) , as well as a list of the abbreviations used in the database</definiens>
			</definition>
			<definition id="2">
				<sentence>UNIV discovers logical patterns ( =universals ) , including ( but not limited to ) : • A ( absolute , non-implicational universal ) • If A1 and A2 and A3 and ... An , then B ( implicational universal ) UNIV can compute `` non-statistical '' universals ( holding without exceptions ) or `` statistical '' universals ( holding with some userspecified percentage of exceptions ) .</sentence>
				<definiendum id="0">UNIV discovers logical patterns ( =universals</definiendum>
				<definiens id="0">including ( but not limited to ) : • A ( absolute , non-implicational universal ) • If A1 and A2 and A3 and ... An , then B ( implicational universal ) UNIV can compute `` non-statistical '' universals ( holding without exceptions ) or `` statistical '' universals ( holding with some userspecified percentage of exceptions</definiens>
			</definition>
			<definition id="3">
				<sentence>AUTO accepts as input the discoveries made by UNIV , but also has access to the input data ( cf. AUTO can generally be characterized as a practical text generation system , of opportunistic type , intended to meet the needs of our particular task , rather than as a system intended to handle , in a general and principled way , scientific articles ' composition or surface generation of a wide range of linguistic phenomena ( reminiscent of earlier work on generation from formatted data of metereological bulletins ( Kittredge et .</sentence>
				<definiendum id="0">AUTO</definiendum>
				<definiens id="0">a system intended to handle , in a general and principled way , scientific articles ' composition or surface generation of a wide range of linguistic phenomena</definiens>
			</definition>
			<definition id="4">
				<sentence>The templates consist of canned text , interspersed with variables whose values are to be computed .</sentence>
				<definiendum id="0">templates</definiendum>
			</definition>
</paper>

		<paper id="1167">
</paper>

		<paper id="1102">
			<definition id="0">
				<sentence>Explanation : Question : de ( c , e ) = 3 de ( q , e ) = 1 i fm fe qd a a c b g h e x WH Question : Explanation : dq ( WH , a ) = 1 dq ( WH , e ) = 0 i f fe qd a a c b g h xethe WH Explanation : Question : de ( q , e ) = 0 i fm fe qd a a c b g h e x WH Explanation : Question : a33a84a144a164a168a65a172a112a153a165a152a64a137a112a167a62a33a112a152a100a149a123a142a165a172a112a153a49a152a100a145a8a162a119a191a76a142a49a143a112a152a171a225a52a162a137a161a32a149a119a181a150a186a180a181a164a144a160a168a119a140a112a159a103a152a11a140a65a142a185a67a10a162a137a176a137a152a32a181a89a167a178a211a1a51a150a185a186a182a215a41a151a105a152a75a211 a206 a134a117a131a21a215a39a174a150a176a137a144a160a145a129a142a165a149a119a140a117a161a158a152a4a191a194a153a49a162a65a159 a142a165a143a112a152 a149a119a140a117a145a129a154a155a152a32a153 a211a99a188a150a162a72a157a137a152a100a176a117a215a21a142a165a162a43a142a49a143a117a152 a149a151a140a150a161a183a143a112a162a119a153a103a211a1a51a150a185 a56a178a215a136a151a53a203a137a211a68a204a153a134a117a131a21a215a39a174a84a176a137a144a160a145a129a142a165a149a119a140a117a161a158a152a33a191a194a153a165a162a119a159 a142a165a143a112a152a33a219a121a216a27a143a117a152a11a149a119a176a148a142a49a162a40a142a49a143a112a152a190a149a119a140a117a161a183a143a112a162a119a153a103a211a1a51a150a185 a168 a215a146a151a105a152a114a211a39a154a155a134a33a131a84a215a19a144a160a168a119a140a112a162a65a153a49a152a100a145a19a176a137a152a32a142a49a152a11a153a49a159a103a144a160a140a112a152a32a153a183a145a33a211a71a51a117a185 a45a182a215 a156 a211 a206 a134a10a204a153a134a117a131a21a215a39a174a69a154a178a143a112a152a32a142a49a143a112a152a11a153a193a142a49a143a112a152a6a219a121a216a208a143a117a152a11a149a119a176a86a149a151a140a150a176a78a142a165a143a112a152a43a149a151a140a117a145a49a154a155a152a32a153a178a149a151a153a165a152a58a162a119a140a40a142a49a143a112a152a43a145a165a149a151a159a103a152a6a145a129a144a146a176a137a152a6a162a119a191a26a142a49a143a117a152a43a149a151a140a117a161a183a143a112a162a65a153 a142a165a143a112a152a4a149a151a140a117a161a183a143a117a162a119a153a21a144a164a140a150a176a137a144a160a161a11a149a123a142a165a152a11a145a8a149a182a154a155a152a11a149a151a166a65a152a32a153a21a153a165a152a32a181a146a149a123a142a49a144a160a162a119a140a150a145a129a143a112a144a160a147a52a185a182a45a180a144a146a145a173a163 a142a183a149a151a140a117a161a32a152a19a144a146a145a155a159a81a152a100a149a119a145a49a172a112a153a165a152a11a176a78a149a65a145a8a142a165a143a112a152a58a140a75a172a117a159a189a188a69a152a32a153a155a162a151a191a26a144a160a140a75a142a49a152a32a153a165a169a119a152a11a140a112a144a160a140a112a168 a161a32a162a119a140a75a142a49a152a11a140a65a142a178a154a155a162a119a153a183a176a112a145a11a185 a231 a196a97a157 a211 a236 a134a117a131a21a215a32a174a4a142a49a143a112a152a177a176a137a144a160a145a129a142a165a149a119a140a117a161a158a152a171a144a164a140a121a142a49a143a112a152a177a170a65a172a117a152a11a145a129a142a49a144a160a162a119a140a47a191a194a153a165a162a119a159 a204 a142a165a162a155a131a54a167a43a195a193a143a112a152a78a161a32a181a164a162a75a145a129a152a11a153a182a142a49a162a13a142a49a143a117a152a190a219a121a216a27a147a112a143a112a153a183a149a119a145a49a152a103a149a119a140a156a149a151a140a117a161a183a143a112a162a65a153 a144a146a145a32a174a26a142a49a143a112a152a94a188a150a152a32a142a129a142a49a152a11a153a43a144a164a142a189a144a160a145a6a191a194a162a119a153 a83 a172a117a176a137a168a65a144a164a140a117a168a10a181a164a162a137a161a32a149a119a181a8a149a151a181a160a144a164a168a65a140a112a159a103a152a32a140a75a142a11a185 a45a19a144a160a145a129a142a165a149a119a140a117a161a158a152a6a144a146a145a193a159a103a152a11a149a65a145a129a172a112a153a165a152a11a176a86a149a119a145a178a149a119a188a150a162a123a169a65a152a119a185 a231 a189 a211a39a133a135a134 a236 a134a33a131a84a215a11a174a123a154a178a143a112a152a32a142a49a143a112a152a11a153 a206 a149a151a140a117a176a52a204a54a149a151a153a165a152a8a162a65a140a6a142a49a143a112a152a193a145a165a149a151a159a103a152a175a145a49a144a160a176a112a152 a162a119a191a71a131 a167a81a139a209a191a178a142a49a143a112a152a11a205a177a149a119a153a49a152a190a162a65a140a187a142a49a143a112a152a86a145a49a149a119a159a81a152a40a145a129a144a146a176a137a152a119a174a108a142a49a143a150a149a123a142a81a145a49a172a112a168a119a163 a168a65a152a11a145a129a142a165a145a26a142a49a143a117a149a151a142a84a142a165a143a112a152a155a172a112a140a117a176a137a152a11a153a49a181a160a205a114a144a164a140a112a168a58a145a49a152a32a159a190a149a151a140a75a142a49a144a146a161a8a153a165a152a32a181a146a149a123a142a165a144a164a162a65a140a117a145a49a143a112a144a164a147a150a145 a144a160a140a86a142a49a143a112a152a58a142a173a154a155a162a190a145a129a152a11a140a65a142a165a152a32a140a117a161a32a152a11a145a178a149a119a153a49a152a58a145a129a144a160a159a103a144a164a181a146a149a151a153a100a185 a33a84a144a164a168a65a172a112a153a165a152a171a137a81a144a160a181a164a181a160a172a117a145a129a142a49a153a183a149a123a142a165a152a11a145a155a142a49a143a117a152a58a191a194a152a11a149a123a142a165a172a112a153a165a152a11a145a193a162a151a191a84a142a49a143a117a144a160a145a178a159a103a162a137a176a137a152a11a181a96a185 a139a141a140a184a145a129a162a65a159a103a152a189a161a32a149a65a145a129a152a100a145a32a174a117a142a49a143a112a144a146a145a180a181a160a162a137a161a32a149a119a181a108a149a151a181a160a144a160a168a119a140a112a159a103a152a32a140a75a142a180a159a103a152a32a142a49a143a112a162a137a176a10a159a103a144a160a159a33a163 a144a146a161a32a145a4a154a178a143a150a149a123a142a19a149a103a159a103a162a119a153a165a152a189a145a129a142a49a153a165a172a117a161a39a142a165a172a112a153a165a152a11a176a13a145a129a152a11a159a190a149a151a140a75a142a49a144a146a161a6a153a165a152a32a147a112a153a165a152a11a145a49a152a32a140a75a142a165a149a151a142a49a144a160a162a119a140 a154a155a162a119a172a112a181a146a176a40a176a112a162a117a174a114a220a117a140a117a176a112a144a164a140a112a168a103a142a165a143a112a152a6a143a112a152a100a149a119a176a40a162a151a191a84a142a49a143a112a152a6a169a65a152a32a153a165a188a94a142a165a162a81a154a178a143a117a144a160a161a183a143a86a142a49a143a117a152 a149a151a140a150a145a129a154a155a152a32a153a193a144a146a145a193a142a49a143a112a152a6a162a65a188 a83 a152a11a161a158a142a11a167 Who wrote Lord Jim r ( Conrad , Who , wrote ) = 0 Dorsai soldier ask not Joseph Conrad , who wrote one of my all−time favorite books , Lord Jim dq ( Who , wrote ) = 0 de ( Conrad , wrote ) = 1 a139a141a140a177a162a119a142a49a143a112a152a11a153a58a144a164a140a117a145a129a142a165a149a119a140a117a161a158a152a100a145a32a174a52a142a49a143a112a152a78a149a119a181a164a144a160a168a119a140a112a152a100a176a184a154a155a162a119a153a183a176a184a161a32a162a119a153a165a153a49a152a100a145a129a147a69a162a119a140a150a176a112a145 a142a49a162a190a149a103a153a165a152a32a181a160a152a32a169a123a149a151a140a75a142a178a154a155a162a119a153a183a176a78a144a160a140a40a142a165a143a112a152a43a170a75a172a112a152a11a145a129a142a49a144a160a162a119a140a26a167 What was the name of the spanish waiter in Fawlty Towers ?</sentence>
				<definiendum id="0">Explanation</definiendum>
				<definiens id="0">qd a a c b g h e x WH Explanation : Question : a33a84a144a164a168a65a172a112a153a165a152a64a137a112a167a62a33a112a152a100a149a123a142a165a172a112a153a49a152a100a145a8a162a119a191a76a142a49a143a112a152a171a225a52a162a137a161a32a149a119a181a150a186a180a181a164a144a160a168a119a140a112a159a103a152a11a140a65a142a185a67a10a162a137a176a137a152a32a181a89a167a178a211a1a51a150a185a186a182a215a41a151a105a152a75a211 a206 a134a117a131a21a215a39a174a150a176a137a144a160a145a129a142a165a149a119a140a117a161a158a152a4a191a194a153a49a162a65a159 a142a165a143a112a152 a149a119a140a117a145a129a154a155a152a32a153 a211a99a188a150a162a72a157a137a152a100a176a117a215a21a142a165a162a43a142a49a143a117a152 a149a151a140a150a161a183a143a112a162a119a153a103a211a1a51a150a185 a56a178a215a136a151a53a203a137a211a68a204a153a134a117a131a21a215a39a174a84a176a137a144a160a145a129a142a165a149a119a140a117a161a158a152a33a191a194a153a165a162a119a159 a142a165a143a112a152a33a219a121a216a27a143a117a152a11a149a119a176a148a142a49a162a40a142a49a143a112a152a190a149a119a140a117a161a183a143a112a162a119a153a103a211a1a51a150a185 a168 a215a146a151a105a152a114a211a39a154a155a134a33a131a84a215a19a144a160a168a119a140a112a162a65a153a49a152a100a145a19a176a137a152a32a142a49a152a11a153a49a159a103a144a160a140a112a152a32a153a183a145a33a211a71a51a117a185 a45a182a215 a156 a211 a206 a134a10a204a153a134a117a131a21a215a39a174a69a154a178a143a112a152a32a142a49a143a112a152a11a153a193a142a49a143a112a152a6a219a121a216a208a143a117a152a11a149a119a176a86a149a151a140a150a176a78a142a165a143a112a152a43a149a151a140a117a145a49a154a155a152a32a153a178a149a151a153a165a152a58a162a119a140a40a142a49a143a112a152a43a145a165a149a151a159a103a152a6a145a129a144a146a176a137a152a6a162a119a191a26a142a49a143a117a152a43a149a151a140a117a161a183a143a112a162a65a153 a142a165a143a112a152a4a149a151a140a117a161a183a143a117a162a119a153a21a144a164a140a150a176a137a144a160a161a11a149a123a142a165a152a11a145a8a149a182a154a155a152a11a149a151a166a65a152a32a153a21a153a165a152a32a181a146a149a123a142a49a144a160a162a119a140a150a145a129a143a112a144a160a147a52a185a182a45a180a144a146a145a173a163 a142a183a149a151a140a117a161a32a152a19a144a146a145a155a159a81a152a100a149a119a145a49a172a112a153a165a152a11a176a78a149a65a145a8a142a165a143a112a152a58a140a75a172a117a159a189a188a69a152a32a153a155a162a151a191a26a144a160a140a75a142a49a152a32a153a165a169a119a152a11a140a112a144a160a140a112a168 a161a32a162a119a140a75a142a49a152a11a140a65a142a178a154a155a162a119a153a183a176a112a145a11a185 a231 a196a97a157 a211 a236 a134a117a131a21a215a32a174a4a142a49a143a112a152a177a176a137a144a160a145a129a142a165a149a119a140a117a161a158a152a171a144a164a140a121a142a49a143a112a152a177a170a65a172a117a152a11a145a129a142a49a144a160a162a119a140a47a191a194a153a165a162a119a159 a204 a142a165a162a155a131a54a167a43a195a193a143a112a152a78a161a32a181a164a162a75a145a129a152a11a153a182a142a49a162a13a142a49a143a117a152a190a219a121a216a27a147a112a143a112a153a183a149a119a145a49a152a103a149a119a140a156a149a151a140a117a161a183a143a112a162a65a153 a144a146a145a32a174a26a142a49a143a112a152a94a188a150a152a32a142a129a142a49a152a11a153a43a144a164a142a189a144a160a145a6a191a194a162a119a153 a83 a172a117a176a137a168a65a144a164a140a117a168a10a181a164a162a137a161a32a149a119a181a8a149a151a181a160a144a164a168a65a140a112a159a103a152a32a140a75a142a11a185 a45a19a144a160a145a129a142a165a149a119a140a117a161a158a152a6a144a146a145a193a159a103a152a11a149a65a145a129a172a112a153a165a152a11a176a86a149a119a145a178a149a119a188a150a162a123a169a65a152a119a185 a231 a189 a211a39a133a135a134 a236 a134a33a131a84a215a11a174a123a154a178a143a112a152a32a142a49a143a112a152a11a153 a206 a149a151a140a117a176a52a204a54a149a151a153a165a152a8a162a65a140a6a142a49a143a112a152a193a145a165a149a151a159a103a152a175a145a49a144a160a176a112a152 a162a119a191a71a131 a167a81a139a209a191a178a142a49a143a112a152a11a205a177a149a119a153a49a152a190a162a65a140a187a142a49a143a112a152a86a145a49a149a119a159a81a152a40a145a129a144a146a176a137a152a119a174a108a142a49a143a150a149a123a142a81a145a49a172a112a168a119a163 a168a65a152a11a145a129a142a165a145a26a142a49a143a117a149a151a142a84a142a165a143a112a152a155a172a112a140a117a176a137a152a11a153a49a181a160a205a114a144a164a140a112a168a58a145a49a152a32a159a190a149a151a140a75a142a49a144a146a161a8a153a165a152a32a181a146a149a123a142a165a144a164a162a65a140a117a145a49a143a112a144a164a147a150a145 a144a160a140a86a142a49a143a112a152a58a142a173a154a155a162a190a145a129a152a11a140a65a142a165a152a32a140a117a161a32a152a11a145a178a149a119a153a49a152a58a145a129a144a160a159a103a144a164a181a146a149a151a153a100a185 a33a84a144a164a168a65a172a112a153a165a152a171a137a81a144a160a181a164a181a160a172a117a145a129a142a49a153a183a149a123a142a165a152a11a145a155a142a49a143a117a152a58a191a194a152a11a149a123a142a165a172a112a153a165a152a11a145a193a162a151a191a84a142a49a143a117a144a160a145a178a159a103a162a137a176a137a152a11a181a96a185 a139a141a140a184a145a129a162a65a159a103a152a189a161a32a149a65a145a129a152a100a145a32a174a117a142a49a143a112a144a146a145a180a181a160a162a137a161a32a149a119a181a108a149a151a181a160a144a160a168a119a140a112a159a103a152a32a140a75a142a180a159a103a152a32a142a49a143a112a162a137a176a10a159a103a144a160a159a33a163 a144a146a161a32a145a4a154a178a143a150a149a123a142a19a149a103a159a103a162a119a153a165a152a189a145a129a142a49a153a165a172a117a161a39a142a165a172a112a153a165a152a11a176a13a145a129a152a11a159a190a149a151a140a75a142a49a144a146a161a6a153a165a152a32a147a112a153a165a152a11a145a49a152a32a140a75a142a165a149a151a142a49a144a160a162a119a140 a154a155a162a119a172a112a181a146a176a40a176a112a162a117a174a114a220a117a140a117a176a112a144a164a140a112a168a103a142a165a143a112a152a6a143a112a152a100a149a119a176a40a162a151a191a84a142a49a143a112a152a6a169a65a152a32a153a165a188a94a142a165a162a81a154a178a143a117a144a160a161a183a143a86a142a49a143a117a152 a149a151a140a150a145a129a154a155a152a32a153a193a144a146a145a193a142a49a143a112a152a6a162a65a188 a83 a152a11a161a158a142a11a167 Who wrote Lord Jim r</definiens>
			</definition>
</paper>

		<paper id="1096">
			<definition id="0">
				<sentence>Word prediction systems based on n-gram statistics are an important component of AAC devices , i.e. , software and possibly hardware typing aids for disabled users ( Copestake , 1997 ; Carlberger , 1998 ) .</sentence>
				<definiendum id="0">Word prediction systems</definiendum>
			</definition>
			<definition id="1">
				<sentence>Usually , the ksr is defined by ksr = ( 1 ki +ksk n ) 100 ( 1 ) where : ki is the number of input characters actually typed , ks is the number of keystrokes needed to select among the predictions presented by the model and kn is the number of keystrokes that would be needed if the whole text was typed without any prediction aid .</sentence>
				<definiendum id="0">kn</definiendum>
				<definiens id="0">ksr = ( 1 ki +ksk n ) 100 ( 1 ) where : ki is the number of input characters actually typed , ks is the number of keystrokes needed to select among the predictions presented by the model and</definiens>
				<definiens id="1">the number of keystrokes that would be needed if the whole text was typed without any prediction aid</definiens>
			</definition>
			<definition id="2">
				<sentence>In an analysis of the APA newswire corpus ( a corpus of over 28 million words ) , we found that almost half ( 47 % ) of the word types were compounds .</sentence>
				<definiendum id="0">newswire corpus</definiendum>
				<definiens id="0">a corpus of over 28 million words</definiens>
			</definition>
			<definition id="3">
				<sentence>While we obtained encouraging results with it ( Baroni et al. , 2002 ) , we feel that a particularly unsatisfactory aspect of the model described in the previous section is that information on the modifier is not 2Here and below , c stands for the last word in the left context of w ; w is the suffix of the word to be predicted minus the ( possibly empty ) prefix typed by the user up to the current point .</sentence>
				<definiendum id="0">w</definiendum>
				<definiens id="0">the suffix of the word to be predicted minus the ( possibly empty ) prefix typed by the user up to the current point</definiens>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>Spoken queries are a natural medium for searching the Web in settings where typing on a keyboard is not practical .</sentence>
				<definiendum id="0">Spoken queries</definiendum>
				<definiens id="0">a natural medium for searching the Web in settings where typing</definiens>
			</definition>
			<definition id="1">
				<sentence>A speech recognition system uses a language model to determine the probability of different recognition hypotheses .</sentence>
				<definiendum id="0">speech recognition system</definiendum>
				<definiens id="0">uses a language model to determine the probability of different recognition hypotheses</definiens>
			</definition>
</paper>

		<paper id="1055">
</paper>

		<paper id="1073">
</paper>

		<paper id="1062">
			<definition id="0">
				<sentence>The newspapers corpora defined by the US-governmentsponsored Text Retrieval Conference ( TREC , 2000 ) has been used as a test corpus .</sentence>
				<definiendum id="0">US-governmentsponsored Text Retrieval Conference</definiendum>
				<definiens id="0">a test corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>Definition : Nearest common ancestors ( NCA ) The nearest common ancestors between two words A and B are the set of nodes that are daughters of c ( A ) ∩ c ( B ) and that are not ancestors in c ( A ) ∩ c ( B ) .</sentence>
				<definiendum id="0">NCA</definiendum>
				<definiens id="0">The nearest common ancestors between two words A and B are the set of nodes that are daughters of c ( A ) ∩ c ( B ) and that are not ancestors in c ( A ) ∩ c ( B )</definiens>
			</definition>
			<definition id="2">
				<sentence>Definition : Asymmetric nearest common ancestor ( ANCA ) The asymmetric nearest common ancestors from a node A to a node B is contained into the set of ancestors of c ( B ) ∩ c ( A ) which have a direct node belonging to h ( A ) but not to h ( B ) .</sentence>
				<definiendum id="0">Asymmetric nearest common ancestor</definiendum>
				<definiendum id="1">ANCA</definiendum>
				<definiens id="0">The asymmetric nearest common ancestors from a node A to a node B is contained into the set of ancestors of c ( B ) ∩ c ( A ) which have a direct node belonging to h ( A ) but not to h ( B )</definiens>
			</definition>
			<definition id="3">
				<sentence>Definition : proximity ( d ⊥ ) The proximity measure takes into account the common features but also the differences between two elements A and B and is defined by the following function : d ⊥ ( A , B ) = d c190 ( A , B ) + ∑ = + n 1i ii ) ) ANCA , B ( d ) ANCA , A ( d ( n 1 Because the set of ANCA from a node A to a node B is not the same as the one from a node B to a node A , the proximity measure has the following properties : − d ⊥ ( A , A ) = 0 , because ANCA ( A , A ) = ∅ .</sentence>
				<definiendum id="0">proximity</definiendum>
				<definiendum id="1">proximity measure</definiendum>
				<definiens id="0">takes into account the common features but also the differences between two elements A and B and is defined by the following function : d ⊥ ( A , B ) = d c190 ( A , B ) + ∑ = + n 1i ii ) ) ANCA , B ( d ) ANCA , A ( d ( n 1 Because the set of ANCA from a node A to a node B is not the same as the one from a node B to a node A , the proximity measure has the following properties : − d ⊥ ( A , A ) = 0 , because ANCA ( A , A ) = ∅</definiens>
			</definition>
			<definition id="4">
				<sentence>This node has the following properties : G85 n 1i ) m ( h ) M ( h i = = G85 n 1i ) m ( c ) M ( c i = = where h ( M ) is the set of ancestors of M and c ( M ) the set of links between M and the root of the graph .</sentence>
				<definiendum id="0">h ( M )</definiendum>
				<definiens id="0">the set of ancestors of M and c ( M ) the set of links between M and the root of the graph</definiens>
			</definition>
			<definition id="5">
				<sentence>A filtering profile is a set of words in relation with the domain or the slot to be fill , defined by the end-user .</sentence>
				<definiendum id="0">filtering profile</definiendum>
				<definiens id="0">a set of words in relation with the domain or the slot to be fill , defined by the end-user</definiens>
			</definition>
			<definition id="6">
				<sentence>The Semiograph links independent mechanisms of expansion defined by the user .</sentence>
				<definiendum id="0">Semiograph</definiendum>
				<definiens id="0">links independent mechanisms of expansion defined by the user</definiens>
			</definition>
			<definition id="7">
				<sentence>Fellbaum C. ( 1998 ) WordNet : An Electronic Lexical Database , edited by Fellbaum , M.I.T. press .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">An Electronic Lexical Database , edited by Fellbaum , M.I.T. press</definiens>
			</definition>
</paper>

		<paper id="1046">
</paper>

		<paper id="2027">
</paper>

		<paper id="1162">
			<definition id="0">
				<sentence>WordNet was constructed with what is commonly referred to as a differential theory of lexical semantics ( Miller et al. , 1990 ) , which aims to differentiate word senses by grouping words into synonym sets ( synsets ) , which are constructed as to allow a user to easily distinguish between different senses of a word .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">aims to differentiate word senses by grouping words into synonym sets ( synsets ) , which are constructed as to allow a user to easily distinguish between different senses of a word</definiens>
			</definition>
			<definition id="1">
				<sentence>At the most atomic level is a set of almost 1500 basic definitions , or sememes , such as “human” , or “aValue” ( attributevalue ) .</sentence>
				<definiendum id="0">most atomic level</definiendum>
				<definiens id="0">a set of almost 1500 basic definitions , or sememes</definiens>
			</definition>
			<definition id="2">
				<sentence>There has also been a lot of work involving bilingual corpora , including the IBM Candide project ( Brown et al. , 1990 ) , which used statistical data to align words in sentence pairs from parallel corpora in an unsupervised fashion through the EM algorithm ; Church ( 1993 ) used character frequencies to align words in a parallel corpus ; Smadja et al. ( 1996 ) used cooccurrence functions to extract phrasal collocations for translation , and Melamed ( 1997 ) identified non-compositional compounds by comparing the objective functions of a translation model with and without NCCs .</sentence>
				<definiendum id="0">IBM Candide project</definiendum>
				<definiens id="0">used statistical data to align words in sentence pairs from parallel corpora in an unsupervised fashion through the EM algorithm ; Church ( 1993 ) used character frequencies to align words in a parallel corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>Wordnet : An on-line lexical database .</sentence>
				<definiendum id="0">Wordnet</definiendum>
				<definiens id="0">An on-line lexical database</definiens>
			</definition>
			<definition id="4">
				<sentence>EuroWordNet : A Multilingual Database with Lexical Semantic Networks .</sentence>
				<definiendum id="0">EuroWordNet</definiendum>
			</definition>
</paper>

		<paper id="1136">
			<definition id="0">
				<sentence>However , since these 1 A bunsetsu is one of the linguistic units in Japanese , and roughly corresponds to a basic phrase in English .</sentence>
				<definiendum id="0">bunsetsu</definiendum>
				<definiens id="0">one of the linguistic units in Japanese , and roughly corresponds to a basic phrase in English</definiens>
			</definition>
			<definition id="1">
				<sentence>A bunsetsu consists of one independent word and more than zero ancillary words .</sentence>
				<definiendum id="0">bunsetsu</definiendum>
			</definition>
			<definition id="2">
				<sentence>The probability of the dependency between bunsetsus are calculated using these attribute values as follows : P ( i rel → j|B ) = C ( i → j , h i , h j , t i , t j , r i ) C ( h i , h j , t i , t j , r i ) ( 1 ) × C ( i → j , r i , d ij , p ij , l i ) C ( r i , d ij , p ij , l i ) Here , C is a cooccurrence frequency function and B is a sequence of bunsetsus ( b 1 b 2 ···b n ) .</sentence>
				<definiendum id="0">C</definiendum>
			</definition>
			<definition id="3">
				<sentence>Kawaguchi , N. , Matsubara , S. , Takeda , K. , and Itakura , F. : Multi-Dimensional Data Acquisition for Integrated Acoustic Information Research , Proceedings of 3rd International Conference on Language Resources and Evaluation ( LREC2002 ) , pp .</sentence>
				<definiendum id="0">Evaluation</definiendum>
				<definiens id="0">Multi-Dimensional Data Acquisition for Integrated Acoustic Information Research</definiens>
			</definition>
</paper>

		<paper id="1107">
			<definition id="0">
				<sentence>A dependency relation expresses a part of syntactic and semantic characteristics of the sentence , and can be strongly in relation to the intentional content .</sentence>
				<definiendum id="0">dependency relation</definiendum>
				<definiens id="0">expresses a part of syntactic and semantic characteristics of the sentence , and can be strongly in relation to the intentional content</definiens>
			</definition>
			<definition id="1">
				<sentence>α m = 2C M M A + M B ( 3 ) M A : the number of morphemes in S A M B : the number of morphemes in S B C M : the number of morphemes in correspondence In our research , if a word class is given to nouns and proper nouns characteristic of a dialogue task and two morphemes belong to the same class , these morphemes are also considered to be in correspondence .</sentence>
				<definiendum id="0">C M</definiendum>
				<definiens id="0">the number of morphemes in S A M B : the number of morphemes in S B</definiens>
			</definition>
			<definition id="2">
				<sentence>Then , the conditional occurrence probability P ( I n |I n−1 n−N+1 ) is defined as a formula ( 4 ) .</sentence>
				<definiendum id="0">conditional occurrence probability P</definiendum>
			</definition>
</paper>

		<paper id="1126">
			<definition id="0">
				<sentence>The reestimation adjusts the model’s parameters in the augmented parse-tree space to maximize the likelihood of the observed ( incomplete ) data , in the hopes of finding a better distribution over augmented parse trees ( the complete data ) .</sentence>
				<definiendum id="0">reestimation</definiendum>
			</definition>
			<definition id="1">
				<sentence>Most existing head-finding rules and argumentfinding rules work by specifying parent-child relations ( e.g. , NN is the head of NP , or NP is an argument of VP ) .</sentence>
				<definiendum id="0">NN</definiendum>
				<definiendum id="1">NP</definiendum>
				<definiens id="0">the head of NP</definiens>
			</definition>
			<definition id="2">
				<sentence>pattern , which matches any label .</sentence>
				<definiendum id="0">pattern</definiendum>
				<definiens id="0">matches any label</definiens>
			</definition>
			<definition id="3">
				<sentence>TIG ( Schabes and Waters , 1995 ) is a weakly-context free restriction of tree adjoining grammar ( Joshi and Schabes , 1997 ) , in which tree fragments called elementary trees are combined by two composition operations , substitution and adjunction ( see Figure 3 ) .</sentence>
				<definiendum id="0">TIG</definiendum>
				<definiens id="0">a weakly-context free restriction of tree adjoining grammar ( Joshi and Schabes , 1997 ) , in which tree fragments called elementary trees are combined by two composition operations</definiens>
			</definition>
			<definition id="4">
				<sentence>( 7 ) where di is the number of occurrences in training of the context i ( Y ) ( and d0 =0 ) , and ui is the number of unique outcomes for that context seen in training .</sentence>
				<definiendum id="0">di</definiendum>
				<definiendum id="1">ui</definiendum>
				<definiens id="0">the number of unique outcomes for that context seen in training</definiens>
			</definition>
			<definition id="5">
				<sentence>Tree insertion grammar : a cubic-time parsable formalism that lexicalizes context-free grammar without changing the trees produced .</sentence>
				<definiendum id="0">Tree insertion grammar</definiendum>
			</definition>
</paper>

		<paper id="1118">
</paper>

		<paper id="1081">
			<definition id="0">
				<sentence>CallHome The CallHome ( CH ) corpus ( Linguistic Data Consortium , 1997 ) contains 80 dialogues of 10 minutes unconstrained conversation between two humans over the telephone .</sentence>
				<definiendum id="0">CallHome</definiendum>
				<definiendum id="1">CH ) corpus</definiendum>
				<definiens id="0">Linguistic Data Consortium , 1997 ) contains 80 dialogues of 10 minutes unconstrained conversation between two humans over the telephone</definiens>
			</definition>
			<definition id="1">
				<sentence>The STTS tagset consists of more than 50 different tags .</sentence>
				<definiendum id="0">STTS tagset</definiendum>
			</definition>
			<definition id="2">
				<sentence>The TABA corpus features nearly no interjections , while the number of numerals in the CH corpus is rather low ( in the other corpora times and dates were explicit elements of the tasks ) .</sentence>
				<definiendum id="0">TABA corpus</definiendum>
				<definiens id="0">features nearly no interjections , while the number of numerals in the CH corpus is rather low ( in the other corpora times and dates were explicit elements of the tasks )</definiens>
			</definition>
			<definition id="3">
				<sentence>Learning the user’s language : A step towards automated creation of user models .</sentence>
				<definiendum id="0">Learning the user’s language</definiendum>
				<definiens id="0">A step towards automated creation of user models</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>Rete : A fast algorithm for the many pattern / many object pattern match problem .</sentence>
				<definiendum id="0">Rete</definiendum>
				<definiens id="0">A fast algorithm for the many pattern / many object pattern match problem</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>`` An Unsupervised Iterative Method for Chinese New Lexicon Extraction '' , International Journal of Computational Linguistics &amp; Chinese Language Processing , 1997 .</sentence>
				<definiendum id="0">An Unsupervised Iterative Method</definiendum>
				<definiens id="0">for Chinese New Lexicon Extraction ''</definiens>
			</definition>
</paper>

		<paper id="1075">
			<definition id="0">
				<sentence>In this section , we briefly review a simple and intuitive approximation method for turning unificationbased grammars , such as HPSG ( Pollard and Sag , UBG CFG approximation a0a2a1a4a3a5a3a6a3a7a8a0a2a9a10a3a6a3a11a3a7 a0 a12 a3a5a3a6a3a7 . . . a0a13a1a4a3a5a3a6a3a7a14a0a15a9a16a3a5a3a6a3a7 a0a17a12 a3a6a3a5a3a7 a b c X S . . . a b c X S a b c X S . . . a b c X S . . . a b c X S . . . a b c X S Figure 1 : The readings of a sentence , analyzed by a UBG ( top ) and its CFG approximation ( bottom ) .</sentence>
				<definiendum id="0">b c X</definiendum>
			</definition>
			<definition id="1">
				<sentence>Baseline is the disambiguation accuracy of the symbolic approximated UBG .</sentence>
				<definiendum id="0">Baseline</definiendum>
				<definiens id="0">the disambiguation accuracy of the symbolic approximated UBG</definiens>
			</definition>
</paper>

		<paper id="2015">
			<definition id="0">
				<sentence>For these applications , we use alternative acquisition methods that communicate with the particular application using an application-specific protocol , which provides accurate text recognition .</sentence>
				<definiendum id="0">application-specific protocol</definiendum>
				<definiens id="0">provides accurate text recognition</definiens>
			</definition>
			<definition id="1">
				<sentence>From the aspect of dictionary development ( and even linguistic research ) , the comprehension assistant is an ideal source of linguistic information because it reaches a potentially large number of users ( since it is not a special application but a utility that has its place in every computing environment ) .</sentence>
				<definiendum id="0">comprehension assistant</definiendum>
				<definiens id="0">a special application but a utility that has its place in every computing environment )</definiens>
			</definition>
			<definition id="2">
				<sentence>The parser/translator engine ( called MetaMorpho ) is still under development .</sentence>
				<definiendum id="0">parser/translator engine</definiendum>
				<definiens id="0">still under development</definiens>
			</definition>
			<definition id="3">
				<sentence>Conclusion MoBiMouse is a context-sensitive instant comprehension tool that offers translations for words and expressions displayed on computer screens .</sentence>
				<definiendum id="0">Conclusion MoBiMouse</definiendum>
				<definiens id="0">a context-sensitive instant comprehension tool that offers translations for words and expressions displayed on computer screens</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>The TOEFL is an obligatory test for foreign students who would like to study at an American or English university .</sentence>
				<definiendum id="0">TOEFL</definiendum>
				<definiens id="0">an obligatory test for foreign students who would like to study at an American or English university</definiens>
			</definition>
			<definition id="1">
				<sentence>SVD , as described by Schütze ( 1997 ) and Landauer &amp; Dumais ( 1997 ) , is a method similar to factor analysis or multi-dimensional scaling that allows a significant reduction of the dimensionality of a matrix with minimum information loss .</sentence>
				<definiendum id="0">SVD</definiendum>
				<definiens id="0">a method similar to factor analysis or multi-dimensional scaling that allows a significant reduction of the dimensionality of a matrix with minimum information loss</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , if we consider the sentence “Peter drives the blue car” , then we should not count the co-occurrence of Peter and blue , because blue is neither head nor modifier of Peter .</sentence>
				<definiendum id="0">blue</definiendum>
			</definition>
</paper>

		<paper id="1110">
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>features The general form of the equation we use to find the frequency distribution of each feature of the verb is the following : P ( V j ) = C ( V j ) P 1 x N C ( Vx ) where P ( Vj ) is the distribution of feature j of the verb , N is the total number of features of the particular type ( e.g. , the total number of CAUS features or ANIM features as described below ) and C ( Vj ) is the number of times this feature of the verb was observed in the corpus .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the following : P ( V j ) = C ( V j</definiens>
				<definiens id="1">the distribution of feature j of the verb</definiens>
				<definiens id="2">the total number of features of the particular type</definiens>
				<definiens id="3">the number of times this feature of the verb was observed in the corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>Then , we find the frequency distribution by using the same formula as before : P ( V j ) = C ( V j ) P 1 x N C ( Vx ) where P ( Vj ) is the distribution of part of speech j , N is the total number of relevant POS features and C ( V j ) is the number of occurrences of part of speech j. Also , we limit the part of speech to only the following tags of speech : NNP , NNPS , EX , PRP , and SUCH , where NNP is singular noun phrase , NNPS is plural noun phrase , EX is ‘there’ , PRP is personal pronoun , and SUCH is ‘such’ .</sentence>
				<definiendum id="0">P ( Vj )</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">NNP</definiendum>
				<definiendum id="3">NNPS</definiendum>
				<definiendum id="4">EX</definiendum>
				<definiendum id="5">PRP</definiendum>
				<definiendum id="6">SUCH</definiendum>
				<definiens id="0">the distribution of part of speech j</definiens>
				<definiens id="1">the total number of relevant POS features and C ( V j ) is the number of occurrences of part of speech j. Also</definiens>
				<definiens id="2">NNP , NNPS , EX , PRP , and SUCH , where</definiens>
				<definiens id="3">singular noun phrase</definiens>
				<definiens id="4">plural noun phrase</definiens>
				<definiens id="5">personal pronoun , and</definiens>
			</definition>
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>The paraphrasing process aims to bridge the gap between the unrestricted expressions in the input and the limited expressions that the transfer can translate .</sentence>
				<definiendum id="0">paraphrasing process</definiendum>
				<definiens id="0">aims to bridge the gap between the unrestricted expressions in the input and the limited expressions that the transfer can translate</definiens>
			</definition>
			<definition id="1">
				<sentence>Paraphrasing is a process that automatically generates new expressions that have the same meaning as the input sentence .</sentence>
				<definiendum id="0">Paraphrasing</definiendum>
				<definiens id="0">a process that automatically generates new expressions that have the same meaning as the input sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>The tool consists of description symbols and a transformation program .</sentence>
				<definiendum id="0">tool</definiendum>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>Computation of modifier scope , revised than all other groups ; wider scope than other modifiers not covered in ( II.1 ) ; modifiers that are not relative clauses have 1 The special status of such adjectives has been noted in other contexts : For example , Hawkins ( 1978 ) groups same together with superlatives into a class of ‘unexplanatory’ modifiers ; Vieira and Poesio ( 2000 ) , extending this class to include only and a few others , make use of them in identifying discourse-new definite descriptions .</sentence>
				<definiendum id="0">Vieira</definiendum>
				<definiens id="0">same together with superlatives into a class of ‘unexplanatory’ modifiers ;</definiens>
			</definition>
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>11 C ( &lt; X &gt; → &lt; Y &gt; ) is the number of X which have been wrongly classified as Y. In ( 16 ) , P stands for the precision , and R for the recall .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">the number of X</definiens>
			</definition>
			<definition id="1">
				<sentence>( 18 ) Results of classification for single articles from WSJ F ( B ) F ( S ) E ( B ) E ( S ) WSJ_1 88.00 77.78 21.43 28.57 WSJ_2 83.87 100.00 27.78 0.00 WSJ_3 100.00 100.00 0.00 0.00 WSJ_4 81.82 97.30 30.77 3.85 WSJ_5 66.67 85.71 50.00 16.67 WSJ_6 89.66 96.30 18.75 6.25 WSJ_7 100.00 100.00 0.00 0.00 WSJ_8 88.00 90.00 21.43 14.29 WSJ_9 47.06 72.73 69.23 23.08 WSJ_10 83.33 100.00 28.57 0.00 µ 82.84 91.98 26.80 9.27 ( 19 ) Results of classification for single articles from NZZ F ( B ) F ( S ) E ( B ) E ( S ) NZZ_1 95.08 100.00 9.38 0.00 NZZ_2 93.02 97.56 13.04 4.35 NZZ_3 96.00 98.97 7.69 1.92 NZZ_4 96.15 100.00 7.41 0.00 NZZ_5 93.18 98.80 12.77 2.13 NZZ_6 96.84 98.92 6.12 2.04 NZZ_7 97.50 97.37 4.88 4.88 NZZ_8 89.66 100.00 18.75 0.00 NZZ_9 96.97 97.14 5.88 2.86 NZZ_10 93.94 99.71 11.43 0.29 µ 94.83 98.18 9.73 1.82 In general , the articles from NZZ contained fewer abbreviations , which is reflected in the comparatively high baseline scores .</sentence>
				<definiendum id="0">NZZ F ( B ) F ( S ) E ( B ) E ( S</definiendum>
				<definiens id="0">general , the articles from NZZ contained fewer abbreviations</definiens>
			</definition>
</paper>

		<paper id="1086">
			<definition id="0">
				<sentence>ci t i qi q ww q c cqsimC ⋅⋅= ∑ =1 ) , ( ( 2 ) where |q| is the number of terms in the query , |c q | is the number of query terms included in a cluster centroid , |c q |/|q| is the query inclusion ratio for the cluster .</sentence>
				<definiendum id="0">|q|</definiendum>
				<definiens id="0">the number of query terms included in a cluster centroid , |c q |/|q| is the query inclusion ratio for the cluster</definiens>
			</definition>
			<definition id="1">
				<sentence>Mutual information MI ( x , y ) is defined as following ( Church and Hanks , 1990 ) : ) ( ) ( ) , ( log ) ( ) ( ) , ( log ) , ( 22 yfxf yxfN ypxp yxp yxMI ⋅ == ( 4 ) where f ( x ) and f ( y ) are frequency of term x and term y , respectively .</sentence>
				<definiendum id="0">Mutual information MI</definiendum>
				<definiendum id="1">y )</definiendum>
				<definiendum id="2">f ( x</definiendum>
				<definiens id="0">following ( Church and Hanks , 1990 ) : ) ( ) ( ) , ( log ) ( ) ( ) , ( log )</definiens>
			</definition>
			<definition id="2">
				<sentence>Co-occurrence frequency of term x and term y , f ( x , y ) , is taken in window size 6 for AP 1988 news documents .</sentence>
				<definiendum id="0">Co-occurrence frequency</definiendum>
				<definiens id="0">of term x and term y , f ( x , y ) , is taken in window size 6 for AP 1988 news documents</definiens>
			</definition>
			<definition id="3">
				<sentence>Cluster C85 is a singleton whose centroid includes one of three query terms : bowl 0.101 marble 0.19 Since query inclusion ratio is low , the cluster preference is low .</sentence>
				<definiendum id="0">Cluster C85</definiendum>
				<definiens id="0">a singleton whose centroid includes one of three query terms</definiens>
			</definition>
</paper>

		<paper id="2006">
			<definition id="0">
				<sentence>Wehave therefore developed the Linguistic Annotation Language # 28orLAL # 29 , which is an XML-compliant tagset for assisting natural language processing programs , and NLP tools such as parsers and machine translation programs which can accept LAL-annotated input .</sentence>
				<definiendum id="0">NLP</definiendum>
				<definiens id="0">tools such as parsers and machine translation programs which can accept LAL-annotated input</definiens>
			</definition>
			<definition id="1">
				<sentence>LAL is an XML-compliant tag set and its XML namespace pre # 0Cx is lal .</sentence>
				<definiendum id="0">LAL</definiendum>
				<definiens id="0">an XML-compliant tag set</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>Statistical NERs usually find the sequence of tags that maximizes the probability a0a2a1a4a3a6a5a8a7a10a9 , where a7 is the sequence of words in a sentence , and a3 is the sequence of named-entity tags assigned to the words in a7 .</sentence>
				<definiendum id="0">Statistical NERs</definiendum>
				<definiendum id="1">a3</definiendum>
				<definiens id="0">the sequence of tags that maximizes the probability a0a2a1a4a3a6a5a8a7a10a9 , where a7 is the sequence of words in a sentence</definiens>
				<definiens id="1">the sequence of named-entity tags assigned to the words in a7</definiens>
			</definition>
			<definition id="1">
				<sentence>We propose maximizing a0a2a1a4a3a6a5a8a7a12a11a14a13a16a15a18a17a19a9 , where a3 is the sequence of namedentity tags assigned to the words in the sentence a7 , and a13a20a15a18a17 is the information that can be extracted from the whole document containing a7 .</sentence>
				<definiendum id="0">a3</definiendum>
				<definiendum id="1">a13a20a15a18a17</definiendum>
				<definiens id="0">the sequence of namedentity tags assigned to the words in the sentence a7 , and</definiens>
			</definition>
			<definition id="2">
				<sentence>MENE ( Maximum Entropy Named Entity ) ( Borthwick , 1999 ) was combined with Proteus ( a handcoded system ) , and came in fourth among all MUC7 participants .</sentence>
				<definiendum id="0">MENE ( Maximum Entropy Named Entity</definiendum>
			</definition>
			<definition id="3">
				<sentence>Among machine learning-based NERs , IdentiFinder has proven to be the best on the official MUC-6 and MUC-7 test data .</sentence>
				<definiendum id="0">IdentiFinder</definiendum>
				<definiens id="0">has proven to be the best on the official MUC-6 and MUC-7 test data</definiens>
			</definition>
			<definition id="4">
				<sentence>The probability of the classes a17 a14 a11a4a3a5a3a5a3 a11a14a17a7a6 assigned to the words in a sentence a8 in a document a13 is defined as follows : a0 a1 a17 a14 a11a5a3a4a3a5a3 a11 a17 a6 a5 a8 a11 a13a20a9a5a4 a6 a10 a1 a12a15a14 a0 a1 a17 a1 a5 a8 a11 a13a20a9a10a9 a0 a1a4a17 a1 a5a17 a1a12a11 a14 a9a19a11 where a0 a1a4a17 a1 a5a8 a11a14a13a16a9 is determined by the maximum entropy classifier .</sentence>
				<definiendum id="0">probability of the</definiendum>
				<definiens id="0">follows : a0 a1 a17 a14 a11a5a3a4a3a5a3 a11 a17 a6 a5 a8 a11 a13a20a9a5a4 a6 a10 a1 a12a15a14 a0 a1 a17 a1 a5 a8 a11 a13a20a9a10a9 a0 a1a4a17 a1 a5a17 a1a12a11 a14 a9a19a11 where a0 a1a4a17 a1 a5a8 a11a14a13a16a9 is determined by the maximum entropy classifier</definiens>
			</definition>
			<definition id="5">
				<sentence>The local features used are similar to those used in BBN 's IdentiFinder ( Bikel et al. , 1999 ) or MENE ( Borthwick , 1999 ) .</sentence>
				<definiendum id="0">MENE</definiendum>
				<definiens id="0">similar to those used in BBN 's IdentiFinder ( Bikel et al. , 1999 ) or</definiens>
			</definition>
			<definition id="6">
				<sentence>Case and Zone : If the token a13 starts with a capital letter ( initCaps ) , then an additional feature ( initCaps , zone ) is set to 1 .</sentence>
				<definiendum id="0">Case</definiendum>
				<definiendum id="1">Zone</definiendum>
				<definiendum id="2">initCaps</definiendum>
				<definiens id="0">If the token a13 starts with a capital letter</definiens>
			</definition>
			<definition id="7">
				<sentence>Token Information : This group consists of 10 features based on the string a13 , as listed in Table 1 .</sentence>
				<definiendum id="0">Token Information</definiendum>
				<definiens id="0">This group consists of 10 features based on the string a13</definiens>
			</definition>
			<definition id="8">
				<sentence>For MUC-6 , for example , CorporateSuffix-List is made up of a0 ltd. , associates , inc. , co , corp , ltd , inc , committee , institute , commission , university , plc , airlines , co. , corp.a1 and Person-PrefixList is made up of a0 succeeding , mr. , rep. , mrs. , secretary , sen. , says , minister , dr. , chairman , ms.a1 .</sentence>
				<definiendum id="0">CorporateSuffix-List</definiendum>
				<definiens id="0">associates , inc. , co , corp , ltd , inc , committee , institute , commission , university , plc , airlines</definiens>
				<definiens id="1">secretary , sen. , says , minister</definiens>
			</definition>
			<definition id="9">
				<sentence>The global feature groups are : InitCaps of Other Occurrences ( ICOC ) : There are 2 features in this group , checking for whether the first occurrence of the same word in an unambiguous position ( non first-words in the TXT or TEXT zones ) in the same document is initCaps or not-initCaps .</sentence>
				<definiendum id="0">ICOC</definiendum>
			</definition>
			<definition id="10">
				<sentence>In this case , News has an additional feature of I begin set to 1 , Broadcasting has an additional feature of I continue set to 1 , and Corp. has an additional feature of I end set to 1 .</sentence>
				<definiendum id="0">News</definiendum>
				<definiens id="0">has an additional feature of I begin set to 1 , Broadcasting has an additional feature of I continue set to 1 , and Corp. has an additional feature of I end set to 1</definiens>
			</definition>
			<definition id="11">
				<sentence>If a13 is unique , then a feature ( Unique , Zone ) is set to 1 , where Zone is the document zone where a13 appears .</sentence>
				<definiendum id="0">Zone</definiendum>
				<definiens id="0">the document zone where a13 appears</definiens>
			</definition>
</paper>

		<paper id="1060">
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>In this paper , however , it only covers requests to display charts , which takes the form a78a80a79a82a81a82a83a85a84a87a86a89a88a85a90a87a91a93a92a95a94 ListofVars , ListofAspectsa96 , where ListofVars is the list of variables plotted on the chart .</sentence>
				<definiendum id="0">ListofVars</definiendum>
				<definiens id="0">takes the form a78a80a79a82a81a82a83a85a84a87a86a89a88a85a90a87a91a93a92a95a94 ListofVars , ListofAspectsa96</definiens>
				<definiens id="1">the list of variables plotted on the chart</definiens>
			</definition>
			<definition id="1">
				<sentence>ListofAspects is the list of aspects of the data the user is focusing on and represents the perspectives from which she wants to look at the data .</sentence>
				<definiendum id="0">ListofAspects</definiendum>
			</definition>
			<definition id="2">
				<sentence>Descriptions describe constraints or conditions that the objects related to the action should satisfy , which has the form , a73 Quantifier , Var/Class , Restrictiona77 , where Quantifier is a generalized quantifier , Var is the variable of quantification , and the quantification ranges over the objects each of which is a member of Class and satisfies Restriction .</sentence>
				<definiendum id="0">Quantifier</definiendum>
				<definiendum id="1">Var</definiendum>
				<definiens id="0">has the form</definiens>
				<definiens id="1">a generalized quantifier ,</definiens>
			</definition>
			<definition id="3">
				<sentence>The second description states variable a150 ranges over objects with prefecture granularity that are subsumed by Shikoku , which is itself an object with district granularity .</sentence>
				<definiendum id="0">Shikoku</definiendum>
				<definiens id="0">itself an object with district granularity</definiens>
			</definition>
			<definition id="4">
				<sentence>Revisions of perspective are summarized as follows.4 a168 As a result of domain alteration on variable X , if the number of instances of X turns into more than one and the current perspective includes no aspect relating to X , that is , X is a uniquely instantiated variable , check a155a89a163 a91a108a161a104a164a80a79a93a165a85a84 a159 a163 a94 Xa96 , a163a104a166a80a156a108a157 a88a80a158a87a92a95a94 Xa96 , and a167 a166a80a156a108a157 a88a80a158a87a92a95a94 Xa96 in this order , and add the first possible one to the perspective .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a uniquely instantiated variable , check a155a89a163 a91a108a161a104a164a80a79a93a165a85a84 a159 a163 a94 Xa96</definiens>
			</definition>
			<definition id="5">
				<sentence>Postgraphe : A system for the generation of statistical graphics and text .</sentence>
				<definiendum id="0">Postgraphe</definiendum>
				<definiens id="0">A system for the generation of statistical graphics and text</definiens>
			</definition>
</paper>

		<paper id="1153">
			<definition id="0">
				<sentence>The XTAG Project ( Joshi , 2001 ) is an ongoing project at the University of Pennsylvania since about 1988 , aiming at the development of natural language resources based on Tree Adjoining Grammars ( TAGs ) ( Joshi and Schabes , 1997 ) .</sentence>
				<definiendum id="0">XTAG Project</definiendum>
			</definition>
			<definition id="1">
				<sentence>An LTAG is a set of lexicalized elementary trees that can be combined , through the operations of tree adjunction and tree substitution , to derive syntactic structures for sentences .</sentence>
				<definiendum id="0">LTAG</definiendum>
				<definiens id="0">a set of lexicalized elementary trees that can be combined , through the operations of tree adjunction and tree substitution , to derive syntactic structures for sentences</definiens>
			</definition>
			<definition id="2">
				<sentence>Ftag : A lexicalized Tree Adjoining Grammar for French .</sentence>
				<definiendum id="0">Ftag</definiendum>
			</definition>
			<definition id="3">
				<sentence>HyTAG : A new Type of Tree Adjoining Grammars for Hybrid Syntactic Representation of Free Word Order Lang uages .</sentence>
				<definiendum id="0">HyTAG</definiendum>
			</definition>
</paper>

		<paper id="1077">
</paper>

		<paper id="1144">
			<definition id="0">
				<sentence>The complexity of these algorithms is O ( n 2 logn ) , where n is the number of elements to be clustered ( Jain , Murty , Flynn 1999 ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of elements to be clustered ( Jain , Murty</definiens>
			</definition>
			<definition id="1">
				<sentence>Chameleon is a hierarchical algorithm that employs dynamic modeling to improve clustering quality ( Karypis , Han , Kumar 1999 ) .</sentence>
				<definiendum id="0">Chameleon</definiendum>
				<definiens id="0">a hierarchical algorithm that employs dynamic modeling to improve clustering quality</definiens>
			</definition>
			<definition id="2">
				<sentence>K-means clustering is often used on large data sets since its complexity is linear in n , the number of elements to be clustered .</sentence>
				<definiendum id="0">K-means clustering</definiendum>
			</definition>
			<definition id="3">
				<sentence>K-means is a family of partitional clustering algorithms that iteratively assigns each element to one of K clusters according to the centroid closest to it and recomputes the centroid of each cluster as the average of the cluster�s elements .</sentence>
				<definiendum id="0">K-means</definiendum>
				<definiens id="0">a family of partitional clustering algorithms that iteratively assigns each element to one of K clusters according to the centroid closest to it and recomputes the centroid of each cluster as the average of the cluster�s elements</definiens>
			</definition>
			<definition id="4">
				<sentence>Let c be a context and F c ( w ) be the frequency count of a word w occurring in context c. The pointwise mutual information between c and w is defined as : ( ) ( ) ( ) N jF N wF N wF cw j c i i c mi ∑ × ∑ = , where N = ( ) ∑∑ ij i jF is the total frequency counts of all words and their contexts .</sentence>
				<definiendum id="0">F c ( w )</definiendum>
				<definiendum id="1">jF</definiendum>
			</definition>
			<definition id="5">
				<sentence>Input : A list of elements E to be clustered , a similarity database S from Phase I , thresholds θ 1 and θ 2 .</sentence>
				<definiendum id="0">Input</definiendum>
				<definiens id="0">A list of elements E to be clustered</definiens>
			</definition>
			<definition id="6">
				<sentence>For each cluster discovered c compute the following score : |c| � avgsim ( c ) , where |c| is the number of elements in c and avgsim ( c ) is the average pairwise similarity between elements in c. Store the highest-scoring cluster in a list L. Step 2 : Sort the clusters in L in descending order of their scores .</sentence>
				<definiendum id="0">|c|</definiendum>
				<definiendum id="1">avgsim ( c )</definiendum>
				<definiens id="0">the number of elements in c and</definiens>
				<definiens id="1">the average pairwise similarity between elements in c. Store the highest-scoring cluster in a list L. Step 2 : Sort the clusters in L in descending order of their scores</definiens>
			</definition>
			<definition id="7">
				<sentence>An example of the first approach considers the average entropy of the clusters , which measures the purity of the clusters ( Steinbach , Karypis , and Kumar 2000 ) .</sentence>
				<definiendum id="0">average entropy of the clusters</definiendum>
			</definition>
			<definition id="8">
				<sentence>A ) The classes in the answer key ; B ) the clusters to be transformed ; C ) the sets used to reconstruct the classes ( Rule 1 ) ; D ) the sets after three merge operations ( Step 2 ) ; E ) the sets after one move operation ( Step 3 ) .</sentence>
				<definiendum id="0">B</definiendum>
			</definition>
			<definition id="9">
				<sentence>RANK MEMBERS TOP-15 FEATURES wn ( c ) 1 handgun , revolver , shotgun , pistol , rifle , machine gun , sawed-off shotgun , submachine gun , gun , automatic pistol , automatic rifle , firearm , carbine , ammunition , magnum , cartridge , automatic , stopwatch __ blast , barrel of __ , brandish __ , fire __ , point __ , pull out __ , __ discharge , __ fire , __ go off , arm with __ , fire with __ , kill with __ , open fire with __ , shoot with __ , threaten with __ artifact / artifact 236 whitefly , pest , aphid , fruit fly , termite , mosquito , cockroach , flea , beetle , killer bee , maggot , predator , mite , houseplant , cricket __ control , __ infestation , __ larvae , __ population , infestation of __ , specie of __ , swarm of __ , attract __ , breed __ , eat __ , eradicate __ , feed on __ , get rid of __ , repel __ , ward off __ animal / animate being / beast / brute / creature / fauna 471 supervision , discipline , oversight , control , governance , decision making , jurisdiction breakdown in __ , lack of __ , loss of __ , assume __ , exercise __ , exert __ , maintain __ , retain __ , seize __ , tighten __ , bring under __ , operate under __ , place under __ , put under __ , remain under __ act / human action / human activity 706 blend , mix , mixture , combination , juxtaposition , combine , amalgam , sprinkle , synthesis , hybrid , melange dip in __ , marinate in __ , pour in __ , stir in __ , use in __ , add to __ , pour __ , stir __ , curious __ , eclectic __ , ethnic __ , odd __ , potent __ , unique __ , unusual __ group / grouping 941 employee , client , patient , applicant , tenant , individual , participant , renter , volunteer , recipient , caller , internee , enrollee , giver benefit for __ , care for __ , housing for __ , benefit to __ , service to __ , filed by __ , paid by __ , use by __ , provide for __ , require for -- , give to __ , offer to __ , provide to __ , disgruntled __ , indigent __ worker We presented a clustering algorithm , CBC , for automatically discovering concepts from text .</sentence>
				<definiendum id="0">RANK MEMBERS TOP-15 FEATURES wn</definiendum>
				<definiens id="0">supervision , discipline , oversight , control , governance , decision making , jurisdiction breakdown in __ , lack of __ , loss of __ , assume __ , exercise __ , exert __ , maintain __ , retain __ , seize __ , tighten __ , bring under __ , operate under __ , place under __ , put under __ , remain under __ act / human action / human activity 706 blend , mix , mixture , combination , juxtaposition , combine , amalgam , sprinkle , synthesis</definiens>
				<definiens id="1">pour __ , stir __ , curious __ , eclectic __ , ethnic __ , odd __ , potent __ , unique __ , unusual __ group / grouping 941 employee , client , patient , applicant , tenant , individual , participant , renter , volunteer , recipient , caller , internee , enrollee</definiens>
			</definition>
			<definition id="10">
				<sentence>CBC white blood cell , red blood cell , brain cell , cell , blood cell , cancer cell , nerve cell , embryo , neuron K-means cadaver , meteorite , secretion , receptor , serum , handwriting , cancer cell , thyroid , body part , hemoglobin , red blood cell , nerve cell , urine , gene , chromosome , embryo , plasma , heart valve , saliva , ovary , white blood cell , intestine , lymph node , sperm , heart , colon , cell , blood , bowel , brain cell , central nervous system , spinal cord , blood cell , cornea , bladder , prostate , semen , brain , spleen , organ , nervous system , pancreas , tissue , marrow , liver , lung , marrow , kidney Buckshot cadaver , vagina , meteorite , human body , secretion , lining , handwriting , cancer cell , womb , vein , bloodstream , body part , eyesight , polyp , coronary artery , thyroid , membrane , red blood cell , plasma , gene , gland , embryo , saliva , nerve cell , chromosome , skin , white blood cell , ovary , sperm , uterus , blood , intestine , heart , spinal cord , cell , bowel , colon , blood vessel , lymph node , brain cell , central nervous system , blood cell , semen , cornea , prostate , organ , brain , bladder , spleen , nervous system , tissue , pancreas , marrow , liver , lung , bone marrow , kidney Bisecting K-means picket line , police academy , sphere of influence , bloodstream , trance , sandbox , downtown , mountain , camera , boutique , kitchen sink , kiln , embassy , cellblock , voting booth , drawer , cell , skylight , bookcase , cupboard , ballpark , roof , stadium , clubhouse , tub , bathtub , classroom , toilet , kitchen , bathroom , WordNet Class blood cell , brain cell , cancer cell , cell , cone , egg , nerve cell , neuron , red blood cell , rod , sperm , white blood cell Cutting , D. R. ; Karger , D. ; Pedersen , J. ; and Tukey , J. W. 1992 .</sentence>
				<definiendum id="0">plasma</definiendum>
				<definiens id="0">ovary , white blood cell , intestine , lymph node , sperm , heart , colon , cell , blood , bowel , brain cell , central nervous system , spinal cord , blood cell , cornea , bladder , prostate , semen , brain , spleen , organ , nervous system , pancreas , tissue , marrow</definiens>
				<definiens id="1">sphere of influence , bloodstream , trance , sandbox , downtown , mountain , camera , boutique , kitchen sink , kiln , embassy , cellblock , voting booth , drawer , cell , skylight , bookcase , cupboard , ballpark , roof , stadium , clubhouse , tub , bathtub , classroom , toilet , kitchen , bathroom</definiens>
			</definition>
			<definition id="11">
				<sentence>ROCK : A robust clustering algorithm for categorical attributes .</sentence>
				<definiendum id="0">ROCK</definiendum>
			</definition>
			<definition id="12">
				<sentence>WordNet : An Online Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1105">
</paper>

		<paper id="1106">
</paper>

		<paper id="1104">
			<definition id="0">
				<sentence>Chink/Chunk algorithm is a simple but efficient way to detect syntactic boundaries .</sentence>
				<definiendum id="0">Chink/Chunk algorithm</definiendum>
				<definiens id="0">a simple but efficient way to detect syntactic boundaries</definiens>
			</definition>
			<definition id="1">
				<sentence>It would have been interesting to compare them in terms of maximal complexity , but this is not actually possible because of an important difference between the two first parsers which are deterministic , and the last one which is not : for the first two techniques , the minimal , average and maximal complexities are polynomial , the deep parser has an exponential maximal complexity and polynomial minimal and average complexities .</sentence>
				<definiendum id="0">deep parser</definiendum>
				<definiens id="0">has an exponential maximal complexity and polynomial minimal and average complexities</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>Our approach to disambiguating T = R D consists rst of computing the composition T and thereafter to disambiguate the transducer T. We will give an important consequence of this result that allows us to compose any number of transducers R with the transducer D , in contrast to the previous approach which consisted in rst disambiguating transducers D and R to produce respectively D0 and R0 , then computing T0 = R0 D0 where T0 is unambiguous .</sentence>
				<definiendum id="0">T0</definiendum>
				<definiens id="0">rst of computing the composition T and thereafter to disambiguate the transducer T. We will give an important consequence of this result that allows us to compose any number of transducers R with the transducer D , in contrast to the previous approach which consisted in rst disambiguating transducers D and R to produce respectively</definiens>
			</definition>
			<definition id="1">
				<sentence>Transducer R is a mapping from phones to phones which implements phonological rules .</sentence>
				<definiendum id="0">Transducer R</definiendum>
				<definiens id="0">a mapping from phones to phones which implements phonological rules</definiens>
			</definition>
			<definition id="2">
				<sentence>Transducer M represents a language model : it converts sequences of words into sequences of words , while restricting the possible sequences or assigning a score to the sequences .</sentence>
				<definiendum id="0">Transducer M</definiendum>
				<definiens id="0">a language model : it converts sequences of words into sequences of words</definiens>
			</definition>
			<definition id="3">
				<sentence>The speech recognition problem consists of nding the path of least cost in transducer O T , where O is a sequence of acoustic observations .</sentence>
				<definiendum id="0">speech recognition problem</definiendum>
				<definiendum id="1">O</definiendum>
				<definiens id="0">a sequence of acoustic observations</definiens>
			</definition>
			<definition id="4">
				<sentence>de nitions Formally , a weighted transducer over a semiring K = ( K ; ; ; 0 ; 1 ) is de ned as a 6-tuple T = ( Q ; I ; 1 ; 2 ; E ; F ) where Q is a nite set of states , I Q is a nite set of initial states , 1 is the input alphabet , 2 is the output alphabet , E is a nite set of transitions and F Q is a nite set of nal states .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">E</definiendum>
				<definiendum id="2">F Q</definiendum>
				<definiens id="0">a nite set of states</definiens>
				<definiens id="1">a nite set of transitions</definiens>
				<definiens id="2">a nite set of nal states</definiens>
			</definition>
			<definition id="5">
				<sentence>A transition is an element of Q 1 Transitions are of the form t = ( p ( t ) ; i ( t ) ; o ( t ) ; n ( t ) ; w ( t ) ) ; t2E where p ( t ) denotes the transition’s origin state , i ( t ) its input label , o ( t ) its output label , n ( t ) the transition’s destination state and w ( t ) 2K is the weight of t. The tropical semiring de ned as ( R+ [ 1 ; min ; + ; 1 ; 0 ) is commonly used in speech recognition , but our results are applicable to the case of general semirings as well .</sentence>
				<definiendum id="0">transition</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">an element of Q 1 Transitions are of the form t = ( p ( t ) ; i ( t )</definiens>
			</definition>
			<definition id="6">
				<sentence>A path = t1 tn of T is an element of E verifying n ( ti 1 ) = p ( ti ) for 2 i n. We can easily extend the functions p and n to those paths : p ( ) = p ( t1 ) ; ( 1 ) n ( ) = n ( tn ) : ( 2 ) We denote by P ( r ; s ) the set of paths whose origin is state r and whose destination is state s. We can also extend the function P to the sets R Q and S Q : P ( R ; S ) = S r2R ; s2S P ( r ; s ) We can extend the functions i and o to the paths by taking the concatenations of the input and output symbols : i ( ) = i ( t1 ) i ( tn ) ; ( 3 ) o ( ) = o ( t1 ) o ( tn ) : ( 4 ) De nition 1 ( unambiguous transducer , ( Berstel , 1979 ) ) A transducer T is said to be unambiguous if for each w 2 1 , there exists at most one path in T such that i ( ) = w. De nition 2 ( ambiguous paths ) Two paths and are ambiguous if 6= and i ( ) = i ( ) .</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">A path = t1 tn of T is an element of E verifying n ( ti 1 ) = p ( ti ) for 2 i n. We can easily extend the functions p and n to those paths : p ( ) = p ( t1</definiens>
				<definiens id="1">the function P to the sets R Q and S Q : P ( R ; S ) = S r2R ; s2S P ( r ; s ) We can extend the functions i and o to the paths by taking the concatenations of the input and output symbols</definiens>
			</definition>
			<definition id="7">
				<sentence>Output : T1 = ( Q ; i ; X [ X1 ; Y ; ET ; F ) is an unambiguous transducer , X1 is the set of auxiliary symbols .</sentence>
				<definiendum id="0">X1</definiendum>
				<definiens id="0">an unambiguous transducer</definiens>
				<definiens id="1">the set of auxiliary symbols</definiens>
			</definition>
			<definition id="8">
				<sentence>The composition of R with S is a transducer R S = ( Q ; Q ; X ; Z ; E ; F ) de ned by : Let D = ( QD ; ID ; Y ; Z ; ED ; FD ) be a transducer verifying the fundamental property .</sentence>
				<definiendum id="0">FD</definiendum>
				<definiens id="0">a transducer R S = ( Q ; Q ; X</definiens>
			</definition>
			<definition id="9">
				<sentence>The set of transitions of D is de ned as E = E0 ] f ( f ; # ; x ; 0 ; w ) g where f is the unique nal state ofD , 0 is the unique initial state of D , x is any symbol and # is a symbol representing the end of a word .</sentence>
				<definiendum id="0">f</definiendum>
				<definiens id="0">a symbol representing the end of a word</definiens>
			</definition>
			<definition id="10">
				<sentence>The transducer R represents the phonological rule that handles liaison in the French language .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">the phonological rule that handles liaison in the French language</definiens>
			</definition>
</paper>

		<paper id="1090">
			<definition id="0">
				<sentence>It is defined by ) ) , ( ) , ( ) , ( ( minarg ) , ( crootcbcabalcs Nc δδδ ++= ∈ ( 1 ) where δ ( a , b ) describes the number of edges on the shortest path between a and b. The taxonomic similarity between a and b is then given by ( ) ) , ( ) , ( ) , ( ) , ( , crootcbca croot ba δδδ δ ++ =Τ ( 2 ) where c = lcs ( a , b ) .</sentence>
				<definiendum id="0">δ</definiendum>
				<definiendum id="1">c = lcs</definiendum>
				<definiens id="0">describes the number of edges on the shortest path between a and b. The taxonomic similarity between a</definiens>
			</definition>
</paper>

		<paper id="2024">
			<definition id="0">
				<sentence>This paper describes an indexing substrate for typed feature structures ( ISTFS ) , which is an efficient retrieval engine for typed feature structures .</sentence>
				<definiendum id="0">ISTFS</definiendum>
				<definiens id="0">an efficient retrieval engine for typed feature structures</definiens>
			</definition>
			<definition id="1">
				<sentence>This paper describes an indexing substrate for typed feature structures ( ISTFS ) , which is an efficient retrieval engine for typed feature structures ( TFSs ) ( Carpenter , 1992 ) .</sentence>
				<definiendum id="0">ISTFS</definiendum>
			</definition>
			<definition id="2">
				<sentence>Our ISTFS is an indexing substrate that enables such knowledge-based systems to keep and retrieve TFSs , which can represent symbolic structures such as quasi-logical forms or a taxonomy and the output of parsing of unification-based grammars for a very large set of documents .</sentence>
				<definiendum id="0">ISTFS</definiendum>
				<definiendum id="1">TFSs</definiendum>
				<definiens id="0">quasi-logical forms or a taxonomy and the output of parsing of unification-based grammars for a very large set of documents</definiens>
			</definition>
			<definition id="3">
				<sentence>The ISTFS checks unifiability by using dynamically determined paths , not statically determined paths .</sentence>
				<definiendum id="0">ISTFS</definiendum>
			</definition>
			<definition id="4">
				<sentence>It should also be noted that using all paths defined in a query TFS severely degrades the system performance because a TFS is a huge data structure comprised of hundreds of nodes and paths , i.e. , most of the retrieval time will be consumed in filtering .</sentence>
				<definiendum id="0">TFS</definiendum>
				<definiens id="0">a huge data structure comprised of hundreds of nodes and paths</definiens>
			</definition>
			<definition id="5">
				<sentence>The 1More precisely , FollowedType ( pi ; F ) returns the type assigned to the node reached by following pi from the root node of FSPAT H ( pi ; F ) , which is defined as follows .</sentence>
				<definiendum id="0">FollowedType ( pi ; F )</definiendum>
				<definiendum id="1">F )</definiendum>
				<definiens id="0">returns the type assigned to the node reached by following pi from the root node of FSPAT H ( pi ;</definiens>
			</definition>
			<definition id="6">
				<sentence>We define the path value table and the unifiability checking table as follows : Dpi ; σ · fFjF 2D ^ FollowedType ( pi ; F ) = σg Upi ; σ · ∑ τ ( τ2Type ^ σtτ is defined ) jDpi ; τj 2Type is a finite set of types .</sentence>
				<definiendum id="0">2Type</definiendum>
				<definiens id="0">the path value table and the unifiability checking table as follows : Dpi ; σ · fFjF 2D ^ FollowedType ( pi ; F ) = σg Upi ; σ · ∑ τ</definiens>
			</definition>
			<definition id="7">
				<sentence>Instead , the ISTFS finds the best paths by referring to Upi ; σ and calculates only U0pi ; σ where pi is the best index path .</sentence>
				<definiendum id="0">ISTFS</definiendum>
				<definiendum id="1">pi</definiendum>
				<definiens id="0">finds the best paths by referring to Upi</definiens>
			</definition>
			<definition id="8">
				<sentence>Such TFSs ( =U0pi ; σ ) can be collected by taking the union of Dpi ; τ , where τ is unifiable with σ .</sentence>
				<definiendum id="0">TFSs</definiendum>
				<definiendum id="1">σ )</definiendum>
				<definiens id="0">unifiable with σ</definiens>
			</definition>
</paper>

		<paper id="2008">
			<definition id="0">
				<sentence>Anontology is a set of descriptions ofconcepts such as things , events , and relations that are speciﬁed in some way ( such as speciﬁc natural language ) in order to create an agreedupon vocabulary for exchanging information .</sentence>
				<definiendum id="0">Anontology</definiendum>
				<definiens id="0">a set of descriptions ofconcepts such as things , events , and relations that are speciﬁed in some way ( such as speciﬁc natural language</definiens>
			</definition>
			<definition id="1">
				<sentence>Theselectedareaisautomatically assigned an XPointer ( i.e. , a location identiﬁer in the document ) ( World Wide Web Consortium , http : //www.w3.org/TR/xptr/ ) .</sentence>
				<definiendum id="0">XPointer</definiendum>
				<definiens id="0">a location identiﬁer in the document ) ( World Wide Web Consortium , http : //www.w3.org/TR/xptr/ )</definiens>
			</definition>
			<definition id="2">
				<sentence>Org stands for the node in the original document to be paraphrased by Def which represents the word sense deﬁnition node .</sentence>
				<definiendum id="0">Org</definiendum>
				<definiens id="0">the node in the original document to be paraphrased by Def which represents the word sense deﬁnition node</definiens>
			</definition>
			<definition id="3">
				<sentence>Wehavedescribedamethod , “Interactive Paraphrasing” , which enables users to interactively paraphrase words in a document by their deﬁnitions , making use of syntactic annotation and word sense annotation .</sentence>
				<definiendum id="0">Wehavedescribedamethod</definiendum>
				<definiendum id="1">“Interactive Paraphrasing”</definiendum>
				<definiens id="0">enables users to interactively paraphrase words in a document by their deﬁnitions , making use of syntactic annotation and word sense annotation</definiens>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>Of these , TDT , which is a long-term project , proposed many diverse applications , e.g. , story segmentation ( Greiff et al. , 2000 ) , topic tracking ( Levow et al. , 2000 ; Leek et al. , 2002 ) , topic detection ( Chen and Ku , 2002 ) and link detection ( Allan et al. , 2000 ) .</sentence>
				<definiendum id="0">TDT</definiendum>
				<definiens id="0">a long-term project , proposed many diverse applications , e.g. , story segmentation</definiens>
			</definition>
			<definition id="1">
				<sentence>Topic segmentation is a technique extensively utilized in information retrieval and automatic document summarization ( Hearst et al. , 1993 ; Nakao , 2001 ) .</sentence>
				<definiendum id="0">Topic segmentation</definiendum>
			</definition>
			<definition id="2">
				<sentence>We used the TDT2 corpus as training data , and evaluated the performance with the augmented version of TDT3 corpus .</sentence>
				<definiendum id="0">TDT2 corpus</definiendum>
				<definiens id="0">as training data , and evaluated the performance with the augmented version of TDT3 corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>The noun terms denote interesting entities such as people names , location names , and organization names , and so on .</sentence>
				<definiendum id="0">location</definiendum>
				<definiens id="0">names , and organization names</definiens>
			</definition>
			<definition id="4">
				<sentence>TextTiling subdivides text into multi-paragraph units that represent passages or subtopics .</sentence>
				<definiendum id="0">TextTiling</definiendum>
				<definiens id="0">subdivides text into multi-paragraph units that represent passages or subtopics</definiens>
			</definition>
			<definition id="5">
				<sentence>Strategy ( I ) is computing the similarity using the most similar passage pair .</sentence>
				<definiendum id="0">Strategy</definiendum>
				<definiens id="0">computing the similarity using the most similar passage pair</definiens>
			</definition>
			<definition id="6">
				<sentence>Strategy ( II ) is computing the similarity using passage-averaged similarity .</sentence>
				<definiendum id="0">Strategy</definiendum>
				<definiens id="0">computing the similarity using passage-averaged similarity</definiens>
			</definition>
			<definition id="7">
				<sentence>( E , E ) denotes an English pair ; ( C , C ) denotes a Chinese pair ; and ( C , E ) or ( E , C ) denotes a multilingual pair .</sentence>
				<definiendum id="0">E )</definiendum>
				<definiendum id="1">( C , C )</definiendum>
				<definiendum id="2">C )</definiendum>
				<definiens id="0">an English pair ;</definiens>
				<definiens id="1">a Chinese pair ; and ( C , E ) or ( E ,</definiens>
				<definiens id="2">a multilingual pair</definiens>
			</definition>
			<definition id="8">
				<sentence>w ( d , t e ) = w ( d , t c ) / N , where w ( d , t c ) is the weight of a Chinese term in story d , w ( d , t e ) is the weight of its English translation in story d , and N is the number of English translation candidates for the Chinese term .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the weight of its English translation in story d</definiens>
			</definition>
			<definition id="9">
				<sentence>In Table 9 , “One” denotes the weights of expanded terms are the same as the original ones , and “Half” denotes the weights of the expanded terms are only half of the original ones .</sentence>
				<definiendum id="0">“One”</definiendum>
				<definiendum id="1">“Half”</definiendum>
				<definiens id="0">the weights of expanded terms</definiens>
				<definiens id="1">the weights of the expanded terms</definiens>
			</definition>
			<definition id="10">
				<sentence>Levow G.A. and Oard D.W. ( 2000 ) Translingual Topic Detection : Applying Lessons from the MEI Project .</sentence>
				<definiendum id="0">Translingual Topic Detection</definiendum>
				<definiens id="0">Applying Lessons from the MEI Project</definiens>
			</definition>
</paper>

		<paper id="1097">
			<definition id="0">
				<sentence>A static sense vector is the centroid of context vectors of training samples where a target word is used as a certain sense ( section 2.3 ) .</sentence>
				<definiendum id="0">static sense vector</definiendum>
			</definition>
			<definition id="1">
				<sentence>Let W ij ( t k ) represent a weighting function for a term t k , which appears in the j th training sample for the i th sense , tf ijk 5 POS , collocations , semantic word associations , subcategorization information , semantic roles , selectional preferences and frequency of senses are useful for WSD ( Agirre et al. , 2001 ) .</sentence>
				<definiendum id="0">W ij</definiendum>
				<definiens id="0">subcategorization information , semantic roles , selectional preferences and frequency of senses are useful for WSD</definiens>
			</definition>
			<definition id="2">
				<sentence>ij ijk kij Z Z tW = ) ( ( 1 ) where ,         ×××= ik ik ijk ijkijk N N DF df D tfZ 1 ∑∑ == sensesall ikk sensesall i dfDFNN __ , ( ) ∑ = = termof k ijkij ZZ __ # 1 2 In formula ( 1 ) , Z is a normalization factor , which forces all values of W ij ( t k ) to fall into between 0 and 1 , inclusive ( Salton et al. , 1983 ) .</sentence>
				<definiendum id="0">Z</definiendum>
				<definiens id="0">a normalization factor , which forces all values of W ij ( t k ) to fall into between 0 and 1</definiens>
			</definition>
			<definition id="3">
				<sentence>, w ij ( t n ) ) where v ij represents a context vector of the j th training sample for the i th sense and w ij ( t k ) is the weight of a term t k calculated by formula ( 1 ) .</sentence>
				<definiendum id="0">w ij</definiendum>
				<definiendum id="1">v ij</definiendum>
				<definiendum id="2">w ij</definiendum>
			</definition>
			<definition id="4">
				<sentence>∑∑ ∑ == = = N i i N i i N i ii wv wv wvsim 1 2 1 2 1 ) , ( ( 4 ) where , N represents the dimension of the vector space , v and w represent vectors. )</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the dimension of the vector space</definiens>
			</definition>
			<definition id="5">
				<sentence>The precision rate is defined as the proportion of the correct answers to the generated results .</sentence>
				<definiendum id="0">precision rate</definiendum>
				<definiens id="0">the proportion of the correct answers to the generated results</definiens>
			</definition>
</paper>

		<paper id="1093">
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>The latter , P ( e ) , is the language model denoting the likelihood of the channel source text .</sentence>
				<definiendum id="0">P ( e )</definiendum>
			</definition>
			<definition id="1">
				<sentence>The IBM Model 4 , main focus in this paper , is composed of the following models ( see Figure 2 ) : Lexical Model t ( fje ) : Word-for-word translation model , representing the probability of a source word f being translated into a target word e. Fertility Model n ( je ) : Representing the probability of a source word e generating words .</sentence>
				<definiendum id="0">IBM Model</definiendum>
				<definiens id="0">Lexical Model t ( fje ) : Word-for-word translation model , representing the probability of a source word f being translated into a target word e. Fertility Model n ( je ) : Representing the probability of a source word e generating words</definiens>
			</definition>
			<definition id="2">
				<sentence>NULL Translation Model p1 : A xed probability of inserting a NULL word after determining each target word f .</sentence>
				<definiendum id="0">NULL Translation Model p1</definiendum>
				<definiens id="0">A xed probability of inserting a NULL word after determining each target word f</definiens>
			</definition>
			<definition id="3">
				<sentence>The algorithm is depicted in Algorithm 1 where C = fjk : k = 1 : : : jCjg represents a set of input string position 1 .</sentence>
				<definiendum id="0">jCjg</definiendum>
				<definiens id="0">a set of input string position 1</definiens>
			</definition>
			<definition id="4">
				<sentence>The e0 is a sequence of output word , consisting of a word with the fertility more than one ( translation of f j ) and other words with zero fertility .</sentence>
				<definiendum id="0">e0</definiendum>
				<definiens id="0">a sequence of output word , consisting of a word with the fertility more than one ( translation of f j</definiens>
			</definition>
			<definition id="5">
				<sentence>The computational complexity for the left-to-right and right-to-left is the same , O ( jEj3m22m ) , as reported by Tillmann and Ney ( 2000 ) , in which jEj is the size of the vocabulary for output sentences 3 .</sentence>
				<definiendum id="0">computational complexity</definiendum>
			</definition>
			<definition id="6">
				<sentence>The test set consists of 150 Japanese sentences varying by the sentence length of 6 , 8 and 10 .</sentence>
				<definiendum id="0">test set</definiendum>
				<definiens id="0">consists of 150 Japanese sentences varying by the sentence length of 6 , 8 and 10</definiens>
			</definition>
			<definition id="7">
				<sentence>The WER is the measure by penalizing insertion/deletion/replacement by 1 .</sentence>
				<definiendum id="0">WER</definiendum>
				<definiens id="0">the measure by penalizing insertion/deletion/replacement by 1</definiens>
			</definition>
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>Text representation is a central task for any approach to automatic learning from texts .</sentence>
				<definiendum id="0">Text representation</definiendum>
				<definiens id="0">a central task for any approach to automatic learning from texts</definiens>
			</definition>
			<definition id="1">
				<sentence>Text representation is a central task for approaches to text classification or categorization .</sentence>
				<definiendum id="0">Text representation</definiendum>
				<definiens id="0">a central task for approaches to text classification or categorization</definiens>
			</definition>
			<definition id="2">
				<sentence>It builds a lexical semantic space by modeling syntagmatic regularities with a correlation coefficient α : W → C ⊂ a82n and their differences with an Euclidean metric δ : C → S ⊂ a82n , where W is the set of words , C is called corpus space representing syntagmatic regularities , and S is called semantic space representing paradigmatic regularities .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">builds a lexical semantic space by modeling syntagmatic regularities with a correlation coefficient α : W → C ⊂ a82n and their differences with an Euclidean metric δ : C → S ⊂ a82n , where W is the set of words</definiens>
			</definition>
			<definition id="3">
				<sentence>M1 : In a second step , we use S as a format for representing meaning points of texts , which are mapped onto S with the help of a weighted mean of the meaning points assigned to their lexical constituents : vectorxk = summationdisplay ai∈W ( xk ) wikvectorai ∈ S ( 1 ) vectorxk is the meaning point of text xk ∈ C , vectorai the meaning point of word ai ∈ W , and W ( xk ) is the set of all types of all tokens in xk .</sentence>
				<definiendum id="0">W ( xk )</definiendum>
				<definiens id="0">a format for representing meaning points of texts , which are mapped onto S with the help of a weighted mean of the meaning points assigned to their lexical constituents</definiens>
			</definition>
			<definition id="4">
				<sentence>Let G = 〈V , E〉 be a graph and P = ( v1 , ... , vk ) a simple path in G. The path sensitive distance ˆδ ( P , y ) of y ∈ V with respect to P is defined as ˆδ ( P , y ) = 1 max ( δ ) summationdisplay vi∈V ( P ) ωiδ ( vectory , vectorvi ) ∈ [ 0,1 ] , wheresummationtextvi∈V ( P ) ωi ≤ 1 , max ( δ ) is the maximal value assumed by distance measure δ , and V ( P ) is the set of all nodes of path P. It is clear that for any of the text representation models M1 , M2 , M3 and their corresponding similarity measures we get different distance measures ˆδ which can be used to instantiate the order relations ≤2y in order to determine the end vertex of the path of minimal loss of cohesion when y is attached to it .</sentence>
				<definiendum id="0">V ( P )</definiendum>
				<definiens id="0">a graph and P = ( v1 , ... , vk ) a simple path in G. The path sensitive distance ˆδ ( P , y ) of y ∈ V with respect to P is defined as ˆδ ( P , y ) = 1 max ( δ ) summationdisplay vi∈V ( P ) ωiδ ( vectory , vectorvi ) ∈ [ 0,1 ] , wheresummationtextvi∈V ( P ) ωi ≤ 1</definiens>
				<definiens id="1">the maximal value assumed by distance measure δ</definiens>
				<definiens id="2">the set of all nodes of path P. It is clear that for any of the text representation models M1 , M2 , M3 and their corresponding similarity measures we get different distance measures ˆδ which can be used to instantiate the order relations ≤2y in order to determine the end vertex of the path of minimal loss of cohesion when y is attached to it</definiens>
			</definition>
			<definition id="5">
				<sentence>Furthermore , we observe that model M3 induces trees of highest cohesion and lowest variance , whereas VS shows the highest variance and lowest cohesion scores in case of STs and CTs .</sentence>
				<definiendum id="0">VS</definiendum>
				<definiens id="0">shows the highest variance and lowest cohesion scores in case of STs and CTs</definiens>
			</definition>
			<definition id="6">
				<sentence>In case of unsupervised clustering , where fine-grained class labels are missed , ( Steinbach et al. , 2000 ) propose a measure which estimates the overall cohesion of a cluster .</sentence>
				<definiendum id="0">fine-grained class labels</definiendum>
				<definiens id="0">Steinbach et al. , 2000 ) propose a measure which estimates the overall cohesion of a cluster</definiens>
			</definition>
			<definition id="7">
				<sentence>Moreover , even the stochastically organized so called random successor trees ( RST ) , in which successor node’s and their predecessors are randomly chosen , produce more cohesive structures than lists ( i.e. MDTs and MHTs ) , which form the predominant format used to organize search results in Internet .</sentence>
				<definiendum id="0">RST</definiendum>
				<definiendum id="1">lists</definiendum>
				<definiens id="0">form the predominant format used to organize search results in Internet</definiens>
			</definition>
</paper>

		<paper id="1163">
			<definition id="0">
				<sentence>All of the MT models are either those where the bilingual processor takes the initiative over the source language analyzer ( conventional analyze-transfer-generate model ) or integration models of analyzer and transfer , such as example-based or statistical models .</sentence>
				<definiendum id="0">bilingual processor</definiendum>
				<definiens id="0">takes the initiative over the source language analyzer ( conventional analyze-transfer-generate model ) or integration models of analyzer and transfer , such as example-based or statistical models</definiens>
			</definition>
			<definition id="1">
				<sentence>html process consists of two parts , i.e. , template retrieval and template matching .</sentence>
				<definiendum id="0">html process</definiendum>
				<definiens id="0">consists of two parts , i.e. , template retrieval and template matching</definiens>
			</definition>
			<definition id="2">
				<sentence>The transfer knowledge contains a bilingual dictionary of approximately 51,000 source language lexical entries , as well as up to 233,000 0 20 40 60 80 100 100 1k 10k 100k Output Performance ( % ) Transfer Knowledge ( templates ) partial whole no paraphrasing Figure 3 : Changes in output ratios by amount of transfer knowledge utterances , in the domain of travel situations , and their translations .</sentence>
				<definiendum id="0">transfer knowledge</definiendum>
				<definiens id="0">contains a bilingual dictionary of approximately 51,000 source language lexical entries , as well as up to 233,000 0 20 40 60 80 100 100 1k 10k 100k Output Performance ( % ) Transfer Knowledge ( templates ) partial whole no paraphrasing Figure 3 : Changes in output ratios by amount of transfer knowledge utterances , in the domain of travel situations , and their translations</definiens>
			</definition>
</paper>

		<paper id="1119">
</paper>

		<paper id="1100">
			<definition id="0">
				<sentence>Given a strict feature structure F and a default feature structure G , default unification is defined as unification that satisfies the following ( written as F &lt; t G ) : 1 ) It is always defined. 2 ) All strict information is preserved. That is , F v ( F &lt; t G ) . 3 ) It reduces to standard unification in the case of F and G being consistent. That is , ( F &lt; t G ) = ( F tG ) if F tG is defined. With these definitions , Douglas’ relaxation technique can be regarded as a sort of default unification. They classify constraints into necessary constraints and optional constraints , which can be regarded as strict information and default information in the definition of default unification. Carpenter ( 1993 ) gave concise and comprehensive definitions of default unification. However , the problem in Carpenter’s default unification is that it tries to maximize the amount of information in a default feature structure , not the result of default unification. Consider the case where a grammar rule is the default feature structure and the daughters are the strict feature structure. The head feature principle can be described as the structure-sharing between the values of the head feature in a mother and in a head daughter. The set of constraints that represent the head feature principle consists of only one element. When we lose just one element in the head feature principle , a large amount of information in the daughter’s substructure is not propagated to its mother. As Copestake ( 1993 ) mentioned , another problem in Carpenter’s default unification is that the time complexity for finding the optimal answer of default unification is exponential because we have to verify the unifiability of the power set of constraints in a default feature structure. Here , we propose ideal lenient default unification , which tries to maximize the amount of information of a result , not the amount of default information. Thus , the problem of losing a large amount of information in structure-sharing never arises. We also propose lenient default unification whose algorithm is much more efficient than the ideal one. Its time complexity is linear to the size of the strict feature structure and the default feature structure. Instead , the amount of information of a result derived by lenient default unification is equal to or less than that of the ideal one. We apply lenient default unification to robust processing. Given an HPSG grammar , our approach takes two steps ; i ) extraction of grammar rules from the results of robust parsing using lenient default unification for applying the HPSG grammar rules ( offline parsing ) , and ii ) runtime parsing using the HPSG grammar with the extracted rules. The extracted rules work robustly since they reflect the effects of recovery rules applied during offline robust parsing and the conditions in which they are applied. Sections 3 and 4 describe our default unification. Our robust parsing is explained in Section 5. Section 6 shows a series of experiments of robust parsing with default unification. Default unification has been investigated by many researchers ( Bouma , 1990 ; Russell et al. , 1991 ; Copestake , 1993 ; Carpenter , 1993 ; Lascarides and Copestake , 1999 ) in the context of developing lexical semantics. Here , we first explain the definition given by Carpenter ( 1993 ) because his definition is both concise and comprehensive. Carpenter proposed two types of default unification , credulous default unification and skeptical default unification. ( Credulous Default Unification ) F &lt; tc G = n F tG0 flfl fl G0 v G is maximal such thatF tG0 is defined o ( Skeptical Default Unification ) F &lt; ts G = F ( F &lt; tc G ) F is called a strict feature structure , whose information must not be lost , and G is called a default feature structure , whose information might be lost but as little as possible so that F and G can be unified. A credulous default unification operation is greedy in that it tries to maximize the amount of information it retains from the default feature structure. This definition returns a set of feature structures rather than a unique feature structure. Skeptical default unification simply generalizes the set of feature structures which results from credulous default unification. The definition of skeptical default unification leads to a unique result. The default information which can be found in every result of credulous default unification remains. Following is an example of skeptical default unification. [ F : a ] &lt; ts `` F : 1 b G : 1H : c # =u ( • F : a G : bH : c ‚ ; `` F : 1 a G : 1H : c # ) = •F : a G : ? H : c ‚ Forced unification is another way to unify inconsistent feature structures. Forced unification always succeeds by supposing the existence of the top type ( the most specific type ) in a type hierarchy. Unification of any pair of types is defined in the type hierarchy , and therefore unification of any pair of feature structures is defined. One example is described by Imaichi and Matsumoto ( 1995 ) ( they call it costbased unification ) . Their unification always succeeds by supposing the top type , and it also keeps the information about inconsistent types. Forced unification can be regarded as one of the toughest robust processing because it always succeeds and never loses the information embedded in feature structures. The drawback of forced unification is the postprocessing of parsing , i.e. , feature structures with top types are not tractable. We write Ftf G for the forced unification of F and G. In this section , we explain our default unification , ideal lenient default unification. Ideal lenient default unification tries to maximize the amount of information of the result , subsuming the result of forced unification. In other words , ideal lenient default unification tries to generate a result as similar as possible to the result of forced unification such that the result is defined in the type hierarchy without the top type. Formally , we have : Definition 3.1 Ideal Lenient Default Unification F &lt; ti G = F ( F tG0 flfl flfl fl G0 vf ( Ftf G ) is maximal such that F tG0 is definedwithout the top type ) where vf is a subsumption relation where the top type is defined. From the definition of skeptical default unification , ideal lenient default unification is equivalent to F &lt; ts ( Ftf G ) assuming that skeptical default unification does not add the default information that includes the top type to the strict information. Consider the following feature structures. F = 2 66 4 F : •F : a G : bH : c ‚ G : •F : a G : aH : c ‚ 3 77 5 ; G = hF : 1 G : 1 i In the case of Carpenter’s default unification , the results of skeptical and credulous default unification become as follows : F &lt; ts G = F ; F &lt; tc G =fFg. This is because G is generalized to the bottom feature structure , and hence the result is equivalent to the strict feature structure. With ideal lenient default unification , the result becomes as follows. F &lt; ti G = 2 66 66 4 F : `` F : 1 a G : b H : 2 c # G : `` F : 1G : a H : 2 # 3 77 77 5 vf 2 4F : 1 •F : a G : &gt; H : c ‚ G : 1 3 5 Note that the result of ideal lenient default unification subsumes the result of forced unification .</sentence>
				<definiendum id="0">Douglas’ relaxation technique</definiendum>
				<definiens id="0">Given a strict feature structure F and a default feature structure G , default unification is defined as unification that satisfies the following ( written as F &lt; t G ) : 1 ) It is always defined. 2 ) All strict information is preserved. That is , F v ( F &lt; t G ) . 3 ) It reduces to standard unification in the case of F and G being consistent. That is , ( F &lt; t G ) = ( F tG ) if F tG is defined. With these definitions</definiens>
				<definiens id="1">a sort of default unification. They classify constraints into necessary constraints and optional constraints , which can be regarded as strict information and default information in the definition of default unification. Carpenter ( 1993 ) gave concise and comprehensive definitions of default unification. However , the problem in Carpenter’s default unification is that it tries to maximize the amount of information in a default feature structure , not the result of default unification. Consider the case where a grammar rule is the default feature structure and the daughters are the strict feature structure. The head feature principle can be described as the structure-sharing between the values of the head feature in a mother and in a head daughter. The set of constraints that represent the head feature principle consists of only one element. When we lose just one element in the head feature principle , a large amount of information in the daughter’s substructure is not propagated to its mother. As Copestake ( 1993 ) mentioned , another problem in Carpenter’s default unification is that the time complexity for finding the optimal answer of default unification is exponential because we have to verify the unifiability of the power set of constraints in a default feature structure. Here , we propose ideal lenient default unification , which tries to maximize the amount of information of a result , not the amount of default information. Thus , the problem of losing a large amount of information in structure-sharing never arises. We also propose lenient default unification whose algorithm is much more efficient than the ideal one. Its time complexity is linear to the size of the strict feature structure and the default feature structure. Instead , the amount of information of a result derived by lenient default unification is equal to or less than that of the ideal one. We apply lenient default unification to robust processing. Given an HPSG grammar , our approach takes two steps ; i ) extraction of grammar rules from the results of robust parsing using lenient default unification for applying the HPSG grammar rules ( offline parsing ) , and ii ) runtime parsing using the HPSG grammar with the extracted rules. The extracted rules work robustly since they reflect the effects of recovery rules applied during offline robust parsing and the conditions in which they are applied. Sections 3 and 4 describe our default unification. Our robust parsing is explained in Section 5. Section 6 shows a series of experiments of robust parsing with default unification. Default unification has been investigated by many researchers ( Bouma , 1990 ; Russell et al. , 1991 ; Copestake , 1993 ; Carpenter , 1993 ; Lascarides and Copestake , 1999 ) in the context of developing lexical semantics. Here , we first explain the definition given by Carpenter ( 1993 ) because his definition is both concise and comprehensive. Carpenter proposed two types of default unification , credulous default unification and skeptical default unification. ( Credulous Default Unification ) F &lt; tc G = n F tG0 flfl fl G0 v G is maximal such thatF tG0 is defined o ( Skeptical Default Unification ) F &lt; ts G = F ( F &lt; tc G ) F is called a strict feature structure , whose information must not be lost , and G is called a default feature structure , whose information might be lost but as little as possible so that F and G can be unified. A credulous default unification operation is greedy in that it tries to maximize the amount of information it retains from the default feature structure. This definition returns a set of feature structures rather than a unique feature structure. Skeptical default unification simply generalizes the set of feature structures which results from credulous default unification. The definition of skeptical default unification leads to a unique result. The default information which can be found in every result of credulous default unification remains. Following is an example of skeptical default unification. [ F : a ] &lt; ts `` F : 1 b G : 1H : c # =u ( • F : a G : bH : c ‚ ; `` F : 1 a G : 1H : c # ) = •F : a G : ? H : c ‚ Forced unification is another way to unify inconsistent feature structures. Forced unification always succeeds by supposing the existence of the top type ( the most specific type ) in a type hierarchy. Unification of any pair of types is defined in the type hierarchy , and therefore unification of any pair of feature structures is defined. One example is described by Imaichi and Matsumoto ( 1995 ) ( they call it costbased unification ) . Their unification always succeeds by supposing the top type , and it also keeps the information about inconsistent types. Forced unification can be regarded as one of the toughest robust processing because it always succeeds and never loses the information embedded in feature structures. The drawback of forced unification is the postprocessing of parsing , i.e. , feature structures with top types are not tractable. We write Ftf G for the forced unification of F and G. In this section , we explain our default unification , ideal lenient default unification. Ideal lenient default unification tries to maximize the amount of information of the result , subsuming the result of forced unification. In other words , ideal lenient default unification tries to generate a result as similar as possible to the result of forced unification such that the result is defined in the type hierarchy without the top type. Formally , we have : Definition 3.1 Ideal Lenient Default Unification F &lt; ti G = F ( F tG0 flfl flfl fl G0 vf ( Ftf G ) is maximal such that F tG0 is definedwithout the top type ) where vf is a subsumption relation where the top type is defined. From the definition of skeptical default unification , ideal lenient default unification is equivalent to F &lt; ts ( Ftf G ) assuming that skeptical default unification does not add the default information that includes the top type to the strict information. Consider the following feature structures. F = 2 66 4 F : •F : a G : bH : c ‚ G : •F : a G : aH : c ‚ 3 77 5 ; G = hF : 1 G : 1 i In the case of Carpenter’s default unification , the results of skeptical and credulous default unification become as follows : F &lt; ts G = F ; F &lt; tc G =fFg. This is because G is generalized to the bottom feature structure , and hence the result is equivalent to the strict feature structure. With ideal lenient default unification , the result becomes as follows. F &lt; ti G = 2 66 66 4 F : `` F : 1 a G : b H : 2 c # G : `` F : 1G : a H : 2 # 3 77 77 5 vf 2 4F : 1 •F : a G : &gt; H : c ‚ G : 1 3 5 Note that the result of ideal lenient default unification subsumes the result of forced unification</definiens>
			</definition>
			<definition id="1">
				<sentence>Given a strict feature structure F and a default feature structure G , let H be the result of forced unification , i.e. , H = Ftf G. We define topnode ( H ) as a function that returns the fail points ( the nodes that are assigned the top type in H ) , f pnode ( H ) as a function that returns the fail path nodes ( the nodes from which a fail point can be reached ) , and f pchild ( H ) as a a function that returns all the nodes that are not fail path nodes but the immediate children of fail path nodes .</sentence>
				<definiendum id="0">H</definiendum>
				<definiendum id="1">fail path nodes</definiendum>
				<definiendum id="2">f pchild ( H</definiendum>
				<definiens id="0">a function that returns the fail points</definiens>
			</definition>
			<definition id="2">
				<sentence>The basic ideas are that i ) the inconsistency caused by path value specifications can be removed by generalizing the types assigned to the fail points in H , and that ii ) the inconsistency caused by path equivalence specifications can be removed by unfolding the structure-sharing of fail path nodes in H. Let H be hQH ; ¯qH ; θH ; δHi , where Q is a set of a feature structure’s nodes , ¯q is the root node , θ ( q ) is a total node typing function , and δ ( pi ; q ) is a partial function that returns a node reached by following path pi from q. We first give several definitions to define a generalized feature structure .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">¯q</definiendum>
				<definiendum id="2">δ</definiendum>
				<definiens id="0">a set of a feature structure’s nodes ,</definiens>
				<definiens id="1">the root node</definiens>
				<definiens id="2">a total node typing function</definiens>
			</definition>
			<definition id="3">
				<sentence>Offline parsing is a training phase to extract grammar rules , and runtime parsing is a phase where we apply the extracted rules to practice .</sentence>
				<definiendum id="0">Offline parsing</definiendum>
				<definiendum id="1">runtime parsing</definiendum>
				<definiens id="0">a training phase to extract grammar rules</definiens>
			</definition>
			<definition id="4">
				<sentence>The training corpus consists of 5,903 sentences selected from the Wall Street Journal ( Wall Street Journal 00 – 02 ) , and we prepared two sets of test corpora , TestSetA and TestSetB .</sentence>
				<definiendum id="0">training corpus</definiendum>
				<definiens id="0">consists of 5,903 sentences selected from the Wall Street Journal ( Wall Street Journal 00 – 02 ) , and we prepared two sets of test corpora , TestSetA and TestSetB</definiens>
			</definition>
			<definition id="5">
				<sentence>TestSetA consists of 1,480 sentences ( Wall Street Journal 03 ) and is used for measuring coverage.5 TestSetB consists of 100 sentences and is used for measuring the degree of overgeneration .</sentence>
				<definiendum id="0">TestSetA</definiendum>
				<definiens id="0">consists of 1,480 sentences ( Wall Street Journal 03 ) and is used for measuring coverage.5 TestSetB consists of 100 sentences and is used for measuring the degree of overgeneration</definiens>
			</definition>
			<definition id="6">
				<sentence>Phenomena ( A ) ( B ) ( C ) ( D ) ( % ) lack of lexical entry 118 32 86 72.9 inconsistency between XHPSG and Penn Treebank 44 13 31 70.5 punctuation , quotation , parenthesis 36 15 21 58.3 coordination 21 8 13 61.9 apposition 16 6 10 62.5 compound noun/adjective/adverb 14 3 11 78.6 Adv modifying PP 12 2 10 83.3 relative clause 12 5 7 58.3 topicalization 11 0 11 100.0 noun modifier 10 2 8 80.0 omission 8 2 6 75.0 parenthetical expression 7 3 4 57.1 verb saying 7 1 6 85.7 expression of frequency , NP + a + N 7 0 7 100.0 present participle construction 6 3 3 50.0 idiom 5 2 3 60.0 violation of agreement 3 0 3 100.0 adverbial noun 3 0 3 100.0 present progressive , be + Adv + present progressive 3 1 2 66.7 sentence modification from the beginning of a sentence 2 0 2 100.0 be + complement sentence 2 0 2 100.0 nominalization of adjective 1 0 1 100.0 double numerals ( NP + roughly + double + NP ) 1 1 0 0.0 total 349 99 250 71.6 ( A ) ... frequency of phenomena that the XHPSG grammar fails to analyze ( B ) ... frequency of phenomena that the XHPSG grammar with the extracted rules fails to analyze ( C ) = ( A ) ¡ ( B ) ... frequency of phenomena that can not be analyzed by the XHPSG grammar but can be analyzed by the XHPSG grammar with the extracted rules ( D ) = ( C ) = ( A ) ... the ratio of phenomena that are recovered Table 2 : Analysis of phenomena that are recovered which is a translation into HPSG of the manuallydeveloped XTAG English grammar ( The XTAG Research Group , 1995 ) .</sentence>
				<definiendum id="0">Phenomena</definiendum>
				<definiendum id="1">XHPSG grammar</definiendum>
				<definiens id="0">A ) ( B ) ( C )</definiens>
				<definiens id="1">fails to analyze ( B ) ... frequency of phenomena that the XHPSG grammar with the extracted rules fails to analyze ( C ) = ( A ) ¡ ( B ) ... frequency of phenomena that can not be analyzed by the XHPSG grammar but can be analyzed by the XHPSG grammar with the extracted rules ( D ) = ( C ) = ( A ) ... the ratio of phenomena that</definiens>
			</definition>
</paper>

		<paper id="2009">
			<definition id="0">
				<sentence>The document planning module consists of two tasks : `document structuring ' and `content determination ' .</sentence>
				<definiendum id="0">document planning module</definiendum>
				<definiens id="0">consists of two tasks : `document structuring ' and `content determination '</definiens>
			</definition>
			<definition id="1">
				<sentence>XML is useful especially for content determination from a hierarchical structured database .</sentence>
				<definiendum id="0">XML</definiendum>
				<definiens id="0">useful especially for content determination from a hierarchical structured database</definiens>
			</definition>
			<definition id="2">
				<sentence>createTextNode ( item1 ) ) ; ; ... .. return pc ; ; ... .. B The DocumentPlanDTD Example &lt; ? xml version= '' 1.0 '' encoding= '' Shift_JIS '' ? &gt; &lt; ! ELEMENT Set ( EconomyEvent+ ) &gt; &lt; ! ATTLIST Set Year NMTOKEN # REQUIRED &gt; &lt; ! ATTLIST Set Object NMTOKEN # REQUIRED &gt; &lt; ! ATTLIST Set Month NMTOKEN # REQUIRED &gt; &lt; ! ELEMENT EconomyEvent ( PersonalConsumption , Wages , HousingConstruction , PlantandEquipmentInvestment , MiningIndustory ? , EmploymentState ? , Bankruptcy ? , Export ? , Import ? , BalanceofPayments ? , DomesticWholesalePricesSituation ? , BargainPricesforEnterpriseSituation ? , ConsumerPricesSituation ? , FinancialSituation ? , MoneySupply ? ) &gt; &lt; ! ATTLIST EconomyEvent Type NMTOKEN # REQUIRED &gt; &lt; ! ELEMENT PersonalConsumption ( LivingExpenditures+ , LivingExpendituresforWorkers , LevelofConsumption , LevelofConsumptionforWorkers ) &gt; &lt; ! ELEMENT LivingExpendituresforWorkers ( # PCDATA ) &gt; &lt; ! ATTLIST LivingExpendituresforWorkers Month NMTOKEN # REQUIRED &gt; &lt; ! ATTLIST LivingExpendituresforWorkers ComparedTo NMTOKEN # REQUIRED &gt; ... .. C The Text Speci cation DTD Example &lt; ? xml version= '' 1.0 '' encoding= '' Shift_JIS '' ? &gt; &lt; ! ELEMENT Set ( EconomyEvent+ ) &gt; &lt; ! ATTLIST Set Year NMTOKEN # REQUIRED &gt; &lt; ! ATTLIST Set Object NMTOKEN # REQUIRED &gt; &lt; ! ATTLIST Set Time NMTOKEN # REQUIRED &gt; &lt; ! ELEMENT EconomyEvent ( Heading , SubHeading+ ) &gt; &lt; ! ELEMENT Heading ( # PCDATA ) &gt; &lt; ! ELEMENT SubHeading ( Phrase+ ) &gt; &lt; ! ATTLIST SubHeading Title CDATA # REQUIRED &gt; &lt; ! ELEMENT Phrase ( # PCDATA ) &gt; &lt; ! ATTLIST Phrase Use CDATA # IMPLIED &gt; &lt; ! ATTLIST Phrase Class NMTOKEN # IMPLIED &gt; &lt; ! ATTLIST Phrase Head ( True ) # IMPLIED &gt; &lt; ! ATTLIST Phrase Post CDATA # IMPLIED &gt; &lt; ! ATTLIST Phrase Household NMTOKEN # IMPLIED &gt; &lt; ! ATTLIST Phrase Sbj CDATA # REQUIRED &gt; &lt; ! ATTLIST Phrase Rhetoric ( Sequence | Embed ) # IMPLIED &gt; &lt; ! ATTLIST Phrase Product CDATA # IMPLIED &gt; &lt; ! ATTLIST Phrase Time ( September | May | August | October | July ) # REQUIRED &gt; &lt; ! ATTLIST Phrase Prep CDATA # IMPLIED &gt; D Monthly Economic Report D.1 English Output Example Personal Consumption Living expenditures�whole�for July decreased 2.6 �compared to the same period last year , and for August a 4.1�decrease compared to the same period last year .</sentence>
				<definiendum id="0">createTextNode</definiendum>
				<definiendum id="1">ELEMENT LivingExpendituresforWorkers</definiendum>
				<definiens id="0">Sequence | Embed ) # IMPLIED &gt; &lt; ! ATTLIST Phrase Product CDATA # IMPLIED &gt; &lt; ! ATTLIST Phrase Time ( September | May | August | October | July</definiens>
			</definition>
</paper>

		<paper id="1156">
			<definition id="0">
				<sentence>FrameNet ( Fillmore et al. , 2001 ) is an online lexical resource1 designed according to the principles of frame semantics ( Fillmore , 1985 ; Petruck , 1996 ) .</sentence>
				<definiendum id="0">FrameNet</definiendum>
				<definiens id="0">an online lexical resource1 designed according to the principles of frame semantics</definiens>
			</definition>
			<definition id="1">
				<sentence>FE tags act as a shorthand that allows diverse verbs to tap into a common subset of encyclopedic knowledge .</sentence>
				<definiendum id="0">FE tags</definiendum>
				<definiens id="0">act as a shorthand that allows diverse verbs to tap into a common subset of encyclopedic knowledge</definiens>
			</definition>
			<definition id="2">
				<sentence>ECG is a constraintbased formalism similar in many respects to other unification-based linguistic formalisms , such as HPSG ( Pollard and Sag , 1994 ) .4 It differs from other lingustically motivated proposals in that it is 4ECG includes formalisms for both schemas ( conceptual representations ) and constructions ( conventionalized pairings of form and meaning ) , described in ( Bergen and Chang , 2002 ) .</sentence>
				<definiendum id="0">ECG</definiendum>
				<definiens id="0">conceptual representations ) and constructions ( conventionalized pairings of form and meaning</definiens>
			</definition>
			<definition id="3">
				<sentence>The PropBank project ( Kingsbury and Palmer , 2002 ) is , like FrameNet , geared toward the creation of a semantically annotated corpus ( by adding general logical predicates to the Penn Treebank ) , though without any common background frame structures across lexical items .</sentence>
				<definiendum id="0">PropBank project</definiendum>
				<definiens id="0">geared toward the creation of a semantically annotated corpus ( by adding general logical predicates to the Penn Treebank</definiens>
			</definition>
			<definition id="4">
				<sentence>WordNet : An Electronic Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1099">
			<definition id="0">
				<sentence>8 C4.5 is one of the popular method for recognizing boundary of chunks .</sentence>
				<definiendum id="0">C4.5</definiendum>
			</definition>
			<definition id="1">
				<sentence>Test set II ( Kang et al. , 2000 ) consists of 7,185 E-K pairs – the number of training data is 6,185 and that of test data is 1,000 .</sentence>
				<definiendum id="0">Test set II</definiendum>
			</definition>
			<definition id="2">
				<sentence>Evaluation is performed by word accuracy ( W. A. ) and character accuracy ( C.A. ) , which were used as the evaluation measure in the previous works ( Lee and Choi 1998 ; Kim and Choi 1999 ; Kang and Choi 2000 ) .</sentence>
				<definiendum id="0">Evaluation</definiendum>
			</definition>
			<definition id="3">
				<sentence>++− = ( 9 ) where L represents the length of the original string , and di , , and s represent the number of insertion , deletion and substitution respectively .</sentence>
				<definiendum id="0">L</definiendum>
				<definiens id="0">the length of the original string</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>Named Entity ( NE ) recognition is a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person , organization , and date .</sentence>
				<definiendum id="0">Entity ( NE ) recognition</definiendum>
				<definiens id="0">a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person , organization , and date</definiens>
			</definition>
			<definition id="1">
				<sentence>Named Entity ( NE ) recognition is a task in which proper nouns and numerical information in a document are detected and classified into categories such as person , organization , and date .</sentence>
				<definiendum id="0">Entity ( NE ) recognition</definiendum>
				<definiens id="0">a task in which proper nouns and numerical information in a document are detected and classified into categories such as person , organization , and date</definiens>
			</definition>
			<definition id="2">
				<sentence>A non-linear SVM classifier gives a decision function a40 a2a4a3a41a12a6a42 signa2a44a43a45a2a4a3a41a12a10a12 for an input vectora3 where a43a45a2a4a3a41a12a46a42 a47 a48 a23a50a49a41a5a11a51 a23a53a52a54a2a4a3a6a7a56a55a57a23a58a12a24a33a54a59a16a15 Here , a40 a2a4a3a41a12a60a42a61a33a36a35 meansa3 is a member of a certain class anda40 a2a4a3a41a12a36a42a32a38a20a35 meansa3 is not a member .</sentence>
				<definiendum id="0">non-linear SVM classifier</definiendum>
				<definiens id="0">gives a decision function a40 a2a4a3a41a12a6a42 signa2a44a43a45a2a4a3a41a12a10a12 for an input vectora3 where a43a45a2a4a3a41a12a46a42 a47 a48 a23a50a49a41a5a11a51 a23a53a52a54a2a4a3a6a7a56a55a57a23a58a12a24a33a54a59a16a15 Here</definiens>
			</definition>
			<definition id="3">
				<sentence>a62 is the number of support vectors .</sentence>
				<definiendum id="0">a62</definiendum>
			</definition>
			<definition id="4">
				<sentence>a52a54a2a4a3a6a7a56a55a64a12 is a kernel that implicitly maps vectors into a higher dimensional space .</sentence>
				<definiendum id="0">a52a54a2a4a3a6a7a56a55a64a12</definiendum>
				<definiens id="0">a kernel that implicitly maps vectors into a higher dimensional space</definiens>
			</definition>
			<definition id="5">
				<sentence>Recall = M/ ( the number of correct NEs ) , Precision = M/ ( the number of NEs extracted by a system ) , where M is the number of NEs correctly extracted and classified by the system .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">the number of NEs extracted by a system )</definiens>
				<definiens id="1">the number of NEs correctly extracted and classified by the system</definiens>
			</definition>
			<definition id="6">
				<sentence>XQK makes the classifiers faster , but memory requirement increases from a197a60a2a154 a47 a23a131a49a41a5 a152 a23a12 to a197a60a2 a154 a47 a23a131a49a41a5 a152 a23a2a4a152 a23a33a29a35a17a12a10a122a37a133a114a12 wherea152 a23 ( =15 ) is the number of non-zero elements ina55a23 .</sentence>
				<definiendum id="0">XQK</definiendum>
				<definiens id="0">makes the classifiers faster , but memory requirement increases from a197a60a2a154 a47 a23a131a49a41a5 a152 a23a12 to a197a60a2 a154 a47 a23a131a49a41a5 a152 a23a2a4a152 a23a33a29a35a17a12a10a122a37a133a114a12 wherea152 a23 ( =15 ) is the number of non-zero elements ina55a23</definiens>
			</definition>
			<definition id="7">
				<sentence>TinySVM’s classifier prepares a list fi2sia90a157a156a11a93 that contains alla55a23s whose a156 -th coordinates are not zero .</sentence>
				<definiendum id="0">TinySVM’s classifier</definiendum>
				<definiens id="0">prepares a list fi2sia90a157a156a11a93 that contains alla55a23s whose a156 -th coordinates are not zero</definiens>
			</definition>
</paper>

		<paper id="1149">
			<definition id="0">
				<sentence>In short , the free entry of text turns out to be too dicult with a four-button device unless we adopt 2 As with any system where a predictive method is applied , the weak pointofTouchMeKey4 is the processing of unknown words which do not appear in the dictionary .</sentence>
				<definiendum id="0">pointofTouchMeKey4</definiendum>
				<definiens id="0">the processing of unknown words which do not appear in the dictionary</definiens>
			</definition>
			<definition id="1">
				<sentence>Broadly , PPM interpolates the n-gram counts in the user corpus and the statistics in the base dictionary .</sentence>
				<definiendum id="0">PPM</definiendum>
				<definiens id="0">interpolates the n-gram counts in the user corpus and the statistics in the base dictionary</definiens>
			</definition>
			<definition id="2">
				<sentence>P k ( w i ) is calculated as : P k ( w i ) = c k ( w i ) C k ( 2 ) where C k is the frequency of the order k as a context , and c k ( w i ) is the frequency with which w i occurs in that context .</sentence>
				<definiendum id="0">C k</definiendum>
				<definiendum id="1">c k ( w i )</definiendum>
				<definiens id="0">the frequency of the order k as a context , and</definiens>
				<definiens id="1">the frequency with which w i occurs in that context</definiens>
			</definition>
			<definition id="3">
				<sentence>The de nition of this quantity , H ( WjC ) , is given by : H ( WjC ) ( 3 ) = w ; ; c P ( C = c ) H ( W = wjC = c ) = ; w ; ; c P ( C = c ; ; W = w ) logP ( W = wjC = c ) where P ( C = c ) is the probability of the input sequence c and P ( W = wjC = c ) is the conditional probability of words for the given c. When the estimation of W is less certain , H ( WjC ) has a larger value .</sentence>
				<definiendum id="0">c P</definiendum>
				<definiendum id="1">c P</definiendum>
				<definiendum id="2">P</definiendum>
				<definiens id="0">the probability of the input sequence c</definiens>
			</definition>
			<definition id="4">
				<sentence>Wehave presented TouchMeKey4 , a text entry device that requires only four buttons , and aspects of its design and testing .</sentence>
				<definiendum id="0">TouchMeKey4</definiendum>
				<definiens id="0">a text entry device that requires only four buttons , and aspects of its design and testing</definiens>
			</definition>
</paper>

		<paper id="1133">
			<definition id="0">
				<sentence>@ 05 3 ( 05 , 94 : / (  ;  ; 6  :  (  @ 6 &gt;  ; 6  :  (  @ Figure 3 : Percentage of Titles that Respondents Regard as Interesting Table 6 : Results of the Chi-square Test and Cramer’s V ( In this table , “Chi-square” means the significant level tested by the Chi-square , and “Cramer’s” means the value calculated by Cramer’s V. ) Unconcerned Concerned Commoner Engineer Researcher Comprehensible Chi-square 1 % 1 % 1 % 1 % Cramer’s 0.62 0.59 0.38 0.43 Able to Evoke Chi-square 1 % 1 % 1 % 1 % Positive Feelings Cramer’s 0.45 0.45 0.22 0.24 Interesting Chi-square 1 % 1 % 1 % 5 % Cramer’s 0.27 0.35 0.22 0.1 contingency table ( expression patterns × yes/no answer on impressions ) for each impression and each readership and calculated the significance level of the χ 2 -test and the Cramer’s V 2 .</sentence>
				<definiendum id="0">“Chi-square”</definiendum>
				<definiendum id="1">“Cramer’s”</definiendum>
				<definiens id="0">Results of the Chi-square Test</definiens>
			</definition>
			<definition id="1">
				<sentence>Cramer’s V ranges 0 to 1 where 0 means that the two value variables are perfectly unrelated and 1 means that they are perfectly related .</sentence>
				<definiendum id="0">Cramer’s V</definiendum>
				<definiens id="0">ranges 0 to 1 where 0 means that the two value variables are perfectly unrelated and 1 means that they are perfectly related</definiens>
			</definition>
</paper>

		<paper id="1012">
</paper>

		<paper id="2017">
			<definition id="0">
				<sentence>Thus , co-occurrence statistics consist of typical noun phrases as they appear in newspaper texts .</sentence>
				<definiendum id="0">co-occurrence statistics</definiendum>
				<definiens id="0">consist of typical noun phrases as they appear in newspaper texts</definiens>
			</definition>
			<definition id="1">
				<sentence>[ ... ] The typical chunk consists of a single content word surrounded by a constellation of function words , matching a fixed template .</sentence>
				<definiendum id="0">typical chunk</definiendum>
				<definiens id="0">consists of a single content word surrounded by a constellation of function words</definiens>
			</definition>
			<definition id="2">
				<sentence>Underspecified queries can be interpreted in the discourse contexts , and parameters are filled .</sentence>
				<definiendum id="0">Underspecified queries</definiendum>
				<definiens id="0">the discourse contexts , and parameters are filled</definiens>
			</definition>
			<definition id="3">
				<sentence>The interface language between the language analysis module and the controller consists of a fixed set of parameters , which are assigned appropriate values : AF DIRECTION the direction for browsing ( forward , backward ) AF SECTION the section in the newspaper ( politics , sports , ... ) AF SEARCHSTRING the string which has to be searched by the newspaper search engine AF DATE the date when the article to be searched has appeared ( also intervals ) AF ZEITUNG ( NEWSPAPER ) the newspaper which is supposed to be searched AF OPENLINK the link in a document which should be followed in the browser AF OPENURL the URL which is supposed to be opened by the browser The outcome – or left-hand side – of a rule-based simplification can be divided into three command types : AF Simple Search Command , New Search Command : E.g. “Suche nach Camilleri im Kulturressort” ( Search for Camilleri in the cultural section ) or “Neue Suche beginnen mit Krimis” ( Start a new search on thrillers ) .</sentence>
				<definiendum id="0">interface language</definiendum>
				<definiens id="0">a fixed set of parameters , which are assigned appropriate values : AF DIRECTION the direction for browsing ( forward , backward ) AF SECTION the section in the newspaper ( politics , sports , ... ) AF SEARCHSTRING the string which has to be searched by the newspaper search engine AF DATE the date when the article to be searched has appeared ( also intervals ) AF ZEITUNG ( NEWSPAPER ) the newspaper which is supposed to be searched AF OPENLINK the link in a document which should be followed in the browser AF OPENURL the URL which is supposed to be opened by the browser The outcome – or left-hand side – of a rule-based simplification can be divided into three command types : AF Simple Search Command</definiens>
				<definiens id="1">the cultural section ) or “Neue Suche beginnen mit Krimis”</definiens>
			</definition>
			<definition id="4">
				<sentence>AF WWW Browsing : E.g. “Geh zum heutigen Sportbereich” ( Go to today’s sport section ) , “den Standard lesen” ( read the Standard ) or “Geh zur Homepage” ( Go home ) .</sentence>
				<definiendum id="0">AF WWW Browsing</definiendum>
				<definiens id="0">E.g. “Geh zum heutigen Sportbereich” ( Go to today’s sport section ) , “den Standard lesen” ( read the Standard ) or “Geh zur Homepage” ( Go home )</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>We adopt Support Vector Machines ( SVM ) as the device by which a given adnoun clause is analyzed as one of three relative functions ( subject , object , or adverbial ) or an appositive .</sentence>
				<definiendum id="0">Support Vector Machines</definiendum>
				<definiens id="0">the device by which a given adnoun clause is analyzed as one of three relative functions ( subject , object , or adverbial ) or an appositive</definiens>
			</definition>
			<definition id="1">
				<sentence>The functional morpheme ‘eul’ , which represents the object relation between ‘chaeg’ and ‘sseoss-da’ in 1 .</sentence>
				<definiendum id="0">functional morpheme ‘eul’</definiendum>
				<definiens id="0">represents the object relation between ‘chaeg’ and ‘sseoss-da’ in 1</definiens>
			</definition>
			<definition id="2">
				<sentence>In 2 , ‘geu-ga jeongjig-han’ is an appositive adnoun clause and ‘sasil’ is a complement noun .</sentence>
				<definiendum id="0">‘sasil’</definiendum>
				<definiens id="0">an appositive adnoun clause and</definiens>
			</definition>
			<definition id="3">
				<sentence>The technique of Support Vector Machines ( SVM ) is a learning approach for solving two-class pattern recognition problems introduced by Vapnik ( 1995 ) .</sentence>
				<definiendum id="0">technique of Support Vector Machines ( SVM )</definiendum>
				<definiens id="0">a learning approach for solving two-class pattern recognition problems</definiens>
			</definition>
			<definition id="4">
				<sentence>, ‘geu’ is a previos noun pharse feature , ‘ga’ is its functional morpheme feature , ‘sseu’ is a verb feature , ‘n’ is a verb ending feature , ‘chaeg’ is a head noun feature and all POS tags of lexical items are features .</sentence>
				<definiendum id="0">‘geu’</definiendum>
				<definiendum id="1">‘n’</definiendum>
				<definiendum id="2">‘chaeg’</definiendum>
				<definiens id="0">a verb feature</definiens>
				<definiens id="1">a verb ending feature ,</definiens>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>A state merging method typically begins by constructing what is known as a Pre x Tree Automaton ( PTA ) from the positive examples of the language to be inferred .</sentence>
				<definiendum id="0">state merging method</definiendum>
				<definiens id="0">begins by constructing what is known as a Pre x Tree Automaton ( PTA ) from the positive examples of the language to be inferred</definiens>
			</definition>
			<definition id="1">
				<sentence>In particular , we adapt the formula developed for PFSA ( Raman 1997 ) as below : MML ( A ) = NX j=1 log2 ( tj 1 ) !</sentence>
				<definiendum id="0">MML</definiendum>
			</definition>
			<definition id="2">
				<sentence>where : N is the number of states in the PFSA V is the cardinality of the alphabet plus one tj is the number of times the jth state is visited mj is the number of arcs from the jth state ( plus one for nal states ) m0j is the number of arcs from the jth state ( no change for nal states ) nij is the frequency of the ith arc from the jth state M is the sum of all mj values and M0 is the sum of all m0j values One of the goals of this work was to both produce new inference methods and make comprehensive comparisons with existing techniques .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">tj</definiendum>
				<definiendum id="2">nij</definiendum>
				<definiendum id="3">M0</definiendum>
				<definiens id="0">the number of states in the PFSA V is the cardinality of the alphabet plus one</definiens>
				<definiens id="1">the sum of all mj values and</definiens>
			</definition>
			<definition id="3">
				<sentence>The base algorithm is known as Alergia , introduced in ( Carrasco &amp; Oncina 1994b ) .</sentence>
				<definiendum id="0">base algorithm is</definiendum>
			</definition>
			<definition id="4">
				<sentence>XTRACT : A System for Extracting Document Type Descriptors from XML Documents .</sentence>
				<definiendum id="0">XTRACT</definiendum>
				<definiens id="0">A System for Extracting Document Type Descriptors from XML Documents</definiens>
			</definition>
</paper>

		<paper id="1158">
			<definition id="0">
				<sentence>Rule-Based Machine Translation ( MT ) ( Hutchins and Somers , 1992 ) requires large-scale knowledge to analyze both source language ( SL ) sentences and target language ( TL ) sentences .</sentence>
				<definiendum id="0">Rule-Based Machine Translation</definiendum>
				<definiens id="0">requires large-scale knowledge to analyze both source language ( SL ) sentences and target language ( TL ) sentences</definiens>
			</definition>
			<definition id="1">
				<sentence>RCL is a method with an ability that automatically acquires translation knowledge in a computerwithoutanyanalyticalknowledge , suchas GA-ILMT .</sentence>
				<definiendum id="0">RCL</definiendum>
				<definiens id="0">a method with an ability that automatically acquires translation knowledge in a computerwithoutanyanalyticalknowledge</definiens>
			</definition>
			<definition id="2">
				<sentence>Moreover , part translation rules are pairs of source parts and 1 In Figure 2 , the use of a Greek character means that all language characters correspond to unknown character strings for a computer .</sentence>
				<definiendum id="0">character</definiendum>
				<definiens id="0">means that all language characters correspond to unknown character strings for a computer</definiens>
			</definition>
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>The SVM determines the optimal hyperplane by maximizing the margin .</sentence>
				<definiendum id="0">SVM</definiendum>
				<definiens id="0">determines the optimal hyperplane by maximizing the margin</definiens>
			</definition>
			<definition id="1">
				<sentence>Accuracy is deﬁned as follows : Accuracy= b/a ×100 , where a is the speciﬁed number of important sentences , and b is the number of true important sentences that were contained in system’s output .</sentence>
				<definiendum id="0">Accuracy</definiendum>
				<definiendum id="1">b</definiendum>
				<definiens id="0">deﬁned as follows : Accuracy= b/a ×100 , where a is the speciﬁed number of important sentences , and</definiens>
				<definiens id="1">the number of true important sentences that were contained in system’s output</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>We argue that for fluent output from German sentence realization , clausal extraposition needs to be included .</sentence>
				<definiendum id="0">clausal extraposition</definiendum>
				<definiens id="0">needs to be included</definiens>
			</definition>
			<definition id="1">
				<sentence>The Encarta corpora consist of 100,000 randomly selected sentences from the Encarta encyclopedia in both English and German .</sentence>
				<definiendum id="0">Encarta corpora</definiendum>
			</definition>
			<definition id="2">
				<sentence>We also used the NEGRA corpus to verify the accuracy of our data profiling with NLPWin .</sentence>
				<definiendum id="0">NEGRA corpus</definiendum>
				<definiens id="0">to verify the accuracy of our data profiling with NLPWin</definiens>
			</definition>
			<definition id="3">
				<sentence>The shared features fall into the following categories ( where node refers to the starting node for multi-step movement ) : � features relating to verbal properties of the node o a separable prefix verb as ancestor node o tense and mood of ancestor nodes o presence of a verb-final or verb-second VP ancestor o presence of Modals attribute ( indicating the presence of a modal verb ) on ancestors o verb-position in the current node and ancestors � “heaviness”-related features on the extraposable clause and the whole sentence : o sentence length in characters o number of words in the extraposable clause � syntactic labels � the presence of a prepositional relation � the presence of semantic subjects and objects on the node and ancestors � definiteness features � the presence of modifiers on the parent � person and number features � some basic subcategorization features ( e.g. , transitive versus intransitive ) Interestingly , the features that are not shared ( 33 in the model for the technical domain and 27 in the model for the Encarta domain ) fall roughly into the same categories as the features that are shared .</sentence>
				<definiendum id="0">node</definiendum>
				<definiens id="0">the starting node for multi-step movement ) : � features relating to verbal properties of the node o a separable prefix verb as ancestor node o tense and mood of ancestor nodes o presence of a verb-final or verb-second VP ancestor o presence of Modals attribute ( indicating the presence of a modal verb ) on ancestors o verb-position in the current node and ancestors � “heaviness”-related features on the extraposable clause and the whole sentence : o sentence length in characters o number of words in the extraposable clause � syntactic labels � the presence of a prepositional relation � the presence of semantic subjects and objects on the node and ancestors � definiteness features � the presence of modifiers on the parent � person and number features � some basic subcategorization features ( e.g. , transitive versus intransitive ) Interestingly , the features that are not shared</definiens>
			</definition>
			<definition id="4">
				<sentence>Amalgam : A machine-learned generation module .</sentence>
				<definiendum id="0">Amalgam</definiendum>
				<definiens id="0">A machine-learned generation module</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>We define a Base NP as a simple and non-recursive noun phrase .</sentence>
				<definiendum id="0">Base NP</definiendum>
				<definiens id="0">a simple and non-recursive noun phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>All of the proposed methods manage to find out the translation ( s ) of a given word or phrase , on the basis of the linguistic phenomenon that the contexts of a translation tend to be similar to the contexts of the given word or phrase .</sentence>
				<definiendum id="0">phrase</definiendum>
				<definiens id="0">find out the translation ( s ) of a given word or</definiens>
			</definition>
			<definition id="2">
				<sentence>Web is an extremely rich source of data for natural language processing , not only in terms of data size but also in terms of data type ( e.g. , multilingual data , link data ) .</sentence>
				<definiendum id="0">Web</definiendum>
			</definition>
			<definition id="3">
				<sentence>Finally , we calculate ) ( cf E for all Cc ∈ as : ∑ ∈ = Ee E efcPcf ) ( ) ( ) ( ( 2 ) In this way , we can transform the frequency vector in English ) ) ( ) , .</sentence>
				<definiendum id="0">efcPcf ) ( ) ( )</definiendum>
				<definiens id="0">all Cc ∈ as : ∑ ∈ = Ee E</definiens>
			</definition>
			<definition id="4">
				<sentence>In our implementation , we use an equivalent       −− ∑ ∈ ∈ ) ~ | ( log ) ( ) ~ ( logminarg ~ ~ ccPcfcP Cc E Cc α ( 4 ) where 1≥α is an additional parameter used to emphasize the prior information .</sentence>
				<definiendum id="0">1≥α</definiendum>
				<definiens id="0">an additional parameter used to emphasize the prior information</definiens>
			</definition>
</paper>

		<paper id="1160">
			<definition id="0">
				<sentence>The phrase “MT in a day” is strongly associated with research in statistical MT. In this paper we demonstrate that “MT in a day” is possible with a non-statistical MT system provided that the transfer component is learned from aligned bilingual corpora ( bi-texts ) , and does not rely on any large hand-crafted bilingual resource .</sentence>
				<definiendum id="0">phrase “MT</definiendum>
				<definiens id="0">possible with a non-statistical MT system provided that the transfer component is learned from aligned bilingual corpora ( bi-texts ) , and does not rely on any large hand-crafted bilingual resource</definiens>
			</definition>
</paper>

		<paper id="2022">
			<definition id="0">
				<sentence>Archiphonemes ( Trubetzkoy , 1939 ) are used to generalise over phonemes within a language to represent cases where neutralisations arise in certain contexts .</sentence>
				<definiendum id="0">Archiphonemes</definiendum>
				<definiens id="0">used to generalise over phonemes within a language to represent cases where neutralisations arise in certain contexts</definiens>
			</definition>
</paper>

		<paper id="1138">
			<definition id="0">
				<sentence>Request ( DEPART−DATE ) Surface Generator FERGUS TTS SPoT Dialog Manager Sentence Planner DM Imp−conf ( N ) soft−merge Text to Speech Implicit−confirm ( NEWARK ) Implicit−confirm ( DALLAS ) period Imp−conf ( D ) Flying from Newark to Dallas .</sentence>
				<definiendum id="0">Request</definiendum>
				<definiendum id="1">soft−merge Text to Speech Implicit−confirm</definiendum>
				<definiens id="0">DEPART−DATE ) Surface Generator FERGUS TTS SPoT Dialog Manager Sentence Planner DM Imp−conf ( N )</definiens>
			</definition>
			<definition id="1">
				<sentence>Request ( D−D ) Figure 1 : Components of an NLG system .</sentence>
				<definiendum id="0">Request</definiendum>
			</definition>
			<definition id="2">
				<sentence>A text plan is a set of communicative goals which is assumed to be output by a dialog manager of a spoken dialog system .</sentence>
				<definiendum id="0">text plan</definiendum>
				<definiens id="0">a set of communicative goals which is assumed to be output by a dialog manager of a spoken dialog system</definiens>
			</definition>
			<definition id="3">
				<sentence>FERGUS consists of three models : tree chooser , unraveler , and linear precedence chooser .</sentence>
				<definiendum id="0">FERGUS</definiendum>
				<definiens id="0">consists of three models : tree chooser , unraveler , and linear precedence chooser</definiens>
			</definition>
			<definition id="4">
				<sentence>Finally , the linear precedence ( LP ) chooser nds the best path through the word lattice according to a trigram language model ( LM ) , specifying the output string completely .</sentence>
				<definiendum id="0">linear precedence ( LP ) chooser</definiendum>
				<definiens id="0">nds the best path through the word lattice according to a trigram language model ( LM ) , specifying the output string completely</definiens>
			</definition>
			<definition id="5">
				<sentence>The HH corpus consists of approximately 13,000 words in the air-travel reservation domain .</sentence>
				<definiendum id="0">HH corpus</definiendum>
				<definiens id="0">consists of approximately 13,000 words in the air-travel reservation domain</definiens>
			</definition>
			<definition id="6">
				<sentence>Test data consists of about 2,200 words derived from Communicator template data .</sentence>
				<definiendum id="0">Test data</definiendum>
			</definition>
			<definition id="7">
				<sentence>RealPro has the advantage of producing high quality surface strings , but at the cost of having to be hand-tuned to a particular domain .</sentence>
				<definiendum id="0">RealPro</definiendum>
				<definiens id="0">has the advantage of producing high quality surface strings , but at the cost of having to be hand-tuned to a particular domain</definiens>
			</definition>
			<definition id="8">
				<sentence>Spot : A trainable sentence planner .</sentence>
				<definiendum id="0">Spot</definiendum>
				<definiens id="0">A trainable sentence planner</definiens>
			</definition>
</paper>

		<paper id="1137">
			<definition id="0">
				<sentence>To allow titles being generated from the ‘distilled information source’ instead of the original document , we can expand the probability P ( T|D ) as the sum of the probabilities P ( T| ‘information source’ S ) over all the possible ‘information sources’ S , where probability P ( T|S ) stands for the probability of using the word sequence T as the title for the ‘information source’ S. Formally , this idea can be expressed as : ∑ = S DSPSTPDTP ) | ( ) | ( ) | ( ( 4 ) where symbol S stands for a possible ‘information source’ S for the document D. In Equation ( 4 ) , term P ( T|S ) P ( S|D ) represents the idea of two noisy channels , with term P ( S|D ) corresponding to the first channel that samples ‘information source’ S out of the original document D and term P ( T|S ) corresponding to the second noisy channel that creates title T from the ‘distilled information source’ S. Since the first noisy channel , i.e. P ( S|D ) , is new to the old framework for title generation , we will focus on the discussion of the noisy channel P ( S|D ) .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">the sum of the probabilities P ( T| ‘information source’ S ) over all the possible ‘information sources’ S</definiens>
				<definiens id="1">∑ = S DSPSTPDTP ) | ( ) | ( ) | ( ( 4 ) where symbol</definiens>
				<definiens id="2">the idea of two noisy channels</definiens>
			</definition>
			<definition id="1">
				<sentence>Therefore , by putting Equations ( 2 ) , ( 4 ) and ( 5 ) together , our new model for title generation can be expressed as ∑ ∏ ∈ ∈ ∝ S dw DdwgSTtwP TtwP TP DTP ) , ( ) | } ( { } ) ( { ) ( ) |’ ( ( 6 ) By further assuming that the number of words in any ‘information source’ S is equal to the number of words in the title T and , words in title T are created from the ‘information source’ S by first aligning every title word with a different word in the ‘information source’ S and then generating every title word tw from its aligned document word dw according to the probability distribution P ( tw|dw ) , Equation ( 5 ) can be simplified as ∏ ∑ ∈ ∈ ∈ ∝ Ttw Ddw DdwgdwtwP TtwP TP DTP ) , ( ) | ( } ) ( { ) ( ) | ( ( 7 ) Equation ( 7 ) is the center of the new probabilistic model for title generation .</sentence>
				<definiendum id="0">∏ ∑ ∈ ∈ ∈ ∝ Ttw Ddw DdwgdwtwP TtwP TP DTP</definiendum>
				<definiens id="0">| ( } ) ( { ) ( ) | ( ( 7 ) Equation ( 7 ) is the center of the new probabilistic model for title generation</definiens>
			</definition>
			<definition id="2">
				<sentence>F1 metric is a common evaluation metric that has been widely used in information retrieval and automatic text summarization .</sentence>
				<definiendum id="0">F1 metric</definiendum>
				<definiens id="0">a common evaluation metric that has been widely used in information retrieval and automatic text summarization</definiens>
			</definition>
			<definition id="3">
				<sentence>M. Witbrock and V. Mittal ( 1999 ) UltraSummarization : A Statistical Approach to Generating Highly Condensed Non-Extractive Summaries , Proceedings of SIGIR 99 , Berkeley , CA R. Jin and A. G. Hauptmann ( 2001 ) Learn to Select Good Title Word : A New Approach based on Reverse Information Retrieval , ICML 2001 .</sentence>
				<definiendum id="0">UltraSummarization</definiendum>
				<definiens id="0">A Statistical Approach to Generating Highly Condensed Non-Extractive Summaries</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>The predicate SHORTLY-PRECEDE ( t 1 , t 2 ) indicates that 5 �A process consists potentially of three components : ( i ) the process itself ; ( ii ) participants in the process ; ( iii ) circumstances associated with the process .</sentence>
				<definiendum id="0">predicate SHORTLY-PRECEDE</definiendum>
				<definiens id="0">5 �A process consists potentially of three components : ( i ) the process itself ; ( ii ) participants in the process ; ( iii ) circumstances associated with the process</definiens>
			</definition>
			<definition id="1">
				<sentence>So when the long-andunmarked-durative aspect 一直+V+着 ( yi1 zhi2+V+zhe ) combines with the unmarkedfuture-existing aspect 将+V ( jiang1+V ) , the temporal relations of the long-and-unmarkeddurative aspect 一直+V+着 ( yi1zhi2+V+zhe ) ( Figure 5 ) become : t s &lt; t i , t i �t r , and t r &lt; t f. This is represented graphically in Figure 7 .</sentence>
				<definiendum id="0">long-andunmarked-durative aspect 一直+V+着</definiendum>
				<definiendum id="1">yi1 zhi2+V+zhe )</definiendum>
				<definiens id="0">combines with the unmarkedfuture-existing aspect 将+V ( jiang1+V ) , the temporal relations of the long-and-unmarkeddurative aspect 一直+V+着 ( yi1zhi2+V+zhe ) ( Figure 5 ) become : t s &lt; t i</definiens>
			</definition>
</paper>

		<paper id="2018">
			<definition id="0">
				<sentence>XML – and ist precursor SGML – offers a formalism to annotate pieces of ( natural language ) texts .</sentence>
				<definiendum id="0">ist precursor SGML –</definiendum>
				<definiens id="0">offers a formalism to annotate pieces of ( natural language ) texts</definiens>
			</definition>
			<definition id="1">
				<sentence>This is not only the case for most tokens from ‘nonlexical’ types – like telephone numbers , enzyme names , material codes , ... – , even for lexical types there will always be ‘lexical gaps’ .</sentence>
				<definiendum id="0">enzyme</definiendum>
				<definiens id="0">names , material codes , ... –</definiens>
			</definition>
			<definition id="2">
				<sentence>Example 3 unknown token classified as noun with heuristics &lt; NP TYPE= '' COMPLEX '' RULE= '' NPC3 '' GEN= '' FEM '' NUM= '' PL '' CAS= '' _ '' &gt; &lt; NP TYPE= '' FULL '' RULE= '' NP1 '' CAS= '' _ '' NUM= '' PL '' GEN= '' FEM '' &gt; &lt; N SRC= '' UNG '' &gt; Blutanhaftungen &lt; /N &gt; &lt; /NP &gt; &lt; PP CAS= '' DAT '' &gt; &lt; PRP CAS= '' DAT '' &gt; an &lt; /PRP &gt; &lt; NP TYPE= '' FULL '' RULE= '' NP2 '' CAS= '' DAT '' NUM= '' SG '' GEN= '' FEM '' &gt; &lt; DETD &gt; der &lt; /DETD &gt; &lt; N SRC= '' UC1 '' &gt; Gekroesewurzel &lt; /N &gt; &lt; /NP &gt; &lt; /PP &gt; &lt; /NP &gt; The latter case results from some heuristics in POS tagging that allow to assume e.g. the class noun for a token but do not suffice to detect its full paradigm from the token ( note that there are ca two dozen different morphosyntactic paradigms for noun declination in German ) .</sentence>
				<definiendum id="0">N SRC=</definiendum>
				<definiens id="0">unknown token classified as noun with heuristics &lt; NP TYPE= '' COMPLEX '' RULE= '' NPC3 '' GEN= '' FEM '' NUM= '' PL '' CAS= '' _ '' &gt; &lt; NP TYPE= '' FULL '' RULE= '' NP1 '' CAS= '' _ '' NUM= '' PL '' GEN= '' FEM '' &gt; &lt; N SRC= '' UNG '' &gt; Blutanhaftungen &lt; /N &gt; &lt; /NP &gt; &lt; PP CAS= '' DAT '' &gt; &lt; PRP CAS= '' DAT '' &gt; an &lt; /PRP &gt; &lt; NP TYPE= '' FULL '' RULE= '' NP2 '' CAS= '' DAT '' NUM= '' SG '' GEN= '' FEM '' &gt; &lt; DETD &gt; der &lt; /DETD &gt; &lt;</definiens>
			</definition>
			<definition id="3">
				<sentence>Multiple classification can be resolved through the following analysis of the case frame and through its combination with the syntactic structure which includes the token .</sentence>
				<definiendum id="0">Multiple classification</definiendum>
				<definiens id="0">includes the token</definiens>
			</definition>
			<definition id="4">
				<sentence>The following example describes the DTD for the annotation of the results of case frame analysis .</sentence>
				<definiendum id="0">DTD</definiendum>
				<definiens id="0">the annotation of the results of case frame analysis</definiens>
			</definition>
			<definition id="5">
				<sentence>MORPHIX : a fast Realization of a classification-based Approach to Morphology .</sentence>
				<definiendum id="0">MORPHIX</definiendum>
				<definiens id="0">a fast Realization of a classification-based Approach to Morphology</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>The Inversion Transduction Grammar is a bilingual context-free grammar that generates two matched output languages ( referred to as L 1 and L 2 ) .</sentence>
				<definiendum id="0">Inversion Transduction Grammar</definiendum>
				<definiens id="0">a bilingual context-free grammar that generates two matched output languages</definiens>
			</definition>
			<definition id="1">
				<sentence>If probability is associated with each production , the ITG is called the Stochastic Inversion Transduction Grammar ( SITG ) .</sentence>
				<definiendum id="0">ITG</definiendum>
			</definition>
			<definition id="2">
				<sentence>bracketing Because of the difficulty in finding a suitable bilingual syntactic grammar for Chinese and English , a practical ITG is the generic Bracketing Inversion Transduction Grammar ( BTG ) ( Wu 1995 ) .</sentence>
				<definiendum id="0">ITG</definiendum>
			</definition>
			<definition id="3">
				<sentence>BTG is a simplified ITG that has only one nonterminal and does not use any syntactic grammar .</sentence>
				<definiendum id="0">BTG</definiendum>
				<definiens id="0">a simplified</definiens>
			</definition>
			<definition id="4">
				<sentence>Here , “a” denotes the probability of syntactic rules .</sentence>
				<definiendum id="0">“a”</definiendum>
				<definiens id="0">the probability of syntactic rules</definiens>
			</definition>
			<definition id="5">
				<sentence>, , , ( ) , , , ( ) , ( max ) , , , ( , ) , , , ( ) , , , ( ) , ( max ) , , , ( , ) ] , , , ( ) , , , , ( max [ ) , , , ( 0 ) ) ( ( ) ) ( ( 0 ) ) ( ( ) ) ( ( [ ] [ ] UutSvUSstsFvuts vUtSUuSstsFvuts vutsvutsvuts e UvuUStsS vUu tSs e UvuUStsS vUu tSs δδδ δδδ δδδ ≠−−+−− ≤≤ ≤≤ &lt; &gt; ≠−−+−− ≤≤ ≤≤ &lt; &gt; = = = Initialization is as follows : V1,1 ) , / ( V1,1 ) , / ( V1,1 ) , / ( ,1 , , , ,,1 ,1 , ,1 ≤≤≤≤= ≤≤≤≤= ≤≤≤≤= − − −− vTtceb vTteeb vTtceb vvvtt tvvtt vtvvtt δ δ δ where , T , V is the length of English and Chinese sentence respectively. )</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">max ) , , , ( , ) , , , ( ) , , , ( ) , ( max ) , , , ( , ) ] , , , ( ) , , , , ( max [ ) , , , ( 0</definiens>
			</definition>
			<definition id="6">
				<sentence>/ ( vt ceb is the probability of translating English word t e into Chinese word v c .</sentence>
				<definiendum id="0">vt ceb</definiendum>
				<definiens id="0">the probability of translating English word t e into Chinese word v c</definiens>
			</definition>
			<definition id="7">
				<sentence>The leaf node is the aligned words of the two languages and their POS tag categories .</sentence>
				<definiendum id="0">leaf node</definiendum>
				<definiens id="0">the aligned words of the two languages and their POS tag categories</definiens>
			</definition>
			<definition id="8">
				<sentence>Exact match rate ( EMR ) , violate match rate ( VMR ) , and inside match rate ( IMR ) denote the ratio of three types of bracketing numbers in all bracketing numbers respectively .</sentence>
				<definiendum id="0">Exact match rate</definiendum>
				<definiendum id="1">EMR</definiendum>
				<definiens id="0">violate match rate ( VMR ) , and inside match rate ( IMR ) denote the ratio of three types of bracketing numbers in all bracketing numbers respectively</definiens>
			</definition>
</paper>

		<paper id="1084">
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>This significance is defined as in ( Kozima , 1993 ) as its normalized information in a reference corpus2 : ) 1 ( log ) ( log ) ( 2 2 c cw S Sfwsignif − −= ( 2 ) where fw is the number of occurrences of the word w in the corpus and Sc , the size of the corpus .</sentence>
				<definiendum id="0">fw</definiendum>
				<definiens id="0">the number of occurrences of the word w in the corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>More precisely , the combination of these two factors is achieved by a geometric mean : ∑ ⋅= i iitxtcoll wwcohwwgthwwght ) , ( ) ( ) ( ( 3 ) where coh ( w , wi ) is the measure of the cohesion between w and wi in the collocation network .</sentence>
				<definiendum id="0">wi )</definiendum>
				<definiens id="0">the measure of the cohesion between w and wi in the collocation network</definiens>
			</definition>
			<definition id="2">
				<sentence>As ( Choi , 2000 ) or ( Kaufmann , 1999 ) , we use the cosine measure for evaluating the similarity between a vector of the context window ( Vw ) and the equivalent vector in the segment context ( Vs ) : ∑∑ ∑ ⋅ ⋅ = i ix i ix ix i ix xx CwwwgCswwg CwwgwCswwg VwVssim 22 ) , ( ) , ( ) , ( ) , ( ) , ( ( 5 ) where wgx ( wi , C { s , w } ) is the weight of the word wi in the vector x ( txt or coll ) of the context C { s , w } .</sentence>
				<definiendum id="0">wgx</definiendum>
				<definiens id="0">the weight of the word wi in the vector x ( txt or coll ) of the context C { s , w }</definiens>
			</definition>
			<definition id="3">
				<sentence>When TOPICOLL stays in this state for a too long time ( this time is defined as 10 positions of the focus window in our experiments ) , it assumes that the topic of the current part of text is difficult to characterize by using word recurrence or selection from a collocation network and it creates a new segment that covers all the concerned positions .</sentence>
				<definiendum id="0">TOPICOLL</definiendum>
				<definiens id="0">the topic of the current part of text is difficult to characterize by using word recurrence or selection from a collocation network and it creates a new segment that covers all the concerned positions</definiens>
			</definition>
			<definition id="4">
				<sentence>TOPICOLL2 is the same system but without its link detection part .</sentence>
				<definiendum id="0">TOPICOLL2</definiendum>
				<definiens id="0">the same system but without its link detection part</definiens>
			</definition>
			<definition id="5">
				<sentence>TOPICOLL3 is a version of TOPICOLL that only relies on word recurrence .</sentence>
				<definiendum id="0">TOPICOLL3</definiendum>
				<definiens id="0">a version of TOPICOLL that only relies on word recurrence</definiens>
			</definition>
			<definition id="6">
				<sentence>A segment is the first n sentences of a randomly selected document for the Brown corpus” .</sentence>
				<definiendum id="0">segment</definiendum>
				<definiens id="0">the first n sentences of a randomly selected document for the Brown corpus”</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>The POS tag translation probability in equation ( 9 ) was estimated from c ) .</sentence>
				<definiendum id="0">POS tag translation probability</definiendum>
				<definiens id="0">estimated from c )</definiens>
			</definition>
			<definition id="1">
				<sentence>7 chunk types were used for Chinese , including BDP ( adverb phrase ) , BNP ( noun phrase ) , BAP ( adjective 2 http : //mtlab.hit.edu.cn/download/4.TXT phrase ) , BVP ( verb phrase ) , BMP ( quantifier phrase ) , BPP ( prepositional phrase ) and O ( words outside any other chunks ) .</sentence>
				<definiendum id="0">BNP ( noun phrase</definiendum>
				<definiens id="0">//mtlab.hit.edu.cn/download/4.TXT phrase ) , BVP ( verb phrase ) , BMP ( quantifier phrase ) , BPP ( prepositional phrase</definiens>
			</definition>
			<definition id="2">
				<sentence>k∆ is the difference in number of content words between these two chunks , j∆ is the difference of functional words .</sentence>
				<definiendum id="0">k∆</definiendum>
				<definiendum id="1">j∆</definiendum>
				<definiens id="0">the difference in number of content words between these two chunks</definiens>
				<definiens id="1">the difference of functional words</definiens>
			</definition>
</paper>

		<paper id="1139">
</paper>

		<paper id="1109">
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>Metonymy is a natural language phenomenon that contributes to expressing information in an effective and economic way .</sentence>
				<definiendum id="0">Metonymy</definiendum>
				<definiens id="0">a natural language phenomenon that contributes to expressing information in an effective and economic way</definiens>
			</definition>
			<definition id="1">
				<sentence>The event predicates in the TELIC ( or AGENTIVE ) roles are exploited to infer the relation involved .</sentence>
				<definiendum id="0">event</definiendum>
			</definition>
			<definition id="2">
				<sentence>In addition , agreement between syntactic number and semanfruit-dumpling ( x ) CONST = { dough , fruit , … } FORMAL = eatable ( x ) TELIC = ( DEFSINGLE y ( DEFMULTIPLE x ( eat ( eT , y , x ) ) ) ) AGENTIVE = cook ( e'T , z , x ) meat-plate ( x ) CONST = { pork , beef , … } FORMAL = eatable ( x ) TELIC = ( DEFSINGLE x ( DEFMULTIPLE y ( eat ( eT , y , x ) ) ) ) AGENTIVE = prepare ( e'T , z , x ) table ( x ) CONST = { legs , plate , … } FORMAL = physobj ( x ) TELIC = ( DEFSINGLE x ( DEFMULTIPLE y ( sit-at ( eS , y , x ) ) ) ) AGENTIVE = build ( e'T , z , x ) Fig .</sentence>
				<definiendum id="0">semanfruit-dumpling</definiendum>
				<definiendum id="1">DEFMULTIPLE x</definiendum>
				<definiendum id="2">DEFMULTIPLE y</definiendum>
				<definiens id="0">( x ) CONST = { dough , fruit , … } FORMAL = eatable ( x ) TELIC = ( DEFSINGLE y</definiens>
				<definiens id="1">e'T , z , x ) meat-plate ( x ) CONST = { pork , beef , … } FORMAL = eatable ( x ) TELIC = ( DEFSINGLE x ( DEFMULTIPLE y ( eat ( eT , y , x</definiens>
			</definition>
			<definition id="3">
				<sentence>Some 'extended ' examples of Qualia Structures , for special food sorts and 'table ' office ( x ) CONST = { employees , ... } FORMAL = organization ( x ) TELIC = ( SINGLE x ( MULTIPLE y PERSON ( work ( eP , y , x ) ) ) ) AGENTIVE = establish ( e'T , z , x ) airline ( x ) CONST = { planes , office , ... } FORMAL = organization ( x ) TELIC = ( SINGLE x ( MULTIPLE y FLIGHT ( organize ( eT , y , x ) ) ) ) AGENTIVE = found ( e'T , z , x ) flight ( x ) CONST = { place , source , ... } FORMAL = loc-change ( x ) TELIC = ( SINGLE x ( DEFMULTIPLE y PERSON ( carry ( eT , y , x ) ) ) ) AGENTIVE = organize ( e'T , z , x ) Fig .</sentence>
				<definiendum id="0">MULTIPLE y PERSON</definiendum>
				<definiens id="0">loc-change ( x ) TELIC = ( SINGLE x ( DEFMULTIPLE y PERSON ( carry ( eT , y , x</definiens>
			</definition>
			<definition id="4">
				<sentence>For another predicate , such an ambiguity may not be present , as in the example ( SINGLE x OFFICE ( AND ( BOSTONIAN x ) ( CALL x ) ) ) Making use of the TELIC role in the lexical entry of 'office ' , as exposed in Figure 3 , yields ( SINGLE x OFFICE ( AND ( BOSTONIAN x ) ( MULTIPLE y PERSON ( AND ( WORK y x ) ( CALL y ) ) ) ) ) The representation is composed as an expression of the form ( QE xE SE &lt; P &gt; ) : xE being the variable whose representation is to be extended ( initially x , the literal referent ) , QE being its quantifier , and SE its sort ( initially Q and S , associated with the literal referent ) , and &lt; P &gt; being a structured representation of the sentence predicate and its modifiers .</sentence>
				<definiendum id="0">SE</definiendum>
				<definiens id="0">exposed in Figure 3 , yields ( SINGLE x OFFICE ( AND ( BOSTONIAN x ) ( MULTIPLE y PERSON ( AND ( WORK y x ) ( CALL y )</definiens>
			</definition>
			<definition id="5">
				<sentence>If SN is compatible with SR , but QN is incompatible with QR , insert MEMBER between xE and xN .</sentence>
				<definiendum id="0">QN</definiendum>
				<definiens id="0">compatible with SR , but</definiens>
			</definition>
			<definition id="6">
				<sentence>, shows how chained metonymic extensions are handled : ( WH x AIRLINE ( AND ( SERVE x FOOD ) ( SOURCE x NEW YORK ) ( GOAL x BOSTON ) ) ) The first metonymic extension , based on the lexicon entry for 'airline ' ( see Figure 3 ) , tentatively inserts 'flights ' linked to 'airline ' via an ORGANIZE relation , and yields ( WH x AIRLINE ( MULTIPLE y FLIGHT ( AND ( ORGANIZE x y ) ( SERVE y FOOD ) ( SOURCE y NEW YORK ) ( GOAL y BOSTON ) ) ) ) and the final operation based on the lexicon entry for 'flight ' ( see Figure 3 ) leads to a similar extension , inserting 'person ' related to 'flight ' via a CARRY relation : ( WH x AIRLINE ( MULTIPLE y FLIGHT ( AND ( ORGANIZE x y ) ( SOURCE y NEW YORK ) ( GOAL y BOSTON ) ( MULTIPLE z PERSON ( AND ( CARRY y z ) ( SERVE z FOOD ) ) ) ) ) ) Note the distinguished treatment of the predications containing the variable which represents the phrase to be extended , as opposed to the previous examples .</sentence>
				<definiendum id="0">MULTIPLE y FLIGHT</definiendum>
				<definiendum id="1">) ) ) ) ) Note</definiendum>
				<definiens id="0">ORGANIZE x y ) ( SERVE y FOOD ) ( SOURCE y NEW YORK ) ( GOAL y BOSTON</definiens>
				<definiens id="1">the distinguished treatment of the predications containing the variable which represents the phrase to be extended</definiens>
			</definition>
			<definition id="7">
				<sentence>TEAM : An Experiment in the Design of Transportable Natural-Language Interfaces .</sentence>
				<definiendum id="0">TEAM</definiendum>
				<definiens id="0">An Experiment in the Design of Transportable Natural-Language Interfaces</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>A lexical frame encodes the word category ( part of speech ) , subcategorization features , and morphological diacritics ( person , gender , case , etc. ) of its lexical anchor ( cf. the elementary trees of Tree Adjoining Grammar ( TAG ; e.g. Joshi &amp; Schabes , 1997 ) .</sentence>
				<definiendum id="0">lexical frame</definiendum>
				<definiens id="0">encodes the word category ( part of speech ) , subcategorization features</definiens>
			</definition>
			<definition id="1">
				<sentence>Associated with every categorial node ( i.e. , lexical or phrasal node ) is a feature matrix , which includes two types of features : agreement features ( not to be discussed here ; see Kempen &amp; Harbusch , forthcoming ) and topological features .</sentence>
				<definiendum id="0">categorial node</definiendum>
				<definiens id="0">a feature matrix , which includes two types of features</definiens>
			</definition>
			<definition id="2">
				<sentence>Substitution is PG 's sole composition operation .</sentence>
				<definiendum id="0">Substitution</definiendum>
			</definition>
			<definition id="3">
				<sentence>Substitution involves unification of the feature matrices that are associated with the substituted phrasal foot node and the root node of the substituting lexical frame .</sentence>
				<definiendum id="0">Substitution</definiendum>
				<definiens id="0">involves unification of the feature matrices that are associated with the substituted phrasal foot node and the root node of the substituting lexical frame</definiens>
			</definition>
			<definition id="4">
				<sentence>Slot types are defined as attributes that take as value a non-branching list of lemmas or phrases ( e.g. SUBJect-NP , CoMPlement-S or HeaD-v ) .</sentence>
				<definiendum id="0">Slot types</definiendum>
				<definiens id="0">attributes that take as value a non-branching list of lemmas or phrases ( e.g. SUBJect-NP , CoMPlement-S or HeaD-v )</definiens>
			</definition>
			<definition id="5">
				<sentence>The expression `` L1 ⊕L2 '' represents the list composed of the members of L1 followed by the members of L2 .</sentence>
				<definiendum id="0">L1 ⊕L2 ''</definiendum>
				<definiens id="0">represents the list composed of the members of L1 followed by the members of L2</definiens>
			</definition>
			<definition id="6">
				<sentence>Syntactic theory : a formal introduction .</sentence>
				<definiendum id="0">Syntactic theory</definiendum>
				<definiens id="0">a formal introduction</definiens>
			</definition>
</paper>

		<paper id="1143">
			<definition id="0">
				<sentence>Word Sense Disambiguation ( WSD ) is a central open problem at the lexical level of Natural Language Processing ( NLP ) .</sentence>
				<definiendum id="0">Word Sense Disambiguation</definiendum>
				<definiendum id="1">WSD )</definiendum>
			</definition>
			<definition id="1">
				<sentence>The automatic tagger estimates the conditional probability that a word has sense a2 given that it occurs in context a3 , where a3 is a conjunction of features .</sentence>
				<definiendum id="0">automatic tagger</definiendum>
				<definiens id="0">estimates the conditional probability that a word has sense a2 given that it occurs in context a3 , where a3 is a conjunction of features</definiens>
			</definition>
			<definition id="2">
				<sentence>They include collocational features requiring no linguistic preprocessing beyond partof-speech tagging ( 1 ) , syntactic features that capture relations between the verb and its complements ( 2-4 ) , and semantic features that incorporate information about noun classes for subjects and objects ( 5-6 ) : of speech of words at positions -1 and +1 relative to a4 , and words at positions -2 , -1 , +1 , +2 , relative to a4 rect object , or clausal complement ( a complement whose node label is S in the parse tree ) direct object , indirect object , particle , prepositional complement ( and its object ) TION , LOCATION ) for proper nouns appearing in ( 4 ) appearing in ( 4 ) The maximum entropy system’s performance on the verbs from the evaluation data for SENSEVAL1 ( Kilgarriff and Rosenzweig , 2000 ) rivaled that of the best-performing systems .</sentence>
				<definiendum id="0">maximum entropy</definiendum>
				<definiens id="0">a complement whose node label is S in the parse tree ) direct object , indirect object , particle , prepositional complement</definiens>
			</definition>
</paper>

		<paper id="1059">
</paper>

		<paper id="1155">
			<definition id="0">
				<sentence>for Text Classification Category is a powerful tool to manage a large number of text documents .</sentence>
				<definiendum id="0">Text Classification Category</definiendum>
				<definiens id="0">a powerful tool to manage a large number of text documents</definiens>
			</definition>
			<definition id="1">
				<sentence>With this assumption , a NB classifier finds the most probable class c i ∈ C , called a maximum a posteriori ( MAP ) c MAP for the document , where C= { c 1 , c 2 , … , c k } is a set of predefined classes .</sentence>
				<definiendum id="0">c k }</definiendum>
				<definiens id="0">a set of predefined classes</definiens>
			</definition>
			<definition id="2">
				<sentence>A centroid-based classifier ( CB ) is a modified version of k-NN classifier .</sentence>
				<definiendum id="0">centroid-based classifier ( CB )</definiendum>
				<definiens id="0">a modified version of k-NN classifier</definiens>
			</definition>
			<definition id="3">
				<sentence>Instead of comparing the test document with all training documents , CB calculates a centroid ( a vector ) for all training documents in each class and compares the test document with these centroids to find the most probable ( similar ) class .</sentence>
				<definiendum id="0">CB</definiendum>
				<definiens id="0">calculates a centroid ( a vector ) for all training documents in each class and compares the test document with these centroids to find the most probable ( similar ) class</definiens>
			</definition>
			<definition id="4">
				<sentence>A simple centroid-based classifier represents a document with a vector each dimension of which expresses a term in the document with a weight of tf×idf .</sentence>
				<definiendum id="0">simple centroid-based classifier</definiendum>
				<definiens id="0">a document with a vector each dimension of which expresses a term in the document with a weight of tf×idf</definiens>
			</definition>
</paper>

		<paper id="1161">
			<definition id="0">
				<sentence>Mihalcea and Moldovan ( 1999 ) and Lytinen et al. ( 2000 ) used a machine readable thesaurus , specifically WordNet ( Miller et al. , 1990 ) , to obtain the sense of a word , while Sch¨utze and Pedersen ( 1995 ) and Lin ( 1998 ) used automatically constructed thesauri .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">Miller et al. , 1990 ) , to obtain the sense of a word</definiens>
			</definition>
			<definition id="1">
				<sentence>For instance , if a0 a3 appears 10 times in the corpus and a0a5a1 -a0a6a3 appears 4 times , a1 a8 a0a5a1 a23 a0a6a3 a17a3a2a5a4 a13a7a6a9a8 ( where a8 is a normalizing constant ) .</sentence>
				<definiendum id="0">a8</definiendum>
				<definiens id="0">a normalizing constant )</definiens>
			</definition>
			<definition id="2">
				<sentence>In the above example , this yields a1 a8 a0a7a1a10a11a15a0a6a3 a17a21a2a22a6a24a23 and a1 a8 a0 a10a1 a11a15a0 a10a3 a17a25a2a27a26a20a4a20a23 ( where a23 is a normalizing constant ) .</sentence>
				<definiendum id="0">a23</definiendum>
				<definiens id="0">a normalizing constant )</definiens>
			</definition>
			<definition id="3">
				<sentence>Our retrieval process consists of the following steps : # para a70 ) , where a1a67a72 is the lemmatized query : ( a ) Extract the content lemmas from a1 a1 : a0a2a1a11a73 a9a14a11a14a13a14a13a14a13 a0a2a1a11a73 a74 , where a75 is the number of content lemmas in paraphrase a1 a1 .</sentence>
				<definiendum id="0">retrieval process</definiendum>
				<definiendum id="1">a75</definiendum>
				<definiens id="0">consists of the following steps : # para a70 ) , where a1a67a72 is the lemmatized query : ( a ) Extract the content lemmas from a1 a1 : a0a2a1a11a73 a9a14a11a14a13a14a13a14a13 a0a2a1a11a73 a74 , where</definiens>
			</definition>
			<definition id="4">
				<sentence>ColScore is a hybrid setting , where WordNet was used for scoring lemma-pairs in the proposed paraphrases , but not for generating them .</sentence>
				<definiendum id="0">ColScore</definiendum>
				<definiens id="0">a hybrid setting , where WordNet was used for scoring lemma-pairs in the proposed paraphrases , but not for generating them</definiens>
			</definition>
			<definition id="5">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1028">
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>The construction part generates text sentences in the form of dependency trees by using complementary information to replace information that is missing because of a “knowledge gap” and other missing function words to generate natural text sentences based on a particular monolingual corpus .</sentence>
				<definiendum id="0">construction part</definiendum>
				<definiens id="0">generates text sentences in the form of dependency trees by using complementary information to replace information that is missing because of a “knowledge gap” and other missing function words to generate natural text sentences based on a particular monolingual corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>The evaluation part consists of a model for generating an appropriate text when given keywords .</sentence>
				<definiendum id="0">evaluation part</definiendum>
				<definiens id="0">consists of a model for generating an appropriate text when given keywords</definiens>
			</definition>
			<definition id="2">
				<sentence>Text generation is an important technique used for applications like machine translation , summarization , and human/computer dialogue .</sentence>
				<definiendum id="0">Text generation</definiendum>
				<definiens id="0">an important technique used for applications like machine translation</definiens>
			</definition>
			<definition id="3">
				<sentence>( 1 ) In this equation , P ( S|T ) represents the model used to replace words or phrases in a source language with those in the target language .</sentence>
				<definiendum id="0">P ( S|T )</definiendum>
				<definiens id="0">the model used to replace words or phrases in a source language with those in the target language</definiens>
			</definition>
			<definition id="4">
				<sentence>P ( T|K ) represents a model that generates text sentence T when given a set of headwords , K. We call the model represented by P ( T|K ) atext-generation model .</sentence>
				<definiendum id="0">P ( T|K )</definiendum>
				<definiens id="0">a model that generates text sentence T when given a set of headwords</definiens>
			</definition>
			<definition id="5">
				<sentence>( 4 ) In this equation , M denotes an ordered set of morphemes and D denotes an ordered set of dependencies in a sentence .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">an ordered set of morphemes and D denotes an ordered set of dependencies in a sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>P ( K|M , D , T ) represents a keyword-production model .</sentence>
				<definiendum id="0">T )</definiendum>
				<definiens id="0">a keyword-production model</definiens>
			</definition>
			<definition id="7">
				<sentence>Abunsetsu is a phrasal unit that usually consists of several content and function words .</sentence>
				<definiendum id="0">Abunsetsu</definiendum>
				<definiens id="0">a phrasal unit that usually consists of several content and function words</definiens>
			</definition>
			<definition id="8">
				<sentence>( 5 ) In this rule , h k represents the head morpheme whose word is equal to keyword k ; m ∗ represents zero , one , or a series of morphemes that are connected to h k in the same bunsetsu .</sentence>
				<definiendum id="0">h k</definiendum>
				<definiens id="0">the head morpheme whose word is equal to keyword k ; m ∗ represents zero , one , or a series of morphemes that are connected to h k in the same bunsetsu</definiens>
			</definition>
			<definition id="9">
				<sentence>We used two keyword-production models : model KM3+MM+DM , which achieved the best results in the first experiment , and model KM5+MM+DM , which considers the richest information .</sentence>
				<definiendum id="0">model KM3+MM+DM</definiendum>
				<definiens id="0">considers the richest information</definiens>
			</definition>
			<definition id="10">
				<sentence>The Unknown Word Problem : a Morphological Analysis of Japanese Using Maximum Entropy Aided by a Dictionary .</sentence>
				<definiendum id="0">Unknown Word Problem</definiendum>
				<definiens id="0">a Morphological Analysis of Japanese Using Maximum Entropy Aided by a Dictionary</definiens>
			</definition>
</paper>

		<paper id="1095">
</paper>

		<paper id="1142">
</paper>

		<paper id="1112">
			<definition id="0">
				<sentence>While Dlist does not attempt to combine the features , i.e. it takes the strongest feature only , Boost tries combinations of features and also uses negative evidence , i.e. the absence of features .</sentence>
				<definiendum id="0">Boost</definiendum>
				<definiens id="0">tries combinations of features and also uses negative evidence</definiens>
			</definition>
			<definition id="1">
				<sentence>Minipar provides simple subcategorization information in the PoS itself , e.g. V_N_N for a verb taking two arguments .</sentence>
				<definiendum id="0">Minipar</definiendum>
				<definiens id="0">provides simple subcategorization information in the PoS itself</definiens>
			</definition>
			<definition id="2">
				<sentence>Decision Lists for Lexical Ambiguity Resolution : Application to Accent Restoration in Spanish and French .</sentence>
				<definiendum id="0">Decision Lists</definiendum>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>`` impossible unigrams '' ( as above ) makes little sense apart from serving as a motivation for error detection based on search for `` impossible n-grams '' , i.e. n-tuples ( n ∈ N ) of tags which , if occuring as tags of adjacent words in a text of a particular language , constitute a violation of ( syntactic ) rules of this language .</sentence>
				<definiendum id="0">n-tuples</definiendum>
				<definiens id="0">( n ∈ N ) of tags which , if occuring as tags of adjacent words in a text of a particular language , constitute a violation of ( syntactic ) rules of this language</definiens>
			</definition>
			<definition id="1">
				<sentence>Correcting the NEGRA® Corpus : Methods , Results , Implications , ÖFAI Technical Report Müller F.H. and T. Ule ( 2001 ) Satzklammer annotieren und tags korrigieren : Ein mehrstufiges topdown-bottom-up System zur flachen , robusten Annotierung von Sätzen im Deutschen , in : Proceedings der GLDV-Frühjahrstagung 2001 , Gießen NEGRA® : www.coli.uni-sb.de/sfb378/negra-corpus Oliva K. ( 2001 ) The possibilities of automatic detection/correction of errors in tagged corpora : a pilot study on a German corpus , in : 4th International conference `` Text , Speech and Dialogue '' TSD 2001 , Lecture Notes in Artificial Intelligence 2166 , Springer , Berlin 2001 Oliva K. ( to appear ) Linguistics-based tagging of Czech : disambiguation of 'se ' as a test case , in : Proceedings of 4th European Conference on Formal Description of Slavic Languages held in Potsdam from 28th till 30th November 2001 Petkevia2 V. ( 2001 ) Grammatical agreement and automatic morphological disambiguation of inflectional languages , in : 4th International conference `` Text , Speech and Dialogue '' TSD 2001 , Lecture Notes in Artificial Intelligence 2166 , Springer , Berlin 2001 Schiller A. , S. Teufel , C. Stöckert and C. Thielen ( 1999 ) Guidelines für das Tagging deutscher Textcorpora , University of Stuttgart / University of Tübingen Skut W. , B. Krenn , T. Brants &amp; H. Uszkoreit ( 1997 ) An annotation scheme for free word order languages , in : Proceedings of the 3rd Applied Natural Language Processing Conference , Washington D.C .</sentence>
				<definiendum id="0">Correcting the NEGRA® Corpus</definiendum>
				<definiens id="0">Grammatical agreement and automatic morphological disambiguation of inflectional languages</definiens>
			</definition>
</paper>

		<paper id="1165">
</paper>

		<paper id="2012">
</paper>

		<paper id="1168">
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>The Reconstruction Engine ( Lowe and Mazaudon , 1994 ) , a set of programs designed to be an aid in language reconstruction , requires a set of correspondences to be provided beforehand .</sentence>
				<definiendum id="0">Reconstruction Engine</definiendum>
				<definiens id="0">a set of programs designed to be an aid in language reconstruction</definiens>
			</definition>
			<definition id="1">
				<sentence>Method A re-estimates the likelihood scores as the logarithm of the probability of jointly generating the pair of words u and v : score A B4uBNvB5BPlog linksB4uBNvB5 ∑ u BC BNv BC linksB4u BC BNv BC B5 where linksB4uBNvB5 denotes the number of links induced between u and v. Note that the co-occurrence counts of u and v are not used for the re-estimation , In Method B , an explicit noise model with auxiliary parameters λ B7 and λ A0 is constructed in order to improve the estimation of likelihood scores .</sentence>
				<definiendum id="0">linksB4uBNvB5</definiendum>
				<definiens id="0">the number of links induced between u and v. Note that the co-occurrence counts of u</definiens>
			</definition>
			<definition id="2">
				<sentence>Let coocB4uBNvB5 be the number of co-occurrences of u and v.Thescore function is defined as : score B B4uBNvB5BPlog BB4linksB4uBNvB5CYcoocB4uBNvB5BNλ B7 B5 BB4linksB4uBNvB5CYcoocB4uBNvB5BNλ A0 B5 where BB4k CYnBN pB5 denotes the probability of k being generated from a binomial distribution with parameters n and p. In Method C , bitext tokens are divided into classes , such as content words , function words , punctuation , etc. , with the aim of producing more accurate translation models .</sentence>
				<definiendum id="0">BB4k CYnBN pB5</definiendum>
				<definiens id="0">the probability of k being generated from a binomial distribution with parameters n and p. In Method C , bitext tokens are divided into classes , such as content words , function words , punctuation , etc. , with the aim of producing more accurate translation models</definiens>
			</definition>
			<definition id="3">
				<sentence>The ranking is then evaluated against a gold standard by computing the n-point average precision , a generalization of the 11-point average precision , where n is the total number of cognate pairs in the list .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the total number of cognate pairs in the list</definiens>
			</definition>
</paper>

		<paper id="1170">
</paper>

		<paper id="1032">
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>Performance ( % ) a159 Distance Threshold precision ( classifier ) precision ( type ) recall a25 a206a136a213a132a216a100a195a45a192a51a41a153a101a75a203a54a192a138a208a47a205a138a199a207a199a14a160a108a204a40a195a45a192a96a208a30a206a209a191a180a206a131a197a91a211a147a204a40a192a47a195a185a201a128a197a91a195a45a200a123a205a108a211a18a208a81a192 a248a25a215a100a192a31a191a185a200a123a205a96a199a207a199a136a192a47a195a20a210a45a215a114a192a31a212a98a206a209a191a185a210a30a205a108a211a18a208a81a192a59a210a45a215a114a195a45a192a138a191a185a215a114a197a154a199a209a212a149a226a98a210a168a215a100a192a34a201a128a192 a214 a192a47a195 a192a81a223a12a205a108a200a106a196a66a199a131a192a138a191a105a208a47a205a154a211 a198a18a192a36a195a45a192a194a210a168a195a185a206a131a192a194a193a98a192a138a212a70a226a4a195a168a192a138a191a185a216a66a199a131a210a148a206a136a211a114a213 a206a131a211 a205a178a212a88a192a194a218 a208a81a195a45a192a96a205a91a191a185a192a68a206a136a211a16a210a168a215a100a192a4a191a185a222a114a191a185a210a168a192a194a200a105a242a191a6a195a168a192a138a208a47a205a138a199a207a199a180a224a155a8a34a197 a214 a192a194a193a91a192a47a195a96a226a96a210a168a215a100a192a25a200a106a197a132a195a45a192 a191a180a206a131a200a16a206a207a199a128a205a108a195a20a210a45a215a114a192a34a192a81a223a12a205a108a200a106a196a66a199a131a192a138a191a47a226a98a210a168a215a100a192a25a200a106a197a132a195a45a192a59a205a132a208a194a208a81a216a100a195a30a205a108a210a45a192a4a210a45a215a114a192a34a211a100a216a114a218 a200a106a192a47a195a30a205a96a199a29a208a56a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a20a208a47a205a154a211a16a198a18a192a34a191a185a192a81a199a131a192a138a208a81a210a45a192a96a212a149a226a96a199a131a192a138a205a132a212a98a206a131a211a100a213a240a210a45a197a240a205a108a211a247a206a131a211a114a218 a208a81a195a45a192a96a205a91a191a185a192a79a206a131a211a53a196a114a195a45a192a138a208a56a206a209a191a180a206a136a197a132a211a6a224a150a225a87a201 a214 a192a40a191a185a192a125a199a131a192a138a208a81a210a41a210a168a215a100a192a139a73a240a204a147a200a123a205a154a210a56a208a56a215a66a206a131a211a100a213 a210a168a215a100a195a45a192a96a191a185a215a100a197a108a199a128a212a53a216a12a191a185a192a138a212a240a206a131a211a100a12a154a14a14a216a114a200a16a206a131a210a30a205a66a226a70a16a68a67a68a67a43a61a81a18a56a226a96a211a12a205a154a200a106a192a81a199a136a222a227a244a29a84a72a80a66a245a142a161 a67a66a224a182a56a66a226a29a210a45a215a114a192a57a195a168a192a138a208a47a205a138a199a207a199a150a206a209a191 a41a19a16a66a224a178a67a23a158 a212a66a216a114a192a57a210a168a197a141a205a106a199a128a205a132a208a56a221a75a197a154a201a171a191a180a206a131a200a16a206a207a199a209a205a154a195 a192a81a223a12a205a108a200a106a196a66a199a131a192a138a191a68a201a128a197a91a195a171a210a168a215a100a192a31a195a45a192a194a210a168a195a185a206a131a192a194a193a132a205a138a199a6a191a185a210a168a192a194a196a102a197a91a195a171a210a168a197a123a205a31a201a87a205a138a206a207a199a131a216a100a195a45a192a59a197a108a201 a210a168a215a100a192a59a205a96a199a207a206a136a213a132a211a100a200a106a192a47a211a100a210a150a200a106a197a88a212a66a216a66a199a131a192a91a224a26a248a25a215a100a192a183a212a66a192a194a210a56a205a138a206a207a199a131a192a138a212a10a211a100a216a114a200a16a198a114a192a194a195a30a191a150a197a154a201 a192a96a205a91a208a81a215a141a195a56a205a154a211a114a221a105a205a154a195a45a192a31a213a154a206a131a193a91a192a194a211a102a206a136a211a105a248a40a205a108a198a66a199a131a192 a41a12a224 a248a26a205a154a198a88a199a136a192 a41a39a101a139a203a34a205a108a211a100a221a14a206a131a211a100a213a89a203a4a192a96a191a185a216a66a199a131a210a30a191a68a201a128a197a132a195a102a244a19a84a72a80a66a245a158a161a77a67a114a224a178a56 a63a75a117a90a113a68a218 a121a3a118a189a117a90a132a112a132a112a115a62a61a19a107a126a110 a103a3a104a64a106a23a107 a114 a205 a66 a205 a205a1a205 a85 a117 a205 a74 a66a96a77 a121 a66a16a100 a66a16a100 a120 a205 a100a7a100 a205 a100a16a100 a133a112a109a1a133a74a117a90a118 a209a1a209a68a205 a248a25a215a100a192a47a195a45a192a81a201a252a197a132a195a45a192a91a226 a214 a192a178a191a185a216a18a208a47a208a125a192a47a192a138a212a88a192a138a212a97a206a131a211 a213a91a192a47a211a100a192a47a195a30a205a154a210a148a206a131a211a100a213 a211a114a216a100a218 a200a106a192a47a195a30a205a96a199a254a208a56a199a128a205a132a191a45a191a180a206a217a243a12a192a194a195a56a191 a208a125a197a132a195a45a195a45a192a96a208a125a210a148a199a131a222a99a201a252a197a132a195a251a71a64a41a12a224a178a16a23a158 a197a108a201a83a210a45a215a114a192 a191a185a216a12a208a194a208a81a192a138a191a45a191a180a201a128a216a66a199a207a199a131a222a112a200a123a205a154a210a56a208a125a215a114a192a138a212a112a205a154a211a12a212a94a205a138a199a207a206a131a213a91a211a114a192a138a212 a210a168a192a138a191a185a210a250a216a100a210a168a210a45a192a47a195a45a218 a205a108a211a18a208a81a192a138a191a47a224a94a248a25a215a100a192a103a196a100a195a168a192a138a208a56a206a128a191a180a206a131a197a91a211 a197a154a201a247a192a125a223a100a205a91a208a81a210a45a218a252a200a123a205a108a210a30a208a81a215a52a191a45a205a154a200a106a196a88a199a136a192a96a191 a12a180a212a98a206a209a191a185a210a90a246a102a67a114a224a178a67a23a18a249a206a128a191a27a63a70a67a23a158a123a226a77a210a45a215a88a216a18a191 a206a136a200a106a196a18a197a14a191a180a206a131a211a100a213a37a205a108a211a50a216a114a196a100a196a18a192a47a195 a198a70a197a91a216a114a211a18a212a100a205a108a195a45a222a119a201a128a197a91a195a123a197a132a216a100a195a123a200a106a192a47a210a45a215a100a197a100a212a250a210a168a215a18a205a108a210a141a208a125a197a132a211a66a201a128a197a91a195a168a200a123a191 a214 a206a131a210a168a215 a210a168a215a100a192a31a195a45a192a138a191a185a216a88a199a131a210a30a191a4a212a66a192a96a191a45a208a125a195a148a206a136a198a18a192a96a212a57a206a136a211a17a12a74a157a68a197a132a211a18a212a102a205a154a211a12a212a220a204a20a205a138a206a131a221a70a226a29a16a68a67a70a67a68a67a19a18a30a224 a162 a163a36a61a87a188a154a63a94a4a68a188a154a188a154a61a87a90a6a0 a248a25a215a114a192a25a205a138a199a131a213a91a197a132a195a185a206a131a210a45a215a114a200 a196a100a195a168a197a91a196a18a197a14a191a185a192a138a212a53a206a136a211a10a210a168a215a66a206a209a191a41a196a18a205a154a196a70a192a47a195a120a216a12a191a185a192a138a191a150a198a66a206a207a199a217a206a131a211a100a218 a213a91a216a12a205a138a199a34a216a100a210a168a210a168a192a194a195a30a205a108a211a18a208a81a192a102a196a18a205a138a206a131a195a30a191a106a205a108a211a18a212a119a196a100a215a100a195a56a205a91a191a185a192a105a205a138a199a207a206a131a213a91a211a100a200a106a192a47a211a88a210a46a201a252a197a132a195 a210a168a215a100a192a34a192a81a223a114a210a168a195a30a205a132a208a125a210a148a206a136a197a132a211a57a197a108a201a70a199a209a205a154a211a114a213a91a216a12a205a154a213a132a192a194a218a64a191a185a196a18a192a138a208a56a206a217a243a29a208a54a221a98a211a100a197 a214 a199a136a192a96a212a66a213a132a192a68a197a154a201 a210a168a215a100a192a16a210a30a205a108a195a45a213a91a192a47a210a183a199a128a205a108a211a100a213a132a216a18a205a108a213a91a192a16a211a100a197a132a210a46a205a108a193a154a205a96a206a207a199a128a205a108a198a66a199a131a192a53a206a131a211a251a210a168a215a100a192a106a191a185a197a91a216a114a195a30a208a81a192 a199a209a205a154a211a114a213a91a216a18a205a108a213a91a192a91a224a244 a192a25a205a154a196a114a196a66a199a207a206a136a192a96a212a247a210a168a215a66a206a209a191a40a200a106a192a47a210a45a215a100a197a88a212a247a210a168a197a46a210a168a215a100a192a4a205a154a211a12a205a138a199a131a222a88a218 a191a180a206a209a191a79a197a108a201a29a211a100a216a114a200a106a192a194a195a30a205a96a199a100a192a125a223a66a196a100a195a45a192a96a191a45a191a180a206a131a197a91a211a18a191a47a224a155a73a183a216a114a192a34a210a45a197a53a210a45a215a114a192a54a192a81a223a12a205a108a200a106a196a66a199a131a192a194a218 a198a18a205a132a191a185a192a138a212a220a221a98a211a100a197 a214 a199a131a192a138a212a66a213a132a192a53a192a81a223a114a210a168a195a30a205a132a208a125a210a148a206a136a197a132a211a6a226a29a210a168a215a100a192a10a196a114a195a45a197a91a198a88a199a136a192a47a200a123a191a31a208a81a197a91a211a114a218 a208a81a192a194a195a45a211a88a206a136a211a114a213a247a211a100a197a91a211a114a218a64a208a125a197a132a211a18a208a125a192a47a196a100a210a168a216a18a205a96a199a149a197a91a195a171a197a91a211a114a192a194a218a252a210a168a197a91a218a252a200a123a205a154a211a114a222a106a208a125a197a132a211a18a208a81a192a194a196a114a218 a210a168a216a18a205a138a199a18a195a45a192a47a196a100a195a45a192a96a191a185a192a194a211a88a210a56a205a154a210a148a206a136a197a132a211a18a191a150a197a154a201a41a210a168a215a100a192a4a195a45a216a66a199a131a192a47a218a87a198a12a205a91a191a185a192a4a205a154a196a114a196a100a195a45a197a14a205a91a208a81a215a100a192a96a191 a200a106a192a47a211a100a210a148a206a136a197a132a211a100a192a96a212a247a206a131a211a44a14a14a192a96a208a125a210a148a206a131a197a91a211a57a56 a214 a192a194a195a45a192a183a205a154a193a91a197a154a206a209a212a66a192a96a212a149a226a132a196a100a195a168a197a91a193a14a206a209a212a14a206a136a211a114a213 a205a123a216a100a211a66a206a207a201a128a197a91a195a168a200a39a205a108a196a100a196a100a195a168a197a98a205a132a208a125a215a102a201a252a197a132a195a31a210a168a215a100a192a16a205a132a191a45a191a180a206a136a213a132a211a100a200a106a192a47a211a100a210a183a197a108a201a20a193a98a205a154a195a148a206a136a218 a197a91a216a12a191 a239a205a154a196a12a205a154a211a114a192a138a191a185a192a34a211a114a216a100a200a106a192a47a195a30a205a96a199a100a208a30a199a209a205a132a191a45a191a148a206a207a243a12a192a194a195a171a208a81a197a91a211a12a191a185a210a45a195a45a216a12a208a125a210a148a206a131a197a91a211a18a191a150a210a45a197 a208a81a197a91a195a45a195a168a192a138a191a185a196a18a197a91a211a12a212a98a206a131a211a100a213a249a211a100a216a114a200a106a192a194a195a56a205a138a199a209a191a8a197a108a201a106a210a168a215a100a192a36a241a40a211a100a213a108a199a217a206a209a191a185a215 a191a185a197a91a216a100a195a56a208a125a192 a216a100a210a168a210a168a192a194a195a30a205a108a211a18a208a81a192a138a191a47a224 a225a180a211a147a210a45a215a88a206a128a191a183a191a185a192a138a208a81a210a185a206a131a197a132a211 a214 a192a240a201a128a197a66a208a81a216a18a191a4a197a91a211a147a210a168a215a100a192a53a199a207a206a136a200a247a206a136a210a56a205a154a210a148a206a131a197a91a211a18a191a4a197a154a201 a197a91a216a114a195a105a205a108a196a100a196a114a195a45a197a98a205a132a208a125a215 a205a154a211a12a212a83a196a114a195a45a197a91a196a18a197a14a191a185a192a119a191a185a197a132a200a106a192a246a201a128a216a100a210a168a216a100a195a168a192a119a212a98a206a131a195a45a192a96a208a125a218 a210a148a206a136a197a132a211a18a191a25a197a132a211a220a215a100a197 a214 a210a168a197a106a197a132a193a91a192a194a195a56a208a125a197a91a200a57a192a240a210a168a215a100a192a96a191a185a192a53a196a114a195a45a197a132a198a66a199a131a192a194a200a123a191a47a224 a203a29a206a131a221a88a192a10a205a96a199a207a199a26a192a125a223a100a205a154a200a106a196a88a199a136a192a47a218a252a198a18a205a91a191a185a192a96a212a147a205a154a196a114a196a100a195a45a197a14a205a91a208a81a215a100a192a96a191a47a226 a214 a192a31a191a185a216a43a69a149a192a47a195 a201a128a195a45a197a91a200 a210a45a215a114a192a34a199a217a206a131a200a16a206a131a210a168a192a138a212a220a191a180a206a21a20a47a192a59a197a154a201a150a197a132a216a100a195a54a212a100a205a154a210a56a205a154a198a12a205a91a191a185a192a91a224a26a225a64a201a26a205a247a191a180a206a131a200a16a206a131a218 a199a209a205a154a195a20a192a81a223a12a205a108a200a106a196a66a199a131a192 a214 a206a131a210a168a215a66a206a131a211a8a210a45a215a114a192a34a213a108a206a136a193a98a192a194a211a16a210a45a215a114a195a45a192a96a191a185a215a100a197a154a199a209a212a8a212a88a197a98a192a138a191a40a211a100a197a132a210 a192a81a223a88a206a209a191a185a210a96a226 a214 a192a25a208a47a205a154a211a114a211a100a197a132a210a20a205a108a196a100a196a88a199a136a222a247a197a91a216a100a195a26a200a106a192a47a210a45a215a100a197a88a212a149a224a150a190a59a211a10a206a131a211a18a208a125a195a168a192a138a205a132a191a185a192 a206a131a211a57a210a168a215a100a192a4a208a125a197a132a195a45a196a114a216a18a191a40a191a180a206a21a20a194a192a68a199a131a192a138a191a45a191a185a192a47a211a18a191a40a210a45a215a88a206a209a191a40a212a66a192a81a243a70a208a56a206a136a192a47a211a18a208a81a222a88a226a108a198a114a216a100a210a26a210a45a215a114a192 a212a66a197a132a200a16a206a131a211a18a205a154a211a114a210a171a196a100a195a168a197a91a198a66a199a131a192a47a200a112a201a252a197a132a195a25a210a168a215a100a192a31a195a45a192a47a210a45a195a148a206a136a192a47a193a98a205a138a199a149a197a108a201a40a192a81a223a12a205a108a200a106a196a66a199a131a192a138a191 a206a209a191a8a206a131a211a52a212a66a192a138a205a96a199a207a206a136a211a114a213 a214 a206a136a210a168a215a254a195a168a192a125a199a209a205a154a210a148a206a131a193a91a192a125a199a131a222a119a199a136a197a132a211a100a213a132a192a194a195a141a216a100a210a168a210a45a192a47a195a30a205a108a211a18a208a125a192a96a191 a212a66a216a114a192a59a210a45a197a53a210a45a215a114a192a183a199a209a205a132a208a30a221a247a197a154a201a6a205a10a196a114a195a45a192a47a196a100a195a45a197a66a208a81a192a138a191a45a191a180a206a131a211a100a213a57a191a185a210a168a192a194a196a16a201a252a197a132a195a4a191a185a196a88a199a217a206a131a210a168a218 a210a148a206a136a211a114a213a246a216a100a210a45a210a168a192a47a195a30a205a154a211a12a208a125a192a96a191a53a206a131a211a88a210a45a197a246a191a185a200a8a205a138a199a207a199a136a192a47a195a57a208a125a215a88a216a100a211a114a221a114a191a47a224a214a8a34a197 a214 a192a47a193a154a192a194a195a194a226 a210a168a215a100a192a240a206a209a212a88a192a194a211a100a210a148a206a207a243a70a208a47a205a154a210a148a206a136a197a132a211a102a197a154a201a79a208a30a199a209a205a154a216a12a191a185a192a138a191a4a208a81a197a91a211a114a210a30a205a96a206a136a211a88a206a136a211a114a213a247a211a100a216a114a200a106a192a194a195a30a205a96a199 a192a81a223a114a196a100a195a168a192a138a191a45a191a180a206a131a197a91a211a12a191a25a205a154a211a12a212a8a210a45a215a114a192a34a195a45a192a47a210a45a195a148a206a136a192a47a193a98a205a138a199a18a197a154a201a150a212a88a192a138a208a81a197a91a200a106a196a70a197a98a191a185a192a96a212a57a216a114a210a45a218 a210a168a192a194a195a30a205a108a211a18a208a81a192a247a201a128a195a30a205a108a213a91a200a106a192a47a211a88a210a30a191 a214 a197a132a216a66a199a209a212a141a199a136a192a96a205a91a212a220a210a168a197a220a205a154a211a147a206a136a211a12a208a125a195a45a192a96a205a91a191a185a192a10a206a131a211 a195a45a192a96a208a194a205a96a199a217a199a180a224 a24a34a211a103a210a45a215a114a192a57a197a132a210a45a215a114a192a194a195a183a215a12a205a154a211a18a212a70a226a70a206a207a201a20a210a168a215a100a192a16a216a100a210a168a210a168a192a194a195a30a205a108a211a18a208a81a192a138a191a31a205a108a195a45a192a10a210a168a197a66a197 a191a185a215a100a197a132a195a45a210a194a226a98a210a45a215a114a192a194a195a168a192a34a200a16a206a131a213a91a215a100a210a26a211a114a197a91a210a20a198a70a192a34a192a47a211a100a197a132a216a100a213a132a215a102a208a125a197a132a211a100a210a168a192a125a223a66a210a45a216a12a205a138a199a66a206a131a211a100a218 a201a128a197a91a195a45a200a123a205a108a210a148a206a136a197a132a211 a210a45a197a2a205a138a199a207a199a131a197 a214 a210a45a215a114a192a119a212a14a206a128a191a45a205a108a200a106a198a66a206a131a213a91a216a12a205a154a210a148a206a136a197a132a211a2a197a154a201a57a210a45a215a114a192 a208a81a197a91a195a45a195a168a192a138a208a81a210a106a211a114a216a100a200a106a192a47a195a30a205a138a199a34a208a56a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a123a205a91a191a45a191a180a206a131a213a91a211a114a200a106a192a194a211a114a210a96a224 a25 a197a132a195a247a192a125a223a66a218 a205a154a200a57a196a66a199a131a192a14a226a98a216a114a210a45a210a168a192a194a195a56a205a154a211a18a208a81a192a138a191a120a199a207a206a131a221a88a192 a211a64a38 a35 a231a45a230a81a230 a30 a233a235a230a47a232a154a236a125a230a126a212a183a210a45a195a185a206a131a213a132a213a91a192a47a195a25a210a168a215a100a192 a195a45a192a47a210a45a195a148a206a136a192a47a193a91a205a96a199a247a197a154a201a220a205 a199a209a205a154a195a45a213a132a192a119a211a88a216a100a200a16a198a114a192a194a195a246a197a108a201a123a192a125a223a100a205a154a200a106a196a66a199a131a192a96a191a47a226a247a198a100a216a100a210 a206a131a211a77a197a91a195a56a212a66a192a47a195a16a210a168a197a246a191a185a192a81a199a136a192a96a208a125a210a57a205a105a208a56a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a10a206a131a211a119a205a105a195a45a192a81a199a207a206a128a205a108a198a66a199a131a192 a214 a205a96a222a91a226 a208a81a197a91a211a100a210a168a192a125a223a66a210a41a206a131a211a66a201a128a197a91a195a168a200a123a205a154a210a148a206a136a197a132a211a57a198a18a192a47a222a88a197a132a211a18a212a53a210a168a215a66a206a209a191a120a216a100a210a168a210a168a192a194a195a30a205a108a211a18a208a81a192a54a206a209a191a120a195a45a192a47a218 a219 a216a88a206a131a195a45a192a138a212a70a224a40a190a183a211a100a197a91a210a168a215a100a192a47a195a4a192a81a223a12a205a108a200a106a196a66a199a131a192a53a201a128a197a91a195a4a210a168a215a100a192a31a211a100a192a47a192a138a212a102a197a154a201a26a192a125a223a66a210a45a195a56a205a154a218 a191a185a192a194a211a114a210a45a192a47a211a14a210a148a206a128a205a96a199a79a221a88a211a114a197 a214 a199a136a192a96a212a66a213a132a192a247a206a209a191a59a210a168a215a100a192a16a197a66a208a47a208a125a216a114a195a45a195a45a192a47a211a18a208a125a192a106a197a154a201a171a200a106a192a47a211a100a218 a191a185a216a100a195a56a205a138a199a79a208a56a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a30a191a47a226a12a199a217a206a131a221a98a192a10a208a81a216a100a195a45a195a168a192a194a211a18a208a56a206a131a192a138a191a51a12a148a212a88a197a154a199a207a199a209a205a154a195a240a193a114a191a47a224a183a222a91a192a194a211a88a18 a197a91a195a171a199a217a206a131a211a114a192a138a205a154a195a54a200a106a192a138a205a132a191a185a216a100a195a45a192a96a191 a12a87a200a16a206a217a199a131a192a10a193a66a191a47a224a40a221a91a206a217a199a131a197a132a200a106a192a194a210a168a192a194a195a90a18a56a224 a248a25a215a114a192a68a196a100a215a114a195a30a205a91a191a185a192a20a205a138a199a207a206a131a213a91a211a100a200a57a192a194a211a100a210a149a200a106a197a66a212a88a216a66a199a131a192a68a191a185a216a23a69a149a192a194a195a56a191a149a201a128a195a45a197a132a200 a210a45a215a114a192 a199a131a197 a214 a205a132a208a194a208a81a216a100a195a30a205a132a208a30a206a131a192a96a191a68a197a154a201a26a210a168a215a100a192a59a216a100a210a148a206a207a199a217a206a21a20a194a192a96a212a8a196a18a205a108a195a30a191a185a192a194a195a56a191a47a224a139a8a4a197 a214 a192a194a193a154a192a47a195a96a226 a210a168a215a100a192a240a206a131a211a12a208a125a197a132a195a45a196a18a197a91a195a56a205a154a210a148a206a136a197a132a211a220a197a108a201a40a196a12a205a154a195a30a191a185a192a47a195a30a191 a214 a206a131a210a168a215a141a198a70a192a194a210a168a210a168a192a194a195a171a196a149a192a47a195a185a201a128a197a91a195a168a218 a200a123a205a108a211a18a208a125a192 a214 a197a132a216a66a199a209a212a102a205a138a199a207a199a136a197 a214 a210a45a215a114a192a10a208a81a197a91a200a106a196a12a205a154a195a148a206a128a191a185a197a132a211a220a197a108a201a20a208a81a197a91a200a106a196a88a199a136a192a47a210a45a192 a191a185a192a194a211a114a210a45a192a47a211a18a208a81a192a103a191a185a210a168a195a45a216a18a208a81a210a45a216a114a195a45a192a138a191a123a210a45a195a148a206a136a213a132a213a91a192a47a195a185a206a131a211a100a213a36a206a131a200a106a196a100a195a168a197a91a193a91a192a138a212a2a205a96a199a217a206a131a213a132a211a100a218 a200a106a192a47a211a100a210a141a195a45a192a96a191a185a216a66a199a131a210a30a191a47a224 a197a105a197a91a195a45a192a47a197a91a193a91a192a47a195a96a226a4a210a168a215a100a192a103a199a128a205a132a208a125a221a178a197a108a201a106a211a100a216a114a200a106a192a194a195a56a205a138a199 a214 a197a132a195a30a212a220a208a81a197a91a195a168a195a45a192a138a191a185a196a70a197a91a211a12a212a66a192a47a211a18a208a81a192a138a191a183a208a194a205a108a216a18a191a185a192a96a212a246a210a168a215a100a192a10a201a64a205a96a206a207a199a136a216a114a195a45a192a16a197a154a201a20a210a168a215a100a192 a196a100a215a114a195a30a205a132a191a168a192a43a205a138a199a207a206a131a213a91a211a114a200a106a192a194a211a114a210a105a191a185a210a168a192a194a196a41a226a31a192a91a224a213a12a224a37a210a168a215a100a192a77a211a114a197a91a211a114a218a87a211a114a216a100a200a106a192a47a195a30a205a96a199 a216a12a191a185a192a10a197a154a201a51a211a55a31a14a227a100a230a126a212a16a206a136a211a143a211a138a232 a234a90a35 a230a30a232 a30 a230a96a231a51a31a14a227a100a230a34a212a57a12a180a205a154a211a12a205a154a196a114a215a100a197a132a195a185a206a209a208a64a18a171a197a132a195 a211a55a31a14a227a100a230a130a31a24a164 a28a81a31a14a228a88a231a59a33a91a228a66a230a47a236a34a38a128a236a184a212a149a12a180a212a66a192a47a210a45a192a47a195a45a200a16a206a131a211a100a192a47a195a90a18a30a226a53a208a81a197a91a216a66a199a209a212 a211a100a197a132a210 a198a70a192a103a205a138a199a207a206a131a213a91a211a114a192a138a212a249a206a131a211a52a205a154a211a52a205a108a196a100a196a114a195a45a197a91a196a114a195a185a206a209a205a154a210a168a192 a214 a205a96a222a91a224a254a248a25a215a100a192a47a195a45a192a81a201a252a197a132a195a45a192a91a226 a192 a219 a216a88a206a136a193a98a205a96a199a136a192a47a211a88a210a20a196a100a215a114a195a30a205a91a191a185a192a96a191a20a208a125a197a132a216a66a199a209a212a8a198a18a192a240a198a70a192a47a210a45a210a168a192a194a195a20a192a81a223a114a210a168a195a30a205a132a208a125a210a168a192a138a212a8a198a100a222 a216a12a191a180a206a136a211a114a213a123a205 a214 a197a91a195a56a212a8a205a138a199a207a206a131a213a91a211a100a200a57a192a194a211a100a210a171a200a106a192a47a210a168a215a100a197a66a212 a214 a206a136a210a168a215a75a205a16a215a88a206a136a213a132a215a220a195a45a192a47a218 a208a47a205a138a199a207a199a26a195a30a205a108a210a45a192a91a224 a202a34a216a100a200a106a192a47a195a30a205a96a199a249a208a56a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a30a191a143a22a148a216a12a212a66a213a91a192a96a212a50a205a132a208a194a208a81a192a194a196a114a210a30a205a108a198a66a199a131a192a14a226a43a206a180a224a192a91a224 a210a168a215a100a197a14a191a185a192 a197a154a201a36a195a56a205a154a211a114a221 a157a59a226a123a208a47a205a154a211a94a198a18a192 a212a14a206a131a193a14a206a209a212a66a192a96a212 a206a131a211a100a210a45a197a97a210a168a215a100a195a45a192a47a192 a213a132a195a45a197a91a216a114a196a18a191a90a101a139a12a114a61a81a18a120a191a185a192a125a199a131a192a138a208a81a210a185a206a131a197a132a211a57a197a108a201a41a210a168a215a100a192a25a213a91a192a47a211a100a192a47a195a30a205a138a199a18a208a56a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a139a245a154a80a55a82 a206a131a211a18a191a185a210a168a192a138a205a132a212a246a197a108a201a171a200a106a197a91a195a168a192a16a191a185a196a149a192a96a208a30a206a207a243a70a208a10a208a56a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a30a191a34a199a207a206a131a221a14a192a107a165a43a79a57a12a148a208a81a197a91a211a114a218 a208a81a195a45a192a47a210a45a192a10a197a132a198a43a22a148a192a138a208a81a210a56a191a131a18a56a226a132a166a195a87a23a84a122a12a53a54a70a205a108a210a4a197a132a198a39a22a148a192a138a208a81a210a30a191a34a18a56a226a66a197a91a195a13a83a36a79a81a86a92a12a252a199a131a197a91a211a114a213a12a226 a210a168a215a66a206a131a211a2a197a91a198a23a22a148a192a138a208a81a210a30a191a34a18a72a167a51a12a112a16a23a18a247a216a114a211a18a212a88a192a194a195a45a218a64a191a185a196a18a192a96a208a30a206a207a243a70a208a47a205a154a210a148a206a136a197a132a211a6a226a20a206a180a224a192a14a224a249a210a45a215a114a192 a197a132a200a16a206a128a191a45a191a180a206a131a197a132a211a250a197a154a201a240a208a56a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a30a191a47a226 a214 a215a66a206a209a208a56a215a77a200a16a206a131a213a91a215a114a210a46a198a70a192a75a205a91a208a47a208a125a192a47a196a100a210a168a218 a205a108a198a66a199a131a192a141a201a128a197a91a195a8a208a30a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a30a191a10a199a207a206a131a221a88a192a169a168a43a79a64a80a64a83a153a84a162a245a184a82a9a12a87a195a45a197a66a197a91a200a123a191a34a18a240a197a132a195a52a170a131a84a72a86 a12a53a54a100a206a131a213a91a215a114a210a30a191a34a18a72a167a159a12a112a56a23a18a53a196a149a197a108a199a207a206a136a210a168a192a194a211a114a192a138a191a45a191a47a226a40a206a180a224a192a14a224a249a205a251a208a30a199a209a205a132a191a45a191a180a206a217a243a12a192a194a195a106a197a108a201a240a210a45a215a100a192 a191a45a205a108a200a106a192a34a200a106a192a96a205a154a211a66a206a131a211a114a213a46a198a100a216a114a210a20a216a18a191a185a192a96a212a8a205a108a210a171a212a98a206a189a69a149a192a47a195a45a192a47a211a100a210a150a199a131a192a194a193a91a192a125a199a209a191a120a197a108a201a41a196a18a197a132a218 a199a207a206a131a210a45a192a47a211a100a192a96a191a45a191a47a226a68a199a207a206a131a221a88a192a171a166a111a172a55a84a201a80a64a87a80a166a57a87a18a226a142a166a111a172a1a84a45a226a54a205a154a211a12a212a42a86a62a84a201a86 a12a87a215a100a216a114a200a123a205a154a211 a198a70a192a125a206a131a211a100a213a14a191a34a18a30a224 a225a148a211a36a210a45a215a114a192a8a192a125a223a66a196a149a192a47a195a185a206a131a200a106a192a47a211a66a210a56a191a31a212a66a192a96a191a45a208a125a195a148a206a136a198a70a192a96a212a77a205a154a198a18a197a132a193a14a192a91a226a29a210a45a215a114a192a102a197a91a198a114a218 a210a56a205a138a206a131a211a100a192a96a212a251a208a56a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a30a191 a214 a192a47a195a45a192a16a205a132a208a194a208a81a192a194a196a114a210a45a192a96a212a246a198a88a199a217a206a131a211a18a212a14a199a131a222a88a226a12a206a180a224a192a91a224 a214 a192 a212a14a206a128a212a8a211a100a197a132a210a20a193a91a192a47a195a185a206a207a201a252a222a247a210a45a215a100a192a183a208a56a215a18a205a108a195a30a205a91a208a81a210a45a192a47a195a185a206a209a191a185a210a185a206a209a208a47a191a79a197a108a201a41a210a168a215a100a192a4a211a66a216a114a200a106a192a194a195a56a205a138a199 a192a81a223a114a196a114a195a45a192a138a191a45a191a180a206a131a197a91a211a103a197a154a201a79a210a45a215a114a192a53a206a131a211a100a196a100a216a114a210a240a205a154a213a14a205a138a206a131a211a18a191a185a210a183a210a168a215a100a197a14a191a185a192a10a197a154a201a171a210a168a215a100a192a16a191a185a192a47a218 a199a131a192a138a208a81a210a168a192a138a212 a208a56a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a96a224 a25 a197a91a195a246a192a125a223a100a205a154a200a106a196a88a199a136a192a91a226a247a210a45a215a114a192a2a208a56a199a128a205a132a191a45a191a180a206a207a243a18a192a194a195a56a191 a245a184a80a55a82a42a12a64a213a132a192a194a211a114a192a194a195a30a205a96a199a201a18a46a205a108a211a18a212a43a173a64a84a227a12a64a196a18a192a47a195a30a191a185a197a132a211a62a18a53a205a154a195a168a192a8a216a18a191a185a192a96a212a77a197a91a211a66a199a131a222a103a206a131a211 a208a81a197a91a211a114a211a100a192a138a208a81a210a148a206a136a197a132a211 a214 a206a136a210a168a215 a191a185a196a18a192a138a208a56a206a217a243a29a208a57a211a114a216a100a200a16a198a114a192a194a195a56a191a72a174a16a198a100a216a114a210a57a205a154a195a45a192a106a211a100a197a91a210 a205a132a208a194a208a81a192a194a196a114a210a30a205a108a198a66a199a131a192a220a201a128a197a91a195a106a199a209a205a154a195a45a213a132a192a194a195a57a211a100a216a100a200a247a198a100a192a47a195a30a191a47a224a9a197a105a197a91a195a168a192a194a197a132a193a154a192a194a195a194a226a25a210a168a215a100a192 a205a132a191a45a191a180a206a136a213a132a211a100a200a106a192a47a211a100a210a183a197a108a201a183a208a56a199a128a205a132a191a45a191a180a206a207a243a18a192a194a195a56a191a31a208a125a197a132a195a45a195a45a192a96a191a185a196a18a197a91a211a12a212a98a206a131a211a100a213a141a210a45a197a147a205a75a191a185a192a47a218 a219 a216a100a192a47a211a18a208a125a192a106a197a154a201a54a211a100a216a100a200a57a192a194a195a30a205a96a199a209a191a54a206a209a191a59a199a217a206a131a200a16a206a131a210a45a192a96a212a251a210a168a197a246a208a30a199a209a205a132a191a56a191a180a206a207a243a12a192a194a195a30a191a4a199a217a206a131a221a88a192 a168a39a79a81a80a55a83a153a84a162a245a184a82a195a12a87a195a45a197a98a197a132a200a123a191a34a18a29a197a91a195a3a170a131a87a23a86a195a12a87a196a100a215a114a197a91a211a114a192a54a211a88a216a114a200a16a198a100a192a47a195a30a191a34a18a56a226a98a205a108a211a18a212a240a206a131a210 a206a209a191a25a211a100a197a132a210a4a196a18a197a14a191a45a191a180a206a136a198a88a199a136a192a59a201a128a197a91a195a183a191a185a197a132a195a45a210a56a205a138a199a120a208a30a199a209a205a91a191a45a191a180a206a207a243a12a192a194a195a30a191a40a199a217a206a131a221a98a192a175a165a43a79a40a12a180a208a125a197a132a211a100a218 a208a81a195a45a192a47a210a45a192a10a197a132a198a43a22a148a192a138a208a81a210a56a191a131a18a56a224 a176 a50a102a90a6a0a68a63a114a71a201a4a68a188a154a61a87a90a6a0a68a188 a225a180a211a50a210a168a215a66a206a209a191a52a196a18a205a108a196a18a192a194a195 a214 a192 a196a100a195a168a197a91a196a18a197a14a191a185a192a138a212a50a205a23a211a114a192 a214 a226a251a216a100a211a88a206a217a201a128a197a132a195a45a200 a200a106a192a47a210a45a215a114a197a12a212a103a216a18a191a180a206a131a211a100a213a105a196a114a215a100a195a30a205a132a191a45a205a138a199a25a205a138a199a207a206a131a213a91a211a100a200a57a192a194a211a100a210a56a191a34a201a128a197a91a195a247a210a168a215a100a192a102a205a108a211a18a205a96a199a136a218 a222a66a191a180a206a128a191a4a197a154a201a68a208a125a197a132a195a45a195a45a192a96a191a185a196a149a197a132a211a18a212a98a206a131a211a114a213a106a211a100a216a114a200a106a192a194a195a56a205a138a199a6a196a100a215a114a195a30a205a91a191a185a192a16a208a81a197a91a211a12a191a185a210a45a195a168a216a18a208a81a218 a210a148a206a136a197a132a211a18a191a120a206a131a211a8a198a66a206a207a199a207a206a136a211a114a213a91a216a12a205a138a199a18a216a100a210a168a210a45a192a47a195a30a205a154a211a12a208a125a192a25a196a18a205a96a206a131a195a30a191a47a224a79a248a25a215a100a192a25a196a100a195a168a192a138a208a56a206a128a191a180a206a131a197a91a211 a205a108a211a18a212a249a195a168a192a138a208a47a205a138a199a207a199a34a195a30a205a154a210a168a192a138a191a46a197a154a201a240a197a91a216a114a195a16a200a57a192a194a210a168a215a100a197a100a212a249a205a108a195a45a192a46a71a81a41a100a224a182a16a19a158a245a205a108a211a18a212 a41a29a16a114a224a178a67a23a158a123a226a100a195a45a192a138a191a185a196a18a192a96a208a125a210a148a206a131a193a88a192a81a199a136a222a91a224 a248a25a215a100a192 a219 a216a88a206a136a210a168a192a57a199a131a197 a214 a195a45a192a96a208a194a205a96a199a207a199a20a206a209a191a46a212a66a216a100a192a106a210a45a197a105a210a168a215a100a192a123a212a66a192a81a243a70a208a56a206a136a192a47a211a18a208a81a222 a197a108a201a171a210a45a215a100a192a247a208a125a216a114a195a45a195a45a192a47a211a100a210a4a206a131a200a106a196a66a199a131a192a47a200a106a192a194a211a88a210a56a205a154a210a148a206a136a197a132a211a147a197a154a201a20a195a168a192a194a210a168a195a185a206a131a192a194a193a91a206a136a211a114a213a75a192a81a223a114a218 a205a108a200a106a196a66a199a131a192a138a191a123a210a45a215a12a205a154a210a141a205a108a195a45a192a103a191a180a206a131a200a16a206a217a199a209a205a108a195a102a210a168a197a43a210a168a215a100a192a220a206a131a211a100a196a114a216a100a210a123a198a114a222 a212a66a192a96a205a138a199a131a218 a206a131a211a100a213 a214 a206a136a210a168a215a147a195a45a192a81a199a128a205a108a210a185a206a131a193a88a192a81a199a131a222a8a199a131a197a91a211a114a213a91a192a47a195a183a216a100a210a45a210a168a192a47a195a30a205a154a211a12a208a125a192a96a191a47a224a185a8a34a197 a214 a192a47a193a91a192a194a195a96a226 a210a168a215a100a192a25a212a88a192a138a208a81a197a91a200a106a196a18a197a14a191a180a206a136a210a148a206a131a197a91a211a31a197a154a201a12a210a168a215a100a192a120a206a136a211a114a196a100a216a114a210a41a216a114a210a45a210a168a192a194a195a56a205a154a211a18a208a81a192a138a191 a214 a197a91a216a66a199a209a212 a206a131a200a106a196a100a195a168a197a91a193a91a192a8a210a168a215a100a192a105a208a125a197a108a193a91a192a194a195a56a205a154a213a132a192a8a197a154a201a240a197a132a216a100a195a57a191a185a222a114a191a185a210a168a192a194a200a105a224 a244 a192a8a196a66a199a209a205a108a211 a210a168a197a105a195a45a192a47a210a45a195a148a206a136a192a47a193a88a192a10a192a81a223a12a205a108a200a106a196a66a199a131a192a96a191a53a198a12a205a91a191a185a192a96a212a246a197a132a211a193a73a240a204a52a200a123a205a154a210a56a208a125a215a88a206a136a211a114a213a123a197a154a201 a196a114a215a100a195a30a205a132a191a185a192a138a191a25a206a136a211a12a191a185a210a45a192a96a205a91a212a102a197a154a201a79a208a125a197a132a200a106a196a66a199a131a192a47a210a45a192a46a191a185a192a194a211a114a210a45a192a47a211a18a208a81a192a138a191a47a224 a24a34a211a100a192a16a197a108a201a25a210a168a215a100a192a16a200a123a205a96a206a136a211a36a195a45a192a138a205a132a191a185a197a91a211a12a191a34a201a252a197a132a195a59a206a131a211a18a208a81a197a91a195a168a195a45a192a138a208a81a210a53a208a30a199a209a205a91a191a45a191a180a206a131a218 a243a12a192a194a195a171a191a185a192a81a199a136a192a96a208a125a210a148a206a131a197a91a211a10a206a128a191a150a210a45a215a114a192a171a199a209a205a91a208a81a221a31a197a154a201a149a208a81a197a91a211a88a210a168a192a125a223a66a210a45a216a12a205a138a199a100a221a88a211a114a197 a214 a199a136a192a96a212a66a213a132a192 a177a9a178a45a179a18a180a182a181a51a183a34a167a112a164a70a167a112a168a72a169a114a170a5a184a23a165a70a175a162a167a184a183a37a185a189a186a114a168a108a186a20a187a52a188a189a180a191a190a192a187a52a193a91a194a196a195a72a197a154a181a5a198a68a167a112a168a72a175a162a186a126a164a69a184a23a165a81a175a201a167a184a183 a185a189a186a114a168a12a186a20a187a60a188a189a180a191a190a199a187a101a200 a214 a206a131a210a45a215a88a206a131a211a102a191a185a215a100a197a132a195a45a210a20a216a114a210a45a210a168a192a194a195a56a205a154a211a18a208a81a192a138a191a47a226 a214 a215a88a206a128a208a81a215a247a199a131a192a96a205a91a212a100a191a40a210a168a197a247a205a154a200a247a198a66a206a131a213a132a216a100a218 a206a131a210a185a206a131a192a138a191a25a210a168a215a18a205a108a210a240a208a47a205a154a211a100a211a114a197a91a210a54a198a18a192a16a191a185a197a154a199a131a193a91a192a138a212 a214 a206a131a210a45a215a114a197a91a216a114a210a4a210a168a215a100a192a46a208a125a197a132a211a100a210a168a192a125a223a66a210 a206a131a211 a214 a215a88a206a128a208a81a215a57a210a168a215a100a192a183a191a185a192a194a211a88a210a45a192a47a211a18a208a81a192a68a206a128a191a40a216a100a210a168a210a168a192a194a195a45a192a96a212a149a224a75a197a105a197a91a195a168a192a194a197a108a193a91a192a194a195a194a226a154a206a131a210a40a206a209a191 a211a100a192a96a208a125a192a96a191a45a191a45a205a154a195a168a222a16a210a45a197a53a193a14a192a47a195a185a206a207a201a128a222a10a210a168a215a100a192a183a208a125a215a18a205a108a195a30a205a132a208a125a210a168a192a194a195a148a206a128a191a185a210a148a206a128a208a47a191a120a197a108a201a41a211a88a216a100a200a106a192a47a195a45a218 a205a138a199a209a191a183a205a154a211a12a212 a219 a216a18a205a108a211a100a210a148a206a207a243a18a192a138a212a102a211a100a197a132a216a100a211a18a191a183a210a45a197a141a205a108a193a154a197a154a206a209a212a141a206a131a211a18a208a81a197a91a200a106a196a12a205a154a210a148a206a136a198a88a199a136a192 a205a91a191a45a191a180a206a131a213a132a211a100a200a106a192a47a211a100a210a56a191a194a224 a73a183a192a138a191a185a196a88a206a136a210a168a192a102a210a45a215a114a192a138a191a185a192a123a191a168a215a114a197a91a195a168a210a30a208a81a197a91a200a16a206a131a211a100a213a14a191a47a226a150a197a91a216a114a195a247a205a154a196a114a196a100a195a45a197a14a205a91a208a81a215a246a206a209a191 a214 a206a209a212a66a192a81a199a131a222 a205a154a196a114a196a66a199a207a206a128a208a47a205a154a198a88a199a136a192a103a206a136a211a52a193a98a205a154a195a148a206a136a197a132a216a18a191a106a211a18a205a108a210a45a216a114a195a30a205a138a199a53a199a128a205a108a211a100a213a132a216a18a205a108a213a91a192 a205a154a196a114a196a66a199a207a206a209a208a194a205a108a210a185a206a131a197a91a211a12a191a47a224 a25 a216a100a195a45a210a168a215a100a192a47a195a45a200a106a197a132a195a45a192a91a226a70a206a131a210a31a206a209a191a31a211a100a197a91a210a240a199a207a206a136a200a16a206a131a210a168a192a138a212a77a210a45a197 a210a168a215a100a192a77a210a56a205a91a191a185a221a83a192a194a193a91a205a96a199a136a216a12a205a154a210a168a192a138a212a2a215a100a192a47a195a45a192a77a198a114a216a100a210a220a208a47a205a154a211 a198a18a192a77a192a81a223a114a210a168a192a47a211a18a212a66a192a96a212 a210a168a197a178a210a45a215a114a192 a214 a197a132a195a30a212a52a212a98a206a209a191a45a205a154a200a106a198a88a206a136a213a132a216a18a205a108a210a185a206a131a197a91a211a52a197a154a201a57a211a100a216a114a200a106a192a194a195a56a205a138a199a53a211a100a197a132a216a100a211 a196a100a215a114a195a30a205a132a191a168a192a96a191a75a197a91a195a246a192a194a193a91a192a47a211 a197a91a210a168a215a100a192a47a195a75a199a207a206a136a211a114a213a91a216a88a206a128a191a185a210a148a206a209a208a246a201a128a192a138a205a108a210a45a216a100a195a168a192a138a191a47a226a240a199a217a206a131a221a91a192 a227a100a228a66a229a169a201a30a230a96a231a81a226a97a38a252a230a96a227a114a236a81a230a96a226a114a197a132a195a46a229a98a31a55a26a14a232a14a233a235a237a96a38a162a28a154a226a114a210a168a215a18a205a108a210a183a205a108a195a45a192a31a211a100a197a132a210a4a192a81a223a114a196a66a199a207a206a209a208a125a218 a206a131a210a185a199a131a222a106a192a125a223a66a196a100a195a45a192a96a191a45a191a185a192a138a212a10a206a136a211a102a197a132a211a100a192a54a199a209a205a154a211a114a213a91a216a12a205a154a213a132a192a240a198a114a216a100a210a79a197a91a198a66a199a207a206a131a213a98a205a108a210a45a197a132a195a45a222a16a206a131a211 a210a168a215a100a192a53a197a132a210a168a215a100a192a194a195a171a199a128a205a108a211a100a213a132a216a18a205a108a213a91a192a91a224 a73a251a63a80a202a155a0a68a90a99a203a246a71a87a69a39a2a75a191a192a49a83a69a153a0a6a189a132a188 a248a25a215a114a192a2a205a108a216a100a210a168a215a100a197a91a195a56a191 a214 a197a132a216a66a199a209a212a254a199a217a206a131a221a88a192a119a210a168a197 a210a168a215a18a205a108a211a100a221a156a194a53a205a91a212a88a197a91a213a14a205 a214 a205a154a218 a14a14a215a114a197a91a210a168a192a194a211a2a201a128a197a91a195a123a196a100a195a168a197a91a193a14a206a209a212a14a206a136a211a114a213a43a216a18a191 a214 a206a131a210a45a215a52a210a45a215a114a192a17a203a54a216a66a206a131a213a91a197a132a218a154a14a91a215a66a206a131a211a100a218 a239a206a131a210a45a192a47a211 a210a45a215a100a192a96a191a45a205a154a216a114a195a45a216a12a191a194a224 a248a25a215a114a192a249a195a168a192a138a191a185a192a96a205a154a195a30a208a81a215 a195a45a192a47a196a70a197a91a195a45a210a168a192a138a212 a215a114a192a194a195a45a192 a214 a205a132a191 a191a185a216a114a196a100a196a18a197a91a195a168a210a45a192a96a212a245a206a131a211 a196a18a205a108a195a45a210a97a198a66a222a245a205a58a208a81a197a91a211a100a210a168a195a30a205a132a208a125a210 a214 a206a136a210a168a215 a210a168a215a100a192a106a248a40a192a125a199a131a192a138a208a81a197a91a200a106a200a46a216a114a211a66a206a209a208a194a205a108a210a185a206a131a197a132a211a18a191a59a190a31a212a88a193a91a205a154a211a12a208a125a192a47a200a106a192a194a211a88a210a227a24a4a195a45a213a98a205a108a211a66a206a131a218 a20a96a205a154a210a148a206a136a197a132a211a220a197a108a201 a239a205a108a196a18a205a154a211a75a192a194a211a114a210a185a206a131a210a148a199a136a192a96a212a149a226a205a204a30a206a112a236a34a38a87a228a23a26a29a28a173a31a24a164a31a236 a30 a230a45a230 a234a90a35 a26a14a237a11a10 a232a14a233a116a31a90a33a91a228a66a230a202a38a87a231a45a232a14a227a66a236a125a233a235a232a68a38a87a237a96a31a14a227a198a38a252a230 a234a90a35 a227a39a31a14a233a116a31a32a33a68a28a207a201a168a232a132a236a125a230a126a26a193a31a14a227a254a232a2a233a235a232a14a231a162a33a91a230 a234 a31a14a231 a30 a228a98a236a7a208a134a209a194a224 a152 a69a23a223a30a69a100a89a91a69a39a0a68a63a114a69a100a188 a194a57a224a171a190a31a212a114a205a154a200a123a191a75a205a108a211a18a212a52a202a10a224a159a60a20a197a132a211a100a221a91a199a217a206a131a211a41a224a130a61a66a63a23a58a55a56a114a224a147a248a120a197 a214 a205a108a195a30a212a114a191a106a205 a210a168a215a100a192a194a197a132a195a45a222a178a197a154a201a53a211a18a205a154a210a168a216a100a195a56a205a138a199a10a208a56a199a128a205a132a191a45a191a180a206a217a243a29a208a194a205a108a210a185a206a131a197a91a211a41a224a141a225a148a211a211a210a171a231a184a31 a234 a208a77a31a24a164 a38 a35 a230a169a212a64a38 a35 a229a57a230a30a230a1a38a252a237a209a227a29a33a173a31a24a164a51a38 a35 a230a214a213 a35 a237 a234 a232a1a33a29a31a88a215a120a237a209a227a29a33a91a228a66a237a217a236a131a38a252a237 a234 a25a36a31a70a10 a234 a237a209a230a1a38a162a28a154a226a12a196a12a205a154a213a132a192a138a191a51a61a9a216a97a61a1a67a66a226 a91 a211a66a206a131a193a88a192a47a195a30a191a180a206a131a210a64a222a123a197a154a201a142a60a20a215a66a206a209a208a194a205a108a213a91a197a100a224 a200a16a224a96a190a240a191a45a205a154a215a66a206a131a197a132a221a91a205a114a226a55a8a10a224a64a8a54a206a131a195a30a205a108a221a91a205 a214 a205a66a226a194a205a154a211a18a212a100a14a12a224a96a190a59a200a123a205a108a211a100a197a100a224a139a61a66a63a68a63a70a67a114a224 a14a14a192a47a200a123a205a154a211a114a210a185a206a209a208a31a208a30a199a209a205a91a191a45a191a180a206a207a243a70a208a47a205a154a210a148a206a131a197a91a211a147a205a108a211a18a212a75a205a154a211a103a205a108a211a18a205a138a199a131a222a19a20a125a206a131a211a100a213a8a191a185a222a114a191a185a218 a210a168a192a194a200a62a197a108a201a239a205a154a196a18a205a108a211a100a192a96a191a185a192a59a211a100a216a114a200a106a192a194a195a185a206a209a208a47a205a138a199a18a192a125a223a66a196a100a195a45a192a96a191a45a191a180a206a136a197a132a211a18a191a47a224a37a22a24a210a12a25a99a217 a25a154a22a7a218a220a219a13a31a68a38a87a230a47a236a81a226a153a63a70a67a154a218a64a202a59a203a149a218a154a58a55a71a94a101a189a61a66a16a68a63a91a216a36a61a1a56a70a76a114a224 a25 a224a97a157a68a197a91a211a12a212a251a205a108a211a18a212a59a194a57a224a149a204a40a205a96a206a136a221a29a224a185a16a68a67a70a67a68a67a66a224a185a203a4a192a47a216a18a191a180a206a131a211a100a213a141a205a108a211a251a197a132a211a100a210a168a197a154a199a131a218 a197a91a213a132a222a103a210a168a197a105a213a91a192a47a211a100a192a47a195a30a205a108a210a45a192a16a211a100a216a114a200a106a192a194a195a56a205a138a199a79a208a56a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a30a191a47a224a31a225a180a211a221a210a171a231a114a31 a234 a208 a31a24a164a227a38 a35 a230a107a222a189a223a64a38 a35 a213a142a224a37a215a108a22a24a219a39a218a4a226a98a196a18a205a108a213a91a192a96a191a159a63a68a67a91a216a23a63a68a76a66a226a153a243a183a192a47a195a45a200a123a205a108a211a100a222a91a224 a25 a224a19a157a68a197a91a211a12a212a149a226a29a194a57a224a94a24a4a213a91a216a100a195a56a205a114a226a14a205a154a211a18a212a89a14a18a224a132a225a148a221a91a192a47a215a18a205a154a195a56a205a114a224a142a61a66a63a68a63a70a76a114a224a139a60a40a199a209a205a91a191a185a218 a191a180a206a207a243a18a192a47a195a30a191a34a206a131a211 a239a205a108a196a18a205a108a211a100a192a138a191a185a192a47a218a252a210a45a197a132a218a180a241a40a211a100a213a108a199a207a206a128a191a185a215a103a200a123a205a91a208a81a215a66a206a131a211a100a192a31a210a45a195a56a205a154a211a12a191a185a218 a199a209a205a154a210a148a206a136a197a132a211a6a224a240a225a180a211a207a210a171a231a114a31 a234 a208a75a31a24a164a196a38 a35 a230a225a222a189a226a64a38 a35 a213a142a224a158a215a227a22a24a219a39a218a34a226a29a196a12a205a154a213a132a192a138a191 a61a1a16a70a45a91a216a36a61a1a56a70a67a114a226a6a60a79a197a91a196a18a192a47a211a100a215a18a205a108a213a91a192a47a211a6a226a39a73a183a192a194a211a114a200a123a205a154a195a45a221a29a224 a8a10a224a192a60a20a197a132a195a45a200a106a192a47a211a6a226a192a60a183a224a149a203a149a192a125a206a209a191a185a192a47a195a30a191a185a197a91a211a41a226a149a205a154a211a18a212a103a203a79a224a62a203a25a206a131a193a98a192a138a191a185a210a194a224a51a61a66a63a68a63a70a76a114a224 a22a56a227a39a38a87a231a32a31a55a26a14a228 a234 a38a252a237a96a31a14a227a214a38a201a31a107a206a46a233a33a68a31a14a231a81a237a96a38 a35 a229a10a236a125a224a155a197a105a225a185a248 a204a40a195a45a192a138a191a45a191a47a224 a244 a224a105a60a79a195a45a197a154a201a128a210a194a224a196a61a1a63a70a63a81a41a100a224a98a14a91a192a194a200a123a205a108a211a100a210a148a206a209a208a247a216a114a211a66a206a131a193a88a192a47a195a30a191a45a205a138a199a209a191a34a206a131a211 a208a30a199a209a205a91a191a45a191a180a206a131a218 a243a18a192a47a195a240a191a185a222a114a191a185a210a168a192a194a200a123a191a47a224a229a228a214a31a14a231a184a26a14a226a43a41a19a45a43a101a189a61a90a41a29a45a91a216a97a61a64a58a19a61a132a224 a204a40a224a62a73a183a197 a214 a211a66a206a131a211a100a213a100a224a227a61a1a63a70a63a68a76a66a224a56a219a240a228a66a229a16a230a96a231a185a232a14a233a37a213a79a233a235a232a132a236a56a236a125a237a238a171a230a96a231a88a25a36a28a138a236a131a38a252a230a96a229a52a230 a231 a35 a230a232a213a79a232a154a236a125a230 a31a24a164a233a217a66a232 a30 a232a14a227a100a230a47a236a125a230a96a224 a239a197a91a215a100a211 a157a171a192a47a211a70a22a168a205a154a200a247a206a136a211a114a192a138a191a47a226 a190a59a200a123a191a185a210a168a192a194a195a56a212a100a205a154a200a220a224 a194a57a224a26a225a148a200a8a205a154a200a106a216a100a195a56a205a114a224a100a16a70a67a68a67a94a61a132a224a196a8a25a206a136a192a47a195a30a205a108a195a30a208a125a215a88a206a128a208a47a205a138a199a25a196a100a215a100a195a56a205a91a191a185a192a105a205a138a199a207a206a136a213a132a211a100a218 a200a106a192a194a211a114a210a106a215a12a205a154a195a168a200a106a197a91a211a66a206a21a20a47a192a138a212 a214 a206a136a210a168a215a2a196a18a205a108a195a30a191a180a206a131a211a100a213a100a224a75a225a180a211a234a210a171a231a114a31 a234 a208a102a31a24a164 a219a49a215a108a210a12a235a58a25a37a236a237a104a222a154a226a66a196a12a205a154a213a91a192a96a191a159a56a23a58a70a58a96a216a23a56a68a71a64a41a12a226a100a248a40a197a91a221a98a222a154a197a12a226a239a205a108a196a18a205a108a211a6a224 a8a10a224a100a194a53a205a64a22a180a206a180a226a196a200a16a224 a194a183a206a128a212a114a205a114a226a220a205a154a211a12a212a138a200a16a224a100a197a105a197a91a195a148a206a136a200a106a197a132a210a168a197a12a224 a61a66a63a68a63a68a16a66a224 a203a149a192a138a205a108a195a45a211a66a206a131a211a100a213a183a210a168a195a30a205a108a211a18a191a180a199a209a205a154a210a148a206a131a197a91a211a16a210a45a192a47a200a106a196a66a199a209a205a154a210a168a192a138a191a70a201a128a195a45a197a91a200a97a198a66a206a207a199a217a206a131a211a114a213a91a216a18a205a96a199 a210a45a192a81a223a114a210a194a224a26a225a148a211a33a210a171a231a114a31 a234 a208a6a31a116a164a227a38 a35 a230a169a222a96a238a94a38 a35 a213a142a224a158a215a227a22a24a219a39a218a34a226 a25 a195a30a205a108a211a18a208a81a192a14a224 a197a43a224a97a194a34a206a136a210a56a205a154a200a106a216a114a195a30a205a57a205a154a211a18a212a89a200a16a224a97a197a246a205a108a210a30a191a185a216a114a200a106a197a91a210a168a197a12a224a77a61a1a63a70a63a68a45a114a224a79a190 a200a123a205a154a218 a208a125a215a88a206a136a211a114a192a25a210a45a195a30a205a108a211a18a191a180a199a209a205a154a210a148a206a131a197a91a211a102a191a185a222a66a191a185a210a45a192a47a200a23a198a18a205a91a191a185a192a96a212a10a197a91a211a8a210a168a195a30a205a108a211a18a191a180a199a209a205a154a210a148a206a136a197a132a211 a195a45a216a66a199a131a192a96a191a68a205a132a208 a219 a216a66a206a131a195a45a192a96a212a46a201a128a195a45a197a132a200a23a196a18a205a154a195a56a205a138a199a207a199a131a192a125a199a70a208a125a197a132a195a45a196a18a197a91a195a56a205a114a224a150a225a180a211a101a210a171a231a114a31 a234 a208 a31a24a164a239a235a4a230 a234 a230a138a227a39a38a157a206a51a26a189a240a96a232a91a227 a234 a230a47a236a240a237a136a227a171a219a49a215a108a210a171a226a100a196a18a205a154a213a132a192a138a191a159a16a19a58a96a216a23a56a68a76a66a224 a200a16a224a224a197a246a205a108a210a30a191a185a216a100a200a57a197a91a210a168a197a12a226a100a8a10a224a240a225a168a191a185a215a88a206a131a200a106a197a91a210a168a197a12a226a57a205a154a211a12a212 a248a31a224 a91 a210a30a191a185a216a114a195a45a197a12a224 a61a1a63a70a63a68a56a66a224a46a14a14a210a168a195a45a216a18a208a81a210a45a216a114a195a30a205a96a199a183a200a123a205a108a210a30a208a81a215a66a206a131a211a100a213a246a197a108a201a31a196a12a205a154a195a56a205a138a199a207a199a136a192a81a199a240a210a168a192a125a223a66a210a30a191a47a224 a225a148a211a33a210a171a231a114a31 a234 a208 a31a24a164a227a38 a35 a230a107a241a242a222a81a38 a35 a206a169a213a122a215a120a226a98a196a18a205a108a213a91a192a96a191 a16a70a56a91a216a23a56a68a67a66a224 a190a16a224a13a197a105a192a47a222a88a192a47a195a30a191a47a226a224a203a53a224a159a200a25a205a108a195a45a211a114a213a98a205a154a198a18a192a47a195a96a226a53a205a154a211a12a212a85a203a46a224a13a243a183a195a185a206a209a191a185a215a114a200a123a205a154a211a6a224 a61a1a63a70a63a68a76a66a224a106a190a34a199a207a206a131a213a91a211a114a200a106a192a194a211a114a210a247a197a154a201a247a191a185a215a12a205a154a195a45a192a96a212a43a201a128a197a91a195a168a192a138a191a185a210a56a191a10a201a252a197a132a195a106a198a66a206a207a199a207a206a131a211a100a218 a213a91a216a12a205a138a199a79a208a81a197a91a195a168a196a149a197a132a195a30a205a114a224a171a225a180a211a243a210a171a231a114a31 a234 a208a105a31a116a164a98a38 a35 a230a53a222a69a226a81a38 a35 a213a142a224a158a215a227a22a18a219a39a218a4a226 a196a18a205a108a213a91a192a96a191a122a41a19a76a70a67a91a216a19a41a29a76a68a45a66a226a88a60a79a197a91a196a18a192a47a211a100a215a18a205a108a213a91a192a47a211a6a226a36a73a59a192a47a211a100a200a123a205a108a195a45a221a70a224 a14a12a224a88a24a34a215a114a211a100a197a8a205a154a211a12a212a214a197a43a224a39a8a59a205a154a200a123a205a108a211a66a206a209a191a185a215a66a206a180a224a185a61a1a63a70a71a81a41a100a224a158a235a183a228a66a237a116a33a29a31a81a10a136a25 a35 a237a209a227a43a10 a217a66a237a210a38a252a230a96a227a12a224a75a194a53a205a132a212a66a197a91a221a98a205 a214 a205a66a224 a203a53a224a94a93a4a216a66a206a131a195a45a221a70a226a19a14a12a224a19a243a183a195a45a192a47a192a194a211a114a198a18a205a154a216a114a200a105a226a19a243a247a224a132a203a149a192a194a192a96a208a81a215a6a226a108a205a108a211a18a212 a239a224a19a14a91a193a91a205a154a195a45a218 a210a45a193a91a206a131a221a70a224a124a61a66a63a68a71a70a45a114a224a243a206a244a213a75a31a14a229 a30 a231a185a230 a35 a230a96a227a66a236a125a237a45a240a138a230a192a218a4a231a56a232a14a229a16a229a16a232a14a231a59a31a116a164 a38 a35 a230a88a245a171a227a29a33a91a233a235a237a131a236 a35 a215a6a232a14a227a29a33a14a228a88a232a55a33a91a230a96a224a26a203a149a197a91a211a100a213a132a200a123a205a154a211a41a226a100a241a68a191a45a191a185a192a81a223a41a224 a14a12a224a196a203a171a206a128a208a81a215a18a205a108a195a30a212a114a191a168a197a132a211a6a226 a244 a224a196a73a183a197a108a199a128a205a108a211a6a226a105a190a247a224a196a197a105a192a47a211a100a192a90a20a194a192a96a191a47a226a246a205a154a211a12a212 a239a224a46a204a120a206a131a211a100a221a98a215a18a205a108a200a105a224 a16a70a67a68a67a43a61a91a224a83a190a31a208a81a215a66a206a131a192a194a193a91a206a136a211a114a213a254a208a125a197a132a200a106a200a106a192a194a195a56a208a30a206a209a205a138a199a131a218 a219 a216a12a205a138a199a207a206a136a210a168a222a59a210a168a195a30a205a108a211a18a191a180a199a209a205a154a210a148a206a131a197a91a211 a214 a206a131a210a168a215a57a192a81a223a12a205a108a200a106a196a66a199a131a192a194a218a252a198a18a205a132a191a185a192a138a212a53a200a106a192a47a210a45a215a114a218 a197a66a212a100a191a47a224a97a225a180a211a246a210a171a231a32a31 a234 a208a40a31a24a164a198a38 a35 a230a234a247a246a232 a234a66a35 a237a209a227a100a230 a231 a231a185a232a14a227a66a236a125a233a235a232a29a38a252a237a96a31a14a227 a25a29a228a88a229a16a229a16a237a210a38a233a248a157a22a18a22a24a22a56a226a57a196a18a205a108a213a91a192a96a191a42a16a70a63a68a56a16a216a43a16a70a63a23a58a14a226a187a14a88a205a108a211a100a210a148a206a128a205a108a213a91a197 a212a66a192 a60a20a197a91a200a57a196a18a197a98a191a185a210a168a192a125a199a209a205a114a226a97a14a14a196a12a205a138a206a131a211a6a224 a7 a224a208a14a14a197a132a195a45a211a66a199a131a192a194a195a168a210a185a199a209a205a154a200a57a193a154a205a108a211a66a206a209a208a56a215a6a226 a244 a224 a204a20a205a154a211a14a210a56a205a91a208a56a215a12a205a154a210a194a226 a205a154a211a12a212 a14a18a224a62a197a105a192a47a221a88a211a18a205a108a193a14a206a131a211a6a224a13a61a66a63a68a63a64a41a12a224a13a60a40a199a209a205a91a191a45a191a180a206a207a243a18a192a47a195a31a205a132a191a45a191a180a206a131a213a91a211a100a200a106a192a47a211a100a210a34a198a100a222 a208a125a197a132a195a45a196a100a216a12a191a185a218a87a198a12a205a91a191a185a192a96a212a77a205a154a196a114a196a100a195a45a197a14a205a91a208a81a215a6a224a54a225a148a211a249a210a171a231a114a31 a234 a208a75a31a24a164a196a38 a35 a230a225a222a189a250a64a38 a35 a213a142a224a158a215a227a22a24a219a39a218a34a226a66a196a18a205a108a213a91a192a96a191a51a61a1a45a70a16a91a216a97a61a66a45a68a63a66a226a39a194a240a222a88a197a132a210a45a197a100a226a239a205a154a196a18a205a108a211a6a224 a241a4a224a6a14a91a216a100a200a16a206a131a210a30a205a57a205a154a211a12a212a214a8a10a224a66a225a87a206a128a212a114a205a114a224a185a61a1a63a70a63a94a61a132a224a171a241a120a223a114a196a18a192a47a195a185a206a131a200a106a192a194a211a114a210a30a191a171a205a154a211a18a212 a196a100a195a45a197a14a191a185a196a18a192a138a208a81a210a30a191a4a197a108a201a25a192a81a223a12a205a108a200a106a196a66a199a131a192a194a218a252a198a12a205a91a191a185a192a138a212a220a200a8a205a91a208a81a215a66a206a131a211a100a192a31a210a45a195a30a205a108a211a18a191a180a199a209a205a154a218 a210a185a206a131a197a132a211a6a224a106a225a180a211a199a210a171a231a114a31 a234 a208a142a31a24a164a46a38 a35 a230a171a251a104a212a64a38 a35 a206a39a213a122a215a26a226a150a196a12a205a154a213a132a192a138a191a57a61a66a71a68a45a91a216 a61a1a63a70a16a114a224 a241a4a224a198a14a14a216a100a200a247a206a136a210a56a205a114a224 a16a70a67a68a67a43a61a91a224 a241a120a223a12a205a108a200a106a196a66a199a131a192a194a218a252a198a18a205a132a191a185a192a138a212a32a200a123a205a91a208a81a215a66a206a131a211a100a192 a210a45a195a56a205a154a211a12a191a148a199a209a205a108a210a185a206a131a197a91a211a106a216a18a191a180a206a131a211a100a213a15a73a240a204a40a218a87a200a123a205a108a210a30a208a56a215a88a206a136a211a114a213a240a198a149a192a47a210 a214 a192a47a192a194a211 a214 a197a132a195a30a212 a191a185a192 a219 a216a100a192a47a211a18a208a81a192a138a191a47a224a54a225a148a211a221a210a171a231a114a31 a234 a208a155a31a24a164a100a38 a35 a230a53a241a242a212a81a38 a35 a206a214a213a27a215a227a252a56a228a214a31a14a231a72a253a1a10 a236 a35 a31 a30 a230 a23a53a232a68a38a87a232a81a10a136a23a53a231a81a237a45a240a96a230a138a227a254a247a246a230a1a38 a35 a31a55a26a154a236a50a237a209a227a254a247a246a232 a234a90a35 a237a136a227a100a230 a231 a231a185a232a14a227a66a236a125a233a235a232a29a38a252a237a96a31a14a227a100a226a98a196a18a205a108a213a91a192a96a191a51a61a20a216a43a71a66a226a29a248a150a197a91a216a66a199a131a197a132a216a18a191a185a192a14a226 a25 a195a30a205a154a211a12a208a125a192a91a224 a248a31a224a53a248a20a205a108a221a154a192a66a20a96a205 a214 a205a66a226a247a241a4a224a15a14a14a216a100a200a16a206a131a210a56a205a114a226 a25 a224a15a14a14a216a100a213a14a205a154a222a91a205a66a226a100a8a10a224a13a200a25a205a108a218 a200a123a205a154a200a106a197a132a210a45a197a100a226a183a205a108a211a18a212 a14a18a224a75a200a4a205a154a200a123a205a108a200a106a197a91a210a168a197a12a224a195a16a70a67a68a67a70a16a114a224a246a248a150a197 a214 a205a108a195a30a212 a205a75a198a100a195a168a197a98a205a132a212a66a218a64a208a125a197a108a193a91a192a194a195a56a205a154a213a132a192a57a198a88a206a217a199a207a206a131a211a100a213a132a216a18a205a138a199a68a208a81a197a91a195a168a196a100a216a18a191a59a201a128a197a91a195a53a191a185a196a149a192a47a192a138a208a56a215 a210a45a195a56a205a154a211a12a191a148a199a209a205a108a210a185a206a131a197a91a211a75a197a154a201a26a210a45a195a56a205a96a193a14a192a81a199a6a208a81a197a91a211a100a193a108a192a194a195a30a191a45a205a108a210a185a206a131a197a132a211a18a191a79a206a131a211a220a210a168a215a100a192a31a195a45a192a96a205a138a199 a214 a197a91a195a148a199a128a212a70a224a150a225a180a211a255a210a171a231a32a31 a234 a208a97a31a24a164a227a38 a35 a230a107a241a91a231a114a26a229a215a227a235 a245a49a213a79a226a98a196a12a205a154a213a132a192a138a191a51a61a131a41a94a58a96a216 a61a1a45a70a16a114a226a100a203a6a205a91a191a4a204a20a205a138a199a131a200a123a205a132a191a47a226a153a14a14a196a18a205a96a206a131a211a6a224 a194a57a224a75a200a4a205a154a200a123a205a108a200a106a197a91a210a168a197a43a205a108a211a18a212 a200a247a224a142a197a246a205a154a210a56a191a185a216a100a200a106a197a132a210a45a197a100a224a214a16a68a67a70a67a68a67a114a224a141a190a31a208a81a218 a219 a216a88a206a128a191a180a206a131a210a148a206a136a197a132a211a77a197a154a201a183a196a114a215a100a195a30a205a132a191a185a192a194a218a209a199a131a192a194a193a98a192a125a199a79a198a88a206a217a199a207a206a131a211a100a213a91a216a12a205a138a199a34a208a81a197a91a195a168a195a45a192a138a191a185a196a18a197a132a211a100a218 a212a66a192a47a211a18a208a81a192a220a216a12a191a180a206a136a211a114a213a43a212a66a192a47a196a18a192a194a211a12a212a66a192a47a211a18a208a81a222 a191a185a210a168a195a45a216a18a208a81a210a45a216a114a195a45a192a91a224a106a225a180a211a199a210a171a231a114a31 a234 a208 a31a24a164a202a38 a35 a230a221a222a189a223a81a38 a35 a213a58a224a37a215a108a22a24a219a39a218a4a226a20a196a12a205a154a213a91a192a96a191a57a63a70a56a68a56a16a216a43a63a68a56a70a63a114a226a13a14a88a205a132a205a154a195a45a218 a198a100a195a45a216a114a192a138a208a81a221a154a192a47a211a6a226a97a243a183a192a47a195a45a200a123a205a154a211a88a222a98a224 K. Adams and N. Conklin .</sentence>
				<definiendum id="0">Performance ( % ) a159 Distance Threshold precision</definiendum>
			</definition>
			<definition id="1">
				<sentence>Learning Translation templates from bilingual text .</sentence>
				<definiendum id="0">Learning Translation</definiendum>
			</definition>
</paper>

		<paper id="1074">
			<definition id="0">
				<sentence>For these reasons , TCFP can be a useful classifier in the areas , which need a fast and high-performance text categorization task .</sentence>
				<definiendum id="0">TCFP</definiendum>
				<definiens id="0">need a fast and high-performance text categorization task</definiens>
			</definition>
			<definition id="1">
				<sentence>The main computation is the on-line scoring of all training documents , in order to find the k nearest neighbors of a test document .</sentence>
				<definiendum id="0">main computation</definiendum>
				<definiens id="0">the on-line scoring of all training documents , in order to find the k nearest neighbors of a test document</definiens>
			</definition>
			<definition id="2">
				<sentence>The instance pruning technique is one of the most straightforward ways to speed classification in a nearest neighbor system .</sentence>
				<definiendum id="0">instance pruning technique</definiendum>
				<definiens id="0">one of the most straightforward ways to speed classification in a nearest neighbor system</definiens>
			</definition>
			<definition id="3">
				<sentence>The weight of term t in document d is calculated as follows : d nNdttf dtw t r r r ) /log ( ) ) , ( log1 ( ) , ( ×+ = ( 2 ) where i ) ) , ( dtw r is the weight of term t in document d r ii ) ) , ( dttf r is the within-document Term Frequency ( TF ) iii ) ) /log ( t nN is the Inverted Document Frequency ( IDF ) iv ) N is the number of documents in the training set v ) n t is the number of training documents in which t occurs vi ) ∑ ∈ = dt dtwd r rr 2 ) , ( is the 2-norm of vector d r Given an arbitrary test document d , thek-NN classifier assigns a relevance score to each candidate category c j using the following formula : ∑ ∈′ ′= jk DdRd j dddcs I rr rrr ) ( ) , cos ( ) , ( ( 3 ) where ) ( dR k r denotes a set of the k nearest neighbors of document d and D j is a set of training documents in class c j .</sentence>
				<definiendum id="0">dtw r</definiendum>
				<definiendum id="1">dttf r</definiendum>
				<definiendum id="2">N</definiendum>
				<definiens id="0">The weight of term t in document d is calculated as follows : d nNdttf dtw t r r r ) /log ( ) ) , ( log1 ( )</definiens>
				<definiens id="1">the weight of term t in document d r ii</definiens>
				<definiens id="2">the within-document Term Frequency ( TF ) iii ) ) /log ( t nN is the Inverted Document Frequency ( IDF ) iv )</definiens>
				<definiens id="3">the number of documents in the training set v</definiens>
				<definiens id="4">the number of training documents in which t occurs vi ) ∑ ∈ = dt dtwd r rr 2 )</definiens>
				<definiens id="5">the 2-norm of vector d r Given an arbitrary test document d , thek-NN classifier assigns a relevance score to each candidate category c j using the following formula : ∑ ∈′ ′= jk DdRd j dddcs I rr rrr ) ( ) , cos ( ) , ( ( 3 ) where ) ( dR k r denotes a set of the k nearest neighbors of document d and D j is a set of training documents in class c j</definiens>
			</definition>
			<definition id="4">
				<sentence>Finally , the co-occurrence frequency value of two terms is obtained by a maximum value among co-occurrence frequency values in each category as follows : { } ) , , ( max ) , ( jli c li cttcottco j = ( 6 ) where ) , ( li ttco denotes a co-occurrence frequency value of t i and t l , and ) , , ( jli cttco denotes a co-occurrence frequency value of t i and t l in a category c j .</sentence>
				<definiendum id="0">li ttco</definiendum>
				<definiendum id="1">jli cttco</definiendum>
				<definiens id="0">a co-occurrence frequency value of t i and t l , and )</definiens>
				<definiens id="1">a co-occurrence frequency value of t i and t l in a category c j</definiens>
			</definition>
			<definition id="5">
				<sentence>Using the inverted-file indexing of training documents , the time complexity of k-NN is O ( m 2 l/n ) ( Yang , 1994 ) , where m is the number of unique words in the document , l is the number of training documents , and n is the number of unique terms in the training collection .</sentence>
				<definiendum id="0">m</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the number of unique words in the document</definiens>
				<definiens id="1">the number of training documents , and</definiens>
				<definiens id="2">the number of unique terms in the training collection</definiens>
			</definition>
			<definition id="6">
				<sentence>Even more , the time complexity of TCFP without contextual information is O ( mc ) , wherec is the number of categories .</sentence>
				<definiendum id="0">wherec</definiendum>
				<definiens id="0">the number of categories</definiens>
			</definition>
</paper>

		<paper id="1089">
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>Next , we describe the experiment , which uses the semantic classes of words in a Japanese-to-English transfer dictionary to predict their countability ( x 4 ) .</sentence>
				<definiendum id="0">predict their countability</definiendum>
				<definiens id="0">uses the semantic classes of words in a Japanese-to-English transfer dictionary to</definiens>
			</definition>
			<definition id="1">
				<sentence>Grammatical countability is motivated by the semantic distinction between object and substance reference ( also known as bounded/non-bounded or individuated/ non-individuated ) .</sentence>
				<definiendum id="0">Grammatical countability</definiendum>
			</definition>
			<definition id="2">
				<sentence>WordNet : An Electronic Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1103">
			<definition id="0">
				<sentence>A text categorization task consists of a training phase and a text classification phase .</sentence>
				<definiendum id="0">text categorization task</definiendum>
			</definition>
			<definition id="1">
				<sentence>The similarity value between title T and sentence S i in a document d is calculated by the following formula : ) ( max ) , ( TS TS TSSim i dS i i i rr rr ⋅ ⋅ = ∈ ( 1 ) where T r denotes a vector of the title , and i S r denotes a vector of a sentence .</sentence>
				<definiendum id="0">T r</definiendum>
				<definiens id="0">a vector of a sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>∑ ∈ ×= dS ii i SScoretStftdWTF ) ( ) , ( ) , ( ( 4 ) where tf ( S i , t ) denotes TF of the term t in sentence S i .</sentence>
				<definiendum id="0">∑ ∈ ×= dS</definiendum>
				<definiendum id="1">tf ( S i</definiendum>
				<definiens id="0">TF of the term t in sentence S i</definiens>
			</definition>
			<definition id="3">
				<sentence>The weight of a term t in a document d is calculated as follows : ∑ =                 ×         × = T i t i t i n N tdWTF n N tdWTF tdw 1 2 log ) , ( log ) , ( ) , ( ( 5 ) where N is the number of documents in the training set , T is the number of features limited by feature selection , and n t is the number of training documents in which t occurs .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">T</definiendum>
				<definiendum id="2">n t</definiendum>
				<definiens id="0">the number of features limited by feature selection , and</definiens>
			</definition>
			<definition id="4">
				<sentence>In Figure 3 , Sim ( S , T ) denotes the method using the title , Cen ( S ) the method using the importance of terms , and Sim ( S , T ) +Cen ( S ) the combination method .</sentence>
				<definiendum id="0">Sim ( S , T )</definiendum>
				<definiens id="0">the method using the title</definiens>
			</definition>
			<definition id="5">
				<sentence>The former is calculated by formula ( 6 ) and the latter by formula ( 7 ) : 1 , 1 1 ∑∑∑ = ∈∈ ⋅== K k Id kwithiin Id k k kk Cd D Cod I C rr rrrr ( 6 ) ( ) || 1 , || 1 1 1 ∑ ∑ = = ⋅⋅= ⋅= K k kglobkbetween K k kkglob CCI D Co CI D C rr rr ( 7 ) where D denotes the total training document set , I k denotes training document set in k-th category , k C r denotes a centroid vector of k-th category , and glob C r denotes a centroid vector of the total training documents .</sentence>
				<definiendum id="0">D</definiendum>
				<definiens id="0">the total training document set</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>• Query formation : Singleand multi-word units ( content words ) are extracted from the analysis , and WordNet synsets ( Fellbaum , 1998 ) are used for query expansion .</sentence>
				<definiendum id="0">WordNet synsets</definiendum>
				<definiens id="0">Singleand multi-word units ( content words</definiens>
			</definition>
			<definition id="1">
				<sentence>Unlike ( Prager et al. , 1999 ) , we do not first annotate the source corpus , but perform IR directly on the source text , using MG ( Witten et al. , 1994 ) .</sentence>
				<definiendum id="0">IR</definiendum>
				<definiens id="0">annotate the source corpus , but perform</definiens>
			</definition>
			<definition id="2">
				<sentence>and the Semantic Ontology We classify desired answers by their semantic type , which have been taxonomized in the Webclopedia QA Typology ( Hovy et al. , 2002 ) , Candidate answer parsing • Steps : parse sentences • Engines : CONTEX Matching • Steps : match general constraint patterns against parse trees match desired semantic type against parse tree elements assign score to words in sliding window • Engine : Matcher Ranking and answer extraction • Steps : rank candidate answers extract and format them • Engine : Answer ranker/formatter QA typology • QA types , categorized in taxonomy Constraint patterns • Identify likely answers in relation to other parts of the sentence Create query Retrieve documents Select &amp; rank sentences Parse top sentences Parse question Input question Perform additional inference Rank and prepare answers Output answers Question parsing • Steps : parse question find desired semantic type • Engines : IdentiFinder ( BBN ) CONTEX Match sentences against answers Query creation • Steps : extract , combine important words expand query words using WordNet create queries , order by specificity • Engines : Query creator IR • Steps : retrieve top 1000 documents • Engines : MG ( RMIT Melbourne ) Sentence selection and ranking • Steps : score each sentence in each document rank sentences and pass top 300 along • Engines : Ranker Figure 1 .</sentence>
				<definiendum id="0">semantic type</definiendum>
				<definiens id="0">parse sentences • Engines : CONTEX Matching • Steps : match general constraint patterns against parse trees match desired semantic type against parse tree elements assign score to words in sliding window • Engine : Matcher Ranking and answer extraction • Steps : rank candidate answers extract and format them • Engine : Answer ranker/formatter QA typology • QA types , categorized in taxonomy Constraint patterns • Identify likely answers in relation to other parts of the sentence Create query Retrieve documents Select &amp; rank sentences Parse top sentences Parse question Input question Perform additional inference Rank and prepare answers Output answers Question parsing • Steps : parse question find desired semantic type • Engines : IdentiFinder ( BBN ) CONTEX Match sentences against answers Query creation • Steps : extract , combine important words expand query words using WordNet create queries , order by specificity • Engines : Query creator IR • Steps : retrieve top 1000 documents • Engines : MG ( RMIT Melbourne ) Sentence selection and ranking • Steps : score each sentence in each document rank sentences and pass top 300 along • Engines : Ranker Figure 1</definiens>
			</definition>
			<definition id="3">
				<sentence>Webclopedia identifies the qtargets respectively as ZIP-CODE and TEMPERATURE-QUANTITY .</sentence>
				<definiendum id="0">Webclopedia</definiendum>
				<definiens id="0">identifies the qtargets respectively as ZIP-CODE and TEMPERATURE-QUANTITY</definiens>
			</definition>
			<definition id="4">
				<sentence>[ D-QUESTION-MARK ] [ 1 ] Jack Ruby , who killed John F. Kennedy assassin Lee Harvey Oswald [ S-NP ] ( PRED ) [ 2 ] &lt; Jack Ruby &gt; 1 [ S-NP ] ( DUMMY ) [ 6 ] , [ D-COMMA ] ( MOD ) [ 7 ] who killed John F. Kennedy assassin Lee Harvey Oswald [ S-REL-CLAUSE ] ( SUBJ ) [ 8 ] who &lt; 1 &gt; [ S-INTERR-NP ] ( PRED ) [ 10 ] killed [ S-TR-VERB ] ( OBJ ) [ 11 ] JFK assassin…Oswald [ S-NP ] ( PRED ) [ 12 ] JFK…Oswald [ S-PROP-NAME ] ( MOD ) [ 13 ] JFK [ S-PROPER-NAME ] ( MOD ) [ 19 ] assassin [ S-NOUN ] ( PRED ) [ 20 ] …Oswald [ S-PROPER-NAME ] Although the PREDs of both S1 and S2 match that of the question “killed” , only S1 matches “Lee Harvey Oswald” as the head of the logical OBJect .</sentence>
				<definiendum id="0">SUBJ</definiendum>
				<definiendum id="1">] ( PRED</definiendum>
				<definiendum id="2">MOD</definiendum>
				<definiens id="0">] ( DUMMY ) [ 6 ] , [ D-COMMA ] ( MOD ) [ 7 ] who killed John F. Kennedy assassin Lee Harvey Oswald [ S-REL-CLAUSE ] (</definiens>
			</definition>
			<definition id="5">
				<sentence>Semantic relation scores measured only on questions in which they could logically apply .</sentence>
				<definiendum id="0">Semantic relation</definiendum>
				<definiens id="0">scores measured only on questions in which they could logically apply</definiens>
			</definition>
			<definition id="6">
				<sentence>WordNet : An Electronic Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="7">
				<sentence>FALCON : Boosting Knowledge for Answer Engines .</sentence>
				<definiendum id="0">FALCON</definiendum>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>SVMs are ones of large margin classifiers ( Smola et al. , 2000 ) which are based on the strategy where margins between separating boundary and vectors of which elements express the features of training samples is maximized .</sentence>
				<definiendum id="0">SVMs</definiendum>
				<definiens id="0">ones of large margin classifiers ( Smola et al. , 2000 ) which are based on the strategy where margins between separating boundary and vectors of which elements express the features of training samples is maximized</definiens>
			</definition>
			<definition id="1">
				<sentence>It is distinguished whether given sample ~x = ( x1 ; x2 ; : : : ; xd ) belongs toX1 orX2 by equation ( 1 ) : f ( ~x ) =sign ( g ( ~x ) ) = ( 1 ~x2X 1 1 ~x2X2 ( 1 ) where g ( ~x ) is the hyperplain which separates two classes in which ~w and b are decided by optimization .</sentence>
				<definiendum id="0">g ( ~x )</definiendum>
				<definiens id="0">the hyperplain which separates two classes in which ~w and b are decided by optimization</definiens>
			</definition>
			<definition id="2">
				<sentence>~w can be obtained from equation ( 5 ) and b can be obtained from b=yi ~w ~xi where ~xi is an arbitrary support vector .</sentence>
				<definiendum id="0">~xi</definiendum>
				<definiens id="0">an arbitrary support vector</definiens>
			</definition>
			<definition id="3">
				<sentence>At this time , the maximal margin problem is enhanced as minimizing jj~wjj2=2+CPni=1 i , where C expresses the weight of errors .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">expresses the weight of errors</definiens>
			</definition>
			<definition id="4">
				<sentence>K ( ~x ; ~x0 ) = ( ~x ~x0+1 ) p ( 13 ) K ( ~x ; ~x0 ) =exp 0B BBB @ jj~x ~x0jj2 2 2 1C CCCA ( 14 ) A non-linear separating using one of these kernel functions is corresponding to separating with consideration of the dependencies between the features in Rd .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">corresponding to separating with consideration of the dependencies between the features in Rd</definiens>
			</definition>
</paper>

		<paper id="1129">
</paper>

		<paper id="1123">
</paper>

		<paper id="1030">
</paper>

		<paper id="2019">
			<definition id="0">
				<sentence>Morphological analysis is one of the basic techniques used in Japanese sentence analysis .</sentence>
				<definiendum id="0">Morphological analysis</definiendum>
				<definiens id="0">one of the basic techniques used in Japanese sentence analysis</definiens>
			</definition>
			<definition id="1">
				<sentence>A morpheme is a minimal grammatical unit , such as a word or a suﬃx , and morphological analysis is the process of segmenting a given sentence into a row of morphemes and assigning to each morpheme grammatical attributes such as part-of-speech ( POS ) and inflection type .</sentence>
				<definiendum id="0">morpheme</definiendum>
				<definiens id="0">a minimal grammatical unit , such as a word or a suﬃx , and morphological analysis is the process of segmenting a given sentence into a row of morphemes and assigning to each morpheme grammatical attributes such as part-of-speech ( POS ) and inflection type</definiens>
			</definition>
			<definition id="2">
				<sentence>We used the spontaneous speech corpus , CSJ , which is a tagged corpus of transcriptions of academic presentations and simulated public speech .</sentence>
				<definiendum id="0">CSJ</definiendum>
				<definiens id="0">a tagged corpus of transcriptions of academic presentations and simulated public speech</definiens>
			</definition>
			<definition id="3">
				<sentence>Each feature consists of a type and a value , which are given in the rows of the table .</sentence>
				<definiendum id="0">feature</definiendum>
				<definiens id="0">consists of a type and a value , which are given in the rows of the table</definiens>
			</definition>
			<definition id="4">
				<sentence>“TOC ( 0 ) ( Transition ) ” represents the transition from the leftmost character to the rightmost character in a string .</sentence>
				<definiendum id="0">Transition ) ”</definiendum>
				<definiens id="0">represents the transition from the leftmost character to the rightmost character in a string</definiens>
			</definition>
			<definition id="5">
				<sentence>“TOC ( 1 ) ( Transition ) ” represents the transition from the rightmost character in the adjacent morpheme on the left to the leftmost character in the target string .</sentence>
				<definiendum id="0">Transition ) ”</definiendum>
				<definiens id="0">represents the transition from the rightmost character in the adjacent morpheme on the left to the leftmost character in the target string</definiens>
			</definition>
			<definition id="6">
				<sentence>Recall is the percentage of morphemes in the test corpus whose segmentation and major POS tag are identified correctly .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">the percentage of morphemes in the test corpus whose segmentation and major POS tag are identified correctly</definiens>
			</definition>
			<definition id="7">
				<sentence>Precision is the percentage of all morphemes identified by the system that are identified correctly .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the percentage of all morphemes identified by the system that are identified correctly</definiens>
			</definition>
			<definition id="8">
				<sentence>The Unknown Word Problem : a Morphological Analysis of Japanese Using Maximum Entropy Aided by a Dictionary .</sentence>
				<definiendum id="0">Unknown Word Problem</definiendum>
				<definiens id="0">a Morphological Analysis of Japanese Using Maximum Entropy Aided by a Dictionary</definiens>
			</definition>
</paper>

		<paper id="1098">
			<definition id="0">
				<sentence>A video scene description consists of semi-automatically detected keyframes of each scene in a video clip and time codes of scenes .</sentence>
				<definiendum id="0">video scene description</definiendum>
				<definiens id="0">consists of semi-automatically detected keyframes of each scene in a video clip and time codes of scenes</definiens>
			</definition>
			<definition id="1">
				<sentence>Multimedia annotation is an extension of document annotation such as GDA ( Global Document Annotation ) ( Hasida , 2002 ) .</sentence>
				<definiendum id="0">Multimedia annotation</definiendum>
			</definition>
			<definition id="2">
				<sentence>The sem attribute encodes a word sense .</sentence>
				<definiendum id="0">sem attribute</definiendum>
				<definiens id="0">encodes a word sense</definiens>
			</definition>
			<definition id="3">
				<sentence>Our video annotation consists of creation of text data related to video content , linguistic annotation of the text data , automatic segmentation of video , semi-automatic linking of video segments with corresponding text data , and interactive naming of people and objects in video scenes .</sentence>
				<definiendum id="0">video annotation</definiendum>
			</definition>
			<definition id="4">
				<sentence>The scene description consists of time codes of the start and end frames , a keyframe ( image data in JPEG format ) ﬁlename , a scene title , and some text representing topics .</sentence>
				<definiendum id="0">scene description</definiendum>
				<definiens id="0">consists of time codes of the start and end frames , a keyframe ( image data in JPEG format ) ﬁlename , a scene title</definiens>
			</definition>
			<definition id="5">
				<sentence>Video transformation is an initial process of multimedia summarization and translation .</sentence>
				<definiendum id="0">Video transformation</definiendum>
			</definition>
			<definition id="6">
				<sentence>The multimodal document consists of an embedded video window , keyframes of scenes , and transcipts aligned withthe scenes as shown in Figure 8 .</sentence>
				<definiendum id="0">multimodal document</definiendum>
				<definiens id="0">consists of an embedded video window , keyframes of scenes , and transcipts aligned withthe scenes as shown in Figure 8</definiens>
			</definition>
</paper>

		<paper id="1101">
			<definition id="0">
				<sentence>The optimal hyperplane to separate them is found by solving the following quadratic programming problem : minimizefi 1 ; : : : ; fil 1 2 lX i ; j=1 fiifijyiyjK ( xi ; xj ) ¡ lX i=1 fii ; subject to 0 • fii • C ( 1 • i • l ) ; lX i=1 fiiyi = 0 ; where the function K ( xi ; xj ) is the inner product of the nonlinear function ( K ( xi ; xj ) = Φ ( xi ) ¢Φ ( xj ) ) called a kernel function , and the constant C controls the training errors and becomes the upper bound of fii .</sentence>
				<definiendum id="0">xj )</definiendum>
				<definiens id="0">the inner product of the nonlinear function</definiens>
			</definition>
			<definition id="1">
				<sentence>The similarity between two examples xi and xj on SVMs is measured by the following distance : d ( xi ; xj ) = q kΦ ( xi ) ¡Φ ( xj ) k2 ; = q K ( xi ; xi ) +K ( xj ; xj ) ¡2K ( xi ; xj ) : We can extract inconsistencies from a corpus as follows : given an example x which was detected as an exceptional example ( following the proposal in the previous subsection ) , we extract an example z with the smallest values of the distance d ( x ; z ) from the examples whose label is different from x. Intuitively , z is a closest opposite example to x in the SVMs’ higher dimensional space and may be a cause for x to be attached a large weight .</sentence>
				<definiendum id="0">z</definiendum>
				<definiens id="0">an exceptional example ( following the proposal in the previous subsection )</definiens>
			</definition>
			<definition id="2">
				<sentence>Table 2 : Recall for the Artificial Data fi # of Correctly Detected Errors Recall 1 10 100 1000 10000 100000 1000000 0 0.2 0.4 0.6 0.8 1 Number α Positive ExamplesNegative Examples Figure 3 : Distribution of the Value fi on the RWCP Corpus ( Japanese ) We use the RWCP corpus , which consists of 35,743 sentences ( 921,946 morphemes ) .</sentence>
				<definiendum id="0">RWCP corpus</definiendum>
				<definiens id="0">Recall for the Artificial Data fi # of Correctly Detected Errors Recall 1 10 100 1000 10000 100000 1000000 0 0.2 0.4 0.6 0.8 1 Number α Positive ExamplesNegative Examples Figure 3 : Distribution of the Value fi on the RWCP Corpus ( Japanese ) We use the</definiens>
			</definition>
			<definition id="3">
				<sentence>Transformation-Based ErrorDriven Learning and Natural Language Processing : A Case Study in Part-of-Speech Tagging .</sentence>
				<definiendum id="0">Transformation-Based ErrorDriven Learning</definiendum>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>Let D = { d 1 , ··· , d N } be a collection of N target documents , and let S D be a subset of documents such that S D ⊆ D. Likewise , let T = { t 1 , ··· , t M } be a set of M distinct terms that appear in the target document collection , and let S T be a subset of terms such that S T ⊆ T. A cluster , denoted as c , is defined as a combination of S T and S D : c = ( S T , S D ) .</sentence>
				<definiendum id="0">let S T</definiendum>
				<definiens id="0">a subset of terms such that S T ⊆ T. A cluster</definiens>
			</definition>
			<definition id="1">
				<sentence>In this case , freq ( t i , S D ) and freq ( S T , d j ) represent the frequencies of t i and d j within c = ( S T , S D ) , respectively .</sentence>
				<definiendum id="0">freq</definiendum>
				<definiens id="0">the frequencies of t i and d j within c = ( S T , S D ) , respectively</definiens>
			</definition>
			<definition id="2">
				<sentence>For high-frequency terms , P ( t i ) ≈ freq ( t i ) /F. In the original definition , the value of δ was uniquely determined , for example as δ = m ( 1 ) M with m ( 1 ) being the number of terms that appear exactly once in the text .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the number of terms that appear exactly once in the text</definiens>
			</definition>
			<definition id="3">
				<sentence>P ( d j ) is determined by P ( d j ) = summationtext t i ∈T P ( t i , d j ) , or approximated simply by P ( d j ) =freq ( d j ) /F. Next , the mutual information after agglomerating S T and S D into a single cluster ( Figure 2 ) is calculated as : I prime ( T , D ) = summationdisplay t i /∈S T summationdisplay d j /∈S D P ( t i , d j ) log P ( t i , d j ) P ( t i ) P ( d j ) +P ( S T , S D ) log P ( S T , S D ) P ( S T ) P ( S D ) , ( 7 ) where P ( S T ) = summationtext t i ∈S T P ( t i ) and P ( S D ) = summationtext d j ∈S D P ( d j ) .</sentence>
				<definiendum id="0">P ( S T</definiendum>
				<definiendum id="1">P ( S</definiendum>
				<definiens id="0">determined by P ( d j ) = summationtext t i ∈T P ( t i , d j ) , or approximated simply by P ( d j ) =freq ( d j</definiens>
			</definition>
			<definition id="4">
				<sentence>The fitness of a cluster , denoted as δI ( S T , S D ) , is defined as the diﬀerence of the two information values given by Eqs .</sentence>
				<definiendum id="0">fitness of a cluster</definiendum>
				<definiendum id="1">δI ( S T</definiendum>
				<definiens id="0">the diﬀerence of the two information values given by Eqs</definiens>
			</definition>
			<definition id="5">
				<sentence>The cluster generation process is defined as the repeated iterations of cluster initiation and cluster improvement steps ( Aizawa , 2002 ) .</sentence>
				<definiendum id="0">cluster generation process</definiendum>
				<definiens id="0">the repeated iterations of cluster initiation and cluster improvement steps</definiens>
			</definition>
			<definition id="6">
				<sentence>Web document clustering : A feasibility demonstration .</sentence>
				<definiendum id="0">Web document clustering</definiendum>
				<definiens id="0">A feasibility demonstration</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>A governor label ( implicitly ) encodes a grammatical relation type ( such as subject or object ) and a governing lexical head .</sentence>
				<definiendum id="0">governor label</definiendum>
				<definiens id="0">encodes a grammatical relation type ( such as subject or object</definiens>
			</definition>
			<definition id="1">
				<sentence>Precision ( % ) Recall ( % ) F-score Best parse 76.25 76.77 76.51 All parses 74.63 75.33 74.98 weighted precision and recall measures , although there is an option for associating weights with complete parses in the distributed software implementing the PARSEVAL scheme ( Harrison et al. , 1991 ) for evaluating parser accuracy with respect to phrase structure bracketings .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">an option for associating weights with complete parses in the distributed software implementing the PARSEVAL scheme ( Harrison et al. , 1991 ) for evaluating parser accuracy with respect to phrase structure bracketings</definiens>
			</definition>
			<definition id="2">
				<sentence>However , we wanted to see whether the precise method for assigning weights to GRs has an effect on accuracy , and if so , to what extent .</sentence>
				<definiendum id="0">GRs</definiendum>
				<definiens id="0">has an effect on accuracy , and if so , to what extent</definiens>
			</definition>
			<definition id="3">
				<sentence>Lafferty , J. , D. Sleator and D. Temperley ( 1992 ) Grammatical trigrams : A probabilistic model of link grammar .</sentence>
				<definiendum id="0">Grammatical trigrams</definiendum>
				<definiens id="0">A probabilistic model of link grammar</definiens>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>For instance , the closest terms ordered by increasing distance to , doorare : V ( , door- ) = , portal- , , portiere- , , opening- , , gate- , , barrier- , . . . Let us de ne Sim ( A ; B ) as one of the similarity measures between two vectors A et B , often used in information retrieval ( Mor99 ) .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">one of the similarity measures between two vectors A et B , often used in information retrieval</definiens>
			</definition>
			<definition id="1">
				<sentence>DA is a real distance function .</sentence>
				<definiendum id="0">DA</definiendum>
			</definition>
			<definition id="2">
				<sentence>The antonymy measure between CAR and EXISTENCE is an example of our previous remark about vectors sharing few ideas and that around =2 this measure is close to the angular distance ( we have DA ( existence ; car ) = 1:464 . )</sentence>
				<definiendum id="0">EXISTENCE</definiendum>
				<definiens id="0">an example of our previous remark about vectors sharing few ideas and that around =2 this measure is close to the angular distance</definiens>
			</definition>
</paper>

		<paper id="1122">
			<definition id="0">
				<sentence>Japanese is a head-final language .</sentence>
				<definiendum id="0">Japanese</definiendum>
				<definiens id="0">a head-final language</definiens>
			</definition>
</paper>

		<paper id="1125">
			<definition id="0">
				<sentence>The bias of the word distribution in the co-occurring words is defined as the number of distinct words whose occurrences are saliently biased in the co-occurring words .</sentence>
				<definiendum id="0">bias of the word distribution</definiendum>
				<definiens id="0">the number of distinct words whose occurrences are saliently biased in the co-occurring words</definiens>
			</definition>
			<definition id="1">
				<sentence>Only hgs ( N , K , n , l ) , the sum of hg ( N , K , n , l ) over l ( k≤l≤min { n , K } ) can tell which is the case since the sum indicates how far the event “v occurs k-times in D ( w ) ” is from the extreme event “v occurs min { n , K } times in D ( w ) ” .</sentence>
				<definiendum id="0">event “v</definiendum>
				<definiens id="0">n , l ) , the sum of hg ( N , K , n , l ) over l ( k≤l≤min { n</definiens>
				<definiens id="1">the case since the sum indicates how far the event “v occurs k-times in D ( w ) ” is from the extreme</definiens>
			</definition>
			<definition id="2">
				<sentence>Now we can define SAL ( D ( T ) , s ) using the saliency measure defined above and a parameter s ≥ 0 : } , ) ( | ) ( { ) ) , ( ( swHGSTDwDIFFNUMsTDSAL ≥∈= where DIFFNUM ( X ) stands for the number of distinct items in set X. That is , SAL ( D ( T ) , s ) is the number of distinct words in D ( T ) whose saliency of occurrence is not less than s. For instance , using the 1996 archive of Nihon Keizai Shimbun ( a Japanese financial newspaper ) , SAL ( D ( “Aum ３ ” ) , 110 ) = 74 , SAL ( D ( “Aum” ) , 200 ) = 50 , SAL ( D* ( “do” ) , 110 ) = 1 , and SAL ( D* ( “do” ) , 200 ) = 0 , where D* ( “do” ) is a set of N 0 randomly chosen articles from D ( “do” ) and N 0 is the threshold value stated in subsection 1.2 .</sentence>
				<definiendum id="0">DIFFNUM</definiendum>
				<definiendum id="1">, SAL ( D</definiendum>
				<definiendum id="2">SAL</definiendum>
				<definiens id="0">a Japanese financial newspaper )</definiens>
			</definition>
			<definition id="3">
				<sentence>Figure 2 plots the coordinates { # D rand , ３ Aum is the name of a religious cult that attacked Tokyo subway with sarin gas in 1995 .</sentence>
				<definiendum id="0">３ Aum</definiendum>
				<definiens id="0">the name of a religious cult that attacked Tokyo subway with sarin gas in 1995</definiens>
			</definition>
			<definition id="4">
				<sentence>SAL ( D rand , s ) } for D rand and s , where D rand varies over randomly sampled article sets and s varies over several discrete values .</sentence>
				<definiendum id="0">SAL</definiendum>
				<definiens id="0">s ) } for D rand and s , where D rand varies over randomly sampled article sets and s varies over several discrete values</definiens>
			</definition>
			<definition id="5">
				<sentence>Four measures were compared by Hisamitsu et al. ( 2000 ) : NormDist ( D ( T ) ) , NormDIFFNUM ( D ( T ) ) , tf-idf , and tf ( term frequency ) , where NormDIFFNUM ( D ( T ) ) is a normalized version of a measure called DIFFNUM ( D ( T ) ) , which gives the number of distinct words in D ( T ) .</sentence>
				<definiendum id="0">tf</definiendum>
				<definiendum id="1">NormDIFFNUM</definiendum>
			</definition>
			<definition id="6">
				<sentence>The definition of tf-idf used in the comparison was as follows : , ) ( log ) ( TN N TTFidftf total ×=− where T is a term , TF ( T ) is the term frequency of T , N total is the total number of documents , and N ( T ) is the number of documents that contain T. We compared these four measures with SAL ( D ( T ) , s ) , varying s. We compared the ability of each measure to gather class P words .</sentence>
				<definiendum id="0">tf-idf</definiendum>
				<definiendum id="1">TF ( T )</definiendum>
				<definiendum id="2">N total</definiendum>
				<definiendum id="3">N ( T )</definiendum>
				<definiens id="0">used in the comparison was as follows : , ) ( log ) ( TN N TTFidftf total ×=− where T is a term</definiens>
				<definiens id="1">the term frequency of T</definiens>
			</definition>
			<definition id="7">
				<sentence>0 10 20 30 40 50 60 70 80 90 100 110 120 130 0 5000 10000 15000 20000 Rank Accum u lated Num b er of Class P Word s NormDist SAL ( D ( T ) ,30 ) SAL ( D ( T ) ,70 ) SAL ( D ( T ) ,100 ) 0 20 40 60 80 100 120 0 5000 10000 15000 20000 Rank Acc u m u la ted Num b e r of Class P Words NormDist SAL ( D ( T ) ,30 ) SAL ( D ( T ) ,40 ) SAL ( D ( T ) ,50 ) Figure 4 Comparison of ADP-scores using D 0 Figure 3 Comparison of DP-scores using D 0 Figure 5 Comparison of DP-scores using D 1/2 Figure 6 Comparison of DP-scores using D 1/4 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 22000 NormDist s=50 70 90 110 130 150 170 AD P-score ADP ( M , 20,000 ) , ADP ( M , 10,000 ) , ADP ( M , 5,000 ) , 0 20 40 60 80 100 120 140 160 0 5000 10000 15000 20000 Rank A ccu m u la ted N u m b er o f Class P W o rd s NormDist NormDIFFNUM tf tf-idf SAL ( D ( T ) ,30 ) SAL ( D ( T ) ,110 ) SAL ( D ( T ) ,180 ) Conclusion We proposed a novel measure of the representativeness of a term T in a given corpus .</sentence>
				<definiendum id="0">NormDist SAL</definiendum>
				<definiendum id="1">AD P-score ADP</definiendum>
				<definiendum id="2">ADP</definiendum>
				<definiens id="0">T ) ,180 ) Conclusion We proposed a novel measure of the representativeness of a term T in a given corpus</definiens>
			</definition>
			<definition id="8">
				<sentence>Denoting the words co-occurring with T by D ( T ) , the measure is defined as SAL ( D ( T ) , s ) , the number of words in D ( T ) whose saliency of occurrences is over a threshold s. This measure embodies the idea that the distribution of words in D ( T ) should be saliently biased according to that of the whole corpus if T is a representative term .</sentence>
				<definiendum id="0">Denoting the words co-occurring with T by D ( T )</definiendum>
				<definiens id="0">s ) , the number of words in D ( T ) whose saliency of occurrences</definiens>
				<definiens id="1">a representative term</definiens>
			</definition>
</paper>

		<paper id="1068">
</paper>

		<paper id="2016">
			<definition id="0">
				<sentence>However , for most of the world 's people , the Information Era is still limited to using hardware and software that do not meet their needs in terms of language and script resources .</sentence>
				<definiendum id="0">Information Era</definiendum>
				<definiens id="0">still limited to using hardware and software that do not meet their needs in terms of language and script resources</definiens>
			</definition>
			<definition id="1">
				<sentence>The Minority Language Engineering ( MILLE ) project 1 , “jointly based in the Department of Linguistics at Lancaster University and Oxford University Computing Services , seeks to investigate the development of corpus resources for UK non-indigenous minority languages 2 ( NIMLs ) ” .</sentence>
				<definiendum id="0">Minority Language Engineering</definiendum>
				<definiens id="0">seeks to investigate the development of corpus resources for UK non-indigenous minority languages 2 ( NIMLs ) ”</definiens>
			</definition>
			<definition id="2">
				<sentence>This multi-engine system , based on a corpus-based machine translation ( CBMT ) , uses both EBMT and SMT 4 as well as an elicitation tool 5 that learns transfer rules from a small and controlled corpus .</sentence>
				<definiendum id="0">CBMT</definiendum>
				<definiens id="0">uses both EBMT and SMT 4 as well as an elicitation tool 5 that learns transfer rules from a small and controlled corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>The Perl scripts handle the interaction with the XML base .</sentence>
				<definiendum id="0">Perl scripts</definiendum>
				<definiens id="0">handle the interaction with the XML base</definiens>
			</definition>
			<definition id="4">
				<sentence>7 : In the Isan area of Thailand where Lao is spoken , Thai scripts are used and also the language itself is somehow different from Lao spoken in Laos .</sentence>
				<definiendum id="0">Lao</definiendum>
				<definiens id="0">spoken , Thai scripts are used and also the language itself is somehow different from Lao spoken in Laos</definiens>
			</definition>
</paper>

		<paper id="1115">
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>During multimodal understanding , MIND combines semantic meanings of unimodal inputs ( i.e. , modality units ) , and uses contexts ( e.g. , conversation context and domain context ) to form an overall understanding of user multimodal inputs .</sentence>
				<definiendum id="0">MIND</definiendum>
				<definiens id="0">combines semantic meanings of unimodal inputs ( i.e. , modality units ) , and uses contexts ( e.g. , conversation context and domain context</definiens>
			</definition>
			<definition id="1">
				<sentence>MIND components gesture speech text Multimodal Interpreter Discourse Interpreter Language Interpreter Gesture Interpreter Speech Recognizer Gesture Recognizer Modality Unit ( Speech &amp; Text ) Modality Unit ( Gesture ) Conversation Unit Unimodal Understanding Discourse Understanding Multimodal Understanding Other RIA Components C onv er sat io n H ist o r y Conversation Segment MIND D o main , V isual C ontexts Semantics-based Representation for Multimodal Interpretation in Conversational Systems Joyce Chai IBM T. J. Watson Research Center 19 Skyline Drive Hawthorne , NY 10532 , USA { jchai @ us.ibm.com } gesture ( shown in Figure 3 ) is ambiguous .</sentence>
				<definiendum id="0">MIND</definiendum>
				<definiens id="0">components gesture speech text Multimodal Interpreter Discourse Interpreter Language Interpreter Gesture Interpreter Speech Recognizer Gesture Recognizer Modality Unit ( Speech &amp; Text ) Modality Unit ( Gesture ) Conversation Unit Unimodal Understanding Discourse Understanding Multimodal Understanding Other RIA Components C onv er sat io n H ist o r y Conversation Segment MIND D o main , V isual C ontexts Semantics-based Representation for Multimodal Interpretation in Conversational Systems Joyce Chai IBM T. J. Watson</definiens>
			</definition>
			<definition id="2">
				<sentence>An intention is modeled by three dimensions : Motivator indicating one of the three high level purposes : DataPresentation , DataAnalysis ( e.g. , comparison ) , and ExceptionHandling ( e.g. , clarification ) , Act specifying whether the input is a request or a reply , and Method indicating a specific task , e.g. , Search ( activating the relevant objects based on some criteria ) or Lookup ( evaluating/retrieving attributes of objects ) .</sentence>
				<definiendum id="0">DataPresentation</definiendum>
				<definiendum id="1">ExceptionHandling</definiendum>
				<definiendum id="2">Lookup</definiendum>
				<definiens id="0">activating the relevant objects based on some criteria</definiens>
			</definition>
			<definition id="3">
				<sentence>Constraint describes constraints to be satisfied ( described later ) .</sentence>
				<definiendum id="0">Constraint</definiendum>
				<definiens id="0">describes constraints to be satisfied ( described later )</definiens>
			</definition>
			<definition id="4">
				<sentence>Therefore , MIND models two major types of constraints : referGiven the semantic models of intention , attention and constraints , MIND represents those models using a combination of feature structures ( Carpenter , 1992 ) .</sentence>
				<definiendum id="0">MIND</definiendum>
				<definiens id="0">referGiven the semantic models of intention , attention and constraints</definiens>
			</definition>
			<definition id="5">
				<sentence>During unimodal understanding , MIND applies a decision tree based semantic parser on natural language inputs ( Jelinek et al. , 1994 ) to identify salient information .</sentence>
				<definiendum id="0">MIND</definiendum>
				<definiens id="0">applies a decision tree based semantic parser on natural language inputs ( Jelinek et al. , 1994 ) to identify salient information</definiens>
			</definition>
			<definition id="6">
				<sentence>For the gesture input , MIND applies a simple geometry-based recognizer .</sentence>
				<definiendum id="0">MIND</definiendum>
				<definiens id="0">applies a simple geometry-based recognizer</definiens>
			</definition>
			<definition id="7">
				<sentence>Attention structures for U4 Base : House Topic : Collection Constraint : Attention ( A1 ) Category : Attributeive Manner : Comparative Aspect : Style Relation : Equals Anchor : * Topic : Instance Constraint : Category : Anaphora Manner : Demonstrative ( THIS ) Number:1 Attention ( A2 ) Category : Attributive Manner : Comparative Aspect : Location Relation : Equals Anchor : * Base : GeoLocation Topic : Instance Constraint : Category : Anaphora Manner : HERE Attention ( A3 ) Constraint : ( a ) Attention structure in the modality unit for U4 speech input Base : House Topic : Collection Constraint : Attention ( A1 ) Category : Attributeive Manner : Comparative Aspect : Style Relation : Equals Anchor : “Victorian” Category : Attributive Manner : Comparative Aspect : Location Relation : Equals Anchor : “White Plains” Constraint : ( b ) Attention structure in the conversation unit for U4 speech input nation of different feature structures .</sentence>
				<definiendum id="0">Attention structures</definiendum>
				<definiens id="0">House Topic : Collection Constraint : Attention ( A1 ) Category : Attributeive Manner : Comparative Aspect : Style Relation : Equals Anchor : * Topic : Instance Constraint : Category : Anaphora Manner : Demonstrative ( THIS ) Number:1 Attention ( A2 ) Category : Attributive Manner : Comparative Aspect : Location Relation : Equals Anchor : * Base : GeoLocation Topic</definiens>
				<definiens id="1">Comparative Aspect : Style Relation : Equals Anchor : “Victorian” Category : Attributive Manner : Comparative Aspect : Location Relation : Equals Anchor : “White Plains” Constraint : ( b ) Attention structure in the conversation unit for U4 speech input nation of different feature structures</definiens>
			</definition>
			<definition id="8">
				<sentence>A2 indicates an unknown object that is referred by a Demonstrative reference constraint ( this style ) , and A3 indicates a geographic location object referred by HERE .</sentence>
				<definiendum id="0">A3</definiendum>
				<definiens id="0">a geographic location object referred by HERE</definiens>
			</definition>
			<definition id="9">
				<sentence>During multimodal understanding , MIND combines information from modality units together and generates a conversation unit that represents the overall meaning of user multimodal inputs .</sentence>
				<definiendum id="0">MIND</definiendum>
				<definiens id="0">combines information from modality units together and generates a conversation unit that represents the overall meaning of user multimodal inputs</definiens>
			</definition>
			<definition id="10">
				<sentence>To reflect this progress , our conversation history is a hierarchical structure which consists of conversation segments and conversation units ( in Figure 9 ) .</sentence>
				<definiendum id="0">conversation history</definiendum>
			</definition>
			<definition id="11">
				<sentence>Specifically , multimodal understanding consists of two sub-processes : multimodal fusion and context-based inference .</sentence>
				<definiendum id="0">multimodal understanding</definiendum>
				<definiens id="0">consists of two sub-processes : multimodal fusion and context-based inference</definiens>
			</definition>
			<definition id="12">
				<sentence>Deriving unspecified information for U3 Act : Request Intention Base : House Topic : Instance Content : { MLS7689432 } Attention U3 Motivator : DataPresentation Method : Lookup Intention Base : House Topic : Instance Focus : SpecificAspect Aspect : Price Content : { MLS0234765 } Attention DS1 Initiator : User ( a ) Conversation unit for U3 as a result of multimodal fusion ( b ) Conversation segment DS1 in the conversation history Motivator : DataPresentation Act : Request Method : Lookup Intention Base : House Topic : Instance Focus : SpecificAspect Aspect : Price Content : { MLS7689432 } Attention U1 ( c ) Revised conversation unit for U3 as a result of context-based inference ence of “here” .</sentence>
				<definiendum id="0">Attention U1 ( c ) Revised conversation</definiendum>
				<definiens id="0">Request Intention Base : House Topic : Instance Content : { MLS7689432 } Attention U3 Motivator : DataPresentation Method : Lookup Intention Base : House Topic : Instance Focus : SpecificAspect Aspect : Price Content : { MLS0234765 } Attention DS1 Initiator : User ( a ) Conversation unit for U3 as a result of multimodal fusion ( b ) Conversation segment DS1 in the conversation history Motivator : DataPresentation Act : Request Method : Lookup Intention Base : House Topic : Instance Focus : SpecificAspect Aspect : Price Content : { MLS7689432 }</definiens>
			</definition>
			<definition id="13">
				<sentence>Improving alignment for U5 Motivator : DataAnalysis Act : Request Method : Compare Intention Base : House Topic : Collection Focus : MainAspect Constraint : Attention Base : House Topic : Instance Content : { MLS0765489 } Attention A 1 ( a ) Modality unit for U5 speech input ( b ) Modality unit for U5 gesture input Category : Anaphora Manner : Demonstrative Number:2 Base : House Topic : Instance Focus : MainAspect Constraint : A 2 Category : Temporal Manner : Relative Relation : Precede Anchor : Current Number:1 Base : House Topic : Instance Content : { MLS0468709 } Motivator : DataAnalysis Act : Request Method : Compare Intention Base : House Topic : Collection Focus : MainAspect Content : { MLS0468709 , MLS0765489 , MLS7689432 } Attention A 1 ( c ) Conversation unit for U5 Bolt , R. ( 1980 ) Voice and gesture at the graphics interface .</sentence>
				<definiendum id="0">Precede Anchor</definiendum>
				<definiens id="0">DataAnalysis Act : Request Method : Compare Intention Base : House Topic : Collection Focus : MainAspect Constraint : Attention Base : House Topic : Instance Content : { MLS0765489</definiens>
				<definiens id="1">Current Number:1 Base : House Topic : Instance Content : { MLS0468709 } Motivator : DataAnalysis Act : Request Method : Compare Intention Base : House Topic : Collection Focus : MainAspect Content : { MLS0468709 , MLS0765489 , MLS7689432 } Attention A 1 ( c ) Conversation unit for U5 Bolt</definiens>
			</definition>
			<definition id="14">
				<sentence>Chai , J. ; Pan , S. ; and Zhou , M. X. ( 2002 ) MIND : A Semantics-based multimodal interpretation framework for conversational systems .</sentence>
				<definiendum id="0">MIND</definiendum>
				<definiens id="0">A Semantics-based multimodal interpretation framework for conversational systems</definiens>
			</definition>
</paper>

		<paper id="2020">
			<definition id="0">
				<sentence>We obtained these from the UMLS metathesaurus , which includes French versions of MeSH , WHOART , ICPC and their English counterparts ( www.nlm.nih.gov/research/umls ) .</sentence>
				<definiendum id="0">UMLS metathesaurus</definiendum>
				<definiens id="0">includes French versions of MeSH , WHOART , ICPC and their English counterparts ( www.nlm.nih.gov/research/umls )</definiens>
			</definition>
</paper>

		<paper id="1132">
			<definition id="0">
				<sentence>Unraveling the mapping between syntactic functions such as subject and object and semantic roles such as agent and patient is an important piece of the language understanding problem .</sentence>
				<definiendum id="0">patient</definiendum>
				<definiens id="0">subject and object and semantic roles such as agent and</definiens>
				<definiens id="1">an important piece of the language understanding problem</definiens>
			</definition>
			<definition id="1">
				<sentence>Verb-Slot Aspect : This is the model of Rooth et al. ( 1999 ) , in which the verb and slot are combined into one atomic variable before the aspect model is trained : C8 CR DAD7 BP C8B4CRB5C8B4DABND7CYCRB5C8B4D2CYCRB5 Noun-Slot Aspect : A variation on the above model combines the slot with the noun , rather than the verb : C8 CR D2D7 BP C8B4CRB5C8B4DACYCRB5C8B4D2BND7CYCRB5 Alternation : This model , described in more detail above , introduces a new unobserved variable D6 for the semantic role of the noun , which can take two values : C8 CPD0D8 BP C8B4CRB5C8B4DACYCRB5C8B4D7CYCRB5C8B4D6CYD7BNCRB5C8B4D2CYD6BNCRB5 000000 000000 000000 000000 000000 111111 111111 111111 111111 111111 00000 00000 00000 00000 00000 11111 11111 11111 11111 11111 00000 00000 00000 00000 00000 11111 11111 11111 11111 11111 cluster verb slot noun 00000 00000 00000 00000 00000 00000 11111 11111 11111 11111 11111 11111 00000 00000 00000 00000 00000 11111 11111 11111 11111 11111 cluster verb , slot noun 00000 00000 00000 00000 00000 11111 11111 11111 11111 11111 00000 00000 00000 00000 00000 11111 11111 11111 11111 11111 00000 00000 00000 00000 00000 11111 11111 11111 11111 11111 verb noun slot role cluster 00000 00000 00000 00000 00000 11111 11111 11111 11111 11111 00000 00000 00000 00000 00000 11111 11111 11111 11111 11111 cluster verb noun , slot Alternation Verb-Slot AspectThree-way Aspect Noun-Slot Aspect Figure 1 : Graphical models : shading represents observed variables , arrows probabilistic dependencies .</sentence>
				<definiendum id="0">Verb-Slot Aspect</definiendum>
				<definiendum id="1">Noun-Slot Aspect</definiendum>
				<definiens id="0">Graphical models : shading represents observed variables , arrows probabilistic dependencies</definiens>
			</definition>
			<definition id="2">
				<sentence>Perplexity is the geometric mean of the reciprocal of the probability assigned by the model to each triple of verb , noun , and slot in the test data : C8C8 BP CT A0 BD C6 C8 CX D0D3CV C8B4DA CX BND2 CX BND7 CX B5 For the single-variable clustering models ( 4 , 5 and 6 ) 128 values were allowed for the cluster variable CR .</sentence>
				<definiendum id="0">Perplexity</definiendum>
				<definiens id="0">the geometric mean of the reciprocal of the probability assigned by the model to each triple of verb , noun , and slot in the test data : C8C8 BP CT A0 BD C6 C8 CX D0D3CV C8B4DA CX BND2 CX BND7 CX B5 For the single-variable clustering models ( 4 , 5 and 6 ) 128 values were allowed for the cluster variable CR</definiens>
			</definition>
			<definition id="3">
				<sentence>WordNet : An Electronic Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1108">
</paper>

		<paper id="1150">
			<definition id="0">
				<sentence>In the TREC competition ( Voorhees , 2000 ) , participants are requested to build a system which , given a set of English questions , can automatically extract answers ( a short phrase ) of no more than 50 bytes from a 5-gigabyte document library .</sentence>
				<definiendum id="0">TREC competition</definiendum>
				<definiens id="0">requested to build a system which , given a set of English questions , can automatically extract answers ( a short phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>We define a two-layered taxonomy , which represents a natural semantic classification for typical answers in the TREC task .</sentence>
				<definiendum id="0">two-layered taxonomy</definiendum>
				<definiens id="0">represents a natural semantic classification for typical answers in the TREC task</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , when “away” , which belongs to a list of words semantically related to the class distance , occurs in the sentence , the sensor Rel ( distance ) will be active .</sentence>
				<definiendum id="0">“away”</definiendum>
				<definiens id="0">a list of words semantically related to the class distance , occurs in the sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>Given a confusion set and a question , SNoW outputs a density over the classes derived from the activation of each class .</sentence>
				<definiendum id="0">SNoW</definiendum>
				<definiens id="0">outputs a density over the classes derived from the activation of each class</definiens>
			</definition>
			<definition id="4">
				<sentence>As discussed earlier , for each question we output the first CZ classes ( BD AK CZ AK BH ) , CR BD BNCR BE BNBMBMBMCR CZ where CZ satisfies , CZ BP D1CXD2B4CPD6CVD1CXD2 D8 B4 D8 CG BD D4 CX AL CCB5BNBHB5 ( 1 ) T is a threshold value in [ 0,1 ] .</sentence>
				<definiendum id="0">BD AK CZ AK BH</definiendum>
				<definiendum id="1">CR BD BNCR BE BNBMBMBMCR CZ</definiendum>
				<definiendum id="2">T</definiendum>
				<definiens id="0">a threshold value</definiens>
			</definition>
			<definition id="5">
				<sentence>We define C1 CXCY BP CU BDBN CXCU D8CWCT CRD3D6D6CTCRD8 D0CPCQCTD0 D3CU D8CWCT CXD8CW D5D9CTD7D8CXD3D2 CXD7 D3D9D8D4D9D8 CXD2 D6CPD2CZ CYBN BCBN D3D8CWCTD6DBCXD7CTBM ( 2 ) Then , C8 BD BP C8 D1 CXBPBD C1 CXBD BPD1 and C8 AKBH BP C8 D1 CXBPBD C8 CZ CX CYBPBD C1 CXCY BPD1 where m is the total number of test examples .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">the total number of test examples</definiens>
			</definition>
			<definition id="6">
				<sentence>We define the tendency of Class CX to be confused with Class CY as follows : BW CXCY BP BXD6D6 CXCY A3BEBPB4C6 CX B7 C6 CY B5BN ( 3 ) where ( when using C8 BD ) , BXD6D6 CXCY is the number of questions in Class i that are misclassified as belongC8 BD Word Pos Chunk NE Head RelWord h 77.60 78.20 77.40 78.80 78.80 84.20 f 52.40 77.20 77.00 78.40 76.80 84.00 C8 BOBPBH Word Pos Chunk NE Head RelWord h 86.00 86.60 87.60 88.60 89.40 95.00 f 83.20 86.80 86.60 88.40 89.80 95.60 Table 5 : Comparing accuracy of the hierarchical ( h ) and flat ( f ) classifiers on 500 TREC 10 question ; training is done on 5,500 questions .</sentence>
				<definiendum id="0">BW CXCY BP BXD6D6 CXCY A3BEBPB4C6 CX B7 C6 CY B5BN</definiendum>
				<definiendum id="1">BXD6D6 CXCY</definiendum>
				<definiens id="0">the tendency of Class CX to be confused with Class CY as follows</definiens>
				<definiens id="1">the number of questions in Class i that are misclassified as belongC8 BD Word Pos Chunk NE Head RelWord h 77.60 78.20 77.40 78.80 78.80 84.20 f 52.40 77.20 77.00 78.40 76.80 84.00 C8 BOBPBH Word Pos Chunk NE Head RelWord h 86.00 86.60 87.60 88.60 89.40 95.00 f 83.20 86.80 86.60 88.40 89.80 95.60 Table 5 : Comparing accuracy of the hierarchical ( h</definiens>
			</definition>
</paper>

		<paper id="1037">
</paper>

		<paper id="1111">
</paper>

		<paper id="2007">
			<definition id="0">
				<sentence>Fellbaum , C. ( 1999 ) WordNet : an Electronic Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="1">
				<sentence>Keil , F. C. ( 1979 ) Smantic and Conceptual Development : an Ontological Perspective .</sentence>
				<definiendum id="0">Conceptual Development</definiendum>
				<definiens id="0">an Ontological Perspective</definiens>
			</definition>
</paper>

		<paper id="1038">
</paper>

		<paper id="1120">
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>Natural Language Processing ( NLP ) systemswithmonolithicgrammars , inaddition , havetodealwithhugesearchspaceduetoseveral sources of non { determinism ( i.e. ambiguity ) .</sentence>
				<definiendum id="0">Natural Language Processing ( NLP )</definiendum>
				<definiens id="0">systemswithmonolithicgrammars , inaddition , havetodealwithhugesearchspaceduetoseveral sources of non { determinism ( i.e. ambiguity )</definiens>
			</definition>
			<definition id="1">
				<sentence>The former|TextHandling ( TH ) andorphographemic analyses|accountforsurfacepropertiesofinput text ( document formatting , delimitation oftextual structural elements , orthographemic aspects of morphology ) , while the latter | parsing and re nement| deal with its non { surface properties ( morphosyntactic analysis , constituent structure , semantic representation ) .</sentence>
				<definiendum id="0">former|TextHandling ( TH</definiendum>
				<definiens id="0">formatting , delimitation oftextual structural elements , orthographemic aspects of morphology ) , while the latter | parsing and re nement| deal with its non { surface properties ( morphosyntactic analysis</definiens>
			</definition>
			<definition id="2">
				<sentence>2 A distinctive feature of the ALEP processing architecture is the division of the analysis task into two sub { tasks : `parsing ' , which builds up a complete but shallow phrase structure tree , and `re nement ' , which traverses the structure top { down , thus monotonically performing feature decoration , typically with semantic information .</sentence>
				<definiendum id="0">ALEP processing architecture</definiendum>
				<definiens id="0">`parsing ' , which builds up a complete but shallow phrase structure tree</definiens>
			</definition>
			<definition id="3">
				<sentence>Integrating PoS information in a system like ALEP means de ning TS { LS rules propagatingthemorphosyntacticinformationassociated tofullforms ( i.e. PoStagandlemma ) delivered by the tagger to the relevant morphosyntactic featuresatthelexicalentriesofthegrammar .</sentence>
				<definiendum id="0">LS rules propagatingthemorphosyntacticinformationassociated tofullforms</definiendum>
				<definiens id="0">i.e. PoStagandlemma ) delivered by the tagger to the relevant morphosyntactic featuresatthelexicalentriesofthegrammar</definiens>
			</definition>
			<definition id="4">
				<sentence>6 The output of the lifting process is a Partial Linguistic Structure ( PLS ) where the hierarchical relations between the di erent structural elements is expressed in terms of week dominance relations .</sentence>
				<definiendum id="0">lifting process</definiendum>
				<definiens id="0">a Partial Linguistic Structure ( PLS ) where the hierarchical relations between the di erent structural elements is expressed in terms of week dominance relations</definiens>
			</definition>
</paper>

		<paper id="1014">
</paper>

		<paper id="2028">
			<definition id="0">
				<sentence>Lavie ( Lavie A. , 1996 ) used the GLR* parsing algorithm for spoken language system .</sentence>
				<definiendum id="0">Lavie</definiendum>
			</definition>
</paper>

		<paper id="1083">
			<definition id="0">
				<sentence>XML-based Tag Information Management System ( TIMS ) is a core machinery for managing XML tag information obtained from sub functional components .</sentence>
				<definiendum id="0">XML-based Tag Information Management System</definiendum>
				<definiens id="0">a core machinery for managing XML tag information obtained from sub functional components</definiens>
			</definition>
			<definition id="1">
				<sentence>ATRACT and LiLFeS play a central role in the knowledge acquisition process , which includes term recognition , ontology population , and ontology-based inference .</sentence>
				<definiendum id="0">ATRACT</definiendum>
				<definiendum id="1">LiLFeS</definiendum>
				<definiens id="0">includes term recognition , ontology population</definiens>
			</definition>
			<definition id="2">
				<sentence>JTAG is an XML-based manual annotation and resource description aid tool .</sentence>
				<definiendum id="0">JTAG</definiendum>
				<definiens id="0">an XML-based manual annotation and resource description aid tool</definiens>
			</definition>
			<definition id="3">
				<sentence>ATRACT ( Mima et al. , 2001a ) is a terminology management workbench that integrates ATR and ATC .</sentence>
				<definiendum id="0">ATRACT</definiendum>
				<definiens id="0">a terminology management workbench that integrates ATR and ATC</definiens>
			</definition>
			<definition id="4">
				<sentence>Besides term recognition , term clustering is an indispensable component in a knowledge management process ( see figure 2 ) .</sentence>
				<definiendum id="0">term clustering</definiendum>
				<definiens id="0">an indispensable component in a knowledge management process</definiens>
			</definition>
			<definition id="5">
				<sentence>The tag data manager retrieves the relevant data from the collection of documents via a tag database and ontology-based inference ( such as POS Tagger Acronym Recognition C-value ATR Orthographic Variants Morphological Variants Syntactic Variants NC-value Analyzer Term Clustering ( Semantic Analyzer ) XML Documents Including Term Tags and Term Variation/Class Information Input Documents Figure 2 .</sentence>
				<definiendum id="0">tag data manager</definiendum>
				<definiendum id="1">Tagger Acronym Recognition C-value ATR Orthographic Variants Morphological Variants Syntactic Variants NC-value Analyzer Term Clustering</definiendum>
				<definiens id="0">retrieves the relevant data from the collection of documents via a tag database and ontology-based inference ( such as POS</definiens>
			</definition>
			<definition id="6">
				<sentence>Each interval operation takes two sets of intervals as input and returns a set of intervals according to the specified logical operations .</sentence>
				<definiendum id="0">interval operation</definiendum>
				<definiens id="0">takes two sets of intervals as input and returns a set of intervals according to the specified logical operations</definiens>
			</definition>
			<definition id="7">
				<sentence>Similarly , suppose X denotes a set of intervals of manually annotated tags for a document and Y denotes a set of intervals of automatically annotated tags for the same document .</sentence>
				<definiendum id="0">suppose X</definiendum>
				<definiendum id="1">Y</definiendum>
				<definiens id="0">a set of intervals of manually annotated tags for a document</definiens>
				<definiens id="1">a set of intervals of automatically annotated tags for the same document</definiens>
			</definition>
			<definition id="8">
				<sentence>Figure 3 : Question-answering process in TIMS Database A A A A A A XML / HTML Knowledge Sources Tag Data Language Analyzer Tag Data Manager TIQL Processor NLP Components TIMS Query to TIQL Translator Query ATRACT Ontology Data A A A A A A LiLFeS Knowledge Acquisition Knowledge Integration … 26 15 VERB … 110 100 ADJ … 150 140 DNA … … … … .</sentence>
				<definiendum id="0">Data A A A A A A LiLFeS</definiendum>
				<definiens id="0">Question-answering process in TIMS Database A A A A A A XML / HTML Knowledge Sources Tag Data Language Analyzer Tag Data Manager TIQL Processor NLP Components TIMS Query to TIQL Translator Query ATRACT Ontology</definiens>
			</definition>
			<definition id="9">
				<sentence>The basic expression in TIQL has the following form : SELECT [ n-tuple variables ] FROM [ XML document ( s ) ] WHERE [ interval operation ] FROM [ XML document ( s ) ] WHERE [ interval operation ] …… where , [ n-tuple variables ] specifies the table output format , [ XML document ( s ) ] denotes the document ( s ) to be processed , and [ interval operation ] denotes an interval operation to be performed over the corresponding document with variables of each interval to be bound .</sentence>
				<definiendum id="0">XML document</definiendum>
				<definiens id="0">n-tuple variables ] FROM [ XML document ( s ) ] WHERE [ interval operation ] FROM [ XML document ( s ) ] WHERE [ interval operation ] …… where , [ n-tuple variables ] specifies the table output format</definiens>
				<definiens id="1">the document ( s ) to be processed , and [ interval operation ] denotes an interval operation to be performed over the corresponding document with variables of each interval to be bound</definiens>
			</definition>
			<definition id="10">
				<sentence>The term recognition evaluation was performed on the NACSIS AI-domain corpus ( Koyama et al. , 1998 ) , which includes 1800 abstracts and on a set of MEDLINE abstracts .</sentence>
				<definiendum id="0">NACSIS AI-domain corpus</definiendum>
			</definition>
			<definition id="11">
				<sentence>We described TIMS , an XML-based integrated KA aid system , in which we have integrated automatic term recognition , term clustering , tagged data management and ontology-based knowledge retrieval .</sentence>
				<definiendum id="0">TIMS</definiendum>
				<definiens id="0">an XML-based integrated KA aid system , in which we have integrated automatic term recognition , term clustering , tagged data management and ontology-based knowledge retrieval</definiens>
			</definition>
			<definition id="12">
				<sentence>New York University , available at http : //www.tipster.org/arch.htm Jacquemin C. and Tzoukermann E. ( 1999 ) NLP for Term Variant Extraction : A Synergy of Morphology , Lexicon and Syntax .</sentence>
				<definiendum id="0">Extraction</definiendum>
				<definiens id="0">A Synergy of Morphology , Lexicon and Syntax</definiens>
			</definition>
			<definition id="13">
				<sentence>MEDLINE ( 2002 ) National Library of Medicine , http : //www.ncbi.nlm.nih.gov/PubMed/ Mima H. , Ananiadou S. and Nenadic G. ( 2001a ) ATRACT Workbench : An Automatic Term Recognition and Clustering of Te rms , in Text , Speech and Dialogue TSD2001 , Lecture Notes in AI 2166 , Springer Verlag Mima H. and Ananiadou S. ( 2001b ) An Application and Evaluation of the C/NC-value Approach for the Automatic term Recognition of Multi-Word units in Japanese , in International Journal on Terminology , Vol .</sentence>
				<definiendum id="0">C/NC-value Approach</definiendum>
				<definiens id="0">An Application and Evaluation of the</definiens>
			</definition>
</paper>

		<paper id="2002">
</paper>

		<paper id="1146">
</paper>

		<paper id="1066">
			<definition id="0">
				<sentence>Uni cation grammars have originated as an extension of context-free grammars , the basic idea being to augment the context-free rules with feature structures ( FSs ) in order to express additional information .</sentence>
				<definiendum id="0">Uni cation grammars</definiendum>
				<definiendum id="1">FSs</definiendum>
				<definiens id="0">an extension of context-free grammars , the basic idea being to augment the context-free rules with feature structures</definiens>
			</definition>
			<definition id="1">
				<sentence>The recognition problem ( also known as the membership problem ) , for a grammar a0 and a string a1 , is whether a1a3a2a5a4a10a6 a0a9a8 .</sentence>
				<definiendum id="0">recognition problem</definiendum>
			</definition>
			<definition id="2">
				<sentence>A multi-rooted feature structure ( MRS ) is a pair a11a13a12a14a16a15a17a0a9a18 where a0a20a19 a11 a14a21a15a23a22a24a15a23a25a26a18 is a nite , directed , labelled graph consisting of a set a14 a27 NODES of nodes , a partial function , a22a29a28a26a14a31a30 FEATS a32 a14 , specifying the arcs and a partial function , a25a33a28a34a14 a32 ATOMS , labelling the sinks , and a12a14 is an ordered set of distinguished nodes in a14 called roots .</sentence>
				<definiendum id="0">MRS )</definiendum>
				<definiendum id="1">a12a14</definiendum>
				<definiens id="0">a pair a11a13a12a14a16a15a17a0a9a18 where a0a20a19 a11 a14a21a15a23a22a24a15a23a25a26a18 is a nite , directed , labelled graph consisting of a set a14 a27 NODES of nodes , a partial function , a22a29a28a26a14a31a30 FEATS a32 a14 , specifying the arcs and a partial function</definiens>
				<definiens id="1">an ordered set of distinguished nodes in a14 called roots</definiens>
			</definition>
			<definition id="3">
				<sentence>The length of an MRS is the number of its roots , a35 a12a14 a35 .</sentence>
				<definiendum id="0">length of an MRS</definiendum>
				<definiens id="0">the number of its roots</definiens>
			</definition>
			<definition id="4">
				<sentence>Skeletal grammars are a variant of uni cation grammars which have an explicit context-free backbone/skeleton .</sentence>
				<definiendum id="0">Skeletal grammars</definiendum>
				<definiens id="0">a variant of uni cation grammars which have an explicit context-free backbone/skeleton</definiens>
			</definition>
			<definition id="5">
				<sentence>A skeletal grammar ( over FEATS , ATOMS and CATS ) is a tuplea0a90a19 a11a88a91 a15a38a92a7a15 a45a94a93 a18 where a91 is a nite set of rules , each of which is an MRS of length a95a97a96a99a98 ( with a designated rst element , the head of the rule ) , and a sequence of length a95 of categories ; a92 is a lexicon , which associates with every terminal a100 ( over a xed nite set a101 of terminals ) a nite set of extended categoriesa92 a6a83a100 a8 ; a45 a93 is the start symbol ( an extended category ) .</sentence>
				<definiendum id="0">skeletal grammar</definiendum>
				<definiendum id="1">ATOMS</definiendum>
				<definiendum id="2">a92</definiendum>
				<definiens id="0">a tuplea0a90a19 a11a88a91 a15a38a92a7a15 a45a94a93 a18 where a91 is a nite set of rules , each of which is an MRS of length a95a97a96a99a98 ( with a designated rst element , the head of the rule )</definiens>
				<definiens id="1">associates with every terminal a100 ( over a xed nite set a101 of terminals ) a nite set of extended categoriesa92 a6a83a100 a8 ; a45 a93 is the start symbol ( an extended category</definiens>
			</definition>
			<definition id="6">
				<sentence>A skeletal form is a pair a11a50a36 a15a103a102a89a38a18 , where a36 is an MRS of length a95 and a102a89 is a sequence of a95 categories ( a89 a42 a2 CATS for a98a105a104a106a60a107a104a106a95 ) .</sentence>
				<definiendum id="0">skeletal form</definiendum>
				<definiendum id="1">a36</definiendum>
				<definiens id="0">a sequence of a95 categories ( a89 a42 a2 CATS for a98a105a104a106a60a107a104a106a95</definiens>
			</definition>
			<definition id="7">
				<sentence>A general uni cation grammar ( over FEATS and ATOMS ) is a tuple a0 a19 a11a83a91 a15a38a92a56a15 a45 a93 a18 where a91 is a nite set of rules , each of which is an MRS of length a95a71a96a151a98 ; a92 is a lexicon , which associates with every terminal a100 a nite set of FSs a92 a6a83a100 a8 ; a45 a93 is the start symbol ( an FS ) .</sentence>
				<definiendum id="0">general uni cation grammar</definiendum>
				<definiendum id="1">a92</definiendum>
				<definiens id="0">a tuple a0 a19 a11a83a91 a15a38a92a56a15 a45 a93 a18 where a91 is a nite set of rules , each of which is an MRS of length a95a71a96a151a98</definiens>
			</definition>
			<definition id="8">
				<sentence>A constituent structure satis es the off-line parsability constraint iff it does not include a non-branching dominance chain in which the same category appears twice and the empty string a158 does not appear as a lexical form annotation of any ( terminal ) node .</sentence>
				<definiendum id="0">constituent structure satis</definiendum>
				<definiens id="0">es the off-line parsability constraint iff it does not include a non-branching dominance chain in which the same category appears twice and the empty string a158 does not appear as a lexical form annotation of any ( terminal ) node</definiens>
			</definition>
			<definition id="9">
				<sentence>In other words , Kuhn assumes that OLP is , in fact , a condition that is intended to guarantee nite ambiguity .</sentence>
				<definiendum id="0">Kuhn</definiendum>
				<definiens id="0">intended to guarantee nite ambiguity</definiens>
			</definition>
			<definition id="10">
				<sentence>The feature DEPTH represents the current depth of the derivation tree ; at each derivation step an item is added to the DEPTH list .</sentence>
				<definiendum id="0">DEPTH</definiendum>
				<definiens id="0">the current depth of the derivation tree</definiens>
			</definition>
			<definition id="11">
				<sentence>The feature TEMP represents the number of derivation steps before generating the next a200 symbol .</sentence>
				<definiendum id="0">TEMP</definiendum>
				<definiens id="0">the number of derivation steps before generating the next a200 symbol</definiens>
			</definition>
			<definition id="12">
				<sentence>A grammar whose language consists of only one word , and its derivation is of a constant depth , may still contain a redundant rule generating arbitrarily deep trees whose frontier is of length a98 .</sentence>
				<definiendum id="0">grammar whose language</definiendum>
				<definiens id="0">consists of only one word , and its derivation is of a constant depth , may still contain a redundant rule generating arbitrarily deep trees whose frontier is of length a98</definiens>
			</definition>
</paper>

		<paper id="1128">
			<definition id="0">
				<sentence>There are important analogies between this process and the process of authoring an XML document under the control of a DTD or a Schema , but DCGs are more expressive in terms of the contextual constraints that can be expressed and also are more adapted to the production of grammatical text .</sentence>
				<definiendum id="0">XML document</definiendum>
				<definiens id="0">under the control of a DTD or a Schema , but DCGs are more expressive in terms of the contextual constraints that can be expressed</definiens>
			</definition>
			<definition id="1">
				<sentence>A given grammar covers a semantically unified class of documents ( e.g. employment offers , drug package leaflets , etc. ) , in a way analogous to the customized XML DTDs used for technical documentation .</sentence>
				<definiendum id="0">given grammar</definiendum>
				<definiens id="0">covers a semantically unified class of documents ( e.g. employment offers , drug package leaflets</definiens>
				<definiens id="1">in a way analogous to the customized XML DTDs used for technical documentation</definiens>
			</definition>
			<definition id="2">
				<sentence>10 An introduction to DLs would take us too far afield ; let’s just say that there is a whole family of DLs , which differ by the logical constructors they allow , and that most can be seen as decidable fragments of first-order logic .</sentence>
				<definiendum id="0">DLs</definiendum>
				<definiens id="0">differ by the logical constructors they allow , and that most can be seen as decidable fragments of first-order logic</definiens>
			</definition>
			<definition id="3">
				<sentence>However the two constraints can not be simultaneously satisfied in KB1 ; This can be shown computationally by using the satisfiability check in KB1 , but also by the following informal reasoning : df ( D , tablet ) and da ( D , drink ) imply that D is both in TabletDrugs and in DrinkDrugs ; by the second fact it is in SolutionDrugs , butSolutionDrugs and TabletDrugs have an empty intersection .</sentence>
				<definiendum id="0">TabletDrugs</definiendum>
				<definiens id="0">an empty intersection</definiens>
			</definition>
			<definition id="4">
				<sentence>Then the KB “knows” that tablet is the only choice for F and swallow the only choice for A. Indeed they are possible choices ( because df ( diprox , tablet ) and da ( diprox , swallow ) are in the ABOX of the KB ) , but are also the only choices , for diprox is now known to be in TabletDrugs and in SwallowDrugs ; it can therefore not be in SolutionDrugs or in ChewDrugs or in DrinkDrugs , which means that none of the facts df ( dirprox , solution ) , da ( diprox , chew ) or da ( diprox , drink ) may hold .</sentence>
				<definiendum id="0">KB “knows” that tablet</definiendum>
				<definiendum id="1">da</definiendum>
				<definiendum id="2">diprox</definiendum>
				<definiens id="0">diprox , swallow ) are in the ABOX of the KB ) , but are also the only choices , for diprox is now known to be in TabletDrugs and in SwallowDrugs ; it can therefore not be in SolutionDrugs or in ChewDrugs or in DrinkDrugs , which means that none of the facts df ( dirprox , solution ) , da ( diprox , chew</definiens>
			</definition>
			<definition id="5">
				<sentence>CARIN : A representation language combining horn rules and description logics .</sentence>
				<definiendum id="0">CARIN</definiendum>
				<definiens id="0">A representation language combining horn rules and description logics</definiens>
			</definition>
</paper>

		<paper id="1130">
			<definition id="0">
				<sentence>Topic Sig Score Type = Σ N [ λ-score of word n , Type / ( distance from instance ) 2 ] where N is the number of words in the sentence , λ-score of word n , Type is the topic signature score of word n for topic Type , and distance from instance is the number of words away from the instance that word n is .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">Type</definiendum>
				<definiens id="0">the number of words in the sentence , λ-score of word n</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>The cooccurrence value is the ratio of the bigram frequency count freq ( word ; preposition ) divided by the unigram frequency freq ( word ) .</sentence>
				<definiendum id="0">cooccurrence value</definiendum>
				<definiens id="0">the ratio of the bigram frequency count freq ( word ; preposition ) divided by the unigram frequency freq ( word )</definiens>
			</definition>
			<definition id="1">
				<sentence>if ( support_verb_unit ( V , P , N2 ) ) then verb attachment elsif ( cooc ( N1 , P , N2 ) &amp; &amp; cooc ( V , P , N2 ) ) then if ( ( cooc ( N1 , P , N2 ) * nf ) &gt; = cooc ( V , P , N2 ) ) then noun attachment else verb attachment elsif ( cooc ( N1 , P ) &amp; &amp; cooc ( V , P ) ) then if ( ( cooc ( N1 , P ) * nf ) &gt; = cooc ( V , P ) ) then noun attachment else verb attachment elsif ( cooc ( N1 , P ) &gt; threshold ( N ) ) then noun attachment elsif ( cooc ( V , P ) &gt; threshold ( V ) ) then verb attachment The noun factors for triple comparison and factor correct incorrect accuracy threshold noun attachment 5.47 ; 5.97 2213 424 83.92 % 0.020 verb attachment 1077 314 77.43 % 0.109 total 3290 738 81.67 % decidable test cases 4028 ( of 4469 ) coverage : 90.13 % Table 1 : Attachment accuracy for the CZ test set using cooccurrence values from unsupervised learning .</sentence>
				<definiendum id="0">support_verb_unit</definiendum>
			</definition>
			<definition id="2">
				<sentence>Next is the application of supervised quadruples ( level 2 ) , followed by supervised triples ( level 3 ) .</sentence>
				<definiendum id="0">Next</definiendum>
				<definiens id="0">the application of supervised quadruples ( level 2 ) , followed by supervised triples ( level 3 )</definiens>
			</definition>
</paper>

		<paper id="1114">
</paper>

		<paper id="1151">
			<definition id="0">
				<sentence>Definition 2.1 ( Sentence &amp; Entity ) A sentence S is a linked list which consists of words w and entities E. An entity can be a single word or a set of consecutive words with a predefined boundary .</sentence>
				<definiendum id="0">sentence S</definiendum>
				<definiens id="0">a linked list which consists of words w and entities E. An entity can be a single word or a set of consecutive words with a predefined boundary</definiens>
			</definition>
			<definition id="1">
				<sentence>Definition 2.3 ( Classes ) We denote the set of predefined entity classes and relation classes as CE and CR respectively .</sentence>
				<definiendum id="0">Definition 2.3 ( Classes</definiendum>
				<definiens id="0">the set of predefined entity classes and relation classes as CE and CR respectively</definiens>
			</definition>
			<definition id="2">
				<sentence>CE has one special element other ent , which represents any unlisted entity class .</sentence>
				<definiendum id="0">CE</definiendum>
				<definiens id="0">represents any unlisted entity class</definiens>
			</definition>
			<definition id="3">
				<sentence>Definition 2.4 ( Constraint ) A constraint C is a 3-tuple ( R ; E1 ; E2 ) , where R 2 CR and E1 ; E2 2 CE .</sentence>
				<definiendum id="0">constraint C</definiendum>
				<definiens id="0">a 3-tuple ( R ; E1 ; E2 ) , where R 2 CR and E1</definiens>
			</definition>
			<definition id="4">
				<sentence>( e1 ; : : : ; en ; r12 ; r21 ; : : : ; rn ( n¡1 ) ) = argmaxei ; rjkProb ( E1 ; : : : ; En ; R12 ; R21 ; : : : ; Rn ( n¡1 ) ) : Each nontrivial property of the entities and relations , such as the class label , depends on a very large number of variables .</sentence>
				<definiendum id="0">rjkProb</definiendum>
				<definiens id="0">the class label , depends on a very large number of variables</definiens>
			</definition>
			<definition id="5">
				<sentence>The propositional learner we use is SNoW ( Roth , 1998 ; Carleson et al. , 1999 ) 1 SNoW is a multi-class classifier that is specifically tailored for large scale learning tasks .</sentence>
				<definiendum id="0">SNoW</definiendum>
				<definiens id="0">a multi-class classifier that is specifically tailored for large scale learning tasks</definiens>
			</definition>
			<definition id="6">
				<sentence>The learning architecture makes use of a network of linear functions , in which the targets ( entity classes or relation classes , in this case ) are represented as linear 1available at http : //L2R.cs.uiuc.edu/»cogcomp/cc-software.html functions over a common feature space .</sentence>
				<definiendum id="0">learning architecture</definiendum>
				<definiens id="0">makes use of a network of linear functions , in which the targets ( entity classes or relation classes</definiens>
			</definition>
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>Word sense disambiguation ( WSD ) is an “intermediate” task that is necessary for accomplishing most natural language processing tasks , especially machine translation and information retrieval .</sentence>
				<definiendum id="0">Word sense disambiguation ( WSD</definiendum>
				<definiens id="0">an “intermediate” task that is necessary for accomplishing most natural language processing tasks , especially machine translation and information retrieval</definiens>
			</definition>
			<definition id="1">
				<sentence>A variety of unsupervised WSD methods , which use a machinereadable dictionary or thesaurus in addition to a corpus , have also been proposed ( Yarowsky 1992 ; Yarowsky 1995 ; Karov and Edelman 1998 ) .</sentence>
				<definiendum id="0">variety of unsupervised WSD methods</definiendum>
				<definiens id="0">use a machinereadable dictionary or thesaurus in addition to a corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>A comparable corpus consists of a first-language corpus and a second-language corpus of the same domain .</sentence>
				<definiendum id="0">comparable corpus</definiendum>
				<definiens id="0">consists of a first-language corpus and a second-language corpus of the same domain</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , the alignment of ( tank , gasoline ) with ( タンク &lt; TANKU &gt; , ガソリン &lt; GASORIN &gt; ) implies that “gasoline” is a clue for selecting the “container” sense of “tank” , which is translated as “タンク &lt; TANKU &gt; ” , and the alignment of ( tank , soldier ) with ( 戦 車 &lt; SENSYA &gt; , 兵士 &lt; HEISI &gt; ) implies that “soldier” is a clue for selecting the “military vehicle” sense of “tank” , which is translated as “戦車 &lt; SENSYA &gt; ” .</sentence>
				<definiendum id="0">“soldier”</definiendum>
				<definiens id="0">is translated as “タンク &lt; TANKU &gt; ”</definiens>
				<definiens id="1">a clue for selecting the “military vehicle” sense of “tank” , which is translated as “戦車 &lt; SENSYA &gt; ”</definiens>
			</definition>
			<definition id="4">
				<sentence>MI ( x , x’ ) is defined by the following formula : ) 'xPr ( ) xPr ( ) 'x , xPr ( log ) 'x , x ( MI ⋅ = , where Pr ( x ) is the occurrence probability of x , and Pr ( x , x’ ) is the co-occurrence probability of x and x’ .</sentence>
				<definiendum id="0">MI ( x , x’ )</definiendum>
				<definiendum id="1">Pr ( x )</definiendum>
				<definiendum id="2">Pr ( x , x’ )</definiendum>
				<definiens id="0">the following formula : ) 'xPr ( ) xPr ( ) 'x , xPr ( log ) 'x , x ( MI ⋅ = , where</definiens>
				<definiens id="1">the occurrence probability of x</definiens>
				<definiens id="2">the co-occurrence probability of x and x’</definiens>
			</definition>
			<definition id="5">
				<sentence>Let X ( x ) be the set of clues for determining the sense of a first-language polysemous word x , i.e. , X ( x ) = { x’| ( x , x’ ) ∈R X } .</sentence>
				<definiendum id="0">X ( x )</definiendum>
				<definiens id="0">the set of clues for determining the sense of a first-language polysemous word x</definiens>
			</definition>
			<definition id="6">
				<sentence>The score of each sense of the polysemous word is defined as the sum of the correlations between the sense and clues appearing in the context , i.e. , ( ) ( ) ∑ ∈ = ) x ( Context ) j ( 'x ) j ( 'x ) , i ( SC ) i ( SScore .</sentence>
				<definiendum id="0">score of each sense of the polysemous word</definiendum>
				<definiens id="0">the sum of the correlations between the sense and clues appearing in the context , i.e. , ( ) ( ) ∑ ∈ = ) x ( Context ) j ( 'x ) j</definiens>
			</definition>
			<definition id="7">
				<sentence>EDR ( Japan Electronic Dictionary Research Institute ) English-to-Japanese and Japanese-to-English dictionaries were merged for the experiment .</sentence>
				<definiendum id="0">EDR</definiendum>
				<definiens id="0">Japan Electronic Dictionary Research Institute ) English-to-Japanese and Japanese-to-English dictionaries were merged for the experiment</definiens>
			</definition>
			<definition id="8">
				<sentence>0 1 2 3 012345678910 Iteration Correlation C ( { tank , タンク &lt; TANKU &gt; , 水槽 &lt; SUISOU &gt; , 槽 &lt; SOU &gt; } , troop ) C ( { tank , 戦車 &lt; SENSYA &gt; } , troop ) C ( { tank , タンク &lt; TANKU &gt; , 水槽 &lt; SUISOU &gt; , 槽 &lt; SOU &gt; } , ozone ) C ( { tank , 戦車 &lt; SENSYA &gt; } , ozone ) C ( { tank , タンク &lt; TANKU &gt; , 水槽 &lt; SUISOU &gt; , 槽 &lt; SOU &gt; } , safety ) C ( { tank , 戦車 &lt; SENSYA &gt; } , safety ) Fig .</sentence>
				<definiendum id="0">Iteration Correlation C</definiendum>
				<definiens id="0">{ tank , タンク &lt; TANKU &gt; , 水槽 &lt; SUISOU &gt; , 槽 &lt; SOU &gt; } , troop ) C ( { tank , 戦車 &lt; SENSYA &gt; } , troop ) C ( { tank , タンク &lt; TANKU &gt; , 水槽 &lt; SUISOU &gt; , 槽 &lt; SOU &gt; } , ozone ) C ( { tank , 戦車 &lt; SENSYA &gt; } , ozone ) C ( { tank , タンク &lt; TANKU &gt; , 水槽 &lt; SUISOU &gt; , 槽 &lt; SOU &gt; } , safety ) C ( { tank , 戦車 &lt; SENSYA &gt; } , safety</definiens>
			</definition>
			<definition id="9">
				<sentence>The precision is the proportion of disambiguated instances of the test word ( s ) that the method disambiguated correctly .</sentence>
				<definiendum id="0">precision</definiendum>
				<definiens id="0">the proportion of disambiguated instances of the test word ( s ) that the method disambiguated correctly</definiens>
			</definition>
			<definition id="10">
				<sentence>Total S1= { promotion , 宣伝 , 売り込み , 販売促進 , プロモーション } 71 1 0 1 73 S2= { promotion , 昇格 , 昇進 , 昇任 , 就任 , 登用 , 進級 } 6 15 0 3 24 S3= { promotion , 奨励 , , 促進 , 進 , } 21003 Total 79 17 0 4 100 [ Note ] S1 : an activity intended to help sell a product S2 : advancement in rank or position S3 : action to help something develop or succeed ( c ) Polysemous word “race” ( applicability=79.0 % ; precision=57.0 % ) Results Correct sense S1 S2S3 ?</sentence>
				<definiendum id="0">S2</definiendum>
				<definiens id="0">an activity intended to help sell a product</definiens>
			</definition>
			<definition id="11">
				<sentence>Total S1= { title , 肩書き , 称号 , 敬称 } 43100246 S2= { title , 題名 , 題目 , 表題 , 書名 } 62601538 S3= { title , 権 , 格 , 権 } 11011 4 S4= { title , 選手権 } 3306012 Total 533108810 [ Note ] S1 : a word or name given to a person to be used before his/her name as a sign rank , profession , etc .</sentence>
				<definiendum id="0">profession</definiendum>
				<definiens id="0">a word or name given to a person to be used before his/her name as a sign rank ,</definiens>
			</definition>
			<definition id="12">
				<sentence>S3 : the legal right to own something S4 : the position of being the winner of an sports competition ( f ) Polysemous word “trial” ( applicability=92.0 % ; precision=92.4 % ) Results Correct sense S1 S2S3 S4 S5 ?</sentence>
				<definiendum id="0">S3</definiendum>
				<definiens id="0">the legal right to own something S4 : the position of being the winner of an sports competition ( f ) Polysemous word “trial”</definiens>
			</definition>
			<definition id="13">
				<sentence>Tanaka and Iwasaki ( 1996 ) exploited the idea of translingually aligning word co-occurrences to extract pairs consisting of a word and its translation form a non-aligned ( comparable ) corpus .</sentence>
				<definiendum id="0">non-aligned</definiendum>
				<definiens id="0">the idea of translingually aligning word co-occurrences to extract pairs consisting of a word</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>Since the bilingual corpus is a valuable resource for training statistical language models [ Dagon , 91 ; Su et al. , 95 ; Su and Chang , 99 ] and sentence alignment is the first step for most such tasks , many alignment approaches have been proposed in the literature [ Brown , 91 ; Gale and Church , 93 ; Wu , 94 ; Vogel et al. , 96 ; Och and Ney , 2000 ] .</sentence>
				<definiendum id="0">sentence alignment</definiendum>
				<definiens id="0">a valuable resource for training statistical language models [ Dagon , 91 ; Su et al. , 95 ; Su and Chang , 99 ] and</definiens>
			</definition>
			<definition id="1">
				<sentence>Let Mi = { typei,1 , ··· , typei , Ni } denote the i-th possible alignment-candidate , consisting of Ni Alignment-Passages of typei , j , j = 1 , ··· , Ni ; where typei , j is the matching type ( e.g. , 1−1 , 0−1 , 1−0 , etc. ) of the j-th Alignment-Passage in the i-th alignment-candidate , and Ni denotes the number of the total Alignment-Passages in the i-th alignmentcandidate .</sentence>
				<definiendum id="0">Ni</definiendum>
			</definition>
			<definition id="2">
				<sentence>( 2.1 ) According to the Bayesian rule , the maximization problem in ( 2.1 ) is equivalent to solving the following maximization equation M∗ = argmax Mi P ( ESm1 , CSn1 |Mi ) P ( Mi ) = argmax Mi { P ( Aligned-Pairi , Nii,1 |typei , Nii,1 ) P ( typei , Nii,1 ) } = argmax Mi Niproductdisplay j=1 { P ( Aligned-Pairi , j|Aligned-Pairi , j−1i,1 , typei , ji,1 ) × P ( typei , j|typei , j−1i,1 ) } , ( 2.2 ) where Aligned-Pairi , j , j = 1 , ··· , Ni , denotes the j-th aligned English-Chinese bilingual sentence groups pair in the i-th alignment candidate .</sentence>
				<definiendum id="0">Aligned-Pairi</definiendum>
				<definiens id="0">equivalent to solving the following maximization equation M∗ = argmax Mi P ( ESm1 , CSn1 |Mi ) P ( Mi ) = argmax Mi { P ( Aligned-Pairi , Nii,1 |typei , Nii,1 ) P ( typei , Nii,1 ) } = argmax Mi Niproductdisplay j=1 { P ( Aligned-Pairi , j|Aligned-Pairi , j−1i,1 , typei , ji,1 ) × P ( typei , j|typei , j−1i,1</definiens>
				<definiens id="1">the j-th aligned English-Chinese bilingual sentence groups pair in the i-th alignment candidate</definiens>
			</definition>
			<definition id="3">
				<sentence>The baseline model will use both the length of sentence [ Brown et al. , 91 ; Gale and Church , 93 ] and English cognates [ Wu , 94 ] , and is shown as follows : argmaxM i Niproductdisplay j=1 f ( δc , δw|typei , j ) P ( δcognate ) P ( typei , j ) , ( 2.5 ) where δc and δw denote the normalized differences of characters and words as explained in the following ; δc is defined to be ( ltc − clsc ) /radicalbiglscs2c , where lsc and ltc are the character numbers of the aligned bilingual portions of source text and target text , respectively , under consideration ; c denotes the proportional constant for target-character-count and s2c denotes the corresponding target-charactercount variance per source-character .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">use both the length of sentence [ Brown et al. , 91 ; Gale and Church , 93 ] and English cognates [ Wu , 94 ] , and is shown as follows : argmaxM i Niproductdisplay j=1 f ( δc , δw|typei , j ) P ( δcognate ) P ( typei , j ) , ( 2.5 ) where δc and δw denote the normalized differences of characters</definiens>
				<definiens id="1">the character numbers of the aligned bilingual portions of source text and target text , respectively , under consideration ;</definiens>
				<definiens id="2">the corresponding target-charactercount variance per source-character</definiens>
			</definition>
			<definition id="4">
				<sentence>Similarly , δw is defined to be ( ltw − wlsw ) /radicalbiglsws2w , where lsw and ltw are the word numbers of the aligned bilingual portions of source text and target text , respectively ; w denotes the proportional constant for target-word-count and s2w denotes the corresponding target-word-count variance per sourceword .</sentence>
				<definiendum id="0">w</definiendum>
				<definiendum id="1">s2w</definiendum>
				<definiens id="0">the word numbers of the aligned bilingual portions of source text and target text</definiens>
			</definition>
			<definition id="5">
				<sentence>Then S ( m , n ) can be evaluated recursively with the initial condition of S ( 0,0 ) = 0 in the following way : S ( m , n ) = max 0≤h , k≤4 S ( m−h , n−k ) +score ( h , k ) , ( 3.1 ) where score ( h , k ) denotes the local scoring function to evaluate the local passage of matching type h−k. In the experiments , a training set consisting of 7,331 pairs of bilingual sentences , and a testing set with 1,514 pairs of bilingual sentences are extracted from the Caterpillar User Manual which is mainly about machinery .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the initial condition of S ( 0,0 ) = 0 in the following way : S ( m , n ) = max 0≤h , k≤4 S</definiens>
			</definition>
			<definition id="6">
				<sentence>The 5Which is defined as 2pr p+r. Training Set Testing Set I Testing Set II [ Caterpillar User Manual ] [ Caterpillar User Manual ] [ Sinorama Manazine ] Baseline Model 98.91 98.21 85.56 CTL 98.26 97.51 97.51 CTL+CL 99.32 98.19 89.61 CTL+CL+WL 99.61 98.83 94.07 CTL+CL+WL+EC 99.75 99.11 94.16 Table 4.1 : Performance ( F-measure % ) of each model and SFS result also indicates that the length-related features are still useful , even though they are relatively unreliable .</sentence>
				<definiendum id="0">5Which</definiendum>
				<definiens id="0">2pr p+r. Training Set Testing Set I Testing Set II [ Caterpillar User Manual ] [ Caterpillar User Manual ] [ Sinorama Manazine ] Baseline Model</definiens>
			</definition>
			<definition id="7">
				<sentence>A robust statistical sentences alignment model , which integrates the associated transfer-lexicons into the original lengthbased model , is thus proposed in this paper .</sentence>
				<definiendum id="0">robust statistical sentences alignment model</definiendum>
				<definiens id="0">integrates the associated transfer-lexicons into the original lengthbased model</definiens>
			</definition>
</paper>

		<paper id="1141">
			<definition id="0">
				<sentence>The User model is a knowledge base ΣU = ( TU , AU ) such that TU and AU are respectivly subsets of TD and AD .</sentence>
				<definiendum id="0">User model</definiendum>
				<definiens id="0">such that TU and AU are respectivly subsets of TD and AD</definiens>
			</definition>
			<definition id="1">
				<sentence>As is well known , designing knowledge bases ( dkb and ukb ) and translating the input of the generator into concepts and roles of the DL is a difficult task which has to be fulfilled for every generator .</sentence>
				<definiendum id="0">designing knowledge bases</definiendum>
				<definiendum id="1">DL</definiendum>
				<definiens id="0">a difficult task which has to be fulfilled for every generator</definiens>
			</definition>
			<definition id="2">
				<sentence>U ( the universe ) is the set of discourse referents .</sentence>
				<definiendum id="0">U</definiendum>
			</definition>
			<definition id="3">
				<sentence>Con contains the truth conditions representing the meaning of the discourse .</sentence>
				<definiendum id="0">Con</definiendum>
				<definiens id="0">contains the truth conditions representing the meaning of the discourse</definiens>
			</definition>
			<definition id="4">
				<sentence>U is a set of labels of drs or sdrs which can be viewed as “speech act discourse referents” ( Asher and Lascarides , 1998 ) .</sentence>
				<definiendum id="0">U</definiendum>
			</definition>
			<definition id="5">
				<sentence>Con is a set of conditions on labels of the form : • pi : K , where pi is a label from U and K is a ( s ) drs • R ( pii , pij ) , where pii and pij are labels and R a discourse relation .</sentence>
				<definiendum id="0">Con</definiendum>
				<definiendum id="1">K</definiendum>
				<definiens id="0">a set of conditions on labels of the form : • pi : K , where pi is a label from U and</definiens>
				<definiens id="1">a ( s ) drs • R ( pii , pij )</definiens>
			</definition>
			<definition id="6">
				<sentence>( 1 ) a50 ( Narration ( pi1 , pi2 ) → me ( pi1 ) &lt; me ( pi2 ) ) For text generation , this axiom is reversed as shown below ( Roussarie , 2000 , p. 154 ) : • If k1 and k2 are drss whose main eventualities are not states , • and if the main event of k1 occurs before the main event of k2 , • then Narration ( pi1 , pi2 ) is valid when pi1 and pi2 respectively label k1 and k2 .</sentence>
				<definiendum id="0">Narration</definiendum>
				<definiens id="0">valid when pi1 and pi2 respectively label k1 and k2</definiens>
			</definition>
			<definition id="7">
				<sentence>, where Si generates pii , Sj pij , and Cue is a cue phrase which is encoded in the lexical database associated with R ( Cue may be empty ) .</sentence>
				<definiendum id="0">Si</definiendum>
				<definiendum id="1">Cue</definiendum>
				<definiens id="0">a cue phrase which is encoded in the lexical database associated with R ( Cue may be empty )</definiens>
			</definition>
			<definition id="8">
				<sentence>G-TAG : A lexicalized formalism for text generation inspired by Tree Adjoining Grammar .</sentence>
				<definiendum id="0">G-TAG</definiendum>
				<definiens id="0">A lexicalized formalism for text generation inspired by Tree Adjoining Grammar</definiens>
			</definition>
</paper>

		<paper id="2025">
			<definition id="0">
				<sentence>Instead , the treebank records complete syntactosemantic analyses as defined by the LinGO ERG and provide tools to extract different types of linguistic information at varying granularity .</sentence>
				<definiendum id="0">treebank records</definiendum>
				<definiens id="0">complete syntactosemantic analyses as defined by the LinGO ERG and provide tools to extract different types of linguistic information at varying granularity</definiens>
			</definition>
			<definition id="1">
				<sentence>Most importantly , however , representation ( i ) provides all the information required to replay the full HPSG analysis ( using the original grammar and one of the open-source HPSG processing environments , e.g. , the LKB or PET , which already have been interfaced to [ incr tsdb ( ) ] ) .</sentence>
				<definiendum id="0">representation ( i )</definiendum>
				<definiendum id="1">PET</definiendum>
				<definiens id="0">provides all the information required to replay the full HPSG analysis ( using the original grammar and one of the open-source HPSG processing environments</definiens>
			</definition>
			<definition id="2">
				<sentence>The HPSG derivations that the treebank makes available can be viewed as just such a branching process , and a stochastic model of the trees can be built as a probabilistic context-free grammar ( PCFG ) model .</sentence>
				<definiendum id="0">HPSG derivations</definiendum>
				<definiendum id="1">PCFG</definiendum>
				<definiens id="0">a probabilistic context-free grammar (</definiens>
			</definition>
			<definition id="3">
				<sentence>The PCFG models define probability distributions over the trees of derivational types corresponding to the HPSG analyses of sentences .</sentence>
				<definiendum id="0">PCFG models</definiendum>
				<definiens id="0">define probability distributions over the trees of derivational types corresponding to the HPSG analyses of sentences</definiens>
			</definition>
			<definition id="4">
				<sentence>T// is the probability of the sequence of preterminals t1 tn in T according to a trigram tag model : PTRIG .</sentence>
				<definiendum id="0">T//</definiendum>
				<definiens id="0">the probability of the sequence of preterminals t1 tn in T according to a trigram tag model : PTRIG</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>In these contexts , the use of icons has many advantages : it makes no assumption about the language competences of the users , allowing impaired users , or users from a different linguistic background ( which may not include a good command of one of the major languages involved in research on natural language processing ) , to access the systems ; it may trigger a communication-motivated , implicit learning process , which helps the users to gradually improve their level of literacy in the target language .</sentence>
				<definiendum id="0">process</definiendum>
				<definiens id="0">helps the users to gradually improve their level of literacy in the target language</definiens>
			</definition>
			<definition id="1">
				<sentence>Parsers are systems designed to analyse natural language input , on the base of such clues , and to yield a representation of its informational contents .</sentence>
				<definiendum id="0">Parsers</definiendum>
				<definiens id="0">systems designed to analyse natural language input , on the base of such clues , and to yield a representation of its informational contents</definiens>
			</definition>
			<definition id="2">
				<sentence>The input is a sequence of icons a0a2a1 , a0a4a3 , . . . a0a6a5 , each of which has a set of intrinsic features : a7a9a8a11a10 a0a13a12a15a14a17a16a19a18a20a12 ( where a18 a12 is a set of simple Attribute-Value semantic features , used to represent intrinsic features of the concept—like { &lt; human , +1 &gt; , &lt; male , +1 &gt; } for Daddy ) .</sentence>
				<definiendum id="0">a18 a12</definiendum>
				<definiens id="0">a set of simple Attribute-Value semantic features</definiens>
			</definition>
			<definition id="3">
				<sentence>Every couple a26a29a28 a45 a30a31a18 a12a47a45 a32 present in the case structure means that a18 a12a47a45 is a set of Attribute-Value couples which are attached to a0 a12 as selectional features for the case a28 a45 : a22 a8a48a10 a0 a12 a30a31a28 a45 a14a17a16a19a18 a12a47a45a50a49a52a51 a26a29a28 a45 a30a31a18 a12a47a45 a32a54a53 a21a55a22 a10 a0 a12 a14 For example , we can write : a22 a8 ( write , agent ) a16 { &lt; human , +1 &gt; } The semantic compatibility is the value we seek to maximize to determine the best assignments .</sentence>
				<definiendum id="0">semantic compatibility</definiendum>
				<definiens id="0">a set of Attribute-Value couples which are attached to a0 a12 as selectional features for the case a28 a45 : a22 a8a48a10 a0 a12 a30a31a28 a45 a14a17a16a19a18 a12a47a45a50a49a52a51 a26a29a28 a45 a30a31a18 a12a47a45 a32a54a53 a21a55a22 a10 a0 a12 a14 For example</definiens>
				<definiens id="1">write , agent ) a16 { &lt; human</definiens>
			</definition>
			<definition id="4">
				<sentence>Hence we also introduce a “fading” function , to weight the virtual semantic compatibility of a candidate actor to a predicate , by its actual distance to the predicate in the sequence : a76a61a10 a0a6a12a77a30a31a28a46a45a2a30a43a0a4a78a36a14a9a16a19a79 a10 a0a13a12a77a30a43a0a13a78a75a14a34a37 a21 a10a56a7a9a8a80a10 a0a13a78a75a14a34a30 a22 a8a81a10 a0a13a12a82a30a31a28a46a45a36a14a46a14 ( 3 ) where : a76a61a10 a0 a12 a30a31a28 a45 a30a43a0a4a78a35a14 is the value of the assignment of candidate icon a0a13a78 as filler of the role a28 a45 of predicate a0 a12 ; a79 is the fading function ( decreasing from 1 to 0 when the distance between the two icons goes from 0 to a83 ) ; and a21 a10a56a7a71a8a80a10 a0a4a78a35a14a34a30 a22 a8a81a10 a0a6a12a77a30a31a28a46a45a4a14a46a14 the ( virtual ) semantic compatibility of the intrinsic features of a0a13a78 to the selectional features of a0 a12 for the case a28 a45 , with no consideration of distance ( as defined in Eq .</sentence>
				<definiendum id="0">a79</definiendum>
				<definiens id="0">a “fading” function , to weight the virtual semantic compatibility of a candidate actor to a predicate , by its actual distance to the predicate in the sequence : a76a61a10 a0a6a12a77a30a31a28a46a45a2a30a43a0a4a78a36a14a9a16a19a79 a10 a0a13a12a77a30a43a0a13a78a75a14a34a37 a21 a10a56a7a9a8a80a10 a0a13a78a75a14a34a30 a22 a8a81a10 a0a13a12a82a30a31a28a46a45a36a14a46a14 ( 3 ) where : a76a61a10 a0 a12 a30a31a28 a45 a30a43a0a4a78a35a14 is the value of the assignment of candidate icon a0a13a78 as filler of the role a28 a45 of predicate a0 a12 ;</definiens>
				<definiens id="1">the fading function ( decreasing from 1 to 0 when the distance between the two icons goes from 0 to a83 ) ; and a21 a10a56a7a71a8a80a10 a0a4a78a35a14a34a30 a22 a8a81a10 a0a6a12a77a30a31a28a46a45a4a14a46a14 the ( virtual ) semantic compatibility of the intrinsic features of a0a13a78 to the selectional features of a0 a12 for the case a28 a45 , with no consideration of distance ( as defined in Eq</definiens>
			</definition>
			<definition id="5">
				<sentence>An assignment is an application of the set of icons ( other than the predicate being considered ) into the set of cases of the predicate .</sentence>
				<definiendum id="0">assignment</definiendum>
				<definiens id="0">an application of the set of icons ( other than the predicate being considered</definiens>
			</definition>
			<definition id="6">
				<sentence>The semantic compatibility of this global assignment is defined as the sum of the values ( as defined in Eq .</sentence>
				<definiendum id="0">semantic compatibility</definiendum>
				<definiens id="0">the sum of the values ( as defined in Eq</definiens>
			</definition>
			<definition id="7">
				<sentence>a149 An interpretation is an element of the cartesian product of the sets of all possible assignments for every predicate .</sentence>
				<definiendum id="0">interpretation</definiendum>
				<definiens id="0">an element of the cartesian product of the sets of all possible assignments for every predicate</definiens>
			</definition>
</paper>

		<paper id="1087">
			<definition id="0">
				<sentence>SOM learns to place similar data on topologically close areas on the map .</sentence>
				<definiendum id="0">SOM</definiendum>
				<definiens id="0">learns to place similar data on topologically close areas on the map</definiens>
			</definition>
			<definition id="1">
				<sentence>VSM ( Vector Space Model ) is a basic technique to transform text documents to numeric vectors .</sentence>
				<definiendum id="0">VSM ( Vector Space Model</definiendum>
				<definiens id="0">a basic technique to transform text documents to numeric vectors</definiens>
			</definition>
			<definition id="2">
				<sentence>WordNet ( Miller , 1985 ) is a network of semantic relationships between English words .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="3">
				<sentence>Significance vectors are defined by the frequency of a word in different topics .</sentence>
				<definiendum id="0">Significance vectors</definiendum>
				<definiens id="0">the frequency of a word in different topics</definiens>
			</definition>
			<definition id="4">
				<sentence>Thus a document x is presented with : ∑ ∑ = = = n i m j ji ji j ttopicinwwordforFrequency ttopicinwwordforFrequency twx 1 1 ) , ( where n is the number of words and m is the number of topics .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of words</definiens>
			</definition>
			<definition id="5">
				<sentence>The total number of distinct words in training set with and without the help of WordNet News source without With reduction Headline 10,185 2,766 72.84 % Full-text 22,848 3,851 83.15 % The label shown on a trained SOM is a preference and it is possible that several different labels are assigned to the same SOM unit .</sentence>
				<definiendum id="0">SOM</definiendum>
				<definiens id="0">a preference</definiens>
			</definition>
			<definition id="6">
				<sentence>SOM represents the original distribution of source data so it is important to describe the distribution of data sets ( Table 4 ) .</sentence>
				<definiendum id="0">SOM</definiendum>
			</definition>
			<definition id="7">
				<sentence>WordNet : A Dictionary Browser .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">A Dictionary Browser</definiens>
			</definition>
			<definition id="8">
				<sentence>WordNet : an electronic lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">an electronic lexical database</definiens>
			</definition>
</paper>

		<paper id="1092">
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>Wen-Hsiang Lu Institute of Information Science Academia Sinica ; Dept. of Computer Science and Information Engineering National Chiao Tung University Hsinchu 300 , Taiwan , ROC whlu @ iis.sinica.edu.tw Lee-Feng Chien Institute of Information Science , Academia Sinica Nangang 115 , Taiwan , ROC lfchien @ iis.sinica.edu.tw Hsi-Jian Lee Dept. of Computer Science and Information Engineering National Chiao Tung University Hsinchu 300 , Taiwan , ROC hjlee @ csie.nctu.edu.tw One of the existing difficulties of cross-language information retrieval ( CLIR ) and Web search is the lack of appropriate translations of new terminology and proper names .</sentence>
				<definiendum id="0">cross-language information retrieval</definiendum>
				<definiendum id="1">CLIR</definiendum>
				<definiens id="0">Wen-Hsiang Lu Institute of Information Science Academia Sinica ; Dept. of Computer Science and Information Engineering National Chiao Tung University Hsinchu 300 , Taiwan , ROC whlu @ iis.sinica.edu.tw Lee-Feng Chien Institute of Information Science , Academia Sinica Nangang 115 , Taiwan , ROC lfchien @ iis.sinica.edu.tw Hsi-Jian Lee Dept. of Computer Science and Information Engineering National Chiao Tung University Hsinchu 300</definiens>
				<definiens id="1">the lack of appropriate translations of new terminology and proper names</definiens>
			</definition>
			<definition id="1">
				<sentence>Cross-language information retrieval ( CLIR ) , addressing the special need where users can query in one language and retrieve relevant documents written or indexed in another language , has become an important issue in the research of information retrieval ( Dumais et al. , 1996 ; Davis et al. , 1997 ; Ballesteros &amp; Croft , 1998 ; Nie et al. , 1999 ) .</sentence>
				<definiendum id="0">Cross-language information retrieval</definiendum>
				<definiendum id="1">CLIR</definiendum>
				<definiens id="0">addressing the special need where users can query in one language and retrieve relevant documents written or indexed in another language , has become an important issue in the research of information retrieval</definiens>
			</definition>
			<definition id="2">
				<sentence>For a Web page ( or URL ) u i , its anchor-text set is defined as all of the anchor texts of the links , i.e. , u i 's in-links , pointing to u i .</sentence>
				<definiendum id="0">Web page</definiendum>
				<definiendum id="1">URL</definiendum>
			</definition>
			<definition id="3">
				<sentence>By considering the link structures and concept space of Web pages , P ( u i ) is estimated with the probability of u i being linked , and its estimation is defined as follows : P ( u i ) = L ( u i ) /Σ j=1 , n L ( u j ) , where L ( u j ) = the number of in-links of page u j .</sentence>
				<definiendum id="0">P ( u i )</definiendum>
				<definiendum id="1">L</definiendum>
				<definiens id="0">the number of in-links of page u j</definiens>
			</definition>
			<definition id="4">
				<sentence>( log ) ( log ) ] ( ) ( log [ ) , ( log ) , ( tmPmsP tmPmsP tmmsPtsPindirect ↔+↔= ↔×↔≈ ↔↔= , where m is the transitive translation of s and t in the intermediate language , P ( s↔ m ) and P ( m↔ t ) are the probability values obtained with the direct translation model which can be calculated by Equation ( 2 ) .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">the transitive translation of s and t in the intermediate language</definiens>
				<definiens id="1">the probability values obtained with the direct translation model which can be calculated by Equation ( 2 )</definiens>
			</definition>
			<definition id="5">
				<sentence>For a set of test query terms , its top-n inclusion rate is defined as the percentage of the query terms whose effective translation ( s ) can be found in the top n extracted translations .</sentence>
				<definiendum id="0">top-n inclusion rate</definiendum>
				<definiens id="0">the percentage of the query terms whose effective translation ( s ) can be found in the top n extracted translations</definiens>
			</definition>
</paper>

		<paper id="2010">
			<definition id="0">
				<sentence>Like the SGC the NGC has three main parts in addition to an initial tokenizer ( spell checking is performed at a previous stage ) : A morphological analyser ( NOBTWOL ) , which provides each word form with all of its lexically possible readings ( grammatical tags ) .</sentence>
				<definiendum id="0">morphological analyser</definiendum>
				<definiendum id="1">NOBTWOL</definiendum>
				<definiens id="0">provides each word form with all of its lexically possible readings ( grammatical tags )</definiens>
			</definition>
</paper>

		<paper id="1079">
			<definition id="0">
				<sentence>Ambiguity is the fundamental property of natural language .</sentence>
				<definiendum id="0">Ambiguity</definiendum>
				<definiens id="0">the fundamental property of natural language</definiens>
			</definition>
			<definition id="1">
				<sentence>Ambiguity on all levels of representation is an inherent property of natural languages and it also forms a central problem of natural language parsing .</sentence>
				<definiendum id="0">Ambiguity on all levels of representation</definiendum>
			</definition>
</paper>

		<paper id="1148">
			<definition id="0">
				<sentence>PPM learns an n-gram language model by supervised training on a given set of hand segmented Chinese text .</sentence>
				<definiendum id="0">PPM</definiendum>
				<definiens id="0">learns an n-gram language model by supervised training on a given set of hand segmented Chinese text</definiens>
			</definition>
			<definition id="1">
				<sentence>To segment a new sentence , PPM seeks the segmentation which gives the best compression using the learned model .</sentence>
				<definiendum id="0">PPM</definiendum>
				<definiens id="0">seeks the segmentation which gives the best compression using the learned model</definiens>
			</definition>
			<definition id="2">
				<sentence>We considered a refined term weighting scheme based on the the standard term weighting function w0 = logN ¡n+0:5n+0:5 ( 1 ) where N is the number of indexed documents in the collection , and n is the number of documents containing a specific term ( Spark Jones , 1979 ) .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">n</definiendum>
			</definition>
			<definition id="3">
				<sentence>For example , one standard augmentation is to use w1 = w0 ⁄ ( c1 +1 ) ⁄tfK +tf ⁄ ( c2 +1 ) ⁄qtfc 2 +qtf ( 2 ) where K = c1 ⁄ 1¡c3 +c3 dlavdl ¶ Here tf is within-document term frequency , qtf is within-query term frequency , dl is the length of the document , avdl is the average document length , and c1 , c2 , c3 are tuning constants that depend on the database , the nature of the queries , and are empirically determined .</sentence>
				<definiendum id="0">qtf</definiendum>
				<definiendum id="1">dl</definiendum>
			</definition>
			<definition id="4">
				<sentence>Here ' indicates that the component is added only once per document , rather than for each term , and y = 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; : ln ( dlavdl ) +ln ( c4 ) if dl • rel avdl ¡ln ( rel avdl avdl ) +ln ( c4 ) ¢‡1¡ dl¡rel avdl c5⁄avdl¡rel avdl · if dl &gt; rel avdl where rel avdl is the average relevant document length calculated from previous queries based on the same collection of documents .</sentence>
				<definiendum id="0">rel avdl</definiendum>
				<definiens id="0">the average relevant document length calculated from previous queries based on the same collection of documents</definiens>
			</definition>
			<definition id="5">
				<sentence>Then the precision , recall and F measures are defined precision : p = N3N2 recall : r = N3N1 F-measure : F = 2£p£rp+r In this paper , we only report the performance in F-measure , which is a comprehensive measure that combines precision and the recall .</sentence>
				<definiendum id="0">F-measure</definiendum>
				<definiens id="0">a comprehensive measure that combines precision and the recall</definiens>
			</definition>
</paper>

		<paper id="2026">
			<definition id="0">
				<sentence>Parsli is a finite-state ( FS ) parser which can be tailored to the lexicon , syntax , and semantics of a particular application using a hand-editable declarative lexicon .</sentence>
				<definiendum id="0">Parsli</definiendum>
				<definiens id="0">a finite-state ( FS ) parser which can be tailored to the lexicon , syntax , and semantics of a particular application using a hand-editable declarative lexicon</definiens>
			</definition>
			<definition id="1">
				<sentence>WORDSEYE ( Coyne and Sproat , 2001 ) is a system for converting English text into threedimensional graphical scenes that represent that text .</sentence>
				<definiendum id="0">WORDSEYE</definiendum>
				<definiens id="0">a system for converting English text into threedimensional graphical scenes that represent that text</definiens>
			</definition>
			<definition id="2">
				<sentence>WORDSEYE performs syntactic and semantic analysis on the input text , producing a description of the arrangement of objects in a scene .</sentence>
				<definiendum id="0">WORDSEYE</definiendum>
				<definiens id="0">performs syntactic and semantic analysis on the input text , producing a description of the arrangement of objects in a scene</definiens>
			</definition>
			<definition id="3">
				<sentence>Tree Adjoining Grammar ( TAG ) represents the entire syntactic projection from a lexeme in its elementary structures in an elementary tree ; because of this , each elementary tree can be associated with a lexical item ( lexicalization , ( Joshi and Schabes , 1991 ) ) .</sentence>
				<definiendum id="0">Tree Adjoining Grammar ( TAG )</definiendum>
				<definiens id="0">the entire syntactic projection from a lexeme in its elementary structures in an elementary tree</definiens>
			</definition>
			<definition id="4">
				<sentence>A TAG consists of a set of elementary trees of two types , initial trees and auxiliary trees .</sentence>
				<definiendum id="0">TAG</definiendum>
			</definition>
			<definition id="5">
				<sentence>Adjunct auxiliary trees are used for adjuncts ; they have the property that the footnode is alVerb Supertag Verb semantics Argument semantics paid A nx0Vnx1 transaction 0=Customer 1=Amount cost A nx0Vnx1 transaction 0=Item 1=Amount Implicit=Customer cost A nx0Vnx2nx1 transaction 0=Item 1=Amount 2=Customer bought , purchased A nx0Vnx1 transaction 0=Customer 1=Item socks A NXN none none Figure 1 : Sample entries for a commercial transaction situation In : I bought socks Out : ( ( I ) GF=0 AS=CUSTOMER TRANSACTION ( socks ) GF=1 AS=ITEM ) In : the pajamas cost my mother-in-law 12 dollars Out : ( ( ( the ) pajamas ) GF=0 AS=ITEM TRANSACTION ( ( my ) mother-in-law ) GF=2 AS=CUSTOMER ( ( 12 ) dollars ) GF=1 AS=AMOUNT ) In : the pajamas cost 12 dollars Out : ( ( ( the ) pajamas ) GF=0 AS=ITEM TRANSACTION IMP : CUSTOMER ( ( 12 ) dollars ) GF=1 AS=AMOUNT ) Figure 2 : Sample transductions generated by Parsli ( “GF” for grammatical function , “AS” for argument semantics , “Imp” for implicit argument ) ways a daughter node of the root node , and the label on these nodes is not , linguistically speaking , part of the projection of the lexical item of that tree .</sentence>
				<definiendum id="0">Adjunct auxiliary trees</definiendum>
				<definiendum id="1">nx0Vnx1 transaction 0=Customer 1=Item</definiendum>
				<definiendum id="2">AS=ITEM TRANSACTION ( ( my ) mother-in-law ) GF=2 AS=CUSTOMER</definiendum>
				<definiens id="0">ways a daughter node of the root node</definiens>
			</definition>
			<definition id="6">
				<sentence>To determine the mapping from word to supertag , we use the lexical probability a2a4a3a6a5a8a7a9a11a10 where a9 is the word and a12 the class .</sentence>
				<definiendum id="0">a9</definiendum>
				<definiens id="0">the word and a12 the class</definiens>
			</definition>
			<definition id="7">
				<sentence>We evaluate performance using accuracy , the ration of n Correctness Accuracy Nb 2 1.00 1.00 12 4 0.83 0.84 30 6 0.70 0.82 121 8 0.62 0.80 178 12 0.59 0.79 202 16 0.58 0.79 204 20 0.58 0.78 205 Figure 5 : Results for sentences with a17 or fewer words ; Nb refers to the number of sentences in this category n Correctness Accuracy 1 0.58 0.78 2 0.60 0.79 4 0.62 0.81 8 0.69 0.85 12 0.68 0.86 20 0.70 0.87 30 0.73 0.89 Figure 6 : Results for a17 -best analyses the number of dependency arcs which are correctly found ( same head and daughter nodes ) in the best parse for each sentence to the number of arcs in the entire test corpus .</sentence>
				<definiendum id="0">Nb</definiendum>
				<definiens id="0">Results for sentences with a17 or fewer words</definiens>
				<definiens id="1">Results for a17 -best analyses the number of dependency arcs which are correctly found ( same head and daughter nodes ) in the best parse for each sentence to the number of arcs in the entire test corpus</definiens>
			</definition>
			<definition id="8">
				<sentence>PARSLI uses Tree Adjoining Grammar as an interface between syntax and lexical semantics .</sentence>
				<definiendum id="0">PARSLI</definiendum>
				<definiens id="0">uses Tree Adjoining Grammar as an interface between syntax and lexical semantics</definiens>
			</definition>
			<definition id="9">
				<sentence>WordsEye : An automatic text-to-scene conversion system .</sentence>
				<definiendum id="0">WordsEye</definiendum>
				<definiens id="0">An automatic text-to-scene conversion system</definiens>
			</definition>
</paper>

		<paper id="1124">
</paper>

		<paper id="1121">
</paper>

		<paper id="1135">
</paper>

		<paper id="1113">
			<definition id="0">
				<sentence>DL is a fragment of first-order logic which only allows unary and binary predicates ( concepts and roles ) and only very restricted quantification .</sentence>
				<definiendum id="0">DL</definiendum>
				<definiens id="0">a fragment of first-order logic which only allows unary and binary predicates ( concepts and roles</definiens>
			</definition>
			<definition id="1">
				<sentence>A knowledge base consists of a T-Box , which contains axioms relating the concepts and roles , and one or more A-Boxes , which state that individuals belong to certain concepts , or are related by certain roles .</sentence>
				<definiendum id="0">knowledge base</definiendum>
				<definiens id="0">contains axioms relating the concepts and roles , and one or more A-Boxes , which state that individuals belong to certain concepts , or are related by certain roles</definiens>
			</definition>
			<definition id="2">
				<sentence>The DM is a data structure that stores an ordered list of the most salient discourse entities according to their “information status” and text position and provides methods for retrieving and inserting elements .</sentence>
				<definiendum id="0">DM</definiendum>
				<definiens id="0">a data structure that stores an ordered list of the most salient discourse entities according to their “information status” and text position and provides methods for retrieving and inserting elements</definiens>
			</definition>
</paper>

		<paper id="1147">
</paper>

		<paper id="1080">
			<definition id="0">
				<sentence>An Agent-based Approach to Chinese Named Entity Recognition Shiren Ye Tat-Seng Chua Liu Jimin School of Computing , National University of Singapore , Singapore , 117543 yesr @ comp.nus.edu.sg chuats @ comp.nus.edu.sg Liujm @ comp.nus.edu.sg Chinese NE ( Named Entity ) recognition is a difficult problem because of the uncertainty in word segmentation and flexibility in language structure .</sentence>
				<definiendum id="0">recognition</definiendum>
				<definiens id="0">a difficult problem because of the uncertainty in word segmentation and flexibility in language structure</definiens>
			</definition>
			<definition id="1">
				<sentence>Named entity ( NE ) recognition is a fundamental step to many language processing tasks .</sentence>
				<definiendum id="0">recognition</definiendum>
				<definiens id="0">Named entity ( NE )</definiens>
			</definition>
			<definition id="2">
				<sentence>The expected value of a token sequence ab representing a PER when a is red and b is blue is : |||||| |||| BRB RB R bab ab a NN ⋅ =⋅= ( 1 ) The expected value of the cases when the token pair ab is not a PER name is the sum of expected values of four cases : a B b R , a B b W , a W b R , a W b W ( see Table 2 ) , which after simplification , is given by : ||||| || || | |||||||||||||||| ( | | | | ) ( | | | | ) ( 2 ) BR BW W R WW RB B WWRWRBR BW RW ab ab ab a b a b ab ab abab NM NM NM NM aa bb NM =+ + + ⋅⋅⋅⋅ =+++ ++++ +⋅+ = + The ratio between the cases when ab is a PER versus when ab is not a PER is : || || CRBRB ab a b RB ab ab λℜ= =⋅ℜ⋅ℜ ( 3 ) where || || ; ( | | | | ) ( | | | | ) RB ab BW RW ab aa bb ℜ= ℜ= ++ ; and NM N λ + = .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">the sum of expected values of four cases : a B b</definiens>
				<definiens id="1">BW W R WW RB B WWRWRBR BW RW ab ab ab a b a b ab ab abab NM NM NM NM aa bb NM =+ + + ⋅⋅⋅⋅ =+++ ++++ +⋅+ = + The ratio between the cases when ab is a PER versus when ab is not a PER is : || || CRBRB ab a b RB ab ab λℜ= =⋅ℜ⋅ℜ ( 3 ) where || || ; ( | | | | ) ( | | | | ) RB ab BW RW ab aa bb ℜ= ℜ= ++ ;</definiens>
			</definition>
			<definition id="3">
				<sentence>The PKU is a manually tagged corpus containing one-month of news report from China’s People Daily .</sentence>
				<definiendum id="0">PKU</definiendum>
			</definition>
			<definition id="4">
				<sentence>Type N C N P N W N M N S Rc Pr F 1 Baseline test ( a ) ORG PER LOC 79 3 0 295 0 * * * * * 363 84 0 303 26 21 98 35.0 * * * 54 86 66.0 Config ( b ) ORG PER LOC 309 5 28 35 47 154 2 7 11 87 618 0 29 103 112 83 79 81.0 89 62 73.4 82 81 81.7 Config ( c ) ORG PER LOC 356 2 5 14 21 167 1 2 4 9 703 0 18 29 52 95 93 93.7 96 93 94.7 94 91 92.3 Results of Chen et ( 98 ) ORG PER LOC 393 0 7 77 44 159 0 0 25 56 583 0 65 102 194 78 83 81.3 91 74 81.6 78 69 73.2 Results of Yu et al. ( 98 ) ORG PER LOC 331 0 14 32 25 160 0 7 7 74 682 0 1 67 83 88 89 88.5 92 66 76.7 91 89 0.0 where Pr = ( N C + 0.5*N P ) / ( N C + N W + N P + N S ) ; Rc = ( N C + 0.5*N P ) / ( N C + N W + N P + N M ) ; F 1 = 2*Pr*Rc/ ( Pr+Rc ) .</sentence>
				<definiendum id="0">Type N C N P N W N M N S Rc Pr F 1 Baseline test</definiendum>
				<definiens id="0">a ) ORG PER LOC 79 3 0 295 0 * * * * * 363 84 0 303 26 21 98 35.0 * *</definiens>
			</definition>
			<definition id="5">
				<sentence>and N C gives the number of NEs correctly recognized ; N P denotes the number of NEs partially recognized ; N W gives the number of NEs incorrectly recognized ; N M denotes the number of NEs missed ; and finally N S gives the number of NEs found by the system but not in the tagged list .</sentence>
				<definiendum id="0">N C</definiendum>
				<definiendum id="1">N P</definiendum>
				<definiendum id="2">N M</definiendum>
				<definiens id="0">gives the number of NEs correctly recognized ;</definiens>
				<definiens id="1">the number of NEs partially recognized ; N W gives the number of NEs incorrectly recognized</definiens>
				<definiens id="2">the number of NEs missed ; and finally N S gives the number of NEs found by the system but not in the tagged list</definiens>
			</definition>
</paper>

		<paper id="1134">
</paper>

		<paper id="2023">
			<definition id="0">
				<sentence>Weblas imitates this process by prompting the test creator for the scoring rubrid .</sentence>
				<definiendum id="0">Weblas</definiendum>
				<definiens id="0">imitates this process by prompting the test creator for the scoring rubrid</definiens>
			</definition>
			<definition id="1">
				<sentence>Task development consists of all the efforts that lead to the test administration .</sentence>
				<definiendum id="0">Task development</definiendum>
				<definiens id="0">consists of all the efforts that lead to the test administration</definiens>
			</definition>
			<definition id="2">
				<sentence>The task development portion of WebLAS consists of three modulestask creation , task modification , and lexicon modification .</sentence>
				<definiendum id="0">WebLAS</definiendum>
				<definiens id="0">consists of three modulestask creation , task modification , and lexicon modification</definiens>
			</definition>
			<definition id="3">
				<sentence>Linguana : :LinkGrammar interfaces with the Link Grammar for parts of speech ( POS ) tagging and syntactic parsing .</sentence>
				<definiendum id="0">Linguana</definiendum>
				<definiens id="0">:LinkGrammar interfaces with the Link Grammar for parts of speech ( POS ) tagging and syntactic parsing</definiens>
			</definition>
			<definition id="4">
				<sentence>e. WebLAS takes the input ( model answer , elements , alternatives , score assignments ) to create a scoring key .</sentence>
				<definiendum id="0">e. WebLAS</definiendum>
				<definiens id="0">takes the input ( model answer , elements , alternatives , score assignments ) to create a scoring key</definiens>
			</definition>
			<definition id="5">
				<sentence>Chung , Gregory K.W.K ; O’Neil , Harold F. , Jr. ( 1997 ) Methodological approaches to online scoring of essays .</sentence>
				<definiendum id="0">Methodological</definiendum>
				<definiens id="0">approaches to online scoring of essays</definiens>
			</definition>
			<definition id="6">
				<sentence>( 1998 ) Wordnet : An electronic lexical database .</sentence>
				<definiendum id="0">Wordnet</definiendum>
				<definiens id="0">An electronic lexical database</definiens>
			</definition>
</paper>

		<paper id="1140">
			<definition id="0">
				<sentence>The Japanese writing system consists of the three orthographies of hiragana , katakana and kanji , which appear intermingled in modern-day texts ( NLI , 1986 ) .</sentence>
				<definiendum id="0">Japanese writing system</definiendum>
				<definiens id="0">consists of the three orthographies of hiragana , katakana and kanji</definiens>
			</definition>
			<definition id="1">
				<sentence>The hiragana and katakana syllabaries , collectively referred to as kana , are relatively small ( 46 characters each ) , and each character takes a unique and mutually exclusive reading which can easily be memorized .</sentence>
				<definiendum id="0">character</definiendum>
				<definiens id="0">takes a unique and mutually exclusive reading which can easily be memorized</definiens>
			</definition>
			<definition id="2">
				<sentence>P ( r|s ) =P ( r |k ) ( 1 ) P ( r |k ) = n productdisplay i=1 P ( r i |k i ) × ×P phon ( r i ) ×P conj ( r i ) ( 2 ) each dictionary entry s in the corpus and then the string probability P ( s ) , according to equation ( 3 ) .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">×P phon</definiendum>
				<definiens id="0">2 ) each dictionary entry s in the corpus and then the string probability P ( s ) , according to equation ( 3 )</definiens>
			</definition>
			<definition id="3">
				<sentence>In both ﬁgures , Baseline is calculated over only those readings in the original dictionary ( i.e. correct readings ) ; Existing is the subset of readings in the generated set that existed in the original dictionary ; and All is all readings in the generated set .</sentence>
				<definiendum id="0">Baseline</definiendum>
				<definiendum id="1">Existing</definiendum>
				<definiens id="0">calculated over only those readings in the original dictionary ( i.e. correct readings ) ;</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>LINGUA uses knowledge poor , heuristically based algorithms for language analysis , in this way getting round the lack of resources for Bulgarian .</sentence>
				<definiendum id="0">LINGUA</definiendum>
				<definiens id="0">uses knowledge poor , heuristically based algorithms for language analysis , in this way getting round the lack of resources for Bulgarian</definiens>
			</definition>
			<definition id="1">
				<sentence>LINGUA performs the pre-processing , needed as an input to the anaphora resolution algorithm : sentence , paragraph and clause splitters , NP grammar , part-of-speech tagger , 2MARS stands for Mitkov’s Anaphora Resolution System .</sentence>
				<definiendum id="0">LINGUA</definiendum>
				<definiens id="0">performs the pre-processing , needed as an input to the anaphora resolution algorithm : sentence , paragraph and clause splitters</definiens>
			</definition>
			<definition id="2">
				<sentence>Lexical reiteration : A score of +2 is assigned those NPs repeated twice or more in the paragraph in which the pronoun appears , a score of +1 is assigned to those NP , repeated once in the paragraph .</sentence>
				<definiendum id="0">Lexical reiteration</definiendum>
				<definiens id="0">A score of +2 is assigned those NPs repeated twice or more in the</definiens>
			</definition>
			<definition id="3">
				<sentence>Sequential instructions : A score of +2 is applied to NPs in the NP1 position of constructions of the form : “To V1 NP1 ... To V2 it ... ” Term preference : a score of +1 is applied to those NPs identified as representing domain terms .</sentence>
				<definiendum id="0">Sequential instructions</definiendum>
				<definiens id="0">a score of +1 is applied to those NPs identified as representing domain terms</definiens>
			</definition>
			<definition id="4">
				<sentence>Referential distance gives scores of +2 and +1 for the NPs in the same and in the previous sentence respectively , and -1 for the NPs two sentences back .</sentence>
				<definiendum id="0">Referential distance</definiendum>
				<definiens id="0">gives scores of +2 and +1 for the NPs in the same and in the previous sentence respectively , and -1 for the NPs two sentences back</definiens>
			</definition>
			<definition id="5">
				<sentence>Indefiniteness assigns a score of -1 to indefinite NPs , 0 to the definite ( not full article ) and +1 to these which are definite , containing the definite ’full’ article in Bulgarian .</sentence>
				<definiendum id="0">Indefiniteness</definiendum>
				<definiens id="0">assigns a score of -1 to indefinite NPs , 0 to the definite ( not full article ) and +1 to these which are definite , containing the definite ’full’ article in Bulgarian</definiens>
			</definition>
			<definition id="6">
				<sentence>Success rate is the ratio SR = AC=A , where AC is the number of correctly resolved and A is the number of all anaphors .</sentence>
				<definiendum id="0">Success rate</definiendum>
				<definiendum id="1">AC</definiendum>
				<definiens id="0">the number of correctly resolved and A is the number of all anaphors</definiens>
			</definition>
			<definition id="7">
				<sentence>Critical success rate is the success rate for the anaphors which have more than one candidates for antecedent after the gender and number agreement filter is applied .</sentence>
				<definiendum id="0">Critical success rate</definiendum>
				<definiens id="0">the success rate for the anaphors which have more than one candidates for antecedent after the gender</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>In an attempt to further progress in information retrieval research , the Text REtrieval Conference ( TREC ) sponsored by the National Institute of Standards and Technology ( NIST ) started a series of large-scale evaluations of domain independent automated question answering systems in TREC-8 ( Voorhees 2000 ) and continued in TREC-9 and TREC-10 .</sentence>
				<definiendum id="0">Text REtrieval Conference</definiendum>
				<definiens id="0">TREC ) sponsored by the National Institute of Standards and Technology ( NIST ) started a series of large-scale evaluations of domain independent automated question answering systems in TREC-8 ( Voorhees 2000 ) and continued in TREC-9 and TREC-10</definiens>
			</definition>
			<definition id="1">
				<sentence>Research systems participating in TRECs and the coming QAC focused on the problem of answering closed-class questions that have short fact-based answers ( “factoids” ) from a large collection of text .</sentence>
				<definiendum id="0">Research systems</definiendum>
			</definition>
			<definition id="2">
				<sentence>The TREC-10 QA document collection consists of newspaper and newswire articles on TREC disks 1 to 5 .</sentence>
				<definiendum id="0">TREC-10 QA document collection</definiendum>
			</definition>
			<definition id="3">
				<sentence>Mean reciprocal rank ( MRR ) was used as the indicator of system performance .</sentence>
				<definiendum id="0">Mean reciprocal rank</definiendum>
				<definiendum id="1">MRR</definiendum>
				<definiens id="0">the indicator of system performance</definiens>
			</definition>
			<definition id="4">
				<sentence>( 3 ) Candidate Answer Extraction : We again used CONTEX to parse each of the top N sentences , marked candidate answers by named entities and special answer patterns such as definition patterns , and then started the ranking process .</sentence>
				<definiendum id="0">Candidate Answer Extraction</definiendum>
				<definiens id="0">We again used CONTEX to parse each of the top N sentences , marked candidate answers by named entities and special answer patterns such as definition patterns</definiens>
			</definition>
			<definition id="5">
				<sentence>, the following answers are found in the TREC-10 corpus using the patterns described by the InsightSoft-M system : ① autism Q , a nourishing A , equivocal … ② autism Q , the disorder is A , in fact , … ③ autism Q , the discovery could open new approaches for treating t A he … ④ autism Q is a mental disorder that is a “severely incapacitatin A g … ⑤ autism Q , the inability to communicate with others A .</sentence>
				<definiendum id="0">disorder</definiendum>
				<definiens id="0">a mental disorder that is a “severely incapacitatin A g … ⑤ autism Q , the inability to communicate with others A</definiens>
			</definition>
			<definition id="6">
				<sentence>Closed class words are thrown away and each word w i in the glosses is assigned a gloss weight wn i s as follows 3 : ) 1/log ( += i wn i nNs where n i is the number of times word w i occurring in the WordNet noun glosses and N is total number of occurrences of all noun gloss words in the WordNet .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">total number of occurrences of all noun gloss words in the WordNet</definiens>
			</definition>
			<definition id="7">
				<sentence>The World Wide Web contains massive amounts of information covering almost any thinkable topic .</sentence>
				<definiendum id="0">World Wide Web</definiendum>
				<definiens id="0">contains massive amounts of information covering almost any thinkable topic</definiens>
			</definition>
			<definition id="8">
				<sentence>( 4 ) The gloss weight web i s for each word c i w is computed as follows 5 : ) 1/log ( +•= ii web i nNts where t i is the frequency of c i w in the set of context words extracted in ( 3 ) , N is the total number of training questions , and n i is the number of training questions in which c i w occurs .</sentence>
				<definiendum id="0">t i</definiendum>
				<definiendum id="1">N</definiendum>
			</definition>
			<definition id="9">
				<sentence>To investigate the effectiveness of using dictionary and web-based answer reranking on question of different difficulty , we define question difficulty as : ) / ( 1 Nnd −= , where n is the number of systems participating in TREC-10 that returned answers in top 5 and N is the number of total runs ( that is , 67 for TREC-10 ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">the number of systems participating in TREC-10 that returned answers in top 5</definiens>
				<definiens id="1">the number of total runs ( that is , 67 for TREC-10 )</definiens>
			</definition>
			<definition id="10">
				<sentence>WordNet : An Electronic Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="11">
				<sentence>FALCON : Boosting Knowledge for Answer Engines .</sentence>
				<definiendum id="0">FALCON</definiendum>
			</definition>
</paper>

		<paper id="1076">
</paper>

		<paper id="1085">
			<definition id="0">
				<sentence>Margin w Positive examples Negative examples Figure 2 : The decision surface of a linear SVM Solidline denotes a decision surface , and twodashed lines refer to the boundaries .</sentence>
				<definiendum id="0">SVM Solidline</definiendum>
				<definiens id="0">a decision surface</definiens>
			</definition>
			<definition id="1">
				<sentence>We ﬁrst represent each document as a vector in an n dimensional space , where n is the number of words in the collection .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of words in the collection</definiens>
			</definition>
			<definition id="2">
				<sentence>We compute arelevance score foreach negativetrainingdocument by the cosine of the angle between a vector of the center of gravity on positive training documents and a vector of the negative training document , i.e. , cos ( vectorg , vectory j ) ( 1≤ j ≤ q ) , where vectory j is the j-th negative training document , and vectorg is deﬁned as follows : vectorg = ( g 1 , ··· , g n ) = ( 1 p p summationdisplay i=1 x i1 , ··· , 1 p p summationdisplay i=1 x in ) ( 5 ) x ij ( 1 ≤ j ≤ n ) is the term frequency of word j in the positive document vectorx i .</sentence>
				<definiendum id="0">vectory j</definiendum>
				<definiens id="0">the term frequency of word j in the positive document vectorx i</definiens>
			</definition>
			<definition id="3">
				<sentence>The classiﬁer event ( x p ) is induced by trainingdata which consists oftwodifferent sets : one is a set of all documents including x \1 , and concerning the target event .</sentence>
				<definiendum id="0">classiﬁer event</definiendum>
				<definiens id="0">a set of all documents including x \1 , and concerning the target event</definiens>
			</definition>
			<definition id="4">
				<sentence>sbj ( x p ) =1 &amp; sbj class ( x p ) =1 &amp; event ( x p ) =1 ( 6 ) We used the TDT1 corpus which comprises a set of diﬀerent sources , Reuters ( 7,965 documents ) and CNN ( 7,898 documents ) ( Allan et al. , 1998a ) .</sentence>
				<definiendum id="0">sbj ( x p</definiendum>
				<definiendum id="1">sbj class</definiendum>
				<definiendum id="2">TDT1 corpus</definiendum>
				<definiendum id="3">CNN</definiendum>
				<definiens id="0">comprises a set of diﬀerent sources</definiens>
			</definition>
			<definition id="5">
				<sentence>‘Prec’ stands for precision , which is the ratio of correct assignments by the system divided by the total number of the system’s assignments .</sentence>
				<definiendum id="0">‘Prec’</definiendum>
				<definiendum id="1">precision</definiendum>
				<definiens id="0">the ratio of correct assignments by the system divided by the total number of the system’s assignments</definiens>
			</definition>
			<definition id="6">
				<sentence>‘F1’ is a measure that balances recall and precision , where recall denotes the ratio of correct assignments by the system divided by the total number of correct assignments .</sentence>
				<definiendum id="0">‘F1’</definiendum>
				<definiendum id="1">recall</definiendum>
				<definiens id="0">a measure that balances recall and precision</definiens>
			</definition>
			<definition id="7">
				<sentence>Recall denotes the number of documents selected by both the system and human judges divided by the total number of documents selected by human judges , and precision shows the number of documents selected by both the system and human judges divided by the total number of documents selected by the system .</sentence>
				<definiendum id="0">Recall</definiendum>
			</definition>
			<definition id="8">
				<sentence>‘CNN’ refers to the results using the CNN corpus as both training and test data .</sentence>
				<definiendum id="0">CNN</definiendum>
			</definition>
			<definition id="9">
				<sentence>Precisely , the documents judged as Yes but were not evaluated as Yes Table 3 : Data Event CNN Reuters doc para doc para 3 ( Carter in Bosnia ) 26 314 8 37 5 ( Clinic Murders ( Salvi ) ) 36 416 5 34 6 ( Comet into Jupiter ) 41 539 4 23 8 ( Death of Kim Jong Il ) 28 337 39 353 9 ( DNA in OJ trial ) 108 1,407 6 75 11 ( Hall’s copter ( N. Korea ) ) 77 875 22 170 12 ( Humble , TX , ﬂooding ) 22 243 0 0 15 ( Kobe Japan quake ) 72 782 12 64 16 ( Lost in Iraq ) 34 395 10 78 17 ( NYC Subway bombing ) 22 374 2 2 18 ( OK-City bombing ) 214 3,209 59 439 21 ( Serbians down F-16 ) 50 572 15 135 22 ( Serbs violate Bihac ) 56 669 35 349 24 ( USAir 427 crash ) 32 435 7 98 25 ( WTC Bombing trial ) 18 132 4 54 Avg .</sentence>
				<definiendum id="0">OK-City bombing</definiendum>
			</definition>
			<definition id="10">
				<sentence>One reason behind this lies in the diﬀerence between the two corpora : CNN consists of a larger number of words per paragraph than Reuters .</sentence>
				<definiendum id="0">CNN</definiendum>
				<definiens id="0">consists of a larger number of words per paragraph than Reuters</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>Calculate TFIDF for each word Calculate weight of each word with TF , IDF and IGR Hierarchical Clustering by Maximum Distance Algorithm INPUT : Set of Retrieved Documents OUTPUT : Set of Summaries Document Database ( Whole set of Documents ) Calculate TF and DF for each word Tokenize and Extract Nouns Tokenize and Extract Nouns TF and DF database for each word in every document Document Vectors of Retrieved Documents Calculate Information Gain Ratio for each word Cluster Structure of Document Set Computed in advance BYCXCVD9D6CTBDBM C7DACTD6DACXCTDBD3CUD3D9D6CBCRCWCTD1CT CCCWD6D3D9CVCW CBD8CTD4 BDB8 CXD8 CXD7 CTDCD4CTCRD8CTCS D8CWCPD8 CSD3CRD9B9 D1CTD2D8D7 D6CTD0CTDACPD2D8 D8D3 D5D9CTD6DD CPD2CS CXD6D6CTD0CTDACPD2D8 CSD3CRD9B9 D1CTD2D8D7 CPD6CT D7CTD4CPD6CPD8CTD0DD D3D6CVCPD2CXDECTCS CXD2D8D3 CSCXABCTD6CTD2D8 CRD0D9D7D8CTD6D7BA C6D3D8CT D8CWCPD8 DBCT CWCPDACT D8D3 D8CPCZCT CPCRCRD3D9D2D8 D3CU D8CWCTCSD3CRD9D1CTD2D8D7DBCWCXCRCWCPD6CTD2D3D8D6CTD8D6CXCTDACTCSCQD9D8CTDCCXD7D8 CXD2D8CWCTCSD3CRD9D1CTD2D8CSCPD8CPCQCPD7CTCXD2D3D6CSCTD6D8D3D3CQD8CPCXD2D8CWCT CXD2CUD3D6D1CPD8CXD3D2DBCWCPD8DBD3D6CSD7D6CTCPD0D0DDCRD3D2D8D6CXCQD9D8CTD8D3D7CTB9 D0CTCRD8CXD2CV D8CWCT D6CTD8D6CXCTDACTCS CSD3CRD9D1CTD2D8D7BA CCCWCTD6CTCUD3D6CTB8 CPD7 D7CWD3DBD2 CXD2 BYCXCVD9D6CT BE DBCT CXD2D8D6D3CSD9CRCT CPD2D3D8CWCTD6 D0CPDDCTD6 D3CU CRD0D9D7D8CTD6B8 DBCWCXCRCW CRD3D6D6CTD7D4D3D2CSD7 D8D3 D8CWCT D7CTD8 D3CU D8CWCT DBCWD3D0CTCSD3CRD9D1CTD2D8CSCPD8CPCQCPD7CTBA CCCWCTCRD0D9D7D8CTD6CRD3D2D7CXD7D8D7 D3CUD8DBD3D7D9CQB9CRD0D9D7D8CTD6D7BA C7D2CTD7D9CQB9CRD0D9D7D8CTD6CXD7D8CWCTCRD0D9D7B9 D8CTD6 D3CU D6CTD8D6CXCTDACTCS CSD3CRD9D1CTD2D8D7B8DBCWCXCRCW CXD7D8CWCT D8CPD6CVCTD8 D3CUCUD9D6D8CWCTD6CRD0D9D7D8CTD6CXD2CVBA CCCWCTD3D8CWCTD6D3D2CTCRD3D6D6CTD7D4D3D2CSD7 D8D3 D8CWCT D6CTD7D8 D3CU CSCPD8CPCQCPD7CTBA CCCWCT CRD3D2D8D6CPD7D8 CQCTD8DBCTCTD2 D8CWD3D7CTD7D9CQB9CRD0D9D7D8CTD6D7D7CWD3D9D0CSCXD1D4D0CXCRCXD8D0DDCRCPD6D6DDD8CWCTCXD2B9 CUD3D6D1CPD8CXD3D2D3CUD5D9CTD6DDBA Cluster of All Documents Cluster of Other Documents Cluster of Retrieved Documents Sub-Clusters BYCXCVD9D6CTBEBM BVD0D9D7D8CTD6CXD2CVCACTD8D6CXCTDACTCSBWD3CRD9D1CTD2D8D7 BYD3D6 CBD8CTD4 BDB8 DBCT CPCSD3D4D8 CP D6CTCRD9D6D7CXDACT DACTD6D7CXD3D2 D3CUC5CPDCCXD1D9D1CSCXD7D8CPD2CRCTCRD0D9D7D8CTD6CXD2CVCPD0CVD3D6CXD8CWD1B4CCD3D9 CPD2CSBZD3D2DECPD0CTDEB8 BDBLBJBGB5B8 CXD2DBCWCXCRCWD3D2CTCRD0D9D7D8CTD6D1CPDD CWCPDACT D1D3D6CT D8CWCPD2 D8DBD3 D7D9CQB9CRD0D9D7D8CTD6D7 CPCRCRD3D6CSCXD2CV D8D3 CSCXD7D8CPD2CRCT CPD1D3D2CV CSD3CRD9D1CTD2D8D7BA CBCXD1CXD0CPD6CXD8DD CPD1D3D2CV CSD3CRD9D1CTD2D8D7 CPD6CT CSCTACD2CTCS CXD2 D8CTD6D1D7 D3CU D8CWCT DACTCRD8D3D6 D7D4CPCRCTD1D3CSCTD0BA CCCWCTCWCXCTD6CPD6CRCWCXCRCPD0 D7D8D6D9CRD8D9D6CTCXD2BYCXCVD9D6CTBE D6CTD4D6CTB9 D7CTD2D8D7 D2D3D8 D3D2D0DD D8CWCT D7CXD1CXD0CPD6CXD8DD D7D8D6D9CRD8D9D6CT CPD1D3D2CV D8CWCT D6CTD8D6CXCTDACTCS CSD3CRD9D1CTD2D8D7CQD9D8CPD0D7D3 D8CWCTD7D8D6D9CRD8D9D6CT D3CU CRD3D2D8D6CPD7D8 CQCTD8DBCTCTD2 D8CWCT D6CTD8D6CXCTDACTCS CSD3CRD9D1CTD2D8D7 CPD2CS D8CWCT D2D3D2B9D6CTD8D6CXCTDACTCS CSD3CRD9D1CTD2D8D7BA CCCWCTD6CTCUD3D6CTB8 CXD2CBD8CTD4BEDBCTDBD3D9D0CSD0CXCZCTD8D3D1CPD4D7D9CRCWD7D8D6D9CRD8D9D6CPD0 CXD2CUD3D6D1CPD8CXD3D2 CXD2D8D3 DBCTCXCVCWD8 D3CU DBD3D6CSD7BA BTD7 CUD3D6 D8CWCT D7D8CTD4B8DBCTCXD2D8D6D3CSD9CRCTCPDBCPDDD8D3D1CTCPD7D9D6CTD8CWCTCRD3D2D8D6CXB9 CQD9D8CXD3D2 D3CU CTCPCRCW DBD3D6CS D8D3 CUD3D6D1CXD2CVCP CVCXDACTD2 CRD0D9D7D8CTD6 D7D8D6D9CRD8D9D6CTBA C1D8 CXD7 CQCPD7CTCS D3D2 CP D1CTCPD7D9D6CTB8 CRCPD0D0CTCS CXD2B9 CUD3D6D1CPD8CXD3D2 CVCPCXD2 D6CPD8CXD3 B4C1BZCAB5BA BUDD CRD3D1CQCXD2CXD2CV D8CWCT DBCTCXCVCWD8 D3CU C1BZCA DBCXD8CW D3D8CWCTD6 DBCTCXCVCWD8D7 D7D9CRCW CPD7 CCBY CPD2CS D8CWCT CXD2DACTD6D7CT CSD3CRD9D1CTD2D8 CUD6CTD5D9CTD2CRDDB4C1BWBYB5B8DBCTD3CQD8CPCXD2CPCRD3D1D4D3D7CXD8CTDBCTCXCVCWD8 CUD3D6 CTCPCRCW DBD3D6CS CXD2 CSD3CRD9D1CTD2D8D7BA C6D3D8CT D8CWCPD8 D8CWD3D7CT D8CWD6CTCT D8DDD4CTD7 D3CU DBCTCXCVCWD8 CWCPDACT CSCXABCTD6CTD2D8 CUCTCPD8D9D6CTD7BA C1BZCAB8 CCBYB8 C1BWBY D6CTD4D6CTD7CTD2D8 D8CWCT CXD1D4D3D6D8CPD2CRCT D3CU CP DBD3D6CS CXD2 CP CRD0D9D7D8CTD6B8 CXD2 CP CSD3CRD9D1CTD2D8B8 CPD2CS CXD2 D8CWCT DBCWD3D0CTCSD3CRD9D1CTD2D8CSCPD8CPCQCPD7CTB8D6CTD7D4CTCRD8CXDACTD0DDBA CCCWCTD6CTB9 CUD3D6CTB8DBCTCTDCD4CTCRD8D8CWCPD8D8CWCTCRD3D1D4D3D7CXD8CTDBCTCXCVCWD8DBD3D9D0CS CQCTCPD2D3DACTD6CPD0D0DBCTCXCVCWD8CXD2D6CTD8D6CXCTDACTCSCSD3CRD9D1CTD2D8D7BA BEBABE CCCTD6D1 CFCTCXCVCWD8CXD2CV CQCPD7CTCS D3D2 C1D2CUD3D6D1CPD8CXD3D2 BZCPCXD2 CACPD8CXD3 BXCPCRCW CXD2D2CTD6 D2D3CSCT D3CU D8CWCT CRD0D9D7D8CTD6 D8D6CTCT D6CTD4D6CTD7CTD2D8D7 D8CWCT D4CPD6D8CXD8CXD3D2 D3CU CP CRD0D9D7D8CTD6 CQCPD7CTCS D3D2 D8CWCT D7CXD1CXD0CPD6B9 CXD8DD CPD1D3D2CVCSD3CRD9D1CTD2D8D7BA CCCWCTD6CTCUD3D6CTB8 D8CWCT CXD2CUD3D6D1CPB9 D8CXD3D2CPCQD3D9D8D7CXD1CXD0CPD6CXD8DDCPD1D3D2CVCSD3CRD9D1CTD2D8D7CRCPD2 CQCT D1CPD4D4CTCSCXD2D8D3D8CWCTDBCTCXCVCWD8D3CUDBD3D6CSD7CQDDD8CWCTCUD3D0D0D3DBB9 CXD2CVD7D8CTD4D7BA BDBA BYD3D6 CTCPCRCW CRD0D9D7D8CTD6B8 CRCPD0CRD9D0CPD8CT D8CWCT DBCTCXCVCWD8 D3CU CTCPCRCWDBD3D6CSCPCRCRD3D6CSCXD2CVD8D3D8CWCTD4CPD6D8CXD8CXD3D2BA BEBA CBCXD2CRCTCTCPCRCWCSD3CRD9D1CTD2D8CXD7D7D4CTCRCXACCTCSCQDDD8CWCTD7CTB9 D6CXCTD7 D3CU D4CPD6D8CXD8CXD3D2D7 CUD6D3D1 D8CWCT D6D3D3D8 D2D3CSCT D8D3 CP D0CTCPCU D3CU D8CWCT CRD0D9D7D8CTD6 D8D6CTCTB8 CRCPD0CRD9D0CPD8CT D8CWCT D8D3D8CPD0 DBCTCXCVCWD8 D3CU CTCPCRCW DBD3D6CS CXD2 CP CSD3CRD9D1CTD2D8 CQDD CXD2B9 D8CTCVD6CPD8CXD2CV D8CWCT DBCTCXCVCWD8D7 D3CU CTCPCRCW DBD3D6CS CXD2 D8CWCT D7CTD6CXCTD7D3CUD4CPD6D8CXD8CXD3D2D7BA CBD8CTD4 BD CXD7 D8CWCT DACTD6DD CRD3D6CT D3CU D3D9D6 D1CTD8CWD3CSBA CCCWCT CQCPD7CXCR CXCSCTCP CXD7 D8CWCPD8 DBCT CPD7D7CXCVD2 CP CWCXCVCWCTD6 DBCTCXCVCWD8 D8D3 D8CWCT DBD3D6CS D8CWCPD8 D1CPCZCTD7 D1D3D6CT CRD3D2D8D6CXCQD9D8CXD3D2 D8D3 CSCTB9 D8CTD6D1CXD2CT D8CWCT D7D8D6D9CRD8D9D6CT D3CU D8CWCT D7D9CQB9CRD0D9D7D8CTD6D7BA CCCWCT CSCTCVD6CTCT D3CU CRD3D2D8D6CXCQD9D8CXD3D2 CRCPD2 CQCTD1CTCPD7D9D6CTCSCQDD D8CWCT CSCTCVD6CTCT D3CU CRD3D2D7CXD7D8CTD2CRDD CQCTD8DBCTCTD2 D8CWCT CSCXD7D8D6CXCQD9D8CXD3D2 D3CUCPDBD3D6CSCPD2CSD8CWCTD4CPD6D8CXD8CXD3D2D3CUCPCRD0D9D7D8CTD6BA BYD3D6CTDCCPD1D4D0CTB8D0CTD8D9D7CRD3D2D7CXCSCTD6D4CPD6D8CXD8CXD3D2CXD2CVD8CWCT CRD0D9D7D8CTD6CXD2D8D3D8CWD6CTCTD7D9CQB9CRD0D9D7D8CTD6D7CPD7D7CWD3DBD2CXD2BYCXCVB9 D9D6CT BFBA CBCXD2CRCT D8CWCT DBD3D6CS BT CXD7 D8CWCT D1D3D7D8 CUD6CTD5D9CTD2D8 DBD3D6CSCXD2D8CWCTCRD0D9D7D8CTD6 BV BC B8 BT CRCPD2CQCTD6CTCVCPD6CSCTCSCPD7CP CRCWCPD6CPCRD8CTD6CXD7D8CXCRDBD3D6CSD3CUBV BC BA CBCTDACTD6CPD0D1CTD8CWD3CSD7D4D6D3B9 D4D3D7CTCS D7D3 CUCPD6B8 CXD2CSCTCTCSB8 CPCSD3D4D8 D8CWCT D1D3D7D8 CUD6CTD5D9CTD2D8 DBD3D6CSD7 CPD7 D8CWCT CZCTDDDBD3D6CSD7 D3CU CRD0D9D7D8CTD6D7BA C0D3DBCTDACTD6B8 DBCT CRCPD2 D7CTCT D8CWCPD8 D8CWCT DBD3D6CS BT CXD7 D2D3D8 D9D7CTCUD9D0 CUD3D6 D7CTD0CTCRD8CXD2CV D3D2CT D3CU D7D9CQB9CRD0D9D7D8CTD6D7B8 CQCTCRCPD9D7CT D8CWCT DBD3D6CS D9D2CXCUD3D6D1D0DDCPD4D4CTCPD6D7CXD2CPD0D0D3CUD7D9CQB9CRD0D9D7D8CTD6D7BA C7D2D8CWCT D3D8CWCTD6CWCPD2CSB8D8CWCTDBD3D6CSBY CXD7D2D3D8CWCXCVCWCUD6CTD5D9CTD2CRDDCXD2 BV BC CQD9D8 CRD3D2CRCTD2D8D6CPD8CTD7 D3D2 D8CWCT CRD0D9D7D8CTD6 BV BF BA CCCWCTD6CTB9 CUD3D6CTB8 D8CWCT CPD4D4CTCPD6CPD2CRCT D3CU D8CWCT DBD3D6CS BY CXD7 CP CVD3D3CS CRD0D9CT D8D3 D7CTD0CTCRD8 D8CWCT CRD0D9D7D8CTD6 BV BF BA C1D2 D8CWCXD7 CRCPD7CTB8 DBCT D1CPDD CRD3D2D7CXCSCTD6 D8CWCPD8 D8CWCT DBD3D6CS BY CRD3D2D8D6CXCQD9D8CTD7 D8D3 CSCTCRCXCSCXD2CVD8CWCTD7D9CQB9D7D8D6D9CRD8D9D6CTD3CUD8CWCTCRD0D9D7D8CTD6CQCTD8D8CTD6 D8CWCPD2 D8CWCT DBD3D6CS BTBA C1D2 D8CWCT D2CTDCD8 D7CTCRD8CXD3D2B8 DBCT D4D6D3B9 D4D3D7CT D8CWCT D9D7CT D3CU C1D2CUD3D6D1CPD8CXD3D2 BZCPCXD2 CACPD8CXD3B4C1BZCAB5 CPD7CPD1CTCPD7D9D6CTD3CUD7D9CRCWCRD3D2D8D6CXCQD9D8CXD3D2BA AAA BBB DDD C 1 AAA BBB DEE C 2 AAA FFF DGG C 3 C 0 BTAOBZBMCFD3D6CSD7 BYCXCVD9D6CT BFBM CFD3D6CS BWCXD7D8D6CXCQD9D8CXD3D2 CPD2CS C8CPD6D8CXD8CXD3D2 D3CU BVD0D9D7D8CTD6 BEBABEBABD C1D2CUD3D6D1CPD8CXD3D2 BZCPCXD2 CACPD8CXD3 CCCWCTC1BZCA D3D6CXCVCXD2CPD0D0DD CXD7 D8CWCT D1CTCPD7D9D6CTD3CU CVD3D3CSD2CTD7D7 CUD3D6 CPD8D8D6CXCQD9D8CTD7 D9D7CTCS CXD2 D8CWCT CSCTCRCXD7CXD3D2 D8D6CTCT D0CTCPD2CXD2CV CPD0CVD3D6CXD8CWD1 BVBGBABHB4C9D9CXD2D0CPD2B8 BDBLBLBFB5BA C1D8 D6CTD4D6CTD7CTD2D8D7 CWD3DBD4D6CTCRCXD7CTD0DDD8CWCTCPD8D8D6CXCQD9D8CTD7 D4D6CTCSCXCRD8D8CWCTCRD0CPD7D7CTD7 D3CUCTDCCPD1D4D0CTCRCPD7CTD7BA BUDDD6CTCVCPD6CSCXD2CVCPCRD0D9D7D8CTD6D7D8D6D9CRB9 D8D9D6CT D3CU CSD3CRD9D1CTD2D8D7 CPD7 CP CSCTCRCXD7CXD3D2 D8D6CTCTB8 DBCT D1CPDD D9D7CT D8CWCT C1BZCA CPD7 CP D1CTCPD7D9D6CT D3CU D8CWCT CRD3D2D7CXD7D8CTD2CRDD CQCTD8DBCTCTD2D8CWCTCSCXD7D8D6CXCQD9D8CXD3D2D3CUCPDBD3D6CSCPD2CSD8CWCTD4CPD6B9 D8CXD8CXD3D2D3CUCPCRD0D9D7D8CTD6BA CCCWCT C1BZCA DACPD0D9CT D3CU D8CWCT DBD3D6CS DB CXD2 D8CWCT CRD0D9D7D8CTD6 BVB8 CVCPCXD2 D6B4DBBNBVB5B8CXD7CRCPD0CRD9D0CPD8CTCSCPD7CUD3D0D0D3DBBM CVCPCXD2 D6B4DBBNBVB5 BP CVCPCXD2B4DBBNBVB5 D7D4D0CXD8 CXD2CUD3B4BVB5 BN B4BDB5 CVCPCXD2B4DBBNBVB5 BP CTD2D8D6D3D4DDB4DBBNBVB5A0 CTD2D8D6D3D4DD D4 B4DBBNBVB5BN CTD2D8D6D3D4DDB4DBBNBVB5 BP A0D4B4DBCYBVB5D0D3CV BE D4B4DBCYBVB5 A0B4BDA0D4B4DBCYBVB5B5D0D3CV BE B4BDA0 D4B4DBCYBVB5B5BN D4B4DBCYBVB5 BP CUD6CTD5B4DBBNBVB5BPCYBVCYBN CTD2D8D6D3D4DD D4 B4DBBNBVB5 BP CG CX CYBV CX CY CYBVCY CTD2D8D6D3D4DDB4DBBNBV CX B5BN D7D4D0CXD8 CXD2CUD3B4BVB5 BP A0 CG CX CYBV CX CY CYBVCY D0D3CV CYBV CX CY CYBVCY BN DBCWCTD6CTCUD6CTD5B4DBBNBVB5B8BV CX CPD2CSCYBV CX CYCPD6CTD8CWCTCUD6CTD5D9CTD2CRDD D3CU D8CWCT DBD3D6CS DB CXD2 BVB8 D8CWCT CXB9D8CW D7D9CQB9CRD0D9D7D8CTD6 D3CU BVB8 CPD2CSD8CWCTD2D9D1CQCTD6D3CUDBD3D6CSD7CXD2 BV CX B8D6CTD7D4CTCRD8CXDACTD0DDBA BYD3D6CTDCCPD1D4D0CTB8D8CWCTC1BZCADACPD0D9CTD7D3CUDBD3D6CSD7CXD2BYCXCVB9 D9D6CTBFCWD3D0CSD8CWCTCUD3D0D0D3DBCXD2CVD6CTD0CPD8CXD3D2BM CVCPCXD2 D6B4BUBNBV BC B5B3 CVCPCXD2 D6B4BYBNBV BC B5 BQ CVCPCXD2 D6B4BXBNBV BC B5BPCVCPCXD2 D6B4BZBNBV BC B5 BQCVCPCXD2D6B4BWBNBV BC B5BQCVCPCXD2D6B4BTBNBV BC B5BA CVCPCXD2 D6B4BTBNBV BC B5BPBCBMBCBCBCBNCVCPCXD2D6B4BUBNBV BC B5BPBCBMBDBIBDBN CVCPCXD2 D6B4BWBNBV BC B5BPBCBMBCBFBDBNCVCPCXD2 D6B4BXBNBV BC B5BPBCBMBCBKBCBN CVCPCXD2 D6B4BYBNBV BC B5BPBCBMBDBHBJBNCVCPCXD2 D6B4BZBNBV BC B5BPBCBMBCBKBCBM BEBABEBABE CFCTCXCVCWD8CXD2CV CCCTD6D1D7 CQCPD7CTCS D3D2 C1D2CUD3D6D1CPD8CXD3D2 BZCPCXD2 CACPD8CXD3 BYD3D6 CTCPCRCW DBD3D6CS CXD2 CTDACTD6DD CSD3CRD9D1CTD2D8B8 DBCT CRCPD2 CRD3D0B9 D0CTCRD8 CP D7CTD8 D3CUC1BZCA DACPD0D9CTD7 CQDDD8D6CPCRCXD2CV D8CWCT D4CPD8CWCXD2 D8CWCT CRD0D9D7D8CTD6 D8D6CTCT CUD6D3D1 D8CWCT D6D3D3D8 D8D3 D8CWCT D0CTCPCU CRD3D6B9 D6CTD7D4D3D2CSCXD2CV D8D3 D8CWCT CSD3CRD9D1CTD2D8BA CCCWCTD6CT DBD3D9D0CS CQCT D7CTDACTD6CPD0 DBCPDDD7 D8D3 D9D7CT D8CWCT D7CTD8 D3CU C1BZCA DACPD0D9CTD7 CPCRB9 CRD3D6CSCXD2CVD8D3D8CWCTCSCTD7CXCVD2D3CUD9D7CTD6CXD2D8CTD6CUCPCRCTD7BA BYD3D6 CXD2D7D8CPD2CRCTB8 DBCT D2CTCTCSD8D3 CXD2D8CTCVD6CPD8CT D8CWCT D7CTD8 D3CU C1BZCADACPD0D9CTD7CXD2D8D3D3D2CTDACPD0D9CTCXCUDBCTCPCSD3D4D8CPD0CXD7D8B9D7D8DDD0CT D9D7CTD6CXD2D8CTD6CUCPCRCTB8DBCWCXCRCWCSCXD7D4D0CPDDD7D8CWCTD6CPD2CZCTCSD0CXD7D8D3CU CSD3CRD9D1CTD2D8D7 CPD0D3D2CV DBCXD8CW CTCPCRCW D7D9D1D1CPD6DDB8 D0CXCZCT D8CWCT CXD2D8CTD6CUCPCRCTD7 D3CU CFCTCQ D7CTCPD6CRCW CTD2CVCXD2CTD7BA CCCWCTD6CT DBD3D9D0CS CQCTD7CTDACTD6CPD0DBCPDDD7D3CUCXD2D8CTCVD6CPD8CXD3D2B8 CTBACVBAB8 D7D9D1D1CPD8CXD3D2 D3CUCPD0D0DACPD0D9CTD7B8D8CWCTD1CPDCCXD1D9D1DACPD0D9CTB8CPD2CSD7D3D3D2BA C1D2 D8CWCXD7 D4CPD4CTD6B8 DBCT D7D9D4D4D3D7CT D8CWCT D0CXD7D8B9D7D8DDD0CT D9D7CTD6 CXD2D8CTD6CUCPCRCTB8DBCWCTD6CTCPD0D0D7D9D1D1CPD6CXCTD7D3CUD6CTD8D6CXCTDACTCSCSD3CRB9 D9D1CTD2D8D7CPD6CTD7CWD3DBD2D8D3D8CWCTD9D7CTD6CPD8D3D2CRCTCPD7CPD0CXD7D8B8 CPD2CSCPCSD3D4D8D8CWCTCXD2D8CTCVD6CPD8CXD3D2CVCXDACTD2CQDDD8CWCTD7D9D1D1CPB9 D8CXD3D2D7CWD3DBD2CXD2B4BEB5CPD2CSBYCXCVD9D6CTBGBM CXCVD6B4DBBNBWB5 BP CG BVBEBVD7CTD8B4BWB5 CVCPCXD2 D6B4DBBNBVB5BN B4BEB5 DBCWCTD6CT BVD7CTD8B4BWB5 CXD7 D8CWCT D7CTD8 D3CU CPD0D0 CRD0D9D7D8CTD6D7 D8D3 DBCWCXCRCWD8CWCTCSD3CRD9D1CTD2D8BWCQCTD0D3D2CVD7BA CCCWCTCXD2D8CTCVD6CPD8CXD3D2 D1CTD8CWD3CSCTD5D9CPD0D0DDD8CPCZCTD7CPCRCRD3D9D2D8D3CUCTCPCRCWC1BZCADACPD0D9CT CXD2CPD4CPD8CWD3CUCRD0D9D7D8CTD6BA C1D8CXD7CQCPD7CTCSD3D2D3D9D6CPD7D7D9D1D4B9 D8CXD3D2 D8CWCPD8 CPD0D0 D3CU CRD0D9D7D8CTD6 D4CPD6D8CXD8CXD3D2D7 CTDACTD2D0DD CRD3D2B9 D8D6CXCQD9D8CT D8D3 D7CTD0CTCRD8CXD2CV CRD3D6D6CTD7D4D3D2CSCXD2CV CSD3CRD9D1CTD2D8D7BA CFCXD8CWD8CWCTDBCTCXCVCWD8CXCVD6B4DBBNBWB5B8DBCTCSCTACD2CTD8CWCTDBCTCXCVCWD8 w gain_r ( w , C1 ) gain_r ( w , C2 ) gain_r ( w , C3 ) Cluster C1 Cluster C2 Cluster C3 + + igr ( w , D ) Document D BYCXCVD9D6CTBGBM C1D2D8CTCVD6CPD8CXD3D2D3CUC1BZCADACPD0D9CTD7 DBCTCXCVCWD8B4DBBNBWB5D3CUD8CWCTDBD3D6CSDB CXD2D8CWCTCSD3CRD9D1CTD2D8BW CPD7D8CWCTCRD3D1CQCXD2CPD8CXD3D2D3CUD8CWCTD8CWD6CTCTD8DDD4CTD7D3CUCUD9D2CSCPB9 D1CTD2D8CPD0DBCTCXCVCWD8D7B8CCBYB8C1BWBYCPD2CSC1BZCABA DBCTCXCVCWD8B4DBBNBWB5 BP CXCVD6B4DBBNBWB5A1 D8CUB4DBBNBWB5A1 CXCSCUB4DBB5 B4BFB5 BF BXDCD4CTD6CXD1CTD2D8CPD0 BXDACPD0D9CPD8CXD3D2 BFBABD CBD9D1D1CPD6CXDECPD8CXD3D2 CQDD CBCTD2D8CTD2CRCT BXDCD8D6CPCRD8CXD3D2 C7D9D6 CPCXD1 CXD2 D8CWCXD7 D4CPD4CTD6 CXD7 D8D3 D7CWD3DB D8CWCPD8 D3D9D6 DBCTCXCVCWD8CXD2CVD1CTD8CWD3CSCXD7CTABCTCRD8CXDACTCXD2D7D9D1D1CPD6CXDECXD2CVD6CTB9 D8D6CXCTDACTCSCSD3CRD9D1CTD2D8D7BA CCCWCTD6CTCUD3D6CTB8DBCTD9D7CTD8CWCTD1D3D7D8 CUD9D2CSCPD1CTD2D8CPD0 D7CRCWCTD1CT D3CU D7D9D1D1CPD6CXDECPD8CXD3D2B8 DBCWCXCRCW CXD7 D8CWCT D7CTD2D8CTD2CRCT CTDCD8D6CPCRD8CXD3D2 CQCPD7CTCS D3D2 D8CWCT D8CTD6D1 DBCTCXCVCWD8CXD2CVCPD7CUD3D0D0D3DBD7BA BDBA C4CTD8D8CWCTCXD1D4D3D6D8CPD2CRCT D7 CXD1D4B4D7BNBWB5D3CUD8CWCTD7CTD2B9 D8CTD2CRCT D7 CXD2D8CWCTCSD3CRD9D1CTD2D8 BW CQCTD8CWCTCPDACTD6CPCVCT DBCTCXCVCWD8D3CUCZCTDDDBD3D6CSD7CXD2D8CWCTD7CTD2D8CTD2CRCTBA D7 CXD1D4B4D7BNBWB5 BP BD CYD7CY CG DBBECZCTDDDBB4D7B5 DBCTCXCVCWD8B4DBBNBWB5BN DBCWCTD6CT CYD7CY CPD2CS CZCTDDDBB4D7B5 CPD6CT D8CWCT D2D9D1CQCTD6D3CU DBD3D6CSD7CXD2 D7 CPD2CSD8CWCTD0CXD7D8D3CUCZCTDDDBD3D6CSD7CXD2 D7BA BEBA BXDCD8D6CPCRD8 D7CTD2D8CTD2CRCTD7 DBCXD8CW CWCXCVCWCTD6 CXD1D4D3D6D8CPD2CRCT CUD6D3D1 D8CWCT D3D6CXCVCXD2CPD0 CSD3CRD9D1CTD2D8B8 D9D2D8CXD0 D8CWCT D8D3B9 D8CPD0 D0CTD2CVD8CW D3CU D7CTD0CTCRD8CTCS D7CTD2D8CTD2CRCTD7 CTDCCRCTCTCSD7 CP CRCTD6D8CPCXD2D4D6CTCSCTD8CTD6D1CXD2CTCSD0CTD2CVD8CWD3CUD7D9D1D1CPD6DDBA C7D9D6CTDCD4CTD6CXD1CTD2D8CXD7 D4CTD6CUD3D6D1CTCSD9D2CSCTD6D8CWCTCRD3D2B9 CSCXD8CXD3D2D7D8CWCPD8CZCTDDDBD3D6CSD7CPD6CTD2D3D9D2D7B8CPD2CSD8CWCTCRD9D8D3AB D0CTD2CVD8CWCXD7BDBHBCDBD3D6CSD7BA BFBABE BXDACPD0D9CPD8CXD3D2 D3CU CBD9D1D1CPD6CXDECPD8CXD3D2 CXD2 C1CA CCCPD7CZD7 CFCT CTDACPD0D9CPD8CTCS D3D9D6 D8CTD6D1 DBCTCXCVCWD8CXD2CV D1CTD8CWD3CS CXD2 C1CAD8CPD7CZD7D3CUC6CCBVC1CABECC CTDCD8CBD9D1D1CPD6CXDECPD8CXD3D2BVCWCPD0B9 D0CTD2CVCTBDB4CCCBBVBDB5B4BYD9CZD9D7CXD1CPCPD2CSC7CZD9D1D9D6CPB8BEBCBCBDB5BA CCCWCT CSCPD8CP D7CTD8 CSCXD7D8D6CXCQD9D8CTCS CQDD CCCBBVBD CRD3D1D1CXD8D8CTCT CRD3D2D7CXD7D8D7D3CUBDBED8D3D4CXCRD7BA BXCPCRCWD8D3D4CXCRCWCPD7D3D2CTD5D9CTD6DD CPD2CSBHBCD6CTD8D6CXCTDACTCSCSD3CRD9D1CTD2D8D7BA CCCWD3D7CTCSD3CRD9D1CTD2D8D7 CPD6CT D6CTD8D6CXCTDACTCS CQDD CP D7CTCPD6CRCW CTD2CVCXD2CT CUD6D3D1 CPD0D0 D3CU C5CPCXD2CXCRCWCXCBCWCXD1CQD9D2C6CTDBD7D4CPD4CTD6CPD6D8CXCRD0CTD7CXD2BDBLBLBKBA BYCXCVD9D6CTBHD7CWD3DBD7D8CWCTD7CRCWCTD1CTD3CUD8CWCTCTDACPD0D9CPD8CXD3D2BA BXDACTD6DD D4CPD6D8CXCRCXD4CPD2D8 D1CPCSCT CP D7D9D1D1CPD6DD CUD3D6 CTCPCRCW CSD3CRD9D1CTD2D8 DBCXD8CW CWCXD7BBCWCTD6 D7DDD7D8CTD1 CPD2CS D7D9CQD1CXD8D8CTCS BIBCBCD7D9D1D1CPD6CXCTD7D8D3CCCBBVBDCRD3D1D1CXD8D8CTCTBA CCCBBVBDCRD3D1B9 D1CXD8D8CTCT CTDACPD0D9CPD8CTCS D8CWCT D7D9D1D1CPD6CXCTD7 CQDD D4D6CTD7CTD2D8CXD2CV D8CWCTD5D9CTD6CXCTD7CPD2CSD8CWCTD7D9D1D1CPD6CXCTD7D8D3BFBID7D9CQCYCTCRD8D7B4BFBI D7D8D9CSCTD2D8D7B5BA CCCWD6CTCTD7D9CQCYCTCRD8D7DBCTD6CTCPD7D7CXCVD2CTCSD8D3D3D2CT D3CU D8D3D4CXCRD7 CPD2CS D8CWCTDD CYD9CSCVCTCS D8CWCT D6CTD0CTDACPD2CRCT CQCTB9 D8DBCTCTD2D8CWCTD5D9CTD6DDCPD2CSCTCPCRCWD7D9D1D1CPD6DDBA CCCWCTD5D9CPD0CXD8DD D3CU D7D9D1D1CPD6CXCTD7CPD6CT CTDACPD0D9CPD8CTCS CQDD CRD3D1D4CPD6CXD2CV D7D9CQB9 CYCTCRD8D7B3 D6CTD0CTDACPD2CRCT CYD9CSCVD1CTD2D8D7 CUD3D6 D7D9D1D1CPD6CXCTD7 DBCXD8CW D8CWCT D6CTD0CTDACPD2CRCT CYD9CSCVD1CTD2D8 CUD3D6 D8CWCT D3D6CXCVCXD2CPD0 CSD3CRB9 D9D1CTD2D8D7B8 DBCWCXCRCW CXD7 CRCPD6CTCUD9D0D0DD CPD7D7CXCVD2CTCS CQDD CCCBBVBD CRD3D1D1CXD8D8CTCTBA C1CU D8CWD3D7CT D8DBD3 D6CTD0CTDACPD2CRCT CYD9CSCVD1CTD2D8D7 CPD6CT CWCXCVCWD0DD CRD3D2D7CXD7D8CTD2D8 DBCXD8CW CTCPCRCW D3D8CWCTD6B8 DBCT D1CPDD CRD3D2CRD0D9CSCT D8CWCPD8 D8CWCT D7DDD7D8CTD1 CXD7 DACTD6DD CTABCTCRD8CXDACT CXD2 D7D9D1D1CPD6DDCVCTD2CTD6CPD8CXD3D2CUD3D6D6CTD8D6CXCTDACTCSCSD3CRD9D1CTD2D8D7BA BTD0D8CWD3D9CVCWD8CWCTD6CTD0CTDACPD2CRCTD3CUCTCPCRCWD3D6CXCVCXD2CPD0CSD3CRD9B9 D1CTD2D8CXD7CVD6CPCSCTCSCTCXD8CWCTD6COBTB4D6CTD0CTDACPD2D8B5B3B8COBUB4D6CTD0CPD8CTCSB5B3 D3D6 COBVB4D2D3D8 D6CTD0CTDACPD2D8B5B3B8 CTCPCRCW D7D9CQCYCTCRD8 CPD2D7DBCTD6D7 D8CWCT D6CTD0CTDACPD2CRCT D3CU CTCPCRCW D7D9D1D1CPD6DD DBCXD8CW COCHCTD7B3 D3D6 B3C6D3B3BA CCCWCTD6CTCUD3D6CTB8 D8CWCTD6CT CPD6CTB8 CPD8 D0CTCPD7D8B8 D8DBD3 CRD6CXD8CTD6CXCP CUD3D6 CTDACPD0D9CPD8CXD2CV CRD3D2D7CXD7D8CTD2CRDDBM BTD2D7DBCTD6 C4CTDACTD0 BTB8 CXD2 DBCWCXCRCWCSD3CRD9D1CTD2D8D7D3CUD8CWCTCVD6CPCSCTBTCPD6CTD6CTCVCPD6CSCTCSCPD7 COD6CTD0CTDACPD2D8B3B4CBD8D6CXCRD8BXDACPD0D9CPD8CXD3D2B5B8CPD2CSBTD2D7DBCTD6 C4CTDACTD0 BUB8CXD2DBCWCXCRCWCSD3CRD9D1CTD2D8D7D3CUCTCXD8CWCTD6D8CWCTCVD6CPCSCTBTD3D6 D8CWCT CVD6CPCSCT BU CPD6CT D6CTCVCPD6CSCTCS CPD7 COD6CTD0CTDACPD2D8B3 B4C4D3D3D7CT BXDACPD0D9CPD8CXD3D2B5BA 1 2 3 1 2 3 B A C Original Documents Query Subject Relevance Judgment Summary Summarization Relevance Judgment by Subject Yes Yes No Aaccuracy Quickness BYCXCVD9D6CTBHBM BXDACPD0D9CPD8CXD3D2D3CUCBD9D1D1CPD6CXCTD7CXD2C1CAD8CPD7CZ BFBABF BXDCD4CTD6CXD1CTD2D8CPD0 CACTD7D9D0D8 CCCWCTCTDCD4CTD6CXD1CTD2D8CPD0D6CTD7D9D0D8D3CUD3D9D6D7DDD7D8CTD1CXD2CCCBBVBD CXD7 D7CWD3DBD2 CXD2 CCCPCQD0CT BD CPD0D3D2CV DBCXD8CW D8CWCT D6CTD7D9D0D8D7 D3CU D3D8CWCTD6 D4CPD6D8CXCRCXD4CPD8CXD2CV D7DDD7D8CTD1D7 CPD2CS D8CWD6CTCT CQCPD7CTD0CXD2CT D7DDD7D8CTD1D7B8COBYD9D0D0D8CTDCD8B3B8COCCBYDBCXD8CWC9BUB3CPD2CSCOC4CTCPCSB3BA C1D2 D8CWCT D7D9D1D1CPD6CXDECPD8CXD3D2 CUD3D6 D6CTD8D6CXCTDACTCS CSD3CRD9B9 D1CTD2D8D7B8 CXD8 CXD7 CXD1D4D3D6D8CPD2D8 D8D3 CXD1D4D6D3DACT CQD3D8CW D3CU D8CWCT CPCRCRD9D6CPCRDDD3CUD8CWCTCYD9CSCVD1CTD2D8D7D3CUD6CTD0CTDACPD2CRCTCPD2CSD8CWCT D8CXD1CTD6CTD5D9CXD6CTCSD8D3 D1CPCZCT D8CWCT CYD9CSCVD1CTD2D8D7B8D7CXD1D9D0D8CPB9 D2CTD3D9D7D0DDBA C0D3DBCTDACTD6B8 D8CWCTD6CT CXD7 CP D8D6CPCSCTB9D3AB D6CTD0CPD8CXD3D2 CQCTD8DBCTCTD2 D8CWCTD1BA CCCWCTD6CTCUD3D6CTB8 DBCT D4D0D3D8 D8CWCT D6CTD0CPD8CXD3D2 CQCTD8DBCTCTD2 D8CWCT D8CXD1CT CUD3D6 CYD9CSCVD1CTD2D8 CPD2CS D8CWCT D3D8CWCTD6 D1CTCPD7D9D6CTD7CXD2BYCXCVD9D6CTBIBA BG BWCXD7CRD9D7D7CXD3D2 BGBABD BTCRCRD9D6CPCRDD D3CU C8CTD6CUD3D6D1CPD2CRCT D3CU CCCPD7CZ BGBABDBABD BTD2D7DBCTD6 C4CTDACTD0 BT C1D2 D8CWCXD7 D7CTCRD8CXD3D2B8 DBCT CRD3D2D7CXCSCTD6 D8CWCT CTDACPD0D9CPD8CXD3D2 D3CU COBTD2D7DBCTD6 C4CTDACTD0 BTB3BA C7D9D6 D7DDD7D8CTD1 D3D9D8D4CTD6CUD3D6D1D7 D3D8CWCTD6 D4CPD6D8CXCRCXD4CPD8CXD2CV D7DDD7D8CTD1D7 CXD2 D8CTD6D1D7 D3CU CPD0D0 D3CU D1CTCPD7D9D6CTD7B8 D8CWCT CPDACTD6CPCVCT D4D6CTCRCXD7CXD3D2B8 D8CWCT CPDACTD6CPCVCT D6CTCRCPD0D0 CPD2CS D8CWCT CPDACTD6CPCVCT BY D1CTCPD7D9D6CTBA BTD0D8CWD3D9CVCW D8CWCT D4D6CTCRCXD7CXD3D2 D3CU D8CWCT COC4CTCPCSB3 D1CTD8CWD3CSCXD7 BDBABH D4D3CXD2D8 CWCXCVCWCTD6D8CWCPD2D3D9D6D7DDD7D8CTD1B8D3D9D6D7DDD7D8CTD1D3D9D8D4CTD6CUD3D6D1D7 CPD0D0CQCPD7CTD0CXD2CTD7DDD7D8CTD1D7CXD2D3D8CWCTD6D1CTCPD7D9D6CTD7BA CCCWCTBYD1CTCPD7D9D6CTD3CUCOC4CTCPCSB3D1CTD8CWD3CSCXD7BJBABJD4D3CXD2D8 D0D3DBCTD6D8CWCPD2D3D9D6D7DDD7D8CTD1B8CQCTCRCPD9D7CTD8CWCTD4D6CTCRCXD7CXD3D2D3CU D8CWCT D1CTD8CWD3CS CXD7 D8CWCT D0D3DBCTD7D8BA CCCWCT COC4CTCPCSB3 CRCPD2 CQCT D6CTCVCPD6CSCPD7CPD4D6CTCRCXD7CXD3D2B9D3D6CXCTD2D8CTCSD1CTD8CWD3CSBA C1D2 CRD3D1D4CPD6CXD7D3D2 DBCXD8CW COCCBY DBCXD8CW C9BUB3 D1CTD8CWD3CSB8 D3D9D6 D7DDD7D8CTD1 CXD7 BDBCBABL D4D3CXD2D8 CWCXCVCWCTD6 CXD2 D8CWCT D6CTCRCPD0D0B8 BEBABJ D4D3CXD2D8 CWCXCVCWCTD6 CXD2 D8CWCT D4D6CTCRCXD7CXD3D2 CPD2CS BJBABC D4D3CXD2D8 CWCXCVCWCTD6CXD2D8CWCTBYB9D1CTCPD7D9D6CTBA C1D8DBD3D9D0CSD7CWD3DBD8CWCPD8DBCT CRCPD2D1CPCZCTCTABCTCRD8CXDACTD7D9D1D1CPD6CXCTD7CTDACTD2CXCUDBCTCSD3D2D3D8 D9D7CTD8CWCTCXD2CUD3D6D1CPD8CXD3D2D3CUD5D9CTD6DDBA C6D3D8CTD8CWCPD8D8CWCTD4CTD6CUD3D6D1CPD2CRCTD3CUCOBYD9D0D0D8CTDCD8B3CXD7D2D3D8 D8CWCT CQCTD7D8 D3D2CTBA C1D8 D7CWD3DBD7 D8CWCPD8 D7D9D1D1CPD6CXDECTCS D4D6CTB9 D7CTD2D8CPD8CXD3D2 CRCPD2 D6CTCSD9CRCT D9D2D2CTCRCTD7D7CPD6DD CXD2CUD3D6D1CPD8CXD3D2 D8CWCPD8 DBD3D9D0CS CSCTCVD6CPCSCT D8CWCT D4CTD6CUD3D6D1CPD2CRCT D8CWD6D3D9CVCW CXD2CUD3D6D1CPD8CXD3D2 D3DACTD6D0D3CPCSB8 CPD7 DBCTD0D0 CPD7 C7CZD9D1D9D6CP CTD8 CPD0BAB4C7CZD9D1D9D6CPCPD2CSC5D3CRCWCXDED9CZCXB8BEBCBCBCB5D4D3CXD2D8CTCSD3D9D8BA C6CTDCD8B8D0CTD8D9D7CRD3D2D7CXCSCTD6D8CWCTD6CTD0CPD8CXD3D2CQCTD8DBCTCTD2D8CWCT CPCRCRD9D6CPCRDD D3CU D8CWCT D6CTD0CTDACPD2CRCT CYD9CSCVD1CTD2D8D7 CPD2CS D8CWCT CCCPCQD0CTBDBM BXDCD4CTD6CXD1CTD2D8CPD0CACTD7D9D0D8CXD2CCCBBVBD BTD2D7BA C4CTDABA C8D6D3D4D3D7CTCS CBDDD7 BD CBDDD7 BE CBDDD7 BF CBDDD7 BG CBDDD7 BI CBDDD7 BJ CBDDD7 BK CBDDD7 BL BYD9D0D0D8CTDCD8 CCBY DBCXD8CW C9BU C4CTCPCS CACTCRBA BCBABLBCBJ BCBABKBFBF BCBABKBLBL BCBABJBLBF BCBABKBDBK BCBABKBHBK BCBABKBFBD BCBABKBEBG BCBABKBGBL BCBABKBGBF BCBABJBLBK BCBABJBGBC BT C8D6CTBA BCBABJBHBD BCBABJBEBK BCBABJBDBJ BCBABIBKBH BCBABIBJBG BCBABJBDBK BCBABJBFBL BCBABJBFBK BCBABJBGBD BCBABJBDBD BCBABJBEBG BCBABJBIBI BY BCBABKBCBK BCBABJBIBD BCBABJBKBH BCBABJBDBH BCBABJBDBK BCBABJBIBF BCBABJBIBI BCBABJBGBL BCBABJBIBK BCBABJBHBD BCBABJBFBK BCBABJBFBD CACTCRBA BCBABJBHBG BCBABJBGBD BCBABJBLBF BCBABJBDBH BCBABJBFBJ BCBABJBGBH BCBABJBDBL BCBABJBDBL BCBABJBHBE BCBABJBFBI BCBABJBCBC BCBABIBEBH BU C8D6CTBA BCBABKBLBJ BCBABLBEBD BCBABLBCBG BCBABKBLBK BCBABKBJBH BCBABKBLBE BCBABLBCBK BCBABLBDBF BCBABLBEBF BCBABKBKBK BCBABLBDBF BCBABLBEBD BY BCBABJBLBJ BCBABKBCBK BCBABKBEBK BCBABJBJBI BCBABJBJBF BCBABJBKBH BCBABJBJBL BCBABJBJBH BCBABKBCBH BCBABJBJBF BCBABJBJBI BCBABJBDBE CCCXD1CT BKBMBFBF BLBMBGBD BDBEBMBGBK BIBMBEBH BIBMBGBG BLBMBCBD BDBCBMBDBI BLBMBDBI BLBMBFBD BDBFBMBGBI BKBMBGBG BJBMBFBE C4CTD2BA BEBFBGBABG BEBLBJBABK BHBKBHBABJ BKBLBABH BDBFBIBABG BEBKBKBABG BEBLBEBABL BEBIBIBABD BEBIBEBABH BKBDBLBABG BEBHBFBABI BDBJBGBABH CBDDD7 BD AO BLBM C7D8CWCTD6 D4CPD6D8CXCRCXD4CPD8CXD2CV D7DDD7D8CTD1D7BA BYD9D0D0D8CTDCD8BM CCCWCT D7DDD7D8CTD1 DBCWCXCRCW CYD9D7D8 D6CTD8D9D6D2D7 D8CWCT D3D6CXCVD2CPD0 CSD3CRD9D1CTD2D8D7BA CCBY DBCXD8CW C9BUBM CCCWCT D7DDD7D8CTD1 DBCWCXCRCW CVCTD2CTD6CPD8CTD7 D7D9D1D1CPD6CXCTD7 DBCXD8CW CCBYB9CQCPD7CTCS D7CTD2D8CTD2CRCT CTDCD8D6CPCRD8CXD3D2BA CCCWCT DBCTCXCVCWD8D3CUDBD3D6CSD7 CXD2 D8CWCT D5D9CTD6DD CXD7 CSD3D9CQD0CTCSBA BVD3D1D4D6CTD7D7CXD3D2 D6CPD8CXD3 CXD7 BEBCB1BA C4CTCPCSBM CCCWCT D7DDD7D8CTD1 DBCWCXCRCW D6CTD8D9D6D2D7 D8CWCT D0CTCPCS D3CU CSD3CRD9D1CTD2D8BA BVD3D1D4D6CTD7D7CXD3D2 D6CPD8CXD3 CXD7 BEBCB1BA BTD2D7BAC4CTDABABM BTD2D7DBCTD6 C4CTDACTD0BA CACTCRBAB8C8D6CTBAB8BYBM CACTCRCPD0D0B8 C8D6CTCRCXD7CXD3D2 CPD2CS BYB9D1CTCPD7D9D6CTBA CCCXD1CTBM BTDACTD6CPCVCT D8CXD1CT D8D3 CPCRCRD3D1D4D0CXD7CW D8CWCT D6CTD0CTDACPD2CRCT CYD9CSCVD1CTD2D8 D3CU D3D2CT D8CPD7CZ B4BHBC D7D9D1D1CPD6CXCTD7B5BA C4CTD2BABM BTDACTD6CPCVCT D0CTD2CVD8CW D3CU CBD9D1D1CPD6CXCTD7 CXD2 CRCWCPD6CPCRD8CTD6BA 6 7 8 9 10 11 12 13 14 Recall ( Level A ) Time ( min . )</sentence>
				<definiendum id="0">Calculate TFIDF for</definiendum>
				<definiendum id="1">D3CU CRD0D9D7D8CTD6B8 DBCWCXCRCW CRD3D6D6CTD7D4D3D2CSD7</definiendum>
				<definiens id="0">each word Calculate weight of each word with TF , IDF and IGR Hierarchical Clustering by Maximum Distance Algorithm INPUT : Set of Retrieved Documents OUTPUT : Set of Summaries Document Database ( Whole set of Documents ) Calculate TF and DF for each word Tokenize and Extract Nouns Tokenize and Extract Nouns TF and DF</definiens>
			</definition>
</paper>

		<paper id="1159">
			<definition id="0">
				<sentence>We modified the raw email programmatically to delete the attachments , HTML tags , headers and sender information .</sentence>
				<definiendum id="0">HTML</definiendum>
				<definiens id="0">tags , headers and sender information</definiens>
			</definition>
</paper>

		<paper id="2013">
			<definition id="0">
				<sentence>InfoMap is designed to perform natural language understanding .</sentence>
				<definiendum id="0">InfoMap</definiendum>
				<definiens id="0">designed to perform natural language understanding</definiens>
			</definition>
			<definition id="1">
				<sentence>Gruber defines an ontology to be a description of concepts and relationships ( Gruber 1993 ) .</sentence>
				<definiendum id="0">Gruber</definiendum>
			</definition>
			<definition id="2">
				<sentence>InfoMap provides the knowledge necessary for understanding natural language related to a certain knowledge domain .</sentence>
				<definiendum id="0">InfoMap</definiendum>
				<definiens id="0">provides the knowledge necessary for understanding natural language related to a certain knowledge domain</definiens>
			</definition>
			<definition id="3">
				<sentence>InfoMap consists of domain concepts and their associated attributes , activities , etc. , which are its related concepts .</sentence>
				<definiendum id="0">InfoMap</definiendum>
				<definiens id="0">consists of domain concepts and their associated attributes , activities , etc. , which are its related concepts</definiens>
			</definition>
			<definition id="4">
				<sentence>Generally speaking , an ontology consists of definitions of concepts , relations and axioms .</sentence>
				<definiendum id="0">ontology</definiendum>
				<definiens id="0">consists of definitions of concepts , relations and axioms</definiens>
			</definition>
			<definition id="5">
				<sentence>Technically , InfoMap matches a natural language sentence to a collection of concept notes .</sentence>
				<definiendum id="0">InfoMap</definiendum>
				<definiens id="0">matches a natural language sentence to a collection of concept notes</definiens>
			</definition>
			<definition id="6">
				<sentence>As described in previous sections , InfoMap consists of two major relations among concepts , i.e. , Taxonomic relations ( category and synonym ) and Non-taxonomic relations ( attribute and event ) .</sentence>
				<definiendum id="0">InfoMap</definiendum>
				<definiens id="0">consists of two major relations among concepts , i.e. , Taxonomic relations ( category and synonym ) and Non-taxonomic relations ( attribute and event )</definiens>
			</definition>
			<definition id="7">
				<sentence>We defined sentence templates , which consists of patterns of keywords and variables , to capture these relations .</sentence>
				<definiendum id="0">sentence templates</definiendum>
				<definiens id="0">consists of patterns of keywords and variables , to capture these relations</definiens>
			</definition>
			<definition id="8">
				<sentence>We use POS tags defined by CKIP ( CKIP 1993 ) , in which Na is the generic noun , Nb is the proper noun , and Nc is the toponym .</sentence>
				<definiendum id="0">Nb</definiendum>
				<definiendum id="1">Nc</definiendum>
				<definiens id="0">in which Na is the generic noun</definiens>
				<definiens id="1">the proper noun , and</definiens>
				<definiens id="2">the toponym</definiens>
			</definition>
			<definition id="9">
				<sentence>An Nc is the name of a place .</sentence>
				<definiendum id="0">Nc</definiendum>
				<definiens id="0">the name of a place</definiens>
			</definition>
</paper>

		<paper id="1067">
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>In Garside , Leech , and McEnery , editors , Corpus Annotation : Linguistic Information from Computer Text Corpora .</sentence>
				<definiendum id="0">Corpus Annotation</definiendum>
				<definiens id="0">Linguistic Information from Computer Text Corpora</definiens>
			</definition>
</paper>

		<paper id="1082">
</paper>

		<paper id="1157">
</paper>

		<paper id="1044">
</paper>

		<paper id="1154">
			<definition id="0">
				<sentence>1 0 0.2 0.4 0.6 0.8 1 Precision Diseases &amp; Locations : Recall Dis + Loc + Sym + Other : Locations ( 100k ) Locations ( 26k ) Diseases ( 26k ) Diseases ( 100k ) BYCXCVD9D6CT BDBM C6CPD1CTD7BM CACTCRCPD0D0 DAD7BA C8D6CTCRCXD7CXD3D2 D8CPD2CTD3D9D7D0DDBA BJ CFCT CXD2D8D6D3CSD9CRCT CP CRCPD8CTCVD3D6DD CUD3D6 D7DDD1D4B9 D8D3D1D7B8 CSCXD7CRD9D7D7CTCS CXD2 D8CWCT D2CTDCD8 D7CTCRD8CXD3D2BA CFCT CPD0D7D3 CXD2D8D6D3CSD9CRCT CP D2CTCVCPD8CXDACT CRCPD8CTCVD3D6DD CUD3D6 D0CTCPD6D2CXD2CV D8CTD6D1D7 CQCTD0D3D2CVCXD2CV D8D3 D2D3D2CT D3CU D8CWCT CRD0CPD7D7CTD7BA BTD7 D7CTCTCSD7B8 DBCT D9D7CT D8CWCT BDBC D1D3D7D8 CUD6CTD5D9CTD2D8 C6BZD7 CXD2 D8CWCT CRD3D6D4D9D7B8 CTDCCRD0D9CSCXD2CV CSCXD7CTCPD7CT CPD2CS D0D3CRCPD8CXD3D2 D2CPD1CTD7B8 CPD2CS CVCTD2CTD6CXCR DBD3D6CSD7 CUD3D6 CSCXD7CTCPD7CTD7 D3D6 D0D3CRCPB9 D8CXD3D2D7 B4CKDACXD6D9D7B8AY CKD3D9D8CQD6CTCPCZB8AY CKCPD6CTCPAYB5BA BK CCCWCT D4CPD6CPD1CTD8CTD6D7 CXD2 D8CWCTD7CT CTDCD4CTD6CXD1CTD2D8D7 CPD6CTBM D2D9D1CQCTD6 D3CU D7CTCTCSD7 BP BDBC D4CTD6 CRCPD8CTCVD3D6DDBN D4CPD8D8CTD6D2 CPCRB9 CRD9D6CPCRDD D8CWD6CTD7CWD3D0CS AI D4D6CTCR BP BCBMBKBCBN D2 BP D1 BP BH CUD3D6 D8CWCT D2D9D1CQCTD6 D3CU D6CTD8CPCXD2CTCS D4CPD8D8CTD6D2D7 CPD2CS CRCPD2CSCXCSCPD8CTD7BA CCCWCT D0CTCPD6D2CXD2CV CRD9D6DACTD7 CXD2 BYCXCVD9D6CT BD D7CWD3DBCWD3DBD6CTB9 CRCPD0D0 CPD2CS D4D6CTCRCXD7CXD3D2CUD3D6 CSCXD7CTCPD7CTD7 CPD2CS D0D3CRCPD8CXD3D2D7 DACPD6DD CPCRD6D3D7D7 D8CWCT CXD8CTD6CPD8CXD3D2D7BA CCCWCT CQD3D0CS CRD9D6DACTD7 D7CWD3DBD8CWCT D6CTD7D9D0D8 CUD3D6 CSCXD7CTCPD7CTD7 CPD2CS D0D3CRCPD8CXD3D2D7 D3D2 D8CWCT CSCTDACTD0B9 D3D4D1CTD2D8 CRD3D6D4D9D7 B4BEBIC3B5BN CTBACVBAB8 CQDD D8CWCT CTD2CSB8 BJBCB1 D3CU CSCXD7CTCPD7CTD7 B4CUD6D3D1 D8CWCT D6CTCRCPD0D0 D0CXD7D8 D3CU BFBEBE CXD8CTD1D7B5 DBCTD6CT D0CTCPD6D2CTCSB8 CPD8 BHBCB1 D4D6CTCRCXD7CXD3D2DGCWCPD0CU D3CU D8CWCT D0CTCPD6D2CTCS D2CPD1CTD7 DBCTD6CT D2D3D8 D3D2 D8CWCT D4D6CTCRCXD7CXD3D2 D0CXD7D8BA C7D2 D8CWCT BDBCBCC3 CRD3D6D4D9D7 B4DBCXD8CW BIBGBD CSCXD7CTCPD7CTD7 D3D2 D8CWCT D6CTCRCPD0D0 D0CXD7D8B5 D8CWCT D4D6CTCRCXD7CXD3D2 DBCPD7 D3D2D0DD D7D0CXCVCWD8D0DD D0D3DBCTD6BA CCCWCT D4D6CTCRCXD7CXD3D2 D1CTCPD7D9D6CTD7B8 CWD3DBCTDACTD6B8 CPD6CT D9D2CSCTD6B9 D7D8CPD8CTCSBA BUCTCRCPD9D7CT CXD8 CXD7D2D3D8 D4D3D7D7CXCQD0CTD8D3 CVCTD8 CP CUD9D0D0D0CXD7D8 CUD3D6 D1CTCPD7D9D6CXD2CV D4D6CTCRCXD7CXD3D2B8 DBCT ACD2CS D8CWCPD8 C6D3D1CTD2 CXD7 D4CTD2CPD0CXDECTCS CUD3D6 ACD2CSCXD2CV CRD3D6D6CTCRD8 CPD2D7DBCTD6D7BA CCCWCXD7 CXD7 CP CVCTD2CTD6CPD0 D4D6D3CQD0CTD1 D3CU D8DDD4CTB9CQCPD7CTCS CTDACPD0D9CPD8CXD3D2BA CCD3D5D9CPD2D8CXCUDDD8CWCXD7CTABCTCRD8B8 DBCT D1CPD2D9CPD0D0DDCTDCCPD1CXD2CTCS D8CWCT CSCXD7CTCPD7CT D2CPD1CTD7 D0CTCPD6D2CTCS CQDD C6D3D1CTD2 D3D2 D8CWCT CSCTB9 DACTD0D3D4D1CTD2D8 CRD3D6D4D9D7 CPD2CS D6CTB9CXD2D8D6D3CSD9CRCTCS D8CWD3D7CT D8CWCPD8 BJ C4D3CRCPD8CXD3D2D7 D7CTCTCSD7BM CDD2CXD8CTCS CBD8CPD8CTD7B8 C5CPD0CPDDD7CXCPB8 BTD9D7D8D6CPD0CXCPB8 BUCTD0CVCXD9D1B8 BVCWCXD2CPB8 BXD9D6D3D4CTB8 CCCPCXDBCPD2B8 C0D3D2CV C3D3D2CVB8 CBCXD2CVCPB9 D4D3D6CTB8 BYD6CPD2CRCTBA BK CCCWCT D2CTCVCPD8CXDACT D7CTCTCSD7 DBCTD6CTBM CRCPD7CTB8 CWCTCPD0D8CWB8 CSCPDDB8 D4CTD3D4D0CTB8 DDCTCPD6B8 D4CPD8CXCTD2D8B8 CSCTCPD8CWB8 D2D9D1CQCTD6B8 D6CTD4D3D6D8B8 CUCPD6D1BA 0 1 0 0.2 0.4 0.6 0.8 1 Precision Disease Names : Recall Dis + Loc + Sym + Other Diseases ( 26K ) , as Figure 1 Enhanced precision list BYCXCVD9D6CT BEBM BXABCTCRD8 D3CU CDD2CSCTD6D7D8CPD8CTCS C8D6CTCRCXD7CXD3D2 DBCTD6CT CXD2CRD3D6D6CTCRD8D0DD D1CPD6CZCTCS CPD7 CTD6D6D3D6D7B8 CXD2D8D3 D8CWCT D4D6CTB9 CRCXD7CXD3D2 D0CXD7D8 D3D2D0DDBA CCCWCT D9D4CSCPD8CTCS CVD6CPD4CW CXD7 D7CWD3DBD2 CXD2 BYCXCVD9D6CT BEBN CPD8 BJBCB1 D6CTCRCPD0D0 D8CWCT D8D6D9CT D4D6CTCRCXD7CXD3D2CXD7 BIBHB1BA C6D3D8CT D8CWCPD8 D4D6CTCRCXD7CXD3D2 CXD7 D7CXD1CXD0CPD6D0DD D9D2CSCTD6D7D8CPD8CTCS CUD3D6 CPD0D0 D8DDD4CTB9CQCPD7CTCS CRD9D6DACTD7 CXD2 D8CWCXD7 D4CPD4CTD6BA BTD1D3D2CV D8CWCT D6CTB9CXD2D8D6D3CSD9CRCTCS D2CPD1CTD7 D8CWCTD6CT DBCTD6CT BLBL D2CTDB CSCXD7CTCPD7CTD7 DBCWCXCRCWDBCTD6CT D1CXD7D7CTCS CXD2 D8CWCT D1CPD2B9 D9CPD0 CRD3D1D4CXD0CPD8CXD3D2 D3CU D6CTCUCTD6CTD2CRCT D0CXD7D8D7BA BL CCCWCXD7 CXD7 CPD2 CTD2CRD3D9D6CPCVCXD2CV D6CTD7D9D0D8B8 D7CXD2CRCT D8CWCXD7 CXD7 D9D0D8CXD1CPD8CTD0DD CWD3DB C6D3D1CTD2 CXD7 CXD2D8CTD2CSCTCS D8D3 CQCT D9D7CTCSBM CUD3D6 CSCXD7CRD3DACTD6CXD2CV D2CTDBB8 D4D6CTDACXD3D9D7D0DD D9D2CZD2D3DBD2 D2CPD1CTD7BA BH BWCXD7CRD9D7D7CXD3D2 BHBABD BVD3D1D4CTD8CXD2CV BVCPD8CTCVD3D6CXCTD7 BYCXCVD9D6CT BF CSCTD1D3D2D7D8D6CPD8CTD7 D8CWCT D9D7CTCUD9D0D2CTD7D7 D3CU CRD3D1B9 D4CTD8CXD8CXD3D2 CPD1D3D2CV D8CPD6CVCTD8 CRCPD8CTCVD3D6CXCTD7BA BTD0D0 CRD9D6DACTD7 D7CWD3DB D8CWCT D4CTD6CUD3D6D1CPD2CRCT D3CU C6D3D1CTD2 D3D2 D8CWCT CSCXD7B9 CTCPD7CT CRCPD8CTCVD3D6DDB8 DBCWCTD2 D8CWCT CPD0CVD3D6CXD8CWD1 CXD7 D7CTCTCSCTCS D3D2D0DD DBCXD8CW CSCXD7CTCPD7CTD7 B4D8CWCT CRD9D6DACT D0CPCQCTD0CTCS BWCXD7B5B8 DBCWCTD2 D7CTCTCSCTCS DBCXD8CW CSCXD7CTCPD7CTD7 CPD2CS D0D3CRCPD8CXD3D2D7 B4BWCXD7B7C4D3CRB5B8 CPD2CS DBCXD8CW D7DDD1D4D8D3D1D7B8 CPD2CS D8CWCT CKD3D8CWCTD6AY CRCPD8CTCVD3D6DDBA CCCWCT CRD9D6DACTD7 BWCXD7 CPD2CS BWCXD7B7C4D3CR CPD6CT DACTD6DD D7CXD1CXD0CPD6BA C0D3DBCTDACTD6B8 DBCWCTD2 D1D3D6CT CRCPD8CTCVD3D6CXCTD7 CPD6CT CPCSCSCTCSB8 D4D6CTB9 CRCXD7CXD3D2 CPD2CS D6CTCRCPD0D0 CXD2CRD6CTCPD7CT CSD6CPD1CPD8CXCRCPD0D0DDBA CFCWCTD2 D3D2D0DD D3D2CT CRCPD8CTCVD3D6DD CXD7 CQCTCXD2CV D0CTCPD6D2CTCSB8 CPCRCRB4D4B5 BP BDBMBC CUD3D6 CPD0D0 D4CPD8D8CTD6D2D7 D4BA CCCWCT D0CPCRCZ D3CU CPD2 CTABCTCRD8CXDACT CPCRCRD9D6CPCRDD D1CTCPD7D9D6CT CRCPD9D7CTD7 D9D7 D8D3 CPCRB9 D5D9CXD6CT D9D2D7CTD0CTCRD8CXDACT CSCXD7CTCPD7CT D2CPD1CT D4CPD8D8CTD6D2D7 D8CWCPD8 D3CUB9 D8CTD2 CPD0D7D3 D1CPD8CRCW D2D3D2B9CSCXD7CTCPD7CTD7 B4CTBACVBAB8 CKBABABA CG CWCPD7 CQCTCTD2 CRD3D2ACD6D1CTCSAYB5BA CCCWCXD7 CWD9D6D8D7 D4D6CTCRCXD7CXD3D2BA BL BXDCCPD1D4D0CTD7 D3CU D2CTDB CSCXD7CTCPD7CTD7BM D6CXD2CSCTD6D4CTD7D8B8 CZD3D2DED3B8 C5CTCSCXD8CTD6D6CPD2CTCPD2 D7D4D3D8D8CTCS CUCTDACTD6B8 CRD3CRD3D2D9D8 CRCPCSCPD2CVB9CRCPCSCPD2CVB8 D7DBCPD1D4 CUCTDACTD6B8 D0CPD8CWDDD6CXD7D1B8 C8CACACB B4CUD3D6 CKD4D3D6CRCXD2CT D6CTD4D6D3CSD9CRB9 D8CXDACT CPD2CS D6CTD7D4CXD6CPD8D3D6DD D7DDD2CSD6D3D1CTAYB5BN D0D3CRCPD8CXD3D2D7BM C3CXD2D8CPB8 CDD0D9 C8CXCPCWB8 C5CTD0CXD0D0CPB8 BTD2D7D8D3CWCXCWDDB8 CTD8CRBA 0 1 0 0.2 0.4 0.6 0.8 1 Precision Disease Names : Recall Disease names : Dis + Loc + Sym + Other Dis + Loc + Other Dis + Loc Dis BYCXCVD9D6CT BFBM BWCXD7CTCPD7CTD7BM BXABCTCRD8 D3CU BVD3D1D4CTD8CXD8CXD3D2 CACTCRCPD0D0 CPD0D7D3 D7D9ABCTD6D7B8 B4CPB5 CQCTCRCPD9D7CT D7D3D1CT D4CPD8D8CTD6D2D7 D8CWCPD8 CPD6CT D1D3D6CT D7CTD0CTCRD8CXDACT B4CQD9D8 CWCPDACT D0D3DBCTD6 CRD3D2ACB9 CSCTD2CRCT D3D6 CRD3DACTD6CPCVCTB5 CPD6CT D2CTCVD0CTCRD8CTCSB8 CPD2CS B4CQB5 CQCTB9 CRCPD9D7CT D2D3D2B9CSCXD7CTCPD7CTD7 CRD3D2D8CPD1CXD2CPD8CT D8CWCT D7CTCTCS D7CTD8 CPD2CS CVCTD2CTD6CPD8CT D9D7CTD0CTD7D7 D4CPD8D8CTD6D2D7BA B4BVD3D0D0CXD2D7 CPD2CS CBCXD2CVCTD6B8 BDBLBLBLB5 CPD0D7D3 D1CPCZCTD7 D9D7CT D3CU CRD3D1D4CTD8CXD2CV CRCPD8CTCVD3D6CXCTD7 B4D4CTD6D7D3D2B8 D3D6CVCPD2CXDECPD8CXD3D2B8 CPD2CS D0D3CRCPD8CXD3D2B5B8 DBCWCXCRCWCRD3DACTD6 BLBIB1 D3CU CPD0D0 D8CWCT CXD2D7D8CPD2CRCTD7 CXD8 D7CTD8 D3D9D8 D8D3 CRD0CPD7D7CXCUDDBA C1D2 D3D9D6 CRCPD7CTB8 D8CWCT D7D3D9CVCWD8 CRCPD8B9 CTCVD3D6CXCTD7B8 B4CSCXD7CTCPD7CTD7 CPD2CS D0D3CRCPD8CXD3D2D7B5B8 CSD3 D2D3D8 CRD3DACTD6 D8CWCT CQD9D0CZ D3CU D4D3D8CTD2D8CXCPD0 CRCPD2CSCXCSCPD8CTD7 CUD3D6 CVCTD2CTD6CPD0CXDECTCS D2CPD1CTD7DGDBD3D6CS D7CTD5D9CTD2CRCTD7 D1CPD8CRCWCXD2CV CJBTBWC2B6 C6B7CLBA C1D2D8D6D3CSD9CRCXD2CV D8CWCT CKD2CTCVCPD8CXDACTAY CRCPD8CTCVD3D6DD CWCTD0D4D7 D9D7 CRD3DACTD6 D1D3D6CT D3CU D8CWCT D4D3D8CTD2D8CXCPD0 CRCPD2CSCXCSCPD8CTD7BA CCCWCXD7 CXD2 D8D9D6D2 CQD3D3D7D8D7 D8CWCT D9D8CXD0CXD8DD D3CU D8CWCT CPCRCRD9D6CPCRDD D1CTCPD7D9D6CTBA BTCSCSCXD8CXD3D2CPD0 CRD3D1D4CTD8CXD2CV CRCPD8CTCVD3D6CXCTD7 D1CPDDCWCTD0D4D8D3 D4D6CTDACTD2D8 CP CRCPD8CTCVD3D6DD CUD6D3D1 CKCRD6CTCTD4CXD2CVAY CXD2D8D3 CPD2 D3DACTD6B9 D0CPD4D4CXD2CV CRD3D2CRCTD4D8BA BXBACVBAB8 DBCT CWCPCS D1CTD2D8CXD3D2CTCS D8CWCPD8 D8CWCT CSCXD7CTCPD7CT CPD2CS D7DDD1D4D8D3D1 CRD0CPD7D7CTD7 D1CPDD D3DACTD6D0CPD4BA CFCWCTD2 D8CWCT D8CPD6CVCTD8 CRCPD8CTCVD3D6CXCTD7 CXD2CRD0D9CSCT CSCXD7CTCPD7CTD7 CQD9D8 D2D3D8 D7DDD1D4D8D3D1D7B8 C6D3D1CTD2 D0CTCPD6D2D7 D7D3D1CT D2CPD1CTD7 D8CWCPD8 CRCPD2 CUD9D2CRD8CXD3D2 CPD7 CTCXD8CWCTD6BA CCCWCXD7 D0CTCPCSD7 D8D3 D0CTCPD6D2CXD2CV D3CU D7D3D1CT D4CPD8D8CTD6D2D7 DBCWCXCRCW D8CTD2CS D8D3 D3CRCRD9D6 DBCXD8CW D7DDD1D4B9 D8D3D1D7 D3D2D0DDB8 D6CTD7D9D0D8CXD2CVCXD2 D4D6CTCRCXD7CXD3D2CTD6D6D3D6D7BA BYCXCVD9D6CT BF D7CWD3DBD7 D8CWCT CXD1D4D6D3DACTD1CTD2D8 CXD2D4D6CTCRCXD7CXD3D2CUD6D3D1CPCSCSCXD2CV D8CWCT D7DDD1D4D8D3D1 CRCPD8CTCVD3D6DDBA C7D2 D8CWCT D3D8CWCTD6 CWCPD2CSB8 D8CWCTD6CT D1CPDD CQCT CSCXD7CPCSDACPD2B9 D8CPCVCTD7 D8D3 D7D4D0CXD8D8CXD2CV CRCPD8CTCVD3D6CXCTD7 D8D3D3 ACD2CTD0DDBA BYD3D6 CTDCB9 CPD1D4D0CTB8 D3D2CT D4D6D3CQD0CTD1 CXD7 D1CTD8D3D2DDD1DD CPD1D3D2CV CRD0CPD7D7CTD7 D3CU CVCTD2CTD6CPD0CXDECTCS D2CPD1CTD7BA C1D8 CPD4D4CTCPD6D7 D8D3 CQCT CSCXD7D8CXD2CRD8 CUD6D3D1 D8CWCT D4D6D3CQD0CTD1 D3CU CPD1CQCXCVD9CXD8DD CXD2 C8C6D7B8 CTBACVBAB8 DBCWCTD2 CKCFCPD7CWCXD2CVD8D3D2AY D1CPDD D6CTCUCTD6 D8D3 CP D4CTD6D7D3D2B8 D3D6 CP D0D3CRCPD8CXD3D2BA C1D2 D8CWCT CRCPD7CT D3CU C8C6D7B8 D8CWCTD6CT CPD6CT D9D7D9CPD0D0DD CRD0D9CTD7 CXD2 D8CWCT CRD3D2D8CTDCD8 D8D3 CPABD3D6CS CSCXD7CPD1CQCXCVD9CPD8CXD3D2BA 0 1 0 0.2 0.4 0.6 0.8 1 Precision Recall Dis+Sym+Loc+Other : Locations ( 100k ) Locations ( 26k ) Diseases ( 26k ) Diseases ( 100k ) BYCXCVD9D6CT BGBM CCD3CZCTD2B9CQCPD7CTCSB8 C5CDBVB9D7D8DDD0CT BXDACPD0D9CPD8CXD3D2 C1D2 D8CWCT CRCPD7CT D3CU BZC6D7B8 D6CPD8CWCTD6B8 D8CWCT D2CPD8D9D6CT D3CU CPD1CQCXB9 CVD9CXD8DDD1CPDD CQCT D6CTD0CPD8CTCS D8D3 D6CTCVD9D0CPD6 D1CTD8D3D2DDD1DDBA BYD3D6 CTDCCPD1D4D0CTB8 D2CPD1CTD7 D3CU CPCVCTD2D8D7 D6CTCVD9D0CPD6D0DD CUD9D2CRD8CXD3D2 CPD7 D8CWCT D2CPD1CT D3CU D8CWCT CSCXD7CTCPD7CT D8CWCTDD CRCPD9D7CTBM CKBXBA CRD3D0CXBAAY CCCWCTD6CTCUD3D6CTB8 CXD2 D0CTCPD6D2CXD2CV CPCVCTD2D8D7 CPD2CS CSCXD7CTCPD7CTD7 D7CTD4CPB9 D6CPD8CTD0DDB8 D8CWCT CPD0CVD3D6CXD8CWD1DBCXD0D0D2CPD8D9D6CPD0D0DDCRD3D2CUD3D9D2CSD8CWCT D8DBD3 CRD0CPD7D7CTD7B8 DBCWCXCRCW DBCXD0D0 CXD2CWCXCQCXD8 D0CTCPD6D2CXD2CVBA C1D2 D8CWCTD7CT CTDCD4CTD6CXD1CTD2D8D7B8 DBCT D0CTCPD6D2 D8CWCTD1 CPD7 CP D7CXD2CVD0CT CRD0CPD7D7BA C1D8 D1CPDD D8CWCTD2 CQCT D1D3D6CT CPD4D4D6D3D4D6CXCPD8CT D8D3 CPD4D4D0DD CPD2B9 D3D8CWCTD6 D4D6D3CRCTCSD9D6CT D8D3 D7CTD4CPD6CPD8CT D8CWCT CRD0CPD7D7CTD7 CQCPD7CTCS D3D2 CP D1CTCPD7D9D6CT D3CU D4D6CTDACPD0CTD2CRCT D3CU CRD3B9D3CRCRD9D6D6CTD2CRCT DBCXD8CW D8CWCT D6CTD7D4CTCRD8CXDACTD0DD CRCWCPD6CPCRD8CTD6CXD7D8CXCR CRD3D2D8CTDCD8D7BA BHBABE BXDACPD0D9CPD8CXD3D2 CCCWCT D6CTD7D9D0D8D7 CXD2 D8CWCT D4D6CTCRCTCSCXD2CV ACCVD9D6CTD7 CPD6CT D2D3D8 CSCXB9 D6CTCRD8D0DD CRD3D1D1CTD2D7D9D6CPD8CT DBCXD8CW D8CWD3D7CT CXD2 D8CWCT D1CTD2B9 D8CXD3D2CTCS D0CXD8CTD6CPD8D9D6CTB8 CTBACVBAB8 B4CBD8D6DECPD0CZD3DBD7CZCX CPD2CS CFCPD2CVB8 BDBLBLBIBN BVD3D0D0CXD2D7 CPD2CS CBCXD2CVCTD6B8 BDBLBLBLB5BA CCCWCXD7 D6CTD0CPD8CTD7 D8D3 D8CWCT D8D3CZCTD2B9D8DDD4CT CSCXCRCWD3D8D3D1DDBA CCCWCT CTDACPD0D9CPD8CXD3D2 CXD2 D8CWCT D4D6CXD3D6 DBD3D6CZ CXD7 D8D3CZCTD2B9 CQCPD7CTCSB8 DBCWCTD6CT D8CWCT D0CTCPD6D2CTD6 CVCTD8D7 CRD6CTCSCXD8DGD6CTCRCPD0D0 D4D3CXD2D8D7DGCUD3D6 CXCSCTD2D8CXCUDDCXD2CV CPD2 CXD2D7D8CPD2CRCT CRD3D6D6CTCRD8D0DDB8 CUD3D6 CTDACTD6DD D8CXD1CT CXD8 D3CRCRD9D6D7 CXD2 D8CWCT CRD3D6D4D9D7BA C1D2 D3D9D6 D8DDD4CTB9 CQCPD7CTCS CTDACPD0D9CPD8CXD3D2B8 CXD8 CVCTD8D7 CRD6CTCSCXD8 D3D2D0DD D3D2CRCT D4CTD6 D2CPD1CTB8 D2D3 D1CPD8D8CTD6 CWD3DBD1CPD2DD D8CXD1CTD7 CXD8 D3CRCRD9D6D7BA CFCT CPD0D7D3 CRD3D2CSD9CRD8CTCS CPD2 CXD2D7D8CPD2CRCTB9CQCPD7CTCS CTDACPD0D9CPB9 D8CXD3D2B8 D1D3D6CT CRD3D1D4CPD8CXCQD0CT DBCXD8CW D8CWCT D1CTD2D8CXD3D2CTCS D4D6CXD3D6 DBD3D6CZBA CFCT D1CPD2D9CPD0D0DD D8CPCVCVCTCS CPD0D0 CSCXD7CTCPD7CTD7 CPD2CS D0D3B9 CRCPD8CXD3D2D7 CXD2 CP BHBCBCB9D7CTD2D8CTD2CRCT D8CTD7D8 D7D9CQB9CRD3D6D4D9D7BA CDD7CXD2CV D8CWCT D3D9D8D4D9D8 CUD6D3D1 D8CWCT D6D9D2D7 CXD2 BYCXCVD9D6CT BD DBCT D1CTCPB9 D7D9D6CTCS D6CTCRCPD0D0 CPD2CS D4D6CTCRCXD7CXD3D2 D9D7CXD2CV D8CWCT D7D8CPD2CSCPD6CS C5CDBV C6BX D7CRD3D6CXD2CV D7CRCWCTD1CTB8 D7CWD3DBD2 CXD2 BYCXCVD9D6CT BGBA BDBC BDBC CCCWCT D7CWCPD6D4 CSCXD4 CXD2 D8CWCT CKCSCXD7CTCPD7CTD7 B4BDBCBCC3B5AY CRD9D6DACT CXD7 CSD9CT D8D3 D7CTDACTD6CPD0 CVCTD2CTD6CXCR D8CTD6D1D7 D8CWCPD8 DBCTD6CT D0CTCPD6D2CTCS CTCPD6D0DD D3D2BN CVCTD2CTD6CXCRD7 DBCTD6CT D2D3D8 D8CPCVCVCTCS CXD2 D8CWCT D8CTD7D8 CRD3D6D4D9D7BA C1D8CTD6CPD8CXD3D2 CCDDD4CTB9BUCPD7CTCS C1D2D7D8CPD2CRCTB9BUCPD7CTCS BC BCBABCBF BCBABFBH BEBC BCBABDBK BCBABIBK BGBC BCBABFBD BCBABKBH BIBC BCBABGBE BCBABKBH BFBCBC BCBABIBL BCBABKBI CCCPCQD0CT BEBM BXDACPD0D9CPD8CXD3D2 D3CU BWCXD7CTCPD7CT CACTCRCPD0D0 CCCPCQD0CT BE CRD3D2D8D6CPD7D8D7 D8DDD4CTB9CQCPD7CTCS CPD2CS CXD2D7D8CPD2CRCTB9 CQCPD7CTCS D6CTCRCPD0D0 CPCRD6D3D7D7 D8CWCT CXD8CTD6CPD8CXD3D2D7BA CCCWCT CXD2D7D8CPD2CRCTB9 CQCPD7CTCSCTDACPD0D9CPD8CXD3D2 CRCPD2CWCPD6CSD0DDCSCXD7D8CXD2CVD9CXD7CWCQCTD8DBCTCTD2 CPD2 CPD0CVD3D6CXD8CWD1 D8CWCPD8 D0CTCPD6D2D7 BFBDB1 D3CU D8CWCT D8DDD4CTD7 DAD7BA D3D2CT D8CWCPD8 D0CTCPD6D2D7 BIBLB1 D3CU D8CWCT D8DDD4CTD7BA CCCWCT CPD0CVD3D6CXD8CWD1 CZCTCTD4D7 D0CTCPD6D2CXD2CV D0D3D8D7 D3CU D2CTDBB8 CXD2CUD6CTD5D9CTD2D8 D8DDD4CTD7 D9D2D8CXD0 CXD8CTD6CPD8CXD3D2 BFBGBCB8 CQD9D8 D8CWCT CXD2D7D8CPD2CRCTB9CQCPD7CTCS CTDACPD0D9CPD8CXD3D2 CSD3CTD7 D2D3D8 CSCTD1D3D2D7D8D6CPD8CT D8CWCXD7BA BHBABF BVD9D6D6CTD2D8 CFD3D6CZ C6D3D1CTD2 CRCPD2 CQCT CXD1D4D6D3DACTCS CXD2 D7CTDACTD6CPD0 D6CTD7D4CTCRD8D7BA CCCWCT CRD9D6D6CTD2D8 D6CTCVD9D0CPD6B9CTDCD4D6CTD7D7CXD3D2 C6BZ D4CPD8D8CTD6D2 CXD7 DACTD6DD D7CXD1D4D0CXD7D8CXCRBA C1D2 CXD8D7 D4D6CTD7CTD2D8 CUD3D6D1B8 CXD8 CSD3CTD7 D2D3D8 CPD0D0D3DB CKCUD3D3D8 CPD2CS D1D3D9D8CW CSCXD7CTCPD7CTAY D8D3 CQCT D0CTCPD6D2CTCSB8 D2D3D6 CKD0CTCVCXD3D2D2CPCXD6CTD7B3 CSCXD7CTCPD7CTAYBN D8CWCXD7 CXD2D8D6D3CSD9CRCTD7 CXD2B9 CPCRCRD9D6CPCRDDB8 D7CXD2CRCT D4CPD6D8D7 D3CU D8CWCTD7CT D2CPD1CTD7 CPD6CT D0CTCPD6D2CTCS CPD2CS CRD3D2D8CPD1CXD2CPD8CT D8CWCT D4D3D3D0BA CCCWCT CRD9D6D6CTD2D8 D4CPD8D8CTD6D2 CVCTD2CTD6CPD0CXDECPD8CXD3D2 D7CRCWCTD1CT CRD3D9D0CS CQCT CTDCD4CPD2CSCTCSBA B4C4C8B5 BE CVCTD2CTD6CPD0CXDECTD7 D3D2 D7D9D6B9 CUCPCRCT CUD3D6D1B8 CRCPD7CTB8 CPD2CS D7CTD1CPD2D8CXCR CXD2CUD3D6D1CPD8CXD3D2BA CFCT CRD3D9D0CS D9D7CTB8 CTBACVBAB8 D4CPD6D8D7 D3CU D7D4CTCTCRCW CUD6D3D1 D8CWCT D8CPCVCVCTD6B8 CPD7 CP D0CTDACTD0 D3CU CVCTD2CTD6CPD0CXDECPD8CXD3D2 CQCTD8DBCTCTD2 D0CTD1D1CPD7 CPD2CS DBCXD0CSCRCPD6CSD7BA BT CRD3D1D4D0CTD1CTD2D8CPD6DD CPD4D4D6D3CPCRCW DBD3D9D0CS CQCT D8D3 D9D7CT CP C6C8 CRCWD9D2CZCTD6B8 D8D3 CRCPD4D8D9D6CT D0D3D2CVCTD6B9 CSCXD7D8CPD2CRCT D6CTD0CPD8CXD3D2D7B8 CXD2 D8CWCT CWCTCPCSD7 CPD2CS D4D6CTD4D3D7CXD8CXD3D2D7 D3CU CPCSCYCPCRCTD2D8 D4CWD6CPD7CTD7BA B4B4BVD3D0D0CXD2D7 CPD2CS CBCXD2CVCTD6B8 BDBLBLBLB5 CPCRCWCXCTDACTD7 D8CWCXD7 CTABCTCRD8 CQDD CUD9D0D0 D4CPD6D7CXD2CVBAB5 CFCT CPD6CT CTDCD4D0D3D6CXD2CV CPCRD5D9CXD7CXD8CXD3D2 D3CU D1D3D6CT D8DDD4CTD7 D3CU CVCTD2CTD6CPD0CXDECTCS D2CPD1CTD7DGCPCVCTD2D8D7 CPD2CS DACTCRD8D3D6D7B8 CPD7 DBCTD0D0 CPD7 D4CTD3D4D0CT CPD2CS D3D6CVCPD2CXDECPD8CXD3D2D7BA CFCWCPD8 CXD7 D8CWCT CTABCTCRD8 D3CU D0CTCPD6D2CXD2CV D4D3D7D7CXCQD0DD D6CTD0CPD8CTCS CRD0CPD7D7CTD7 D7CXD1D9D0D8CPD2CTB9 D3D9D7D0DDB8 DBCWCPD8 CWCPD4D4CTD2D7 D8D3 D8CWCT CXD8CTD1D7 CXD2 D8CWCTCXD6 CXD2D8CTD6B9 D7CTCRD8CXD3D2B8 CPD2CS D8D3 DBCWCPD8 CTDCD8CTD2D8 D8CWCTDD CXD2CWCXCQCXD8 D0CTCPD6D2B9 CXD2CVB8 D6CTD1CPCXD2D7 CP D4D6CPCRD8CXCRCPD0 D5D9CTD7D8CXD3D2BA BTCRCZD2D3DBD0CTCSCVCTD1CTD2D8D7 CCCWCXD7 D6CTD7CTCPD6CRCW CXD7 D7D9D4D4D3D6D8CTCS CQDD D8CWCT BWCTCUCTD2D7CT BTCSDACPD2CRCTCS CACTD7CTCPD6CRCW C8D6D3CYCTCRD8D7 BTCVCTD2CRDD CPD7 D4CPD6D8 D3CU D8CWCT CCD6CPD2D7D0CXD2B9 CVD9CPD0 C1D2CUD3D6D1CPD8CXD3D2 BWCTD8CTCRD8CXD3D2B8 BXDCD8D6CPCRD8CXD3D2 CPD2CS CBD9D1B9 D1CPD6CXDECPD8CXD3D2 B4CCC1BWBXCBB5 D4D6D3CVD6CPD1B8 D9D2CSCTD6 BZD6CPD2D8 C6BIBIBCBCBDB9 BCBCBDB9BDB9BKBLBDBJ CUD6D3D1 D8CWCT CBD4CPCRCT CPD2CS C6CPDACPD0 CFCPD6CUCPD6CT CBDDD7B9 D8CTD1D7 BVCTD2D8CTD6 CBCPD2 BWCXCTCVD3B8 CPD2CS CQDD D8CWCT C6CPD8CXD3D2CPD0 CBCRCXCTD2CRCT BYD3D9D2CSCPD8CXD3D2 D9D2CSCTD6 BZD6CPD2D8 C1C1CBB9BCBCBKBDBLBIBEBA CACTCUCTD6CTD2CRCTD7 BWBA BUCXCZCTD0B8 CBBA C5CXD0D0CTD6B8 CABA CBCRCWDBCPD6D8DEB8 CPD2CS CABA CFCTCXD7CRCWCTCSCTD0BA BDBLBLBJBA C6DDD1CQD0CTBM CP CWCXCVCWB9D4CTD6CUD3D6D1CPD2CRCT D0CTCPD6D2CXD2CV D2CPD1CTB9ACD2CSCTD6BA C1D2 C8D6D3CRBA BHD8CW BTD4D4D0CXCTCS C6CPD8D9D6CPD0 C4CPD2B9 CVD9CPCVCT C8D6D3CRCTD7D7CXD2CV BVD3D2CUBAB8CFCPD7CWCXD2CVD8D3D2B8 BWBVBA BTBA BUD3D6D8CWDBCXCRCZB8 C2BA CBD8CTD6D0CXD2CVB8 BXBA BTCVCXCRCWD8CTCXD2B8 CPD2CS CABA BZD6B9 CXD7CWD1CPD2BA BDBLBLBKBA BXDCD4D0D3CXD8CXD2CV CSCXDACTD6D7CT CZD2D3DBD0CTCSCVCT D7D3D9D6CRCTD7 DACXCP D1CPDCCXD1D9D1 CTD2D8D6D3D4DD CXD2 D2CPD1CTCS CTD2D8CXD8DD D6CTCRD3CVD2CXD8CXD3D2BA C1D2 C8D6D3CRBA BID8CW CFD3D6CZD7CWD3D4 D3D2 CECTD6DD C4CPD6CVCT BVD3D6D4D3D6CPB8C5D3D2D8D6CTCPD0B8 BVCPD2CPCSCPBA BYBA BVCXD6CPDACTCVD2CPBA BEBCBCBDBA BTCSCPD4D8CXDACT CXD2CUD3D6D1CPD8CXD3D2 CTDCD8D6CPCRB9 D8CXD3D2 CUD6D3D1 D8CTDCD8 CQDD D6D9D0CT CXD2CSD9CRD8CXD3D2 CPD2CS CVCTD2CTD6CPD0CXD7CPB9 D8CXD3D2BA C1D2 C8D6D3CRBA BDBJD8CW C1D2D8D0BA C2D3CXD2D8 BVD3D2CUBA D3D2 BTC1 B4C1C2BVBTC1 BEBCBCBDB5B8 CBCTCPD8D8D0CTB8 CFBTBA C5BA BVD3D0D0CXD2D7 CPD2CS CHBA CBCXD2CVCTD6BA BDBLBLBLBA CDD2D7D9D4CTD6DACXD7CTCS D1D3CSB9 CTD0D7 CUD3D6 D2CPD1CTCS CTD2D8CXD8DD CRD0CPD7D7CXACCRCPD8CXD3D2BA C1D2 C8D6D3CRBA C2D3CXD2D8 CBC1BZBWBTCC BVD3D2CUBA D3D2 BXC5C6C4C8BBCEC4BVBA CBBA BVD9CRCTD6DECPD2 CPD2CS BWBA CHCPD6D3DBD7CZDDBA BDBLBLBLBA C4CPD2CVD9CPCVCT CXD2B9 CSCTD4CTD2CSCTD2D8 D2CPD1CTCS CTD2D8CXD8DD D6CTCRD3CVD2CXD8CXD3D2 CRD3D1CQCXD2CXD2CV D1D3D6D4CWD3D0D3CVCXCRCPD0 CPD2CS CRD3D2D8CTDCD8D9CPD0 CTDACXCSCTD2CRCTBA C1D2 C8D6D3CRBA C2D3CXD2D8 CBC1BZBWBTCC BVD3D2CUBA D3D2 BXC5C6C4C8BBCEC4BVBA C3BA BYD6CPD2D8DECXB8 CBBA BTD2CPD2CXCPCSD3D9B8 CPD2CS C0BA C5CXD1CPBA BEBCBCBCBA BTD9B9 D8D3D1CPD8CXCR D6CTCRD3CVD2CXD8CXD3D2 D3CU D1D9D0D8CXB9DBD3D6CS D8CTD6D1D7BM D8CWCT BVB9 DACPD0D9CTBBC6BVB9DACPD0D9CT D1CTD8CWD3CSBA C1D2D8D0BA C2D3D9D6D2CPD0 D3D2 BWCXCVCXD8CPD0 C4CXCQD6CPD6CXCTD7B8 BEBCBCBCB4BFB5BMBDBDBHDFBDBFBCBA CABA BZD6CXD7CWD1CPD2B8 CBBA C0D9D8D8D9D2CTD2B8 CPD2CS CABA CHCPD2CVCPD6CQCTD6BA BEBCBCBEBA BXDACTD2D8 CTDCD8D6CPCRD8CXD3D2 CUD3D6 CXD2CUCTCRD8CXD3D9D7 CSCXD7CTCPD7CT D3D9D8CQD6CTCPCZD7BA C1D2 C8D6D3CRBA BED2CS C0D9D1CPD2 C4CPD2CVBA CCCTCRCWD2D3D0D3CVDD BVD3D2CUBA B4C0C4CCBEBCBCBEB5B8 CBCPD2 BWCXCTCVD3B8 BVBTBA C2BACBBA C2D9D7D8CTD7D3D2 CPD2CS CBBAC5BA C3CPD8DEBA BDBLBLBHBA CCCTCRCWD2CXCRCPD0 D8CTD6B9 D1CXD2D3D0D3CVDDBM CBD3D1CT D0CXD2CVD9CXD7D8CXCR D4D6D3D4CTD6D8CXCTD7 CPD2CS CPD2 CPD0CVD3B9 D6CXD8CWD1 CUD3D6 CXCSCTD2D8CXACCRCPD8CXD3D2 CXD2 D8CTDCD8BA C6CPD8D9D6CPD0 C4CPD2CVD9CPCVCT BXD2CVCXD2CTCTD6CXD2CVB8 BDB4BDB5BMBLDFBEBJBA BXBA CACXD0D3AB CPD2CS CABA C2D3D2CTD7BA BDBLBLBLBA C4CTCPD6D2CXD2CV CSCXCRD8CXD3B9 D2CPD6CXCTD7 CUD3D6 CXD2CUD3D6D1CPD8CXD3D2 CTDCD8D6CPCRD8CXD3D2 CQDD D1D9D0D8CXB9D0CTDACTD0 CQD3D3D8D7D8D6CPD4D4CXD2CVBA C1D2 C8D6D3CRBA BDBID8CW C6CPD8D0BA BVD3D2CUBA D3D2 BTC1 B4BTBTBTC1B9BLBLB5B8 C7D6D0CPD2CSD3B8 BYC4BA CCBA CBD8D6DECPD0CZD3DBD7CZCX CPD2CS C2BA CFCPD2CVBA BDBLBLBIBA BT D7CTD0CUB9D0CTCPD6D2CXD2CV D9D2CXDACTD6D7CPD0 CRD3D2CRCTD4D8 D7D4D3D8D8CTD6BA C1D2 C8D6D3CRBA BDBID8CW C1D2D8D0BA BVD3D2CUBA BVD3D1D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7 B4BVC7C4C1C6BZB9BLBIB5BA BTBA CDD7CWCXD3CSCPBA BDBLBLBIBA C0CXCTD6CPD6CRCWCXCRCPD0 CRD0D9D7D8CTD6CXD2CV D3CU DBD3D6CSD7BA C1D2 C8D6D3CRBA BDBID8CW C1D2D8D0BA BVD3D2CUBA BVD3D1D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7B9 D8CXCRD7 B4BVC7C4C1C6BZB9BLBIB5B8 BVD3D4CTD2CWCPCVCTD2B8 BWCTD2D1CPD6CZBA CCBA CFCPCZCPD3B8 CABA BZCPCXDECPD9D7CZCPD7B8 CPD2CS CHBA CFCXD0CZD7BA BDBLBLBIBA BXDACPD0D9CPD8CXD3D2 D3CU CPD2 CPD0CVD3D6CXD8CWD1 CUD3D6 D8CWCT D6CTCRD3CVD2CXD8CXD3D2 CPD2CS CRD0CPD7D7CXACCRCPD8CXD3D2 D3CU D4D6D3D4CTD6 D2CPD1CTD7BA C1D2 C8D6D3CRBA BDBID8CW C1D2D8B3D0 BVD3D2CUBA D3D2 BVD3D1D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7 B4BVC7C4B9 C1C6BZ BLBIB5B8 BVD3D4CTD2CWCPCVCTD2B8 BWCTD2D1CPD6CZBA CABA CHCPD2CVCPD6CQCTD6B8 CABA BZD6CXD7CWD1CPD2B8 C8BA CCCPD4CPD2CPCXD2CTD2B8 CPD2CS CBBA C0D9D8D8D9D2CTD2BA BEBCBCBCBA BTD9D8D3D1CPD8CXCR CPCRD5D9CXD7CXD8CXD3D2 D3CU CSD3B9 D1CPCXD2 CZD2D3DBD0CTCSCVCT CUD3D6 CXD2CUD3D6D1CPD8CXD3D2 CTDCD8D6CPCRD8CXD3D2BA C1D2 C8D6D3CRBA BDBKD8CW C1D2D8D0BA BVD3D2CUBA BVD3D1D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7 B4BVC7C4C1C6BZ BEBCBCBCB5B8 CBCPCPD6CQD6DJD9CRCZCTD2B8 BZCTD6D1CPD2DDBA CABA CHCPD2CVCPD6CQCTD6BA BEBCBCBEBA BTCRD5D9CXD7CXD8CXD3D2 D3CU CSD3D1CPCXD2 CZD2D3DBD0B9 CTCSCVCTBA C1D2 C5BACCBA C8CPDECXCTD2DECPB8 CTCSCXD8D3D6B8 C1D2CUD3D6D1CPD8CXD3D2 BXDCB9 D8D6CPCRD8CXD3D2BA CBD4D6CXD2CVCTD6B9CECTD6D0CPCVB8 C4C6BTC1B8 CAD3D1CTBA</sentence>
				<definiendum id="0">D7CTCTCSD7 BP BDBC D4CTD6 CRCPD8CTCVD3D6DDBN D4CPD8D8CTD6D2 CPCRB9 CRD9D6CPCRDD D8CWD6CTD7CWD3D0CS AI D4D6CTCR BP BCBMBKBCBN D2 BP D1 BP BH CUD3D6 D8CWCT D2D9D1CQCTD6 D3CU D6CTD8CPCXD2CTCS D4CPD8D8CTD6D2D7 CPD2CS CRCPD2CSCXCSCPD8CTD7BA CCCWCT D0CTCPD6D2CXD2CV CRD9D6DACTD7 CXD2 BYCXCVD9D6CT BD D7CWD3DBCWD3DBD6CTB9 CRCPD0D0 CPD2CS D4D6CTCRCXD7CXD3D2CUD3D6 CSCXD7CTCPD7CTD7 CPD2CS D0D3CRCPD8CXD3D2D7 DACPD6DD CPCRD6D3D7D7 D8CWCT CXD8CTD6CPD8CXD3D2D7BA CCCWCT</definiendum>
				<definiens id="0">Recall Dis + Loc + Sym + Other : Locations ( 100k ) Locations ( 26k ) Diseases ( 26k ) Diseases ( 100k ) BYCXCVD9D6CT BDBM C6CPD1CTD7BM CACTCRCPD0D0 DAD7BA C8D6CTCRCXD7CXD3D2 D8CPD2CTD3D9D7D0DDBA BJ CFCT CXD2D8D6D3CSD9CRCT CP CRCPD8CTCVD3D6DD CUD3D6 D7DDD1D4B9 D8D3D1D7B8 CSCXD7CRD9D7D7CTCS CXD2 D8CWCT D2CTDCD8 D7CTCRD8CXD3D2BA CFCT CPD0D7D3 CXD2D8D6D3CSD9CRCT CP D2CTCVCPD8CXDACT CRCPD8CTCVD3D6DD CUD3D6 D0CTCPD6D2CXD2CV D8CTD6D1D7 CQCTD0D3D2CVCXD2CV D8D3 D2D3D2CT D3CU D8CWCT CRD0CPD7D7CTD7BA BTD7 D7CTCTCSD7B8 DBCT D9D7CT D8CWCT BDBC D1D3D7D8 CUD6CTD5D9CTD2D8 C6BZD7 CXD2 D8CWCT CRD3D6D4D9D7B8 CTDCCRD0D9CSCXD2CV CSCXD7CTCPD7CT CPD2CS D0D3CRCPD8CXD3D2 D2CPD1CTD7B8 CPD2CS CVCTD2CTD6CXCR DBD3D6CSD7 CUD3D6 CSCXD7CTCPD7CTD7 D3D6 D0D3CRCPB9 D8CXD3D2D7 B4CKDACXD6D9D7B8AY CKD3D9D8CQD6CTCPCZB8AY CKCPD6CTCPAYB5BA BK CCCWCT D4CPD6CPD1CTD8CTD6D7 CXD2 D8CWCTD7CT CTDCD4CTD6CXD1CTD2D8D7 CPD6CTBM D2D9D1CQCTD6 D3CU</definiens>
				<definiens id="1">D1CTCPD7D9D6CTD7B8 CWD3DBCTDACTD6B8 CPD6CT D9D2CSCTD6B9 D7D8CPD8CTCSBA BUCTCRCPD9D7CT CXD8 CXD7D2D3D8 D4D3D7D7CXCQD0CTD8D3 CVCTD8 CP CUD9D0D0D0CXD7D8 CUD3D6 D1CTCPD7D9D6CXD2CV D4D6CTCRCXD7CXD3D2B8 DBCT ACD2CS D8CWCPD8 C6D3D1CTD2 CXD7 D4CTD2CPD0CXDECTCS CUD3D6 ACD2CSCXD2CV CRD3D6D6CTCRD8 CPD2D7DBCTD6D7BA CCCWCXD7 CXD7 CP CVCTD2CTD6CPD0 D4D6D3CQD0CTD1 D3CU D8DDD4CTB9CQCPD7CTCS CTDACPD0D9CPD8CXD3D2BA CCD3D5D9CPD2D8CXCUDDD8CWCXD7CTABCTCRD8B8 DBCT D1CPD2D9CPD0D0DDCTDCCPD1CXD2CTCS D8CWCT CSCXD7CTCPD7CT D2CPD1CTD7 D0CTCPD6D2CTCS CQDD C6D3D1CTD2 D3D2 D8CWCT CSCTB9 DACTD0D3D4D1CTD2D8 CRD3D6D4D9D7 CPD2CS D6CTB9CXD2D8D6D3CSD9CRCTCS D8CWD3D7CT D8CWCPD8 BJ C4D3CRCPD8CXD3D2D7 D7CTCTCSD7BM CDD2CXD8CTCS CBD8CPD8CTD7B8 C5CPD0CPDDD7CXCPB8 BTD9D7D8D6CPD0CXCPB8 BUCTD0CVCXD9D1B8 BVCWCXD2CPB8 BXD9D6D3D4CTB8 CCCPCXDBCPD2B8 C0D3D2CV C3D3D2CVB8 CBCXD2CVCPB9 D4D3D6CTB8 BYD6CPD2CRCTBA BK CCCWCT D2CTCVCPD8CXDACT D7CTCTCSD7 DBCTD6CTBM CRCPD7CTB8 CWCTCPD0D8CWB8 CSCPDDB8 D4CTD3D4D0CTB8 DDCTCPD6B8 D4CPD8CXCTD2D8B8 CSCTCPD8CWB8 D2D9D1CQCTD6B8 D6CTD4D3D6D8B8 CUCPD6D1BA 0 1 0 0.2 0.4 0.6 0.8 1 Precision Disease Names : Recall Dis + Loc + Sym +</definiens>
			</definition>
</paper>

		<paper id="2014">
			<definition id="0">
				<sentence>A direct conversion is either impossible ( phonetic symbols of any kind do not directly correspond to printed characters ) or insufficient ( source symbols are underspecified or incorrectly recognized ) .</sentence>
				<definiendum id="0">direct conversion</definiendum>
				<definiens id="0">phonetic symbols of any kind do not directly correspond to printed characters ) or insufficient ( source symbols are underspecified or incorrectly recognized )</definiens>
			</definition>
			<definition id="1">
				<sentence>A standard feedback interface consists of a formalism for describing the interaction between a recognition source and the correction framework , regardless of the characteristics of the recognition subsystem .</sentence>
				<definiendum id="0">standard feedback interface</definiendum>
				<definiens id="0">consists of a formalism for describing the interaction between a recognition source and the correction framework , regardless of the characteristics of the recognition subsystem</definiens>
			</definition>
			<definition id="2">
				<sentence>As service modules , the framework incorporates the Humor ( morphological analyser ) , the Helyesebb ( grammatical validator ) , and the HumorESK ( full parser ) technologies .</sentence>
				<definiendum id="0">Humor</definiendum>
				<definiens id="0">morphological analyser ) , the Helyesebb ( grammatical validator ) , and the HumorESK ( full parser ) technologies</definiens>
			</definition>
			<definition id="3">
				<sentence>Walter de Gruyter Publishers , Berlin &amp; New York Koskenniemi , K. ( 1983 ) Two-level morphology : A general computational model for word-form recognition and production .</sentence>
				<definiendum id="0">Two-level morphology</definiendum>
				<definiens id="0">A general computational model for word-form recognition and production</definiens>
			</definition>
			<definition id="4">
				<sentence>Humor : a Morphological System for Corpus Analysis First TELRI Seminar on Language Resources and Language Technology , 149–158 , Tihany , Hungary Prószéky , G. ( 1996 ) .</sentence>
				<definiendum id="0">Humor</definiendum>
				<definiens id="0">a Morphological System for Corpus Analysis First TELRI Seminar on Language Resources and Language Technology , 149–158 , Tihany</definiens>
			</definition>
			<definition id="5">
				<sentence>Language Determination : Natural Language Processing from Scanned Document Images .</sentence>
				<definiendum id="0">Language Determination</definiendum>
				<definiens id="0">Natural Language Processing from Scanned Document Images</definiens>
			</definition>
</paper>

		<paper id="1117">
			<definition id="0">
				<sentence>The law discovered empirically by Zipf ( 1949 ) for word tokens in a corpus states that if f is the frequency of a word in the corpus and r is the rank , then : r k f = ( 1 ) where k is a constant for the corpus .</sentence>
				<definiendum id="0">r</definiendum>
				<definiendum id="1">k</definiendum>
				<definiens id="0">the frequency of a word in the corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>The Mandarin language is a syllable-class language , in which each syllable is at the same time a word and a Chinese character .</sentence>
				<definiendum id="0">Mandarin language</definiendum>
				<definiens id="0">a syllable-class language , in which each syllable is at the same time a word and a Chinese character</definiens>
			</definition>
			<definition id="2">
				<sentence>The number of syllable-types ( i.e. unigrams ) in the TREC corpus is only 6,300 , very different from English ( the WSJ87 corpus has 114,718 word types ) ; so it is not surprising that the Zipf curve for unigrams in Mandarin in Figure 7 is very different from the Zipf curve for unigrams in English .</sentence>
				<definiendum id="0">number of syllable-types</definiendum>
				<definiens id="0">the WSJ87 corpus has 114,718 word types</definiens>
			</definition>
</paper>

		<paper id="1078">
			<definition id="0">
				<sentence>P ( a i |φ c ) ≈ P ( p i |c ) ·P ( d i ) ·P ( r i ) ·P ( n i |v , c ) ( 4 ) Here , the first three factors , P ( p i |c ) · P ( d i ) · P ( r i ) , are related to syntactic properties , and P ( n i |v , c ) is a semantic property associated with zero pronouns and antecedents .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">a semantic property associated with zero pronouns and antecedents</definiens>
			</definition>
</paper>

		<paper id="1024">
</paper>

	</volume>
