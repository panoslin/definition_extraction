<?xml version="1.0" encoding="UTF-8"?>
	<volume id="H05">

		<paper id="1066">
			<definition id="0">
				<sentence>In particular , we define the score of an edge to be the dot product be524 tween a high dimensional feature representation of the edge and a weight vector , s ( i , j ) = w f ( i , j ) Thus the score of a dependency tree y for sentence x is , s ( x , y ) = summationdisplay ( i , j ) ∈y s ( i , j ) = summationdisplay ( i , j ) ∈y w f ( i , j ) Assuming an appropriate feature representation as well as a weight vector w , dependency parsing is the task of finding the dependency tree y with highest score for a given sentence x. For the rest of this section we assume that the weight vector w is known and thus we know the score s ( i , j ) of each possible edge .</sentence>
				<definiendum id="0">dependency parsing</definiendum>
				<definiens id="0">the score of an edge to be the dot product be524 tween a high dimensional feature representation of the edge and a weight vector , s ( i</definiens>
				<definiens id="1">the score of a dependency tree y for sentence x is , s ( x , y ) = summationdisplay ( i , j</definiens>
			</definition>
			<definition id="1">
				<sentence>A maximum spanning tree ( MST ) of G is a tree y E that maximizes the value summationtext ( i , j ) ∈y s ( i , j ) such that every vertex in V appears in y. The maximum projective spanning tree of G is constructed similarly except that it can only contain projective edges relative to some total order on the vertices of G. The MST problem for directed graphs is also known as the maximum arborescence problem .</sentence>
				<definiendum id="0">maximum spanning tree ( MST ) of G</definiendum>
				<definiens id="0">a tree y E that maximizes the value summationtext ( i , j ) ∈y s ( i , j ) such that every vertex in V appears in y. The maximum projective spanning tree of G is constructed similarly except that it can only contain projective edges relative to some total order on the vertices of G. The MST problem for directed graphs is also known as the maximum arborescence problem</definiens>
			</definition>
			<definition id="2">
				<sentence>For each sentence x we define the directed graph Gx = ( Vx , Ex ) given by Vx = fx0 = root , x1 , . . . , xng Ex = f ( i , j ) : i 6= j , ( i , j ) 2 [ 0 : n ] [ 1 : n ] g That is , Gx is a graph with the sentence words and the dummy root symbol as vertices and a directed edge between every pair of distinct words and from the root symbol to every word .</sentence>
				<definiendum id="0">g That</definiendum>
				<definiendum id="1">Gx</definiendum>
				<definiens id="0">a graph with the sentence words and the dummy root symbol as vertices and a directed edge between every pair of distinct words and from the root symbol to every word</definiens>
			</definition>
			<definition id="3">
				<sentence>Chu-Liu-Edmonds ( G , s ) Graph G = ( V , E ) Edge weight function s : E → R contract ( G = ( V , E ) , C , s ) Add edge ( c , x ) to GC with s ( c , x ) = maxxprime∈C s ( xprime , x ) Add edge ( x , c ) to GC with s ( x , c ) = maxxprime∈C [ s ( x , xprime ) − s ( a ( xprime ) , xprime ) + s ( C ) ] where a ( v ) is the predecessor of v in C and s ( C ) = Pv∈C s ( a ( v ) , v ) Figure 3 : Chu-Liu-Edmonds algorithm for finding maximum spanning trees in directed graphs .</sentence>
				<definiendum id="0">Chu-Liu-Edmonds</definiendum>
				<definiens id="0">s ) Graph G = ( V , E ) Edge weight function s : E → R contract ( G = ( V , E</definiens>
				<definiens id="1">the predecessor of v in C and s ( C ) = Pv∈C s ( a ( v ) , v ) Figure 3 : Chu-Liu-Edmonds algorithm for finding maximum spanning trees in directed graphs</definiens>
			</definition>
			<definition id="4">
				<sentence>Well-known algorithms exist for the less general case of finding spanning trees in undirected graphs ( Cormen et al. , 1990 ) .</sentence>
				<definiendum id="0">Well-known algorithms</definiendum>
			</definition>
			<definition id="5">
				<sentence>An online learning algorithm considers a single training instance at each update to w. The auxiliary vector v accumulates the successive values of w , so that the final weight vector is the average of the weight vecTraining data : T = { ( xt , yt ) } Tt=1 ‚‚ ‚w ( i+1 ) − w ( i ) ‚‚ ‚ s.t. s ( xt , yt ) − s ( xt , yprime ) ≥ L ( yt , yprime ) , ∀yprime ∈ dt ( xt ) Figure 4 : MIRA learning algorithm .</sentence>
				<definiendum id="0">online learning algorithm</definiendum>
				<definiens id="0">considers a single training instance at each update to w. The auxiliary vector v accumulates the successive values of w</definiens>
				<definiens id="1">the average of the weight vecTraining data : T = {</definiens>
			</definition>
			<definition id="6">
				<sentence>On each update , MIRA attempts to keep the new weight vector as close as possible to the old weight vector , subject to correctly classifying the instance under consideration with a margin given by the loss of the incorrect classifications .</sentence>
				<definiendum id="0">MIRA</definiendum>
				<definiens id="0">attempts to keep the new weight vector as close as possible to the old weight vector</definiens>
			</definition>
			<definition id="7">
				<sentence>The resulting online update ( to be inserted in Figure 4 , line 4 ) would then be : min vextenddoublevextenddoublew ( i+1 ) w ( i ) vextenddoublevextenddouble s.t. s ( xt , yt ) s ( xt , yprime ) L ( yt , yprime ) where yprime = arg maxyprime s ( xt , yprime ) McDonald et al. ( 2005 ) used a similar update with k constraints for the k highest-scoring trees , and showed that small values of k are sufficient to achieve the best accuracy for these methods .</sentence>
				<definiendum id="0">yprime</definiendum>
				<definiens id="0">a similar update with k constraints for the k highest-scoring trees , and showed that small values of k are sufficient to achieve the best accuracy for these methods</definiens>
			</definition>
</paper>

		<paper id="2013">
			<definition id="0">
				<sentence>The Speech Enhancer removes noises and echo .</sentence>
				<definiendum id="0">Speech Enhancer</definiendum>
				<definiens id="0">removes noises and echo</definiens>
			</definition>
			<definition id="1">
				<sentence>The NLU module takes a sequence of recognized words and tags , performs a deep linguistic analysis with probabilistic models , and produces an XMLbased semantic feature structure representation .</sentence>
				<definiendum id="0">NLU module</definiendum>
				<definiens id="0">takes a sequence of recognized words and tags , performs a deep linguistic analysis with probabilistic models , and produces an XMLbased semantic feature structure representation</definiens>
			</definition>
			<definition id="2">
				<sentence>The Knowledge Manager ( KM ) controls access to knowledge base sources ( such as domain knowledge and device information ) and their updates .</sentence>
				<definiendum id="0">Knowledge Manager</definiendum>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>Sentiment analysis is the task of identifying positive and negative opinions , emotions , and evaluations .</sentence>
				<definiendum id="0">Sentiment analysis</definiendum>
			</definition>
			<definition id="1">
				<sentence>Contextual polarity may also be influenced by modality ( e.g. , whether the proposition is asserted to be real ( realis ) or not real ( irrealis ) – no reason at all to believe is irrealis , for example ) ; word sense ( e.g. , Environmental Trust versus He has won the people’s trust ) ; the syntactic role of a word in the sentence ( e.g. , polluters are versus they are polluters ) ; and diminishers such as little ( e.g. , little truth , little threat ) .</sentence>
				<definiendum id="0">Environmental Trust</definiendum>
				<definiens id="0">versus He has won the people’s trust ) ; the syntactic role of a word in the sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>A subjective expression is any word or phrase used to express an opinion , emotion , evaluation , stance , speculation , 1The MPQA Corpus is described in ( Wiebe et al. , 2005 ) and available at nrrc.mitre.org/NRRC/publications.htm .</sentence>
				<definiendum id="0">subjective expression</definiendum>
				<definiens id="0">any word or phrase used to express an opinion , emotion , evaluation , stance , speculation</definiens>
			</definition>
			<definition id="3">
				<sentence>2In the MPQA Corpus , subjective expressions are direct subjective expressions with non-neutral expression intensity , plus all the expressive subjective elements .</sentence>
				<definiendum id="0">MPQA Corpus</definiendum>
				<definiens id="0">direct subjective expressions with non-neutral expression intensity , plus all the expressive subjective elements</definiens>
			</definition>
			<definition id="4">
				<sentence>Structure Features : These are binary features that are determined by starting with the word instance and climbing up the dependency parse tree toward the root , looking for particular relationships , words , or patterns .</sentence>
				<definiendum id="0">Structure Features</definiendum>
			</definition>
			<definition id="5">
				<sentence>Negated is a binary feature that captures whether the word is being locally negated : its value is true if a negation word or phrase is found within the four preceeding words or in any of the word’s children in the dependency tree , and if the negation word is not in a phrase that intensifies rather than negates ( e.g. , not only ) .</sentence>
				<definiendum id="0">Negated</definiendum>
				<definiens id="0">a binary feature that captures whether the word is being locally negated : its value is true if a negation word or phrase is found within the four preceeding words or in any of the word’s children in the dependency tree</definiens>
			</definition>
			<definition id="6">
				<sentence>The modifies polarity , modified by polarity , and conj polarity features capture specific relationships between the word instance and other polarity words it may be related to .</sentence>
				<definiendum id="0">conj polarity features</definiendum>
				<definiens id="0">modifies polarity , modified by polarity , and</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>Describing word alignment is one of the fundamental goals of Statistical Machine Translation ( SMT ) .</sentence>
				<definiendum id="0">Describing word alignment</definiendum>
			</definition>
			<definition id="1">
				<sentence>Phrase pairs are extracted from the aligned bitext and used in the SMT system .</sentence>
				<definiendum id="0">Phrase pairs</definiendum>
			</definition>
			<definition id="2">
				<sentence>We use a simple , single parameter distribution , with η = 8.0 throughout P ( K|m , e ) = P ( K|m , l ) ∝ ηK Word-to-Phrase Alignment Alignment is a Markov process that specifies the lengths of phrases and their alignment with source words P ( aK1 , hK1 , φK1 |K , m , e ) = Kproductdisplay k=1 P ( ak , hk , φk|ak−1 , φk−1 , e ) = Kproductdisplay k=1 p ( ak|ak−1 , hk ; l ) d ( hk ) n ( φk ; eak ) The actual word-to-phrase alignment ( ak ) is a firstorder Markov process , as in HMM-based word-toword alignment ( Vogel et al. , 1996 ) .</sentence>
				<definiendum id="0">Alignment Alignment</definiendum>
				<definiens id="0">a simple , single parameter distribution , with η = 8.0 throughout P ( K|m , e ) = P ( K|m , l ) ∝ ηK Word-to-Phrase</definiens>
				<definiens id="1">a Markov process that specifies the lengths of phrases and their alignment with source words P ( aK1 , hK1 , φK1 |K , m , e ) = Kproductdisplay k=1 P ( ak , hk , φk|ak−1 , φk−1 , e</definiens>
			</definition>
			<definition id="3">
				<sentence>The phrase length model n ( φ ; e ) gives the probability that a word e produces a phrase with φ words in the target language ; n ( φ ; e ) is defined for φ = 1 , ··· , N. The hallucination process is a simple i.i.d. process , where d ( 0 ) = p0 , and d ( 1 ) = 1−p0 .</sentence>
				<definiendum id="0">hallucination process</definiendum>
				<definiens id="0">phrase length model n ( φ ; e ) gives the probability that a word e produces a phrase with φ words in the target language</definiens>
			</definition>
			<definition id="4">
				<sentence>For example , constraining the phrase length component n ( φ ; e ) to permit only phrases of one word would give a word-to-word HMM alignment model .</sentence>
				<definiendum id="0">e )</definiendum>
				<definiens id="0">constraining the phrase length component n ( φ ;</definiens>
			</definition>
			<definition id="5">
				<sentence>The phrase length model is motivated by Toutanova et al. ( 2002 ) who introduced ‘stay’ probabilities in HMM alignment as an alternative to word fertility .</sentence>
				<definiendum id="0">phrase length model</definiendum>
				<definiens id="0">2002 ) who introduced ‘stay’ probabilities in HMM alignment as an alternative to word fertility</definiens>
			</definition>
			<definition id="6">
				<sentence>Given a sentence pair ( el1 , fm1 ) , the forward probability αj ( i , φ ) is defined as the probability of generating the first j target words with the added condition that the target words fjj−φ+1 form a phrase aligned to source word ei .</sentence>
				<definiendum id="0">forward probability αj</definiendum>
				<definiens id="0">the probability of generating the first j target words with the added condition that the target words fjj−φ+1 form a phrase aligned to source word ei</definiens>
			</definition>
			<definition id="7">
				<sentence>The alignment test set consists of 124 sentences from the NIST 2001 dry-run MT-eval2 set that are manually word aligned .</sentence>
				<definiendum id="0">alignment test set</definiendum>
				<definiens id="0">consists of 124 sentences from the NIST 2001 dry-run MT-eval2 set that are manually word aligned</definiens>
			</definition>
			<definition id="8">
				<sentence>AER gives a general measure of word alignment quality .</sentence>
				<definiendum id="0">AER</definiendum>
			</definition>
			<definition id="9">
				<sentence>Condition ( A ) extracts phrase pairs based on the geometric mean of the E→F and F→E posteriors ( Tg = 0.01 throughout ) .</sentence>
				<definiendum id="0">Condition</definiendum>
				<definiens id="0">A ) extracts phrase pairs based on the geometric mean of the E→F and F→E posteriors ( Tg = 0.01 throughout )</definiens>
			</definition>
			<definition id="10">
				<sentence>The language model is an equal-weight interpolated trigram model trained over 373M English words taken from the English side of the bitext and the LDC Gigaword corpus .</sentence>
				<definiendum id="0">language model</definiendum>
			</definition>
</paper>

		<paper id="1116">
			<definition id="0">
				<sentence>The OpQA corpus consists of 98 documents that appeared in the world press between June 2001 and May 2002 .</sentence>
				<definiendum id="0">OpQA corpus</definiendum>
				<definiens id="0">consists of 98 documents that appeared in the world press between June 2001 and May 2002</definiens>
			</definition>
			<definition id="1">
				<sentence>The QA system invokes an IR subsystem that employs traditional text similarity measures ( e.g. , tf/idf ) to retrieve and rank document fragments ( sentences or paragraphs ) w.r.t. the question ( query ) .</sentence>
				<definiendum id="0">QA system</definiendum>
				<definiens id="0">invokes an IR subsystem that employs traditional text similarity measures ( e.g. , tf/idf ) to retrieve and rank document fragments ( sentences or paragraphs</definiens>
			</definition>
			<definition id="2">
				<sentence>We consider four constituent types – noun phrase ( n ) , verb phrase ( v ) , prepositional phrase ( p ) , and clause ( c ) – and three matching criteria : 4The parser is available from http : //www.vinartus.net/spa/ .</sentence>
				<definiendum id="0">noun phrase</definiendum>
				<definiens id="0">n ) , verb phrase ( v ) , prepositional phrase ( p )</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>Assuming P ( δe|e ) to be a uniform distribution , the posterior of selecting a hidden block given observations : P ( δ [ ] = ( δe , δf ) |e , f ) is proportional to block level relative frequency Prel ( δf∈|δe∈ ) updated in each iteration ; and can be smoothed with P ( δf|δe , f , e ) = P ( δf∈|δe∈ ) P ( δf/∈|δe/∈ ) /summationtext { δprimef } P ( δprimef∈|δe∈ ) P ( δprimef/∈|δe/∈ ) assuming Model-1 alignment in the inner and outer parts independently to reduce the risks of data sparseness in estimations .</sentence>
				<definiendum id="0">Assuming P ( δe|e )</definiendum>
				<definiens id="0">proportional to block level relative frequency Prel ( δf∈|δe∈ ) updated in each iteration</definiens>
				<definiens id="1">assuming Model-1 alignment in the inner and outer parts independently to reduce the risks of data sparseness in estimations</definiens>
			</definition>
			<definition id="1">
				<sentence>P ( δf∈|δf , δe , e ) is a bracket level emission probabilistic model which generates a bag of contiguous words fj ∈ δf∈ under the constraints from the given hidden block δ [ ] = ( δf , δe ) .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">e )</definiendum>
				<definiens id="0">a bracket level emission probabilistic model which generates a bag of contiguous words fj ∈ δf∈ under the constraints from the given hidden block</definiens>
			</definition>
			<definition id="2">
				<sentence>Thebaseline system ( HMM ) used phrase pairs built from the HMM-EC-P maximum posterior word alignment and thecorrespondinglexicons .</sentence>
				<definiendum id="0">Thebaseline system</definiendum>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>Lexical Information Only LSA ( Landauer , 1997 ) is a technique for extracting the ‘hidden’ dimensions of the semantic representation of terms , sentences , or documents , on the basis of their contextual use .</sentence>
				<definiendum id="0">Lexical Information Only LSA</definiendum>
				<definiens id="0">a technique for extracting the ‘hidden’ dimensions of the semantic representation of terms , sentences , or documents , on the basis of their contextual use</definiens>
			</definition>
			<definition id="1">
				<sentence>Gong and Liu propose to start by creating a term by sentences matrix A = [ A1 , A2 , ... , An ] , where each column vector Ai represents the weighted term-frequency vector of sentence i in the document under consideration .</sentence>
				<definiendum id="0">A2 , ...</definiendum>
				<definiens id="0">the weighted term-frequency vector of sentence i in the document under consideration</definiens>
			</definition>
			<definition id="2">
				<sentence>Formally : ( 2 ) sk = radicalBigsummationtextr i=1 v2k , i ·σ2i , where sk is the length of the vector of k’th sentence in the modified latent vector space , and its significance score for summarization too .</sentence>
				<definiendum id="0">sk</definiendum>
				<definiens id="0">the length of the vector of k’th sentence in the modified latent vector space</definiens>
			</definition>
			<definition id="3">
				<sentence>Resolver The system we used in these experiments , GUITAR ( Poesio and Kabadjov , 2004 ) , is an anaphora resolution system designed to be high precision , modular , and usable as an off-the-shelf component of a NL processing pipeline .</sentence>
				<definiendum id="0">GUITAR</definiendum>
				<definiens id="0">an anaphora resolution system designed to be high precision</definiens>
			</definition>
			<definition id="4">
				<sentence>The top e sentences according to utility score4 are then called a sentence extract of size e. We can then define the following system performance metric : ( 6 ) RU = summationtextn j=1 δj summationtextN i=1 uijsummationtext n j=1 ǫj summationtextN i=1 uij , where uij is a utility score of sentence j from annotator i , ǫj is 1 for the top e sentences according to the sum of utility scores from all judges and δj is equal to 1 for the top e sentences extracted by the system .</sentence>
				<definiendum id="0">uij</definiendum>
				<definiens id="0">1 for the top e sentences according to the sum of utility scores from all judges</definiens>
				<definiens id="1">equal to 1 for the top e sentences extracted by the system</definiens>
			</definition>
</paper>

		<paper id="1110">
			<definition id="0">
				<sentence>Let us , therefore , assume that we have a set of models { P ( x , y|θij ) x∈Li , y∈Lj } inegationslash=j where θij is a parameter vector for pairwise model for languages Li and Lj .</sentence>
				<definiendum id="0">θij</definiendum>
				<definiens id="0">a parameter vector for pairwise model for languages Li and Lj</definiens>
			</definition>
			<definition id="1">
				<sentence>This leads us to look for an estimate that is as close to all of our models as possible , under the 876 optimal values of θi’s , or more formally : Pest = argmin ˆP ( · ) min θ1 ... min θn d ( ˆP ( · ) , P1 ( ·|θ1 ) , ... Pn ( ·|θn ) ) where d measures the distance between ˆP and all the Pi under the parameter setting θi .</sentence>
				<definiendum id="0">Pn</definiendum>
			</definition>
			<definition id="2">
				<sentence>Since we have no reason to prefer any of the Pi , we choose the following symmetric form for d : nsummationdisplay i=1 D ( ˆP ( · ) ||Pi ( ·|θi ) ) where D is a reasonable measure of distance between probability distributions .</sentence>
				<definiendum id="0">D</definiendum>
			</definition>
			<definition id="3">
				<sentence>Taking a partial derivative and solving , we obtain : ˆP ( x ) = producttextn i=1 Pi ( x|θi ) 1/nsummationtext xprime∈X producttextn i=1 Pi ( xprime|θi ) 1/n Substituting this value into the expression for function d , we obtain the following distance measure between the Pi’s : dprime ( P1 ( X|θ1 ) ... Pn ( X|θn ) ) = minˆP d ( ˆP , P1 ( X|θ1 ) , ... Pn ( X|θn ) ) = −logsummationtextx∈X producttextni=1 Pi ( x|θi ) 1/n This function is a generalization of the wellknown Bhattacharyya distance for two distributions ( Bhattacharyya , 1943 ) : b ( p , q ) =summationdisplay i √p iqi These results suggest the following Algorithm 1 to optimize d ( and dprime ) : • Set all θi randomly • Repeat until change in d is very small : – Compute ˆP according to the above formula – For i from 1 to n ∗ Set θi in such a way as to minimize D ( ˆP ( X ) ||Pi ( X|θi ) ) – Compute d according to the above formula Each step of the algorithm minimizes d. It is also easy to see that minimizing D ( ˆP ( X ) ||Pi ( X|θi ) ) is the same as setting the parameters θi in order to maximize producttextx∈X Pi ( x|θi ) ˆP ( x ) , which can be interpreted as maximizing the probability under Pi of a corpus in which word x appears ˆP ( x ) times .</sentence>
				<definiendum id="0">Pn</definiendum>
				<definiens id="0">the same as setting the parameters θi in order to maximize producttextx∈X Pi ( x|θi ) ˆP ( x ) , which can be interpreted as maximizing the probability under Pi of a corpus in which word x appears ˆP ( x ) times</definiens>
			</definition>
			<definition id="4">
				<sentence>Let LA and LB be the portions of the parallel text in languages A and B respectively , and LA = ( xi ) i=1 ... n and LB = ( yi ) i=1 ... m. We can define P ( LB|LA ) as maxP A→B maxP aligns nsummationdisplay i=1 msummationdisplay j=1 PA→B ( yj|xi ) Paligns ( xi|j ) The GIZA software does the maximization by building a variety of models , mostly described by Brown et al. ( 1993 ) .</sentence>
				<definiendum id="0">LB</definiendum>
				<definiendum id="1">LB</definiendum>
				<definiendum id="2">GIZA software</definiendum>
			</definition>
			<definition id="5">
				<sentence>However , y is not a random sequence of characters , but a word in language B , moreover , it is a word that can serve as a potential translation of word x. So , to define a proper distribution over words y given a word x and a set of possible translations of x , T ( x ) Pchar ( y|x ) = Puchar ( y|x , y ∈ T ( x ) ) = δyprime∈T ( x ) Puchar ( y , y∈T ( x ) |x ) summationtext yprime∈T ( x ) Puchar ( y prime|x ) This is the complete definition of Pchar , except for the fact that we are implicitly relying upon the character-mapping model , Pc , which we need to somehow obtain .</sentence>
				<definiendum id="0">Pc</definiendum>
				<definiens id="0">a word that can serve as a potential translation of word x. So , to define a proper distribution over words y given a word x and a set of possible translations of x , T ( x ) Pchar ( y|x ) = Puchar ( y|x , y ∈ T ( x</definiens>
			</definition>
			<definition id="6">
				<sentence>f ( x ) is defined as the longest possible suffix of x that is in the set SA , and g is defined similarly , for SB .</sentence>
				<definiendum id="0">f ( x )</definiendum>
				<definiendum id="1">g</definiendum>
				<definiens id="0">the longest possible suffix of x that is in the set SA , and</definiens>
			</definition>
			<definition id="7">
				<sentence>If we have some model PA→B , we can define PCA→CB ( j|i ) = 1CPA→B ( yj−1|xi−1 ) PA→B ( yj|xi ) × PA→B ( yj+1|xi+1 ) where C is the sum over j of the above products , and serves to normalize the distribution .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">the sum over j of the above products</definiens>
			</definition>
			<definition id="8">
				<sentence>Our evaluation metric is the number of entries that match between these dictionaries .</sentence>
				<definiendum id="0">evaluation metric</definiendum>
				<definiens id="0">the number of entries that match between these dictionaries</definiens>
			</definition>
			<definition id="9">
				<sentence>The “oracle” setting is the setting obtained by tuning on the test set ( the dictionary ) .</sentence>
				<definiendum id="0">“oracle” setting</definiendum>
				<definiens id="0">the setting obtained by tuning on the test set ( the dictionary )</definiens>
			</definition>
			<definition id="10">
				<sentence>881 We have built a system for multi-dictionary induction from parallel corpora which significantly improves quality over the standard existing tool ( GIZA ) by taking advantage of the fact that languages are related and we have a group of more than two of them .</sentence>
				<definiendum id="0">GIZA</definiendum>
			</definition>
</paper>

		<paper id="1096">
			<definition id="0">
				<sentence>The target language model captures the well-formedness or the syntax in the target language .</sentence>
				<definiendum id="0">target language model</definiendum>
				<definiens id="0">captures the well-formedness or the syntax in the target language</definiens>
			</definition>
			<definition id="1">
				<sentence>Let ( jK0 , iK0 ) be a segmentation of the source sentence into phrases , with the corresponding ( bilingual ) phrase pairs ( ˜fk , ˜ek ) = ( fjkjk−1+1 , eikik−1+1 ) , k = 1 , ... , K. The phrasebased approach to SMT is then expressed by the following equation : ˆeˆI1 = argmax jK0 , iK0 , I , eI1 braceleftbigg Iproductdisplay i=1 bracketleftBig c1 ·p ( ei|ei−1i−2 ) λ1 bracketrightBig ( 2 ) · Kproductdisplay k=1 bracketleftBig c2 · p ( ˜fk |˜ek ) λ2 ·p ( ˜ek | ˜fk ) λ3 · jkproductdisplay j=jk−1+1 p ( fj |˜ek ) λ4 · ikproductdisplay i=ik−1+1 p ( ei| ˜fk ) λ5 bracketrightBigbracerightbigg , where p ( ˜fk |˜ek ) and p ( ˜ek | ˜fk ) are the phrase lexicon models in both translation directions .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">a segmentation of the source sentence into phrases</definiens>
			</definition>
			<definition id="2">
				<sentence>p ( fj |˜ek ) is defined as the IBM-1 model probability of fj over the whole phrase ˜ek , and p ( ei| ˜fk ) is the inverse model , respectively .</sentence>
				<definiendum id="0">p ( fj |˜ek )</definiendum>
				<definiens id="0">the IBM-1 model probability of fj over the whole phrase ˜ek , and p ( ei| ˜fk ) is the inverse model</definiens>
			</definition>
			<definition id="3">
				<sentence>c1 is the so-called word penalty , and c2 is the phrase penalty , assigning constant costs to each target language word/phrase .</sentence>
				<definiendum id="0">c1</definiendum>
				<definiendum id="1">c2</definiendum>
				<definiens id="0">the so-called word penalty</definiens>
			</definition>
			<definition id="4">
				<sentence>The confidence of a target word can be expressed by its posterior probability , i.e. the probability of the word to occur in the target sentence , given the source sentence .</sentence>
				<definiendum id="0">posterior probability</definiendum>
				<definiens id="0">the probability of the word to occur in the target sentence , given the source sentence</definiens>
			</definition>
			<definition id="5">
				<sentence>The forward probability Φi ( nprime , n ) of an edge is the probability of reaching this edge from the source of the graph , where the word e ( nprime , n ) is the i-th word on the path .</sentence>
				<definiendum id="0">forward probability Φi</definiendum>
				<definiendum id="1">n )</definiendum>
				<definiens id="0">the probability of reaching this edge from the source of the graph , where the word e</definiens>
				<definiens id="1">the i-th word on the path</definiens>
			</definition>
			<definition id="6">
				<sentence>The backward probability expresses the probability of completing a sentence from the current edge , i.e. of reaching the sink of the graph .</sentence>
				<definiendum id="0">backward probability</definiendum>
				<definiens id="0">expresses the probability of completing a sentence from the current edge</definiens>
			</definition>
			<definition id="7">
				<sentence>Analogously , define p ( fj+sj , ei+ti ) as the score of the phrase pair which consists of the phrase penalty and the phrase and word lexicon model scores ( cf. section 2.2 ) .</sentence>
				<definiendum id="0">define p</definiendum>
			</definition>
			<definition id="8">
				<sentence>Therefore , we determine the maximal translation probability of the target word e over the source sentence words : pIBM−1 ( e|fJ1 ) = maxj=0 , ... , J p ( e|fj ) , ( 9 ) where f0 is the “empty” source word ( Brown et al. , 1993 ) .</sentence>
				<definiendum id="0">J p</definiendum>
				<definiendum id="1">f0</definiendum>
			</definition>
</paper>

		<paper id="1070">
			<definition id="0">
				<sentence>The importance of the expressive power of the query language is a consequence of the sizes of the available treebanks , which can contain several tenthousand trees .</sentence>
				<definiendum id="0">query language</definiendum>
			</definition>
			<definition id="1">
				<sentence>The query language is monadic secondorder logic ( MSO henceforth ) , an extension of rstorder logic that additionally allows for the quanti cation over sets of tree nodes .</sentence>
				<definiendum id="0">query language</definiendum>
				<definiens id="0">monadic secondorder logic ( MSO henceforth ) , an extension of rstorder logic that additionally allows for the quanti cation over sets of tree nodes</definiens>
			</definition>
			<definition id="2">
				<sentence>Hence MSO is a natural choice of query language under the given assumption that the syntax of natural language is ( at least ) context-free on the string or token level .</sentence>
				<definiendum id="0">Hence MSO</definiendum>
				<definiens id="0">a natural choice of query language under the given assumption that the syntax of natural language is ( at least ) context-free on the string or token level</definiens>
			</definition>
			<definition id="3">
				<sentence>MONA is an implementation of this relationship .</sentence>
				<definiendum id="0">MONA</definiendum>
				<definiens id="0">an implementation of this relationship</definiens>
			</definition>
			<definition id="4">
				<sentence>A rst-order variable is a rst-order term .</sentence>
				<definiendum id="0">rst-order variable</definiendum>
			</definition>
			<definition id="5">
				<sentence>A set variable is a second-order term .</sentence>
				<definiendum id="0">set variable</definiendum>
			</definition>
			<definition id="6">
				<sentence>The main program of MONA is a compiler that compiles formulae in the above described language into tree automata .</sentence>
				<definiendum id="0">MONA</definiendum>
				<definiens id="0">a compiler that compiles formulae in the above described language into tree automata</definiens>
			</definition>
			<definition id="7">
				<sentence>x y ) Node x is to the left of y , ( = x y ) Node x and y are identical , ( = X Y ) Node sets X and Y are identical , ( in x X ) Node x is a member of set X. Complex formulae are constructed by boolean connectives and quanti cation .</sentence>
				<definiendum id="0">x y ) Node x</definiendum>
				<definiens id="0">the left of y , ( = x y ) Node x and y are identical</definiens>
			</definition>
			<definition id="8">
				<sentence>y 2 X is a formula with free variables x and y that de nes the transitive closure of R. If we now take R ( x , y ) in the above formula to be ( x.0 = y j x.1 = y ) we de ne dominance ( dom ) .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a formula with free variables x and y</definiens>
			</definition>
			<definition id="9">
				<sentence>E.g. , ( E x f ) translates to ( ex1 x : x in Carcase &amp; f0 ) where f0 is the translation of f. The same holds mutatis mutandis for set variable quanti cation .</sentence>
				<definiendum id="0">f0</definiendum>
				<definiens id="0">the translation of f. The same holds mutatis mutandis for set variable quanti cation</definiens>
			</definition>
			<definition id="10">
				<sentence>Thus the set of MONA formulae to evaluate a query consist of the translation of the query , the formulae for dom and rb , and an import declaration for one tree from the treebank .</sentence>
				<definiendum id="0">import declaration</definiendum>
				<definiens id="0">Thus the set of MONA formulae to evaluate a query consist of the translation of the query</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>a0 Interpretability : A good metric should be easy to interpret .</sentence>
				<definiendum id="0">metric</definiendum>
				<definiens id="0">A good</definiens>
			</definition>
			<definition id="1">
				<sentence>A widely-used metric is the link-based F-measure ( Vilain et al. , 1995 ) adopted in the MUC task .</sentence>
				<definiendum id="0">widely-used metric</definiendum>
				<definiens id="0">the link-based F-measure ( Vilain et al. , 1995 ) adopted in the MUC task</definiens>
			</definition>
			<definition id="2">
				<sentence>It is computed by first counting the number of common links between the reference ( or “truth” ) and the system output ( or “response” ) ; the link precision is the number of common links divided by the number of links in the system output , and the link recall is the number of common links divided by the number of links in the reference .</sentence>
				<definiendum id="0">link precision</definiendum>
				<definiendum id="1">link recall</definiendum>
				<definiens id="0">the number of common links divided by the number of links in the system output</definiens>
				<definiens id="1">the number of common links divided by the number of links in the reference</definiens>
			</definition>
			<definition id="3">
				<sentence>The total cost is the sum of the three costs , which is then normalized against the cost of a nominal system that does not output any entity .</sentence>
				<definiendum id="0">total cost</definiendum>
				<definiens id="0">the sum of the three costs , which is then normalized against the cost of a nominal system that does not output any entity</definiens>
			</definition>
			<definition id="4">
				<sentence>Casting the entity alignment problem as the maximum bipartite matching is trivial : each entity in a12 and a42 is a vertex and the node pair a13 a21a110a29a27a44a109a15 , where a21a140a90 a12 , a44a141a90 a42 , is connected by an edge with the weight a107 a13 a21a108a29a36a44a109a15 .</sentence>
				<definiendum id="0">a42</definiendum>
				<definiens id="0">a vertex and the node pair a13 a21a110a29a27a44a109a15 , where a21a140a90 a12 , a44a141a90 a42 , is connected by an edge with the weight a107 a13 a21a108a29a36a44a109a15</definiens>
			</definition>
			<definition id="5">
				<sentence>The system response ( c ) represents a trivial output : all mentions are put in the same entity .</sentence>
				<definiendum id="0">system response ( c )</definiendum>
				<definiens id="0">a trivial output : all mentions are put in the same entity</definiens>
			</definition>
			<definition id="6">
				<sentence>Yet the MUC metric will lead to a a6a8a7a9a7 a4 recall ( a1 out of a1 reference links are System CEAF response MUC B-cube a107 a201 a13 a34a49a29a27a34a88a15 a107 a203 a13 a34a49a29a27a34a88a15 ( a ) 0.947 0.865 0.833 0.733 ( b ) 0.947 0.737 0.583 0.667 ( c ) 0.900 0.545 0.417 0.294 ( d ) – 0.400 0.250 0.178 Table 1 : Comparison of coreference evaluation metrics correct ) and a a204a48a6a30a50 a31 a4 precision ( a1 out of a6a3a6 system links are correct ) , which gives rise to a a1 a7 a4 F-measure .</sentence>
				<definiendum id="0">MUC metric</definiendum>
			</definition>
			<definition id="7">
				<sentence>But B-cube uses the same entity “intersecting” procedure found in computing the MUC F-measure ( Vilain et al. , 1995 ) , and it sometimes can give counterintuitive results .</sentence>
				<definiendum id="0">B-cube</definiendum>
				<definiendum id="1">MUC F-measure</definiendum>
				<definiens id="0">uses the same entity “intersecting” procedure found in computing the</definiens>
			</definition>
			<definition id="8">
				<sentence>Symmetry is a desirable property .</sentence>
				<definiendum id="0">Symmetry</definiendum>
				<definiens id="0">a desirable property</definiens>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>Lexical ambiguity refers to words that share the same orthography but have different meanings ( word senses ) .</sentence>
				<definiendum id="0">Lexical ambiguity</definiendum>
			</definition>
			<definition id="1">
				<sentence>Homonymy describes when two senses of a given word ( or derivation ) are distinct .</sentence>
				<definiendum id="0">Homonymy</definiendum>
				<definiens id="0">describes when two senses of a given word ( or derivation ) are distinct</definiens>
			</definition>
			<definition id="2">
				<sentence>Pseudowords ( Gale et al. , 1992 ) are created by joining together randomly selected constituent words to create a unique term that has multiple controlled meanings .</sentence>
				<definiendum id="0">Pseudowords</definiendum>
				<definiens id="0">Gale et al. , 1992 ) are created by joining together randomly selected constituent words to create a unique term that has multiple controlled meanings</definiens>
			</definition>
			<definition id="3">
				<sentence>IDF ( Salton and McGill , 1983 ) which is comparable to the studies in section were produced where additional ambiguity in the form of pseudowords had been added .</sentence>
				<definiendum id="0">IDF</definiendum>
			</definition>
			<definition id="4">
				<sentence>WordNet ( 2.0 ) is a hierarchical semantic network developed at Princeton University .</sentence>
				<definiendum id="0">WordNet ( 2.0 )</definiendum>
			</definition>
			<definition id="5">
				<sentence>A unique word sense consists of a lemma and the particular synset in which that lemma occurs .</sentence>
				<definiendum id="0">unique word sense</definiendum>
			</definition>
			<definition id="6">
				<sentence>WordNet is a fine-grained lexical resource and polysemy can be derived to varying degrees of granularity by traversing the link structure between synsets ( figure 1 ) .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="7">
				<sentence>Examples include Zipf’s law ( Zipf , 1949 ) which denotes that a small number of words make up a large percentage of word use and Krovetz and Croft’s ( 1992 ) observation that one sense of a word accounts for the majority of all use .</sentence>
				<definiendum id="0">Zipf’s law</definiendum>
				<definiens id="0">denotes that a small number of words make up a large percentage of word use and Krovetz and Croft’s ( 1992 ) observation that one sense of a word accounts for the majority of all use</definiens>
			</definition>
			<definition id="8">
				<sentence>WordNet : An On-line Lexical Database , International journal of lexicography , 3 ( 4 ) , Oxford University Press , Pp 235 – 244 .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">An On-line Lexical Database</definiens>
			</definition>
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>A convenient lower bound is the lexical stress pattern of the word , devoid of any segmental information : e.g. , “unidirectional” has its stress on the 4th of 6 syllables ; hence , 000100 is its lexical-stress profile .</sentence>
				<definiendum id="0">convenient lower bound</definiendum>
				<definiens id="0">the lexical stress pattern of the word , devoid of any segmental information : e.g. , “unidirectional” has its stress on the 4th of 6 syllables ; hence , 000100 is its lexical-stress profile</definiens>
			</definition>
			<definition id="1">
				<sentence>This confusion matrix consists of three parts , corresponding to substitutions , insertions , and deletions .</sentence>
				<definiendum id="0">confusion matrix</definiendum>
			</definition>
</paper>

		<paper id="1090">
			<definition id="0">
				<sentence>Judging from the experiences of the 0 1 0 0.2 0.4 0.6 0.8 1 Recall Precision Precision , recall tradeoff Cos &lt; = .10 Cos &lt; = .20 Cos &lt; = .30 Cos &lt; = .40 Cos &gt; .40 ( Novelty = 1 Cos ) Figure 2 : The precision and recall scores of a vector-space model with cosine similarity at different thresholds , on the TREC 2003 data .</sentence>
				<definiendum id="0">Recall Precision Precision</definiendum>
				<definiens id="0">The precision and recall scores of a vector-space model with cosine similarity at different thresholds</definiens>
			</definition>
</paper>

		<paper id="1111">
			<definition id="0">
				<sentence>VerbNet is a manually developed hierarchical lexicon based on the verb classification of Levin ( 1993 ) .</sentence>
				<definiendum id="0">VerbNet</definiendum>
			</definition>
			<definition id="1">
				<sentence>As noted , VerbNet lacks a corpus of example role assignments against which to evaluate a role labelling based upon it .</sentence>
				<definiendum id="0">VerbNet</definiendum>
				<definiens id="0">a corpus of example role assignments against which to evaluate a role labelling based upon it</definiens>
			</definition>
			<definition id="2">
				<sentence>The final set consists of 16 roles : Agent , Amount , Attribute , Beneficiary , Cause , Destination , Experiencer , Instrument , Location , Material , Predicate , Recipient , Source , Stimulus , Theme and Time ; plus the NoRole label .</sentence>
				<definiendum id="0">final set</definiendum>
				<definiens id="0">consists of 16 roles : Agent , Amount , Attribute , Beneficiary , Cause , Destination , Experiencer , Instrument , Location , Material , Predicate , Recipient , Source , Stimulus , Theme and Time</definiens>
			</definition>
			<definition id="3">
				<sentence>Last is the overall role labelling task , which evaluates the system on the combined tasks of identification and labelling of all target arguments .</sentence>
				<definiendum id="0">Last</definiendum>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>Max-margin methods for structured data share problems of computational cost ( Altun et al. , 2003 ) .</sentence>
				<definiendum id="0">Max-margin methods</definiendum>
			</definition>
			<definition id="1">
				<sentence>Sequential classification approaches decompose the probability as follows , P ( t1 ... tn|o ) = nproductdisplay i=1 p ( ti|t1 ... ti−1o ) .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the probability as follows</definiens>
			</definition>
			<definition id="2">
				<sentence>dprimei is the direction of the edge between ti−1 and ti+1 .</sentence>
				<definiendum id="0">dprimei</definiendum>
				<definiens id="0">the direction of the edge between ti−1 and ti+1</definiens>
			</definition>
			<definition id="3">
				<sentence>The training set consists of section 15-18 of the WSJ corpus , and the test set is section 20 .</sentence>
				<definiendum id="0">training set</definiendum>
			</definition>
			<definition id="4">
				<sentence>Method Recall Precision F-score SVM ( Kudoh and Matsumoto , 2000 ) 93.51 93.45 93.48 SVM voting ( Kudo and Matsumoto , 2001 ) 93.92 93.89 93.91 Regularized Winnow ( with basic features ) ( Zhang et al. , 2002 ) 93.60 93.54 93.57 Perceptron ( Carreras and Marquez , 2003 ) 93.29 94.19 93.74 Easiest-first ( IOB2 , second-order ) 93.59 93.68 93.63 Full Bidirectional ( Start/End , first-order ) 93.70 93.65 93.70 Table 6 : Chunking F-scores on the test set ( Section 20 of the WSJ , 2012 sentences ) .</sentence>
				<definiendum id="0">Method Recall Precision F-score SVM</definiendum>
			</definition>
</paper>

		<paper id="1074">
			<definition id="0">
				<sentence>TimeOnPage is the number of seconds the user spent on a page , and EventOnScroll is the number of clicks on the scroll bars .</sentence>
				<definiendum id="0">TimeOnPage</definiendum>
				<definiendum id="1">EventOnScroll</definiendum>
				<definiens id="0">the number of clicks on the scroll bars</definiens>
			</definition>
			<definition id="1">
				<sentence>X → Y means X is a direct cause of Y. X−Y means the algorithm can not tell if X causes Y or if Y causes X. X ←→Y means the algorithm found some problem , which may happen due to a latent common cause of X and Y , a chance pattern in the sample , or other violations of assumptions .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a direct cause of Y. X−Y means the algorithm can not tell if X causes Y or if Y causes X. X ←→Y means the algorithm found some problem</definiens>
				<definiens id="1">a chance pattern in the sample , or other violations of assumptions</definiens>
			</definition>
			<definition id="2">
				<sentence>The PC algorithm assumes no hidden variables , however besides relevant , novel , authoritative , and readable , other hidden variables , such as whether a document is up-todate , interesting , misleading , etc. ( Schamber and Bateman , 1996 ) , may exist and influence a user’s preference for a document .</sentence>
				<definiendum id="0">PC algorithm</definiendum>
				<definiens id="0">assumes no hidden variables , however besides relevant , novel , authoritative , and readable , other hidden variables</definiens>
			</definition>
			<definition id="3">
				<sentence>actions = ( TimeOnPage , ... ) is a 12 dimensional vector representing the user actions in Table 1 .</sentence>
				<definiendum id="0">TimeOnPage , ... )</definiendum>
				<definiens id="0">a 12 dimensional vector representing the user actions in Table 1</definiens>
			</definition>
			<definition id="4">
				<sentence>If the parents of node X are Y , P ( X|Y ) = N ( m + W ×Y , Σ ) , where N ( µ , Σ ) is a gaussian distribution with mean µ and covariance Σ .</sentence>
				<definiendum id="0">N ( µ</definiendum>
				<definiens id="0">a gaussian distribution with mean µ and covariance Σ</definiens>
			</definition>
			<definition id="5">
				<sentence>Corr is the correlation coefficient between the predicted value of user likes using the model and the true explicit feedback provided by the users .</sentence>
				<definiendum id="0">Corr</definiendum>
				<definiens id="0">the correlation coefficient between the predicted value of user likes using the model and the true explicit feedback provided by the users</definiens>
			</definition>
			<definition id="6">
				<sentence>LR mean uses mean substitution , LR different uses case deletion , and graphical models follow the ML approach .</sentence>
				<definiendum id="0">LR mean</definiendum>
				<definiens id="0">uses mean substitution</definiens>
			</definition>
</paper>

		<paper id="1119">
			<definition id="0">
				<sentence>= max , where O denotes the totality of all audio files ( O for observation ) , W = ( w1 , w2 , ... , wN ) a hypothesized transcription of the entire collection , and T = ( t1 , t2 , ... , tN+1 ) the associated time boundaries on a shared collection-wide time axis .</sentence>
				<definiendum id="0">O</definiendum>
				<definiendum id="1">wN</definiendum>
				<definiens id="0">the totality of all audio files ( O for observation ) , W = ( w1 , w2 , ... ,</definiens>
			</definition>
			<definition id="1">
				<sentence>Specifically , w [ a ] =pac ( a ) 1/λ·PLM ( a ) with acoustic likelihood pac ( a ) , language model probability PLM , and languagemodel weight λ .</sentence>
				<definiendum id="0">] =pac</definiendum>
				<definiens id="0">language model probability PLM , and languagemodel weight λ</definiens>
			</definition>
			<definition id="2">
				<sentence>In the simplest case , Q is a single word token .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiens id="0">a single word token</definiens>
			</definition>
			<definition id="3">
				<sentence>The node posterior is the probability that the correct path passes through a connection point .</sentence>
				<definiendum id="0">node posterior</definiendum>
				<definiens id="0">the probability that the correct path passes through a connection point</definiens>
			</definition>
			<definition id="4">
				<sentence>For an audio segment i , we define the expected term frequency ( ETF ) of a query string Q as summation of phrase posteriors of all hits in this segment : ETFi ( Q ) = summationdisplay ∀ts , te P ( ∗ , ts , Q , te , ∗|Oi ) = summationdisplay pi∈Πi : I [ pi ] =Q p [ pi ] with Πi being the set of all paths of segment i. At indexing time , ETFs of a list of M-grams for each segment are calculated .</sentence>
				<definiendum id="0">expected term frequency</definiendum>
				<definiendum id="1">ETF</definiendum>
				<definiens id="0">summation of phrase posteriors of all hits in this segment : ETFi ( Q ) = summationdisplay ∀ts , te P ( ∗ , ts , Q , te</definiens>
			</definition>
			<definition id="5">
				<sentence>Besides FOM , we use a second metric – “Top Hit Precision” ( THP ) , defined as the correct rate of the best ranked hit .</sentence>
				<definiendum id="0">second metric – “Top Hit Precision” ( THP</definiendum>
				<definiens id="0">the correct rate of the best ranked hit</definiens>
			</definition>
</paper>

		<paper id="1114">
			<definition id="0">
				<sentence>FL consists of the features that occur in unlabeled data , but never appear in labeled data .</sentence>
				<definiendum id="0">FL</definiendum>
				<definiens id="0">consists of the features that occur in unlabeled data , but never appear in labeled data</definiens>
			</definition>
			<definition id="1">
				<sentence>This semi-supervised feature clustering algorithm is defined as follows : Input : Feature set F = FL uniontextFL ( the first |FL| features in F belong to FL , and the remaining |FL| features belong to FL ) , context set X , the label information of xg ( 1 ≤ g ≤ l ) , NtildewideF ( the number of clusters in tildewideF ) ; Output : Clustering solution tildewideF ; Algorithm : MF , X , where entry MF , Xi , j is the number of times of fi co-occurring with example xj ( 1 ≤ j ≤ n ) .</sentence>
				<definiendum id="0">semi-supervised feature clustering algorithm</definiendum>
				<definiendum id="1">NtildewideF (</definiendum>
				<definiens id="0">follows : Input : Feature set F = FL uniontextFL ( the first |FL| features in F belong to FL , and the remaining |FL| features belong to FL ) , context set X , the label information of xg ( 1 ≤ g ≤ l )</definiens>
			</definition>
			<definition id="2">
				<sentence>Latent semantic indexing technique ( LSI ) is used to perform factor analysis in MF , X before calculating the distance between features in step For empirical study of dimensionality reduction techniques on WSD task , we evaluated five dimensionality reduction algorithms on the data in English lexical sample ( ELS ) task of SENSEVAL-3 ( Mihalcea et al. , 2004 ) ( including all the 57 English words ) 1 : supervised feature clustering ( SuFC ) ( Baker and McCallum , 1998 ; Bekkerman et al. , 2003 ; Slonim 1Available at http : //www.senseval.org/senseval3 909 and Tishby , 2001 ) , iterative double clustering ( IDC ) ( El-Yaniv and Souroujon , 2001 ) , semi-supervised feature clustering ( SemiFC ) ( our algorithm ) , supervised feature selection ( SuFS ) ( Forman , 2003 ) , and latent semantic indexing ( LSI ) ( Deerwester et .</sentence>
				<definiendum id="0">Latent semantic indexing technique</definiendum>
				<definiendum id="1">LSI</definiendum>
				<definiendum id="2">semi-supervised feature clustering</definiendum>
				<definiendum id="3">latent semantic indexing</definiendum>
				<definiens id="0">supervised feature clustering ( SuFC )</definiens>
			</definition>
			<definition id="3">
				<sentence>IDC is an extension of double clustering method ( DC ) ( Slonim and Tishby , 2000 ) , which performs iterations of DC .</sentence>
				<definiendum id="0">IDC</definiendum>
				<definiens id="0">an extension of double clustering method ( DC )</definiens>
				<definiens id="1">performs iterations of DC</definiens>
			</definition>
			<definition id="4">
				<sentence>Information gain ( IG ) is one of state of the art criteria for feature selection , which measures the decrease in entropy when the feature is given vs. absent .</sentence>
				<definiendum id="0">Information gain ( IG )</definiendum>
				<definiens id="0">measures the decrease in entropy when the feature is given vs. absent</definiens>
			</definition>
			<definition id="5">
				<sentence>LSI is an unsupervised factor analysis technique based on Singular Value Decomposition of a |X|× |F| example-feature matrix .</sentence>
				<definiendum id="0">LSI</definiendum>
				<definiens id="0">an unsupervised factor analysis technique based on Singular Value Decomposition of a |X|× |F| example-feature matrix</definiens>
			</definition>
			<definition id="6">
				<sentence>F consists of two disjoint subsets : FL and FL .</sentence>
				<definiendum id="0">F</definiendum>
			</definition>
			<definition id="7">
				<sentence>FL consists of features occurring in training set of target word in SENSEVAL-3 , while FL consists of features that occur in test set , but never appear in training set .</sentence>
				<definiendum id="0">FL</definiendum>
				<definiens id="0">consists of features occurring in training set of target word in SENSEVAL-3 , while FL consists of features that occur in test set , but never appear in training set</definiens>
			</definition>
			<definition id="8">
				<sentence>SuFC , IDC , SuFS , and SemiFC use label information to guide feature clustering or feature selection , while LSI is an unsupervised factor analysis method that can conduct dimensionality reduction without the use of label information from labeled data .</sentence>
				<definiendum id="0">LSI</definiendum>
				<definiens id="0">an unsupervised factor analysis method that can conduct dimensionality reduction without the use of label information from labeled data</definiens>
			</definition>
			<definition id="9">
				<sentence>FL consists of features occurring in sampled training set of target word in SENSEVAL3 , while FL consists of features that occur in unlabeled data ( including unselected training data and all the test set ) , but never appear in labeled data ( sampled training set ) .</sentence>
				<definiendum id="0">FL</definiendum>
				<definiens id="0">consists of features occurring in sampled training set of target word in SENSEVAL3 , while FL consists of features that occur in unlabeled data ( including unselected training data and all the test set ) , but never appear in labeled data ( sampled training set )</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>For example , centering theory ( Grosz et al. , 1995 ) deals predominantly with local salience ( local attentional status ) , and the givenness hierarchy ( information status ) of Prince ( 1992 ) focuses on how a referent got in the discourse model ( e.g. through a direct mention in the current discourse , through previous knowledge , or through inference ) , leading to distinctions such as discourseold , discourse-new , hearer-old , hearer-new , inferable and containing inferable .</sentence>
				<definiendum id="0">centering theory</definiendum>
				<definiens id="0">local attentional status ) , and the givenness hierarchy ( information status ) of Prince ( 1992 ) focuses on how a referent got in the discourse model ( e.g. through a direct mention in the current discourse , through previous knowledge , or through inference ) , leading to distinctions such as discourseold , discourse-new , hearer-old , hearer-new , inferable and containing inferable</definiens>
			</definition>
</paper>

		<paper id="2006">
			<definition id="0">
				<sentence>NooJ : A Linguistic Annotation System For Corpus Processing Max Silberztein LASELDI Université de Franche-Comté Besançon , 25000 France max.silberztein @ univ-fcomte.fr NooJ is a new corpus processing system , similar to the INTEX software , 1 and designed to replace it .</sentence>
				<definiendum id="0">NooJ</definiendum>
				<definiens id="0">A Linguistic Annotation System For Corpus Processing Max Silberztein LASELDI</definiens>
			</definition>
			<definition id="1">
				<sentence>NET “Component programming” technology , which goes a step beyond the Object-Oriented approach ( Silberztein 2004 ) .</sentence>
				<definiendum id="0">NET “Component programming” technology</definiendum>
				<definiens id="0">goes a step beyond the Object-Oriented approach ( Silberztein 2004 )</definiens>
			</definition>
			<definition id="2">
				<sentence>See various INTEX WEB sites for references and information on its applications , workshops and communities : http : //intex.univ-fcomte.fr and the NooJ WEB site for a description of NooJ : http : //www.nooj4nlp.net .</sentence>
				<definiendum id="0">INTEX WEB</definiendum>
				<definiendum id="1">NooJ WEB site</definiendum>
				<definiens id="0">sites for references and information on its applications , workshops and communities : http : //intex.univ-fcomte.fr and the</definiens>
			</definition>
			<definition id="3">
				<sentence>An annotation is a pair ( position , information ) that states that at a certain position in the text , a sequence is associated with a certain piece of information .</sentence>
				<definiendum id="0">annotation</definiendum>
				<definiens id="0">a pair ( position , information</definiens>
			</definition>
			<definition id="4">
				<sentence>NooJ contains largecoverage dictionaries for Arabic , Armenian , Chinese , Danish , English , French , Hungarian , Italian and Spanish .</sentence>
				<definiendum id="0">NooJ</definiendum>
				<definiens id="0">contains largecoverage dictionaries for Arabic , Armenian , Chinese , Danish , English , French , Hungarian , Italian and Spanish</definiens>
			</definition>
			<definition id="5">
				<sentence>The annotation system can represent all types of lexical ambiguities , such as between compounds and sequences of simple words ( e.g. “round table” ) , overlapping or embedded compounds ( e.g. “round table mat” ) , etc. -NooJ’s local grammars are Recursive Transition Networks ; they allow users to recognize certain sequences of texts , and to associate them with annotations .</sentence>
				<definiendum id="0">annotation system</definiendum>
				<definiens id="0">local grammars are Recursive Transition Networks ; they allow users to recognize certain sequences of texts , and to associate them with annotations</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>Information Structure ( IS ) is a partitioning of the content of a sentence according to its relation to the discourse context .</sentence>
				<definiendum id="0">Information Structure</definiendum>
			</definition>
			<definition id="1">
				<sentence>Information Structure is an important factor in determining the felicity of a sentence in a given context .</sentence>
				<definiendum id="0">Information Structure</definiendum>
				<definiens id="0">an important factor in determining the felicity of a sentence in a given context</definiens>
			</definition>
			<definition id="2">
				<sentence>The Prague Dependency Treebank ( PDT ) consists of newspaper articles from the Czech National Corpus ( ˇ Cerm´ak , 1997 ) and includes three layers of annotation : phemic analysis in which 13 categories are marked for all sentence tokens ( including punctuation marks ) .</sentence>
				<definiendum id="0">Prague Dependency Treebank</definiendum>
				<definiendum id="1">PDT )</definiendum>
				<definiens id="0">consists of newspaper articles from the Czech National Corpus ( ˇ Cerm´ak , 1997 ) and includes three layers of annotation : phemic analysis in which 13 categories</definiens>
			</definition>
			<definition id="3">
				<sentence>syntax ( Hajiˇc , 1998 ) is annotated , contains analytical tree structures , in which every token from the surface shape of the sentence has a corresponding node labeled with main syntactic functions like SUBJ , PRED , OBJ , ADV .</sentence>
				<definiendum id="0">syntax</definiendum>
			</definition>
			<definition id="4">
				<sentence>In the Praguian approach to IS , the content of the sentence is divided into two parts : the Topic is “what the sentence is about” and the Focus represents the information asserted about the Topic .</sentence>
				<definiendum id="0">Focus</definiendum>
				<definiens id="0">the information asserted about the Topic</definiens>
			</definition>
			<definition id="5">
				<sentence>Sentence ( a ) is the context in which the sentence ( b ) is uttered , and sentence ( c ) is the question for which the sentence ( b ) is an appropriate answer : ( 1 ) ( a ) Tom and Mary both came to John’s party .</sentence>
				<definiendum id="0">Sentence</definiendum>
				<definiendum id="1">sentence ( c )</definiendum>
				<definiens id="0">the context in which the sentence ( b ) is uttered</definiens>
			</definition>
			<definition id="6">
				<sentence>The following rules determine which lexical items ( CB or NB ) belong to the Topic or to the Focus of the sentence ( Hajiˇcov´a et al. , 1998 ; Hajiˇcov´a and Sgall , 2001 ) : 10 belong to the Focus if they are NB ; main verb and is subordinated to a Focus element belongs to the Focus ( where “subordinated to” is defined as the irreflexive transitive closure of “depend on” ) ; then those dependents d i of the verb which have subordinated items s m that are NB are called ‘proxi foci’ ; the items s m together with all items subordinated to them belong to the Focus ( i , m &gt; 1 ) ; ing to 1 – 3 belongs to the Topic .</sentence>
				<definiendum id="0">Focus</definiendum>
				<definiendum id="1">“subordinated to”</definiendum>
				<definiens id="0">lexical items ( CB or NB ) belong to the Topic or to the Focus of the sentence ( Hajiˇcov´a et al. , 1998 ; Hajiˇcov´a and Sgall , 2001 ) : 10 belong to the Focus if they are NB ; main verb</definiens>
			</definition>
			<definition id="7">
				<sentence>Applying these rules for the sentence ( b ) in example ( 1 ) we find the Topic and the Focus of the sentence : [ John invited ] Topic [ only her ] Focus .</sentence>
				<definiendum id="0">Applying these rules for the sentence</definiendum>
				<definiens id="0">find the Topic and the Focus of the sentence : [ John invited ] Topic [ only her ] Focus</definiens>
			</definition>
			<definition id="8">
				<sentence>0 10 20 30 40 50 60 70 80 90 Correctly Classified Instances % of Training Data Figure 3 : Learning curves for C4.5 ( + ) , RIPPER ( × ) , MaxEnt ( ∗ ) and a na¨ıve predictor ( a50 ) ( introduced in Section 3.3 ) .</sentence>
				<definiendum id="0">RIPPER</definiendum>
				<definiens id="0">Learning curves for C4.5 ( + )</definiens>
			</definition>
</paper>

		<paper id="1118">
			<definition id="0">
				<sentence>In question answering ( QA ) , however , the task is to match natural language questions to relevant answers within document collections .</sentence>
				<definiendum id="0">question answering</definiendum>
			</definition>
			<definition id="1">
				<sentence>Lucene is a widelyused open-source Java library with several extensions and useful features .</sentence>
				<definiendum id="0">Lucene</definiendum>
				<definiens id="0">a widelyused open-source Java library with several extensions and useful features</definiens>
			</definition>
			<definition id="2">
				<sentence>We can also combine constraints , for example , RootPOS keywords can be restricted to nouns that are in an object relation within the question .</sentence>
				<definiendum id="0">RootPOS</definiendum>
				<definiens id="0">keywords can be restricted to nouns that are in an object relation within the question</definiens>
			</definition>
			<definition id="3">
				<sentence>We may , for example , form a query with the following elements : ( 1 ) all plain text tokens ; ( 2 ) named entities ( ne ) boosted with factor 2 ; ( 3 ) RootHead bigrams where the root is in an object relation ; ( 4 ) RootRel keywords for all nouns .</sentence>
				<definiendum id="0">RootHead</definiendum>
				<definiens id="0">bigrams where the root is in an object relation</definiens>
			</definition>
			<definition id="4">
				<sentence>Therefore , we use the mean of the total reciprocal ranks ( MTRR ) , cf. ( Radev et al. , 2002 ) , to combine features of all three measures : MTRR = 1x xsummationdisplay i=1 summationdisplay d∈Ai 1 rankRi ( d ) Ai is the set of retrieved passages containing an answer to question number i ( subset of Ri ) and rankRi ( d ) is the rank of document d in the list of retrieved passages Ri .</sentence>
				<definiendum id="0">rankRi</definiendum>
				<definiens id="0">the mean of the total reciprocal ranks ( MTRR ) , cf. ( Radev et al. , 2002 ) , to combine features of all three measures : MTRR = 1x xsummationdisplay i=1 summationdisplay d∈Ai 1 rankRi ( d ) Ai is the set of retrieved passages containing an answer to question number i ( subset of Ri</definiens>
				<definiens id="1">the rank of document d in the list of retrieved passages Ri</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>We introduce discourse chunking ( i.e. , the identification of intra-sentential nucleus and satellite spans ) as an alternative to full-scale discourse parsing .</sentence>
				<definiendum id="0">discourse chunking</definiendum>
			</definition>
			<definition id="1">
				<sentence>For instance , Marcu ( 2000 ) proposes a summarisation algorithm that builds an RST tree for the entire text , and identifies its most important parts according to discourse salience .</sentence>
				<definiendum id="0">summarisation algorithm</definiendum>
			</definition>
			<definition id="2">
				<sentence>Alternatively , we could treat discourse chunking as two distinct subtasks involving two binary classifiers : a segmenter , which determines the chunk boundaries and assigns each token a chunk-initial ( B ) or non-chunkinitial tag ( I ) , and a labeller , which classifies each chunk identified by the segmenter as either nucleus ( NUC ) or satellite ( SAT ) .1 The second approach has a number of advantages .</sentence>
				<definiendum id="0">segmenter</definiendum>
				<definiendum id="1">labeller</definiendum>
				<definiendum id="2">NUC</definiendum>
				<definiendum id="3">satellite</definiendum>
				<definiens id="0">determines the chunk boundaries and assigns each token a chunk-initial ( B ) or non-chunkinitial tag</definiens>
				<definiens id="1">classifies each chunk identified by the segmenter as either nucleus</definiens>
			</definition>
			<definition id="3">
				<sentence>BoosTexter applies n-gram models when forming classification hypotheses for textvalued features .</sentence>
				<definiendum id="0">BoosTexter</definiendum>
				<definiens id="0">applies n-gram models when forming classification hypotheses for textvalued features</definiens>
			</definition>
			<definition id="4">
				<sentence>For the one-step chunking method , our training set consists of approximately 130,000 instances ( i.e. , tokens ) .</sentence>
				<definiendum id="0">training set</definiendum>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>GNIS covers cities , states , 1 www.ldc.upenn.edu/Projects/ACE/ and counties in the U.S. , which are classified as “civil” and “populated place” geographical entities .</sentence>
				<definiendum id="0">GNIS</definiendum>
				<definiens id="0">covers cities , states , 1 www.ldc.upenn.edu/Projects/ACE/ and counties in the U.S. , which are classified as “civil” and “populated place” geographical entities</definiens>
			</definition>
			<definition id="1">
				<sentence>ATLAS provided a simple list of countries and their capitals .</sentence>
				<definiendum id="0">ATLAS</definiendum>
			</definition>
			<definition id="2">
				<sentence>The resulting tagged corpus , called MAC-DEV , Tag Attribute Description CLASS Civil ( Political Region or Administrative Area , e.g. Country , Province , County ) , Ppl ( Populated Place , e.g. City , Town ) , Cap ( Country Capital , Provincial Capital , or County Seat ) G1 Country G2 Province ( State ) or Country-Capital G3 County or Independent City G4 City , Town ( Within County ) Table 2 : WAG Gazetteer Attributes Corpus Size Use How Annotated MAC1 6.51 million words with 61,720 place names ( 4553 distinct ) from GNIS Ambiguity Study ( Gigaword NYT Sept. 2001 ) ( Section 2 ) LexScan of all senses , no attributes marked MACDEV 124,175 place names ( 1229 distinct ) from WAG Development Corpus ( Gigaword AFP May 2002 ) ( Section 4 ) LexScan using attributes from WAG , with heuristic preference MACML 181,866 place names ( 1322 distinct ) from WAG Machine Learning Corpus ( Gigaword AP Worldwide January 2002 ) ( Section 5 ) LexScan using attributes from WAG , with heuristic preference HAC 83,872 words with 1275 place names ( 435 distinct ) from WAG .</sentence>
				<definiendum id="0">WAG Machine Learning Corpus ( Gigaword AP Worldwide January 2002 )</definiendum>
				<definiens id="0">WAG Gazetteer Attributes Corpus Size Use How Annotated MAC1 6.51 million words with 61,720 place names</definiens>
			</definition>
			<definition id="3">
				<sentence>, k } Values are the ordered tokens up to k positions to the left/right WkContext Value is the set of MI collocated terms found in windows of ± k tokens ( to the left and right ) TagDiscourse Value is the set of CLASS values represented by all toponyms from the document : e.g. , the set { civil , capital , ppl } CorefClass Value is the CLASS if any for a prior mention of a toponym in the document , or none Table 5 .</sentence>
				<definiendum id="0">WkContext Value</definiendum>
				<definiendum id="1">TagDiscourse Value</definiendum>
				<definiendum id="2">CorefClass Value</definiendum>
				<definiens id="0">the set of MI collocated terms found in windows of ± k tokens ( to the left and right</definiens>
				<definiens id="1">the set of CLASS values represented by all toponyms from the document : e.g. , the set { civil , capital , ppl }</definiens>
			</definition>
			<definition id="4">
				<sentence>The poor performance of the supervised approach can be attributed to the lack of human-annotated training data : HAC is a small , 83,872-word corpus .</sentence>
				<definiendum id="0">HAC</definiendum>
				<definiens id="0">a small , 83,872-word corpus</definiens>
			</definition>
</paper>

		<paper id="1126">
			<definition id="0">
				<sentence>The knowledge base consists of the following three kinds : glossary , frequently asked questions ( FAQ ) , and support articles .</sentence>
				<definiendum id="0">knowledge base</definiendum>
				<definiens id="0">consists of the following three kinds : glossary , frequently asked questions ( FAQ ) , and support articles</definiens>
			</definition>
			<definition id="1">
				<sentence>The information gain ( IG ) is defined as a criterion for the selection .</sentence>
				<definiendum id="0">information gain</definiendum>
				<definiendum id="1">IG</definiendum>
				<definiens id="0">a criterion for the selection</definiens>
			</definition>
			<definition id="2">
				<sentence>The IG represents a reduction of entropy , or how many retrieved documents can be eliminated by incorporating additional information ( a reply to a question in this case ) .</sentence>
				<definiendum id="0">IG</definiendum>
				<definiens id="0">a reduction of entropy , or how many retrieved documents</definiens>
			</definition>
			<definition id="3">
				<sentence>Therefore , we define IG H ( S ) for a candidate question S by the following equations .</sentence>
				<definiendum id="0">IG H ( S )</definiendum>
				<definiens id="0">a candidate question S by the following equations</definiens>
			</definition>
			<definition id="4">
				<sentence>H ( S ) =− n summationdisplay i=0 P ( i ) · log P ( i ) P ( i ) = |C i | summationtext n i=0 |C i | |C i | = summationdisplay D k ∈i CM ( D k ) Here , D k denotes the k-th retrieved document by matching the query to the KB , and CM ( D ) denotes the matching score of document D. Thus , C i represents the number of documents classified into category i by candidate question S , which is weighted with the matching score .</sentence>
				<definiendum id="0">H</definiendum>
				<definiendum id="1">D k</definiendum>
				<definiendum id="2">D )</definiendum>
				<definiens id="0">the k-th retrieved document by matching the query to the KB</definiens>
				<definiens id="1">the matching score of document D. Thus</definiens>
				<definiens id="2">the number of documents classified into category i by candidate question S , which is weighted with the matching score</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>Word-level alignment is a critical component of a wide range of NLP applications , such as construction of bilingual lexicons ( Melamed , 2000 ) , word sense disambiguation ( Diab and Resnik , 2002 ) , projection of language resources ( Yarowsky et al. , 2001 ) , and statistical machine translation .</sentence>
				<definiendum id="0">Word-level alignment</definiendum>
				<definiens id="0">a critical component of a wide range of NLP applications , such as construction of bilingual lexicons ( Melamed , 2000 ) , word sense disambiguation ( Diab and Resnik , 2002 ) , projection of language resources ( Yarowsky et al. , 2001 ) , and statistical machine translation</definiens>
			</definition>
			<definition id="1">
				<sentence>At the heart of the matter is a set of assumptions that word-alignment algorithms must make in order to reduce the hypothesis space , since word alignment is an exponential problem .</sentence>
				<definiendum id="0">word alignment</definiendum>
				<definiens id="0">a set of assumptions that word-alignment algorithms must make in order to reduce the hypothesis space</definiens>
			</definition>
			<definition id="2">
				<sentence>ALP starts with an initial alignment and then fills out ( i.e. , projects ) new word-level alignment relations ( i.e. , links ) from existing alignment relations .</sentence>
				<definiendum id="0">ALP</definiendum>
				<definiens id="0">starts with an initial alignment and then fills out ( i.e. , projects ) new word-level alignment relations ( i.e. , links ) from existing alignment relations</definiens>
			</definition>
			<definition id="3">
				<sentence>ALP attempts to find an ordered list of transformation rules ( within a prespecified search space ) to improve a baseline annotation .</sentence>
				<definiendum id="0">ALP</definiendum>
				<definiens id="0">attempts to find an ordered list of transformation rules ( within a prespecified search space ) to improve a baseline annotation</definiens>
			</definition>
			<definition id="4">
				<sentence>We show that ALP yields a significant reductions in alignment error rate over that of the best performing alignment system .</sentence>
				<definiendum id="0">ALP</definiendum>
			</definition>
			<definition id="5">
				<sentence>These approaches include an enhanced HMM alignment model that uses part-ofspeech tags ( Toutanova et al. , 2002 ) , a log-linear combination of IBM translation models and HMM models ( Och and Ney , 2003 ) , techniques that rely on dependency relations ( Cherry and Lin , 2003 ) , and a log-linear combination of IBM Model 3 alignment probabilities , POS tags , and bilingual dictionary coverage ( Liu et al. , 2005 ) .</sentence>
				<definiendum id="0">HMM models</definiendum>
			</definition>
			<definition id="6">
				<sentence>Initial Annotation Corpus Templates Rule Instantiation Best Rule Selection Rule Application Rules CorpusAnnotated Ground Truth Figure 1 : TBL Architecture As shown in Figure 1 , the input to TBL is an unannotated corpus that is first passed to an initial annotator and then iteratively updated through comparison to a manually-annotated reference set ( or ground truth ) .</sentence>
				<definiendum id="0">TBL</definiendum>
				<definiens id="0">an unannotated corpus that is first passed to an initial annotator</definiens>
			</definition>
			<definition id="7">
				<sentence>ALP is a TBL implementation that projects alignment links from an initial input alignment .</sentence>
				<definiendum id="0">ALP</definiendum>
				<definiens id="0">a TBL implementation that projects alignment links from an initial input alignment</definiens>
			</definition>
			<definition id="8">
				<sentence>We induce several variations of ALP by setting four parameters in different ways : 186 ei fj fj+1 NULL ei fj fj+1 Figure 2 : Graphical Representation of a Template We describe each of these below using the following definitions and notation : • E = e1 , ... , ei , ... , et is a sentence in language L1 and F = f1 , ... , fj , ... , fs is a sentence in language L2 .</sentence>
				<definiendum id="0">... , fs</definiendum>
				<definiens id="0">a sentence in language L1 and F = f1 , ... , fj ,</definiens>
				<definiens id="1">a sentence in language L2</definiens>
			</definition>
			<definition id="9">
				<sentence>The action portion involves the addition or deletion of an alignment link .</sentence>
				<definiendum id="0">action portion</definiendum>
			</definition>
			<definition id="10">
				<sentence>Condition Action ( i , j ) ∈A , ( i−1 , j−1 ) ∈V add ( i−1 , j−1 ) ( i , j ) ∈A , ( i−1 , j ) ∈V add ( i−1 , j ) ( i , j ) ∈A , ( i−1 , j + 1 ) ∈V add ( i−1 , j + 1 ) ( i , j ) ∈A , ( i , j−1 ) ∈V add ( i , j−1 ) ( i , j ) ∈A , ( i , j + 1 ) ∈V add ( i , j + 1 ) ( i , j ) ∈A , ( i+ 1 , j−1 ) ∈V add ( i+ 1 , j−1 ) ( i , j ) ∈A , ( i+ 1 , j ) ∈V add ( i+ 1 , j ) ( i , j ) ∈A , ( i+ 1 , j + 1 ) ∈V add ( i+ 1 , j + 1 ) ( i−1 , j−1 ) ∈A , ( i+ 1 , j + 1 ) ∈A , add ( i , j ) ( i , j ) ∈V ( i+ 1 , j−1 ) ∈A , ( i−1 , j + 1 ) ∈A , add ( i , j ) ( i , j ) ∈V Table 1 : Templates for Expanding the Alignment A According to a Validation Alignment V Existing alignment algorithms ( e.g. , GIZA++ ) are biased toward aligning some words , especially infrequent ones , in one language to many words in the other language in order to minimize the number of unaligned words , even if many incorrect alignment 2A thick line indicates an added link .</sentence>
				<definiendum id="0">Condition Action</definiendum>
				<definiendum id="1">∈A , ( i+ 1</definiendum>
				<definiens id="0">Templates for Expanding the Alignment A According to a Validation Alignment V Existing alignment algorithms</definiens>
			</definition>
			<definition id="11">
				<sentence>We define the predicate neighbor existsA ( i , j ) to denote whether there is an alignment link in the neighborhood of the link ( i , j ) in a given alignment A. For example , the first template deletes spurious links for a particular word ei in E. Condition Action ( i , j ) ∈A , ( i , k ) ∈A , neighbor existsA ( i , j ) , del ( i , k ) not ( neighbor existsA ( i , k ) ) ( i , j ) ∈A , ( k , j ) ∈A , neighbor existsA ( i , j ) , del ( e , j ) not ( neighbor existsA ( k , j ) ) Table 2 : Templates for Deleting Spurious Links in a Given Alignment A Current alignment algorithms produce one-to-one word correspondences quite successfully .</sentence>
				<definiendum id="0">neighbor existsA</definiendum>
				<definiens id="0">Templates for Deleting Spurious Links in a Given Alignment A Current alignment algorithms produce one-to-one word correspondences quite successfully</definiens>
			</definition>
			<definition id="12">
				<sentence>ALP starts with a set of templates and an initial alignment and attempts to instantiate the templates during the learning process .</sentence>
				<definiendum id="0">ALP</definiendum>
				<definiens id="0">starts with a set of templates and an initial alignment and attempts to instantiate the templates during the learning process</definiens>
			</definition>
			<definition id="13">
				<sentence>Thus , ALP adds new links based on linguistic features of words , rather than the words themselves .</sentence>
				<definiendum id="0">ALP</definiendum>
				<definiens id="0">adds new links based on linguistic features of words</definiens>
			</definition>
			<definition id="14">
				<sentence>In our experiments , we used a dependency parser only in English ( a version of the Collins parser ( Collins , 1997 ) that has been adapted for building dependencies ) but not in the other language .</sentence>
				<definiendum id="0">dependency parser</definiendum>
				<definiens id="0">a version of the Collins parser ( Collins , 1997 ) that has been adapted for building dependencies</definiens>
			</definition>
			<definition id="15">
				<sentence>To measure accuracy of a rule r , we usegood ( r ) −2×bad ( r ) , wheregood ( r ) is the number of alignment links that are corrected by the rule , and bad ( r ) is the number of incorrect alignment links produced .</sentence>
				<definiendum id="0">bad</definiendum>
				<definiens id="0">the number of alignment links that are corrected by the rule</definiens>
			</definition>
			<definition id="16">
				<sentence>Precision ( Pr ) , recall ( Rc ) and alignment error rate ( AER ) are defined as follows : Pr = |A∩P||A| Rc = |A∩S||S| AER = 1−|A∩S|+|A∩P||A|+|S| A manually aligned corpus is used as our gold standard .</sentence>
				<definiendum id="0">Precision ( Pr )</definiendum>
				<definiens id="0">recall ( Rc ) and alignment error rate ( AER ) are defined as follows : Pr = |A∩P||A| Rc</definiens>
			</definition>
			<definition id="17">
				<sentence>Tables 5–7 compare ALP to each of these four alignments using different settings of 4 parameters : ALP [ IA , T , I , BRS ] , where IA is the initial alignment , T is the set of templates , I is the instantiation method , and BRS is the metric for the best rule selection at each iteration .</sentence>
				<definiendum id="0">IA</definiendum>
				<definiendum id="1">T</definiendum>
				<definiendum id="2">BRS</definiendum>
				<definiens id="0">the initial alignment</definiens>
				<definiens id="1">the set of templates</definiens>
			</definition>
			<definition id="18">
				<sentence>Alignments Pr Rc AER Intersection ( Int ) 98.2 59.6 25.9 ALP [ Int , TE , gen , aer ] 90.9 69.9 21.0 ALP [ Int , ( TD , TMW ) , gen , aer ] 88.8 72.3 20.3 RA 83.8 74.4 21.2 Table 5 : ALP Results Using GIZA++ Intersection as Initial Alignment for English-Spanish Using the expansion templates ( TE ) against a val7We use only sure alignment links as the ground truth to learn rules inside ALP .</sentence>
				<definiendum id="0">Alignments Pr Rc AER Intersection</definiendum>
				<definiens id="0">the ground truth to learn rules inside ALP</definiens>
			</definition>
			<definition id="19">
				<sentence>In the best case , ALP achieves a statistically significant relative reduction of 21.6 % in AER over the Intersection alignment .</sentence>
				<definiendum id="0">ALP</definiendum>
				<definiens id="0">achieves a statistically significant relative reduction of 21.6 % in AER over the Intersection alignment</definiens>
			</definition>
			<definition id="20">
				<sentence>For the other direction , ALP produces a rel191 Alignments Pr Rc AER Intersection ( Int ) 94.8 53.6 31.2 ALP [ Int , ( TD , TMW ) , gen , aer ] 91.7 56.8 29.5 E-to-C 70.4 68.3 30.7 ALP [ E-to-C , ( TD , TMW ) , gen , aer ] 79.1 68.1 26.6 C-to-E 66.0 69.8 32.2 ALP [ C-to-E , ( TD , TMW ) , gen , aer ] 83.3 66.0 26.2 RA 61.9 82.6 29.7 ALP [ RA , ( TD , TMW ) , gen , aer ] 82.1 72.7 22.8 Table 8 : ALP Results Using Different Initial Alignments for English-Chinese ative reduction of 18.6 % in AER .</sentence>
				<definiendum id="0">ALP</definiendum>
				<definiens id="0">produces a rel191 Alignments Pr Rc AER Intersection</definiens>
				<definiens id="1">English-Chinese ative reduction of 18.6 % in AER</definiens>
			</definition>
			<definition id="21">
				<sentence>Finally , when RA is given to ALP as an initial alignment , ALP results in a relative reduction of 23.2 % in AER .</sentence>
				<definiendum id="0">RA</definiendum>
				<definiens id="0">an initial alignment</definiens>
			</definition>
			<definition id="22">
				<sentence>ALP takes advantage of clustering of alignment links to project new links given a reasonable initial alignment .</sentence>
				<definiendum id="0">ALP</definiendum>
				<definiens id="0">takes advantage of clustering of alignment links to project new links given a reasonable initial alignment</definiens>
			</definition>
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>The parser achieves a precision of more than 68 % on difficult test data , which is 23 % more than the baseline obtained by randomly choosing one of the simplest analyses .</sentence>
				<definiendum id="0">parser</definiendum>
				<definiens id="0">achieves a precision of more than 68 % on difficult test data , which is 23 % more than the baseline obtained by randomly choosing one of the simplest analyses</definiens>
			</definition>
			<definition id="1">
				<sentence>HL-PCFGs are therefore able to learn that nouns headed by Problem ( problem ) are more likely to be compounds ( e.g. Schulprobleme ( school problems ) ) than nouns headed by Samstag ( Saturday ) .</sentence>
				<definiendum id="0">HL-PCFGs</definiendum>
			</definition>
			<definition id="2">
				<sentence>The probability of a head-lexicalized parse tree is therefore : pstart ( cat ( root ) ) pstart ( head ( root ) |cat ( root ) ) ∗producttext n∈N prule ( rule ( n ) |cat ( n ) , head ( n ) ) ∗producttext n∈A phead ( head ( n ) |cat ( n ) , pcat ( n ) , phead ( n ) ) where root is the root node of the parse tree cat ( n ) is the syntactic category of node n head ( n ) is the lexical head of node n rule ( n ) is the grammar rule which expands node n pcat ( n ) is the syntactic category of the parent of n phead ( n ) is the lexical head of the parent of n HL-PCFGs have a large number of parameters which need to estimated from training data .</sentence>
				<definiendum id="0">n )</definiendum>
				<definiens id="0">pstart ( cat ( root ) ) pstart ( head ( root ) |cat ( root ) ) ∗producttext n∈N prule ( rule ( n ) |cat ( n ) , head ( n ) ) ∗producttext n∈A phead ( head ( n ) |cat ( n ) , pcat ( n ) , phead ( n ) ) where root is the root node of the parse tree cat ( n ) is the syntactic category of node n head ( n ) is the lexical head of node n rule</definiens>
			</definition>
			<definition id="3">
				<sentence>HL-PCFGs can either be trained on labeled data ( supervised training ) or on unlabeled data ( unsupervised training ) using the Inside-Outside algorithm , an instance of the EM algorithm .</sentence>
				<definiendum id="0">Inside-Outside algorithm</definiendum>
				<definiens id="0">trained on labeled data ( supervised training ) or on unlabeled data ( unsupervised training ) using the</definiens>
			</definition>
			<definition id="4">
				<sentence>SMOR ( Schmid et al. , 2004 ) is a German FSTbased morphological analyzer which covers inflection , compounding , and prefix as well as suffix derivation .</sentence>
				<definiendum id="0">SMOR</definiendum>
				<definiens id="0">a German FSTbased morphological analyzer which covers inflection , compounding , and prefix as well as suffix derivation</definiens>
			</definition>
			<definition id="5">
				<sentence>Andreas Eisele ( unpublished work ) implemented a statistical disambiguator for German based on weighted finite-state transducers as described in the introduction .</sentence>
				<definiendum id="0">Andreas Eisele</definiendum>
				<definiens id="0">implemented a statistical disambiguator for German based on weighted finite-state transducers as described in the introduction</definiens>
			</definition>
</paper>

		<paper id="2009">
</paper>

		<paper id="1117">
			<definition id="0">
				<sentence>A test collection consists of a corpus , a series of well-defined tasks , and a set of judgments indicating the “correct answers” .</sentence>
				<definiendum id="0">test collection</definiendum>
				<definiens id="0">consists of a corpus , a series of well-defined tasks , and a set of judgments indicating the “correct answers”</definiens>
			</definition>
			<definition id="1">
				<sentence>Following these recent developments in evalua931 1 vital 32 kilograms plutonium powered 2 vital seven year journey 3 vital Titan 4-B Rocket 4 vital send Huygens to probe atmosphere of Titan , Saturn’s largest moon 5 okay parachute instruments to planet’s surface 6 okay oceans of ethane or other hydrocarbons , frozen methane or water 7 vital carries 12 packages scientific instruments and a probe 8 okay NASA primary responsible for Cassini orbiter 9 vital explore remote planet and its rings and moons , Saturn 10 okay European Space Agency ESA responsible for Huygens probe 11 okay controversy , protest , launch failure , re-entry , lethal risk , humans , plutonium 12 okay Radioisotope Thermoelectric Generators , RTG 13 vital Cassini , NASA’S Biggest and most complex interplanetary probe 14 okay find information on solar system formation 15 okay Cassini Joint Project between NASA , ESA , and ASI ( Italian Space Agency ) 16 vital four year study mission Table 1 : The “answer key” to the question “What is the Cassini space probe ? ”</sentence>
				<definiendum id="0">NASA’S Biggest</definiendum>
				<definiens id="0">information on solar system formation 15 okay Cassini Joint Project between NASA , ESA , and ASI ( Italian Space Agency</definiens>
			</definition>
			<definition id="2">
				<sentence>To evaluate system responses , NIST pools answer strings from all systems , removes their association with the runs that produced them , and presents them to a human assessor .</sentence>
				<definiendum id="0">NIST</definiendum>
				<definiens id="0">pools answer strings from all systems , removes their association with the runs that produced them , and presents them to a human assessor</definiens>
			</definition>
			<definition id="3">
				<sentence>An information nugget is defined as a fact for which the assessor could make a binary decision as to whether a response contained that nugget ( Voorhees , 2003 ) .</sentence>
				<definiendum id="0">information nugget</definiendum>
			</definition>
			<definition id="4">
				<sentence>The metric is a harmonic mean between nugget precision and nugget recall , where recall is heavily favored ( controlled by the β parameter , set to five in 2003 and three in 2004 ) .</sentence>
				<definiendum id="0">metric</definiendum>
			</definition>
			<definition id="5">
				<sentence>We decided to capture this intuition using inverse document frequency , a commonly-used measure in information retrieval ; idf ( ti ) is defined as log ( N/ci ) , where N is the number of documents in the collection , and ci is the number of documents that contain the term ti .</sentence>
				<definiendum id="0">inverse document frequency</definiendum>
				<definiendum id="1">idf</definiendum>
				<definiendum id="2">N</definiendum>
				<definiendum id="3">ci</definiendum>
				<definiens id="0">a commonly-used measure in information retrieval</definiens>
				<definiens id="1">log ( N/ci ) , where</definiens>
				<definiens id="2">the number of documents in the collection , and</definiens>
				<definiens id="3">the number of documents that contain the term ti</definiens>
			</definition>
			<definition id="6">
				<sentence>With scoring based on idf , term counts are simply replaced with idf sums in computing the match score , i.e. , the match score of a particular nugget is the sum of the idfs of matching terms in the system response divided by the sum of all term idfs from the answer nugget .</sentence>
				<definiendum id="0">match score</definiendum>
				<definiens id="0">the sum of the idfs of matching terms in the system response divided by the sum of all term idfs from the answer nugget</definiens>
			</definition>
</paper>

		<paper id="1062">
			<definition id="0">
				<sentence>Named Entity Recognition ( NER ) is a crucial step in many Information Extraction ( IE ) tasks .</sentence>
				<definiendum id="0">Named Entity Recognition</definiendum>
				<definiendum id="1">NER )</definiendum>
			</definition>
			<definition id="1">
				<sentence>The ESTER corpus is made of 100 hours of Broadcast News data ( from 6 French speaking radio channels ) , manually transcribed , and labeled with a tagset of about 30 Named Entity categories folded in 8 main types : • persons ( pers ) : human beings , fiction characters , animals ; • locations ( loc ) : geographical , traffic lines , electronic and real addresses , dial numbers ; • organizations ( org ) : political , business , non profit ; • geo-socio-political groups ( gsp ) : clans , families , nations , administrative regions ; • amounts ( amount ) : durations , money , lengths , temperature , age , weight and speed ; • time ( time ) : relative and absolute time expressions , hours ; • products ( prod ) : art , printings , awards and vehicles ; 492 • facilities ( fac ) : buildings , monuments .</sentence>
				<definiendum id="0">ESTER corpus</definiendum>
			</definition>
			<definition id="2">
				<sentence>1 , 1 , .5 , .5 , .8 , 1.5 ) ; P is the precision and R the recall ; F1 is used in this paper .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the precision and R the recall ; F1 is used in this paper</definiens>
			</definition>
			<definition id="3">
				<sentence>For instance , many document retrieval models use inverse document frequency ( rareness ) as a word weighting parameter .</sentence>
				<definiendum id="0">inverse document frequency</definiendum>
				<definiens id="0">a word weighting parameter</definiens>
			</definition>
			<definition id="4">
				<sentence>Similarly , first and last words of entities are represented by special tags ( this helps getting more accurate boundaries ) and low frequency words ( appearing less than a fixed number of times in the training corpus ) are generalized using their Part-Of-Speech tags .</sentence>
				<definiendum id="0">special tags</definiendum>
				<definiens id="0">appearing less than a fixed number of times in the training corpus</definiens>
			</definition>
</paper>

		<paper id="1086">
			<definition id="0">
				<sentence>Sentence retrieval is the task of retrieving a relevant sentence in response to a user’s query .</sentence>
				<definiendum id="0">Sentence retrieval</definiendum>
				<definiens id="0">the task of retrieving a relevant sentence in response to a user’s query</definiens>
			</definition>
			<definition id="1">
				<sentence>The TREC Novelty Track provides this type of data in the form of topic titles and descriptions , and sentence-level relevance judgments for a small subset of the collection .</sentence>
				<definiendum id="0">TREC Novelty Track</definiendum>
				<definiens id="0">provides this type of data in the form of topic titles and descriptions</definiens>
			</definition>
			<definition id="2">
				<sentence>We estimate this probability by interpolating the term distribution in the sentence with the term distribution in the collection : P ( QjS ) = P ( S ) |Q|productdisplay i=1 parenleftbigg λP ( qijS ) + ( 1 λ ) P ( qijC ) parenrightbigg ( 3 ) where Q is the query , S is the sentence , P ( S ) is the ( uniform ) prior probability of the sentence , P ( qijS ) is the probability that term qi in the query appears in the sentence , and P ( qijC ) is the probability that qi appears in the collection .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">S</definiendum>
				<definiendum id="2">P ( qijS</definiendum>
				<definiens id="0">this probability by interpolating the term distribution in the sentence with the term distribution in the collection : P ( QjS ) = P ( S ) |Q|productdisplay i=1 parenleftbigg λP</definiens>
				<definiens id="1">the query</definiens>
				<definiens id="2">the sentence</definiens>
				<definiens id="3">the ( uniform ) prior probability of the sentence</definiens>
				<definiens id="4">the probability that term qi in the query appears in the sentence</definiens>
				<definiens id="5">the probability that qi appears in the collection</definiens>
			</definition>
			<definition id="3">
				<sentence>In the experiments with document smoothing , we estimate the probability of a sentence generating the query : P ( QjS ) = P ( S ) |Q|productdisplay i=1 parenleftbigg αP ( qijS ) + βP ( qijDS ) + γP ( qijC ) parenrightbigg ( 4 ) where α+β +γ = 1.0 and P ( qijDS ) is the probability that the term qi in the query appears in the document the sentence came from .</sentence>
				<definiendum id="0">P ( qijDS )</definiendum>
				<definiens id="0">the probability of a sentence generating the query : P ( QjS ) = P ( S</definiens>
				<definiens id="1">the probability that the term qi in the query appears in the document the sentence came from</definiens>
			</definition>
			<definition id="4">
				<sentence>The Novelty queries from 2002 were included in the “event” set .</sentence>
				<definiendum id="0">Novelty</definiendum>
				<definiens id="0">queries from 2002 were included in the “event” set</definiens>
			</definition>
			<definition id="5">
				<sentence>The TREC corpus consisted of approximately 15,000 sentence pairs , whereas the Arabic-English corpus was trained on more than a million sentence pairs .</sentence>
				<definiendum id="0">TREC corpus</definiendum>
				<definiens id="0">consisted of approximately 15,000 sentence pairs</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>While CRFs model source identification as a sequence tagging task , AutoSlog learns extraction patterns .</sentence>
				<definiendum id="0">AutoSlog</definiendum>
				<definiens id="0">learns extraction patterns</definiens>
			</definition>
			<definition id="1">
				<sentence>We define the set of possible label values as ’S’ , ’T’ , ’-’ , where ’S’ is the first token ( or Start ) of a source , ’T’ is a non-initial token ( i.e. , a conTinuation ) of a source , and ’-’is a token that is not part of any source.2 A detailed description of CRFs can be found in 2This is equivalent to the IOB tagging scheme used in syntactic chunkers ( Ramshaw and Marcus , 1995 ) .</sentence>
				<definiendum id="0">’S’</definiendum>
				<definiendum id="1">’T’</definiendum>
				<definiendum id="2">conTinuation</definiendum>
				<definiens id="0">a non-initial token</definiens>
			</definition>
			<definition id="2">
				<sentence>For each sentence x , we define a non-negative clique potential exp ( summationtextKk=1 λkfk ( yi−1 , yi , x ) ) for each edge , and exp ( summationtextKprimek=1 λprimekfprimek ( yi , x ) ) for each node , where fk ( ... ) is a binary feature indicator function , λk is a weight assigned for each feature function , and K and Kprime are the number of features defined for edges and nodes respectively .</sentence>
				<definiendum id="0">fk ( ... )</definiendum>
				<definiendum id="1">λk</definiendum>
				<definiens id="0">a binary feature indicator function</definiens>
				<definiens id="1">a weight assigned for each feature function , and K and Kprime are the number of features defined for edges and nodes respectively</definiens>
			</definition>
			<definition id="3">
				<sentence>Following Lafferty et al. ( 2001 ) , the conditional probability of a sequence of labels y given a sequence of tokens x is : P ( y|x ) = 1Z x exp „X i , k λk fk ( yi−1 , yi , x ) + X i , k λprimek fprimek ( yi , x ) « ( 1 ) Zx = X y exp „X i , k λk fk ( yi−1 , yi , x ) + X i , k λprimek fprimek ( yi , x ) « ( 2 ) where Zx is a normalization constant for each x. Given the training data D , a set of sentences paired with their correct ’ST-’ source label sequences , the parameters of the model are trained to maximize the conditional log-likelihoodproducttext ( x , y ) ∈D P ( yjx ) .</sentence>
				<definiendum id="0">log-likelihoodproducttext</definiendum>
				<definiens id="0">λprimek fprimek ( yi , x ) « ( 2 ) where Zx is a normalization constant for each x. Given the training data D , a set of sentences paired with their correct ’ST-’ source label sequences , the parameters of the model are trained to maximize the conditional</definiens>
			</definition>
			<definition id="4">
				<sentence>Sundance uses named entity recognition rules to label noun phrases as belonging to named entity classes , and assigns semantic tags to individual words based on a semantic dictionary .</sentence>
				<definiendum id="0">Sundance</definiendum>
				<definiens id="0">uses named entity recognition rules to label noun phrases as belonging to named entity classes , and assigns semantic tags to individual words based on a semantic dictionary</definiens>
			</definition>
			<definition id="5">
				<sentence>AutoSlog ( Riloff , 1996a ) is a supervised extraction pattern learner that takes a training corpus of texts and their associated answer keys as input .</sentence>
				<definiendum id="0">AutoSlog</definiendum>
				<definiens id="0">a supervised extraction pattern learner that takes a training corpus of texts and their associated answer keys as input</definiens>
			</definition>
			<definition id="6">
				<sentence>The final system , which combines the basic features , the IE pattern features , and feature induction achieves an F-measure of 69.4 ( recall=60.6 % , precision=81.2 % ) for the OL measure , an F-measure of 68.0 ( recall=59.5 % , precision=79.3 % ) for the HM measure , and an F-measure of 62.0 ( recall=54.1 % , precision=72.7 % ) for the EM measure .</sentence>
				<definiendum id="0">final system</definiendum>
				<definiens id="0">combines the basic features , the IE pattern features , and feature induction achieves an F-measure of 69.4</definiens>
			</definition>
			<definition id="7">
				<sentence>AutoSlog is a supervised learner that requires annotated training data but does not compute statistics .</sentence>
				<definiendum id="0">AutoSlog</definiendum>
				<definiens id="0">a supervised learner that requires annotated training data but does not compute statistics</definiens>
			</definition>
			<definition id="8">
				<sentence>AutoSlog-TS is a weakly supervised learner that does not require annotated data but generates coarse statistics that measure each pattern’s correlation with relevant and irrelevant documents .</sentence>
				<definiendum id="0">AutoSlog-TS</definiendum>
				<definiens id="0">a weakly supervised learner that does not require annotated data but generates coarse statistics that measure each pattern’s correlation with relevant and irrelevant documents</definiens>
			</definition>
			<definition id="9">
				<sentence>OpinionFinder : A system for subjectivity analysis .</sentence>
				<definiendum id="0">OpinionFinder</definiendum>
				<definiens id="0">A system for subjectivity analysis</definiens>
			</definition>
			<definition id="10">
				<sentence>Sentiment Analyzer : Extracting Sentiments about a Given Topic using Natural Language Processing Techniques .</sentence>
				<definiendum id="0">Sentiment Analyzer</definiendum>
			</definition>
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>Mgmt-Game is a subcorpora consisting of all emails written over a five-day period .</sentence>
				<definiendum id="0">Mgmt-Game</definiendum>
				<definiens id="0">a subcorpora consisting of all emails written over a five-day period</definiens>
			</definition>
			<definition id="1">
				<sentence>In MUC-6 , for instance , almost 80 % of the 446 Single-Document Repetition 0 % 10 % 20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 % 100 % Mgmt Game Mgmt Teams Enron Meetings Enron Random MUC-6 To ke n R ec all SDR CRF SDR+CRF ( a ) SDR Multiple-Document Repetition 0 % 10 % 20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 % 100 % Mgmt Game Mgmt Teams Enron Meetings Enron Random MUC-6 To ke n R ec all MDR CRF MDR+CRF ( b ) MDR Figure 3 : Upper bounds on recall and recall improvements associated with methods that look for terms that re-occur within a single document ( SDR ) or across multiple documents ( MDR ) .</sentence>
				<definiendum id="0">MDR CRF MDR+CRF</definiendum>
				<definiendum id="1">single document</definiendum>
				<definiendum id="2">SDR</definiendum>
				<definiens id="0">Upper bounds on recall and recall improvements associated with methods that look for terms that re-occur within a</definiens>
			</definition>
			<definition id="2">
				<sentence>Analogously , the multiple document repetition ( MDR ) rule marks every token that occurs in more than one document as a name .</sentence>
				<definiendum id="0">multiple document repetition</definiendum>
				<definiendum id="1">rule</definiendum>
				<definiens id="0">marks every token that occurs in more than one document as a name</definiens>
			</definition>
			<definition id="3">
				<sentence>The first metric , predicted frequency ( PF ) , estimates the degree to which a word appears to be used consistently as a name throughout the corpus : PF ( w ) ≡ cpf ( w ) ctf ( w ) where cpf ( w ) denotes the number of times that a word w is predicted as part of a name by the extractor , and ctf ( w ) is the number of occurrences of the word w in the entire test corpus ( we emphasize that estimating this statistic based on test data is valid , as it is fully automatic ”blind” procedure ) .</sentence>
				<definiendum id="0">frequency</definiendum>
				<definiendum id="1">PF</definiendum>
				<definiendum id="2">cpf ( w )</definiendum>
				<definiendum id="3">ctf ( w )</definiendum>
				<definiens id="0">estimates the degree to which a word appears to be used consistently as a name throughout the corpus</definiens>
				<definiens id="1">the number of times that a word w is predicted as part of a name by the extractor</definiens>
				<definiens id="2">the number of occurrences of the word w in the entire test corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>We elected to use the inverse document frequency ( IDF ) of w to measure word frequency : IDF ( w ) ≡ log ( N+0.5df ( w ) ) log ( N + 1 ) Here df ( w ) is the number of documents that contain a word w , and N is the total number of documents in the corpus .</sentence>
				<definiendum id="0">inverse document frequency</definiendum>
				<definiendum id="1">IDF</definiendum>
				<definiendum id="2">IDF</definiendum>
				<definiendum id="3">N</definiendum>
				<definiens id="0">the number of documents that contain a word w</definiens>
				<definiens id="1">the total number of documents in the corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>IDF takes into consideration both the probability of a word being a name , and how common it is in the entire corpus .</sentence>
				<definiendum id="0">IDF</definiendum>
				<definiens id="0">takes into consideration both the probability of a word being a name</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>Motoda and Yoshida ( 1997 ) and Davison and Hirsch ( 1998 ) developed a Unix shell which predicts the command stubs that a user is most likely to enter , given the current history of entered commands .</sentence>
				<definiendum id="0">Unix shell</definiendum>
				<definiens id="0">predicts the command stubs that a user is most likely to enter , given the current history of entered commands</definiens>
			</definition>
			<definition id="1">
				<sentence>Assistance tools have furthermore been developed for translators .</sentence>
				<definiendum id="0">Assistance tools</definiendum>
				<definiens id="0">developed for translators</definiens>
			</definition>
			<definition id="2">
				<sentence>Recall equals the fraction of saved keystrokes ( disregarding the interface-dependent single keystroke that is most likely required to accept a suggestion ) ; precision is the ratio of characters that the users have to scan for each character they accept .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiendum id="1">precision</definiendum>
				<definiens id="0">equals the fraction of saved keystrokes ( disregarding the interface-dependent single keystroke that is most likely required to accept a suggestion ) ;</definiens>
			</definition>
			<definition id="3">
				<sentence>δt , s ( wprime1 , ... , wprimeN|wt−N+2 , ... , wt ) ( 6 ) = max wt+1 , ... , wt+s P ( wt+1 , ... , wt+s , wt+s+1 = wprime1 , ... , wt+s+N = wprimeN|wt−N+2 , ... , wt ) = max wt+1 , ... , wt+s P ( wprimeN|wprime1 , ... , wprimeN−1 ) ( 7 ) P ( wt+1 , ... , wt+s , wt+s+1 = wprime1 , ... , wt+s+N−1 = wprimeN−1|wt−N+2 , ... , wt ) = max wprime0 max wt+1 , ... , wt+s−1 P ( wprimeN|wprime1 , ... , wprimeN−1 ) ( 8 ) P ( wt+1 , ... , wt+s−1 , wt+s = wprime0 , ... , wt+s+N−1 = wprimeN−1|wt−N+2 , ... , wt ) = max wprime0 P ( wprimeN|wprime1 , ... , wprimeN−1 ) δt , s−1 ( wprime0 , ... , wprimeN−1|wt+N−2 , ... , wt ) ( 9 ) Exploiting the N-th order Markov assumption , we can now express our target probability ( Equation 3 ) in terms of δ in Equation 10 .</sentence>
				<definiendum id="0">, wt )</definiendum>
				<definiens id="0">s ( wprime1 , ... , wprimeN|wt−N+2 , ... , wt ) ( 6 ) = max wt+1 , ... , wt+s P ( wt+1 , ... , wt+s , wt+s+1 = wprime1 , ... , wt+s+N = wprimeN|wt−N+2 , ... , wt ) = max wt+1 , ... , wt+s P ( wprimeN|wprime1 , ... , wprimeN−1 ) ( 7 ) P ( wt+1 , ... , wt+s , wt+s+1 = wprime1 , ... , wt+s+N−1 = wprimeN−1|wt−N+2 , ... , wt ) = max wprime0 max wt+1 , ... , wt+s−1 P ( wprimeN|wprime1 , ...</definiens>
				<definiens id="1">wprimeN−1|wt−N+2 , ... , wt ) = max wprime0 P ( wprimeN|wprime1 , ... , wprimeN−1 ) δt , s−1 ( wprime0 , ... , wprimeN−1|wt+N−2 , ...</definiens>
			</definition>
			<definition id="4">
				<sentence>By contrast , when 197 1 0 0.2 0.4 0.6 Precision Recall service center N-graminstance-based 0 0 0.01 0.02 0.03 0.04 0.05 Precision Recall Enron emails N-graminstance-based 0 1 0 0.02 0.04 0.06 Precision Recall weather reports N-graminstance-based 0 1 0 0.05 0.1 0.15 Precision Recall cooking recipes N-graminstance-based Figure 2 : Precision recall curves for N-gram and instance-based methods of sentence completion .</sentence>
				<definiendum id="0">Precision Recall Enron</definiendum>
				<definiens id="0">emails N-graminstance-based 0 1 0 0.02 0.04 0.06 Precision Recall weather reports N-graminstance-based 0 1 0 0.05 0.1 0.15 Precision Recall cooking recipes N-graminstance-based Figure 2 : Precision recall curves for N-gram and instance-based methods of sentence completion</definiens>
			</definition>
			<definition id="5">
				<sentence>We find precision ( the number of suggested characters that the user has to read for every character that is accepted ) and recall ( the rate of keystroke savings ) to be appropriate performance metrics .</sentence>
				<definiendum id="0">recall</definiendum>
				<definiens id="0">the number of suggested characters that the user has to read for every character that is accepted ) and</definiens>
			</definition>
</paper>

		<paper id="1093">
			<definition id="0">
				<sentence>BLANC implements a practical algorithm with learnable parameters for automatic MT evaluation which estimates the reference-candidate translation overlap by computing a weighted sum of common subsequences ( also known as skip-ngrams ) .</sentence>
				<definiendum id="0">BLANC</definiendum>
			</definition>
			<definition id="1">
				<sentence>BLEU is a weighted precision evaluation metric introduced by IBM ( Papineni et al. , 2001 ) .</sentence>
				<definiendum id="0">BLEU</definiendum>
			</definition>
			<definition id="2">
				<sentence>Our algorithm – ACS ( all common skip-ngrams ) – directly constructs the set of overlapping skip-ngrams through incremental composition of word-level matches .</sentence>
				<definiendum id="0">ACS</definiendum>
				<definiens id="0">all common skip-ngrams ) – directly constructs the set of overlapping skip-ngrams through incremental composition of word-level matches</definiens>
			</definition>
			<definition id="3">
				<sentence>wSKIPmin ( |X| , |Y | ) } ( 8 ) where wSkipn is the set of all skip-ngrams of size n and is defined as : wSKIPn = { “w1 .</sentence>
				<definiendum id="0">wSKIPmin</definiendum>
				<definiendum id="1">wSkipn</definiendum>
				<definiens id="0">the set of all skip-ngrams of size n and is defined as : wSKIPn = { “w1</definiens>
			</definition>
			<definition id="4">
				<sentence>n ] } Given two sentences X and Y we observe a match ( w , x , y ) if word w is found in sentence X at index x and in sentence Y at index y : ( w , x , y ) ≡ { 0≤x≤|X|,0≤y≤|Y| , w∈V , and X [ x ] = Y [ y ] = w } ( 9 ) where V is the vocabulary with a finite set of words .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">the vocabulary with a finite set of words</definiens>
			</definition>
			<definition id="5">
				<sentence>// depth first search ( DFS ) for each node N in DAG compute node N’s depth // initialize skip-ngram counts for each node N in DAG vN [ 1 ] ←1 for i=2 to LCS ( X , Y ) vN [ i ] = 0 // compute ngram counts for d=1 to MAXDEPTH for each node N of depth d in DAG for each edge E : N→M for i=2 to d vM [ i ] += Sxy ( δ ( E ) , ϕ ( E ) , vN [ i-1 ] ) After algorithm ACS is run , the number of skipngrams ( weighted skip-ngram score ) of size k is simply the sum of the number of skip-ngrams of size k ending in each node N’s corresponding match : wSKIPk = summationdisplay Ni∈DAG vNi [ k ] ( 10 ) In the worst case scenario , both sentences X and Y are composed of exactly the same repeated word : X = “w w w w .</sentence>
				<definiendum id="0">DFS</definiendum>
				<definiens id="0">composed of exactly the same repeated word : X = “w w w w</definiens>
			</definition>
			<definition id="6">
				<sentence>In the worst case scenario , the number of directed edges is O ( M2 ) and furthermore if the sentences are uniformly composed of the same repeated word as seen above , the worst-case time and space complexity is m ( m+1 ) /2·n ( n+1 ) /2 = O ( m2n2 ) .</sentence>
				<definiendum id="0">space complexity</definiendum>
				<definiens id="0">if the sentences are uniformly composed of the same repeated word as seen above , the worst-case time and</definiens>
			</definition>
			<definition id="7">
				<sentence>Although dynamic programming or suffix trees can be used to compute the LCS much faster , under this parameter setting the ACS algorithm can also produce the longest common subsequence : LCS ( X , Y ) ←argmax k ACS ( wSKIPk ) &gt; 0 where Acs ( wSKIPk ) is the number of common skip-ngrams ( common subsequences ) produced by the ACS algorithm .</sentence>
				<definiendum id="0">LCS</definiendum>
				<definiens id="0">the number of common skip-ngrams ( common subsequences ) produced by the ACS algorithm</definiens>
			</definition>
			<definition id="8">
				<sentence>ROUGE-W uses the polynomial function xa in the weighted LCS computation .</sentence>
				<definiendum id="0">ROUGE-W</definiendum>
				<definiens id="0">uses the polynomial function xa in the weighted LCS computation</definiens>
			</definition>
			<definition id="9">
				<sentence>BLANC proves to be robust across criteria and granularity levels .</sentence>
				<definiendum id="0">BLANC</definiendum>
				<definiens id="0">proves to be robust across criteria and granularity levels</definiens>
			</definition>
</paper>

		<paper id="1075">
			<definition id="0">
				<sentence>Here , topic means an aspect of biographical facts , e.g. , marriage , children , birthplace , and so on .</sentence>
				<definiendum id="0">topic</definiendum>
				<definiens id="0">means an aspect of biographical facts , e.g. , marriage , children , birthplace , and so on</definiens>
			</definition>
			<definition id="1">
				<sentence>Instance-based question answering : a data driven approach .</sentence>
				<definiendum id="0">Instance-based question answering</definiendum>
				<definiens id="0">a data driven approach</definiens>
			</definition>
</paper>

		<paper id="1107">
			<definition id="0">
				<sentence>TYPE ( Ssel ) : number of types in Ssel .</sentence>
				<definiendum id="0">TYPE ( Ssel</definiendum>
				<definiens id="0">number of types in Ssel</definiens>
			</definition>
			<definition id="1">
				<sentence>MWC : randomly choose Ssel ⊂ S such that|Ssel| ≤ M. For each sentence si in Ssel find a sentence rj in Sunsel which maximizes swap score ( si , rj ) .</sentence>
				<definiendum id="0">MWC</definiendum>
				<definiens id="0">sentence si in Ssel find a sentence rj in Sunsel which maximizes swap score</definiens>
			</definition>
			<definition id="2">
				<sentence>Given a trained model , the most likely tag sequence ˆT = { t1 , t2 , ... tn } is computed for the input word sentence : ˆW = { w1 , w2 , ... wn } : ˆT = arg max T P ( T|W ) = arg max T P ( T|W ) P ( T ) The transition probability P ( T ) is approximated by a trigram model : P ( T ) ≈ p ( t1 ) p ( t2|t1 ) nproductdisplay i=3 p ( ti|ti−1 , ti−2 ) , and the observation probability P ( W|T ) is computed by P ( W|T ) ≈ nproductdisplay i=1 p ( wi|ti ) .</sentence>
				<definiendum id="0">observation probability P</definiendum>
				<definiens id="0">the most likely tag sequence ˆT = { t1 , t2 , ... tn } is computed for the input word sentence : ˆW = { w1 , w2 , ... wn } : ˆT = arg max T P ( T|W ) = arg max T P ( T|W ) P ( T ) The transition probability P ( T ) is approximated by a trigram model : P ( T ) ≈ p</definiens>
			</definition>
			<definition id="3">
				<sentence>Based on this principle , one way of merging these two taggers into a single HMM ( denoted as Tinterp ) is to use interpolation : pinterp ( w|t ) = λ×panno ( w|t ) + ( 1−λ ) ×pproj ( w|t ) pinterp ( ti|ti−1 , ti−2 ) = panno ( ti|ti−1 , ti−2 ) where λ is a tunable weighting parameter1 of the merged tagger .</sentence>
				<definiendum id="0">HMM</definiendum>
				<definiendum id="1">λ</definiendum>
				<definiens id="0">a tunable weighting parameter1 of the merged tagger</definiens>
			</definition>
			<definition id="4">
				<sentence>The discounting coefficient is computed using the Witten-Bell discounting method : α ( t ) = Canno ( t ) C anno ( t ) +Sanno ( t ) , where Canno ( t ) is the count of tokens whose tag is t in the manually annotated corpus and 1In our experiments , the value of λ is set to 0.8 based on held-out data .</sentence>
				<definiendum id="0">discounting coefficient</definiendum>
				<definiens id="0">the count of tokens whose tag is t in the manually annotated corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>Another is that the success of the merged HMM tagger hinges on the goodness of the observation probabilities , p ( w|t ) .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the success of the merged HMM tagger hinges on the goodness of the observation probabilities</definiens>
			</definition>
</paper>

		<paper id="1082">
			<definition id="0">
				<sentence>Intrinsic evaluations of information extraction ( IE ) have a history dating back to the Third Message Understanding Conference 1 ( MUC-3 ) and continuing today in the Automatic Content Extraction ( ACE ) evaluations .</sentence>
				<definiendum id="0">information extraction</definiendum>
			</definition>
			<definition id="1">
				<sentence>tion , and Extraction Source documents were selected based on the availability of manual annotation .</sentence>
				<definiendum id="0">Extraction Source</definiendum>
				<definiens id="0">documents were selected based on the availability of manual annotation</definiens>
			</definition>
			<definition id="2">
				<sentence>FactBrowser allows one to enter a string , which 656 is matched against the database of entity mentions .</sentence>
				<definiendum id="0">FactBrowser</definiendum>
				<definiens id="0">656 is matched against the database of entity mentions</definiens>
			</definition>
			<definition id="3">
				<sentence>657 Experiment 1 Performance and Time vs DB Blend 56 68 75 82 202 174 152 140 50 55 60 65 70 75 80 85 90 95 100 DB Blend ( % Human ) Pe r f or m a nc e ( % Cor r e c t ) 100 125 150 175 200 225 Ti m e ( s e c onds ) Experiment 2 Performance and Time vs DB Blend 52 61 64 70 210 187 183 166 50 55 60 65 70 75 80 85 90 95 100 DB Blend ( % Human ) P e r f or m a nce ( % Cor r e ct ) 100 125 150 175 200 225 Ti m e ( s e cond s ) Experiment 3 Performance and Time vs DB Blend 61 63 68 67 173 171 164 178 50 55 60 65 70 75 80 85 90 95 100 DB Blend ( % Human ) Pe rfo r ma n ce ( % C o rre ct ) 100 125 150 175 200 Ti m e ( s ec onds ) Experiments 1 &amp; 2 Performance and Time vs DB Blend 54 61 66 70 75 82 206 187 179 166 152 140 50 55 60 65 70 75 80 85 90 95 100 DB Blend ( % Human ) Pe r f or m a nc e ( % Cor r e c t ) 100 125 150 175 200 225 Ti m e ( s e c onds ) % Correct Blend Lower Bound Logistic Fit for Blend Blend Upper Bound Time Figure 4 QA Performance ( upward-sloping ) and QA Time ( downward-sloping ) vs. Extraction Blend Error Bars are Plus/Minus Standard Error of Mean ( SEM ) Within Each Blend Upper and Lower Bounds Are Approximate 95 % Confidence Intervals Based on the Logistic Fit For the Blend ( X ) to Produce a Given Performance ( Y ) ( Read these bounds horizontally , as bounds on X , with the upper bound to the right of the lower bound . )</sentence>
				<definiendum id="0">DB Blend</definiendum>
				<definiens id="0">s ec onds ) Experiments 1 &amp; 2 Performance and Time vs DB Blend 54 61 66 70 75 82 206 187 179 166 152 140 50 55 60 65 70 75 80 85 90 95 100</definiens>
				<definiens id="1">QA Time ( downward-sloping ) vs. Extraction Blend Error Bars are Plus/Minus Standard Error of Mean ( SEM ) Within Each Blend Upper and Lower Bounds Are Approximate 95 % Confidence Intervals Based on the Logistic Fit For the Blend ( X ) to Produce a Given Performance</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>We view word alignment as a pattern classification problem , where alignment combination is treated as a classifier ensemble , and alignment links are adorned with linguistic features .</sentence>
				<definiendum id="0">alignment combination</definiendum>
				<definiens id="0">a classifier ensemble , and alignment links are adorned with linguistic features</definiens>
			</definition>
			<definition id="1">
				<sentence>Word-level alignment is a critical component of a wide range of NLP applications , such as construction of bilingual lexicons ( Melamed , 2000 ) , word sense disambiguation ( Diab and Resnik , 2002 ) , projection of language resources ( Yarowsky et al. , 2001 ) , and statistical machine translation .</sentence>
				<definiendum id="0">Word-level alignment</definiendum>
				<definiens id="0">a critical component of a wide range of NLP applications , such as construction of bilingual lexicons ( Melamed , 2000 ) , word sense disambiguation ( Diab and Resnik , 2002 ) , projection of language resources ( Yarowsky et al. , 2001 ) , and statistical machine translation</definiens>
			</definition>
			<definition id="2">
				<sentence>Even with large amounts of training data , statistical aligners have been shown to be susceptible to mis-aligning phrasal constructions ( Dorr et al. , 2002 ) due to many-to-many correspondences , morphological language distinctions , paraphrased and free translations , and a high percentage of function words ( about 50 % of the tokens in most texts ) .</sentence>
				<definiendum id="0">statistical aligners</definiendum>
				<definiens id="0">due to many-to-many correspondences , morphological language distinctions , paraphrased and free translations</definiens>
			</definition>
			<definition id="3">
				<sentence>A multi-layer perceptron ( MLP ) is a feed-forward neural network that consists of several units ( neurons ) that are connected to each other by weighted links .</sentence>
				<definiendum id="0">multi-layer perceptron</definiendum>
				<definiendum id="1">MLP</definiendum>
				<definiens id="0">a feed-forward neural network that consists of several units ( neurons ) that are connected to each other by weighted links</definiens>
			</definition>
			<definition id="4">
				<sentence>As illustrated in Figure 1 , an MLP consists of one input layer , one or more hidden layers , and one output layer .</sentence>
				<definiendum id="0">MLP</definiendum>
				<definiens id="0">consists of one input layer , one or more hidden layers , and one output layer</definiens>
			</definition>
			<definition id="5">
				<sentence>A neighborhood of an alignment link ( i , j ) —denoted by N ( i , j ) — consists of 8 possible alignment links in a 3×3 window with ( i , j ) in the center of the window .</sentence>
				<definiendum id="0">neighborhood of an alignment link</definiendum>
				<definiens id="0">consists of 8 possible alignment links in a 3×3 window with ( i , j ) in the center of the window</definiens>
			</definition>
			<definition id="6">
				<sentence>Alignment features consist of features that are extracted from the outputs of individual alignment systems .</sentence>
				<definiendum id="0">Alignment features</definiendum>
				<definiens id="0">consist of features that are extracted from the outputs of individual alignment systems</definiens>
			</definition>
			<definition id="7">
				<sentence>We conducted a preliminary analysis on 100 randomly selected English-Spanish sentence pairs from a mixed corpus ( UN + Bible + FBIS ) to observe the SPANISH Adj Adv Comp Det Noun Prep Verb E Adj 18 82 40 96 66 N Adv 8 50 67 75 G Comp 12 46 37 96 L Det 10 60 100 I Noun 42 77 100 94 23 98 84 S Prep 93 70 22 100 H Verb 42 100 66 78 43 Table 1 : Error Rates according to POS Tags for GIZA++ ( E-to-S ) ( in percentages ) ClassificationData DataPartitioning Output Truth Parta Parti Partz NNa NNz NNi NNCombination Figure 4 : NeurAlign2—Alignment Combination with Partitioning distribution of errors according to POS tags in both languages .</sentence>
				<definiendum id="0">ClassificationData DataPartitioning Output Truth Parta Parti Partz NNa NNz NNi NNCombination</definiendum>
				<definiens id="0">a preliminary analysis on 100 randomly selected English-Spanish sentence pairs from a mixed corpus ( UN + Bible + FBIS ) to observe the SPANISH Adj</definiens>
				<definiens id="1">Error Rates according to POS Tags for GIZA++ ( E-to-S ) ( in percentages</definiens>
			</definition>
			<definition id="8">
				<sentence>Precision ( Pr ) , recall ( Rc ) and alignment error rate ( AER ) are defined as follows : Pr = |A∩P||A| Rc = |A∩S||S| AER = 1−|A∩S|+|A∩P||A|+|S| A manually aligned corpus is used as our gold standard .</sentence>
				<definiendum id="0">Precision ( Pr )</definiendum>
				<definiens id="0">recall ( Rc ) and alignment error rate ( AER ) are defined as follows : Pr = |A∩P||A| Rc</definiens>
			</definition>
			<definition id="9">
				<sentence>The hidden layer consists of 10 units , and the output layer consists of 1 unit .</sentence>
				<definiendum id="0">hidden layer</definiendum>
			</definition>
			<definition id="10">
				<sentence>69 This section describes the experiments on EnglishSpanish and English-Chinese data for testing the effects of feature selection , training on the entire data ( NeurAlign1 ) or on the partitioned data ( NeurAlign2 ) , using two input alignments : GIZA++ ( E-to-F ) and GIZA++ ( F-to-E ) .</sentence>
				<definiendum id="0">GIZA++</definiendum>
				<definiens id="0">the experiments on EnglishSpanish and English-Chinese data for testing the effects of feature selection , training on the entire data ( NeurAlign1 ) or on the partitioned data</definiens>
			</definition>
</paper>

		<paper id="2017">
			<definition id="0">
				<sentence>Given a set of relations of interest , KNOWITALL instantiates relation-specific generic extraction patterns into extraction rules which find candidate facts .</sentence>
				<definiendum id="0">KNOWITALL</definiendum>
			</definition>
			<definition id="1">
				<sentence>It Input : product class C , reviews R. Output : set of [ feature , ranked opinion list ] tuples R’ parseReviews ( R ) ; E findExplicitFeatures ( R’ , C ) ; O findOpinions ( R’ , E ) ; CO clusterOpinions ( O ) ; I findImplicitFeatures ( CO , E ) ; RO rankOpinions ( CO ) ; f ( f , oi , ... oj ) g outputTuples ( RO , I [ E ) ; Figure 1 : OPINE Overview .</sentence>
				<definiendum id="0">E findExplicitFeatures</definiendum>
				<definiendum id="1">I findImplicitFeatures</definiendum>
				<definiendum id="2">RO rankOpinions</definiendum>
				<definiens id="0">reviews R. Output : set of [ feature , ranked opinion list ] tuples R’ parseReviews ( R )</definiens>
			</definition>
			<definition id="2">
				<sentence>t. f 2F and oi , ... oj 2O , where : a ) F is the set of product class features in R. b ) O is the set of opinion phrases in R. c ) opinions associated with a particular feature are ranked based on their strength .</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">the set of opinion phrases in R. c ) opinions associated with a particular feature are ranked based on their strength</definiens>
			</definition>
			<definition id="3">
				<sentence>Explicit Feature Extraction OPINE parses the reviews using the MINIPAR dependency parser ( Lin , 1998 ) and applies a simple pronoun-resolution module to the parsed data .</sentence>
				<definiendum id="0">Explicit Feature Extraction OPINE</definiendum>
			</definition>
			<definition id="4">
				<sentence>This problem is similar to labeling problems in computer vision and OPINE uses a well-known computer vision technique , relaxation labeling , as the basis of a 3-step SO label assignment procedure .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">uses a well-known computer vision technique , relaxation labeling</definiens>
			</definition>
			<definition id="5">
				<sentence>First , OPINE identifies the average SO label for a word w in the context of the review set .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">identifies the average SO label for a word w in the context of the review set</definiens>
			</definition>
			<definition id="6">
				<sentence>OPINE then uses Web-derived constraints on the relative strength of phrases in order to improve this ranking .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">uses Web-derived constraints on the relative strength of phrases in order to improve this ranking</definiens>
			</definition>
			<definition id="7">
				<sentence>Unlike previous research on identifying the subjective character and the polarity of phrases and sentences ( ( Hatzivassiloglou and Wiebe , 2000 ; Turney , 2003 ) and many others ) , OPINE identifies the contextsensitive polarity of opinion phrases .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">the subjective character and the polarity of phrases and sentences</definiens>
				<definiens id="1">identifies the contextsensitive polarity of opinion phrases</definiens>
			</definition>
</paper>

		<paper id="1049">
			<definition id="0">
				<sentence>Then , we derive a dependency tree representation of the sentence using a slightly modified version of Collins’ head propagation rules ( Collins , 1999 ) , which make main verbs not auxiliaries the head of sentences .</sentence>
				<definiendum id="0">propagation rules</definiendum>
				<definiens id="0">make main verbs not auxiliaries the head of sentences</definiens>
			</definition>
			<definition id="1">
				<sentence>The dependency graph is the basis for our graphical representation , but it is enhanced in the following ways : 388 Task Text Hypothesis Entailed Question Answer ( QA ) Prince Charles was previously married to Princess Diana , who died in a car crash in Paris in August 1997 .</sentence>
				<definiendum id="0">dependency graph</definiendum>
				<definiens id="0">the basis for our graphical representation , but it is enhanced in the following ways : 388 Task Text Hypothesis Entailed Question Answer ( QA ) Prince Charles was previously married to Princess Diana , who died in a car crash in Paris in August 1997</definiens>
			</definition>
			<definition id="2">
				<sentence>If M is the set of such matchings , we define the cost of matching H to T to be MatchCost ( H , T ) = minM∈M Cost ( M ) ( 1 ) Suppose we have a model , VertexSub ( v , M ( v ) ) , which gives us a cost in [ 0,1 ] , for substituting vertex v in H for M ( v ) in T. One natural cost model 389 is to use the normalized cost for each of the vertex substitutions in M : VertexCost ( M ) = 1Z summationdisplay v∈HV w ( v ) VertexSub ( v , M ( v ) ) ( 2 ) Here , w ( v ) represents the weight or relative importance for vertex v , and Z = summationtextv∈HV w ( v ) is a normalization constant .</sentence>
				<definiendum id="0">v , M ( v</definiendum>
				<definiens id="0">gives us a cost in [ 0,1 ] , for substituting vertex v in H for M</definiens>
				<definiens id="1">the weight or relative importance for vertex v</definiens>
				<definiens id="2">a normalization constant</definiens>
			</definition>
			<definition id="3">
				<sentence>Hypothesis : The Osaka World Trade Center is the tallest building in Japan .</sentence>
				<definiendum id="0">Hypothesis</definiendum>
				<definiens id="0">The Osaka World Trade Center is the tallest building in Japan</definiens>
			</definition>
			<definition id="4">
				<sentence>Additionally , during error analysis on the development set , we spotted the following cases where our VertexSub function erroneously labeled vertices as similar , and required special case consideration : • Antonym Check : We consistently found that the WordNet : :Similarity modules gave highsimilarity to antonyms.5 We explicitly check whether a matching involved antonyms and reject unless one of the vertices had a negation modifier .</sentence>
				<definiendum id="0">Check</definiendum>
				<definiens id="0">similar , and required special case consideration : • Antonym</definiens>
			</definition>
</paper>

		<paper id="2018">
			<definition id="0">
				<sentence>OpinionFinder : A system for subjectivity analysis Theresa Wilson‡ , Paul Hoffmann‡ , Swapna Somasundaran† , Jason Kessler† , Janyce Wiebe†‡ , Yejin Choi§ , Claire Cardie§ , Ellen Riloff∗ , Siddharth Patwardhan∗ ‡Intelligent Systems Program , University of Pittsburgh , Pittsburgh , PA 15260 †Department of Computer Science , University of Pittsburgh , Pittsburgh , PA 15260 §Department of Computer Science , Cornell University , Ithaca , NY 14853 ∗School of Computing , University of Utah , Salt Lake City , UT 84112 { twilson , hoffmanp , swapna , jsk44 , wiebe } @ cs.pitt.edu , { ychoi , cardie } @ cs.cornell.edu , { riloff , sidd } @ cs.utah.edu OpinionFinder is a system that performs subjectivity analysis , automatically identifying when opinions , sentiments , speculations , and other private states are present in text .</sentence>
				<definiendum id="0">OpinionFinder</definiendum>
				<definiendum id="1">OpinionFinder</definiendum>
				<definiens id="0">a system that performs subjectivity analysis , automatically identifying when opinions , sentiments , speculations , and other private states are present in text</definiens>
			</definition>
			<definition id="1">
				<sentence>In batch mode , OpinionFinder takes a list of documents to process .</sentence>
				<definiendum id="0">OpinionFinder</definiendum>
				<definiens id="0">takes a list of documents to process</definiens>
			</definition>
			<definition id="2">
				<sentence>BoosTexter : A boostingbased system for text categorization .</sentence>
				<definiendum id="0">BoosTexter</definiendum>
				<definiens id="0">A boostingbased system for text categorization</definiens>
			</definition>
</paper>

		<paper id="1079">
			<definition id="0">
				<sentence>Recognising textual entailment ( RTE ) is the task to find out whether some text T entails a hypothesis H. This task has recently been the focus of a challenge organised by the PASCAL network in 2004/5.1 In Example 1550 H follows from T whereas this is not the case in Example 731 .</sentence>
				<definiendum id="0">RTE )</definiendum>
				<definiens id="0">the task to find out whether some text T entails a hypothesis</definiens>
			</definition>
			<definition id="1">
				<sentence>The semantic representation language is a first-order fragment of the DRSlanguage used in Discourse Representation Theory ( Kamp and Reyle , 1993 ) , conveying argument structure with a neo-Davidsonian analysis and including the recursive DRS structure to cover negation , disjunction , and implication .</sentence>
				<definiendum id="0">semantic representation language</definiendum>
				<definiens id="0">a first-order fragment of the DRSlanguage used in Discourse Representation Theory ( Kamp and Reyle , 1993 ) , conveying argument structure with a neo-Davidsonian analysis and including the recursive DRS structure to cover negation , disjunction , and implication</definiens>
			</definition>
			<definition id="2">
				<sentence>drs ( T ) : x1 x2 x3 book ( x1 ) book ( x2 ) ¬ x1=x2 clinton ( x3 ) of ( x1 , x3 ) ¬ e4 x5 big ( x5 ) seller ( x5 ) be ( e4 ) agent ( e4 , x1 ) patient ( e4 , x5 ) loc ( e4 , here ) drs ( H ) : x1 x2 e3 x4 book ( x1 ) clinton ( x2 ) of ( x1 , x2 ) big ( x4 ) seller ( x4 ) be ( e3 ) agent ( e3 , x1 ) patient ( e3 , x4 ) 629 Proper names and definite descriptions are treated as anaphoric , and bound to previously introduced discourse referents if possible , otherwise accommodated .</sentence>
				<definiendum id="0">drs</definiendum>
				<definiens id="0">anaphoric , and bound to previously introduced discourse referents if possible</definiens>
			</definition>
			<definition id="3">
				<sentence>Instead of giving just FOL ( DRS ( T ) ; DRS ( H ) ) to the theorem prover , we supply it with ( BK ∧ FOL ( DRS ( T ) ; DRS ( H ) ) ) where BK is short for the relevant background knowledge .</sentence>
				<definiendum id="0">DRS</definiendum>
				<definiendum id="1">BK</definiendum>
				<definiens id="0">short for the relevant background knowledge</definiens>
			</definition>
			<definition id="4">
				<sentence>Formally , a model is a pair 〈D , F〉 where D is the set of entities in the domain , and F a function mapping predicate symbols to sets of domain members .</sentence>
				<definiendum id="0">D</definiendum>
				<definiens id="0">the set of entities in the domain , and F a function mapping predicate symbols to sets of domain members</definiens>
			</definition>
			<definition id="5">
				<sentence>Put differently , the domain size for T+H would equal the domain size of T. In contrast , if T does not entail H , H normally introduce some new information ( except when it contains negated information ) , and this will be reflected in the domain size of T+H , which then is larger than the domain size of T. It turns out that this difference between the domain sizes is a useful way of measuring the likelihood of entailment .</sentence>
				<definiendum id="0">T+H</definiendum>
				<definiens id="0">a useful way of measuring the likelihood of entailment</definiens>
			</definition>
</paper>

		<paper id="1124">
			<definition id="0">
				<sentence>The most common approach to text segmentation is to use finite-state sequence tagging models , in which each atomic text element ( character or token ) is labeled with a tag representing its role in a segmentation .</sentence>
				<definiendum id="0">segmentation</definiendum>
				<definiens id="0">in which each atomic text element ( character or token</definiens>
			</definition>
			<definition id="1">
				<sentence>In this view , a segmentation of a text is a set of segments , each of which is defined by the set of text positions that belong to the segment .</sentence>
				<definiendum id="0">segmentation of a text</definiendum>
				<definiens id="0">a set of segments , each of which is defined by the set of text positions that belong to the segment</definiens>
			</definition>
			<definition id="2">
				<sentence>In that case , the set of segment labels is seg ( x ) = T fO , Ign , where T is the set of segment types .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">seg ( x ) = T fO , Ign , where</definiens>
			</definition>
			<definition id="3">
				<sentence>Our model is based on a linear score s ( x , y ; w ) for each segment y of text x , defined as s ( x , y ; w ) = w f ( x , y ) where f ( x , y ) is a feature vector representation of the sentence-segment pair , and w is a vector of feature weights .</sentence>
				<definiendum id="0">f ( x , y )</definiendum>
				<definiendum id="1">w</definiendum>
				<definiens id="0">a feature vector representation of the sentence-segment pair</definiens>
			</definition>
			<definition id="4">
				<sentence>For a given text x , act ( x ) seg ( x ) denotes the set of correct segments for x , and bestk ( x ; w ) denotes the set of k segments with highest score relative to the weight vector w. For learning , we use a training set T = f ( xt , act ( xt ) ) g|T |t=1 of texts labeled with the correct segmentation .</sentence>
				<definiendum id="0">w )</definiendum>
				<definiens id="0">the set of k segments with highest score relative to the weight vector w. For learning , we use a training set T = f ( xt , act ( xt ) ) g|T |t=1 of texts labeled with the correct segmentation</definiens>
			</definition>
			<definition id="5">
				<sentence>For segmentation , the number of possible labels ( segments ) is exponential on the length of the text .</sentence>
				<definiendum id="0">segments</definiendum>
			</definition>
			<definition id="6">
				<sentence>Then , we restrict ourselves to feature functions f ( x , y ) that factor relative to the input as f ( x , y ) = |i ( y ) |summationdisplay j=1 g ( i ( y ) j−1 , i ( y ) j ) ( 1 ) where i ( y ) j is the jth integer in i ( y ) and g is a feature function depending on arbitrary properties of the input relative to the indices i ( y ) j−1 and i ( y ) j. Applying ( 1 ) to the segment Bill Clinton in Figure 3 , its score would be w [ g ( 0 , 3 ) + g ( 3 , 6 ) + g ( 6 , 11 ) ] This feature representation allows us to include dependencies between non-contiguous segment positions , as well as dependencies on any properties of the input , including properties of skipped positions .</sentence>
				<definiendum id="0">g</definiendum>
				<definiens id="0">the jth integer in i ( y )</definiens>
				<definiens id="1">a feature function depending on arbitrary properties of the input relative to the indices i ( y )</definiens>
			</definition>
			<definition id="7">
				<sentence>Clearly , this program requires O ( n2 ) time for a text of length n. Furthermore we can easily augment this algorithm in the standard fashion to find the k best segments , and multiple segment types , resulting in a runtime of O ( n2kT ) , where T is the number of types .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">the number of types</definiens>
			</definition>
			<definition id="8">
				<sentence>For these experiments , we set k = n , where n is the length of the sentence .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the sentence</definiens>
			</definition>
</paper>

		<paper id="1115">
			<definition id="0">
				<sentence>While passage retrieval ( PR ) is clearly not a new problem ( e.g. ( Robertson et al. , 1992 ; Salton et al. , 1993 ) ) , it remains important and yet often overlooked .</sentence>
				<definiendum id="0">passage retrieval</definiendum>
				<definiendum id="1">PR</definiendum>
			</definition>
			<definition id="1">
				<sentence>Asnoted by ( Gaizauskas etal. , 2004 ) , while PR is the crucial first step for question answering , Q &amp; A research has typically not empha915 Hurricane Isabel 's outer bands moving onshore produced on 09/18 , 6:18 AM 2 % Summary The North Carolina coast braced for a weakened but still potent Hurricane Isabel while already rain-soaked areas as far away as Pennsylvania prepared for possibly ruinous flooding .</sentence>
				<definiendum id="0">PR</definiendum>
				<definiens id="0">far away as Pennsylvania prepared for possibly ruinous flooding</definiens>
			</definition>
			<definition id="2">
				<sentence>The degree of a given node is an indication of how much information the respective sentence has in common with other sentences .</sentence>
				<definiendum id="0">degree of a given node</definiendum>
				<definiens id="0">an indication of how much information the respective sentence has in common with other sentences</definiens>
			</definition>
			<definition id="3">
				<sentence>In topic-sensitive LexRank , we first stem all of the sentences in a set of articles and compute word IDFs by the following formula : idf w =log parenleftBig N +1 w parenrightBig ( 1 ) where N is the total number of sentences in the cluster , and sf w is the number of sentences that the word w appears in .</sentence>
				<definiendum id="0">w parenrightBig</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">sf w</definiendum>
				<definiens id="0">all of the sentences in a set of articles and compute word IDFs by the following formula : idf w =log parenleftBig N +1</definiens>
				<definiens id="1">the total number of sentences in the cluster</definiens>
			</definition>
			<definition id="4">
				<sentence>This idea is captured by the following mixture model , where p ( s|q ) , the score of a sentence s given a question q , is determined as the sum of its relevance to the question ( using the same measure as the baseline described above ) and the similarity to the other sentences in the document cluster : p ( s|q ) =d rel ( s|q ) P z∈C rel ( z|q ) + ( 1−d ) X v∈C sim ( s , v ) P z∈C sim ( z , v ) p ( v|q ) ( 3 ) where C is the set of all sentences in the cluster .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">the score of a sentence s given a question q , is determined as the sum of its relevance to the question ( using the same measure as the baseline described above ) and the similarity to the other sentences in the document cluster : p ( s|q ) =d rel</definiens>
				<definiens id="1">the set of all sentences in the cluster</definiens>
			</definition>
			<definition id="5">
				<sentence>1 A simpler version of Equation 5 , where A is a uniform matrix andBis a normalized binary matrix , is known as PageRank ( Brin and Page , 1998 ; Page et al. , 1998 ) and used to rank the web pages by the Google search engine .</sentence>
				<definiendum id="0">PageRank ( Brin</definiendum>
			</definition>
			<definition id="6">
				<sentence>The “gold standard” extracts list the sentences judged as containing answers to a given question by the annotators ( and therefore have variable sizes ) in no particular order .</sentence>
				<definiendum id="0">“gold standard” extracts</definiendum>
				<definiens id="0">the sentences judged as containing answers to a given question by the annotators ( and therefore have variable sizes ) in no particular order</definiens>
			</definition>
			<definition id="7">
				<sentence>To contrast , TRDR is the total of the reciprocal ranks of all answers found by the system .</sentence>
				<definiendum id="0">TRDR</definiendum>
				<definiens id="0">the total of the reciprocal ranks of all answers found by the system</definiens>
			</definition>
</paper>

		<paper id="2008">
			<definition id="0">
				<sentence>Besides the veri cation of ’pre-PDT’ theories and formulation of new ones , PDT serves as training data for machine learning methods .</sentence>
				<definiendum id="0">PDT</definiendum>
				<definiens id="0">serves as training data for machine learning methods</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>Linguistic salience describes the accessibility of entities in a speaker/hearer’s memory and its implication in language production and interpretation .</sentence>
				<definiendum id="0">Linguistic salience</definiendum>
				<definiens id="0">describes the accessibility of entities in a speaker/hearer’s memory and its implication in language production and interpretation</definiens>
			</definition>
			<definition id="1">
				<sentence>Given an observed speech utterance O , the goal of speech recognition is to find a sequence of words W* so that WP , where P ( O|W ) is the acoustic model and P ( W ) is the language model .</sentence>
				<definiendum id="0">P ( O|W )</definiendum>
				<definiens id="0">the acoustic model</definiens>
				<definiens id="1">the language model</definiens>
			</definition>
			<definition id="2">
				<sentence>is the class transition probability , which reflects the grammatical formation of utterances .</sentence>
				<definiendum id="0">class transition probability</definiendum>
				<definiens id="0">reflects the grammatical formation of utterances</definiens>
			</definition>
			<definition id="3">
				<sentence>In Equation ( 4 ) , P is the salience value for an entity at time t i ( the onset of the spoken word w i ) , which can be calculated by Equation ( 1 ) .</sentence>
				<definiendum id="0">P</definiendum>
			</definition>
			<definition id="4">
				<sentence>Precision measures the percentage of correctly identified concepts out of the total number of concepts identified based on a sequence of words .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">measures the percentage of correctly identified concepts out of the total number of concepts identified based on a sequence of words</definiens>
			</definition>
			<definition id="5">
				<sentence>Recall measures the percentage of correctly identified concepts out of the total number of intended concepts from user’s utterance .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">measures the percentage of correctly identified concepts out of the total number of intended concepts from user’s utterance</definiens>
			</definition>
</paper>

		<paper id="1123">
			<definition id="0">
				<sentence>980 ( 2 ) Calculating cluster centroids : calculate a probability distribution pa0 ya1ca2 over all features y given each cluster c , based on the feature distribution of cluster elements , weighed by the pa0 ca1xa2 assignment probability calculated in step ( 1 ) above .</sentence>
				<definiendum id="0">Calculating cluster centroids</definiendum>
				<definiens id="0">calculate a probability distribution pa0 ya1ca2 over all features y given each cluster c , based on the feature distribution of cluster elements</definiens>
			</definition>
			<definition id="1">
				<sentence>Tishby et al.’s ( 1999 ) information bottleneck method ( IB ) includes the marginal cluster entropy H a10 Ca12 in the cost term2 ( it is marked with dotted underline to denote its inclusion is optional , so that Eq .</sentence>
				<definiendum id="0">IB )</definiendum>
				<definiens id="0">includes the marginal cluster entropy H a10 Ca12 in the cost term2 ( it is marked with dotted underline to denote its inclusion is optional , so that Eq</definiens>
			</definition>
			<definition id="2">
				<sentence>The CP framework introduces an additional parameter , a44 .</sentence>
				<definiendum id="0">CP framework</definiendum>
				<definiens id="0">introduces an additional parameter</definiens>
			</definition>
			<definition id="3">
				<sentence>It is interesting to have a notion of those features y with high p* c y , within each cluster c. We exemplify those typical features , for each one of the seven clusters , through four of the highest p* c y features ( excluding those terms that function as both features and clustered terms ) : “schools” cluster : central , dominanta0 , mainstream , affiliate ; “divinity” cluster : omnipotenta0 , almighty , mercy , infinite ; “religious experience” cluster : intrinsic , mental , realm , mature ; “writings” cluster : commentary , manuscript , dictionary , grammar ; “festivals and rite” cluster : annual , funeral , rebuild , feast ; “material existence , sin , and suffering” cluster : vegetable , insect , penalty , quench ; “community and family” cluster : parent , nursing , spouse , elderly .</sentence>
				<definiendum id="0">“writings” cluster</definiendum>
				<definiendum id="1">rite” cluster</definiendum>
				<definiendum id="2">suffering” cluster</definiendum>
				<definiens id="0">intrinsic , mental , realm</definiens>
			</definition>
			<definition id="4">
				<sentence>As an overlap measure we employed the Jaccard coefficient , which is the ratio n    n n n   , where : n  is the number of term pairs assigned to the same cluster by both our method and the expert ; n  is the number of term pairs co-assigned by our method but not by the expert ; n  is the number of term pairs co-assigned by the expert but not by our method .</sentence>
				<definiendum id="0">Jaccard coefficient</definiendum>
				<definiendum id="1">n </definiendum>
				<definiens id="0">the ratio n    n n n  </definiens>
			</definition>
			<definition id="5">
				<sentence>Methods that do not consider this direction ( IB ) or that incorporate it within a more conventional cost based search ( IB-SI ) yield notably poorer performance .</sentence>
				<definiendum id="0">IB</definiendum>
				<definiens id="0">incorporate it within a more conventional cost based search ( IB-SI ) yield notably poorer performance</definiens>
			</definition>
			<definition id="6">
				<sentence>In this paper , we studied and demonstrated the cross partition method , a computational framework that addresses the task of identifying analogies and correspondences in texts .</sentence>
				<definiendum id="0">computational framework</definiendum>
				<definiens id="0">the task of identifying analogies and correspondences in texts</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>The F measure ( which is itself derived from van Rijsbergen’s E measure ( van Rijsbergen , 1979 ) ) is a function of set recall and precision , together with a parameter β which determines the relative importance of recall and precision : F = ( β 2 + 1 ) PR β2P + R A β value of 1 , indicating equal weight , is used in the novelty track : Fβ=1 = 2PRP + R Alternatively , this can be formulated as Fβ=1 = 2× ( # relevant retrieved ) ( # retrieved ) + ( # relevant ) For any choice of β , F lies in the range [ 0 , 1 ] , and the average of the F measure is meaningful even when the judgment sets sizes vary widely .</sentence>
				<definiendum id="0">F measure</definiendum>
				<definiens id="0">a function of set recall and precision , together with a parameter β which determines the relative importance of recall and precision : F = ( β 2 + 1 ) PR β2P + R A β value of 1</definiens>
			</definition>
			<definition id="1">
				<sentence>idf and Okapi coupled with a threshold for retrieving a relevant or novel sentence , expansion of the topic and/or document sentences using dictionaries or corpus-based methods , and using named entities as features .</sentence>
				<definiendum id="0">idf</definiendum>
				<definiens id="0">retrieving a relevant or novel sentence</definiens>
			</definition>
</paper>

		<paper id="1080">
			<definition id="0">
				<sentence>Lemmatization is the process of deriving the base form , or lemma , of a word from one of its inflected forms .</sentence>
				<definiendum id="0">Lemmatization</definiendum>
			</definition>
			<definition id="1">
				<sentence>The lemmatization algorithm considers the context and grammatical features of the language to lemmatize German words .</sentence>
				<definiendum id="0">lemmatization algorithm</definiendum>
				<definiens id="0">considers the context and grammatical features of the language to lemmatize German words</definiens>
			</definition>
			<definition id="2">
				<sentence>GATE provides an infrastructure for developing and deploying software components that process human language .</sentence>
				<definiendum id="0">GATE</definiendum>
				<definiens id="0">provides an infrastructure for developing and deploying software components that process human language</definiens>
			</definition>
</paper>

		<paper id="1067">
			<definition id="0">
				<sentence>( Knuth , 1993 ) Humor is an essential element in personal communication .</sentence>
				<definiendum id="0">Humor</definiendum>
				<definiens id="0">an essential element in personal communication</definiens>
			</definition>
			<definition id="1">
				<sentence>A one-liner is a short sentence with comic effects and an interesting linguistic structure : simple syntax , deliberate use of rhetoric devices ( e.g. alliteration , rhyme ) , and frequent use of creative language constructions meant to attract the readers attention .</sentence>
				<definiendum id="0">one-liner</definiendum>
				<definiens id="0">a short sentence with comic effects and an interesting linguistic structure : simple syntax , deliberate use of rhetoric devices ( e.g. alliteration , rhyme ) , and frequent use of creative language constructions meant to attract the readers attention</definiens>
			</definition>
			<definition id="2">
				<sentence>The set of keywords used in the current implementation consists of six words that explicitly indicate humor-related content : oneliner , one-liner , humor , humour , joke , 532 One-liners Take my advice ; I don’t use it anyway .</sentence>
				<definiendum id="0">keywords</definiendum>
				<definiens id="0">used in the current implementation consists of six words that explicitly indicate humor-related content : oneliner , one-liner , humor</definiens>
			</definition>
			<definition id="3">
				<sentence>Table 1 : Sample examples of one-liners , Reuters titles , BNC sentences , and proverbs .</sentence>
				<definiendum id="0">Reuters</definiendum>
				<definiens id="0">titles , BNC sentences , and proverbs</definiens>
			</definition>
			<definition id="4">
				<sentence>Support Vector Machines ( SVM ) are binary classifiers that seek to find the hyperplane that best separates a set of positive examples from a set of negative examples , with maximum margin .</sentence>
				<definiendum id="0">Support Vector Machines</definiendum>
				<definiens id="0">binary classifiers that seek to find the hyperplane that best separates a set of positive examples from a set of negative examples , with maximum margin</definiens>
			</definition>
</paper>

		<paper id="1095">
			<definition id="0">
				<sentence>The NIST evaluation metric computes a weighted n-gram precision between T and R , multiplied by a factor B ( S , T , R ) that penalizes short translations .</sentence>
				<definiendum id="0">NIST evaluation metric</definiendum>
				<definiens id="0">computes a weighted n-gram precision between T and R , multiplied by a factor B ( S , T , R ) that penalizes short translations</definiens>
			</definition>
			<definition id="1">
				<sentence>It can be formulated as : B ( S , T , R ) × Nsummationdisplay n=1 summationtext s∈S In ( ts , rs ) summationtext s∈S Cn ( ts ) ( 3 ) where N is the largest n-gram considered ( usually N = 4 ) , In ( ts , rs ) is a weighted count of common n-grams between the target ( ts ) and reference ( rs ) translations of sentence s , and Cn ( ts ) is the total number of n-grams in ts .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">Cn</definiendum>
				<definiens id="0">a weighted count of common n-grams between the target ( ts ) and reference ( rs ) translations of sentence s , and</definiens>
				<definiens id="1">the total number of n-grams in ts</definiens>
			</definition>
			<definition id="2">
				<sentence>758 by a factor w ( λ , s , k ) , proportional to the score mλ ( ts , k|s ) that ts , k is assigned by the log-linear model for a given λ : w ( λ , s , k ) = bracketleftBigg mλ ( ts , k|s ) summationtext kprime mλ ( ts , kprime|s ) bracketrightBiggα where α is the smoothing factor .</sentence>
				<definiendum id="0">α</definiendum>
				<definiens id="0">s , k ) , proportional to the score mλ ( ts , k|s ) that ts , k is assigned by the log-linear model for a given λ : w ( λ , s , k ) = bracketleftBigg mλ ( ts , k|s ) summationtext kprime mλ ( ts</definiens>
			</definition>
</paper>

		<paper id="1094">
			<definition id="0">
				<sentence>Formally , DCRFs are the class of conditionally-trained undirected models that repeat structure and parameters over a sequence .</sentence>
				<definiendum id="0">DCRFs</definiendum>
				<definiens id="0">the class of conditionally-trained undirected models that repeat structure and parameters over a sequence</definiens>
			</definition>
			<definition id="1">
				<sentence>In the above wt is the word at position t , Tt is the POS tag at position t , w ranges over all words in the training data , and T ranges over all Penn Treebank part-of-speech tags .</sentence>
				<definiendum id="0">Tt</definiendum>
			</definition>
			<definition id="2">
				<sentence>wt = w wt matches [ A-Z ] [ a-z ] + wt matches [ A-Z ] [ A-Z ] + wt matches [ A-Z ] wt matches [ A-Z ] + wt matches [ A-Z ] + [ a-z ] + [ A-Z ] + [ a-z ] wt is punctuation wt appears in list of first names , last names , honorifics , etc. qk ( x , t + δ ) for all k and δ ∈ [ −2,2 ] Conjunction qk ( x , t ) and qkprime ( x , t ) for all features k , kprime Conjunction qk ( x , t ) and qkprime ( x , t + 1 ) for all features k , kprime Table3 : Inputfeatures qk ( x , t ) fortheACEnamed-entity data .</sentence>
				<definiendum id="0">etc. qk</definiendum>
				<definiendum id="1">Conjunction qk</definiendum>
				<definiendum id="2">kprime Conjunction qk</definiendum>
				<definiens id="0">A-Z ] [ a-z ] + wt matches [ A-Z ] [ A-Z ] + wt matches [ A-Z ] wt matches [ A-Z ] + wt matches [ A-Z ] +</definiens>
			</definition>
			<definition id="3">
				<sentence>GPE means geopolitical entities , such as countries .</sentence>
				<definiendum id="0">GPE</definiendum>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>Adomain is a set of objects .</sentence>
				<definiendum id="0">Adomain</definiendum>
				<definiens id="0">a set of objects</definiens>
			</definition>
			<definition id="1">
				<sentence>In the simplest case , an all-knowing agent A 0 sits in the background , without taking any physical actions , and uses its message policy ( µ 01 ) to send messages to a listener agent A 1 .</sentence>
				<definiendum id="0">all-knowing agent A 0</definiendum>
				<definiens id="0">a listener agent A 1</definiens>
			</definition>
			<definition id="2">
				<sentence>Let T k ( s ) : = braceleftBig s prime ∈S|s prime [ k ] = s [ k ] bracerightBig be the subset of full states which map to the same value of s. Given a state distribution P ( s ) we can define distributions over partial states : P ( s [ k ] , s [ j ] ) = summationdisplay s prime ∈T k ( s ) ∩T j ( s ) P ( s prime ) .</sentence>
				<definiendum id="0">T k ( s</definiendum>
				<definiens id="0">prime ∈S|s prime [ k ] = s [ k ] bracerightBig be the subset of full states which map to the same value of s. Given a state distribution P ( s ) we can define distributions over partial states : P ( s [ k ] , s [ j ] ) = summationdisplay s prime ∈T k ( s ) ∩T j ( s ) P ( s prime )</definiens>
			</definition>
</paper>

		<paper id="1015">
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>NE machine transliteration generates a phonetically similar equivalent in the target language for a source NE , and transliteration patterns highly depend on the name’s origin , e.g. , the country or the language family this name is from .</sentence>
				<definiendum id="0">NE machine transliteration</definiendum>
				<definiens id="0">generates a phonetically similar equivalent in the target language for a source NE , and transliteration patterns highly depend on the name’s origin</definiens>
			</definition>
			<definition id="1">
				<sentence>The distance between clusters and is defined as the average distance between all origin pairs in each cluster .</sentence>
				<definiendum id="0">distance between clusters</definiendum>
				<definiens id="0">the average distance between all origin pairs in each cluster</definiens>
			</definition>
			<definition id="2">
				<sentence>Given a held-out data set L , a list of name translation pairs from different origins , the probability of generating L from a cluster configuration ω Θ is the product of generating each name pair from its most likely origin cluster : ∏ ∏ = Θ∈ = Θ∈ = =Θ || 1 ) ( ) ( || 1 ) ( ) ( ) ( max ) ( ) | , ( max ) | ( L t j t je t jcj L t jj tt j PEPFP PEFPLP θ θθ ω ω ω We calculate the language model perplexity : ||/1 ) | ( log || 1 ) | ( 2 ) , ( L LP L LPLpp − Θ− Θ==Θ ωω ω , and select the model configuration with the smallest perplexity .</sentence>
				<definiendum id="0">|| 1 ) ( ) ( )</definiendum>
				<definiens id="0">L LP L LPLpp − Θ− Θ==Θ ωω ω , and select the model configuration with the smallest perplexity</definiens>
			</definition>
			<definition id="3">
				<sentence>A transliteration model provides a conditional probability distribution of target candidates for a given source transliteration unit : a single character or a character sequence , i.e. , “phrase” .</sentence>
				<definiendum id="0">transliteration model</definiendum>
				<definiens id="0">a single character or a character sequence</definiens>
			</definition>
			<definition id="4">
				<sentence>Assuming the character occurrence frequency follows a binomial distribution , knk xx k n xnkL − − ⎟ ⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛ = ) 1 ( ) , , ( , 1221 , , ccc are the frequencies of , and , and 1 f 2 f 21 ^ ff N is the total number of characters .</sentence>
				<definiendum id="0">Assuming the character occurrence frequency</definiendum>
				<definiendum id="1">ccc</definiendum>
				<definiens id="0">the total number of characters</definiens>
			</definition>
			<definition id="5">
				<sentence>al. , 2003 ) ; based on the character alignment path ; phrase alignment path , searching for the optimal alignment which minimizes the overall phrase alignment cost , defined as : ∑ ∈ = Aa aiA i i efDA ) , ( minarg* .</sentence>
				<definiendum id="0">efDA</definiendum>
				<definiens id="0">2003 ) ; based on the character alignment path ; phrase alignment path , searching for the optimal alignment which minimizes the overall phrase alignment cost , defined as : ∑ ∈ = Aa aiA i i</definiens>
			</definition>
			<definition id="6">
				<sentence>λ is a cluster438 specific interpolation weight , reflecting the relative contributions of the transliteration cost and the translation cost .</sentence>
				<definiendum id="0">λ</definiendum>
				<definiens id="0">a cluster438 specific interpolation weight , reflecting the relative contributions of the transliteration cost and the translation cost</definiens>
			</definition>
			<definition id="7">
				<sentence>In Table 5 we compared translation examples from the baseline system ( CharGen ) , the phrasebased cluster-specific system ( PhraCls ) and a online machine translation system , the BabelFish 3 .</sentence>
				<definiendum id="0">CharGen</definiendum>
			</definition>
</paper>

		<paper id="1101">
			<definition id="0">
				<sentence>Formalisms based on synchronous rewriting have been empowered with the use of statistical parameters , and specialized estimation and translation ( decoding ) algorithms were newly developed .</sentence>
				<definiendum id="0">Formalisms</definiendum>
			</definition>
			<definition id="1">
				<sentence>The contribution of this paper can be stated as follows : • we show that the membership problem is NPhard , unless a constant bound is imposed on the length of the productions ( Section 3 ) ; • we show an exponential time lower bound for the membership problem , in case chart parsing is adopted ( Section 3 ) ; • we show that translating an input string into the best parse tree in the target language is NPhard , even in case productions are bounded in length ( Section 4 ) .</sentence>
				<definiendum id="0">NPhard</definiendum>
			</definition>
			<definition id="2">
				<sentence>Definition 1 A synchronous context-free grammar ( SCFG ) is a tuple G = ( VN , VT , P , S ) , where VN , VT are finite , disjoint sets of nonterminal and terminal symbols , respectively , S ∈ VN is the start symbol and P is a finite set of synchronous productions , each of the form [ A1 → α1 , A2 → α2 ] , with A1 , A2 ∈ VN and α1 , α2 ∈ V ∗I synchronous strings .</sentence>
				<definiendum id="0">synchronous context-free grammar</definiendum>
				<definiendum id="1">SCFG</definiendum>
				<definiendum id="2">S ∈ VN</definiendum>
				<definiendum id="3">P</definiendum>
				<definiens id="0">a tuple G = ( VN , VT , P , S ) , where VN , VT are finite , disjoint sets of nonterminal and terminal symbols , respectively ,</definiens>
				<definiens id="1">the start symbol</definiens>
				<definiens id="2">a finite set of synchronous productions , each of the form [ A1 → α1 , A2 → α2 ] , with A1</definiens>
			</definition>
			<definition id="3">
				<sentence>The translation generated by a SCFG G is a binary relation over V ∗T defined as T ( G ) = { [ w1 , w2 ] | [ S ( 1 ) , S ( 1 ) ] ⇒∗G [ w1 , w2 ] , w1 , w2 ∈ V ∗T } .</sentence>
				<definiendum id="0">SCFG G</definiendum>
				<definiens id="0">a binary relation over V ∗T defined as T ( G ) = { [ w1 , w2 ] | [ S ( 1 )</definiens>
			</definition>
			<definition id="4">
				<sentence>A probabilistic SCFG ( PSCFG ) is a pair ( G , pG ) where G = ( VN , VT , P , S ) is a SCFG and pG is a function from P to real numbers in [ 0,1 ] such that , for each A1 , A2 ∈ VN , we have : summationdisplay α1 , α2 pG ( [ A1 → α1 , A2 → α2 ] = 1 .</sentence>
				<definiendum id="0">probabilistic SCFG</definiendum>
				<definiendum id="1">S )</definiendum>
				<definiendum id="2">pG</definiendum>
				<definiendum id="3">pG</definiendum>
				<definiens id="0">a pair ( G , pG ) where G = ( VN , VT , P ,</definiens>
			</definition>
			<definition id="5">
				<sentence>Let 〈U , C〉 be an instance of the 3SAT problem , where U = { u1 , ... , up } is a set of variables and C = { c1 , ... , cn } is a set of clauses .</sentence>
				<definiendum id="0">〈U , C〉</definiendum>
				<definiens id="0">a set of variables and C = { c1 , ... , cn } is a set of clauses</definiens>
			</definition>
			<definition id="6">
				<sentence>The general idea of the proof is to use a string pair [ w1w2 ···wp , wc ] , where wc is a string representation of C and each wi is a string controlling the truth assignment for the variable ui .</sentence>
				<definiendum id="0">wc</definiendum>
				<definiens id="0">a string controlling the truth assignment for the variable ui</definiens>
			</definition>
			<definition id="7">
				<sentence>Let t be some parse tree ; we write y ( t ) to denote the string in the yield of t. For a string w ∈ V ∗T and a parse tree t , we also consider the probability that t is obtained from w through Gp , defined as : pG ( [ w , t ] ) = summationdisplay y ( tprime ) =w pG ( [ tprime , t ] ) .</sentence>
				<definiendum id="0">pG</definiendum>
				<definiendum id="1">pG</definiendum>
				<definiens id="0">some parse tree ; we write y ( t ) to denote the string in the yield of t. For a string w ∈ V ∗T and a parse tree t</definiens>
			</definition>
			<definition id="8">
				<sentence>Given as input a PSCFG Gp = ( G , pG ) and two strings w1 , w2 ∈ V ∗T , output the pair of parse trees argmax y ( t1 ) = w1 , y ( t2 ) = w2 pG ( [ t1 , t2 ] ) .</sentence>
				<definiendum id="0">pG</definiendum>
				<definiens id="0">input a PSCFG Gp = ( G , pG ) and two strings w1 , w2 ∈ V ∗T , output the pair of parse trees argmax y ( t1 ) = w1 , y ( t2 ) = w2</definiens>
			</definition>
</paper>

		<paper id="1097">
			<definition id="0">
				<sentence>We can now define our conditional distribution : 5Available at http : //www.cs.jhu.edu/ brill/ Pθa ( b | a , s ) = 1Z a , s eθabφa , s with partition function Za , s =summationtextbprime∈Ua exp ( θabprimeφa , s ) .</sentence>
				<definiendum id="0">conditional distribution</definiendum>
				<definiens id="0">5Available at http : //www.cs.jhu.edu/ brill/ Pθa ( b | a , s ) = 1Z a , s eθabφa , s with partition function Za</definiens>
			</definition>
</paper>

		<paper id="2015">
			<definition id="0">
				<sentence>To help the speech recognizer , users can provide their own supplemental text files , such as journal articles , book chapters , etc. , which can be used to adapt the language model and vocabulary of the system .</sentence>
				<definiendum id="0">etc.</definiendum>
				<definiens id="0">own supplemental text files , such as journal articles , book chapters</definiens>
			</definition>
			<definition id="1">
				<sentence>These results are similar in nature to the findings in the SpeechBot project ( which performs a similar service for online broadcast news archives ) [ 6 ] .</sentence>
				<definiendum id="0">SpeechBot project</definiendum>
			</definition>
</paper>

		<paper id="1105">
			<definition id="0">
				<sentence>Past work on PP-attachment has often cast these associations as the quadruple ( v , n1 , p , n2 ) , where v is the verb , n1 is the head of the direct object , p is the preposition ( the head of the PP ) and n2 is the head of the NP inside the PP .</sentence>
				<definiendum id="0">v</definiendum>
				<definiendum id="1">p</definiendum>
				<definiens id="0">the head of the direct object</definiens>
				<definiens id="1">the head of the PP ) and n2 is the head of the NP inside the PP</definiens>
			</definition>
			<definition id="1">
				<sentence>Their algorithm uses POS tagging , syntactic parses , semantic senses of the nouns ( manually annotated ) , lookups in a semantic network ( WordNet ) and the type of the coordination conjunction to make a 3-way classification : ellipsis , no ellipsis and all-way coordination .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">syntactic parses , semantic senses of the nouns ( manually annotated ) , lookups in a semantic network</definiens>
			</definition>
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>The source ( modeled probabilistically by ps ) generates a sequence of unambiguous tagged morphemes y = 〈y1 , y2 , ... 〉 ∈ Y+ ( Y is the set of unambiguous tagged morphemes in the language ) .1 The precise contents of the tag will vary by language and corpus but will minimally include POS .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">modeled probabilistically by ps ) generates a sequence of unambiguous tagged morphemes y = 〈y1</definiens>
				<definiens id="1">the set of unambiguous tagged morphemes in the language</definiens>
			</definition>
			<definition id="1">
				<sentence>y passes through a channel ( modeled by pc ) , which outputs x = 〈x1 , x2 , ... 〉 ∈ ( X∪ { OOV } ) + , a sequence of surface-level words in the language and out-of-vocabulary words ( OOV ; X is the language’s vocabulary ) .</sentence>
				<definiendum id="0">y</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">passes through a channel ( modeled by pc ) , which outputs x = 〈x1 , x2 , ... 〉 ∈ ( X∪ { OOV } ) + , a sequence of surface-level words in the language and out-of-vocabulary words</definiens>
				<definiens id="1">the language’s vocabulary )</definiens>
			</definition>
			<definition id="2">
				<sentence>A unigram channel model defines 2Probabilistic modeling of what we call the morphological channel was first carried out by Levinger et al. ( 1995 ) , who used unlabeled data to estimate p ( vectory | x ) for Hebrew , with the support defined by a dictionary .</sentence>
				<definiendum id="0">unigram channel</definiendum>
				<definiens id="0">who used unlabeled data to estimate p ( vectory | x ) for Hebrew , with the support defined by a dictionary</definiens>
			</definition>
			<definition id="3">
				<sentence>That is , we maximize summationdisplay ( x , y ) ∈X+×Y+ ˜p ( x , y ) logps parenleftBig y | d ( x ) , vectorθ parenrightBig ( 3 ) where ˜p ( · , · ) is the empirical distribution defined by the training data and vectorθ are the model parameters .</sentence>
				<definiendum id="0">)</definiendum>
				<definiens id="0">x , y ) ∈X+×Y+ ˜p ( x , y ) logps parenleftBig y | d ( x ) , vectorθ parenrightBig ( 3 ) where ˜p ( · , ·</definiens>
				<definiens id="1">the empirical distribution defined by the training data and vectorθ are the model parameters</definiens>
			</definition>
			<definition id="4">
				<sentence>Both used dictionaries and estimated their ( generative ) models using maximum likelihood ( with smoothing ) .5 Given enough data , a ML-estimated model will learn to recognize a good path y , but it may not learn to discriminate a good y from wrong alternatives per se .</sentence>
				<definiendum id="0">estimated their ( generative</definiendum>
			</definition>
			<definition id="5">
				<sentence>Lemma accuracy is the fraction of words whose lemma was correctly identified .</sentence>
				<definiendum id="0">Lemma accuracy</definiendum>
				<definiens id="0">the fraction of words whose lemma was correctly identified</definiens>
			</definition>
			<definition id="6">
				<sentence>Bold scores indicate a significant improvement over the trigram HMM ( binomial sign test , p &lt; 0.05 ) .</sentence>
				<definiendum id="0">Bold scores</definiendum>
			</definition>
			<definition id="7">
				<sentence>Decoding that finds y ( given x ) to maximize some weighted average of log-probabilities is known as a logarithmic opinion pool ( LOP ) .</sentence>
				<definiendum id="0">LOP</definiendum>
				<definiens id="0">Decoding that finds y ( given x ) to maximize some weighted average of log-probabilities is known as a logarithmic opinion pool</definiens>
			</definition>
			<definition id="8">
				<sentence>Their experts ( each a CRF ) contained overlapping feature sets , and the combined model achieved much the same effect as training a single model with smoothing .</sentence>
				<definiendum id="0">experts</definiendum>
				<definiens id="0">a CRF ) contained overlapping feature sets</definiens>
			</definition>
			<definition id="9">
				<sentence>We compare to the HMM of ( Hajiˇc et al. , 2001 ) without its OOV component.12 We report morphological tagging accuracy on words ; we also report lemma accuracy ( on non-OOV words ) , POS accu10We used less than the full corpus to keep training time down ; note that the training sets are nonetheless substantially larger than in the Korean and Arabic experiments .</sentence>
				<definiendum id="0">POS accu10We</definiendum>
				<definiens id="0">Hajiˇc et al. , 2001 ) without its OOV component.12 We report morphological tagging accuracy on words</definiens>
			</definition>
			<definition id="10">
				<sentence>Klex : Finite-state lexical transducer for Korean .</sentence>
				<definiendum id="0">Klex</definiendum>
				<definiens id="0">Finite-state lexical transducer for Korean</definiens>
			</definition>
			<definition id="11">
				<sentence>Two-level morphology : A general computational model of word-form recognition and production .</sentence>
				<definiendum id="0">Two-level morphology</definiendum>
				<definiens id="0">A general computational model of word-form recognition and production</definiens>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>This kind of interaction can be modeled as one problem-initiating message and one or more corresponding problem-solving messages , formally defined as Adjacent Pairs ( AP ) .</sentence>
				<definiendum id="0">Adjacent Pairs</definiendum>
				<definiens id="0">one problem-initiating message and one or more corresponding problem-solving messages</definiens>
			</definition>
			<definition id="1">
				<sentence>Lastly , taking into consideration the interactions among summary sentences , a MMR ( Maximum Marginal Relevancy ) model ( Goldstein et al. , 1999 ) is used to extract sentences from the list of top-ranked sentences computed from the second step .</sentence>
				<definiendum id="0">MMR</definiendum>
				<definiens id="0">Goldstein et al. , 1999 ) is used to extract sentences from the list of top-ranked sentences computed from the second step</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>The bigram precision is a128a36a129a88a130a132a131a134a133a120a135a78a136 ( one out of two bigrams in generated phrase occurs in the model set ) , bigram recall is a130a86a129a67a137a138a131a139a133a120a135a78a130a67a140a67a141 ( two out of 7 bigrams in the models occurs in the generated phrase ) and f-measure ( a142a143a131a144a130a33a145a147a146a149a148a150a129a120a151a125a145a9a152a153a148a150a154 ) is a133a120a135a78a155a67a141a20a156 .</sentence>
				<definiendum id="0">bigram precision</definiendum>
			</definition>
			<definition id="1">
				<sentence>BLAST is an ef cient alignment algorithm that assumes that words in the two sentences are roughly in the same order from a global perspective .</sentence>
				<definiendum id="0">BLAST</definiendum>
				<definiens id="0">an ef cient alignment algorithm that assumes that words in the two sentences are roughly in the same order from a global perspective</definiens>
			</definition>
</paper>

		<paper id="1092">
			<definition id="0">
				<sentence>Identifying the interactions between proteins is one of the most important challenges in modern genomics , with applications throughout cell biology , including expression analysis , signaling , and rational drug design .</sentence>
				<definiendum id="0">Identifying the interactions between proteins</definiendum>
				<definiens id="0">one of the most important challenges in modern genomics , with applications throughout cell biology</definiens>
			</definition>
			<definition id="1">
				<sentence>Figure 1 : Dynamic graphical model ( DM ) for protein interaction classification ( and role extraction ) .</sentence>
				<definiendum id="0">DM</definiendum>
				<definiens id="0">Dynamic graphical model</definiens>
			</definition>
			<definition id="2">
				<sentence>We used a second type of graphical model , a simple Naive Bayes , in which the node representing the interaction generates the observable features ( all the words in the sentence ) .</sentence>
				<definiendum id="0">simple Naive Bayes</definiendum>
				<definiens id="0">in which the node representing the interaction generates the observable features ( all the words in the sentence )</definiens>
			</definition>
			<definition id="3">
				<sentence>DM : dynamic model , NB : Naive Bayes , NN : neural network .</sentence>
				<definiendum id="0">DM</definiendum>
				<definiendum id="1">Naive Bayes</definiendum>
				<definiens id="0">dynamic model</definiens>
			</definition>
			<definition id="4">
				<sentence>Baselines : Key : trigger word approach , KeyB : trigger word with backoff , Base : the accuracy of choosing the most frequent interaction .</sentence>
				<definiendum id="0">Base</definiendum>
				<definiens id="0">trigger word with backoff</definiens>
			</definition>
			<definition id="5">
				<sentence>The F-measure10 achieved by this model for this task is 0.79 for “all , ” 0.67 for “papers” and 0.79 for “citances” ; again , the model parameters were chosen with cross validation on the training set , and “ci10The F-measure is a weighted combination of precision and recall .</sentence>
				<definiendum id="0">“ci10The F-measure</definiendum>
				<definiens id="0">a weighted combination of precision and recall</definiens>
			</definition>
</paper>

		<paper id="1120">
			<definition id="0">
				<sentence>F ≈ Crm ( 1 ) where F is the frequency , r is rank , C is a constant , and m is an exponent close to 1 .</sentence>
				<definiendum id="0">F</definiendum>
				<definiendum id="1">r</definiendum>
				<definiendum id="2">C</definiendum>
				<definiendum id="3">m</definiendum>
				<definiens id="0">a constant</definiens>
			</definition>
			<definition id="1">
				<sentence>957 The spelling correction problem can be considered in terms of the noisy channel model , which considers the misspelled word v to be a corrupted version of the correctly spelled word w. P ( w|v ) = P ( v|w ) P ( w ) P ( v ) ( 3 ) Finding the best candidate correction W involves maximizing the above probability .</sentence>
				<definiendum id="0">noisy channel model</definiendum>
				<definiens id="0">considers the misspelled word v to be a corrupted version of the correctly spelled word w. P ( w|v ) = P ( v|w ) P ( w</definiens>
			</definition>
			<definition id="2">
				<sentence>P ( v|w ) models the errors that corrupt string w into string v , and P ( w ) is the language model , or prior probability , of word w. Given two strings v and w , P ( v|w ) is the probability that v is transmitted given that the desired word is w. One method of describing the noise model is to consider P ( v|w ) to be proportional to the number of edit operations required to transform w into v. This gives P ( v|w ) ∝ ED ( v , w ) ( 5 ) where ED ( v , w ) is the edit distance between v and w. The traditional edit distance calculation assigns a xed cost for each insertion , deletion , and substitution operation .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">P ( w )</definiendum>
				<definiendum id="2">ED</definiendum>
				<definiendum id="3">w )</definiendum>
				<definiens id="0">the language model , or prior probability , of word w. Given two strings v and w</definiens>
				<definiens id="1">the probability that v is transmitted given that the desired word is w. One method of describing the noise model is to consider P ( v|w ) to be proportional to the number of edit operations required to transform w into v. This gives P ( v|w ) ∝ ED ( v</definiens>
				<definiens id="2">the edit distance between v and w. The traditional edit distance calculation assigns a xed cost for each insertion , deletion , and substitution operation</definiens>
			</definition>
			<definition id="3">
				<sentence>As described in ( Ristad and Yianilos , 1997 ) , character-to-character edit distance costs ED ( e ) can be related to edit probability P ( e ) by means of the equation : ED ( e ) = −log [ P ( e ) ] ( 6 ) where e is an edit operation consisting of a substitution of one alphanumeric character for another ( c1 → c2 ) , an insertion ( → c1 ) , or a deletion ( c1 → ) .</sentence>
				<definiendum id="0">e</definiendum>
				<definiens id="0">an edit operation consisting of a substitution of one alphanumeric character for another ( c1 → c2 ) , an insertion ( → c1 ) , or a deletion ( c1 → )</definiens>
			</definition>
			<definition id="4">
				<sentence>The expectation of each candidate correction is the probability that the word wi was desired given that the query was v : P ( wi|v ) = P ( v|wi ) P ( wi ) P ( v ) ( 11 ) where P ( v|w ) and P ( w ) are determined using the error and language models described in Equations ( 9 ) and ( 10 ) .</sentence>
				<definiendum id="0">expectation of each candidate correction</definiendum>
			</definition>
			<definition id="5">
				<sentence>In the case of a correctly spelled query , the most likely candidate correction is the word itself .</sentence>
				<definiendum id="0">most likely candidate correction</definiendum>
				<definiens id="0">the word itself</definiens>
			</definition>
			<definition id="6">
				<sentence>If only the top recommended correction is considered , EMBED fares better than Ispell , but worse than Aspell .</sentence>
				<definiendum id="0">EMBED</definiendum>
				<definiens id="0">fares better than Ispell , but worse than Aspell</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>Language model ( LM ) adaptation attempts to adjust the parameters of a LM so that it performs well on a particular ( sub- ) domain of data .</sentence>
				<definiendum id="0">Language model ( LM ) adaptation</definiendum>
				<definiens id="0">attempts to adjust the parameters of a LM so that it performs well on a particular ( sub- ) domain of data</definiens>
			</definition>
			<definition id="1">
				<sentence>The performance of IME is typically measured by the character error rate ( CER ) , which is the number of characters wrongly converted from the phonetic string divided by the number of characters in the correct transcript .</sentence>
				<definiendum id="0">CER</definiendum>
				<definiens id="0">typically measured by the character error rate</definiens>
			</definition>
			<definition id="2">
				<sentence>In many ways , IME is a similar task to speech recognition .</sentence>
				<definiendum id="0">IME</definiendum>
				<definiens id="0">a similar task to speech recognition</definiens>
			</definition>
			<definition id="3">
				<sentence>The most obvious similarity is that IME can also be viewed as a Bayesian decision problem : let A be the input phonetic string ( which corresponds to the acoustic signal in speech ) ; the task of IME is to choose the most likely word string W * among those candidates that could have been converted from A : ) | ( ) ( maxarg ) | ( maxarg* ) ( ) ( WAPWPAWPW AWAW GENGEN ∈∈ == ( 1 ) where GEN ( A ) denotes the candidate set given A. Unlike speech recognition , however , there is no acoustic ambiguity in IME , because the phonetic string is provided directly by users .</sentence>
				<definiendum id="0">phonetic string</definiendum>
				<definiendum id="1">GEN ( A )</definiendum>
				<definiens id="0">the acoustic signal in speech ) ; the task of IME is to choose the most likely word string W * among those candidates that could have been converted from A : ) | ( ) ( maxarg ) |</definiens>
				<definiens id="1">the candidate set given A. Unlike speech recognition</definiens>
			</definition>
			<definition id="4">
				<sentence>The margin of the pair ( W R , W ) with respect to the model λ is given by ) , ( ) , ( ) , ( λλ WScoreWScoreWWM RR −= ( 5 ) The loss function is then defined as ∑ ∑ =∈ = Mi i A i W i R i WWMI ... 1 ) ( ) ] , ( [ ) RLoss ( GEN λ ( 6 ) where I [ π ] = 1 if π ≤ 0 , and 0 otherwise .</sentence>
				<definiendum id="0">margin of the pair</definiendum>
				<definiendum id="1">λλ WScoreWScoreWWM RR</definiendum>
				<definiens id="0">∑ ∑ =∈ = Mi i A i W i R i WWMI ... 1 ) ( ) ] , ( [ ) RLoss ( GEN λ ( 6 ) where I [ π ] = 1 if π ≤ 0 , and 0 otherwise</definiens>
			</definition>
			<definition id="5">
				<sentence>We used the following update for the dth feature f d : ZC ZC d d d ε ε δ + + = + _ log 2 1 ( 8 ) where C d + is a value increasing exponentially with the sum of margins of ( W R , W ) pairs over the set where f d is seen in W R but not in W ; C d is the value related to the sum of margins over the set where f d is seen in W but not in W R .</sentence>
				<definiendum id="0">f d</definiendum>
				<definiens id="0">the dth feature f d : ZC ZC d d d ε ε δ + + = + _ log 2 1 ( 8 ) where C d + is a value increasing exponentially with the sum of margins of ( W R , W ) pairs over the set where</definiens>
			</definition>
			<definition id="6">
				<sentence>ε is a smoothing factor ( whose value is optimized on held-out data ) and Z is a normalization constant .</sentence>
				<definiendum id="0">ε</definiendum>
				<definiendum id="1">Z</definiendum>
				<definiens id="0">a smoothing factor ( whose value is optimized on held-out data ) and</definiens>
				<definiens id="1">a normalization constant</definiens>
			</definition>
			<definition id="7">
				<sentence>0 = 1 and λ d = 0 for d=1…D d which has largest estimated impact on reducing ExpLoss of Equation ( 7 ) 3 Update λ d by Equation ( 8 ) , and return to Step 2 Figure 1 : The boosting algorithm The perceptron algorithm can be viewed as a form of incremental training procedure that optimizes a minimum square error ( MSE ) loss function , which is an approximation of SR ( Mitchell , 1997 ) .</sentence>
				<definiendum id="0">SR</definiendum>
				<definiens id="0">a form of incremental training procedure that optimizes a minimum square error</definiens>
				<definiens id="1">an approximation of</definiens>
			</definition>
			<definition id="8">
				<sentence>Let GEN ( A ) be the set of n-best candidate word strings that could be converted from A. By adjusting λ d for a selected feature f d , we can find a set of intervals for λ d within which a particular candidate word string is selected .</sentence>
				<definiendum id="0">GEN ( A )</definiendum>
			</definition>
			<definition id="9">
				<sentence>We used four adaptation domains : Yomiuri ( newspaper corpus ) , TuneUp ( balanced corpus containing newspaper and other sources of text ) , Encarta ( encyclopedia ) and Shincho ( collection of novels ) .</sentence>
				<definiendum id="0">TuneUp (</definiendum>
				<definiendum id="1">Encarta ( encyclopedia</definiendum>
				<definiens id="0">balanced corpus containing newspaper and other sources of text )</definiens>
			</definition>
			<definition id="10">
				<sentence>Diversity is measured by the entropy of the corpus and indicates the inherent variability within the domain .</sentence>
				<definiendum id="0">Diversity</definiendum>
				<definiens id="0">measured by the entropy of the corpus and indicates the inherent variability within the domain</definiens>
			</definition>
			<definition id="11">
				<sentence>Similarity , on the other hand , is intended to capture the difficulty of a given adaptation task , and is measured by the cross entropy .</sentence>
				<definiendum id="0">Similarity</definiendum>
				<definiens id="0">intended to capture the difficulty of a given adaptation task , and is measured by the cross entropy</definiens>
			</definition>
			<definition id="12">
				<sentence>H ( A , A ) along the diagonal of Table 1 ( in boldface ) is the entropy of the corpus , indicating the corpus diversity .</sentence>
				<definiendum id="0">H</definiendum>
				<definiens id="0">the entropy of the corpus , indicating the corpus diversity</definiens>
			</definition>
			<definition id="13">
				<sentence>In contrast , Shincho is a collection of novels , on which no style or content restriction is imposed .</sentence>
				<definiendum id="0">Shincho</definiendum>
				<definiens id="0">a collection of novels , on which no style or content restriction is imposed</definiens>
			</definition>
			<definition id="14">
				<sentence>The probability according to the combined model is given by ) | ( ) 1 ( ) | ( ) | ( hwPhwPhwp i A i B i λλ −+= , where P B is the probability of the background model , P A the probability of the adaptation model , and the history h corresponds to two preceding words .</sentence>
				<definiendum id="0">P B</definiendum>
				<definiens id="0">the probability of the background model</definiens>
			</definition>
			<definition id="15">
				<sentence>In order to measure side effects , we introduce the notion of error ratio ( ER ) , which is defined as || || B A E E ER = , where |E A | is the number of errors found only in the new ( adaptation ) model , and |E B | the number of errors corrected by the new model .</sentence>
				<definiendum id="0">ER )</definiendum>
				<definiens id="0">|| || B A E E ER = , where |E A | is the number of errors found only in the new ( adaptation ) model , and |E B | the number of errors corrected by the new model</definiens>
			</definition>
			<definition id="16">
				<sentence>Figure 8 is derived from running MSR on the TuneUp test corpus , which depicts a typical case of overfitting : the CER drops in the beginning , but after a certain number of iterations , it goes up again .</sentence>
				<definiendum id="0">test corpus</definiendum>
				<definiens id="0">depicts a typical case of overfitting : the CER drops in the beginning , but after a certain number of iterations</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>The Topic Detection and Tracking ( TDT ) program , a DARPA funded initiative , seeks to develop technologies that search , organize and structure multilingual news-oriented textual materials from a variety of broadcast news media .</sentence>
				<definiendum id="0">Topic Detection</definiendum>
				<definiens id="0">a DARPA funded initiative , seeks to develop technologies that search , organize and structure multilingual news-oriented textual materials from a variety of broadcast news media</definiens>
			</definition>
			<definition id="1">
				<sentence>A topic is defined as “a seminal event or activity , along with directly related events and activities” ( Allan , 2002 ) .</sentence>
				<definiendum id="0">topic</definiendum>
			</definition>
			<definition id="2">
				<sentence>Similarity metrics , effect of named entities , pre-processing of data , and language and Hidden Markov Models were explored ( Allan et al. , 1999 ) .</sentence>
				<definiendum id="0">Similarity metrics</definiendum>
				<definiens id="0">pre-processing of data , and language and Hidden Markov Models were explored</definiens>
			</definition>
			<definition id="3">
				<sentence>Turkey accuses Syria of harboring Turkish Kurdish rebels fighting for autonomy in Turkey’s southeast ; it says rebel leader Abdullah Ocalan lives in Damascus .</sentence>
				<definiendum id="0">Turkey</definiendum>
			</definition>
			<definition id="4">
				<sentence>Sim ( S , X ) = summationtext w weight ( w , S ) ∗ weight ( w , X ) radicalBigsummationtext w weight ( w , S ) 2 radicalBigsummationtext w weight ( w , X ) 2 ( 1 ) where weight ( w , d ) =tf idf tf =log ( termfrequency + 1.0 ) idf = log ( ( docCount+1 ) ( documentfreq+0.5 ) The maximum similarity of the story S with stories seen in the past was taken as the confidence score that S was old .</sentence>
				<definiendum id="0">Sim</definiendum>
				<definiendum id="1">weight</definiendum>
			</definition>
			<definition id="5">
				<sentence>To develop SVM models we used SV MLight ( Joachims , 1999 ) , which is an implementation of SVMs in C. SV MLight is an implementation of Vapnik’s Support Vector Machine ( Vapnik , 1995 ) .</sentence>
				<definiendum id="0">SV MLight</definiendum>
				<definiens id="0">an implementation of SVMs in C. SV MLight is an implementation of Vapnik’s Support Vector Machine</definiens>
			</definition>
			<definition id="6">
				<sentence>We re-trained IdentiFinder on this simulated ASR corpus and used it to tag named entities in only the bn-asr stories in TDT2 .</sentence>
				<definiendum id="0">IdentiFinder</definiendum>
				<definiens id="0">on this simulated ASR corpus and used it to tag named entities in only the bn-asr stories in TDT2</definiens>
			</definition>
</paper>

		<paper id="1078">
			<definition id="0">
				<sentence>The architecture of an SSN parser comprises two components , one which estimates the parameters of a stochastic model for syntactic trees , and one which searches for the most probable syntactic tree given the parameter estimates .</sentence>
				<definiendum id="0">architecture of an SSN</definiendum>
				<definiens id="0">estimates the parameters of a stochastic model for syntactic trees , and one which searches for the most probable syntactic tree given the parameter estimates</definiens>
			</definition>
			<definition id="1">
				<sentence>The set of well-formed sequences of derivation moves in this parser is defined 621 by a Predictive LR pushdown automaton ( Nederhof , 1994 ) , which implements a form of left-corner parsing strategy.2 The probability of a phrase-structure tree is equated to the probability of a finite ( but unbounded ) sequence of derivation moves .</sentence>
				<definiendum id="0">Predictive LR pushdown automaton</definiendum>
				<definiens id="0">implements a form of left-corner parsing strategy.2 The probability of a phrase-structure tree is equated to the probability of a finite ( but unbounded ) sequence of derivation moves</definiens>
			</definition>
			<definition id="2">
				<sentence>622 ASSIGNED LABELS ADV BNF DIR EXT LOC MNR NOM PRP TMP SEM-NULL SUM ADV 143 0 0 0 0 0 0 1 3 11 158 BNF 0 0 0 0 0 0 0 0 0 1 1 DIR 0 0 39 0 3 4 0 0 1 51 98 EXT 0 0 0 37 0 0 0 0 0 17 54 ACTUAL LOC 0 0 1 0 345 3 0 0 15 148 512 LABELS MNR 0 0 0 0 3 35 0 0 16 40 94 NOM 2 0 0 0 0 0 88 0 0 4 94 PRP 0 0 0 0 0 0 0 54 1 33 88 TMP 18 0 1 0 24 11 0 1 479 105 639 SEM-NULL 12 0 13 5 81 28 12 24 97 20292 20564 SUM 175 0 54 42 456 81 100 80 612 20702 22302 Table 2 : Confusion matrix for simple baseline model , tested on the validation set ( section 24 of PTB ) .</sentence>
				<definiendum id="0">ADV BNF DIR EXT LOC MNR NOM PRP TMP SEM-NULL SUM ADV</definiendum>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>Information Extraction ( IE ) is a technology for finding facts in plain text , and coding them in a logical representation , such as , e.g. , a relational database .</sentence>
				<definiendum id="0">Information Extraction</definiendum>
				<definiendum id="1">IE</definiendum>
				<definiens id="0">a technology for finding facts in plain text , and coding them in a logical representation , such as , e.g. , a relational database</definiens>
			</definition>
			<definition id="1">
				<sentence>For an incident a15a57a56 a55 , we write a58 a3a59a49a61a60a62a15 when location a58 , state a49 , etc. , “belong” to I , i.e. , are extracted as fills in I. In the baseline method , a63 , for each incident a15 where a58 a60a62a15 is one of the a1a12a11a18a11 ambiguous location names , we define : a52a65a64 a14a16a15a51a17a20a19a67a66a46a49a51a68a70a69a72a71a72a15a44a68a73a56 a55a75a74 a14 a58 a3a59a49a51a68a76a17a77a60a78a15a44a68a80a79 a6a42a41a44a43a18a45a81a2 a64 a14a50a49a51a68 a3a26a15a51a17a53a19 a74 a66a51a15a44a68a73a56 a55a75a74 a14 a58 a3a59a49a51a68a76a17a77a60a78a15a44a68a80a79 a74 i.e. , a49 a68 is a candidate if it is a state fill in some incident whose location fill is also a58 ; the score is the number of times the pair a14 a58 a3a59a49a18a68a76a17 appear together in some incident in a55 .</sentence>
				<definiendum id="0">a58 a60a62a15</definiendum>
				<definiendum id="1">score</definiendum>
				<definiens id="0">one of the a1a12a11a18a11 ambiguous location names</definiens>
				<definiens id="1">a candidate if it is a state fill in some incident whose location fill is also a58</definiens>
			</definition>
			<definition id="2">
				<sentence>Finally , we tried propagating multiple candidate state hypotheses for each instance of an ambiguous location name a58 : a52a1a0 a14a16a15a51a17a53a19 a2 a37a39a38 a32a26a40 a41a43 a47 a37a39a38 a6a9a8a11a10 a8a11a2a46a49a10a22a17a23 a6a9a8 a43a18a45a25a24 a14a16a15 a68 a17 a6a42a41a44a43a18a45a47a2 a0 a14a50a49 a68 a3a26a15a51a17a20a19 a37 a37a39a38 a32a26a40a42a41a43a33a47 a37a39a38a4a3 a45a81a43a6a5a42a14a50a49 a68 a3a26a15 a68 a17 where the proximity is inversely proportional to the distance of a49a51a68 from incident a15a5a68 , in the story of a15 a68 : a3 a45a24a43a7a5 a14a50a49a2a3a26a15a51a17a20a19 a8a9 a10 a9a11 a7 a12a61a14a16a15a51a17a14a13 a7 a15 a14a50a49a2a3a26a15a51a17a17a16 a7 a18 a28a57a49 a60a62a15 a11 a43a18a8a20a19 a2a5a45a18a4 a18 a49a18a2 For an incident a15 mentioning location a58 , the IE system outputs the list of all states a66a46a49a81a79 mentioned in the same story ; we then rank each a49 according to the inverse of distance a15 : the number of sentences between a15 and a49 .</sentence>
				<definiendum id="0">list of all</definiendum>
				<definiens id="0">the number of sentences between a15 and a49</definiens>
			</definition>
			<definition id="3">
				<sentence>a12a61a14a16a15a51a17 is a normalization factor .</sentence>
				<definiendum id="0">a12a61a14a16a15a51a17</definiendum>
				<definiens id="0">a normalization factor</definiens>
			</definition>
</paper>

		<paper id="1127">
			<definition id="0">
				<sentence>The dialog agent’s state consists of information regarding the departure city , destination city , flight date , etc .</sentence>
				<definiendum id="0">dialog agent’s state</definiendum>
			</definition>
			<definition id="1">
				<sentence>Levin takes a useful approach in reducing the size of true state space by simply tracking when a particular state variable has a value rather than including the specific value in the state .</sentence>
				<definiendum id="0">Levin</definiendum>
				<definiens id="0">takes a useful approach in reducing the size of true state space by simply tracking when a</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>K· SC avg , where K is a parameter that allows us to control the amount of error filtering ( K % of the average semantic coherence score ) .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">a parameter that allows us to control the amount of error filtering</definiens>
			</definition>
			<definition id="1">
				<sentence>The semantic similarity score between two words w 1 and w 2 is defined as the probability of seeing the two words together divided by the probability of each word separately : PMI ( w 1 , w 2 ) = log [ P ( w 1 , w 2 ) / ( P ( w 1 ) ·P ( w 2 ) ) ] = log [ C ( w 1 , w 2 ) ⋅N / ( C ( w 1 ) ⋅C ( w 2 ) ) ] , where C ( w 1 , w 2 ) , C ( w 1 ) , C ( w 2 ) are frequency counts , and N is the total number of words in the corpus .</sentence>
				<definiendum id="0">semantic similarity score</definiendum>
				<definiendum id="1">C ( w</definiendum>
				<definiendum id="2">C ( w 1</definiendum>
				<definiendum id="3">N</definiendum>
				<definiens id="0">the probability of seeing the two words together divided by the probability of each word separately : PMI ( w 1</definiens>
				<definiens id="1">the total number of words in the corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>cision is the proportion of truly correct words contained in the list of content words which the algorithm labeled as correct .</sentence>
				<definiendum id="0">cision</definiendum>
				<definiens id="0">the proportion of truly correct words contained in the list of content words which the algorithm labeled as correct</definiens>
			</definition>
			<definition id="3">
				<sentence>Recall is the proportion of truly correct content words that the algorithm was able to retain .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">the proportion of truly correct content words that the algorithm was able to retain</definiens>
			</definition>
			<definition id="4">
				<sentence>F-measure is the geometric mean of P and R and expresses a tradeoff between those two measures .</sentence>
				<definiendum id="0">F-measure</definiendum>
				<definiens id="0">the geometric mean of P and R and expresses a tradeoff between those two measures</definiens>
			</definition>
			<definition id="5">
				<sentence>Content Words Error Rate ( cWER ) , % Lost good keywords ( % Lost ) and F-measure as a function of the filtering level K for the Window-PMI-3MAXconfiguration on the BBN dataset .</sentence>
				<definiendum id="0">Content Words Error Rate</definiendum>
				<definiens id="0">a function of the filtering level K for the Window-PMI-3MAXconfiguration on the BBN dataset</definiens>
			</definition>
			<definition id="6">
				<sentence>Note however that their results and ours are not completely comparable since the experiments used different audio corpora ( WSJCAM0 vs. TDT2 ) , but those two corpora seem to exhibit similar initial WERs ( the WER appears to be around 30 % for WSJCAM0 ; the WER is 27.6 % for our BBN dataset ) .</sentence>
				<definiendum id="0">WER</definiendum>
				<definiens id="0">the WER appears to be around 30 % for WSJCAM0</definiens>
			</definition>
</paper>

		<paper id="1091">
			<definition id="0">
				<sentence>One of the key tasks in natural language processing is that of Information Extraction ( IE ) , which is traditionally divided into three subproblems : coreference resolution , named entity recognition , and relation extraction .</sentence>
				<definiendum id="0">IE )</definiendum>
				<definiens id="0">coreference resolution , named entity recognition , and relation extraction</definiens>
			</definition>
			<definition id="1">
				<sentence>K ( x , y ) = braceleftBigg 0 , m negationslash= nproducttext n i=1 c ( xi , yi ) , m = n ( 1 ) where c ( xi , yi ) = |xi∩yi| is the number of common word classes between xi and yi .</sentence>
				<definiendum id="0">K</definiendum>
				<definiendum id="1">c</definiendum>
				<definiens id="0">the number of common word classes between xi and yi</definiens>
			</definition>
			<definition id="2">
				<sentence>The ROLE relation links people to an organization to which they belong , own , founded , or provide some service .</sentence>
				<definiendum id="0">ROLE relation</definiendum>
				<definiens id="0">links people to an organization to which they belong , own , founded , or provide some service</definiens>
			</definition>
			<definition id="3">
				<sentence>The PART relation indicates subset relationships , such as a state to a nation , or a subsidiary to its parent company .</sentence>
				<definiendum id="0">PART relation</definiendum>
				<definiens id="0">indicates subset relationships</definiens>
			</definition>
			<definition id="4">
				<sentence>Method Precision Recall F-measure ( S1 ) SP-CCG 67.5 37.2 48.0 ( S1 ) SP-CFG 71.1 39.2 50.5 ( S1 ) K4 70.3 26.3 38.0 ( S2 ) SP-CCG 63.7 41.4 50.2 ( S2 ) SP-CFG 65.5 43.8 52.5 ( S2 ) K4 67.1 35.0 45.8 Table 5 : Extraction Performance on ACE .</sentence>
				<definiendum id="0">Method Precision Recall F-measure</definiendum>
				<definiens id="0">Extraction Performance on ACE</definiens>
			</definition>
			<definition id="5">
				<sentence>In ( Zelenko et al. , 2003 ) , the 730 tree kernel is computed in O ( mn ) time , where m and n are the number of nodes in the two trees .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
			<definition id="6">
				<sentence>Our shortest-path intuition bears some similarity with the underlying assumption of the relational pathfinding algorithm from ( Richards and Mooney , 1992 ) : “in most relational domains , important concepts will be represented by a small number of fixed paths among the constants defining a positive instance – for example , the grandparent relation is defined by a single fixed path consisting of two parent relations.”</sentence>
				<definiendum id="0">shortest-path intuition</definiendum>
			</definition>
</paper>

		<paper id="2014">
			<definition id="0">
				<sentence>Japanese Speech Understanding Using Grammar Specialization Manny Rayner , Nikos Chatzichrisafis , Pierrette Bouillon University of Geneva , TIM/ISSCO 40 bvd du Pont-d’Arve , CH-1211 Geneva 4 , Switzerland mrayner @ riacs.edu { Pierrette.Bouillon , Nikolaos.Chatzichrisafis } @ issco.unige.ch Yukie Nakao , Hitoshi Isahara , Kyoko Kanzaki National Institute of Information and Communications Technology 3-5 Hikaridai , Seika-cho , Soraku-gun , Kyoto , Japan 619-0289 yukie-n @ khn.nict.go.jp , { isahara , kanzaki } @ nict.go.jp Beth Ann Hockey UCSC/NASA Ames Research Center Moffet Field , CA 94035 bahockey @ riacs.edu Marianne Santaholma , Marianne Starlander University of Geneva , TIM/ISSCO 40 bvd du Pont-d’Arve CH-1211 Geneva 4 , Switzerland Marianne.Santaholma @ eti.unige.ch Marianne.Starlander @ eti.unige.ch The most common speech understanding architecture for spoken dialogue systems is a combination of speech recognition based on a class N-gram language model , and robust parsing .</sentence>
				<definiendum id="0">spoken dialogue systems</definiendum>
				<definiens id="0">a combination of speech recognition based on a class N-gram language model</definiens>
			</definition>
			<definition id="1">
				<sentence>Voice recognition platforms like the Nuance Toolkit provide CFG-based languages for writing grammar-based language models ( GLMs ) , but it is challenging to develop and maintain grammars consisting of large sets of ad hoc phrase-structure rules .</sentence>
				<definiendum id="0">Voice recognition platforms</definiendum>
				<definiens id="0">challenging to develop and maintain grammars consisting of large sets of ad hoc phrase-structure rules</definiens>
			</definition>
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>KNOWITALL has a generateand-test architecture that extracts information in two stages .</sentence>
				<definiendum id="0">KNOWITALL</definiendum>
				<definiens id="0">a generateand-test architecture that extracts information in two stages</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , the generic pattern NP1 such as NPList2 indicates that the head of each simple noun phrase ( NP ) in NPList2 is a member of the class named in NP1 .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">the head of each simple noun phrase</definiens>
				<definiens id="1">a member of the class named in NP1</definiens>
			</definition>
			<definition id="2">
				<sentence>The Bindings Engine supports queries containing typed variables ( such as NounPhrase ) and 564 string-processing functions ( such as head ( X ) or ProperNoun ( X ) ) as well as standard query terms .</sentence>
				<definiendum id="0">Bindings Engine</definiendum>
				<definiendum id="1">ProperNoun</definiendum>
				<definiens id="0">supports queries containing typed variables ( such as NounPhrase ) and 564 string-processing functions ( such as head ( X</definiens>
			</definition>
			<definition id="3">
				<sentence>BE’s novel neighborhood index enables it to process these queries with O ( k ) random disk seeks and O ( k ) serial disk reads , where k is the number of non-variable terms in its query .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">the number of non-variable terms in its query</definiens>
			</definition>
			<definition id="4">
				<sentence>The neighborhood index is an augmented inverted index structure .</sentence>
				<definiendum id="0">neighborhood index</definiendum>
				<definiens id="0">an augmented inverted index structure</definiens>
			</definition>
			<definition id="5">
				<sentence>k is the number of concrete terms in the query , B is the number of variable bindings found in the corpus , and N is the number of documents in the corpus .</sentence>
				<definiendum id="0">k</definiendum>
				<definiendum id="1">B</definiendum>
				<definiendum id="2">N</definiendum>
				<definiens id="0">the number of concrete terms in the query</definiens>
				<definiens id="1">the number of variable bindings found in the corpus</definiens>
				<definiens id="2">the number of documents in the corpus</definiens>
			</definition>
			<definition id="6">
				<sentence>comes from integrating the results of corpus processing with the inverted index ( which determines which of those results are relevant ) .</sentence>
				<definiendum id="0">inverted index</definiendum>
				<definiens id="0">determines which of those results are relevant )</definiens>
			</definition>
			<definition id="7">
				<sentence>Table 1 shows that BE requires only O ( k ) random disk seeks to process queries with an arbitrary number of variables whereas a standard engine takes O ( k + B ) , where k is the number of concrete query terms , and B is the number of bindings found in a corpus of N documents .</sentence>
				<definiendum id="0">k</definiendum>
				<definiendum id="1">B</definiendum>
				<definiens id="0">process queries with an arbitrary number of variables whereas a standard engine takes O ( k + B )</definiens>
				<definiens id="1">the number of concrete query terms , and</definiens>
				<definiens id="2">the number of bindings found in a corpus of N documents</definiens>
			</definition>
			<definition id="8">
				<sentence>For example , Turney operates a search engine with a terabyte-sized index of Web pages , running on a local eight-machine Beowulf cluster ( Turney , 2004 ) .</sentence>
				<definiendum id="0">Turney</definiendum>
			</definition>
			<definition id="9">
				<sentence>Our probabilistic model , which we call URNS , takes the form of a classic balls-and-urns model from combinatorics .</sentence>
				<definiendum id="0">probabilistic model</definiendum>
				<definiens id="0">takes the form of a classic balls-and-urns model from combinatorics</definiens>
			</definition>
			<definition id="10">
				<sentence>P ( x 2 Cjx appearsk times in n draws ) =summationtext r∈num ( C ) ( r s ) k ( 1 r s ) n−k summationtext rprime∈num ( C∪E ) ( rprime s ) k ( 1 rprime s ) n−k ( 1 ) where s is the total number of balls in the urn , and the sum is taken over possible repetition rates r. A few numerical examples illustrate the behavior of this equation .</sentence>
				<definiendum id="0">C∪E )</definiendum>
				<definiendum id="1">s</definiendum>
				<definiens id="0">the total number of balls in the urn , and the sum is taken over possible repetition rates r. A few numerical examples illustrate the behavior of this equation</definiens>
			</definition>
			<definition id="11">
				<sentence>Let freq ( R ( X , y ) ) denote the number of times that the binary relation R ( X , y ) is extracted ; we de ne : smoothed freq ( R ( X , y ) ) = freq ( R ( X , y ) ) max yprime freq ( R ( X , yprime ) ) + 1 We found that sorting by smoothed frequency ( in descending order ) performed better than simply sorting by freq for relations R ( X , y ) in which different known X values may have widely varying Web presence .</sentence>
				<definiendum id="0">R</definiendum>
				<definiendum id="1">max yprime freq</definiendum>
				<definiendum id="2">R</definiendum>
				<definiens id="0">in descending order ) performed better than simply sorting by freq for relations R ( X , y ) in which different known X values may have widely varying Web presence</definiens>
			</definition>
			<definition id="12">
				<sentence>Search time for KNOWITNOW is measured in seconds and search time for KNOWITALL is measured in hours .</sentence>
				<definiendum id="0">Search time for KNOWITNOW</definiendum>
				<definiens id="0">measured in seconds and search time for KNOWITALL is measured in hours</definiens>
			</definition>
			<definition id="13">
				<sentence>The reason for KNOWITNOW’s dif culty at precision 0.9 is due to extraction errors that occur with high frequency , particularly generic references to companies ( the Seller is a corporation ... , corporations such as Banks , etc. ) and truncation of certain company names by the extraction rules .</sentence>
				<definiendum id="0">Seller</definiendum>
				<definiens id="0">a corporation ...</definiens>
			</definition>
			<definition id="14">
				<sentence>There is little published information about its indexing system , but the user manual suggests its corpus is a combination of indexed sentences and user-speci c document collections driven by the user’s AltaVista queries .</sentence>
				<definiendum id="0">user-speci c document</definiendum>
			</definition>
			<definition id="15">
				<sentence>We described KNOWITNOW , which extracts thousands of facts in minutes instead of days .</sentence>
				<definiendum id="0">KNOWITNOW</definiendum>
				<definiens id="0">extracts thousands of facts in minutes instead of days</definiens>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>On the Switchboard corpus of conversational speech , the system achieves improved parse accuracy over a state-of-the-art system which uses only lexical and syntactic features .</sentence>
				<definiendum id="0">Switchboard</definiendum>
				<definiens id="0">corpus of conversational speech , the system achieves improved parse accuracy over a state-of-the-art system which uses only lexical and syntactic features</definiens>
			</definition>
			<definition id="1">
				<sentence>PCFG TAG Oracle Edit F ( Table 2 ) 65.8 78.2 100.0 Parser 1-best 84.4 85.0 86.9 Prosodic feats 85.0 85.6 87.6 Syntactic feats 85.9 86.4 88.4 Combined feats 86.0 86.6 88.6 Oracle-rate 92.6 93.2 95.2 used to parse the test data.5 The TAG-based detector was trained on the same conversation sides , with its channel model trained on the Penn Treebank disfluency-annotated files and its language model trained on trees with the EDITED nodes excised .</sentence>
				<definiendum id="0">PCFG TAG Oracle Edit F</definiendum>
				<definiens id="0">with its channel model trained on the Penn Treebank disfluency-annotated files and its language model trained on trees with the EDITED nodes excised</definiens>
			</definition>
</paper>

		<paper id="1112">
			<definition id="0">
				<sentence>The judges’ agreement was measured using the Kappa statistics ( Siegel and Castelan , 1988 ) , one of the most frequently used measure of interannotator agreement for classification tasks : K = Pr ( A ) −Pr ( E ) 1−Pr ( E ) , where Pr ( A ) is the proportion of times the raters agree and Pr ( E ) is the probability of agreement by chance .</sentence>
				<definiendum id="0">Kappa statistics</definiendum>
				<definiendum id="1">Pr ( A )</definiendum>
				<definiens id="0">the proportion of times the raters agree and Pr ( E ) is the probability of agreement by chance</definiens>
			</definition>
			<definition id="1">
				<sentence>Let’s define with xi the feature vector of an instance i and let X be the space of all instances ; i.e. xi ∈ X. The multi-class classification is performed by a function that maps the feature space X into a semantic space S F : X → S , where S is the set of semantic relations from Table 1 , i.e. rk ∈ S. Let T be the training set of examples or instances T = ( x1r1 , x2r2 , ... , xnrn ) ⊆ ( X ×S ) n where n is the number of examples x each accompanied by its semantic relation label r. The problem is to decide which semantic relation r to assign to a new , unseen example xn+1 .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">T</definiendum>
				<definiendum id="2">n</definiendum>
				<definiens id="0">the space of all instances ; i.e. xi ∈ X. The multi-class classification is performed by a function that maps the feature space X into a semantic space S F : X → S , where</definiens>
				<definiens id="1">the set of semantic relations from Table 1 , i.e. rk ∈ S. Let</definiens>
				<definiens id="2">the number of examples x each accompanied by its semantic relation</definiens>
			</definition>
			<definition id="2">
				<sentence>The positive and negative genitive examples of the training corpus are pairs of concepts of the format : &lt; modifier semclass # WNsense ; head semclass # WNsense ; target &gt; , where target is a set of at least one of the 36 semantic relations .</sentence>
				<definiendum id="0">target</definiendum>
				<definiens id="0">a set of at least one of the 36 semantic relations</definiens>
			</definition>
			<definition id="3">
				<sentence>A boundary G∗ in the WordNet noun hierarchies is a set of synset pairs such that : a ) for any feature pair on the boundary , denoted fG∗ij ∈ G∗ , fG∗ij maps uniquely into only one relation r , and b ) for any fuij follows fG∗ij , fuij maps into more than one relation r , and c ) for any flij ≺ fG∗ij , flij maps uniquely into a semantic relation r. Here relations follows and ≺ mean “semantically more general” and “semantically more specific” respectively .</sentence>
				<definiendum id="0">boundary G∗</definiendum>
				<definiens id="0">a set of synset pairs such that : a ) for any feature pair on the boundary , denoted fG∗ij ∈ G∗ , fG∗ij maps uniquely into only one relation r , and b ) for any fuij follows fG∗ij , fuij maps into more than one relation r , and c ) for any flij ≺ fG∗ij , flij maps uniquely into a semantic relation r. Here relations follows and ≺ mean “semantically more general” and “semantically more specific” respectively</definiens>
			</definition>
			<definition id="4">
				<sentence>The specialization procedure consists of first identifying features fkij to which correspond more than one semantic relation , then replace these features with their hyponyms synsets .</sentence>
				<definiendum id="0">specialization procedure</definiendum>
			</definition>
			<definition id="5">
				<sentence>LIBSVM : a Library for Support Vector Machines , http : //www.csie.ntu.edu.tw/ cjlin/papers/libsvm .</sentence>
				<definiendum id="0">LIBSVM</definiendum>
				<definiens id="0">a Library for Support Vector Machines , http : //www.csie.ntu.edu.tw/ cjlin/papers/libsvm</definiens>
			</definition>
</paper>

		<paper id="1089">
			<definition id="0">
				<sentence>D is the total number of documents in the collection .</sentence>
				<definiendum id="0">D</definiendum>
				<definiens id="0">the total number of documents in the collection</definiens>
			</definition>
			<definition id="1">
				<sentence>For each word x , there are a set of postings , X. X contains a set of document IDs , one for each document containing x. The size of postings , fx = |X| , corresponds to the margins of the contingency tables in Figure 1 ( a ) , also known as document frequencies in IR .</sentence>
				<definiendum id="0">X. X</definiendum>
				<definiens id="0">contains a set of document IDs , one for each document containing x. The size of postings , fx = |X| , corresponds to the margins of the contingency tables in Figure 1 ( a ) , also known as document frequencies in IR</definiens>
			</definition>
			<definition id="2">
				<sentence>Similarly , Y denotes the postings for word y , and skY denotes its sketch , MINsy ( Y ) .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiendum id="1">skY</definiendum>
				<definiens id="0">the postings for word y , and</definiens>
			</definition>
			<definition id="3">
				<sentence>Broder assumed sx = sy = s. Broder defined resemblance ( R ) and sample resemblance ( Rs ) to be : R = aa + b + c , Rs = |MINs ( skX ∪skY ) ∩skX ∩skY||MIN s ( skX ∪skY ) | .</sentence>
				<definiendum id="0">Broder</definiendum>
			</definition>
			<definition id="4">
				<sentence>0 0.2 0.4 0.6 0.8 10 1 Sampling rates Percentage of inersections Random sampling Sketch Figure 2 : Sketches ( solid curves ) dominate random sampling ( dashed curve ) .</sentence>
				<definiendum id="0">Sketches</definiendum>
				<definiens id="0">solid curves ) dominate random sampling ( dashed curve )</definiens>
			</definition>
			<definition id="5">
				<sentence>When we compare two sketches , skX and skY , we have effectively looked at Ds = min { skX ( sx ) , skY ( sy ) } documents , where skX ( j ) is the jth smallest element in skX .</sentence>
				<definiendum id="0">skY</definiendum>
				<definiens id="0">the jth smallest element in skX</definiens>
			</definition>
			<definition id="6">
				<sentence>D = 36 ( b ) Figure 3 : ( a ) : The two sketches , skX and skY ( larger shaded box ) , are used to construct a sample contingency table : as , bs , cs , ds .</sentence>
				<definiendum id="0">skX</definiendum>
			</definition>
			<definition id="7">
				<sentence>A popular metric is mean square error ( MSE ) : MSE ( ˆa ) = E ( ˆa−a ) 2 = Var ( ˆa ) +Bias2 ( ˆa ) .</sentence>
				<definiendum id="0">popular metric</definiendum>
				<definiens id="0">mean square error ( MSE ) : MSE ( ˆa ) = E</definiens>
			</definition>
			<definition id="8">
				<sentence>Note that skX ( sx ) is the order statistics of a discrete random variable ( Siegrist , 1997 ) with expectation E`skX ( sx ) ´= sx ( D + 1 ) f x + 1 ≈ sxf x D. ( 14 ) By Jensen’s inequality , we know that E „D s D « ≤ min E`skX ( sx ) ´ D , E`skY ( sy ) ´ D !</sentence>
				<definiendum id="0">sx )</definiendum>
				<definiens id="0">the order statistics of a discrete random variable ( Siegrist , 1997 ) with expectation E`skX</definiens>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>Automatic Detection of Translation Errors : The State of the Art Graham Russell and Ngoc Tran Nguyen IIT–ILT , National Research Council Canada RALI-DIRO , Universit´e de Montr´eal∗ { russell , nguyentt } @ iro.umontreal.ca George Foster IIT–ILT , National Research Council Canada† george.foster @ nrc-cnrc.gc.ca The demonstration presents TransCheck , a translation quality-assurance tool developed jointly by the RALI group at the University of Montreal and the Interactive Language Technologies section of the Canadian National Research Council’s Institute for Information Technology .</sentence>
				<definiendum id="0">Automatic Detection of Translation Errors</definiendum>
				<definiens id="0">a translation quality-assurance tool developed jointly by the RALI group at the University of Montreal and the Interactive Language Technologies section of the Canadian National Research Council’s Institute for Information Technology</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>The edge weight s jk represents the degree to which word j in one sentence can translate into the word k in the other sentence .</sentence>
				<definiendum id="0">edge weight s jk</definiendum>
				<definiens id="0">represents the degree to which word j in one sentence can translate into the word k in the other sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>The score of an assignment is the sum of edge scores : s ( y ) = summationtext jk s jk y jk .</sentence>
				<definiendum id="0">score of an assignment</definiendum>
				<definiens id="0">the sum of edge scores : s ( y ) = summationtext jk s jk y jk</definiens>
			</definition>
			<definition id="2">
				<sentence>While the F-measure is a natural loss function for this task , we instead chose a sensible surrogate that fits better in our framework : Hamming distance between y i and ¯y i , which simply counts the number of edges predicted incorrectly .</sentence>
				<definiendum id="0">F-measure</definiendum>
				<definiens id="0">the number of edges predicted incorrectly</definiens>
			</definition>
			<definition id="3">
				<sentence>Hence , without any approximations , we have a continuous optimization problem instead of a combinatorial one : max ¯y i ∈Y i w latticetop f i ( ¯y i ) +lscript i ( ¯y i ) =d i +max z i ∈Z i ( w latticetop F i +c i ) latticetop z i , where d i = summationtext jk c y i , jk is the constant term , F i is the appropriate matrix that has a column of features f ( x i , jk ) for each edge jk , c i is the vector of the loss terms c + − ( c + c + ) y i , jk and finally Z i = { z i : summationtext j z i , jk ≤ 1 , summationtext k z i , jk ≤ 1 , 0 ≤ z i , jk ≤ 1 } .</sentence>
				<definiendum id="0">F i</definiendum>
				<definiendum id="1">c i</definiendum>
				<definiens id="0">the constant term</definiens>
			</definition>
			<definition id="4">
				<sentence>Using these alignments , alignment error rate ( AER ) is calculated as : AER ( A , S , P ) =1− |A ∩ S| + |A ∩ P| |A| + |S| Here , A is a set of proposed index pairs , S is the sure gold pairs , and P is the possible gold pairs .</sentence>
				<definiendum id="0">AER</definiendum>
				<definiendum id="1">AER</definiendum>
				<definiendum id="2">S</definiendum>
				<definiendum id="3">P</definiendum>
				<definiens id="0">the possible gold pairs</definiens>
			</definition>
			<definition id="5">
				<sentence>value is the Dice coeﬃcient ( Dice , 1945 ) : Dice ( e , f ) = 2C EF ( e , f ) C E ( e ) C F ( f ) Here , C E and C F are counts of word occurrences in each language , while C EF is the number of co-occurrences of the two words .</sentence>
				<definiendum id="0">f ) C E ( e ) C F ( f</definiendum>
				<definiendum id="1">C EF</definiendum>
				<definiens id="0">the number of co-occurrences of the two words</definiens>
			</definition>
			<definition id="6">
				<sentence>One source of constraint which our model still does not explicitly capture is the first-order dependency between alignment positions , as in the HMM model ( Vogel et al. , 1996 ) and IBM models 4+ .</sentence>
				<definiendum id="0">IBM</definiendum>
				<definiens id="0">the first-order dependency between alignment positions</definiens>
			</definition>
			<definition id="7">
				<sentence>Learning structured prediction models : a large margin approach .</sentence>
				<definiendum id="0">Learning structured prediction models</definiendum>
				<definiens id="0">a large margin approach</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>K ( T1 , T2 ) = summationdisplay Si W ( Si ) · # Si ( T1 ) · # Si ( T2 ) , ( 1 ) where Si is a possible subtree , # Si ( Tj ) is the number of times Si is included in Tj , and W ( Si ) is the weight of Si .</sentence>
				<definiendum id="0">K</definiendum>
				<definiendum id="1">Si</definiendum>
				<definiendum id="2">Si ( Tj )</definiendum>
				<definiendum id="3">W ( Si</definiendum>
				<definiens id="0">a possible subtree</definiens>
				<definiens id="1">the number of times Si is included in Tj</definiens>
				<definiens id="2">the weight of Si</definiens>
			</definition>
			<definition id="1">
				<sentence>DEFINITION 2.1 S is included in T iff there exists a one-to-one function ψ from a node of S to a node of T such that ( i ) pa ( ψ ( ni ) ) = ψ ( pa ( ni ) ) ( pa ( ni ) returns the parent of node ni ) , ( ii ) ψ ( ni ) followsequal ψ ( nj ) iff ni followsequal nj ( ni followsequal nj means that ni is an elder sibling of nj ) , and ( iii ) l ( ψ ( ni ) ) = l ( ni ) ( l ( ni ) returns the label of ni ) .</sentence>
				<definiendum id="0">ni</definiendum>
				<definiens id="0">pa ( ni ) returns the parent of node ni ) , ( ii ) ψ ( ni ) followsequal ψ</definiens>
				<definiens id="1">an elder sibling of nj )</definiens>
				<definiens id="2">l ( ni ) returns the label of ni )</definiens>
			</definition>
			<definition id="2">
				<sentence>( 1 ) can be transformed : K ( T1 , T2 ) = summationdisplay n1∈T1 summationdisplay n2∈T2 C ( n1 , n2 ) , C ( n1 , n2 ) ≡PSi W ( Si ) · # Si ( T1 triangle n1 ) · # Si ( T2 triangle n2 ) , where # Si ( Tj triangle nk ) is the number of times Si is included in Tj with ψ ( root ( Si ) ) = nk .</sentence>
				<definiendum id="0"># Si ( Tj triangle nk )</definiendum>
			</definition>
			<definition id="3">
				<sentence>The key is the use of Cr ( n1 , n2 ) , which stores the sum over only marked subtrees , and its recursive calculation using C ( n1 , n2 ) and Cr ( n1 , n2 ) ( B ) .</sentence>
				<definiendum id="0">key</definiendum>
				<definiens id="0">stores the sum over only marked subtrees</definiens>
			</definition>
			<definition id="4">
				<sentence>In most kernelbased methods , such as SVMs , we actually need to calculate the kernel values with all the training examples for a given example Ti : KS ( Ti ) = { K ( Ti , T1 ) , ... , K ( Ti , TL ) } , where L is the number of training examples .</sentence>
				<definiendum id="0">KS</definiendum>
				<definiendum id="1">L</definiendum>
				<definiens id="0">the number of training examples</definiens>
			</definition>
			<definition id="5">
				<sentence>• occ ( Fi ) returns occurrence list of Fi whose element ( j , n ) indicates that Fi appears in Tj and that n ( of Tj ) is the node added to generated Fi in Tj by the RME ( n works as the position of Fi in Tj ) .</sentence>
				<definiendum id="0">Fi )</definiendum>
				<definiens id="0">returns occurrence list of Fi whose element ( j , n ) indicates that Fi appears in Tj and that n ( of Tj ) is the node added to generated Fi in Tj by the RME ( n works as the position of Fi in Tj )</definiens>
			</definition>
			<definition id="6">
				<sentence>• malicious ( Fi ) returns true iff all pairs in sup ( Fi ) are already registered in the set of malicious pairs , M. ( Currently , this returns false if |sup ( Fi ) | &gt; M where M is the maximum support size of the malicious subtrees so far .</sentence>
				<definiendum id="0">M. ( Currently</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">iff all pairs in sup ( Fi ) are already registered in the set of malicious pairs</definiens>
				<definiens id="1">the maximum support size of the malicious subtrees so far</definiens>
			</definition>
			<definition id="7">
				<sentence>We used TinySVM3 as the implementation of SVM and added our tree kernels , Krlo and Krc .</sentence>
				<definiendum id="0">TinySVM3</definiendum>
				<definiens id="0">the implementation of SVM and added our tree kernels</definiens>
			</definition>
			<definition id="8">
				<sentence>Number of examples conversionSVM ( DP+lookup ) SVM ( proposed ) Figure 4 : Scaling of conversion time and SVM training time .</sentence>
				<definiendum id="0">SVM</definiendum>
				<definiens id="0">Scaling of conversion time and SVM training time</definiens>
			</definition>
</paper>

		<paper id="2010">
</paper>

		<paper id="1103">
			<definition id="0">
				<sentence>WordNet is a lexical resource in which English nouns , verbs , adjectives , and adverbs are grouped into synonym sets .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">a lexical resource in which English nouns , verbs , adjectives , and adverbs are grouped into synonym sets</definiens>
			</definition>
			<definition id="1">
				<sentence>Hypernym is the generic term used to describe a whole class of specific instances .</sentence>
				<definiendum id="0">Hypernym</definiendum>
				<definiens id="0">the generic term used to describe a whole class of specific instances</definiens>
			</definition>
			<definition id="2">
				<sentence>Wordbank : verbose infallible obdurate opaque Choose the word from the wordbank that best completes each phrase below : Fig .</sentence>
				<definiendum id="0">Wordbank</definiendum>
			</definition>
			<definition id="3">
				<sentence>The Nelson-Denny Reading Test is a standardized test of vocabulary and reading comprehension ( Brown , 1981 ) .</sentence>
				<definiendum id="0">Nelson-Denny Reading Test</definiendum>
			</definition>
			<definition id="4">
				<sentence>The Raven’s Matrices Test is a test of non-verbal reasoning ( Raven , 1960 ) .</sentence>
				<definiendum id="0">Raven’s Matrices Test</definiendum>
			</definition>
			<definition id="5">
				<sentence>The Lexical Knowledge Battery has multiple subsections that test orthographic and phonological skills ( Perfetti and Hart , 2001 ) .</sentence>
				<definiendum id="0">Lexical Knowledge Battery</definiendum>
				<definiens id="0">has multiple subsections that test orthographic and phonological skills</definiens>
			</definition>
</paper>

		<paper id="1077">
			<definition id="0">
				<sentence>Most previous work on distributional similarity has either focused on a specific word-word relation ( such as Pereira et al. ( 1993 ) referring to a direct object noun for describing verbs ) , or used any dependency relation detected by the chunker or parser ( such as Lin ( 1999 ; 1998 ) , and McCarthy et al. ( 2003 ) ) .</sentence>
				<definiendum id="0">McCarthy</definiendum>
				<definiens id="0">1993 ) referring to a direct object noun for describing verbs ) , or used any dependency relation detected by the chunker or parser</definiens>
			</definition>
			<definition id="1">
				<sentence>The morpho-syntactic analysis is a preparatory step for the analyses to follow .</sentence>
				<definiendum id="0">morpho-syntactic analysis</definiendum>
				<definiens id="0">a preparatory step for the analyses to follow</definiens>
			</definition>
			<definition id="2">
				<sentence>( Also see is an underspecified relation , which captures relationships other than the preceding ones .</sentence>
				<definiendum id="0">underspecified relation</definiendum>
				<definiens id="0">captures relationships other than the preceding ones</definiens>
			</definition>
			<definition id="3">
				<sentence>Table 3 shows the number of semantic relations encoded in our GermaNet version , and the frequencies and probabilities of our response tokens found among them.4 For example , there are 9,275 verb-verb instances where GermaNet defines a hypernymy-hyponymy relation between their synsets ; for 2,807 of our verb-verb pairs ( verb response tokens with respect to target verbs ) we found a hypernymy relation among the GermaNet definitions , which accounts for 14 % of all our verb responses .</sentence>
				<definiendum id="0">GermaNet</definiendum>
				<definiens id="0">a hypernymy-hyponymy relation between their synsets</definiens>
			</definition>
</paper>

		<paper id="2016">
			<definition id="0">
				<sentence>We use another tool , Nstein NconceptTM , to extract concepts , which capture the themes or relevant phrases in a document .</sentence>
				<definiendum id="0">extract concepts</definiendum>
				<definiens id="0">capture the themes or relevant phrases in a document</definiens>
			</definition>
			<definition id="1">
				<sentence>NConcept uses a combination of statistics and linguistic rules .</sentence>
				<definiendum id="0">NConcept</definiendum>
				<definiens id="0">uses a combination of statistics and linguistic rules</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>SVM contains a large class of neural nets , radial margin separation ( RBF ) nets , and polynomial classifiers as special cases .</sentence>
				<definiendum id="0">SVM</definiendum>
				<definiens id="0">contains a large class of neural nets , radial margin separation ( RBF ) nets , and polynomial classifiers as special cases</definiens>
			</definition>
			<definition id="1">
				<sentence>idf formula L ( tf , d ) = tf / |d| , where tf is the number of occurrences of the term t in the document , |d| is the length of the document vector and G ( t ) = log ( N / df ( t ) ) , where df ( t ) is the total number of documents in the collection that have term t and N is the total number of documents .</sentence>
				<definiendum id="0">tf</definiendum>
				<definiendum id="1">|d|</definiendum>
				<definiendum id="2">df ( t )</definiendum>
				<definiendum id="3">N</definiendum>
				<definiens id="0">the number of occurrences of the term t in the document</definiens>
				<definiens id="1">the length of the document vector and G ( t ) = log</definiens>
				<definiens id="2">the total number of documents in the collection that have term t</definiens>
				<definiens id="3">the total number of documents</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , in some very “primitive” DBL approach , we can define two classes : Class S ( “strong” ) , containing all multiple occurrences of a rare query term ( e.g. “discretization” ) in a document and Class W ( “weak” ) , containing all single occurrences of a frequent term ( e.g. “information” ) .</sentence>
				<definiendum id="0">Class S</definiendum>
				<definiens id="0">containing all multiple occurrences of a rare query term ( e.g. “discretization” ) in a document</definiens>
			</definition>
			<definition id="3">
				<sentence>The bin determines the weight of the contribution of each occurrence of the query term in the ranking score .</sentence>
				<definiendum id="0">bin</definiendum>
				<definiens id="0">determines the weight of the contribution of each occurrence of the query term in the ranking score</definiens>
			</definition>
			<definition id="4">
				<sentence>We discretized the shape of the G ( t ) function by assigning each term to its global weighting bin g , which is an integer number in the [ 1 , |B| ] range , |B| is the total number of global weighting bins .</sentence>
				<definiendum id="0">|B|</definiendum>
				<definiens id="0">the total number of global weighting bins</definiens>
			</definition>
			<definition id="5">
				<sentence>A feature vector f ( d , q ) represents each document d with respect to query q. The value of each feature in the vector is just the number of the term occurrences assigned to the pair of bins ( g , l ) : f ( d , q ) [ g , l ] = ∑ ==⊂ ldtlgtg ) , ( , ) ( q , t 1 ( 2 ) Since our features capture local ( tf ) and global ( df ) term occurrence information , in order to represent a ranking function , we can simply use the dot product between the feature vector and the vector of learned optimal weights w : R ( q , d ) = w * f ( d , q ) .</sentence>
				<definiendum id="0">feature vector f</definiendum>
				<definiens id="0">local ( tf ) and global ( df ) term occurrence information</definiens>
			</definition>
			<definition id="6">
				<sentence>We used the most popular average ( noninterpolated ) precision as our performance metric , computed by the script included with the Lemur toolkit ( later verified by trec_eval ) .</sentence>
				<definiendum id="0">toolkit</definiendum>
				<definiens id="0">performance metric , computed by the script included with the Lemur</definiens>
			</definition>
			<definition id="7">
				<sentence>lw , which includes practically all the popular “bag of words” ranking formulas such as tf .</sentence>
				<definiendum id="0">lw</definiendum>
				<definiens id="0">includes practically all the popular “bag of words” ranking formulas such as tf</definiens>
			</definition>
</paper>

		<paper id="1113">
			<definition id="0">
				<sentence>Multi-word expressions ( MWEs ) are those whose structure and meaning can not be derived from their component words , as they occur independently .</sentence>
				<definiendum id="0">Multi-word expressions ( MWEs )</definiendum>
				<definiens id="0">those whose structure and meaning can not be derived from their component words</definiens>
			</definition>
			<definition id="1">
				<sentence>a0 is defined as , a1a3a2 a4a6a5a8a7a10a9a12a11a14a13a10a15a17a16a8a18 a5a20a19 a18 a9a22a21 a11a14a13a10a15a17a16a8a23 a5a24a19 a23 a9a25a21 a26 a16a28a27a30a29 a19 a27a14a31 a21 a16a28a27a32a29 a19 a27a30a33 a21 a27a14a31 a2 a4a35a34 a5 a16a8a34 a5a20a19a37a36a38a21 a39 a40 a27a30a33 a2 a4a42a41 a5 a16a8a41 a5a20a19a43a36a38a21 a39 a40 a27a32a29 a2 a15a44a16a8a15 a19a37a36a38a21 a39 1computed from the ratings 901 where a0a2a1 ’s are the rankings of annotator1 and a3a4a1 ’s are the rankings of annotator2 , n is the number of collocations , a5a6a1 is the number of values in the a7a9a8 a10 group of tied a0 values and a11 a1 is the number of values in the a7 a8 a10 group of tied a3 values .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">a5a6a1</definiendum>
				<definiens id="0">the number of collocations</definiens>
			</definition>
			<definition id="2">
				<sentence>Point-wise Mutual information of a collocation ( Church and Hanks , 1989 ) is defined as , a16a18a17a19a11a21a20a23a22a25a24a27a26 a15a28a17a19a11a2a20a23a22a25a24a30a29a31a15a28a17a33a32a34a20a35a32a36a24 a15a28a17a19a11a2a20a35a32a36a24a30a29a37a15a28a17a33a32a34a20a23a22a25a24 where , a11 is the verb and a22 is the object of the collocation .</sentence>
				<definiendum id="0">Point-wise Mutual information of a collocation</definiendum>
				<definiendum id="1">a11</definiendum>
				<definiendum id="2">a22</definiendum>
				<definiens id="0">the verb</definiens>
				<definiens id="1">the object of the collocation</definiens>
			</definition>
			<definition id="3">
				<sentence>In this case , a38 is defined as , a38a41a17a19a11a2a20a23a22a25a24a41a26a46a16 a52 a0a21a1a19a63a64 a0 a38a41a17a19a11a44a1a33a20a23a22a65a64a53a24 a1 where a11 and a22 are the verb and object of collocations for which similar collocations do not exist .</sentence>
				<definiendum id="0">a38</definiendum>
				<definiendum id="1">a22</definiendum>
				<definiens id="0">the verb and object of collocations for which similar collocations do not exist</definiens>
			</definition>
			<definition id="4">
				<sentence>It is defined as , a0a2a17a57a22a25a24a27a26 a4a2a1 a1 a15a28a17a19a11 a1 a20a23a22a14a24 a48 where a48 is the number of verbs occurring with the object ( a22 ) , a11 a1 ’s are the verbs cooccuring with a22 and a15a28a17a19a11a44a1 a20a23a22a25a24a4a3a6a5 .</sentence>
				<definiendum id="0">a48</definiendum>
			</definition>
			<definition id="5">
				<sentence>Here , a5 is a threshold which can be set based on the corpus .</sentence>
				<definiendum id="0">a5</definiendum>
				<definiens id="0">a threshold which can be set based on the corpus</definiens>
			</definition>
			<definition id="6">
				<sentence>We define this feature as , a15 a17a19a11a21a20a23a22a25a24a41a26 a4a16a1 a1 a15a28a17a19a11 a1 a20a23a22a14a24 a29a16a0a25a7 a56a18a17 a17a19a11a21a20a60a11 a1 a24 a48 where a48a20a19a22a21a24a23 is the number of verbs occurring with a22 , a11a14a1 ’s are the verbs cooccuring with a22 and a15a28a17a19a11a44a1 a20a23a22a25a24a2a3a25a5 .</sentence>
				<definiendum id="0">a48a20a19a22a21a24a23</definiendum>
			</definition>
			<definition id="7">
				<sentence>LSA is a method of representing words/collocations as points in vector space .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">a method of representing words/collocations as points in vector space</definiens>
			</definition>
			<definition id="8">
				<sentence>The feature is defined as a33a23a17a57a39 a20a60a11 a42 a24a27a26a35a34 a32 a56 a7a49a16a37a36a61a51a14a38a31a17a57a39a53a20a60a11 a42 a24 a56 a7 a16a37a36a55a51a39a38a37a17a57a39 a20a60a11 a42 a24a28a26 a40 a56a53a52 a17a57a39a65a24a42a41 a40 a56 a52 a17a19a11 a42 a24 a43 a40 a56 a52 a17a57a39a35a24 a43 a29 a43 a40 a56a53a52 a17a19a11 a42 a24 a43 where , a39 is the collocation , a11 a42 is the verb of the collocation and lsa ( a0 ) is representation of a0 using the LSA model .</sentence>
				<definiendum id="0">a39</definiendum>
				<definiendum id="1">a11 a42</definiendum>
				<definiens id="0">a33a23a17a57a39 a20a60a11 a42 a24a27a26a35a34 a32 a56 a7a49a16a37a36a61a51a14a38a31a17a57a39a53a20a60a11 a42 a24 a56 a7 a16a37a36a55a51a39a38a37a17a57a39 a20a60a11 a42 a24a28a26 a40 a56a53a52 a17a57a39a65a24a42a41 a40 a56 a52 a17a19a11 a42 a24 a43 a40 a56 a52 a17a57a39a35a24 a43 a29 a43 a40 a56a53a52 a17a19a11 a42 a24 a43 where ,</definiens>
				<definiens id="1">the collocation</definiens>
				<definiens id="2">the verb of the collocation and lsa ( a0 ) is representation of a0 using the LSA model</definiens>
			</definition>
			<definition id="9">
				<sentence>This feature is defined as a44 a17a57a39 a20a60a11a4a22 a42 a24a41a26 a40 a56a53a52 a17a57a39a35a24 a41 a40 a56a53a52 a17a19a11a55a22a44a42a45a24 a43 a40 a56 a52 a17a57a39a35a24 a43 a29 a43 a40 a56 a52 a17a19a11a4a22 a42 a24 a43 where , a39 is the collocation and a11a4a22 a42 is the verb-form of the object a22 a42 .</sentence>
				<definiendum id="0">a39</definiendum>
				<definiens id="0">a44 a17a57a39 a20a60a11a4a22 a42 a24a41a26 a40 a56a53a52 a17a57a39a35a24 a41 a40 a56a53a52 a17a19a11a55a22a44a42a45a24 a43 a40 a56 a52 a17a57a39a35a24 a43 a29 a43 a40 a56 a52 a17a19a11a4a22 a42 a24 a43 where ,</definiens>
			</definition>
			<definition id="10">
				<sentence>a8 a17a57a39 a1a33a20a23a39a60a64a53a24a10a9a47a12 a2a12a11 a6 a7a14a13 a17a57a39 a1a49a24 a3a15a6 a7a14a13 a17a57a39a33a64 a24 where a39a43a1 and a39a60a64 are the collocations , a17a57a39a65a1a33a20a23a39a60a64a53a24a16a9 a12a3a2 if the collocation a39 a1 is ranked higher than a39 a64 for the optimal ranking a12 a2 , a13 a17a57a39 a1 a24 and a13 a17a57a39a60a64 a24 are the mapping onto features ( section 6 ) that represent the properties of the V-N collocations a39a65a1 and a39a60a64 respectively and a6a7 is the weight vector representing the ranking function a15a5a17 .</sentence>
				<definiendum id="0">a6a7</definiendum>
				<definiens id="0">the mapping onto features ( section 6 ) that represent the properties of the V-N collocations a39a65a1 and a39a60a64 respectively and</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>Named Entity Recognition ( NER ) is one of the key techniques in the fields of Information Extraction , Question Answering , Parsing , Metadata Tagging in Semantic Web , etc .</sentence>
				<definiendum id="0">Named Entity Recognition</definiendum>
				<definiendum id="1">NER )</definiendum>
				<definiens id="0">one of the key techniques in the fields of Information Extraction</definiens>
			</definition>
			<definition id="1">
				<sentence>Given a word/pos sequence as equation ( 1 ) : nnii twtwtwTW //// 11 LL= ( 1 ) where n is the number of words and t i is the POS of word w i .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of words</definiens>
			</definition>
			<definition id="2">
				<sentence>The POS Model employs POS features for NER .</sentence>
				<definiendum id="0">POS Model</definiendum>
				<definiens id="0">employs POS features for NER</definiens>
			</definition>
			<definition id="3">
				<sentence>ni21 wwwwW LL= ( 3 ) ni21 ttttT LL= ( 4 ) The Word Model estimates the probability of generating a NE from the viewpoint of word sequence , which can be expressed in equation ( 5 ) .</sentence>
				<definiendum id="0">Word Model</definiendum>
				<definiens id="0">estimates the probability of generating a NE from the viewpoint of word sequence</definiens>
			</definition>
			<definition id="4">
				<sentence>Therefore , the Hybrid Model consists of four sub-models : word context model P ( WC ) , POS context model P ( TC ) , word entity model P ( W|WC ) and POS entity model P ( T|TC ) .</sentence>
				<definiendum id="0">Hybrid Model</definiendum>
			</definition>
			<definition id="5">
				<sentence>( ) ( ) ( ) ( ) ( ) 1kiik 1liil1i ik1i ik1i wcwc 1k 2l wcwcwc 2k wcwc iwcwc w , ENe|wP w , MNe|wPBNe|wP ENeMNeMNeBNe|wwP wc|wwP − − × ×≅         = ∏ − = − 4484476 LL L ( 10 ) where , BNe , MNe and ENe denotes the first , middle and last characters respectively .</sentence>
				<definiendum id="0">ENe</definiendum>
				<definiens id="0">the first , middle and last characters respectively</definiens>
			</definition>
			<definition id="6">
				<sentence>JPN surnames list ( including 9189 items ) : Only those characters in the surname list can trigger person name recognition .</sentence>
				<definiendum id="0">JPN surnames list</definiendum>
				<definiens id="0">including 9189 items ) : Only those characters in the surname list can trigger person name recognition</definiens>
			</definition>
			<definition id="7">
				<sentence>WWW ( f ) WWW ( q e 1N21 1N21 N L L = ( 16 ) q ( w 1 w 2 …w N-1 ) in ( 16 ) denotes the number of different symbol w N that have directly followed the word sequence w 1 w 2 …w N-1 .</sentence>
				<definiendum id="0">WWW</definiendum>
				<definiens id="0">the number of different symbol w N that have directly followed the word</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>The whenever operator in line 4 specifies a side condition that restricts the set of expressions in the sum ( i.e. , only when N is the sentence length ) .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the sentence length )</definiens>
			</definition>
			<definition id="1">
				<sentence>“np”/Needed is the label of a partial np constituent that is still missing the list of subconstituents in Needed .</sentence>
				<definiendum id="0">“np”/Needed</definiendum>
				<definiens id="0">the label of a partial np constituent that is still missing the list of subconstituents in Needed</definiens>
			</definition>
			<definition id="2">
				<sentence>For typical algorithms , only finitely many different items ( theorems ) can be derived from a given finite input ( set of axioms ) .12 This ensures termination if one is doing unweighted deduction with 〈W , ⊕ , ⊗〉 = 〈 { T , F } , ∨ , ∧〉 , since the test at line 7 ensures that no item is processed more than once.13 The same test ensures termination if one is searching for the best proof or parse with ( say ) 〈W , ⊕ , ⊗〉 = 〈R≥0 , min , +〉 , where values are negated log probabilities .</sentence>
				<definiendum id="0">theorems )</definiendum>
				<definiens id="0">〈W , ⊕ , ⊗〉 = 〈R≥0 , min , +〉 , where values</definiens>
			</definition>
			<definition id="3">
				<sentence>DynaMITE supports the EM algorithm ( and many variants ) , supervised and unsupervised training of log-linear ( “maximum entropy” ) models using quasi-Newton methods , and smoothing-parameter tuning on development data .</sentence>
				<definiendum id="0">DynaMITE</definiendum>
				<definiens id="0">supports the EM algorithm ( and many variants ) , supervised and unsupervised training of log-linear ( “maximum entropy” ) models using quasi-Newton methods , and smoothing-parameter tuning on development data</definiens>
			</definition>
			<definition id="4">
				<sentence>IBAL ( Pfeffer , 2001 ) is an elegant and powerful language for probabilistic modeling ; it generalizes Bayesian networks in interesting ways.28 Since 26Sentences with ≤10 words , stripping punctuation .</sentence>
				<definiendum id="0">IBAL</definiendum>
			</definition>
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>The statistical significance is the probability that this level of overlap would be reached by chance under independent classifications given the values a , b , n : p = summationdisplay max ( a+b−n,0 ) ≤ c ≤ ⌊ ( a+b−E ) /2⌋or ⌈ ( a+b− ( n−E ) ) /2⌉ ≤ c ≤ min ( a , b ) „a c «„n − a b − c « / „n b « Also , we can measure the agreement between any two learned classifiers as − ( logp ) /n. Note that a classifier that strongly favors one sense will have low agreement with other classifiers .</sentence>
				<definiendum id="0">statistical significance</definiendum>
				<definiens id="0">the probability that this level of overlap would be reached by chance under independent classifications given the values a , b , n : p = summationdisplay max ( a+b−n,0 ) ≤ c ≤ ⌊ ( a+b−E ) /2⌋or ⌈ ( a+b− ( n−E ) ) /2⌉ ≤ c ≤ min ( a , b ) „a c «„n − a b − c « / „n b « Also , we can measure the agreement between any two learned classifiers as − ( logp ) /n. Note that a classifier that strongly favors one sense will have low agreement with other classifiers</definiens>
			</definition>
			<definition id="1">
				<sentence>Active learning is a kind of bootstrapping method that periodically requires new seeds : it turns to the user whenever it gets confused .</sentence>
				<definiendum id="0">Active learning</definiendum>
				<definiens id="0">a kind of bootstrapping method that periodically requires new seeds : it turns to the user whenever it gets confused</definiens>
			</definition>
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>We will also introduce a web-based interface to MindNet lexicons ( MNEX ) that is intended to make the data contained within MindNets more accessible for exploration .</sentence>
				<definiendum id="0">MNEX</definiendum>
				<definiens id="0">intended to make the data contained within MindNets more accessible for exploration</definiens>
			</definition>
			<definition id="1">
				<sentence>A MindNet is a collection of semantic relations that is automatically extracted from text data using a broad coverage parser .</sentence>
				<definiendum id="0">MindNet</definiendum>
				<definiens id="0">a collection of semantic relations that is automatically extracted from text data using a broad coverage parser</definiens>
			</definition>
			<definition id="2">
				<sentence>Semantic Relations The semantic relations that are stored in MindNet are directed , labeled relationships between two words ; see Table 1 : Attributive Manner Source Cause Means Synonym Goal Part Time Hypernym Possessor TypicalObject Location Result TypicalSubject Table 1 : A sampling of the semantic relations stored in MindNet These semantic relations are obtained from the Logical Form analysis of our broad coverage parser NLPwin ( Heidorn , 2000 ) .</sentence>
				<definiendum id="0">broad coverage parser NLPwin</definiendum>
				<definiens id="0">A sampling of the semantic relations stored in MindNet These semantic relations</definiens>
			</definition>
			<definition id="3">
				<sentence>The Logical Form is a labeled dependency analysis with function words removed .</sentence>
				<definiendum id="0">Logical Form</definiendum>
				<definiens id="0">a labeled dependency analysis with function words removed</definiens>
			</definition>
			<definition id="4">
				<sentence>MNEX ( MindNet Explorer ) is the web-based interface to MindNet that is designed to facilitate browsing MindNet structure and relations .</sentence>
				<definiendum id="0">MNEX ( MindNet Explorer )</definiendum>
				<definiens id="0">the web-based interface to MindNet that is designed to facilitate browsing MindNet structure and relations</definiens>
			</definition>
			<definition id="5">
				<sentence>MNEX displays paths based on the word or words that the 1 Swallow : a small bird with wings ( LDOCE ) .</sentence>
				<definiendum id="0">LDOCE</definiendum>
				<definiens id="0">a small bird with wings</definiens>
			</definition>
			<definition id="6">
				<sentence>A path is a set of links that connect one word to another within either a single semrel structure or by combining fragments from multiple semrel structures .</sentence>
				<definiendum id="0">path</definiendum>
				<definiens id="0">a set of links that connect one word to another within either a single semrel structure or by combining fragments from multiple semrel structures</definiens>
			</definition>
			<definition id="7">
				<sentence>MNEX allows the exploration of the rich set of relations through which paths connecting words are linked .</sentence>
				<definiendum id="0">MNEX</definiendum>
				<definiens id="0">allows the exploration of the rich set of relations through which paths connecting words are linked</definiens>
			</definition>
			<definition id="8">
				<sentence>WordNet : An Electronic Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="2019">
			<definition id="0">
				<sentence>POSBIOTM/W : A Development Workbench For Machine Learning Oriented Biomedical Text Mining System ∗ Kyungduk Kim , Yu Song , Gary Geunbae Lee Department of Computer Science and Engineering Pohang University of Science &amp; Technology ( POSTECH ) San 31 , Hyoja-Dong , Pohang , 790-784 , Republic of Korea { getta , songyu , gblee } @ postech.ac.kr The POSBIOTM/W1 is a workbench for machine-learning oriented biomedical text mining system .</sentence>
				<definiendum id="0">POSBIOTM/W</definiendum>
				<definiens id="0">A Development Workbench For Machine Learning Oriented Biomedical Text Mining System ∗ Kyungduk Kim</definiens>
				<definiens id="1">a workbench for machine-learning oriented biomedical text mining system</definiens>
			</definition>
</paper>

		<paper id="1104">
			<definition id="0">
				<sentence>First is the prior , which serves as a baseline .</sentence>
				<definiendum id="0">First</definiendum>
				<definiens id="0">the prior , which serves as a baseline</definiens>
			</definition>
			<definition id="1">
				<sentence>Second is the positive adaptation , which is the probability of a word appearing given that it has been primed .</sentence>
				<definiendum id="0">Second</definiendum>
				<definiens id="0">the probability of a word appearing given that it has been primed</definiens>
			</definition>
			<definition id="2">
				<sentence>The application of the adaptation metric is straightforward : we pick NP1 as the prime set and NP2 as the target set .</sentence>
				<definiendum id="0">NP2</definiendum>
				<definiens id="0">the prime set and</definiens>
			</definition>
			<definition id="3">
				<sentence>Only in the case of a single common noun in the WSJ and Switchboard corpora is the prior probability higher than the positive adaptation .</sentence>
				<definiendum id="0">Switchboard corpora</definiendum>
				<definiens id="0">the prior probability higher than the positive adaptation</definiens>
			</definition>
			<definition id="4">
				<sentence>In the first case , we predict that the parallelism effect is restricted to coordinate structures , while in the second case , we expect that parallelism ( a ) is independent of coordination , and ( b ) occurs in the wider discourse , i.e. , not only within sentences but also between sentences .</sentence>
				<definiendum id="0">parallelism</definiendum>
				<definiens id="0">independent of coordination , and ( b ) occurs in the wider discourse , i.e. , not only within sentences but also between sentences</definiens>
			</definition>
</paper>

		<paper id="1068">
			<definition id="0">
				<sentence>Caruana et al. ( 2004 ) have recently proposed ensemble selection as a technique for building an ensemble of classifiers that is optimized to an arbitrary performance metric .</sentence>
				<definiendum id="0">ensemble selection</definiendum>
				<definiens id="0">a technique for building an ensemble of classifiers that is optimized to an arbitrary performance metric</definiens>
			</definition>
			<definition id="1">
				<sentence>Ensemble selection is a tool usable by non-experts to find good classifiers optimized to task-specific metrics .</sentence>
				<definiendum id="0">Ensemble selection</definiendum>
			</definition>
			<definition id="2">
				<sentence>A model family is the set of models made by varying the parameters for one machine learning algorithm .</sentence>
				<definiendum id="0">model family</definiendum>
				<definiens id="0">the set of models made by varying the parameters for one machine learning algorithm</definiens>
			</definition>
			<definition id="3">
				<sentence>Standard Performance Metrics : We evaluate the framework with 8 metrics : accuracy ( ACC ) , average precision ( APR ) , break even point ( BEP ) , F-measure ( F1 ) , mean cross entropy ( MXE ) , root mean squared error ( RMS ) , area under the ROC curve ( ROC ) , and SAR .</sentence>
				<definiendum id="0">ACC</definiendum>
				<definiendum id="1">BEP</definiendum>
				<definiendum id="2">SAR</definiendum>
				<definiens id="0">mean cross entropy ( MXE ) , root mean squared error</definiens>
			</definition>
			<definition id="4">
				<sentence>Because computing the MUC-F1 and BCUBED metrics ( see Section 4.1 ) is expensive , ensemble selection only iterates 50 times for these metrics .</sentence>
				<definiendum id="0">BCUBED metrics</definiendum>
				<definiens id="0">expensive , ensemble selection only iterates 50 times for these metrics</definiens>
			</definition>
			<definition id="5">
				<sentence>Data Set : For our experiments we use the MUC6 corpus , which contains 60 documents annotated with coreference information .</sentence>
				<definiendum id="0">MUC6 corpus</definiendum>
				<definiens id="0">contains 60 documents annotated with coreference information</definiens>
			</definition>
			<definition id="6">
				<sentence>Essentially , a sentence’s score is the fraction of parent links correctly identified .</sentence>
				<definiendum id="0">sentence’s score</definiendum>
				<definiens id="0">the fraction of parent links correctly identified</definiens>
			</definition>
</paper>

		<paper id="2012">
			<definition id="0">
				<sentence>Information Extraction ( IE ) is a technology for finding facts in plain text , and coding them in a logical representation , such as a relational database .</sentence>
				<definiendum id="0">Information Extraction</definiendum>
				<definiendum id="1">IE</definiendum>
				<definiens id="0">a technology for finding facts in plain text , and coding them in a logical representation , such as a relational database</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>Bilingual word alignment is the first step of most current approaches to statistical machine translation .</sentence>
				<definiendum id="0">Bilingual word alignment</definiendum>
			</definition>
			<definition id="1">
				<sentence>We find it better , however , to adjust these probabilities by subtracting a small fixed discount from the link count : LP d ( f , e ) = links 1 ( f , e ) − d cooc ( f , e ) LP d ( f , e ) represents the estimated conditional link probability for the words f and e , links 1 ( f , e ) is the number of times they are linked by the simpler alignment model , d is the discount , and cooc ( f , e ) is the number of times they co-occur .</sentence>
				<definiendum id="0">e )</definiendum>
				<definiendum id="1">cooc</definiendum>
				<definiendum id="2">e )</definiendum>
				<definiens id="0">represents the estimated conditional link probability for the words f</definiens>
				<definiens id="1">the number of times they are linked by the simpler alignment model</definiens>
				<definiens id="2">the number of times they co-occur</definiens>
			</definition>
			<definition id="2">
				<sentence>An important difference between the LLR-based model and CLP-based model is that the LLR-based model considers each word-to-word link separately , but allows multiple links per word , as long as they 83 lead to an alignment consisting only of one-to-one and one-to-many links ( in either direction ) .</sentence>
				<definiendum id="0">CLP-based model</definiendum>
				<definiens id="0">the LLR-based model considers each word-to-word link separately , but allows multiple links per word , as long as they 83 lead to an alignment consisting only of one-to-one and one-to-many links ( in either direction )</definiens>
			</definition>
			<definition id="3">
				<sentence>We report the performance of our alignment models in terms of precision , recall , and alignment error rate ( AER ) as defined by Och and Ney ( 2003 ) : recall = |A ∩ S| |S| precision = |A ∩P| |A| AER = 1− |A ∩ P|+|A ∩S| |A|+|S| In these definitions , S denotes the set of alignments annotated as sure , P denotes the set of alignments annotated possible or sure , and A denotes the set of alignments produced by the method under test .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">the performance of our alignment models in terms of precision , recall , and alignment error rate ( AER ) as defined by Och and Ney ( 2003 ) : recall = |A ∩ S| |S| precision = |A ∩P| |A| AER = 1− |A ∩ P|+|A ∩S| |A|+|S| In these definitions ,</definiens>
				<definiens id="1">the set of alignments annotated as sure ,</definiens>
				<definiens id="2">the set of alignments annotated possible or sure , and A denotes the set of alignments produced by the method under test</definiens>
			</definition>
			<definition id="4">
				<sentence>We evaluated three models on the final test data : the LLR-based model ( LLR ) and the two CLP-based models , one with conditional link probabilities from 86 Alignment Recall Precision AER LLR 0.829 0.848 0.160 CLP 1 CLP 2 Table 1 : Discriminative Model Results .</sentence>
				<definiendum id="0">LLR</definiendum>
				<definiens id="0">Discriminative Model Results</definiens>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>339 Input : product class C , reviews R. Output : set of [ feature , ranked opinion list ] tuples R’←parseReviews ( R ) ; E←findExplicitFeatures ( R’ , C ) ; O←findOpinions ( R’ , E ) ; CO←clusterOpinions ( O ) ; I←findImplicitFeatures ( CO , E ) ; RO←rankOpinions ( CO ) ; { ( f , oi , ... oj ) ... } ←outputTuples ( RO , I∪E ) ; Figure 1 : OPINE Overview .</sentence>
				<definiendum id="0">; E←findExplicitFeatures</definiendum>
				<definiendum id="1">O←findOpinions</definiendum>
				<definiendum id="2">I←findImplicitFeatures</definiendum>
				<definiendum id="3">RO←rankOpinions</definiendum>
				<definiens id="0">product class C , reviews R. Output : set of [ feature , ranked opinion list ] tuples R’←parseReviews ( R )</definiens>
			</definition>
			<definition id="1">
				<sentence>Goal Given product class C with instances I and reviews R , OPINE’s goal is to find a set of ( feature , opinions ) tuples { ( f , oi , ... oj ) } s.t. f ∈F and oi , ... oj ∈O , where : a ) F is the set of product class features in R. b ) O is the set of opinion phrases in R. c ) f is a feature of a particular product instance .</sentence>
				<definiendum id="0">O</definiendum>
				<definiendum id="1">f</definiendum>
				<definiens id="0">the set of product class features in R. b )</definiens>
				<definiens id="1">the set of opinion phrases in R. c )</definiens>
			</definition>
			<definition id="2">
				<sentence>OPINE then uses the data to find explicit product features ( E ) .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">uses the data to find explicit product features ( E )</definiens>
			</definition>
			<definition id="3">
				<sentence>Given a set of relations of interest , KnowItAll instantiates relation-specific generic extraction patterns into extraction rules which find candidate facts .</sentence>
				<definiendum id="0">KnowItAll</definiendum>
			</definition>
			<definition id="4">
				<sentence>The Assessor uses a form of Point-wise Mutual Information ( PMI ) between phrases that is estimated from Web search engine hit counts ( Turney , 2001 ) .</sentence>
				<definiendum id="0">Assessor</definiendum>
			</definition>
			<definition id="5">
				<sentence>OPINE’s Feature Assessor , which is an instantiation of KnowItAll’s Assessor , evaluates each noun phrase by computing the PMI scores between the phrase and meronymy discriminators associated with the product class ( e.g. , “of scanner” , “scanner has” , “scanner comes with” , etc. for the Scanner class ) .</sentence>
				<definiendum id="0">OPINE’s Feature Assessor</definiendum>
				<definiens id="0">an instantiation of KnowItAll’s Assessor , evaluates each noun phrase by computing the PMI scores between the phrase</definiens>
			</definition>
			<definition id="6">
				<sentence>OPINE’s recall is 3 % lower than the recall of Hu’s original system ( precision level = 0.8 ) .</sentence>
				<definiendum id="0">OPINE’s recall</definiendum>
			</definition>
			<definition id="7">
				<sentence>This subsection describes how OPINE extracts potential opinion phrases , distinguishes between opinions and nonopinions , and finds the polarity of each opinion in the context of its associated feature in a particular review sentence .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">extracts potential opinion phrases , distinguishes between opinions and nonopinions , and finds the polarity of each opinion in the context of its associated feature in a particular review sentence</definiens>
			</definition>
			<definition id="8">
				<sentence>This idea is similar to that of ( Kim and Hovy , 2004 ) and ( Hu and Liu , 2004 ) , but instead of using a window of size k or the output of a noun phrase chunker , OPINE takes advantage of the syntactic dependencies computed by the MINIPAR parser .</sentence>
				<definiendum id="0">OPINE</definiendum>
			</definition>
			<definition id="9">
				<sentence>In the following , we describe how OPINE finds the semantic orientation of words .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">finds the semantic orientation of words</definiens>
			</definition>
			<definition id="10">
				<sentence>OPINE finds the semantic orientation of a word w in the context of an associated feature f and sentence s. We restate this task as follows : Task Given a set of semantic orientation ( SO ) labels ( { positive , negative , neutral } ) , a set of reviews and a set of tuples ( w , f , s ) , where w is a potential opinion word associated with feature f in sentence s , assign a SO label to each tuple ( w , f , s ) .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiendum id="1">w</definiendum>
				<definiens id="0">finds the semantic orientation of a word w in the context of an associated feature f and sentence s. We restate this task as follows : Task Given a set of semantic orientation ( SO ) labels ( { positive , negative , neutral } ) , a set of reviews and a set of tuples</definiens>
			</definition>
			<definition id="11">
				<sentence>In each case , OPINE is given a set of objects ( words , pairs or tuples ) and a set of labels ( SO labels ) ; OPINE then searches for a global assignment of labels to objects .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">given a set of objects ( words , pairs or tuples ) and a set of labels</definiens>
			</definition>
			<definition id="12">
				<sentence>OPINE uses a well-known computer vision technique , relaxation labeling ( Hummel and Zucker , 1983 ) , in order to solve the three subtasks described above .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiendum id="1">relaxation labeling</definiendum>
				<definiens id="0">uses a well-known computer vision technique</definiens>
			</definition>
			<definition id="13">
				<sentence>Relaxation labeling is an unsupervised classification technique which takes as input : a ) a set of objects ( e.g. , words ) b ) a set of labels ( e.g. , SO labels ) c ) initial probabilities for each object’s possible labels d ) the definition of an object o’s neighborhood ( a set of other objects which influence the choice of o’s label ) e ) the definition of neighborhood features f ) the definition of a support function for an object label The influence of an object o’s neighborhood on its label L is quantified using the support function .</sentence>
				<definiendum id="0">Relaxation labeling</definiendum>
				<definiens id="0">an unsupervised classification technique which takes as input : a ) a set of objects ( e.g. , words ) b ) a set of labels</definiens>
			</definition>
			<definition id="14">
				<sentence>Relaxation labeling is an iterative procedure whose output is an assignment of labels to objects .</sentence>
				<definiendum id="0">Relaxation labeling</definiendum>
				<definiens id="0">an iterative procedure whose output is an assignment of labels to objects</definiens>
			</definition>
			<definition id="15">
				<sentence>RL uses an update equation to re-estimate the probability of a word label based on its previous probability estimate and the features of its neighborhood ( see Neighborhood Features ) .</sentence>
				<definiendum id="0">RL</definiendum>
			</definition>
			<definition id="16">
				<sentence>RL Initialization Step OPINE uses a version of Turney’s PMI-based approach ( Turney , 2003 ) in order to derive the initial probability estimates ( P ( l ( w ) = L ) ( 0 ) ) for a subset S of the words .</sentence>
				<definiendum id="0">RL Initialization Step OPINE</definiendum>
				<definiens id="0">uses a version of Turney’s PMI-based approach ( Turney , 2003 ) in order to derive the initial probability estimates</definiens>
			</definition>
			<definition id="17">
				<sentence>OPINE then uses the labeled S set in order to compute prior probabilities P ( l ( w ) = L ) , L ∈ { pos , neg , neutral } by computing the ratio between the number of words in S labeled L and |S| .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">uses the labeled S set in order to compute prior probabilities P ( l ( w ) = L ) , L ∈ { pos , neg , neutral } by computing the ratio between the number of words in S labeled L and |S|</definiens>
			</definition>
			<definition id="18">
				<sentence>Support Function The support function computes the probability of each label for word w based on the labels of objects in w’s neighborhood N. Let Ak = { ( wj , Lj ) |wj ∈ N } , 0 &lt; k ≤ 3|N| represent one of the potential assignments of labels to the words in N. Let P ( Ak ) ( m ) denote the probability of this particular assignment at iteration m. The support for label L of word w at iteration m is : q ( w , L ) ( m ) = 3|N|X k=1 P ( l ( w ) = L|Ak ) ( m ) ∗ P ( Ak ) ( m ) We assume that the labels of w’s neighbors are independent of each other and so the formula becomes : q ( w , L ) ( m ) = 3|N|X k=1 P ( l ( w ) = L|Ak ) ( m ) ∗ |N|Y j=1 P ( l ( wj ) = Lj ) ( m ) Every P ( l ( wj ) = Lj ) ( m ) term is the estimate for the probability that l ( wj ) = Lj ( which was computed at iteration m using the RL update equation ) .</sentence>
				<definiendum id="0">support function</definiendum>
				<definiens id="0">computes the probability of each label for word w based on the labels of objects in w’s neighborhood N. Let Ak = { ( wj , Lj ) |wj ∈ N } , 0 &lt; k ≤ 3|N| represent one of the potential assignments of labels to the words in N. Let P ( Ak ) ( m</definiens>
			</definition>
			<definition id="19">
				<sentence>Using Bayes’s Law and assuming that these labels are independent given l ( w ) , we have the following formula for fT at iteration m : fT ( w , L , Ak , T ) ( m ) = P ( l ( w ) = L ) ( m ) ∗ |NT|Y j=1 P ( Lj|l ( w ) = L ) P ( Lj|l ( w ) = L ) is the probability that word wj has label Lj if wj and w are linked by a relationship of type T and w has label L. We make the simplifying assumption that this probability is constant and depends only of T , L and Lprime , not of the particular words wj and w. For each tuple ( T , L , Lj ) , L , Lj ∈ { pos , neg , neutral } , OPINE builds a probability table using a small set of bootstrapped positive , negative and neutral words .</sentence>
				<definiendum id="0">Ak , T )</definiendum>
				<definiendum id="1">L , Lj ∈ { pos , neg</definiendum>
				<definiendum id="2">OPINE</definiendum>
			</definition>
			<definition id="20">
				<sentence>In order to correctly update SO labels in this last step , OPINE takes into consideration the presence of negation modifiers .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">takes into consideration the presence of negation modifiers</definiens>
			</definition>
			<definition id="21">
				<sentence>We compared OPINE against two baseline methods , PMI++ and Hu++ .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">against two baseline methods</definiens>
			</definition>
			<definition id="22">
				<sentence>Hu++ is a WordNet-based method for finding a word’s context-independent semantic orientation .</sentence>
				<definiendum id="0">Hu++</definiendum>
				<definiens id="0">a WordNet-based method for finding a word’s context-independent semantic orientation</definiens>
			</definition>
			<definition id="23">
				<sentence>OPINE is an unsupervised information extraction system which extracts fine-grained features , and associated opinions , from reviews .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">an unsupervised information extraction system which extracts fine-grained features , and associated opinions , from reviews</definiens>
			</definition>
			<definition id="24">
				<sentence>OPINE uses a novel relaxation-labeling technique to determine the semantic orientation of potential opinion words in the context of the extracted product features and specific review sentences ; this technique allows the system to identify customer opinions and their polarity with high precision and recall .</sentence>
				<definiendum id="0">OPINE</definiendum>
				<definiens id="0">uses a novel relaxation-labeling technique to determine the semantic orientation of potential opinion words in the context of the extracted product features and specific review sentences ; this technique allows the system to identify customer opinions and their polarity with high precision and recall</definiens>
			</definition>
</paper>

		<paper id="1125">
			<definition id="0">
				<sentence>The VJ approach has three main characteristics : 1 ) Continuous control parameters : Unlike standard speech recognition , the VJ engine exploits continuous vocal characteristics that go beyond simple sequences of discrete speech sounds ( such as syllables or words ) and include e.g. , pitch , vowel quality , and loudness , which are then mapped to continuous con995 trol parameters .</sentence>
				<definiendum id="0">VJ engine</definiendum>
				<definiens id="0">exploits continuous vocal characteristics that go beyond simple sequences of discrete speech sounds</definiens>
			</definition>
			<definition id="1">
				<sentence>The features we use are energy , normalized cross-correlation coefficients ( NCCC ) , formant estimates , Mel-frequency cepstral coefficients ( MFCCs ) , and formant estimates .</sentence>
				<definiendum id="0">NCCC</definiendum>
				<definiens id="0">formant estimates , Mel-frequency cepstral coefficients ( MFCCs ) , and formant estimates</definiens>
			</definition>
			<definition id="2">
				<sentence>Specifically , we obtain probability estimates of the form p ( Vt|Xt−τ , ... , Xt+τ ) , where V is a vowel class , and Xt−τ , ... , Xt+τ are feature frames within a length 2τ + 1 window of features centered at time t. After several empirical trials , we decided on neural networks for vowel classification because of the availability of efficient discriminative training algorithms and their computational simplicity .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">a vowel class , and Xt−τ , ... , Xt+τ are feature frames within a length 2τ + 1 window of features centered at time t. After several empirical trials</definiens>
			</definition>
			<definition id="3">
				<sentence>First , a raw direction value is calculated for each axis j ∈ { x , y } as dj = summationdisplay i pi · 〈vi , ej〉 ( 1 ) in which pi = p ( Vt = i|Xt−τ , ... , t+τ ) is the probability for vowel i at time t , vi is a unit vector in the direction of vowel i , ej is the unit-length positive directional basis vector along the j axis , and 〈v , e〉 is the projection of vector v onto unit vector e. To determine movement speed , we first calculate a scalar for each axis j as sj = summationdisplay i max bracketleftbigg 0 , gi parenleftbigg pi · f ( Eµ i ) parenrightbiggbracketrightbigg · |〈vi , ej〉| where E is the energy in the current frame , µi is the average energy for vowel i , and f ( · ) and gi ( · ) are functions used for energy normalization and perceptual scaling ( such as logs and/or cube-roots ) .</sentence>
				<definiendum id="0">vi</definiendum>
				<definiendum id="1">e〉</definiendum>
				<definiendum id="2">µi</definiendum>
				<definiens id="0">the unit-length positive directional basis vector along the j axis</definiens>
				<definiens id="1">the projection of vector v onto unit vector e. To determine movement speed</definiens>
				<definiens id="2">sj = summationdisplay i max bracketleftbigg 0 , gi parenleftbigg pi · f ( Eµ i ) parenrightbiggbracketrightbigg · |〈vi , ej〉| where E is the energy in the current frame ,</definiens>
				<definiens id="3">the average energy for vowel i , and f ( · ) and gi ( · ) are functions used for energy normalization and perceptual scaling ( such as logs and/or cube-roots )</definiens>
			</definition>
			<definition id="4">
				<sentence>The first task was a link navigation task ( Link ) , in which the participants were asked to start from a specific web page and follow a particular set of links to reach a destination .</sentence>
				<definiendum id="0">Link</definiendum>
				<definiens id="0">in which the participants were asked to start from a specific web page</definiens>
			</definition>
			<definition id="5">
				<sentence>We recorded the completion time for each trial , as well as the number of false positives ( system interprets a click when the user did not make a click sound ) , missed recognitions ( the user makes a click sound but the system fails to recognize it as a click ) , and user errors ( whenever the user clicks on an incorrect link ) .</sentence>
				<definiendum id="0">system</definiendum>
				<definiens id="0">interprets a click when the user did not make a click sound ) , missed recognitions ( the user makes a click sound but the system fails to recognize it as a click ) , and user errors ( whenever the user clicks on an incorrect link )</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>For a given model m , BIC seeks an analytical approximation for equation 4 , which looks like the following : lnP ( yi|m ) = lnφ ( yi|ˆθ , m ) − k2 lnN , ( 9 ) where k denotes the number of free parameters in m , and N that of observations .</sentence>
				<definiendum id="0">BIC</definiendum>
				<definiendum id="1">k</definiendum>
				<definiens id="0">the number of free parameters in m</definiens>
			</definition>
			<definition id="1">
				<sentence>Alternatively , one might take a more straightforward ( and fully Bayesian ) approach known as the Monte Carlo integration method ( MacKay , 1998 ) ( MC , hereafter ) where the integral is approximated by : P ( yi|v ) ≈ 1n nsummationdisplay j=1 φ ( yi|x ( j ) ) , ( 10 ) where we draw each sample x ( j ) randomly from the distribution P ( θ|v ) , and n is the number of x ( i ) ’s so collected .</sentence>
				<definiendum id="0">MC</definiendum>
				<definiendum id="1">n</definiendum>
			</definition>
			<definition id="2">
				<sentence>‘LocSen’ gives a normalized location of ψ in the text , i.e. , a normalized distance from the top of the text ; likewise , ‘LocPar’ gives a normalized location of the paragraph in which ψ occurs , and ‘LocWithinPar’ records its normalized location within a paragraph .</sentence>
				<definiendum id="0">‘LocSen’</definiendum>
				<definiendum id="1">‘LocPar’</definiendum>
				<definiens id="0">gives a normalized location of the paragraph in which ψ occurs , and ‘LocWithinPar’ records its normalized location within a paragraph</definiens>
			</definition>
			<definition id="3">
				<sentence>Here ‘tf ( w ) ’ denotes the frequency of word w in a given document , ‘df ( w ) ’ denotes the ’document frequency’ of w , or the number of documents which contain an occurrence of w. N represents the total number of documents.5 Also among the features used here is ‘Pos , ’ a feature intended to record the position or textual order of ψ , given by how many sentences away it occurs from the top of text , starting with 0 .</sentence>
				<definiendum id="0">‘tf ( w ) ’</definiendum>
				<definiens id="0">the frequency of word w in a given document , ‘df ( w ) ’ denotes the ’document frequency’ of w , or the number of documents which contain an occurrence of w. N represents the total number of documents.5 Also among the features used here is ‘Pos , ’ a feature intended to record the position or textual order of ψ , given by how many sentences away it occurs from the top of text , starting with 0</definiens>
			</definition>
			<definition id="4">
				<sentence>Precision is defined by the ratio of hits ( positive sentences ) to the number of sentences retrieved , i.e. , r-percent of sentences in the text.6 In each table , figures to the left of the vertical line indicate performance of summarizers with BIC/MC and those to the right that of summarizers without them .</sentence>
				<definiendum id="0">Precision</definiendum>
			</definition>
			<definition id="5">
				<sentence>253 Table 1 : N represents the number of sentences in G1K3 to G3K3 .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of sentences in G1K3 to G3K3</definiens>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>We study semantic role labeling ( SRL ) , defined as follows : for each verb in a sentence , the goal is to identify all constituents that fill a semantic role , and to determine their roles ( such as Agent , Patient or Instrument ) and their adjuncts ( such as Locative , Temporal or Manner ) .</sentence>
				<definiendum id="0">goal</definiendum>
				<definiens id="0">to identify all constituents that fill a semantic role , and to determine their roles ( such as Agent , Patient or Instrument ) and their adjuncts ( such as Locative , Temporal or Manner )</definiens>
			</definition>
			<definition id="1">
				<sentence>The PropBank project ( Kingsbury and Palmer , 2002 ) , which provides a large humanannotated corpus of semantic verb-argument relations , has opened doors for researchers to apply machine learning techniques to this task .</sentence>
				<definiendum id="0">PropBank project</definiendum>
				<definiens id="0">provides a large humanannotated corpus of semantic verb-argument relations</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>where Z is the normalizing constant .</sentence>
				<definiendum id="0">Z</definiendum>
				<definiens id="0">the normalizing constant</definiens>
			</definition>
			<definition id="1">
				<sentence>f = ψ ( li ) h = bracketleftbigti−11 , sK1 bracketrightbig p ( fjh ) = 1Z ( h ) exp summationdisplay i λiφi ( h , f ) , where Z ( h ) is the normalizing constant , Z ( h ) = summationdisplay f exp summationdisplay i λiφi ( h , f ) .</sentence>
				<definiendum id="0">Z ( h )</definiendum>
				<definiens id="0">the normalizing constant</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>A content selection component determines which information should be conveyed in the output of a natural language generation system .</sentence>
				<definiendum id="0">content selection component</definiendum>
				<definiens id="0">determines which information should be conveyed in the output of a natural language generation system</definiens>
			</definition>
			<definition id="1">
				<sentence>We generate a range of candidate link types using the following template : For every pair of entity types Ei and E j , and for every attribute k that is associated with both of them , create a link of type Li ; j ; k. A pair of entities ha , bi is linked by Li ; j ; k , if a is of type Ei , b is of type E j and they have the same value for the attribute k. For example , a link that associates statistics on Passing and Rushing performed by the same player is an instantiation of the above with Ei = Rushing , E j = Passing , and k = Player .</sentence>
				<definiendum id="0">k</definiendum>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>Recognition errors hinder the proliferation of speech recognition ( SR ) systems .</sentence>
				<definiendum id="0">Recognition errors</definiendum>
				<definiens id="0">hinder the proliferation of speech recognition ( SR ) systems</definiens>
			</definition>
			<definition id="1">
				<sentence>Transformation-based learning ( TBL ) is a rulebased learning method .</sentence>
				<definiendum id="0">Transformation-based learning ( TBL</definiendum>
				<definiens id="0">a rulebased learning method</definiens>
			</definition>
			<definition id="2">
				<sentence>A link requirement is a set of disjuncts , each of which represents a possible usage of the word .</sentence>
				<definiendum id="0">link requirement</definiendum>
				<definiens id="0">represents a possible usage of the word</definiens>
			</definition>
			<definition id="3">
				<sentence>Each condition C is the conjunction of sub-conditions in form of f op v , where f represents a feature , v is a possible categorical value of f , and op is the possible operations such as &lt; , &gt; and = .</sentence>
				<definiendum id="0">f</definiendum>
				<definiendum id="1">v</definiendum>
				<definiendum id="2">op</definiendum>
				<definiens id="0">a possible categorical value of f</definiens>
			</definition>
			<definition id="4">
				<sentence>lrHaslink represents whether the preceding word and the following word have links , 44 Category Group Example Word CS cs ( wi ) &lt; ci Alone L position ( wi ) = ti &amp; syllables ( wi ) = si LCS cs ( wi ) &lt; ci &amp; pos ( wi ) = pi Local Local position ( wi ) = ti &amp; label ( wi−1 ) = li−1 &amp; word ( wi ) = di Context CSLocal cs ( wi ) &lt; ci &amp; position ( wi ) = ti &amp; label ( wi−1 ) = li−1 &amp; label ( wi+1 ) = li+1 Sentence Long position ( wi ) = ti &amp; lrHaslink ( wi ) = hi &amp; haslink ( wi ) = hli Context CSLong cs ( wi ) &lt; ci &amp; position ( wi ) = ti &amp; llinkLabel ( wi ) = lli &amp; pos ( wi ) = pi Table 1 : Condition Categories and Examples and llinkLabel represents the label of the word to which w has the shortest left link .</sentence>
				<definiendum id="0">lrHaslink</definiendum>
				<definiens id="0">cs ( wi ) &lt; ci Alone L position ( wi ) = ti &amp; syllables ( wi ) = si LCS cs ( wi ) &lt; ci &amp; pos ( wi ) = pi Local Local position</definiens>
				<definiens id="1">Long position ( wi ) = ti &amp; lrHaslink ( wi ) = hi &amp; haslink ( wi ) = hli Context CSLong cs ( wi ) &lt; ci &amp; position ( wi ) = ti &amp; llinkLabel ( wi ) = lli &amp; pos ( wi ) = pi Table 1 : Condition Categories and Examples and llinkLabel represents the label of the word to which w has the shortest left link</definiens>
			</definition>
			<definition id="5">
				<sentence>For each transformation , its positive effect ( PE ) is the number of words whose labels are correctly updated by applying it , and its negative effect ( NE ) is the number of words wrongly updated .</sentence>
				<definiendum id="0">PE )</definiendum>
				<definiens id="0">the number of words whose labels</definiens>
				<definiens id="1">the number of words wrongly updated</definiens>
			</definition>
			<definition id="6">
				<sentence>The data corpus consists of the recognition output of their dictations excluding corrections .</sentence>
				<definiendum id="0">data corpus</definiendum>
				<definiens id="0">consists of the recognition output of their dictations excluding corrections</definiens>
			</definition>
			<definition id="7">
				<sentence>CER is the percentage of words that are wrongly classified .</sentence>
				<definiendum id="0">CER</definiendum>
			</definition>
			<definition id="8">
				<sentence>PRE is the percentage of words classified as errors that are in fact recognition errors .</sentence>
				<definiendum id="0">PRE</definiendum>
				<definiens id="0">the percentage of words classified as errors that are in fact recognition errors</definiens>
			</definition>
			<definition id="9">
				<sentence>REC denotes the proportion of actual recognition errors that are categorized as errors by the classifier .</sentence>
				<definiendum id="0">REC</definiendum>
				<definiens id="0">the proportion of actual recognition errors that are categorized as errors by the classifier</definiens>
			</definition>
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>The PageRank score associated with the vertex Va is then defined using a recursive function that integrates the scores of its predecessors : P ( Va ) = ( 1 − d ) + d ∗ summationdisplay Vb∈In ( Va ) P ( Vb ) |Out ( Vb ) | ( 1 ) where d is a parameter that is set between 0 and 11 .</sentence>
				<definiendum id="0">PageRank score</definiendum>
				<definiendum id="1">recursive function</definiendum>
				<definiendum id="2">d</definiendum>
				<definiens id="0">integrates the scores of its predecessors : P ( Va ) = ( 1 − d ) + d ∗ summationdisplay Vb∈In</definiens>
				<definiens id="1">a parameter that is set between 0 and 11</definiens>
			</definition>
			<definition id="1">
				<sentence>In matrix notation , the PageRank vector of stationary probabilities is the principal eigenvector for the matrix Arow , which is obtained from the adjacency matrix A representing the graph , with all rows normalized to sum to 1 : ( P = ATrowP ) .</sentence>
				<definiendum id="0">Arow</definiendum>
				<definiens id="0">the principal eigenvector for the matrix</definiens>
			</definition>
			<definition id="2">
				<sentence>The algorithm consists of three main steps : ( 1 ) construction of label dependencies graph ; ( 2 ) label scoring using graph-based ranking algorithms ; ( 3 ) label assignment .</sentence>
				<definiendum id="0">algorithm</definiendum>
				<definiens id="0">consists of three main steps : ( 1 ) construction of label dependencies graph ; ( 2 ) label scoring using graph-based ranking algorithms ; ( 3 ) label assignment</definiens>
			</definition>
			<definition id="3">
				<sentence>Label dependencies can be defined in various ways , depending on the application at hand and on the knowledge sources that are available .</sentence>
				<definiendum id="0">Label dependencies</definiendum>
				<definiens id="0">depending on the application at hand and on the knowledge sources that are available</definiens>
			</definition>
			<definition id="4">
				<sentence>Word sense disambiguation is a labeling task consisting of assigning the correct meaning to each open-class word in a sequence ( usually a sentence ) .</sentence>
				<definiendum id="0">Word sense disambiguation</definiendum>
				<definiens id="0">a labeling task consisting of assigning the correct meaning to each open-class word in a sequence ( usually a sentence )</definiens>
			</definition>
			<definition id="5">
				<sentence>Relying exclusively on a machine readable dictionary , a sense dependency can be defined as a measure of similarity between word senses .</sentence>
				<definiendum id="0">sense dependency</definiendum>
				<definiens id="0">a measure of similarity between word senses</definiens>
			</definition>
			<definition id="6">
				<sentence>context size ( MaxDist ) equal to the length of each sentence , using : ( a ) sequence data labeling with iterative graph-based algorithms ; ( b ) individual data labeling with a version of the Lesk algorithm ; ( c ) random baseline .</sentence>
				<definiendum id="0">MaxDist</definiendum>
				<definiens id="0">a ) sequence data labeling with iterative graph-based algorithms ; ( b ) individual data labeling with a version of the Lesk algorithm ; ( c ) random baseline</definiens>
			</definition>
			<definition id="7">
				<sentence>The window size parameter limits the number of surrounding words considered when seeking label dependencies ( sequence data labeling ) , or the words counted in the measure of definition–context overlap ( individual data labeling ) .</sentence>
				<definiendum id="0">window size parameter</definiendum>
				<definiens id="0">limits the number of surrounding words considered when seeking label dependencies ( sequence data labeling ) , or the words counted in the measure of definition–context overlap ( individual data labeling )</definiens>
			</definition>
</paper>

		<paper id="1099">
			<definition id="0">
				<sentence>Two commonly reported shallow parsing tasks are Noun-Phrase ( NP ) Chunking ( Ramshaw and Marcus , 1995 ) and the CoNLL-2000 Chunking task ( Sang and Buchholz , 2000 ) , which extends the NPChunking task to recognition of 11 phrase types1 To determine the effects of the conversion routine and different evaluation conventions , we compare the performance of several different models on one of the tasks presented in Li and Roth ( 2001 ) .</sentence>
				<definiendum id="0">Noun-Phrase</definiendum>
				<definiens id="0">extends the NPChunking task to recognition of 11 phrase types1 To determine the effects of the conversion routine and different evaluation conventions</definiens>
			</definition>
			<definition id="1">
				<sentence>Both the Bikel ( 2004 ) imple789 System NP-Chunking CoNLL-2000 Li &amp; Roth task SPRep averaged perceptron 94.21 93.54 95.12 Kudo and Matsumoto ( 2001 ) 94.22 93.91 Sha and Pereira ( 2003 ) CRF 94.38 Voted perceptron 94.09 Zhang et al. ( 2002 ) 94.17 Li and Roth ( 2001 ) 93.02 94.64 Table 2 : Baseline results on three shallow parsing tasks : the NP-Chunking task ( Ramshaw and Marcus , 1995 ) ; the CoNLL-2000 Chunking task ( Sang and Buchholz , 2000 ) ; and the Li &amp; Roth task ( Li and Roth , 2001 ) , which is the same as CoNLL-2000 but with more training data and a different test section .</sentence>
				<definiendum id="0">Bikel</definiendum>
				<definiens id="0">the same as CoNLL-2000 but with more training data and a different test section</definiens>
			</definition>
			<definition id="2">
				<sentence>The score for a shallow parse output by the Charniak parser is the log of the sum of the probabilities of all context-free parses mapping to that shallow parse .</sentence>
				<definiendum id="0">Charniak parser</definiendum>
			</definition>
			<definition id="3">
				<sentence>Again , the best-scoring candidate using this composite score is selected from among the shallow 791 NP-Chunking CoNLL-2000 Li &amp; Roth task Punctuation Punctuation Punctuation System Leave Ignore Leave Ignore Leave Ignore SPRep averaged perceptron 94.21 94.25 93.54 93.70 95.12 95.27 Charniak ( 2000 ) 94.17 94.20 93.77 93.92 95.15 95.32 Unweighted intersection 95.13 95.16 94.52 94.64 95.77 95.92 Weighted intersection 95.57 95.58 95.03 95.16 96.20 96.33 Table 3 : F-measure shallow bracketing accuracy on three shallow parsing tasks , for the SPRep perceptron shallow parser , the Charniak ( 2000 ) context-free parser , and for systems combining the SPRep and Charniak system outputs .</sentence>
				<definiendum id="0">Charniak</definiendum>
				<definiens id="0">context-free parser , and for systems combining the SPRep and Charniak system outputs</definiens>
			</definition>
			<definition id="4">
				<sentence>Weighted intersection between the reranked list and the shallow parser as described earlier , with a newly estimated scaling factor ( α=30 ) , provides a roughly tained by the reranker .</sentence>
				<definiendum id="0">Weighted intersection</definiendum>
				<definiens id="0">provides a roughly tained by the reranker</definiens>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>Using translations as queries , the sense examples were automatically extracted from the Chinese Gigaword Corpus ( CGC ) , distributed by the LDC7 , which contains 2.7GB newswire text , of which 900MB are sourced from Xinhua News Agency of Beijing , and 1.8GB are drawn from Central News from Taiwan .</sentence>
				<definiendum id="0">Gigaword Corpus</definiendum>
				<definiens id="0">contains 2.7GB newswire text , of which 900MB are sourced from Xinhua News Agency of Beijing</definiens>
			</definition>
			<definition id="1">
				<sentence>“UNED” is one of the best unsupervised participants in the Senseval-2 competition and “Lesk ( U ) ” is the highest unsupervised-baseline set in the workshop .</sentence>
				<definiendum id="0">“UNED”</definiendum>
				<definiens id="0">one of the best unsupervised participants in the Senseval-2 competition and “Lesk ( U ) ” is the highest unsupervised-baseline set in the workshop</definiens>
			</definition>
			<definition id="2">
				<sentence>This is a manually sense-tagged corpus ( Mihalcea , 2003 ) , which contains 2-way sense-tagged text instances , drawn from the British National Corpus , for 6 nouns .</sentence>
				<definiendum id="0">sense-tagged corpus</definiendum>
			</definition>
			<definition id="3">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>Prepositional Phrase-attachment is a common source of ambiguity in natural language .</sentence>
				<definiendum id="0">Prepositional Phrase-attachment</definiendum>
				<definiens id="0">a common source of ambiguity in natural language</definiens>
			</definition>
			<definition id="1">
				<sentence>We propose to solve the PP-attachment ambiguity with a Support Vector Machines learning model that uses complex syntactic and semantic features as well as unsupervised information obtained from the World Wide Web .</sentence>
				<definiendum id="0">Support Vector Machines learning model</definiendum>
			</definition>
			<definition id="2">
				<sentence>Prepositional Phrase-attachment is a source of ambiguity in natural language that generates a significant number of errors in syntactic parsing .</sentence>
				<definiendum id="0">Prepositional Phrase-attachment</definiendum>
				<definiens id="0">a source of ambiguity in natural language that generates a significant number of errors in syntactic parsing</definiens>
			</definition>
			<definition id="3">
				<sentence>To be able to extract the required features from a dataset instance , one must identify the verb , the phrase identifying the object of the verb that precedes the prepositional phrase in question ( np1 ) which usually is part of the predicate-argument structure of the verb , its head noun , the prepositional phrase ( np2 ) , its preposition and its head noun ( the second most important word in the PP ) .</sentence>
				<definiendum id="0">prepositional phrase</definiendum>
				<definiendum id="1">np2</definiendum>
				<definiens id="0">phrase identifying the object of the verb that precedes the prepositional phrase in question ( np1 ) which usually is part of the predicate-argument structure of the verb</definiens>
			</definition>
			<definition id="4">
				<sentence>We have adopted the notation from ( Collins and Brooks , 1995 ) , where v is the verb , n1 is the head noun of object phrase , p is the preposition and n2 is the head noun of the prepositional phrase .</sentence>
				<definiendum id="0">v</definiendum>
				<definiendum id="1">n1</definiendum>
				<definiendum id="2">p</definiendum>
				<definiendum id="3">n2</definiendum>
				<definiens id="0">the verb</definiens>
				<definiens id="1">the head noun of object phrase</definiens>
			</definition>
			<definition id="5">
				<sentence>Compared to our datasets , Ratnaparkhi’s dataset ( Ratnaparkhi et al. , 1994 ) contains only the lexical heads v , n1 , p and n2 .</sentence>
				<definiendum id="0">Ratnaparkhi’s dataset</definiendum>
				<definiens id="0">contains only the lexical heads v , n1 , p and n2</definiens>
			</definition>
			<definition id="6">
				<sentence>FN FN TB2 TB2 of 13.47 % 6.17 % 30.14 % 2.74 % to 13.27 % 80.14 % 9.55 % 60.49 % in 12.42 % 73.64 % 16.94 % 42.58 % for 6.87 % 82.44 % 8.95 % 39.72 % on 6.21 % 75.51 % 5.16 % 47.73 % with 6.17 % 86.30 % 3.79 % 46.92 % from 5.37 % 75.90 % 5.76 % 52.76 % at 4.09 % 76.63 % 3.21 % 66.02 % as 3.95 % 86.51 % 2.49 % 51.69 % by 3.53 % 88.02 % 3.27 % 68.11 % Table 3 : Distribution of the first 10 most-frequent prepositions in the FN and TB2 datasets n1-sr represents the semantic role of the object phrase np1 – the label attached to the Frame Element ( manual semantic annotation that can be found in FrameNet ) .</sentence>
				<definiendum id="0">FN FN TB2 TB2</definiendum>
				<definiendum id="1">TB2 datasets</definiendum>
			</definition>
			<definition id="7">
				<sentence>subcategorization contains a semi-lexicalized description of the structure of the verb phrase .</sentence>
				<definiendum id="0">subcategorization</definiendum>
				<definiens id="0">contains a semi-lexicalized description of the structure of the verb phrase</definiens>
			</definition>
			<definition id="8">
				<sentence>n1-parent represents the phrase label of the parent of np1 and it can not be used on gold parse trees ( TB2 dataset ) because it will provide a clue about the correct attachment type .</sentence>
				<definiendum id="0">n1-parent</definiendum>
				<definiens id="0">represents the phrase label of the parent of np1 and it can not be used on gold parse trees ( TB2 dataset ) because it will provide a clue about the correct attachment type</definiens>
			</definition>
			<definition id="9">
				<sentence>parser-vote feature represents the choice of the parser ( Charniak’s parser ) in the PP-attachment resolution .</sentence>
				<definiendum id="0">parser-vote feature</definiendum>
				<definiens id="0">represents the choice of the parser ( Charniak’s parser ) in the PP-attachment resolution</definiens>
			</definition>
			<definition id="10">
				<sentence>count-ratio represents the estimated ratio between the frequency of an unambiguous verb attachment construction based on v , p and n2 and the frequency of a probably unambiguous noun attachment construction based on n1 , p and n2 in a very large corpus .</sentence>
				<definiendum id="0">count-ratio</definiendum>
				<definiens id="0">represents the estimated ratio between the frequency of an unambiguous verb attachment construction based on v , p</definiens>
			</definition>
			<definition id="11">
				<sentence>Let’s consider the estimated frequency of unambiguous verb-attachments and respectively nounattachments defined as : fv = cv−p−n2c v ·cp−n2 fn = cn1−p−n2c n1 ·cp−n2 where : • cv−p−n2 is the number of occurrences of the phrase “v pn2” , “v p∗n2” ( where * symbolizes any word ) , “v-lemma p n2” or “v-lemma p * n2” in World Wide Web , as reported by Google • cv is the number of occurrences of the word “v” or “v-lemma” in WWW • cp−n2 is the number of occurrences of the phrase “p n2” or “p∗n2” in WWW • cn1−p−n2 is the number of occurrences of the phrase “n1 p n2” or “v p∗n2” in WWW • cn1 is the number of occurrences of the word “n1” in WWW The value for this feature is : count−ratio = log10 fvf n = log10 cv−p−n2 ·cn1c n1−p−n2 ·cv We chose logarithmic values for this feature because experiments showed that logarithmic values provide a higher accuracy than linear values .</sentence>
				<definiendum id="0">Let’s</definiendum>
				<definiendum id="1">cv</definiendum>
				<definiens id="0">consider the estimated frequency of unambiguous verb-attachments and respectively nounattachments defined as : fv = cv−p−n2c v ·cp−n2 fn = cn1−p−n2c n1 ·cp−n2 where : • cv−p−n2 is the number of occurrences of the phrase “v pn2”</definiens>
				<definiens id="1">where * symbolizes any word ) , “v-lemma p n2” or “v-lemma p * n2” in World Wide Web</definiens>
				<definiens id="2">the number of occurrences of the word</definiens>
				<definiens id="3">the number of occurrences of the phrase “p n2” or “p∗n2” in WWW • cn1−p−n2 is the number of occurrences of the phrase “n1 p n2” or “v p∗n2” in WWW • cn1 is the number of occurrences of the word “n1” in WWW The value for this feature is : count−ratio = log10 fvf n = log10 cv−p−n2 ·cn1c n1−p−n2 ·cv We chose logarithmic values for this feature because experiments showed that logarithmic values provide a higher accuracy than linear values</definiens>
			</definition>
			<definition id="12">
				<sentence>pp-count depicts the estimated count of occurrences in World Wide Web of the prepositional phrases based on p and n2 .</sentence>
				<definiendum id="0">pp-count</definiendum>
				<definiens id="0">depicts the estimated count of occurrences in World Wide Web of the prepositional phrases based on p and n2</definiens>
			</definition>
			<definition id="13">
				<sentence>• Continuous features : assign a dimension and put the scaled value in the multi-dimensional vector ( all examples in training data will span between 0 and 1 for that particular dimension ) .</sentence>
				<definiendum id="0">multi-dimensional vector</definiendum>
				<definiens id="0">assign a dimension and put the scaled value in the</definiens>
			</definition>
			<definition id="14">
				<sentence>FN-best-no-sem uses v-surface , v-pos , vlemma , subcategorization , path ( reduced POS ) , position , n1-preposition , n1-surface , n1-pos , n1lemma-mpf , n1-parent , p , n2-surface , n2-pos , n2-lemma-mpf , n2-det , parser-vote , count-ratio and pp-count on all examples .</sentence>
				<definiendum id="0">FN-best-no-sem</definiendum>
				<definiens id="0">uses v-surface , v-pos , vlemma , subcategorization , path ( reduced POS ) , position , n1-preposition , n1-surface , n1-pos , n1lemma-mpf , n1-parent , p , n2-surface</definiens>
			</definition>
			<definition id="15">
				<sentence>TB2-best-no-www uses vsurface , v-pos , v-lemma , subcategorization , path ( reduced POS ) , n1-preposition , n1-surface , n1mpf , n1-pos , n1-lemma , n1-np-label , p , n2surface , n2-mpf and n1-p-distance .</sentence>
				<definiendum id="0">TB2-best-no-www</definiendum>
				<definiens id="0">uses vsurface , v-pos , v-lemma , subcategorization , path ( reduced POS ) , n1-preposition , n1-surface , n1mpf , n1-pos , n1-lemma , n1-np-label , p , n2surface , n2-mpf and n1-p-distance</definiens>
			</definition>
			<definition id="16">
				<sentence>The difference in accuracy between a SVM model applied to RRR dataset ( RRR-basic experiment ) and the same experiment applied to TB2 dataset ( TB2278 Description Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human , headwords ( Ratnaparkhi et al. , 1994 ) 88.2 RRR Average human , whole sentence ( Ratnaparkhi et al. , 1994 ) 93.2 RRR Maximum Likelihood-based ( Hindle and Rooth , 1993 ) 79.7 AP Maximum entropy , words ( Ratnaparkhi et al. , 1994 ) 77.7 RRR Maximum entropy , words &amp; classes ( Ratnaparkhi et al. , 1994 ) 81.6 RRR Decision trees ( Ratnaparkhi et al. , 1994 ) 77.7 RRR Transformation-Based Learning ( Brill and Resnik , 1994 ) 81.8 WordNet Maximum-Likelihood based ( Collins and Brooks , 1995 ) 84.5 RRR Maximum-Likelihood based ( Collins and Brooks , 1995 ) 86.1 TB2 Decision trees &amp; WSD ( Stetina and Nagao , 1997 ) 88.1 RRR WordNet Memory-based Learning ( Zavrel et al. , 1997 ) 84.4 RRR LexSpace Maximum entropy , unsupervised ( Ratnaparkhi , 1998 ) 81.9 Maximum entropy , supervised ( Ratnaparkhi , 1998 ) 83.7 RRR Neural Nets ( Alegre et al. , 1999 ) 86.0 RRR WordNet Boosting ( Abney et al. , 1999 ) 84.4 RRR Semi-probabilistic ( Pantel and Lin , 2000 ) 84.31 RRR Maximum entropy , ensemble ( McLauchlan , 2001 ) 85.5 RRR LSA SVM ( Vanschoenwinkel and Manderick , 2003 ) 84.8 RRR Nearest-neighbor ( Zhao and Lin , 2004 ) 86.5 RRR DWS FN dataset , w/o semantic features ( FN-best-no-sem ) 91.79 FN PR-WWW FN dataset , w/ semantic features ( FN-best-sem ) 92.85 FN PR-WWW TB2 dataset , best feature set ( TB2-best ) 93.62 TB2 PR-WWW Table 5 : Accuracy of PP-attachment ambiguity resolution ( our results in bold ) basic experiment ) is 2.9 % .</sentence>
				<definiendum id="0">RRR Transformation-Based Learning</definiendum>
				<definiendum id="1">RRR Maximum-Likelihood based ( Collins</definiendum>
				<definiens id="0">Maximum entropy , words</definiens>
				<definiens id="1">Memory-based Learning ( Zavrel et al. , 1997 ) 84.4 RRR LexSpace Maximum entropy</definiens>
			</definition>
</paper>

		<paper id="2007">
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>Names , especially rare and foreign ones are a problem for automatic speech recognition ( ASR ) systems as they are often out of vocabulary ( OOV ) i.e. , they do not exist in the lexicon of the ASR system .</sentence>
				<definiendum id="0">Names</definiendum>
			</definition>
			<definition id="1">
				<sentence>In particular , we use the Levenshtein distance ( Levenshtein , 1966 ) , that is the number of insertions , deletions and substitutions needed to convert one string to the other .</sentence>
				<definiendum id="0">Levenshtein distance</definiendum>
			</definition>
			<definition id="2">
				<sentence>Given a pair of sentences ( c , a ) , an alignment A ( c , a ) is defined as the mapping from the words in c to the words in a. If there are l closed caption words and m ASR words , there are 2lm alignments in A ( c , a ) .</sentence>
				<definiendum id="0">alignment A ( c</definiendum>
				<definiens id="0">the mapping from the words in c to the words in a. If there are l closed caption words and m ASR words</definiens>
			</definition>
			<definition id="3">
				<sentence>Then P ( ajc ) is computed as follows : P ( ajc ) = summationdisplay l P ( a , ljc ) P ( a , ljc ) = P ( mjc ) mproductdisplay j P ( ljjlj−11 , aj−11 , m , e ) P ( ajjlj1 , aj−11 , m , c ) ( 2 ) where aj is a word in position j of the string a , and aj1 is the series a1 ... aj .</sentence>
				<definiendum id="0">aj</definiendum>
				<definiendum id="1">aj1</definiendum>
				<definiens id="0">a word in position j of the string a , and</definiens>
			</definition>
			<definition id="4">
				<sentence>For the Generative Unsupervised and Generative Supervised methods , we use the same models , but in this case the training set consists of pairs of words obtained from the ASR and closed caption text as opposed to sentences .</sentence>
				<definiendum id="0">Generative Unsupervised</definiendum>
				<definiens id="0">pairs of words obtained from the ASR and closed caption text as opposed to sentences</definiens>
			</definition>
			<definition id="5">
				<sentence>The TREC-7 corpus consists of the output of the Dragon systems speech recognizer ( WER 29.5 % ) .</sentence>
				<definiendum id="0">TREC-7 corpus</definiendum>
			</definition>
			<definition id="6">
				<sentence>UI measures the total number of missed links between words and OI measures the total number of false alarm links .</sentence>
				<definiendum id="0">UI</definiendum>
				<definiens id="0">measures the total number of missed links between words</definiens>
				<definiens id="1">the total number of false alarm links</definiens>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>Given a source key phrase ( here a Chinese phrase ) , we first retrieve web page snippets containing this phrase using the Google search engine .</sentence>
				<definiendum id="0">source key phrase</definiendum>
				<definiens id="0">a Chinese phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>2 A snippet is a sentence or paragraph containing the key phrase , returned with the web page URLs .</sentence>
				<definiendum id="0">snippet</definiendum>
				<definiens id="0">a sentence or paragraph containing the key phrase , returned with the web page URLs</definiens>
			</definition>
			<definition id="2">
				<sentence>The preprocessing steps are : 3 http : //www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ? catalogI d=LDC2002L27 4 http : //www.natcorp.ox.ac.uk/ “ &amp; lt” ) to corresponding ASCII code ( “ &gt; ” ) ; mum string matching algorithm , which is used to calculate the translation probability between a Chinese key phrase and an English translation candidate .</sentence>
				<definiendum id="0">string matching algorithm</definiendum>
				<definiens id="0">used to calculate the translation probability between a Chinese key phrase and an English translation candidate</definiens>
			</definition>
			<definition id="3">
				<sentence>The transliteration model captures the phonetic similarity between a Chinese phrase and an English translation candidate via string alignment .</sentence>
				<definiendum id="0">transliteration model</definiendum>
				<definiens id="0">captures the phonetic similarity between a Chinese phrase and an English translation candidate via string alignment</definiens>
			</definition>
			<definition id="4">
				<sentence>, ( efd ji f , Define the confidence measure for the transliteration model as : where e and e’ are English candidate phrases , and m is the weight of the distance model .</sentence>
				<definiendum id="0">efd ji f</definiendum>
				<definiens id="0">the confidence measure for the transliteration model as : where e and e’ are English candidate phrases</definiens>
				<definiens id="1">the weight of the distance model</definiens>
			</definition>
			<definition id="5">
				<sentence>Similarly the translation model confidence measure is defined as : The overall feature cost is the linear combination of transliteration cost and translation cost , which are weighted by their confidence scores respectively : C jij , ) ' ( ) ] , ' ( exp [ ) ( ) ] , ( exp [ ) | ( ' ∑ = e m trl m trl trl ewfeC ewfeC feφ . ) '</sentence>
				<definiendum id="0">translation model confidence measure</definiendum>
				<definiens id="0">the linear combination of transliteration cost and translation cost , which are weighted by their confidence scores respectively : C jij , ) '</definiens>
			</definition>
			<definition id="6">
				<sentence>Movie Title 阿甘正传 Forrest Gump Book Title 红楼梦 Dream of the Red Mansion 茶花女 La Dame aux camellias Organization Name 圣母大学 University of Notre Dame 大卫与露西派克德基金会 David and Lucile Packard Foundation Person Name 贝多芬 Ludwig Van Beethoven 奥黛丽赫本 Audrey Hepburn Location Name 勘察加半岛 Kamchatka 塔克拉玛干沙漠 Taklamakan desert Company / Brand 汉莎航空 Lufthansa German Airlines 雅诗兰黛 Estee Lauder Sci &amp; Tech Terms 遗传算法 genetic algorithm 语音识别 speech recognition Specie Term 秃鹫 Aegypius monachus 穿山甲 Manispentadactyla Military Term 宙斯盾 Aegis 费尔康 Phalcon Medical Term 非典型性肺炎 SARS 动脉硬化 Arteriosclerosis Music Term 空山鸟语 Bird-call in the Mountain 巴松管 Bassoon Sports Term 休斯敦火箭队 Houston Rockets 环法自行车赛 Tour de France ) ] ( ) | ( ) ( ffφλ exp [ 1 ) ] , ( exp [ ) | ( ) , ( eCe feCfefeC transtrans trltrl =λφ + , − 487 Table 2 .</sentence>
				<definiendum id="0">( )</definiendum>
				<definiens id="0">Movie Title 阿甘正传 Forrest Gump Book Title 红楼梦 Dream of the Red Mansion 茶花女 La Dame aux camellias Organization Name 圣母大学 University of Notre Dame 大卫与露西派克德基金会 David and Lucile Packard Foundation Person Name 贝多芬 Ludwig Van Beethoven 奥黛丽赫本 Audrey Hepburn Location Name 勘察加半岛 Kamchatka 塔克拉玛干沙漠 Taklamakan desert Company / Brand 汉莎航空 Lufthansa German Airlines 雅诗兰黛 Estee Lauder Sci &amp; Tech Terms 遗传算法 genetic algorithm 语音识别 speech recognition Specie Term 秃鹫 Aegypius monachus 穿山甲 Manispentadactyla Military Term 宙斯盾 Aegis 费尔康 Phalcon Medical Term 非典型性肺炎 SARS 动脉硬化 Arteriosclerosis Music Term 空山鸟语 Bird-call in the Mountain 巴松管 Bassoon Sports Term 休斯敦火箭队 Houston Rockets 环法自行车赛 Tour de France ) ] ( ) |</definiens>
			</definition>
</paper>

		<paper id="1084">
			<definition id="0">
				<sentence>FrameNet manual annotation also comprises a layer of grammatical functions : For example , the subject of finite verbs is labeled Ext , and Mod is a label used for modifiers of heads , e.g. an adjective modifying a noun .</sentence>
				<definiendum id="0">Mod</definiendum>
				<definiens id="0">a label used for modifiers of heads</definiens>
			</definition>
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>CSSE performs co-occurrence analysis at the document level and computes the following values : df ( t1 , t2 ) is the joint document frequency , i.e. , the number of web pages where both terms t1 and t2 occur .</sentence>
				<definiendum id="0">CSSE performs co-occurrence analysis at the document level</definiendum>
				<definiens id="0">the joint document frequency</definiens>
			</definition>
			<definition id="1">
				<sentence>df ( t ) is the document frequency of the term t , i.e. , the number of web pages in which the term t occurs .</sentence>
				<definiendum id="0">df ( t )</definiendum>
				<definiens id="0">the document frequency of the term t</definiens>
			</definition>
			<definition id="2">
				<sentence>Then , CSSE applies a well known signal to noise ratio formula coming from data mining ( Church , 1991 ) to establish similarity between terms t1 and t2 : sim ( t1 , t2 ) = ) 2 ( ) 1 ( ) 2,1 ( log tdftdf ttdfN ⋅⋅ / Nlog , ( 1 ) where N is the total number of documents in the mining collection ( corpus ) , log N is the normalizing factor , so the sim value would not exceed 1 and be comparable across collections of different size .</sentence>
				<definiendum id="0">CSSE</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">applies a well known signal to noise ratio formula coming from data mining ( Church , 1991 ) to establish similarity between terms t1 and t2</definiens>
				<definiens id="1">the total number of documents in the mining collection ( corpus ) , log N is the normalizing factor</definiens>
			</definition>
			<definition id="3">
				<sentence>We used a modified version of Lewis’ ( 1992 ) suggestion to derive our evaluation metric , which is similar to the metric derived from Kruskal-Goodman statistics used in Haveliwala et al. ( 2002 ) for a study with Yahoo web directory ( www.yahoo.com ) .</sentence>
				<definiendum id="0">metric</definiendum>
				<definiens id="0">similar to the metric derived from Kruskal-Goodman statistics used in Haveliwala et al.</definiens>
			</definition>
			<definition id="4">
				<sentence>Some sensitivity of the results with respect to the parameters Thresh , Ca is a limitation as occurs similarly to virtually all modern IR improvement techniques .</sentence>
				<definiendum id="0">Ca</definiendum>
				<definiens id="0">a limitation as occurs similarly to virtually all modern IR improvement techniques</definiens>
			</definition>
			<definition id="5">
				<sentence>Although we have involved only one test collection in this study , this collection ( Reuters ) varies greatly in the content and the size of the documents , so we hope our results will generalize to other collections .</sentence>
				<definiendum id="0">Reuters</definiendum>
				<definiens id="0">varies greatly in the content and the size of the documents</definiens>
			</definition>
</paper>

		<paper id="1106">
			<definition id="0">
				<sentence>Moreover , although the need to assemble and extend technical and scienti c terminologies is currently most pressing in the biomedical domain , virtually any ( sub- ) eld of human research/expertise in which we deal with terminologically structured knowledge calls for high-performance terminology identi cation and extraction methods .</sentence>
				<definiendum id="0">scienti c terminologies</definiendum>
				<definiens id="0">currently most pressing in the biomedical domain , virtually any ( sub- ) eld of human research/expertise in which we deal with terminologically structured knowledge calls for high-performance terminology identi cation and extraction methods</definiens>
			</definition>
			<definition id="1">
				<sentence>With these considerations in mind , the biomedical domain is an ideal test bed for evaluating the goodness of ATR algorithms because it hosts one of the most extensive and most carefully curated terminological resources , viz .</sentence>
				<definiendum id="0">biomedical domain</definiendum>
				<definiens id="0">an ideal test bed for evaluating the goodness of ATR algorithms because it hosts one of the most extensive and most carefully curated terminological resources , viz</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus , with s being the number of distinct possible selections for a particular k , the k-modi ability , modk , of an n-gram can be de ned as follows ( f stands for frequency ) : modk ( n-gram ) : = sproductdisplay i=1 f ( n-gram ) f ( seli , n-gram ) ( 2 ) The paradigmatic modi ability , P -Mod , of an ngram is the product of all its k-modi abilities:8 P -Mod ( n-gram ) : = nproductdisplay k=1 modk ( n-gram ) ( 3 ) Comparing the trigram P -Mod values for k = 1 , 2 in Tables 2 and 3 , it can be seen that the term long terminal repeat gets a much higher weight than the non-term t cell response , although their mere frequency values suggest the opposite .</sentence>
				<definiendum id="0">f</definiendum>
				<definiendum id="1">although their mere frequency</definiendum>
				<definiens id="0">the product of all its k-modi abilities:8 P -Mod ( n-gram ) : = nproductdisplay k=1 modk ( n-gram )</definiens>
			</definition>
			<definition id="3">
				<sentence>With increasing portions of all ranked lists considered , the precision curves start to converge toward the baseline , but P -Mod maintains a steady advantage .</sentence>
				<definiendum id="0">P -Mod</definiendum>
				<definiens id="0">maintains a steady advantage</definiens>
			</definition>
			<definition id="4">
				<sentence>847 0 1 100908070605040302010 Portion of ranked list ( in % ) Precision : P-Mod Precision : T-test Precision : C-value Recall : P-Mod Recall : T-test Recall : C-value Base Figure 1 : Precision/Recall for bigram biomedical term extraction 0 1 100908070605040302010 Portion of ranked list ( in % ) Precision : P-Mod Precision : T-test Precision : C-value Recall : P-Mod Recall : T-test Recall : C-value Base Figure 2 : Precision/Recall for trigram biomedical term extraction 0 1 100908070605040302010 Portion of ranked list ( in % ) Precision : P-Mod Precision : T-test Precision : C-value Recall : P-Mod Recall : T-test Recall : C-value Base Figure 3 : Precision/Recall for quadgram biomedical term extraction The ( ascending ) recall curves in Figures 1 , 2 and 3 and their corresponding values in Table 5 indicate which proportion of all true positives ( i.e. , the proportion of all terms in a candidate set ) is identi ed by a particular measure at a certain point of the ranked list .</sentence>
				<definiendum id="0">P-Mod Precision</definiendum>
				<definiendum id="1">P-Mod Precision</definiendum>
				<definiendum id="2">T-test Precision</definiendum>
				<definiens id="0">T-test Precision : C-value Recall : P-Mod Recall : T-test Recall : C-value Base</definiens>
				<definiens id="1">C-value Recall : P-Mod Recall : T-test Recall : C-value Base</definiens>
				<definiens id="2">P-Mod Precision : T-test Precision : C-value Recall : P-Mod Recall : T-test Recall : C-value Base</definiens>
			</definition>
			<definition id="5">
				<sentence>0 1 100908070605040302010 Portion of ranked list ( in % ) Precision : P-Mod Precision : T-test Precision : C-value Recall : P-Mod Recall : T-test Recall : C-value Base Figure 4 : Precision/Recall for trigram biomedical term extraction on the 10-million-word corpus ( cutoff c ≥ 4 , with 6,760 term candidate types ) The P -Mod extraction criterion still clearly outperforms the other ones on that 10-million-word corpus , both in terms of precision and recall .</sentence>
				<definiendum id="0">P-Mod Precision</definiendum>
				<definiens id="0">T-test Precision : C-value Recall : P-Mod Recall : T-test Recall : C-value Base</definiens>
			</definition>
</paper>

		<paper id="1081">
			<definition id="0">
				<sentence>To date , PropBank addresses mainly predicates lexicalized by verbs and a small number of predicates lexicalized by verb nominalizations and adjectives .</sentence>
				<definiendum id="0">PropBank</definiendum>
				<definiens id="0">addresses mainly predicates lexicalized by verbs and a small number of predicates lexicalized by verb nominalizations and adjectives</definiens>
			</definition>
			<definition id="1">
				<sentence>Content words , which add informative lexicalized information different from the head word , were detected using the heuristics of ( Surdeanu et al. , 2003 ) .</sentence>
				<definiendum id="0">Content words</definiendum>
				<definiens id="0">add informative lexicalized information different from the head word</definiens>
			</definition>
			<definition id="2">
				<sentence>We generalize syntactic paths with more than 3 elements using two templates : ( a ) Arg↑Ancestor↓Ni ↓Pred , where Arg is the argument label , Pred is the predicate label , Ancestor is the label of the common ancestor , and Ni is instantiated with all the labels between Pred and Ancestor in the full path ; and ( b ) Arg↑Ni ↑Ancestor↓Pred , where Ni is instantiated with all the labels between Arg and Ancestor in the full path .</sentence>
				<definiendum id="0">Arg</definiendum>
				<definiendum id="1">Pred</definiendum>
				<definiendum id="2">Ancestor</definiendum>
				<definiendum id="3">Ni</definiendum>
				<definiendum id="4">Ni</definiendum>
				<definiens id="0">the predicate label</definiens>
				<definiens id="1">instantiated with all the labels between Pred and Ancestor in the full path</definiens>
				<definiens id="2">instantiated with all the labels between Arg and Ancestor in the full path</definiens>
			</definition>
			<definition id="3">
				<sentence>Mihai Surdeanu is a research fellow within the Ram´on y Cajal program of the Spanish Ministry of Education and Science .</sentence>
				<definiendum id="0">Mihai Surdeanu</definiendum>
				<definiens id="0">a research fellow within the Ram´on y Cajal program of the Spanish Ministry of Education and Science</definiens>
			</definition>
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>The tables required for P ( sijsi−1 ) and P ( oijsi ) are signi cantly smaller than the one for P ( sijsi−1 , oi ) which may be dif cult to estimate due to either data sparsity or normalization issues .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">( oijsi ) are signi cantly smaller than the one for P ( sijsi−1 , oi ) which may be dif cult to estimate due to either data sparsity or normalization issues</definiens>
			</definition>
			<definition id="1">
				<sentence>In other words , all states ( tags ) are independent of future observations ( words ) .</sentence>
				<definiendum id="0">all states</definiendum>
				<definiens id="0">tags ) are independent of future observations ( words )</definiens>
			</definition>
			<definition id="2">
				<sentence>The presence of this observed-child node results in a term in the factorization of the joint probability distribution that couples its parents : P ( c , s , o ) / productdisplay i P ( cijsi−1 , si ) P ( sijwi , fi ) where ci is the observed-child node of tags si−1 and si , and we omit the probability of the observations , P ( wi , fi ) which do not affect the nal choice of s. By the rules of d-separation ( Pearl , 1988 ) , the existence of ci de ned in this way means that the parents ( the adjacent tags ) are not conditionally independent given the child .</sentence>
				<definiendum id="0">ci</definiendum>
				<definiendum id="1">fi</definiendum>
				<definiens id="0">the observed-child node of tags si−1 and si , and we omit the probability of the observations , P ( wi ,</definiens>
				<definiens id="1">the adjacent tags ) are not conditionally independent given the child</definiens>
			</definition>
			<definition id="3">
				<sentence>We rst rewrite the conditional probability ( henceforth abbreviated as p ) as : p = P ( ci = 1jsi−1 , si ) = P ( ci = 1 , si−1 , si ) P ( s i−1 , si ) If the probabilities are maximum likelihood ( ML ) estimates derived from counts on the training data , we can equivalently write : p = N ( ci = 1 , si−1 , si ) N ( s i−1 , si ) 2This use of implied negative training data is similar to the neighborhood concept described in ( Smith and Eisner , 2005 ) 461 where N ( ) is the count function .</sentence>
				<definiendum id="0">conditional probability</definiendum>
				<definiendum id="1">N ( )</definiendum>
				<definiens id="0">estimates derived from counts on the training data</definiens>
			</definition>
			<definition id="4">
				<sentence>We now formally state two hypotheses : H1 that there is a relationship between adjacent tags which can be described by some joint probability distribution P ( si−1 , si ) , and the null hypothesis , H0 , that there is no such relationship , i.e. si−1 and si are independent : PH1 = P ( si−1 , si ) PH0 = P ( si−1 ) P ( si ) Now we can express the counts as follows : N ( ci = 1 , si−1 , si ) = M1 P ( si−1 , si ) N ( ci = 0 , si−1 , si ) = M0 P ( si−1 ) P ( si ) where M1 is the total number of tokens in the ( positive ) training data , and M0 is the total number of tokens in the induced negative training data .</sentence>
				<definiendum id="0">M1</definiendum>
				<definiendum id="1">M0</definiendum>
				<definiens id="0">a relationship between adjacent tags which can be described by some joint probability distribution P ( si−1 , si ) , and the null hypothesis</definiens>
				<definiens id="1">PH1 = P ( si−1 , si ) PH0 = P ( si−1 ) P ( si ) Now we can express the counts as follows : N ( ci = 1 , si−1 , si ) = M1 P ( si−1 , si ) N ( ci = 0 , si−1 , si ) = M0 P ( si−1 ) P ( si ) where</definiens>
				<definiens id="2">the total number of tokens in the ( positive ) training data , and</definiens>
			</definition>
			<definition id="5">
				<sentence>Finally , we write the conditional probability as a function of λ : P ( ci = 1jsi−1 , si ) = 11 + nλ−1 = λλ + n where λ = PH1P H0 = P ( si−1 , si ) P ( s i−1 ) P ( si ) = P ( sijsi−1 ) P ( s i ) The conditional probability , P ( ci = 1jsi−1 , si ) is a mapping g ( λ ) from λ 2 [ 0,1 ) to p 2 [ 0 , 1 ) .</sentence>
				<definiendum id="0">conditional probability</definiendum>
			</definition>
			<definition id="6">
				<sentence>During training , these features are used to learn a smoothed back-off model for P ( sijwi , fi ) ( where fi is a vector of features associated with word wi ) .</sentence>
				<definiendum id="0">fi</definiendum>
				<definiens id="0">a vector of features associated with word wi )</definiens>
			</definition>
			<definition id="7">
				<sentence>This model factorizes the joint probability as : P ( c , s , o ) / productdisplay i P ( cijsi−1 , si , si+1 ) P ( sijwi , fi ) where fi is the appropriate feature bundle for word wi , depending on whether wi is known or unknown .</sentence>
				<definiendum id="0">fi</definiendum>
				<definiens id="0">the appropriate feature bundle for word wi , depending on whether wi is known or unknown</definiens>
			</definition>
</paper>

		<paper id="1076">
			<definition id="0">
				<sentence>RC resembles the ad hoc question answering ( QA ) task that aims to extract an answer from a collection of documents when posed with a question .</sentence>
				<definiendum id="0">RC</definiendum>
				<definiens id="0">resembles the ad hoc question answering ( QA ) task that aims to extract an answer from a collection of documents when posed with a question</definiens>
			</definition>
			<definition id="1">
				<sentence>A reading comprehension ( RC ) system attempts to understand a document and returns an answer sentence when posed with a question .</sentence>
				<definiendum id="0">reading comprehension</definiendum>
				<definiendum id="1">RC ) system</definiendum>
				<definiens id="0">attempts to understand a document and returns an answer sentence when posed with a question</definiens>
			</definition>
			<definition id="2">
				<sentence>HumSent answers were compiled by a human annotator , who examined the stories and chose the sentence ( s ) that best answered the questions .</sentence>
				<definiendum id="0">HumSent answers</definiendum>
			</definition>
			<definition id="3">
				<sentence>RC resembles the ad hoc question answering ( QA ) task in TREC .</sentence>
				<definiendum id="0">RC</definiendum>
				<definiens id="0">resembles the ad hoc question answering ( QA ) task in TREC</definiens>
			</definition>
			<definition id="4">
				<sentence>A popular approach in reading comprehension is to represent the information content of each question or passage sentence as a bag of words ( BOW ) .</sentence>
				<definiendum id="0">reading comprehension</definiendum>
				<definiens id="0">the information content of each question or passage sentence as a bag of words</definiens>
			</definition>
			<definition id="5">
				<sentence>In view of this , we propose a representation for questions and answer sentences that emphasizes three types of metadata : ( i ) Main Verbs ( MVerb ) , identified by the link parser ( Sleator and Temperley 1993 ) ; ( ii ) Named Entities ( NE ) , including names of locations ( LCN ) , persons ( PRN ) and organizations ( ORG ) , identified by a home-grown named entity identification tool ; and ( iii ) Base Noun Phrases ( BNP ) , identified by a home-grown base noun phrase parser respectively .</sentence>
				<definiendum id="0">LCN</definiendum>
				<definiens id="0">a representation for questions and answer sentences that emphasizes three types of metadata : ( i ) Main Verbs ( MVerb ) , identified by the link parser ( Sleator and Temperley 1993 ) ; ( ii ) Named Entities ( NE</definiens>
				<definiens id="1">persons ( PRN ) and organizations ( ORG ) , identified by a home-grown named entity identification tool</definiens>
				<definiens id="2">identified by a home-grown base noun phrase parser respectively</definiens>
			</definition>
			<definition id="6">
				<sentence>The Remedia test set , which contains 60 stories , is set aside for evaluation .</sentence>
				<definiendum id="0">Remedia test set</definiendum>
				<definiens id="0">contains 60 stories , is set aside for evaluation</definiens>
			</definition>
			<definition id="7">
				<sentence>Examples and sizes of question sets ( Q_SETS ) with different metadata – main verb ( MVerb ) , named entity ( NE ) and base noun phrase ( BNP ) .</sentence>
				<definiendum id="0">Q_SETS</definiendum>
				<definiendum id="1">base noun phrase</definiendum>
				<definiendum id="2">BNP</definiendum>
				<definiens id="0">) with different metadata – main verb</definiens>
			</definition>
			<definition id="8">
				<sentence>Eqn ( 1 ) where S Metadata is the set of answer sentences in |A_SET Metadata | that contain the metadata of its corresponding question .</sentence>
				<definiendum id="0">S Metadata</definiendum>
				<definiens id="0">the set of answer sentences in |A_SET Metadata | that contain the metadata of its corresponding question</definiens>
			</definition>
			<definition id="9">
				<sentence>The use of metadata matching to extend the bag-of-words approach in reading comprehension .</sentence>
				<definiendum id="0">metadata</definiendum>
				<definiens id="0">matching to extend the bag-of-words approach in reading comprehension</definiens>
			</definition>
			<definition id="10">
				<sentence>RC resembles the question answering ( QA ) task in TREC which returns an answer for a given question from a collection of documents .</sentence>
				<definiendum id="0">RC</definiendum>
				<definiens id="0">resembles the question answering ( QA ) task in TREC which returns an answer for a given question from a collection of documents</definiens>
			</definition>
			<definition id="11">
				<sentence>This paper presents our initial effort in designing an approach for RC that leverages a variety of knowledge sources beyond the context of the passage , in an attempt to improve RC performance beyond the baseline set by the bag-of-words ( BOW ) approach .</sentence>
				<definiendum id="0">RC</definiendum>
				<definiens id="0">leverages a variety of knowledge sources beyond the context of the passage , in an attempt to improve RC performance beyond the baseline set by the bag-of-words</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>Entity detection and tracking ( EDT ) is the task of identifying textual mentions of real-world entities in documents , extending the named entity detection and coreference resolution task by considering mentions other than names ( pronouns , de nite descriptions , etc. ) .</sentence>
				<definiendum id="0">EDT</definiendum>
				<definiens id="0">the task of identifying textual mentions of real-world entities in documents , extending the named entity detection</definiens>
			</definition>
			<definition id="1">
				<sentence>Optimization ( LaSO ) framework exploits this dif culty as an opportunity and seeks to nd model parameters that are good within the context of search .</sentence>
				<definiendum id="0">Optimization ( LaSO</definiendum>
				<definiens id="0">an opportunity and seeks to nd model parameters that are good within the context of search</definiens>
			</definition>
			<definition id="2">
				<sentence>LaSO parameterizes the search process with a weight vector a23a24a16a26a25a28a27 , where weights correspond to features of search space nodes and inputs .</sentence>
				<definiendum id="0">LaSO</definiendum>
				<definiens id="0">parameterizes the search process with a weight vector a23a24a16a26a25a28a27 , where weights correspond to features of search space nodes and inputs</definiens>
			</definition>
			<definition id="3">
				<sentence>The update has the form given below : a4a39a7 proj a40a41a4a43a42a45a44a47a46a49a48a51a50a53a52a55a54a57a56a59a58 a56a31a60 proj a61a63a62 a64a66a65 sibs a67 a40a41a5a51a68a70a69a32a58 a71sibsa71a26a72 a62 a64a73a65 nodes a67 a40a41a5a51a68a74a69a32a58 a71nodesa71a76a75 Where a77 is the update number , a78 is a tunable parameter and proj projects a vector into the unit sphere .</sentence>
				<definiendum id="0">a78</definiendum>
				<definiens id="0">the update number</definiens>
				<definiens id="1">a tunable parameter and proj projects a vector into the unit sphere</definiens>
			</definition>
			<definition id="4">
				<sentence>These include : the total number of entities detected thus far ; the total number of mentions ; the entity to mention ratio ; the entity 100 to word ratio ; the mention to word ratio ; the size of the hypothesized entity chain ; the ratio of the number of mentions in the current entity chain to the total number of mentions ; the number of intervening mentions between the current mention and the last one in our chain ; the number of intervening mentions of the same type ; the number of intervening sentence breaks ; the Hobbs distance computed over syntactic chunks ; and the decayed density of the hypothesized entity , which is computed as a88a82a89a91a90a93a92 a80a95a94a97a96a99a98a101a100 a89a38a102a104a103 a88a31a89 a80a95a94a97a96a99a98a101a100 a89a91a102 , where a85 ranges over all previous mentions ( constrained in the numerator to be in the same coreference chain as our mention ) and a105a107a106a108a85a17a109 is the number of entities away this mention is .</sentence>
				<definiendum id="0">a105a107a106a108a85a17a109</definiendum>
				<definiens id="0">the total number of entities detected thus far ; the total number of mentions ; the entity to mention ratio ; the entity 100 to word ratio ; the mention to word ratio ; the size of the hypothesized entity chain ; the ratio of the number of mentions in the current entity chain to the total number of mentions</definiens>
				<definiens id="1">the number of intervening sentence breaks ; the Hobbs distance computed over syntactic chunks ; and the decayed density of the hypothesized entity , which is computed as a88a82a89a91a90a93a92 a80a95a94a97a96a99a98a101a100 a89a38a102a104a103 a88a31a89 a80a95a94a97a96a99a98a101a100 a89a91a102 , where a85 ranges over all previous mentions ( constrained in the numerator to</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>Word and Phrase Reordering is a crucial component of Statistical Machine Translation ( SMT ) systems .</sentence>
				<definiendum id="0">Phrase Reordering</definiendum>
			</definition>
			<definition id="1">
				<sentence>The Translation Template Model ( TTM ) is a generative model of phrase-based translation ( Brown et al. , 1993 ) .</sentence>
				<definiendum id="0">Translation Template Model ( TTM )</definiendum>
			</definition>
			<definition id="2">
				<sentence>Translation is modeled via component distributions realized as WFSTs ( Fig 1 and Eqn 1 ) : Source Language Model ( G ) , Source Phrase Segmentation ( W ) , Phrase Translation and Reordering ( R ) , Target Phrase Insertion ( Φ ) , and Target Phrase Segmentation ( Ω ) ( Kumar et al. , 2005 ) .</sentence>
				<definiendum id="0">Translation</definiendum>
			</definition>
			<definition id="3">
				<sentence>The jump bk measures the displacement of the kth phrase xk , i.e. xk → yk+bk , k ∈ { 1,2 , ... , K } .</sentence>
				<definiendum id="0">jump bk</definiendum>
				<definiens id="0">measures the displacement of the kth phrase xk</definiens>
			</definition>
			<definition id="4">
				<sentence>We first present the Finite State Machine of the phrase reordering process ( Fig 3 ) which has two equivalence classes ( FSM states ) for any given history bk−11 ; φ ( bk−11 ) ∈ { 1,2 } .</sentence>
				<definiendum id="0">FSM states</definiendum>
				<definiens id="0">present the Finite State Machine of the phrase reordering process ( Fig 3 ) which has two equivalence classes</definiens>
			</definition>
			<definition id="5">
				<sentence>The input to the WFST ( Fig 4 ) is a lattice of French phrase sequences derived from the French sentence to be translated .</sentence>
				<definiendum id="0">WFST</definiendum>
				<definiens id="0">a lattice of French phrase sequences derived from the French sentence to be translated</definiens>
			</definition>
			<definition id="6">
				<sentence>S is an acceptor for the English sentence E , and T is an acceptor for the French sentence F. The Viterbi alignment is found as the path with the highest probability in B. The WFST composition gives the word-to-word alignments between the sentences .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">an acceptor for the English sentence E , and</definiens>
			</definition>
			<definition id="7">
				<sentence>The Translation Template Model relies on an inventory of target language phrases and their source language translations .</sentence>
				<definiendum id="0">Translation Template Model</definiendum>
				<definiens id="0">relies on an inventory of target language phrases and their source language translations</definiens>
			</definition>
			<definition id="8">
				<sentence>Cx , u ( φ , b ) is the expected number of times the phrase-pair x , u is aligned with a jump of b phrases when the jump history is φ .</sentence>
				<definiendum id="0">Cx</definiendum>
				<definiens id="0">the expected number of times the phrase-pair x</definiens>
			</definition>
			<definition id="9">
				<sentence>As desired , the Alignment Recall ( AR ) and Alignment Error Rate ( AER ) improve modestly while Alignment Precision ( AP ) remains constant .</sentence>
				<definiendum id="0">Alignment Recall ( AR</definiendum>
				<definiens id="0">Alignment Error Rate ( AER ) improve modestly while Alignment Precision ( AP ) remains constant</definiens>
			</definition>
			<definition id="10">
				<sentence>The bitext consists of chunk pairs aligned at sentence and sub-sentence level ( Deng et al. , 2004 ) .</sentence>
				<definiendum id="0">bitext</definiendum>
			</definition>
			<definition id="11">
				<sentence>The Language Model ( LM ) training data consists of approximately 400M words of English text derived from Xinhua and AFP ( English Gigaword ) , the English side of FBIS , the UN and A-E News texts , and the online archives of The People’s Daily .</sentence>
				<definiendum id="0">Language Model ( LM ) training data</definiendum>
			</definition>
</paper>

		<paper id="1098">
			<definition id="0">
				<sentence>Hiero is a stochastic synchronous CFG , whose productions are extracted automatically from unannotated parallel texts , and whose rule probabilities form a log-linear model learned by minimum-errorrate training ; together with a modified CKY beamsearch decoder ( similar to that of Wu ( 1996 ) ) .</sentence>
				<definiendum id="0">Hiero</definiendum>
				<definiens id="0">a stochastic synchronous CFG , whose productions are extracted automatically from unannotated parallel texts , and whose rule probabilities form a log-linear model learned by minimum-errorrate training</definiens>
			</definition>
			<definition id="1">
				<sentence>The basic model uses the following features , analogous to Pharaoh’s default feature set : • P ( γ | α ) and P ( α | γ ) • the lexical weights Pw ( γ | α ) and Pw ( α | γ ) ( Koehn et al. , 2003 ) ; 1 • a phrase penalty exp ( 1 ) ; • a word penalty exp ( l ) , where l is the number of terminals in α .</sentence>
				<definiendum id="0">l</definiendum>
				<definiens id="0">analogous to Pharaoh’s default feature set : • P ( γ | α ) and P ( α | γ ) • the lexical weights Pw ( γ | α ) and Pw ( α | γ ) ( Koehn et al. , 2003 ) ; 1 • a phrase penalty exp ( 1 ) ; • a word penalty exp ( l )</definiens>
				<definiens id="1">the number of terminals in α</definiens>
			</definition>
			<definition id="2">
				<sentence>If a rule occurs in training with more than one possible word alignment , Koehn et al. take the maximum lexical weight ; Hiero uses a weighted average .</sentence>
				<definiendum id="0">Hiero</definiendum>
				<definiens id="0">occurs in training with more than one possible word alignment</definiens>
				<definiens id="1">uses a weighted average</definiens>
			</definition>
			<definition id="3">
				<sentence>780 〈S 1 , S 1 〉 ⇒ 〈S 2 X 3 , S 2 X 3 〉 ⇒ 〈S 4 X 5 X 3 , S 4 X 5 X 3 〉 ⇒ 〈X 6 X 5 X 3 , X 6 X 5 X 3 〉 ⇒ 〈Aozhou X 5 X 3 , Australia X 5 X 3 〉 ⇒ 〈Aozhou shi X 3 , Australia is X 3 〉 ⇒ 〈Aozhou shi X 7 zhiyi , Australia is one of X 7 〉 ⇒ 〈Aozhou shi X 8 de X 9 zhiyi , Australia is one of the X 9 that X 8 〉 ⇒ 〈Aozhou shi yu X 1 you X 2 de X 9 zhiyi , Australia is one of the X 9 that have X 2 with X 1 〉 Figure 2 : Example partial derivation of a synchronous CFG .</sentence>
				<definiendum id="0">Australia</definiendum>
				<definiendum id="1">Australia</definiendum>
				<definiendum id="2">Australia</definiendum>
				<definiens id="0">one of X</definiens>
			</definition>
			<definition id="4">
				<sentence>6For this task , the translation output was uppercased using the SRI-LM toolkit : essentially , it was decoded again using an HMM whose states and transitions are a trigram language model of cased English , and whose emission probabilities are reversed , i.e. , probability of cased word given lowercased word .</sentence>
				<definiendum id="0">SRI-LM toolkit</definiendum>
				<definiens id="0">a trigram language model of cased English , and whose emission probabilities are reversed , i.e. , probability of cased word given lowercased word</definiens>
			</definition>
			<definition id="5">
				<sentence>Finally , we computed the recall of tag patterns , R ( ti ... tj ) = recalled ( ti ... tj ) /freq ( ti ... tj ) , for all patterns in the corpus .</sentence>
				<definiendum id="0">R (</definiendum>
				<definiens id="0">ti ... tj ) = recalled ( ti ... tj ) /freq ( ti ... tj ) , for all patterns in the corpus</definiens>
			</definition>
</paper>

		<paper id="1121">
			<definition id="0">
				<sentence>wpqt = parenleftBigr t R − nt − rt N − R parenrightBig ∗log rt/ ( R − rt ) ( n t − rt ) / ( N − nt − R + rt ) ( 1 ) where rt is the number of seen relevant documents containing term t. nt is the number of documents containing t. R is the number of seen relevant documents for a query .</sentence>
				<definiendum id="0">rt</definiendum>
				<definiendum id="1">nt</definiendum>
				<definiens id="0">parenleftBigr t R − nt − rt N − R parenrightBig ∗log rt/ ( R − rt ) ( n t − rt ) / ( N − nt − R + rt</definiens>
				<definiens id="1">the number of seen relevant documents containing term t.</definiens>
				<definiens id="2">the number of documents containing t. R is the number of seen relevant documents for a query</definiens>
			</definition>
			<definition id="1">
				<sentence>N is the number of documents in the collection .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of documents in the collection</definiens>
			</definition>
			<definition id="2">
				<sentence>The second term of this formula is called the Robertson/Spark Jones weight ( Robertson , 1990 ) which is the core of the term weighting function in the Okapi system ( Robertson , 1997 ) .</sentence>
				<definiendum id="0">Robertson/Spark Jones weight</definiendum>
			</definition>
			<definition id="3">
				<sentence>wpqt = ( pt −qt ) log pt ( 1−qt ) q t ( 1−pt ) ( 2 ) where pt is the probability that a term t appears in relevant documents .</sentence>
				<definiendum id="0">pt</definiendum>
				<definiens id="0">the probability that a term t appears in relevant documents</definiens>
			</definition>
			<definition id="4">
				<sentence>qt is the probability that a term t appears in non-relevant documents .</sentence>
				<definiendum id="0">qt</definiendum>
				<definiens id="0">the probability that a term t appears in non-relevant documents</definiens>
			</definition>
			<definition id="5">
				<sentence>Transductive learning is a machine learning technique based on the transduction which directly derives the classification labels of test data without making any approximating function from training data ( Vapnik , 1998 ) .</sentence>
				<definiendum id="0">Transductive learning</definiendum>
			</definition>
			<definition id="6">
				<sentence>X consists of training data set L = ( ⃗x1 , ⃗x2 , ... , ⃗xl ) and test data set U = ( ⃗xl+1 , ⃗xl+2 , ... , ⃗xl+u ) ; typically l lessmuch u. The purpose of the learning is to assign a label to each data point in U under the condition that the label of each data point in L are given .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">consists of training data set L = ( ⃗x1 , ⃗x2 , ... , ⃗xl ) and test data set U = ( ⃗xl+1 , ⃗xl+2 , ... , ⃗xl+u ) ; typically l lessmuch u. The purpose of the learning is to assign a label to each data point in U under the condition that the label of each data point in L are given</definiens>
			</definition>
			<definition id="7">
				<sentence>γ+ = + √1−f p fp , γ− = − √ f p 1−fp , θ = 12 ( γ+ + γ− ) , and fp is the fraction of relevant documents in X. We can not know the true value of fp in advance , thus we have to estimate its approximation value before applying SGT. According to Joachims , parameter k ( the number of k-nearest points of a data ⃗x ) and d ( the number of eigen values to ... ) give large influence to SGT’s learning performance .</sentence>
				<definiendum id="0">fp</definiendum>
				<definiens id="0">the fraction of relevant documents in X. We can not know the true value of fp in advance</definiens>
				<definiens id="1">the number of k-nearest points of a data ⃗x</definiens>
			</definition>
			<definition id="8">
				<sentence>pt = ∑ i rit∑ i Ri ( 3 ) qt = ∑ i nt −rit∑ i N −Ri ( 4 ) Here , Ri is the number of documents which SGT predicts as relevant with ith value of fip , and rit is the number of documents in Ri where a term t appears .</sentence>
				<definiendum id="0">Ri</definiendum>
				<definiendum id="1">rit</definiendum>
				<definiens id="0">the number of documents which SGT predicts as relevant with ith value of fip</definiens>
				<definiens id="1">the number of documents in Ri where a term t appears</definiens>
			</definition>
			<definition id="9">
				<sentence>score ( d ) = ∑ T∈Q w ( 1 ) · ( k1 + 1 ) tf ( k3 + 1 ) qtf ( K + tf ) ( k 3 + qtf ) ( 5 ) w ( 1 ) = log ( rt + 0.5 ) / ( R − rt + 0.5 ) ( n t − rt + 0.5 ) / ( N − nt − R + rt + 0.5 ) ( 6 ) K = k1 parenleftBig ( 1 − b ) + b dlavdl parenrightBig ( 7 ) where Q is a query containing terms T , tf is the term’s frequency in a document , qtf is the term’s frequency in a text from which Q was derived .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">tf</definiendum>
				<definiendum id="2">qtf</definiendum>
				<definiens id="0">a query containing terms T</definiens>
				<definiens id="1">the term’s frequency in a text from which Q was derived</definiens>
			</definition>
			<definition id="10">
				<sentence>R-Prec : The precision after the first R documents are retrieved , where R is the number of relevant documents for the current topic .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">the number of relevant documents for the current topic</definiens>
			</definition>
			<definition id="11">
				<sentence>MAP : Mean average precision ( MAP ) is the average precision for a single topic is the mean of the precision obtained after each relevant document is retrieved ( using zero as the precision for relevant documents that are not retrieved ) .</sentence>
				<definiendum id="0">MAP</definiendum>
				<definiens id="0">the precision for relevant documents that are not retrieved )</definiens>
			</definition>
			<definition id="12">
				<sentence>Okapi pse : This is a pseudo version of Okapi which assumes top 10 documents in the initial search as relevant ones as well as TREC-8 settings .</sentence>
				<definiendum id="0">Okapi pse</definiendum>
				<definiens id="0">a pseudo version of Okapi which assumes top 10 documents in the initial search as relevant ones as well as TREC-8 settings</definiens>
			</definition>
</paper>

		<paper id="2011">
			<definition id="0">
				<sentence>CSLU Toolkit ( Sutton et al. , 1998 ) and Emu ( Cassidy and Harrington , 2001 ) are built for words transcription or speech events ( such as accent ) ; DAT is built for coding dialogue acts using the DAMSL scheme ( Core and Allen , 1997 ) ; Nb is built for annotating hierarchical discourse structure ( Flammia , 1998 ) ; annotation toolkits , such as Mate ( McKelvie et al. , 2001 ) , AGTK ( Bird et al. , 2001 ) , and Nite ( Carletta et al. , 2003 ) , are built for users to create their own tools .</sentence>
				<definiendum id="0">CSLU Toolkit</definiendum>
				<definiendum id="1">DAT</definiendum>
				<definiendum id="2">annotation toolkits</definiendum>
				<definiens id="0">words transcription or speech events ( such as accent ) ;</definiens>
			</definition>
			<definition id="1">
				<sentence>The annotation tool , DialogueView , consists of three views : WordView , UtteranceView , and BlockView .</sentence>
				<definiendum id="0">annotation tool</definiendum>
				<definiens id="0">consists of three views : WordView , UtteranceView , and BlockView</definiens>
			</definition>
			<definition id="2">
				<sentence>UtteranceView shows the dialogue as a sequence of utterances .</sentence>
				<definiendum id="0">UtteranceView</definiendum>
				<definiens id="0">shows the dialogue as a sequence of utterances</definiens>
			</definition>
			<definition id="3">
				<sentence>BlockView shows the dialogue as a hierarchy of discourse blocks , and abstracts away from the exact utterances that were said .</sentence>
				<definiendum id="0">BlockView</definiendum>
				<definiens id="0">shows the dialogue as a hierarchy of discourse blocks , and abstracts away from the exact utterances that were said</definiens>
			</definition>
			<definition id="4">
				<sentence>For example , for annotating speech repairs , utterance boundaries , and overlapping and abandoned utterances , WordView provides the exact timing information .</sentence>
				<definiendum id="0">WordView</definiendum>
				<definiens id="0">provides the exact timing information</definiens>
			</definition>
			<definition id="5">
				<sentence>In this presentation , we will show how DialogueView helps users annotate speech repairs , utterance boundaries , utterance tags , and hierarchical discourse blocks .</sentence>
				<definiendum id="0">DialogueView</definiendum>
			</definition>
			<definition id="6">
				<sentence>The first view is WordView , which takes as input two audio files ( one for each speaker ) , the words said by each speaker and the start and stop times of each word ( in XML format ) , and shows the words time-aligned with the audio signal .</sentence>
				<definiendum id="0">WordView</definiendum>
				<definiens id="0">takes as input two audio files ( one for each speaker ) , the words said by each speaker and the start and stop times of each word ( in XML format</definiens>
			</definition>
			<definition id="7">
				<sentence>WordView supports playing a region of speech but with the annotated reparanda and editing terms skipped over .</sentence>
				<definiendum id="0">WordView</definiendum>
				<definiens id="0">supports playing a region of speech but with the annotated reparanda and editing terms skipped over</definiens>
			</definition>
</paper>

		<paper id="1087">
			<definition id="0">
				<sentence>Log-linear models have been used in many areas of Natural Language Processing ( NLP ) and Information Retrieval ( IR ) .</sentence>
				<definiendum id="0">Log-linear models</definiendum>
			</definition>
			<definition id="1">
				<sentence>Bernoulli regression models are conditional probability models of a binary response variable Y given a vector vectorX of k explanatory variables ( X1 , ... , Xk ) .</sentence>
				<definiendum id="0">Bernoulli regression models</definiendum>
			</definition>
			<definition id="2">
				<sentence>Let us summarize this first , before expanding the relevant definitions : Y ∼Bernoulli ( p ) logit ( p ) = θ0 +x1 θ1 +x2 θ2 +···+xk θk What this means is that there is an unobserved quantity p , the success probability of the Bernoulli distribution , which we interpret as the probability that Y will take on the value +1 : Pr ( Y = +1|vectorX = ( x1 , x2 , ... , xk ) , vectorθ ) = p The logit ( log odds ) function is defined as follows : logit ( p ) = ln parenleftbigg p 1−p parenrightbigg The logit function is used to transform a probability , constrained to fall within the interval ( 0,1 ) , into a real number ranging over ( −∞ , +∞ ) .</sentence>
				<definiendum id="0">Y ∼Bernoulli ( p ) logit</definiendum>
				<definiens id="0">Y = +1|vectorX = ( x1 , x2 , ... , xk ) , vectorθ ) = p The logit ( log odds ) function is defined as follows : logit ( p ) = ln parenleftbigg p 1−p parenrightbigg The logit function is used to transform a probability , constrained to fall within the interval ( 0,1 ) , into a real number ranging over ( −∞ , +∞ )</definiens>
			</definition>
			<definition id="3">
				<sentence>function of the standard logistic distribution ( also known as the sigmoid or logistic function ) , which we call g : g ( z ) = 11+exp ( −z ) This allows us to write p = g ( θ0 +x1 θ1 +x2 θ2 +···+xk θk ) .</sentence>
				<definiendum id="0">function of the standard logistic distribution</definiendum>
				<definiendum id="1">−z</definiendum>
				<definiens id="0">the sigmoid or logistic function )</definiens>
				<definiens id="1">) This allows us to write p = g ( θ0 +x1 θ1 +x2 θ2 +···+xk θk )</definiens>
			</definition>
			<definition id="4">
				<sentence>Recall ( R ) and precision ( P ) are defined in terms of the number of true positives ( A ) , misses ( B ) , and false alarms ( C ) of the classifier ( cf. Table 1 ) : R = AA+B P = AA+C 693 predicted +1 −1 total true +1 A B npos −1 C D nneg total mpos mneg n Table 1 : Schema for a 2×2 contingency table The Fα measure – familiar from Information Retrieval – combines recall and precision into a single utility criterion by taking their α-weighted harmonic mean : Fα ( R , P ) = parenleftbigg α 1R + ( 1−α ) 1P parenrightbigg−1 The Fα measure can be expressed in terms of the triple ( A , B , C ) as follows : Fα ( A , B , C ) = AA+α B+ ( 1−α ) C ( 2 ) In order to define A , B , and C formally , we use the notation llbracketpirrbracket to denote a variant of the Kronecker delta defined like this , where pi is a Boolean expression : llbracketpirrbracket = braceleftBigg 1 if pi 0 if¬pi Given an evaluation dataset ( vectorx1 , y1 ) , ... , ( vectorxn , yn ) the counts of hits ( true positives ) , misses , and false alarms are , respectively : A = n∑ i=1 largellbrackety map ( vectorxi ) = +1 largerrbracketllbrackety i = +1rrbracket B = n∑ i=1 largellbrackety map ( vectorxi ) =−1 largerrbracketllbrackety i = +1rrbracket C = n∑ i=1 largellbrackety map ( vectorxi ) = +1 largerrbracketllbrackety i =−1rrbracket Note that F-measure is seemingly a global measure of utility that applies to an evaluation dataset as a whole : while the F-measure of a classifier evaluated on a single supervised instance is well defined , the overall F-measure on a larger dataset is not a function of the F-measure evaluated on each instance in the dataset .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiendum id="1">precision</definiendum>
				<definiendum id="2">Fα measure</definiendum>
				<definiendum id="3">pi</definiendum>
				<definiens id="0">defined in terms of the number of true positives ( A ) , misses ( B ) , and false alarms ( C ) of the classifier ( cf. Table 1 ) : R = AA+B P = AA+C 693 predicted +1 −1 total true +1 A B npos −1 C D nneg total mpos mneg n Table 1 : Schema for a 2×2 contingency table The Fα measure – familiar from Information Retrieval – combines recall and precision into a single utility criterion by taking their α-weighted harmonic mean : Fα ( R</definiens>
				<definiens id="1">A , B , C ) as follows : Fα ( A , B , C ) = AA+α B+</definiens>
				<definiens id="2">true positives ) , misses , and false alarms are , respectively : A = n∑ i=1 largellbrackety map ( vectorxi ) = +1 largerrbracketllbrackety i = +1rrbracket B = n∑ i=1 largellbrackety map</definiens>
			</definition>
			<definition id="5">
				<sentence>Define the following vector-valued utility function u , where u ( ˜y|y ) is the utility of choosing the label ˜y if the true label is y : u ( +1|+1 ) = ( 1,0,0 ) u ( −1|+1 ) = ( 0,1,0 ) u ( +1|−1 ) = ( 0,0,1 ) u ( −1|−1 ) = ( 0,0,0 ) This function indicates whether a classification decision is a hit , miss , or false alarm .</sentence>
				<definiendum id="0">u</definiendum>
				<definiens id="0">miss , or false alarm</definiens>
			</definition>
			<definition id="6">
				<sentence>We make the dependence on vectorθ explicit in the formulation of the optimization task : vectorθstar = argmax vectorθ Fα ( A ( vectorθ ) , B ( vectorθ ) , C ( vectorθ ) ) , where ( A ( vectorθ ) , B ( vectorθ ) , C ( vectorθ ) ) =US ( vectorθ ) as defined in ( 3 ) .</sentence>
				<definiendum id="0">B</definiendum>
				<definiendum id="1">B</definiendum>
				<definiens id="0">the dependence on vectorθ explicit in the formulation of the optimization task : vectorθstar = argmax vectorθ Fα ( A ( vectorθ ) ,</definiens>
			</definition>
			<definition id="7">
				<sentence>We encounter the usual problem : the basic quantities involved are integers ( counts of hits , misses , and false alarms ) , and the optimization objective is a piecewise-constant functions of the parameter vector vectorθ , due to the fact that vectorθ occurs exclusively inside Kronecker deltas .</sentence>
				<definiendum id="0">optimization objective</definiendum>
				<definiens id="0">the basic quantities involved are integers ( counts of hits , misses , and false alarms</definiens>
				<definiens id="1">a piecewise-constant functions of the parameter vector vectorθ</definiens>
			</definition>
			<definition id="8">
				<sentence>The training portion consists of 3 535 instances , the test portion of 408 instances .</sentence>
				<definiendum id="0">training portion</definiendum>
			</definition>
</paper>

		<paper id="2002">
			<definition id="0">
				<sentence>IN-SPIRE ( Hetzler et al. , 2004 ) is a visual analytics tool developed by Pacific Northwest National Laboratory to facilitate the collection and rapid understanding of large textual corpora .</sentence>
				<definiendum id="0">IN-SPIRE</definiendum>
				<definiens id="0">a visual analytics tool developed by Pacific Northwest National Laboratory to facilitate the collection and rapid understanding of large textual corpora</definiens>
			</definition>
			<definition id="1">
				<sentence>INSPIRE generates a compiled document set from mathematical signatures for each document in a set .</sentence>
				<definiendum id="0">INSPIRE</definiendum>
				<definiens id="0">generates a compiled document set from mathematical signatures for each document in a set</definiens>
			</definition>
			<definition id="2">
				<sentence>In addition to standard Boolean word queries , QBE is a process in which a user document query is converted into a mathematical signature and compared to the multidimensional mathematical representation of the document corpus .</sentence>
				<definiendum id="0">QBE</definiendum>
				<definiens id="0">a process in which a user document query is converted into a mathematical signature and compared to the multidimensional mathematical representation of the document corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>Information analysts need to sift through large datasets quickly and efficiently to identify relevant information for knowledge discovery .</sentence>
				<definiendum id="0">Information analysts</definiendum>
				<definiens id="0">large datasets quickly and efficiently to identify relevant information for knowledge discovery</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>TAxiom3 , TAxiom4 and HAxiom1 are the frame-related axioms used by the logic prover .</sentence>
				<definiendum id="0">HAxiom1</definiendum>
				<definiens id="0">the frame-related axioms used by the logic prover</definiens>
			</definition>
			<definition id="1">
				<sentence>373 POSSESSION ( POS ) MAKE-PRODUCE ( MAK ) RECIPIENT ( REC ) THEME-PATIENT ( THM ) KINSHIP ( KIN ) INSTRUMENT ( INS ) FREQUENCY ( FRQ ) RESULT ( RSL ) PROPERTY-ATTRIBUTE ( PAH ) LOCATION-SPACE ( LOC ) INFLUENCE ( IFL ) STIMULUS ( STI ) AGENT ( AGT ) PURPOSE ( PRP ) ASSOCIATED WITH ( OTH ) EXTENT ( EXT ) TEMPORAL ( TMP ) SOURCE-FROM ( SRC ) MEASURE ( MEA ) PREDICATE ( PRD ) DEPICTION ( DPC ) TOPIC ( TPC ) SYNONYMY-NAME ( SYN ) CAUSALITY ( CSL ) PART-WHOLE ( PW ) MANNER ( MNR ) ANTONYMY ( ANT ) JUSTIFICATION ( JST ) HYPERNYMY ( ISA ) MEANS ( MNS ) PROBABILITY OF EXISTENCE ( PRB ) GOAL ( GOL ) ENTAIL ( ENT ) ACCOMPANIMENT ( ACC ) POSSIBILITY ( PSB ) BELIEF ( BLF ) CAUSE ( CAU ) EXPERIENCER ( EXP ) CERTAINTY ( CRT ) MEANING ( MNG ) Table 2 : The set of semantic relations Our goal is to devise semantic axioms for combinations of two relations , R1 and R2 , by observing the semantic connection between the w1 and w3 words for which there exists at least one other word , w2 , such that R1 ( w1 , w2 ) and R2 ( w2 , w3 ) hold true3 .</sentence>
				<definiendum id="0">POS ) MAKE-PRODUCE ( MAK ) RECIPIENT ( REC ) THEME-PATIENT ( THM ) KINSHIP ( KIN ) INSTRUMENT ( INS ) FREQUENCY ( FRQ ) RESULT</definiendum>
				<definiendum id="1">ANT ) JUSTIFICATION ( JST ) HYPERNYMY ( ISA ) MEANS</definiendum>
				<definiendum id="2">GOL</definiendum>
				<definiendum id="3">BLF ) CAUSE ( CAU ) EXPERIENCER ( EXP ) CERTAINTY ( CRT ) MEANING</definiendum>
				<definiens id="0">to devise semantic axioms for combinations of two relations , R1 and R2 , by observing the semantic connection between the w1</definiens>
			</definition>
			<definition id="2">
				<sentence>◦ symbolizes the semantic composition between two relations compatible with respect to the part-of-speech of their arguments : for any two concepts , w1 and w3 , ( Ri◦Rj ) ( w1 , w3 ) if and only if∃w2 , a third concept , such that Ri ( w1 , w2 ) and Rj ( w2 , w3 ) hold .</sentence>
				<definiendum id="0">◦</definiendum>
				<definiens id="0">symbolizes the semantic composition between two relations compatible with respect to the part-of-speech of their arguments : for any two concepts , w1 and w3</definiens>
			</definition>
			<definition id="3">
				<sentence>By R−1 , 3R ( x , y ) indicates that relation R holds between x and y. 4This set includes the AGENT , OBJECT , INSTRUMENT , BENEFICIARY , PURPOSE , ATTRIBUTE , REASON , STATE , LOCATION , THEME , TIME , and MANNER relations .</sentence>
				<definiendum id="0">y )</definiendum>
				<definiens id="0">indicates that relation R holds between x and y. 4This set includes the AGENT , OBJECT , INSTRUMENT , BENEFICIARY , PURPOSE , ATTRIBUTE , REASON , STATE , LOCATION , THEME , TIME</definiens>
			</definition>
			<definition id="4">
				<sentence>ISA ◦ ATTRIBUTE = ATTRIBUTE ( ISA ( x , y ) ∧ ATTRIBUTE ( y , a ) → ATTRIBUTE ( x , a ) ) Example : Mike is a rich man .</sentence>
				<definiendum id="0">Mike</definiendum>
				<definiens id="0">a rich man</definiens>
			</definition>
			<definition id="5">
				<sentence>THEME ( kill , Israeli ) ∧ PART-WHOLE ( child , Israeli ) → THEME ( kill , child ) Similar statements can be made for all the thematic roles : AGENT , EXPERIENCER , INSTRUMENT , CAUSE , LOCATION , etc .</sentence>
				<definiendum id="0">THEME</definiendum>
			</definition>
			<definition id="6">
				<sentence>SRC ( fighter-bomber aircraft , countries other than the USA and the Soviet Union ) Table 4 : Frames-related semantic rules frame’s lexical units .</sentence>
				<definiendum id="0">SRC</definiendum>
				<definiens id="0">Frames-related semantic rules frame’s lexical units</definiens>
			</definition>
			<definition id="7">
				<sentence>If we consider only the DEPARTING frame and its corresponding rules , without looking at the context , then our conclusions ( ¬ LOC ( John , kitchen ) and SRC ( John , kitchen ) ) will be false .</sentence>
				<definiendum id="0">SRC</definiendum>
				<definiens id="0">the DEPARTING frame and its corresponding rules , without looking at the context</definiens>
			</definition>
			<definition id="8">
				<sentence>The ACC and F columns ( Table 7 ) show the performance of the system before and after we added our semantic rules to the list of axioms needed by the logic prover .</sentence>
				<definiendum id="0">ACC</definiendum>
				<definiens id="0">show the performance of the system before and after we added our semantic rules to the list of axioms needed by the logic prover</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>The combiner is a linear multi-class one-vs-one SVM2 , as in the Zhang and Lee ( 2003 ) baseline .</sentence>
				<definiendum id="0">combiner</definiendum>
			</definition>
			<definition id="1">
				<sentence>WHNP is the Noun Phrase corresponding to the Wh-word , given by a sentence parser ( see §4.2 ) .</sentence>
				<definiendum id="0">WHNP</definiendum>
				<definiens id="0">the Noun Phrase corresponding to the Wh-word , given by a sentence parser</definiens>
			</definition>
</paper>

		<paper id="1109">
			<definition id="0">
				<sentence>Igbo is an African language spoken mainly in Nigeria by an estimated 10 to 18 million people , written in Latin script .</sentence>
				<definiendum id="0">Igbo</definiendum>
				<definiens id="0">an African language spoken mainly in Nigeria by an estimated 10 to 18 million people , written in Latin script</definiens>
			</definition>
			<definition id="1">
				<sentence>Cebuano is a language spoken by about 15 million people in the Philippines , written in Latin script .</sentence>
				<definiendum id="0">Cebuano</definiendum>
				<definiens id="0">a language spoken by about 15 million people in the Philippines , written in Latin script</definiens>
			</definition>
			<definition id="2">
				<sentence>The size column represents the number of dictionary entries used for training , where each entry consists of one or more Cebuano words .</sentence>
				<definiendum id="0">size column</definiendum>
			</definition>
</paper>

		<paper id="1088">
			<definition id="0">
				<sentence>Evita performs the identification and tagging of event expressions based on fairly simple strategies , informed by both linguisticand statistically-based data .</sentence>
				<definiendum id="0">Evita</definiendum>
				<definiens id="0">performs the identification and tagging of event expressions based on fairly simple strategies , informed by both linguisticand statistically-based data</definiens>
			</definition>
			<definition id="1">
				<sentence>Evita ( ’Events InText Analyzer’ ) is an event recognition system developed under the ARDA-funded TARSQI research framework .</sentence>
				<definiendum id="0">Evita</definiendum>
				<definiens id="0">an event recognition system developed under the ARDA-funded TARSQI research framework</definiens>
			</definition>
			<definition id="2">
				<sentence>AF Event class distinguishes among states ( e.g. , be the director of ) , general occurrences ( walk ) , reporting ( tell ) , intensional ( attempt ) , and perception ( observe ) events .</sentence>
				<definiendum id="0">AF Event class</definiendum>
				<definiens id="0">distinguishes among states ( e.g. , be the director of ) , general occurrences ( walk ) , reporting ( tell ) , intensional ( attempt ) , and perception ( observe ) events</definiens>
			</definition>
			<definition id="3">
				<sentence>TimeML does not identify event participants , but the event tag and its attributes have been designed to interface with Named Entity taggers in a straightforward manner .</sentence>
				<definiendum id="0">TimeML</definiendum>
				<definiens id="0">does not identify event participants , but the event tag and its attributes have been designed to interface with Named Entity taggers in a straightforward manner</definiens>
			</definition>
			<definition id="4">
				<sentence>Figure 1 illustrates the rule for verbal phrases of future tense , progressive aspect , which bear the modal form have to ( as in , e.g. , Participants will have to be working on the same topics ) : [ form in futureForm ] , [ form==’have’ ] , [ form==’to’ , pos==’TO’ ] , [ form==’be’ ] , [ pos==’VBG’ ] , == &gt; [ tense=’FUTURE’ , aspect=’PROGRESSIVE’ , nf morph=’NONE’ ] Figure 1 : Grammatical Rule For event-denoting expressions containing no verbal chunk , tense and aspect is established as null ( ’NONE’ value ) , and non-finite morphology is ’noun’ or ’adjective’ , depending on the part-ofspeech of their head .</sentence>
				<definiendum id="0">aspect=’PROGRESSIVE’</definiendum>
				<definiens id="0">Grammatical Rule For event-denoting expressions containing no verbal chunk</definiens>
			</definition>
			<definition id="5">
				<sentence>Evita extracts the values of these two attributes using basic pattern-matching techniques over the approapriate verbal , nominal , or adjectival chunk .</sentence>
				<definiendum id="0">Evita</definiendum>
				<definiens id="0">extracts the values of these two attributes using basic pattern-matching techniques over the approapriate verbal , nominal , or adjectival chunk</definiens>
			</definition>
			<definition id="6">
				<sentence>In addition , Evita identifies the grammatical information that is associated with the eventreferring expression , such as tense , aspect , polarity , and modality .</sentence>
				<definiendum id="0">grammatical information</definiendum>
				<definiens id="0">tense , aspect , polarity , and modality</definiens>
			</definition>
			<definition id="7">
				<sentence>Evita is a component within a larger suite of tools .</sentence>
				<definiendum id="0">Evita</definiendum>
				<definiens id="0">a component within a larger suite of tools</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>In the IL setting , a category Ci is described by providing a set of relevant features , termed an intensional description ( ID ) , idci ⊆ V , where V is the vocabulary .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">described by providing a set of relevant features , termed an intensional description ( ID ) , idci ⊆ V , where</definiens>
				<definiens id="1">the vocabulary</definiens>
			</definition>
			<definition id="1">
				<sentence>It is possible first to follow the intuitively appealing and principled approach of ( Liu et al. , 2004 ) , in which IDs ( category seeds ) and instances are represented by vectors in a usual IR-style Vector Space Model ( VSM ) , and similarity is measured by the cosine function : simvsm ( idci , tj ) = cos ( vectoridci , vectortj ) ( 1 ) where vectoridci ∈ R|V | and vectortj ∈ R|V | are the vectorial representations in the space R|V | respectively of the category ID idci and the instance tj , and V is the set of all the features ( the vocabulary ) .</sentence>
				<definiendum id="0">similarity</definiendum>
				<definiendum id="1">V</definiendum>
				<definiens id="0">in which IDs ( category seeds ) and instances are represented by vectors in a usual IR-style Vector Space Model</definiens>
				<definiens id="1">the set of all the features ( the vocabulary )</definiens>
			</definition>
			<definition id="2">
				<sentence>For each category Ci , GM induces a mapping from the similarity scores between its ID and any instance tj , sim ( idci , tj ) , into the probability of Ci given the text tj , P ( Ci|tj ) .</sentence>
				<definiendum id="0">GM</definiendum>
				<definiens id="0">induces a mapping from the similarity scores between its ID and any instance tj , sim ( idci , tj ) , into the probability of Ci given the text tj</definiens>
			</definition>
			<definition id="3">
				<sentence>To achieve this goal GM performs the following operations : ( i ) it computes the set Si = { sim ( idci , tj ) |tj ∈ T } of the similarity scores between the ID idci of the category Ci and all the instances tj in the unlabeled training set T ; ( ii ) it induces from the empirical distribution of values in Si a Gaussian Mixture distribution which is composed of two “hypothetic” distributions Ci and Ci , which are assumed to describe respectively the distributions of similarity scores for positive and negative examples ; and ( iii ) it estimates the conditional probability P ( Ci|sim ( idci , tj ) ) by applying the Bayes theorem on the distributions Ci and Ci .</sentence>
				<definiendum id="0">conditional probability P ( Ci|sim</definiendum>
				<definiens id="0">|tj ∈ T } of the similarity scores between the ID idci of the category Ci and all the instances tj in the unlabeled training set T</definiens>
				<definiens id="1">assumed to describe respectively the distributions of similarity scores for positive and negative examples</definiens>
			</definition>
</paper>

		<paper id="1122">
			<definition id="0">
				<sentence>Lexical cohesion measured by latent semantic analysis ( LSA ) ( Landauer and Dumais , 1997 ; Dumais , 1993 ; Manning and Schütze , 1999 ) has been used in automated essay grading ( Landauer , Foltz , and Laham , 1998 ) and in understanding student input during tutorial dialogue ( Graesser et al. , 2001 ) .</sentence>
				<definiendum id="0">Lexical cohesion</definiendum>
			</definition>
			<definition id="1">
				<sentence>LSA ( Landauer and Dumais , 1997 ; Dumais 1993 ) is an extension of the vector space model that uses singular value decomposition ( SVD ) .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">an extension of the vector space model that uses singular value decomposition ( SVD )</definiens>
			</definition>
			<definition id="2">
				<sentence>SVD is a technique that creates an approximation of the original word by document matrix .</sentence>
				<definiendum id="0">SVD</definiendum>
			</definition>
			<definition id="3">
				<sentence>Cohesion can be measured by comparing the cosines of two successive sentences or paragraphs ( Foltz , Kintsch , and Landauer , 1998 ) .</sentence>
				<definiendum id="0">Cohesion</definiendum>
				<definiens id="0">comparing the cosines of two successive sentences or paragraphs</definiens>
			</definition>
			<definition id="4">
				<sentence>The basis is a subspace of the higher dimensional LSA space , in the same way as a plane or line is a subspace of 3D space .</sentence>
				<definiendum id="0">line</definiendum>
				<definiens id="0">a subspace of the higher dimensional LSA space , in the same way as a plane or</definiens>
			</definition>
			<definition id="5">
				<sentence>Instead , the window consists of two orthonormal bases , one on either side of an utterance .</sentence>
				<definiendum id="0">window</definiendum>
				<definiens id="0">consists of two orthonormal bases , one on either side of an utterance</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>Question answering ( QA ) systems for document collections typically aim to identify in the collections text snippets ( e.g. , 50 or 250 characters long ) or exact answers ( e.g. , names , dates ) that answer natural language questions submitted by their users .</sentence>
				<definiendum id="0">Question answering</definiendum>
			</definition>
			<definition id="1">
				<sentence>323 DEFQA , which handles definition questions .</sentence>
				<definiendum id="0">DEFQA</definiendum>
			</definition>
			<definition id="2">
				<sentence>m patterns with the highest precision scores are retained , where precision is the number of training definition windows the pattern matches divided by the total number of training windows it matches .</sentence>
				<definiendum id="0">precision</definiendum>
				<definiens id="0">the pattern matches divided by the total number of training windows it matches</definiens>
			</definition>
			<definition id="3">
				<sentence>We then compute the similarity of W to C as : sim ( W , C ) = 1/|W| · Σ|W|i=1sim ( wi , C ) where |W| is the number of distinct words in W , and sim ( wi , C ) is the similarity of the i-th distinct word of W to C , defined as follows : sim ( wi , C ) = fdef ( wi , C ) ·idf ( wi ) fdef ( wi , C ) is the percentage of definitions in C that contain wi , and idf ( wi ) is the inverse document frequency of wi in the British National Corpus ( BNC ) : idf ( wi ) = 1 + log Ndf ( w i ) N is the number of documents in BNC , and df ( wi ) the number of BNC documents where wi occurs ; if wi does not occur in BNC , we use the lowest df score of BNC .</sentence>
				<definiendum id="0">|W|</definiendum>
				<definiendum id="1">C )</definiendum>
				<definiendum id="2">idf</definiendum>
				<definiens id="0">the number of distinct words in W , and sim ( wi ,</definiens>
				<definiens id="1">the similarity of the i-th distinct word of W to C , defined as follows : sim ( wi</definiens>
				<definiens id="2">the percentage of definitions in C that contain wi</definiens>
				<definiens id="3">the inverse document frequency of wi in the British National Corpus ( BNC ) : idf ( wi ) = 1 + log Ndf ( w i ) N is the number of documents in BNC , and df ( wi ) the number of BNC documents where wi occurs ; if wi does not occur in BNC</definiens>
			</definition>
</paper>

		<paper id="1102">
			<definition id="0">
				<sentence>Because of the adjoining operation , the time complexity of LTAG parsing is as large as a2a4a3a6a5a8a7a10a9 , compared with a2a11a3a6a5a13a12a14a9 of CFG parsing , where a5 is the length of the sentence to be parsed .</sentence>
				<definiendum id="0">a5</definiendum>
			</definition>
			<definition id="1">
				<sentence>811 JJ VP S JJ VP S TO CC VP *VBZNNDT XP XP WDT PRP interestingnew andseemswhichparsera meto T T T T T A T C VP XPXP XPXP Figure 1 : An example in LTAG-spinal .</sentence>
				<definiendum id="0">JJ VP S TO CC VP *VBZNNDT XP XP WDT PRP</definiendum>
				<definiens id="0">interestingnew andseemswhichparsera meto T T T T T A T C VP XPXP XPXP Figure 1 : An example in LTAG-spinal</definiens>
			</definition>
			<definition id="2">
				<sentence>The X axis represents the number of iterations of training , and the Y axis represents the f-score of dependency with respected to the LTAG derivation tree .</sentence>
				<definiendum id="0">X axis</definiendum>
				<definiendum id="1">Y axis</definiendum>
				<definiens id="0">represents the number of iterations of training</definiens>
				<definiens id="1">represents the f-score of dependency with respected to the LTAG derivation tree</definiens>
			</definition>
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>Most researchers working on word sense disambiguation ( WSD ) use manually sense tagged data such as SemCor ( Miller et al. , 1993 ) to train statistical classifiers , but also use the information in SemCor on the overall sense distribution for each word as a backoff model .</sentence>
				<definiendum id="0">WSD</definiendum>
				<definiens id="0">Miller et al. , 1993 ) to train statistical classifiers</definiens>
			</definition>
			<definition id="1">
				<sentence>We computed salience as a ratio of normalised document frequencies , using the formula a12a14a13 a2a16a15a18a17a20a19a21a4a23a22a25a24a9a26a28a27a29a22a25a26 a22 a24 a27a29a22 where a30a32a31a34a33 is the number of documents in domaina17 containing the noun ( lemma ) a2 , a30a35a33 is the number of documents in domain a17 , a30a32a31 is the total number of documents containing the noun a2 and a30 is the total number of documents .</sentence>
				<definiendum id="0">salience</definiendum>
				<definiendum id="1">a30a32a31</definiendum>
				<definiendum id="2">a30</definiendum>
				<definiens id="0">a ratio of normalised document frequencies , using the formula a12a14a13 a2a16a15a18a17a20a19a21a4a23a22a25a24a9a26a28a27a29a22a25a26 a22 a24 a27a29a22 where a30a32a31a34a33 is the number of documents in domaina17 containing the noun</definiens>
				<definiens id="1">the number of documents in domain a17</definiens>
				<definiens id="2">the total number of documents</definiens>
			</definition>
			<definition id="2">
				<sentence>Open Mind is a web based system for annotating sentences .</sentence>
				<definiendum id="0">Open Mind</definiendum>
				<definiens id="0">a web based system for annotating sentences</definiens>
			</definition>
</paper>

		<paper id="1100">
			<definition id="0">
				<sentence>Each POS tagset can be thought of as a particular morphological model or a subset of morphological attributes .</sentence>
				<definiendum id="0">POS tagset</definiendum>
			</definition>
			<definition id="1">
				<sentence>In Model 1 , the probability of generating the noun phrase ( NP ) with headword gatos and headtag noun ( n ) is defined as follows:3 P ( gatos , n , NP | corri´o , v , S , VP ) = P1 ( n , NP | corri´o , v , S , VP ) × P2 ( gatos | n , NP , corri´o , v , S , VP ) The parser smooths parameter values using backedoff statistics , and in particular smooths statistics based on headwords with coarser statistics based on POS tags alone .</sentence>
				<definiendum id="0">VP ) × P2</definiendum>
				<definiens id="0">the probability of generating the noun phrase ( NP ) with headword gatos and headtag noun ( n ) is defined as follows:3 P ( gatos , n , NP | corri´o , v , S , VP ) = P1 ( n , NP | corri´o , v , S ,</definiens>
				<definiens id="1">gatos | n , NP , corri´o , v , S , VP ) The parser smooths parameter values using backedoff statistics , and in particular smooths statistics based on headwords with coarser statistics based on POS tags alone</definiens>
			</definition>
			<definition id="2">
				<sentence>In our example , each term is estimated as follows : P1 ( n , NP | corri´o , v , S , VP ) = λ1,1 ˆP1,1 ( n , NP | corri´o , v , S , VP ) + λ1,2 ˆP1,2 ( n , NP | v , S , VP ) + λ1,3 ˆP1,3 ( n , NP | S , VP ) and P2 ( gatos | n , NP , corri´o , v , S , VP ) = λ2,1 ˆP2,1 ( gatos | n , NP , corri´o , v , S , VP ) + λ2,2 ˆP2,2 ( gatos | n , NP , v , S , VP ) + λ2,3 ˆP2,3 ( gatos | n ) 3Note that the parsing model includes other features such as distance which we omit from the parameter definition for the sake of brevity .</sentence>
				<definiendum id="0">P2</definiendum>
				<definiens id="0">gatos | n , NP , corri´o , v</definiens>
				<definiens id="1">gatos | n , NP , v</definiens>
			</definition>
			<definition id="3">
				<sentence>The model BL is the baseline , and M is the morphological model n ( A , D , N , V , P ) +m ( V ) .</sentence>
				<definiendum id="0">model BL</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">the baseline</definiens>
			</definition>
</paper>

		<paper id="1073">
			<definition id="0">
				<sentence>, emk } , where em1 denotes the special case of neutrality , or absence of emotion .</sentence>
				<definiendum id="0">em1</definiendum>
				<definiens id="0">the special case of neutrality , or absence of emotion</definiens>
			</definition>
			<definition id="1">
				<sentence>, fn } , where F contains the features derived from the text .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">contains the features derived from the text</definiens>
			</definition>
			<definition id="2">
				<sentence>For the adjectives , Py-WordNet’s ( Steele et al. , 2004 ) SIMILAR feature was used to retrieve similar items of the primary emotion adjectives , exploring one additional level in the hierarchy ( i.e. similar items of all senses of all words in the synset ) .</sentence>
				<definiendum id="0">Py-WordNet’s</definiendum>
				<definiens id="0">similar items of all senses of all words in the synset )</definiens>
			</definition>
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>Each sentence si for i = 1 ... n in our training data has a set of ni candidate parse trees ti,1 , ... , ti , ni , which are the output of an N–best baseline parser .</sentence>
				<definiendum id="0">ni</definiendum>
				<definiens id="0">a set of ni candidate parse trees ti,1 , ... , ti ,</definiens>
			</definition>
			<definition id="1">
				<sentence>A global hidden–value assignment , which attaches a hidden value to every word in ti , j , is written h = ( h1 , ... , hm ) ∈ H ( ti , j ) , where H ( ti , j ) = H1 ( ti , j ) × ... ×Hm ( ti , j ) is the set of all possible global assignments for ti , j. We define a feature–based representation Φ such that Φ ( ti , j , h ) ∈ Rd is a vector of feature occurrence counts that describes candidate parse ti , j with global assignment h ∈ H ( ti , j ) .</sentence>
				<definiendum id="0">global hidden–value assignment</definiendum>
				<definiendum id="1">Rd</definiendum>
				<definiens id="0">attaches a hidden value to every word in ti , j , is written h = ( h1 , ... , hm ) ∈ H ( ti , j )</definiens>
				<definiens id="1">the set of all possible global assignments for ti , j. We define a feature–based representation Φ such that Φ ( ti , j , h ) ∈</definiens>
				<definiens id="2">a vector of feature occurrence counts that describes candidate parse ti , j with global assignment h ∈ H ( ti , j )</definiens>
			</definition>
			<definition id="2">
				<sentence>For instance , define φ12 ( ti , j , w , hw ) = Largellbracketh w = the3 and tree ti , j assigns word w to part–of–speech DT Largerrbracket φ101 ( ti , j , u , v , hu , hv ) = LARGEllbracket ( h u , hv ) = ( CEO1 , owns2 ) and tree ti , j places ( u , v ) in a subject–verb relationship LARGErrbracket where the notation llbracketPrrbracket signifies a 0/1 indicator of predicate P. When summed over the tree , these definitions of φ12 and φ101 yield global features Φ12 and Φ101 as given in the previous example ( see Eq .</sentence>
				<definiendum id="0">llbracketPrrbracket</definiendum>
			</definition>
			<definition id="3">
				<sentence>We created mixtures of different models using a weighted average : logp ( ti , j|si ) = Msummationdisplay m=1 λm logpm ( ti , j|si , Θm ) −Z ( si ) where Z ( si ) is a normalization constant that can be ignored , as it does not affect the ranking of parses .</sentence>
				<definiendum id="0">Z ( si )</definiendum>
				<definiens id="0">a normalization constant that can be ignored</definiens>
			</definition>
</paper>

		<paper id="1085">
			<definition id="0">
				<sentence>We expected that the class of tags most likely to be useful as pseudowords would be the person tags , because Czech is a pro-drop language .</sentence>
				<definiendum id="0">Czech</definiendum>
				<definiens id="0">a pro-drop language</definiens>
			</definition>
</paper>

		<paper id="1048">
			<definition id="0">
				<sentence>( e.g *S ) *S ) marks a position that two clauses end ) Named entities-1 : The IOB tags of named entities .</sentence>
				<definiendum id="0">IOB</definiendum>
				<definiens id="0">tags of named entities</definiens>
			</definition>
			<definition id="1">
				<sentence>Gazetteer labels : indicate the name of the list to which the token belongs .</sentence>
				<definiendum id="0">Gazetteer labels</definiendum>
			</definition>
			<definition id="2">
				<sentence>Gazetteer labels : indicate the name of the list to which the token belongs .</sentence>
				<definiendum id="0">Gazetteer labels</definiendum>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>The ESK is a kind of convolution kernel ( Collins and Duffy , 2001 ) .</sentence>
				<definiendum id="0">ESK</definiendum>
			</definition>
			<definition id="1">
				<sentence>The ESK is an extension of the String Subsequence Kernel ( SSK ) ( Lodhi et al. , 2002 ) and the Word Sequence Kernel ( WSK ) ( Cancedda et al. , 2003 ) .</sentence>
				<definiendum id="0">ESK</definiendum>
			</definition>
			<definition id="2">
				<sentence>ESKa58a29a59a61a60a63a62a29a64a66a65a68a67 a58 a69a71a70a73a72 a74a76a75a78a77a80a79 a81a83a82a83a77a85a84 a86 a69 a59a61a87a89a88a90a62a90a91a47a92a83a65 ( 1 ) a86 a69 a59a61a87a93a88a90a62a90a91a47a92a94a65a68a67 a95a34a96 a97 a59a61a87a89a88a98a62a90a91a47a92a29a65 if a99a100a67a102a101 a86a104a103 a69a106a105a27a72 a59a61a87a93a88a90a62a90a91a51a92a29a65a47a107 a95a34a96 a97 a59a61a87a93a88a90a62a90a91a51a92a29a65 otherwise ( 2 ) Here , a108 is the upper bound of the subsequence length and a109a111a110a112 a28a78a113a29a114a93a32a93a115a117a116a118a36 is defined as follows .</sentence>
				<definiendum id="0">a108</definiendum>
			</definition>
			<definition id="3">
				<sentence>a115a120a116 is the a121 -th node of a57 .</sentence>
				<definiendum id="0">a115a120a116</definiendum>
				<definiens id="0">the a121 -th node of a57</definiens>
			</definition>
			<definition id="4">
				<sentence>Sima58a133a90a134a136a135 a59a61a60a63a62a94a64a66a65a68a67 ESKa58 a59a61a60a137a62a93a64a66a65 ESKa58 a59a61a60a63a62a90a60a19a65 ESKa58 a59a132a64a138a62a94a64a66a65 a131 ( 5 ) Suppose , a139 is a system output , which consists of a140 sentences , and a141 is a human written reference , which consists of a142 sentences .</sentence>
				<definiendum id="0">a139</definiendum>
				<definiendum id="1">a141</definiendum>
				<definiens id="0">a system output</definiens>
				<definiens id="1">consists of a140 sentences , and</definiens>
			</definition>
			<definition id="5">
				<sentence>First , we define a precision-oriented measure as follows : a145 a58 a133a90a134a136a135 a59a61a146a31a62a90a147a49a65a68a67 a101 a148 a149 a88 a70a73a72a151a150a49a152a132a153 a72a68a154 a92 a154a4a69 Sima58a133a98a134a61a135 a59a61a155 a88 a62a90a156 a92 a65 ( 6 ) Symmetrically , we define a recall-oriented measure as follows : a157 a58 a133a98a134a61a135 a59a61a146a31a62a98a147a49a65a68a67 a101 a99 a69 a92 a70a73a72 a150a49a152a132a153 a72a68a154 a88 a154 a149 Sima58a133a90a134a136a135 a59a61a155a158a88a78a62a98a156a159a92a29a65 ( 7 ) Finally , we define a unified measure , i.e. , Fmeasure , as follows : a160 a58 a133a98a134a61a135 a59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a104a162 a2 a65a117a163 a157 a133a90a134a136a135 a59a61a146a31a62a90a147a49a65a117a163 a145 a133a90a134a136a135 a59a61a146a31a62a164a147a49a65 a157 a133a98a134a61a135 a59a61a146a31a62a164a147a49a65a47a128a49a162 a2 a163 a145 a133a98a134a61a135 a59a61a146a31a62a164a147a49a65 ( 8 ) a165 is a cost parameter for a166a168a167a132a169a78a170 and a171a138a167a132a169a78a170 .</sentence>
				<definiendum id="0">a165</definiendum>
				<definiens id="0">a cost parameter for a166a168a167a132a169a78a170 and a171a138a167a132a169a78a170</definiens>
			</definition>
			<definition id="6">
				<sentence>a166 indicates a set of references ; a166a177a38a54a15a159a141a179a178a80a32a34a180a34a180a34a180a85a32a93a141a182a181a138a16 .</sentence>
				<definiendum id="0">a166</definiendum>
				<definiens id="0">indicates a set of references</definiens>
			</definition>
			<definition id="7">
				<sentence>Step 2 For a144a25a116 , the human assessor finds the most relevant sentence set a186 from the system output .</sentence>
				<definiendum id="0">human assessor</definiendum>
				<definiens id="0">finds the most relevant sentence set a186 from the system output</definiens>
			</definition>
			<definition id="8">
				<sentence>ROUGE-N ROUGE-N is an N-gram-based evaluation measure defined as follows ( Lin , 2004b ) : ROUGE-Na59a61a146a31a62a90a147a49a65a68a67 a215 a77a83a216 a209a68a217a61a173 a172a27a218 a77 a215 a219a27a220a158a221a183a222a85a223 a172 a173a78a224a76a225a164a226 a59a136a227a158a228 a152a130a150a104a229 a65 a215 a77a29a216 a209a68a217a76a173 a172 a218 a77 a215 a219a27a220a159a221a183a222a85a223 a59a136a227a158a228 a152a130a150 a229 a65 ( 10 ) Here , a230a66a231a37a232a21a233a27a234a118a28a78a235a37a236a25a237a37a238a11a239a168a36 is the number of an N-gram and a230a66a231a37a232a21a233a27a234 a196 a197a29a240a98a241a243a242a244a28a78a235a37a236a25a237a37a238a49a239a168a36 denotes the number of ngram co-occurrences in a system output and the reference .</sentence>
				<definiendum id="0">ROUGE-N ROUGE-N</definiendum>
				<definiendum id="1">a230a66a231a37a232a21a233a27a234a118a28a78a235a37a236a25a237a37a238a11a239a168a36</definiendum>
				<definiendum id="2">a196 a197a29a240a98a241a243a242a244a28a78a235a37a236a25a237a37a238a49a239a168a36</definiendum>
				<definiens id="0">an N-gram-based evaluation measure defined as follows ( Lin , 2004b ) : ROUGE-Na59a61a146a31a62a90a147a49a65a68a67 a215 a77a83a216 a209a68a217a61a173 a172a27a218 a77 a215 a219a27a220a158a221a183a222a85a223 a172 a173a78a224a76a225a164a226 a59a136a227a158a228 a152a130a150a104a229 a65 a215 a77a29a216 a209a68a217a76a173 a172 a218 a77 a215 a219a27a220a159a221a183a222a85a223 a59a136a227a158a228 a152a130a150 a229 a65 ( 10 ) Here ,</definiens>
				<definiens id="1">the number of an N-gram and a230a66a231a37a232a21a233a27a234</definiens>
			</definition>
			<definition id="9">
				<sentence>ROUGE-S ROUGE-S is an extension of ROUGE-2 defined as follows ( Lin , 2004b ) : ROUGE-Sa59a61a146a31a62a98a147a49a65a68a67 a59a68a101a161a128a104a162 a2 a65a161a163 a157 a134a61a135a93a245a246 a2 a59a61a146a31a62a98a147a49a65a161a163 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a164a147a49a65 a157 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a51a128a104a162 a2 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a98a147a49a65 ( 11 ) Where a166a168a169a78a170a248a247a250a249 a26 and a171a138a169a90a170a158a247a250a249 a26 are defined as follows : a251 a134a61a135a89a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a248a253a85a254a255 a1 a59a61a146a31a62a90a147a49a65 # of skip bigram a2a23a147 ( 12 ) a3 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a83a253a118a254a255 a1 a59a61a146a31a62a90a147a49a65 # of skip bigram a2 a146 ( 13 ) Here , function Skip2 returns the number of skipbi-grams that are common to a141 and a139 .</sentence>
				<definiendum id="0">ROUGE-S ROUGE-S</definiendum>
				<definiens id="0">an extension of ROUGE-2 defined as follows ( Lin , 2004b ) : ROUGE-Sa59a61a146a31a62a98a147a49a65a68a67 a59a68a101a161a128a104a162 a2 a65a161a163 a157 a134a61a135a93a245a246 a2 a59a61a146a31a62a98a147a49a65a161a163 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a164a147a49a65 a157 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a51a128a104a162 a2 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a98a147a49a65 ( 11 ) Where a166a168a169a78a170a248a247a250a249 a26 and a171a138a169a90a170a158a247a250a249 a26 are defined as follows : a251 a134a61a135a89a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a248a253a85a254a255 a1 a59a61a146a31a62a90a147a49a65 # of skip bigram a2a23a147 ( 12 ) a3 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a83a253a118a254a255 a1 a59a61a146a31a62a90a147a49a65 # of skip bigram a2 a146 ( 13 ) Here , function Skip2 returns the number of skipbi-grams that are common to a141 and a139</definiens>
			</definition>
			<definition id="10">
				<sentence>ROUGE-SU ROUGE-SU is an extension of ROUGE-S , which includes unigrams as a feature defined as follows ( Lin , 2004b ) : ROUGE-SUa59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a117a163 a157 a134a5a4 a59a61a146a31a62a98a147a49a65a71a163 a145 a134a6a4 a59a61a146a31a62a98a147a49a65 a157 a134a5a4 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145 a134a5a4 a59a61a146a31a62a164a147a49a65 ( 14 ) Where a166 a169a8a7 and a171 a169a8a7 are defined as follows : a251 a134a5a4 a59a61a146a31a62a98a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 ( # of skip bigrams + # of unigrams ) a2 a147 ( 15 ) a3 a134a5a4 a59a61a146a31a62a90a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 ( # of skip bigrams + # of unigrams ) a2 a146 ( 16 ) Here , function SU returns the number of skip-bigrams and unigrams that are common to a141 and a139 .</sentence>
				<definiendum id="0">ROUGE-SU ROUGE-SU</definiendum>
				<definiens id="0">an extension of ROUGE-S , which includes unigrams as a feature defined as follows ( Lin , 2004b ) : ROUGE-SUa59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a117a163 a157 a134a5a4 a59a61a146a31a62a98a147a49a65a71a163 a145 a134a6a4 a59a61a146a31a62a98a147a49a65 a157 a134a5a4 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145 a134a5a4 a59a61a146a31a62a164a147a49a65 ( 14 ) Where a166 a169a8a7 and a171 a169a8a7 are defined as follows : a251 a134a5a4 a59a61a146a31a62a98a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 ( # of skip bigrams + # of unigrams ) a2 a147 ( 15 ) a3 a134a5a4 a59a61a146a31a62a90a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 ( # of skip bigrams + # of unigrams ) a2 a146 ( 16 ) Here , function SU returns the number of skip-bigrams and unigrams that are common to a141 and a139</definiens>
			</definition>
			<definition id="11">
				<sentence>ROUGE-L ROUGE-L is an LCS-based evaluation measure defined as follows ( Lin , 2004b ) : ROUGE-La59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a161a163 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a161a163 a145a12a10 a225a90a134 a59a61a146a31a62a98a147a49a65 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145a12a10 a225a98a134 a59a61a146a31a62a90a147a49a65 ( 17 ) where a166a14a13a250a241a132a169 and a171a15a13a250a241a130a169 are defined as follows : a157a11a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a91 a16 a75 a77a29a216 LCSa17a244a59a61a156 a88 a62a90a146a21a65 ( 18 ) a145a18a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a95 a16 a75a78a77a83a216 LCSa17 a59a61a156a34a88a78a62a98a146a21a65 ( 19 ) Here , LCSa19a244a28a78a144a183a114a93a32a93a139a102a36 is the LCS score of the union longest common subsequence between reference sentences a144a25a114 and a139 .</sentence>
				<definiendum id="0">ROUGE-L ROUGE-L</definiendum>
				<definiendum id="1">LCSa19a244a28a78a144a183a114a93a32a93a139a102a36</definiendum>
				<definiens id="0">an LCS-based evaluation measure defined as follows ( Lin , 2004b ) : ROUGE-La59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a161a163 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a161a163 a145a12a10 a225a90a134 a59a61a146a31a62a98a147a49a65 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145a12a10 a225a98a134 a59a61a146a31a62a90a147a49a65 ( 17 ) where a166a14a13a250a241a132a169 and a171a15a13a250a241a130a169 are defined as follows : a157a11a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a91 a16 a75 a77a29a216 LCSa17a244a59a61a156 a88 a62a90a146a21a65 ( 18 ) a145a18a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a95 a16 a75a78a77a83a216 LCSa17 a59a61a156a34a88a78a62a98a146a21a65 ( 19 ) Here ,</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>To represent semantic relations , we use an ontology ( the ACE 2004 relation ontology ) that describes 7 main types of relations between entities and 23 subtypes ( Table 1 ) .2 These relations prove to be more reliable guides for coreference than simple lexical context or even tests for the semantic compatibility of heads and modifiers .</sentence>
				<definiendum id="0">ontology</definiendum>
				<definiens id="0">the ACE 2004 relation ontology ) that describes 7 main types of relations between entities and 23 subtypes ( Table 1 ) .2 These relations prove to be more reliable guides for coreference than simple lexical context or even tests for the semantic compatibility of heads and modifiers</definiens>
			</definition>
			<definition id="1">
				<sentence>Both the English and the Chinese coreference models incorporate features representing agreement of various kinds between noun phrases ( number , gender , humanness ) , degree of string similarity , synonymy between noun phrase heads , measures of distance between noun phrases ( such as the number of intervening sentences ) , the presence or absence of determiners or quantifiers , and a wide variety of other properties .</sentence>
				<definiendum id="0">Chinese coreference models</definiendum>
				<definiens id="0">the number of intervening sentences ) , the presence or absence of determiners or quantifiers</definiens>
			</definition>
			<definition id="2">
				<sentence>Each training / test example consists of the pair of mentions and the sequence of intervening words .</sentence>
				<definiendum id="0">test example</definiendum>
				<definiens id="0">consists of the pair of mentions and the sequence of intervening words</definiens>
			</definition>
			<definition id="3">
				<sentence>Our nominal mention tagger ( noun phrase chunker ) is a maximum entropy tagger trained on treebanks from the University of Pennsylvania .</sentence>
				<definiendum id="0">nominal mention tagger ( noun phrase chunker )</definiendum>
				<definiens id="0">a maximum entropy tagger trained on treebanks from the University of Pennsylvania</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>Otherwise , answer-string is a string containing precisely an answer to the question , and doc-id is the id of a document in the collection that supports answer-string as an answer .</sentence>
				<definiendum id="0">answer-string</definiendum>
				<definiendum id="1">doc-id</definiendum>
				<definiens id="0">a string containing precisely an answer to the question</definiens>
			</definition>
			<definition id="1">
				<sentence>An information nugget is an atomic piece of information about the target that is interesting ( in the assessor’s opinion ) and is not part of an earlier question in the series or an answer to an earlier question in the series .</sentence>
				<definiendum id="0">information nugget</definiendum>
				<definiens id="0">interesting ( in the assessor’s opinion</definiens>
			</definition>
			<definition id="2">
				<sentence>The main purpose of evaluations such as TREC is to provide system builders with the information needed to improve their systems .</sentence>
				<definiendum id="0">TREC</definiendum>
				<definiens id="0">to provide system builders with the information needed to improve their systems</definiens>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>To extract copulas , SERIF traverses the parse trees of the sentences and extracts copulas based on rules .</sentence>
				<definiendum id="0">SERIF</definiendum>
				<definiens id="0">traverses the parse trees of the sentences and extracts copulas based on rules</definiens>
			</definition>
			<definition id="1">
				<sentence>Relations used for Who-Is questions ROLE/MANAGEMENT , ROLE/GENERAL-STAFF , ROLE/CITIZEN-OF , ROLE/FOUNDER , ROLEWNER , AT/RESIDENCE , SOC/SPOUSE , SOC/PARENT , ROLE/MEMBER , SOCTHER-PROFESSIONAL Relation used for What-Is questions AT/BASED-IN , AT/LOCATED , PART/PART-OF Table 1 : Relations used in our system Many relevant sentences do not contain the query key words .</sentence>
				<definiendum id="0">Relations</definiendum>
				<definiens id="0">Relations used in our system Many relevant sentences do not contain the query key words</definiens>
			</definition>
			<definition id="2">
				<sentence>A manual pattern is a commonly used linguistic expression that specifies aliases , super/subclass and membership relations of a term ( Xu et al. , 2004 ) .</sentence>
				<definiendum id="0">manual pattern</definiendum>
			</definition>
			<definition id="3">
				<sentence>Run Run description Rouge ( 1 ) Baseline 0.3399 ( 2 ) ( 1 ) + manual patterns 0.3657 ( 3 ) Learned patterns 0.3549 ( 4 ) ( 1 ) + learned patterns 0.3860 ( 5 ) ( 2 ) + learned patterns 0.4026 Table 3 : Results on Who-is ( Biographical ) Questions Run Run description Rouge ( 1 ) Baseline 0.2126 ( 2 ) ( 1 ) + manual patterns 0.2153 ( 3 ) Learned patterns 0.2117 ( 4 ) ( 1 ) + learned patterns 0.2167 ( 5 ) ( 2 ) + learned patterns 0.2201 Table 4 : Results on “What-is” ( Other Definitional ) Questions The third question is how much annotation is needed for a pattern based system to achieve good performance .</sentence>
				<definiendum id="0">“What-is”</definiendum>
				<definiens id="0">how much annotation is needed for a pattern based system to achieve good performance</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>RavenClaw is a dialog management framework for task-oriented spoken dialog systems .</sentence>
				<definiendum id="0">RavenClaw</definiendum>
				<definiens id="0">a dialog management framework for task-oriented spoken dialog systems</definiens>
			</definition>
			<definition id="1">
				<sentence>A Dialog Task Specification consists of a tree of dialog agents , where each agent manages a subpart of the interaction .</sentence>
				<definiendum id="0">Dialog Task Specification</definiendum>
				<definiens id="0">consists of a tree of dialog agents , where each agent manages a subpart of the interaction</definiens>
			</definition>
			<definition id="2">
				<sentence>Subsequently , the engine repeatedly takes the agent on the top of the stack and executes it .</sentence>
				<definiendum id="0">engine repeatedly</definiendum>
			</definition>
			<definition id="3">
				<sentence>During an Input Phase , the system assembles the expectation agenda , which captures what the system expects to hear from the user in a given turn .</sentence>
				<definiendum id="0">expectation agenda</definiendum>
				<definiens id="0">captures what the system expects to hear from the user in a given turn</definiens>
			</definition>
			<definition id="4">
				<sentence>The responsibility for handling potential understanding errors1 is delegated to the Error Handling Process which runs in the Dialog Engine ( see Figure 2 ) .</sentence>
				<definiendum id="0">Engine</definiendum>
				<definiens id="0">delegated to the Error Handling Process which runs in the Dialog</definiens>
			</definition>
			<definition id="5">
				<sentence>Finally , Vera is a phone-based taskable agent that can be instructed to deliver messages to a third party , make wake-up calls , etc .</sentence>
				<definiendum id="0">Vera</definiendum>
			</definition>
			<definition id="6">
				<sentence>The Let’s Go Bus Information system uses the YouCanSay and FullHelp strategies .</sentence>
				<definiendum id="0">Let’s Go Bus Information system</definiendum>
			</definition>
</paper>

		<paper id="1083">
			<definition id="0">
				<sentence>660 Our coreference system uses a binary entity-mention model PL ( je , m ) ( henceforth link model ) to score the action of linking a mention m to an entity e. In our implementation , the link model is computed as PL ( L = 1je , m ) max mprime∈e ˆPL ( L = 1je , mprime , m ) , ( 1 ) where mprime is one mention in entity e , and the basic model building block ˆPL ( L = 1je , mprime , m ) is an exponential or maximum entropy model ( Berger et al. , 1996 ) : ˆPL ( Lje , mprime , m ) = exp braceleftbig summationtext i λigi ( e , m prime , m , L ) bracerightbig Z ( e , mprime , m ) , ( 2 ) where Z ( e , mprime , m ) is a normalizing factor to ensure that ˆPL ( je , mprime , m ) is a probability , fgi ( e , mprime , m , L ) g are features and fλig are feature weights .</sentence>
				<definiendum id="0">mprime</definiendum>
				<definiendum id="1">L ) bracerightbig Z</definiendum>
				<definiens id="0">an exponential or maximum entropy model ( Berger et al. , 1996 ) : ˆPL ( Lje , mprime , m ) = exp braceleftbig summationtext i λigi ( e , m prime , m ,</definiens>
				<definiens id="1">a probability , fgi ( e , mprime , m</definiens>
			</definition>
			<definition id="1">
				<sentence>Given two mentions m1 and m2 in a sentence , we compute the following dependency features : ( 1 ) same head ( m1 , m2 ) : The feature compares the bilexical dependencies hm1 , h ( m1 ) i , and hm2 , h ( m2 ) i , where h ( x ) is the head word which x modi es .</sentence>
				<definiendum id="0">h ( x )</definiendum>
				<definiens id="0">the head word which x modi es</definiens>
			</definition>
			<definition id="2">
				<sentence>Apposition is a phenomenon where two adjacent NPs refer to the same entity , as Jimmy Carter and the former president in the following example : ( II ) Jimmy Carter , the former president of US , is visiting Europe .</sentence>
				<definiendum id="0">Apposition</definiendum>
				<definiens id="0">a phenomenon where two adjacent NPs refer to the same entity</definiens>
			</definition>
			<definition id="3">
				<sentence>The ACE-Value is an entity-based metric computed by subtracting a normalized cost from 1 ( so it is unbounded below ) .</sentence>
				<definiendum id="0">ACE-Value</definiendum>
			</definition>
			<definition id="4">
				<sentence>A pronouns event is de ned as a tuple of training instance ( e , m1 , m2 ) where m1 is a mention in entity e , and the second mention m2 is a pronoun .</sentence>
				<definiendum id="0">m1</definiendum>
				<definiens id="0">a mention in entity e</definiens>
				<definiens id="1">a pronoun</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>Performance on IME is measured in terms of the character error rate ( CER ) , which is the number of characters wrongly converted from the phonetic string divided by the number of characters in the correct transcript .</sentence>
				<definiendum id="0">CER</definiendum>
				<definiens id="0">measured in terms of the character error rate</definiens>
			</definition>
			<definition id="1">
				<sentence>The MSR training algorithm is cast as a multidimensional function optimization approach ( Press et al. 1992 ) : taking the feature vector as a set of directions ; the first direction ( i.e. feature ) is selected and the objective function ( i.e. sample risk ) is minimized along that direction using a line search ; then from there along the second direction to its minimum , and so on , cycling through the whole set of directions as many times as necessary , until the objective function stops decreasing .</sentence>
				<definiendum id="0">objective function</definiendum>
				<definiens id="0">taking the feature vector as a set of directions</definiens>
			</definition>
			<definition id="2">
				<sentence>Then the effectiveness of f d , denoted by E ( f d ) , is given by ) ) SR ( ) ( SR ( max ) SR ( ) SR ( ) ( 00 ... 1 , 00 ii Dif dd d fff fff fE i λ λ +− +− = = , ( 5 ) where the denominator is a normalization term to ensure that E ( f ) ∈ [ 0 , 1 ] .</sentence>
				<definiendum id="0">SR ( ) ( SR</definiendum>
				<definiens id="0">( max ) SR ( ) SR</definiens>
			</definition>
			<definition id="3">
				<sentence>Now , similar to ( Theodoridis and Koutroumbas 2003 ) , the feature selection procedure consists of the following steps , where f i denotes any selected feature and f j denotes any candidate feature to be selected .</sentence>
				<definiendum id="0">feature selection procedure</definiendum>
				<definiens id="0">consists of the following steps , where f i denotes any selected feature and f j denotes any candidate feature to be selected</definiens>
			</definition>
			<definition id="4">
				<sentence>Select the second feature f according to { } ) ,1 ( ) 1 ( ) ( maxarg* ... 2 jCfEj j Dj αα −−= = where α is the weight that determines the relative importance we give to the two terms .</sentence>
				<definiendum id="0">α</definiendum>
				<definiens id="0">the weight that determines the relative importance</definiens>
			</definition>
			<definition id="5">
				<sentence>In particular , MSR outperforms MLE by more than 20 % CER reduction .</sentence>
				<definiendum id="0">MSR</definiendum>
				<definiens id="0">outperforms MLE by more than 20 % CER reduction</definiens>
			</definition>
			<definition id="6">
				<sentence>We used four adaptation domains : Yomiuri ( newspaper corpus ) , TuneUp ( balanced corpus containing newspapers and other sources of text ) , Encarta ( encyclopedia ) and Shincho ( collection of novels ) .</sentence>
				<definiendum id="0">TuneUp (</definiendum>
				<definiendum id="1">Encarta ( encyclopedia</definiendum>
				<definiens id="0">balanced corpus containing newspapers and other sources of text )</definiens>
			</definition>
			<definition id="7">
				<sentence>Our implementation takes the form of linear interpolation as P ( w i |h ) = λ P b ( w i |h ) + ( 1-λ ) P a ( w i |h ) , where P b is the probability of the background model , P a is the probability trained on adaptation data using MLE and the history h corresponds to two preceding words ( i.e. P b and P a are trigram probabilities ) .</sentence>
				<definiendum id="0">P b</definiendum>
				<definiendum id="1">P a</definiendum>
				<definiens id="0">the probability of the background model</definiens>
			</definition>
			<definition id="8">
				<sentence>MSR uses an alternative approach .</sentence>
				<definiendum id="0">MSR</definiendum>
				<definiens id="0">uses an alternative approach</definiens>
			</definition>
			<definition id="9">
				<sentence>We show that MSR is a very successful discriminative training algorithm for LM .</sentence>
				<definiendum id="0">MSR</definiendum>
				<definiens id="0">a very successful discriminative training algorithm for LM</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>Language models play an important role in many applications like character and speech recognition , machine translation and information retrieval .</sentence>
				<definiendum id="0">Language models</definiendum>
				<definiens id="0">an important role in many applications like character and speech recognition , machine translation and information retrieval</definiens>
			</definition>
			<definition id="1">
				<sentence>The inputs to the neural network are the indices of the n−1 previous words in the vocabulary hj = wj−n+1 , ... , wj−2 , wj−1 and the outputs are the posterior probabilities of all words of the vocabulary : P ( wj = i|hj ) ∀i ∈ [ 1 , N ] ( 1 ) where N is the size of the vocabulary .</sentence>
				<definiendum id="0">outputs</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">the indices of the n−1 previous words in the vocabulary hj = wj−n+1 , ... , wj−2 , wj−1 and the</definiens>
				<definiens id="1">the size of the vocabulary</definiens>
			</definition>
			<definition id="2">
				<sentence>P is the size of one projection and H and N is the size of the hidden and output layer respectively .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">the size of one projection</definiens>
				<definiens id="1">the size of the hidden and output layer respectively</definiens>
			</definition>
			<definition id="3">
				<sentence>Training is performed with the standard back-propagation algorithm minimizing the following error function : E = Nsummationdisplay i=1 ti logpi + β ( summationdisplay jl m2jl +summationdisplay ij v2ij ) ( 5 ) where ti denotes the desired output , i.e. , the probability should be 1.0 for the next word in the training 202 sentence and 0.0 for all the other ones .</sentence>
				<definiendum id="0">ti</definiendum>
				<definiens id="0">the desired output</definiens>
			</definition>
			<definition id="4">
				<sentence>The complexity to calculate one probability with this basic version of the neural network LM is quite high : O = ( n−1 ) ×P ×H + H + H ×N + N ( 6 ) where P is the size of one projection and H and N is the size of the hidden and output layer respectively .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the size of one projection and H and N is the size of the hidden and output layer respectively</definiens>
			</definition>
			<definition id="5">
				<sentence>The speech features consist of 39 cepstral parameters derived from a Mel frequency spectrum estimated on the 0-8kHz band ( or 0-3.8kHz for telephone data ) every 10ms .</sentence>
				<definiendum id="0">speech features</definiendum>
				<definiens id="0">consist of 39 cepstral parameters derived from a Mel frequency spectrum estimated on the 0-8kHz band ( or 0-3.8kHz for telephone data ) every 10ms</definiens>
			</definition>
			<definition id="6">
				<sentence>Best results were obtained when taking 10 % of the commercial + Train network for one epoch Repeat Select training data : Use all acoustic transcriptions ( 4M words ) Extract random subset of examples from the large corpora Shuffle data ( performing weight updates after each example ) + Test performance on development data Until convergence Figure 4 : Training algorithm for large corpora 206 Back-off LM Neural Network LM Training data [ # words ] 600M 4M 22M 92.5M∗ 600M∗ Training time [ h/epoch ] 2h40 14h 9h40 12h 3 × 12h Perplexity ( NN LM alone ) 103.0 97.5 84.0 80.0 76.5 Perplexity ( interpolated LMs ) 70.2 67.6 67.9 66.7 66.5 65.9 Word error rate ( interpolated LMs ) 14.24 % 14.02 % 13.88 % 13.81 % 13.75 % 13.61 % ∗ By resampling different random parts at the beginning of each epoch .</sentence>
				<definiendum id="0">Perplexity</definiendum>
				<definiens id="0">the commercial + Train network for one epoch Repeat Select training data : Use all acoustic transcriptions ( 4M words ) Extract random subset of examples from the large corpora Shuffle data ( performing weight updates after each example ) + Test performance on development data Until convergence Figure</definiens>
			</definition>
</paper>

		<paper id="1108">
			<definition id="0">
				<sentence>In addition , we are given the semantic annotation of the source sentences from which we can directly read off the source semantic role assignment as : R→2Es , where R is the set of semantic roles .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">given the semantic annotation of the source sentences from</definiens>
				<definiens id="1">the set of semantic roles</definiens>
			</definition>
			<definition id="1">
				<sentence>Let yieldc ( c ) denote the set of tokens in the yield of c that are content words , then : owc ( cs , ct ) = | ( S ts∈yieldc ( cs ) al ( ts ) ) ∩yieldc ( ct ) | |yieldc ( ct ) | ( 7 ) Constituent alignment .</sentence>
				<definiendum id="0">Let yieldc ( c ) denote</definiendum>
				<definiens id="0">the set of tokens in the yield of c that are content words , then : owc ( cs , ct ) = | ( S ts∈yieldc ( cs ) al ( ts ) ) ∩yieldc ( ct ) | |yieldc ( ct ) | ( 7 ) Constituent alignment</definiens>
			</definition>
</paper>

	</volume>
