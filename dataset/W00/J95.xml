<?xml version="1.0" encoding="UTF-8"?>
	<volume id="J95">

		<paper id="2002">
			<definition id="0">
				<sentence>In their probabilistic version , which defines a language as a probability distribution over strings , they have been used in a variety of applications : for the selection of parses for ambiguous inputs ( Fujisaki et al. 1991 ) ; to guide the rule choice efficiently during parsing ( Jones and Eisner 1992 ) ; to compute island probabilities for non-linear parsing ( Corazza et al. 1991 ) .</sentence>
				<definiendum id="0">probabilistic version</definiendum>
				<definiens id="0">defines a language as a probability distribution over strings , they have been used in a variety of applications : for the selection of parses for ambiguous inputs ( Fujisaki et al. 1991</definiens>
			</definition>
			<definition id="1">
				<sentence>What is the probability that x occurs as a prefix of some string generated by G ( the prefix probability of x ) ?</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">the probability that x occurs as a prefix of some string generated by</definiens>
			</definition>
			<definition id="2">
				<sentence>The input string is denoted by x. Ix\ [ is the length of x. Individual input symbols are identified by indices starting at 0 : x0 , xl ... .. Xlxl_ 1 .</sentence>
				<definiendum id="0">input string</definiendum>
				<definiens id="0">the length of x. Individual input symbols</definiens>
			</definition>
			<definition id="3">
				<sentence>S , where S is the sentence nonterminal ( note the empty left-hand side ) .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">the sentence nonterminal ( note the empty left-hand side )</definiens>
			</definition>
			<definition id="4">
				<sentence>the right of the entire RHS ) are the result of a completion or scanning step , but completion also produces states that are not yet complete .</sentence>
				<definiendum id="0">RHS</definiendum>
				<definiens id="0">the result of a completion or scanning step , but completion also produces states that are not yet complete</definiens>
			</definition>
			<definition id="5">
				<sentence>Definition 1 The following quantities are defined relative to a SCFG G , a nonterminal X , and a string x over the alphabet y~ of G. a ) The probability of a ( partial ) derivation/71 ~/72 ~ `` '' /Tk is inductively defined by 1 ) 2 ) P ( /71 ) = 1 P ( /71 ~ `` '' ~ /Tk ) = P ( X -- * , X ) P ( u2 ~ ... ~/Tk ) , b ) c ) d ) where /71 , /72 ... . , /Tk are strings of terminals and nonterminals , X ~ A is a production of G , and u2 is derived from/71 by replacing one occurrence of X with &amp; .</sentence>
				<definiendum id="0">X ~ A</definiendum>
				<definiens id="0">relative to a SCFG G , a nonterminal X , and a string x over the alphabet y~ of G. a ) The probability of a ( partial</definiens>
				<definiens id="1">P ( X -- * , X ) P ( u2 ~ ... ~/Tk ) , b ) c ) d ) where /71 , /72 ... . , /Tk are strings of terminals and nonterminals</definiens>
				<definiens id="2">a production of G , and u2 is derived from/71 by replacing one occurrence of X with &amp;</definiens>
			</definition>
			<definition id="6">
				<sentence>The string probability P ( X =g x ) ( of x given X ) is the sum of the probabilities of all left-most derivations X = &gt; ... = &gt; x producing x from X. s The sentence probability P ( S ~ x ) ( of x given G ) is the string probability given the start symbol S of G. By definition , this is also the probability P ( x I G ) assigned to x by the grammar G. The prefix probability P ( S g &gt; L X ) ( of X given G ) is the sum of the probabilities of all sentence strings having x as a prefix , P ( S x ) = p ( s xy ) yEY~* ( In particular , P ( S ~L e ) = 1 ) .</sentence>
				<definiendum id="0">string probability P ( X =g x )</definiendum>
				<definiendum id="1">probability P</definiendum>
				<definiens id="0">the sum of the probabilities of all left-most derivations X = &gt; ... = &gt; x producing x from X. s The sentence probability P ( S ~ x ) ( of x given G ) is the string probability given the start symbol</definiens>
			</definition>
			<definition id="7">
				<sentence>The uniqueness condition ( b ) above , which is irrelevant to the correctness of a standard Earley parser , justifies ( probabilistic ) counting of paths in lieu of derivations .</sentence>
				<definiendum id="0">uniqueness condition</definiendum>
				<definiens id="0">irrelevant to the correctness of a standard Earley parser , justifies ( probabilistic ) counting of paths in lieu of derivations</definiens>
			</definition>
			<definition id="8">
				<sentence>Definition 3 The probability P ( 7 ~ ) of a path ~ is the product of the probabilities of all rules used in the predicted states occurring in ~v. Lemma 2 a ) For all paths 7 ~ starting with a nonterminal X , P ( 7 ~ ) gives the probability of the ( partial ) derivation represented by ~v. In particular , the string probability P ( X ~ x ) is the sum of the probabilities of all paths starting with X that are complete and constrained by x. b ) The sentence probability P ( S ~ x ) is the sum of the probabilities of all complete paths starting with the initial state , constrained by x. c ) The prefix probability P ( S ~L X ) is the sum of the probabilities of all paths 7 ~ starting with the initial state , constrained by x , that end in a scanned state .</sentence>
				<definiendum id="0">prefix probability P ( S ~L X )</definiendum>
				<definiens id="0">the sum of the probabilities of all paths starting with X that are complete and constrained by x. b</definiens>
				<definiens id="1">the sum of the probabilities of all complete paths starting with the initial state</definiens>
				<definiens id="2">the sum of the probabilities of all paths 7 ~ starting with the initial state , constrained by x</definiens>
			</definition>
			<definition id="9">
				<sentence>S. ( a ) follows directly from our definitions of derivation probability , string probability , path probability , and the one-to-one correspondence between paths and derivations established by Lemma 1 .</sentence>
				<definiendum id="0">S.</definiendum>
				<definiens id="0">a ) follows directly from our definitions of derivation probability , string probability , path probability</definiens>
			</definition>
			<definition id="10">
				<sentence>Lemma 3 The following assumes an Earley chart constructed by the parser on an input string x with Ixl = l. a ) Provided that S : GL Xo ... k-lXV is a possible left-most derivation of the grammar ( for some v ) , the probability that a nonterminal X generates the substring Xk ... xi-1 can be computed as the sum P ( x xki-1 ) = Z i ( kx i : k X -- -~ ) ~ .</sentence>
				<definiendum id="0">k-lXV</definiendum>
				<definiendum id="1">Xk</definiendum>
				<definiens id="0">a possible left-most derivation of the grammar ( for some v ) , the probability that a nonterminal X generates the substring</definiens>
			</definition>
			<definition id="11">
				<sentence>The expression x += y means that x is computed incrementally as a sum of various y terms , which are computed in some order and accumulated to finally yield the value of x. 8 ( 2 ) Transitions are denoted by ~ , with predecessor states on the left language C. 175 Computational Linguistics Volume 21 , Number 2 and successor states on the right .</sentence>
				<definiendum id="0">expression x += y</definiendum>
				<definiens id="0">means that x is computed incrementally as a sum of various y terms</definiens>
			</definition>
			<definition id="12">
				<sentence>X -- *YAcG c ) d ) The relation X GL Y is defined as the reflexive , transitive closure of X ~L Y , i.e. , X ~ Y iff X = Y or there is a nonterminal Z such that X -- *L Z and Z GL Y. The probabilistic reflexive , transitive left-corner relation RL = RL ( G ) is a matrix of probability sums R ( X : GL Y ) .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">relation X GL Y</definiendum>
				<definiendum id="2">X ~L Y</definiendum>
				<definiens id="0">the reflexive , transitive closure of</definiens>
			</definition>
			<definition id="13">
				<sentence>Each R ( X ~L Y ) is defined as a series e ( x Y ) e ( x = Y ) + P ( x Y ) 4y'~ P ( X -+L Z1 ) P ( Z1 -- +L Y ) z1 + ~_ , P ( X -- -~L Z1 ) P ( Z1 -- -+L Za ) P ( Z2 -- +L Y ) Z1 , Z2 4- ... Alternatively , RL is defined by the recurrence relation R ( X ~L Y ) = 6 ( X , Y ) + Z e ( x -~L Z ) R ( Z ~ , ~ Y ) , z where we use the delta function , defined as 6 ( X , Y ) = 1 if X = Y , and 6 ( X , Y ) = 0 if X # Y. The recurrence for RL can be conveniently written in matrix notation RL = 1 4PLRL , from which the closed-form solution is derived : RL = ( I -PL ) -1 .</sentence>
				<definiendum id="0">R ( X ~L Y</definiendum>
				<definiendum id="1">RL</definiendum>
				<definiendum id="2">recurrence relation R</definiendum>
				<definiens id="0">a series e ( x Y ) e ( x = Y ) + P ( x Y</definiens>
				<definiens id="1">in matrix notation RL = 1 4PLRL , from which the closed-form solution is derived : RL = ( I -PL ) -1</definiens>
			</definition>
			<definition id="14">
				<sentence>The relation X ~ Y is defined as the reflexive , transitive closure of X ~ Y , i.e. , X G Y iff X = Y or there is a nonterminal Z such that X -- + Z and Z ~ Y. The probabilistic reflexive , transitive unit-production relation Ru = Ru ( G ) is the matrix of probability sums R ( X ~ Y ) .</sentence>
				<definiendum id="0">relation X ~ Y</definiendum>
				<definiens id="0">the reflexive , transitive closure of X ~ Y , i.e. , X G Y iff X = Y or there is a nonterminal Z such that X -- + Z and Z ~ Y. The probabilistic reflexive , transitive unit-production relation Ru = Ru ( G ) is the matrix of probability sums R ( X ~ Y )</definiens>
			</definition>
			<definition id="15">
				<sentence>Each R ( X ~ Y ) is defined as a series e ( x Y ) P ( x = Y ) + P ( X -- * Y ) q~~ P ( X -- .4 Zl ) P ( Zl -- 4 Y ) z1 12 Even with null productions , these would not be used for Earley transitions ; see Section 4.7 .</sentence>
				<definiendum id="0">R ( X ~ Y</definiendum>
				<definiens id="0">a series e ( x Y ) P ( x = Y ) + P ( X -- * Y ) q~~ P ( X -- .4 Zl</definiens>
			</definition>
			<definition id="16">
				<sentence>Completion ( probabilistic , transitive ) .</sentence>
				<definiendum id="0">Completion</definiendum>
				<definiens id="0">probabilistic , transitive )</definiens>
			</definition>
			<definition id="17">
				<sentence>The probability ex can be seen as the precomputed inner probability of an expansion of X to the empty string ; i.e. , it sums the probabilities of all Earley paths that derive c from X. This is the justification for the way these probabilities can be used in modified prediction and completion steps , described next .</sentence>
				<definiendum id="0">probability ex</definiendum>
				<definiens id="0">the precomputed inner probability of an expansion of X to the empty string</definiens>
			</definition>
			<definition id="18">
				<sentence>185 Computational Linguistics Volume 21 , Number 2 Definition 7 A Viterbi parse for a string x , in a grammar G , is a left-most derivation that assigns maximal probability to x , among all possible derivations for x. Both the definition of Viterbi parse and its computation are straightforward generalizations of the corresponding notion for Hidden Markov Models ( Rabiner and Juang 1986 ) , where one computes the Viterbi path ( state sequence ) through an HMM .</sentence>
				<definiendum id="0">Viterbi path</definiendum>
				<definiens id="0">a left-most derivation that assigns maximal probability to x , among all possible derivations for x. Both the definition of Viterbi parse and its computation are straightforward generalizations of the corresponding notion for Hidden Markov Models ( Rabiner and Juang 1986 )</definiens>
			</definition>
			<definition id="19">
				<sentence>M-step : Reset the parameters so as to maximize the likelihood relative to the expected rule counts found in the E-step .</sentence>
				<definiendum id="0">M-step</definiendum>
				<definiens id="0">Reset the parameters so as to maximize the likelihood relative to the expected rule counts found in the E-step</definiens>
			</definition>
			<definition id="20">
				<sentence>After that , the Mstep consists of a simple normalization of these counts to yield the new production probabilities .</sentence>
				<definiendum id="0">Mstep</definiendum>
				<definiens id="0">consists of a simple normalization of these counts to yield the new production probabilities</definiens>
			</definition>
			<definition id="21">
				<sentence>187 Computational Linguistics Volume 21 , Number 2 Definition 8 Given a string x , Ix\ ] = l , the outer probability fli ( kX -- -* A. # ) of an Earley state is the sum of the probabilities of all paths that .</sentence>
				<definiendum id="0">Earley state</definiendum>
			</definition>
			<definition id="22">
				<sentence>The probability of the surrounding of Y ( fl '' ) is the probability of the surrounding of X ( fl ) , plus the choice of the rule of production for X and the expansion of the partial LHS A , which are together given by ~ , ' .</sentence>
				<definiendum id="0">LHS A</definiendum>
				<definiens id="0">the probability of the surrounding of X ( fl ) , plus the choice of the rule of production for X and the expansion of the partial</definiens>
			</definition>
			<definition id="23">
				<sentence>G has no useless nonterminals iff all nonterminals X appear in at least one derivation of some string x c G* with nonzero probability , i.e. , P ( S : ~ AX # =~ x ) &gt; O. It is useful to translate consistency into `` process '' terms .</sentence>
				<definiendum id="0">G</definiendum>
				<definiens id="0">has no useless nonterminals iff all nonterminals X appear in at least one derivation of some string x c G* with nonzero probability , i.e. , P ( S : ~ AX # =~ x ) &gt; O. It is useful to translate consistency into `` process '' terms</definiens>
			</definition>
			<definition id="24">
				<sentence>Proof Entry ( X , Y ) in the left-corner matrix PL is the probability Of generating Y as the immediately succeeding left-corner below X. Similarly , entry ( X , Y ) in the nth power P~ is the probability of generating Y as the left-corner of X with n 1 intermediate nonterminals .</sentence>
				<definiendum id="0">power P~</definiendum>
				<definiens id="0">the probability Of generating Y as the immediately succeeding left-corner below X. Similarly , entry ( X , Y ) in the nth</definiens>
				<definiens id="1">the left-corner of X with n 1 intermediate nonterminals</definiens>
			</definition>
			<definition id="25">
				<sentence>Then R can be computed as R = I+ ( I+p+p2+ ... ) P = I + ( I ' + P ' + p,2 + ... ) , p = I+ ( I'-P ' ) -I , P = I+R ' , P. Here R ' is the inverse of I ' P ' , and * denotes a matrix multiplication in which the left operand is first augmented with zero elements to match the dimensions of the right operand , P. The speedups obtained with this technique can be substantial .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the inverse of I ' P ' , and * denotes a matrix multiplication in which the left operand is first augmented with zero elements to match the dimensions of the right operand</definiens>
			</definition>
			<definition id="26">
				<sentence>It can be computed as the product RLT = RLPLT where PLT has a nonzero entry at ( X , a ) iff there is a production for nonterminal X that starts with terminal a. RL is the old left-corner relation .</sentence>
				<definiendum id="0">PLT</definiendum>
				<definiendum id="1">a. RL</definiendum>
				<definiens id="0">the old left-corner relation</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>Compare , in detail , the output of the system with the text found in the corpus , differentiating between the predictions concerning text that was specifically used in the analysis ( the training set ) and text that was not ( the testing set ) .</sentence>
				<definiendum id="0">Compare</definiendum>
				<definiens id="0">in detail , the output of the system with the text found in the corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>Definitions of relations of this sort specify constraints that apply to the nucleus ( N ) , the satellite ( S ) , and the combination of the two and specify the effects of the expression .</sentence>
				<definiendum id="0">N )</definiendum>
				<definiens id="0">the satellite ( S ) , and the combination of the two and specify the effects of the expression</definiens>
			</definition>
			<definition id="2">
				<sentence>Finally , RST has been used by many researchers for the purpose of text generation ( e.g. , Moore and Paris 1988 ; Hovy and McCoy 1989 ; Scott and Souza 1990 ; R6sner 33 Computational Linguistics Volume 21 , Number 1 Precondition \ [ ( 1 ) Instruct ~ 1 ( 2 ) Remove ~uence ( 3 ) Grasp ( 4 ) Pull Purpose 151 Return ( 6 ) Place Figure 1 The RST representation of the Remove-Phone text .</sentence>
				<definiendum id="0">RST</definiendum>
				<definiens id="0">RST representation of the Remove-Phone text</definiens>
			</definition>
			<definition id="3">
				<sentence>The current study has used the two aspects of the RST specification that can be mapped onto the procedural structure of the process being expressed , namely , the hierarchical structure of RST and the subset of RST relations that correspond to procedural relations .</sentence>
				<definiendum id="0">RST specification</definiendum>
				<definiens id="0">the hierarchical structure of RST and the subset of RST relations that correspond to procedural relations</definiens>
			</definition>
			<definition id="4">
				<sentence>The RST representation of the Remove-Phone text is a small portion of the full hierarchy that represents the entire manual .</sentence>
				<definiendum id="0">RST representation of the Remove-Phone text</definiendum>
				<definiens id="0">a small portion of the full hierarchy that represents the entire manual</definiens>
			</definition>
			<definition id="5">
				<sentence>PRECONDITION ( taken from R6sner and Stede 1992a ) constraints on N : presents an action constraints on S : presents an unrealized situation constraints on the N+S combination : S must be realized in order to make it possible or sensible to carry out N the effect : R recognizes that situation S must be realized in order to successfully carry out action N The Precondition relation is a simple amalgam of the standard RST relations Circumstance and Condition .</sentence>
				<definiendum id="0">PRECONDITION</definiendum>
				<definiendum id="1">Precondition relation</definiendum>
				<definiens id="0">a simple amalgam of the standard RST relations Circumstance and Condition</definiens>
			</definition>
			<definition id="6">
				<sentence>RESULT ( adapted from constraints on N : constraints on S : constraints on the N+S the effect : Mann and Thompson 1987 ) none presents either a volitional or non-volitional action or the situation that could have arisen from one combination : N presents a situation that could have caused the situation presented in S ; presentation of N is more central to W 's purposes in putting forth the N-S combination than is the presentation of S. R recognizes that the situation presented in N could be a cause for the action or situation presented in S The Result relation is a simple amalgam of RST 's volitional and non-volitional results .</sentence>
				<definiendum id="0">RESULT</definiendum>
				<definiendum id="1">relation</definiendum>
				<definiens id="0">constraints on the N+S the effect : Mann and Thompson 1987 ) none presents either a volitional or non-volitional action</definiens>
				<definiens id="1">a simple amalgam of RST 's volitional and non-volitional results</definiens>
			</definition>
			<definition id="7">
				<sentence>CONCURRENT ( adapted from Mann and Thompson 1987 ) constraints on N : multi-nuclear constraints on the combination of nuclei : A simultaneous relation between distinct situations is presented in the nuclei the effect : R recognizes the simultaneous relations among the nuclei Finally , the Concurrent relation is a simple extension of Sequence , referring to actions that are distinct but simultaneous .</sentence>
				<definiendum id="0">CONCURRENT</definiendum>
				<definiendum id="1">R</definiendum>
				<definiendum id="2">Concurrent relation</definiendum>
				<definiens id="0">a simple extension of Sequence , referring to actions that are distinct but simultaneous</definiens>
			</definition>
			<definition id="8">
				<sentence>The IMAGENE project employs a hypothesis generation and test cycle , such as the one advocated by Cumming ( 1990 ) in an attempt to identify correlations between the contextual features of communicative context on the one hand , and the lexical and grammatical forms on the other .</sentence>
				<definiendum id="0">IMAGENE project</definiendum>
				<definiens id="0">employs a hypothesis generation and test cycle</definiens>
			</definition>
			<definition id="9">
				<sentence>IMAGENE 'S network consists of approximately 70 systems .</sentence>
				<definiendum id="0">IMAGENE 'S network</definiendum>
				<definiens id="0">consists of approximately 70 systems</definiens>
			</definition>
			<definition id="10">
				<sentence>The Text Structure , to be discussed more fully in Section 5.3 , is represented in IMAGENE 'S Text Representation Language ( TRL ) .</sentence>
				<definiendum id="0">Text Structure</definiendum>
			</definition>
			<definition id="11">
				<sentence>A second input shown in Figure 2 , the Process Structure , is a representation of the process to be expressed .</sentence>
				<definiendum id="0">Process Structure</definiendum>
			</definition>
			<definition id="12">
				<sentence>The Sentence Builder uses the lexical information given in the Process Structure just described , to translate the Text Structure , described above , into the appropriate sentence specification to be passed to Penman for surface realization .</sentence>
				<definiendum id="0">Sentence Builder</definiendum>
				<definiens id="0">uses the lexical information given in the Process Structure just described , to translate the Text Structure , described above , into the appropriate sentence specification to be passed to Penman for surface realization</definiens>
			</definition>
			<definition id="13">
				<sentence>The first element of the form selection sub-network is Conditional-Status , which determines whether the high-level purpose being expressed has special conditions pertaining to it , such as the expressed precondition in example ( 6a ) or other conditions that restrict the applicability of the purpose , as in example ( 7a ) ( `` to wall unit from which it was taken '' ) .</sentence>
				<definiendum id="0">Conditional-Status</definiendum>
				<definiens id="0">determines whether the high-level purpose being expressed has special conditions pertaining to it , such as the expressed precondition in example ( 6a ) or other conditions that restrict the applicability of the purpose</definiens>
			</definition>
			<definition id="14">
				<sentence>When a purpose does not have conditions upon it and the scope is global , PurposeTNF marks the purpose as a to infinitive ( TNF ) .</sentence>
				<definiendum id="0">PurposeTNF</definiendum>
				<definiens id="0">marks the purpose as a to infinitive</definiens>
			</definition>
			<definition id="15">
				<sentence>This metonymy occurs in purposes in which the direct object ( or goal ) of the purpose clause is more important than the action , as in ( 9 ) For frequently busy numbers , you 'll want to use REDIAL \ [ 7\ ] , and the pause will have to be in Redial memory .</sentence>
				<definiendum id="0">metonymy</definiendum>
				<definiens id="0">occurs in purposes in which the direct object</definiens>
			</definition>
			<definition id="16">
				<sentence>FLASH uses proper timing to avoid an accidental hangup .</sentence>
				<definiendum id="0">FLASH</definiendum>
				<definiens id="0">uses proper timing to avoid an accidental hangup</definiens>
			</definition>
			<definition id="17">
				<sentence>TRL allows the Text Structure to include a representation of the hierarchical structure of the text in terms of RST , including both nucleus-satellite and multi-nuclear schemata .</sentence>
				<definiendum id="0">TRL</definiendum>
				<definiens id="0">allows the Text Structure to include a representation of the hierarchical structure of the text in terms</definiens>
			</definition>
			<definition id="18">
				<sentence>In addition , TRL specifies the textual order and clause combining using additional New-Sentence and Continue-Sentence links .</sentence>
				<definiendum id="0">TRL</definiendum>
				<definiens id="0">specifies the textual order and clause combining using additional New-Sentence and Continue-Sentence links</definiens>
			</definition>
			<definition id="19">
				<sentence>The Form feature specifies the general grammatical form .</sentence>
				<definiendum id="0">Form feature</definiendum>
				<definiens id="0">specifies the general grammatical form</definiens>
			</definition>
			<definition id="20">
				<sentence>Currently , IMAGENE uses simple algorithms for pronominalization and determiners , which are not based on a detailed corpus study of the forms and functions of the object reference domain .</sentence>
				<definiendum id="0">IMAGENE</definiendum>
				<definiens id="0">uses simple algorithms for pronominalization and determiners</definiens>
			</definition>
			<definition id="21">
				<sentence>IMAGENE , which produces five linkers and nine forms , produces a match for 67 % of the precondition expressions in the training set and 35 % in the testing set .</sentence>
				<definiendum id="0">IMAGENE</definiendum>
				<definiens id="0">produces five linkers and nine forms , produces a match for 67 % of the precondition expressions in the training set</definiens>
			</definition>
			<definition id="22">
				<sentence>`` Focusing your RST : A step toward generating coherent multisentential text . ''</sentence>
				<definiendum id="0">Focusing your RST</definiendum>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>The topic-focus articulation ( TFA ) is both expressed by grammatical means ( word order , morphemes or their clitic versus `` strong '' shapes , syntactic constructions , position of the sentence stress or `` intonation center '' ) and semantically relevant .</sentence>
				<definiendum id="0">topic-focus articulation</definiendum>
				<definiendum id="1">TFA</definiendum>
				<definiens id="0">both expressed by grammatical means ( word order , morphemes or their clitic versus `` strong '' shapes , syntactic constructions , position of the sentence stress or `` intonation center '' ) and semantically relevant</definiens>
			</definition>
			<definition id="1">
				<sentence>Topic-Focus Identification focus proper as the most dynamic element ( carrying the intonation center ) , s CD is semantically relevant for the scopes of quantifiers , as illustrated by example ( 10 ) .</sentence>
				<definiendum id="0">Topic-Focus Identification focus proper</definiendum>
				<definiens id="0">the most dynamic element ( carrying the intonation center ) , s CD is semantically relevant for the scopes of quantifiers</definiens>
			</definition>
			<definition id="2">
				<sentence>Topic-Focus Identification ( iii ) If the verb and all its immediate complementations ( in other words , all elements of the center of the sentence ) are CB , then only the NB item ( s ) embedded under the most dynamic element of the center constitutes the focus , with the rest of the sentence belonging to its topic .</sentence>
				<definiendum id="0">Topic-Focus Identification</definiendum>
				<definiens id="0">the center of the sentence ) are CB , then only the NB item ( s ) embedded under the most dynamic element of the center constitutes the focus</definiens>
			</definition>
			<definition id="3">
				<sentence>( d ) If all the complementations have been determined as t or t/f , then ( da ) if the verb was t/f after point ( ca ) and the rightmost complementation is a definite noun group , an indexical word , or pronoun , then this rightmost element gets t ( f ) , which denotes a specific kind of ambiguity : this element is to be understood as having f only in case there is no other f in the reading of the sentence ; ( db ) if ( da ) does not apply , then both the rightmost element of the sentence and its verb get t/f .</sentence>
				<definiendum id="0">rightmost complementation</definiendum>
				<definiens id="0">the reading of the sentence ; ( db ) if ( da ) does not apply , then both the rightmost element of the sentence</definiens>
			</definition>
			<definition id="4">
				<sentence>The symbol topic denotes here whether the given item belongs to the topic or to the focus , touch stores the information if the complementation has been already determined , sem is the semantic information about the verb ( general , specific , intermediate ) , and Itree and rtree are the left and right subtrees in the dependency tree .</sentence>
				<definiendum id="0">sem</definiendum>
				<definiens id="0">the left and right subtrees in the dependency tree</definiens>
			</definition>
			<definition id="5">
				<sentence>( 1 ) ( A ) a neighbor ( t/f ) gave ( t/f ) ( B ) a neighbor ( t/f ) gave ( t/f ) ( C ) a neighbor ( t/f ) gave ( t/f ) ( D ) a neighbor ( t/f ) gave ( t/f ) ( E ) the neighbor ( t ) gave ( t/f ) ( F ) the neighbor ( t ) gave ( t/f ) ( G ) the neighbor ( t ) gave ( t/f ) ( H ) the neighbor ( t ) gave ( t/f ) ( I ) a neighbor ( t/f ) gave ( t/f ) ( J ) a neighbor ( t/f ) gave ( t/f ) ( K ) the neighbor ( t ) gave ( t/f ) ( L ) the neighbor ( t ) gave ( t/f ) ( M ) a neighbor ( t/f ) gave ( t/f ) ( N ) a neighbor ( t/f ) gave ( t/f ) ( 0 ) a neighbor ( t/f ) gave ( t/f ) ( P ) a neighbor ( t/f ) gave ( t/f ) ( Q ) the neighbor ( t ) gave ( t/f ) ( R ) the neighbor ( t ) gave ( t/f ) ( S ) the neighbor ( t ) gave ( t/f ) ( T ) the neighbor ( t ) gave ( t/f ) ( U ) a neighbor ( t/f ) gave ( t/f ) ( V ) the neighbor ( t ) gave ( t/f ) ( 2 ) ( A ) ( B ) ( C ) ( D ) a boy ( t/f ) a book ( f ) a boy ( t/f ) the book ( t/f ) the boy ( t/f ) a book ( f ) the boy ( t/f ) the book ( t/f ) a boy ( t/f ) a book ( f ) a boy ( t/f ) the book ( t/f ) the boy ( t/f ) a book ( f ) the boy ( t/f ) the book ( t/f ) him ( t/f ) a book ( f ) him ( t/f ) the book ( t/f ) him ( t/f ) a book ( f ) him ( t/f ) the book ( t/f ) a book ( t ) to a boy ( f ) the book ( t ) to a boy ( f ) a book ( t ) to the boy ( f ) the book ( t ) to the boy ( f ) a book ( t ) to a boy ( f ) the book ( t ) to a boy ( f ) a book ( t ) to the boy ( f ) the book ( t ) to the boy ( f ) it ( t ) to him ( t/f ) it ( t ) to him ( t/f ) a painter ( t/f ) arrived ( t/f ) at a french village ( t ) on a nice september day ( f ) a painter ( t/f ) arrived ( t/f ) at a french village ( t/f ) yesterday ( t ( f ) ) a painter ( t/f ) arrived ( t/f ) at the french village ( t ) on a nice september day ( f ) a painter ( t/f ) arrived ( t/f ) at the french village ( t/f ) yesterday ( t ( f ) ) 92 Eva Haji~ov~i et al .</sentence>
				<definiendum id="0">G</definiendum>
				<definiendum id="1">H</definiendum>
				<definiendum id="2">K</definiendum>
				<definiendum id="3">M</definiendum>
				<definiendum id="4">t/f ) gave ( t/f )</definiendum>
				<definiendum id="5">V</definiendum>
				<definiens id="0">t/f ) ( P ) a neighbor ( t/f ) gave ( t/f ) ( Q ) the neighbor ( t ) gave ( t/f ) ( R ) the neighbor ( t ) gave ( t/f ) ( S ) the neighbor ( t ) gave ( t/f ) ( T ) the neighbor ( t ) gave ( t/f ) ( U ) a neighbor</definiens>
				<definiens id="1">A ) ( B ) ( C ) ( D ) a boy ( t/f ) a book ( f ) a boy ( t/f ) the book ( t/f ) the boy ( t/f ) a book ( f ) the boy ( t/f ) the book ( t/f ) a boy ( t/f ) a book ( f ) a boy ( t/f ) the book ( t/f ) the boy ( t/f ) a book ( f ) the boy ( t/f ) the book ( t/f ) him ( t/f ) a book ( f ) him ( t/f ) the book ( t/f ) him ( t/f ) a book ( f ) him ( t/f ) the book ( t/f ) a book ( t ) to a boy</definiens>
			</definition>
			<definition id="6">
				<sentence>Topic-Focus Identification ( E ) ( F ) ( a ) ( H ) ( I ) ( J ) ( K ) ( 3 ) ( A ) ( B ) ( C ) ( O ) ( E ) ( F ) ( G ) ( H ) ( I ) the painter ( t ) arrived ( t/f ) at a french village ( t ) on a nice september day ( f ) the painter ( t ) arrived ( t/f ) at a french village ( t/f ) yesterday ( t ( f ) ) the painter ( t ) arrived ( t/f ) at the french village ( t ) on a nice september day ( f ) the painter ( t ) arrived ( t/f ) at the french village ( t/f ) yesterday ( t ( f ) ) a painter ( t/f ) arrived ( t/f ) there ( t ) on a nice september day ( f ) a painter ( t/f ) arrived ( t/f ) there ( t/f ) yesterday ( t ( f ) ) yesterday ( t ) a painter ( t/f ) arrived ( t/f ) there ( t ( f ) ) a neighbor ( t/f ) day ( f ) a neighbor ( t/f ) a neighbor ( t/f ) day ( f ) a neighbor ( t/f ) the neighbor ( t ) day ( f ) the neighbor ( t ) the neighbor ( t ) day ( f ) the neighbor ( t ) the neighbor ( t ) met ( t/f ) a boy ( t ) on a nice september met ( t/f ) a boy ( t/f ) yesterday ( t ( f ) ) met ( t/f ) the boy ( t ) on a nice september met ( t/f ) the boy ( t/f ) yesterday ( t ( f ) ) met ( t/f ) a boy ( t ) on a nice september met ( t/f ) a boy ( t/f ) yesterday ( t ( f ) ) met ( t/f ) the boy ( t ) on a nice september met ( t/f ) the boy ( t/f ) yesterday ( t ( f ) ) met ( t/f ) him ( t/f ) yesterday ( t ( f ) ) We assume that the sentences are pronounced so that the intonation center is carried by the rightmost sentence part bearing an index f. Thus , for instance , ( 3 ) ( H ) corresponds to the following sentences : ( 3 ) ( H1 ) The neighbor MET the boy yesterday .</sentence>
				<definiendum id="0">Topic-Focus Identification</definiendum>
				<definiens id="0">a ) ( H ) ( I ) ( J ) ( K ) ( 3 ) ( A ) ( B ) ( C ) ( O ) ( E ) ( F ) ( G ) ( H ) ( I ) the painter</definiens>
				<definiens id="1">a neighbor ( t/f ) day ( f ) a neighbor ( t/f ) a neighbor ( t/f ) day ( f ) a neighbor ( t/f ) the neighbor ( t ) day ( f ) the neighbor ( t ) the neighbor ( t ) day ( f ) the neighbor ( t ) the neighbor ( t ) met ( t/f ) a boy</definiens>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>Finite-state transducers can be composed , intersected , merged with the union operation and sometimes determinized .</sentence>
				<definiendum id="0">Finite-state transducers</definiendum>
				<definiens id="0">merged with the union operation and sometimes determinized</definiens>
			</definition>
			<definition id="1">
				<sentence>We will use the following notation when pictorially describing a finite-state transducer : final states are depicted with two concentric circles ; e represents the empty string ; on a transition from state i to state j , a/b indicates a transition on input symbol a and output symbol ( s ) b ; a a question mark ( ? )</sentence>
				<definiendum id="0">e</definiendum>
				<definiens id="0">represents the empty string</definiens>
			</definition>
			<definition id="2">
				<sentence>Given a function fl that transforms , say , a into b ( i.e. fl ( a ) = b ) , we want to extend it to a function f2 such that f2 ( w ) = w / where w ' is the word built from the word w where each occurrence of a has been replaced by b. We say that f2 is the local extension 3 of fl , and we write f2 = LocExt ( fl ) .</sentence>
				<definiendum id="0">function fl</definiendum>
				<definiendum id="1">f2</definiendum>
				<definiens id="0">a into b ( i.e. fl ( a ) = b )</definiens>
				<definiens id="1">the local extension 3 of fl</definiens>
			</definition>
			<definition id="3">
				<sentence>A finite-state transducer T is a five-tuple ( ~ , Q , i , F , E ) where : G is a fnite alphabet ; Q is a finite set of states or vertices ; i c Q is the initial state ; F C Q is the set of final states ; E c Q x ( y , u { c } ) x ~ , * x Q is the set of edges or transitions .</sentence>
				<definiendum id="0">finite-state transducer T</definiendum>
				<definiendum id="1">G</definiendum>
				<definiendum id="2">Q</definiendum>
				<definiens id="0">a five-tuple ( ~ , Q , i , F , E ) where :</definiens>
				<definiens id="1">a fnite alphabet ;</definiens>
				<definiens id="2">a finite set of states or vertices ; i c Q is the initial state ; F C Q is the set of final states</definiens>
			</definition>
			<definition id="4">
				<sentence>In the example of Figure 13 , IT41 is defined by IT4i ( ah ) = bh and IT4i ( ae ) = ce .</sentence>
				<definiendum id="0">IT41</definiendum>
				<definiens id="0">ah ) = bh and IT4i ( ae ) = ce</definiens>
			</definition>
			<definition id="5">
				<sentence>This leads to the definition of subsequential transducers : a subsequential transducer T ' is a seven-tuple ( G , Q , / , F , ® , * , p ) where : ~ , Q , i , F are defined as above ; ® is the deterministic state transition function that maps Q x on Q , one writes q®a = q~ ; * is the deterministic emission function that maps Q x ~ on Y , * , one writes q • a = w~ ; and the final emission function p maps F on G* , one writes , ( q ) = w. For instance , T4 is not deterministic because d ( 0 , a ) = C1,2 } , but it is equivalent to T5 represented Figure 14 in the sense that they represent the same function , i.e. are also called generalized sequential machines ( Eilenberg 1974 ) .</sentence>
				<definiendum id="0">subsequential transducers</definiendum>
				<definiens id="0">a subsequential transducer T ' is a seven-tuple ( G , Q , / , F , ® , * , p ) where : ~</definiens>
				<definiens id="1">the deterministic state transition function that maps Q x on Q , one writes q®a = q~ ; * is the deterministic emission function that maps Q x ~ on Y , * , one writes q • a = w~ ; and the final emission function p maps F on G* , one writes , ( q ) = w. For instance</definiens>
			</definition>
			<definition id="6">
				<sentence>10 dom ( f ) denotes the domain of f , that is , the set of words that have at least one output through f. 11 If wi , w2 C ~* , Wl W2 denotes the concatenation of Wl and w 2 .</sentence>
				<definiendum id="0">Wl W2</definiendum>
				<definiens id="0">the domain of f , that is , the set of words that have at least one output through f. 11 If wi , w2 C ~* ,</definiens>
				<definiens id="1">the concatenation of Wl and w 2</definiens>
			</definition>
			<definition id="7">
				<sentence>the state transition function is defined by : S®a= U U { ( q ' , ( S*a ) -l '' u'6 ( q , a , q ' ) ) } ( q , u ) cS q , ~d ( q , a ) Given u , v E E* , u v denotes the concatenation of u and v and u -1 • v -w , if w is such that u w -v , u -I • v = 0 if no such w exists .</sentence>
				<definiendum id="0">state transition function</definiendum>
				<definiendum id="1">v E E* , u v</definiendum>
				<definiens id="0">the concatenation of u and v</definiens>
			</definition>
			<definition id="8">
				<sentence>Definition A transformation-based system is a finite sequence ( f\ ] , ... , fn ) of subsequential functions whose domains are bounded .</sentence>
				<definiendum id="0">transformation-based system</definiendum>
				<definiens id="0">f\ ] , ... , fn ) of subsequential functions whose domains are bounded</definiens>
			</definition>
			<definition id="9">
				<sentence>+ , the right-minimal local extension of f , denoted RmLocExt ( f ) , is the composition of a right-minimal Y-decomposition mdy with Ida , • ( \ [ /~ .</sentence>
				<definiendum id="0">RmLocExt ( f )</definiendum>
				<definiens id="0">the composition of a right-minimal Y-decomposition mdy with Ida</definiens>
			</definition>
			<definition id="10">
				<sentence>RmLocExt being the composition of two subsequential functions , it is itself subsequential ; this proves the following final proposition , which states that given a rulebased system similar to Brill 's system , one can build a subsequential transducer that represents it : Proposition If ( fl ... . , fn ) is a sequence of subsequential functions with bounded domains and such that fi ( ~ ) = 0 , then RmLocExt ( h ) o..</sentence>
				<definiendum id="0">RmLocExt</definiendum>
				<definiendum id="1">fn )</definiendum>
				<definiens id="0">being the composition of two subsequential functions , it is itself subsequential</definiens>
				<definiens id="1">states that given a rulebased system similar to Brill 's system , one can build a subsequential transducer that represents it : Proposition If ( fl ... . ,</definiens>
			</definition>
</paper>

		<paper id="4001">
			<definition id="0">
				<sentence>Misconceptions are errors in the prior knowledge of a participant ; for example , believing that Canada is one of the United States .</sentence>
				<definiendum id="0">Misconceptions</definiendum>
			</definition>
			<definition id="1">
				<sentence>Conversation analysts classify repairs according to how soon after the problematic turn a participant initiates a repair ( Schegloff 1992 ) .</sentence>
				<definiendum id="0">Conversation analysts</definiendum>
			</definition>
			<definition id="2">
				<sentence>According to the ethnomethodological account of human communication known as Conversation Analysis ( CA ) , agents design their behavior with the understanding that they will be held accountable for it .</sentence>
				<definiendum id="0">Conversation Analysis ( CA</definiendum>
				<definiens id="0">agents design their behavior with the understanding that they will be held accountable for it</definiens>
			</definition>
			<definition id="3">
				<sentence>440 McRoy and Hirst The Repair of Speech Act Misunderstandings Act type Speech act name Linguistic intentions informative assert ( S , H , P ) know ( S , P ) assertref ( S , H , P ) knowref ( S , P ) assertif ( S , H , P ) knowif ( S , P ) inform ( S , H , P ) know ( P ) intend ( S , know ( H , P ) ) informref ( S , H , P ) knowref ( S , P ) intend ( S , knowref ( H , P ) ) informif ( S , H , P ) knowif ( S , P ) intend ( S , knowif ( H , P ) ) inquisitive askref ( S , H , P ) not knowref ( S , P ) intend ( S , knowref ( S , P ) ) intend ( S , do ( H , informref ( H , S , P ) ) ) askif ( S , H , P ) not knowif ( S , P ) intend ( S , knowif ( S , P ) ) intend ( S , do ( H , informif ( H , S , P ) ) ) requestive request ( S , H , do ( H , P ) ) intend ( S , do ( H , P ) ) pretell ( S , H , P ) knowref ( S , P ) knowsBetterRef ( S , H , P ) intend ( S , do ( S , informref ( S , H , P ) ) ) intend ( S , knowref ( H , P ) ) testref ( S , H , P ) knowref ( S , P ) intend ( S , do ( H , assertref ( H , S , P ) ) ) testif ( S , H , P ) knowif ( S , P ) intend ( S , do ( H , assertif ( H , S , P ) ) ) Figure 1 Linguistic intentions .</sentence>
				<definiendum id="0">P ) intend</definiendum>
				<definiens id="0">H , S , P ) ) ) requestive request ( S , H , do</definiens>
			</definition>
			<definition id="4">
				<sentence>Theorist typifies what is known as a `` proof12 Non-understanding , which entails non-acceptance ( or deferred acceptance ) , is signaled by second-turn repair .</sentence>
				<definiendum id="0">Theorist</definiendum>
				<definiens id="0">typifies what is known as a `` proof12 Non-understanding , which entails non-acceptance ( or deferred acceptance ) , is signaled by second-turn repair</definiens>
			</definition>
			<definition id="5">
				<sentence>If 9 t '' is the set of facts and AP is the set of defaults with priority p , then an expression DEFAULT ( p , d ) : W asserts that d E AP and ( d D w ) E 5 r. The language lacks explicit quantification ; as in Prolog , variable names are understood to be universally quantified .</sentence>
				<definiendum id="0">AP</definiendum>
				<definiendum id="1">DEFAULT</definiendum>
				<definiens id="0">the set of defaults with priority p , then an expression</definiens>
				<definiens id="1">language lacks explicit quantification ; as in Prolog , variable names are understood to be universally quantified</definiens>
			</definition>
			<definition id="6">
				<sentence>Given these three subtheories , an interpretation of an utterance is a set of ground instances of assumptions that explain the utterance .</sentence>
				<definiendum id="0">interpretation of an utterance</definiendum>
				<definiens id="0">a set of ground instances of assumptions that explain the utterance</definiens>
			</definition>
			<definition id="7">
				<sentence>FACT ~l intentionsOk ( al , ts ) D -~adoptPlan ( 81 , S2 , al , a2 , ts ) .</sentence>
				<definiendum id="0">FACT ~l intentionsOk</definiendum>
				<definiens id="0">al , ts ) D -~adoptPlan ( 81 , S2 , al , a2 , ts )</definiens>
			</definition>
			<definition id="8">
				<sentence>&gt; &gt; &gt; surface-request ( m , r , informif ( r , m , knowref ( r , whoIsGoing ) ) ) ***Interpreting Utterance*** Explaining utter ( m , r , surface-request ( m , r , informif ( r , m , knowref ( r , whoIsGoing ) ) ) , ts ( O ) ) Is formula pickForm ( m , r , surface-request ( m , r , informif ( r , m , knowref ( r , whoIsGoing ) ) ) , pretell ( m , r , whoIsGoing ) , ts ( O ) ) ok ( y/n ) ?</sentence>
				<definiendum id="0">r</definiendum>
				<definiendum id="1">knowref</definiendum>
				<definiens id="0">ts ( O ) ) Is formula pickForm</definiens>
			</definition>
			<definition id="9">
				<sentence>459 Computational Linguistics Volume 21 , Number 4 Explaining shouldTry ( r , m , A , ts ( 1 ) ) , decomp ( A2 , A ) Answer : shouldTry ( r , m , askref ( r , m , whoIsGoing ) , ts ( 1 ) ) , decomp ( surface-request ( r , m , informref ( m , r , whoIsGoing ) ) , askref ( r , m , whoIsGoing ) ) Explanation : intentionalAct ( r , m , askref ( r , m , whoIsGoing ) , ts ( 1 ) ) acceptance ( r , askref ( r , m , whoIsGoing ) , ts ( 1 ) ) expectedReply ( do ( m , pretell ( m , r , whoIsGoing ) ) , knowsBetterKef ( m , r , whoIsGoing ) , do ( r , askref ( r , m , whoIsGoing ) ) , ts ( 1 ) ) ***Updating Discourse Model*** Interpretation : askref ( r , m , whoIsGoing ) ( turn number 2 ) expressed ( do ( r , askref ( r , m , whoIsGoing ) ) ,2 ) Linguistic Intentions of askref ( r , m , whoIsGoing ) : not knowref ( r , whoIsGoing ) and intend ( r , knowref ( r , whoIsGoing ) ) and intend ( r , do ( m , informref ( m , r , whoIsGoing ) ) ) Suppositions Added : expressedNot ( knowref ( r , whoIsGoing ) ,2 ) expressed ( intend ( r , knowref ( r , whoIsGoing ) ) ,2 ) expressed ( intend ( r , do ( m , informref ( m , r , whoIsGoing ) ) ) ,2 ) Agent r performed expected act : askref ( r , m , whoIsGoing ) ***Generating Utterance*** &lt; &lt; &lt; surface-request ( r , m , informref ( m , r , whoIsGoing ) ) Figure 7 The output for turn 2 from Russ 's perspective. DEFAULT ( 2 , acceptance ( s1 , areply , ts ) ) : expected ( s1 , areply , ts ) D shouldTry ( sl , $ 2 , areply , ts ) . The askref would be expected ( see Section 3.3.3 ) because : 39 • According to the discourse model , it is true that active ( do ( m pretell ( m , r , whoIsGoing ) ) , ts ( 1 ) ) . • There is a linguistic expectation that askref follow pretell. • Russ believes the conditions of this relation : knowsBetterRef ( m , r , whoIsGoing ) . • The linguistic intentions of askref are compatible with those already expressed. 39 See Figure 8 for how Mother might interpret this turn. 460 McRoy and Hirst The Repair of Speech Act Misunderstandings If we assume that Mother produced the first turn as an askif , she might also hear T2 as an intentional askref , but for a reason different than Russ would. Her explanation would include the metaplanning assumption that he was doing so as part of an adopted plan to get her to produce an informref. Although T2 might also be explained by abducing that Russ misunderstood T1 as an attempted pretelling , we see that she considers this explanation to be less likely because otherwise she would have been more inclined to make T3 a third-turn repair ( `` No , I 'm asking you '' ) . 40 Plan adoption ( see Table 1 ) provides Mother a plausible explanation for T2 because : because Mother has a linguistic expectation that says that an askref normally creates an expectation for the listener to tell the speaker the answer : ~ACT lexpectation ( do ( r , askref ( r , m , whoIsGoing ) ) , knowref ( m , whoIsGoing ) , do ( m , informref ( m , r , whoIsGoing ) ) ) . perform the expected informref. so it is consistent to assume that Russ is intending to use it as part of a plan. ( They are consistent with the context because T1 expresses only that Mother does not know whether Russ knows and not that she does not herself know. ) whoIsGoing ) , ts ( 0 ) ) is explainable. Assuming this interpretation , Mother can then demonstrate acceptance using an inform-notknowref. Figure 8 How Mother interprets T2. Mother replies with a surface-inform. This is interpreted as a discourse-level informnot-knowref. This act signals a misunderstanding , because the linguistic intentions associated with it are incompatible with those previously assumed , ruling out an explanation that uses the default for intentional acts. 41 Figure 9 shows that Theorist abduces that T3 is attributable to a misunderstanding on Russ 's part , in particular , to his having incorrectly interpreted one of Mother 's utterances as a pretelling , rather than as an askref. This explanation succeeded because each of the conditions of the default for self-misunderstanding were explainable. Below we will repeat this rule and then sketch the proof , considering each of the premises in the default. 40 In the model , it is always possible to begin an embedded sequence without addressing the question on the floor ; however , when the embedded sequence is complete , the top-level one is resumed. It is a limitation of the model that we do not distinguish interruptions from clarifications. 41 For Russ to have heard T3 as demonstrating Mother 's acceptance of his T2 ( i.e. , as a display of understanding ) , the linguistic intentions of inform ( m , r , not knowref ( m , whoIsGoing ) ) would need to have been compatible with this interpretation of the discourse. However , not knowref ( m , whoIsGoing ) is among these intentions , while active ( knowref ( m , whoIsGoing ) , ts ( 2 ) ) . As a result , T3 can not be attributed to any expected act , and must be attributed to a misunderstanding either by Russ or by Mother. 461 Computational Linguistics Volume 21 , Number 4 &gt; &gt; &gt; surface-inform ( m , r , not knowref ( m , whoIsGoing ) ) ***Interpreting Utterance*** Explaining utter ( m , r , inform ( m , r , not knowref ( m , wholsGoing ) ) , ts ( 2 ) ) Is formula pickForm ( m , r , surface-inform ( m , r , not knowref ( m , whoIsGoing ) ) , inform ( m , r , not knowref ( m , whoIsGoing ) ) , ts ( 2 ) ) ok ( y/n ) ?</sentence>
				<definiendum id="0">expectedReply</definiendum>
				<definiendum id="1">askref</definiendum>
				<definiendum id="2">pickForm</definiendum>
				<definiens id="0">a linguistic expectation that says that an askref normally creates an expectation for the listener to tell the speaker the answer : ~ACT lexpectation</definiens>
				<definiens id="1">intending to use it as part of a plan. ( They are consistent with the context because T1 expresses only that Mother does not know whether Russ knows and not that she does not herself know. ) whoIsGoing</definiens>
				<definiens id="2">attributable to a misunderstanding on Russ 's part , in particular , to his having incorrectly interpreted one of Mother 's utterances as a pretelling</definiens>
			</definition>
			<definition id="10">
				<sentence>Misconceptions are a deficit in an agent 's knowledge of the world ; they can become a barrier to understanding if they cause an agent to unintentionally evoke a concept or relation .</sentence>
				<definiendum id="0">Misconceptions</definiendum>
				<definiens id="0">a deficit in an agent 's knowledge of the world ; they can become a barrier to understanding if they cause an agent to unintentionally evoke a concept or relation</definiens>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>The testing material consists of newspaper texts with 60,000-180,000 words for each language and an English EEC-law text with 110,000 words .</sentence>
				<definiendum id="0">testing material</definiendum>
			</definition>
			<definition id="1">
				<sentence>The tagging process can be modeled by an HMM by assuming that each hidden tag state produces a word in the sentence , each word wi is uncorrelated with neighboring words and their tags , and each tag is probabilistic dependent on the N previous tags only .</sentence>
				<definiendum id="0">tagging process</definiendum>
				<definiens id="0">probabilistic dependent on the N previous tags only</definiens>
			</definition>
			<definition id="2">
				<sentence>observation sequence of words is given by the following equation : N M M Z~ HMM-TS ) -argmaxP ( h ) H P ( ti \ [ ti-1 ... .. h ) H P ( ti \ ] ti-1 , ... , ti-N ) H P ( wi \ [ ti ) tl , ... , tM i=2 i=N+I i=1 ( 2 ) where M is the number of words in the sentence W. The optimal solution is estimated by the well-known Viterbi algorithm .</sentence>
				<definiendum id="0">tM</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">the number of words in the sentence W. The optimal solution is estimated by the well-known Viterbi algorithm</definiens>
			</definition>
			<definition id="3">
				<sentence>The probability P ( t I Less probable word ) and the tags probability P ( t ) are measured in the training text .</sentence>
				<definiendum id="0">probability P</definiendum>
			</definition>
			<definition id="4">
				<sentence>Finally , each tag-conditional probability of the unknown word tags is normalized : L ~_ , P ( wj \ [ ti ) + P ( Unknown word I ti ) = 1 , j=l Vi = 1 , T ( 6 ) where L is the number of the known words and T is the number of tags .</sentence>
				<definiendum id="0">L</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">the number of the known words and</definiens>
				<definiens id="1">the number of tags</definiens>
			</definition>
			<definition id="5">
				<sentence>It is realized by the following quantization process : \ [ /max ( ln ( Pmin ) -ln ( Px ) ) \ ] ( 8 ) Ix -- -Round Mw ln ( Pmin ) where : Px is a conditional probability , Pmin is the minimum conditional probability in the model parameter set , /max is the maximum integer of the fixed-point arithmetic system , Mw is the maximum number of words in a sentence and Round\ [ .</sentence>
				<definiendum id="0">Px</definiendum>
				<definiendum id="1">Pmin</definiendum>
				<definiendum id="2">/max</definiendum>
				<definiendum id="3">Mw</definiendum>
				<definiens id="0">a conditional probability</definiens>
				<definiens id="1">the minimum conditional probability in the model parameter set</definiens>
				<definiens id="2">the maximum number of words in a sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>After the logarithmic and the fixed-point transformation , equations 2 and 7 become : N I ( HMM-Ts ) -argmaxI ( tl ) + ~_ , I ( tilti_l , ... , h ) tl , ... , tM i=2 M M + ~_ , I ( ti \ ] ti-1 , ... , ti-N ) + ~I ( wi I ti ) ( 9 ) i=N+l i=1 N M I~ MLM ) = argmaxI ( tl ) + ~__I ( ti I ti_ , ... . , tl ) + ~ I ( ti I ti_ , , ... , ti-N ) ( 10 ) tl ... .. tM i=2 i=N+I The quantization function approximates the computations , producing theoretically differing solutions .</sentence>
				<definiendum id="0">tM i=2 i=N+I The quantization function</definiendum>
				<definiens id="0">HMM-Ts ) -argmaxI ( tl ) + ~_ , I ( tilti_l , ... , h ) tl , ... , tM i=2 M M + ~_ , I ( ti \ ] ti-1 , ... , ti-N ) + ~I ( wi I ti ) ( 9 ) i=N+l i=1 N M I~ MLM ) = argmaxI ( tl ) + ~__I ( ti I ti_ , ... . , tl ) + ~ I ( ti I ti_ , , ... , ti-N ) ( 10 ) tl ... ..</definiens>
			</definition>
			<definition id="7">
				<sentence>When the taggers are trained using the 170,000 words of the English newspaper corpus , a greater number of lexicon entries and a greater number of transition probabilities ( Figure 7 ) is measured than in the case of the EEC-law corpus ( 100K words training text ) .</sentence>
				<definiendum id="0">newspaper corpus</definiendum>
				<definiens id="0">measured than in the case of the EEC-law corpus ( 100K words training text )</definiens>
			</definition>
</paper>

		<paper id="4003">
			<definition id="0">
				<sentence>Inefficiency is a problem that can not simply be cast aside .</sentence>
				<definiendum id="0">Inefficiency</definiendum>
				<definiens id="0">a problem that can not simply be cast aside</definiens>
			</definition>
			<definition id="1">
				<sentence>Chains consist of the word that undergoes movement and all the positions this word occupies in the course of a derivation .</sentence>
				<definiendum id="0">Chains</definiendum>
				<definiens id="0">consist of the word that undergoes movement and all the positions this word occupies in the course of a derivation</definiens>
			</definition>
			<definition id="2">
				<sentence>Principle-based grammars engender compactness : Given a set of principles , P1 , P2 , ... , Pn , the principles are stored separately and their interaction is computed on-line ; the multiplicative interaction of the principles , P1 x P2 x ... × Pn does not need to be stored .</sentence>
				<definiendum id="0">Principle-based grammars engender compactness</definiendum>
				<definiens id="0">Given a set of principles , P1 , P2 , ... , Pn , the principles are stored separately and their interaction is computed on-line ; the multiplicative interaction of the principles</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , X theory ( a condition on graphs ) , the Case Filter ( an output filter on strings ) , and the 0 criterion ( a bijection relation on predicates and arguments ) all fall under the label of principles .</sentence>
				<definiendum id="0">X theory</definiendum>
				<definiendum id="1">Case Filter</definiendum>
			</definition>
			<definition id="4">
				<sentence>The output consists of a tree and a list of two chains : the list of A chains and the list of A chains , that is , chains formed by wh-movement and NP movement , respectively .</sentence>
				<definiendum id="0">list of A chains</definiendum>
				<definiens id="0">a tree and a list of two chains : the list of A chains and the</definiens>
			</definition>
			<definition id="5">
				<sentence>Two main patterns of conflict are observed : In those states that have the highest number of conflicts , all rules that cover the empty string can apply ; in those states that have an intermediate number 525 Computational Linguistics Volume 21 , Number 4 Table 3 Comparison of the 3 grammars ( compiled into LL tables ) NB OF NB OF NB OF AVERAGE ENTRIES ACTIONS RULES CONFLICTS GRAMMAR 1 19 62 16 3.26 GRAMMAR 1 ' 19 46 13 2.42 GRAMMAR 2 112 255 51 2.28 GRAMMAR 3 144 368 41 2.62 of conflicts , only some rules can apply , namely , those that have a certain X projection level , and that cover the empty string ( e.g. , all XP 's , independent of category , that cover the empty string ) .</sentence>
				<definiendum id="0">empty string</definiendum>
			</definition>
			<definition id="6">
				<sentence>Second , on positing an empty element , the parser must decide to which chain it belongs ) 3 The two decisions can be seen as instances of the same problem , which consists in identifying the type of link in the chain that a given input node can form ( whether head , intermediate or foot , abbreviated as H , I , F in what follows . )</sentence>
				<definiendum id="0">input node</definiendum>
				<definiens id="0">consists in identifying the type of link in the chain that a given</definiens>
			</definition>
			<definition id="7">
				<sentence>NLAB takes an input word and outputs a label , while CSEL takes a triple ( Node , Label , Chains ) as input , and returns a new chain list .</sentence>
				<definiendum id="0">NLAB</definiendum>
				<definiens id="0">takes an input word and outputs a label , while CSEL takes a triple ( Node , Label , Chains ) as input</definiens>
			</definition>
			<definition id="8">
				<sentence>The second factor accounts for the growth rate proper , which is reducible to counting the set of k-strings over an n-sized alphabet , hence n k. Here , k is the number of relevant links in the sentence ( for instance , feet in NLAB ' ) , and n is given by the size of the set of features collapsed by lifting some of these checks , hence , 2 , 2 , 2 and 4 , respectively .</sentence>
				<definiendum id="0">k</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the number of relevant links in the sentence ( for instance , feet in NLAB ' ) , and</definiens>
			</definition>
			<definition id="9">
				<sentence>In NLAB ~ , where there is no restriction on the number of active chains , the growth rate is n k. For NLAB '' and NLAB m , the formula is NA k , where NA is the number of active chains .</sentence>
				<definiendum id="0">NA</definiendum>
				<definiens id="0">the number of active chains</definiens>
			</definition>
			<definition id="10">
				<sentence>Modularity , if it exists , is to be found in the linguistic content , and not in the organization of the theory .</sentence>
				<definiendum id="0">Modularity</definiendum>
				<definiens id="0">to be found in the linguistic content</definiens>
			</definition>
</paper>

		<paper id="4002">
			<definition id="0">
				<sentence>Strong lexicalization that preserves the trees derived is possible using context-sensitive formalisms such as tree adjoining grammar ( TAG ) ( Joshi and Schabes 1992 ; Schabes 1990 ) .</sentence>
				<definiendum id="0">Strong lexicalization</definiendum>
			</definition>
			<definition id="1">
				<sentence>However , these context-sensitive formalisms entail larger computation costs than CFGs -- O ( n6 ) -time in the case of TAG ( Vijay-Shanker and Joshi 1985 ) , instead of O ( n 3 ) for CFG .</sentence>
				<definiendum id="0">TAG ( Vijay-Shanker</definiendum>
				<definiens id="0">n 3 ) for CFG</definiens>
			</definition>
			<definition id="2">
				<sentence>Tree Insertion Grammar ( TIG ) is a compromise between CFG and TAG that combines the efficiency of the former with the strong lexicalizing power of the latter .</sentence>
				<definiendum id="0">Tree Insertion Grammar ( TIG )</definiendum>
				<definiens id="0">a compromise between CFG and TAG that combines the efficiency of the former with the strong lexicalizing power of the latter</definiens>
			</definition>
			<definition id="3">
				<sentence>Tree insertion grammar ( TIG ) is a tree generating system that is a restricted variant of tree-adjoining grammar ( TAG ) ( Joshi and Schabes 1992 ; Schabes 1990 ) .</sentence>
				<definiendum id="0">Tree insertion grammar</definiendum>
				<definiendum id="1">TIG</definiendum>
			</definition>
			<definition id="4">
				<sentence>As in TAG , a TIG grammar consists of two sets of trees : initial trees , which are combined by substitution and auxiliary trees , which are combined with each other and the initial trees by adjunction .</sentence>
				<definiendum id="0">TIG grammar</definiendum>
				<definiens id="0">consists of two sets of trees : initial trees</definiens>
			</definition>
			<definition id="5">
				<sentence>Definition 6 \ [ TIG\ ] A tree insertion grammar ( TIG ) is a five-tuple ( G , NT , L A , S ) , where ~ .</sentence>
				<definiendum id="0">TIG</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">a five-tuple ( G , NT , L A ,</definiens>
			</definition>
			<definition id="6">
				<sentence>is a set of terminal symbols , NT is a set of nonterminal symbols , I is a finite set of finite initial 480 Schabes and Waters Tree Insertion Grammar trees , A is a finite set of finite auxiliary trees , and S is a distinguished nonterminal symbol .</sentence>
				<definiendum id="0">NT</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">a set of nonterminal symbols</definiens>
				<definiens id="1">a distinguished nonterminal symbol</definiens>
			</definition>
			<definition id="7">
				<sentence>Adjunction replaces a node with an auxiliary tree .</sentence>
				<definiendum id="0">Adjunction</definiendum>
				<definiens id="0">replaces a node with an auxiliary tree</definiens>
			</definition>
			<definition id="8">
				<sentence>Definition 7 \ [ LTIG\ ] A lexicalized tree insertion grammar ( LTIG ) 4 ( G , NT , L A , S ) is a TIG where every elementary tree in I U A is lexicalized .</sentence>
				<definiendum id="0">S )</definiendum>
			</definition>
			<definition id="9">
				<sentence>Second , TIG prohibits adjunction on the roots of auxiliary trees and allows simultaneous adjunction while TAG allows adjunction on the roots of auxiliary trees and prohibits simultaneous adjunction .</sentence>
				<definiendum id="0">TIG</definiendum>
				<definiens id="0">prohibits adjunction on the roots of auxiliary trees and allows simultaneous adjunction while TAG allows adjunction on the roots of auxiliary trees and prohibits simultaneous adjunction</definiens>
			</definition>
			<definition id="10">
				<sentence>Third , TIG imposes a number of detailed restrictions on the interaction of left and right auxiliary trees .</sentence>
				<definiendum id="0">TIG</definiendum>
			</definition>
			<definition id="11">
				<sentence>Like CFG , TIG generates context-free languages .</sentence>
				<definiendum id="0">TIG</definiendum>
			</definition>
			<definition id="12">
				<sentence>Theorem 1 If G = ( E , NT , LA , S ) is a TIG then there is a CFG G ' = ( E , NT ' , P , S ) that generates the same string set .</sentence>
				<definiendum id="0">S )</definiendum>
				<definiens id="0">Theorem 1 If G = ( E , NT , LA ,</definiens>
				<definiens id="1">a CFG G ' = ( E , NT ' , P , S ) that generates the same string set</definiens>
			</definition>
			<definition id="13">
				<sentence>5 symbols , NT is a set of nonterminal symbols , P is a finite set of finite production rules that rewrite nonterminal symbols to , possibly empty , strings of terminal and nonterminal symbols , and S is a distinguished nonterminal symbol that is the start symbol of any derivation .</sentence>
				<definiendum id="0">NT</definiendum>
				<definiendum id="1">P</definiendum>
				<definiendum id="2">S</definiendum>
				<definiens id="0">a set of nonterminal symbols ,</definiens>
				<definiens id="1">a finite set of finite production rules that rewrite nonterminal symbols to , possibly empty , strings of terminal and nonterminal symbols , and</definiens>
				<definiens id="2">a distinguished nonterminal symbol that is the start symbol of any derivation</definiens>
			</definition>
			<definition id="14">
				<sentence>TIG generates context-free path sets .</sentence>
				<definiendum id="0">TIG</definiendum>
				<definiens id="0">generates context-free path sets</definiens>
			</definition>
			<definition id="15">
				<sentence>Since TIG is a restricted case of tree-adjoining grammar ( TAG ) , standard O ( n6 ) -time TAG parsers ( Lang 1990 ; Schabes 1991 ; Vijay-Shanker 1987 ; Vijay-Shanker and Weir 1993 ; Vijay-Shanker and Joshi 1985 ) can be used for parsing TIG .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">a restricted case of tree-adjoining grammar (</definiens>
			</definition>
			<definition id="16">
				<sentence>The predicate Init ( # x ) is true if and only if # x is the root of an initial tree .</sentence>
				<definiendum id="0">predicate Init</definiendum>
				<definiens id="0">the root of an initial tree</definiens>
			</definition>
			<definition id="17">
				<sentence>The Earley-style TIG parser collects states into a set called the chart , C. A state is a 3-tuple , \ [ p , i , j\ ] where : p is a position in an elementary tree as described below ; and 0 &lt; i &lt; j _ &lt; n are integers indicating a span of the input string .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">integers indicating a span of the input string</definiens>
			</definition>
			<definition id="18">
				<sentence>The substitution rules are triggered by states of the form \ [ # A -- *c~euB fl , i , j\ ] where UB is a node at which substitution can occur .</sentence>
				<definiendum id="0">UB</definiendum>
				<definiens id="0">a node at which substitution can occur</definiens>
			</definition>
			<definition id="19">
				<sentence>In this equation , n is the length of the input string and IG I is the size of the grammar G. For the TIG parser , IGI is computed as the sum over all the non-leaf nodes # in all the elementary trees in G of : one plus the number of children of # .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">IG I</definiendum>
				<definiens id="0">the length of the input string and</definiens>
				<definiens id="1">the sum over all the non-leaf nodes # in all the elementary trees in G of : one plus the number of children of #</definiens>
			</definition>
			<definition id="20">
				<sentence>Another way to lexicalize CFGs is to convert them into categorial grammars ( BarHillel 1964 ) .</sentence>
				<definiendum id="0">CFGs</definiendum>
				<definiens id="0">to convert them into categorial grammars</definiens>
			</definition>
			<definition id="21">
				<sentence>In particular , the growth in the number of rules is at worst O ( mS ) , where m is the number of nonterminals .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">the number of nonterminals</definiens>
			</definition>
			<definition id="22">
				<sentence>A variety of steps can be taken to reduce the number of elementary trees produced by the LTIG procedure .</sentence>
				<definiendum id="0">variety of steps</definiendum>
				<definiens id="0">the number of elementary trees produced by the LTIG procedure</definiens>
			</definition>
			<definition id="23">
				<sentence>The Tomita III grammar contains an empty rule .</sentence>
				<definiendum id="0">Tomita III grammar</definiendum>
				<definiens id="0">contains an empty rule</definiens>
			</definition>
			<definition id="24">
				<sentence>The current English LTAG consists of 392,001 elementary trees .</sentence>
				<definiendum id="0">current English LTAG</definiendum>
			</definition>
			<definition id="25">
				<sentence>Tree insertion grammar ( TIG ) is a restricted form of tree adjoining grammar ( TAG ) that is O ( ng ) -time parsable , generates context-free languages , and yet allows the strong lexicalization of CFG .</sentence>
				<definiendum id="0">Tree insertion grammar</definiendum>
				<definiendum id="1">TIG</definiendum>
				<definiendum id="2">TAG</definiendum>
				<definiens id="0">a restricted form of tree adjoining grammar</definiens>
			</definition>
</paper>

		<paper id="3002">
			<definition id="0">
				<sentence>Synj , k ( 1 ~ j &lt; Nk ) is the jth alternative syntactic structure corresponding to LeXk , and Nk is the number of possible syntactic structures associated with LeXk .</sentence>
				<definiendum id="0">Nk</definiendum>
				<definiens id="0">the jth alternative syntactic structure corresponding to LeXk , and</definiens>
				<definiens id="1">the number of possible syntactic structures associated with LeXk</definiens>
			</definition>
			<definition id="1">
				<sentence>In other words , this process is to find the index ( d , k ) such that P ( Synj , k , Lex~ I w~ ) represents the maximum value among different syntactic structures , as shown in Equation 1 : ( j , lc ) = argmax { P ( Synj , k , LeXk I w~ ) } ( 1 ) j , k The integrated scoring function for the syntactic structure Synj , k is defined as Score ( Synj , k ) = P ( Synj , k , LeXklW~ ) = P ( Synj , k I LeXk , W~ ) x P ( LeXk I w~ ) = Ssyn ( Synj , k ) x Slex ( LeXk ) ( 2 ) 323 Computational Linguistics Volume 21 , Number 3 W= { wlwz .</sentence>
				<definiendum id="0">x Slex</definiendum>
				<definiens id="0">to find the index ( d , k ) such that P ( Synj , k , Lex~ I w~ ) represents the maximum value among different syntactic structures</definiens>
				<definiens id="1">j , lc ) = argmax { P ( Synj , k , LeXk I w~ ) } ( 1 ) j , k The integrated scoring function for the syntactic structure Synj , k is defined as Score ( Synj , k ) = P ( Synj , k , LeXklW~ ) = P ( Synj , k I LeXk , W~ ) x P ( LeXk I w~ ) = Ssyn ( Synj , k )</definiens>
			</definition>
			<definition id="2">
				<sentence>Wr , } LeXl ~ Synl'l SynN~j Lex2 Lexk ~ Synl'k SynNk , k LeXM ~ SynNu , M Figure 1 An illustration of the syntactic ambiguities for an input word sequence W. where Ssyn ( Synj , k ) ( = P ( Synj , k I LeXk , W~ ) ) denotes the syntactic scoring function , and Sle x ( LeXk ) ( = P ( LeXk I w~ ) ) denotes the lexical scoring function .</sentence>
				<definiendum id="0">W~ ) )</definiendum>
				<definiendum id="1">Sle</definiendum>
				<definiens id="0">An illustration of the syntactic ambiguities for an input word sequence W. where Ssyn ( Synj , k ) ( = P ( Synj , k I LeXk</definiens>
				<definiens id="1">the syntactic scoring function</definiens>
			</definition>
			<definition id="3">
				<sentence>~lP ( Ck , i ICk , i-1 ) , I-\ [ P ( Ck , i \ ] Ck , i-1 , Ck , i-2 ) , i=1 bigram model trigram model ( 7 ) Therefore , the lexical score Slex ( LeXk ) is expressed as : ; l * k , i-l '' ~ P ( wilCk , i ) S'ex ( Lexk ) ~ I-\ [ P ( Ck , i \ [ Ck , , ) X i=1 n I-\ [ P ( Ck , i l Ck , i_l ) X P ( wi \ [ £k , i ) , bigram model in1 1-I P ( ck , i \ ] Ck , i-l , Ck , i-2 ) X P ( wi l Ck , i ) , trigram model i=1 ( 8 ) Conventional stochastic context-free grammar ( CFG ) approaches ( Wright and Wrigley 1991 ) evaluate the likelihood probability of a syntactic tree by computing the product of the probabilities associated with the grammar rules being applied .</sentence>
				<definiendum id="0">~lP</definiendum>
				<definiens id="0">stochastic context-free grammar ( CFG ) approaches ( Wright and Wrigley 1991 ) evaluate the likelihood probability of a syntactic tree by computing the product of the probabilities associated with the grammar rules being applied</definiens>
			</definition>
			<definition id="4">
				<sentence>A phrase level is a sequence of symbols ( terminal or nonterminal ) that acts as an intermediate result in parsing the input sentence , and is also called a sentential form in formal language theory ( Hopcroft and Ullman 1974 ) .</sentence>
				<definiendum id="0">phrase level</definiendum>
			</definition>
			<definition id="5">
				<sentence>3 Lex ( L2 ) +Syn ( L1 ) : this model uses a trigram model in computing lexical scores and the L1 mode of operation in computing syntactic scores .</sentence>
				<definiendum id="0">Lex ( L2 ) +Syn ( L1 )</definiendum>
				<definiens id="0">this model uses a trigram model in computing lexical scores and the L1 mode of operation in computing syntactic scores</definiens>
			</definition>
			<definition id="6">
				<sentence>Lex ( L1 ) +Syn ( L2 ) : this model uses a bigram model in computing lexical scores and the L2 mode of operation in computing syntactic scores .</sentence>
				<definiendum id="0">Lex</definiendum>
				<definiens id="0">this model uses a bigram model in computing lexical scores and the L2 mode of operation in computing syntactic scores</definiens>
			</definition>
			<definition id="7">
				<sentence>Lex ( L2 ) +Syn ( L2 ) : this model uses a trigram model in computing lexical scores and the L2 mode of operation in computing syntactic scores .</sentence>
				<definiendum id="0">Lex</definiendum>
				<definiens id="0">this model uses a trigram model in computing lexical scores and the L2 mode of operation in computing syntactic scores</definiens>
			</definition>
			<definition id="8">
				<sentence>The number of parameters for Syn ( L1 ) and Syn ( L2 ) modes is Np x Nnt and Np x N2t , respectively , where Nnt ( -- -95 ) denotes the number of nonterminal symbols , and Np ( = 96 , 699 ) is the number of patterns corresponding to all possible reduce actions .</sentence>
				<definiendum id="0">Syn</definiendum>
				<definiens id="0">Np x Nnt and Np x N2t , respectively , where Nnt ( -- -95</definiens>
				<definiens id="1">the number of nonterminal symbols</definiens>
				<definiens id="2">the number of patterns corresponding to all possible reduce actions</definiens>
			</definition>
			<definition id="9">
				<sentence>SP is defined as the average selection factor ( SF ) of the disambiguation mechanism on the task of interest .</sentence>
				<definiendum id="0">SP</definiendum>
				<definiens id="0">the average selection factor ( SF ) of the disambiguation mechanism on the task of interest</definiens>
			</definition>
			<definition id="10">
				<sentence>The selection factor for an input sentence is defined as the least proportion of all possible alternative structures that includes the selected syntactic structure .</sentence>
				<definiendum id="0">selection factor for an input sentence</definiendum>
				<definiens id="0">the least proportion of all possible alternative structures that includes the selected syntactic structure</definiens>
			</definition>
			<definition id="11">
				<sentence>Formally , SP is expressed as 1~-~ l ~ ri ( 13 ) sd ( i ) = N n-i SP E\ [ SF\ ] ~ ~ i=1 = ri where sf ( i ) = G is the selection factor for the ith sentence ; ni is the total number of alternative syntactic structures for the ith sentence ; ri is the rank of the most preferred candidate .</sentence>
				<definiendum id="0">SP</definiendum>
				<definiendum id="1">G</definiendum>
				<definiendum id="2">ni</definiendum>
				<definiendum id="3">ri</definiendum>
				<definiens id="0">the selection factor for the ith sentence ;</definiens>
				<definiens id="1">the total number of alternative syntactic structures for the ith sentence</definiens>
			</definition>
			<definition id="12">
				<sentence>The selection power for a disambiguation mechanism basically serves as an indicator of the selection ability that includes the most preferred candidate within a particular ( N-best ) region .</sentence>
				<definiendum id="0">particular</definiendum>
				<definiens id="0">an indicator of the selection ability that includes the most preferred candidate within a</definiens>
			</definition>
			<definition id="13">
				<sentence>Part-of-Speech Accuracy Rate Parse Tree in Word in Sentence Accuracy Rate Selection Model ( % ) ( % ) ( % ) Power Lex ( L1 ) +Syn ( L1 ) 99.62 ( 99.59 ) 95.6 ( 95.0 ) 75.4 ( 72.3 ) 0.34 ( 0.26 ) Lex ( L2 ) +Syn ( L1 ) 99.64 ( 99.61 ) 95.9 ( 95.4 ) 75.8 ( 72.7 ) 0.34 ( 0.26 ) Lex ( L1 ) +Syn ( L2 ) 99.67 ( 99.64 ) 96.1 ( 95.6 ) 78.7 ( 75.9 ) 0.34 ( 0.25 ) Lex ( L2 ) +Syn ( L2 ) 99.69 ( 99.67 ) 96.5 ( 96.0 ) 79.0 ( 76.4 ) 0.33 ( 0.25 ) ( a ) Training set performance Part-of-Speech Accuracy Rate Parse Tree in Word in Sentence Accuracy Rate Selection Model ( % ) ( % ) ( % ) Power Lex ( L1 ) +Syn ( L1 ) 98.89 ( 98.80 ) 88.7 ( 87.13 ) 49.3 ( 42.3 ) 0.45 ( 0.38 ) Lex ( L2 ) +Syn ( L1 ) 98.93 ( 98.84 ) 88.9 ( 87.36 ) 49.7 ( 42.7 ) 0.45 ( 0.38 ) Lex ( L1 ) +Syn ( L2 ) 98.82 ( 98.71 ) 88.0 ( 86.33 ) 52.8 ( 46.2 ) 0.44 ( 0.37 ) Lex ( L2 ) +Syn ( L2 ) 98.89 ( 98.79 ) 88.5 ( 86.90 ) 53.1 ( 46.6 ) 0.44 ( 0.37 ) ( b ) Test set performance information .</sentence>
				<definiendum id="0">L2 ) +Syn</definiendum>
				<definiens id="0">b ) Test set performance information</definiens>
			</definition>
			<definition id="14">
				<sentence>Asy , ~ ( j , i ) = \ [ log P ( Lj , i I -j,1 jj • ~j , k = where Alex ( k , i ) = \ [ -- logP ( Ck , i l C-k,1 , wlJ~ , 1J'i-1~\ ] 1/2 \ [ &amp; syn ( J , 1 ) , ~Zex ( k , 1 ) , ... , `` ~s~ , n ( , J , n ) , Alex ( k , n ) \ ] is regarded as a parameter vector composed of the lexical and syntactic score components , and II , ~j , kll is defined as the Euclidean norm of the vector 'I~j , k. However , in such a formulation , the lexical scores as well as the syntactic scores are assumed to contribute equally to the disambiguation process .</sentence>
				<definiendum id="0">Alex</definiendum>
				<definiendum id="1">syn</definiendum>
				<definiendum id="2">... , `` ~s~</definiendum>
				<definiendum id="3">Euclidean norm of the vector 'I~j , k. However</definiendum>
				<definiens id="0">n ( , J , n ) , Alex ( k , n ) \ ] is regarded as a parameter vector composed of the lexical and syntactic score components</definiens>
				<definiens id="1">scores as well as the syntactic scores are assumed to contribute equally to the disambiguation process</definiens>
			</definition>
			<definition id="15">
				<sentence>Then the misclassification distance , denoted by d~ , k , for selecting the syntactic structure Synj , k as the final output is defined by the following equation : A ) I = \ [ -- go~ , fl ( Wl ) \ ] 2 -- gd , ~ ( Wl ( 21 ) Such a definition makes the distance be the difference of the lengths ( or norms ) of the score vectors in the parameter space .</sentence>
				<definiendum id="0">following equation</definiendum>
				<definiendum id="1">fl ( Wl</definiendum>
				<definiens id="0">the distance be the difference of the lengths ( or norms ) of the score vectors in the parameter space</definiens>
			</definition>
			<definition id="16">
				<sentence>Note that according to the definition in Equation 21 , an error will occur if ddz &gt; 0 , i.e. , 114. , ~ll &gt; 114'j , ~\ [ I. Next , similar to the probabilistic-descent approach ( Amari 1967 ) , a loss function Ij , ~ ( A ) is defined as a nondecreasing and differentiable function of the misclassification distance ; i.e. , lj , k ( A ) = l ( dj , k ( w~ ; A ) ) .</sentence>
				<definiendum id="0">~ ( A )</definiendum>
				<definiendum id="1">i.e. , lj , k</definiendum>
				<definiens id="0">ddz &gt; 0 , i.e. , 114. , ~ll &gt; 114'j , ~\ [ I. Next , similar to the probabilistic-descent approach</definiens>
				<definiens id="1">a nondecreasing and differentiable function of the misclassification distance ;</definiens>
			</definition>
			<definition id="17">
				<sentence>It has been proved by Amari ( 1967 ) that the average loss function will decrease if the adjustments in the learning process satisfy the following equation : At+l = At q6At , 6At = -e ( t ) UVl ( d ; , k ( w~ ; A ) ) , ( 23 ) 333 Computational Linguistics Volume 21 , Number 3 where e ( t ) is a positive function , which usually decreases with time , to control the convergence speed of the learning process ; U is a positive-definite matrix , which is assumed to be an identity matrix in the current implementation , and 27 is the gradient operator .</sentence>
				<definiendum id="0">U</definiendum>
				<definiens id="0">a positive function , which usually decreases with time , to control the convergence speed of the learning process</definiens>
				<definiens id="1">a positive-definite matrix</definiens>
			</definition>
			<definition id="18">
				<sentence>Part-of-Speech Accuracy Rate Parse Tree in Word in Sentence Accuracy Rate Selection Model ( % ) ( % ) ( % ) Power Lex ( L1 ) +Syn ( L1 ) 99.62 ( 99.59 ) + Discrimination Learning 99.95 ( 99.94 ) Lex ( L2 ) +Syn ( L1 ) 99.64 ( 99.61 ) ÷ Discrimination Learning 99.97 ( 99.96 ) Lex ( L1 ) +Syn ( L2 ) 99.67 ( 99.64 ) + Discrimination Learning 99.96 ( 99.95 ) Lex ( L2 ) +Syn ( L2 ) 99.69 ( 99.67 ) + Discrimination Learning 99.97 ( 99.97 ) 95.57 ( 94.99 ) 75.43 ( 72.26 ) 0.34 ( 0.26 ) 99.32 ( 99.23 ) 92.04 ( 91.02 ) 0.30 ( 0.21 ) 95.93 ( 95.41 ) 75.81 ( 72.69 ) 0.34 ( 0.26 ) 99.53 ( 99.47 ) 92.29 ( 91.29 ) 0.30 ( 0.21 ) 96.07 ( 95.56 ) 78.69 ( 75.93 ) 0.34 ( 0.25 ) 99.40 ( 99.32 ) 92.54 ( 91.58 ) 0.30 ( 0.21 ) 96.46 ( 96.00 ) 79.04 ( 76.34 ) 0.33 ( 0.25 ) 99.61 ( 99.56 ) 92.77 ( 91.83 ) 0.30 ( 0.21 ) ( a ) Training set performance Part-of-Speech Accuracy Rate Parse Tree Model in Word in Sentence Accuracy Rate Selection ( % ) ( % ) ( % ) Power Lex ( L1 ) +Syn ( L1 ) 98.89 ( 98.80 ) + Discrimination Learning 98.82 ( 98.72 ) Lex ( L2 ) +Syn ( L1 ) 98.93 ( 98.84 ) + Discrimination Learning 99.05 ( 98.97 ) Lex ( L1 ) +Syn ( L2 ) 98.82 ( 98.71 ) + Discrimination Learning 98.88 ( 98.78 ) Lex ( L2 ) +Syn ( L2 ) 98.89 ( 98.79 ) + Discrimination Learning 98.92 ( 98.83 ) 88.7 ( 87.1 ) 49.3 ( 42.3 ) 0.45 ( 0.38 ) 88.0 ( 86.3 ) 55.5 ( 49.3 ) 0.42 ( 0.34 ) 88.9 ( 87.4 ) 49.7 ( 42.7 ) 0.45 ( 0.38 ) 90.1 ( 88.7 ) 55.3 ( 49.1 ) 0.42 ( 0.34 ) 88.0 ( 86.3 ) 52.8 ( 46.3 ) 0.44 ( 0.37 ) 88.2 ( 88.6 ) 56.6 ( 50.6 ) 0.42 ( 0.34 ) 88.5 ( 86.9 ) 53.1 ( 46.6 ) 0.44 ( 0.37 ) 88.3 ( 86.7 ) 56.4 ( 50.3 ) 0.42 ( 0.34 ) ( b ) Test set performance a real application .</sentence>
				<definiendum id="0">Discrimination Learning 99.05</definiendum>
				<definiendum id="1">L1 ) +Syn ( L2</definiendum>
				<definiens id="0">b ) Test set performance a real application</definiens>
			</definition>
			<definition id="19">
				<sentence>Part-of-Speech Accuracy Rate Parse Tree *Learning in Word in Sentence Accuracy Rate Selection Model Procedure ( % ) ( % ) ( % ) Power +DL 99.95 ( 99.94 ) 99.32 ( 99.23 ) 92.04 ( 91.02 ) 0.42 ( 0.34 ) Lex ( L1 ) +Syn ( L1 ) +RL 99.90 ( 99.89 ) 98.77 ( 98.61 ) 91.84 ( 90.79 ) 0.38 ( 0.29 ) +DL 99.97 ( 99.96 ) 99.53 ( 99.47 ) 92.29 ( 91.29 ) 0.42 ( 0.34 ) Lex ( L2 ) +Syn ( L1 ) +RL 99.92 ( 99.92 ) 99.06 ( 98.93 ) 92.08 ( 91.05 ) 0.38 ( 0.30 ) +DL 99.96 ( 99.95 ) 99.40 ( 99.32 ) 92.54 ( 91.58 ) 0.42 ( 0.34 ) Lex ( L1 ) +Syn ( L2 ) +RL 99.92 ( 99.92 ) 99.03 ( 98.91 ) 92.94 ( 92.03 ) 0.38 ( 0.30 ) +DL 99.97 ( 99.97 ) 99.61 ( 99.56 ) 92.77 ( 91.83 ) 0.42 ( 0.34 ) Lex ( L2 ) +Syn ( L2 ) +RL 99.93 ( 99.93 ) 99.19 ( 99.08 ) 93.12 ( 92.23 ) 0.38 ( 0.30 ) ( a ) Training set performance Part-of-Speech Accuracy Rate Parse Tree *Learning in Word in Sentence Accuracy Rate Selection Model Procedure ( % ) ( % ) ( % ) Power +DL 98.82 ( 98.72 ) 88.0 ( 86.3 ) 55.5 ( 49.3 ) 0.42 ( 0.34 ) Lex ( L1 ) +Syn ( L1 ) +RL 99.23 ( 99.16 ) 91.5 ( 90.3 ) 63.8 ( 58.8 ) 0.38 ( 0.29 ) +DL 99.05 ( 98.97 ) 90.1 ( 88.7 ) 55.3 ( 49.1 ) 0.42 ( 0.34 ) Lex ( L2 ) +Syn ( L1 ) +RL 99.27 ( 99.21 ) 91.5 ( 90.3 ) 64.2 ( 59.2 ) 0.38 ( 0.29 ) +DL 98.88 ( 98.78 ) 88.2 ( 88.6 ) 56.6 ( 50.6 ) 0.42 ( 0.34 ) Lex ( L1 ) +Syn ( L2 ) +RL 99.19 ( 99.12 ) 90.9 ( 89.6 ) 63.7 ( 58.7 ) 0.38 ( 0.30 ) +DL 98.92 ( 93.83 ) 88.3 ( 86.7 ) 56.4 ( 50.3 ) 0.42 ( 0.34 ) Lex ( L2 ) +Syn ( L2 ) +RL 99.18 ( 99.10 ) 90.7 ( 89.4 ) 64.3 ( 59.3 ) 0.38 ( 0.30 ) ( b ) Test set performance ~DL and RL denote `` Discriminative Learning '' and `` Robust Learning , '' respectively The above-mentioned robust learning algorithm starts with the initial parameters estimated by using MLE method .</sentence>
				<definiendum id="0">Lex</definiendum>
				<definiendum id="1">Lex</definiendum>
				<definiendum id="2">RL denote</definiendum>
				<definiens id="0">Discriminative Learning '' and `` Robust Learning , '' respectively The above-mentioned robust learning algorithm starts with the initial parameters estimated by using MLE method</definiens>
			</definition>
			<definition id="20">
				<sentence>For example , MLE gives a zero probability to events that were never observed in the training set .</sentence>
				<definiendum id="0">MLE</definiendum>
				<definiens id="0">gives a zero probability to events that were never observed in the training set</definiens>
			</definition>
			<definition id="21">
				<sentence>Then the following equation holds : N = y'~r. nr ( 37 ) r The maximum likelihood estimate PML for the probability of an event e occurring r times is defined as follows : r PML ( e ) = ~ ( 38 ) The estimate based on Turing 's formula ( Good 1953 ) is given by the following equation : r • PTU ( e ) = ~ ( 39 ) where r* = ( r + 1 ) nr+l ( 40 ) Hr The total probability estimate , using Turing 's formula , for all the events that actually occurred in the sample space is equal to PTU ( e ) = 1 nl ( 41 ) N e : C ( e ) &gt; 0 where C ( e ) stands for the frequency count of the event e in the sample .</sentence>
				<definiendum id="0">space</definiendum>
				<definiendum id="1">N e : C ( e</definiendum>
				<definiens id="0">The maximum likelihood estimate PML for the probability of an event e occurring r times is defined as follows : r PML ( e ) = ~ ( 38 ) The estimate based on Turing 's formula ( Good 1953 ) is given by the following equation : r • PTU ( e ) = ~ ( 39 ) where r* = ( r + 1 ) nr+l ( 40 ) Hr The total probability estimate , using Turing 's formula , for all the events that actually occurred in the sample</definiens>
			</definition>
			<definition id="22">
				<sentence>: C ( wD &gt; O ( 44 ) 339 Computational Linguistics Volume 21 , Number 3 is a normalized factor such that PBF ( Wm I WT ) + Z PBF ( Wm I WT ) = 1 ( 45 ) wm : C ( wD &gt; 0 wm : C ( wT ) =0 Compared with Turing 's formula , the probability for an m-gram that does not occur in the sample is `` backed off '' to refer to its corresponding ( m-1 ) -gram probability .</sentence>
				<definiendum id="0">PBF</definiendum>
				<definiens id="0">a normalized factor such that PBF ( Wm I WT ) + Z</definiens>
			</definition>
			<definition id="23">
				<sentence>Initial Estimation : For an m-gram model , the conditional probability P ( x~ I m-1 x I ) is estimated by the following equation : P ( Xm I m-1 C ( Xl ... .. Xm-I , X ) ( 46 ) where V denotes the vocabulary and C ( . )</sentence>
				<definiendum id="0">Initial Estimation</definiendum>
				<definiendum id="1">V</definiendum>
				<definiens id="0">For an m-gram model , the conditional probability P ( x~ I m-1 x I ) is estimated by the following equation : P ( Xm I m-1 C ( Xl ... .. Xm-I</definiens>
			</definition>
			<definition id="24">
				<sentence>If ~y~v C ( xl ... . , Xm-l , y ) &gt; Q~ , where Qa is a present threshold , it is assumed that the estimated value of P ( xm \ ] x~ n-l ) is reliable and no action is required in this situation .</sentence>
				<definiendum id="0">Qa</definiendum>
				<definiens id="0">a present threshold , it is assumed that the estimated value of P ( xm \ ] x~ n-l ) is reliable and no action is required in this situation</definiens>
			</definition>
</paper>

		<paper id="3003">
			<definition id="0">
				<sentence>Below , we give an algorithmic interpretation of Clark and Wilkes-Gibbs 's collaborative model , where present , judge , and refashion are the conversational moves that the participants make , and ref , re , and judgment are variables that represent the referent , the current referring expression , and its judgment , respectively .</sentence>
				<definiendum id="0">refashion</definiendum>
				<definiendum id="1">judgment</definiendum>
				<definiens id="0">an algorithmic interpretation of Clark and Wilkes-Gibbs 's collaborative model , where present , judge , and</definiens>
				<definiens id="1">the conversational moves that the participants make , and ref , re</definiens>
			</definition>
			<definition id="1">
				<sentence>Cohen and Perrault ( 1979 ) developed a system that uses plan construction to map an agent 's goals to speech acts , and Allen and Perrault ( 1980 ) use plan inference to understand an agent 's plan from its speech acts .</sentence>
				<definiendum id="0">Allen</definiendum>
				<definiens id="0">developed a system that uses plan construction to map an agent 's goals to speech acts , and</definiens>
			</definition>
			<definition id="2">
				<sentence>An action schema consists of a header , constraints , a decomposition , and an effect ; and it encodes the constraints under which an effect can be achieved by performing the steps in the decomposition .</sentence>
				<definiendum id="0">action schema</definiendum>
				<definiens id="0">consists of a header , constraints , a decomposition , and an effect ; and it encodes the constraints under which an effect can be achieved by performing the steps in the decomposition</definiens>
			</definition>
			<definition id="3">
				<sentence>The second is s-attrib ( Entity , Predicate ) , and is used for describing an object in terms of an attribute ; Entity is the discourse entity of the object , and Predicate is a lambda expression , such as &amp; X. category ( X , bird ) , that encodes the attribute .</sentence>
				<definiendum id="0">Entity</definiendum>
				<definiendum id="1">Predicate</definiendum>
				<definiens id="0">the discourse entity of the object</definiens>
				<definiens id="1">a lambda expression , such as &amp; X. category</definiens>
			</definition>
			<definition id="4">
				<sentence>Actions are the primitive actions that are added to the plan .</sentence>
				<definiendum id="0">Actions</definiendum>
				<definiens id="0">the primitive actions that are added to the plan</definiens>
			</definition>
			<definition id="5">
				<sentence>modifier-absolute-pred ( Pred ) : Pred is a predicate that an object can be described in terms of .</sentence>
				<definiendum id="0">Pred</definiendum>
				<definiens id="0">a predicate that an object can be described in terms of</definiens>
			</definition>
			<definition id="6">
				<sentence>modifier-relative-pred ( Pred ) : Pred is a predicate that describes the relationship between two objects .</sentence>
				<definiendum id="0">Pred</definiendum>
				<definiens id="0">a predicate that describes the relationship between two objects</definiens>
			</definition>
			<definition id="7">
				<sentence>speaker ( Agt ) : Agt is the current speaker .</sentence>
				<definiendum id="0">Agt</definiendum>
				<definiens id="0">the current speaker</definiens>
			</definition>
			<definition id="8">
				<sentence>into two steps : s-refer , which expresses the speaker 's intention to refer , and describe , which accounts for the content of the referring expression ( given next ) .</sentence>
				<definiendum id="0">describe</definiendum>
				<definiens id="0">expresses the speaker 's intention to refer</definiens>
				<definiens id="1">accounts for the content of the referring expression ( given next )</definiens>
			</definition>
			<definition id="9">
				<sentence>The decomposition consists of s-reject , which takes as its parameter the surface speech actions that are in the yield of the problematic action .</sentence>
				<definiendum id="0">decomposition</definiendum>
				<definiens id="0">consists of s-reject , which takes as its parameter the surface speech actions that are in the yield of the problematic action</definiens>
			</definition>
			<definition id="10">
				<sentence>bmb ( system , user , goal ( system , bel ( user , bel ( system , error ( p l , p22 ) ) ) ) ) ( 19 ) Then by Rule 2 , which captures the cooperativity of the agents in communicative goals , it adds the belief that it is mutually believed that the system believes there is an error .</sentence>
				<definiendum id="0">bel</definiendum>
				<definiens id="0">captures the cooperativity of the agents in communicative goals</definiens>
			</definition>
			<definition id="11">
				<sentence>21 We assume that the parser determines the appropriate discourse entities in these actions : entity1 is the discourse entity for the object being referred to , and entity3 is another discourse entity .</sentence>
				<definiendum id="0">entity3</definiendum>
				<definiens id="0">the parser determines the appropriate discourse entities in these actions : entity1 is the discourse entity for the object being referred to , and</definiens>
			</definition>
</paper>

		<paper id="3004">
			<definition id="0">
				<sentence>We consider written languages , and for the purpose of this paper , a word is a string of letters delimited by spaces or punctuation .</sentence>
				<definiendum id="0">word</definiendum>
				<definiens id="0">a string of letters delimited by spaces or punctuation</definiens>
			</definition>
			<definition id="1">
				<sentence>A reduction of the ambiguity level of an ambiguous word w , with k morphological analyses : A1 ... .. Ak , occurs when it is possible to select from A1 ... .. Ak , a proper subset of I analyses 1 G 1 &lt; k , such that the right analysis of w is one of these 1 analyses .</sentence>
				<definiendum id="0">Ak</definiendum>
				<definiens id="0">morphological analyses : A1 ... .. Ak , occurs when it is possible to select from A1 ... ..</definiens>
			</definition>
			<definition id="2">
				<sentence>For a word w with k analyses , A1 , ... , Ak , the morpho-lexical probability of Ai is the estimate of the conditional probability P ( Ai \ [ w ) from the given corpus , i.e. , Pi = P ( ai \ ] w ) = no. of times Ai was the right analysis of w no. of occurrences of w Note that Pi is the probability that Ai is the right analysis of w independently of the context in which w appears .</sentence>
				<definiendum id="0">A1 , ... , Ak</definiendum>
				<definiens id="0">the estimate of the conditional probability P ( Ai \ [ w ) from the given corpus , i.e. , Pi = P ( ai \ ] w ) = no. of times Ai was the right analysis of w no. of occurrences of w Note that Pi is the probability that Ai is the right analysis of w independently of the context in which w appears</definiens>
			</definition>
			<definition id="3">
				<sentence>Part-of-speech tagging -- deciding the correct part of speech in the current context of the sentence -- has received major attention .</sentence>
				<definiendum id="0">Part-of-speech tagging --</definiendum>
				<definiens id="0">deciding the correct part of speech in the current context of the sentence -- has received major attention</definiens>
			</definition>
			<definition id="4">
				<sentence>Morphological ambiguity is a severe problem in modern Hebrew .</sentence>
				<definiendum id="0">Morphological ambiguity</definiendum>
			</definition>
			<definition id="5">
				<sentence>A morphological analysis of a word in Hebrew should extract the following information : • lexical entry 385 Computational Linguistics Volume 21 , Number 3 • category • tense ( for verbs only ) • attached particles ( i.e. , prepositions , connectives , determiners ) • status -- a flag indicating whether a noun is in its construct or absolute form • gender , number , and person ( for nouns , adjectives , verbs etc. ) • gender , number , and person of pronoun suffixes For example , the morphological analysis of the Hebrew string 1 ) D~I'~'lVd~I ( written in a Latin transliteration 2 WK $ RAYTYW ) is as follows : • lexical entry : R^H ( nN'l ) -- the verb 'to see ' • category : verb • tense : past • attached particles : W + K $ ( ~ + ~ ) = 'and when ' • gender : feminine/masculine , number : singular , person : first person • object pronoun : masculine , singular , third person Thus , WK $ R^YTYW should be translated into English as : 'and when I saw him . '</sentence>
				<definiendum id="0">R^H ( nN'l</definiendum>
				<definiens id="0">prepositions , connectives , determiners ) • status -- a flag indicating whether a noun is in its construct or absolute form • gender , number , and person ( for nouns , adjectives , verbs etc. ) • gender , number</definiens>
			</definition>
			<definition id="6">
				<sentence>These counters suggest that if we manually tagged the 200 occurrences of the string HQPH in the corpus , we would find that the first analysis of HQPH is the right one 18 times out of the 200 times that the word appears in the corpus , that the second analysis is the right one 180 times , and that the third analysis is the right analysis only twice .</sentence>
				<definiendum id="0">HQPH</definiendum>
				<definiens id="0">the right one 18 times out of the 200 times that the word appears in the corpus</definiens>
			</definition>
			<definition id="7">
				<sentence>Test-group1 consists only of very frequent word types in Hebrew , but the test-corpus probabilities for these word types can be viewed as a reliable estimate of the morpho-lexical probabilities .</sentence>
				<definiendum id="0">Test-group1</definiendum>
				<definiens id="0">consists only of very frequent word types in Hebrew , but the test-corpus probabilities for these word types can be viewed as a reliable estimate of the morpho-lexical probabilities</definiens>
			</definition>
			<definition id="8">
				<sentence>Formally , the mapping from the probability of an analysis to its category is done using two thresholds , upper threshold and lower threshold , as follows : 1 prob ~ upper threshold CAT ( prob ) = 2 prob ~ lower threshold 3 otherwise The quality of the approximated probabilities we acquire using our method is now measured by examining the proportion of words for which the estimated category for each of their analyses agrees with the category defined by the approximated probabilities .</sentence>
				<definiendum id="0">mapping from the probability of an</definiendum>
				<definiens id="0">measured by examining the proportion of words for which the estimated category for each of their analyses agrees with the category defined by the approximated probabilities</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>EDWARD ( Huls and Bos 1993 ; Bos et al. 1994 ) integrates a graphical graph-editor called Gr 2 ( Bos in press ) and a Dutch natural language ( NL ) dialogue system called DoNaLD ( Claassen and Huls 1991 ) .</sentence>
				<definiendum id="0">EDWARD</definiendum>
				<definiendum id="1">Bos</definiendum>
			</definition>
			<definition id="1">
				<sentence>Temporal deixis is realized by the tense system of a language ( e.g. , he lives in Amsterdam ) and by temporal modifiers ( e.g. , in an hour ) .</sentence>
				<definiendum id="0">Temporal deixis</definiendum>
				<definiens id="0">realized by the tense system of a language</definiens>
			</definition>
			<definition id="2">
				<sentence>Temporal deixis relates the time of speech to the relation ( s ) expressed by the utterance .</sentence>
				<definiendum id="0">Temporal deixis</definiendum>
				<definiens id="0">relates the time of speech to the relation ( s ) expressed by the utterance</definiens>
			</definition>
			<definition id="3">
				<sentence>Referents are presented by the name of the concept class they belong to , followed by the number sign ( # ) and a unique number enclosed in angle brackets , e.g. , &lt; directory # 4001 &gt; and &lt; spin-report # 4929 &gt; ( spin-reports are a special kind of project reports ) .</sentence>
				<definiendum id="0">Referents</definiendum>
				<definiens id="0">the name of the concept class they belong to , followed by the number sign ( # ) and a unique number enclosed in angle brackets</definiens>
			</definition>
			<definition id="4">
				<sentence>To be able to interpret referring expressions , EDWARD uses three knowledge sources : a knowledge base , a context model , and a lexicon .</sentence>
				<definiendum id="0">EDWARD</definiendum>
				<definiens id="0">uses three knowledge sources : a knowledge base , a context model</definiens>
			</definition>
			<definition id="5">
				<sentence>A CF is defined by a scope , which is a collection of individual instances ; a significance weight , represented by an integer ; and a decay function , which indicates by what amount the CF 's significance weight is to be decreased at the next update .</sentence>
				<definiendum id="0">CF</definiendum>
				<definiendum id="1">decay function</definiendum>
				<definiens id="0">a collection of individual instances ; a significance weight , represented by an integer</definiens>
				<definiens id="1">indicates by what amount the CF 's significance weight is to be decreased at the next update</definiens>
			</definition>
			<definition id="6">
				<sentence>The intension of the personal pronouns ik ( I ) and jij ( you ) is represented using the following predicates : ik -- * 3 ( x , y ) cognizer ( x ) Atalking-to ( x , y ) jq -- * 3 ( x , y ) cognizer ( x ) Atalking-to ( y , x ) where the predicate cognizer is taken from Pylyshyn ( 1984 ) , meaning any rational agent , e.g. , a person or a dialogue system , and talking-to is a predicate that represents the dialogue situation at any time .</sentence>
				<definiendum id="0">talking-to</definiendum>
				<definiens id="0">a predicate that represents the dialogue situation at any time</definiens>
			</definition>
			<definition id="7">
				<sentence>EDWARD uses the machine time as an anchoring point .</sentence>
				<definiendum id="0">EDWARD</definiendum>
				<definiens id="0">uses the machine time as an anchoring point</definiens>
			</definition>
			<definition id="8">
				<sentence>The graphics analyzer interprets the pointing gestures produced by the user .</sentence>
				<definiendum id="0">graphics analyzer</definiendum>
			</definition>
			<definition id="9">
				<sentence>The referent of dit ( this one ) in sentences ( 2a ) and ( 2b ) would be the most salient report at that moment , which would be the report about Gr2 in sentence ( 2a ) , but the report pointed to ( donald~report ) in sentence ( 2b ) .</sentence>
				<definiendum id="0">referent of dit</definiendum>
			</definition>
			<definition id="10">
				<sentence>A second mechanism , called centering ( or immediate focusing ) , is used for pronoun resolution .</sentence>
				<definiendum id="0">centering</definiendum>
				<definiens id="0">immediate focusing ) , is used for pronoun resolution</definiens>
			</definition>
</paper>

		<paper id="3005">
			<definition id="0">
				<sentence>( car p ) X ) ) ( list ( cdr p ) ) ' ( ) ) ) ) The expression ( seq fA fB ) evaluates to a function that maps a string position 1 to the set of string positions { ri } such that there exists an m 6 fA ( 1 ) , and ri 6 fB ( rrl ) .</sentence>
				<definiendum id="0">seq fA fB )</definiendum>
				<definiens id="0">evaluates to a function that maps a string position 1 to the set of string positions</definiens>
			</definition>
			<definition id="1">
				<sentence>( 6 ) ( define ( seq A B ) ( lambda ( p ) ( reduce union ' ( ) ( map B ( A p ) ) ) ) ) The expression ( alt fA fB ) evaluates to a function that maps a string position 1 to fa ( l ) U fB ( 1 ) .</sentence>
				<definiendum id="0">seq A B ) ( lambda</definiendum>
				<definiens id="0">A p ) ) ) ) ) The expression ( alt fA fB ) evaluates to a function that maps a string position 1</definiens>
			</definition>
			<definition id="2">
				<sentence>( 7 ) ( define ( alt A B ) ( lambda ( p ) ( union ( A p ) ( B p ) ) ) ) While terminal , seq , and alt suffice to define ( epsilon-free ) context-free grammars , we can easily define other useful higher-order functions .</sentence>
				<definiendum id="0">alt A B ) ( lambda</definiendum>
				<definiens id="0">A p ) ( B p ) ) ) ) While terminal , seq</definiens>
			</definition>
			<definition id="3">
				<sentence>It returns a list of the input string 's suffixes that correspond to the right string position of an S. ( 13 ) &gt; ( s ' ( Kim knows every student likes Sandy ) ) ( ( likes sandy ) ( ) ) In example ( 13 ) , the list resulting from the evaluation contains two suffixes , corresponding to the fact that both Kim knows every student and Kim knows every student likes Sandy can be analysed as Ss .</sentence>
				<definiendum id="0">Kim</definiendum>
				<definiens id="0">returns a list of the input string 's suffixes that correspond to the right string position of an S. ( 13 ) &gt; ( s ' ( Kim knows every student likes Sandy ) ) ( ( likes sandy )</definiens>
			</definition>
			<definition id="4">
				<sentence>The expression ( recognize words ) is true iff words is a list of words that can be analysed as an S , i.e. , if the empty string is a one of right string positions of an S whose left string position is the whole string to be recognized .</sentence>
				<definiendum id="0">true iff words</definiendum>
			</definition>
			<definition id="5">
				<sentence>408 Mark Johnson Memoization in Top-Down Parsing ( define S ( vacuous ( seq NP VP ) ) ) ; S -- ~NP VP ( define VP ( vacuous ( alt ( seq V NP ) ; VP-+VNP ( seq ( V S ) ) ) ) ) ; \ ] VS ( define NP ( vacuous ( alt PN ; NP -- *PN ( seq Det N ) ) ) ) ; \ [ DetN ( define PN ( alt ( terminal 'Kim ) ( terminal 'Sandy ) ) ) ( define V ( alt ( terminal 'likes ) ( terminal 'knows ) ) ) ( define Det ( alt ( terminal 'every ) ( terminal 'no ) ) ) ( define N ( alt ( terminal 'student ) ( terminal 'professor ) ) ) Figure 1 A CFG &amp; agmentdefined using the highe~orderconstructors .</sentence>
				<definiendum id="0">define Det ( alt</definiendum>
				<definiendum id="1">N ( alt</definiendum>
				<definiendum id="2">CFG</definiendum>
				<definiens id="0">vacuous ( alt ( seq V NP ) ; VP-+VNP ( seq ( V S ) ) ) ) )</definiens>
			</definition>
			<definition id="6">
				<sentence>Memoization is a standard technique for avoiding redundant computation , and as Norvig ( 1991 ) noted , it can be applied to top-down recognizers to convert exponentialtime recognizers into polynomial-time recognizers .</sentence>
				<definiendum id="0">Memoization</definiendum>
				<definiens id="0">a standard technique for avoiding redundant computation , and as Norvig ( 1991 ) noted , it can be applied to top-down recognizers to convert exponentialtime recognizers into polynomial-time recognizers</definiens>
			</definition>
			<definition id="7">
				<sentence>Each category A is associated with a function gA that represents the relation rA , i.e. , ( gA C I ) reduces ( in an applicative-order reduction ) in such a fashion that at some stage in the reduction the expression ( c r ) is reduced iff A can derive the substring spanning string positions I to r of the input string .</sentence>
				<definiendum id="0">relation rA</definiendum>
				<definiens id="0">an applicative-order reduction ) in such a fashion that at some stage in the reduction the expression ( c r ) is reduced iff A can derive the substring spanning string positions I to r of the input string</definiens>
			</definition>
			<definition id="8">
				<sentence>The recognition process begins by passing the function corresponding to the root category the string to be recognized , and a continuation ( to be evaluated after successful recognition ) that records the successful analysis .</sentence>
				<definiendum id="0">recognition process</definiendum>
				<definiens id="0">begins by passing the function corresponding to the root category the string to be recognized , and a continuation ( to be evaluated after successful recognition ) that records the successful analysis</definiens>
			</definition>
			<definition id="9">
				<sentence>In either case , the caller continuation needs to be stored in the continuations component of the table entry , so that it can receive any additional results produced by the unmemoized procedure .</sentence>
				<definiendum id="0">caller continuation</definiendum>
				<definiens id="0">needs to be stored in the continuations component of the table entry , so that it can receive any additional results produced by the unmemoized procedure</definiens>
			</definition>
			<definition id="10">
				<sentence>( 27 ) ( 28 ) ( define ( make-table ) ( list '~head~ ) ) ( define ( table-ref table key ) ( let ( ( pair ( assoc key ( cdr table ) ) ) ) ( if pair ; an entry alreadyexists ( cdr pair ) ; ~turnit ( let ( ( new-entry ( make-entry ) ) ) ( set-cdr !</sentence>
				<definiendum id="0">make-table )</definiendum>
				<definiens id="0">table-ref table key )</definiens>
			</definition>
			<definition id="11">
				<sentence>table ( cons ( cons key new-entry ) ( cdr table ) ) ) new-entry ) ) ) ) Entries are manipulated by the following procedures .</sentence>
				<definiendum id="0">table ( cons ( cons</definiendum>
				<definiens id="0">key new-entry ) ( cdr table ) ) ) new-entry ) ) ) ) Entries are manipulated by the following procedures</definiens>
			</definition>
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>An item is a triplet that represents an X structure ~ : &lt; surface-string , attributevalues , source-messages &gt; , where surface-string is an integer interval \ [ i , j\ ] denoting the i'th to j'th word in the input sentence ; attribute-values specifies syntactic features of the root node ( fl ) ; and source-messages is a set of messages that represent immediate constituents of fl and from which this item is combined .</sentence>
				<definiendum id="0">surface-string</definiendum>
				<definiendum id="1">source-messages</definiendum>
				<definiens id="0">an integer interval \ [ i , j\ ] denoting the i'th to j'th word in the input sentence ; attribute-values specifies syntactic features of the root node</definiens>
				<definiens id="1">a set of messages that represent immediate constituents of fl and from which this item is combined</definiens>
			</definition>
			<definition id="1">
				<sentence>The Case Filter rules out sentences containing an NP with no case .</sentence>
				<definiendum id="0">Case Filter</definiendum>
				<definiens id="0">rules out sentences containing an NP with no case</definiens>
			</definition>
			<definition id="2">
				<sentence>The grammar network construction algorithm consists of two steps : the first defines the basic structural description ( i.e. , bar-level nodes ) ; and the second defines the satellites ( i.e. , adjunct and specifier nodes ) .</sentence>
				<definiendum id="0">grammar network construction algorithm</definiendum>
			</definition>
			<definition id="3">
				<sentence>261 Computational Linguistics Volume 21 , Number 2 Consider the following English and Korean sentences : 5 . . . Structural Divergence : E : John married Sally K : John-i Sally-wa kyelhonhayssta -Nom -with married 'John married with Sally ' Conflational Divergence : E : John helped Bill K : John-i Bill-eykey towum-ul cwuessta -Nom -Dative help-Acc gave 'John gave help to Bill ' Categorial Divergence : E : John is fond of music K : John-un umak-ul coahanta -Nom music-Acc like 'It is John ( who ) likes music ' .15 seconds .12 seconds .10 seconds .19 seconds .12 seconds .07 seconds In general , the times demonstrate a speedup of two to three orders of magnitude over previous principle-based parsers on analogous examples such as those given in Dorr ( 1993 ) .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">E : John married Sally K : John-i Sally-wa kyelhonhayssta -Nom -with married 'John married with Sally ' Conflational Divergence : E : John helped Bill</definiens>
			</definition>
</paper>

		<paper id="4004">
			<definition id="0">
				<sentence>For example , one could accurately assign a part-of-speech tag to the word race in ( 1-3 ) without any reference to phrase structure or constituent movement : One would only have to realize that , usually , a word one or two words to the right of a modal is a verb and not a noun .</sentence>
				<definiendum id="0">modal</definiendum>
				<definiens id="0">a verb</definiens>
			</definition>
			<definition id="1">
				<sentence>Learning continues until no transformation can be found whose application results in an improvement to the annotated corpus .</sentence>
				<definiendum id="0">Learning</definiendum>
				<definiens id="0">continues until no transformation can be found whose application results in an improvement to the annotated corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>Learning stops when no transformations can be found whose application reduces errors beyond some prespecified threshold .</sentence>
				<definiendum id="0">Learning</definiendum>
				<definiens id="0">stops when no transformations can be found whose application reduces errors beyond some prespecified threshold</definiens>
			</definition>
			<definition id="3">
				<sentence>In figure 3 , we give pseudocode for the learning algorithm in the case where there is only one transformation template : Change the tag from X to Y if the previous tag is Z. In each learning iteration , the entire training corpus is examined once for every pair of tags X and Y , finding the best transformation whose rewrite changes tag X to tag Y. For every word in the corpus whose environment matches the triggering environment , if the word has tag X and X is the correct tag , then making this transformation will result in an additional tagging error , so we increment the number of errors caused when making the transformation given the part-of-speech tag of the previous word ( lines 8 and 9 ) .</sentence>
				<definiendum id="0">X</definiendum>
			</definition>
			<definition id="4">
				<sentence>If X is the current tag and Y is the correct tag , then the transformation will result in one less error , so we increment the number of improvements caused when making the transformation given the part-of-speech tag of the previous word ( lines 6 and 7 ) .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">the current tag</definiens>
			</definition>
			<definition id="5">
				<sentence>The tenth transformation is for the token 's , which is a separate token in the Penn Treebank .</sentence>
				<definiendum id="0">tenth transformation</definiendum>
				<definiens id="0">a separate token in the Penn Treebank</definiens>
			</definition>
			<definition id="6">
				<sentence>The The The The t. The preceding ( following ) word is w. The word two before ( after ) is w. One of the two preceding ( following ) words is w. current word is w and the preceding ( following ) word is x. current word is w and the preceding ( following ) word is tagged z. current word is w. preceding ( following ) word is w and the preceding ( following ) tag is The current word is w , the preceding ( following ) word is w2 and the preceding ( following ) tag is t. where w and x are variables over all words in the training corpus , and z and t are variables over all parts of speech .</sentence>
				<definiendum id="0">tag</definiendum>
				<definiens id="0">w. current word is w and the preceding ( following ) word is x. current word is w and the preceding ( following ) word is tagged z. current word is w. preceding ( following ) word is w and the preceding ( following )</definiens>
				<definiens id="1">variables over all words in the training corpus , and z and t are variables over all parts of speech</definiens>
			</definition>
			<definition id="7">
				<sentence>Also , absolutely no English-specific information ( such as an affix list ) need be prespecified in the learner .</sentence>
				<definiendum id="0">English-specific information</definiendum>
				<definiens id="0">such as an affix list</definiens>
			</definition>
</paper>

		<paper id="4005">
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>Although supporting many different linguistic formalisms , its straightforwardness and elegance have perhaps best been demonstrated with definite clause grammars ( DCGs ) ( Pereira and Warren 1980 ) , an extension to PROLOG 's syntax allowing direct implementation of rules of context-free grammars as Horn clauses .</sentence>
				<definiendum id="0">DCGs )</definiendum>
			</definition>
			<definition id="1">
				<sentence>Which dependents a particular word takes depends not only on its function within the sentence , but also on its meaning -- like other contemporary linguistic frameworks , DG integrates both syntactic and semantic aspects of natural language .</sentence>
				<definiendum id="0">DG</definiendum>
				<definiens id="0">integrates both syntactic and semantic aspects of natural language</definiens>
			</definition>
			<definition id="2">
				<sentence>Whereas context-free grammars differentiate between terminals ( coding the words of a language ) and non-terminals ( representing the constituents that are to be expanded ) , the symbols of a DG uniformly serve both purposes : like terminals they must be part of the sentence to be accepted ( or generated ) , and like non-terminals , they call for additional constituents of the sentence .</sentence>
				<definiendum id="0">context-free grammars</definiendum>
				<definiens id="0">coding the words of a language ) and non-terminals ( representing the constituents that are to be expanded ) , the symbols of a DG uniformly serve both purposes : like terminals they must be part of the sentence to be accepted ( or generated ) , and like non-terminals , they call for additional constituents of the sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>n ( give , verb ( N ) ) -- &gt; n ( _ , noun ( N ) ) , \ [ n ( give , verb ( N ) ) \ ] , n ( _ , noun ( _ ) ) , n ( _ , noun ( _ ) ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">give , verb ( N ) ) -- &gt; n ( _ , noun ( N ) ) , \ [ n ( give , verb ( N ) ) \ ] , n ( _ , noun ( _ ) ) , n ( _ , noun ( _ ) )</definiens>
			</definition>
			<definition id="4">
				<sentence>However , as shown above , DUG is a hybrid grammar : although dependency rules are the backbone of the formalism , it allows the introduction of quasi non-terminals that are integrated into the grammar via references .</sentence>
				<definiendum id="0">DUG</definiendum>
				<definiens id="0">a hybrid grammar : although dependency rules</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>Noun-based disambiguation is a structured form of co-occurrence-based disambiguation , various forms of which are prominent in corpusbased work .</sentence>
				<definiendum id="0">Noun-based disambiguation</definiendum>
				<definiens id="0">a structured form of co-occurrence-based disambiguation , various forms of which are prominent in corpusbased work</definiens>
			</definition>
			<definition id="1">
				<sentence>Antonym co-occurrence is a frequent and pervasive phenomenon , and it takes place under highly restricted semantic and syntactic conditions ( Charles and Miller 1989 ; Justeson and Katz 1991 , 1992 ) .</sentence>
				<definiendum id="0">Antonym co-occurrence</definiendum>
				<definiens id="0">a frequent and pervasive phenomenon , and it takes place under highly restricted semantic and syntactic conditions</definiens>
			</definition>
			<definition id="2">
				<sentence>The APHB Corpus contains 64 sentences in which both young and old modify man ; e.g. , In Bihzad 's paintings we see people and animals as individuals -- rich men and poor men , old and young , the elders in the mosque and the herdsmen camping among their horses in the fields .</sentence>
				<definiendum id="0">APHB Corpus</definiendum>
				<definiens id="0">contains 64 sentences in which both young and old modify man</definiens>
			</definition>
			<definition id="3">
				<sentence>In contrast , the APHB Corpus contains no sentence in which both old and young modify house or houses .</sentence>
				<definiendum id="0">APHB Corpus</definiendum>
				<definiens id="0">contains no sentence in which both old and young modify house or houses</definiens>
			</definition>
			<definition id="4">
				<sentence>The Appendix gives the formula needed to project , from the disambiguated subcorpora to the corpus at large , the probability of each sense of the target adjective given the noun it modifies .</sentence>
				<definiendum id="0">Appendix</definiendum>
				<definiens id="0">gives the formula needed to project , from the disambiguated subcorpora to the corpus at large</definiens>
			</definition>
			<definition id="5">
				<sentence>Lengthrelated ( 'not long ' ) senses of short are indicated by nouns that are +time period ( term , period , day , duration , minute , month , night , time , weekend , in the co-occurrence sentences ) , but this attribute is subsumable under -concrete .</sentence>
				<definiendum id="0">Lengthrelated</definiendum>
				<definiens id="0">indicated by nouns that are +time period ( term , period , day , duration , minute , month , night , time , weekend , in the co-occurrence sentences</definiens>
			</definition>
			<definition id="6">
				<sentence>This indicator is found in statements of the form It BE ADJ + infinitival clause , where BE is a form of the verb to be .</sentence>
				<definiendum id="0">BE</definiendum>
				<definiens id="0">a form of the verb to be</definiens>
			</definition>
			<definition id="7">
				<sentence>In this table , nl is the number of instances of sense $ 1 of a target in the 100-sentence sample for that target , and n2 is the number of such instances for the other sense ; P { Si \ ] F } is the corresponding probability of the sense Si of the target in the samples .</sentence>
				<definiendum id="0">n2</definiendum>
				<definiens id="0">the number of such instances for the other sense</definiens>
			</definition>
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>A third interesting project has produced the TINA system ( Seneff 1992 ) , which uses probabilistic networks to parse token sequences provided by a speech recognition system , SUMMIT , created by Zue et al. ( 1989 ) .</sentence>
				<definiendum id="0">TINA system</definiendum>
				<definiens id="0">uses probabilistic networks to parse token sequences provided by a speech recognition system</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , after the computer produces an utterance that is an attempt to have a specific task step S performed , there are expectations for any of the following types of responses : ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) A statement about missing or uncertain background knowledge necessary for the accomplishment of S. A statement about a subgoal of S. A statement about the underlying purpose for S. A statement about ancestor task steps of which accomplishment of S is a part .</sentence>
				<definiendum id="0">utterance</definiendum>
				<definiens id="0">a part</definiens>
			</definition>
			<definition id="2">
				<sentence>The ZmodSubdialog routine is a Prolog-style interpreter with a number of special features designed for the dialog processing application .</sentence>
				<definiendum id="0">ZmodSubdialog routine</definiendum>
				<definiens id="0">a Prolog-style interpreter with a number of special features designed for the dialog processing application</definiens>
			</definition>
			<definition id="3">
				<sentence>measurevoltage ( X , Y , V ) *-find ( voltmeter ) , 294 Smith , Hipp , and Biermann An Architecture for Voice Dialog Systems set ( voltmeterscale,20 ) , connect ( bw , com , X ) , connect ( rw , + , Y ) , vocalize ( read ( voltmeter , V ) ) set ( voltmeterscale,20 ) connect ( W , X , Y ) +connectend ( W , X ) , connectend ( W , Y ) General Dialog Rules Y +-usercan ( Y ) , vocalize ( Y ) vocalize ( Y ) find ( Y ) ~ vocalize ( find ( Y ) ) User Modeling Rules find ( knob ) usercan ( adjust ( knob , X ) ) usercan ( measurevoltage ( X , Y , V ) ) usercan ( find ( voltmeter ) ) usercan ( connect ( X , Y , Z ) ) usercan ( connectend ( X , Y ) ) We assume that the machine has selected a new goal that comes from the domain processor : Tl_circuit_Test2 ( V ) , where V is a voltage to be returned by the test .</sentence>
				<definiendum id="0">knob , X ) ) usercan ( measurevoltage ( X , Y , V )</definiendum>
				<definiendum id="1">V</definiendum>
				<definiens id="0">a voltage to be returned by the test</definiens>
			</definition>
			<definition id="4">
				<sentence>Device / I \ Subsystem1 Subsystem 2 Subsystem 3 For example , in the implemented system , the top level device is a circuit called the RSl11 ; its subsystems are the power circuit , the T1 and T2 circuits , and the LED circuit .</sentence>
				<definiendum id="0">subsystems</definiendum>
				<definiens id="0">a circuit called the RSl11</definiens>
			</definition>
			<definition id="5">
				<sentence>U is the sum of the edit costs required to traverse the graph path and match a grammatical input .</sentence>
				<definiendum id="0">U</definiendum>
				<definiens id="0">the sum of the edit costs required to traverse the graph path and match a grammatical input</definiens>
			</definition>
			<definition id="6">
				<sentence>The original hypothesized combining function for U and E was C = flU+ ( 1 fl ) E where fl is a weighting factor between 0 and 1 .</sentence>
				<definiendum id="0">fl</definiendum>
				<definiens id="0">a weighting factor between 0 and 1</definiens>
			</definition>
			<definition id="7">
				<sentence>Reflecting this experimentally determined result , the cost computation was revised to C= ~ E ifU=U~n \ oo Otherwise where U~n is the smallest observed utterance cost for the given utterance .</sentence>
				<definiendum id="0">U~n</definiendum>
				<definiens id="0">the smallest observed utterance cost for the given utterance</definiens>
			</definition>
			<definition id="8">
				<sentence>The grammar used by the parser consists of 491 rules and 263 dictionary entries .</sentence>
				<definiendum id="0">grammar used by the parser</definiendum>
				<definiens id="0">consists of 491 rules and 263 dictionary entries</definiens>
			</definition>
			<definition id="9">
				<sentence>Grosz and Sidner introduce the relation of `` dominance '' between two DSPs ; one DSP dominates another if the other `` is intended to provide part of the satisfaction '' of the first .</sentence>
				<definiendum id="0">DSP</definiendum>
				<definiens id="0">intended to provide part of the satisfaction '' of the first</definiens>
			</definition>
			<definition id="10">
				<sentence>As a trivial example , in our theory applied to the block-stacking world , the DSP buildtower ( A on B on C on ... on X ) is a legal goal regardless of how many blocks are specified to be stacked .</sentence>
				<definiendum id="0">DSP buildtower</definiendum>
			</definition>
			<definition id="11">
				<sentence>Their solution is to propose the identity V x ( ( fluid ( x ) A etq ( x ) ) lube-oil ( x ) ) where etcl ( x ) specifies a set of additional characteristics of x needed to ensure the identity .</sentence>
				<definiendum id="0">etq</definiendum>
				<definiendum id="1">etcl ( x )</definiendum>
				<definiens id="0">specifies a set of additional characteristics of x needed to ensure the identity</definiens>
			</definition>
			<definition id="12">
				<sentence>`` TINA : A natural language system for spoken language applications . ''</sentence>
				<definiendum id="0">TINA</definiendum>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>If a discourse is multi-party ( e.g. , a dialogue ) , then the DSP for a given segment is an intention of the conversational participant who initiates that segment .</sentence>
				<definiendum id="0">multi-party</definiendum>
				<definiens id="0">an intention of the conversational participant who initiates that segment</definiens>
			</definition>
			<definition id="1">
				<sentence>More specifically , the initial utterance ( a ) in each segment could begin a segment about an individual named 'John ' or one about John 's favorite music store or one about the fact that John wants to buy a piano .</sentence>
				<definiendum id="0">initial utterance</definiendum>
				<definiens id="0">begin a segment about an individual named 'John ' or one about John 's favorite music store</definiens>
			</definition>
			<definition id="2">
				<sentence>Realizes is a generalization of directly realizes .</sentence>
				<definiendum id="0">Realizes</definiendum>
			</definition>
			<definition id="3">
				<sentence>More importantly , when NP is a pronoun , the principles that determine the c 's for which it is the case that NP directly realizes c do not derive exclusively from syntactic , semantic , or pragmatic factors .</sentence>
				<definiendum id="0">NP</definiendum>
				<definiens id="0">a pronoun , the principles that determine the c 's for which it is the case that NP directly realizes c do not derive exclusively from syntactic , semantic , or pragmatic factors</definiens>
			</definition>
			<definition id="4">
				<sentence>209 Computational Linguistics Volume 21 , Number 2 purposes , we will use the following schematic to refer to the centers of utterances in a sequence : For Un : Cb ( Un ) = a , Cf ( Un ) - ( el , e2 , ... ep ) , a = ek , for some k. For Un+t : Cb ( Unq-1 ) realizes ern and , for all j , j &lt; m , e } is not realized in Un+l ; i.e. , em is realized in Un+l , and no higher ranked ej is realized in U~+I. Finally , we also define three types of transition relations across pairs of utterances. . . . CENTER CONTINUATION : Cb ( Un+l ) = Cb ( Un ) , and this entity is the most highly ranked element of Cf ( Un+l ) .</sentence>
				<definiendum id="0">Cb</definiendum>
			</definition>
			<definition id="5">
				<sentence>A third complication arises in the application of Rule 1 in sequences in which the CB of an utterance is realized but not directly realized in that utterance .</sentence>
				<definiendum id="0">third complication</definiendum>
				<definiens id="0">arises in the application of Rule 1 in sequences in which the CB of an utterance is realized but not directly realized in that utterance</definiens>
			</definition>
			<definition id="6">
				<sentence>On Sidner 's account , Carl is the actor focus after ( 34b ) and Jeff is the discourse focus .</sentence>
				<definiendum id="0">Carl</definiendum>
				<definiendum id="1">Jeff</definiendum>
				<definiens id="0">the discourse focus</definiens>
			</definition>
</paper>

	</volume>
