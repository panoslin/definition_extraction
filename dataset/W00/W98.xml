<?xml version="1.0" encoding="UTF-8"?>
	<volume id="W98">

		<paper id="0210">
			<definition id="0">
				<sentence>• Trend Predicates : Remain-constant I \ [ considerably I slightly\ ] Increase , Decrease \ [ : from : to\ ] I Drop , Fall , Rise \ [ : from : to\ ] I Reach-aPlateau I Fluctuate , Trend Predicates apply only to time series ( a set of data ordered chronologically ) , e.g. , Production of television sets in Russia fell from The first three requirements described in the Introduction ( representing quantitative and temporal relations and aggregate properties , com4Economics : The Economist ( March-August 1996 ) .</sentence>
				<definiendum id="0">com4Economics</definiendum>
				<definiendum id="1">Economist</definiendum>
				<definiens id="0">from : to\ ] I Reach-aPlateau I Fluctuate , Trend Predicates apply only to time series ( a set of data ordered chronologically</definiens>
			</definition>
			<definition id="1">
				<sentence>71 positionality , and representing certain pragmatic distinctions ) led us to make use of a firstorder logic with restricted quantification ( RQFOL ) , which has been used for representing the meaning of natural language queries involving complex referring expressions \ [ Woods1983 , Webber1983\ ] .</sentence>
				<definiendum id="0">RQFOL</definiendum>
				<definiens id="0">has been used for representing the meaning of natural language queries involving complex referring expressions \ [ Woods1983</definiens>
			</definition>
			<definition id="2">
				<sentence>x National-only ) ) ) ) ) In general , each element of the Referents list has the form ( term description ) , where term is a variable or a database object identifier ; and term denotes a discourse entity .</sentence>
				<definiendum id="0">term</definiendum>
			</definition>
			<definition id="3">
				<sentence>The COMET \ [ Feiner and McKeown1991\ ] and WIP \ [ Wahlster et ah1993\ ] systems generate instructions for operating physical devices , and \ [ Maybury1991\ ] describes a system that designs narrated or animated route directions in a cartographic information system .</sentence>
				<definiendum id="0">COMET</definiendum>
				<definiens id="0">describes a system that designs narrated or animated route directions in a cartographic information system</definiens>
			</definition>
</paper>

		<paper id="1412">
			<definition id="0">
				<sentence>In the above query , Goal is the observation to be proved by the abductive meta-interpreter , Assumable is a set of literals that may be assumed , and Proved and Assumed are multisets of literals that were used or assumed , respectively , during the abductive proof of Goal .</sentence>
				<definiendum id="0">Goal</definiendum>
				<definiendum id="1">Assumable</definiendum>
				<definiens id="0">the observation to be proved by the abductive meta-interpreter</definiens>
				<definiens id="1">a set of literals that may be assumed , and Proved and Assumed are multisets of literals that were used or assumed , respectively , during the abductive proof of Goal</definiens>
			</definition>
</paper>

		<paper id="0903">
			<definition id="0">
				<sentence>A constraint disjunction would thus be a function that returns the disjunction of the outputs of its component constraints .</sentence>
				<definiendum id="0">constraint disjunction</definiendum>
				<definiens id="0">a function that returns the disjunction of the outputs of its component constraints</definiens>
			</definition>
			<definition id="1">
				<sentence>Specifically , if constraint Cx is defined as a weighted finite state machine T1 = ( ~E1 , ~2 , Qi , F1,81 , Ex ) , where E1 is 22 the alphabet of labels , E2 is the alphabet of weights , drawn from the natural numbers , Q1 is the set of states of the machine , F1 C Q1 is the final states , Sl is the start state , and E1 C Q1 × Y.q × Z,2 × Q1 is the set of edges , and constraint C2 is another weighted deterministic finite state machine T2 - ( ~1 , ~2 , Q2 , F2 , s2 , E2 ) , then the disjunction of the two constraints may be defined as follows : T = ( ~1 , ~2 , Q1 × Q2 , F1 × F2 , ( 81,82 ) , E &gt; , ( ( q1,1 , q2,1 &gt; , a , n , ( ql,2 , q2,2 ) &gt; 6 E iff ( ql,1 , al , nl , ql,2 &gt; E EiA ( q2,1 , a2 , n2 , q2,2 ) E E2A a -- -a I N a2A n = ( nl v n2 ) A possible notation for the disjunction of two constraints C1 and C2 is C1 v C2 , for example `` ( yce -- + vce ) V ( cont -- + cont ) '' .</sentence>
				<definiendum id="0">E2</definiendum>
				<definiendum id="1">Q1</definiendum>
				<definiendum id="2">F1 C Q1</definiendum>
				<definiendum id="3">Sl</definiendum>
				<definiens id="0">a weighted finite state machine T1 = ( ~E1 , ~2 , Qi , F1,81 , Ex ) , where E1 is 22 the alphabet of labels</definiens>
				<definiens id="1">the alphabet of weights , drawn from the natural numbers</definiens>
				<definiens id="2">the set of states of the machine ,</definiens>
				<definiens id="3">the final states</definiens>
				<definiens id="4">the start state</definiens>
				<definiens id="5">the set of edges , and constraint C2 is another weighted deterministic finite state machine T2 - ( ~1 , ~2 , Q2 , F2 , s2 , E2 ) , then the disjunction of the two constraints may be defined as follows : T = ( ~1 , ~2 , Q1 × Q2 , F1 × F2 , ( 81,82 ) , E &gt; , ( ( q1,1 , q2,1 &gt; , a , n , ( ql,2 , q2,2 ) &gt; 6 E iff ( ql,1 , al , nl , ql,2 &gt; E EiA ( q2,1 , a2 , n2 , q2,2 ) E E2A a -- -a I N a2A n = ( nl v n2 ) A possible notation for the disjunction of two constraints C1 and C2 is C1 v C2 , for example `` ( yce -- + vce ) V ( cont -- + cont ) ''</definiens>
			</definition>
			<definition id="2">
				<sentence>The formal definition of a complex constraint in terms of mutually unranked subconstraints is identical to the definition of a constraint disjunction , except that the weight n of a new edge is defined as the sum of the weights of the input edges nl and n2 rather than the disjunction : T = ( El , E : , Q1 × Q2 , F1 × F2 , ( sl , s2 ) , E ) , ( ( q1,1 , q2,1 ) , a , n , ( ql,2 , q2.2 ) ) E E iff ( ql,1 , al , nl , ql,2 ) E E1A ( q2,1 , a2 , n2 , q : ,2 ) E E2A a 1 N a2 -- -aA A possible notation for a complex constraint C combining mutually unranked constraints C1 and C2 is C1 + C2 , for example `` ( vce ~ vce ) + ( cont ~ cont ) '' .</sentence>
				<definiendum id="0">C2</definiendum>
				<definiens id="0">the sum of the weights of the input edges nl and n2 rather than the disjunction : T = ( El , E</definiens>
				<definiens id="1">a , n , ( ql,2 , q2.2 ) ) E E iff ( ql,1 , al , nl , ql,2 ) E E1A ( q2,1 , a2 , n2 , q : ,2</definiens>
				<definiens id="2">C1 + C2 , for example `` ( vce ~ vce ) + ( cont ~ cont ) ''</definiens>
			</definition>
			<definition id="3">
				<sentence>A local conjunction of constraints is defined as a constraint that outputs a violation for each domain of a specified type in which all of the component constraints are violated .</sentence>
				<definiendum id="0">local conjunction of constraints</definiendum>
				<definiens id="0">a constraint that outputs a violation for each domain of a specified type in which all of the component constraints are violated</definiens>
			</definition>
			<definition id="4">
				<sentence>The following algorithm computes the local conjunction of constraint C1 , where C1 is represented by the weighted finite state machine T1 = ( El , 22 , Q1 , Sl , F1 , El ) , with itself within 23 a domain 7 defined as the intersection of the domains '71 A..</sentence>
				<definiendum id="0">C1</definiendum>
			</definition>
			<definition id="5">
				<sentence>where G\ [ represents the beginning of domain '7 , G\ [ represents anything other than G\ [ , -4-6 represents the interior of the domain , \ ] G represents a boundary between two '7 domains , and \ ] G represents the end of the '7 domain .</sentence>
				<definiendum id="0">G\ [</definiendum>
				<definiendum id="1">G</definiendum>
				<definiens id="0">a boundary between two '7 domains , and \ ] G represents the end of the '7 domain</definiens>
			</definition>
</paper>

		<paper id="1227">
			<definition id="0">
				<sentence>Stochastic language models are also helpful in reducing the complexity of speech and language processing by way of providing probabilistic linguistic constraints ( Lee , 1989 ) .</sentence>
				<definiendum id="0">Stochastic language models</definiendum>
				<definiens id="0">helpful in reducing the complexity of speech and language processing by way of providing probabilistic linguistic constraints</definiens>
			</definition>
			<definition id="1">
				<sentence>The action part of an LR table consists of lookahead symbols and states .</sentence>
				<definiendum id="0">action part of an LR table</definiendum>
			</definition>
			<definition id="2">
				<sentence>For Va , b E VT ( the set of terminal symbols ) , the probabilistic connection matrix named PConnect is defined as follows .</sentence>
				<definiendum id="0">PConnect</definiendum>
				<definiens id="0">the set of terminal symbols ) , the probabilistic connection matrix named</definiens>
			</definition>
			<definition id="3">
				<sentence>PConnect ( a , b ) = P ( bla ) ( 3 ) where P ( bJa ) is a conditional probability and P ( bla ) = 1 .</sentence>
				<definiendum id="0">PConnect</definiendum>
				<definiendum id="1">P ( bJa )</definiendum>
				<definiens id="0">a conditional probability and P ( bla ) = 1</definiens>
			</definition>
			<definition id="4">
				<sentence>table An algorithm to construct a probabilistic LR table , combining both bigram and CFG constraints , is given in Algorithm I : Algorithm 1 Input : A CFG G = ( Vjv , VT , P , S ) and a probabilistic connection matrix PConnect .</sentence>
				<definiendum id="0">CFG constraints</definiendum>
				<definiens id="0">A CFG G = ( Vjv , VT , P ,</definiens>
			</definition>
			<definition id="5">
				<sentence>Output : An LR table T with CFG and big-ram constraints .</sentence>
				<definiendum id="0">Output</definiendum>
				<definiens id="0">An LR table T with CFG and big-ram constraints</definiens>
			</definition>
			<definition id="6">
				<sentence>_~2 ) x P ( 4 , bl , sh11 ) xP ( ll , a2 , shlO ) x P ( 10 , $ , re7 ) xP ( 14 , $ , re5 ) x P ( 13 , $ , re1 ) ×P ( 5 , $ , acc ) + P ( O , a2 , she ) x P ( 2 , bl , re7 ) xP ( 3 , bl , sh_66 ) x P ( 6 , a2 , reS ) xP ( 8 , ae , re3 ) x P ( 4 , ae , shlO ) xP ( 10 , $ , re7 ) x P ( 12 , $ , re4 ) xP ( 13 , $ , re1 ) x P ( 5 , $ , acc ) = 0.4 x 1.0 x 0.5 x 1.0 x 1.0 x 1.0 x 1.0 x 1.0 x 1.0 + 0.4 x 1.0 x 0.5 x 0.1 x 1.0 x 1.0 x 1.0 x 1.0 x 1.0 x 1.0 = 0.2 + 0.02 = 0.22 where P ( Treei ) means the probability of the ith parsing tree generated by the GLR parser and P ( S , L , A ) means the probability of an action A in state S with Iookahead L. On the other hand , using only bigram constraints , the probability P2 of the string `` ae b1 a , ~ ' is calculated as : P2 = P ( a2 bl a2 ) = P ( ael # ) × P ( bllae ) x P ( aelbl ) × P ( $ 1ae ) = x0.3 x0.1 x0.7 = 0.0084 The reason why P1 &gt; P2 can be explained as follows .</sentence>
				<definiendum id="0">)</definiendum>
				<definiens id="0">x 1.0 x 0.5 x 1.0 x 1.0 x 1.0 x 1.0 x 1.0 x 1.0 + 0.4 x 1.0 x 0.5 x 0.1 x 1.0 x 1.0 x 1.0 x 1.0 x 1.0 x 1.0 = 0.2 + 0.02 = 0.22 where P ( Treei ) means the probability of the ith parsing tree generated by the GLR parser and P ( S , L , A</definiens>
			</definition>
			<definition id="7">
				<sentence>Perplexity is a measure of the constraint imposed by the language model .</sentence>
				<definiendum id="0">Perplexity</definiendum>
				<definiens id="0">a measure of the constraint imposed by the language model</definiens>
			</definition>
			<definition id="8">
				<sentence>Test-set perplexity for a language model L is simply the geometric mean of probabilities defined by : where Q ( L ) = 2// ( L ) S ( L ) = logP ( S , ) i=l Here N is the number of terminal symbols in the test set , M is the number of test sentences and P ( S , ) is the probability of generating i-th test sentence Si .</sentence>
				<definiendum id="0">Test-set perplexity</definiendum>
				<definiendum id="1">M</definiendum>
				<definiendum id="2">P ( S , )</definiendum>
				<definiens id="0">the geometric mean of probabilities defined by : where Q ( L ) = 2// ( L ) S ( L ) = logP ( S , ) i=l Here N is the number of terminal symbols in the test set</definiens>
				<definiens id="1">the number of test sentences</definiens>
				<definiens id="2">the probability of generating i-th test sentence Si</definiens>
			</definition>
			<definition id="9">
				<sentence>The CFG used is a phrase contextfree grammar used in speech recognition tasks , and the number of rules and preterminals is 777 and 407 , respectively .</sentence>
				<definiendum id="0">CFG used</definiendum>
				<definiens id="0">a phrase contextfree grammar used in speech recognition tasks</definiens>
			</definition>
</paper>

		<paper id="1126">
			<definition id="0">
				<sentence>The event categorization task is a challenging test for the issues concerning collocations addressed in this paper .</sentence>
				<definiendum id="0">event categorization task</definiendum>
				<definiens id="0">a challenging test for the issues concerning collocations addressed in this paper</definiens>
			</definition>
			<definition id="1">
				<sentence>A collocational property is a set of constraints , PI Pp .</sentence>
				<definiendum id="0">collocational property</definiendum>
				<definiendum id="1">PI Pp</definiendum>
				<definiens id="0">a set of constraints</definiens>
			</definition>
			<definition id="2">
				<sentence>In wordsense disambiguation , for example , we might have an adjacent collocational property , defined by four constraints : P1 = being one word left of target word , P2 = being two words left of target word , P3 = being one word right of target word , P4 = being two words right of target word .</sentence>
				<definiendum id="0">wordsense disambiguation</definiendum>
				<definiens id="0">P1 = being one word left of target word , P2 = being two words left of target word , P3 = being one word right of target word , P4 = being two words right of target word</definiens>
			</definition>
			<definition id="3">
				<sentence>Finally , a collocation word is a potential collocation word that is judged to be correlated with the classification , according to a metric such as conditional probability , an information theoretic criterion , or a goodness-of-fit test .</sentence>
				<definiendum id="0">collocation word</definiendum>
				<definiens id="0">a potential collocation word that is judged to be correlated with the classification , according to a metric such as conditional probability , an information theoretic criterion , or a goodness-of-fit test</definiens>
			</definition>
			<definition id="4">
				<sentence>E.g. , `` She is/seems happy '' A potential collocation word is a word that satisfies one of the constraints .</sentence>
				<definiendum id="0">E.g.</definiendum>
				<definiens id="0">a word that satisfies one of the constraints</definiens>
			</definition>
			<definition id="5">
				<sentence>A binary feature is defined for each word in each set WordsSj , l &lt; j &lt; s. This organization is used by , for example , Ng &amp; Lee 1996. One feature is defined per subproperty Sj. It has I WordsSj I +l values , one value for each word in IVordsSj , corresponding to the presence of that word. Each feature also has a value for the absence of any word in WordsSj. E.g. , for both CO and SP collocations , there is one feature for adjectives and one for verbs. The adjective feature has a value for each selected adjective , and a value for none of them occurring. ( The verb feature is analogous. ) There is one binary feature for each class Ci , whose value is 1 if any member of any of the sets WordsCiSj appears in the sentence , 1 _ &lt; j &lt; s. 1Because all models have the same degrees of freedom , ranking values based on the raw G 2 value is equivalent to rank based on a significance test. 228 For each subproperty Sj , a feature is defined with c + 1 values as follows. There is one value for each class Ci , corresponding to the presence of a word in WordsCiSj. Each feature also has a value for the absence of any of those words. E.g. , for both CO and SP collocations , there is one feature for adjectives and one for verbs. The adjective feature has one value for each class , corresponding to the presence of any of the adjectives chosen for that class ; there is also a value for the absence of any of them. ( The verb feature is analogous. ) Note that , in the over-range organizations , increasing the number of words increases the complexity of the event space , in ORe by increasing the number of feature values and in ORb by increasing the number of features. These increases in complexity can worsen accuracy and computation time ( Goldberg 1995 , Bruce etal. 1996 , Cohen 1996 ) . The per-class organizations allow the number of collocation words to be increased without a corresponding increase in complexity. The algorithms included in this study are representative of the major types suggested by Michie et al. ( 1994 ) of the StatLog project comparing machine learning algorithms. ( 1 ) PEBLS , a K-Nearest Neighbor algorithm ( Cost and Salzberg 1993 ) ; ( 2 ) C4.5 , a decision tree algorithm ( Quinlan 1994 ) ; ( 3 ) Ripper , an inductive rule based classifier ( Cohen 1996 ) ; ( 4 ) the Naive Bayes classifier ; and ( 5 ) , a probabilistic model search procedure ( Bruce &amp; Wiebe 1994 ) using the public domain software CoCo ( Badsberg 1995 ) . Linear discriminant classifiers are omitted because they are not appropriate for categorical data. Neural network classifiers are omitted as well. Figure 1 presents the accuracy of~ach of the machine learning algorithms on each combination of collocational property and feature organization. Table 1 shows the mean accuracy across algorithms. In addition to collocational features , all experiments included seven other ( automatically determined ) features , such as position in the paragraph. Two main modiORe ORb PCb PCe CO .690 .719 .584 .607 SP .698 .710 .737 .746 Table 1 : Mean Accuracy Across Algorithms fications of Wiebe et al. ( 1997a ) were made to facilitate the comparisons at issue here. First , nouns were originally included in the CO but not the SP collocational property. Here , they are not included in either. Second , a weakness in the method for selecting the collocation sets is changed so that , for each collocational property , the words in the sets WordsCiSj are identical for both per-class experiments. The data consists of 2,544 main clauses from the Wall Street Journal Treebank corpus ( Marcus et al. , 1993 ) . 2 There are six classes , and the lower bound for the classification problem -- the frequency in the data set of the most frequent class -- is 52 % . 10-fold cross-validation was performed. All experiments were independent , so that , for each fold , the collocations were determined and rule induction or model search , etc. , was performed anew on the training set. We performed an analysis of variance to detect significant differences in accuracy considering algorithm , collocational property , and feature organization. When there are , we performed post-hoc analyses ( using Tukey 's HSD , to control for multiple comparison error rates ( SAS Institute 1989 ) ) to identify the differences. The algorithms differ in accuracy , i.e. , the analysis shows there is a significant main effect of algorithm on accuracy ( p &lt; 0.0001 ) . Posthoc analysis shows that there is only one significant difference : the lower performance of PEBLS relative to the others. However , the pattern of interaction between algorithm and features is extremely consistent across algorithms. The analysis shows that there is no higher level interaction between algorithm , on the one hand , and collocational prop~The Treebank syntax trees are used only to identify the main clause. This must be done only because the problem is defined as classifying the main clause. 229 u 0.7 '' t~ U 0.5Bayes CoCo c4.5 PEBLS Ripper \ [ \ ] CO-ORe \ [ \ ] CO-ORb \ [ \ ] CO-PCb \ [ \ ] CO-PCe \ [ \ ] SP-ORe \ [ \ ] SP-ORb \ [ \ ] SP-PCb \ [ \ ] SP-PCe Figure 1 : Accuracy of Machine Learning Algorithms ( means across folds ) erty and organization , on the other ( p &gt; 0.996 ) .</sentence>
				<definiendum id="0">binary feature</definiendum>
				<definiens id="0">each word in each set WordsSj , l &lt; j &lt; s. This organization is used by , for example , Ng &amp; Lee 1996. One feature is defined per subproperty Sj. It has I WordsSj I +l values , one value for each word in IVordsSj , corresponding to the presence of that word. Each feature also has a value for the absence of any word in WordsSj. E.g. , for both CO and SP collocations , there is one feature for adjectives and one for verbs. The adjective feature has a value for each selected adjective , and a value for none of them occurring. ( The verb feature is analogous. ) There is one binary feature for each class Ci , whose value is 1 if any member of any of the sets WordsCiSj appears in the sentence , 1 _ &lt; j &lt; s. 1Because all models have the same degrees of freedom , ranking values based on the raw G 2 value is equivalent to rank based on a significance test. 228 For each subproperty Sj , a feature is defined with c + 1 values as follows. There is one value for each class Ci , corresponding to the presence of a word in WordsCiSj. Each feature also has a value for the absence of any of those words. E.g. , for both CO and SP collocations , there is one feature for adjectives and one for verbs. The adjective feature has one value for each class , corresponding to the presence of any of the adjectives chosen for that class ; there is also a value for the absence of any of them. ( The verb feature is analogous. ) Note that , in the over-range organizations , increasing the number of words increases the complexity of the event space , in ORe by increasing the number of feature values and in ORb by increasing the number of features. These increases in complexity can worsen accuracy and computation time ( Goldberg 1995 , Bruce etal. 1996 , Cohen 1996 ) . The per-class organizations allow the number of collocation words to be increased without a corresponding increase in complexity. The algorithms included in this study are representative of the major types suggested by Michie et al. ( 1994 ) of the StatLog project comparing machine learning algorithms. ( 1 ) PEBLS , a K-Nearest Neighbor algorithm ( Cost and Salzberg 1993 ) ; ( 2 ) C4.5 , a decision tree algorithm ( Quinlan 1994 ) ; ( 3 ) Ripper , an inductive rule based classifier ( Cohen 1996 ) ; ( 4 ) the Naive Bayes classifier ; and ( 5 ) , a probabilistic model search procedure ( Bruce &amp; Wiebe 1994 ) using the public domain software CoCo ( Badsberg 1995 ) . Linear discriminant classifiers are omitted because they are not appropriate for categorical data. Neural network classifiers are omitted as well. Figure 1 presents the accuracy of~ach of the machine learning algorithms on each combination of collocational property and feature organization. Table 1 shows the mean accuracy across algorithms. In addition to collocational features , all experiments included seven other ( automatically determined ) features , such as position in the paragraph. Two main modiORe ORb PCb PCe CO .690 .719 .584 .607 SP .698 .710 .737 .746 Table 1 : Mean Accuracy Across Algorithms fications of Wiebe et al. ( 1997a ) were made to facilitate the comparisons at issue here. First , nouns were originally included in the CO but not the SP collocational property. Here , they are not included in either. Second , a weakness in the method for selecting the collocation sets is changed so that , for each collocational property , the words in the sets WordsCiSj are identical for both per-class experiments. The data consists of 2,544 main clauses from the Wall Street Journal Treebank corpus ( Marcus et al. , 1993 ) . 2 There are six classes , and the lower bound for the classification problem -- the frequency in the data set of the most frequent class -- is 52 % . 10-fold cross-validation was performed. All experiments were independent , so that , for each fold , the collocations were determined and rule induction or model search , etc. , was performed anew on the training set. We performed an analysis of variance to detect significant differences in accuracy considering algorithm , collocational property , and feature organization. When there are , we performed post-hoc analyses ( using Tukey 's HSD , to control for multiple comparison error rates ( SAS Institute 1989 ) ) to identify the differences. The algorithms differ in accuracy</definiens>
			</definition>
</paper>

		<paper id="1433">
			<definition id="0">
				<sentence>_ • The GoalGetter system is a Data-to-Speech system which generates spoken soccer reports ( in Dutch ) on the basis of tabular data .</sentence>
				<definiendum id="0">GoalGetter system</definiendum>
				<definiens id="0">a Data-to-Speech system which generates spoken soccer reports ( in Dutch ) on the basis of tabular data</definiens>
			</definition>
			<definition id="1">
				<sentence>The LGM takes data as input .</sentence>
				<definiendum id="0">LGM</definiendum>
				<definiens id="0">takes data as input</definiens>
			</definition>
</paper>

		<paper id="0613">
			<definition id="0">
				<sentence>The premise of our approach is that relations in our ontology 1 coincide with the relations of semantic contiguity at some level , thus the task of the metonymy resolution/WSD process is to identify the nature of contiguity in each case by identifying the best path in the ontology from the candidate meaning of a word to a constraining concept ( see Mahesh et al. ( 1997 ) for a discussion of the richness and specificity of semantic constraints in our approach , which projected an average of 15 constraints on each openclass word in our Spanish test corpus ) .</sentence>
				<definiendum id="0">resolution/WSD process</definiendum>
				<definiendum id="1">Spanish test corpus</definiendum>
				<definiens id="0">to identify the nature of contiguity in each case by identifying the best path in the ontology from the candidate meaning of a word to a constraining concept</definiens>
			</definition>
			<definition id="1">
				<sentence>For the above example , the most specific information that is available from the path ( identifiable by following SUBCLASSES arcs after the metonymie arc ) is utilized in making an inference about the replaced metonym and instantiating an appropriate concept % ENGINE-PROPELLED-VEHICLE460 ( the TMR is our interlingua or meaning representation language ) : THR : ( DRIVE435 ( AGENT ( VALUE PERSON440 ) ) ; abbreviated of course ( THEME ( SEM *ENGINE-PROPELLED-VEHICLE ) ( VALUE ( source FOR-PROFIT-MANUFACTURING-CORP417 ) ( inference inference306 ENGINE-PROPELLED-VEHICLE460 ) ) ) ) ( PERSON440 ~ ( NAME $ LYNN ) ) ( inference480 ( TYPE metonymy ) ( ENGINE-PROPELLED-VEHICLE460 ( MANUFACTURED-BY ( VALUE FOR-PROFIT-MANUFACTURING-CORP417 ) ) ) ) ( FOR-PROFIT-MANUFACTURING-CORP417 ( NAME ( VALUE $ SAAB ) ) ( PRODUCER-OF inference480 ( SEa *ARTIFACT ) ) ( VALUE ENGINE-PROPELLED-VEHICLE460 ) ) ) The inference notation used in this example is more generally available to represent inferences made by a variety of specialized mechanisms or microtheories during the course of semantic analysis .</sentence>
				<definiendum id="0">TMR</definiendum>
				<definiendum id="1">THEME</definiendum>
				<definiendum id="2">SEM *ENGINE-PROPELLED-VEHICLE ) ( VALUE</definiendum>
				<definiens id="0">our interlingua or meaning representation language ) : THR : ( DRIVE435 ( AGENT ( VALUE PERSON440 )</definiens>
			</definition>
			<definition id="2">
				<sentence>; ; ; ~The White House said it does not know n ( USA Today ) ; ; ; Metonymy Type : PLACENAME-FOR-OCCUPANTS ; ; ; Metonymy Type : ROLE-FOR-PERSON ; ; ; ~said n = ASSERTIVE-ACT ; ; ; ASSERTIVE-ACT .</sentence>
				<definiendum id="0">; Metonymy Type</definiendum>
				<definiens id="0">USA Today ) ; ; ; Metonymy Type : PLACENAME-FOR-OCCUPANTS ; ;</definiens>
			</definition>
</paper>

		<paper id="0205">
			<definition id="0">
				<sentence>Text metadata , representing video content , was extracted from the CMU Informedia system , processed by AR &amp; T 's text analysis software , and presented to users via PNNL 's Starlight 3D visualization system .</sentence>
				<definiendum id="0">Text metadata</definiendum>
				<definiens id="0">representing video content , was extracted from the CMU Informedia system , processed by AR &amp; T 's text analysis software , and presented to users via PNNL 's Starlight 3D visualization system</definiens>
			</definition>
			<definition id="1">
				<sentence>The Informedia Project uses the Sphinx-II speech recognition system to transcribe narratives and dialogues automatically .</sentence>
				<definiendum id="0">Informedia Project</definiendum>
				<definiens id="0">uses the Sphinx-II speech recognition system to transcribe narratives and dialogues automatically</definiens>
			</definition>
			<definition id="2">
				<sentence>Segment breaks produced by image processing are examined along with the boundaries identified by the natural language processing of the transcript , and an improved set of segment boundaries are heuristically derived to partition the video library into sets of segments , or `` video paragraphs . ''</sentence>
				<definiendum id="0">Segment breaks</definiendum>
				<definiens id="0">heuristically derived to partition the video library into sets of segments , or `` video paragraphs</definiens>
			</definition>
			<definition id="3">
				<sentence>The Boeing TPT is a prototype software engine that supports automatic coding and categorization of documents , concept-based querying , and visualization over large text document databases .</sentence>
				<definiendum id="0">Boeing TPT</definiendum>
			</definition>
			<definition id="4">
				<sentence>The TPT provides a selection of dimensions ( with associated topic words ) , any three of which can potentially be selected for axes of the scatterplot .</sentence>
				<definiendum id="0">TPT</definiendum>
				<definiens id="0">provides a selection of dimensions ( with associated topic words ) , any three of which can potentially be selected for axes of the scatterplot</definiens>
			</definition>
			<definition id="5">
				<sentence>Starlight presents the entire collection of hundreds of video paragraphs as a scatterplot of points for viewing , which can be color-coded by video title .</sentence>
				<definiendum id="0">Starlight</definiendum>
				<definiens id="0">presents the entire collection of hundreds of video paragraphs as a scatterplot of points for viewing</definiens>
			</definition>
</paper>

		<paper id="1232">
			<definition id="0">
				<sentence>The learning algorithm consists of a procedure which attempts to determine the stem and part of speech for each ( unknown ) inflected form in its input .</sentence>
				<definiendum id="0">learning algorithm</definiendum>
				<definiens id="0">consists of a procedure which attempts to determine the stem and part of speech for each ( unknown ) inflected form in its input</definiens>
			</definition>
			<definition id="1">
				<sentence>The a~xes lexicon consists of 511 entries , of which 236 are inflectional and 275 derivational .</sentence>
				<definiendum id="0">a~xes lexicon</definiendum>
				<definiens id="0">consists of 511 entries , of which 236 are inflectional and 275 derivational</definiens>
			</definition>
			<definition id="2">
				<sentence>The unification-based word grammar consists of 14 rules to account for prefixation , suffixation , and inflection .</sentence>
				<definiendum id="0">unification-based word grammar</definiendum>
			</definition>
			<definition id="3">
				<sentence>In order to establish the correct stem for an inflected form , the learning procedure attempts to combine the accumulated evidence provided by related word forms , i.e. , word forms sharing a common stem .</sentence>
				<definiendum id="0">learning procedure</definiendum>
				<definiens id="0">attempts to combine the accumulated evidence provided by related word forms</definiens>
			</definition>
			<definition id="4">
				<sentence>gc ( X ) ) Where Pc is a function from S to the natural numbers mapping a set X to the number of times it was produced for a given corpus C of inflected forms , and I , Pk , and Pk are functions defined as follows : I : Sx~ ( S ) - , \ [ 0,1\ ] { I-~1 ifxEX ( x , X ) ~ 0 else Pk : s x p ( s ) -~ \ [ o , 1\ ] ( p~_ , .</sentence>
				<definiendum id="0">gc ( X ) ) Where Pc</definiendum>
				<definiendum id="1">0,1\ ] { I-~1 ifxEX</definiendum>
				<definiens id="0">a function from S to the natural numbers mapping a set X to the number of times it was produced for a given corpus C of inflected forms , and I , Pk , and Pk are functions defined as follows : I : Sx~ ( S ) - , \ [</definiens>
				<definiens id="1">s x p ( s ) -~ \ [ o , 1\ ] ( p~_ ,</definiens>
			</definition>
</paper>

		<paper id="1413">
			<definition id="0">
				<sentence>Positive ( negative ) effect axioms describe the conditions under which performing a in situation s causes a fluent to become true ( false ) in do ( a , s ) .</sentence>
				<definiendum id="0">Positive ( negative ) effect axioms</definiendum>
			</definition>
			<definition id="1">
				<sentence>dJocation ( y ) A fits ( x , y ) A exposed ( y , s ) ( i ) Poss ( touch ( x ) , s ) -physical_object ( x ) A exposed ( x , s ) Poss ( get_burned , s ) =_ 3x , t. ( touching ( x , s ) A temperature ( x , t , s ) A t &gt; 70 ) ( 2 ) ( 3 ) Poss ( raise_temp ( x ) , s ) =- ( x = bread_slot V contains ( bread_slot , x , s ) ) A 3t .</sentence>
				<definiendum id="0">dJocation ( y</definiendum>
				<definiendum id="1">Poss ( touch</definiendum>
				<definiendum id="2">exposed</definiendum>
				<definiens id="0">raise_temp ( x ) , s ) =- ( x = bread_slot V contains ( bread_slot , x , s ) ) A 3t</definiens>
			</definition>
</paper>

		<paper id="1431">
			<definition id="0">
				<sentence>FLAUBERT is an engine for text generation .</sentence>
				<definiendum id="0">FLAUBERT</definiendum>
				<definiens id="0">an engine for text generation</definiens>
			</definition>
			<definition id="1">
				<sentence>FLAUBERT uses three databases : standard way , the concepts include objects , actions , states and relations between them ; • A set of lexical data bases associated withconcepts ; the lexical database for a given concept describes its semantico-lexical realizations ( lexical heads + argument structures ) accompanied with tests of applicability for right semantics and well formdness ; • A TAG grammar whose syntactic informations allow a derived tree to be computed from a derivation tree ( see the data flow below ) .</sentence>
				<definiendum id="0">FLAUBERT</definiendum>
				<definiens id="0">uses three databases : standard way , the concepts include objects , actions , states and relations between them ; • A set of lexical data bases associated withconcepts</definiens>
			</definition>
</paper>

		<paper id="0601">
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>Maltese is the national language of Malta and , together with English , one of the two official languages of the Republic of Malta .</sentence>
				<definiendum id="0">Maltese</definiendum>
				<definiens id="0">the national language of Malta and</definiens>
			</definition>
			<definition id="1">
				<sentence>Unsurprisingly in view of the disparate political and cultural influences the islands have been exposed to over the centuries , Maltese is a so-called 'mixed ' language , with a substrate of Arabic , a considerable superstrate of Romance origin ( especially Sicilian ) and , to a much more limited extent , English .</sentence>
				<definiendum id="0">Maltese</definiendum>
				<definiens id="0">a so-called 'mixed ' language</definiens>
			</definition>
</paper>

		<paper id="1217">
			<definition id="0">
				<sentence>Therefore , we can generalize equation 1 to N / : / ( P ' Q ) = Z Pi '' log2 qi ( 2 ) i -- -- 1 where Q is a different distribution representing our best estimate of the true distribution P. This value ( called the cross-entropy ) achieves a minimum when P = Q , and H ( P , P ) = H ( P ) .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">H</definiendum>
				<definiens id="0">a different distribution representing our best estimate of the true distribution P. This value ( called the cross-entropy ) achieves a minimum when P = Q , and</definiens>
			</definition>
			<definition id="1">
				<sentence>Wyner defines the match length Ln ( x ) of a sequence ( xl , x2 , ... , xn , xn+x , ... ) as the length of the the longest prefix of the sequence ( xn+x , ... ) that matches a contiguous substring of ( zl , z2 , ... , xn ) , and proves that this converges in the limit to the value ~ as n increases .</sentence>
				<definiendum id="0">Wyner</definiendum>
			</definition>
			<definition id="2">
				<sentence>• 143 Cross-Entropy and Linguistic Typology the maximal prefix to be found within the database .</sentence>
				<definiendum id="0">Linguistic Typology</definiendum>
				<definiens id="0">the maximal prefix to be found within the database</definiens>
			</definition>
			<definition id="3">
				<sentence>Although this loses some of the time-varying properties of an entropy estimator ( in particular , the database is fixed and will not shift to capture longterm regularities in an input stream ) , this should preserve the fundamental relationship that a closer fit ( smaller cross-entropy ) results in a longer mean match length .</sentence>
				<definiendum id="0">an entropy estimator</definiendum>
				<definiens id="0">a closer fit ( smaller cross-entropy ) results in a longer mean match length</definiens>
			</definition>
</paper>

		<paper id="1116">
			<definition id="0">
				<sentence>The answer is a qualified yes .</sentence>
				<definiendum id="0">answer</definiendum>
				<definiens id="0">a qualified yes</definiens>
			</definition>
			<definition id="1">
				<sentence>We look at the well-known , : ; use of tile ambiguity between a main clause and a reduced relative construction , which arises because regular verbs in English present an ambiguity between the simple past and the past particle ( the -ed form ) .</sentence>
				<definiendum id="0">relative construction</definiendum>
				<definiendum id="1">-ed form</definiendum>
				<definiens id="0">arises because regular verbs in English present an ambiguity between the simple past</definiens>
			</definition>
			<definition id="2">
				<sentence>The transitive form of an unergative ( 2b ) is the causative counterpart of the intransitive form ( 2a ) , in which the subject of the intransitive becomes the object of the transitive ( Hale and Keyser : 1993 ; Levin and Rappaport Hovav , 1995 ) .</sentence>
				<definiendum id="0">transitive form of an unergative</definiendum>
				<definiens id="0">the causative counterpart of the intransitive form ( 2a ) , in which the subject of the intransitive becomes the object of the transitive</definiens>
			</definition>
			<definition id="3">
				<sentence>First , the discrepancy between the frequencies of each of the lexical features and the frequencies of the m : tual construction suggests that the frequency of a construction is a composition fimction of ( at least some of ) its lexical features , even if such t } ; atures are not-independent .</sentence>
				<definiendum id="0">frequency of a construction</definiendum>
				<definiens id="0">a composition fimction of</definiens>
			</definition>
			<definition id="4">
				<sentence>An LTAG lexicon is a tbrest of lexicalised elementary trees .</sentence>
				<definiendum id="0">LTAG lexicon</definiendum>
			</definition>
</paper>

		<paper id="0708">
			<definition id="0">
				<sentence>The subclasses formed by the synsets `` transportl ( move something or somebody around ; usually over long distances ) , '' `` transport2 , carry ( move while supporting , either in a vehicle or in one 's hands or on one 's body ) , '' `` transport3 , send , ship ( transport commercially ) '' and `` transmit , transfer , conduct , transport5 , channel , channelize '' are mapped into subpredicates of the predicate transport which is : \ [ transport ( is-a ( cause-to-change-location ) ) ( theme ( physical-tbing ) ( obj obj2 ) ) ( goal ( human-agent animal ) ( obj-if-obj2 ( prep for ) ) ) ( location phy-thing ) ( ( prep to towards ... same as cause-to-change-location ) ) ) \ ] \ [ transport -over-long-distances ( is-a ( transport ) ) ( wn-map ( transpor~l ) ) \ ] &lt; other transport predicates here &gt; The goal , besides being realized by the same prepositions as those for cause-to-change-location , is also syntactically realized by an indirect object ( obj-ifobj2 ) , e.g. , ( 1 ) '' Susan brought her children a book from Harvard . ''</sentence>
				<definiendum id="0">) ( goal</definiendum>
				<definiens id="0">a vehicle or in one 's hands or on one 's body )</definiens>
				<definiens id="1">transmit , transfer , conduct , transport5 , channel , channelize '' are mapped into subpredicates of the predicate transport which is : \ [ transport ( is-a ( cause-to-change-location ) ) ( theme ( physical-tbing ) ( obj obj2 )</definiens>
			</definition>
			<definition id="1">
				<sentence>A meaning postulate infers the predicate and its roles for the secondary event .</sentence>
				<definiendum id="0">meaning postulate</definiendum>
				<definiens id="0">infers the predicate and its roles for the secondary event</definiens>
			</definition>
			<definition id="2">
				<sentence>I I I I I I I I I I I I I I I \ [ provide ( is-a ( give ) ) ( ran-map ( supply1 ) ( provide2 ) ) ( theme ( t : hing ) ( obj obj 2 ( prep with ) ) ) ( to-poss ( human-agent animal physical-thing thing ) ( obj-if-obj2 obj-if-with ( prep to for ) ) ) ( inanimate-cause ( thing ) ( subj -if-obj ) ) \ ] \ [ provide-inanimate-cause ( is-a ( give ) ) ( agent ( nil ) ( nil ) ) ( wn-map ( supply 1 ) ( provide2 ) ) ( theme ( physical-thing ) ( obj obj 2 ( prep with ) ) ) ( to-poss ( human-agent animal physical-thing thing ) ( obj-if-obj2 obj-if-with ( prep to for ) ) ) ( inanimate-caus e ( -human-agent -animal thing ) ( subj-if-obj ) ) \ ] \ [ get-something ( is-a ( transf er-of-possession ) ) ( un-map ( get I ) ) ( theme ( physical-thing thing ) ( obj obj2 ) ) ( from-poss ( human-agent animal phy-thing thing ) ( ( prep off from out-of ) ) ) Most of its subpredicates require a human as agent , but some take an animal as agent .</sentence>
				<definiendum id="0">to-poss</definiendum>
				<definiendum id="1">inanimate-caus e</definiendum>
				<definiendum id="2">get-something ( is-a ( transf er-of-possession ) )</definiendum>
				<definiens id="0">give ) ) ( agent ( nil ) ( nil ) ) ( wn-map ( supply 1 ) ( provide2 ) ) ( theme ( physical-thing )</definiens>
				<definiens id="1">physical-thing thing ) ( obj obj2 ) ) ( from-poss ( human-agent animal phy-thing thing ) ( ( prep off from out-of ) ) ) Most of its subpredicates require a human as agent , but some take an animal as agent</definiens>
			</definition>
			<definition id="3">
				<sentence>Dowry ( Dowty , 1991 ) , then , asks that , if one assigns a thematic role to measures of distance , why not assign a role to `` quickly '' in `` She walks quickly . ''</sentence>
				<definiendum id="0">Dowry</definiendum>
				<definiens id="0">asks that , if one assigns a thematic role to measures of distance</definiens>
			</definition>
			<definition id="4">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>HPSG is formulated as a set of constraints on typed ( 1 ) feature structures ( TFSs ) that are used to model linguistic information in all levels : from the lexicon , through grammatical principles , to complete ( 2 ) analyses .</sentence>
				<definiendum id="0">HPSG</definiendum>
				<definiens id="0">a set of constraints on typed ( 1 ) feature structures ( TFSs ) that are used to model linguistic information in all levels : from the lexicon</definiens>
			</definition>
			<definition id="1">
				<sentence>Like other current linguistic theories , HPSG is highly lexical : most of the information is encoded in highly articulated lexical entries associated with ( 4 ) words .</sentence>
				<definiendum id="0">HPSG</definiendum>
			</definition>
			<definition id="2">
				<sentence>Hebrew has one definite article , ha- , which attaches to words ( nouns , adjectives , numerals and demonstratives , bencefotl_h nominals ) , not phrases .</sentence>
				<definiendum id="0">bencefotl_h nominals</definiendum>
				<definiens id="0">attaches to words ( nouns , adjectives , numerals and demonstratives</definiens>
			</definition>
			<definition id="3">
				<sentence>MH provides two major ways of forming genitive relations : free genitives ( FG ) , in which the genitive phrase is introduced by the preposition Sell 'of ' ( 2 ) ; and constructs ( CS ) , in which the head noun is morphologically marked ( and is said to be in the construct state , cs ) and the genitive phrase must immediately follow it , preceding any other modifiers ( 3 ) .</sentence>
				<definiendum id="0">MH</definiendum>
				<definiens id="0">provides two major ways of forming genitive relations : free genitives ( FG ) , in which the genitive phrase is introduced by the preposition Sell 'of ' ( 2 )</definiens>
			</definition>
			<definition id="4">
				<sentence>hasepr hagadol haze/ $ 1iSi the book the big the this/third 'this big book / the third big book ' ( ha- ) sparim Sell mSorer ( the ) books of poet ' ( the ) books of a poet ' ( ha- ) sparim Sell hamSorer ( the ) books of the poet ' ( the ) books of the poet ' siprei mSorer xdaSim books-cs poet new 'new books of a poet ' siprei hamSorer haxdaSim books-cs the poet the new 'the new books of the poet ' yruqqat ( ha- ) &amp; einaym green-cs ( the ) eyes 'a/ ( th¢ ) green eyed '' Following Abney ( 1987 ) , analyses carried out in Chomskian frameworks view noun phrases as DPs , headed by the functional category D. The DP hypothesis ( DPH ) has been applied to a variety of languages and is incorporated into most existing accounts for Modem Hebrew .</sentence>
				<definiendum id="0">green eyed '' Following Abney</definiendum>
				<definiens id="0">DP hypothesis ( DPH ) has been applied to a variety of languages</definiens>
			</definition>
			<definition id="5">
				<sentence>Since definiteness is a feature of phrases , inherited from the lexical head , DEF is a head feature , appropriate for all nominals .</sentence>
				<definiendum id="0">DEF</definiendum>
				<definiens id="0">a feature of phrases</definiens>
				<definiens id="1">a head feature , appropriate for all nominals</definiens>
			</definition>
</paper>

		<paper id="0509">
			<definition id="0">
				<sentence>In contrast to the traditional view on parsing as a constructive process , which builds new tree structures from elementary building blocks and intermediate results , eliminative approaches organize structural analysis as a candidate elimination procedure , removing unsuitable interpretations from a maximum set of possible ones .</sentence>
				<definiendum id="0">constructive process</definiendum>
				<definiens id="0">builds new tree structures from elementary building blocks and intermediate results , eliminative approaches organize structural analysis as a candidate elimination procedure</definiens>
			</definition>
			<definition id="1">
				<sentence>The novel contribution consists in the use of graded constraints , which allow to model traditional grammar regularities as well as preferences and defaults .</sentence>
				<definiendum id="0">novel contribution</definiendum>
				<definiens id="0">consists in the use of graded constraints , which allow to model traditional grammar regularities as well as preferences and defaults</definiens>
			</definition>
			<definition id="2">
				<sentence>The set of representational levels L = { ( /x , Lx ) , ... , ( l , ,L , ) } consists of pairs ( li , Li ) where li is a name of the ith representational level and l~ E Li is the jth appropriate label for level l , .</sentence>
				<definiendum id="0">li</definiendum>
				<definiendum id="1">l~ E Li</definiendum>
				<definiens id="0">a name of the ith representational level</definiens>
			</definition>
			<definition id="3">
				<sentence>Basically , a constraint consists of a logical formula which is parameterized by variables ( in our example X and Y ) which can be bound to an edge in the dependency tree .</sentence>
				<definiendum id="0">constraint</definiendum>
				<definiens id="0">consists of a logical formula which is parameterized by variables ( in our example X and Y ) which can be bound to an edge in the dependency tree</definiens>
			</definition>
			<definition id="4">
				<sentence>Being universally quantified , a typical constraint takes the form of an implication with the premise describing the conditions for its application .</sentence>
				<definiendum id="0">typical constraint</definiendum>
				<definiens id="0">takes the form of an implication with the premise describing the conditions for its application</definiens>
			</definition>
			<definition id="5">
				<sentence>{ X } : SubjOrder : Subj : 0.9 : X.label=subj -F X~pos &lt; Xtpos 'Subjects are usually placed in front of the verb. { X } : SemType : SelRestr : 0.8 : X.label ( E { agent , theme } -~ type_match ( Xlid. X.label , X , I.id ) 'Verbs restrict the semantic types of their arguments. ' { X , Y } : SubjAgentObjTheme : Mapping : 0.2 : XTid=YTid A X~id=Y~id -~ ( X.label=subj ~-~ Y.label=agent ) ^ ( X.label=obj ~ Y.label=theme ) 'The subject is the agent and the object the theme. ' { X , Y } : Unique : Unique : 0.0 : X.label E { subj , obj , agent , theme } ^ Xtid=Ytid -4 Y.labelpX.label 'Some labels are unique for a given verb. ' Figure 2 : Some of the constraints needed for the disambiguation of the example sentence. equal to the constraint 's score , namely 0.9. Furthermore there is no structure which has a better assessment. The next example is similar to the last , except that the finite verb appears in plural form now. ( 2 ) Die Knochenpl sehenpl die Katzesg. The bones see the cat. `` The bones see the cat. '' A solution structure analogous to the one discussed above would have a score of 0.09 because not only the constraint SubjOrder but also the constraint SubjNumber would have been violated. But the alternative structure where the V~y n = ( det,2 ) VSem = ( de : f , 2 ) V~yn = ( obj,3 ) V~e m = ( theme , 3 ) V~y n = ( root , O ) V~e m = ( root,0 ) U~y n = ( det , 5 ) V~e m = ( de.f , 5 ) `` ~yn = ( sub.j , 3 ) V~e m = ( agent , 3 ) Figure 3 : Constraint variable assignments corresponding to the dependency trees in Figure 1 subj/agent and the obj/theme edges are interchanged ( meaning that the bones do the seeing ) has a better score of 0.8 this time because it only violates the constraint SemType. This result obviously resembles performance of human beings who first of all note the semantic oddness of the example ( 2 ) before trying to repair the syntactic deviations when reading this sentence in isolation. Thus , the approach successfully arbitrates between conflicting information from different levels , using the constraint scores to determine which of the problem candidates is chosen as the final solution. A lot of research has been carried out in the field of algorithm~ for constraint satisfaction problems ( Meseguer , 1989 ; Kumar , 1992 ) and constraint optimization problems ( Tsang , 1993 ) . Although CSPs are NP -- complete problems in general and , therefore , one can not expect a better than exponential complexity in the worst case , a lot of methods have been developed to allow for a reasonable complexity in most practical cases. Some heuristic methods , for instance , try to arrive at a solution more efficiently at the expense of giving up the property of correctness , i. e. , they find the globally best solution in most cases while they are not guaranteed to do so in all cases. This allows to influence the temporal characteristics of the parsing procedure , a possibility which seems especially important in interactive applications : If the system has to deliver a reasonable solution within a specific time interval a dynamic scheduling of computational resources depending on the remaining ambiguity and available time is necessary. While different kinds of search are more suitable with regard to the correctness property , local pruning strategies lend themselves to resource adaptive procedures. As long as only crisp constraints are considered , procedures based on local consistency , particularly arc consistency can be used ( Maruyama , 1990 ; Harper et al. , 1995 ) . These methods try to delete values from the domain of constraint variables by considering only local information and have a polynomial worst case complexity. 83 Unfortunately , they possibly stop deleting values before a unique solution has been found. In such a case , even if arc consistency has been established one can not be sure whether the problem has zero , one , or more than one solution because alternative value assignments may be locally consistent , but globally mutually incompatible. Consequently , in order to find actual solutions an additional search has to be carried out for which , however , the search space is considerably reduced already. The most straightforward method for constraint parsing is a simple search procedure where the constraint variables are successively bound to values and these value assignments are tested for consistency. In case of an inconsistency alternative values are tried until a solution is found or the set of possible values is exhausted. The basic search algorithm is Branch &amp; Bound which exploits the fact that the score of every subset of variable assignments is already an upper bound of the final score. Additional constraint violations only make the score worse , bemuse the scores of constraints do not exceed a value of one. Therefore , large parts of the search space can be abandoned as soon as the score becomes too low. To further ~ improve the efficiency an agenda is used to sort the search space nodes so that the most promising candidates are tried first. By not allowing the agenda to grow larger than a specified size , one can exclude search states with low scores from further consideration. Note that correctness can not be guaranteed in that case anymore. Figure 4 presents the algorithm in pseudo code notation. Unfortunately , the time requirements of the search algorithms are almost unpredictable since an intermediate state of computation does not give a reliable estimation of the effort that remains to be done. As explained in Section 6.1 consistency-based procedures use local information to delete values from the domain of variables. While these methods only do so if the local information suffices to guarantee that the value under consideration can safely be deleted , pruning goes one step further. Values are successively selected for deletion based on a heuristic ( i. e. , possibly incorprocedure ConstraintSearch set b : = 0.0 ; best score so far set r : = 0 ; set of solutions set a : = { ( 0 , V , 1.0 ) ) ; agenda while a ~ 0 do ; process agenda get best item ( a , V , s ) from agenda a ; best first if V = 0 then if a = b then add ( B , V , s ) to r else set r : = { ( /3 , IF , , ) } set b : -a fl fl select v E V set V ' : = V\ { ~ } foreach d E dora ( v ) do set B ' : = B u compute new score s ~ for B ~ if s ~ ~ b then add ( B ' , V ' , s ' ) to agenda fl done truncate agenda a ( if desired ) done ; complete assignment ? ; best so far ? ; equally good ; better ; try next free variable ; try all values ; already worse ? Figure 4 : Search procedure for constraint parsing : Best-first branch &amp; bound algorithm with limited agenda length ( beam search ) rect ) assessment until a single solution remains ( cf. Figure 5 ) . The selection function considers only local information ( as do the consistencybased methods ) for efficiency reasons. Taking into account global optimality criteria would not help at all since then the selection would be as difficult as the whole problem , i. e. , one would have to expect an exponential worst-case complexity. procedure pruning ( V ) while 3 ( ~fi V ) : Idom ( ~ ) ~ &gt; I do select ( ~ , d ) to be deleted delete d from domain done Figure 5 : The pruning algorithm repeatedly selects and deletes values from the domain of variables .</sentence>
				<definiendum id="0">X~id=Y~id -~</definiendum>
				<definiens id="0">the semantic types of their arguments. ' { X</definiens>
				<definiens id="1">the agent and the object the theme. ' { X</definiens>
				<definiens id="2">X.label E { subj , obj , agent</definiens>
				<definiens id="3">Some of the constraints needed for the disambiguation of the example sentence. equal to the constraint 's score</definiens>
				<definiens id="4">Constraint variable assignments corresponding to the dependency trees in Figure 1 subj/agent and the obj/theme edges are interchanged ( meaning that the bones do the seeing</definiens>
				<definiens id="5">not guaranteed to do so in all cases. This allows to influence the temporal characteristics of the parsing procedure , a possibility which seems especially important in interactive applications : If the system has to deliver a reasonable solution within a specific time interval a dynamic scheduling of computational resources depending on the remaining ambiguity and available time is necessary. While different kinds of search</definiens>
				<definiens id="6">a simple search procedure where the constraint variables are successively bound to values and these value assignments are tested for consistency. In case of an inconsistency alternative values are tried until a solution is found or the set of possible values is exhausted. The basic search algorithm is Branch &amp; Bound which exploits the fact that the score of every subset of variable assignments is already an upper bound of the final score. Additional constraint violations only make the score worse</definiens>
				<definiens id="7">add ( B , V , s ) to r else set r : = { ( /3 , IF , , ) } set b : -a fl fl select v E V set V ' : = V\ { ~ } foreach d E dora ( v ) do set B ' : = B u compute new score s</definiens>
			</definition>
</paper>

		<paper id="1434">
</paper>

		<paper id="1419">
			<definition id="0">
				<sentence>We are interested in a particular kind of efficiency that we call textual economy , which presupposes a view of sentence generation as goal-directed activity that has broad support in Natural Language Generation ( NLG ) research \ [ 1 , 5 , 15 , 17\ ] .</sentence>
				<definiendum id="0">textual economy</definiendum>
				<definiens id="0">presupposes a view of sentence generation as goal-directed activity that has broad support in Natural Language Generation</definiens>
			</definition>
			<definition id="1">
				<sentence>SPUD draws on earlier work by Appelt \ [ 1\ ] in building sentences using planning techniques , sPUD plans the syntax and semantics of a sentence by incorporating lexico-grammatical entries into a partial sentence one-by-one and incrementally assessing the answers to the questions given above .</sentence>
				<definiendum id="0">SPUD</definiendum>
				<definiendum id="1">sPUD</definiendum>
				<definiens id="0">plans the syntax and semantics of a sentence by incorporating lexico-grammatical entries into a partial sentence one-by-one and incrementally assessing the answers to the questions given above</definiens>
			</definition>
			<definition id="2">
				<sentence>For ( 1 ) , then , SPUD is given two 181 features of the action to be described : it involves motion of an intended object by the agent , and its result is achieved when the object reaches a place decisively away from its starting point .</sentence>
				<definiendum id="0">SPUD</definiendum>
				<definiens id="0">given two 181 features of the action to be described : it involves motion of an intended object by the agent</definiens>
			</definition>
			<definition id="3">
				<sentence>For this instruction , SPUD treats the CAUSED-MOTION and AWAY semantic features as semantic contributions . ''</sentence>
				<definiendum id="0">SPUD</definiendum>
				<definiens id="0">treats the CAUSED-MOTION and AWAY semantic features as semantic contributions</definiens>
			</definition>
			<definition id="4">
				<sentence>Thus , SPUD derives a triple effect from use of the word remove -- increasing syntactic satisfaction , making semantic contributions and satisfying semantic requirements -- all of which contribute to SPUD 's task of completing an S syntactic constituent that conveys needed content and refers successfully .</sentence>
				<definiendum id="0">SPUD</definiendum>
				<definiens id="0">derives a triple effect from use of the word remove -- increasing syntactic satisfaction , making semantic contributions and satisfying semantic requirements -- all of which contribute to SPUD 's task of completing an S syntactic constituent that conveys needed content and refers successfully</definiens>
			</definition>
			<definition id="5">
				<sentence>SPUD starts with the goal of describing the holding action in the main clause , and ( if possible ) also describing the filling action and indicating the purpose relation ( i.e. , enablement ) between them .</sentence>
				<definiendum id="0">SPUD</definiendum>
				<definiens id="0">if possible ) also describing the filling action and indicating the purpose relation ( i.e. , enablement ) between them</definiens>
			</definition>
			<definition id="6">
				<sentence>SPUD has no reason to describe the orientation of the cup with additional content .</sentence>
				<definiendum id="0">SPUD</definiendum>
				<definiens id="0">has no reason to describe the orientation of the cup with additional content</definiens>
			</definition>
			<definition id="7">
				<sentence>This section describes in a bit more detail how SPUD computes the effects of incorporating a particular lexical item into the sentence being constructed .</sentence>
				<definiendum id="0">SPUD</definiendum>
				<definiens id="0">computes the effects of incorporating a particular lexical item into the sentence being constructed</definiens>
			</definition>
			<definition id="8">
				<sentence>The important point is that the process enables inferences to be performed that allow more economical texts : the next step is to address the complexity issues that these other authors have elaborated and show how SPUD 's description extension and verification process can be incorporated into a more efficient or more flexible control structure .</sentence>
				<definiendum id="0">verification process</definiendum>
				<definiens id="0">show how SPUD 's description extension and</definiens>
			</definition>
</paper>

		<paper id="0211">
			<definition id="0">
				<sentence>1 So , leaf primitives are fully specified by a series of characters ; layout primitives take one or more operands each of which may be any of the primitives ; shape primitives require a single operand .</sentence>
				<definiendum id="0">shape primitives</definiendum>
				<definiens id="0">fully specified by a series of characters ; layout primitives take one or more operands each of which may be any of the primitives</definiens>
			</definition>
			<definition id="1">
				<sentence>C is a nonterminal or terminal symbol .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a nonterminal or terminal symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>In general , a path is a sequence of elements of one of the following forms ( where t is a diagram type , v the name of a subpart and n an integer ) : ( 5 ) tv ~vn The first assigns a diagram type and picks out a subpart of the diagram .</sentence>
				<definiendum id="0">t</definiendum>
				<definiens id="0">a sequence of elements of one of the following forms</definiens>
			</definition>
</paper>

		<paper id="0706">
			<definition id="0">
				<sentence>In machine learning systems that classify text , E is a set of labeled documents from a corpus such as Reuters21578 .</sentence>
				<definiendum id="0">E</definiendum>
			</definition>
			<definition id="1">
				<sentence>The third corpus , DigiTrad is a public domain collection of 6500 folk song lyrics \ [ Greenhaus 96\ ] .</sentence>
				<definiendum id="0">DigiTrad</definiendum>
				<definiens id="0">a public domain collection of 6500 folk song lyrics</definiens>
			</definition>
			<definition id="2">
				<sentence>Reuters consists of articles written purely as a source of factual information .</sentence>
				<definiendum id="0">Reuters</definiendum>
				<definiens id="0">consists of articles written purely as a source of factual information</definiens>
			</definition>
			<definition id="3">
				<sentence>( A synset is defined as infrequent if its frequency of occurrence over the entire corpus is less than 0.05N , where N is the number of documents in the corpus . )</sentence>
				<definiendum id="0">A synset</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">infrequent if its frequency of occurrence over the entire corpus is less than 0.05N</definiens>
				<definiens id="1">the number of documents in the corpus</definiens>
			</definition>
			<definition id="4">
				<sentence>c ) During the third pass , the density of each synset ( defined as the number of occurrences of a synset in the WordNet output divided by the number of words in the document ) is computed for each example resulting in a set of numerical feature vectors .</sentence>
				<definiendum id="0">c ) During</definiendum>
			</definition>
			<definition id="5">
				<sentence>A special value of h=max is defined as the level in which all hypernym synsets are counted , no matter how far up in the hierarchy they appear .</sentence>
				<definiendum id="0">special value of h=max</definiendum>
				<definiens id="0">the level in which all hypernym synsets are counted</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>If we denote respectively n and b an arbitrary element of each of these sets , the source language of the automaton can be reduced to : A1 = { # , n , b } L1 = { # ( n Vb ) ' # } where V denotes disjunction and • is the Kleene star Language L1 can be generated by the simple grammar : m -+ # A # A A ( lb ) or the as simple automaton : initial states = { 1 } final states : - { 5 } transitions 4As this question has only been taken as an example , the alphabet has been oversimplified .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">n and b an arbitrary element of each of these sets , the source language of the automaton can be reduced to : A1 = { # , n , b } L1 = { # ( n Vb ) ' # } where</definiens>
			</definition>
			<definition id="1">
				<sentence>input output \ [ # , n , # \ ] \ [ # , b , # \ ] \ [ # , n , n , # \ ] \ [ # , n , b , # \ ] \ [ # , b , n , # \ ] \ [ # , b , b , # \ ] \ [ # , n , n , n , # \ ] \ [ # , n , n , b , # \ ] \ [ # , In , # \ ] \ [ # , Ib , # \ ] \ [ # , In , In , # \ ] \ [ # , In , Ib , # \ ] \ [ # , i , fn , # \ ] \ [ # , i , fb , # \ ] \ [ # , In , In , In , # \ ] \ [ # , In , In , Ib , # \ ] \ [ # , n , b , n , # \ ] \ [ # , n , b , b , # \ ] \ [ # , b , n , n , # \ ] \ [ # , b , n , b , # \ ] \ [ # , b , b , n , # \ ] \ [ # , b , b , b , # \ ] \ [ # , In , i , fn , # \ ] \ [ # , In , i , fb , # \ ] \ [ # , i , fn , In , # \ ] \ [ # , i , fn , Ib , # \ ] \ [ # , i , m , fn , # \ ] \ [ # , i , m , fb , # \ ] \ [ # , n , n , n , n , # \ ] \ [ # , n , n , n , b , # \ ] \ [ # , n , n , b , n , # \ ] \ [ # , n , n , b , b , # \ ] \ [ # , n , b , n , n , # \ ] \ [ # , n , b , n , b , # \ ] \ [ # , n , b , b , n , # \ ] \ [ # , n , b , b , b , # l \ [ # , b , n , n , n , # l \ [ # , b , n , n , b , # \ ] \ [ # , b , n , b , n , # \ ] \ [ # , b , n , b , b , # \ ] \ [ # , In , In , In , In , # \ ] \ [ # , In , In , In , Ib , # \ ] \ [ # , In , In , i , fn , # \ ] \ [ # , In , In , i , fb , # \ ] \ [ # , In , i , fn , In , # \ ] \ [ # , In , i , fn , Ib , # \ ] \ [ # , In , i , m , fn , # \ ] \ [ # , In , i , m , fb , # \ ] \ [ # , i , fn , In , In , # \ ] \ [ # , i , fn , In , Ib , # \ ] \ [ # , i , fn , i , fn , # \ ] \ [ # , i , fn , i , fb , # \ ] input \ [ # , b , b , n , n , # \ ] \ [ # , b , b , n , b , # \ ] \ [ # , b , b , b , n , # \ ] \ [ # , b , b , b , b , # \ ] output \ [ # , i , m , fn , In , # \ ] \ [ # , i , m , fn , Ib , # \ ] \ [ # , i , m , m , fn , # \ ] \ [ # , i , m , m , fb , # \ ] The hamza can be written in five different man $ ners ( I , !</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">n , n , n , # \ ] \ [ # , n , n , b</definiens>
				<definiens id="1">n , n , n , n , # \ ] \ [ # , n , n , n , b , # \ ] \ [ # , n , n , b , n , # \ ] \ [ # , n , n , b , b , # \ ] \ [ # , n , b , n , n , # \ ] \ [ # , n , b , n , b , # \ ] \ [ # , n , b , b , n , # \ ] \ [ # , n , b , b , b , # l \ [ # , b , n , n , n , # l \ [ # , b , n , n , b , # \ ] \ [ # , b , n , b , n , # \ ] \ [ # , b , n , b , b</definiens>
			</definition>
</paper>

		<paper id="0214">
			<definition id="0">
				<sentence>Geographic Information Systems ( GIS loosely defined as a spatial database ) can be used as a way to store , manage and manipulate spatial information for the generation of tactile maps .</sentence>
				<definiendum id="0">Geographic Information Systems ( GIS</definiendum>
				<definiens id="0">a spatial database ) can be used as a way to store , manage and manipulate spatial information for the generation of tactile maps</definiens>
			</definition>
			<definition id="1">
				<sentence>Hypermedia which comprises of text , still imagery , sound recordings ( even tactile and olfactory output ) is a network of information 'linked electronically by multiple routes , chains , or trails in an open-ended , perceptually unfinished assemblage described best in terms of links , nodes , networks , webs and paths ' ( Landow 1992 , p.3 ) .</sentence>
				<definiendum id="0">Hypermedia</definiendum>
				<definiens id="0">a network of information 'linked electronically by multiple routes , chains , or trails in an open-ended , perceptually unfinished assemblage described best in terms of links , nodes , networks</definiens>
			</definition>
			<definition id="2">
				<sentence>The help button explains how to use the system .</sentence>
				<definiendum id="0">help button</definiendum>
				<definiens id="0">explains how to use the system</definiens>
			</definition>
			<definition id="3">
				<sentence>A vibro-tactile mouse which registers the mouse 's position over a desired spatial object on a map ( Nissen 1997 ) , tonal interfaces for computer interaction ( Alty 1996 ) , and 'The Voice ' which can convert a two dimensional picture , map or representation into a 'tonal soundseape ' ( Meijers 1993 , 1997 ) .</sentence>
				<definiendum id="0">vibro-tactile mouse</definiendum>
				<definiendum id="1">'The Voice '</definiendum>
				<definiens id="0">registers the mouse 's position over a desired spatial object on a map ( Nissen 1997 ) , tonal interfaces for computer interaction</definiens>
			</definition>
			<definition id="4">
				<sentence>lwards A.D.N. ( 1989 ) Soundtrack : An auditory interface for blind users .</sentence>
				<definiendum id="0">Soundtrack</definiendum>
				<definiens id="0">An auditory interface for blind users</definiens>
			</definition>
</paper>

		<paper id="1427">
			<definition id="0">
				<sentence>Seminar presented at the iTRI , 1994 This first rule of thumb clearly does make good general sense , and a number of successful systems have followed it : ANA takes a set of temporally ordered stock quotes fromthe Dow Jones News and Information Retrieval System and produces a natural language report on the activities of the stock market over the period• ( Kukich , 1983 ) .</sentence>
				<definiendum id="0">ANA</definiendum>
				<definiens id="0">takes a set of temporally ordered stock quotes fromthe Dow Jones News and Information Retrieval System and produces a natural language report on the activities of the stock market over the period•</definiens>
			</definition>
			<definition id="1">
				<sentence>I , | I I 1 11 • GOSSIP takes data from an audit trail of an operating system and produces a report ( for a security officer ) on user activity over the period ( Carcagno and Iordanskaja , 1989 ) .</sentence>
				<definiendum id="0">GOSSIP</definiendum>
				<definiens id="0">takes data from an audit trail of an operating system and produces a report ( for a security officer ) on user activity over the period</definiens>
			</definition>
			<definition id="2">
				<sentence>• FOG takes data from a time series of weather depiction charts and produces a bilingual ( French and English ) weather report for the period ( Goldberg et al , 1994 ) .</sentence>
				<definiendum id="0">FOG</definiendum>
				<definiens id="0">takes data from a time series of weather depiction charts and produces a bilingual ( French and English ) weather report for the period</definiens>
			</definition>
			<definition id="3">
				<sentence>• LFS takes statistical data from labour force surveys and from this produces a report on employment statistics over the given period ( Iordanskaja et al , 1992 ) .</sentence>
				<definiendum id="0">LFS</definiendum>
				<definiens id="0">takes statistical data from labour force surveys and from this produces a report on employment statistics over the given period</definiens>
			</definition>
			<definition id="4">
				<sentence>• POSTGRAPHE takes tabular data ( of the sort found in a typical spreadsheet ) and generates a report integrating both graphics and text ( Fasciano and Lapalme , 1996 ) .</sentence>
				<definiendum id="0">POSTGRAPHE</definiendum>
				<definiens id="0">takes tabular data ( of the sort found in a typical spreadsheet</definiens>
			</definition>
			<definition id="5">
				<sentence>• PLANDOC takes the data from a simulation log file and from this produces a report of the explored simulation options ( McKeown et al , 1994 ) .</sentence>
				<definiendum id="0">PLANDOC</definiendum>
			</definition>
			<definition id="6">
				<sentence>• EXCLASS is an intelligent support tool for personnel officers writing ( bilingual English and French ) job descriptions .</sentence>
				<definiendum id="0">EXCLASS</definiendum>
				<definiens id="0">an intelligent support tool for personnel officers writing ( bilingual English and French ) job descriptions</definiens>
			</definition>
			<definition id="7">
				<sentence>257 WYSIWYM is a technique for creating and maintaining knowledge models ( e.g. , knowledge bases , databases , domain models ) by direct knowledge editing with the benefit of dynamic natural language feedback .</sentence>
				<definiendum id="0">WYSIWYM</definiendum>
				<definiens id="0">a technique for creating and maintaining knowledge models ( e.g. , knowledge bases , databases , domain models ) by direct knowledge editing with the benefit of dynamic natural language feedback</definiens>
			</definition>
			<definition id="8">
				<sentence>WYSIWYM interfaces provide natural language input without the requirement of NL interpretation ( i.e. , no parsing takes place ) ; instead , it makes use exclusively ofNL generation .</sentence>
				<definiendum id="0">WYSIWYM interfaces</definiendum>
				<definiens id="0">provide natural language input without the requirement of NL interpretation</definiens>
			</definition>
			<definition id="9">
				<sentence>PostGraphe : a system for the generatio n of statistical graphics and text .</sentence>
				<definiendum id="0">PostGraphe</definiendum>
				<definiens id="0">a system for the generatio n of statistical graphics and text</definiens>
			</definition>
</paper>

		<paper id="1216">
			<definition id="0">
				<sentence>In their paper they do not give any detail about the cues they use , except to say that they are `` mainly punctuation cues and other separators and delimiters used to mark text categories like phrases , clauses , and sentences '' ( p. 34 ) ; however , Hinrich Schfitze ( personal eornmunication ) has elaborated that `` The cues are punctuation , non-content words ( pronouns , prepositions , auxiliaries ) , counts of words , \ [ of\ ] unique words , \ [ of\ ] sentences , and \ [ of\ ] characters ; and deviation features ( standard deviation of word length and sentence length ) '' .</sentence>
				<definiendum id="0">Hinrich Schfitze</definiendum>
				<definiens id="0">mainly punctuation cues and other separators and delimiters used to mark text categories like phrases , clauses , and sentences ''</definiens>
				<definiens id="1">punctuation , non-content words ( pronouns , prepositions , auxiliaries ) , counts of words , \ [ of\ ] unique words</definiens>
				<definiens id="2">standard deviation of word length and sentence length ) ''</definiens>
			</definition>
			<definition id="1">
				<sentence>A cusum is a graphic plot based on a sequence of measures .</sentence>
				<definiendum id="0">cusum</definiendum>
				<definiens id="0">a graphic plot based on a sequence of measures</definiens>
			</definition>
			<definition id="2">
				<sentence>The formula chosen for the calculation of variance e in ( 2 ) is given in ( 3 ) , where n is the number of sentences in the text .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
</paper>

		<paper id="1112">
			<definition id="0">
				<sentence>Smadja is the only one that attempts to treat other phrases ( such as verb phrases as well what he labels 'flexible phrases ' ) .</sentence>
				<definiendum id="0">Smadja</definiendum>
			</definition>
			<definition id="1">
				<sentence>Basque is an aglutinative language which has postpositions and other functional elements added as suffixes .</sentence>
				<definiendum id="0">Basque</definiendum>
				<definiens id="0">an aglutinative language which has postpositions and other functional elements added as suffixes</definiens>
			</definition>
</paper>

		<paper id="1306">
			<definition id="0">
				<sentence>The FSA Utilities tool-box is a collection of tools to manipulate regular expressions , finite-state automata and finite-state transducers ( both string-to-string and string-to-weight transducers ) .</sentence>
				<definiendum id="0">FSA Utilities tool-box</definiendum>
				<definiens id="0">a collection of tools to manipulate regular expressions , finite-state automata and finite-state transducers</definiens>
			</definition>
			<definition id="1">
				<sentence>Support includes built-in visualisation ( TCl/Tk , TeX+PicTeX , TeX+PsTricks , Postscript ) and interfaces to third party graph visualisation software ( Graphviz ( dot ) , VCG , daWmci ) .</sentence>
				<definiendum id="0">Support</definiendum>
			</definition>
			<definition id="2">
				<sentence>Let a finite-state machine M be specified by a tuple ( Q , 22 , 6 , S , F ) where Q is a finite set of states , is a finite alphabet , 6 is a function from Q x ( 27 u { e } ) -- * 2 Q. Furthermore , S c Q is a set of start states 4 and F C Q is a set of final states .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">S c Q</definiendum>
				<definiendum id="2">C Q</definiendum>
				<definiens id="0">a set of start states 4 and F</definiens>
			</definition>
			<definition id="3">
				<sentence>Following Leslie ( 1995 ) , the absolute transition density of an automaton is defined as the number of transitions divided by the square of the number of states times the number of symbols ( i.e. the number of transitions divided by the number of possible transitions ) .</sentence>
				<definiendum id="0">absolute transition density of an automaton</definiendum>
			</definition>
			<definition id="4">
				<sentence>Deterministic transition density is the number of transitions divided by the number of states times the number of symbols ( i.e. the ratio of the number of transitions and the number of possible trans~'ons in a deterministic machine ) .</sentence>
				<definiendum id="0">Deterministic transition density</definiendum>
				<definiens id="0">the number of transitions divided by the number of states times the number of symbols ( i.e. the ratio of the number of transitions and the number of possible trans~'ons in a deterministic machine )</definiens>
			</definition>
			<definition id="5">
				<sentence>FSA Utilities : A toolbox to manipulate finite-state automata .</sentence>
				<definiendum id="0">FSA Utilities</definiendum>
				<definiens id="0">A toolbox to manipulate finite-state automata</definiens>
			</definition>
</paper>

		<paper id="1105">
			<definition id="0">
				<sentence>the desired output from the system would be { IN = Hensley West , POST = president , IND = named } POST is a slot designating the title of the position , IN is the person coming in to fill the post , and IND is an `` indicator '' usually a verb or a noun used to express the event .</sentence>
				<definiendum id="0">IN</definiendum>
				<definiendum id="1">IND</definiendum>
				<definiens id="0">a slot designating the title of the position</definiens>
				<definiens id="1">an `` indicator '' usually a verb or a noun used to express the event</definiens>
			</definition>
			<definition id="1">
				<sentence>Instead , the method uses a Probabilistic Context Free Grammar ( PCFG ) , which has the advantages of being flexible enough to allow a good parameterization of the problem , while having an efficient decoding algorithm , a variant of the CKY dynamic programming algorithm for parsing with context-free grammars .</sentence>
				<definiendum id="0">PCFG )</definiendum>
			</definition>
			<definition id="2">
				<sentence>A concept node specifies a trigger word , usually a verb , and maps syntactic roles with respect to this trigger to semantic slots for example , a concept node might specify/f trigger = `` destroyed '' and syntax = direct-object then concept = Damaged-Object ( Damaged-object is the name of the slot in this case ) .</sentence>
				<definiendum id="0">concept node</definiendum>
				<definiendum id="1">Damaged-Object ( Damaged-object</definiendum>
				<definiens id="0">specifies a trigger word , usually a verb</definiens>
			</definition>
			<definition id="3">
				<sentence>The system uses the CIRCUS parser ( Lehnert et al. 93 ) to find the syntactic roles in relation to the trigger .</sentence>
				<definiendum id="0">system</definiendum>
				<definiens id="0">uses the CIRCUS parser ( Lehnert et al. 93 ) to find the syntactic roles in relation to the trigger</definiens>
			</definition>
			<definition id="4">
				<sentence>CRYSTAL learns patterns by initially specifying a maximally detailed pattern for each training example , then progressively simplifying and merging patterns until some error bound is exceeded .</sentence>
				<definiendum id="0">CRYSTAL</definiendum>
				<definiens id="0">learns patterns by initially specifying a maximally detailed pattern for each training example</definiens>
			</definition>
			<definition id="5">
				<sentence>CRYSTAL uses the BADGER sentence analyzer to give syntactic information .</sentence>
				<definiendum id="0">CRYSTAL</definiendum>
				<definiens id="0">uses the BADGER sentence analyzer to give syntactic information</definiens>
			</definition>
			<definition id="6">
				<sentence>An example pattern from ( Califf and Mooney 97 ) for identifying locations is pre-filler = in , filler = 2 or fewer words all proper nouns , post-filler = wordl is `` , '' , word2 is a state .</sentence>
				<definiendum id="0">word2</definiendum>
				<definiens id="0">a state</definiens>
			</definition>
			<definition id="7">
				<sentence>IN is the string denoting the person who is filling the post , OUT is the person who is leaving the post , and POST is the name of the post .</sentence>
				<definiendum id="0">OUT</definiendum>
				<definiendum id="1">POST</definiendum>
				<definiens id="0">the name of the post</definiens>
			</definition>
			<definition id="8">
				<sentence>T = { IN = ( 3,4 ) , POST = ( 14 , 14 ) , IND = ( I0 , I0 ) } As alternative notation in this paper we either list the strings in the template , for example T = { IN = `` Hensley West '' , POST = `` president '' , IND = `` joined '' } , or we show the ( W , T ) pair as a bracketed sentence : Last week ( IN Hensley West ) , 59 years old , ( INDjoined ) the company as ( POST president ) , a surprising development .</sentence>
				<definiendum id="0">INDjoined</definiendum>
				<definiens id="0">POST president ) , a surprising development</definiens>
			</definition>
			<definition id="9">
				<sentence>It is useful to note that a ( W , T ) pair can be represented as a tagged sentence wl/tl , w2/t2 , ... w , /tn where T = tl , t2 ... tn is the sequence of tags denoting the semantic type for each word in the sentence .</sentence>
				<definiendum id="0">tn</definiendum>
				<definiens id="0">a tagged sentence wl/tl , w2/t2 , ... w , /tn where T = tl</definiens>
			</definition>
			<definition id="10">
				<sentence>Wi is the string of words under label Li , for example W1 = { Last , week } , W2 = { Hensley , West } .</sentence>
				<definiendum id="0">Wi</definiendum>
				<definiens id="0">the string of words under label Li</definiens>
			</definition>
			<definition id="11">
				<sentence>Our proposal is to replace the Markov assumption in ( 3 ) with a probabilistic context-free grammar , that is we assume that the label sequence has been generated by the application of r context-free rules LHSj =~ RHSj 1 _ &lt; j _ &lt; r ( LHS stands for left hand side , RHS stands for fight hand side ) , and that P ( L1L2 ... .Lm ) = H P ( RHSi l LHSi ) .</sentence>
				<definiendum id="0">LHS</definiendum>
				<definiendum id="1">RHS</definiendum>
				<definiens id="0">stands for left hand side</definiens>
			</definition>
			<definition id="12">
				<sentence>r Each LHS is a single non-terminal , and RHS is a string of one or more non-terminals .</sentence>
				<definiendum id="0">RHS</definiendum>
				<definiens id="0">a single non-terminal , and</definiens>
				<definiens id="1">a string of one or more non-terminals</definiens>
			</definition>
			<definition id="13">
				<sentence>42 Leaf label I , O , P PREN POSTN NOISEDescription The IN , OUT and POST non-terminals `` Noise '' words before the template `` Noise '' words after the template `` Noise '' between slots and the indicator , which comes before the indicator `` Noise '' between slots and the indicator , which comes after the indicator lND ( class ) Leaf dominating the indicator , class can be any one of the morphological stems seen in training data .</sentence>
				<definiendum id="0">POST non-terminals `` Noise</definiendum>
				<definiendum id="1">Noise</definiendum>
				<definiendum id="2">Noise</definiendum>
			</definition>
			<definition id="14">
				<sentence>The maximum likelihood estimate for a CF rule LHS - &gt; RHS is P ( RHSiLHS ) = C ( RHS , LHS ) C ( LHS ) where C ( z ) is the number of times event z has been seen in training data .</sentence>
				<definiendum id="0">RHS</definiendum>
				<definiendum id="1">C ( z )</definiendum>
				<definiens id="0">the number of times event z has been seen in training data</definiens>
			</definition>
			<definition id="15">
				<sentence>IOP ) might be P ( T -- - &gt; IND \ [ Class\ ] IT ) , i.e. an estimate that ignores the slots when choosing the class of indicators .</sentence>
				<definiendum id="0">IOP</definiendum>
				<definiens id="0">P ( T -- - &gt; IND \ [ Class\ ] IT ) , i.e. an estimate that ignores the slots when choosing the class of indicators</definiens>
			</definition>
			<definition id="16">
				<sentence>This method borrows heavily from smoothing techniques in language modeling for speech recognition m ( Jelinek 90 ) describes methods for estimating A. The bigram model is used at the leaves of the tree to generate the words themselves , for example to estimate P ( the president I P ) The most obvious way to estimate this is as P ( theISTART , P ) * P ( presidentlthe , P ) • P ( E N DIpresident , P ) with smoothing being implemented by interpolation between P ( wlw-x , State ) -- r P ( wlState ) ~ ~ where V is the vocabulary size .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">the vocabulary size</definiens>
			</definition>
			<definition id="17">
				<sentence>First , the model would provide pnncipled probability estimates for selecting the most likely set of templates given an input message : T = set of templates ( the final output ) M= the message , C = components P = linguistic patterns , S = slot fillers in T D = descriptions used to express the slots P ( TIM ) = ~ P ( T , C , PIM ) C , P where P ( T , C , PIM ) = P ( T ) x P ( CIT ) x P ( PIC ) x P ( DIS ) P ( M ) The second potential advantage derives from the generative aspect of the proposed model .</sentence>
				<definiendum id="0">P where P ( T</definiendum>
				<definiens id="0">provide pnncipled probability estimates for selecting the most likely set of templates given an input message : T = set of templates ( the final output ) M= the message</definiens>
				<definiens id="1">components P = linguistic patterns , S = slot fillers in T D = descriptions used to express the slots P ( TIM ) = ~ P ( T , C , PIM ) C ,</definiens>
			</definition>
</paper>

		<paper id="1405">
			<definition id="0">
				<sentence>s For the parser , the TAG is •reorganized ( by hand ) by sectioning the trees horizontally into patterns of immediate constituents in the manner of SchabeS and Waters ( 1992 ) as shown in the example below , 9 which is followed by the full detail of the part of the realization field of co-owns-co that goes with this tree family ; syntactic categories on the left side of the mapping are replaced with the semantic categories on the right .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">•reorganized ( by hand ) by sectioning the trees horizontally into patterns of immediate constituents in the manner of SchabeS</definiens>
			</definition>
</paper>

		<paper id="0701">
			<definition id="0">
				<sentence>( 1 ) The Fulton County Grand Jury said Friday an investigation of Atlanta 's recent primary election produced no evidence that any irregularities took place .</sentence>
				<definiendum id="0">Friday</definiendum>
				<definiens id="0">an investigation of Atlanta 's recent primary election produced no evidence that any irregularities took place</definiens>
			</definition>
			<definition id="1">
				<sentence>( 2 ) NP ( NN P ( County ) , NN P ( Jury ) ) N P ( NNP ( Grand ) , NN P ( Jury ) ) NP ( NP ( Atlanta ) , NP ( election ) ) N P ( J J ( recent ) , N P ( election ) ) N P ( J J ( primary ) , N N ( election ) ) N P ( N N ( in vestigation ) , P P ( of ( election ) ) ) S ( N P ( irregularities ) , VP ( took ) ) V P ( VB D ( took ) , NP ( place ) ) N P ( NN ( evidence ) , SBAR ( that ( took ) ) S ( N P ( investigation ) , VP ( produced ) ) V P ( VB D ( produced ) , N P ( evidence ) ) VPIVBD ( said ) , N P ( Friday ) ) V P ( VBD ( said ) , S BA R ( 0 ( produced ) ) ) S ( NP ( Jury ) , VP ( said ) ) Each of the extracted syntactic relations has a certain probability for each combination of the senses of its arguments .</sentence>
				<definiendum id="0">P ( NNP</definiendum>
				<definiendum id="1">N N ( election ) ) N P ( N N (</definiendum>
				<definiendum id="2">VP</definiendum>
				<definiendum id="3">P ( NN</definiendum>
				<definiendum id="4">SBAR</definiendum>
				<definiendum id="5">VP ( produced ) ) V P ( VB D</definiendum>
				<definiendum id="6">Friday ) ) V P ( VBD</definiendum>
				<definiendum id="7">S BA R ( 0 ( produced ) ) ) S ( NP</definiendum>
				<definiendum id="8">VP</definiendum>
				<definiens id="0">in vestigation ) , P P ( of ( election ) ) ) S ( N P ( irregularities )</definiens>
				<definiens id="1">that ( took ) ) S ( N P ( investigation ) ,</definiens>
			</definition>
			<definition id="2">
				<sentence>Every content word can have several meanings , but each of these meanings has a different probability , which is given by the set of semantic relations in which the word participates .</sentence>
				<definiendum id="0">different probability</definiendum>
				<definiens id="0">is given by the set of semantic relations in which the word participates</definiens>
			</definition>
			<definition id="3">
				<sentence>If , for any given sentence , we had extracted N syntactic relations PA , the overall relational probability for the combination of senses X would be : N ( 5 ) ORP ( X ) = I-\ [ p ( &amp; IX ) i=l where p ( RiIX ) is the probability of the i-th relation given the combination of senses X. If we consider , that an average word sense ambiguity in the used corpus is 5.8 senses , a sentence with 10 content words would have 5.8 t° possible sense combinations , leading to a combinatorial explosion of over 43,080,420 overall probability combinations , which is not feasible .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the probability of the i-th relation given the combination of senses</definiens>
			</definition>
			<definition id="4">
				<sentence>The score of co-occurrences is defined as a sum of hits of similar pairs , where a hit is I a multiplication of the similarity measures , sim ( i , x ) and sim ( j , y ) , between both participants , i.e. : r I ( 7 ) fR ( i , j ) = ~= sim ( i , z ) , sim ( j , y ) where x , yER ; r is the number of relations of the same type ( for the above example I R=reI ( NP , ADJ , NOUN , x , y,1 ) ) found in the training corpus .</sentence>
				<definiendum id="0">score of co-occurrences</definiendum>
				<definiendum id="1">r</definiendum>
				<definiendum id="2">R=reI ( NP , ADJ , NOUN , x</definiendum>
				<definiens id="0">a sum of hits of similar pairs , where a hit is I a multiplication of the similarity measures , sim ( i , x ) and sim ( j , y ) , between both participants</definiens>
				<definiens id="1">the number of relations of the same type</definiens>
			</definition>
			<definition id="5">
				<sentence>I Every semantic relation can be represented by a relational matrix , which is a matrix whose first coordinate represents the sense of the modifier , the I il where fl~ ( id ) is a score of co-occurrences of a modifier sense x with a head word sense y , among the same semantic relations R extracted during the learning phase .</sentence>
				<definiendum id="0">relational matrix</definiendum>
				<definiens id="0">a score of co-occurrences of a modifier sense x with a head word sense y , among the same semantic relations R extracted during the learning phase</definiens>
			</definition>
			<definition id="6">
				<sentence>Please note , because fR ( ij ) is not a count but rather a score of co-occurrences ( defined below ) , pR ( i , j ) is not a real probability but Because the occurrence 4 second coordinate represents the sense of the head and the value at the coordinate position ( i j ) is the estimate of the probability of the sense combination ( id ) computed by ( 6 ) .</sentence>
				<definiendum id="0">Please note</definiendum>
				<definiens id="0">not a real probability</definiens>
				<definiens id="1">the estimate of the probability of the sense combination ( id ) computed by ( 6 )</definiens>
			</definition>
			<definition id="7">
				<sentence>We base the definition of the semantic similarity between two concepts ( concepts are defined by their WordNet synsets a , b ) on their semantic distance , as follows : ( 8 ) sire ( a , b ) `` -1 sd ( a , b ) ~- , The semantic distance sd ( a , b ) is squared in the above formula in order to give a bigger weight to closer matches .</sentence>
				<definiendum id="0">semantic distance sd</definiendum>
				<definiens id="0">the definition of the semantic similarity between two concepts ( concepts are defined by their WordNet synsets a , b ) on their semantic distance</definiens>
			</definition>
			<definition id="8">
				<sentence>( ~ + -D-2 where DI is the depth of synset a , D2 is the depth of synset D2 , and D is the depth of their nearest common ancestor in the WordNet hierarchy .</sentence>
				<definiendum id="0">DI</definiendum>
				<definiendum id="1">D2</definiendum>
				<definiendum id="2">D</definiendum>
				<definiens id="0">the depth of synset a</definiens>
				<definiens id="1">the depth of synset D2 , and</definiens>
				<definiens id="2">the depth of their nearest common ancestor in the WordNet hierarchy</definiens>
			</definition>
			<definition id="9">
				<sentence>t ( w ) i + l ) j=t where count ( w ) , is the number of occurrences of the sense i of the word w in the entire training corpus , and n is the number of different WordNet senses of the word w. The sense score vectors of head words propagate up the tree .</sentence>
				<definiendum id="0">count ( w )</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the number of occurrences of the sense i of the word w in the entire training corpus</definiens>
				<definiens id="1">the number of different WordNet senses of the word w. The sense score vectors of head words propagate up the tree</definiens>
			</definition>
			<definition id="10">
				<sentence>head word h and a modifier mi with sense score vector Mi = loll , oi2 ... . oil\ ] , do : Ri ( m , h ) of the relation Ri of the Ri ( m , h ) for which m=oi by oi , yielding Qi the sense score matrix of the modifier mi is now G- '' \ [ gl , g2 , ... , gk\ ] , where Lj ( lo ) g i = 2 -- , h~ Lj/L represents the score of the head word sense j based on the matrices Q calculated in the step 1. , i.e. : ( ll ) Lj = ~ maz ( zi ( j , u ) ) i=I where xi ( j , u ) E Qi and max ( xi ( j , u ) ) is the highest score in the line of the matrix Qi which corresponds to the head word sense j. n is the number of modifiers of the head word h at the current tree level , and k i Lj = j~l Lj where k is the number of senses of the head word h. The reason why gj ( I0 ) is calculated as a sum of the best scores ( ll ) , rather than by using the traditional maximum likelihood estimate ( Berger et al. , 1996 ) ( Gah eta\ [ .</sentence>
				<definiendum id="0">Lj ( lo</definiendum>
				<definiendum id="1">maz ( zi</definiendum>
				<definiendum id="2">k</definiendum>
				<definiens id="0">m , h ) of the relation Ri of the Ri ( m , h ) for which m=oi by oi , yielding Qi the sense score matrix of the modifier mi is now G- '' \ [ gl , g2 , ... , gk\ ] , where</definiens>
				<definiens id="1">the score of the head word sense j based on the matrices Q calculated in the step 1. , i.e. : ( ll ) Lj = ~</definiens>
				<definiens id="2">the number of modifiers of the head word h at the current tree level</definiens>
				<definiens id="3">the number of senses of the head word h. The reason why gj ( I0 ) is calculated as a sum of the best scores ( ll )</definiens>
			</definition>
			<definition id="11">
				<sentence>The top-down disambiguation algorithm , which starts with the sentence head , can be described recursively as follows : Let 1 be the sense of the head word h on the input .</sentence>
				<definiendum id="0">top-down disambiguation algorithm</definiendum>
				<definiens id="0">starts with the sentence head</definiens>
			</definition>
			<definition id="12">
				<sentence>Table 4 : Result Accuracy \ [ % \ ] CONTEXT NOUNS VERBS ADJs ADVs TOTAL First sense 77.8 61.7 81.9 84.5 75.2 Sentence 84.2 63.6 82.9 86.3 79.4 +Discourse 85.7 63.9 83.6 86.5 80.3 where e ( POS , SN ) is the probability that the word with syntactic category POS which already occurred SN sentences before , has the same sense as its previous occurrence .</sentence>
				<definiendum id="0">SN )</definiendum>
				<definiens id="0">the probability that the word with syntactic category POS which already occurred SN sentences before , has the same sense as its previous occurrence</definiens>
			</definition>
</paper>

		<paper id="1110">
			<definition id="0">
				<sentence>This paper presents a generalized unknown morpheme handling method with P OSTAG ( POStech TAGger ) which is a statistical/rule based hybrid POS tagging system .</sentence>
				<definiendum id="0">POStech TAGger )</definiendum>
				<definiens id="0">a statistical/rule based hybrid POS tagging system</definiens>
			</definition>
			<definition id="1">
				<sentence>n eojeol consists of several number of morphemes that have clear-cut morpheme boundaries .</sentence>
				<definiendum id="0">n eojeol</definiendum>
				<definiens id="0">consists of several number of morphemes that have clear-cut morpheme boundaries</definiens>
			</definition>
			<definition id="2">
				<sentence>The morpheme pattern dictionary covers all necessary syllable patterns for unknown morphemes including common nouns , propernouns , adnominals , adverbs , regular and irregular verbs , regular and irregular adjectives , and special symbols for foreign words .</sentence>
				<definiendum id="0">morpheme pattern dictionary</definiendum>
				<definiens id="0">covers all necessary syllable patterns for unknown morphemes including common nouns , propernouns , adnominals , adverbs , regular and irregular verbs , regular and irregular adjectives , and special symbols for foreign words</definiens>
			</definition>
			<definition id="3">
				<sentence>The morpheme-graph is a compact way of representing nmltiple morpheme sequences for a sentence .</sentence>
				<definiendum id="0">morpheme-graph</definiendum>
				<definiens id="0">a compact way of representing nmltiple morpheme sequences for a sentence</definiens>
			</definition>
			<definition id="4">
				<sentence>Pr ( tilti-1 ) is a bi-gram tag transition probability and Pr ( ti lrni ) PT ( td is a modified morpheme lexical probability .</sentence>
				<definiendum id="0">td</definiendum>
				<definiens id="0">a bi-gram tag transition probability and Pr ( ti lrni</definiens>
			</definition>
			<definition id="5">
				<sentence>~ f ( tilti-t ) = N ( ti- : , ti ) N ( ti-t ) ( 10 ) where N ( mi , ti ) indicates the total number of occurrences of morpheme .</sentence>
				<definiendum id="0">ti )</definiendum>
				<definiens id="0">indicates the total number of occurrences of morpheme</definiens>
			</definition>
			<definition id="6">
				<sentence>The N ( ti_l , ti ) and N ( ti-1 ) can be interpreted similarly for two consecutive tags ti-1 and ti .</sentence>
				<definiendum id="0">N ( ti_l</definiendum>
				<definiens id="0">be interpreted similarly for two consecutive tags ti-1 and ti</definiens>
			</definition>
			<definition id="7">
				<sentence>wpN ( ~rl # , # ) xPrMpN ( '~I # , ~ ) x Pr , ~teN ( ~l~ I ' , ~ ) x PrMpN ( # 1 All tri-grams for Korean syllables were precalculated and stored in the table , and are applied with the candidate tags during the unknown morpheme POS guessing and smoothing .</sentence>
				<definiendum id="0">wpN</definiendum>
				<definiens id="0">1 All tri-grams for Korean syllables were precalculated and stored in the table , and are applied with the candidate tags during the unknown morpheme POS guessing and smoothing</definiens>
			</definition>
</paper>

		<paper id="1201">
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>The phrase-finder test uses a set of grammatical rules that identify the phrases in the text .</sentence>
				<definiendum id="0">phrase-finder test</definiendum>
				<definiens id="0">uses a set of grammatical rules that identify the phrases in the text</definiens>
			</definition>
</paper>

		<paper id="1219">
			<definition id="0">
				<sentence>Information extraction ( IE ) is a well defined task ; the aim being to extract data from free text , and put it in a more structured format .</sentence>
				<definiendum id="0">Information extraction</definiendum>
				<definiens id="0">a well defined task ; the aim being to extract data from free text , and put it in a more structured format</definiens>
			</definition>
			<definition id="1">
				<sentence>The DSTO Fact Extractor Workbench provides the tools to create re-usable text skimming components , called fact extractors , that perform IE on a ( very ) limited domain .</sentence>
				<definiendum id="0">DSTO Fact Extractor Workbench</definiendum>
				<definiens id="0">provides the tools to create re-usable text skimming components</definiens>
			</definition>
			<definition id="2">
				<sentence>The Named Entity Test is one component of the message understanding conference ( MUC 57 ( DARPA , 1995 ) ) evaluations .</sentence>
				<definiendum id="0">Named Entity Test</definiendum>
				<definiens id="0">one component of the message understanding conference</definiens>
			</definition>
</paper>

		<paper id="0901">
			<definition id="0">
				<sentence>However , SCC covers the role of ONSET , i.e. , ONSET can be regarded as a subset of SCC .</sentence>
				<definiendum id="0">SCC</definiendum>
				<definiens id="0">a subset of SCC</definiens>
			</definition>
			<definition id="1">
				<sentence>Supposing the input is a phrase like stop it , whose string of phonemes is/s t h a ph # i th/ , the grids look like below : ( 22 ) Grid A ( for syllable position ) s t h a ph # i t h O O O O O O O n n n n n n n nn nn an nn nn nn nn C C C C C C C CO CO CO CO CO CO CO on on on on on on on U U U U U U U ( 23 ) Grid B ( for output segments 8 ) start with an input form , generate candidate syllabifications , and apply constraints to produce a syllabified output .</sentence>
				<definiendum id="0">nn nn nn nn C C C C C C C CO CO CO CO CO CO CO</definiendum>
				<definiens id="0">a phrase like stop it , whose string of phonemes is/s t h a ph # i th/ , the grids look like below</definiens>
				<definiens id="1">n n n n n n n nn nn an</definiens>
			</definition>
			<definition id="2">
				<sentence>( 28 ) ONSET s t h O O ph # i \ [ t h n £ EP_ co on on on on U It is better to delete the portion of the word boundary symbol , before *COM starts to work .</sentence>
				<definiendum id="0">ONSET s</definiendum>
				<definiens id="0">better to delete the portion of the word boundary symbol</definiens>
			</definition>
			<definition id="3">
				<sentence>( 30 ) CA-L ( cont , o ) the same as ( 29 ) Now , DEP plays a role of pruning 'o n ' candidate in a set containing more than one element .</sentence>
				<definiendum id="0">DEP</definiendum>
				<definiens id="0">plays a role of pruning 'o n ' candidate in a set containing more than one element</definiens>
			</definition>
</paper>

		<paper id="1208">
			<definition id="0">
				<sentence>GATE is an architecture and development environment for research and development workers in NLP and Language Engineering 1 .</sentence>
				<definiendum id="0">GATE</definiendum>
				<definiens id="0">an architecture and development environment for research and development workers in NLP and Language Engineering 1</definiens>
			</definition>
			<definition id="1">
				<sentence>GGI has several viewers for the display of TIPSTER annotations .</sentence>
				<definiendum id="0">GGI</definiendum>
			</definition>
			<definition id="2">
				<sentence>An annotation chain is a list of annotations specified by annotation references .</sentence>
				<definiendum id="0">annotation chain</definiendum>
				<definiens id="0">a list of annotations specified by annotation references</definiens>
			</definition>
			<definition id="3">
				<sentence>Sense tagging is the process of assigning the appropriate sense from some semantic lexicon to each word 3 in a text .</sentence>
				<definiendum id="0">Sense tagging</definiendum>
			</definition>
			<definition id="4">
				<sentence>The text is tagged using the Brill tagger ( Brill , 1992 ) and a translation is carried out using a manually defined mapping from the syntactic tags assigned by Brill ( Penn Tree Bank tags ( Marcus , Santorini , and Marcinkiewicz , 1993 ) ) onto the simpler part-of-speech categories associated with LDOCE senses 6 .</sentence>
				<definiendum id="0">Brill tagger</definiendum>
			</definition>
			<definition id="5">
				<sentence>After this process is com7We are using the 1st Edition of LDOCE in which the publishers make no claim that the senses are ordered by pleted every ambiguous word has exactly one sense from LDOCE associated with it , this sense is the tag which our system has assigned to that word .</sentence>
				<definiendum id="0">sense</definiendum>
				<definiens id="0">using the 1st Edition of LDOCE in which the publishers make no claim that the senses are ordered by pleted every ambiguous word has exactly one sense from LDOCE associated with it , this</definiens>
			</definition>
			<definition id="6">
				<sentence>Almost the entire preprocessing of the text was carried out using modules which had already been implemented within GATE : the tokeniser , sentence splitter , Brill part-of-speech tagger and the modules which made up the Named Entity identifier .</sentence>
				<definiendum id="0">GATE</definiendum>
				<definiens id="0">the tokeniser , sentence splitter , Brill part-of-speech tagger and the modules which made up the Named Entity identifier</definiens>
			</definition>
</paper>

		<paper id="1421">
			<definition id="0">
				<sentence>A VIT is a semantic representation formalism following the Discourse Representation Theory ( DRT ) of \ [ Kamp and Reyle 1993\ ] .</sentence>
				<definiendum id="0">VIT</definiendum>
			</definition>
			<definition id="1">
				<sentence>A VIT consists of a set of semantic conditions ( i.e. predicates , roles , operators and quantifiers ) and allows for under-specifications with respect to scope and subordination or inherent under-specifications .</sentence>
				<definiendum id="0">VIT</definiendum>
				<definiens id="0">consists of a set of semantic conditions ( i.e. predicates , roles , operators and quantifiers ) and allows for under-specifications with respect to scope and subordination or inherent under-specifications</definiens>
			</definition>
			<definition id="2">
				<sentence>The most accurat e translation track is a deep linguistic• analysis in combination with semantic transfer and syntactic generation ( see figure 2 ) .</sentence>
				<definiendum id="0">most accurat e translation track</definiendum>
				<definiens id="0">a deep linguistic• analysis in combination with semantic transfer and syntactic generation</definiens>
			</definition>
			<definition id="3">
				<sentence>The translation process consists of two tracks , the deep and the shallow translation .</sentence>
				<definiendum id="0">translation process</definiendum>
			</definition>
			<definition id="4">
				<sentence>The deep linguistic translation track , whose modules all exchange linguistic information encoded in VITs , consists of three components : An HPSG-parser combined with a robust semantic component , 1TEMPoral EXpression 199 the semantic based transfer component \ [ Dorna and Emele 1996\ ] and the generation component \ [ Becket et al. 1998\ ] , an efficient-multi-lingual generator ( some more details below ) .</sentence>
				<definiendum id="0">efficient-multi-lingual generator</definiendum>
				<definiens id="0">An HPSG-parser combined with a robust semantic component , 1TEMPoral EXpression 199 the semantic based transfer component \ [</definiens>
			</definition>
			<definition id="5">
				<sentence>( cond ( ( not ( single-utterance ( current-utterance ) ) ) • ( mark ( current-utterance ) : protocol-relevance nil ) ) ) ; ; dialogue phase ( mark ( current-utterance ) : dialogue-phase ( get-phase-from-context ( current-utterance ) ) ) : leaf FEEDBACK_ACKNOWLEDGMENT ) Figure 4 : A Plan Operator for Processing Utterances of Type FEEDBACK_ACKNOWLEDGMENT Both hand-coded as well as automatically derived operators from the VERBMOBIL corpus are used to build its structure .</sentence>
				<definiendum id="0">dialogue phase</definiendum>
			</definition>
			<definition id="6">
				<sentence>The core planning step consists of the selection and application of an appropriate VIT-pattern of a segment which is determined by the following three main criteria : • The first criterion to find an applicable pattern is the dialogue act of the segment which means that for all 18 possible dialogue acts •there is a structured list of applicable patterns .</sentence>
				<definiendum id="0">core planning step</definiendum>
				<definiens id="0">consists of the selection and application of an appropriate VIT-pattern of a segment which is determined by the following three main criteria : • The first criterion to find an applicable pattern is the dialogue act of the segment which means that for all 18 possible dialogue acts •there is a structured list of applicable patterns</definiens>
			</definition>
			<definition id="7">
				<sentence>The test-key : sentence-mood stands for the test of the sentence mood of a segment representation , : tempex checks for the existence of a tempex-VIT ( which is not Figure 9 : VIT-semantics of a protocol formulation necessarily the case as in figure 6 , an ACCEPT ca21 contain a temporal expression or not . )</sentence>
				<definiendum id="0">tempex-VIT</definiendum>
				<definiens id="0">the test of the sentence mood of a segment representation</definiens>
			</definition>
			<definition id="8">
				<sentence>VM-GECO is a highly efficient multi-lingual generation component which consists of a language independent kernel syntactic generator and language specific declarative knowledge sources for syntactic and lexical choices .</sentence>
				<definiendum id="0">VM-GECO</definiendum>
				<definiens id="0">a highly efficient multi-lingual generation component which consists of a language independent kernel syntactic generator and language specific declarative knowledge sources for syntactic and lexical choices</definiens>
			</definition>
			<definition id="9">
				<sentence>~CHSVERLAUF PROGRESS OF THE DIALOGUE ) are the individual turns which consist of the paraphrased segments .</sentence>
				<definiendum id="0">~CHSVERLAUF PROGRESS OF THE DIALOGUE )</definiendum>
				<definiens id="0">the individual turns which consist of the paraphrased segments</definiens>
			</definition>
</paper>

		<paper id="0905">
			<definition id="0">
				<sentence>Such statements form sets of constraints that apply at a given level of description , and ill-formedness is often defined as constraint violation .</sentence>
				<definiendum id="0">Such statements</definiendum>
				<definiens id="0">form sets of constraints that apply at a given level of description</definiens>
			</definition>
			<definition id="1">
				<sentence>Following ( Hopcroft and Ullman , 1979 , p.17 ) , • a deterministic finite-state automaton A is a 5tuple ( S , I , ~ , so , F ) , in which S is a finite set of states , I is a finite input a alphabet , ~ is the state transition function mapping S x I to S , so E S is the initial state , and F C S is the set of final states .</sentence>
				<definiendum id="0">E S</definiendum>
				<definiendum id="1">F C S</definiendum>
				<definiens id="0">the set of final states</definiens>
			</definition>
			<definition id="2">
				<sentence>The term FSA is taken to refer to n-level transducers , where the input alphabet consists either of individual symbols a E I or of strings x E I* .</sentence>
				<definiendum id="0">FSA</definiendum>
				<definiens id="0">taken to refer to n-level transducers , where the input alphabet consists either of individual symbols a E I or of strings x E I*</definiens>
			</definition>
			<definition id="3">
				<sentence>Size ( C2 ) is assessed in terms of number of states , the reward being higher the fewer states an FSA has .</sentence>
				<definiendum id="0">Size</definiendum>
			</definition>
			<definition id="4">
				<sentence>The bottom diagram is part of the best automaton discovered for the second half of the German reduced syllable set , shown in Figure 4 .</sentence>
				<definiendum id="0">bottom diagram</definiendum>
				<definiens id="0">part of the best automaton discovered for the second half of the German reduced syllable set</definiens>
			</definition>
			<definition id="5">
				<sentence>Task 1 was to discover the language ( 10 ) ' , Task 2 was 0 '' 1 '' 0 '' 1 '' , Task 3 was `` all strings with an even number of 0 's and an even number of l 's '' , and Task 4 was `` all strings such that the difference between the number of l 's and 0 's is 3 times n ( where n is an integer ) '' .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">an integer ) ''</definiens>
			</definition>
			<definition id="6">
				<sentence>In R. Michalski , K. Carbonell , and T. Mitchell , editors , Machine Learning : An Artificial Intelligence Approach Vol .</sentence>
				<definiendum id="0">Machine Learning</definiendum>
				<definiens id="0">An Artificial Intelligence Approach Vol</definiens>
			</definition>
</paper>

		<paper id="1215">
			<definition id="0">
				<sentence>The Generative Lexicon ( GL ) investigates the problem of the multiplicity of usages of a sense of a lexeme and shows how these usages can be analyzed in terms of possible type shirtings w.r.t , the type expected by a usage of that sense defined as the core usage .</sentence>
				<definiendum id="0">Generative Lexicon ( GL )</definiendum>
				<definiens id="0">investigates the problem of the multiplicity of usages of a sense of a lexeme and shows how these usages can be analyzed in terms of possible type shirtings w.r.t , the type expected by a usage of that sense defined as the core usage</definiens>
			</definition>
			<definition id="1">
				<sentence>Even if it is not comprehensive , it turns out , from our experiments on different semantic classes of verbs , that the LCS ( Lexical Conceptual Structure ( Jackendoff 90 ) ) is a relatively adequate framework ( possibly associated with a few attribute-value pairs for some properties ) to represent the semantics of expressions subject to the sense variations we have identified , and to allow for the implementation of the generative operations advocated in the previous section ( see also B. Dorr 's work on LCS forms for verb classes in English ) .</sentence>
				<definiendum id="0">LCS ( Lexical Conceptual Structure</definiendum>
				<definiens id="0">a relatively adequate framework ( possibly associated with a few attribute-value pairs for some properties ) to represent the semantics of expressions subject to the sense variations we have identified , and to allow for the implementation of the generative operations advocated in the previous section ( see also B. Dorr 's work on LCS forms for verb classes in English</definiens>
			</definition>
			<definition id="2">
				<sentence>Bon may also be combined with determiners expressing measures to indicate that the measure is slightly excedeed as in un bon litre ( a good liter ) , and it is at the origin of a few fixed forms as un bon coup , une bonne gifle ( a good slap ) , which are , in fact , synonymous of un mauvais coup , une mauvaise gifle ( a bad slap ) even if bon and mauvais are opposites .</sentence>
				<definiendum id="0">une mauvaise gifle</definiendum>
			</definition>
			<definition id="3">
				<sentence>Let us assume that any noun which can be modified by bon has a telic role in which the main ftmction ( s ) of the object is described ( e.g. execute programmes for a computer , run for a car ) : noun : N , Qualia : \ [ Telic : T , ... \ ] where T denotes the set of properties associated with Saint-Dizier 126 Sense Variation and Lexical Semantics II II II II II II II II II II II II II II m m m m m m m m B m B m m m m B the telic role of the noun N. Let us assume that T is a sequence of predicates of the form Fi ( X , Y ) where Y denotes the noun N. Let us assume that F~ ( X , Y ) is the property modified by the adjective bon , identified by means of semantic types .</sentence>
				<definiendum id="0">Qualia</definiendum>
				<definiendum id="1">T</definiendum>
				<definiendum id="2">Y</definiendum>
				<definiens id="0">the set of properties associated with Saint-Dizier</definiens>
				<definiens id="1">the property modified by the adjective bon , identified by means of semantic types</definiens>
			</definition>
			<definition id="4">
				<sentence>~From a compositional point of view , the combination Adjective + noun is treated as follows , where R denotes the semantic representation of the adjec- .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">the semantic representation of the adjec-</definiens>
			</definition>
			<definition id="5">
				<sentence>Similarly , we are developing for psychological verbs and verbs of feeling a more appropriate primitive system ( Kamel , forthcoming ) , e.g. with a primitive such as FEEL , which could be appropriate also for the above representation .</sentence>
				<definiendum id="0">FEEL</definiendum>
				<definiens id="0">psychological verbs and verbs of feeling a more appropriate primitive system ( Kamel , forthcoming</definiens>
			</definition>
</paper>

		<paper id="1231">
			<definition id="0">
				<sentence>In this paper , we describe a simple formalism ( KURD x ) that is designed to perform some X KURD is an acronym representing the ftrst letters of the implemented actions : K ( ill ) -U ( nify ) -R ( ¢place ) D ( elete ) Carl and Schmidt-Wigger 257 'shallow ' operations on morphologically analyzed texts .</sentence>
				<definiendum id="0">X KURD</definiendum>
				<definiens id="0">an acronym representing the ftrst letters of the implemented actions</definiens>
			</definition>
			<definition id="1">
				<sentence>Lemmatization generates from an input string a basic word form that does not contain inflectional information .</sentence>
				<definiendum id="0">Lemmatization</definiendum>
				<definiens id="0">generates from an input string a basic word form that does not contain inflectional information</definiens>
			</definition>
			<definition id="2">
				<sentence>Local disjunction is an alternation of atomic values , complex disjunction is an alternation of complex features ( FB ) .</sentence>
				<definiendum id="0">Local disjunction</definiendum>
				<definiendum id="1">FB</definiendum>
				<definiens id="0">an alternation of atomic values</definiens>
			</definition>
			<definition id="3">
				<sentence>The FB describes a possible interpretation of the current WD .</sentence>
				<definiendum id="0">FB</definiendum>
			</definition>
			<definition id="4">
				<sentence>a The test is true if the current WD is a subset of the FB .</sentence>
				<definiendum id="0">WD</definiendum>
				<definiens id="0">a subset of the FB</definiens>
			</definition>
			<definition id="5">
				<sentence>Thus KURD has to be classified as a typical shallow parsing system , also Mlowing for partial parsing .</sentence>
				<definiendum id="0">KURD</definiendum>
				<definiens id="0">has to be classified as a typical shallow parsing system</definiens>
			</definition>
			<definition id="6">
				<sentence>In technical documentation , the quality of the text in terms of completeness , correctness , consistency , readability and user-frlend\ ] hess is a central goal ( Fottner-Top , 1996 ) .</sentence>
				<definiendum id="0">hess</definiendum>
				<definiens id="0">the quality of the text in terms of completeness , correctness , consistency , readability</definiens>
			</definition>
			<definition id="7">
				<sentence>Complexity is a central topic in the readability literature ( see footnote 5 ) , but it does not allow the triggering of a concrete reformulation proposition to the user .</sentence>
				<definiendum id="0">Complexity</definiendum>
				<definiens id="0">a central topic in the readability literature</definiens>
			</definition>
			<definition id="8">
				<sentence>CBAG is an example based translation engine whose aim it is to be used as a stand-alone Example Based Machine T~anslation system ( EBMT ) or to be dynamically integrated as a f~ont-end into a Rule Based Machine T~auslation system .</sentence>
				<definiendum id="0">CBAG</definiendum>
				<definiens id="0">an example based translation engine whose aim it is to be used as a stand-alone Example Based Machine T~anslation system ( EBMT ) or to be dynamically integrated as a f~ont-end into a Rule Based Machine T~auslation system</definiens>
			</definition>
			<definition id="9">
				<sentence>KURD reduces those chunks which match a case in the case base into one chunk descriptor according to the schema of rule 3 .</sentence>
				<definiendum id="0">KURD</definiendum>
			</definition>
			<definition id="10">
				<sentence>In the refinement phase , KURD merges lexical and grammatical information which is extracted from two different sets of cases .</sentence>
				<definiendum id="0">KURD</definiendum>
				<definiens id="0">merges lexical and grammatical information which is extracted from two different sets of cases</definiens>
			</definition>
</paper>

		<paper id="0711">
			<definition id="0">
				<sentence>The set-generation algorithm is an iterative application of the algorithm proposed in ( Hearst and Sht , tze .</sentence>
				<definiendum id="0">set-generation algorithm</definiendum>
				<definiens id="0">an iterative application of the algorithm proposed in ( Hearst and Sht , tze</definiens>
			</definition>
			<definition id="1">
				<sentence>The Generality is a gaussian measure that mediates between over generality and overambiguity .</sentence>
				<definiendum id="0">Generality</definiendum>
				<definiens id="0">a gaussian measure that mediates between over generality and overambiguity</definiens>
			</definition>
			<definition id="2">
				<sentence>Coverage ( CO ) This is a measure of the coverage that a given category set C'i has over the words in the corpus .</sentence>
				<definiendum id="0">Coverage ( CO</definiendum>
				<definiens id="0">a measure of the coverage that a given category set C'i has over the words in the corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>The cumulative scoring function for a set of categories Ci is defined as the linear combination of the performance parameters described above : Sco~ ( C'i ) = aG ( C~ ) + 3C . '</sentence>
				<definiendum id="0">cumulative scoring function for a set of categories Ci</definiendum>
			</definition>
			<definition id="4">
				<sentence>We define the typicality T~ ( C ' ) of w in C , as : Nw , c _ - ( 2 ) I'V w where : , V , , is the total number of synsets of a word w , i.e. all the WordNet synonymy sets including w. .</sentence>
				<definiendum id="0">typicality T~</definiendum>
				<definiens id="0">the total number of synsets of a word w</definiens>
			</definition>
			<definition id="5">
				<sentence>V , o.c is the number of synsets of w that belong to the semantic category C , i.e. synsets indexed with C in WordNet .</sentence>
				<definiendum id="0">o.c</definiendum>
				<definiens id="0">the number of synsets of w that belong to the semantic category C , i.e. synsets indexed with C in WordNet</definiens>
			</definition>
			<definition id="6">
				<sentence>A typical noun for a category C is one that is either non ambiguously assigned to C in WordNet , or that has most of its senses ( synsets ) in C. The synonymy S~ , of w in C , i.e. the degree of synonymy showed by words other than w in the synsets of the class C in which w appears .</sentence>
				<definiendum id="0">synsets</definiendum>
				<definiens id="0">either non ambiguously assigned to C in WordNet , or that has most of its senses</definiens>
			</definition>
			<definition id="7">
				<sentence>c is the number of words in the corpus appearing ill at least one of the synsets of w , that belong to C. `` \ [ 'lie synonymy depends both on WordNet and on the corpus .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">the number of words in the corpus appearing ill at least one of the synsets of w</definiens>
			</definition>
			<definition id="8">
				<sentence>Word Sense Ambiguation : Clustering Related Senses .</sentence>
				<definiendum id="0">Word Sense Ambiguation</definiendum>
			</definition>
</paper>

		<paper id="1224">
			<definition id="0">
				<sentence>For example , the memorybased learning algorithm ml-IG ( Daelemans and Van den Bosch , 1992 ; Daclemans , Van den Bosch , and We~jters , 1997b ) , which extends the well-known ml algorithm ( Aha , Kibler , and Albert , 1991 ) with an information-gain weighted similaxity mettic , has been demonstrated to perform adequately and , moreover , consistently and significantly better than eager-lea~'ning algorithms which do invest effort in abstraction during learning ( e.g. , decisiontree learning ( Daelemans , Van den Bosch , and Weijters , 1997b ; Quinlan , 1993 ) , and connectionist learning ( Rumelhart , Hinton , and Williams , 1986 ) ) when trained and tested on a range of morphophonological tasks ( e.g. , morphological segmentation , grapheme-phoneme conversion , syllabitlcation , and word stress assignment ) ( Daelemans , Gillis , and Durieux , 1994 ; Van den Bosch , Daelemans , and We~jters , 1996 ; Van den Bosch , 1997 ) .</sentence>
				<definiendum id="0">connectionist learning</definiendum>
				<definiens id="0">extends the well-known ml algorithm</definiens>
			</definition>
			<definition id="1">
				<sentence>The coding of the output as 159 atomic ( 'local ' ) classes combining grapheme-phoneme conversion and stress assignment is one out of many types of output coding ( Shavlik , Mooney , and Towel\ ] , 1991 ) , e.g. , distributed bit coding using articulatory features ( Sejnowski and Rosenberg , 1987 ) , error-correcting output coding ( Diettefich , Hild , and Bakid , 1990 ) , or split discrete coding of gmpheme-phoneme conversion and stress assignment ( Van den Bosch , 1997 ) .</sentence>
				<definiendum id="0">stress assignment</definiendum>
			</definition>
			<definition id="2">
				<sentence>An instance in the instance base consists of a fixed-length vector of n feature-value pairs ( here , n = 7 ) , an information field containing the classification of that particular feature-value vector , and an information field containing the occurrences of the instance with its classification in the full training set .</sentence>
				<definiendum id="0">feature-value pairs</definiendum>
				<definiens id="0">an information field containing the classification of that particular feature-value vector , and an information field containing the occurrences of the instance with its classification in the full training set</definiens>
			</definition>
			<definition id="3">
				<sentence>1 : ft A ( X , Y ) = E W ( \ [ i ) 6 ( Xi , Yi ) , ( 1 ) i=l where W ( fi ) is the weight of the ith feature , and 6 ( zl , yi ) is the distance between the values of the ith fcature in the instances X and Y. When the values of the instance features are symbolic , as with the Gs task ( i.e. , feature values are letters ) , a simple distance function is used ( Eq .</sentence>
				<definiendum id="0">Xi</definiendum>
				<definiendum id="1">W ( fi )</definiendum>
				<definiens id="0">the weight of the ith feature</definiens>
			</definition>
			<definition id="4">
				<sentence>( 2 ) The classification of the memory instance type Y with the smallest A ( X , Y ) is then taken as the classification of X. This procedure is also known as 1-NN , i.e. , a search for the single nearest neighbour , the simplest variant of k-NN ( Devijver and Kittler , 1982 ) .</sentence>
				<definiendum id="0">the smallest A ( X</definiendum>
			</definition>
			<definition id="5">
				<sentence>The weighting function of IBI-IG , W ( fi ) , represents the information gain of feature fi .</sentence>
				<definiendum id="0">weighting function of IBI-IG , W ( fi ) ,</definiendum>
				<definiens id="0">represents the information gain of feature fi</definiens>
			</definition>
			<definition id="6">
				<sentence>The expression D\ [ y~=~\ ] refers to those patterns in the data base that have value vj for feature fi , j is the number of possible values of f~ , and V is the set of possible values for feature f~ .</sentence>
				<definiendum id="0">expression D\</definiendum>
				<definiendum id="1">j</definiendum>
				<definiendum id="2">V</definiendum>
				<definiens id="0">the set of possible values for feature f~</definiens>
			</definition>
			<definition id="7">
				<sentence>Finally , IDI is the number of patterns in the ( sub ) data base .</sentence>
				<definiendum id="0">IDI</definiendum>
			</definition>
			<definition id="8">
				<sentence>Intra ( X ) _lFam ( X ) } ~ 1.0-Z~ ( X , Fa , ~ ( X ) ' ) i=l ( 7 ) All remaining instances belong to the subset of unrelated instances , Unr ( X ) .</sentence>
				<definiendum id="0">Intra</definiendum>
				<definiendum id="1">Unr ( X</definiendum>
				<definiens id="0">the subset of unrelated instances</definiens>
			</definition>
			<definition id="9">
				<sentence>Apatt fzom the possibility that the lazy and eager leatning algorithms investigated here and in eatllez work do not have a strongly contrasting bias , we conjecture that the editing methods discussed here , and some specific decision-tree leaxning algorithms investigated eaxlier ( i.e. , IGTItEE ( Daclemuns , Van den Bosch , and Weijters , 1997b ) , a decision tree learning algorithm that is an approximate optimisation of IBI-IG ) have a slmilat vatia~lce to that of IB1IG ; they axe virtually as stable as ~I-IQ .</sentence>
				<definiendum id="0">IBI-IG</definiendum>
				<definiens id="0">an approximate optimisation of</definiens>
			</definition>
</paper>

		<paper id="1117">
			<definition id="0">
				<sentence>In other words , we search for the maximum entropy probability distribution p* : p '' = argmax g ( p ) pEP where P = { p : p meets the empirical feature expectations } and H ( p ) denotes the entropy of p. For parameter estimation , we can use the Improved Iterative Scaling ( IIS ) algorithm ( Berger et all. , 1996 ) , which assumes p to have the form : p ( x , y ) = where fi : X x Y ~ { 0 , 1 } is the indicator function of the i-th feature , Ai the weight assigned to this feature , and Z a normalisation constant .</sentence>
				<definiendum id="0">Improved Iterative Scaling ( IIS ) algorithm</definiendum>
				<definiens id="0">the maximum entropy probability distribution p* : p '' = argmax g ( p ) pEP where P = { p : p meets the empirical feature expectations</definiens>
				<definiens id="1">the entropy of p. For parameter estimation</definiens>
			</definition>
			<definition id="1">
				<sentence>• Parameter estimation is based on the maximum entropy technique , which takes full advantage of the multi-dimensional character of the structural tags ( section 4.3 ) .</sentence>
				<definiendum id="0">maximum entropy technique</definiendum>
			</definition>
			<definition id="2">
				<sentence>The more interesting aspect of our parser is the estimation of contextual probabilities , i.e. , calculating the probability of a structural tag Si ( the `` future '' ) conditional on its immediate predecessors Si1 and Si-2 ( the `` history '' ) .</sentence>
				<definiendum id="0">Si</definiendum>
				<definiens id="0">the estimation of contextual probabilities , i.e. , calculating the probability of a structural tag</definiens>
			</definition>
			<definition id="3">
				<sentence>We say that a feature fk defined by the triple ( Mi-2 , Mi-1 , Mi ) of attribute-value matrices is active on a trigram context ( S~_2 , S~_i , S~ ) ( i.e. , fk ( S~_ 2 , S~_1 , S~ ) = 1 ) iff Mj unifies with the attribute-value matrix /t'I~ encoding the information contained in S~ for j = i 2 , i 1 , i. A novel context would on average activate more features than in the standard HMM approach , which treats the ( ti , ri , c~ &gt; triples as atoms .</sentence>
				<definiendum id="0">S~ )</definiendum>
				<definiendum id="1">fk</definiendum>
				<definiens id="0">1 ) iff Mj unifies with the attribute-value matrix /t'I~ encoding the information contained in S~ for j = i 2</definiens>
			</definition>
</paper>

		<paper id="1223">
			<definition id="0">
				<sentence>E ( Daelemans , Van den Bosch , and Weijters , 1997 ) is a top-down induction of decision trees ( TDIDT ) algorithm ( Breiman et al. , 1984 ; Quinlan , 1993 ) .</sentence>
				<definiendum id="0">E</definiendum>
			</definition>
			<definition id="1">
				<sentence>TDIDT is a widely-used method in supervised machine learning ( Mitchell , 1997 ) .</sentence>
				<definiendum id="0">TDIDT</definiendum>
			</definition>
			<definition id="2">
				<sentence>Information gain is a function from information theory , and is used similarly in ID3 ( Qululan , 1986 ) and c4.5 ( Qnlnlan , 1993 ) .</sentence>
				<definiendum id="0">Information gain</definiendum>
				<definiens id="0">a function from information theory</definiens>
			</definition>
			<definition id="3">
				<sentence>van den Bosch , Weijters and Daelemans 186 Modularity in Word Pronunciation systems I I I I I I l | I I I / | / / / refers to those patterns in the data base that have value vj for feature f~ , j is the number of possible values of f~ , and V is the set of possible values for feature /~ .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">those patterns in the data base that have value vj for feature f~ , j is the number of possible values of f~</definiens>
				<definiens id="1">the set of possible values for feature /~</definiens>
			</definition>
			<definition id="4">
				<sentence>Finally , \ ] DI is the number of patterns in the ( sub ) data base .</sentence>
				<definiendum id="0">DI</definiendum>
			</definition>
			<definition id="5">
				<sentence>Apart from storing uniquely identified class labels at leafs , IGTREE stores at each non-terminal node information on the most probable classification given the path so far .</sentence>
				<definiendum id="0">IGTREE</definiendum>
				<definiens id="0">stores at each non-terminal node information on the most probable classification given the path so far</definiens>
			</definition>
			<definition id="6">
				<sentence>We extracted from the Engiish data base of CZLZX all the above information , resulting in a data base containing 77,565 unique items ( word forms with syllabified , stressed pronunciations and morphdogical segmentations ) .</sentence>
				<definiendum id="0">CZLZX</definiendum>
				<definiens id="0">all the above information , resulting in a data base containing 77,565 unique items ( word forms with syllabified , stressed pronunciations and morphdogical segmentations )</definiens>
			</definition>
			<definition id="7">
				<sentence>M stands for morphological decomposition : determine whether a letter is the initial letter of a morpheme ( class '1 ' ) or not ( class 'O ' ) .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">morphological decomposition : determine whether a letter is the initial letter of a morpheme ( class '1 '</definiens>
			</definition>
			<definition id="8">
				<sentence>x is graphemic parsing2 : determine whether a letter is the first or only letter of a grapheme ( class '1 ' ) or not ( class '0 ' ) ; a grapheme is a cluster of one or more letters mapping to a single phoneme .</sentence>
				<definiendum id="0">grapheme</definiendum>
				<definiens id="0">a cluster of one or more letters mapping to a single phoneme</definiens>
			</definition>
			<definition id="9">
				<sentence>Finally , GS is integrated grapheme-phoneme conversion and stress assignment .</sentence>
				<definiendum id="0">GS</definiendum>
				<definiens id="0">integrated grapheme-phoneme conversion and stress assignment</definiens>
			</definition>
			<definition id="10">
				<sentence>syllabification subtask makes finding those syllable boundaries which are rdevant for stress assignment an integrated paxt of stress assignment .</sentence>
				<definiendum id="0">syllabification subtask</definiendum>
				<definiens id="0">makes finding those syllable boundaries which are rdevant for stress assignment an integrated paxt of stress assignment</definiens>
			</definition>
			<definition id="11">
				<sentence>GS is a single-module system in which only one classification task is performed in one pass .</sentence>
				<definiendum id="0">GS</definiendum>
				<definiens id="0">a single-module system in which only one classification task is performed in one pass</definiens>
			</definition>
			<definition id="12">
				<sentence>The GS task integrates grapheme-phoneme conversion and stress assignment : to classify letter windows as corresponding to a phoneme wi~h a stress marker ( PS ) .</sentence>
				<definiendum id="0">GS task</definiendum>
				<definiens id="0">integrates grapheme-phoneme conversion and stress assignment : to classify letter windows as corresponding to a phoneme wi~h a stress marker ( PS )</definiens>
			</definition>
			<definition id="13">
				<sentence>Compartments indicate the numbers of nodes needed for the trees of the subtasks specified by their labels .</sentence>
				<definiendum id="0">Compartments</definiendum>
			</definition>
</paper>

		<paper id="0603">
			<definition id="0">
				<sentence>3 Each entry consists of a list of words , stored in the lexicon independently of their POS ( the verb and noun form of walk are under the same superentry ) .</sentence>
				<definiendum id="0">entry</definiendum>
				<definiens id="0">consists of a list of words , stored in the lexicon independently of their POS ( the verb</definiens>
			</definition>
			<definition id="1">
				<sentence>Lexemes can be mapped to Objects ( O ) ( ' ; Car '' car ) , Events ( E ) ( `` Explode '' explosion ) , Relations ( R ) ( `` Utilizes '' use ) or Attributes ( A ) ( `` ColourAttribute '' colour ) .</sentence>
				<definiendum id="0">Attributes</definiendum>
				<definiens id="0">Events ( E ) ( `` Explode '' explosion ) , Relations ( R ) ( `` Utilizes '' use</definiens>
			</definition>
			<definition id="2">
				<sentence>In the case of `` computer database , '' the lexicon entry for `` database '' encodes the syntagmatic relation ( LSFSyn ) which keeps the semantics of the nouns compositional and signals the processor ( analyser or generator ) to consider the nouns as syntactically linked : # O=\ [ key : `` ~ \ ] ~ '' , reh lsyntagmati¢ : LSFSyn \ [ base : # 0 , co-occur : \ [ key : `` ~ ~\ [ ~ '' , sense : n i , ... \ ] \ ] \ ] \ ] We provide below the example of a Chinese sentence , its English translation and relevant parts of the result of the semantic analysis , showing the analysis of the compound ~ ~ `` tackle-key-problem '' .</sentence>
				<definiendum id="0">LSFSyn</definiendum>
				<definiens id="0">keeps the semantics of the nouns compositional and signals the processor</definiens>
			</definition>
</paper>

		<paper id="1423">
			<definition id="0">
				<sentence>We show how a bag generation algorithm , developed for use with categorial grammar and indexed QLF , can be used with HPSG and MinimalRecursion Semantics .</sentence>
				<definiendum id="0">QLF</definiendum>
				<definiens id="0">a bag generation algorithm , developed for use with categorial grammar and indexed</definiens>
			</definition>
			<definition id="1">
				<sentence>In contrast to categorial grammar , HPSG has some fundamental difficulties with highly incremental generation .</sentence>
				<definiendum id="0">HPSG</definiendum>
				<definiens id="0">some fundamental difficulties with highly incremental generation</definiens>
			</definition>
			<definition id="2">
				<sentence>&lt; verb val ! ( subj ! \ [ @ np &amp; loc ! ( cat ! head ! case ! &lt; nom &amp; cont ! index ! Subj a conx ! backgr ! BO-Bl qstore ! QO-Ql ) \ ] &amp; ¢omps ! \ [ @ np &amp; loc ! ( cat ! head ! case ! &lt; acc cont ! index ! Obj conx ! backgr ! BI-BN qstore ! QI-QN ) \ ] ) ) • cont ! nuc ! ( seer ! Subj a seen ! Ob3 ) conx ! backgr ! BO-BN &amp; qstore ! QO-QN ) ) . 'SHIP ' : = hd_dtr ! 'QUIP ' : = hd_dtr ! 'CHIP ' : = hd_dtr ! synsem ! loc ! cont ! Cont synsem ! ! oc ! cont ! Cont. synsem ! loc ! qstore ! QS synsem ! loc ! qstore ! QS. sYnsem ! loc ! conx ! Conx synsem ! loc ! conx ! Conx. hd_nexus_ph : = &lt; hd_nexus_ph ~ @ hd_ph @ 'SHIP ' &amp; @ 'QUIP ' ~ @ 'CHIP'. Figure 3 : Lex\ [ cal Amalgamation and Logical Form Inheritance The basic Semantics Principle , for semantic content only. was implemented by tile template 'SetuP ' shown in Figure 1. In order to ensure the required sharing of unscoped quantifiers and background .conditions between a phrase and its semant.ic hcad , the Semamics Principle is extended , as proposed by Wilcock ( 1997 ) , to three principles : Semantic Itead Inheritance Principle ( StIIP ) . Quantifier Inheritance Principle ( QUIP ) . 221 and Contextual Head Inheritance Principle ( CHIP ) . These are implemented by templates as shown in Figure 3 , and the three principles are included in the grammar by the modified template for hd : .aexus_ph , which replaces the earlier template in Figure 1. With these revisions , it is possible to include unscoped quantifiers and background conditions in the starting logical form , and perform head-driven generation successfully using the BUG1 generator. We now switch to non , head-driven approaches. Phillips ( 1993 ) proposed a bottom-up chart generation algorithm for use With indexed logical forms and categorial grammar in machine translation. An important property of the algorithm is that the Order of terms in the logical form is not significant. The name bag generation is adopted from related work on shake-and-bake machine translation. We now show how Phillips ' algorithm can-be used with HPSG grammar ; with a bag of MRS relations instead of a bag of indexed logical terms. Though he presents the algorithm as a generator for categorial grammar , PhilliPs suggests that it can be adapted for use with phrasestructure grammar ( PSG ) , provided an indexed logical form is used -and the indices are included in the syntactic categories. HPSG uses indices for agreement , and therefore includes the indices inside syntactic categories. By implementing HPSG as a PSG extended with typed feature structures ( Sect.ion 1.2 ) , and by implementing MRS as a similarly ext.ended : indexed QLF ( SeCtion 4.2 ) , we can adapt• Phillips I algorithm for use with HPSG. . : The adapted algorithm is shown in Figure 4. \Ve use simple chart processing from the bottom-up chart. parser of Gazdar and Mellish ( 1989 ) . The main work is done by start_gen , which looks up the next term in the list of semantic tern ) s , adds appropriate edges to the chart , and calls itself recursively on the remaining terms. Generation finishes when all edges have been built , and is successful if an inactive edge `` 'spans '' the Whole input semantics , i.e. if an inactive edge 's semantics are a laermufalion of the input , semantics. permutation/2 : is a library predicate. generate ( Semantics , Category , String ) -abolish ( edge,2 ) , startigen ( Semantics ) , clause ( edge ( Category , \ [ \ ] ) , true ) , semantics ( Category , EdgeSem ) , permutation ( Semantics , EdgeSem ) , string ( Category , String ) . .start_gen ( \ [ \ ] ) . start_gen ( \ [ TermlTerms\ ] ) . : foreach ( lookup_term ( Term , Category ) , • add edge ( Category , I\ ] ) ) , start_gen ( Terms ) . add_edge ( Cat , Cats ) : clause ( edge ( Cat , Cats ) , true ) , ! . add_edge ( Catl , \ [ \ ] ) `` asserta ( edge ( Catl , \ [ \ ] ) ) , foreach ( rule ( Cat2 , \ [ CatllCats\ ] ) , add_edge ( Cat2 , \ [ CatllCats\ ] ) ) , ~oreach ( edge ( Cat2 , \ [ CatiICatS\ ] ) , add_edge ( Cat2 , Cats ) ) . add_edge ( Cat ! , \ [ Cat21Cats\ ] ) : asserta ( edge ( Cat1 , ECat21Cats\ ] ) ) , foreach ( edge ( Cat2 , \ [ \ ] ) , add_edge ( Cat1 , Cats ) ) . rule ( Mother , Daughters ) : ( Mother -- - &gt; Daughters ) .</sentence>
				<definiendum id="0">Semamics Principle</definiendum>
				<definiendum id="1">PSG</definiendum>
				<definiendum id="2">Sect.ion 1.2</definiendum>
				<definiens id="0">provided an indexed logical form is used -and the indices are included in the syntactic categories. HPSG uses indices for agreement , and therefore includes the indices inside syntactic categories. By implementing HPSG as a PSG extended with typed feature structures</definiens>
				<definiens id="1">looks up the next term in the list of semantic tern ) s , adds appropriate edges to the chart , and calls itself recursively on the remaining terms. Generation finishes when all edges have been built</definiens>
			</definition>
			<definition id="3">
				<sentence>For a particular lexicon , lookup_term returns the Category of a lexical entry which includes the given semantic predicate , giving an interface between the algorithm and the lexicon .</sentence>
				<definiendum id="0">lookup_term</definiendum>
				<definiens id="0">returns the Category of a lexical entry which includes the given semantic predicate , giving an interface between the algorithm and the lexicon</definiens>
			</definition>
			<definition id="4">
				<sentence>A full MRS representation includes a top-level handle and a top-level index , which are specified separately from the flat list'of terms .</sentence>
				<definiendum id="0">full MRS representation</definiendum>
				<definiens id="0">includes a top-level handle and a top-level index , which are specified separately from the flat list'of terms</definiens>
			</definition>
			<definition id="5">
				<sentence>From an incomplete bag and a partial utterance , the generator attempts to continue the utterance as further semantic terms are added .</sentence>
				<definiendum id="0">generator</definiendum>
				<definiens id="0">attempts to continue the utterance as further semantic terms are added</definiens>
			</definition>
			<definition id="6">
				<sentence>Utterance Rule 1 succeeds if it finds an inactive edge ( a coinplete syntactic constituent ) which spans the bag of semantic terms , and whose PHON list is a continuation of the list uttered so far .</sentence>
				<definiendum id="0">inactive edge</definiendum>
				<definiens id="0">a coinplete syntactic constituent ) which spans the bag of semantic terms , and whose PHON list is a continuation of the list uttered so far</definiens>
			</definition>
			<definition id="7">
				<sentence>For example , discussing the possible adaptation of Phillips ' algorithm to incremental generation , Lager and Black ( 1994 ) point out that some versions of Categorial Grammar ( CG ) would make the generator more talkative , by giving rise to `` a more generous notion of constituency '' .</sentence>
				<definiendum id="0">CG</definiendum>
				<definiens id="0">discussing the possible adaptation of Phillips ' algorithm to incremental generation</definiens>
			</definition>
			<definition id="8">
				<sentence>A systemic generation algorithm traverses the system network , making choices within the systems as it goes , and collecting realization rules which will decide the final output : To apply this to HPSG would mean implementing the HPSG type hierarchy as a system network , and traversing it with a systemic generation algorithm , making choices within the subtypes as it goes , and collecting type constraints which will decide the final output .</sentence>
				<definiendum id="0">systemic generation algorithm</definiendum>
				<definiens id="0">a systemic generation algorithm , making choices within the subtypes as it goes , and collecting type constraints which will decide the final output</definiens>
			</definition>
</paper>

		<paper id="0710">
			<definition id="0">
				<sentence>Lexical resources used in natural language processing ( NLP ) have evolved from handcrafted lexical entries to machine readable lexical databases and large corpora which allow statistical manipulation .</sentence>
				<definiendum id="0">Lexical resources</definiendum>
				<definiens id="0">used in natural language processing ( NLP ) have evolved from handcrafted lexical entries to machine readable lexical databases and large corpora which allow statistical manipulation</definiens>
			</definition>
			<definition id="1">
				<sentence>A similarity score B ( j , k ) is computed for the jth WN synset ( taking the synset itself , the hypernyms , and the coordinate terms ) and the k th ROGET class , according to the following : B ( j , k ) = bIISj N P~I + b~iHyp ( Sj ) N Pkl + b31Co ( Sj ) n Pkl We have set bl = b2 = b3 = 1 .</sentence>
				<definiendum id="0">similarity score B</definiendum>
				<definiens id="0">computed for the jth WN synset ( taking the synset itself , the hypernyms , and the coordinate terms</definiens>
			</definition>
			<definition id="2">
				<sentence>Reading the matrices row-wise shows how vaguely a certain sense is defined , whereas reading them columnwise reveals how polysemous a word is .</sentence>
				<definiendum id="0">Reading the matrices row-wise</definiendum>
				<definiens id="0">shows how vaguely a certain sense</definiens>
			</definition>
</paper>

		<paper id="1212">
			<definition id="0">
				<sentence>The Generator ( Section 3 ) uses semantic activation to quickly form the initial Argument Graph for an argument , or to quickly extend an already existing t , Re-nin l LAroment Generator I Agents Analyzer .</sentence>
				<definiendum id="0">Generator</definiendum>
				<definiens id="0">semantic activation to quickly form the initial Argument Graph for an argument , or to quickly extend an already existing t</definiens>
			</definition>
			<definition id="1">
				<sentence>An Argument Graph is a network structure with nodes that represent propositions , and connecting links that represent the inferences that connect these propositions .</sentence>
				<definiendum id="0">Argument Graph</definiendum>
				<definiens id="0">a network structure with nodes that represent propositions</definiens>
			</definition>
			<definition id="2">
				<sentence>An Argument Graph is fleshed out by consulting several sources of information called Reasoning Agents and incorporating the relevant inferences and propositions returned by these sources into the Argument Graph .</sentence>
				<definiendum id="0">Argument Graph</definiendum>
				<definiens id="0">fleshed out by consulting several sources of information called Reasoning Agents and incorporating the relevant inferences and propositions returned by these sources into the Argument Graph</definiens>
			</definition>
			<definition id="3">
				<sentence>The Analyzer uses a normative model to gauge the normative strength of an argument .</sentence>
				<definiendum id="0">Analyzer</definiendum>
			</definition>
			<definition id="4">
				<sentence>There are two important differences between NAG and the work by Hobbs et al. : NAG is a system that reasons under uncertainty , and NAG performs both analysis and generation .</sentence>
				<definiendum id="0">NAG</definiendum>
				<definiendum id="1">NAG</definiendum>
				<definiens id="0">a system that reasons under uncertainty</definiens>
			</definition>
			<definition id="5">
				<sentence>The Generator receives the following inputs : ( 1 ) a proposition to be argued for ; ( 2 ) an initial argument context ; ( 3 ) two target ranges of degrees of belief to be achieved ( one each for the normative model and the user model ) ; and ( 4 ) a system attitude parameter , which determines the extent to which the system will take advantage of the user 's erroneous beliefs .</sentence>
				<definiendum id="0">Generator</definiendum>
				<definiens id="0">determines the extent to which the system will take advantage of the user 's erroneous beliefs</definiens>
			</definition>
			<definition id="6">
				<sentence>A single KB represents information in one format , e.g. , a semantic network ( SN ) , Bayesian network ( BN ) , rule-based system , or database .</sentence>
				<definiendum id="0">single KB</definiendum>
				<definiens id="0">represents information in one format , e.g. , a semantic network ( SN ) , Bayesian network ( BN ) , rule-based system , or database</definiens>
			</definition>
			<definition id="7">
				<sentence>When assembling an Argument Graph , NAG develops two BNs : the BN forming one of the KBs in the user model , and the BN forming one of the KBs in the normative model .</sentence>
				<definiendum id="0">NAG</definiendum>
				<definiens id="0">develops two BNs : the BN forming one of the KBs in the user model</definiens>
			</definition>
			<definition id="8">
				<sentence>Bayesian network propagation ( Pearl , 1988 ) , optimized or otherwise ( see , for example , Li &amp; D'Ambrosio , 1994 ) , is an NP-hard problem in the general case ( Cooper , 1990 ) .</sentence>
				<definiendum id="0">Bayesian network propagation</definiendum>
			</definition>
			<definition id="9">
				<sentence>NAG takes the context in which theargnment occurs as providing an initial set of salient objects .</sentence>
				<definiendum id="0">NAG</definiendum>
				<definiens id="0">takes the context in which theargnment occurs as providing an initial set of salient objects</definiens>
			</definition>
			<definition id="10">
				<sentence>The initial Argument Graph consists of the subset of the BNs which was activated by the attentional mechanism .</sentence>
				<definiendum id="0">Argument Graph</definiendum>
				<definiens id="0">consists of the subset of the BNs which was activated by the attentional mechanism</definiens>
			</definition>
			<definition id="11">
				<sentence>NAG decides whether to introduce new inferences returned by the Reasoning Agents into the Argument Graph ( or to replace existing inferences with new ones ) by applying two simple rules designed to ensure that each relationship between propositions in the Argument Graph is represented only once .</sentence>
				<definiendum id="0">NAG</definiendum>
				<definiens id="0">decides whether to introduce new inferences returned by the Reasoning Agents into the Argument Graph ( or to replace existing inferences with new ones ) by applying two simple rules designed to ensure that each relationship between propositions in the Argument Graph is represented only once</definiens>
			</definition>
			<definition id="12">
				<sentence>NAG applies an S-curve to convert frequencies into user probabilities ( Figure 5 ) .</sentence>
				<definiendum id="0">NAG</definiendum>
				<definiens id="0">applies an S-curve to convert frequencies into user probabilities ( Figure 5 )</definiens>
			</definition>
			<definition id="13">
				<sentence>NAG was designed to be a system which generates arguments and receives user arguments .</sentence>
				<definiendum id="0">NAG</definiendum>
				<definiens id="0">designed to be a system which generates arguments and receives user arguments</definiens>
			</definition>
			<definition id="14">
				<sentence>Partial propagation , performed over the subnetworks in focus ( the current Argument Graph ) , is used to estimate the impact of the resultant argument .</sentence>
				<definiendum id="0">Partial propagation</definiendum>
			</definition>
</paper>

		<paper id="0801">
			<definition id="0">
				<sentence>The discourse manager takes the appropriate action and tells the user the result .</sentence>
				<definiendum id="0">discourse manager</definiendum>
				<definiens id="0">takes the appropriate action and tells the user the result</definiens>
			</definition>
			<definition id="1">
				<sentence>Bigram models give the probability of each word depending on the previous word and trigram models give the probability of each word depending on the previous two words .</sentence>
				<definiendum id="0">Bigram models</definiendum>
				<definiens id="0">give the probability of each word depending on the previous word and trigram models give the probability of each word depending on the previous two words</definiens>
			</definition>
			<definition id="2">
				<sentence>ilnm tl Norte\ [ OpenSpeech The NLU component attempts to extract the meaning from the recognized word string .</sentence>
				<definiendum id="0">NLU component</definiendum>
				<definiens id="0">attempts to extract the meaning from the recognized word string</definiens>
			</definition>
</paper>

		<paper id="0805">
			<definition id="0">
				<sentence>The MARSEC corpus has been mainly collected from the BBC , and is available free on the web ( see references ) .</sentence>
				<definiendum id="0">MARSEC corpus</definiendum>
				<definiens id="0">the BBC , and is available free on the web ( see references )</definiens>
			</definition>
			<definition id="1">
				<sentence>The entropy H ( X ) is defined as H ( X ) = Z p ( x ) • Io9 p ( x ) xE .</sentence>
				<definiendum id="0">entropy H ( X</definiendum>
			</definition>
			<definition id="2">
				<sentence>H0 represents the average number of bits required to determine a letter with no statistical information .</sentence>
				<definiendum id="0">H0</definiendum>
				<definiens id="0">the average number of bits required to determine a letter with no statistical information</definiens>
			</definition>
			<definition id="3">
				<sentence>H2 uses information on the probability of 2 letters occurring together ; Hn , called the ngram entropy , measures the amount of entropy with information extending over n adjacent letters of text , 1 and Hn _ &lt; Hn-1 .</sentence>
				<definiendum id="0">ngram entropy</definiendum>
				<definiens id="0">measures the amount of entropy with information extending over n adjacent letters of text</definiens>
			</definition>
</paper>

		<paper id="0704">
			<definition id="0">
				<sentence>WordNet has been used in numerous natural language processing , such as part of speech tagging ( Segond et al. , 97 ) , word sense disambiguation ( Resnik , 1995 ) , text categorization ( Gomez-Hidalgo and Rodriguez , 1997 ) , information extraction ( Chai and Biermann , 1997 ) , and so on with considerable success .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiendum id="1">information extraction</definiendum>
				<definiens id="0">used in numerous natural language processing , such as part of speech tagging ( Segond et al. , 97 ) , word sense disambiguation</definiens>
			</definition>
			<definition id="1">
				<sentence>The information retrieval system compares the query with documents in the collection and returns the documents that are likely to satisfy the user 's information requirements .</sentence>
				<definiendum id="0">information retrieval system</definiendum>
				<definiens id="0">compares the query with documents in the collection and returns the documents that are likely to satisfy the user 's information requirements</definiens>
			</definition>
			<definition id="2">
				<sentence>An information retrieval test collection consists of a collection of documents along with a set of test queries .</sentence>
				<definiendum id="0">information retrieval test collection</definiendum>
				<definiens id="0">consists of a collection of documents along with a set of test queries</definiens>
			</definition>
			<definition id="3">
				<sentence>First , all the documents are parsed using the Apple Pie Parser , which is a probabilistic chart parser developed by Satoshi Sekine ( Sekine and Grisbman , 1995 ) .</sentence>
				<definiendum id="0">Apple Pie Parser</definiendum>
				<definiendum id="1">Satoshi Sekine</definiendum>
				<definiens id="0">a probabilistic chart parser developed by</definiens>
			</definition>
			<definition id="4">
				<sentence>, ,~ ) f ( vl ) +fobj ( nj ) ' where fobi ( vi , ni ) is the frequency of noun n i occurring as the object of verb vi , fobj ( nj ) is the frequency of the noun nj occurring as object of any verb , and f ( vi ) is the frequency of the verb vi • C~aj ( a~ , n3 ) = 2×/od~ ( .</sentence>
				<definiendum id="0">ni )</definiendum>
				<definiendum id="1">nj )</definiendum>
				<definiendum id="2">f</definiendum>
				<definiens id="0">the frequency of noun n i occurring as the object of verb vi</definiens>
				<definiens id="1">the frequency of the noun nj occurring as object of any verb</definiens>
			</definition>
			<definition id="5">
				<sentence>~ ) f ( ai ) 'l'fadj ( nj ) ' where f ( ai , nj ) is the frequency of noun nj occurring as argument of adjective ai , fadi ( nj ) is the frequency of the noun n i occurring as argument of any adjective , and f ( a 0 is the frequency of the adjective ai We define the object similarity of two nouos with respect to one predicate , as the minimum of each dice coefficient with respect to that predicate , i.e. SI~'t/I , ub ( Vi , rlj , nk ) =min { C.ub ( Vi , nj ) , C.ub ( Vi , nk ) } SIMobi ( vi , n i , n~ ) =rnin { Cobj ( vi , nj ) , Cob1 ( vi , nh ) } $ IM~dj ( ai , n i , nk ) =min { C~dj ( a~ , n j ) , C~dj ( a , , nk ) } Finally the overall similarity between two nouns is defined as the average of all the similarities between those two nouns for all predicateargument structures .</sentence>
				<definiendum id="0">~ ) f ( ai ) 'l'fadj</definiendum>
				<definiendum id="1">f ( ai</definiendum>
				<definiendum id="2">nj )</definiendum>
				<definiendum id="3">fadi ( nj )</definiendum>
				<definiens id="0">the frequency of the noun n i occurring as argument of any adjective , and f ( a 0 is the frequency of the adjective ai We define the object similarity of two nouos with respect to one predicate , as the minimum of each dice coefficient with respect to that predicate , i.e. SI~'t/I , ub ( Vi , rlj , nk ) =min { C.ub ( Vi , nj ) , C.ub ( Vi , nk ) } SIMobi ( vi , n i , n~ ) =rnin { Cobj ( vi , nj ) , Cob1 ( vi , nh ) } $ IM~dj ( ai , n i , nk ) =min { C~dj ( a~ , n j ) , C~dj ( a , , nk ) } Finally the overall similarity between two nouns is defined as the average of all the similarities between those two nouns for all predicateargument structures</definiens>
			</definition>
			<definition id="6">
				<sentence>The weight ( q , tj ) of an expansion term tj is defined as a function of simqt ( q , tj ) : weight ( q , tj ) = simqt ( q , tj ) t , eq qi where 0 _ &lt; weight ( q , tj ) &lt; _ 1 .</sentence>
				<definiendum id="0">expansion term tj</definiendum>
				<definiens id="0">a function of simqt ( q , tj ) : weight ( q , tj ) = simqt ( q , tj ) t , eq qi where 0 _ &lt; weight ( q , tj ) &lt; _ 1</definiens>
			</definition>
			<definition id="7">
				<sentence>SMART is an information retrieval engine based on the vector space model in which term weights are calculated based on term frequency , inverse document frequency and document length normalization .</sentence>
				<definiendum id="0">SMART</definiendum>
				<definiens id="0">an information retrieval engine based on the vector space model in which term weights are calculated based on term frequency , inverse document frequency and document length normalization</definiens>
			</definition>
</paper>

		<paper id="0705">
			<definition id="0">
				<sentence>The best-known publicly available corpus handtagged with WordNet senses is SEMCOR ( Miller et al. , 1993 ) , a subset of the Brown Corpus of about 100 documents that occupies about 11 Mb .</sentence>
				<definiendum id="0">SEMCOR</definiendum>
			</definition>
			<definition id="1">
				<sentence>Each summary is a human explanation of the text contents , not a mere bag of related keywords .</sentence>
				<definiendum id="0">summary</definiendum>
				<definiens id="0">a human explanation of the text contents , not a mere bag of related keywords</definiens>
			</definition>
</paper>

		<paper id="1115">
			<definition id="0">
				<sentence>Fortunately , there are well known O ( n 3 ) algorithms for parsing , where n is the length of the sentence .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>Left-factoring replaces each production A ~ /3 : p , where p is the production probability and Jill = n &gt; 2 , with the following set of binary productions : A ~ '~1 , n-l'fln : P 'fll , i ' ~ '~l , i-l ' ~i : 1.0 '/~1,2 ' ~ /~1 ~2:1.0 for i e \ [ 3 , n\ ] In these productions j3i is the ith element of ~3 and '~3i , j ' is the subsequence /3i ... flj of fl , but treated as a 'new ' single non-terminal in the left-factored grammar ( the quote marks indicate that this subsequence is to be considered a single symbol ) .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the ith element of ~3 and '~3i</definiens>
			</definition>
			<definition id="2">
				<sentence>The key observation is that the 'new ' non-terminals 'fll , i ' in a CKY parse using a left-factored grammar correspond to the set of non-empty incomplete edges A ~ fll , i `` fli+l , n in the bottom-up variant of tim Earley algorithm , where A ~ fll , n is a production of the original grammar .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
</paper>

		<paper id="1003">
</paper>

		<paper id="1234">
			<definition id="0">
				<sentence>A lot of researchers posed the Loebner Prize as the first formal instantiation of the Turing Test .</sentence>
				<definiendum id="0">Loebner Prize</definiendum>
				<definiens id="0">the first formal instantiation of the Turing Test</definiens>
			</definition>
</paper>

		<paper id="1214">
			<definition id="0">
				<sentence>Automatic word categorization is an important field of application in statistical natural language processing where the process is unsupervised and is carried out by working on n-gram statistics to find out the categories of words .</sentence>
				<definiendum id="0">Automatic word categorization</definiendum>
				<definiens id="0">an important field of application in statistical natural language processing where the process is unsupervised and is carried out by working on n-gram statistics to find out the categories of words</definiens>
			</definition>
			<definition id="1">
				<sentence>Lankhorst uses genetic algorithms to determine the members of predetermined classes .</sentence>
				<definiendum id="0">Lankhorst</definiendum>
				<definiens id="0">uses genetic algorithms to determine the members of predetermined classes</definiens>
			</definition>
			<definition id="2">
				<sentence>Wilms ( Wilms , 1995 ) uses corpus based techniques together with knowledge-based techniques in order to induce a lexical sublanguage grammar .</sentence>
				<definiendum id="0">Wilms</definiendum>
				<definiens id="0">uses corpus based techniques together with knowledge-based techniques in order to induce a lexical sublanguage grammar</definiens>
			</definition>
			<definition id="3">
				<sentence>Machine Translation is an other area where knowledge bases and statistics are integrated .</sentence>
				<definiendum id="0">Machine Translation</definiendum>
				<definiens id="0">an other area where knowledge bases and statistics are integrated</definiens>
			</definition>
			<definition id="4">
				<sentence>Since the mutual information denotes the amount of probabilistic knowledge that a word provides on the proceeding word , if similar behaving words would be collected together into the same cluster , then the loss of mutual information would be minimal .</sentence>
				<definiendum id="0">mutual information</definiendum>
				<definiens id="0">the amount of probabilistic knowledge that a word provides on the proceeding word</definiens>
			</definition>
			<definition id="5">
				<sentence>Here Px is the probability of occurrences of word pairs .</sentence>
				<definiendum id="0">Px</definiendum>
				<definiens id="0">the probability of occurrences of word pairs</definiens>
			</definition>
			<definition id="6">
				<sentence>Px ( wl , Z ) is the probability where ~ wl appears as the first element in a word pair and Px ( Z , wl ) is the reverse probability where wl is the second element of the word pair .</sentence>
				<definiendum id="0">wl</definiendum>
				<definiens id="0">the probability where ~ wl appears as the first element in a word pair</definiens>
				<definiens id="1">the second element of the word pair</definiens>
			</definition>
			<definition id="7">
				<sentence>The membership function uij that is the degree of membership of the i th element to the jth cluster is defined as : I ( q-i ) Uij -~ K Ek-~l I 1 I ( q-~l } ( 1 ) Here Xi denotes an element in the search space , Vj is the centroid of the jth cluster .</sentence>
				<definiendum id="0">membership function uij</definiendum>
				<definiendum id="1">Vj</definiendum>
				<definiens id="0">the degree of membership of the i th element to the jth cluster is defined as : I ( q-i ) Uij -~ K Ek-~l I 1 I ( q-~l } ( 1 ) Here Xi denotes an element in the search space</definiens>
			</definition>
			<definition id="8">
				<sentence>K denotes the number of clusters .</sentence>
				<definiendum id="0">K</definiendum>
				<definiens id="0">the number of clusters</definiens>
			</definition>
			<definition id="9">
				<sentence>If we use the Manhattan metric , the distance function would be : Korkmaz and G61aiirk ( \ ] 9oluk 114 Choosing,4 Distance Metric for Word Categorization I I m II | II II II II m II m m Ii m II II / / / / l I / / II / D ( wl , w2 ) = ~ I Nw , i-Nw=i l+ Ni~-Niw= I l &lt; i &lt; n ( 4 ) Here n is the total number of words to be clustered , gwai is the number of times word couple ( wt , wi ) is observed in the corpus and Niwa is the number of times word couple ( wi , wl ) is observed .</sentence>
				<definiendum id="0">Manhattan metric</definiendum>
				<definiendum id="1">n</definiendum>
				<definiendum id="2">gwai</definiendum>
				<definiendum id="3">Niwa</definiendum>
				<definiens id="0">the total number of words to be clustered</definiens>
				<definiens id="1">the number of times word couple ( wt , wi ) is observed in the corpus and</definiens>
			</definition>
</paper>

		<paper id="1221">
			<definition id="0">
				<sentence>Natural language generation is a kind of process that encodes the mental picture of reality into a sequence of words called grammatical units such as clause , verbal group , noun group etc .</sentence>
				<definiendum id="0">Natural language generation</definiendum>
				<definiens id="0">a kind of process that encodes the mental picture of reality into a sequence of words called grammatical units such as clause , verbal group</definiens>
			</definition>
			<definition id="1">
				<sentence>The units of a grammar can be ordered in terms of a rank scale , from the largest to the smallest unit ( structural classification ) ( Halliday , 1985 ) : a sentence consists of one or more clauses ; a clause consists of one or more phrases ( groups ) ; a phrase consists of one or more words ; a word consists of a root word , and zero or more morphemes ; a morpheme is the smallest unit .</sentence>
				<definiendum id="0">morpheme</definiendum>
				<definiens id="0">ordered in terms of a rank scale , from the largest to the smallest unit ( structural classification ) ( Halliday , 1985 ) : a sentence consists of one or more clauses ; a clause consists of one or more phrases ( groups ) ; a phrase consists of one or more words ; a word consists of a root word , and zero or more morphemes ; a</definiens>
			</definition>
			<definition id="2">
				<sentence>A simple sentence consists of only one main protess and several components that complement or modify the main process .</sentence>
				<definiendum id="0">simple sentence</definiendum>
				<definiens id="0">consists of only one main protess and several components that complement or modify the main process</definiens>
			</definition>
			<definition id="3">
				<sentence>A complex sentence consists of more than one simple sentence that may be structurally or semantically connected to each other .</sentence>
				<definiendum id="0">complex sentence</definiendum>
			</definition>
			<definition id="4">
				<sentence>Sentences Turkish sentences can be divided into two groups depending on the type of their predicates : verbal and nominal sentences .</sentence>
				<definiendum id="0">Sentences Turkish sentences</definiendum>
				<definiens id="0">depending on the type of their predicates : verbal and nominal sentences</definiens>
			</definition>
			<definition id="5">
				<sentence>Circums~antials are the optional constituents to describe the process from different perspective such as time , place , manner etc .</sentence>
				<definiendum id="0">Circums~antials</definiendum>
				<definiens id="0">the optional constituents to describe the process from different perspective such as time , place , manner etc</definiens>
			</definition>
			<definition id="6">
				<sentence>ribed , and attribute is a determiner that is ascribed to carrier .</sentence>
				<definiendum id="0">attribute</definiendum>
				<definiens id="0">a determiner that is ascribed to carrier</definiens>
			</definition>
			<definition id="7">
				<sentence>noun ) , m : i. ( question ) , ol ( be ) ) , the relevant suffixes and the components of the verbal group are determined and organized by the systemicfunctional grammar designed for Turkish to express appropriate meanings .</sentence>
				<definiendum id="0">ol</definiendum>
				<definiens id="0">the relevant suffixes and the components of the verbal group are determined and organized by the systemicfunctional grammar designed for Turkish to express appropriate meanings</definiens>
			</definition>
			<definition id="8">
				<sentence>A non-finite verbal group realizes the process of a clause that may be used as a noun ( infinitive ) , adjective ( participle ) or adverb ( adverbial ) .</sentence>
				<definiendum id="0">non-finite verbal group</definiendum>
				<definiens id="0">realizes the process of a clause</definiens>
			</definition>
			<definition id="9">
				<sentence>Adverb group ( AdvG ) is used in the realization of several circumstantial functions given in Section 2.2.3 .</sentence>
				<definiendum id="0">Adverb group</definiendum>
				<definiendum id="1">AdvG</definiendum>
			</definition>
			<definition id="10">
				<sentence>The FUF text generation system consists of two main modules : a unifier and a linearizer ( Elhadad , 1990-2 ) .</sentence>
				<definiendum id="0">FUF text generation system</definiendum>
			</definition>
			<definition id="11">
				<sentence>A FD is a list of pairs .</sentence>
				<definiendum id="0">FD</definiendum>
			</definition>
			<definition id="12">
				<sentence>Lexicalized Semantic Input : ( ( cat simple-clause ) ( time aorist ) ( mode past ) ( mood declarative ) ( desc-verb potential ) ( desc-polarity negative ) ( voice active ) ( process ( ( type material ) ( type-of-base verb ) ( agentive yes ) ( effective yes ) ( lex `` yaz '' ) ) ) ( participants ( ( actor ( ( cat proper ) ( lex `` Veli '' ) ) ) ( agent ( ( cat proper ) ( lex `` All '' ) ) ) ( medium ( ( cat common ) ( definite yes ) SExtra Turkish letters are represented as follows : C is~ , Iisl , Gist , O is 5 , S is ~ , Uis~ .</sentence>
				<definiendum id="0">O</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">type-of-base verb ) ( agentive yes ) ( effective yes ) ( lex `` yaz '' ) ) ) ( participants ( ( actor ( ( cat proper ) ( lex `` Veli '' ) ) ) ( agent ( ( cat proper ) ( lex `` All ''</definiens>
				<definiens id="1">5 ,</definiens>
			</definition>
			<definition id="13">
				<sentence>The semantic description consists of three metafunctions : ideational such as agent , actor , goal , process , location for representing the constituents of the sentence and their roles ; interpersonal such as mood , modality for establishing the relationship between the speaker and the listener ; and teztual such as topic , focus , background for presenting information as text in context .</sentence>
				<definiendum id="0">semantic description</definiendum>
				<definiens id="0">consists of three metafunctions : ideational such as agent , actor , goal , process , location for representing the constituents of the sentence and their roles ; interpersonal such as mood , modality for establishing the relationship between the speaker and the listener ; and teztual such as topic , focus , background for presenting information as text in context</definiens>
			</definition>
			<definition id="14">
				<sentence>GENESYS : An integrated environment for developing systemic functional grammars .</sentence>
				<definiendum id="0">GENESYS</definiendum>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>Akkadian is a semitic dead language which was used in the ancient Mesopotamia .</sentence>
				<definiendum id="0">Akkadian</definiendum>
			</definition>
			<definition id="1">
				<sentence>Akkadian is a dead Semitic language that was spoken and written in Mesopotamia between 2800 and 0 B.C .</sentence>
				<definiendum id="0">Akkadian</definiendum>
				<definiens id="0">a dead Semitic language that was spoken and written in Mesopotamia between 2800 and 0 B.C</definiens>
			</definition>
			<definition id="2">
				<sentence>Akkadian is a dead language of the Semitic family .</sentence>
				<definiendum id="0">Akkadian</definiendum>
			</definition>
			<definition id="3">
				<sentence>Akkadian is the only Semitic language where all vowels are written .</sentence>
				<definiendum id="0">Akkadian</definiendum>
				<definiens id="0">the only Semitic language where all vowels are written</definiens>
			</definition>
			<definition id="4">
				<sentence>• infix T ( index 2 ) : stem groups I , II , and III have a stem with a t infixed .</sentence>
				<definiendum id="0">III</definiendum>
				<definiens id="0">stem groups I , II , and</definiens>
			</definition>
			<definition id="5">
				<sentence>N is a semi-weak consonant : it assimilates easily , but it does not disappear .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a semi-weak consonant : it assimilates easily , but it does not disappear</definiens>
			</definition>
</paper>

		<paper id="0209">
			<definition id="0">
				<sentence>Reading ( V ) Comparison ( S1 , S2 ) Comparison Fractional ( V , S ) Evolution ( V1 , V2 ) Correlation ' &lt; V1 , V2 ) Distribution ( V , S ) Distribution Fractional ( S ) Increase Decrease Stability Recapitulative Figure 1 : Two level decomposition of simple intentions : V is a variable and S is a set of variables from the evolution of a comparison .</sentence>
				<definiendum id="0">Reading</definiendum>
				<definiendum id="1">V</definiendum>
				<definiendum id="2">simple intentions</definiendum>
				<definiendum id="3">V</definiendum>
				<definiendum id="4">S</definiendum>
				<definiens id="0">a variable and</definiens>
				<definiens id="1">a set of variables from the evolution of a comparison</definiens>
			</definition>
			<definition id="1">
				<sentence>Graphics make important elements of the data standout and catch the eye of the reader .</sentence>
				<definiendum id="0">Graphics</definiendum>
				<definiens id="0">make important elements of the data standout and catch the eye of the reader</definiens>
			</definition>
</paper>

		<paper id="0501">
			<definition id="0">
				<sentence>The crucial descriptive problem for a distributional grammar ( i.e. phrase-structure grammar ) is the existence of non-contiguous elements .</sentence>
				<definiendum id="0">distributional grammar</definiendum>
				<definiendum id="1">phrase-structure grammar )</definiendum>
				<definiens id="0">the existence of non-contiguous elements</definiens>
			</definition>
			<definition id="1">
				<sentence>Marcus also formulates a stronger hypothesis , the Projectivity hypothesis , which connects the linear order of the elements of a sentence to the structural order of the sentence .</sentence>
				<definiendum id="0">Projectivity hypothesis</definiendum>
				<definiens id="0">connects the linear order of the elements of a sentence to the structural order of the sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>• The uppermost regent is the central node of the sentence .</sentence>
				<definiendum id="0">uppermost regent</definiendum>
				<definiens id="0">the central node of the sentence</definiens>
			</definition>
</paper>

		<paper id="1303">
			<definition id="0">
				<sentence>The parser consists of the following components : ( i ) a tokeniser ; ( ii ) a morphological analyser ; ( iii ) a simple lookup program for introducing all possible syntactic analyses as alternatives for each word and word-boundary ; and ( iv ) a finite-state syntactic parser ( actually a syntactic disambiguator ) that discards those sentence readings that violate the parser 's grammar .</sentence>
				<definiendum id="0">parser</definiendum>
			</definition>
			<definition id="1">
				<sentence>@ @ '' indicates sentence boundaries ; the centre-embedded finite clause `` who is fond of singing this aria '' is flanked by the clause boundary tags @ &lt; and ~ &gt; and its function is postmodifying , as indicated with the second tag N &lt; Q of `` be '' , the main verb ( @ MV ) of the clause. The pronoun `` who '' is the subject ( ~S .UBJ ) of this clause , and the adjective `` fond '' is the subject complement ( @ sc ) that is followed by the postmodifying ( @ N &lt; ) prepositional phrase starting with `` oi ~ ' , whose complement is the no , ~S-lte main verb ( ~nv ) `` sing '' that has the noun `` aria '' as its object ( ~obj ) ( note that the lower case is reserved for functions in nonfinite clauses ) . The matrix clause `` The man killed his father '' is a finite main clause ( MAINC~ ) whose main verb ( @ MV ) is `` kill '' . The subject ( QSUBJ ) of the finite clause is the noun `` man '' , while the noun `` father '' is the object in the finite clause ( @ OBJ ) . The word `` father '' has one premodifier ( ~N ) , namely the genitive pronoun `` he '' . This representation is designed to follow the principle of surface-syntacticity : distinctions not motivated by surface grammatical phenomena , e.g. many attachment and coordination problems , are avoided by making the syntactic representation sufficiently underspecific in the description of grammatically ( if not semantically ) unresolvable distinctions. The tokeniser identifies words and punctuation marks. The morphological analyser contains a rule-based lexicon and a guesser that assign one or more morphological analyses to each word , cf. the analysis of the word-form `` tries '' `` ~ : l ; z '' i es ) '' `` try '' &lt; SV0 &gt; V PRES SG3 VFIN `` try '' N N0M PL The next step is the introduction of alternative syntactic and word-boundary descriptors with a simple lookup program .</sentence>
				<definiendum id="0">~obj )</definiendum>
				<definiendum id="1">matrix clause</definiendum>
				<definiens id="0">a finite main clause ( MAINC~ ) whose main verb</definiens>
				<definiens id="1">designed to follow the principle of surface-syntacticity : distinctions not motivated by surface grammatical phenomena , e.g. many attachment and coordination problems</definiens>
				<definiens id="2">contains a rule-based lexicon and a guesser that assign one or more morphological analyses to each word</definiens>
				<definiens id="3">the introduction of alternative syntactic and word-boundary descriptors with a simple lookup program</definiens>
			</definition>
			<definition id="2">
				<sentence>PrepComp , states a number of alternative contexts in which the expression ( given left of the arrow ) occurs .</sentence>
				<definiendum id="0">PrepComp</definiendum>
			</definition>
			<definition id="3">
				<sentence>VFIN ) ; forbids the occurrence of two ilnite verbs in the same finite clause .</sentence>
				<definiendum id="0">VFIN ) ;</definiendum>
				<definiens id="0">forbids the occurrence of two ilnite verbs in the same finite clause</definiens>
			</definition>
			<definition id="4">
				<sentence>The syntactic grammar contains some 2,600 finite-state rules each of which have been tested and corrected against a manually parsed corpus of about 250,000 words ( over 10,000 unambiguously parsed sentences ) .</sentence>
				<definiendum id="0">syntactic grammar</definiendum>
				<definiens id="0">contains some 2,600 finite-state rules each of which have been tested and corrected against a manually parsed corpus of about 250,000 words ( over 10,000 unambiguously parsed sentences</definiens>
			</definition>
			<definition id="5">
				<sentence>The mature disamhiguator is an early version of a system presently known as EngCG-2 ( Samuelsson and Voutflainen \ [ 8\ ] ) .</sentence>
				<definiendum id="0">mature disamhiguator</definiendum>
				<definiens id="0">an early version of a system presently known as EngCG-2 ( Samuelsson and Voutflainen \ [ 8\ ] )</definiens>
			</definition>
			<definition id="6">
				<sentence>Recognition rate indicates the percentage of sentences that get at least one analysis , correct or incorrect , from the parser .</sentence>
				<definiendum id="0">Recognition rate</definiendum>
				<definiens id="0">indicates the percentage of sentences that get at least one analysis</definiens>
			</definition>
</paper>

		<paper id="1204">
			<definition id="0">
				<sentence>The major tasks of information extraction systems ( IE-Systems ) are the unsupervised selection , fast analysis and efficient storage of relevant text patterns a person or a group of persons is interested in .</sentence>
				<definiendum id="0">information extraction systems</definiendum>
				<definiens id="0">the unsupervised selection , fast analysis and efficient storage of relevant text patterns a person or a group of persons is interested in</definiens>
			</definition>
			<definition id="1">
				<sentence>Deletion : Characters are dropped ( e.g. due to low print quality ) .</sentence>
				<definiendum id="0">Deletion</definiendum>
				<definiens id="0">Characters are dropped ( e.g. due to low print quality )</definiens>
			</definition>
			<definition id="2">
				<sentence>Compared to other collocation measures , this value does not only take account of the words frequencies and the collocations frequencies ( as e.g. Mutual Information ( Church and P. , 1990 ) ) or their transition likelihood ( as e.g. Markov chains ( Thomason , 1986 ) ) but combines these two properties with a third one : the word 's different modalities as indicated by their number of variants , i.e. their weighted ranks .</sentence>
				<definiendum id="0">collocations frequencies</definiendum>
				<definiendum id="1">Mutual Information</definiendum>
				<definiens id="0">the word 's different modalities as indicated by their number of variants</definiens>
			</definition>
</paper>

		<paper id="0608">
			<definition id="0">
				<sentence>WYSIWYM is a method of editing knowledge bases in which the user interacts with a feedback text generated by a natural language generation system ( e.g. Power and Scott 1998 ) .</sentence>
				<definiendum id="0">WYSIWYM</definiendum>
			</definition>
			<definition id="1">
				<sentence>The acronym WYSIWYM stands for 'What You See Is What You Meant ' : a natural language text ( 'what you see ' ) presents a knowledge base that the author has built by purely semantic decisions ( 'what you meant ' ) .</sentence>
				<definiendum id="0">WYSIWYM</definiendum>
				<definiens id="0">a natural language text ( 'what you see ' ) presents a knowledge base that the author has built by purely semantic decisions ( 'what you meant ' )</definiens>
			</definition>
			<definition id="2">
				<sentence>Cut-all c affects all occurrences of ai , respecting all coreference links between their anchors .</sentence>
				<definiendum id="0">Cut-all c</definiendum>
			</definition>
			<definition id="3">
				<sentence>Save ( the document ) l by entering ( the name of ( the document ) l ) 2 and entering ( the name of this object ) 2 ( further actions ) .</sentence>
				<definiendum id="0">Save</definiendum>
				<definiendum id="1">entering</definiendum>
				<definiens id="0">the document ) l by entering ( the name of</definiens>
			</definition>
			<definition id="4">
				<sentence>DRAFTER-III offers the user the possibility of having coreference indicated either by indices , or by ( colour-based ) highlighting , or both .</sentence>
				<definiendum id="0">DRAFTER-III</definiendum>
				<definiens id="0">offers the user the possibility of having coreference indicated either by indices , or by ( colour-based ) highlighting</definiens>
			</definition>
</paper>

		<paper id="1311">
</paper>

		<paper id="1418">
			<definition id="0">
				<sentence>The validation process includes two aspects : ( 1 ) we test that human coders agree on the semantic relations they use to label complex NPs ; and ( 2 ) we verify that the generator 's decision to produce a smixut construction corresponds to that observed in the corpus .</sentence>
				<definiendum id="0">validation process</definiendum>
				<definiens id="0">includes two aspects : ( 1 ) we test that human coders agree on the semantic relations they use to label complex NPs</definiens>
			</definition>
			<definition id="1">
				<sentence>But depending on the level of cohesiveness of the frozen compound , definite marking may differ : beyt-sefer -house-book ( a school ) may give ha-beyt-sefer ( the school ) instead of the predicted beyt ha-sefer for a productive smixut .</sentence>
				<definiendum id="0">-house-book</definiendum>
				<definiens id="0">a school ) may give ha-beyt-sefer ( the school</definiens>
			</definition>
</paper>

		<paper id="0716">
			<definition id="0">
				<sentence>In Roget 's , for example , the distance ( counting edges ) between Intellect and Grammar is the same as the distance between Grammar and Phrase Structure .</sentence>
				<definiendum id="0">Grammar</definiendum>
				<definiens id="0">the same as the distance between Grammar and Phrase Structure</definiens>
			</definition>
			<definition id="1">
				<sentence>Resnik defines the similarity of two concepts as the maximum of the Information Content of the concepts that subsume them in the taxonomy .</sentence>
				<definiendum id="0">Resnik</definiendum>
				<definiens id="0">defines the similarity of two concepts as the maximum of the Information Content of the concepts that subsume them in the taxonomy</definiens>
			</definition>
			<definition id="2">
				<sentence>i ~ ( c ) , where ~ ( c ) is the probability .</sentence>
				<definiendum id="0">~ ( c )</definiendum>
			</definition>
</paper>

		<paper id="1014">
</paper>

		<paper id="1106">
			<definition id="0">
				<sentence>ATTACK ( passive-verb `` attacked '' ) Victim = subject Target = subject Perpetrator = pp ( by ) Instrument= pp ( by ) ACCUSATION ( active-verb `` blamed '' ) Accuser = subject Perpetrator = direct object Perpetrator = pp ( on ) SABOTAGE ( noun `` sabotage '' ) Perpetrator = pp ( by ) Instrument = pp ( with ) Location = pp ( on ) Victim = pp ( against ) , pp ( of ) , pp ( on ) Target = pp ( against ) , pp ( of ) , pp ( on ) The ATTACK case frame shows a very common situation where multiple conceptual roles map to the same syntactic role .</sentence>
				<definiendum id="0">ATTACK</definiendum>
			</definition>
			<definition id="1">
				<sentence>Another early system , PALKA ( Kim and Moldovan , 1993 ) , requires domain-specific frames with keyword lists , CRYSTAL ( Soderland et al. , 1995 ) requires an annotated training corpus , RAPIER ( Califf and Mooney , 1997 ) requires filled templates , and LIEP ( Huffman , 1996 ) requires keywords and annotated training examples .</sentence>
				<definiendum id="0">CRYSTAL</definiendum>
				<definiendum id="1">LIEP</definiendum>
				<definiens id="0">requires domain-specific frames with keyword lists</definiens>
				<definiens id="1">Soderland et al. , 1995 ) requires an annotated training corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>AutoSlog-TS ( Riloff , 1996b ) is a derivative of AutoSlog that was designed to obviate the need for special training data .</sentence>
				<definiendum id="0">AutoSlog-TS</definiendum>
				<definiens id="0">a derivative of AutoSlog that was designed to obviate the need for special training data</definiens>
			</definition>
			<definition id="3">
				<sentence>AutoSlog-TS generates the same simple extraction patterns that AutoSlog generates .</sentence>
				<definiendum id="0">AutoSlog-TS</definiendum>
			</definition>
			<definition id="4">
				<sentence>`` AutoSlog-TS generates extraction patterns by making two passes over the corpus .</sentence>
				<definiendum id="0">AutoSlog-TS</definiendum>
				<definiens id="0">generates extraction patterns by making two passes over the corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>In the next section , we describe a corpus-based algorithm that constructs conceptual case frames empirically by learning semantic preferences for each extraction pattern and using these preferences to assign conceptual roles automatically .</sentence>
				<definiendum id="0">corpus-based algorithm</definiendum>
				<definiens id="0">constructs conceptual case frames empirically by learning semantic preferences for each extraction pattern and using these preferences to assign conceptual roles automatically</definiens>
			</definition>
			<definition id="6">
				<sentence>Extraction patterns with compatible syntactic constraints are then 1Semantic constraints could be associated with the conceptual roles assigned by the human reviewer , but our goal is to assign both the conceptual roles and selectional restrictions automatically .</sentence>
				<definiendum id="0">Extraction patterns</definiendum>
				<definiens id="0">to assign both the conceptual roles and selectional restrictions automatically</definiens>
			</definition>
			<definition id="7">
				<sentence>The semantic lexicon is a dictionary of words that belong to relevant semantic categories .</sentence>
				<definiendum id="0">semantic lexicon</definiendum>
				<definiens id="0">a dictionary of words that belong to relevant semantic categories</definiens>
			</definition>
			<definition id="8">
				<sentence>PFreq is the number of times that the extraction pattern fired , SFreq is the number of times that the pattern extracted the given semantic category , and Prob is the estimated probability of the pattern extracting the given semantic category ( SFreq/PFreq ) .</sentence>
				<definiendum id="0">PFreq</definiendum>
				<definiendum id="1">SFreq</definiendum>
				<definiendum id="2">Prob</definiendum>
				<definiens id="0">the number of times that the extraction pattern fired ,</definiens>
				<definiens id="1">the number of times that the pattern extracted the given semantic category , and</definiens>
			</definition>
</paper>

		<paper id="1409">
			<definition id="0">
				<sentence>The SA aids a software engineer in • Choosing security measures to protect valuable system assets ( e.g. important data ) against likely threats ( e.g. disclosure or corruption ) .</sentence>
				<definiendum id="0">SA</definiendum>
				<definiens id="0">aids a software engineer in • Choosing security measures to protect valuable system assets ( e.g. important data ) against likely threats</definiens>
			</definition>
			<definition id="1">
				<sentence>CDK is knowledge needed for content selection , but excludes all choices that depend on knowledge of the intended act of communication .</sentence>
				<definiendum id="0">CDK</definiendum>
				<definiens id="0">knowledge needed for content selection , but excludes all choices that depend on knowledge of the intended act of communication</definiens>
			</definition>
			<definition id="2">
				<sentence>Some : examples : • Defense objects have id ( name ) and cost attributes ; • Damage objects have id , severity and type attributes ; • prevent is a relation that holds between a Defense instance and a Damage instance ; • Site , Asset and System component are different sub-classes of ProtectedObject ; • A System consists of one or more system components .</sentence>
				<definiendum id="0">prevent</definiendum>
				<definiens id="0">• Defense objects have id ( name ) and cost attributes ; • Damage objects have id</definiens>
				<definiens id="1">a relation that holds between a Defense instance and a Damage instance ; • Site , Asset and System component are different sub-classes of ProtectedObject ; • A System consists of one or more system components</definiens>
			</definition>
			<definition id="3">
				<sentence>DesignExpert : A knowledge-based tool for developing system-wide properties .</sentence>
				<definiendum id="0">DesignExpert</definiendum>
			</definition>
</paper>

		<paper id="0307">
			<definition id="0">
				<sentence>Lexical seeds are words which are semantically related to the activities of reporting , problem-solving , argumenting or evaluating , or expressions of deixis ( `` we ... `` ) or other textual cues ( e.g. literature references in text were marked up using the symbol \ [ REF\ ] , which is a signal for mentions of other researchers ' solutions , tasks or problems ) .</sentence>
				<definiendum id="0">Lexical seeds</definiendum>
			</definition>
			<definition id="1">
				<sentence>We lowercased all words and counted punctuation ( including brackets ) as a full word .</sentence>
				<definiendum id="0">punctuation</definiendum>
				<definiens id="0">a full word</definiens>
			</definition>
			<definition id="2">
				<sentence>The summarisation process consists of two consecutive steps , sentence extraction and rhetorical classification , and uses other heuristics like location and term frequency .</sentence>
				<definiendum id="0">summarisation process</definiendum>
				<definiens id="0">consists of two consecutive steps , sentence extraction and rhetorical classification , and uses other heuristics like location and term frequency</definiens>
			</definition>
			<definition id="3">
				<sentence>The summarisation system requires a list of metacomments of arbitrary length , containing a quality score for each phrase which estimates how predictive these phrases are in pointing to extract-worthy sentences , and the most likely rhetorical label that sentences with this meta-comment will receive .</sentence>
				<definiendum id="0">summarisation system</definiendum>
				<definiens id="0">requires a list of metacomments of arbitrary length , containing a quality score for each phrase which estimates how predictive these phrases are in pointing to extract-worthy sentences , and the most likely rhetorical label that sentences with this meta-comment will receive</definiens>
			</definition>
</paper>

		<paper id="1114">
			<definition id="0">
				<sentence>The 500-sentence test corpus consists only of in-coverage sentences , and contains a mix of written genres : news reportage ( general and sports ) , belles lettres , biography , memoirs , and scientific writing .</sentence>
				<definiendum id="0">500-sentence test corpus</definiendum>
				<definiens id="0">consists only of in-coverage sentences , and contains a mix of written genres : news reportage ( general and sports ) , belles lettres , biography , memoirs , and scientific writing</definiens>
			</definition>
			<definition id="1">
				<sentence>( 1 ) They made ( NP_WHPP ) a great fuss about what to do .</sentence>
				<definiendum id="0">NP_WHPP</definiendum>
				<definiens id="0">a great fuss about what to do</definiens>
			</definition>
			<definition id="2">
				<sentence>The measures compare the set of GRs in the annotated test corpus with those returned by the parser , in terms of recall , the percentage of GRs correctly found by the parser out of all those in the treebank ; and precision ; 123 Recall Precision ( % ) ( % ) 'Baseline ' 88.6 79.2 With subcat 88.1 88.2 Table 3 : GR evaluation measures , before and after incorporation of subcategorisation information .</sentence>
				<definiendum id="0">Recall Precision</definiendum>
				<definiens id="0">the set of GRs in the annotated test corpus with those returned by the parser , in terms of recall , the percentage of GRs correctly found by the parser out of all those in the treebank</definiens>
			</definition>
			<definition id="3">
				<sentence>The baseline parser returns a mean of 4.65 relations per sentence , whereas the lexicalised parser returns only 4.15 , the same as the test corpus .</sentence>
				<definiendum id="0">baseline parser</definiendum>
				<definiens id="0">returns a mean of 4.65 relations per sentence</definiens>
			</definition>
</paper>

		<paper id="0712">
			<definition id="0">
				<sentence>To overcome the problem of varying link distances , Agirre and Rigau ( 1996 ) propose a semantic similarity measure ( referred to as `` conceptual density '' ) which is sensitive to i ) the length of the path , ii ) the depth of the nodes in the hierarchy ( deeper nodes are ranked closer ) and iii ) the density of nodes in the subhierarchies ( concepts involved in a denser subhierarchy are ranked closer than those in a more sparse region ) .</sentence>
				<definiendum id="0">iii</definiendum>
				<definiens id="0">sensitive to i ) the length of the path , ii ) the depth of the nodes in the hierarchy ( deeper nodes are ranked closer</definiens>
			</definition>
			<definition id="1">
				<sentence>In a similar vein , Resnik ( 1995 ) defines a taxonomic similarity measure which dispenses with the path length approach and is based on the notion of information contentUnder his view , semantic similarity between two words is represented by the log P ( C ) value of the most informative concept C subsuming both words in a semantic taxonomy , where P ( C ) is a maximum likelyhood estimate of C 's probability of occurrence in a reference corpus .</sentence>
				<definiendum id="0">P ( C )</definiendum>
				<definiens id="0">a maximum likelyhood estimate of C 's probability of occurrence in a reference corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>Among the typical collocates of sense 1 of accendere 'light ' there is sigaretta 'cigarette ' which , in WordNet 1.5 , is the terminal node of the following taxonomical path : sigarctla 'cigarette ' = &gt; roll of tobacco = &gt; tobacco , baccy = &gt; narcotic = &gt; drug = &gt; artifact , artefact = &gt; object , inanimate object , physical object = &gt; entity Let us look now at some of the typical verbs with which sigaretta occurs , together with other possible collocates of these verbs , as they are attested in the Collins Italian-English Dictionary ( 1985 ) , in both example sentences and the semantic indicators field .</sentence>
				<definiendum id="0">Collins Italian-English Dictionary</definiendum>
				<definiens id="0">the terminal node of the following taxonomical path : sigarctla 'cigarette ' = &gt; roll of tobacco = &gt; tobacco , baccy = &gt; narcotic = &gt; drug = &gt; artifact , artefact = &gt; object , inanimate object , physical object = &gt; entity Let us look now at some of the typical verbs with which sigaretta occurs , together with other possible collocates of these verbs , as they are attested in the</definiens>
			</definition>
</paper>

		<paper id="1104">
			<definition id="0">
				<sentence>Residual Inverse Document Frequency ( RIDF ) is similar , but it looks for ngrams whose document frequency ( df ) is larger than chance .</sentence>
				<definiendum id="0">RIDF</definiendum>
				<definiens id="0">similar , but it looks for ngrams whose document frequency ( df ) is larger than chance</definiens>
			</definition>
			<definition id="1">
				<sentence>Mutual Information ( MI ) , l ( x ; y ) , compares the probability of observing word x and word y together ( the joint probability ) with the probabilities of observing x and y independently ( chance ) .</sentence>
				<definiendum id="0">Mutual Information</definiendum>
				<definiens id="0">compares the probability of observing word x</definiens>
			</definition>
			<definition id="2">
				<sentence>A suffix array is a data structure designed to make it convenient to compute term frequencies for all substrings in a corpus .</sentence>
				<definiendum id="0">suffix array</definiendum>
				<definiens id="0">a data structure designed to make it convenient to compute term frequencies for all substrings in a corpus</definiens>
			</definition>
			<definition id="3">
				<sentence>Lcp is a vector of length N , where lcp\ [ i\ ] indicates the length of the common prefix between the ith suffix and the/+/st suffix in the suffix array .</sentence>
				<definiendum id="0">Lcp</definiendum>
				<definiens id="0">a vector of length N , where lcp\ [ i\ ] indicates the length of the common prefix between the ith suffix and the/+/st suffix in the suffix array</definiens>
			</definition>
			<definition id="4">
				<sentence>Gray area denotes endpoints of substrings in class ( &lt; 2,4 &gt; ) .</sentence>
				<definiendum id="0">Gray area</definiendum>
			</definition>
			<definition id="5">
				<sentence>The stack elements are pairs ( x , y ) , where x is an index , typically the left edge of a candidate LCPdelimited interval , and y is the SIL of this candidate interval .</sentence>
				<definiendum id="0">stack elements</definiendum>
				<definiendum id="1">x</definiendum>
				<definiendum id="2">y</definiendum>
				<definiens id="0">an index , typically the left edge of a candidate LCPdelimited interval , and</definiens>
			</definition>
			<definition id="6">
				<sentence>When the algorithm is processing s\ [ t\ ] , the algorithm searches the stack to find the suffix , s\ [ k\ ] , with the largest k such k_ &lt; i and s\ [ i\ ] and s\ [ k\ ] are in the same document. This search can be performed in O ( logN ) time. Figure 7 shows the LCP-delimited intervals in a suffix array and four suffixes included in the same document. I1 has four immediate constituents of intervals. S\ [ j\ ] is included in the same document of s\ [ i\ ] . Count for the document of s\ [ j\ ] will be duplicated at computing df of 11. At the point of processing sO'\ ] , the algorithm will increment duplication-counter of I ! to cancel dfcount of sO'\ ] . As the same way , df count of s\ [ k\ ] has to canceled at computing df of 11. Figure 8 shows a snapshot of the stack after processing s\ [ 4\ ] in Figure 2. Each stack element is a 4-tuple of the index of suffix array , lcp , dfcounter and duplication-counter , ( i , lcp , df dc ) . Figure 2 shows s\ [ 1\ ] and s\ [ 4\ ] are in the same document. Looking up the document link table , the algorithm knows s\ [ 1\ ] is the nearest suffix which is in the same document of s\ [ 4\ ] . The duplication-counter of the element of s\ [ 1\ ] is incremented. The duplication of counting s\ [ 1\ ] and s\ [ 4\ ] for the class generated by s\ [ 1\ ] will be avoided using this duplication-counter. At some processing point , the algorithm uses only a part of the document link table. It duplication lcp counter ( 2 , 3 , 3,0 ) \ [ ( 1 , 2 , 1,1 ) I ( -1 , -1 , - , - ) Figure 8 : A snapshot of the stack in dfcomputing Nearest Doc-id index ..° 382 4 oH 6892 3 ° , ° 84987 2 Figure 9 : Nearest indexes of documents needs only the nearest index on the link , but not the whole of the link. So we can compress the link table to dynamic one in which an entry of each document holds the nearest index. Figure 9 shows the nearest index+ table of document after processing s\ [ 4\ ] . The final algorithm to calculate all classes with tfand dftakes O ( NlogN ) time and O ( N ) space in the worst case. We computed all RIDF 's for all substrings of two corpora , Wall Street Journal of ACL/DCI in English ( about 50M words and 113k articles ) and Mainichi News Paper 1991-1995 ( CD-Mainichi Shimbun 91-95 ) in Japanese ( about 216M characters and 436k articles ) , using the algorithm in the previous section. In English , we tokenized the text into words , delimited by white .space , whereas in Japanese we tokenized the text into characters ( usually 2-bytes ) because Japanese text has no word delimiter such as white space. It took a few hours to compute all RIDF 's using the suffix array. It takes much longer to compute the suffix array than to compute tfand df. We ignored substrings with tf &lt; 10 to avoid noise , resulting in about 1.6M English phrases ( # classes = 1.4M ) and about 15M substrings of Japanese words/phrases ( # classes = 10M ) . MI of the longest substring of each class was also computed by the following formula. , p ( xyz ) MI ( xyz ) = xog p ( xy ) p ( z I y ) Where xyz is a phrase or string , x and Z are a word or a character and y is a sub-phrase or substring. We are interested in comparing and contrasting RIDF and MI. Figure 10 ( a ) plots RIDF vs MI for phrases in WSJ ( length &gt; 1 ) , showing little , if any , correlation between RIDF and MI .</sentence>
				<definiendum id="0">O</definiendum>
				<definiendum id="1">xyz</definiendum>
				<definiendum id="2">y</definiendum>
				<definiens id="0">processing s\ [ t\ ] , the algorithm searches the stack to find the suffix</definiens>
				<definiens id="1">) space in the worst case. We computed all RIDF 's for all substrings of two corpora , Wall Street Journal of ACL/DCI in English</definiens>
				<definiens id="2">white space. It took a few hours to compute all RIDF 's using the suffix array. It takes much longer to compute the suffix array than to compute tfand df. We ignored substrings with tf &lt; 10 to avoid noise</definiens>
				<definiens id="3">a phrase or string , x and Z are a word or a character and</definiens>
			</definition>
</paper>

		<paper id="1202">
			<definition id="0">
				<sentence>Greatest prediction uncertainty ( measured as the entropy of the output units ) occurred , not at the sentence boundaries but when the first verb was the input .</sentence>
				<definiendum id="0">Greatest prediction uncertainty</definiendum>
				<definiens id="0">the entropy of the output units</definiens>
			</definition>
			<definition id="1">
				<sentence>Simple recurrent networks ( SRN 's ) of the Elman type are similar to three-layer perceptrons but with recurrent connections from the hidden layer to a context layer ( also called state layer ) which becomes part of the input .</sentence>
				<definiendum id="0">Simple recurrent networks</definiendum>
				<definiens id="0">becomes part of the input</definiens>
			</definition>
</paper>

		<paper id="1429">
			<definition id="0">
				<sentence>AMALIA has a uniform core engine for bottom-up chart processing , which interprets the given ( abstract machine ) program , and realizes the generation or parsing task .</sentence>
				<definiendum id="0">AMALIA</definiendum>
				<definiens id="0">has a uniform core engine for bottom-up chart processing , which interprets the given ( abstract machine ) program , and realizes the generation or parsing task</definiens>
			</definition>
			<definition id="1">
				<sentence>AMALIA supports the same type hierarchies as ALE does , with exactly the same specification syntax .</sentence>
				<definiendum id="0">AMALIA</definiendum>
				<definiens id="0">supports the same type hierarchies as ALE does , with exactly the same specification syntax</definiens>
			</definition>
			<definition id="2">
				<sentence>AMALIA uses a subset of ALE 's syntax for describing totally well-typed , possibly cyclic , nondisjunctive feature structures .</sentence>
				<definiendum id="0">AMALIA</definiendum>
				<definiens id="0">uses a subset of ALE 's syntax for describing totally well-typed , possibly cyclic , nondisjunctive feature structures</definiens>
			</definition>
			<definition id="3">
				<sentence>AMALIA : an interactive , user-friendly program with a graphical user interface , and a non'interactive but more efficient version for batch processing .</sentence>
				<definiendum id="0">AMALIA</definiendum>
				<definiens id="0">an interactive , user-friendly program with a graphical user interface</definiens>
			</definition>
</paper>

		<paper id="1239">
			<definition id="0">
				<sentence>The distribution of an element is the set of the environments in which the element occurs .</sentence>
				<definiendum id="0">distribution of an element</definiendum>
				<definiens id="0">the set of the environments in which the element occurs</definiens>
			</definition>
			<definition id="1">
				<sentence>Frequent bigrams generally correspond to relations between two chunks ( like S-ed S-ly ) .</sentence>
				<definiendum id="0">Frequent bigrams</definiendum>
				<definiens id="0">generally correspond to relations between two chunks</definiens>
			</definition>
</paper>

		<paper id="0503">
			<definition id="0">
				<sentence>N is the set of nonterminais .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the set of nonterminais</definiens>
			</definition>
			<definition id="1">
				<sentence>t. of rool-symbols ( starling symbols ) , and P is the set .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the set</definiens>
			</definition>
			<definition id="2">
				<sentence>Put informally ' , a DR-tree ( created by a FODG G ) is a finite tree with a root and with the following two types of edges : a ) vertical : these edges correspond to the rewriting of the dominant symbol by '' the symbol which is on the left-hand side of the rule ( of G ) used .</sentence>
				<definiendum id="0">DR-tree</definiendum>
				<definiens id="0">a finite tree with a root and with the following two types of edges : a ) vertical : these edges correspond to the rewriting of the dominant symbol by '' the symbol which is on the left-hand side of the rule ( of G ) used</definiens>
			</definition>
			<definition id="3">
				<sentence>In the sequel the symbol : Vat means the set of natural numbers ( without zero ) .</sentence>
				<definiendum id="0">Vat</definiendum>
				<definiens id="0">means the set of natural numbers ( without zero )</definiens>
			</definition>
			<definition id="4">
				<sentence>TN ( G ) denotes the set of compb'le DR-tlvcs rooted in a symbol from , % .</sentence>
				<definiendum id="0">TN</definiendum>
			</definition>
			<definition id="5">
				<sentence>The dependency tree dT ( Tr ) contracted from Tr is defined as follows : The set of nodes of dT ( Tr ) is the set of 3-tuples \ [ ai .</sentence>
				<definiendum id="0">dependency tree dT ( Tr )</definiendum>
				<definiens id="0">The set of nodes of dT ( Tr ) is the set of 3-tuples \ [ ai</definiens>
			</definition>
			<definition id="6">
				<sentence>The symbol dTN ( G ) denotes the union of all dTN ( u , , G ) for u '' E L ( G ) .</sentence>
				<definiendum id="0">symbol dTN ( G )</definiendum>
			</definition>
			<definition id="7">
				<sentence>We say that dTN ( G ) is the set of De-trees parsed by G. An example of a dependency tree is given in Fig .</sentence>
				<definiendum id="0">dTN</definiendum>
				<definiens id="0">the set of De-trees parsed by G. An example of a</definiens>
			</definition>
			<definition id="8">
				<sentence>The symbol Ng ( u , Tr ) represents the number of gaps in Cov ( u , Tr ) .</sentence>
				<definiendum id="0">Tr )</definiendum>
				<definiens id="0">the number of gaps</definiens>
			</definition>
			<definition id="9">
				<sentence>TN ( i ) denotes the class of sets ( of DR-trees } TN ( G. i ) .</sentence>
				<definiendum id="0">TN ( i )</definiendum>
				<definiens id="0">the class of sets ( of DR-trees } TN ( G. i )</definiens>
			</definition>
			<definition id="10">
				<sentence>The symbol CF + denotes the set .</sentence>
				<definiendum id="0">symbol CF +</definiendum>
				<definiens id="0">the set</definiens>
			</definition>
			<definition id="11">
				<sentence>where U means a node of a DR-tree and Cr means its coverage .</sentence>
				<definiendum id="0">U</definiendum>
			</definition>
</paper>

		<paper id="1436">
			<definition id="0">
				<sentence>Pitch modification means the modification of the distances between two consecutive opened-closed cycles , in which the effect of the previous cycle will be combined with the effect of the new excitation in a different manner but exactly in concordance with the theorem of superposition .</sentence>
				<definiendum id="0">Pitch modification</definiendum>
				<definiens id="0">means the modification of the distances between two consecutive opened-closed cycles</definiens>
			</definition>
			<definition id="1">
				<sentence>The TD-PSOLA ( Time Domain Pitch Synchronous Overlap-add ) developed by CNET is a very simple but ingenious method which assures high voice quality , the only disadvantage is that it is based on a time : domain windowing technique which can introduce some spectral distortions during the pitch modification .</sentence>
				<definiendum id="0">TD-PSOLA</definiendum>
				<definiens id="0">it is based on a time : domain windowing technique which can introduce some spectral distortions during the pitch modification</definiens>
			</definition>
</paper>

		<paper id="0513">
			<definition id="0">
				<sentence>Junction gathers the facts of coordination , and factorisation .</sentence>
				<definiendum id="0">Junction</definiendum>
				<definiens id="0">gathers the facts of coordination , and factorisation</definiens>
			</definition>
			<definition id="1">
				<sentence>Transferer Transference applies to a content word , called the transferee .</sentence>
				<definiendum id="0">Transferer Transference</definiendum>
				<definiens id="0">applies to a content word , called the transferee</definiens>
			</definition>
</paper>

		<paper id="0611">
			<definition id="0">
				<sentence>Nationality is a homogeneous semantic category , composed of simple words , or compounds ( South Korean ) , it can easily be stored in a list .</sentence>
				<definiendum id="0">Nationality</definiendum>
				<definiens id="0">a homogeneous semantic category , composed of simple words , or compounds ( South Korean</definiens>
			</definition>
			<definition id="1">
				<sentence>A variable is a word or a sequence of words .</sentence>
				<definiendum id="0">variable</definiendum>
				<definiens id="0">a word or a sequence of words</definiens>
			</definition>
</paper>

		<paper id="0720">
			<definition id="0">
				<sentence>In this framework , a question is translated into a LISP-expression : ( wh x S ( and ( PI x ) ( P2 x ) ) interpreted as a query for all members of S ( the semantic class of the wh-NP ) that satisfy both PI ( defined as the modifiers of the wh-NP ) and P2 ( the predicate of the clause ) .</sentence>
				<definiendum id="0">wh x S</definiendum>
				<definiens id="0">PI x ) ( P2 x ) ) interpreted as a query for all members of S ( the semantic class of the wh-NP ) that satisfy both PI ( defined as the modifiers of the wh-NP ) and P2 ( the predicate of the clause )</definiens>
			</definition>
			<definition id="1">
				<sentence>Based on the davidsonian treatment of action sentences , in which events are treated as individuals , every gloss is transformed in a first-order predicate formula for which ( 1 ) verbs are mapped in predicates t , erb ( e , z.y ) with the convention that variable e represents the eventuality of that action or event to take place , z represents the subject of the action .</sentence>
				<definiendum id="0">z</definiendum>
				<definiens id="0">the eventuality of that action or event to take place</definiens>
				<definiens id="1">the subject of the action</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , the gloss of synset { airline , airline business , airway } is ( a commercial enterprise that provides scheduled flights for passengers ) and has the following logical form transformation ( LFT ) : \ [ ea~ ezptise ( x ) ~ : co~ereial ( z ) &amp; provide ( e , x , y ) &amp; ~ : t li~cht ( y ) ~scheduled ( y ) ~f or ( e , p ) ~passenge : r ( p ) \ ] Characteristic of LFTs is the fact that the gloss genus is always the first predicate , rendering the LFT a formula of the form \ [ genus ( x ) g : differentia ( y ) \ ] .</sentence>
				<definiendum id="0">LFT</definiendum>
				<definiendum id="1">LFTs</definiendum>
				<definiens id="0">a commercial enterprise that provides scheduled flights for passengers</definiens>
			</definition>
			<definition id="3">
				<sentence>Finally , considering the set operator : St ( ~a $ 2 = { e \ ] e E St IJ $ 2 and there is no other e ' E St LI $ 2 such that e ' is a hyeprnym of e } the sorrel constraint approximations are defined as : rl Subject_SortdV ) =Subjeeti ( V ) ~a \ [ Jq subject~ , where subject~ is the subject from some S'~ in which the sense of V is i ; r30bject .</sentence>
				<definiendum id="0">St</definiendum>
				<definiendum id="1">sorrel constraint approximations</definiendum>
				<definiendum id="2">subject~</definiendum>
			</definition>
			<definition id="4">
				<sentence>Sorts satisfaction amounts to ( 1 ) the recognition of the sense of V in the text and ( 2 ) a search for any element from Role-Sorti ( V ) across all concepts semantically related ro all WordNet senses of N ( given that t/has the same role in the text ) .</sentence>
				<definiendum id="0">Sorts satisfaction amounts</definiendum>
				<definiens id="0">the recognition of the sense of V in the text and ( 2 ) a search for any element from Role-Sorti ( V ) across all concepts semantically related ro all WordNet senses of N ( given that t/has the same role in the text )</definiens>
			</definition>
			<definition id="5">
				<sentence>Sorti ( V ) = Subject_Sort ; ( V ) &amp; Object_Sorti ( V ) &amp; &amp; I-Ii Prep_Sorti ( V , prepj ) The satisfaction of Role_Sorti ( V ) is a search for any element from this set along ( 1 ) all synonyms , ( 2 ) all hypernyms and ( 3 ) all ~ geni for each WordNet sense of N. If this search is successful , we rule that N had a literal reading .</sentence>
				<definiendum id="0">Sorti</definiendum>
				<definiens id="0">a search for any element from this set along ( 1 ) all synonyms</definiens>
			</definition>
			<definition id="6">
				<sentence>Lexico-semantic metonymies retrieve concepts Cm E Role .</sentence>
				<definiendum id="0">Lexico-semantic metonymies</definiendum>
			</definition>
			<definition id="7">
				<sentence>The WordNet concepts that morphologically cue meronymic relations are those synsets containing collocations of such lexemes as unit ( e.g. , administrative unit , army unit ) , system ( e.g. , exhaust system , file system ) , par~ ( e.g. body part , academic department ) , group ( e.g. , jazz group , pressure group ) , or other words that form the same hierarchies as part to member .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">unit ( e.g. , administrative unit , army unit ) , system ( e.g. , exhaust system , file system ) , par~ ( e.g. body part , academic department ) , group ( e.g. , jazz group , pressure group ) , or other words that form the same hierarchies as part to member</definiens>
			</definition>
			<definition id="8">
				<sentence>Frequently , N is a nominalization ofa VN , and thus LFI ' ( VN ) brings forward related semantic information .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a nominalization ofa VN , and thus LFI ' ( VN ) brings forward related semantic information</definiens>
			</definition>
			<definition id="9">
				<sentence>Metonymic paths produce the expected coerced knowledge if they bring forward concepts that corefer with nominals from the successive sentences .</sentence>
				<definiendum id="0">Metonymic paths</definiendum>
				<definiens id="0">produce the expected coerced knowledge if they bring forward concepts that corefer with nominals from the successive sentences</definiens>
			</definition>
			<definition id="10">
				<sentence>Noun proposal as a nominalization is a candidate for morph-logic path derivations as well .</sentence>
				<definiendum id="0">nominalization</definiendum>
				<definiens id="0">a candidate for morph-logic path derivations as well</definiens>
			</definition>
</paper>

		<paper id="0507">
			<definition id="0">
				<sentence>A dependency grammar is a five-tuple &lt; W , C , S , D , H &gt; , where W is a finite set of words of a natural language ; C is a finite set of syntactic categories ; S is a non-empty set of categories ( S _C C ) that can act as head of a sentence ; D is the set of dependency relations , for instance SUB J , OBJ , XCOMP , P-OB3 , PRED ; H is a set of dependency rules of the form z : X ( &lt; raYl &gt; ... &lt; ri-l~-l &gt; # &lt; ri+l~+l &gt; ... &lt; rmYrn &gt; ) 1 ) z E W , is the head of the rule ; 2 ) X E C , is its syntactic category ; 3 ) an dement &lt; rjYj &gt; is a d-pair ( which describes a dependent ) ; the sequence of d-pairs , ineluding the special symbol # ( representing the linear position of the head ) , is called the d-pair sequence .</sentence>
				<definiendum id="0">dependency grammar</definiendum>
				<definiendum id="1">W</definiendum>
				<definiendum id="2">C</definiendum>
				<definiendum id="3">S</definiendum>
				<definiendum id="4">D</definiendum>
				<definiendum id="5">H</definiendum>
				<definiens id="0">a finite set of words of a natural language ;</definiens>
				<definiens id="1">a finite set of syntactic categories</definiens>
				<definiens id="2">a non-empty set of categories ( S _C C ) that can act as head of a sentence</definiens>
				<definiens id="3">the set of dependency relations , for instance SUB J , OBJ , XCOMP , P-OB3 , PRED</definiens>
			</definition>
			<definition id="1">
				<sentence>A subcategorization hierarchy is a 6-tuple &lt; T , L , D , Q , F , -- &lt; r &gt; , where : T is a finite set of subcategorie .</sentence>
				<definiendum id="0">subcategorization hierarchy</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">a 6-tuple &lt; T , L , D , Q , F , -- &lt; r &gt;</definiens>
			</definition>
			<definition id="2">
				<sentence>Each subcategorization frame is a total mapping q : D -4 Rx &lt; nl , n~ &gt; such that nl _ &gt; 0 , n2 _ &gt; 0 and nl ~ n2 ; F is a bijection between subcategories and subcatvgorization frames , F : T -4 Q ; -- T is an ordering relation among subcategories .</sentence>
				<definiendum id="0">subcategorization frame</definiendum>
				<definiendum id="1">F</definiendum>
				<definiendum id="2">-- T</definiendum>
				<definiens id="0">a bijection between subcategories and subcatvgorization frames , F : T -4 Q ;</definiens>
			</definition>
			<definition id="3">
				<sentence>-In order to define _ &lt; T , we need some notation : N~ ( d ) , where q E Q and d E D , is the first element of q ( d ) , i.e. the number restr/ct/ons associated with the relation d in the subcategorization frame q. Vq ( d ) , where q E Q and d E D , is the second dement of q ( d ) , i.e. the value restrictions associated with In the relation d in the subcategorization frame q. tuitively , Nq ( d ) is the number of times the dependency relation d can be instantiated according to the subcategorization frame q ; Vq ( d ) is the set of subcategories that can be in relation d with a subcategory having q as a subcategorization frame. Let _ &lt; a , be an order relation of number restrictions ; given two pairs of natural numbers R , and R2 , R , &lt; R , , R2 iff rain ( R , ) &gt; rain ( R2 ) ^ maz ( R , ) &lt; maz ( R2 ) namely , the range RI is inside the range R2 .</sentence>
				<definiendum id="0">Nq</definiendum>
				<definiendum id="1">RI</definiendum>
			</definition>
			<definition id="4">
				<sentence>Let - &lt; av be an order relation of value restrictions ; given two sets of subcategories V\ ] and V2 , V~ _ &lt; av V~ iff V~ C_ V2 Now , we can say that , for each h , t~ E T : tl ~ -- T t2 iff VdED ( NF ( t , ) ( d ) ~R~ NF ( t2 ) ( d ) A ( Vr ( t , ) ( d ) &lt; -- Rv VF ( t2 ) ( d ) ) The relation -- &lt; T is a partial order on T. If we assume the existence of a most general element TOP , it can act as the root of a hierarchy defined on -- -- -r. In the definitions above , each subcategory in the hierarchy defined by _ &lt; r is associated , through F , with a subcategorization frame. So , through L and F , each word in the lexicon is associated with one or more subcategorization frames. Actually , lexical ambiguity is due to L since F is a bijection. In the rest of this section we show that each subcategorization frame q defines a set of dependency rules , in the sense nsed in section 2 for the formal definition of the grammar. In this way , we get that the hierarchy specifies a correspondence between words and rules. Moreover , we show that the hierarchy acts as a taxonomy : given that rules ( t , ) C H is the set of dependency rules whose head is the syntactic category t , , we have that 60 Vtz , t~ E T Vdr E H ( t , &lt; _T t2 ^ dr ~ rules ( h ) -- ~ dr ~ rules ( t~ ) ) In order to specify the correspondence between subcategorization frames and dependency rules , we first define = { ml m = \ [ &lt; d , t &gt; I t e V0 ( d ) \ ] ^ minNq ( d ) &lt; Card ( m ) &lt; maxNq ( d ) } Given a subcategorization frame q and a relation d , Depq ( d ) is the set of all multisets of pairs &lt; d , t &gt; , where t is a subcategory E Vq ( d ) .</sentence>
				<definiendum id="0">Depq ( d )</definiendum>
				<definiendum id="1">t</definiendum>
				<definiens id="0">Let - &lt; av be an order relation of value restrictions ; given two sets of subcategories V\ ] and V2 , V~ _ &lt; av V~ iff V~ C_ V2 Now , we can say that , for each h , t~ E T : tl ~ -- T t2 iff VdED ( NF ( t , ) ( d ) ~R~ NF ( t2 ) ( d ) A ( Vr ( t , ) ( d ) &lt; -- Rv VF ( t2 ) ( d ) ) The relation -- &lt; T is a partial order on T. If we assume the existence of a most general element TOP , it can act as the root of a hierarchy defined on -- -- -r. In the definitions above , each subcategory in the hierarchy defined by _ &lt; r is associated , through F , with a subcategorization frame. So , through L and F , each word in the lexicon is associated with one or more subcategorization frames. Actually , lexical ambiguity is due to L since F is a bijection. In the rest of this section we show that each subcategorization frame q defines a set of dependency rules , in the sense nsed in section 2 for the formal definition of the grammar. In this way , we get that the hierarchy specifies a correspondence between words</definiens>
				<definiens id="1">a taxonomy : given that rules ( t , ) C H is the set of dependency rules whose head is the syntactic category t , , we have that 60 Vtz , t~ E T Vdr E H ( t , &lt; _T t2 ^ dr ~ rules ( h ) -- ~ dr ~ rules ( t~ ) ) In order to specify the correspondence between subcategorization frames and dependency rules , we first define = { ml m = \ [ &lt; d , t &gt; I t e V0 ( d ) \ ] ^ minNq ( d ) &lt; Card ( m ) &lt; maxNq ( d ) } Given a subcategorization frame q and a relation d</definiens>
				<definiens id="2">the set of all multisets of pairs &lt; d , t &gt;</definiens>
			</definition>
			<definition id="5">
				<sentence>The definition of subcategorization frame is modified in the following way : Q is a set of ordered snbcategorization frames .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiens id="0">a set of ordered snbcategorization frames</definiens>
			</definition>
			<definition id="6">
				<sentence>Vq E Q \ [ q : &lt; &lt; D ~ R x 2T &gt; x20 &gt; \ ] , where 1t is as before and O is a set of pairs &lt; dl , dz &gt; where d , ,d2 e DU { # } .</sentence>
				<definiendum id="0">1t</definiendum>
				<definiendum id="1">O</definiendum>
				<definiens id="0">a set of pairs &lt; dl , dz &gt; where d , ,d2 e DU { # }</definiens>
			</definition>
			<definition id="7">
				<sentence>All the sentences containing the verbs under analysis were automatically extracted from the corpus , and the subcategorization patterns ( rules ) exhibited by the verbs in those sentences were manually col° lected .</sentence>
				<definiendum id="0">subcategorization patterns</definiendum>
				<definiens id="0">rules ) exhibited by the verbs in those sentences were manually col° lected</definiens>
			</definition>
			<definition id="8">
				<sentence>The whole hierarchy has 6 levels : the top level ( class VERB ) represents the general constraints for Italian verbs , the top+l level distinguishes the constraints for impersonal ( V1 ) , intransitive ( VERB-INTR ) and transitive ( VERB-TRANS ) verbs , the top+2 , top+3 , etc. levels represent specific classes of verbs ( from V2 to VS0 ) .</sentence>
				<definiendum id="0">top+l level distinguishes the constraints for impersonal</definiendum>
				<definiendum id="1">etc. levels</definiendum>
				<definiens id="0">the top level ( class VERB ) represents the general constraints for Italian verbs</definiens>
			</definition>
			<definition id="9">
				<sentence>64 GRAMMATICAL RELATION subject ( Soot ) object ( Oct ) predicative complement ( PRED ) complements ( COMPL ) , at most 3 adjunct ( AGGIUNTO ) SYNTACTIC CATEGORY ( value restriction ) nominal group , N embedded clause headed by the complementizer che , `` that '' , Cheaub preposition di , `` off , Prep\ [ di\ ] infinitive verb , Verb\ [ inf\ ] nominal group , N embedded clause headed by the complementizer ehe , `` that '' , Chesub preposition d/ , `` of '' , Prep\ [ di\ ] nominal group , N adjective group , Adj prepositional group , Prep prepositional group , Prep clitic , Clitic prepositional group , Prep adverb , Ado conjunction , Conjdip verb of non-finite mood , Verb\ [ non .</sentence>
				<definiendum id="0">GRAMMATICAL RELATION subject</definiendum>
				<definiendum id="1">Prep clitic</definiendum>
				<definiens id="0">value restriction ) nominal group</definiens>
				<definiens id="1">infinitive verb , Verb\ [ inf\ ] nominal group , N embedded clause headed by the complementizer ehe , `` that ''</definiens>
			</definition>
			<definition id="10">
				<sentence>V1 is the class of impersonal verbs , that can only have adjuncts as dependents the restriction is on the range , \ [ 0 , 0\ ] , of the other relations .</sentence>
				<definiendum id="0">V1</definiendum>
				<definiens id="0">the class of impersonal verbs</definiens>
			</definition>
			<definition id="11">
				<sentence>VS0 is a subclass of VERB-TRANS : it restrics the sets of categories associated to the relations OGG and PRED , and specializes the relation Co~lPl .</sentence>
				<definiendum id="0">VS0</definiendum>
				<definiens id="0">a subclass of VERB-TRANS : it restrics the sets of categories associated to the relations OGG and PRED , and specializes the relation Co~lPl</definiens>
			</definition>
</paper>

		<paper id="1102">
			<definition id="0">
				<sentence>This paper describes the motivation and design of the Corpus Encoding Standard ( CES ) ( Ide , et al. , ( 1996 ) ; Ide , 1998 ) , an encoding standard for linguistic corpora intended to meet the need for the development of standardized encoding practices for linguistic corpora .</sentence>
				<definiendum id="0">Corpus Encoding Standard</definiendum>
				<definiens id="0">an encoding standard for linguistic corpora intended to meet the need for the development of standardized encoding practices for linguistic corpora</definiens>
			</definition>
			<definition id="1">
				<sentence>This paper describes the motivation and design of the Corpus Encoding Standard ( CES ) ( Ide , et al. , ( 1996 ) ; Ide , 1998 ) , an enccding standard for linguistic corpora intended to meet the need for the principled development of standardized encoding practices for linguistic corpora .</sentence>
				<definiendum id="0">Corpus Encoding Standard</definiendum>
				<definiens id="0">linguistic corpora intended to meet the need for the principled development of standardized encoding practices for linguistic corpora</definiens>
			</definition>
			<definition id="2">
				<sentence>The CES is an application of SGML ~ ( ISO 8879:1986 , Information Processing -- Text and Office Systems -- Standard Generalized Markup Language ) , conformant to the TEl Guidelines for Electronic Text Encoding and Interchange ( Sperberg-McQueen and Burnard , 1994 ) .</sentence>
				<definiendum id="0">CES</definiendum>
				<definiens id="0">an application of SGML ~ ( ISO 8879:1986 , Information Processing -- Text and Office Systems -- Standard Generalized Markup Language ) , conformant to the TEl Guidelines for Electronic Text Encoding</definiens>
			</definition>
			<definition id="3">
				<sentence>Therefore , a major feature of the CES is the provision for a series of increasingly refined encodings of text , beyond the minimum requirements .</sentence>
				<definiendum id="0">CES</definiendum>
				<definiens id="0">the provision for a series of increasingly refined encodings of text , beyond the minimum requirements</definiens>
			</definition>
			<definition id="4">
				<sentence>The CES addresses this problem as well as several others by defining a data architecture for corpora in which annotation information is not merged with the original , but rather retained in separate SGML documents ( with different DTDs ) and linked to the original or other annotation documents .</sentence>
				<definiendum id="0">CES</definiendum>
				<definiens id="0">several others by defining a data architecture for corpora in which annotation information is not merged with the original , but rather retained in separate SGML documents ( with different DTDs</definiens>
			</definition>
			<definition id="5">
				<sentence>The CES covers the encoding of objects in the primary data that are seen to be relevant to corpus-based work in language engineering research and applications , including : ( 1 ) Document-wide markup : bibliographic description of the document , encoding description , etc. ( 2 ) Gross structural markup : structural units of text , such as volume , chapter , etc. , down to the level of paragraph ; also footnotes , titles , headings , tables , figures , etc. normalization to recommended character sets and entities ( 3 ) Markup for sub-paragraph structures : sentence .</sentence>
				<definiendum id="0">CES</definiendum>
				<definiens id="0">covers the encoding of objects in the primary data that are seen to be relevant to corpus-based work in language engineering research</definiens>
				<definiens id="1">structural units of text , such as volume , chapter , etc. , down to the level of paragraph ; also footnotes , titles , headings , tables , figures , etc. normalization to recommended character sets and entities</definiens>
			</definition>
			<definition id="6">
				<sentence>The CES is intended to cover those areas of corpus encoding on which there exists consensus among the language engineering community , or on which consensus can be easily achieved .</sentence>
				<definiendum id="0">CES</definiendum>
			</definition>
			<definition id="7">
				<sentence>The hyper-document comprising each text in the corpus and its annotations consists Of several documents .</sentence>
				<definiendum id="0">hyper-document</definiendum>
				<definiens id="0">comprising each text in the corpus and its annotations consists Of several documents</definiens>
			</definition>
			<definition id="8">
				<sentence>Because the CES is an application of SGML , document structure is defined using a context free grammar in a document type definition ( DTD ) .</sentence>
				<definiendum id="0">CES</definiendum>
				<definiendum id="1">DTD</definiendum>
				<definiens id="0">an application of SGML</definiens>
			</definition>
			<definition id="9">
				<sentence>The CES documentation provides an informal semantics for tags used in the cesDoc DTD , especially sub-paragraph linguistic elements .</sentence>
				<definiendum id="0">CES documentation</definiendum>
				<definiens id="0">provides an informal semantics for tags used in the cesDoc DTD , especially sub-paragraph linguistic elements</definiens>
			</definition>
			<definition id="10">
				<sentence>The cesAlign DTD defines the annotation document containing alignment information for parallel texts .</sentence>
				<definiendum id="0">cesAlign DTD</definiendum>
				<definiens id="0">defines the annotation document containing alignment information for parallel texts</definiens>
			</definition>
</paper>

		<paper id="0208">
			<definition id="0">
				<sentence>Compound queries can be formulated via the `` Document Restrictions '' menu by selecting the union or intersection of previous queries , in effect an AND or OR Boolean operator across queries .</sentence>
				<definiendum id="0">Compound queries</definiendum>
				<definiens id="0">the union or intersection of previous queries , in effect an AND or OR Boolean operator across queries</definiens>
			</definition>
			<definition id="1">
				<sentence>O study hnk $ ( hemi &lt; al Oou~le clkk to edit ( ol~s : ~rag ( o remange t -e , ght j I~r¢ , du~ t -e~.tl t og/~ 3/96-T RANSC RtPT : Jmako n=~lt on the tame ~*vli c~ble ic ... ... ... . , ~.. : , ,~0 ~ c.~ , L.~. `` IL , *2U ; , °o'Z~ % '' - ; ' ; : ' , ~ % ' ; ; ; : ~'0 , , , ... ... ... ... , , , J ,1 : ... ... ... ... ... ... ... ... ... .. , Ix Figure 2c. J-FISH Multiserver Visualization More recently , we have explored multiple server evaluation on popular World Wide Web search engines. For example , Figure 2c illustrates a query across multiple servers. Research issues include differences in relevancy ranking algorithms , encoding of multiple attributes beyond relevancy using color or size ( e.g. , length , 53 quality , cost , source ) , and document collections which are heterogeneous in size , content , and format. Figure 3a ( Gershon et al. 1995 ; Gershon 1996 ) illustrates another navigation mechanism in which the user is able to view a hierarchy of the browse space. The left : hand of Figure 3a displays the traditional HTML layout of a web page whereas the right hand side illustrates a hierarchical , navigable view automatically generated from the underlying structure of the browsing space. The user can create a personal space by interactively and visually modify the structure of hyperspace or extracting segments of the documents. T'h¢ M I T~4.E ~'por md4n k • mudWac~c ~'o*'idm t~ekn. , e'tJ tnd ma , tepc paidmsee Itn~ ¢lr~irau~t i1 i~n~ i \u~ ~kH Figure 3a. Hyperspace Structure Visualization For discovery and analysis of new information and relationships in retrieved documents , we have developed a method for aggregating relevant information and representing it visually ( Gershon , et al , 1995 ) . The method is based on representing correlations of words within a document in a table. These tables could be very large depending on the size of the document thus making it difficult for the user to perceive and make sense of all the highly relevant correlations. Since the order of the words is not usually based on contents , the order of the words is permuted until the highly relevant correlations are concentrated in one comer. Fat Fatigue 22 Aches 10 Alments 2 nausea 15 Smoking 3 Snacks 47 11 33 sedentary 2 4 Fatigue Aches Nausea Ailments fat snacks 22 10 15 smoking 3 47 11 3 4 33 5 1 2 4 sedentary 2 Figure 3b. Example of Unaggregated ( top ) and Aggregated ( bottom ) Tables Other research at MITRE has focused on automatic discovery and visualization of semantic relations among individual and groups of documents ( Mani and Bloedom 1997 ) . Figure 3c illustrates the results of visualization of a set of documents using the NetMap visualization software after clustering these into related groups which appear around a circle. Outside of each cluster on the circle are displayed intracluster relations ; in the center of the circle are intercluster relations ( e.g. , a shared named entity such as a person , place , or thing which appears in multiple documents ) . The user can zoom in any part of the graph. This is shown in Figure 3d , which shows individual people ( green ) and organizations ( aquamarine ) . Selecting an individual entity from a document returns a display such as that in Figure 3e. Figure 3e illustrates individual entities encoded with color and shapes ( e.g. , people in green stick figures , organizations in aquamarine diamonds , locations in purple jagged rectangles , documents in yellow circles , person-organization relations in white squares ) . Lines and their properties ( e.g. , color , dashed ) can encode relations among these entities ( e.g. , co-occurrence in documents ) . This provides a richer mechanism for discovering 54 interdocument and interentity relationships during analysis. Current research is investigating the role of automated text summarization , document retrieval and navigation and visualization. Figure 3d. Zooming in on Document Cluster Figure 3c. Document Cluster Visualization Figure 3e. Entity Relation Visualization 55 MITRE 's Broadcast News Navigator ( BNN ) is a system that is investigating analysis of trends in news reporting. BNN performs multistream ( audio , video , text ) analysis to eliminate commercials , segment stories , extract named entities ( i.e. , people , organization , location ) and keyframes , and classify and summarize stories ( Merlino , Morey , and Maybury 1997 ) . BNN 's intuitive web-based interface gives the user the ability to browse , query , extract from and customize digitized broadcasts. Figure 4 illustrates a trend analysis display from BNN that shows the most frequently mentioned named entities reported on CNN Prime News TM from October to November of 1997. `` China '' spikes in the center of the graph , associated with a state visit to Washington. Later `` Iraq '' spikes which is correlated with news regarding UN site inspections. The user can click on any point on the line graphs and be brought to a list of stories that mention that named entity. Ik'llq~ 11 ~.~ Figure 4. Broadcast News Visualization \ ] In contrast , the user can formulate a query specifying keywords , named entities or subjects. Figure 5a shows the results of executing the query : Find me stories which have a topic of `` United States '' . BNN performs no co-reference resolution , a topic of current research at MITRE. `` chemicals '' , the keywords `` chemical weapons '' , person `` Sadam Hussein '' , organization `` Pentagon '' , and location `` Iraq '' . Each story in this `` Story Skim '' view is represented by a keyframe and the three most frequent named entities. Selecting one of these stories yields a `` Story Detail '' display , which as shown in Figure 5b including a keyframe , named entities , subject classification and pointers to the closed caption and video source. Figure 5a. BNN `` Story Skim '' Visualization Summary Closed Source Topics Figure 5b. `` Story Detail '' visualization Current research is exploring connecting these broadcast news stories with visualizing topic frequencies mechanisms for low quality transcriptions of broadcast 56 intemet stones , over time , and spoken language stories. Other investigations are focusing on which presentation mixes ( e.g. , keyffames , named entities , one line summary , full video source ) are most effective for story retrieval and fact extraction from news ( Merlino and Maybury 1998 ) . The Geospatial News on Demand Environment ( GeoNODE ) initiative at MITRE is a new project investigating visualizing geographic aspects of news events. This program builds on MITRE 's BNN , described in the previous section , and MSIIA , addressed in the subsequent section. GeoNODE is based on the research area of Geographic Visualization which investigates methods and tools that impact the way scientists and others conceptualize and explore georeferenced data , make decisions critical to society , and learn about the world ( MacEachren and Ganter 1990 , Taylor 1991 ) . Since news reports are about events in the world , the reported events and trends can be assessed , queried , and reviewed effectively by leveraging a person 's preexisting knowledge of the world 's geography. The objective of GeoNODE is to understand the information integration of geospatial/temporal visualizations , information retrieval , multimedia , and other technologies to support browsing , analysis , and rapid inference from broadcast news. As shown in Figure 6 , GoeNODE will analyze global and local cooperation and conflict found in broadcast news , internet , newswire and radio sources as well as broadcast news. Processing will include the identification , extraction , and summarization of events from national and international sources. GeoNODE will consider event types ( e.g. , terrorist acts , narcotrafficking , peace accords ) , frequency , and severity in an interactive geo-spatial/temporal context that supports browsing , retrieval , analysis and inference. 57 Figure 6. GeoNODE Architecture Although a geographical context can enhance a person 's understanding of reported events and therefore facilitate news retrieval and further queries , the same familiar visualization concerns apply to geographic presentation that are salient in visualizing any data rich multivariate information space. The GeoNODE user experience is derived from research , experience and standard practice in the visual search and retrieval domains : Overview first , zoom and filter , then details-on-demand ( Shneiderman 1994 ) . During each stage of the visualization process , cartographic methods and spatial analysis techniques are applied. These can be considered as a kind of grammar that allows for the optimal design , production and use of maps , depending on the application ( Kraat 1997 ) . Select cartographic generalization operators are applied to address key multi-scale and information overload problems ( Buttenfield 1991 ) . GeoNODE addresses Knowledge Representation ( KR ) and information fusion issues that are important to the news event presentation. The KR activities specific to GeoNODE are concerned with discovering and manipulating geospatial and temporal information , specifically investigating the following : improved natural language processing of place names that are central to understanding a news report • news event modeling • cartographic generalization rules • transformation of news events to visual metaphors Spatial information management is currently growing in its utility to commercial applications , and several industries have already begun to explicitly rely on GIS systems , although most ( 53 % ) companies are evaluating while an average of only 7 % are implementing or using a GIS ( IDC 1997 ) . Accompanying the growing interest in spatial information is a technology trend influencing the architecture of GeoNODE , mainly , a shift from single-purpose/standalone GIS applications to geospatial extensions and services for databases , component frameworks , data warehouses and data analysis applications. By supporting a component-based architecture , GeoNODE can more readily take advantage of future geospatial services and an expanding number of news sources ( internet , newswire , radio , and other broadcast sources ) . Further research will investigate incorporation of summarization , geospatial/temporal KR , and other traditional visualization techniques. For example , Figure 7 illustrates some of the kinds of visualizations that are being explored by other researchers , such as the use of color and geolocation to encode relations among geographic entries. Figure 7 is a geographic visualization of early WWW usage available at http : //www.cybergeography.org/atlas/atlas.html. These and other research threads will shape GeoNODE into a visualization component for reasoning about news events in geographic space. As a long term objective , the system architecture should allow for navigation and retrieval from topic , conceptual , and web spaces where a user can access , update and annotate existing data with spatial information. Figure 7. Visualization of Geospatial Relationships The Multisource Integrated Information Analysis ( MSIIA ) project , led by Steve Hansen at MITRE , is exploring effective mechanisms for sensor and battlefield visualization. For example , national and military intelligence analysts are charged with monitoring and exploiting dozens of sources of information in real time. These range from sensors which capture images ( infrared , electrooptical , multispectral ) to moving target indicators characterized by a point and some features ( e.g. , tracked vs. wheeled vehicle ) to signals intelligence characterized by centroids and error elipses. Knowing which source to select and which sensors to task is paramount to successful situation assessment. An integrated view into what sensors are where when , as well as a fused picture of their disparate information types and outputs , would be invaluable. Figure 8 illustrates one such visualization. The x-y dimension of the upper display captures the coordinates of a geospatial area whereas the y coordinate displays time. This enables the user to view which areas are being sensed by which type of sensor ( encoded by color or implicitly by the resultant characteristic shape ) . For example , a large purple cylinder represents the area over time imaged by a geosychronous satellite , the green cylinders are images taken over time of spots on the surface of the earth , whereas the wavy blue line is the ground track of a sensor flying across an area ( e.g. , characteristic of a unmanned air vehicle such as predator ) . If we take a slice at a particular time of the upper display in Figure 8 we get the coverage of particular areas from a specific time. If we project all sensor coverages 58 over an area downward to the surface , we obtain the image shown in the lower display of Figure 8. military for planning and training. The sand can be sculpted to match the terrain in a specific geographic region. People standing around the table can place plastic or metal models of vehicles and other assets over this terrain model to indicate force deployment and move them around the terrain to indicate and/or rehearse force movements. Figure 8. Sensor Coverage Visualization A user can utilize this display to determine what material is available for a given time and space , analyze unattended coverage areas , and plan future collections. MSIIA is also investigating georegistration and display of the results of collections in an integrated , synthetic view of the world ( e.g. , fusing maps with images with radar returns ) . We consider next another example of synthetic views of the world. Just as visualization plays an important role in information space visualization for MSIIA , MITRE 's research on the Collaborative Omniscient Sandtable Metaphor ( COSM ) seeks to define a new generation of human-machine interfaces for military Command and Control ( C2 ) . The `` sandtable '' underlying COSM is a physical table whose top is rimmed with short walls and filled with sand. It is used in the 59 In defining COSM , we expanded the functionality of a sandtable and moved it into an electronic domain. It now taps into global gigabyte databases of C2 information which range from static data on airfield locations , to real-time feeds from hundreds ground , air , and space based sensors. This data is used to synthesize macroscopic or microscopic views of the world that form the foundation of a collaborative visualization system. Manipulating these views leads not only to modifying data , but also directing the actions of the underlying physical assets ( e.g. , moving an icon causes an aircraft to be redirected from point A to point B ) . A conceptual view of COSM is shown in Figure 9 , where participants at air , land , and sea locations collaborate over an electronic sandtable. Some users are physically present , while others are represented by their avatars. The key elements of COSM are geographic independence ( transparent access to people , data , software , or assets regardless of location ) , a multimodal , direct manipulation interface with an initial emphasis is on the visual modality , heterogeneous platform support ( enabling users to tailor data depictions to a range of platform capabilities ) , and data linkage ( maintaining all parent , child , and peer relationships in the data ) . Figure 9. Conceptual View of COSM Figure 10. Virtual Reality Instantiation A first instantiation of COSM was implemented using Virtual Reality ( VR ) technology , as illustrated in Figure 10. The table is a stereoscopic projection system driven by a graphics workstation. It uses a horizontal display surface approximately 6 feet wide and 4 feet deep to display maps , imagery , and models of the terrain and objects upon or above the terrain. Since it is stereoscopic , objects above the terrain , such as airbome aircraft , appear to be above the surface of the table. The vertical screen behind the table is a rear-projection display used primarily used for collaboration support. At the top , we see a panel of faces representing all the remote users who have similar systems and are currently connected to this one with audio , video , and data links. The table serves as a shared whiteboard that is visible to all the users and can be manipulated by them. The larger faces at the bottom of the vertical screen are two users who have `` stepped up to the podium '' and currently have control of what it being seen on the table. The figure shows the user interacting with the table through the use of two magnetic position trackers. The first is attached to a pair of stereoscopic glasses , and as the user moves his head and walks around the table the computer determines his eyepoint location from the tracker and recomputes his view accordingly. The second tracker is attached to a glove that serves as an input device. The user 's gloved hand becomes a cursor and he can use his fingers to touch an object to indicate selection or grab and move an object to indicate an action. Several different kinds of information can be displayed on the table. Figure 11 illustrates a display of current air and ground information. There are several aircraft depicted as realistic models , with the relative scale of the models representing the relative sizes of the respective aircraft. They move in real-time , with the stereoscopic display making them appear to be flying above the table. Conceptually , the positions of the aircraft are provided in real-time by a radar system and the user has the option of displaying them as symbols or models. Remote users worldwide have real-time access to the data. The hemisphere in the upper left is a simple , unclassified representation of the threat dome of a Surface to Air Missile ( SAM ) emplacement. The large arrow is a cursor that is controlled by a remote user who is collaborating over this display. The amorphous blob in the lower left is a depiction of a small storm cell that is also moving through the region. This weather data is visually integrated in real-time with the current air picture data. The aircraft position , weather , and threat information are all provided by different sensor systems. However , they share a common spatiotemporal reference that allows them to be fused in this real-time synthetic view of the world. Every object in this synthetic view also serves as a visual index into the underlying global C2 database. Selecting an aircraft would let us determine its current status ( airborne with a certain speed and heading ) and plans ( origin , 60 destination , and mission ) , as well as associated information such as logistics at its base of origin. Figure 11. Synthetic View of the World Our current research is focused on the use of aggregation and deaggregation of data within visual depictions , in order to support a wide range of users. A weaponeer wants to study the details of a target ( e.g. , construction material , distance below ground ) that is only a few hundred feet by a few hundred feet in size. A commander wants an overview of all airborne assets , targets , etc. for a region that is several hundred by several hundred miles in size. However , those examining an overview will frequently wish to `` drill down '' for maximum detail in certain areas , while those examining a detailed area may wish to examine a more global view to retain context. Allowing the visualization of data with this wide range of geographic scopes , as well as iterative travel between detail and overview , poses challenges in both data depiction , data simplification , and intuitive navigation techniques. The above varied and rich application spaces e.g. , visualizing search results , topics , relations and events in news broadcasts , battlefield activities provide a number of challenges for visualization research. Fundamental issues include : visualization techniques for static and dynamic information visualization , including complex semantic objects such as properties , relations , and events ? utilizing geospatial , temporal , and other contexts in synthetic displays of real world events that facilitate interface tasks ( e.g. , location , navigation ) , comprehension , analysis and inference ? and spatial query ) are most effective for which kinds of tasks ( e.g. , anomaly detection , trend analysis , comparative analysis ) . measures are necessary for these new visualization methods ? In visualization , we tend to deal with complexity through methodologies involving abstraction , aggregation , filtering , and focusing. Insights from natural language processing promise to help extract semantic information from text channels , to provide a richer , task-relevant characterization of the information space. Visualization can certainly benefit from other aspects natural language processing in achieving economy of interaction such as notions of context in reference ( e.g. , `` fast_forward &lt; the next week &gt; '' ) or relation ( e.g. , move `` &lt; enemy_icon &gt; behind &lt; Bunker Hill_icon &gt; '' in the currently focused display ) .</sentence>
				<definiendum id="0">GIS</definiendum>
				<definiens id="0">encoding of multiple attributes beyond relevancy using color or size ( e.g. , length , 53 quality , cost , source ) , and document collections which are heterogeneous in size , content</definiens>
				<definiens id="1">a shared named entity such as a person , place , or thing which appears in multiple documents</definiens>
				<definiens id="2">a system that is investigating analysis of trends in news reporting. BNN performs multistream ( audio , video , text ) analysis to eliminate commercials , segment stories , extract named entities ( i.e. , people , organization , location ) and keyframes , and classify and summarize stories ( Merlino , Morey , and Maybury 1997 ) . BNN 's intuitive web-based interface gives the user the ability to browse</definiens>
				<definiens id="3">over time , and spoken language stories. Other investigations are focusing on which presentation mixes ( e.g. , keyffames , named entities , one line summary , full video source ) are most effective for story retrieval</definiens>
				<definiens id="4">a new project investigating visualizing geographic aspects of news events. This program builds on MITRE 's BNN , described in the previous section , and MSIIA , addressed in the subsequent section. GeoNODE is based on the research area of Geographic Visualization which investigates methods and tools that impact the way scientists and others conceptualize and explore georeferenced data</definiens>
				<definiens id="5">to understand the information integration of geospatial/temporal visualizations , information retrieval , multimedia , and other technologies to support browsing , analysis , and rapid inference from broadcast news. As shown in Figure 6 , GoeNODE will analyze global and local cooperation and conflict found in broadcast news , internet , newswire and radio sources as well as broadcast news. Processing will include the identification , extraction , and summarization of events from national and international sources. GeoNODE will consider event types ( e.g. , terrorist acts , narcotrafficking , peace accords ) , frequency , and severity in an interactive geo-spatial/temporal context that supports browsing , retrieval , analysis</definiens>
				<definiens id="6">the optimal design , production and use of maps , depending on the application ( Kraat 1997 ) . Select cartographic generalization operators are applied to address key multi-scale and information overload problems ( Buttenfield 1991 ) . GeoNODE addresses Knowledge Representation ( KR ) and information fusion issues that are important to the news event presentation. The KR activities specific to GeoNODE are concerned with discovering and manipulating geospatial and temporal information</definiens>
				<definiens id="7">Spatial information management is currently growing in its utility to commercial applications , and several industries have already begun to explicitly rely on GIS systems</definiens>
				<definiens id="8">a technology trend influencing the architecture of GeoNODE , mainly , a shift from single-purpose/standalone GIS applications to geospatial extensions and services for databases , component frameworks , data warehouses and data analysis applications. By supporting a component-based architecture</definiens>
				<definiens id="9">conceptual , and web spaces where a user can access , update and annotate existing data with spatial information. Figure 7. Visualization of Geospatial Relationships The Multisource Integrated Information Analysis ( MSIIA ) project , led by Steve Hansen at MITRE , is exploring effective mechanisms for sensor and battlefield visualization. For example , national and military intelligence analysts are charged with monitoring and exploiting dozens of sources of information in real time. These range from sensors which capture images ( infrared , electrooptical , multispectral ) to moving target indicators characterized by a point and some features ( e.g. , tracked vs. wheeled vehicle ) to signals intelligence characterized by centroids and error elipses. Knowing which source to select and which sensors to task is paramount to successful situation assessment. An integrated view into what sensors are where when , as well as a fused picture of their disparate information types and outputs</definiens>
				<definiens id="10">x-y dimension of the upper display captures the coordinates of a geospatial area whereas the y coordinate displays time. This enables the user to view which areas are being sensed by which type of sensor ( encoded by color or implicitly by the resultant characteristic shape )</definiens>
				<definiens id="11">the terrain in a specific geographic region. People standing around the table can place plastic or metal models of vehicles and other assets over this terrain model to indicate force deployment and move them around the terrain to indicate and/or rehearse force movements. Figure 8. Sensor Coverage Visualization A user can utilize this display to determine what material is available for a given time and space , analyze unattended coverage areas , and plan future collections. MSIIA is also investigating georegistration and display of the results of collections in an integrated , synthetic view of the world ( e.g. , fusing maps with images with radar returns ) . We consider next another example of synthetic views of the world. Just as visualization plays an important role in information space visualization for MSIIA , MITRE 's research on the Collaborative Omniscient Sandtable Metaphor ( COSM ) seeks to define a new generation of human-machine interfaces for military Command</definiens>
				<definiens id="12">a physical table whose top is rimmed with short walls</definiens>
				<definiens id="13">a sandtable and moved it into an electronic domain. It now taps into global gigabyte databases of C2 information which range from static data on airfield locations , to real-time feeds from hundreds ground , air , and space based sensors. This data is used to synthesize macroscopic or microscopic views of the world that form the foundation of a collaborative visualization</definiens>
				<definiens id="14">participants at air , land , and sea locations collaborate over an electronic sandtable. Some users are physically present , while others are represented by their avatars. The key elements of COSM are geographic independence ( transparent access to people , data , software , or assets regardless of location ) , a multimodal , direct manipulation interface with an initial emphasis is on the visual modality , heterogeneous platform support ( enabling users to tailor data depictions to a range of platform capabilities ) , and data linkage ( maintaining all parent , child , and peer relationships in the data ) . Figure 9. Conceptual View of COSM Figure 10. Virtual Reality Instantiation A first instantiation of COSM</definiens>
				<definiens id="15">a stereoscopic projection system driven by a graphics workstation. It uses a horizontal display surface approximately 6 feet wide and 4 feet deep to display maps , imagery , and models of the terrain and objects upon or above the terrain. Since it is stereoscopic , objects above the terrain , such as airbome aircraft</definiens>
				<definiens id="16">a cursor and he can use his fingers to touch an object to indicate selection or grab and move an object to indicate an action. Several different kinds of information can be displayed on the table. Figure 11 illustrates a display of current air and ground information. There are several aircraft depicted as realistic models , with the relative scale of the models representing the relative sizes of the respective aircraft. They move in real-time , with the stereoscopic display making them appear to be flying above the table. Conceptually , the positions of the aircraft are provided in real-time by a radar system and the user has the option of displaying them as symbols</definiens>
				<definiens id="17">a depiction of a small storm cell that is also moving through the region. This weather data is visually integrated in real-time with the current air picture data. The aircraft position , weather , and threat information are all provided by different sensor systems. However , they share a common spatiotemporal reference that allows them to be fused in this real-time synthetic view of the world. Every object in this synthetic view also serves as a visual index into the underlying global C2 database. Selecting an aircraft would let us determine its current status ( airborne with a certain speed and heading ) and plans ( origin , 60 destination , and mission ) , as well as associated information such as logistics at its base of origin. Figure 11. Synthetic View of the World Our current research is focused on the use of aggregation and deaggregation of data within visual depictions</definiens>
				<definiens id="18">poses challenges in both data depiction , data simplification , and intuitive navigation techniques. The above varied and rich application spaces e.g. , visualizing search results , topics , relations and events in news broadcasts , battlefield activities provide a number of challenges for visualization research. Fundamental issues include : visualization techniques for static and dynamic information visualization , including complex semantic objects such as properties , relations , and events ? utilizing geospatial , temporal , and other contexts in synthetic displays of real world events that facilitate interface tasks ( e.g. , location , navigation ) , comprehension , analysis and inference ? and spatial query ) are most effective for which kinds of tasks ( e.g. , anomaly detection , trend analysis , comparative analysis</definiens>
				<definiens id="19">natural language processing promise to help extract semantic information from text channels , to provide a richer , task-relevant characterization of the information space. Visualization can certainly benefit from other aspects natural language processing in achieving economy of interaction such as notions of context in reference ( e.g. , `` fast_forward &lt; the next week &gt; '' ) or relation ( e.g. , move `` &lt; enemy_icon &gt; behind &lt; Bunker Hill_icon &gt; '' in the currently focused display )</definiens>
			</definition>
</paper>

		<paper id="1407">
			<definition id="0">
				<sentence>: : : The distinction of levels , with information Of varying shades ( salience gradation ) , implies that it should be possible to identify corresponding linguistic `` markers '' for each one of them .</sentence>
				<definiendum id="0">distinction of levels</definiendum>
				<definiens id="0">with information Of varying shades ( salience gradation</definiens>
			</definition>
			<definition id="1">
				<sentence>The first onespecifies the relative importance ( with values ranging from 0 to 3 ) of partial distances ( represented by the attributes dimension and order ) , while the second one specifies the relative importance ( possible values : 0=-2 ) of the names of the lines ( attribute name ) .</sentence>
				<definiendum id="0">relative importance</definiendum>
				<definiens id="0">the relative importance ( with values ranging from 0 to 3 ) of partial distances ( represented by the attributes dimension and order</definiens>
			</definition>
			<definition id="2">
				<sentence>The attribute degree-name , which represents the importance of•information concerning the names of the lines ( `` local importance '' ) , takes the Value of the input parameter ( `` global importance '' ) .</sentence>
				<definiendum id="0">attribute degree-name</definiendum>
				<definiens id="0">represents the importance of•information concerning the names of the lines ( `` local importance '' ) , takes the Value of the input parameter ( `` global importance '' )</definiens>
			</definition>
</paper>

		<paper id="1414">
			<definition id="0">
				<sentence>Here , lexicon entries consist of ( inter alia ) the three zones denotation , partial SemSpec ( PSemSpec ) , and stylistic features .</sentence>
				<definiendum id="0">lexicon entries</definiendum>
				<definiens id="0">consist of ( inter alia ) the three zones denotation , partial SemSpec ( PSemSpec ) , and stylistic features</definiens>
			</definition>
			<definition id="1">
				<sentence>The PSemSpec is an SPL-like template that includes a : lex annotation with the actual lexeme and possibly variables that are replaced by other PSemSpecs in the course of the lexicalization process .</sentence>
				<definiendum id="0">PSemSpec</definiendum>
				<definiens id="0">an SPL-like template that includes a : lex annotation with the actual lexeme and possibly variables that are replaced by other PSemSpecs in the course of the lexicalization process</definiens>
			</definition>
</paper>

		<paper id="0303">
			<definition id="0">
				<sentence>Ark , Content employs a novel content vector analysis approach for score assignment based on the individual arguments in an essay .</sentence>
				<definiendum id="0">Content</definiendum>
				<definiens id="0">employs a novel content vector analysis approach for score assignment based on the individual arguments in an essay</definiens>
			</definition>
			<definition id="1">
				<sentence>EssayContent is a content vector analysis program that treats an essay like a `` 'bag of words . ''</sentence>
				<definiendum id="0">EssayContent</definiendum>
				<definiens id="0">a content vector analysis program that treats an essay like a `` 'bag of words</definiens>
			</definition>
			<definition id="2">
				<sentence>All sentences are parsed with the Microsoft Natural Language Processing tool ( MSNLP ) ( see MSNLP ( 1997 ) ) .</sentence>
				<definiendum id="0">MSNLP</definiendum>
				<definiens id="0">parsed with the Microsoft Natural Language Processing tool</definiens>
			</definition>
</paper>

		<paper id="1124">
			<definition id="0">
				<sentence>A combination of the probability distributions defined by each heuristic yields the sentences that are most likely to be included in a summary .</sentence>
				<definiendum id="0">combination of the probability distributions defined by each heuristic</definiendum>
				<definiens id="0">yields the sentences that are most likely to be included in a summary</definiens>
			</definition>
			<definition id="1">
				<sentence>Central to RST is the notion of rhetorical relation , which is a relation that holds between two nonoverlapping text spans called NUCLEUS and SATELLITE .</sentence>
				<definiendum id="0">RST</definiendum>
				<definiens id="0">the notion of rhetorical relation , which is a relation that holds between two nonoverlapping text spans called NUCLEUS and SATELLITE</definiens>
			</definition>
			<definition id="2">
				<sentence>We extend this observation to similarity as well , by introducing the rhetorical-clustering-based metric , which measures the similarity between the salient units associated with two spans .</sentence>
				<definiendum id="0">rhetorical-clustering-based metric</definiendum>
				<definiens id="0">measures the similarity between the salient units associated with two spans</definiens>
			</definition>
			<definition id="3">
				<sentence>209 tics , we consider that a discourse structure A is `` better '' that a discourse structure B if the connectedness-based score of A is higher than the connectedness-based score of B. As we have already mentioned , discourse parsing is ambiguous the same way sentence parsing is : the rhetorical parsing algorithm often derives more than one discourse structure for a given text .</sentence>
				<definiendum id="0">discourse parsing</definiendum>
				<definiens id="0">the rhetorical parsing algorithm often derives more than one discourse structure for a given text</definiens>
			</definition>
			<definition id="4">
				<sentence>The results in table 2 show that , for newspaper articles , the shape-based metric is the best individual metric for distinguishing between discourse trees that are appropriate for 20 % summaries and discourse trees that are not .</sentence>
				<definiendum id="0">shape-based metric</definiendum>
				<definiens id="0">the best individual metric for distinguishing between discourse trees that are appropriate for 20 % summaries and discourse trees that are not</definiens>
			</definition>
			<definition id="5">
				<sentence>The results in table 4 show that , for Scientific American articles , the shape-based metric is the best individual metric for distinguishing between discourse trees that are appropriate for sentence-based summarization .</sentence>
				<definiendum id="0">shape-based metric</definiendum>
				<definiens id="0">the best individual metric for distinguishing between discourse trees that are appropriate for sentence-based summarization</definiens>
			</definition>
			<definition id="6">
				<sentence>The algorithm shown in figure 2 performs a greedy search in the seven-dimensional space defined by the weights , using an approach that mirrors that proposed by Sehnan , Levesque , and Mitchell ( 1992 ) for solving propositional satisfiability problems .</sentence>
				<definiendum id="0">Mitchell</definiendum>
				<definiens id="0">a greedy search in the seven-dimensional space defined by the weights , using an approach that mirrors that proposed by Sehnan , Levesque , and</definiens>
			</definition>
</paper>

		<paper id="1205">
			<definition id="0">
				<sentence>An HMM transducer builds on the data ( probability matrices ) of the underlying HMM .</sentence>
				<definiendum id="0">HMM transducer</definiendum>
				<definiens id="0">builds on the data ( probability matrices</definiens>
			</definition>
			<definition id="1">
				<sentence>In the exampleS : CONI-B2 \ [ DET , PRON\ ] -B1 \ [ ADJ , NOUN , v~aB\ ] : ~a \ [ ~ao~ , v~aB } -al V~B-A2 ( 15 ) ADJ is the most likely tag in the class \ [ £1~J , IY0trN , vFalB\ ] if it is preceded by the tag C0NJ two positions back ( B2 ) , by the class \ [ DET , PRON'I one position back ( B1 ) , and followed by the class I'NOUlY , VEI~\ ] one position ahead ( A1 ) and by the tag VERB two positions ahead ( A2 ) .</sentence>
				<definiendum id="0">ADJ</definiendum>
				<definiens id="0">the most likely tag in the class</definiens>
			</definition>
			<definition id="2">
				<sentence>a=0 ) I 1°°1 b-FST ( fl=l , a=l ) 75.14120.18 t 0.341 3.421 0.801 0.11 b-FST ( ~=2 , a=l ) FST was not computable 45 tags , 214 classes ( reduced tag set ) b-rSZ ( a.4=0 ) I 1°°1 I l I I b-FST ( fl=1,4=1 ) 175.71119.731 0.68\ [ 3.191 0.68\ ] b-FST ( fl=2,4=1 ) \ [ FST was not computable 36 tags , 181 classes ( reduced tag set ) b-FST ( fl-a=0 ) 100 b-FST ( fl=1,4=1 ) 78.56 17.90 0.34 2.85 0.34 b-FST ( /3=2,4=1 ) 99.77 0.23 27 tags , 119 classes ( reduced tag set ) b-FST ( /3-4=0 } 100 b-FST ( fl=1 , a=l ) 90.08 9.46 0.23~ 0.11 0.11 b-FST ( fl=2 , a=l ) 99.77 0.23 18 tags , 97 classes ( reduced tag set ) b-FST ( fl-a=0 ) 100 b-FST ( fl=l,4=l ) \ [ 93.04 6.84 0.11 b-FST ( fl -- 2 , a -- 1 ) 199.89 0.11 9 tags , 67 classes ( reduced tag set ) b-FST ( fl-4=0 ) 1001 b.-FST ( fl=l,4=l ) 86.66112.43 0.91 b-FST ( fl=2,4=1 ) 99.771 0.23 b-FST ( fl=3,4=1 ) 100 Language : English 1 ITest corpus : 19 934 words , 877 sentences \ [ Types of FST ( Finite-State Transducers ) cf. table 1 Table 3 : Percentage of sentences with a particular number of tagging results The algorithm presented in this paper describes the construction of a finite-state transducer ( FST ) that approximates the behaviour of a Hidden Markov Model ( HMM ) in part-of-speech tagging .</sentence>
				<definiendum id="0">reduced tag set ) b-FST</definiendum>
				<definiens id="0">the construction of a finite-state transducer ( FST ) that approximates the behaviour of a Hidden Markov Model ( HMM ) in part-of-speech tagging</definiens>
			</definition>
			<definition id="3">
				<sentence>ANNEX : Regular Expression Operators Below , a and b designate symbols , A and B designate languages , and R and Q designate relations between two languages .</sentence>
				<definiendum id="0">ANNEX</definiendum>
				<definiens id="0">Regular Expression Operators Below , a and b designate symbols , A and B designate languages , and R and Q designate relations between two languages</definiens>
			</definition>
</paper>

		<paper id="1226">
			<definition id="0">
				<sentence>Water , the noun , is a word very commonly met .</sentence>
				<definiendum id="0">Water</definiendum>
				<definiens id="0">a word very commonly met</definiens>
			</definition>
			<definition id="1">
				<sentence>Genuine syntactic ambiguity is a completely different matter from any part of the subject of this paper ; and this is so , even though there are two syntactic readings of each of sentences ( 1 ) and ( 2 ) : we do acknowledge that this means that the two sentences are each syntactically ambiguous ( but surely not semantically so ! )</sentence>
				<definiendum id="0">Genuine syntactic ambiguity</definiendum>
				<definiens id="0">a completely different matter from any part of the subject of this paper</definiens>
			</definition>
</paper>

		<paper id="1307">
			<definition id="0">
				<sentence>A Finite State Transducer ( FST ) is a six tuple r = ( Q , X , Y , q0 , QF , E ) , where Q is a finite set of states , X , Y are input and output alphabets , qo E Q is an initial state , QF c Q is a set of final states and E C Q x x* x Y* × Q are the edges or transitions .</sentence>
				<definiendum id="0">Finite State Transducer ( FST )</definiendum>
				<definiendum id="1">Q</definiendum>
				<definiendum id="2">E Q</definiendum>
				<definiendum id="3">QF c Q</definiendum>
				<definiens id="0">a six tuple r = ( Q , X , Y , q0 , QF , E ) , where</definiens>
				<definiens id="1">a finite set of states</definiens>
				<definiens id="2">an initial state</definiens>
				<definiens id="3">a set of final states and E C Q x x* x Y* × Q are the edges or transitions</definiens>
			</definition>
			<definition id="1">
				<sentence>A Sequential Transducer ( ST ) is a five tuple ~ '' = ( Q , X , Y , qo , E ) , where E C Q x X × Y* x Q and all the states are accepting ( QF = Q ) and determini .</sentence>
				<definiendum id="0">Sequential Transducer ( ST )</definiendum>
				<definiens id="0">a five tuple ~ '' = ( Q , X , Y , qo , E ) , where E C Q x X × Y* x Q and all the states are accepting</definiens>
			</definition>
			<definition id="2">
				<sentence>An important restriction of STs is that they preserve increasing length input-output prefixes ; i.e. , if t is a sequential transduction ' , then t ( X ) = A , t ( uv ) e t ( u ) Y* , where ~ is the empty or Nil string .</sentence>
				<definiendum id="0">STs</definiendum>
				<definiens id="0">the empty or Nil string</definiens>
			</definition>
			<definition id="3">
				<sentence>In the worst case , the number of states required by a SST to achieve this delaying mechanism can grow as much as O ( nk ) , where n is the number of ( functionally equivalent ) words and k the length of the delay .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of ( functionally equivalent ) words and k the length of the delay</definiens>
			</definition>
			<definition id="4">
				<sentence>Under this approach , the input sentence , x , is considered as a corrupted version of some sentence , ~ E L , where L is the domain or input language of the SST .</sentence>
				<definiendum id="0">L</definiendum>
				<definiens id="0">a corrupted version of some sentence</definiens>
				<definiens id="1">the domain or input language of the SST</definiens>
			</definition>
</paper>

		<paper id="1113">
			<definition id="0">
				<sentence>A similarity matrix for left and right contexts was created by calculating the relative entropy or Kullback Leibler ( KL ) distance 4 between the vectors of context frequency counts for each pair of verbs .</sentence>
				<definiendum id="0">similarity matrix</definiendum>
				<definiens id="0">created by calculating the relative entropy</definiens>
			</definition>
			<definition id="1">
				<sentence>nc N C is the number of categories L is the number of clusters mz is the number of instances in cluster l .</sentence>
				<definiendum id="0">nc N C</definiendum>
				<definiens id="0">the number of instances in cluster l</definiens>
			</definition>
			<definition id="2">
				<sentence>fd is the instances of category c in cluster l N is the total number of instances for cut T nc is the number of instances in category c Table 2 : Notation cut point in a hierarchical cluster analysis .</sentence>
				<definiendum id="0">fd</definiendum>
				<definiens id="0">the total number of instances for cut T nc is the number of instances in category c Table 2 : Notation cut point in a hierarchical cluster analysis</definiens>
			</definition>
			<definition id="3">
				<sentence>The entropy for categories H ( C ) is defined by H ( C ) = ~p ( c ) log2p ( c ) where p ( c ) = nc/N in the notation of Table clusters , which is the conditional entropy of categories given clusters , is defined by H ( CIL ) = ~ q ( 1 ) ~ p ( cll ) log2p ( cll ) l c where q ( l ) = mdN and p ( c\ ] l ) = ffct/mt in our notation .</sentence>
				<definiendum id="0">entropy for categories H ( C</definiendum>
				<definiendum id="1">H ( CIL</definiendum>
				<definiens id="0">H ( C ) = ~p ( c ) log2p ( c ) where p ( c ) = nc/N in the notation of Table clusters , which is the conditional entropy of categories given clusters</definiens>
			</definition>
			<definition id="4">
				<sentence>Information gain and mutual information I ( C ; L ) are defined by gain = I ( C ; L ) = H ( C ) H ( CIL ) Information gain increases as the mixture of categories in a cluster decreases .</sentence>
				<definiendum id="0">mutual information I</definiendum>
			</definition>
			<definition id="5">
				<sentence>Let H ( L ) be the entropy for clusters defined , as above , by H ( L ) = ~ q ( l ) log2q ( l ) Then the gain ratio is defined by gain ratio = I ( C ; L ) H ( L ) The gain ratio corrects the mutual information between categories and clusters by the entropy of the clusters .</sentence>
				<definiendum id="0">H ( L )</definiendum>
				<definiendum id="1">gain ratio corrects</definiendum>
			</definition>
			<definition id="6">
				<sentence>revised ratio = I ( C ; L ) H ( L ) H ( C ) The number and composition of the clusters , and hence H ( L ) , changes for each cut of the cluster tree , but the entropy of the categories , H ( C ) , is constant .</sentence>
				<definiendum id="0">H ( C</definiendum>
				<definiens id="0">The number and composition of the clusters</definiens>
			</definition>
</paper>

		<paper id="1425">
			<definition id="0">
				<sentence>The verbalization of NL air quality information in German and French is an additional service reducing the need to look up multiple heterogeneous data .</sentence>
				<definiendum id="0">French</definiendum>
				<definiens id="0">an additional service reducing the need to look up multiple heterogeneous data</definiens>
			</definition>
			<definition id="1">
				<sentence>They are stored as canned texts in the TEMSIS database : Not all choices are needed in every case , and the TEMSIS navigator•restricts the combination of choices to the meaningful ones .</sentence>
				<definiendum id="0">TEMSIS</definiendum>
				<definiens id="0">navigator•restricts the combination of choices to the meaningful ones</definiens>
			</definition>
			<definition id="2">
				<sentence>Assertion statements consist of a top level predicate that represents the assertion 's type ( e.g. threshold-exceeding ) and encapsulates the entire meaning of the associated assertion , except to attached specifications and domain data , to make local parameters and data dependencies explicit .</sentence>
				<definiendum id="0">Assertion statements</definiendum>
				<definiens id="0">consist of a top level predicate that represents the assertion 's type ( e.g. threshold-exceeding ) and encapsulates the entire meaning of the associated assertion , except to attached specifications and domain data , to make local parameters and data dependencies explicit</definiens>
			</definition>
			<definition id="3">
				<sentence>An example for adding a simple structure to an IR expression is the insertion of a marker indicating a strong correspondence between adjacent assertions , which 243 ( defproduction threshold-exceeding `` WUOI '' ( : PRECOND ( : CAT DECL : TEST ( ( coop-eq 'threshold-exceeding ) ( threshold-value-p ) ) ) : ACTIONS ( : TEMPLATE ( : OPTRULE PPtime ( get-param 'time ) ) ( : OPTRULE SITEV ( get-param ' site ) ) ( : RULE THTYPE ( self ) ) `` ( :0PTRULE POLL ( get-param 'pollutant ) ) ( :0PTRULE DUR ( get-param 'duration ) ) `` ( `` ( : RULE VAL ( get-param 'thresh01d-value ) ) ( : OPTRULE LAW ( get-param 'law-name ) ) `` ) `` ( : RULE EXCEEDS ( get-param 'exceeds ) ) `` . ``</sentence>
				<definiendum id="0">RULE VAL</definiendum>
				<definiendum id="1">RULE EXCEEDS</definiendum>
				<definiens id="0">defproduction threshold-exceeding `` WUOI '' ( : PRECOND ( : CAT DECL : TEST ( ( coop-eq 'threshold-exceeding ) ( threshold-value-p ) ) ) : ACTIONS ( : TEMPLATE ( : OPTRULE PPtime ( get-param 'time ) ) ( : OPTRULE SITEV ( get-param ' site ) ) ( : RULE THTYPE ( self ) ) `` ( :0PTRULE POLL ( get-param 'pollutant ) ) ( :0PTRULE DUR ( get-param 'duration ) )</definiens>
			</definition>
			<definition id="4">
				<sentence>Realization techniques of different granularity ( canned text , templates , context-free grammars ) allow the grammar writer to model general , linguistic knowledge as well as more specific task and domain-oriented wordings .</sentence>
				<definiendum id="0">Realization techniques</definiendum>
				<definiendum id="1">context-free grammars</definiendum>
				<definiens id="0">linguistic knowledge as well as more specific task and domain-oriented wordings</definiens>
			</definition>
</paper>

		<paper id="1404">
			<definition id="0">
				<sentence>A museum curator seeks to achieve general educational goals through the description of a set of carefully selected objects .</sentence>
				<definiendum id="0">museum curator</definiendum>
				<definiens id="0">seeks to achieve general educational goals through the description of a set of carefully selected objects</definiens>
			</definition>
			<definition id="1">
				<sentence>The content potential is an intermed .</sentence>
				<definiendum id="0">content potential</definiendum>
				<definiens id="0">an intermed</definiens>
			</definition>
			<definition id="2">
				<sentence>Entities are the participants in facts ( things and qualities in terms of Penman 's Upper Model ) .</sentence>
				<definiendum id="0">Entities</definiendum>
			</definition>
</paper>

		<paper id="0718">
			<definition id="0">
				<sentence>WordNet ( Miller et al. , 1990 ) has been successfully applied in many human language related applications , such as word sense disambiguation , information retrieval , and text categorization ; yet generation is among the fields in which the application of WordNet has rarely been explored .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">word sense disambiguation , information retrieval , and text categorization</definiens>
			</definition>
			<definition id="1">
				<sentence>Lexicalization maps the semantic concepts to be conveyed to appropriate words .</sentence>
				<definiendum id="0">Lexicalization maps</definiendum>
			</definition>
			<definition id="2">
				<sentence>WordNet is by far the richest and largest database among all resources that are indexed by concepts .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">by far the richest and largest database among all resources that are indexed by concepts</definiens>
			</definition>
			<definition id="3">
				<sentence>Figure 1 shows two example pruning operations : ( A ) is a general case , and ( B ) is the case involving ancestor syuset .</sentence>
				<definiendum id="0">B )</definiendum>
				<definiens id="0">the case involving ancestor syuset</definiens>
			</definition>
			<definition id="4">
				<sentence>133 In this paper , we demonstrate that WordNet is a valuable resource for generation : it can produce large amount of paraphrases , provide semantic net for lexicalization , and can be used for building domain ontologies .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">a valuable resource for generation : it can produce large amount of paraphrases , provide semantic net for lexicalization</definiens>
			</definition>
			<definition id="5">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="0510">
			<definition id="0">
				<sentence>Our grammar checker , which performs a complete linguistic analysis of all sentences , is based on a dependency grmmnar .</sentence>
				<definiendum id="0">grammar checker</definiendum>
				<definiens id="0">performs a complete linguistic analysis of all sentences , is based on a dependency grmmnar</definiens>
			</definition>
			<definition id="1">
				<sentence>The English lexicon consists of 65,000 English root words .</sentence>
				<definiendum id="0">English lexicon</definiendum>
			</definition>
			<definition id="2">
				<sentence>The dangling preposition in ( 7 ) continues to head a SSyntRel that subordinates its displaced complement whom labelled a Prepositional SSyntRel , as in ( 7a ) .</sentence>
				<definiendum id="0">SSyntRel</definiendum>
				<definiendum id="1">Prepositional SSyntRel</definiendum>
				<definiens id="0">subordinates its displaced complement whom labelled a</definiens>
			</definition>
</paper>

		<paper id="0215">
			<definition id="0">
				<sentence>The most convenient tactile printing technology is the TIGER Tactile Graphics EmbosseR ( TIGER ) developed in the SAP group .</sentence>
				<definiendum id="0">most convenient tactile printing technology</definiendum>
			</definition>
			<definition id="1">
				<sentence>Virtual Reality Modeling Language ( VRML ) is a high-level well-structured language permitting very flexible modeling of three dimensional time-dependent interactive graphics .</sentence>
				<definiendum id="0">Virtual Reality Modeling Language ( VRML )</definiendum>
				<definiens id="0">a high-level well-structured language permitting very flexible modeling of three dimensional time-dependent interactive graphics</definiens>
			</definition>
</paper>

		<paper id="1213">
			<definition id="0">
				<sentence>A lexical chain ( Morris and Hirst , 1991 ) is a sequence of semantically related words in a text .</sentence>
				<definiendum id="0">lexical chain</definiendum>
				<definiens id="0">a sequence of semantically related words in a text</definiens>
			</definition>
			<definition id="1">
				<sentence>The density of chain c in paragraph p , dc , p , is defined as : dc , p ~ Wc , p wp Green 102 Automatically generating hypertext I I I I I I I I I I I I I II II II II / / / l / / / / / / / / / / / II l Table I : Word Syn working ( 5 ) 40755 ground ( I ) 58279 field ( 1 ) 57992 antarctica ( I ) 58519 michigan ( I ) 57513 feed ( I ) 53429 chain ( I ) 57822 hazard ( 1 ) 77281 risk ( 1 ) 77281 young ( 2 ) 24623 need ( 1 ) 58548 parent ( 7 ) 62334 kid ( 3 ) 60256 child ( 1 ) 60256 baby ( 1 ) 59820 wife ( 1 ) 63852 adult ( I ) 59073 traveller ( 3 ) 59140 substitute ( 1 ) 63327 backup ( 1 ) 63327 computer ( l ) 60118 Some lexical chains from the virtual parenting article .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">dc , p ~ Wc , p wp Green 102 Automatically generating hypertext I I I I I I I I</definiens>
			</definition>
			<definition id="2">
				<sentence>More advice 3 from advertisers t : Business s travellers I can dine with their kids t by speakerL phone or `` tuck them in '' by cordless phone z2 .</sentence>
				<definiendum id="0">Business</definiendum>
				<definiens id="0">s travellers I can dine with their kids t by speakerL phone or `` tuck them in '' by cordless phone z2</definiens>
			</definition>
			<definition id="3">
				<sentence>where wc , p is the number of words from chain c that appear in paragraph p and w v is the number of content words ( i.e. , words that are not stop words ) in p. For example , if we consider paragraph two of our sample article , we see that there are 9 words from chain 1 .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">w v</definiendum>
				<definiens id="0">the number of content words ( i.e. , words that are not stop words ) in p. For example</definiens>
			</definition>
			<definition id="4">
				<sentence>( log ( N lnj ) ) 2 where sfik is the frequency of synset k in document i , N is the size of the document collection , n , is the number of documents in the collection that contain synset k , and s is the number of synsets in all documents .</sentence>
				<definiendum id="0">sfik</definiendum>
				<definiendum id="1">N</definiendum>
			</definition>
</paper>

		<paper id="1220">
			<definition id="0">
				<sentence>Firstly , Genetic Programming ( Koza , 1992 ; Koza 1994 ) is a way of evolving a program that meets some objective criteria , and is closely related to Genetic Algorithms ( Goldberg , 1989 ) .</sentence>
				<definiendum id="0">Genetic Programming</definiendum>
				<definiens id="0">a way of evolving a program that meets some objective criteria , and is closely related to Genetic Algorithms</definiens>
			</definition>
			<definition id="1">
				<sentence>The Genetic Programming algorithm involves making a population of such expression trees ( randomly at first ) , evaluating the fitness of each of the trees ( done with some evaluation function defined by the application ) , and then creating a new population by crossover or mutation of the fit individuals ( crossover would involve choosing a random node and the subtree which starts with it , from tree A , and choosing a random node and the subtree which starts from it , from tree B , and interchanging these two subtrees , to create two new tree expressions ) .</sentence>
				<definiendum id="0">Genetic Programming algorithm</definiendum>
			</definition>
			<definition id="2">
				<sentence>This simulated search engine will order the importance of documents with the following rules : *Rf ( A ) = freq ( A ) /freq ( most frequent word ) -Rf ( A and B ) = sqrt ( R_f ( A ) *Rf ( B ) ) *R.f ( A or B ) = ( Rf ( A ) + Rf ( B ) ) /2 Steele and Powers *RI ( A ) - ( document length first occurrence ) / document length .</sentence>
				<definiendum id="0">*Rf</definiendum>
				<definiendum id="1">-Rf</definiendum>
				<definiendum id="2">R_f</definiendum>
				<definiens id="0">A ) *Rf ( B ) ) *R.f ( A or B ) = ( Rf ( A ) + Rf ( B ) ) /2 Steele and Powers *RI ( A ) - ( document length first occurrence</definiens>
			</definition>
			<definition id="3">
				<sentence>RI ( A and B ) = sqrt ( Rl ( a ) *Rl ( B ) ) .</sentence>
				<definiendum id="0">RI</definiendum>
				<definiendum id="1">Rl</definiendum>
				<definiens id="0">a ) *Rl ( B ) )</definiens>
			</definition>
			<definition id="4">
				<sentence>RI ( A or B ) = ( RI ( A ) + RI ( B ) ) /2 *R = Rf + bPd Where Rf is the relevance based on frequency , RI is the relevance based on location , b is some weight and R is the overall relevance value .</sentence>
				<definiendum id="0">RI</definiendum>
				<definiendum id="1">RI</definiendum>
				<definiendum id="2">R</definiendum>
				<definiens id="0">A or B ) = ( RI ( A ) + RI ( B ) ) /2 *R = Rf + bPd Where Rf is the relevance based on frequency</definiens>
			</definition>
			<definition id="5">
				<sentence>Introduction to WordNet : An On-line Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="0506">
			<definition id="0">
				<sentence>1986 ) , works with dependency syntax and includes the topicfocus articulation into the ( underlying , tectogrammatical ) syntactic representations of sentences , using the left-to-right order of the nodes of the dependency tree for that purpose .</sentence>
				<definiendum id="0">articulation</definiendum>
				<definiens id="0">works with dependency syntax and includes the topicfocus</definiens>
			</definition>
			<definition id="1">
				<sentence>( 1986 ) , there are good reasons ( concerning the semantic relevance as well as syntactic constraints of the topic-focus articulation , see below ) to distinguish three layers of the order of the occurrences of lexical ( autosemantic ) items in the sentence : ( i ) the surface word order , which can be understood as the order of items on the level of morphemics ; on that level the sentence representation has the shape of a string of word-forms ( rather than that of a tree ) ; ( ii ) the communicative dynamism ( underlying word order ) , i.e. the order of nodes in an underlying ( tectogrammatical ) representation ( TR ) , which has the shape of a dependency tree ; the topic of the sentence is characterized as consisting of items which are less dynamic than those belonging to its focus ; thus , if A belongs to the topic in all TRs of a given sentence S and B belongs to the focus in some of the TRs of S , then A is less dynamic than B ; ( iii ) the systemic ordering of valency slots ( kinds of dependency relation ) , which is specified in the valency frames ( included in the lexical entries ) , see Sgall et al. 0986 ) , Sgall ( in press ) .</sentence>
				<definiendum id="0">communicative dynamism</definiendum>
				<definiendum id="1">TR )</definiendum>
				<definiens id="0">concerning the semantic relevance as well as syntactic constraints of the topic-focus articulation</definiens>
				<definiens id="1">autosemantic ) items in the sentence : ( i ) the surface word order</definiens>
				<definiens id="2">kinds of dependency relation</definiens>
			</definition>
			<definition id="2">
				<sentence>Let us assume that the preferred reading of ( 1 ) see Fig.1 can be paraphrased as `` As for the last week in my sister 's life , ( she ) was ( then ) visited by a painter in Paris '' ; the order of the valency slots ( the communicative dynamism ) of the ( tectogrammatical ) syntactic tree representing this reading is , in ascending order , Patient ( Objective ) Temporal-when Actor Locative ( with the Patient being the least dynamic item , i.e. the topic proper , and the verb being less dynamic than the Actor and more dynamic than the temporal adverbial ) .</sentence>
				<definiendum id="0">tectogrammatical</definiendum>
				<definiens id="0">with the Patient being the least dynamic item</definiens>
			</definition>
			<definition id="3">
				<sentence>The lexical entry contains the following parts : ( a ) the lexical meaning , which itself has its internal structure ; in the cases of lexical ambiguity there are several representations , i.e. several lexical units ; ( b ) the slots for values of relevant grammatical categories , i.e. of grammatemes ; while e.g. gender with nouns is specified here , other grammatemes ( number and definiteness with nouns , or tense , aspect , modalities with verbs , degrees of comparison with ( c ) 54 adjectives ) get their values only for individual occurrences of the word forms in discourses ; the valency frame : optional and obligatory dependents are ordered in accordance with systemic ordering ; arguments or 'inner participants ' ( occurring at most once with a head node ) and obligatory adjuncts are indicated ( e.g. arrive at a place , behave somehow , last how long , ... ) ; optional adjuncts may be indicated by lists concerning individual word classes ; surface deletability is indicated ( e.g. Directional.2 with to arrive is deletable , Objective with to meet is not ( We met there is a case of reciprocity , rather than of deletion ) ; further data concern an optional or obligatory controller ( e.g. Actor is an obligatory controller with to try , an optional one with to decide ; Addressee is an optional controller with to advise , to forbid ) , the dependent 's ability to occupy certain syntactic positions ( e.g. of Subject with Passivization , of a whelement ) or to constitute barriers for movement , and subcategorization conditions .</sentence>
				<definiendum id="0">lexical meaning</definiendum>
				<definiendum id="1">other grammatemes</definiendum>
				<definiendum id="2">Addressee</definiendum>
				<definiens id="0">optional and obligatory dependents are ordered in accordance with systemic ordering</definiens>
				<definiens id="1">data concern an optional or obligatory controller ( e.g. Actor is an obligatory controller with to try</definiens>
				<definiens id="2">an optional controller with to advise , to forbid ) , the dependent 's ability to occupy certain syntactic positions ( e.g. of Subject with Passivization , of a whelement ) or to constitute barriers for movement , and subcategorization conditions</definiens>
			</definition>
			<definition id="4">
				<sentence>NOTE : If the chosen complementation value is an inner participant , it is deleted in the frame of n ( as saturated ) ; `` from the left end '' means that optional complementations can be skipped and deleted in the frame of n. ( iii ) if the frame of n contains no complementation , then the mother node of n is considered as node n ; if no mother node is present ; the procedure ( generating the tree from the top down and from the left to the right ) is finished ; ( iv ) only representations containing a focus are accepted , i.e. only those whose branch going from the root to the rightmost daughter of the rightmost daughter of ... of the root includes a non-bound node .</sentence>
				<definiendum id="0">NOTE</definiendum>
				<definiens id="0">those whose branch going from the root to the rightmost daughter of the rightmost daughter of ... of the root includes a non-bound node</definiens>
			</definition>
			<definition id="5">
				<sentence>Loc ) ( U y.Temp W ) where X and Y are well parenthesized strings , x stands for a noun , pronoun or verb , y stands for a noun , adverb or verb ( again , with their dependents ) , the superscript F denotes the placement of the intonation center of the sentence , Z , W , U , V are ( possibly empty ) strings of well parenthesized items ( dependents of x and y ) , X ( U y.Temp W ) Y ( Z x.Loc ) is a 55 I I I I I I I I I I I I I I I i I i well formed TR ( according to Sect .</sentence>
				<definiendum id="0">Loc )</definiendum>
				<definiendum id="1">X</definiendum>
				<definiendum id="2">superscript F</definiendum>
				<definiendum id="3">X</definiendum>
				<definiens id="0">with their dependents ) , the</definiens>
				<definiens id="1">the placement of the intonation center of the sentence , Z , W , U , V are ( possibly empty ) strings of well parenthesized items ( dependents of x and y )</definiens>
			</definition>
			<definition id="6">
				<sentence>Def is a noun , Num the value of Number ( Sg or P1 ) and Def that of Definiteness ( + or- ) , then ( i ) rewrite + ( - ) into Artdf ( Arfin ) and place this symbol to the left of the nodes subordinated to node x ( including x itself ) ; recall that 'subordinated ' is the transitive closure of 'depending ' ; Artdf will be transduced into the phonemic and phonetic ( or graphemic ) shapes of the , and Artin into a , an , or ( with plural ) into zero , ( ii ) rewrite PI ( Sg ) into -s ( 0 ) ( this certainly requires to take into account also several specific sets of exceptions before Rule M2 is applied , cf. forms like mice , oxen , loaves ) ; -s will be then changed into the surface shape of the Plural ( or 3rd Person Singular ) ending by the subsequent components of the procedure .</sentence>
				<definiendum id="0">Def</definiendum>
				<definiendum id="1">rewrite PI</definiendum>
				<definiendum id="2">Plural</definiendum>
				<definiens id="0">a noun</definiens>
				<definiens id="1">+ or- ) , then ( i ) rewrite + ( - ) into Artdf ( Arfin ) and place this symbol to the left of the nodes subordinated to node x ( including x itself ) ; recall that 'subordinated ' is the transitive closure of 'depending '</definiens>
				<definiens id="2">specific sets of exceptions before Rule M2 is applied , cf. forms like mice , oxen , loaves</definiens>
			</definition>
			<definition id="7">
				<sentence>Def W ) - , ( Art Z x.End W ) where x stands for a noun or a pronoun , Z and W are ( possibly empty ) slrings of well parenthesized items ( dependents of x ) , Art is Artdef ( Artin ) if Defis + ( - ) , End is -s ( 0 ) ifNum is PI ( Sg ) .</sentence>
				<definiendum id="0">x</definiendum>
				<definiendum id="1">Art</definiendum>
				<definiendum id="2">End</definiendum>
				<definiens id="0">possibly empty ) slrings of well parenthesized items ( dependents of x )</definiens>
			</definition>
</paper>

		<paper id="0717">
			<definition id="0">
				<sentence>Winnow updates the weight on the links in a multiplicative fashion .</sentence>
				<definiendum id="0">Winnow</definiendum>
				<definiens id="0">updates the weight on the links in a multiplicative fashion</definiens>
			</definition>
			<definition id="1">
				<sentence>Systematic polysemy is the phenomena of word senses that are systematically related and therefore predictable over classes of lexical items .</sentence>
				<definiendum id="0">Systematic polysemy</definiendum>
				<definiens id="0">the phenomena of word senses that are systematically related</definiens>
			</definition>
			<definition id="2">
				<sentence>The finer synset granularity means that a synset carries less information ; thus , the CL classes add richer disjunctions than WN synsets do .</sentence>
				<definiendum id="0">finer synset granularity</definiendum>
				<definiens id="0">add richer disjunctions than WN synsets do</definiens>
			</definition>
</paper>

		<paper id="0703">
			<definition id="0">
				<sentence>A metric is introduced and used to measure the semantic density and to rank all possible combinations of the senses of two words .</sentence>
				<definiendum id="0">metric</definiendum>
				<definiens id="0">introduced and used to measure the semantic density and to rank all possible combinations of the senses of two words</definiens>
			</definition>
			<definition id="1">
				<sentence>Word Sense Disambiguation ( WSD ) is an open problem in Natural Language Processing .</sentence>
				<definiendum id="0">Word Sense Disambiguation ( WSD )</definiendum>
			</definition>
			<definition id="2">
				<sentence>WSD methods can be broadly classified into three types : vided by machine readable dictionaries ( Cowie et a1.1992 ) , ( Miller et a1.1994 ) , ( Agirre and Rigau , 1995 ) , ( Li et a1.1995 ) , ( McRoy , 1992 ) ; ing on a corpus that has already been semantically disambiguated ( supervised training methods ) ( Gale , Church et al. , 1992 ) , ( Ng and Lee , 1996 } ; raw corpora ( unsupervised training methods ) ( Yarowsky 1995 ) ( Resnik 1997 ) .</sentence>
				<definiendum id="0">WSD methods</definiendum>
				<definiens id="0">vided by machine readable dictionaries</definiens>
				<definiens id="1">a corpus that has already been semantically disambiguated ( supervised training methods</definiens>
			</definition>
			<definition id="3">
				<sentence>One approach to WSD is to determine the conceptual distance between words , that is to measure the semantic closeness of the words within a semantic network .</sentence>
				<definiendum id="0">WSD</definiendum>
				<definiens id="0">to determine the conceptual distance between words , that is to measure the semantic closeness of the words within a semantic network</definiens>
			</definition>
			<definition id="4">
				<sentence>A metric is introduced in this sense which when applied to all possible combinations of the senses of two or more words it ranks them .</sentence>
				<definiendum id="0">metric</definiendum>
			</definition>
			<definition id="5">
				<sentence>V r~l ) , N `` ( ~ ) ... .. N `` ~ k '' ) ) where , V l , N `` ~ ... .. N '' represent the different senses of : V , and , V i ( ' ) represents the synonym number s of the sense N i of the noun N as defined in WordNet .</sentence>
				<definiendum id="0">, V i</definiendum>
				<definiens id="0">the different senses of : V , and</definiens>
			</definition>
			<definition id="6">
				<sentence>Conceptual Density Metric For determining the conceptual density between a noun ni and a verb vj , the algorithm considers : • the list of nouns sv~ associated with the glosses of the verbs within the hierarchy determined by hi : ( svk , w~ ) , where : hj is the hypernym of vj w~ is the level in this hierarchy • the list of nouns snt within the class of ni : ( snt ) The common words between these two lists ( svk , wk ) and ( snt ) will produce a list of common concepts with the associated weights cdij &lt; w~ &gt; .</sentence>
				<definiendum id="0">Conceptual Density Metric For</definiendum>
				<definiendum id="1">hj</definiendum>
				<definiendum id="2">vj w~</definiendum>
				<definiens id="0">determining the conceptual density between a noun ni and a verb vj , the algorithm considers : • the list of nouns sv~ associated with the glosses of the verbs within the hierarchy determined by hi : ( svk</definiens>
				<definiens id="1">the level in this hierarchy • the list of nouns snt within the class</definiens>
			</definition>
			<definition id="7">
				<sentence>The conceptual density between rli and vj is given by the formula : Icd. , l ( 1 ) C'/j = log ( desci ) where : • Icdijl is the number of common concepts between the hierarchies of ni and uj • w~ are the weights associated with the nouns from the noun-context of the verb vj • desci is the total number of words within the hierarchy of noun nl As the nouns with a big hierarchy tend to indicate a big value for Icdijl , the weighted sum of common concepts has to be normalized in respect with the dimension of the noun hierarchy .</sentence>
				<definiendum id="0">Icdijl</definiendum>
				<definiendum id="1">desci</definiendum>
				<definiens id="0">the number of common concepts between the hierarchies of ni</definiens>
			</definition>
			<definition id="8">
				<sentence>When nodes correspond to words , the possible values for each node are senseO through senseN , where N is the number of WordNet synsets representing senses of the target word .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of WordNet synsets representing senses of the target word</definiens>
			</definition>
			<definition id="9">
				<sentence>SenseO represents the composite of all other meanings , i.e. , of all meanings that are not represented by WordNet synsets .</sentence>
				<definiendum id="0">SenseO</definiendum>
				<definiens id="0">the composite of all other meanings</definiens>
			</definition>
			<definition id="10">
				<sentence>Suppose Hyper is a synset that is a hypernym of synsets Hypol ... Hypok .</sentence>
				<definiendum id="0">Hyper</definiendum>
				<definiens id="0">a synset that is a hypernym of synsets Hypol ... Hypok</definiens>
			</definition>
			<definition id="11">
				<sentence>The extension to the case where probabilities are associated with each input is relatively straightforward : chitd= - , ( ( - , vl ) ^ -- -A ( -- vn ) ) P ( childlV1 = vl , ... , V~ = v , , ) = When extending to the general case ; the relationship between the value of the child node and the values of its parent nodes is not necessarily defined by a 27 I i I 1 I i I I I i I I i I I I I I I truth function .</sentence>
				<definiendum id="0">probabilities</definiendum>
				<definiens id="0">chitd= - , ( ( - , vl ) ^ -- -A ( -- vn ) ) P ( childlV1 = vl , ... , V~ = v , , ) = When extending to the general case ; the relationship between the value of the child node and the values of its parent nodes is not necessarily defined by a 27 I i I 1 I i I I I i I I i I I I I I I truth function</definiens>
			</definition>
			<definition id="12">
				<sentence>Eizirik et al. ( 1993 ) also describe a Bayesian network model for word-sense disambiguation , which includes syntactic disambiguation as well as lexical information .</sentence>
				<definiendum id="0">word-sense disambiguation</definiendum>
				<definiens id="0">includes syntactic disambiguation as well as lexical information</definiens>
			</definition>
</paper>

		<paper id="0508">
			<definition id="0">
				<sentence>The projectivity constraint states that ifxi Rk xj , for any i , j , k and i &gt; j , there exists no Rp such that Xm Rp x , for any m and n such that m &gt; j and i &lt; n &lt; j or m &lt; i and i &lt; n &lt; j. ( The projectivity constraint prohibits `` crossing '' direct governments. ) It is convenient to study governed strings using two-dimensional government maps ( GM ) . A GM ( S , R ) is a matrix whose rows represent the nodes of a string S and the columns represent the relations of R. The ordering of the rows corresponds to the ordering of the nodes in S , while the ordering of the columns ( relations ) is arbitrary. The direct governor of a node is marked at the intersection of the governing relation and the governed node. For example , if R = { Rb R2 , R3 , R4 } and S = { XL X2 , X3 , X4 , XS } Table 1 shows the GM ( S , R ) of the governed string in Figure 1. Formally , GM ( S , R ) c S x R x S. ( Henceforth we often simply write GM rather than GM ( S , R ) . ) Node/Relation RI R2 R3 R4 XI X2 X4 X5 X3 XI x3 X4 Table I : The GM of Figure 2 69 Two GM 's are called equidimensional if they represent identical strings and identical sets of relations and the relations occupy the same columns in both maps. We borrow a few operations from the set theory. A direct government xi R~x i belongs to a GM ( marked ~ ) iffxi and xj are nodes in the GM and xi Rk x i is marked in GM. A government map GMI includes another equidimensional map GM2 ( GM2 c_ GM1 ) if all direct governors in GM2 are also in GMI , GMI properly includes an equidimensional map GM2 ( GM2 c GMI ) if GM2 c_ GM1 and GMI ¢ : GM2. Any given two equidimensional maps are identical , if they include one another. We also admit unions ( u ) and intersections ( c~ ) of two equidimensional GM 's in the obvious manner. A GM may exhibit just those direct governers which constitute a totally governed string , it may show any subset of the direct governors of the nodes , or it may exhibit all possible direct governors of the nodes. We say that a resolved GM ( GM ' ) is a map that shows only the direct governors of a totally governed string. A complete and unresolved GM ( GM c~ ) is a map that indicates all possible direct governors of the nodes. A ( partially ) unresolved GM ( GM u ) indicates some but not necessarily all direct governors of the nodes. For each GM , GM ' _c GM cu and GM '' c GM cu. Let GMr ( S , R ) and GM~ '' ( S , R ) represent a resolved and the complete and unresolved equidimensional maps , respectively. If S is unambiguous , GM r = GM % If S is locally ambiguous but globally unambiguous , there exists only one GM ' and the GM r c GM ~'~. IfS is globally ambiguous , there exists more than one different GM r and for each GM ' c GM ~. Finally , if there exists no GM ' such that GM ~ _c GM ¢ '' , we say that the string is ungrammatical ( with respect to R ) . Table 2 shows the GM ~ of a locally ambiguous but globally unambiguous string. Node x4 can not be directly governed by both x3 and xs , hence the string is locally ambiguous. Only the former choice results in a totally governed string ( Figure 1 and Table 1 ) . 70 Node/Relation Ri R2 R3 R4 Xl x~ x3 x4 x5 x3 XI X3 X5 X4 Table 2 : Locally ambiguous but globally unambiguous GM ~ '' Table 3 shows the GM c° of a both locally and globally ambiguous string. Figure 1 shows one and Figure 2 shows another governed string corresponding to this GM ¢~. If a string is ambiguous , at least one row has multiple entries in the GM % Node.Relation Ri 17,2 R3 R4 Xi X2 X5 X3 X ! X1 X3 X4 Table 3 : Locally and globally ambiguous GM ¢ '' R2 X3 X4 XS Figure 2 : Another governed string An GM r carries all necessary information about the structure of a governed string. If the process of uncovering the structure of a string is called parsing , a parsing process equals to the finding of the GM r for a given string ( or all resolved maps if the string is globally ambiguous ) , and the found GM ) represents the parse tree of the string. The nodes and relations in a GM generate an abstract search space for governed strings. Therefore , parsing can be viewed as a search for the GM ~ in the space genereted by the string of nodes and the set of available relations. The process begins with an empty map and makes progresssively more and more direct governors known. The process should end with a GM u such that GM ' c_ GM ~. If G1Vf c GM ~ there remains a residual problem of finding GM `` , GM '' c GM '' , such that GM ' '' = GM ~. Let us assume that for each globally ambiguous string there is single fight parse tree , called the preferred tree. We call a parsing process deterministic , if it begins with an empty map and marks direct governors in the map in such an order that when the process ends GM u = GM r , where GM '' is the explored map and GM r represents the parse tree or the preferred parse tree if the string is globally ambiguous. Theorem 1 : Unambiguous strings can be parsed deterministically. A proof is trivial. Any algorithm which finds all possible direct governors of the nodes by iterating through all the relations and all the nodes creates the GM c '' by definition. And with unambiguous strings , GM r = GM ~u. The following OS algorithm ( for Open Search ) , among others , parses unambiguous strings deterministically.Let nR denote the number of available relations and ns stand for the number of nodes in an input string. OS algorithm : a GM in random order. in the GM in their precedence order. row open. ( i=l ... .. n~ ) and each open xj ( j -- i-l , i-2 ... .. 1 , i+ 1 , i+2 ... .. n~ ) for xi R xj , where R is the relation assigned in the column k. Mark each found direct governor xi in the GMJj , k\ ] and mark the rowj closed. Let us call the number of open nodes ( plus 1 ) between a direct governor and the governed node at the moment of a test the distance of the relation test. Distance lt.vpothesis : It is possible to order linguistic dependency relations as columns in a GM in such a way that the maximum distance remains within a fixed boundary when the OS algorithm parses natural language sentences. ( We return to this hypothesis later on in this paper. ) Theorem 2 : If the distance hypothesis holds , unambiguous strings can be parsed in linear time. Let us assume that the distance hypothesis holds and let d stand for the maximum distance. The iteration statement in the OS algorithm is then limited as follows : . , , ( j=i-l , j-2 , ... , i-d , i+l , i+2 , ... , i+d ) ; Let C denote the most expensive relation test. The OS algorithm consumes in the worst case at most C * nR * nN * 2 * d = O ( nN ) . Next we show that even ambiguous natural language sentences can be parsed deterministically in linear time if a certain additional condition holds. Best-First Conjecture : It is possible to order the linguistic relations as columns in a GM in such a way that ( without violating the Distance Hypothesis ) the OS algorithm produces for natural language sentences the right or the preferred GM r most of the time. Due to its heuristic flavor , we call the thus modified OS algorithm the BF algorithm. B F algorithm : columns in a GM in such an order that both the Best-First Conjecture and the Distance Hypothesis hold. The enforcement of the Best-First Conjecture brings a heuristic component in the algorithm , and the algorithm does not explore the search space fully anymore. Once the algorithm chooses a local governor over the alternative 71 I I I I i I ! ! I i I I I I I I ones for a word , the alternative local governors will be rejected forever. Therefore , there is no guarantee that the right parse trees will be always produced , hence the phrase `` most of the time '' . Claim : The BF algorithm parses natural language sentences detenninistically in linear computational cost of the most expensive relation test ) . The theoretical model assumes that sentences are parsed in one pass. The DCParser divides sentences into segments , using conjunctions and delimiters as separators. The BF-algorithm is time so that the right or the preferred parse trees , . applied to each segment separately , and the final are producedmostofthetime , phase unites the structures built in those segments applying the algorithm again. This claim is an unprecise empirical statement Decomposition greatly strengthens the Distance that can be supported only by empirical means. Hypothesis , but it does not alter the linearity That will be done next. proof , since the sum of linear elements is linear : From now on we assume that strings of nodes are natural language sentences and discuss a fully implemented parser ( DCParser ) that parses Finnish sentences. The DCParser differs from the simple theoretical model described above , but , as v~ll be shown below , the differences do not alter the theory. The formal part introduced binary relations as context-free ordered pairs ( 1 ) . Dependency relations in the implemented parser use contexts. Formally , they could be expressed as contextsensitive ordered pairs as in ( 2 ) , but the DCParser uses different rule syntax as discussed in 2.5. ( 2 ) Ri = { &lt; \ [ cxl\ ] x\ [ cxr\ ] , icy~\ ] y\ [ cy , \ ] &gt; l x , y are morphosyntactic representations of the direct governor and the governed word form , cx~ , cxr , cyi , cy , are morpho-syntaetie representations of the left and the right contexts ofx and y , respectively , and x Riy } .</sentence>
				<definiendum id="0">R )</definiendum>
				<definiendum id="1">GM ''</definiendum>
				<definiendum id="2">GM r</definiendum>
				<definiendum id="3">R</definiendum>
				<definiens id="0">convenient to study governed strings using two-dimensional government maps ( GM ) . A GM ( S ,</definiens>
				<definiens id="1">a matrix whose rows represent the nodes of a string S and the columns represent the relations of R. The ordering of the rows corresponds to the ordering of the nodes in S , while the ordering of the columns ( relations ) is arbitrary. The direct governor of a node is marked at the intersection of the governing relation and the governed node. For example , if R = { Rb R2 , R3 , R4 } and S = { XL X2 , X3 , X4 , XS } Table 1 shows the GM ( S , R ) of the governed string in Figure 1. Formally , GM ( S , R ) c S x R x S. ( Henceforth we often simply write GM rather than GM ( S , R ) . ) Node/Relation RI R2 R3 R4 XI X2 X4 X5 X3 XI x3 X4 Table I : The GM of Figure 2 69 Two GM 's are called equidimensional if they represent identical strings and identical sets of relations and the relations occupy the same columns in both maps. We borrow a few operations from the set theory. A direct government xi R~x i belongs to a GM ( marked ~ ) iffxi and xj are nodes in the GM and xi Rk x i is marked in GM. A government map GMI includes another equidimensional map GM2 ( GM2 c_ GM1 ) if all direct governors in GM2 are also in GMI , GMI properly includes an equidimensional map GM2 ( GM2 c GMI ) if GM2 c_ GM1 and GMI ¢ : GM2. Any given two equidimensional maps are identical , if they include one another. We also admit unions ( u ) and intersections ( c~ ) of two equidimensional GM 's in the obvious manner. A GM may exhibit just those direct governers which constitute a totally governed string , it may show any subset of the direct governors of the nodes , or it may exhibit all possible direct governors of the nodes. We say that a resolved GM ( GM ' ) is a map that shows only the direct governors of a totally governed string. A complete and unresolved GM ( GM c~ ) is a map that indicates all possible direct governors of the nodes. A ( partially ) unresolved GM ( GM u ) indicates some but not necessarily all direct governors of the nodes. For each GM , GM ' _c GM cu and GM '' c GM cu. Let GMr ( S , R ) and GM~ '' ( S , R ) represent a resolved and the complete and unresolved equidimensional maps , respectively. If S is unambiguous , GM r = GM % If S is locally ambiguous but globally unambiguous</definiens>
				<definiens id="2">a locally ambiguous but globally unambiguous string. Node x4 can not be directly governed by both x3 and xs , hence the string is locally ambiguous. Only the former choice results in a totally governed string ( Figure 1 and Table 1 ) . 70 Node/Relation Ri R2 R3 R4 Xl x~ x3 x4 x5 x3 XI X3 X5</definiens>
				<definiens id="3">Another governed string An GM r carries all necessary information about the structure of a governed string. If the process of uncovering the structure of a string is called parsing , a parsing process equals to the finding of the GM r for a given string ( or all resolved maps if the string is globally ambiguous ) , and the found GM ) represents the parse tree of the string. The nodes and relations in a GM generate an abstract search space for governed strings. Therefore , parsing can be viewed as a search for the GM ~ in the space genereted by the string of nodes and the set of available relations. The process begins with an empty map and makes progresssively more and more direct governors known. The process should end with a GM u such that GM ' c_ GM ~. If G1Vf c GM ~ there remains a residual problem of finding GM `` , GM '' c GM '' , such that GM ' ''</definiens>
				<definiens id="4">fight parse tree , called the preferred tree. We call a parsing process deterministic , if it begins with an empty map and marks direct governors in the map in such an order that when the process ends GM u = GM r , where</definiens>
				<definiens id="5">Unambiguous strings can be parsed deterministically. A proof is trivial. Any algorithm which finds all possible direct governors of the nodes by iterating through all the relations and all the nodes creates the GM c '' by definition. And with unambiguous strings , GM r = GM ~u. The following OS algorithm ( for Open Search ) , among others , parses unambiguous strings deterministically.Let nR denote the number of available relations and ns stand for the number of nodes in an input string. OS algorithm : a GM in random order. in the GM in their precedence order. row open. ( i=l ... .. n~ ) and each open xj</definiens>
				<definiens id="6">the relation assigned in the column k. Mark each found direct governor xi in the GMJj , k\ ] and mark the rowj closed. Let us call the number of open nodes ( plus 1 ) between a direct governor and the governed node at the moment of a test the distance of the relation test. Distance lt.vpothesis : It is possible to order linguistic dependency relations as columns in a GM in such a way that the maximum distance remains within a fixed boundary when the OS algorithm parses natural language sentences. ( We return to this hypothesis later on in this paper. ) Theorem 2 : If the distance hypothesis holds , unambiguous strings can be parsed in linear time. Let us assume that the distance hypothesis holds and let d stand for the maximum distance. The iteration statement in the OS algorithm is then limited as follows : . , , ( j=i-l , j-2 , ... , i-d , i+l , i+2 , ... , i+d ) ; Let C denote the most expensive relation test. The OS algorithm consumes in the worst case at most C * nR * nN * 2 * d = O</definiens>
				<definiens id="7">ambiguous natural language sentences can be parsed deterministically in linear time if a certain additional condition holds. Best-First Conjecture : It is possible to order the linguistic relations as columns in a GM in such a way that ( without violating the Distance Hypothesis ) the OS algorithm produces for natural language sentences the right or the preferred GM r most of the time. Due to its heuristic flavor , we call the thus modified OS algorithm the BF algorithm. B F algorithm : columns in a GM in such an order that both the Best-First Conjecture and the Distance Hypothesis hold. The enforcement of the Best-First Conjecture brings a heuristic component in the algorithm</definiens>
				<definiens id="8">The BF algorithm parses natural language sentences detenninistically in linear computational cost of the most expensive relation test ) . The theoretical model assumes that sentences are parsed in one pass. The DCParser divides sentences into segments , using conjunctions and delimiters as separators. The BF-algorithm is time so that the right or the preferred parse trees , . applied to each segment separately , and the final are producedmostofthetime , phase unites the structures built in those segments applying the algorithm again. This claim is an unprecise empirical statement Decomposition greatly strengthens the Distance that can be supported only by empirical means. Hypothesis , but it does not alter the linearity That will be done next. proof , since the sum of linear elements is linear : From now on we assume that strings of nodes are natural language sentences and discuss a fully implemented parser ( DCParser ) that parses Finnish sentences. The DCParser differs from the simple theoretical model described above , but , as v~ll be shown below , the differences do not alter the theory. The formal part introduced binary relations as context-free ordered pairs ( 1 ) . Dependency relations in the implemented parser use contexts. Formally , they could be expressed as contextsensitive ordered pairs as in ( 2 ) , but the DCParser uses different rule syntax as discussed in 2.5. ( 2 ) Ri = { &lt; \ [ cxl\ ] x\ [ cxr\ ] , icy~\ ] y\ [ cy , \ ] &gt; l x , y are morphosyntactic representations of the direct governor and the governed word form , cx~ , cxr , cyi , cy , are morpho-syntaetie representations of the left and the right contexts ofx and y , respectively , and x Riy }</definiens>
			</definition>
			<definition id="1">
				<sentence>They only increase the value of the constant C ( the ( 3 ) O ( ni ) + O ( nj ) + ... + O ( nk ) = O ( nl~ ) , ni , nj , ... , m. &lt; __ns where n~ is the number of the words in a sentence .</sentence>
				<definiendum id="0">n~</definiendum>
				<definiens id="0">the number of the words in a sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>The DCParser has a separate morphological analysis phase which produces all possible morphotactic interpretations for the word forms of input sentences .</sentence>
				<definiendum id="0">DCParser</definiendum>
				<definiens id="0">a separate morphological analysis phase which produces all possible morphotactic interpretations for the word forms of input sentences</definiens>
			</definition>
</paper>

		<paper id="1229">
			<definition id="0">
				<sentence>Attention-sharing is the activity of paying one 's attention to someone else 's attentional target .</sentence>
				<definiendum id="0">Attention-sharing</definiendum>
				<definiens id="0">the activity of paying one 's attention to someone else 's attentional target</definiens>
			</definition>
			<definition id="1">
				<sentence>Figure 1 illustrates how attention-sharing is achieved : self ( S ) captures gaze-direction of an agent ( A ) , then the self searches in the direction and identifies the target ( T ) .</sentence>
				<definiendum id="0">attention-sharing</definiendum>
				<definiens id="0">self ( S ) captures gaze-direction of an agent ( A ) , then the self searches in the direction and identifies the target ( T )</definiens>
			</definition>
			<definition id="2">
				<sentence>The gaze-monitoring process consists of the following tasks , as also shown in Figure 5 : ( 1 ) detect a face in a scene , ( 2 ) saccade to the face and switch to the zoom cameras , ( 3 ) detect eyes and determine the gaze-direction in terms of the position of the pupils , and ( 4 ) search for an object in the direction .</sentence>
				<definiendum id="0">gaze-monitoring process</definiendum>
				<definiens id="0">consists of the following tasks , as also shown in Figure 5 : ( 1 ) detect a face in a scene , ( 2 ) saccade to the face and switch to the zoom cameras , ( 3 ) detect eyes and determine the gaze-direction in terms of the position of the pupils , and ( 4 ) search for an object in the direction</definiens>
			</definition>
</paper>

		<paper id="0610">
			<definition id="0">
				<sentence>As it processes the text , LinkIT stores the sentence number and token span of simplex NP , and assigns it a unique identifier reflecting the order in which it appeared in the document .</sentence>
				<definiendum id="0">LinkIT</definiendum>
				<definiens id="0">stores the sentence number and token span of simplex NP , and assigns it a unique identifier reflecting the order in which it appeared in the document</definiens>
			</definition>
			<definition id="1">
				<sentence>Nominator , a module which identifies proper names developed at the IBM TJ Watson Research categorizes them , and links expressions in the same document which refer to the same entity successfully exploited this property of documents ( Wacholder et al. 1997 ) .</sentence>
				<definiendum id="0">Nominator</definiendum>
			</definition>
</paper>

		<paper id="1103">
			<definition id="0">
				<sentence>wt= \ [ log ( f ( ti , d ) ) + 1\ ] * log ( N/n ) where f ( ti , d ) is the frequency of term t i in document d , N is the total number of documents in the collection , and n is the number of documents including t i. The indexing process maps each document and query onto a vector of weights within the vector space of the indexes of the corpus .</sentence>
				<definiendum id="0">f</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">n</definiendum>
				<definiens id="0">the frequency of term t i in document d ,</definiens>
				<definiens id="1">the total number of documents in the collection</definiens>
				<definiens id="2">the number of documents including t i. The indexing process maps each document and query onto a vector of weights within the vector space of the indexes of the corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>The same query contained the word stem used as a verb and SYSTRAN mistranslated it as the noun `` tige '' ( `` tree stem '' ) .</sentence>
				<definiendum id="0">SYSTRAN</definiendum>
				<definiens id="0">a verb and</definiens>
			</definition>
			<definition id="2">
				<sentence>One possible way to derive a o~idf-like weighting is to use the following transformed weight in the query vector : wq = p ( ~l e ) * log ( N/n ) where p ( fjl e ) is the probability obtained by the probabilistic translation model , and log ( N/n ) represents the idf criterion as described in section 3.1 , In our experiments , we tested different lengths of the list of translation words , as well as the two weighting methods in query vectors .</sentence>
				<definiendum id="0">p ( fjl e )</definiendum>
				<definiens id="0">the probability obtained by the probabilistic translation model</definiens>
			</definition>
</paper>

		<paper id="1109">
			<definition id="0">
				<sentence>ACRONYM uses two publicly available clustering tools , PAM and AGNES , described in Kaufman and Rousseeuw ( 1990 ) .</sentence>
				<definiendum id="0">ACRONYM</definiendum>
			</definition>
			<definition id="1">
				<sentence>The first , PAM ( Partitioning Around Medoids ) , is a k-medoid partitioning method , while AGNES is a variant on agglomerative nesting .</sentence>
				<definiendum id="0">AGNES</definiendum>
				<definiens id="0">a k-medoid partitioning method</definiens>
				<definiens id="1">a variant on agglomerative nesting</definiens>
			</definition>
			<definition id="2">
				<sentence>Gulf Shias military war HAERI Egyptians Iran-Iraq MANAGUA ADEL IRAQI Iraq Scuds Jordanians al-Arab CAIRO NICOSIA SAFA Dhahran US-led KABUL al-Assad Khafji emirate starve TEHRAN ISLAMABAD oilfields waterway Barco UAE invading DARWISH Saddam Table 10b : 'down ' nyms of war ( 1993 ) The main reference reflected in the 'down nyms ' of Table 10b is to the Gulf War , which followed Iraq 's invasion of Kuwait .</sentence>
				<definiendum id="0">HAERI Egyptians Iran-Iraq MANAGUA ADEL IRAQI Iraq Scuds Jordanians al-Arab CAIRO NICOSIA SAFA Dhahran US-led KABUL al-Assad Khafji emirate starve TEHRAN</definiendum>
				<definiens id="0">to the Gulf War , which followed Iraq 's invasion of Kuwait</definiens>
			</definition>
</paper>

		<paper id="1305">
			<definition id="0">
				<sentence>The MyhiU-Nerode theorem ( see Hopcroft and Ullman \ [ I\ ] ) states that among the many automata that accept a given language , there is a unique automaton ( excluding isomorphisms ) that has a minimal number of states .</sentence>
				<definiendum id="0">MyhiU-Nerode theorem</definiendum>
				<definiendum id="1">] )</definiendum>
				<definiens id="0">states that among the many automata that accept a given language , there is a unique automaton ( excluding isomorphisms ) that has a minimal number of states</definiens>
			</definition>
			<definition id="1">
				<sentence>Formally , we define a deterministic finite-state automaton to be a 5-tuple M = ( Q , , ~ , 6 , q0 , F ) , where Q is a finite set of states , qo • Q is the start state , F C Q is a set of final states , /7 is a finite set of symbols called the alphabet and 6 is a partial mapping ~ : Q × 27 ~ Q denoting transitions .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">states</definiendum>
				<definiendum id="2">Q</definiendum>
				<definiens id="0">the start state</definiens>
			</definition>
			<definition id="2">
				<sentence>We also define a property of an automaton specifying that all states can be reached from the start state : Use/ t ( M ) = VqE , 3 z. ( 6* ( qo , z ) = q ) The property of being a minimal automata is traditionally defined as follows ( see Watson \ [ 3 , 5\ ] ) : Min ( M ) =_ VM , ~DAFSA ( / : ( M ) = ~ ( M ' ) =~ IMI &lt; IM'I ) We will , however , use an alternative definition of •in•reality , which is shown to be equivalent ( see Watson \ [ 3 , 5\ ] ) : Mi , Umat ( M ) ( -Z ( q ) ^ Us4 t ( M ) 49 es O_ _ .</sentence>
				<definiendum id="0">Mi , Umat ( M ) ( -Z</definiendum>
			</definition>
			<definition id="3">
				<sentence>Algorithm 3.1 : Register : = 0 do there is another word -- * Word : = next word ; CommonPrefix : = common_prefix ( Word ) ; LastState : = 6* ( qo , CornmonPrefix ) ; 51 \ [ \ ] \ [ \ ] CurrentSuff~ : = Word\ [ length ( CommonPrefix ) + 1 ... length ( Word ) \ ] ; if has_children ( LastState ) -- * replace_or_register ( LastS tate ) fi ; add_suffix ( LastState , CurrentSuff~x ) od ; replace_or_register ( qo ) func comraon_prefix ( Word ) -- * return Word\ [ 1 ... n\ ] : n = max i : 3qEQ~* ( qo , Word\ [ 1 ... i\ ] ) = q cnuf func replace .</sentence>
				<definiendum id="0">add_suffix</definiendum>
				<definiens id="0">= Word\ [ length ( CommonPrefix ) + 1 ... length ( Word ) \ ] ; if has_children ( LastState ) -- * replace_or_register ( LastS tate ) fi ;</definiens>
			</definition>
			<definition id="4">
				<sentence>or_register ( State ) Child : = last_child ( State ) ; if not marked_as_registered ( Child ) -- * if has_children ( Child ) replace_or_register ( Child ) fi ; if 3qeQ ( marked-as_registered ( q ) A q = Child ) delete_branch ( Child ) ; last_child ( State ) : = q else Register : = Register U { Child } ; mark_as_registered ( Child ) fi fi cnuf m i m m m m m n \ [ \ ] m m m \ [ \ ] The function common_prefi~ finds the longest prefix ( of the word to be added ) that is a prefix of a word already in the automaton .</sentence>
				<definiendum id="0">or_register</definiendum>
				<definiens id="0">State ) : = q else Register : = Register U { Child }</definiens>
			</definition>
			<definition id="5">
				<sentence>The main loop of the algorithm runs m times , where ra is the number of words to be accepted by the dictionary .</sentence>
				<definiendum id="0">ra</definiendum>
			</definition>
			<definition id="6">
				<sentence>Algorithm 4.1 : Register : = 0 do there is another word Word : = next word ; CommonPrefix : = common_pre\ ] L~ ( Word ) ; FirstState : = first_state ( CommonPrefiz ) ; if FirstState = 0 LastState : = 6 ( q0 , ComrnonPrefix ) else LastState : = clone ( 6 ( qo , CommonPrefiz ) ) fi ; CurrentSuffix : = Word\ [ length ( CommonPrefiz ) + 1 ... length ( Word ) \ ] ; adoLsuffix ( LastState , CurrentSuffix ) ; if FirstState ~ 0 -- * Currentlndex : = ( length ( x ) : 6* ( qo , x ) = F irstState ) ; for i from length ( CommonPrefix ) 1 downto CurrentIndex CurrentState : = clone ( 6* ( qo , CornrnonPrefix \ [ 1 ... i\ ] ) ) ; 6 ( CurrentState , ComrnonPrefix \ [ i\ ] ) : = LastState ; replace_or_register ( CurrentState ) ; LastState : -CurrentState ; rof else CurrentIndex : = length ( CommonPrefix ) fi ; Changed : = true ; 54 I I m i Ha I t i I I I m I m l n m i i i I I I m i m i aid i od do od Currentlnde~ : 'Currentlnde~ 1 ; CurrentState : = 6* ( qo , Wont\ [ 1 ... OurrentInde~\ ] ) ; OldState : = Lo .</sentence>
				<definiendum id="0">adoLsuffix</definiendum>
				<definiendum id="1">Changed</definiendum>
				<definiens id="0">= next word ; CommonPrefix : = common_pre\ ] L~ ( Word ) ; FirstState : = first_state ( CommonPrefiz ) ; if FirstState = 0 LastState : = 6 ( q0 , ComrnonPrefix ) else LastState : = clone ( 6 ( qo , CommonPrefiz ) ) fi ; CurrentSuffix : = Word\ [ length ( CommonPrefiz ) + 1 ... length ( Word ) \ ] ;</definiens>
				<definiens id="1">qo , CornrnonPrefix \ [ 1 ... i\ ] ) ) ; 6 ( CurrentState , ComrnonPrefix \ [ i\ ] ) : = LastState ; replace_or_register ( CurrentState ) ; LastState : -CurrentState ; rof else CurrentIndex : = length ( CommonPrefix ) fi ;</definiens>
			</definition>
			<definition id="7">
				<sentence>If no such state exists , first_state returns O. As in the sorted case , the main loop of the unsorted algorithm executes ra times , where ra is the number of words accepted by the dictionary .</sentence>
				<definiendum id="0">ra</definiendum>
			</definition>
</paper>

		<paper id="1209">
			<definition id="0">
				<sentence>However despite these capabilities , to an end user an ANN is an arcane web of interconnected input , hidden , and output units .</sentence>
				<definiendum id="0">ANN</definiendum>
				<definiens id="0">an arcane web of interconnected input , hidden</definiens>
			</definition>
			<definition id="1">
				<sentence>Where sequences , such as PR , VB , AR , are referred to in the text , AR is the current input , VB the previous input ( at time step t-I ) and PR the input at time step t-2 .</sentence>
				<definiendum id="0">AR</definiendum>
				<definiens id="0">the current input</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , Figure 2 shows cluster formation when NN is the current input and either AIL NN , PR , PS , VB or/S is the previous input .</sentence>
				<definiendum id="0">NN</definiendum>
				<definiendum id="1">VB or/S</definiendum>
				<definiens id="0">the previous input</definiens>
			</definition>
			<definition id="3">
				<sentence>For example where the network has been required to induce a grammar , cluster analysis of hidden unit activations and preparation of an FSA is a powerful technique to explicate the learned grammar .</sentence>
				<definiendum id="0">FSA</definiendum>
				<definiens id="0">a powerful technique to explicate the learned grammar</definiens>
			</definition>
</paper>

		<paper id="1233">
			<definition id="0">
				<sentence>To begin with , the input received from the user is parsed into an alternating sequence of words and non-words , where a word is a series of alphanumeric characters and a non-word is a series of other characters .</sentence>
				<definiendum id="0">non-word</definiendum>
				<definiens id="0">a series of alphanumeric characters and a</definiens>
			</definition>
			<definition id="1">
				<sentence>I ( wls ) = log2 P ( wls ) ( 1 ) MegaHAL chooses the reply which assigns the keywords the highest information .</sentence>
				<definiendum id="0">MegaHAL</definiendum>
				<definiens id="0">chooses the reply which assigns the keywords the highest information</definiens>
			</definition>
			<definition id="2">
				<sentence>MegaRAL : THE NOBEL PRIZE FOR PHYSICS IN 1900 WAS WON BY ROENTGEN .</sentence>
				<definiendum id="0">MegaRAL</definiendum>
				<definiens id="0">THE NOBEL PRIZE FOR PHYSICS IN 1900 WAS WON BY ROENTGEN</definiens>
			</definition>
			<definition id="3">
				<sentence>MegaHAL uses a technique which differs significantly from that used by previous entrants to the Loebner contest .</sentence>
				<definiendum id="0">MegaHAL</definiendum>
				<definiens id="0">uses a technique which differs significantly from that used by previous entrants to the Loebner contest</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>Two-level morphology is characterized by an emphasis on phonological ( or orthographic ) rules , and has a rudimentary treatment of morphology itself .</sentence>
				<definiendum id="0">Two-level morphology</definiendum>
			</definition>
			<definition id="1">
				<sentence>The feature component is a hierarchy of types related by monotonic multiple inheritance .</sentence>
				<definiendum id="0">feature component</definiendum>
				<definiens id="0">a hierarchy of types related by monotonic multiple inheritance</definiens>
			</definition>
			<definition id="2">
				<sentence>The morphological component is a set of paradigms which relate feature structures of the feature component to inflected forms .</sentence>
				<definiendum id="0">morphological component</definiendum>
			</definition>
			<definition id="3">
				<sentence>The feature component consists of a hierarchy of grammatical objects constrained by relevant features , similar to the type hierarchy of Headdriven Phrase Structure Grammar ( Pollard and Sag 1994 ) .</sentence>
				<definiendum id="0">feature component</definiendum>
			</definition>
			<definition id="4">
				<sentence>Helping paradigms are sets of pairs ( F , S ) , where S is a string specification and F a feature structure associated with the string specification .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">a string specification</definiens>
			</definition>
			<definition id="5">
				<sentence>The string specification is a partial description of string which may contain variables ( Calder 1989 ) .</sentence>
				<definiendum id="0">string specification</definiendum>
				<definiens id="0">a partial description of string which may contain variables</definiens>
			</definition>
			<definition id="6">
				<sentence>Main paradigms relate lexemes ( lexical entries , so in the case of Arabic , consonantal roots ) to fully specified forms , through reference to helping paradigms : which helping paradigms are combined together is defined by each main paradigm .</sentence>
				<definiendum id="0">Main paradigms</definiendum>
				<definiens id="0">relate lexemes ( lexical entries , so in the case of Arabic , consonantal roots ) to fully specified forms</definiens>
			</definition>
			<definition id="7">
				<sentence>Main paradigms have the syntax in ( 6 ) : ( 6 ) M-P Name ( Phon , FS ) : FS n { ( H-P , ( x ) H-P2 ( x ) ... ) , ( H-P3 ( x ) H-P4 ( x ) ... ) , ... } In ( 6 ) , Name is the name of the paradigm , and its argument is a lexical entry , which has two components , a phonology and a feature structure .</sentence>
				<definiendum id="0">Main paradigms</definiendum>
				<definiendum id="1">Name</definiendum>
				<definiens id="0">the name of the paradigm</definiens>
			</definition>
			<definition id="8">
				<sentence>( 9 ) jVVlWs = RaaSaT RIj Via S/l W/a T/s String unification is a powerful tool which allows other kinds of morphological relations to be specified , besides concatenation : words are defined as lists of elements , which can be partial strings or individual characters .</sentence>
				<definiendum id="0">RaaSaT RIj Via S/l W/a T/s String unification</definiendum>
				<definiens id="0">a powerful tool which allows other kinds of morphological relations to be specified , besides concatenation : words are defined as lists of elements , which can be partial strings or individual characters</definiens>
			</definition>
			<definition id="9">
				<sentence>Inheritance ( monotonic and non-monotonic ) is a feature of object-oriented programming languages , and both kinds of unification have been extensively used , unification of feature structures in the NLP community , string unification in the field of automatic theorem proving ( Calder 1989 ) ; thus we can hope that implementation will be straightforward and results acceptable in terms of efficiency .</sentence>
				<definiendum id="0">Inheritance</definiendum>
				<definiens id="0">a feature of object-oriented programming languages</definiens>
			</definition>
</paper>

		<paper id="1302">
			<definition id="0">
				<sentence>13 I I I II II A context-free grammar G is a 4-tuple ( 27 , N , P , S ) , where 27 and N are two finite disjoint sets of terminals and nonterminals , respectively , S E N is the start symbol , and P is a finite set of rules .</sentence>
				<definiendum id="0">context-free grammar G</definiendum>
				<definiendum id="1">S E N</definiendum>
				<definiendum id="2">P</definiendum>
				<definiens id="0">the start symbol</definiens>
				<definiens id="1">a finite set of rules</definiens>
			</definition>
			<definition id="1">
				<sentence>A ( nondeterministie ) finite a~tomaton ~ is a 5-tuple ( K , 27 , A , s , F ) , where K is a finite set of states , of which s is the initial state and those in F _C K are the final states , ~ is the input alphabet , and the transition relation A is a finite subset of K × 27 '' × K. We define a configuration to be an element of K x 27 '' .</sentence>
				<definiendum id="0">K</definiendum>
				<definiendum id="1">_C K</definiendum>
				<definiens id="0">a finite set of states , of which s is the initial state</definiens>
				<definiens id="1">a finite subset of K × 27 '' × K. We define a configuration to be an element of K x 27 ''</definiens>
			</definition>
			<definition id="2">
				<sentence>state ; make_fst ( qo , X , q ) ; make_\ ] st ( q , ~ , ql ) else let A = a ; ( * a must consist of a single nonterminal * ) if A E Ni , some i then for each B E Ni do let qs = fresh_state end ; if recursive ( N~ ) = right then for each ( B -- ~ X1 ... Xm ) E P such that B E NiAX1 , ... , Xm ~ Ni do let q = fresh_state ; make_\ ] st ( qs , X1 ... X , n , q ) ; let A = /1 U { ( q , el ( B -~ X1 ... X~ o ) , ql ) } end ; for each ( B -~ X1 ... XmC ) E P such that B , C E Ni A X1 , ... , Xrn ~ Ni do let q = fresh_state ; make_\ ] st ( qs , X1 ... Xm , q ) ; let Zi = A O { ( q , el ( B -* X1 ... Xrn 0 C ) , qc ) } end ; let A -AU { ( qo,6Ie , qA ) } else for each ( B -4 ) ( 1 ... X~ ) E P such that B E N~ A X1 , ... , Xm ~ Ni do let q = fresh_state ; make_\ ] st ( qo , ) ( 1 ... Xm , q ) ; let zi = A U { ( q , el ( B -+Sx ... X~ o ) , qB ) } end ; for each ( B -- ~ CX1 ... Xm ) E P such that B , C E N~ A ) ( 1 , ... , Xm ~ Ni do let q = fresh_state ; make_\ ] st ( qc , X1 ... Xm , q ) ; let A = A U { ( q , el ( B ~ CX1 ... X~ o ) , qB ) } end ; let iI = ,413 { ( qA , ~le , ql ) } end else for each ( A -~ ~ ) E P ( * A is not recursive * ) do let q = fresh_state ; make_\ ] st ( qo , l~ , q ) ; let A = A U { ( q , c\ [ ( A -+/3 o ) , ql ) } end end end end .</sentence>
				<definiendum id="0">E Ni</definiendum>
				<definiens id="0">B -- ~ X1 ... Xm ) E P such that B E NiAX1 , ... , Xm ~ Ni do let q = fresh_state</definiens>
			</definition>
			<definition id="3">
				<sentence>state ( ) : create some fresh object q ; let K = K U { q } ; return q end .</sentence>
				<definiendum id="0">state ( )</definiendum>
				<definiens id="0">create some fresh object q</definiens>
			</definition>
			<definition id="4">
				<sentence>The preferred way of looking at these two tables is as a set of states and a set of transitions of a finite automaton ~ '' = ( K ' , ~2 , /1 ' , ( 0 , s ) , F ' ) , where F ' is a subset of { n } x F. Initially K ~ = { ( 0 , s ) } and A ' = 0 .</sentence>
				<definiendum id="0">F '</definiendum>
				<definiens id="0">a subset of { n } x F. Initially K ~ = { ( 0 , s ) }</definiens>
			</definition>
			<definition id="5">
				<sentence>Our particular kind of parse forest is a table U consisting of dott~ items of the form \ [ q , A ~ t~ , /~ , q'\ ] , where q and q ' are states from K ' and A ~ cl/~ is a rule .</sentence>
				<definiendum id="0">~ cl/~</definiendum>
				<definiens id="0">a table U consisting of dott~ items of the form \ [ q , A ~ t~ , /~</definiens>
			</definition>
			<definition id="6">
				<sentence>state , qd , e = fresh_state end ; for each A E Ni and ( C -~ ~ ... Y , n ) E P such that C E Ni ^ ~ , ... , Y , n ¢ Ni 4-do let q = fresh_state ; make_Jst ( qA~ , Ac ~ ... Ym , q ) ; let A = A U { ( q , e\ [ ( C ~ ~ ... Ym o ) , qc , s ) } 4end ; ( * for A~ ~ Ac ~ ... Y~CTB * ) for each A E Ni and ( D ~ aCYI ... YmE~ ) E P such that C , D , E E Ni A ~ , . . . , Ym ¢ Ni -- t do let q = fresh_state ; make_/st ( qA~ , CA ~ ... Ym , q ) ; let A =/t U { ( q , e\ [ ( D ~ aC~ ... Ym o E/~ ) , qE~ ) } end ; ( * for ATe -- ~ ~ Y1 ... Y , nE~ * ) for each A E Ni do make-fst ( qA~s , BA , ql ) ( * for ATe -~BA * ) end ; let ~ = ~ u ( ( q0 , ~le , qE : ) } elseif a is of form DE such that D , B E Ne , some i then for each A E Ni do let qA~ = fresh .</sentence>
				<definiendum id="0">E Ni</definiendum>
				<definiendum id="1">E Ni</definiendum>
				<definiens id="0">E Ni do make-fst ( qA~s , BA , ql ) ( * for ATe -~BA *</definiens>
			</definition>
			<definition id="7">
				<sentence>Regular approximations of CFLs : A grammatical view .</sentence>
				<definiendum id="0">CFLs</definiendum>
			</definition>
</paper>

		<paper id="0309">
			<definition id="0">
				<sentence>John is a student : he majors in history .</sentence>
				<definiendum id="0">John</definiendum>
				<definiens id="0">a student : he majors in history</definiens>
			</definition>
			<definition id="1">
				<sentence>The first-order system we propose is identical to commonsense entailment , except that each firstorder defensible rule is associated with a strength , represented by a pair of values s/t , where t is the number of times the rule has been triggered and s is the number of times it has succeeded .</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">the number of times it has succeeded</definiens>
			</definition>
</paper>

		<paper id="1410">
			<definition id="0">
				<sentence>AcT-R has two types of knowledge bases , or memories , to store permanent~ knowledge in : declarative and procedural representations of knowledge are explicitly separated into the declarative memory and the procedural production rule base , but are intimately connected .</sentence>
				<definiendum id="0">AcT-R</definiendum>
				<definiens id="0">has two types of knowledge bases , or memories , to store permanent~ knowledge in : declarative and procedural representations of knowledge are explicitly separated into the declarative memory and the procedural production rule base</definiens>
			</definition>
			<definition id="1">
				<sentence>On the right is an example for a chunk encoding the ±sa subset-fact fact that F C_ G , where subset-fact is a concept and F and G are setl F contextual chunks associated to ~actFsubsetG .</sentence>
				<definiendum id="0">subset-fact</definiendum>
				<definiens id="0">a concept and F and G are setl F contextual chunks associated to ~actFsubsetG</definiens>
			</definition>
			<definition id="2">
				<sentence>The activation Ai of a chunk Ci is defined as Ai = Si + ~WjSjl ( 1 ) J where Bi is the base-level activation , Wj is the weighting of a contextual chunk Cj , and Sji is the strength of the association of C/ with Cj .</sentence>
				<definiendum id="0">activation Ai of a chunk Ci</definiendum>
				<definiendum id="1">Wj</definiendum>
				<definiendum id="2">Sji</definiendum>
				<definiens id="0">Ai = Si + ~WjSjl ( 1 ) J where Bi is the base-level activation ,</definiens>
				<definiens id="1">the weighting of a contextual chunk Cj , and</definiens>
			</definition>
			<definition id="3">
				<sentence>I 1 AcT-R provides a learning mechanism , called knowledge compilation , which allows for the learning of new productions .</sentence>
				<definiendum id="0">knowledge compilation</definiendum>
				<definiens id="0">allows for the learning of new productions</definiens>
			</definition>
			<definition id="4">
				<sentence>Each theory in it may contain axioms , definitions , theorems along with proofs , as well as proof methods , and Control rules how to apply proof methods .</sentence>
				<definiendum id="0">Control</definiendum>
				<definiens id="0">rules how to apply proof methods</definiens>
			</definition>
			<definition id="5">
				<sentence>A justification consists of a rule and a list of labels , the premises of the node .</sentence>
				<definiendum id="0">justification</definiendum>
				<definiens id="0">consists of a rule and a list of labels , the premises of the node</definiens>
			</definition>
			<definition id="6">
				<sentence>The user model contains assumptions on the knowledge of the user that are relevant to proof explanation .</sentence>
				<definiendum id="0">user model</definiendum>
				<definiens id="0">contains assumptions on the knowledge of the user that are relevant to proof explanation</definiens>
			</definition>
			<definition id="7">
				<sentence>Then a microplanner ( sentenCe planner ) determines how to say it , i.e. it plans the scope and the internal structure of the sentences .</sentence>
				<definiendum id="0">sentenCe planner )</definiendum>
				<definiens id="0">determines how to say it</definiens>
			</definition>
			<definition id="8">
				<sentence>Mathematics communicating acts ( MCAs ) are the primitive actions planned by the dialog planner .</sentence>
				<definiendum id="0">Mathematics communicating acts</definiendum>
				<definiens id="0">the primitive actions planned by the dialog planner</definiens>
			</definition>
			<definition id="9">
				<sentence>( Derive : Reasons ( a 6 U , U C_ V ) : Conclusion a 6 V : Method Deft ) `` Since a is an element of U and U is a subset of V , a is all element of V by the definition of subset . ''</sentence>
				<definiendum id="0">Reasons</definiendum>
				<definiendum id="1">U</definiendum>
			</definition>
			<definition id="10">
				<sentence>Thus E ( p3 ) : P ( pa ) G ( p3 ) C ( p3 ) &gt; P ( p2 ) G ( p2 ) -C ( p2 ) -- -E ( p2 ) Therefore , the dialog planner chooses ( P3 ) for the explanation , thus producing ghe MCA ( Case-Analysis : Goal a E UUV : Cases ( a E U , a E V ) ) that could be realized as `` To prove a E U O V let us consider the two cases by assuming a E U and a E V , '' and then explains both cases .</sentence>
				<definiendum id="0">Cases</definiendum>
				<definiens id="0">a E V ) ) that could be realized as `` To prove a E U O V let us consider the two cases by assuming a E U and a E V , ''</definiens>
			</definition>
</paper>

		<paper id="1225">
			<definition id="0">
				<sentence>NLC ( : A ) : the analysis of concepts that play a role in natural language ; ( NL ) CA : the lattice theKaraphuis and Sarbo 205 Natural Language Concept Analysis V. Kamphuis and J.J. Sarbo ( 1998 ) Natural Language Concept Analysis .</sentence>
				<definiendum id="0">NLC</definiendum>
			</definition>
			<definition id="1">
				<sentence>The extension consists of all elements ( set of objects ) belonging to the concept while the intension covers all properties ( set of attributes ) valid for all those elements .</sentence>
				<definiendum id="0">extension</definiendum>
			</definition>
			<definition id="2">
				<sentence>We say , for g E G , m E M , ( g , m ) E R or equivalently , ( gRin ) , iff the object g has the attribute m. For a context the following mappings are defined : A ' = { m e M I gRrn for all g e A } for A C_ G ; and B ' = { ge G I gRrnforallme B } forB C M. A ( formal ) concept of a context ( G , M , R ) is a pair ( A , B ) with A C G , B C M , which satisfies the conditions ( i ) A ' = B and ( ii ) A = B ' .</sentence>
				<definiendum id="0">gRin</definiendum>
				<definiendum id="1">B</definiendum>
				<definiendum id="2">B C M</definiendum>
				<definiens id="0">all g e A } for A C_ G ; and B ' = { ge G I gRrnforallme B } forB C M. A ( formal ) concept of a context ( G , M , R ) is a pair</definiens>
			</definition>
			<definition id="3">
				<sentence>NLCA applies Wille 's theory to natural language by the equivalence : attributes are functors , and objects are arguments .</sentence>
				<definiendum id="0">NLCA</definiendum>
			</definition>
			<definition id="4">
				<sentence>The set of lexical types defines a partition of L , the set of lexical items and semantic roles involved in the analysis , which is further partitioned according to the dyadic model , yielding the sets G and M. From G C_ L and M C L follows that there is an embedding of the relation R C G x M in L x L. This means that any pair ( g , m ) E R can be defined as the unique yield of 11 and l~ ( ll , 12 E L ) by the assignments ll ~ g and 12 ~-r m , where ~ respects the mapping of lexical types .</sentence>
				<definiendum id="0">m ) E R</definiendum>
				<definiens id="0">a partition of L , the set of lexical items and semantic roles involved in the analysis , which is further partitioned according to the dyadic model , yielding the sets G and M. From G C_ L and M C L follows that there is an embedding of the relation R C G x M in L x L. This means that any pair ( g ,</definiens>
			</definition>
			<definition id="5">
				<sentence>Besides these relations , NLCA applies a set of general principles , like word order ( e.g. SVO ) , inheritance of relation and 'greedy ' binding of lexical items .</sentence>
				<definiendum id="0">NLCA</definiendum>
				<definiens id="0">applies a set of general principles , like word order</definiens>
			</definition>
			<definition id="6">
				<sentence>A Relation Matrix shows the relation between objects ( represented in rows ) and attributes ( columns ) .</sentence>
				<definiendum id="0">Relation Matrix</definiendum>
				<definiens id="0">shows the relation between objects ( represented in rows</definiens>
			</definition>
			<definition id="7">
				<sentence>4 , we say a Relation Matrix is well-formed if each external argument position of each attribute is filled ( meaning that the Kamphuis and Sarbo 209 Natural Language Concept Analysis semantic roles of major predicates are realized and that all other combinatorial demands of attributes have been fulfilled ) and each object is the external argument of some attribute .</sentence>
				<definiendum id="0">Relation Matrix</definiendum>
				<definiens id="0">well-formed if each external argument position of each attribute is filled ( meaning that the Kamphuis and Sarbo 209 Natural Language Concept Analysis semantic roles of major predicates are realized and that all other combinatorial demands of attributes have been fulfilled ) and each object is the external argument of some attribute</definiens>
			</definition>
			<definition id="8">
				<sentence>A PTR is depicted as a directed edge• Internal argument positions of objects and attributes are displayed to their left-hand side ( there is one argument position for the qualifiers , and one for the modifiers ) ; external argument positions to their right .</sentence>
				<definiendum id="0">PTR</definiendum>
				<definiendum id="1">left-hand side</definiendum>
				<definiens id="0">a directed edge• Internal argument positions of objects and attributes</definiens>
			</definition>
</paper>

		<paper id="1108">
			<definition id="0">
				<sentence>ng examines a portion of the OHSUMED ( Hersh et al. , 1994 ) corpus of medical abstracts , a part of the National Library of Medicine corpus that has over 9 million abstracts organized into over 10,000 categories in a taxonomy ( called MESH ) which is seven levels deep in some places .</sentence>
				<definiendum id="0">OHSUMED</definiendum>
				<definiens id="0">Hersh et al. , 1994 ) corpus of medical abstracts , a part of the National Library of Medicine corpus that has over 9 million abstracts organized into over 10,000 categories in a taxonomy ( called MESH ) which is seven levels deep in some places</definiens>
			</definition>
			<definition id="1">
				<sentence>We associate a weight , Wlc , with each surviving feature , f , in category c. We define W/¢ by : = + ( 1 ( 1 ) where NI¢ is the number of times f appears in c , Mc is the maximum frequency of any feature in c , and is a parameter ( currently set to 0.4 ) .</sentence>
				<definiendum id="0">NI¢</definiendum>
				<definiendum id="1">Mc</definiendum>
				<definiens id="0">the number of times f appears in c</definiens>
			</definition>
			<definition id="2">
				<sentence>where N ( fc ) is the number of times f appears in c , Mc is the maximum frequency of any feature in c , and is a parameter ( currently set to 0.4 ) .</sentence>
				<definiendum id="0">N ( fc )</definiendum>
				<definiendum id="1">Mc</definiendum>
				<definiens id="0">the number of times f appears in c</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus Nip is the number of times f appears In the parent of c , which is In turn the number of times f appears in all siblings of c since it does not appear in c itself at all .</sentence>
				<definiendum id="0">Nip</definiendum>
				<definiens id="0">the number of times f appears In the parent of c , which is In turn the number of times f appears in all siblings of c since it does not appear in c itself at all</definiens>
			</definition>
			<definition id="4">
				<sentence>Mp is the maximum frequency of any feature in c 's parent .</sentence>
				<definiendum id="0">Mp</definiendum>
				<definiens id="0">the maximum frequency of any feature in c 's parent</definiens>
			</definition>
			<definition id="5">
				<sentence>where the sum is over all positive and negative features associated with c and IVI , ~ is the number of times f appears in d. Note that , in practice , the sum is taken only over features that are in the intersection of the sets of features actually appearing in d and actually associated with c. Note that R¢4 may be positive , negative or zero .</sentence>
				<definiendum id="0">~</definiendum>
				<definiens id="0">the number of times f appears in d. Note that , in practice , the sum is taken only over features that are in the intersection of the sets of features actually appearing in d and actually associated with c. Note that R¢4 may be positive , negative or zero</definiens>
			</definition>
			<definition id="6">
				<sentence>We then define EFtc , the effective frequency of subtree rooted at node c with respect to feature fas EF/c = E ( 4 ) jcS , Thus , EFIc is the total number of occurrences of f in c and all subcategories , S¢ of node c. Finally , we define i'~ , c , the significance value of c with respect to f , as = × ( 5 ) Thus , a node gets credit , in proportion to its level , for occurrences of f in itself and in its successors .</sentence>
				<definiendum id="0">EFIc</definiendum>
				<definiens id="0">the total number of occurrences of f in c and all subcategories</definiens>
			</definition>
			<definition id="7">
				<sentence>For each feature , f , EFIc is compared with , EFIp , where p is the parent of c and if EFIc is smaller then f is removed from node c. Thus a parent can remove a feature from a child but not vice versa .</sentence>
				<definiendum id="0">EFIc</definiendum>
				<definiendum id="1">p</definiendum>
				<definiens id="0">remove a feature from a child but not vice versa</definiens>
			</definition>
</paper>

		<paper id="0202">
			<definition id="0">
				<sentence>A keyword consists of strings of kanji , e.g. , `` 1 .</sentence>
				<definiendum id="0">keyword</definiendum>
				<definiens id="0">consists of strings of kanji</definiens>
			</definition>
</paper>

		<paper id="1123">
			<definition id="0">
				<sentence>Note that this is not exactly analogous to IDF ; we do not compute inverse segment frequency ( ISF ) ; this is because we are looking for segments with noun phrases that occur throughout a text rather that segments which are characterized by local noun phrases .</sentence>
				<definiendum id="0">ISF</definiendum>
				<definiens id="0">looking for segments with noun phrases that occur throughout a text rather that segments which are characterized by local noun phrases</definiens>
			</definition>
</paper>

		<paper id="1242">
			<definition id="0">
				<sentence>A categorial grammar G is a 4-tuple G= &lt; V , C , f , S &gt; with : V is the finite alphabet ( or vocabulary ) of G ; C is the finite set of basic categories ofG ; From C , we define the set of all possible categories of G , noted C ' , as the closure of C for the operators / and \ .</sentence>
				<definiendum id="0">categorial grammar G</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">a 4-tuple G= &lt; V , C , f , S &gt; with : V is the finite alphabet ( or vocabulary ) of G</definiens>
				<definiens id="1">the finite set of basic categories ofG ; From C , we define the set of all possible categories of G , noted C ' , as the closure of C for the operators</definiens>
			</definition>
			<definition id="1">
				<sentence>C ' is the smallest set of categories verifying : * Cc_C ' ; * if XeC ' and YeC ' then : X/Y~C ' and Y~XeC ' ; f is a function : V -- &gt; Pf ( C ' ) where Pf ( C ' ) is the set of finite subsets of C ' , which associates each element v in V with the finite set f ( v ) _cC ' of its categories ; SeC is the axiomatic category of G. In this framework , the set of syntactically correct sentences is the set of finite concatenations of elements of the vocabulary for which there exists an affectation of categories that can be &lt; ~ reduced ~ to the axiomatic category S. In CCGs , the admitted reduction rules for any categories X and Y in C ' are : -R1 : X/Y .</sentence>
				<definiendum id="0">)</definiendum>
				<definiendum id="1">SeC</definiendum>
				<definiens id="0">a function : V -- &gt; Pf ( C ' ) where Pf ( C '</definiens>
				<definiens id="1">the set of finite subsets of C ' , which associates each element v in V with the finite set f ( v ) _cC ' of its categories</definiens>
			</definition>
			<definition id="2">
				<sentence>The class of languages defined by CCGs is the class of context-free languages ( Bar Hillel , Gaifman &amp; Shamir 60 ) .</sentence>
				<definiendum id="0">class of languages</definiendum>
				<definiendum id="1">CCGs</definiendum>
			</definition>
			<definition id="3">
				<sentence>The set of basic categories is C= { S , T , CN } where T stands for a terms , and is affected to proper names , CN means a common nouns &gt; ~ , intransitive verbs receive the category `` l'kS , transitive ones : ( 'IAS ) /T and determiners : ( S/ ( T~S ) ) /CN .</sentence>
				<definiendum id="0">CN</definiendum>
				<definiens id="0">C= { S , T , CN } where T stands for a terms , and is affected to proper names</definiens>
				<definiens id="1">means a common nouns &gt; ~ , intransitive verbs receive the category</definiens>
			</definition>
			<definition id="4">
				<sentence>IL is a typed language : the set I of all possible types of IL includes * elementary types : eel ( type of &lt; ~ entities &gt; &gt; ) and tel ( type of &lt; &lt; truth values &gt; &gt; ) ; * for any types uel and vel , &lt; u , v &gt; el ( &lt; u , v &gt; is the type of functions taking an argument of type u and giving a result of type v ) .</sentence>
				<definiendum id="0">IL</definiendum>
				<definiens id="0">a typed language : the set I of all possible types of IL includes * elementary types : eel ( type of &lt; ~ entities &gt; &gt;</definiens>
			</definition>
			<definition id="5">
				<sentence>For every couple &lt; s , x ( s ) &gt; where s is a sentence and x ( s ) its logical translation in IL , do : if there is one , affect to the words in s their category in the current hypothesis set ; else , make hypotheses on the category associated by fwith the unknown words ofs ; For every possible analysis tree : * translate the tree into IL ; * compare the final translation with x ( s ) and infer possible values for the unknown semantic translation of words to update the current hypothesis set .</sentence>
				<definiendum id="0">s</definiendum>
				<definiendum id="1">else</definiendum>
				<definiens id="0">a sentence and x ( s ) its logical translation in IL</definiens>
				<definiens id="1">s ) and infer possible values for the unknown semantic translation of words to update the current hypothesis set</definiens>
			</definition>
			<definition id="6">
				<sentence>Previous models built in the syntactieo-semantic spirit ( Anderson 77 , Hamburger &amp; Wexler 75 , Hill 83 , Langley 82 , ) used more traditional syntax and semantic representations very close to syntactic structures ( Pinker 79 ) : they failed to represent complex logical relations like quantification or Boolean operators .</sentence>
				<definiendum id="0">Previous models</definiendum>
			</definition>
</paper>

		<paper id="0318">
			<definition id="0">
				<sentence>In example ( 1 ) , yeah is a positive answer to a proposal .</sentence>
				<definiendum id="0">yeah</definiendum>
				<definiens id="0">a positive answer to a proposal</definiens>
			</definition>
</paper>

		<paper id="1428">
			<definition id="0">
				<sentence>Exemplars \ [ Rambow et al. 98\ ] are schenm-like text planning rules that are so called because they are meant to capture an exemplary way of achieving a communicative goal in a given communicative context , as determined by the system designer .</sentence>
				<definiendum id="0">Exemplars</definiendum>
				<definiens id="0">determined by the system designer</definiens>
			</definition>
			<definition id="1">
				<sentence>Project Reporter is an innovative web-based tool for monitoring the status of a project .</sentence>
				<definiendum id="0">Project Reporter</definiendum>
				<definiens id="0">an innovative web-based tool for monitoring the status of a project</definiens>
			</definition>
			<definition id="2">
				<sentence>~ ... ... .. ~ : ~i~ ; ~ : ~ : '~ ~ ~ '' Detailed Design ~i : i , : ProjectRepode~\ ] m-Programming andTesting : i~J i : i * -- -- Integration Testing =~i ; ii~ Project Summary ~ -- -\ ] system Test : : i Options , -'Derive test cases ~ ii : !</sentence>
				<definiendum id="0">m-Programming andTesting</definiendum>
				<definiens id="0">i~J i : i * -- -- Integration Testing =~i ; ii~ Project Summary ~ -- -\ ] system Test : : i Options</definiens>
			</definition>
</paper>

		<paper id="1118">
			<definition id="0">
				<sentence>A maximum entropy solution to this , or any other similar problem allows the computation of p ( f\ [ h ) for any f from the space of possible futures , F , for every h from the space of possible histories , H. A `` history '' in maximum entropy is all of the conditioning data which enables you to make a decision among the space of futures .</sentence>
				<definiendum id="0">maximum entropy</definiendum>
				<definiendum id="1">H. A `` history</definiendum>
				<definiens id="0">all of the conditioning data which enables you to make a decision among the space of futures</definiens>
			</definition>
			<definition id="1">
				<sentence>gi ( h , f ) = Z 15 ( h ) 'Z PME ( flh ) 'gi ( h , f ) h , f h \ [ ( 4 ) Here P is an empirical probability and PME is the probability assigned by the M.E. model .</sentence>
				<definiendum id="0">PME</definiendum>
				<definiendum id="1">PME</definiendum>
				<definiens id="0">an empirical probability</definiens>
				<definiens id="1">the probability assigned by the M.E. model</definiens>
			</definition>
			<definition id="2">
				<sentence>Histories and Futures MENE consists of a set of C++ and Perl modules which forms a wrapper around a publicly available M.E. toolkit ( Ristad , 1998 ) which computes the values of the a parameters of equation 2 from a pair of training files created by MENE .</sentence>
				<definiendum id="0">Futures MENE</definiendum>
				<definiens id="0">consists of a set of C++ and Perl modules which forms a wrapper around a publicly available M.E. toolkit ( Ristad , 1998 ) which computes the values of the a parameters of equation 2 from a pair of training files created by MENE</definiens>
			</definition>
			<definition id="3">
				<sentence>Consequently , MENE allows all features to fire in overlapping cases .</sentence>
				<definiendum id="0">MENE</definiendum>
				<definiens id="0">allows all features to fire in overlapping cases</definiens>
			</definition>
			<definition id="4">
				<sentence>Multi-word dictionaries are a key element of MENE .</sentence>
				<definiendum id="0">Multi-word dictionaries</definiendum>
				<definiens id="0">a key element of MENE</definiens>
			</definition>
			<definition id="5">
				<sentence>We are not sure why MENEProteus was hurt more badly by the evaluationtime switch from aviation disaster articles to mis158 Conditions Trained on Official training data Tested on dry run ( within domain ) Each organization trained on all of its own data and tested on dry run Same as above , but run against official MUC-7 data \ [ \ ] Identifinder MENE MENE + Proteus 92.5 89.17 94.30 95.1 92.20 95.61 90.44 84.22 88.80 Table 4 : Comparison of BBN and NYU statistical systems sile/rocket launch articles , but suspect that it may have been due to Identifinder 's greater quantity and quality of training data .</sentence>
				<definiendum id="0">MENEProteus</definiendum>
				<definiens id="0">Comparison of BBN and NYU statistical systems sile/rocket launch articles</definiens>
			</definition>
</paper>

		<paper id="1408">
			<definition id="0">
				<sentence>ARRLPLACE~ &gt; is an abbreviation of &lt; 1 '' 1 '' 1 '' 1 '' 1 '' 1 '' 1 '' DATA ARR-PLACE &gt; ZThis generator , called SEM2SYN , is a reusable surface generator for Dutch implemented in FUF ( Marsi 1998 ) .</sentence>
				<definiendum id="0">ARRLPLACE~ &gt;</definiendum>
				<definiens id="0">an abbreviation of &lt; 1 '' 1 '' 1 '' 1 '' 1 '' 1 '' 1 '' DATA ARR-PLACE &gt; ZThis generator , called SEM2SYN</definiens>
			</definition>
</paper>

		<paper id="1015">
</paper>

		<paper id="0306">
			<definition id="0">
				<sentence>b ) p ( X ) can be inferred from the assertion of 5'1 and p ( x ) can be inferred from the assertion of $ 2 , where x is a member or subset of X ( as for specification or exemplification ) .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">x</definiendum>
				<definiens id="0">a member or subset of X</definiens>
			</definition>
			<definition id="1">
				<sentence>c ) p ( x ) can be inferred from the assertion of St and p ( X ) can be inferred from the assertion of S~ , where x is a member or subset of X ( opposite case of generalization ) .</sentence>
				<definiendum id="0">c ) p ( x )</definiendum>
				<definiendum id="1">x</definiendum>
				<definiens id="0">a member or subset of X ( opposite case of generalization )</definiens>
			</definition>
			<definition id="2">
				<sentence>two versions Table 4 shows a comparison of the relations employed in the spoken and in the written versions of the narratives ( where A means additive type , CS consequential , and CTcontrastive , respectively ) .</sentence>
				<definiendum id="0">CTcontrastive</definiendum>
				<definiens id="0">employed in the spoken and in the written versions of the narratives ( where A means additive type</definiens>
			</definition>
			<definition id="3">
				<sentence>si°KE r 'v ITTi A CS CT A CS CT 21.6 59.7 95.3 24.8 43.4 91.7 Table 6 : Percentage of relations signalled by a connective for the two versions ( mean values ) As it can be seen , the contrastive type holds the highest degree of marking ( over 90 % in the two versions ) ; the consequential type of relations shows a relatively high degree of marking ( around 60 % in speaking , around 40 % in writing ) .</sentence>
				<definiendum id="0">CS CT A CS CT</definiendum>
				<definiens id="0">Percentage of relations signalled by a connective for the two versions ( mean values</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>A WFSA is a state/transition diagram with we .</sentence>
				<definiendum id="0">WFSA</definiendum>
				<definiens id="0">a state/transition diagram with we</definiens>
			</definition>
			<definition id="1">
				<sentence>These are P ( w ) , the probability of a particular English word sequence and P ( elw ) , the probability of an English sound sequence given a word sequence .</sentence>
				<definiendum id="0">P ( w )</definiendum>
				<definiens id="0">the probability of a particular English word sequence and P ( elw ) , the probability of an English sound sequence given a word sequence</definiens>
			</definition>
			<definition id="2">
				<sentence>s I ) ) ( ( K R IH S ) ( k r y s ) ) ( ( K R IH S CH AH N ) ( k r y s t s h n ) ) ( ( K R IH S T AH F ER ) ( k r y s t w f r ) ) ( ( K L AO D ) ( k 1 w d ) ) ( ( K LAY D ) ( k 1 !</sentence>
				<definiendum id="0">K R IH S )</definiendum>
				<definiens id="0">s h n ) ) ( ( K R IH S T AH F ER</definiens>
			</definition>
			<definition id="3">
				<sentence>n ) ) ( ( K UH K ) ( k w k ) ) ( ( K AO R IH G AH N ) ( k w r y G !</sentence>
				<definiendum id="0">n ) ) ( ( K UH K )</definiendum>
			</definition>
</paper>

		<paper id="1304">
			<definition id="0">
				<sentence>Oflazer ( 1996 ) used a similar structure called `` vertex lists '' which he defined as the path from a leaf to the root of the tree but , different from our definition , including the tag and the word .</sentence>
				<definiendum id="0">vertex lists</definiendum>
				<definiens id="0">the path from a leaf to the root of the tree but , different from our definition , including the tag and the word</definiens>
			</definition>
			<definition id="1">
				<sentence>In some cases trees can be said to match approximately , and using vertex lists to quantify the amount of difference between trees , Oflazer shows how trees similar to a given tree can be retrieved from a database ( treebank ) .</sentence>
				<definiendum id="0">Oflazer</definiendum>
				<definiens id="0">shows how trees similar to a given tree can be retrieved from a database ( treebank )</definiens>
			</definition>
			<definition id="2">
				<sentence>We do not re~stimate probabilities using the Bantu-Welch algorithm ( Bantu , 1972 ) but we use smoothed Maximum Likelihood estimates from treebank data .</sentence>
				<definiendum id="0">Bantu-Welch algorithm</definiendum>
				<definiens id="0">estimates from treebank data</definiens>
			</definition>
			<definition id="3">
				<sentence>ei1 where s is a traversal string , the symbol Bi-I indicates the set of tag-traversal string pairs that is being considered for word wi-l , and ~ indicates the `` forward probability '' according to the HMM .</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">a traversal string , the symbol Bi-I indicates the set of tag-traversal string pairs that is being considered for word wi-l</definiens>
			</definition>
			<definition id="4">
				<sentence>qt maximum traversal string length and section 23 ( 2,416 sentences ) was used exclusively for testing .</sentence>
				<definiendum id="0">qt maximum traversal</definiendum>
				<definiens id="0">string length and section 23 ( 2,416 sentences ) was used exclusively for testing</definiens>
			</definition>
</paper>

		<paper id="1120">
</paper>

		<paper id="0308">
			<definition id="0">
				<sentence>c the succession of Ted 's departure and Mary 's arrival ( totally upset Fred ) In summary , a discourse relation can be lexicalized by a subordinating conjunction , an adverbial , an operator verb , or the nominalization of an operator verb .</sentence>
				<definiendum id="0">discourse relation</definiendum>
				<definiens id="0">an operator verb , or the nominalization of an operator verb</definiens>
			</definition>
			<definition id="1">
				<sentence>Among the existing lexicalized grammars , TAG has long been seen as especially well suited for text generation ( Joshi 1987 ) .</sentence>
				<definiendum id="0">TAG</definiendum>
				<definiens id="0">has long been seen as especially well suited for text generation</definiens>
			</definition>
			<definition id="2">
				<sentence>GT , ,K3has been first implemented in ADA ( Meunier 1997 ) and used in three technical domains ( chemical , software , and aeronautic ) .</sentence>
				<definiendum id="0">GT</definiendum>
				<definiens id="0">Meunier 1997 ) and used in three technical domains ( chemical , software , and aeronautic )</definiens>
			</definition>
</paper>

		<paper id="0604">
			<definition id="0">
				<sentence>NOMLEX , a dictionary of nominalizations currently under development at NYU , ( Macleod et al. , 1997 ) provides a way to handle nominalizations more automatically , and with INominalizations are nouns which are related to words of another part of speech , most commonly verbs .</sentence>
				<definiendum id="0">NOMLEX</definiendum>
			</definition>
			<definition id="1">
				<sentence>2The Message Understanding Colfference Scenario Template Task ( MUC , 1995 ) , ( MUC , 1998 ) is ore '' model for the kind of information that we are attempting to extract ( who does what to whom , and when and where ) .</sentence>
				<definiendum id="0">Colfference Scenario Template Task</definiendum>
				<definiens id="0">ore '' model for the kind of information that we are attempting to extract ( who does what to whom , and when and where )</definiens>
			</definition>
			<definition id="2">
				<sentence>Proteus applies meta-rules to this pattern to produce new patterns for other clausal types , e.g. , a passive clause : np ( C-person ) vg-pass ( appoint ) `` as '' np ( C-position ) `` by '' np ( C-company ) ( vg-pass is a passive verb group ) .</sentence>
				<definiendum id="0">C-company )</definiendum>
				<definiens id="0">a passive verb group )</definiens>
			</definition>
			<definition id="3">
				<sentence>PET allows the user to input an example sentence and specify the mappings from syntactic to semantic form .</sentence>
				<definiendum id="0">PET</definiendum>
				<definiens id="0">allows the user to input an example sentence and specify the mappings from syntactic to semantic form</definiens>
			</definition>
			<definition id="4">
				<sentence>PET creates patterns to fill the semantic slots ( employer , employee , position ) from the gramatical roles ( subject , object , NP following as , etc. ) in the sentence .</sentence>
				<definiendum id="0">PET</definiendum>
				<definiens id="0">creates patterns to fill the semantic slots ( employer , employee , position ) from the gramatical roles ( subject , object , NP following as , etc. ) in the sentence</definiens>
			</definition>
			<definition id="5">
				<sentence>Furthermore , vhether or not one position is filled may affect the interpretation of other positions ; thus , in Rome 's destruction , Rome is the object , whereas in Rome 's destruction of Carthage , Rome is the subject .</sentence>
				<definiendum id="0">Rome</definiendum>
				<definiendum id="1">Rome</definiendum>
				<definiens id="0">the object , whereas in Rome 's destruction of Carthage</definiens>
			</definition>
			<definition id="6">
				<sentence>4 For example , in COMLEX ( Wolff et al. , 1994 ) , NP-PP consists of a Noun Phrase followed by a Prepositional Phrase as in put the milk in the refrigerator , where *put the milk , *put in the refrigerator and *put in the refrigerator the milk are not acceptable .</sentence>
				<definiendum id="0">NP-PP</definiendum>
				<definiens id="0">consists of a Noun Phrase followed by a Prepositional Phrase as in put the milk in the refrigerator</definiens>
			</definition>
			<definition id="7">
				<sentence>Optionality , obligatoriness and alternative positions of phrases is stated in a simple notation , e.g. , it can be stated in the entry for consideration that the verbal object for the NPAS-NP complement of consider maps to either the DET-POSS position ( HIS consideration as a candidate ) or the PP-OF position ( The consideration OF HIM as a candidate ) and that this object is obligatory for mappings of NP-AS-NP , i.e. , if the object is not present in the phrase containing consideration , then the phrase can not be mapped to the NP-AS-NP complement , although other complements are possible .</sentence>
				<definiendum id="0">PP-OF position</definiendum>
				<definiendum id="1">NP-AS-NP</definiendum>
				<definiens id="0">DET-POSS position ( HIS consideration as a candidate</definiens>
				<definiens id="1">a candidate</definiens>
			</definition>
			<definition id="8">
				<sentence>( NOM : ORTH `` appointment '' : VERB `` appoint '' : PLURAL `` appointments '' : NOUN ( exists ) : NOM-TYPE ( VERB-NOM ) : VERB-SUBJ ( ( N-N-MOD ) ( DEW-BOSS ) ) : SUB J-ATTRIBUTE ( COMMUNICATOR ) : OBJ-ATTRIBUTE ( NHUMAN ) : VERB-SUBC ( ( NOM-NP : OBJECT ( ( DEW-BOSS ) ( N-N-MOD ) ( PP-OF ) ) : REQUIRED ( ( OBJECT ) ) ) ( NOM-NP-PP : OBJECT ( ( DEW-BOSS ) ( N-N-MOD ) ( PP-OF ) ) : PVAL ( `` for '' `` to '' ) : REQUIRED ( ( OBJECT ) ) ) ( NOM-NP-TO-INF-OC : OBJECT ( ( DEW-BOSS ) ( PP-OF ) ) : REQUIRED ( ( OBJECT ) ) ) ( NOM-NP-AS-NP : OBJECT ( ( DEW-BOSS ) ( PP-OF ) ) : REQUIRED ( ( OBJECT ) ) ) } ) Figure 1 : NOMLEX entry for appointment ( NOM : ORTH `` appointee '' : VERB `` appoint '' : PLURAL `` appointees '' : NOM-TYPE ( OBJECT ) : VERB-SUBJ ( ( PP-OF ) ( NOT-PP-BY ) ( N-N-MOD ) ( DEW-BOSS ) ) : SUB J-ATTRIBUTE ( COMMUNICATOR ) : OBJ-ATTRIBUTE ( NHUMAN ) : VERB-SUBC ( ( NOM-NP ) ( NOM-NP-PP : P\ ( A.L ( `` for '' `` to '' ) ) ( NOM-NP-AS-NP ) ) ) Figure 2 : NOMLEX entry for appointee However , NOM-NP-AS-NP does not allow tile N-N-MOD position ( * the Alice Smith appointment as vice president ) .</sentence>
				<definiendum id="0">NOM</definiendum>
				<definiendum id="1">VERB-SUBJ ( ( N-N-MOD )</definiendum>
				<definiendum id="2">DEW-BOSS )</definiendum>
				<definiendum id="3">N-N-MOD )</definiendum>
				<definiendum id="4">VERB-SUBJ ( ( PP-OF )</definiendum>
				<definiendum id="5">NOT-PP-BY )</definiendum>
				<definiendum id="6">N-N-MOD )</definiendum>
				<definiendum id="7">DEW-BOSS ) ) : SUB J-ATTRIBUTE</definiendum>
				<definiendum id="8">NOM-NP )</definiendum>
				<definiens id="0">REQUIRED ( ( OBJECT ) ) ) ( NOM-NP-TO-INF-OC : OBJECT ( ( DEW-BOSS ) ( PP-OF ) ) : REQUIRED ( ( OBJECT ) ) ) ( NOM-NP-AS-NP : OBJECT ( ( DEW-BOSS ) ( PP-OF ) ) : REQUIRED ( ( OBJECT ) ) ) }</definiens>
			</definition>
			<definition id="9">
				<sentence>The : OBJECT is not indicated for the : VERB-SUBC of appointee because the nominalization itself corresponds to the verbal object ( it is : NOM-TYPE ( ( OBJECT ) ) ) .</sentence>
				<definiendum id="0">OBJECT</definiendum>
				<definiens id="0">the verbal object ( it is : NOM-TYPE ( ( OBJECT ) ) )</definiens>
			</definition>
			<definition id="10">
				<sentence>Furthermore , objects are obligatory for a particular complement NP-X for a particular nominalization N , if N takes both NP-X and X as complements , where NP-X includes all the phrases in X plus an object ( e.g. , NOM-NP vs. NOM-INTRANS , NOM-NPPP vs. NOM-PP , NOM-NP-THAT-S vs. NOM-THAT-S , etc. ) .</sentence>
				<definiendum id="0">NP-X</definiendum>
				<definiens id="0">includes all the phrases in X plus an object</definiens>
			</definition>
			<definition id="11">
				<sentence>First PET analyzes the sample sentence and identifies the main verb and its arguments ( e.g. , subject , direct object , etc. ) .</sentence>
				<definiendum id="0">First PET</definiendum>
				<definiens id="0">analyzes the sample sentence and identifies the main verb and its arguments ( e.g. , subject , direct object , etc. )</definiens>
			</definition>
			<definition id="12">
				<sentence>9 9Given a clausal pattern for an example sentence like They appointed Alice Smith to IBM , the NOM-NPPP class would be matched and nominalization patterns would be generated in which IBM ( the indirect object ) S ... ... ... ... ... ... ...</sentence>
				<definiendum id="0">IBM</definiendum>
				<definiens id="0">the indirect object</definiens>
			</definition>
			<definition id="13">
				<sentence>*° Det n ( appointee ) of np ( C-company ) The appointee of IBM Det np ( C-company ) n ( appointee ) The IBM appointee np ( C-company ) 's n ( appointee ) IBM 's appointee np ( C-company ) 's np ( C-person ) n ( C-appointment ) IBM 's Alice Smith appointment np ( C-company ) 's n ( C-appointment ) of np ( C-person ) IBM 's appointment of Alice Smith Det np ( C-company ) np ( C-person ) n ( C-appointment ) The IBM Alice Smith appointment Det np ( C-company ) n ( C-appointment ) of np ( C-person ) The IBM appointment of Alice Smith np ( C-person ) 's n ( C-appointment ) by np ( C-company ) Alice Smith 's appointment by IBM Det np ( C-person ) n ( C-appointment ) by np ( C-company ) The Alice Smith appointment by IBM Det n ( C-appointment ) of np ( C-person ) by np ( C-company ) The appointment of Alice Smith by IBM Det n ( C-appointment ) by np ( C-company ) of np ( C-person ) The appointment by IBM of Alice Smith np ( C-company ) 's n ( C-appointment ) IBM 's appointment rip ( C-person ) 's n ( C-appointment ) Alice Smith 's appointment Det np ( C-person ) n ( C-appointment ) The Alice Smith appointment Det n ( C-appointment ) of np ( C-person ) The appointment of Alice Smith Det n ( C-appointment ) by np ( C-company ) The appointment by IBM Det rip ( C-company ) n ( C-appointment ) The IBM appointment Figure 3 : Deriving Nominalization Patterns with PET 30 PET can then use these mappings to generate patterns , as it does for the various types of clauses .</sentence>
				<definiendum id="0">IBM Det np</definiendum>
				<definiendum id="1">IBM appointee np</definiendum>
				<definiendum id="2">C-person ) n ( C-appointment</definiendum>
				<definiendum id="3">C-company ) The Alice Smith appointment by IBM Det n</definiendum>
				<definiendum id="4">C-person ) by np</definiendum>
				<definiendum id="5">IBM Det rip</definiendum>
				<definiens id="0">C-company ) 's n ( appointee ) IBM 's appointee np ( C-company ) 's np ( C-person ) n ( C-appointment ) IBM 's Alice Smith appointment np ( C-company ) 's n ( C-appointment ) of np ( C-person ) IBM 's appointment of Alice Smith Det np ( C-company ) np</definiens>
				<definiens id="1">appointment of Alice Smith by IBM Det n ( C-appointment ) by np ( C-company ) of np ( C-person ) The appointment by IBM of Alice Smith np ( C-company ) 's n ( C-appointment ) IBM 's appointment rip ( C-person ) 's n ( C-appointment )</definiens>
				<definiens id="2">C-appointment ) The Alice Smith appointment Det n ( C-appointment ) of np ( C-person ) The appointment of Alice Smith Det n ( C-appointment ) by np ( C-company ) The appointment by</definiens>
			</definition>
			<definition id="14">
				<sentence>Using pattern matching and dictionary look-up , PET associates the verbal arguments with semantic classes .</sentence>
				<definiendum id="0">PET</definiendum>
				<definiens id="0">associates the verbal arguments with semantic classes</definiens>
			</definition>
			<definition id="15">
				<sentence>For example , the mapping ( SUBJECT : DET-POSS , OBJECT : PP-OF ) generates the nominalization pattern : np ( C-company ) 's n ( appointment ) of np ( C-person ) ( IBM 's appointment of Alice Smith ) .</sentence>
				<definiendum id="0">OBJECT</definiendum>
				<definiendum id="1">PP-OF )</definiendum>
			</definition>
</paper>

		<paper id="1218">
			<definition id="0">
				<sentence>Mass corresponds to length in Zipf 's model , and distance to access time .</sentence>
				<definiendum id="0">Mass</definiendum>
				<definiens id="0">corresponds to length in Zipf 's model , and distance to access time</definiens>
			</definition>
			<definition id="1">
				<sentence>Interestingly , the terms of both sequences of series approach those of the series Y~ 2 `` L* ( r ) ( where L* ( x-1 ) is defined as log c + log x + log log x + ... ) which does converge and is optimal in the sense that any monotonic decreasing distribution which satisfies our constraint must equal or exceed L* ( x ) -2k* ( x ) infinitely often ( Rissanen , 1989 , p35 ) , where k* ( x ) is the number of positive log terms in L* ( x ) excluding the constant .</sentence>
				<definiendum id="0">the terms of both sequences of series</definiendum>
				<definiendum id="1">k* ( x )</definiendum>
				<definiens id="0">log c + log x + log log x + ... ) which does converge and is optimal in the sense that any monotonic decreasing distribution which satisfies our constraint must equal or exceed L*</definiens>
				<definiens id="1">the number of positive log terms in L* ( x ) excluding the constant</definiens>
			</definition>
</paper>

		<paper id="0605">
			<definition id="0">
				<sentence>Similar to a verbal semantic dictionary , a nominal semantic dictionary describes what kind of nouns have what relation with each noun obligately ( like obligate cases of a verb ) as follows : 33 KAKAKU ( price ) • an attribute of something like KURUMA ( car ) , PASOKON ( personal computer ) , RINGO ( apple ) , NIKU ( meat ) YANE ( roof ) • a part of a building like IE ( house ) , KOYA ( hut ) SENSEI ( teacher ) • belongs to some institute like SHOGAKKO ( elementary school ) , KOUKOU ( high school ) , and • teaches something like SUGAKU ( mathematics ) , ONGAKU ( music ) A nominal semantic dictionary is necessary for indirect anaphora resolution .</sentence>
				<definiendum id="0">NIKU ( meat ) YANE</definiendum>
				<definiendum id="1">KOYA</definiendum>
				<definiendum id="2">KOUKOU</definiendum>
				<definiendum id="3">mathematics ) , ONGAKU ( music</definiendum>
				<definiens id="0">personal computer</definiens>
				<definiens id="1">elementary school ) ,</definiens>
			</definition>
			<definition id="1">
				<sentence>Sumita et al. caluculated the similarity between an input `` Ai NO Bi '' and an example `` Ae NO Be '' as follows : w A • sim ( Ai , Ae ) + wB '' sim ( Bi , Be ) , where sim ( Ai , Ae ) is the similarity between Ai and Ae calculated based on the distance of the two words in a thesaurus tree , sim ( Bi , Be ) is the same for Bi and Be , WA and WB are weights showing which similarity should be considered more relevant , Ai and Ae or Bi and Be .</sentence>
				<definiendum id="0">sim</definiendum>
				<definiendum id="1">Ae )</definiendum>
				<definiendum id="2">Be )</definiendum>
				<definiens id="0">the similarity between Ai and Ae calculated based on the distance of the two words in a thesaurus tree</definiens>
				<definiens id="1">the same for Bi and Be , WA and WB are weights showing which similarity should be considered more relevant , Ai and Ae or Bi and Be</definiens>
			</definition>
			<definition id="2">
				<sentence>Decision Tree Induction Each example phrase `` A NO B '' is expressed by a triple ( TA , TB , Rj ) , where TA and TB are the position ( node ) in a thesaurus matching the word A and B , respectively , Rj is the semantic relation of the phrase given by hand .</sentence>
				<definiendum id="0">Decision Tree Induction Each example phrase</definiendum>
				<definiendum id="1">NO B</definiendum>
				<definiendum id="2">TB</definiendum>
				<definiendum id="3">Rj</definiendum>
				<definiens id="0">the position ( node ) in a thesaurus matching the word A and B , respectively</definiens>
				<definiens id="1">the semantic relation of the phrase given by hand</definiens>
			</definition>
			<definition id="3">
				<sentence>Each node in the decision tree , D , corresponds to the information expressed by a triple ( T ( A ) , T ( B ) , S ) , where T ( A ) and T ( B ) are the position ( node ) in A-side thesaurus and B-side thesaurus , respectively , S is a subset of example phrases .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">the information expressed by a triple ( T ( A ) , T ( B ) , S ) , where T ( A ) and T ( B ) are the position ( node ) in A-side thesaurus and B-side thesaurus , respectively</definiens>
				<definiens id="1">a subset of example phrases</definiens>
			</definition>
			<definition id="4">
				<sentence>on = L logs i 3 where N is the number of examples in S , Ni is the number of examples in Si , and Nq is the number of examples in Si which is given the j-th semantic relation .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">Ni</definiendum>
				<definiendum id="2">Nq</definiendum>
				<definiens id="0">the number of examples in Si</definiens>
				<definiens id="1">the number of examples in Si which is given the j-th semantic relation</definiens>
			</definition>
			<definition id="5">
				<sentence>At each internal node D , we follow the branch depending on the D 's selected attribute ( T ( A ) or T ( s ) ) and the thesaurus position of the input nouns ( TA~ or TB~ ) .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">A ) or T ( s ) ) and the thesaurus position of the input nouns ( TA~ or TB~ )</definiens>
			</definition>
			<definition id="6">
				<sentence>-belong : KOUKOU ( high school ) , SHOUGAKKOU ( elementary school ) , JYUKU ( crammer ) -- YANE ( roof ) part-whole : JYUTAKU ( house ) , KURUMA ( car ) , STADIUM ( stadium ) , KOYA ( hut ) UDE ( arm ) field : TENNIS ( tennis ) , SHODOU ( calligraphy ) , KARATE ( karate ) , ENSOU ( musical performance ) - .</sentence>
				<definiendum id="0">KOUKOU</definiendum>
				<definiendum id="1">SHOUGAKKOU</definiendum>
				<definiendum id="2">KOYA</definiendum>
				<definiendum id="3">TENNIS</definiendum>
				<definiens id="0">elementary school ) , JYUKU ( crammer ) -- YANE ( roof ) part-whole : JYUTAKU ( house )</definiens>
			</definition>
</paper>

		<paper id="1411">
			<definition id="0">
				<sentence>The tree will be right-branching and if the reader just remembers the last fact at any point , then they can follow the connection between the text so far and the next fact 2 Interestingly , Marcu uses `` right skew '' to b disambiguate between alternative ~tree s produced in rhetorical parsing .</sentence>
				<definiendum id="0">Marcu</definiendum>
				<definiens id="0">uses `` right skew '' to b disambiguate between alternative ~tree s produced in rhetorical parsing</definiens>
			</definition>
			<definition id="1">
				<sentence>Mutation is a unary operation which , given one sequence , generates a new one .</sentence>
				<definiendum id="0">Mutation</definiendum>
				<definiens id="0">a unary operation which</definiens>
			</definition>
			<definition id="2">
				<sentence>Mutation selects a random element , removes it from the sequence and then inserts it again in a random place .</sentence>
				<definiendum id="0">Mutation</definiendum>
				<definiens id="0">selects a random element , removes it from the sequence and then inserts it again in a random place</definiens>
			</definition>
			<definition id="3">
				<sentence>Iti is an Organic style jewel and is 72.0 cm long .</sentence>
				<definiendum id="0">Iti</definiendum>
				<definiens id="0">an Organic style jewel and is 72.0 cm long</definiens>
			</definition>
</paper>

		<paper id="1238">
			<definition id="0">
				<sentence>AUG encodes lexical properties as feature structures ( specifying such things as part-of-speech , number , tense , person , thematic role , etc. ) whose values percolate up through a subsumption hierarchy by the process of unification ( Sanfilippo , 1993 ) .</sentence>
				<definiendum id="0">AUG</definiendum>
			</definition>
			<definition id="1">
				<sentence>A WST is a derivative of a letter-based multiway trie built from an ordered set of words .</sentence>
				<definiendum id="0">WST</definiendum>
				<definiens id="0">a derivative of a letter-based multiway trie built from an ordered set of words</definiens>
			</definition>
			<definition id="2">
				<sentence>The WST is a suitable data structure for uncovering suffixes , but is insufficient for identifying those which mark inflection .</sentence>
				<definiendum id="0">WST</definiendum>
			</definition>
</paper>

		<paper id="1235">
			<definition id="0">
				<sentence>The Silver Medal will be awarded for the straight Turing Test , whilst the Gold Medal and Grand Prize are for a version of the Total Turing Test .</sentence>
				<definiendum id="0">Silver Medal</definiendum>
				<definiens id="0">the straight Turing Test , whilst the Gold Medal and Grand Prize are for a version of the Total Turing Test</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>Hebrew determiners are characterized by two properties : ( 1 ) definiteness is marked in several places in the NP and it interacts with compound constructs ; ( 2 ) the order of appearance of determiners and quantifiers within a complex NP is flexible but still restricted .</sentence>
				<definiendum id="0">Hebrew determiners</definiendum>
			</definition>
			<definition id="1">
				<sentence>Syntactically , Hebrew determiners are characterized by two properties : ( 1 ) definiteness is marked in several places in the NP and it interacts with compound constructs ; ( 2 ) the order of appearance of determiners and quantifiers within a complex NP is flexible but still restricted .</sentence>
				<definiendum id="0">Hebrew determiners</definiendum>
			</definition>
			<definition id="2">
				<sentence>Where pre-det can be any one of all , both , half , multipliers and fractions , det is a deictic determiner , deicticg is an adjective from a restricted class that expresses the anaphoric status of the thing referred to ( e.g. , above , same , different ) ; and quantifier expresses the amount or quantity of the thing referred to .</sentence>
				<definiendum id="0">pre-det</definiendum>
				<definiendum id="1">det</definiendum>
				<definiendum id="2">deicticg</definiendum>
				<definiens id="0">a deictic determiner</definiens>
			</definition>
			<definition id="3">
				<sentence>We will not discuss how a noun phrase is defined as semantically definite ( a development on this issue is provided in \ [ 1\ ] Chap.5 ) , and we assume that the decision that a referent is definite is taken by the content determination module and that the semantic definite feature is given in the input to the generation grammar .</sentence>
				<definiendum id="0">definite</definiendum>
				<definiens id="0">definite is taken by the content determination module and that the semantic definite feature is given in the input to the generation grammar</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>Morphotactics is the study of how morphemes combine together to make wellformed words .</sentence>
				<definiendum id="0">Morphotactics</definiendum>
				<definiens id="0">the study of how morphemes combine together to make wellformed words</definiens>
			</definition>
			<definition id="1">
				<sentence>Relations , and Finlte-State Transducers A regular expression that contains an alphabet of one-level symbols defines a regular language and compiles into a finite-state machine ( FSM ) that accepts this regular language .</sentence>
				<definiendum id="0">Finlte-State</definiendum>
				<definiendum id="1">regular expression</definiendum>
				<definiens id="0">contains an alphabet of one-level symbols defines a regular language and compiles into a finite-state machine ( FSM ) that accepts this regular language</definiens>
			</definition>
			<definition id="2">
				<sentence>A regular expression that contains an alphabet of paired symbols defines a regular relation ( a relation between two regular languages ) and compiles into a finite-state transducer ( FST ) that maps from every string of one language into strings of the other .</sentence>
				<definiendum id="0">regular expression</definiendum>
				<definiendum id="1">FST</definiendum>
				<definiens id="0">contains an alphabet of paired symbols defines a regular relation ( a relation between two regular languages</definiens>
			</definition>
			<definition id="3">
				<sentence>The Xerox implementation of finite-state morphology includes a complete range of fundamental algorithms ( concatenation , union , intersection , complementation , etc. ) plus higherlevel shorthand languages such as lexc ( Karttunen , 1993 ) , twolc ( Karttunen and Beesley , 1992 ) and Replace Rules ( Karttunen , 1995 ; Karttunen and Kempe , 1995 ; Karttunen , 1996 ) .</sentence>
				<definiendum id="0">Replace Rules</definiendum>
				<definiens id="0">concatenation , union , intersection , complementation , etc.</definiens>
			</definition>
			<definition id="4">
				<sentence>A-B \B `` B the zero-length string ( often called E ) bracketing ; denotes the same language as A the concatenation of B after A the union of A and B the intersection of A and B optionality , equivalent to \ [ A I 0 \ ] Kleene star iteration , zero or more concatenations of A equivalent to \ [ A A* \ ] ) i.e. one or more concatenations of A the regular language A , ignoring any instances of B any symbol , i.e. the union of all single-symbol strings language A , minus all strings in language B equivalent to \ [ ?</sentence>
				<definiendum id="0">A-B \B</definiendum>
				<definiendum id="1">zero-length string</definiendum>
				<definiens id="0">often called E ) bracketing ; denotes the same language as A the concatenation of B after A the union of A and B the intersection of A and B optionality , equivalent to \ [ A I 0 \ ] Kleene star iteration , zero or more concatenations of A equivalent to \ [ A A* \ ] ) i.e. one or more concatenations of A the regular language A , ignoring any instances of B any symbol , i.e. the union of all single-symbol strings language A , minus all strings in language B equivalent to \ [</definiens>
			</definition>
			<definition id="5">
				<sentence>; define C \ [ k ~ t ~ b \ [ d m r m s \ ] ; define V \ [ a I i \ ] u \ ] ; define FormI \ [ C V C V C \ ] ; define FormII \ [ C V C X V C \ ] ; define formIII \ [ C V V C V C \ ] ; Vocalizations are also defined as regular expressions denoting regular languages , e.g. Perfect Active as \ [ a*\ ] /\V , the set of all strings containing zero or more as , ignoring all other symbols except vowels .</sentence>
				<definiendum id="0">define C</definiendum>
				<definiens id="0">] ; define FormI \ [ C V C V C \ ] ; define FormII \ [ C V C X V C \ ] ; define formIII \ [ C V V C V C \ ] ; Vocalizations are also defined as regular expressions denoting regular languages , e.g. Perfect Active as \ [ a*\ ] /\V , the set of all strings containing zero or more as , ignoring all other symbols except vowels</definiens>
			</definition>
			<definition id="6">
				<sentence>Kataja and Koskenniemi ( 1988 ) were apparently the first to understand that concatenating languages were just a special case ; they showed that by generalizing lexicography to allow regular expressions , Semitic ( specifically Akkadian ) roots and patterns could denote regular languages , and that stems could be computed as the intersection of these regular languages .</sentence>
				<definiendum id="0">Semitic</definiendum>
				<definiens id="0">a special case ; they showed that by generalizing lexicography to allow regular expressions</definiens>
			</definition>
			<definition id="7">
				<sentence>The Arabic morphological analyzer starts out as a dictionary database containing entries for prefixes , suffixes , roots and patterns of Arabic .</sentence>
				<definiendum id="0">Arabic morphological analyzer</definiendum>
				<definiens id="0">starts out as a dictionary database containing entries for prefixes , suffixes , roots and patterns of Arabic</definiens>
			</definition>
			<definition id="8">
				<sentence>For recognition purposes , the rules applied to the bottom side include \ [ a I i I u I o I \ ] ( - &gt; ) 0 ; which optionally maps the fatha ( a ) , kasra ( i ) , d.amma ( u ) , sukuun ( o ) and shadda ( ' ) to the empty string .</sentence>
				<definiendum id="0">shadda</definiendum>
			</definition>
			<definition id="9">
				<sentence>KIMMO : a general morphological processor .</sentence>
				<definiendum id="0">KIMMO</definiendum>
				<definiens id="0">a general morphological processor</definiens>
			</definition>
			<definition id="10">
				<sentence>Two-level morphology : A general computational model for word-form recognition and production .</sentence>
				<definiendum id="0">Two-level morphology</definiendum>
				<definiens id="0">A general computational model for word-form recognition and production</definiens>
			</definition>
</paper>

		<paper id="1230">
			<definition id="0">
				<sentence>• The case vocabulary contains the language in which cases axe written .</sentence>
				<definiendum id="0">case vocabulary</definiendum>
				<definiens id="0">contains the language in which cases axe written</definiens>
			</definition>
			<definition id="1">
				<sentence>A case base CB covers a set of chunks c G S iff there exists for each c at least one solution case s E CB where both c and s are instantistions of the same concept : Vc G S 3s E CB : C ( c ) C ( s ) • the retrieved solutions axe adaptable .</sentence>
				<definiendum id="0">case base CB</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">covers a set of chunks c G S iff there exists for each c at least one solution case s E CB where both c and s are instantistions of the same concept : Vc G S 3s E CB : C ( c )</definiens>
			</definition>
			<definition id="2">
				<sentence>Specification consists in replacing sl in the abstraction by the butcher and s3 by the slice .</sentence>
				<definiendum id="0">Specification</definiendum>
				<definiens id="0">consists in replacing sl in the abstraction by the butcher and s3 by the slice</definiens>
			</definition>
			<definition id="3">
				<sentence>=_ NC ( s. ) c ( T ) ( 2 ) To preserve the conceptual equivalence between a source sentence S and its translation T , all three components need to be synchronized : a certain type of decomposition requires an adequate case base which covers the decomposed chunks and an appropriate adaptation mechanism which is able to re-combine the retrieved solutions into a target translation .</sentence>
				<definiendum id="0">=_ NC</definiendum>
				<definiens id="0">s. ) c ( T ) ( 2 ) To preserve the conceptual equivalence between a source sentence S and its translation T</definiens>
				<definiens id="1">covers the decomposed chunks and an appropriate adaptation mechanism which is able to re-combine the retrieved solutions into a target translation</definiens>
			</definition>
			<definition id="4">
				<sentence>Rehability thus increases with coarse decomposition granularity and low degree of abstraction .</sentence>
				<definiendum id="0">Rehability</definiendum>
				<definiens id="0">increases with coarse decomposition granularity and low degree of abstraction</definiens>
			</definition>
			<definition id="5">
				<sentence>Carl 252 A Constructivist Approach to Machine Translation | | m m m I m | m | Figure 6 English Phrase Descriptor of the sentence : The big man ea~s a green apple WD : WDThe I WDbia I • WDman I WDea , , I WD , I WDg ... . LMA : the CAT : art VTP : TNS : NUM : CAS : DEG : WNR : 1 big adj base 2 man verb noun fin infin pres -- -- sing -- n ; a 3 3 3 eats verb tin pres 4 a art 5 green adj noun -n ; a base -i 6 6 \ ] WDa~ple apple noun sing n ; a 7 German Phrase Descriptor of the sentence : Der grosse Mann isst einen gr~nen Apfel WD : WDve , l wDg ... .. \ [ WDM. , , , , I WD , .</sentence>
				<definiendum id="0">VTP</definiendum>
				<definiendum id="1">NUM</definiendum>
				<definiens id="0">the CAT : art</definiens>
			</definition>
			<definition id="6">
				<sentence>LMA : CAT : NUM : VTP : TNS : CAS : DEG : WNR : lemma ( basic word form without inflectional information ) part-of-speech ( syntactic category ) ( adj ; adv ; art ; noun ; punct ; re1 ; verb ) number ( sing ; ph ) verb type ( fro ; infin ) tense ( pres ; past ) case ( n ; g ; d ; a ) degree of adjectives ( base ; comp ; sup ) word number and the expected reliability of the results .</sentence>
				<definiendum id="0">infin ) tense</definiendum>
				<definiens id="0">lemma ( basic word form without inflectional information</definiens>
			</definition>
			<definition id="7">
				<sentence>CBAG consists of three modules : the Case Based Compilation module ( CBC ) , the Case Base Analysis module ( CBA ) and the Case Base Generation module ( CBG ) .</sentence>
				<definiendum id="0">CBAG</definiendum>
				<definiens id="0">consists of three modules : the Case Based Compilation module ( CBC ) , the Case Base Analysis module ( CBA ) and the Case Base Generation module ( CBG )</definiens>
			</definition>
			<definition id="8">
				<sentence>MPRO is a very powerful tool , which generates more than 95 % correct analyses for arbitrary GetLemmatization yields for a surface string a basic word form ( lemma ) that abstracts away from inflectional information which is contained in the surface form .</sentence>
				<definiendum id="0">MPRO</definiendum>
				<definiens id="0">a very powerful tool , which generates more than 95 % correct analyses for arbitrary GetLemmatization yields for a surface string a basic word form ( lemma ) that abstracts away from inflectional information which is contained in the surface form</definiens>
			</definition>
			<definition id="9">
				<sentence>A case CASE is a pair of a source phrase descriptor PD , o~ce and a target phrase descriptor PDtarget that axe considered to be translations of each other .</sentence>
				<definiendum id="0">case CASE</definiendum>
			</definition>
			<definition id="10">
				<sentence>A case base CB is a set of cases .</sentence>
				<definiendum id="0">case base CB</definiendum>
				<definiens id="0">a set of cases</definiens>
			</definition>
			<definition id="11">
				<sentence>Specification extends the chunk descriptors index by searching from the case base the solutions ( i.e. translations ) of the appropriate cases .</sentence>
				<definiendum id="0">Specification</definiendum>
				<definiens id="0">extends the chunk descriptors index by searching from the case base the solutions</definiens>
			</definition>
</paper>

		<paper id="0606">
			<definition id="0">
				<sentence>The QUASI-LOGICAL FORM describes the combination of the elements which are needed for mapping the query representation onto the appropriate database access statement , and DESCRIPTION documents a legal explanation of the term .</sentence>
				<definiendum id="0">QUASI-LOGICAL FORM</definiendum>
			</definition>
			<definition id="1">
				<sentence>For the expression Ersatzzeit wegen Kindererziehung , we use the following rules : GRAMMAR FRAGMENT : TOP - &gt; DerP AP N NP PP* NP - &gt; NPi Narg NPI - &gt; DeZP A* N PP - &gt; P NP Categories in curly brackets are optional .</sentence>
				<definiendum id="0">GRAMMAR FRAGMENT</definiendum>
				<definiens id="0">TOP - &gt; DerP AP N NP PP* NP - &gt; NPi Narg NPI - &gt; DeZP A* N PP - &gt; P NP Categories in curly brackets are optional</definiens>
			</definition>
			<definition id="2">
				<sentence>A Modular and Flexible Architecture for an Integrated Corpus Query System .</sentence>
				<definiendum id="0">Modular</definiendum>
				<definiendum id="1">Flexible Architecture for</definiendum>
				<definiens id="0">an Integrated Corpus Query System</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>A1-Fedaghi and A1-Anzi ( 1989 ) present an algorithm to generate the root and the pattern of a given Arabic word .</sentence>
				<definiendum id="0">A1-Fedaghi</definiendum>
				<definiens id="0">present an algorithm to generate the root and the pattern of a given Arabic word</definiens>
			</definition>
			<definition id="1">
				<sentence>Algorithm to find quadriliteral roots WELCOME TO THE MORPHOLOGY MAIN MENU \ [ 1\ ] GetParadigm ( display all information ) \ [ 2\ ] GetForm ( get specific tense form ) \ [ 3\ ] Analyze ( get mood/person/number/gender ) \ [ 4\ ] GetRoot SYSTEM Figure 2 .</sentence>
				<definiendum id="0">GetParadigm</definiendum>
				<definiendum id="1">Analyze</definiendum>
				<definiens id="0">display all information</definiens>
			</definition>
</paper>

		<paper id="0311">
			<definition id="0">
				<sentence>Rather than consider each of these a distinct lexical item , a nmre parsimonious account may be reached by means of the following hypotheses : A : these sounds are not fixed sequences of phonemes , but are formed for each occasion from basic acoustic components ; B : these acoustic components individually bear meanings ; C : the meaning of a combination of acoustic components is the combination of the meanings of each component .</sentence>
				<definiendum id="0">B</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">these acoustic components individually bear meanings</definiens>
			</definition>
</paper>

		<paper id="1210">
			<definition id="0">
				<sentence>A predictive model M is a model which , when presented with a sequence of symbols s , is able to make a prediction about the next symbol in the sequence in the form of a probability distribution over the alphabet E ( for the purposes of this investigation , is the set of ASCII characters ) .</sentence>
				<definiendum id="0">predictive model M</definiendum>
				<definiens id="0">a model which , when presented with a sequence of symbols s , is able to make a prediction about the next symbol in the sequence in the form of a probability distribution over the alphabet E ( for the purposes of this investigation</definiens>
			</definition>
			<definition id="1">
				<sentence>The training corpus used in this example was 3.5 megabytes of Sherlock Holmes stories , minus the testing sentence .</sentence>
				<definiendum id="0">Sherlock Holmes</definiendum>
				<definiens id="0">stories , minus the testing sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>1 Segmentation is a matter of chunking the data whenever the instantaneous entropy exceeds some threshold value ( Wolff , 1977 ) .</sentence>
				<definiendum id="0">Segmentation</definiendum>
			</definition>
			<definition id="3">
				<sentence>The previous experiment was repeated using a version of the Sherlock Holmes corpus which had many clues to word boundaries removed ; all characters were replaced with their uppercase equivalents , and whitespace and punctuation symbols were deleted .</sentence>
				<definiendum id="0">Sherlock Holmes</definiendum>
				<definiens id="0">corpus which had many clues to word boundaries removed ; all characters were replaced with their uppercase equivalents , and whitespace and punctuation symbols were deleted</definiens>
			</definition>
			<definition id="4">
				<sentence>Entropic chunking may be used to discover this automatically , by recording which symbols occur immediately prior to a large jump in entropy .</sentence>
				<definiendum id="0">Entropic chunking</definiendum>
				<definiens id="0">by recording which symbols occur immediately prior to a large jump in entropy</definiens>
			</definition>
</paper>

		<paper id="0612">
			<definition id="0">
				<sentence>Adaptability between a stored exemplar and an input context is defined as follows : Definition 2 : Adaptability between an exemplar context C~ and an input context C , is a function ( f ) of the compatibility between the sense-views associated with C~ and their thematically corresponding concepts in C , .</sentence>
				<definiendum id="0">Adaptability</definiendum>
				<definiendum id="1">input context</definiendum>
				<definiens id="0">between a stored exemplar and an</definiens>
				<definiens id="1">follows : Definition 2 : Adaptability between an exemplar context C~ and an input context C , is a function ( f ) of the compatibility between the sense-views associated with C~ and their thematically corresponding concepts in C</definiens>
			</definition>
			<definition id="1">
				<sentence>SENSE Generated for Input Word Read \ [ Sense-Concept ( s ) \ ] READ , IN-MIND \ [ Sense View\ ] Thematic Role : Action No. of Exemplars : 3 Marker ( s ) : verb , MODE ( 1 ) -- -- &gt; : IN-MIND , IsKindOf-ACTIVE-COGNITIVE-EVENT ( 0.9 ) -- -- &gt; : IsKindOf-COGNITIVE-EVENT ( 0.81 ) -- -- &gt; : IsKindOf-M ENTAL-EVENT ( 0.729 ) -- -- &gt; : IsKindOf-EVENT ( 0.6561 ) -- - &gt; : SENSE Generated for Input Word Book \ [ Sense-Concept ( s ) \ ] BOOK \ [ Sense View\ ] Thematic Role : Theme No. of Exemplars : 3 ° Marker ( s ) : p-obj , CONTAINS ( 0.666667 ) -- -- &gt; : INFORMATION , MADE-OF ( 0.666667 ) -- -- &gt; : PAPE R , INK , IsKindOfP RINTED-MEDIA ( 0.6 ) -- -- &gt; : LOCATION ( 0.6 ) -- -- &gt; : ACEDEMIC-BUILDING , IsKindOf-VISUAL-MEDIA-ARTIFACT ( 0.54 ) -- -- &gt; : IsKindOf-DOCUMENT ( 0.57 ) -- -- &gt; : PRODUCE D-BY ( 0.57 ) -- -- &gt; : HUMAN , IsKindOf-LANGUAGE-RELATE DOBJECT ( 0.513 ) -- -- &gt; : REPRESENTS ( 0.756 ) -- &gt; : OBJECT , EVENT , LANGUAGE , And finally book as an object of moving .</sentence>
				<definiendum id="0">SENSE Generated</definiendum>
				<definiens id="0">s ) \ ] READ , IN-MIND \ [ Sense View\ ] Thematic Role : Action No. of Exemplars : 3 Marker ( s ) : verb</definiens>
				<definiens id="1">IsKindOf-DOCUMENT ( 0.57 ) -- -- &gt; : PRODUCE D-BY ( 0.57 ) -- -- &gt; : HUMAN , IsKindOf-LANGUAGE-RELATE DOBJECT ( 0.513 ) -- -- &gt; : REPRESENTS ( 0.756 ) -- &gt; : OBJECT , EVENT , LANGUAGE , And finally book as an object of moving</definiens>
			</definition>
</paper>

		<paper id="0713">
			<definition id="0">
				<sentence>Lexicons are a key component of machine translation systems ( Onyshkevych and Nirenburg , 1994 ) .</sentence>
				<definiendum id="0">Lexicons</definiendum>
				<definiens id="0">a key component of machine translation systems</definiens>
			</definition>
			<definition id="1">
				<sentence>Like a thesaurus , WordNet is structured around groups of synonymous words .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">structured around groups of synonymous words</definiens>
			</definition>
			<definition id="2">
				<sentence>Therefore , the semantics for the lexicon entry might just be a direct mapping to the associated I I i i I I i i i I I I ( book ( book-N1 ( cat n ) ; ; category ( morph ) ; ; morphology ( anno ; ; annotations ( def `` a copy of a written work or composition that has been published '' ) ( ex `` I just read a good book on economics '' ) ( cro~-ref ) ) ( syn ) ; ; syntactic features ( syn-struc ; ; syntactic structure ( 1 ( ( root $ var0 ) ( cat n ) ) ) ) ( sem-struc ; ; semantic structure ( lex-map ; ; lexical mapping ( 1 ( COOK ) ) ) ) ; ; to concept book ( lex-rules ) ; ; lexical rules ( pragm ) ; ; pragmatics ( styl ) ) ) ; ; stylistics Figure I : Lexical representation for `` book '' concept .</sentence>
				<definiendum id="0">; annotations</definiendum>
				<definiens id="0">root $ var0 ) ( cat n ) )</definiens>
			</definition>
			<definition id="3">
				<sentence>Specifi ( ally , they augment WordNet by linking in entries fr , ) m an ontology describing word processing .</sentence>
				<definiendum id="0">Specifi</definiendum>
				<definiens id="0">they augment WordNet by linking in entries fr , ) m an ontology describing word processing</definiens>
			</definition>
</paper>

		<paper id="0319">
			<definition id="0">
				<sentence>A continuer is a short utterance which plays discourse-structuring roles like indicating that the other speaker should go on talking ( Jefferson , t984 ; Schegloff , 1982 ; Yngve , 1970 ) .</sentence>
				<definiendum id="0">continuer</definiendum>
				<definiens id="0">a short utterance which plays discourse-structuring roles like indicating that the other speaker should go on talking ( Jefferson , t984</definiens>
			</definition>
			<definition id="1">
				<sentence>This suggests a generalization of the 'cue word ' hypothesis : while some utterances may be ambiguous , in general the lexical form of a DA places strong constraints on which DA the utterance can realize .</sentence>
				<definiendum id="0">DA</definiendum>
				<definiens id="0">places strong constraints on which DA the utterance can realize</definiens>
			</definition>
			<definition id="2">
				<sentence>The fourth author ( an original labeler ) listened to and relabeled 44 randomly selected conversations that she had previously labeled only from text .</sentence>
				<definiendum id="0">fourth author</definiendum>
				<definiens id="0">an original labeler ) listened to and relabeled 44 randomly selected conversations that she had previously labeled only from text</definiens>
			</definition>
</paper>

		<paper id="1402">
			<definition id="0">
				<sentence>FCA starts from the notion of a formal context ( G , M , 1 ) representing the data in which G is a set of objects , M is a set of attributes and I establishes a binary relation between the two sets .</sentence>
				<definiendum id="0">FCA</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">starts from the notion of a formal context ( G , M , 1 ) representing the data in which G is a set of objects</definiens>
				<definiens id="1">a set of attributes</definiens>
			</definition>
			<definition id="1">
				<sentence>The formal Concepts of concern in FCA are defined as consisting of an extension and an intension , where the extension is a subset A of the set of objects G and the intension is a subset B of the set of attributes .</sentence>
				<definiendum id="0">intension</definiendum>
				<definiens id="0">a subset A of the set of objects G and the</definiens>
				<definiens id="1">a subset B of the set of attributes</definiens>
			</definition>
			<definition id="2">
				<sentence>A multivalued context is a generalisation of a one-valued context that may formally be represented as a quadruple ( G , M , W , I ) where G , M and I are as before .</sentence>
				<definiendum id="0">multivalued context</definiendum>
				<definiens id="0">a generalisation of a one-valued context that may formally be represented as a quadruple ( G , M , W , I ) where G , M</definiens>
			</definition>
			<definition id="3">
				<sentence>I HUberseimer 1970 ( b ) • Figure 3 : Example generated diagrams for the example data A dependency lattice , in which the edges represent functions between the domains and the non-existing edges represent set-valuedmappings , may be interpreted as a set of classifications of the relational input .</sentence>
				<definiendum id="0">dependency lattice</definiendum>
				<definiens id="0">a set of classifications of the relational input</definiens>
			</definition>
			<definition id="4">
				<sentence>Wille , R. ( 1982 ) , Restructuring lattice theory : an approach based on hierarchies of concept , in I. Rival , ed. , 'Ordered Sets ' , Reidel , Dordecht/Boston , pp .</sentence>
				<definiendum id="0">Restructuring lattice theory</definiendum>
				<definiens id="0">an approach based on hierarchies of concept , in I. Rival , ed. , 'Ordered Sets ' , Reidel , Dordecht/Boston , pp</definiens>
			</definition>
</paper>

		<paper id="0204">
			<definition id="0">
				<sentence>We present a system , called Texplore , which assists readers in exploring the content of expository texts .</sentence>
				<definiendum id="0">Texplore</definiendum>
				<definiens id="0">assists readers in exploring the content of expository texts</definiens>
			</definition>
			<definition id="1">
				<sentence>Rather than the common presentation of documents by static abstracts , Texplore provides dynamic presentation of the text 's content , where the user controls the level of detail .</sentence>
				<definiendum id="0">Texplore</definiendum>
				<definiens id="0">provides dynamic presentation of the text 's content , where the user controls the level of detail</definiens>
			</definition>
			<definition id="2">
				<sentence>Many researches ( Longacre , 1979 ; Hinds , 1979 ; Kieras , 1982 ) have shown that the paragraph is a basic unit of coherency , and that it functions very slmilarly in many languages of vastly different origin ( Chafe , 1979 ) .</sentence>
				<definiendum id="0">Many researches</definiendum>
				<definiens id="0">a basic unit of coherency , and that it functions very slmilarly in many languages of vastly different origin</definiens>
			</definition>
			<definition id="3">
				<sentence>Cohesion is defined as the non-structural mechanlam by which discourse units of different sizes can be connected across gaps of any texts .</sentence>
				<definiendum id="0">Cohesion</definiendum>
				<definiens id="0">the non-structural mechanlam by which discourse units of different sizes can be connected across gaps of any texts</definiens>
			</definition>
			<definition id="4">
				<sentence>Navigation follows what may be called a stream of associations .</sentence>
				<definiendum id="0">Navigation</definiendum>
				<definiens id="0">follows what may be called a stream of associations</definiens>
			</definition>
			<definition id="5">
				<sentence>n tBk , i `` Wk , i+ l Pro imi U ( s , si+ ) = \ [ 18 l\ [ ( 1 ) k=l Here Wk , i is the weight of the k'th term of si , and \ ] lsiH is the length of the vector .</sentence>
				<definiendum id="0">lsiH</definiendum>
				<definiens id="0">the weight of the k'th term of si , and \ ]</definiens>
			</definition>
</paper>

		<paper id="1203">
			<definition id="0">
				<sentence>Link grammar ( Grinberg , Lafferty &amp; Sleator , 1995 ; Sleator &amp; Temperley 1991 ) is a highly lexieal , context-free formalism that does not rely on constituent structure .</sentence>
				<definiendum id="0">Link grammar</definiendum>
				<definiens id="0">a highly lexieal , context-free formalism that does not rely on constituent structure</definiens>
			</definition>
			<definition id="1">
				<sentence>Unfortunately , German is one of the languages in which this principle is violated in a number of cases .</sentence>
				<definiendum id="0">German</definiendum>
				<definiens id="0">one of the languages in which this principle is violated in a number of cases</definiens>
			</definition>
			<definition id="2">
				<sentence>A ( complete ) link grammar can be represented as a ( crisp ) relation G among the set W of all words and the set D of all potential disjuncts G : W × L -- &gt; { 0,1 } with its characteristic function gc ( w 'd ) = { ; /felse &lt; w , d &gt; is grammatical where an ordered pair &lt; w , d &gt; is assigned the membership value 1 if d is a valid linkage for the word w. Now if only a fragment of the grammar is known , the fuzzy relation G* is defined as G* : W x L -- ~ \ [ 0,1\ ] where the membership value does not indicate whether the ordered pair is in the grammar but whether the pair is known to be in the grammar or to what degree it is assumed to be in the grammar ( for the characteristic function see section 4.1 ) .</sentence>
				<definiendum id="0">link grammar</definiendum>
				<definiendum id="1">d</definiendum>
				<definiens id="0">a ( crisp ) relation G among the set W of all words and the set D of all potential disjuncts G : W × L -- &gt; { 0,1 } with its characteristic function gc</definiens>
			</definition>
			<definition id="3">
				<sentence>Grammatical trigrams : a probabilistic model of link grammar .</sentence>
				<definiendum id="0">Grammatical trigrams</definiendum>
				<definiens id="0">a probabilistic model of link grammar</definiens>
			</definition>
</paper>

		<paper id="0314">
			<definition id="0">
				<sentence>`` Part '' is used as a generic term subsuming chapter , section , sub-section , etc .</sentence>
				<definiendum id="0">Part</definiendum>
			</definition>
			<definition id="1">
				<sentence>A definition typically consists of two functional elements : the class , expressed by a hypernym , and the specificity , expressed by a modifier attached to the hypernym .</sentence>
				<definiendum id="0">definition</definiendum>
				<definiens id="0">consists of two functional elements : the class</definiens>
			</definition>
			<definition id="2">
				<sentence>In the rightmost formulation , the interpretation of `` Display '' as a type of command relies solely on layout clues : Three commands may be applied `` J Commands : Display is a command which \ [ Display : this command ... Export is a command allowing ... . I Export : this command ... Print is a command which ... . \ [ Print : this command ... Commands : Display : &lt; function &gt; Export : &lt; function &gt; Print : &lt; function &gt; Figure 3 . ''</sentence>
				<definiendum id="0">Export</definiendum>
				<definiendum id="1">Print</definiendum>
				<definiens id="0">a command which \ [ Display : this command ...</definiens>
			</definition>
			<definition id="3">
				<sentence>Within these patterns , the classifier Nc states the class ( what type of command it is ) while the modifier ( Vc VP ) expresses the specificity .</sentence>
				<definiendum id="0">Within these patterns</definiendum>
				<definiendum id="1">Vc VP )</definiendum>
				<definiens id="0">expresses the specificity</definiens>
			</definition>
			<definition id="4">
				<sentence>The regular lexico-syntactic , layout and typographical patterns which we have called definition patterns have a dual status : they signal definitional text objects as well as being nuclei of a particular type of elaboration schema .</sentence>
				<definiendum id="0">definition patterns</definiendum>
				<definiens id="0">they signal definitional text objects as well as being nuclei of a particular type of elaboration schema</definiens>
			</definition>
			<definition id="5">
				<sentence>A problem for RST : the need for multi-level discourse analysis , Computational Linguistics , 18/4 , pp .</sentence>
				<definiendum id="0">problem for RST</definiendum>
				<definiens id="0">the need for multi-level discourse analysis</definiens>
			</definition>
</paper>

		<paper id="1207">
			<definition id="0">
				<sentence>Po ( TIG ) = axgmax a PQ ( T ) = axgmaxPo ( a ) • Pq ( TIG ) G Assuming the Markov property we have k PQ ( TIG ) = I '' I PQ ( T~IG , ) ( 2 ) i -- -- 1 and ( using a trigram model ) k po ( G ) : IX Po ( V , IGi-2 , G , -1 ) ( a ) i=1 The contexts are smoothed by linear interpolation of unigrams , bigrams , and trigrams .</sentence>
				<definiendum id="0">Po</definiendum>
				<definiendum id="1">k po</definiendum>
				<definiens id="0">V , IGi-2 , G , -1 ) ( a ) i=1 The contexts are smoothed by linear interpolation of unigrams , bigrams , and trigrams</definiens>
			</definition>
			<definition id="1">
				<sentence>in Tel Aviv living 'a poet living in Tel Aviv ' Figure 3 : Structural tags ( Ratnaparkhi , 1997 ) uses an iterative procedure to assign two types of tags ( start X and join X , where X denotes the type of the phrase ) combined with a process to build trees .</sentence>
				<definiendum id="0">Structural tags</definiendum>
				<definiendum id="1">X</definiendum>
			</definition>
			<definition id="2">
				<sentence>proc compare ( A , B ) for each non-terminal node X in A : search node Y in B such that yield ( X ) = yield ( Y ) if Y exists : emit different labels if any if Y does not exist : emit X and its yield end end Figure 4 : Basic asymmetric algorithm to compare annotation A with annotation B of the same sentence ( Calder , 1997 ) presents a method of comparing the structure of context free trees found in different annotations .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">Basic asymmetric algorithm to compare annotation A with annotation B of the same sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>Therefore , we normalize the probabilities w.r.t. the number of nodes in the annotation , which yields the perplexity PP ( A ) of an annotation A : PP ( A ) =~'p~A ) ( 6 ) The procedures of tree matching and probability calculation were applied to our corpus , which currently consists of approx .</sentence>
				<definiendum id="0">: PP</definiendum>
				<definiens id="0">yields the perplexity PP ( A ) of an annotation A</definiens>
			</definition>
			<definition id="4">
				<sentence>, and ( 2 ) the number of correctly assigned grammatical functions ( is a word a head , modifier , subject , etc. ? )</sentence>
				<definiendum id="0">grammatical functions</definiendum>
				<definiens id="0">a word a head</definiens>
			</definition>
			<definition id="5">
				<sentence>At the node level ( non-terminals , phrases ) we measure ( 3 ) the number of identical nodes , i.e. , if there is a node in one annotation , we check whether it corresponds to a node in the other annotation having the same yield .</sentence>
				<definiendum id="0">node level</definiendum>
				<definiens id="0">a node in the other annotation having the same yield</definiens>
			</definition>
</paper>

		<paper id="1211">
			<definition id="0">
				<sentence>Magerman ( 10 ) grounded this in the idea that a parse tree is constructed by a sequence of generalized derivation actions and the derivation probability is the parse probability , a framework that is sometimes referred to as history-based parsing ( 2 ) , at least when decision trees are employed to determine the probability of each derivation action taken .</sentence>
				<definiendum id="0">derivation probability</definiendum>
				<definiens id="0">at least when decision trees are employed to determine the probability of each derivation action taken</definiens>
			</definition>
			<definition id="1">
				<sentence>The Constraint Grammar framework ( 9 ) introduced by Fred Karlsson and championed by Atro Voutilainen is a grammar formalism without derivations .</sentence>
				<definiendum id="0">Constraint Grammar framework</definiendum>
				<definiendum id="1">championed by Atro Voutilainen</definiendum>
				<definiens id="0">a grammar formalism without derivations</definiens>
			</definition>
</paper>

		<paper id="0213">
			<definition id="0">
				<sentence>CabriII ( or Cabri-g~om~tre II ) is a direct manipulation program for interactive `` exploration '' of geometrical diagrams ( Laborde , 85 ) .</sentence>
				<definiendum id="0">CabriII</definiendum>
				<definiens id="0">a direct manipulation program for interactive `` exploration '' of geometrical diagrams</definiens>
			</definition>
			<definition id="1">
				<sentence>All objects ( for example , geometric objects and interface elements ) are manipulated directly .</sentence>
				<definiendum id="0">All objects</definiendum>
				<definiens id="0">for example , geometric objects and interface elements ) are manipulated directly</definiens>
			</definition>
			<definition id="2">
				<sentence>Specific tools associated with the relevant domain ( dynamic geometry ) are also useful .</sentence>
				<definiendum id="0">Specific tools</definiendum>
				<definiens id="0">associated with the relevant domain ( dynamic geometry ) are also useful</definiens>
			</definition>
			<definition id="3">
				<sentence>Ubiquity is the ability to be in several places at the same time .</sentence>
				<definiendum id="0">Ubiquity</definiendum>
				<definiens id="0">the ability to be in several places at the same time</definiens>
			</definition>
			<definition id="4">
				<sentence>CabriII produces demonstration strings which help the user to choose which objects to select and to understand how they will be used by the current tool .</sentence>
				<definiendum id="0">CabriII</definiendum>
				<definiens id="0">produces demonstration strings which help the user to choose which objects to select and to understand how they will be used by the current tool</definiens>
			</definition>
</paper>

		<paper id="0504">
			<definition id="0">
				<sentence>A word order domain is a set of words , generalizing the notion of positions in DUG .</sentence>
				<definiendum id="0">word order domain</definiendum>
				<definiens id="0">a set of words , generalizing the notion of positions in DUG</definiens>
			</definition>
			<definition id="1">
				<sentence>LFG posits several different representation levels , called projections .</sentence>
				<definiendum id="0">LFG</definiendum>
			</definition>
			<definition id="2">
				<sentence>33 I I I I I l I i i I I I II I l I i I I The plattform used is the Xerox Linstructure is a projective tree over the input .</sentence>
				<definiendum id="0">Xerox Linstructure</definiendum>
				<definiens id="0">a projective tree over the input</definiens>
			</definition>
			<definition id="3">
				<sentence>For this , we annotate in every c-structure rule the category of the head word with the template ~ ( HEAV ) , which identifies the head word 's f-structure with the order domain 's f-structure ( cf. ( 9 ) ) .</sentence>
				<definiendum id="0">HEAV</definiendum>
			</definition>
			<definition id="4">
				<sentence>These paths are of the form p d , where p is a ( possibly empty ) regular expression over dependency attributes , and d is a dependency attribute , d names the dependency relation the modifier finally fills , while p describes the path of dependencies which may separate the positional from the direct head of the modifier .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">a ( possibly empty ) regular expression over dependency attributes</definiens>
			</definition>
			<definition id="5">
				<sentence>Slot Grammar : A System for Simpler Construction of Practical Natural Language Grammars .</sentence>
				<definiendum id="0">Slot Grammar</definiendum>
				<definiens id="0">A System for Simpler Construction of Practical Natural Language Grammars</definiens>
			</definition>
			<definition id="6">
				<sentence>Formal Devices for Linguistic Generalizations : West Germanic Word Order in LFG .</sentence>
				<definiendum id="0">Formal Devices for Linguistic Generalizations</definiendum>
				<definiens id="0">West Germanic Word Order in LFG</definiens>
			</definition>
</paper>

		<paper id="0715">
			<definition id="0">
				<sentence>WordNet ( Miller , 1990 ) has been used as a general resource of broad-coverage lexical information in many Natural Language Processing ( NLP ) tasks , including sense tagging , text summarization and machine translation .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="1">
				<sentence>However , like other large-scale knowledge-base systems or machine readable dictionaries ( MRDs ) , WordNet contains massive ambiguity and redundancy .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">contains massive ambiguity and redundancy</definiens>
			</definition>
			<definition id="2">
				<sentence>To this end , WordNet is a good resource because word senses ( or synsets ) are organized in taxonomies .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">a good resource because word senses ( or synsets ) are organized in taxonomies</definiens>
			</definition>
			<definition id="3">
				<sentence>In our present work however , WordNet is compiled from human lexicographers ' entries , thus the data has a fair amount of arbitrariness ( i.e. , noisy data ) .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="4">
				<sentence>First , for all type-pairs extracted from the ambiguous words in WordNet , mutual information is computed to obtain the association by using the standard formula : for type tl , t2 , a mutual information I ( tl , t2 ) is f ( tt^t.- ) l ( tl , t2 ) -lg l ( ttlx l ( tt ) N N where f ( t ) is the number of occurrence of the type t , and N is the size of the data .</sentence>
				<definiendum id="0">t2</definiendum>
				<definiendum id="1">f</definiendum>
				<definiendum id="2">N</definiendum>
				<definiens id="0">the ambiguous words in WordNet , mutual information is computed to obtain the association by using the standard formula : for type tl , t2 , a mutual information I ( tl ,</definiens>
				<definiens id="1">the number of occurrence of the type t , and</definiens>
			</definition>
			<definition id="5">
				<sentence>A partition is an ordered set of basic types ( abstracted from the fine-grained word senses in the first step ) keyed by the primary type emcompassing the secondary types .</sentence>
				<definiendum id="0">partition</definiendum>
				<definiens id="0">an ordered set of basic types ( abstracted from the fine-grained word senses in the first step</definiens>
			</definition>
			<definition id="6">
				<sentence>6 As an example , the verb write ( VCR-COMM-PCR-CHA ) takes an object noun paper ( AFT-COHM ) in a sentence in Brown corpus In 19 , J8 , Afranio Do Amaral , the noted Brazilian herpetologist , wrote a technical paper on the giant snakes .</sentence>
				<definiendum id="0">VCR-COMM-PCR-CHA )</definiendum>
				<definiens id="0">takes an object noun paper ( AFT-COHM ) in a sentence in Brown corpus In 19</definiens>
			</definition>
			<definition id="7">
				<sentence>For example , a class PKR-COG-CONT consists of two binary relations : PER-COG ( to reason about what is perceived , eg .</sentence>
				<definiendum id="0">class PKR-COG-CONT</definiendum>
				<definiens id="0">consists of two binary relations : PER-COG ( to reason about what is perceived , eg</definiens>
			</definition>
			<definition id="8">
				<sentence>Such information , which captures the implicit , complicated interactions between different aspects of an action which may involve implied objects , can be encoded in a structured lexical representation that is along the same line of some recent research in lexical semantics ( eg .</sentence>
				<definiendum id="0">Such information</definiendum>
				<definiens id="0">captures the implicit , complicated interactions between different aspects of an action which may involve implied objects</definiens>
			</definition>
</paper>

		<paper id="0707">
			<definition id="0">
				<sentence>But many such syntactically idiosyncratic idiom strings raise a second problem having to do with their conceptual-semantic rather than their syntactic nature .</sentence>
				<definiendum id="0">idiosyncratic idiom strings</definiendum>
				<definiens id="0">a second problem having to do with their conceptual-semantic rather than their syntactic nature</definiens>
			</definition>
</paper>

		<paper id="0512">
			<definition id="0">
				<sentence>Function Annotation Generalizing over approaches adopted in DGbased parsing of Chinese , Lai and Huang ( 1994 ) noted that grammatical functions like subject and object are generally used to label dependency links .</sentence>
				<definiendum id="0">Function Annotation Generalizing</definiendum>
				<definiens id="0">grammatical functions like subject and object are generally used to label dependency links</definiens>
			</definition>
			<definition id="1">
				<sentence>i PATR outputs a phrase-structure tree , but after Control sentences are one of the motivations applying a pruning operation on the two tv 's , a for DG grammarians like Hudson ( 1994 ) to give structure equivalent to Fig .</sentence>
				<definiendum id="0">PATR</definiendum>
				<definiens id="0">outputs a phrase-structure tree , but after Control sentences are one of the motivations applying a pruning operation on the</definiens>
			</definition>
			<definition id="2">
				<sentence>R ule VS -- &gt; \ [ V , N\ ] : sub j , tv , VS : slash : here - ' `` VS : cat V : cat , V : cat -v , V : subcat : tran N : cat `` n , VS : ds V : ds , By introducing a level of grammatical function to accommodate such complications , Lai and Huang ( 1995 ; in press ) preserve single-headedness and projectivity in the syntactic dependency structure as in Fig .</sentence>
				<definiendum id="0">V</definiendum>
				<definiendum id="1">VS</definiendum>
				<definiens id="0">cat V : cat , V : cat -v</definiens>
			</definition>
			<definition id="3">
				<sentence>The pop operation , which is procedural in nature ( hence the braces ) , hands over to the caller the head elements of a the subcategorization list , removing it from the list at the same time .</sentence>
				<definiendum id="0">pop operation</definiendum>
				<definiens id="0">procedural in nature ( hence the braces ) , hands over to the caller the head elements of a the subcategorization list , removing it from the list at the same time</definiens>
			</definition>
			<definition id="4">
				<sentence>Grammatical function constraints , which work like functional annotations in LFG , provide the main facilities to resolve grammatical problems like control .</sentence>
				<definiendum id="0">Grammatical function constraints</definiendum>
				<definiens id="0">work like functional annotations in LFG</definiens>
			</definition>
</paper>

		<paper id="1308">
			<definition id="0">
				<sentence>Haskell code consists of definitions of data types and of functions between data types -- -one can think of it as LISP with types .</sentence>
				<definiendum id="0">Haskell code</definiendum>
				<definiens id="0">consists of definitions of data types and of functions between data types -- -one can think of it as LISP with types</definiens>
			</definition>
			<definition id="1">
				<sentence>A syllable is a string that begins with an optional sequence of consonants and continues by a nonempty sequence of vowels followed by an optional sequence of consonants .</sentence>
				<definiendum id="0">syllable</definiendum>
				<definiens id="0">a string that begins with an optional sequence of consonants and continues by a nonempty sequence of vowels followed by an optional sequence of consonants</definiens>
			</definition>
			<definition id="2">
				<sentence>X ... .. Y. ) C , where C is an already defined regular expression possibly containing the variable symbols X , ... , Y ( these symbols of course can not be used as names of these letters in C -- but we need not reserve a special class of variable symbols ) .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">an already defined regular expression possibly containing the variable symbols X , ...</definiens>
			</definition>
			<definition id="3">
				<sentence>; where Np , Vp , and S are some previously defined sets of noun phrases , verb phrases , and sentences , respectively .</sentence>
				<definiendum id="0">Np</definiendum>
				<definiendum id="1">S</definiendum>
			</definition>
</paper>

		<paper id="1401">
			<definition id="0">
				<sentence>They might be inhabited by user-directed avatars that manipulate objects in the world and lifelike agents that will need to coordinate speech , gesture , and locomotion as they explain and demonstrate complex phenomena .</sentence>
				<definiendum id="0">locomotion</definiendum>
				<definiens id="0">user-directed avatars that manipulate objects in the world and lifelike agents that will need to coordinate speech , gesture , and</definiens>
			</definition>
			<definition id="1">
				<sentence>Learning environments for the basic sciences frequently focus on physical structures and the fundamental forces that act on them in the world , and training systems for technical domains often revolve around the structure and function of complex devices .</sentence>
				<definiendum id="0">Learning environments</definiendum>
				<definiens id="0">for the basic sciences frequently focus on physical structures and the fundamental forces that act on them in the world , and training systems for technical domains often revolve around the structure and function of complex devices</definiens>
			</definition>
			<definition id="2">
				<sentence>The VIRTUAL COMPUTER ( Bares et al. 1998 ; Bares , Zettlemoyer , &amp; Lester 1998 ) ( Figure 3 ) is a habitable 3D learning environment that teaches novices the fundamentals of computer architecture and system algorithms , e.g. , the fetch-execute cycle .</sentence>
				<definiendum id="0">VIRTUAL COMPUTER</definiendum>
				<definiens id="0">a habitable 3D learning environment that teaches novices the fundamentals of computer architecture and system algorithms</definiens>
			</definition>
</paper>

		<paper id="0207">
			<definition id="0">
				<sentence>A multimedia system for digital dissemination of knowledge usually consists of editors , search tools , and browser .</sentence>
				<definiendum id="0">multimedia system</definiendum>
				<definiens id="0">consists of editors , search tools , and browser</definiens>
			</definition>
</paper>

		<paper id="0614">
			<definition id="0">
				<sentence>Metonymy belongs to a variety of natural language phenomena that contribute to expressing information in an effective and economic way .</sentence>
				<definiendum id="0">Metonymy</definiendum>
				<definiens id="0">a variety of natural language phenomena that contribute to expressing information in an effective and economic way</definiens>
			</definition>
			<definition id="1">
				<sentence>The TACITUS system ( Hobbs , Martin 1987 ) uses similar methods for dealing with metonymy and for interpreting noun-noun components , which are considered special cases of reference resolution that approach , which is also described in ( Hobbs et al. 1993 ) , treats interpretation as a uniform abduction process to find the best explanation for the observables ' .</sentence>
				<definiendum id="0">TACITUS system</definiendum>
				<definiendum id="1">interpreting noun-noun components</definiendum>
				<definiens id="0">treats interpretation as a uniform abduction process to find the best explanation for the observables '</definiens>
			</definition>
			<definition id="2">
				<sentence>These sentences demonstrate that both intra ( ( 1 ) and ( 3 ) ) and intersentential ( ( la ) and ( 3b ) ) prononminal reference work fine , if the literal referents ( here , various sorts of food ) and the real referents ( here , the persons ) agree in number .</sentence>
				<definiendum id="0">intersentential</definiendum>
				<definiens id="0">various sorts of food ) and the real referents ( here , the persons ) agree in number</definiens>
			</definition>
			<definition id="3">
				<sentence>We distinguish several types of quantifiers to cover the cases elaborated in the previous section , in addition to the standard quantifiers EXIST and WH ( the first two constitute default information , and the others express definitional restrictions ) : FORMAL TELIC fruit-dumpling ( x ) CONST = { dough , fruit , ... I = eatable ( x ) = ( DEFSINGLE y ( DEFMULTIPLE x ( eat ( er , y , x ) ) ) ) AGENTIVE = cook ( e'r , z , x ) FORMAL TELIC meat-plate ( x ) CONST = { pork , beef , .</sentence>
				<definiendum id="0">FORMAL TELIC meat-plate</definiendum>
				<definiens id="0">TELIC fruit-dumpling ( x ) CONST = { dough , fruit , ... I = eatable ( x ) = ( DEFSINGLE y ( DEFMULTIPLE x ( eat ( er , y , x ) ) ) ) AGENTIVE = cook ( e'r , z , x )</definiens>
			</definition>
			<definition id="4">
				<sentence>FORMAL = organization ( x ) TELIC = ( SINGLE x ( MULTIPLE y FLIGHT ( organize ( eT , y , x ) ) ) ) AGENTIVE = found ( e'r , z , x ) flight ( x ) CONST = { place , source ... . } FORMAL = location-change ( x ) TELIC = ( SINGLE x ( DEFMULTIPLE y PERSON ( carry ( e r , y , x ) ) ) ) AGENTIVE = organize ( e'r , z , x ) Figure 3 : Some 'extended ' examples of Qualia Structures , for the nouns 'office ' , 'airline ' , and 'flight ' 108 The representation is composed as an expression of the form ( Qs xE Sz &lt; P &gt; ) , with XE being the variable whose representation is to be extended ( initially equal to x , denoting the literal referent ) , QE being its quantifier , and SE its sort ( initially equal to Q and S , which are associated with the literal referent ) , and &lt; P &gt; being an eventually structured representation of the sentence predicate and its modifiers .</sentence>
				<definiendum id="0">MULTIPLE y FLIGHT</definiendum>
				<definiendum id="1">SE</definiendum>
			</definition>
			<definition id="5">
				<sentence>If SN is compatible with SR , but Qr~ is incompatible with QR , then the expression is expanded as under 2a , by a MEMBER relation between xe and xN .</sentence>
				<definiendum id="0">Qr~</definiendum>
				<definiens id="0">compatible with SR , but</definiens>
			</definition>
</paper>

		<paper id="1435">
			<definition id="0">
				<sentence>Interactive• Generation is a viable alternative to Automatic Translation .</sentence>
				<definiendum id="0">Interactive• Generation</definiendum>
			</definition>
			<definition id="1">
				<sentence>MultiMeteo is a 3-year project funded partially by the Language Engineering programme of the European Commission , and partially by European weather offices .</sentence>
				<definiendum id="0">MultiMeteo</definiendum>
				<definiens id="0">a 3-year project funded partially by the Language Engineering programme of the European Commission , and partially by European weather offices</definiens>
			</definition>
			<definition id="2">
				<sentence>: Once the forecaster saves the result , MultiMeteo generates the weather forecast in all the selected • languages ( which are English , Spanish and German ) .</sentence>
				<definiendum id="0">MultiMeteo</definiendum>
			</definition>
</paper>

		<paper id="1228">
			<definition id="0">
				<sentence>Focal attention ( Niebur and Koch , 1995 ) , ( Niebur and Koch , 1997 ) is a sequential search through a series of progressively less salient locations , selection being driven primarily from below saliency being determined from the contributions of elementary features extracted during the preattentive phase .</sentence>
				<definiendum id="0">Focal attention</definiendum>
				<definiens id="0">a sequential search through a series of progressively less salient locations , selection being driven primarily from below saliency being determined from the contributions of elementary features extracted during the preattentive phase</definiens>
			</definition>
			<definition id="1">
				<sentence>Network learning depends upon presentation of frames exemplifying each of these phases , and object tagging ( identification of objects as respectively TR and LM ) relies upon `` visual search '' initiated by parsing of the language fragment , and subsequent binding of object feature and location information .</sentence>
				<definiendum id="0">object tagging</definiendum>
				<definiens id="0">identification of objects as respectively TR and LM ) relies upon `` visual search '' initiated by parsing of the language fragment , and subsequent binding of object feature and location information</definiens>
			</definition>
</paper>

		<paper id="1122">
			<definition id="0">
				<sentence>The phrase acquisition method is a greedy algorithm that performs local optimization based on an iterative process which converges to a local minimum of PP ( T ) .</sentence>
				<definiendum id="0">phrase acquisition method</definiendum>
			</definition>
			<definition id="1">
				<sentence>In MI ( z , y ) are such that P ( z , y ) _ lThe perplexity PP ( T ) of a corpus 7 '' is PP ( T ) = exp ( -~ log P ( T ) ) , where n is the number of words in T. : aWe ranked symbol pairs and increased the phrase length by successive iteration .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">z , y ) _ lThe perplexity PP ( T ) of a corpus 7 '' is PP ( T ) = exp ( -~ log P ( T )</definiens>
				<definiens id="1">the number of words in T. : aWe ranked symbol pairs and increased the phrase length by successive iteration</definiens>
			</definition>
			<definition id="2">
				<sentence>( T ) fl~ , , ( T ) ( 4 ) 1 to ( 7 ) = mart , i ( 5 ) where/~,1 ( 7 '' ) is the entropy of the corpus 7 '' based on the phrase n-gram model Aft and Ao is the initial model and equation 5 follows from equation 4 in the sense of the law of large numbers .</sentence>
				<definiendum id="0">Ao</definiendum>
				<definiens id="0">the entropy of the corpus 7 '' based on the phrase n-gram model</definiens>
				<definiens id="1">the initial model and equation 5 follows from equation 4 in the sense of the law of large numbers</definiens>
			</definition>
</paper>

		<paper id="0709">
			<definition id="0">
				<sentence>Monolingual dictionaries have been used basically as a source for extracting taxonomic ( hypemym ) links between words ( or senses \ [ Bruce &amp; Guthrie 92\ ] , \ [ Rigau et al. 97\ ] ) and in lower extent for extracting other kinds of semantic relations \ [ Richardson 97\ ] ( e.g. meronymic links ) .</sentence>
				<definiendum id="0">Monolingual dictionaries</definiendum>
				<definiens id="0">a source for extracting taxonomic ( hypemym ) links between words ( or senses \ [ Bruce &amp; Guthrie 92\ ] , \ [ Rigau et al. 97\ ] ) and in lower extent for extracting other kinds of semantic relations \ [ Richardson 97\ ] ( e.g. meronymic links</definiens>
			</definition>
			<definition id="1">
				<sentence>• Filter I ( F1 ) removes all FOOD genus terms not assigned to the FOOD semantic file during the mapping process between the bilingual dictionary and WN .</sentence>
				<definiendum id="0">Filter I</definiendum>
				<definiens id="0">F1 ) removes all FOOD genus terms not assigned to the FOOD semantic file during the mapping process between the bilingual dictionary and WN</definiens>
			</definition>
			<definition id="2">
				<sentence>The experiment was carried out on four file senses which in our opinion would differ in their behaviour , food and artifact , which are classified very similarly in Spanish and in English , and mental process and communication , which are not so dear : .</sentence>
				<definiendum id="0">artifact</definiendum>
				<definiens id="0">are classified very similarly in Spanish and in English , and mental process and communication</definiens>
			</definition>
			<definition id="3">
				<sentence>First , following \ [ Atserias et al. 97\ ] , we applied a set of complementary techniques for linking Spanish and Catalan words collected from a bilingual MRDs ( for nouns ) and lexicons ( for verbs ) to English WordNet .</sentence>
				<definiendum id="0">Catalan</definiendum>
				<definiens id="0">words collected from a bilingual MRDs ( for nouns ) and lexicons ( for verbs ) to English WordNet</definiens>
			</definition>
</paper>

		<paper id="1424">
			<definition id="0">
				<sentence>Muitex is a generator in thePenman tradition .</sentence>
				<definiendum id="0">Muitex</definiendum>
				<definiens id="0">a generator in thePenman tradition</definiens>
			</definition>
			<definition id="1">
				<sentence>KPML includes Penman , but it goes considerably beyond the original Penman system , eg .</sentence>
				<definiendum id="0">KPML</definiendum>
			</definition>
			<definition id="2">
				<sentence>The meaning base publishes an extensive Application Programming Interfaces ( APIs ) , which NLP processes can use to access and to reason about the vast resources managed in the meaning base .</sentence>
				<definiendum id="0">meaning base</definiendum>
				<definiendum id="1">extensive Application Programming Interfaces ( APIs )</definiendum>
				<definiendum id="2">NLP</definiendum>
				<definiens id="0">processes can use to access and to reason about the vast resources managed in the meaning base</definiens>
			</definition>
			<definition id="3">
				<sentence>Multex uses interstratal mapping patterns to efficiently draft semantic and lexicogrammatical plans , and it uses constraint-posting and plan-criticising methods to refine , reject and regenerate the drafted plans .</sentence>
				<definiendum id="0">Multex</definiendum>
				<definiens id="0">uses interstratal mapping patterns to efficiently draft semantic and lexicogrammatical plans , and it uses constraint-posting and plan-criticising methods to refine</definiens>
			</definition>
			<definition id="4">
				<sentence>The generation algorithm consists of a plar~ controller that controls the execution of a number of small content-creation agents , called Meaning Agents .</sentence>
				<definiendum id="0">generation algorithm</definiendum>
				<definiens id="0">consists of a plar~ controller that controls the execution of a number of small content-creation agents , called Meaning Agents</definiens>
			</definition>
			<definition id="5">
				<sentence>Each Meaning Agent specialises in generating a particular kind of meaning and modality .</sentence>
				<definiendum id="0">Meaning Agent</definiendum>
				<definiens id="0">specialises in generating a particular kind of meaning and modality</definiens>
			</definition>
			<definition id="6">
				<sentence>The information consumer is an agent who needs data from the information producer but can not process the data directly ; the information consumer needs to consume the data in a processed form .</sentence>
				<definiendum id="0">information consumer</definiendum>
				<definiens id="0">an agent who needs data from the information producer but can not process the data directly ; the information consumer needs to consume the data in a processed form</definiens>
			</definition>
			<definition id="7">
				<sentence>In addition , the Meaning Base supplies a full range of methods to populate itself with linguistic resources , as well as the methods to access and reason about the resources it maintains .</sentence>
				<definiendum id="0">Meaning Base</definiendum>
				<definiens id="0">supplies a full range of methods to populate itself with linguistic resources</definiens>
			</definition>
			<definition id="8">
				<sentence>An NLP process is an application or a service that performs some NLP functionality for the information consumer by drawing on the resources in the Meaning Base .</sentence>
				<definiendum id="0">NLP process</definiendum>
				<definiens id="0">an application or a service that performs some NLP functionality for the information consumer by drawing on the resources in the Meaning Base</definiens>
			</definition>
			<definition id="9">
				<sentence>Text generator is a NLP process of Multex , another partially implemented NLP process is a visual navigator of the Meaning Base .</sentence>
				<definiendum id="0">Text generator</definiendum>
				<definiens id="0">a visual navigator of the Meaning Base</definiens>
			</definition>
			<definition id="10">
				<sentence>The meaning base is the implementation of the systemic metalanguage plus the linguistic • resources maintained by Multex .</sentence>
				<definiendum id="0">meaning base</definiendum>
			</definition>
			<definition id="11">
				<sentence>A Java-based Meaning Base Application Programming Interface ( MB API ) , which consists of around 60 metalanguage concepts and over 400 methods , is available for programmers to create NLP processes .</sentence>
				<definiendum id="0">Java-based Meaning Base Application Programming Interface ( MB API )</definiendum>
			</definition>
			<definition id="12">
				<sentence>The plan control layer consists of the processes for : ( 1 ) creating goals to be solved by the meaning agents ; ( 2 ) spawning , scheduling and starting meaning agents ; ( 3 ) introspecting on the local plans generated by the .</sentence>
				<definiendum id="0">plan control layer</definiendum>
				<definiens id="0">consists of the processes for : ( 1 ) creating goals to be solved by the meaning agents ; ( 2 ) spawning , scheduling and starting meaning agents ; ( 3 ) introspecting on the local plans generated by the</definiens>
			</definition>
			<definition id="13">
				<sentence>The meaning agent layer consists of a number of meaning agents .</sentence>
				<definiendum id="0">meaning agent layer</definiendum>
			</definition>
			<definition id="14">
				<sentence>A meaning agent is a self-contained • process that creates a specific kind of meaning in the form of semantic and lexicogrammatical objects by instantiating resources in the meaning base .</sentence>
				<definiendum id="0">meaning agent</definiendum>
				<definiens id="0">a self-contained • process that creates a specific kind of meaning in the form of semantic and lexicogrammatical objects by instantiating resources in the meaning base</definiens>
			</definition>
			<definition id="15">
				<sentence>From the point of view of Multex , HINTS constitutes an information production , for which Multex provides a service in the form of multimodal communicable disease reports .</sentence>
				<definiendum id="0">HINTS</definiendum>
				<definiens id="0">constitutes an information production , for which Multex provides a service in the form of multimodal communicable disease reports</definiens>
			</definition>
</paper>

		<paper id="0804">
			<definition id="0">
				<sentence>The recognition rules have been tested over the SpeechDat Spanish corpus , used to train the recogniser , and on the synthesis corpus , including both isolated words and sentences ; however , single words composing sentences have been separately treated and then reassembled , because restructuring rules at word boundary to account for inter-word coarticulation were not yet implemented .</sentence>
				<definiendum id="0">recognition rules</definiendum>
				<definiens id="0">tested over the SpeechDat Spanish corpus , used to train the recogniser</definiens>
			</definition>
</paper>

		<paper id="0301">
			<definition id="0">
				<sentence>In this paper , I present a surface-based algorithm that uses cue phrases ( connectives ) in order to deterrnine not only the elementary textual units of text but also the phrases that have a discourse function .</sentence>
				<definiendum id="0">surface-based algorithm</definiendum>
			</definition>
			<definition id="1">
				<sentence>Whenever a cue phrase is detected , the shallow analyzer executes an action from a predetermined set , whose effect is one of the following : create an elementary textual unit boundary in the input text stream ; or set a flag .</sentence>
				<definiendum id="0">shallow analyzer</definiendum>
				<definiens id="0">executes an action from a predetermined set , whose effect is one of the following : create an elementary textual unit boundary in the input text stream</definiens>
			</definition>
			<definition id="2">
				<sentence>The names and the intended semantics of the actions used by the shallow analyzer are : • Action NOTHING instructs the shallow analyzer to treat the cue phrase under consideration as a simple word .</sentence>
				<definiendum id="0">Action NOTHING</definiendum>
				<definiens id="0">instructs the shallow analyzer to treat the cue phrase under consideration as a simple word</definiens>
			</definition>
			<definition id="3">
				<sentence>• Action END instructs the analyzer to insert a textual boundary immediately after the cue phrase .</sentence>
				<definiendum id="0">Action END</definiendum>
				<definiens id="0">instructs the analyzer to insert a textual boundary immediately after the cue phrase</definiens>
			</definition>
			<definition id="4">
				<sentence>• Action MATCH_DASH instructs the analyzer to insert a textual boundary before the occurrence of the cue phrase .</sentence>
				<definiendum id="0">Action MATCH_DASH</definiendum>
				<definiens id="0">instructs the analyzer to insert a textual boundary before the occurrence of the cue phrase</definiens>
			</definition>
			<definition id="5">
				<sentence>• Actions SET_AND ( SET_OR ) instructs the analyzer to store the information that the input stream contains the lexeme and ( or ) .</sentence>
				<definiendum id="0">SET_OR )</definiendum>
				<definiens id="0">instructs the analyzer to store the information that the input stream contains the lexeme and ( or )</definiens>
			</definition>
			<definition id="6">
				<sentence>\ ] ( 11 ) \ [ John is a nice guy , \ ] \ [ although he made a couple of nasty remarks last night .</sentence>
				<definiendum id="0">John</definiendum>
				<definiens id="0">a nice guy , \ ] \</definiens>
			</definition>
</paper>

		<paper id="1206">
			<definition id="0">
				<sentence>Grammars A PCFG is a CFG in which each production A ~ a in the grammar 's set of productions R is associated with an emission probability P ( A a ) that satisfies a normalization constraint y~ P ( A~a ) = 1 ~ : A -- +c~ER and a consistency or tightness constraint not discussed here .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">a CFG in which each production A ~ a in the grammar 's set of productions R is associated with an emission probability P ( A a ) that satisfies a normalization constraint y~ P ( A~a ) = 1 ~ : A -- +c~ER and a consistency or tightness constraint not discussed here</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus the estimated likelihoods using P1 of the tree structures ( A1 ) and ( B1 ) are : 4f 2 ~1 ( A1 ) ( 2 + f ) 3 4 ( i f ) PI ( B1 ) ( 2 + f ) 2 '' Clearly PI ( A1 ) &lt; f and PI ( B1 ) &lt; ( 1 f ) except at f = 0 and f = 1 , so in general the estimated frequencies using P1 differ from the frequencies of A1 and B1 in the training corpus .</sentence>
				<definiendum id="0">PI</definiendum>
				<definiens id="0">general the estimated frequencies using P1 differ from the frequencies of A1 and B1 in the training corpus</definiens>
			</definition>
			<definition id="2">
				<sentence>F22 : the trees from the F22 subcorpus of the Penn II tree bank , F22 Id : the maximum likelihood parses of the yields of the F22 subcorpus using the PCFG estimated from the F22 subcorpus itself , Id : the maximum likelihood parses of the yields of the F22 subcorpus using the PCFG estimated from the F2-21 subcorpus ( i.e. , this corresponds to applying an identity transform ) , Parent : as above , except that the parent annotation transform described in subsection 4.3 was used in training and evaluation , VP : as in Id , except that the flat VP structures used in the Penn II tree bank were transformed into recursive Chomsky adjunction structures as described below , NP : as above , except that the one-level NP structures used in the Penn II tree bank were transformed into recursive Chomsky adjunction structures , and VP-NP : as above , except that both NP and VP structures were transformed into recursive Chomsky adjunction structures .</sentence>
				<definiendum id="0">F22</definiendum>
				<definiendum id="1">Id</definiendum>
				<definiendum id="2">Parent</definiendum>
				<definiens id="0">using the PCFG estimated from the F22 subcorpus itself ,</definiens>
			</definition>
			<definition id="3">
				<sentence>The F22 Id PCFG gives data on the case where the PCFG is trained on the same data that it is evaluated on , namely the F22 subcorpus .</sentence>
				<definiendum id="0">F22 Id PCFG</definiendum>
				<definiens id="0">gives data on the case where the PCFG is trained on the same data that it is evaluated on</definiens>
			</definition>
			<definition id="4">
				<sentence>as a multiset or bag E ( ~ ) of edges , i.e. , triples ( N , l , r &gt; where N is a nonterminal label and l and r are left and right string positions in yield of the entire corpus .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a multiset or bag E ( ~ ) of edges , i.e. , triples ( N , l , r &gt; where</definiens>
				<definiens id="1">a nonterminal label and l and r are left and right string positions in yield of the entire corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>Precision is the fraction of edges in the tree sequence to be evaluated which also appear in the test sequence , and recall is the fraction of edges in the test sequence which also appear in sequence to be evaluated .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiendum id="1">recall</definiendum>
				<definiens id="0">the fraction of edges in the tree sequence to be evaluated which also appear in the test sequence</definiens>
				<definiens id="1">the fraction of edges in the test sequence which also appear in sequence to be evaluated</definiens>
			</definition>
</paper>

		<paper id="0719">
			<definition id="0">
				<sentence>Lexical FreeNet is a semantic network that leverages WordNet and other knowledge and data sources to facilitate the discovery of nontrivial lexical connections between words and concepts .</sentence>
				<definiendum id="0">Lexical FreeNet</definiendum>
				<definiens id="0">a semantic network that leverages WordNet and other knowledge and data sources to facilitate the discovery of nontrivial lexical connections between words and concepts</definiens>
			</definition>
			<definition id="1">
				<sentence>By organizing words semantically rather than alphabetically , WordNet provides a means by which users can navigate its vocabulary logically , establishing connections between concepts and not simply character sequences .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="2">
				<sentence>FreeNet consists of software to do the second and third phases .</sentence>
				<definiendum id="0">FreeNet</definiendum>
				<definiens id="0">consists of software to do the second and third phases</definiens>
			</definition>
			<definition id="3">
				<sentence>The power behind FreeNet lies in the user 's ability to compose primitive relations to build more complex relations that it may use in its queries .</sentence>
				<definiendum id="0">FreeNet</definiendum>
			</definition>
			<definition id="4">
				<sentence>The expression rl r2 denotes the set of pairs ( a , b ) such that for some token c , ( a , c ) E rl and ( c , b ) E r2 .</sentence>
				<definiendum id="0">expression rl r2</definiendum>
			</definition>
			<definition id="5">
				<sentence>136 Conjunction Conjunction takes the intersection of two relations : plainly , the intersection of their respective pair sets .</sentence>
				<definiendum id="0">Conjunction Conjunction</definiendum>
				<definiens id="0">takes the intersection of two relations : plainly , the intersection of their respective pair sets</definiens>
			</definition>
			<definition id="6">
				<sentence>The expression rl • r2 denotes the set of pairs ( a , b ) such that ( a , b ) E rl and ( a , b ) E r2 .</sentence>
				<definiendum id="0">expression rl • r2</definiendum>
			</definition>
			<definition id="7">
				<sentence>The inverse operator swaps every pair : rdenotes the set of pairs ( a , b ) such that ( b.a ) E r. Taking the union of a relation with its inverse produces a new relation that is guaranteed to be symmetric .</sentence>
				<definiendum id="0">inverse operator</definiendum>
				<definiens id="0">the union of a relation with its inverse produces a new relation that is guaranteed to be symmetric</definiens>
			</definition>
			<definition id="8">
				<sentence>The complement operator produces a set containing all pairs but those in a certain relation , r ' denotes the set of pairs ( a , b ) such that ( b , a ) ~r. ( The vocabulary is assumed to be fixed after the graph is built , and so the universe is well-defined . )</sentence>
				<definiendum id="0">complement operator</definiendum>
				<definiens id="0">produces a set containing all pairs but those in a certain relation</definiens>
			</definition>
			<definition id="9">
				<sentence>rX denotes the set of pairs ( a , b ) such that a ~ b and there exists a c such that ( a , c ) E r and ( b , c ) E r. Thus ( parent- ) ~ , relation is the genealogical sibling relation formed by applying the inverse operator and then the sibling operator to the `` parent '' relation .</sentence>
				<definiendum id="0">rX</definiendum>
				<definiendum id="1">relation</definiendum>
				<definiens id="0">the set of pairs</definiens>
			</definition>
			<definition id="10">
				<sentence>FreeNet returns a shortest path ( or all paths ) in the multigraph that match the query , binding the variables in the query to concrete tokens .</sentence>
				<definiendum id="0">FreeNet</definiendum>
				<definiens id="0">returns a shortest path ( or all paths ) in the multigraph that match the query , binding the variables in the query to concrete tokens</definiens>
			</definition>
			<definition id="11">
				<sentence>Below , the `` ANY '' regexp is the union of all available ( or selected ) primitive relations .</sentence>
				<definiendum id="0">ANY '' regexp</definiendum>
				<definiens id="0">the union of all available ( or selected ) primitive relations</definiens>
			</definition>
			<definition id="12">
				<sentence>Lexical FreeNet is an instance of FreeNet supporting a range of lexical semantic applications .</sentence>
				<definiendum id="0">Lexical FreeNet</definiendum>
				<definiens id="0">an instance of FreeNet supporting a range of lexical semantic applications</definiens>
			</definition>
			<definition id="13">
				<sentence>Relations Lexical FreeNet includes seven semantic relations , two phonetic relations , and one orthographic relation .</sentence>
				<definiendum id="0">Lexical FreeNet</definiendum>
				<definiens id="0">includes seven semantic relations , two phonetic relations</definiens>
			</definition>
			<definition id="14">
				<sentence>`` Triggers '' ( ~ ) Trigger pairs are ordered word pairs that co-occur significantly in data ; that is , they are pairs that appear near each other in text more frequently than 138 would be expected if the words were unrelated .</sentence>
				<definiendum id="0">Triggers</definiendum>
				<definiens id="0">pairs are ordered word pairs that co-occur significantly in data</definiens>
			</definition>
			<definition id="15">
				<sentence>Shortest path queries The shortest path query is the primary vehicle for establishing connections between words and concepts : • Shortest path queries that allow all lexical relations can be used to aid in generating puns 2See http : //~w.link.cs.cmu.edu/lexfn/ I I L=xical FreeNet Lee ~ 'm I~ ~ me e~ : e ~I ' -- -~I-'F '' -- -- ~ , -- ~ -- -- -- , - , Figure 3 : The front page of the Web interface to Lexical FreeNet and quips involving the two endpoint concepts .</sentence>
				<definiendum id="0">shortest path query</definiendum>
				<definiens id="0">The front page of the Web interface to Lexical FreeNet and quips involving the two endpoint concepts</definiens>
			</definition>
			<definition id="16">
				<sentence>Every successive word pair exhibits a different sense : 140 ZERO ~ CIPHER ~=~ CALCULATE SYN DIRECT ~ LEAD ~ STAR &lt; ~ ACE &lt; ~ ONE * Using only the trigger ( ~=~ ) relation , one can connect concepts that occur in the domain of the data used to train the trigger pairs , in this case broadcast news : TRG SMOKING ~ CIGARETTES ~ MACHINES COMPUTERS • The trigger relation enriches the WordNetderived vocabulary of common nouns with topical proper names , as in the shortest paths shown below .</sentence>
				<definiendum id="0">trigger relation</definiendum>
				<definiens id="0">enriches the WordNetderived vocabulary of common nouns with topical proper names</definiens>
			</definition>
			<definition id="17">
				<sentence>The coercion function on the Web interface is hardcoded such that the relation ret ( see Section 2.3 ) is simply the union of all semantic relations , and re2 is the union of all phonetic relations .</sentence>
				<definiendum id="0">coercion function</definiendum>
				<definiendum id="1">re2</definiendum>
				<definiens id="0">the union of all phonetic relations</definiens>
			</definition>
</paper>

		<paper id="0206">
			<definition id="0">
				<sentence>SMART tokenizes the transcribed audio , removes common stop words ( e.g. the , o\ ] ) , normalizes word variants ( e.g. persons ~ person ) , and weights term occurrences based upon document length and word frequency .</sentence>
				<definiendum id="0">SMART</definiendum>
				<definiens id="0">tokenizes the transcribed audio , removes common stop words</definiens>
				<definiens id="1">normalizes word variants ( e.g. persons ~ person ) , and weights term occurrences based upon document length and word frequency</definiens>
			</definition>
</paper>

		<paper id="0312">
			<definition id="0">
				<sentence>Theory of Discourse Structure Other theories have defined the relata in discourse structure as clauses ( Trabasso and Sperry , 1985 ) , pieces of text ( Hobbs , 1985 , Mann and Thompson , 1987 ) , pieces of text plus connectives ( Cohen , 1984 , Reichman , 1985 ) , propositions ( Van Dijk and Kintsch , 1983 , Polanyi , 1988 ) , plans ( Lochbaum , Grosz and Sidner , 1990 ) and segmented discourse representation structures , the SDRT theory ( Asher , 1993 ) .</sentence>
				<definiendum id="0">Polanyi</definiendum>
				<definiens id="0">pieces of text ( Hobbs , 1985 , Mann and Thompson , 1987 ) , pieces of text plus connectives ( Cohen , 1984 , Reichman , 1985 ) , propositions ( Van Dijk and Kintsch , 1983 ,</definiens>
			</definition>
			<definition id="1">
				<sentence>A discourse segment is a set of discourse events and entities that cohere among themselves , and share a single coherence relation to another discourse segment ( which could consist of just one event or entity , such as the discourse topic ) .</sentence>
				<definiendum id="0">discourse segment</definiendum>
				<definiens id="0">a set of discourse events and entities that cohere among themselves , and share a single coherence relation to another discourse segment ( which could consist of just one event or entity , such as the discourse topic )</definiens>
			</definition>
			<definition id="2">
				<sentence>Naive semantics is a lexical theory which equates the meaning of a word sense with a concept , so that concept representations and word sense meaning representations have the same form .</sentence>
				<definiendum id="0">Naive semantics</definiendum>
				<definiens id="0">a lexical theory which equates the meaning of a word sense with a concept</definiens>
			</definition>
			<definition id="3">
				<sentence>clear ( X ) &amp; colorless ( X ) &amp; liquid ( X ) The classical theory does n't work because : 1 ) true scientific theories of the nature of categories are not necessarily known by speakers of a language ; 2 ) the categories Concepts name are gradient , with some members better examples than others ; and 3 ) typical 68 properties of objects are n't necessary properties .</sentence>
				<definiendum id="0">liquid ( X</definiendum>
				<definiens id="0">1 ) true scientific theories of the nature of categories are not necessarily known by speakers of a language ; 2</definiens>
			</definition>
			<definition id="4">
				<sentence>Naive semantics posits that word meanings are shallow , limited naive theories of objects and events .</sentence>
				<definiendum id="0">Naive semantics</definiendum>
				<definiens id="0">posits that word meanings are shallow , limited naive theories of objects and events</definiens>
			</definition>
</paper>

		<paper id="0203">
			<definition id="0">
				<sentence>Coreference breaks down into several readily identified areas based on the form of the phrase being resolved and the method of calculating coreference .</sentence>
				<definiendum id="0">Coreference</definiendum>
				<definiens id="0">breaks down into several readily identified areas based on the form of the phrase being resolved and the method of calculating coreference</definiens>
			</definition>
			<definition id="1">
				<sentence>Cross-document coreference occurs when the same person , place , event , or concept is discussed in more than one text source .</sentence>
				<definiendum id="0">Cross-document coreference</definiendum>
				<definiens id="0">occurs when the same person , place , event , or concept is discussed in more than one text source</definiens>
			</definition>
			<definition id="2">
				<sentence>• Next , for the coreference chain of interest within each article ( for example , the coreference chain that contains `` John Perry '' ) , the Sentence Extractor module extracts all the sentences that contain the noun phrases which form the coreference chain .</sentence>
				<definiendum id="0">Sentence Extractor module</definiendum>
				<definiens id="0">extracts all the sentences that contain the noun phrases which form the coreference chain</definiens>
			</definition>
			<definition id="3">
				<sentence>Therefore , for doc.36 ( Figure 2 ) , since at least one of the three noun phrases ( `` John Perry , '' `` he , '' and `` Perry '' ) in the coreference chain of interest appears in each of the three sentences in the extract , the summary produced by SentenceExtractor is the extract itself .</sentence>
				<definiendum id="0">SentenceExtractor</definiendum>
				<definiens id="0">the extract itself</definiens>
			</definition>
			<definition id="4">
				<sentence>The final precision and recall numbers are computed by the following two formulae : N Final Precision = Z wi * Precision~ i=l N Final Recall = ~ wl * Recall~ i=l where N is the number of entities in the document , and wi is the weight assigned to entity i in the document .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">wi</definiendum>
				<definiens id="0">the number of entities in the document , and</definiens>
			</definition>
			<definition id="5">
				<sentence>Figure 7 shows the Precision , Recall , and the FMeasure ( the average of precision and recall with equal weights for both ) statistics for the John Smith data set .</sentence>
				<definiendum id="0">FMeasure</definiendum>
				<definiens id="0">the average of precision and recall with equal weights for both ) statistics for the John Smith data set</definiens>
			</definition>
</paper>

		<paper id="1111">
			<definition id="0">
				<sentence>Language identification is an example of a general class of problems in which we want to assign an input data stream to one of several categories as quickly and accurately as possible .</sentence>
				<definiendum id="0">Language identification</definiendum>
				<definiens id="0">an example of a general class of problems in which we want to assign an input data stream to one of several categories as quickly and accurately as possible</definiens>
			</definition>
			<definition id="1">
				<sentence>To obtain the probability of a language l given a token t , p ( llt ) , we use Bayes ' rule : p ( l\ [ t ) = p ( tll ) p ( l ) p ( t ) where p ( t\ [ l ) is the probability of the token if the language is known , p ( t ) is the a pr/or/probability of the token , and p ( l ) is the a priori probability of the language .</sentence>
				<definiendum id="0">p</definiendum>
				<definiendum id="1">p</definiendum>
				<definiendum id="2">p ( l )</definiendum>
				<definiens id="0">the probability of the token if the language</definiens>
				<definiens id="1">the a pr/or/probability of the token</definiens>
				<definiens id="2">the a priori probability of the language</definiens>
			</definition>
			<definition id="2">
				<sentence>f ( t ) is the count of the token t across all the \ ] languages , and F the count of all tokens across all languages .</sentence>
				<definiendum id="0">f ( t )</definiendum>
				<definiens id="0">the count of the token t across all the \ ] languages</definiens>
			</definition>
			<definition id="3">
				<sentence>For large f ( t , l ) , the underlying probability can be calculated by using the normal approximation to the binomial , giving the base probability ps ( tll ) = ff ( t , t ) f ( t ) The standard deviation of this quantity is a ( t , 1 ) = ~/f ( l ) ps ( tll ) ( 1 ps ( tll ) ) The low and high probabilities are found by taking a given number of standard deviations d from the base probability .</sentence>
				<definiendum id="0">base probability ps</definiendum>
			</definition>
</paper>

		<paper id="0607">
			<definition id="0">
				<sentence>( c ) Classifying : an indefinite NP which provides only the superclass of the item , e.g. , this is a brooch ( d ) Defining : for generic entities , an NP which provides the entities defining characteristics , e.g. , a necklace is an item ofjewellery worn around the neck . . . . . ( e ) Eliciting : a whNP for the referent .</sentence>
				<definiendum id="0">necklace</definiendum>
				<definiens id="0">an indefinite NP which provides only the superclass of the item</definiens>
			</definition>
			<definition id="1">
				<sentence>Sem : the referent of the NP , an entity in the information graph ( or a set of such entities if desired , realised as either a coordinated NP , or using plural anaphora ) .</sentence>
				<definiendum id="0">Sem</definiendum>
				<definiens id="0">the referent of the NP , an entity in the information graph ( or a set of such entities if desired , realised as either a coordinated NP , or using plural anaphora )</definiens>
			</definition>
			<definition id="2">
				<sentence>Orth : the slot to hold the eventual surface string for the NP .</sentence>
				<definiendum id="0">Orth</definiendum>
				<definiens id="0">the slot to hold the eventual surface string for the NP</definiens>
			</definition>
			<definition id="3">
				<sentence>The aggregation module uses this slot to state its requirements from the NP , which facts the NP is to express .</sentence>
				<definiendum id="0">aggregation module</definiendum>
				<definiens id="0">facts the NP is to express</definiens>
			</definition>
			<definition id="4">
				<sentence>Focal Objects : the focal space includes a set of entities which may potentially be referred to as this z. Firstly , we have the Prior-Cb ( backward looking centre , usually the subject of the prior sentence ) .</sentence>
				<definiendum id="0">Focal Objects</definiendum>
				<definiens id="0">backward looking centre , usually the subject of the prior sentence )</definiens>
			</definition>
			<definition id="5">
				<sentence>of the NP We consider the basic structure of the NP to fall into two components : a nucleus , which performs the nominal function of the NP , and optional satelites , where additional information can be placed .</sentence>
				<definiendum id="0">nucleus</definiendum>
				<definiens id="0">performs the nominal function of the NP</definiens>
			</definition>
			<definition id="6">
				<sentence>While this style is more typical of newspaper reporting , where compact information delivery is important , it is still an issue which needs to be addressed in any NP-planner .</sentence>
				<definiendum id="0">compact information delivery</definiendum>
				<definiens id="0">needs to be addressed in any NP-planner</definiens>
			</definition>
			<definition id="7">
				<sentence>Stage 2 : Adding Satelites The Owner fact can be incorporated into the NP as a satelite ( as a non-referring relative clause ) , e.g. , the Granny Smith apple on the table , which John owns .</sentence>
				<definiendum id="0">Owner fact</definiendum>
			</definition>
</paper>

		<paper id="1006">
</paper>

		<paper id="1236">
			<definition id="0">
				<sentence>The sentence analysis is carried out as a CF instantiation process , in which several CFs axe combined to form a Concept Compound ( CC ) , a nested relational structure in which the syntactic and semantic properties of the sentence are encoded .</sentence>
				<definiendum id="0">CF instantiation process</definiendum>
				<definiens id="0">a nested relational structure in which the syntactic and semantic properties of the sentence are encoded</definiens>
			</definition>
			<definition id="1">
				<sentence>The overall structure of the CF is defined as follows : ( I ) c ( g , A , P ) , where C and K are the concept identifier and the key-phrase respectively , A represents a list of attribute values of the concept , and P is the concept pattern which is a sequence of several terms : variables , constants , and the symbol * which represents the key-phrase itself or one of its derivative expressions .</sentence>
				<definiendum id="0">CF</definiendum>
				<definiendum id="1">P</definiendum>
				<definiens id="0">follows : ( I ) c ( g , A , P ) , where C and K are the concept identifier and the key-phrase respectively , A represents a list of attribute values of the concept , and</definiens>
				<definiens id="1">the concept pattern which is a sequence of several terms : variables , constants , and the symbol * which represents the key-phrase itself or one of its derivative expressions</definiens>
			</definition>
			<definition id="2">
				<sentence>The CC is an instantiated CF and is defined as shown in ( 3 ) : ( 3 ) C ( H , R , A , where the concept identifier C is used to indicate the root node of the CC and represents the , whole CC structure ( 3 ) , and H , R , and A are the head , role , and attribute slot respectively .</sentence>
				<definiendum id="0">CC</definiendum>
				<definiens id="0">an instantiated CF and is defined as shown in ( 3 ) : ( 3 ) C ( H , R , A , where the concept identifier C is used to indicate the root node of the CC and represents the</definiens>
			</definition>
			<definition id="3">
				<sentence>1 AI As sketched in the last section , each linguistic expression such as a sentence or a phrase is mapped onto an instance of a CF. For example , the sentence ( 4a ) is mapped onto the CC given in ( 4b ) which is an instance of the sentential CF defined in ( 2 ) .</sentence>
				<definiendum id="0">AI As</definiendum>
				<definiens id="0">a sentence or a phrase is mapped onto an instance of a CF. For example</definiens>
			</definition>
			<definition id="4">
				<sentence>The concept pattern consists of the symbol * , for which box or boxes is to'be substituted .</sentence>
				<definiendum id="0">concept pattern</definiendum>
				<definiens id="0">consists of the symbol * , for which box or boxes is to'be substituted</definiens>
			</definition>
			<definition id="5">
				<sentence>Sentence-to-CC conversion corresponds to sentence analysis , and the obtained CC encodes syntactic and semantic information of the sentence ; the CC representation can be used as a model for sentence analysis .</sentence>
				<definiendum id="0">CC</definiendum>
				<definiendum id="1">CC representation</definiendum>
				<definiens id="0">encodes syntactic and semantic information of the sentence</definiens>
			</definition>
			<definition id="6">
				<sentence>The sentence-CC conversion operations consists of two operators : the sentence-to-CC conversion which invokes the sentence analysis program and parses the input sentence to obtain the corresponding CC as the output , and the CC-to-sentence conversion which generates a sentence corresponding to the indicated CC .</sentence>
				<definiendum id="0">sentence-CC conversion operations</definiendum>
				<definiens id="0">consists of two operators : the sentence-to-CC conversion which invokes the sentence analysis program and parses the input sentence to obtain the corresponding CC as the output , and the CC-to-sentence conversion which generates a sentence corresponding to the indicated CC</definiens>
			</definition>
			<definition id="7">
				<sentence>, pn\ ] , where the integer p~ ( i = 1 , 2 , ... , n ) represents the old position for the pi-th constituent of the CC in question , and the current position of Pi in the list ( 17 ) indicates the new position for that constituent .</sentence>
				<definiendum id="0">n )</definiendum>
				<definiens id="0">represents the old position for the pi-th constituent of the CC in question</definiens>
			</definition>
			<definition id="8">
				<sentence>Examples : ( 19a ) Get the interrogative form of the sentence Hilde began to describe her plan .</sentence>
				<definiendum id="0">Get</definiendum>
				<definiens id="0">the interrogative form of the sentence Hilde began to describe her plan</definiens>
			</definition>
			<definition id="9">
				<sentence>The general format of $ retrieve_cc is as fallows : ( 28 ) $ retrieve_cc ( S electionaIC onditionList , Kawasaki , Takida and Tajima 285 Language Model and Sentence Structure Manipulations RetrievedC C List ) , where SelectionalConditionList is a list of constralnts imposed on CCs to be retrieved .</sentence>
				<definiendum id="0">SelectionalConditionList</definiendum>
				<definiens id="0">a list of constralnts imposed on CCs to be retrieved</definiens>
			</definition>
			<definition id="10">
				<sentence>Each element of the list consists of either of the following terms : ( 29a ) SlotName = ValueList .</sentence>
				<definiendum id="0">element of the list</definiendum>
			</definition>
			<definition id="11">
				<sentence>CCML provides two update operations , i.e. , Sappend_cc and $ delete_cc .</sentence>
				<definiendum id="0">CCML</definiendum>
				<definiens id="0">provides two update operations</definiens>
			</definition>
</paper>

		<paper id="0212">
			<definition id="0">
				<sentence>regNi ) ( SYNSEM ( ( SYN ( ( HEAD ( ( POS-L2 IregNi ) ) ) ) ) ) ) ( EKE '~ { ~ '' ) ) ____~ \ [ ... . feature strucl'~tre ( 2 N ... .. 3 4 ( ( ( SEM ( ( INSTANCE llI ) ) ) ( CONTEXT ( ( RIGHT ( ( SUFFIX + ) ) ) ) ) ( LEMMA i -- .</sentence>
				<definiendum id="0">regNi ) ( SYNSEM</definiendum>
				<definiendum id="1">) )</definiendum>
				<definiendum id="2">SEM</definiendum>
				<definiendum id="3">CONTEXT ( ( RIGHT</definiendum>
				<definiens id="0">( ( SYN ( ( HEAD ( ( POS-L2 IregNi ) ) ) ) )</definiens>
			</definition>
			<definition id="1">
				<sentence>Number\ ] ) ( SYNSEM ( ( SYN ( ( HEAD ( ( POS-L2 INumber\ [ ) ) ) ) ) ) ) ( TREE ... .. ) ) ) ) ... \ ] ( 22 NP ( ~-~'~ ~ '' ~ ) ~ -- ~ ) S 9 ( *OR* ( ( SEM ( ( tiP2 ( ( INSTANCE ( *OR* `` globe '' \ \ t C ' '' \ ] A node may have .</sentence>
				<definiendum id="0">Number\ ] ) ( SYNSEM</definiendum>
				<definiens id="0">( ( SYN ( ( HEAD ( ( POS-L2 INumber\ [ ) ) ) ) ) ) ) ( TREE ... ..</definiens>
			</definition>
			<definition id="2">
				<sentence>Individual parse trees are two-dimensional , the two dimensions being precedence ( the linear order in which the elements occur in the sentence ) and dominance ( th e relation between a mother node and its daughter node ( s ) ) .</sentence>
				<definiendum id="0">dominance</definiendum>
				<definiens id="0">the linear order in which the elements occur in the sentence</definiens>
			</definition>
</paper>

		<paper id="0304">
			<definition id="0">
				<sentence>We argued above that we envision a modular architecture where independent sentence planning modules posit their constraints regarding tense selection , lexicalization , syntactic realization , etc. 6 In case no constraints are put forward by the sentence planning modules , sobald ( Sb ) would be selected , as it is the most specific and at the same time neutral realization .</sentence>
				<definiendum id="0">Sb</definiendum>
				<definiens id="0">a modular architecture where independent sentence planning modules posit their constraints regarding tense selection , lexicalization , syntactic realization</definiens>
			</definition>
</paper>

		<paper id="1416">
			<definition id="0">
				<sentence>To this end , NAG consults a normative model , which contains our best understanding of the domain of discourse , and a model of the user 's beliefs .</sentence>
				<definiendum id="0">normative model</definiendum>
				<definiens id="0">contains our best understanding of the domain of discourse</definiens>
			</definition>
			<definition id="1">
				<sentence>( An Argument Graph is a network with nodes that represent propositions , and links that represent the inferences that connect these propositions . )</sentence>
				<definiendum id="0">Argument Graph</definiendum>
				<definiens id="0">a network with nodes that represent propositions , and links that represent the inferences that connect these propositions</definiens>
			</definition>
			<definition id="2">
				<sentence>When constructing an argument , NAG relies on two collections of information : a normative •model composed of different types of Knowledge Bases ( KBs ) , and a user model also composed of different types of KBs , which represent the user 's presumed beliefs and inferences .</sentence>
				<definiendum id="0">NAG</definiendum>
			</definition>
			<definition id="3">
				<sentence>A KB represents information in a single format , e.g. , semantic network ( SN ) , Bayesian network ( BN ) , rule-based system , or database .</sentence>
				<definiendum id="0">KB</definiendum>
				<definiens id="0">represents information in a single format , e.g. , semantic network ( SN ) , Bayesian network ( BN ) , rule-based system , or database</definiens>
			</definition>
			<definition id="4">
				<sentence>Initially , the goal proposition and the preamble activate any propositions containing two or more of the italicized concepts , i.e. , nodes Nr , N13 and N15 ( the goal node ) in Figure 3 ( a ) ( shown in dark grey boxes ) .</sentence>
				<definiendum id="0">N13</definiendum>
				<definiens id="0">the preamble activate any propositions containing two or more of the italicized concepts , i.e. , nodes Nr ,</definiens>
			</definition>
			<definition id="5">
				<sentence>In this manner , NAG combines goal-based content planning with the associative inspection of highly active nodes .</sentence>
				<definiendum id="0">NAG</definiendum>
				<definiens id="0">combines goal-based content planning with the associative inspection of highly active nodes</definiens>
			</definition>
</paper>

		<paper id="1310">
			<definition id="0">
				<sentence>By comparison , many current morphological systems are based on finite-state technology , and the most popular ones use some version of two-level morphology ( Koskenniemi 83 ) which is based on finite-state transduction : left projections are characters and right projections are also characters .</sentence>
				<definiendum id="0">two-level morphology</definiendum>
				<definiens id="0">left projections are characters and right projections are also characters</definiens>
			</definition>
			<definition id="1">
				<sentence>A set of definitions is compiled as a finite-state transducer where the lower part of the transduction ( left projection ) is a siring and the higher part ( fight projection ) is a feature structure .</sentence>
				<definiendum id="0">fight projection )</definiendum>
				<definiens id="0">a finite-state transducer where the lower part of the transduction ( left projection ) is a siring and the higher part</definiens>
				<definiens id="1">a feature structure</definiens>
			</definition>
			<definition id="2">
				<sentence>Formally , a Samba transducer T is a tuple ( I , 0 , S , s , F , 6 ) where I is the input alphabet , O the output alphabet , Sis a finite set of states , s is the initial state , F is a set of final states , 8 is the transition function from S x I x O to S. The input alphabet I is a ( finite ) set of characters .</sentence>
				<definiendum id="0">Samba transducer T</definiendum>
				<definiendum id="1">I</definiendum>
				<definiendum id="2">F</definiendum>
				<definiens id="0">a tuple ( I , 0 , S , s , F , 6 ) where</definiens>
			</definition>
			<definition id="3">
				<sentence>A simple table is a triple &lt; ~ , F , { &lt; ~ , si &gt; } &gt; , where P is the paradigm ( a feature structure ) , F represents the features of the table , and { &lt; ~ , si &gt; } the set of rules .</sentence>
				<definiendum id="0">simple table</definiendum>
				<definiendum id="1">P</definiendum>
				<definiendum id="2">F</definiendum>
				<definiens id="0">the paradigm ( a feature structure )</definiens>
			</definition>
			<definition id="4">
				<sentence>Plu\ ] Regular expr'~ , ~osions All Samba definitions which are regular expressions on rules are compiled as finite-state transducers as follows ( we use standard construction algorithms , see e.g. , Hopcroft &amp; Ullman 79 ) : • The concatenation of two morphological rules is simply defined as the rule whose form is the concatenation of the forms of the two rules and whose structure is the unification of the structures : &lt; fl , sl &gt; ° &lt; f2 , S2 &gt; '' -~ &lt; fl `` f2 , Sl ^s2 &gt; '' • The Kleene closure is defined as &lt; f , s &gt; * -- ~ &lt; f* , s &gt; .</sentence>
				<definiendum id="0">Plu\ ] Regular expr'~</definiendum>
				<definiendum id="1">Kleene closure</definiendum>
				<definiens id="0">the rule whose form is the concatenation of the forms of the two rules and whose structure is the unification of the structures</definiens>
			</definition>
			<definition id="5">
				<sentence>Since recursivity is not allowed , this process terminates , producing a single FST which is defined as a composition of sub-FSTs .</sentence>
				<definiendum id="0">single FST</definiendum>
				<definiens id="0">a composition of sub-FSTs</definiens>
			</definition>
			<definition id="6">
				<sentence>`` Multiple-Tape Two-Level Morphology : A Case study in Semitic nonlinear Morphology '' .</sentence>
				<definiendum id="0">Multiple-Tape Two-Level Morphology</definiendum>
				<definiens id="0">A Case study in Semitic nonlinear Morphology ''</definiens>
			</definition>
			<definition id="7">
				<sentence>Morphology : An Introduction to the Theory of Word Structure .</sentence>
				<definiendum id="0">Morphology</definiendum>
				<definiens id="0">An Introduction to the Theory of Word Structure</definiens>
			</definition>
			<definition id="8">
				<sentence>`` Morfogen : a morphology grammar builder and dictionary interface tool '' .</sentence>
				<definiendum id="0">Morfogen</definiendum>
				<definiens id="0">a morphology grammar builder and dictionary interface tool ''</definiens>
			</definition>
</paper>

		<paper id="1417">
			<definition id="0">
				<sentence>NewInfo provides a natural way of to conceptualize the planning process and to generate utterances on the level of granularity •required in spoken interaction .</sentence>
				<definiendum id="0">NewInfo</definiendum>
				<definiens id="0">provides a natural way of to conceptualize the planning process and to generate utterances on the level of granularity •required in spoken interaction</definiens>
			</definition>
			<definition id="1">
				<sentence>A conference participant calls the conference office and asks for information on how to get to the conference center from Kyoto station .</sentence>
				<definiendum id="0">conference participant</definiendum>
				<definiens id="0">calls the conference office and asks for information on how to get to the conference center from Kyoto station</definiens>
			</definition>
			<definition id="2">
				<sentence>IE 's evasive feedback confirms A 's tacit assumptions of IE 's scarce knowledge of Kyoto % history , and is embedded inside A 's turn .</sentence>
				<definiendum id="0">'s tacit</definiendum>
				<definiens id="0">assumptions of IE 's scarce knowledge of Kyoto % history , and is embedded inside A 's turn</definiens>
			</definition>
			<definition id="3">
				<sentence>One or more intermediate phrases plus a boundary tone then make Up an intonational phrase , roughly corresponding to anutterance .</sentence>
				<definiendum id="0">Up</definiendum>
				<definiens id="0">an intonational phrase , roughly corresponding to anutterance</definiens>
			</definition>
			<definition id="4">
				<sentence>2Ward ( 1997 ) discusses the reflexive nature of backchannels and demonstrates how they can be generated in a highly interactive system relying only on acoustic features like the pitch and the length of pauses .</sentence>
				<definiendum id="0">2Ward ( 1997 )</definiendum>
				<definiens id="0">discusses the reflexive nature of backchannels and demonstrates how they can be generated in a highly interactive system relying only on acoustic features like the pitch and the length of pauses</definiens>
			</definition>
			<definition id="5">
				<sentence>CC is the discourse referent which the utterance is about or which the participants focus their actions on .</sentence>
				<definiendum id="0">CC</definiendum>
				<definiens id="0">the discourse referent which the utterance is about or which the participants focus their actions on</definiens>
			</definition>
			<definition id="6">
				<sentence>2 , CC is first the instantiated discourse referent of the node shrine , ther/shifts to kyoto , heian , etc .</sentence>
				<definiendum id="0">CC</definiendum>
				<definiens id="0">first the instantiated discourse referent of the node shrine , ther/shifts to kyoto</definiens>
			</definition>
			<definition id="7">
				<sentence>NewInfo is the information centre of the utterance , identified as the discourse referent ( s ) to be presented , but not yet established as part of mutual knowledge .</sentence>
				<definiendum id="0">NewInfo</definiendum>
				<definiens id="0">the information centre of the utterance , identified as the discourse referent ( s ) to be presented , but not yet established as part of mutual knowledge</definiens>
			</definition>
			<definition id="8">
				<sentence>We think that NewInfo is helpful in this respect , since it is a flexible unit : defined as a minimal information unit on a given planning level , it can be of different complexities , thus allowing efficient information exchange , cf. Inui et al. ( 1996 ) .</sentence>
				<definiendum id="0">NewInfo</definiendum>
				<definiens id="0">a minimal information unit on a given planning level , it can be of different complexities , thus allowing efficient information exchange</definiens>
			</definition>
			<definition id="9">
				<sentence>DM decides on the appropriate communicative intention and the presentation of NewInfo , especially the level of explicitness in the utterance , and its content organisation .</sentence>
				<definiendum id="0">DM</definiendum>
				<definiens id="0">decides on the appropriate communicative intention and the presentation of NewInfo</definiens>
			</definition>
			<definition id="10">
				<sentence>DM exploits a number of communicative strategies , and collects the concepts to be rea/ised in Agenda .</sentence>
				<definiendum id="0">DM</definiendum>
				<definiens id="0">exploits a number of communicative strategies , and collects the concepts to be rea/ised in Agenda</definiens>
			</definition>
			<definition id="11">
				<sentence>Agenda is initialised with the Newlnfo concepts related to the current ICG ( one of DM , s own .</sentence>
				<definiendum id="0">ICG</definiendum>
			</definition>
			<definition id="12">
				<sentence>Since each conjunct is an independent NewInfo , DM has a choice of passing the conjuctive goal to SR as such ( to be realised as a single , but long conjunction of utterances ) , or drop each NewInfo separately to SR , with a pause after each item requiring the partner 's explicit acknowledgement .</sentence>
				<definiendum id="0">DM</definiendum>
				<definiens id="0">such ( to be realised as a single , but long conjunction of utterances ) , or drop each NewInfo separately to SR , with a pause after each item requiring the partner 's explicit acknowledgement</definiens>
			</definition>
</paper>

		<paper id="0313">
			<definition id="0">
				<sentence>The idea that discourse markers ( DMs ) like then or anyway signal underlying discourse relations ( DRs ) like cause , opposition , contrast , etc. , has been adopted in a certain number of works on text and conversation structure ( see Roulet 1985 , Martin 1992 , Knott 1996 for various examples ) .</sentence>
				<definiendum id="0">DMs</definiendum>
				<definiens id="0">signal underlying discourse relations ( DRs ) like cause , opposition , contrast</definiens>
			</definition>
			<definition id="1">
				<sentence>Accommodation gives us the opportunity of importing information in a possible world .</sentence>
				<definiendum id="0">Accommodation</definiendum>
				<definiens id="0">gives us the opportunity of importing information in a possible world</definiens>
			</definition>
			<definition id="2">
				<sentence>So , the Option ( iii ) seems to be the right candidate , but the only difference between ( 3-b ) and ( 3-a ) is the occurrence of DONC in the former .</sentence>
				<definiendum id="0">Option ( iii</definiendum>
				<definiens id="0">the occurrence of DONC in the former</definiens>
			</definition>
			<definition id="3">
				<sentence>Condition ( i ) echoes the current belief that questions do not introduce propositions , that is , semantic constructs evaluated as true or false ( in some world ) .</sentence>
				<definiendum id="0">Condition ( i )</definiendum>
				<definiens id="0">echoes the current belief that questions do not introduce propositions , that is , semantic constructs evaluated as true or false ( in some world</definiens>
			</definition>
			<definition id="4">
				<sentence>1 -- Information states and updates Let P be a set of atomic propositions p , q ... . and B ( P ) the set of boolean combinations of members of P. Members of B ( P ) are called expressions and axe denoted by ¢ , ~b , ... . A world ( w , w ' , ... ) is a set of expressions .</sentence>
				<definiendum id="0">world</definiendum>
				<definiens id="0">a set of expressions</definiens>
			</definition>
			<definition id="5">
				<sentence>A V-state ( s , s ' , ... ) is a set of worlds .</sentence>
				<definiendum id="0">V-state</definiendum>
				<definiens id="0">a set of worlds</definiens>
			</definition>
			<definition id="6">
				<sentence>An information state ( henceforth simply state ) is a set of V-states .</sentence>
				<definiendum id="0">information state</definiendum>
				<definiens id="0">a set of V-states</definiens>
			</definition>
			<definition id="7">
				<sentence>I P DONC Q\ ] with respect to 7~ , ¢ , ~b is the set of pairs S , S '' ) such that : a. O ( ~ ) is the conditional version of the operation associated with P and is an update,90 ' ( ¢ ) is the conditional version of the operation associated with Q. b. There exists S ' such that ( S , S ' ) E \ [ p\ ] 1 , ~ and &lt; s ' , s '' ) e \ [ q\ ] c. O ( ¢ ) T~-entails O ' ( ¢ ) . To motivate informally this definition , consider ( 2-b ) again. The first assertion results in updating S~ ''~t and ¢irnp with an expression not watch TV. This \ [ ¢assert oimP~ results into a state v~2 , o 2 ; which accepts not watch TV. Let us assume that we have a rule ¢ , R2 = ¢ ^ X ~ `` ~¢. When ¢ and X are both true ¢ and ~g , are both true. 9Actually , we could eliminate this condition by defining a more general notion of stability , but this would require some extra technical machinery. in 7~ : not watch TV ~ not know result. Then , ~ , mp with the rule results in updating S~ 8serf and ~,2 a global state where the two members accept not know result. The question Did the Red Sox win is interpreted as connected with answers like Red Sox win or Red Sox not win. But , clearly , the fact that not know result is accepted does not warrant that Red Sox win is tolerated by any V-state in the question test on S~ sSert. The same holds for Red Sox not win. So , we are in no position to conclude that the test will be successful , unless we ascribe to the sentence some contrived interpretation. The definition distinguishes between ( i ) the conditional operations which are used to check out 7~entailment and ( ii ) ( absolute ) operations associated with P and Q. This allows for situations in which 7~entailment holds , but there are still problems with P and/or Q , which is precisely the case in ( 4-c ) . In the next section , we show how the proposed constraints shed light upon other observations. Assertion-Imperative This the ( 4-b ) case. • You are late : ( S~ ' '' 't , S'l '~p ) -- -+ = ¢irnp ¢imp ec late ) ( by def. 6 S~ ssert ~ late , ~2 : ~1 and 8 ) . We assume a rule r : late ~ Must highway. When somebody is late , she must take the highway ( in certain circumstances ) . ¢irnp ~c r ) accepts 2vlust highway. ( S~ ... . t • r , ~2 Take the highway : ( S ~r , S~mP~BCr~.BChighway ) ~sser~ &gt; • r , s ; # ¢ ) .</sentence>
				<definiendum id="0">~b</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">the conditional version of the operation associated with P and is an update,90 '</definiens>
				<definiens id="1">the conditional version of the operation associated with Q. b. There exists S ' such that ( S , S ' ) E \ [ p\ ] 1 , ~ and &lt; s ' , s '' ) e \ [ q\ ] c. O ( ¢ ) T~-entails O ' ( ¢ )</definiens>
			</definition>
</paper>

		<paper id="0906">
</paper>

		<paper id="1430">
			<definition id="0">
				<sentence>Students are given a simplified qualitative model of the heart , followed by a series of problems which utilize the model .</sentence>
				<definiendum id="0">Students</definiendum>
				<definiens id="0">given a simplified qualitative model of the heart , followed by a series of problems which utilize the model</definiens>
			</definition>
</paper>

		<paper id="1415">
			<definition id="0">
				<sentence>For example , clauses like `` Jones is a patient '' and `` Jones has hypertension '' can be combined into a more concise sentence `` Jones is a hypertensive patient. '</sentence>
				<definiendum id="0">Jones</definiendum>
			</definition>
			<definition id="1">
				<sentence>CASPER ( Clause Aggregation in Sentence PlannER ) is a sentence planner which focuses on generating concise sentences .</sentence>
				<definiendum id="0">CASPER</definiendum>
				<definiens id="0">a sentence planner which focuses on generating concise sentences</definiens>
			</definition>
			<definition id="2">
				<sentence>Text summarization is an application which uses inferential operators extensively .</sentence>
				<definiendum id="0">Text summarization</definiendum>
				<definiens id="0">an application which uses inferential operators extensively</definiens>
			</definition>
			<definition id="3">
				<sentence>A proposition is defined as a piece of information that the physician ( the speaker ) might choose to convey in a stand-alone sentence tothe nurses in the Intensive Care Unit ( the hearer ) .</sentence>
				<definiendum id="0">proposition</definiendum>
				<definiendum id="1">Care Unit</definiendum>
				<definiens id="0">a piece of information that the physician ( the speaker ) might choose to convey in a stand-alone sentence tothe nurses in the Intensive</definiens>
			</definition>
			<definition id="4">
				<sentence>We consider clause aggregation as an integral part of a text generation system , not as a revision .</sentence>
				<definiendum id="0">clause aggregation</definiendum>
				<definiens id="0">an integral part of a text generation system</definiens>
			</definition>
			<definition id="5">
				<sentence>\ [ Kaplan and Bresnan , 1982\ ] Kaplan , R. M. and Bresnan , J. Lexical-functional grammar : A formal system for grammatical representation .</sentence>
				<definiendum id="0">Lexical-functional grammar</definiendum>
				<definiens id="0">A formal system for grammatical representation</definiens>
			</definition>
</paper>

		<paper id="1420">
			<definition id="0">
				<sentence>To facifitate this , TMR language provides special frames for representing aspectual properties , temporal relations , speech-acts , stylistic factors , etc .</sentence>
				<definiendum id="0">TMR language</definiendum>
			</definition>
			<definition id="1">
				<sentence>Lexicon , besides its other usages , provides information about the relationship between concept instances and word senses of tile target language \ [ Dorr , 1993\ ] .</sentence>
				<definiendum id="0">Lexicon</definiendum>
			</definition>
			<definition id="2">
				<sentence>Lexical selection is activated whenever the TMR frame is a concept instance , and it is based on the semantic and the pragmatic properties of the candidate lexemes .</sentence>
				<definiendum id="0">TMR frame</definiendum>
				<definiens id="0">a concept instance , and it is based on the semantic and the pragmatic properties of the candidate lexemes</definiens>
			</definition>
</paper>

		<paper id="0302">
			<definition id="0">
				<sentence>RASTA ( Rhetorical Structure Theory Analyzer ) , a system for automatic discourse analysis , reliably , identifies rhetorical relations present m written discourse by examining information available in syntactic and logical form analyses .</sentence>
				<definiendum id="0">RASTA</definiendum>
				<definiens id="0">a system for automatic discourse analysis , reliably , identifies rhetorical relations present m written discourse by examining information available in syntactic and logical form analyses</definiens>
			</definition>
			<definition id="1">
				<sentence>RASTA ( CorstonOliver 1998a , 1998b ) , a discourse analysis component within the Microsoft English Grammar , automatically produces RST analyses of texts .</sentence>
				<definiendum id="0">RASTA</definiendum>
				<definiens id="0">a discourse analysis component within the Microsoft English Grammar , automatically produces RST analyses of texts</definiens>
			</definition>
			<definition id="2">
				<sentence>In the second stage , RASTA examines all possible pairs of terminal nodes to determine which discourse relation , if any , might hold between the two nodes .</sentence>
				<definiendum id="0">RASTA</definiendum>
				<definiens id="0">examines all possible pairs of terminal nodes to determine which discourse relation</definiens>
			</definition>
			<definition id="3">
				<sentence>In the third stage , RASTA combines the terminal nodes according to the discourse relations that it hypothesized to form RST analyses of a complete text .</sentence>
				<definiendum id="0">RASTA</definiendum>
				<definiens id="0">combines the terminal nodes according to the discourse relations that it hypothesized to form RST analyses of a complete text</definiens>
			</definition>
			<definition id="4">
				<sentence>The Microsoft English Grammar ( MEG ) is a broad-coverage grammar of English that performs a morphological analysis , a conventional syntactic constituent analysis and a logical form analysis ( involving the normalization of syntactic alternations to yield a representation with the flavor of a predicate representation ) .</sentence>
				<definiendum id="0">Microsoft English Grammar ( MEG )</definiendum>
				<definiens id="0">a broad-coverage grammar of English that performs a morphological analysis , a conventional syntactic constituent analysis and a logical form analysis ( involving the normalization of syntactic alternations to yield a representation with the flavor of a predicate representation )</definiens>
			</definition>
			<definition id="5">
				<sentence>RASTA identifies rhetorical relations by directly examining a text , and is therefore most closely aligned with the first of these three strands .</sentence>
				<definiendum id="0">RASTA</definiendum>
				<definiens id="0">identifies rhetorical relations by directly examining a text</definiens>
			</definition>
			<definition id="6">
				<sentence>Like previous work in this vein , RASTA considers cue phrases to be a useful indicator of rhetorical relations ( section 2.3 ) .</sentence>
				<definiendum id="0">RASTA considers cue</definiendum>
			</definition>
			<definition id="7">
				<sentence>However , RASTA goes beyond a simple examination of cue phrases and considers such linguistic evidence as clausal status ( section 2.1 ) , anaphora , deixis and referential continuity ( section 2.2 ) and tense , aspect , and polarity ( section 5 ) .</sentence>
				<definiendum id="0">RASTA</definiendum>
				<definiens id="0">goes beyond a simple examination of cue phrases and considers such linguistic evidence as clausal status ( section 2.1 ) , anaphora , deixis and referential continuity ( section 2.2 ) and tense , aspect , and polarity ( section 5 )</definiens>
			</definition>
			<definition id="8">
				<sentence>In rare cases , this correlation between clausal status and rhetorical status is the only clue to discourse structure that RASTA is able to identify , i.e. having correctly identified a hypotactic relationship , RASTA is unable to identify a specific corresponding asymmetric rhetorical relation .</sentence>
				<definiendum id="0">RASTA</definiendum>
				<definiens id="0">the only clue to discourse structure that RASTA is able to identify</definiens>
				<definiens id="1">unable to identify a specific corresponding asymmetric rhetorical relation</definiens>
			</definition>
			<definition id="9">
				<sentence>The MEG system resolves pronominal anaphoric references during the construction of the logical form .</sentence>
				<definiendum id="0">MEG system</definiendum>
				<definiens id="0">resolves pronominal anaphoric references during the construction of the logical form</definiens>
			</definition>
			<definition id="10">
				<sentence>RASTA , however , attempts to overcome two problems related to cue phrases : compositionality , i.e. some cue phrases are amenable to different compositional analyses , and coverage , i.e. not all clauses contain cue phrases .</sentence>
				<definiendum id="0">RASTA</definiendum>
				<definiendum id="1">compositionality</definiendum>
				<definiens id="0">attempts to overcome two problems related to cue phrases</definiens>
			</definition>
			<definition id="11">
				<sentence>RASTA resolves this tension by distinguishing two kinds of evidence .</sentence>
				<definiendum id="0">RASTA</definiendum>
				<definiens id="0">resolves this tension by distinguishing two kinds of evidence</definiens>
			</definition>
			<definition id="12">
				<sentence>When constructing RST trees , RASTA applies the relations with the highest scores first .</sentence>
				<definiendum id="0">RASTA</definiendum>
				<definiens id="0">applies the relations with the highest scores first</definiens>
			</definition>
			<definition id="13">
				<sentence>The SEQUENCE relation is a symmetric relation in which two or more clauses report events that are in a relationship of temporal succession .</sentence>
				<definiendum id="0">SEQUENCE relation</definiendum>
				<definiens id="0">a symmetric relation in which two or more clauses report events that are in a relationship of temporal succession</definiens>
			</definition>
			<definition id="14">
				<sentence>PlSA : A procedure for analyzing the structure of explanatory texts .</sentence>
				<definiendum id="0">PlSA</definiendum>
				<definiens id="0">A procedure for analyzing the structure of explanatory texts</definiens>
			</definition>
</paper>

		<paper id="1406">
			<definition id="0">
				<sentence>In contrast to modularization by tasks such as discourse structuring , clause structuring and lexical choice , the Mikrokosmos project ( http : //crl.nmsu.edu/Research/Projects/mikro/index.html ) attempts to modularize on the ontological and linguistic data that serves as inputs to the text generation process , that is , based on the types of inputs we expect , not on the types of processing we need to perform .</sentence>
				<definiendum id="0">Mikrokosmos project</definiendum>
				<definiens id="0">attempts to modularize on the ontological and linguistic data that serves as inputs to the text generation process</definiens>
			</definition>
			<definition id="1">
				<sentence>Note that this is allowed by unification processors \ [ Elhadad et a1.1997\ ] , but HG gives the added benefits of speed and capability of `` fuzzy '' constraint processing .</sentence>
				<definiendum id="0">HG</definiendum>
				<definiens id="0">gives the added benefits of speed</definiens>
			</definition>
</paper>

		<paper id="0216">
			<definition id="0">
				<sentence>The vision system of J.Edgar consists in a oneeye monochrome camera mounted on a smail frame with two independent drive wheels and a pan head .</sentence>
				<definiendum id="0">vision system of J.Edgar</definiendum>
				<definiens id="0">consists in a oneeye monochrome camera mounted on a smail frame with two independent drive wheels and a pan head</definiens>
			</definition>
</paper>

		<paper id="1121">
			<definition id="0">
				<sentence>Language models for speech recognition concenIrate solely on recognizing the words that were spoken .</sentence>
				<definiendum id="0">Language models</definiendum>
				<definiens id="0">for speech recognition concenIrate solely on recognizing the words that were spoken</definiens>
			</definition>
			<definition id="1">
				<sentence>l~V = argr~axPr ( AIW ) Pr ( W ) ( 3 ) The first term , Pr ( AIW ) , is the acoustic model and the second term , Pr ( W ) , is the lanffuage model , which assigns a probability to the sequence of words W. We can rewrite W explicitly as a sequence of words W1W2W3 ... WN , where N is the number of words in the sequence .</sentence>
				<definiendum id="0">lanffuage model</definiendum>
				<definiens id="0">the acoustic model and the second term</definiens>
				<definiens id="1">assigns a probability to the sequence of words W. We can rewrite W explicitly as a sequence of words W1W2W3 ... WN , where N is the number of words in the sequence</definiens>
			</definition>
			<definition id="2">
				<sentence>Perplexity is an estimate of how well the language model is able to predict the next word of a test corpus in terms of the number of alternatives that need to be considered at each point .</sentence>
				<definiendum id="0">Perplexity</definiendum>
				<definiens id="0">an estimate of how well the language</definiens>
			</definition>
			<definition id="3">
				<sentence>The perplexity of a test set wl , N is calculated as 2 t't , where H is the entropy , which is defined as follows .</sentence>
				<definiendum id="0">perplexity of a test set wl , N</definiendum>
				<definiendum id="1">H</definiendum>
				<definiendum id="2">entropy</definiendum>
				<definiens id="0">the</definiens>
			</definition>
			<definition id="4">
				<sentence>DDiscourse connective J JR Relative Adjective RBR Relative Adverb VBP CD Cardinal number JJS Superlative Adjective RBS Superlative Adverb VBZ DO Base form of '' do '' MD Modal RB .</sentence>
				<definiendum id="0">JR Relative Adjective RBR Relative Adverb VBP CD</definiendum>
				<definiens id="0">Cardinal number JJS Superlative Adjective RBS Superlative Adverb VBZ DO Base form of '' do '' MD Modal RB</definiens>
			</definition>
			<definition id="5">
				<sentence>For a numeric variable N , the decision tree searches for questions of the form 'is N &gt; = n ' , where n is a numeric constant .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">a numeric constant</definiens>
			</definition>
			<definition id="6">
				<sentence>The recall rate is the number of times that the algorithm correctly identifies an event over the total number of times that it actually occurred .</sentence>
				<definiendum id="0">recall rate</definiendum>
				<definiens id="0">the number of times that the algorithm correctly identifies an event over the total number of times that it actually occurred</definiens>
			</definition>
			<definition id="7">
				<sentence>The precision rate is the number of times the algorithm correctly identifies it over the total number of times it identifies it .</sentence>
				<definiendum id="0">precision rate</definiendum>
			</definition>
			<definition id="8">
				<sentence>The error rate is the number of errors in identifying an event over the number of times that the event occurred .</sentence>
				<definiendum id="0">error rate</definiendum>
				<definiens id="0">the number of errors in identifying an event over the number of times that the event occurred</definiens>
			</definition>
			<definition id="9">
				<sentence>The final column gives the results of the full model , which accounts for interactions with speech repair correction and intonational phrasing , and uses silence information .</sentence>
				<definiendum id="0">full model</definiendum>
				<definiens id="0">accounts for interactions with speech repair correction and intonational phrasing , and uses silence information</definiens>
			</definition>
</paper>

		<paper id="0305">
			<definition id="0">
				<sentence>In general terms , a causal reasoning process consists of three elements : rule ( general law ) , circumstance ( specific case ) , and result , like in the following example : Rule : Every time when it rains ( a lot ) , the river is high .</sentence>
				<definiendum id="0">causal reasoning process</definiendum>
				<definiens id="0">consists of three elements : rule ( general law ) , circumstance ( specific case ) , and result , like in the following example : Rule : Every time when it rains ( a lot ) , the river is high</definiens>
			</definition>
</paper>

		<paper id="0904">
			<definition id="0">
				<sentence>b. Annotate the candidate set with independently needed morphosemantic interpretation and choose as the correct form for certain morphological features ( e.g.PI+OBJ ) the phonologically optimal form annotated with it .</sentence>
				<definiendum id="0">e.g.PI+OBJ</definiendum>
				<definiens id="0">the candidate set with independently needed morphosemantic interpretation</definiens>
			</definition>
			<definition id="1">
				<sentence>GCrossover product ( A , T ) is equivalent to the image of A under T ( Kaplan &amp; Kay , 1994:340-42 ) , defined as the range of the composition Id ( A ) o R , where Id ( A ) is the identity relation that carries every member of A into itself .</sentence>
				<definiendum id="0">GCrossover product ( A , T )</definiendum>
				<definiendum id="1">Id ( A )</definiendum>
				<definiens id="0">equivalent to the image of A under T ( Kaplan &amp; Kay , 1994:340-42 ) , defined as the range of the composition Id ( A ) o R , where</definiens>
				<definiens id="1">the identity relation that carries every member of A into itself</definiens>
			</definition>
			<definition id="2">
				<sentence>Phonological constraints on the other side are violable and ranked as in OT z. Looking at constraint based formalisms as tentatives to restrict the rather rich theoretical inventory of generative SPE theory ( Chomsky &amp; Halle , 1968 ) it becomes obvious that OM indeed fills a conceptual gap : ( s ) SPE OT DP OM arbitrary rule/ yes yes no yes constraint order language specific yes no yes yes rules/constraints underlying yes yes no no representations Neglecting here the difference between rules and constraints , OT has chosen to eliminate language specific constraints s while maintaining underlying representations .</sentence>
				<definiendum id="0">OT</definiendum>
				<definiens id="0">fills a conceptual gap : ( s ) SPE OT DP OM arbitrary rule/ yes yes no yes constraint order language specific yes no yes yes rules/constraints underlying yes yes no no representations Neglecting here the difference between rules and constraints</definiens>
			</definition>
			<definition id="3">
				<sentence>Now , when the toy example above proves to be representative , OM ( like OT ) allows to maintain universal constraints where DP can not .</sentence>
				<definiendum id="0">OM</definiendum>
				<definiens id="0">allows to maintain universal constraints where DP can not</definiens>
			</definition>
</paper>

		<paper id="1426">
			<definition id="0">
				<sentence>The Nitrogen statistical extractor ranks these alternative s using bigram ( adjacent word pairs ) and unigram ( single word ) statistics Collected from two years of the Wall Street Journal .</sentence>
				<definiendum id="0">Nitrogen statistical extractor</definiendum>
				<definiendum id="1">unigram</definiendum>
				<definiens id="0">single word ) statistics Collected from two years of the Wall Street Journal</definiens>
			</definition>
</paper>

		<paper id="0511">
			<definition id="0">
				<sentence>• ( V ) V ( N , * , N ) N ( D , * ) N ( D , A , * ) N ( A , * ) D ( * ) V ( * ) A ( * ) N ( * ) 95 V= { drinks , eats ) D= { the , a } N= { dog , cat , cup , milk ) A=tblack , white , hot } With this grammar one can build the structure : drinks / cat : ~milk hot : Generation Dependency grammars are generative , working with the following generating rules : a ) choose a type i ) rule ( which determines the main governor ) , b ) choose and apply type ii ) rules until we obtain a complete structure , entirely made of terminating rules .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">~milk hot : Generation Dependency grammars are generative , working with the following generating rules : a</definiens>
				<definiens id="1">determines the main governor</definiens>
			</definition>
			<definition id="1">
				<sentence>So the three relations above are equivalent to the following dependency grammar : N ( A , * ) N ( A , A , * ) N ( D , * ) N ( D , A , * ) N ( D , A , A , * ) V ( N , * ) V ( * , N ) V ( N , * , N ) N ( * ) A ( * ) D ( * ) V ( * ) It can be noted that these relations are in some sense similar to disjunctive forms of Sleator 's link grammars ( Sleator and Temperley 91 ) .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">in some sense similar to disjunctive forms of Sleator 's link grammars</definiens>
			</definition>
			<definition id="2">
				<sentence>subj = &gt; UL ( sem = &gt; S : AN'DIA'I~ ) ; A correct interpretation ( according to a given obj = &gt; UL ( s~-'ta = &gt; O : EATABI~ ) ; grammar ) of the input sentence can be found in sere = &gt; /2qGEST ( agent = &gt; S ; each stack which contains exactly one tree : this patient = &gt; O ) ) A tree is a dependency structure of the sentence .</sentence>
				<definiendum id="0">correct interpretation</definiendum>
				<definiendum id="1">tree</definiendum>
				<definiens id="0">according to a given obj = &gt; UL ( s~-'ta = &gt; O : EATABI~ ) ; grammar ) of the input sentence can be found in sere = &gt; /2qGEST ( agent = &gt;</definiens>
			</definition>
			<definition id="3">
				<sentence>Unification allows implicit inheritance of properties , and can be efficiently implemented ( Ai't-Kaci &amp; al. 89 ) .</sentence>
				<definiendum id="0">Unification</definiendum>
			</definition>
</paper>

		<paper id="1237">
</paper>

		<paper id="1437">
			<definition id="0">
				<sentence>DRAFTER-If is an interactive software tool designed to assist the production of technical documents in several languages at once .</sentence>
				<definiendum id="0">DRAFTER-If</definiendum>
				<definiens id="0">an interactive software tool designed to assist the production of technical documents in several languages at once</definiens>
			</definition>
			<definition id="1">
				<sentence>The prototype system allows a technical author or domain expert to create software manual instructions in English , French and Italian .</sentence>
				<definiendum id="0">prototype system</definiendum>
				<definiens id="0">allows a technical author or domain expert to create software manual instructions in English</definiens>
			</definition>
			<definition id="2">
				<sentence>Symbolic Authoring Symbolic Authoring is a document authoring method in which the author generates language-neutral 'symbolic ' representations of the content of a document , from which documents in each target language are generated automatically , using NLG technology .</sentence>
				<definiendum id="0">Symbolic Authoring Symbolic Authoring</definiendum>
				<definiens id="0">a document authoring method in which the author generates language-neutral 'symbolic ' representations of the content of a document , from which documents in each target language</definiens>
			</definition>
			<definition id="3">
				<sentence>To achieve this , it is clear that a key requirement of a Symbolic Authoring system is an effective user interface -one which enables the author to construct the knowledge base without assuming prior expertise in knowledge representation languages or in computational linguistics .</sentence>
				<definiendum id="0">Symbolic Authoring system</definiendum>
				<definiens id="0">an effective user interface -one which enables the author to construct the knowledge base without assuming prior expertise in knowledge representation languages or in computational linguistics</definiens>
			</definition>
			<definition id="4">
				<sentence>WYSIWYM editing WYSIWYM is a technique for creating and maintaining complex data objects such as typically found in knowledge bases , but presenting them to the author or knowledge editor as natural language texts .</sentence>
				<definiendum id="0">WYSIWYM editing WYSIWYM</definiendum>
				<definiens id="0">a technique for creating and maintaining complex data objects such as typically found in knowledge bases , but presenting them to the author or knowledge editor as natural language texts</definiens>
			</definition>
			<definition id="5">
				<sentence>Whereas WYS1WYG editors ( e.g. , Microsoft Word , FRAMEMAKER and INTERLEAF ) present the user with text as it will appear on the printed page , wYSIWYM editors present a text that reflects only what the user meant .</sentence>
				<definiendum id="0">wYSIWYM</definiendum>
				<definiens id="0">editors present a text that reflects only what the user meant</definiens>
			</definition>
</paper>

		<paper id="1222">
			<definition id="0">
				<sentence>The MML principle is based on the following premise : if a sender knows both the attribute values and the class of the objects in a set , and wants to send the class of each object to a receiver ( w.ho knows the attribute values but not the classes ) , the sender aims to send the shortest possible message ( in bits ) .</sentence>
				<definiendum id="0">MML principle</definiendum>
				<definiens id="0">the class of the objects in a set , and wants to send the class of each object to a receiver ( w.ho knows the attribute values but not the classes ) , the sender aims to send the shortest possible message ( in bits</definiens>
			</definition>
			<definition id="1">
				<sentence>b , d , g p , q , t , k , dx bcl , dcl , gcl pcl , tcl , kcl jli ch z , zli v , dh s , sh f , v , tli m , n , nx , ng , em , en , eng l , r , y , w , el hh hv iy , ih , eh , ae aa , er , ah , ax , ao uw , uh , ow axr , ax-h pau , epi wb sb Stop Release , voiced , obstruent , non-sibilant Stop Release , unvoiced , obstruent , non-sibilant Stop Closure , voiced Stop Closure , unvoiced Affricate , voiced , obstruent , sibilant Affricate , unvoiced , obstruent , sibilant Fricative , voiced , obstruent , sibilant Fricative , voiced , obstruent , non-sibilant Fricative , unvoiced , obstruent , sibilant Fricative , unvoiced , obstruent , non-sibilant Nasal , voiced , sonorant Semivowel/Glide , voiced , sonorant Semivowel/Glide , unvoiced , obstruent , non-sibilant Semivowel/Glide , voiced , obstruent , non-sibilant Vowel , Front Position Vowel , Mid Position Vowel , Back Position Vowel Pause Word boundary Sentence boundary Missing phoneme ( 2 ) the labels for the classes of the objects in the leaves of the tree .</sentence>
				<definiendum id="0">ax-h pau , epi wb sb</definiendum>
				<definiens id="0">g p , q , t , k , dx bcl , dcl , gcl pcl , tcl , kcl jli ch z , zli v , dh s , sh f , v , tli m , n , nx , ng , em , en , eng l , r , y , w</definiens>
				<definiens id="1">unvoiced , obstruent , sibilant Fricative , voiced , obstruent , sibilant Fricative , voiced , obstruent , non-sibilant Fricative , unvoiced , obstruent , sibilant Fricative , unvoiced , obstruent , non-sibilant Nasal , voiced , sonorant Semivowel/Glide , voiced , sonorant Semivowel/Glide , unvoiced , obstruent , non-sibilant Semivowel/Glide , voiced</definiens>
			</definition>
</paper>

		<paper id="0201">
			<definition id="0">
				<sentence>The Pausanian method is a diagrammatic notation for representing the static and dynamic properties of a single or a collection of hypermedia documents in order to help the author to visualise the structure they are creating .</sentence>
				<definiendum id="0">Pausanian method</definiendum>
				<definiens id="0">a diagrammatic notation for representing the static and dynamic properties of a single or a collection of hypermedia documents in order to help the author to visualise the structure they are creating</definiens>
			</definition>
			<definition id="1">
				<sentence>HTML ( any HTML document , viewed by the Microcosm 's Web viewer ) , VR ( any virtual reality objects ) viewed by the Microcosm 's 3D viewer , External Object ( any independent application linked with Microcosm ) .</sentence>
				<definiendum id="0">HTML</definiendum>
				<definiendum id="1">Object</definiendum>
				<definiens id="0">any HTML document , viewed by the Microcosm 's Web viewer ) , VR ( any virtual reality objects ) viewed by the Microcosm 's 3D viewer</definiens>
			</definition>
			<definition id="2">
				<sentence>One to Many ( Indirect ) The source anchor leads the user to a choice of destination nodes , e.g. the source node is about the ancient Greek philosophers and in the tburth paragraph the name Plato is a visible anchor linked to a choice of three different destinations : a document about his life .</sentence>
				<definiendum id="0">Plato</definiendum>
				<definiens id="0">a visible anchor linked to a choice of three different destinations : a document about his life</definiens>
			</definition>
			<definition id="3">
				<sentence>The Pausanian Map includes a visual representation of links traversed up to the Iburth step .</sentence>
				<definiendum id="0">Pausanian Map</definiendum>
				<definiens id="0">includes a visual representation of links traversed up to the Iburth step</definiens>
			</definition>
			<definition id="4">
				<sentence>Subversive Association Hypermedia authors build a subversive relation when they connect two documents , and the destination document subverts the content of its source .</sentence>
				<definiendum id="0">destination document</definiendum>
				<definiens id="0">subverts the content of its source</definiens>
			</definition>
			<definition id="5">
				<sentence>The words 'black-figure '' technique are a visible anchor , which leads the reader to a document containing a well composed detailed description on the actual technique , including information about different painters and their vase paintings .</sentence>
				<definiendum id="0">visible anchor</definiendum>
				<definiens id="0">leads the reader to a document containing a well composed detailed description on the actual technique</definiens>
			</definition>
			<definition id="6">
				<sentence>The kermatism of a hypermedia application is the segmentation and classification of the aggregate content into interrelated atomic subsets each of which comprise an individual subject .</sentence>
				<definiendum id="0">kermatism of a hypermedia application</definiendum>
				<definiens id="0">the segmentation and classification of the aggregate content into interrelated atomic subsets each of which comprise an individual subject</definiens>
			</definition>
</paper>

		<paper id="0316">
			<definition id="0">
				<sentence>Conjunctive adverbials are expressions like ja ( then ) , de 'then ' , ato 'in addition ' many of which are derived from conjunctions and conjunctive particles that are used to express various relations between sentences .</sentence>
				<definiendum id="0">Conjunctive adverbials</definiendum>
				<definiens id="0">derived from conjunctions and conjunctive particles that are used to express various relations between sentences</definiens>
			</definition>
			<definition id="1">
				<sentence>One possible way of doing so would be to posit a rule like the following : ( r- ) ( ( { L ) r qr ) ' , UNIT ) + ( HLY , ) , where UNIT is some antecedently defined unit of discourse , possibly bunsetsu , r is either H or L , the raised Phoriclty Filler i Cataphoric Anaphoric ' Responsive i Anaphoric \ [ T ) ne \ [ Examples ( L ) It+It \ [ ee , anoo L- , Hi a , e , ma , HL % I hal , ee , un Table 3 : + indicates the possibility of iterating the same element once or more times , and the raised * indicates the possibility of iterating the same element zero or more times .</sentence>
				<definiendum id="0">UNIT</definiendum>
				<definiens id="0">some antecedently defined unit of discourse , possibly bunsetsu , r is either H or L , the raised Phoriclty Filler i Cataphoric Anaphoric ' Responsive i Anaphoric \ [ T ) ne \ [ Examples</definiens>
			</definition>
</paper>

		<paper id="1301">
			<definition id="0">
				<sentence>Although the work of ~EN and the assignment of violation marks can be carried out by finite-state transducers , the sorting and counting of the marks envisioned by Ellison and subsequent work ( Walther \ [ 26\ ] ) is an off-line activity that is not a finite-state process .</sentence>
				<definiendum id="0">] )</definiendum>
				<definiens id="0">the sorting and counting of the marks envisioned by Ellison and subsequent work ( Walther \ [ 26\</definiens>
			</definition>
			<definition id="1">
				<sentence>The application of a set of two-level constraints is a combination of intersection and composition ( Karttunen \ [ 18\ ] ) .</sentence>
				<definiendum id="0">two-level constraints</definiendum>
				<definiens id="0">a combination of intersection and composition ( Karttunen \ [ 18\ ] )</definiens>
			</definition>
			<definition id="2">
				<sentence>Input , Parse , and OverParse A replace expression of the type A - &gt; B ... C in the Xerox calculus denotes a relation that wraps the prefix strings in B and the sutF~ strings in C around every string in A. Thus Parse is a transducer that inserts appropriate bracket pairs around input segments .</sentence>
				<definiendum id="0">OverParse</definiendum>
			</definition>
			<definition id="3">
				<sentence>With these preliminaries we can now define GEN as a simple composition of the four components ( Figure 6 ) .</sentence>
				<definiendum id="0">GEN</definiendum>
			</definition>
			<definition id="4">
				<sentence>Each constraint network encodes an infinite regular language .</sentence>
				<definiendum id="0">constraint network</definiendum>
				<definiens id="0">encodes an infinite regular language</definiens>
			</definition>
			<definition id="5">
				<sentence>N denotes a relation and that the constraints can be thought of as identity relations on sets , the simplest idea is to proceed in the same way as with the rewrite rules in Figure 2 .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a relation and that the constraints can be thought of as identity relations on sets</definiens>
			</definition>
			<definition id="6">
				<sentence>D-PATR : A development environment for unification-based grammars .</sentence>
				<definiendum id="0">D-PATR</definiendum>
			</definition>
			<definition id="7">
				<sentence>Two-level morphology : A general computational model for word-form recognition and production .</sentence>
				<definiendum id="0">Two-level morphology</definiendum>
				<definiens id="0">A general computational model for word-form recognition and production</definiens>
			</definition>
			<definition id="8">
				<sentence>Optimality Theory : Constraint Interaction in Generative Grammar .</sentence>
				<definiendum id="0">Optimality Theory</definiendum>
			</definition>
</paper>

		<paper id="0902">
			<definition id="0">
				<sentence>As a limiting case , the IOP predicts the possibility of vowelless surface stems , e.g. formed by two root consonants combined with vowel-final prefix and suffix .</sentence>
				<definiendum id="0">IOP</definiendum>
				<definiens id="0">predicts the possibility of vowelless surface stems , e.g. formed by two root consonants combined with vowel-final prefix and suffix</definiens>
			</definition>
			<definition id="1">
				<sentence>Moreover , the IOP supports economic communication , as it leads to shortest surface forms wherever possible .</sentence>
				<definiendum id="0">IOP</definiendum>
				<definiens id="0">supports economic communication</definiens>
			</definition>
			<definition id="2">
				<sentence>MicroCUF implements a subset of CUF ( D6rre &amp; Dorna 1993 ) , inheriting its formal semantics .</sentence>
				<definiendum id="0">MicroCUF</definiendum>
				<definiens id="0">implements a subset of CUF</definiens>
			</definition>
			<definition id="3">
				<sentence>Instead of Prolog 's fixed-arity first order terms , MicroCUF has typed feature terms as its basic data structures .</sentence>
				<definiendum id="0">MicroCUF</definiendum>
				<definiens id="0">typed feature terms as its basic data structures</definiens>
			</definition>
			<definition id="4">
				<sentence>Disharmony is defined as the binary number that results from application of the mapping { unmarked ~ 012 , marked ~ 102 } to the left-to-right markedness vector of a segmental string : e.g. , jolioxgolal0moxroluol yields the disharmony value 010101100101012 = 552510 &gt; 54771o = 010101011001012 from joli01golmoleloroluol .</sentence>
				<definiendum id="0">Disharmony</definiendum>
				<definiens id="0">the binary number that results from application of the mapping</definiens>
			</definition>
			<definition id="5">
				<sentence>With these instantiations , a second run of MicroCUF uses the full generateand-minimize mechanism to compute optimal strings OptStringl ... .. OptStringN .</sentence>
				<definiendum id="0">MicroCUF</definiendum>
			</definition>
</paper>

		<paper id="0310">
</paper>

		<paper id="1240">
</paper>

		<paper id="1309">
			<definition id="0">
				<sentence>Constraint-based morphological disambiguation systems ( e.g. \ [ 6 , 7 , 15\ ] ) typically look at a context of several sequential tokens each annotated with their possible morphological interpretations ( or tags ) , and in a reductionistic way , remove parses that are considered to be impossible in the given context .</sentence>
				<definiendum id="0">Constraint-based morphological disambiguation systems</definiendum>
				<definiens id="0">annotated with their possible morphological interpretations ( or tags ) , and in a reductionistic way</definiens>
			</definition>
			<definition id="1">
				<sentence>We represent , using a directed acyclic graph ( DAG ) , a sentence consisting n tokens wl , w2 , ... to , , each with morphological parses/tags ti,1 , ti,2 , ... , ti , a , , ai being the number of ambiguous parses for token i. The nodes in the DAG represent token boundaries and arcs are labeled with triplets of the sort L - ( wi , tij , vij ) where vij ( initially 0 ) is the total vote associated with tag tij of wi .</sentence>
				<definiendum id="0">DAG</definiendum>
				<definiens id="0">a sentence consisting n tokens wl , w2 , ... to , , each with morphological parses/tags ti,1 , ti,2 , ... , ti , a , , ai being the number of ambiguous parses for token i. The nodes in the DAG represent token boundaries and arcs are labeled with triplets of the sort L - ( wi , tij , vij ) where vij</definiens>
			</definition>
			<definition id="2">
				<sentence>- ' , Cn ; V ) , where the Ci 's are , in general , feature constraints on a sequence of ambiguous parses , and V is an integer denoting the Vote assigned to the rule .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">an integer denoting the Vote assigned to the rule</definiens>
			</definition>
			<definition id="3">
				<sentence>TAGS denotes a regular expression which is the union of all ( possibly mslti-chazacter ) tag symbols .</sentence>
				<definiendum id="0">TAGS</definiendum>
				<definiens id="0">a regular expression which is the union of all ( possibly mslti-chazacter ) tag symbols</definiens>
			</definition>
			<definition id="4">
				<sentence>VOTES denotes a regular expression of the sort `` &lt; `` \ [ '+ '' I '' - '' 3 DIGITS+ `` &gt; '' with DIGITS being the union of all decimal digit symbols .</sentence>
				<definiendum id="0">VOTES</definiendum>
				<definiens id="0">a regular expression of the sort `` &lt; `` \ [ '+ '' I '' - '' 3 DIGITS+ `` &gt; '' with DIGITS being the union of all decimal digit symbols</definiens>
			</definition>
			<definition id="5">
				<sentence>s Note that this simple version does not deal with rules whose constraints may overlap ( e.g. ( \ [ TAG=NN\ ] , \ [ TAG=NN\ ] ; 100 ) ) .</sentence>
				<definiendum id="0">s</definiendum>
				<definiens id="0">Note that this simple version does not deal with rules whose constraints may overlap</definiens>
			</definition>
			<definition id="6">
				<sentence>The ADD100 is a transducer that `` adds '' 100 to the vote field of the matching triplet .</sentence>
				<definiendum id="0">ADD100</definiendum>
				<definiens id="0">a transducer that `` adds '' 100 to the vote field of the matching triplet</definiens>
			</definition>
</paper>

		<paper id="1422">
			<definition id="0">
				<sentence>Direct generation algo• rithms apply the phra~se-structure rules ( schemata ) of the grammar on : line which is an computationally expensive step .</sentence>
				<definiendum id="0">phra~se-structure rules</definiendum>
				<definiens id="0">an computationally expensive step</definiens>
			</definition>
			<definition id="1">
				<sentence>• A tree selection phase determines the set of relevant TAG trees .</sentence>
				<definiendum id="0">tree selection phase</definiendum>
			</definition>
			<definition id="2">
				<sentence>• A combination phase finds a successful combination of trees to build a ( derived ) phrase structure tree .</sentence>
				<definiendum id="0">combination phase</definiendum>
				<definiens id="0">finds a successful combination of trees to build a ( derived ) phrase structure tree</definiens>
			</definition>
			<definition id="3">
				<sentence>* An inflection phase uses the information in the feature structures of the leaves ( i.e. the words ) to apply appropriate morphological functions , including the use of irregular forms as provided by the HPSG lexiconand regular inflection function as supplied ( as LISP code ) by the HPSG grammar .</sentence>
				<definiendum id="0">inflection phase</definiendum>
				<definiendum id="1">lexiconand regular inflection function</definiendum>
				<definiens id="0">uses the information in the feature structures of the leaves ( i.e. the words ) to apply appropriate morphological functions , including the use of irregular forms as provided by the HPSG</definiens>
			</definition>
			<definition id="4">
				<sentence>The tree selection phase consists of two steps .</sentence>
				<definiendum id="0">tree selection phase</definiendum>
			</definition>
			<definition id="5">
				<sentence>The combination phase explores the search space of all possible combinations of trees from the candidate sets for each lexical item ( instance ) .</sentence>
				<definiendum id="0">combination phase</definiendum>
				<definiens id="0">explores the search space of all possible combinations of trees from the candidate sets for each lexical item ( instance )</definiens>
			</definition>
			<definition id="6">
				<sentence>( ( FORM OKI ) INARY ) ( VFOKM BSE ) ( CAT V ) ( GOVE~-BY WH-SENTENCE ) ( OPTIONAL-AGENT NO ) ( HEAD ( OR SUIT_VI SUIT_V2 ) ) ( REALIZED LOCAL ) ( KEG LGVl ) ) ) ( ENTITY LI3-PRON ( ( REALIZED LOCAL ) ( CAT PPRON ) ( PERS 3 ) ( NUM SG ) ( GENDER NTR ) ( TYPE NORMAL ) ( GOVERNED-BY V ) ( IS-COMPLEMENT T ) ( FORM CONTINUOUS ) ( KEG LGVI ) ( FUNC AGENT ) ) ) ( ENTITY LI0-PRON ( ( REALIZED LOCAL ) ( CAT PPRON ) ( PERS 2A ) ( NUM SG ) ( GENDER FEM ) ( TYPE NORMAL ) ( GOVERNED-BY ( 0R V PREP SENTENCE ) ) ( FORM CONTINUOUS ) ( KEG L5-WORK_ACCEPTABLE ) ( FUNC • PATIENT ) ) ) ( ENTITY L6-TEMP_LOC ( ( CA T ADV ) ( REAL WH_QUEST ) ( SORT TIME ) ( POINTED'BY TEMP_LOC ) ( GOVERNED-BY ( 0R V N ADV SENTENCE ) ) ( PRED TIME ) ( HEAD WHEN1 ) • ( REALIZED L0CAL ) ( WH-FOCUS T ) ( KEG L5-WORE_ACCEpTABLE ) ( FUNC TEMP-SPEC ) ) ) ( ENTITY LI5-TEMP_LOC ( ( CAT ADV ) ( BEAD THEN_ADV ) ( REALIZED GRouP-TIME-DEMONSTRATiVE ) ( REAL ( 0R ADV WH_QUEST YOFC ) ) ( SORT ( SUBSORT TIME ) ) ( POINTED-BY TEMP_LOC ) ( GOVERNED-BY ( OR V N ADV SENTENCE ) ) ( BEG LS-WORK_ACCEPTABLE ) ( FUNC TEMP-SPEC ) ) ) ) • .</sentence>
				<definiendum id="0">FORM OKI ) INARY ) ( VFOKM BSE ) ( CAT V ) ( GOVE~-BY WH-SENTENCE )</definiendum>
				<definiendum id="1">BEG LS-WORK_ACCEPTABLE )</definiendum>
				<definiendum id="2">FUNC TEMP-SPEC</definiendum>
				<definiens id="0">NUM SG ) ( GENDER NTR ) ( TYPE NORMAL ) ( GOVERNED-BY V ) ( IS-COMPLEMENT T ) ( FORM CONTINUOUS ) ( KEG LGVI ) ( FUNC AGENT ) ) ) ( ENTITY LI0-PRON ( ( REALIZED LOCAL ) ( CAT PPRON ) ( PERS 2A ) ( NUM SG ) ( GENDER FEM ) ( TYPE NORMAL ) ( GOVERNED-BY ( 0R V PREP SENTENCE ) ) ( FORM CONTINUOUS ) ( KEG L5-WORK_ACCEPTABLE ) ( FUNC • PATIENT ) ) ) ( ENTITY L6-TEMP_LOC ( ( CA T ADV ) ( REAL WH_QUEST ) ( SORT TIME ) ( POINTED'BY TEMP_LOC ) ( GOVERNED-BY ( 0R V N ADV SENTENCE ) ) ( PRED TIME ) ( HEAD WHEN1 ) • ( REALIZED L0CAL ) ( WH-FOCUS T ) ( KEG L5-WORE_ACCEpTABLE ) ( FUNC TEMP-SPEC ) ) ) ( ENTITY LI5-TEMP_LOC ( ( CAT ADV ) ( BEAD THEN_ADV ) ( REALIZED GRouP-TIME-DEMONSTRATiVE )</definiens>
			</definition>
			<definition id="7">
				<sentence>Minimal recursion semantics : An introduction , available at ftp : //csli-ftp , stanford , edu/linguistics/sag/mrs.ps , gz. '</sentence>
				<definiendum id="0">Minimal recursion semantics</definiendum>
			</definition>
</paper>

		<paper id="1107">
			<definition id="0">
				<sentence>The final measure , generality , measures the number of terms and predicates in the meaning .</sentence>
				<definiendum id="0">generality</definiendum>
				<definiens id="0">measures the number of terms and predicates in the meaning</definiens>
			</definition>
</paper>

		<paper id="1101">
			<definition id="0">
				<sentence>The revised estimate was the mean It , of the density , defined as l / , ~kf ( xk ) , where l is the number of points evaluated for the function ( 1,000,001 ) .</sentence>
				<definiendum id="0">l</definiendum>
				<definiens id="0">revised estimate was the mean It , of the density , defined as l / , ~kf ( xk )</definiens>
			</definition>
			<definition id="1">
				<sentence>Posterior probability density for proportion of real documents The traditional way to find this interval is to assume a normal distribution , compute the variance c 2 of the posterior , defined as l f ( xk ) ( Xk-l .</sentence>
				<definiendum id="0">Posterior probability density</definiendum>
				<definiens id="0">for proportion of real documents The traditional way to find this interval is to assume a normal distribution</definiens>
			</definition>
			<definition id="2">
				<sentence>This approach is advantageous because the variance of a binomial density , ~ ( where n is the number sampled , and p the percentage of `` yes '' answers ) , shrinks dramatically for extreme values of p. Therefore , one can generally reduce sampling uncertainty by combining results from several homogeneous strata , rather than doing an overall sample from a heterogeneous population .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number sampled , and p the percentage of `` yes '' answers</definiens>
			</definition>
			<definition id="3">
				<sentence>Newbold ( 1971 ) gives the fraction q/allocated to each stratum i by cil/2Aill2 ( ni+l ) l/2 ( 2 ) qi = k cjl/2 Ajll2 ( nj+ I ) I/2 j=l where k is the number of strata , Ci is the cost of sampling a stratum ( assumed here to be 1 ) , n i 6 is the number of documents in the presample for the stratum , and Ai is Ai Hi 2 Pi ( 1-Pi ) = ( ni+2 ) ( 3 ) where Hi is the fraction of the overall population that comes from the ith stratum , and Pi is the population mean for the posterior density in the ith stratum .</sentence>
				<definiendum id="0">k</definiendum>
				<definiendum id="1">Ci</definiendum>
				<definiendum id="2">Hi</definiendum>
				<definiendum id="3">Pi</definiendum>
				<definiens id="0">the number of strata</definiens>
				<definiens id="1">the cost of sampling a stratum ( assumed here to be 1 )</definiens>
				<definiens id="2">the fraction of the overall population that comes from the ith stratum , and</definiens>
				<definiens id="3">the population mean for the posterior density in the ith stratum</definiens>
			</definition>
			<definition id="4">
				<sentence>Newbold ( 1971 ) k 'bi gives the weighted mean as i=~l~i n'-i ' where bi is the number of real documents found in stratum i out of ni sampled .</sentence>
				<definiendum id="0">bi</definiendum>
				<definiens id="0">1971 ) k 'bi gives the weighted mean as i=~l~i n'-i ' where</definiens>
				<definiens id="1">the number of real documents found in stratum i out of ni sampled</definiens>
			</definition>
			<definition id="5">
				<sentence>Recall is the percentage of relevant documents fiar a query that an IR system actually finds .</sentence>
				<definiendum id="0">Recall</definiendum>
			</definition>
			<definition id="6">
				<sentence>We thank the TIPSTER program and the Linguistic Data Consortium for making the Federal Register corpus available , and Mike Cannon and Tony Wamock for helpful discussions on statistical issues .</sentence>
				<definiendum id="0">Mike Cannon</definiendum>
				<definiens id="0">the Linguistic Data Consortium for making the Federal Register corpus available , and</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>Arabic is a highly inflected language .</sentence>
				<definiendum id="0">Arabic</definiendum>
				<definiens id="0">a highly inflected language</definiens>
			</definition>
			<definition id="1">
				<sentence>The morphological analysis process consists of a series of regular expressions partially supported by lists of noun , verb , and adjective stems , as well as closed-class items .</sentence>
				<definiendum id="0">morphological analysis process</definiendum>
				<definiens id="0">consists of a series of regular expressions partially supported by lists of noun , verb , and adjective stems , as well as closed-class items</definiens>
			</definition>
			<definition id="2">
				<sentence>We interpret this to mean that a manually built system with a moderate lexicon , having the capacity to only select one reading for a given form and not paying any attention l°The colunm headings are the standard ones from MUC : POS : possible number of points ( one point for identifying a constituent boundary , another for identifying its category ) , ACT : actual responses given , COP , : correct answers , PAR : boundary errors , INC : category labelling errors , SPU : responses given that are not in answer key , MIS : items in key missing from response , REC : recall ( COR/POS ) , PRE : precision ( COR/ACT ) , F-M : f-measure ( ( 2 .</sentence>
				<definiendum id="0">PAR</definiendum>
				<definiendum id="1">INC</definiendum>
				<definiendum id="2">SPU</definiendum>
				<definiendum id="3">MIS</definiendum>
				<definiendum id="4">REC</definiendum>
				<definiendum id="5">PRE</definiendum>
				<definiens id="0">boundary errors</definiens>
				<definiens id="1">responses given that are not in answer key ,</definiens>
				<definiens id="2">items in key missing from response ,</definiens>
				<definiens id="3">recall ( COR/POS ) ,</definiens>
			</definition>
</paper>

		<paper id="0602">
			<definition id="0">
				<sentence>A nucleus-structure consists of three parts : the inception-point ( IP ) , the development-portion ( DP ) and the culmination-point ( CP ) .</sentence>
				<definiendum id="0">nucleus-structure</definiendum>
			</definition>
			<definition id="1">
				<sentence>Thus , to each event e corresponds its execution-sequence G ( = ( s , s ' ) ) such that each event-type P , induces a binary relation R , , on S such that each element of R.v is the execution sequence of a ( completed ) event e e P~ .</sentence>
				<definiendum id="0">R.v</definiendum>
				<definiens id="0">s , s ' ) ) such that each event-type P , induces a binary relation R , , on S such that each element of</definiens>
			</definition>
			<definition id="2">
				<sentence>ss'\ [ ~Q ( s ) A Q ( s ' ) ^ Vs '' \ [ s &lt; s '' &lt; s ' - &gt; -~Q ( s '' ) \ ] \ ] In ( 8 ) Q is a property of states corresponding to the result that an event of type P~ is supposed to bring about .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiens id="0">s ) A Q ( s ' ) ^ Vs '' \ [ s &lt; s '' &lt; s '</definiens>
			</definition>
			<definition id="3">
				<sentence>Definition : a dynamic-temporal structure of events , intervals and states of signature &lt; VERB~ , VERBs , VAR , TR , DM &gt; is a tuple &lt; D , E , S , O , T , I , c~ , B , g , y , 5 , K , ~¢* , { Re } v'~ VERBs , { 0i } i~TR , coingco 0 , cog~ &gt; such that ( i ) D is the domain of entities ( or individuals ) ( ii ) E = &lt; E , m , { Pv } v ~ VERBn~ &gt; is an eventuality structure with E is a set of events m is a binary relation on E ( the part-of relation on E ) which is a partial order the Pv are unary relations on E ( iii ) S = &lt; S , { Rop ( Q ) } OP ~ DM , { Qp } pc VAR &gt; is a transition structure with S is a set of states { Rop ( Q ) } OP ~ DM is a set of parametrized relations on S. For each Q ~ { Qp } p VAR RopcQ ) is a set of ( finite ) sequences , i.e. a subset of S* ( S* is the set of all finite sequences of elements from S ) on which Q is evaluated in the same way .</sentence>
				<definiendum id="0">VERBn~ &gt;</definiendum>
				<definiens id="0">a dynamic-temporal structure of events , intervals and states of signature &lt; VERB~ , VERBs , VAR , TR , DM &gt; is a tuple &lt; D , E , S , O , T , I , c~ , B , g , y , 5 , K , ~¢*</definiens>
				<definiens id="1">the domain of entities ( or individuals ) ( ii ) E = &lt; E , m</definiens>
				<definiens id="2">a set of events m is a binary relation on E ( the part-of relation on E ) which is a partial order the Pv are unary relations on E ( iii ) S = &lt; S</definiens>
				<definiens id="3">a set of states { Rop ( Q ) } OP ~ DM is a set of parametrized relations on S. For each Q ~ { Qp } p VAR RopcQ ) is a set of ( finite ) sequences , i.e. a subset of S* ( S* is the set of all finite sequences of elements from S ) on which Q is evaluated in the same way</definiens>
			</definition>
			<definition id="4">
				<sentence>( iv ) O is the domain of urelements ( 'ordinary ' objects ) ( v ) T = &lt; T , &lt; &gt; is a ( time- ) point-structure with T the domain of time-points ( moments of time ) &lt; is a strict partial , linear and discrete ordering on T ( vi ) I = &lt; I , &lt; ~ , m &gt; is the interval-structure induced by T such that ( vii ) ( viii ) ( ix ) ( x ) ( xi ) ( xii ) I is the set of all non-empty finite convex sets ( intervals ) over &lt; T , &lt; &gt; \ [ t , t'\ ] &lt; I \ [ h , h'\ ] iff t ' &lt; t I \ [ t , t'\ ] ~ \ [ h , tl'\ ] iff tt_ &lt; t and t ' &lt; _ h ' Instead of construing the interval-structure I from the point-structure , one can take intervals as primitive and define a relation In between elements of T and elements of I which holds just in case t is an element of i in the structure I induced by T. cx : E- &gt; Tand13 : E - &gt; T assign to an event e its beginning-time o~ ( e ) and endtime 13 ( e ) , respectively ( it is required that co ( e ) &lt; 13 ( e ) ) .</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">the domain of urelements ( 'ordinary ' objects ) ( v ) T = &lt; T , &lt; &gt; is a ( time- ) point-structure with T the domain of time-points ( moments of time ) &lt; is a strict partial , linear and discrete ordering on T ( vi ) I = &lt; I</definiens>
				<definiens id="1">the set of all non-empty finite convex sets ( intervals ) over &lt; T , &lt; &gt; \ [ t</definiens>
				<definiens id="2">primitive and define a relation In between elements of T and elements of I which holds just in case t is an element of i in the structure I induced by T. cx : E- &gt; Tand13 : E - &gt; T assign to an event e its beginning-time o~ ( e ) and endtime 13 ( e ) , respectively ( it is required that co ( e ) &lt; 13 ( e ) )</definiens>
			</definition>
			<definition id="5">
				<sentence>The functions c~ and B induce a function x : E - &gt; I which assigns to each e e E its run-time interval x ( e ) such that z ( e ) = i = { t ~ T I ~ ( e ) _ &lt; t _ &lt; 13 ( e ) } Thus , z can also be defined as the product-mapping &lt; ct,13 &gt; : E - &gt; T x T. It : T - &gt; S is a function which assigns to each time point t ~ T the state It ( t ) ~ S which holds at t y : { P~ } - &gt; \ [ { Qp } - &gt; { Rdm ( Q ) } \ ] is a function which assigns to each P~ a dynamic mode , i.e. , a relation between properties of states and ( finite ) sequences of states ( see ( 8 ) above for details ) .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">The functions c~ and B induce a function x : E - &gt; I which assigns to each e e E its run-time interval x ( e ) such that z ( e ) = i = { t ~ T I ~ ( e ) _ &lt; t _ &lt; 13 ( e ) } Thus</definiens>
				<definiens id="1">the product-mapping &lt; ct,13 &gt; : E - &gt; T x T. It : T - &gt;</definiens>
				<definiens id="2">a function which assigns to each time point t ~ T the state It ( t ) ~ S which holds at t y : { P~ } - &gt; \ [ { Qp } - &gt; { Rdm ( Q ) } \ ] is a function which assigns to each P~ a dynamic mode , i.e. , a relation between properties of states</definiens>
			</definition>
			<definition id="6">
				<sentence>The role of ~¢ is similar to that of axioms like ( i ) which are used in ES : ( i ) Ve\ [ Pgiv , ( e ) - &gt; ~Xl , X2 , X3\ [ ( e , x I ) ~ 0ngen t A ( e , x2 ) e 0soo~ : o A ( e , X3 ) e 0~ , ~\ ] \ ] K* is a ( partial ) function which assigns to an event-type P and an event e an n-tuple of objects : * ( P ) ( e ) = &lt; x I ... .. x , &gt; iff ( i ) e e P , ( ii ) K ( P ) = &lt; 01 ... .. 0~ &gt; and ( iii ) ni\ [ ~c ( P ) \ ] ( e ) = xi ( xiv ) ( xv ) where ~ is the n-th projection function which when applied to the value of ~c for some P yields the n-th element of ~c ( P ) : if ~ : ( P ) = &lt; 0~ ... .. 0 , &gt; , then ~i ( n ( P ) ) = 0 i if 1 _ &lt; i _ &lt; n , undefined otherwise .</sentence>
				<definiendum id="0">~</definiendum>
				<definiens id="0">an event-type P and an event e an n-tuple of objects : * ( P ) ( e ) = &lt; x I ... .. x , &gt; iff ( i ) e e P , ( ii ) K ( P ) = &lt; 01 ... .. 0~ &gt; and ( iii ) ni\ [ ~c ( P ) \ ] ( e ) = xi ( xiv ) ( xv ) where</definiens>
			</definition>
			<definition id="7">
				<sentence>15 ( 10 ) a. b. Rv ( X~ ) ... ( x~ ) ( s ) ( s ' ) = 1 iff there is an e Pv such that ( i ) ~¢* ( P , ) ( e ) = &lt; x~ ... .. x~ &gt; ( ii ) ct* ( e ) = s ( iii ) B* ( e ) = s ' Rv = { &lt; x~ , ... x , ,s , s ' &gt; ~ D n x S x S I 3e\ [ e Pv A ( e , xi ) ~ 0 i A x* ( e ) = ( s , s ' ) \ ] } The condition ( i ) requires that the value for e of each e-role for which Pv is defined is an argument of the relation and that the i-th argument of R is the value of e for the i-th e-role assigned to Pv .</sentence>
				<definiendum id="0">x~ ) ( s</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">an argument of the relation and that the i-th argument of</definiens>
			</definition>
			<definition id="8">
				<sentence>( 11 ) R , ,¢o~o9 ~ = { &lt; x~ , ... x , ,s , s ' &gt; ~ D '' x S x S I 3e\ [ e ~ Pv A ( e , x i ) E 0 i A x* ( e ) = ( s , s ' ) A ( S , S ' ) ~ OP ( Q ) \ ] } ( 1 - &lt; i_ &lt; n ) For a given v , Rv~oPcoo~ is a subset of Rv. Similarly to R , , it is determined by ~¢ , ~¢* , ~* and , additionally , by 8 and y. According to ( 11 ) , a non-stative verb in the lexicon denotes a ( generalized ) relation between states , i.e. , it is an expression of sort &lt; ct , &lt; s , &lt; s , t &gt; &gt; &gt; where ¢x is a ( non-empty ) sequence of basic sorts , as required at the beginning of this section .</sentence>
				<definiendum id="0">OP</definiendum>
				<definiendum id="1">Rv~oPcoo~</definiendum>
				<definiendum id="2">¢x</definiendum>
				<definiens id="0">s ' ) A ( S , S ' ) ~</definiens>
				<definiens id="1">&lt; n ) For a given v ,</definiens>
				<definiens id="2">a subset of Rv. Similarly to R , , it is determined by ~¢ , ~¢* , ~* and , additionally , by 8 and y. According to ( 11 ) , a non-stative verb in the lexicon denotes a ( generalized ) relation between states , i.e. , it is an expression of sort &lt; ct</definiens>
				<definiens id="3">a ( non-empty ) sequence of basic sorts , as required at the beginning of this section</definiens>
			</definition>
			<definition id="9">
				<sentence>The relation R~ ( or Rv~opcoo~ ) represents the perspective of a change as a relation between states .</sentence>
				<definiendum id="0">relation R~</definiendum>
				<definiendum id="1">Rv~opcoo~ )</definiendum>
				<definiens id="0">the perspective of a change as a relation between states</definiens>
			</definition>
			<definition id="10">
				<sentence>The ingo/-morphem is then interpreted as mapping R~oP~op ) to the corresponding relation Rc ( o~Q , , i.e. as the function coi , g { oO : { R~ ( o~ , ~ } - &gt; { R~ ) { O~ ( Q , } which assigns to each element in its domain the corresponding relation in its range : cO~°~oo ( R. , ~O~ &lt; Q~ ) ) = R~ ( ~ ) &lt; Op~Q , . Using X-notation ( and the equivalence between sets and their characteristic functions ) , coiog ( oO is defined in ( 15 ) . ( R ranges over the relations in the domain of COing ( oO ) '' ( 15 ) ~i~toO = XRkx~ ... Xx , Xe\ [ \ [ rfl ( R ) \ ] ( x0 ... ( x , ) ( e ) \ ] According to ( 15 ) , coi.g~o o is the inverse of 1 '' 1. The ingo/-morphem does not change the argument-grid : each argument of the verb is also an argument of the nominalized expression ( see Zucchi ( 1993 ) for details ) . Furthermore , ingof is polymorphic in the sense that it applies to expressions of different arity. The definition of ~i , g ( oO in ( 15 ) is based on a relation between R~c~ ) ( oP { o9 ) and R~ , cOp ( Q~ ) . As was shown above , the relations R , , ( OptQ ) ) and R , cv ) ~oPco9 ) can be determined as the ranges of two functions p and p* , respectively , which map P~ to these relations. From these two relationships it follows that R~ ( ~ ) ( oP ( o9 ) can be defined in terms of R~ ( oP ( op ) if it is possible to 'recover ' P , from IL , ( op ( Q , . This is the case if P is injective ( i.e. , for v ~ : v ' , p ( P~ ) ¢ : p ( P¢ ) ) . C0ing ( o o can then be defined as the composition of p-i and p* , ( 16a ) , which yields ( 16b ) when spelled out : ( 16 ) a. ( .oing ( o 0 = p , . p-i b. ( .0ing ( o 0 = XRXxr..Xx , Xe\ [ e e fiX ( R ) ^ ( e , x i ) ~ 0 i ^ x* ( e ) a ~P~ ) ( ~i ( P~ ) ) \ ] ( l_ &lt; i_ &lt; n ) According to ( 16 ) , first p '' is applied to R. , ( oP~oj ) ( i.e. , the result of p for P , ) , yielding Pv , and next p* is applied to P , yielding Rccv~OPCOp ) : R~¢oPco~ ~ -- * ( P '' ) Pv -~ ( P* ) Rc~v~opcopr The aspectual restriction on ingofnominals is explained as follows. Stative verbs like know or love do not express changes. This is captured by interpreting them as generalized binary relations on S to which no events correspond ( the relations Re v ' ~ VERB , in ( xi ) above ) . The relations are therefore not derived from event-types Pv but are rather taken to be primitive. Consequently , there are no R , &lt; ~ ) ( and R~c~ ) ~opco~ ) ) such that the function p ( or p* ) and therefore co~ , g ( o o is not defined for them. Stative verbs denote sequences of states on which some condition Q continuously holds. The dynamic mode that corresponds to stative verbs is HOLD = XQXss'Vs '' \ [ s &lt; s '' &lt; s ' - &gt; Q ( s '' ) \ ] which can be interpreted as a kind of ( iterated ) testprogram from DL .</sentence>
				<definiendum id="0">C0ing</definiendum>
				<definiens id="0">mapping R~oP~op ) to the corresponding relation Rc ( o~Q , , i.e. as the function coi , g { oO : { R~ ( o~ , ~ } - &gt; { R~ ) { O~ ( Q , } which assigns to each element in its domain the corresponding relation in its range : cO~°~oo ( R. , ~O~ &lt; Q~ ) ) = R~ ( ~ ) &lt; Op~Q , . Using X-notation ( and the equivalence between sets and their characteristic functions ) , coiog ( oO is defined in ( 15 ) . ( R ranges over the relations in the domain of COing ( oO ) '' ( 15 ) ~i~toO = XRkx~ ... Xx , Xe\ [ \ [ rfl ( R ) \ ] ( x0 ... ( x , ) ( e ) \ ] According to ( 15 ) , coi.g~o o is the inverse of 1 '' 1. The ingo/-morphem does not change the argument-grid : each argument of the verb is also an argument of the nominalized expression ( see Zucchi ( 1993 ) for details ) . Furthermore , ingof is polymorphic in the sense that it applies to expressions of different arity. The definition of ~i , g ( oO in ( 15 ) is based on a relation between R~c~ ) ( oP { o9 ) and R~ , cOp ( Q~ ) . As was shown above , the relations R , , ( OptQ ) ) and R , cv ) ~oPco9 ) can be determined as the ranges of two functions p and p* , respectively , which map P~ to these relations. From these two relationships it follows that R~ ( ~ )</definiens>
				<definiens id="1">injective ( i.e. , for v ~ : v ' , p ( P~ ) ¢ : p ( P¢ ) ) .</definiens>
				<definiens id="2">the composition of p-i and p* , ( 16a ) , which yields ( 16b ) when spelled out : ( 16 ) a. ( .oing ( o 0 = p , . p-i b. ( .0ing ( o 0 = XRXxr..Xx , Xe\ [ e e fiX ( R ) ^ ( e , x i ) ~ 0 i ^ x* ( e ) a ~P~ ) ( ~i ( P~ ) ) \ ] ( l_ &lt; i_ &lt; n ) According to ( 16 ) , first p '' is applied to R. , ( oP~oj ) ( i.e. , the result of p for P , ) , yielding Pv , and next p* is applied to P , yielding Rccv~OPCOp ) : R~¢oPco~ ~ -- * ( P '' ) Pv -~ ( P* ) Rc~v~opcopr The aspectual restriction on ingofnominals is explained as follows. Stative verbs like know or love do not express changes. This is captured by interpreting them as generalized binary relations on S to which no events correspond ( the relations Re v ' ~ VERB , in ( xi ) above ) . The relations are therefore not derived from event-types Pv but are rather taken to be primitive. Consequently , there are no R , &lt; ~ ) ( and R~c~ ) ~opco~ ) ) such that the function p ( or p* ) and therefore co~ , g ( o o is not defined for them. Stative verbs denote sequences of states on which some condition Q continuously holds. The dynamic mode that corresponds to stative verbs is HOLD = XQXss'Vs '' \ [ s &lt; s '' &lt; s ' - &gt; Q ( s '' ) \ ] which can be interpreted as a kind of ( iterated ) testprogram from DL</definiens>
			</definition>
			<definition id="11">
				<sentence>The trace of an event can be defined as a pair &lt; i , OP ( Q ) &gt; and thus as a property of intervals .</sentence>
				<definiendum id="0">trace of an event</definiendum>
				<definiens id="0">a pair &lt; i , OP ( Q ) &gt; and thus as a property of intervals</definiens>
			</definition>
			<definition id="12">
				<sentence>The ingg , denotes the function oog , : { R* , ( Op ( Q~ ) } v ~ VERB , , u { R*v. ( HOLD ( Q , D } v'~ VERB s - &gt; ~o ( I ) which maps those binary relations R on S which result from an n+2-nary relation R~ ( op ( o : ) v ~ VERB after the first n arguments have been discharged to the set of intervals i on which an element of R has been actualized .</sentence>
				<definiendum id="0">Op</definiendum>
				<definiens id="0">the function oog , : { R*</definiens>
			</definition>
			<definition id="13">
				<sentence>Thus , for a non-stative verb v with non-mass singular arguments , R*~ ( op ( Q , is the parametrized relation { &lt; s , s ' &gt; ~ S x S I 3e\ [ e e P , A ( e , xi ) e 0 i A `` ~* ( e ) = ( S , S ' ) A ( S , S ' ) e y ( Pv ) ( 8 ( P~ ) ) \ ] } such that &lt; xv -- , x , ,s , s ' &gt; e Rv ( oP ( Q ) r For other types of arguments , i.e. plural ones , the situation is more complicated , as explained in section ( 2 .</sentence>
				<definiendum id="0">R*~ ( op</definiendum>
				<definiendum id="1">S ' )</definiendum>
				<definiens id="0">A ( S , S ' ) e y ( Pv ) ( 8 ( P~ ) ) \ ] } such that &lt; xv -- , x</definiens>
			</definition>
</paper>

		<paper id="0702">
			<definition id="0">
				<sentence>For example , shaw denotes a state in , H/ $ lumbar puncture showed evidence of white cells , but denotes an event in , He showed me the photographs .</sentence>
				<definiendum id="0">shaw</definiendum>
			</definition>
			<definition id="1">
				<sentence>Counts are over all have-clauses in the medical reports corpus , from which the supervised training and testing data were extracted .</sentence>
				<definiendum id="0">Counts</definiendum>
				<definiens id="0">are over all have-clauses in the medical reports corpus , from which the supervised training and testing data were extracted</definiens>
			</definition>
</paper>

		<paper id="0317">
			<definition id="0">
				<sentence>By using a machine learning technique , a variety of features concerning discourse structure , task structure , and dialogue context are examined in terms of their effectiveness and the best set of learning &lt; features is identified. Our result reveals that , in addition to discourse structure , already identified in previous studies , task structure and dialogue context play an important role. Moreover , an evaluation using a large dialogue corpus shows the utihty of applying machine learning techniques to cue phrase selection. Cue phrases are words and phrases , such as `` first '' , `` and '' , `` now '' , that connect discourse spans and add structure to the discourse both in text and dialogue. They signal topic shifts and changes in attentional state ( Grosz and Sidner , 1986 ) as well as expressing the relation between the individual units of discourse ( Moore , 1995 ; RSsner and Stede , 1992 ) . In this study , we focus on the former kind of cue phrases , organization cue phrases that signal the structural organization of discourse. In instruction dialogue , the organization cue phrases play a crucial role in controlling dialogue and making the material easy to understand. Moreover , in dialogue systems , the user can not comprehend the structural organization of the dialogue unless the appropriate cue phrases are included in the system 's utterances. Therefore , for dialogue generation , we must identify the determining factors of organization cue phrases and select the cue phrases appropriately. In previous studies that have investigated the relationship between cue phrases and the types of structural change ( e.g. pop , push ) , the taxonomies of cue phrases have been presented ( Grosz and Sidner , 1986 ; Cohen , 1984 ; Schiffrin , 1987 ) . These taxonomies are , however , not sufficient for generation because the correspondence between cue phrase and structural change is many-to-many quite often. For example , `` now '' , '' and '' , and `` next '' are all classified as the category signaling push in attentional state. Therefore , the indication of structural shifts in discourse is not sufficient to fully constrain cue phrase selection. In this study , we reveal what factors affect organization cue phrase selection , and establish more precise selection rules for generating instruction dialogues. As factors for cue phrase selection , we examine a variety of features concerning discourse structure , task structure , and dialogue context. The reason that we examine these three factors is as follows. First , discourse structure is indispensable for selecting cue phrase as claimed in previous studies ( Grosz and Sidner , 1986 ; Cohen , 1984 ; Eugenio et al. , 1997 ) . We examine some features concerning this factor such as the global structure of discourse and structural shifts in discourse. Second , while the discourse structure provides information about the preceding discourse , Cawsey ( 1993 ) claimed that information about the succeeding discourse ( e.g. , length and complexity ) is also necessary in order to select cue phrases dynamically in dialogue systems. From this point of view , task structure is expected to be effective because discourse structure strongly reflects task structure in task oriented dialogue ( Grosz , 1977 ; Guindon , 1986 ) . Finally , in contrast to these structural aspects of dialogue , we think it important to consider sequential contexts of dialogue such as the types of dialogue exchange ( StenstrSm , 1994 ) immediately preceding to the cue phrase. In this paper , using a machine learning technique , C4.5 ( Quinlan , 1993 ) , we examine these features in terms of their effectiveness in selecting organization cue phrase and identify the most effective set of learning features. In addition , we evaluate the accuracy of decision trees obtained using a large corpus. Our result reveals that , in addition to discourse structure whose effectiveness has already revealed in previous studies , task structure and dialogue context play important roles. Especially important are the place of the segment in the global structure of the dialogue and the type of the immediately preceding dialogue exchange. The organization of this paper is as follows. Section 2 discusses related work. Section 3 mentions the annotation of our dialogue corpus while section 4 details the learning experiment and its results are discussed. Section 5 refers to further work and concludes this paper. 100 While cue phrases can appear in different places in instruction dialogues , we focus on the organization cue phrases that occur at the beginning of discourse segments referring to goals or direct actions. This is because such kind of cue phrases have the important function of describing the basic structure of the dialogue. In a procedural instruction dialogue , the sequence of actions for the procedure is directed step by step. In terms of Rhetorical Structure Theory ( RST ) ( Mann and Thompson , 1987 ) , it is considered that the basic structure of such kind of discourse is constructed by connecting segments that refer to goals or primitive actions with `` sequence '' relation ( RSsner and Stede , 1992 ; Kosseim and Lapalme , 1994 ) . Therefore , the cue phrases which occur at the beginning of segments that are connected with `` sequence '' relation and refer to goals or direct actions play important roles in signaling the basic structure of the dialogue. Moreoyer , such kind of cue phrases are observed very frequently in instruction dialogues. In their empirical study on the characteristics of task oriented dialogues , Oviatt and Cohen ( 1990 ) reported that , in instruction dialogues on assembling a pump , cue phrases such as `` Okay '' , `` now '' and `` next '' occur at the beginning of 98.6 % of the new segments that instruct assembly actions in telephone dialogues. Based on the above , we think it important for dialogue generation to select and set appropriate cue phrases at the beginning of discourse segments that refer to goals or direct actions. Moser and Moore ( 1995a ) and Moser and Moore ( 1995b ) investigated the relationship between cue placement and selection. They showed that the cue phrases are selected and distinguished depending on their placement. Somewhat differently , we tackle the problem of selecting cue phrases that occur at the same place in the segment ( at the beginning of the segment ) . As indicated in ( Eugenio et al. , 1997 ) , in terms of natural language generation , cue usage consists of three problems , occurrence : whether or not a cue should be included , placement : where the cue should be placed , and selection : what cue should be used. We tackle the third problem , the selection of cue phrases. Our final goal is to establish a strategy for selecting organization cue phrases and apply it in the generation of instruction dialogues. While the empirical approach of this study is close to that of ( Eugenio et al. , 1997 ) , they apply a machine learning technique to predicating cue occurrence and placement , not cue phrase selection. In this section , we mention the way of the annotation in our corpus. Then , the inter-coder agreement for the annotations is discussed. The domain of our dialogue corpus in Japanese is to instruct the initial setting of an answering machine. The corpus consists of nine dialogues with 5,855 utterances. There are , 1,117 cue phrases in 96 distinct ds3.3 ds3.4 ds3.5 D T : And , there is a time-punch button under the panel , { P : Yes. } push it. P : Yes. I '' '' T : An.._.d month and day are input as L integers. ds3.4.1 P : Yes. T : Input by dial button. P : Yes f-T : First , It is January 27th. P : Yes. -- T'Input0 1 2 7 ds3 4 21 ... .. • `` | i-P : Can I input under this I ds3 '' 4 '' 2 '' 1/ condition ? | l-T : Yes t__ p : I 've done. i T : Yes , ~ today is Thursday , { P : Yes } the days of the week are numbered from one to seven ds3.4.3 starting with Sunday , { P : Yes } since today is Thursday , input number is 5. t__p : Yes , I 've input it. -- T : An._..d , it is two thirty now , { P : Yes } using the 24 hour time ds3.4.4 system , { P : yes } input 1 , 4 , 3 , O. m a__p : Yes. I 've input it. B T : Finally , push the registration button again. -P : Yes. Figure 1 : An example of annotated dialogue cues 1. There are 31 cue phrases that occur more than five times. As the result of classifying these 31 cue phrases based on the classification of Japanese connectives ( Ichikawa , 1978 ; Moriyama , 1997 ) and cue phrase classification in Enghsh ( Grosz and Sidner , 1986 ; Cohen , 1984 ; Knott and Dale , 1994 ; Moser and Moore , 1995b ) , 20 cue phrases , which occurred total of 848 times , were classified into three classes : changeover , such as soredeha , deha ( `` now '' , `` now then '' in English ) , conjunctive , such as sore.de , de ( `` and '' , `` and then '' ) , and ordinal , such as mazu , tsugini ( `` first '' , `` next '' ) . Besides these simple cue phrases , there are composite cue phrases such as soredeha-tsugini ( `` now first '' ) . Note that meaning and the usage of each of these Japanese cue phrases does not completely correspond to those of the English words and phrases in parentheses. For example. the meaning of the Japanese cue phrase soredeha is close to the English word now in its discourse sense. However , soredeha does not have a sentential sense though now does. The purpose of this study is to decide which of these three classes of simple cue phrases should be selected as the cue phrase at the beginning of a disICue phrases which occur in the middle of the segment and in the segment other than action direction such as clarification segment are included. I01 course segment. We do not deal with composite types of cue phrases. As the basis for examining the relationship between cue phrase and dialogue structure , discourse segment boundary and the level of embedding of the segments were annotated in each dialogue. We define discourse segment ( or simply segment ) as chunks of utterances that have a coherent goal ( Grosz and Sidner , 1986 ; Nakatani et al. , 1995 ; Passonneau and Litman , 1997 ) . The annotation of hierarchical relations among segments was based on ( Nakatani et al. , 1995 ) . Figure 1 shows an example from the annotated dialogue corpus. This dialogue was translated from the original Japanese. This example provides instruction on setting the calendar and clock of the answering machine. The purpose of ds3.4 is to input numbers by dial buttons and each input action is directed in ds3.4.2 , ds3.4.3 , and ds3.4.4 , for inputting the date , the day of the week , and the time , respectively. Subdialogues such as confirmation and pupil initiative clarification are treated as one segment as in ds3.4.2.1. The organization cue phrases are underlined in the sample dialogue. For example , the cue phrase for ds3.3 is `` And '' , and that for ds3.5 is `` Finally '' 2 pre-exchange As the information about task structure and dialogue context , we annotated the discourse purpose of each segment and the dialogue exchange at the end of the immediately preceding segment. In annotating the discourse purpose , the coders selected the purpose of each segment from a topic list. The topic list consists of 127 topics. It has a hierarchical structure and represents the task structure of the domain of our corpus. When the discourse purpose can not be selected from the topic list , the segment was annotated as `` others '' . In such segments , the information about task structure can not be obtained. The pre-exchange is annotated as a kind of dialogue context and used as one of the learning features itself. The coders annotated the kind of preexchange by selecting one of nine categories of exchanges which are defined in section 4.1 in detail. As mentioned in the previous sections , we annotated our corpus with regard to the following characteristics : the class of cue phrases ( ordinal , changeover , conjunctive ) , segment boundary , and hierarchical structure of the segment , the purpose of the segment , and the dialogue exchange at the end of the immediately preceding segment. The extent of inter-coder agreement between two coders in these annotation are calculated by using : When a cue phrase follows acknowledgement ( Yes ) or a stammer , these speech fragments that do not have ~ftropositional content axe ignored and the cue phrases er the fragments axe annotated as the beginning of the segment. Cohen 's Kappa ~ ( Bakeman and Gottman , 1986 ; Carletta , 1996 ) . The inter-coder agreement ( to ) about the class of cue phrase is 0.68 , about the purpose of the segment is 0.79 , and about the type of pre-exchange is 0.67. The extent of agreement about the segment boundary and the hierarchical structure is calculated using modified Cohen 's Kappa presented by ( Flammia and Zue , 1995 ) . This Cohen 's Kappa is 0.66. Fleiss et al. ( 1981 ) characterizes kappas of .40 to .60 as fair , .60 to .75 as good , and over .75 as excellent. According to this categorization of levels of inter-coder agreement , the inter-coder agreement for cue phrase , pre-exchange , and discourse boundary and structure is good. The agreement on segment purpose is excellent. Thus , these results indicate that our corpus coding is adequately rehable and objective. When the two coders ' analyses did not agree , the third coder judged this point ; only those parts whose analysis is output by more than two coders was used as learning data. This section describes a learning experiment using C4.5 ( Quinlan , 1993 ) . First , we define 10 learning features concerned with three factors. ( 1 ) Discourse structure : Structural information about the preceding dialogue. Embedding The depth of embedding from the top level. Place The number of elder sister segments. Place2 The number of elder sister segments except pupil initiative segments. Recent elder sister 's cue ( Res-cue ) The cue phrase that occurs at the beginning of the most recent elder sister segment. They axe classified into three kinds of simple cue phrases : ord ( ordinal ) , ch ( changeover ) , con ( conjunctive ) or a kind of composite cue phrase such as ch+ord ( changeover + ordinal ) . Res-cue2 The cue phrase that occurs at the beginning of the most recent elder sister segment except pupil initiative segments. Discourse transition ( D-trans ) Types of change in attentional state accompanied by topic change 3 such as push and pop. Pop from the pupil initiative subdialogue is categorized as `` ui-pop '' . ( 2 ) Task structure : Information that estimates the complexity of succeeding dialogue. 3Clark ( 1997 ) presents a term `` discourse topic '' as concept equivalent to focus space in ( Grosz and Sidner , 1986 ) , and call their transition `` discourse transition '' . For example , `` push '' is defied as the transition to the sub topic , and `` next '' is defined as the transition to the same level proceeding topic. 102 factor Discourse structure Task structure Dialogue structure Table 1 : The learning features feature name Embedding Fla~ : e l~ , lace2 H.es-cue l~es-cue2 D-trans T-hmraxchy ~ubgoal Fre-exchange Fs-cue values integer mteger mteger nil , ord , Oh , con , ch+ord , con÷ord , con+ch , other nil , ord , C\ [ 1 , Cou , cn÷ord~ con+ord , con+ch , other pop , push , next , m-pop , ~A integer mteger conf , req , inf , quest , ui-conf , ui-req , tti-inf , ui-quest , NA nil , oral , ch , con , ch+ord , con+ord , con+ch , other Task-hierarchy ( T-hierarchy ) The number of goal-subgoal relations from the current goal to primitive actions. This estimates the depth of embedding in the succeeding dialogue. Subgoal The number of direct subgoals of the current goal. If zero , then it is a primitive action. ( 3 ) Dialogue context Information about the preceding segment. Pre-exchange Type of exchange that occurs at the end of the immediately preceding segment , or type of exchange immediately preceding the cue phrase. There are four categories , conf ( confirmation-answer ) , req ( request-answer ) , inf ( information-reply ) . ques ( question-answer ) . They are also distinguished by the initiator of the exchange ; explainer initiative or pupil initiative. When the category of the exchange is not clear , it is classified as not applicable ( NA ) . Therefore , there are nine values for this feature. Preceding segment 's cue ( Ps-cue ) The cue phrase that occurs at the beginning of the immediately preceding segment. The values of these features are shown in Table 1. Among the above learning features , Embedding , Place , Place $ , Res-cue , Res-cue~ , Ps-cue , and D-trans are derived automatically from the information about segment boundary and the segment hierarchy annotated in the corpus ( an example is shown in Figure 1 ) . The depth of task hierarchy ( Thierarchy ) and the number of direct subgoais ( Subgoal ) are determined by finding the annotated segment purpose in the given task structure. In this study , C4.5 ( Quinlan , 1993 ) is used as learning program. This program takes two inputs , ( 1 ) the definition of classes that should be learned , and the names and the values of a set of features , and ( 2 ) the data which is a set of instances whose class and feature values are specified. As a result of machine learning , the program outputs a decision tree fe~ judgement. We use cross-validation for estimating the accuracy of the model because this method avoids the disadvantages common with small data sets whose number of cases is less than 1000. In this study , 10-fold cross-validation is applied , so that in each run 90 % of the cases are used for training and the remaining 10 % are used for testing. The C4.5 program also has an option that causes the values of discrete attribute to be grouped. We selected this , option because there are many values in some features and the decision tree becomes very complex if each value has one branch. Decision trees for distinguishing the usage of three kinds of cue phrases ( changeover , ordinal , and conjunctive ) were computed by the machine learning al : gorithm C4.5. As learning features , the 10 features mentioned in section 4.1 are used. From nine dialogues ; 545 instances were derived as training data. In 545 instances , 300 were conjunctive , 168 were changeover , and 77 were ordinal. The most frequent category , conjunctive , accounts for 557o of all cases. Thus , the baseline error rate is 4570. This means that one would be wrong 45~0 of the time if this category was always chosen. First , the prediction power of each learning feature is examined. The results of learning experiments using single features are shown in Table 2. I.~ pruning the initial tree , C4.5 calculates actual and estimated error rates for the pruned tree. The error rate shown in this table is the mean of estimated error rates for the pruned trees under 10-fold crossvalidation. The 95 % confidence intervals are shown after `` '± '' . Those are calculated using Student 's t distribution. The error rate el is significantly better than e2 if the upper bound of the 95 % confidence interval for e~ is lower than the lower bound of the 95 % confidence interval for e2. As shown in Table 2 , the decision tree obtained with the Pre-exchange fen103 Table 2 : The error rates with each model Embedding 'Place Place'~ -Res-cue l-tes-cue2 D-trans T-hierarchy `` ' Subg0al 46.5 2:0.1 42.5 ± 0.4 43.8 : :k 0.4 44.9 ± 0.3 45.1 ± 0.4 45.0 ± 0.5 42.4 ± 0.3 42.5 ± 0.3 Fre-exchaage Fs-cue DS model Task model D ( _ , model All Ieature model Simplest model 41.5 ± 0.5 46,5 =k 0.3 35,6 ± 0.4 41,8 2:0.3 39.1 : :l : : 0.6 29.9 2:0.4 30.6 ± 0.3 Table 3 : The set of learning features for each model I Discourse Structure il rl'ask Structure Model EmbeddFlace I Place2 Resl-tes { D-trans I T5ubgoal in~ ; . : cue cue 2 hierarchy DS ~ 4 4 4 V , / Task V , / DC An feature ¢ V ~ q , / ¢ : , ¢_ 4 Simplest ' , h f ~/ ' ~/ Ii .. ~/ II Dialogue Context I PreFs-cue exch~nse I 4 4 4 4 4 4 ture performs best , and its error rate is 41.5 % . In all experiments , the error rates are more than 40 % and none are considerably better than the baseline. These results suggest that using only a single learning feature is not sufficient for selecting cue phrases correctly. As the single feature models are not sufficient , it is necessary to find the best set of learning features for selecting cue phrases. We call a set of features a model and the best model ( the best set of features ) is obtained using the following procedure. First , we set some multiple features models and carry out learning experiments using these models in order to find the best performing model and the best error rate. We then eliminate the features from the best performance model in order to make the model simpler. Thus , the best model we try to find is the one that uses the smallest number of learning features but whose performance equals the best error rate. We construct four multiple feature models. The name of the model and the combination of features in the model are shown in Table 3. The discourse structure model ( the DS model ) used learning features concerned with discourse structure. The Task model used those concerned with task structure , and the dialogue context ( the DC model ) used those concerned with dialogue context. The All .feature model uses all learning features. The best error rate among these models is 29.9 % in All .feature model as shown in Table 2. The error rate is reduced about 15 % from the baseline. Therefore , the best model is the one that uses fewer learning features than the All .feature model and that equals the performance of that model. In order to reduce the number of features considered , we examined which features have redundant information , and omitted these features from the All \ ] eature model. The overlapping features were found by examining the correlation between the features. As for numerical features that take number values , the correlation coefficient between Place and Place~ , and between T-hierarchy and Subgoal are high ( p=0.694 , agreement between Res-cue and Res-cue2 is 95 % . These highly correlated features can be represented by just one of them. As the result of many experiments varying the combination of features used , we determined the Simplest model which uses six features : Embedding , Place , D-trans , Subgoal , Preezchange , and Ps-cue as shown at the bottom line in Table 3. The error rate of the Simplest model is 30.6 % as shown in Table 2. It is very close to that of the All \ ] eature model though the difference is statistically significant. In addition to comparing only the overall error rates , in order to compare the performance of these two models in more detail , we calculated the information retrieval metrics for each category , changeover , ordinal , and conjunctive. Figure 2 shows the equations used to calculate the metrics. For example , recall rate is the ratio of the cue phrases correctly predicted by the model as class X to the cue phrases of class X in the corpus. Precision rate is the ratio of cue phrases correctly predicted to be class X to all cue phrases predicted to be class X. In addition , in order to get an intuitive feel of overall performance , we also calculated the sum of the deviation from ideal values in each metric as in ( Passonneau and Litman , 1997 ) . The summed deviation is calculated by the following numerical formula : ( 1 Recall ) + ( 1 Precision ) + Fallout + Error Table 4 shows the results of these metrics for the two models. Standard deviation is shown in parentheses. The value of each metric is the average of 104 Table 4 : Performance on training set using cross-validation \ [ Model All feature model Simplest model Cue phrase t.tecaU Precimon FaUout \ [ Error \ [ Summed Deviation I ordinal 0.50 ( 0.HI ) 0.64 ( 0.10 ) 0.05 ( 0.03 ) , 0.11 ( 0.03 ) , 1.03 ( 0.23 ) , changeover 0.53 ( 0.1 , : ) 0.58 ( 0.07 ) 0.17 ( 0.05 ) \ [ 10 '' 26 ( 0.04 ) 1.32 ( 0.23 ) conjunctive 0.80 ( 0.01 ; ) 0.73 ( 0.05 ) 0.38 ( 0.11 ) 0.28 ( 0.04 ) 1.12 ( 0.16 ) ordinal 0.48 ( 0.17 ) 0.66 ( 0.17 ) 0.45 ( 0.03 ) 0.11 ( 0.02 ) 1.01 ( 0.26 ) I changeover I 0.50 ( 0.12 ) 0.62 ( 0.08 ) 0.14 ( 0.03 ) 0.25 ( 0.05 ) 1.27 ( 0.24 ) I I conjunctive \ [ 0.85 ( 0.04 ) 0.72 ( 0.04 ) 0.40 ( 0.08 ) 0.26 ( 0.04 ) 1.09 ~0.17 ) I Class-X C4.5 Program not-Class-X Corpus Class-X not-Class-X a b c d Recall = a Fallout = b ( a+c ) ( b+d ) Precision = a Error = ( b+c ) ( a+b ) ( a+b+c+d ) Figure 2 : Information retrieval metrics the metrics on the test set in each run of 10-fold cross-validation. Comparing the summed deviation. the performance of the Simplest model is better than that of the All feature model in all categories of cue phrases. The summed deviations of the Simplest model , 1.01 for ordinal , 1.27 for changeover , and 1.09 for conjunctive , are lower than those of the All feature model. Thus , as a result of evaluating the models in detail using the information retrieval metrics , it is concluded that the Simplest model is the best performing model. In addition , the Simplest model is the most elegant model because it uses fewer learning features than the All feature model. Just six features , Embedding , Place , D-trans , Subgoal , Preexchange , and Ps-cue , are enough for selecting organization cue phrases. Classifying the six features in the Simplest model , it is found that these features come from all factors , discourse structure , task structure , and dialogue context. Embedding , Place , D-trans are the features of discourse structure , Subgoal is about task structure , and Pre-exchange and Ps-cue are about dialogue context. This result indicates that all the factors are necessary to predict cue phrases. The important factors for cue phrase selection are task structure and dialogue context as well as discourse structure , the focus of many earlier studies. While we identified the six features from the three kinds of factors , by looking at the decision trees created in the learning experiment , we found which features were more important than others in selecting cue phr~es. The features appearing near the root node are more important. Figure 3 shows the top part of a decision tree obtained from the Simplest model. In all 10 decision trees resulting from the cross-validation experiment in the Simplest model , Place feature appears at the root node. In 7 of 10 trees , Embedding and Pre-exchange appeared just below the root node. In these trees , if the Place of the segment is the first at that level ( i.e. there is no elder sister. ) , then Embedding appears at the next node , otherwise if the segment is not the first one at that level , then Pre-exchange appears atthe next node. Thus , if there are some elder sister segments , information about dialogue context is used for selecting cue phrases. On the other hand , if there is no elder sister segment , information about discourse structure is used for the judgement. These results suggest that the information about discourse structure , especially place of segments and the depth of embedding , and the dialogue context , especially the kind of immediately preceding dialogue exchange , play important roles in cue phrase selection. This paper reported the results of using a machine learning algorithm for identifying learning features and obtaining decision trees for selecting cue phrases. It also reported the result of a quantitative evaluation of the decision trees learned. Learning features concerning three factors , discourse structure , task structure , and dialogue context , were examined. By carrying out many experiments in which the combinations of learning features were varied , we found the most simple and effective learning feature set. The accuracy of the best model that uses 6 learning features is about 70 % . The error rate is reduced about 25 % from the baseline. These results support the claims of previous studies that discourse structure influence cue selection. In addition , it is revealed that task structure and dialogue context are also indispensable factors. We focus on predicting the cue phrases that occur at the beginning of discourse segments for signaiing inter-segment `` sequence '' relation. Elhadad and McKeown ( 1990 ) , on the other hand , has presented a model for distinguishing connectives , which link two propositions , using some pragmatic constraints. In ( Moser and Moore , 1995a ; Moser and Moore , 1995b ) , the relationship between placement and selection of cue phrases was investigated using the core : contributor relations among units within a segment ( Moser and Moore , 1995a ) . Although we discussed only the `` sequence '' relation between the 10,5 ( Ernbedding~ ~Pre-exchan'ge ) &gt; 3~3 ¢onf , ui-inf , ~ req , inf , uiothers `` X~onf , ui-q changeover ... . ~ ... . ~ Oh , con , .</sentence>
				<definiendum id="0">recall rate</definiendum>
				<definiendum id="1">model</definiendum>
				<definiendum id="2">Simplest model</definiendum>
				<definiens id="0">By using a machine learning technique , a variety of features concerning discourse structure , task structure , and dialogue context are examined in terms of their effectiveness and the best set of learning &lt; features is identified. Our result reveals that , in addition to discourse structure , already identified in previous studies , task structure and dialogue context play an important role. Moreover , an evaluation using a large dialogue corpus shows the utihty of applying machine learning techniques to cue phrase selection. Cue phrases are words and phrases , such as `` first '' , `` and '' , `` now '' , that connect discourse spans and add structure to the discourse both in text and dialogue. They signal topic shifts and changes in attentional state ( Grosz and Sidner , 1986 ) as well as expressing the relation between the individual units of discourse ( Moore , 1995 ; RSsner and Stede , 1992 ) . In this study , we focus on the former kind of cue phrases , organization cue phrases that signal the structural organization of discourse. In instruction dialogue , the organization cue phrases play a crucial role in controlling dialogue and making the material easy to understand. Moreover , in dialogue systems , the user can not comprehend the structural organization of the dialogue unless the appropriate cue phrases are included in the system 's utterances. Therefore , for dialogue generation , we must identify the determining factors of organization cue phrases and select the cue phrases appropriately. In previous studies that have investigated the relationship between cue phrases and the types of structural change ( e.g. pop , push ) , the taxonomies of cue phrases have been presented ( Grosz and Sidner , 1986 ; Cohen , 1984 ; Schiffrin , 1987 ) . These taxonomies are , however , not sufficient for generation because the correspondence between cue phrase and structural change is many-to-many quite often. For example , `` now '' , '' and '' , and `` next '' are all classified as the category signaling push in attentional state. Therefore , the indication of structural shifts in discourse is not sufficient to fully constrain cue phrase selection. In this study , we reveal what factors affect organization cue phrase selection , and establish more precise selection rules for generating instruction dialogues. As factors for cue phrase selection , we examine a variety of features concerning discourse structure , task structure , and dialogue context. The reason that we examine these three factors is as follows. First , discourse structure is indispensable for selecting cue phrase as claimed in previous studies ( Grosz and Sidner , 1986 ; Cohen , 1984 ; Eugenio et al. , 1997 ) . We examine some features concerning this factor such as the global structure of discourse and structural shifts in discourse. Second , while the discourse structure provides information about the preceding discourse , Cawsey ( 1993 ) claimed that information about the succeeding discourse ( e.g. , length and complexity ) is also necessary in order to select cue phrases dynamically in dialogue systems. From this point of view , task structure is expected to be effective because discourse structure strongly reflects task structure in task oriented dialogue ( Grosz , 1977 ; Guindon , 1986 ) . Finally , in contrast to these structural aspects of dialogue , we think it important to consider sequential contexts of dialogue such as the types of dialogue exchange ( StenstrSm , 1994 ) immediately preceding to the cue phrase. In this paper , using a machine learning technique , C4.5 ( Quinlan , 1993 ) , we examine these features in terms of their effectiveness in selecting organization cue phrase and identify the most effective set of learning features. In addition , we evaluate the accuracy of decision trees obtained using a large corpus. Our result reveals that , in addition to discourse structure whose effectiveness has already revealed in previous studies , task structure and dialogue context play important roles. Especially important are the place of the segment in the global structure of the dialogue and the type of the immediately preceding dialogue exchange. The organization of this paper is as follows. Section 2 discusses related work. Section 3 mentions the annotation of our dialogue corpus while section 4 details the learning experiment and its results are discussed. Section 5 refers to further work and concludes this paper. 100 While cue phrases can appear in different places in instruction dialogues , we focus on the organization cue phrases that occur at the beginning of discourse segments referring to goals or direct actions. This is because such kind of cue phrases have the important function of describing the basic structure of the dialogue. In a procedural instruction dialogue , the sequence of actions for the procedure is directed step by step. In terms of Rhetorical Structure Theory ( RST ) ( Mann and Thompson , 1987 ) , it is considered that the basic structure of such kind of discourse is constructed by connecting segments that refer to goals or primitive actions with `` sequence '' relation ( RSsner and Stede , 1992 ; Kosseim and Lapalme , 1994 ) . Therefore , the cue phrases which occur at the beginning of segments that are connected with `` sequence '' relation and refer to goals or direct actions play important roles in signaling the basic structure of the dialogue. Moreoyer , such kind of cue phrases are observed very frequently in instruction dialogues. In their empirical study on the characteristics of task oriented dialogues , Oviatt and Cohen ( 1990 ) reported that , in instruction dialogues on assembling a pump , cue phrases such as `` Okay '' , `` now '' and `` next '' occur at the beginning of 98.6 % of the new segments that instruct assembly actions in telephone dialogues. Based on the above , we think it important for dialogue generation to select and set appropriate cue phrases at the beginning of discourse segments that refer to goals or direct actions. Moser and Moore ( 1995a ) and Moser and Moore ( 1995b ) investigated the relationship between cue placement and selection. They showed that the cue phrases are selected and distinguished depending on their placement. Somewhat differently , we tackle the problem of selecting cue phrases that occur at the same place in the segment ( at the beginning of the segment ) . As indicated in ( Eugenio et al. , 1997 ) , in terms of natural language generation , cue usage consists of three problems , occurrence : whether or not a cue should be included , placement : where the cue should be placed , and selection : what cue should be used. We tackle the third problem , the selection of cue phrases. Our final goal is to establish a strategy for selecting organization cue phrases and apply it in the generation of instruction dialogues. While the empirical approach of this study is close to that of ( Eugenio et al. , 1997 ) , they apply a machine learning technique to predicating cue occurrence and placement , not cue phrase selection. In this section , we mention the way of the annotation in our corpus. Then , the inter-coder agreement for the annotations is discussed. The domain of our dialogue corpus in Japanese is to instruct the initial setting of an answering machine. The corpus consists of nine dialogues with 5,855 utterances. There are , 1,117 cue phrases in 96 distinct ds3.3 ds3.4 ds3.5 D T : And , there is a time-punch button under the panel , { P : Yes. } push it. P : Yes. I '' '' T : An.._.d month and day are input as L integers. ds3.4.1 P : Yes. T : Input by dial button. P : Yes f-T : First , It is January 27th. P : Yes. -- T'Input0 1 2 7 ds3 4 21 ... .. • `` | i-P : Can I input under this I ds3 '' 4 '' 2 '' 1/ condition ? | l-T : Yes t__ p : I 've done. i T : Yes , ~ today is Thursday , { P : Yes } the days of the week are numbered from one to seven ds3.4.3 starting with Sunday , { P : Yes } since today is Thursday , input number is 5. t__p : Yes , I 've input it. -- T : An._..d , it is two thirty now , { P : Yes } using the 24 hour time ds3.4.4 system , { P : yes } input 1 , 4 , 3 , O. m a__p : Yes. I 've input it. B T : Finally , push the registration button again. -P : Yes. Figure 1 : An example of annotated dialogue cues 1. There are 31 cue phrases that occur more than five times. As the result of classifying these 31 cue phrases based on the classification of Japanese connectives ( Ichikawa , 1978 ; Moriyama , 1997 ) and cue phrase classification in Enghsh ( Grosz and Sidner , 1986 ; Cohen , 1984 ; Knott and Dale , 1994 ; Moser and Moore , 1995b ) , 20 cue phrases , which occurred total of 848 times , were classified into three classes : changeover , such as soredeha , deha ( `` now '' , `` now then '' in English ) , conjunctive , such as sore.de , de ( `` and '' , `` and then '' ) , and ordinal , such as mazu , tsugini ( `` first '' , `` next '' ) . Besides these simple cue phrases , there are composite cue phrases such as soredeha-tsugini ( `` now first '' ) . Note that meaning and the usage of each of these Japanese cue phrases does not completely correspond to those of the English words and phrases in parentheses. For example. the meaning of the Japanese cue phrase soredeha is close to the English word now in its discourse sense. However , soredeha does not have a sentential sense though now does. The purpose of this study is to decide which of these three classes of simple cue phrases should be selected as the cue phrase at the beginning of a disICue phrases which occur in the middle of the segment and in the segment other than action direction such as clarification segment are included. I01 course segment. We do not deal with composite types of cue phrases. As the basis for examining the relationship between cue phrase and dialogue structure , discourse segment boundary and the level of embedding of the segments were annotated in each dialogue. We define discourse segment ( or simply segment ) as chunks of utterances that have a coherent goal ( Grosz and Sidner , 1986 ; Nakatani et al. , 1995 ; Passonneau and Litman , 1997 ) . The annotation of hierarchical relations among segments was based on ( Nakatani et al. , 1995 ) . Figure 1 shows an example from the annotated dialogue corpus. This dialogue was translated from the original Japanese. This example provides instruction on setting the calendar and clock of the answering machine. The purpose of ds3.4 is to input numbers by dial buttons and each input action is directed in ds3.4.2 , ds3.4.3 , and ds3.4.4 , for inputting the date , the day of the week , and the time , respectively. Subdialogues such as confirmation and pupil initiative clarification are treated as one segment as in ds3.4.2.1. The organization cue phrases are underlined in the sample dialogue. For example , the cue phrase for ds3.3 is `` And '' , and that for ds3.5 is `` Finally '' 2 pre-exchange As the information about task structure and dialogue context , we annotated the discourse purpose of each segment and the dialogue exchange at the end of the immediately preceding segment. In annotating the discourse purpose , the coders selected the purpose of each segment from a topic list. The topic list consists of 127 topics. It has a hierarchical structure and represents the task structure of the domain of our corpus. When the discourse purpose can not be selected from the topic list , the segment was annotated as `` others '' . In such segments , the information about task structure can not be obtained. The pre-exchange is annotated as a kind of dialogue context and used as one of the learning features itself. The coders annotated the kind of preexchange by selecting one of nine categories of exchanges which are defined in section 4.1 in detail. As mentioned in the previous sections , we annotated our corpus with regard to the following characteristics : the class of cue phrases ( ordinal , changeover , conjunctive ) , segment boundary , and hierarchical structure of the segment , the purpose of the segment , and the dialogue exchange at the end of the immediately preceding segment. The extent of inter-coder agreement between two coders in these annotation are calculated by using : When a cue phrase follows acknowledgement ( Yes ) or a stammer , these speech fragments that do not have ~ftropositional content axe ignored and the cue phrases er the fragments axe annotated as the beginning of the segment. Cohen 's Kappa ~ ( Bakeman and Gottman , 1986 ; Carletta , 1996 ) . The inter-coder agreement ( to ) about the class of cue phrase is 0.68 , about the purpose of the segment is 0.79 , and about the type of pre-exchange is 0.67. The extent of agreement about the segment boundary and the hierarchical structure is calculated using modified Cohen 's Kappa presented by ( Flammia and Zue , 1995 ) . This Cohen 's Kappa is 0.66. Fleiss et al. ( 1981 ) characterizes kappas of .40 to .60 as fair , .60 to .75 as good , and over .75 as excellent. According to this categorization of levels of inter-coder agreement , the inter-coder agreement for cue phrase , pre-exchange , and discourse boundary and structure is good. The agreement on segment purpose is excellent. Thus , these results indicate that our corpus coding is adequately rehable and objective. When the two coders ' analyses did not agree , the third coder judged this point ; only those parts whose analysis is output by more than two coders was used as learning data. This section describes a learning experiment using C4.5 ( Quinlan , 1993 ) . First , we define 10 learning features concerned with three factors. ( 1 ) Discourse structure : Structural information about the preceding dialogue. Embedding The depth of embedding from the top level. Place The number of elder sister segments. Place2 The number of elder sister segments except pupil initiative segments. Recent elder sister 's cue ( Res-cue ) The cue phrase that occurs at the beginning of the most recent elder sister segment. They axe classified into three kinds of simple cue phrases : ord ( ordinal ) , ch ( changeover ) , con ( conjunctive ) or a kind of composite cue phrase such as ch+ord ( changeover + ordinal ) . Res-cue2 The cue phrase that occurs at the beginning of the most recent elder sister segment except pupil initiative segments. Discourse transition ( D-trans ) Types of change in attentional state accompanied by topic change 3 such as push and pop. Pop from the pupil initiative subdialogue is categorized as `` ui-pop '' . ( 2 ) Task structure : Information that estimates the complexity of succeeding dialogue. 3Clark ( 1997 ) presents a term `` discourse topic '' as concept equivalent to focus space in ( Grosz and Sidner , 1986 ) , and call their transition `` discourse transition '' . For example , `` push '' is defied as the transition to the sub topic , and `` next '' is defined as the transition to the same level proceeding topic. 102 factor Discourse structure Task structure Dialogue structure Table 1 : The learning features feature name Embedding Fla~ : e l~ , lace2 H.es-cue l~es-cue2 D-trans T-hmraxchy ~ubgoal Fre-exchange Fs-cue values integer mteger mteger nil , ord , Oh , con , ch+ord , con÷ord , con+ch , other nil , ord , C\ [ 1 , Cou , cn÷ord~ con+ord , con+ch , other pop , push , next , m-pop , ~A integer mteger conf , req , inf , quest , ui-conf , ui-req , tti-inf , ui-quest , NA nil , oral , ch , con , ch+ord , con+ord , con+ch , other Task-hierarchy ( T-hierarchy ) The number of goal-subgoal relations from the current goal to primitive actions. This estimates the depth of embedding in the succeeding dialogue. Subgoal The number of direct subgoals of the current goal. If zero , then it is a primitive action. ( 3 ) Dialogue context Information about the preceding segment. Pre-exchange Type of exchange that occurs at the end of the immediately preceding segment , or type of exchange immediately preceding the cue phrase. There are four categories , conf ( confirmation-answer ) , req ( request-answer ) , inf ( information-reply ) . ques ( question-answer ) . They are also distinguished by the initiator of the exchange ; explainer initiative or pupil initiative. When the category of the exchange is not clear , it is classified as not applicable ( NA ) . Therefore , there are nine values for this feature. Preceding segment 's cue ( Ps-cue ) The cue phrase that occurs at the beginning of the immediately preceding segment. The values of these features are shown in Table 1. Among the above learning features , Embedding , Place , Place $ , Res-cue , Res-cue~ , Ps-cue , and D-trans are derived automatically from the information about segment boundary and the segment hierarchy annotated in the corpus ( an example is shown in Figure 1 ) . The depth of task hierarchy ( Thierarchy ) and the number of direct subgoais ( Subgoal ) are determined by finding the annotated segment purpose in the given task structure. In this study , C4.5 ( Quinlan , 1993 ) is used as learning program. This program takes two inputs , ( 1 ) the definition of classes that should be learned , and the names and the values of a set of features , and ( 2 ) the data which is a set of instances whose class and feature values are specified. As a result of machine learning , the program outputs a decision tree fe~ judgement. We use cross-validation for estimating the accuracy of the model because this method avoids the disadvantages common with small data sets whose number of cases is less than 1000. In this study , 10-fold cross-validation is applied , so that in each run 90 % of the cases are used for training and the remaining 10 % are used for testing. The C4.5 program also has an option that causes the values of discrete attribute to be grouped. We selected this , option because there are many values in some features and the decision tree becomes very complex if each value has one branch. Decision trees for distinguishing the usage of three kinds of cue phrases ( changeover , ordinal , and conjunctive ) were computed by the machine learning al : gorithm C4.5. As learning features , the 10 features mentioned in section 4.1 are used. From nine dialogues ; 545 instances were derived as training data. In 545 instances , 300 were conjunctive , 168 were changeover , and 77 were ordinal. The most frequent category , conjunctive , accounts for 557o of all cases. Thus , the baseline error rate is 4570. This means that one would be wrong 45~0 of the time if this category was always chosen. First , the prediction power of each learning feature is examined. The results of learning experiments using single features are shown in Table 2. I.~ pruning the initial tree , C4.5 calculates actual and estimated error rates for the pruned tree. The error rate shown in this table is the mean of estimated error rates for the pruned trees under 10-fold crossvalidation. The 95 % confidence intervals are shown after `` '± '' . Those are calculated using Student 's t distribution. The error rate el is significantly better than e2 if the upper bound of the 95 % confidence interval for e~ is lower than the lower bound of the 95 % confidence interval for e2. As shown in Table 2 , the decision tree obtained with the Pre-exchange fen103 Table 2 : The error rates with each model Embedding 'Place Place'~ -Res-cue l-tes-cue2 D-trans T-hierarchy `` ' Subg0al 46.5 2:0.1 42.5 ± 0.4 43.8 : :k 0.4 44.9 ± 0.3 45.1 ± 0.4 45.0 ± 0.5 42.4 ± 0.3 42.5 ± 0.3 Fre-exchaage Fs-cue DS model Task model D ( _ , model All Ieature model Simplest model 41.5 ± 0.5 46,5 =k 0.3 35,6 ± 0.4 41,8 2:0.3 39.1 : :l : : 0.6 29.9 2:0.4 30.6 ± 0.3 Table 3 : The set of learning features for each model I Discourse Structure il rl'ask Structure Model EmbeddFlace I Place2 Resl-tes { D-trans I T5ubgoal in~ ; . : cue cue 2 hierarchy DS ~ 4 4 4 V , / Task V , / DC An feature ¢ V ~ q , / ¢ : , ¢_ 4 Simplest ' , h f ~/ ' ~/ Ii .. ~/ II Dialogue Context I PreFs-cue exch~nse I 4 4 4 4 4 4 ture performs best , and its error rate is 41.5 % . In all experiments , the error rates are more than 40 % and none are considerably better than the baseline. These results suggest that using only a single learning feature is not sufficient for selecting cue phrases correctly. As the single feature models are not sufficient , it is necessary to find the best set of learning features for selecting cue phrases. We call a set of features a model and the best model ( the best set of features ) is obtained using the following procedure. First , we set some multiple features models and carry out learning experiments using these models in order to find the best performing model and the best error rate. We then eliminate the features from the best performance model in order to make the model simpler. Thus , the best model we try to find is the one that uses the smallest number of learning features but whose performance equals the best error rate. We construct four multiple feature models. The name of the model and the combination of features in the model are shown in Table 3. The discourse structure model ( the DS model ) used learning features concerned with discourse structure. The Task model used those concerned with task structure , and the dialogue context ( the DC model ) used those concerned with dialogue context. The All .feature model uses all learning features. The best error rate among these models is 29.9 % in All .feature model as shown in Table 2. The error rate is reduced about 15 % from the baseline. Therefore , the best model is the one that uses fewer learning features than the All .feature model and that equals the performance of that model. In order to reduce the number of features considered , we examined which features have redundant information , and omitted these features from the All \ ] eature model. The overlapping features were found by examining the correlation between the features. As for numerical features that take number values , the correlation coefficient between Place and Place~ , and between T-hierarchy and Subgoal are high ( p=0.694 , agreement between Res-cue and Res-cue2 is 95 % . These highly correlated features can be represented by just one of them. As the result of many experiments varying the combination of features used , we determined the Simplest model which uses six features : Embedding , Place , D-trans , Subgoal , Preezchange , and Ps-cue as shown at the bottom line in Table 3. The error rate of the Simplest model is 30.6 % as shown in Table 2. It is very close to that of the All \ ] eature model though the difference is statistically significant. In addition to comparing only the overall error rates , in order to compare the performance of these two models in more detail , we calculated the information retrieval metrics for each category , changeover , ordinal , and conjunctive. Figure 2 shows the equations used to calculate the metrics. For example ,</definiens>
				<definiens id="1">the ratio of the cue phrases correctly predicted by the model as class X to the cue phrases of class X in the corpus. Precision rate is the ratio of cue phrases correctly predicted to be class X to all cue phrases predicted to be class X. In addition , in order to get an intuitive feel of overall performance , we also calculated the sum of the deviation from ideal values in each metric as in ( Passonneau and Litman , 1997 ) . The summed deviation is calculated by the following numerical formula : ( 1 Recall ) + ( 1 Precision ) + Fallout + Error Table 4 shows the results of these metrics for the two models. Standard deviation is shown in parentheses. The value of each metric is the average of 104 Table 4 : Performance on training set using cross-validation \ [ Model All feature model Simplest model Cue phrase t.tecaU Precimon FaUout \ [ Error \ [ Summed Deviation I ordinal 0.50 ( 0.HI ) 0.64 ( 0.10 ) 0.05 ( 0.03 ) , 0.11 ( 0.03 ) , 1.03 ( 0.23 ) , changeover 0.53 ( 0.1 , : ) 0.58 ( 0.07 ) 0.17 ( 0.05 ) \ [ 10 '' 26 ( 0.04 ) 1.32 ( 0.23 ) conjunctive 0.80 ( 0.01 ; ) 0.73 ( 0.05 ) 0.38 ( 0.11 ) 0.28 ( 0.04 ) 1.12 ( 0.16 ) ordinal 0.48 ( 0.17 ) 0.66 ( 0.17 ) 0.45 ( 0.03 ) 0.11 ( 0.02 ) 1.01 ( 0.26 ) I changeover I 0.50 ( 0.12 ) 0.62 ( 0.08 ) 0.14 ( 0.03 ) 0.25 ( 0.05 ) 1.27 ( 0.24 ) I I conjunctive \ [ 0.85 ( 0.04 ) 0.72 ( 0.04 ) 0.40 ( 0.08 ) 0.26 ( 0.04 ) 1.09 ~0.17 ) I Class-X C4.5 Program not-Class-X Corpus Class-X not-Class-X a b c d Recall = a Fallout = b ( a+c ) ( b+d ) Precision = a Error = ( b+c ) ( a+b ) ( a+b+c+d ) Figure 2 : Information retrieval metrics the metrics on the test set in each run of 10-fold cross-validation. Comparing the summed deviation. the performance of the Simplest model is better than that of the All feature model in all categories of cue phrases. The summed deviations of the Simplest model , 1.01 for ordinal , 1.27 for changeover , and 1.09 for conjunctive , are lower than those of the All feature model. Thus , as a result of evaluating the models in detail using the information retrieval metrics , it is concluded that the Simplest</definiens>
				<definiens id="2">the most elegant model because it uses fewer learning features than the All feature model. Just six features , Embedding , Place , D-trans , Subgoal , Preexchange , and Ps-cue , are enough for selecting organization cue phrases. Classifying the six features in the Simplest model , it is found that these features come from all factors , discourse structure , task structure , and dialogue context. Embedding , Place , D-trans are the features of discourse structure , Subgoal is about task structure , and Pre-exchange and Ps-cue are about dialogue context. This result indicates that all the factors are necessary to predict cue phrases. The important factors for cue phrase selection are task structure and dialogue context as well as discourse structure , the focus of many earlier studies. While we identified the six features from the three kinds of factors , by looking at the decision trees created in the learning experiment , we found which features were more important than others in selecting cue phr~es. The features appearing near the root node are more important. Figure 3 shows the top part of a decision tree obtained from the Simplest model. In all 10 decision trees resulting from the cross-validation experiment in the Simplest model , Place feature appears at the root node. In 7 of 10 trees , Embedding and Pre-exchange appeared just below the root node. In these trees , if the Place of the segment is the first at that level ( i.e. there is no elder sister. ) , then Embedding appears at the next node , otherwise if the segment is not the first one at that level , then Pre-exchange appears atthe next node. Thus , if there are some elder sister segments , information about dialogue context is used for selecting cue phrases. On the other hand , if there is no elder sister segment , information about discourse structure is used for the judgement. These results suggest that the information about discourse structure , especially place of segments and the depth of embedding , and the dialogue context , especially the kind of immediately preceding dialogue exchange , play important roles in cue phrase selection. This paper reported the results of using a machine learning algorithm for identifying learning features and obtaining decision trees for selecting cue phrases. It also reported the result of a quantitative evaluation of the decision trees learned. Learning features concerning three factors , discourse structure , task structure , and dialogue context , were examined. By carrying out many experiments in which the combinations of learning features were varied , we found the most simple and effective learning feature set. The accuracy of the best model that uses 6 learning features is about 70 % . The error rate is reduced about 25 % from the baseline. These results support the claims of previous studies that discourse structure influence cue selection. In addition , it is revealed that task structure and dialogue context are also indispensable factors. We focus on predicting the cue phrases that occur at the beginning of discourse segments for signaiing inter-segment `` sequence '' relation. Elhadad and McKeown ( 1990 ) , on the other hand , has presented a model for distinguishing connectives , which link two propositions , using some pragmatic constraints. In ( Moser and Moore , 1995a ; Moser and Moore , 1995b ) , the relationship between placement and selection of cue phrases was investigated using the core : contributor relations among units within a segment ( Moser and Moore , 1995a ) . Although we discussed only the `` sequence '' relation between the 10,5 ( Ernbedding~ ~Pre-exchan'ge ) &gt; 3~3 ¢onf , ui-inf , ~ req , inf , uiothers `` X~onf , ui-q changeover ... . ~ ... . ~ Oh , con ,</definiens>
			</definition>
</paper>

		<paper id="0502">
			<definition id="0">
				<sentence>Coordination is a long standing problem for linguistic theories , because of its particular aspects which do not fit well with the dominance-based character of the vast majority of paradigms .</sentence>
				<definiendum id="0">Coordination</definiendum>
				<definiens id="0">do not fit well with the dominance-based character of the vast majority of paradigms</definiens>
			</definition>
			<definition id="1">
				<sentence>The core of the grammar consists of primitive and non primitive dependency rules , representing predicateargument structures associated with lexical items .</sentence>
				<definiendum id="0">core of the grammar</definiendum>
			</definition>
			<definition id="2">
				<sentence>Metarules ( see section 3 ) , which produce non primitive rules from primitive rules ( by means of one or several applications ) , obey linguistic principles ( cf. GPSG approach ( Gazdar et al. 1985 ) ) .</sentence>
				<definiendum id="0">Metarules</definiendum>
			</definition>
			<definition id="3">
				<sentence>A dependency grammar is a six-tuple &lt; W , C , S , D , I , H &gt; , where W is a finite set of words ; C is a set of syntactic categories ; S is a non-empty set of root categories ( CD_ S ) ; D is the set of dependency relations ( e.g. SUB J , OBJ , XCOMP , P-OBJ , PRED2 ) ; I is a finite set of symbols ( among which the special symbol 0 ) , called u-indices ; H is a set of dependency rules of the form x : X ( &lt; rlYlUl'Cl &gt; ... &lt; ri.lYi.lUi.l'Ci_l &gt; # &lt; a'i+IYi+lUi+l'Ci+l &gt; ... &lt; rmYmum'Cm &gt; ) 1 ) xe W , is the head of the rule ; 2 ) Xe C , is its syntactic category ; 3 ) an element &lt; rj Yuq : j &gt; is a d-quadruple • J J ( which describes a dependent ) ; the sequence of d-quads , including the special symbol # ( representing the linear position of the head ) , is called the d-quad sequence .</sentence>
				<definiendum id="0">dependency grammar</definiendum>
				<definiendum id="1">W</definiendum>
				<definiendum id="2">C</definiendum>
				<definiendum id="3">S</definiendum>
				<definiendum id="4">D</definiendum>
				<definiendum id="5">H</definiendum>
				<definiens id="0">a finite set of words ;</definiens>
				<definiens id="1">a set of syntactic categories</definiens>
				<definiens id="2">a non-empty set of root categories ( CD_ S )</definiens>
				<definiens id="3">a set of dependency rules of the form</definiens>
				<definiens id="4">a d-quadruple • J J ( which describes a dependent</definiens>
			</definition>
			<definition id="4">
				<sentence>Given a grammar G , L ' ( G ) is the language of sequences of word objects : L ' ( G ) = { ¢xa Wx ( G ) * I &lt; TOP , Q ( O ) , 0 , 0 &gt; : =~* ¢x and Qe S ( G ) } where TOP is a dummy dependency relation for the root of the tree and ~ is the derivation relation ( defined below ) .</sentence>
				<definiendum id="0">TOP</definiendum>
				<definiens id="0">the language of sequences of word objects : L ' ( G ) = { ¢xa Wx ( G ) * I &lt; TOP</definiens>
				<definiens id="1">a dummy dependency relation for the root of the tree</definiens>
				<definiens id="2">the derivation relation ( defined below )</definiens>
			</definition>
			<definition id="5">
				<sentence>The derivation procedure performs a leftmost generation of a sentence of L ' ( G ) .</sentence>
				<definiendum id="0">derivation procedure</definiendum>
			</definition>
			<definition id="6">
				<sentence>The derivation relation consists of two expressions .</sentence>
				<definiendum id="0">derivation relation</definiendum>
				<definiens id="0">consists of two expressions</definiens>
			</definition>
			<definition id="7">
				<sentence>Let ¢xa ( Wx ( G ) ) * and ~ e ( Wx ( G ) u Cx ( G ) ) * .</sentence>
				<definiendum id="0">¢xa ( Wx</definiendum>
				<definiens id="0">( G ) ) * and ~ e ( Wx ( G ) u Cx ( G ) ) *</definiens>
			</definition>
			<definition id="8">
				<sentence>DRI : a &lt; r , X ( Tp ) , u , Tx &gt; W =~ ( X &lt; r l , Y1 ( p l ) , U l , X l &gt; &lt; r2 , Y2 ( P2 ) , u2 , x2 &gt; &lt; ri-l , Yi-l ( Pi-1 ) , u i-l , Xi-l &gt; uXO &lt; ri+l , Y i+l ( l~i+l ) , U i+l , Xi+l &gt; o.o &lt; rm , Ym ( Pm ) , Um , 'Cm &gt; where x : X ( &lt; rlY lUl x 1 &gt; ... &lt; ri-lYi-lUi-lXi-l &gt; # &lt; ri+lYi+lUi+lZi+l &gt; ... . &lt; rmYmumXm &gt; ) is a dependency rule , and Pl u ... u Pm -- Tp u TxI I I I I I I i I I I I I I I I I I i DR2 : a &lt; r , X ( &lt; v , r , X &gt; ) , u , ( ) &gt; ~=~ a nevV =~* is the reflexive , transitive closure of =~ .</sentence>
				<definiendum id="0">DRI</definiendum>
				<definiendum id="1">x : X</definiendum>
				<definiens id="0">a &lt; r</definiens>
				<definiens id="1">a dependency rule</definiens>
				<definiens id="2">a &lt; r</definiens>
			</definition>
			<definition id="9">
				<sentence>DR1 distributes over the dependents of a dependency rule the u-triples still to be satisfied ( yp ) plus the new u-indices introduced by the rule itself ( ~'x ) .</sentence>
				<definiendum id="0">DR1</definiendum>
				<definiens id="0">distributes over the dependents of a dependency rule the u-triples still to be satisfied ( yp ) plus the new u-indices introduced by the rule itself ( ~'x )</definiens>
			</definition>
			<definition id="10">
				<sentence>DR2 implements the utriple satisfaction : the u-triple &lt; v , r , X &gt; represents an expectation for a trace of category X attached to its head through a relation r. When DR2 applies , the trace ( word object l~Ev ) is in fact inserted in the derivation with v-index v. Notice that the trace has u ( the u-index of the derivation object ) as It-index : this means that the trace itself can be the reference for another trace .</sentence>
				<definiendum id="0">X &gt;</definiendum>
				<definiens id="0">an expectation for a trace of category X attached to its head through a relation r. When DR2 applies , the trace ( word object l~Ev</definiens>
				<definiens id="1">the u-index of the derivation object</definiens>
			</definition>
			<definition id="11">
				<sentence>A derivation for the sentence `` Beans I know John likes '' is the following ( at each step , the leftmost derivation object is underlined , and the derivation relation to be applied marks the symbol =~ ) : &lt; TOP. V+EXf.~ ) . 0. O &gt; = : ~DR 1 &lt; VISITOR. N ( O ) . ul. O~ : &lt; SUBJ , N ( ~ ) , 0 , @ &gt; know &lt; SCOMP , V ( ~ ) , 0 , { &lt; uI , OBJ , N &gt; } &gt; = : :~DRI ulbeans &lt; SUBJ. N ( O ) . 0. 0 &gt; know 14 &lt; SCOMP , V ( O ) , 0 , { &lt; ul , OBJ , N &gt; } &gt; : =~DRI ulbeans I know &lt; SCOMP. V ( Q~t. 0. | &lt; uI.OBJ.N &gt; I &gt; ~DRI utbeans I know &lt; StlBJ. N ( ~X_9~likes &lt; OBJ , N ( ~'uI , OBJ , N &gt; ) , 0 , ~ : =b DRI u\ ] beans I know John likes &lt; OBJ. N ( &lt; ul.OBJ.N &gt; k 0 .</sentence>
				<definiendum id="0">OBJ</definiendum>
				<definiens id="0">=b DRI u\ ] beans I know John likes &lt; OBJ. N ( &lt; ul.OBJ.N &gt; k 0</definiens>
			</definition>
			<definition id="12">
				<sentence>A PATrERN is an abstraction over a dependency rule , where the head can ( possibly ) reduce to the syntactic category ( from x : X to X ) , and some subsequences of d-quads can be ( possibly ) replaced by some variable symbol .</sentence>
				<definiendum id="0">PATrERN</definiendum>
				<definiens id="0">an abstraction over a dependency rule</definiens>
			</definition>
			<definition id="13">
				<sentence>The metarule psi matches this dependency rule through its SOURCE PATIERN , and produces the TARGET dependency rule , which licenses the dependency tree in fig .</sentence>
				<definiendum id="0">metarule psi</definiendum>
				<definiendum id="1">dependency rule</definiendum>
				<definiens id="0">licenses the dependency tree in fig</definiens>
			</definition>
			<definition id="14">
				<sentence>coord-unit X ( ( ~ &lt; COORD , CONJ-X , 0 , ~ ) For each dependency rule with a head of category X ( variable ) , coord-unit produces a dependency rule having exactly the same dquad sequence ( o ) , but with the added fightmost dependent CONJ-X , whose relation with the head is COORD ( fig. 4a ) . The dependency rule and : CONJ-V ( # &lt; 2nd , V , 0 , 0 &gt; ) licenses the second conjunct of a coordination of finite verbs .</sentence>
				<definiendum id="0">coord-unit X</definiendum>
				<definiendum id="1">coord-unit</definiendum>
				<definiendum id="2">COORD</definiendum>
				<definiens id="0">produces a dependency rule having exactly the same dquad sequence ( o ) , but with the added fightmost dependent CONJ-X , whose relation with the head</definiens>
			</definition>
			<definition id="15">
				<sentence>Literature usually distinguishes between non-traditionalarguments of predicate-argument structures .</sentence>
				<definiendum id="0">Literature</definiendum>
				<definiens id="0">distinguishes between non-traditionalarguments of predicate-argument structures</definiens>
			</definition>
			<definition id="16">
				<sentence>Consider the following case of a v eoord-gap V I 3 ~ VP-coordination : stmJ s Lucy saw a butterfly and laughed .</sentence>
				<definiendum id="0">stmJ</definiendum>
				<definiens id="0">s Lucy saw a butterfly and laughed</definiens>
			</definition>
			<definition id="17">
				<sentence>gapped dependents ( trace nodes ) • This implies ii U-triple specifications allow to produce a that right dependents attach to the farther head I i ' uniform treatment of many kinds of argument of the two , always respecting the condition the ~ gaps , among which the following ( single and projectivity .</sentence>
				<definiendum id="0">trace nodes</definiendum>
				<definiens id="0">a that right dependents attach to the farther head I i ' uniform treatment of many kinds of argument of the two , always respecting the condition the ~ gaps , among which the following</definiens>
			</definition>
			<definition id="18">
				<sentence>( 1 ) To associate a u-index with the head position in the dependency rules , we need to modify its definition : A dependency grammar is a six-tuple &lt; W , C , S , D , I , H &gt; , where W , C , S , D , I are as defined in section 2 , and H is a set of dependency rules , of the form x : X ( &lt; 1 '' lYlUl'\ [ 1 &gt; ... &lt; ri-lYi-lUi-l'\ [ i-l &gt; &lt; # , ui &gt; &lt; ri+lYi+lUi+l'ti+l &gt; ... &lt; rmYmum'tm &gt; ) where all symbols are the same as in section 2 , except for the head position ( # ) , which has been associated an index ui ( possibly ¢ ) , such S The derivation process defined in section 2 loosely constrains the satisfaction of u-triples to occur in a specific subtree .</sentence>
				<definiendum id="0">dependency grammar</definiendum>
				<definiendum id="1">H</definiendum>
				<definiendum id="2">X</definiendum>
				<definiens id="0">a set of dependency rules , of the form x</definiens>
				<definiens id="1">associated an index ui ( possibly ¢ ) , such S The derivation process defined in section 2 loosely constrains the satisfaction of u-triples to occur in a specific subtree</definiens>
			</definition>
			<definition id="19">
				<sentence>18 that uiE I. ( 2 ) • To introduce and keep apart the u-indices for a subtree and the ones for a single node in the derivation process , we must modify the word objects in 4-tuples consisting of a word w ( E W ) or the trace symbol 8 ( ~W ) and three annotated indices r I , St and v. Given a grammar G , the set of word objects of Gis Wx ( G ) = { q , lax v / r i , St , v E I + , xE W u { 8 } } .</sentence>
				<definiendum id="0">Gis Wx</definiendum>
				<definiens id="0">modify the word objects in 4-tuples consisting of a word w</definiens>
			</definition>
</paper>

		<paper id="1125">
			<definition id="0">
				<sentence>A parse of discourse is defined as a set of semantic dependencies among sentences that make up the discourse .</sentence>
				<definiendum id="0">parse of discourse</definiendum>
				<definiens id="0">a set of semantic dependencies among sentences that make up the discourse</definiens>
			</definition>
			<definition id="1">
				<sentence>In statistical parsing , this could be formulated as a problem of finding a best parse with a model P ( T I D ) , where T is a set of dependencies and D a discourse .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">a set of dependencies</definiens>
			</definition>
			<definition id="2">
				<sentence>Surprisingly , the latter tree consists of only one test node .</sentence>
				<definiendum id="0">latter tree</definiendum>
			</definition>
			<definition id="3">
				<sentence>( Cj , T ) x log 2 freq ( Cj ' T ) ITI IVl j=l freq ( C , T ) is the number of cases from a class C divided by the sum of cases in T. Now infox ( T ) is the average amount of information generated by partitioning T with respect to a test X. That is , in/ox ( T ) = ~ 1~ \ [ info ( T , ) i=1 Thus a good classifier would give a small value for info X ( T ) and a large value for in/o X ( T ) .</sentence>
				<definiendum id="0">freq ( C , T )</definiendum>
				<definiens id="0">the number of cases from a class C divided by the sum of cases in T. Now infox</definiens>
			</definition>
			<definition id="4">
				<sentence>This can be done by assuming : P ( A ~ B \ [ CF ( D ) ) = P ( A ~ B \ [ CF ( D ) , DTF ) Z P ( X ~B \ [ CF ( D ) , DTF ) X &lt; B ( 1 ) DTF is a decision tree constructed with a feature set F by C4.5. 'X &lt; B ' means that X is a sentence that precedes B : P ( X ~ Y \ [ CF ( D ) , DTv ) is the probability that sentence Y depends on sen ' tence X under the condition that both CF ( D ) and DTF are used. We estimate P , using class distributions from the decision tree DTF. For example , we have numbers in parentheses after leaves in the decision tree in Figure 3. They indicate the number of cases that reach a particular leaf and also the number of misclassified cases. Thus a leaf with the label inexpensive has the total of 4 cases , one of which is misclassified. This means that we have 3 cases correctly classified as `` NO '' and one case wrongly classified. Thus a class distribution for `` NO '' is 3/4 and that for `` YES '' is 1/4. In practice , however , we slightly correct class frequencies , using Laplace 's rule of succession , i.e. , x/n ~ x + 1/n + 2. Now suppose that we have a discourse D = { ... , Si , ... , Sj , ... , Sk , ... } and want to know what Si depends on , assuming that Si depends on either Sj or Sk. To find that out involves constructing the structure of a discourse , namely that a sentence modifies one that precedes it. Changing it to something like 'X 6 D , X # B ' allows one to have forward as well as backward dependencies. 218 Figure 4 : A hypothetical decision tree. dist YES ( 10/3 ) YES ( 14/8 ) CF ( D ) and DTF. Let us represent sentences Sj and Sk in terms of how far they are separated from Si , measured in sentences. Suppose that dist ( Sj ) = 2 and dist ( Sj ) = 4 ; that is , sentence S. # appears 2 sentences behind Si and Sk 4 sentences behind. Assume further that we have a decision tree constructed from data elsewhere that looks like Figure 4. With CF ( D ) and DTF at hand , we are now in a position to find P ( A ~ B I CF ( D ) ) , for each possible dependency Sj ~ Si , and Sk +-Si. P ( Sj ~ Si l Cai , t ( D ) , DTai , t ) = ( 103 + 1 ) / ( 10 + 2 ) . = .67 P ( Sk +Si \ [ Caist ( D ) , DTaist ) = ( 14 s + 1 ) / ( 14 + 2 ) = .44 Since Si links with either Sj or Sk , by Equation 1 , we normalize the probability estimates so that they sum to 1. P ( S i ~ Si I Cd , t ( D ) ) = .67/ ( .67 + .44 ) = .60 P ( .-~ ~ Si \ [ Caist ( D ) ) = .44/ ( .67 + .44 ) = .40 Recall that class frequencies are corrected by Laplace 's rule. Let T 1 = { Sj ~ Si } and Tk = { , -~ Si } Then P ( Tj I D ) &gt; P ( Tk t D ) .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">DTF</definiendum>
				<definiendum id="2">DTv )</definiendum>
				<definiens id="0">a decision tree constructed with a feature set F by C4.5. 'X &lt; B ' means that X is a sentence that precedes B</definiens>
				<definiens id="1">the probability that sentence Y depends on sen ' tence X under the condition that both CF ( D ) and DTF are used. We estimate P</definiens>
				<definiens id="2">a discourse D = { ... , Si , ... , Sj , ... , Sk , ... } and want to know what Si depends on , assuming that Si depends on either Sj or Sk. To find that out involves constructing the structure of a discourse</definiens>
				<definiens id="3">sentences Sj and Sk in terms of how far they are separated from Si , measured in sentences. Suppose that dist ( Sj ) = 2 and dist ( Sj ) = 4 ; that is , sentence S. # appears 2 sentences behind Si and Sk 4 sentences behind. Assume</definiens>
			</definition>
			<definition id="5">
				<sentence>Distance 'Par ( X ) ' is a paragraph that contains a sentence X , and ' # Par ( X ) ?</sentence>
				<definiendum id="0">Distance 'Par</definiendum>
				<definiens id="0">a paragraph that contains a sentence X</definiens>
			</definition>
			<definition id="6">
				<sentence>LocSen takes values between 0 and 1 .</sentence>
				<definiendum id="0">LocSen</definiendum>
				<definiens id="0">takes values between 0 and 1</definiens>
			</definition>
			<definition id="7">
				<sentence>LocWithinPar takes continuous values ranging from 0 to 1 .</sentence>
				<definiendum id="0">LocWithinPar</definiendum>
				<definiens id="0">takes continuous values ranging from 0 to 1</definiens>
			</definition>
			<definition id="8">
				<sentence>We define Siva2 as 'SIM ( A , Concat ( Par ( B ) ) ) ' ( see footnote 4 for the definition of SIM ) , where 'Concat ( Par ( B ) ) ' is a concatenation of sentences in Par ( B ) .</sentence>
				<definiendum id="0">'SIM ( A , Concat ( Par</definiendum>
				<definiendum id="1">'Concat</definiendum>
			</definition>
			<definition id="9">
				<sentence>etc. , which are thought of as an indicator of a discourse relationship .</sentence>
				<definiendum id="0">etc.</definiendum>
			</definition>
			<definition id="10">
				<sentence>N is the number of clues used .</sentence>
				<definiendum id="0">N</definiendum>
			</definition>
			<definition id="11">
				<sentence>The tree pruning is a technique by which to prevent a decision tree from fitting training data too closely .</sentence>
				<definiendum id="0">tree pruning</definiendum>
				<definiens id="0">a technique by which to prevent a decision tree from fitting training data too closely</definiens>
			</definition>
</paper>

		<paper id="0803">
</paper>

		<paper id="1119">
			<definition id="0">
				<sentence>Given the above possible sources of informar tion , we arrive at the following equation , where F ( p ) denotes a function from pronouns to their antecedents : F ( p ) = argmaxP ( A ( p ) = alp , h , l~ ' , t , l , so , d~ A~ ' ) where A ( p ) is a random variable denoting the referent of the pronoun p and a is a proposed antecedent .</sentence>
				<definiendum id="0">F ( p )</definiendum>
				<definiendum id="1">p )</definiendum>
				<definiens id="0">a function from pronouns to their antecedents : F ( p ) = argmaxP ( A ( p ) = alp , h , l~ ' , t , l , so , d~ A~ '</definiens>
				<definiens id="1">a random variable denoting the referent of the pronoun p</definiens>
			</definition>
			<definition id="1">
				<sentence>In the conditioning events , h is the head constituent above p , l~ r is the list of candidate antecedents to be considered , t is the type of phrase of the proposed antecedent ( always a noun-phrase in this study ) , I is the type of the head constituent , sp describes the syntactic structure in which p appears , dspecifies the distance of each antecedent from p and M '' is the number of times the referent is mentioned .</sentence>
				<definiendum id="0">h</definiendum>
				<definiens id="0">the type of phrase of the proposed antecedent ( always a noun-phrase in this study</definiens>
			</definition>
			<definition id="2">
				<sentence>P ( A ( p ) = alp , h , fir , t , l , sp , d~ .</sentence>
				<definiendum id="0">P (</definiendum>
				<definiens id="0">A ( p ) = alp , h , fir , t , l , sp , d~</definiens>
			</definition>
			<definition id="3">
				<sentence>Q ' ) = P ( alA~ ) P ( p , h , fir , t , l , sp , ~a , 2~ ) ( 1 ) P ( p , h , fir , t , t , sp , diM ) o¢ PCalM ) P ( p , h , fir , t , l , sp , ~a , .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">( p , h , fir , t , t , sp , diM ) o¢ PCalM ) P ( p , h , fir , t , l , sp</definiens>
			</definition>
			<definition id="4">
				<sentence>After we have identified the correct antecedents it is a simple counting procedure to compute P ( p\ [ wa ) where wa is in the correct antecedent for the pronoun p ( Note the pronouns are grouped by their gender ) : \ [ wain the antecedent for p \ [ P ( pl o ) = When there are multiple relevant words in the antecedent we apply the likelihood test designed by Dunning ( 1993 ) on all the words in the candidate NP .</sentence>
				<definiendum id="0">wa</definiendum>
			</definition>
			<definition id="5">
				<sentence>The P ( plw , d probability gives the system information about gender and animaticity .</sentence>
				<definiendum id="0">probability</definiendum>
				<definiens id="0">gives the system information about gender and animaticity</definiens>
			</definition>
			<definition id="6">
				<sentence>A statistical approach is present in the discourse module only where it is used to determine the probability that a noun ( verb ) phrase is the center of a sentence .</sentence>
				<definiendum id="0">phrase</definiendum>
				<definiens id="0">the center of a sentence</definiens>
			</definition>
</paper>

		<paper id="0714">
			<definition id="0">
				<sentence>For any agent A and domain element O , ifA knows about or can think about O , then there exists a mental representation C in A 's mind , which represents O. We write \ [ \ [ C\ ] \ ] A = O , mind of an agent and serves to represent some domain element for that agent .</sentence>
				<definiendum id="0">mind</definiendum>
				<definiens id="0">mind of an agent and serves to represent some domain element for that agent</definiens>
			</definition>
			<definition id="1">
				<sentence>A hierarchical ontological relation is any ontological relation that organizes concepts into a hierarchy , taxonomy , or similar structure .</sentence>
				<definiendum id="0">hierarchical ontological relation</definiendum>
				<definiens id="0">any ontological relation that organizes concepts into a hierarchy , taxonomy , or similar structure</definiens>
			</definition>
			<definition id="2">
				<sentence>We define the following operations : • Ontology ( A ) : return the set of ore-concepts that OM currently uses to represent concepts in A 's ontology .</sentence>
				<definiendum id="0">A )</definiendum>
				<definiens id="0">return the set of ore-concepts that OM currently uses to represent concepts in A 's ontology</definiens>
			</definition>
			<definition id="3">
				<sentence>Evaluate takes a relation R. and an ore-concept C. and returns a set of om-concepts such that Agent ( C ) believes R ( ~C~Age , ~ ( c ) , ~C~A~ , ~ ( c~ ) for each om-concept C ' in the set .</sentence>
				<definiendum id="0">Evaluate</definiendum>
				<definiens id="0">takes a relation R. and an ore-concept C. and returns a set of om-concepts such that Agent ( C ) believes R ( ~C~Age , ~ ( c ) , ~C~A~ , ~ ( c~ ) for each om-concept C ' in the set</definiens>
			</definition>
			<definition id="4">
				<sentence>1995 ) lexical ontology organizes concepts called `` synsets , '' which are sets of words considered synonymous in a certain context .</sentence>
				<definiendum id="0">lexical ontology</definiendum>
				<definiens id="0">organizes concepts called `` synsets , '' which are sets of words considered synonymous in a certain context</definiens>
			</definition>
			<definition id="5">
				<sentence>The Ontological Mediator asks appropriate questions of a speaker and listener to find words in the listener 's ontology it believes mean the same as words in the speaker 's .</sentence>
				<definiendum id="0">Ontological Mediator</definiendum>
				<definiens id="0">asks appropriate questions of a speaker and listener to find words in the listener 's ontology it believes mean the same as words in the speaker 's</definiens>
			</definition>
			<definition id="6">
				<sentence>The ontological mediator running t , ledCount : must explore a sizable portion of each agent 's ontology to arrive at its conclusion .</sentence>
				<definiendum id="0">ontological mediator</definiendum>
				<definiendum id="1">ledCount</definiendum>
				<definiens id="0">explore a sizable portion of each agent 's ontology to arrive at its conclusion</definiens>
			</definition>
			<definition id="7">
				<sentence>Introduction to WordNet : An On-line Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="8">
				<sentence>WordNet : A Lexical Database for English .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1432">
			<definition id="0">
				<sentence>The pseudo-semantic component , which defines the semantic input of the generation process and the syntactic component , which produces a sentence ( in written or spoken format ) from the pseudo-semantic specifications .</sentence>
				<definiendum id="0">pseudo-semantic component</definiendum>
				<definiens id="0">defines the semantic input of the generation process and the syntactic component , which produces a sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>A pseudo-semantic structure ( PSS ) contains both lexical and abstract information ( whence the term pseudo ) .</sentence>
				<definiendum id="0">pseudo-semantic structure</definiendum>
				<definiendum id="1">PSS</definiendum>
			</definition>
			<definition id="2">
				<sentence>SLSs ( Semantic Label Structures ) consist of a semantic label/function and an associated PSS .</sentence>
				<definiendum id="0">SLSs</definiendum>
				<definiens id="0">Semantic Label Structures ) consist of a semantic label/function and an associated PSS</definiens>
			</definition>
			<definition id="3">
				<sentence>Tense is represented through a modified version of Reichenbach 's analysis ( \ [ Reichenbach 47\ ] ) , where E is the event time point and S the speech time point , the two points being either equal or ordered with a precedence relation .</sentence>
				<definiendum id="0">Tense</definiendum>
				<definiendum id="1">E</definiendum>
				<definiens id="0">the event time point and S the speech time point , the two points being either equal or ordered with a precedence relation</definiens>
			</definition>
			<definition id="4">
				<sentence>The DPS has a lexical Property dog and an Operator some_ individual ( the interpretation of DPSs follows the generalized quantifiers analysis , see \ [ Barwise &amp; Cooper 81\ ] ) .</sentence>
				<definiendum id="0">DPS</definiendum>
				<definiens id="0">a lexical Property dog and an Operator some_ individual ( the interpretation of DPSs follows the generalized quantifiers analysis</definiens>
			</definition>
			<definition id="5">
				<sentence>The label states that the semantic function of the CHS is an evaluation of the truth of the statement expressed in the CLS .</sentence>
				<definiendum id="0">CHS</definiendum>
				<definiens id="0">an evaluation of the truth of the statement expressed in the CLS</definiens>
			</definition>
			<definition id="6">
				<sentence>Spec ( ifier ) X Compl ( ement ) ( Head ) The Head/Projection distinction should be seen as a convenient presentational device .</sentence>
				<definiendum id="0">Spec</definiendum>
				<definiens id="0">a convenient presentational device</definiens>
			</definition>
			<definition id="7">
				<sentence>Actually , a Projection is a record of the properties of the lexical item .</sentence>
				<definiendum id="0">Projection</definiendum>
			</definition>
			<definition id="8">
				<sentence>Nous phrases are formed with an NP , which contains the noun , its complements and adjectives , and a DP , the projection Of determiners , which subcategorizes for NPs .</sentence>
				<definiendum id="0">Nous phrases</definiendum>
				<definiendum id="1">projection Of determiners</definiendum>
				<definiens id="0">subcategorizes for NPs</definiens>
			</definition>
</paper>

		<paper id="1241">
			<definition id="0">
				<sentence>In the first , segmentation is a side effect of the fuzzification of input units during classification ( the segments chosen are those which give the best classification according to some metric ) .</sentence>
				<definiendum id="0">segmentation</definiendum>
			</definition>
			<definition id="1">
				<sentence>The Powers ( 1992 ) approach works by finding the groups of segments which have the largest cosets , and thus have high frequency and low information , their information content tending to be more syntactic than semantic .</sentence>
				<definiendum id="0">Powers</definiendum>
				<definiens id="0">low information , their information content tending to be more syntactic than semantic</definiens>
			</definition>
</paper>

		<paper id="0802">
			<definition id="0">
				<sentence>TransTool is a transcription editor meant to facilitate and partially automate the task of a human transcriber , while SyncTool is a tool for aligning the resulting transcriptions with a digitized audio and video recording in order to allow synchronized presentation of different representations ( e.g. , text , audio , video , acoustic analysis ) .</sentence>
				<definiendum id="0">TransTool</definiendum>
				<definiendum id="1">SyncTool</definiendum>
				<definiens id="0">a tool for aligning the resulting transcriptions with a digitized audio and video recording in order to allow synchronized presentation of different representations ( e.g. , text , audio , video , acoustic analysis )</definiens>
			</definition>
			<definition id="1">
				<sentence>• A transcription consists of a header , containing background information such as type of activity , participants , date of recording , duration , transcriber , etc. , and a body , containing the transcription proper .</sentence>
				<definiendum id="0">transcription</definiendum>
				<definiens id="0">consists of a header , containing background information such as type of activity , participants , date of recording , duration , transcriber , etc.</definiens>
			</definition>
			<definition id="2">
				<sentence>• The transcription body consists of speech lhws , containing the transcribed speech of dialog participants ( each line introduced by a speaker initial ) ; comment lhws , containing comments pertaining to phenomena in speech lines ( see below ) ; section lhws , indicating boundaries between subactivities or topics ; and time lines , containing information about the amount of time elapsed from the start of the activity .</sentence>
				<definiendum id="0">transcription body</definiendum>
				<definiens id="0">consists of speech lhws , containing the transcribed speech of dialog participants ( each line introduced by a speaker initial ) ; comment lhws , containing comments pertaining to phenomena in speech lines ( see below ) ; section lhws , indicating boundaries between subactivities or topics</definiens>
			</definition>
			<definition id="3">
				<sentence>\ ] 13 TransTool is a computer tool for transcribing spoken language in accordance with the standard developed within the research program Semantics and Spoken Language at G/Steborg University , Department of Linguistics , and described in Nivre ( 1998 ) ( cf. section 2 ) .</sentence>
				<definiendum id="0">TransTool</definiendum>
				<definiens id="0">a computer tool for transcribing spoken language in accordance with the standard developed within the research program Semantics</definiens>
				<definiens id="1">cf. section 2 )</definiens>
			</definition>
			<definition id="4">
				<sentence>This is done through the command MSO indices ( where MSO stands for Modified Standard Orthography ) , which automatically identifies all word forms that need to be indexed and prompts the user for disambiguation .</sentence>
				<definiendum id="0">MSO</definiendum>
				<definiens id="0">automatically identifies all word forms that need to be indexed and prompts the user for disambiguation</definiens>
			</definition>
			<definition id="5">
				<sentence>The Tools menu 16 SyncTool is an application developed for synchronizing transcriptions with digitized audio/video recordings .</sentence>
				<definiendum id="0">SyncTool</definiendum>
				<definiens id="0">an application developed for synchronizing transcriptions with digitized audio/video recordings</definiens>
			</definition>
			<definition id="6">
				<sentence>SyncTool is meant to be a synchronizing and viewing tool , allowing the researcher to set time codes in appropriate places in the transcriptions , and to view the transcription and play the recording without having to manually locate the specific passage in the recording .</sentence>
				<definiendum id="0">SyncTool</definiendum>
				<definiens id="0">meant to be a synchronizing and viewing tool , allowing the researcher to set time codes in appropriate places in the transcriptions , and to view the transcription and play the recording without having to manually locate the specific passage in the recording</definiens>
			</definition>
			<definition id="7">
				<sentence>• The Media Window ( currently an external tool ) displays the audio/video recording , allowing the user to swiftly move back and forth in the recording ( top right window in Figure 9 ) .</sentence>
				<definiendum id="0">Media Window</definiendum>
				<definiens id="0">an external tool</definiens>
			</definition>
</paper>

		<paper id="1403">
			<definition id="0">
				<sentence>System : Read the required textbook for it .</sentence>
				<definiendum id="0">System</definiendum>
			</definition>
			<definition id="1">
				<sentence>Realtors Inc. serves the city with the largest population in Somerset County .</sentence>
				<definiendum id="0">Realtors Inc.</definiendum>
				<definiens id="0">serves the city with the largest population in Somerset County</definiens>
			</definition>
			<definition id="2">
				<sentence>•Realtors Inc. serves the city with the worst pollution in Somerset .</sentence>
				<definiendum id="0">•Realtors Inc.</definiendum>
				<definiens id="0">serves the city with the worst pollution in Somerset</definiens>
			</definition>
			<definition id="3">
				<sentence>6 Discourse entities are specified in the list either by an internal identifier ( an identifier referring to a database object ) or by descriptions stated as RQFOL expressions .</sentence>
				<definiendum id="0">internal identifier</definiendum>
			</definition>
</paper>

		<paper id="0315">
			<definition id="0">
				<sentence>A lexicalized TAG contains two kinds of elementary trees : initial ( non-recursive ) trees that reflect basic functor-argument dependencies and auxiliary trees that introduce recursion and allow elementary trees to be modified and/or elaborated .</sentence>
				<definiendum id="0">lexicalized TAG</definiendum>
				<definiens id="0">contains two kinds of elementary trees : initial ( non-recursive ) trees that reflect basic functor-argument dependencies and auxiliary trees that introduce recursion and allow elementary trees to be modified and/or elaborated</definiens>
			</definition>
			<definition id="1">
				<sentence>Also optional is the realization of the initial anchor in disjunction ( omitting `` either '' ) , addition ( omitting `` not only '' ) , and concession ( omitting `` admittedly '' ) .</sentence>
				<definiendum id="0">optional</definiendum>
				<definiens id="0">the realization of the initial anchor in disjunction ( omitting `` either ''</definiens>
			</definition>
			<definition id="2">
				<sentence>Three types of elements participate in the analysis : ( 1 ) the syntactic analyses ( trees ) of the two clauses ( `` John went to the zoo , `` he took his cell phone with him '' ) labelled a and fl in in Figure 4. , along with their respective meanings ( call them Pl ( j ) and P2 ( J ) ) ; ( 2 ) the auxiliary tree for the discourse cue `` however '' , labelled 7 , along with its feature structure ; and ( 3 ) the description-extending auxiliary tree labelled 6 .</sentence>
				<definiendum id="0">elements</definiendum>
				<definiendum id="1">respective meanings</definiendum>
				<definiens id="0">participate in the analysis : ( 1 ) the syntactic analyses ( trees ) of the two clauses ( `` John went to the zoo</definiens>
			</definition>
			<definition id="3">
				<sentence>While previous authors have adopted only certain aspects of TAG or LTAG , here we have explored the possibility of a `` fully '' lexicalized TAG for discourse , which allows to examine how the basic insights of a lexicalized grammar carry over to discourse .</sentence>
				<definiendum id="0">discourse</definiendum>
				<definiens id="0">allows to examine how the basic insights of a lexicalized grammar carry over to discourse</definiens>
			</definition>
</paper>

		<paper id="0505">
			<definition id="0">
				<sentence>1988 ) ) or Word Grammar ( Hudson , 1984 ) is the notion of lexicalization : the descriptive burden is in the lexicon , which carries information that acts as constraint on syntactic structure .</sentence>
				<definiendum id="0">Word Grammar</definiendum>
				<definiens id="0">carries information that acts as constraint on syntactic structure</definiens>
			</definition>
			<definition id="1">
				<sentence>Coordinate structures are in fact a notorious prol ) lem for both dependency and constituency approaches , and there are numerous proposals of how to treat them .</sentence>
				<definiendum id="0">Coordinate structures</definiendum>
			</definition>
			<definition id="2">
				<sentence>While the problem for the majority of constituency-based approaches is how to accommodate the conjunction in the phrase structure representation and how to deal with phrasally incomplete conjuncts , in a dependency grammar the IA conjunct is a component part of a coordinate structure ( Hudson , 1984 ) .</sentence>
				<definiendum id="0">conjunct</definiendum>
				<definiens id="0">how to accommodate the conjunction in the phrase structure representation and how to deal with phrasally incomplete conjuncts</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , a traditional phrase structure for Fred ate the beans would reflect tile following bracketing : ( Fred ) ( ate the beans ) , which coincides only with one possible information structuring , where Fred is the Given element , but not with an information structure where Fred ate is the Given element ( of .</sentence>
				<definiendum id="0">Fred</definiendum>
				<definiendum id="1">Fred ate</definiendum>
				<definiens id="0">a traditional phrase structure for Fred ate the beans would reflect tile following bracketing : ( Fred ) ( ate the beans ) , which coincides only with one possible information structuring , where</definiens>
			</definition>
			<definition id="4">
				<sentence>These have to communicate , however , and the representation is 2Another term that has been used for Given is Topic -however , the notion of Topic is often a conflation of Given and Theme , which in Halliday 's view is distinct from Given : While Given represents that part of an utterance that is presented as known , Theme is that part which is taken to be the point of departure of a message , whether that is given or new information .</sentence>
				<definiendum id="0">Theme</definiendum>
				<definiens id="0">that part which is taken to be the point of departure of a message , whether that is given or new information</definiens>
			</definition>
			<definition id="5">
				<sentence>In terms of agreement relations , tile German nominal group ( NG ) can be briefly characterized as follows : German nouns carr .</sentence>
				<definiendum id="0">NG</definiendum>
				<definiens id="0">tile German nominal group (</definiens>
			</definition>
			<definition id="6">
				<sentence>System networks are descriptions of paradignlatic ga'ammatical relations intended as declarative statements of grammatical features and the coocurrence constraints between them .</sentence>
				<definiendum id="0">System networks</definiendum>
				<definiens id="0">descriptions of paradignlatic ga'ammatical relations intended as declarative statements of grammatical features and the coocurrence constraints between them</definiens>
			</definition>
			<definition id="7">
				<sentence>Agreement is a syntagmatic phenomenon that is ptvsodic in nature , in the sense that a particular realizational effect spreads over more than one constituent , similar to prosodic features that are strung throughout an intonational unit ( see Figure 5 displaying Subject-Finite agreelnellt ) .</sentence>
				<definiendum id="0">Agreement</definiendum>
				<definiens id="0">a syntagmatic phenomenon that is ptvsodic in nature , in the sense that a particular realizational effect spreads over more than one constituent , similar to prosodic features that are strung throughout an intonational unit</definiens>
			</definition>
			<definition id="8">
				<sentence>the intonation focus , which falls into the New part of the utterance , marks the informational pronfinence by carrying the major pitch change .</sentence>
				<definiendum id="0">intonation focus</definiendum>
				<definiens id="0">falls into the New part of the utterance , marks the informational pronfinence by carrying the major pitch change</definiens>
			</definition>
			<definition id="9">
				<sentence>The kernel of an SI : G is the grammatical classification hierarchy , representing the paradigmatic relations that characterize the gramnmr of a language .</sentence>
				<definiendum id="0">kernel of an SI : G</definiendum>
				<definiens id="0">the grammatical classification hierarchy , representing the paradigmatic relations that characterize the gramnmr of a language</definiens>
			</definition>
</paper>

		<paper id="1312">
			<definition id="0">
				<sentence>The term MORPHOLOGY , as used by linguists in the Two-Level and Finite-State traditions , encompasses both MORPHOTACTICS ( also called MORPHOSYNTAX ) , and the phonological or orthographical VARIATION rules that map between LEXICAL strings ( i.e. abstract or underlying strings ) and SURFACE strings .</sentence>
				<definiendum id="0">MORPHOLOGY</definiendum>
				<definiens id="0">used by linguists in the Two-Level and Finite-State traditions , encompasses both MORPHOTACTICS ( also called MORPHOSYNTAX ) , and the phonological or orthographical VARIATION rules that map between LEXICAL strings ( i.e. abstract or underlying strings</definiens>
			</definition>
</paper>

		<paper id="0609">
			<definition id="0">
				<sentence>In general , for noun-noun compounds , Left Element , In our example , the Linker s joins Anwendung and Programm , whilst the null Linker joins Anwendungsprogramm and Schnittstelle .</sentence>
				<definiendum id="0">Linker s</definiendum>
				<definiens id="0">joins Anwendung and Programm , whilst the null Linker joins Anwendungsprogramm and Schnittstelle</definiens>
			</definition>
			<definition id="1">
				<sentence>Duden 1995 reports that the hyphen is prescribed if the Left Element is an abbreviation and generally present if the Left Element is a proper name , and otherwise , it is generally employed to improve readability or to emphasize the individual components of the compound .</sentence>
				<definiendum id="0">Left Element</definiendum>
				<definiens id="0">an abbreviation and generally present if the Left Element is a proper name</definiens>
			</definition>
			<definition id="2">
				<sentence>In general , the choice of a Linker ( as well as umlauting and desuffixing ) is determined by the Left Element : Part-of-speech combinations of the Left Element and Right Element include noun-noun , nounverb , verb-noun , adjective-noun , noun-adjective , etc .</sentence>
				<definiendum id="0">Linker</definiendum>
			</definition>
</paper>

	</volume>
