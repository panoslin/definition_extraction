<?xml version="1.0" encoding="UTF-8"?>
	<volume id="A92">

		<paper id="1022">
			<definition id="0">
				<sentence>The regularized parse is then given to the semantic analyzer , which produces a predicate-argument structure .</sentence>
				<definiendum id="0">semantic analyzer</definiendum>
				<definiens id="0">produces a predicate-argument structure</definiens>
			</definition>
			<definition id="1">
				<sentence>Roughly speaking , the scoring program computes three counts : Std , the number of data base fills in the standard ( correct ) data base ; Sys , the number of fills generated by the system ; and Cor , the number of correct fills generated by the system .</sentence>
				<definiendum id="0">Sys</definiendum>
				<definiendum id="1">Cor</definiendum>
				<definiens id="0">the number of data base fills in the standard ( correct ) data base ;</definiens>
				<definiens id="1">the number of fills generated by the system</definiens>
				<definiens id="2">the number of correct fills generated by the system</definiens>
			</definition>
			<definition id="2">
				<sentence>Our syntactic analyzer uses an augmented-contextfree grammar : a context-free core plus a set of constraints .</sentence>
				<definiendum id="0">augmented-contextfree grammar</definiendum>
				<definiens id="0">a context-free core plus a set of constraints</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>The necessary dependency rules ( b ) For each dependency rule the compiler must find all the conceptual graphs that can be associated to such rule and to use them to generate a KS .</sentence>
				<definiendum id="0">necessary dependency rules</definiendum>
				<definiens id="0">the conceptual graphs that can be associated to such rule and to use them to generate a KS</definiens>
			</definition>
			<definition id="1">
				<sentence>The C KS propagates the constraints for the A KS : it propagates AI+A2 and the time constraint that the constituent must be adjacent ( on the right ) to the header .</sentence>
				<definiendum id="0">C KS</definiendum>
				<definiens id="0">propagates the constraints for the A KS : it propagates AI+A2 and the time constraint that the constituent must be adjacent ( on the right</definiens>
			</definition>
			<definition id="2">
				<sentence>The score of a PH made up by a number of word hypotheses is defined as the average of the scores of its component words , weighted by their time durations .</sentence>
				<definiendum id="0">score of a PH</definiendum>
				<definiens id="0">the average of the scores of its component words , weighted by their time durations</definiens>
			</definition>
			<definition id="3">
				<sentence>The JOIN operator describes the activity in which a KS merges together parsing processes that had evolved separately ; this may correspond either to a bottom-up or to a top-down step .</sentence>
				<definiendum id="0">JOIN operator</definiendum>
				<definiens id="0">describes the activity in which a KS merges together parsing processes that had evolved separately</definiens>
			</definition>
			<definition id="4">
				<sentence>The failure rate is the percentage of sentences for which no result has been obtained by the parser within the real-time imposed constraints .</sentence>
				<definiendum id="0">failure rate</definiendum>
				<definiens id="0">the percentage of sentences for which no result has been obtained by the parser within the real-time imposed constraints</definiens>
			</definition>
			<definition id="5">
				<sentence>O. Stock , R. Falcone , and P. Insinnamo , `` Bidirectional Charts : a Potential Technique for Parsing Spoken Natural Language Sentences '' , Computer , Speech , and Language , 3 ( 3 ) , 1989 .</sentence>
				<definiendum id="0">Bidirectional Charts</definiendum>
				<definiens id="0">a Potential Technique for Parsing Spoken Natural Language Sentences '' , Computer , Speech</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>The ACQUILEX LKB is designed to support representation of multilingual lexical information extracted from machine readable dictionaries ( MRDs ) in such a way that it can be utilised by NLP systems .</sentence>
				<definiendum id="0">ACQUILEX LKB</definiendum>
			</definition>
			<definition id="1">
				<sentence>In contrast to lexical database systems ( LDBs ) or thesauruslike representations ( e.g. Alshawi et al. , 1989 ; Calzolari , 1988 ) which represent extracted data in such a way as to support browsing and querying , our goal is to build a knowledge base which can be used as a highly structured reusable lexicon , albeit one much richer in lexical semantic information than those commonly used in NLP .</sentence>
				<definiendum id="0">LDBs</definiendum>
				<definiens id="0">to build a knowledge base which can be used as a highly structured reusable lexicon</definiens>
			</definition>
			<definition id="2">
				<sentence>Since natural is a parent of c_nat_subst , ORIGIN is an appropriate feature for c_nat .</sentence>
				<definiendum id="0">ORIGIN</definiendum>
				<definiens id="0">an appropriate feature for c_nat</definiens>
			</definition>
			<definition id="3">
				<sentence>We define a typed feature structure as a tuple F = ( Q , q0 , /f , 0 ) , where the only difference from the untypec case is that every node of a typed FS has a type , 6 ( q ) The type of a FS is the type of its initial node , O ( qo ) The definition of subsumption of typed FSs is very sire liar to that for untyped FSs , with the additional provis ( that the ordering must be consistent with the ordering oI their types .</sentence>
				<definiendum id="0">FS</definiendum>
				<definiendum id="1">FS</definiendum>
				<definiens id="0">a typed feature structure as a tuple F = ( Q , q0 , /f , 0 ) , where the only difference from the untypec case is that every node of a typed</definiens>
				<definiens id="1">the type of its initial node</definiens>
			</definition>
			<definition id="4">
				<sentence>C ( t ) denotes the constraint FS associated with type t. We define the notion of appropriate features as follows : Definition 1 /f C ( t ) = ( Q , qo , 6 , O ) we define Appfeat ( t ) = reat ( qo ) where we define Feat ( q ) to be the set of features labelling transitions from the node q such that f e Feat ( q ) if 6 ( f , q ) is defined .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">the constraint FS associated with type t. We define the notion of appropriate features as follows : Definition 1 /f C ( t ) = ( Q , qo , 6</definiens>
			</definition>
			<definition id="5">
				<sentence>Pl &lt; P2 : =~ Typeof ( pl ) E Typeof ( p2 ) where Pl and P2 are psorts The typing system thus restricts default inheritance essentially to the filling in of values for features which are defined by the type system .</sentence>
				<definiendum id="0">Pl</definiendum>
				<definiens id="0">=~ Typeof ( pl ) E Typeof ( p2 ) where</definiens>
			</definition>
			<definition id="6">
				<sentence>The most important factor in space efficiency is the use of inheritance , both in the type system and the psort system , which allows unexpanded lexical entries to be very compact .</sentence>
				<definiendum id="0">psort system</definiendum>
				<definiens id="0">allows unexpanded lexical entries to be very compact</definiens>
			</definition>
			<definition id="7">
				<sentence>SEISD : An Environment for Extraction of Semantic Information from On-Line Dictionaries .</sentence>
				<definiendum id="0">SEISD</definiendum>
				<definiens id="0">An Environment for Extraction of Semantic Information from On-Line Dictionaries</definiens>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>JASPER uses a template-driven approach , partial understanding techniques , and heuristic procedures to extract certain key pieces of information from a limited range of text .</sentence>
				<definiendum id="0">JASPER</definiendum>
				<definiens id="0">uses a template-driven approach , partial understanding techniques , and heuristic procedures to extract certain key pieces of information from a limited range of text</definiens>
			</definition>
			<definition id="1">
				<sentence>JASPER ( Journalist 's Assistant for Preparing Earnings Reports ) is a fact extraction system recently developed and deployed by Carnegie Group for Reuters Ltd .</sentence>
				<definiendum id="0">JASPER</definiendum>
				<definiens id="0">a fact extraction system recently developed and deployed by Carnegie Group for Reuters Ltd</definiens>
			</definition>
			<definition id="2">
				<sentence>Some systems base their processing on a particular linguistic theory ; for example , CAUCUS uses Lexical Functional Grammar and NAS uses a Government-Binding approach to syntax and semantics .</sentence>
				<definiendum id="0">CAUCUS</definiendum>
				<definiens id="0">uses Lexical Functional Grammar and NAS uses a Government-Binding approach to syntax and semantics</definiens>
			</definition>
			<definition id="3">
				<sentence>Once all available information has been extracted and stored in the frame , JASPER generates a news story from the information and makes the story available to reporters for editing .</sentence>
				<definiendum id="0">JASPER</definiendum>
				<definiens id="0">generates a news story from the information and makes the story available to reporters for editing</definiens>
			</definition>
			<definition id="4">
				<sentence>For each sentence in the release , JASPER checks every item on an ordered list of targeted information types , or slots , to determine whether a value has already been assigned to the corresponding slot .</sentence>
				<definiendum id="0">JASPER</definiendum>
				<definiens id="0">checks every item on an ordered list of targeted information types</definiens>
			</definition>
			<definition id="5">
				<sentence>If no value has yet been stored , JASPER tries to match the current sentence against a set of patterns associated with that slot .</sentence>
				<definiendum id="0">JASPER</definiendum>
				<definiens id="0">tries to match the current sentence against a set of patterns associated with that slot</definiens>
			</definition>
			<definition id="6">
				<sentence>One important component of Carnegie Group 's Text Categorization Shell is a powerful pattern matcher which matches complex patterns of words written in a specialized pattern language against text .</sentence>
				<definiendum id="0">Text Categorization Shell</definiendum>
			</definition>
			<definition id="7">
				<sentence>The latter will fall because more than eight words intervene between earnings and the number , JASPER uses an extended version of the TCS pattern marcher for extracting information .</sentence>
				<definiendum id="0">JASPER</definiendum>
				<definiens id="0">uses an extended version of the TCS pattern marcher for extracting information</definiens>
			</definition>
			<definition id="8">
				<sentence>Recall is the percentage of actual earnings and dividend announcements that the selection process succeeds in finding .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">the percentage of actual earnings and dividend announcements that the selection process succeeds in finding</definiens>
			</definition>
			<definition id="9">
				<sentence>Precision is the percentage of announcements that JASPER selects that are actually relevant , i.e. relate to earnings or dividends .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the percentage of announcements that JASPER selects that are actually relevant , i.e. relate to earnings or dividends</definiens>
			</definition>
			<definition id="10">
				<sentence>JASPER is a deployed system which solves a real business problem .</sentence>
				<definiendum id="0">JASPER</definiendum>
				<definiens id="0">a deployed system which solves a real business problem</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>World modeling is supported by the ONTOS system , which consists of a ) a constraint language , b ) an ontology , or set of general concepts , c ) a set of domain models and d ) an intelligent knowledge acquisition interface .</sentence>
				<definiendum id="0">World modeling</definiendum>
				<definiendum id="1">c</definiendum>
				<definiens id="0">consists of a ) a constraint language</definiens>
			</definition>
			<definition id="1">
				<sentence>A world model is a collection of frames .</sentence>
				<definiendum id="0">world model</definiendum>
				<definiens id="0">a collection of frames</definiens>
			</definition>
			<definition id="2">
				<sentence>A frame is a named set of slots , interpreted as an ontological concept ( voluntary-olfactory-event , geopolitical-entity ) .</sentence>
				<definiendum id="0">frame</definiendum>
				<definiens id="0">a named set of slots , interpreted as an ontological concept ( voluntary-olfactory-event , geopolitical-entity )</definiens>
			</definition>
			<definition id="3">
				<sentence>A slot represents an ontological property ( temperature , caused-by ) and consists of a named set of facets .</sentence>
				<definiendum id="0">slot</definiendum>
				<definiens id="0">an ontological property ( temperature , caused-by ) and consists of a named set of facets</definiens>
			</definition>
			<definition id="4">
				<sentence>A facet is a named set of fillers .</sentence>
				<definiendum id="0">facet</definiendum>
				<definiens id="0">a named set of fillers</definiens>
			</definition>
			<definition id="5">
				<sentence>8 : TAMERLAN is a formal language for representing the meaning of NL texts .</sentence>
				<definiendum id="0">TAMERLAN</definiendum>
			</definition>
			<definition id="6">
				<sentence>12 '' theoutputofsemantic analysis is the representation of text meaning ( interlingua text ) , expressed in TAMERLAN .</sentence>
				<definiendum id="0">theoutputofsemantic analysis</definiendum>
				<definiens id="0">the representation of text meaning ( interlingua text ) , expressed in TAMERLAN</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>A user-adapted guidance system must collect information about the user 's knowledge of the guidance domain and build a User Model ( UM ) .</sentence>
				<definiendum id="0">user-adapted guidance system</definiendum>
			</definition>
			<definition id="1">
				<sentence>FIGSI : A flexible guidance dialog system .</sentence>
				<definiendum id="0">FIGSI</definiendum>
				<definiens id="0">A flexible guidance dialog system</definiens>
			</definition>
</paper>

		<paper id="1046">
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>KOMET/Penman is a domain-independent text generation system based on systemic-functional grammar ( Halliday , 1978 ) .</sentence>
				<definiendum id="0">KOMET/Penman</definiendum>
			</definition>
			<definition id="1">
				<sentence>The Upper Model is the component of the system that is primarily responsible for mediating between the knowledge specific to any given domain and the general lexical and grammatical expressions that are provided by a language .</sentence>
				<definiendum id="0">Upper Model</definiendum>
				<definiens id="0">the component of the system that is primarily responsible for mediating between the knowledge specific to any given domain and the general lexical and grammatical expressions that are provided by a language</definiens>
			</definition>
			<definition id="2">
				<sentence>An SPL expression defines the semantic content of a sentence to be generated ; it consists of a set of typed variables and relations defined between those variables .</sentence>
				<definiendum id="0">SPL expression</definiendum>
				<definiens id="0">defines the semantic content of a sentence to be generated ; it consists of a set of typed variables and relations defined between those variables</definiens>
			</definition>
			<definition id="3">
				<sentence>, e.g. workshops and topics ( cf. Figure 1 ) , and a Presentation-Domain ( P-Domain ) , which contains concepts related to the way this information is presented by SIC !</sentence>
				<definiendum id="0">Presentation-Domain</definiendum>
				<definiens id="0">contains concepts related to the way this information is presented by SIC</definiens>
			</definition>
			<definition id="4">
				<sentence>RST is a theory of the organization of natural language texts ( Mann and Thompson , 1987 ) .</sentence>
				<definiendum id="0">RST</definiendum>
			</definition>
			<definition id="5">
				<sentence>An RST relation consists of two parts , a nucleus and a satellite .</sentence>
				<definiendum id="0">RST relation</definiendum>
				<definiens id="0">consists of two parts , a nucleus and a satellite</definiens>
			</definition>
			<definition id="6">
				<sentence>The dialog manager is one of the main components of our interface system ( cf. Figure 6 ) .</sentence>
				<definiendum id="0">dialog manager</definiendum>
			</definition>
			<definition id="7">
				<sentence>Then , the SPL creator produces the appropriate SPL plans by combining information on the misconception and possible alternatives with elements from the SPL library .</sentence>
				<definiendum id="0">SPL creator</definiendum>
				<definiens id="0">produces the appropriate SPL plans by combining information on the misconception and possible alternatives with elements from the SPL library</definiens>
			</definition>
			<definition id="8">
				<sentence>XTRA : A Natural Language Access System to Expert Systems .</sentence>
				<definiendum id="0">XTRA</definiendum>
			</definition>
			<definition id="9">
				<sentence>Nigel : A Systemic Grammar for Text Generation , Technical Report RR-83-105 , USC/Information Sciences Institute , 1983 Mann , W.C. and Thompson , S.A. 1987 .</sentence>
				<definiendum id="0">Nigel</definiendum>
				<definiens id="0">A Systemic Grammar for Text Generation</definiens>
			</definition>
			<definition id="10">
				<sentence>Pointing : A Way Toward Explanation Dialogue .</sentence>
				<definiendum id="0">Pointing</definiendum>
				<definiens id="0">A Way Toward Explanation Dialogue</definiens>
			</definition>
			<definition id="11">
				<sentence>Natural Language and the Exploration of an Information Space : the ALFresco Interactive System .</sentence>
				<definiendum id="0">Information Space</definiendum>
				<definiens id="0">the ALFresco Interactive System</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>A part-of-speech tagger is a system that uses context to assign parts of speech to words .</sentence>
				<definiendum id="0">part-of-speech tagger</definiendum>
			</definition>
			<definition id="1">
				<sentence>Church uses the tagged Brown corpus for training \ [ Church , 1988\ ] .</sentence>
				<definiendum id="0">Church</definiendum>
			</definition>
			<definition id="2">
				<sentence>The use of an HMM permits complete flexibility in the choice of training corpora .</sentence>
				<definiendum id="0">HMM</definiendum>
				<definiens id="0">permits complete flexibility in the choice of training corpora</definiens>
			</definition>
			<definition id="3">
				<sentence>In brief , an HMM is a doubly stochastic process that generates sequence of symbols S = { Si , S2 , ... , ST } , Si E W I &lt; i &lt; T , where W is some finite set of possible symbols , by composing an underlying Markov process with a state-dependent symbol generator ( i.e. , a Markov process with noise ) , i Th Markov process captures the notion of sequence depen dency and is described by a set of N states , a matrix c transition probabilities A = { aij } 1 &lt; _ i , j &lt; _ N where ai is the probability of moving from state i to state j , and vector of initial probabilities H = { rq } 1 &lt; i &lt; N where is the probability of starting in state i. The symbol ger erator is a state-dependent measure on V described by matrix of symbol probabilities B = { bjk } 1 _ &lt; j &lt; __ N an 1 &lt; k &lt; M where M = IWI and bjk is the probability generating symbol s~ given that the Markov process is i state j.2 In part-of-speech tagging , we will model word order d , pendency through an underlying Markov process that ot crates in terms of lexical tags , 'yet we will only be ab to observe the sets of tags , or ambiguity classes , that aJ possible for individual words. The ambiguity class of eac word is the set of its permitted parts of speech , only or of which is correct in context. Given the parameters A , and H , hidden Markov modeling allows us to compute tt most probable sequence of state transitions , and hence tt mostly likely sequence of lexical tags , corresponding to sequence of ambiguity classes. In the following , N can identified with the number of possible.tags , and W wit the set of all ambiguity classes. Applying an HMM consists of two tasks : estimating tt model parameters A , B and H from a training set ; ar computing the most likely sequence of underlying sta transitions given new observations. Maximum likeliho ( estimates ( that is , estimates that maximize the probabili of the training set ) can be found through application of ternating expectation in a procedure known as the Baur Welch , or forward-backward , algorithm \ [ Baum , 1972\ ] . proceeds by recursively defining two sets of probabiliti , the forward probabilities , at+i ( J ) = \ [ ~-~at ( i ) ai~\ ] i=i b , ( St+i ) 1 &lt; t &lt; T-l , ( where at ( i ) = ribi ( Si ) for all i ; and the backward prob bilities , N = Ti &lt; t &lt; i , ( j=i where \ [ 3T ( j ) = 1 for all j. The forward probabili at ( i ) is the joint probability of the sequence up to tir t , { Si , S2 , ... , St } , and the event that the Markov pr cess is in state i at time t. Similarly , the backwa probability \ [ 3t ( j ) is the probability of seeing the sequen { St+i , St+2 ... . , ST } given that the Markov process is state i at time t. It follows that the probability of t entire sequence is N N P = E E °~t ( i ) ai~bJ ( St+i ) /3t+i ( j ) imi j=i for any t in the range l &lt; t &lt; T1.a iFor an introduction to hidden Markov modeling see \ [ l : biner and Juang , 1986\ ] . 2In the following we will write hi ( St ) for bjk if St = s~. 3This is most conveniently evaluated at t = T 1 , in whi ca e P = 134 Given an initial choice for the parameters A , B , and II the expected number of transitions , 7ij , from state i to state j conditioned on the observation sequence S may be computed as follows : T-1 1 7ij = -fi E at ( i ) aijbj ( St+l ) ~t+l ( j ) . t=l Hence we can estimate aij by : _ ET : 5'i = N Ej=l 7ij ET : I 1 at ( i ) ~t ( i ) Similarly , bj~ and 7ri can be estimated as follows : bjk = Et~s , : , , at ( j ) ~t ( j ) ET=I at ( j ) Zt ( j ) and ( 3 ) ( 4 ) 1 ~i -- -~Ot1 ( i ) ~1 ( i ) . ( 5 ) In summary , to find maximum likelihood estimates for A , B , and II , via the Baum-Welch algorithm , one chooses some starting values , applies equations 3-5 to compute new values , and then iterates until convergence. It can be shown that this algorithm will converge , although possibly to a non-global maximum \ [ Baum , 1972\ ] . Once a model has been estimated , selecting the most likely underlying sequence of state transitions corresponding to an observation S can be thought of as a maximization over all sequences that might generate S. An efficient dynamic programming procedure , known as the Viterbi algorithm \ [ Viterbi , 1967\ ] , arranges for this computation to proceed in time proportional to T. Suppose V = { v ( t ) } 1 &lt; t &lt; T is a state sequence that generates S , then the probability that V generates S is , T P ( v ) = % ( ub~ ( 1 ) ( S1 ) H a~ ( t-1 ) ~ ( t ) b~ ( t ) ( St ) . t=2 • To find the most probable such sequence we start by defining ¢1 ( i ) = ~rib~ ( S1 ) for 1 &lt; i &lt; N and then perform the recursion et ( j ) = ~a &lt; x\ [ ¢t-l ( i ) aij\ ] bj ( St ) ( 6 ) and Ct ( j ) = max-tCt_l ( i ) I &lt; i &lt; N for 2 &lt; t &lt; T and i _ &lt; j _ &lt; N. The crucial observation is-that-for each time t and each state i one need only consider the most probable sequence arriving at state i at time t. The probability of the most probable sequence is maxl &lt; _i &lt; .N\ [ ¢T ( i ) \ ] while the sequence itself can be reconstructed by defining v ( T ) = maxl -- &lt; _li &lt; g eT ( i ) and v ( t I ) = et ( qt ) for T &gt; t &gt; 2 .</sentence>
				<definiendum id="0">HMM</definiendum>
				<definiendum id="1">W</definiendum>
				<definiendum id="2">matrix c transition</definiendum>
				<definiendum id="3">ai</definiendum>
				<definiendum id="4">ger erator</definiendum>
				<definiendum id="5">bjk</definiendum>
				<definiens id="0">a doubly stochastic process that generates sequence of symbols S = { Si , S2 , ... , ST } , Si E W I &lt; i &lt; T , where</definiens>
				<definiens id="1">some finite set of possible symbols , by composing an underlying Markov process with a state-dependent symbol generator ( i.e. , a Markov process with noise ) , i Th Markov process captures the notion of sequence depen dency and is described by a set of N states</definiens>
				<definiens id="2">the probability of moving from state i to state j , and vector of initial probabilities H = { rq } 1 &lt; i &lt; N where is the probability of starting in state i. The symbol</definiens>
				<definiens id="3">a state-dependent measure on V described by matrix of symbol probabilities B = { bjk } 1 _ &lt; j &lt; __ N an 1 &lt; k &lt; M where M = IWI and</definiens>
				<definiens id="4">the probability generating symbol s~ given that the Markov process is i state j.2 In part-of-speech tagging , we will model word order d , pendency through an underlying Markov process that ot crates in terms of lexical tags , 'yet we will only be ab to observe the sets of tags , or ambiguity classes , that aJ possible for individual words. The ambiguity class of eac word is the set of its permitted parts of speech , only or of which is correct in context. Given the parameters A , and H , hidden Markov modeling allows us to compute tt most probable sequence of state transitions , and hence tt mostly likely sequence of lexical tags , corresponding to sequence of ambiguity classes. In the following , N can identified with the number of possible.tags , and W wit the set of all ambiguity classes. Applying an HMM consists of two tasks : estimating tt model parameters A , B and H from a training set ; ar computing the most likely sequence of underlying sta transitions given new observations. Maximum likeliho ( estimates ( that is , estimates that maximize the probabili of the training set ) can be found through application of ternating expectation in a procedure known as the Baur Welch , or forward-backward , algorithm \ [ Baum , 1972\ ] . proceeds by recursively defining two sets of probabiliti , the forward probabilities , at+i ( J ) = \ [ ~-~at ( i ) ai~\ ] i=i b , ( St+i ) 1 &lt; t &lt; T-l , ( where at ( i ) = ribi ( Si ) for all i ; and the backward prob bilities , N = Ti &lt; t &lt; i , ( j=i where \ [ 3T ( j ) = 1 for all j. The forward probabili at ( i ) is the joint probability of the sequence up to tir t , { Si , S2 , ... , St } , and the event that the Markov pr cess is in state i at time t. Similarly , the backwa probability \ [ 3t ( j ) is the probability of seeing the sequen { St+i , St+2 ... . , ST } given that the Markov process is state i at time t. It follows that the probability of t entire sequence is N N P = E E °~t ( i ) ai~bJ ( St+i ) /3t+i ( j ) imi j=i for any t in the range l &lt; t &lt; T1.a iFor an introduction to hidden Markov modeling see \ [ l : biner and Juang , 1986\ ] . 2In the following we will write hi ( St ) for bjk if St = s~. 3This is most conveniently evaluated at t = T 1 , in whi ca e P = 134 Given an initial choice for the parameters A , B , and II the expected number of transitions , 7ij , from state i to state j conditioned on the observation sequence S may be computed as follows : T-1 1 7ij = -fi E at ( i ) aijbj ( St+l ) ~t+l ( j ) . t=l Hence we can estimate aij by : _ ET : 5'i = N Ej=l 7ij ET : I 1 at ( i ) ~t ( i ) Similarly , bj~ and 7ri can be estimated as follows : bjk = Et~s , : , , at ( j ) ~t ( j ) ET=I at ( j ) Zt ( j ) and ( 3 ) ( 4 ) 1 ~i -- -~Ot1 ( i ) ~1 ( i ) . ( 5 ) In summary , to find maximum likelihood estimates for A , B , and II , via the Baum-Welch algorithm , one chooses some starting values , applies equations 3-5 to compute new values , and then iterates until convergence. It can be shown that this algorithm will converge , although possibly to a non-global maximum \ [ Baum , 1972\ ] . Once a model has been estimated , selecting the most likely underlying sequence of state transitions corresponding to an observation S can be thought of as a maximization over all sequences that might generate S. An efficient dynamic programming procedure , known as the Viterbi algorithm \ [ Viterbi , 1967\ ] , arranges for this computation to proceed in time proportional to T. Suppose V = { v ( t ) } 1 &lt; t &lt; T is a state sequence that generates S , then the probability that V generates S is , T P ( v ) = % ( ub~ ( 1 ) ( S1 ) H a~ ( t-1 ) ~ ( t ) b~ ( t ) ( St ) . t=2 • To find the most probable such sequence we start by defining ¢1 ( i ) = ~rib~ ( S1 ) for 1 &lt; i &lt; N and then perform the recursion et ( j ) = ~a &lt; x\ [ ¢t-l ( i ) aij\ ] bj ( St ) ( 6 ) and Ct ( j ) = max-tCt_l ( i ) I &lt; i &lt; N for 2 &lt; t &lt; T and i _ &lt; j _ &lt; N. The crucial observation is-that-for each time t and each state i one need only consider the most probable sequence arriving at state i at time t. The probability of the most probable sequence is maxl &lt; _i &lt; .N\ [ ¢T ( i ) \ ] while the sequence itself can be reconstructed by defining v ( T ) = maxl -- &lt; _li &lt; g eT ( i ) and v ( t I ) = et ( qt ) for T &gt; t &gt; 2</definiens>
			</definition>
			<definition id="4">
				<sentence>Let 51 ( i ) = al ( i ) and define ct = 5t i l &lt; t &lt; T. Now define &amp; t ( i ) = ctK~t ( i ) and use a in place of a in equation 1 to define &amp; for the next iteration : 5t+l ( j ) = &amp; t ( i ) aij bj ( St+l ) l &lt; t &lt; T-1. Note that Ein__=l ~t ( i ) = 1 for 1 &lt; t &lt; T. Similarly , let ~T ( i ) = ~T ( i ) and define 3t ( i ) = ct~t ( i ) for T &gt; t &gt; 1 where N ~t ( i ) = E aiJ bj ( St+l ) 3t+l ( j ) j=l T-l &lt; t &lt; l. The scaled backward and forward probabilities , 5 and ~ , can be exchanged for the unscaled probabilities in equations 3-5 without affecting the value of the ratios. To see this , note that at ( i ) = C\ [ at ( i ) and ~t ( i ) = ~t ( i ) C/+l where J C~=Hct. Now , in terms of the scaled probabilities , equation 5 , for example , can be seen to be unchanged : ( ~ 1 ( i ) f } l ( i ) _ EN=I aT ( i ) E~=l CTaT ( i ) = ~'i. A slight difficulty occurs in equation 3 that can be cured by the addition of a new term , ct+l , in each product of the upper sum : T-1 ^ • ^ . ~~t=l at ( z ) aijbj ( St+ , ) ~t+l ( J ) Ct+l ^ ET_~ll &amp; t ( i ) ~t ( i ) = a , j. Numerical instability in the Viterbi algorithm can be ameliorated by operating on a logarithmic scale \ [ Levinson et al. , 1983\ ] . That is , one maximizes the log probability of each sequence of state transitions , log ( P ( v ) ) = + log ( b ( 1 ) ( Sl ) ) + T E log ( a~ ( t_ 1 ) ~ ( t ) ) + log ( b~ ( t ) ( St ) ) . t=2 Hence , equation 6 is replaced by et ( J ) = max \ [ ¢t-1 ( i ) + log ( ao ) \ ] + logbj ( St ) . I &lt; i &lt; N Care must be taken with zero probabilities. However , this can be elegantly handled through the use of IEEE negative infinity \ [ P754 , 1981\ ] . 135 As can be seen from equations 1-5 , the time cost of training is O ( TN~ ) . Similarly , as given in equation 6 , the Viterbi algorithm is also O ( TN2 ) . However , in part-of-speech tagging , the problem structure dictates that the matrix of symbol probabilities B is sparsely populated. That is , bij 3£ 0 iff the ambiguity class corresponding to symbol j includes the part-of-speech tag associated with state i. In practice , the degree of overlap between ambiguity classes is relatively low ; some tokens are assigned unique tags , and hence have only one non-zero symbol probability. The sparseness of B leads one to consider restructuring equations 1-6 so a check for zero symbol probability can obviate the need for further computation. Equation 1 is already conveniently factored so that the dependence on bj ( St+l ) is outside the inner sum. Hence , ifk is the average number of non-zero entries in each row of B , the cost of computing equation 1 can be reduced to O ( kTN ) . Equations 2-4 can be similarly reduced by switching the order of iteration. For example , in equation 2 , rather than for a given t computing/3t ( i ) for each i one at a time , one can accumulate terms for all i in parallel. The net effect of this rewriting is to place a bj ( St+l ) = 0 check outside the innermost iteration. Equations 3 and 4 submit to a similar approach. Equation 5 is already only O ( N ) . Hence , the overall cost of training can be reduced to O ( kTN ) , which , in our experience , amounts to an order of magnitude speedupfl The time complexity of the Viterbi algorithm can also be reduced to O ( kTN ) by noting that bj ( St ) can be factored out of the maximization of equation 6. Adding up the sizes of the probability matrices A , B , and H , it is easy to see that the storage cost for directly representing one model is proportional to N ( N -tM + 1 ) . Running the Baum-Welch algorithm requires storage for the sequence of observations , the a and /3 probabilities , the vector { ci } , and copies of the A and B matrices ( since the originals can not be overwritten until the end of each iteration ) . Hence , the grand total of space required for training is proportional to T q2N ( T qN + M + 1 ) . Since N and M are fixed by the model , the only parameter that can be varied to reduce storage costs is T. Now , adequate training requires processing from tens of thousands to hundreds of thousands of tokens \ [ Kupiec , 1989a\ ] . The training set can be considered one long sequence , it which case T is very large indeed , or it can be broken up into a number of smaller sequences at convenient boundaries. In first-order hidden Markov modeling , the stochastic process effectively restarts at unambiguous tokens , such as sentence and paragraph markers , hence these tokens are convenient points at which to break the training set. If the Baum-Weleh algorithm is run separately ( from the same starting point ) on each piece , the resulting trained models must be recombined in some way. One obvious approach is simply to average. However , this fails if any two 4An equivalent approach maintains a mapping from states i to non-zero symbol probabilities and simply avoids , in the inner iteration , computing products which must be zero \ [ Kupiec , 1992\ ] . states are indistinguishable ( in the sense that they had the same transition probabilities and the same symbol probabilities at start ) , because states are then not matched across trained models. It is therefore important that each state have a distinguished role , which is relatively easy to achieve in part-of-speech tagging. Our implementation of the Baum-Welch algorithm breaks up the input into fixed-sized pieces of training text. The Baum-Welch algorithm is then run separately on each piece and the results are averaged together. Running the Viterbi algorithm requires storage for the sequence of observations , a vector of current maxes , a scratch array of the same size , and a matrix of ¢ indices , for a total proportional to T + N ( 2 + T ) and a grand total ( including the model ) of T -tN ( N HM + T ÷ 3 ) . Again , N and M are fixed. However , T need not be longer than a single sentence , since , as was observed above , the HMM , and hence the Viterbi algorithm , restarts at sentence boundaries. An HMM for part-of-speech tagging can be tuned in a variety of ways. First , the choice of tagset and lexicon determines the initial model. Second , empirical and a priori information can influence the choice of starting values for the Baum-Welch algorithm. For example , counting instances of ambiguity classes in running text allows one to assign non-uniform starting probabilities in A for a particular tag 's realization as a particular ambiguity class. Alternatively , one can state a priori that a particular ambiguity class is most likely to be the reflection of some subset of its component tags. For example , if an ambiguity class consisting of the open class tags is used for unknown words , one may encode the fact that most unknown words are nouns or proper nouns by biasing the initial probabilities in B. Another biasing of starting values can arises from noting that some tags are unlikely to be followed by others. For example , the lexical item `` to '' maps to an ambiguity class containing two tags , infinitive-marker and to-aspreposition , neither of which occurs in any other ambiguity class. If nothing more were stated , the HMM would have two states which were indistinguishable. This can be remedied by setting the initial transition probabilities from infinitive-marker to strongly favor transitions to such states as verb-uninflected and adverb. Our implementation allows for two sorts of biasing of starting values : ambiguity classes can be annotated with favored tags ; and states can be annotated with favored transitions. These biases may be specified either as sets or as set complements. Biases are implemented by replacing the disfavored probabilities with a small constant ( machine epsilon ) and redistributing mass to the other possibilities. This has the effect of disfavoring the indicated outcomes without disallowing them ; sufficient converse data can rehabilitate these values. In support of this and other work , we have developed a system architecture for text access \ [ Cutting et al. , 1991\ ] . This architecture defines five components for such systems : 136 Search Index Analysis Corpus °4 °°°° ... ... ... ... ... ... . °° ... ~°°°°°°° `` '-.°° o.O° °°°oo ... . '' '' ( further analysis ) `` '' '' stem , tag l //~ : / Tagging -~ Training . / ? ~trainedHMM/ ambiguityelass , &lt; stem , tag &gt; * ~/ambiguityclass Lexicon token l Tokenizer - ... character ~'°o~ , oQ .</sentence>
				<definiendum id="0">aiJ bj</definiendum>
				<definiendum id="1">ifk</definiendum>
				<definiens id="0">stem , tag &gt; * ~/ambiguityclass Lexicon token l Tokenizer - ... character ~'°o~</definiens>
			</definition>
			<definition id="5">
				<sentence>, °°°°°°°°~ °° ... .. • ... ... .. * ... . , • Figure 1 : Tagger Modules in System Context corpus , which provides text in a generic manner ; analysis , which extracts terms from the text ; index which stores term occurrence statistics ; and search , which utilizes these statistics to resolve queries .</sentence>
				<definiendum id="0">analysis</definiendum>
				<definiendum id="1">search</definiendum>
				<definiens id="0">Tagger Modules in System Context corpus , which provides text in a generic manner ;</definiens>
				<definiens id="1">extracts terms from the text ; index which stores term occurrence statistics</definiens>
			</definition>
			<definition id="6">
				<sentence>A lookahead mechanism allows us to specify in the sentence-boundary regular expression that the final character matched should not be considered a part of the token .</sentence>
				<definiendum id="0">lookahead mechanism</definiendum>
				<definiens id="0">allows us to specify in the sentence-boundary regular expression that the final character matched should not be considered a part of the token</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>CLARE is a combined natural language and *CLARE is being developed as part of a collaborative project involving BP Research , British Aerospace , British Telecom , Cambridge University , SRI International and the UK Defence Research Agency .</sentence>
				<definiendum id="0">CLARE</definiendum>
				<definiens id="0">a combined natural language and *CLARE is being developed as part of a collaborative project involving BP Research</definiens>
			</definition>
			<definition id="1">
				<sentence>The CLE produces semantic interpretations of sentences in a notation called Quasi Logical Form .</sentence>
				<definiendum id="0">CLE</definiendum>
				<definiens id="0">produces semantic interpretations of sentences in a notation called Quasi Logical Form</definiens>
			</definition>
			<definition id="2">
				<sentence>The translation process then corresponds to abductive reasoning that views the meaning postulates as conditional definitions of the linguistic predicates in terms of database ( or intermediate ) predicates , the conditions being either discharged or taken as assumptions for a particular derivation .</sentence>
				<definiendum id="0">translation process</definiendum>
				<definiens id="0">corresponds to abductive reasoning that views the meaning postulates as conditional definitions of the linguistic predicates in terms of database</definiens>
			</definition>
			<definition id="3">
				<sentence>cates and DB_PROJECT_MEMBER is a database relation ( w ( will adhere to the convention of capitalizing names ol database relations ) .</sentence>
				<definiendum id="0">DB_PROJECT_MEMBER</definiendum>
				<definiens id="0">a database relation</definiens>
			</definition>
			<definition id="4">
				<sentence>For example , the rule and ( manl ( X ) , employeel ( X ) ) &lt; - &gt; exists ( \ [ HasCar\ ] , employee ( X , m , HasCar ) ) produces two normal Horn-clause readings , manl ( X ) &lt; employee ( X , m , HasCar ) . employeel ( X ) &lt; employee ( X , m , HasCar ) . and one backward Horn-clause reading , employee ( X , m , skl ( X ) ) &lt; manl ( X ) , employeel ( X ) . where ski is a Skolem function. Note that in the equivalential reading , as well as in the backward one , it is essential to distinguish between existential and universal quantification of variables on the left-hand side. The equivalential reading of a rule of type p ( X , Y ) &lt; - &gt; q ( Y ) licences , for example , expansion of p ( a , b ) to q ( b ) ; the justification for this is that q ( b ) implies p ( X , b ) for any value of X. However , if the rule is changed to exisgs ( \ [ X\ ] , p ( X , Y ) ) &lt; - &gt; q ( Y ) the expansion is no longer valid , since q ( b ) only implies that p ( X , b ) is valid for some value of X , and not necessarily for a. This pair of examples should clarify why the constants involved in schema ( 2 ) must be unique .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">ski</definiendum>
				<definiens id="0">produces two normal Horn-clause readings , manl ( X ) &lt; employee ( X , m , HasCar ) . employeel ( X ) &lt; employee ( X , m</definiens>
				<definiens id="1">essential to distinguish between existential and universal quantification of variables on the left-hand side. The equivalential reading of a rule of type p</definiens>
			</definition>
			<definition id="5">
				<sentence>Translate : find a rule Byi .</sentence>
				<definiendum id="0">Translate</definiendum>
				<definiens id="0">find a rule Byi</definiens>
			</definition>
			<definition id="6">
				<sentence>The simplest way to ensure that this is so is to provide a Horn-clause meaning postulate , employeel ( X ) &lt; DB_PROJECT_MEMBER ( Proj ect , X ) . ( HCI ) which encodes the fact that project members are employees. Similarly , we will need an equivalence rule to convert between work_onl and DB_PROJECT_MEMBER. Here the fact we want to state is that project-members are precisely people who work on projects , which we write as follows : exists ( \ [ Event\ ] , and ( work_onl ( Event , Person , Project ) , project l ( Project ) ) ) &lt; - &gt; DB_PRO JECT_MEMBER ( Pro j ect , Person ) ( EQ2 ) We will also make indirect use of the rule that states that projects are objects that can be found in the first field of a DB_PROJECT tuple , project l ( Proj ) &lt; - &gt; exists ( \ [ ProjNum , Start , End\ ] , DB_PROJECT ( Pro3 , ProjNum , Start , End ) ) ( EQ3 ) since this will allow us to infer ( by looking in the database ) that the predicate project 1 holds of clare .</sentence>
				<definiendum id="0">HCI</definiendum>
				<definiens id="0">to provide a Horn-clause meaning postulate , employeel ( X ) &lt; DB_PROJECT_MEMBER ( Proj ect</definiens>
				<definiens id="1">Pro j ect , Person ) ( EQ2 ) We will also make indirect use of the rule that states that projects are objects that can be found in the first field of a DB_PROJECT tuple , project l ( Proj ) &lt; - &gt; exists</definiens>
			</definition>
			<definition id="7">
				<sentence>Thi , , new goal can now be reduced again , by applying the rul~ ( EQ2 ) as a backwards Horn-clause , to and ( work_onl ( Event , C* , Project ) , project I ( Project ) ) ) , The first conjunct can be proved from the assumptions instantiating Project to clare ; the second conjunct ca* now be derived from the normal Horn-clause reading o rule ( EQ3 ) , together with the fact that clare is listed a a project in the database .</sentence>
				<definiendum id="0">EQ2</definiendum>
				<definiens id="0">the normal Horn-clause reading o rule</definiens>
			</definition>
			<definition id="8">
				<sentence>So for example the meaning postulate relating the linguistic predicate paymentl and the intermediate predicate transaction is and ( payment I ( Trans ) , payment from_SRI ( Trans ) ) &lt; - &gt; exist s ( \ [ Cheque , Dat e , Payee\ ] , transaction ( Trans , Cheque , Date , Payee ) ) ) ( EQS ) `` transactions are payments from SRI '' and there is also a Horn-clause meaning postulate payment_from_SRI ( X ) &lt; payment s _ref erred_t o_are_f rom_SRI. and an assumptiondeclaration as sume ( payment s _ref erred_t o_are_f rom_SRI , cost ( 0 ) ) The advantage of this mechanism ( which may at first sight seem rather indirect ) is that it makes it possible explicitly to keep track of when the assumption payments._veferred_to_are_from_SRI has been used in the course of deriving a database query from the original LF. Applied systematically , it allows a set of assumptions to be collected in the course of performing the translation ; if required , CLARE can then inform the user as to their nature. In the current version of the PRM application , there are about a dozen types of assumption that can be made. Most of these are similar to the one shown above : that is to say , they are low-cost assumptions that cheques , payments , projects and so on are SRI-related. One type of assumption , however , is sufficiently different as to deserve explicit mention. These are related to the problem , mentioned in Section 1 , of queries `` contingently '' outside the database 's domain. The PRM database , for instance , is limited in time , only containing records of transactions carried out over a specified eighteen-month period. Reflecting this , meaning postulates distinguish between the two predicates transaction and DB_TRANSACTION , which respectively are intended to mean `` A transaction of this type took place '' and `` A transaction of this type is recorded in the database '' . The meaning postulate linking them is and ( transaction ( Id , CNum , Date , Payee ) , transaction_data_available ( Date ) ) &lt; - &gt; DB_TRANSACTION ( Id~ CNum , Dat e , Payee ) ( EQ6 ) transaction_data_available is defined by the further postulate transaction_data_available ( Date ) &lt; and ( c_before ( date ( 17,8,89 ) , Date ) , c_before ( Dat e , date ( 31,3,91 ) ) ) ( HC2 ) The interesting thing about ( HC2 ) is that the information needed to prove the condition transaction_data_available ( Date ) is sometimes , though not always , present in the LF. It will be present in a query like ( $ I ) , which explicitly mentions a period ; there are further axioms that allow the system to infer in these circumstances that the conditions are fulfilled. However , a query like ( $ 5 ) , ( $ 5 ) Show the largest payment to Cow 's Milk. contains no explicit mention of time. To deal with sentences like ( $ 5 ) , there is a meaning postulate transaction_data_available ( X ) &lt; payments_referred_to made_between ( 17/8/89 , 31/3/91 ) . with an associated assumption declaration as sume ( payments_referred_to_made_between ( 17/8/89 , 31/3/91 ) , cost ( 15 ) ) . The effect of charging the substantial cost of 15 units for the assumption ( the maximum permitted cost for an expansion step being 20 ) is in practice strongly to prefer proofs where it is not used ; the net result from the user 's perspective is that s/he is informed of the contingent temporal limitation of the database only when it is actually relevant to answering a query. This has obvious utility in terms of increasing the interface 's userfriendliness. Information A problem arising from the definition-expansion process which we have so far not mentioned is that the 5 database queries it produces tend to contain a considerable amount of redundancy. For example , we shall see below in section 9 that the database query derived from sentence ( S1 ) originally contains three separate instances of the transaction relation , one from each of the original linguistic predicates paymentl , make2 and duringl. Roughly speaking , payraentl ( Ev ) expands to transaction ( Ev ... ... ) , make2 ( Ev , Ag , P , To ) to transaction ( Ev , _ , To , _ ) and during_Temporal ( Ev , Date ) to transaction ( Ev ... .. Date ) ; the database query will conjoin all three of these together. It is clearly preferable , if possible , to merge them instead , yielding a composite predication transact ion ( Ev , _ , To , Dat e ) . Our framework allows an elegant solution to this problem if a little extra declarative information is provided , specifically information concerning functional relationships in predicates. The key fact is that transaction is a function from its first argument ( the transaction identifier ) to the remaining ones ( the cheque number , the payee and the date ) . The system allows this information to be entered as a `` function '' meaning postulate in the form funct ion ( transact ion ( Id , ChequeNo , Payee , Date ) , \ [ Id\ ] - &gt; \ [ ChequeNo , Payee , Date\ ] ) This is treated as a concise notation for the meaning postulate transaction ( i , cl , Pl , dl ) ( transaction ( i , c2 , P2 , d2 ) ~-~ Cl -c2 A Pl = P2 A dl = d2 ) which is just a conditional form of the equivalential meaning postulates already described .</sentence>
				<definiendum id="0">transaction ( Trans , Cheque , Date , Payee ) ) ) ( EQS ) `` transactions</definiendum>
				<definiendum id="1">Payee )</definiendum>
				<definiens id="0">example the meaning postulate relating the linguistic predicate paymentl and the intermediate predicate transaction is and ( payment I ( Trans ) , payment from_SRI ( Trans ) ) &lt; - &gt; exist s ( \ [ Cheque , Dat e</definiens>
				<definiens id="1">payments from SRI '' and there is also a Horn-clause meaning postulate payment_from_SRI ( X ) &lt; payment s _ref erred_t o_are_f rom_SRI. and an assumptiondeclaration as sume ( payment s _ref erred_t o_are_f rom_SRI , cost ( 0 ) ) The advantage of this mechanism ( which may at first sight seem rather indirect ) is that it makes it possible explicitly to keep track of when the assumption payments._veferred_to_are_from_SRI has been used in the course of deriving a database query from the original LF. Applied systematically , it allows a set of assumptions to be collected in the course of performing the translation ; if required , CLARE can then inform the user as to their nature. In the current version of the PRM application , there are about a dozen types of assumption that can be made. Most of these are similar to the one shown above : that is to say , they are low-cost assumptions that cheques , payments , projects and so on are SRI-related. One type of assumption , however , is sufficiently different as to deserve explicit mention. These are related to the problem , mentioned in Section 1 , of queries `` contingently '' outside the database 's domain. The PRM database , for instance , is limited in time , only containing records of transactions carried out over a specified eighteen-month period. Reflecting this , meaning postulates distinguish between the two predicates transaction and DB_TRANSACTION , which respectively are intended to mean `` A transaction of this type took place '' and `` A transaction of this type is recorded in the database '' . The meaning postulate linking them is and ( transaction ( Id , CNum , Date , Payee ) , transaction_data_available ( Date ) ) &lt; - &gt; DB_TRANSACTION ( Id~ CNum , Dat e ,</definiens>
				<definiens id="2">Dat e , date ( 31,3,91 ) ) ) ( HC2 ) The interesting thing about ( HC2 ) is that the information needed to prove the condition transaction_data_available ( Date ) is sometimes , though not always , present in the LF. It will be present in a query like ( $ I ) , which explicitly mentions a period ; there are further axioms that allow the system to infer in these circumstances that the conditions</definiens>
			</definition>
			<definition id="9">
				<sentence>( $ 7 ) Clara is a woman .</sentence>
				<definiendum id="0">Clara</definiendum>
				<definiens id="0">a woman</definiens>
			</definition>
			<definition id="10">
				<sentence>After ( $ 6 ) and ( $ 7 ) have been processed , the local cache contains the clauses DB_EMPLOYEE ( clara , sk 1 , y ) DB_EMPLOYEE ( clara , w , sk2 ) A = A0 is then the formula and ( DB EMPLOYEE ( clara , sk 1 , y ) DB_EMPLOYEE ( clara , w , sk2 ) ) and A2 is exists ( \ [ Xl , X2\ ] and ( DB EMPLOYEE ( clara , X 1 , y ) DB_EMPLOYEE ( clara , w , X2 ) ) Since DB_EMPLOYEE is declared functional on its first argument , the second conjunct is reduced to two equalities : giving the formula exists ( \ [ Xl , X2\ ] and ( DB_EMPLOYEE ( clara , X I , y ) and ( Xl = w , y = x2 ) ) which finally simplifies to A ' , DB_EMPLOYEE ( clara , w , y ) a record without Skolem constants , which can be added to a normal relational database .</sentence>
				<definiendum id="0">A2</definiendum>
				<definiendum id="1">DB EMPLOYEE</definiendum>
				<definiens id="0">y = x2 ) ) which finally simplifies to A ' , DB_EMPLOYEE ( clara , w , y ) a record without Skolem constants , which can be added to a normal relational database</definiens>
			</definition>
			<definition id="11">
				<sentence>The relevant equivalences are now duringl ( EI , E2 ) &lt; - &gt; exists ( \ [ Ti , T2\ ] , and ( associated_time ( El , TI ) , and ( associat ed_t ime ( E2 , T2 ) , c_during ( TI , T2 ) ) ) ) ) ( EQ9 ) `` The duringl relation holds between E1 and E2 if and only if the calendar event associated with E1 is inside that associated with E2 . ''</sentence>
				<definiendum id="0">relevant equivalences</definiendum>
			</definition>
			<definition id="12">
				<sentence>and ( associat ed_t ime ( Id , Dat e ) , transactionl ( Id ) ) &lt; - &gt; exists ( \ [ C , Payee , Y , M , D\ ] , transaction ( Id , C , Date , P ) ) ( EQ I0 ) `` Date is the event associated with a transaction event if and only if they enter into the transaction relation as third and first arguments respectively . ''</sentence>
				<definiendum id="0">Date</definiendum>
			</definition>
			<definition id="13">
				<sentence>Applying ( EQ9 ) and ( EQ10 ) in succession , the query is translated to find ( \ [ PayEr\ ] , exists ( \ [ Date , A , B , C , D , E , F , G\ ] , and ( transact ion ( PayEv , A , B , C ) , and ( transact ion ( PayEv , D , E , bt ) ) , and ( transact ion ( PayEv , F , Date , G ) , 7 and ( c_during ( Dat e , interval ( date ( 1990,1,1 ) ) , daze ( 1990,12,31 ) ) ) ) ) ) The query is now simplified by exploiting the fact that transaction is functional on its first argument : it is possible to merge all three occurrences , as described in section 7 , to produce the form find ( \ [ PayEr\ ] , exists ( \ [ ChequeId , Dat e\ ] , and ( trans act ion ( PayEr , Cheque Id , Dat e , bt ) , c_during ( Date , interval ( date ( 1990 , I , I ) ) , date ( 1990,12,31 ) ) ) ) ) Equivalences for temporal predicates then expand the second conjunct , producing the form find ( \ [ PayEr\ ] , exists ( \ [ ChequeId , Date\ ] , and ( transact ion ( PayEr , ChequeId , Date , bt ) , and ( c_before ( date ( 1990 , I , I ) , Date ) , c_before ( Date , date ( 1990,12,31 ) ) ) ) ) ) Finally , ( EQ6 ) above is applied , to expand the intermediate predicate transaction into the database relation DB_TKANSACTION .</sentence>
				<definiendum id="0">PayEv</definiendum>
				<definiens id="0">ion ( PayEr , ChequeId , Date , bt )</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>The Intelligent Documentation Advisory System generates on-line documentation and help messages from a domain knowledge base , using natural-language ( NL ) generation techniques .</sentence>
				<definiendum id="0">Intelligent Documentation Advisory System</definiendum>
				<definiens id="0">generates on-line documentation and help messages from a domain knowledge base , using natural-language ( NL ) generation techniques</definiens>
			</definition>
			<definition id="1">
				<sentence>The ATE contains an assortment of electronic instruments , a switching system that connects these instruments to a UUT ( Unit Under Test ) , and a computer which runs test programs that test the UUT with the instruments .</sentence>
				<definiendum id="0">ATE</definiendum>
			</definition>
			<definition id="2">
				<sentence>y.stem ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... .. User Figure 1 : Simplified IDAS architecture the output window ; if the user clicks on one , he invokes IDAS and the Listener creates an initial query about that component , i.e. , an initial point in question space ( Section 2.1 ) .</sentence>
				<definiendum id="0">Listener</definiendum>
				<definiens id="0">creates an initial query about that component</definiens>
			</definition>
			<definition id="3">
				<sentence>The question space point is given to IDAS 's NL generation system , which generates a response using three modules : content determination , which picks relevant information out of the knowledge base to communicate to the user ; text planning , which converts this information into an expression in SPL , the ISI Sentence Planning Language \ [ Kasper , 1989\ ] ; and surface realization , which produces a surface form , i.e. , an annotated text string .</sentence>
				<definiendum id="0">content determination</definiendum>
				<definiendum id="1">text planning</definiendum>
				<definiens id="0">produces a surface form</definiens>
			</definition>
			<definition id="4">
				<sentence>The annotations consist of text-formatting commands ( e.g. , Begin-New-Line ) and hypertext specifications .</sentence>
				<definiendum id="0">annotations</definiendum>
				<definiens id="0">consist of text-formatting commands ( e.g. , Begin-New-Line ) and hypertext specifications</definiens>
			</definition>
			<definition id="5">
				<sentence>Question space is the set of queries that can be given to IDAS 's NL generation system ; IDAS 's hypertext system can be viewed as a tool that enables a user to move around question space until he finds a point that gives him the information he is looking for .</sentence>
				<definiendum id="0">Question space</definiendum>
			</definition>
			<definition id="6">
				<sentence>Zltow-do-I-use-it is the interpretation of Itow-do-lperform-the-ta~k under an Operations user-task ~Other questions , e.g. , What-are-its-parts , C/Ill be asked by clicking on test head in the title question , alld selecting 69 Figure 2 : Example IDAS Texts button was described above ; it allows the user to explicitly specify a new point in question space .</sentence>
				<definiendum id="0">Zltow-do-I-use-it</definiendum>
				<definiens id="0">the interpretation of Itow-do-lperform-the-ta~k under an Operations user-task ~Other questions , e.g. , What-are-its-parts , C/Ill be asked by clicking on test head in the title question</definiens>
			</definition>
			<definition id="7">
				<sentence>IDAS is based around the ideas of ( 1 ) having a well-structured question space ; ( 2 ) using KR and NL generation systems that produce short targeted responses and allow canned text to be used when necessary ; and ( 3 ) presenting users with a hypertext-like interface that allows them to pose follow-up and elaboration questions .</sentence>
				<definiendum id="0">IDAS</definiendum>
			</definition>
			<definition id="8">
				<sentence>WIP : The coordinated generation of multimodal presentations from a common representation .</sentence>
				<definiendum id="0">WIP</definiendum>
				<definiens id="0">The coordinated generation of multimodal presentations from a common representation</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>This list consists of triples &lt; tag~ , tagb , number &gt; , indicating the number of times the tagger mistagged a word with taga when it should have been tagged with tagb in the patch corpus .</sentence>
				<definiendum id="0">list</definiendum>
				<definiens id="0">consists of triples &lt; tag~ , tagb , number &gt; , indicating the number of times the tagger mistagged a word with taga when it should have been tagged with tagb in the patch corpus</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>Such strategies involve deemphasizing the role of syntactic analysis ( Jacobs et al. , 1991 ) , producing partial parses with stochastic or heuristic parsers ( de Marcken , 1990 ; Weischedel et al 1991 ) or resorting to weaker syntactic processing methods such as conceptual or case-frame based parsing ( e.g. , Schank and Riesbeck , 1981 ) or template matching techniques ( Jackson et M. , 1991 ) .</sentence>
				<definiendum id="0">template matching techniques</definiendum>
				<definiens id="0">deemphasizing the role of syntactic analysis ( Jacobs et al. , 1991 ) , producing partial parses with stochastic or heuristic parsers ( de Marcken , 1990 ; Weischedel et al 1991 ) or resorting to weaker syntactic processing methods such as conceptual or case-frame based parsing</definiens>
			</definition>
			<definition id="1">
				<sentence>Variations in computing these metrics are possible , but intuitively understood , recall measures the percentage of correct fills a system finds ( ignoring wrong and spurious answers ) , and precision measures the percentage of correct fills provided out of the total number of answers posited .</sentence>
				<definiendum id="0">recall</definiendum>
			</definition>
			<definition id="2">
				<sentence>`` DIAGRAM : A Grammar for Dialogues '' , Communications of the A CM , Vol .</sentence>
				<definiendum id="0">DIAGRAM</definiendum>
				<definiens id="0">A Grammar for Dialogues ''</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>With an eye to semi-Thue or extended axiomatic systems one could say that a linearly ordered sequence of strings W , C1 , C2 , ... , Cm is a derivation of Cm iff ( 1 ) W is a ( faulty ) string ( in the text to be corrected ) and ( 2 ) each Ci follows from the immediately preceding string by one of the productions listed in the lexicon ( Partee et al. , 1990 ) .</sentence>
				<definiendum id="0">Cm</definiendum>
				<definiendum id="1">W</definiendum>
				<definiens id="0">a linearly ordered sequence of strings W , C1 , C2 , ... ,</definiens>
				<definiens id="1">a derivation of Cm iff ( 1 )</definiens>
			</definition>
			<definition id="1">
				<sentence>In case a correction C is proposed to the user , additionally a message will be displayed to him identifying the reason why C is correct rather than W. Depending on the user 's knowledge of the language under investigation , he can take this either as an opportunity to learn or rather as a help for deciding whether to finally accept or reject the proposal .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">a help for deciding whether to finally accept or reject the proposal</definiens>
			</definition>
			<definition id="2">
				<sentence>A first prototype of the system described above has been developed in C under UNIX within the ESPRIT II project 2315 `` Translator 's Workbench '' ( TWB ) as one of several separate modules checking basic as well as higher levels of various languages Ilike grammar and style ; see ( Thurmair , 1990 ) and ( Winkelmann , 1990 ) \ ] .</sentence>
				<definiendum id="0">TWB</definiendum>
				<definiens id="0">system described above has been developed in C under UNIX within the ESPRIT II project 2315 `` Translator 's Workbench '' (</definiens>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>We describe a workbench ( XTAG ) for the development of tree-adjoining grammars and their parsers , and discuss some issues that arise in the design of the graphical interface .</sentence>
				<definiendum id="0">XTAG</definiendum>
				<definiens id="0">the development of tree-adjoining grammars and their parsers , and discuss some issues that arise in the design of the graphical interface</definiens>
			</definition>
			<definition id="1">
				<sentence>XTAG provides such a graphical interface in which the elementary objects are trees ( or tree sets ) and not symbols ( or strings ) .</sentence>
				<definiendum id="0">XTAG</definiendum>
				<definiens id="0">provides such a graphical interface in which the elementary objects are trees ( or tree sets ) and not symbols ( or strings )</definiens>
			</definition>
			<definition id="2">
				<sentence>XTAG runs under Common Lisp and X Window ( CLX ) .</sentence>
				<definiendum id="0">XTAG</definiendum>
			</definition>
			<definition id="3">
				<sentence>Tree-adjoining grammar ( TAG ) \ [ Joshi et al. , 1975 ; Joshi , 1985 ; Joshi , 1987\ ] and its lexicalized variant \ [ Schabes et al. , 1988 ; Schabes , 1990 ; Joshi and Schabes , 1991\ ] are tree-rewriting systems in which the syntactic properties of words are encoded as tree structured-objects of extended size .</sentence>
				<definiendum id="0">Tree-adjoining grammar</definiendum>
			</definition>
			<definition id="4">
				<sentence>XTAG runs under Common Lisp \ [ Steele , 1990\ ] and it uses the Common LISP X Interface ( CLX ) to access the graphical primitives defined by the Xll protocol .</sentence>
				<definiendum id="0">XTAG</definiendum>
			</definition>
			<definition id="5">
				<sentence>• View the derivational history of a derived tree and its components ( elementary trees ) .</sentence>
				<definiendum id="0">View</definiendum>
				<definiens id="0">the derivational history of a derived tree and its components ( elementary trees )</definiens>
			</definition>
			<definition id="6">
				<sentence>XTAG includes a predictive left to right parser for unification-based tree-adjoining grammar \ [ Schabes , 1991\ ] .</sentence>
				<definiendum id="0">XTAG</definiendum>
				<definiens id="0">includes a predictive left to right parser for unification-based tree-adjoining grammar \</definiens>
			</definition>
			<definition id="7">
				<sentence>All the syntactic concepts of lexicalized TAG ( such as the grouping of the trees in tree families which represents the possible variants on a basic subcategorization frame ) are accessible through mouse-sensitive items .</sentence>
				<definiendum id="0">lexicalized TAG</definiendum>
				<definiens id="0">represents the possible variants on a basic subcategorization frame ) are accessible through mouse-sensitive items</definiens>
			</definition>
			<definition id="8">
				<sentence>The grammar consists of a morphological English analyzer and a syntactic lexicon , which is the domain of structural choice , subcategorization and selectional information .</sentence>
				<definiendum id="0">grammar</definiendum>
				<definiendum id="1">syntactic lexicon</definiendum>
				<definiens id="0">consists of a morphological English analyzer and a</definiens>
			</definition>
			<definition id="9">
				<sentence>INDEX : cut ENTRY : NP0 cut down ( NPI ) POS : NP0 V PL ( NPI ) FS : # pass+ DEF : consume less ; reduce .</sentence>
				<definiendum id="0">INDEX</definiendum>
				<definiens id="0">cut ENTRY : NP0 cut down ( NPI ) POS : NP0 V PL ( NPI ) FS : # pass+ DEF : consume less ; reduce</definiens>
			</definition>
			<definition id="10">
				<sentence>XTAG runs under Common Lisp and X Window ( CLX ) .</sentence>
				<definiendum id="0">XTAG</definiendum>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>The most important relation is , of course , the ISA one , which allows us to build a taxonomy of concepts related by the hypemym-hyponym links .</sentence>
				<definiendum id="0">ISA one</definiendum>
				<definiens id="0">allows us to build a taxonomy of concepts related by the hypemym-hyponym links</definiens>
			</definition>
			<definition id="1">
				<sentence>Vox is a monolingual Spanish dictionary containing about 90.000 entries ( around 150.000 senses ) .</sentence>
				<definiendum id="0">Vox</definiendum>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>Morph~ is a Common Lisp compiler for reversible inflectional morphology rules developed at the Center for Machine Translation at Carnegie Mellon University .</sentence>
				<definiendum id="0">Morph~</definiendum>
				<definiens id="0">a Common Lisp compiler for reversible inflectional morphology rules developed at the Center for Machine Translation at Carnegie Mellon University</definiens>
			</definition>
			<definition id="1">
				<sentence>To implement the first step , Morph~ uses a feature-based discrimination network with orthographically-based inflection rules at the leaves .</sentence>
				<definiendum id="0">Morph~</definiendum>
				<definiens id="0">uses a feature-based discrimination network with orthographically-based inflection rules at the leaves</definiens>
			</definition>
			<definition id="2">
				<sentence>The map operator invokes a string-to-string mapping on a bound portion of a word .</sentence>
				<definiendum id="0">map operator</definiendum>
				<definiens id="0">invokes a string-to-string mapping on a bound portion of a word</definiens>
			</definition>
			<definition id="3">
				<sentence>DIMoRPH : A Morphological Analyzer .</sentence>
				<definiendum id="0">DIMoRPH</definiendum>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>The useJ manipulates icons that symbolize components ( boxes ) data ports of components ( circles ) and data flow between ports ( arrows ) .</sentence>
				<definiendum id="0">useJ</definiendum>
				<definiens id="0">manipulates icons that symbolize components ( boxes ) data ports of components ( circles ) and data flow between ports ( arrows )</definiens>
			</definition>
			<definition id="1">
				<sentence>The Kernel is a complex component .</sentence>
				<definiendum id="0">Kernel</definiendum>
				<definiens id="0">a complex component</definiens>
			</definition>
			<definition id="2">
				<sentence>Figure 4 : The BLACK BOX Text Joyce consists of three separate modules , which perform distinct tasks and access their own knowledge bases ( Figure 5 ) .</sentence>
				<definiendum id="0">BLACK BOX Text Joyce</definiendum>
				<definiens id="0">consists of three separate modules , which perform distinct tasks and access their own knowledge bases ( Figure 5 )</definiens>
			</definition>
			<definition id="3">
				<sentence>Incrementality reduces the initial response time of the system ( though not the overall processing time ) .</sentence>
				<definiendum id="0">Incrementality</definiendum>
				<definiens id="0">reduces the initial response time of the system</definiens>
			</definition>
			<definition id="4">
				<sentence>The Locator is a data-base .</sentence>
				<definiendum id="0">Locator</definiendum>
				<definiens id="0">a data-base</definiens>
			</definition>
			<definition id="5">
				<sentence>Conjunction includes multiple conjunctions of more than one clause , and may lead to elision of repeated sentence elements ( `` conjunction reduction '' ) .</sentence>
				<definiendum id="0">Conjunction</definiendum>
				<definiens id="0">includes multiple conjunctions of more than one clause</definiens>
			</definition>
			<definition id="6">
				<sentence>The process of clause combination can be very easily modeled at the DSyntR level : relative clause formation and conjunction reduce to simple tree composition operations .</sentence>
				<definiendum id="0">DSyntR level</definiendum>
				<definiens id="0">relative clause formation and conjunction reduce to simple tree composition operations</definiens>
			</definition>
			<definition id="7">
				<sentence>• The Deep Morphological Representation ( DMorphR ) is a linearization of the nodes of the SSyntR .</sentence>
				<definiendum id="0">Deep Morphological Representation</definiendum>
				<definiens id="0">a linearization of the nodes of the SSyntR</definiens>
			</definition>
			<definition id="8">
				<sentence>Porting is an important way to evaluate complete applied text generation systems , since there is no canonical set of tasks that such a system must be able to perform and on which it can be tested .</sentence>
				<definiendum id="0">Porting</definiendum>
				<definiens id="0">an important way to evaluate complete applied text generation systems</definiens>
			</definition>
			<definition id="9">
				<sentence>WIP : The Coordinated Generation of Multimodal Presentations from a Common Representation .</sentence>
				<definiendum id="0">WIP</definiendum>
				<definiens id="0">The Coordinated Generation of Multimodal Presentations from a Common Representation</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>In this paper we describe DILEMMA-2 , a lemmatizertagger for medical abstracts , which is an updated version of DILEMMA-1 , a lemmatizer-tagger for general texts .</sentence>
				<definiendum id="0">DILEMMA-1</definiendum>
				<definiens id="0">an updated version of</definiens>
			</definition>
			<definition id="1">
				<sentence>Putting the missing scientific terms in the dictionary was not considered a good solution : this would have been against the basic principle of the DILEMMA concept , which was to keep the dictionary as small as possible ; in any case , it would have been a practically impossible task ( ESP has a database of more than 100.000 scientific terms ) .</sentence>
				<definiendum id="0">ESP</definiendum>
				<definiens id="0">has a database of more than 100.000 scientific terms )</definiens>
			</definition>
</paper>

		<paper id="1039">
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>The core of our approach is the extraction of elementary syntactic relationships ( ESR ) from a possibly ill-formed sentence .</sentence>
				<definiendum id="0">ESR</definiendum>
				<definiens id="0">the extraction of elementary syntactic relationships (</definiens>
			</definition>
			<definition id="1">
				<sentence>Currently , our knowledge base consists of a taxonomy of 3000 concepts , together with 360 semantic constraints for the conceptual relations .</sentence>
				<definiendum id="0">knowledge base</definiendum>
				<definiens id="0">consists of a taxonomy of 3000 concepts , together with 360 semantic constraints for the conceptual relations</definiens>
			</definition>
			<definition id="2">
				<sentence>Representation and Control Strategies for large Knowledge Domains : An Application to NLP .</sentence>
				<definiendum id="0">Representation</definiendum>
				<definiens id="0">An Application to NLP</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>In practice , it has also proved useful to include in the training data the database query expression ( for example , an SQL expression ) which was used to produce the reference answer : this often makes it possible for system developers to understand what was expected for a query , even if the answer is empty or otherwise limited in content .</sentence>
				<definiendum id="0">SQL expression</definiendum>
				<definiens id="0">empty or otherwise limited in content</definiens>
			</definition>
			<definition id="1">
				<sentence>A REF may be ambiguous , containing several sub expressions each of which is itself a REF : in this case , if HYP matches any of the answers in REF , the comparison succeeds .</sentence>
				<definiendum id="0">REF</definiendum>
				<definiendum id="1">REF</definiendum>
				<definiendum id="2">HYP</definiendum>
				<definiens id="0">matches any of the answers in REF</definiens>
			</definition>
			<definition id="2">
				<sentence>3 Another source of variation in the scoring metric is the fact that queries taken from different speakers can vary widely in terms of how easy it is for systems to understand and answer them correctly .</sentence>
				<definiendum id="0">metric</definiendum>
				<definiens id="0">the fact that queries taken from different speakers can vary widely in terms of how easy it is for systems to understand and answer them correctly</definiens>
			</definition>
			<definition id="3">
				<sentence>3Assuming there is some probability of error in each trial ( query ) , the variance in this error rate can be estimated using the formula where e is the error rate expressed as a decimal ( so 55 % error = better scores from the February 91 ATIS evaluation ) , and n -145 , differences in scores greater than 0.08 ( 8 % ) have a 95 % likelihood of being significant .</sentence>
				<definiendum id="0">e</definiendum>
				<definiens id="0">the error rate expressed as a decimal ( so 55 % error = better scores from the February 91 ATIS evaluation ) , and n -145 , differences in scores greater than 0.08</definiens>
			</definition>
			<definition id="4">
				<sentence>As with any black-box approach , it may give undue credit to a system that gets the right answer for the wrong reason ( i.e. , without really understanding the query ) , although this should be mitigated by using larger and more varied test 4The primary members of the original committee are : Lyn Bates ( BBN ) , Debbie Dahl ( UNISYS ) , Bill Fisher ( NIST ) , Lynette Hirschman ( M1T ) , Bob Moore ( SRI ) , and Rich Stern ( CMU ) .</sentence>
				<definiendum id="0">Bill Fisher</definiendum>
				<definiendum id="1">Bob Moore</definiendum>
				<definiens id="0">gets the right answer for the wrong reason ( i.e. , without really understanding the query</definiens>
			</definition>
			<definition id="5">
				<sentence>This differs from the MUC evaluations , where an answer template is a composite of many bits of information , and is scored along the dimensions of recall , precision , and overgeneration .</sentence>
				<definiendum id="0">answer template</definiendum>
				<definiens id="0">a composite of many bits of information , and is scored along the dimensions of recall , precision , and overgeneration</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>A OBJ B in publish-PASSIVE `` A is published in B. '' is the passive expression of `` osubj publishes A in B '' in which the subject has become a zero pronoun .</sentence>
				<definiendum id="0">OBJ B in publish-PASSIVE</definiendum>
				<definiens id="0">the passive expression of `` osubj publishes A in B '' in which the subject has become a zero pronoun</definiens>
			</definition>
			<definition id="1">
				<sentence>Of the two zero pronouns , in the former case , the `` shingata-koukanki '' ( new model switchboard ) , which is the object of the former sentence , and in the latter case , `` NTT '' , which is the subject of the former sentence become the referents .</sentence>
				<definiendum id="0">the two zero pronouns</definiendum>
				<definiens id="0">the object of the former sentence , and in the latter case , `` NTT '' , which is the subject of the former sentence become the referents</definiens>
			</definition>
			<definition id="2">
				<sentence>`` ha '' ( FOP/SUBJ ) , `` ga '' ( SUBJ ) `` wo '' ( OSJ ) Table 3 Constraints to Zero Pronouns and their referent with Connecting Words * The arrows go from the sentence which include referents to the sentence including the zero pronouns capable of correspondence .</sentence>
				<definiendum id="0">ha</definiendum>
				<definiens id="0">Constraints to Zero Pronouns and their referent with Connecting Words * The arrows go from the sentence which include referents to the sentence including the zero pronouns capable of correspondence</definiens>
			</definition>
			<definition id="3">
				<sentence>( in English ) `` This is the only paper that contains the news '' &lt; unit sentence `` This is the only paper '' , `` the only paper contains the news '' &lt; simple unit sentences ) modality , tense and aspect is extracted from the simple unit sentence to yield the objective simple unit sentence. This objective simple unit sentence , as shown in Figure 4 , is collated with two types of pattern dictionaries having predicates as index words ( the idiomatic expression transfer dictionary and the semantic valentz pattern transfer dictionary ) . When there is no appropriate pattern , a general pattern transfer rule is applied. This determines the syntactic and semantic structure pattern that is used in Japanese to English conversion. In the cases of ( 3 ) and ( 4 ) in Chapter 2 , ( 1 ) Morphological analysis : Separation of words , determination of words part of speech ( 2 ) Dependency analysis : -Determination of relations between sentence elements ( 3 ) J-J conversion : -Conversion of expressions within Japanese ( 4 ) Simple sentence extraction : -Determining the scope of influence of all predicates from dependency analysis results ( 5 ) Simple sentence analysis : ( 5.1 ) Predicate analysis : -Extraction of modality and other elements and conversion to an ordinary sentence ( 5.2 ) Gerund phrase analysis : -Determination of semantic structure of gerund phrases and compound words ( 6 ) Embedded sentence analysis : -Determination of the semantic structure of embedded sentences ( 7 ) Ordinary sentence conversion to English : -Conversion of objective expression by means of pattern dictionary ( 8 ) Connection analysis : -Determination of relations between declinable words ( 9 ) Optimal result selection : -The best ( semantically and syntactically most plausible ) interpretation is selected ( 10 ) Zero anaphora resolution : -Resolution of zero anaphora by use of contextual information ( 11 ) Resolved element conversion : -Determination of the conversion method for resolved zero anaphora ( 12 ) Unit sentence generation : ( 12.1 ) Basic structure generation : -Determination of the structure of the entire English sentence ( 12.2 ) Adverbial phrase generation : -Determination of adverbial phrase translation from modality , tense , verb and other elements 02.3 ) Noun phrase generation : -Conversion of phrase and compound word structures and embedding of embedded sentences ( 13 ) Connecting structure generation : -connection of the unit sentences according to connection attributes and the presence or absence of a subject ( 14 ) Modality tense structure generation : -Insertion of auxiliary verbs and infinitives , transformation of word model / syntactic structure ( 15 ) English sentence coordination : -Contraction , setting of determiner Figure 3 Process Outline of Japanese-English Machine Translation System , ALT-J/E 206 \ [ Example of Idiomatic Expressions\ ] ( 1 ) Example of idiomatic phrase pattern X ( Subject ) ha se ga takai = &gt; X be tall X TOP back SUB high ( 2 ) Example of functional verb combination X ( subject ) ha Y ( subject ) no h/nan wo abiru X TOP Y by criticism OBJ be-subjected-to `` X ( subject ) is subjected to criticism by Y '' ( - &gt; X is criticized by Y ) I Conversion within ( - &gt; Y criticizes X + passive ) IJapanese language ( = &gt; Y claim X ( +passive ) IApplication of Japanese to I English conversion pattern = &gt; X be claimed by Y. I Transformation of English \ [ Example of Semantic Combined Value Pattern\ ] X ( subject ) ga Y ( cultural , human activity ) wo anki-suru .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the simple unit sentence to yield the objective simple unit sentence. This objective simple unit sentence</definiens>
				<definiens id="1">the idiomatic expression transfer dictionary and the semantic valentz pattern transfer dictionary</definiens>
				<definiens id="2">cultural , human activity ) wo anki-suru</definiens>
			</definition>
</paper>

		<paper id="1043">
</paper>

		<paper id="1047">
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>Hypertext is a typical system to answer this problem , whose primary objective is to establish flexible associative links between relevant text parts and to allow users to select and trace links to see relevant text contents which are connected by links .</sentence>
				<definiendum id="0">Hypertext</definiendum>
				<definiens id="0">a typical system to answer this problem , whose primary objective is to establish flexible associative links between relevant text parts and to allow users to select and trace links to see relevant text contents which are connected by links</definiens>
			</definition>
			<definition id="1">
				<sentence>Typical sentential styles of intensional definition are : ( i ) A is defined as B. A is regarded as B. ( ii ) A means B. A connotes B. A is B. ( iii ) A is a { kind , form , way , branch , method , ... ) of B. ( iv ) A is regarded as B , so C as D. By identifying these patterns in a term description part , the relation between the defined word ( A ) and the definition sentences is established as : ( i ) p-link is set up from the defined word to the definition sentence when the defined word is not the headword of the term description .</sentence>
				<definiendum id="0">Typical sentential styles</definiendum>
				<definiendum id="1">B. ( ii ) A means B. A connotes B. A</definiendum>
				<definiens id="0">a { kind , form , way , branch , method , ... ) of B. ( iv ) A is regarded as B , so C as D. By identifying these patterns in a term description part , the relation between the defined word ( A ) and the definition sentences is established as : ( i ) p-link is set up from the defined word to the definition sentence when the defined word is not the headword of the term description</definiens>
			</definition>
			<definition id="2">
				<sentence>( i ) When W is a headword the term description of W is displayed .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">a headword the term description of W is displayed</definiens>
			</definition>
			<definition id="3">
				<sentence>( ii ) When W is an index word the term description which includes the explanation of the index word is displayed with the special mark of that explanation part .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">an index word the term description which includes the explanation of the index word is displayed with the special mark of that explanation part</definiens>
			</definition>
			<definition id="4">
				<sentence>( iii ) When W is a basic componential word of some compound words these words are shown to the user to select a proper compound word .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">a basic componential word of some compound words these words are shown to the user to select a proper compound word</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>Hypothesis-tuple T and T ' have all the same elements except one .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">the same elements except one</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>This trend has largely arisen from the recognition that access to a large scale Lexical Knowledge Base ( LKB ) is a sine qua non for real world applications of NLP systems .</sentence>
				<definiendum id="0">LKB )</definiendum>
			</definition>
			<definition id="1">
				<sentence>The LDB ( Carroll , 1990 ) gives flexible access to MRDs and is endowed with a graphic interface which provides a user-friendly environment for query formation and information retrieval .</sentence>
				<definiendum id="0">LDB</definiendum>
				<definiens id="0">gives flexible access to MRDs and is endowed with a graphic interface which provides a user-friendly environment for query formation and information retrieval</definiens>
			</definition>
			<definition id="2">
				<sentence>The LKB allows the user to define an inheritance network of types plus restrictions associated with them , and to create lexicons where such types are assigned to lexical templates extracted through LDB queries which give word-sense specific information .</sentence>
				<definiendum id="0">LKB</definiendum>
				<definiens id="0">allows the user to define an inheritance network of types plus restrictions associated with them , and to create lexicons where such types are assigned to lexical templates extracted through LDB queries which give word-sense specific information</definiens>
			</definition>
			<definition id="3">
				<sentence>The LLOCE is a thesaurus which was developed from LDOCE and there is substantial overlap ( although not identity ) between the definitions and entries of both MRDs .</sentence>
				<definiendum id="0">LLOCE</definiendum>
				<definiens id="0">a thesaurus which was developed from LDOCE and there is substantial overlap ( although not identity</definiens>
			</definition>
			<definition id="4">
				<sentence>Io¢o , = ~ ( colip &lt; ma ~l be//ec~e ; the .~¢~'tt. f/¢zL ) . I $ h4 felt h.w'mw/ ! , *~ be ~ , *e. ; T clexlcon Entr~l feel ( 4 ) , Id : FI Headwo/~d : feel ( 4 ) , id : FI Setld : Category : Set Header : Set Group : Set Main Header : ( F ) Index headword : Proneunolatlon : Possible Indices : FI v relating to feel Ing I v Feel Ing and b41houlour Feel ings , emotions , a~ fee I ( ''24/f 1 '' 5 '' 3 IZl/s % i : '' '5el Ifel t '' `` ~/felt/= ) ( felt ) Sublinse e4 : TSo I ( fig ) Index Homonyms : VERB I NIL I `` Ft '' Sense el : to think or ( ~onsider He says he feels that he has not been we eg : I fell that ~ do n't understand th oi Xeodword/Sense Number I dentlfler Category Score feel/3 feel/7 feel/4 feel/14 feel/l feel/I O feel/12 feell2 I VERB 40 % I VERB 40 % I VERB 39 % I VERB 39 % I VERB 38 % I VERB 38 % I VERB 38 % I VERB 37 % Source Entry ( feell4 it Threshold : 65 % ~tl el I , ! r , 'Ill FI VERB Tolerance : 7 % Source/Destlnatlon Dictionary : CLEXICON/LDOCE Figure 3 : Sample interaction with correlation dialog The correlation dialog allows the user to examine correlation structures and select none , one or more destinationdict senses to be matched with the source-dict sense under analysis. A typical interaction can be seen in Figure 3. A scrollable window in the centre of the dialog box provides information about the destination-dict senses and their associated scores. Single clicking the mouse button on one or more rows makes them the current selection. The large button above the threshold and tolerance indicators summarizes source-dict sense information. Clicking on this button invokes an LDB query window which inspects the source-dict sense ( cf. bottom left window in Figure 3 ) . The dialog can be in one of three modes : • Explain Scores -the mode specific key pops up a window for each destination-dict sense in the current selection , explaining how each score was obtained from the comparators ; • Display Entries -the mode specific key invokes standard LDB browsers on the destination-dict senses in the current selection ( cf. top-left window in Figure 3 ) , and • Accept Entries -the mode specific key terminates the dialog and accepts the current selection as the best match. Two additional buttons on the top right of the dialog box allow the current selection to be accepted independent of the current mode , or all senses to be rejected ( i.e. no match is found ) . At the bottom of the screen , two 'thermometers ' allow the user to adjust the threshold and tolerance parameters dynamically. We run DCK with LLOCE as source-dict and LDOCE as destination-dict to produce a derived dictionary , LDOCE_Link , which when loaded together with LDOCE would allow us to form LDOCE queries which integrated thesaurus information from LLOCE. The work was carried out with specific reference to verbs which express 'Feelings , Emotions , Attitudes , and Sensations ' and 'Movement , Location , Travel , and Transport ' ( sets 'F ' and 'M ' in LLOCE ) . Correlation structures were derived for 1194 verb senses ( over 1/5 of all verb senses in LLOCE ) using as matching parameters degree of overlap in grammar codes , definitions and examples , as well as equality in headword and part-of-speech. After some trim runs , correlations appeared to yield best results when all parameters were assigned the same weight except the comparator for 'degree of overlap in examples ' which was set to be twice as determinant than the others. Tolerance was set at 7 % and threshold at 65 % . The rate of interactions through the correlation dialog was about one for every 8-10 senses. It took about 10 hours running time on a Macintosh IIcx to complete the work , with less than three hours ' worth of interactions. A close examination of over 500 correlated entries disclosed an extremely low incidence of infelicitous matches ( below 1 % ) . In some cases , sense-matching inadequacies could be easily redressed without reassignment of correlation links. For example , DCK erroneously correlated the verb sense for float in LLOCE with the first verb sense of float in LDOCE. As shown in ( 4 ) , the LLOCE sense refers only to the intransitive use of the verb , while the LDOCE sense refers to both transitive and intransitive uses of the verb ( i.e. the LLOCE sense is subsumed 84 by LDOCE sense ) . ( 4 ) a LLOCE float\ [ I0\ ] to stay on or very near the surface of a liquid , esp. water b LDOCE float 2 v 1 \ [ 10 ; T1\ ] to ( cause to ) stay at the top of a liquid or be held up in the air without sinking water One way to redress this kind of inadequate match would be to augment DCK with a lexical rule module catering for diathesis alternations which made it possible to establish a clear relation between distinct syntactic realizations of the same verb. For example , the transitive and intransitive senses of float could be related to each other via the 'cansative/inchoative ' alternation. This augmentation would be easy to implement since information about amenability of verbs to diathesis alternations is recoverable from LDOCE_Inter , as shown below for float ( Ergative is the term used in LDOCE-Inter to characterize verbs which like float are amenable to the causative/inchoative alternation ) . ( 5 ) ( float ) ( I 2 ) ( 21 &lt; ( ( Cat Y ) ( Takes NP NP ) ( Type 2 Ergative ) ) ) ( 2 2 ... Notice , incidentally , that even though DCK yielded an incorrect sense correlation for the verb entry float , the information which was inherited by LDOCE from LLOCE through the correlation link was still valid. In LLOCE , float is classified as a verb whose set , group and main identifiers are : floating-and-sinking , Shipping and Movement-location-travel-and-transport. This information is useful in establishing the semantic class of both the transitive and intransitive uses of float. This is also true in those rare cases where DCK incorrectly preferred a sense match to another as shown below for the first LLOCE sense of behave which DCK linked to the third LDOCE sense rather than the first. Either sense of behave is adequately characterized by the set , group and main identifiers 'behaving ' , 'Feeling-andbehaviour-generally ' , and 'Feelings-emotions-attitudesand-sensations ' which LDOCE inherits from LLOCE through the incorrect sense correlation established by DCK. ( 6 ) a LLOCE behave 1 \ [ L9\ ] to do things , live , etc. usu in a stated way : She behaved with great courage when her husband died ... b LDOCE behave v 1 \ [ L9\ ] to act ; bear oneself : She behaved with great courage ... . 3 \ [ L9\ ] ( of things ) to act in a particular way : /t can behave either as an acid or as a salt ... c DCK Correlation LLOCE behave 1 = LDOCE behave 3 from Combined MRD Sources LDOCE_Link was derived as a list of entries consisting of correlated LLOCE-LDOCE sense pairs plus an explicit reference to the corresponding set identifier in LLOCE , as shown in ( 7 ) . ( 7 ) ( ( amaze ) ( LL F237 &lt; amaze &lt; 0 ) ) ( ( desire ) ( 2 I &lt; &lt; ) ( SN0 I ) ( LL F6 &lt; desire &lt; I ) ( SN0 2 ) ( LL F6 &lt; desire &lt; 2 ) ) Loading LDOCE with LDOCE_Link makes it possible to form LDOCE queries which include thesaurus information from LLOCE ( i.e. the set identifiers ) . The integration of thesaurus information provides adequate means for developing a semantic classification of verbs. With respect to psychological verbs , for example , the set identifiers proved to be very helpful in identifying members of the six subtypes described in ( 3 ) . The properties used in this classification could thus be used to define a hierarchy of thematic types in the LKB which gave a detailed characterization of argument roles. This is shown in the lattice fragment in Figure 4 where the underlined types correspond to the role types used to distinguish the six semantic varieties of psychological predicates. 3 The correspondence between LLOCE set identifiers and the thematic role types shown in Figure 4 made it possible to create word-sense templates for psychological verbs from LDB queries which in addition to providing information about morphological paradigm , subcategorization patterns , diathesis alternations and selectional restrictions , supplied thematic restrictions on the stimulus and experiencer roles. Illustrative LKB entries relative to the six verb subtypes described in ( 3 ) are shown in Figure 5. Taking into consideration the size of the LLOCE fragment correlated to LDOCE ( 1/5 of LLOCE verb senses ) and the results obtained , it seems reasonable to expect that this work should extend straightforwardly to other verbs as well as word senses of different category types. As far as we were able to establish , the major limitation of the work carried out arises from the fact that the entries and senses per homonyn in the source dictionary were considerably fewer than those in the destination dictionary ( e.g. 16,049 entries with 25,100 senses in LLOCE vs. 41,122 entries with 74,086 senses in LDOCE ) . Consequently , many senses of correlated verb entries as well as entire verb entries in LDOCE are bound to be left without a specification of thesaurus information. We are currently exploring the possibility of using verb taxonomies to extend the results of LLOCELDOCE correlations to those LDOCE entries and verb 3The labels 'p-agt ' and 'p-pat ' are abbreviations for 'proto-typica\ ] ' agent and patient roles which subsume clusters of entailments of verb meanings which qualify the most and least agentive event participants for each choice of predicate ( see footnote 2 ) . 85 th-alfected th-reacUve th-sentlent Is th-soume p-pat p-_agt \ .n~Oee-an~tive Figure 4 : LKB types for thematic roles of psychological verbs \ [ striot-tranxign ORTH : experlence CAT strlct-trans-cat SE~ : \ [ strtct-trans-sem I IND : , cO &gt; =eve PRED : and ARG11verl0-tormula | ARG2 : \ [ binary-forrnuia INO : &lt; 0 &gt; PRED : and ARG1 : &lt; 1 &gt; =\ [ pagt-formula PRED : p-agt-react Ive-e motive\ ] ARG2 : &lt; 2 &gt; = \ [ p-pat-formula PRED : p-pat-soume-no-cause\ ] \ ] \ ] \ ] \ [ , qrict-tran , ,-ilgn ORTH : admire CAT I strict-trans-cat I SICM : \ [ strlet-trans-sem IND : &lt; 0 &gt; = eve PRED : and ARG11verb-formula | ARG2 : \ [ blnary-lormula IND : , ~.0~ PRED : and ARG1 : &lt; 1 &gt; : \ [ pagt-formula PRED : p-agtpos-readivH mot Ive\ ] ARG2 : &lt; 2 &gt; \ [ p-pat-formula PRED : ppit-sou me-no-cause\ ] \ ] \ ] \ ] \ [ strict-tmne4ign ORTH : fear CAT strlct-trana-cat SEMI : \ [ strmt-tran ' , -sem \ ] IND : &lt; 0 &gt; =eve PRED : and ARG11verb-tormula I ARG2 : \ [ binary-formula IND : &lt; 0 &gt; PRED : and ARG1 : &lt; 1 &gt; = \ [ p-agt-formula PRED : p-agt-neg-reac , tlve-ernotive\ ] ARG2 : &lt; 2 &gt; = \ [ p-pat-formula PRED : ppat-sou me-no-caun\ ] \ ] \ ] \ ] \ [ strk : t-tranHIgn ORTH : lntereet CATlstrlct'trans'cat I SEM : \ [ etrtct-t fans,4JemIND : &lt; 0 &gt; = eve PRED : and ARG1 ARG2 : \ [ binary-formula IND : ~0 &gt; PRED : and ARG1 : &lt; 1 &gt; = \ [ pagt-formul , , PRED : pagt-cause\ ] ARG2 : &lt; 2 &gt; = ~-pat-lorrrmla PRED : ppat-affect ed-emotlve\ ] \ ] \ ] \ ] \ [ strict-trans-slgn ORTH : dellght CATlstrlct'trans'eat I SEM : \ [ stMct-trans-sem IND : &lt; 0 &gt; = eve PRED : and ARGI Iver~tormula I ARG2 : \ [ binary-formuls IND : &lt; 0~ , PRED : and ARG1 : &lt; 1 &gt; = \ [ IPIKit-lormull PRED : pagt-pos-cauee\ ] ARG2 : &lt; 2 &gt; = \ [ p-pat-formula PRED : iP Imt-pos-alfected-emotlve\ ] \ ] \ ] \ ] \ [ strict-t rans-slgn ORTH : scare CATIst rlct'trans'cat \ ] SEM ' : \ [ strlct-trans .</sentence>
				<definiendum id="0">Possible Indices</definiendum>
				<definiendum id="1">a~ fee I</definiendum>
				<definiendum id="2">Transport</definiendum>
				<definiendum id="3">Ergative</definiendum>
				<definiendum id="4">PRED</definiendum>
				<definiens id="0">CLEXICON/LDOCE Figure 3 : Sample interaction with correlation dialog The correlation dialog allows the user to examine correlation structures and select none , one or more destinationdict senses to be matched with the source-dict sense under analysis. A typical interaction can be seen in Figure 3. A scrollable window in the centre of the dialog box provides information about the destination-dict senses and their associated scores. Single clicking the mouse button on one or more rows makes them the current selection. The large button above the threshold</definiens>
				<definiens id="1">on the destination-dict senses in the current selection ( cf. top-left window in Figure 3 ) , and • Accept Entries -the mode specific key terminates the dialog and accepts the current selection as the best match. Two additional buttons on the top right of the dialog box allow the current selection to be accepted independent of the current mode , or all senses to be rejected ( i.e. no match is found ) . At the bottom of the screen , two 'thermometers ' allow the user to adjust the threshold and tolerance parameters dynamically. We run DCK with LLOCE as source-dict and LDOCE as destination-dict to produce a derived dictionary , LDOCE_Link , which when loaded together with LDOCE would allow us to form LDOCE queries which integrated thesaurus information</definiens>
				<definiens id="2">sets 'F ' and 'M ' in LLOCE ) . Correlation structures were derived for 1194 verb senses ( over 1/5 of all verb senses in LLOCE ) using as matching parameters degree of overlap in grammar codes , definitions and examples , as well as equality in headword and part-of-speech. After some trim runs , correlations appeared to yield best results when all parameters were assigned the same weight except the comparator for 'degree of overlap in examples ' which was set to be twice as determinant than the others. Tolerance was set at</definiens>
				<definiens id="3">both transitive and intransitive uses of the verb</definiens>
				<definiens id="4">the term used in LDOCE-Inter to characterize verbs which like float are amenable to the causative/inchoative alternation</definiens>
				<definiens id="5">] ( of things ) to act in a particular way : /t can behave either as an acid or as a salt ... c DCK Correlation LLOCE behave 1 = LDOCE behave 3 from Combined MRD Sources LDOCE_Link was derived as a list of entries consisting of correlated LLOCE-LDOCE sense pairs plus an explicit reference to the corresponding set identifier in LLOCE</definiens>
				<definiens id="6">psychological verbs from LDB queries which in addition to providing information about morphological paradigm , subcategorization patterns , diathesis alternations and selectional restrictions , supplied thematic restrictions on the stimulus and experiencer roles. Illustrative LKB entries relative to the six verb subtypes described in</definiens>
				<definiens id="7">1/5 of LLOCE verb senses</definiens>
				<definiens id="8">entries with 25,100 senses in LLOCE vs. 41,122 entries with 74,086 senses in LDOCE ) . Consequently , many senses of correlated verb entries as well as entire verb entries in LDOCE are bound to be left without a specification of thesaurus information. We are currently exploring the possibility of using</definiens>
			</definition>
			<definition id="5">
				<sentence>The Derivation of a Large Computational Lexicon for English from LDOCE .</sentence>
				<definiendum id="0">Derivation of</definiendum>
				<definiens id="0">a Large Computational Lexicon for English from LDOCE</definiens>
			</definition>
			<definition id="6">
				<sentence>Copestake , A. ( 1992 ) The ACQUILEX LKB : Representation Issues in Semi-Automatic Acquisition of Large Lexicons .</sentence>
				<definiendum id="0">ACQUILEX LKB</definiendum>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>111 Homophony is an important source of orthographical errors : words having the same pronunciation but a different spelling .</sentence>
				<definiendum id="0">Homophony</definiendum>
				<definiens id="0">an important source of orthographical errors : words having the same pronunciation but a different spelling</definiens>
			</definition>
			<definition id="1">
				<sentence>Examples are the forms gebeurt ( third person singular , present tense ) and gebeurd ( past participle ) of the verb gebeuren ; word ( first person , singular , present tense ) and wordt ( third person , singular , present tense ) of the verb worden ; and besteden ( infinitive and plural , present tense ) , besteedden ( plural , past tense ) , and bestede ( an adjective , derived from the past participle ) .</sentence>
				<definiendum id="0">bestede</definiendum>
				<definiens id="0">infinitive and plural , present tense ) , besteedden ( plural , past tense )</definiens>
				<definiens id="1">an adjective , derived from the past participle )</definiens>
			</definition>
			<definition id="2">
				<sentence>Before entering the sentence level ( i.e. , parsing a sentence ) , a spelling module should check on all the words in the sentence .</sentence>
				<definiendum id="0">sentence level</definiendum>
				<definiens id="0">parsing a sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>The dictionary itself is a computerised expanded version of the `` Hedendaags Nederlands '' ( `` Contemporary Dutch '' ) dictionary , published by Van Dale Lexicografie ( Utrecht ) , which was enriched with syntactic information from the CELEX database ( University of Nijmegen ) .</sentence>
				<definiendum id="0">CELEX database</definiendum>
				<definiens id="0">a computerised expanded version of the</definiens>
			</definition>
			<definition id="4">
				<sentence>3pruning the search space is almost obligatory , since trigram and triphone analysis require O ( n*m ) space , where n is the length of the word and m the number of entries in the dictionary .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>In practice , TWS consists of a number of application ( functionality ) modules which are integrated through the central MOTIF-based user interface module .</sentence>
				<definiendum id="0">TWS</definiendum>
				<definiens id="0">consists of a number of application ( functionality ) modules which are integrated through the central MOTIF-based user interface module</definiens>
			</definition>
			<definition id="1">
				<sentence>Serpent : A User Interface Management System .</sentence>
				<definiendum id="0">Serpent</definiendum>
				<definiens id="0">A User Interface Management System</definiens>
			</definition>
			<definition id="2">
				<sentence>Garnet : Comprehensive Support for Graphical , Highly-Interactive User Interfaces .</sentence>
				<definiendum id="0">Garnet</definiendum>
				<definiens id="0">Comprehensive Support for Graphical , Highly-Interactive User Interfaces</definiens>
			</definition>
			<definition id="3">
				<sentence>Luke : An Experiment in the Early Integration of Natural Language Processing .</sentence>
				<definiendum id="0">Luke</definiendum>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>• The constant CENTER denotes the center rectangle of a given picture .</sentence>
				<definiendum id="0">constant CENTER</definiendum>
				<definiens id="0">the center rectangle of a given picture</definiens>
			</definition>
			<definition id="1">
				<sentence>For given polygons P1 and P2 tile associative and commutative operator N , ( `` 1 : POLY X POLY ~ POLY computes the intersection polygon .</sentence>
				<definiendum id="0">POLY X POLY ~ POLY</definiendum>
				<definiens id="0">computes the intersection polygon</definiens>
			</definition>
			<definition id="2">
				<sentence>• The fimction PR ( Partial Rectangle ) , PR : CLOC x POLY ~ POLY , computes the rectangle corresponding to a given composite localisation and the rectangle partition of the picture induced by a given polygon .</sentence>
				<definiendum id="0">fimction PR</definiendum>
				<definiens id="0">computes the rectangle corresponding to a given composite localisation and the rectangle partition of the picture induced by a given polygon</definiens>
			</definition>
			<definition id="3">
				<sentence>R denotes the set of the real numbers .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">the set of the real numbers</definiens>
			</definition>
			<definition id="4">
				<sentence>Then the applicability degree Ac of a composite localisation can be defined as : Ac : CLOC × POLY X POLY ~ { } ~ Ac ( l , LO , REFO ) = w eval ( l , CG ( p ) , SR ( REFO ) ) with p = PR ( I , REFO ) fq LO f ( P ) W -- f ( LO ) 2In reality eval is slightly more complicated because it maps into ~ ' × ~ ' and not only into 3 .</sentence>
				<definiendum id="0">applicability degree Ac of a composite localisation</definiendum>
				<definiendum id="1">SR</definiendum>
				<definiendum id="2">REFO</definiendum>
				<definiens id="0">with p = PR ( I ,</definiens>
			</definition>
			<definition id="5">
				<sentence>The only difference results from tile previous computation of Mn : tile subexpression p = PR ( ( x , y ) , REFO ) M LO is factored from A¢ and therefore computed only once .</sentence>
				<definiendum id="0">REFO ) M LO</definiendum>
				<definiens id="0">tile subexpression p = PR ( ( x , y ) ,</definiens>
			</definition>
			<definition id="6">
				<sentence>WIP : The Coordinated Generation of Multimodal Presentations from a Common Representation .</sentence>
				<definiendum id="0">WIP</definiendum>
				<definiens id="0">The Coordinated Generation of Multimodal Presentations from a Common Representation</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>parameters specified in the template ( e.g. length of the text ) , and if the result does not meet the requirements of the user , re-activates the previous processors with adjusted parameters for providing alternative solutions .</sentence>
				<definiendum id="0">parameters</definiendum>
				<definiens id="0">specified in the template ( e.g. length of the text )</definiens>
			</definition>
			<definition id="1">
				<sentence>The template consists of two types of statements : statements defining the modal structure of the document and content production statements .</sentence>
				<definiendum id="0">template</definiendum>
				<definiens id="0">consists of two types of statements : statements defining the modal structure of the document and content production statements</definiens>
			</definition>
			<definition id="2">
				<sentence>The weather model consists of the set of weather attributes , their domains , relations between some domains , and rules for calculation of derived attributes .</sentence>
				<definiendum id="0">weather model</definiendum>
			</definition>
			<definition id="3">
				<sentence>The territory model represents the set of regions , their carriers and certain logical links between them .</sentence>
				<definiendum id="0">territory model</definiendum>
				<definiens id="0">the set of regions , their carriers and certain logical links between them</definiens>
			</definition>
			<definition id="4">
				<sentence>The predicate path ( rl , r2 , ... , r , ~ ) indicates that there is a path starting from region rl , passing through r2 , ... , r , ~_l , and reaching rn .</sentence>
				<definiendum id="0">predicate path</definiendum>
				<definiens id="0">a path starting from region rl , passing through r2 , ... , r</definiens>
			</definition>
			<definition id="5">
				<sentence>Two relations between time periods supported by the TKB are partial order ( tt &lt; t2 iff end ( tl ) ~_ begin ( t2 ) ) and inclusion ( tl C t2 iff \ [ begin ( tl ) , end ( t1 ) \ ] C \ [ begin ( t2 ) , end ( t 2 ) \ ] ) The relations between weather values , regions and time periods are employed in the selection of rhetorical schemas ( cf. section 6 ) .</sentence>
				<definiendum id="0">inclusion</definiendum>
				<definiens id="0">weather values , regions and time periods are employed in the selection of rhetorical schemas ( cf. section 6 )</definiens>
			</definition>
			<definition id="6">
				<sentence>The monitor calls it with two types of queries specifying the goal ( a single weather attribute ) , the context and a parameter concerning either the precision rate or the maximum number of assertions to be produced : scanp ( clouds , Bul , whole day , 0.8 ) scana ( clouds , Bul , whole_day , 3 ) The first query makes the scanner extract assertions about the clouds attribute applied to Bulgaria and the whole day , and with a precision rate greater than or equal to 0.8 .</sentence>
				<definiendum id="0">) scana</definiendum>
				<definiens id="0">a single weather attribute ) , the context and a parameter concerning either the precision rate or the maximum number of assertions to be produced : scanp ( clouds , Bul , whole day , 0.8</definiens>
			</definition>
			<definition id="7">
				<sentence>The map generator interprets the map plan and con verts it into an image by rendering the successive state ments ( cf. section 6.3 ) .</sentence>
				<definiendum id="0">map generator</definiendum>
			</definition>
			<definition id="8">
				<sentence>XTRA : A Natural-Language Access System to Expert Systems .</sentence>
				<definiendum id="0">XTRA</definiendum>
			</definition>
			<definition id="9">
				<sentence>Natural Language and Exploration of an Information Space : The AlFresco Interactive System .</sentence>
				<definiendum id="0">Exploration of an Information Space</definiendum>
				<definiens id="0">The AlFresco Interactive System</definiens>
			</definition>
</paper>

		<paper id="1036">
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>The Sundial dialogue manager consists of five modules .</sentence>
				<definiendum id="0">Sundial dialogue manager</definiendum>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>Datenbank-DIALOG is a German language interface to relational databases .</sentence>
				<definiendum id="0">Datenbank-DIALOG</definiendum>
				<definiens id="0">a German language interface to relational databases</definiens>
			</definition>
			<definition id="1">
				<sentence>Comt ) arisons involve a relation between values associated with a dimension and units of measure .</sentence>
				<definiendum id="0">Comt ) arisons</definiendum>
				<definiens id="0">involve a relation between values associated with a dimension and units of measure</definiens>
			</definition>
			<definition id="2">
				<sentence>Datenbank-DIALOG uses a compositional semantics and separates the lexical item from the underlying semantic relation , which may be shared by different words .</sentence>
				<definiendum id="0">Datenbank-DIALOG</definiendum>
				<definiens id="0">uses a compositional semantics and separates the lexical item from the underlying semantic relation</definiens>
			</definition>
			<definition id="3">
				<sentence>DatenbankDIALOG recognizes the different interpretations by the semantic type associated with the value of the phrase to be cornpared .</sentence>
				<definiendum id="0">DatenbankDIALOG</definiendum>
			</definition>
			<definition id="4">
				<sentence>Datenbank-DIALOG splits the interpretation of an utterance into two stages : An interpretation in the domain model , i.e. a caseframe , which is then mapped ( using a translation table ) to an interpretation in the database lnodel , i.e. a DB-caseframe .</sentence>
				<definiendum id="0">Datenbank-DIALOG</definiendum>
				<definiens id="0">splits the interpretation of an utterance into two stages : An interpretation in the domain model</definiens>
			</definition>
			<definition id="5">
				<sentence>DatenbankDIALOG solves this problem -- by defining a normalized form azsociating values with units and transformation rules between measures of different units -- at the scanner level ( l ) atterns , e.g. ( late formats ) , at the parser level ( linguistic information to fill the slots in the normalized value frame ) , at the interpretation level ( procedures to transform constant values from one unit to another ) , at the database level ( transformation fimctions of the query language ) .</sentence>
				<definiendum id="0">DatenbankDIALOG</definiendum>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>\ [ Allen et al. , 1989\ ] describe their own architecture which concentrates on representations for subdialog mechanisms and their interactiom , with sentence level processing .</sentence>
				<definiendum id="0">own architecture</definiendum>
				<definiens id="0">concentrates on representations for subdialog mechanisms and their interactiom , with sentence level processing</definiens>
			</definition>
			<definition id="1">
				<sentence>The algorithm offers three choices depending on whether R is trivial ( a single predicate which is not vocalize ( X ) ) , tt is vocalize ( X ) , or R is a rule with antecedents as is the case here .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">trivial ( a single predicate which is not vocalize ( X ) ) , tt is vocalize ( X ) , or</definiens>
			</definition>
			<definition id="2">
				<sentence>Foremost , the theorem prover is able to suspend itself when it encounters a missing axiom , permitting natural language interaction to assist in axiom acquisition .</sentence>
				<definiendum id="0">Foremost</definiendum>
				<definiens id="0">permitting natural language interaction to assist in axiom acquisition</definiens>
			</definition>
			<definition id="3">
				<sentence>Particular points to notice from this usercontrolled dialog are : 13 Dialog 1 Declarative Mode ( C denotes the computer , U the user , and R the recognizer ) S C : 6 7 8 9 10 .</sentence>
				<definiendum id="0">Declarative Mode ( C</definiendum>
				<definiens id="0">the computer</definiens>
			</definition>
			<definition id="4">
				<sentence>( R : the same same ) Did you mean to say the LED is displaying the same thing ?</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">the same same</definiens>
			</definition>
			<definition id="5">
				<sentence>1Quintus Prolog is a trademark of Quintus Computer Systems , Incorporated 2DECtalk is a trademark of Digital Equipment Corporation .</sentence>
				<definiendum id="0">Prolog</definiendum>
				<definiendum id="1">Incorporated 2DECtalk</definiendum>
				<definiens id="0">a trademark of Quintus Computer Systems ,</definiens>
			</definition>
			<definition id="6">
				<sentence>KNOME : Modeling what the user knows in UC .</sentence>
				<definiendum id="0">KNOME</definiendum>
				<definiens id="0">Modeling what the user knows in UC</definiens>
			</definition>
			<definition id="7">
				<sentence>The Hearsay-II speechunderstanding system : Integrating knowledge to resolve uncertainty .</sentence>
				<definiendum id="0">Hearsay-II speechunderstanding system</definiendum>
				<definiens id="0">Integrating knowledge to resolve uncertainty</definiens>
			</definition>
			<definition id="8">
				<sentence>KING : a knowledgeintensive natural language generator .</sentence>
				<definiendum id="0">KING</definiendum>
				<definiens id="0">a knowledgeintensive natural language generator</definiens>
			</definition>
			<definition id="9">
				<sentence>Learning the user 's language : A step towards automated creation of user models .</sentence>
				<definiendum id="0">Learning</definiendum>
				<definiens id="0">the user 's language : A step towards automated creation of user models</definiens>
			</definition>
			<definition id="10">
				<sentence>Control of Mixed-Initiatiw Discourse Through Meta-Locutionary Acts : A Corn putational Model .</sentence>
				<definiendum id="0">Mixed-Initiatiw Discourse Through Meta-Locutionary Acts</definiendum>
			</definition>
</paper>

		<paper id="1041">
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>For example , Interjectionl includes the phrase byebye for now , the adverb however , the noun phrase my goodness , and the verb smile , as in I APPRECIATE THE I-lFff .</sentence>
				<definiendum id="0">Interjectionl</definiendum>
				<definiens id="0">includes the phrase byebye for now</definiens>
			</definition>
			<definition id="1">
				<sentence>with the TDD input characters as they are being typed The output of the canonical filters consists of TDD wor , tokens i.e. groups of characters separated by whit spaces .</sentence>
				<definiendum id="0">TDD input characters</definiendum>
				<definiens id="0">they are being typed The output of the canonical filters consists of TDD wor , tokens i.e. groups of characters separated by whit spaces</definiens>
			</definition>
			<definition id="2">
				<sentence>The phrasal segmentation process applies a modest set of disambiguation and phrasing roles to a sliding window containing three contiguous tdd-term structures .</sentence>
				<definiendum id="0">phrasal segmentation process</definiendum>
				<definiens id="0">applies a modest set of disambiguation and phrasing roles to a sliding window containing three contiguous tdd-term structures</definiens>
			</definition>
			<definition id="3">
				<sentence>TDD : I DONT THINK SO I WILL THINK ABOUT IT GA T/S : I do n't think so I will think about it .</sentence>
				<definiendum id="0">TDD</definiendum>
				<definiens id="0">I DONT THINK SO I WILL THINK ABOUT IT GA T/S : I do n't think so I will think about it</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>Text categorization is an excellent application domain for natural language processing systems .</sentence>
				<definiendum id="0">Text categorization</definiendum>
				<definiens id="0">an excellent application domain for natural language processing systems</definiens>
			</definition>
			<definition id="1">
				<sentence>Precision is the percentage of system-assigned categories that also appeared in the human indices .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the percentage of system-assigned categories that also appeared in the human indices</definiens>
			</definition>
			<definition id="2">
				<sentence>179 Industry Segments advertising electronics photography aerospace entertainment plastics agriculture environmental + services ' precious + metals autos financial + services publishing aviation food railroads banking forestry + products beverages freight real + estate biotechnology health + care restaurants broadcasting industrial + products retail building + material insurance rubber business + services machinery ship building chemicals metals telecommunications computers mining textiles construction nuclear + energy tobacco consumer + products office + equipment toys defense + contracting personal + care + products travel services educational + services petroleum + products electronic + publishing pharmaceuticals trucks utilities Subject Indicators air + force depression money antitrust divestiture nasd halt appointment dividend nasd resume bankruptcy earnings navy boycott economy new product budget election business executive change news cabinet expansion newsbrief capitol export prime + rate career government public offering chg-naq import recession commodity inflation refinancing congress insider + trading resignation contract joint venture restructuring corporate labor socialism coup lawsuit crime layoff space debt legislation strike deficit market taxes democracy merger trade military unemployment Figure 2 : Keywords for Industry and Topic Segments Categorization Task Topic assignment ( NL ) Topic assignment ( Stats ) Topic assic~nment ( Stats + NL / Industry assignment ( NL ) Industry assignment ( Stats } Industry assignment ( Stats + NL ) Recall .94 .73 .95 .34 .64 .67 Precision .61 .79 .65 .18 .50 .46 All categories ( NL ) .74 All categories ( Stats + NL ) .79 .64 Figure 3 : Overall Results For example , in the news categorization task described earlier , natural language delivered the weakest performance relative to statistics on the assignment of industry categories .</sentence>
				<definiendum id="0">Industry Segments advertising electronics photography aerospace entertainment plastics</definiendum>
				<definiens id="0">products office + equipment toys defense + contracting personal + care + products travel services educational + services petroleum + products electronic +</definiens>
				<definiens id="1">Keywords for Industry and Topic Segments Categorization Task Topic assignment ( NL ) Topic assignment ( Stats ) Topic assic~nment ( Stats + NL / Industry assignment ( NL ) Industry assignment</definiens>
				<definiens id="2">Overall Results For example , in the news categorization task described earlier , natural language delivered the weakest performance relative to statistics on the assignment of industry categories</definiens>
			</definition>
			<definition id="3">
				<sentence>Phrases like X is a leading marketer of Y or X is the maker of Y appear throughout news stories , and are sure indicators of industry information , even though they do not point to any particular industry .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">the maker of Y appear throughout news stories , and are sure indicators of industry information</definiens>
			</definition>
			<definition id="4">
				<sentence>Linguistic approaches probably wo n't help to guess that Love My Carpet is a consumer product , but they can help to determine that the industry discriminators lie in the text following patterns such as X is the maker of Y. Statistics can then guess the industries associated with Y. The NLP method used in NLDB associates categories with linguistic patterns .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">a consumer product , but they can help to determine that the industry discriminators lie in the text following patterns such as</definiens>
				<definiens id="1">the maker of Y. Statistics can then guess the industries associated with Y. The NLP method used in NLDB associates categories with linguistic patterns</definiens>
			</definition>
			<definition id="5">
				<sentence>A similar case is the word Yankee which is an excellent indicator of NUCLEAR ENERGY ( because of the New Hampshire Yankee Power Plant ) , except in an isolated cluster of articles about George Steinbrenner , the owner of the New York Yankees .</sentence>
				<definiendum id="0">NUCLEAR ENERGY</definiendum>
				<definiens id="0">the word Yankee which is an excellent indicator of</definiens>
			</definition>
			<definition id="6">
				<sentence>As evidence of the problem with proper names , the words Flint ( a city in Michigan where General Motors produces cars and trucks ) , Donahue ( the name of a popular daytime TV show ) , and Warner ( a communications conglomerate ) are all good statistical indicators .</sentence>
				<definiendum id="0">General Motors</definiendum>
				<definiendum id="1">Donahue</definiendum>
				<definiendum id="2">Warner (</definiendum>
				<definiens id="0">produces cars and trucks</definiens>
				<definiens id="1">the name of a popular daytime TV show</definiens>
			</definition>
			<definition id="7">
				<sentence>Television is a weak indicator of several industry categories , but television viewers and television networks are strong indicators .</sentence>
				<definiendum id="0">Television</definiendum>
				<definiens id="0">a weak indicator of several industry categories , but television viewers and television networks are strong indicators</definiens>
			</definition>
			<definition id="8">
				<sentence>SCISOR : Extracting information from on-line news .</sentence>
				<definiendum id="0">SCISOR</definiendum>
				<definiens id="0">Extracting information from on-line news</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>Sparser includes a semantically-driven search mechanism that scans the forest for compatible phrases whenever a syntactically incomplete or unattached phrase is left after conventional rules have been applied .</sentence>
				<definiendum id="0">Sparser</definiendum>
				<definiens id="0">includes a semantically-driven search mechanism that scans the forest for compatible phrases whenever a syntactically incomplete or unattached phrase is left after conventional rules have been applied</definiens>
			</definition>
			<definition id="1">
				<sentence>Sparser includes a set of function word-driven phrase boundary rules .</sentence>
				<definiendum id="0">Sparser</definiendum>
				<definiens id="0">includes a set of function word-driven phrase boundary rules</definiens>
			</definition>
			<definition id="2">
				<sentence>DD 02/14/91 SO WALL STREET JOURNAL ( J ) , PAGE B8 CO * GTWNEWS IN PIPELINE OPERATORS ( PIP ) PETROLEUM ( PET ) AUTO PARTS AND EQUIPMENT INCLUDING Note that this is now `` semantics '' in the sense of finding the denotation of a formula ( English phrase ) in some model , not in the sense of the choice of labels in a `` semantic grammar '' .</sentence>
				<definiendum id="0">PAGE B8 CO * GTWNEWS IN PIPELINE OPERATORS</definiendum>
				<definiens id="0">a formula ( English phrase ) in some model</definiens>
			</definition>
			<definition id="3">
				<sentence>It looks through all legal values of three indices , 0 _ &lt; i &lt; j &lt; k _ &lt; n ( where n is the length of the input text ) , to determine whether two adjacent candidate daughters , one spanning the text from index i to index j and the other from j to k , can be combined to form a new node from i to k. This algorithm takes only a few lines to write , but since it is driven by the space of index values , it necessarily requires On3 time to complete its search , along with potentially n2/2 storage cells to record its intermediate results .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the input text ) , to determine whether two adjacent candidate daughters</definiens>
			</definition>
			<definition id="4">
				<sentence>With common grammars ( bounded state ) , Earley 's algorithm runs in order of n time , but at a cost in storage potentially as great as 62 , where G is the number of rules in the grammar .</sentence>
				<definiendum id="0">common grammars</definiendum>
				<definiendum id="1">G</definiendum>
				<definiens id="0">the number of rules in the grammar</definiens>
			</definition>
			<definition id="5">
				<sentence>All of the edges in its chart are what would be called `` inactive '' edges in the above approaches -- they all represent actual constituents in the text rather hypothesized ones .</sentence>
				<definiendum id="0">inactive</definiendum>
				<definiens id="0">'' edges in the above approaches -- they all represent actual constituents in the text rather hypothesized ones</definiens>
			</definition>
			<definition id="6">
				<sentence>Sparser addresses the spurious edge problem by drastically restricting its search space of adjacent edges .</sentence>
				<definiendum id="0">Sparser</definiendum>
				<definiens id="0">addresses the spurious edge problem by drastically restricting its search space of adjacent edges</definiens>
			</definition>
			<definition id="7">
				<sentence>Tokens are minimal sequences of the same character type , e.g. the sequence `` $ 43.3 million '' is seen as six tokens : `` $ '' , `` 43 '' , `` . '</sentence>
				<definiendum id="0">Tokens</definiendum>
				<definiens id="0">minimal sequences of the same character type</definiens>
			</definition>
			<definition id="8">
				<sentence>The Scan operation will make the array wrap around and write over earlier position objects as needed when the length of the text exceeds the length of the array .</sentence>
				<definiendum id="0">Scan operation</definiendum>
				<definiens id="0">make the array wrap around and write over earlier position objects as needed when the length of the text exceeds the length of the array</definiens>
			</definition>
			<definition id="9">
				<sentence>Edge-vectors link positions to edges .</sentence>
				<definiendum id="0">Edge-vectors link</definiendum>
			</definition>
			<definition id="10">
				<sentence>The phrase structure grammar consists of a set of rewrite rules .</sentence>
				<definiendum id="0">phrase structure grammar</definiendum>
				<definiens id="0">consists of a set of rewrite rules</definiens>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>Recent approaches to intelligent information retrieval have used natural language ( NL ) understanding methods instead of keywords and statistical methods .</sentence>
				<definiendum id="0">natural language</definiendum>
				<definiendum id="1">NL</definiendum>
				<definiens id="0">) understanding methods instead of keywords and statistical methods</definiens>
			</definition>
			<definition id="1">
				<sentence>We believe that the restricted use of natural language in captions for multimedia data retrieval is a less difficult task than full natural language fact retrieval .</sentence>
				<definiendum id="0">retrieval</definiendum>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>Basque is an agglutinative language ; that is , for the formation of words the dictionary entry independently takes each of the elements necessary for the different functions ( syntactic case included ) .</sentence>
				<definiendum id="0">Basque</definiendum>
				<definiens id="0">an agglutinative language</definiens>
			</definition>
			<definition id="1">
				<sentence>The entry code gives , when appropriate , information on part of speech , determination , number , declension case , gender ( exceptional cases ) , relation ( of subordination ) , part of speech transformation that a relational affix produces , type of verb , root of finite verb forms , tense-mood , grammatical person , etc. along with the specific information each entry requires .</sentence>
				<definiendum id="0">entry code</definiendum>
				<definiens id="0">gives , when appropriate , information on part of speech , determination , number , declension case , gender ( exceptional cases ) , relation ( of subordination ) , part of speech transformation that a relational affix produces , type of verb , root of finite verb forms , tense-mood , grammatical person , etc. along with the specific information each entry requires</definiens>
			</definition>
			<definition id="2">
				<sentence>( DAT13 ( LAT ) ) ) is the way to express that ha ( nominative , 2nd person ) is to be combined with dative morphemes of 1st and 3rd person but not with those of 2nd .</sentence>
				<definiendum id="0">DAT13</definiendum>
			</definition>
			<definition id="3">
				<sentence>Selection marks depending on knowledge contained in the database such as part of speech , subcategorization of nouns , special endings for certain categories , etc. may be automatically derived from the information in the base .</sentence>
				<definiendum id="0">Selection marks</definiendum>
				<definiens id="0">depending on knowledge contained in the database such as part of speech , subcategorization of nouns , special endings for certain categories</definiens>
			</definition>
			<definition id="4">
				<sentence>Karttunen , L. KIMMO : A two-level Morphological Analyzer .</sentence>
				<definiendum id="0">Karttunen , L. KIMMO</definiendum>
			</definition>
			<definition id="5">
				<sentence>Two-level Morphology : A general Computational Model for Word-Form Recognition and Production , University of Helsinki , Department of General Linguistics .</sentence>
				<definiendum id="0">Two-level Morphology</definiendum>
				<definiens id="0">A general Computational Model for Word-Form Recognition</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>The BCP package consists of four submodules : preprocesser , morphology , alignment , and access .</sentence>
				<definiendum id="0">BCP package</definiendum>
				<definiens id="0">consists of four submodules : preprocesser , morphology , alignment , and access</definiens>
			</definition>
			<definition id="1">
				<sentence>The alignment module is the subject of much experimentation and currently is running with the Church-Gale alignment algorithm \ [ Gale and Church , 1991\ ] .</sentence>
				<definiendum id="0">alignment module</definiendum>
				<definiens id="0">the subject of much experimentation and currently is running with the Church-Gale alignment algorithm \ [</definiens>
			</definition>
			<definition id="2">
				<sentence>The Gale-Church algorithm relies on the length of regions , where the character is the unit of measurement .</sentence>
				<definiendum id="0">Gale-Church algorithm</definiendum>
				<definiens id="0">relies on the length of regions , where the character is the unit of measurement</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>In ( Smadja , 1989 ) , ( Zernik and Jacobs , 1990 ) , the associations are filtered by selecting the word pairs ( x , y ) whose frequency of occurrence is above f+ks , where f is the average appearance , s is the standard deviation , and k is an empirically determined factor .</sentence>
				<definiendum id="0">f</definiendum>
				<definiendum id="1">k</definiendum>
				<definiens id="0">an empirically determined factor</definiens>
			</definition>
			<definition id="1">
				<sentence>Smadja attempts to apply syntactic information to a set of automatically collected collocations ( statistics-first ) .</sentence>
				<definiendum id="0">Smadja</definiendum>
				<definiens id="0">attempts to apply syntactic information to a set of automatically collected collocations ( statistics-first )</definiens>
			</definition>
			<definition id="2">
				<sentence>In short , the parser consists of a segmentation algorithm to cut texts into phrases ( NP , PP , VP etc ) , and a phrase parser that is able to detect the following 15 links : N_V , V_N , N_ADJ , N N , N_prep_N , V_prep_N , N_prep_V , V_prep_V , N_cong_N , ADJ_cong_ADJ , V_ADV , ADV_cong_ADV , V_cong V , N_prep_ADJ , V_prep_ADJ .</sentence>
				<definiendum id="0">ADV_cong_ADV</definiendum>
				<definiens id="0">consists of a segmentation algorithm to cut texts into phrases ( NP , PP , VP etc ) , and a phrase parser that is able to detect the following 15 links : N_V , V_N , N_ADJ , N N , N_prep_N , V_prep_N , N_prep_V , V_prep_V , N_cong_N , ADJ_cong_ADJ , V_ADV ,</definiens>
			</definition>
			<definition id="3">
				<sentence>Domain-dependent tags render the classification task more simple and ensure that the clustered association data are appropriate given the application .</sentence>
				<definiendum id="0">Domain-dependent tags</definiendum>
				<definiens id="0">render the classification task more simple and ensure that the clustered association data are appropriate given the application</definiens>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>These include the availability of efficient and high precision word segmentation methods for Chinese text \ [ Chang et al. , 1991 ; Sproat and Shih , 1990 ; Wang et al. , 1990\ ] , the availability of statistical analysis of a Chinese corpus \ [ Liu et al. , 1975\ ] and large-scale electronic Chinese dictionaries with partof-speech information \ [ Chang et al. , 1988 ; BDC , 1992\ ] , the corpus-based statistical part-of-speech tagger \ [ Church , 1988 ; DeRose , 1988 ; Beale , 1988\ ] , as well as phrasal and clausal analyzers \ [ Church 1988 ; Ejerhed 1990\ ] As being pointed out in \ [ Salton , 1988\ ] , back-of-book indexes may consist of more than one word that are derived from a noun phrase .</sentence>
				<definiendum id="0">BDC</definiendum>
				<definiens id="0">partof-speech information \ [ Chang et al. , 1988 ;</definiens>
				<definiens id="1">back-of-book indexes may consist of more than one word that are derived from a noun phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>The morphological analysis for Chinese text is mainly a so-called word segmentation process , which segments a sequence of Chinese character into a sequence of words .</sentence>
				<definiendum id="0">segmentation process</definiendum>
				<definiens id="0">segments a sequence of Chinese character into a sequence of words</definiens>
			</definition>
			<definition id="2">
				<sentence>The CSP with binary constraints can be defined as follows : Given a set of n variables XI , X2 ... . , Xn and a set of binary constraints Kij , find all possible n-tuples ( Xl , x2 ... .. Xn ) such that each n-tuple is an instantiation of the n variables satisfying ( ~ , x\ ] ) in Kij , for all Kij Segmentation through Constraint Satisfaction The word segmentation problem for Chinese can be simply stated as follows : Given a Chinese sentence , segment the sentence into words .</sentence>
				<definiendum id="0">CSP with binary constraints</definiendum>
				<definiens id="0">follows : Given a set of n variables XI , X2 ... . , Xn and a set of binary constraints Kij , find all possible n-tuples ( Xl , x2 ... .. Xn ) such that each n-tuple is an instantiation of the n variables satisfying ( ~ , x\ ] ) in Kij , for all Kij Segmentation through Constraint Satisfaction The word segmentation problem for Chinese can be simply stated as follows : Given a Chinese sentence , segment the sentence into words</definiens>
			</definition>
			<definition id="3">
				<sentence>where ~q ( Liu ) is a surname and ~ ( Xian-Zhong ) is a last name .</sentence>
				<definiendum id="0">~q</definiendum>
				<definiens id="0">a surname</definiens>
			</definition>
			<definition id="4">
				<sentence>The statistics used in the system consists of four parts : ( S1 ) Appearance counts of 40,032 distinct words from a corpus of 1,000,000 words of Chinese text \ [ Liu el al. , 1975\ ] .</sentence>
				<definiendum id="0">statistics</definiendum>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>We shall show how our approach to multilevel semantics is concretely applied to organize the semantic component developed by the NLP group at IRST ; this component is currently responsible for semantic analysis in two dialog systems , ALFresco and MAIA .</sentence>
				<definiendum id="0">ALFresco</definiendum>
				<definiens id="0">concretely applied to organize the semantic component developed by the NLP group at IRST</definiens>
			</definition>
			<definition id="1">
				<sentence>Three modules interact at this level : the quantification module , which finds the correct interpretation of the quantifiers , resolving possible scoping ambiguities ; the topic module , which organizes the mentioned referents ; the interpretation module , which identifies the part of the sentence to extensionalize and is responsible for referent retrieval .</sentence>
				<definiendum id="0">quantification module</definiendum>
				<definiendum id="1">topic module</definiendum>
				<definiens id="0">organizes the mentioned referents</definiens>
				<definiens id="1">identifies the part of the sentence to extensionalize</definiens>
			</definition>
			<definition id="2">
				<sentence>The domain independent level , called Functional-Semantic Structure ( FSS ) , is intended as an intermediate structure that incorporates linguistic knowledge , substantially invariant in respect to the particular application domain .</sentence>
				<definiendum id="0">Functional-Semantic Structure</definiendum>
				<definiendum id="1">FSS</definiendum>
				<definiens id="0">an intermediate structure that incorporates linguistic knowledge</definiens>
			</definition>
			<definition id="3">
				<sentence>In XTRA it is necessary that each analysis produced by the parser is consistent with the FSS level : this is achieved by means of a classification of the sentence instance with the SB-ONE mechanisms ( the realizer and the matcher ) .</sentence>
				<definiendum id="0">FSS level</definiendum>
				<definiens id="0">the realizer and the matcher )</definiens>
			</definition>
			<definition id="4">
				<sentence>As we have already mentioned , the semantic component consists of two levels and each level includes one or more specialized modules .</sentence>
				<definiendum id="0">semantic component</definiendum>
				<definiens id="0">consists of two levels and each level includes one or more specialized modules</definiens>
			</definition>
			<definition id="5">
				<sentence>i ' ( Quantification ) Abox module ~'~ J Q Interpretation m o , e ) -v i ty Oomain ... . Model Topic module 3 Figure 1 : Sketch of the semantic process The discrimination module uses semantic information from two different sources : lexical entries ( which are domain dependent ) and phrase-structure rules ( which are domain independent ) .</sentence>
				<definiendum id="0">phrase-structure rules</definiendum>
				<definiens id="0">Sketch of the semantic process The discrimination module uses semantic information from two different sources : lexical entries ( which are domain dependent</definiens>
			</definition>
			<definition id="6">
				<sentence>However , to get an acceptable number of readings ( possibly only one ) , the scoping generation algorithm , which takes advantage of the idea of Cooper storage , needs some heuristics based on linguistic/semantic knowledge .</sentence>
				<definiendum id="0">scoping generation algorithm</definiendum>
				<definiens id="0">takes advantage of the idea of Cooper storage , needs some heuristics based on linguistic/semantic knowledge</definiens>
			</definition>
			<definition id="7">
				<sentence>ALFresco is an interactive system for a user interested in frescoes .</sentence>
				<definiendum id="0">ALFresco</definiendum>
				<definiens id="0">an interactive system for a user interested in frescoes</definiens>
			</definition>
			<definition id="8">
				<sentence>The videodisc includes images of Fourteenth Century Italian frescoes and relevant monuments and hypertext includes art critics ' comment .</sentence>
				<definiendum id="0">videodisc</definiendum>
				<definiens id="0">includes images of Fourteenth Century Italian frescoes and relevant monuments and hypertext includes art critics ' comment</definiens>
			</definition>
			<definition id="9">
				<sentence>MAIA is the IRST global project .</sentence>
				<definiendum id="0">MAIA</definiendum>
				<definiens id="0">the IRST global project</definiens>
			</definition>
			<definition id="10">
				<sentence>The Upper Model establishes a level of linguistically motivated knowledge organization specifically designed for the task of constraining linguistic realizations .</sentence>
				<definiendum id="0">Upper Model</definiendum>
				<definiens id="0">establishes a level of linguistically motivated knowledge organization specifically designed for the task of constraining linguistic realizations</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>ELU is a unification-based gram mar development environment in the style of PATR-I ( Shieber , 1986 ) , designed especially for experimentatio\ ] with machine translation .</sentence>
				<definiendum id="0">ELU</definiendum>
			</definition>
			<definition id="1">
				<sentence>In constrast to the case of adjectives within compound , which are assigned a special path in the representation ( MOD : N_COMP ) , a complex NP in German , e.g. Abgang yon Lawinen ( 'going-down of avalanches ' ) will get the following representation : PRED lawinen \ ] ARGS \ [ DETYPE indefinlte I\ ] L NCOl~ ( PRBD abgang which is the same as the representation for the compound Lawinenabgang , containing exactly the same information .</sentence>
				<definiendum id="0">PRBD abgang</definiendum>
				<definiens id="0">the same as the representation for the compound Lawinenabgang , containing exactly the same information</definiens>
			</definition>
			<definition id="2">
				<sentence>We have shown how we used ELU , a unificationbased hnguistic environment which has been developed for the implementation of machine translation systems for the testing of two different approaches to the translation of compounds : a transfer-based approach and an intedingua-oriented approach .</sentence>
				<definiendum id="0">ELU</definiendum>
				<definiens id="0">developed for the implementation of machine translation systems for the testing of two different approaches to the translation of compounds : a transfer-based approach and an intedingua-oriented approach</definiens>
			</definition>
</paper>

	</volume>
